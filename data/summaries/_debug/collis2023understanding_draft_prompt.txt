=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Understanding Tool Discovery and Tool Innovation Using Active Inference
Citation Key: collis2023understanding
Authors: Poppy Collis, Paul F Kinghorn, Christopher L Buckley

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2023

Key Terms: understanding, tool, universityofsussex, schoolofengineeringandinformatics, poppycollis, inference, active, animals, discovery, innovation

=== FULL PAPER TEXT ===

Understanding
Tool Discovery and Tool Innovation Using Active Inference
PoppyCollis ⋆1,PaulFKinghorn1,andChristopherLBuckley1,2
1 SchoolofEngineeringandInformatics,UniversityofSussex,Brighton,UK
{pzc20, p.kinghorn, c.l.buckley}@sussex.ac.uk,
2 VERSESAIResearchLab,LosAngeles,California,USA
Abstract. Theabilitytoinventnewtoolshasbeenidentifiedasanimportantfacetofourability
asaspeciestoproblemsolveindynamicandnovelenvironments[17].Whiletheuseoftoolsby
artificialagentspresentsachallengingtaskandhasbeenwidelyidentifiedasakeygoalinthefield
ofautonomousrobotics,farlessresearchhastackledtheinventionofnewtoolsbyagents.Inthis
paper,(1)wearticulatethedistinctionbetweentooldiscoveryandtoolinnovationbyproviding
aminimaldescriptionofthetwoconceptsundertheformalismofactiveinference.Wethen(2)
applythisdescriptiontoconstructatoymodeloftoolinnovationbyintroducingthenotionoftool
affordancesintothehiddenstatesoftheagent’sprobabilisticgenerativemodel.Thisparticular
state factorisation facilitates the ability to not just discover tools but invent them through the
offlineinductionofanappropriatetoolproperty.Wediscusstheimplicationsofthesepreliminary
resultsandoutlinefuturedirectionsofresearch.
Keywords: activeinference,toolinnovation,modelfactorisation,one-shotgeneralization
1 Introduction
Toolinnovationhasbeenidentifiedasacorefeatureofhumancognitiveandculturaldevelopment,and
has provided us with a key adaptive advantage as a species to survive adverse environments [17,24,4].
Whileboththeuseandinnovationoftoolswasinitiallyseenasauniquelyhumancapability,evidence
has shown that a phylogenetically widespread variety of non-human animals engage in forms of tool
manipulation, innovation and manufacture [19]. A large body of research has approached the topic of
tool use in humans, animals and robotic systems [3,7,18]. Here, we define tool use to be “the exertion
of control over a freely manipulable external object (the tool) with the goal of altering the physical
propertiesofanotherobject,substance,surfaceormedium(thetarget,whichmaybethetooluseror
anotherorganism)viaadynamicmechanicalinteraction”[23].However,developingtheunderstanding
ofhowtouseagiventoolissignificantlydifferentfromtheprocessofinventinganewtool.
Toolinnovationrefers to theprocessby whichan agentindependently constructsnovel tools without
relyingonsocialdemonstrationorobservation.Thisrequirestheabilitytoenvisionandconceptualisethe
appropriatetoolforagivenproblem,whiletheknowledgeofhowtophysicallytransformmaterialsduring
construction is referred to as tool manufacture [2]. The task of tool innovation presents a challenging
probleminartificialagents,yet itisonethatweashumans are inherentlyverygoodat.Indeed,research
indicates that we develop innovation skills at a very early age [6]. The animal innovation literature
⋆ Correspondingauthor
P.CollisandP.F.Kinghorn—Theseauthorscontributedequallytothiswork.
3202
voN
7
]IA.sc[
1v39830.1132:viXra
2 P.Collis,P.F.KinghornandC.L.Buckley
suggeststhatwecandistinguishbetweentwodifferentclassesoftoolinnovation:1)thatwhicharisesasa
resultofincidentaldiscoverywheretheanimalthensimplyrepeatsthisactioninthesamecontextand2)
thatwhichistheresultofintentionalactionbytheanimalresultingfromsomeprocessofcausalinference
[25].Herein,wedefinethesetwoclassesofinnovationas tool discovery and tool innovation respectively.
Makingsuchadistinctionforbothanimalsandhumaninfantsischallenginggiventhedifficultyin
determiningtheintentionsdrivingsubjects’proposedsolutionstoaproblem[6].Whilehumanbehavioural
experiments often explore the putative cognitive abilities required for tool innovation, no attempt is
made to model such behaviour [8]. We therefore seek to offer a simple model of the cognitive phenomena
underpinningtheprocessoftoolinnovation.Intheinterestofafocusedinquiryandtomaintainconceptual
clarity,welimitourselvestobeingconcernedwiththecausalreasoninginvolvedintheprocessoftool
innovation,whilechoosingtoomitthechallengesassociatedwiththemotorskillsrequiredtomanipulate
objectsandmanufacturetoolsfromphysicalmaterials.
Inrecentyears,theorieswhichdescribethebrainasbroadlyBayesianhavegainedconsiderabletraction
in the field of neuroscience. The ‘Bayesian brain hypothesis’ posits that perception arises as a result
ofBayesianmodelinversion,withincomingsensorydataupdatingthesecausalmodelsoftheworldin
accordancewithBayes’rule[10].Thetheoryofactiveinference(AIF)extendsthisideaandcastsaction,
perception and learning as being underwritten by the same underlying process of Bayesian inference.
Derivedfromfirstprinciples,thetheoryprovidesaformalaccountofbehaviourarisingasaresultofthe
imperativetominimiseoftheinformation-theoreticquantityofsurprisal.Inotherwords,anautonomous
agentiscontinuallyintheactofaccumulatingBayesianmodelevidence(“self-evidencing”)anditisfrom
thisperspectivethatwecanunderstanddecision-makingunderuncertainty[21].AIFoffersarichdescrip-
tionoftheinternalmechanismsofbelief-basedreasoningandprincipledaccountofthenaturalemergence
of curious and insightful adaptive behaviour [11]. It has also recently been proposed as a framework
well-suitedtorobotics[9].Wethereforechosetoexploretheconceptoftoolinnovationusingthisframework.
Themaincontributionofthispaperis(1)thearticulationofthedistinctionbetweentooldiscovery
andtoolinnovationwithintheAIFframeworkand(2)aminimalmodelofnon-trivialtoolinnovation
thatrequiresgeneralisedinferencesaboutthetoolstructurerequiredtosolveatask.Firstofall,weshow
that witha perfect generative model, theagent can straightforwardly use toolsoptimally to solve a task.
Wethendemonstratethattheagentcandiscoverthecorrecttoolsandlearntosolvethetaskwhenitis
notprovidedwiththisinformationinitsmodel a priori.Finally,weprovideevidencethatfactorisingthe
hiddenstatesofthegenerativemodelintotheaffordancesofthetoolcanenabletheagenttoconceive
offlinetheappropriatepropertiesofthetoolrequiredtosolvethetask.Itisthisdifferencebetweenthe
generativemodelandthegenerativeprocesswhichiskeytofacilitatingtoolinvention.Thisenablesthe
agentto not simply happen upon theappropriate tool during exploration of environmental contingencies,
buttoinventthemthroughtheinductionofanappropriatetoolproperty.Wediscusstheimplications
ofthesepreliminaryresultsandoutlinefuturedirectionsofresearch.
2 Active Inference in Discrete State Space
InAIF,theminimisationofsensorysurprisalisachievedthroughtheminimisationofatractablequantity
calledthevariationalfreeenergyF,knownas(negative)evidencelower-bound(ELBO)inthevariational
inferenceliterature[5].Thisminimisationisperformedviathemaintenanceofaprobabilisticgenerative
modeloftheenvironment.AIFhasbeenwidelyimplementedusingdiscrete-timestochasticcontrolpro-
cessesknownaspartially-observableMarkovdecisionprocesses(POMDPs)[9].Wethereforeimplementour
simulationsagentwithanAIFframeworkindiscretestatespaceusingthePythonpackagepymdp[13].This
specifiesastandardPOMDPgenerativemodelasajointprobabilitydistributionoverobservationso,hidden
UnderstandingToolDiscoveryandToolInnovationUsingActiveInference 3
Fig.1:Thetaskofthetheagentistoreachtherewardbyusingthetoolsprovided.Thesimulationenvironment
showstheagent(robot)canonlymovebetweentheleftandrightrooms(grey)andthereward(shownasapotof
gold)canbeplacedinanyoftheotherrooms(blue).Averticaltool(V)canbepickedupintheleftroom,anda
horizontaltool(H)intherightroom
statess,policiesπandmodelparametersϕ.Incontrasttomuchofthereinforcementlearningliterature,a
policyinthiscaseisdefinedasafixedsequenceofcontrolstatesu foreachtimestepτ thattogetherrepre-
τ
sentaplanofactionoflengthT,π={u ,...,u }[21].WeassumethestandardfactorisationofthePOMDP
1 T
asaproductofconditional(likelihood)distributionsandpriordistributionsoverafinitetimehorizon[1:T].
Themostimportantdistributionswhenspecifyingthisgenerativemodelaretheobservationlikelihood
P(o |s ;ϕ),thetransitionlikelihoodP(s |s ,π;ϕ),andthepriorpreferenceoverobservationsP(o ),
τ τ τ τ−1 τ
knownin pymdp astheA,BandCmatricesrespectively.Wealsofurtherfactoriseourrepresentationsof
o ands intoseparatemodalitiesandfactors:o ={o1,o2,...,oM}ands ={s1,s2,...,sF}inwhichM is
τ τ τ τ τ τ τ τ τ τ
thenumberofmodalitiesandF thenumberofhiddenstatefactorssuchthatthelikelihooddistributions
canbewrittenas:
M
(cid:89)
P(o |s )= P(om|s ) (1)
τ τ τ τ
m=1
F
(cid:89)
P(s |s ,u )= P(sf|s ,u ) (2)
τ τ−1 τ−1 τ τ−1 τ−1
f=1
Weallowstatefactorsinthetransitionlikelihoodstodependonthemselvesandaspecifiedsubset
ofotherstatefactors.3 Sinceweareworkingindiscretespace,theprobabilityofstatesandobservations
canbedescribedbyacategoricalprobabilitydistribution.
Inthiswork,weconsiderthesimpleenvironmentshowninFig.1.Itconsistsofa2x4gridoflocations
in which the agent can only move between two rooms: left and right (shown here in grey). The agent
isalwaysinitialisedintheleft-handroom.Averticaltool(V)islocatedintheleft-handroomwhilea
horizontaltool(H)islocatedintheright-handroom.Inoneoftheremainingrooms(showninblue),a
rewardislocated,anditisthegoaloftheagenttotryandreachthisrewardusingthetoolsprovided.For
example,iftherewardisintheroomdirectlynorthoftheright-handroomasshown,theagentisrequired
tobeintheright-handroomholdingtoolVinordertoreachit.Theagentcanchoosetopickupthetool
ifitisintherelevantroom,whileitmaydroptoolswhilstitisinanyroom(inwhichcase,anyofthetools
intheagent’spossessionaredroppedandreturnedtotheiroriginalrooms).Iftheagentalreadypossesses
atoolandpicksupadifferenttool,thiscreatesacompoundtool(HV).Theroomsdirectlynorth,east
andwestoftheleftandrightroomsareknownastheadjacentrooms andtheseonlyrequiretheindividual
3 This requires a recent branch of pymdp which enables this kind of factorisation. See https://github.com/
infer-actively/pymdp/tree/sparse_likelihoods_111
4 P.Collis,P.F.KinghornandC.L.Buckley
toolsVorHtosolve.Thenortheasternandnorthwesternroomsaretermedthecornerrooms andpresent
agreaterchallengefortheagentastheyrequiretheconstructionofthecompoundtool(HV)tosolve.
Table1:GenerativeModelStructure
States Factors Dimensions Values
Hiddenstates Room 2 Left,Right
Tool 4 Null,V,H,HV
Observations Room 2 Left,Right
Tool 4 Null,V,H,HV
Reward 2 Null,Reward
ControlStates 4 Null,Move,Pick-up,Drop
For the initial experiments, the hidden states of the environment are factorised into two factors,
s ={s1,s2},whichconsistof:roomstateandtoolstate (seeTable.1).Apolicylengthof4time-stepsis
τ τ τ
chosengiventhatthetaskofretrievingtherewardcanalwaysbesolvedoptimallywithin4steps(forany
rewardlocation).As wehaveset thepolicylengthto be4time-steps andwehave4 possibleactions,we
therefore have 256 (44) possible policies which we must individually evaluate by calculating the expected
freeenergyforeverytime-step(seeSection3).Inallexperiments,theagentisequippedwithastrong
priorpreferencefortheobservationofrewardintherewardmodality.Intermsofrelativelogprobabilities,
wespecifythistobe0foranobservationofnulland50foranobservationofreward.Observationsin
allothermodalitieshaveaflatprior(i.e.nopreferencegiven).
3 Policy Inference
InAIF,policyinferenceiseffectivelyasearchprocedureinwhichafreeenergyfunctionalofexpectedstates
andobservationsunderapolicyisevaluatedforeachpossiblepolicy.Oncewehavecalculatedthisquantity
(knownasthe expectedfree energy,G)for each policy,wecan convertthis into aprobability distribution
overtheset.Actionselectionthensimplyamountstosamplingfromthisdistributionaccordingly.Policies
whichmostminimiseG willbeassignedahigherprobabilityandarethereforemorelikelytobechosen.
Sincethevariationalposteriorfactorisesovertime,wecancalculate G foreachtimestepindependently.
Theexpectedfreeenergyforaparticularfuturetimestepunderaparticularpolicyisgivenby:
G (π)=E [lnQ(s |π)−lnP˜(o ,s |π)] (3)
τ Q τ τ τ
where P˜(o ,s |π)=P(s |o ,π)P˜(o ), representing a generative model that is biased to produce
τ τ τ τ τ
preferred observations (for full derivations, see [13]). G (π) can be rearranged in various ways to give
τ
intuitionaboutwhatitactuallyrepresents.Onesuchrepresentationdecomposesthisfreeenergyfunctional
intoanepistemicvalue(informationgain)termandapragmaticvalue(utility)term:
G (π)≤−E [D [Q(s |o ,π)∥Q(s |π)]]−E [lnP˜(o )] (4)
τ Q(oτ|π) KL τ τ τ Q(oτ|π) τ
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
StateInformationGain Utility
Epistemicvaluereferstotheinformationgainfromtheexpectedoutcomesofhiddenstates.Given
a policy, it measures the divergence between the expected states and the expected states conditioned
UnderstandingToolDiscoveryandToolInnovationUsingActiveInference 5
on the observations. This gives rise to curious behaviour in which the agent is compelled to minimise
uncertaintyaboutitsenvironmentviaexploration.Ontheotherhand,theutilitytermsimplymeasures
theextenttowhichtheobservationsexpectedunderapolicyalignwiththeobservationstheagentwishes
toencounter.Thispromotestheexploitationofknowledgeinordertosatisfypreferenceoveroutcomes.
Thistrade-offbetweenexplorationandexploitationthereforenaturallyarisesinAIF;bothimperatives
arecastaswaysinwhichanagentactstoresolveuncertainty.
(a)Informationgaindominatingpolicyselection (b)Utilitydominatingpolicyselection
Fig.2:DecomposingexpectedfreeenergyG intorespectiveinformationgainandutilitycontributionscan
elucidatetheagent’sintendedconsequencesofanaction.TheexpectedfreeenergyG (blackline)isevaluatedover
asetof256policies.Thecomponentswhichcontributetotheselectionofthebestpolicy(circled)arestate
informationgain(darkgreen),parameterinformationgain(lightgreen)andutility(orange).Examplesshownare
instanceswhentheselectedpolicyisa)drivenbyinformationgainasthereislittlevariationinutilityandb)
drivenbyutilityasthereislittlevariationininformationgain
We can visualise this trade-off by plotting the respective utility and information gain components
of the total G. Fig. 2a shows an example in which each of the policies vary little with respect to their
expected utility and the policy selected has been driven by the high information gain component. In
contrast,Fig.2bshowsanexampleofwhenthedominantdrivingforceinpolicyselectionistheutility
component while information gain remains largely invariant across policies. Note that we also include
aparameterinformationgaintermwhichisexplainedinSection4.
4 Parameter Inference
Learningin AIFis a processof inferenceover themodel parameters,ϕ, which aresimply the categorical
likelihood distributions. We treat these parameters as something over which the agent maintains and
updates beliefs (i.e. as random variables). Consider the example of an A matrix, which encodes the
observationlikelihoodmodelP(o|s),withtheentryA[i,j]representingtheprobabilityofseeingobservation
igivenstatej.Thereisthereforeaseparatecategoricaldistributionforeachstate(i.e.eachcolumnsums
to 1). The Dirichlet distribution is a conjugate prior for the categorical distribution, and we therefore
modelpriorbeliefsoverthecategoricalasaDirichlet.Itcanbeshownthat,whentheagentobtainsnew
empirical information, the Bayesian process of updating this prior is simply a count-based increase of
theDirichletparametersaccordingtotheobservationoandinferredstates[13,15]:
α =α +o⊗s (5)
posterior prior
6 P.Collis,P.F.KinghornandC.L.Buckley
where α represents the Dirichlet parameters. Now that we are treating model parameters as random
variables,wecanexpandG toincludetheexpectedparameterinformationgaincomponent:
G (π)≤−E [D [Q(s |o ,π)∥Q(s |π)]]−E [D [Q(ϕ|o ,π)∥Q(ϕ|π)]]−E [lnP˜(o )]
τ Q(oτ|π) KL τ τ τ Q(oτ|π) KL τ Q(oτ|π) τ
(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
StateInformationGain ParameterInformationGain Utility
(6)
Thiswilldrivetheagenttoseekobservationswhichleadtoalargerchangeinthecategoricaldistribution.
5 Experiment 1: Tool Use
Inthefirstsetofexperiments,theagenthasaperfectprobabilisticgenerativemodeloftheworld.This
means that the correct transition likelihood and observation likelihood distributions are provided and
thereforenolearningisrequired.Wethen showthatthe agentcan straightforwardlyinfertheoptimal
actionsinordertoachieveitsgoalofreachingthereward.Weusethisasasimplemodeloftooluseinan
autonomousagent,giventhedefinitionoftoolusedefinedpreviously[23].Bythisaccount,oursimulated
agentconductstoolusebyactingto“exertcontrolover”toolsV,HorHVinorderto“alterthephysical
properties”ofthetooluser(byextendingtheagent’sreach)enablingittosuccessfullyretrievethereward.
Inthissense,wereducetoolusetoanactionsequencingproblem.
Table2:Comparingoptimalnumberofstepsrequiredtosolveeachrewardlocationwithactionstaken.Whenthe
generativemodelisperfectlyknown,theagentsolvesthetaskoptimally
RewardLocation OptimalNo.ofSteps ActionsofAgent
North-left 1 Pick-up
North-right 2 Pick-up,Move
East 2 Move,Pick-up
West 3 Move,Pick-up,Move
Northeast 3 Pick-up,Move,Pick-up
Northwest 4 Pick-up,Move,Pick-up,Move/
Move,Pick-up,Move,Pick-up
For each trial, we place the reward in one of the possible reward locations and allow the agent 12
time-steps in which to act in the world and obtain the reward. The agent uses all 12 time-steps, and
thereforeif it hasfoundthereward, theoptimalbehaviourwouldbe toperforman actionthatwillkeep
itinthesamestate(i.e.theaction“Null”).
As expected, the agent solves the task of obtaining the reward optimally for each reward location
(Table.2).Giventhestochasticnatureofpolicyselection,wenoteinthattheagentsolvesthenorthwest
roomviatwodifferentmethods,yetbothareoptimal(i.e.oflength4).Sincethegenerativemodelisfully
known, the agent gains no newinformation aboutstates or parametersduring inference.Indeed, Fig. 3
shows thatifweplotthe relativeutilityand information gaincontributionstotheexpectedfree energy
of each policy during action selection, we see that it only comprises of a utility component compared
toFig.2(i.e.thereisnoepistemicvaluecontributiontoG).
UnderstandingToolDiscoveryandToolInnovationUsingActiveInference 7
Fig.3:Whenthegenerativemodelisperfectlyknown,theselectedpolicyisbasedsolelyontheutilitycomponent
ofG.AnexampleofG (blackline)evaluatedforall256policiesandtheselectedpolicy(circled)whichistheone
withthehighestutility(orange).Notethatsincethegenerativemodelisfullyknown,andtheenvironmentis
fullyobserved,allpolicieshavezeroinformationgaincomponent
6 Experiment 2: Tool Discovery
Next,weinvestigatetheabilityoftheagenttolearnhowaparticulartoolsolvesthetask.Wepresentthis
asa toyexampleoftooldiscoverygiven thatknowledgeabout howtocreate atool arisesincidentallyas
aresultofenvironmentalexploration.Whilstweagainprovidetheagentwithafullyknownobservation
likelihooddistribution,forthefollowingexperimentweinitialisetheagentwithauniformlydistributed
transition likelihood model. This means that the agent initially knows nothing about how states and
actions effect future states. It therefore must learn these state transitions rather than being provided
with this information from the outset (as in experiment 1). The agent happens upon the correct tool
touseforagivenrewardlocation,andthenrepeatsthisactioninthesamecontexts.Thisisinlinewith
ourpreviousdefinitionoftooldiscovery[25].
(a)Finishingwithrewardinnortheastroom (b)Finishingwithrewardinnorthwestroom
Fig.4:Thenumberofstepstakentosolvethetaskforeachrewardlocationdecreasesquicklyoverrunstothe
optimalnumberofsteps,reflectingtheagentlearningviadiscovery.Graphsshowthemean(+/-ste)numberof
stepstosolverewardlocationaveragedover20independenttrials.Theagentisexposedtoadifferentreward
locationsevery10runs(dashedlines).Therewardisfirstlocatedintheadjacentrooms(intheordernorth-right,
west,north-left,east)beforebeingpresentedwitha)thenortheastorb)thenorthwestroomforfinal3runs
(40-42).Forbothcases,despitelearninghowtocreateaVandHtoolintheearlierruns,theagentstillhasto
learnabouttheHVtoolwhentherewardisplacedinacornerroom
8 P.Collis,P.F.KinghornandC.L.Buckley
Fig.4showsthatthenumberofstepstheagenttakestofindtherewarddecreasesoverthenumberof
runs.Inthiscontinuallearningtask,eachtimetherewardlocationchanges(atruns0,10,20,30and40)
itdemandsthelearningofanewtoolandweseeaninitialincreaseinthenumberofstepsrequirestosolve
thetask.Thisisbecausetheinformationtheagenthasaboutstatetransitions(i.e.howstatesandactions
giverisetostatesatthenexttime-step)isnotsufficienttosolvethetask.Theagentthereforeexploresmore
oftheenvironmentbeforeencounteringthecorrecttoolrequiredtosatisfyitspreferenceforthereward
observation.Wethenseeasharpdropaftertheagenthaslearnedabouttherequiredstatetransitions,
andthenumberofstepstakentosolvethetaskquicklyplateaustotheoptimalnumbershowninTable.2.
Interestingly,asaresultoftheorderinginwhichtherewardlocationsarepresented(north-right,west,
north-left,east,...),theagentsolvesthenorth-leftandeastrewardlocationsoptimallyfromtheoutset.
Thisisduetothefactthatthesolvingofpreviousadjacentrooms(north-rightandwest)resultedfromthe
learningoftoolVandHrespectively.Whentheagentthenencounterstherewardintheremainingadjacent
rooms,ithasalreadylearnedaboutthecorrectactionstocreatethesetoolstosolvethetaskdespitenever
havingseentheseparticularrewardlocationsbefore.Thecornerroomsrequiremorestepsdespitehaving
alreadylearnedVandH,astheagentmuststilldiscoverthenewtoolHV.Giventhattheagentisalways
initialisedintheleft-handroom,thenorthwestcorner(Fig.4b)takesmorestepstosolvethatthenortheast
corner(Fig.4b)becauseitinvolvesamorecomplexactionsequencetoretrievethereward(seeTable.2).
(a)North-rightroom (b)Eastroom (c)Northeastroom
Fig.5:Theagentonlylearnsthetoolsthatitneedstolearninordertosolvethetask.Weprovideameasureof
howwelltheagentknowseachtoolbylookingattheposteriorprobabilityassociatedwiththecorrectcontrolstate
(i.e.action)forcreatingeachtoolwhensolvingforroomsa)north-rightb)eastandc)northeastover125steps
Importantly, given that the minimisation of G naturally incorporates two competing imperatives
(utilityandinformationgain),thismeansthattheagentlearnsonlythetoolsthatitneedstolearnin
ordertosolvethetask,anddoesnotcontinueexploringitsenvironmentifitisabletoleverageitscurrent
knowledgetoeffectivelyrealisepriorpreferences.Fig.5ashowsthatforthenorth-rightroom,theagent
onlylearnstheverticaltool(V).Thisisbecausethefirsttoolitpickedup(V)allowedittosolvethetask
andthereforetheagentdidnotneedtocontinueexploringthehiddenstatesoftheenvironmentasithad
alloftheknowledgeitneeded.Fig.5b)showsthatfortheeastroom,theagentfirsttriedtheverticaltool
(V),howeverthisdidnotleadtotheagentobservingpreferredobservations(reward)andthereforeitdoes
notinfertheactionofpickingupthistoolagain.Instead,itpursuespolicieswhichyieldhighinformation
gain(i.e.itexploresnewstatesoftheenvironment)andfindsthatpickinguptoolHleadstoarewarding
observation.Byselectingpolicieswhichmaximiseutility,itthereforerepeatsthisaction(“pick-up”)inthe
samecontext,andlearnsthistoolwithmoreconfidencewhileneglectingtoexploreotheroptions.Finally,
Fig.5cshowsthatinordertodiscoverthecompoundtool(HV),theagentfirsthappensupontoolsV
andH(asthesetoolsaremorelikelytobestumbledacrossgiventheyrequirealesscomplexsequence
UnderstandingToolDiscoveryandToolInnovationUsingActiveInference 9
ofactions inorderto learnaboutthem).However,these donotprovideitwith high utility.Sincethere
areunknownstates(suchastoolHV)thatprovideitwithhighinformationgain,theagentcontinues
exploringandthenfindsthatcreatingthecompoundtoolbringsaboutitspreferredobservations.
(a)Finishingwithrewardinnortheastroom (b)Finishingwithrewardinnorthwestroom
Fig.6:Policyselectionisinitiallydominatedbyinformationgain,butisthenveryquicklydrivenbyutilityasthe
agentlearnsnewinformation.Graphshowshowtheselectedpolicyranksinthecontextofallpossiblepoliciesin
termsofutilityandinformationgain(averagedover20independenttrials)(bestrankis0,worstis256).Like
Fig.4,therewardlocationchangesevery10runs(dashedlines)intheordernorth-right,west,north-left,east.
Theagentisthenpresentedwitha)thenortheastorb)northwestroomforthefinal3runs
A policy is selected on its value of G which is composed of both expected utility and expected
information gain. We can visualise the evolution of this trade-off in driving policy selection during a
continual learning trial. For each time-step, we see how the chosen policy ranks in the ordered list of
all policies with respect to utility and the ordered list of all policies with respect to information gain.
Thisrank providesuswith ameasure oftherelative contributions ofutilityand informationgainin the
selectionofapolicy.Forexample,ifthechosenpolicyranksveryhighlyforutility,andyetranksverylow
in the context ofthe best policiesfor informationgain, we knowthat thepolicy (and therefore resultant
action)hasbeenselectedprimarilyduetoitshighutility.
AsFig.6shows,foreachrewardlocation,theinformationgaincomponentisinitiallyveryhighand
therefore dominates action selection. This is because when the reward location is changed, the state
transition information is not adequate to solve the task. The gain in information quickly drops as the
agentlearnstransitionsviaexploration,whiletheutilityrankofthepolicyincreasesasitcanleverage
thisnewlylearnedinformationtoseekthepreferredobservationofthereward.Notethatatruns20and
30,thisspikeininformationgainislowerthatat0and10.Thisisbecausetheagenthasalreadylearned
aboutcreatingtoolVandHinthenorth-rightandwestrewardlocationsrespectively.Whentheagent
is then presented with the novel adjacent reward locations (north-left and east), it has the advantage
ofalreadyhavingtheknowledgeofhowtopickupthecorrecttooltousetosolvetheproblem.Forthe
finalrewardlocation(northeastforFig.6aandnorthwestforFig.6b),wealsoseeaspikeininformation
gain.ThisisinagreementwithFig.4whichshowsthatwedoindeedseeanincreaseinthenumberof
steps taken to solve these final rooms. Despite having knowledge about the individual tools H and V,
theagentmustexplorefurtherto‘discover’thecompoundtool.
Wehavethereforeshownthattheagentcanleveragetheknowledgegainedintheincidentaldiscovery
ofrequiredstatetransitionstosolvethetask.Thisamountstoasimplemodeloftooldiscoverybehaviour
inaccordancewithourpreviouslydefineddefinition.
10 P.Collis,P.F.KinghornandC.L.Buckley
7 Experiment 3: Tool Innovation
ThefollowingexperimentinvestigatestheconceptoftoolinnovationinourAIFagent.Inordertoachieve
this,theagentmustbeabletoanalysetheproblemandidentifythekindofthetoolrequiredtosolve
thetask.Thisentailsdevelopingagroundedunderstandingoftheobjectsintheenvironmentwhichcan
thenbeleveragedto constructasuitabletoolthroughaprocessofgeneralisation.Fortheacquisitionof
groundedknowledgeabouttheworld,weturntotheconceptof‘affordances’fromecologicalpsychology
[12].Thisreferstoopportunitiesforactionprovidedbytheenvironment.Intheroboticsliterature,this
isdefinedasthe“relationshipbetweenanactor(i.e.,robot),anactionperformedbytheactor,anobject
onwhichthisactionisperformed,andtheobservedeffect”[1].
Weadjust our generative model to incorporate the following tool affordances into thehidden states:
the horizontal reach (x-reach) and vertical reach (y-reach) afforded by each tool and the room state
s ={s1,s2,s3}.Eachaffordancestatecantakeabinaryvalue.Werefertothisasthe Affordance Model
τ τ τ τ
while the previous model which included an unfactorised tool state is referred to as the Tool State
Model. Importantly, these affordances do not depend on one another, which allows for generalisation
of learning in novel situations (i.e. the agent does not need to separately explore the x-reach state in
thecontextoftwodifferenty-reachstates).Thisalignswiththeconceptof disentangled representations,
characterisedasdisjointrepresentationsoftheunderlyingtransformationpropertiesoftheworld[14].That
is,transformationsthatvaryasubsetofpropertiesoftheworldstate,whileleavingallothersinvariant.
In this sense, the agent can learn solely about the tool V and tool H, and when faced with a new
rewardlocationinwhichitrequiresbothapositivex-reachandy-reach,itshouldspontaneouslyproduce
thecompoundtool(HV)inanoptimalway.Thisisasimpleyetnon-trivialnotionofinnovationinwhich
theagentdoesnotmerelyjustdiscoveranewtool(asinexperiment2).Theagentisabletoencounter
a new situation (reward location), understand the structure of the required solution (both a non-zero
x-reach and y-reach) and generate the required solution (tool HV). We can think of this as a simple
exampleof‘one-shot’generalisationtonovelstimuli[20][22].
Totestthishypothesis,wehavetheagentlearn
theentriesofthetransitionlikelihooddistribution
model from scratch (i.e. we initialise it as a uni-
form distribution as in experiment 2). However,
our transition likelihood now includes the new
factorised tool states (see Fig. 7). In a continual
learningtask, wepresent theagentwith theadja-
centrooms(whichonlyrequirethelearningabout
HandV)andthentestitonthenortheastroom
(whichrequirestoolHV).
Fig.7: In the Tool State Model used of experiment 2,
Fig. 8a shows that, indeed, when the Affor-
there is a one to one mapping between the tools the
dance Model agent has only previously learned
agent observes, and the internal representations it has
about tools H and V, it successfully creates tool forthem(None,V,H,HV).IntheAffordanceModelin
HVoptimally(havingneverseenthisobservation experiment3,theagentseparatesthelantenttoolstates
before). Withthe Tool StateModelin experiment intopropertiesofx-reachandy-reach
2,thistaskwasnotsolvedoptimally(asitinitially
tookanaverage ofroughly5steps tosolve).As Fig. 8dshows,thiscoincides withagreater information
gaincomponentdrivingactionselection,meaningtheagentisexploringinordertodiscoverthecompound
tool.OntheotherhandtheinformationgaincomponentfortheagentwiththeAffordanceModelismuch
lower.Thissuggeststhat thefactorisationofhiddenstatesintoaffordances indeedequipstheagentwith
UnderstandingToolDiscoveryandToolInnovationUsingActiveInference 11
theabilitytoleverageitscurrentknowledgeinordertocomposerelevantaffordancesandspontaneously
‘invent’thenewtool.
It is worth noting, that when repeating this experimental procedure of exposing the Affordance
Modelagenttotheadjacentroomsandthentestingonthenorthwest(ratherthanthenortheast)room,
the agent does not solve this optimally, but‘near-optimally’. As Fig.8b shows,the Affordance Model
agentsolvesthistaskmarginallyquickerthantheagentwiththeToolStateModel,howeveritdoesnot
immediately find the optimal solution of 4 steps. Upon inspection of the learned transition likelihood
distributions,itappearsthatthereisalargeinformationgaincomponentofG thatdrivestheagentto
select the action‘drop’ (and thisis reflectedinFig. 8e).The agenthas neverexplored whatthis action
‘drop’ does in the left-hand room with no tools, and therefore it repeats this action until it no longer
yields highstate informationgain. Onceit has learnedthis particularfact, it thengoeson to selectthe
optimalpolicyandsolvesthetaskinthenext4steps.
Toconfirmthatthisisindeedwhatiscausingthesub-optimalbehaviour,wetailorourpolicyselection
strategyonthecriticalruns.Werepeattheexperimentaltrial,butoncetherewardlocationhaschanged
to the final northwest room, we ignore the information gain components of G. The agent therefore
selectspoliciesbasedonutilityalone.Afterthisadjustment,theAffordanceModelagentthensolvesthe
northwest room optimally (see Fig. 8c). Importantly, when information gain is ignored for the Tool State
Model,thisstilldoesnotleadtheagenttosolvethetaskoptimally.Thisisbecauseitdoesnothavethe
requiredknowledgeaboutthecompoundtoolwhiletheAffordanceModelhasalloftheinformationit
needsinordertosolvethetaskbyaprocessofinduction.
(a)Rewardinnortheastroom (b)Rewardinnorthwestroom (c)Northwestroom,utilityonly
(d)Rewardinnortheastroom (e)Rewardinnorthwestroom (f)Northwestroom,utilityonly
Fig.8:Factorisingthehiddenstatesintotoolaffordancesenablestheagenttoperformone-shotgeneralisation.All
graphsaretheresultsof3runsfollowingexposuretoalladjacentrooms(10runsperroom)andaveragedover10
independenttrials.ThetoppanelcomparestheToolStateModeltotheAffordanceModelintermsofthemean
numberofsteps(+/-se)takentosolvea)thenortheastroomb)northwestroomandc)northwestroomselecting
policiesbasedonlyonutility.Thebottompanelshowstheutilityandinformationgainrankoftheselectedpolicy
ford)thenortheastroome)northwestroomandf)northwestroomselectingpoliciesbasedonlyonutility
12 P.Collis,P.F.KinghornandC.L.Buckley
8 Discussion
Wehave distinguished between tool use,tooldiscovery andtool innovation andaskedwhatthis might
look like using the framework of AIF. We then ground this work with the construction of a simple
modelinordertotakeseriouslythisdistinctionandseewhatinsightscanbedrawn.Weprovidethefirst
evidenceforthenecessarypropertiesassociatedwiththeprocessoftoolinnovation:namelythatofoffline
inductionofappropriatetoolstructurethroughcomposingrelevantaffordances.
Wehaveidentifiedthatwhensolvingthenorthwestroom,theagentwiththeAffordanceModelisnot
(sub-optimally) solving the task by having to discover the tool, as is the case with the agent with the
ToolStateModel.Rather,theagentseekstoinvestigateaspecificstatewhichithasneverseenbefore
andwhenithassufficientlylearnedthisfact(suchthattheinformationgainthatityieldsissignificantly
diminished), it subsequently solves the task in the optimal number of steps. Further investigation is
requiredtoaskwhytheutilityisnotenoughtooverridethishighinformationgainwhenitalreadyhas
theknowledgeofthecorrecttooltoemployandthestatetransitionstocreatethistool.
We acknowledge that inour choice to factorise thehidden state of the agent’s generative model into
thetoolaffordancesofx-reachandy-reach,weplaytheroleofanintelligentdesigner.Ideally,wewould
like to have autonomous systems that choose what to learn from the environment and factorise their
modelinawaythatbestexplainsthelatentcausesofsensoryobservations.Smith et al. [22]introducean
approachwherebyaprobabilisticgenerativemodelhasflexibilityinthehiddenstates.Theideaisoneof
furnishing of extra “slots” in the hidden states, allowing the agent to expand its generative model to
incorporatenewinformationwhenencounteringnewconcepts.AprocessofBayesianmodelreduction
thenactstoprunethemodel,ensuringthatmodelcomplexityisreducedifinfacttwoconceptscanbe
explainedbythesame underlyingcause.Thisapproachhasbeenfurtherextendedtodeephierarchical
AIF models,facilitating theformation offlexible andgeneralisable abstractionsduringa spatial foraging
task [16]. This kind of adaptive structure learning would be useful in the context of tool innovation,
allowingustoinferthebestaffordancestorepresentatool.Wethereforeidentifythisapproachasan
interestingavenueforfurtherresearchinthecontextoftoolinnovationinAIFagents.
Finally, we note that our model is limited given our intentional choice to omit the sensorimotor
challengesassociated withbothtoolmanipulationandtoolconstruction.Giventhattoolmanufacture
hasbeenidentifiedbyBeck et al. [2]asakeycomponentintheprocessoftoolinnovation,futurework
shouldlooktowardsconstructingmodelswhichcaneffectivelyhandlemorephysicallyrealistictasks.
9 Conclusion
Overall, we have provided a minimal description of the distinction between tool discovery and tool
innovationundertheformalismofactive inference.Wehaveusedthistothenexplore asimplemodelof
toolinnovationinan AIFagent byintroducinga factorisationofhiddenstates ofthegenerativemodel
intoaffordances.Thisparticularstructuralchoiceaffordstheagentwiththeabilitytogeneralisewhatit
haslearnedaboutstatetransitionsandconceptualiseasuitabletoolviaaprocessofinduction.Wehave
discussedtheimplicationsandlimitationsofourresultsandoutlineddirectionsforfurtherresearch.
10 Author Contributions
P.F.K.conceivedtheprojectandbothdesignedandconductedexperiments.P.C.designedandconducted
experimentsandwrotethemanuscript.C.L.B.supervisedtheproject.
UnderstandingToolDiscoveryandToolInnovationUsingActiveInference 13
Acknowledgements Thisresearch wasfundedundertheUKRI HorizonEuropeGuaranteescheme as
partoftheMETATOOLprojectledbytheUniversidadPolit´ecnicaDeMadrid.
References
1. Andries,M.,Chavez-Garcia,R.O.,Chatila,R.,Giusti,A.,Gambardella,L.M.:Affordanceequivalencesin
robotics:aformalism.Frontiersinneurorobotics 12, 26(2018)
2. Beck,S.R.,Apperly,I.A.,Chappell,J.,Guthrie,C.,Cutting,N.:Makingtoolsisn’tchild’splay.Cognition
119(2),301–306(2011)
3. Bentley-Condit, V., Smith: Animal tool use: current definitions and an updated comprehensive catalog.
Behaviour 147(2),185–32A(2010)
4. Biro,D.,Haslam,M.,Rutz,C.:Tooluseasadaptation(2013)
5. Blei,D.M.,Kucukelbir,A.,McAuliffe,J.D.:Variationalinference:Areviewforstatisticians.Journalofthe
AmericanstatisticalAssociation 112(518),859–877(2017)
6. Breyel,S.,Pauen,S.:Thebeginningsoftoolinnovationinhumanontogeny:Howthree-tofive-year-oldssolve
theverticalandhorizontaltubetask.CognitiveDevelopment 58,101049(2021)
7. Cabrera-A´lvarez,M.J.,Clayton,N.S.:Neuralprocessesunderlyingtooluseinhumans,macaques,andcorvids.
FrontiersinPsychology 11,560669(2020)
8. Chappell,J.,Cutting,N.,Apperly,I.A.,Beck,S.R.:Thedevelopmentoftoolmanufactureinhumans:what
helpsyoungchildrenmakeinnovativetools?PhilosophicalTransactionsoftheRoyalSocietyB:Biological
Sciences 368(1630),20120409(2013)
9. Da Costa, L., Lanillos, P., Sajid, N., Friston, K., Khan, S.: How active inference could help revolutionise
robotics.Entropy 24(3), 361(2022)
10. Friston,K.:Thehistoryofthefutureofthebayesianbrain.NeuroImage 62(2),1230–1233(2012)
11. Friston,K.J.,Lin,M.,Frith,C.D.,Pezzulo,G.,Hobson,J.A.,Ondobaka,S.:Activeinference,curiosityand
insight.Neuralcomputation 29(10),2633–2683(2017)
12. Gibson,J.J.:Thetheoryofaffordances.Hilldale,USA 1(2),67–82(1977)
13. Heins,C.,Millidge,B.,Demekas,D.,Klein,B.,Friston,K.,Couzin,I.,Tschantz,A.:pymdp:Apythonlibrary
foractiveinferenceindiscretestatespaces.arXivpreprintarXiv:2201.03904(2022)
14. Higgins,I.,Amos,D.,Pfau,D.,Racaniere,S.,Matthey,L.,Rezende,D.,Lerchner,A.:Towardsadefinitionof
disentangledrepresentations(2018)
15. Murphy,K.P.:Machinelearning:aprobabilisticperspective.MITpress(2012)
16. Neacsu, V., Mirza, M.B., Adams, R.A., Friston, K.J.: Structure learning enhances concept formation in
syntheticactiveinferenceagents.PLOSONE 17(11),1–34(112022)
17. O’Brien,M.J.,Shennan,S.:Innovationinculturalsystems:contributionsfromevolutionaryanthropology.Mit
Press(2010)
18. Qin,M.,Brawer,J.N.,Scassellati,B.:Robottooluse:Asurvey.FrontiersinRoboticsandAI 9, 369(2022)
19. Reader,S.M.,Morand-Ferron,J.,Flynn,E.:Animalandhumaninnovation:novelproblemsandnovelsolutions
(2016)
20. Rezende,D.J.,Mohamed,S.,Danihelka,I.,Gregor,K.,Wierstra,D.:One-shotgeneralizationindeepgenerative
models(2016)
21. Sajid,N.,Ball,P.J.,Parr,T.,Friston,K.J.:Activeinference:demystifiedandcompared.Neuralcomputation
33(3),674–712(2021)
22. Smith,R.,Schwartenbeck,P.,Parr,T.,Friston,K.J.:Anactiveinferenceapproachtomodelingstructure
learning:Conceptlearningasanexamplecase.Frontiersincomputationalneuroscience 14, 41(2020)
23. StAmant,R.,Horton,T.E.:Revisitingthedefinitionofanimaltooluse.AnimalBehaviour 75(4),1199–1208
(2008)
24. Stout,D.:Stonetoolmakingandtheevolutionofhumancultureandcognition.PhilosophicalTransactionsof
theRoyalSocietyB:BiologicalSciences 366(1567),1050–1059(2011)
25. Whiten, A., Van Schaik, C.P.: The evolution of animal ‘cultures’ and social intelligence. Philosophical
TransactionsoftheRoyalSocietyB:BiologicalSciences 362(1480),603–620(2007)

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Understanding Tool Discovery and Tool Innovation Using Active Inference"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
