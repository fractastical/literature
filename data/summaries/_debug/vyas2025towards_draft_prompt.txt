=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Towards smart and adaptive agents for active sensing on edge devices
Citation Key: vyas2025towards
Authors: Devendra Vyas, Nikola Pižurica, Nikola Milović

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Key Terms: system, agents, devices, perception, adaptive, smart, edge, deep, verses, learning

=== FULL PAPER TEXT ===

Towards smart and adaptive agents for active
sensing on edge devices
Devendra Vyas* Nikola Pizˇurica* Nikola Milovic´
VERSES Computer Science Center, U. of Montenegro Fain Tech
Igor Jovancˇevic´ Miguel de Prado† Tim Verbelen†
Computer Science Center, University of Montenegro VERSES VERSES
Abstract—TinyML has made deploying deep learning models
on low-power edge devices feasible, creating new opportunities
for real-time perception in constrained environments. However,
theadaptabilityofsuchdeeplearningmethodsremainslimitedto
data drift adaptation, lacking broader capabilities that account
for the environment’s underlying dynamics and inherent uncer-
tainty. Deep learning’s scaling laws, which counterbalance this
limitation by massively up-scaling data and model size, cannot
be applied when deploying on the Edge, where deep learning
limitations are further amplified as models are scaled down for
deployment on resource-constrained devices.
This paper presents an innovative agentic system capable of
performing on-device perception and planning, enabling active
sensing on the edge. By incorporating active inference into our
solution,ourapproachextendsbeyonddeeplearningcapabilities,
allowing the system to plan in dynamic environments while
operating in real-time with a compact memory footprint of as
Fig. 1. Conceptual Framework for Smart Edge Agents, composed of a
little as 300 MB. We showcase our proposed system by creating
deep-learning perception module and an active inference planning module
anddeployingasaccadeagentconnectedtoanIoTcamerawith for active (visual) sensing. The camera frames are processed by the object
panandtiltcapabilitiesonanNVIDIAJetsonembeddeddevice. detector,whichforwardsthedetectedresultstotheactiveinferencemodule.
The saccade agent controls the camera’s field of view following Our agent plans its next action, minimizing free energy, and dynamically
optimal policies derived from the active inference principles, adaptingtotheenvironment.
simulating human-like saccadic motion for surveillance and
robotics applications.
Other areas, such as smart cities and surveillance systems [7],
Index Terms—smart agents, edgeAI, dynamic planning
demandrobustmonitoringsolutionsforcrowdedareastotrack
I. INTRODUCTION movement, anticipate potential issues, and enhance safety.
The human visual system has a unique ability to focus on Activesensingbecomesevenmoreapparentinrobotics,where
key details within complex surroundings, a process known as the agent’s actions determine the next observations for the
saccading [1]. This quick and dynamic scanning allows us system, driving exploration [8].
to gather essential information. Saccading is part of a larger Recent advances in machine learning (ML), particularly in
concept known as active (visual) sensing [2], [3], an innate deep learning, have substantially improved sensing accuracy
capability that enables organisms to forage for information and complexity. However, state-of-the-art deep learning mod-
and dynamically adapt to an evolving environment [4]. els show limitations in their adaptability [9], i.e., the ongoing
Activesensingiscriticalinvariousapplications,particularly accumulation and refinement of knowledge over time. These
whentheinformationisunavailableortoovasttoprocess.For limitationsarefurtheramplifiedwhenthesemodelsarescaled
instance, in remote sensing for Earth observation [5] or aerial down for deployment on resource-constrained edge devices,
search-and-rescue operations [6], the system must parse vast, where memory, computational power, and energy efficiency
detailed scenes, focusing only on critical features, e.g., ice- are limited. As a result, true active sensing—requiring both
sea or missing person. Similarly, in sports events, tracking perception and planning—remains challenging to implement
dynamic scenes requires a system to zoom in on players, on embedded systems.
capturingplayers’facesorgestureswhilenotmissingtheplay. Active inference, an approach rooted in the first principles
of physics, offers a promising alternative to address these
* Co-firstauthors.† Co-seniorauthors. limitations [10]. Emerging as a viable paradigm, active infer-
{devendra.vyas,miguel.deprado,tim.verbelen}@verses.ai
encegroundslearningwithinprobabilisticprinciples,enabling
ThisworkwaspartlysupportedbyHorizonEuropedAIEdgeundergrant
No.101120726. smartsystems,oragents,tomodeltheuncertaintyandvariabil-
5202
tcO
61
]OR.sc[
2v26260.1052:viXra
ity inherent in dynamic environments, making it well-suited C. Probabilistic computing
forcontinuallearningandadaptivedecision-making[11].This
Probabilistic computing has shown promise for active sens-
shift represents a move beyond perception-focused AI toward
ing by optimizing information acquisition in dynamic envi-
adaptive systems capable of adjusting their actions based on
ronments. Probabilistic principles can be used to maximize
environmental feedback. Thus, agents on the edge provide
information gain through camera adjustment [17], which is
a powerful framework for real-time perception and planning
valuable in applications like surveillance, sports analysis, and
without dependence on cloud resources, ensuring low-latency
patient monitoring. This is extended to maximize mutual
responses and enhanced data privacy.
informationgaininmulti-camerasetups,combiningobjectives
Thisworkpresentsanintegratedsystemthatcombinesdeep
like exploration and tracking to enable adaptive, informed
learning and active inference to realize an adaptive, memory-
scene monitoring [18]. Unlike these methods, we base our
efficient,real-timesaccadeagentforedgedevices.Oursystem
probabilistic agent on active inference, grounding our ap-
leverages a deep learning-based object detection module for
proach in the Free Energy Principle.
initial perception and an active inference planning module to
activelysenseandadapttotheenvironment.Thesaccadeagent III. METHODOLOGY
can observe, plan, and control a camera for strategic informa- Todevelopaneffectiveactivesensingsolution,itisessential
tion gathering, demonstrating adaptive decision-making and to consider the unpredictable and dynamic nature of real-
exploration. Our deployment on an Nvidia Jetson platform world environments. Smart sensors must be able to handle
showcasesthepotentialforresponsiveapplicationsinrobotics uncertainty and adapt to constant changes. Therefore, any
and smart city environments, highlighting the feasibility of change in the observed environment must influence the policy
edge-based adaptive systems for complex, real-world tasks. selection for the following action. For a system to operate
autonomously and intelligently, it must be able to adjust its
II. RELATEDWORK
perception and actions in real time without relying on cloud
We categorize the related work in three main areas: processing.Thisrequirementforon-deviceadaptationsupports
faster decision-making and enhances data privacy.
A. Active Sensing
In this work, we propose an efficient active sensing agent
Active sensing is an essential building block across diverse composedoftwomodules:i)adeeplearning-basedperception
fieldswhereefficientscanningisneededtolocateandfocuson module and ii) an active inference module that enables plan-
criticaldetails.InEarthobservationapplications,theAutoICE ning and control. This architecture combines deep learning’s
Challenge [5] addresses sea ice detection, where maximiz- feature extraction performance with active inference’s Bayes-
ing area coverage and detection through adaptive zooming optimal control, presenting an adaptable and scalable solution
is indispensable for safe navigation. Similarly, active search for various resource-constrained edge applications.
strategies are also explored in rescue operations [6], stressing
A. Perception
techniques like saccading to enhance search efficiency over
large areas. For public safety in smart cities, deep learning Deeplearningtechniqueshaveachievedremarkablesuccess
methods are proposed for people tracking and counting [12], in detecting features of interest in images, audio, or textual
enabling security, crowd management, and urban analytics data[19].Convolutionalneuralnetworks(CNNs)havedemon-
applications. strated exceptional performance in visual tasks like object
detection and segmentation [20]. By employing deep learning
B. TinyML
forperception,activesensingagentscanrapidlyprocesshigh-
TinyML has made deploying ML models on low-power dimensional data and identify patterns that provide relevant
edge devices feasible, bringing opportunities for real-time spatial information.
perception in constrained embedded devices. The edge de- Recent advances in transformer architectures and large
ployment pipeline is summarized in [13], streamlining end- language models (LLMs) have further expanded the scope
to-end model deployment on embedded platforms to enhance of deep learning. Transformers excel in capturing complex
the accessibility of edgeAI applications. A popular example dependencies and long-range relationships in data, making
of this process is YOLO [14], an efficient architecture for them very powerful for tasks requiring a deeper contextual
deployingobjectdetectionandtrackingattheedge,improving understanding. The self-attention mechanism, being a core
the responsiveness of embedded applications. Recent works componentoftransformers,enablesthemodelstofocusselec-
have made progress in enabling on-device domain adaptation, tively on the most relevant aspects of the data, enhancing the
adjustingdeployedapplicationstoaccountfordatadistribution agent’sabilitytoperformactivesensingbyprioritizingcritical
shifts between training and target environments [15], [16]. visual cues.
However, these adaptations remain limited to addressing data However, while deep learning and LLMs offer powerful
drifts and lack broader capabilities for behavioral changes. feature extraction, their static nature limits adaptability when
Our approach overcomes this limitation by integrating active deployed in uncertain or changing environments, which can
inference on top of a deep learning module, allowing the only be counteracted by massively scaling data or model size.
system to plan and adapt to the environment accordingly. Thus, to address this challenge, our approach integrates deep
learningforperceptionbutreliesonactiveinferenceasamuch This adaptive behavior enhances the camera’s utility as an
lighter, sample- and parameter-efficient higher-level module, intelligent surveillance IoT tool or for scene exploration and
enabling the agent to plan and dynamically adapt based on information foraging in robotic applications.
ongoing observations in real-time.
A. Object detection using YOLOv10
B. Planning
Active inference builds on the Free Energy principle, a We chose YOLOv10 [21] from the YOLO family for our
theoretical framework stating that intelligent agents minimize perceptionmoduleduetoitsstrongbalanceofdetectionaccu-
the discrepancy between their internal (generative) model of racy and computational efficiency. YOLO models are single-
the environment and incoming sensory data. By reducing this pass object detectors that predict object categories and loca-
discrepancy, or ”free energy”, the agent maintains an updated tions,makingthemidealforreal-timeapplications.YOLOv10
and accurate representation of its surroundings, allowing the consistently demonstrates state-of-the-art performance and re-
agent to make predictions about its environment and actively duced latency across various model scales (N/S/M/L/X). We
take actions to reduce uncertainty. employ the Nano variant, with 2.3 million parameters, as
Concretely, the agent’s generative model is a joint proba- it offers an efficient trade-off between accuracy, speed, and
bility distribution over states s and observations o. As infer- memory for efficient edge deployment.
ring hidden states s given some observations o is typically TodeploytheYOLOv10nnetworkasefficientlyaspossible,
intractable,anapproximateposteriorQ(s|o)isintroducedand we export it to ONNX [22]. ONNX has become a standard
optimized by minimizing the free energy F [10]: for neural network representation and exchange and is widely
supportedbyhardwarevendors’softwarestacks,i.e.,inference
engines. This positions ONNX as a strong candidate for
min F =D [Q(s|o)||P(s)]−E [logP(o|s)] (1)
KL Q(s|o) deployment space exploration and optimizations on a range
Q(s|o) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
complexity accuracy of embedded devices. Thus, we create an edge-deployment
workflowtofindthemostsuitabledeploymentforYOLOv10n.
Hence, the agent strives to give the most accurate predic-
tions P(o|s) while minimizing the complexity of the model The edge-deployment workflow contains several edge-
with respect to the prior P(s). To select actions or policies π oriented inference engines, namely ONNX-runtime [23],
for the future τ, an active inference agent will evaluate and TFlite [24], and TensorRT [25] that can input an ONNX
minimize the expected free energy G [10]: model and generate an optimized implementation for a given
target hardware platform. These frameworks apply several
G(π)=E (cid:2) D [Q(s |o ,π)||Q(s |π)] (cid:3) optimizationsacrossthenetwork’sgraph,e.g.,operatorfusion,
Q(oτ|π) KL τ τ τ quantization, on the software stack, e.g., algorithm optimiza-
(cid:124) (cid:123)(cid:122) (cid:125)
(negative)informationgain tion, and leverage parallel hardware acceleration, vectoriza-
(2)
−E (cid:2) logP(o |C) (cid:3) tion, and optimized memory scheduling. The process results
Q(oτ|π) τ
(cid:124) (cid:123)(cid:122) (cid:125) in a bespoke network description and a runtime that is ready
expectedutility for deployment on the hardware platform.
Theagentnowaveragesacrossexpectedfutureoutcomeso
τ
and balances the expected information gain (i.e., exploration)
B. Planning using Active Inference
with the expected utility (i.e., reward) encoded in prior pref-
erencesC.Boththeobservationsoandhiddenstatesscanbe To enable an efficient saccade agent with active inference
modeled as discrete variables with Categorical distributions, planning, we define a discrete action, observation, and hidden
whereas optimizing F and G can be done using tractable statespace.Tothisend,wedividethefullareathecameracan
update rules [11]. Therefore, we convert the outputs of the pan and tilt into a discrete grid of K ×L blocks, see Fig. 2.
deep learning perception module into a discrete observation Given a particular fixation point, the camera’s field of view
space and use active inference for the action selection of our will only span W ×H blocks, highlighted in blue. For each
agent. block, at each timestep, an observation o is a Categorical
w,h
distribution with three bins, i.e., the block can have no object
IV. SMARTEDGEAGENT
detected (0 - blue), an object detected (1 - red) or not visible
Our smart edge agent, the saccade agent, comprises two (2 - gray). The confidence of the bounding box outputs of the
modules, as introduced in the previous section, combining object detection neural network provides the probability of
deep learning perception capabilities with active inference an object being detected. As state space, we similarly have a
planning, as shown in Fig. 1. Specifically, we employ i) a statevariables perblock,whichisBernoulli,i.e.,objectnot
k,l
deep-learning object detection model, offering efficient object present(0)orpresent(1).Inaddition,wealsoequiptheagent
andhumandetectioncapabilitiesdirectlyattheedge,andii)an withaproprioceptivestates andobservation,i.e.,itobserves
p
active inference module that enables adaptive motion control the fixation point it is currently looking at. We specify the
(pan and tilt), allowing the agentic system to dynamically likelihood mapping A which predicts observation o given
w,h
adjustitsfieldofviewortrackdetectedentitiesautonomously. state s and fixation point s :
k,l p
Fig. 3. Applications: Our agentic system enables active sensing solutions
Fig. 2. Action space: We discretize the action space into K×L fixation
for edge robotics and surveillance IoT cameras. On the left, the Tapo IoT
points.Givenafixationpoint,thefieldofviewofthecameraspansW×H
camera [27] used for surveillance applications. On the right, the Locobot
blocks(inblue).Objectdetectionsaretranslatedintodiscretebins(inred).
robotWX250[28]forinformationgatheringandscenediscovery.

V. EXPERIMENTALRESULTS
0 if s =0,k → w,l→ h
 k,l p p
A. Experimental Set up
A = 1 if s =1,k → w,l→ h (3)
w,h,k,l,p k,l p p
2 otherwise Our experimental setup comprises the following compo-
nents, as shown in Fig. 3 :
where k → w means “block k maps to observation
p
1) Tapo Camera: We use an IoT Tapo camera equipped
w given the agent looks at fixation point p”. We currently
with pan and tilt capabilities, which serves both as the
also assume that objects don’t move considerably between
inputfortheobservations,i.e.,images/frames,forwarded
timestepsandusetheprevioustimestepposteriorasthecurrent
to the object detector, and the actuator for our saccade
prior, i.e., P(s ) = Q(s |o ), but we could expand this
t t−1 t−1
agent. The agent commands the camera to perform dy-
with dynamics modeling of the objects as future work.
namic adjustments in its field of view based on optimal
The saccade agent uses the object detections received to
policiesderivedbyminimizingfreeenergy,simulatinga
performinference,updatingitsbeliefsaboutthehiddenstates.
human-like saccadic motion to maintain focus on areas
Forexample:Detectinga“person”withhighconfidencewould
of interest.
increase the probability assigned to the hidden state “person
2) Locobot WX250: We also deploy our agent on the Lo-
present”correspondingtotheparticularblock.Theabsenceof
cobot WX250, a mobile robot platform for autonomous
detection, on the other hand, would decrease the probability
mobility. Equipped with a 6-DOF manipulator and
of that object being present.
pan/tilt camera, the Locobot enables active exploration
Aftereachobservation,theagentevaluatespossibleactions,
of the environment. This capability complements the
i.e.,thenextfixationpoints,basedontheirpredictedoutcomes.
Tapo camera’s pan and tilt actions with navigation. The
Actions are chosen to minimize expected free energy, which
robot’smobilityenhancestheagent’scapacityforactive
combines two key factors:
sensing in new environments, allowing it to reposition
1) Expected observations matching prior preferences:
itself and collect additional perspectives to refine per-
This essentially means choosing the most likely actions
ception in complex settings.
expected to lead to desired outcomes. For example,
3) NvidiaJetsonOrinNX:OursetupleveragestheNvidia
if the agent’s goal is to locate a specific object, we
JetsonOrinNX.Equippedwithan8-coreARMCortex-
set a preference C = 1, which favors actions that
w,h A78CPUanda1024-coreAmpereGPU,theJetsonOrin
increase the probability of detecting that object in the
NXrepresentsapowerfuledgeAIplatformdesignedfor
field of view. Similarly, if we only set C =1, with
cw,ch acceleratedmachinelearningtaskswithupto100TOPs
(c ,c )thecentercoordinatesofthecamera,thisyields
w h of processing capability and 16 GB of memory, running
a “tracking” agent that pans/tilts to keep the object of
at 25W.
interest in the center.
Apre-trainedYOLOv10nmodelontheCOCOdataset[29]
2) Epistemic value: The agent also aims to reduce un-
is deployed on the Nvidia Jetson Orin NX for real-time
certainty about the hidden states. Actions expected to
detection. The active inference module receives bounding
provide more informative observations about the envi-
boxes as observations and returns the optimal pan and tilt
ronment have higher epistemic value (i.e., information
actions for the IoT or robotic camera. Both perception and
gain in eq. 2). In our model, moving the field of view
planning modules employ the edge-deployment workflow to
to previously unobserved blocks will result in a high
findthemostsuitabledeploymentengine,yieldingthehighest
amount of information gain. Hence, in the absence of
performance.
objects of interest, the camera will pan and tilt to cover
the whole area with as few moves as possible.
B. Results
Weprovideaqualitativedemonstrationofoursaccadeagent
at [26]. Next, we summarize our deployment experimental results:
Fig.4. Perceptionmoduleperformance:Optimizationachievedbyexport- Fig.5. PerceptionmodulememoryprofileoftheYoloV10nwhenexecuted
ingtheYOLOv10nTorchmodelfromUltralyticstoONNXandcompilingit withdifferentdeploymentinferenceenginesontheNvidiaJetsonOrinNX’s
withdifferentdeploymentinferenceenginesontheNvidiaJetsonOrinNX’s CPUandGPU.
CPUandGPU.
one that consumes the most memory, with up to 1.2 GB. As
1) Perception: We optimized the YOLOv10n model, as we discussed earlier, TRT is designed explicitly for Nvidia
introduced in Section IV, by exporting the original Torch Jetson devices. As such, it accomplishes a very performant
model from the original Ultralitycs library to ONNX. Then, deployment with an optimized memory usage under 400 MB.
the model gets compiled with one of the various deployment Overall, the results demonstrate that the predominant con-
inferenceengines.Figure4depictstheaverageresultsof1000- tribution to overall latency stems from the neural network
inferenceruns,withonewarm-upsample,whiledeployingthe inference stage. In contrast, the pre-processing phase exhibits
differentimplementationsontheNvidiaJetson’sCPU,single- minimal computational overhead, and the post-processing
and multi-core, and GPU. step remains negligible — a characteristic intrinsic to the
Whenevaluatingthevariousinferenceenginesusingsingle- YOLOv10 architecture. Moreover, the comparative analysis
floating-point precision (FP32) operations on a single-core across inference engines and hardware configurations re-
CPU, Ultralitics comes out as the slowest inference, with veals distinct performance and memory usage behaviors. For
over 600 ms. When the model is exported to ONNX and instance, TFLite achieves favorable latency on single-core
compiled with ONNX-runtime (ORT) and TensorFlow Lite execution, although with the highest memory usage, while
(TFLite), the latency decreases considerably by 23% 35%, ORT offers an excellent performance-memory tradeoff. On
respectively.Weobtainhighergainswhenparallelizingacross the GPU side, TRT is the absolute winner with excellent
all available 8 CPU cores, achieving 427 ms when using latency-memory performance. These observations highlight
Ultralytics. The performance can be notably increased if we the importance of a flexible edge-deployment workflow that
deploy with ORT (intra-operator parallelization) and TFLite, systematically identifies the most suitable software–hardware
with a 3.75x and 4.5x speed-up over Ultralytics, respectively. pairing for a given operational context.
Finally,whenoffloadinginferencetotheGPU,weseeaboost 2) Planning: Next,wedeploytheactiveinferenceplanning
in performance for all GPU-available frameworks by leverag- module,whichonlycontains1.34Kparameters,ontheNvidia
ing the native parallel compute units of the device, achieving Jetson NX. The original active inference model was designed
an inference in under 45 ms. When using TensorRT (TRT), with the Pymdp library [30], which contains a JAX backend.
designed explicitly for Nvidia Jetson devices, we accomplish We then follow a similar workflow as with YOLOv10n and
anoptimizeddeploymentthroughreducednumericalprecision exporttheactiveinferencemodeltoONNXanddeployitwith
(FP16), with as little as 22 ms, i.e., 45 FPS. several inference engines.
Figure 5 shows the memory profile of the YOLOv10n Figure 6 depicts the latency of the active inference model
deployments with the various inference engines. It can be when planning the following position to look at. The pre-
observed that while TFLite provides the fastest deployment processingstepaccountsfordecodingandmappingtheobject
on the single-CPU setup, it also consumes the most memory, detection bounding boxes to the generative model. The infer-
with over 50% more than Ultralytics. On the other hand, ence part involves the infer states method, which updates the
ORT provides a good balance between latency and memory agent’s beliefs about the hidden states given the observations,
utilization, being slightly slower than TFLite, but achieving andtheinfer policiesmethod,whichdefinesthesetofpolicies
inference with as little as 168 MB. This pattern is also fromwhichtheagentwillpickthebestaction.Whendeployed
repeated in the multi-CPU deployment setup. On the GPU onasingleCPU,theplanningprocessexecutesfast,achieving
side. Ultralytics, being a training-oriented framework, is the 6 ms using JAX and 4 ms (250 FPS) when compiled with
Fig. 6. Planning module performance: Performance optimization by Fig. 7. Planning module memory profile of the active inference model
exporting the active inference model (saccade agent) from JAX to ONNX when executed with different deployment inference engines on the Nvidia
and compiling it with different deployment inference engines on the Nvidia JetsonOrinNXCPUandGPU.
JetsonOrinNX’sCPUandGPU.
#Params(M) Memory(GB) Perception Decision-making
TFLite. The saccade agent does not benefit from parallel pro- Rate(Hz) Rate(Hz)
cessing,showingsimilarorworseperformancewhendeployed A 2.3 947 45 250
B 2.3 533 45 108
on multiple CPUs and considerably worse when offloaded
C 2.3 304 9 108
to the GPU. We argue this is due to the small size of the
TABLEI
agent,payingahighpenaltyforthreadsynchronization,which
SYSTEMPARAMETERS:SACCADEAGENTDEPLOYEDONTHENVIDIA
dominates over the benefit from parallel computation. JETSONORINNX
Figure7illustratesatrendconsistentwiththatobservedfor
theYOLOv10nmodel,revealingatrade-offbetweeninference C. Discussion
performance and memory consumption. For example, while
These results highlight the effectiveness of combining deep
TFLite delivers the fastest execution, it may consume up
learning for perception with active inference for planning in
to four times more memory than ORT. This behavior may
edge-based intelligent agents. With only 1.34K parameters,
be related to internal optimization mechanisms in TFLite,
the active inference module achieves real-time planning while
potentiallyinvolvingintermediatedatacachingorbufferreuse
maintaining minimal computational and memory demands.
strategies to reduce computation time.
In contrast to large-scale foundation models that rely on
Overall, these results highlight the lightweight deployment
billions of parameters to achieve general-purpose intelligence,
of our active inference model, achieving real-time planning
our domain-oriented approach demonstrates how compact,
and adaptation to the environment for IoT and robotics sac-
task-specific active inference models can complement deep
cading applications.
learning perception. The adaptability of our edge-deployment
3) System: Integratingtheperceptionandplanningmodules
workflow illustrates deployment interoperability, facilitating
produces a highly efficient saccade agent capable of real-time
the exploration of diverse configurations to achieve a suitable
operation with only 2.3 million parameters. Table 1.1 sum-
balance between latency, accuracy, and resource efficiency.
marizes key configurations, illustrating the trade-offs between
latency and memory consumption. For example, in config-
VI. CONCLUSIONSANDFUTUREWORK
uration A, the perception module optimized with TensorRT-
FP16 achieves a throughput of 45 FPS, enabling rapid feature Thisworkhasintroducedasmartedgeagentcomposedofa
extraction. In parallel, the active inference module using deeplearning-basedperceptionmoduleandanactiveinference
TFLite can perform planning and decision-making at up to planning module for active sensing. The system demonstrates
250 FPS, with a total memory footprint under 1 GB. The thefeasibilityofadaptable,on-devicesurveillanceandrobotic
decision-making rate can be reduced to 108 FPS using ORT, solutions and their potential to handle real-world challenges
loweringmemoryusageto533MB,asshowninconfiguration effectively. Our study highlights the potential of active infer-
B. Finally, configuration C employs the ORT (intra-operator ence in edge-based systems. Active inference offers a robust
parallelization) for perception, decreasing the perception rate framework that accounts for the environment’s underlying
to 9 FPS while providing a compact memory footprint of dynamics and inherent uncertainty. It allows the agent to
304 MB, including all required libraries and runtimes. This actively reduce ambiguity or explore the environment for
configuration effectively balances perception and decision- new information. This approach establishes a foundation for
making’s latency and memory, enabling dynamic adaptation efficient edge agents for adaptive, resource-efficient decision-
and efficient operation on resource-constrained edge devices. making in IoT and edge computing applications.
In future work, we aim to compare the proposed system [20] Z.-Q. Zhao, P. Zheng, S.-T. Xu, and X. Wu, “Object detection with
against other state-of-the-art works, showing a quantitative deep learning: A review,” IEEE Transactions on Neural Networks and
LearningSystems,vol.30,no.11,pp.3212–3232,2019.
functional comparison on popular challenges, such as the
[21] A. Wang, H. Chen, L. Liu, K. Chen, Z. Lin, J. Han, and G. Ding,
Habitat Navigation Challenge. Finally, we envision extending “Yolov10: Real-time end-to-end object detection,” arXiv preprint
this concept to other applications, which could unlock a new arXiv:2405.14458,2024.
[22] J.Bai,F.Lu,K.Zhangetal.,“Onnx:Openneuralnetworkexchange,”
generation of adaptive edge devices capable of context-driven
https://github.com/onnx/onnx,2019.
interactions in complex environments. [23] O. R. developers, “Onnx runtime,” https://onnxruntime.ai/, 2021, ver-
sion:x.y.z.
REFERENCES [24] R.David,J.Duke,A.Jain,V.JanapaReddi,N.Jeffries,J.Li,N.Kreeger,
I.Nappier,M.Natraj,T.Wangetal.,“Tensorflowlitemicro:Embedded
[1] S.Grossberg,K.Roberts,M.Aguilar,andD.Bullock,“Aneuralmodel machinelearningfortinymlsystems,”ProceedingsofMachineLearning
of multimodal adaptive saccadic eye movement control by superior andSystems,vol.3,pp.800–811,2021.
colliculus,” Journal of Neuroscience, vol. 17, no. 24, pp. 9706–9725, [25] E.Jeong,J.Kim,andS.Ha,“Tensorrt-basedframeworkandoptimiza-
1997. tion methodology for deep learning inference on jetson boards,” ACM
[2] M. Ferro, D. Ognibene, G. Pezzulo, and V. Pirrelli, “Reading as TransactionsonEmbeddedComputingSystems(TECS),vol.21,no.5,
active sensing: a computational model of gaze planning during word pp.1–26,2022.
recognition,”FrontiersinNeurorobotics,vol.4,p.1262,2010. [26] “Saccadedemonstration,”https://drive.google.com/drive/folders/1VCN8MAMnsdbj-
[3] T.Parr,N.Sajid,L.DaCosta,M.B.Mirza,andK.J.Friston,“Gener- xY5qudjn7vLFlB5mSfi?usp=sharing,accessed:2025-10-16.
ative models for active vision,” Frontiers in Neurorobotics, vol. 15, p. [27] “Tapo surveillance camera,” https://www.tapo.com/us/product/smart-
651432,2021. camera/tapo-c210/,accessed:2024-11-12.
[4] M. B. Mirza, R. A. Adams, C. D. Mathys, and K. J. Friston, “Scene [28] “Locobotrobot,”http://www.locobot.org/,accessed:2024-11-12.
construction,visualforaging,andactiveinference,”Frontiersincompu- [29] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
tationalneuroscience,vol.10,p.56,2016. P. Dolla´r, and C. L. Zitnick, “Microsoft coco: Common objects in
[5] A. Stokholm, J. Buus-Hinkler, T. Wulf, A. Korosov, R. Saldo, L. T. context,”inComputerVision–ECCV2014:13thEuropeanConference,
Pedersen,D.Arthurs,I.Dragan,I.Modica,J.Pedroetal.,“Theautoice Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13.
challenge,”TheCryosphere,vol.18,no.8,pp.3471–3494,2024. Springer,2014,pp.740–755.
[6] S. McClanahan, “Object recognition and detection: Potential implica- [30] C.Heins,B.Millidge,D.Demekas,B.Klein,K.Friston,I.Couzin,and
tions from vision science for wilderness searching,” J Search Rescue, A.Tschantz,“pymdp:Apythonlibraryforactiveinferenceindiscrete
vol.5,no.1,pp.1–17,2021. statespaces,”arXivpreprintarXiv:2201.03904,2022.
[7] M.Kashef,A.Visvizi,andO.Troisi,“Smartcityasasmartservicesys-
tem:Human-computerinteractionandsmartcitysurveillancesystems,”
ComputersinHumanBehavior,vol.124,p.106923,2021.
[8] L.DaCosta,P.Lanillos,N.Sajid,K.Friston,andS.Khan,“HowActive
InferenceCouldHelpRevolutioniseRobotics,”Entropy,vol.24,no.3,
p.361,Mar.2022.
[9] G.I.Parisi,R.Kemker,J.L.Part,C.Kanan,andS.Wermter,“Continual
lifelonglearningwithneuralnetworks:Areview,”Neuralnetworks,vol.
113,pp.54–71,2019.
[10] T.Parr,G.Pezzulo,andK.J.Friston,ActiveInference:TheFreeEnergy
Principle in Mind, Brain, and Behavior. The MIT Press, 03 2022.
[Online].Available:https://doi.org/10.7551/mitpress/12441.001.0001
[11] K.Friston,T.FitzGerald,F.Rigoli,P.Schwartenbeck,G.Pezzuloetal.,
“Activeinferenceandlearning,”Neuroscience&BiobehavioralReviews,
vol.68,pp.862–879,2016.
[12] N. Krishnachaithanya, G. Singh, S. Sharma, R. Dinesh, S. R. Sihag,
K.Solanki,A.Agarwal,M.Rana,andU.Makkar,“Peoplecountingin
public spaces using deep learning-based object detection and tracking
techniques,”in2023InternationalConferenceonComputationalIntel-
ligenceandSustainableEngineeringSolutions(CISES). IEEE,2023,
pp.784–788.
[13] M. D. Prado, J. Su, R. Saeed, L. Keller, N. Vallez, A. Anderson,
D. Gregg, L. Benini, T. Llewellynn, N. Ouerhani et al., “Bonseyes ai
pipeline—bringingaitoyou:End-to-endintegrationofdata,algorithms,
anddeploymenttools,”ACMTransactionsonInternetofThings,vol.1,
no.4,pp.1–25,2020.
[14] J. Redmon, “You only look once: Unified, real-time object detection,”
inProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,2016.
[15] Q.Zhang,R.Han,C.H.Liu,G.Wang,andL.Y.Chen,“Elasticdnn:On-
deviceneuralnetworkremodelingforadaptingevolvingvisiondomains
atedge,”IEEETransactionsonComputers,2024.
[16] C. Cioflan, L. Cavigelli, M. Rusci, M. de Prado, and L. Benini, “On-
devicedomainlearningforkeywordspottingonlow-powerextremeedge
embeddedsystems,”arXivpreprintarXiv:2403.10549,2024.
[17] E. Sommerlade and I. Reid, “Information-theoretic active scene ex-
ploration,” in 2008 IEEE Conference on Computer Vision and Pattern
Recognition. IEEE,2008,pp.1–7.
[18] E. Sommerlade and I. Reid, “Probabilistic surveillance with multiple
active cameras,” in 2010 IEEE International Conference on Robotics
andAutomation. IEEE,2010,pp.440–445.
[19] Y.LeCun,Y.Bengio,andG.Hinton,“Deeplearning,”nature,vol.521,
no.7553,pp.436–444,2015.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Towards smart and adaptive agents for active sensing on edge devices"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
