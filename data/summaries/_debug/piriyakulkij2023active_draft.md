### OverviewThis paper investigates active preference inference using language models and probabilistic reasoning. The authors propose a novel algorithm to quickly infer user preferences by asking informative questions, addressing the inefficiency of existing methods. The core idea is to minimize the number of user interactions while maximizing the amount of information gained. The research demonstrates improved performance compared to baseline models in a simplified web shopping setting.### MethodologyThe authors introduce an inference-time algorithm that helps LLMs quickly infer preferences by using more informative questions. The algorithm utilizes a probabilistic model, where conditional distributions are defined by prompting an LLM. The algorithm samples a finite set of questions from the LLM and chooses the one that maximizes expected entropy and expected model change. Specifically, the model is defined as p(x,q,a) = p(a|x,q)p(x)p(q), where p(x|q,a)‚àùp(a|x,q)p(x). The algorithm uses a binary reward function, assuming the target product is retrieved with a reward of1, and0 otherwise. The key objective is to minimize the expected entropy of the model given the questions and answers.### ResultsIn a simplified web shopping setting with150 tasks, the authors demonstrate that an LLM equipped with their entropy reduction algorithm outperforms both vanilla instruction-tuned LLM and ReAct LLM while using fewer questions.The algorithm significantly reduces the number of questions needed to infer the target product.Specifically, after three questions, the entropy reduction LLM was able to determine the target product, while the vanilla LLM required five questions. The results highlight the effectiveness of the proposed algorithm in improving the efficiency of active preference inference.### Claims and Supporting Evidence*"Toenablethisabilityforinstruction-tunedlargelanguagemodels(LLMs), one maypromptthemtoaskusersquestionstoinfertheirpreferences,transformingthelanguagemodelsintomorerobust,interactivesystems." (The authors state: "To enable this ability for instruction-tuned large language models (LLMs), one may prompt them to ask users questions to infer their preferences, transforming the language models into more robust, interactive systems.")*"Themaindownsideofthesemethods,however,isthattheyrequireexpensivetask-specificdatacollectionfortraining." (The authors note: "The main downside of these methods, however, is that they require expensive task-specific data collection for training.")*"ThealgorithmusesaprobabilisticmodelwhoconditionaldistributionsaredefinedbypromptinganLLM,andreturnsquestionsthatoptimizeexpectedentropyandexpectedmodelchange." (They argue: "The algorithm uses a probabilistic model where conditional distributions are defined by prompting an LLM, and returns questions that optimize expected entropy and expected model change.")*"Wefindthatourapproachoutperformsbothbaselineswhileusingfewerquestions." (The authors state: "We find that our approach outperforms both baselines while using fewer questions.")*"OurresultsshouldbeseenawayofaugmentingLLMswithinference-timeprobabilisticreasoning." (The authors argue: "Our results should be seen as a way of augmenting LLMs with inference-time probabilistic reasoning.")*"ThealgorithmusesaprobabilisticmodelwhoconditionaldistributionsaredefinedbypromptinganLLM,andreturnsquestionsthatoptimizeexpectedentropyandexpectedmodelchange." (They argue: "The algorithm uses a probabilistic model where conditional distributions are defined by prompting an LLM, and returns questions that optimize expected entropy and expected model change.")*"ThealgorithmusesaprobabilisticmodelwhoconditionaldistributionsaredefinedbypromptinganLLM,andreturnsquestionsthatoptimizeexpectedentropyandexpectedmodelchange." (They argue: "The algorithm uses a probabilistic model where conditional distributions are defined by prompting an LLM, and returns questions that optimize expected entropy and expected model change.")### Key FindingsThe study demonstrates that an LLM equipped with the entropy reduction algorithm significantly outperforms baseline models in active preference inference, reducing the number of questions needed while maintaining high performance. The algorithm's effectiveness stems from its ability to ask informative questions, minimizing user interaction and maximizing information gain. The quantitative results highlight the potential of this approach for building more efficient and personalized AI systems.