=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Discovering Novel Actions from Open World Egocentric Videos with Object-Grounded Visual Commonsense Reasoning
Citation Key: kundu2023discovering
Authors: Sanjoy Kundu, Shubham Trehan, Sathyanarayanan N. Aakur

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2023

Key Terms: novel, world, discovering, grounded, object, symbolic, reasoning, actions, egocentric, open

=== FULL PAPER TEXT ===

Discovering Novel Actions from Open World
Egocentric Videos with Object-Grounded Visual
Commonsense Reasoning
Sanjoy Kundu1, Shubham Trehan1, and Sathyanarayanan N. Aakur1
CSSE Department, Auburn University,
Auburn, AL, USA 36849
{szk0266,szt0113,san0028}@auburn.edu
Abstract. Learningtoinferlabelsinanopenworld,i.e.,inanenviron-
mentwherethetarget“labels” areunknown,isanimportantcharacteris-
ticforachievingautonomy.Foundationmodels,pre-trainedonenormous
amounts of data, have shown remarkable generalization skills through
prompting, particularly in zero-shot inference. However, their perfor-
mance is restricted to the correctness of the target label’s search space,
i.e.,candidatelabelsprovidedintheprompt.Thistargetsearchspacecan
be unknown or exceptionally large in an open world, severely restrict-
ing their performance. To tackle this challenging problem, we propose
a two-step, neuro-symbolic framework called ALGO - Action Learning
with Grounded Object recognition that uses symbolic knowledge stored
inlarge-scaleknowledgebasestoinferactivitiesinegocentricvideoswith
limited supervision. First, we propose a neuro-symbolic prompting ap-
proach that uses object-centric vision-language models as a noisy oracle
to ground objects in the video through evidence-based reasoning. Sec-
ond, driven by prior commonsense knowledge, we discover plausible ac-
tivitiesthroughanenergy-basedsymbolicpatterntheoryframeworkand
learntogroundknowledge-basedaction(verb)conceptsinthevideo.Ex-
tensiveexperimentsonfourpubliclyavailabledatasets(EPIC-Kitchens,
GTEAGaze,GTEAGazePlus,andCharades-Ego)demonstrateitsper-
formanceonopen-worldactivityinference.WealsoshowthatALGOcan
beextendedtozero-shotinferenceanddemonstrateitscompetitiveper-
formance on the Charades-Ego dataset.
Keywords: Open-world Learning · Egocentric Activity Understanding
· Vision-Language Foundation Models
1 Introduction
Humans display a remarkable ability to recognize unseen concepts (actions, ob-
jects, etc.) by associating known concepts gained through prior experience and
reasoning over their attributes. Key to this ability is the notion of “grounded”
reasoning, where abstract concepts can be mapped to the perceived sensory sig-
nals to provide evidence to confirm or reject hypotheses. In this work, we aim
4202
yaM
3
]VC.sc[
2v20661.5032:viXra
2 S. Kundu et al.
Gaze-driven ROI Selection Concept Search Space
Noisy Grounding Model
(CLIP)
stcejbO biscuit, bowl, bread, broccoli, carrot, cereal, cheese, chocolate, coke,
cup, Fanta, fork, fork box, honey, jam, ketchup, knife, pepper, burner, container, drawer, egg, freezer, fridge, microwave, milk, ...
snoitcA
Evidence-Based
Object Grounding
close, cut, compress, crack, open, pour, put, read, take, turn, eat, cook,
spread, slice, measure, wash, dip, store, roast, wipe, clean, ...
Commonsense
Prior-driven Object-driven Concept Knowledgebase
Prompting Contextualization
Action Action-Object
Grounded Object Search Space Search Space Affinity Prior
CLIP Feature Vegetable Energy-based Pattern Update Action Visual Semantic
IsA Theory Inference Prior Action Grounding
IsA IsA
Jalapeno Pepper Green Pepper
CLIP Feature Vegetable
IsA CLIP Feature Appliance Jalapeno IsA IsA Green Pepper Temporal IsA Smoothing IsA AtLocation
Oven Microwave Counter Video Feature Object-Driven Activity
Top-K Activity Interpretations Discovery
oTdetaleR
CLIP Feature Counter
Pepper IsA AtLocatioIsnA Appl / iance IsA IsA Oven Micr H o a w sPr a op v er e ty HasProperty RelatedTo UsedFor
Take Food Video Feature Open Door
Fig.1: Overall architecture of the proposed approach (ALGO) is illustrated here.
Using a two-step process, we first ground the objects within a gaze-driven ROI using
CLIP [47] as a noisy oracle before reasoning over the plausible activities performed in
thevideo.Theinferredactivityandaction(verb)aregroundedinpriorknowledgeand
visual features to refine the activity interpretations.
tocreateacomputationalframeworkthattacklesopen-worldegocentricactivity
understanding. We define an activity as a complex structure whose semantics
areexpressedbyacombinationofactions(verbs)andobjects(nouns).Torecog-
nize an activity, one must be cognizant of the object label, action label, and the
possibility of any combination since not all actions are plausible for an object.
Supervised learning approaches [20,39,51,58] have been the dominant approach
to activity understanding but are trained in a “closed” world, where there is an
implicit assumption about the target labels. The videos during inference will
always belong to the label space seen during training. Zero-shot learning ap-
proaches [6,36,64,65] relax this assumption by considering disjoint “seen” and
“unseen” labelspaceswherealllabelsarenotnecessarilyrepresentedinthetrain-
ing data. This setup is a known world, where the target labels are pre-defined
andawareduringtraining.Inthiswork,wedefineanopen worldtobeonewhere
thetargetlabelsareunknownduringbothtrainingandinference.Thegoalisto
recognize elementary concepts and infer the activity.
Foundation models [9], pre-trained on large amounts of data, have shown
tremendous performance on different problems such as question answering [18],
zero-shotobjectrecognition[47],andactionrecognition[36].Self-supervisedpre-
training [20,65] has helped improve their generalization. However, their ability
to perform open-world inference is constrained by two factors. First, the search
space (i.e., target label candidates) must be well-defined since their output is
constrained to what is presented to them (or “prompted”), which requires prior
Abbreviated paper title 3
knowledge about the environment. Second, their performance is dependent on
thespanoftheirpre-trainingdata.Modelstrainedonthird-personviewsmaynot
generalizetoegocentricvideosduetothelimitedcapabilitytoground semantics
in visual data and reason over object affordances. Learning these associations
duringpre-trainingischallengingsinceitrequiresdatafromeverypossiblecom-
bination of concepts. We propose to tackle this problem using a neuro-symbolic
framework that leverages advances in multimodal foundation models to ground
concepts from symbolic knowledge bases, such as ConceptNet [54], in visual
data.TheoverallapproachisshowninFigure1.Usingtheenergy-basedpattern
theory formalism [2,4,25] to represent symbolic knowledge, we ground objects
(nouns) using CLIP [47] as a noisy oracle. Driven by prior knowledge, novel ac-
tivities (verb+noun) are inferred, and the associated action (verb) is grounded
in the video to learn visual-semantic associations for novel, unseen actions.
The contributions of this work are three-fold: (i) We present a neuro-
symbolic framework to leverage compositional properties of objects to prompt
CLIP for evidence-based grounding. (ii) We propose object-driven activity dis-
coveryasamechanismtoreasonoverpriorknowledgeandprovideaction-object
affinities to constrain the search space. (iii) We demonstrate that the inferred
activitiescanbeusedtogroundunseenactions(verbs)fromsymbolicknowledge
inegocentricvideos,whichcangeneralizetounseenandunknownactionspaces.
2 Related Works
Egocentric video analysis has been extensively explored in computer vision
literature, having applications in virtual reality [27] and human-machine inter-
action. Various tasks have been proposed, such as question-answering [22], sum-
marization [38], gaze prediction [3,23,35], and action recognition [33], among
others. Success has been driven by the development of large-scale datasets such
as Ego-4D [24], Charade-Ego [51], GTEA Gaze [23], GTEA Gaze Plus [35], and
EPIC-Kitchens[16].Inthecontextofegocentricactivityrecognition,whichisthe
focus of this work, supervised learning has been the predominant approach. Re-
searchers have explored various techniques, such as modeling spatial-temporal
dynamics [56], using appearance and motion cues for recognition [39], hand-
object interaction [59,66], and time series modeling of motion information [50],
to name a few. Some studies have addressed the data-intensive nature by ex-
ploring zero-shot learning [51,64]. KGL [4] is one of the first works to address
the problem of open-world understanding. They represent knowledge ele-
ments derived from ConceptNet [54], using pattern theory [2,25,53]. However,
their method relies on an object detector to ground objects in a source domain
before mapping concepts to the target space using ConceptNet-based seman-
tic correspondences. This approach has limitations: (i) false alarms may occur
when the initial object detector fails to detect the object of interest, leading to
the use of the closest object to the gaze, and (ii) reliance on ConceptNet for
correspondences from the source domain to the target domain, resulting in ob-
jects being disregarded if corresponding probabilities are zero. Other efforts in
4 S. Kundu et al.
open-world learning have primarily focused on object-centric tasks, such as
open-world object detection [19,21,26], which do not address the combinatorial
problems inherent in open-world activity recognition.
Vision-language modelinghasgainedsignificantattentioninthecommu-
nity,drivenbythesuccessoftransformermodels[57]innaturallanguageprocess-
ing,suchasBERT[18],RoBERTa[37],OpenAI’sGPTseries[11,12,48,49],and
ELECTRA [14]. The development of object-centric foundation models has en-
abledimpressivecapabilitiesinzero-shotobjectrecognitioninimages,asdemon-
strated by CLIP [47], DeCLIP [34], and ALIGN [28]. These approaches rely on
large amounts of image-text pairs, often in the order of billions, to learn visual-
semantic representations using various forms of contrastive learning [13,31]. Re-
cent works, such as EGO-VLP [36], Hier-VL [6], LAVILLA [65], and CoCa [62]
haveexpandedthescopeofmultimodalfoundationmodelstoincludeegocentric
videos and have achieved impressive performance in zero-shot generalization.
However, these approaches require substantial amounts of curated pre-training
datatolearnsemanticassociationsamongconceptsforegocentricactivityrecog-
nition.Neuro-symbolic models[4,29,45,60]showpromiseinreducingthein-
creasing dependency on data. Our approach extends the idea of neuro-symbolic
reasoning to address egocentric, open-world activity recognition.
3 Proposed Framework: ALGO
Problem Formulation.Weaddressthetaskofrecognizingunknownactivities
in egocentric videos within an open-world setting. Our objective is to develop
a framework that can learn to identify elementary concepts, establish semantic
associations, and systematically explore, evaluate, and reject combinations to
arrive at an interpretation that best describes the observed activity class. In
this context, we define the target classes as activities, which are composed of
elementaryconceptssuchasactions(verbs)andobjects(nouns).Theseactivities
areformedbycombiningconceptsfromtwodistinctsets:anobjectsearchspace
(G ) and an action search space (G ). These sets define the pool of available
obj act
elementary concepts (objects and actions) that can be used to form an activity
(referred to as the “target label”).
Overview. We propose ALGO (Action Learning with Grounded Object
recognition), illustrated in Figure 1, to tackle the problem of discovering novel
actions in an open world. Given a search space of elementary concepts, we first
hypothesize the presence of plausible objects through evidence-based object
grounding (Section 3.1) by exploring prior knowledge from a symbolic knowl-
edge base. A noisy grounding model provides visual grounding to generate a
grounded object search space. We then use an energy-based inference mecha-
nism(Section3.2)todiscovertheplausibleactionsthatcanbeperformedonthe
ground object space, driven by prior knowledge from symbolic knowledge bases,
to recognize unseen and unknown activities (action-object combinations) with-
out supervision. A visual-semantic action grounding mechanism (Section 3.3)
then provides feedback to ground semantic concepts with video-based evidence
Abbreviated paper title 5
for discovering composite activities without explicit supervision. Although our
framework is flexible to work with any noisy grounding model and knowledge
base, we use CLIP [47] and ConceptNet [54], respectively.
Knowledge Representation. We use Grenander’s pattern theory formal-
ism[25]torepresenttheknowledgeelementsandbuildacontextualizedactivity
interpretationthatintegratesneuralandsymbolicelementsinaunified,energy-
basedrepresentation.Patterntheoryprovidesaflexibleframeworktohelpreason
over variables with varying underlying dependency structures by representing
them as compositions of simpler patterns. These structures, called configura-
tions, are composed of atomic elements called generators ({g ,g ,...g }∈G ),
1 2 i s
whichconnectthroughlocalconnectionscalledbonds ({β ,β ,...β }∈g ).The
1 2 i i
collection of all generators is called the generator space (G ), with each gener-
s
ator possessing an arbitrary set of bonds, defined by its arity. Bonds between
generators are constrained through local and global regularities, as defined by
anoverarchinggraphstructure.Aprobabilitystructureovertherepresentations
captures the diversity of patterns. We refer the reader to Aakur et al. [2] and de
Souza et al. [53] for a deeper exploration of pattern theory.
3.1 Evidence-based Object Grounding with Prior-driven Prompting
The first step in our framework is to assess the plausibility of each object con-
cept (represented as generators {go,go,...go} ∈ G ) by grounding them in
1 2 i obj
the input video V . We define grounding as gathering evidence from the input
i
data to support a concept’s presence (or absence) in the final interpretation.
While object-centric vision-language foundation models such as CLIP [47] have
shown impressive abilities in zero-shot object recognition in images, egocentric
videosprovideadditionalchallengessuchascameramotion,lensdistortion,and
out-of-distribution object labels. Follow-up work [41] has focused on addressing
them to a certain extent by probing CLIP for explainable object classification.
However, they do not consider compositional properties of objects and alterna-
tive labels for verifying their presence in the video. To address this issue, we
propose a neuro-symbolic evidence-based object grounding mechanism to com-
pute the likelihood of an object in a given frame. For each object generator (go)
i
in the search space (G ), we first compute a set of compositional ungrounded
obj
generatorsbyconstructinganego-graphofeachobjectlabel(E )fromConcept-
go
i
Net [54] and limiting edges to those that express compositional properties such
as IsA, UsedFor, HasProperty and SynonymOf. Given this set of ungrounded
generators ({g¯o}∀go ∈ G ), we then prompt CLIP to provide likelihoods for
i i obj
eachungroundedgeneratorp(g¯o|I )tocomputetheevidence-based likelihoodfor
i t
each grounded object generator go as defined by the probability
i
(cid:13) (cid:13)2
(cid:13) (cid:13)
p(go i |g¯ i o,I t ,K CS )=p(go i |I t )∗ (cid:13) (cid:13) (cid:13) (cid:88) p(g i o,g¯ i o|E g i o )∗p(g¯ i o)|I t ) (cid:13) (cid:13) (cid:13) (1)
(cid:13)∀g¯o (cid:13)
i
where p(go,g¯o|E ) is the edge weight from the edge graph E (sampled from
i i g i o g i o
aknowledgegraphK )thatactsasapriorforeachungroundedevidencegen-
CS
6 S. Kundu et al.
erator g¯o and p(g¯o|I ) is the likelihood from CLIP for its presence in each frame
i i t
I . Hence the probability of the presence of a grounded object generator is de-
t
termined by (i) its image-based likelihood, (ii) the image-based likelihood of its
evidence generators, and (iii) support from prior knowledge for the presence of
each evidence generator. Hence, we ground the object generators in each video
frame by constructing and evaluating the evidence to support each grounding
assertion and provide an interpretable interface to video object grounding. Em-
pirically, in Section 4.2, we see that this evidence-based grounding outperforms
näive CLIP-based grounding. To navigate clutter and focus only on the object
involved in the activity (i.e., the packaging problem [40]), we use human gaze
to select a 200×200 region centered around the gaze position (from the human
user if available, else we approximate it with center bias [35]).
3.2 Object-driven Activity Discovery
The second step in our approach is to discover plausible activities performed
in the given video. We take an object affordance-based approach to activity
inference, constraining the activity label (verb+noun) to those that conform to
affordancesdefinedinpriorknowledge.Wefirstconstructan“action-objectaffin-
ity” functionthatprovidesaprior probabilityforthevalidityofanactivity.The
probability of each action-object combination is computed by taking a weighted
sumoftheedgeweights(directandindirect)alongeachpaththatconnectsthem
in ConceptNet. An exponential decay function is applied to each term to avoid
generating excessively long paths that can introduce noise into the reasoning
process.Finally,wefilteroutpathsthatdonot containcompositionalassertions
(UsedFor, HasProperty, IsA)sincegenericassertions(suchasRelatedTo)do
notexplicitlycaptureobjectaffordances.Theprobabilityofanactivity(defined
by an action generator ga and a grounded object generator go) is given by
i j
(cid:88)
p(ga,go|K )=argmax w ∗K (g¯ ,g¯ ) (2)
i j CS k CS m n
∀E∈KCS(g¯m,g¯n)∈E
where E is the collection of all paths between ga and go in a commonsense
i j
knowledgegraphK ,w isaweightdrawnfromanexponentialdecayfunction
CS k
based on the distance of the node go from ga. After filtering for compositional
j i
properties,thepathwiththemaximumweightischosenwiththeoptimalaction-
object affinity. The process is repeated for all activities in the search space.
Energy-based Activity Inference. To reason over the different activity
combinations, we assign an energy term to each activity label, represented as a
configuration. These are complex structures composed of individual generators
thatcombinethroughbondsdictatedbytheiraffinityfunctions.Inourcase,each
activityinterpretationisaconfigurationcomposedofagroundedobjectgenera-
tor(go),itsassociatedungroundedevidencegenerators(g¯o),anactiongenerator
i j
(ga) and ungrounded generators from their affinity function, connected via an
k
underlying graph structure. This graph structure will vary for each configura-
tiondependingonthepresenceofaffinity-basedbondsderivedfromConceptNet.
Abbreviated paper title 7
Hence, the energy of a configuration c is given by
i
E(c)=ϕ(p(go|g¯o,I ,K ))+ϕ(p(ga,go|K ))+ϕ(p(ga|I )) (3)
i j t CS k i CS k t
where the first term provides the energy of grounded object generators (from
Equation 1), the second term provides the energy from the affordance-based
affinity between the action and object generators (from Equation 2), and the
third term is the likelihood of an action generator. The probability of a configu-
ration c is given by p(c)∝exp(−E(c)). Hence, the lower the energy, the higher
its probability. We initially set ϕ(p(ga|I )) = 1 to reason over all possible ac-
k t
tions for each object and later update this using a posterior refinement process
(Section3.3).Hence,activityinferencebecomesanoptimizationoverEquation3
to find the configuration (or activity interpretation) with the least energy. For
tractablecomputation,weusetheMCMC-basedsimulatedannealingmechanism
proposedinKGL[4]toavoidanexpensivebrute-forcesearchoverallverb-noun
combinations. If action priors are available from video-centric foundation mod-
els [36,65], ϕ(p(ga|I )) can be initialized by prompting it with plausible action
k t
labels.Empirically,inSection4,wecanshowthatleveragevision-languagefoun-
dation models, if available, to significantly improve the performance.
3.3 Visual-Semantic Action Grounding
Thethirdstepinourframeworkistheideaofvisual-semanticactiongrounding,
where we aim to learn to ground the inferred actions (verbs) from the overall
activity interpretation. While CLIP provides a general purpose, if noisy, object
groundingmethod,acomparableapproachforactionsdoesnotexist.Hence,we
learnanactiongroundingmodelbybootstrappingasimplefunction(ψ(ga,f ))
i V
tomapclip-levelvisualfeaturestothesemanticembeddingspaceassociatedwith
ConceptNet, called ConceptNet Numberbatch [54]. The mapping function is a
simple linear projection to go from the symbolic generator space (ga ∈G ) to
i act
thesemanticspace(fa),whichisa300-dimension(R1×300)vectorrepresentation
i
explicitly trained to capture concept-level attributes captured in ConceptNet.
Whiletherecanbemanysophisticatedmechanisms[6,36],includingcontrastive
loss-based training, we use the mean squared error (MSE) loss as the objective
function to train the mapping function since our goal is to provide a mechanism
togroundabstractconceptsfromtheknowledge-baseinthevideodata.Weleave
the exploration of more sophisticated grounding mechanisms to future work.
Temporal Smoothing Sincewepredictframe-levelactivityinterpretations
to account for gaze transitions, we first perform temporal smoothing to label
the entire video clip before training the mapping function ψ(ga,f ) to reduce
i V
noise in the learning process. For each frame in the video clip, we take the
five most common actions predicted at the activity level (considering the top-
10 predictions) and sum their energies to consolidate activity predictions and
accountforerroneouspredictions.Wethenrepeattheprocessfortheentireclip,
i.e., get the top-5 actions based on their frequency of occurrence at the frame
level and consolidated energies across frames. These five actions provide targets
8 S. Kundu et al.
forthemappingfunctionψ(ga,f ),whichisthentrainedwiththeMSEfunction.
i V
We use the top-5 action labels as targets to limit the effect of frequency bias.
Posterior-basedActivityRefinement.Thefinalstepinourframeworkis
aniterativerefinementprocessthatupdatestheactionconceptpriors(thethird
term in Equation 3) based on the predictions of the visual-semantic ground-
ing mechanism described in Section 3.3. Since our predictions are made on a
per-frame basis, it does not consider the overall temporal coherence and visual
dynamicsoftheclip.Hence,therecanbecontradictingpredictionsfortheactions
doneovertime.Hence,weiterativelyupdatetheactionpriorsfortheenergycom-
putation to re-rank the interpretations based on the clip-level visual dynamics.
We iteratively refine the activity labels and update the visual-semantic action
groundingmodulessimultaneouslybyalternatingbetweenposteriorupdateand
action grounding until the generalization error (i.e., the performance on unseen
actions) saturates, which indicates overfitting.
Implementation Details.WeuseanS3D-Gnetworkpre-trainedbyMiech
et al. [42,43] on Howto100M [43] as our visual feature extraction for visual-
semantic action grounding. We use a CLIP model with the ViT-B/32 [20] as its
backbone network. ConceptNet was used as our source of commonsense knowl-
edge for neuro-symbolic reasoning, and ConceptNet Numberbatch [54] was used
as the semantic representation for action grounding. The mapping function, de-
fined in Section 3.3, was a 1-layer feedforward network trained with the MSE
loss for 100 epochs with a batch size of 256 and learning rate of 10−3. General-
izationerrorswereusedtopickthebestmodel.Experimentswereconductedon
a desktop with a 32-core AMD ThreadRipper and an NVIDIA TitanRTX.
4 Experimental Evaluation
Data. To evaluate the open-world inference capabilities, we evaluate the ap-
proach on GTEA Gaze [23], GTEA GazePlus [35], and EPIC-Kitchens-100 [15,
17] datasets, which contain egocentric, multi-subject videos of meal preparation
activities. GTEA Gaze and GazePlus have frame-level gaze information and ac-
tivity labels, providing an ideal test bed for our setup. EPIC-Kitchens-100 is a
much larger dataset and does not have gaze information, offering a much more
challenging evaluation of the approach. We also evaluate on Charades-Ego [51],
alargeregocentricvideodatasetfocusedonactivitiesofdailyliving,toevaluate
on the zero-shot setting. The evaluation datasets, under an open-world setting,
offerasignificantchallengewithanincreasinglylargersearchspace.TheGTEA
Gaze dataset has 10 verbs and 38 nouns (search space of 380 activities), while
GTEA GazePlus has 15 verbs and 27 nouns (search space of 405), Charades-
Ego has 33 verbs and 38 nouns (search space of 1254), and Epic-Kitchens has
97 verbs and 300 nouns (search space of 29100).
Evaluation Metrics. Following prior work in open-world activity recogni-
tion [2,4], we use accuracy to evaluate action and object recognition and use
word-level accuracy for evaluating the activity (verb+noun) recognition perfor-
mance.Itprovidesaless-constrainedmeasurementtomeasurethequalityofpre-
Abbreviated paper title 9
GTEAGaze GTEAGazePlus
Approach Search VLM?
Space ObjectActionActivityObjectActionActivity
Two-StreamCNN[52] Closed ✗ 38.05 59.54 53.08 61.87 58.65 44.89
IDT[58] Closed ✗ 45.07 75.55 40.41 53.45 66.74 51.26
ActionDecomposition[64] Closed ✗ 60.01 79.39 55.67 65.62 75.07 57.79
Random Known ✗ 3.22 7.69 2.50 3.70 4.55 2.28
ActionDecompositionZSL*[64] Known ✗ 65.81 89.17 68.70 53.40 32.48 29.19
ALGOZSL(Ours) Known ✗ 49.47 74.74 27.34 47.67 29.31 16.68
KGL[4] Open ✗ 5.12 8.04 4.91 14.78 6.73 10.87
KGL+CLIP[4] Open ✗ 10.36 8.15 9.21 20.49 9.23 14.86
ALGO(Ours) Open ✗ 13.07 17.05 15.05 26.23 11.44 18.84
EgoVLP[36] Open ✓ 10.17 8.45 9.31 29.43 17.17 23.30
LaViLa[65] Open ✓ 6.07 23.07 14.57 28.27 25.47 26.87
ALGO+LaViLa Open ✓ 17.50 26.60 22.05 30.74 27.00 28.87
Table 1: Open-world activity recognition performance on the GTEA Gaze and
GTEA Gaze Plus datasets. We compare approaches with a closed search space, those
with a known search space, and those with a partially open one. Accuracy is reported
forpredictedobjects,actions,andactivities.VLM:Vision-LanguageModelpre-trained
onegocentricvideodata.*indicatestrainingon“seen” classesfromthesamedataset(s)
and leave-one-action-out evaluation.
dictionsbeyondaccuracybyconsideringallunitswithoutdistinguishingbetween
insertions, deletions, or misclassifications. This helps quantify the performance
without penalizing semantically similar interpretations. We use class-wise mAP
to evaluate ALGO in the zero-shot learning setup on Charades-Ego.
Baselines. We compare against different egocentric action recognition ap-
proaches, including those with a closed-world learning setup. For open-world
inference, we compare it against Knowledge Guided Learning (KGL) [4], which
introducedthenotionofopen-worldegocentricactionrecognition.Wealsocreate
abaselinecalled“KGL+CLIP” byaugmentingKGLwithCLIP-basedgrounding
by including CLIP’s similarity score for establishing semantic correspondences.
WecomparewithsupervisedlearningmodelssuchasActionDecomposition[64],
IDT[58],andTwo-StreamCNN[52],withastrongclosed-worldassumptionand
a dependency on labeled training data. We compare against the zero-shot ver-
sion of Action Decomposition, which can work under a known world where the
final activity labels are known. Not that this is not a fair comparison since it
is evaluated under a leave-one-action-out zero-shot learning setting trained on
examples from the corresponding dataset for “seen” actions. We report it for
completeness. We also compare against large vision-language models, such as
EGO-VLP [36], HierVL [6], and LAVILA [65] in both zero-shot and open-world
settings (by prompting with all possible verb+noun combinations).
4.1 Open World Activity Recognition
Table 1 summarizes the evaluation results under the open-world inference set-
ting. Top-1 prediction results are reported for all approaches. As can be seen,
CLIP-basedgroundingsignificantlyimprovestheperformanceofobjectrecogni-
tion for KGL, as opposed to the originally proposed, prior-only correspondence
10 S. Kundu et al.
Approach VLM?ActionObjectActivity
Random ✗ 1.03 0.33 0.68
KGL [4] ✗ 3.89 2.56 3.23
KGL+CLIP [4] ✗ 5.32 4.67 4.99
ALGO (Ours) ✗ 10.21 6.76 8.48
EgoVLP [36] ✓ 10.77 19.51 15.14
LaViLa [65] ✓ 11.16 23.25 17.21
ALGO+LaViLa ✓ 12.54 22.84 17.69
Table 2: Evaluation on the EPIC-Kitchens-100 dataset. VLM: Vision-Language pre-
training on egocentric data. Accuracy for actions, objects, and activity are reported.
function. However, our neuro-symbolic grounding mechanism (Section 3.1) im-
provesitfurther,achievinganobjectrecognitionperformanceof13.07%onGaze
and 26.23% on Gaze Plus. It is interesting to note that naïvely adding CLIP as
a mechanism for grounding objects, while effective, does not provide significant
gains in the overall action recognition performance (an average of 2% across
Gaze and Gaze Plus). We attribute it to the fact that the inherent camera mo-
tion in egocentric videos introduces occlusions and visual variations that make
ithardtorecognizeactionsconsistently.Evidence-basedgrounding,asproposed
in ALGO, makes it more robust to such changes and improves object and ac-
tion recognition performance. Similarly, the posterior-based action refinement
module (Section 3.3) helps achieve a top-1 action recognition performance of
17.05% on Gaze and 11.44% on Gaze Plus, outperforming KGL (8.04% and
6.73%). Adding action priors from LaViLa (ϕ(p(ga|I )) in Equation 3) allows
k t
us to improve the performance further, as indicated by ALGO+LaViLa. We see
that LaVila’s performance is consistently improved on all metrics. Note that
evenwithouttheactionpriors,weoutperformLaViLaonGTEAGazeandoffer
competitiveperformanceonGazePluswithoutpre-trainingonegocentricvideos.
We also evaluate our approach on the Epic-Kitchens-100 dataset, a larger-
scale dataset with a significantly higher number of concepts (actions, verbs, and
activities). Table 2 summarizes the results. We significantly outperform non-
VLM models while offering competitive performance to the VLM-based models.
We see that even without any video-based training data, we achieve an action
accuracy of 10.21% and object accuracy of 6.76%, indicating that we can learn
affordance-based relationships for discovering and grounding novel actions in
egocentric data. Adding action priors from LaViLa further improves the perfor-
mance. Interestingly, the action (verb) prediction performance of both LaViLa
is improved, although it is at the cost of reduced object accuracy. Note that the
predictionsarenotseparateforverbsandnounsbutarecomputedfromthepre-
dicted activity. These are remarkable results, considering that the search space
is open, i.e., the verb+noun combination is unknown and can be large (380 for
Gaze, 405 for Gaze Plus, and 29100 on EPIC-Kitchens).
To evaluate the generalization capabilities of ALGO, we presented videos
with unseen actions, i.e., actions not in the original training domain, and an
Abbreviated paper title 11
Pre-TrainingData
Approach VisualBackbonePre-Training? mAP
Ego? Source Size
EGO-VLPw/oEgoNCE[36] TimeSformer [8] VisLang ✗ Howto100M[43] 136M 9.2
EGO-VLPw/oEgoNCE [36] TimeSformer[8] VisLang ✗ CC3M+WebVid-2m5.5M 20.9
EGO-VLP+EgoNCE [36] TimeSformer[8] VisLang ✓ EgoClip[36] 3.8M 23.6
HierVL [6] FrozenInTime[7] VisLang ✓ EgoClip[36] 3.8M 26.0
LAVILA [65] TimeSformer[8] VisLang ✓ Ego4D[24] 4M 26.8
ALGO(Ours) S3D-G [43] VisionOnly ✗ Howto100M[43] 136M 17.3
ALGO(Ours) S3D [61] VisionOnly ✗ Kinetics-400[30] 240K 16.8
Table 3: Evaluation of ALGO under zero-shot learning settings on Charades-Ego
where the search space is constrained to ground truth activity semantics. VisLang:
Vision Language Pre-Training.
open search action space, i.e., not derived from the dataset annotations. We
prompted GPT-4 [12] using the ChatGPT interface to provide 100 everyday
actions and objects that can be performed in the kitchen to construct our open-
world search space and evaluated on GTEA Gaze and GazePlus. On unseen
actions and unknown search space, the performance was competitive, achieving
anaccuracyof9.87%onGazeand8.45%onGazePlus.Pleaseseesupplementary
for more details and results from the generalization studies. These results are
encouraging, as they significantly reduce the gap between closed-world learning
(supervised), known-world learning (zero-shot), and open-world learning.
Extension to Zero-Shot Egocentric Activity Recognition Open-world
learning involves the combinatorial search over the different, plausible compo-
sitions of elementary concepts. In activity recognition, this involves discovering
the action-object (verb-noun) combinations that make up an activity. However,
in many applications, such as zero-shot recognition, the search space is known,
andthereisaneedtopredictpre-specifiedlabels.Tocompareourapproachwith
such foundation models, we evaluate ALGO on the Charades-Ego dataset and
summarize the results in Table 3. We consider the top-10 interpretations made
for each clip and perform a nearest neighbor search using ConceptNet Number-
batchembeddingtothesetofground-truthlabelsandpicktheonewiththeleast
distance. It provides a simple yet effective mechanism to extend our approach
to zero-shot settings. We achieve an mAP score of 16.8% using an S3D [61]
model pre-trained on Kinetics-400 [30] and an S3D-G [42] model pre-trained on
Howto100M [43]. This significantly outperforms a comparable TimeSFormer [8]
model pre-trained with a vision-language alignment objective function and pro-
vides competitive performance to state-of-the-art vision-language models with
significantly lower training requirements - both data and time. We observe a
similarperformanceintheGazeandGazePlusdatasetsasshowninTable1.We
obtain27.34%onGazeand16.69%onGazePlus,performingcompetitivelywith
the zero-shot approaches. These results are obtained without large amounts of
paired text-video pairs and a simple visual-semantic grounding approach. Note
that the performance for zero-shot ActionDecomposition is reported for leave-
one-classcross-validation,whileourapproachtreatsallclassesasunseenclasses.
12 S. Kundu et al.
(a) (b) (c) (d)
Fig.2: Ablation studies showing the impact of (a) the quality of object grounding
techniques, (b) posterior-based action refinement, (c) iterative action refinement on
generalization capabilities, and (d) the choice of visual and semantic representations.
4.2 Ablation Studies
Wesystematicallyexaminetheimpactoftheindividualcomponentsontheover-
all performance of the approach. Specifically, we focus on three aspects: (i) the
impactofobjectgrounding,(ii)theimpactofposterior-basedactionrefinement,
and(iii)thegeneralizationoflearnedmodelswithrefinement,and(iv)thechoice
of visual and semantic features. We experiment on the GTEA Gaze dataset and
present results in Figure 2 and discuss the results below.
Quality and Impact of Object Grounding.First,weevaluatetheobject
recognitionperformanceofdifferentobjectgroundingtechniquesandpresentre-
sultsinFigure2(a).Weconsider5differenttechniques:theprior-basedapproach
proposedinKGL,updatingthepriorwithCLIP-basedlikelihood(KGL+CLIP),
näively using CLIP to recognize the object in the gaze-based ROI (CLIP Only),
the proposed evidence-based object grounding (CLIP+Evidence), and using ev-
idence only without checking object-level likelihood (Evidence Only). As can
be seen, using CLIP improves performance significantly across the different ap-
proacheswhileusingevidenceprovidesgainsoverthenäiveCLIPOnlymethod.
KGL+CLIP and the proposed CLIP+Evidence approaches perform similarly,
with KGL+CLIP being slightly better when considering more than the top-
5 recognized objects. However, this does not always transfer to better action
recognition because the object probabilities are much closer in KGL+CLIP
than in the proposed CLIP+Evidence. We also evaluated the performance of
CLIP+Evidence on an unknown search space by prompting GPT-4 to provide
a list of 100 objects commonly found in the kitchen. The Top-3 performance is
excellent, reaching 45% before saturating, which is remarkable considering that
the unknown search space with possibly unseen objects. We anticipate using vi-
sual commonsense priors, such as from scene graphs [32], can help disambiguate
between visually similar objects.
Impact of Posterior-based Action Refinement. One of the major con-
tributions of ALGO is the use of continuous posterior-based action refinement,
where the energy of the action generator is refined based on an updated prior
from the visual-semantic action grounding to improve the activity recognition
performance. One key question is how many iterations of such refinements are
idealbeforeoverfittingoccurs.Figure2(b)visualizestheactivityrecognitionper-
formancewithdifferentlevelsofiteration,alongwiththeresultsofaconstrained
Abbreviated paper title 13
search space (zero-shot) approach. As can be seen, the first two iterations sig-
nificantly improved the performance, while the third iteration provided very
negligible improvement, which provided indications of overfitting. Constraining
thesearchspaceinthezero-shotsettingsignificantlyimprovestheperformance.
Generalization of Visual-Semantic Action Grounding. To evaluate
the impact of the posterior-based refinement on the generalization capabilities,
weevaluatedthetrainedmodels,atdifferentiterations,ontheGTEAGazePlus
dataset. As can be seen from Figure 2, each iteration improves the performance
of the model before the performance starts to stagnate (at the third iteration).
These results indicate that while iterative refinement is useful, it can lead to
overfitting to the domain-specific semantics and can hurt the generalization ca-
pabilities of the approach. To this end, we keep the termination criteria for the
iterative posterior-based refinement based on the generalization performance of
the action grounding model on unseen actions.
Impact of Visual-Semantic Features. Finally, we evaluate ALGO with
different visual and semantic features and visualize the results in Figure 2 (d).
We see that the use of ConceptNet Numberbatch (NB) considerably improves
theperformanceoftheapproachasopposedtousingGloVeembeddings[46].The
choiceofvisualfeatures(S3DGvs.S3D)doesnotimpacttheperformancemuch.
We hypothesize that the NB’s ability to capture semantic attributes [55] allows
it to generalize better than GloVe. We anticipate custom training of embedding
vectors using contextualized, ConceptNet-based pattern theory interpretations
could lead to better generalization capabilities.
4.3 Generalization of Learned Actions to Unknown Vocabulary
To measure the generalization capability of the approach to unknown actions,
we use the word similarity score (denoted as NB-WS) to measure the semantic
similarity between the predicted and ground truth actions. NB-WS has demon-
strated the ability to capture attribute-based representations when computing
similarity [55]. We evaluate ALGO’s ability to recognize actions from out of
its training distribution by presenting videos from datasets with unseen actions
andanunknownsearchspace.Specifically,werefertoactionsnotintheoriginal
trainingdomainsas“unseen” actions,followingconventionfromzero-shotlearn-
ing.Similarly,inanunknownsearchspace,i.e.,completely open world inference,
thesearchspaceisnotpre-specifiedbutinferredfromgeneral-purposeknowledge
sources. For these experiments, we prompted GPT-4 [12] using the ChatGPT
interface to provide 100 everyday actions that can be performed in the kitchen
to construct our search space. The results are summarized in Table 4, where we
present the verb accuracy and the ConceptNet Numberbatch Word Similarity
(NB-WS) score. ALGO generalizes consistently across datasets. Of particular
interest is the generalization from Gaze and Gaze Plus to Charades-Ego, where
there is a significantly higher number of unseen and unknown actions. Models
trainedonGTEAGaze,whichhasmorevariationincameraqualityandactions,
generalizebetterthanthosefromGazePlus.Withunseenactionsandunknown
search space, the performance was competitive, achieving an accuracy of 9.87%
14 S. Kundu et al.
Training Data Evaluation Data
UnknownSearchAccuracyNB-WS
Dataset # Verbs Dataset # Verbs Verbs? Space
Gaze 10 Gaze 10 ✗ K 14.11 27.24
GazePlus 15 GazePlus 15 ✗ K 11.44 24.45
Charades-Ego 33 Charades-Ego 33 ✗ K 11.92 36.02
Gaze 10 Charades-Ego 33 ✓ K 13.55 34.83
GazePlus 15 Charades-Ego 33 ✓ K 10.24 31.11
GazePlus 15 Gaze 10 ✓ K 5.27 29.68
Charades-Ego 33 Gaze 10 ✓ K 10.17 32.65
Gaze 10 GazePlus 15 ✓ K 10.37 23.55
Charades-Ego 33 GazePlus 15 ✓ K 11.22 24.25
Gaze 10 Gaze 10 ✓ U 9.87 14.51
GazePlus 15 GazePlus 15 ✓ U 8.45 11.78
Table 4: Generalization studies to analyze the performance of the action (verb)
recognition models learned in an open-world setting. The models are trained in one
domain and evaluated in another, containing possible unknown and unseen actions.
NB-WS: ConceptNet Numberbatch Word Similarity
on Gaze and 8.45% on Gaze Plus. NB-WS was higher, indicating better agree-
ment with the ground truth, i.e., the predicted verbs were similar to the ground
truth. While there is room for improvement, these results present a significant
step towards truly open-world learning without any constraints.
5 Discussion, Limitations, and Future Work
In this work, we proposed ALGO, a neuro-symbolic framework for open-world
egocentricactivityrecognitionthataimstolearnnovelactionandactivityclasses
withoutexplicitsupervision.Bygroundingobjectsandusinganobject-centered,
knowledge-based approach to activity inference, we reduce the need for labeled
datatolearnsemanticassociationsamongelementaryconcepts.Wedemonstrate
thattheopen-worldlearningparadigmisaneffectiveinferencemechanismtodis-
tillcommonsenseknowledgefromsymbolicknowledgebasesforgroundedaction
understanding. While showing competitive performance, there are two key lim-
itations: (i) it is restricted to ego-centric videos due to the need to navigate
clutter by using human attention as a contextual cue for object grounding, and
(ii) it requires a knowledge base such as ConceptNet to learn associations be-
tween actions and objects and hence is restricted to its vocabulary. While we
demonstrated its performance on an unknown search space, much work remains
to effectively build a search space (both action and object) to move towards a
trulyopen-worldlearningparadigm.Weaimtoexploretheuseofattention-based
mechanisms[1,44]toextendtheframeworktothird-personvideosandusingab-
ductive reasoning [5,63] and neural knowledgebase completion approaches [10]
to integrate visual commonsense into the reasoning while moving beyond the
vocabulary encoded in symbolic knowledgebases.
Abbreviated paper title 15
Acknowledgements. This research was supported in part by the US Na-
tional Science Foundation grants IIS 2143150, and IIS 1955230. We thank Dr.
Anuj Srivastava (FSU) and Dr. Sudeep Sarkar (USF) for their thoughtful feed-
back during the discussion about the problem formulation and experimental
analysis phase of the project.
References
1. Aakur, S., Sarkar, S.: Actor-centered representations for action localization in
streaming videos. In: Computer Vision–ECCV 2022: 17th European Conference,
Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXVIII. pp. 70–87.
Springer (2022) 14
2. Aakur,S.,deSouza,F.,Sarkar,S.:Generatingopenworlddescriptionsofvideous-
ingcommonsenseknowledgeinapatterntheoryframework.QuarterlyofApplied
Mathematics 77(2), 323–356 (2019) 3, 5, 8
3. Aakur, S.N., Bagavathi, A.: Unsupervised gaze prediction in egocentric videos by
energy-based surprise modeling. In: International Joint Conference on Computer
Vision, Imaging and Computer Graphics Theory and Applications (2021) 3
4. Aakur, S.N., Kundu, S., Gunti, N.: Knowledge guided learning: Open world ego-
centricactionrecognitionwithzerosupervision.PatternRecognitionLetters156,
38–45 (2022) 3, 4, 7, 8, 9, 10, 1
5. Aakur, S.N., Sarkar, S.: Leveraging symbolic knowledge bases for commonsense
natural language inference using pattern theory. IEEE Transactions on Pattern
Analysis and Machine Intelligence (2023) 14
6. Ashutosh, K., Girdhar, R., Torresani, L., Grauman, K.: Hiervl: Learning hierar-
chical video-language embeddings (2023) 2, 4, 7, 9, 11
7. Bain, M., Nagrani, A., Varol, G., Zisserman, A.: Frozen in time: A joint video
and image encoder for end-to-end retrieval. In: Proceedings of the IEEE/CVF
International Conference on Computer Vision. pp. 1728–1738 (2021) 11
8. Bertasius, G., Wang, H., Torresani, L.: Is space-time attention all you need for
videounderstanding?In:InternationalConferenceonMachineLearning.pp.813–
824. PMLR (2021) 11
9. Bommasani, R., Hudson, D.A., Adeli, E., Altman, R., Arora, S., von Arx, S.,
Bernstein, M.S., Bohg, J., Bosselut, A., Brunskill, E., et al.: On the opportunities
and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021) 2
10. Bosselut,A.,Rashkin,H.,Sap,M.,Malaviya,C.,Celikyilmaz,A.,Choi,Y.:Comet:
Commonsense transformers for automatic knowledge graph construction. In: Pro-
ceedings of the 57th Annual Meeting of the Association for Computational Lin-
guistics. pp. 4762–4779 (2019) 14
11. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-
lakantan,A.,Shyam,P.,Sastry,G.,Askell,A.,etal.:Languagemodelsarefew-shot
learners.AdvancesinNeuralInformationProcessingSystems33,1877–1901(2020)
4
12. Bubeck,S.,Chandrasekaran,V.,Eldan,R.,Gehrke,J.,Horvitz,E.,Kamar,E.,Lee,
P., Lee, Y.T., Li, Y., Lundberg, S., et al.: Sparks of artificial general intelligence:
Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 (2023) 4, 11, 13
13. Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for con-
trastivelearningofvisualrepresentations.In:InternationalConferenceonMachine
Learning. pp. 1597–1607. PMLR (2020) 4
16 S. Kundu et al.
14. Clark, K., Luong, M.T., Le, Q.V., Manning, C.D.: Electra: Pre-training text en-
coders as discriminators rather than generators. arXiv preprint arXiv:2003.10555
(2020) 4
15. Damen, D., Doughty, H., Farinella, G.M., Fidler, S., Furnari, A., Kazakos, E.,
Moltisanti, D., Munro, J., Perrett, T., Price, W., Wray, M.: The epic-kitchens
dataset:Collection,challengesandbaselines.IEEETransactionsonPatternAnal-
ysis and Machine Intelligence (TPAMI) 43(11), 4125–4141 (2021). https://doi.
org/10.1109/TPAMI.2020.2991965 8
16. Damen, D., Doughty, H., Farinella, G.M., Fidler, S., Furnari, A., Kazakos, E.,
Moltisanti,D.,Munro,J.,Perrett,T.,Price,W.,etal.:Theepic-kitchensdataset:
Collection, challenges and baselines. IEEE Transactions on Pattern Analysis and
Machine Intelligence 43(11), 4125–4141 (2020) 3
17. Damen,D.,Doughty,H.,Farinella,G.M.,Furnari,A.,Ma,J.,Kazakos,E.,Molti-
santi,D.,Munro,J.,Perrett,T.,Price,W.,Wray,M.:Rescalingegocentricvision:
Collection, pipeline and challenges for epic-kitchens-100. International Journal of
Computer Vision (IJCV) 130, 33–55 (2022), https://doi.org/10.1007/s11263-
021-01531-2 8
18. Devlin,J.,Chang,M.W.,Lee,K.,Toutanova,K.:Bert:Pre-trainingofdeepbidirec-
tional transformers for language understanding. arXiv preprint arXiv:1810.04805
(2018) 2, 4
19. Dong, N., Zhang, Y., Ding, M., Lee, G.H.: Open world detr: transformer based
open world object detection. arXiv preprint arXiv:2212.02969 (2022) 4
20. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth
16x16 words: Transformers for image recognition at scale. In: International Con-
ference on Learning Representations 2, 8, 1
21. Du,Y.,Wei,F.,Zhang,Z.,Shi,M.,Gao,Y.,Li,G.:Learningtopromptforopen-
vocabulary object detection with vision-language model. In: Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.14084–
14093 (2022) 4
22. Fan, C.: Egovqa-an egocentric video question answering benchmark dataset. In:
Proceedings of the IEEE/CVF International Conference on Computer Vision
Workshops. pp. 0–0 (2019) 3
23. Fathi, A., Li, Y., Rehg, J.M.: Learning to recognize daily actions using gaze. In:
European Conference on Computer Vision. pp. 314–327. Springer (2012) 3, 8
24. Grauman,K.,Westbury,A.,Byrne,E.,Chavis,Z.,Furnari,A.,Girdhar,R.,Ham-
burger, J., Jiang, H., Liu, M., Liu, X., et al.: Ego4d: Around the world in 3,000
hours of egocentric video. In: Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition. pp. 18995–19012 (2022) 3, 11
25. Grenander, U.: Elements of pattern theory. JHU Press (1996) 3, 5
26. Gu, X., Lin, T.Y., Kuo, W., Cui, Y.: Open-vocabulary object detection via vision
andlanguageknowledgedistillation.In:InternationalConferenceonLearningRep-
resentations (2021), https://api.semanticscholar.org/CorpusID:238744187 4
27. Han, S., Liu, B., Cabezas, R., Twigg, C.D., Zhang, P., Petkau, J., Yu, T.H., Tai,
C.J., Akbay, M., Wang, Z., et al.: Megatrack: monochrome egocentric articulated
hand-tracking for virtual reality. ACM Transactions on Graphics (ToG) 39(4),
87–1 (2020) 3
28. Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q.V., Sung, Y.,
Li, Z., Duerig, T.: Scaling up visual and vision-language representation learning
with noisy text supervision (2021) 4
Abbreviated paper title 17
29. Jiang, J., Ahn, S.: Generative neurosymbolic machines. In: Larochelle, H., Ran-
zato, M., Hadsell, R., Balcan, M., Lin, H. (eds.) Advances in Neural Infor-
mation Processing Systems. vol. 33, pp. 12572–12582. Curran Associates, Inc.
(2020), https://proceedings.neurips.cc/paper_files/paper/2020/file/
94c28dcfc97557df0df6d1f7222fc384-Paper.pdf 4
30. Kay,W.,Carreira,J.,Simonyan,K.,Zhang,B.,Hillier,C.,Vijayanarasimhan,S.,
Viola,F.,Green,T.,Back,T.,Natsev,P.,etal.:Thekineticshumanactionvideo
dataset. arXiv preprint arXiv:1705.06950 (2017) 11
31. Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., Maschinot,
A., Liu, C., Krishnan, D.: Supervised contrastive learning. Advances in Neural
Information Processing Systems 33, 18661–18673 (2020) 4
32. Kundu, S., Aakur, S.N.: Is-ggt: Iterative scene graph generation with generative
transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition. pp. 6292–6301 (2023) 12
33. Li,H.,Cai,Y.,Zheng,W.S.:Deepdualrelationmodelingforegocentricinteraction
recognition.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionand
Pattern Recognition. pp. 7932–7941 (2019) 3
34. Li,Y.,Liang,F.,Zhao,L.,Cui,Y.,Ouyang,W.,Shao,J.,Yu,F.,Yan,J.:Super-
vision exists everywhere: A data efficient contrastive language-image pre-training
paradigm (2022) 4
35. Li, Y., Fathi, A., Rehg, J.M.: Learning to predict gaze in egocentric video. In:
ProceedingsoftheIEEEInternationalConferenceonComputerVision.pp.3216–
3223 (2013) 3, 6, 8, 1
36. Lin,K.Q.,Wang,J.,Soldan,M.,Wray,M.,Yan,R.,XU,E.Z.,Gao,D.,Tu,R.C.,
Zhao, W., Kong, W., et al.: Egocentric video-language pretraining. Advances in
Neural Information Processing Systems 35, 7575–7586 (2022) 2, 4, 7, 9, 10, 11, 1
37. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
Zettlemoyer, L., Stoyanov, V.: Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692 (2019) 4
38. Lu, Z., Grauman, K.: Story-driven summarization for egocentric video. In: Pro-
ceedings of the IEEE conference on Computer Vision and Pattern Recognition.
pp. 2714–2721 (2013) 3
39. Ma,M.,Fan,H.,Kitani,K.M.:Goingdeeperintofirst-personactivityrecognition.
In:IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR).
pp. 1894–1903 (2016) 2, 3
40. Maguire, M.J., Dove, G.O.: Speaking of events: event word learning and event
representation. Understanding Events: How Humans See, Represent, and Act on
Events pp. 193–218 (2008) 6
41. Menon, S., Vondrick, C.: Visual classification via description from large language
models. arXiv preprint arXiv:2210.07183 (2022) 5
42. Miech,A.,Alayrac,J.B.,Smaira,L.,Laptev,I.,Sivic,J.,Zisserman,A.:End-to-end
learningofvisualrepresentationsfromuncuratedinstructionalvideos.In:Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
pp. 9879–9889 (2020) 8, 11, 1
43. Miech, A., Zhukov, D., Alayrac, J.B., Tapaswi, M., Laptev, I., Sivic, J.:
Howto100m: Learning a text-video embedding by watching hundred million nar-
rated video clips. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision. pp. 2630–2640 (2019) 8, 11, 1
44. Mounir, R., Shahabaz, A., Gula, R., Theuerkauf, J., Sarkar, S.: Towards au-
tomated ethogramming: Cognitively-inspired event segmentation for streaming
18 S. Kundu et al.
wildlife video monitoring. International Journal of Computer Vision pp. 1–31
(2023) 14
45. Nye, M., Tessler, M., Tenenbaum, J., Lake, B.M.: Improving coherence and con-
sistency in neural sequence models with dual-system, neuro-symbolic reasoning.
Advances in Neural Information Processing Systems 34, 25192–25204 (2021) 4
46. Pennington,J.,Socher,R.,Manning,C.:Glove:Globalvectorsforwordrepresen-
tation. In: Proceedings of the 2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP). pp. 1532–1543 (2014) 13
47. Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,Agarwal,S.,Sastry,G.,
Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from
natural language supervision. In: International Conference on Machine Learning.
pp. 8748–8763. PMLR (2021) 2, 3, 4, 5
48. Radford,A.,Narasimhan,K.,Salimans,T.,Sutskever,I.:Improvinglanguageun-
derstanding by generative pre-training (2018) 4
49. Radford,A.,Wu,J.,Child,R.,Luan,D.,Amodei,D.,Sutskever,I.,etal.:Language
models are unsupervised multitask learners. OpenAI blog 1(8), 9 (2019) 4
50. Ryoo, M.S., Rothrock, B., Matthies, L.: Pooled motion features for first-person
videos. In: Proceedings of the IEEE conference on Computer Vision and Pattern
Recognition (CVPR) (June 2015) 3
51. Sigurdsson, G.A., Gupta, A., Schmid, C., Farhadi, A., Alahari, K.: Actor and
observer: Joint modeling of first and third-person videos. In: Proceedings of the
IEEE/CVF International Conference on Computer Vision. pp. 7396–7404 (2018)
2, 3, 8
52. Simonyan,K.,Zisserman,A.:Two-streamconvolutionalnetworksforactionrecog-
nition in videos. Advances in Neural Information Processing Systems 27 (2014)
9
53. deSouza,F.D.,Sarkar,S.,Srivastava,A.,Su,J.:Patterntheoryforrepresentation
and inference of semantic structures in videos. Pattern Recognition Letters 72,
41–51 (2016) 3, 5
54. Speer,R.,Chin,J.,Havasi,C.:Conceptnet5.5:Anopenmultilingualgraphofgen-
eralknowledge.In:ProceedingsoftheAAAIConferenceonArtificialIntelligence.
vol. 31 (2017) 3, 5, 7, 8, 1
55. Speer, R., Lowry-Duda, J.: Luminoso at SemEval-2018 task 10: Distinguishing
attributes using text corpora and relational knowledge. In: Proceedings of the
12th International Workshop on Semantic Evaluation. pp. 985–989. Association
forComputationalLinguistics,NewOrleans,Louisiana(Jun2018).https://doi.
org/10.18653/v1/S18-1162, https://aclanthology.org/S18-1162 13
56. Sudhakaran,S.,Escalera,S.,Lanz,O.:Lsta:Longshort-termattentionforegocen-
tricactionrecognition.In:ProceedingsoftheIEEE/CVFConferenceonComputer
Vision and Pattern Recognition (CVPR) (June 2019) 3
57. Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,
Ł.,Polosukhin,I.:Attentionisallyouneed.AdvancesinNeuralInformationPro-
cessing Systems 30 (2017) 4
58. Wang, H., Schmid, C.: Action recognition with improved trajectories. In:
IEEE/CVFInternationalConferenceonComputerVision(ICCV).pp.3551–3558
(2013) 2, 9
59. Wang,X.,Zhu,L.,Wang,H.,Yang,Y.:Interactiveprototypelearningforegocen-
tricactionrecognition.In:ProceedingsoftheIEEE/CVFInternationalConference
on Computer Vision (ICCV). pp. 8168–8177 (October 2021) 3
Abbreviated paper title 19
60. Wu,T.,Tjandrasuwita,M.,Wu,Z.,Yang,X.,Liu,K.,Sosic,R.,Leskovec,J.:Zeroc:
Aneuro-symbolicmodelforzero-shotconceptrecognitionandacquisitionatinfer-
encetime.In:Koyejo,S.,Mohamed,S.,Agarwal,A.,Belgrave,D.,Cho,K.,Oh,A.
(eds.)AdvancesinNeuralInformationProcessingSystems.vol.35,pp.9828–9840.
CurranAssociates,Inc.(2022),https://proceedings.neurips.cc/paper_files/
paper/2022/file/3ff48dde82306fe8f26f3e51dd1054d7-Paper-Conference.pdf
4
61. Xie,S.,Sun,C.,Huang,J.,Tu,Z.,Murphy,K.:Rethinkingspatiotemporalfeature
learning: Speed-accuracy trade-offs in video classification. In: Proceedings of the
European Conference on Computer Vision. pp. 305–321 (2018) 11
62. Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., Wu, Y.: Coca:
Contrastive captioners are image-text foundation models (2022) 4
63. Zellers, R., Bisk, Y., Farhadi, A., Choi, Y.: From recognition to cognition: Visual
commonsense reasoning. In: Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) (June 2019) 14
64. Zhang, Y.C., Li, Y., Rehg, J.M.: First-person action decomposition and zero-
shot learning. In: IEEE Winter Conference on Applications of Computer Vision
(WACV). pp. 121–129 (2017) 2, 3, 9
65. Zhao, Y., Misra, I., Krähenbühl, P., Girdhar, R.: Learning video representations
from large language models. In: CVPR (2023) 2, 4, 7, 9, 10, 11
66. Zhou, Y., Ni, B., Hong, R., Yang, X., Tian, Q.: Cascaded interactional targeting
network for egocentric video analysis. In: Proceedings of the IEEE conference on
Computer Vision and Pattern Recognition (CVPR) (June 2016) 3
Abbreviated paper title 1
Implementation Details.
WeuseanS3D-Gnetworkpre-trainedbyMiechetal.[42,43]onHowto100M[43]
as our visual feature extraction for visual-semantic action grounding. We use a
CLIP model with the ViT-B/32 [20] as its backbone network. ConceptNet was
usedasoursourceofcommonsenseknowledgeforneuro-symbolicreasoning,and
ConceptNetNumberbatch[54]wasusedasthesemanticrepresentationforaction
grounding. The MCMC-based inference from KGL [4] was used as our reason-
ing mechanism. For experiments on Charades-Ego, where gaze information was
unavailable, center bias [35] was used to approximate gaze locations. The map-
ping function, defined in Section 3.3, was a 1-layer feedforward network trained
with the MSE loss for 100 epochs with a batch size of 256 and learning rate of
10−3. Generalization errors on unseen actions were used to pick the best model.
Two iterations of posterior-based action refinement were performed per video.
Experiments were conducted on a desktop with a 32-core AMD ThreadRipper
and an NVIDIA Titan RTX.
Why temporal smoothing?
Since we predict frame-level activity interpretations to account for gaze transi-
tions, we first perform temporal smoothing to label the entire video clip before
training the mapping function ψ(ga,f ) to reduce noise in the learning process.
i V
Foreachframeinthevideoclip,wetakethefivemostcommonactionspredicted
at the activity level (considering the top-10 predictions) and sum their energies
to consolidate activity predictions and account for erroneous predictions. We
then repeat the process for the entire clip, i.e., get the top-5 actions based on
theirfrequencyofoccurrenceattheframelevelandconsolidatedenergiesacross
frames. These five actions provide targets for the mapping function ψ(ga,f ),
i V
which is then trained with the MSE function. We use the top-5 action labels
as targets to limit the effect of frequency bias. For example, some actions, such
as clean, can possess a high affinity to many objects and hence be the most
commonly predicted action for a frame. Hence, temporal smoothing acts as a
regularizer to reduce overfitting by forcing the model to predict the embedding
for the top five actions for each video clip.
Why posterior-based refinement?
Since our predictions are made on a per-frame basis, it does not consider the
overall temporal coherence and visual dynamics of the clip. Hence, there can be
contradicting predictions for the actions done over time. Similarly, when setting
the action priors to 1, we consider all actions equally plausible and do not re-
strict the action labels through grounding, as done for objects in Section 3.1.
Hence, we iteratively update the action priors for the energy computation to re-
ranktheinterpretationsbasedontheclip-levelvisualdynamics.Thispriorcould
be updated to consider predictions from other models, such as EGO-VLP [36]
2 S. Kundu et al.
throughpromptingmechanismssimilartoourneuro-symbolicobjectgrounding.
We iteratively refine the activity labels and update the visual-semantic action
groundingmodulessimultaneouslybyalternatingbetweenposteriorupdateand
action grounding until the generalization error (i.e., the performance on un-
seenactions)saturates,whichindicatesoverfitting.Weempiricallyverifythisin
Section 4.2, where we observe the impact of this posterior update on activity
recognition performance and the resulting loss of generalization.
Datasets
The GTEA Gaze dataset consists of 14 subjects performing activities composed
of 10 verbs and 38 nouns across 17 videos. The gaze information is collected
using Tobii eye-tracking glasses at 30 frames per second. The Gaze Plus dataset
has27nounsand15verbsfrom6subjectsperforming7mealpreparationactiv-
ities across 37 videos. The gaze information is collected at 30 frames per second
for both datasets using SMI eye-tracking glasses. Charades-Ego contains 7,860
videos containing 157 activities. Following prior work [36], we use the 785 ego-
centric clips in the test set for evaluation. EpicKitchens-100 is a large dataset
comprising several hours of egocentric videos with 300 nouns (objects) and 97
verbs (actions). We use the validation set to evaluate our approach following
prior works [65].
Visualizations of generated interpretations
Evidence-basedgrounding.Someexamplesofevidence-basedgroundingthrough
ConceptNetareshowninFigure3.Ascanbeseen,eachconceptcanhavemulti-
ple evidence generators derived from ConceptNet using its ego-graph and limit-
ingedgestothosethatexpresscompositional propertiessuchasIsA, UsedFor,
HasPropertyandSynonymOf.Usingego-graphhelpspreservethecontextualin-
formationwithinthe semanticlocalityof theobject tofilterhigh-ordernoise in-
duced by regular k-hop neighborhoods. The derived concepts provide additional
context for verifying the presence of a given object in the video by querying
CLIP as a noisy object oracle. Note that we do not visualize the edge label to
avoidclutter.Eachedgeisqualifiedbyasemanticassertionandisquantifiedby
a value between -2 and 2 to express its strength. This provides a more explain-
ablerepresentationthatenhancesthefinalinterpretationgenerated,asdiscussed
next.
Semantically rich interpretations. The final interpretations generated
by the approach are shown in Figure 4. It can be seen that the final interpreta-
tion is semantically rich and has concepts that are not directly in the scene but
are compositionally relevant, either through affordance or object-level evidence.
We visualize two different interpretations for affordance-based reasoning for the
activity “cut fork” in (a) and (b), where it can be seen that the approach can
generate graphs of varying structures. It also shows the impact of the noise in
the knowledge graph that can introduce irrelevant concepts into the reasoning
Abbreviated paper title 3
process. We anticipate that having an additional reasoning step can reduce the
impact of noise. Interestingly, we see that combinations of similar verbs and
nouns such as “pour honey” (c) and “pour ketchup” (d) result in different un-
grounded generators, indicating that the affordance of each concept is used for
reasoning,resultinginsemanticallyrichgraphicalinterpretations.Weanticipate
that learning customized embeddings from these graphs can result in a better
grounding of novel compositional concepts such as actions and activities.
Baselines. Since it is the most comparable, deep learning-only baseline, we
choose ActionDecomposition [65] as our primary ZSL baseline for the GTEA
datasets. It decouples verb and noun recognition and uses similar feature ex-
tractors for noun+verb prediction. However, they only report leave-one-class-
outcross-validation,whichassumesaccesstootherexamplesfrom“seen” classes
during training, which is not a fair comparison with our approach. We do not
require any labels during training. Given the list of possible objects and actions
in the videos, we ground the objects using CLIP, infer plausible actions us-
ing affordance-based reasoning, and predict the final activities driven purely by
priorknowledgeencodedinConceptNet.BesidesCLIP(trainedonlyonobjects),
ALGOisnottrainedwithanylabels.Zero-shotmodelsand“foundationmodels”
aretrainedonconsiderableamountsofvideo datawith action/activity/object la-
belsandlearnverb+nounassociationsfromtheselabels.While“chain-of-thought
prompting” orotherdecompositionapproachescanpossiblyimprovetheirgener-
alization, no such models exist. We report the supervised/zero-shot/VLMs per-
formance to illustrate that ALGO performs competitively despite not requiring
large-scale video pretraining and labels (especially for actions and activities).
While there is a gap between their performance, note that we do not have ac-
cess to any data about actions (verbs) or activities, while VLMs and ZS mod-
els are trained exclusively on videos with such information. We learn to rec-
ognize actions with no labels and show that VLMs can benefit from ALGO
(ALGO+LAVILA and ALGO+EGOVLP).
Action Recognition and Metrics. While activity recognition is the focus
of the work, we would like to point out that the action (verb) recognition is also
doneinanopen-world,inferredwithoutlabeleddata.Theperformancesreported
for“Action” inTables1-3representALGO’sabilitytoinfertheverbfromcontex-
tualinformation.Thetop-5performanceofALGO(obj/action/activity)onGaze
(40.75/34.89/37.82vs.KGL’s32.39/10.73/18.78)andGaze+(42.77/53.88/48.33
vs.KGL’s24.64/37.99/27.53)aswellastheexactmatchactivityaccuracy(Gaze
- ALGO: 1.8, ALGO+LAVILA: 3.5, KGL: 0.3, EGOVLP: 0.9, LAVILA: 2.1,
Gaze+ - ALGO: 3.4, ALGO+LAVILA: 8.3, KGL: 1.1, EGOVLP: 4.1, LAVILA:
7.9) show consistent improvements over the baselines across all metrics. While
VLMs perform better than ALGO on exact match, which we attribute to their
ability to identify the verb correctly, augmenting them with ALGO consistently
improves their performance.
Search space and knowledge source. We use ConceptNet as the source
ofknowledge,overLLMorwordembeddingapproaches,duetoitsabilitytosup-
port probabilistic reasoning and an interpretable internal mechanism (see [4]).
4 S. Kundu et al.
While this does limit its vocabulary, it serves us well for reasoning over action-
object affordance and affinity. We could replace ConceptNet priors with word
embedding techniques such as GloVe or GPT 4, and the framework would still
functionwithoutanymodifications.PreliminaryanalysiswithGloVeembedding
on GTEA Gaze results in object/action/activity accuracy of 12.86/14.53/13.70,
which outperforms KGL/KGL+CLIP/LAVILA/EGOVLP. More complex em-
bedding could improve this performance at the cost of interpretability. Addi-
tionally, we show in Fig 2a that ConceptNet can be augmented with ChatGPT
to move beyond its vocabulary. The vocabulary of ConceptNet is not the
sameasthesearchspaceforCLIP/LAVILA/EGOVLP.Thesearchspace
is the space of prompts one provides to VLMs to select from and is usually pre-
definedinzero-shotinference.Open-worldinferencerequiresbuildingthissearch
space for VLMs to infer labels. We propose (including prior-driven prompting)
toconstructsuchasearchspacewithoutbrute-forcesearchoverallcombinations
of verbs/nouns.
Qualitative Analysis Our analysis shows that the performance across the
datasets varies and primarily stems from how the verbs and nouns are defined
in the dataset. For example, Gaze and Gaze+ have activity labels with less
ambiguous verb-noun combinations than EK100. For example, “clean”, “put”,
and “take”, common verbs in EK100, can apply to every single object and have
very high affinity in ConceptNet, leading to a higher likelihood of prediction.
This is one of the failure modes of ALGO and is one of our future research
directions.
Abbreviated paper title 5
(a) (b)
(c) (d)
(e) (f)
Fig.3: Visualization of alternative concepts that were tested for grounding concepts
in the video such as (a) fork, (b) knife, (c) table, (d) pepperoni, (e) biscuit, and (f)
chocolate.TheseareautomaticallyderivedfromConceptNetandhavesemanticasser-
tions quantifying how they are related.
6 S. Kundu et al.
(a) (b)
(c) (d)
(e) (f)
Fig.4: Visualization of final interpretations for videos containing the activity (a) cut
fork(topinterpretation),(b)cutfork(secondbestinterpretation),(c)pourhoney,(d)
pourketchup,(e)mixketchup,and(f)mixbowl.Theseareautomaticallyderivedfrom
ConceptNet and have semantic assertions quantifying how they are related.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Discovering Novel Actions from Open World Egocentric Videos with Object-Grounded Visual Commonsense Reasoning"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
