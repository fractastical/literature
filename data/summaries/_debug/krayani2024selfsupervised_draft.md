### OverviewThis paper investigates a novel self-supervised path-planning method for UAV-aided wireless networks, leveraging active inference. The core concept involves learning a world model from demonstrations provided by an offline path planning optimizer. This allows the UAV to understand the environment and implicitly discover the optimizer’s policy, enabling real-time autonomous decisions and online planning. The method centers on “expected surprise,” a metric used to score different policies and minimize deviations from the desired goal. The study demonstrates faster, more stable, and highly generalized solutions compared to traditional Q-learning approaches.### MethodologyThe proposed method, as detailed in Fig.1, consists of two primary stages. Firstly, a world model is learned from demonstrations generated by a TSPWP optimizer. This learning process creates a multi-scale GeneralizeddynamicBayesianNetwork(M-GDBN) that captures the underlying rules governing the optimization objective. Secondly, the learned world model is used as an internal generative model enriched with active states to simulate the environment and plan actions that minimize the agent’s surprise using active inference. The authors state: "We employ an optimizer to solve training examples offline and then used the resulting solutions as demonstrations from which the UAV can learn the world model to understand the environment and implicitly discover the optimizer’s policy." According to the paper: "The UAV equipped with the world model can make real-time autonomous decisions and engage in online planning using active inference." The system model includes a UAV and N hotspot areas randomly distributed on the ground. Each hotspot area has K n (n∈N) ground users (GUs) requesting data service. The UAV aims to find the best route from an initial location to visit each hotspot, provided data service to hotspot users, and then return to the starting location within a specified time period T, divided into M time slots with tduration each. The UAV path at timeslot t, while flying at altitude h u and constant velocity v, can be denoted as q u (t) = [x u (t),y u (t),h u ] and must satisfy: q u (1) = q u (M) and ||q u (t)−q u (t−1)||2 ≤ (vt)2. Furthermore, the flight duration is segmented into a series of E events, where each event is triggered upon the UAV’s arrival at a new hotspot. The designated sequence of hotspots targeted during the flight mission is identified as p u (e) = [C u (e)] where C u ∈ {1,...,N}. The probability of moving towards the next hotspot C u (e+1) from the current hotspot C u (e) can be represented by Pr(C u (e+1)|C u (e),τ Cu(e+1) ). Here, C u ∈ {1,...,N} is the hotspot number. The achievable sum-rate in each hotspot can be calculated as follows: R = (cid:88) Kn B log (cid:0)1+ p k g k,u (t)(cid:1), (1) where B represents the RB’s bandwidth and p k represents the transmitted power of a GU, σ2 = B N is the AWGN power spectral density, and g (t) represents the probabilistic channel gain between UAV and GU calculatedas [Pr µ +Pr µ ]−1. The value of K is determined by the number of LOs = L (cid:0) oS4πfc (cid:1)2, N w Lo S where N f Lo S is the frequency of the carrier wave and c is the speed of light. d k,u is the3D distance between GU and UAV and α is the path loss exponent.### ResultsSimulation results, as shown in Fig.4, demonstrate the effectiveness of the proposed method compared to modified-QL in path planning for several testing scenarios with different numbers of hotspot areas. Specifically, the proposed method produces solutions comparable to those of the TSPWP optimizer. This indicates that the proposed method has successfully captured the optimizer’s strategy in a self-supervised manner and generates shorter paths when compared to modified-QL. The authors state: “Ourmethodsurpassed the modified Q-learning approach, offering faster, more stable, and reliable solutions while demonstrating exceptional generalization proficiency.” The study shows that the proposed method achieves faster adaptation to new situations and better performance than traditional Q-learning, leading to broader generalization. The results highlight the importance of incorporating active inference for robust and adaptable path planning in UAV-aided wireless networks.### DiscussionThe proposed method’s efficacy is evaluated across various testing scenarios, showcasing its ability to learn and mimic the strategies of an offline path planning optimizer. The authors note: “The proposed method employs the concept of active inference and comprises two main stages. Firstly, a world model is learned from demonstrations provided by the TSPWP optimizer. Secondly, the learned world model is used as an internal generative model enriched with active states to simulate the environment and plan actions that minimize the agent’s surprise using active inference.” The study demonstrates that the proposed method surpasses conventional Q-learning approaches in terms of speed, stability, and reliability, while exhibiting exceptional generalization proficiency. The method’s ability to minimize expected surprise – a key element – contributes to its robust performance and adaptability to changing network conditions. The authors further emphasize the importance of incorporating active inference for achieving faster adaptation and better performance compared to traditional reinforcement learning techniques. The study concludes that the proposed method represents a significant advancement in UAV-aided wireless network path planning, offering a more efficient and adaptable solution for real-world applications.