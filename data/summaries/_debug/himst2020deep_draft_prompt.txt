=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Deep Active Inference for Partially Observable MDPs
Citation Key: himst2020deep
Authors: Otto van der Himst, Pablo Lanillos

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2020

Key Terms: action, free, partially, donders, observable, deep, learning, proach, mdps, inference

=== FULL PAPER TEXT ===

Deep Active Inference for Partially Observable
MDPs(cid:63)
Otto van der Himst and Pablo Lanillos
Department of Artificial Intelligence
Donders Insitute for Brain, Cognition and Behaviour
Radboud University
Montessorilaan 3, 6525 HR Nijmegen, the Netherlands
o.vanderhimst@student.ru.nl
p.lanillos@donders.ru.nl
Abstract. Deep active inference has been proposed as a scalable ap-
proach to perception and action that deals with large policy and state
spaces.However,currentmodelsarelimitedtofullyobservabledomains.
In this paper, we describe a deep active inference model that can learn
successful policies directly from high-dimensional sensory inputs. The
deep learning architecture optimizes a variant of the expected free en-
ergyandencodesthecontinuousstaterepresentationbymeansofavari-
ational autoencoder. We show, in the OpenAI benchmark, that our ap-
proach has comparable or better performance than deep Q-learning, a
state-of-the-art deep reinforcement learning algorithm.
Keywords: DeepActiveInference·DeepLearning·POMDP·Control
as Inference
1 Introduction
Deep active inference (dAIF) [1–6] has been proposed as an alternative to Deep
ReinforcementLearning(RL)[7,8]asageneralscalableapproachtoperception,
learning and action. The active inference mathematical framework, originally
proposed by Friston in [9], relies on the assumption that an agent will perceive
and act in an environment such as to minimize its free energy [10]. Under this
perspective,actionisaconsequenceoftop-downproprioceptivepredictionscom-
ingfromhighercorticallevels,i.e.,motorreflexesminimizepredictionerrors[11].
On the one hand, works on dAIF, such as [2,12,13], have focused on scal-
ing the optimization of the Variational Free-Energy bound (VFE), as described
in [9,14], to high-dimensional inputs such as images, modelling the generative
processwithdeeplearningarchitectures.Thistypeofapproachpreservestheop-
timization framework (i.e., dynamic expectation maximization [15]) under the
Laplace approximation by exploiting the forward and backward passes of the
(cid:63) 1st International Workshop on Active inference, European Conference on Machine
Learning (ECML/PCKDD 2020)
0202
peS
8
]GL.sc[
1v22630.9002:viXra
2 O. van der Himst, P. Lanillos
neural network. Alternatively, pure end-to-end solutions to VFE optimization
can be achieved by approximating all the probability density functions with
neural networks [1,3].
On the other hand, Expected Free Energy (EFE) and Generalized Free En-
ergy (GFE) were proposed to extend the one-step ahead implicit action compu-
tation into an explicit policy formulation, where the agent is able to compute
the best action taking into account a time horizon [16]. Initial agent implemen-
tations of these approaches needed the enumeration over every possible policy
projected forward in time up to the time horizon, resulting in significant scaling
limitations. As a solution, deep neural networks were also proposed to approxi-
matethedensitiescomprisingtheagent’sgenerativemodel[1–6],allowingactive
inference to be scaled up to larger and more complex tasks.
However,despitethegeneraltheoreticalformulation,currentstate-of-the-art
dAIF, has only been successfully tested in toy problems with fully observable
state spaces (Markov Decision Processes, MDP). Conversely, Deep Q-learning
(DQN) approaches [7] can deal with high-dimensional inputs such as images.
Here,weproposeadAIFmodel1thatextendstheformulationpresentedin[3]
to tackle problems where the state is not observable2 (i.e. Partially Observable
Markov Decision Processes, POMDP), in particular, the environment state has
tobeinferreddirectlyhigh-dimensionalfromvisualinput.Theagent’sobjective
is to minimize its EFE into the future up to some time horizon T similarly
as a receding horizon controller. We compared the performance of our proposed
dAIFalgorithmintheOpenAICartPole-v1environmentagainstDQN.Weshow
that the proposed approach has comparable or better performance depending
on observability.
2 Deep Active Inference Model
Fig.1: Observations-state neural network architecture. The VAE encodes the
visual features that are relevant to reconstruct the input images. The encoder
network encodes observations to a state representation of the environment. The
decoder reconstructs the input observations from this representation.
1 The code is available on: https://github.com/Grottoh/
Deep-Active-Inference-for-Partially-Observable-MDPs
2 We formulate image-based estimation and control as a POMDP—See [17] for a
discussion
Deep Active Inference for Partially Observable MDPs 3
Wedefinetheactiveinferenceagent’sobjectiveasoptimizingitsvariationalfree
energy (VFE) at time t, which can be expressed as:
−F =D [q(s,a)(cid:107)p(o ,s ,a )] (1)
t KL t 0:t 0:t
=−E [lnp(o |s )]+D [q(s )(cid:107)p(s |s ,a )]
q(st) t t KL t t t−1 t−1
+D [q(a |s (cid:107)p(a |s )] (2)
KL t t t t
Whereo istheobservationattimet,s isthestateoftheenvironment,a isthe
t t t
agent’s action and E is the expectation over the variational density q(s ).
q(st) t
WeapproximatethedensitiesofEq.2withdeepneuralnetworksasproposed
in [1,3,4]. The first term, containing densities q(s ) and p(o |s ) concerns the
t t t
mappingofobservationstostates,andvice-versa.Wecapturethisobjectivewith
a variational autoencoder (VAE). A graphical representation of this part of the
neural network architecture is depicted in Fig. 1 – see the appendix for network
details.
We can use an encoder network q (s |o ) with parameters θ to model
θ t t−3:t
q(s ), and we can use a decoder network p (o |z ) with parameters ϑ to
t ϑ t−3:t t
model p(o |s ). The encoder network encodes high-dimensional input as a dis-
t t
tribution over low-dimensional latent states, returning the sufficient statistics of
a multivariate Gaussian, i.e. the mean s and variance s . The decoder net-
µ Σ
workconsequentlyreconstructstheoriginalinputfromreparametrizedsufficient
statistics z. The distribution over latent states can be used as a model of the
environmentincasethetruestateofanenvironmentisinaccessibletotheagent
(i.e. in a POMDP).
The second term of Eq. 2 can be interpreted as state prediction error, which
is expressed as the Kullback-Leibler (KL) divergence between the state derived
at time t and the state that was predicted for time t at the previous time
point. In order to compute this term the agent must, in addition to the already
addressedq(s ),haveatransitionmodelp(s |s ,a ),whichistheprobability
t t t−1 t−1
of being in a state given the previous state and action. We compute the MAP
estimatewithafeedforwardnetworksˆ =f (s ,a ).Tocomputethestate
t φ µ,t−1 t−1
prediction error, instead of using the KL-divergence over the densities, we use
the Mean-Squared-Error (MSE) between the encoded mean state s and the
µ
predicted state sˆreturned by f
φ
The third and final term contains the last two unaddressed densities q(a |s )
t t
and p(a |s ). We model variational density q(a |s ) using a feedforward neu-
t t t t
ralnetworkq (a |s ,s )parameterized by ξ,whichreturnsa distributionover
ξ t µ,t Σ
actionsgivenamultivariateGaussianoverstates.Finally,wemodelactioncondi-
tionedbythestateorpolicyp(a |s ).Accordingtotheactiveinferenceliterature,
t t
if an agent that minimizes the free energy does not have the prior belief that
it selects policies that minimize its (expected) free energy (EFE), it would infer
policiesthatdonotminimizeitsfreeenergy[16].Therefore,wecanassumethat
our agent expects to act as to minimize its EFE into the future. The EFE of a
policy π after time t onwards can be expressed as:
4 O. van der Himst, P. Lanillos
Fig.2: Computing the gradient of the value network with the aid of a boot-
strapped EFE estimate.
(cid:88)
G = G
π π,t
τ>t
(3)
G =−lnp(o )+D [q(s |π)(cid:107)q(s |o )]
π,τ τ KL τ τ τ
(cid:124) (cid:123)(cid:122) (cid:125)
−rτ
Note that the EFE has been transformed into a RL instance by substituting
the negative log-likelihood of an observation −lnp(o ) (i.e. surprise) by the
τ
reward r [3,18]. Since under this formulation minimizing one’s EFE involves
τ
computing one’s EFE for each possible policy π for potentially infinite time
points τ, a tractable way to compute G is required. Here we estimate G
π π
via bootstrapping, as proposed in [3]. To this end the agent is equipped with
an EFE-value (feedforward) network f (s ,s ) with parameters ψ, which
ψ µ,t Σ,t
returnsanestimateG˜ thatspecifiesanestimatedEFEforeachpossibleaction.
t
This network is trained with the aid of a bootstrapped EFE estimate Gˆ , which
t
consists of the free energy for the current time step, and a β ∈(0,1] discounted
value net approximation of the free energy expected under q(a|s) for the next
time step:
Gˆ =−r +D [q(s )(cid:107)q(s |o )]+βE G˜ (4)
t t KL t t t q(at+1|st+1) t
In this form the parameters of f (s ,s ) can be optimized through gradient
ψ µ,t Σ,t
descent on (see Fig. 2):
L =MSE(G˜ ,Gˆ ) (5)
t t t
Deep Active Inference for Partially Observable MDPs 5
The distribution over actions can then at last be modelled as a precision-
weighted Boltzmann distribution over our EFEs estimate [3,16]:
p(a |s )=σ(−γG˜ ) (6)
t t t
Finally, Eq. 2 is computed with the neural network density approximations as –
See Fig. 3.
−F =−E [lnp (o |z )]
t qθ(st|ot−3:t) ϑ t−3:t t
+MSE(s ,f (s ,a ))
µ,t φ µ,t−1 t−1
+D [q (a |s ,s )(cid:107)σ(−γf (s ,s ))] (7)
KL ξ t µ,t Σ,t ψ µ,t Σ,t
Where s and s are encoded by q (s |o ).
µ,t Σ,t θ t t−3:t
Fig.3: Variational Free Energy computation using the approximated densi-
ties. The VAE encodes high-dimensional input as a latent state space, which
is used as input to the other networks. Note that the third term of Eq. 7
(D [q (a |s ,s )(cid:107)σ(−γf (s ,s ))]) has been split into an energy and
KL ξ t µ,t Σ,t ψ µ,t Σ,t
an entropy term3.
3 Experimental Setup
ToevaluatetheproposedalgorithmweusedtheOpenAIGym’sCartPole-v1,as
depicted in Fig. 4. In the CartPole-v1 environment, a pole is attached to a cart
3 Any KL divergence can be split into an energy term and an entropy term:
(cid:88) P(x)
D [P(cid:107)Q]= P(x)ln
KL Q(x)
x∈X
(cid:88) (cid:88)
=− P(x)lnQ(x)− P(x)lnP(x)
x∈X x∈X
(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
Energy Entropy
6 O. van der Himst, P. Lanillos
Fig.4: Cartpole-v1 benchmark (left) and cropped visual input used (right).
thatmovesalongatrack.Thepoleisinitiallyupright,andtheagent’sobjective
is to keep the pole from tilting too far to one side or the other by increasing or
decreasingthecart’svelocity.Additionally,thepositionofthecartmustremain
within certain bound. An episode of the task terminates when the agent fails
either of these objectives, or when it has survived for 500 time steps. Each time
step the agent receives a reward of 1.
The CartPole state consists of four continuous values: the cart position, the
cart velocity, the pole angle and the velocity of the pole at its tip. Each run the
statevaluesareinitializedatrandomwithinasmallmargintoensurevariability
between runs. The agent can exact influence on the next state through two
discrete actions, by pushing the cart to the left, or by pushing it to the right.
Testswereconductedintwoscenarios:1)anMDPscenarioinwhichtheagent
has direct access to the state of the environment, and 2) a POMDP scenario in
which the agent does not have direct access the environment state, and instead
receives pixel value from which it must derive meaningful hidden states. By
default, rendering the CartPole-v1 environment returns a 3×400×600 (color,
height,width)arrayofpixelvalues.InourexperimentsweprovidethePOMDP
agents with a downscaled and cropped image. There the agents receive a 3×
37×85 pixel value array in which the cart is centered until it comes near the
left or right border.
4 Results
The performance of our dAIF agents was compared against DQN agents for the
MDP and the POMDP scenarios, and against an agent that selects it actions at
random. Each agent was equipped with a memory buffer and a target network
[19]. The memory buffer stores transitions from which the agent can sample
randombatchesonwhichtoperformbatchgradientdescent.Thetargetnetwork
is a copy of the value network of which the weights are not updated directly
through gradient descent, butare instead updatedperiodically withthe weights
ofthevaluenetwork.InbetweenupdatesthisprovidestheagentwithfixedEFE-
value or Q-value targets, such that the value network does not have to chase a
constantly moving objective.
The VAE of the POMDP dAIF agent is pre-trained to deconstruct input
images into a distribution over latent states and to subsequently reconstruct
them as accurately as possible.
Deep Active Inference for Partially Observable MDPs 7
Fig.5: Average reward comparison for the CartPole-v1 problem.
Fig. 5 shows the mean and standard deviation of the moving average reward
(MAR) over all runs for the five algorithms at each episode. Each agent per-
formed 10 runs of 5000 episodes. The moving average reward for an episode e is
calculated using an smoothing average:
MAR =0.1CR +0.9MAR (8)
e e e−1
Where CR is the cumulative reward of episode e and MAR is the MAR of
e e−1
the previous episode.
The dAIF MDP agent results closely resemble those presented in [3] and
outperformstheDQNMDPagentbyasignificantmargin.Further,thestandard
deviationshadingsshowthatthedAIFMDPisagentismoreconsistentbetween
runs than the DQN agent. The POMDP agents are both demonstrated to be
capable of learning successful policies, attaining comparable performance.
We have exploited probabilistic model based control through a VAE that
encodes the state. On the one hand, this allows the tracking of an internal
state which can be used for a range of purposes, such the planning of rewarding
policies and the forming of expectations about the future. On the other hand,
it makes every part of the algorithm dependent on the proper encoding of the
latent space, conversely to the DQN that did not require a state representation
to achieve the same performance. However, we expect our approach to improve
relative to DQN in more complex environments where the world state encoding
can play a more relevant role.
5 Conclusion
WedescribedadAIFmodelthattacklespartiallyobservablestateproblems,i.e.,
it learns the policy from high-dimensional inputs, such as images. Results show
thatintheMDPcasethedAIFagentoutperformstheDQNagent,andperforms
8 O. van der Himst, P. Lanillos
more consistently between runs. Both agents were also shown to be capable of
learning(less)successfulpoliciesinthePOMDPversion,wheretheperformance
between dAIF and DQN models was found to be comparable. Further work will
focus on validating the model on a broader range of more complex problems.
Appendix
Deep Q Agent MDP
Networks & params. Description
N Number of states.
s
N Number of actions.
a
Q-value network Fully connected network using an Adam optimizer with
a learning rate of 10−3, of the form: N ×64×N .
s a
γ Discount factor set to 0.98
Memory size Maximum amount of transition that can be stored in
the memory buffer: 65,536
Mini-batch size 32
Target network freeze The amount of time steps the target network’s parame-
period ters are frozen, until they are updated with the param-
eters of the value network: 25
Deep Q Agent POMDP
Networks & params. Description
N Number of actions.
a
Q-value network Consists of three 3D convolutional layers (each followed
by batch normalization and a rectified linear unit) with
5×5×1kernelsand2×2×1strideswithrespectively3,
16 and 32 input channels, ending with 32 output chan-
nels. The output is fed to a 2048×1024 fully connected
layer which leads to a 1024×N fully connected layer.
a
Uses an Adam optimizer with the learning rate set to
10−5.
γ Discount factor set to 0.99
Memory size Maximum amount of transition that can be stored in
the memory buffer: 65,536
Mini-batch size 32
Target network freeze The amount of time steps the target network’s parame-
period ters are frozen, until they are updated with the param-
eters of the value network: 25
Deep Active Inference for Partially Observable MDPs 9
Deep Active Inference Agent MDP
Networks & params. Description
N Number of states.
s
N Number of actions.
a
Transition network Fully connected network using an Adam optimizer with
a learning rate of 10−3, of the form: (N +1)×64×N .
s s
Policy network Fully connected network using an Adam optimizer with
a learning rate of 10−3, of the form: N ×64×N , a
s a
softmax function is applied to the output.
EFE-value network Fully connected network using an Adam optimizer with
a learning rate of 10−4, of the form: N ×64×N .
s a
γ Precision parameter set to 1.0
β Discount factor set to 0.99
Memory size Maximum amount of transition that can be stored in
the memory buffer: 65,536
Mini-batch size 32
Target network freeze The amount of time steps the target network’s parame-
period ters are frozen, until they are updated with the param-
eters of the value network: 25
10 O. van der Himst, P. Lanillos
Deep Active Inference Agent POMDP
Networks & params. Description
N Size of the VAE latent space, here set to 32.
l
N Number of actions.
a
Encoder-network Consists of three 3D convolutional layers (each followed
q (s |o ) by batch normalization and a rectified linear unit) with
θ t t−3:t
5×5×1kernelsand2×2×1strideswithrespectively3,
16 and 32 input channels, ending with 32 output chan-
nels. The output is fed to a 2048×1024 fully connected
layer which splits to two additional 1024×N fully con-
l
nectedlayers.UsesanAdamoptimizerwiththelearning
rate set to 10−5.
Decoder-network Consists of a N ×1024 fully connected layer leading to
l
p (o |z ) a 1024×2048 fully connected layer leading to three 3D
ϑ t−3:t t
transposed convolutional layers (each followed by batch
normalization and a rectified linear unit) with 5×5×1
kernelsand2×2×1strideswithrespectively32,16and
3 input channels, ending with 3 output channels. Uses
an Adam optimizer with the learning rate set to 10−5.
Transition-network Fully connected network using an Adam optimizer with
f (s ,a ) alearningrateof10−3,oftheform:(2N +1)×64×N .
φ µ,t t l l
Policy-network Fully connected network using an Adam optimizer with
q (s ,s ) a learning rate of 10−3, of the form: 2N ×64×N , a
ξ µ,t Σ,t l a
softmax function is applied to the output.
EFE-value-network Fully connected network using an Adam optimizer with
f (s ,s ) a learning rate of 10−4, of the form: 2N ×64×N .
ψ µ,t Σ,t l a
γ Precision parameter set to 12.0
β Discount factor set to 0.99
α A constant that is multiplied with the VAE loss to take
it to the same scale as the rest of the VFE terms, set to
4×10−5
Memory size Maximum amount of transition that can be stored in
the memory buffer: 65,536
Mini-batch size 32
Target network freeze The amount of time steps the target network’s parame-
period ters are frozen, until they are updated with the param-
eters of the value network: 25
Deep Active Inference for Partially Observable MDPs 11
References
1. Ueltzh¨offer,K.:Deepactiveinference.BiologicalCybernetics112(6),547–573(Oct
2018). https://doi.org/10.1007/s00422-018-0785-7
2. Sancaktar, C., Lanillos, P.: End-to-end pixel-based deep active inference for body
perception and action. arXiv preprint arXiv:2001.05847 (2019)
3. Millidge, B.: Deep active inference as variational policy gradi-
ents. Journal of Mathematical Psychology 96, 102348 (2020).
https://doi.org/10.1016/j.jmp.2020.102348
4. Tschantz, A., Baltieri, M., Seth, A.K., Buckley, C.L.: Scaling active inference.
arXiv Prepr. arXiv:1911.10601v1 (2019)
5. C¸atal,O.,Wauthier,S.,Verbelen,T.,Boom,C.D.,Dhoedt,B.:Deepactiveinfer-
ence for autonomous robot navigation. arXiv Prepr. arXiv:2003.03220v1 (2020)
6. Fountas,Z.,Sajid,N.,Mediano,P.A.M.,Friston,K.:Deepactiveinferenceagents
using monte-carlo methods. arXiv Prepr. arXiv:2006.04176v1 (2020)
7. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
D., Riedmiller, M.: Playing atari with deep reinforcement learning. arXiv Prepr.
arXiv:1312.5602v1 (2013)
8. Arulkumaran,K.,Deisenroth,M.P.,Brundage,M.,Bharath,A.A.:Deepreinforce-
ment learning: A brief survey. IEEE Signal Processing Magazine 34(6), 26–38
(2017)
9. Friston, K.J., Daunizeau, J., Kilner, J., Kiebel, S.J.: Action and be-
havior: a free-energy formulation. Biol Cybern 102, 227–260 (2010).
https://doi.org/https://doi.org/10.1007/s00422-010-0364-z
10. Friston,K.J.:Thefree-energyprinciple:aunifiedbraintheory?Nature11,127–138
(2010). https://doi.org/https://doi.org/10.1038/nrn2787
11. Adams, R.A., Shipp, S., Friston, K.J.: Predictions not commands: active in-
ference in the motor system. Brain Struct Funct. 218(3), 611–643 (2012).
https://doi.org/doi: 10.1007/s00429-012-0475-5
12. Lanillos, P., Pages, J., Cheng, G.: Robot self/other distinction: active inference
meets neural networks learning in a mirror. In: Proceedings of the 24th Eu-
ropean Conference on Artificial Intelligence (ECAI). pp. 2410 – 2416 (2020).
https://doi.org/10.3233/FAIA200372
13. Rood,T.,vanGerven,M.,Lanillos,P.:Adeepactiveinferencemodeloftherubber-
hand illusion. arXiv Prepr. arXiv:2008.07408 (2020)
14. Oliver, G., Lanillos, P., Cheng, G.: Active inference body perception and action
for humanoid robots. arXiv Prepr. arXiv:1906.03022v3 (2019)
15. Friston, K., Trujillo-Barreto, N., Daunizeau, J.: Dem: a variational treat-
ment of dynamic systems. NeuroImage 41(3), 849—885 (July 2008).
https://doi.org/https://doi.org/10.1016/j.neuroimage.2008.02.054
16. Parr, T., Friston, K.J.: Generalised free energy and active inference. Biol Cybern
113, 495–513 (2019). https://doi.org/10.1007/s00422-019-00805-w
17. Hausknecht,M.,Stone,P.:Deeprecurrentq-learningforpartiallyobservablemdps.
In: AAAI Fall Symposium on Sequential Decision Making for Intelligent Agents
(AAAI-SDMIA15) (November 2015)
18. Friston, K.J., Samothrakis, S., Montague, R.: Active inference and agency:
optimal control without cost functions. Biol Cybern 106, 523–541 (2012).
https://doi.org/10.1007/s00422-012-0512-8
19. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G.,
Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G., Petersen, S., Sadik,
12 O. van der Himst, P. Lanillos
C.B.A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., Hassabis,
D.:Human-levelcontrolthroughdeepreinforcementlearning.Nature518,529–533

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Deep Active Inference for Partially Observable MDPs"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
