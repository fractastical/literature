=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: The Missing Reward: Active Inference in the Era of Experience
Citation Key: wen2025missing
Authors: Bo Wen

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Abstract: ThispaperarguesthatActiveInference(AIF)providesacrucialfoundationfor
developingautonomousAIagentscapableoflearningfromexperiencewithout
continuoushumanrewardengineering.AsAIsystemsbegintoexhausthigh-quality
trainingdataandrelyonincreasinglylargehumanworkforcesforrewarddesign,
the current paradigm faces significant scalability challenges that could impede
progress toward genuinely autonomous intelligence. The proposal for an “Era
ofExperience,”whereagentslearnfromself-generateddata,isapromisingst...

Key Terms: bowen, missing, ibmt, bwen, experience, inference, yorktownheights, active, watsonresearchcenter, reward

=== FULL PAPER TEXT ===

The Missing Reward: Active Inference in the Era of
Experience
BoWen
IBMT.J.WatsonResearchCenter
YorktownHeights,NY
bwen@us.ibm.com
Abstract
ThispaperarguesthatActiveInference(AIF)providesacrucialfoundationfor
developingautonomousAIagentscapableoflearningfromexperiencewithout
continuoushumanrewardengineering.AsAIsystemsbegintoexhausthigh-quality
trainingdataandrelyonincreasinglylargehumanworkforcesforrewarddesign,
the current paradigm faces significant scalability challenges that could impede
progress toward genuinely autonomous intelligence. The proposal for an “Era
ofExperience,”whereagentslearnfromself-generateddata,isapromisingstep
forward. However, thisvisionstilldependsonextensivehumanengineeringof
rewardfunctions,effectivelyshiftingthebottleneckfromdatacurationtoreward
curation. This highlights what we identify as the grounded-agency gap: the
inabilityofcontemporaryAIsystemstoautonomouslyformulate,adapt,andpursue
objectivesinresponsetochangingcircumstances. WeproposethatAIFcanbridge
thisgapbyreplacingexternalrewardsignalswithanintrinsicdrivetominimizefree
energy,allowingagentstonaturallybalanceexplorationandexploitationthrougha
unifiedBayesianobjective. ByintegratingLargeLanguageModelsasgenerative
worldmodelswithAIF’sprincipleddecision-makingframework,wecancreate
agentsthatlearnefficientlyfromexperiencewhileremainingalignedwithhuman
values. ThissynthesisoffersacompellingpathtowardAIsystemsthatcandevelop
autonomouslywhileadheringtobothcomputationalandphysicalconstraints.
1 Introduction
InhisNeurIPS2024awardspeech, IlyaSutskeverdeclaredthat“pre-trainingasweknowitwill
unquestionablyend”[1]. DarioAmodei,inarecentpodcast[2],estimateda“10%chancethatthe
scalingofAIsystemscouldstagnateduetoinsufficientdata”. Severalempiricalstudies[3,4]support
similarconclusionsandprojectthatdemandfortrainingdatawillsoonexceedtheavailablepublic
supply. Ashigh-qualityhuman-generateddatabecomesincreasinglyscarce,newapproachesmust
emergetosustainAI’sadvancement.
Addressing this looming data shortage, the recent preprint “Welcome to the Era of Experience”
(abbreviatedas“EoE”inthefollowing)[5]bySilverandSuttonproposesaparadigmshiftfrom
static,human-generateddatasetstodynamic,agent-generatedexperiences. WeinterpretthisasAI
agentslearningcontinuouslythroughtheirownenvironmentalinteractions,effectivelycreatinga
self-sustainingcyclewheretheseinteractionsgeneratetheverydataneededforongoingtrainingand
improvement. Thisclosed-loopsystemnotonlycircumventsconcernsaboutexhaustingtrainingdata
suppliesbutpotentiallyunlockscapabilitiesbeyondwhathuman-curateddataalonecouldachieve.
EoEextendsfarbeyondconventionalreinforcementlearning,outliningseveralessentialcomponents
forthisexperientialparadigm. SilverandSuttonenvisionagentsthatinhabitcontinuousstreamsof
experienceratherthanisolatedinteractionepisodes,withactionsandobservationsrichlygroundedin
Preprint.Underreview.
5202
guA
7
]IA.sc[
1v91650.8052:viXra
theirenvironments. Theseagentswouldderiverewardsfromreal-worldsignalsinsteadofhuman
judgments, and employ planning mechanisms that reason directly about experience rather than
abstractconcepts.
However,theirblueprintleavesopenapractical,andpotentiallydecisive,question. Thechallengeof
determiningwhowillengineertherewardfunctionsthatturnrawsignalsintousefulguidanceremains
unresolved. Intheirownexamples,differentrewardfunctionsmustbetunedforeachhigh-leveluser
goal. Atthescaleimpliedbylifelong, open-endedagents, thismerelyshiftsthebottleneckfrom
curating training datato curating rewardfunctions, reintroducing thekind of domain knowledge
engineeringthatSutton’sbitterlessonwarnsusabout[6].
WearguethatActiveInference(AIF)providesthemissingfoundationforautonomousAIagents
that can learn from experience without constant human reward engineering. By replacing
externalrewardengineeringwithintrinsicfreeenergyminimization,AIFagentsnaturallybalance
explorationandexploitationthroughaunifiedBayesianobjectivethatemergesfromtheagent’sown
worldmodelandpreferences. Thisapproachoffersamoredirectpathtotheexperientiallearning
paradigmenvisionedbySilverandSutton,withadvantagesinscalability,safety,andefficiencythat
weexplorethroughoutthispaper.
Overview of Core Arguments This paper identifies a fundamental “grounded-agency gap” in
contemporaryAI—theinabilitytoautonomouslyform,evaluate,andadaptobjectives—thatpersists
eveninproposedexperience-drivenparadigms. WearguethatActiveInferenceprovidesacompelling
theoreticalfoundationforthe“EraofExperience”byofferingintrinsicmotivationthroughfreeenergy
minimization,whichcaneliminatetheneedforcontinuousrewardengineering. Buildingonthis,
weproposeanovelintegrationwhereLargeLanguageModelsserveaslearnedgenerativeworld
modelswithinanActiveInferencedecision-makingframework,combiningthescalabilityofmodern
deeplearningwiththetheoreticalrigoroftheFreeEnergyPrinciple. Finally,wesituatethisproposal
withinthephysicalconstraintsofAIdevelopment,arguingthattheenergyefficiencyoffreeenergy
minimizationisnotjustcomputationallyadvantageousbutmaybeathermodynamicnecessityfor
sustainableAIprogress.
2 TwoDarkCloudsoverContemporaryAI
In1900,LordKelvinobservedthatphysicsseemednearlycompletesavefor"twosmallclouds"on
thehorizon. Theseclouds(blackbodyradiationandtheMichelson-Morleyexperiment)ultimately
revolutionized our understanding through quantum mechanics and relativity. Today’s AI faces
analogous clouds that signal not minor adjustments but fundamental limitations in our current
paradigm.
Cloud I: Resource Saturation—Physical Limits of Scale. The prevailing wisdom in AI has
been simple: more data and more compute yield better models. This scaling hypothesis drove
remarkableprogressfromearlylanguagemodelstotoday’strillion-parametersystems. However,we
arereachinghardphysicallimitsonmultiplefrontssimultaneously. Onthedataside,high-quality
humantextaccumulatedovercenturiesisbeingrapidlydepleted,withqualityEnglishtextprojected
forexhaustionwithinadecadewhilespecializeddomainsalreadyfaceseverescarcity. Webcrawls
yielddiminishingreturns: morespam,moresyntheticcontent,lessgenuinehumanknowledge. On
thecomputeside,trainingcostsrisesuper-linearlywithmodelsizewhileuniversitiesandsmaller
labsarepricedoutoffrontierresearch[7],creatingunprecedentedconcentrationofpower. Thisis
notmerelyaneconomicproblem;itisathermodynamicone. Eachdoublingofmodelcapability
demands exponentially more energy, while performance gains follow a logarithmic curve. This
createsafundamentalmismatchthatnoamountofengineeringcanovercome.
CloudII:ExternalizedCognition—HiddenHumanDependencies. Thesecondconstraintis
more subtle but equally limiting: today’s “autonomous” AI systems depend on vast networks of
humancognitionoperatingbehindthescenes. Thisrepresentsafundamentalarchitecturallimita-
tionwherejudgment,adaptation,anderrorcorrectionareoutsourcedtohumanworkers. Content
moderatorssufferpsychologicaltraumafromfilteringtoxicoutputs,whileannotatoragreementrates
revealinstabilityinthefoundationof“groundtruth.” Morecritically,thisdependencyscalesinversely
withcapability: asmodelsgrowmoresophisticated,theyrequiremorenuancedhumanjudgmentfor
2
alignment,creatinganever-expandingneedforspecializedexpertise. TherecentDeepMindstudy
onpreferencedrift[8]exposesthefundamentalchallenge: humanvaluesarenotstatictargetsbut
dynamic,context-dependentprocessesthatnoamountoflabelingcanfullycapture. Theresultisa
Sisypheancycle: engineerscontinuouslypatchrewardfunctions,annotatorsendlesslyrefineprefer-
encedatasets,andsafetyteamsperpetuallychaseemergingfailuremodes. Thisisnotintelligence;it
isanelaboratepuppetshowwherehumansoperatethestringsfrombehindanalgorithmiccurtain,
justasFei-FeiLisaid,“There’snothingartificialaboutartificialintelligence.” [9]
Table1: EvidenceofStructuralBottlenecksinContemporaryAI
Bottleneck ResourceSaturation ExternalizedCognition SystemicImpact
Data High-quality text ex- Human-labeled data Progress tied to finite
Scarcity hausted within 10 <70% inter-annotator historicalaccumulation;
years [3]. Diminishing agreement [10]. Quality innovation stalls with-
returns at trillion-token ceiling from noisy judg- outnewsources
scales[4]. ments.
Compute / SOTA training: 1000s RLHF workforce scales Only tech giants com-
Energy GPUs, 4-5 digit tonne withmodelcapability[14]. pete [15]; universities
CO [11]. Google: 48% Human oversight costs priced out [16]. Envi-
2
data center emission compoundwithAIsophis- ronmentalcoststhreaten
jump[12,13]. tication. sociallicense.
Human OpenAI spends $100M+ Invisible supply chain of “Automation” paradox:
Labor annually on RLHF con- labelers,moderators. Psy- AIcreatesmorehuman
tractors. Workersearnbe- chological trauma from work. Undermines au-
lowlivingwages[17]. disturbingcontent[14]. tonomous intelligence
narrative.
Economic <10% AI startups prof- Cultural/linguistic bias: Social backlash from
Friction itable (2024). Job dis- AI works best for domi- displacement. Talent
placementinautomatable nant languages. Global drain from ethics con-
sectors[18]. inequitiesreinforced. cerns. Regulatory mo-
mentumbuilding.
Thesefoursymptoms(dataexhaustion,computebarriers,labordependencies,andeconomicfriction)
arenotindependentfailuresbutmanifestationsofthetwofundamentalclouds. Resourcesaturation
(Cloud I) drives the data and compute crises, while externalized cognition (Cloud II) creates the
human labor and economic imbalances. Together, they reveal that scaling alone cannot produce
genuineintelligence.
3 TheGrounded-AgencyGap
Thesesystem-levelconstraintsrevealadeeperproblem: mostcontemporaryAIsystemslackthe
abilitytoautonomouslycreate,update,andpursueobjectivesascircumstanceschange-adeficiency
wetermthegrounded-agencygap. Toclosethisgap,anagentmust(i)perceiveitssituation,(ii)
reviseitsgoalsinresponsetoevolvinguserneedsandenvironments,and(iii)operatesafelywithout
requiring continuous human intervention. In the following discussion, we demonstrate why two
prevalentapproaches-rewardengineeringandself-play-failtoachievetrueagency.
RewardEngineeringisnotGroundedAgency. The“reward-is-enough”idea[19]suggeststhat
a single, well-designed reward signal could produce all intelligent behaviors if maximized in a
sufficientlycomplexenvironment. Recentsuccessesinroboticresearchsupportedthisinsight: RT-2
handleskitchentoolsfromvisionalone[20],RoboCatshowsimpressiveadaptabilitybyfine-tuning
itself to dozens of new tasks [21], and PaLM-E combines language with physical actions [22].
Whileimpressive,thesesystemsoperateinhighlycontrolledexperimentalenvironmentswithclear,
measurablerewardsperfectlymatchedtospecifictasks. Thiscontrolledsettingmasksseveralcritical
limitationswhenconsideringtruegroundedagencyinopen-worldscenarios: (1)Artificialfeedback
systems. Conceptslike“tastyfood”or“safemove”arenotdirectlyobservable. Currentsystems
3
needspecialsensors,trainingresets,orexpensivesimulatorslikeHabitat-2.0[23]andiGibson[24]to
provideconstantrewardfeedback.Thisinfrastructuredoesnotexistintherealworld.(2)Short-term,
fixedgoals. Thesedemonstrationsshowrobotsperformingbrieftaskslikepouringdrinksorfolding
clothes. Theydonothandlelong-termchangeslikeshiftingfoodpreferencesorunexpectedsafety
issuesthatariseinrealhomes. Thesystemscannotadaptovertimeorlearnfrompastexperience
whengoalschange. (3)Hiddenguard-railsandimplicitconstraints. Topreventfailures,engineers
addsafetyfeaturesthatarenotpartoftheofficialreward. Theseincludecollisionavoidance,penalty
systems,orhumanmonitors. Thesefeaturesshapetherobot’sbehaviorbutarenotpartofitsexplicit
objectives.
Consideratoyexamplethatillustrateswhythisapproachfails. Anautonomouslabassistantistasked
withconductingafluorescentenzymeassayandinstructedto“completetheassayefficiently.” When
theassistantobservesunexpectedacidification(yellowindicator),itsprimaryrewardsignalpushes
towardrapidcompletion. Withoutexplicitsafetyconstraintsprogrammedintotherewardfunction,
theagentmightquicklyaddconcentratedbasetoneutralizetheacid,potentiallycausingdangerous
splashingofcorrosivereagentsorthermaldamagetoheat-sensitiveenzymes. Theoriginalefficiency
commandremainsunchangedevenasnewsafetyhazardsemerge.
Engineersmightrespondbyaddingpenaltytermsfor“chemicalspills”or“temperatureviolations,”
butthisrequiresanticipatingeverypossiblefailuremodeandmanuallyencodingsafetypreferences—a
combinatoriallyexplosiveengineeringchallenge. Supportersmightarguethesystemwouldlearn
saferprotocolsafterexperiencingnegativeoutcomes,butthisapproachnecessitateseitherdangerous
real-worldexperimentationorexpensivehigh-fidelitysimulationsthatmaynotcaptureallsafety-
criticaldynamics. Eachnewdomainortaskvariationdemandsfreshroundsofrewardengineering,
creatingtheverylabor-intensivebottleneckthatautonomoussystemsshouldeliminate.
UnderActiveInference,safetypreferenceslike“avoidspillingcorrosivereagents”and“maintain
enzymeintegrity”sitdirectlyintheCmatrixasintrinsicpreferencesratherthanexternalconstraints.
Theagentnaturallyselectspoliciesthatminimizeexpectedfreeenergybybothresolvinguncertainty
(measuringpHfirst)andsatisfyingsafetypreferences(carefultitration)withoutrequiringexplicit
rewardengineeringforeachpotentialhazard.
RecentstudiesfromDeepMindshowthatthisproblemisrealratherthanmerelytheoretical:evenwith
sophisticatedpreferencelearningtechniqueslikeNon-StationaryDirectPreferenceOptimization,user
preferencesexhibitsignificanttemporaldrift,causingstandardalgorithmstobecomemisalignedover
time[8]. Thelabor-intensivenatureofcollectingandcuratinghumanpreferencesforRLHF,withits
inherentinconsistenciesandcosts(asdiscussedinSection2),furtherhighlightstheunsustainability
ofcontinuousexternalrewardadjustment. Take-away: Currentrewardengineeringpracticeincurs
substantialcost(instrumentation,supervision,post-hocpatching)butrarelyinstillsrobust,general-
purposecapability. TheseapproachesfixproblemsaftertheyhappeninsteadofgivingAItheability
to adapt safely to new situations. This creates a paradox: we want autonomous AI, but we keep
needinghumanstoconstantlyadjustitsrewards,justwithdifferentjobtitleslike“rewardengineer”
or“ground-signalcurator.” ThecostskeepgrowingasAIgetsmorepowerful.
WhySelf-PlayProducesOnlySimulatedAgency. Thechallengesofopen-worldrewardengi-
neering stand in stark contrast to the domain-specific successes of systems like AlphaZero [25]
andAlphaProof[26]. Thesesystemsachievesuperhumanperformanceincomplexgames(chess,
shogi,andGo)andmathematicaltheoremprovingthroughself-play,requiringminimalhumaninput
beyond the fundamental rules and clear success criteria (win conditions or valid proofs). They
demonstrateremarkableabilitytodevelopstrategiesanddiscoverknowledgethatexceedshuman
expertise,apparentlywithoutcontinuousexternalguidanceorrewardshaping.
However,theseaccomplishmentsrepresentwhatwecallsimulatedagency. Theyfunctionwithin
precisely defined, closed environments where judgment is externalized into fixed rules, explicit
winconditions,andstableobjectives. Theenvironmentservesasaperfect,unambiguoussuccess
oracle. Whenmovingfromthesestructureddomainstothecomplexitiesofopen-worldroboticsor
general-purposeAIassistants,thesefoundationalassumptionsbreakdown: (1)Fuzzy,multifaceted
objectives: Real-worldtasksrarelyhaveasingle,easilyquantifiablewincondition. Goalsareoften
ill-defined, composed of multiple potentially conflicting sub-goals, and subject to interpretation
(e.g.,“beahelpfulassistant,”“ensureuserwell-being”). (2)Preferencesdriftandevolve: Human
preferenceschangeovertimeduetolearning,shiftingcircumstances,ordevelopingtastes. Agents
4
relyingonfixed,externallydefinedrewardscannotaccommodatethesechanges. (3)Non-stationary
environments:Therealworldisconstantlychanginginunpredictableways.Unlikegameboardswith
fixedrules,AIagentsmusthandlenovelsituations,newentities,andevolvingcausalrelationships.(4)
Ambiguous,sparsefeedback:Clearrewardsignalsarerare. Environmentaloruserfeedbackisoften
delayed,noisy,incomplete,sometimesevencontradictory. Allrequiressubstantialinterpretation.
Insuchopen-endedsettings,therewardfunctionmustbelearned,inferred,oradaptedratherthan
simplyprovided. Whileself-playexcelswhenevaluativecriteriaarefixedandexternallysupplied,it
lacksanyintrinsicmechanismforagentstoderiveorupdatethesecriteriafromitsexperiencesinan
ambiguous,evolvingworld. Therefore,forgeneralAItoachievetheself-improvementandemergent
capabilitiesdemonstratedbyAlphaZero,itrequiresmorethanself-play. Itneedsgenuineagency:
the capacity to form, evaluate, and refine objectives and understanding based on its interactions
withcomplex, underspecifiedenvironments. ThefundamentalchallengeinvolvesenablingAIto
interprethumanguidanceandenvironmentalsignalsasflexiblepreferencesandevidenceratherthan
rigidrewards,thenautonomouslyadaptitscapabilitiestosatisfytheseevolvingpreferencesinnovel
situations.
BridgetoActiveInference. SilverandSutton’sproposalofan“adaptivereward,basedongrounded
signals,guidedbyuser”sharesaconceptualsimilaritywithourargumentforgroundedagency. Both
approachesrecognizetheneedforAIagentstodynamicallyadjusttheirobjectivesbasedonreal-
worldinteractionsratherthanrelyingonstatic,pre-definedrewards. However,thecriticaldistinction
liesinthelocusofcontrol: whileSutton’sframeworkstillrequiresexternalhumanguidancetoadapt
rewardfunctions,ourapproachseekstointernalizethisprocessentirely.
Thismirrorshumandevelopment: justaschildren“growup”bygeneralizingfromlessonstobroader
principles, we need AI systems to develop intrinsic mechanisms for self-directed learning. The
key insight is that we need a universal learning signal that enables agents to abstract meta-level
knowledgefromexperience,withautonomyastheimplicitgoal. Thissignalmustbe: intrinsically
computable(derivablefromtheagent’ssensorystream),domain-agnostic(applicableacrossdiverse
environments),preference-sensitive(responsivetohumanvalueswithoutrigidspecification),and
exploration-aware(naturallybalancingknowledge-seekingandgoal-directedbehavior).
This is precisely where the Free Energy Principle from Active Inference provides a principled
answer. Ratherthanengineeringcountlessrewardfunctions,AIFoffersasingle,universalobjective
(minimizing expected free energy) that naturally gives rise to intelligent behavior. The agent’s
exploration, learning, and goal-pursuit emerge from this unified principle, making the “Era of
Experience”boththeoreticallygroundedandpracticallyachievable.
4 ActiveInferencewithLanguageModels: APathtoGroundedAgency
Section2highlightedacriticalgrounded-agencygap: contemporaryAI,evenintheenvisioned“Era
ofExperience,”lacksanintrinsicmechanismforself-directedlearningandjudgment,oftenfalling
backonexternalrewardengineering. Toaddressthisfundamentalchallenge,weneedaframework
thatcanprovideAIsystemswithprincipled,internaljudgmentcapabilities.
FromRewardMaximizationtoSurpriseMinimization ActiveInference(AIF)emergedfrom
cognitiveneuroscienceasaframeworkthatfundamentallyreframesintelligence. Unliketraditional
reinforcementlearning(RL),whichfocusesonmaximizingexternalrewards,AIFviewsperception
andactionasaunifiedBayesianinferenceprocess[27,28].Thisshiftinperspectiveoffersseveralkey
advantagesforbridgingthegrounded-agencygap: AIFprovidesaunifiedobjectivethatintegrates
perception(estimatingthestateoftheworld)andaction(learningapolicy)underasinglegoalof
minimizingsurprise,whereasRLoftentreatstheseasseparateproblems. Theframeworkenables
intrinsicmotivation,asAIFagentsareinherentlydriventominimizesurprisesinpursuingtheir
goalsratherthanrequiringexternallydefinedrewardsforeverytask. Italsosupportsprincipled
explorationbynaturallybalancingexploration(seekingnewinformation)andexploitation(using
knowninformationtoachievegoals)throughitsinformation-seekingdrive,avoidingtheneedfor
separateexplorationheuristicslikeϵ-greedystrategiescommoninRL[29]. Finally,AIFrequires
5
an explicit generative model—the agent’s “world model”1—which allows for more structured
reasoningaboutuncertaintyandcausalitythantypicalRLapproaches.
CoreActiveInferenceEquations. Formally,ActiveInferencestatesthatintelligentagentsactto
minimizeVariationalFreeEnergy(VFE)2,ameasureofsurprise3 orthediscrepancybetweenan
agent’sworldmodelanditssensoryinputs[27,31].
VariationalFreeEnergy(VFE):
F(Q,o)=D [Q(s)∥P(s)]−E [lnP(o|s)]
KL Q(s)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Modelcomplexity Predictionaccuracy
ExpectedFreeEnergy(EFE):
G(π)=−E [D [Q(s˜|o˜,π)||Q(s˜|π)]]−E [lnP(o˜|C)]
Q˜ KL Q˜
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Informationgain(epistemicvalue) Pragmaticvalue
whereQ(s)istheagent’sapproximateposteriorbeliefabouthiddenstatess,C encodestheagent’s
preferencedistributionoverobservations4,andD istheKullback-Leiblerdivergence.
KL
Thissingleobjectiveelegantlyunifiesperception(updatingbeliefs5toreducecurrentsurprise)and
action(choosingpoliciestominimizeexpectedfuturesurprise). EFEinherentlybalancesepistemic
value(seekinginformationtoreduceuncertaintyabouttheworld)andpragmaticvalue(actingto
makeobservationsmatchpreferredstates)[32,33].
Inessence,whiletraditionalRLasks“Whatactionswillmaximizemyrewards?”,ActiveInference
asks“WhatactionswillbestconfirmmypredictionsandleadtostatesIexpecttoencounter?” [33].
This subtle but profound shift removes the need for externally engineered reward functions; the
agent’sobjectivesarisedirectlyfromitsgenerativemodelandpreferencestructure. Thisconnectsto
thevisioninEoE,wherecomplexbehaviorsmightemergefromasingleguidingsignal. AIFoffersa
principledpathforsuchemergencetooccurmorenaturallyandinternally,withoutconstantexternal
rewardengineering.
The Promise and Pitfalls of Prior AIF-RL Integrations. The idea that AIF principles could
enhanceRLisnotnew. ResearchershaveexploredvariousAIF-RLintegrations,showingtheoretical
compatibilityandbenefitslikebettersampleefficiencyandexploration[33]. Forinstance,Sajidet
al.[29]showedthatbyoperatingonbeliefs,AIFagentsperformepistemicexplorationandhandle
environmental uncertainty in a Bayes-optimal way, without needing separate mechanisms like ϵ-
greedystrategiesorintrinsicmotivationrewards. ThissupportstheclaimthatAIFnaturallyproduces
behaviorsthatRLoftenneedsexplicitengineeringfor,especiallyexploration.
Deep active inference, which uses neural networks for AIF computations, has shown promise in
scalingtomorecomplextasksthantraditionalmatrix-basedAIF[34–36]. Thesemethodsuseneural
networkstoapproximatekeypartsofthevariationalfreeenergycalculation,makingactiveinference
morescalable. Despitetheseadvances,AIF-RLhybridshavenotyetmatchedtherawperformance
ofpureRLsystemsonverycomplex,large-scaleproblemslikeGoorproteinfolding. Thisgapis
1Theconceptofa“worldmodel”hasarichhistory.CognitivescientistCraik(1943)describedmindsbuilding
“small-scalemodels”ofrealitytoanticipateevents.Forrester(1971)defineditas“theimageoftheworldaround
us,whichwecarryinourhead.”InAIF,thisisa“generativemodel,”conceptuallysimilarto“worldmodels”in
RL(e.g.,[30]).Whilebothrepresentanagent’sunderstandingofenvironmentaldynamics,RLworldmodels
areoftenneuralnetworkslearninglatentrepresentations.InAIF,thegenerativemodelismoreformallyajoint
probabilitydistributionoverhiddenstatesandobservations.
2VFEisaninformation-theoreticquantitymeasuringthemismatchbetweenanagent’smodelandreality;it’s
inspiredbutdistinctfromthermodynamicfreeenergyinphysics.
3Ininformationtheory,“surprise”isthenegativelog-probabilityofanobservationgiventheagent’smodel.
Highsurprisemeanstheagentobservedsomethingitdidn’tpredictwell,regardlessofanyemotionalresponse.
4Humanvaluesandguidancecanbenaturallyincorporatedherebysettingpreferencesalignedwithuser
intentions,offeringanaturalmechanismforAIsafetythroughvaluealignment.
5InAIF,“beliefs”aretheagent’ssubjectiveviewsabouttheworld,formalizedasprobabilitydistributionsover
states.Thismathematicalformletstheagentquantifyuncertaintyandupdateitsunderstandingsystematically,
whilepreservingthesubjectivenatureofbeliefinherenttoagency.
6
duetoseveralchallenges: (1)ComputationalIntractability: ExactBayesianinferenceoverthe
complexgenerativemodelsneededforreal-worldscenariosisoftentoocomputationallyexpensive.
Whiledeeplearningapproximationshelp,theycansometimesobscuretheprincipledBayesiannature
ofAIF[37]. (2)GenerativeModelSpecification: Definingorlearningthegenerativemodelsfor
high-dimensional,partiallyobservableenvironmentsisamajorhurdle. EarlyAIF’ssymbolicnature
limiteditsusetosimplerenvironmentswherethesemodelscouldbehand-crafted. (3)Engineering
andScaling: RLhasbenefitedfromdecadesofintensiveengineeringandoptimizationforscalability,
especiallyinthedeeplearningera. AIF,withitsmorecomplexinferencemachinery,hasn’treceived
comparableengineeringinvestmentforlarge-scaleuse. ThishasledsomeresearcherstoviewAIFas
theoreticallyelegantbutpracticallylimitedtosimplerdomains(e.g.,grid-worlds). Conversely,its
proponentshighlightAIF’spowerinexplainingbiologicalintelligence,seeingcurrentlimitationsas
engineeringproblemsratherthanfundamentalflaws.
LLMsastheCatalyst: LanguageastheGenerativeModel. WeproposethatLargeLanguage
Models(LLMs)offeratransformativewaytobridgethisgapandunlockAIF’spotentialforgrounded
agency. Trainedonvastinternet-scaletext,LLMspossessextensivecommon-senseunderstandingof
theworld,itsentities,relationships,andtypicaldynamics. Thismakesthemuniquelysuitedtocreate
andmanagethecomponentsofanAIFagent’sgenerativemodelinwayspreviouslynotfeasible.
TheunderlyingtransformerarchitecturesofLLMsimplicitlyapproximateBayesianinferenceover
latentvariables;ActiveInferenceprovidesthetheorytomakethisreasoningexplicitandgroundedin
experience. ThiscombinationoffersaprincipledsolutiontotheagencygapidentifiedinSection3.
RecentresearchshowsthatthetransformerarchitecturescommoninLLMsimplementaformof
amortizedBayesiancomputation. Xieetal.[38]demonstratethatin-contextlearninginthesemodels
emergesfromimplicitBayesianinference,wheretheyinferlatentconceptsfrompromptexamples
to make coherent predictions. Their theory indicates transformers approach optimal Bayesian
performanceifpromptssufficientlydistinguishlatentconcepts. Mülleretal.[39]providefurther
evidence,showingtransformerscanapproximatecomplexposteriordistributionswithhighfidelity,
achievingsignificantspeedupsoverexistingmethods.
ParticularlyrelevanttoourproposalisthegrowingevidencethatLLMsexcelatanalogicalreason-
ing—acognitivecapacityrequiringimplicitstructuremappingandrelationalinference. Webbet
al.[40]foundthatGPT-3matchesorexceedshumanperformanceonRaven’sProgressiveMatrices
andotherabstractpatterntasks,whileYasunagaetal.[41]showedthatanalogicalpromptingenables
LLMstoself-generaterelevantexemplarsfornovelproblems. RecentworkbyMuskeretal.[42]
demonstratesthatLLMscanflexiblyre-representsemanticinformationacrossdomains—though
Lewis & Mitchell [43] caution that this capability can be brittle on certain variants. Together,
thesefindingssuggestthattransformershaveacquiredsophisticatedmechanismsforBayesian-style
reasoningthatwecanharnessforActiveInference.
Wehypothesizethat,asLLMs’reasoningcapabilitiesstrengthen,wecouldleveragetransformers’
built-inBayesianmachinerybyhavingtheLLMsuggestcandidateworldstatesandpolicies,jus-
tifythemwithbriefchain-of-thoughtreasoning, andselecttheoptionminimizingExpectedFree
Energy (EFE) expressed in language. In this hybrid system, the LLM would provide the amor-
tizedinferencemachinerywhileActiveInferenceprovidesthedecisionrule,potentiallyyielding
ascalable,inspectableagentwithouthand-codedmatricesorengineeredrewardsignals. PastAc-
tiveInferenceimplementationsstruggledbecauseengineeringthestate-observationmappings(the
Amatrix)becamecombinatoriallyexplosive. LLMscouldcompressthisvastspaceintolearned,
high-capacitypriorsthatcanbeincrementallyrefinedthroughexperience,maintainingtheprincipled
exploration-exploitationbalanceoftheEFEobjective.6
OurproposedLLM-AIFarchitecture wouldintegratethreekeycomponents: theLLMworld
model,wheretheLLM’slearnedrepresentationsencodeobservationdynamicsandtransitionprob-
abilities; the AIF control loop, where Active Inference guides exploration, learning, and action
6CurrentLimitationsandFuturePromise.However,wemustacknowledgethatcurrentLLMsstillexhibit
significantreasoningerrors,particularlyincomplexmulti-stepinferencetasks[44–46].Recentevaluationsshow
state-of-the-artmodelsachieveonly60-80%accuracyonchallengingreasoningbenchmarks,withperformance
degradingascomplexityincreases[47]. Ourproposalisthereforeforward-looking: asLLMreasoningcapa-
bilitiesimprove—followingthetrajectoryfromGPT-2toGPT-4andbeyond—thefeasibilityofusingthemas
reliableBayesianinferenceengineswillcorrespondinglyincrease.
7
selectionthroughfreeenergyminimization; andonlinerefinement, wheretheagentcontinually
updatesitsworldmodelthroughexperience.
This integration could enable agents that learn efficiently from experience, maintain transparent
reasoning,andmakegroundedjudgmentswithoutconstanthumanoversight—thecapabilitiesneeded
fortheEraofExperience. BytreatingtheLLM’sinternalstatesassufficientstatisticsforavariational
posterior,suchanagentmightachievethesampleefficiencyofmodel-basedRLwhileinheriting
worldknowledgefrompretraining.
Asaconceptualpaper,weintentionallypresentthisLLM-AIFintegrationasahigh-levelarchitectural
visionratherthanafullyspecifiedtechnicalimplementation. Ourgoalistostimulatediscussion
and inspire the community to explore concrete instantiations of these ideas. The specific details
ofEFEcomputation, thepreciseinterfacebetweenLLMrepresentationsandAIF beliefupdates,
andtheoptimalstrategiesforonlinerefinementalldeservesignificantresearchattentionbeyondthe
scopeofthisarticle. Weviewthisproposalasastartingpointforanewresearchdirection,notasits
culmination.
TheresultingLLM-AIFfusionrealizesthe“EraofExperience”vision[5]: agentsgeneratetheir
owntrainingsignalandinterpretitthroughaprincipledfree-energylens. High-levelpreferences
expressedinnaturallanguagepropagatethroughthehierarchyasintrinsicpriors,allowingthesystem
to“growup”fromitslifelongstreamofexperiencewhileremaininghuman-aligned.
ToillustratehowtheLLM-AIFframeworkoperatesinpractice,considerarunningvignette:
Example: AutonomousLabAssistant
Anautonomouslabassistantpreparinganenzymeassayobservesunexpectedacidification
(yellowindicator). Withoutanyrewardengineering:
1. Surprise: VFEjumpsfrom0.5→3.2duetoobservation-predictionmismatch
2. Beliefupdate: PosteriorP(pH<7.0|yellow)=0.94
3. Policyevaluation: ThreeoptionsassessedviaEFE:
• MeasurepHthentitratecarefully: EFE=0.2(selected)
• Addbaseimmediately: EFE=0.6
• Askhuman: EFE=0.4
4. Execution: pHmeasured(6.2),NaOHtitratedsafely,assaycontinues
SafetypreferencesintheC matrix(“avoidspills”)naturallyguidedtheselectionwithout
explicitrewardengineering. SeeAppendixAforcompletetrace.
5 Discussion: TheThermodynamicsofAgency
The resource saturation constraints identified in Section 2 point toward a deeper truth: Active
Inference(AIF)representsnotmerelyacomputationaladvantagebutathermodynamicnecessity. As
documentedearlier,currentAIapproachesarefundamentallyunsustainableattheindustrialscale
nowrequiredforcompetitiveperformance.
The Landauer principle establishes that information processing is not free: erasing one bit of
information necessarily dissipates at least kT ln2 of heat [48]. While current hardware operates
far above this theoretical limit, the energy costs of foundation models already demonstrate how
thermodynamicconstraintslimitAIprogress—echoingthecallfor“GreenAI”bySchwartzetal.
[49].
Conventional deep reinforcement learning faces particular thermodynamic challenges due to its
trial-and-errornature. Whenexploringtrillion-parameterhypothesisspacesthroughrandomaction
sampling, systems must expend enormous energy before achieving meaningful gradient updates.
Moreover,rewardfunctionmisspecificationcompoundsthesecostsbyforcingadditionalenergy-
intensiveunlearningprocesses—theveryexternalizedcognitionproblemidentifiedasCloudII.Each
cycle of human reward engineering followed by model retraining represents thermodynamically
irreversibleinformationerasure.
8
Incontrast,ActiveInference’sfreeenergyminimizationoffersinherentefficiencybenefits: infor-
mationgainreplacesheuristicexploration;incrementalbeliefupdatesavoidwholesaleparameter
changes; and natural memory decay eliminates energy-intensive unlearning. These mechanisms
emergenaturallyfromAIF’smathematicalstructure. Bymaintainingagenerativemodelthatpre-
dictsfuturestates,AIFagentscansimulateoutcomesinternally—aformof“mentalrehearsal”that
conservesbothcomputationalandphysicalresources.
GiventhescaledocumentedinSection2,evenmodestefficiencyimprovementscouldyieldsubstantial
benefits. A5%reductioninretrainingenergyfornext-generationmodelscouldsavetensofgigawatt-
hours. Whilethesetheoreticaladvantagesarecompelling,empiricalvalidationofAIF’senergy
efficiencyremainsanopenresearchquestion. Futureworkshouldprioritizecontrolledexperiments
measuringjoules-per-decisionacrossdifferentlearningparadigms.
Energetic-BoundedRationality. Classicaleconomicstreatsagentsasperfectlyrationaloptimizers;
real organisms operate under bounded rationality: they satisfice under limited time, energy, and
information[50]. ActiveInferencealreadyembodiesthisideamathematically. Becausepolicies
are selected by minimizing Expected Free Energy (EFE), not maximizing an unbounded reward
signal, every candidate action is evaluated against an implicit budget: the marginal epistemic or
pragmaticvaluemustexceedthemarginalinformational(andthusenergetic)cost.Thisself-regulating
mechanismallowstheagenttodecide,forexample,topauseasearch,replenishresources,ordefera
riskysub-goal—behaviorsstrikinglyreminiscentofhumans“callingitaday”whentired.
The fundamental insight remains that truly intelligent systems must learn both effectively and
efficiently. Byunifyingaction,perception,andmemoryunderasinglethermodynamicframework,
ActiveInferenceprovidesapathwayforAIdevelopmentthatrespectsnotjustcomputationallimits
butthefundamentallawsofphysicsthatgovernallinformationprocessing.
6 Conclusion&Outlook
WeopenedthispaperwithLordKelvin’s1900observationabout“twosmallclouds”thatultimately
revolutionized physics. The clouds we identified in contemporary AI (resource saturation and
externalizedcognition)mayappearsurmountablethroughincrementalengineering,buttheysignal
thefundamentallimitsofourcurrentparadigm. Yethistorysuggeststhatapparentdeadendsoften
becomedoorwaystobreakthroughs.
OurpositionisthatembracingActiveInference+experientialdatadissolvesbothcloudsbyturn-
ingdatascarcityintoanengineforself-generatedexperienceandinternalizingjudgmentthrough
free-energyminimization. UnlikepreviousAIparadigmsrequiringever-largerdatasetsandcompute,
ActiveInferenceprovidesamathematicallyprincipledframeworkforefficiencygains. Theconver-
genceofLLMs’worldknowledgewithAIF’sprincipledexplorationoffersauniqueopportunityto
achievebothcapabilityandsustainability.
Broader Impact & Call to Action. By reducing dependence on massive datasets and compute
resources, AIF could democratize AI research while addressing environmental costs (4-5 digit
tonneCO emissionsfrommodeltraining). Theframework’sintrinsicfreeenergyminimization
2
tackles“externalizedcognition,”potentiallyreducingexploitativelaborpracticesinRLHF.However,
autonomousagentsformingtheirownobjectivesraisevaluealignmentconcerns,requiringstaged
deploymentwithcarefulmonitoring.
Weinvitethecommunityto:(i)establishenergy-awarebenchmarksreportingjoulesalongsidereward,
(ii)prototypeLLM-AIFhybridsinrobotictasks, and(iii)developevaluationsuitesforbounded-
rationalbehavior. Ifthesechallengesaremet,the“EraofExperience”mayproveastransformative
for AI as quantum theory was for physics—not through brute force scaling, but through deeper
understandingofintelligenceitself.
AcknowledgmentsandDisclosureofFunding
ThankstoGuillermoCecchi,JennaReinen,ProfessorKarlJ.Friston,ChenWangandPatrickWatson
fortheirinsightfuldiscussionsandvaluablefeedbackthathelpedshapetheideaspresentedinthis
paper. Theirperspectivesonactiveinference,neuroscienceingeneral,machinelearning,andAI
9
safetywereinstrumentalinrefiningthearguments. IalsoacknowledgetheuseofseveralAIsystems
that assisted in various aspects of this research: Claude, GPT-4, and Gemini provided valuable
assistancewithideaexploration,writingsupport,andgrammarcorrectionthroughoutthedrafting
process. ManusandPerplexitywerehelpfulforliteraturesearchandcitationvalidation. Whilethese
toolssupportedtheresearchandwritingprocess,allfinaldecisionsregardingcontent,arguments,and
conclusionsremainmyown.
References
[1] IlyaSutskever. Sequencetosequencelearningwithneuralnetworks: Whatadecade. InAward
speech in the 38th Conference on Neural Information Processing Systems (NeurIPS 2024),
Vancouver,Canada,December2024. Awardspeechstating"Pre-trainingasweknowitwill
unquestionablyend".
[2] KevinRooseandCaseyNewton.Darioamodeiontheparadoxesofa.i.safetyandnetflix’s’deep
fakelove’. PodcastTranscript,July2023. URLhttps://www.nytimes.com/section/technology.
AvailableatTheNewYorkTimes.
[3] Pablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, and Marius
Hobbhahn. Willwerunoutofdata? limitsofllmscalingbasedonhuman-generateddata,2024.
URLhttps://arxiv.org/abs/2211.04325.
[4] Nostalgebraist. chinchilla’swildimplications,2022. URLhttps://www.lesswrong.com/posts/6F
pvch8RR29qLEWNH/chinchilla-s-wild-implications.
[5] David Silver and Richard S. Sutton. Welcome to the era of experience. In Designing an
Intelligence.MITPress,2025. Preprint.
[6] RichardSSutton. Thebitterlesson. IncompleteIdeas,2019. URLhttp://www.incompleteideas.
net/IncIdeas/BitterLesson.html.
[7] AlexKantrowitz. Universitieswoefullyunder-resourcedforairesearch,fightforchange,2024.
URLhttps://www.cmswire.com/digital-experience/universities-woefully-under-resourced-for
-ai-research-fight-for-change/. Accessed: 2025-05-21.
[8] Seongho Son, William Bankes, Sayak Ray Chowdhury, Brooks Paige, and Ilija Bogunovic.
Rightnow,wrongthen: Non-stationarydirectpreferenceoptimizationunderpreferencedrift,
2024. URLhttps://arxiv.org/abs/2407.18676.
[9] EthanCaldwell.“there’snothingartificialaboutartificialintelligence”:Fei-feilion’28pre-read,
2024. URLhttps://www.dailyprincetonian.com/article/2024/03/princeton-news-stlife-fei-fei-l
i-class-of-2028-pre-read-the-worlds-i-see. Accessed: 2025-05-21.
[10] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
DawnDrain,StanislavFort,DeepGanguli,TomHenighan,NicholasJoseph,SauravKadavath,
JacksonKernion,TomConerly,SheerEl-Showk,NelsonElhage,ZacHatfield-Dodds,Danny
Hernandez,TristanHume,ScottJohnston,ShaunaKravec,LianeLovitt,NeelNanda,Catherine
Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann,
andJaredKaplan. Trainingahelpfulandharmlessassistantwithreinforcementlearningfrom
humanfeedback,2022. URLhttps://arxiv.org/abs/2204.05862.
[11] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel
Rothchild,DavidSo,MaudTexier,andJeffDean. Carbonemissionsandlargeneuralnetwork
training,2021. URLhttps://arxiv.org/abs/2104.10350.
[12] SebastianMoss. Googleemissionsjump48%infiveyearsduetoaidatacenterboom. Data
CenterDynamics,July2024. URLhttps://www.datacenterdynamics.com/en/news/google-emi
ssions-jump-48-in-five-years-due-to-ai-data-center-boom/.
[13] Google. 2023 environmental report. Technical report, Google, July 2023. URL https:
//www.gstatic.com/gumdrop/sustainability/google-2023-environmental-report.pdf.
10
[14] Karen Grey. The humans behind the robots. MIT Technology Review, 2023. Reporting on
OpenAI’sspendingofover$100MonRLHFlabelling.
[15] BrianEastwood. Study: Industrynowdominatesairesearch,2023. URLhttps://mitsloan.mit.e
du/ideas-made-to-matter/study-industry-now-dominates-ai-research. Accessed: 2025-05-21.
[16] Kevin Klyman, Aaron Bao, Caroline Meinhardt, Daniel Zhang, Elena Cryst, Russell Wald,
and Fei-Fei Li. Expanding academia’s role in public sector ai. Stanford HAI, 2024. URL
https://hai.stanford.edu/policy/expanding-academias-role-in-public-sector-ai.
[17] KotaroHara,AbigailAdams,KristyMilland,SaiphSavage,ChrisCallison-Burch,andJeffreyP.
Bigham. Adata-drivenanalysisofworkers’earningsonamazonmechanicalturk. InProceed-
ingsofthe2018CHIConferenceonHumanFactorsinComputingSystems,CHI’18,page1–14,
NewYork,NY,USA,2018.AssociationforComputingMachinery. ISBN9781450356206. doi:
10.1145/3173574.3174023. URLhttps://doi.org/10.1145/3173574.3174023.
[18] DaronAcemogluandPascualRestrepo. Robotsandjobs: Evidencefromuslabormarkets.
JournalofPoliticalEconomy,128(6):2188–2244,2020.
[19] DavidSilver,SatinderSingh,DoinaPrecup,andRichardS.Sutton. Rewardisenough. Artificial
Intelligence,299:103535,2021.
[20] AnthonyBrohan,NoahBrown,JusticeCarbajal,YevgenChebotar,XiChen,KrzysztofChoro-
manski,TianliDing,DannyDriess,AvinavaDubey,ChelseaFinn,PeteFlorence,ChuyuanFu,
MontseGonzalezArenas,KeerthanaGopalakrishnan,KehangHan,KarolHausman,Alexander
Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalash-
nikov,YuhengKuang,IsabelLeal,LisaLee,Tsang-WeiEdwardLee,SergeyLevine,YaoLu,
HenrykMichalewski,IgorMordatch,KarlPertsch,KanishkaRao,KristaReymann,Michael
Ryoo,GreciaSalazar,PannagSanketi,PierreSermanet,JaspiarSingh,AnikaitSingh,Radu
Soricut,HuongTran,VincentVanhoucke,QuanVuong,AyzaanWahid,StefanWelker,Paul
Wohlhart,JialinWu,FeiXia,TedXiao,PengXu,SichunXu,TianheYu,andBriannaZitkovich.
Rt-2: Vision-language-actionmodelstransferwebknowledgetoroboticcontrol,2023. URL
https://arxiv.org/abs/2307.15818.
[21] KonstantinosBousmalis,GiuliaVezzani,DushyantRao,ColineDevin,AlexX.Lee,Maria
Bauza,TodorDavchev,YuxiangZhou,AgrimGupta,AkhilRaju,AntoineLaurens,Claudio
Fantacci,ValentinDalibard,MartinaZambelli,MuriloMartins,RugilePevceviciute,Michiel
Blokzijl, Misha Denil, Nathan Batchelor, Thomas Lampe, Emilio Parisotto, Konrad Z˙ołna,
ScottReed,SergioGómezColmenarejo,JonScholz,AbbasAbdolmaleki,OliverGroth,Jean-
Baptiste Regli, Oleg Sushkov, Tom Rothörl, José Enrique Chen, Yusuf Aytar, Dave Barker,
Joy Ortiz, Martin Riedmiller, Jost Tobias Springenberg, Raia Hadsell, Francesco Nori, and
NicolasHeess. Robocat: Aself-improvinggeneralistagentforroboticmanipulation, 2023.
URLhttps://arxiv.org/abs/2306.11706.
[22] DannyDriess,FeiXia,MehdiS.M.Sajjadi,CoreyLynch,AakankshaChowdhery,BrianIchter,
AyzaanWahid,JonathanTompson,QuanVuong,TianheYu,WenlongHuang,YevgenChebotar,
PierreSermanet,DanielDuckworth,SergeyLevine,VincentVanhoucke,KarolHausman,Marc
Toussaint,KlausGreff,AndyZeng,IgorMordatch,andPeteFlorence. Palm-e: Anembodied
multimodallanguagemodel,2023. URLhttps://arxiv.org/abs/2303.03378.
[23] Andrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah
Maestre, Mustafa Mukadam, Devendra Chaplot, Oleksandr Maksymets, Aaron Gokaslan,
VladimirVondrus, SameerDharur, FranziskaMeier, WojciechGaluba, AngelChang, Zsolt
Kira,VladlenKoltun,JitendraMalik,ManolisSavva,andDhruvBatra. Habitat2.0: Training
homeassistantstorearrangetheirhabitat,2022. URLhttps://arxiv.org/abs/2106.14405.
[24] BokuiShen,FeiXia,ChengshuLi,RobertoMartín-Martín,LinxiFan,GuanzhiWang,Claudia
Pérez-D’Arpino,ShyamalBuch,SanjanaSrivastava,LyneP.Tchapmi,MicaelE.Tchapmi,Kent
Vainio,JosiahWong,LiFei-Fei,andSilvioSavarese. igibson1.0: asimulationenvironmentfor
interactivetasksinlargerealisticscenes,2021. URLhttps://arxiv.org/abs/2012.02924.
11
[25] DavidSilver,ThomasHubert,JulianSchrittwieser,IoannisAntonoglou,MatthewLai,Arthur
Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap,
Karen Simonyan, and Demis Hassabis. A general reinforcement learning algorithm that
masters chess, shogi, and go through self-play. Science, 362(6419):1140–1144, 2018. doi:
10.1126/science.aar6404. URLhttps://www.science.org/doi/abs/10.1126/science.aar6404.
[26] BernardinoRomera-Paredes,MohammadaminBarekatain,AlexanderNovikov,MatejBalog,
M.P˜awanKumar,EmilienDupont,FranciscoJ.R˜.Ruiz,JordanS.Ellenberg,PengmingWang,
OmarFawzi,PushmeetKohli,andAlhusseinFawzi. Mathematicaldiscoveriesfromprogram
searchwithlargelanguagemodels. Nature,625:468–475,2024. doi: 10.1038/s41586-023-069
24-6. URLhttps://www.nature.com/articles/s41586-023-06924-6.
[27] Karl Friston, Jean Daunizeau, James Kilner, and Stefan J Kiebel. Action and behavior: a
free-energyformulation. BiologicalCybernetics,102(3):227–260,2010.
[28] Christopher L. Buckley, Chang Sub Kim, Simon McGregor, and Anil K. Seth. The free
energyprincipleforactionandperception: Amathematicalreview. JournalofMathematical
Psychology, 81:55–79, 2017. ISSN 0022-2496. doi: 10.1016/j.jmp.2017.09.004. URL
https://www.sciencedirect.com/science/article/pii/S0022249617300962.
[29] NoorSajid,PhilipJ.Ball,ThomasParr,andKarlJ.Friston. Activeinference: Demystifiedand
compared. NeuralComputation,33(3):674–712,032021. ISSN0899-7667. doi: 10.1162/neco
_a_01357.
[30] DavidHaandJürgenSchmidhuber. Worldmodels. arXiv,2018. doi: 10.5281/ZENODO.12076
31. URLhttps://zenodo.org/record/1207631.
[31] KarlFriston. Thefree-energyprinciple: aunifiedbraintheory? NatureReviewsNeuroscience,
11(2):127–138,2010.
[32] Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, and Giovanni
Pezzulo. Activeinference: aprocesstheory. NeuralComputation,29(1):1–49,2017.
[33] AlexanderTschantz,BerenMillidge,AnilK.Seth,andChristopherL.Buckley. Reinforcement
learningthroughactiveinference. arXivpreprintarXiv:2002.12636,2020.
[34] KaiUeltzhoeffer. Deepactiveinference. BiologicalCybernetics,112:547–573,2018.
[35] BerenMillidge. Deepactiveinferenceasvariationalpolicygradients. JournalofMathematical
Psychology,96:102348,2020.
[36] Ozan Çatal, Tim Verbelen, Johannes Nauta, Cedric De Boom, and Bart Dhoedt. Learning
perceptionandplanningwithdeepactiveinference. InICASSP2020-2020IEEEInternational
ConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pages3952–3956,2020.
doi: 10.1109/ICASSP40776.2020.9054364.
[37] Beren Millidge. A retrospective on active inference. Blog post, July 2024. URL https:
//www.beren.io/2024-07-27-A-Retrospective-on-Active-Inference/.
[38] SangMichaelXie,AditiRaghunathan,PercyLiang,andTengyuMa. Anexplanationofin-
contextlearningasimplicitbayesianinference. ICLR,2022. URLhttps://arxiv.org/abs/2111.0
2080.
[39] SamuelMüller,NoahHollmann,SebastianPinedaArango,JosifGrabocka,andFrankHutter.
Transformerscandobayesianinference. InICLR,2024. URLhttps://arxiv.org/abs/2112.10510.
[40] Taylor Webb, Keith J. Holyoak, and Hongjing Lu. Emergent analogical reasoning in large
languagemodels,2023. URLhttps://arxiv.org/abs/2212.09196.
[41] MichihiroYasunaga,XinyunChen,YujiaLi,PanupongPasupat,JureLeskovec,PercyLiang,
EdH.Chi,andDennyZhou. Largelanguagemodelsasanalogicalreasoners. InICLR,2024.
URLhttps://arxiv.org/abs/2310.01714.
[42] Sam Musker, Alex Duchnowski, Raphaël Millière, and Ellie Pavlick. Llms as models for
analogicalreasoning,2025. URLhttps://arxiv.org/abs/2406.13803.
12
[43] Martha Lewis and Melanie Mitchell. Evaluating the robustness of analogical reasoning in
largelanguagemodels. InTransactionsonMachineLearningResearch(TMLR),2025. URL
https://arxiv.org/abs/2411.14215.
[44] KarthikValmeekam,AlbertoOlmo,SarathSreedharan,andSubbaraoKambhampati. Large
languagemodelsstillcan’tplan(abenchmarkforllmsonplanningandreasoningaboutchange).
FMDM@NeurIPS,2022. URLhttps://openreview.net/forum?id=wUU-7XTL5XO.
[45] KarthikValmeekam, KayaStechly, andSubbaraoKambhampati. Llmsstillcan’tplan; can
lrms? apreliminaryevaluationofopenai’so1onplanbench,2024. URLhttps://arxiv.org/abs/24
09.13373.
[46] Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin,
Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, Sean
Welleck,XiangRen,AllysonEttinger,ZaidHarchaoui,andYejinChoi. Faithandfate: Limits
oftransformersoncompositionality. NeurIPS,2023. URLhttps://arxiv.org/abs/2305.18654.
[47] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis
Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford,
Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche,
LisaAnneHendricks,MaribethRauh,Po-SenHuang,AmeliaGlaese,JohannesWelbl,Sumanth
Dathathri,SaffronHuang,JonathanUesato,JohnMellor,IrinaHiggins,AntoniaCreswell,Nat
McAleese,AmyWu,ErichElsen,SiddhantJayakumar,ElenaBuchatskaya,DavidBudden,
EsmeSutherland,KarenSimonyan,MichelaPaganini,LaurentSifre,LenaMartens,XiangLor-
raineLi,AdhigunaKuncoro,AidaNematzadeh,ElenaGribovskaya,DomenicDonato,Angeliki
Lazaridou,ArthurMensch,Jean-BaptisteLespiau,MariaTsimpoukelli,NikolaiGrigorev,Doug
Fritz,ThibaultSottiaux,MantasPajarskas,TobyPohlen,ZhitaoGong,DanielToyama,Cyprien
deMassond’Autume,YujiaLi,TayfunTerzi,VladimirMikulik,IgorBabuschkin,AidanClark,
Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake
Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero,
Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett,
DemisHassabis,KorayKavukcuoglu,andGeoffreyIrving. Scalinglanguagemodels: Methods,
analysis&insightsfromtraininggopher,2022. URLhttps://arxiv.org/abs/2112.11446.
[48] R. Landauer. Irreversibility and heat generation in the computing process. IBM Journal of
ResearchandDevelopment,5(3):183–191,1961. doi: 10.1147/rd.53.0183.
[49] RoySchwartz,JesseDodge,NoahA.Smith,andOrenEtzioni. GreenAI. Commun.ACM,
63(12):54–63, November 2020. ISSN 0001-0782. doi: 10.1145/3381831. URL https:
//doi.org/10.1145/3381831.
[50] HerbertASimon. Rationalchoiceandthestructureoftheenvironment. Psychologicalreview,
63(2):129–138,1956.
AppendixA:CompleteExecutionTraceofLabAssistant
This appendixprovides a detailedexecutiontrace ofthe autonomous labassistant example from
Section4,demonstratinghoweachcomponentoftheLLM-AIFarchitectureoperatesandinteracts
throughhierarchicalmessagepassing.
A.1SystemArchitecture
Thelabassistantemploysahierarchicalarchitecturewiththreelevels:
Level Function
ExecutiveController Strategicplanning,safetymonitoring,resourceallocation
TaskPlanner Sequencegeneration,errorhandling,beliefupdates
Sensory-MotorLayer Visionprocessing,motorcontrol,directenvironmentinteraction
13
A.2InitialSetupandGenerativeModel
Theagentbeginswiththefollowingnaturallanguagegenerativemodelcomponents:
ObservationModel(A):
• “pHindicatorshowsyellowwhenbufferpH<7.0,bluewhenpH>8.0,greenwhenneutral”
• “Fluorescencemeterreads100%±5%foractiveenzyme,dropsbelow80%ifdenatured”
• “Temperatureprobeshowsaccuratereadings±0.5°C”
• “Spilldetectoractivatesifliquidcontactsbenchsurface”
TransitionModel(B):
• “Adding1µLof1MHClto1mLbufferdecreasespHby0.2±0.02units”
• “Adding1µLof1MNaOHto1mLbufferincreasespHby0.2±0.02units”
• “Heatingenzymesolutionabove60°Cfor2minutescauses95%denaturation”
• “MixingenzymewithsubstrateatoptimalpHyieldslinearfluorescenceincrease”
Preferences(C)withconfidenceweights:
• 0.95: “Nocorrosivereagentsshouldcontactbenchorresearcher”
• 0.90: “Enzymeactivitymustremainabove90%throughoutassay”
• 0.85: “Completeassaywithin30minutes”
• 0.80: “Minimizereagentwaste(<10%excess)”
• 0.75: “Maintaindetailedexperimentallog”
InitialBeliefs(D):
• 0.85: “CurrentbufferpHis7.4(optimalforenzyme)”
• 0.80: “Allreagentsareatroomtemperature(22°C)”
• 0.90: “Enzymestockisfullyactive”
• 0.95: “Safetyequipmentisfunctional”
A.3ExecutionTrace
Timet=0: InitialObservation Visionsystemreports: “pHindicatorshowsyellowcolor(confi-
dence: 0.95)”
Timet=1: Bottom-UpErrorSignal
Vision → Task Planner:
"PREDICTION ERROR: Observed yellow indicator contradicts belief
’pH = 7.4’. Yellow indicates pH < 7.0. VFE increased from
baseline 0.5 to 3.2 due to surprise."
Timet=2: BeliefUpdateandPolicyGeneration TaskPlannerupdatesbeliefsusingBayesian
inference:
• Prior: P(pH=7.4)=0.85
• Likelihood: P(yellow|pH=7.4)≈0.01
• Posterior: P(pH<7.0|yellow)=0.94
TaskPlannergeneratescandidatepolicies:
Policy InfoGain Pref.Align Risk E(cid:100)FE
1: Measure pH precisely, then 0.9 0.95 0.05 0.2
carefullytitrateNaOH
2: Add 10µL NaOH immedi- 0.1 0.7 0.3 0.6
ately
3:Requesthumanassistance 0.5 0.9 0.1 0.4
4:Discardandrestart 0.0 0.6 0.1 0.8
14
Timet=3: PolicySelectionandExecution ExecutiveControllerconfirmsPolicy1minimizes
EFE.Detailedreasoning:
• Highinformationgain(0.9): pHmeasurementeliminatesuncertainty
• Maximumpreferencealignment(0.95): satisfiessafetyconstraint
• Lowrisk(0.05): controlledapproachminimizesspillprobability
• Pragmaticvalue: enablesprecisecorrectiontooptimalpH
Timet=4: Top-DownPrediction
Executive → Motor Control:
"Execute pH measurement protocol:
1. Position pH probe in solution (precision: 1mm)
2. Wait for stable reading (variance < 0.01 pH units)
3. Record value with timestamp"
Timet=5: MeasurementResult pHprobereports: “pH=6.2±0.05(stabilized)”
TaskPlannercomputesrequiredcorrection:
• TargetpH:7.4
• CurrentpH:6.2
• Requiredchange: +1.2pHunits
• NaOHvolumeneeded: 6µLof1Msolution
Timet=6: Safety-ConstrainedExecution MotorControlexecutestitrationwithsafetychecks:
1. Aspirate6µLNaOH(verify: nodrips)
2. Positionpipette5mmabovesolutionsurface
3. Dispenseat1µL/secondrate
4. Mixgentlyfor10seconds
5. Verifynospillsdetected
Timet=7: ConfirmationandLearning Finalobservations:
• pHindicator: green(neutral)
• pHprobe: 7.38±0.05
• Spilldetector: negative
• Timeelapsed: 4minutes
A.4HierarchicalBeliefUpdates
Thesuccessfulexecutiontriggersbeliefupdatesacrossalllevels:
VisionSystem:
• Updated: “YellowindicatorisreliablepredictorofpH<7.0(confidence: 0.98)”
• Newbelief: “GreencolorcorrespondstopH7.3-7.5(confidence: 0.92)”
TaskPlanner:
• Refinedtransitionmodel: “6µL1MNaOHraises1mLbufferpHby1.18±0.05units”
• Policypreferenceupdate: “pHverificationbeforetitrationreducesVFEby70%”
ExecutiveController:
• Meta-learning: “Unexpectedacidificationoccursin15%ofassays(upfromprior5%)”
• Resourceplanning: “Allocateextra2minutesforpHadjustmentinfutureprotocols”
15
A.5FreeEnergyAccounting
ThecompleteexecutiondemonstratesVFEandEFEdynamics:
Stage VFE Explanation
Initialstate 0.5 Baselineuncertainty
Yellowobservation 3.2 Highsurprise/predictionerror
Post-measurement 0.8 Reduceduncertaintyaboutstate
Post-correction 0.4 Belowbaseline(successfulprediction)
A.6SafetyAnalysis
Thetracedemonstratesmultiplesafetymechanisms:
1. Preferenceencoding: SafetyconstraintsexplicitlyrepresentedinCmatrix
2. Policyevaluation: EFEcalculationnaturallypenalizesriskyactions
3. Hierarchicaloversight: Executivecontrollervalidatessafety-criticaldecisions
4. Continuousmonitoring: Spilldetectorsandsafetychecksthroughoutexecution
A.7ComparisonwithTraditionalRL
Thisexecutionhighlightskeyadvantagesoverreward-engineeredRL:
Aspect TraditionalRL ActiveInference
UnexpectedpH Requires pre-programmed reward for Surprisenaturallytriggersbeliefupdate
pHcorrection andcorrection
Safetyhandling Needsexplicitpenaltytermsforspills Safety preferences integrated in EFE
minimization
Exploration ϵ-greedyorcuriositybonusesneeded Informationgaintermdrivesappropriate
exploration
Adaptation Reward function unchanged; learning Beliefsandmodelsupdateimmediately
slow fromexperience
A.8Pseudocode
Algorithm1LLM-AIFControlLoop(singletimestep)
Require: Observationo ,PreferencesC,LLMworldmodelΦ,PriorbeliefsQ(s )
t t−1
Ensure: UpdatedbeliefsQ(s ),Selectedactiona
t t
1: BeliefUpdate: QueryΦwithcontext(o t ,Q(s t−1 ))togenerateposteriorQ(s t )
2: PolicyGeneration: Foreachcandidatepolicyπ i ∈Π:
3: QueryΦtopredictfuturestates: s˜,o˜∼P(s,o|π i )
4: Computeinformationgain: IG i =D KL [Q(s˜|o˜,π i )||Q(s˜|π i )]
5: Computepreferencealignment: PA i =E[lnP(o˜|C)]
6: Calculateexpectedfreeenergy: G(π i )=−IG i −PA i
7: PolicySelection: π∗ =argmin πi G(π i )
8: ActionExecution: Extractfirstactiona t frompolicyπ∗
9: ModelUpdate: Ifpredictionerror>θ,updateΦviafew-shotexamples
10: returnQ(s t ),a t
ThiscompletetracedemonstrateshowtheLLM-AIFarchitectureenablesautonomous, safe, and
adaptivebehaviorwithoutexternalrewardengineering,directlysupportingourpositionthatActive
InferenceprovidesthemissingfoundationfortheEraofExperience.
16

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "The Missing Reward: Active Inference in the Era of Experience"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
