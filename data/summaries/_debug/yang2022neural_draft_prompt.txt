=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: A Neural Active Inference Model of Perceptual-Motor Learning
Citation Key: yang2022neural
Authors: Zhizhuo Yang, Gabriel J. Diaz, Brett R. Fajen

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2022

Abstract: Theactiveinferenceframework(AIF)isapromisingnewcomputationalframeworkgroundedin
contemporaryneurosciencethatcanproducehuman-likebehaviorthroughreward-basedlearning.
In this study, we test the ability for the AIF to capture the role of anticipation in the visual
guidanceofactioninhumansthroughthesystematicinvestigationofavisual-motortaskthat
hasbeenwell-explored–thatofinterceptingatargetmovingoveragroundplane. Previous
researchdemonstratedthathumansperformingthistaskresortedtoanticipatorychangesi...

Key Terms: rochester, rochesterinstituteoftechnology, computersciencedepartment, neural, diaz, learning, perceptual, inference, active, model

=== FULL PAPER TEXT ===

A NEURAL ACTIVE INFERENCE MODEL OF
PERCEPTUAL-MOTOR LEARNING
ZhizhuoYang GabrielJ.Diaz
ComputerScienceDepartment ChesterF.CarlsonCenterforImagingScience
RochesterInstituteofTechnology RochesterInstituteofTechnology
Rochester,NY14623 Rochester,NY14623
zy8981@rit.edu gabriel.diaz@rit.edu
BrettR.Fajen ReynoldBailey AlexanderOrorbia
CognitiveScienceDepartment ComputerScienceDepartment ComputerScienceDepartment
RensselaerPolytechnicInstitute RochesterInstituteofTechnology RochesterInstituteofTechnology
Troy,NY,USA Rochester,NY14623 Rochester,NY14623
fajenb@rpi.edu rjb@cs.rit.edu ago@cs.rit.edu
ABSTRACT
Theactiveinferenceframework(AIF)isapromisingnewcomputationalframeworkgroundedin
contemporaryneurosciencethatcanproducehuman-likebehaviorthroughreward-basedlearning.
In this study, we test the ability for the AIF to capture the role of anticipation in the visual
guidanceofactioninhumansthroughthesystematicinvestigationofavisual-motortaskthat
hasbeenwell-explored–thatofinterceptingatargetmovingoveragroundplane. Previous
researchdemonstratedthathumansperformingthistaskresortedtoanticipatorychangesinspeed
intendedtocompensateforsemi-predictablechangesintargetspeedlaterintheapproach. To
capturethisbehavior,ourproposed“neural”AIFagentusesartificialneuralnetworkstoselect
actionsonthebasisofaveryshorttermpredictionoftheinformationaboutthetaskenvironment
that these actions would reveal along with a long-term estimate of the resulting cumulative
expectedfreeenergy. Systematicvariationrevealedthatanticipatorybehavioremergedonly
whenrequiredbylimitationsontheagent’smovementcapabilities,andonlywhentheagent
wasabletoestimateaccumulatedfreeenergyoversufficientlylongdurationsintothefuture. In
addition,wepresentanovelformulationofthepriorfunctionthatmapsamulti-dimensional
world-statetoauni-dimensionaldistributionoffree-energy. Together,theseresultsdemonstrate
theuseofAIFasaplausiblemodelofanticipatoryvisuallyguidedbehaviorinhumans.
Keywords Activeinference·Interception·Locomotion·Anticipation
1 Introduction
Theactiveinferenceframework(AIF)[23]isanemergingtheoryofneuralencodingandprocessingthatcaptures
awiderangeofcognitive,perceptual,andmotorphenomena,whilealsoofferinganeurobiologicallyplausible
meansofconductingreward-basedlearningthroughthecapacitytopredictsensoryinformation. Thebehaviorof
anAIFagentinvolvestheselectionofaction-plansthatspanintothenearfutureandcentersaroundthelearningof
2202
voN
61
]CN.oib-q[
1v91401.1122:viXra
Preprint,WorkinProgress
Figure 1: A top-down view of the interception problem. The agent (triangle) and target (circle) approach the
invisible interception point (square) by going straight ahead. Ψ denotes the exocentric direction of the target
(bearingangle)andαdenotesthetarget’sapproachangle. Imageadaptedfrom[11].
aprobabilisticgenerativemodeloftheworldthroughinteractionwiththeenvironment. Ultimately,theagentmust
takeactionsuchthatitismakingprogresstowardsitsgoals(goal-seekingbehavior)whilealsobalancingthedrive
toexploreandunderstanditsenvironment(informationmaximizingbehavior),adjustingtheinternalstatesofits
worldtobetteraccountfortheevidencethatitacquiresovertime. Asaresult,AIFunifiesperception,action,and
learningbyframingthemasprocessesthatresultfromapproximateBayesianinference.
TheAIFframeworkhasbeenusedtostudyavarietyofreinforcementlearning(RL)tasks,includingtheinverted
pendulumproblem(CartPole)[35,43],themountaincarproblem(MountainCar)[23,47,45,5,43]andthefrozen
lakeproblem(FrozenLake)[41]. Eachtaskplacesdifferentdemandsonmotorandcognitiveabilities. Forinstance,
CartPolerequiresonlinecontrolofapaddletobalanceapoleupright,whereasMountainCarrequiresintelligent
explorationofthetaskenvironment;asimple“greedy”policy(typicalofmanymodern-dayRLapproaches)would
failtosolvetheproblem. ThepopularFrozenLakerequiresskillsrelatedtospatialnavigationandplanningifthe
agentistofindthegoalwhileavoidingunsafestates.
Onefundamentalaspectofhumanandanimalbehaviorthathassofarnotbeensufficientlystudiedfromanactive
inferenceperspectiveistheon-linevisualguidanceoflocomotion. On-linevisualguidancecomprisesaclassof
ecologicallyimportantbehaviorsforwhichmovementsofthebodyarecontinuouslyregulatedbasedoncurrently
availablevisualinformationseenfromthefirst-personperspective. Someofthemostextensivelystudiedtasks
includesteeringtowardagoal[50],negotiatingcomplexterrainonfoot[10,34],interceptingmovingtargets[14],
brakingtoavoidacollision[13,52],andinterceptingaflyball[6,12]. Foreachofthesetasks,researchershave
formulatedcontrolstrategiesthatcapturethecouplingofvisualinformationandaction.
Oneaspectofon-linevisualguidancethatAIFmightbeparticularlywell-suitedtocaptureisanticipation. To
successfullyperformthesekindsoftasks,actorsmustbeabletoregulatetheiractionsinanticipationoffuture
events. Oneapproachtocapturinganticipationinvisualguidanceistoidentifysourcesofvisualinformationthat
specifyhowtheactorshouldmoveatthecurrentinstantinordertoreachthegoalinthefuture. Forexample,when
runningtointerceptamovingtarget,thesufficiencyoftheinterceptor’scurrentspeedisspecifiedbytherateof
changeintheexocentricvisualdirectionofthetarget,orbearingangle(Figure1). Iftheinterceptorisabletomove
soastomaintainaconstantbearingangle(CBA),thenaninterceptionisguaranteed. Suchaccountsofanticipation
areappealingbecausetheyavoidtheneedforplanningonthebasisofpredictionsorextrapolationsoftheagent’s
ortarget’smotion,therebypresumablyrequiringfewercognitiveresourcesfortaskexecution. Similaraccountsof
anticipationinthecontextoflocomotorcontrolhavebeendevelopedforflyballcatching[6]andbraking[33].
However, there are other aspects of anticipatory control that are more difficult to capture based on currently
availableinformationalone. Forexample,movingtargetssometimeschangespeedsanddirectionsinwaysthatare
somewhatpredictable,allowingactorstoaltertheirmovementinadvanceinanticipationofthemostlikelychange
2
Preprint,WorkinProgress
intargetmotion. Thiswasdemonstratedinapreviouslypublishedstudyinwhichsubjectswereinstructedtoadjust
theirself-motionspeedwhilemovingalongalinearpathinordertointerceptamovingtargetthatchangedspeed
partwaythrougheachtrial[11]. Thefinaltargetspeedrandomlyvariedbetweentrialssuchthatthetargetusually
acceleratedbutoccasionallydecelerated. Inresponse,subjectsquicklylearnedtoadjusttheirspeedduringthefirst
partofthetrialinanticipationofthechangeintargetspeedthatwasmostlikelygivenpastexperienceandthe
initialconditionsofthattrial.
Activeinferenceoffersapotentiallyusefulframeworkforunderstandingandmodelingthiskindofanticipatory
behavior. ThebehaviorofanAIFagentinvolvestheselectionofactionplans(orpolicies)thatspanintothenear
future. Theseplansareselectedbasedonexpectedfreeenergy(EFE),i.e.,asignalthattakesintoaccountboththe
action’scontributiontoreachingadesiredgoalstate(i.e.,aninstrumentalcomponent),andthenewinformation
gained by the action (i.e., an epistemic component). This method of action selection is ideal for the study of
predictive and anticipatory behavior in that it allows for the selection of action plans that do not immediately
contributetotaskcompletion,butthatrevealtotheagentsomethingpreviouslyunknownabouthowtheagent’s
actionaffectstheenvironment. Similarly,inthetaskpresentedin[11],thehumanparticipantslearnedthatsuccess
requiredincreasingspeedearlyinthetrialinordertoincreasethelikelihoodofaninterceptionafterthetarget’s
semi-predictablechangeinspeed. Critically,thisearlychangeinspeedwasnotmotivatedbycurrentlyavailable
visualinformation,butratherbythepositivereinforcementofactionsselectedintheprocessoftaskexploration.
In contrast to reinforcement learning methods, active inference (AIF) formulates action-driven learning and
inferencefromaBayesian,belief-basedperspective[41,40]. Generally,AIFoffers: 1)flexibilitytodefineaprior
preference(orpreferredoutcome)overtheobservationspace(whichpushestheagenttouncovergoal-orienting
policies),whichprovidesanalternativetodesigningarewardfunction,2)aprincipledtreatmentforepistemic
explorationasameansofuncertaintyreduction,informationgain,andintrinsicmotivation[38,42,40],and3)an
encompassinguncertaintyorprecisionoverthebeliefsthatthegenerativemodeloftheAIFagentcomputesasa
naturalpartofthenagent’sbeliefupdating[38]. Despitebeingapopularandpowerfulframeworkofperception,
action[17,18,20,4],decision-makingandplanning[29,39]withbiologicalplausibility,AIFhasbeenmostly
appliedtoproblemswithalow-dimensionalityandoftendiscretestatespaceandactions[21,20,24,25,22]. We
referto[7]foracomprehensivereviewonAIF.
Thepresentstudymakesseveralcontributionstotheunderstandingofvisuallyguidedactionandactiveinference:
• Wepresentanovelmodelforlocomotorinterceptionofatargetthatchangesspeedssemi-predictably,
asin[11]. ThismodelisageneralizationofAIFwhereEFEistreatedasanegativevaluefunctionin
reinforcementlearning(RL)[43]anddeepRLmethodologyisutilizedtoscaleAIFtosolvetaskssuchas
locomotorinterceptionwithcontinuousstatespaces. Specifically,ourmethodpredictsaction-conditioned
EFEvalueswitharecognitionnetwork(see2.4.2)andbybootstrappingonthecontinuousobservation
spaceoveralongtimehorizon. Thisallowstheagenttoaccountforthelong-termeffectsofitscurrent
chosenaction(s).
• Tocalculatetheinstrumentalvalue,wedesignedaproblem-specificpriorfunctiontoconverttheoriginal
observationsintoaone-dimensionalpriorspacewhereapriorpreferencecanbe(moreeasily)specified.
Thisallowsustoinjectdomainknowledgeintotheinstrumentalreward.Theinstrumentalmeasurementsin
priorspacesimultaneouslypromoteinterpretabilityaswellascomputationallyefficienttaskperformance.
• Wepresentacomparisonoftaskperformanceofabaselinedeep-Qnetwork(DQN)agent,oranAIFagent
inwhichEFEiscomputedusingonlytheinstrumentalsignal/component,withafullAIFagentinwhich
EFEiscomputedusingbothinstrumentalandepistemicsignals/components.
• We demonstrate behavioral differences among our full AIF agent under the influence of two varying
parameters: thediscountfactorγ,whichdescribestheweightonfutureaccumulatedquantitieswhen
calculating EFE value at each time step, and pedal lag coefficient K, which specifies how responsive
3
Preprint,WorkinProgress
changesinpedalpositionisreflectedonagent’sspeed(ortheamountofinertiathatisassociatedwiththe
agent’svehicle).
• Weinterpretourfindingsinasamodelforanticipationinthecontextofvisuallyguidedactionaswellas
intermsofspecificcontributionstotheactiveinferenceandmachinelearningcommunities.
2 MaterialsandMethods
Ouraiminthisstudywastodevelopanagentthatselectsfromasetofdiscreteactionsinordertoperformthe
taskofinterception. Inthissection,wedescribethetaskthatweaimtosolveaswellasformallydescribetheAIF
modeldesignedtotackleit. Westartwiththeproblemformulationandbriefnotationanddefinitions,thenmoveon
todescribeourproposedagent’sinferenceandlearningdynamics.
2.1 ThePerceptual-MotorProblem: InterceptingaMovingTarget
Wedesignedandsimulatedaperception-motorproblembasedonthehumaninterceptiontaskusedby[11]. In
theoriginalstudy, subjectssatinfrontofalargerear-projectionscreendepictinganopenfieldwithaheavily
texturedgroundplane. Thesubject’staskwastointerceptamovingsphericaltargetbycontrollingthespeedof
self-movementalongalineartrajectorywithafootpedal,thepositionofwhichwasmappedontospeedaccording
toafirst-orderlag. Subjectsbeganeachtrialfromastationarypositionataninitialdistancesampleduniformly
frombetween25and30meters(m)fromtheinterceptionpoint. Thesphericaltargetapproachedthesubject’s
pathatoneofthreeinitialspeeds(11.25,9.47,8.18meters/secondorm/s). Between2.5and3.25safterthetrial
began,thetargetchangedspeedslinearlybyanamountthatwassampledfromanormaldistributionofpossible
finalspeeds. Themeanofthedistributionwas15m/ssuchthattargetspeedusuallyincreased,butoccasionally
decreased(standarddeviationwas5m/s,finalspeedistruncatedbyonestandarddeviationfromthemean). The
changeoftargetspeedtakesexactly500ms.
In[11],subjectswerefoundtoincreasetheirspeedduringtheearlypartofthetrialinordertoanticipatethemost
likelychangeintargetspeed,whichhelpedthemperformatnearoptimallevels. Differencesbetweenthebehavior
ofsubjectsandtheidealpursuerwerealsofoundundersomeconditions. Findingsintheoriginalstudyfurther
yieldedinsightintothestrategiesthathumansadoptwhendealingwithuncertaintyinrealisticinterceptiontasks.
2.2 Notation
We next define the notation and mathematical operators that we will use throughout the rest of this paper. (cid:12)
indicatesaHadamardproduct,·indicatesamatrix/vectormultiplication(ordotproductifthetwoobjectsitis
appliedtoarevectorsofthesameshape),and(v)T denotesthetranspose. ||v|| isusedtorepresentthep-norm
p
wherep=2resultsinthe2-normorEuclidean(L2)distance.
2.3 ActionandInputSpaceSpecification
Tosimplifytheproblemforthiswork,weassumethatthemappingbetweenenvironmental(latent)statesand
observations is the identity matrix. Furthermore, we formulate the problem as a Markov Decision Process
(MDP)withadiscreteactionspace. Theactionspacea (actionvectorattimet)isdefinedasaone-hotvector
t
a∈{0,1}6×1,whereeachdimensioncorrespondstoauniqueactionandtheactionsaremutuallyexclusive. Each
dimensioncorrespondstooneofthepedalspeeds(m/s)in{2,4,8,10,12,14}respectively. Onceapedalspeedis
selected,theagentwillchangeitsownspeedbytheamountof∆V =K∗(V −V )inonetimestepwhereV
p s p
ispedalspeed,V iscurrentsubjectspeedandKisthelagcoefficient. SimilartoTschantzet.al[46],weassume
s
thatthecontrolstatevector(which,inAIF,controlstatesareoriginallytreatedseparatelyfromactionstates)lines
upone-to-onewiththeactionvector,meaningthatittooisavectoroftheformu ∈ {0,1}6×1. Wedefinethe
4
Preprint,WorkinProgress
Figure2: OurneuralAIFarchitecturefortheinterceptiontask. Therecognitionmodelisatwo-headedartificial
neural network which consists of shared hidden layers, an EFE (estimation) head, and a transition dynamics
(prediction)head. TheEFEheadestimatesEFEvaluesforallpossibleactionsgiventhecurrent(latent/hidden)
state. AnactionwhichisassociatedwithmaximumEFEvalueisselectedandexecutedintheenvironmentandthe
resultingobservationisfedintothepriorfunctionwheretheinstrumentalvalueR iscalculatedinpriorspace.
t,i
Meanwhile,thetransitiondynamicsheadpredictstheresultingobservationgiventhecurrent(latent/hidden)state.
Theerrorbetweenthepredictedandactualobservationatt+1formstheepistemicvalueR . Thesummationof
t,e
R andR resultsinthefinalEFE(target)value.
t,i t,e
observation/statespace(o ∈ R4×1)tobea4-dimensionalvectoro = (cid:104)x ,v ,x ,v (cid:105)T,whichcorrespondsto
t t t s s
targetdistance,targetspeed,subjectdistanceandsubjectspeed. Alldistancesaforementionedarewithrespectto
theinvisibleinterceptionpoint.
2.4 NeuralActiveInference
Activeinference(AIF)isa(Bayesian)computationalframeworkthatbringstogetherperceptionandactionunder
onesingleimperative: minimizingfreeenergy. Itaccountsforhowself-organizingagentsoperateindynamic,non-
stationaryenvironments[19],offeringanalternativetostandard,rewardfunction-centricreinforcementlearning
(RL).Inthisstudy,wecraftasimpleAIFagentthatresemblesQ-learning[43]wheretheexpectedfreeenergy
(EFE)servestheroleofanegativeaction-valuefunctioninRL.WeframethedefinitionofEFEinthecontextofa
stochasticpolicyandcastaction-conditionedEFEasanegativeaction-valueusingapolicyφ=φ(a |s )(where
t t
s =o asperourassumptionearlier). Thesamepolicyφisusedforeachfuturetimestepτ,andtheprobability
t t
distributionoverthefirst-stepactionisseparatedfromφresultinginasubstitutiondistributionq(a )forφ(a ).
t t
Therefore,theone-stepsubstitutedEFEcanbeinterpretedastheEFEofapolicyof(φ(a ),φ(a ),...,φ(a ))).
t t+1 T
Asin[43],wesimplifyandapproximatethesearchforoptimalEFEvaluesbyadaptinganestimationapproach
basedontheBellmanequation,arrivingataQ-learningbootstrapscheme. NotethattheQ-learningstyleframingof
negativeEFEestimationisreferredtoasG-learning. WeutilizetheAIFframeworkwithintheG-learningframing
fortheinterceptiontaskandmodifytheframeworktofittheinterceptiontask,seeFigure2. Spatialvariables,i.e.
distanceandspeed,willserveastheinputstoourframeworkand,asmentionedbefore,anidentitymappingis
assumedtoconnecttheobservationdirectlytothestatevariables(allowingustoavoidhavingtolearnadditional
5
Preprint,WorkinProgress
Figure3:Thepriorpreferencespecifiedinthepriorspacewhereeachactioncorrespondstoadifferentinstrumental
value. Circlescorrespondtopedalpositionstochoosefrom.
parameterizedencoder/decoderfunctions). Asaresult,theAIFagentwedesignedforthispaper’sexperiments
consistsoftwomajorcomponents: apriorfunctionandamulti-headedrecognitionneuralmodel.
Notably,ourparticularproposedrecognitionmodelworksjointlyasafunctionapproximatorofEFEvaluesaswell
asaforwarddynamicspredictor. Ittakesinthecurrentobservationo asinputandthenconducts,jointly,action
t
selectionandnext-stateprediction(aswellasepistemicvalueestimation). Theselectedactionisexecutedandthe
resultingobservationisreturnedbytheenvironment. Thepriorfunctionitselftakesinasinputthenextobservation
o , the consequence/result of the agent’s currently selected action, and calculates the log likelihood of the
t+1
preferred/priordistribution(setaccordingtoexpertknowledgerelatedtotheproblem),ortheinstrumentalterm
R . Thedifferencebetweentheoutcomeoftheselectedactiono anditsestimationoˆ (asperthegenerative
t,i t+1 t+1
transition component of our model) forms the epistemic term R . The summation of the instrumental and
t,e
epistemictermsformstheG-value(ornegativeEFEvalue)whichisultimatelyusedtotrain/adapttherecognition
model. Weexplaineachcomponentindetailbelow.
2.4.1 ThePriorPreferenceFunctionandPriorSpace
Withtheabilityandfreedomofdesigningapriorpreference(ordistributionoverproblemgoalstates)afforded
byAIF,weintegratedomainknowledgeoftheinterceptiontaskintothedesignofapriorfunction. Inessence,
ourdesignedprior functiontransformstheoriginalobservationvector o toalower-dimensionalspace where
t
asemanticallymeaningfulvariableiscalculated(thepriorspace)andpriorpreferencedistributionisspecified
over this new variable – in our case, this is set to be the speed difference, as shown in Figure 3. The speed
differencerepresentsthedifferencebetweentheagent’sspeedaftertakingtheselectedactionandthespeedrequired
forsuccessfulinterception,i.e. speeddifference = speed −speed . Giventhecurrentobservation,
agent required
therequiredspeediscalculatedastheagent’sdistancetotheinterceptionpointdividedbythefirst-ordertarget
time-to-contact(TTC).Werefertothispriorfunctionasthefirst-orderprior. InourneuralAIFframework,the
instrumentalvaluesarecalculatedgivenallpossibleactions(bluecirclesinFigure3)andapriordistributionover
speeddifference. Thesmallerthespeeddifferenceassociatedwithaparticularaction,thehighertheinstrumental
valueourfunctionassigns.
Notethattheagentmightnothaveenoughtimetoadjustitsspeedlaterintheinterceptiontaskifitonlyfollows
theguidanceofthispriorfunctionwithoutanticipatingthelikelyfuturespeedchangeoftarget,sincethisprior
functiononlyaccounts/embodiesfirst-orderinformation. Toovercomethislimitation,weinvestigatedtheeffects
ofdiscountedlong-termEFEvalueonthebehavioroftheagentinSection3.4.
2.4.2 Recognitionmodel
Our proposed recognition model embodies two key functionalities: EFE estimation and transition dynamics
prediction,whicharetypicallyimplementedasseparateartificialneuralnetworks(ANNs)inearlierAIFstudies[43]
6
Preprint,WorkinProgress
(incontrast,wefoundthat,duringpreliminaryexperimentation,thatajoint,fusedarchitectureimprovedboththe
agent’soverallgeneralizationabilityaswellasitstrainingstability). Concretely,weimplementtherecognition
modelasamulti-headedANNwithanEFEheadandatransitionhead(seeFigure2). Thesystemtakesinthe
currentobservationo andpredicts: 1)theEFEvaluesforallpossibleactions,and2)afutureobservationata
t
distanceo (inthiswork, wefixthetemporaldistancetobeonestep, i.e. D = 1). Withintherecognition
t+D
model,thecurrentobservationo istakenasinputandalatenthiddenactivityvectorz isproduced,whichisthen
t t
providedtobothoutputheadsasinput. Thetransitionheadp(o |z )servesasagenerativemodel(oraforward
t+D t
dynamicsmodel)andtheEFEheadG (z ,a)representsavariationaldensityovertheEFEvalues. Asaresult,
φ t
EFEmoduleandtransitionmodulesarewiredtogethersuchthatthepredictionofthefutureobservationo
t+D
andtheestimationofEFEvaluesG aredrivenbythesharedencodingfromthetopmost(hidden)layerofthe
t+D
recognitionmodel. Thisenablesthesharingofunderlyingknowledgebetweenthemoduleselectingactionsand
themodulepredictingtheoutcome(s)ofanaction. Ourintuitionisthatwehumanstendtoevaluatethe“value”of
anactionbytheconsequencesthatitproduces.
Wenextformallydescribethedynamicsofourrecognitionmodel,specificallyitsinferenceandlearningprocesses.
Inference: Ingeneral,ouragentismeanttoproduceanactionconditionedonobservations(orstates)sampled
fromtheenvironmentatparticulartime-steps. Specifically,withinanygivenT-stepepisode,ouragentreceives
asinputtheobservationo ∈RD×1,whereDisthedimensionalityoftheobservationspaceo (D =4forthe
t t
probleminvestigatedinthisstudy). Theagentthenproducesasetofapproximatefreeenergyvalues,oneforeach
action(similarinspirittoQ-values)aswellasapredictionofthenextobservationthatitistoreceivefromits
environment(i.e.,theperceptualconsequenceofitsselectedaction).
Formally,inthiswork,theoutputsdescribedaboveareultimatelyproducedbyamulti-outputfunctionz3,z3 =
a o
f (o ),implementedasamulti-layerperceptron(MLP),wherez3 containsestimatedexpectedfreeenergyvalues
Θ t a
(one per discrete action) while z3 is the generative component’s estimation of the next incoming observation
o
o . Notethatwedenoteonlyoutputtinganactionvaluesetfromthismodelasz3 = fa(o )(usingonlythe
t+1 a Θ t
actionoutputhead)andonlyoutputtinganobservationpredictionasz3 =fo(o )(usingonlythestateprediction
o Θ t
head). ThisMLPisparameterizedbyasetofsynapticweightmatricesΘ={W1,W2,W3,W3},thatoperates
a o
accordingtothefollowing:
z1 =φ (W1·z0), z2 =φ (W2·z1) (1)
z z
z3 =φ (W3·z2)), z3 =φ (W3·z2)) (2)
a a a o a o
where z0 = o (the input layer to our model is the observation at t). Note that a single discrete action is
t
read out/chosen from our agent function’s action output head as: a = argmax fa(o ). The linear rectifier
a Θ t
φ (v) = max(0,v)waschosentobetheactivationfunctionappliedtotheinternallayersofourmodelwhile
z
φ (v)=v(theidentity)isthefunctionspecificallyappliedtotheactionneuralactivitylayerz3 andφ (v)=vis
a a o
thefunctionappliedtopredictedobservationlayerneurons. Notethatthefirsthiddenlayerz1 ∈RJ1×1contains
J
1
neuronsandz2 ∈ RJ2×1 containsJ
2
neurons,respectively. Theactionoutputlayerz3
a
∈ RA×1 containsA
neurons(A=6fortheprobleminvestigatedinthisstudy),oneneuronperdiscreteaction(outofAtotalpossible
actionsasdefinedbytheenvironment/problem),whiletheobservationpredictionlayerz3 ∈RD×1containsD =6
o
neurons,makingitthesamedimensionality/shapeastheobservationspace.
Learning: WhiletherearemanypossiblewaystoadjustthevaluesinsideofΘ,weoptedtodesignacostfunction
andcalculatethegradientsofthisobjectivewithrespecttothesynapticweightmatricesofourmodelforthesake
ofsimulationspeed. Thecostfunctionthatwedesignedtotrainourfullagentwasmulti-objectiveinnatureandis
7
Preprint,WorkinProgress
definedinthefollowingmanner:
L(o ,t;Θ)=L (o ;Θ)+L (t ;Θ) (3)
t+1 a t+1 o t+1
1
L (t;Θ)= ||t−z3||2 (4)
a 2σ2 a 2
a
1
L (o ;Θ)= ||o −z3||2 (5)
o t+1 2σ2 t+1 o 2
o
wherethetargetvaluefortheactionoutputheadiscalculatedast =r +γmax fa(o )whilethetargetaction
j j a Θ t
vectoriscomputedast =t a +(1−a )⊗fa(o ). Intheabovesetofequations,weseethattheMLPmodel’s
j j j j Θ t
weightsareadjustedsoastominimizethelinearcombinationoftwoterms,thecostassociatedwiththedifference
between a target vector t, which contains the bootstrap-estimated of the EFE values, and the agent’s original
estimatez3 aswellasthecostassociatedwithhowfarofftheagent’sprediction/expectationz3ofitsenvironment
a o
isfromtheactualobservationo . Inthisstudy,thestandarddeviationcoefficientsassociatedwithbothoutput
t+1
layersaresettoone,i.e.,σ = σ = 1(highlightingthatweassumeunitvarianceforourmodel’sfreeenergy
a o
estimatesanditsenvironmentalstatepredictions–notethatadynamicvariancecouldbemodeledbyaddingan
additionaloutputheadresponsibleforcomputingthealeatoricuncertaintyassociatedwitho ).
t+1
UpdatingtheparametersΘoftheneuralsystemthenconsistsofcomputingthegradient ∂L(ot+1,t;Θ) usingreverse-
∂Θ
modedifferentiationandadjustingtheirvaluesusingamethodsuchasstochasticgradientdescentorvariants,
e.g.,Adam[30],RMSprop[44]. Specifically,ateachtimestepofanysimulatedepisode,ouragentfirststores
thecurrenttransitionoftheform(o ,a ,r ,o )intoanepisodicmemoryreplaybufferandthenimmediately
t t t t+1
calculates ∂L(ot+1,t;Θ) fromabatchofobservation/transitiondata(uniformly)sampledfromthereplaybuffer,
∂Θ
whichstoresupto105transitions. Wewilldemonstratethebenefitofthisdesignempiricallyintheresultssection.
3 Results
3.1 HypothesesforInterceptionStrategies
Giventhefactthatthetargetchangesitsspeedduringatrialinourinterceptiontask,theagent/humansubject
couldgainadvantagebyanticipatingthetargetspeedchangepriortothechangeoftargetspeed. Toselectan
optimalactionearlywithinthetrail,theagentneedstotakeintoconsiderationtheinitialtargetspeedinthecurrent
trialandmakeadjustmentsbasedontheexperienceacquiredfromprevioustrials. So,thequestionbecomes: how
doestheagentadaptitsbehavioronthebasisofcurrenttrials’observationoftargetspeed/distancefromthe
interceptionpointandthelearnedstatisticsacrosstrials?
3.2 ExperimentalSetup
WeimplementedtheinterceptiontaskasanenvironmentinPythonbasedontheOpenAIgym[3]library. This
integrationprovidesthefullfunctionalityandusabilityofthegymenvironment,whichmeansthattheenvironment
canwork/beusedwithanyRLalgorithmandismadeaccessibletothemachinelearningcommunityaswell. Our
AIFagentsandbaselinealgorithmDQNareimplementedwiththeTensorflow2[1]library. Experimentaldataand
codewillbemadepubliclyavailableuponacceptance.
3.3 TaskPerformance
WecompareAIFagentswithandwithouttheepistemiccomponentandabaselinealgorithm,i.e. adeep-Qnetwork
(DQN).Experimentsareconductedfor20trialswhereeachtrialcontains3000episodes. Thetaskperformanceof
agentsisshownascurvesplottingwindow-averagedrewards(withawindowsizeof100episodes)inFigure4,
wherethesolidlinedepictsthemeanvalueacrosstrialsandtheshadedarearepresentsstandarddeviation. We
conductedasetofexperimentswherethediscountfactorγ ofthemodelsandthepedallagcoefficientKwere
8
Preprint,WorkinProgress
Figure4: Window-averagedrewardmeasurementsofagentperformanceontheinterceptiontask. DQN_Reward
represents a DQN agent that utilized the sparse reward signal and (cid:15)−greedy exploration; AIF_InstOnly
representsourAIFagentwithonlyinstrumentalcomponentwhichisdefinedbythepriorfunction;AIF_InstEpst
representsanAIFagentthatconsistsofbothinstrumentalandepistemiccomponents. Discountfactorisdenoted
byγ,pedallagcoefficientisdenotedbyK.
varied(notethat,inAIFandRLresearch,γ istypicallyfixedtoavaluebetween0.9and1toenablethemodelto
accountforlongtermreturns). Inordertocomparetheperformanceofouragentstothatofhumansubjects,we
applytheoriginalpedallagcoefficientinonesetofourexperiments(specificallyshowninFigure4C).
Observe that our AIF agents are able to reach around a 90% success rate stably with very low variance. This
beatshumanperformancewith47%(std=11.31)onaverageand54.9%inthefinalblockofexperimentsreported
in[11]. ThebaselineDQNagent,whichlearnsfromtheproblem’ssparserewardsignalattheendofeachepisode,
yieldsanaveragesuccessrateof22%attesttime. Similarly,theAIFagentwithbothinstrumentalandepistemic
componentsachievesa90%meansuccessrate.
NotethattheDQNagentisoutperformedbytheAIFagentstrainedwithourcustomizedpriorpreferencefunction
byalargemargin. Thisrevealsthattheflexibilityofinjectingpriorknowledgeiscrucialforsolvingcomplextasks
moreefficientlyandvalidatesourmotivationofapplyingAIFtocognitivetasks. Inourpreliminaryexperiments,
wetestedanAIFagentwhichconsistsofanEFEnetworkandatransitionnetworkseparately. ThisAIFagent
is out-performed by the AIF agent with recognition model in terms of windowed mean rewards and stability.
Furthermore,theAIFagentwithrecognitionmodelhaslowermodelcomplexity. Specifically,AIFagentwith
recognitionmodelhasonly66.8%oftheparametercountsofthatofAIFagentwithseparatemodels. Thissupports
ourintuitionthatcombiningtheEFEmodelwiththetransitionmodelyieldsanoverallbettermodelagent.
Interestingly,theAIFagentwithonlyinstrumentalcomponentwasabletonearlyreachthesamelevelofperfor-
manceasthefullAIFagent. However,successrateofthisagentexhibitedalargervariancethanthefullAIFagent.
9
Preprint,WorkinProgress
Basedoncomparisonbetweenagentswithandwithoutepistemiccomponent,wearguethatepistemiccomponent
serves,atleastinthecontextoftheinterceptiontaskweinvestigate,asaregularizerfortheAIFmodels,providing
improvedrobustness. SinceweapplyexperiencereplayandbootstrappingtotraintheAIFmodels,itispossible
thatalocalminimumisreachedintheoptimizationprocessbecausethereplaybufferisfilledupwithsamples
whichcomefromthesamesubspaceasthestatespace. Therefore,withthehelpofepistemiccomponent,theagent
isencouragedtoexploretheenvironmentmoreoftenandadjustsitspredictionoffutureobservationssuchthatit
hasahigherchanceofescapingpoorerlocaloptima. OurproposedAIFagentreachesaplateauinperformance
afterabout1000episodesandstabilizesmoreafter1500episodes. Notethat,incontrast,humansubjectswereable
toperformthetaskatanaveragesuccessrateafter9episodesofinitialpractice[11].
3.4 AnticipatoryBehaviorofAIFAgents
DotheAIFagentsexhibitasimilarcapacityforanticipatorybehaviorashumansdo? Toanswerthisquestionand
tocomparethestrategyusedbyourAIFagentstothatofhumansubjects,werecordtheTime-To-Contact(TTC)
fromtrainedAIFagentsattheonsetofthetarget’sspeedchangeineachepisode. Wethencalculate,atthesame
time: 1)thetarget’sTTCusingfirst-orderinformation,and2)target’sTTCwiththeassumptionthatthetarget
wouldchangeitsspeedatthemostlikelytimeandreachanaveragedfinalspeed. Finally,wecomposethesethree
typesofTTCdatagroupedbytargetinitialspeedintoasingleboxplotinFigure5.
Figure5: TTCvaluestakenattheonsetoftarget’sspeedchange. Ineachsubplot,thetarget’sfirst-orderTTC,the
target’sactualmeanTTC,andtheagent’sTTCareshownindifferentcolors,withdatagroupedbytargetinitial
speed. Thediscountfactorisdenotedbyγ whilethepedallagcoefficientisdenotedbyK.
Inourexperimentalanalysis,wefoundthatthediscountfactorγ playsabigroleinformingdifferentbehavior
patternswithinAIFagents. AllvariantsofAIFagentsweretrainedwiththeinstrumentalvaluecomputedusing
ourfirst-orderpriorfunction. Intuitively,theagent’sbehaviorshouldconformtoareactiveagentwhousesonly
10
Preprint,WorkinProgress
thefirst-orderinformationandactstomatchitsownTTCtothetarget’sfirst-orderTTC,justlikewhathasbeen
observedinFigure5A(pleaseseethatthegreenboxisnearlyidenticaltotheblueboxunderalltargetinitial
conditions). TheAIFagentdepictedinFigure5Aissettouseadiscountfactorof0,whichmeansthattheagent
onlyseekstomaximizeitsimmediaterewardwithoutconsideringthelong-termimpactoftheaction(s)thatit
selects. Suchanagentconvergestoareactivebehavior. However,whenweincreasethediscountfactorto0.99
(whichisacommonpracticeinRLliterature),theAIFagentstartstobehavemoreinterestingly. InFigure5C,the
agent’sTTC(greenbox)liesinbetweentarget’sfirst-orderTTC(bluebox)andtarget’sactualmeanTTC(orange
box),whichsuggeststhattheAIFagenttendstomovefasterthanapure-reactive,first-orderagentwouldinthe
earlyphaseofinterception. Inotherwords,theagenttendstoanticipatethelikelytargetspeedchangeinthefuture
andadjustsitsactionselectionpolicy. Thisbehavioralpatterncanbeexplainedasexploitingthebenefitsprovided
byestimatinglong-termaccumulatedinstrumentalrewardsignal(whenthediscountfactorvalueisincreased).
Givenahigherdiscountfactor,inthiscaseγ =0.99,theAIFagentestimatesthesummationofinstrumentalvalues
fromitscurrent(time)stepinthetaskuntiltheendoftheinterceptionusingdiscounting. Thisleadstoanagent
whoseekstomaximizelong-termbenefitsintermsofreachingthegoalwhenselectingactions.
3.5 EffectofVehicleDynamicsonAgentBehavior
Totesthowanticipatorybehaviorisaffectedwhensimplereactivebehaviorisnolongersufficient,weincreased
theinertiaontheagent’svehiclebychangingthepedallagcoefficientK. Giventhesamediscountfactorγ =0.99,
wecomparetwodifferentpedallagcoefficientsK = 1.0inFigure5CandK = 0.5inFigure5D,wherelower
Kindicateslessresponsivevehicledynamics. Withthesamediscountfactor,theAIFagentperformingthetask
underalowerpedallagcoefficientinFigure5Dhasalowersuccessrateininterceptingthetarget. Thisisdueto
thefactthattheagent’sabilitytomanipulateitsownspeedislimited,thereforethereislessroomleftforerror.
However,theAIFagentinthisconditionyieldsTTCvaluesthatareclosertothetarget’sactualmeanTTC.Note
that,whenthetargetinitialspeedis11.25m/s(Figure5D),themedianofagent’sTTCvalueisactuallysmaller
thantarget’sactualmeanTTC.Thissupportsourhypothesisthatpurelyreactivebehaviorisnotsufficientfor
successfulinterceptionandanticipatorybehaviorisemergentwhenthevehiclebecomeslessresponsive.
4 Discussion
VariationsofanAIFagentweretrainedtomanipulatethespeedofmovementsoastointerceptatargetmoving
acrossthegroundplane,andeventuallyacrosstheagent’slinearpathoftravel. Oneachtrial,thetargetwould
change in speed on most trials to a value that was selected from a Gaussian distribution of final speeds. The
resultsdemonstratethattheAIFframeworkisabletomodelbothon-linevisualandanticipatorycontrolstrategies
inaninterceptiontask,aswaspreviouslydemonstratedbyhumansperformingthesametask[11]. Theagent’s
anticipatorybehavioraimedtomaximizethecumulativeexpectedfreeenergyinthedurationthatfollowsaction
selection. Variationoftheagent’sdiscountfactormodifiedthelengthofthisduration. Atlowerdiscountfactors,the
agentbehavedinareactivemannerthroughouttheapproach,consistentwiththeconstantbearinganglestrategyof
interception. Athighervalues,actionsthatwereselectedbeforethepredictablechangeinspeedtookintoaccount
themostlikelychangeintargetspeedthatwouldoccurlaterinthetrial. Anticipatorybehaviorwasalsoinfluenced
bytheagent’scapabilitiesforaction.Thisanticipatorybehaviorwasmostapparentwhenthepedallagcoefficient
wassettolowervalues,whichhadtheeffectofchangingtheagent’smovementdynamicssothatpurelyreactive
controlwasinsufficientforinterceptionbehavior.
Despite the agent’s demonstration of qualitatively human-like prediction, careful comparison of the agent’s
behaviortothehumanperformanceandlearningratesdemonstratedin[11]revealsnotabledifferences. Analysis
ofparticipantbehaviorinthefourthandfinalblockofExperiment1in[11]revealsthatsubjectTTCattheonsetof
thetarget’schangeinspeedwaswellmatchedtothemostlikelytimeandmagnitudeofthetarget’slikelychange
11
Preprint,WorkinProgress
Figure6: HumansubjectdatafromExp1. of[11]. TTCsweretakenattheonsetoftarget’sspeedchange. Dotted
linerepresentsthemeanoftarget’sfirst-orderTTC,solidlinerepresentsthemeanoftarget’sactualTTC,black
diskrepresentsthemeanofsubject’sTTCwithabarindicating95%confidenceintervalofthemean.
inspeed(i.e.,themeanactualtargetTTCinFigure6). Incontrast,theAIFagentwithanequivalentpedallag(K=
1.0;i.e. thematched agent)demonstratedonlypartialmatchingofitsTTCtothelikelychangeintargetspeed
(thetarget’smeanactualTTCinFigure5C).Althoughonemightattributethistounder-trainingoftheagent,itis
notablethattheagentachievedahitrateexceeding80%bytheendoftraining,whilehumanparticipantsinthe
originalstudyconsistentlyimprovedinperformanceuntilreaching55%hitrateattheendoftheexperiment.
Tobetterunderstandthepotentialcausesofthesedifferencesbetweenagentandhumanperformance,itishelpful
toconsiderhowtheagent’smechanismforanticipationdiffersfromthatofhumans. Theagentchoosesactionson
thebasisofaweightedcombinationofreward-basedreinforcement(instrumentalreward)andshortmodel-based
prediction(epistemicreward),bothofwhicharecomputedwithinthetwo-headedrecognitionmodel. Instrumental
reward is computed in the EFE head, which is responsible for selecting the action (i.e., pedal position) that it
estimateswouldproducethegreatestexpectedfreeenergylaterintheagent’sapproach. TheestimateofEFE
associatedwitheachpedalpositiondoesnotinvolveanexplicitprocessofmodel-basedprediction,butislearned
retrospectively,throughtheuseofanexperiencereplaybuffer. Followingactionselection,visualfeedbackprovides
anindicationofthecumulativeEFEoverthedurationofthereplaybuffer. ThevaluesofEFEwithinthisbuffer
areweightedbytheirtemporaldistancefromtheselectedactioninaccordancewiththeparameterofdiscount
factor. Thisissimilartobothreward-basedlearningandisoftencomparedtothedopaminergicrewardsystem
inhumans[26,32,28,36]. TheepistemiccomponentoftheEFErewardsignalisthoughttodriveexploration
towardsuncertainworldstates,anditreliesonpredictionsmadeinthetransitionhead. Thiscomponentofthe
modelreliesonthehiddenstatesprovidedbythesharedneurallayersintherecognitionmodelandpredictsan
observationatnexttimestepˆo . Theestimatedobservationatnexttimestepisthencomparedtothegroundtruth
t+1
observationo andthedifferencebetweenthemgeneratestheepistemicsignalR . Theroleofthetransition
t+1 t,e
headisinmanywaysconsistentwitha“strongmodel-based”formofprediction[53],wherebypredictivebehaviors
areplannedonthebasisofaninternalmodelofworldstatesanddynamicsthatfacilitatecontinuousextrapolation.
Insummary,whereastheEFEheadisconsistentwithrewardbasedlearning,thetransitionheadisconsistentwith
relativelyshort-termmodelbasedprediction.
Howdoesthisaccountofanticipationdemonstratedbyouragentcomparewithwhatweknowaboutanticipation
inhumans? Asdiscussedintheintroduction,empiricaldataonthequalityofmodel-basedpredictionsuggests
thatitdegradessufficientlyquicklythatitcannotexplainbehaviorsofthesortdemonstratedhere,byouragent,
orbythehumansin[11]. Incontrast,acommontheoryinmotorcontrolandlearningreliesuponacomparison
12
Preprint,WorkinProgress
of a very short-term prediction (e.g. milliseconds) of self-generated action with immediate sensory feedback
[2,51,27,48]. However,thissimilarityisweakenedbytheobservationthat,inthecontextofmotor-learning,
short-termpredictionisthoughttorelyuponaccesstoanefferentcopyofthemotorsignalusedtogeneratethe
action. Forthisreason, itisproblematicthattheAIFagentispredictingbothitsownfuturestate(x ,v )and
s s
the future state of the target (x ,v ), for which there is no efferent copy or analogous information concerning
t t
movementdynamics. Althoughresearchoneyemovementshasrevealedevidencefortheshort-termprediction
of future object position and trajectory [9, 8, 15], it remains unclear whether these behaviors are the result of
predictivemodelsofobjectdynamicsorrepresentation-minimalheuristics.
Anotherpossiblecontributiontotheobserveddifferencesbetweenagentandhumanperformanceistheperceptual
input. Whenconsideringpotentialcausesforthedifferencebetweenagentandhumananticipatorybehavior,it
isnotablethattheagentreliesuponanobservationvectordefinedbyagent’sandtarget’spositionandvelocity
measuredinmeters,andmeterspersecond,respectively. However,inthenaturalcontext,thesespatialvariables
mustberecoveredorestimatedonthebasisofperceptualsourcesofinformation,suchastherateofglobaloptic
flowduetotranslationoverthegroundplane,theexocentricdirectionofthetarget,theinstantaneousangularsize
ofthetarget,ortheloomingrateofthetargetduringtheagent’sapproach. Itispossiblethatbydeprivingtheagent
oftheseopticalvariables,wearealsodeprivingtheagentofopportunitiestoexploittask-relevantrelationships
betweentheagentandenvironment,suchasthebearingangle. Itisalsonotablethatsomeperceptualvariables
mayprovideredundantinformationaboutaparticularspatialvariable(e.g. bothchangeinbearingangleandrate
ofchangeinangularsizemaybeinformativeaboutanobjectsapproachspeed). However,redundantvariables
willdifferinreliabilitybyvirtueofsensorythresholdsandresolutions. Forthesereasons,amorecompleteand
comprehensivemodelofhumanvisuallyguidedactionandanticipationwouldtakeasinputpotentialsourcesof
informationandlearntoweightthemaccordingtocontext-dependentreliabilityandvariability.
Anotherpotentialcontributortodifferencesbetweenhumanandagentperformanceisthenotablelackofvisuo-
motordelayswithintheagent’sarchitecture. Incontrast,humanvisuo-motordelayhasbeenestimatedtobeon
theorderof100-200msbetweenthearrivalofnewvisualinformationandthemodificationorexecutionofan
action[37,31]. Becauseuncompensateddelayswouldhavedevastatingconsequencesonhumanvisualandmotor
control,theyareoftencitedasevidencethathumansmusthavesomeformofpredictivemechanismthatactsin
compensation[51]. Futureattemptstomakethismodel’santicipatorybehaviormorehuman-likeinnaturemay
dosobyimposingsimilarlengthdelaybetweentheagent’schoiceofmotorplanonthebasisoftheobserved
world-stateandthetimethatthismotorplanisexecuted[49].
4.1 Conclusion
We present a novel generalized active inference framework (AIF) model for studying online visually guided
locomotionusinganinterceptiontaskwhereamovingtargetchangesitsspeedsinasemi-predictablemanner. In
ordertodrivetheagenttowardsthegoalmoreeffectively,wedevisedaproblem-specificpriorfunction,improving
theagent’scomputationalefficiencyandinterpretability. Notably,wefoundthatourproposedAIFagentexhibits
bettertaskperformancewhencomparedtoacommonlyusedRLagent,i.e. thedeep-Qnetwork(DQN).Thefull
AIFagent,containingbothinstrumentalandepistemiccomponents,exhibitedslightlybettertaskperformanceand
lowervariancecomparedtotheAIFagentwithonlyaninstrumentalcomponent. Furthermore,wedemonstrated
behavioraldifferencesamongourfullAIFagentsgivendifferentdiscountfactorγ valuesaswellaslevelsofthe
agent’saction-to-speedresponsiveness. Finally,weanalyzedtheanticipatorybehaviordemonstratedbyouragent
andexaminedthedifferencesbetweentheagent’sbehaviorandhumanbehavior. Whileourresultsarepromising,
futureworkshouldaddressthefollowinglimitations-first,inputstoouragentaredefinedinasimplifiedvector
spacewhereassensoryinputstothehumansthatactuallyperformtheinterceptiontaskarevisualinnature(i.e.,the
modelshouldworkdirectlywithunstructuredsensorydatasuchaspixelvalues). Weremarkthatavision-based
approach could facilitate the extraction of additional information and features that are useful for solving the
13
Preprint,WorkinProgress
interceptiontaskmorereliably. Second,oursimulationsdonotaccountforvisuo-motordelaysinherenttothe
human visual and motor systems, and that might be modeled using techniques like delayed Markov decision
processformulations[49,16].
References
[1] ABADI,M.,AGARWAL,A.,BARHAM,P.,BREVDO,E.,CHEN,Z.,CITRO,C.,CORRADO,G.S.,DAVIS,
A.,DEAN,J., DEVIN,M., GHEMAWAT,S., GOODFELLOW,I., HARP,A., IRVING,G., ISARD,M., JIA,
Y., JOZEFOWICZ, R., KAISER, L., KUDLUR, M., LEVENBERG, J., MANÉ, D., MONGA, R., MOORE,
S.,MURRAY,D.,OLAH,C.,SCHUSTER,M.,SHLENS,J.,STEINER,B.,SUTSKEVER,I.,TALWAR,K.,
TUCKER,P., VANHOUCKE,V.,VASUDEVAN,V.,VIÉGAS,F.,VINYALS,O., WARDEN,P., WATTENBERG,
M.,WICKE,M.,YU,Y.,ANDZHENG,X. TensorFlow: Large-scalemachinelearningonheterogeneous
systems,2015. Softwareavailablefromtensorflow.org.
[2] BLAKEMORE, S.-J., WOLPERT, D. M., AND FRITH, C. D. Centralcancellationofself-producedtickle
sensation. Natureneuroscience1,7(1998),635–640.
[3] BROCKMAN, G., CHEUNG, V., PETTERSSON, L., SCHNEIDER, J., SCHULMAN, J., TANG, J., AND
ZAREMBA,W. Openaigym(2016). arXivpreprintarXiv:1606.01540(2016).
[4] BUCKLEY,C.L.,KIM,C.S.,MCGREGOR,S.,ANDSETH,A.K. Thefreeenergyprincipleforactionand
perception: Amathematicalreview. JournalofMathematicalPsychology81(2017),55–79.
[5] ÇATAL,O.,WAUTHIER,S.,DEBOOM,C.,VERBELEN,T.,ANDDHOEDT,B. Learninggenerativestate
spacemodelsforactiveinference. FrontiersinComputationalNeuroscience14(2020),574372.
[6] CHAPMAN,S. Catchingabaseball. AmericanJournalofPhysics36,10(1968),868–870.
[7] DACOSTA,L.,PARR,T.,SAJID,N.,VESELIC,S.,NEACSU,V.,ANDFRISTON,K. Activeinferenceon
discretestate-spaces: Asynthesis. JournalofMathematicalPsychology99(2020),102447.
[8] DIAZ,G.,COOPER,J.,ANDHAYHOE,M. Memoryandpredictioninnaturalgazecontrol. Philosophical
TransactionsoftheRoyalSocietyB:BiologicalSciences368,1628(Oct.2013),20130064.
[9] DIAZ,G.,COOPER,J.,ROTHKOPF,C.,ANDHAYHOE,M. Saccadestofutureballlocationrevealmemory-
basedpredictioninavirtual-realityinterceptiontask. Journalofvision13,1(2013),20–20.
[10] DIAZ,G.J.,PARADE,M.S.,BARTON,S.L., ANDFAJEN,B.R. Thepickupofvisualinformationabout
sizeandlocationduringapproachtoanobstacle. PLOSONE13,2(Feb.2018),e0192044.
[11] DIAZ, G. J., PHILLIPS, F., AND FAJEN, B. R. Interceptingmovingtargets: alittleforesighthelpsalot.
Experimentalbrainresearch195,3(2009),345–360.
[12] FAJEN,B.,DIAZ,G.,ANDCRAMER,C. Reconsideringtheroleofactioninperceivingthecatchabilityof
flyballs. JournalofVision8,6(2008),621–621.
[13] FAJEN,B.R.,ANDDEVANEY,M.C. Learningtocontrolcollisions: Theroleofperceptualattunementand
actionboundaries. JournalofExperimentalPsychology: HumanPerceptionandPerformance32,2(2006),
300–313.
[14] FAJEN,B.R.,ANDWARREN,W.H. Behavioraldynamicsofinterceptingamovingtarget. Experimental
BrainResearch180,2(June2007),303–319.
[15] FERRERA, V. P., AND BARBORICA, A. InternallyGeneratedErrorSignalsinMonkeyFrontalEyeField
duringanInferredMotionTask. JournalofNeuroscience30,35(Sept.2010),11612–11623.
[16] FIROIU,V.,JU,T.,ANDTENENBAUM,J. Athumanspeed: Deepreinforcementlearningwithactiondelay.
arXivpreprintarXiv:1810.07286(2018).
14
Preprint,WorkinProgress
[17] FRISTON, K. Thefree-energyprinciple: aroughguidetothebrain? Trendsincognitivesciences13, 7
(2009),293–301.
[18] FRISTON,K. Thefree-energyprinciple: aunifiedbraintheory? Naturereviewsneuroscience11,2(2010),
127–138.
[19] FRISTON,K. Afreeenergyprincipleforaparticularphysics. arXivpreprintarXiv:1906.10184(2019).
[20] FRISTON,K.,FITZGERALD,T.,RIGOLI,F.,SCHWARTENBECK,P.,ANDPEZZULO,G. Activeinference:
aprocesstheory. Neuralcomputation29,1(2017),1–49.
[21] FRISTON, K., RIGOLI, F., OGNIBENE, D., MATHYS, C., FITZGERALD, T., AND PEZZULO, G. Active
inferenceandepistemicvalue. Cognitiveneuroscience6,4(2015),187–214.
[22] FRISTON, K., SAMOTHRAKIS, S., AND MONTAGUE, R. Activeinferenceandagency: optimalcontrol
withoutcostfunctions. Biologicalcybernetics106,8(2012),523–541.
[23] FRISTON,K.J.,DAUNIZEAU,J.,ANDKIEBEL,S.J. Reinforcementlearningoractiveinference? PloSone
4,7(2009),e6421.
[24] FRISTON, K. J., LIN, M., FRITH, C. D., PEZZULO, G., HOBSON, J. A., AND ONDOBAKA, S. Active
Inference,CuriosityandInsight. NeuralComputation29,10(102017),2633–2683.
[25] FRISTON,K.J.,ROSCH,R.,PARR,T.,PRICE,C.,ANDBOWMAN,H. Deeptemporalmodelsandactive
inference. Neuroscience&BiobehavioralReviews90(2018),486–501.
[26] HARUNO, M. ANeuralCorrelateofReward-BasedBehavioralLearninginCaudateNucleus: AFunctional
Magnetic Resonance Imaging Study of a Stochastic Decision Task. Journal of Neuroscience 24, 7 (Feb.
2004),1660–1665.
[27] HOIST, E. V., MITTELSTAEDT, H., AND MARTIN, R. Dasreafferenzprìnzip.wechselwirkungzwischen
zentralnervensystemundperipherie. DieNaturwissenschaften37(1950),464.
[28] HOLROYD, C. B., AND COLES, M. G. H. Theneuralbasisofhumanerrorprocessing: Reinforcement
learning,dopamine,andtheerror-relatednegativity. PsychologicalReview109,4(Oct.2002),679–709.
[29] KAPLAN,R.,ANDFRISTON,K.J. Planningandnavigationasactiveinference. Biologicalcybernetics112,
4(2018),323–343.
[30] KINGMA, D., ANDBA, J. Adam: Amethodforstochasticoptimization. arXivpreprintarXiv:1412.6980
(2014).
[31] LERUNIGO,C.,BENGUIGUI,N.,ANDBARDY,B.G. Visuo-motordelay,information–movementcoupling,
andexpertiseinballsports. JournalofSportsSciences28,3(Feb.2010),327–337.
[32] LEE, D., SEO, H., AND JUNG, M. W. Neural Basis of Reinforcement Learning and Decision Making.
AnnualReviewofNeuroscience35,1(July2012),287–308.
[33] LEE,D.N. ATheoryofVisualControlofBrakingBasedonInformationaboutTime-to-Collision.Perception
5,4(Dec.1976),437–459.
[34] MATTHIS,J.S.,ANDFAJEN,B.R. Humansexploitthebiomechanicsofbipedalgaitduringvisuallyguided
walkingovercomplexterrain. ProceedingsoftheRoyalSocietyB:BiologicalSciences280,1762(July2013),
20130700.
[35] MILLIDGE,B. Deepactiveinferenceasvariationalpolicygradients. JournalofMathematicalPsychology96
(2020),102348.
[36] MOMENNEJAD,I.,RUSSEK,E.M.,CHEONG,J.H.,BOTVINICK,M.M.,DAW,N.D.,ANDGERSHMAN,
S.J. Thesuccessorrepresentationinhumanreinforcementlearning. NatureHumanBehaviour1,9(Sept.
2017),680–692.
15
Preprint,WorkinProgress
[37] NIJHAWAN, R. Visualprediction: Psychophysicsandneurophysiologyofcompensationfortimedelays.
BehavioralandBrainSciences31,2(Apr.2008),179–198.
[38] PARR,T.,ANDFRISTON,K.J. Uncertainty,epistemicsandactiveinference. JournalofTheRoyalSociety
Interface14,136(2017),20170376.
[39] PARR,T.,ANDFRISTON,K.J. Theanatomyofinference: generativemodelsandbrainstructure. Frontiers
incomputationalneuroscience12(2018),90.
[40] PARR,T.,ANDFRISTON,K.J. Generalisedfreeenergyandactiveinference. Biologicalcybernetics113,5
(2019),495–513.
[41] SAJID, N., BALL, P. J., PARR, T., AND FRISTON, K. J. Active inference: demystified and compared.
Neuralcomputation33,3(2021),674–712.
[42] SCHWARTENBECK,P.,PASSECKER,J.,HAUSER,T.U.,FITZGERALD,T.H.,KRONBICHLER,M.,AND
FRISTON, K. J. Computational mechanisms of curiosity and goal-directed exploration. Elife 8 (2019),
e41703.
[43] SHIN, J., KIM, C., AND HWANG, H. J. Priorpreferencelearningfromexperts: Designingarewardwith
activeinference. arXivpreprintarXiv:2101.08937(2021).
[44] TIELEMAN,T.,HINTON,G.,ETAL. Lecture6.5-rmsprop: Dividethegradientbyarunningaverageofits
recentmagnitude. COURSERA:Neuralnetworksformachinelearning4,2(2012),26–31.
[45] TSCHANTZ, A., BALTIERI, M., SETH, A. K., AND BUCKLEY, C. L. Scalingactiveinference. In2020
internationaljointconferenceonneuralnetworks(ijcnn)(2020),IEEE,pp.1–8.
[46] TSCHANTZ, A., SETH, A. K., AND BUCKLEY, C. L. Learning action-oriented models through active
inference. PLoScomputationalbiology16,4(2020),e1007805.
[47] UELTZHÖFFER,K. Deepactiveinference. Biologicalcybernetics112,6(2018),547–573.
[48] WADE,N.J. Hermannvonhelmholtz(1821–1894),1994.
[49] WALSH, T. J., NOURI, A., LI, L., AND LITTMAN, M. L. Learningandplanninginenvironmentswith
delayedfeedback. AutonomousAgentsandMulti-AgentSystems18,1(2009),83–105.
[50] WARREN, W., FAJEN, B., AND BELCHER, D. Behavioraldynamicsofsteering,obstacleavoidance,and
routeselection. JournalofVision1,3(Mar.2010),184–184.
[51] WOLPERT,D.M.,GHAHRAMANI,Z.,ANDJORDAN,M.I. Aninternalmodelforsensorimotorintegration.
Science269,5232(1995),1880–1882.
[52] YILMAZ, E. H., AND WARREN, W. H. Visualcontrolofbraking: Atestofthe˙τ hypothesis. Journalof
ExperimentalPsychology: HumanPerceptionandPerformance21,5(1995),996.
[53] ZHAO,H.,ANDWARREN,W.H. On-lineandmodel-basedapproachestothevisualcontrolofaction. Vision
Research110(2015),190–202. On-lineVisualControlofAction.
16

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "A Neural Active Inference Model of Perceptual-Motor Learning"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
