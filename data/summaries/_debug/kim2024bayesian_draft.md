=== IMPORTANT: ISOLATE THIS PAPER ===You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.Do NOT mix information from different papers. Only use information from THIS specific paper.Paper Title: Bayesian Mechanics of Synaptic Learning under the Free Energy PrincipleCitation Key: kim2024bayesianAuthors: Chang Sub KimREMEMBER: Extract quotes, claims, and findings ONLY from the paper text.Key Terms: principle, energy, mechanics, physics, brain, learning, synaptic, bayesian, free, under=== FULL PAPER TEXT ===4202tcO3]CN.oib-q[1v27920.0142:viXraBayesian Mechanics of Synaptic Learning under theFree Energy PrincipleChang Sub KimDepartment of Physics, Chonnam National University, Gwangju61186,Republic ofKoreaE-mail: cskim@jnu.ac.krAbstract. Thebrainisabiologicalsystemcomprisingnervecellsandorchestratesitsembodiedagent’sperception,behavior,andlearninginthedynamicenvironment. Thefree energy principle (FEP) advocated by Karl Friston explicates the local, recurrent,andself-supervisedneurodynamicsofthebrain’shigher-orderfunctions. Inthispaper,we continue to finesse the FEP through the physics-guided formulation; specifically,we apply our theoryto synaptic learningby consideringit aninference problemunderthe FEP and derive the governing equations, called Bayesian mechanics. Our studyuncovers how the brain infers weight change and postsynaptic activity, conditionedon the presynaptic input, by deploying the generative models of the likelihood andprior belief. Consequently, we exemplify the synaptic plasticity in the brain with asimple model: we illustrate that the brain organizes an optimal trajectory in neuralphase space during synaptic learning in continuous time, which variationally minimizessynaptic surprisal.Keywords free energy principle; synaptic learning; Bayesian mechanics; continuous-state formulationBayesian Mechanics of Synaptic Learning under the Free Energy Principle21. IntroductionThe brain’s perception, body movement, and learning are conjointly organized toensure the embodied agents’ homeostasis and adaptive fitness in the environment. Itis tempting to imagine a neural observer in the brain presiding over higher animals’cognitive control. Such a homunculus idea is untenable and must be discarded in thepresent-day brain theory [1]. However, there is still a much distance to the completescientific understanding of the emergent higher-order functions from the brain matter;it demands a comprehension of the profound interplay between the scientific reductionismand teleological holism standpoints [2,3].The brain-inspired FEP is a purposive theory that bridges the gap between top-down teleology and bottom-up scientific constructionism. According to the FEP [4,5],allliving systems areself-organizedtotendtoavoidanatypical niche intheenvironmentforexistence. TheFEPadoptstheautopoietichypothesis[6]andscientificallyformalizesthe abductive rationale of organisms’ making optimal predictions and behavior fromincomplete sensory data. To be precise, the FEP suggests an information-theoretic,variational measure of environmental atypicality, termed free energy (FE). The FEobjective is technically defined as a functional of the probabilistic generative densityspecifying the brain’s internal model of sensory-data generation and environmentalchange and an online auxiliary density actuating variation. The Bayesian braincomputes the posterior of the environmental causes of uncertain sensory data byminimizing the FE, whose detailed continuous-state description can be found in [7]. Fordiscrete-state models of the FEP with discrete time, we recommend [8,9] to readers.When a Gaussian probability is employed for the variational density [10], the FEbecomes a L2 norm specified by the Gaussian means and variances and termed theLaplace-encoded FE [7]. Thus, the Laplace-encoded FE provides a scientific base of theL2 objectives in a principled manner, which are widely used in machine learning andartificial intelligence. For instance, the optimization function in the predictive-codingframework is proposed to be a sum of the squared prediction errors [11]. Also, theloss function of a typical artificial neural network (ANN) is often written as a sum ofsquareddifferencesbetweenthegroundtruthandthepredictiveentriesfromthenetwork[12]. Furthermore, it is argued that the Gaussian sufficient statistics are encoded bythe biophysical brain variables, which form the brain’s low-dimensional representationsof environmental states. This way the brain acquires access to the encoded FE forminimization as it becomes fully specified in terms of the brain’s internal states.Our research over the years has been devoted to developing continuous-stateimplementation of the FE minimization in a manner guided by physics laws andprinciples [13,14,15]. We endeavored to advance the FEP to the point where itcoalesces into a unified principle of top-down architecture and material base. Moreover,to promote the FEP to nonstationary problems, we incorporated the fact that thephysical brainis ina nonequilibrium (NEQ) stationary stateandis generally continuallyaroused by nonstationary sensory stimuli. The functional brain must perform theBayesian Mechanics of Synaptic Learning under the Free Energy Principle3variational Bayesian inversion of nonstationary sensory data to compute the posteriormentioned above. Previously, we accounted for the brain behavior of perception andmotor control as described by attractor dynamics and termed the governing equationsBayesian mechanics (BM). The BM coordinates the brain’s sensory estimation andmotor prediction in neural phase space. In this paper, we make further progress byincorporating the brain’s synaptic learning into the BM, which we did not accommodatein our earlier studies. Learning constitutes the crucial brain function of consolidatingmemory, e.g., via Hebbian plasticity [16].This paper aims to provide a simple but insightful model for synaptic learningin the brain. Our agendas are that the functional brain operates continually usingcontinuous environmental representations and that synaptic learning is a cognitivephenomenon that may very well be understood when statistical-physical laws guideit. The notion cognition throughout this paper is meant to be the brain’s higher-order capability that involves a top-down, internal model. We consider the NEQ braina problem-solving matter, cognitively interacting with the environment. To quantifythe synaptic cognition, we will specify the generative densities furnishing the Laplace-encoded FEin a manner to meet the NEQ stationarity and present the FE minimizationscheme by practicing the principle of least action (Hamilton’s principle) [17]. The novelcontributions worked out in this paper are discussed in Section7.The rest of the paper is organized as follows. In Section2, the single-synapsestructure of our interest is described. The essence of the FEP is recapitulated withrevision made for synaptic learning in Section3. In Section4, an NEQ formulationis presented, which determines the likelihood and prior densities in the physical brain.Next, Section5 identifies the FEobjective asa classical action andderives the governingequations of synaptic dynamics by exercising the Hamilton principle. The utility of ourtheory is demonstrated in Section6 using a simple model. After the discussion inSection7, a conclusion is given in Section8.2. Single Synapse ModelThis workconcerns thebrain’ssynapticlearning without considering howenvironmentalprocesses arouse stimuli at the sensory interface; as a parsimonious model, we focuson a single synapse within the brain’s internal environment. For instance, in thehippocampus, the postsynaptic action potential in the dentate gyrus is evoked by apresynaptic signal from the entorhinal cortex caused by a neural signal from otherbrain areas. Accordingly, the synaptic coupling between two pyramidal neurons in thehippocampus constitutes a single synaptic assembly of interest.We depict the single synaptic model in Figure1, where the presynaptic andpostsynaptic signals are denoted by s and µ, respectively; both are the brain’srepresentations of noisy synaptic signals. In addition, thesynaptic plasticity is mediatedby the weight strength denoted by w. The synaptic structure considered is generic forall neurons; accordingly, the ensuing formulation below applies to other brain regions.Bayesian Mechanics of Synaptic Learning under the Free Energy Principle4Figure1. Single synaptic assembly. The postsynaptic neural state µ isneurophysicallyevokedbythepresynapticsignals,mediatedbytheweightchange∆waccordingtoHebb’srule9sµ. WeadopttheBayesian-inferenceperspective,suggestingthatthe brainstate µ infers the causeofthe presynapticinput s, andthe weightstatew makes up the synaptic input-output interface.Note that we will handle the weight variable w as a neurophysical degree of freedomlike s and µ; this handling contrasts with ANN models, where the weights are treatedas a static parameter.3. Free Energy Principle for Synaptic LearningThe brain-inspired