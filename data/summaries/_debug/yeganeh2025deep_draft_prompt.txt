=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Deep Active Inference Agents for Delayed and Long-Horizon Environments
Citation Key: yeganeh2025deep
Authors: Yavar Taheri Yeganeh, Mohsen Jafari, Andrea Matta

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Abstract: With the recent success of world-model agentsâ€”which extend the core idea of
model-basedreinforcementlearningbylearningadifferentiablemodelforsample-
efficientcontrolacrossdiversetasksâ€”activeinference(AIF)offersacomplemen-
tary,neuroscience-groundedparadigmthatunifiesperception,learning,andaction
withinasingleprobabilisticframeworkpoweredbyagenerativemodel. Despite
this promise, practical AIF agents still rely on accurate immediate predictions
andexhaustiveplanning,alimitationthatisexacerbatedind...

Key Terms: delayed, agents, environments, deep, long, horizon, inference, step, active, politecnicodimilano

=== FULL PAPER TEXT ===

5202
yaM
62
]GL.sc[
1v76891.5052:viXra
Deep Active Inference Agents for Delayed and
Long-Horizon Environments
YavarTaheriYeganehâˆ— MohsenJafari AndreaMatta
PolitecnicodiMilano RutgersUniversity PolitecnicodiMilano
Abstract
With the recent success of world-model agentsâ€”which extend the core idea of
model-basedreinforcementlearningbylearningadifferentiablemodelforsample-
efficientcontrolacrossdiversetasksâ€”activeinference(AIF)offersacomplemen-
tary,neuroscience-groundedparadigmthatunifiesperception,learning,andaction
withinasingleprobabilisticframeworkpoweredbyagenerativemodel. Despite
this promise, practical AIF agents still rely on accurate immediate predictions
andexhaustiveplanning,alimitationthatisexacerbatedindelayedenvironments
requiringplanningoverlonghorizonsâ€”tenstohundredsofsteps. Moreover,most
existingagentsareevaluatedonroboticorvisionbenchmarkswhich,whilenatural
forbiologicalagents,fallshortofreal-worldindustrialcomplexity. Weaddress
theselimitationswithagenerativeâ€“policyarchitecturefeaturing(i)amulti-step
latenttransitionthatletsthegenerativemodelpredictanentirehorizoninasingle
look-ahead,(ii)anintegratedpolicynetworkthatenablesthetransitionandreceives
gradientsoftheexpectedfreeenergy,(iii)analternatingoptimizationschemethat
updates model and policy from a replay buffer, and (iv) a single gradient step
thatplansoverlonghorizons,eliminatingexhaustiveplanningfromthecontrol
loop. Weevaluateouragentinanenvironmentthatmimicsarealisticindustrial
scenariowithdelayedandlong-horizonsettings. Theempiricalresultsconfirmthe
effectivenessoftheproposedapproach,demonstratingthecoupledworld-model
withtheAIFformalismyieldsanend-to-endprobabilisticcontrollercapableof
effectivedecisionmakingindelayed,long-horizonsettingswithouthandcrafted
rewardsorexpensiveplanning.
1 Introduction
Therehasbeensignificantprogressindata-drivendecision-makingalgorithms,particularlyinre-
inforcementlearning(RL),whereagentslearnpoliciesthroughinteractionwiththeenvironment
andreceivefeedback [1]. Deeplearning, inparallel, offersapowerfulframeworkforextracting
representationsandpatterns,whilealsoenablingprobabilisticmodeling[2,3],drivingadvancements
incomputervision,naturallanguageprocessing,biomedicalapplications,finance,androbotics. Deep
RLmergestheseideasâ€”forexample,byusingneuralfunctionapproximationinDeepQ-Networks
(DQN),whichachievedhuman-levelperformanceonAtarigames[4]. Model-basedRL(MBRL)
goesfurtherbyexplicitlyincorporatingamodelâ€”eitherlearnedorprovidedâ€”oftheenvironmentto
guidelearningandplanning[5].Similarly,theconceptofworldmodelscentersonlearninggenerative
modelsoftheenvironmenttoexploitrepresentationsandpredictionsoffutureoutcomes,especially
fordecision-making[6]. Thisresonateswithcognitivetheoriesofthebiologicalbrain,whichem-
phasizetheroleofinternalgenerativemodels[7]. Atabroadertheoreticallevel,activeinference
(AIF),anemergingfieldinneuroscience,unifiesperception,action,andlearninginbiologicalagents
throughtheuseofinternalgenerativemodels[8,9].
âˆ—Email:yavar.taheri@polimi.itoryavaryeganeh@gmail.com
Preprint.
AIFisgroundedinthefreeenergyprinciple(FEP),whichformulatesneuralinferenceandlearning
underuncertaintyasminimizationofsurprise[10]. Itprovidesacoherentmathematicalframework
thatcalibratesaprobabilisticmodelgovernedbyBayesianinference,enablingbothlearningand
goal-directedactiondirectlyfromrawsensoryinputs(i.e.,observations)[9]. Thiscansupportthe
developmentofmodel-driven,adaptiveagentsthataretrainedend-to-endwhileofferinguncertainty
quantificationandsomeinterpretability[11,12]. Similartoworldmodelsandmodel-basedRL,AIF
ispoweredbyaninternalmodeloftheenvironment,whichcanhelptocapturedynamicsandboost
sampleefficiency. DespitethepotentialoftheAIFframework,itspracticalagentstypicallyrelyon
accurateimmediatepredictionsandextensiveplanning[12]. Suchreliancecanhinderperformance,
particularlyindelayedenvironments,wheretheconsequencesofactionsarenotimmediatelyobserv-
ableâ€”commonlyframedinRLassparserewards,whichexacerbatesthecredit-assignmentproblem
[1]. Likewise,long-horizontasksdemandeffectiveplanningoverextendedtemporalhorizons,posing
anadditionalchallenge. Thesedifficultiesappearacrossdiverseoptimizationtasksâ€”suchasmanu-
facturingsystems[11],robotics[13,6,14],andproteindesign[15,16]â€”wheretheconsequences
becomeapparentonlyaftermanystepsoruponcompletionoftheentireprocess.
WeexplorehowthepotentialoftheAIFframeworkcanbeharnessedtobuildagentsthatremain
effectiveinenvironmentsthataredelayedanddemandinglong-horizonplanning. Recentadvances
indeepgenerativemodeling[17]haveunlockedbreakthroughsacrossdiversedomainsâ€”suchas
AlphaFoldâ€™shigh-accuracyprotein-structurepredictions[18]. Becausethegenerativemodelisthe
core of AIF, our objective is to extend its capacity and fidelity as the world model by predicting
deepintothefuture. Concretely,weproposeagenerativemodelwithanintegratedpolicynetwork,
trainedend-to-endundertheAIFformalism,allowingthemodeltoproducelong-horizonroll-outs
andsupplygradientsignalstothepolicyduringoptimization. Thesummaryofourcontributionsisas
follows:
â€¢ WeintroduceanAIF-consistentgenerativeâ€“policyarchitecturethatenableslong-horizon
predictionswhileprovidingdifferentiablesignalstothepolicy.
â€¢ Wederiveajointtrainingalgorithmthatalternatelyupdatesthegenerativemodelandthe
policynetwork,andweshowhowthelearnedmodelcanbeleveragedduringplanningvia
gradientupdatestothepolicy.
â€¢ Weempiricallydemonstratetheconceptâ€™seffectivenessinanindustrialenvironment,high-
lightingitsrelevancetodelayedandlong-horizonscenarios.
Theremainderofthepaperisorganizedasfollows: Section2reviewstheformalismandplanning
strategies. Section3presentsourproposedconceptandagentarchitecture,whileSection4detailsthe
experimentalresults. Finally,Section5concludeswithimplicationsandoutlinesfuturedirections.
2 Background
AgentsbasedontheworldmodelsconceptextendthecoreideaofMBRL,learningadifferentiable
predictivemodeltofacilitatepolicyoptimizationandplanningviaimaginationsinthemodel[19,6].
Theycreatelatentrepresentationsthatcapturespatialandtemporalaspectstomodeldynamicsand
predict the future [19]. The architecture governing this dynamicsâ€”generative modelâ€”and how
it is leveraged for policy and planning is foundational in this concept. Many designs resemble
variationalautoencoder[20]andareoftenaugmentedwithRecurrentState-SpaceModels(RSSMs)
to provide memory and help with credit assignment [21, 6, 14]. At the same time, RL methods
such as actorâ€“critic [1] are integrated with the model to optimize the policy [13, 6, 14], yielding
sample-efficientagentsthatrelyonimaginationratherthanextensiveenvironmentinteraction.
AIFoffersacomplementary,neuroscience-groundedperspectivethatsubsumespredictivecoding
thatpostulatesthatthebrainminimizespredictionerrorsâ€”relativetoaninternalgenerativemodel
of the worldâ€”under uncertainty [22]. It casts the brain as a hierarchy that performs variational
Bayesian inference continuously to suppress prediction error [9]. It was originally advanced to
explain how organisms actively control and navigate their environments by iteratively updating
beliefs and inferring actions from sensory observations [9]. AIF emphasizes the dependency of
observations on actions [22]; accordingly, it posits that actions are chosen, while calibrating the
model,toalignwithpreferencesandreduceuncertainty,therebyunifyingperception,action,and
learning [22]. The free-energy principle provides the mathematical bedrock for this framework
2
[23,24],andagrowingbodyofempiricalworksupportsitsbiologicalplausibility[25]. AIF-based
agentshavebeendeployedinrobotics,autonomousdriving,andclinicaldecisionsupport[26,27,28],
demonstratingrobustperformanceinuncertain,dynamicsettings. Inthiswork,weadopttheAIF
formulationofFountasetal.(2020)[12],whichwasextendedin[29,11]andhasbeenshownto
resultineffectiveagentsacrossdifferentenvironmentsâ€”suchasvisualandindustrialtasks.
2.1 Formalism
WithinAIF,agentsemployanintegratedprobabilisticframeworkconsistingofaninternalgenerative
model[30]withinferencemechanismsthatallowthemtorepresentandactupontheworld. The
frameworkassumesaPartiallyObservableMarkovDecisionProcess[31,30,32],whereanagentâ€™s
interactionwithitsenvironmentisformalizedintermsofthreerandomvariablesâ€”observation,latent
state,andactionâ€”denoted(o ,s ,a )attimet. IncontrasttoRL,thisformalismdoesnotrelyon
t t t
explicitrewardfeedbackfromtheenvironment;instead,theagentlearnssolelyfromthesequenceof
observationsitreceives.Theagentâ€™sgenerativemodel,parameterizedbyÎ¸,isdefinedovertrajectories
asP (o ,s ,a )uptotimet. Theagentâ€™sbehaviorisdrivenbytheimperativetominimize
Î¸ 1:t 1:t 1:tâˆ’1
surprise,whichisformulatedasthenegativelog-evidenceforthecurrentobservation,âˆ’logP (o )
Î¸ t
[12]. Theagentapproachesthisimperativefromtwoperspectiveswheninteractingwiththeworld,as
follows[9,12]:
1)Usingthecurrentobservation,theagentcalibratesitsgenerativemodelbyoptimizingtheparameters
Î¸toyieldmoreaccuratepredictions. Mathematically,thissurprisecanbeexpandedasfollows[20]:
âˆ’logP (o )â‰¤E [logQ (s ,a )âˆ’logP (o ,s ,a )] , (1)
Î¸ t QÏ•(st,at) Ï• t t Î¸ t t t
whichprovidesanupperbound,commonlyknownasthenegativeEvidenceLowerBound(ELBO)
[33]. It is widely used as a loss function for training variational autoencoders [20]. In AIF, it
correspondstotheVariationalFreeEnergy(VFE),whoseminimizationreducesthesurpriseassociated
withpredictionsrelativetoactualobservations[12,34,32].
2)Lookingintothefuture,wheretheagentneedstoplanactions,anestimateofthesurpriseoffuture
predictionscanbeobtained. Consideringasequenceofactionsâ€”orpolicyâ€”denotedasÏ€,forÏ„ â‰¥t,
thiscorrespondstoâˆ’logP(o |Î¸,Ï€),whichcanbeestimatedanalogouslytoVFE[35]:
Ï„
G(Ï€,Ï„)=E E [logQ (s ,Î¸|Ï€)âˆ’logP(o ,s ,Î¸|Ï€)] . (2)
P(oÏ„|sÏ„,Î¸) QÏ•(sÏ„,Î¸|Ï€) Ï• Ï„ Ï„ Ï„
ThisisknownastheExpectedFreeEnergy(EFE)intheframework,whichquantifiestherelative
qualityofpoliciesâ€”lowervaluescorrespondtobetterpolicies.
TheEFEinEq.2canbederivedasadecompositionofdistincttermsfortimeÏ„,asfollows[35,12]:
G(Ï€,Ï„)=âˆ’E [logP(o |Ï€)] (3a)
QËœ Ï„
+E [logQ(s |Ï€)âˆ’logP(s |o ,Ï€)] (3b)
QËœ Ï„ Ï„ Ï„
+E [logQ(Î¸|s ,Ï€)âˆ’logP(Î¸|s ,o ,Ï€)] , (3c)
QËœ Ï„ Ï„ Ï„
whereQËœ =Q(o ,s ,Î¸|Ï€). Fountasetal. (2020)[12]rearrangedthisformulationwithfurtheruse
Ï„ Ï„
ofsamplingleadingtoatractableestimatefortheEFEthatisbothinterpretableandeasytocalculate
[12]:
G(Ï€,Ï„)=âˆ’E [logP(o |Ï€)] (4a)
Q(Î¸|Ï€)Q(sÏ„|Î¸,Ï€)Q(oÏ„|sÏ„,Î¸,Ï€) Ï„
+E (cid:2)E H(s |o ,Ï€)âˆ’H(s |Ï€) (cid:3) (4b)
Q(Î¸|Ï€) Q(oÏ„|Î¸,Ï€) Ï„ Ï„ Ï„
+E H(o |s ,Î¸,Ï€)âˆ’E H(o |s ,Ï€). (4c)
Q(Î¸|Ï€)Q(sÏ„|Î¸,Ï€) Ï„ Ï„ Q(sÏ„|Ï€) Ï„ Ï„
Conceptually,thecontributionofeachtermintheEFEcanbeinterpretedasfollows[12]: Extrinsic
value(Eq.4a)â€”theexpectedsurprise,whichmeasuresthemismatchbetweentheoutcomespredicted
underpolicyÏ€andtheagentâ€™spriorpreferencesoveroutcomes. Thistermisanalogoustorewardin
RL,asitquantifiesthemisalignmentbetweenpredictedandpreferredoutcomes. However,rather
thanmaximizingcumulativereward,theagentminimizessurpriserelativetopreferredobservations.
Stateepistemicuncertainty(Eq.4b)â€”mutualinformationbetweentheagentâ€™sbeliefsaboutstates
beforeandafterobtainingnewobservations. Thistermincentivizesexplorationofregionsinthe
environmentthatreduceuncertaintyaboutlatentstates[12]. Parameterepistemicuncertainty(Eq.4c)
3
â€”theexpectedinformationgainaboutmodelparametersgivennewobservations. Thistermalso
corresponds to active learning or curiosity [12], and reflects the role of model parameters Î¸ in
generatingpredictions. Thelasttwotermscapturedistinctformsofepistemicuncertainty,providing
anintrinsicdrivefortheagenttoexploreandrefineitsgenerativemodel. Theyplayaroleanalogous
tointrinsicrewardsinRLthatbalancetheexplorationâ€“exploitationtrade-off. Similarinformation-
seekingorcuriositysignalsunderpinmanysuccessfulRLalgorithmsâ€”rangingfromcuriosity-driven
bonuses[36,37]totheentropy-regularizedobjectiveoptimizedbySoftActor-Critic[38]â€”andhave
beenshowntoyieldstrong,sample-efficientagents.
Environment
Observation Observation
Decision Calibration EFE Model â†”VFE
Agent Inference & Prediction
Figure1: TwoperspectivesoftheAIFframework: generalsteps(left)andcoreelements(right).
In summary (Fig. 1), the framework is realized through a mathematical formalism that unfolds
as follows: An observation is ingested and propagated through the generative model, yielding a
perceptualupdateâ€”beliefsaboutcurrentandfuturestates. Thesebeliefsenablecomputationofthe
EFE(Eq.4),whichisusedduringplanningtoselectactions. Afterthenextobservationarrives,the
VFE(Eq.1)isevaluatedandusedtocalibrateâ€”learnâ€”themodelbymatchingthenewobservation
tothepriorprediction. EachiterationoptimizesthemodelwiththeVFEfromthepreviousloop,and
theupdatedmodelthenguidessubsequentinferenceforplanningandaction.
2.2 PlanningStrategy
AgentsbasedonMBRLtypicallyleveragetheirworldmodeltoimaginefuturetrajectoriesbefore
acting,tradingextracomputationforlargegainsinsample-efficiencyandperformance. MonteCarlo
TreeSearch(MCTS)[39,40]isanotablesearchalgorithm,whichselectivelyexplorespromising
trajectoriesinarestrictedmanner. ItseffectivenesswashighlightedinAlphaGoZero[40]andlater
byMuZero,whichfoldsalearnedlatentdynamicsmodeldirectlyintothesearchloop[41]. Inthe
AIFconcept,theagentâ€™splanningbeforetakingactionsistominimizetheEFE;mathematically,it
correspondstothenegativeaccumulatedEFEGasfollows:
(cid:32) (cid:33)
(cid:88)
P(Ï€)=Ïƒ(âˆ’G(Ï€))=Ïƒ âˆ’ G(Ï€,Ï„) , (5)
Ï„>t
whereÏƒ(Â·)representstheSoftmaxfunction. Theagentsimulatespossibletrajectoriesviaroll-outs
fromitsgenerativemodel,underpolicyÏ€,toevaluatetheEFE.However,calculatingthisforany
possibleÏ€isinfeasibleasthepolicyspacegrowsexponentiallywiththedepthofplanning. Fountas
et al. (2020) [12] an auxiliary module along with the MCTS to alleviate this obstacle. They
proposed a recognition module [42, 43, 44], parameterized with Ï• as follows: Habit, Q (a ),
a Ï•a t
whichapproximatestheposteriordistributionoveractionsusingthepriorP(a )thatisreturnedfrom
t
theMCTS[12]. Thisissimilartothefastandhabitualdecision-makinginbiologicalagents[45].
Theyusedthismoduleforfastexpansionsofthesearchtreeduringplanning,followedbycalculating
theEFEoftheleafnodesandbackpropagatingoverthetrajectory. Iteratively,itresultsinaweighted
treewithmemoryupdatesforthevisitednodes. TheyalsousedtheKullbackâ€“Leiblerdivergence
betweentheplannerâ€™spolicyandthehabitprovidesasprecisiontomodulatethelatentstate[12].
Anotherapproachtoenhancetheplanningisusingahybridhorizon[11],inwhichtheshort-sighted
EFE termsâ€”based on immediate next-step predictionsâ€”are augmented with an additional term
duringplanningtoaccountforlongerhorizons. TaheriYeganehetal.(2024)[11]employedaQ-value
network,Q (a ),torepresenttheamortizedinferenceofactions,trainedinamodel-freemanner
Ï•a t
usingextrinsicvalues. Thesetermswerethencombinedintheplannerasfollows:
P(a )=Î³Â·Q (a )+(1âˆ’Î³)Â·Ïƒ(âˆ’G(Ï€)) , (6)
t Ï•a t
4
balancinglong-horizonextrinsicvalueagainstshort-horizonepistemicdrive.
Modernworld-modelagentsincreasinglyshiftthelook-aheadintolatentspace; PlaNet[21]uses
cross-entropymethodroll-outsinsideaRSSMtrainedwithlatentovershooting,whiletheDreamer
family[13,6]propagatesanalyticvaluegradientsthroughhundredsofimaginedtrajectories,without
a tree search. EfficientZero [46] blends AlphaZero-style MCTS with latent-space imagination,
surpassinghumanAtariperformancewithonly100kframes. Theseapproachestypicallycouple
multi-stepmodelroll-outswithanactor(policy)andoftenacritic(value)networkthatarequeried
duringimagination. Ineachsimulatedstep,thepolicyproposesthenextactionandthecriticsupplies
abootstrappedvalue,enablingefficientmulti-steplook-aheadwithoutenumeratingthefullaction
tree. Insteadofsequentiallysamplingactionsandstates,TaheriYeganehetal.[11]trainedmulti-step
latenttransitions,conditionedonrepeatedactions;duringplanning,asingletransitionpredictsthe
outcomewhilekeepinganactionforafixednumberoftime-steps. Thisway,theimpactofactions
overalonghorizoniscapturedusingrepeated-actionsimulations. Whileitcanbecombinedwith
MCTS,thisapproximationhelpeddistinguishdifferentactionsbasedontheEFEinhighlystochastic
controltaskswithasinglelook-ahead[11].Itislimitedtodiscreteactions,cannotgobeyondrepeated
actions,andstillrequiresplanningviaEFEcomputationbeforeeveryaction.
3 DeepActiveInferenceAgent
Fromhabit-integratedMCTStohybrid-horizonandgradient-basedlatentimagination,state-of-the-art
agentsincreasinglyintegratepolicylearningwithplanningtocapturethelong-termeffectsessential
forscalableandsample-efficientcontrol.Aprominentapproachislatentimagination,notablyusedby
Dreameragents[6,21,13],whichperformsequentialrolloutsinlatentspaceusingaRSSM.Besides
itscomputationalburden,thismethodrisksaccumulatingerrorsasnetworksarerepeatedlyinferred
andsampled. Thesemodelsembedthepolicynetworkinthelatentspacebysamplingactionsalong
eachlatent-statetrajectory,sopolicyoptimizationdependsonalargenumberofsamplingsinthe
modelâ€™simaginations.
Asimplerstrategyassumesagenerativemodelthatknowstheexactformofthepolicyfunctionâ€”in
otherwords,thenetworkparametersthemselves. Wecantrainsuchamodeltogenerateaprediction
deepintothehorizonwithasinglelook-ahead,onceprovidedwiththepolicyparametersgoverning
interactionwiththeenvironmentoverthathorizon. Thus,theEFEcanbecomputeddirectlyover
thehorizon,andgradientscanbebackpropagatedtominimizetheEFE,therebyguidingtheagent
towarditsintrinsicandextrinsicobjectives. Giventhatthepolicyisoptimizedthroughthegradient
stepsoftheEFE,thisapproachnaturallyscalestobothdiscreteandcontinuousactionspacesrather
thanchoosingdiscreteactions,asinearlierAIF-agentimplementations[12]. Here,weadoptthisAIF-
consistentgenerative-policymodeling,withoutincorporatingfurthermechanismstypicallyemployed
tofurtherenhanceworldmodelsorAIFagents.
3.1 Architecture
Theagentcomprises,ataminimum,apolicynetworkthatdirectlyinteractswiththeenvironmentand
agenerativemodelthatistrainedtooptimizethatpolicy. Conditionedonthepolicy,thegenerative
modelconstitutesthecoreofAIFandcanbeinstantiatedwithvariousarchitectures. Inthisworkwe
adoptagenericâ€”yetcommonlyusedâ€”autoencoderassembly[12]toinstantiatetheformalismof
Sec.2.1,whichrequiresthetightlycoupledmodulesillustratedinFig.2. Leveragingamortization
[20,43,47]toscaleinference[12],thegenerativemodelisparameterizedbytwosets: Î¸ ={Î¸ ,Î¸ }
s o
forpriorgenerationandÏ• = {Ï• }forrecognition. Accordingly, theEncoderQ (s )performs
s Ï•s t
amortized inference by mapping the currently sampled observation oËœ to a posterior distribution
t
overthelatentstates [48]. Thekeydifferencehereisthat,ratherthansamplingactionsinsidethe
t
latentdynamics, weincorporateapolicyfunctionâ€”orActorâ€”Q (a |oËœ), whichitselfinfersa
Ï•a t t
distributionoveractionswithparametersÏ• . Wethereforeintroduceanexplicitrepresentationfor
a
thefunctionitselfwiththemappingÎ :Q â†’Ï€Ë†,resultinginÏ€Ë†(Ï• ). Thisapproachiscommonin
Ï•a a
neuralimplicitrepresentations[49];recentworkhasmoreoverdemonstratedthatneuralfunctions
withdiversecomputationalgraphscanbeembeddedefficiently[50]. Conditionedontheactor,the
Transition,P (s |sËœ,Ï€Ë†),overshootsthelatentdynamicsuptoaplanninghorizonH,producing
Î¸s t+1 t
adistributionfors giventhesampledlatentstateattimet,whiletheactorâ€“denotedbyÏ• â€“is
t+H a
5
assumedfixedthroughoutthehorizon. Finally,theDecoderP (o |sËœ )convertsthepredicted
Î¸o t+H t+H
latentstatebackintoadistributionoverfutureobservations.
ğ‘œà·¤ğ‘¡â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦ğ‘„ğœ™ğ‘  ğ‘ ğ‘¡ â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦ğ‘ƒğœƒğ‘  (ğ‘ ğ‘¡+ğ»|ğ‘ Çğ‘¡,ğœ‹à·œ)â€¦â€¦â€¦ğ‘ƒğœƒğ‘œ (ğ‘œğ‘¡+ğ»|ğ‘ Çğ‘¡+ğ»)
Encoder Transition Decoder
ğœ™ ğœƒ ğœƒ
ğ‘  ğ‘  ğ‘œ
ğœ‹à·œ(ğœ™ğ‘) âˆ‡ğœ™ğ‘
Actor
ğœ™
âˆ¼ğ‘à·¥ğ‘¡
ğ‘
ğ‘œà·¤ğ‘¡ â€¦â€¦â€¦â€¦â€¦â€¦â€¦ ğ‘„ğœ™ğ‘ ğ‘ğ‘¡|ğ‘œà·¤ğ‘¡
Environment
Figure2: TheDeepAIFagentarchitectureillustratesitsinteractionwiththeenvironment. Theactor
independentlyselectsactions,whilethegenerativemodelisusedtooptimizethepolicy.
Eachofthethreemodulesinthegenerativemodelisrealizedbyaneuralnetworkthatoutputsthe
parameters of a diagonal multivariate Gaussian, thereby approximating a pre-selected likelihood
family. They can be trained end-to-end by minimizing the VFE (Eq. 1), whereas the actor is
optimizedâ€”usingpredictionsfromthecalibratedmodelâ€”byminimizingtheEFE(Eq.4). Inthis
way,theagentunifiesthetwofree-energyparadigmsderivedintheformalism. Asidefromtheactor
andtransition,whichaccountforlatentdynamicswithasinglelook-ahead,thearchitectureresembles
avariationalautoencoder(VAE)[20];nevertheless,othergenerativemechanisms,suchasdiffusion
ormemory-basedRSSMmodels,canbeextendedtosupportthesameobjective.
3.2 PolicyOptimization
Weproposeaconciseyeteffectiveformulationforembeddingtheactorwithinthegenerativemodel
sothatitservesasaplannerthatminimizestheEFEviagradientdescent. Conditionedonafixed
policyÏ€Ë†(Ï• ),themodelgeneratesthepredictiondistributionP (o |Ï• ),fromwhichwecompute
a Î¸ t+H a
theEFE,denotedasthefunctionG (oËœ,Ï• ). Policyoptimizationthenproceedsbyupdatingtheactor
Î¸ a
parametersaccordingtothegradientâˆ‡ G (oËœ,Ï• ).Mostworld-modelagentsintroducestochasticity
Ï•a Î¸ a
bysamplingactionsduringimagination,whichpromotesexplorationâ€”typicallyaidedbyauxiliary
terms during the policy gradient. This results in a Monte Carlo estimation of the policy across
imaginedtrajectories,whichisthendifferentiatedbasedonthereturn[13]. Incontrast,ourapproach
assumestheexactformofthepolicyisintegratedintothedynamics,andexplorationisdrivenbythe
AIFformalismbasedonthegenerativemodel.
To effectively estimate the different components of the EFE in Eq. 4, Fountas et al. (2020) [12]
employedmultiplelevelsofMonteCarlosampling. Whiletheiroriginalformulationincorporated
sampledactionsovermulti-stephorizons,thesamestructureandsamplingschemeremainbeneficial
when using an integrated actor with deep temporal overshooting. Similarly, we adopt ancestral
samplingtogeneratethepredictionP (o |Ï• )andleveragedropout[51]inthenetworks. Itâ€™s
Î¸ t+H a
coupledwithfurthersamplingfromthelatentdistributionstocomputetheentropiesnecessaryfor
calculating the EFE terms. Crucially, under the AIF framework, agents need a prior preference
over predictions to guide behaviorâ€”this is formalized through the extrinsic value (i.e., Eq. 4a).
Accordingly, we define an analytical mapping that transforms the prediction distribution into a
continuouspreferencespectrum,Î¨:P (o )â†’[0,1].
Î¸ Ï„
6
UnlikeRL,whichreliesonthereturnofaccumulatedrewards, thisformulationallowstheagent
toexpressmoregeneralandnuancedformsofpreference. Inpractice,designingasuitablereward
functionforRLagentsremainsadifficulttask,oftenresultinginsparseorhand-craftedsignalsthat
canbecostlytodesignandcompute. Theflexibilityinpreference,however,introduceschallengesâ€”
particularly when agents have complex preference space and must act with short-sighted EFE
approximations. Ourapproach,byoptimizingplanningthroughdeeptemporalprediction,mitigates
thisissueandenableslonger-termevaluationoftheextrinsicvalue.
3.2.1 Training&Planning
Duringtraining,thegenerativemodelgraduallylearnshowdifferentactorparametersÏ• affectthe
a
dynamics, andduringpolicyoptimization, thislearneddynamicsisthenusedtodifferentiatethe
actortowardlowerEFEorsurprise. Criticalforeffectivepolicylearningistheaccuracyoftheworld
model,whichformsthefoundationofAIF[23,9,12]andpredictivecoding[22]. Toimprovemodel
training,weintroduceexperiencereplay[4]usinganexperiencememory/bufferM,fromwhich
wesamplebatchesofexperiences,whileensuringthateachbatchincludesthemostrecentone. We
compute the VFE in Eq. 1 for these experiences to train the model with Î²-regularization. With
theupdatedmodel,wedifferentiatetheEFEoverabatchofobservationsâ€”includingpreviousand
currentonesâ€”withinimaginedtrajectoriesoflengthH,trainingtheactorsimilarlytoworld-model
methods [13, 6, 19]. This results in a joint training algorithm 1 that alternates between updating
thegenerativemodelandthepolicy,usingthemodeltoguideplanningviapolicygradients. This
approach,policylearningâ€”ratherthanexplicitactionplanningâ€”relaxesthebounded-sightconstraint
ofEFE,asthepolicyisiterativelytrainedacrossdiversescenarioswithintheplanninghorizon,and
itseffectivesight extendsbeyondthenominalhorizonH. RecentworkonAIF-basedagentshas
alsoemphasizedtheadvantagesofintegratingapolicynetworkwiththeEFEobjective[14]. After
trainingconcludesandtheagentâ€™smodelisfixed,theagentcanstillleverageitsmodelforplanning.
Specifically,EFE-basedgradientupdatescanbeappliedattheobservationlevelonceeveryH steps,
effectivelyfine-tuningthepolicyfortheimmediatehorizon.
4 Experiments
MostexistingAIFagentshaveshowneffectivenessacrossarangeoftaskstypicallyperformedby
biologicalagents,suchashumansandanimals. Thesetasksofteninvolveimage-basedobservations
[14]. For example, Fountas et al. (2020) [12] evaluated their agent on Dynamic dSprites [52]
and Animal-AI [53], which biological agents can perform with relative ease. AIF has also been
successfully applied in robotics [54, 29], including object manipulation [14, 27], aligning with
behaviors humans naturally perform. This effectiveness is largely attributed to AIFâ€™s grounding
intheoriesofdecision-makinginbiologicalbrains[9]. However,applyingAIFtomorecomplex
domainsâ€”suchasindustrialsystemcontrolâ€”posessignificantchallenges.Evenhumansmaystruggle
to design effective policies in these settings. Such environments often exhibit high stochasticity,
where short observation trajectories are dominated by noise, making it difficult to optimize free
energyforlearningandactionselection. Thisissueislesspronouncedinworldmodelagents,which
often use memory-based (e.g., recurrent) architectures [13, 6]. Moreover, realistic environments
frequentlycombinediscreteandcontinuousobservationmodalities, complicatinggenerativeand
samplingpredictions. Delayedfeedbackandlong-horizonrequirementsfurtherchallengeplanning
undertheAIFframework. Additionally,manyreal-worldtasksrequirerapid,frequentdecisionsand
sustainedperformanceinnon-episodic,stochasticsettings. Toassessourapproach,weemploya
high-fidelitysimulationenvironmentvalidatedtoreflectrealisticindustrialcontrolscenarios[55],
whichincorporatesalltheabovechallenges[11].
4.1 Application
Asenergyefficiencybecomesincreasinglycriticalinmanufacturing[56],RLoffersamodel-free
alternative to traditional control, though it may struggle with rapid adaptations in non-stationary
environments[57]. Wefocusonsimulatingworkstationsinanautomotivemanufacturingsystem
composed of parallel, identical machines (see Appendix A for details). Governed by Poisson
processesforarrivals,processing,failures,andrepairs[55],thesystemevolvesasadiscrete-time
Markovchain[58].Controlactionsâ€”switchingmachinesonoroffâ€”aimtoimproveenergyefficiency
withoutcompromisingthroughput. Theproblemiscontinualwithnoterminalstate,anddecisions
7
Algorithm1DeepAIFAgentTraining(perepoch)
1: InitializeÎ¸ ={Î¸ s ,Î¸ o }, Ï•={Ï• s ,Ï• a }, M Agentcomponents:
2: RandomlyinitializeE Model:
3: forn=1,2,...,N do EncoderQ Ï•s .
â–·ENVIRONMENTINTERACTION TransitionP
Î¸s
.
4: Ï€Ë† t â†Î (Q Ï•a ) DecoderP Î¸o .
5: forÏ„ =t+1,t+2,...,t+H do ActorQ Ï•a .
6: SampleanewobservationoËœ Ï„ fromE ActormappingÎ .
7: ApplyaËœ Ï„ âˆ¼Q Ï•a (a Ï„ |oËœ Ï„ )toE PreferencemappingÎ¨.
8: SampleanewobservationoËœ Ï„+1 fromE
Othercomponents:
9: Mâ†Mâˆª{(oËœ t ,Ï€Ë† t ,oËœ t+H )} EnvironmentE.
â–·MODELLEARNING
10: {(oËœ tâ€² ,Ï€Ë† tâ€² ,oËœ tâ€²+H )}B1 âˆ¼M ExperienceMemoryM.
11: fortâ€² =1,2,...,B 1 do Hyperparameters:
12: runModel(oËœ tâ€² ,Ï€Ë† tâ€² ,oËœ tâ€²+H ) IterationsN.
13: L s â†L s +D KL (cid:2) Q Ï•s (s tâ€²+H )||N(Âµ,Ïƒ2) (cid:3) BetaÎ².
1 1 1 1 1 4 5 6 7 8 : : : : : Î¸ Ï• Î¸ s o s â† â† â† L L o o Î¸ Î¸ Ï• â† â† s o s âˆ’ âˆ’ âˆ’ L L Î¾ Î· Î³ o o âˆ‡ âˆ‡ âˆ‡ + âˆ’ Î¸ Î¸ Ï• s o Î² E E s E E Q âˆ— (cid:2) (cid:2) L (cid:2) ( L D s L s t o â€² K ( s ( + Î¸ ( Î¸ L H Ï• s o (cid:2) ) ) ) o (cid:3) Q (cid:3) [ ) l (cid:3) o Ï• g s ( P s Î¸ tâ€² o + ( H o t ) â€²+ || H N |sËœ ( t ÂµËœ â€²+ , H ÏƒËœ2 ) ) ] (cid:3) Ru L H B S n a e a o m a M t r c r i p n z h o o l i d e s n n i e g z s l H i e ( r z oËœ a e B . i te , S 1 Ï€Ë† Î¾ 1 , , B , , oËœ Î³ S i 2 + 2 . , H . Î· ) , : Î±.
â–·POLICYOPTIMIZATION ComputeQ Ï•s (s i )usingoËœ i
19: {oËœ Ï„ }B2 âˆ¼M SamplesËœ i âˆ¼Q Ï•s (s i )
20: forÏ„ =1,2,...,B 2 do ComputeÂµ,Ïƒ â†P Î¸s (s i+H |sËœ i ,Ï€Ë†)
2
2
1
2
:
:
C
Sa
o
m
m
p
p
l
u
e
te
sËœ Ï„
Q
âˆ¼
Ï•s
Q
(s
Ï•
Ï„
s
)
(
u
s Ï„
si
)
ngoËœ Ï„ C
C
o
o
m
m
p
p
u
u
t
t
e
e
Q
Âµâ€²
Ï•
,
s
Ïƒ
(
â€²
sËœ
â†
i+H
Q
)
Ï•
u
s
s
(
i
sËœ
n
i
g
+H
oËœ i
)
+H
23: fors=1,2,...,S 1 do SamplesËœ i+H âˆ¼N(Âµ,Ïƒ2)
24: ComputeÂµ,Ïƒ â†P Î¸s (s Ï„+H |sËœ Ï„ ,Ï€Ë† t ) ComputeP Î¸o (o i+H |sËœ i+H )
25: SamplesËœ Ï„+H âˆ¼N(Âµ,Ïƒ2)
26: ComputeP Î¸o (o Ï„+H |sËœ Ï„+H )
27: ComputeQ Ï•s (sËœ Ï„+H )usingoËœ Ï„+H
28: ComputeÂµâ€²,Ïƒâ€² â†Q Ï•s (sËœ Ï„+H )
29: Gâ†Gâˆ’log Î¨[P Î¸o (o Ï„+H |sËœ Ï„+H )]
30: Gâ†G+[H(Âµâ€²,Ïƒâ€²)âˆ’H(Âµ,Ïƒ)]
31: fors=1,2,...,S 2 do
32: SamplesËœ Ï„+H âˆ¼P Î¸s (s Ï„+H |sËœ Ï„ ,Ï€Ë† Ï„ )â–·Re-
computedwithdropout.
33: ComputeÂµâ€²â€², Ïƒâ€²â€² â†P Î¸o (o Ï„+H |sËœ Ï„+H )
34: SamplesËœ Ï„+H âˆ¼N(Âµ,Ïƒ2)
35: ComputeÂµâ€²â€²â€², Ïƒâ€²â€²â€² â†P Î¸o (o Ï„+H |sËœ Ï„+H )
36: Gâ†G+[H(Âµâ€²â€², Ïƒâ€²â€²)âˆ’H(Âµâ€²â€²â€², Ïƒâ€²â€²â€²)]
37: Ï• a â†Ï• a âˆ’Î±âˆ‡ Ï•a
E(cid:2)
G(Ï• a )
(cid:3)
relyonbothdiscreteandcontinuousobservations. Duetostochasticdelays,thesystemconnects
continuous-timedynamicstodiscrete-timedecisions,makingperformanceonlyobservableoverlong
horizons. Accordingly,weemployawindow-basedpreferencemetric[11]thatevaluatesKPIsover
thepasteighthours. TheproductionrateisdefinedasT = N(t)âˆ’N(tâˆ’ts),whereN(t)isthenumber
ts
of parts produced, and the energy consumption rate as E = C(t)âˆ’C(tâˆ’ts), where C(t) denotes
ts
totalenergyconsumed,withtâˆ’t â‰ˆ 8hrs. Thiswindowmayspanthousandsofactions,where
s
duetostochasticityandtheintegralnatureofperformance,immediateobservationsarenoisyand
uninformative. Asaresult,theAIFagentsbasedonshort-horizonEFEplanningarenotfeasiblein
thissetting. Byoperatingdirectlyonrawperformancesignalsratherthanhandcraftedrewards,the
approachenablesscalabilitytodomainswhererewardsignalsaresparseorexpensive. Theagent
must handle delayed feedback and plan over extended horizons to move the system towards the
preferredperformance.
8
4.2 Results
Tovalidatetheperformanceofouragentintheaforementionedenvironment,weadoptedarigorous
evaluationscheme(seeAppendixCfordetails)basedonAlgorithm1. Unlikepreviousworksthat
usedinteractionswithabatchofenvironmentstoimprovetrainingefficiency[12],ouragentwas
trainedineachepochbyinteractingwithasingleenvironmentinstance,reflectingamorechallenging
setting. The trained agentâ€™s performance was then evaluated across several randomly initialized
environments. Fromthese,thebest-performinginstancewasselectedforaone-monthsimulationrun
toassessenergyefficiencyandproductionloss,incomparisontoabaselinescenariowherenocontrol
wasappliedandmachineswerecontinuouslyactive. Wealsoconstructedacompositionalpreference
scoreâ€”analogoustoarewardfunctionâ€”basedontime-windowKPIsforenergyconsumptionand
production,servingasanoverallindicatorofagentperformance,whichispartoftheobservationof
theagent. Toenforcefurtherregularizationinthelatentspacetomatchanormaldistribution,we
usedaSigmoidfunctioninitsnon-saturateddomain. Sinceweneededtoencodetheactorfunction,
whichisessentiallyacomputationalgraph[50],weadoptedasimple,non-parametricmappingÎ 
thatconcatenatestheinputwiththefirsthiddenandoutputvalues. Givenitsinput-outputstructure
andthefactthatthemodelwascontinuouslytrainedwiththat,thismappingeffectivelyservesasan
approximationoftheactorâ€™sneuralfunction(seeAppendixBfordetails).
Weimplementedtheagentintheexactproductionsystem,usingparametersverifiedtoreflectrealistic
conditions,followingtheaforementionedscheme. Figure3presentstheperformanceoftheagent
withanovershootinghorizonofH = 300. Duringevaluationsaftereachepoch(100iterations),
theagentimprovedthepreferencescoreofobservations(Fig.3a),whichcorrelateswithincreased
energy efficiency (Fig. 3b). Notably, the EEF of imagined trajectories (Fig. 3c) used for policy
updates decreased as the agent learned to control the system. This trend is observed in both the
extrinsicanduncertaintycomponentsoftheEFE.Sincepolicyoptimizationreliesheavilyonlearning
arobustgenerativemodelâ€”withtheactorintegratedwithinitâ€”theagentgraduallyimprovedits
predictivecapacityandreducedreconstructionerroracrossbothcontinuous(Fig.3d,preference)and
discrete(Fig.3e,f,machineandbufferstates)elementsoftheobservationspace. WhileEFEand
overallperformanceeventuallystabilized,thegenerativemodelcontinuedtoimprove,indicatingthat
fullreconstructionoffutureobservationsisnotstrictlyrequiredforeffectivecontrol. Finally,we
thenevaluatedthetrainedagentoveronemonthofsimulatedinteraction(10replications),applying
gradientupdateseveryH stepsduringplanning. Loffredoetal.(2023)[57]testedmodel-freeRL
agentsacrossarewardparameterÏ•,withDQNemergingasthetopperformer. Table1showsthatour
DAIFagentoutstripsthisbaseline,raisingenergyefficiencyperproductionunitby10.21%Â±0.14%
whilekeepingthroughputlossnegligible.
(a) Preference Score (b) Improving Consumption/Part (%) (c) EFE Terms
1.00 15 0.8 EFE
EFE Term 1
0.95 10 0.6 EFE Term 2
EFE Term 3
0.90 5 0.4
0.85
0 0.2
0.80
0.75 5 0.0
0.70 10 0.2
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Epoch Epoch Epoch
(d) Preference Reconstruction (MAE) (e) Machine State Reconstruction (BCE) (f) Buffer Level Reconstruction (BCE)
0.250
0.55 0.36
0.225
0.50
0.200 0.34
0.175 0.45
0.150 0.40 0.32
0.125 0.35
0.100 0.30
0.30
0.075
0.050 0.25 0.28
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Epoch Epoch Epoch
Figure3: TheperformanceoftheagentwithH =300ontherealindustrialsystem.
9
Agent(Ï•) ProductionLoss[%] ENSaving[%]
DQN(0.93) 4.82Â±0.34 10.87Â±0.76
DQN(0.94) 3.34Â±0.23 9.92Â±0.69
DAIF 2.59Â±0.16 12.49Â±0.04
DQN(0.95) 1.27Â±0.05 7.00Â±0.07
DQN(0.96) 1.27Â±0.09 7.62Â±0.12
DQN(0.97) 1.20Â±0.05 7.72Â±0.10
DQN(0.98) 0.54Â±0.04 2.72Â±0.19
DQN(0.99) 0.40Â±0.03 2.46Â±0.01
Table1: Productionlossversusenergy-saving(EN)acrossrewardparametersÏ•ofDQNagents[57]
andfortheDAIFagent.
4.2.1 EffectofDepth
Theagentmanagestoimprovetheperformanceevenwhentheovershootinghorizoncanbelonger
(e.g., H = 1000 steps). We conducted experiments with different overshooting horizons H to
evaluate the performance of the agent. As shown in Figure 4, we report the preference scores
fromthebestepochduringthevalidationphase. Wealsoextractedthepercentageimprovementin
energy-efficientconsumption. Theresultsdemonstratethatevenwithlongerovershootinghorizons,
theagentisstillabletolearnrobustpolicies.
(a) Preference Score (b) Improving Consumption/Part (%)
15
Mean
0.995 95% CI
14
0.990
0.985 13
0.980
12
0.975
0.970 11
0.965 Mean
10
95% CI
0.960
200 400 600 800 1000 200 400 600 800 1000
H H
Figure4: PerformanceoftheagentsversusovershootinghorizonH.
5 ConclusionandFutureWork
WeintroducedDeepActiveInference(DAIF)Agentsthatintegrateamulti-steplatenttransitionand
anexplicit,differentiablepolicyinsideasinglegenerativemodel. Byovershootingthedynamics
toalonghorizonandback-propagatingexpected-free-energygradientsintothepolicy, theagent
planswithoutanexhaustivetreesearch,scalesnaturallytocontinuousactions,andpreservesthe
epistemicâ€“exploitativebalancethatdrivesactiveinference. WeevaluatedDAIFonahigh-fidelity
industrialcontrolproblemwhosefeaturecomplexityhasrarelybeentackledinpreviousworksbased
onactiveinference. Empirically,DAIFclosedtheloopbetweenmodellearningandcontrolinhighly
stochastic, delayed, long-horizonenvironment. WithasinglegradientupdateeveryH steps, the
trainedagentplanned,andachievedstrongperformanceâ€”surpassingmodel-freeRLbaselineâ€”while
itsworldmodelcontinuedtorefinepredictiveaccuracyevenafterthepolicystabilized.
Limitationsandfuturework: WhilepredictinganH-steptransitionremovestheexpensiveper-step
planningloop,theagentstillhastogatherexperienceafterH interactionsandstoreitinthereplay
memory for training, so its sample-efficiency can still be improved. To update the world model
aftereachnewenvironmentinteractionâ€”obtainedunderdifferentactor/movingparametersâ€”we
need an operator that aggregates the sequence of actor representations. Recurrent models are a
naturalchoiceforthis,buttheirsequentialunrollingaddslatencyandcanhindergradientflow. A
10
lighteralternativeistotreattheH embeddingsasan(almost)unorderedsetanduseasetfunction
[59];whenthetemporalstructurewithsimplepositionalembeddings(e.g.sinusoidal[60])canbe
concatenatedbeforethesetpooling. Thisallowsustobreakthehorizonintosegmentsâ€”downto
a single stepâ€”while still enabling EFE gradient backpropagation via aggregation of the current
policyrepresentation. Finally,(neural)operator-learningtechniquescouldenableresolution-invariant
aggregationacrossfunctionspaces[61,62]. AdditionalextensionsincludereplacingtheVAEworld
modelwithdiffusion-orflow-matching-basedgenerators[28],adoptingactorâ€“criticoptimization
(asinDreamerandrelatedworld-modelagents[13,6,14]),andintroducingregularizationschemes
to stabilize EFE gradient updates and reduce their variance. Rapid adaptation in non-stationary
settingsâ€”wheremodel-freeagentsoftenstruggleâ€”remainsanespeciallypromisingdirection.
Overall,thisworkbridgesneuroscience-inspiredactiveinferenceandcontemporaryworld-modelRL,
demonstratingthatacompact,end-to-endprobabilisticagentcandeliverefficientcontrolindomains
wherehand-craftedrewardsandstep-wiseplanningareimpractical.
Acknowledgments
TheworkpresentedinthispaperwassupportedbyHiCONNECTS,whichhasreceivedfundingfrom
theKeyDigitalTechnologiesJointUndertakingundergrantagreementNo. 101097296.
References
[1] RichardSSuttonandAndrewGBarto. Reinforcementlearning: Anintroduction. MITpress,
2018.
[2] YannLeCun,YoshuaBengio,andGeoffreyHinton. Deeplearning. Nature,521(7553):436â€“444,
2015.
[3] ChristopherM.BishopandHughBishop. DeepLearning: FoundationsandConcepts. Springer
InternationalPublishing,2024.
[4] VolodymyrMnih, KorayKavukcuoglu, DavidSilver, AndreiARusu, JoelVeness, MarcG
Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.
Human-levelcontrolthroughdeepreinforcementlearning. nature,518(7540):529â€“533,2015.
[5] Thomas M. Moerland, Joost Broekens, Aske Plaat, and Catholijn M. Jonker. Model-based
reinforcementlearning:Asurvey. FoundationsandTrendsÂ®inMachineLearning,16(1):1â€“118,
2023.
[6] DanijarHafner,JurgisPasukonis,JimmyBa,andTimothyLillicrap. Masteringdiversecontrol
tasksthroughworldmodels. Nature,640:647â€“653,2025.
[7] KarlFriston,RosalynJ.Moran,YukieNagai,TadahiroTaniguchi,HiroakiGomi,andJoshuaB.
Tenenbaum. Worldmodellearningandinference. NeuralNetworks,144:573â€“590,2021.
[8] Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, and Giovanni
Pezzulo. Activeinference: aprocesstheory. Neuralcomputation,29(1):1â€“49,2017.
[9] ThomasParr,GiovanniPezzulo,andKarlJFriston. Activeinference: thefreeenergyprinciple
inmind,brain,andbehavior. MITPress,2022.
[10] KarlFriston. Thefree-energyprinciple: aunifiedbraintheory? Naturereviewsneuroscience,
11(2):127â€“138,2010.
[11] YavarTaheriYeganeh,MohsenJafari,andAndreaMatta. Activeinferencemeetingenergy-
efficientcontrolofparallelandidenticalmachines. InInternationalConferenceonMachine
Learning,Optimization,andDataScience,pages479â€“493.Springer,2024.
[12] Z.Fountas,NoorSajid,PedroA.M.Mediano,andKarlJ.Friston. Deepactiveinferenceagents
usingmonte-carlomethods. ArXiv,abs/2006.04176,2020.
11
[13] DanijarHafner, TimothyLillicrap, JimmyBa, andMohammadNorouzi. Dreamtocontrol:
Learningbehaviorsbylatentimagination. InInternationalConferenceonLearningRepresenta-
tions,2020.
[14] Viet Dung Nguyen, Zhizhuo Yang, Christopher L Buckley, and Alexander Ororbia. R-aif:
Solvingsparse-rewardrobotictasksfrompixelswithactiveinferenceandworldmodels. arXiv
preprintarXiv:2409.14216,2024.
[15] Christof Angermueller, David Dohan, David Belanger, Ramya Deshpande, Kevin Murphy,
andLucyColwell. Model-basedreinforcementlearningforbiologicalsequencedesign. In
Internationalconferenceonlearningrepresentations,2019.
[16] Chenyu Wang, Masatoshi Uehara, Yichun He, Amy Wang, Tommaso Biancalani, Avantika
Lal,TommiJaakkola,SergeyLevine,HanchenWang,andAvivRegev. Fine-tuningdiscrete
diffusionmodelsviarewardoptimizationwithapplicationstodnaandproteindesign. arXiv
preprintarXiv:2410.13643,2024.
[17] JakubMTomczak. DeepGenerativeModeling. SpringerCham,2024.
[18] Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel,
Olaf Ronneberger, Lindsay Willmore, Andrew J. Ballard, Joshua Bambrick, Sebastian W.
Bodenstein, David A. Evans, Chia Chun Hung, Michael Oâ€™Neill, David Reiman, Kathryn
Tunyasuvunakool,ZacharyWu,AkvileË™ Å½emgulyteË™,EiriniArvaniti,CharlesBeattie,Ottavia
Bertolli,AlexBridgland,AlexeyCherepanov,MilesCongreve,AlexanderI.Cowen-Rivers,
AndrewCowie,MichaelFigurnov,FabianB.Fuchs,HannahGladman,RishubJain,YousufA.
Khan,CarolineM.R.Low,KubaPerlin,AnnaPotapenko,PascalSavy,SukhdeepSingh,Adrian
Stecula, Ashok Thillaisundaram, Catherine Tong, Sergei Yakneen, Ellen D. Zhong, Michal
Zielinski,AugustinÅ½Ã­dek,VictorBapst,PushmeetKohli,MaxJaderberg,DemisHassabis,and
JohnM.Jumper. Accuratestructurepredictionofbiomolecularinteractionswithalphafold3.
Nature,630(8016):493â€“500,2024.
[19] DavidHaandJÃ¼rgenSchmidhuber. Worldmodels. arXivpreprintarXiv:1803.10122,2018.
[20] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114,2013.
[21] DanijarHafner,TimothyLillicrap,IanFischer,RubenVillegas,DavidHa,HonglakLee,and
JamesDavidson.Learninglatentdynamicsforplanningfrompixels.InInternationalconference
onmachinelearning,pages2555â€“2565.PMLR,2019.
[22] BerenMillidge,TommasoSalvatori,YuhangSong,RafalBogacz,andThomasLukasiewicz.
Predictivecoding: towardsafutureofdeeplearningbeyondbackpropagation? arXivpreprint
arXiv:2202.09467,2022.
[23] KarlFriston,FrancescoRigoli,DimitriOgnibene,ChristophMathys,ThomasFitzgerald,and
GiovanniPezzulo. Actionandbehavior: Afree-energyformulation. BiologicalCybernetics,
102(3):227â€“260,2010.
[24] BerenMillidge. Applicationsofthefreeenergyprincipletomachinelearningandneuroscience.
arXivpreprintarXiv:2107.00140,2021.
[25] TakuyaIsomura,KiyoshiKotani,YasuhikoJimbo,andKarlJFriston. Experimentalvalidation
ofthefree-energyprinciplewithinvitroneuralnetworks. NatureCommunications,14(1):4547,
2023.
[26] Corrado Pezzato, Carlos HernÃ¡ndez Corbato, Stefan Bonhof, and Martijn Wisse. Active
inference and behavior trees for reactive action planning and execution in robotics. IEEE
TransactionsonRobotics,39(2):1050â€“1069,2023.
[27] TimSchneider,BorisBelousov,GeorgiaChalvatzaki,DiegoRomeres,DeveshKJha,andJan
Peters.Activeexplorationforroboticmanipulation.In2022IEEE/RSJInternationalConference
onIntelligentRobotsandSystems(IROS),pages9355â€“9362.IEEE,2022.
12
[28] YufeiHuang,YulinLi,AndreaMatta,andMohsenJafari. Navigatingautonomousvehicleon
unmarkedroadswithdiffusion-basedmotionpredictionandactiveinference. arXivpreprint
arXiv:2406.00211,2024.
[29] LancelotDaCosta,PabloLanillos,NoorSajid,KarlFriston,andShujhatKhan. Howactive
inferencecouldhelprevolutioniserobotics. Entropy,24(3):361,2022.
[30] LancelotDaCosta,NoorSajid,ThomasParr,KarlFriston,andRyanSmith. Rewardmaximiza-
tionthroughdiscreteactiveinference. NeuralComputation,35(5):807â€“852,2023.
[31] LesliePackKaelbling,MichaelLLittman,andAnthonyRCassandra. Planningandactingin
partiallyobservablestochasticdomains. Artificialintelligence,101(1-2):99â€“134,1998.
[32] AswinPaul,NoorSajid,LancelotDaCosta,andAdeelRazi. Onefficientcomputationinactive
inference. arXivpreprintarXiv:2307.00504,2023.
[33] David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for
statisticians. JournaloftheAmericanstatisticalAssociation,112(518):859â€“877,2017.
[34] NoorSajid,FrancescoFaccio,LancelotDaCosta,ThomasParr,JÃ¼rgenSchmidhuber,andKarl
Friston. BayesianbrainsandtherÃ©nyidivergence. NeuralComputation,34(4):829â€“855,2022.
[35] PhilippSchwartenbeck,JohannesPassecker,TobiasUHauser,ThomasHBFitzGerald,Martin
Kronbichler, andKarlJFriston. Computationalmechanismsofcuriosityandgoal-directed
exploration. elife,8:e41703,2019.
[36] DeepakPathak,PulkitAgrawal,AlexeiAEfros,andTrevorDarrell.Curiosity-drivenexploration
byself-supervisedprediction. InInternationalconferenceonmachinelearning,pages2778â€“
2787.PMLR,2017.
[37] Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random
networkdistillation. arXivpreprintarXiv:1810.12894,2018.
[38] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-
policymaximumentropydeepreinforcementlearningwithastochasticactor. InInternational
conferenceonmachinelearning,pages1861â€“1870.Pmlr,2018.
[39] RÃ©mi Coulom. Efficient selectivity and backup operators in monte-carlo tree search. In
Internationalconferenceoncomputersandgames,pages72â€“83.Springer,2006.
[40] DavidSilver,JulianSchrittwieser,KarenSimonyan,IoannisAntonoglou,AjaHuang,Arthur
Guez,ThomasHubert,LucasBaker,MatthewLai,AdrianBolton,etal. Masteringthegameof
gowithouthumanknowledge. nature,550(7676):354â€“359,2017.
[41] JulianSchrittwieser,IoannisAntonoglou,ThomasHubert,KarenSimonyan,LaurentSifre,Si-
monSchmitt,ArthurGuez,EdwardLockhart,DemisHassabis,ThoreGraepel,etal. Mastering
atari,go,chessandshogibyplanningwithalearnedmodel. Nature,588(7839):604â€“609,2020.
[42] AlexandrePichÃ©,ValentinThomas,CyrilIbrahim,YoshuaBengio,andChrisPal. Probabilistic
planning with sequential monte carlo methods. In International Conference on Learning
Representations,2018.
[43] JoeMarino,YisongYue,andStephanMandt. Iterativeamortizedinference. InInternational
ConferenceonMachineLearning,pages3403â€“3412.PMLR,2018.
[44] Alexander Tschantz, Beren Millidge, Anil K Seth, and Christopher L Buckley. Control as
hybridinference. arXivpreprintarXiv:2007.05838,2020.
[45] MatthijsVanDerMeer,ZebKurth-Nelson,andADavidRedish. Informationprocessingin
decision-makingsystems. TheNeuroscientist,18(4):342â€“359,2012.
[46] WeiruiYe,ShaohuaiLiu,ThanardKurutach,PieterAbbeel,andYangGao. Masteringatari
gameswithlimiteddata. Advancesinneuralinformationprocessingsystems,34:25476â€“25488,
2021.
13
[47] SamuelGershmanandNoahGoodman. Amortizedinferenceinprobabilisticreasoning. In
Proceedingsoftheannualmeetingofthecognitivesciencesociety,volume36,2014.
[48] CharlesCMargossianandDavidMBlei. Amortizedvariationalinference: Whenandwhy?
arXivpreprintarXiv:2307.11018,2023.
[49] Emilien Dupont, Hyunjik Kim, SM Eslami, Danilo Rezende, and Dan Rosenbaum. From
data to functa: Your data point is a function and you can treat it like one. arXiv preprint
arXiv:2201.12204,2022.
[50] MiltiadisKofinas, BorisKnyazev, YanZhang, YunluChen, GertjanJBurghouts, Efstratios
Gavves,CeesGMSnoek,andDavidWZhang. Graphneuralnetworksforlearningequivariant
representationsofneuralnetworks. arXivpreprintarXiv:2403.12143,2024.
[51] YarinGalandZoubinGhahramani. Dropoutasabayesianapproximation: Representingmodel
uncertaintyindeeplearning.Ininternationalconferenceonmachinelearning,pages1050â€“1059.
PMLR,2016.
[52] IrinaHiggins,LoicMatthey,ArkaPal,ChristopherBurgess,XavierGlorot,MatthewBotvinick,
ShakirMohamed,andAlexanderLerchner. beta-vae: Learningbasicvisualconceptswitha
constrainedvariationalframework. InInternationalconferenceonlearningrepresentations,
2016.
[53] MatthewCrosby,BenjaminBeyret,andMartaHalina. Theanimal-aiolympics. NatureMachine
Intelligence,1(5):257â€“257,2019.
[54] PabloLanillos,CristianMeo,CorradoPezzato,AjithAnilMeera,MohamedBaioumy,Wataru
Ohata, Alexander Tschantz, Beren Millidge, Martijn Wisse, Christopher L Buckley, et al.
Active inference in robotics and artificial agents: Survey and challenges. arXiv preprint
arXiv:2112.01871,2021.
[55] AlbertoLoffredo,MarvinCarlMay,LouisSchÃ¤fer,AndreaMatta,andGiselaLanza. Reinforce-
mentlearningforenergy-efficientcontrolofparallelandidenticalmachines. CIRPJournalof
ManufacturingScienceandTechnology,44:91â€“103,2023.
[56] AlbertoLoffredo,NiclaFrigerio,EttoreLanzarone,andAndreaMatta. Energy-efficientcontrol
inmulti-stageproductionlineswithparallelmachineworkstationsandproductionconstraints.
IISETransactions,56(1):69â€“83,2024.
[57] AlbertoLoffredo,MarvinCarlMay,AndreaMatta,andGiselaLanza. Reinforcementlearning
forsustainabilityenhancementofproductionlines. JournalofIntelligentManufacturing,pages
1â€“17,2023.
[58] SheldonMRoss. Introductiontoprobabilitymodels. Academicpress,2014.
[59] ManzilZaheer,SatwikKottur,SiamakRavanbakhsh,BarnabasPoczos,RussRSalakhutdinov,
andAlexanderJSmola. Deepsets. Advancesinneuralinformationprocessingsystems,30,
2017.
[60] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,
ÅukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. Advancesinneuralinformation
processingsystems,30,2017.
[61] ZongyiLi,NikolaKovachki,KamyarAzizzadenesheli,BurigedeLiu,KaushikBhattacharya,
AndrewStuart,andAnimaAnandkumar. Fourierneuraloperatorforparametricpartialdifferen-
tialequations. arXivpreprintarXiv:2010.08895,2020.
[62] Lu Lu, Pengzhan Jin, Giovanni Pang, Zhiping Zhang, and George Karniadakis. Learning
nonlinearoperatorsviadeeponetbasedontheuniversalapproximationtheoremofoperators.
NatureMachineIntelligence,3(3):218â€“229,2021.
[63] PaoloRennaandSergioMateri. Aliteraturereviewofenergyefficiencyandsustainabilityin
manufacturingsystems. AppliedSciences,11(16):7366,2021.
[64] JohnFrankCharlesKingman. Poissonprocesses,volume3. ClarendonPress,1992.
14
A Application
A.1 Energy-EfficiencyControl
Energy-Efficiency Control (EEC) is attracting growing attention in both academia and industrial
researchwithinmanufacturingsystems. Actingatthecomponent,machine,andproduction-system
levels, EEC can deliver substantial energy savings. Fundamentally, it adjusts an assetâ€™s power-
consumption state to its operating context: equipment remains fully powered when its function
is required and shifts to a low-power state when idle. Implementing this strategy is complicated
bystochasticdemandpatternsandbythepenaltiesincurredduringstatetransitionsâ€”namelythe
productiontimelostwhiletheassetaddsnovalueandtheenergyconsumedbythetransitionitself. A
comprehensive,up-to-datesurveyofthefieldisprovidedin[63]. Motivatedbythese,wefocusona
systemthatreplicatesthekeycharacteristicsofanactualautomotivemanufacturingline.
A.2 SystemDescription
FollowingthebenchmarksetbyLoffredoetal. (2023)[55]â€”andreadilyextensibletoamulti-stage
production line [56]â€”we study a stand-alone workstation comprising a finite-capacity upstream
bufferBthatfeedscidentical,parallelmachines(Fig.5).Partsarrivestochasticallyandeachmachine
mayresideinoneoffivestates: busy,idle,standby,startup,orfailed. Thecorrespondingpowerrates
satisfy:
w >w >w >w â‰ˆw â‰ˆ0.
b su id sb f
Figure5: Layoutofparallel,identicalmachinesintheworkstation[55].
AllsystemprocessesaremodeledasPoissonprocesses[64];thispertainstothearrivalrate(Î»)to
bufferBwithcapacityK,machineprocessingtimes(Âµ),startuptimes(Î´),timebetweenfailures(Ïˆ),
andtimetorepair(Î¾),allwithexpectedvalues,independentandstationary. Table2summarizesthe
parametersusedtoreplicatetherealindustrialcasestudyreportedin[55].
Table2: Parametersforreplicatingtheindustrialsystem[55].
Parameter c K Âµ Î´ Ïˆ Î¾
Value 6 10 0.012 0.033 0.001 0.033
Parameter Î» w w w w w
b id su sb f
Value 0.050 15kW 9.30kW 10kW 0kW 0kW
Eachmachineprocessesasingleparttypeunderafirst-come-first-servedpolicy. Machinescannotbe
powereddownwhileprocessingorduringstartup. IfamachineisreadytoworkbutthebufferB
isempty,itbecomesstarvedandenterstheidlestate. ThecentralchallengeofEECinthissystem
istodynamicallydeterminehowmanymachinesshouldremainactiveversushowmanyshouldbe
transitionedtolow-powerstates. Thisdecisionmustbemadeadaptivelyinresponsetotheunfolding
stochasticconditions,strikinganoptimalbalancebetweenreducingenergyusageandmaintaining
highproductionrate(i.e.,throughput).
A.2.1 Modeling
AsmachinestatetransitionsandpartarrivalsaremodeledasPoissonprocesses[64,55],weadopt
theevent-drivenschemeofLoffredoetal. (2023)[55,57],wherecontroldecisionsaretriggered
15
immediatelyaftereachstatechangeinthesystemratherthanatfixedsamplingintervalsâ€”anapproach
proveneffectiveformanagingactivemachines. Inthisway,thesystemitselfrequestsdecisionsfrom
theagentinastochasticmanner.
Thiscontroltaskadmitstwodifferentformulations[11]: (i)continuous-timestochasticcontrolor
(ii)adiscrete-timeMarkovchain(DTMC)[58]. Acontinuous-timemodelmustprovidetheraw
inter-eventintervalâˆ†ttotheagentforeverymachineandsubsequentobservation,whereastheDTMC
abstractionletstheagentinfertransitionprobabilitiesdirectlyfromobservedevents. Becauseâˆ†t
variesfromeventtoevent,acontinuous-timeformulationwouldhavetoalignthepredictorP (o )
Î¸ t+H
with the reference observation oËœ , whichâ€”although beneficial for planningâ€”complicates the
t+H
networkarchitectureowingtostateoccupancydurations. Therefore,tokeepthemodelsimpler,we
adoptthediscrete-time,event-drivenformulation.
A.3 PreferenceMapping
InAIF,theagentactstoreachitspreferredobservation,akintoasetpointincontroltheory[8,22].
ThisimpliesthattheagentpossessesaninternalpreferencemappingÎ¨,whichquantifieshowclose
itspredictedobservationistoadesiredtarget. Whileconceptuallyrelatedtorewardfunctionsin
RL,thispreferencereflectsacontrol-basedobjectiveratherthancumulativerewardsintheMarkov
DecisionProcessframework[1].
Building on the EEC framework introduced by Loffredo et al. [57], a generic preference or re-
wardfunctionforthemulti-objectiveoptimizationofthesystemunderstudycanincludetermsfor
production,energyconsumption,andaweightedcombinationthereof[11]:
T E
R = current, R =1âˆ’ avg, (7a)
production T energy E
max max
R=Ï•Â·R +(1âˆ’Ï•)Â·R , (7b)
production energy
where Ï• âˆˆ [0,1] is a weighting coefficient balancing the importance of production and energy
efficiency.
Loffredoetal.[57]computedtheproductiontermastheaveragethroughputfromthestartofthe
interaction, and the energy term as the difference between consecutive time steps, followed by
exponentialtransformations. Incontrast,weemployawindow-basedapproach[11],whichbetter
captures the stochastic performance of the system and aligns with the concept of a delayed and
long-horizoncontrolproblem. Specifically,weevaluateaveragesystemperformanceoverafixed
timespant 2,leadinguptothecurrentobservationattimet. Accordingly:
s
NP(t)âˆ’NP(tâˆ’t )
T = s ,
current t
s
C(t)âˆ’C(tâˆ’t )
E = s ,
avg t
s
whereNP(t)isthenumberofpartsproducedandC(t)isthetotalenergyconsumeduptotimet.
T correspondstothemaximumachievablethroughputundertheALLON policy[57],andE
max max
denotesthetheoreticalpeakenergyconsumptionwhenallmachinesoperateinthebusystate.
ToencourageEEC,Ï•istypicallysetcloseto1toavoidexcessiveproductionloss. However,this
linearformulationmayoverestimateperformanceincaseswhereenergysavingsarenegligibleâ€”i.e.,
the composite term remains high due to production alone, even when control is not applied. To
addressthis,weadjustthepreferencefunctionbyapplyingasigmoidtransformationtotheenergy
term:
R=R Â·Ïƒ(c R ), (8)
production r energy
where Ïƒ(x) = 1 is the sigmoid function, and c is a scalar hyperparameter controlling the
1+eâˆ’x r
sensitivity to energy savings. This formulation ensures that energy savings sharply amplify the
preference,therebyenforcingabalancedfocusonbothproductivityandenergysaving. Notably,in
theabsenceofanycontrolactions(i.e.,undertheALLON policy),theenergytermsaturatesnear
zero,andthecompositetermisnaturallylowerthanthatofthelinearformulationinEquation7.
2Eighthoursoroneshiftinourimplementation.
16
B Agent
B.1 Setup
Fortherepresentationoftheactorfunction,weadoptedasimpleapproximatingmappingÎ ,which
concatenatestheinputwithboththefirsthiddenlayerandtheoutputvalues. Inthisway,thepolicy
parametersareintroducedwithinthegenerativemodelandoptimizedviagradientdescentonthe
EFE,whiletherestoftheagentparametersarekeptfixed. WeadheretotheMonteCarlosampling
methodologyforcalculatingtheEFEasoutlinedbyFountasetal. (2020)[12]. Wealsoachieved
similarcontrolperformancebycomputingallEFEtermsusingsingle-loopforwardpassesofthe
generativemodelrepeatedmultipletimes,whichwasfasterandlesscomputationallydemandingthan
themulti-loopschemepresentedinthealgorithm.
BernoulliandGaussiandistributionsareemployedtomodelthepredictionandstatedistributions,
respectively. Wealsoregularizethestatespacebyapplyingnon-linearactivationfunctionstoboth
theencoderandtransitionnetworks. Theoutputsofthesenetworksdefinethemeansandvariancesof
Gaussian-distributedlatentstates. Specifically,weusethetangenthyperbolic(tanh)functionforthe
means,andthesigmoidfunction,scaledbyafactorÎ» âˆˆ[1,2]3,forthevariances. Thiscombination
s
enforcesadditionalregularizationandcontributestothestabilityofthelatentstatespace,ensuring
valuesremainboundedandwell-suitedtoanormaldistribution.
B.1.1 Observation
Giventhattheproblemunderstudyincludesdiscreteandcontinuouselements(aswealsoinclude
preferencescoresinthesystemstates),ithasacompositeformat;ateachdecisionstepttheagent
observes o(t) = (cid:2) o(t), o(t), o(t)(cid:3) , where o(t): discrete is oneâ€“hot buffer-occupancy indicator,
b m r b
o(t): discrete is oneâ€“hot machine-state indicators, and o(t): continuous real-valued preference
m r
scores,includesproduction,energy,compositeterms. Similarly,thegenerativemodeloutputsthe
correspondingpredictionP(o(t)) = [P , P , P ],withallcomponentsgeneratedwithBernoulli
b m r
parameters.Asthepreferenceelementscontainacontinuouscomponent,duringEFEcomputationwe
needtheentropyofthepredictedobservationdistribution.Tokeeptheprocedureanalyticallytractable
andconsistentwiththebinarypartoftheobservation,weapproximatethesecontinuouspreference
outputswithBernoulli-likeparameters;i.e.wetreateachscalarrewardpredictionP âˆˆ[0,1]asif
r
itwerethemeanofaBernoullivariablewhenevaluatingtheentropyterm. Inpractice,thisisan
approximationP alreadyliesin[0,1]â€”yetallowsustoreusethesameclosed-formbinary-entropy
r
expressionforbothdiscreteandcontinuouspreferencechannelstoeasethecomputation.
WhenwecalculatethethirdtermoftheEFE,weneedtofeedthepredictiontotheencoder. We
sample4 theone-hotpartsfirstandthenapplythemtotheencoder. ThisdiffersfromearlierAIF
implementationssuchasFountasetal.[12],whichfeedmeanpixelintensities(i.e.predictedBernoulli
parametersin[0,1])straightbackintotheencoder. Thatworksforimages,butinoursetting(one-hot
vectors)itwould(i)ignorethesemanticsofone-hotcodesand(ii)treateachfeatureindependently,
discardingcorrelations.
B.1.2 ReconstructionLoss
Based on the format of the observation and the respective prediction, we need to distinguish the
reconstructionloss. Accordingly,foramini-batchofsizeN wecompute
N
BCE = 1 (cid:88) BCE (cid:0) P(i), o(i)(cid:1) (9a)
b N b b
i=1
N
BCE = 1 (cid:88) BCE (cid:0) P(i), o(i)(cid:1) (9b)
m N m m
i=1
3WesetÎ» = 1.5toensurethevarianceoutputremainswithinthenon-saturateddomainofthesigmoid
s
function,therebypreservinginformativegradientflowduringtraining.
4Foreaseofcalculation,wetakethemaxforthestates.
17
MAE(i) = (cid:12) (cid:12)P(i)âˆ’o(i)(cid:12) (cid:12) (9c)
r r r
N
MSE = 1 (cid:88)(cid:104) âˆ’log (cid:0) 1âˆ’MAE(i)+Îµ (cid:1)(cid:105) (9d)
r N r
i=1
Thiscombinesbinarycross-entropy(BCE)withanexponential-liketermforthecontinuouselements.
Giventhatpreferenceisgenerallymoreimportantthantheotherelements,andthatbufferlevelis
moreimportantthanthemachinestate,weweighttheseelements. Finally,itiscombinedwiththe
usualÎ²-VAEregularizationtermandpassedtotheoptimizeras:
2 1 4 (cid:0) (cid:1)
L = BCE + BCE + MSE + Î² âˆ—D q(s)âˆ¥N(0,I) . (10)
o 7 b 7 m 7 r KL
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125)
Î²regularization
reconstruction
C Experiments
C.1 Code
Thecodeanddataareavailableathttps://github.com/YavarYeganeh/Deep_AIF.
C.2 TrainingandEvaluationProcedure
Duringeachtrainingepoch,anenvironmentisinitializedwiththeparametersoftheindustrialsystem,
includingstochasticprocesses. Thesystemfirstundergoesaone-daywarm-upperiodinsimulation
timeusingtheALLON policy. Theresultingprofilefromthiswarm-upisdiscarded. Thisisfollowed
by another one-day simulation using a random policy to bring the system to a fully random and
uncontrolled state. Then, the agent, equipped with experience replay, interacts with the system.
Duringeachepoch,themodelistrainedforseveraliterations,withupdatesbothforthemodeland
actoroccurringeveryH steps,followingsamplingfromtheexperiencememory. Aftereachepoch,
theagentâ€™sperformanceisvalidatedonthreeindependentandrandomlyinstantiatedenvironments
thatundergothesamewarm-upandrandominitializationsteps. Thesevalidationepisodesspanone
dayofsimulation,duringwhichnomodellearningorexperiencegatheringoccurs,exceptforgradient
fine-tuningoftheactoreveryH steps. Theperformanceisthenaveraged,andthestandarddeviation
iscomputed. Whileallpreviousperformancemetricsarebasedonthepreferencescore,thebest-
performingagentduringvalidationisretrievedandtestedfor30daysofsimulationon10independent
andrandomlyinstantiatedenvironments. Theseenvironmentsfollowthesameinitializationprotocol,
exceptfora10-daywarm-uptoensureconsistency. Finalperformanceofrelativeenergyefficiencyis
averagedoverthe10environments,alongwiththecomputationofstandarddeviations.
18

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Deep Active Inference Agents for Delayed and Long-Horizon Environments"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   âŒ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   âœ… GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   âŒ BAD: Repeating the same claim 3+ times with slight variations
   âœ… GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
