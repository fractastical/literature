=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Capsule Networks as Generative Models
Citation Key: kiefer2022capsule
Authors: Alex B. Kiefer, Beren Millidge, Alexander Tschantz

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2022

Key Terms: scene, university, sussex, neural, models, capsule, architecture, generative, visual, networks

=== FULL PAPER TEXT ===

Capsule Networks as Generative Models
Alex B. Kiefer∗1,2, Beren Millidge∗1,3, Alexander Tschantz∗1, and
Christopher L Buckley1,4
1 VERSES Research Lab
2 Monash University
3 MRC Brain Network Dynamics Unit, University of Oxford
4 Sussex AI Group, Department of Informatics, University of Sussex
Abstract. Capsule networks are a neural network architecture special-
ized for visual scene recognition. Features and pose information are ex-
tractedfromasceneandthendynamicallyroutedthroughahierarchyof
vector-valued nodes called ‘capsules’ to create an implicit scene graph,
withtheultimateaimoflearningvisiondirectlyasinversegraphics.De-
spite these intuitions, however, capsule networks are not formulated as
explicitprobabilisticgenerativemodels;moreover,theroutingalgorithms
typically used are ad-hoc and primarily motivated by algorithmic intu-
ition. In this paper, we derive an alternative capsule routing algorithm
utilizingiterativeinferenceundersparsityconstraints.Wethenintroduce
anexplicitprobabilisticgenerativemodelforcapsulenetworksbasedon
the self-attention operation in transformer networks and show how it is
relatedtoavariantofpredictivecodingnetworksusingVon-Mises-Fisher
(VMF) circular Gaussian distributions.
1 Introduction
Capsule networks are a neural network architecture designed to accurately cap-
ture and represent part-whole hierarchies, particularly in natural images [17,
18,39], and have been shown to outperform comparable CNNs at visual object
classification, adversarial robustness, and ability to segment highly overlapping
patterns [18,39]. A capsule network comprises layers of ‘capsules’ where each
capsule represents both the identity and existence of a visual feature as well as
its current ‘pose’ (position, orientation, etc.) relative to a canonical baseline.
Thisapproachisheavilyinspiredbytheconceptofascenegraphincomputer
graphics, which represents the objects in a scene in precisely such a hierarchical
tree structure where lower-level objects are related to the higher-level nodes by
theirpose.Thecapsulenetworkaimstoflexiblyparameterizesuchascenegraph
as its generative model and then perform visual object recognition by inverting
this generative model [16,17] to infer 3D scene structure from 2D appearances.
It is argued that the factoring of scene representations into transformation-
equivariantcapsuleactivityvectors(i.e.vectorsthatchangelinearlywithtrans-
lation, rotation, etc.) and invariant pose transformation matrices is more flexi-
ble and efficient than the representation used in convolutional neural networks,
2202
tcO
6
]CN.oib-q[
2v76520.9022:viXra
2 Kiefer, Millidge, Tschantz, et al
whereactivitiesinhigherlayersaremerelyinvarianttochangesinviewpoint.In
addition to arguably providing a better scene representation, capsule networks
can use agreement between higher-level poses and their predictions based on
lower-level poses to solve the binding problem of matching both the ‘what’ and
the ‘where’ of an object or feature together.
Capsule networks are in part motivated by the idea that ‘parse-trees’ of the
object hierarchy of a scene must be constructed at run-time, since they can
be different for different images. Crucially, it is assumed that this dynamically
constructed parse-tree must be sparse and almost singly connected - each low-
level capsule or feature can be matched to only one high-level parent. This is
because in natural scenes it is sensible to assume that each feature only belongs
tooneobjectatatime–forinstance,itisunlikelythatoneeyewillbelongtotwo
facessimultaneously.In[39],itisproposedtodynamicallyconstructtheseparse-
trees by an algorithm called ‘routing by agreement’ whereby low-level capsules
are assigned to the high-level capsule whose pose matrix most closely matches
their pose matrix under certain transformations.
Whilecapsulenetworksappeartobeahighlyefficientarchitecture,invented
usingdeepinsightsintothenatureofvisualscenes,thereare,nevertheless,many
elementsoftheconstructionthatappearrelativelyad-hoc.Thereisnoconstruc-
tion of an explicit probabilistic generative model of the network. Moreover, it is
unclear why the routing algorithm works and how it is related to other frame-
works in machine learning. Indeed, some research [31,36] suggests that typical
routing algorithms do not perform well which suggests that the goals of routing
are better attained in some other way.
Inthispaperweproposeaprobabilisticinterpretationofcapsulesnetworksin
terms of Gaussian mixture models and VMF (circular Gaussian) distributions,
which applies the self-attention mechanism used in modern transformer net-
works[15,47].Wearguethatfundamentally,thepurposeoftheoriginalrouting-
by-agreement algorithm of [39] is to approximate posterior inference under a
generative model with the particular sparsity structure discussed above. We
first demonstrate in experiments that we can achieve routing-like behaviour us-
ing sparse iterative inference, and show in addition that even in the original
implementation of dynamic routing in capsules [39], sparsity of the top-level
capsules is enforced via the margin loss function alone when iterative routing is
turned off. This loss function can be interpreted as implementing a low-entropy
prior on digit classes. We then write down a principled top-down generative
model for capsules networks that provides a plausible description of the model
that routing attempts to approximately invert. Overall, our results aim to pro-
vide a clear and principled route toward understanding capsule networks, and
interpreting the idea of routing as fundamentally performing sparse iterative in-
ference to construct sparse hierarchical program trees at runtime – a method
that can be implemented in many distinct ways.
Capsule Networks as Generative Models 3
2 Capsule Networks
Acapsulenetworkcomprisesahierarchicalsetoflayerseachofwhichconsistsof
a large number of parallel capsules. In practice, several non-capsule layers such
as convolutional layers are often used to provide input data preprocessing. We
do not consider non-capsule layers in this analysis.
Each capsule j in a layer receives an input vector s consisting of a weighted
j
combination of the outputs of the capsules i in the layer below, multiplied by
their respective affine transformation matrices T , which define the invariant
i,j
relationshipsbetweentheposesrepresentedbyiandj.Theinputfromcapsulei
isdenoteduˆ =T v ,wherev istheoutputactivityofcapsuleiafteritsinput
j|i i,j i i
has been passed through a ‘squash’ nonlinearity defined as f(x)= ||x||2 · x .
1+||x||2 ||x||
The higher-level capsule then weights the contributions from its low-level input
capsulesbyweightingcoefficientsc whicharedeterminedbyiterativerouting.
i,j
Toobtaintheoutputofthecapsule,allitsinputsareweightedandsummedand
then the output is fed through the nonlinear activation function f. The forward
pass of a capsule layer can thus be written as,
(cid:88)
v =f( c T v ) (1)
(l)j i,j i,j (l−1)
i
i
The core algorithm in the capsule network is the routing-by-agreement algo-
rithm which iteratively sets the agreement coefficients c :
i,j
bk =bk−1+(T v )Tvk−1
i,j i,j i,j (l−1) i (l) j
(2)
ck =σ(bk−1)
i i
where k is the iteration index of the routing algorithm, σ(x) is the softmax
functionsuchthatσ(x) = exp(xi) ,andbk arethelogitinputstothesoftmax
i (cid:80)
j
exp(xj) i
at iteration k, which act as log priors on the relevance of lower-level capsule i’s
output to all the higher-level capsules. These are initialized to 0 so all capsules
are initially weighted equally.
The routing algorithm weights the lower-level capsule’s contribution to de-
termining the activities at the next layer by the dot-product similarity between
theinputfromthelow-levelcapsuleandthehigher-levelcapsule’soutputatthe
previous iteration. Intuitively, this procedure will match the pose of the higher-
level capsule and the ‘projected pose’ uˆ = T v from the lower-level
j|i i,j (l−1)
i
capsule, so that each lower-level capsule will predominantly send its activity
to the higher-level capsule whose pose best fits its prediction. In addition to
matching parts to wholes, this procedure should also ensure that only higher-
level capsules that receive sufficiently accurate pose-congruent ‘votes’ from the
capsules below are activated, leading to the desired sparsity structure in the
inferred scene representation.
4 Kiefer, Millidge, Tschantz, et al
3 Sparse Capsule PCN
Intuitively, the goal of routing is to match the poses of higher- and lower-level
capsules and thus to construct a potential parse tree for a scene in terms of
relations between higher-level ‘objects’ and lower-level ’features’. Crucially, this
parse tree must be highly sparse such that, ideally, each lower-level feature is
bound to only a single high-level object. To represent uncertainty, some assign-
ment of probability to other high-level capsules may be allowed, but only to a
few alternatives.
We argue that all of this is naturally accommodated if we interpret routing
as implementing sparse iterative inference, where the sparsity constraints derive
from an implicit underlying generative model. This is because the fundamental
goal of routing is to obtain a ‘posterior’ over the capsule activations throughout
the network given the input as ‘data’. Unlike standard neural networks, this
posterior is not only over the classification label at the output but over the
‘parsetree’comprisingactivationsattheintermediatelayers.Takinginspiration
from Predictive Coding Networks (PCNs) [3,6,12,27], we can imagine the parse
tree posterior as being inferred in a principled way through iterative variational
inference [2,48] applied to the activities at each layer during a single stimulus
presentation.
The idea of using variational inference to perform capsule routing is also
explored and shown to be very effective in [38]. Most closely related to our aims
here,[42]proposeafullgenerativemodelandvariationalinferenceprocedurefor
capsulesnetworks,focusinginsteadontheE-Mroutingversionofcapsules[18]in
which existence is explicitly represented using a distinct random variable. They
show that performing iterative inference to further optimize solutions at test
time leads to improved digit reconstructions for rotated MNIST digits. [28] also
proposes a generative model that aims to capture the intuitions behind capsule
networks, and likewise derives a variational inference scheme for inverting this
model.
There are various ways to achieve sparsity. [36] investigated unsupervised
versions of capsules networks, and found that while routing in the CapsNet
architecture did not produce the intended effects (i.e. sparse activations at each
capsule layer and feature equivariance) without supervision at the output layer,
these properties could be restored by adding a sparsity constraint adapted from
k-sparseautoencoders[24].A‘lifetimesparsityconstraint’thatforcesallcapsules
to be active a small fraction of the time was also found to be necessary to
discouragesolutionsinwhichasmallnumberofcapsulesareusedtoreconstruct
the input and the rest are ignored (which interferes with the ability to learn
the desired equivariances). We experiment with a simpler form of sparsity in
combinationwithiterativePCinference,usinganL1penalty,whichisknownto
encourage sparsity, as an additional regularizing term added to the free energy.
In Appendix B, we demonstrate this effect on a toy two-layer network where
sparse iterative inference routs all inputs through a specific intermediate layer,
thus constructing a single preferred parse-tree.
Capsule Networks as Generative Models 5
3.1 Experiments
To test our interpretation of iterative routing-by-agreement as inference under
sparsity constraints, we investigated the role of routing in the canonical ‘Cap-
sNet’ capsules network proposed in [39]. This network, diagrammed in Fig. 1A,
consistsofapreliminaryconventionalconvolutionallayer,followedbyaconvolu-
tionalcapsuleslayer,andafinallayerwhosetencapsulesaremeanttorepresent
the presence of the digits 0-9 in input images. Three iterations of the routing-
by-agreement algorithm are used between the two capsules layers.
Fig.1. A: CapsNet architecture. Input (left) is passed through Conv1, yielding 256
feature maps which provide the input to the first (convolutional) capsules layer, Pri-
maryCaps. This yields 1152 (32 dimensions x 6 * 6 output) 8-dimensional capsules,
which are each connected to each of the 10 16-dimensional DigitCaps via their own
transformationmatrices.TheL2normsofthedigitcapsulesarethencomparedwitha
one-hot encoding of the target. The auxiliary reconstruction network is not pictured.
Rows B-E: Samples of DigitCaps activity vectors for test set examples under varying
conditions. Red borders indicate correct digits and the number above each box is the
correspondingvectornorm.B:DigitCapsactivitiesusinganetworktrainedwiththree
iterations of dynamic routing. Sparsity is clearly enforced at the capsule level, though
it is more extreme in some cases than others. C: Two random activity vectors from
an otherwise identical network trained without routing. Note that the ambiguous im-
age on the left predictably leads to less decisive capsule outputs (note also that this
occurrence was not unique to the no-routing condition). D: Capsule network trained
withoutrouting,with500iterationsofiterativeinferenceperformedinplaceofrouting
at inference time. E: Same as (D) but with an L1 regularization term on the capsule
activities (i.e. Σ (cid:107)v (cid:107)) added to the standard predictive coding (squared prediction
j j
error) loss function.
6 Kiefer, Millidge, Tschantz, et al
Intheexperimentsreportedin[39],thenetworkistrainedtoclassifyMNIST
digitsviabackpropagation,usingaseparate‘marginloss’functionforeachdigit:
L =T max(0,m+−(cid:107)v (cid:107))2+λ(1−T )max(0,(cid:107)v (cid:107)−m−)2 (3)
k k k k k
Here, L is the margin loss for digit k (0 through 9), T is a Boolean indicating
k k
thepresenceofthatdigitintheinputimage,(cid:107)v (cid:107)istheL2normofcapsuleout-
k
putv ,andm+ andm− arethresholdsusedtoencouragethesevectornormsto
k
be close to 1 or 0, in the digit-present or digit-absent conditions, respectively. λ
isanadditionaltermusedtodown-weightthecontributionofnegativeexamples
to early learning. In the full CapsNet architecture, this loss is combined with a
downweighted reconstruction regularization term from an auxiliary reconstruc-
tion network used to encourage capsule activities to capture relevant features in
the input.
In our experiments, we first trained a standard CapsNet on MNIST for 13
epochs, using the same hyperparameters and data preprocessing as in [39]. It is
worth noting that, as a baseline for comparison, CapsNet does indeed produce
sparse higher-level capsule activations (i.e. sparsity in the vector of L2 norms of
the activity vectors of each of the 10 capsules in the DigitCaps layer). However,
in Fig. 1C we show that training the same network architecture with 0 rout-
ing iterations (simply setting the higher-level capsule activities to the squashed
sum of the unweighted predictions from lower layers) produces sparsity as well,
suggesting that the transformation matrices learn to encode this sparsity struc-
ture based on the margin loss function alone. In addition to exhibiting simi-
lar higher-level capsule activities, the two networks also performed comparably
(with > 99% accuracy on the test set) after training, though the no-routing
network was initially slower to converge (see Appendix A).
To test whether the intuitions explored in the toy example in Appendix
B would play out in a larger-scale architecture, we also tried using iterative
inference in place of dynamic routing. In these experiments, we began with a
forward pass through a trained CapsNet, clamped the target (label) nodes, and
ran iterative inference for 500 iterations with a learning rate of 0.01, either with
the standard squared-error predictive coding objective or with the standard PC
loss plus L1 regularization applied to the final output vector of the CapsNet
(see Appendix C). We found that, as in the toy experiment, standard iterative
inference with an L1 penalty (in this case applied per-capsule) produced sparse
outputs,whilewithouttheL1penaltyactivitywasmoreevenlydistributedover
thecapsules,thoughthevectornormsforthe‘correct’capsuleswerestilllongest.
Overall,ourfindingsonCapsNetareconsistentwithresultsreportedin[36],
which suggest that the sparsity seen at the output layer of CapsNet is at-
tributable to its supervised learning objective alone and does not occur without
this objective. Further confirming these results, we performed experiments on
a modification of CapsNet with an intermediate capsule layer between Prima-
ryCaps and DigitCaps, and did not observe sparsity in the intermediate-layer
activitiesdespitecomparableperformance.Despiteourlargelynegativefindings,
theseexperimentssupportourbroaderviewthatthemainpointofroutingisto
Capsule Networks as Generative Models 7
induce sparsity in the capsule outputs, and that this objective can be achieved
byvariousmeans,includingiterativeinferenceinapredictivecodingnetwork.In
the following section we propose an explicit generative model for capsules that
produces the right kind of sparsity in a principled way.
4 A Generative Model for Capsules
Tobefullyconsistentwiththegoaloflearningvisionasinversecomputergraph-
ics, a capsules network should be formulated as a top-down model of how 2D
appearances are generated from a hierarchy of object and part representations,
whose inversion recovers a sensible parse-tree. We now develop an explicit prob-
abilistic generative model for the capsule network that achieves this which, in-
terestingly, involves the self-attention mechanism used in transformer networks.
4.1 Attention and the Part-Whole Hierarchy
In recent years it has become increasingly clear that neural attention [15,47]
provides the basis for a more expressive class of artificial neural network that
incorporates interactions between activity vectors on short timescales. As noted
in [39], while conventional neural networks compute their feedforward pass by
taking the dot products of weight vectors with activity vectors, neural atten-
tion relies on the the dot product between two activity vectors, thus producing
representations that take short-term context into account. In particular, atten-
tion allows for the blending of vectors via a weighted sum, where the weights
depend on dot-product similarities between input and output vectors. The core
computation in neural attention can be written as,
Z=σ(QKT)V (4)
withZbeingtheoutputoftheattentionblock,K,Q,Vbeingthe‘Key’,‘Query’,
and ‘Value’ matrices, and σ the softmax function, as above. Intuitively, the at-
tentionoperationcanbethoughtofasfirstcomputing‘similarityscores’between
the query and key matrices and then normalizing them with the softmax. The
similarity scores are then multiplied by the value matrix to get the output. In
the transformer architecture [1,47], these matrices are typically produced from
a given input representation (e.g. a word embedding) via a learned linear trans-
formation.
There is a tempting analogy between the capsule layer update and neural
attention, since the output capsule activities are determined as a blend of func-
tions of the inputs, using weights determined by applying the softmax function
to dot-product similarity scores. A key difference, which seems to ruin the anal-
ogy, is that in routing-by-agreement, each of the weights that determine the
output mixture comes from a distinct softmax, over the outputs of one lower-
level capsule. Simply swapping out a neural attention module for the routing
8 Kiefer, Millidge, Tschantz, et al
algorithm gives the wrong result however, since this enforces a ‘single-child con-
straint’ where in the limit each higher-level object is connected to at most one
lower-level part in the parse-tree.
Itturnsouthoweverthattheattentionmechanismispreciselywhatisneeded
to naturally construct a top-down generative model of parse trees within a cap-
sules architecture. Firstly, we note that we can aggregate the small transforma-
tion matrices T connecting input capsule i to output capsule j into a large
i,j
 
T ... T
1,1 m,1
tensor structure W = 

. .
.
... . .
.


, for m lower-level and n higher-level
T ... T
1,n m,n
capsules. Similarly, the N individual d-dimensional capsule vectors in a layer
can be stacked to form an N × d matrix with vector-valued entries, V =
(l)
[v ,v ...v ]T, and the routing coefficients c collected into a matrix C
(l)1 (l)2 (l)N ij
withthesameshapeasW.Wecanthenwritetheforwardpassthroughavector
of capsules in a way that is analogous to a forward pass through a large ANN:
(cid:104) (cid:105)
V =f (C(cid:12)W)V (5)
(l) (l−1)
Here, (cid:12) denotes element-wise multiplication and the nonlinearity f is also ap-
plied element-wise. The expression WV should be read as a higher-level
(l−1)
matrix-matrixmultiplicationinwhichmatrix-vectormultiplicationisperformed
in place of scalar multiplication per element, i.e. V =
(cid:80)m
C W V .
Thistermimplementsthesumofpredictions
(cid:80)m
uˆ
(l)j
froml
i
o
=
w
1
er-
j
l
i
evel
ji
cap
(l
s
−
u
1
l
)
e
i
s
i=1 j|i
for the pose of each higher-level capsule j, where each transformation matrix is
first scaled by the appropriate entry in the routing coefficient matrix C.
Wehavearguedthatwhattheforwardpassinequation5aimstoimplement
isineffectposteriorinferenceunderatop-downgenerativemodel.Towritedown
such a model, we first define W˜ as the transpose of the original weight tensor
W,i.e.anm×ncollectionoftransformationsfromnhigher-levelcapsulestom
lower-level capsules. Since each row of W˜ collects the transformations from all
higher-level capsules to the intrinsic coordinate frame of one lower-level capsule
 (W˜ (cid:12)V )T 
1 (l)
v ,wecanthendefineamatrixUˆ = . . thatcontainsall
(l−1)i (l−1)  . 
(W˜ (cid:12)V )T
m (l)
the predictions for the lower-level capsules, where matrix-vector multiplication
is applied element-wise to the submatrices.
Setting V = K = Uˆ and Q = V , we can then frame the top-
(l−1) (l−1)
i i
down generation of one lower-level capsule vector within a capsules network as
an instance of neural attention, where Vk is the matrix of capsule activities at
(l)
layer l and iteration k, as above:
Vk =σ(Vk−1 UˆT )Uˆ (6)
(l−1) i (l−1) i (l−1) i (l−1) i
These updates are independent for each lower-level capsule but can clearly be
vectorized by adding an extra leading dimension of size m to each matrix, so
that an entire attention update is carried out per row.
Capsule Networks as Generative Models 9
The above can be viewed as an inverted version of routing-by-agreement
in which the higher-level capsules cast ‘votes’ for the states of the lower-level
capsules, weighted by the terms in the softmax which play a role similar to
routing coefficients. There is also a relation to associative memory models [21,
26,35] since we can think of the capsule network as associating the previous
lower-level output (query) to ‘memories’ consisting of the predictions from the
layer above, and using the resulting similarity scores to update the outputs as a
blend of the predictions weighted by their accuracy.
Crucially, when used in this way for several recurrent iterations, neural at-
tentionencourageseachrowoftheoutputmatrixtobedominatedbywhichever
input it is most similar to. Since the output in this case is the lower level in a
part-whole hierarchy (where rows correspond to capsule vectors), this precisely
enforces the single-parent constraint that routing-by-agreement aspires to.
In the routing-by-agreement algorithm, the routing logits b are initially set
ij
to 0 (or to their empirical prior value, if trained along with the weights) and
then accumulate the dot-product similarities during each iteration of routing.
It is clear that the application of attention alone without the accumulation of
log evidence over iterations should produce a routing-like effect, since there is
a positive feedback loop between the similarities and the softmax weights. If
one wanted to emulate routing-by-agreement more closely, the above could be
supplemented with an additional recurrent state formed by the similarity scores
of the previous iteration.
While the single-parent constraint alone is not sufficient to ensure that only
lower-level capsules that are a good fit with some active higher-level capsule are
activated, it is reasonable to expect that when no constant relationship between
a higher- and lower-level entity exists in the data, training would encourage the
weights of the corresponding transformation matrix to be close to 0, on pain of
inappropriatelyactivatingalower-levelcapsule,whichwouldleadtoanincrease
in the loss function (e.g. squared prediction error).
4.2 Probabilistic Generative Model
We now formulate the generative model sketched above explicitly in probabilis-
tic terms, which therefore also doubles as a generative model of transformer
attention (see Appendix D).
As remarked above, capsule networks can be seen as combining something
like a standard MLP forward pass with additional vector-level operations. In
particular, for a given lower-level capsule i, the attention mechanism can be
interpreted as mixing the MLPs defined by each capsule pair (i,j) with the
weights given by the attention softmax. If we interpret each MLP update in
terms of a Gaussian distribution, as in PCNs (that is, where the uncertainty
about the hidden state V is Gaussian around a mean given by the sum of
(l−1)
weighted ‘predictions’), we arrive at a mixture of Gaussians distribution over
each lower-level capsule pose, whose mixing weights are determined by scaled
dot-product similarity scores.
10 Kiefer, Millidge, Tschantz, et al
These similarity scores must be interpreted differently from the MLP-like
part of the model, and we argue that these are correctly parametrized via a
von Mises-Fisher (VMF) distribution with a mean of the pose input. The VMF
implements the dot-product similarity score required by the routing coefficient
inaprobabilisticway,andcanbethoughtofasparametrizingadistributionover
vectoranglesbetweentheoutputandinputposes,whichisappropriatesincethe
purposeoftheroutingcoefficientsistoreinforcepredictionsthatmatchahigher-
levelpose(wheredegreeofmatchcanbecapturedintermsoftheanglebetween
the pose and prediction vectors). Importantly, since it parametrizes an angle
the distribution is circular since angles ‘wrap-around’. The VMF distribution
is a Gaussian defined on the unit hypersphere and so correctly represents this
circular property.
Given the above, we can express the update in equation 6 above as a proba-
bilistic generative model as follows:
(cid:88)(cid:104) (cid:105)
p(V |Uˆ )= π N(V ;Uˆ ,σ )
(L) (L) (i) (L) (L) ij
i i j i ij
j
π =Cat(n,p )
(i) (i)
(cid:104) (cid:105)
p =σ VMF(V ;Uˆ ,κ )... VMF(V ;Uˆ ,κ )
(i) (L) (L) i1 (L) (L) in
i i1 i in
(7)
where π are the mixing weights, σ is the standard deviation of the Gaussian
(i) ij
distribution over capsule i conditioned on higher-level capsule j, and κ is the
ij
‘concentration parameter’ of the corresponding VMF distribution, which deter-
mineshowtightlyprobabilitymassisconcentratedonthedirectiongivenbythe
mean Uˆ .
(L)
ij
The generative model then defines the conditional probability of an entire
capsule layer given the predictions as the product of these per-capsule mixture
distributions, mirroring the conditional independence of neurons within a layer
in conventional MLPs:
(cid:89)
p(V |Uˆ )= p(V |Uˆ ) (8)
(L) (L) (L) (L)
i i
i
It would be simpler to use the attention softmax itself directly to determine the
mixingweightsofeachGMM,butusingtheprobabilitiesreturnedbyindividual
VMF distributions instead affords a fully probabilistic model of this part of the
generative process, where not only the vector angle match but also the variance
can be taken into account for each capsule pair i,j. It remains for future work
to write down a process for inverting this generative model using variational
inference.
Capsule Networks as Generative Models 11
5 Conclusion
In this paper, we have aimed to provide a principled mathematical interpre-
tation of capsule networks and link many of its properties and algorithms to
other better known fields. Specifically, we have provided a probabilistic genera-
tive model of the capsule network in terms of Gaussian and VMF distributions
which provides a principled mathematical interpretation of its core computa-
tions. Secondly, we have shown how the ad-hoc routing-by-agreement algorithm
described in [39] is related to self-attention. Moreover, we have demonstrated
both in a toy illustrative example and through large-scale simulation of capsule
networks how the desiderata of the routing algorithm can be achieved through
a general process of sparse iterative inference.
Acknowledgements
AlexKieferandAlexanderTschantzaresupportedbyVERSESResearch.Beren
Millidge is supported by the BBSRC grant BB/S006338/1 and by VERSES
Research. CLB is supported by BBRSC grant number BB/P022197/1 and by
Joint Research with the National Institutes of Natural Sciences (NINS), Japan,
program No. 0111200.
Code Availability
Code for the capsules network is adapted from
https://github.com/adambielski/CapsNet-pytorch
and can be found at: https://github.com/exilefaker/capsnet-experiments. Code
reproducing the toy model experiments and figure in Appendix B can be found
at: https://github.com/BerenMillidge/Sparse Routing.
References
1. Bahdanau,D.,Cho,K.,Bengio,Y.:Neuralmachinetranslationbyjointlylearning
to align and translate. arXiv preprint arXiv:1409.0473 (2014)
2. Beal, M.J.: Variational algorithms for approximate Bayesian inference. Tech. rep.
(2003)
3. Bogacz,R.:Atutorialonthefree-energyframeworkformodellingperceptionand
learning. Journal of mathematical psychology 76, 198–211 (2017)
4. Bricken, T., Pehlevan, C.: Attention approximates sparse distributed memory.
arXiv preprint arXiv:2111.05498 (2021)
5. Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Nee-
lakantan,A.,Shyam,P.,Sastry,G.,Askell,A.,etal.:Languagemodelsarefew-shot
learners. arXiv preprint arXiv:2005.14165 (2020)
6. Buckley, C.L., Kim, C.S., McGregor, S., Seth, A.K.: The free energy principle for
action and perception: A mathematical review. Journal of Mathematical Psychol-
ogy 81, 55–79 (2017)
12 Kiefer, Millidge, Tschantz, et al
7. Buzsa´ki,G.,Mizuseki,K.:Thelog-dynamicbrain:howskeweddistributionsaffect
network operations. Nature Reviews Neuroscience 15(4), 264–278 (2014)
8. Chen,L.,Lu,K.,Rajeswaran,A.,Lee,K.,Grover,A.,Laskin,M.,Abbeel,P.,Srini-
vas, A., Mordatch, I.: Decision transformer: Reinforcement learning via sequence
modeling. Advances in neural information processing systems 34, 15084–15097
(2021)
9. De Zeeuw, C.I., Hoebeek, F.E., Bosman, L.W., Schonewille, M., Witter, L.,
Koekkoek,S.K.:Spatiotemporalfiringpatternsinthecerebellum.NatureReviews
Neuroscience 12(6), 327–344 (2011)
10. Demircigil,M.,Heusel,J.,Lo¨we,M.,Upgang,S.,Vermet,F.:Onamodelofasso-
ciative memory with huge storage capacity. Journal of Statistical Physics 168(2),
288–299 (2017)
11. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is
worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929 (2020)
12. Friston,K.:Atheoryofcorticalresponses.PhilosophicaltransactionsoftheRoyal
Society B: Biological sciences 360(1456), 815–836 (2005)
13. Graham, D.J., Field, D.J.: Sparse coding in the neocortex. Evolution of nervous
systems 3, 181–187 (2006)
14. Greff,K.,Srivastava,R.K.,Schmidhuber,J.:Highwayandresidualnetworkslearn
unrolled iterative estimation. arXiv preprint arXiv:1612.07771 (2016)
15. Gregor, K., Danihelka, I., Graves, A., Rezende, D., Wierstra, D.: Draw: A recur-
rentneuralnetworkforimagegeneration.In:Internationalconferenceonmachine
learning. pp. 1462–1471. PMLR (2015)
16. Hinton, G.: How to represent part-whole hierarchies in a neural network. arXiv
preprint arXiv:2102.12627 (2021)
17. Hinton, G.E., Krizhevsky, A., Wang, S.D.: Transforming auto-encoders. In: Inter-
national conference on artificial neural networks. pp. 44–51. Springer (2011)
18. Hinton, G.E., Sabour, S., Frosst, N.: Matrix capsules with em routing. In: Inter-
national conference on learning representations (2018)
19. Jastrzbski, S., Arpit, D., Ballas, N., Verma, V., Che, T., Bengio, Y.: Residual
connections encourage iterative inference. arXiv preprint arXiv:1710.04773 (2017)
20. Kanerva, P.: Sparse Distributed Memory. MIT Press (1988)
21. Krotov, D., Hopfield, J.: Large associative memory problem in neurobiology and
machine learning. arXiv preprint arXiv:2008.06996 (2020)
22. Krotov, D., Hopfield, J.J.: Dense associative memory for pattern recognition. Ad-
vances in Neural Information Processing Systems 29, 1172–1180 (2016)
23. Lamme,V.A.,Roelfsema,P.R.:Thedistinctmodesofvisionofferedbyfeedforward
and recurrent processing. Trends in neurosciences 23(11), 571–579 (2000)
24. Makhzani, A., Frey, B.J.: k-sparse autoencoders. CoRR abs/1312.5663 (2014)
25. Melloni,L.,vanLeeuwen,S.,Alink,A.,Mu¨ller,N.G.:Interactionbetweenbottom-
up saliency and top-down control: how saliency maps are created in the human
brain. Cerebral cortex 22(12), 2943–2952 (2012)
26. Millidge,B.,Salvatori,T.,Song,Y.,Lukasiewicz,T.,Bogacz,R.:Universalhopfield
networks: A general framework for single-shot associative memory models. arXiv
preprint arXiv:2202.04557 (2022)
27. Millidge, B., Seth, A., Buckley, C.L.: Predictive coding: a theoretical and experi-
mental review. arXiv preprint arXiv:2107.12979 (2021)
28. Naza´bal, A., Williams, C.K.I.: Inference for generative capsule models. CoRR
abs/2103.06676 (2021), https://arxiv.org/abs/2103.06676
Capsule Networks as Generative Models 13
29. Olshausen,B.A.,Field,D.J.:Emergenceofsimple-cellreceptivefieldpropertiesby
learning a sparse code for natural images. Nature 381(6583), 607–609 (1996)
30. Olshausen, B.A., Field, D.J.: Sparse coding of sensory inputs. Current opinion in
neurobiology 14(4), 481–487 (2004)
31. Paik,I.,Kwak,T.,Kim,I.:Capsulenetworksneedanimprovedroutingalgorithm.
ArXiv abs/1907.13327 (2019)
32. Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., Tran, D.:
Image transformer. In: International conference on machine learning. pp. 4055–
4064. PMLR (2018)
33. Pearl, J.: Probabilistic reasoning in intelligent systems: networks of plausible in-
ference. Morgan kaufmann (1988)
34. Radford,A.,Wu,J.,Child,R.,Luan,D.,Amodei,D.,Sutskever,I.,etal.:Language
models are unsupervised multitask learners. OpenAI Blog 1(8), 9 (2019)
35. Ramsauer,H.,Scha¨fl,B.,Lehner,J.,Seidl,P.,Widrich,M.,Adler,T.,Gruber,L.,
Holzleitner, M., Pavlovi´c, M., Sandve, G.K., et al.: Hopfield networks is all you
need. arXiv preprint arXiv:2008.02217 (2020)
36. Rawlinson, D., Ahmed, A., Kowadlo, G.: Sparse unsupervised capsules generalize
better. ArXiv abs/1804.06094 (2018)
37. Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S.G., Novikov, A., Barth-Maron,
G.,Gimenez,M.,Sulsky,Y.,Kay,J.,Springenberg,J.T.,etal.:Ageneralistagent.
arXiv preprint arXiv:2205.06175 (2022)
38. Ribeiro,F.D.S.,Leontidis,G.,Kollias,S.D.:Capsuleroutingviavariationalbayes.
In: AAAI. pp. 3749–3756 (2020)
39. Sabour,S.,Frosst,N.,Hinton,G.E.:Dynamicroutingbetweencapsules.Advances
in neural information processing systems 30 (2017)
40. Schweighofer, N., Doya, K., Lay, F.: Unsupervised learning of granule cell sparse
codes enhances cerebellar adaptive control. Neuroscience 103(1), 35–50 (2001)
41. Shepherd, G.M., Grillner, S.: Handbook of brain microcircuits. Oxford University
Press (2018)
42. Smith, L., Schut, L., Gal, Y., van der Wilk, M.: Capsule networks - A probabilis-
tic perspective. CoRR abs/2004.03553 (2020), https://arxiv.org/abs/2004.
03553
43. Sterling, P., Laughlin, S.: Principles of neural design. MIT press (2015)
44. Theeuwes, J.: Top–down and bottom–up control of visual selection. Acta psycho-
logica 135(2), 77–99 (2010)
45. Tschantz, A., Millidge, B., Seth, A.K., Buckley, C.L.: Hybrid predictive coding:
Inferring, fast and slow. arXiv preprint arXiv:2204.02169 (2022)
46. VanRullen, R.: The power of the feed-forward sweep. Advances in Cognitive Psy-
chology 3(1-2), 167 (2007)
47. Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,
L(cid:32)., Polosukhin, I.: Attention is all you need. In: Advances in Neural Information
Processing Systems. pp. 5998–6008 (2017)
48. Wainwright, M.J., Jordan, M.I., et al.: Graphical models, exponential families,
andvariationalinference.FoundationsandTrends®inMachineLearning1(1–2),
1–305 (2008)
49. Weidner, R., Krummenacher, J., Reimann, B., Mu¨ller, H.J., Fink, G.R.: Sources
of top–down control in visual search. Journal of Cognitive Neuroscience 21(11),
2100–2113 (2009)
50. Willmore,B.D.,Mazer,J.A.,Gallant,J.L.:Sparsecodinginstriateandextrastriate
visual cortex. Journal of neurophysiology 105(6), 2907–2919 (2011)
51. Zheng, Q., Zhang, A., Grover, A.: Online decision transformer. arXiv preprint
arXiv:2202.05607 (2022)
14 Kiefer, Millidge, Tschantz, et al
Appendix A: Convergence of CapsNet with and without
routing
The following plots show the loss per epoch (plotted on a log scale for visibil-
ity) during training of a CapsNet architecture with 3 and 0 rounds of dynamic
routing-by-agreement and without an auxiliary reconstruction net. The figure
showsthatafterinitiallyslowerlearning,CapsNetwithoutroutingconvergedto
nearly the same test set loss as with routing.
Fig.2. Left: Training set loss during training of CapsNet under standard routing (3
iterations)andno-routingconditions.Middle:Testsetloss.Right:Comparisonoffinal
lossesacrossstandard,no-routing,no-reconstruction,andno-routingno-reconstruction
conditions. Note that the loss functions differ between the reconstruction and no-
reconstruction conditions.
Interestingly, although classification performance was very similar across
these networks, the test set accuracy for the four conditions (standard, no rout-
ing, routing without reconstruction loss, and neither routing nor reconstruction
loss)were99.23%,99.34%,99.32%,and99.29%respectively.Inthiscaseatleast,
dynamicroutingappearsnottohaveledtoimprovedaccuracy,althoughitdoes
lead to slightly lower values of the loss function both when using the full loss
(margin + reconstruction) and when using margin loss alone.
This is consistent with the findings in [31] that iterative routing does not
greatly improve the performance of capsules networks and can even lead to
worseperformance,thoughitisalsoconsistentwithSabouretal[39],whoreport
a roughly 0.14% performance improvement using routing against a no-routing
baseline.
Appendix B: Toy Model of Routing as Sparse Iterative
Inference
Capsulenetworksassumethatasparseparsetreerepresentationofastimulusis
preferred and achieve this using the routing algorithm while an equivalent ANN
would typically produce dense representations. To gain intuition for why sparse
iterativeinferencemaybeabletoachievethisresultaswellascapsulerouting,we
Capsule Networks as Generative Models 15
provide a simple illustrative example of how iterative inference with a sparsity
penalty can result in routing-like behaviour. We consider a simple three-layer
neural network with a single hidden layer and visible input and output layers.
We fix both the top and bottom layers to an input or a target respectively. We
can then infer the hidden layer activities which can both sufficiently ‘explain’
the output given the input. If we imagine the input layer of the network as
representing features and the output as a classification label, then in the hidden
layer we wish to uniquely assign the input features all to the best matching
‘object’.Weconstructsuchanetworkwithinputsize3,hiddensize3,andoutput
size 1, with input weights set to identity and the output weights set to a matrix
of all 1s. The network is linear although this is not necessary. Following the
Gaussian generative model proposed for the capsule network, we implemented
this network as a predictive coding network (PCN) and performed iterative
inferencebyupdatingactivitiestominimizethevariationalfreeenergywhichcan
beexpressedasasumofsquaredpredictionerrorsateachlayer[6].Inadditional
to the standard iterative free energy, we also experimented with adding either
a sparsity penalty (L1 regularisation) or L2 activity norm regularization to the
network.Weinvestigatedtheextenttowhichiterativeinferencewiththesparsity
penalty can reproduce the desired routing effect with only a single high-level
feature being active, and found that it can (see Figure 3).
Fig.3. A: Default behaviour of the outcome of iterative inference purely minimizing
squared prediction errors. Probability mass is distributed between all three potential
‘objects’ in the hidden layer. B: Outcome of sparse iterative inference using an L1
penaltytermintheobjectivefunction.Allprobabilitymassisconcentratedonasingle
‘best’ object so that a singly connected scene parse tree is constructed. C: Outcome
of iterative inference with an L2 penalty term in the loss function which encourages
probability mass to spread out. All objects have approximately equal probability of
being selected.
Moreover, this sparsity penalty was necessary in this network in order for
inference to exhibit routing-like behaviour. Without any regularization, itera-
tive inference has a tendency to distribute probability mass between various
high-level objects. This tendency is exacerbated with L2 regularisation which
encourages the inference to spread probability mass as evenly as possible.
Interestingly, a similar intuition is applied in [18] where routing is explicitly
derivedaspartofanEMalgorithmwithaclearprobabilisticinterpretationand
16 Kiefer, Millidge, Tschantz, et al
where the MDL penalties derived for simply activating a capsule, which do not
depend on its degree of activation, can perhaps also be thought of as effectively
implementing a similar sparsity penalty which encourages the EM algorithm
to assign capsule outputs to a single high-level capsule instead of spreading
probability mass between them.
Appendix C: Iterative inference process for CapsNet
Ourimplementationofsparseiterativeinferenceinplaceofcapsuleroutingisthe
sameinoutlineasthatusedforthetoymodeldiscussedinAppendixB,applied
totheCapsNetarchitecture.Thatis,weminimizethesumofsquaredprediction
errorsperlayer,whichisequivalenttothevariationalfreeenergy[27].Inthiscase
the prediction error for the output layer is given by the difference between the
prediction from the penultimate (PrimaryCaps) layer and the clamped target
values. For the sparsity condition, we also add the capsule-level L1 sparsity
penaltydiscussedinthecaptionoffigure1attheoutputlayer.Dynamicrouting
was turned off for this experiment, both at inference time and during training.
Appendix D: Relationship between Capsule Routing and
Attention
As noted above, our generative model of the capsule network can also describe
the self-attention block in transformers, providing a fundamental building block
towards building a full transformer generative model. Explicitly writing down
such a generative model for the transformer architecture could enable a signifi-
cantly greater understanding of the core mechanisms underlying the success of
transformersatmodellinglarge-scalesequencedataaswellaspotentiallysuggest
various improvements to current architectures.
This relationship to transformer attention is important because transformer
attentioniswell-understoodandfoundtobehighlyeffectiveinnaturallanguage
processing tasks [5,34] as well as recently in vision [11,32] and reinforcement
learning [8,37,51]. Since capsule networks appear highly effective at processing
natural scene statistics, this provides yet another example of the convergence
of machine learning architectures towards a universal basis of attention mecha-
nisms.
The basis of attention mechanisms can then be further understood in terms
ofassociativememoryarchitecturesbasedonModernHopfieldnetworks[21,35],
as briefly discussed above. It has been found that sparsity of similarity scores
is necessary for effective associative memory performance to prevent retrieved
memories from interfering with each other [20,22,26]. The softmax operation in
self-attention can be interpreted as a separation function with the goal of spar-
sifying the similarity scores by exponentially boosting the highest score above
the others. Indeed, it is a general result that the capacity of associative mem-
ory models can be increased dramatically by using highly sparsifying separation
Capsule Networks as Generative Models 17
functions such as high-order polynomials [10,22], softmaxes [35] and top-k acti-
vation functions [4].
AninterestingaspectofourgenerativemodelistheuseofVMFdistributions
torepresentthedot-productsimilarityscores.Intuitively,thisarisesbecausethe
cosine similarity is ‘circular’ in that angles near 360 degrees are very similar to
angles near 0. In most transformer and associative memory models, the update
rules are derived from Gaussian assumptions which do not handle the wrap-
around correctly and hence may be subtly incorrect for angles near the wrap-
around point. By deriving update rules directly from our generative model, it is
possible to obtain updates which handle this correctly and which may therefore
perform better in practice. A second potential improvement relates to the VMF
variance parameter κ. In transformer networks this is typically treated as a
constantandsetto √1 .Inessence,thisbakesintheassumptionthatthevariance
d
of the distribution is inversely proportional to the data dimension. Future work
could also investigate dynamically learning values of κ from data which could
also improve performance.
One feature of routing-by-agreement not captured by iterative inference in
standard PCNs is the positive feedback loop, in which low prediction error en-
couragesevencloseragreementbetweenactivitiesandpredictions.Thisissimilar
toapplyingself-attentionovertime.Akeydistinctionbetweenattentionasused
in transformers and the routing mechanism in capsule networks is that the lat-
ter is iterative and can be applied sequentially for many iterations (although
usually only 3-5), unlike in transformers where it is applied only once. Capsule
networks therefore could provide ideas for improving transformer models by en-
abling them to work iteratively and adding the recurrent state that arises from
the ‘bias’ term in the routing algorithm.
It has been proposed that highly deep networks with residual connections,
a set of architectures that includes transformers, are implicitly approximating
iterativeinferenceusingdepthinsteadoftime[14,19]whichisahighlyinefficient
use of parameters. Instead, it is possible that similar performance may be ob-
tained with substantially smaller models which can explicitly perform iterative
inference similar to capsule networks. Some evidence for this conjecture comes
from the fact that empirically it appears that large language models such as
GPT2[34]appeartoperformmostoftheirdecisionsastotheiroutputtokensin
their first few layers. These decisions are then simply refined over the remaining
layers – a classic use-case for iterative inference.
The link between capsule routing and sparse iterative inference also has sig-
nificant resonances in neuroscience. It is known that cortical connectivity and
activations are both highly sparse (approximately only 1-5% neurons active si-
multaneously)[7,13,50]withevenhigherlevelsofsparsityexistinginotherbrain
regions such as the cerebellum [9,40,41]. Such a level of sparsity is highly en-
ergy efficient [43] and may provide an important inductive bias for the efficient
parsing and representation of many input signals which are generated by highly
sparse processes – i.e. dense pixel input is usually only generated by a relatively
small set of discrete objects. Secondly, iterative inference is a natural fit for the
18 Kiefer, Millidge, Tschantz, et al
ubiquitous recurrent projections that exist in cortex [23,25,44,46,49] and many
properties of visual object recognition in the brain can be explained through a
hybridmodelofarapidamortizedfeedforwardsweepfollowedbyrecurrentitera-
tiveinference[45].Theseconsiderationscombinetoprovideafairbitofevidence
towardsarouting-likesparseiterativeinferencealgorithmbeinganintegralpart
of cortical functioning. Moreover, it has been demonstrated many times in the
sparse-coding literature that adding sparse regularisation on a variety of recon-
structionandclassificationobjectivescanresultinnetworksdevelopingreceptive
fields and representations that resemble those found in the cortex [29,30,50].
Iterative inference is also important for enabling object discrimination and
disambiguation in highly cluttered and occluded scenes because it can model
the vital ‘explaining away’ [33] effect where inferences about one object can
then inform parallel inferences about other objects. This is necessary in the
case of occlusion since by identifying the occluder and implicitly subtracting
out its visual features, it is often possible to make a much better inference
about the occluded object [17]. It is therefore noteworthy, and suggestive of
our hypothesis that routing can really be interpreted as iterative inference, that
capsule networks perform much better at parsing such occluded scenes than
purely feedforward models such as CNNs.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Capsule Networks as Generative Models"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
