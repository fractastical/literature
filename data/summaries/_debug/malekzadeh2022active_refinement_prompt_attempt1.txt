=== IMPORTANT: ISOLATE THIS PAPER ===
You are revising a summary for ONLY the paper below. Do NOT reference or use content from any other papers.
Paper Title: Active Inference and Reinforcement Learning: A unified inference on continuous state and action spaces under partial observability
Citation Key: malekzadeh2022active
REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Issues to fix:
1. CRITICAL: The current summary has severe repetition issues. You MUST eliminate all repeated sentences, phrases, and paragraphs. Each idea should be expressed only once. If you find yourself repeating content, remove the duplicates entirely. Focus on variety and uniqueness in your wording.
2. Severe repetition detected: Same sentence appears 3 times (severe repetition)

Current draft:
The authors state: “Active Inference and Reinforcement Learning: A unified inference on continuous state and action spaces under partial observability”. This paper investigates a unified approach to reinforcement learning (RL) within partially observable Markov decision processes (POMDPs). The central argument is that active inference (AIF) provides a natural framework for solving these complex problems, leveraging reward-maximizing behavior alongside information-seeking exploration. The authors aim to overcome limitations of existing RL methods in partially observable environments by establishing a theoretical connection between AIF and RL, enabling seamless integration and addressing the challenges associated with large state spaces and long horizons.The authors state: “Active Inference and Reinforcement Learning: A unified inference on continuous state and action spaces under partial observability”. This paper investigates a unified approach to reinforcement learning (RL) within POMDPs. The central argument is that active inference (AIF) provides a robust and efficient method for solving complex problems, particularly when dealing with the challenges of modeling the environment. The paper details the development of a unified actor-critic algorithm that combines the strengths of model-based and model-free approaches to solve problems.The authors state: “Active Inference and Reinforcement Learning: A unified inference on continuous state and action spaces under partial observability”. The paper details the development of a unified actor-critic algorithm that combines the strengths of

Key terms: malekzadeh, continuous, action, state, spaces, problems, actions, reinforcement

=== FULL PAPER TEXT ===
1
Active Inference and Reinforcement Learning: A unified infer-
ence on continuous state and action spaces under partial observabil-
ity
Parvin Malekzadeh
p.malekzadeh@mail.utoronto.ca
Konstantinos N. Plataniotis
The Edward S. Rogers Sr. Department of Electrical and Computer Engineering, Uni-
versityofToronto,Toronto,ON,M5S3G8,Canada.
Keywords: Active inference; Expected free energy; Partially observability; Policy;
Reinforcementlearning
Abstract
Reinforcementlearning(RL)hasgarneredsignificantattentionfordevelopingdecision-
makingagentsthataimtomaximizerewards,specifiedbyanexternalsupervisor,within
fully observable environments. However, many real-world problems involve partial
or noisy observations, where agents cannot access complete and accurate informa-
tion about the environment. These problems are commonly formulated as partially
observable Markov decision processes (POMDPs). Previous studies have tackled RL
in POMDPs by either incorporating the memory of past actions and observations or by
inferring the true state of the environment from observed data. Nevertheless, aggre-
gating observations and actions over time becomes impractical in problems with large
decision-making time horizons and high-dimensional spaces. Furthermore, inference-
based RL approaches often require many environmental samples to perform well, as
they focus solely on reward maximization and neglect uncertainty in the inferred state.
Active inference (AIF) is a framework naturally formulated in POMDPs and directs
agents to select actions by minimizing a function called expected free energy (EFE).
Thissuppliesreward-maximizing(orexploitative)behaviour,asinRL,withinformation-
ThispreprinthasbeenacceptedbyNeuralComputation.
4202
yaM
13
]GL.sc[
3v64970.2122:viXra
seeking(orexploratory)behaviour. DespitethisexploratorybehaviourofAIF,itsusage
islimitedtoproblemswithsmalltimehorizonsanddiscretespaces,duetothecomputa-
tionalchallengesassociatedwithEFE.Inthispaper,weproposeaunifiedprinciplethat
establishesatheoreticalconnectionbetweenAIFandRL,enablingseamlessintegration
ofthesetwoapproachesandovercomingtheiraforementionedlimitationsincontinuous
spacePOMDPsettings. Wesubstantiateourfindingswithrigoroustheoreticalanalysis,
providing novel perspectives for utilizing AIF in designing and implementing artifi-
cial agents. Experimental results demonstrate the superior learning capabilities of our
method compared to other alternative RL approaches in solving partially observable
taskswithcontinuousspaces. Notably,ourapproachharnessesinformation-seekingex-
ploration, enabling it to effectively solve reward-free problems and rendering explicit
taskrewarddesignbyanexternalsupervisoroptional.
1 Introduction
Decision-making is the process of evaluating and selecting a course of action from
various alternatives based on specific criteria or goals. This process occurs within an
environment where an agent interacts and influences its state through actions. When
making decisions,1 the agent carefully assesses available actions and their potential
outcomes. Once a decision is made, the agent translates it into action by executing a
specific course of action. The effectiveness of a decision is determined by examining
its outcomes, which can involve achieving desired results or mitigating undesirable
consequences(Puterman,2014).
Reinforcement learning (RL) (Sutton & Barto, 2018) is a framework that models
agents to interact with an environment typically represented as a Markov decision pro-
cess (MDP), where the agent has complete and accurate observation of the true state
of the environment. In RL, outcomes are often associated with rewards designed by
an external supervisor and received by the agent based on its actions. The main objec-
tiveofRListolearnoptimalpoliciesthatdefineanagent’sdecision-makingstrategyto
maximizeavaluefunction,whichrepresentstheexpectedlong-termreward. Recentad-
vancements in using deep neural networks (DNNs) as parametric function approxima-
torsenabledRLalgorithmstosuccessfullysolvetaskswithcontinuousstateandaction
spaces (Haarnoja, Zhou, Abbeel, & Levine, 2018; Schulman, Wolski, Dhariwal, Rad-
ford,&Klimov,2017;Dai,Du,Fang,&Bharath,2022). AmongthepopularRLmeth-
ods designed for continuous state and action spaces, actor-critic methods (Haarnoja,
Zhou, Abbeel, & Levine, 2018; Mnih et al., 2016; Schulman et al., 2017) have gained
1Weusethetermsdecision-makingandactionselectioninterchangeablythroughoutthework.
significant attention. These methods employ a policy iteration algorithm to learn the
optimal policy. However, in many real-world scenarios, the true and complete state of
the environment is often inaccessible, leading to a situation known as learning under
uncertaintyorpartialobservability. Partialobservabilitycanarisefromvarioussources,
suchastemporaryinformationlikeawaypointsigninanavigationtask,sensorlimita-
tions,noise,andalimitedviewoftheenvironment. Consequently,theagentmustselect
optimalactionsbasedonincompleteornoisyinformationabouttheenvironment’strue
(hidden or latent) states. Such environments can be formulated as partially observ-
able Markov decision processes (POMDPs). Unfortunately, identifying the objectives
on which an optimal policy is based for POMDPs is generally undecidable (Madani,
Hanks, & Condon, 1999) because partial and noisy observations lack the necessary
informationfordecision-making.
RL algorithms typically assume complete and accurate observation of the environ-
ment’s states, which limits their performance on partially observable tasks. To address
this,variousapproacheshavebeenproposedtoextendRLmethodsforsolvingpartially
observable tasks. One popular approach is the use of memory-based methods, where
recurrentneuralnetworks(RNNs)areemployedtorememberpastobservationsandac-
tions (Zhu, Li, Poupart, & Miao, 2017; Nian, Irissappane, & Roijers, 2020; Haklidir
& Temeltas¸, 2021; Ni, Eysenbach, & Salakhutdinov, 2022). However, these methods
may become impractical when dealing with large dimensions of actions and/or obser-
vations, or when the decision-making time horizon is extensive, due to the demands of
maintaining long-term memory. Consequently, these approaches are mostly applicable
to environments with finite and countable action and observation spaces. Addition-
ally, training an RNN is more challenging than training a feed-forward neural network
since RNNs are relatively more sensitive to the hyperparameters and structure of the
network (Pascanu, Mikolov, & Bengio, 2013). To address these issues, some recent
work (Ramicic & Bonarini, 2021; Igl, Zintgraf, Le, Wood, & Whiteson, 2018; Lee,
Nagabandi, Abbeel, & Levine, 2020; Han, Doya, & Tani, 2020; Hafner, Lillicrap, Ba,
& Norouzi, 2020) has proposed inferring the belief state, which is a representation of
thehiddenstatesgiventhepastobservationsandactions. Fromtheinferredbeliefstate,
an optimal policy can be derived to maximize the expected long-term reward. How-
ever, due to the partial observability and limitations of the available observations, the
agent cannot accurately determine the exact hidden state with certainty. As a result,
the inferred belief state is often represented as a probability distribution, indicating the
likelihoodofdifferenthiddenstatesgivenpastobservationsandactions.
Giventheuncertaintysurroundingtheinferredstate,itiscrucialfortheagentnottorely
solely on its existing knowledge (exploitation). Instead, in a partially observable envi-
ronment, the agent should actively engage in an information-directed interaction with
the environment, aiming to minimize uncertainty and maximize information about the
truestateoftheenvironment,givenonlypartialornoisyobservationsofstates(Mavrin,
Yao, Kong, Wu, & Yu, 2019; Dong, Zhang, Liu, Zhang, & Shen, 2021; Likmeta,
Sacco, Metelli, & Restelli, 2022; Maddox, Izmailov, Garipov, Vetrov, & Wilson, 2019;
Malekzadeh, Salimibeni, Hou, Mohammadi, & Plataniotis, 2022; Yin, Chen, Pan, &
Tschiatschek,2021;Malekzadeh,Salimibeni,Mohammadi,Assa,&Plataniotis,2020).
Active inference (AIF) (K. Friston, FitzGerald, Rigoli, Schwartenbeck, & Pezzulo,
2017;K.J.Friston,Daunizeau,Kilner,&Kiebel,2010)isaframeworkderivedfromthe
freeenergyprinciplethatmodelsandexplainsthedecision-makingbehaviourofagents
in environments modelled as POMDPs. AIF optimizes two complementary objective
functions: variationalfreeenergy(VFE)andexpectedfreeenergy(EFE).TheVFEob-
jectiveisminimizedwithrespecttopastinteractionswiththeenvironment,enablingthe
agent to learn a generative model of the environment and infer the belief state used for
action selection. This process of learning the generative model and inferring the belief
state is called perceptual learning and inference. Action selection involves minimizing
theEFEobjectivewithrespecttothefuture,aimingtofindanoptimalplan—sequences
of actions.2 By minimizing the EFE, AIF simultaneously maximizes the agent’s infor-
mationgainaboutthehiddenstates(exploration)andoptimizingitsexpectedlong-term
reward (exploitation). The information gain represents the enhancement in the agent’s
knowledge of the environment, allowing it to make more informed decisions as it in-
teracts with the world. Notably, AIF agents select actions that maximize the expected
long-term reward, akin to classical RL, while maximizing information about the envi-
ronment’shiddenstates.
WhileAIFisnaturallymodelledinPOMDPsandprovidesintrinsicinformation-directed
exploration,itsapplicationsarelimitedtoenvironmentswithdiscretestate,observation,
andactionspacesduetocomputationalissueswiththeEFE(Millidge,2020;DaCosta,
Sajid,Parr,Friston,&Smith,2023;Lanillosetal.,2021;Sajid,Tigas,Zakharov,Foun-
tas, & Friston, 2021). This limitation arises because AIF finds the optimal plan by
computingtheEFEforeachpossibleplanandsubsequentlyselectingtheplanthatmin-
imizes the EFE (Tschantz, Baltieri, Seth, & Buckley, 2020). Recent studies have pro-
posed approaches to calculate the EFE in a more tractable manner. These approaches
include limiting the future decision-making time horizon (Tschantz et al., 2020), em-
ploying bootstrapping estimation techniques (Millidge, 2020; Hafner et al., 2022), and
utilizingMonteCarlotreesearch(MCTS)methods(Maisto,Gregoretti,Friston,&Pez-
2In this paper, we will use the term ‘plan’ to denote a sequence of actions, distinguishing it from a
(state-action)policytypicallydefinedinRL.
zulo, 2021; Fountas, Sajid, Mediano, & Friston, 2020). However, it’s important to note
that these methods are (partially) heuristic and typically applicable only to finite time
horizonsordiscreteactionspaces.
Considering the recent advancements in scaling RL for complex, fully observable
environments with continuous state and action spaces, and acknowledging the capa-
bility of AIF to perform belief state inference and information-seeking exploration in
partially observable environments, a compelling question emerges: Is there a relation-
ship between AIF and RL that enables the development of integrated solutions, lever-
aging ideas from both fields and surpassing the limitations of individual AIF or RL ap-
proaches? Inthispaper,wepropose”unifiedinference”—aconvergenceofAIFandRL
on a common theoretical ground—to harness the mutual advantages of both paradigms
and address the challenges posed by realistic POMDP settings with high-dimensional
continuousspaces.
The key contributions of our proposed unified inference framework are summarized as
follows:
• Extension of EFE to stochastic belief state-action policy: We extend the EFE,
originally defined for plans (K. Friston et al., 2017) in AIF, to accommodate the
learning of a stochastic policy in an infinite time horizon POMDP setting with
continuous state, action, and observation spaces. This extension of the EFE to a
belief state-action policy allows actions to be chosen at each time step based on
the inferred belief state, eliminating the need to enumerate every possible plan
into the future. This extension integrates information-seeking exploration and
reward-maximization under partial observability learning, making it a practical
and effective objective function for action selection in both AIF and RL frame-
works within POMDP settings with continuous spaces. We hence refer to this
extension as the unified objective function. Our experiments demonstrate that
introducingstochasticitytothepolicysignificantlyimprovesthestabilityandro-
bustnessofouralgorithmintaskswithcontinuousspaces,wherechallengessuch
asbeliefstateinferenceandhyperparametertuningposesignificantobstacles.
• Unified policy iteration in continuous space POMDPs: To optimize the pro-
posedunifiedobjectivefunctionandfindtheoptimalpolicy,weintroduceacom-
putationally efficient algorithm called unified policy iteration. This algorithm
generalizes the policy iteration guarantees in MDPs (Haarnoja, Zhou, Abbeel, &
Levine, 2018) to POMDPs and provides theoretical proof of its convergence to
the optimal policy. Within the unified policy iteration framework, the extended
EFE can be treated as a negative value function from an RL perspective. This
theoretical connection demonstrates three important aspects: (i) AIF can be an-
alyzed within the framework of RL algorithms, enabling insights from scaling
RLtoinfinitetimehorizonsandcontinuousspaceMDPstobedirectlyappliedto
scalingAIFforuseininfinitetimehorizonsandcontinuousspacePOMDPs. This
provides deep learning practitioners with a starting point to leverage RL findings
and further advance AIF in challenging tasks. (ii) We can generalize a range
of RL approaches designed for continuous state and action MDPs to continuous
state, action, and observation space POMDPs while incorporating an inherent
information-seeking exploratory term. This bridges the gap between MDPs and
POMDPs and also opens up new possibilities for applying state-of-the-art RL
techniquestochallengingdecision-makingproblems. (iii)Wecanextendarange
ofreward-dependentRLmethodstoasettingwheretherewardfunctionisnotde-
terminedbyanexternalsupervisor. Thisaspectoftheproposedpolicyiterationis
significant as it eliminates the challenges associated with designing reward func-
tions. For instance, a poorly designed reward function may lead to slow learning
or even convergence to sub-optimal policies, highlighting the importance of mit-
igatingthenecessityofdefiningtaskrewardsthroughunifiedpolicyiteration.
• Unified actor-critic for continuous space POMDPs: Building upon the para-
metricfunctionapproximationsintheunifiedpolicyiteration,wepresentanovel
unifiedactor-criticalgorithmthatunifiesactor-criticmethodsforbothMDPsand
POMDPs. Our approach stands out as one of the few computationally feasi-
ble methods for addressing POMDPs and opens new possibilities for enhancing
the performance of the popular reward-maximizing actor-critic RL algorithms,
such as soft actor-critic (SAC) (Haarnoja, Zhou, Abbeel, & Levine, 2018) and
Dreamer (Hafner et al., 2020) when applied to real-world scenarios that inher-
ently involve partial observability. To evaluate the effectiveness of our unified
actor-criticalgorithminaddressinghigh-dimensionalcontinuousspacePOMDPs,
weconductexperimentsonmodifiedversionsofRoboschooltasks(Brockmanet
al., 2016), where agents have access to partially observed and noisy states. The
experimental results demonstrate that our proposed unified actor-critic method
achievessuperiorsampleefficiencyandasymptoticperformancecomparedtoex-
istingframeworksintheliterature.
Acomparativeoverviewoffoundationalelementsanddecision-makingstrategieswithin
RL, AIF, and our proposed unified inference approach, with a particular focus on their
applicationincontinuousdecisionspaces,ispresentedinTable1. Thissummaryencap-
sulatesthemethodologiesemployedbytheseframeworks,highlightingtheirdistinctive
Table 1: Comparative overview of foundational and decision-making aspects in RL,
AIF,andunifiedinferenceundercontinuousdecisionspaces.
Aspect RL AIF UnifiedInference
Continuous Continuous Continuous
DecisionSpace
MDP POMDP POMDP
Foundationfor Perceptualinference Perceptualinference
N/A
Decision-Making &learningviaVFE &learningviaVFE
Decision-Making State-action Beliefstate-action
Planlearning
Strategy policylearning policylearning
Unifiedobjective
Valuefunction EFE(reward+ (reward+
Decision-Making
(reward informationgain informationgain+
Objective
maximization) maximization) policyentropy
maximization)
Optimization Unifiedpolicy
Policyiteration Enumeration
Mechanism iteration
Learning Bootstrapping,
Actor-critic Unifiedactor-critic
Approach MCTS
approachestolearninganddecision-makinginenvironmentscharacterizedbycontinu-
ousdecisionspaces.
1.1 Overview
In Section 2, we present a comprehensive introduction to the problem’s nature, which
serves as the foundation for our study. Section 3 offers an introductory tutorial on
POMDPsandcoversessentialconceptslikepolicies,generativemodels,andinference.
We review the RL paradigm for MDPs and the AIF paradigm for POMDPs in Sec-
tion 4. Section 5 introduces the proposed unified inference framework and establishes
its convergence to the optimal policy. In Section 6, we delve into the implementa-
tion and modeling aspects of the proposed unified inference, demonstrating how our
formulation extends various existing MDP-based actor-critic methods directly to their
respective POMDP cases. Section 7 discusses existing works related, and Section 8
presents the experiments utilized to evaluate the performance of our method. Finally,
weconcludethepaperandoutlinepotentialavenuesforfutureresearchinSection9.
2 Problem characteristics
Thissectionoutlinesourassumptionsconcerningvariouskeyelementsofthedecision-
makingproblemaddressedinthispaper.
1. Finite or infinite horizon: We assume agents interact with environments over
an infinite horizon, appropriate in scenarios where decisions have long-term im-
pacts. Infinite horizon problems, requiring analysis of endless action sequences,
areinherentlymorecomplexthantheirfinitecounterparts.
2. Fully or partially observable: We tackle sequential decision-making in partially
observable environments. Partial observability is prevalent in many real-world
tasks and challenges decision-making compared to fully observable settings (Igl
etal.,2018).
3. Discrete or continuous state, observation, and action spaces: We consider envi-
ronmentswithcontinuousstate,observation,andactionspaces. Thesecontinuous
spaces are common in complex applications, as they enhance the representation
oftheenvironmentandactions,andbetterhandlecomplexity.
4. Stochastic or deterministic environment: We consider stochastic environments
introducing randomness in action outcomes. Unlike deterministic environments,
stochastic environments require strategies that account for outcome variability,
enhancingdecisionrobustnessandcapturingreal-worldcomplexity.
5. Stationary or non-stationary environment: We assume stationary environments
with consistent statistical properties, a common simplification to manage the
computationalcomplexityofnon-stationaryenvironments(Sutton&Barto,2018;
Puterman, 2014). While focused on stationary settings, the methodologies pro-
posedinthispaperalsoapplytonon-stationaryenvironments.
6. Markovian or non-Markovian policy: We focus on Markovian policies that rely
solely on the current belief state, streamlining computation and memory com-
paredtonon-Markovian(history-dependent)policies.
7. Stochastic or deterministic policy: We adopt stochastic policies for their en-
hancedabilitytoexploreenvironmentsandhandleuncertainties,facilitatingadap-
tive responses. These features promote better generalization and task-specific
fine-tuning(Ramicic&Bonarini,2021;Haarnoja,Zhou,Abbeel,&Levine,2018).
Table 2 provides a comprehensive comparison between the key characteristics of
our problem and the problems addressed by most RL and AIF paradigms. This com-
parisoncoversaspectssuchastheobservabilityofenvironmentstates,decision-making
horizon, stochastic properties of the environment, and policy class. By analyzing these
elements in comparison to typical RL and AIF problems, we gain insights into the dis-
tinctnatureanduniquechallengesofourproblemsetting.
Table 2: Comparison of our problem setting with common problem settings addressed
byRLandAIFalgorithms.
State&
Action Environ-
Method Horizon Observ- observation Policy
space ment
ability space
Stochastic
Finite/ Discrete/ Discrete/ Markovian&
RL Full &
Infinite Continuous Continuous Stochastic
Stationary
Stochastic Non-
AIF Finite Partial Discrete Discrete & Markovian&
Stationary Deterministic
Stochastic
Markovian&
Ours Infinite Partial Continuous Continuous &
Stochastic
Stationary
3 Preliminaries and Problem modeling
Intheprevioussection,weoutlinedthecharacteristicsoftheproblemaddressedinthis
paper, involving an agent making decisions within a stochastic environment with con-
tinuous state, observation, and action spaces. This section delves into modeling these
characteristics, beginning with an overview of relevant concepts including POMDPs,
policies, generative models, and belief state inference, and then detailing the specific
modelsappliedtoourproblem.
We assume the agent receives observations and makes decisions at discrete time steps
t, continuing indefinitely, where t ∈ {0,1,2,...}. For notation, x represents vari-
t
able x at time step t, and x includes all elements from t = f to t = h, i.e.,
f:h
x = (x ,x ,...,x ). Additionally, ∆(X) denotes the set of all probability dis-
f:h f f+1 h
tributions3 over set X, and |X| represents its cardinality. For further clarification, a
comprehensivelistofallnotationsusedisprovidedinTableD.1intheappendix.
3.1 Partially observed Markov decision processes (POMDPs)
A POMDP is fully specified by M = (S,O,A,d ,Ω,R,U,Θ), where S ∈ RDS rep-
0
resents the state space encompassing all possible (hidden or latent) states of the envi-
ronment. Here, R denotes the set of real numbers, and D is the dimension of the state
S
space. O ∈ RDO denotes the observation space, the set of all possible observations
the agent can receive, with D as the dimension of the observation space. A ∈ RDA
O
3Weusetheprobabilitydistributionfunctionforboththeprobabilitymassfunction(PMF)andprob-
abilitydensityfunction(PDF).
is the action space, indicating all possible actions the agent can take, where D is the
A
dimension of action space. These spaces can be either discrete or continuous. At time
stept,s ,o ,anda denoteelementsfromS,O,andA,respectively.
t t t
d : S → ∆(S)denotestheprobabilitydistributionoftheinitiallatentstate,withd (s )
0 0 0
specifying the probability of the environment starting in state s ∈ S. Ω : S × A →
0
∆(S) is the (forward) transition function, such that Ω(s |s ,a ) specifies the proba-
t+1 t t
bilityoftransitioningtos ∈ S froms afteractinga .
t+1 t t
Risthesetofpossiblerewardsandiscalledtherewardspace. Wewilluser todenote
t
anelementoftherewardsetRattimet. AdheringtoRLandAIFstandards(K.Friston
et al., 2017; Sutton & Barto, 2018), we assume that R ∈ R, although more general
vector-valued reward functions are also possible. U : S × A → ∆(R) is the reward
function,suchthatU(r |s ,a )indicatestheprobabilityofarewardvaluer givenstate
t t t t
s and action a . The reward function U is also known as the external (or extrinsic)
t t
rewardfunctionsinceitsvalueisdeterminedbyanexternalsupervisorordesigner.4
Θ : S → ∆(O) is the observation function, such that Θ(o |s ) specifies the probabil-
t t
ity of observing o given state s . This distinguishes POMDPs from MDPs, where the
t t
agent indirectly observes s through o . In MDPs, observations directly reflect the true
t t
state, making o = s . A POMDP is deemed deterministic if Ω, U, and Θ are all de-
t t
terministic. Otherwise, it is considered stochastic. Furthermore, the POMDP is termed
stationaryifthesefunctionsremainconstantovertimeandnon-stationaryiftheyvary.
Followingassumptions2-5inSection2,werepresentourenvironmentasastation-
arystochasticPOMDPwithcontinuousstate,observation,andactionspaces.
3.2 Policy
Apolicyisasequenceofdecisionrulesdenotedbyπ = (π ,π ,π ,...,π ,...). Each
0:∞ 0 1 2 t
decision rule π models how an agent selects action a ∈ A at time step t. To identify
t t
anoptimalpolicy,aspecificoptimalitycriterionisdefined.
AMarkoviandecisionruleπ (a |o )operatessolelyonthecurrentobservationo ,whereas
t t t t
a history-dependent decision rule π (a |a ,o ) utilize the complete history of ac-
t t 0:t−1 0:t
tions a and observations o until time t. Decision rules can be deterministic,
0:t−1 0:t−1
selecting actions with certainty, or stochastic, introducing probability into the selection
process(Puterman,2014).
ThedistinctionsbetweenMarkovian,history-dependent,deterministic,andstochas-
ticdecisionrulesgiverisetocorrespondingclassesofpolicies.
4Inthiswork,weinterchangethetermsrewardfunctionandextrinsicrewardfunctionwhenreferring
toU.
Moreover, a policy π is considered stationary if the decision rule π remains con-
0:∞ t
stant for all time instants t ∈ {0,1,2,...}. Otherwise, it is called non-stationary. In
the case of a stationary POMDP with an infinite time horizon, it is common to assume
a stationary policy (Puterman, 2014; Russell, 2010). This assumption arises from the
understanding that at each time step, an infinite number of future time steps exist, and
the statistical properties of the environment remain unchanged. Adopting a stationary
policycansimplifytheagent’sdecision-makingprocess.
In an MDP where o = s and states exhibit the Markovian property, all policies
t t
are Markovian (Puterman, 2014). A Markovian policy within an MDP selects action
a based solely on the current state, denoted as π(a |s ), and is thus referred to as a
t t t
state-action policy. Conversely, in POMDPs where observations lack the Markovian
property,history-dependentpoliciesmayoutperformMarkovianonesbyenablingmore
informed decisions (Igl et al., 2018; Montufar, Ghazi-Zahedi, & Ay, 2015; Puterman,
2014). However, each time step in POMDPs introduces one new action and one new
observation to the history, adding D + D -dimensional data to the existing history.
O A
Furthermore, the addition of one observation and one action to the history at each time
step leads to an exponential increase in the number of possible histories. Specifically,
thenumberofpossiblehistoriesexpandsateachstepbyafactorof(|Ω|×|A|),reflect-
ing the agent’s |A| possible actions and |Ω| possible observations. Consequently, the
size of the policy space, which represents the number of possible policies derived from
thesehistories,alsoundergoesthesameexponentialgrowth.
Totacklethesechallenges,certainmethods(Igletal.,2018;Kochenderfer,2015;Mont-
ufaretal.,2015)employbeliefstate-actionpolicies,wheredecisionsattimetarebased
on a belief state b ∈ B, i.e., π (a |b ). Here, the belief state b represents a probability
t t t t t
distribution over potential states of the environment at time t, and B ∈ R|S| encom-
passes all possible belief states. Derived using Bayes’ rule through a process known as
inference,thebeliefstateconsolidatesallpertinenthistoricalinformationnecessaryfor
actionselection,effectivelyrenderingbeliefstate-actionpoliciesMarkovian.
Unlike history-dependent policies, belief states in belief state-action policies main-
tain a constant size, |S|, at each time step. Consequently, the memory and com-
putation required to store and evaluate each belief state and its corresponding policy
doesnotinherentlyincreaseovertime,providingsignificantefficiencyadvantagesover
history-dependent approaches, particularly in environments with infinite horizons or
high-dimensional action and observation spaces. However, inferring and storing belief
statescanbecomputationallyandmemory-intensive,particularlyinlargeorcontinuous
state spaces. Nevertheless, this memory and computational requirement generally falls
below that of history-dependent policies (Yang & Nguyen, 2021). Further details com-
paring the complexity of history-based versus belief state-action policies are provided
inAppendixA.
3.3 Generative model
With policy π and initial state distribution d , interactions between an agent and a sta-
0
tionary POMDP M gives the following sequence (s ,o ,a ,s ,o ,a ,...) with the
0 0 0 1 1 1
probabilitydistribution:
∞
(cid:89)
p(s ,o ,a ,s ,o ,a ,...|π)5 = p(s ,o ,s ,o ,...|a ) p(a |π), (1)
0 0 0 1 1 1 0 0 1 1 0:∞ k−1
k=1
where p(a |π) denotes the probability that policy π chooses action a . Given Θ
k−1 k−1
andΩ,p(s ,o ,s ,o ,...|π)inEq.(1)simplifiesto:
0 0 1 1
∞
(cid:89)
p(s ,o ,s ,o ,...|a ) = d (s )p(o |s ) p(o |s )p(s |s ,a )
0 0 1 1 0:∞ 0 0 0 0 k k k k−1 k−1
k=1
∞
(cid:89)
= d (s )Θ(o |s ) Θ(o |s )Ω(s |s ,a ). (2)
0 0 0 0 k k k k−1 k−1
k=1
AsexplainedinSub-section3.2,thebeliefstateisavariableestimatedbytheagent.
To infer the belief state using Bayesian inference (as detailed in Sub-section 3.4), the
agent relies on p(s ,o |a ) = p(s ,o ,s ,o ,...|a ). However, the agent
0:∞ 0:∞ 0:∞ 0 0 1 1 0:∞
lacks direct access to Ω and Θ; thus, it constructs a generative model of the POMDP
denoted as P(s ,o |a ) = P(s ,o ,s ,o ,...|a ), which is decomposed as
0:∞ 0:∞ 0:∞ 0 0 1 1 0:∞
follows(Hanetal.,2020;K.Fristonetal.,2017;Leeetal.,2020): 6
∞
(cid:89)
P(s ,o |a ) = d (s )P(o |s ) P(o |s )P(s |s ,a ), (3)
0:∞ 0:∞ 0:∞ 0 0 0 0 k k k k−1 k−1
k=1
where P(o |s ) and P(s |s ,a ) are the agent’s models of observation function Θ
t t t t−1 t−1
andthetransitionfunctionΩ,respectively. Thesearereferredtoasthelikelihoodfunc-
tionortheobservationmodel,andthetransitionmodel,respectively.
ThegenerativemodelofanMDPisderivedbysettingP(o |s ) = 1inthePOMDP’s
t t
model. Fig.1showsthedependencyrelationshipsinthegenerativemodelsforPOMDPs
5Weabusenotationbywritingp(.)forboththePMFandPDF.Moreover,weusep(x )asshorthand
t
forp(X =x ),whereX isarandomvariableattimettakingonvaluesx ∈X.
t t t t
6WeuseP torepresenttheagent’sprobabilisticmodelofthePOMDP,whichisalearnedapproxima-
tion,whilepdenotesthetrueprobabilisticcomponentsofthePOMDP.Sincetheagentisunawareofthe
trueprobabilisticcomponents,wesolelyrefertotheagent’sgenerativemodelfromnowon.
Fig. 1. Relationship between generative models of MDPs and POMDPs. Arrows indi-
catedependence.
andMDPs,illustratinghowaPOMDP’smodelencompassesthatofanMDP.
3.4 Inference
Thebeliefstateb utilizedbyaMarkovianbeliefstate-actionpolicyπ(a |b )signifiesa
t t t
probabilitydistributionoverthestatespaceS attimet,whereb (s ) = P(s |o ,a ).
t t t 0:t 0:t−1
Through a generative model for the environment, the agent estimates b , a process
t
knownasinference(Igletal.,2018). InferenceisperformedusingBayes’rule:
(cid:82)
b (s )P(o |s )P(s |s ,a )ds
t−1 t−1 t t t t−1 t−1 t−1
b = . (4)
t (cid:82) (cid:82)
b (s )P(o |s )P(s |s ,a )ds ds
t−1 t−1 t t t t−1 t−1 t t−1
(cid:124) (cid:123)(cid:122) (cid:125)
BeliefUpdate(bt−1,st,ot,at−1)
Eq.(4)presentsBayes’ruleforinferenceinPOMDPswithcontinuousstateandobser-
vationspaces. Fordiscretespaces,summationswouldbeusedinstead. Giventhebelief
state b , an action a , an observation o , and the latent state s , the belief state b is
t−1 t−1 t t t
fullydeterminedviatherecurrentupdatefunctionBeliefUpdate. Consequently,thebe-
lief state forward distribution P(b |s ,o ,b ,a ), representing the distribution over
t t t t−1 t−1
the subsequent belief state, is δ(b −BeliefUpdate(b ,s ,o ,a )), where δ denotes
t t−1 t t t−1
the Dirac delta function. Therefore, the belief state b depends solely on the previous
t
beliefstateb ,exhibitingtheMarkovianproperty.
t−1
Inpractice,exactinferenceoftenbecomescomputationallyintractableincontinuous
spaces due to the integrals involved. To address this, we use variational inference, a
common approximation technique in the RL and AIF literature (Hafner et al., 2020;
Lee et al., 2020; Millidge, 2020; Mazzaglia, Verbelen, & Dhoedt, 2021). Details on
variationalinferenceareprovidedinSub-section4.2.1.
4 Review of the State-of-The-Art algorithms
RLandAIFaretheprimaryframeworksfordecision-makingproblems. RLalgorithms
typically assume full observability and model environments as MDPs, whereas AIF
assumes partial observability and models them as POMDPs. This section overviews
theirfundamentalconceptsandunderscorestheirkeydistinctions.
Note 4.1: In the rest of this paper, we fix the current time t and assume that the
agentfocusesonoptimizingfutureactionsfortimestepsτ ∈ {t,t+1,...}.
4.1 Reinforcement learning (RL)
AsRLalgorithmsareformulatedwithinMDPs,itissufficienttoconsideraMarkovian
state-action policy π(a |s ) (Sutton & Barto, 2018). The goal of RL is to find an op-
τ τ
timal state-action policy π∗ maximizing the expected long-term reward Jπ, expressed
asJπ = E (cid:81)∞ τ=t P(sτ|sτ−1,aτ−1)P(rτ|sτ,aτ)π(aτ|sτ) [ (cid:80)∞ τ=t γτ−tr τ ],whereγ ∈ [0,1)isthedis-
count factor used to ensure the sum is bounded. RL employs various methods to find
theoptimalpolicyπ∗. Threecommoncategoriesrelevanttoourcontextareasfollows:
• Policy-based approaches: These algorithms directly find π∗ by maximizing Jπ
using gradient ascent methods. Policy-based approaches have shown success
in dealing with continuous state and action spaces (Y. Ma, Zhao, Hatano, &
Sugiyama, 2016; Schulman, Levine, Abbeel, Jordan, & Moritz, 2015), but they
oftensufferfromhighvarianceingradientestimates(Tuckeretal.,2018).
• Value-based methods: Value-based algorithms rely on learning either the state
value function V(π)(s ) or the state-action value function Q(π)(s ,a ), which are
t t t
obtained by conditioning Jπ on the state S = s and the state-action pair (S =
t t t
s ,A = a ),respectively:
t t t
(cid:20) ∞ (cid:21)
(cid:88)
V(π)(s t ) = E (cid:81)∞
τ=t
P(sτ+1|sτ,aτ)P(rτ|sτ,aτ)π(aτ|sτ) γτ−tr τ |s t , (5)
τ=t
(cid:20) ∞ (cid:21)
(cid:88)
Q(π)(s t ,a t ) = E (cid:81)∞
τ=t
P(sτ+1|sτ,aτ)P(rτ|sτ,aτ)π(aτ|sτ) γτ−tr τ |s t ,a t , (6)
τ=t
Value-basedalgorithmscalculateV(π)(s )andQ(π)(s ,a )recursivelyusingBell-
t t t
manequations:
V(π)(s ) = E (cid:2) r +γE [Vπ(s )] (cid:3) , (7)
t π(at|st)P(rt|st,at) t P(st+1|st,at) t+1
Qπ(s ,a ) = E [r ]+E [Qπ(s ,a )]. (8)
t t P(rt|st,at) t P(st+1|st,at)π(at+1|st+1) t+1 t+1
Using the Bellman equations, RL iteratively updates these value functions using
the Bellman operators (Sutton & Barto, 2018). The Bellman operator for state-
actionvaluefunctionisdenotedasTRL andisdefinedas:
π
TRLQ(s ,a ):=E [r ]+E [Q(s ,a )]. (9)
π t t P(rt|st,at) t P(st+1|st,at)π(at+1|st+1) t+1 t+1
Theoptimalpolicyisthenfoundπ∗ = argmax V(π)(s ) = argmax Q(π)(s ,a ).
π t π t t
It has been shown that the following Bellman optimality equations on the opti-
malstatevaluefunctionV∗(s ) := V(π∗)(s )andtheoptimalstate-actionvalue
t+1 t
functionQ∗(s ,a ) := Q(π∗)(s ,a )hold(Sutton&Barto,2018):
t t t t
(cid:26) (cid:27)
V∗(s ) = max E [r ]+γE [V∗(s )] , (10)
t
a∈A
P(rt|st,a) t P(st+1|st,a) t+1
Q∗(s ,a ) = E [r ]+γE [maxQ∗(s ,a′)]. (11)
t t P(rt|st,a) t P(st+1|st,at)
a′∈A
t+1
Thus,value-basedalgorithmscancalculatetheoptimalstatevaluefunctionV∗(s )
t
or state-action value function Q∗(s ,a ), and derive the optimal action a∗ as
t t
a∗ = argmax Q∗(s ,a) or a∗ = argmax V∗(s ). However, when dealing with
a t a t
large action spaces, using the max operator to select the best action can become
computationallyexpensive(Okuyama,Gonsalves,&Upadhay,2018).
• Actor-critic methods: These hybrid approaches blend policy-based and value-
based techniques. The policy, or actor, selects actions, while the critic estimates
the state or state-action value function, evaluating the actor’s decisions. Using
policy gradients, the actor improves its policy, while the critic assesses this en-
hanced policy by estimating its corresponding value function. This iterative pro-
cess, known as policy iteration, guarantees convergence to the optimal policy
under certain conditions with sufficient iterations. Additionally, policy iteration
iscomputationallyefficientsinceitupdatesthepolicyateachiteration,makingit
suitableforproblemswithcontinuousstateandactionspaces.
RL algorithms, including value-based, policy-based, and actor-critic methods, can be
either model-free or model-based, depending on how they learn the optimal policy.
Model-based algorithms learn a generative model of the MDP and a reward model, of-
fering improved sample efficiency (Hafner et al., 2020). Conversely, model-free meth-
ods, while not relying on explicit models, often require more environment interactions
foroptimization. Recentapproaches,likeCVRL(X.Ma,Chen,Hsu,&Lee,2021),in-
tegratemodel-freeandmodel-basedcomponentstoleveragetheirrespectivestrengths.
Despite RL’s advancements in continuous spaces, it primarily operates in fully ob-
servable environments. As such, its applicability in partially observable scenarios re-
mainslimited(DaCostaetal.,2023;Hanetal.,2020;Igletal.,2018).
4.2 Active inference (AIF)
AIF has gained attention as a framework unifying inference and action selection under
the free energy principle in POMDPs (K. Friston et al., 2017; K. Friston, Mattout, &
Kilner, 2011). In AIF, the agent engages in inferring and learning a generative model
of the POMDP and then utilizes the inferred belief state and generative model to seek
anoptimalplan,asequenceoffutureactions,thatminimizestheEFE.
4.2.1 Perceptualinferenceandlearning
AIF assumes that the agent has access to past observations and actions and models
the instant generative model as P(s ,o |a ,s ) (K. Friston et al., 2017; Millidge,
t t t−1 t−1
Tschantz,&Buckley,2021). Uponreceivingtheobservationo ,AIFlearnsP(s ,o |a ,s )
t t t t−1 t−1
andutilizesvariationalinferencetoapproximatethebeliefstateb usingthevariational
t
posterior q(s |o ).7 This is achieved by minimizing the VFE at time t, denoted as F .
t t t
The VFE is an upper bound on the current Bayesian surprise, defined as −logp(o ),
t
where log represents the natural logarithm function. In machine learning, the VFE
corresponds to the negative Evidence Lower Bound (ELBO) within the framework of
VariationalAutoEncoder(VAE)(Kingma&Welling,2013). F isdefinedasfollows:
t
F = E [logq(s |o )−logP(s ,o |a ,s )]. (12)
t q(st|ot) t t t t t−1 t−1
ByfactoringP(s ,o |a ,s )intoP(o |s)andP(s |s ,a ),F canberewrittenas
t t t−1 t−1 t t t t−1 t−1 t
F = −E [logp(o |s )]+D [q(s |o ),P(s |s ,a )], (13)
t q(st|ot) t t KL t t t t−1 t−1
where D is Kullback Leibler (KL)-divergence. By minimizing F , the agent ensures
KL t
that its generative model aligns with the received observations and the inferred states.
The process of inferring q(s |o ) by minimizing F is called perceptual inference, and
t t t
minimizing F with respect to the transition model P(s |s ,a ) and the likelihood
t t t−1 t−1
functionP(o |s )isknownasperceptuallearninginAIF(K.Fristonetal.,2017).
t t
7Throughoutthispaper,thenotationqisusedtorepresentvariationalprobabilitydistributions.
4.2.2 Planselection
˜
AIF assumes that the agent has a desired distribution P(o ) over future observations
τ+1
o for τ ∈ {t,t+1,...,T}, where T represents a finite time horizon. This desired dis-
τ
tribution,knownasthepriorpreference,encodestheagent’sgoals.
Looking ahead to future time steps, the agent in AIF aims to minimize its expected
˜
future Bayesian surprise based on its prior preference P(o ). This is achieved by
τ+1
minimizing the EFE given the current observation o over all possible plans a˜ :=
t
a (Millidge et al., 2021). The EFE is denoted as G (o ) and is defined as fol-
t:T−1 AIF t
lows:
(cid:20) (cid:21)
q(s ,a˜|s )
G (o ) = E log t+1:T t , (14)
AIF t q(st:T,ot+1:T,a˜|ot)
P(s ,o |s ,a˜)
t+1:T t+1:T t
where the variational distribution q(s ,a˜) infers future states and actions and the
t+1:T
variational distribution q(s ,o ,a˜|o ) considers expectations for future observa-
t:T t+1:T t
tions. AIF factors q(s ,a˜|s ) as q(s ,a˜|s ) =
(cid:81)T−1q(a˜)P(s
|s ,a ) and
t+1:T t t+1:T t τ=t τ+1 τ τ
approximatesthegenerativemodelP(s ,o |s ,a˜)inEq.(14)withabiasedgen-
t+1:T t+1:T t
˜
erativemodelP(s ,o |s ,a˜),definedasfollows:
t+1:T t+1:T t
Definition4.1 (Biasedgenerativemodel)
˜
Given the prior preference P(o ), the generative model P(s ,o |s ,a˜) in
τ+1 t+1:T t+1:T t
˜
Eq. (14) is approximated by a biased generative model P(s ,o |s ,a˜), which
t+1:T t+1:T t
factorsas:
T−1
(cid:89)
˜ ˜
P(s ,o |s ,a˜) = P(o |s ,a )q(s |o ). (15)
t+1:T t+1:T t τ+1 τ τ τ+1 τ+1
τ=t
ByreplacingthegenerativemodelP(s ,o |s ,a˜)withthebiasedgenerative
t+1:T t+1:T t
˜
modelP(s ,o |s ,a˜),G (o )inEq.(14)canberewrittenas:
t+1:T t+1:T t AIF t
(cid:20)T−1 (cid:21)
(cid:88) q(a˜)P(s |s ,a )
G (o ) = E log τ+1 τ τ . (16)
AIF t q(st:T,ot+1:T,a˜|ot)
P
˜
(o |s ,a )q(s |o )
τ=t τ+1 τ τ τ+1 τ+1
Focusing on deriving the optimal variational distribution q∗(a˜) (Millidge et al., 2021;
Mazzaglia, Verbelen, C¸atal, & Dhoedt, 2022) that minimizes G (o ), it has been
AIF t
(cid:16) (cid:17)
shown that q∗(a˜) = σ −G(a˜)(o ) , where σ is the Softmax function and G(a˜t)(o )
AIF t AIF t
istheEFEforafixedplana˜,definedas:
(cid:20)T−1 (cid:21)
(cid:88) P(s |s ,a )
G(a˜)(o )=E log τ+1 τ τ −logP ˜ (o |s ,a ) . (17)
AIF t q(st:T,ot+1:T|ot,a˜) q(s |o ) τ+1 τ τ
τ+1 τ+1
τ=t
(cid:16) (cid:17)
As q∗(a˜) = σ −G(a˜)(o ) , the most probable plan, denoted as a˜∗, corresponds to the
AIF t
one that minimizes G(a˜)(o ). Thus, a˜∗ is referred to as the optimal plan and can be
AIF t
found as a˜∗ = argmin G(a˜)(o ). AIF achieves this optimal plan by running its biased
a˜ AIF t
generative model forward from the current time t to the time horizon T, generating
hypothetical future states and observations for all possible plans a˜. It then selects the
plana˜∗ thatresultsintheminimumvalueofG(a˜)(o ).
AIF t
Note 4.2: It should be noted that AIF (K. Friston et al., 2017) chooses its actions
according to the optimal plan a˜∗. This is in contrast to the optimal state-action policy
π∗ inRL,whichmapsstatestoprobabilitydistributionsovertheactionspace.
According to the complete class theorem in AIF (K. Friston, Samothrakis, & Mon-
tague, 2012), E [r ] can be encoded as logP ˜ (o |s ,a ). Therefore, G(a˜)(o )
P(rτ|sτ,aτ) τ τ+1 τ τ AIF t
inEq.(17)canberewrittenas
(cid:34) (cid:35)
T−1
(cid:88)
G(a˜)(o ) = −E r
AIF t (cid:81)T
τ=
−
t
1q(sτ|oτ)P˜(oτ+1|sτ,aτ)P(rτ|sτ,aτ) τ
τ=t
(cid:124) (cid:123)(cid:122) (cid:125)
Expectedlong-termreward
(cid:34) (cid:35)
T−1
(cid:88)
−E D [q(.|o ),P(.|s ,a ) . (18)
(cid:81)T
τ=
−
t
1q(sτ|oτ)P˜(oτ+1|sτ,aτ) KL τ+1 τ τ
τ=t
(cid:124) (cid:123)(cid:122) (cid:125)
Expectedinformationgain
ThefirstterminEq.(18)representstheexpectedlong-term(extrinsic)reward,whichis
known as the goal-directed term in AIF (Millidge et al., 2021). The second term, KL-
divergence between the variation posterior q(.|o ) and the transition model P(.|s ,a ),
τ τ τ
represents the expected information gain, referred to as the epistemic (intrinsic) value,
which measures the amount of information acquired by visiting a particular state. By
following the optimal plan a˜∗ that minimizes G(a˜)(o ), the agent indeed maximizes the
AIF t
expected long-term extrinsic reward (exploitation) while reducing its uncertainty and
maximizingtheexpectedinformationgain(exploration)aboutthehiddenstates.
In this section, we have explored the strengths and weaknesses of RL and AIF al-
gorithms. RL excels in fully observable problems with high-dimensional continuous
spaces, utilizing policy iteration and function approximations like DNNs. However, it
struggles with partial observability and relies heavily on well-defined extrinsic reward
signals. Asaresult,RLcanfacedifficultieswhentheextrinsicrewardfunctionisabsent
or provides sparse or zero rewards from the environment. In contrast, AIF addresses
partial observability by integrating belief state inference and information-seeking ex-
ploratorybehavior,whichisoftenlackingintraditionalRLalgorithms(Haarnoja,Zhou,
Abbeel,&Levine,2018;Okuyamaetal.,2018). TheinformationgaintermallowsAIF
agents to learn even without explicitly defined extrinsic reward functions, as any plan
inherently possesses intrinsic value. This aspect of AIF is particularly valuable when
designing appropriate reward functions is challenging. However, the computational
burden of considering all possible plans limits many AIF methods to discrete spaces or
finitehorizonPOMDPs. CombiningRLandAIFcanleveragetheirrespectivestrengths,
offeringmoreeffectivedecision-makinginPOMDPs. Thefollowingsectionintroduces
aunifiedframeworkthatmergesRLandAIF,addressingpartiallyobservableproblems
withinfinitetimehorizonsandcontinuousspaces.
5 Unified inference integrating AIF and RL in continu-
ous space POMDPs
In this section, we present a unified inference framework that combines the strengths
ofAIFandRL.Thisframeworkenablesanagenttolearnanoptimalbeliefstate-action
policy in an infinite time horizon POMDP with continuous spaces, while promoting
exploration through information-seeking. The proposed unified inference approach in-
troduces a unified objective function that formulates the action selection problem and
establishes the optimality criteria for the policy. This unified objective function com-
bines the objective functions of both the AIF and RL frameworks within the context
of a POMDP with continuous state and action spaces. Next, we show that the unified
objective function obeys the so-called unified Bellman equation and the unified Bell-
man optimality equation. These equations generalize the standard Bellman equation
and Bellman optimality equation from MDPs (i.e., Eqs. (7), (8), (10), and (11)) to the
broader class of POMDPs. By utilizing these unified Bellman equations, we derive a
unifiedpolicyiterationframeworkthatiterativelyoptimizestheproposedunifiedobjec-
tivefunction.
The proposed unified policy iteration serves two main purposes. Firstly, it extends the
guaranteesofpolicyiterationfromMDPs(Puterman,2014)toPOMDPs,allowingusto
apply insights from MDP-based RL algorithms, such as actor-critic algorithms, in the
contextofPOMDP-basedAIF.ThisextensionenablesthegeneralizationoftheseMDP-
based algorithms to POMDPs and facilitates computationally feasible action selection
throughAIFinproblemswithcontinuousspacesandaninfinitetimehorizon. Secondly,
this approach enables us to leverage recent breakthroughs in RL methods that rely on
extrinsicrewardsandapplythemtotaskswithoutpre-specifiedrewardfunctions. Byin-
corporatingthiscapability,wecanaddressabroaderrangeofproblemdomainsbeyond
thoselimitedtoexplicitreward-basedlearning.
Prior to detailing the unified objective function, we must outline assumptions re-
garding our decision-making model. To establish the foundational formulation of the
performancecriterioninaPOMDPwithcontinuousstate,action,andobservationspaces,
itisnecessarytoconsiderspecificregularityassumptionsoutlinedbyPuterman(2014),
knownasthePOMDPregularity.
Assumption5.1 (POMDPregularity)
(A1) ThestatespaceS isacompactsetinRDS.
(A2) TheactionspaceAisacompactsetinRDA.
(A3) TheobservationspaceO isacompactsetinRDO.
(A4) TheextrinsicrewardspaceRisacompactsetinR
(A5) The transition function Ω, observation function Θ, and reward function U are
Lipschitzcontinuous.
AppendixCprovidesdetailedexplanationsofeachregularityassumption. Thesecondi-
tions enable us to focus on deriving an objective function as an optimality criterion for
anoptimalpolicy. However,asdiscussedinSub-section3.2,tomitigatethehighmem-
ory requirement associated with history-dependent policies, we adopt a belief state-
action policy π(a |b ), requiring the agent to infer the belief state b before selecting an
t t t
action at time step t. In line with common practices in RL and AIF approaches applied
to POMDPs (von Helmholtz, 2001; Hafner et al., 2020; Millidge et al., 2021; Han et
al.,2020),weassumethattheagentperformsperceptualinferenceandlearningpriorto
actionselectionattimesteptasfollows:
Assumption5.2 (Perceptualinferenceandlearning)
Prior to action selection at time step t, given the current observation o , the agent per-
t
forms inference by approximating its belief state b with a variational posterior distri-
t
bution. Thisperceptualinferenceisperformedconcurrentlywiththeperpetuallearning
of the generative model through the minimization of the VFE (ELBO in VAEs (Kingma
&Welling,2013)).
Appendix E.1 offers further details on the learning process of variational inference and
thegenerativemodel.
5.1 Problem formulation: Unified objective function for AIF and
RL in POMDPs
Given the regularity assumption in Assumption 5.1 and the Perceptual Inference and
Learning assumption in Assumption 5.2, our attention now turns to defining a perfor-
mancecriterion,orobjectivefunction,forlearningtheoptimalbeliefstate-actionpolicy.
In POMDPs, where agents lack complete knowledge of true environmental states,
devisingaperformancecriterionpresentsachallenge(Nianetal.,2020;Krishnamurthy,
2015;Chatterjee,Chmelik,&Tracol,2016). TheRLliteratureconcerningPOMDPsof-
tenfocusesonmaximizingtheexpectedlong-termextrinsicreward(Haklidir&Temeltas¸,
2021; Montufar et al., 2015) as the optimality criterion. The EFE in AIF, as expressed
inEq.(18),inherentlybalancesinformation-seekingexplorationandrewardmaximiza-
tion in POMDPs. Hence, if we can extend the EFE to a stochastic belief state-action
policyπ andreducethecomputationalrequirementsforlearningtheoptimalpolicy,the
extended EFE can serve as a suitable objective function for our infinite time horizon
POMDPwithcontinuousstate,action,andobservationspaces.
By extending the EFE to a stochastic belief state-action policy π, action a is sampled
t
from policy π(a |b )8 instead of being selected from a predetermined plan a˜. The main
t t
idea is to learn a mapping from belief states to actions without explicitly searching
through all possible plans. Thus, our goal becomes finding an optimal stochastic belief
state-actionpolicyπ∗ thatminimizesG(π) (b ),whichrepresentstheEFEcorrespond-
Unified t
ing to π(a |b ), i.e., π∗ = argmin G(π) (b ). Learning the optimal belief state-action
t t π Unified t
policy is computationally more efficient than finding the optimal plan in continuous
spaces, facilitating quicker decision-making. Moreover, by selecting actions individu-
ally from the policy at each time step instead of a pre-determined plan, the agent can
updateitspolicyiterativelybasedonnewobservationsovertime,allowingittoadaptto
environmentaldynamics,avoidrepeatingerrors,andgeneralizebettertonewscenarios.
Theorem5.3statestheextendedEFEforthebeliefstate-actionpolicyπ.
Theorem5.3 Let π be a stochastic belief state-action policy selecting action a ac-
τ
cordingtoπ(a |b )forτ = {t,t+1,t+2,...}inaPOMDPsatisfyingAssumption5.1.
τ τ
G(π) (b ),theEFEcorrespondingtothepolicyπ,canbeachievedas
Unified t
∞
G(π) (b ) = E
(cid:2)(cid:88)
logπ(a |b )
Unified t (cid:81)∞
τ=t
P(bτ+1|oτ+1,bτ,aτ)P˜(oτ+1|aτ,bτ)π(aτ|bτ)q(sτ+1|oτ+1,bτ) τ τ
τ=t
8In the context of our proposed unified inference framework, when we refer to the term ”policy”
denotedasπ,wemeanaMarkovianbeliefstate-actionpolicy.
P(s |a ,b )
˜ τ+1 τ τ (cid:3)
− logP(o |a ,b )+log . (19)
τ+1 τ τ
q(s |o ,b )
τ+1 τ+1 τ
Proof. SeeAppendixD.1. (cid:50)
The expression P(b |s ,o ,b ,a ) in Eq. (19) represents the agent’s model of
τ+1 τ+1 τ+1 τ τ
the belief state forward distribution, as defined in Sub-section 3.4. P(s |a ,b ) rep-
τ+1 τ τ
resents the belief state-conditioned transition model b . Furthermore, q(s |o ,b )
τ τ+1 τ+1 τ
˜
and P(o |b ,a ) denotes the variational posterior q(s |o ) and the prior prefer-
τ+1 τ τ τ+1 τ+1
ence P ˜ (o |s ,a ) in AIF conditioned on the belief state b .9 Thus, Eq. (19) com-
τ+1 τ τ τ
putes the EFE, where the agent possesses a biased generative model over the sequence
˜
(s ,...,b ,s ,o ,b ,...)givenb . Thissequenceinvolveso ∼ P(o |a ,b ),
t+1 τ τ+1 τ+1 τ+1 t τ+1 τ+1 τ τ
s ∼ q(s |o ,b ), and b ∼ P(b |s ,o ,b ,a ). We can regard this bi-
τ+1 τ+1 τ+1 τ τ+1 τ+1 τ+1 τ+1 τ τ
asedgenerativemodeloverthissequenceasanextensionofthebiasedgenerativemodel
ofAIFdefinedinEq.(15)toencompassastochasticbeliefstate-actionpolicyπ. Hence,
˜
we denote this biased generative model as P(s ,o ,b |b ,a ) and refer
t+1:∞ t+1:∞ t+1:∞ t t:∞
toitasthebiasedbeliefstategenerativemodel,whichisformallydefinedasfollows:
Definition5.1 (Biasedbeliefstategenerativemodel)
˜
GiventhepriorpreferenceP(o |b ,a )forτ = {t,t+1,...},thebiasedbeliefstate
τ+1 τ τ
˜
generativemodelisP(s ,o ,b |b ,a ),whichfactorizesas:
t+1:∞ t+1:∞ t+1:∞ t t:∞
∞
(cid:89)
˜ ˜
P(s ,o ,b |b ,a ) = P(s ,o ,b |b ,a )
t+1:∞ t+1:∞ t+1:∞ t t:∞ τ+1 τ+1 τ+1 τ τ
τ=t
∞
(cid:89)
˜
= P(o |b ,a )q(s |o ,b )P(b |s ,o ,b ,a ). (20)
τ+1 τ τ τ+1 τ+1 τ τ+1 τ+1 τ+1 τ τ
τ=t
ThebiasedbeliefstategenerativemodelapproximatesP(s ,o ,b |b ,a ),
t+1:∞ t+1:∞ t+1:∞ t t:∞
which extends the generative model of a POMDP defined in Eq. (3) to account for a
stochasticbeliefstate-actionpolicy. Hence,werefertoP(s ,o ,b |b ,a )
t+1:∞ t+1:∞ t+1:∞ t t:∞
asthebeliefstategenerativemodel,whichcanbefactorizedas:
∞
(cid:89)
P(s ,o ,b |b ,a ) = P(s ,o ,b |b ,a )
t+1:∞ t+1:∞ t+1:∞ t t:∞ τ+1 τ+1 τ+1 τ τ
τ=t
∞
(cid:89)
= P(s |a ,b )P(o |s ,b )P(b |s ,o ,b ,a ), (21)
τ+1 τ τ τ+1 τ+1 τ τ+1 τ+1 τ+1 τ τ
τ=t
whereP(o |s ,b )representsthebeliefstate-conditionedlikelihoodmodel.
τ+1 τ+1 τ
9Asthebeliefstateb encapsulatestheagent’sknowledgeabouttheunderlyingstates basedonall
τ τ
availableobservationsandactions,thedirectconditioningons canbeomittedwhenconditioningonb
τ τ
Toaccommodatethereward-maximizingRLobjective,followingthecompleteclass
˜
theoreminAIF(K.Fristonetal.,2012),weemploythereparameterizationlogP(o |a ,b ) =
τ+1 τ τ
E [r ],whereP(r |b ,a )representsthebeliefstate-conditionedrewardmodel.
P(rτ|bτ,aτ) τ τ τ τ
Additionally,toensurethatthevalueofG(π) (b )inEq.(19)remainsbounded,weuse
Unified t
the discount factor γ ∈ [0,1), following the convention in RL literature. We denote the
modifiedversionofG(π) (b )asG(π)(b ),whichcanbeexpressedasfollows:
Unified t t
(cid:20) ∞ (cid:18)
(cid:88)
G(π)(b )= E γτ−t logπ(a |b )
t (cid:81)∞
τ=t
P(bτ+1|sτ+1,oτ+1,bτ,aτ)P˜(oτ+1|aτ,bτ)π(aτ|bτ)q(sτ+1|oτ+1,bτ) τ τ
τ=t
(cid:19)(cid:21)
P(s |a ,b )
˜ τ+1 τ τ
−logP(o |b ,a )+log (22)
τ+1 τ τ
q(s |o ,b )
τ+1 τ+1 τ
∞
=E (cid:81)∞
τ=t
P(oτ+1,bτ+1,sτ+1|bτ,aτ) (cid:2)(cid:88) −γτ−tH(π(.|b τ )) (cid:3)
τ=t
(cid:124) (cid:123)(cid:122) (cid:125)
Expectedentropy
∞
−E
(cid:81)∞
τ=t
π(aτ|bτ)P(sτ+1,oτ+1,bτ+1|bτ,aτ)P(rτ|bτ,aτ)
(cid:2)(cid:88) γτ−tr
τ
(cid:3)
τ=t
(cid:124) (cid:123)(cid:122) (cid:125)
Expectedlong-termextrinsicreward
∞
−E (cid:81)∞
τ=t
π(aτ|bτ)P(oτ+1,bτ+1|bτ,aτ) (cid:2)(cid:88) γτ−tD KL [q(.|o τ+1 ,b τ ),P(.|a τ ,b τ )] (cid:3) , (23)
τ=t
(cid:124) (cid:123)(cid:122) (cid:125)
Expectedinformationgain
where H(π(.|b )) is the Shannon entropy of π(.|b ) and is calculated as H(π(.|b )) =
τ τ τ
−E [logπ(a |b )]. This entropy term provides a bonus for random exploration,
π(aτ|bτ) τ τ
encouraging the agent to distribute the probability across all possible actions as evenly
as possible. This helps prevent the phenomenon of policy collapse (Millidge, 2020),
where the policy quickly converges to a degenerate distribution (i.e., a deterministic
policy). The second term in Eq. (23) corresponds to the agent’s objective of maximiz-
ing the expected long-term extrinsic reward, emphasizing exploitation. Including this
termencouragestheagenttofocusonimmediaterewardsandexploititscurrentknowl-
edge. The third term represents the expected information gain. By incorporating this
term into the objective function, the agent is motivated to actively seek out informa-
tion to reduce uncertainty about the hidden state of the environment. By combining
both reward maximization and information-seeking, G(π)(b ) serves as a unified objec-
t
tive function for both RL and AIF in continuous space POMDPs. The problem is thus
formulatedasfindingtheoptimalbeliefstate-actionpolicyπ∗ thatminimizesG(π)(b ).
t
Since G(π)(b ) is a composite objective function that comprises multiple terms, in-
t
cludingtheexpectedlong-termextrinsicreward,theentropyofthepolicy,andtheinfor-
mationgain,weintroducethescalingfactors0 ≤ α < ∞,0 ≤ β < ∞,and0 ≤ ζ < ∞
in G(π)(b ) to adjust the relative weights of these components in deriving the optimal
t
policy. This flexibility allows us to prioritize specific objectives according to the task
and desired behaviour. By incorporating these scaling factors, we rewrite G(π)(b ) in
t
Eq.(22)asfollows:
∞
G(π)(b t ) = E (cid:81)∞
τ=t
π(aτ|bτ)P(sτ+1,oτ+1,bτ+1|bτ,aτ)P(rτ|bτ,aτ) (cid:2)(cid:88) γτ−t (cid:0) βlogπ(a τ |b τ )
τ=t
P(s |b ,a )
τ+1 τ τ (cid:3)
− αr +ζlog . (24)
τ
q(s |o ,b )
τ+1 τ+1 τ
Note 5.1: Throughout this paper, when we refer to G(π)(b ), we specifically mean
t
the scaled version expressed in Eq. (24). Furthermore, as G(π)(b ) represents the EFE
t
as a function of a given belief state b , we refer to G(π)(b ) as the belief state EFE to
t t
distinguishitfromtheEFEG(a˜)(o )definedinAIF(Eq.(17)).
t
GiventhethebeliefstateEFEG(π)(b ),theproblemcannowbeformulatedas
t
π∗ ∈ argminG(π)(b ). (25)
t
π
The belief state EFE corresponding to the optimal policy π∗ is referred to as the op-
timal belief state EFE and is denoted as G∗(b ). In other words, we have G∗(b ) :=
t t
G(π∗)(b ) = min G(π)(b ). It should be noted that π∗ ∈ argmin G(π)(b ) and not
t π t π t
π∗ = argmin G(π)(b ) because the optimization problem in Eq. (25) may admit more
π t
thanoneoptimalpolicy.
Considering that the action space A in our problem is continuous, the space of possi-
blepoliciesπ representingprobabilitydistributionfunctionsoverthecontinuousaction
space A is vast and continuous. Therefore, solving the optimization problem stated in
Eq. (25) is a challenging and non-trivial task. In the next sub-section, we will demon-
stratethatG(π)(b )exhibitsarecursiverelationship,whichoffersapathwayforsolving
t
theoptimizationprobleminEq.(25).
5.2 Unified Bellman equation
In this sub-section, we introduce a recursive solution for the problem in Eq. (25) in
the spirit of the classical Bellman approach in MDPs (Bellman, 1952). This recursion
allows us to solve the optimization problem presented in Eq. (25) by breaking it down
intosmallersub-problems.
We begin by demonstrating that the belief state EFE G(π)(b ), defined in Eq. (24), fol-
t
lows a Bellman-like recursion in the context of a POMDP. Since this recursion is de-
fined for the belief state EFE G(π)(b ), which serves as a unified objective function for
t
both RL and AIF in the POMDP setting, we refer to this Bellman-like recursion as the
unifiedBellmanequation.
Proposition5.4 (UnifiedBellmanequationforG(π)(b ))
t
The belief state EFE G(π)(b ) defined in Eq. (24) for a POMDP satisfying Assump-
t
tion 5.1 can be computed recursively starting from the belief state b and following
t
policyπ,asfollows:
(cid:20)
G(π)(b ) = E βlogπ(a |b )−αr (26)
t π(at|bt)P(rt|bt,at) t t t
(cid:21)
P(s |b ,a )
+ E (cid:2) ζlog t+1 t t +γG(π)(b ) (cid:3) .
P(bt+1,ot+1,st+1|bt,at)
q(s |o ,b )
t+1
t+1 t+1 t
Proof. SeeAppendixD.2. (cid:50)
The unified Bellman equation establishes a recursive relationship between the belief
state EFE at a given belief state b and the belief state EFE at its successor belief state
t
b . This recursive relationship allows us to calculate the belief state EFE G(π)(b )
t+1 t
at time instant t by considering the belief state EFE for the remaining time horizon,
startingfromthenextbeliefstateb .
t+1
To demonstrate how the unified Bellman equation in Proposition 5.4 facilitates the
optimization problem in Eq. (25), we present the following theorem, which proves that
theoptimalbeliefstateEFEG∗(b )canbecomputedrecursivelyusingtheoptimalbelief
t
state EFE at the successor belief state b , i.e., G∗(b ). We refer to this recursive
t+1 t+1
computationoftheoptimalbeliefstateEFEastheunifiedBellmanoptimalityequation.
Theorem5.5 (UnifiedBellmanoptimalityequationforG∗(b ))
t
TheoptimalbeliefstateEFEG∗(b ) = min G(π)(b )inaPOMDPsatisfyingAssump-
t π t
tion5.1canbecalculatedrecursivelystartingfromthebeliefstateb andfollowingthe
t
optimalpolicyπ∗ asfollows:
(cid:20)
G∗(b ) = min E E (cid:2) βlogπ(a |b )−αr (27)
t π(at|bt) p(rt|bt,at) t t t
π(at|bt)
(cid:21)
P(s |b ,a )
+ E [ζlog t+1 t t +γG∗(b )] (cid:3) .
P(bt+1,ot+1,st+1|bt,at)
q(s |o ,b )
t+1
t+1 t+1 t
Proof. SeeAppendixD.3. (cid:50)
Using the unified Bellman optimality equation, the following corollary demonstrates
how finding the optimal instant policy π∗(a |b ) ∈ argmin G∗(b ) for selecting
t t π(at|bt) t
the instant action a based on the belief state b , involves finding an optimal policy
t t
π˜∗ ∈ argmin G(π˜)(b ) for the remaining actions, starting from the belief state b
π˜ t+1 t+1
resultingfromthefirstactiona .
t
Corollary5.6 The instant optimal policy π∗(a |b ) ∈ argmin G∗(b ) for choos-
t t π(at|bt) t
inginstantactiona canbeachievedintermsoftheoptimalpolicyπ˜∗ ∈ argmin G(π˜)(b )
t π˜ t+1
withregardtothenextbeliefstateb resultingfroma :
t+1 t
(cid:20)
π∗(a |b ) ∈ arg min E E [βlogπ(a |b )−αr ]
t t π(at|bt) P(rt|bt,at) t t t
π(at|bt)
(cid:21)
P(s |b ,a )
+ E (cid:2) ζlog t+1 t t +γminG(π˜)(b ) (cid:3) . (28)
P(bt+1,ot+1,st+1|st,at)
q(s t+1 |o t+1 ,b t ) π˜
t+1
Proof. SeeAppendixD.4. (cid:50)
Eq. (28) demonstrates how the optimization problem π∗ ∈ argmin G∗(b ) in Eq. (25)
π t
with an infinite time horizon, can be decomposed into two sub-problems with sepa-
rate time horizons. The first sub-problem has a time horizon equal to 1 and involves
finding an instant optimal policy π∗(a |b ) based on the belief state b as π∗(a |b ) ∈
t t t t t
argmin G∗(b ). The second sub-problem determines the optimal policy for the
π(at|bt) t
remaining time horizon from t + 1 onwards based on the next belief state b as
t+1
π˜∗ ∈ argmin G(π˜)(b ). In other words, the optimal policy π∗ ∈ argmin G(π)(b )
π˜ t+1 π t
can be decomposed as π∗ = (π∗(.|b ),π˜∗), and it can be constructed by recursively
t
combining the policies of the sub-problems with one-time horizons over time, namely
π∗(a |b ) for τ = {t,t+1,...}. The crucial improvement achieved from Corollary 5.6
τ τ
over current AIF methods is that choosing the action a from π∗(a |b ) is now per-
t t t
formed based on subsequent counterfactual action a from π∗(a |b ), as opposed
t+1 t+1 t+1
to considering all future courses of actions. This approach allows for more efficient
decision-making by focusing on the immediate consequences of the selected action,
ratherthanexploringallpossiblefuturetrajectories.
5.3 Existence of a unique optimal policy
In Theorem 5.5, we have shown that the optimal belief state EFE, G∗(b ), follows the
t
unified Bellman optimality recursion. This recursion allows for the recursive combina-
tion and computation of π∗(a |b ) for τ = {t,t+1,...}, enabling the derivation of an
τ τ
optimal policy π∗ ∈ argmin G(π)(b ). In this sub-section, we aim to demonstrate the
π t
existence of a unique optimal policy π∗(a |b ) that satisfies Eq. (28), as well as provide
t t
amethodforcomputingit.
Theorem5.7 (Existenceofauniqueoptimalpolicy)
Minimizingtheright-handsideoftheunifiedBellmanoptimalityequation(i.e.,Eq.(27))
with respect to π(a |b ) in a POMDP satisfying Assumption 5.1 leads to the following
t t
uniqueminimizer:
π∗(a |b )=arg min G∗(b )
t t t
π(at|bt)
 (cid:104) (cid:105)
E [αr −E ζlog P(st+1|bt,at) +γG∗(b )]
=σ
P(rt|bt,at) t P(bt+1,ot+1,st+1|bt,at) q(st+1|ot+1,bt) t+1
.(29)
β
Proof. SeeAppendixD.5. (cid:50)
Theorem 5.7 showed that π∗(a |b ) (the unique minimizer of Eq. (27)) is a Softmax
t t
functionofthefollowingterm:
(cid:20) (cid:21)
P(s |b ,a )
E αr −E (cid:2) ζlog t+1 t t +γG∗(b ) (cid:3) , (30)
P(rt|bt,at) t P(bt+1,ot+1,st+1|bt,at)
q(s |o ,b )
t+1
t+1 t+1 t
which is a function of a given belief state-action pair (b ,a ). To gain intuition about
t t
thisexpression,werewriteitasfollows
(cid:20) (cid:21)
P(s |b ,a )
E αr −E (cid:2) ζlog t+1 t t +γG∗(b ) (cid:3)
P(rt|bt,at) t P(bt+1,ot+1,st+1|bt,at)
q(s |o ,b )
t+1
t+1 t+1 t
(cid:20) (cid:21)
P(s |b ,a )
=−E −αr +E (cid:2) ζlog t+1 t t +γG∗(b ) (cid:3) .(31)
P(rt|bt,at) t P(bt+1,ot+1,st+1|bt,at)
q(s |o ,b )
t+1
t+1 t+1 t
(cid:124) (cid:123)(cid:122) (cid:125)
G∗(bt,at)
Wecannowsimplifyandrewriteπ∗(a |b )inEq.(29)intermsofG∗(b ,a )asfollows:
t t t t
 E (cid:104) αr −E (cid:2) ζlog P(st+1|bt,at) +γG∗(b ) (cid:3) (cid:105)
π∗(a t |b t )=σ
P(rt|bt,at) t P(bt+1,ot+1,st+1|bt,at) q(st+1|ot+1,bt) t+1

β
(cid:18) −G∗(b ,a ) (cid:19)
t t
=σ (32)
β
(cid:16) (cid:17)
exp
−G∗(bt,at)
β
= , (33)
Z∗(b )
t
(cid:16) (cid:17)
whereexp(.)istheexponentialfunction,andZ∗(b ) = (cid:82) exp −G∗(bt,a′) da′. Wecan
t A β
control the stochasticity of π∗(a |b ) by adjusting the value of β. As β → 0, π∗(a |b )
t t t t
converges to a deterministic policy, while as β → ∞, π∗(a |b ) approaches a uniform
t t
distributionovertheactionspaceA.
(cid:16) (cid:17)
However, computing Z∗(b ) = (cid:82) exp −G∗(bt,a′) da′ in the denominator of the Soft-
t A β
max function in Eq. (33) is computationally challenging or intractable. This is because
it involves the integration of the complex function G∗(b ,a ) over the continuous ac-
t t
tion space A. To address this issue, common practices in handling intractable integrals
overcontinuousspaces(Wright,2006)involvemethodslikediscretizationoftheaction
spaceintoafinitenumberofpointsorregions,oremployingnumericalintegrationtech-
niques such as Monte Carlo integration, numerical quadrature, or adaptive integration
algorithms. These methods aim to approximate the integral over the continuous space
numerically. However, discretization and numerical integration methods can introduce
errorsandleadtoalossofinformation. Problemreformulation(Boyd&Vandenberghe,
2004), on the other hand, allows us to maintain the inherent continuity of the origi-
nal problem, enabling a more accurate representation of the underlying optimization
problem. By reformulating the problem and imposing constraints, we can optimize the
¯
policywithinasetofstochasticbeliefstate-actionpoliciesΠthataretractableovercon-
tinuousactionspaces. Theseconstraintslimitthesearchspaceofpoliciestoasubsetof
¯
feasible solutions Π, ensuring that the optimization problem remains computationally
tractable. Consequently, using the problem formulation technique, our objective func-
tion in Eq. (25) is transformed into finding an optimal stochastic policy π¯∗ ∈ Π ¯ that
¯
minimizesthebeliefstateEFEcorrespondingtotheconstrainedpolicyπ¯ ∈ Π,denoted
asG(π¯)(b ):
t
π¯∗ ∈ argminG(π¯)(b ). (34)
t
π¯∈Π¯
¯
ThechoiceofthesetΠdependsonthespecificproblemanditsconstraints. Itisdriven
by factors such as the complexity of the problem, the desired level of policy expres-
siveness, and computational tractability. In some cases, a more complex policy may
be necessary to capture the nuances and intricacies of the problem at hand. This could
involve using flexible parametric distributions or even non-parametric representations
for the policies. These complex policies allow for more expressive modelling of the
belief state-actionpolicies, accommodatinga wide rangeof possible behaviors. On the
other hand, in certain scenarios, a simpler distribution may be sufficient to effectively
modelthebeliefstate-actionpolicies. Thiscouldinvolveusingparametricdistributions
with fewer parameters or selecting a specific functional form that is well-suited to the
problem’s characteristics. These simpler policies offer computational advantages, as
theytypicallyrequirefewercomputationalresourcesandcanbeeasiertooptimize.
Since the unified Bellman equation and unified Bellman optimality equation pre-
sentedinEqs.(26)and(27)holdforthebeliefstateEFEunderanyunconstrainedpolicy
π, these equations also hold for the belief state EFE corresponding to the constrained
policy π¯, i.e., G(π¯)(b ), and the belief state EFE corresponding to the constrained opti-
t
malpolicyπ¯∗ ∈ Π ¯ ,denotedasG(π¯∗)(b ) = min G(π¯)(b ). Therefore,wecanexpress
t π¯∈Π¯ t
the unified Bellman equation and the unified Bellman optimality equation for G(π¯)(b )
t
andG(π¯∗)(b )as
t
(cid:20)
G(π¯)(b ) = E βlogπ¯(a |b )−αr (35)
t π¯(at|bt)P(rt|bt,at) t t t
(cid:21)
P(s |b ,a )
+ E (cid:2) ζlog t+1 t t +γG(π¯)(b ) (cid:3) .
P(bt+1,ot+1,st+1|bt,at)
q(s |o ,b )
t+1
t+1 t+1 t
and
(cid:20)
G(π¯∗)(b ) = min E E (cid:2) βlogπ¯(a |b )−αr (36)
t
π¯(at|bt)∈Π¯
π¯(at|bt) P(rt|bt,at) t t t
(cid:21)
P(s |b ,a )
+ E [ζlog t+1 t t +γG(π¯∗)(b )] (cid:3) .
P(bt+1,ot+1,st+1|bt,at)
q(s |o ,b )
t+1
t+1 t+1 t
UsingtheunifiedBellmanoptimalityequationinEq.(36),theinstantconstrainedopti-
malpolicyπ¯∗(a |b ) ∈ argmin G(π¯∗)(b )canbeachievedas
t t π¯(at|bt)∈Π¯ t
π¯∗(a |b ) ∈ arg min G(π¯∗)(b ) (37)
t t t
π¯(at|bt)∈Π¯
(cid:20)
= arg min E E [βlogπ¯(a |b )−αr ]
π¯(at|bt)∈Π¯
π¯(at|bt) P(rt|bt,at) t t t
(cid:21)
P(s |b ,a )
+ E (cid:2) ζlog t+1 t t +γG(π¯∗)(b ) (cid:3) . (38)
P(bt+1,ot+1,st+1|bt,at)
q(s |o ,b )
t+1
t+1 t+1 t
Solving the constrained minimization problem in Eq. (37) is non-trivial as it may not
have a closed-form solution for π¯∗(a |b ). However, according to Theorem 5.7, we
t t
know that the global solution of the unconstrained problem argmin G(π¯∗)(b ) is
σ
(cid:16) −G(π¯∗)(bt,at) (cid:17)
,whereG(π¯∗)(b ,a )isdefinedas
π¯(at|bt) t
β t t
P(s |b ,a )
G(π¯∗)(b ,a ) = E (cid:2) −αr +E [ζlog t+1 t t
t t P(rt|bt,at) t P(bt+1,ot+1,st+1|bt,at)
q(s |o ,b )
t+1 t+1 t
+ γG(π¯∗)(b )] (cid:3) . (39)
t+1
¯
If we choose the policy set Π to be convex, like a set of Gaussian or T-distributions
over the compact action space A, the constrained optimization problem in Eq. (37) for
a POMDP satisfying Assumption 5.1 meets Slater’s condition (Boyd & Vandenberghe,
2004).10 This implies the existence of a unique minimizer π¯∗(a |b ) ∈ Π ¯ that is ob-
t t
tained by projecting the solution of the unconstrained problem, σ
(cid:16) −G(π¯∗)(bt,at) (cid:17)
, onto
β
¯
theconstrainedsetΠ. Thisprojectionoperationinvolvesfindingtheclosestdistribution
inΠ ¯ toπ∗(a |b ) = σ
(cid:16) −G(π¯∗)(bt,at) (cid:17)
,accordingtosomedistancemetricordivergence.
t t β
ConsistentwithstandardpracticesinRLandAIFformulations(Haarnoja,Zhou,Abbeel,
& Levine, 2018; K. Friston et al., 2017), we use information projection based on the
KL-divergence to perform this mapping. Therefore, the unique solution of the con-
strainedoptimizationprobleminEq.(37)is:
(cid:20) (cid:18) −G(π¯∗)(b ,.) (cid:19)(cid:21)
π¯∗(.|b ) = argminD π¯(.|b ),σ t . (40)
t KL t
π¯∈Π¯ β
AstheintractabletermZ∗(b )isindependentofactiona ,Eq.(40)canbesimplifiedas:
t t
(cid:20) (cid:18) −G(π¯∗)(b ,.) (cid:19)(cid:21)
π¯∗(.|b ) = argminD π¯(.|b ),σ t (41)
t KL t
π¯∈Π¯ β
(cid:20) G(π¯∗)(b ,a ) (cid:21)
= argminE logπ¯(a |b )+ t t . (42)
π¯∈Π¯
π¯(at|bt) t t
β
Eq. (42) demonstrates that by selecting actions from π¯∗(.|b ) instead of π∗(.|b ) =
t t
σ
(cid:16) −G(π∗)(bt,·) (cid:17)
, the intractable term Z∗(b ) is no longer involved in the action selec-
β t
tionprocess.
Note5.2: Throughouttherestofthispaper,anypolicyπ andanyoptimalpolicyπ∗
mentioned are considered unconstrained. Conversely, any policy π¯ and optimal policy
π¯∗ mentionedareassumedtobelongtotheconstrainedpolicyspaceΠ ¯ .
5.4 Existence of a unique optimal belief state-action EFE
In the previous sub-section, assuming the existence of G(π¯∗)(b ,a ), we have demon-
t t
strated that the solution to the minimization problem in Eq. (37) is unique and can be
computedasfollows:
(cid:20) (cid:18) −G(π¯∗)(b ,.) (cid:19)(cid:21)
π¯∗(.|b ) = argminD π¯(.|b ),σ t , (43)
t KL t
π¯∈Π¯ β
Our objective in this sub-section is to demonstrate the existence of a unique value for
G(π¯∗)(b ,a )thatleadstoderivingtheoptimalpolicyπ¯∗viaEq.(43). Thisdemonstration
t t
involvesthreesteps.
First, we show through Lemma 5.8 that G(π¯∗)(b ,a ) in Eq. (39) can be expressed as
t t
10FormoredetailsaboutSlater’scondition,wereferthereaderto(Boyd&Vandenberghe,2004).
G(π¯∗)(b ,a ) = min G(π¯)(b ,a ),where
t t π¯∈Π¯ t t
(cid:20)
P(s |b ,a )
G(π¯)(b ,a ) = E −αr+E (cid:2) ζlog t+1 t t
t t P(rt|bt,at) t P(bt+1,ot+1,st+1|bt,at)
q(s |o ,b )
t+1 t+1 t
(cid:21)
(cid:3)
+ γG(π¯)(b ) . (44)
t+1
Lemma5.8 G(π¯∗)(b ,a )inEq.(39)canberewrittenasG(π¯∗)(b ,a ) = min G(π¯)(b ,a ),
t t t t π¯∈Π¯ t t
whereG(π¯)(b ,a )isgivenby
t t
(cid:20)
P(s |b ,a )
G(π¯)(b ,a ) = E −αr+E (cid:2) ζlog t+1 t t
t t P(rt|bt,at) t P(bt+1,ot+1,st+1|bt,at)
q(s |o ,b )
t+1 t+1 t
(cid:21)
(cid:3)
+ γG(π¯)(b ) . (45)
t+1
Proof. SeeAppendixD.6. (cid:50)
While G(π¯)(b ) represents the belief state EFE for a given belief state b following a
t t
given policy π¯, G(π¯)(b ,a ) in Eq. (45) quantifies the belief state EFE G(π¯)(b ) condi-
t t t
tioned on a given action a ∈ A followed by the policy π¯ afterwards. Henceforth,
t
we refer to G(π¯)(b ,a ) and G(π¯∗)(b ,a ) = min G(π¯)(b ,a ) as the belief state-action
t t t t π¯∈Π¯ t t
EFEandtheoptimalbeliefstate-actionEFE,respectively.
Second, by utilizing the result of Lemma 5.8, we establish the following proposition,
whichintroducestheunifiedBellmanequationforthebeliefstate-actionG(π¯)(b ,a ).
t t
Proposition5.9 (UnifiedBellmanequationforG(π)(b ,a ))
t t
The belief state-action G(π¯)(b ,a ) in a POMDP satisfying Assumption 5.1 can be cal-
t t
culatedrecursivelystartingfromthebeliefstateb andactiona andthenfollowingthe
t t
¯
policyπ¯ ∈ Πasfollows:
(cid:20)
P(s |b ,a )
G(π¯)(b ,a ) = E −αr +E (cid:2) ζlog t+1 t t
t t P(rt|bt,at) t P(bt+1,ot+1,st+1|bt,at)
q(s |o ,b )
t+1 t+1 t
(cid:21)
+ γE [βlogπ¯(a |b )+G(π¯)(b ,a )] (cid:3) . (46)
π¯(at+1|bt+1) t+1 t+1 t+1 t+1
Proof. SeeAppendixD.7. (cid:50)
Third,byutilizingtheresultofProposition5.9,weestablishthroughTheorem5.10that
the belief state-action EFE G(π¯)(b ,a ), for a given policy π¯ ∈ Π ¯ , exists and is a unique
t t
fixed point of the operator TunifiedG(b ,a ). This operator, which we refer to as the
π¯ t t
unifiedBellmanoperator,isdefinedasfollows:
(cid:20)
P(s |b ,a )
TunifiedG(b ,a ):=−E −αr +E (cid:2) ζlog t+1 t t
π¯ t t P(rt|bt,at) t P(bt+1,ot+1,st+1|bt,at) q(s |o ,b )
t+1 t+1 t
(cid:21)
+ γE [βlogπ¯(a |b )+G(b ,a )] (cid:3) . (47)
π¯(at+1|bt+1) t+1 t+1 t+1 t+1
Theorem5.10 (Existenceofauniquebeliefstate-actionG(π¯)(b ,a )foragivenπ¯)
t t
Thebeliefstate-actionEFEG(π¯)(b ,a )forallb ∈ B anda ∈ AinaPOMDPsatisfy-
t t t t
ing Assumption 5.1 exists and is a unique fixed point G(π¯)(b ,a ) = TunifiedG(π¯)(b ,a )
t t π¯ t t
oftheunifiedBellmanoperatorTunified inEq.(47).
π¯
Proof. SeeAppendixD.8. (cid:50)
Finally, using Theorem 5.10, Corollary 5.11 shows that that the optimal belief state-
actionEFEG(π¯∗)(b ,a ) = min G(π¯)(b ,a )existsandistheuniquefixedpointgiven
t t π¯∈Π¯ t t
byG(π¯∗)(b ,a ) = TunifiedG(π¯∗)(b ,a ).
t t π¯∗ t t
Corollary5.11 (Existenceofauniqueoptimalbeliefstate-actionG(π¯∗)(b ,a ))
t t
The optimal belief state-action EFE G(π¯∗)(b ,a ) = min G(π¯)(b ,a ) defined in a
t t π¯∈Π t t
POMDP satisfying Assumption 5.1 exits and is the unique fixed point of the following
equation:
G(π¯∗)(b ,a )=TunifiedG(π¯∗)(b ,a ) (48)
t t π¯∗ t t
(cid:20)
P(s |b ,a )
=E −αr +E (cid:2) ζlog t+1 t t
P(rt|bt,at) t P(bt+1,ot+1,st+1|bt,at)
q(s |o ,b )
t+1 t+1 t
(cid:21)
+γE [βlogπ¯∗(a |b )+G(π¯∗)(b ,a )] (cid:3) . (49)
π¯∗(at+1|bt+1) t+1 t+1 t+1 t+1
Proof. SeeAppendixD.9. (cid:50)
Corollary5.11demonstratestheexistenceanduniquenessofG(π¯∗)(b ,a ),whichplaysa
t t
crucialroleinEq.(43)todeterminetheoptimalpolicyπ¯∗(a |b ). Furthermore,Eq.(49)
t t
extendstheunifiedBellmanoptimalityequationtoincorporatetheoptimalbeliefstate-
action EFE G(π¯∗)(b ,a ), and therefore, we refer to it as the unified Bellman optimality
t t
equationforG(π¯∗)(b ,a ).
t t
5.5 Derivation of unified policy iteration
Sofar,wehaveproventheexistenceanduniquenessoftheoptimalpolicyπ¯∗(.|b )as
t
(cid:20) (cid:18) −G(π¯∗)(b ,.) (cid:19)(cid:21)
π¯∗(.|b ) = argminD π¯(.|b ),σ t , (50)
t KL t
π¯∈Π¯ β
where
(cid:20)
P(s |b ,a )
G(π¯∗)(b ,a ) = E −αr +E (cid:2) ζlog t+1 t t
t t P(rt|bt,at) t P(bt+1,ot+1,st+1|bt,at)
q(s |o ,b )
t+1 t+1 t
(cid:21)
+ γE [βlogπ¯∗(a |b )+G(π¯∗)(b ,a )] (cid:3) . (51)
π¯∗(at+1|bt+1) t+1 t+1 t+1 t+1
FromEqs.(50)and(51),itisevidentthatthelearningoftheoptimalpolicyπ¯∗(a |b )at
t t
time t depends on the optimal belief state-action G(π¯∗)(b ,a ), which, in turn, depends
t t
on the optimal policy π¯∗(a |b ) at the subsequent time step. As a result, π¯∗(a |b )
t+1 t+1 t t
and its corresponding belief state-action G(π¯∗)(b ,a ) follow a recursive pattern over
t t
time, enabling their recursive determination. To initiate this recursive process, we can
startwithanarbitrarypolicy,whichservesastheinitialoptimalpolicy(baselinepolicy),
and initialize its corresponding belief state-action EFE with arbitrary values. We then
update the belief state-action EFE corresponding to the baseline policy using Eq. (51).
Next, we update the baseline policy using Eq. (50). This updated policy becomes the
new baseline policy, and the process is repeated iteratively over k ∈ {0,1,2,...}. In-
spired by this recursive procedure, we propose an iterative algorithm called unified
policy iteration for concurrent learning of π¯∗ and G(π¯∗). The algorithm involves up-
dating the baseline policy and its corresponding belief state-action EFE iteratively in a
computationally efficient manner. We provide a proof of convergence for the proposed
unified policy iteration, showing that it converges to the optimal policy π¯∗ and its cor-
responding belief state-action EFE G(π¯∗). This algorithm offers an effective approach
forsolvingthePOMDPproblemwithcontinuousstate,action,andobservationspaces,
providingapracticalframeworkfordecision-makingincomplexenvironments.
Consider a randomly selected policy π¯(old) ∈ Π ¯ and its randomly initialized be-
lief state-action EFE G(π¯(old)). We can update G(π¯(old)) by applying the unified Bellman
operatorTunifiedG(π¯(old))(b ,a ),asfollows
π¯(old) t t
G(π¯(old))(b ,a )←TunifiedG(π¯(old))(b ,a ) (52)
t t π¯(old) t t
(cid:20)
P(s |b ,a )
= E −αr +E (cid:2) ζlog t+1 t t
P(rt|bt,at) t P(bt+1,ot+1,st+1|bt,at)
q(s |o ,b )
t+1 t+1 t
(cid:21)
+ γE [βlogπ¯(old)(a |b )+G(π¯(old))(b ,a )] (cid:3) . (53)
π¯(old)(at+1|bt+1) t+1 t+1 t+1 t+1
Next,let’sconsideranewpolicyπ¯(new)(.|b )resultingfromthefollowingequation:
t
(cid:34) (cid:32) (cid:33)(cid:35)
−G(π¯(old))(b ,.)
π¯(new)(.|b )=argminD π¯(.|b ),σ t . (54)
t KL t
π¯∈Π¯ β
The next lemma proves that the belief state-action EFE corresponding to π¯(new) has a
lowervaluethanthebeliefstate-actionEFEcorrespondingtothebaselinepolicyπ¯(old).
We thus refer to the new policy π¯(new) as an improved version of the baseline policy
π¯(old),andwerefertothenextlemmaastheunifiedpolicyimprovement.
Lemma5.12 (Unifiedpolicyimprovement)
Let π¯(old) ∈ Π be a randomly selected policy, and let π¯(new)(.|b ) be π¯(new)(.|b ) =
t t
argmin D [π¯(.|b
),σ(−G(π¯(old))(bt,.))]inaPOMDPsatisfyingAssumption5.1.
Then:
π¯∈Π¯ KL t β
G(π¯(new))(b ,a ) ≤ G(π¯(old))(b ,a ) (55)
t t t t
forall(b ,a ) ∈ B ×A.
t t
Proof. SeeAppendixD.10. (cid:50)
Using Lemma 5.12, we formally state our proposed unified policy iteration algorithm
throughthefollowingtheorem.
Theorem5.13 (Unifiedpolicyiteration)
Consider a POMDP satisfying Assumption 5.1. By starting from an initial policy π¯ ∈
0
Π ¯ and an initial mapping G : B × A → R and recursively applying the following
0
updatesfork = 0,1,2,...:
G (b ,a ) = TunifiedG (b ,a ), (56)
k+1 t t π¯ k t t
k
(cid:20) (cid:18) (cid:19)(cid:21)
−G (b ,·)
k+1 t
π¯ (·|b ) = argminD π¯(·|b ),σ , (57)
k+1 t KL t
π¯∈Π¯ β
theiterativeprocessconvergestoπ¯∗ = argmin G(π¯)(b ).
π¯∈Π¯ t
Proof. SeeAppendixD.11. (cid:50)
Theorem 5.13 demonstrates that the optimal policy π¯∗ = argmin G(π¯)(b ) can be
π¯∈Π¯ t
obtained by recursively alternating between the belief state-action EFE update step,
givenbyEq.(56),andthepolicyimprovementstep,givenbyEq.(57).
5.6 Unified reward function for AIF and RL in POMDPs
The unified Bellman equation for the belief state EFE G(π¯)(b ) and the Bellman op-
t
timality equation for G(π¯∗)(b ) in a POMDP, as stated in Eqs. (35) and (36) respec-
t
tively,exhibitastrongresemblancetotheBellmanequationforthestatevaluefunction
V(π)(s ) and the Bellman optimality equation for V∗(s ) in an MDP, as represented by
t t
Eqs. (7) and (10) respectively. Given this similarity and considering that the proposed
unifiedinferencealgorithmaimstominimizethebeliefstateEFEwhileRLendeavours
to maximize the state value function, we can interpret G(π¯)(b ) as −V ¯(π¯)(b ), where
t t
V
¯(π¯)(b
)isreferredtoasthebeliefstatevaluefunction,definedasfollows:
t
(cid:20) ∞ (cid:18)
(cid:88) P(s |a ,b )
V ¯(π¯)(b t ) = E (cid:81)∞
τ=t
π(aτ|bτ)P(sτ+1,rτ,oτ+1,bτ+1|bτ,aτ) γτ−t αr τ −ζlog
q(s
τ+
|
1
o
τ
,b
τ
)
τ+1 τ+1 τ
τ=t
(cid:19)(cid:21)
− βlogπ¯(a |b ) . (58)
τ τ
ThebeliefstatevaluefunctionV
¯(π¯)(b
)inEq.(58)canbeconsideredasavaluefunction
t
foraPOMDPwiththerewardfunctionrunified attimestepτ definedas:
τ
(cid:18) (cid:19)
P(s |a ,b )
runified = α r +ζ −log τ+1 τ τ +β (−logπ¯(a |b )). (59)
τ (cid:124)(cid:123)(cid:122) τ (cid:125) q(s τ+1 |o τ+1 ,b t ) (cid:124) (cid:123)(cid:122) τ τ (cid:125)
r τ extrinsic (cid:124) (cid:123)(cid:122) (cid:125) rτ entropy
rintrinsic
τ
The reward function runified represents the agent’s total reward at time step τ after per-
τ
forming action a in belief state b . It encompasses the reward r obtained, the transi-
τ τ τ
tiontothenextstates ,andtheobservationo withinthePOMDPframework. The
τ+1 τ+1
firsttermofrunified(i.e.,rextrinsic)correspondstotheextrinsicreward,whichisthereward
τ τ
function commonly used in MDP-based RL. The second term (rintrinsic) represents the
τ
intrinsicrewardthatencouragestheagenttovisitstatesprovidingthemostinformation
about the hidden states of the environment. The combination of the extrinsic reward
rextrinsic and the intrinsic reward rintrinsic balances exploration for information gain with
τ τ
the exploitation of extrinsic rewards. The third term in Eq. (59) arises from taking
the expected value over future actions and corresponds to entropy maximization. This
term, referred to as the entropy reward rentropy, promotes random exploration and pre-
τ
vents the policy from becoming overly deterministic, thereby encouraging the learning
of stochastic policies π¯. As explained in Section 2, learning a stochastic policy enables
the agent to adapt to different situations resulting from environmental changes, lead-
ing to improved stability in the proposed unified policy iteration approach. The reward
function runified combines both the extrinsic reward employed in RL and the intrinsic
τ
reward that promotes information-seeking exploration as used in AIF. Hence, we des-
ignaterunified asaunifiedrewardfunctionapplicabletobothAIFandRLinPOMDPs.
τ
Giventhatthestate-actionvaluefunctionQ(π)(s ,a )andthebeliefstate-actionEFE
t t
G(π¯)(b ,a ) are obtained by conditioning V(π)(s ) and G(π¯)(b ) on the current action
t t t t
A = a , we can similarly interpret G(π¯)(b ,a ) as negative counterpart of the belief
t t t t
state-actionvaluefunctionQ
¯(π¯)(b
,a ),definedasfollows:
t t
(cid:20) ∞ (cid:18)
(cid:88) P(s |a ,b )
Q ¯(π¯)(b t ,a t )=E (cid:81)∞
τ=t
π(aτ+1|bτ+1)P(sτ+1,rτ,oτ+1,bτ+1|bτ,aτ) γτ−t αr τ −ζlog
q(s
τ+
|
1
o
τ
,b
τ
)
τ+1 τ+1 τ
τ=t
(cid:19)(cid:21)
−βlogπ¯(a |b ) . (60)
τ+1 τ+1
Bysettingα = 1,β = 0,andζ = 0intheunifiedrewardfunctionrunified inEq.(59),
τ
theoptimalpolicyπ¯∗ = argmax V ¯(π¯)(b )aimstomaximizetheexpectedlong-term
π¯∈Π¯ t
extrinsic reward alone, which is the objective in MDP-based RL algorithms. However,
bysettingα = 1,β = 0,andζ = 1,theoptimalpolicyπ¯∗ = argmax V ¯(π¯)(b )aims
π¯∈Π¯ t
tomaximizeboththeexpectedlong-termextrinsicrewardthroughrextrinsic andtheex-
τ
pected long-term information gain through rintrinsic, which is crucial in the context of
τ
POMDPs. Therefore, we can extend RL methods developed for MDPs to POMDPs
by augmenting the external reward r in these RL approaches with the intrinsic reward
τ
−log P(sτ+1|bτ,aτ) .
q(sτ+1|oτ+1,bτ)
Furthermore,bysettingα = 0,β = 0,andζ = 1intheunifiedrewardfunctionrunified
τ
in Eq. (59), the optimal policy π¯∗ = argmax V ¯(π¯)(b ) solely focuses on maximiz-
π¯∈Π¯ t
ingtheexpectedinformationgainthroughtheintrinsicrewardtermrintrinsic. Therefore,
τ
by replacing the extrinsic reward r in these RL approaches with the intrinsic reward
τ
−log P(sτ+1|bτ,aτ) , we can extend extrinsic reward-dependent RL algorithms to a set-
q(sτ+1|oτ+1,bt)
ting whereextrinsic reward valueshave not beendetermined by anexternal supervisor.
This extension is particularly important in scenarios where designing extrinsic reward
valuesiscostlyorspecifyingthemischallenging.
6 Unified inference model
AsmentionedinAssumption5.2,theproposedunifiedinferenceassumesthattheagent
performs perceptual inference and learning before action selection at each time step by
minimizing the VFE. The action selection phase is facilitated by the proposed unified
policyiterationmethodoutlinedinTheorem5.13.
However,inscenarioswithcontinuousstate,action,andobservationspaces,theinfinite
number of possible states, actions, and observations makes it impractical to explic-
itlyrepresentthegenerativemodel,variationalposteriordistribution,beliefstate-action
EFE, and policy using a finite set of values (Sutton & Barto, 2018; Haarnoja, Zhou,
Abbeel, & Levine, 2018). To overcome this challenge, following common practices
in both RL and AIF (Haarnoja, Zhou, Abbeel, & Levine, 2018; Mnih et al., 2013;
Ueltzho¨ffer, 2018; Lee et al., 2020), we utilize DNNs to approximate the generative
model,variationalposteriordistribution,beliefstate-actionEFE,andpolicy.
Considering that perceptual inference and learning are derived from existing literature,
detailed explanations are provided in Appendix E.1. This section primarily focuses on
our contributions: learning the optimal belief state-action policy through our proposed
unifiedpolicyiterationmethod.
6.1 Unified actor-critic
In this sub-section, we focus on approximating the belief state-action EFE G(π¯)(b ,a )
t t
and the belief state-action policy π¯ within the context of the proposed unified policy
iteration, which involves alternating between the belief state-action EFE update step,
as given by Eq. (56), and the policy update step, as given by Eq. (57). Inspired by
actor-critic algorithms in RL, we refer to the approximated policy as the actor and the
approximated belief state-action EFE as the critic. Hence, we term our approximation
oftheunifiedpolicyiterationframeworkastheunifiedactor-critic.
ByutilizingtherelationshipG(π¯)(b ) = −V ¯(π¯)(b ),whicharisesfromtheunifiedreward
t t
function runified in Eq. (59), we can leverage and adapt a wide range of advanced MDP-
t
based actor-critic algorithms to learn the critic G(π¯)(b ,a ) and the actor π¯(a |b ) in our
t t t t
unifiedactor-criticalgorithm. Thisenablesustotakeadvantageoftheexistingmethods
and techniques developed for MDPs and extend them to address the challenges posed
bypartialobservabilityinPOMDPs.
To approximate G(π¯)(b ,a ) and π¯(a |b ), we parameterize them with ψ and ϕ, re-
t t t t
spectively. However, directly inputting the infinite-dimensional continuous belief state
b totheDNNsmodelingthepolicyandthebeliefstate-actionEFEfunctionisimpracti-
t
cal. Instead,weuseabeliefstaterepresentationh ∈ RDH,whereD isthedimension
t H
ofh . ThisrepresentationisexplainedinAppendixE.1. Wethenmodelthebeliefstate-
t
action EFE and the policy as G(π¯)(h ,a ) and π¯ (a |h ), respectively. Thus, the belief
ψ t t ϕ t t
state-actionEFEupdateandthepolicyimprovementstepscanbeexpressedasfollows
(cid:20)
P(s |h ,a )
G(π¯)(h ,a ) = E −αr +E (cid:2) ζlog t+1 t t
ψ t t P(rt|ht,at) t P(ht+1,ot+1,st+1|ht,at) q(s |o )
t+1 t+1
(cid:21)
+ γE [βlogπ¯(a |h )+G(π¯)(h ,a )] (cid:3) , (61)
π¯(at+1|ht+1) t+1 t+1 ψ t+1 t+1
and
(cid:34) (cid:32) (cid:33)(cid:35)
−G(π¯)(h ,.)
ψ t
π¯ (.|h )=argminD π¯(.|h ),σ . (62)
ϕ t KL t
π¯∈Π¯ β
We train ψ to minimize the squared residual error derived from Eq. (61) using
batches of size B. Each batch can consist of M sequential data points denoted as
{(a ,r ,o )M }B , sampled from a replay buffer D containing real environment
k−1 k k k=1 i=1
interactions. Alternatively, batches may include N simulated interaction data points
{(a ,r ,o )t+N}B , starting from h and simulating interactions up to N steps. We
τ τ τ+1 τ=t i=1 t
can also use a combination of both real and imagined interactions for training ψ. De-
pendingonthetypeofdataused,wedevisethreeunifiedactor-criticapproaches: model-
basedunifiedactor-critic,model-freeunifiedactor-critic,andhybridunifiedactor-critic.
6.1.1 Model-freeunifiedactor-critic
Inspired by model-free RL algorithms, our proposed model-free unified actor-critic
learns ψ by minimizing the squared residual error from Eq. (61) over a batch of real
data {(a ,r ,o )M }B sampled from the replay buffer D. This involves minimiz-
k k k+1 k=1 i=1
ingthesquaredresidualerrorLMF(ψ),whichisdefinedas
G
(cid:20) (cid:18)
1 P(s |h ,a )
LMF(ψ) = E G(π¯)(h ,a )+αr −E (cid:2) ζlog t+1 t t
G D(a k ,r k ,o k+1 ) 2 ψ t k k q(st+1|ht,o k+1 ) q(s |o ,h )
t+1 t+1 t
(cid:19)2(cid:21)
−γE [βlogπ¯ (a |h )+G(π¯)(h ,a )] (cid:3) ,(63)
P(ht+1|ht,a k ,o k+1 ,st+1)π¯ ϕ (at+1|ht+1) ϕ t+1 t+1 ψ t+1 t+1
Afterupdatingψ,theparameterϕofthepolicyπ¯ (a |h )canbetrainedbyminimizing
ϕ t t
theright-handsideofEq.(62):
(cid:34) (cid:32) (cid:33)(cid:35)
−G(π¯)(h ,.)
LMF(ϕ) = D π¯ (.|h ),σ ψ t (64)
π¯ KL ϕ t β
(cid:104) (cid:105)
= E βlogπ¯ (a |h )+G(π¯)(h ,a ) . (65)
π¯ ϕ (at|ht) ϕ t t ψ t t
To minimize Eq. (65), we need to calculate its gradient with respect to ϕ by sam-
pling from π¯ (a |h ). However, computing this gradient involves differentiating with
ϕ t t
respect to π¯ (a |h ), which we sample from. Therefore, we use the reparameterization
ϕ t t
trick(Kingma&Welling,2013)forsamplingfromπ¯ (a |h ).
ϕ t t
By employing the relationship G(π¯)(h ) = −V ¯(π¯)(h ) and considering a specific
ψ t ψ t
case of the model-free unified actor-critic algorithm, where α = 1 and ζ = 0, we re-
cover the variational recurrent model (VRM) algorithm proposed in (Han et al., 2020)
as an extension of SAC (Haarnoja, Zhou, Abbeel, & Levine, 2018) to POMDPs. Addi-
tionally,byassigningα = 0andβ ̸= 0inthemodel-freeunifiedactor-criticalgorithm,
we can formulate the so-called reward-free variants of SAC (Haarnoja, Zhou, Abbeel,
& Levine, 2018; Haarnoja, Zhou, Hartikainen, et al., 2018) in settings without an ex-
trinsic reward function. Moreover, by setting α = 1 and assigning non-zero values to
ζ (i.e., ζ ̸= 0), our model-free unified actor-critic algorithm is transformed into a gen-
eralized version of SAC for POMDPs. This generalized version maximizes both the
expectedfutureextrinsicrewardandinformationgain. Werefertothisextensionasthe
generalizedSAC(G-SAC).
Generalized SAC (G-SAC) generalizes model-free SAC (Haarnoja, Zhou, Abbeel, &
Levine,2018)toPOMDPswiththefollowinglossfunctions:11
(cid:20) (cid:18)
1 P(s |h ,a )
LG-SAC(ψ) = E G(π¯)(h ,a )+r −E (cid:2) ζlog t+1 t t
G D(a k ,r k ,o k+1 ) 2 ψ t k k q(st+1|ht,o k+1 ) q(s |o ,h )
t+1 t+1 t
(cid:19)2(cid:21)
−γE [βlogπ¯ (a |h )+G(π¯)(h ,a )] (cid:3) ,(66)
P(ht+1|ht,a k ,o k+1 ,st+1)π¯ ϕ (at+1|ht+1) ϕ t+1 t+1 ψ t+1 t+1
and
(cid:104) (cid:105)
LG-SAC(ϕ) = E βlogπ¯ (a |h )+G(π¯)(h ,a ) . (67)
π¯ π¯ ϕ (at|ht) ϕ t t ψ t t
Note 6.1: It is important to clarify that the proposed model-free unified actor-critic
algorithm incorporates a learned belief state generative model to compute the intrinsic
reward rintrinsic and the belief state representation h . Therefore, from a strict RL per-
t t
spective, it is not considered a purely model-free algorithm. However, in this context,
theterm”model-free”referstothefactthatthegenerativemodelisnotusedfordatase-
lectionduringbeliefstate-actionEFE(critic)learning. Instead,theagentdirectlylearns
thepolicyandbeliefstate-actionEFEfromobservedinteractionswiththeenvironment.
Duringtheiterativeprocessofthemodel-freeunifiedactor-critic,theagentinteracts
withtheenvironment,updatesitsbeliefstate-actionEFEbasedonobservedrewardsand
observations, and learns through trial and error. Model-free methods, not relying on a
learned model for data selection, are more robust to modeling errors or inaccuracies.
However, this trial-and-error approach can be time-consuming and sample inefficient,
11In the original formulation of SAC (Haarnoja, Zhou, Abbeel, & Levine, 2018), they introduced
an additional function approximator for the state value function, but later they found it to be unneces-
sary(Haarnoja,Zhou,Hartikainen,etal.,2018).
often requiring numerous samples to converge to an optimal or near-optimal policy. To
tacklethis,weintroduceamodel-basedversionoftheunifiedactor-criticmethodinthe
nextphasetolearntheoptimalpolicy.
6.1.2 Model-basedunifiedactor-critic
Theproposedmodel-basedunifiedactor-criticalgorithmutilizesthelearnedbeliefstate
generativemodeltosimulateandpredictfutureobservationsandrewardsresultingfrom
differentactions. Thisenablestheagenttomakedecisionsandoptimizeitspolicywith-
outneedingdirectinteractionwiththeenvironment.
Toimplementthemodel-basedunifiedactor-criticapproach,theagentcollectstrajecto-
ries{(a ,r ,o )t+N}B startingfromtheinitialbeliefstateh andforwardingtillN.
τ τ τ+1 τ=t i=1 t
Thesetrajectoriesaregeneratedbyimaginingfollowingthepolicya ∼ π¯ (a |h ),uti-
τ ϕ τ τ
lizingthetransitionmodelP(s |h ,a ),therewardmodelP(r |h ,a ),andthelike-
τ+1 τ τ τ τ τ
lihoodmodelP(o |h ,s ). It’simportanttonotethattherewardmodelP(r |s ,a )
τ+1 τ τ+1 t t t
alsoneedstobelearned,withtheprocessdetailedinAppendixE.2.
Next,thecriticparameterψ isupdatedbyminimizingthefollowinglossfunction:
(cid:20) (cid:18)
1
LMB(ψ)=E G(π¯)(h ,a )+αr
G π¯ ϕ (at|ht)P(rt|ht,at) 2 ψ t t t
P(s |h ,a )
−E (cid:2) ζlog t+1 t t (68)
P(st+1|ht,at)P(ot+1|ht,st+1)
q(s |o ,h )
t+1 t+1 t
(cid:19)2(cid:21)
− γE [βlogπ¯ (a |h )+G(π¯)(h ,a )] ,
P(ht+1|ht,at,ot+1,st+1)π¯ ϕ (at+1|ht+1) ϕ t+1 t+1 ψ t+1 t+1
wheretheexpectationsareestimatedundertheimaginedtrajectories{(a ,r ,o )t+N}B .
τ τ τ+1 τ=t i=1
Theactorparameterϕisestimatedbyminimizingthefollowinglossfunction:
LMB(ϕ) = E (cid:2) βlogπ¯ (a |h )+G(π¯)(h ,a ) (cid:3) . (69)
π¯ π¯ ϕ (at|ht) ϕ t t ψ t t
Note 6.2: This model-based unified actor-critic approach reduces memory usage
and improves sample efficiency compared to the model-free unified actor-critic. How-
ever, it requires learning the reward model P(r |h ,a ), unlike the model-free unified
τ τ τ
actor-critic. Learning the reward model of an environment is typically more challeng-
ingthanlearningthetransitionandlikelihoodmodelssincepredictionerrorsinthestate
and observation of the environment provide a richer source of information than reward
prediction errors. This is because rewards are usually scalar, while the environment’s
stateandobservationaretypicallycharacterizedbyhighdimensionality.
By relating G(π¯)(h ) to −V ¯(π¯)(h ), we can recover the model-based actor-critic
ψ t ψ t
Dreamer algorithm (Hafner et al., 2020), which focuses on maximizing rewards, as a
specialcaseofourmodel-basedunifiedactor-criticalgorithmwhenα = 1andζ = 0.12
Additionally, we can create a reward-free version of the Dreamer by assigning α = 0
and β ̸= 0 in the model-based unified actor-critic algorithm. Furthermore, by selecting
α = 1 and assigning non-zero values to ζ, we extend Dreamer to POMDPs, aiming
to maximize both the expected long-term extrinsic reward and the expected long-term
information gain during action selection. This extended version is referred to as the
GeneralizedDreamer(G-Dreamer).
GeneralizedDreamer(G-Dreamer)generalizesmodel-basedDreamerframework(Hafner
etal.,2020)toPOMDPswiththefollowinglossfunctions:
(cid:20) (cid:18)
1
LG-Dreamer(ψ)=E G(π¯)(h ,a )+r
G π¯ ϕ (at|ht)P(rt|ht,at) 2 ψ t t t
P(s |h ,a )
−E (cid:2) ζlog t+1 t t (70)
P(st+1|ht,at)P(ot+1|ht,st+1)
q(s |o ,h )
t+1 t+1 t
(cid:19)2(cid:21)
− γE [βlogπ¯ (a |h )+G(π¯)(h ,a )] ,
P(ht+1|ht,a k ,ot+1,st+1)π¯ ϕ (at+1|ht+1) ϕ t+1 t+1 ψ t+1 t+1
and
LG-Dreamer(ϕ) = E (cid:2) βlogπ¯ (a |h )+G(π¯)(h ,a ) (cid:3) . (71)
π¯ π¯ ϕ (at|ht) ϕ t t ψ t t
6.1.3 Hybridunifiedactor-critic
While model-based unified actor-critic methods offer the potential for higher sample
efficiency and advanced trajectory integration, their reliance on accurate models for
performance is crucial. Errors in learned generative models can accumulate over time,
leading to significant deviations from desired behavior (X. Ma et al., 2021). Learn-
ing precise models, especially with complex observations, is challenging. Inspired
by (X. Ma et al., 2021), to mitigate model errors, we adopt a hybrid unified actor-
critic learning scheme. This approach combines the sample efficiency of model-based
learningwiththerobustnessofmodel-freelearningtoaddressinaccuracies.
In the hybrid unified actor-critic scheme, the critic parameter ψ is learned using both
realtrajectories{(a ,r ,o )M }B andimaginedtrajectories{(a ,r ,o )t+N}B :
k k k+1 k=1 i=1 τ τ τ+1 τ=t i=1
LHybrid(ψ) = LMB(ψ)+cLMF(ψ), (72)
G G G
12Dreamer computes estimates of G(π¯)(h ) instead of G(π¯)(h ,a ). However, these two estimates
t t t
are equivalent due to the relationship between G(π¯)(h ) and G(π¯)(h ,a ) illustrated in Appendix D.7.
t t t
Specifically,wehaveG(π¯)(h )=E (cid:2) βlogπ¯(a |h )+G(π¯)(h ,a ) (cid:3) .
t π¯(at|ht) t t t t
where c represents the scaling factor that determines the relative importance of the
model-freecriticlossfunctionLMF(ψ)comparedtothemodel-basedcriticlossfunction
G
LMB(ψ). Theactorparameterϕislearnedbyminimizingthefollowinglossfunction:
G
LHybrid(ϕ) = E (cid:2) βlogπ¯ (a |h )+G(π¯)(h ,a ) (cid:3) . (73)
π¯ π¯ ϕ (at|ht) ϕ t t ψ t t
The complete learning process of the unified inference model is outlined in Algo-
rithmE.1oftheappendix.
7 Related work
This section provides a concise overview of relevant literature, specifically addressing
theextensionofRLandAIFtechniquestoPOMDPsfeaturingcontinuousspaces.
7.1 RL approaches for continuous space POMDPs
While RL conventionally targets MDPs, recent progress has broadened its application
to POMDPs. These advancements frequently employ memory-based neural networks
toencodepastobservationsandactionsoremploybeliefstateinference.
7.1.1 Memory-basedapproaches
Hausknecht and Stone (2015) developed a variant of (Mnih et al., 2013) to handle
POMDPsbyincorporatingarecurrentlayer,suchasLongShort-TermMemory(LSTM)
or Gated Recurrent Unit (GRU), to capture the history of observations. However, this
method did not consider the history of actions, focusing solely on the observation se-
quence. Later, Zhu et al. (2017); Heess, Hunt, Lillicrap, and Silver (2015); Nian et
al. (2020) utilized recurrent layers to capture both the observation and action history.
Theydemonstratedthatitispossibletostoreonlythenecessarystatisticsofthehistory
using recurrent layers instead of storing the entire preceding history. It is worth noting
thattheseworksprimarilyfocusedontaskswithdiscreteactionspacesratherthancon-
tinuousones. HaklidirandTemeltas¸ (2021)proposedtheguidedSACapproach,which
augmentstheoriginalSACwithaguidingpolicy. TheguidedSACarchitectureconsists
of two actors and a critic, where the original actor incorporates the history of observa-
tions and actions, while the guiding actor uses the true state as input. Although the
guided SAC has been applied to tasks with continuous observation and action spaces,
it still requires storing the history of observations and actions and relies on an external
supervisor to provide additional information about the true state of the environment,
whichremainsachallenge. Meng,Gorbet,andKulic´ (2021);Nietal.(2022);Yangand
Nguyen (2021) extended actor-critic algorithms to POMDPs by adding recurrent lay-
ers to both the actor and critic components to compress history into a into a fixed-size
representation. This compresses history into a manageable form, passing to the actor
and critic. This adaptation allowed the models to effectively handle continuous action
spaces.
Although memory-based approaches have demonstrated promise in tasks involving
continuousstate,action,andobservationspaces,ourproposedunifiedinferenceframe-
work, which expands AIF into continuous spaces through the extension of the EFE to
stochasticbeliefstate-actionpolicies,offersthreemainadvantages:
i) Computational and memory efficiency: Firstly, although we also transform the con-
tinuous belief state b into a fixed-length representation h using an LSTM, the LSTM
t t
in our method updates the belief state representation based on fixed-size inputs h ,
t−1
a , o , and s (see Appendix E.1 for more details). In contrast, the memory-based
t−1 t t
methods in (Meng et al., 2021; Ni et al., 2022; Yang & Nguyen, 2021) require passing
the entire history to their LSTM, leading to substantial memory demands in large or
infinite time horizon problems. Although our belief state inference-based method may
not require as much memory since it does not need to store extensive historical data, it
stilldemandssignificantcomputationalresources. Thesedemandsariseprimarilyfrom
the complexity involved in learning the generative model, belief state, and belief state
representation. However, belief state inference-based algorithms, including ours, often
becomemorecomputationallyefficientovertimecomparedtomemory-basedmethods.
Thisefficiencystemsfromtwomainfactors: a)Beliefstateinference-basedapproaches
learn generative models and belief states through feed-forward neural networks, which
are less computationally demanding than RNNs (Pascanu et al., 2013). b) In memory-
basedmethods,thedimensionofinputstoLSTMsgrowsashistoryaccumulates,even-
tually surpassing the fixed input dimension of our LSTM for belief state representation
learninginourinfinitehorizonPOMDPsetting. Thisincreaseintheinputdimensionof
RNNs leads to increasingly computationally expensive matrix multiplications at each
step, while the computational complexity of our algorithms remains fixed across time
steps. This illustrates one of the advantages of basing our proposed unified inference
on AIF, as it allows us to leverage inference from AIF for decision-making through a
beliefstate-actionpolicy.
Therefore, over time, the rapid increase in input dimensions in memory-based meth-
ods surpasses the memory and computational demand of learning generative models,
belief states, and belief state representations in our method, especially in scenarios
with large/infinite horizons. A detailed comparison of the memory and computational
complexities between our algorithm and memory-based baselines is provided in Ap-
pendixH.
ii) Enhanced exploration: Beyond the computational and memory efficiencies, our ap-
proachincorporatesaninformationgainexploratorytermwhenmakingdecisions,high-
lightinganothernecessityofAIFformanagingtheuncertaintyinherentinpartiallyob-
servable environments. In recent years, extensive exploration methods for RL in fully
observableenvironmentshaveemerged(Pathak,Agrawal,Efros,&Darrell,2017;Choi
et al., 2018; Savinov et al., 2019). However, due to the limited observability of states
in partially observable tasks, designing intrinsic exploration methods is non-trivial and
challenging. For tasks with partial observability, one prominent line of exploration uti-
lized in memory-based RL relates to prediction error-based approaches, such as the
intrinsic curiosity model (ICM) (Pathak et al., 2017) and random network distillation
(RND) (Burda, Edwards, Storkey, & Klimov, 2018). These methods, which can be
enhanced by incorporating memory mechanisms such as RNNs to handle partial ob-
servability, learn a forward model to predict the next state and sometimes a backward
modeltoinferpaststates. Thepredictionlossfromthesemodelsservesasintrinsicmo-
tivation for the agent to explore new and informative states. Oh and Cavallaro (2019)
introduced a triplet ranking loss to push the prediction output of the forward dynamics
modeltobefarfromtheoutputgeneratedbytakingalternativeactions. However,these
prediction error-based approaches face challenges in accurately discerning the novelty
ofanagent’sstateinpartiallyobservablesettingsfortwomainreasons: 1)Asstatesare
not fully observable, applying these methods in partially observable environments re-
quires constructing forward and backward models based solely on observations, which
are noisy or incomplete representations of states. Relying solely on local observations
is insufficient for accurately inferring novelty over the true world state in tasks with
partial observability. This is because two observations might appear identical at vari-
ouslocationsonthemap,althoughtheirunderlyingtrueworldstatesarefundamentally
different. 2) Prediction errors might restrict the expressiveness of the inferred intrinsic
reward scores. This challenge is especially prominent in environments characterized
by continuous state spaces, where state changes occur subtly over short intervals. Con-
sequently, agents may incorrectly perceive such states as familiar or well-understood,
leading to minimal prediction errors and inadequate novelty detection. Moreover, in
stochastic environments where outcomes following the same action can vary unpre-
dictably,predictionerrorsmaypersistentlyremainhighduetotheintrinsicrandomness
of the environment. This situation often results in the agent receiving high intrinsic
rewards for exploring parts of the environment it may already comprehend, albeit ap-
pearing different due to randomness. As a result, these methods often fall short in
generating a robust novelty measure capable of accurately distinguishing novel states
fromthosepreviouslyencounteredbytheagent(Yinetal.,2021).
Apart from such prediction error-based approaches, Houthooft et al. (2016) proposed
variationalinformationmaximizingexploration(VIME),whichutilizesaBayesianneu-
ral network to learn a state forward model and considers the information gain from the
network parameters as an intrinsic reward function. Although this method does not
suffer from the issue of subtle changes in states and high variability in stochastic envi-
ronments (issue2) found in otherprediction error-based methods, it stillstruggles with
issue 1: it relies on observations, which contain noisy or incomplete information about
thetruestatesoftheenvironment.
In contrast, our unified inference approach actively engages with the environment to
sequentially infer the information gain, which then determines the intrinsic reward of a
state. Our method not only encourages exploration but does so in a manner inherently
alignedwiththeagent’simperativetoreduceuncertaintyandenhanceitsunderstanding
ofthetruestatesoftheenvironment. Evenwhenstatesexhibitsimilarities,theidentifi-
cationofsubtledifferencesthatoffernewinsightsabouttheenvironmentcontributesto
increasedinformationgain. Thisapproachprovidesamorenuancedandsensitivemea-
sure compared to traditional prediction error-based methods. Moreover, the incorpora-
tion of information gain encompasses the entire distribution of potential states, rather
thansolelypredictingthemostprobablenextstate. Thisbroaderperspectiverendersour
methodmoreresilienttothevariabilityintroducedbystochasticity. Theinclusionofin-
formation gain in our generalized actor-critic methods, as detailed in Sub-sections 8.1
and 8.2, not only enhances robustness to noisy observations but also improves sample
efficiencycomparedtoothermemory-basedandexplorationbaselines.
iii)Flexibilityinextrinsicrewarddesign: Anothersignificantadvantageofourapproach
over memory-based algorithms in POMDPs is its flexibility in designing extrinsic re-
wards. ThisflexibilitystemsfromAIF’sinherentcapabilitytointegratetheinformation
gain term. In contrast, memory-based POMDP methods often rely heavily on well-
defined extrinsic reward functions, which can be limiting in complex environments
where specifying every desired outcome is impractical. Our method allows for a more
arbitrarydesignofextrinsicrewards,reducingdependenceonexplicitrewardstructures.
By reducing reliance on rigid reward structures, our method provides a robust frame-
work for developing more adaptable decision-making agents capable of operating in a
broaderarrayofenvironments,especiallywherecraftingspecificrewardsischallenging
orrewardsareinherentlysparse.
In conclusion, our proposed unified inference framework inherently embodies the
three aforementioned advantages: computational and memory efficiency, improved ex-
ploration through state information gain, and the ability to function without extrinsic
rewards. These benefits, derived from AIF, provide a comprehensive approach within
thefreeenergyprinciple,surpassingmemory-basedRLmethods.
7.1.2 Beliefstateinference-basedapproaches
Igl et al. (2018) introduced deep variational RL, which utilizes particle filtering to in-
fer belief states and learn an optimal Markovian policy. This approach minimizes the
ELBO to maximize the expected long-term extrinsic reward using the A2C algorithm.
Building upon this work, Lee et al. (2020) and Han et al. (2020) proposed stochastic
latent actor-critic (SLAC) and VRM, respectively, to extend SAC to POMDPs. SLAC
and VRM also employ the ELBO objective to learn belief states and generative mod-
els. SLAC focuses on pixel-based robotic control tasks, where velocity information is
inferredfromthird-personimagesoftherobot. WhileSLACutilizestheinferredbelief
state solely in the critic network, VRM incorporates the belief state in both the actor
and critic networks. VRM does not utilize the generative model for action selection,
andasshowninSub-section6.1.1,VRMcanbederivedfromourproposedmodel-free
actor-critic. It is important to note that these methods do not explicitly consider the
informationgainassociatedwiththeinferredbeliefstateforactionselection.
Some other works, including Dreamer (Hafner et al., 2020) and PlaNet (Hafner et
al.,2019),learnbeliefstateand generative modelalongwiththeextrinsicrewardfunc-
tion model. These methods adopt a model-based learning approach that utilizes image
observations to maximize the expected long-term reward. However, unlike our infer-
ence model, Dreamer does not consider the belief state representation transition model
and, therefore, does not incorporate this representation into its variational distribution,
generative model, and reward model. As discussed in Sub-section 6.1.2, Dreamer can
be viewed as a special case of our proposed model-based unified actor-critic. A recent
evolutionofDreamer,calledDreamer-v2(Hafner,Lillicrap,Norouzi,&Ba,2021),has
been proposed; however, it is applicable only to discrete state spaces. It is important
to note that these model-based RL methods do not explicitly take into account the in-
formation gained regarding the inferred belief state for action selection. In contrast,
ourproposedmodel-basedunifiedactor-criticapproachencouragestheagenttoengage
in information-seeking exploration, enabling it to leverage that information to reduce
uncertaintyaboutthetruestatesoftheenvironment.
Additionally, X. Ma, Chen, Hsu, and Lee (2020) and Laskin, Srinivas, and Abbeel
(2020) focused on learning the generative model and belief state in pixel-based envi-
ronments with image-based observations, utilizing a form of consistency enforcement
known as contrastive learning between states and their corresponding observations.
However, contrastive learning poses distinct challenges that are beyond the scope of
our work. Nevertheless, our idea of using a hybrid model-based and model-free ap-
proachinourproposedunifiedactor-criticisinspiredbytheirwork.
Recently, there have been several RL works (Yin et al., 2021; Mazzaglia, Catal,
Verbelen, & Dhoedt, 2022; Klissarov, Islam, Khetarpal, & Precup, 2019) that have in-
troduced information gain as an intrinsic reward function for exploration. These meth-
ods involve inferring the belief state and learning a generative model by minimizing
the ELBO. They then use actor-critic RL methods to learn an optimal policy that max-
imizes both the expected long-term extrinsic reward and information gain. In these
approaches, the information gain term is added in an ad-hoc manner. In contrast, our
method provides a theoretical justification by extending the EFE within AIF to infinite
horizon POMDPs with continuous state, action, and observation spaces to incorporate
stochasticbeliefstate-actionpolicies.
Therefore,ourunifiedinferencemethodframesbeliefstateinference,generativemodel
learning, and the integration of information gain as an intrinsic reward function, as-
pectsthathavepreviouslybeenconsidered(partially)heuristicinthosestudies,undera
comprehensiveinterpretationoffreeenergyprinciple(K.Fristonetal.,2017).
7.2 AIF approaches for continuous space POMDP
As mentioned earlier, AIF approaches are mostly limited to discrete spaces or short,
finite-horizonPOMDPs. Thislimitationarisesfromthecomputationalexpenseofeval-
uating each plan based on its EFE and selecting the next action from the plan with the
lowest EFE. However, recent efforts have been made to extend AIF to POMDPs with
continuous observation and/or action spaces. These efforts include approaches that fo-
cus on deriving the optimal distribution over actions (i.e., state-action policy) instead
of plans or utilizing MCTS for plan selection. These advancements aim to address the
computationalchallengesassociatedwithscalingAIFtocontinuousspaces.
7.2.1 State-actionpolicylearning
Ueltzho¨ffer (2018) introduced a method where action a is sampled from a state-action
t
policy π(a |s ) instead of placing probabilities over a number of plans a˜. They min-
t t
imize the EFE by approximating its gradient with respect to π(a |s ). However, this
t t
approach requires knowledge of the partial derivatives of the observation given the ac-
tion,whichinvolvespropagatingthroughtheunknowntransitionmodelP(s |s ,a ).
τ+1 τ τ
Toaddressthischallenge,Ueltzho¨ffer(2018)usedablackboxevolutionarygeneticop-
timizer,whichisconsiderablysample-inefficient.
Later, Millidge (2020) proposed a similar scheme that includes the transition model
P(s |s ,a ). Theyheuristicallysettheoptimalstate-actionpolicyasaSoftmaxfunc-
τ+1 τ τ
tion of the EFE. They introduced a recursive scheme to approximate the EFE through
a bootstrapping method inspired by deep Q-learning (Mnih et al., 2013, 2016), which
is limited to discrete action spaces. However, our approach is specifically designed for
continuous action spaces. In addition, it is important to note that their approach as-
sumes that the current hidden state is fully known to the agent after inference and does
not consider the agent’s belief state for action selection in the EFE and policy. In con-
trast, our approach focuses on learning the belief state representation, which is utilized
foractionselectioninthebeliefstate-actionEFEandbeliefstate-actionpolicy. Wealso
provideananalyticaldemonstrationoftherecursioninourproposedbeliefstate-action
EFE and establish its convergence in Proposition 5.9 and Theorem 5.10. Furthermore,
(cid:16) (cid:17)
weprovetheexpressionπ∗(a |b ) = σ −G∗(bt,at) inTheorem5.7.
t t β
In a related context, K. Friston, Da Costa, Hafner, Hesp, and Parr (2021) explored a
recursive form of the EFE in a problem with discrete state and action spaces, consid-
ering it as a more sophisticated form of AIF. This approach involves searching over
sequences of belief states and considering the counterfactual consequences of actions
rather than just states and actions themselves. Building upon the work of Millidge
(2020), Mazzaglia et al. (2021) assumed that D [q(.|o ),P(.|s ,a )] = 0 and fo-
KL t t−1 t−1
cused on contrastive learning for the generative model and belief state in environments
withimage-basedobservations.
In a subsequent study, Da Costa et al. (2023) investigated the relationship between
AIF and RL methods in finite-horizon fully observable problems modeled as MDPs.
Theydemonstratedthattheoptimalplana˜∗ thatminimizestheEFEalsomaximizesthe
expectedlong-termrewardintheRLsetting,i.e.,argmin G(a˜)(o ) ⊂ argmax V(π)(s ).
a˜ AIF t π t
Shin,Kim,andHwang(2022)extendedAIFtocontinuousobservationspacesbyshow-
ing that the minimum of the EFE follows a recursive form similar to the Bellman op-
timality equation in RL. Based on this similarity, they derived a deterministic optimal
policy akin to deep Q-learning. However, their method is limited to discrete action
spaces and only provides a deterministic policy. In contrast, our approach is designed
for continuous action spaces and learns a stochastic policy, which enhances robustness
toenvironmentalchanges.
7.2.2 MCTSplanselection
Tschantz et al. (2020) extended AIF to continuous observation and action space prob-
lems by limiting the decision-making time horizon. They parametrized a probabil-
ity distribution over all possible plans and sampled multiple plans. Each sample was
weighted proportionately to its EFE value, and the mean of the sampling distribution
was then returned as the optimal plan a˜∗. However, this solution cannot capture the
(cid:16) (cid:17)
precise shape of the plans in AIF, as q∗(a˜) = σ −G(a˜)(o ) , and is primarily suitable
AIF t
forshorttimehorizonproblems.
Fountas et al. (2020) and Maisto et al. (2021) proposed an amortized version of MCTS
for plan selection. They consider the probability of choosing action a as the sum of
t
the probabilities of all the plans that begin with action a . However, these methods are
t
limitedtodiscreteactionspacesandarenotapplicabletocontinuousactionspaces.
8 Experimental results
This section describes the experimental design used in our study to evaluate the ef-
fectiveness of our unified inference approach for extending RL and AIF methods to
POMDP settings with continuous state, action, and observation spaces. Our principal
aim is to compare the performance of our approach with state-of-the-art techniques
proposed in the literature. We assess our approach across various tasks character-
izedbypartiallyobservablecontinuousspaces,whicharemodeledascontinuousspace
POMDPs.
Furthermore, by focusing on the exploration behaviors, we compare the information
gain intrinsic reward in our unified inference approach with other exploration methods
intheRLliterature.
Furthermore,wedelveintotheindividualcontributionsofdifferentcomponentswithin
our framework, the extrinsic reward, intrinsic reward, and entropy reward term, to the
overall performance. To achieve this, we carry out a series of ablation studies on these
tasks,analyzingtheeffectsofremovingormodifyingthesecomponents.
8.1 Comparativeevaluationonpartiallyobservablecontinuousspace
tasks
In this sub-section, we examine the effectiveness of our unified inference approach in
overcomingthelimitationsofexistingRLandAIFalgorithmswhenappliedtopartially
observableproblemswithcontinuousstate,action,andobservationspaces. Toevaluate
itsperformance,weconductedexperimentsundertwospecificconditionsofpartialob-
servability: (i) Partial state information (partial observations): In this setting, the agent
does not have access to complete information about the environment states. This con-
dition emulates scenarios where the agent has limited visibility into the true state of
the environment. (ii) Noisy state information (noisy observations): In this condition,
all observations received from the environment are noisy or contain inaccuracies. This
situation replicates real-world scenarios where observations are affected by noise or
errors,makingitchallengingtoaccuratelyestimatetheunderlyingstateoftheenviron-
ment.
Environments and tasks: The experimental evaluations encompassed four continu-
ous space Roboschool tasks (HalfCheetah, Walker2d, Hopper, and Ant) from the Py-
Bullet (Coumans & Bai, 2016), which is the replacement of the deprecated OpenAI
Roboschool (Brockman et al., 2016). These tasks have high-dimensional state spaces
characterized by quantities such as positions, angles, velocities (angular and linear),
and forces. Each episode in these tasks terminates on failure (e.g. when the hopper
or walker falls over). The choice of these environments is driven by two key factors:
(i) they offer challenging tasks with high-dimensional state spaces and sparse reward
functions, and (ii) recent efforts have been made to enhance the sample efficiency of
model-free and model-based RL methods in the partially observable variants of these
benchmarks,providingsuitablebaselinesforcomparison.
To create partially observable versions of these tasks, we made modifications to the
taskenvironments. Thefirstmodificationrestrictstheagent’sobservationstovelocities
only, transforming the tasks into partial observation tasks. This modification proposed
byHanetal.(2020)isrelevantinreal-worldscenarioswhereagentsmayestimatetheir
speed but not have direct access to their position. For the noisy observation versions of
thetasks,weaddedzero-meanGaussiannoisewithastandarddeviationofσ = 0.05to
theoriginalstatesreturnedfromtheenvironment. Thismodificationallowsustosimu-
laterealisticsensornoiseintheenvironment. Furtherdetailsofthemodificationsmade
to create partially and noisy observable environments are provided in Appendix F. We
denote the partial observation modification and noisy observation modification of the
four tasks as {Hopper, Ant, Walker-2d, Cheetah}-{P} and {Hopper, Ant, Walker-2d,
Cheetah}-{N},respectively.
Baselines: To evaluate the potential of the proposed unified inference framework in
extending MDP-based RL methods to POMDPs, we compare the performance of our
proposed model-free G-SAC and model-based G-Dreamer algorithms on the {Hopper,
Ant,Walker,Cheetah}-{P,N}taskswiththefollowingstate-of-the-artmodel-basedand
model-freealgorithmsfromtheliterature:
• SAC:SAC(Haarnoja,Zhou,Abbeel,&Levine,2018)isamodel-freeactor-critic
RL algorithm designed for MDPs. We include experiments showing the perfor-
mance of SAC based on true states (referred to as State-SAC) as an upper bound
on performance. The State-SAC serves as an oracle approximation representing
an upper bound on the performance that any POMDP method should strive to
achieve.
• VRM:VRM(Hanetal.,2020)isaPOMDP-basedactor-criticRLalgorithmthat
learns a generative model, infers the belief state, and constructs the state value
function in a model-free manner. By comparing our approach with VRM, we
can evaluate the impact of the information gain exploration term on model-free
learninginPOMDPsettings.
• Dreamer: Dreamer(Hafneretal.,2020)isamodel-basedactor-criticRLmethod
designed for image observations. It learns a generative model, infers the belief
state, and learns the state value function through imagined trajectories using the
generative model. To compare our approach with Dreamer, we made some mod-
ifications to its implementation (see Appendix G). Despite these modifications,
we expect the comparison to demonstrate the effects of the information gain ex-
plorationtermonmodel-basedlearninginPOMDPsettings.
• Recurrent Model-Free: Recurrent Model-Free (Ni et al., 2022) is a model-free
memory-basedRLalgorithmforPOMDPs. Itexploresdifferentarchitectures,hy-
perparameters,andinputconfigurationsfortherecurrentactorandrecurrentvalue
functionacrossRoboschoolenvironmentsandselectsthebest-performingconfig-
uration. We consider Recurrent Model-Free as a baseline to compare the perfor-
manceofmemory-basedapproacheswithbeliefstateinference-basedmethodsin
partiallyobservableenvironments.
Evaluation metrics: The performance of RL and AIF algorithms can be evaluated
using multiple metrics. In this sub-section, we assess the performance based on the
commonly used metric of cumulative extrinsic reward (return) after 1 million steps.
This metric quantifies the sum of all extrinsic rewards obtained by the agent up to that
point, providing an indication of the agent’s overall success in accomplishing its task
within the given time frame. Additionally, we evaluate the sample efficiency of each
baselinealgorithmbymeasuringthenumberofstepsrequiredforthemtoreachthebest
performanceachievedat1millionsteps. Itshouldbenotedthatthenumberofenviron-
mentstepsineachepisodeisvariable,dependingonthetermination.
Table 3: The mean of the return on Roboschool tasks (partial and noisy observations)
averaged at the last 20% of the total 1 million environment steps across 5 seeds. The
model-freebaselineState-SACisusedasareferencefortheperformance.
Task State- SAC Dreamer VRM G- G- Recurrent
SAC SAC Dreamer Model-Free
HalfCheetah-P 2994 193 1926 2938 3859 3413 1427
HalfCheetah-N 2994 −512 1294 2170 3655 3226 408
Hopper-P 2434 724 2043 1781 2468 2766 1265
Hopper-N 2434 466 1674 1343 2308 2448 771
Ant-P 3394 411 847 1503 2743 2325 994
Ant-N 3394 328 762 1256 2545 1996 367
Walker2d-P 2225 305 821 761 1837 2173 210
Walker2d-N 2225 123 578 424 1698 1956 116
Experimental setup: We implemented SAC, VRM, and Recurrent Model-Free us-
ing their original implementations on the Hopper, Ant, Walker, Cheetah-P,N tasks.
For state-SAC, we utilized the results from (Raffin, Kober, & Stulp, 2022). Regard-
ing Dreamer, we mostly followed the implementation described in the original pa-
per (Hafner et al., 2020). However, there was one modification: since their work
employed pixel observations, we replaced the convolutional neural networks (CNNs)
andtransposedCNNswithtwo-layermulti-layerperceptrons(MLPs)consistingof256
units each for the variational posterior and likelihood model learning. Feed-forward
neural networks were used to represent the actors (policies) and critics (state-action
valuefunctionsorthenegativeofbeliefstate-actionEFE).Toensureafaircomparison,
we maintained identical hyperparameters for the actor and critic models of Dreamer,
G-SAC,G-Dreamer,andVRM.
The same set of hyperparameters was used for both the noisy observation and partial
observation versions of each task. We trained each algorithm for a total of 1 million
environmentstepsoneachtaskandconductedeachexperimentwith5differentrandom
seeds. Foradditionalinformationregardingthemodelarchitectures,pleaserefertoAp-
pendixG.
Results: Table 3 provides a summary of the results, displaying the mean return aver-
aged over the last 20% of the total 1 million environment steps across the five random
seeds. The complete learning curves can be found in Appendix I. We will now analyze
thequantitativeresultsinthefollowing:
(i) Unified inference model successfully generalizes model-free and model-based RL to
POMDPs. As expected, SAC encountered difficulties in solving the tasks with partial
andnoisyobservationsduetoitsMDP-basedimplementation. Incontrast,ourproposed
G-SAC algorithm demonstrated superior performance compared to SAC. Furthermore,
while Dreamer and VRM are regarded as state-of-the-art methods for POMDP tasks,
G-Dreamer and G-SAC consistently outperformed them in all scenarios. This empha-
sizes the advantages of leveraging the belief state representation in both the actor and
critic, along with the information gain term in our G-Dreamer and G-SAC algorithms.
The number of steps required for Dreamer and VRM to match the performance of G-
Dreamer and G-SAC at the end of 1 million steps was significantly higher, indicating
that the information gain intrinsic term in G-SAC and G-Dreamer enhances sample ef-
ficiency. It should be noted that while G-SAC and G-Dreamer outperform Dreamer
and VRM algorithms in terms of final performance and sample efficiency in both the
partial observation and noisy observation cases, they achieve these improvements with
comparablecomputationalrequirementstothecomparedalgorithms.
Remarkably, both our proposed methods, G-SAC and G-Dreamer, performed on par
with, or even surpassed, the oracle State-SAC framework, which has access to the true
stateoftheenvironment. Thisfurtherconfirms: 1)Effectivenessoftheperceptualinfer-
enceandlearningmethodforinferringthebeliefstate-conditionedvariationalposterior
q(s |o ,b )andthebeliefstategenerativemodelP(o ,s ,b |a ,b ),aswellasthe
t t t−1 t t t t−1 t−1
belief state representation model for learning a belief representation h , used in our
t
approaches. 2) Effectiveness of the information-seeking exploration facilitated by the
intrinsicrewardterminourunifiedobjectivefunctionG(π)(b ).
t
(ii) Belief state inference-based approaches are more robust than memory-based base-
line given noisy observations. Belief state inference-based approaches, such as VRM,
Dreamer, and G-SAC, consistently outperform the memory-based baseline, Recurrent
Model-Free, in both partial observations and noisy observations settings. This supe-
riority can be attributed to the ability of VRM, Dreamer, and G-SAC to encode the
observations and actions history into the variational posterior, enabling more effective
encoding of underlying states compared to Recurrent Model-Free when dealing with
velocity observations or noisy observations. The performance gap between these ap-
proaches becomes more pronounced in the case of noisy observations, highlighting the
advantage of inferring the belief state for use in the actor and critic, especially in the
presence of observation noise. This advantage is linked to the complexity inherent in
noisy environments, where various observations may correspond to a single underly-
ing state due to noise interference. Belief state methods address this uncertainty by
maintainingaprobabilitydistributionoverpossiblestates,allowingthemtoconsolidate
informationfrommultiplenoisyinputsandupdatetheirbeliefsaccordingly. Thus,even
when faced with differing noisy observations that might relate to the same state, belief
state methods can assimilate this uncertainty and assign probabilities to each possible
state based on the collected data. Conversely, memory-based methods often falter un-
der such uncertainty. These methods typically depend on storing and processing an
extensive history of observations and actions, which can become cumbersome in noisy
settings where observations might be ambiguous or misleading. Lacking a structured
waytohandleandupdateuncertainty,memory-basedapproachesmaystruggletodiffer-
entiate between observations leading to the same underlying state, potentially resulting
ininferiordecision-making.
Itisnoteworthythatwhilememory-basedmethodslikeRecurrentModel-Freetypically
result in higher memory consumption due to storing extensive past observations and
actions,beliefstateinferencemethodssuchasVRM,G-SAC,andG-Dreameroftenre-
quire less memory. However, they may necessitate additional computational resources
for inference and learning the generative model. Nonetheless, as elaborated in Ap-
pendixH,thecomputationalburdeninducedbyongoingbeliefstateinferenceandgen-
erative model learning in VRM and our proposed G-SAC and G-Dreamer frameworks
is generally lower than that of processing a sequence of past actions and observations
through an RNN, as seen in Recurrent Model-Free. This contrast is especially no-
ticeable in large or infinite time horizon problems with high-dimensional action and
observationspaces.
(iii) Maximizing expected long-term information gain improves robustness to the noisy
observations. While the performance of VRM and Dreamer degraded from the par-
tial observation setting to the noisy observation setting, G-SAC and G-Dreamer were
able to maintain comparable performance in the presence of observation noise. This
robustness to observation noise can be attributed to the KL-divergence term in the in-
trinsic information-seeking term, as highlighted by Hafner et al. (2022) in the context
ofdivergenceminimizationframeworks.
The results presented in this sub-section highlight the effectiveness of the proposed
unified inference algorithm in various aspects: (i) It successfully generalizes MDP
actor-critic methods to the POMDP setting, allowing for more effective exploration
and learning under partial observability. (ii) It outperforms memory-based approaches
in scenarios with noisy observations, indicating the advantage of leveraging the belief
staterepresentationinhandlingobservationnoise. (iii)Theinclusionoftheinformation
gain intrinsic term into the generalized actor-critic methods improves their robustness
tonoisyobservations.
8.2 Comparative evaluation of exploration methods
In this sub-section, we evaluate the performance of our unified inference approach,
employing information gain as an intrinsic reward for exploration, in contrast to RL
methods incorporating alternative exploratory intrinsic rewards. We particularly inves-
tigateitsefficacyacrossdeterministicandstochasticpartiallyobservableenvironments.
To facilitate this analysis, we utilize extrinsic reward-free agents dedicated solely to
exploration. This emphasis enables us to comprehensively scrutinize and highlight the
exploratorybehaviorsexhibitedbytheagents.
Environments and tasks: We conduct a series of experiments on a partially observ-
able variant of the MountainCarContinuous-v0 environment, where only the velocity
is observable. The problem’s state space is continuous and includes the car’s position
and velocity along the horizontal axis. The action space is one-dimensional, allowing
control over the force applied to the car for movement, and the transition function is
deterministic. We chose the MountainCarContinuous-v0 task as it is relatively easy to
solve when extrinsic rewards are available. Thus, by considering the reward-free case,
we can emphasize the exploration challenge and evaluate the effectiveness of different
explorationmethods. AsstatedinSub-section7.1.1,explorationbasedontheprediction
error of a state forward model is sensitive to the inherent stochasticity of the environ-
ment (Burda et al., 2018). Therefore, we also performed an experiment on the same
task but with a stochastic transition function. We used the stochastic version of the en-
vironmentintroducedin(Mazzaglia,Catal,etal.,2022),whichaddsaone-dimensional
state referred to as the NoisyState, and a one-dimensional action ranging from [−1,1]
that acts as a remote for the NoisyState. When this action’s value is higher than 0,
the remote is triggered, updating the NoisyState value by sampling uniformly from the
[−1,1]interval.
Baselines: We compare G-SAC against the following RL frameworks that incorporate
exploratorytermsasintrinsicrewardsinSAC:
• ICM:ICM(Pathaketal.,2017)isapredictionerror-basedRLalgorithmthatgen-
erates intrinsic rewards through a state forward-backward dynamics model. The
main source of intrinsic reward is the prediction error from the forward model.
The backward transition model supports state feature learning by reconstructing
thepreviousstate’sfeatures.
• RND: RND (Burda et al., 2018) is a prediction error-based RL method where
state features are learned using a fixed, randomly initialized neural network. In-
trinsic rewards are calculated based on the prediction errors between the next
state features and the outputs of a distillation network. This distillation network
is continuously trained to emulate the outputs of the randomly initialized feature
network.
• VIME VIME (Houthooft et al., 2016) employs a Bayesian neural network to
learn the forward model. It utilizes intrinsic rewards based on the information
gainabouttheparametersoftheBayesiannetwork. Theserewardsarequantified
by the change in information before and after updating the network with data
fromnewinteractions.
Performancemetrics: Wemeasureexplorationabilitydirectlybycalculatinganagent’s
environment coverage. Following the approach in (Mazzaglia, Catal, et al., 2022), we
discretize the state space into 100 bins and evaluate the coverage percentage of the
number of bins explored. An agent visiting a certain bin corresponds to the agent suc-
cessfully accomplishing a task that requires reaching that particular area of the state
space. Hence, it is crucial that a good exploration method can explore as many bins as
possible.
Experimental Setup: For G-SAC, we use the same model architectures and hyper-
parameters described in Sub-section 8.1 of the main manuscript. Since ICM, RND,
and VIME were originally developed for fully observable environments, we adapted
them for our partially observable setting of MountainCarContinuous-v0. Specifically,
the forward and backward models are reconstructed based on observations rather than
fully observable states. Consequently, the state-action policies are changed to history-
dependent policies, as the observations no longer have the Markovian property. This
adaptation allows both our method and these three baselines to incorporate exploratory
intrinsicrewards,enablingafaircomparison.
WethentrainG-SACandthebaselinesonthedeterministicandstochasticpartiallyob-
servableMountainCarContinuous-v0foratotalof100episodes. Anepisodeterminates
whentheagentreachesthegoalortheepisodelengthexceeds1000steps.
Results: Fig. 2 presents training curves averaged over 10 different random seeds. The
resultsshowthatmethodsutilizingintrinsicrewardsbasedoninformationgain,specif-
ically G-SAC and VIME, learn significantly faster in deterministic environments. This
acceleratedlearningsuggeststhattheirexplorationmechanismsaremoreeffectivethan
thoseofICMandRND,whichrelyonpredictionerror-basedexploration. Thesuperior-
ityofinformationgain-basedmethodslargelystemsfromtheireffectivenessinhandling
statesimilaritieswithincontinuousstatespaces. Unlikepredictionerror-basedmethods
that struggle to differentiate between subtly different states, information gain methods
assess the entire distribution of possible states, enabling more precise and meaningful
exploration even in the presence of similar states. Notably, G-SAC performs slightly
better than VIME because it directly assesses uncertainty reduction based on inferred
states, while VIME considers uncertainty reduction based on observations, which in-
cludethecar’svelocitybutnotitsposition. Therefore,VIMEcannoteffectivelycapture
uncertaintyreductioninthecar’spositionforexploration.
Moreover, although VIME explores less than G-SAC, both methods demonstrate re-
silience to stochasticity. In contrast, the performance of ICM and RND is significantly
compromised by randomness, with ICM being the most adversely affected. It is well-
documented that intrinsic motivation strategies based on the prediction error of a for-
ward model are vulnerable to the inherent stochasticity of the environment (Burda et
al.,2018). Thisdegradationis largelyduetotheinherent stochasticityofenvironments
whereoutcomesvaryunpredictablyfollowingthesameaction,causingpersistentlyhigh
prediction errors and leading to misguided exploration efforts. This highlights the vul-
nerabilityofpredictionerror-basedintrinsicmotivationstrategiesinstochasticsettings.
In conclusion, information gain-based methods like G-SAC and VIME, which re-
spectivelyassesstheentiredistributionofpossiblelatentstatesandparametersofneural
networks generating next states rather than just the most likely ones, demonstrate ro-
bustness against subtle changes in an agent’s state and the variability introduced by
stochastic conditions. This comprehensive consideration of potential states and param-
eters significantly improves their effectiveness, especially in stochastic environments
withcontinuousstateandactionspaces.
(a)Partiallyobservabledeterministicmountain (b) Partially observable stochastic mountain
car car
Fig. 2. The average state-space coverage in terms of percentage of bins visited by the
agents for deterministic and stochastic partially observable MountainCarContinuous-
v0. The more state space coverage in an episode, the better the agent explores the
environmentandthusperformsinthatepisode.
8.3 Ablation studies
In this sub-section, we conduct a comprehensive ablation study on the partial obser-
vation variants of the four Roboschool tasks discussed in Sub-section 8.1, namely
{Hopper, Ant, Walker-2d, Cheetah}-{P}. The primary objective of this study is to
gain a deeper understanding of the contribution of each individual component in the
proposed unified actor-critic framework. We evaluate the performance based on the
averagereturnacross5differentrandomseeds.
8.3.1 Stochasticpolicyversusdeterministicpolicy
TheproposedG-SACframeworklearnsanoptimalstochasticpolicybyminimizingthe
loss functions corresponding to the belief state-action EFE update step (Eq. (66)) and
policy update step (Eq. (67)), with the policy entropy term included in both of these
loss functions. In the belief state-action EFE update step, the entropy term encourages
exploration by reducing the belief state-action EFE in regions of the state space that
leadtohigh-entropybehavior. Inthepolicyupdatestep,theentropytermhelpsprevent
prematurepolicyconvergence.
To assess the impact of policy stochasticity (policy entropy term) on G-SAC’s perfor-
mance, we compare it to an algorithm called G-DDPG, obtained by setting β = 0 in
G-SAC.G-DDPGisageneralizationoftheMDP-basedDeepDeterministicPolicyGra-
dient (DDPG) algorithm (Silver et al., 2014) to POMDPs, where a deterministic policy
islearnedbyremovingthepolicyentropytermfromthebeliefstate-actionEFE.
Experimental setup: For G-DDPG, we utilize identical hyperparameters and network
architecturesasthoseemployedinG-SAC.WetrainG-DDPGonHopper,Ant,Walker-
2d,Cheetah-Pforatotalof1milliontimestepsusing5differentrandomseeds.
Results: Fig. 3 presents the performance comparison between G-SAC and G-DDPG.
The results indicate that G-DDPG suffers from premature convergence due to the ab-
sence of theentropy term. Furthermore, G-DDPG exhibits a higherstandard deviation,
resulting in reduced stability compared to G-SAC. This finding suggests that learn-
ing a stochastic policy with policy entropy maximization in POMDPs with uncertain
environmental states can significantly enhance training stability, particularly for more
challengingtaskswherehyperparametertuningcanbedifficult.
8.3.2 Model-basedactor-criticvshybridactor-critic
Whenα = 1inthehybridunifiedactor-criticmethod,thecriticlossfunctionLHybrid(ψ)
π¯
combinesthecriticlossfunctionsofG-SAC(LG-SAC(ψ))andG-Dreamer(LG-Dreamer(ψ))
G G
(a)HalfCheetah-P (b)Hopper-P
(c)Ant-P (d)Walker2d-P
Fig.3. AblationstudycomparingtheaveragereturnofG-SACandG-DDPGalgorithms
acrossthepartialobservationversionofRoboschooltasks.
using a scaling factor c. We adopt a hybrid G-Dreamer-SAC approach by setting c to
1 in LHybrid(ψ). To assess the influence of the G-SAC component on the performance
π¯
of the hybrid G-Dreamer-SAC, we compare its performance with that of G-Dreamer,
whichcanbeconsideredaspecialcaseofthehybridunifiedactor-criticwhenc = 0.
Results: Fig.4presentsaperformancecomparisonbetweenG-DreamerandG-Dreamer-
SAC. G-Dreamer-SAC outperforms G-Dreamer on HalfCheetah-P and Ant-P, while
performingonparwithG-DreameronHopper-PandWalker2d-P.Thisdiscrepancycan
be attributed to the higher number of hidden values in the state vector that need to be
inferredsolelyfromvelocitiesinHalfCheetah-PandAnt-P,comparedtoHopper-Pand
Walker2d-P. Accurately learning the belief state and generative model becomes more
challenging when a larger number of states are unknown. Therefore, relying solely on
the learned generative model for actor and critic learning in HalfCheetah-P and Ant-
P leads to inaccurate data trajectories. However, by utilizing ground-truth trajectories
from the replay buffer, the G-SAC component of G-Dreamer-SAC can provide accu-
rate data to compensate for the compositional errors of the generative models. As a
(a)HalfCheetah-P (b)Hopper-P
(c)Ant-P (d)Walker2d-P
Fig. 4. Ablation study comparing the average return of the hybrid G-Dreamer-SAC
algorithm and the model-based G-Dreamer algorithm across four partial observation
versionsofRoboschooltasks.
result, G-Dreamer-SAC benefits from the sample efficiency of model-based learning
while maintaining the robustness to complex observations, which are characteristic of
model-freelearning.
9 Conclusion and Perspectives
In this paper, we extended the EFE formulation to stochastic Markovian belief state-
action policies, which allowed for a unified objective function formulation encom-
passing both exploitation of extrinsic rewards and information-seeking exploration in
POMDPs. We then introduced a unified policy iteration framework to optimize this
objective function and provided proof of its convergence to the optimal solution. Our
proposed unified policy iteration framework not only generalized existing RL and AIF
algorithms but also revealed a theoretical relationship between them, showing that the
belief state EFE can be interpreted as a negative state value function. Additionally, our
method successfully scaled up AIF to tasks with continuous state and action spaces
and enhanced actor-critic RL algorithms to handle POMDPs while incorporating an
inherent information-seeking exploratory term. We evaluated our approach on high-
dimensionalRoboschooltaskswithpartialandnoisyobservations,andourunifiedpol-
icy iteration algorithm outperformed recent alternatives in terms of expected long-term
reward, sample efficiency, and robustness to estimation errors, while requiring com-
parable computational resources. Furthermore, our experimental results indicated that
the proposed information-seeking exploratory behavior is effective in guiding agents
towards their goals even in the absence of extrinsic rewards, making it possible to op-
erate in reward-free environments without the need for external supervisors to define
task-specificrewardfunctions.
However,therearestillchallengestoovercome,particularlyinscalingAIFtohigh-
dimensional environments, such as those based on images. Accurate variational pos-
terior and generative models are required to reconstruct observations in detail. One
potential solution is to incorporate a state-consistency loss that enforces consistency
between states and their corresponding observations, which has shown promise in self-
supervised learning methods (He, Fan, Wu, Xie, & Girshick, 2020; Grill et al., 2020).
We plan to explore the combination of our unified inference agents with such learning
methodsinRL.
Anotherexcitingdirectionforfutureresearchistoinvestigatetheimpactofinformation-
seeking exploration in a multi-task setting, where initially explored agents may benefit
fromtransferringknowledgeacrosstasks.
References
Bellman, R. (1952). On the theory of dynamic programming. Proceedings of
the National Academy of Sciences of the United States of America, 38(8),
716.
Boyd, S. P., & Vandenberghe, L. (2004). Convex optimization. Cambridge
universitypress.
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J.,
&Zaremba,W. (2016). Openaigym. arXivpreprintarXiv:1606.01540.
Burda, Y., Edwards, H., Storkey, A., & Klimov, O. (2018). Exploration by
randomnetworkdistillation. arXivpreprintarXiv:1810.12894.
Chatterjee, K., Chmelik, M., & Tracol, M. (2016). What is decidable about
partially observable markov decision processes with ω-regular objectives.
JournalofComputerandSystemSciences,82(5),878–911.
Choi,J.,Guo,Y.,Moczulski,M.,Oh,J.,Wu,N.,Norouzi,M.,&Lee,H. (2018).
Contingency-aware exploration in reinforcement learning. arXiv preprint
arXiv:1811.01483.
Chung,J.,Kastner,K.,Dinh,L.,Goel,K.,Courville,A.C.,&Bengio,Y.(2015).
A recurrent latent variable model for sequential data. Advances in neural
informationprocessingsystems,28.
Coumans, E., & Bai, Y. (2016). Pybullet, a python module for physics simula-
tionforgames,roboticsandmachinelearning.(2016). URLhttp://pybullet.
org.
Da Costa, L., Sajid, N., Parr, T., Friston, K., & Smith, R. (2023). Reward max-
imization through discrete active inference. Neural Computation, 35(5),
807–852.
Dai, T., Du, Y., Fang, M., & Bharath, A. A. (2022). Diversity-augmented
intrinsicmotivationfordeepreinforcementlearning.Neurocomputing,468,
396–406.
Dong, Y., Zhang, S., Liu, X., Zhang, Y., & Shen, T. (2021). Variance aware
reward smoothing for deep reinforcement learning. Neurocomputing, 458,
327–335.
Dufour, F., & Prieto-Rumeau, T. (2012). Approximation of markov decision
processes with general state space. Journal of Mathematical Analysis and
applications,388(2),1254–1267.
Dufour,F.,&Prieto-Rumeau,T. (2013). Finitelinearprogrammingapproxima-
tions of constrained discounted markov decision processes. SIAM Journal
onControlandOptimization,51(2),1298–1324.
Fountas, Z., Sajid, N., Mediano, P. A., & Friston, K. (2020). Deep active infer-
enceagentsusingmonte-carlomethods. arXivpreprintarXiv:2006.04176.
Friston,K.,DaCosta,L.,Hafner,D.,Hesp,C.,&Parr,T. (2021). Sophisticated
inference. NeuralComputation,33(3),713–763.
Friston,K.,FitzGerald,T.,Rigoli,F.,Schwartenbeck,P.,&Pezzulo,G. (2017).
Activeinference: aprocesstheory. Neuralcomputation,29(1),1–49.
Friston, K., Mattout, J., & Kilner, J. (2011). Action understanding and active
inference. Biologicalcybernetics,104(1),137–160.
Friston, K., Samothrakis, S., & Montague, R. (2012). Active inference and
agency: optimal control without cost functions. Biological cybernetics,
106(8),523–541.
Friston, K. J., Daunizeau, J., Kilner, J., & Kiebel, S. J. (2010). Action and
behavior: a free-energy formulation. Biological cybernetics, 102(3), 227–
260.
Gregor,K.,JimenezRezende,D.,Besse,F.,Wu,Y.,Merzic,H.,&vandenOord,
A. (2019). Shaping belief states with generative environment models for
rl. AdvancesinNeuralInformationProcessingSystems,32.
Grill, J.-B., Strub, F., Altche´, F., Tallec, C., Richemond, P., Buchatskaya, E., ...
others (2020). Bootstrapyourownlatent-anewapproachtoself-supervised
learning. Advances in neural information processing systems, 33, 21271–
21284.
Haarnoja, T., Zhou, A., Abbeel, P., & Levine, S. (2018). Soft actor-critic:
Off-policymaximumentropydeepreinforcementlearningwithastochastic
actor. InInternationalconferenceonmachinelearning(pp.1861–1870).
Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., ... oth-
ers (2018). Soft actor-critic algorithms and applications. arXiv preprint
arXiv:1812.05905.
Hafner, D., Lillicrap, T., Ba, J., & Norouzi, M. (2020). Dream to control:
Learning behaviors by latent imagination. In International conference on
learningrepresentations.
Hafner,D.,Lillicrap,T.,Fischer,I.,Villegas,R.,Ha,D.,Lee,H.,&Davidson,J.
(2019). Learninglatentdynamicsforplanningfrompixels. InInternational
conferenceonmachinelearning(pp.2555–2565).
Hafner, D., Lillicrap, T. P., Norouzi, M., & Ba, J. (2021). Mastering atari with
discrete world models. In International conference on learning represen-
tations.
Hafner, D., Ortega, P. A., Ba, J., Parr, T., Friston, K., & Heess, N.
(2022). Action and perception as divergence minimization. arXiv preprint
arXiv:2009.01791.
Haklidir, M., & Temeltas¸, H. (2021). Guided soft actor critic: A guided deep
reinforcement learning approach for partially observable markov decision
processes. IEEEAccess,9,159672–159683.
Han, D., Doya, K., & Tani, J. (2020). Variational recurrent models for solving
partially observable control tasks. In International conference on learning
representations.
Hausknecht, M., & Stone, P. (2015). Deep recurrent q-learning for partially
observablemdps. In2015aaaifallsymposiumseries.
He,K.,Fan,H.,Wu,Y.,Xie,S.,&Girshick,R. (2020). Momentumcontrastfor
unsupervised visual representation learning. In Proceedings of the ieee/cvf
conferenceoncomputervisionandpatternrecognition(pp.9729–9738).
Heess, N., Hunt, J. J., Lillicrap, T. P., & Silver, D. (2015). Memory-based
controlwithrecurrentneuralnetworks. arXivpreprintarXiv:1512.04455.
Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural
computation,9(8),1735–1780.
Houthooft, R., Chen, X., Duan, Y., Schulman, J., De Turck, F., & Abbeel, P.
(2016). Vime: Variational information maximizing exploration. Advances
inneuralinformationprocessingsystems,29.
Igl, M., Zintgraf, L., Le, T. A., Wood, F., & Whiteson, S. (2018). Deep varia-
tional reinforcement learning for pomdps. In International conference on
machinelearning(pp.2117–2126).
Kingma, D. P., & Welling, M. (2013). Auto-encoding variational bayes. arXiv
preprintarXiv:1312.6114.
Klissarov, M., Islam, R., Khetarpal, K., & Precup, D. (2019). Variational
state encoding as intrinsic motivation in reinforcement learning. In Task-
agnostic reinforcement learning workshop at proceedings of the interna-
tionalconferenceonlearningrepresentations(Vol.15,pp.16–32).
Kochenderfer, M. J. (2015). Decision making under uncertainty: theory and
application. MITpress.
Krishnamurthy, V. (2015). Structural results for partially observed markov de-
cisionprocesses. arXivpreprintarXiv:1512.03873.
Lanillos, P., Meo, C., Pezzato, C., Meera, A. A., Baioumy, M., Ohata, W., ...
others (2021). Aif in robotics and artificial agents: Survey and challenges.
arXivpreprintarXiv:2112.01871.
Laskin, M., Srinivas, A., & Abbeel, P. (2020). Curl: Contrastive unsupervised
representations for reinforcement learning. In International conference on
machinelearning(pp.5639–5650).
Lee, A. X., Nagabandi, A., Abbeel, P., & Levine, S. (2020). Stochastic la-
tentactor-critic: Deepreinforcementlearningwithalatentvariablemodel.
AdvancesinNeuralInformationProcessingSystems,33,741–752.
Leibfried, F., Pascual-Diaz, S., & Grau-Moya, J. (2019). A unified bellman
optimality principle combining reward maximization and empowerment.
In Advances in neural information processing systems (Vol. 32). Curran
Associates,Inc.
Likmeta, A., Sacco, M., Metelli, A. M., & Restelli, M. (2022). Directed ex-
plorationviauncertainty-awarecritics. InDecisionawarenessinreinforce-
mentlearningworkshopaticml.
Ma,X.,Chen,S.,Hsu,D.,&Lee,W.S. (2020). Contrastivevariationalmodel-
based reinforcement learning for complex observations. In In proceedings
ofthe4thconferenceonrobotlearning,virtualconference.
Ma, X., Chen, S., Hsu, D., & Lee, W. S. (2021). Contrastive variational re-
inforcement learning for complex observations. In Conference on robot
learning(pp.959–972).
Ma, Y., Zhao, T., Hatano, K., & Sugiyama, M. (2016). An online policy gra-
dient algorithm for markov decision processes with continuous states and
actions. NeuralComputation,28(3),563–593.
Madani, O., Hanks, S., & Condon, A. (1999). On the undecidability of proba-
bilistic planning and infinite-horizon partially observable markov decision
problems. InAaai/iaai(pp.541–548).
Maddox,W.J.,Izmailov,P.,Garipov,T.,Vetrov,D.P.,&Wilson,A.G. (2019).
A simple baseline for bayesian uncertainty in deep learning. Advances in
NeuralInformationProcessingSystems,32.
Maisto,D.,Gregoretti,F.,Friston,K.,&Pezzulo,G. (2021). Activetreesearch
inlargepomdps. arXivpreprintarXiv:2103.13860.
Malekzadeh,P.,Salimibeni,M.,Hou,M.,Mohammadi,A.,&Plataniotis,K.N.
(2022). Akf-sr: Adaptive kalman filtering-based successor representation.
Neurocomputing,467,476–490.
Malekzadeh,P.,Salimibeni,M.,Mohammadi,A.,Assa,A.,&Plataniotis,K.N.
(2020). Mm-ktd: multiple model kalman temporal differences for rein-
forcementlearning. IEEEAccess,8,128716–128729.
Mavrin, B., Yao, H., Kong, L., Wu, K., & Yu, Y. (2019). Distributional rein-
forcementlearningforefficientexploration. InInternationalconferenceon
machinelearning(pp.4424–4434).
Mazzaglia, P., Catal, O., Verbelen, T., & Dhoedt, B. (2022). Curiosity-driven
explorationvialatentbayesiansurprise. InProceedingsoftheaaaiconfer-
enceonartificialintelligence(Vol.36,pp.7752–7760).
Mazzaglia, P., Verbelen, T., C¸atal, O., & Dhoedt, B. (2022). The free energy
principle for perception and action: A deep learning perspective. Entropy,
24(2),301.
Mazzaglia, P., Verbelen, T., & Dhoedt, B. (2021). Contrastive active inference.
AdvancesinNeuralInformationProcessingSystems,34,13870–13882.
Meng, L., Gorbet, R., & Kulic´, D. (2021). Memory-based deep reinforcement
learning for pomdps. In 2021 ieee/rsj international conference on intelli-
gentrobotsandsystems(iros)(pp.5619–5626).
Millidge,B. (2020). Deepactiveinferenceasvariationalpolicygradients. Jour-
nalofMathematicalPsychology,96,102348.
Millidge, B., Tschantz, A., & Buckley, C. L. (2021). Whence the expected free
energy? NeuralComputation,33(2),447–482.
Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., ...
Kavukcuoglu, K. (2016). Asynchronous methods for deep reinforce-
mentlearning. InInternationalconferenceonmachinelearning(pp.1928–
1937).
Mnih,V.,Kavukcuoglu,K.,Silver,D.,Graves,A.,Antonoglou,I.,Wierstra,D.,
& Riedmiller, M. (2013). Playing atari with deep reinforcement learning.
arXivpreprintarXiv:1312.5602.
Montufar, G., Ghazi-Zahedi, K., & Ay, N. (2015). Geometry and determin-
ism of optimal stationary control in partially observable markov decision
processes. arXivpreprintarXiv:1503.07206.
Ni, T., Eysenbach, B., & Salakhutdinov, R. (2022). Recurrent model-free rl
can be a strong baseline for many pomdps. In International conference on
machinelearning(pp.16691–16723).
Nian, X., Irissappane, A. A., & Roijers, D. (2020). Dcrac: Deep conditioned
recurrentactor-criticformulti-objectivepartiallyobservableenvironments.
InProceedingsofthe19thinternationalconferenceonautonomousagents
andmultiagentsystems(pp.931–938).
Ogishima,R.,Karino,I.,&Kuniyoshi,Y. (2021). Reinforcedimitationlearning
byfreeenergyprinciple. arXivpreprintarXiv:2107.11811.
Oh, C., & Cavallaro, A. (2019). Learning action representations for self-
supervisedvisualexploration. In2019internationalconferenceonrobotics
andautomation(icra)(pp.5873–5879).
Okuyama,T.,Gonsalves,T.,&Upadhay,J. (2018). Autonomousdrivingsystem
based on deep q learnig. In 2018 international conference on intelligent
autonomoussystems(icoias)(pp.201–205).
Pascanu, R., Mikolov, T., & Bengio, Y. (2013). On the difficulty of training
recurrentneuralnetworks.InInternationalconferenceonmachinelearning
(pp.1310–1318).
Paternain, S., Bazerque, J. A., & Ribeiro, A. (2020). Policy gradient for con-
tinuing tasks in non-stationary markov decision processes. arXiv preprint
arXiv:2010.08443.
Pathak, D., Agrawal, P., Efros, A. A., & Darrell, T. (2017). Curiosity-driven
exploration by self-supervised prediction. In International conference on
machinelearning(pp.2778–2787).
Pezzulo, G., Rigoli, F., & Friston, K. (2015). Active inference, homeostatic
regulationandadaptivebehaviouralcontrol.Progressinneurobiology,134,
17–35.
Puterman, M. L. (2014). Markov decision processes: discrete stochastic dy-
namicprogramming. JohnWiley&Sons.
Raffin, A., Kober, J., & Stulp, F. (2022). Smooth exploration for robotic rein-
forcementlearning. InConferenceonrobotlearning(pp.1634–1644).
Ramicic, M., & Bonarini, A. (2021). Uncertainty maximization in par-
tially observable domains: A cognitive perspective. arXiv preprint
arXiv:2102.11232.
Russell, S. J. (2010). Artificial intelligence a modern approach. Pearson Edu-
cation,Inc.
Sajid,N.,Tigas,P.,Zakharov,A.,Fountas,Z.,&Friston,K. (2021). Exploration
andpreferencesatisfactiontrade-offinreward-freelearning. arXivpreprint
arXiv:2106.04316.
Savinov, N., Raichuk, A., Marinier, R., Vincent, D., Pollefeys, M., Lillicrap, T.,
& Gelly, S. (2019). Episodic curiosity through reachability. In Interna-
tionalconferenceonlearningrepresentations.
Schulman, J., Levine, S., Abbeel, P., Jordan, M., & Moritz, P. (2015). Trust re-
gionpolicyoptimization. InInternationalconferenceonmachinelearning
(pp.1889–1897).
Schulman,J.,Wolski,F.,Dhariwal,P.,Radford,A.,&Klimov,O. (2017). Prox-
imalpolicyoptimizationalgorithms. arXivpreprintarXiv:1707.06347.
Shin, J. Y., Kim, C., & Hwang, H. J. (2022). Prior preference learning from
experts: Designing a reward with active inference. Neurocomputing, 492,
508–515.
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., & Riedmiller, M.
(2014). Deterministic policy gradient algorithms. In International con-
ferenceonmachinelearning(pp.387–395).
Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction.
MITpress.
Tschantz, A., Baltieri, M., Seth, A. K., & Buckley, C. L. (2020). Scaling active
inference.In2020internationaljointconferenceonneuralnetworks(ijcnn)
(pp.1–8).
Tucker, G., Bhupatiraju, S., Gu, S., Turner, R., Ghahramani, Z., & Levine, S.
(2018). The mirage of action-dependent baselines in reinforcement learn-
ing. InInternationalconferenceonmachinelearning(pp.5015–5024).
Ueltzho¨ffer, K. (2018). Deep active inference. Biological cybernetics, 112(6),
547–573.
von Helmholtz, H. (2001). Concerning the perceptions in general. Visual per-
ception,24–44.
Wright,J.N.S.J. (2006). Numericaloptimization. springerpublication.
Yang, Z., & Nguyen, H. (2021). Recurrent off-policy baselines for memory-
basedcontinuouscontrol. arXivpreprintarXiv:2110.12628.
Yin, H., Chen, J., Pan, S. J., & Tschiatschek, S. (2021). Sequential genera-
tive exploration model for partially observable reinforcement learning. In
Proceedings of the aaai conference on artificial intelligence (Vol. 35, pp.
10700–10708).
Zhu, P., Li, X., Poupart, P., & Miao, G. (2017). On improving deep reinforce-
mentlearningforpomdps. arXivpreprintarXiv:1704.07978.
Appendices
AppendixA Markovianbeliefstate-actionversushistory-dependentpolicies: mem-
oryandcomputationalcomplexity
In POMDPs, each time step introduces one new action and one new observation to the
history,addingD +D -dimensionaldatatotheexistinghistory. Consequently,start-
O A
ing with a D -dimensional observation o , the total dimensionality of the history by
O 0
time t is given by D +(D +D )×t. This increase in the dimensionality of inputs
O O A
necessitates more substantial memory for storage and larger matrix operations at each
step,escalatingcomputationaldemands.
Furthermore, the addition of one observation and one action to the history at each time
step leads to an exponential increase in the number of possible histories. Specifically,
this number expands at each step by a factor of (|Ω| × |A|), reflecting the agent’s |A|
possible actions and |Ω| possible observations. Thus, starting from any of |Ω| poten-
tial initial observations o ∈ Ω, the number of possible histories grows exponentially
0
as (|Ω| × |A|)t across t time steps, resulting in a total number of histories at time t
of |Ω| × (|Ω| × |A|)t. Consequently, the size of the policy space, which represents
the number of possible policies derived from these histories, also undergoes the same
exponential growth. This exponential growth over time significantly heightens mem-
ory and computational requirements to store and evaluate each possible history and its
corresponding policy. These challenges are particularly pronounced in scenarios with
high-dimensional action and observation spaces, where D and D are large, and in
A O
environments with extensive or continuous action and observation spaces, where |A|
and |Ω| are also large. Additionally, these challenges are accentuated in environments
withlongtimehorizons,wheretissubstantial.
Unlike history-dependent policies, where the length of history grows linearly, be-
lief states in belief state-action policies maintain a constant size, |S|, at each time step.
Moreover,thesizeofallpossiblebeliefstates(policyspace)remainsfixedas|B|. Con-
sequently, the memory required to store each belief state and its corresponding pol-
icy does not inherently increase over time, offering significant memory efficiency ad-
vantages over history-dependent approaches, particularly in environments with infinite
horizons or high-dimensional action and observation spaces. However, storing belief
states can be memory-intensive, especially in large or continuous state spaces where
|S|islarge. Nonetheless,thismemoryrequirementisgenerallymoremanageablecom-
paredtotheexpansionofhistorydimensionandpolicyspaceinhistory-dependentpoli-
cies(Yang&Nguyen,2021).
ToinferthebeliefstateusingBayesianinference(asdetailedinSub-section3.4),the
agentmustlearnthestructureoftheenvironment’sobservationandtransitionfunctions
through a generative model. While belief state-action policies offer memory efficiency
advantages, their computational complexity introduces distinct challenges, primarily
contingent on the learning process for the generative model and the belief state update
mechanism. Nonetheless, particularly in environments with infinite horizons, belief
state-action policies often achieve greater computational efficiency over time by utiliz-
ing fixed-dimension inputs, which streamline processing and ensure consistent compu-
tationaldemandsregardlessofthedurationofoperation.
AppendixB Rewardmodel
As mentioned in Sub-section 3.3, interacting with a POMDP through a policy π gen-
eratesthesequence(s ,o ,a ,s ,o ,a ,...). Duringtheinteraction,theagentreceives
0 0 0 1 1 1
reward values (r ,r ,...,r ,...), where r is generated according to the reward func-
0 1 t t
tionU(r |s ,a ). Giventhetrajectory(s ,a ,s ,a ,...),wecancalculatetheprobabil-
t t t 0 0 1 1
itydistributionofreceivingtherewardsequence(r ,r ,...)asfollows:
0 1
∞ T
(cid:89) (cid:89)
p(r ,r ,...|s ,a ,s ,a ,,...) = p(r |s ,a ) = U(r |s ,a ). (B.1)
0 1 0 0 1 1 k k k k k k
k=0 k=0
Due to the agent’s lack of knowledge about the true reward function U(r |s ,a ), it
t t t
learnsarewardmodelP(r |s ,a )inasupervisedmannerthroughenvironmentalinter-
t t t
actions,enablingtheagenttoestimatetherewardsequence’sdistribution:
∞
(cid:89)
p(r ,r ,...|s ,a ,s ,a ,,...) ≈ P(r |s ,a ). (B.2)
0 1 0 0 1 1 k k k
k=0
It is worth noting that learning the reward model poses challenges due to reward
sparsity and its scalar nature. While the generative model’s learning is crucial for a
belief state-action policy, our study explores scenarios with and without reward model
learning,asdetailedinSub-section6.1.
AppendixC ThePOMDPregularityassumptions
The regularity assumptions play a critical role in establishing the convergence of al-
gorithms that solve POMDPs (Puterman, 2014; Paternain, Bazerque, & Ribeiro, 2020;
Dufour & Prieto-Rumeau, 2012, 2013). These assumptions ensure the existence of
solutions for specific types of optimization problems that arise in POMDPs. The com-
pactness assumptions on the state, action, observation, and reward spaces are standard
requirements in the continuous space MDP and POMDP literature (Puterman, 2014;
Paternain et al., 2020; Dufour & Prieto-Rumeau, 2012, 2013). They also facilitate the
numericalimplementationofMDPandPOMDPproblemswithcontinuousspaces. For
instance, in grid-based methods, which involve dividing the continuous state or action
space into a finite set of discrete grid points, the compactness assumption ensures that
the discretization is not excessively coarse, which could lead to inaccuracies in the so-
lutions. It’sworthnotingthatthecompactnessassumptionoftheextrinsicrewardspace
Risnotastrictrequirementasitisuser-defined.
The Lipschitz continuity assumption ensures that the transition function, observation
function, and reward function are well-behaved and exhibit smoothness and bounded-
ness. Thisassumptionpreventsoverlysteeporabruptchangesinthesefunctions,which
couldresultininstabilityandconvergence issues. TheLipschitzcontinuityassumption
is crucial for effective and reliable decision-making in continuous spaces, as it guaran-
tees the stability and convergence of the algorithms (Puterman, 2014; Paternain et al.,
2020;Dufour&Prieto-Rumeau,2012,2013).
AppendixD Theoreticalanalysis
This section presents a detailed theoretical analysis of the main paper. To facilitate un-
derstanding,TableD.1liststherelevantnotationsusedinourproposedunifiedinference
framework.
TableD.1: Overviewofthenotationsusedinourproposedunifiedinferenceframework.
Expression Explanation
T ∈ {0,N} FinitetimehorizoninAIF
t ∈ {0,1,...,} Currenttimestep
τ ∈ {t,t+1,...,} Futuretimestep
s ∈ S Currenthiddenstate
t
D ∈ N Dimensionofstatespace
S
S ⊂ RDS Continuousstatespace
b ∈ B Currentbeliefstate
t
B Continuousbeliefstatespace
h ∈ B Representationofbeliefstateb
t t
D ∈ N Dimensionofbeliefstaterepresentation
H
H ⊂ RDH Continuousbeliefstaterepresentationspace
o ∈ O Currentobservation
t
D ∈ N Dimensionofobservationspace
O
O ⊂ RDO Continuousobservationspace
a ∈ A Currentaction(decision)
t
D ∈ N Dimensionofactionspace
A
A ⊂ RDA Continuousactionspace
r ∈ R Current(extrinsic)reward
t
R ⊂ R Continuous(extrinsic)rewardspace
a˜ = (a ,a ,...,a ) PlaninAIF(sequenceoffutureactionsincludingcurrenttimestept)
t t+1 T−1
¯
Π AsetofdesiredstochasticMarkovianbeliefstate-actionpoliciesπ¯
E[.] Expectationfunction
log Naturallog
P(.) Agent’sprobabilisticmodel
H(.) Shannonentropy
D (.) KullbackLeiblerdivergence
KL
σ(.) Boltzmann(softmax)function
P(o ,s ,b |a ,b ,) Beliefstategenerativemodel
t t t t−1 t−1
P(o |s ,b ) Beliefstate-conditionedobservation(likelihood)model
t t t−1
P(s |b ,a ) Beliefstate-conditionedtransitionmodel
t t−1 t−1
q(s |o ,b ) Beliefstate-conditionedvariationalposteriordistribution
t t t−1
F OriginaldefinitionofVFE(i.e.,ELBO)inAIF(K.Fristonetal.,2017)
t
FVRNN ExtensionofVFE(i.e.,ELBO)inVRNN(Chungetal.,2015)
G (o ) OriginaldefinitionoftheEFEinAIF(K.Fristonetal.,2017)
AIF t
G(a˜)(o ) OriginaldefinitionoftheEFE(K.Fristonetal.,2017)forplana˜
AIF t
G(π)(b ) OurproposedbeliefstateEFEforbeliefstate-actionpolicyπ
t
G(π)(b ,a ) Ourproposedbeliefstate-actionEFEforbeliefstate-actionpolicyπ
t t
π∗ = argmin G(π)(b ) Optimalunconstrainedbeliefstate-actionpolicy
π t
π¯∗ = argmin G(π¯)(b ) Optimalconstrainedbeliefstate-actionpolicy
π¯∈Π¯ t
V(π)(s ) StatevaluefunctioninRLforstate-actionpolicyπ
t
Q(π)(s ,a ) State-actionvaluefunctioninRLforstate-actionpolicyπ
t t
γ ∈ [0,1) Discountfactor
0 ≤ α,β,ζ < ∞ Scalingfactors
∆(.) Thesetofallprobabilitydistributionsoverthespecifiedset
|.| Cardinalityoperator
D.1 ProofofTheorem.5.3
Proof. By conditioning the EFE G (o ) in Eq. (16) on the belief state-action policy
AIF t
π andtakingthelimitasT → ∞,weobtaintheresultingexpressionG(π) (o )as:
Unified t
(cid:20) ∞ (cid:21)
(cid:88) q(a |π)P(s |s ,a )
G(π) (o ) = E log t:∞ τ+1 τ τ . (D.1)
Unified t q(st:∞,ot+1:∞,at:∞|ot,π)
P
˜
(o |a )q(s |o )
τ=t τ+1 τ τ+1 τ+1
UsingthebiasedgenerativemodelfromAIFliterature(K.Fristonetal.,2017;Pezzulo,
Rigoli, & Friston, 2015), the variational distribution q(s ,o ,a |o ,π) can be
t:∞ t+1:∞ t:∞ t
factoredas:
∞
(cid:89)
˜
q(s ,o ,a |o ,π) = q(a |π) q(s |o )P(o |s ,a ). (D.2)
t:∞ t+1:∞ t:∞ t t:∞ τ τ τ+1 τ τ
τ=t
Since action a is chosen based on the stochastic belief state-action policy π and does
t
not depend on the other actions, we have q(a |π) =
(cid:81)∞
q(a |π). Consequently,
t:∞ τ=t τ
Eq.(D.2)canberewrittenas:
∞
(cid:89)
˜
q(s ,o ,a |o ,π) = q(a |π)q(s |o )P(o |s ,a ). (D.3)
t:∞ t+1:∞ t:∞ t τ τ τ τ+1 τ τ
τ=t
Now, by substituting q(a |π) with
(cid:81)∞
q(a |π) and q(s ,o ,a |o ,π) with
t:∞ τ=t τ t:∞ t+1:∞ t:∞ t
theright-handsideofEq.(D.3),G(π) (o )inEq.(D.1)canberewrittenas:
Unified t
(cid:20)T−1
(cid:88)
G(π) (o ) = E logq(a |π) (D.4)
Unified t (cid:81)∞
τ=t
q(aτ|π)q(sτ|oτ)P˜(oτ+1|sτ,aτ) τ
τ=t
(cid:21)
P(s |a ,s )
˜ τ+1 τ τ
− logP(o |π,s )+log . (D.5)
τ+1 τ
q(s |o )
τ+1 τ+1
Furthermore, since policy π selects action a based on the belief state b , i.e., a ∼
τ τ τ
π(a |b ),wecanexpressG(π) (o )usingthelawoftotalexpectationasfollows:
τ τ Unified t
(cid:20) ∞
G(π) (o ) = E E
(cid:2)(cid:88)
logq(a |π,b )
Unified t P(bt:∞) q(st|ot)(cid:81)∞
τ=t
q(aτ|π,bτ)q(sτ+1|oτ+1,bτ)P˜(oτ+1|sτ,aτ,bτ) τ τ
τ=t
(cid:21)
P(s |a ,s ,b )
˜ τ+1 τ τ τ (cid:3)
− logP(o |a ,s ,b )+log (D.6)
τ+1 τ τ τ
q(s |o ,b )
τ+1 τ+1 τ
(cid:20) ∞
( = a) E E (cid:2)(cid:88) logπ(a |b )
P(bt:∞) q(st|ot)(cid:81)∞
τ=t
π(aτ|bτ)q(sτ+1|oτ+1,bτ)P˜(oτ+1|sτ,aτ,bτ) τ τ
τ=t
(cid:21)
P(s |a ,s ,b )
˜ τ+1 τ τ τ (cid:3)
− logP(o |a ,s ,b )+log (D.7)
τ+1 τ τ τ
q(s |o ,b )
τ+1 τ+1 τ
(cid:20) ∞
( = b) E E (cid:2)(cid:88) logπ(a |b )
P(bt:∞) (cid:81)∞
τ=t
π(aτ|bτ)q(sτ+1|oτ+1,bτ)P˜(oτ+1|aτ,bτ) τ τ
τ=t
(cid:21)
P(s |a ,b )
˜ τ+1 τ τ (cid:3)
− logP(o |a ,b )+log (D.8)
τ+1 τ τ
q(s |o ,b )
τ+1 τ+1 τ
( = c) E
P(bt|st,ot,bt−1,at−1)(cid:81)∞
τ=t
P(bτ+1|sτ+1,oτ+1,bτ,aτ)π(aτ|bτ)q(sτ+1|oτ+1,bτ)P˜(oτ+1|aτ,bτ)
(cid:20) ∞ (cid:21)
(cid:88) P(s |a ,b )
˜ τ+1 τ τ
logπ(a |b )−logP(o |a ,b )+log , (D.9)
τ τ τ+1 τ τ
q(s |o ,b )
τ+1 τ+1 τ
τ=t
where (a) follows from q(a |π,b ) = π(a |b ), and (b) follows because the belief state
τ τ τ τ
b contains the necessary information about the hidden state s that is relevant for pre-
τ τ
dicting s and o . As a result, the direct dependence on s becomes redundant
τ+1 τ+1 τ
when conditioned on the belief state b . (c) follows due to the factorization resulting
τ
fromthebeliefstateupdatefunctioninEq.(4)ofthemainmanuscript:
∞
(cid:89)
P(b )= P(b |b )] = P(b |s ,o ,b ,a ) (D.10)
t:∞ τ τ−1 t t t t−1 t−1
τ=t
∞
(cid:89)
× E [P(b |s ,o ,b ,a )].
(cid:81)∞
τ=t
π(aτ|bτ)q(sτ+1|oτ+1,bτ)P˜(oτ+1|aτ,bτ) τ+1 τ+1 τ+1 τ τ
τ=t
As stated in Assumption 5.2, following AIF, we assume that prior to action selection
at time step t, the agent performs inference and approximates its belief state b by a
t
variationalposterior(i.e.,hereq(.|o ,b )). Hence,thebeliefstateb isuniquelydeter-
t t−1 t
mined, and thus P(b |s ,o ,b ,a ) is equivalent to δ(b −q(.|o ,b )). Therefore,
t t t t−1 t−1 t t t−1
taking the expected value with respect to P(b |s ,o ,b ,a ) in Eq. (D.9) simply
t t t t−1 t−1
evaluates G(π) (o ) at the given b = q(.|o ,b ). Therefore, G(π) (o ) can be ex-
Unified t t t t−1 Unified t
pressed as a function of both o and b . Since b contains sufficient information about
t t t
o ,itisadequatetoexpressG(π) (o )inEq.(D.9)solelyasafunctionofb ,i.e.,
t Unified t t
(cid:20) ∞
(cid:88)
G(π) (b ) = E logπ(a |b )
Unified t (cid:81)∞
τ=t
P(bτ+1|sτ+1,oτ+1,bτ,aτ)P˜(oτ+1|aτ,bτ)π(aτ|bτ)q(sτ+1|oτ+1,bτ) τ τ
τ=t
(cid:21)
P(s |a ,b )
˜ τ+1 τ τ
− logP(o |a ,b )+log . (D.11)
τ+1 τ τ
q(s |o ,b )
τ+1 τ+1 τ
(cid:50)
D.2 ProofofProposition5.4
Proof. Theresultisobtainedbytakingoutthetermsβπ(a |b ),αr ,andζlog P(st+1|bt,at)
t t t q(st+1|ot+1,bt)
fromG(π)(b )inEq.(24):
t
∞
G(π)(b t ) = E (cid:81)∞
τ=t
π(aτ|bτ)P(sτ+1,oτ+1,bτ+1|bτ,aτ)P(rτ|bτ,aτ) (cid:2)(cid:88) γτ−t (cid:0) βlogπ(a τ |b τ )
τ=t
P(s |b ,a )
τ+1 τ τ (cid:3)
− αr +ζlog
τ
q(s |o ,b )
τ+1 τ+1 τ
(cid:20)
P(s |b ,a )
( = a) E βlogπ(a |b )−αr +ζlog t+1 t t
π(at|bt)P(st+1,ot+1|bt,at)P(rt|bt,at) t t t
q(s |o ,b )
t+1 t+1 t
+ γE P(bt+1|st+1,bt,at,ot+1) (cid:2)E (cid:81)∞ τ′=t+1 π(a τ′|b τ′)P(s τ′+1 ,o τ′+1 ,b τ′+1 |b τ′,a τ′)P(r τ′|b τ′,a τ′)
∞ (cid:21)
[ (cid:88) γτ′−t (cid:0) βlogπ(a |b )−αr +ζlog P(s τ′+1 |a τ′ ,b τ′ ) (cid:1) ] (cid:3)
τ′ τ′ τ′
q(s |o ,b )
τ′+1 τ′+1 τ′
τ′=t+1
(cid:20)
P(s |b ,a )
( = b) E βlogπ(a |b )−αr +ζlog t+1 t t
π(at|bt)P(rt|bt,at)P(st+1,ot+1|bt,at) t t t
q(s |o ,b )
t+1 t+1 t
(cid:21)
+ γE (cid:2) G(π)(b ) (cid:3)
P(bt+1|bt,at,ot+1) t+1
(cid:20)
P(s |b ,a )
( = c) E βlogπ(a |b )−αr +E (cid:2) ζlog t+1 t t
π(at|bt)P(rt|bt,at) t t t P(st+1,ot+1,bt+1|bt,at)
q(s |o ,b )
t+1 t+1 t
(cid:21)
(cid:3)
+ γG(π)(b ) ,
t+1
where (a) follows by applying the law of total expectation, (b) is because of the defi-
nition of the belief state EFE, and (c) follows since π(a |b ) and r are independent of
t t t
P(s ,o ,b |b ,a ). (cid:50)
t+1 t+1 t+1 t t
D.3 ProofofTheorem5.5
Proof. AccordingtoProposition5.4,wehave:
(cid:20)
G(π)(b ) = E βlogπ(a |b )−αr (D.12)
t π(at|bt)P(rt|bt,at) t t t
(cid:21)
P(s |b ,a )
+ E (cid:2) ζlog t+1 t t +γG(π)(b ) (cid:3) .
P(bt+1,ot+1,st+1|bt,at)
q(s |o ,b )
t+1
t+1 t+1 t
Therefore:
(cid:20)
G∗(b ) = minG(π)(b ) = minE βlogπ(a |b )−αr
t
π
t
π
π(at|bt)P(rt|bt,at) t t t
(cid:21)
P(s |b ,a )
+E (cid:2) ζlog t+1 t t +γG(π)(b ) (cid:3)
P(bt+1,ot+1,st+1|bt,at)
q(s |o ,b )
t+1
t+1 t+1 t
(cid:20)
( = a) min E E (cid:2) βlogπ(a |b )−αr (D.13)
π(at|bt) P(rt|bt,at) t t t
π(.|bt),π˜
(cid:21)
p(s |s ,a )
+E [ζlog t+1 t t +γG(π˜)(s )] (cid:3)
P(bt+1,ot+1,st+1|bt,at)
q(s |o )
t+1
t+1 t+1
(cid:20)
( = b) min E E (cid:2) βlogπ(a |b )−αr (D.14)
π(at|bt) P(rt|bt,at) t t t
π(.|bt)
(cid:21)
P(s |b ,a )
+E [ζlog t+1 t t +γminG(π˜)(b )] (cid:3)
P(bt+1,ot+1,st+1|bt,at)
q(s t+1 |o t+1 ,b t ) π˜
t+1
(cid:20)
( = c) min E E (cid:2) βlogπ(a |b )−αr (D.15)
π(at|bt) P(rt|bt,at) t t t
π(.|bt)
(cid:21)
P(s |b ,a )
+E [ζlog t+1 t t +γG∗(b )] (cid:3) ,
P(bt+1,ot+1,st+1|bt,at)
q(s |o ,b )
t+1
t+1 t+1 t
where (a) is because of the decomposition of the policy π as π = (π(.|s ),π˜). Let
t
π˜∗ = argmin G(π˜)(b ),then(b)followsbecauseofthetrivialinequality:
π˜ t+1
minE [G(π˜)(b )] ≥ E [minG(π˜)(b )],
π˜
P(bt+1,ot+1,st+1|bt,at) t+1 P(bt+1,ot+1,st+1|bt,at)
π˜
t+1
and
E [minG(π˜)(b )] = E [Gπ˜∗(b )]
P(bt+1,ot+1,st+1|bt,at)
π˜
t+1 P(bt+1,ot+1,st+1|bt,at) t+1
≤ minE [G(π˜)(b )]. (D.16)
π˜
p(st+1,ot+1|at) t+1
Finally,(c)followsfromthedefinitionoftheoptimalbeliefstateEFE. (cid:50)
D.4 ProofofCorollary.5.6
Proof. ProofofthiscorollaryisstraightforwardbytakingtheargumentofEq.(27). (cid:50)
D.5 ProofofTheorem5.7
Proof. FromCorollary5.6,wehave:
(cid:20)
π∗(a |b ) ∈ arg min E E [βlogπ(a |b )−αr ] (D.17)
t t π(at|bt) P(rt|bt,at) t t t
π(at|bt)
(cid:21)
P(s |b ,a )
+ E (cid:2) ζlog t+1 t t +γminG(π˜)(b ) (cid:3) .
P(bt+1,ot+1,st+1|st,at)
q(s t+1 |o t+1 ,b t ) π˜
t+1
(cid:82)
The minimization in the equation above subject to the constraint π(a′|b )da′ = 1
A t
yields minimizing the Lagrangian function L(λ,π(a |b )) with the Lagrange multiplier
t t
λ:
(cid:20)
P(s |b ,a )
L(λ,π(a |b ))=E E [βlogπ(a |b )−αr ]+E (cid:2) ζlog t+1 t t
t t π(at|bt) P(rt|bt,at) t t t P(bt+1,ot+1,st+1|st,at)
q(s |o ,b )
t+1 t+1 t
(cid:21) (cid:18)(cid:90) (cid:19)
(cid:3)
+γG∗(b ) −λ π(a′|b )da′ −1 . (D.18)
t+1 t
A
ThederivativeofL(λ,π(a |b ))withrespecttoπ(a |b )isgivenby:
t t t t
∂L(λ,π(a |b ))
t t =E [βlogπ(a |b )+β −αr ] (D.19)
∂π(a |b ))
P(rt|bt,at) t t t
t t
P(s |b ,a )
+E (cid:2) ζlog t+1 t t +γG∗(b ) (cid:3) −λ.
P(bt+1,ot+1,st+1|st,at)
q(s |o ,b )
t+1
t+1 t+1 t
Setting ∂L(λ,π(at|bt)) = 0givestheoptimalπ∗(a |b )forthespecificactiona as:
∂π(at|bt)) t t t
(cid:32) λ−β −E (cid:2) αr −E [ζlog P(st+1|bt,at) +γG∗(b )] (cid:3)(cid:33)
π∗(a |b )=exp P(rt|bt,at) t P(bt+1,ot+1,st+1|bt,at) q(st+1|ot+1,bt) t+1 .
t t
β
(D.20)
(cid:82)
Byaccountingtheconstraint π(a′|b )da′ = 1:
A t
(cid:90)
λ−β
π(a′|b )da′ = exp( ) (D.21)
t
β
A
(cid:90) (cid:32)E (cid:2) αr −E [ζlog P(st+1|bt,at) +γG∗(b )] (cid:3)(cid:33)
exp
P(rt|bt,at) t P(bt+1,ot+1,st+1|bt,at) q(st+1|ot+1,bt) t+1
= 1.
β
A
PuttingtogetherEqs.(D.20)and(D.21)resultsin:
(cid:32) (cid:2) (cid:3)(cid:33)
exp
E
P(rt|bt,at)
αrt−E
P(bt+1,ot+1,st+1|bt,at)
[ζlog
q
P
(s
(
t
s
+
t+
1|
1
o
|
t
b
+
t,
1
a
,
t
b
)
t)
+γG∗(bt+1)]
β
π∗(a |b ) =
t t (cid:82) exp (cid:32) E P(rt|bt,a′) (cid:2) αrt−E P(bt+1,ot+1,st+1|bt,a′) [ζlog q( P s ( t s + t 1 + | 1 o | t b + t 1 ,a ,b ′) t) +γG∗(bt+1)] (cid:3)(cid:33) da′
A β
(cid:32)E (cid:2) αr −E [ζlog P(st+1|bt,at) +γG∗(b )] (cid:3)(cid:33)
= σ
P(rt|bt,at) t P(bt+1,ot+1,st+1|bt,at) q(st+1|ot+1,bt) t+1
.
β
(D.22)
TakingthesecondderivativeofL(λ,π(a |b ))withrespecttoπ(a |b )yields:
t t t t
∂2L(λ,π(a |b )) β
t t
= , (D.23)
∂π2(a |b ) π(a |b )
t t t t
which is a positive value for the compact action space A. Therefore, the expression
in Eq. (D.22) is the unique minimizer of L(λ,π(a |b )) and, consequently, the unique
t t
minimizeroftheunifiedBellmanequation. (cid:50)
D.6 ProofofLemma5.8
Proof. FromthedefinitionofG(π¯∗)(b ,a )inEq.(39):
t t
(cid:20) (cid:21)
P(s |b ,a )
G(π¯∗)(b ,a ) = E −αr +E (cid:2) ζlog t+1 t t +γG(π¯∗)(b ) (cid:3)
t t P(rt|bt,at) t P(bt+1,ot+1,st+1|bt,at)
q(s |o ,b )
t+1
t+1 t+1 t
(cid:20) (cid:21)
P(s |b ,a )
( = a)E −αr +E (cid:2) ζlog t+1 t t +γminG(π¯)(b ) (cid:3)
P(rt|bt,at) t P(bt+1,ot+1,st+1|bt,at)
q(s t+1 |o t+1 ,b t ) π¯∈Π
t+1
(cid:20) (cid:21)
P(s |b ,a )
( = b) minE −αr +E (cid:2) ζlog t+1 t t +γG(π¯)(b ) (cid:3)
π¯∈Π
P(rt|bt,at) t P(bt+1,ot+1,st+1|bt,at)
q(s t+1 |o t+1 ,b t )
t+1
(cid:124) (cid:123)(cid:122) (cid:125)
G(π¯)(bt,at)
= minG(π¯)(b ,a ), (D.24)
t t
π¯∈Π
where (a) follows from the definition of G(π¯∗)(b ), and (b) follows since P(r |b ,a ),
t+1 t t t
r ,P(b ,o ,s |b ,a ),andlog P(st+1|bt,at) areindependentofπ¯. (cid:50)
t t+1 t+1 t+1 t t q(st+1|ot+1,bt)
D.7 ProofofProposition5.9
Proof. Theprooffollowstwosteps:
First,byreplacingG(π¯)(b ,a )withE (cid:2) −αr +E [ζlog P(st+1|bt,at) +
t t P(rt|bt,at) t P(bt+1,ot+1,st+1|bt,at) q(st+1|ot+1,bt)
(cid:3)
γG(π¯)(b )] in Eq. (35), we can express the belief state EFE G(π¯)(b ) as a function of
t+1 t
thebeliefstate-actionEFEG(π¯)(b ,a )asfollows:
t t
G(π¯)(b ) = E (cid:2) βlogπ¯(a |b )+G(π¯)(b ,a ) (cid:3) . (D.25)
t π¯(at|bt) t t t t
Now, by plugging G(π¯)(b ) = E (cid:2) logπ¯(a |b )+G(π¯)(b ,a ) (cid:3) ob-
t+1 π¯(at+1|bt+1) t+1 t+1 t+1 t+1
tainedfromEq.(D.25)intotheresultofLemma5.8,theproofiscompleted:
(cid:20)
P(s |b ,a )
G(π¯)(b ,a ) = E −αr +E (cid:2) ζlog t+1 t t
t t P(rt|bt,at) t P(bt+1,ot+1,st+1|bt,at)
q(s |o ,b )
t+1 t+1 t
(cid:21)
+ γE [βlogπ(a |b )+G(π¯)(b ,a )] (cid:3) . (D.26)
π¯(at+1|bt+1) t+1 t+1 t+1 t+1
(cid:50)
D.8 ProofofTheorem5.10
Proof. Following (Sutton & Barto, 2018; Leibfried, Pascual-Diaz, & Grau-Moya,
2019),let’sdefinePπ(b ,o ,s ,b ,a )andRunified(b ,a )as
t+1 t+1 t+1 t t t t
P (b ,a ,b ,a ) := E [P(b |s ,o ,b ,a )π¯(a |b )], (D.27)
π¯ t t t+1 t+1 P(ot+1,st+1|bt,at) t+1 t+1 t+1 t t t+1 t+1
(cid:20)
P(s |b ,a )
Runified(b ,a ) := E −αr +E (cid:2) ζlog t+1 t t
t t P(rt|bt,at) t P(bt+1,ot+1,st+1|bt,at)
q(s |o ,b )
t+1 t+1 t
(cid:21)
+ γE [βlogπ(a |b )] (cid:3) . (D.28)
π¯(at+1|bt+1) t+1 t+1
The compactness assumptions on the state space S, action space A, and observation
space O guarantee that the space of all belief states b ∈ B is a compact subset of a
τ
separable Hilbert space. As a result, both P (b ,a ,b ,a ) and Runified(b ,a ) are
π¯ t t t+1 t+1 t t
bounded. Withtheseassumptionsinplace,wecanexpressTunifiedG inacompactform
π¯
asfollows:
TunifiedG = Runified +γPπG,
π¯
BystartingfromaninitialboundedmappingG : B×A → Randrecursivelyapplying
0
G = TunifiedG fork = 0,1,2,...,wehave
k+1 π¯ k
k−1
(cid:88)
Gπ¯ := lim TunifiedG = lim γiPiRunified +γkPkG
π¯ k k→∞ π¯ π¯ 0
k→∞
i=0
k−1
( = a) lim (cid:88) γiPiRunified, (D.29)
k→∞ π¯
i=0
where(a)isbecauseG andP areboundedandthusthetermγkPkG willconvergeto
0 π¯ π¯ 0
zero. Therefore the convergence of TunifiedGπ¯ does not depend on the initial value G .
π¯ 0
Now,bysubstitutingGπ¯ withlim (cid:80)k−1γiPiRunified,wehave:
k→∞ i=0 π¯
k−1 k
(cid:88) (cid:88)
TunifiedGπ¯ = Runified +γPπ lim γiPiRunified = lim γiPiRunified
π¯ π¯ π¯
k→∞ k→∞
i=0 i=0
k−1
( = a) lim (cid:88) γiPiRunified +γkPkRunified = Gπ¯, (D.30)
π¯ π¯
k→∞ (cid:124) (cid:123)(cid:122) (cid:125)
i=0
0
where (a)follows sinceRunified is bounded. Eq. (D.30) demonstrates thatGπ¯ is afixed
pointofTunifiedGπ¯.
π¯
To establish the uniqueness of this fixed point, let’s assume there exists another fixed
point G′ of Tunified such that TunifiedG′ = G′. Then, G′ = lim TunifiedG with
π¯ π¯ k→∞ π¯ k
G = G′ converges to Gπ¯ since the convergence behavior of Tunified is independent of
0 π¯
theinitialvalueG′. Consequently,wecanconcludethatG′ = Gπ¯. (cid:50)
D.9 ProofofCorollary5.11
Proof. Thisresultcanbeeasilyobtainedbyreplacingπ¯ andG(π¯)(b ,a )intheresultof
t t
Theorem5.10withtheoptimalpolicyπ¯∗ anditscorrespondingbeliefstate-actionvalue
functionG(π¯∗)(b ,a ),i.e.,theoptimalbeliefstate-actionEFE. (cid:50)
t t
D.10 ProofofLemma5.12
Proof. Basedonthedefinitionofπ¯(new),foranyπ¯ ∈ Π ¯ :
(cid:34) (cid:32) (cid:33)(cid:35) (cid:34) (cid:32) (cid:33)(cid:35)
−G(π¯(old))(b ,.) −G(π¯(old))(b ,.)
D π¯(new)(.|b ),σ t ≤ D π¯(.|b ),σ t .(D.31)
KL t KL t
β β
Bychoosingπ¯ = π¯(old) inEq.(D.31)andusingdefinitionofKL-divergence:
(cid:104) (cid:105)
−βH(π¯(new)(.|b ))+E G(π¯(old))(b ,a ) (D.32)
t π¯(new)(at|bt) t t
(cid:104) (cid:105)
≤ −βH(π¯(old)(.|b ))+E G(π¯(old))(b ,a ) = G(π¯(old))(b ).
t π¯(old)(at|bt) t t t
Moreover,fromEq.(45),wehave:
(cid:20)
P(s |b ,a )
G(π¯(old))(b ,a ) = E −αr +E (cid:2) ζlog t+1 t t
t t P(rt|bt,at) t P(bt+1,ot+1,st+1|bt,at)
q(s |o ,b )
t+1 t+1 t
(cid:21)
+ γG(π¯(old))(b ) (cid:3) (D.33)
t+1
Now,byrepeatedlyapplyingEq.(D.33)andtheboundinEq.(D.32):
(cid:20)
P(s |b ,a )
G(π¯(old))(b ,a ) = E −αr +E (cid:2) ζlog t+1 t t
t t P(rt|bt,at) t P(bt+1,ot+1,st+1|bt,at)
q(s |o ,b )
t+1 t+1 t
(cid:21)
+ γG(π¯(old))(b ) (cid:3)
t+1
(cid:20)
P(s |b ,a )
≥ E −αr +E (cid:2) ζlog t+1 t t
P(rt|bt,at) t P(bt+1,ot+1,st+1|bt,at)
q(s |o ,b )
t+1 t+1 t
(cid:104) (cid:105)
− βH(π¯(new)(.|b ))+E G(π¯(old))(b ,a ) (D.34)
t π¯(new)(at|bt) t t
.
.
.
(a)
≥ G(π¯(new))(b ,a ), (D.35)
t t
where(a)resultsfromTheorem5.10. (cid:50)
D.11 ProofofTheorem5.13
Proof. Let π¯ be the policy at iteration k of the policy iteration process. According
k
toLemma5.12,thesequenceG(π¯ k ) ismonotonicallydecreasing. SinceRunified defined
in Eq. (D.28) is bounded, G(π¯) = E[ (cid:80)∞ γτ−tRunified(b ,a ))] is also bounded. The
τ=t τ τ
sequence G(π¯ k ) and consequently sequence π¯ converge to some G(π¯∗) and policy π¯∗.
k
We still need to show that π¯∗ is the optimal policy. To do so, we need to show that
¯
the belief state-action of the converged policy is lower than any other policy in Π, i.e.,
G(π¯∗)(b ,a ) < G(π¯)(b ,a )forallπ¯ ∈ Πandall(b ,a ) ∈ B ×A. Thiscanbeachieved
t t t t t t
byfollowingthesameiterativestepsoutlinedinproofofLemma5.12. (cid:50)
AppendixE Unifiedinferencemodel
In this section, we elaborate on the proposed unified inference model, encompassing
perceptual inference and learning, as well as the unified actor-critic framework. We
delve into three training methods for the unified actor-critic: model-based, model-free,
and hybrid approaches. Furthermore, we present the pseudocode for the unified infer-
encemodel.
E.1 Perceptualinferenceandlearningmodel
AsstatedinSub-section4.2.1,AIFapproximatesgenerativemodelP(o ,s |a ,s ) =
t t t−1 t−1
P(o |s )P(s |s ,a )andthevariationaldistributionq(s |o )byminimizingtheVFE
t t t t−1 t−1 t t
(negativeofELBOinVAE).However,inourproposedunifiedinference,weneedtoap-
proximate the belief state-conditioned variational posterior q(s |o ,b ) and the belief
t t t−1
stategenerativemodeldefinedinEq.(21),i.e.,
P(o ,s ,b |a ,b ) = P(o |s ,b )P(s |b ,a )P(b |s ,o ,b ,a ). (E.1)
t t t t−1 t−1 t t t−1 t t−1 t−1 t t t t−1 t−1
Toachievethis,weusetheVariationalRecurrentNeuralNetworks(VRNN)model(Chung
et al., 2015). VRNNs combine the variational inference of VAEs with the sequential
modeling capabilities of RNNs. They minimize the VFE conditioned on a variable of
an RNN, enabling the modeling of temporal dependencies and generating sequential
data. However, in our case, where the POMDP assumes a continuous state space (i.e.,
an infinite number of latent states), the corresponding belief space b is a continuous
t−1
probability distribution function with an infinite number of dimensions. Therefore, we
need to approximate b to use it as an input to the VRNN model for approximating
t−1
P(o |s ,b ),P(s |b ,a ),P(b |s ,o ,b ,a ),andq(s |o ,b ).
t t t−1 t t−1 t−1 t t t t−1 t−1 t t t−1
Various approaches can be used to approximate a continuous belief state for input
to a neural network model, including vector representation, kernel density estimation
(KDE), discretization, and particle filtering (Igl et al., 2018). These methods aim to
transform the continuous belief state into a fixed-length representation that can be fed
into the neural network. One common approach is to learn a belief representation
through the belief state update function in Eq. (4) (Gregor et al., 2019). Following
established practices, we learn an explicit belief state representation h ∈ RDH, where
t
D representsthedimensionofthebeliefstaterepresentationh .13 Welearnthisbelief
H t
state representation through a deterministic non-linear function f that updates the be-
lief state representation based on the previous belief state representation, the previous
action,thecurrentobservation,andthecurrentstate:
h = f(h ,a ,o ,s ). (E.2)
t t−1 t−1 t t
By iteratively applying the update function f, the belief state representation can adapt
and evolve over time as new observations and actions are encountered. This approach
allows us to approximate the belief state representation in a continuous space and uti-
13Whileourunifiedinferenceframeworkcanbecombinedwithvariousalgorithmsforapproximating
thebeliefstate,weemployacommonlyusedapproximationmethodthathasdemonstratedeffectiveness
inmanyrecentapproaches.
lizeitasaninputtoaVRNNmodelforapproximatingP(o |s ,h ),P(s |h ,a ),
t t t−1 t t−1 t−1
P(h |s ,o ,h ,a ), and q(s |o ,h ). Given the transition model P(s |b ,a )
t t t t−1 t−1 t t t−1 t t−1 t−1
and the update function f(h ,a ,o ,s ), we can intuitively say that the states are
t−1 t−1 t t
split into a stochastic part s and a deterministic part h . This aligns with the modifi-
t t
cations made in previous RL and AIF works that heuristically incorporate this separa-
tion (Hafner et al., 2020; Ogishima, Karino, & Kuniyoshi, 2021; Lee et al., 2020; Han
etal.,2020).
We parameterize the posterior distribution with ν, denoted as q (s |o ,h ), and the
ν t t t−1
agent’s belief state generative model with θ, i.e., P (o |s ,h ), P (s |h ,a ), and
θ t t t−1 θ t t−1 t−1
P (h |s ,o ,h ,a ) = δ(h −f (h ,a ,o ,s )). Tomodelq (s |o ,b ),P (o |s .b ),
θ t t t t−1 t−1 t θ t−1 t−1 t t ν t t t−1 θ t t t−1
and P (s |b ,a ) we use DNNs that output the mean and standard deviation of
θ t t−1 t−1
the random variables according to the Gaussian distribution. For example, we model
q (s |o ,b ) as a network that takes o and b as inputs, calculates through several
ν t t t−1 t t−1
hidden layers, and outputs the Gaussian distribution of s . The update function f can
t θ
be implemented using gated activation functions such as Long Short-Term Memory
(LSTM) or Gated Recurrent Unit (GRU). We use LSTM (Hochreiter & Schmidhuber,
1997)asithasshowngoodperformanceingeneralcases.
The parameters ν and θ are trained by minimizing the objective function of VRNN,
givenby
FVRNN=−E [logP(o |s ,h )]+D [q (.|o ,h ),P (.|b ,a )].(E.3)
θ,ν q(st|ot,ht−1) t t t−1 KL ν t t−1 θ t−1 t−1
The VRNN objective function FVRNN is minimized using gradient descent on batches
θ,ν
of data sampled from a dataset called a replay buffer, denoted as D. The replay buffer
contains quadruples consisting of the agent’s action a , the received extrinsic reward
k
r (caused by the action), and the subsequent observation o . The VFE can be
k+1 k+1
expressed as an expectation over a batch of size B, consisting of M sequential data
points,denotedas{(a ,o )M }B ,whicharesampledfromthereplaybufferD.
k−1 k k=1 i=1
(cid:20)
FVRNN = E −E [logP (o |s ,h )]
θ,ν D(a k−1 ,o k ) qν(st|o k ,ht−1) θ k t t−1
(cid:21)
+ D [q (.|o ,h ),P (.|h ,a )] (E.4)
KL ν t t−1 θ t−1 k−1
Theexpectationwithrespecttoq (s |o ,h )inEq.(E.4)involvesintegratingoverthe
ν t k t−1
continuous state space S, which is not analytically tractable. To estimate FVRNN, we
θ,ν
use Monte Carlo estimation, which provides an unbiased estimation by sampling sl for
t
l ∈ {1,2,...,L}fromq (s |o ,h )andevaluatingFVRNN foreachsampledstate. The
ν t k t−1 θ,ν
estimate of FVRNN is obtained by taking the average over the samples. However, com-
θ,ν
puting the gradient of FVRNN with respect to ν requires differentiating with respect to
θ,ν
q (s |o ,h ),whichisthedistributionwesampledfrom. Toenablegradient-basedop-
ν t k t−1
timization,itisnecessarytohaveadifferentiablesamplingprocessthatgeneratessam-
ples from a probability distribution. The reparameterization trick (Kingma & Welling,
2013) provides a solution to this problem by reparameterizing the original distribution
and introducing an auxiliary variable that follows a fixed distribution, such as a stan-
dard Gaussian. This reparameterization allows us to obtain differentiable samples by
applying a deterministic transformation to the auxiliary variable. As a result, we can
backpropagate gradients through the sampling operation and compute gradients with
respect to the distribution parameters. In our approach, we draw inspiration from es-
tablished practices in the field (Lee et al., 2020; Han et al., 2020; Hafner et al., 2020)
and leverage the reparameterization trick (Kingma & Welling, 2013) to sample sl from
t
thedistributionq (s |o ,h ). Givensampless(l) ,beliefstaterepresentationh isthen
ν t k t−1 t t
computedash = 1 (cid:80)L f (h ,a ,o ,sl).
t L l=1 θ t−1 k−1 k t
E.2 Rewardmodellearning
We approximate P(r |h ,a ) using a DNN with parameter ξ, denoted as P (r |h ,a ).
t t t ξ t t t
Therewardmodelapproximationisasupervisedlearningproblemthatcanbeaddressed
through likelihood maximization, using samples from the replay buffer D. This results
inminimizingthefollowinglossfunction:
L (ξ) = −E [logP (r |h ,a )]. (E.5)
r D(a ,r ) ξ k t k
k k
E.3 Pseudocodefortheunifiedinference
Algorithm E.1 outlines the pseudocode for the proposed unified inference framework.
Weuseavariablemodetorepresenttheapproach(model-based,model-freeorhybrid)
usedfortrainingtheproposedunifiedactor-criticmodel.
AppendixF Environments
We utilize the PyBullet (Coumans & Bai, 2016) benchmarks, replacing the deprecated
Roboschool as recommended by the official GitHub repository14. To convert these Py-
Bullet benchmarks into tasks with partial observations, we made modifications by re-
taining the velocities while removing all position/angle-related entries from the obser-
14https://github.com/openai/roboschool
AlgorithmE.1 UNIFIED INFERENCE MODEL
1: Model components: Likelihood P (o |s ,h ), variational posterior
θ t t t−1
q (s |o ,h ), transition model P (s |h ,a ), belief state representation update
ν t t t−1 θ t+1 t t
function f (h ,a ,o ,s ), reward model P (r |h ,a ), belief state-action EFE
θ t−1 t−1 t t ξ t t t
G (h ,a ),beliefstate-actionpolicyπ¯ (a |h ).
ψ t t ϕ t t
2: Hyperparameters: Scaling parameters α, β, ζ, and c, discount factor γ, learning
rateλ,actor-criticlearningmodeindicatorI ,batchsizeB,sequencelengthM,
mode
imaginationhorizonN.
3: Initialize: Neuralnetworkparametersθ,ν,ξ,ψ,andϕ,globalstept ← 0.
4: whilenotconvergeddo:
5: Resettheenvironmentandinitializeh randomly.
0
6: foreachenvironmentsteptdo:
7: Receiveobservationo
t
8: Samples ∼ q (s |o ,h )
t ν t t t−1
9: Sampleactiona ∼ π¯ (a |h )andexecutea intheenvironment.
t ϕ t t t
10: Receiveobservationo andextrinsicrewardr .
t+1 t
11: Record(a ,r ,o )intoD.
t t t+1
12: endfor
13: foreachgradientstepdo:
14: Sampleaminibatch{(a ,o )M }B fromD.
k−1 k k=1 i=1
15: ComputeFVRNN fromEq.(E.4).
θ,ν
16: Updateθ ← θ−λ∇ FVRNN.
θ θ,ν
17: Updateν ← ν −λ∇ FVRNN.
ν θ,ν
18: ifI == model−freethen: ▷Model-freeunifiedactor-critic.
mode
19: Sampleaminibatch{(a ,r ,o )M }B fromD.
k k k+1 k=1 i=1
20: ComputeLMF(ψ)fromEq.(63)andsetL (ψ) = LMF(ψ).
G G G
21: endif
22: ifI == model−basedthen: ▷Model-basedunifiedactor-critic.
mode
23: ComputeL (ξ)fromEq.(E.5).
r
24: Updateξ ← ξ −λ ∇ L (ξ).
r ξ r
25: Imaginetrajectories{(a ,r ,o )t+N}B startingfromh .
τ τ τ+1 τ=t i=1 t
26: ComputeLMB(ψ)fromEq.(68)andsetL (ψ) = LMB(ψ).
G G G
27: endif
28: ifI == hybridthen: ▷Hybridunifiedactor-critic.
mode
29: Sampleaminibatch{(a ,o )M }B fromD.
k−1 k k=1 i=1
30: ComputeLMF(ψ)fromEq.(63).
G
31: Imaginetrajectories{(a ,r ,o )t+N}B startingfromh .
τ τ τ+1 τ=t i=1 t
32: ComputeLMB(ψ)fromEq.(68).
G
33: SetL (ψ) = cLMF(ψ)+LMB(ψ).
G G G
34: endif
35: ψ ← ψ −λ∇ L (ψ).
ψ G
36: ComputeL (ϕ)fromEq.(65).
π¯
37: ϕ ← ϕ−λ∇ L (ϕ).
ϕ π¯
38: t ← t+1.
39: endfor
40: endwhile
vation vector. Table F.1 provides a summary of key information for each environment.
TableF.1: InformationoftheRoboschoolPyBulletenvironmentsweused.
Task D D (velocitiesonly)
S O
HalfCheetahPyBulletEnv 26 9
HopperPyBulletEnv 15 6
AntPyBulletEnv 28 9
Walker2DPyBulletEnv 22 9
AppendixG Implementationdetails
In this section, we describe the implementation details for our algorithm and the alter-
nativeapproaches.
For Recurrent Model-Free (Ni et al., 2022), we followed its original implementation,
includingthehyperparameters. TheofficialimplementationofVRM(Hanetal.,2020)
was also used for the inference, encoding, decoding, and actor-critic networks. We
ensured a fair comparison for G-Dreamer and G-SAC by adopting the same network
structureasVRM,includingVRNNandactor-criticnetworks.
WhiletheoriginalDreamerframework(Hafneretal.,2020)usedpixelobservations
andemployedconvolutionalencoderanddecodernetworks,wemadeadaptationstosuit
our partially observable case. Specifically, we replaced CNNs and transposed CNNs
with two-layer MLPs, each containing 256 units. Additionally, we adjusted Dreamer’s
actor and critic network parameters to match those of VRM, G-Dreamer, and G-SAC,
ensuringaconsistentconfigurationforfaircomparison.
G.1 VRNN
In accordance with (Han et al., 2020), we set the dimensionality of the belief state
representation h to 256. To model the variational posterior q (s |o ,h ), we employ
ν t t t−1
a one-hidden-layer fully-connected network with 128 hidden neurons. The likelihood
functionP (o |s ,h ),thetransitionfunctionP (s |h ,a ),andtherewardfunction
θ t t t−1 θ t+1 t t
P (r |h ,a )aremodelledusingtwo-layerMLPswith128neuronsineachlayer. Allof
ξ t t t
these networks are designed as Gaussian layers, wherein the output is represented by
a multivariate normal distribution with diagonal variance. The output functions for the
meanarelinear,whiletheoutputfunctionsforthevarianceemployanon-linearsoftplus
activation. For the belief state representation update model f (h ,a ,o ,s ), we
θ t−1 t−1 t t
utilizeanLSTMarchitecture.
G.2 Actor-Critic
We represent the actor π¯ (a |h ) as a diagonal multivariate Gaussian distribution. Both
ϕ t t
theactornetworkπ¯ (a |h )andthecriticnetworkG (h ,a )consistof4fullyconnected
ϕ t t ψ t t
layerswithanintermediatehiddendimensionof256. Theactormodelgeneratesalinear
mean for the Gaussian distribution, while the standard deviation is computed using the
softplusactivation function. Theresultingstandard deviationisthen transformedusing
the tanh function. On the other hand, the critic model utilizes a linear output layer,
whichprovidestheestimatedvalueofthegivenbeliefstate-actionpair.
G.3 ModelLearning
The VRNN parameters are trained with a learning rate of 0.0008. The training is con-
ducted using batches of 4 sequences, each with a length of 64. The critic and actor pa-
rameters are trained with a learning rate of 0.0003 for G-SAC. Batches of 4 sequences,
each with a length of 64, are used during training. For G-Dreamer, the actor and critic
are trained with a learning rate of 0.00008, and batches of 50 sequences, each with
a length of 50, are employed. All of the model parameters are optimized using the
Adam optimizer. During training, a single gradient step is taken per environment step.
The imagination horizon, N, is set to 15, and the discount factor γ is set to 0.99 for
both G-Dreamer and G-SAC. In addition, the scaling parameter β and ζ is set to 1 for
bothalgorithms. Foracomprehensivesummaryofthehyperparameters,pleasereferto
TablesG.1.
TableG.1: HyperparametersandnetworksetupusedtoimplementtheproposedG-SAC
andG-Dreameralgorithms.
Hyperparameter G-SAC G-Dreamer
Discountfactorγ 0.99 0.99
Scalingfactorβ andζ 1 1
Optimizerforallthenetworks Adam Adam
LearningrateforVRNNparametersν andθ 0.0008 0.0008
Learningratefortherewardparameterξ − 0.0008
Batchsizeforν,θ,andξ 4 4
Sequencelengthforν,θ,andξ 64 64
LearningrateforG andπ¯ 0.0003 0.00008
ψ ϕ
Batchsizeforψ andϕ 4 50
Sequencelengthforforψ andϕ 64 50
TheimaginationhorizonN − 15
AppendixH Comparisonofcomputationalandmemorycomplexity
Table H.1 compares the computational speeds and memory usage of VRM, Recurrent
Model-Free, G-SAC, and G-Dreamer when implemented over 1 million steps in the
Hopper-Penvironment,utilizinganNvidiaV100GPUand10CPUcoresforeachtrain-
ing run. This comparison allows us to assess the computational time required for each
methodasfollows:
VRM has the lowest computing time and memory. VRM involves perceptual learning
andinferencebylearningagenerativemodelandinferringbeliefstatesviafeed-forward
neural networks. It also learns belief state representations through an LSTM. Our pro-
posed model-free G-SAC algorithm exhibits similar computational efficiency, with the
additionofKLdivergencecalculationintheinformationgaintermofitsunifiedobjec-
tive function (referenced in Eq. (24)), leading to slightly increased computational and
memory demands compared to VRM. Our proposed model-based G-Dreamer requires
morecomputationandmemorythanbothVRMandG-SAC,asitnotonlyencompasses
the tasks performed by G-SAC but also learns the reward function, further elevating its
computationalload.
Recurrent Model-Free, on the other hand, showcases the highest computational time,
primarily attributed toits RNN layer (LSTM), which processes thecomplete history of
observations and actions as inputs. At each time step t, an additional 7-dimensional
input(comprisinga6-dimensionalobservationo andascalaractiona inHopper-P)is
t t
incorporated into the LSTM used in Recurrent Model-Free, posing challenges in RNN
training.
ItshouldbenotedthatwhileVRM,G-SAC,andG-Dreameralsoentailsignificantcom-
putational demands arising from generative model learning, belief state inference, and
representation via h = f(h ,a ,o ,s ), with h being 256-dimensional, their com-
t t−1 t t t t
plexity in our experiments is comparatively lower than that of Recurrent Model-Free.
Thisdisparitycanbeattributedtotwokeyfactors:
i) VRM, G-SAC, and G-Dreamer learn the generative model and belief state via feed-
forwardneuralnetworks,whicharelesscomputationallydemandingthanRNNs.
ii) Although belief state representation learning h = f(h ,a ,o ,s ) in VRM, G-
t t−1 t t t
TableH.1: TimeandmemorycostcomparisoninHopper-Pfor1millionsteps.
Method Time Memory
G-Dreamer(ours) 8.7hours 840MB
VRM(Hanetal.,2020) 3hours 600MB
G-SAC(ours) 4.8hours 720MB
RecurrentModel-Free(Nietal.,2022) 17.5hours 1.2GB
SAC,andG-DreamerisconductedviaanLSTMwithahigh-dimensionalinput(a278-
dimensionalcombination,includinga256-dimensionalh ,ascalara ,a6-dimensional
t−1 t
o ,anda15-dimensionals inHopper-P),theinputlengthremainsfixedoveralldecision-
t t
making steps into the future. In contrast, the LSTM in Recurrent Model-Free starts
with an initial 6-dimensional observation o , with an additional 7 dimensions added
0
after each step. Consequently, after about 39 time steps, the input dimension of the
LSTM in Recurrent Model-Free (6 + 39 × 7 = 279) surpasses the 278-dimensional
input in VRM, G-SAC, and G-Dreamer. Table H.1 shows that this increase in input
dimension leads to an increase in computation that exceeds that of VRM, G-SAC, and
G-Dreamer. The subsequent increase in input dimensionality leads to higher compu-
tational demands, surpassing those of VRM, G-SAC, and G-Dreamer. This is because
inputs with higher dimensions in a network require larger matrix multiplications per
step,leadingtoincreasedcomputationalcosts. Thishighlightsthesignificantimpactof
increased input dimensions in RNNs on computational demand, especially relevant in
our infinite horizon POMDP setting, where the agent may operate over a vast number
of steps. Furthermore, more memory is required to store inputs, outputs, intermediate
states,andgradients.
In conclusion, within an infinite horizon POMDP setting where the agent consis-
tently interacts with its environment, the escalating memory and computation demands
per step, driven by increased input dimensions, become increasingly significant. Em-
ployingbeliefstateinference-basedmethods,ratherthanhistory-basedapproaches,not
only substantially reduces the memory footprint by abstracting detailed histories into
compact probabilistic representations but also improves computational efficiency, de-
spite the initial complexities associated with learning the underlying generative model
andbeliefstaterepresentation.
AppendixI VisualizationsofresultsinSub-section8.1
Inthissub-section,weprovidethelearningcurvesforallthecomparedmethodsinsub-
section 8.1. The average returns are shown in Fig. I.1, with the shaded areas indicating
thestandarddeviation.
(a)HalfCheetah-P (b)HalfCheetah-N
(c)Hopper-P (d)Hopper-N
(e)Ant-P (f)Ant-N
(g)Walker-2d-P (h)Walker-2d-N
Fig. I.1. Mean return for four Roboschool benchmarks with partial observations (left),
andnoisyobservations(right). Shadedareasindicatestandarddeviation.

=== REVISE TO ===
PROFESSIONAL TONE: Begin directly with content - NO conversational openings like 'Okay, here's...'

1. Fix all issues above
2. Title: "Active Inference and Reinforcement Learning: A unified inference on continuous state and action spaces under partial observability"
3. Include 10-15 quotes from paper text
   - Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
   - Use consistent quote formatting: 'The authors state: "quote"' or vary attribution phrases
   - Vary attribution phrases to avoid repetition
   - CRITICAL: Only extract quotes that actually appear in the paper text
4. ELIMINATE ALL REPETITION - each sentence must be unique
   - Check before each sentence: 'Have I already said this?' If yes, write something new
   - Vary attribution phrases - do NOT repeat 'The authors state' multiple times
5. Extract methodology, results with numbers, key quotes
6. 1000-1500 words, structured with ### headers

Generate COMPLETE revised summary.