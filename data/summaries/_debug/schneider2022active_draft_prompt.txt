=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Active Inference for Robotic Manipulation
Citation Key: schneider2022active
Authors: Tim Schneider, Boris Belousov, Hany Abdulsamad

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2022

Abstract: Roboticmanipulationstandsasalargelyunsolvedproblemdespitesignificantadvancesinroboticsandmachinelearning
inthelastdecades. Oneofthecentralchallengesofmanipulationispartialobservability,astheagentusuallydoesnot
knowallphysicalpropertiesoftheenvironmentandtheobjectsitismanipulatinginadvance. Arecentlyemergingtheory
thatdealswithpartialobservabilityinanexplicitmannerisActiveInference. Itdoessobydrivingtheagenttoactina
waythatisnotonlygoal-directedbutalsoinformativeabouttheenvironment. Inthiswork,we...

Key Terms: manipulation, intelligentautonomoussystems, robotic, robot, technicaluniversityofdarmstadt, learning, inference, germany, active, timschneider

=== FULL PAPER TEXT ===

Active Inference for Robotic Manipulation
TimSchneider BorisBelousov HanyAbdulsamad
IntelligentAutonomousSystems IntelligentAutonomousSystems IntelligentAutonomousSystems
TechnicalUniversityofDarmstadt TechnicalUniversityofDarmstadt TechnicalUniversityofDarmstadt
64289Darmstadt,Germany 64289Darmstadt,Germany 64289Darmstadt,Germany
tim@robot-learning.de boris@robot-learning.de hany@robot-learning.de
JanPeters
IntelligentAutonomousSystems
TechnicalUniversityofDarmstadt
64289Darmstadt,Germany
mail@jan-peters.net
Abstract
Roboticmanipulationstandsasalargelyunsolvedproblemdespitesignificantadvancesinroboticsandmachinelearning
inthelastdecades. Oneofthecentralchallengesofmanipulationispartialobservability,astheagentusuallydoesnot
knowallphysicalpropertiesoftheenvironmentandtheobjectsitismanipulatinginadvance. Arecentlyemergingtheory
thatdealswithpartialobservabilityinanexplicitmannerisActiveInference. Itdoessobydrivingtheagenttoactina
waythatisnotonlygoal-directedbutalsoinformativeabouttheenvironment. Inthiswork,weapplyActiveInferenceto
ahard-to-exploresimulatedroboticmanipulationtasks,inwhichtheagenthastobalanceaballintoatargetzone. Since
therewardofthistaskissparse,inordertoexplorethisenvironment,theagenthastolearntobalancetheballwithout
anyextrinsicfeedback,purelydrivenbyitsowncuriosity. Weshowthattheinformation-seekingbehaviorinducedby
ActiveInferenceallowstheagenttoexplorethesechallenging,sparseenvironmentssystematically. Finally,weconclude
thatusinganinformation-seekingobjectiveisbeneficialinsparseenvironmentsandallowstheagenttosolvetasksin
whichmethodsthatdonotexhibitdirectedexplorationfail.
Keywords: Model-BasedReinforcementLearning,RoboticManipulation,Ac-
tiveInference
2202
nuJ
1
]OR.sc[
1v31301.6022:viXra
1 IntroductionandRelatedWork
Acommonbeliefincognitivescienceisthattheevolutionofdexterousmanipulation
capabilities was one of the major driving factors in the development of the human
mind[1]. Performingmanipulationiscognitivelyhighlydemanding,forcingtheactor
toreasonnotonlyabouttheimpactofitsactionsonitselfbutalsoabouttheimpacton
itsenvironment. Thisinherentcomplexityleavesautonomousroboticmanipulationa
largelyunsolvedtopic,despitesignificantadvancesinroboticsandmachinelearningin
thelastdecades.
One of the central challenges of manipulation is partial observability. While we are
manipulatinganobject,werarelyknowallofitsphysicalpropertiesinadvance. Instead,
wemustresorttoinferringthosepropertiesbasedonobservationsandtouch. Todeal
withthisissueaseffectivelyaspossible,humanshavedevelopedvariousactivehaptic
explorationstrategiesthattheyconstantlyapplyduringmanipulationtasks[2].
A recently emerging theory from cognitive science that tries to explain this notion of
constantactiveexplorationisActiveInference(AI)[3]. AIformulatesbothactionand Figure 1: Robot using Active
perceptionastheminimizationofasinglefree-energyfunctional,calledtheVariational Inference to solve a challeng-
FreeEnergy(VFE).Indoingso,Fristonetal.[4]deriveanobjectivefunctionthatconsists ingmanipulationtask.
ofanextrinsic,goal-directedtermandanintrinsic,information-seekingterm. Thecom-
binationofthesetwotermsdrivestheagenttoactinawaythatisbothgoal-directedand
informative,inthattheagentlearnsaboutitsenvironmentthroughitsactions.
Inthiswork,weshowhowAIcanbeusedtolearnchallengingroboticmanipulationtaskswithoutpriorknowledge. For
now,weassumethattheenvironmentisfullyobservableandonlyconsiderepistemicuncertainty1. ToimplementAI
inpractice,weuseaneuralnetworkensembleanddeployModelPredictiveControlforactionselection. Weshowthat
agentsdrivenbyAIexploretheirenvironmentsinadirectedandsystematicway. Theseexploratorycapabilitiesallowthe
agentstosolvecomplexsparsemanipulationtasks,onwhichagentsthatarenotexplicitlyinformation-seekingfail.
RelatedtoourapproachisPETS[5],whichalsotrainsensemblemodelsforthetransitionandrewarddistributionsand
selectsactionswithaCross-EntropyMethodplanner. ThekeydifferencetoourapproachisthatPETSdoesnotusean
intrinsictermandinsteadgreedilyselecttheactionstheypredicttoyieldthehighestreward.
AnapproachsimilartooursisTschantzetal.[6],whoalsotackleRLtaskswithAI.Thedifferencetoourapproachisthat
theyuseadifferentfreeenergyfunctionalusedforplanningandchoseadifferentapproximationoftheirintrinsicterm,
whichrequiresthemtomakeamean-fieldassumptionoverconsecutivestates. Theyevaluatetheirapproachonmultiple
RLbenchmarks,includingMountainCarandCupCatch.
2 ActiveInference
According to the Free Energy Principle (FEP) [3], any organism must restrict the states it is visiting to a manageable
amount. Mathematically,AIimplementsthisrestrictionasfollows: Everyagentmaintainsagenerativemodelpofthe
worldandavoidssensationsothataresurprising,hencehavealowmarginallog-probabilitylnp(o). Thus,theobjective
canbewrittenas
min −lnp(o) (1)
π
whereoisgeneratedbysomeexternalprocessthatcanbeinfluencedbychangingthepolicyπ.
Theagent’sgenerativemodelisassumedtoconsistofnotonlyobservationso,butalsocontainhiddenstatesx,giving
(cid:82) (cid:82)
p(o) = p(o,x)dx = p(o|x)p(x)dx. TomakeEq.(1)tractable, weapplyvariationalinferenceandobtaintheELBO
usingJensen’sinequality:
(cid:90) (cid:90) q (x)
−lnp(o)=−ln p(o,x)dx=−ln φ p(o,x)dx≤D [q (x)(cid:107)p(x|o)]−lnp(o)=:F(o,φ)
q (x) KL φ
φ
whereq (x)isthevariationalposterior,parameterizedbyφ,andF(o,φ)istermedtheVariationalFreeEnergy(VFE)in
φ
theAIliterature.
Minimizing F(o,φ) w.r.t. the variational parameters φ corresponds to minimizing the KL divergence between the
variationalposteriorq (x)andthetrueposteriorp(x|o). Inotherwords,byminimizingtheVFEw.r.t.φ,theagentis
φ
solvingtheperceptionproblemofmappingitsobservationstotheirlatentcauses.
1Epistemicuncertaintyistheuncertaintytheagenthasoveritsmodeloftheworld.Incontrast,aleatoricuncertaintyisuncertainty
overtheagent’sstate.
1
Tofacilitateplanningintothefuture,theVFEcanbemodifiedtoincorporateanexpectationoverfuturestates,yielding
theExpectedFreeEnergy(EFE)Fristonetal.[4]:
G (φ)=−E [lnp(o ,x )−lnq (x |π)]
π qφ(ot+1:T,xt+1:T|π) t+1:T t+1:T φ t+1:T
≈−E [D [q (o|x,π)(cid:107)q (o|π)]]−E [lnp(o)]
qφ(x|π) KL φ φ qφ(o|π)
(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
intrinsicterm(expectedinformationgain) extrinsicterm
whereweomittedsubscriptsforreadabilityanddefinedq (o|x):=p(o|x),suchthatq andpfollowthesameobservation
φ φ
model.
TheminimizationoftheEFEw.r.t.thepolicyπcausestheagenttoactinawaythatmaximizesbothinformationgainand
theextrinsicterm. Here,theextrinsictermactsasanexternalsignalthatallowsustomaketheagentpreferordisprefer
certainobservations. WhileitiscommoninRLliteraturetousearewardfunctiontogivetheagentanotionof“good”
and“bad”behavior,intheAIframework,wedefineapriordistributionovertargetobservationsp(o)thatwewouldlike
theagenttomake. Notethatbymakingtherewardpartoftheobservationandsettingthemaximumrewardastarget
observation[6],wecantransformanyreward-basedtasktofitintotheAIframework.
3 Method
In this work, we propose a model-based Reinforcement Learning algorithm that uses AI to efficiently explore chal-
lenging state spaces. Therefore, we assume that the environment is fully observable, governed by unknown dynam-
ics P(x |x ,a ) and provides the agent with a reward P(r |x ,a ) in every time step. We model both the dy-
τ τ−1 τ τ τ τ
namics and the reward with neural network conditioned Gaussians p(r |x ,a ,θ) := N (x |µx(x ,a ),σxI) and
τ τ τ τ θ τ−1 τ
p(r |x ,a ,θ):=N (r |µr(x ,a ),σrI),resultinginthefollowinggenerativemodel:
τ τ τ τ θ τ τ
T
(cid:89)
p(x ,a ,r ,θ)=p(x )p(a )p(θ) p(r |x ,a ,θ)p(x |x ,a ,θ)
0:T 1:T 1:T 0 1:T τ τ τ τ τ−1 τ
τ=1
Sincetheenvironmentisfullyobserved,theonlyhiddenvariablesaretheneuralnetworkparametersθ. Thus,weareleft
withthefollowingminimizationproblemforselectingapolicyπ :=a attimet:
t+1:T
(cid:34) T (cid:35)
(cid:88)
minG (φ):=−E [D [p(x ,r |θ,π)(cid:107)q (x ,r |π)]]−E r (2)
π
π qφ(θ|π) KL t+1:T t+1:T φ t+1:T t+1:T qφ(rt+1:T|π) τ
(cid:124) (cid:123)(cid:122) (cid:125) τ=t+1
expectedparameterinformationgain (cid:124) (cid:123)(cid:122) (cid:125)
expectedcumulativereward
wherewedefinedtheobservationpreferencedistributionsuchthatp(o
τ
)∝erτ,anddefinedq
φ
(x,r|θ,π):=p(x,r|θ,π).
Hence,bythisdefinition,qandpdifferonlyinthemarginalprobabilityofthemodelparametersθ.
SimilartoothermethodsutilizingModelPredictiveControl[5],byminimizingthisobjectivefunctionweselectapolicy
thatmaximizestheexpectedcumulativerewardoverafixedhorizon. However,additionallywearemaximizingthe
expectedparameterinformationgain,drivingtheagenttoseekoutstatesthatareinformativeaboutitsmodelparameters
θ. Thistermcausestheagenttobecuriousaboutitsenvironmentandexploreitsystematically,eveninthetotalabsence
ofextrinsicreward. Theoptimizationofthisobjectivecannowtheoreticallybedonebyanyplannerthatiscapableof
handlingcontinuousactionspaces. Inthiswork,similartoChuaetal.[5],weuseavariantoftheCross-EntropyMethod
tofindanopenloopsequenceofactionsa thatmaximizesEq.(2).
t+1:T
AmajorchallengeincomputingG (φ)isthatneithertheintrinsic,northeextrinsictermcanbecomputedinclosedform.
π
WhiletheextrinsictermcanstraightforwardlybeapproximatedwithsufficientaccuracyviaMonteCarlo,theintrinsic
termisknowntobenotoriouslydifficulttocompute[7]. Thus,insteadofmaximizingitdirectly,manymethodsmaximize
avariationallowerboundofit[8]. However,duetothehigh-dimensionalnatureofθ,theseapproachesaretooexpensive
tobeexecutedduringplanninginrealtime.
Hence,insteadweproposetouseaNestedMonteCarloestimatorthatreusessamplesfromtheouterestimatorinthe
innerestimatortoapproximatetheintrinsicterm:
n n
1 (cid:88) 1 (cid:88)
IG((x,r),θ)≈ lnp(x ,r |θ )−ln p(x ,r |θ )
n i i i n i i k
i=1 k=1
k(cid:54)=i
(cid:124) (cid:123)(cid:122) (cid:125)
innerestimator
(cid:124) (cid:123)(cid:122) (cid:125)
outerestimator
Althoughusingthesamesamplesθ ,...,θ intheinnerestimatorasintheouterestimatorviolatesthei.i.d. assumption,
1 n
wefoundthisreuseofsamplestoincreasethesampleefficiencysubstantially. Sincethisestimatoronlyrequiressamples
ofθ,werepresentq (θ)byasetofparticlesθ ,...,θ ,makingourmodelaneuralnetworkensemble.
φ 1 n
2
4 ExperimentalResults
A central feature that sets our method apart from other
purelymodel-basedapproaches[5,9]istheintrinsicterm,
thatexplicitlydrivestheagenttoexploreitsenvironment
inasystematicmanner. Toevaluatetheexploratorycapa-
bilitiesofourmethod,wedesignedtwohard-to-explore
manipulationtasks: TiltedPushingandTiltedPushingMaze.
Inbothtasks,theagenthastopushaballupatiltedtable
intoatargetzonetoreceivereward. Theagentcanmove
thegripperinaplaneparalleltothetableandrotatethe
blackend-effectoraroundtheZ-axis(Z-axisbeingorthog-
onal to the brown table and pointing up). As input, the
agentreceivesthe2Dpositionsandvelocitiesofboththe
gripperandtheball,andtheangularpositionandvelocity
oftheend-effector. Toaddanadditionalchallenge,inthe Figure2:Visualizationofthetwoenvironmentconfigurations
Tilted Pushing Maze task we add holes to the table, that wetestourmethodson:TiltedPushing(left)andTiltedPushing
irrecoverablytraptheballifitfallsin. Foravisualization Maze(right). Thetargetzoneismarkedinred.
ofthesetasks,refertoFig.2.
Therearetwoaspectsmakethesetasksparticularlychallenging: First,therewardissparse,meaningthattheonlywaythe
agentcanlearnabouttherewardatthetopofthetableisbymovingtheballthereandexploringit. Second,balancingthe
ballonthefingerandmovingitaroundrequiresafairamountofdexterity,especiallygiventhelowcontrolfrequencyof4
Hz2 weoperateouragenton. Oncetheagentdropstheball,itcannotberecovered,givingtheagentnochoicebutto
waitfortheepisodetoterminatetocontinueexploring. Bothoftheseaspectsmakesolvingthesetaskswithconventional,
undirectedexplorationmethodslikeBoltzmannexplorationoraddingGaussiannoisetotheactionextremelychallenging.
Consequently,theagenthastolearntobalancetheballwithoutreceivinganyextrinsicreward,purelydrivenbyitsown
curiosity.
AsvisibleinFig.3,ourmethodisabletosolvetheTiltedPushing. BothSAC[10]andourmethodwithoutanintrinsicterm
failtofindtherewardwithin10,000episodes. TheholesofTiltedPushingMazemakethisenvironmentsignificantlyharder
toexplore,astheballhastobemaneuveredaroundtwocornersinordertoreachthetargetzone. Inthisexperiment,only
ourmethodfindstherewardwithin30,000episodes. AscanbeseeninFig.4,thereasonforthebadperformanceofthe
non-intrinsicagentisitsfailuretoexplorethefullstatespace. Whileouragentcontinuestosystematicallymaneuverthe
ballaroundtheholesinunseenlocations,thenon-intrinsicagentrarelypassesthelowerholesandleavestheupperhalfof
thetableunexplored.
40
20
0
0 2,000 4,000 6,000 8,000 10,000
Numberofepisodes
drawerevitalumuC
edosiperep
TiltedPushing TiltedPushingMaze
30 Ours Ours
Non-intr. Non-intr.
SAC SAC
20
10
0
0 10,000 20,000 30,000
Numberofepisodes
Figure3: Cumulativeper-episoderewardfortwodifferentversionsofouragent(onewithintrinsicterm,onewithout)
andSAConbothvariantsofourenvironment. Thisgraphdisplaystheevaluationreward,whichisobtainedbyrolling
outthelearnedmodelwithoutconsideringtheintrinsicreward. Bothnon-intrinsicconfigurationsandSACfailedtofind
theobjectiveandconvergedtolocalminima.
Theseexperimentshowthatourmethodisabletosystematicallyexploreacomplex,contact-richenvironmentwithmany
dead-ends. Withoutanyextrinsicfeedback,ouragentslearnedtobalancetheballontheend-effectorandsystematically
moveitaroundtheenvironmentuntilthetargetzonewasfound. Thesolereasonforthisbehaviortooccurinthefirst
2Thecomputationoftheintrinsictermiscomputationallyheavy,limitingustothisratherlowcontrolfrequency.
3
placeisthatouragentsunderstoodtheycouldonlyexploretheentirestatespaceiftheykeptbalancingtheballandmove
ittounseenlocations.
Ours
Non-intrinsic
Episodes 7000 14000 21000 28000 35000
Figure 4: Comparison of the states visited by our method and an agent using no intrinsic term, relying on Gaussian
explorationinstead. Thebrightnessofeachpixelindicateshowoftentheballhasvisitedtherespectivepointofthetableat
thegivenpointinthetraining. Thecoordinateoriginisatthebottomofeachimage,meaningthattheimagesarerotated
180°comparedtothetop-downviewinFig.2. Eachconfigurationwasrunonce.
5 Conclusion
Inthiswork,wedevelopedamethodcapableofapplyingActiveInferencetocomplexReinforcementLearningtasks. We
evaluatedourmethodintwochallengingroboticmanipulationtask,bothdesignedtobeparticularlyhard-to-explore.
Throughoutourexperiments,weshowedthatourmethodinducessystematicexplorationbehaviorandiscapableof
solvingeventhemostchallengingoftheseenvironments. Neitherthenon-intrinsicconfigurationsnorthemaximum
entropymethodSACmanagedtosolvetheroboticmanipulationtasks. Hence,weconcludethattheinformation-seeking
behaviorofouragentsisbeneficialforsolvingchallengingexplorationproblemswithsparserewards.
Finally,infutureworkweplantoapplyourmethodtoarealrobotandevaluatewhetherActiveInferencecanbeusedin
realroboticmanipulationtasks.
References
[1] RobertMacDougall.“Thesignificanceofthehumanhandintheevolutionofmind”.In:TheAmericanJournalof
Psychology16.2(1905),pp.232–242.
[2] AgnesLacreuseandDorothyMFragaszy.“Manualexploratoryproceduresandasymmetriesforahapticsearch
task:Acomparisonbetweencapuchins(Cebusapella)andhumans”.In:Laterality:AsymmetriesofBody,Brainand
Cognition2.3-4(1997),pp.247–266.
[3] KarlJFristonetal.“Actionandbehavior:afree-energyformulation”.In:Biologicalcybernetics102.3(2010),pp.227–
260.
[4] KarlFristonetal.“Activeinferenceandepistemicvalue”.In:Cognitiveneuroscience6.4(2015),pp.187–214.
[5] KurtlandChuaetal.“Deepreinforcementlearninginahandfuloftrialsusingprobabilisticdynamicsmodels”.In:
arXivpreprintarXiv:1805.12114(2018).
[6] AlexanderTschantzetal.“Reinforcementlearningthroughactiveinference”.In:arXivpreprintarXiv:2002.12636
(2020).
[7] DavidMcAllesterandKarlStratos.“Formallimitationsonthemeasurementofmutualinformation”.In:International
ConferenceonArtificialIntelligenceandStatistics.PMLR.2020,pp.875–884.
[8] BenPooleetal.“Onvariationalboundsofmutualinformation”.In:InternationalConferenceonMachineLearning.
PMLR.2019,pp.5171–5180.
[9] DanijarHafneretal.“Learninglatentdynamicsforplanningfrompixels”.In:InternationalConferenceonMachine
Learning.PMLR.2019,pp.2555–2565.
[10] TuomasHaarnojaetal.“Softactor-critic:Off-policymaximumentropydeepreinforcementlearningwithastochastic
actor”.In:Internationalconferenceonmachinelearning.PMLR.2018,pp.1861–1870.
4

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Active Inference for Robotic Manipulation"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
