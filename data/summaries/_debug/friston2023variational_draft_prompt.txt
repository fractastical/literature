=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: A variational synthesis of evolutionary and developmental dynamics
Citation Key: friston2023variational
Authors: Karl Friston, Daniel Ari Friedman, Axel Constant

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2023

Abstract: This paper introduces a variational formulation of natural selection, paying special
attention to the nature of ‘things’ and the way that different ‘kinds’ of ‘things’ are individuated
from—and influence—each other. We use the Bayesian mechanics of particular partitions to
understand how slow phylogenetic processes constrain—and are constrained by—fast,
phenotypic processes. The main result is a formulation of adaptive fitness as a path integral of
phenotypic fitness. Paths of least action, at t...

Key Terms: synthesis, dynamics, developmental, variational, university, natural, processes, evolutionary, phenotypic, gmail

=== FULL PAPER TEXT ===

A variational synthesis of evolutionary and
developmental dynamics
(Dedicated to the memory of John O. Campbell)
Karl Friston1, Daniel A. Friedman*2,3, Axel Constant4, V. Bleu Knight3,5, Thomas Parr1, John
O. Campbell6
1 Wellcome Centre for Human Neuroimaging, Institute of Neurology, University College London,
London, UK
2Department of Entomology and Nematology, University of California, Davis, Davis, CA, United States
3Active Inference Institute
4Theory and Method in Biosciences, The University of Sydney, Sydney, NSW, Australia
5Department of Biology, New Mexico State University, Las Cruces, NM. United States
6Independent Researcher, Victoria, BC. Canada
Corresponding author*: Daniel Friedman
Emails: k.friston@ucl.ac.uk, danielarifriedman@gmail.com, axel.constant.pruvost@gmail.com,
virginiableuknight@gmail.com; thomas.parr.12@ucl.ac.uk, jockocampbell@gmail.com
Keywords: self-organisation, nonequilibrium, variational inference, Bayesian, particular partition,
evolution, natural selection, Markov blanket, renormalisation group.
Abstract
This paper introduces a variational formulation of natural selection, paying special
attention to the nature of ‘things’ and the way that different ‘kinds’ of ‘things’ are individuated
from—and influence—each other. We use the Bayesian mechanics of particular partitions to
understand how slow phylogenetic processes constrain—and are constrained by—fast,
phenotypic processes. The main result is a formulation of adaptive fitness as a path integral of
phenotypic fitness. Paths of least action, at the phenotypic and phylogenetic scales, can then be
read as inference and learning processes, respectively. In this view, a phenotype actively infers
the state of its econiche under a generative model, whose parameters are learned via natural
1
(Bayesian model) selection. The ensuing variational synthesis features some unexpected
aspects. Perhaps the most notable is that it is not possible to describe or model a population of
conspecifics per se. Rather, it is necessary to consider populations—and nested meta-
populations—of different natural kinds that influence each other. This paper is limited to a
description of the mathematical apparatus and accompanying ideas. Subsequent work will use
these methods for simulations and numerical analyses—and identify points of contact with
related mathematical formulations of evolution.
Introduction
This paper is an attempt to show that some fundaments of theoretical evolution—and
(neuro)biology—emerge when applying the free energy principle to dynamical systems with
separation of temporal scales. It offers a technical and generic treatment with minimal
assumptions or commitments to specific biological processes. As such, it does not borrow from
established constructs in evolutionary theory; rather, it tries to show how some of these
constructs are emergent properties, when seen through the lens of the free energy principle. In
subsequent work, we will use the ensuing variational synthesis to consider established—and
current—evolutionary theories. Our aim in this paper is to introduce a formalism that may be
useful for addressing specific questions—about evolutionary or developmental dynamics—
using analytic or numerical recipes that have proven useful when applying the free energy
principle in other fields.
A key phylogenetic process—underlying the development and diversification of
species in evolutionary time—is known as natural selection, regarded by some as the central
organizing principle of biology. While Darwin conceived of natural selection in terms of
heredity, variation, and selection [1,2], he only detailed selection, as the mechanisms of
heredity and variation would not be understood for some time [3,4]. The integration of
Mendelian genetics with natural selection in the early twentieth century was followed by an
integration with molecular genetics [5] in the mid-century to form Neo-Darwinism, or the
modern synthesis. The modern synthesis, along with the selfish gene hypothesis—put forth in
the 1970s [6]—provide a largely gene-centric view of Darwinian evolution that dominates the
current perspective.
2
This gene-centric view of evolutionary biology has remained largely disconnected from
phenotypic processes that impact organisms in developmental time [7,8]. Lewontin
characterised this disconnect—between genetic and phenotypic understanding—as the major
challenge facing the field [9]. While some progress has been made in the following fifty years,
biologists continue to highlight the gaps remaining for modelling biology as a single integrated
process over multiple scales [10–13]. By ‘gene-centric’ we refer not just to theories of sequence
evolution [14], but also to the central role genes (or summary statistics of genes) play either
explicitly or implicitly in accounts of phenotypic evolution. For instance, the Price Equation
[15] and the, closely related, replicator equation [16] of evolutionary game theory express the
relationship between the change in (the average of) some phenotypic trait over time. This
implicitly relies upon a mapping between that trait and the genetic material passed from
generation to generation but focuses upon the phenotypic effects of genes as opposed to the
alleles themselves. Similarly, adaptive dynamic approaches [17] typically focus upon
ecological interactions at a phenotypic level. The modern focus upon phenotypic traits reflects
the importance of the interaction between a phenotype and its environment in determining
fitness. However, it is important to note that such perspectives do not conflict with the central
role of genetic inheritance, and implicitly score the fitness of genotypes in terms of the
phenotypes they imply.
An organism inherits a set of instructions for growth and development (i.e., an extended
genotype) that is, in essence, a prediction about the niche environment (including temperature,
humidity, chemical composition, available resources, statistical patterns, etc.). Interrogating
the phrase ‘survival of the fittest,’ leads to the understanding of ‘fittest’ as organisms that are
the best ‘fit’ to their niche environment [18]. For example, a bacterium from thermal hot springs
will fail to thrive in a cool pond because its genotype does not accurately predict the niche
environment. Therefore, “fittest” must be relative to the niche. A series of slow phylogenetic
processes has selected for an extended genotype that enhances the growth and proliferation of
organisms in the environment where the corresponding species expects to find itself.
An organism can also ‘fit’ itself to the niche through adaptation (i.e., action, learning,
and development) during its lifetime. For example, a bacterium that normally subsists on
sulphur reduction—but can also survive through reducing oxygen—will outlast its sulphur-
dependent competitors in an environment that is devoid of sulphur. Such an organism can adapt
to its environment through learning and optimising for oxygen reduction, thereby increasing
its fit to the niche, and, implicitly, its capacity to reproduce in a high-oxygen environment. In
3
this way, the phenotypic processes can enhance the fit of organisms to their environment in
developmental time, and through reproduction, phenotypic processes can lead to the
enhancement of fit in evolutionary time (i.e., across generations). As the (extended) genotype
of organisms produces phenotypes, phylogenetic processes over evolutionary time also impact
phenotypic (ontogenetic) processes in developmental time.
Here, we offer a synthesis of evolution and development through a mathematical
framework that unifies slow, multi-generational (phylogenetic) processes with single-lifetime,
phenotypic (developmental and behavioural) processes using the same principles, as they apply
to each temporal scale. The ensuing variational account of evolution focuses on the coupling
between phylogenetic processes at evolutionary timescales and ontogenetic processes over
phenotypic lifetimes. In principle this—relatively abstract—treatment is agnostic to specific
mechanisms, and could be applied to biological as well as non-biological systems provided
their ‘fitness’ depends upon events during a lifetime, and where this fitness influences
dynamics over a generational scale. This foregrounds the circular causality that arises from the
implicit separation of timescales [19].
In brief, we treat the slow phylogenetic process (natural selection) as furnishing top-
down constraints (i.e., top-down causation) on fast phenotypic processes (action selection). In
turn, the active exchange of the phenotype with its environment provides evidence that is
assimilated by natural selection (i.e., bottom-up causation). This multi-scale ontological
account is licensed by describing both phylogenetic and phenotypic processes as selecting
(extended) genotypes and (extended) phenotypes [7,20] with the greatest fitness; where fitness
is quantified with (free energy) functionals of probability density functions (a functional is a
function of a function).
This formulation means that both natural selection and intra-generational action
selection can be described as updating probabilistic beliefs: namely, learning and inference
[21–23]. This affords an interpretation of natural selection as Bayesian model selection [24–
26], while action selection becomes planning as inference [27–30]—both (appearing to)
optimise the same fitness functional: namely, Bayesian model evidence or marginal likelihood.
A narrative version of this (multiscale) account can be told from the point of view of the
genotype (from the bottom-up) or the phenotype (from the top-down):
4
● From the perspective of the genotype, we can consider evolution as belief-updating over
generations, where the belief in question corresponds to a probability density over extended
genotypes (henceforth, genotype). This is analogous to treatments of evolution in terms of
changes in allele frequencies from generation to generation [15]. This belief updating can
be described by the probability of a genotype appearing in subsequent generations, in a way
that depends lawfully on the marginal likelihood of extended phenotypes (henceforth,
phenotype) in the current generation. The basic idea is that the genotype parameterises or
encodes a generative model, which the phenotype uses to infer and act on its environment.
On this view, evolution can be regarded as testing hypotheses—in the form of generative
models—that this kind of phenotype can persist in this environment. These hypotheses are
tested by exposing the phenotype to the environment and are rejected if the phenotype
‘strays from the path’ of a persistent phenotype. In this way, the evolutionary process
selects models or hypotheses about persistent phenotypes for which it has the greatest
evidence. In short, natural selection just is Bayesian model selection [25,26,31,32].
● From the point of view of a phenotype, each conspecific is equipped with a generative
model and initial/prior/inherited conditions that underwrite its epigenetic, developmental
and ethological trajectories. The states of the phenotype trace out a path through state-space
over its lifetime. These phenotypic states encode or parameterise beliefs about
environmental states—and the way the phenotype acts. This leads to active inference and
learning, in which the phenotype tries to make sense of its world and—through a process
of belief updating—to realise the kind of creature it thinks it is. We use the term ‘thinks’ in
a loose (anthropomorphic) sense here and do not mean to imply that all living entities have
explicit existential thoughts. More precisely, what we mean is that these entities behave as
if they implicitly hold a set of beliefs about the sort of entity they are (e.g., the meta-
Bayesian stance as considered in [33]). In virtue of its genetic endowment, it thinks it is a
persistent phenotype. If endowed with a good generative model of its environment [34], it
will persist and supply evidence of its ‘fit’ to the environment (i.e., fitness); namely,
evidence (i.e., marginal likelihood) that has been accumulated by the slow evolutionary
process.
What follows is a formal version of this narrative that calls upon some standard results
from statistical physics. The resulting synthesis is both dense and delicate, because it tries to
account for coupling between a phenotype and its econiche—and the coupling between
5
phenotypic and phylogenetic processes—using the same principles. Specifically, we describe
a variational synthesis that calls on the path integral formulation of stochastic dynamics, the
apparatus of the renormalisation group, and the Poincaré recurrence theorem. The ensuing
synthesis considers natural selection and action selection as emergent properties of two random
dynamical processes unfolding at slow (phylogenetic) and fast (phenotypic) timescales. The
key aspect of this synthesis is that both processes have an attracting set (a.k.a., pullback
attractor) or steady-state solution [35]. These solutions correspond to an evolutionary stable
state [36] and a nonequilibrium steady-state density [37] over phylogenetic and phenotypic
states, respectively. By describing these steady states in terms of a phylogenetically encoded
generative model—namely, a joint density over the paths of the phenotype and its
environment—one can recover an ontological description of how the two processes inform,
and are informed by, each other.
Much of the analysis presented in this paper follows that in [21–23], which also appeals
to the notion of a renormalization group. These treatments are based upon the emergence of
separable timescales and the interpretation of the dynamics at each scale in analogy with
inference and learning processes. The key differences are as follows. The renormalization in
[21] depends upon a reduction in the number of degrees of freedom with learning, whereas our
formulation depends upon a partitioning operation as part of the renormalization. The
differences in timescales between variables in [21] emerges from the structure of the neural
network used, whereas it is a direct consequence of the reduction operator implicit in our choice
of renormalization. Finally, we extend our analysis to sentient phenotypes, whose dynamics
can be interpreted explicitly in terms of Bayesian belief-updating.
A variational formulation
We assume that evolution can be described with two random dynamical systems,
describing phylogenetic (evolutionary) and phenotypic (particular) processes, respectively.
The idea is to couple these systems using the apparatus of the renormalisation group [38–40].
This implies a mapping from fast phenotypic dynamics to slow phylogenetic dynamics in
evolutionary time.
This mapping rests upon a dimension reduction and coarse graining or grouping
operator (RG for Renormalization Group) that maps the path of a phenotype  to relevant
6
variables at the evolutionary scale  
7
= R . On this view, bottom-up causation is simply the
application of a reduction operator,  R to select variables that change very slowly. Top-
down causation entails a specification of fast phenotypic trajectories in terms of slow genotypic
variations, which are grouped into populations,  G , according to the influences they exert
on each other. The implicit separation into fast and slow variables can be read as an adiabatic
approximation [41] or—in the sense of synergetics—into fast, dynamically stable and slow,
dynamically unstable modes, respectively [42]. This separation can also be seen in terms of
vectorial geometric formulations [43]. Please see [21], who deal carefully with the separation
of time scales, by analogy with temporal dilation in physics. Intuitively, this analogy rests upon
the idea that time can be rescaled, depending upon whether we take the perspective of things
that move quickly or slowly.
The final move is to express the dynamics—at fast and slow levels—in terms of
functionals that have the same form. These functionals are functions of probability densities
that can be read as Bayesian beliefs. Expressing the dynamics in this way allows one to interpret
phenotypic dynamics as active inference and learning, under a generative model that depends
on the extended genotype. This also allows one to interpret the phylogenetic state as inferring
states of the environment over evolutionary time. Crucially, the extended genotype
accumulates evidence for its phenotype; thereby evincing a form of Bayesian model selection
or structure learning [25,44–48]. For an analogous thermodynamic treatment, please see [22],
who refine and extend the free energy formulation of [49]. In context of learning dynamics, a
thermodynamic free energy was derived in [50]—using the maximum entropy principle
[51,52]—and later applied to study phenomenological models of evolution [22]. Please see
[50,53,54] for further discussion in terms of neural networks and information theory.
Particular partitions
There are lots of moving parts in this formulation because it tries to account for the
behaviour of ‘things’ [55] and how this behaviour underwrites the emergence of ‘kinds’ (e.g.
individuals and populations) at nested (i.e., developmental and evolutionary) timescales.
We will use [ x ( t ) ]  x to denote the history or path of a time varying state. These paths
are determined by state-dependent flow f
x
( x ) , with parameters x  x that include initial
states x(0)= x  x . These parameters denote a (natural) kind.
0
Everything that follows rests upon a particular partition of states. A particular partition
is considered necessary to talk about ‘things’, such as a ‘phenotype’ or ‘population’. In brief,
a particular partition enables the (internal) states of some ‘thing’ to be separated from the
(external) states of every ‘thing’ else by (sensory and active) blanket states [56–60]. In the
absence of this partition, there would be no way of distinguishing a phenotype from its external
milieu—or a population from the environment. In this setup, external states can only influence
themselves and sensory states, while internal states can only influence themselves and active
states: see Figure 1 for an influence diagram representing the coupling among internal, external,
and blanket states:
States: x=(,s,a,). States comprise the external, sensory, active and internal states of a
phenotype. Sensory and active states constitute blanket states b=(s,a), while phenotypic
states comprise internal and blanket states,
8
( b , ) ( s , )    = = . The autonomous states of a
phenotype ( a , )   = are not influenced by external states:
● External states respond to sensory and active states. These are the states of a
phenotype’s external milieu: e.g., econiche, body, or extracellular space, depending
upon the scale of analysis.
● Sensory states respond to fluctuations in external and active states: e.g., chemo-
reception, proprioception, interception, et cetera.
● Active states respond to sensory and internal states and mediate action on the
environment; either directly or vicariously through sensory states: e.g., actin filaments,
motor action, autonomic reflexes, et cetera.
● Internal states respond to sensory and active states: e.g., transcription, intracellular
concentrations, synaptic activity, et cetera.
The evolution of these sparsely coupled states can be expressed as a Langevin or stochastic
differential equation; namely, a high dimensional, nonlinear, state-dependent flow plus
independent random (Wiener) fluctuations, , with a variance of 2Γ:
 f (,s,a)  
 
     
s f (,s,a) 
x= f (x)+=  =  s  +  s  (1)
x a f (s,a,)  
   a   a
  f

(s,a,)  


The flow per se can be expressed using the Helmholtz-Hodge decomposition [61] as follows:
9
f
f
f
f
s
a
(
(
(
(
b
b
f (
x
, b
, b
,
,
x
)
)
)
)
) ( Q
Q
Q T
)
s
(
Q
x )
Q
s
s
s
Q
a
Q Ta
a
Q
Q
a
s
a
(
(
(
(
b
b
|
|
|
|
b
b
)
)
)
)
 
    
 

 

 
    
 
=
=

−
−

− 
 
− 
−
− 
− 
  








(2)
Note that our appeal to an equation of this form means with have implicitly stipulated that there
is a steady state density or potential function, which remains constant (or at least changes very
slowly) over the timescale we are interested in. Equation (2) expresses the flow as a mixture of
a dissipative, gradient flow and a conservative, solenoidal flow [62–64]. The gradient flow
   depends upon the amplitude of random fluctuations, while the solenoidal flow Q  
circulates on the isocontours of potential function called self-information,  ( x ) = − l n p ( x ) ,
where p ( x ) is called the nonequilibrium steady-state density or NESS density [37,65–67].
In principle, other partitioning schemes could have been employed. Given that many
schemes of this sort also rely upon identifying sparsity in the coupling between dynamic
variables [68,69] some of the key results of this paper would be unchanged by choosing an
alternative grouping operator. The potential downsides of alternative partitions include the loss
of the clear relationship between things and their environments—the ‘action-perception cycle’
[70] —and the role this relationship has in formulations of inference and learning. Methods
like that in [69], which pay careful attention to the relationship between (active) inputs,
(external) states, (sensory) outputs, and (internal) controllers that receive the outputs and
generate the inputs preserve approximately the same sort of partition as we have employed.
The terms ‘external’ and ‘internal’ offer useful intuitions, but it is worth being cautious
about overinterpreting these labels in spatial terms. For instance, it might seem that some
‘external’ variables like ambient temperature might directly influence ‘internal’ variables like
the temperature within a cell. However, this would not be an appropriate way of thinking about
this system’s partition. Either we would have to assume that there is an intervening variable
(e.g., the temperature within the cell membrane) or we would have to treat the internal
temperature as a sensory variable, which itself influences internal variables like the rates of
enzymatic reactions. There is now an emerging literature asking about the appropriate ways to
think of particular partitions in biology, including what is internal to a neuronal network [71],
or a spinal reflex arc [72].
Figure 1: schematic (i.e., influence diagram) illustrating the sparse coupling among states that
constitute a particular partition at two scales
Ensemble dynamics and paths of least action
To describe dynamics at the phenotypic or phylogenetic scale, we first need to rehearse
some standard results from statistical physics that furnish a probabilistic description of
trajectories or paths at any scale. This description calls on the self-information of states x(t),
generalised states
10
x = ( x , x , ) , and paths, x = [ x ( t ) ] , where Dx =(x,x, ) denotes
generalised notion, and 2Γ is the covariance of generalised random fluctuations:
11
 (
(
(
x
x
x
)
)
)
=
=
=
−
−
−
l
l
l
n
n
n
p
p
p
(
(
(
x
x
x
)
|
|
x
x
0
0
)
)
=
=
12

[
d
l n
t
| Γ
( x
|
)
+ ( D x − f ( x ) )  12
Γ
( D x − f ( x ) ) +   f ] (3)
The first measure  ( x ) is the self-information or surprisal of a state; namely, the implausibility
of a state being occupied. When the state is an allele frequency and evolves according to
Wright–Fisher dynamics, this is sometimes referred to as an ‘adaptive landscape’ [73]. The
second, ( x ) is the Lagrangian, which is the surprisal of a generalised state; namely, the
instantaneous path associated with the motion from an initial state. In generalised coordinates
of motion, the state, velocity, acceleration, etc are treated as separate (generalised) states that
are coupled through the flow [74,75]. Finally, the surprisal of a path ( x ) is called action,
namely, the path integral of the Lagrangian.
Generalised states afford a convenient way of expressing the path of least action in
terms of the Lagrangian
x
( x ) ( x x ) 0 x x
x
( x ) x ( ) x
x
( x )   + − D =  − D = −   = D −  (4)
The first equality resembles a Lagrange equation of the first kind that ensures the
generalised motion of states is the state of generalised motion. Alternatively, it can be read as
a gradient descent on the Lagrangian, in a moving frame of reference (second equality). When
the Lagrangian is convex, solutions to this generalised gradient descent on the Lagrangian
(third equality) necessarily converge to the path of least action. Denoting paths of least action
with boldface:
x
x
(
(
f (
)
)
)
0
0
a
a
r g
r g
m
m
i n
i n
x
x
(
(
x
x
)
) 
x


= D

x =
x
x
x
=
=


x
x
=
=
(5)
Convergence is guaranteed by the quadratic form (i.e., convexity) of the Lagrangian,
which inherits from Gaussian assumptions about random fluctuations. This is sometimes
described as convergence to the path of least action, in a frame of reference that moves with
the state of generalised motion [76].
We can also express the conditional independencies implied by a particular partition
using the Lagrangian of generalised states. Because there are no flows that depend on both
internal and external states, external and internal paths are independent, when conditioned on
blanket paths:
12
2 f
0
2
0 ( , | s , a ) ( | s , a ) ( | s , a ) ( ) | s , a , x
0
    
  


= 



=  = +  ⊥ (6)
In other words, blanket paths furnish a Markov blanket over internal paths. We will use
this result later to disambiguate the role of active and sensory dynamics in sentient behaviour—
i.e., active inference—of a phenotype. First, we have to establish a formalism for ensembles or
populations of phenotypes. Here, we draw on the apparatus of the renormalisation group.
Different kinds of things
To deal with multiple ‘things’ (e.g., particles, phenotypes and populations), we first
introduce a grouping operator G that partitions the states at the i-th scale of analysis into N
particles on the basis of the sparse coupling implied by a particular partition. In other words,
we group states into an ensemble of particles, where each particle has their own internal and
blanket states. With a slight abuse of the set builder notation:
{ (i)
1
, , (i)}
N
{ x (i)
1
, , x (i)
j
,
(i) sn
, x (i)
k
, x (i) ,
a
,
(i) n
(i)
n
x (i)
m
, x (i)
o
, ,
(i) n
x (i)
p
, }  


= G (7)
The grouping operator means the external states of a given particle are the (blanket)
states of remaining particles that influence it. See [55] for a worked example and numerical
analysis. This enables us to express the dynamics of each particle in terms of its sensory
states—that depend upon the blanket states of other particles—and autonomous states—that
only depend upon the states of the particle in question:
f
f
sn
n
n
s
Q
n
n
sn
0
f
sn
sn
f
( b
n
Q
n
(
,
b
n
n
,
0
, b
n
N
)
n
)
n
sn
n
( b
1
(
,
n
|
, b
s
n
N
)
)
 
 


   
 
=
=



−
=


− 



+





(8)
At this point, we pause to consider that the states in the particular ensemble have to be
the states of some ‘thing’: namely, the states of a particle at a lower scale. This means that
states must be the states of particles (e.g., phenotypic states) that constitute the particular states
at the next scale (e.g., phylogenetic states) This recursive truism can be expressed in terms of
grouping G operator—that creates particles—and a reduction R operator—that picks out
certain particular states for the next scale:
13
{ x (i)} { (i)}
n
{ x (i
n
1 )} { (i
m
1 )}   ⎯ R⎯ → ⎯ G⎯ → ⎯ R⎯ → + ⎯ G⎯ → + ⎯ R⎯ → (9)
The composition of the two operators can be construed as mapping from the states of
particles at one scale to the next or, equivalently, from particular states at one scale to the next.
In short, creating particles of particles, namely, populations. See Figure 2.
{ ( ) (i
1
1 ) , , ( ) (iM 1 )} {
{
{
x (i)}
(i)}
n
(i)
1
, ,
{
{
(i 1 ) x }
n
(i 1 )}
m
(i) , ,
j
(i 1) sm
(i) ,
k
(im 1)
(i) ,
(im
,
1)
(i)
m
 
      


+ + =
⎯
⎯
G
R⎯
G⎯
G ⎯→
R ⎯→
R
{ (i
m
⎯
⎯
(i)
n
1 )}
R⎯
G⎯
G
R
R
⎯→
⎯→
{
+
+
+
(i)
n
(i)}
n
⎯
⎯
R
R⎯
G⎯
G ⎯→
R ⎯→
R
+
+
R , }
 
  +
=
=
R
G
(10)
The reduction operator R typically selects relevant variables whose slow fluctuations
contextualise dynamics at the scale below. Here, R simply recovers the states of a particle that
are time invariant, or vary slowly with time (i.e., the initial states and flow parameters). This
instantiates a separation of timescales, such that the lifetime of a particle (e.g., phenotype)
unfolds during an instant from the perspective of the next scale (e.g., evolution). The separation
of timescales could have been achieved without the grouping (partitioning) operator. We could
simply have projected onto the eigenvectors of a dynamical system’s Jacobian—effectively,
taking linear (or nonlinear) mixtures of our system to arrive at fast and slow coordinates.
However, all we would be left with are fast and slow continuous variables which have nothing
of the character of the individuals, phenotypes, or populations in a system. In short, the
grouping operator is key in identifying fast and slow ‘things’—as opposed to just fast and slow
coordinates of a dynamical system.
Figure 2: schematic showing the hierarchical relationship between particles at scales i and i+1. For
clarity, only sensory and autonomous states are illustrated in blue and pink, respectively. Note that each
variable is a (very large) vector state that itself is partitioned into multiple vector states. At scale i+1,
each particle represents an ensemble (e.g.,
14
(im 1)  + is population m), the elements of which are partitioned
into autonomous and sensory subsets (e.g., (i+1) is the n-th autonomous genotype from population m).
mn
At scale i, each particle represents an element of an ensemble (e.g., (i)  is the -th phenotype), which
is itself partitioned into sensory and autonomous subsets. The slow states of each element (e.g.,
phenotype) are recovered by the reduction operator R, to furnish the states at the ensemble level (e.g.,
genotype). A key feature of this construction is that it applies recursively over scales.
In short, the renormalisation group operator creates particles of particles, retaining only
particular variables that change very slowly and then grouping them according to their sparse
coupling. This means that particles increase in their size from one scale to the next—in virtue
of the grouping of particles at the lower scale—and change more slowly—in virtue of the
coarse graining afforded by temporal reduction.
In an evolutionary setting, the existence of steady-state solutions—implicit in the
Langevin formalism above—means that phenotypic dynamics possess a pullback attractor.
This means their paths will return to the neighbourhood of previously occupied states. In other
words, their ‘lifecycle’ will intersect with some Poincaré section in phenotypic state-space
(possibly many times). We will take this intersection to be a mathematical image of persistence,
which is underwritten by the flow parameters at any point in evolutionary time.
At the phylogenetic scale we have a partition into populations of phenotypes based
upon which phenotypes influence each other. At this slow scale, states can be read as
characterising the ‘kind’ of ‘thing’ that has particular states at the scale below. We will
therefore refer to states at this level as (natural) kinds; noting that the ‘kind of thing’ in question
does not change at the fast scale. We can now rehearse the particular partition at the
phylogenetic scale, noting that for a population to exist, it must have a particular partition.
Here, a population corresponds to a set of particular kinds (i.e., sensory and autonomous kinds):
Kinds:x(i+1) =(,s,a,). These include external, sensory, active, and internal kinds.
● External kinds of particles or phenotypes outside the population that change as a
function of themselves, sensory and active kinds: c.f., the target of niche construction,
from a molecular through to a cultural level, depending upon the scale of analysis
[77,78].
• Sensory kinds mediate the effects of external kinds on the internal members of the
population in question: e.g., nutrients or prey
• Active kinds mediate the effects of internal kinds on external kinds: e.g., agents who
mediate niche construction, from a molecular through to a cultural level, depending
upon the scale of analysis.
• Internal kinds influence themselves and respond to changes in sensory and active
kinds.
This concludes our formal setup. Next, we consider the coupling between fast
phenotypic and slow phylogenetic dynamics. As in other applications of the free energy
principle, this coupling emerges as a property of any phylogenetics that possesses an
evolutionary steady-state. In other words, the idea here is to identify the properties of a system
that exists; as opposed to identifying the properties that underwrite existence. We will see that
the emergent properties look very much like natural selection.
15
Natural selection: a variational formulation
To specialise particular partitions to natural selection, we will associate autonomous
(active and internal) kinds with the (extended) genotypes that constitute a population of agents,
noting that there is no requirement for agents to belong to the same equivalence class—they
just interact, in virtue of the sparse coupling that defines their grouping into a population. For
example, some agents could be animals and others could be plants.
At the phylogenetic scale, an agent is an autonomous kind from a particular population.
At the phenotypic scale, the agent has particular (phenotypic) states, whose dynamics or paths
depend upon its (genotypic) kind. For ease of notation, we will deal with a single population
where the phenotypic state of the n-th agent,
16
(i
n
1 )  + will be denoted by (i)  (i.e., dropping the
m in Figure 2). With this formalism in place, we can formulate the coupling between
phenotypic and phylogenetic dynamics with the following lemma
Lemma 1 (variational fitness): if, at non-equilibrium evolutionary steady-state, the likelihood
of an agent’s genotype (i
n
1 ) (i)   + = R is proportional to the likelihood of its phenotypic
trajectory (i)  (where \ denotes exclusion),
(in 1 ) (
(
(i 1
n
(i)
)
|
|
x
(i
(i)
1 ) \ (i
n
(i 1 )
1 )
)
)
(i)
  
 
 + =
=
 + +
 +
+
=
(11)
then the following holds:
An agent’s autonomous dynamics can be cast as a gradient descent on a Lagrangian,
whose path integral (i.e., action) corresponds to negative fitness. This Lagrangian depends
upon the flow parameters (and initial states) supplied by the genotype. The agent’s genotype
can then be cast as a stochastic gradient descent on negative fitness. This emphasises the
relationship between gradients on fitness (selection) and the stochastic terms that are
uncorrelated with selection (drift):
Fast (c.f., phenotypic) dynamics Slow (c.f., phylogenetic) dynamics
(i) =D(i) − (i) (i+1) =(Q(i+1) −(i+1)) (i) +(i+1)
(i) n  n  n  n (i+1) n (12)
(i) = ((i) |x(i) (i+1)) (i) =dt ()(i)
Lagrangian (c.f., surprisal) Action (c.f., adaptive fitness)
Formally, the generalised gradient descent at the phenotypic scale corresponds to Bayesian
filtering or inference [76] that maximises the marginal likelihood of phenotypic paths. This is
almost tautological, in that it says that deviations from the most likely developmental trajectory,
given some genotype, are unlikely. An additional subtlety here is that the Lagrangian, which
plays the role of a (negative) Lyapunov function, is a function of sensory states. The implication
is that the gradients are not static, but themselves change based upon the way in which the
environment interacts with a creature during its development. The stochastic gradient descent
at the phylogenetic scale corresponds to Bayesian learning via stochastic gradient Langevin
dynamics [79], equipped with solenoidal mixing [80].
On this Bayesian reading, phenotypic dynamics infer their external dynamics, under a
probabilistic model of how external dynamics generate phenotypic dynamics. Intergenerational
genetic changes can be seen as learning the parameters of a generative model, given the
Bayesian model evidence supplied by the scale below (e.g. extended phenotype). This reading
rests upon the action (i.e., negative fitness) scoring the accumulated evidence
17
p ( | x )  for a
phenotype’s generative model, p ( , | x )  encoded by the extended genotype x . This evidence
is also known as a marginal likelihood because it marginalises over external dynamics; i.e.,
other agents.
Proof: the condition in (11) means that the probability of finding an agent of a particular kind
is proportional to the likelihood of its phenotypic path; namely, the likelihood a phenotype
keeps to the ‘trodden path’, characteristic of the ‘kind’ of ‘things’ that persist. The existence of
a nonequilibrium evolutionary steady-state solution to the density dynamics (at both scales)
allows us to express the fast and slow dynamics of agents and their autonomous states in terms
of Helmholtz-Hodge decompositions. From (1) and (2), we have
(i) =(Q(i) −(i)) (i) +(i) (i+1) =(Q(i+1) −(i+1)) (i+1) +(i+1)
  (i)   (i+1) (13)
(i) =((i) |x(i) (i+1)) (i+1) =((i+1))
The gradients of surprisal at the slow scale, with respect to any given agent’s ‘kind’ or
genotype, are the gradients of action by (11):
 (i+1) = (i+1) = (i) (14)
(i+1) (i+1) n (i+1)
n n n
Substituting (14) into (13) gives the slow, phylogenetic dynamics in (12) (ignoring certain
solenoidal terms).
For the fast, phenotypic dynamics, we assume that random fluctuations vanish to
describe phenotypes that possess classical (i.e., Lagrangian) mechanics, i.e., that are dominated
by conservative or solenoidal dynamics. In the limit of small fluctuations, the autonomous
paths become the paths of least action; i.e., when the fluctuations take their most likely value
of zero. From (4), the autonomous paths of least action are as follows (setting
18
1  = ):
(i) (i)
(i)
( (i) | x (i) )   

= D −  (15)
Substituting (15) into (13) gives the fast dynamics in (12) 
Remarks: Note that the extended genotype x (i) { (i) , (i)} (i 1 )    =  + includes the initial states
of the extended phenotype. In other words, the extended genotype covers both the genetic and
epigenetic specification of developmental trajectories and the initial conditions necessary to
realise those trajectories, including external states (e.g., conditions necessary for
embryogenesis), ( 0 ) (i) (i)    .
A useful intuition as to the biological role of the Lagrangian function in Equation 11 is
that it specifies the states (or trajectories) of a system that has achieved homeostasis. The
function will return a small value when physiological measurements are within homeostatic
ranges, and increasingly large values as deviations from these ranges become larger. The
conditioning upon slow (genotypic) variables means that different sorts of homeostatic ranges
are allowable for different sorts of creatures. The relationship between the (fast) action and
(slow) Lagrangian in Equation 11 implies that phenotypic trajectories—in which homeostasis
is maintained—are associated with genotypes that are more likely to be replicated. To some
extent, this is a simplification. More precisely, the Lagrangian favours (i.e., its integrated value
is smaller for) those trajectories in which opportunities for replication are attained—and
successful maintenance of homeostasis is only one aspect of this.
The suppression of random phenotypic fluctuations does not preclude itinerant
trajectories. Indeed, it foregrounds the loss of detailed balance and accompanying nonequilibria
that characterise phenotypic and population dynamics [81–83]; for example, biorhythms and
chaotic oscillations at the phenotypic scale [84–88] or Red Queen dynamics at the phylogenetic
scale [83,89,90]. A system that has the property of detailed balance is one in which time-
reversal makes no qualitative difference to the dynamics of that system. The implication is that
systems in which the solenoidal flow is zero possess detailed balance, while those with a non-
zero solenoidal flow will not. The presence of solenoidal flow means that time reversal also
leads to a reversal in the direction of this flow. Please see [31], as a relatively recent example
of the Helmholtz-Hodge decomposition in Darwinian processes and [80] for a generic
treatment of stochastic chaos in this setting. Furthermore, there is no requirement for the
grouping operator to return the same partition at each instant of its application. This follows
because the grouping operator is determined by sparse coupling among particles at the scale
below, which itself may change as certain particles become ‘shielded’ from others [91]; for
example, during the self-assembly of particular partitions associated with cell-division,
multicellular organisation and development [57]. Mathematically, this permits wandering sets
(i.e., partitions) at each scale, where fitness gradients remain well-defined, because they inherit
from the dynamics of the scale below.
Implicit in the renormalisation group construction, is the notion that variational
selection could operate at multiple scales. In other words, although framed in terms of natural
selection and evolution, the variational formulation above does not commit to separation of
temporal scales apt for replication or reproduction. Any selective mechanism that fulfils the
fitness lemma (Lemma 1) will, in principle, be subject to the same selective mechanics.
Common examples could include the optimisation of weights in neural networks and their
structure learning [45,76,92]. In a biological setting, this could correspond to developmental
stages that have well-defined (separation of) temporal scales. Finally, we take a closer look at
phenotypic dynamics and explain why they can be construed as sentient behaviour.
The sentient phenotype
An ontological interpretation of phenotypic dynamics—in terms of sentient behaviour
or active inference—obtains by expressing the Lagrangian as a variational free energy. For
clarity, we will drop the sub and superscripts (and condition on the extended genotype x ) to
focus on the generalised states of a given phenotype.
19
Lemma 2 (variational free energy): if the autonomous dynamics of a particle or phenotype
evince classical (Lagrangian) mechanics, then they can be expressed as minimising a
variational free energy functional of Bayesian beliefs—about external states—encoded by their
internal phenotypic states,
20
p ( )  , under a generative model encoded by their (extended)

genotype p
x
( , | x
0
)  :
F ( , s )
D
D
p
K
K
E
L
L
[
n
[
[
e rg
p
p
x
y
(
c o
(
C
(
F ( , s )
, s , ) ]
n stra in t
) | p (
x
o m p le x ity
) || p (
x
| x
| s
p
)
0
,
[
E n
]
tro p
(
y
p
)
[
]
—
x
A
(
c
s
c u
,
ra c y
| ) ]
  

   

 
  


 

x
( x )
=
=
=
=
D
l n
−
p

x
( x |
D
x
iv
0
e rg
)
e n c
−
e
a
+
, x
0
) ]
—
(
x
L o g
s
e
,
v id e
)
n c e

−
+
(16)
This variational free energy can be rearranged in several ways. First, it can be expressed
as an energy constraint minus the entropy of the variational density, which licences the name
free energy [93]. In this decomposition, minimising variational free energy corresponds to the
maximum entropy principle, under the constraint that the expected Lagrangian is minimised
[51,94]. The energy constraint is a functional of the marginal density over external and sensory
states that plays the role of a generative model (i.e., parameterised by the extended genotype);
namely, a joint density over causes (external dynamics) and their consequences (autonomous
dynamics). Second—on a statistical reading—variational free energy can be decomposed into
the (negative) log likelihood of particular paths (i.e., accuracy) and the KL divergence between
posterior and prior densities over external paths (i.e., complexity). Finally, it can be written as
the negative log evidence plus the KL divergence between the variational and conditional (i.e.,
posterior) density. In variational Bayesian inference [95], negative free energy is called an
evidence lower bound or ELBO [96–98].
Proof: the sparse coupling—that underwrites a particular partition—means autonomous paths
(i.e., generalised states) depend only on sensory paths. This means there is a (deterministic and
injective) map from the most likely autonomous paths (of sufficiently high order generalised
motion) to the conditional density over external paths, where both are conditioned on sensory
paths. This injection means we can consider the conditional density over external paths as being
parameterised by internal paths. We will call this a variational density (noting from (6) that
internal paths are conditionally independent of external paths):
21
p ( ) p
a
a
a
x
r g
r g
r g
(
m
m
m
|
i
i
i
s
n
n
n
,
a
, x
x
x
x
)
0
(
(
( a
| s
| s
| s
p
)
,
,
x
(
)
)
| s , , , x
0
)    




μ
α
μ
a
=
=
=
a =

a
μ
a
(17)
This definition means that the Lagrangian and variational free energy share the same minima,
where their gradients vanish:
α =argmin (|s)=argmin F(,s)
 x 
 (α,s)=0 (18)

 F(α,s)= D [p ()|| p (|s,a)]+ (s,α)=0
  KL μ x  x
Divergence=0 =0
If autonomous dynamics are conservative, their trajectory is a path of least action and we can
replace the Lagrangian gradients in (12) with variational free energy gradients to give (16) 
Remarks: The free energy lemma (Lemma 2) associates negative fitness with variational free
energy, such that phenotypic behaviour will appear to pursue paths of least free energy or
greatest fitness. Because variational free energy is an upper bound on log evidence, the pursuit
of maximum fitness can be read as self-evidencing [99]: namely, actively soliciting evidence
for generative models endowed by evolution. In short, autonomous dynamics (appear to)
actively infer external states under a generative model, whose parameters are (apparently)
learned by minimising a path integral of variational free energy.
The functional form of variational free energy licences a teleological interpretation of
autonomous dynamics; the internal paths can be read as the sufficient statistics or parameters
of (approximate) Bayesian beliefs about external states, while active paths will (appear to) to
change the posterior over external states to ‘fit’ internal (Bayesian) beliefs. In other words,
active dynamics will look as if they are trying to fulfil the predictions of internal
representations. A complementary interpretation inherits from the decomposition of variational
free energy into complexity and accuracy. Minimising complexity means that generalised
internal states encode Bayesian beliefs about external states that are as close as possible to prior
beliefs, while generalised active states will look as if they are changing sensory states to realise
those beliefs. These interpretations—in terms of perception and action—furnish an elementary
but fairly expressive formulation of active inference. For example, the free energy formulations
above have been used to emulate many kinds of sentient behaviour, ranging from
morphogenesis [100], through action observation [101], to birdsong [102].
Although not developed here, the renormalisation group construction means that we
can apply the same arguments to autonomous kinds—i.e., agents—at the slow scale. In other
words, on average, the extended genotype of internal kinds comes to encode Bayesian beliefs
about external kinds, while active kinds will look as if they are trying to realise those beliefs,
via niche construction [77,103–105]. In virtue of the minimisation of variational free energy,
we have an implicit maximum entropy principle, which brings us back to [21,22] via [49].
Discussion
One insight from the above analysis is that populations are not necessarily quotient sets
of equivalence classes. In other words, there is no assumption that any given particle shares
phenotypic or genotypic characteristics with any other particle. This is interesting on two
counts. First, it suggests that treating a population as an equivalence class of conspecifics may
not be sufficient, in the sense that the population includes all the (natural) kinds that interact to
maintain their particular partition. The constituents of this population could range from
macromolecules to marmosets. Furthermore, even if some agents share the same genotype,
their phenotypes can specialise in distinct ways to minimise their joint variational free energies:
see [100] for a worked example in the context of morphogenesis. This mandates a
quintessentially co-evolutionary perspective that emphasises co-dependencies and co-creation
[20,106–108]. However, the emergence of equivalence classes begs explanation. A potential
answer is the generalised synchrony between particles, as they find their joint variational free
energy minima—and become mutually predictable; e.g., [58,102]. In an evolutionary setting,
one can imagine this leading to convergent evolution or speciation (Luc Ciompi, personal
communication; Ciompi, L. Synergie und Schizophrenie - und noch viel mehr. In Kriz, J,
Tschacher, W. Synergetik als Ordner. Die strukturelle Wirkung der interdisziplinären Ideen
Hermann Hakens. Pabst Science Publishers, Lengerich, Germany, 2017, pp. 15-20).
22
The synthesis of biological evolution and development on offer here is an example of
a generalised synthesis: applicable, under the free energy principle, to all kinds of things. This
synthesis can be read as generative models autopoietically generating entities and then using
the ‘fit’ of the model to the niche as evidence for updating the model, in a cyclical process
summarised in Figure 3.
Figure 3: Phylogeny and Ontogeny as bottom-up and top-down causation.
Effectively, we are describing the evolutionary-developmental process with the following
protocol (where the first two items correspond to the right arrow in Figure 3, and the last two
the left arrow):
● First, generate an ensemble of particles (i.e., extended phenotypes) by sampling their
flow parameters and initial states (i.e., extended genotypes) from some initial density.
● For each particle, find the path of least action using a generalised Bayesian filter (i.e.,
active inference).
● After a suitable period of time, evaluate the path integral of variational free energy (i.e.,
action) to supply a fitness functional.
● Update the flow parameters and initial states, using a stochastic gradient descent on the
action (i.e., Darwinian evolution).
If this protocol were repeated for a sufficiently long period of time, it would converge to
an attracting set, assuming this pullback attractor exists [35]. In statistical mechanics this would
23
be a nonequilibrium steady-state, while in theoretical biology, it would correspond to an
evolutionary steady-state.
Limitations
As with most applications of the free energy principle, the variational account alone
does not supply a process theory. Rather, it starts from the assumption that a nonequilibrium
(evolutionary) steady-state exists and then describes the dynamics that the system must exhibit.
This enables various process theories to be proposed as specific hypotheses about multiscale
biological systems. For example, the genetic variation in the above formulation follows from
the Helmholtz-Hodge decomposition or fundamental theorem of vector calculus. However, the
ensuing stochastic gradient Langevin dynamics does not specify the particular processes that
give rise to this kind of dynamics, e.g., [109]. There are many candidates one could consider:
for example, simple rejection sampling or more involved genetic algorithms that provide a
plausible account of bisexual reproduction [110,111]. A computationally expedient way of
evaluating the requisite gradients—for example those for simulating artificial evolution—
could call upon Bayesian model reduction [48,112]. Irrespective of the replication or
reproduction process, it must, on the present analysis, conform to a stochastic gradient flow on
‘fitness’ with solenoidal mixing [83,89,90].
The issue of relating variational treatments to process theories is a problem also faced
in other fields, notably the cognitive sciences—where the difficulty is in relating the
optimization problems our brains solve to the anatomical [113] and physiological [114]
mechanisms that underwrite its solution. The link in the neurosciences turns out to depend upon
the form of the Lagrangian, from which the dynamics of Equation 16 imply specific
connectivity structures and electrophysiological responses. This means that different
Lagrangians can be proposed as alternative hypotheses that can be assessed against the
measurements they predict. By analogy, different Lagrangians in Equation 12 will have
consequences for the trajectory an individual follows during their lifetime and the changes a
population undergoes over several generations.
As an example, consider a bacterial culture exposed to a new antibiotic. In this setting,
the autonomous states might include expression of mRNA and proteins essential for cell-wall
maintenance and antibiotic resistance (e.g., β-lactamases). Sensory states might include
intracellular antibiotic concentrations. We could hypothesize a Lagrangian that assigns high
24
probability to an intact cell wall and low probability to significant levels of intracellular
antibiotics. In the absence of the antibiotic, the path of least action will be pursued during the
lifetime of a bacterium (i.e., between replications). However, the antibiotic causes deviation
from the path of least action through disruption of cell wall integrity and unexpectedly high
intracellular antibiotic concentrations. The slow dynamics of Equation 12 might, through
changes in allele frequencies, change the Lagrangian such that higher β-lactamase
concentrations are anticipated, altering the path of least action to one in which intracellular
antibiotics are degraded. This sort of narrative is a reframing of the simplest mechanism of
antibiotic resistance but is formulated in such a way that specific choices of Lagrangian result
in subtly different trajectories at both time-scales.
In other words, the primary offering of this variational formulation of natural
selection—from an empirical science perspective—is that one can hypothesize alternative
forms for the Lagrangian. Each choice of Lagrangian will have consequences not only for the
dynamics over physiological and developmental timescales but will also allow for predictions
as to evolution over phylogenetic timescales. It is also worth noting that the account of natural
selection set out here, in which genotypic evolution depends upon the action of phenotypic
paths, applies only for systems that satisfy the variational fitness lemma (Lemma 1): namely,
the likelihood of an agent’s genotype corresponds to the likelihood of its phenotypic trajectory.
While a plausible assumption—that is intuitively consistent with Darwinian evolution—we did
not examine the conditions under which this assumption holds. This means there is an
opportunity to further the ideas set out in this paper by examining the sorts of stochastic systems
in which the variational fitness lemma (Lemma 1) holds. It could be argued that Lemma 1
must hold at least in those systems where the genotype transforms into the phenotype retaining
an equivalence within stochastic limits. For example, gene expression is the most fundamental
level at which the genotype gives rise to the phenotype and this mapping from genotype to
phenotype is the subject of the many process theories studied by developmental biology. On a
teleological view, one might further argue that active inference is necessary to maintain a high
degree of equivalence during the course of this transformation and to preserve a
correspondence between genotype and phenotype.
One could suggest that Lemma 1, and the broader scope of the formalisms described
here, may be applicable to systems where a population of entities engages in intergenerational
replication (modelled here using the renormalization operations), and where those entities at a
faster timescale engage in rapid adaptation (e.g. development, learning, behaviour, modelled
25
with Active Inference) during their lifetime. These two levels could, for example, model how
genome-based intergenerational evolution sets initial conditions for organismal molecular and
behavioural developments. For the faster intra-generational scale, the external states model the
material basis of what the phenotype is a generative model of. For the slower inter-generational
scale, the external states are updated through time as a process of renormalization (reduction
and grouping) of the extended genotype-phenotype.
Conclusion
This work attempts to unify the slow, multi-generational phylogenetic process of
natural selection with the single-lifetime, phenotypic process of development. In this
perspective, a bidirectional flow of information occurs as evolution imposes top-down
constraints on phenotypic processes, and action-selection provides evidence that is selected for
by the environment (i.e., bottom-up causation). In this account, learning and inference occur
through updating probabilistic beliefs via Bayesian model selection in evolutionary time and
active inference in developmental time. The fitness of (extended) genotypes and (extended)
phenotypes is selected for through the minimisation of the same free energy functional;
Bayesian model evidence or marginal likelihood.
Additional Information
Funding Statement
KF is supported by funding for the Wellcome Centre for Human Neuroimaging (Ref:
205103/Z/16/Z) and a Canada-UK Artificial Intelligence Initiative (Ref: ES/T01279X/1). DAF
is supported by a National Science Foundation postdoctoral fellowship (ID 2010290).
Acknowledgements
We would like to thank members of the Active Inference Laboratory and their guests for
invaluable discussions and guidance on the presentation of these ideas.
Competing Interests
The authors have no competing interests.
Authors' Contributions
All authors made substantial contributions to conception and design, and writing of the article,
and approved publication of the final version.
26
References
1. Darwin C. On the Origin of the Species by Natural Selection. Murray; 1859. Available:
http://www.citeulike.org/group/1788/article/1262916
2. Dennett DC. Darwin’s Dangerous Idea: Evolution and the Meanins of Life. Simon and Schuster; 1996. Available:
https://play.google.com/store/books/details?id=FvRqtnpVotwC
3. Fairbanks DJ, Abbott S. Darwin’s Influence on Mendel: Evidence from a New Translation of Mendel’s Paper.
Genetics. 2016;204: 401–405. doi:10.1534/genetics.116.194613
4. Mendel G. Versuche über Pflanzen-Hybriden. https://www.biodiversitylibrary.org ›
parthttps://www.biodiversitylibrary.org › part. 1866. Available: https://www.biodiversitylibrary.org/part/175272
5. Watson JD, Crick FHC. Molecular Structure of Nucleic Acids: A Structure for Deoxyribose Nucleic Acid. Nature.
1953;171: 737–738. doi:10.1038/171737a0
6. Dawkins R, Charles Simonyi Professor of the Public Understanding of Science Richard Dawkins, Dawkins D,
Dawkins RA. The Selfish Gene. Oxford University Press, USA; 1989. Available:
https://books.google.com/books/about/The_Selfish_Gene.html?hl=&id=WkHO9HI7koEC
7. Noble D. A theory of biological relativity: no privileged level of causation. Interface Focus. 2012;2: 55–64.
doi:10.1098/rsfs.2011.0067
8. Keller EF. The Mirage of a Space between Nature and Nurture. Duke University Press; 2010. Available:
https://play.google.com/store/books/details?id=3up1Eo2OdzIC
9. Powell JR. Evolutionary Biology The Genetic Basis of Evolutionary Change R. C. Lewontin. BioScience. 1975. pp.
118–118. doi:10.2307/1297112
10. Jablonka E, Lamb MJ. Evolution in Four Dimensions: Genetic, Epigenetic, Behavioral, and Symbolic Variation in the
History of Life. Life and mind. 2005;462. Available: https://psycnet.apa.org/fulltext/2005-04046-000.pdf
11. Heiner M, Gilbert D. BioModel engineering for multiscale Systems Biology. Prog Biophys Mol Biol. 2013;111: 119–
128. doi:10.1016/j.pbiomolbio.2012.10.001
12. Sultan SE. Eco-Evo-Devo. In: Nuno de la Rosa L, Müller G, editors. Evolutionary Developmental Biology: A
Reference Guide. Cham: Springer International Publishing; 2017. pp. 1–13. doi:10.1007/978-3-319-33038-9_42-1
13. Smart JM. Evolutionary Development: A Universal Perspective. Evolution, Development and Complexity. 2019. pp.
23–92. doi:10.1007/978-3-030-00075-2_2
14. McGuire G, Denham MC, Balding DJ. Models of sequence evolution for DNA sequences containing gaps. Mol Biol
Evol. 2001;18: 481–490. doi:10.1093/oxfordjournals.molbev.a003827
15. Price GR. Fisher’s “fundamental theorem” made clear. Ann Hum Genet. 1972;36: 129–140. doi:10.1111/j.1469-
1809.1972.tb00764.x
16. Page KM, Nowak MA. Unifying evolutionary dynamics. J Theor Biol. 2002;219: 93–98. Available:
https://www.ncbi.nlm.nih.gov/pubmed/12392978
17. Geritz SAH, Kisdi E, Mesze´NA G, Metz JAJ. Evolutionarily singular strategies and the adaptive growth and
branching of the evolutionary tree. Evol Ecol. 1998;12: 35–57. doi:10.1023/A:1006554906681
18. Bruineberg J, Rietveld E, Parr T, van Maanen L, Friston KJ. Free-energy minimization in joint agent-environment
systems: A niche construction perspective. J Theor Biol. 2018;455: 161–178. doi:10.1016/j.jtbi.2018.07.002
19. Ellis GFR, Noble D, O’Connor T. Top-down causation: an integrating theme within and across the sciences? Interface
Focus. 2012;2: 1–3. doi:10.1098/rsfs.2011.0110
20. Carthey AJR, Gillings MR, Blumstein DT. The Extended Genotype: Microbially Mediated Olfactory Communication.
Trends Ecol Evol. 2018;33: 885–894. doi:10.1016/j.tree.2018.08.010
27
21. Vanchurin V, Wolf YI, Katsnelson MI, Koonin EV. Toward a theory of evolution as multilevel learning. Proc Natl
Acad Sci U S A. 2022;119. doi:10.1073/pnas.2120037119
22. Vanchurin V, Wolf YI, Koonin EV, Katsnelson MI. Thermodynamics of evolution and the origin of life. Proc Natl
Acad Sci U S A. 2022;119. doi:10.1073/pnas.2120042119
23. McGee RS, Kosterlitz O, Kaznatcheev A, Kerr B, Bergstrom CT. The cost of information acquisition by natural
selection. bioRxiv. 2022. p. 2022.07.02.498577. doi:10.1101/2022.07.02.498577
24. Geisler WS, Diehl RL. Bayesian natural selection and the evolution of perceptual systems. Philos Trans R Soc Lond B
Biol Sci. 2002;357: 419–448. doi:10.1098/rstb.2001.1055
25. Campbell JO. Universal Darwinism As a Process of Bayesian Inference. Front Syst Neurosci. 2016;10: 49.
doi:10.3389/fnsys.2016.00049
26. Ramírez JC, Marshall JAR. Can natural selection encode Bayesian priors? J Theor Biol. 2017;426: 57–66.
doi:10.1016/j.jtbi.2017.05.017
27. Attias H. Planning by Probabilistic Inference. In: Bishop CM, Frey BJ, editors. Proceedings of the Ninth International
Workshop on Artificial Intelligence and Statistics. PMLR; 03--06 Jan 2003. pp. 9–16. Available:
https://proceedings.mlr.press/r4/attias03a.html
28. Botvinick M, Toussaint M. Planning as inference. Trends Cogn Sci. 2012;16: 485–488.
doi:10.1016/j.tics.2012.08.006
29. Kaplan R, Friston KJ. Planning and navigation as active inference. Biol Cybern. 2018;112: 323–343.
doi:10.1007/s00422-018-0753-2
30. Millidge B. Deep Active Inference as Variational Policy Gradients. arXiv [cs.LG]. 2019. Available:
http://arxiv.org/abs/1907.03876
31. Ao P. Laws in Darwinian Evolutionary Theory. arXiv [q-bio.PE]. 2006. Available: http://arxiv.org/abs/q-bio/0605020
32. Frank SA. Natural selection. V. How to read the fundamental equations of evolutionary change in terms of
information theory. J Evol Biol. 2012;25: 2377–2396. doi:10.1111/jeb.12010
33. Parr T, Pezzulo G, Friston KJ. Active Inference: The Free Energy Principle in Mind, Brain, and Behavior. MIT Press;
2022. Available: https://mitpress.mit.edu/books/active-inference
34. Conant RC, Ross Ashby W. Every good regulator of a system must be a model of that system. Int J Syst Sci. 1970;1:
89–97. doi:10.1080/00207727008920220
35. Crauel H, Flandoli F. Attractors for random dynamical systems. Probab Theory Related Fields. 1994;100: 365–393.
doi:10.1007/BF01193705
36. Smith JM. Evolutionary game theory. Physica D. 1986;22: 43–49. doi:10.1016/0167-2789(86)90232-0
37. Kwon C, Ao P. Nonequilibrium steady state of a stochastic system driven by a nonlinear drift force. Phys Rev E Stat
Nonlin Soft Matter Phys. 2011;84: 061106. doi:10.1103/PhysRevE.84.061106
38. Schwabl F. Phase Transitions, Scale Invariance, Renormalization Group Theory, and Percolation. In: Schwabl F,
editor. Statistical Mechanics. Berlin, Heidelberg: Springer Berlin Heidelberg; 2002. pp. 327–404. doi:10.1007/978-3-
662-04702-6_7
39. Cardy J. Scaling and Renormalization in Statistical Physics. Cambridge University Press; 1996.
doi:10.1017/CBO9781316036440
40. Fields C, Glazebrook JF, Levin M. Minimal physicalism as a scale-free substrate for cognition and consciousness.
Neurosci Conscious. 2021;2021: niab013. doi:10.1093/nc/niab013
41. Koide T. Perturbative expansion of irreversible work in Fokker–Planck equation à la quantum mechanics. J Phys A:
Math Theor. 2017;50: 325001. doi:10.1088/1751-8121/aa7af4
42. Haken H. Synergetics: An Introduction : Nonequilibrium Phase Transitions and Self-organization in Physics,
28
Chemistry and Biology. Springer; 1978. doi:10.1007/978-3-642-96469-5
43. Buckminster Fuller R. Synergetics: Explorations in the Geometry of Thinking. Estate of R. Buckminster Fuller; 1982.
Available: https://play.google.com/store/books/details?id=AKDgDQAAQBAJ
44. Hoeting JA, Madigan D, Raftery AE, Volinsky CT. Bayesian model averaging: a tutorial (with comments by M.
Clyde, David Draper and E. I. George, and a rejoinder by the authors. SSO Schweiz Monatsschr Zahnheilkd. 1999;14:
382–417. doi:10.1214/ss/1009212519
45. Gershman SJ, Niv Y. Learning latent structure: carving nature at its joints. Curr Opin Neurobiol. 2010;20: 251–256.
doi:10.1016/j.conb.2010.02.008
46. Tenenbaum JB, Kemp C, Griffiths TL, Goodman ND. How to grow a mind: statistics, structure, and abstraction.
Science. 2011;331: 1279–1285. doi:10.1126/science.1192788
47. Lu H, Rojas RR, Beckers T, Yuille AL. A Bayesian Theory of Sequential Causal Learning and Abstract Transfer.
Cogn Sci. 2016;40: 404–439. doi:10.1111/cogs.12236
48. Smith R, Schwartenbeck P, Parr T, Friston KJ. An Active Inference Approach to Modeling Structure Learning:
Concept Learning as an Example Case. Front Comput Neurosci. 2020;14: 41. doi:10.3389/fncom.2020.00041
49. Sella G, Hirsh AE. The application of statistical physics to evolutionary biology. Proc Natl Acad Sci U S A.
2005;102: 9541–9546. doi:10.1073/pnas.0501865102
50. Vanchurin V. Toward a theory of machine learning. Mach Learn: Sci Technol. 2021;2: 035012. doi:10.1088/2632-
2153/abe6d7
51. Jaynes ET. Information Theory and Statistical Mechanics. Phys Rev. 1957;106: 620–630.
doi:10.1103/PhysRev.106.620
52. Ramstead MJD, Sakthivadivel DAR, Heins C, Koudahl M, Millidge B, Da Costa L, et al. On Bayesian Mechanics: A
Physics of and by Beliefs. arXiv [cond-mat.stat-mech]. 2022. Available: http://arxiv.org/abs/2205.11543
53. Vanchurin V. The world as a neural network. arXiv [physics.gen-ph]. 2020. Available:
http://arxiv.org/abs/2008.01540
54. Katsnelson MI, Vanchurin V. Emergent Quantumness in Neural Networks. Found Phys. 2021;51: 1–20.
doi:10.1007/s10701-021-00503-3
55. Friston K. A free energy principle for a particular physics. arXiv [q-bio.NC]. 2019. Available:
http://arxiv.org/abs/1906.10184
56. Kirchhoff M, Parr T, Palacios E, Friston K, Kiverstein J. The Markov blankets of life: autonomy, active inference and
the free energy principle. J R Soc Interface. 2018;15: 20170792. doi:10.1098/rsif.2017.0792
57. Levin M. The Computational Boundary of a “Self”: Developmental Bioelectricity Drives Multicellularity and Scale-
Free Cognition. Front Psychol. 2019;10: 2688. doi:10.3389/fpsyg.2019.02688
58. Palacios ER, Razi A, Parr T, Kirchhoff M, Friston K. On Markov blankets and hierarchical self-organisation. J Theor
Biol. 2019; 110089. doi:10.1016/j.jtbi.2019.110089
59. Parr T, Da Costa L, Friston K. Markov blankets, information geometry and stochastic thermodynamics. Philos Trans
A Math Phys Eng Sci. 2020;378: 20190159. doi:10.1098/rsta.2019.0159
60. Fields C, Friston K, Glazebrook JF, Levin M. A free energy principle for generic quantum systems. arXiv [quant-ph].
2021. Available: http://arxiv.org/abs/2112.15242
61. Bhatia H, Norgard G, Pascucci V, Bremer P-T. The Helmholtz-Hodge Decomposition—A Survey. IEEE Trans Vis
Comput Graph. 2013;19: 1386–1404. doi:10.1109/TVCG.2012.316
62. Graham R. Covariant formulation of non-equilibrium statistical thermodynamics. Z Phys B: Condens Matter.
1977;26: 397–405. doi:10.1007/BF01570750
63. Ao P. Potential in stochastic differential equations: novel construction. J Phys A Math Gen. 2004;37: L25.
29
doi:10.1088/0305-4470/37/3/L01
64. Ruoshi Yuan, Yi-An Ma, Bo Yuan,Ping Ao. (21) (PDF) Potential function in dynamical systems and the relation with
Lyapunov function. In: ResearchGate [Internet]. 2011 [cited 15 Feb 2022]. Available:
https://www.researchgate.net/profile/Ping-
Ao/publication/241189592_Potential_function_in_dynamical_systems_and_the_relation_with_Lyapunov_function/li
nks/59f26d82a6fdcc1dc7bb1925/Potential-function-in-dynamical-systems-and-the-relation-with-Lyapunov-
function.pdf
65. Nicolis G, Prigogine I. Self-Organization in Nonequilibrium Systems: From Dissipative Structures to Order Through
Fluctuations. Wiley; 1977. Available: https://play.google.com/store/books/details?id=mZkQAQAAIAAJ
66. Seifert U. Entropy production along a stochastic trajectory and an integral fluctuation theorem. Phys Rev Lett.
2005;95: 040602. doi:10.1103/PhysRevLett.95.040602
67. Yan H, Zhao L, Hu L, Wang X, Wang E, Wang J. Nonequilibrium landscape theory of neural networks. Proc Natl
Acad Sci U S A. 2013;110: E4185-94. doi:10.1073/pnas.1310692110
68. Wehage RA, Haug EJ. Generalized Coordinate Partitioning for Dimension Reduction in Analysis of Constrained
Dynamic Systems. J Mech Des. 1982;104: 247–255. doi:10.1115/1.3256318
69. Kamelian S, Salahshoor K. A novel graph-based partitioning algorithm for large-scale dynamical systems. Int J Syst
Sci. 2015;46: 227–245. doi:10.1080/00207721.2013.775395
70. Fuster JM. Upper processing stages of the perception-action cycle. Trends Cogn Sci. 2004;8: 143–145.
doi:10.1016/j.tics.2004.02.004
71. Hipólito I, Ramstead MJD, Convertino L, Bhat A, Friston K, Parr T. Markov blankets in the brain. Neurosci Biobehav
Rev. 2021;125: 88–97. doi:10.1016/j.neubiorev.2021.02.003
72. Bruineberg J, Dołęga K, Dewhurst J, Baltieri M. The Emperor’s New Markov Blankets. Behav Brain Sci. 2021;45:
e183. doi:10.1017/S0140525X21002351
73. Jiao S, Xu S, Jiang P, Yuan B, Ao P. Wright-Fisher dynamics on adaptive landscape. IET Syst Biol. 2013;7: 153–164.
doi:10.1049/iet-syb.2012.0058
74. Kerr WC, Graham AJ. Generalized phase space version of Langevin equations and associated Fokker-Planck
equations. The European Physical Journal B - Condensed Matter and Complex Systems. 2000;15: 305–311.
doi:10.1007/s100510051129
75. Da Costa L, Friston K, Heins C, Pavliotis GA. Bayesian mechanics for stationary processes. Proc Math Phys Eng Sci.
2021;477: 20210518. doi:10.1098/rspa.2021.0518
76. Friston K, Stephan K, Li B, Daunizeau J. Generalised Filtering. Math Probl Eng. 2010;2010.
doi:10.1155/2010/621670
77. Laland KN, Odling-Smee FJ, Feldman MW. Evolutionary consequences of niche construction and their implications
for ecology. Proc Natl Acad Sci U S A. 1999;96: 10242–10247. doi:10.1073/pnas.96.18.10242
78. Lehmann L. The adaptive dynamics of niche constructing traits in spatially subdivided populations: evolving
posthumous extended phenotypes. Evolution. 2008;62: 549–566. doi:10.1111/j.1558-5646.2007.00291.x
79. Welling M, Teh YW. Bayesian Learning via Stochastic Gradient Langevin Dynamics. 2011 [cited 13 Aug 2022].
Available: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.226.363
80. Friston K, Heins C, Ueltzhöffer K, Da Costa L, Parr T. Stochastic Chaos and Markov Blankets. Entropy. 2021;23.
doi:10.3390/e23091220
81. Ao P. Emerging of Stochastic Dynamical Equalities and Steady State Thermodynamics from Darwinian Dynamics.
Commun Theor Phys. 2008;49: 1073–1090. doi:10.1088/0253-6102/49/5/01
82. Seifert U. Stochastic thermodynamics, fluctuation theorems, and molecular machines. arXiv [cond-mat.stat-mech].
2012. Available: http://arxiv.org/abs/1205.4176
30
83. Zhang F, Xu L, Zhang K, Wang E, Wang J. The potential and flux landscape theory of evolution. J Chem Phys.
2012;137: 065102. doi:10.1063/1.4734305
84. Lopes da Silva F. Neural mechanisms underlying brain waves: from neural membranes to networks.
Electroencephalogr Clin Neurophysiol. 1991;79: 81–93. doi:10.1016/0013-4694(91)90044-5
85. Buzsáki G, Draguhn A. Neuronal oscillations in cortical networks. Science. 2004;304: 1926–1929.
doi:10.1126/science.1099745
86. Lisman J. Excitation, inhibition, local oscillations, or large-scale loops: what causes the symptoms of schizophrenia?
Curr Opin Neurobiol. 2012;22: 537–544. doi:10.1016/j.conb.2011.10.018
87. Levin M. Endogenous bioelectrical networks store non-genetic patterning information during development and
regeneration. J Physiol. 2014;592: 2295–2305. doi:10.1113/jphysiol.2014.271940
88. Manicka S, Levin M. Modeling somatic computation with non-neural bioelectric networks. Sci Rep. 2019;9: 18612.
doi:10.1038/s41598-019-54859-8
89. Ignacio-Espinoza JC, Ahlgren NA, Fuhrman JA. Long-term stability and Red Queen-like strain dynamics in marine
viruses. Nat Microbiol. 2020;5: 265–271. doi:10.1038/s41564-019-0628-x
90. Schenk H, Schulenburg H, Traulsen A. How long do Red Queen dynamics survive under genetic drift? A comparative
analysis of evolutionary and eco-evolutionary models. BMC Evol Biol. 2020;20: 8. doi:10.1186/s12862-019-1562-5
91. Baross JA, Martin WF. The Ribofilm as a Concept for Life’s Origins. Cell. 2015;162: 13–15.
doi:10.1016/j.cell.2015.06.038
92. LeCun Y, Bengio Y, Hinton G. Deep learning. Nature. 2015;521: 436–444. doi:10.1038/nature14539
93. Feynman, P. Statistical mechanics. Notes taken by Kikuchi, R.
94. Sakthivadivel DAR. A Constraint Geometry for Inference and Integration. arXiv [math-ph]. 2022. Available:
http://arxiv.org/abs/2203.08119
95. Beal MJ. Variational algorithms for approximate Bayesian inference. Doctoral, UCL (University College London).
2003. Available: https://discovery.ucl.ac.uk/id/eprint/10101435/
96. Winn J, Bishop CM. Variational Message Passing. J Mach Learn Res. 2005;6: 661–694. Available:
https://jmlr.org/papers/v6/winn05a.html
97. Bishop CM. Pattern Recognition and Machine Learning. Springer; 2006. Available:
https://play.google.com/store/books/details?id=qWPwnQEACAAJ
98. Dauwels J. On Variational Message Passing on Factor Graphs. 2007 IEEE International Symposium on Information
Theory. 2007. pp. 2546–2550. doi:10.1109/ISIT.2007.4557602
99. Hohwy J. The self-evidencing brain. Nous. 2016;50: 259–285. doi:10.1111/nous.12062
100. Friston K, Levin M, Sengupta B, Pezzulo G. Knowing one’s place: a free-energy approach to pattern regulation. J R
Soc Interface. 2015;12: 20141383–20141383. doi:10.1098/rsif.2014.1383
101. Friston K, Mattout J, Kilner J. Action understanding and active inference. Biol Cybern. 2011;104: 137–160.
doi:10.1007/s00422-011-0424-z
102. Isomura T, Parr T, Friston K. Bayesian Filtering with Multiple Internal Models: Toward a Theory of Social
Intelligence. Neural Comput. 2019; 1–42. doi:10.1162/neco_a_01239
103. Laland K, Matthews B, Feldman MW. An introduction to niche construction theory. Evol Ecol. 2016;30: 191–202.
doi:10.1007/s10682-016-9821-z
104. Constant A, Ramstead MJD, Veissière SPL, Campbell JO, Friston KJ. A variational approach to niche construction. J
R Soc Interface. 2018;15: 20170685. doi:10.1098/rsif.2017.0685
31
105. Blackiston D, Lederer E, Kriegman S, Garnier S, Bongard J, Levin M. A cellular platform for the development of
synthetic living machines. Sci Robot. 2021;6. doi:10.1126/scirobotics.abf1571
106. Kauffman SA, Johnsen S. Coevolution to the edge of chaos: coupled fitness landscapes, poised states, and
coevolutionary avalanches. J Theor Biol. 1991;149: 467–505. doi:10.1016/s0022-5193(05)80094-3
107. Rosenman M, Saunders R. Self-regulatory hierarchical coevolution. Artif Intell Eng Des Anal Manuf. 2003;17: 273–
285. doi:10.1017/S089006040317401X
108. Traulsen A, Claussen JC, Hauert C. Coevolutionary dynamics in large, but finite populations. Phys Rev E Stat Nonlin
Soft Matter Phys. 2006;74: 011901. doi:10.1103/PhysRevE.74.011901
109. Richardson K. Genes and knowledge: Response to Baverstock, K. the gene an appraisal.
https://doi.org/10.1016/j.pbiomolbio.2021.04.005. Prog Biophys Mol Biol. 2021;167: 12–17.
doi:10.1016/j.pbiomolbio.2021.10.003
110. Casella G, Robert CP, Wells MT. Generalized Accept-Reject sampling schemes. Institute of Mathematical Statistics
Lecture Notes - Monograph Series. Beachwood, Ohio, USA: Institute of Mathematical Statistics; 2004. pp. 342–347.
doi:10.1214/lnms/1196285403
111. Mehrabian AR, Lucas C. A novel numerical optimization algorithm inspired from weed colonization. Ecol Inform.
2006;1: 355–366. doi:10.1016/j.ecoinf.2006.07.003
112. Friston K, Parr T, Zeidman P. Bayesian model reduction. arXiv [stat.ME]. 2018. Available:
http://arxiv.org/abs/1805.07092
113. Parr T, Friston KJ. The Anatomy of Inference: Generative Models and Brain Structure. Front Comput Neurosci.
2018;12: 90. doi:10.3389/fncom.2018.00090
114. Friston K, FitzGerald T, Rigoli F, Schwartenbeck P, Pezzulo G. Active Inference: A Process Theory. Neural Comput.
2017;29: 1–49. doi:10.1162/NECO_a_00912
32

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "A variational synthesis of evolutionary and developmental dynamics"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
