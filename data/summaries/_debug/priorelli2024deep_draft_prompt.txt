=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Deep hybrid models: infer and plan in a dynamic world
Citation Key: priorelli2024deep
Authors: Matteo Priorelli, Ivilin Peev Stoianov

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2024

Abstract: To determine an optimal plan for complex tasks, one often deals with dynamic and hierarchical
relationshipsbetweenseveralentities. Traditionally,suchproblemsaretackledwithoptimalcontrol,
whichreliesontheoptimizationofcostfunctions;instead,arecentbiologically-motivatedproposal
castsplanningandcontrolasaninferenceprocess.Activeinferenceassumesthatactionandperception
aretwocomplementaryaspectsoflifewherebytheroleoftheformeristofulfillthepredictions
inferred by the latter. Here, we present an active...

Key Terms: plan, world, instituteofcognitivesciencesandtechnologies, models, dynamic, deep, infer, nationalresearchcouncilofitaly, hybrid, matteopriorelli

=== FULL PAPER TEXT ===

DEEP HYBRID MODELS: INFER AND PLAN IN A DYNAMIC WORLD
MatteoPriorelli
InstituteofCognitiveSciencesandTechnologies
NationalResearchCouncilofItaly
SapienzaUniversityofRome,Italy
matteo.priorelli@gmail.com
IvilinPeevStoianov
InstituteofCognitiveSciencesandTechnologies
NationalResearchCouncilofItalyivilinpeev.stoianov@cnr.it
ABSTRACT
To determine an optimal plan for complex tasks, one often deals with dynamic and hierarchical
relationshipsbetweenseveralentities. Traditionally,suchproblemsaretackledwithoptimalcontrol,
whichreliesontheoptimizationofcostfunctions;instead,arecentbiologically-motivatedproposal
castsplanningandcontrolasaninferenceprocess.Activeinferenceassumesthatactionandperception
aretwocomplementaryaspectsoflifewherebytheroleoftheformeristofulfillthepredictions
inferred by the latter. Here, we present an active inference approach that exploits discrete and
continuousprocessing,basedonthreefeatures: therepresentationofpotentialbodyconfigurationsin
relationtotheobjectsofinterest;theuseofhierarchicalrelationshipsthatenabletheagenttoeasily
interpretandflexiblyexpanditsbodyschemafortooluse; thedefinitionofpotentialtrajectories
relatedtotheagent’sintentions,usedtoinferandplanwithdynamicelementsatdifferenttemporal
scales. Weevaluatethisdeephybridmodelonahabitualtask: reachingamovingobjectafterhaving
pickedamovingtool.Weshowthatthemodelcantacklethepresentedtaskunderdifferentconditions.
Thisstudyextendspastworkonplanningasinferenceandadvancesanalternativedirectiontooptimal
control.
1 Introduction
Imagine a baseball player striking a ball with a bat. State-of-the-art approaches to simulate such goal-directed
movementsinahuman-likemanneroftenrelyonoptimalcontrol[1,2]. Thistheoryrestsupontheformulationof
goalsintermsofvaluefunctionsandtheiroptimizationviacostfunctions. Whilethisapproachhasadvancedboth
roboticsandourunderstandingofhumanmotorcontrol,itsbiologicalplausibilityremainsdisputed[3]. Moreover,value
functionsformotorcontrolmightrestricttherangeofmotionsanagentcanlearn[4]. Incontrast,complexmovements
likehandwritingorwalkingcanemergenaturallyfromgenerativemodelsencodinggoalsaspriorbeliefsaboutthe
environment[5]. Thetheoryofactiveinferencebuildsuponthispremise,proposingthatgoal-directedbehaviorresults
fromabiasedinternalrepresentationoftheworld. Thisbiasgeneratesacascadeofpredictionerrorsforcingtheagent
tosamplethoseobservationsthatmakeitsbeliefstrue[6,7,8,9]. Inthisview,thetradeoffbetweenexplorationand
exploitationarisesnaturally,drivingtheagenttominimizetheuncertaintyofitsinternalmodelbeforemaximizing
potentialrewards[10]. Activeinferencesharesfoundationalprincipleswithpredictivecoding[11,12]andtheBayesian
brainhypothesis,whichpostulatesthatthebrainmakessenseoftheworldbyconstructingahierarchicalgenerative
modelthatcontinuouslymakesperceptualhypothesesandrefinesthem[13].
This perspective offers a promising avenue for advancing current robotics and machine learning, particularly, in a
researchthatframescontrolandplanningasaninferenceprocess[14,15,16,17,18]. Adistinctivefeatureofactive
inference is its ability to model the environment as a hierarchy of causes and dynamic states evolving at different
timescales [19], which is fundamental for biological phenomena such as linguistic communication [20], and for
advancedmovementssuchasthatofabaseballplayer. Inthelatter,severalcharacteristicsofthenervoussystemcanbe
5202
yaM
9
]OR.sc[
4v88001.2042:viXra
wellcapturedbyanactiveinferenceagent,providingarobustalternativetooptimalcontrol. First,thehumanbrainis
assumedtomaintainahierarchicalrepresentationofthebodythatgeneratesthemotorcommandsrequiredtoachieve
thedesiredgoal[21]. Thisrepresentationmustbeflexible,inthatthebaseballplayershouldrelatetheconfigurationof
thebattohisbody,actingasanextensionofhishand[22]. Tool-useexperimentsinnon-humanprimatesrevealedthat
parietalandmotorregionsrapidlyadapttoincorporatetoolsintothebodyschema,enablingseamlessinteractionwith
objects[23]. Further,theplayer’sbrainshouldmaintainadynamicrepresentationofthemovingballinordertopredict
itstrajectorybeforeandafterthehit. Theposteriorparietalcortexisknowntoencodemultipleobjectsinparallelduring
actionsequences,formingvisuomotorrepresentationsthatalsoaccountforobjectaffordances[24]. Inaddition,distinct
neuralpopulationsinthedorsalpremotorcortexareknowntoencodemultiplereachingoptionsindecision-making
tasks,withonepopulationbeingactivatedandtheotherssuppressedasthedecisionunfolds[25]. Inshort,thehuman
braincanmaintainmultiplepotentialbodyconfigurationsandpotentialtrajectoriesappropriateforspecifictasks.
Despiteitspotential,thedevelopmentofdeephierarchicalmodelswithinactiveinferenceremainslimited. Adaptation
tocomplexdatastillreliesprimarilyonneuralnetworksasgenerativemodels[26,27,28,29,30,31,32,33,34,35].
Onestudydemonstratedthatadeephierarchicalagent,equippedwithindependentdynamicsfunctionsforeachdegree
offreedom(DoF),couldcontinuouslyadjustitsinternaltrajectoriestoalignwithpriorexpectations,enablingadvanced
controlofcomplexkinematicchains[36]. Thisabilitytolearnandactacrossintermediatetimescalesofferssignificant
advantagesforsolvingcontroltasks. Moreover, simulatingreal-worldscenarioscanbenefitfromso-calledhybrid
or mixed models in active inference, which integrate discrete decision-making with continuous motion. However,
state-of-the-artapplicationsofsuchmodelshavesimulatedstaticcontextsonly[8,37,38,39,40].
Inthiswork,weaddressthesechallengesfromaunifiedperspective. Ourkeycontributionsaresummarizedasfollows:
• We present an active inference agent affording robust planning in dynamic environments. The basic unit
ofthisagentmaintainspotentialtrajectories, enablingahigh-leveldiscretemodeltoinferthestateofthe
world and plan composite movements. The units are hierarchically combined to represent potential body
configurations,incorporatingobjectaffordances(e.g.,graspingacupbythehandleorwiththewholehand)
andtheirhierarchicalrelationships(e.g.,howatoolcanextendtheagent’skinematicchain).
• Weintroduceamodulararchitecturedesignedfortasksthatinvolvedeephierarchicalmodeling,suchastool
use. Itsmulti-inputandmulti-outputconnectivityresemblestraditionalneuralnetworksandrepresentsan
initialsteptowarddesigningdeepstructuresinactiveinferencecapableofgeneralizingacrossandlearning
noveltasks.
• Weevaluatetheagent’sperformanceinacommontask: reachingamovingballafterreachingandpickinga
movingtool. Theresultshighlighttheinterplaybetweentheagent’spotentialtrajectoriesandthedynamic
accumulation of sensory evidence. We demonstrate the agent’s ability to infer and plan under different
conditions,suchasrandomobjectpositionsandvelocities.
2 Methods
2.1 Predictivecoding
Accordingtopredictivecoding(PC),thehumanbrainmakessenseoftheworldbyconstructinganinternalgenerative
modelofhowhiddenstatesoftheenvironmentgeneratetheperceivedsensations[11,12,41]. Thisinternalmodelis
continuouslyrefinedbyminimizingthediscrepancy(calledpredictionerror)betweensensationsandtherespective
predictions. Moreformally,givensomepriorknowledgep(x)overhiddenstatesxandpartialevidencep(y)over
sensationsy,thenervoussystemcanfindtheposteriordistributionofthehiddenstatesgiventhesensationsviaBayes
rule:
p(x,y)
p(x|y)= (1)
p(y)
However,directcomputationoftheposteriorp(x|y)isunfeasiblesincetheevidencerequiresmarginalizingoverevery
(cid:82)
possibleoutcome,i.e.,p(y)= p(x,y)dx. Predictivecodingsupposesthat,insteadofperformingexactBayesian
inference,organismsareengagedinavariationalapproach[42],i.e.,theyapproximatetheposteriordistributionwith
asimplerrecognitiondistributionq(x) ≈ p(x|y),andminimizethedifferencebetweenthetwodistributions. This
2
differenceisexpressedintermsofaKullback-Leibler(KL)divergence:
(cid:90) q(x)
D [q(x)||p(x|y)]= q(x)ln dx (2)
KL p(x|y)
x
Giventhatthedenominatorp(x|y)stilldependsonthemarginalp(y),weexpresstheKLdivergenceintermsofthe
logevidenceandtheVariationalFreeEnergy(VFE),andminimizethelatterquantityinstead. TheVFEisthenegative
ofwhatinthemachinelearningcommunityisknownastheevidencelowerboundorELBO[43]:
(cid:20) (cid:21) (cid:20) (cid:21)
q(x) q(x)
F = E ln = E ln −lnp(y) (3)
q(x) p(x,y) q(x) p(x|y)
SincetheKLdivergenceisalwaysnonnegative,theVFEprovidesanupperboundonsurprise,i.e.,F ≥ −lnp(y).
Therefore, minimizingF isequivalenttominimizingtheKLdivergencewithrespecttoq(x). ThemoretheVFE
approaches0,theclosertheapproximatedistributionistotherealposterior,andthehigherthemodelevidence(or,
equivalently,thelowerthesurpriseaboutsensoryoutcomeswillbe). Thediscrepancybetweenthetwodistributionsalso
dependsonthespecificassumptionsmadeovertherecognitiondistribution:acommononeistheLaplaceapproximation
[42],whichassumesGaussianprobabilitydistributions,e.g.,q(x)=N(µ,Σ),whereµrepresentsthemostplausible
hypothesis - also called belief about the hidden states x - and Σ is its covariance matrix. Now, we factorize the
generativemodelp(x,y)intolikelihoodandpriorterms,andwefurtherparameterizeitwithsomeparametersθ:
p(x,y)=p(y|x,θ)p(x)
p(y|x,θ)=N(g(x,θ),Σ ) (4)
y
p(x)=N(η,Σ )
η
whereg(x,θ)isalikelihoodfunctionandη isaprior. Inthisway,afterapplyingthelogarithmovertheGaussian
terms,theVFEbreaksdowntothefollowingsimpleformula:
F =− 1(cid:2) Π ε2 +Π ε2 +ln2πΣ +ln2πΣ (cid:3) (5)
2 y y η η y η
where we expressed the difference in the exponents of the Gaussian distributions in terms of prediction errors
ε = y−g(µ,θ)andε = µ−η,andwewrotethecovariancesintermsoftheirinverse,i.e.,precisionsΠ and
y η y
Π . InordertominimizetheKLdivergencebetweentherealandapproximateposteriors,wecanminimizetheVFE
η
withrespecttothebeliefsµ–aprocessassociatedwithperception–andtheparametersθ–generallyreferredtoas
learning. Inpractice,theupdaterulesfollowgradientdescent:
µ=argminF θ =argminF
µ θ (6)
µ˙ =−∂ F =∂ gTΠ ε −Π ε θ˙ =−∂ F =−∂ gTΠ ε
µ µ y y η η θ θ y y
Inordertoseparatethetimescalesoffastperceptionandslow-varyinglearning,thetwophasesaretreatedasthesteps
ofanEMalgorithm,i.e.,optimizingthebeliefswhilekeepingtheparametersfixed,andthenoptimizingtheparameters
whilekeepingthebeliefsfixed.
Predictive coding, originally rooted in data compression [44], has inspired biologically plausible alternatives to
traditionaldeeplearning,suchasPredictiveCodingNetworks(PCNs)[45,46]. Infact,thepredictivecodingalgorithm
canbescaleduptolearnhighlycomplexstructures,composedofcausalrelationshipsofarbitrarilyhighdepth–in
asimilarwaytodeepneuralnetworks. Inparticular,wecanfactorizeagenerativemodelintoaproductofdifferent
distributions,whereinaspecificlevelonlydependsonthelevelabove:
L
(cid:89)
p(x(0),...,x(l))=p(x(0)) p(x(l)|x(l−1))
(7)
l=1
p(x(l)|x(l−1))=N(g(x(l−1),θ(l−1)),Σ(l))
where0indicatesthehighestlevel,andListhenumberoflevelsinthehierarchy.Inthisway,x(l)actsasanobservation
forlevell−1,andg(x(l),θ(l))actsasapriorforlevell+1.Asaresult,thisfactorizationallowsustoexpresstheupdate
ofbeliefsandparametersofaspecificlevelonlybasedonthelevelaboveandthelevelbelow–withasimpleVFE
3
minimizationasinthepreviouscase. Differentlyfromthebackpropagationalgorithmofneuralnetworks,themessage
passingofpredictionerrorsimplementsabiologicallyplausibleHebbianrulebetweenpresynapticandpostsynaptic
activities. Infact, thepredictionsucomputedbythelikelihoodfunctionsaretypicallyaweightedcombinationof
neuronspassedtoanonlinearactivationfunctionϕ:
J
u(l) = (cid:88) W(l−1)ϕ(x(l−1)) (8)
i i,j j
j=1
whereW(l−1) aretheweightsfromneuronj atlevell−1toneuroniatlevell. Crucially,whatisbackpropagated
i,j
inPCNsarenotsignalsdetectingincreasinglycomplexfeatures,butmessagesrepresentinghowmuchthemodelis
surprisedaboutsensoryobservations(e.g.,apredictionequaltoanobservationmeansthatthenetworkstructureisa
goodapproximationoftherealprocess,andnoerrorshavetobeconveyed).
2.2 Hierarchicalactiveinference
Thetheoryofactiveinferencebuildsonthesameassumptionsofpredictivecoding,butwithtwocriticaldifferences.
Thefirstone–whichisactuallysharedwithsomeimplementationsofpredictivecodingsuchastemporalPC[47]–
assumesthatlivingorganismsconstantlydealwithhighlydynamicenvironments,andtheinternalmodelsthatthey
buildmustreflectthechangesoccurringintherealgenerativeprocess[48]. Thisrelationisusuallyexpressedinterms
ofgeneralizedcoordinatesofmotion(encoding,e.g.,position,velocity,acceleration,andsoon)[49];consequently,the
environmentismodeledwiththefollowingnonlinearsystem:
y˜=g˜(x˜)+w
y
(9)
Dx˜ =f˜(x˜,v˜)+w
x
wherex˜arethegeneralizedhiddenstates,v˜arethegeneralizedhiddencauses,y˜arethegeneralizedsensorysignals,
D isadifferentialoperatorthatshiftsallthetemporalordersbyone,i.e.: Dx˜ = [x′,x′′,x′′′,...],andtheletterw
indicates(Gaussian)noiseterms. Thelikelihoodfunctiong˜defineshowhiddenstatesgeneratesensoryobservations(as
inpredictivecoding),whilethedynamicsfunctionf˜specifiestheevolutionofthehiddenstates(see[50,51]formore
details). Theassociatedjointprobabilityisfactorizedintoindependentdistributions:
p(y˜,x˜,v˜)=p(y˜|x˜)p(x˜|v˜)p(v˜) (10)
whereeachdistributionisGaussian:
p(y˜|x˜)=N(g˜(x˜),Σ˜ )
y
p(Dx˜|v˜)=N(f˜(x˜,v˜),Σ˜ ) (11)
x
p(v˜|η)=N(η,Σ˜ )
v
Asinpredictivecoding,thesedistributionsareinferredthroughapproximateposteriorsq(x˜)andq(v˜),minimizingthe
relatedVFEF. Asaresult,theupdatesofthebeliefsµ˜ andν˜respectivelyoverthehiddenstatesandhiddencauses
become:
µ˜˙ −Dµ˜ =−∂ F =∂g˜TΠ˜ ε˜ +∂ f˜TΠ˜ ε˜ −DTΠ˜ ε˜
µ y y µ x x x x
(12)
ν˜˙ −Dν˜ =−∂ F =∂ f˜TΠ˜ ε˜ −Π˜ ε˜
ν ν x x v v
whereΠ˜ ,Π˜ ,andΠ˜ aretheprecisions,andε˜ ,ε˜ ,andε˜ arerespectivelythepredictionerrorsofsensorysignals,
y x v y x v
dynamics,andpriors:
ε˜ =y˜−g˜(µ˜)
y
ε˜ =Dµ˜−f˜(µ˜,ν˜) (13)
x
ε˜ =ν˜−η
v
For a full account of free energy minimization in active inference, see [8]. Unlike the update rules of predictive
coding,additionaltermsarisefromtheinferreddynamics,throughwhichthemodelcancapturetheevolutionofthe
4
environment. Alsonotethatsinceweareminimizingover(dynamic)pathsandnot(static)states,anadditionaltermDµ˜
ispresent: thisimpliestrajectorytracking,duringwhichtheVFEisminimizedonlywhenthebeliefofthegeneralized
hiddenstatesDµ˜ matchesitsinstantaneoustrajectoryµ˜˙.
Thesecondassumptionmadebyactiveinferenceisthatourbrainsnotonlyperceive(andlearn)theexternalgenerative
process,butalsointeractwithittoreachdesiredstates(e.g.,inordertosurvive,notonlyonemustunderstandwhich
causeleadstoanincreaseordecreaseintemperature,butalsotakeactionstoliveinanarrowrangearound37degrees
Celsius). The free energy principle, which is at the core of active inference, states that all living organisms act to
minimize the free energy (or, equivalently, surprise). In fact, in addition to the perceptual inference of predictive
coding,theVFEcanbeminimizedbysamplingthosesensoryobservationsthatconformtosomepriorbeliefs,e.g.,
a=argmin F,whereaarethemotorcommands. Thisprocessistypicallycalledself-evidencing,implyingthatifI
a
believetofindmyselfinanarrowrangeoftemperatures,minimizingsurpriseviaactionwillleadtofindingplaceswith
suchtemperatures–hencemakingmybeliefstrue. Oneroleofthehiddencausesistodefinetheagent’spriorsthat
ensuresurvival. Thesepriorsgenerateproprioceptivepredictionswhicharesuppressedbymotorneuronsviaclassical
reflexarcs[52]:
a˙ =−∂ F =−∂ y˜ Π˜ ε˜ (14)
a p a p p p
where∂ y˜ isaninversemodelfrom(proprioceptive)observationstoactions,andε˜ =y˜ −g˜ (µ˜)arethegeneralized
a p p p p
proprioceptivepredictionerrors.
Asinpredictivecoding,wecanscaleupthisgenerativemodeltocapturethecausalrelationshipsofseveralrelated
entities(e.g.,thejointsofakinematicstructure),uptoacertaindepth[37,19,53,36]. Therefore,thepriorbecomesthe
predictionfromthelayerabove,whiletheobservationbecomesthelikelihoodofthelayerbelow:
µ˜˙(l) =Dµ˜(l)+∂g˜(l)TΠ˜(l)ε˜(l)+∂ f˜(l)TΠ˜(l)ε˜(l)−DTΠ˜(l)ε˜(l)
v v µ x x x x
(15)
ν˜˙(l) =Dν˜(l)+∂ f˜(l)TΠ˜(l)ε˜(l)−Π˜(l−1)ε˜(l−1)
ν x x v v
where:
ε˜(l) =Dµ˜(l)−f˜(l)(µ˜(l),ν˜(l))
x x
(16)
ε˜(l) =µ˜(l+1)−g˜(l)(µ˜(l))
v v
andthesubscriptindicatestheindexofthelevel,asbefore. Here,theroleofthehiddencausesistolinkhierarchical
levels. Thisallowstheagenttoconstructahierarchyofrepresentationsvaryingatdifferenttemporalscales,criticalfor
realizingrichlystructuredbehaviorssuchaslinguisticcommunication[20]orsinging[54]. Inthiscase,themotor
commandsminimizethegeneralizedpredictionerrorsofthelowestlevelofthehierarchy.
Thiscontinuousformulationofactiveinferenceishighlyeffectivefordealingwithrealisticenvironments. However,
minimizingtheVFE(whichisthefreeenergyofthepastandpresent)doesnotaffordplanninganddecision-makingin
theimmediatefuture. Todothis,weconstructadiscretegenerativemodelinwhichwediscretizethepossiblefuture
statesandencodetheirexpectationswithcategoricaldistributions. WethenminimizetheExpectedFreeEnergy(EFE)
which,asthenamesuggests,isthefreeenergythattheagentsexpecttoperceiveinthefuture[55,8]. Thisdiscrete
generativemodelissimilartothecontinuousmodeldefinedabove,withthedifferencethatweconditionthehidden
statesoverpoliciesπ(whichinactiveinferencearesequencesofdiscreteactions):
p(s,o,π)=p(o|s,π)p(s|π)p(π) (17)
Here,sandoarediscretestatesandoutcomes,whichrepresentpast,present,andfuturestatesasinHiddenMarkov
Models. Then,theEFEisspecificallyconstructedbyconsideringfuturestatesasrandomvariablesthatneedtobe
inferred:
(cid:20) (cid:21) (cid:20) (cid:21)
q(s|π) q(s)
G = E ln ≈ E ln − E [lnp(o|C)] (18)
π q(s,o|π) p(s,o|π) q(s,o|π) q(s|o,π) q(o|π)
whereq(o|π)isarecognitiondistributionthattheagentconstructstoinfertherealposteriorofthegenerativeprocess.
Critically,theprobabilitydistributionp(o|C)encodespreferredoutcomes,actingsimilartoaprioroverthehidden
causesinthecontinuouscounterpart. Thelasttwotermsarerespectivelycalledepistemic(uncertaintyreducing)and
pragmatic (goal seeking). In practice, this quantity is used by first factorizing the agent’s generative model as in
5
POMDPs:
T T
(cid:89) (cid:89)
p(s ,o ,π)=p(s )·p(π)· p(o |s )· p(s |s ,π) (19)
1:T 1:T 1 τ τ τ τ−1
τ=1 τ=2
Eachoftheseelementscanberepresentedwithcategoricaldistributions:
p(s )=Cat(D) p(o |s )=Cat(A)
1 τ τ
(20)
p(π)=Cat(E) p(s |s ,π)=Cat(B )
τ τ−1 π,τ
whereDencodesbeliefsabouttheinitialstate,E encodestheprioroverpolicies,AisthelikelihoodmatrixandB
π,τ
isthetransitionmatrix. TheminimizationofEFEindiscretemodelsfollowsthevariationalmethodusedbypredictive
codingandcontinuous-timeactiveinference;specifically,theuseofcategoricaldistributionsbreaksdowntheinference
ofhiddenstatesandpoliciestoasimplelocalmessagepassing:
s =σ(lnB s +lnBT s +lnATo )
π,τ π,τ−1 π,τ−1 π,τ π,τ+1 τ
π =σ(lnE−G)
(21)
(cid:88)
G ≈ As (lnAs −lnp(o |C))−s diag(AT lnA)
π π,τ π,τ τ π,τ
τ
ForacompletetreatmentofhowtheEFEandtheapproximateposteriorsarecomputed–alongwithotherinsightful
implicationsofthefreeenergyprincipleindiscretemodels–see[50],while[56]providesamorepracticaltutorialwith
basicapplications. Equation21showsthattheupdateruleforthediscretehiddenstatesattimeτ andconditionedover
apolicyπisacombinationofmessagescomingfromthepreviousandnextdiscretetimesteps,andamessagecoming
fromthediscreteoutcome. Thiscombinationispassedtoasoftmaxfunctioninordertogetaproperprobability. In
addition,theoptimalpolicyπisfoundbyacombinationofapolicypriorandtheEFE,wherethelatterisacomposition
ofepistemicandpragmaticbehaviors. PolicyinferencecanberefinedbycomputingtheEFEofseveralstepsaheadin
thefuture–aprocesscalledsophisticatedinference. Then,ateachdiscretestepτ,theagentsselectsthemostlikely
actionuunderallpolicies,i.e.,u =argmax π·[U =u].
t u π,t
Asbefore,wecanconstructahierarchicalstructureabletoexpressmoreandmoreinvariantrepresentationsofhidden
states[57]. Indiscretestate-space,linksbetweenhierarchicallevelsareusuallydonebetweenhiddenstatesviathe
matrixD–althoughinsomeformulationsthehiddenstatesofalevelconditionthepoliciesofthesubordinatelevels.
Theupdaterulesforthishierarchicalalternativebecome:
s(l) =σ(lnB(l) s(l) +lnB(l)Ts(l) +lnA(l)To(l)+lnD(l+1)Ts(l+1))
π,τ π,τ−1 π,τ−1 π,τ π,τ+1 τ 1
π(l) =σ(lnE(l)−G(l))
(22)
(cid:88)
G(l) ≈ A(l)s(l) (lnA(l)s(l) −lnp(o(l)|C))−s(l) diag(A(l)T lnA(l))
π π,τ π,τ τ π,τ
τ
ThisimpliesthateachleveltakesabstractactionstominimizeitsEFE,onlydependingonthelevelsimmediatelybelow
andabove.
2.3 Bayesianmodelcomparison
Bayesianmodelcomparisonisatechniqueusedtocompareaposterioroversomedatawithafewsimplehypotheses
knowna-priori[58]. Consideragenerativemodelp(y,θ)withparametersθanddatay:
p(θ,y)=p(y|θ)p(θ) (23)
Weintroduceadditionaldistributionsp(y,θ|m)whicharereducedversionsofthefirstmodelifthelikelihoodofsome
dataisthesameunderbothmodels–i.e.,p(y|θ,m)=p(y|θ)–andtheonlydifferencerestsuponthespecificationof
thepriorsp(θ|m). Wecanexpresstheposteriorofthereducedmodelsintermsoftheposteriorofthefullmodeland
theratiosofthepriorsandtheevidence:
p(θ|m)p(y)
p(θ|y,m)=p(θ|y) (24)
p(θ)p(y|m)
6
Theprocedureforcomputingthereducedposteriorsisthefollowing: first,weintegrateovertheparameterstoobtain
theevidenceratioofthetwomodels:
(cid:90) p(θ|m)
p(y|m)=p(y) p(θ|y) dθ (25)
p(θ)
Then,wedefineanapproximateposteriorq(θ)andwecomputethereducedfreeenergiesofeachmodelm:
(cid:20) (cid:21)
p(θ|m)
F[p(θ|m)]≈F[p(θ)]+lnE (26)
q p(θ)
ThisVFEactsasahinttohowwellthereducedrepresentationexplainsthefullmodel. Similarly,theapproximate
posteriorofthereducedmodelcanbewrittenintermsoftheposteriorofthefullmodel:
(cid:20) (cid:21)
p(θ|m) p(θ|m)
lnq(θ|m)=lnq(θ)+ln −lnE (27)
p(θ) q p(θ)
The Laplace approximation [59] leads to a simple form of the approximate posterior and the reduced free energy.
AssumingthefollowingGaussiandistributions:
p(θ)=N(η,Σ) q(θ)=N(µ,C)
(28)
p(θ|m)=N(η ,Σ ) q(θ|m)=N(µ ,C )
m m m m
thereducedfreeenergyturnsto:
1
F[p(θ|m)]≈F[p(θ)]+ ln|Π PC Σ|
2 m m
(29)
1
− (µTPµ−µTP µ −ηTΠη+ηTΠ η )
2 m m m m m m
expressedintermsofprecisionofpriorsΠandposteriorsP. Thereducedposteriormeanandprecisionarecomputed
viaEquation27:
µ =C (Pµ−Πη+Π η )
m m m m
(30)
P =P −Π+Π
m m
Inthisway,twodifferenthypothesesiandj canbeeasilycomparedtoinferwhichisthemostlikelytohavegenerated
the observed data; this comparison has the form of a log-Bayes factor F[p(θ|i)]−F[p(θ|j]. For a more detailed
treatmentofBayesianmodelcomparison(inparticularundertheLaplaceapproximation),see[58,60].
3 Results
3.1 Deephybridmodels
Analyzingtheemergenceofdistributedintelligence,Fristonetal. [61]emphasizedthreekindsofdepthwithinthe
frameworkofactiveinference: factorial,hierarchical,andtemporal. Factorialdepthassumesindependentfactorsin
theagent’s generative model(e.g., objectsand qualitiesofanenvironment, ormore abstractstates), whichcanbe
combinedtogenerateoutcomesandtransitions. Hierarchicaldepthintroducescausalrelationshipsbetweenlevels,
inducingaseparationoftemporalscaleswherebyhigherlevelshappentoconstructmoreinvariantrepresentations,
whilelowerlevelsbettercapturetherapidchangesofsensorystimuli. Temporaldepthentails,indiscreteterms,avision
intotheimminentfuturethatcanbeusedfordecision-making;or,incontinuousterms,increasinglypreciseestimatesof
dynamictrajectories.
Inthefollowing,wepresentthemainfeaturesofadeephybridmodelintermsoffactorial,temporal,andhierarchical
depthsinthecontextofflexiblebehavior,iterativetransformationsofreferenceframes,anddynamicplanning. Bydeep
hybridmodel,weintendanactiveinferencemodelcomposedofhybridunitsconnectedhierarchically. Here,hybrid
meansthatdiscreteandcontinuousrepresentationsareencodedwithineachunit,whereinthecommunicationbetween
thetwodomainsisachievedbyBayesianmodelreduction[60,58]. Asatechnicalnote,alltheinternaloperations
can be computed through automatic differentiation, i.e., by maintaining the gradient graph when performing each
7
Figure1:(a)Factorgraphofahybridunit.Continuoushiddenstatesx˜generatepredictionsy˜throughparallelpathways.
Modeldynamicsisencodedbypotentialtrajectoriesf ,whicharehypothesesofhowtheworldmayevolveandare
m
associatedwithdiscretehiddencausesv. (b)Illustrativeexampleofahybridunit. Inthistask,theagenthastoinfer
whichoneamongtwoobjects(aredcircleandagraysquaremovingalongacirculartrajectory)isbeingtrackedby
another1-DoFagent. Thetimestepisshowninthebottomleftofeachframe. Thehiddenstatesx˜encodetheangle
andangularvelocityofthearm(generatingproprioceptivepredictions),aswellasthepositionsandvelocitiesofthe
twoobjects(generatingvisualpredictions). Thebluearrowrepresentstheactualhandtrajectory,whiletheredand
greenarrowsrepresentthetwopotentialtrajectoriesassociatedwithreachingmovementstowardthetwoobjects. See
[62]formoredetails.
forwardpassandpropagatingbackthepredictionerrors. Foradetailedtreatmentofpredictivecoding,hierarchical
activeinferenceindiscreteandcontinuousstate-spaces,andBayesianmodelcomparison,seeSections2.1,2.2,and2.3,
respectively.
3.1.1 Factorialdepthandflexiblebehavior
Considerthecasewhereoneneedstoinferwhichobjectisbeingfollowedbyanagent. Thiscanbedonethrougha
hybridunitU,whosefactorgraphisdepictedinFigure3.1.1. Thevariablesare: continuoushiddenstatesx˜ =[x,x′],
observationsy˜=[y,y′],and(discrete)hiddencausesv. Hiddenstatesandobservationscomprisetwotemporalorders
(e.g.,xencodesthepositionandx′thevelocity),andthefirsttemporalorderwillbeindicatedasthe0thorder–seethe
Methodssectionformoreinformation. Thefactorsare: dynamicsfunctionsf,andlikelihoodfunctionsg˜ =[g,g′].
Notealsoaprioroverthe0th-orderhiddenstatesη ,andaprioroverthehiddencausesH encodingtheagent’sgoals.
x v
Thegenerativemodelisthefollowing:
p(x˜,v,y˜)=p(y˜|x˜)p(x′|x,v)p(x)p(v) (31)
Weassumethathiddencausesandhiddenstateshavedifferentdimensions,resultingintwofactorizations. Thehidden
states,withdimensionN,aresampledfromindependentGaussiandistributionsandgeneratepredictionsinparallel
pathways:
N N
p(x)= (cid:89) N(η ,Σ ) p(y˜|x˜)= (cid:89) N(g˜ (x˜ ),Σ˜ ) (32)
x,n η,x,n n n y,n
n n
whereΣ andΣ˜ aretheircovariancematrices. Inturn,thehiddencauses,withdimensionM,aresampledfrom
η,x,n y,n
acategoricaldistribution:
p(v)=Cat(H ) (33)
v
This differs from state-of-the-art hybrid architectures whichassume separate continuous and discrete models with
continuoushiddencauses(seeSection2.2)[48].Adiscretehiddencausev concurs,withhiddenstatesx,ingenerating
m
8
aspecificpredictionforthe1sttemporalorderx′:
p(x′|x,m)=N(f (x),Σ ) (34)
m x,m
Thisprobabilitydistributionentailsapotentialtrajectory,orhypotheticalevolutionofthehiddenstates,whichthe
agentmaintainstoinferthestateofaffairsoftheworldandact. Moreformally,weconsiderp(x′|x,m)asbeingthe
mthreducedversionofafullmodel:
p(x′|x,v)=N(η′,Σ ) (35)
x x
Thisallowsus–usingthevariationalapproachforapproximatingthetrueposteriordistributions–toconvertdiscrete
signals into continuous signals and vice versa through Bayesian model average and Bayesian model comparison,
respectively(seeSection2.3and[60,58]). Inparticular,top-downmessagescombinethepotentialtrajectoriesf (x)
m
withtherelatedprobabilitiesencodedinthediscretehiddencausesv:
M
(cid:88)
η′ = v f (x) (36)
x m m
m
This computes a dynamic path that is an average of the agent’s hypotheses, based on its prior H . Conversely,
v
bottom-upmessagescomparetheagent’spriorsurprise−lnH withthelogevidencel ofeveryreducedmodel,
v m
i.e.,v =σ(lnH +l),wherel=[l ,...,l ]andσisasoftmaxfunction. Thelogevidenceisaccumulatedovera
v 1 M
continuoustimeT:
(cid:90) T 1
l = (µ′TP µ′ −f (x)TΠ f (x)−µ′TP µ′+η′TΠ η′)dt (37)
m 2 m x,m m m x,m m x x x x
0
whereµ ,P ,andΠ arethemean,posteriorprecision,andpriorprecisionofthemthreducedmodel. Inthis
m x,m x,m
way,theagentcaninferwhichdynamichypothesisismostlikelytohavegeneratedtheperceivedtrajectory–seeFigure
3.1.1and[62]formoredetailsaboutthisapproach.
Ahybridunithasusefulfeaturesderivingfromthefactorialdepthsofhiddenstatesandcauses. Considerthecasewhere
thehiddenstatesencodetheagent’sconfigurationandotherenvironmentalobjects,whilethehiddencausesrepresent
theagent’sintentions. Ahybridunitcoulddynamicallyassignthecausesofitsactionsataparticularmoment: thisis
critical,e.g.,inapickandplaceoperation,duringwhichanobjectisfirstthecauseofthehandmovements–resulting
in a picking action – but then it is the consequence of another cause (i.e., a goal position) – resulting in a placing
action. Thisapproachdiffersfromothersolutions[63,6]thatdirectlyencodeatargetlocationinthehiddencauses.
Further,embeddingenvironmentalentities–andnotjusttheself–intothehiddenstatespermitsinferringtheirdynamic
trajectories, which is fundamental for interactions, e.g., in catching objects on the fly [64] or in tracking a hidden
targetwiththeeyes[65]. ConsideringtheexampleinFigure3.1.1,thehiddenstatesxandx′mayencodetheangle
andangularvelocityofanagent’sarm,andtwohiddencausesv andv maybeassociatedwithdynamics
circle square
functions f and f encoding (potential) reaching movements toward the two objects. The environment
circle square
maybeinferredbytwolikelihoodfunctions,one–g –predictingproprioceptiveobservations(i.e.,armandangular
p
velocity),andanotherone–g –predictingvisualobservations(i.e.,thehandpositionandvelocity). Sinceweare
v
interestedininferringwhichobjectisbeingfollowedbytheagent,wesetauniformdiscretepriorH . Then,Equation
v
37comparestheactualagent’strajectorytothetworeachingmovementstowardthetwoobjects,andassignsahigher
probabilitytotheonebetterresemblingtherealtrajectory.
3.1.2 Hierarchicaldepthanditerativetransformations
Hierarchicaldepthiscriticalinmanytasksthatrequirelearningofmodularandflexiblefunctions. Consideringmotor
control,forwardkinematicsisrepeatedthroughouteveryelementofthekinematicchain,computingtheendeffector
positionfromthebody-centeredreferenceframe. Iterativetransformationsarealsofundamentalincomputervision,
wherecameramodelsperformroto-translationsandperspectiveprojectionsinsequence. Howcanweexpresssuch
hierarchical computations in terms of inference? We design a structure called Intrinsic-Extrinsic (or IE) module,
performingiterativetransformationsbetweenreferenceframes[36,66]. AunitU(i)–wherethesuperscriptindicates
e
theithlevel–encodesasignalx(i) inanextrinsicreferenceframe(e.g.,Cartesiancoordinates),whileanotherunit
e
U(i) representsanintrinsicsignalx(i) (e.g.,polarcoordinates). Ateachleveli,alikelihoodfunctiong(i) appliesa
i i e
transformationtotheextrinsicsignalprovidedbythehigherlevelbasedontheintrinsicinformation,andreturnsanew
9
Figure2: (a)AnIEmoduleiscomposedoftwounitsU andU ,whichrepresentasignalinintrinsicandextrinsic
i e
referenceframes,respectively. DifferentIEmodulescanbecombinedinahierarchicalfashion: theextrinsicsignal
x(i)isiterativelytransformedthroughlineartransformationmatricesencodedintheextrinsiclikelihoodfunctiong(i).
e e
Hierarchicallevelscommunicateviathe0th-orderhiddenstates. (b)Illustrativeexamplesofahierarchicalmodelwith
IEmodules. Inthefirsttask,theagent(a23-DoFhumanbody)hastoavoidamovingobstacle;inthesecondtask,the
agent(a28-DoFkinematictree)hastoreachfourtargetlocationswiththeextremitiesofitsbranches. Inbothcases,
themodulein(a)isrepeatedforeveryDoFoftheagents,matchingtheirkinematicstructures. Proprioceptiveand
exteroceptive(e.g.,visual)foreachDoFarerespectivelygeneratedbytheintrinsicandextrinsicunitsviaappropriate
likelihoodfunctions. See[36]formoredetails.
extrinsicstate:
x(i) =g(i)(x(i),x(i−1))+w =T(i)(x(i))·x(i−1)+w (38)
e e i e e i e e
wherew isanoisetermandT(i)isalineartransformationmatrix. Thisnewstateactsasapriorforthesubordinate
e
levelsinamultiple-outputsystem;indicatingwiththesuperscript(i,j)theithhierarchicallevelandthejthunitwithin
thesamelevel,welinktheIEmodulesinthefollowingway:
y(i,j) ≡x(i+1,j) x(i−1,j) ≡η(i,j) (39)
e e e e
as displayed in Figure 2; hence, the observation of level i becomes the prior over the hidden states of level i+1.
Ill-posedproblemsthatgenerallyhavemultiplesolutions–suchasinversekinematicsordepthestimation–canbe
solvedbyinvertingtheagent’sgenerativemodelandbackpropagatingthesensorypredictionerrors,withtwoadditional
featurescomparedtotraditionalmethods: (i)thepossibilityofsteeringtheoptimizationbyimposingappropriatepriors,
e.g.,foravoidingsingularitiesduringinversekinematics;(ii)thepossibilityofactingovertheenvironmenttominimize
uncertainty,e.g.,withmotionparallaxduringdepthestimation.
Encodingsignalsinintrinsicandextrinsicreferenceframesalsoinducesadecompositionoverproprioceptiveand
exteroceptivepredictions, aswellasintrinsicandextrinsicdynamicsfunctions, leadingtosimpler(andyetricher)
attractorstates. Figure3.1.2showstwoexamplesofgoal-directedbehaviorwithcomplexkinematicstructures–a
28-DoF kinematic tree and a 23-DoF human body. In these cases, an IE module of Figure 3.1.2 is employed for
each DoF of the agent. Each IE module encodes the position and velocity of a specific limb, both in an extrinsic
(e.g.,Cartesian)–x(i) andx(i)′ –andintrinsic(e.g.,polar)–x(i) andx(i)′ –referenceframes. Thesemodulesare
e e i i
connectedhierarchically,i.e.,thetrunkpositiongeneratespredictionsforthepositionsofbotharmsandlegsthrough
thelikelihoodfunctiong(i)computingforwardkinematics. Thegoal-directedbehaviorofthekinematictreeisrealized
e
bydefiningfourreachingdynamicsfunctions(towardtheredobjects)atthelastlevelsofthehierarchyrepresenting
theendeffectors. Instead,thebehaviorofthehumanbodyisachievedbydefiningrepulsivedynamicsfunctionsfor
theextrinsicreferenceframesofeachIEmodule. Thedecompositionofindependentdynamicsisalsousefulfortasks
requiringmultipleconstraintsinbothdomains,e.g.,whenwalkingwithaglassinhand[36],andithasalsobeenapplied
10
tocontrollingrobotsin3Denvironments[67];orforestimatingthedepthofanobjectbymovingtheeyes[66]. Further,
thefactorialdepthpreviouslydescribedpermitsrepresentinghierarchicallynotonlytheself,butalsotheobjectsin
relationtotheself,alongwiththekinematicchainsofotheragents[68]. Inthisway,anagentcouldmaintainapotential
bodyconfigurationwheneveritobservesarelevantentity. Thisrepresentationalsoaccountsfortheaffordancesof
objectstobemanipulated,andcanberealizedefficientlyassoonasnecessary.
3.1.3 Temporaldepthanddynamicplanning
Considerthefollowingdiscretegenerativemodel:
(cid:89)
p(s ,o ,π)=p(s )p(π) p(o |s )p(s |s ,π) (40)
1:τ 1:τ 1 τ τ τ τ−1
τ
where:
p(s )=Cat(D) p(o |s )=Cat(A)
1 τ τ
(41)
p(π)=σ(−G) p(s |s ,π)=Cat(B )
τ+1 τ π,τ
Here,A,B,Darethelikelihoodmatrix,transitionmatrix,andprior,πisthepolicy,s arethediscretehiddenstates
τ
attimeτ,o arediscreteobservations,andG istheexpectedfreeenergy(seeSection2.2formoredetails).
τ
Wecanletthelikelihoodp(o |s )directlybiasthe(discrete)hiddencausesofahybridunit:
τ τ
H ≡As o ≡v (42)
τ τ τ τ
Hence,thepriorH overthediscretehiddencausesbecomesthepredictionbythediscretemodel,whilethediscrete
τ
observationbecomesthediscretehiddencausesofthehybridunit. Thisisanalternativemethodtothestate-of-the-art,
whichconsidersanadditionallevelbetweendiscreteobservationsandstaticpriorsovercontinuoushiddencauses[37].
Here,thediscretemodelcanimposepriorsovertrajectorieseveninthesameperiodτ,thusaffordingdynamicplanning
[62,68]. Ifadiscretemodelislinkedtodifferenthybridunitsinparallel–asshowninFigure3–thediscretehidden
statesareinferredbycombiningmultipleevidences:
s =σ(lnB s +BT s + (cid:88) lnA(i)T v(i))
π,τ π,τ−1 π,τ−1 π,τ+1 π,τ+1 τ (43)
n
wheres arethediscretehiddenstatesconditionedoverpolicyπattimeτ,whilethesuperscriptiindicatestheith
π,τ
hybridunit. Theseparallelpathwayssynchronizethebehaviorofalllow-levelunitsbasedonthesamehigh-levelplan,
allowing,e.g.,simultaneouscoordinationofeverylimbofthehumanbody.
InFigure3.1.3,wenoticethetwokindsoftemporaldepths,peculiartohybridactiveinference. Thefirstonecomes
fromthediscretecomponentandunfoldsoverfuturestates(s ,s ,s ,...)overatimehorizondefinedbythepolicy
1 2 3
length: it allows the agent to make plans by computing the expected free energy of those states [69]. The second
temporaldepthderivesfromthecontinuouslevelandunfoldsoverthetemporalderivativesofthehiddenstates,i.e.,
(x,x′,x′′,...): thisrefinestheestimatedtrajectorieswithanincreasingsamplingrate.
Inadditiontothis,theoveralldeephybridmodelpresentstwohierarchicaldepthswithdifferentroles. First,ahybrid
scalethatseparatestheslow-varyingrepresentationofthediscretizedtaskwiththefastupdateofcontinuoussignals.
Here,thetemporalpredictionsfromthecontinuousdynamicsareusedtoinferaccuratelythediscretevariables,sothat
theagentcanmakecomplexhigh-levelplansevenwhenthesurroundingenvironmentischangingfrequently,andis
abletorevisethoseplanswhennewevidencehasbeenaccumulated. Second,acontinuousscalelinkingthehybrid
unitsandinherenttothehierarchicalrepresentationoftheagent’skinematicstructure,whichcanbeappreciatedfrom
Figure2. Thisinducesaseparationoftemporalscalesbetweenhighandlowlevelsofthehierarchy(e.g.,thetrunkvs
thehands),asthepredictionserrorsgeneratedfromthedynamicsofthehandhavealessandlessimpactastheyflow
backtotheshoulderandtrunkdynamics.
Consideringthepick-and-placeoperationshowninFigure3.1.3,wecanencodethethreekeymomentsofthetask
(startposition,ballpicked,andballplaced)intermsofdiscretehiddenstatess. Ateachdiscretestepτ,thesestates
make(intrinsicandextrinsic)predictionsforthehybridunitsrepresentingtheagent’skinematicchain(asinFigure
2). Forinstance,theseconddiscretehiddenstategeneratesapotentialbodyconfigurationwiththehandclosedandat
theballposition. Wecanuseanidentitymappingforthelikelihoodmatrices,sothattheintrinsicandextrinsichidden
11
Figure3: (a)Interfacebetweenadiscretemodelandseveralhybridunits. Thehiddencausesv(i)aredirectlygenerated,
inparallelpathways,fromdiscretehiddenstatess vialikelihoodmatricesA(i). (b)Illustrativeexamplewiththe
τ
hybridunitscombinedwithadiscretemodel. Inthistask, theagent(a4-DoFarmwithanadditional4-DoFhand
composedoftwofingers)hastopickamovingball(theredcircle)andplaceitatagoalposition(thegreysquare).
Thediscretehiddenstatess encodetheagentposition(startposition,attheball,oratthegoal)andthestatusofthe
τ
hand(openorclosed). Theseareinformedbytwocontinuousmodelsencodingintrinsic(jointangles)andextrinsic
(handandobjectspositions)information,respectively. Thehiddencausesvoftheintrinsicmodelarerelatedtohand
openingandclosingactions,whilethehiddencausesoftheextrinsicmodelrelatetotworeachingmovements,asin
thepreviouscase. Notethattheobjectbelief(purplecircle)israpidlyinferred,andassoonasthepickingactionis
complete,thebeliefisgraduallypulledtowardthegoalposition,resultinginasecondreachingmovement. Thetop
rightpanelshowsthehand-objectdistanceovertime,whilethebottomrightpaneldisplaysthedynamicsofthediscrete
actionprobabilitiesusedtoinferthenextdiscretestate. Theverticaldashedlinesdistinguishfivedifferentphases:
apurereachingmovement,anintermediatephasewhentheagentpreparesthegraspingaction,agraspingphase,a
secondreachingmovementand,finally,theballrelease. Thesteppedbehavioroftheactionprobabilitiesisduetothe
replanningmadebythediscretemodelevery10continuoustimesteps. See[64]formoredetails.
12
Figure4: (a)Virtualenvironmentofthetoolusetask. Anagentcontrollinga4-DoFarmhastograspamovingtool(in
green)andreachamovingball(inred)withthetool’sextremity. (b)Agent’sbeliefsoverthecontinuoushiddenstates
ofthearm(blue),tool(lightgreen),andball(lightred). Therealpositionsofthetoolandballarerepresentedindark
greenanddarkred,respectively. Thevirtuallevelisplottedwithmoretransparentcolors. (c)Graphicalrepresentation
oftheagent’scontinuousgenerativemodel. Everyenvironmentalentityisencodedhierarchicallybyconsideringthe
wholearm’skinematicstructure. Forclarity,thethreepathwaysaredisplayedseparately,whilelateralconnections
andthehigh-leveldiscretemodelarenotshown. Theendeffector’slevelencodesintrinsicandextrinsicinformation
about the end effector, regarding the three configurations (the actual end effector position, the belief over the end
effectoratthetool’sorigin,oratanappropriatepositiontoreachtheballwiththetool’sextremity). Instead,thevirtual
levelisnotpresentintheactualconfiguration,sincethetoolisnotpartoftheagent’skinematicchainanditisonly
usedinthegenerativemodelforgoal-directedbehavior–asifitwereanewjoint. Thislevelencodesintrinsicand
extrinsicinformationaboutthetool,regardingthetwopotentialconfigurations(thebeliefoverthetool’sextremityat
theactualtool’sextremity,andattheactualballposition.) Smallpurpleandyellowcirclesrepresentproprioceptiveand
exteroceptiveobservations,respectively.
causesofthehybridunitsallhavethesamedecompositionintothreesteps–i.e.,v(i) =[v(i) ,v(i) ,v(i) ]
i i,start i,picked i,placed
andv(i) = [v(i) ,v(i) ,v(i) ]. Notably,sincethesehiddencausesarerelatedtopotentialtrajectories,the
e e,start e,picked e,placed
agentcanpickandplacetheballevenindynamiccontexts,e.g.,iftheballismoving.
3.2 Adeephybridmodelfortooluse
Inthissection,weshowhowadeephybridmodelcanbeusedefficientlyinataskthatrequiresplanninginadynamic
environmentandcoordinationofallelementsoftheagent’sbody. Theimplementationdetailsarefoundhereafterin
Section3.2.1,whileAppendixAillustratesthealgorithmsfortheinferenceofthediscretemodelandhybridunits. Then,
inSection3.3weanalyzemodelperformanceanddescribetheeffectsofdynamicplanningintermsofaccumulated
sensoryevidenceandtransitionsoverdiscretehiddenstates.
Reachinganobjectwithatoolisacomplextaskthatrequiresallthefeaturesdelineatedintheprevioussection. First,
thetaskhastobedecomposedintosubgoals–reachingthetoolandreachingtheobject–whichrequireshigh-level
discreteplanning. Second,theagenthastomaintaindistinctbeliefsaboutitsarm,thetool,andtheobject,allofwhich
mustbeinferredfromsensoryobservationsiftheirlocationsareunknownorconstantlychanging. Third,ifthetoolhas
tobegraspedattheoriginwhiletheobjecthastobereachedwiththetool’sextremity,theagent’sgenerativemodel
shouldencodeahierarchicalrepresentationoftheselfandeveryentity,andspecifygoals(intheformofattractors)at
differentlevelsofthehierarchy.
As shown in the graphical representation of the virtual environment of Figure 3.2, the agent controls an arm of 4
DoF.Theagentreceivesproprioceptiveinformationaboutitsjointangles,andexteroceptive(e.g.,visual)observations
13
encodingthepositionsofitslimbs,thetool,andtheball. Forsimplicity,weassumethatthetoolstickstotheagent’s
endeffectorassoonasitistouched. Thegenerativemodelprovidesaneffectivedecompositionintothreeparallel
pathways,displayedinFigure3.2: onemaintaininganestimateoftheagent’sactualconfiguration(indicatedbythe
subscript 0), and two others representing potential configurations in relation to the tool and the ball (respectively
indicatedbythesubscriptstandb). Inotherwords,theobjectsofinterestarenotjustencodedbytheirqualitiesor
location,butalreadydefineabodyconfigurationappropriatetoachieveaspecificinteraction. Inourcase,thepotential
configurationrelatedtothetoolrepresentsnotonlytheestimatedtool’slocation,butalsotheestimatedendeffector’s
locationneededtoreachthetool. Inaddition,eachbodyconfigurationiscomposedofasmanyIEmodulesasthe
agent’sDoF,followingthehierarchicalrelationshipsoftheforwardkinematicsandallowingtoexpressbothjointangles
andlimbpositions. Messagepassingofextrinsicpredictionerrors,i.e.,differencesbetweentheestimatedlimbpositions
andtheirpredictionsgivenbytheestimatedjointangles,allowstoinferthewholebodyconfiguration(eitheractual
orpotential)viaexteroceptiveobservations. Inaddition,everycomponentofalevelexchangeslateralmessageswith
theothercomponents,intheformofdynamicspredictionerrors. Theseerrorsarecausedbythepotentialdynamics
functionsdescribedinSection3.1.1,whichdefinetheinteractionsbetweenentitiesforgoal-directedbehavior.
Twocrucialaspectsarisewhenmodelingentitiesinadeephierarchicalfashionandrelatedtotheself. First,different
configurationsareinferred(hence,differentmovements)dependingonthedesiredinteractionwiththeobjectconsidered
–forexample,reachingaballwitheithertheelbowortheendeffector.Second,entitiescouldhavetheirownhierarchical
structures: inourapplication,thetoolconsistsoftwoCartesianpositionsandanorientation,andtheagentshould
somehowrepresentthisadditionallink. Forthesereasons,weconsideravirtuallevelforthetoolconfiguration,attached
tothelastIEmodule(i.e.,endeffector),asexemplifiedinFigure3.2;thevisualobservationsofthetoolarethenlinked
tothelasttwolevels. Fromtheseobservations,thecorrecttoolanglecanbeinferredasifitwereanewjointangleof
thearm. Additionally,sincewewanttheagenttotouchtheballwiththetool’sextremity,wemodelthethird(ball)
configurationwithasimilarstructure,inwhichavisualobservationoftheballisattachedtothevirtuallevel. The
overallarchitecturecanbebetterunderstoodfromFigure3.2,showingtheagent’scontinuousbeliefsofallthreeentities.
Assoonastheagentperceivesthetool,itinfersapossiblekinematicconfigurationasifithadvisualaccessonlytoits
lasttwojoints(whichareactuallythetool’soriginandextremity). Likewise,perceivingtheballcausestheagenttofind
anextendedkinematicconfigurationasifthetoolwerepartofthearm.
Withthistaskformalization,specifyingthecorrectdynamicsforgoal-directedbehaviorissimple. First,wedefine
twosetsofdynamicsfunctionsimplementingeverysubgoal,oneforreachingthetoolandanotherforreachingthe
ballwiththetool’sextremity. AsexplainedinSection3.2.1,thesecondsubgoalrequiresspecifyinganattractoratthe
virtuallevel,whichmakestheagentthinkthatthetool’sextremitywillbepulledtowardtheball. Thebiasedstate
generatesanextrinsicpredictionerrorthatisbackpropagatedtothepreviouslevelencodingthetool’soriginandthe
endeffector. Notably,definingdiscretehiddenstatesanddiscretehiddencausesrelatedtotheagent’sintentionsallows
theagenttoaccumulateevidenceovertrajectoriesfromdifferentmodalities(e.g.,intrinsicorextrinsic)andhierarchical
locations(e.g.,elboworendeffector),ultimatelysolvingthetaskviainference. Thefourmainprocessesofthetask–
i.e.,perception,dynamicinference,dynamicplanning,andaction–aresummarizedinFigure5.
3.2.1 Implementationdetails
Theagent’ssensorymodalitiesare:(i)aproprioceptiveobservationy forthearm’sjointangles;(ii)avisualobservation
p
y encodingtheCartesianpositionsofeverylinkofthearm,bothextremitiesofthetool,andtheball;(iii)adiscrete
v
tactileobservationo signalingwhetherornotthetargetisgrasped.
t
WedecomposeintrinsicandextrinsichiddenstatesofeveryIEmoduleintothreecomponents,thefirstonecorresponding
totheactualarmconfiguration,andtheothertworelatedtopotentialconfigurationsforthetoolandtheball. Hence:
(cid:104) (cid:105)
x(i) = x(i) x(i) x(i)
i i,0 i,t i,b
(44)
(cid:104) (cid:105)
x(i) = x(i) x(i) x(i)
e e,0 e,t e,b
Theendeffector’slevelandthevirtuallevel(seeFigure3.2)areindicatedwiththesuperscripts(4)and(5),respectively.
RegardingtheIEmoduleofthevirtuallevel,theintrinsicandextrinsichiddenstatesonlyhavetwocomponentsrelated
tothepotentialstatesofthetoolandtheball,i.e.,x(5) =[x(5),x(5)]andx(5) =[x(5),x(5)].
i i,t i,b e e,t e,b
14
Figure5: Graphicalrepresentationofadeephybridmodelfortooluse,composedofadiscretemodelatthetopand
severalIEmodules. Everymoduleisfactorizedintothreeelements,relatedtotheobservationsoftheagent’sarm(in
blue),atool(ingreen),andaball(inred).Notethatthelast(virtual)levelonlyconsidersthetool’sextremityandtheball.
Thecomputationoftheactionforasingletimestepisdividedintofourmainprocesses. (a)Perception. Proprioceptive
and visual observations y and y are compared with the agent’s predictions. The resulting prediction errors are
p e
propagatedthroughoutthehierarchytoinfertheactualkinematicconfiguration,aswellaspotentialconfigurations
relatedtotheobjects. (b)Dynamicinference. Thebottom-upmessagesl fromtheIEmodulesinformthediscrete
e
modelaboutthemostlikelystatethatmayhavegeneratedtheperceivedarmtrajectory. Thisisdonebycomparingthe
latterwithpotentialtrajectoriesf relatedtodynamichypothesesv (seeEquation57). Forinstance,iftheagentis
m e
reachingthetoolandtheballismovingaway,thebottom-upmessagesassignahigherprobabilitytothetool-reaching
hypothesisandalowerprobabilitytotheinitialsteadystate. (c)Dynamicplanning. Theagentinfersthenextdiscrete
actiontotakebyminimizingtheexpectedfreeenergyG (seeEquation59). Asaresult,theagentbelievestobeatthe
nextdiscretestate,correspondingtotheball-reachinghypothesis. Inturn,thisbiasedstategeneratesanewcombined
trajectory(throughthediscreteextrinsicpredictionA sinEquation57),actingasapriorforthecontinuoushidden
e
statesoftheIEmodules. (d)Action. Thecontinuoushiddenstatesgeneratepredictions,whichareagaincompared
withtherelatedobservations. Theproprioceptivepredictionerrorsclimbbackthehierarchyasbefore,buttheyarealso
suppressedthroughmovementbymotorunits(seeEquation56). Thissecondprocesseventuallyproducesacontinuous
actionthatmovestheendeffectortowardtheball.
15
Figure6: Sequenceoftimeframesofthesimulation. Realball,tool,andarmaredisplayedindarkred,darkgreen,and
darkbluerespectively. Beliefsoftoolandball,intermsofpotentialkinematicconfigurations,areshowninlightgreen
andlightred,respectively. Trajectoriesoftheendeffector,tool,andballaredisplayedaswell. Thenumberoftime
stepsisshowninthelower-leftcornerofeachframe.
Foreachentity,theintrinsichiddenstatesencodepairsofjointanglesandlimblengths,e.g.,x(i) =[θ(i),l(i)]whilethe
i,0 0 0
extrinsicreferenceframeisexpressedintermsofthepositionofalimb’sextremityanditsabsoluteorientation,e.g.,
x(i) =[p(i),p(i),ϕ(i)]. Thelikelihoodfunctiong ofEquation38computesextrinsicpredictionsindependentlyfor
e,0 0,x 0,y 0 e
eachentity:
(cid:104) (cid:105)
g (x(i),x(i−1))= T(x(i),x(i−1)) T(x(i),x(i−1)) T(x(i),x(i−1)) (45)
e i e i,0 e,0 i,t e,t i,b e,b
Here,themappingT(x ,x )reducestoasimpleroto-translation:
i e
 
p +lc
x θ,ϕ
T(x i ,x e )=p y +ls θ,ϕ (46)
ϕ+θ
wherex =[θ,l],x =[p ,p ,ϕ],andweusedacompactnotationtoindicatethesineandcosineofthesumoftwo
i e x y
angles, i.e., c = cos(θ)cos(ϕ)−sin(θ)sin(ϕ). Eachlevelthencomputesproprioceptiveandvisualpredictions
θ,ϕ
throughlikelihoodfunctionsg andg ,whichinthiscasearesimplemappingsthatextractthejointanglesoftheactual
p v
armconfigurationandtheCartesianpositionsofthelimbsandobjectsfromtheintrinsicandextrinsichiddenstates,
respectively:
g (x(i))=θ(i)
p i 0
(cid:34) (cid:35)
p(i) p(i) p(i) (47)
g (x(i))= 0,x t,x b,x
v e p(i) p(i) p(i)
0,y t,y b,y
Reachingthetool’soriginwiththeendeffectorisachievedbyafunction(relatedtoanagent’sintention)thatsetsthe
firstcomponentofthecorrespondingextrinsichiddenstatesequaltothesecondone:
(cid:104) (cid:105)
i(4)(x(4))= x(4) x(4) x(4) (48)
e,t e e,t e,t e,b
Then,wedefineapotentialdynamicsfunctionbysubtractingthecurrenthiddenstatesfromthisintentionalstate:
(cid:104) (cid:105)
f(4)(x(4))=i(4)(x(4))−x(4) = x(4)−x(4) 0 0 (49)
e,t e e,t e e e,t e,0
Notethedecompositionintoseparateattractors. Anon-zerovelocityforthefirstcomponentexpressestheagent’sdesire
tomovetheendeffector,whileazerovelocityfortheothertwocomponentsmeansthattheagentdoesnotintendto
manipulatetheobjectsduringthefirststepofthetask. Sinceapotentialkinematicconfigurationforthetoolisalready
attheagent’sdisposal,inordertospeedupthemovementsimilarfunctionscanbedefinedateveryhierarchicallevel,
16
Figure7: Representationofthedynamicsactiveduringthetwosteps. (a)Theendeffectorispulledtowardthebelief
aboutthetool’soriginthroughf(4). (b)Thetool’sextremityispulledtowardtheballthroughdynamicsf(5). This
e,t e,b
generates an extrinsic prediction error ε(5) that steers the previous level of the potential configuration of the tool.
e
Concurrently,aseconddynamicsf(4)alsopullsbothactualandtoolcomponentsoftheendeffector’sleveltowardthe
e,b
potentialconfigurationoftheball.
bothinintrinsicandextrinsicreferenceframes. Thesecondstepofthetaskinvolvesreachingtheballwiththetool’s
extremity. Hence,wedefinetwointentionalstatesfortheendeffector’sandvirtuallevels,settingeverycomponent
equaltothe(potential)componentrelatedtotheball:
(cid:104) (cid:105)
i(4)(x(4))= x(4) x(4) x(4)
e,b e e,b e,b e,b
(50)
(cid:104) (cid:105)
i(5)(x(5))= x(5) x(5)
e,b e e,b e,b
Theattractorsencodedinthesecondsetofpotentialdynamicsfunctionsexpresstheagent’sdesiretomodifythetool’s
location:
(cid:104) (cid:105)
f(4)(x(4))=i(4)(x(4))−x(4) = x(4)−x(4) x(4)−x(4) 0
e,b e e,b e e e,b e,0 e,b e,t
(51)
(cid:104) (cid:105)
f(5)(x(5))=i(5)(x(5))−x(5) = x(5)−x(5) 0
e,b e e,b e e e,b e,t
Maintainingthissetofdynamicseventuallydrivesthehandintoasuitablepositionthatmakesthetool’sextremity
touchtheball. ArepresentationoftherelationsbetweensuchdynamicsisdisplayedinFigure7.
Now,wedefinethehiddencausesofthelasttwolevels(forsimplicity,weonlydescribetheextrinsichiddenstates):
(cid:104) (cid:105)
v(4) = v(4) v(4) v(4)
e s t b
(52)
(cid:104) (cid:105)
v(5) = v(5) v(5)
e s b
wherethesubscriptss,t,andbindicatetheagent’sintentionstomaintainthecurrentstateoftheworld("stay"),reach
thetool,andreachtheball,respectively. Thefirsthiddencause,relatedtothefollowingintentionalstates:
i(4)(x(4))=x(4) i(5)(x(5))=x(5) (53)
e,s e e e,s e e
areneededtoensurethatv(4)andv(5)encodeproperprobabilitieswhenthediscretemodelisintheinitialstate. The
e e
averagetrajectoryisthenfoundbyweightingthepotentialdynamicsfunctionswiththecorrespondinghiddencauses.
Hence,havingindicatedwithµ(4)thebelief oftheextrinsichiddenstatesoftheendeffector,thegeneratedtrajectoryis:
e
η′(4) =v(4)f(4)(µ(4))+v(4)f(4)(µ(4))+v(4)f(4)(µ(4)) (54)
x,e s e,s e t e,t e b e,b e
17
Thebeliefisthenupdatedaccordingtothefollowingupdaterules:
µ˙(4) =µ′(4)−π(4)ε(4)+∂gTπ(5)ε(5)+∂gTπ(4)ε(4)+∂η′(4)Tπ(4)ε(4)
e e e e e e e v v v x,e x,e x,e
(55)
µ˙′(4) =−π(4)ε(4)
e x,e x,e
Inshort,the0th-orderissubjectto: (i)aquantityproportionaltotheestimatedtrajectory;(ii)anextrinsicprediction
errorcomingfromtheelbow,i.e.,ε(4) = µ(4)−g (µ(4),µ(3)); (iii)abackwardextrinsicpredictionerrorcoming
e e e i e
fromthevirtuallevel,i.e.,ε(5) =µ(5)−g (µ(5),µ(4));(iv)avisualpredictionerror,i.e.,ε(4) =y(4)−µ(4);(v)a
e e e i e v v e
backwarddynamicserrorencodingthegeneratedtrajectory,i.e.,ε(4) =µ′(4)−η′(4). Foramoredetailedtreatmentof
x,e e x,e
inferenceanddynamicsofkinematicconfigurationsinhierarchicalsettings,see[36].
Theactionsaareinsteadcomputedbyminimizingproprioceptivepredictionerrors:
a˙ =−∂ gTπ ε (56)
a p p p
where∂ g performsaninversedynamicsfromproprioceptivepredictionstoactions.
a p
Asconcernsthediscretemodel,itshiddenstatessexpress: (i)whethertheagentisatthetoolposition,attheball
position,ornoneofthetwo;(ii)whethertheagenthasgraspedornotthetool. Thesetwofactorscombinein6process
statesintotal. Thefirstfactorgeneratespredictionsfortheextrinsichiddencausesofthehybridunitsthroughlikelihood
matrices,i.e.,A(4)sandA(5)s. Thisallowstheagenttosynchronizethebehaviorofboththetoolandendeffector;
e e
additionallikelihoodmatricescanbedefinedtoimposepriorsfortheintrinsichiddenstatesandatdifferentlevelsofthe
hierarchy. Thesecondfactorreturnsadiscretetactileprediction,i.e.,A s.
t
Finally,wedefineadiscreteactionforeachstepofthetask,andatransitionmatrixBsuchthattheballcanbereached
onlywhenthetoolhasbeengrasped. Discreteactionsarereplannedevery10continuoustimesteps,andtransitions
betweendiscretestatesoccurdynamicallydependingoncontinuousevidence. IntheexampleofFigure6,thetransition
betweenreachingthetoolandreachingtheballhappensafter350timesteps. Theextrinsichiddencausesv(4)andv(5)
e e
arefoundbyBayesianmodelcomparison,asexplainedinSection3.1.1:
v(4) =σ(lnA(4)s+l(4))
e e e
(57)
v(5) =σ(lnA(5)s+l(5))
e e e
Asnotedabove,A(4)sandA(5)srepresentpredictionsofextrinsichypothesesmadebythediscretemodelfortheend
e e
effectorandvirtuallevels,e.g.,ahighervalueofv(4) andv(5) meansthatthediscretemodelwantstoreachtheball.
b b
Conversely,l(4)andl(5)arethebottom-upmessagesthataccumulatecontinuouslogevidenceoversometimeT,that
e e
is,theyprovideinformationwhethertheendeffectorisreachingthetoolortheballbasedonthecontext. Comparison
betweensuchhigh-levelexpectationsandlow-levelevidencespermitsinferringthediscretehiddenstatesbasedon
potentialtrajectories. Infact,thehiddencausesactasadditionalobservationsforthediscretemodel,whichinfersthe
statesattimeτ bycombiningthemwiththetactileobservationandthehiddenstatesattimeτ −1:
s =σ(lnB s +lnA(4)Tv(4)+lnA(5)Tv(5)+lnATo ) (58)
π,τ π,τ−1 τ−1 e e,τ e e,τ t t
Ifweassumeforsimplicitythattheagent’spreferencesareencodedinatensorC intermsofexpectedstates, the
expectedfreeenergybreaksdownto:
(cid:88)
G ≈ s [lns −lnp(s |C)] (59)
π π,τ π,τ τ
τ
Computingthesoftmaxoftheexpectedfreeenergyreturnstheposteriorprobabilityoverthepoliciesπ,whichareused
toinferthenewdiscretehiddenstatesattimeτ +1.
3.3 Analysisofmodelperformances
Figure6illustratestaskprogressduringasampletrial.Althoughbothobjectsaremoving,thediscretemodelsuccessfully
infersandimposescontinuoustrajectoriesallowingtheagenttooperatecorrectlyandachieveitsgoal. Atthebeginning
ofatrial,thebeliefsofthehiddenstatesareinitializedwiththeactualstartingconfigurationofthearm. Theagentinfers
18
twopotentialkinematicconfigurationsforthetoolandtheball. Whilethetwoobservationsofthetoolconstrainthe
correspondinginference,theballbeliefisonlysubjecttoitsactualposition,thuslettingtheagentinitiallyoverestimate
thelengthofthevirtuallevel. Duringthefirstphase,onlythetoolreachingintentionisactive: asaconsequence,the
toolbeliefconstantlybiasesthearmbelief,whichinturnpullstherealarm. After350steps,boththesebeliefsarein
thesameconfiguration,whiletheballbeliefhasinferredthecorrespondingposition. Atthispoint,thetoolisgrasped,
causingthediscretemodeltopredictadifferentcombinationofhiddencauses. Now,boththetoolandarmbeliefs
arepulledtowardtheballbelief. Afterabout800steps,theagentinfersthesameconfigurationforallthreebeliefs,
successfullyreachingtheballwiththetool’sextremity,andtrackingituntilthetrialends. Notethatevenduringthe
firstreachingmovement,theagentcontinuouslyupdatesitsconfigurationinrelationtotheball;asaresult,thesecond
reachingmovementisfaster.
ThetransitionscanbebetterappreciatedfromFigure3.3,showingthebottom-upmessages(i.e.,accumulatedevidences)
l(4) and l(5) for the last two levels of the hierarchy (i.e., end effector’s and virtual levels), and the discrete hidden
e e
statess . Asevident,thevirtualleveldoesnotcontributetotheinferenceofthefirstreachingtrajectory,sincethis
τ
onlyinvolvestheendeffector. Notehowtheagentisabletodynamicallyaccumulatetheevidencesoveritsdiscrete
hypotheses: duringthefirstphase,theevidencel(4)(relatedtothetoolbeliefattheendeffector’slevel)increasesas
e,t
soonastheendeffectorapproachesthetool’sorigin,whilel(4)andl(5)(relatedtotheballbeliefattheendeffector’s
e,b e,b
and virtual levels, respectively) decrease as the ball moves away. During the second phase, the latter two rapidly
increaseastheendeffectorapproachestheball;finally,everyprobabilityofbothlevelsslowlystabilizesastheextrinsic
beliefsconvergetothesamevalueandtheerrorsareminimized. Theslowdecreaseoftheinitialstateandthefast
transitionbetweenthetwostepsarewellsummarizedinthebottomgraph. Thetrajectoriesofthehiddenstatesshow
thattheagentcanplannewtrajectorieswithahighfrequency(inthiscase,10continuoustimesteps),allowingitto
reactrapidlytoenvironmentalstimuli.
TherelationshipbetweenhiddencausesandcontinuousdynamicsissummarizedinFigure3.3,showingtheextrinsic
potentialandestimateddynamicsfortheendeffector’sandvirtuallevels,aswellasthedynamicsoftheextrinsichidden
causes. Forsimplicity,weconsideredthesamediscretehiddencausesforeveryhierarchicallevel,andthetopplot
recapitulatesthestateofthewholekinematicconfiguration. Here,wenoteasimilarbehaviortothediscretehidden
states(i.e.,slowdecreaseofthestaycauseandincreaseofthefirstreachingmovement,andrapidincreaseofthesecond
reachingmovementinthemiddleofthetrial). Notethattheagentmaintainspotentialdynamicsrelatedtothethree
intentionsforthedurationofthewholetrial;thesedynamicsarecombinedtoproduceatrajectorythatthemotorunits
accomplish. Infact,twospikesareevidentinthedynamicsoftheendeffector’slevel,andoneinthedynamicsofthe
virtuallevel,regardingtheballreachingaction.
Inordertoassessthemodelperformancesindynamicplanningtasks,werunthreedifferentexperiments,eachcomposed
of150trials. Thefirstexperimentassessedthecapacityofpickingamovingtoolandreachingastatictarget,hence,for
eachtrialwevariedthetoolvelocity. Thesecondexperimentassessedthecapacityofpickingastatictoolandtracking
amovingtarget: here,wevariedtheballvelocity. Thethirdexperimentevaluatedtheperformancesoftheagentin
pickingamovingtoolandtrackingamovingtarget: wehencevariedbothtoolandballvelocities. Inallexperiments,
werandomlysampledtoolandballpositions,andtheirdirectionswhererelevant. Also,velocityvariedfrom0to8
pixelspertimestep. Thewidthandheightofthevirtualenvironmentwas1300x1300pixels,twicethetotalarmlength
plusthetoollength;thus,theballandtoolwereoutofreachforasignificantperiodofeachtrial. Thedurationofeach
trialwassetto3000steps.
TheresultsofthesimulationsarevisualizedinFigure9,showingthetaskaccuracy,thetimeneededtocompletethe
task,andtheaveragefinalerror(seethecaptionformoredetails). Withstaticorslow-varyingenvironments,theagent
completesthetaskinallconditions. Thefirstcondition(movingtool)achievedgoodperformancesevenwithhightool
velocity,althoughwithlowvelocitiesthereisaslightdecreaseinaccuracy: sincethetooloftenmovesoutofreach,the
agentcannotaccomplishthetool-pickingaction. Thisspecificbehaviorisnotpresentinthesecondcondition(moving
ball),probablyduetotheincreasedoperationalspacewhichallowstheagenttomovealongthewholeenvironment.
However,theperformancedecreasesforhighballvelocitiesalsoduetotheadditionaldifficultyoftrackingmoving
objects. Thethirdcombinedcondition(movingtoolandball)achievedslightlylowerperformancesthanthesecond
condition,withatimeneededtocompletethetasksimilartothefirstcondition. However,theaveragetrackingerror
showninthebottompanelremainsrestrictedevenwithhighballandtoolvelocities.
19
Figure8: (a)Normalizedlogevidences,for60discretestepsτ (composed,inturn,of10continuoustimesteps),ofthe
endeffector’slevell(4)(top),andvirtuallevell(5)(middle). Discretehiddenstates(bottom). Thegreenandreddashed
e e
linesrespectivelyrepresentthetool’sextremity-balldistance,andthetool’sorigin-endeffectordistance,normalizedto
fitintheplots. AsexplainedinSection3.2.1,theagenthastwodiscretestatesandtwohiddencausesrelatedtothesteps
ofthetask,i.e.,reachingtheballandreachingthetool,withadditionalstaydiscretestateandcause. (b)Dynamics
ofextrinsichiddencausesv (topplot). Normofextrinsicpotentialdynamics||f(4)||(stay),||f(4)||(reachtool)and
e e,s e,t
||f(4)||(reachball),alongwithestimateddynamics||µ(4)′||forendeffector’slevel(middleplot). Normofextrinsic
e,b e
potentialdynamics||f(5)||(stay)and||f(5)||(reachball),alongwithestimateddynamics||µ(5)′||forvirtuallevel
e,s e,b e
(bottomplot). Thedynamicsareplottedfor600continuoustimesteps.
20
Figure9: Performancesofthedeephybridmodelduringtooluseforthreeconditions: amovingtool(inred),amoving
ball(ingreen),andmovingtoolandball(inblue). Accuracy(top),forwhichweconsideredatrialsuccessfulifthetool
waspickedandtheaverageball-tooldistanceforthelast300stepswaslessthan100pixels. Time(middle),measured
asthenumberofstepsneededforthetooltobepickedandfortheball-tooldistancetobelessthan100pixels. Error
(bottom), measuredastheaverageball-tooldistanceforthelast300steps. Foreachcondition, weaggregatedthe
measuresfor150trials. Themiddleandbottomplotsalsoshowthe95%confidenceinterval.
Finally,thedynamicbehavioroftheextrinsicbeliefscanbeanalyzedfromFigure10,showingthetrajectories,fora
sampletrialofthethirdcondition,ofalltheforcesthatmakeuptheupdateofEquation55,forthelasttwolevelsand
everyenvironmentalentity. Thetransitionbetweenthetwophasesofthetaskishereevident: the1st-orderderivative
ofthearmbeliefµ′(4) (bluelineinthetopleftpanel)isnon-zeroduringthewholetask,andpresentstwospikesat
e,a
thebeginningofeachphase, signalinganincreasedpredictionerrorduetothenewintention. Thearmmovement
duringthefirstphaseistheconsequenceofthenon-zero1st-orderderivativeofthetoolbeliefµ′(4)(bluelineinthe
e,t
middleleftpanel). Thedynamicsofthecorrespondingextrinsicpredictionerrorε(5) (greenlineinthemiddleleft
e,t
panel)combinesboththisderivativeandthevisualpredictionerrorofthenextlevelε(5)(redlineinthemiddleright
v,t
panel). Notethatthisextrinsicpredictionerrordoesnotexistforthearmbelief,andthatthebackwarddynamicserror
ε(4) hasasmallerimpactontheoverallupdatewithrespecttothe1st-orderderivative. Thesecondphasebeginswith
e,x,a
aspikeinthe1st-orderderivativeofthetoolbeliefatthevirtuallevelµ′(5)(bluelineinthemiddlerightpanel),which
e,t
ispropagatedbacktothepreviouslevelasanextrinsicpredictionerror. Finally,notethattheballbeliefisonlysubject
toitsvisualobservationandtheextrinsicpredictionerrorcomingfromthepreviouslevels.
21
Figure10: Trajectoryofeverycomponentoftheextrinsicbeliefupdatesfortheendeffector(leftpanels),andvirtual
(right panels) levels. Every environmental entity is shown separately. Blue, orange, green, red, and purple lines
respectively indicate the 1st-order derivative, the extrinsic prediction error from the previous level, the extrinsic
predictionerrorfromthenextlevel,thevisualpredictionerror,andthebackwarddynamicserror.
4 Discussion
Weproposedacomputationalmethod,basedonhybridactiveinference,thataffordsdynamicplanningforhierarchical
settings. Our goal was twofold. First, to show the effectiveness of casting control problems as inference and, in
particular,ofexpressingentitiesinrelationtoahierarchicalconfigurationoftheself. Whiletherecouldbeseveral
waystocombinetheunitsoftheproposedarchitecture,weshowedaspecificdesignasaproof-of-concepttosolve
a typical task: reaching a moving object with a tool. The agent had to rely on three kinds of depth, i.e., it had to
dynamicallyinferitsintentionsfordecision-making,andformdifferenthierarchicalgenerativemodelsdependingon
thestructureandaffordancesoftheentities. Theproposedmodelunifiesseveralcharacteristicsstudiedintheactive
inferenceliterature: themodelingofobjects,recentlydoneinthecontextofactiveobjectreconstruction[70,71,72];
theanalysesofaffordancesinrelationtotheagent’sbeliefs[73];themodelingofitinerantmovements,withabehavior
similar to the Lotka-Volterra dynamics implemented in [74]; planning and control in extrinsic coordinates [75, 6];
inferenceofdiscretestatesbasedoncontinuoussignalsindynamicenvironments,achievedthroughadifferentkindof
post-hocBayesianmodelselection[76]orothervariousapproachessuchasbio-inspiredSLAM[33],dynamicBayesian
networks[77],recurrentswitchinglineardynamicalsystems[78];theuseoftoolsforsolvingcomplextasks[79].
Oursecondgoalwastoshowthata(deep)hierarchicalformulationofactiveinferencecouldlenditselftolearningand
generalizationofnoveltasks. Althoughweusedafixedgenerativemodel,werevealedthatanadvancedbehavioris
possiblebyusinglikelihoodanddynamicsfunctionsthatcouldbeeasilyimplementedwithneuralconnections,and
bydecomposingthemodelintosmallunitslinkedtogether. In[80],ahierarchicalkinematicmodelwasusedtolearn
thelimbsofanagent’skinematicchain,bothduringperceptionandaction. Thesamemechanismcouldbeusedto
inferthelengthoftoolsneededforobjectmanipulation,extendingthekinematicchaininaflexibleway. Therefore,an
encouragingresearchdirectionwouldbetodesignadeephybridmodelinthewakeofPCNs,andlettheagentlearn
appropriatestructureandinternalattractorsforaspecificgoalviafreeenergyminimization. PCNshavedemonstrated
robust performance in tasks like classification and regression [81, 46], while approximating the backpropagation
algorithm[82,83,84,85]. However, fewstudieshaveleveragedthemodularandhierarchicalnatureofpredictive
codingtomodelcomplexdynamics[86,47,87,88,89]orenableinteractionswiththeenvironment[90,91,92,93,18],
mostlydonethroughRL.Well-knownissuesofdeepRLaredataefficiency,explainability,andgeneralization[45].
22
Instead,thehumanbrainiscapableoflearningnewtaskswithasmallamountofsamples,transferringtheknowledge
previouslyacquiredinsimilarsituations. AnothercommoncriticismisthatdeepRLlacksexplainability,whichis
ofgreaterconcernasAIsystemsrapidlygrow. Aviablealternativeistolearnamodeloftheenvironment[94],e.g.,
withBayesiannon-parametrics[18];however,theseapproachesarestillcomputationallydemanding. Albarracinet
al. describedhowactiveinferencemayfindananswertotheblackboxproblem[95], andwefurthershowedhow
differentelementsofanactiveinferenceagenthavepracticalandinterpretablemeanings. Inthisview,optimization
ofparametersinhybridmodelscouldbeaneffectivealternativetodeepRLalgorithms,orotherapproachesinactive
inferencerelyingontheuseofneuralnetworksasgenerativemodels.
Besidesthefixedgenerativemodel,anotherlimitationoftheproposedstudyisthatweonlyusedtwotemporalorders,
whileamorecompleteandeffectivemodelwouldmakeuseofagreatersetofgeneralizedcoordinates[49].Nonetheless,
everyaspectweintroducedcanbeextendedbyconsideringincreasingtemporalorders. Forinstance,discretevariables
coulddependontheposition,velocity,andaccelerationofanobject,thusinferringamoreaccuraterepresentationof
dynamictrajectories. Also,flexiblebehaviorcouldbespecifiedinthe2ndtemporalorder,resultinginamorerealistic
force-controlledsystem.
Aninterestingdirectionofresearchregardsthegenerationofstatesandpaths,aboutwhichusefulindicationsmight
comefromplanningandcontrolwithPOMDPmodels[18]. Someimplementationsofdiscreteactiveinferencemodels
usedadditionalconnectionsbetweenpoliciesandbetweendiscretehiddenstates[96,61];hence,itmightbebeneficial
todesignsimilarconnectionsincontinuousandhybridcontextsaswell. Inthisstudy,asinglehigh-leveldiscretemodel
imposedthebehaviorofeveryotherhybridunit;analternativewouldbetodesignindependentconnectionsbetween
hiddencausessuchthatahigh-leveldecisionwouldbepropagateddowntolowerlevelswithlocalmessagepassing.
Thisapproachmayalsoprovideinsightsintohow,byrepetitionofthesametask,discretepoliciesadapttoconstruct
compositemovements(e.g.,areachingandgraspingaction)fromsimplercontinuouspaths.
Acknowledgments
ThisresearchreceivedfundingfromtheEuropeanUnion’sHorizonH2020-EIC-FETPROACT-2019Programmefor
Research and Innovation under Grant Agreement 951910 to I.P.S. The funders had no role in study design, data
collectionandanalysis,decisiontopublish,orpreparationofthemanuscript.
References
[1] EmanuelTodorov. Optimalityprinciplesinsensorimotorcontrol. NatureNeuroscience,7:907–915,2004.
[2] JörnDiedrichsen,RezaShadmehr,andRichardBIvry. Thecoordinationofmovement: optimalfeedbackcontrol
andbeyond. Trendsincognitivesciences,14(1):31–39,2010.
[3] KarlJ.Friston,TamaraShiner,ThomasFitzGerald,JosephM.Galea,RickAdams,HarrietBrown,RaymondJ.
Dolan,RosalynMoran,KlaasEnnoStephan,andSvenBestmann. Dopamine,affordanceandactiveinference.
PLoSComputationalBiology,8(1):e1002327,January2012.
[4] KarlJ.Friston,JeanDaunizeau,andStefanJ.Kiebel. Reinforcementlearningoractiveinference? PLoSONE,
4(7),2009.
[5] KarlFriston. Whatisoptimalaboutmotorcontrol? Neuron,72(3):488–498,2011.
[6] Karl J. Friston, Jean Daunizeau, James Kilner, and Stefan J. Kiebel. Action and behavior: A free-energy
formulation. BiologicalCybernetics,102(3):227–260,2010.
[7] KarlFriston. Thefree-energyprinciple: Aunifiedbraintheory? NatureReviewsNeuroscience,11(2):127–138,
2010.
[8] ThomasParr,GiovanniPezzulo,andKarlJFriston. Activeinference: thefreeenergyprincipleinmind,brain,and
behavior. Cambridge,MA:MITPress,2021.
[9] MatteoPriorelli,FedericoMaggiore,AntonellaMaselli,FrancescoDonnarumma,DomenicoMaisto,Francesco
Mannella, Ivilin Peev Stoianov, and Giovanni Pezzulo. Modeling motor control in continuous-time Active
Inference: asurvey. IEEETransactionsonCognitiveandDevelopmentalSystems,pages1–15,2023.
23
[10] ThomasParrandKarlJ.Friston. Uncertainty,epistemicsandactiveinference. JournalofTheRoyalSociety
Interface,14(136):20170376,November2017.
[11] RajeshP.N.RaoandDanaH.Ballard. Predictivecodinginthevisualcortex: Afunctionalinterpretationofsome
extra-classicalreceptive-fieldeffects. NatureNeuroscience,2(1):79–87,1999.
[12] AndyClark. Whatevernext? Predictivebrains,situatedagents,andthefutureofcognitivescience. Behavioral
andBrainSciences,36(3):181–204,2013.
[13] JakobHohwy. ThePredictiveMind. OxfordUniversityPressUK,2013.
[14] BerenMillidge,AlexanderTschantz,AnilK.Seth,andChristopherL.Buckley. Ontherelationshipbetween
activeinferenceandcontrolasinference. CommunicationsinComputerandInformationScience,1326:3–11,
2020.
[15] MatthewBotvinickandMarcToussaint. Planningasinference. TrendsinCognitiveSciences,16(10):485–488,
2012.
[16] Marc Toussaint and Amos Storkey. Probabilistic inference for solving discrete and continuous state Markov
DecisionProcesses. ACMInternationalConferenceProceedingSeries,148:945–952,2006.
[17] MarcToussaint. Probabilisticinferenceasamodelofplannedbehavior. KünstlicheIntelligenz,3/09:23–29,2009.
[18] IStoianov,CPennartz,CLansink,andGPezzulo. Model-basedspatialnavigationinthehippocampus-ventral
striatumcircuit: acomputationalanalysis. PlosComputationalBiology,14(9):1–28,2018.
[19] KarlFriston. Hierarchicalmodelsinthebrain. PLoSComputationalBiology,4(11),2008.
[20] KarlJ.Friston,ThomasParr,YanYufik,NoorSajid,CatherineJ.Price,andEmmaHolmes. Generativemodels,
linguisticcommunicationandactiveinference. NeuroscienceI&BiobehavioralReviews,118:42–64,November
2020.
[21] EricRKandel,JamesHSchwartz,ThomasM.Jessell,StevenA.Siegelbaum,andA.J.Hudspeth. Principlesof
Neuroscience. McGraw-Hill,NewYork,5edition,2013.
[22] LucillaCardinali, FrancescaFrassinetti, ClaudioBrozzoli, ChristianUrquizar, AliceC.Roy, andAlessandro
Farnè. Tool-useinducesmorphologicalupdatingofthebodyschema. CurrentBiology,19(13):478,2009.
[23] AngeloMaravitaandAtsushiIriki. Toolsforthebody(schema). TrendsinCognitiveSciences,8(2):79–86,2004.
[24] DanielBaldauf,HeCui,andRichardA.Andersen. Theposteriorparietalcortexencodesinparallelbothgoalsfor
double-reachsequences. JournalofNeuroscience,28(40):10081–10089,2008.
[25] PaulCisekandJohnF.Kalaska. Neuralcorrelatesofreachingdecisionsindorsalpremotorcortex: Specification
ofmultipledirectionchoicesandfinalselectionofaction. Neuron,45(5):801–814,March2005.
[26] KaiUeltzhöffer. DeepActiveInference. pages1–40,2017.
[27] BerenMillidge. Deepactiveinferenceasvariationalpolicygradients. JournalofMathematicalPsychology,96,
2020.
[28] Zafeirios Fountas, Noor Sajid, Pedro A.M. Mediano, and Karl Friston. Deep active inference agents using
Monte-Carlomethods. AdvancesinNeuralInformationProcessingSystems,2020-Decem(NeurIPS),2020.
[29] ThomasRood,MarcelvanGerven,andPabloLanillos. Adeepactiveinferencemodeloftherubber-handillusion.
2020.
[30] CansuSancaktar,MarcelA.J.vanGerven,andPabloLanillos. End-to-EndPixel-BasedDeepActiveInference
forBodyPerceptionandAction. In2020JointIEEE10thInternationalConferenceonDevelopmentandLearning
andEpigeneticRobotics(ICDL-EpiRob),pages1–8,2020.
[31] ThéophileChampion,MarekGrzes´,LisaBonheme,andHowardBowman. Deconstructingdeepactiveinference.
2023.
[32] AlekseyZelenovandVladimirKrylov. Deepactiveinferenceincontroltasks. In2021InternationalConference
onElectrical,Communication,andComputerEngineering(ICECCE),pages1–3,2021.
[33] OzanÇatal,TimVerbelen,ToonVandeMaele,BartDhoedt,andAdamSafron. Robotnavigationashierarchical
activeinference. NeuralNetworks,142:192–204,2021.
24
[34] KaiYuan,KarlFriston,ZhibinLi,andNoorSajid. Hierarchicalgenerativemodellingforautonomousrobots.
ResearchSquare,2023.
[35] Matteo Priorelli and Ivilin Peev Stoianov. Flexible Intentions: An Active Inference Theory. Frontiers in
ComputationalNeuroscience,17:1–41,2023.
[36] MatteoPriorelli, GiovanniPezzulo, andIvilinPeevStoianov. Deepkinematicinferenceaffordsefficientand
scalablecontrolofbodilymovements. PNAS,120,2023.
[37] KarlJ.Friston,ThomasParr,andBertdeVries. Thegraphicalbrain: Beliefpropagationandactiveinference.
1(4):381–414,2017.
[38] KarlJ.Friston,RichardRosch,ThomasParr,CathyPrice,andHowardBowman. Deeptemporalmodelsand
activeinference. NeuroscienceandBiobehavioralReviews,77(November2016):388–402,2017.
[39] Thomas Parr and Karl J. Friston. Active inference and the anatomy of oculomotion. Neuropsychologia,
111(January):334–343,2018.
[40] T. Parr and K. J. Friston. The computational pharmacology of oculomotion. Psychopharmacology (Berl.),
236(8):2473–2484,August2019.
[41] JakobHohwy. Newdirectionsinpredictiveprocessing. MindandLanguage,35(2):209–223,2020.
[42] KarlFristonandStefanKiebel. Predictivecodingunderthefree-energyprinciple. PhilosophicalTransactionsof
theRoyalSocietyB:BiologicalSciences,364(1521):1211–1221,2009.
[43] Michael I. Jordan, Zoubin Ghahramani, Tommi S. Jaakkola, and Lawrence K. Saul. Machine Learning,
37(2):183–233,1999.
[44] H.KobayashiandL.R.Bahl. Imagedatacompressionbypredictivecodingi: Predictionalgorithms. IBMJournal
ofResearchandDevelopment,18(2):164–171,1974.
[45] BerenMillidge,TommasoSalvatori,YuhangSong,RafalBogacz,andThomasLukasiewicz. PredictiveCoding:
TowardsaFutureofDeepLearningbeyondBackpropagation? IJCAIInternationalJointConferenceonArtificial
Intelligence,pages5538–5545,2022.
[46] TommasoSalvatori,AnkurMali,ChristopherL.Buckley,ThomasLukasiewicz,RajeshP.N.Rao,KarlFriston,
andAlexanderOrorbia. Brain-inspiredcomputationalintelligenceviapredictivecoding,2023.
[47] BerenMillidge,MahyarOsanlouy,andRafalBogacz. PredictiveCodingNetworksforTemporalPrediction. pages
1–59,2023.
[48] ThomasParrandKarlJ.Friston. TheDiscreteandContinuousBrain: FromDecisionstoMovement—AndBack
AgainThomas. NeuralComputation,30:2319–2347,2018.
[49] KarlFriston,KlaasStephan,BaojuanLi,andJeanDaunizeau. Generalisedfiltering. MathematicalProblemsin
Engineering,2010:ArticleID621670,34p.–ArticleID621670,34p.,2010.
[50] ThomasParr,GiovanniPezzulo,andKarlJFriston. Activeinference: thefreeenergyprincipleinmind,brain,and
behavior. 2022.
[51] KarlFriston,LancelotDaCosta,NoorSajid,ConorHeins,KaiUeltzhöffer,GrigoriosA.Pavliotis,andThomas
Parr. Thefreeenergyprinciplemadesimplerbutnottoosimple. PhysicsReports,1024:1–29,2022.
[52] RickA.Adams,StewartShipp,andKarlJ.Friston. Predictionsnotcommands: Activeinferenceinthemotor
system. BrainStructureandFunction,218(3):611–643,2013.
[53] GiovanniPezzulo,FrancescoRigoli,andKarlJ.Friston. HierarchicalActiveInference: ATheoryofMotivated
Control. TrendsinCognitiveSciences,22(4):294–306,2018.
[54] KarlJ.FristonandChristopherD.Frith. Activeinference,communicationandhermeneutics. Cortex,68:129–143,
July2015.
[55] Thomas Parr and Karl J. Friston. Generalised free energy and active inference. Biological Cybernetics,
113(5–6):495–513,September2019.
[56] Ryan Smith, Karl J. Friston, and Christopher J. Whyte. A step-by-step tutorial on active inference and its
applicationtoempiricaldata. JournalofMathematicalPsychology,107:102632,2022.
25
[57] Lancelot DaCosta, Thomas Parr, Noor Sajid, Sebastijan Veselic, Victorita Neacsu, and Karl Friston. Active
inferenceondiscretestate-spaces: Asynthesis. JournalofMathematicalPsychology,99,2020.
[58] KarlFriston,ThomasParr,andPeterZeidman. Bayesianmodelreduction. pages1–32,2018.
[59] KarlFriston,JérémieMattout,NelsonTrujillo-Barreto,JohnAshburner,andWillPenny. Variationalfreeenergy
andtheLaplaceapproximation. NeuroImage,34(1):220–234,2007.
[60] KarlFristonandWillPenny. PosthocBayesianmodelselection. NeuroImage,56(4):2089–2099,2011.
[61] KarlJ.Friston,ThomasParr,ConorHeins,AxelConstant,DanielFriedman,TakuyaIsomura,ChrisFields,Tim
Verbelen,MaxwellRamstead,JohnClippinger,andChristopherD.Frith. Federatedinferenceandbeliefsharing.
Neuroscience&BiobehavioralReviews,156:105500,2024.
[62] M.PriorelliandI.P.Stoianov. Dynamicinferencebymodelreduction. bioRxiv,2023.
[63] LéoPio-Lopez,AngeNizard,KarlFriston,andGiovanniPezzulo. Activeinferenceandrobotcontrol: Acase
study. JournaloftheRoyalSocietyInterface,13(122),2016.
[64] MatteoPriorelliandIvilinPeevStoianov. Slowbutflexibleorfastbutrigid? discreteandcontinuousprocesses
compared. Heliyon,pagee39129,October2024.
[65] RickA.Adams,EduardoAponte,LouiseMarshall,andKarlJ.Friston. Activeinferenceandoculomotorpursuit:
Thedynamiccausalmodellingofeyemovements. JournalofNeuroscienceMethods,242:1–14,2015.
[66] M.Priorelli,G.Pezzulo,andI.P.Stoianov. Activevisioninbinoculardepthestimation: Atop-downperspective.
Biomimetics,8(5),2023.
[67] CorradoPezzato,ChristopherBuckley,andTimVerbelen. Whylearnifyoucaninfer? robotarmcontrolwith
hierarchicalactiveinference. InTheFirstWorkshoponNeuroAI@NeurIPS2024,2024.
[68] MatteoPriorelliandIvilinPeevStoianov. Dynamicplanninginhierarchicalactiveinference. NeuralNetworks,
page107075,January2025.
[69] KarlFriston,LancelotDaCosta,DanijarHafner,CasperHesp,andThomasParr. Sophisticatedinference. Neural
Computation,33(3):713–763,2021.
[70] StefanoFerraro,ToonVandeMaele,PietroMazzaglia,TimVerbelen,andBartDhoedt. Disentanglingshapeand
poseforobject-centricdeepactiveinferencemodels,2022.
[71] RubenS.vanBergenandPabloL.Lanillos. Object-basedactiveinference,2022.
[72] ToonVandeMaele,TimVerbelen,Ozanundefinedatal,andBartDhoedt. Embodiedobjectrepresentationlearning
andrecognition. FrontiersinNeurorobotics,16,April2022.
[73] FrancescoDonnarumma,MarcelloCostantini,EttoreAmbrosini,KarlFriston,andGiovanniPezzulo. Action
perceptionashypothesistesting. Cortex,89:45–60,April2017.
[74] Karl J Friston, Jérémie Mattout, and James Kilner. Action understanding and active inference. Biological
cybernetics,104(1-2):137–60,feb2011.
[75] GuillermoOliver,PabloLanillos,andGordonCheng. Anempiricalstudyofactiveinferenceonahumanoidrobot.
IEEETransactionsonCognitiveandDevelopmentalSystems,8920(c):1–10,2021.
[76] TakuyaIsomura,ThomasParr,andKarlFriston. Bayesianfilteringwithmultipleinternalmodels: Towardatheory
ofsocialintelligence. NeuralComputation,31(12):2390–2431,2019.
[77] SheidaNozari,AliKrayani,PabloMarin-Plaza,LucioMarcenaro,DavidMartinGomez,andCarloRegazzoni.
Activeinferenceintegratedwithimitationlearningforautonomousdriving. IEEEAccess,10:49738–49756,2022.
[78] PoppyCollis,RyanSingh,PaulFKinghorn,andChristopherLBuckley. Learninginhybridactiveinference
models,2024.
[79] Ajith Anil Meera and Pablo Lanillos. Towards metacognitive robot decision making for tool selection. In
ChristopherL.Buckley,DanielaCialfi,PabloLanillos,MaxwellRamstead,NoorSajid,HideakiShimazaki,Tim
Verbelen,andMartijnWisse,editors,ActiveInference,pages31–42,Cham,2024.SpringerNatureSwitzerland.
[80] Matteo Priorelli and Ivilin Peev Stoianov. Efficient motor learning through action-perception cycles in deep
kinematicinference. InActiveInference,pages59–70.SpringerNatureSwitzerland,2024.
26
[81] Alexander Ororbia and Daniel Kifer. The neural coding framework for learning generative models. Nature
Communications,13(1),2022.
[82] JamesCRWhittingtonandRafalBogacz.Anapproximationoftheerrorbackpropagationalgorithminapredictive
codingnetworkwithlocalhebbiansynapticplasticity. NeuralComput,29(5):1229–1262,March2017.
[83] JamesC.R.WhittingtonandRafalBogacz. TheoriesofErrorBack-PropagationintheBrain. TrendsinCognitive
Sciences,23(3):235–250,2019.
[84] BerenMillidge,AlexanderTschantz,andChristopherL.Buckley. PredictiveCodingApproximatesBackprop
AlongArbitraryComputationGraphs. NeuralComputation,34(6):1329–1368,2022.
[85] TommasoSalvatori,YuhangSong,ThomasLukasiewicz,RafalBogacz,andZhenghuaXu. Predictivecodingcan
doexactbackpropagationonconvolutionalandrecurrentneuralnetworks,2021.
[86] IvilinStoianov,DomenicoMaisto,andGiovanniPezzulo. Thehippocampalformationasahierarchicalgenerative
modelsupportinggenerativereplayandcontinuallearning. ProgressinNeurobiology,217(102329):1–20,2022.
[87] Linxing Preston Jiang and Rajesh P. N. Rao. Dynamic predictive coding: A model of hierarchical sequence
learningandpredictionintheneocortex. PLOSComputationalBiology,20(2):e1011801,February2024.
[88] TungNguyen,RuiShu,TuanPham,HungBui,andStefanoErmon. Temporalpredictivecodingformodel-based
planninginlatentspace,2021.
[89] MufengTang,HelenBarron,andRafalBogacz. Sequentialmemorywithtemporalpredictivecoding,2023.
[90] BerenMillidge. CombiningActiveInferenceandHierarchicalPredictiveCoding: aTutorialIntroductionand
CaseStudy. PsyArXiv,2019.
[91] AlexanderOrorbiaandAnkurMali. ActivePredictingCoding: Brain-InspiredReinforcementLearningforSparse
RewardRoboticControlProblems. 2022.
[92] Rajesh P. N. Rao, Dimitrios C. Gklezakos, and Vishwas Sathish. Active predictive coding: A unified neural
frameworkforlearninghierarchicalworldmodelsforperceptionandplanning,2022.
[93] AresFisherandRajeshPNRao. Recursiveneuralprograms: Adifferentiableframeworkforlearningcomposi-
tionalpart-wholehierarchiesandimagegrammars. PNASNexus,2(11),October2023.
[94] ThomasM.Moerland,JoostBroekens,AskePlaat,andCatholijnM.Jonker. Model-basedreinforcementlearning:
Asurvey. 2022.
[95] MahaultAlbarracin,InêsHipólito,SafaeEssafiTremblay,JasonG.Fox,GabrielRené,KarlFriston,andMaxwell
J.D.Ramstead. Designingexplainableartificialintelligencewithactiveinference: Aframeworkfortransparent
introspectionanddecision-making. 2023.
[96] ToonVandeMaele,TimVerbelen,PietroMazzaglia,StefanoFerraro,andBartDhoedt. Object-centricscene
representationsusingactiveinference,2023.
27
A Algorithms
Algorithm1Computeexpectedfreeenergy
Input:
lengthofpoliciesN
p
discretehiddenstatess
policiesπ
transitionmatrixB
preferenceC
Output:
expectedfreeenergyG
G ←0
foreachpolicyπ do
π
s ←s
π,τ
forτ =0toN do
p
s ←B s
π,τ π,τ π,τ
G ←G +s (lns −lnC)
π π π,τ π,τ
endfor
endfor
Algorithm2Accumulatelogevidence
Input:
meanoffullpriorη
meanofreducedpriorsη
m
meanoffullposteriorµ
precisionoffullpriorΠ
precisionofreducedpriorsΠ
m
precisionoffullposteriorP
logevidencesL
m
Output:
logevidencesL
m
foreachreducedmodelmdo
P ←P −Π+Π
m m
µ ←P−1(Pµ−Πη+Π η )
m m m m
L ←L +(µTP µ −ηTΠ η −µTP µ+ηTΠη)/2
m m m m m m m m x
endfor
28
Algorithm3Activeinferencewithdeephybridmodels
Input:
continuoustimeT
discretetimeT
intrinsicunitsU(i,j)
i
extrinsicunitsU(i,j)
e
inversedynamics∂ g
a p
proprioceptiveprecisionsΠ
y,p
learningrate∆ ,
t
actiona
fort=0toT do
Getobservations
ift mod T =0then
UpdatediscretemodelviaAlgorithm4
endif
foreachunitU(i,j)andU(i,j)do
i e
UpdateintrinsicunitviaAlgorithm5
UpdateextrinsicunitviaAlgorithm6
endfor
Getproprioceptivepredictionerrorsε fromintrinsicunits
y,p
a˙ ←−∂ gTΠ ε
a p y,p y,p
a←a+∆ a˙
t
Takeactiona
endfor
Algorithm4Updatediscretemodelattimeτ
Input:
discretehiddenstatess
policiesπ
likelihoodmatricesA(i,j)
transitionmatrixB
priorD
accumulatedlogevidencesl(i,j)
Output:
accumulatedlogevidencesl(i,j)
discretehiddencausesv(i,j)
foreachunitU(i,j)do
v(i,j) ←σ(lnA(i,j)s+l(i,j))
endfor
s←σ(lnD+ (cid:80) lnA(i,j)Tv(i,j))
i,j
ComputeexpectedfreeenergyG viaAlgorithm1
π ←σ(−G)
(cid:80)
D ← π B s
π π,0 π,0
foreachunitU(i,j)do
v(i,j) ←A(i,j)D
l(i,j) ←0
endfor
29
Algorithm5Updateintrinsicunit
Input:
beliefofextrinsichiddenstatesofpreviouslevelµ(i−1)
e
beliefofintrinsichiddenstatesµ˜(i)
i
intrinsic(discrete)hiddencausesv(i)
i
proprioceptiveobservationy(i)
p
beliefofextrinsichiddenstatesµ˜(i)
e
intrinsicdynamics(reduced)functionsf(i)
i,m
proprioceptivelikelihoodg
p
extrinsiclikelihoodg
e
proprioceptiveprecisionΠ(i)
y,p
extrinsicprecisionΠ(i)
y,e
intrinsicdynamicsprecisionΠ(i)
x,i
learningrate∆
t
Output:
proprioceptivepredictionerrorε(i)
y,p
extrinsicpredictionerrorε(i)
y,e
η(i)′ ← (cid:80) v(i) f(i)(µ(i))
x,i m i,m i,m i
ε(i) ←y(i)−g (µ(i))
y,p p p i
ε(i) ←µ(i)′−η(i)′
x,i x,i x,i
ε(i) ←µ(i)−g (µ(i),µ(i−1))
y,e e e i e
AccumulatelogevidenceviaAlgorithm2
µ˙(i) ←µ(i)′+∂ gTΠ(i)ε(i) +∂ gTΠ(i)ε(i) +∂ η(i)′TΠ(i)ε(i)
i i i p y,p y,p i e y,e y,e i x,i x,i x,i
µ˙(i)′ ←−Π(i)ε(i)
i x,i x,i
µ˜(i) ←µ˜(i)+∆ µ˜˙(i)
i i t i
30
Algorithm6Updateextrinsicunit
Input:
extrinsicpredictionerrorε(i)
y,e
beliefofextrinsichiddenstatesµ˜(i)
e
extrinsic(discrete)hiddencausesv(i)
e
visualobservationy(i)
v
extrinsicpredictionerrorsofnextlevelsε(i+1,l)
y,e
extrinsicdynamics(reduced)functionsf(i)
e,m
visuallikelihoodg
v
extrinsicprecisionΠ(i)
y,e
extrinsicprecisionsofnextlevelsΠ(i+1,l)
y,e
visualprecisionΠ(i)
y,v
extrinsicdynamicsprecisionΠ(i)
x,e
learningrate∆
t
Output:
beliefofextrinsichiddenstatesµ˜(i)
e
η(i)′ ← (cid:80) v(i) f(i) (µ(i))
x,e m e,m e,m e
ε(i) ←y(i)−g (µ(i))
y,v v v e
ε(i) ←µ(i)′−η(i)′
x,e x,e x,e
AccumulatelogevidenceviaAlgorithm2
µ˙(i) ←µ(i)′−Π(i)ε(i) + (cid:80) ∂ gTΠ(i+1,l)ε(i+1,l)+∂ gTΠ(i)ε(i) +∂ η(i)′TΠ(i)ε(i)
e e y,e y,e l e e y,e y,e e v y,v y,v e x,e x,e x,e
µ˙(i)′ ←−Π(i)ε(i)
e x,e x,e
µ˜(i) ←µ˜(i)+∆ µ˜˙(i)
e e t e
31

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Deep hybrid models: infer and plan in a dynamic world"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
