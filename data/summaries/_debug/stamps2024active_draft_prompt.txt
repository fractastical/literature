=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Active Inference Demonstrated with Artificial Spin Ice
Citation Key: stamps2024active
Authors: Robert L. Stamps, Rehana Begum Popy, Johan van Lierop

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2024

Abstract: A numerical model of interacting nanomagnetic elements is used to demonstrate
active inference with a three dimensional Artificial Spin Ice structure. It is shown that
thermal fluctuations can drive this magnetic spin system to evolve under dynamic con-
straints imposed through interactions with an external environment as predicted by
the neurological free energy principle and active inference. The structure is defined
by two layers of magnetic nanoelements where one layer is a square Artificial...

Key Terms: energy, processes, structure, artificial, neurological, magnetic, demonstrated, inference, active, spin

=== FULL PAPER TEXT ===

Active Inference and Artificial Spin Ice:
Control Processes and State Selection
Robert L. Stamps,∗ Rehana Begum Popy, and Johan van Lierop∗
Department of Physics & Astronomy, University of Manitoba, Winnipeg, Canada
E-mail: Robert.Stamps@umanitoba.ca; Johan.van.Lierop@umanitoba.ca
Abstract
A numerical model of interacting nanomagnetic elements is used to demonstrate
active inference with a three dimensional Artificial Spin Ice structure. It is shown that
thermal fluctuations can drive this magnetic spin system to evolve under dynamic con-
straints imposed through interactions with an external environment as predicted by
the neurological free energy principle and active inference. The structure is defined
by two layers of magnetic nanoelements where one layer is a square Artificial Spin Ice
geometry. The other magnetic layer functions as a sensory filter that mediates interac-
tion between the external environment and the “hidden” Artificial Spin Ice layer. Spin
dynamics displayed by the bilayer structure are shown to be well described using a con-
tinuous form of a neurological free energy principle that has been previously proposed
as a high level description of certain biological neural processes. Numerical simulations
demonstratethatthisproposedbilayergeometryisabletoreproducetheoreticalresults
derived previously for examples of active inference in neurological contexts.
1
4202
beF
72
]llah-sem.tam-dnoc[
4v11221.1042:viXra
Keywords
artificial spin ice, bilayer, active inference, free energy principal, action and perception,
variational Bayesian, Monte Carlo
In recent years new insights into how a brain might processes and respond to information
have emerged from a neurological theory whose primary components are called active infer-
enceandthefreeenergyprinciple. Thetheoryprovidesahigh-levelmathematicaldescription
of neural processes where comparisons are made between predictions and sensory percep-
tions resulting in adjustments that affect future sensory input. The theory is built from
simple general assumptions and has been proposed as a biologically plausible mechanism
that can describe key aspects of motor control and movement regulation.1–5 A fundamental
hypothesis is that many brain functions can be described by Bayesian inference that can be
expressed mathematically using a variational Bayes technique.6–8
The purpose of the present paper is to show that the same theory can describe dynamics
of a non-biological system and how this system might be studied experimentally. Moreover,
we show that active inference in a physical device that permits detailed experimental study
would open up new avenues of investigation that provide a new methodology for probing
complex out-of-equilibrium dynamics and would also have significant potential for a new
type of physical neuromorphic computing.9
We demonstrate these ideas with a model based on Artificial Spin Ice (ASI). Research
into ASI was initially directed towards investigations of complex frustrated systems with an
emphasis on experiment.10,11 One of the benefits has been to provide experimental systems
that can be probed with unprecedented detail on time- and length-scales that are not oth-
erwise amenable to study.12 Some of the most recent developments have expanded the field
to include neuromorphic applications for machine learning13–15 and fabrication of non-trivial
three dimensional structures in complex geometries.16,17
2
Nanomagnet Model
We take inspiration from the long history of using binary spins in models for neural opera-
tions18 and theories for their operation in neural networks (examples are discussed in Ref.19
and references therein). Likewise, Ising spin models have been shown to be well suited to
correctly predict the overall physics of ASIs because the spin’s binary ‘up’ or ‘down’ de-
scribes the nanomagnet’s north-south pole alignment. In the following, we lay out how our
proposed model behaves magnetically, and then map out how that behaviour is a physical
implementation of an active inference agent. In our model, an Ising spin is represented by a
nanometer scale magnet shaped approximately like a rectangular needle. As single-domain
nano-sized magnetic particles, each nanomagnet presents a magnetization aligned along the
needle axis below a critical temperature T without the application of an external magnetic
c
field. At their simplest, nanomagnets can be considered to behave like compass needles in
that they attempt to align their north and south poles according to the direction of magnetic
fields (but constrained by the needle geometry).
We propose two arrays of nanomagnets in a bilayer configuration arranged in a three
dimensional geometry as sketched in figure 1a. This structure is a modification of a geometry
suggestedinBegumPopyet al.20. Thetoplayerismadeofwell-separatedsuperparamagnetic
nanomagnetswhosemagnetizationcanflipdirectionrandomlyandindependentlybythermal
excitation as superparamagnets. However, also as with superparamagnets, the nanomagnets
will try to align their magnetizations as parallel as possible to a sufficiently strong external
magnetic field. The bottom layer is an array of nanomagnets arranged in an ASI geometry
whose nanoelements are placed close enough to interact strongly with one another through
their stray magnetic fields. They also respond to the stray fields produced by the top layer
of superparamagnetic nanomagnets.
Effects due to the magnetic field acting on the top superparamagnetic layer are assumed
to be negligible at the bottom square ASI layer. In this sense the top layer acts as a
“sensor” of magnetic fields from an external environment that transmits information to the
3
“hidden” (from the environment fields) bottom ASI layer. The ASI nanomagnets produce
an average magnetization that can be measured and used to provide a signal which is fed
back to the environment to control future input to the top-layer sensory nanomagnets. We
find that when viewed on the time-scale of the environment, the average magnetization
produced by the hidden layer follows a trajectory described by the magnetic response that
can converge to the neighbourhood of a preset value. In what follows we will show that
this behaviour is described accurately by the neurological free energy principle with active
inference. This formalism provides a useful means of describing complex out-of-equilibrium
non-linear dynamics that evolves on internal timescales that can differ from the timescale
over which the environment changes.
The bottom hidden ASI layer of figure 1a is made of 16 spins in a so-called “square” ice
geometry. This geometry is known to have a two-fold degenerate ground state where the two
lowestenergyspinconfigurationsproduceazeroaveragemagnetizationforthearray.11 These
ground states are characterized by “ice rules” that require two spins at each vertex to point
inwards towards the vertex centre with the other two spins pointing out from the vertex.
The stability of vertex spin configurations depends on temperature as thermal fluctuations
have a finite probability to reverse spins randomly. The configurations of vertex spins can
form topological excitations that propogate through the ASI and govern the emergence of
global metastable nanomagnet spin configurations.10
The local fields acting on the ASI, produced by the top sensory layer, affect directly the
creation and propagation of vertex excitations. We show below how these local fields ma-
nipulate relaxation dynamics in the ASI. Note that our requirements for spins in the sensory
layer are that they do not reverse as easily as the spins in the ASI bottom hidden layer and
are assumed to align independently of both the ASI and of one another. These requirements
present challenges for practical design which are not unrelated to those addressed by mag-
netic data storage technologies. Some considerations are discussed more fully at the end of
this paper.
4
Figure 1: a) Geometry of the bilayer with Artificial Spin Ice for use in active inference. The
top layer of spins (shaded blue in the figure) are placed above the vertices of the so-called
‘square’ ASI geometry (unshaded). The top layer elements interact with the bottom layer
through their individual (stray) magnetic fields as discussed in the text.
b)andc)Anexternalfieldisapplieddirectlytothehiddenspinsquareicelayerfornumerical
simulation of magnetic hysteresis taken over 700 field steps. The field is applied uniformly
to all of the hidden spins and response is simulated with Monte Carlo using N = 100 and
h
T = 0.4 in b) and T = 1.0 in c). The states of the hidden spins are distributed as shown in
h h
the main plot at small fields for three neighbouring field steps near H = 0 for the hysteresis
shown in the insets. Only two significant peaks appear at low temperature, one at M = 0
and the other at M = 0.4. The hysteresis apparent in the b)M(H) indicates relaxation into
long lived metastable states.
d and e) Hidden state response to sensory spins driven by an external field are shown in
(inset to d) for T = 0.4 and in (inset to e) for T = 1.0. Values T = 1.0 and N = 1 are
h h s s
used in both cases. Hysteresis appears for each T and is widest at the higher temperature.
h
The hidden spin states occupied at small external fields are shown in the main plot (d)
for the low T case, and (e) for the higher T case. The wide distributions are because
h h
sensory spin response to the external field creates local fields in the hidden spin array that
access configuration states which are not possible if the external field is applied directly and
uniformly to the entire hidden spin array.
Properties of the Sensory Spin Layer
We use numerical simulations to illustrate the importance of how the sensory layer facilitates
sampling of hidden layer spin configurations. For the numerical simulations a 36 element
square ice is used with nine sensory control elements placed above the vertices internal to
the lattice in the manner depicted in figure 1 (for relevant parameters and other definitions
see Methodology). For simplicity, m is set to unity and m then appears only in the
h s
environment field energy and the magnetic interaction between sensory and hidden spins.
Monte Carlo sampling is made for an ensemble of 100 identical bilayer array replicas sampled
5
independently. ThenumberofMonteCarloSteps(MCS)performedduringeachtimeinterval
are specified separately for the sensory and hidden spins of numbers N and N , respectively.
s h
The algorithm used for sampling is Glauber dynamics.
The stray magnetic field produced by a sensory spin affects most strongly the four spins
thatcomprisetheASIvertexdirectlybelow. TheseadditionalfieldsontheASIspinsimpacts
the stability of each four-spin vertex and will bias the probability of spin reversals for each of
the vertex spins. In this way the sensory layer embodies some aspects of a Markov Blanket in
that it mediates information that the hidden array can receive from the environment.7,22,23
Information flow between the sensory and hidden spin arrays is directed in that the sensory
array is designed to respond to the environment external field whereas the hidden spins
respond only to the individual sensory spin fields.
The importance of this sensory ‘blanket’ design can be appreciated from two aspects of
how spin states in the hidden layer are accessed by the external field. To illustrate this
point, in figures 1b-e comparisons of state access are made between the case of an external
environment field applied directly to the hidden spins without sensory spin fields and the
case of only sensory spin fields acting on the hidden spins.
Weconsiderfirstthecasewithoutsensoryspinmediationoftheexternalfield. Infigure1b
magnetization states accessed in hidden spin array by a magnetic field applied directly to
the ASI are shown for fields sampled during a hysteresis loop. The complete hysteresis loop
is run with 700 field steps as shown in the inset M − H plot. In the hysteresis plot the
dark line represents the average taken of M over all replicas and the lightly shaded lines
are the individual replica M hysteresis loops. The magnetic field is applied uniformly across
all spins in the hidden layer without mediation from sensory spins. The simulation is run
for N = 100 MCS which is long enough to allow the hidden spin system to relax towards
h
long-lived meta-stable states at each applied field and the distribution of these states for
three small applied fields are shown in the main plot.
The temperature in figure 1b is T = 0.4 (in units of dumbbell strength) and state
h
6
occupation is shown in the main plot for the three closely spaced fields chosen near H = 0.
ThedistributionisdiscreteduetotheASIgeometrywhichconstrainspossibleconfigurational
states. Only two of the possible states are occupied significantly which is consistent with
the low open loop fields in the M −H hysteresis. Small changes in the applied field do not
excite additional states as seen by the close proximity of the three neighbouring sampled
field strengths. Changes in the distribution appear at higher the temperature T = 1.0
h
as illustrated in figure 1c. More states are accessed at this higher temperature but the
broadening is limited to states near the two accessed at lower temperature. Note also that
the M−H loop is closed so that the state distribution is now peaked significantly only about
H = 0.
Figures ??d and 1e represent the case where the external environment field only acts on
thesensorspinlayer, andtheASIhiddenlayerexperiencesonlyfieldsgeneratedbythesensor
spins. The temperatures are T = 1.0 with T = 0.4 in figure ??d and T = 1.0 in figure 1e.
s h h
A small hysteresis appears in the inset M −H loops, but the ASI M has large fluctuations
in both cases as evidenced by the replica loops. The main plots show the occupation of ASI
states for (external environment) fields near H = 0, again at three neighbouring values. The
low temperature distribution has six significant peaks spread across all possible values (±1).
The spread associated with the three neighbouring H values is broader than what occurred
for the directly applied field case shown in figure 1b. The higher temperature example in
figure 1e shows a much broader distribution of accessed states as well as a broader loop in
the hysteresis plot inset.
Thedifferencesbetweenhowthe fieldsonthehiddenlayeraregeneratedshownin figure1
can be understood in terms the spin flip processes responsible for changing M values. The
nanomagnet elements in the hidden layer interact strongly, and alignment of these elements
occurs generally in square ASI via a type of avalanche process. When the external field is
applied directly to all the square ASI elements, excitation of these processes will most likely
begin at array edges where the total local interaction field acting on nanomagnets is weakest.
7
Figure 2: Pulse field profile and averaged M response are shown in the left panels. Cross-
correlation is shown in the middle panels, and the distribution of hidden spin states is shown
in the rightmost panels. In (a) a 20 time step wide pulse is applied as directly without
sensory spin mediation. In (b) the pulse is delivered to the hidden spin layer via the sensor
layer. The sensory spins are sampled at T = 0.1 with N = 1. In (c) the 20 time step pulse
s s
width is delivered to the hidden spin layer through the sensory spin layer with T = 0.1 and
s
N = 10. In (d) T = 0.1, N = 10 and the pulse width is 4.
s s s
Avalanches for an ASI directly responding to a uniform applied field will typically begin only
at the edges. When the sensor layer is present, sensor generated local fields can destabilize
orientations within the hidden layer ASI at sites away from the array edges. This leads to
a greater probability of nanomagnet reversal avalanches nucleated at sites within the ASI
array closest to the sensor spins.
From these examples, one can see that the top sensor layer provides local fields that fa-
cilitate broader sampling of states than a globally applied field can. This is further enhanced
by spatial differences of local fields that will appear because each sensory spin fluctuates
randomly due to thermally driven reversal but in a manner that the average remains con-
sistent with the value of the external field. The ability to leverage multiple states through
defects or direct control of ASI states was noted several years ago by Budrikis using graph
theoretical methods.24,25 The behaviour observed in the present paper can be understood
similarly in that local fields acting on a subset of spins can open pathways for avalanche
processes that would otherwise have a low probability of occurring. This gives the overall
hidden layer ASI access to a larger portion of configurational phase space through the top
sensor layer than is possible when a field is applied directly to the ASI. Another perspective
is to consider that the spread of states sampled through the ensemble of hidden spins is wide
8
because there exists a multitude of possible trajectories available to the correlated hidden
spins as they evolve in time towards a lower energy configuration. This distinguishes the
hidden spin system from a purely random Markov sampling of states. A range of possible
trajectories towards some minimal energy is fundamental to the operation of the system in
the active inference applications that will be discussed later.
The temporal duration of a signal (i.e. the time-span or pulse-length of the applied field
of the environment) is also important, and here too the bottom hidden layer’s response is
facilitated by the top sensor layer. To understand this, we examine the bottom hidden layer
response to a pulsed environment field without a top sensor layer. Figure 2a contains results
for a pulsed external field where H lasts for 20 time steps (with no sensory spin mediation)
at T = 1.0. The left panel of figure 2a shows H for a 20 time-step wide pulse profile
h
(black line), while the M response averaged over all replicas of the system is shown in blue.
t
The middle panel of figure. 2a is the cross-correlation response between the pulse and the
M response to the pulse for delay times ranging up to 15 time steps. The correlation is
t
normalized and calculated as described in Hon et al.14 using the definition
(cid:34) (cid:35)2
(cid:88) (M −⟨M ⟩ ){p (d)−⟨p ⟩ }
R2(d) = t t t t t−d t . (1)
(N −1)σ σ
M p
t
Theaveragesateachtimesteparetakenoverthereplica-averagedhiddenlayermagnetization
response M calculated at each time step t, and the pulse input to the sensory spins p . The
t t
averages are over the time interval sampled, and σ and σ are the corresponding standard
M p
deviations. The delay between M and p is d. The cross-correlation presented in figure 2a
t t
reaches a maximum at around six time steps into the pulse, and decays rapidly thereafter.
The states accessed throughout all times are tightly grouped around M = 0 as shown in the
t
right panel of figure 2a.
The effects of sensor layer mediation are shown in figure 2b where T = 0.1 and N = 1.
s s
Clearly, sensor-layer-mediated input to the hidden spins increases dramatically the hidden
9
layer’s sensitivity and response to the field pulse. The correlation R2 and distribution of
states (middle and right panels of figure 2b, respectively) present a delayed, but significant,
M(t) response to the onset of the pulse, with a clear asymmetry for M = −1 in the state
distribution that corresponds to the pulse.
The results of a larger sensory spin MCS, with N = 10, is shown in figure 2c with
s
T = 0.1. The M response is now better synchronized with the pulse as can be seen from
s t
the response profile and the R2 correlation. The distribution of states has states spread away
from the M = −1 value. Lastly, figure 2d illustrates sensitivity to pulses through sensor
t
layer mediation. Here the field pulse width is reduced from 20 to 4 time steps. We find
that when there is no sensor layer, the M(t) response of the hidden layer to the significantly
shorter field pulse is very weak (not shown). The left panel of figure 2d shows that the
hidden layer M response to the shorter field pulse is substantial when done through the
sensory spins local fields (right panel). Also, strong correlation with the pulse remains, and
the distribution of states is similar to that in figure 2c.
This sensitivity to changes in environment external field and the ability to activate a
range of states is perhaps central to the operation of the system for active inference. As
discussed earlier, the sensor layer creates local fields that, through avalanche dynamics,
instigate configurational changes that drive the hidden system through its spin state phase
space. Inwhatfollows, itwillbeseenthatthissensitivityenablesthesystemtosearchwidely
for configurations that, when active inference is enabled, direct the hidden spin evolution
trajectory towards targets.
Sampling the Variational Free Energy and Implementation
of Active Inference
A few definitions are required before we can describe how the above bilayer system is able to
perform active inference. The theory we describe is a formulation in terms of stochastic dif-
10
ferentialequationsproposedbyFriston.26 Onlyasummaryoftheessentialpointsispresented
here as the complete theory is well described in numerous other papers. The formulation
and examples we use largely follow the description presented by Buckley et al.27.
Theexternalenvironmentdefinesatimesequencethatissampledregularlybythesensory
spins. The variables sampled consist of data encoded as time varying fields and can represent
differentcomponentssuchaspositions,velocitiesandaccelerations,andotherratesofchange.
In the formalism used here, variables are defined as generalized coordinates which facilitate
the multiple timescale feature highlighted earlier and enables definition of continuous time
evolution of otherwise stochastic quantities. A generalized x(t) coordianate is defined by x(t)
and its derivatives. These can be represented as a list x˜ of data values where each component
is a derivative with respect to t. To simplify the notation, a component of x˜ is denoted by
x = dαx/dtα. A notable feature of this representation is that the action of a derivative on
α
x˜ is defined as the promotion of a component to its next higher order derivative component.
This operation is denoted by the operator D where Dx → x (within the same vector
α α+1
component set x˜).
The configurational states of the sensory spins create a mapping of the environment with
˜
a component of ϕ corresponding to a component of x˜ at time t. Here, a sensory state ϕ
α
is sampled during a MCS over the sensory spins in the presence of an external environment
magnetic field, representing a component of x˜. The value assigned to ϕ is an average over N
s
MCS for the ensemble of system replicas. An average generated this way is made for each
˜
member of x˜, and thereby mapped to the corresponding values of ϕ.
In a similar manner, state averages µ˜ are defined for the hidden spins where an average
˜
of an ensemble of hidden spin states is mapped to the ϕ (which are in turn mapped to the
x˜). Note that the timescale for this mapping is not specified by a particular t, but depends
on the number of MCS taken during a particular t. This provides an interesting separation
of time scales between the changing values of the x˜ that occur on the environment time, and
˜
the response of the bilayer components µ˜ and ϕ that are changing during stochastic thermal
11
relaxation processes.
The probability for a thermal reversal of a nanomagnet element ‘spin’ during a time
interval ∆t depends on the energy difference between an initial and possible final state and
the attempt frequency. This can be described by an Arrhenius relation, νexp(−βϵ) where β
istheinversetemperatureandϵtheenergydifferencebetweenstates. Theattemptfrequency
ν measures the number of pathways to an energy barrier saddle point relative to the number
of pathways over that barrier. The Monte Carlo algorithm used herein does not take ν
into account directly. However the number of MCS determine to what degree configurations
sampled during one ∆t time interval are correlated with those of the previous interval. This
provides a very rough analogy to the prefactor ν, and captures some sense of the different
timescales for relaxation that the spins in the two layers experience. Larger MCS values
correspond to less ‘memory’ between time steps (determined by the environment applied
field). The parameters N and N thereby determine the rates at which the spins in the
s h
layers remain coherent over time with respect to the ‘environment clock’ determined by ∆t.
As will be seen below, judicious choice of N and N are important to optimize performance
s h
of the bilayer system for active inference.
During thermal relaxation, transitions change states µ˜ to new average values. A varia-
tional Bayes relaxation is used to infer the best estimate of the distribution describing the
joint probability P(µ,ϕ) of having a state µ when the sensory spins are in a state ϕ. The
algorithm for this describes the evolution of a trajectory for each component of µ˜ defined by
˜
dµ ∂E(µ˜,ϕ)
α
−Dµ = −κ . (2)
α
dt ∂µ
α
The quantity E(µ˜,ϕ ˜ ) plays the role of a negative entropy as defined in ensemble learning27
˜
which assigns E to a distribution P(µ,ϕ) ∼ exp{−βE(µ˜,ϕ)} that represents the unknown
joint probability relating hidden states to sensory states.
˜
As key point of the present paper, E(µ˜,ϕ) is here viewed as a thermodynamic energy
associated with the bilayer system. The gradient in Eqn. 2 suggests an interpretation of the
12
right hand side as a conjugate field to µ driving states of the hidden spins along a trajectory
α
that leads to the condition dµ /dt−Dµ = 0. This condition describes a trajectory through
α α
state phase space where the time evolution of the average µ˜ is directed towards the maximum
of the distribution that is being inferred.28
These ideas are illustrated for the ASI bilayer through the results presented below. The
first example is for tracking of the magnetization M to a target parameter value subject to
unknown (to the bilayer spins) external constraints imposed by the environment.
This example is set up with a model adapted from Baltieri29. Here, the environment is
defined by a one dimensional equation of motion specified as
dx(t)
= −γx(t)+a(t). (3)
dt
This equation describes dissipative motion along x that would decay to zero via a friction
γ unless offset by a velocity term a(t). The target specifies a fixed velocity dx/dt = v the
d
system should arrive to as it relaxes to the condition dµ /dt−Dµ = 0.
α α
The sensory spins’ response to the environment field are mapped to position and velocity
information through generalized coordinate vectors x˜ = {x(t),dx(t)/dt} at regular time
steps.28,30 An important note is that here the same bilayer is used for each component of x˜
with each component updated sequentially as discussed below. Each of the bilayer’s hidden
spins will respond with an overall magnetization M that is defined by an ensemble average
α
over a quantity χ(B ) via
α
M = ⟨χ(B )⟩ , (4)
α α T
where χ is the magnetization measured along one direction of the spin lattice array that
˜
experiences the field B . The average M replaces ∂E(µ˜,ϕ)/∂µ in Eqn. 2.
α α α
˜
The quantity B is a field corresponding to a linear combinations of µ˜ and ϕ com-
α α α
ponents that are determined in the following way: χ(B ) samples a component of the log
α
joint probability P(µ˜,ϕ ˜ ). Following Friston et al.31, under the assumption of statistical
13
independence, one arrives at the factorizations
(cid:89)
˜
P(µ˜|ϕ) = P(µ |ϕ ), (5)
α α
α
and
(cid:89)
P(µ˜) = P(µ |µ ) (6)
α+1 α
α
Samplingdistributionsproducedbythelogoftheseprobabilitiesthenreducestheproblemto
asummationofM termswheretheB inEqn.4aredeterminedbytheconditionalvariables
α α
˜
appearing in the P(µ˜|ϕ). For example, the ensemble average of logP(µ |ϕ ) corresponds
α α
to evaluating ⟨χ(ϕ −µ )⟩ . Note that when using the Laplace approximation of Gaussian
α α T
distributions for each P, the precision of the Gaussians enter as adjustable parameters that
can optimize the ability of the system to relax towards target values of sensory input. In the
bilayer, theinversetemperaturesβ andβ becomethecorrespondingadjustableparameters,
s h
although N and N also play a role.
s h
The final step is to include active inference. Active inference provides a feedback from
the hidden spins to the world environment, and in the present context is represented by the
a(t) appearing in Eqn. 3. This term is defined by imposing a functional dependence for ϕ(a)
such that it appears as a time dependent constraint on minimization to the steady state
condition of Eqn. 2. A time evolution defined as a force
˜
da(t) dϕdE(µ˜,ϕ)
= −κ (7)
dt da dϕ
is assumed. Proceeding as above, the bilayer equivalent replaces E with ⟨χ(B )⟩ to deter-
α T
mine the form of Eqn. 7. The manner in which a(t) enters µ˜ is through definition of a target
probability P . In general this corresponds to assigning a(t) to a particular component (or
S
components) of µ˜.
In this particular tracking example, the factorization requires only two generalized com-
14
Figure 3: a) and b) present two example applications of active inference and the variational
free energy principle. In (a), sensory layer spins receive environment position and velocity
information and active inference generated by hidden layer spin states drive the system
towards a target velocity v = 0.6. In (b) environment measures are determined by a
d
function T(x) and its derivative dT(x)/dx, but the sensory spins receive only the values T
and dT/dx without information of x or the functional form of T(x). Active inference drives
the system to a specified target T(x) = T . The trajectory of the sensory spin states is shown
d
in (c) for the example in (b). Active inference feeds information derived from the hidden spin
state averages to the environment that causes changes in subsequent sensory input. In this
way the trajectory of states within the hidden layer are directed toward the target goal by
the sensory spins in response to changes in the environment. The corresponding trajectory
for the hidden spins is shown in (d).
ponents for each of the sensory and hidden spin states. They are ϕ and ϕ for the spin
0 1
layer, and µ and µ for the hidden layer. The target appears in the µ component via the
0 1 1
transition probability P (µ |v −αµ ). Note that the factorization of P(µ˜) is truncated by
S 1 d 0
requiring µ = 0.
2
The resulting evolution equations for the first example shown in figure 3 are
dµ
0
= µ +⟨χ(β (ϕ −µ ))⟩ −α⟨χ(β (µ +αµ −v ))⟩ , (8)
1 z 0 0 T w 1 0 d T
dt
dµ
1
= ⟨χ(β (ϕ −µ ))⟩ −⟨χ(β (µ +αµ −v ))⟩ , (9)
z 1 1 T w 1 0 d T
dt
da
= −⟨χ(β (ϕ −µ ))⟩ . (10)
z 1 1 T
dt
In this expression, a generalization of the effective temperature is introduced which al-
15
lows the temperature of the hidden layer for each bilayer to be treated as a parameter.
In Baltieri29 separate precisions are defined for the Gaussian distributions that are instead
here sampled from the ASI. Here we define corresponding parameters as β = β Π and
z h z
β = β Π with additional parameters Π and Π that are used to adjust the probabilities
w h w z w
affecting the ϕ and µ terms. This generalization provides a functionality analogous to the
α α
precisions used in Baltieri29. Unless specified otherwise, for simplicity we specify only T (so
h
that β = β ) or β and β separately. We also note that T = 0.5 for all simulations that
z w z w s
follow.
In order to confirm minimization, a measure of the free energy is defined as the lowest
˜
order contribution to the factorized logP(µ˜,ϕ):
1
F ≈ [β (ϕ −µ )2 +β (ϕ −µ )2 +β (µ +αµ −v )2]. (11)
z 0 0 z 1 1 w 1 0 d
2
The panels in figure 3a are the result of iterating Eqs. 8 through 10. The top panel of
figure 3a shows how µ and ϕ evolve in time with reference to the target v . The middle
0 0 d
panel shows the time evolution of µ , ϕ and a. The corresponding free energy is in the
1 1
bottom panel. The system evolves to the target value of v = 0.6 using β = 10 and β = 1.
d z w
Here N = 1, and N = 1 at each time step. This system still arrives at the target for larger
s h
values of N or N with generally less noise. The ability of the system to reach the target
s h
is sensitive to the values of the inverse temperatures in analogy to sensitivity to precisions
noted in Baltieri29.
In the above example a model of the environment was encoded in the hidden spin system
throughP byinclusionoftheαdissipationterm. Thisnextexampleillustratesthatonlythe
S
target need be encoded in the hidden states, and additional information can be inferred from
the environment at the sensory level. The basis for this example is the thermostat problem
presented in Buckley27 and discussed in the context of proportional integral derivative (PID)
16
control in Baltieri29,32. The sensory input is from two functions of position x defined by
1
T(x) = , (12)
1+x2
and
dT(x) 2x
= − . (13)
dx (1+x2)2
Action is included in dx/dt:
dx(t)
= a(t), (14)
dt
In this model, target information T is included through P (µ |µ −T ), but no information
d S 1 0 d
about x˜ is passed to the bilayer. The equations of motion are truncated by requiring µ =
3
0. The equations for µ˜ are of the same form as in Buckley et al.27, repeated here for
completeness:
dµ
0
= µ +⟨χ(β (ϕ −µ ))⟩ −⟨χ(β (µ +µ −T ))⟩ , (15)
1 z 0 0 T w 0 1 d T
dt
dµ
1
= µ +⟨χ(β (ϕ −µ ))⟩ −⟨χ(β (µ +µ −T ))⟩ +⟨χ(β (µ +µ ))⟩ , (16)
2 z 1 1 T w 0 1 d T w 1 2 T
dt
dµ
2
= −⟨χ(β (µ +µ ))⟩ . (17)
w 1 2 T
dt
The time dependence of the action for this example given by
da dT(x)
= −⟨χ(β (ϕ −µ ))⟩ . (18)
z 1 1 T
dt dx
Results are shown in figure 3b for the same temperatures as above but with N = 10 (N
s h
remains 1). The increased value of N helps optimize the trajectory to arrive at the target
s
value T = 0.3. The µ and ϕ time evolutions are shown in the top panel, µ , ϕ and a
d 0 0 1 1
are in the middle panel, and the free energy is in the bottom panel; it is clearly minimized
˜
as the system evolves. The decaying oscillations observed in the µ˜ and ϕ are strikingly
0 0
17
reminiscent of PID control behaviour as discussed by Baltieri32. The authors therein note
that optimization of parameters can be performed to enhance the decay toward the target,
and we find the same.
It is illuminating to track the evolution through ϕ and ϕ when viewed as a phase space
0 1
trajectory. The trajectory corresponding to the evolutions of the sensory input shown in
figure 3c is for the time evolution of hidden states shown in figure 3d. The corresponding
trajectory for the hidden state µ˜ is quite different as shown in figure 3d. The sensory inputs
˜
circle through ϕ to oscillate around the target value. The hidden states µ˜ lie roughly along a
quasi-linear line centred about the neighbourhood of the target value. Gradients of the free
energy direct evolution of the hidden states towards a most likely maximum that lies along
this line at the target value. There is a strong dependence on the number of sensory spins,
N , where the system has difficulty finding the target for small values causing it instead to
s
relax to ϕ = 0. The optimal parameter values for achieving the target also depend on the
0
magnitude of the target T . As noted earlier, thermal evolution during environment time
d
scales within the hidden spin system depend on temperatures and MCS.
Details of how these parameters affect the trajectories and relaxation are not well un-
derstood at present, but some qualitative observations can be reported. The results shown
in figure 3 require an amount of environmental time to relax towards the target, and also
depend on T and N . Increasing N generally appears to increase the correlation between
h h h
˜
µ˜ and ϕ, while introducing more noise into µ˜ and the free energy. T (and its generalization
h
to β and β ) is an important optimization parameter for achieving the lowest free energy
z w
for different T .
d
A very interesting aspect is the dependence on β and β of the time evolution for
z w
˜
ϕ and µ˜. Unusual oscillations can appear for different temperatures and targets that may
contain information about internal state selection within the hidden layer. These are aspects
currentlyunderstudythatareoutsidethescopeofthepresentwork, butanexampleisshown
in figure 4.
18
˜
Figure 4: The oscillations of µ˜ and ϕ relax toward a target T = 0.5 in (a) for parameters
d
β = 3.0 and β = 0.75. Similar oscillations are found using the same parameters when
z w
the ⟨χ⟩ is replaced with a Gaussian distribution approixmation. A very different quasi-
T
periodic oscillation arising from the ASI through ⟨χ⟩ is found for β = 2.0 and β = 0.5 as
T z w
shown in (b). A Gaussian approximation for these β values is instead similar to the results
α
shown in (a). The oscillations correspond to limit cycles as seen with the trajectories for the
˜
corresponding x˜ and ϕ displayed in (c).
Results for a T = 0.5 with β = 3.0 and β = 0.75 are shown in figure 4a. These are
d z w
qualitatively similar to those shown in 3b although with noticeable differences that illustrate
sensitivity of the nanomagnet dynamics to temperature. We note that the results can be
fit using hyperparameters of a Gaussian distribution approximation for the µ˜ distributions
(which is done by replacing the ⟨χ⟩ with Gaussians as in Buckley et al.27). Using instead
T
β = 2.0 and β = 0.5 we find the results shown in figure 4b. Quasiperidic oscillations
z w
˜
in ϕ and µ˜ which we were not able to reproduce using the Gaussian approximation. The
corresponding trajectories for the sensory and hidden averages are shown in figure 4c. The
oscillations appear similarly for longer time integrations suggesting a stable limit cycle.
In any case, this strong dependence on temperature and precision weightings suggest that
non-trivial dynamics can arise under certain circumstances. Moreover, there appears to be
sensitivity to details of the ASI design as we find different behaviour when we change the
lattice geometry from square to pinwheel by rotating the nanoelements.
19
Conclusions
Perhaps the most interesting aspect demonstrated in this paper is the possibility of studying
transition dynamics in experimentally accessible magnetic and nonmagnetic systems using
active inference. Viewed as a methodology for probing system dynamics, the mathematical
treatment and intrinsic separation of timescales underlying the variational free energy and
active inference theory may inspire new ways of studying experimentally a range of complex
systems. Therearefewreportsintheliteratureofapplicationsoftheneurologicalfreeenergy
principle in physical experiments. One example is saccadic eye motion2,33 and another is
a recent report on small assemblies of neurons.34 Trajectories of quantities defined for this
˜
paper (µ˜ and ϕ) are sensitive to parameters of the theory that can be associated with
measurable propertiesofthesystemunderstudy. Ourpurposehasbeentosuggestarelatively
simple, non-biological system that can serve as a testing ground for exploring this approach
to active inference theory in contexts that can be studied experimentally.
The essential component of our approach is the sensor layer and how it provides an
interface to the measured environment in a manner analogous to a Markov Blanket (as
discussed by Kirchhoff et al.23). The mechanisms at play in the bilayer system involve local
fields generated by the sensory spins in response to input from an environment. These local
fields act on the spins in the hidden layer, enabling expedited relaxation towards a minimum
free energy defined by a target constraint imposed through active inference. The ability to
sampleregionsofhiddenspinstatespaceinthiswayisreminiscentof,butquitedistinctfrom,
other strategies sometimes used for escaping long lived metastable states in computations,
such as parallel tempering.35 It is interesting to speculate that the need to access a range
of states relevant for achieving a target may be analogous to the requirement of sufficient
memory and computational capacity for a system to be suitable for reservoir computing.14
It is also interesting that the trajectory through states of the hidden spins that satisfies
Eqn. 2 can be described as a temporally shifted reference frame under the Fokker-Planck
Kolmogorovformalism.28,36,37 Fromthisperspective,theconditionforastationarytrajectory,
20
dµ /dt−Dµ = 0, matches the environment time scale governing input to the sensory spins
α α
withthetimescaleoverwhichinternalhiddenstateprocessesarethermallyaveragedbystate
transitions. As noted by Balaji et al.,30 the stationary trajectory condition means the hidden
spin states evolve along time dependent free energy gradients that lead toward target values.
In the simulations discussed above, this condition assigns a physical time interval to thermal
transition probabilities sampled using Glauber dynamics. It may be possible to exploit this
feature to probe timescales for state transitions processes in experiments with systems whose
complex energy landscape are being measured, such as spin- or structural glasses. We note
that spin glasses have been explored somewhat in regards to active inference from a different
direction by Heins et al.38.
The spin models we used are simple yet appear sufficient to capture the essential features
displayedinexperimentsusingactualnanomagnets. Someimplementationofactiveinference
may be possible to observe with suitably designed nanomagnet arrays using fabrication
technology that is already available. For simplicity, we have here considered only magnetic
interactionsfromtheexternalappliedfieldandfromthenanomagnetsinthebilayer. However
ourmodelcreatespracticalchallengesforimplementationbecauseofourrequirementthatthe
environmentmagneticfieldaffectdirectlyonlythetoplayerofmagneticnanoelements. There
are a number of different strategies that would meet this requirement. For example, one
could use vertical magnetic sensor elements with horizontal environment fields or variations
of techniques for domain switching as used in magnetic data storage. Alternatively, a voltage
actuated sensor layer using multiferroic nanomagnets such as ϵ-Fe O 39 would create local
2 3
magnetic fields. Many of these technologies also permit integration into other spintronic and
CMOS-based devices.
Concerning possible applications, a great potential of creating energy efficient platforms
for machine learning algorithms using nanomagnetic arrays have been identified and some
in cases demonstrated in ASI13,14 as well as other magnetic material platforms.40,41 Most re-
cently, magnetic nanoparticle artificial spin ice configurations have been studied experimen-
21
tally15 whose state configurations are manipulated locally and detected using microwaves.
The energy efficiency of nanomagnetic systems for machine learning applications, as well
as the microwave properties that can be associated with configurational states, make these
systems attractive for practical use. The ability to incorporate active inference into architec-
tures based on nanomagnets would be a new aspect to pursue. A very interesting direction
to explore would be implementation of hierarchical structures42 where multiple sensory and
hidden layers could be connected, and features associated with learning and other complex
tasks explored. Although not reported here, stacked bilayers of the form presented above
appear able to mimic some features of learning at different timescales that can facilitate
optimizations toward target values.
Finally, the nanomagnet architecture used here is only one example ASI geometry. Al-
though not reported here, some variants of the square ASI geometry also display analogous
properties under active inference. An advantage to using ASI as a model platform is the free-
domtoimposeconstraintsandintroducefrustrationthatcanbestudiedaspathwaysthrough
configurational states. These can be analysed in a variety of ways including graph theoretical
methods as discussed for athermal ASI by Budrikis43. Moreover, pathways through state
configurationsinthesesystemscanoftenbeunderstoodviatopologicalexcitationswithrules
determined by allowed transitions. Active inference methodologies may be helpful in this
context to understand how trajectories through configuration space can be manipulated via
environmental control of topological excitations. For this reason it would be particularly
exciting to examine in non-Ising spin systems where topological excitations arise through
competing interactions. A prime example is magnetic skyrmions in thin film geometries
which enable control and detection via local electric potentials. Skyrmions are topologically
protected spin textures and display non-linear response to applied fields and undergo tran-
sitions through mediating metastable states.44,45 These systems can display a sufficiently
complex state space for applications in reservoir computing.46,47
22
Methods
Interactions in our numerical model of the bilayer system are provided by magnetic fields
producedbythenanomagnets. Thegeometryusedforthehiddenspinsisdefinedbyasquare
lattice of lattice constant a with elements of length ℓ = a/2 aligned diagonally within each
unit cell. The fields produced by the sensory spins have strengths in the hidden spin layer
that depend on the layer separation distance. The separation assumed throughout is a/10.
Sensory spin nanomagnet elements have a magnetic ‘spin’ moment m while ASI hidden
s
elements have a m ‘spin’ moment for each element. Each nanomagnet spin is assumed to
h
be independent of temperature for the ranges considered here. Nanomagnet generated fields
acting on a single nanomagnet are represented as a sum over all other nanomagnet spins at
distances r with directions specified by unit vectors for each spin ϵˆ,ϵˆ . The corresponding
ij i j
interaction energy in the dipole limit is given by
(cid:20) (cid:21)
(cid:88) ϵˆ ·ϵˆ (ϵˆ ·⃗r ))(ϵˆ ·⃗r ))
i j i ij j ij
D = D −3 (19)
|r |3 |r |5
ij ij
i,j
The prefactor contains the nanomagnet magnetization and volume as D = µ /4πa3 . In
0
our model all dipole sums are approximated by dumbbell charges as outlined in Castelnovo
et al.21 which takes into account the length of the nanomagnet elements.
The sensor layer nanomagnets’ alignment with the external field is stochastic in that each
nanomagnet’s spin has a finite probability of aligning with the external field. In this regard,
the sensory and hidden layer spins experience different temperatures T and T due to the
s h
different magnetic properties of each layer. The probability of a magnetization reversal for
a nanomagnet at a temperature in a field specific to the layer is described by an effective
temperature. Spinsinthesensorylayerareataneffectivetemperatureβ −1 = k T /m while
s B s s
the effective temperature of the hidden spins is β −1 = k T /m , where k is Boltzmann’s
h B h h B
constant.
The energy used for Monte Carlo simulations includes field and dipole interaction terms
23
acting at each spin, and is used to determine the energy change ∆E for a possible spin
reversal at each site. With a spin σ (r ) at site r in the sensory layer, and spin σ (r ) at
s s s h h
site r in the hidden layer, the approximate ∆E for a sensory spin σ (r ) experiencing an
h s s
external applied field h is: (note that σ = ±1)
e
(cid:40)
∆E(r ) =2β −m h σ (r )
s s s e s s
(cid:41)
(cid:88) (cid:88)
+m σ (r ) D(r −r′ )m σ (r′ )+m σ (r ) D(r −r′ )m σ (r′ )
s s s s s s s s s s s s h h h h
r′
s
r′
h
(cid:40) (cid:41)
(cid:88)
≈2β −m h σ (r )+m σ (r ) D(r −r′ )m σ (r′ ) . (20)
s s e s s s s s s h h h h
r′
h
The first term in Eqn. 20 is the interaction energy of the sensory spins with the environment
field and is proportional to m of a sensory element. The second term represents the dipole
s
field interactions between the sensory spins.
The first dipole term in the top part of Eqn. (20) is neglected under the assumption
that the environment field energy is much larger than the dipole interaction between sensory
spins. An additional assumption is that the sensory spin m is much larger than the hidden
s
spin m . This means the h term dominates (except for very small applied fields) and
h e
the D energy contributes effectively as a small noise term; it is neglected in the numerical
calculations.
A spin in the hidden layer is described by the energy change:
(cid:40)
∆E(r ) =2β −m h σ (r )
h h h e h h
(cid:41)
(cid:88) (cid:88)
+m σ (r ) D (r −r′ )m σ (r′ )+m σ (r ) D(r −r′ )m σ (r′ )
h h h β h h h h h h h h h s s s s
h
r′
h
r‘
s
(cid:40) (cid:41)
(cid:88) (cid:88)
≈2β m σ (r ) D(r −r′ )m σ (r′ )+m σ (r ) D(r −r′ )m σ (r′ )
h h h h h h h h h h h h s h s s s
r′
h
rs
(21)
24
Here, the first term in Eqn. 21 is the effect of the external environment field h on the hidden
e
spins. The second term represents interactions between all spins in the hidden layer. This
external field is assumed to be applied locally to the sensor spins and is much weaker than
the hidden spin interaction terms and therefore neglected. The third term represents the
fields generated by sensory spins acting on the hidden layer spins.
An important aspect of this model is that because the effective temperatures for the sen-
sory and hidden layer spins are different, timescales for reversal dynamics are also different.
These are modelled such that the sensory spins relax with respect to the instantaneous value
of the external field, while the hidden spins relax in accord to the fields generated by the
sensory spins in their states. The condition T > T describes sensory spins that sample
h s
the local input field less frequently than the hidden spins sample the sensory spin configu-
ration. We find in the active inference experiments discussed above that the ratio of these
temperatures affects the sampling of ASI configuration states.
Acknowledgement
The authors thank M. Falconbridge for helpful and insightful discussions. This work was
supported from the University of Manitoba, the Natural Sciences and Engineering Research
Council of Canada (NSERC RGPIN 05011-18 and RGPIN-2018-05012), and the Canadian
Foundation for Innovation (CFI) John R. Evans Leaders Fund.
References
1. Friston, K.; Kilner, J.; Harrison, L. A free energy principle for the brain. Journal of
Physiology-Paris 2006, 100, 70–87.
2. Adams, R. A.; Aponte, E.; Marshall, L.; Friston, K. J. Active inference and oculomo-
25
tor pursuit: The dynamic causal modelling of eye movements. Journal of Neuroscience
Methods 2015, 242, 1–14.
3. Friston,K.; Trujillo-Barreto,N.; Daunizeau,J.DEM:Avariationaltreatmentofdynamic
systems. NeuroImage 2008, 41, 849–885.
4. Friston,K.; Stephan,K.; Li,B.; Daunizeau,J.GeneralisedFiltering.Mathematical Prob-
lems in Engineering 2010, 2010, 1–34.
5. Friston, K. The free-energy principle: a unified brain theory? Nat Rev Neurosci 2010,
11, 127–138.
6. Friston, K. The free-energy principle: a rough guide to the brain? Trends in Cognitive
Sciences 2009, 13, 293–301.
7. Aguilera, M.; Millidge, B.; Tschantz, A.; Buckley, C. L. How particular is the physics of
the free energy principle? Physics of Life Reviews 2022, 40, 24–50.
8. FitzGerald,T.H.B.; Schwartenbeck,P.; Moutoussis,M.; Dolan,R.J.; Friston,K.Active
Inference, Evidence Accumulation, and the Urn Task. Neural Computation 2015, 27,
306–328.
9. Nakajima, K. Physical reservoir computing—an introductory perspective. Japanese
Journal of Applied Physics 2020, 59, 060501.
10. Skjærvø, S. H.; Marrows, C. H.; Stamps, R. L.; Heyderman, L. J. Advances in artificial
spin ice. Nature Reviews Physics 2020, 2, 13–28.
11. Nisoli, C.; Moessner, R.; Schiffer, P. Colloquium: Artificial spin ice: Designing and
imaging magnetic frustration. Reviews of Modern Physics 2013, 85, 1473–1490.
12. Marrows, C. H. In Spin Ice; Udagawa, M., Jaubert, L., Eds.; Springer Series in Solid-
State Sciences; Springer International Publishing: Cham, 2021; pp 455–478.
26
13. Jensen, J. H.; Tufte, G. Reservoir Computing in Artificial Spin Ice. 2020; pp 376–383.
14. Hon, K.; Kuwabiraki, Y.; Goto, M.; Nakatani, R.; Suzuki, Y.; Nomura, H. Numerical
simulation of artificial spin ice for reservoir computing. Applied Physics Express 2021,
14, 033001.
15. Gartside, J. C.; Stenning, K. D.; Vanstone, A.; Holder, H. H.; Arroo, D. M.; Dion, T.;
Caravelli, F.; Kurebayashi, H.; Branford, W. R. Reconfigurable training and reservoir
computing in an artificial spin-vortex ice via spin-wave fingerprinting. Nature Nanotech-
nology 2022, 17, 460–469.
16. May, A.; Saccone, M.; van den Berg, A.; Askey, J.; Hunt, M.; Ladak, S. Magnetic charge
propagation upon a 3D artificial spin-ice. Nature Communications 2021, 12, 3217.
17. Saccone,M.; VandenBerg,A.; Harding,E.; Singh,S.; Giblin,S.R.; Flicker,F.; Ladak,S.
Exploring the phase diagram of 3D artificial spin-ice. Communications Physics 2023, 6,
1–9.
18. Hopfield, J. J. Neural networks and physical systems with emergent collective computa-
tional abilities. Proceedings of the National Academy of Sciences 1982, 79, 2554–2558,
Publisher: Proceedings of the National Academy of Sciences.
19. Coolen, A. C. C.; Kuehn, R.; Sollich, P. Theory of Neural Information Processing Sys-
tems; Oxford University Press, Oxford, 2005.
20. Begum Popy, R.; Frank, J.; Stamps, R. L. Magnetic field driven dynamics in twisted
bilayer artificial spin ice at superlattice angles. Journal of Applied Physics 2022, 132,
133902.
21. Castelnovo, C.; Moessner, R.; Sondhi, S. L. Magnetic monopoles in spin ice. Nature
2008, 451, 42–45.
27
22. Friston, K. Life as we know it. Journal of The Royal Society Interface 2013, 10,
20130475.
23. Kirchhoff, M.; Parr, T.; Palacios, E.; Friston, K.; Kiverstein, J. The Markov blankets of
life: autonomy, active inference and the free energy principle. J. R. Soc. Interface. 15,
20170792.
24. Budrikis, Z.; Politi, P.; Stamps, R. L. Diversity Enabling Equilibration: Disorder and
the Ground State in Artificial Spin Ice. Physical Review Letters 2011, 107, 217204,
Publisher: American Physical Society.
25. Budrikis, Z.; Morgan, J. P.; Akerman, J.; Stein, A.; Politi, P.; Langridge, S.; Mar-
rows, C. H.; Stamps, R. L. Disorder Strength and Field-Driven Ground State Domain
Formation in Artificial Spin Ice: Experiment, Simulation, and Theory. Physical Review
Letters 2012, 109, 037203.
26. Friston, K. J.; Stephan, K. E. Free-energy and the brain. Synthese 159, 417–458.
27. Buckley, C. L.; Kim, C. S.; McGregor, S.; Seth, A. K. The free energy principle for action
and perception: A mathematical review. Journal of Mathematical Psychology 81, 55–79.
28. Balaji, B. Continuous-Discrete Path Integral Filtering. Entropy 11, 402–430.
29. Baltieri, M. Active Inference: Building a New Bridge Between Control Theory and Em-
bodied Cognitive Science; University of Sussex, 2019.
30. Balaji,B.; Friston,K.Bayesianstateestimationusinggeneralizedcoordinates.p80501Y.
31. Friston, K.; Mattout, J.; Trujillo-Barreto, N.; Ashburner, J.; Penny, W. Variational free
energy and the Laplace approximation. NeuroImage 34, 220–234.
32. Baltieri, M. A Bayesian perspective on classical control. 2020 International Joint Con-
ference on Neural Networks (IJCNN). 2020; pp 1–8, ISSN: 2161-4407.
28
33. Adams, R. A.; Bauer, M.; Pinotsis, D.; Friston, K. J. Dynamic causal modelling of eye
movements during pursuit: Confirming precision-encoding in V1 using MEG. NeuroIm-
age 2016, 132, 175–189.
34. Isomura, T.; Kotani, K.; Jimbo, Y.; Friston, K. J. Experimental validation of the free-
energy principle with in vitro neural networks. Nat Commun 14, 4547.
35. Baños, R. A.; Cruz, A.; Fernandez, L. A.; Gil-Narvion, J. M.; Gordillo-Guerrero, A.;
Guidetti, M.; Maiorano, A.; Mantovani, F.; Marinari, E.; Martin-Mayor, V.; Monforte-
Garcia, J.; Sudupe, A. M.; Navarro, D.; Parisi, G.; Perez-Gaviro, S.; Ruiz-Lorenzo, J. J.;
Schifano, S. F.; Seoane, B.; Tarancon, A.; Tripiccione, R. et al. Nature of the spin-glass
phase at experimental length scales. J. Stat. Mech. 2010, 2010, P06026.
36. Koudahl, M. T.; de Vries, B. A Worked Example of Fokker-Planck-Based Active Infer-
ence. Active Inference. pp 28–34.
37. Friston, K.; Da Costa, L.; Sajid, N.; Heins, C.; Ueltzhöffer, K.; Pavliotis, G. A.; Parr, T.
The free energy principle made simpler but not too simple. Physics Reports 2023, 1024,
1–29.
38. Heins, C.; Klein, B.; Demekas, D.; Aguilera, M.; Buckley, C. L. Spin Glass Systems
as Collective Active Inference. Active Inference. Cham, 2023; pp 75–98.
39. Nickel, R.; Gibbs, J.; Burgess, J.; Shafer, P.; Motta Meira, D.; Sun, C.; van Lierop, J.
Nanoscalesizeeffectsonpush-pullFe-Ohybridizationthroughthemultiferroictransition
of perovskite ϵ-Fe O . Nano Letters 2023, 23, 7845.
2 3
40. Torrejon,J.; Riou,M.; Araujo,F.A.; Tsunegi,S.; Khalsa,G.; Querlioz,D.; Bortolotti,P.;
Cros, V.; Yakushiji, K.; Fukushima, A.; Kubota, H.; Yuasa, S.; Stiles, M. D.; Grollier, J.
Neuromorphic computing with nanoscale spintronic oscillators. Nature 2017, 547, 428–
431.
29
41. Grollier, J.; Querlioz, D.; Camsari, K. Y.; Everschor-Sitte, K.; Fukami, S.; Stiles, M. D.
Neuromorphic spintronics. Nature Electronics 2020, 3, 360–370.
42. Lee, T. S.; Mumford, D. Hierarchical Bayesian inference in the visual cortex. J. Opt.
Soc. Am. A 2003, 20, 1434.
43. Budrikis, Z. In Chapter Two - Disorder, Edge, and Field Protocol Effects in Athermal
Dynamics of Artificial Spin Ice; Camley, R. E., Stamps, R. L., Eds.; Solid State Physics;
Academic Press, 2014; Vol. 65; pp 109–236.
44. Desplat, L.; Suess, D.; Kim, J.-V.; Stamps, R. L. Thermal stability of metastable mag-
netic skyrmions: Entropic narrowing and significance of internal eigenmodes. Physical
Review B 2018, 98, 134407.
45. Desplat, L.; Kim, J.-V.; Stamps, R. L. Paths to annihilation of first- and second-order
(anti)skyrmions via (anti)meron nucleation on the frustrated square lattice. Physical
Review B 2019, 99, 174409.
46. Pinna, D.; Bourianoff, G.; Everschor-Sitte, K. Reservoir Computing with Random
Skyrmion Textures. Phys. Rev. Appl. 2020, 14, 054020.
47. Raab, K.; Brems, M. A.; Beneke, G.; Dohi, T.; Rothörl, J.; Kammerbauer, F.;
Mentink, J. H.; Kläui, M. Brownian reservoir computing realized using geometrically
confined skyrmion dynamics. Nat Commun 2022, 13, 6982.
30
TOC Graphic
31

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Active Inference Demonstrated with Artificial Spin Ice"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
