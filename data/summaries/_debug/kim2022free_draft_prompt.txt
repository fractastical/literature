=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Free energy and inference in living systems
Citation Key: kim2022free
Authors: Chang Sub Kim

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2022

Key Terms: systems, system, principle, energy, living, neural, brain, inference, nonequilibrium, free

=== FULL PAPER TEXT ===

2202
voN
32
]CN.oib-q[
2v49141.3022:viXra
Free energy and inference in living systems
Chang Sub Kim
Department of Physics, Chonnam National University, Gwangju 61186,Republic of
Korea
E-mail: cskim@jnu.ac.kr
Abstract. Organisms are nonequilibrium, stationary systems self-organized via
spontaneoussymmetrybreakingandundergoingmetaboliccycleswithbrokendetailed
balance in the environment. The thermodynamic free-energy principle describes
an organism’s homeostasis as the regulation of biochemical work constrained by
the physical free-energy cost. In contrast, recent research in neuroscience and
theoreticalbiologyexplainsahigherorganism’shomeostasisandallostasisasBayesian
inference facilitated by the informational free energy. As an integrated approach to
living systems, this study presents a free-energy minimization theory overarching the
essentialfeaturesofboththethermodynamicandneuroscientificfree-energyprinciples.
Our results reveal that the perception and action of animals result from active
inference entailed by free-energy minimization in the brain, and the brain operates
as Schr¨odinger’s machine conducting the neural mechanics of minimizing sensory
uncertainty. A parsimonious model suggests that the Bayesian brain develops the
optimal trajectories in neural manifolds and induces a dynamic bifurcation between
neural attractors in the process of active inference.
Keywords Living system, homeostasis and allostasis, Bayesian brain, free-energy
principle, Schr¨odinger’s machine, neural attractor
24 November 2022
Free energy and inference in living systems 2
1. Introduction
Although there is no standard definition of life [78, 50, 42, 31, 28, 27, 57], literature
often states that a living system tends to reduce its entropy, defying the second law of
thermodynamics to sustain its nonequilibrium (NEQ) existence. However, conforming
to the second law of thermodynamics, adjudication between the entropy reduction and
augmentation of an open system must depend on the direction of irreversible heat flux
at the system-reservoir interface. Organisms are open systems in the environment;
hence, they obey the second law by contributing to the total-entropy increase in the
universe. The above confusion, perhaps, is rooted in Erwin Schro¨dinger’s annotation,
which metaphorically explains living organisms as feeding on negative entropy [96].
In the same monograph, Schro¨dinger continues to explicate that a more appropriate
discussion for metabolism is to be addressed in terms of free energy (FE). He made
this clarification because, in contrast to the Clausius entropy to which he was referring,
thermodynamic FE always decreases during irreversible processes in any open system
[67]. Many studies have been based on Schro¨dinger’s insight into how biological systems
can be explained by physical laws and principles. We examine the definition of life in
terms of FE minimization.
Organisms maintain biologically essential properties, such as body temperature,
blood pressure, and glucose levels, which are distinct from ambient states. Living
systems continuously exchange heat and material fluxes with the environment by
performing metabolic work, which is subject to the energy balance described by
the first law of thermodynamics. The second law posits that the entropy of an
isolated macroscopic system increases monotonically with any spontaneous changes.
Organisms and the environment together constitute the biosphere, which is isolated
and macroscopic; thus, metabolic processes in organisms increase the total entropy.
The second law affects organisms by limiting metabolic efficiency. The thermodynamic
free-energyprinciple(TFEP)encompassesthermodynamiclawsandprovidesqualitative
and quantitative explanation of how living systems biophysically sustain homeostasis by
minimizing FEs. Recent studies have addressed the modern metabolism perspective as
energyregulationofmultisensoryintegrationacrossbothinteroceptiveandexteroceptive
processes [15, 85]. This explains metabolism not only at the level of individual
organisms, but also at the ecosystem and planetary levels [54, 43], and emphasizes the
energetics and power efficiency in brain performance [100, 6, 69]. In contrast, the ability
of organisms to undergo allostasis, which predictively regulates homeostasis [105, 97],
or, more generally, their autopoietic properties [72], are unable to be explained by
the TFEP. Allostatic ability is the main driver of adaptive fitness, the emergence of
which cannot be solely attributed to a (bio)physical self-organization from a myriad
of emergent possibilities in the primitive circuits of neuronal activities. Organisms
are under environmental constraints, and adaptive fitness, or natural selection, is
the consequence of survival optimization in specific environments during evolution.
Therefore, the FE minimization scheme requires a top-down or high-level computational
Free energy and inference in living systems 3
mechanism that facilitates hardwiring of the allostatic capability.
The brain-inspired free-energy principle (FEP) in neuroscience and theoretical
biology suggests a universal biological principle in an axiomatic manner, which
providestheinformationalFE-minimizationformalism, thataccountsfortheperception,
learning, and behavior of living systems [33, 34]. This principle has been also applied to
other cognitive systems, such as artificial intelligence and robots [11, 71, 92, 12, 74, 73,
23]; however, our study primarily focuses on living systems and implications of the FEP
in a biological context, emphasizing the embodied nature of inference [83]. According
to the informational free-energy principle (IFEP), all life forms are evolutionarily self-
organized to minimize surprisal, which is an information-theoretic measure of the
improbability or unexpectedness of theenvironmental niche of organisms. Informational
FE(IFE)isamathematicalconstruct, ratherthanaphysical (thermodynamic)quantity,
specified by the temperature, chemical potential, volume, etc. Informational FE
mathematically bounds the surprisal from above; accordingly, the IFEP suggests that
natural selection reflects minimization of IFE in an organism as a proxy for surprisal
at all biological time scales. The IFEP employs Helmholtz’s early idea of perception as
unconscious inference [111]: an organism’s brain possesses an internal model of sensory
generation and infers the external causes of sensory data by matching them with prior
knowledge. Theactive-inference frameworkfollowing fromtheIFEPencapsulates motor
controlandplanningbeyondHelmholtzianperceptionasanadditionalinferentialscheme
[32,1]. The brainpossesses theprobabilistic internal model whose parameters (sufficient
statistics) are encoded by brain variables in the NEQ stationary state; however, thus
far, no physical theory has been developed for determining NEQ probabilities in the
macroscopic brain. In practice, the IFEP assumes open forms, or some fixed forms,
for the NEQ densities and implements IFE minimization. The Gaussian fixed-form
assumption can be used to convert the IFE to a sum of discrepancies between the
predicted and actual signals [10], which is known as prediction error in predictive
coding theory [48]. Commonly, the transformed IFE objective is minimized by
employing the gradient-descent method widely used in machine learning [106]. The
resulting variational-filtering equations compute the Bayesian inversion of sensory data
byinferringtheexternal sources [5], knownasrecognitiondynamics (RD)[33]. Recently,
the IFEP was generalized in a manner that minimizes sensory uncertainty, which is a
long-term surprisal over a temporal horizon of an organism’s changing environmental
niche [59]. Despite being a promising universal biological principle, the IFEP has led
to controversy regarding its success as the universal principle and its distance between
biophysical reality and epistemological grounds [29, 62, 13, 87, 93, 63, 8, 2, 9, 86].
In this study, the two FE approaches are jointly considered to develop a unified
paradigm for living systems: the TFEP does not describe the brain’s ability to infer
and act in the environment, whereas the brain-inspired IFEP is mainly a purposive
(hypothesis-driven) framework lacking intimate connections to neuronal substrates and
physical laws. Our goal is to link the two FEPs and propose a biological FEP that
integrates the reductionistic base and top-down teleology in the brain. In addition,
Free energy and inference in living systems 4
we unveil the attractor dynamics that computes allostatic regulation, perception and
motor inference, in the brain, based on our proposed FE-minimization framework. A
similar approach was reported in [35], in which formalisms underwriting stochastic
thermodynamics and the IFEP were presented without addressing the direct link
between the thermodynamic and informational FE. In addition, a unified Bayesian and
thermodynamic view attempted to explain the brain’s learning and recognition as a
neuralengineandproposedthelawsofneurodynamics[102]. Wealsonoteanotherrecent
work that made the neural manifold models from a symmetry-breaking mechanism in
brain-network synergetics, commensurate with the maximum information principle [56].
In brain architecture, enormous degrees of freedom of neuronal activities pose the
classical negligence inahigh-dimensional problem; thus, theunderlying neuraldynamics
appears to be stochastic. However, we argue that perception, learning, and motor-
inference in the brain is low-dimensional at the functional level, obeying the law of large
numbers; accordingly, RD becomes deterministic, involving a limited number of latent
variables. For instance, a few joint angles suffice for the brain to infer arm movement
in motor control. In contrast, the emergence of deterministic RD is more subtle in
perception and learning, which demands a systematic coarse-graining of stochastic
neuronal dynamics. Our investigation facilitates the systematic derivation of Bayesian-
brainRD interms ofa few effective variables, which we term Bayesian mechanics (BM);
BM regulates the homeostasis and allostasis (that is, adaptive fitness) of living systems,
conforming with the proposed biological FEP.
The concept of coarse-graining, or effective description, is ubiquitous in
computational neurosciences [46, 4, 89, 24, 38, 109, 14]. Here, we review the
recent research relevant to our work, which motivated the development of BM. Many
previous studies of recorded neurons showed that population dynamics is confined to
a low-dimensional manifold in empirical neural space, where trajectories are neural
representations of the population activity [22]. In mathematical terms, the neural
modes were defined as eigen-fields that span the neural manifold. The latent variables,
or collective coordinates, were defined as projection of the population activity onto
the neural modes [40, 39]. Other theoretical models support the idea that long-
term dynamics in recurrent neural networks gives rise to the attractor manifold [75],
which is a continuous set of fixed points occupying a limited region of neural space.
Consequently, the attractor dynamics and switching between different attractors were
manifested [76], indicating a contextual change in neuronal representations [113, 55].
Moreover, the manifold hypothesis is widely applied in machine learning to approximate
high-dimensional data using a small number of parameters [17]. Experimental studies
showed that a dynamical collapse occurs in the brain from incoherent baseline activity
to low-dimensional coherent activity across neural nodes [103, 82, 110]. Synchronized
patterns emerged when the featured inputs and prediction derived from prior or stored
knowledge matched; in contrast, when there was a mismatch, the high-dimensional
multi-unit activity increased. This observation also provided empirical evidence that
neural signals reduce prediction errors, thereby minimizing the IFE.
Free energy and inference in living systems 5
Based on the results described above, we suggest that the latent dynamics can
be effectively described by a small number of coarse-grained variables in the reduced
dimension. In this study, we formulate the BM of inferential regulation of homeostasis
in living systems in terms of a few latent variables. The latent variables are determined
as the brain activities and their conjugate momenta that represent the external,
environmental and motor, states and online prediction errors, respectively. The sensory
error at the peripheral level acts as a time-dependent driving source in BM, providing
the neural mechanism for sensory, as well as motor, inferences. Our continuous-state
formulation in continuous time may be useful for studying situated-action problems in
which biological systems must make decisions even during ongoing sensorimotor activity
[16].
The remainder of this paper is organized as follows. In Section 2, we describe the
establishment of the TFEP from NEQ fluctuation theorems when applied to living
systems. Section 3 explains how stochastic dynamics at the neuronal level can be
modeled and how a statistical approach can be used to determine the NEQ densities
of neural states in the physical brain. In Section 4, we present the proposed biological
IFEP minimizing long-term surprisal and establish its continuous-state implementation
that yields BM in the neural phase space. Next, in Section 5, we numerically integrate
BM and manifest the attractor dynamics that performs perception and motor inference
in the brain. Finally, we summarize important outcomes of our investigation and the
conclusions inSection 6. InAppendix, wepresent thedual closed-loopcircuitry ofactive
inference resulting from our model.
2. Nonequilibrium fluctuation theorems applied to organisms
Fluctuation theorems (FTs) concisely describe stochastic NEQ processes in terms
of mathematical equalities [52, 20]. Although FTs were initially established for
small systems, where fluctuations are appreciable, they also apply to macroscopic
deterministicdynamics[58]. Here, wepresentFTsinanappropriatecontextofbiological
problems and propose that the FTs suggest a living organism is an NEQ system that
maintains the housekeeping temperature, T, (average 36.5 0C in humans) within its body
and employs metabolism isothermally to act against its environment.
To this end, among the various representations of FTs, we use the NEQ work
relation [52]:
e´βpW´∆Fq
1, (1)
x y “
where β k T, with k being the Boltzmann constant and T being the temperature as
B B
“
described below. The mathematical equality given in Eq. (1) is known as the Jarzynski
relation [98]. Here, W is the amount of experimental work performed on a small system
immersed in a thermal reservoir and ∆F is the induced change in the Helmholtz FE of
the system. Accordingly, W ∆F is the excess energy associated with each irreversible
´
work process in the system, which is unavailable for a useful conversion. The bracket,
Free energy and inference in living systems 6
, indicates the average over many work strokes, that is, work distribution subject to
x¨¨¨y
a protocol. The average must beconsidered because theexperimental work performance
on small systems fluctuates.
The Jarzynski relation can be converted to an expression for entropy as follows. By
applying e´βW e´βxWy to Eq. (1), which is known as the Jensen inequality [19], we
x y ě
obtaintheinequality∆F W . Thisinequalityisanalternativeexpressionthatcanbe
ď x y
used to apply the second law to isothermal irreversible processes of the system initially
prepared in equilibrium with a reservoir [58]. Using the inequality, one can consider
the change in the average total entropy: ∆S ∆S ∆S , where ∆S
tot sys R sys
x y “ x y ` x y
is the change in the system entropy and ∆S is the change in the reservoir entropy.
R
The average associated with ∆S , which is reversible by definition, can be further
R
manipulated to obtain ∆S Q T W ∆U T, where Q Q is
R sys R sys
x y “ ´x y{ “ px y ´ q{ “ ´
used in the first step, and then the thermodynamic first law is applied for Q ; U is
sys
x y
the internal energy of the system. Therefore, T ∆S W ∆U T ∆S
tot sys
x y “ x y´ p ´ x yq “
W ∆F, which leads to the stochastic second law for the combined small system and
x y´
reservoir:
∆S 0. (2)
tot
x y ě
The possibility of tightening the preceding inequality has been investigated among
researchers by revealing a nonzero, positive bound, leading to thermodynamic
uncertainty relations [47, 99]. The unavailable energy associated with individual work
processes amounts to the total entropy change, namely, β W ∆F
k´1
∆S under
p ´ q “ B tot
isothermal conditions. By applying the final identity to Eq. (1), the Jarzynski equality
is cast to the integral form of entropy fluctuation:
e´k B
´1∆Stot
1. (3)
x y “
In the biological context, W is the amount of environmental work involved in the
metabolismofalivingsystem, suchasthebiologicalreactionsofoxygenicphotosynthesis
and aerobic respiration [3, 107]. The biological work is not controllable and thus,
stochastic. The FTs describe the imbalance between energy intake and expenditure
in an organism while maintaining the housekeeping temperature. The Helmholtz FE
increment in the living system over a metabolic work cycle is limited by the average
environmental work done on the organism. The resulting inequality from the Jarzynski
relation can be written in the organism-centric form as
W ∆F, (4)
x y ď
where we set W W and ∆F ∆F, which now states that the work
x y “ ´x y ” ´
performance, W , of a biological system against the environment (e.g., via metabolism)
x y
is bounded from above by the thermodynamic FE cost, ∆F. The preceding inequality
reflects the limited efficiency of metabolic work in living systems. Rare individual
processes that violate Eq. (4) may occur in small systems; however, such statistical
deflection is not expected in a finite biological system with macroscopic degrees of
freedom. The equality in Eq. (4) holds for reversible work cycles in inanimate matter,
Free energy and inference in living systems 7
attaining thermodynamic efficiency at its maximum, but not in the metabolic processes
of living organisms, which are irreversible. Our consideration of metabolic work may be
generalized to the multi-level autocatalytic cycles suggested as the chemical origins of
life [68].
Note here that we considered the temperature appearing in the Jarzynski relation
as the body temperature of a specific biological system, unlike the usual implication
of FTs; in the standard derivation of the Jarzynski relation [53], the temperature, T,
appearing in the NEQ equality is, by construction, the reservoir temperature. The FT is
generally intended for an irreversible process during which the system temperature may
not be defined. However, the initial and end states must be in equilibrium so that the
FE is meaningful. The subtlety lies in the fact that the end-state temperature may or
may not bethe same as thereservoir temperature for experiments performedin isolation
after the initial equilibrium preparation. Living organisms are in an NEQ stationary
state, maintaining a housekeeping temperature, T, that is distinct from the ambient
temperature, to which they equilibrate only when ceasing to exist. Thus, organisms are
viewed as isothermal systems, which are open to heat and particle exchange with the
environment.
The NEQ work relation expresses the second law of thermodynamics as the
mathematical equality in Eq. (1). The second law, in its biological context, renders the
thermodynamic constraint on living organisms given by the inequality in Eq. (4), which
reveals the inevitable (thermodynamic) FE waste produced during metabolic cycles.
However, this inequality accounts for neither self-adaptiveness nor brain functions,
such as perception, learning, and behavior. To address these essential features of life,
researchers currently employ a hybridizing scheme, which first proposes how the system-
level biological functions operate and then attempts to make connections to biophysical
substances. Particularly, the Bayesian mechanism built into the IFEP provides a crucial
component in this promising hybrid explanation of life, which is described in detail in
Section 4.
3. Statistical-physical description of the nonequilibrium brain
The brain is comprised of a myriad of complex neurons; accordingly, its internal
dynamics at the mesoscopic level must obey some stochastic equations of motion on
account of classical indeterminacy. The relevant coarse-grained neural variables are
local-scale population activities, or intra-area brain rhythms. In the following, we
consider that the brain matter itself constitutes the thermal environment at body
temperature for the mesoscopic neural dynamics.
Below, we assume that the neural activity, µ, at the coarse-grained population level
obeys the stochastic dynamics [64]:
dµ
f µ;t w t , (5)
dt “ p q` p q
where the inertial term in the Langevin equation was dropped by taking the over-
Free energy and inference in living systems 8
damping limit. Here, f may represent both conservative and time-dependent metabolic
forces, andw represents randomfluctuationcharacterized asa delta-correlatedGaussian
noise satisfying the following conditions:
w t 0 and w t w t1 Iδ t t1 ,
x p qy “ x p q p qy “ p ´ q
where I is the noise strength. In one dimension (1D), for simplicity, the environmental
perturbation and noise strength are physically specified, respectively, as [88]
1 k T
B
f A and I 2 ,
“ mγ “ mγ
where A is a conservative force acting on a neural unit with mass, m, neglecting time-
dependent driving, T is the body temperature, and γ is the phenomenological frictional
coefficient whose inverse corresponds to momentum relaxation time. The solutions to
Eq. (5) describe the individual trajectories of random dynamical processes.
In general, colored noises can be considered beyond the delta-correlated white noise
by generalizing Eq. (5) to incorporate the non-Markovian memory effect:
t
m dt1γ t t1 µ9 t1 A µ ζ.
p ´ q p q “ p q`
ż´8
To ensure equilibrium at temperature T, the colored Langevin equation must satisfy
the fluctuation-dissipation theorem that accounts for the nonsingular noise correlation
[114]:
ζ t ζ t1 2k Tγ t t1 .
B
x p q p qy “ p| ´ |q
A standard example of such colored noise is the Orstein-Uhlenbeck memory kernel given
by γ t t1 γτ´1 exp t t1 τ , where τ is the noise autocorrelation time.
p| ´ |q “ p´| ´ |{ q
As an alternative to the Langevin equation [Eq. (5)], one may collectively consider
an ensemble of identical systems displaying various values of the state, µ, and ask
how the statistical distribution changes over time. After normalization, the ensemble
distribution is reduced to the probability density, say p µ,t , so that p µ,t dµ specifies
p q p q
the probability that an individual Brownian particle is found in the range µ,µ dµ
p ` q
at time t. In the Markovian approximation, the change in the probability density is
determined by the probability density at the current time, which is generally described
by the master equation given in the continuous-state formulation as
p µ,t
B p q w µ,µ1 p µ1,t w µ1,µ p µ,t dµ1, (6)
t “ p q p q´ p q p q
B ż ! )
where w µ1,µ is the transition rate of the state change from µ to another µ1. We
p q
further assume that the transition occurs between two infinitesimally close states, µ and
µ1, where µ1 µ x 1, so that the transition rate sharply peaks at around x 0 to
´ “ ! “
approximate the value as w µ1,µ w µ;x . Then, p µ1 can be expanded about µ to
p q « p q p q
thesecond-order inxandall higher-ordertermsareneglected. Consequently, themaster
equation can be converted into the Smoluchowski-Fokker-Planck (S-F-P) equation [88]:
p µ,t
B p q B D 1 µ B D 2 µ p µ,t , (7)
t “ µ ´ p q` µ p q p q
B B ! B )
Free energy and inference in living systems 9
where, D 1 and D 2 correspond to the first two expansion coefficients in the Kramers-
Moyal formalism, which are determined in the present case to be
1
D 1 f and D 2 I.
“ “ 2
The S-F-P equation can be expressed in three dimensions (3D) as
p ~µ,t
B p q ∇ f ~ ~µ D∇ p ~µ,t 0, (8)
t ` ¨ p q´ p q “
B ! )
where ∇ is the gradient operation with respect to the three-dimensional state, µ~. In
~
Eq. (8), the drift term, pf, accounts for conservative potential forces. In addition,
the diffusion term, D 2, is denoted as D, assuming spatial isotropy, for simplicity and
notational convention.
The S-F-P equation describes local conservation of the probability, p µ~,t , in the
p q
statespacespannedbythestatevector, ~µ, whichcarriestheprobabilityflux, ~j, identified
as
~j ~µ,t p ~µ,t f ~ ~µ D∇p ~µ,t .
p q “ p q p q´ p q
In steady state (SS), p t 0, where p p µ, ; accordingly, the divergence of
st st
B {B “ ” p 8q
the SS flux, ~j ~j µ, , must vanish in the S-F-P equation:
st
” p 8q
∇ ~j 0. (9)
st
¨ “
If Brownian particles undergo motion in an isolated or infinite medium,
~j
should
st
disappear on the local boundary because the total flux through the surface must vanish
to ensure probability conservation. Because the flux must be continuous over the entire
;
space, the SS condition in Eq. (9) imposes ~j 0 everywhere, reflecting the detailed
st
”
balance between the drift flux and dissipative flux. In this case, the system holds in
equilibrium, where life ceases to exist. The equilibrium probability can be obtained
from the condition j 0, giving canonical Boltzmann probability as the result:
st
“
p µ exp βV µ ,
eq
p q9 t´ p qu
where β 1 k T and V µ is potential energy. The kinetic-energy term does not
B
“ { p q
appear in p because the Langevin dynamics we consider are in the over-damping limit.
eq
However, for a finite open system, such as a living organism, the system’s SS flux
does not necessarily vanish on the local boundary; instead, it must be compensated
by the environmental afferent or efferent fluxes to achieve steady state. Thus, for a
living system, the detailed balance is not satisfied in the steady state [41, 70]; that is,
~j
0. Instead, the vanishing condition of the divergence of the probability flux entails
st
‰
a necessary balance. The mathematical expression in Eq. (9) admits a non-vanishing
vector field B~ ~µ via
p q
~j ~µ ∇ B~ ~µ , (10)
st
p q ” ˆ p q
; In an isolated or infinite medium, the net flux through the entire surface must vanish to ensure
probability conservation, i.e.,~j ” ~j ¨d~a“0, where d~a is the outward, infinitesimal area element.
net st
Accordingly,~j “0 at every point on the surface.
st ű
Free energy and inference in living systems 10
which shows that the SS flux is divergenceless or, equivalently, solenoidal [112, 84, 94].
The life flux,
~j
, defined in this manner is unchanged when
B~
is transformed to
st
B~1 B~ ∇Λ, where Λ is a scalar function of the state, ~µ.§ From Eq. (10), the
“ `
following generalized balance condition must hold locally on the boundary:
p ~µ f ~ ~µ D∇p ~µ ∇ B ~ ~µ . (11)
st st st
p q p q “ p q` ˆ p q
The above modified detailed-balance condition supports the frequent interpretation of
the force field, f ~ , as the gradient flow of the SS probability, p [31, 80]:
st st
f ~ ~µ D Q ∇lnp ~µ , (12)
st st
p q “ p ´ q p q
where we introduced the isotropic coefficient, Q, via
Q∇lnp p´1 ∇ B~ ;
st ” ´ st ˆ
for simplicity, the coefficient Q is assumed to be isotropic as it was for the diffusion
constant, D. The gradient flow is driven by entropy because the most likely equilibrium
state of the combined system and environment is achieved by maximizing the total
entropy; hence, it is an entropic force, conforming to the second law. Note that Eq. (10)
mimics the Ampere law in magnetism [44]; the effective field
B~
may be construed as
an induced field by the static current, ~j . Accordingly, the vector field, B~ µ~ , can be
st
p q
determined by means of
1 ~µ ~µ1
B~ ~µ ~j ~µ1 p ´ qdµ~1. (13)
p q “ 4π st p qˆ ~µ ~µ1 3
ż | ´ |
Note that the modified detailed-balance condition given in Eq. (11) is only a formal
description for determining the NEQ density, p , given SS flux, ~j , or, equivalently,
st st
the environmental magnetic field, B~ , in Eq. (13). Precise determination of p
st
is an independent research subject, which may be non-Gaussian with a colored
autocorrelation.
In general, it is difficult to obtain an analytic expression for the NEQ probability
density for open systems, except in low-density and/or linear-response regimes [61, 30].
Because of morphological complexity, it is practically intractable to derive the NEQ
densities specifying the physical brain states. Accordingly, the neural states under
continual sensory perturbation are assumed to be statistically described by time-
dependent Gaussian densities, predicted from Gaussian random noises imposed on the
Langevin description.
4. Latent dynamics of sensorimotor inference in the brain
Here, we present the BM for conducting Bayesian inversion of sensory observation in
the brain under the proposed generalized IFEP. This idea was previously developed by
considering passive perception [59] and only implicitly including active inference [60].
§ The freedom to choose the vector field, B~, without affecting the physical quantity,~j , is known as
st
gauge symmetry. Recently,researchersattemptedtodeterminethe implicationandutility ofthe gauge
transformation in neuronal dynamics in the brain and emergent functions [101, 91].
Free energy and inference in living systems 11
Here, we advance this formalism by explicitly introducing motor inference and planning
in the generative models to fully conform to the active-inference framework.
The environmental states, ϑ, generate sensory stimuli, ϕ, at the organism’s
receptors through mechanical, optical, or chemical perturbations, which are transduced
in the brain’s functional hierarchy in the form of a nervous signal. The sensory
perturbations may be altered by the organism’s motor manipulation, and we designate
u to denote the motor variables responsible for such control over the effectors. A crucial
point here is that the brain has access only to the sensory data and not their causes;
accordingly, from the brain’s perspective, both the environmental states, ϑ, and motor
variables, u, are external, that is, hidden. In terms of these relevant variables, we define
the variational IFE functional, denoted as F:
q ϑ,u
F q ϑ,u ,p ϕ;ϑ,u dϑ du q ϑ,u ln p q , (14)
r p q p qs” p q p ϕ;ϑ,u
ż ż p q
where q ϑ,u and p ϕ;ϑ,u are the recognition density (R-density) and generative
p q p q
density (G-density), respectively. The R-density is the brain’s online estimate
of posterior beliefs about the external causes of the sensory perturbation (it
probabilistically represents the environmental states). The G-density encapsulates
the brain’s likelihood in beliefs about sensory-data generation and prior beliefs about
the hidden environmental as well as motor dynamics (it probabilistically specifies
the internal model of sensory-data generation, environmental dynamics, and motor
feedback). Note that whereas the R-density is the current estimate, the G-density
contains the stored information in the brain, which can be updated by learning. In this
study, we generalize the R-density as a bi-modal probability of ϑ and u, and G-density
as a tri-modal probability of ϑ, u, and ϕ. Note that a semicolon is used between the
sensory perturbation, ϕ, and hidden variables ϑ and u in the G-density rather than
a comma to emphasize their differential role in perception. The explicit inclusion of
the motor variable, u, in the q and p densities is a key advancement over the standard
definition of IFE [10].
Now, using the product rule, p ϕ;ϑ,u p ϑ,u ϕ p ϕ for the G-density in
p q “ p | q p q
Eq. (14), we decompose the IFE to a form applicable in the biological context:
F q ϑ,u ,p ϕ;ϑ,u D q ϑ,u p ϑ,u ϕ lnp ϕ ,
KL
r p q p qs“ p p q} p | qq´ p q
where D is the Kullback-Leibler divergence [19]. Because D is non-negative, the
KL KL
following inequality holds, which underpins the IFEP described in Section 1:
lnp ϕ F q ϑ,u ,p ϕ;ϑ,u , (15)
´ p q ď r p q p qs
where lnp ϕ is the information-theoretic surprisal. Here, it is important to notice
´ p q
the resemblance between the preceding inequality and that given in Eq. (4) from the
TFEP.
Under the IFEP, the organism’s cognitive goal is to infer the hidden environmental
causes of sensory inputs with feedback from the motor-behavior inference. This goal is
achieved by minimizing F with respect to the R-density, q ϑ,u , which corresponds to
p q
Free energy and inference in living systems 12
the online adaptation of the sensory and motor modules in the brain. For instance, in
the classic reflex arc, the proprioceptive stimulus evokes the activity of sensory neurons
in the dorsal root, and the motor variable is engaged by the effector’s active states
of the motor neurons in the ventral root. The double procedures are involved in the
minimization scheme to cope with the bi-modal cognitive nature of sensory and motor
inferences: 1) the internal model is updated to better predict the sensory perturbation,
and 2) the sensory perturbation is modified by the agent’s motor engagement to further
reduce theresidual discrepancy with theinternal model. Theformer istermed aspassive
perception and the latter as active perception. However, the two inferential mechanisms
do not separately engage, but act as a whole in the sensorimotor closed loop in the
embodied agent, and are therefore jointly termed as active inference under the IFEP
[32, 1].
To draw a connection between the IFE minimization and neuronal correlates, it
is practically convenient to use the fixed form for the unknown R-density [10], whose
sufficient statistics are assumed to be encoded neurophysiologically by brain variables,
that is, neuronal activities. Here, we write the R-density as q ϑ,u q ϑ q u by
p q “ p q p q
considering the external variables ϑ and u as conditionally independent. Furthermore,
it is assumed that the factorizing densities, q ϑ and q u , are Gaussian; the means of
p q p q
the environmental states, ϑ, and motor states, u, are encoded by the neuronal variables
µ and a, respectively. Then, by performing technical approximations similar to those
used in [10], we convert the IFE functional, F, of the R- and G- densities to the IFE
function, F, of the neural representations µ and a, given sensory data, s. The sensory
dataorinputsareaneuralrepresentationoftheevokedperturbation, ϕ,atthereceptors,
detectedbytheorganism’sbrain. Here, thehomunculushypothesis, thebrainasaneural
observer, is implicit, which assumes teleological homology between the environmental
processes and brain’s internal dynamics.
The result for the IFE function, up to an additive constant, is given as
F µ,a;s lnp s;µ,a ; (16)
p q “ ´ p q
here, the dependence on the second-order sufficient statistics, namely (co)variances
of the R-density, was optimally removed. Consequently, the brain must only update
the means in the R-density in conducting the latent RD. The mathematical procedure
involved in Eq. (16) extends the Laplace approximation delineated in the review [10].
To complete the Laplace-encoded IFE, one must specify the inferential structure in
the encoded G-density, p s;µ,a . We facilitate probabilistic implementation of the
p q
generative model using the product rule:
p s;µ,a p s µ,a p µ,a , (17)
p q “ p | q p q
where the likelihood density, p s µ,a , is the brain’s concurrent estimation of the
p | q
encoded sensory data, s, from the neuronal response, µ, and motor manipulation, a.
Assuming conditional independence between µ and a, the joint prior p µ,a can be
p q
further factorized as
p µ,a p µ p a ,
p q “ p q p q
Free energy and inference in living systems 13
where p µ and p a are the brain’s prior beliefs regarding the environmental-state
p q p q
changes and motor dynamics, respectively. Thus, the Laplace-encoded IFE has been
specified solelyinterms oftheneural variablesµanda, which issuitable forbiologically-
plausible implementation of active inference in the physical brain.
Sensory states, s, evoked by exogenous stimuli, neurophysically activate the
neuronal population in the brain. The population dynamics is complex and high-
dimensional; however, the RD of the perceptual and behavioral inferences may be well-
described in lower-dimensional neural manifolds. Below, we formulate the generative
equations of latent neural modes considering classical indeterminacy. First, we assume
that sensory data, s, encoded at the receptors are measured by a neural observer
according to instant mapping:
s g µ,a;θ z, (18)
g
“ p q`
where g is the generative model of the sensory data and z is the observation noise. The
generative map, g µ,a , encapsulates both the perceptual states, µ, and motor states, a,
p q
which conjointly predict the sensory data, s. We consider the sensory generative model
as a continuous process of sensory prediction by µ and error prediction by a via the
effector alteration:
s g µ g a s g µ,a ,
1 2
r ´ p qs´ p q ” ´ p q
where we set g µ,a g 1 µ g 2 a . Second, we assume that the neural activity, µ,
p q “ p q ` p q
obeys internal dynamics as described in Section 3:
dµ
f µ;θ w, (19)
f
dt “ p q`
where f is the generative model of the neuronal change, and w is the involved random
noise. Third, we assume that the motor state, a, bears the motor-neural dynamics:
da
π a;θ η, (20)
π
dt “ p q`
where π is the generative model of the motor-neuronal change and η is the noise in
the process. The generative function, π, in Eq. (20) functions as the policy in machine
learning [106]: the policy, π a;θ , encapsulates the internal model of motor planning
π
p q
in continuous time. The dependence of the generative models on the parameters
θ , θ ,and θ enables incorporation of a longer-term neural efficacy, such as synaptic
g f π
plasticity; below, we omit the parameter dependence for notational simplicity. For the
neuronal generative equations, the continuous Hodgkin-Huxley model [59] or a more
biophysically realistic model can be employed; however, our simple model in Section 5
suffices to unveil the emergence of BM.
Noises in the neural generative models [Eqs. (18)-(20)] indicate stochastic
mismatches between the cognitive objectives on the left-hand side (LHS) and their
prediction through the generative functions/map. Accordingly, we consider that z, w,
and η neurophysically encode the probabilistic generative models p s µ,a , p µ , and
p | q p q
p a , respectively, [Eq. (17)]intheneuronal dynamics. Furthermore, weassume thatthe
p q
Free energy and inference in living systems 14
random noises are continuously distributed according to the normalized NEQ Gaussian.
Therefore, the Laplace-encoded likelihood, p s µ,a , and prior densities, p µ and p a ,
p | q p q p q
in Eq. (17) can be assumed to take the following forms:
p s µ,a N s g;0,σ ,
z
p | q “ p ´ q
p µ N µ9 f;0,σ , (21)
w
p q “ p ´ q
p a N a9 π;0,σ ;
η
p q “ p ´ q
here, N x h;0,σ exp 1 x h 2 ?2πσ denotes a Gaussian density of stochastic
p ´ q ” t´
2σp
´ q u{
variablex h withvarianceσ aboutthe zero mean , andx9 denotes thetimederivative of
´ }
x, that is, dx dt. The generative likelihood and prior densities in Eq. (21) are thought
{
to be stationary solutions to the S-F-P equation or a more general non-Markovian
extension, the biophysical derivation of which is beyond the scope of this work. Instead,
we assume the time-dependent Gaussian probabilities effectively at zero temperature
as physically admissible densities encoding internal models in the brain. Removing the
assumption by deriving physical probability densities is a key theoretical demand in
future studies.
Next, by substituting the expressions in Eq. (21) into Eq. (16) using the
decompositions in Eq. (17), we obtain an explicit expression for the IFE function at
an instant t:
1 1 1
F µ,a;s s g µ,a 2 µ9 f µ 2 a9 π a 2 , (22)
p q “ 2σ p ´ p qq ` 2σ p ´ p qq ` 2σ p ´ p qq
z w η
1
where we dismissed the term ln σ σ σ [59]. Our specific construct of the IFE
2 z w η
t u
encapsulates motor planning explicitly in continuous time via the policy, π a , in the
p q
generative models. Based on the Laplace-encoded IFE, the mathematical statement for
the biological FEP is given as
dt lnp s dtF µ,a;s , (23)
t´ p qu ď p q
ż ż
where the LHS is equivalent to the Shannon uncertainty, ds lnp s p s , under the
t´ p qu p q
ergodic assumption, which is assured by the NEQ stationarity of living systems. The
ş
inequality[Eq.(23)]showsthattheupperboundofsensoryuncertaintycanbeestimated
by minimizing the time integral of F over a temporal horizon. Accordingly, if we regard
the integrand F as a Lagrangian, the systematic framework of the Hamilton principle
can be employed to implement the minimization scheme [66]. Next, we cast Eq. (22) to
a weighted summation of the quadratic terms: F 1 m ε2 i w,z,a , which can
“ 2 i i i p “ q
be expressed as a total time derivative that does not affect the resulting BM [66]. In
ř
the summation, we defined the notations ε as
i
ε µ9 f µ ,
w
” ´ p q
ε a9 π a , (24)
η
” ´ p q
ε s g µ,a ,
z
” ´ p q
} Here, we use σ, notσ2 , to denote the variancesonly to be consistentwith the notations inan earlier
publication [10].
Free energy and inference in living systems 15
which represent the prediction errors involved in state, motor, and sensory inferences,
respectively. Additionally, the weight factors, m , m , and m , are defined through the
w η z
variances as
m 1 σ , m 1 σ , and m 1 σ , (25)
w w η η z z
” { ” { ” {
where m may be considered as a metaphor for the neural inertial masses. The neural
i
masses correspond to the predictive precisions in the standard terminology [10]; heavier
neural masses lead to more precise predictions. The IFE F as a Lagrangian, conforming
to classical dynamics, can be considered as a function of the instant trajectories of µ t
p q
and a t , subject to the time-dependent force, s s t .
p q “ p q
To exercise the Hamilton principle, we define the classical Action, S, as the time
integral of arbitrary trajectories µ t and a t in the configurational state space:
p q p q
t
S µ t ,a t ;t dt1F µ t1 ,a t1 ;s t1 , (26)
r p q p q qs “ p p q p q p qq
żt0
where t 0 is the initial time, and τ t t 0 is the temporal horizon of the relevant
” ´
biological process. The initial time can be chosen either in the past, that is, t 0 ,
Ñ ´8
or at present, that is, t 0 0. In the former, t is the present time, whereas in the
“
latter, t is the future time. Hence, active inference of the living systems mathematically
corresponds to varying S, subject to the sensory stream, to find an optimal trajectory
in the configurational state space spanned by µ and a.
Further, it is advantageous to consider the brain’s RD in phase space rather than
configurational space; the phase space is spanned by positions and momenta. This is
becausethemomentumvariablesaremeaningful prediction errors inthebrain’smessage
passing algorithms; they are defined via the informational Lagrangian, F, as
F
p B m µ9 f , (27)
µ
”
µ9
“
w
p ´ q
B
F
p B m a9 π , (28)
a
”
a9
“
η
p ´ q
B
where p and p are the momentum conjugates corresponding to µ and a, respectively.
µ a
Equation (24) reveals that the momenta, p and p , are indeed the prediction errors,
µ a
ε and ε , weighted by the neural masses, m and m , respectively. The purposive
µ η w z
Hamiltonian, H, can be obtained by performing the Legendre transformation H
”
p µ9 p a9 F. After straightforward manipulation, we obtain the Hamiltonian function:
µ a
` ´
1 1 1
2 2 2
H µ,a,p ,p ;s p p p f µ p π a m ε , (29)
p µ a q “ 2m µ ` 2m a ` µ p q` a p q´ 2 z z
w η
which is a generator of time evolution in neural phase space. The function H is specified
in the cognitive phase space spanned by the four-component column vector, Ψ, in the
present single-column formulation, whose components are defined as
ΨT Ψ1 ,Ψ2 ,Ψ3 ,Ψ4 µ,a,p
µ
,p
a
,
“ p q ” p q
Free energy and inference in living systems 16
where ΨT is the transpose of Ψ. Having determined the Hamiltonian, the Bayesian
mechanical equations of motion (termed as BM) can be abstractly written in the
symplectic representation as
9 H
Ψ J B , (30)
i ij
“ ´ Ψ
j
B
where the block matrix J is defined as
0 1 1 0
J ´ , where 1 .
” 1 0 “ 0 1
˜ ¸ ˜ ¸
Specifically, we unpack Eq. (30) and explicitly display the outcome:
1
µ9 p f µ , (31)
µ
“ m ` p q
w
1
a9 p π a , (32)
a
“ m ` p q
η
f g
p9 p B m s g B , (33)
µ µ z
“ ´ µ ´ p ´ q µ
B B
π g
p9 p B m s g B , (34)
a a z
“ ´ a ´ p ´ q a
B B
which are a coupled set of differential equations that are nonlinear, in general.
Thepreceding Eqs. (31)–(34)comprisetheBMofthebrainvariables, which execute
the RD of the Bayesian perception and motor inference in the brain. The BM was
attained by applying the Hamilton principle, for which we adopted the Laplace-encoded
IFE as an informational Lagrangian and derived the Hamiltonian to generate the
equations of motion. Our latent variables are the neural representations (µ,a) and
their conjugate momenta (p ,p ); they span the reduced-dimensional neural manifold.
µ a
The momenta represent the prediction errors neurophysiologically encoded by the error
units in the neuronal population. Below, we describe two significant features of the
latent dynamics, governed by the BM, subjected to the time-varying sensory input,
s t .
p q
(i) Equations (31)–(34) suggest that the brain mechanistically executes the cognitive
operation, which reflects Schro¨dinger’s suggestion of an organism as a mechanical
work [96]. Our derived BM addresses the continuous-state implementation of IFE
minimizationincontinuous time, which contrastscommondiscrete-time approaches
[37, 18, 90, 104]. We considered that biological phenomena are naturallycontinuous
and, thus, continuous representations better suit perception and behavior.
(ii) BM in symplectic form [Eq. (30)] represents the gradient-descent (GD) on
the Hamiltonian function. However, under nonstationary sensory inputs, the
multi-dimensional energy landscape is not static, but incurs time dependence.
Accordingly, thepresented BMnaturallyfacilitatesfastdynamicsbeyondthequasi-
static limit implied by the usual GD methods. In addition, it does not invoke the
concept of higher-order motions in the conventional framework [36]; accordingly,
our theory is not limited by the issue of average flows vs the rate of change of the
average [2].
Free energy and inference in living systems 17
5. Numerical study of BM
In this section, we numerically develop the latent dynamics of the brain’s sensorimotor
system resulting from the Hamilton principle-based FE minimization formulation.
For simplicity, we consider a homogeneous, but time-dependent, sensory input, such
as nonstationary light intensity or temperature, at the receptors, which emits a
motor output innervating the effectors that alter the sensory observation. There
are approximately 150,000 cortical columns in the mammalian neocortex, and each
cortical column exhibiting a six-laminae structure may be considered as an independent
sensorimotor system [77, 45]. Our simple model features the double closed-loop
microcircuitry delineated in Fig 1 within a single column, which constitutes the basic
computational unit of canonical cortical circuits in an actual large-scale brain network
[7].
Thegenerativemap, g, andfunctions, f andπ, areunknown; theymaybenonlinear
or even undescribable within ordinary mathematics. Here, we exploit the linear models
assuming the generic structure:
g µ,a;θ
θp0q θp1qµ θp2qa,
(35)
p g q “ g ` g ` g
f µ;θ
θp0q θp1qµ,
(36)
p f q “ f ` f
π a;θ
θp0q θp2qa,
(37)
p π q “ π ` π
where
θpiq
(α f,g,π) are the parameters that are to be learned and encoded as long-
α
term plasticity
“
in the neural circuits. We have included the term
θp2qa
in Eq. (35), which
g
facilitates the additive motor-inference mechanism of the sensory data; additionally,
θp1q
g
and
θp2q
magnify or demagnify sensory prediction and motor emission by the internal
g
state, µ, and motor state, a, respectively;
θp0q
denotes the background error in the
g
measurements. The constant terms
θp0q
and
θp0q
in Eqs. (36) and (37) specify the prior
f π
beliefs on the state and motor expectations, respectively; the coefficients
θp1q
and
θp2q
f π
modulate the relaxation times to the targets. In addition to these seven parameters
θpiq
, there appear three neural masses, m , in the BM unpacked in Eqs. (31)–(34).
α α
Hence, the proposed parsimonious BM still encloses 10 parameters, which define a
multidimensional parameter spaceto exploreforlearning. The learning problemwasnot
pursued in this study but should be explored in future investigations. Here, we focus on
theactiveinferenceproblem, assuming thattheoptimalparameterswerealreadylearned
or amortized over the developmental and evolutionary time scales; these parameters are
assumed to be shared for generating present and future sensory data.
Bysubstituting thegenerative functionsgiven inEqs. (35)–(37) into Eqs. (31)–(33),
the BM of the state vector, Ψ, can be concisely expressed as
9
Ψ RΨ I, (38)
` “
Free energy and inference in living systems 18
Figure 1. Spontaneous attractor: For illustrationalpurposes,we depict the attractor
in the 3D state space spanned by pRerµs,Reras,Rerp sq with instantaneous other
µ
variables; the attractor center, Ψ , is positioned at p´10,10,´20q. The full attractor
c
evolves in the hyper space spanned by the eight components of complex vector, Ψ; in
our model, there are the four types of neuronal units pµ,a,p ,p q in a single cortical-
µ a
column, each of which is allowed to be a complex variable. [Data are in arbitrary
units.]
where the relaxation matrix, R, is
θp1q
0
m´1
0
´ f ´ ω
0
θp2q
0
m´1
R ¨ ´ π ´ η ˛ (39)
“ m
θp1qθp1q
m
θp1qθp2q θp1q
0
˚
˚
´
m
z
θ
g p1qθ g p2q ´
m
z
θ
g p2qθ g p2q f
0
θp2q ‹
‹
˚ ´ z g g ´ z g g π ‹
˝ ‚
and the source term, I, on the right-hand side (RHS) is
θp0q
f
θp0q
I t ¨ π ˛. (40)
p q “ m
θp1qs
t m
θp0qθp1q
z g z g g
˚
˚
´
m
θp2qs p
t
q`
m
θp0qθp2q ‹
‹
˚ ´ z g p q` z g g ‹
˝ ‚
Note that the time-dependence in the source term I occurs through the sensory inputs,
s. The general solution for Eq. (38) can be formally expressed by direct integration as
t
Ψ t e´RtΨ 0 dt1e´Rt1 I t t1 . (41)
p q “ p q` p ´ q
0
ż
The first term on the RHS of Eq. (41) describes the homogeneous solution for an initial
condition of Ψ 0 , and the second term is the inhomogeneous solution driven by the
p q
source, I t , manifesting the history-dependent feature. The solution represents the
p q
brain’s cognitive trajectory in action while continuously perceiving the sensory inputs,
s t .
p q
In the long-time limit, t , we mathematically predict that the trajectory in the
Ñ 8 9
statemanifoldwillfallontoeitherafixedpoint, spiralnodeorrepeller, satisfyingΨ 0
st
9 “
or a limit cycle about a center satisfying Ψ iωΨ , where ω is an angular frequency
st st
“ ´
Free energy and inference in living systems 19
characterizing stationarity¶. Thedetailsofthesolution’sapproachtoasteady-statewill
be determined from the eigenvalue spectrum of the matrix R and time-varying feature
of s t . We denote the eigenvalues and eigenvectors by λ iω and φ, respectively, and
p q p” q
set up the eigenvalue problem:
Rφ λ φ .
α α α
“
The trace and determinant are invariant under a similarity transformation; accordingly,
the ensuing eigenvalues must satisfy:
λ tr R 0, (42)
α
“ p q “
α
ÿ
λ det R
α
“ p q
α
ź m m
θp1qθp1qθp2qθp2q z θp1qθp1qθp2qθp2q z θp1qθp1qθp2qθp2q.
(43)
“ f f π π ` m g g π π ` m f f g g
w η
The eigenvalues form the Lyapunov exponents in the finite-dimensional manifold and
characterize the dynamical behavior of the state vector near an attractor. Because of
the multi-dimensionality of the parameter space, it is not ideal to extract the eigenvalue
properties analytically from the trace and determinant conditions. Accordingly,
informative constraints on the parameters must be determined on the heuristic basis.
In this study, we numerically searched for parameters that led to pure-imaginary
eigenvalues, thereby entailing stationary attractors.
Numerical result I: Spontaneous dynamics
We first consider the spontaneous dynamics of the brain evolved from the particular
solution in Eq. (41) with null sensory inputs in our proposed BM. The formal
representation for the spontaneous trajectory, Ψ t , can be obtained by direct
sp
p q
integration as
Ψ t Ψ
R´1 e´RtI
, (44)
sp c sp
p q “ ´
where the constant vector, Ψ , is specified as Ψ R´1I , where I is the
c c sp sp
“
inhomogeneous term solely from the internal driving sources without the sensory inputs,
that is, s 0 [see Eq. (40)].
“
In Fig. 1, we depict the trajectories generated assuming a set of parameters in the
neural generative models [Eqs. (35)–(37)] as`
θp0q,θp0q,θp0q
0,10,10 ,
p g f π q “ p q
θp1q,θp1q 2eiπ{2
, 1 ,
p g f q “ p ´ q
θp2q,θp2q eiπ{2 ,eiπ{2
.
g π
p q “ p q
¶ In this study, we distinguish between the concept of a stationary state and a steady state: a steady
stateisthegeneraltermindicatingthelimitingstateastÑ8,whereasastationarystateisthespecific
steady state where an oscillatory time dependence remains.
` Theseparametervalueswereselectedinanadhocmannerthroughnumericalinspectiontoproducea
dynamicattractor;therefore,thelatentdynamicsofthecognitivevector,Ψ,isevolvedintheextended,
complex-valued phase space in the present manifestation.
Free energy and inference in living systems 20
p CognitiveIntensity
100
(a) 350 (b)
300
50
250
200

-100 -80 -60 -40 -20 20 150 ■ ■
■
■
100 ■
■
■
-50 50■ ■ ■
s
20 40 60 80 100
Figure 2. Latentdynamicsunderstaticsensoryinputs: (a)Attractordevelopedfrom
arestingstate,Ψp0q,anddrivenbythestaticinputs“100,usingthesameparameter
valuesasinFig.1;theinitialstatewaschosenfromthespontaneousstatesinFig.1,and
forillustrationalpurposes,theattractorisdepictedinthetwo-dimensionalstatespace
spanned by pRerΨ2s,RerΨ4sq. (b) Cognitive intensity, |Ψ
c
| 2 , vs sensory input, s. The
filledsquaresarethe resultsfromtheneuralinertialmassespm ,m ,m q“p10,1,10q
z w η
and open circles are the results from pm ,m ,m q “ p1,1,1q; the numerical values
z w η
for the other generative parameters are the same as those used in Fig. 1. [Data are in
arbitrary units.]
In addition, the neural inertial masses were assumed to have values of
m ,m ,m 1,1,1 .
z w η
p q “ p q
The major numerical observations are as follows. The brain’s spontaneous trajectory
occupies a limited region in the state space around a center, Ψ , which describes a
c
dynamic attractor forming the brain’s resting states before sensory influx occurs. The
center is specified by the internal parameters, that is, the generative parameters and
neural masses. We numerically checked that the position of Ψ varies with the values of
c
neural masses and the brain’s prior belief on the hidden causes of the sensory input and
motor state. We also confirmed that the size of attractors is affected by the generative
parameters and neural masses.
Numerical result II: Passive recognition dynamics
To demonstrate passive perception, we exposed the resting brain to a static sensory
signal; that is, we inserted s constant in Eq. (40). In this case, the formal solution
“
Eq. (41) can be reduced to
Ψ t
e´RtΨ
0 Ψ
R´1 e´RtI,
(45)
c
p q “ p q` ´
where, on the RHS, the first term specifies the homogeneous transience of the initial
resting state, Ψ 0 , second term, Ψ , denotes the center of attractors, and last term
c
p q
describes the dynamic development from the inhomogeneous source, I s . In contrast
p q
to the spontaneous attractors, the location of center depends on the sensory input, s:
Ψ Ψ s R´1I s .
c c
“ p q “ p q
We performed numerical integration and obtained the stationary attractor in the
presence of static sensory inputs. Thus, we confirmed that the attractor behaved
Free energy and inference in living systems 21
s(t) p (t)

120
60
100 (b)
(a)
40
80
60 20
40
t
100 200 300 400 500
20
0 t -20
100 200 300 400 500
Figure 3. Active dynamics under time-dependent sensory inputs: (a) Salient feature
of streaming perturbation at the receptor state, sptq; we assume a sigmoid shape for
the temporal dependence with the saturated value s8 “ 100, stiffness k “ 0.2, and
mid-time t “500. (b)Motorinference ofthe sensorysignals;the BMwasintegrated
m
using the same parameter values as in Fig. 2 for the generativeparametersand neural
masses. [All curves are in arbitrary units.]
similarly asinthespontaneous case, but with ashift of thecenter because ofthe nonzero
sensory stimulus. The outcome is presented in Fig. 2. Figure 2(a) shows a typical
attractor in the two-dimensional state space, which is evolved from a spontaneous state
shown in Fig. 1. In addition, in Fig. 2(b), we show the change in the cognitive intensity,
Ψ s 2 , with respect to sensory inputs, s, which is defined as
c
| p q|
Ψ s 2 Ψ Ψ˚.
| c p q| ” c c
Given a sensory stimulus, we numerically observe that the cognitive intensity is weaker
for a larger inertial mass. The neural inertial masses represent the inferential precision
in the internal models; accordingly, the result shows that less cognitive intensity is
required when the internal model is more precise in perceptual inference. The cognitive
intensity may be used as a quantitative measure of awareness or attention in phycology.
Our intensity measure is closely related to neuroimaging analysis [65], where the neural
response to sensory inputs was analyzed as the energy-level change associated with
information encoding.
Numerical result III: Active recognition dynamics
We considered the nonstationary sensory input, s t , that renders the time-dependent
p q
driving I [Eq. (40)] in the latent dynamics: the sensory receptors are continuously
elicited, andthebrainengages inonlinecomputationtointegratethe BM. Fornumerical
purposes, we assumed the salient feature of sensory signal, s t , as a sigmoid temporal
p q
dependence:
s
s t 8 , (46)
p q “ 1 e´kpt´tmq
`
where t indicates the time when the sensory intensity reaches the midpoint and k
m
adjusts the stiffness of transience in approaching the limiting value, s t s . The
8
p q Ñ
sigmoidal sensory inputs are depicted as a function of time in Fig. 3(a).
Free energy and inference in living systems 22
We numerically integrated Eqs. (31)–(34) assuming the same initial state selected
for the data shown in Fig. 2, subject to the sensory stream presented in Fig. 2(a). In
Fig. 3(b), we illustrate the imaginary part of the motor state, a t , in continuous time,
p q
which is the online outcome of active inference of the sensory input. For illustrational
purposes, we adopted the sigmoid shape for the temporal dependence with a saturated
value of s 100, stiffness of k 0.2, and mid-time of t 500. The results suggest
8 m
“ “ “
that the motor state aligns with the sensory variation and successfully infers the sharp
change in the sensory input around t 250.
“
In addition, Fig. 4 presents the attractor dynamics at several time steps exhibiting
state transition, dynamic bifurcation, froma resting state, Ψ 0 , to a cognitive attractor,
p q
Ψ t , over time [51]. The numerical computation reveals the initial development of the
p q
NEQ attractor with passage of time shown in Fig. 4(a) and Fig. 4(b), which corresponds
to the inferential outcome of the lower part of the sigmoid influx depicted in Fig. 3(a).
The intermediate attractor in Fig. 4(b) repeats the spontaneous attractor presented in
Fig.1because thesensory inputisnearlynull apartfromthenegligiblefluctuationinthe
present model. As time elapses from Fig. 4(b) to Fig. 4(c), the cognitive state begins to
escape from the first attractor and build the second attractor. Eventually, with passage
of time shown in Fig. 4(c) and Fig. 4(d), the dynamic transition between two attractors
completes over a relaxation time period, say, τ. At time t τ, the stationary attractor
ą
can be described by the expansion
Ψ t Ψ ¯ c e´iωαtφ , (47)
c α α
p q “ `
α
ÿ
where iω λ and φ are the eigenvalues and corresponding eigenvectors of the
α α α
”
relaxation matrix, R, respectively. The expansion coefficients, c , are specified by the
α
initial condition, Ψ 0 . The center of mass of the attractor, Ψ¯ , is specified by R´1I ,
c 8
p q
where I is the source vector I with the saturated sensory input, s . The shift of the
8 8
center between two stationary attractors is shown in Fig. 4(d).
The concrete example presented above fully accommodates the active inference of a
living agent inferring thesensory signal’s salient feature andperforming feedback motor-
inference in the double closed-loop cognitive architecture [See Appendix]. Although the
illustration accounts for a single sensorimotor system, our formulation can also handle
multiplemodalitiesofsensory inputsposingmultisensory perceptionproblems. Notably,
the time-dependent sensory influx, s t , makes the linear BM nonconservative, which,
p q
froma dynamical-systems perspective, serves asa bifurcationparameter. Our numerical
illustration of the dynamic transition from a resting state to a cognitive attractor is
relevant to recent studies of cognitive control of behavior in psychiatry [21, 79] and
stability of conscious states against external perturbations in patients with brain injury
[25, 95].
Free energy and inference in living systems 23
Figure 4. Attractor dynamics inferring the nonstationary sensory influx depicted in
Fig. 3(a): (a) t “ 5, (b) t “ 100, (c) t “ 260, and (d) t “ 500. The trajectory,
Ψptq, results from the direct numerical integration of the BM described by Eqs. (31)–
(34); the initial state, Ψp0q“p´16.9,21.1,´13.3q,was selected from the spontaneous
attractor given in Fig. 1. For numerical purposes, the attractor evolution is depicted
inthethree-dimensionalstatespacespannedbypRerµs,Reras,Rerp sq. Thenumerical
µ
valuesadoptedforallparametersarethesameasthoseinFig.3. [Dataareinarbitrary
units.]
6. Summary and conclusion
This study is based on the consensus that living systems are self-organized into an
NEQ stationary state that violates the detailed balance while sustaining physiological
and bodily properties. In a biological context, the thermodynamic second law, the
FTs in its modern forms, implies that there is inevitably uncompensated energy in an
organism’s metabolicprocesses of maintaining itshomeostasis intheenvironment. More
precisely, the amount of metabolic work is bounded from above by the thermodynamic
FE expense. Efficiency is important in any irreversible phenomena exhibiting the arrow
oftime, andbyextension, inbrainwork. WeappliedmodernFTstoabiologicalagentas
anopensystemandclarifiedwhytheconceptoftheFEismoreappropriatethanentropy
when discussing the question of What is life?. The thermodynamic and neuroscientific
FEPs were evaluated based on their respective mathematical inequalities, suggesting
the FE bounds as variational objective functions for minimization. Consequently, we
revealed the drawbacks of both principles in accounting for cognitive biological systems
Free energy and inference in living systems 24
and proposed an integrated thermodynamic and Bayesian approach to the biological
FEP as a self-organizing principle of life.
The brain states of higher organisms can only be realistically described in terms
of probability because of the enormous neuronal degrees of freedom and morphological
complexity. And at the core of the biological FEP are the likelihood and prior densities,
making up the G-density, which are thought to be the NEQ probabilities of the
physical brain states. This study argues that the brain dynamics at the mesoscopic,
constitutional level are stochastic because of classical negligence, for which time-
asymmetricLangevinequationswereemployed. Thebrokentime-reversal symmetrywas
attributed to biological systems being open to the environment. To statistically describe
the brain states, we further used the Markovian approximation in state transitions
and adopted the Smoluchowski-Fokker-Planck equation to determine the probability
densities of the continuous brain variables. We viewed the S-F-P equation as a local
balance equation for probability and argued that its steady-state solutions furnish
the NEQ densities. The probability flux appearing in the S-F-P equation does not
vanish at the brain-environment interface, which reflects that a detailed balance will
not be reached in the SS limit, and thus, no standard fluctuation-dissipation theorem
is available in the NEQ brain. Instead, the SS flux resembles the Ampere law in
magnetism, resulting from the modified detailed-balance condition and supporting the
gradient flow of the NEQ probabilities.
We presented the brain as Schro¨dinger’s mechanical machine presiding over
predictive regulation of physiology and adaptive behavior of the body. The BM at
the system level is deterministic, indicating that the brain, as a macroscopic physical
system, obeys the law of large numbers entailing dimensionality reduction. In addition,
thermalfluctuationsfrombodytemperaturedonothavesignificant effectsonthebrain’s
low-dimensional functions; in other words, the brain is cognitively in its ground state
at effective zero temperature. The IFE was specified in terms of the latent variables
that probabilistically encode the environmental and motor states in the brain. As
aforementioned, the encoded probability densities were assumed to be SS solutions to
the S-F-P equation or more realistic ones. Central to our study was the idea that the
encoded, online IFE in the brain is a Lagrangian, defining the informational Action.
Based on Hamilton’s principle, we found that the brain deterministically conducts
allostatic regulation by completing the double closed-loop dynamics of perception and
adaptive motor behavior. We employed a simple model for nonstationary sensory influx
and illustrated the development of optimal trajectories in the neural phase space: we
numerically observed that the brain undergoes a dynamic transition from a resting
state to the stationary attractor, which corresponds to the online inference of the
environmental causes in continuous time. The proposed BM may apply to any generic
cognitive processes at the interoceptive, exteroceptive, and proprioceptive levels.
In conclusion, organisms’ adaptive sustentation cannot be described within
thermodynamic laws and the ensuing TFEP, for which the brain-inspired IFEP provides
a promising avenue. The IFEP, however, utilizes teleological information-theoretic
Free energy and inference in living systems 25
models and then considers the neural bases of those models. To establish an integrated
framework of the organizing principle of life, two rationales of FE minimization and
Bayesian inference were hybridized, and the BM directing the brain’s latent dynamics of
active inference was derived. Consequently, the brain’s perception and motor inference
in higher organisms were revealed to operate effectively as Schro¨dinger’s mechanical
machine. In addition, we numerically illustrated the attractor dynamics that develops
online during a sensory stream in the low-dimensional neural space.
Acknowledgements
The author is grateful to J. Kang for providing assistance with the mathematica
programming.
References
[1] RAAdams,SShipp, andKJ Friston. Predictionsnotcommands: activeinferenceinthe motor
system. Brain Struct Funct,218:611–643,2013.
[2] MiguelAguilera,BerenMillidge,AlexanderTschantz,andChristopherLBuckley. Howparticular
is the physics of the free energy principle? Physics of Life Reviews,2021.
[3] E Albarran-Zavala and F Angulo-Brown. A simple thermodynamic analysis of photosynthesis.
Entropy, 9(4):152–168,2007.
[4] S Amari. Dynamics of pattern formation in lateral-inhibition type neural fields. Biol Cybern,
27:77–87,1977.
[5] Bhashyam Balaji and Karl Friston. Bayesian state estimation using generalized coordinates.
In Ivan Kadar, editor, Signal Processing, Sensor Fusion, and Target Recognition XX,volume
8050, page 80501Y.International Society for Optics and Photonics, SPIE, 2011.
[6] Vijay Balasubramanian. Heterogeneity and efficiency in the brain. Proceedings of the IEEE,
103(8):1346–1358,2015.
[7] A M Bastos, W M Usrey, R A Adams, G R Mangun, P Fries, and K J Friston. Canonical
microcircuits for predictive coding. Neuron, 76(4):695–711,2012.
[8] Martin Biehl, Felix A Pollock, and Ryota Kanai. A technical critique of some parts of the free
energy principle. Entropy, 23(3):293,2021.
[9] Jelle Bruineberg, Krzysztof Dolega, Joe Dewhurst, and Manuel Baltieri. The emperor’s new
markov blankets. Behavioral and Brain Sciences,pages 1–63, 2021.
[10] Christopher L Buckley, Chang Sub Kim, Simon McGregor, and Anil K Seth. The free energy
principle for action and perception: A mathematical review. Journal of Mathematical
Psychology, 81:55–79,2017.
[11] Ozan Catal, Johannes Nauta, Tim Verbelen, Pieter Simoens, and BartDhoedt. Bayesianpolicy
selectionusingactiveinference. InWorkshoponStructure&Priors inReinforcementLearning
at ICLR 2019 : proceedings,page 9, 2019.
[12] Ozan Catal, Tim Verbelen, Toon Van de Maele, Bart Dhoedt, and Adam Safron. Robot
navigation as hierarchical active inference. Neural Networks, 142:192–204,2021.
[13] M Colombo and C Wright. First principles in the life sciences: the free-energy principle,
organicism, and mechanism. Synthese, 198:3463–3488,2021.
[14] Blake J Cook, Andre D H Peterson, Wessel Woldman, and John R Terry.
Neural Field Models: A mathematical overview and unifying framework.
Mathematical Neuroscience and Applications, Volume 2, March 2022.
[15] Andrew W Corcoranand Jakob Hohwy. Allostasis, interoception, and the free energy principle:
Feeling our way forward, 2017.
Free energy and inference in living systems 26
[16] Ignasi Cos, Giovanni Pezzulo, and Paul Cisek. Changes of mind after movement onset depend
on the state of the motor system. eNeuro, 8(6), 2021.
[17] J A Costa and A O Hero. Geodesic entropic graphs for dimension and entropy estimation in
manifold learning. IEEE Transactions on Signal Processing,52(8):2210–2221,2004.
[18] LDa Costa,TParr,NSajid,SVeselic,VNeacsu,andKarlFriston. Active inferenceondiscrete
state-spaces: A synthesis. Journal of Mathematical Psychology,99:102447,2020.
[19] T M Cover and J A Thomas. Elements of Information Theory. Wiley-Interscience, New York,
1991.
[20] Gavin E Crooks. Entropy production fluctuation theorem and the nonequilibrium work relation
for free energy differences. Phys Rev E, 60:2721–2726,1999.
[21] ZCui, JStiso,GLBaum,andetal. Optimizationofenergystatetransitiontrajectorysupports
the development of executive function during youth. Elife, 27(9):e53060,2020.
[22] J Cunningham and B Yu. Dimensionality reduction for large-scale neural recordings. Nat
Neurosci, 17:1500–1509,2014.
[23] Lancelot Da Costa, Pablo Lanillos, Noor Sajid, Karl Friston, and Shujhat Khan. How active
inference could help revolutionise robotics. Entropy, 24(3):361, 2022.
[24] G Deco, V K Jirsa, P A Robinson, M Breakspear, and K Friston. The dynamic brain: From
spiking neurons to neural masses and corticalfields. PLoS Comput Biol,4(8):e1000092,2008.
[25] Gustavo Deco, Josephine Cruzat, Joana Cabral, Enzo Tagliazucchi, Helmut Laufs, Nikos
Logothetis, and Morten Kringelbach. Awakening: Predicting external stimulation to force
transitions between different brain states. Proceedings of the National Academy ofSciences,
116:201905534,08 2019.
[26] KenjiDoya. Canonicalcorticalcircuitsandthedualityofbayesianinferenceandoptimalcontrol.
Current Opinion inBehavioral Sciences,41:160–167,2021.
[27] Xiaona Fang, Karsten Kruse, Ting Lu, and JinWang. Nonequilibrium physics in biology. Rev
Mod Phys, 91:045004,2019.
[28] Keith Douglas Farnsworth. How organisms gained causal independence and how it might be
quantified. Biology, 7(3):38, 2018.
[29] C Fiorillo. A neurocentric approachto bayesian inference. Nat Rev Neurosci,11:605,2010.
[30] Nahuel Freitas, Gianmaria Falasco, and Massimiliano Esposito. Linear response in large
deviationstheory: amethodtocomputenon-equilibriumdistributions. NewJournalofPhysics,
23(9):093003,2021.
[31] K Friston. Life as we know it. Journal of The Royal Society Interface,10(86), 2013.
[32] K Friston, J Mattout, and J Kilner. Action understanding and active inference. Biol Cybern,
104:137–160,2011.
[33] Karl Friston. The free-energy principle: a rough guide to the brain? Trends Cogn Sci, 13:293–
301, 2009.
[34] Karl Friston. The free-energy principle: a unified brain theory? Nat Rev Neurosci,11:127–138,
2010.
[35] Karl Friston. A free energy principle for a particular physics, 2019.
[36] KarlFristonandPingAo. Freeenergy,value,andattractors. ComputationalandMathematical
Methods in Medicine,2012:937860,2012.
[37] Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, and Giovanni
Pezzulo. Active inference: A process theory. Neural Computation, 29(1):1–49,2017.
[38] CCAFung,KYMWong,andSWu. Amovingbumpinacontinuousmanifold: acomprehensive
study of the tracking dynamics of continuous attractor neural networks. Neural Comput,
22(3):752–792,2010.
[39] J A Gallego, M G Perich,R H Chowdhury,and et al. Long-termstability of corticalpopulation
dynamics underlying consistent behavior. Nat Neurosci, 23:260–270,2020.
[40] Juan A Gallego, Matthew G Perich, Lee E Miller, and Sara A Solla. Neural manifolds for the
control of movement. Neuron, 94(5):978–984,2017.
Free energy and inference in living systems 27
[41] F S Gnesotto, F Mura, J Gladrow, and C P Broedersz. Broken detailed balance and non-
equilibriumdynamicsinlivingsystems: areview. ReportsonProgressinPhysics,81(6):066601,
apr 2018.
[42] NigelGoldenfeldandCarlWoese. Life isphysics: Evolutionasacollectivephenomenonfarfrom
equilibrium. Annual Review of Condensed Matter Physics,2(1):375–399,2011.
[43] Joshua E Goldford and Daniel Segre. Modern views of ancient metabolic networks. Current
Opinion in Systems Biology,8:117–124,2018.
[44] DavidJGriffiths. IntroductiontoElectrodynamics. CambridgeUniversityPress,4edition,2017.
[45] JeffHawkins,SubutaiAhmad, andYuweiCui. Atheoryofhowcolumns inthe neocortexenable
learning the structure of the world. Frontiers in Neural Circuits,11:81, 2017.
[46] J J Hopfield. Neural networks and physical systems with emergent collective computational
abilities. Proc Natl Acad Sci U S A,79(8):2554–2558,1982.
[47] JordanM Horowitz andTodd R Gingrich. Thermodynamic uncertainty relationsconstrainnon-
equilibrium fluctuations. Nature Physics, 16:15–20,2019.
[48] YanpingHuangandRajeshPNRao. Predictivecoding. WIREsCognitiveScience,2(5):580–593,
2011.
[49] Takuya Isomura, Hideaki Shimazaki, and Karl J Friston. Canonical neural networks perform
active inference. Communications biology, 5(1):55, January 2022.
[50] GenrikhRIvanitskii. 21stcentury: whatislifefromtheperspectiveofphysics? Physics-Uspekhi,
53(4):327–356,2010.
[51] E Izhikevich. Dynamical systems in neuroscience. MIT Press, page 159, July 2007.
[52] C Jarzynski. Nonequilibrium equality for free energy differences. Phys Rev Lett, 78:2690–2693,
1997.
[53] Christopher Jarzynski. Equalities and inequalities: Irreversibility and the second law of
thermodynamics at the nanoscale. Annual Review of Condensed Matter Physics, 2(1):329–
351, 2011.
[54] Benjamin I Jelen, Donato Giovannelli, and Paul G Falkowski. The role of microbial electron
transfer in the coevolution of the biosphere and geosphere. Annual Review of Microbiology,
70(1):45–62,2016.
[55] K Jezek, E Henriksen, A Treves, and et al. Theta-paced flickering between place-cell maps in
the hippocampus. Nature, 478:246–249,2011.
[56] Viktor Jirsa and Hiba Sheheitli. Entropy, free energy, symmetry and dynamics in the brain.
Journal of Physics: Complexity, 3(1):015007,2022.
[57] Stuart Kauffman. Answering schr¨odinger’s “what is life?”. Entropy, 22(8):815, 2020.
[58] C S Kim. Statistical work-energy theorems in deterministic dynamics. Journal of the Korean
Physical Society, 67:273–289,2015.
[59] C S Kim. Recognition dynamics in the brain under the free energy principle. Neural
Computation, 30(10):2616–2659,2018.
[60] C S Kim. Bayesian mechanics of perceptual inference and motor control in the brain. Biol
Cybern, 115:87–102,2021.
[61] C S Kim and G P Morriss. Local entropy in quasi-one-dimensional heat transport. Phys. Rev.
E, 80:061137,2009.
[62] MichaelKirchhoff,ThomasParr,EnsorPalacios,KarlFriston,andJulianKiverstein. Themarkov
blankets of life: autonomy, active inference and the free energy principle. J R Soc Interface,
15(138):20170792,2018.
[63] T Korbak. Computationalenactivismunderthe freeenergyprinciple. Synthese,198:2743–2763,
2021.
[64] R Kubo, M Toda, and N Hashitsume. Statistical Physics II. Springer, Berlin, 1992.
[65] Strelnikov Kuzma. Energy-informationcoupling during integrative cognitive processes. Journal
of Theoretical Biology, 469:180–186,2019.
[66] L D Landau and E M Lifshitz. Mechanics: Volume 1(Course ofTheoretical Physics Series) 3rd
Free energy and inference in living systems 28
Edition. Elsevier Ltd, Amsterdam, 1976.
[67] L D Landau and E M Lifshitz. Statistical Physics Part 1: Volume 5 (Course of Theoretical
Physics Series) 3rd Edition. PergamonPress, Oxford, 1980.
[68] NilesELehmanandStuartAKauffman. Constraintclosuredrovemajortransitionsintheorigins
of life. Entropy, 23(1):105,2021.
[69] William B Levy and Victoria G Calvert. Communication consumes 35 times more energy than
computation in the human cortex, but both costs are needed to predict synapse number.
Proceedings of the National Academy ofSciences,118(18):e2008173118,2021.
[70] Christopher W Lynn, Eli J Cornblath, Lia Papadopoulos, Maxwell A Bertolero, and Danielle S
Bassett. Brokendetailedbalance andentropy productionin the humanbrain. Proceedings of
the National Academy ofSciences,118(47):e2109889118,2021.
[71] Takazumi Matsumoto and Jun Tani. Goal-directed planning for habituated agents by active
inference using a variational recurrent neural network. Entropy, 22(5):564, 2020.
[72] H R Maturana and F J Varela. Autopoiesis and cognition: The realization of the living. D
Reidel Publishing Company, Boston, 1980.
[73] Pietro Mazzaglia, Tim Verbelen, Ozan Catal, and Bart Dhoedt. The free energy principle for
perception and action: A deep learning perspective. Entropy, 24(2):301,2022.
[74] CristianMeoandPabloLanillos. Multimodalvaeactiveinferencecontroller. In2021IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS),pages 2693–2699,2021.
[75] RMonassonandSRosay. Crosstalkandtransitionsbetweenmultiplespatialmapsinanattractor
neural network model of the hippocampus: Phase diagram. Phys Rev E, 87:062813,2013.
[76] RMonassonandSRosay. Transitionsbetweenspatialattractorsinplace-cellmodels. PhysRev
Lett, 115(5):098101,2015.
[77] V B Mountcastle. The columnar organizationof the neocortex. Brain, 120(4):701–722,1997.
[78] P Nurse. What is life? Five greatidesa inbiology. W W Norton & Company, New York, 2020.
[79] Linden Parkes, Tyler M Moore, Monica E Calkins, Matthew Cieslak, David R Roalf, Daniel H
Wolf,RubenCGur,RaquelEGur,TheodoreDSatterthwaite,andDanielleSBassett. Network
controllabilityintransmodalcortexpredictspositivepsychosisspectrumsymptoms. Biological
Psychiatry, 90(6):409–418,2021.
[80] ThomasParr,LancelotDaCosta,andKarlFriston. Markovblankets,informationgeometryand
stochastic thermodynamics. Phil Trans RSoc A,378:20190159,2020.
[81] Thomas Parr and Karl J Friston. The Discrete and Continuous Brain: From Decisions to
Movement?And Back Again. Neural Computation, 30(9):2319–2347,2018.
[82] APeter,CUran,JKlon-Lipok,RRoese,SvanStijn,WBarnes,JRDowdall,WSinger,PFries,
and M Vinck. Surface color and predictability determine contextual modulation of v1 firing
and gamma oscillations. eLife, 8:e42101,2019.
[83] Giovanni Pezzulo, Francesco Rigoli, and Karl Friston. Active inference, homeostatic regulation
and adaptive behavioural control. Progress in Neurobiology, 134:17–35,2015.
[84] HongQian. Adecompositionofirreversiblediffusionprocesseswithoutdetailedbalance. Journal
of Mathematical Physics, 54(5):053302,2013.
[85] K S Quigley, S Kanoski, W M Grill, L F Barrett, and M Tsakiris. Functions of interoception:
From energy regulation to experience of the self. Trends Neurosci, 44(1):29–38,2021.
[86] Vicente Raja, Dinesh Valluri, EdwardBaggs, Anthony Chemero, and Michael L Anderson. The
markov blanket trick: On the scope of the free energy principle and active inference. Physics
of Life Reviews, 39:49–72,2021.
[87] MJDRamstead,PBBadcock,andKJFriston. Answeringschr¨odinger’squestion: Afree-energy
formulation. Physics of Life Reviews,24:1–16,2018.
[88] H Risken. The Fokker-PlanckEquation. Springer-Verlag, Berlin, 1989.
[89] P A Robinson, C J Rennie, and J J Wright. Propagation and stability of waves of electrical
activity in the cerebral cortex. Phys Rev E, 56:826–840,1997.
[90] Noor Sajid, Philip J Ball, Thomas Parr, and Karl J Friston. Active inference: Demystified and
Free energy and inference in living systems 29
compared. Neural Computation, 33(3):674–712,2021.
[91] Dalton A R Sakthivadivel. Towards a geometry and analysis for bayesian mechanics, 2022.
[92] CansuSancaktar,MarcelAJvanGerven,andPabloLanillos. End-to-endpixel-baseddeepactive
inference for body perception and action. In 2020 Joint IEEE 10th International Conference
on Development and Learning and Epigenetic Robotics (ICDL-EpiRob),pages 1–8, 2020.
[93] Javier Sa´nchez-Can˜izares. The free energy principle: Good science and questionable philosophy
in a grand unifying theory. Entropy, 23(2):238, 2021.
[94] Yonatan Sanz Perl, Hern´an Bocaccio, Carla Pallavicini, Ignacio P´erez-Ipin˜a, Steven Laureys,
Helmut Laufs, Morten Kringelbach, Gustavo Deco, and Enzo Tagliazucchi. Nonequilibrium
brain dynamics as a signature of consciousness. Phys. Rev. E, 104:014411,Jul 2021.
[95] YonatanSanzPerl,CarlaPallavicini,IgnacioP´erezIpin˜a,andetal. Perturbationsindynamical
models of whole-brain activity dissociate between the level and stability of consciousness.
PLoS Computational Biology, 17(7):e1009139,July 2021.
[96] Erwin Schr¨odinger and Roger Penrose. What is Life?: With Mind and Matter and
Autobiographical Sketches. Cambridge University Press, Cambridge, 1992.
[97] Jay Schulkin and Peter Sterling. Allostasis: A brain-centered, predictive mode of physiological
regulation. Trends in Neurosciences, 42(10):740–752,2019.
[98] UdoSeifert. Stochasticthermodynamics: principlesandperspectives. EurPhysJB,64:423–431,
2008.
[99] Udo Seifert. From stochastic thermodynamics to thermodynamic inference. Annual Review of
Condensed Matter Physics, 10(1):171–192,2019.
[100] Biswa Sengupta, Martin B Stemmler, and Karl J Friston. Information and efficiency in the
nervous system–synthesis. PLOS Computational Biology,9:1–12,2013.
[101] BiswaSengupta,ArturoTozzi,GeraldKCooray,PamelaKDouglas,andKarlJFriston. Towards
a neuronal gauge theory. PLoS Biology, 14, 2016.
[102] HideakiShimazaki. Theprinciplesofadaptationinorganismsandmachinesii: Thermodynamics
of the bayesian brain, 2020.
[103] Wolf Singer. Recurrent dynamics in the cerebral cortex: Integration of sensory evidence with
stored knowledge. Proceedings of the National Academy of Sciences, 118(33):e2101043118,
2021.
[104] RyanSmith,KarlJFriston,andChristopherJWhyte. Astep-by-steptutorialonactiveinference
and its application to empirical data. Journal ofMathematical Psychology,107:102632,2022.
[105] PeterSterling. Allostasis: Amodelofpredictiveregulation. Physiology&Behavior,106(1):5–15,
2012.
[106] R Sutton and A Barto, editors. Reinforcement learning. MIT Press, Cambridge, MA, 1998.
[107] Nabil Swedan. Photosynthesis as a thermodynamic cycle. Heat Mass Transfer, 56:1649–1658,
2020.
[108] E Todorov. Optimal control theory. In Bayesian Brain: Probabilistic Approaches to Neural
Coding, pages 269–298.The MIT Press, Cambridge, 2006.
[109] MarkKTranstrum,BenjaminBMachta,KevinSBrown,BryanCDaniels,ChristopherRMyers,
and James P Sethna. Perspective: Sloppiness and emergent theories in physics, biology, and
beyond. The Journal of Chemical Physics,143(1):010901,2015.
[110] Cem Uran, Alina Peter, Andreea Lazar, William Barnes, Johanna Klon-Lipok, Katharine A
Shapcott, Rasmus Roese, Pascal Fries, Wolf Singer, and Martin Vinck. Predictive coding of
natural images by v1 activity revealed by self-supervised deep neural networks, 2021.
[111] HvonHelmholtzandJPCSouthall. Helmholtz’steatiseonphysiologicaloptics,Vol. 3. Courier
Corporation, 2005.
[112] Jin Wang, Li Xu, and Erkang Wang. Potential landscape and flux framework of nonequilibrium
networks: Robustness, dissipation, and coherence of biochemical oscillations. Proceedings of
the National Academy ofSciences,105(34):12271–12276,2008.
[113] TJWills,CLever,FCacucci,NBurgess,andJO’Keefe. Attractordynamicsinthehippocampal
Free energy and inference in living systems 30
representation of the local environment. Science, 308:873–876,2005.
[114] R Zwanzig. Nonequilibrium Statistical Mechanics. Oxford Univ Press, Berlin, 2001.
Appendix: Dual structure of perception and motor inference
Here, we describe a significant feature of our derived BM capturing the dual nature of
the sensory and motor inference in the neocortex [26], and briefly discuss its relevance
to other control theories.
Figure 5 shows the double-loop architecture of the neural circuitry emerging from
the attained BM. The environmental cause, ϑ, encodes the sensory data, s, at the
peripheral interface (receptors or input layers), and the brain conducts the variational
Bayesian inference that conjointly integrates the double closed-loop dynamics of sensory
perception (A) and motor control (B). Note that the neural units µ,p ,a,p are
µ a
p q
connected by arrows for excitatory driving and by lines guided by filled dots for
inhibitory driving. Loop (A): The state unit, µ, in neuronal population predicts the
input, s, based on the internal model, g 1 µ . The error signal, ξ z µ s g 1 µ ,
p q p q “ ´ p q
weighted by the accuracy, m , of the model, innervates the state-error unit, p , in the
z µ
population. Theerrorunit estimates thestateby assimilating thediscrepancy andsends
the feedback signal to the state unit. Then, the state unit updates its expectation and
predicts the sensory input again, which completes the passive perceptual loop. Loop
(B): The motor (effector) unit, a, alters the sensory input, s, according to the protocol,
g 2 a , to promote accurate sensation of the data. The error signal, m z s g 2 a , acts
p q p ´ p qq
as a control command to call for an adjustment in the motor-error unit, p . Then, the
a
adjusted motor-dynamics transmits the feedback signal to the effector state to further
modifythesensory data, which completes theactive perceptive loop. The doubleclosed-
loop dynamics concurrently continue until an optimal trajectory, Ψ t , is fulfilled in the
p q
neural hyper-phase space, which corresponds to optimizing the informational classical
Action, S, defined in Eq. (26).
Our Hamiltonian formulation renders the sensory-driving term, s g µ,a , to
´ p q
appear explicitly in the BM [see Eqs. (33) and (34)]. Its role is similar to the
unsupervised updating rule in the reinforcement-learning framework [106]; specifically,
it resembles the continuous control signal in the optimal control theory described
by the Hamilton-Jacobi-Bellman equation [108]. The sensory-discrepancy signal not
only affects the prediction error, p , in the state prediction [Eq. (33)], but also the
µ
prediction error, p , of the motor inference [Eq. (34)]; this interrelation provides the
a
neural mechanism for adaptive motor feedback via Eq. (32). The momenta in our
formulation are termed a costates in the deterministic optimal control theory.
Also, the policy, π, in Eq. (32) accounts for the online motor behavior, which
prescribesmotorplanning andcanaccommodateasituated decision [16]. Inthediscrete-
state formulations, the policy is defined as a sequence of actions or decisions in discrete
time [90, 49], where the authors incorporate the necessary time-dependence directly in
the definition of FE. On the contrary, our continuous-time theory defines the policy
Free energy and inference in living systems 31
 p

(B)
g ()
2
m ξ ()
z z
ϑ 
m ξ (μ)
g (μ) z z
1
(A)
μ p
μ
Figure 5. Schematic of the neural circuitry exhibiting the double closed-loop
architecture, which emerges from the Bayesian mechanics prescribed by Eqs. (31)–
(34).
as continuous action planning, which we model as the generative function of motor
inference. The time-dependence of policy generates the history-dependent response of
the brain’s cognitive state; see Eq. (41), in which the time, t, can be either at present or
in the future. When the dynamic perception is coupled to categorical-decision making,
the mixed continuous-discrete approaches may shape the active inference problems [81].
Finally, we have employed a set of simple and specific generative models [Eqs. (35)–
(37)]foraconcretenumericalillustration. Inpractice, however, theemployedmodelscan
be readily generalized. For instance, one may consider an action-dependent generative
function, f µ,a;θ , which will make the state dynamics [Eq. (19)] subjected to actions.
f
p q
Further investigations using more realistic models are required to learn the implication
and utility of our theory for the dual closed-loop dynamics related to the standard
control theories.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Free energy and inference in living systems"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
