=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Parallel MCMC Without Embarrassing Failures
Citation Key: souza2022parallel
Authors: Daniel Augusto de Souza, Diego Mesquita, Samuel Kaski

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2022

Key Terms: parallel, embarrassingly, embarrassing, daniel, computing, strategy, diego, mcmc, posterior, data

=== FULL PAPER TEXT ===

Parallel MCMC Without Embarrassing Failures
Daniel Augusto de Souza1, Diego Mesquita2,3, Samuel Kaski2,4, Luigi Acerbi5
1University College London 2Aalto University 3Getulio Vargas Foundation
4University of Manchester 5University of Helsinki
daniel.souza.21@ucl.ac.uk, diego.mesquita@fgv.br, samuel.kaski@aalto.fi, luigi.acerbi@helsinki.fi
Abstract through the data. Various approaches that leverage
distributed computing have been proposed to mitigate
Embarrassingly parallel Markov Chain Monte these limitations (Angelino et al., 2016; Robert et al.,
Carlo (MCMC) exploits parallel computing
2018). In general, we can split these approaches be-
to scale Bayesian inference to large datasets
tween those that incur constant communication costs
by using a two-step approach. First, MCMC
andthoserequiringfrequentinteractionbetweenserver
is run in parallel on (sub)posteriors defined
and computing nodes (Vehtari et al., 2020).
on data partitions. Then, a server combines
local results. While efficient, this framework Embarrassingly parallel MCMC (Neiswanger et al.,
is very sensitive to the quality of subposte- 2014) is a popular class of methods which employs
rior sampling. Common sampling problems a divide-and-conquer strategy to sample from a tar-
such as missing modes or misrepresentation get posterior, requiring only a single communication
of low-density regions are amplified – instead step. For dataset D and model parameters θ ∈ RD,
ofbeingcorrected–inthecombinationphase, suppose we are interested in the Bayesian posterior
leading to catastrophic failures. In this work, p(θ|D)∝p(θ)p(D|θ),wherep(θ)isthepriorandp(D|θ)
we propose a novel combination strategy to the likelihood. Embarrassingly parallel methods be-
mitigate this issue. Our strategy, Parallel Ac- gin by splitting the data D into K smaller partitions
tive Inference (PAI), leverages Gaussian Pro- D 1 ,...,D K so that we can rewrite the posterior as
cess(GP)surrogatemodelingandactivelearn-
K K
Y Y
ing. After fitting GPs to subposteriors, PAI p(θ|D)∝ p(θ)1/Kp(D |θ)≡ p (θ). (1)
k k
(i) shares information between GP surrogates
k=1 k=1
to cover missing modes; and (ii) uses active Next, an MCMC sampler is used to draw samples
sampling to individually refine subposterior S from each subposterior p (θ), for k = 1...K, in
k k
approximations. We validate PAI in challeng- parallel. Then, the computing nodes send the local
ing benchmarks, including heavy-tailed and results to a central server, for a final aggregation step.
multi-modal posteriors and a real-world ap- These local results are either the samples themselves
plication to computational neuroscience. Em- or approximations q ,...,q built using them.
1 K
pirical results show that PAI succeeds where
Works in embarrassingly parallel MCMC mostly focus
previous methods catastrophically fail, with
oncombinationstrategies. Scottetal. (2016)employ a
a small communication overhead.
weighted average of subposterior samples. Neiswanger
et al. (2014) propose using multivariate-normal surro-
1 INTRODUCTION
gates as well as non-parametric and semi-parametric
Markov Chain Monte Carlo (MCMC) methods have forms. Wang et al. (2015) combine subposterior sam-
become a gold standard in Bayesian statistics (Gelman plesintoahyper-histogramwithrandompartitiontrees.
et al., 2013; Carpenter et al., 2017). However, scaling Nemeth and Sherlock (2018) leverage density values
MCMCmethodstolargedatasetsischallengingdueto computed during MCMC to fit Gaussian process (GP)
their sequential nature and that they typically require surrogates to log-subposteriors. Mesquita et al. (2019)
many likelihood evaluations, implying repeated sweeps use subposterior samples to fit normalizing flows and
applyimportancesamplingtodrawfromtheirproduct.
Proceedings of the 25thInternational Conference on Artifi-
Despitetheseadvances,parallelMCMCsuffersfroman
cial Intelligence and Statistics (AISTATS) 2022, Valencia,
unaddressed limitation: its dependence on high-quality
Spain. PMLR: Volume 151. Copyright 2022 by the au-
thor(s). subposterior sampling. This requirement is especially
2202
raM
92
]LM.tats[
2v45111.2022:viXra
Parallel MCMC Without Embarrassing Failures
difficulttomeetwhensubposteriorsaremulti-modalor II: Catastrophic model mismatch. Since the q
k
heavy-tailed, in which cases MCMC chains often visit are approximations of the true subposteriors p , small
k
only a subset of modes and may underrepresent low- deviations between them are expected – this is not
density regions. Furthermore, the surrogates (q )K what we refer to here. Instead, an example of catas-
k k=1
builtonlyonlocalMCMCsamplesmightmatchpoorly trophic model mismatch is when a simple parametric
the true subposteriors if not carefully tuned. modelsuchasamultivariatenormalisusedtomodela
multimodal posterior with separate modes (see Section
Outline and contributions. We first discuss the
4). Evennonparametricmethodscanbevictimsofthis
failure modes of parallel MCMC (Section 2). Draw-
failure. For example, GP surrogates are often used to
ing insight from this discussion, Section 3 proposes
model nonparametric deviations of the log posterior
a novel GP-based solution, Parallel Active Inference
from a parametric ‘mean function’. While these mod-
(PAI). After fitting the subposterior surrogates, PAI
els can well represent multimodal posteriors, care is
shares a subset of samples between computing nodes
needed to avoid grossly mismatched solutions in which
topreventmodecollapse. PAIalsousesactivelearning
a q ‘hallucinates’ posterior mass due to an improper
to refine low-density regions and avoid catastrophic k
placement of the GP mean function (Fig 1B).
model mismatch. Section 4 validates our method on
challenging benchmarks and a real-world neuroscience
example. Finally, Section 5 reviews related work and Insight2: Wecannottakesubposteriormodels
at face value. Reliable algorithms should check
Section 6 discusses strengths and limitations of PAI.
andrefinetheq ’stoavoidcatastrophicfailures.
k
2 EMBARRASSINGLY PARALLEL
MCMC: HOW CAN IT FAIL?
III: Underrepresented tails. This effect is more
Werecallthebasicstructureofagenericembarrassingly subtle than the failure modes listed above, but it con-
parallelMCMCalgorithminAlgorithm1. Thisschema tributes to accumulating errors in the estimate of the
has major failure modes that we list below, before combined posterior. The main issue here is that, by
discussing potential solutions. We also illustrate these construction, MCMC samples and subposterior models
pathologies in Fig 1. In this paper, for a function f based on these samples focus on providing information
withscalaroutputandasetofpointsS ={s ,...,s }, about the high-posterior-mass region of the subpos-
1 N
we denote with f(S)≡{f(s ),...,f(s )}. terior. However, different subposteriors may overlap
1 N
only in their tail regions (Fig 1C), implying that the
Algorithm 1 Generic embarrassingly parallel MCMC tails and the nearby ‘deep’ regions of each subposterior
might actually be the most important in determining
Input: Data partitions D ,...,D ; prior p(θ); likelihood
1 K
function p(D|θ). the exact shape of the combined posterior.
1: parfor 1...K do . Parallel steps
2: S k ←MCMCsamplesfromp k (θ)∝p(θ)1/Kp(D k |θ) Insight3: Subposteriormodelsbuiltonlyfrom
3: build subposterior model q (θ) from S
k k MCMC samples (and their log density) can
4: end parfor
5: Combine: q(θ)∝
QK
q (θ) . Centralized step
miss important information about the tails and
k=1 k nearby regions of the subposterior which would
also contribute to the combined posterior.
2.1 Failure modes
I: Mode collapse. It is sufficient that one subpos-
terior q misses a mode for the combined posterior q 2.2 Past solutions
k
to lack that mode (see Fig 1A). While dealing with Sincethereisnoguaranteethatqapproximateswellthe
multiple modes is an open problem for MCMC, here posterior p, Nemeth and Sherlock (2018) refine q with
theissueisexacerbatedbythefactthatasinglefailure an additional parallel step, called Distributed Impor-
propagatestothefinalsolution. Aback-of-the-envelope tanceSampler(DIS).DISusesq asaproposaldistribu-
calculation shows that even if the chance of missing a tion and draws samples S ∼q, that are then sent back
mode is small, ε>0, the probability of mode collapse forevaluationofthelogdensitylogp (S)ateachparal-
k
in the combined posterior is ≈ (1−ε)K making it a lel node. The true log density logp(S)=P logp (S)
likely occurrence for sufficiently large K. is then used as a target for importance sam k pling k /re-
sampling (Robert and Casella, 2013). Technically, this
Insight 1: For multimodal posteriors, mode step makes the algorithm not ‘embarrassingly paral-
collapse is almost inevitable unless the comput- lel’ anymore, but the prospect of fixing catastrophic
ing nodes can exchange information about the failures outweighs the additional communication cost.
location of important posterior regions. However, DIS does not fully solve the issues raised in
Section 2.1. Notably, importance sampling will not
Daniel Augusto de Souza, Diego Mesquita, Samuel Kaski, Luigi Acerbi
Figure 1: Failure modes of embarrassingly parallel MCMC. A–C. Each column illustrates a distinct
failure type described in Section 2.1. For each column, the top rows show two subposteriors p (θ) (black dashed
k
line: ground truth; blue circles: MCMC samples), and the bottom row shows the full posterior p(θ|D) with the
approximate posterior combined using the method specified in the legend (orange line). These failure modes are
general and not unique to the displayed algorithms (see Appendix A for details and further explanations).
help recover the missing regions of the posterior if q posterior density collapse. Moreover, existing methods
does not cover them in the first place. DIS can help alreadyconsideranextracommunicationstep(Nemeth
in some model mismatch cases, in that ‘hallucinated’ and Sherlock, 2018), as mentioned in Section 2.2.
regions of the posterior will receive near-zero weights
after the true density is retrieved. Active learning. We use active learning as a gen-
eral principle whenever applicable. The general idea is
2.3 Proposed solution toselectpointsthatareinformativeabouttheshapeof
Drawing from the insights in Section 2.1, we propose the subposterior, minimizing the additional communi-
two key ideas to address the blind spots of embarrass- cationrequired. Activelearningisusedhereinmultiple
ingly parallel MCMC. Here we provide an overview of steps: when selecting samples from MCMC to build
our solution, which is described in detail in Section the surrogate model (as opposed to thinning or ran-
3. The starting point is modeling subposteriors via dom subsampling); as a way to choose which samples
Gaussian process surrogates (Fig 2A). fromothernodestoaddtothecurrentsurrogatemodel
of each subposterior q (only informative samples are
k
Sample sharing. We introduce an additional step added); to actively sample new points to reduce un-
in which each node shares a selected subset of MCMC certainty in the local surrogate q (Fig 2C). Active
k
samples with the others (Fig 2B). This step provides learning contributes to addressing both catastrophic
sufficient information for local nodes to address mode model mismatch and underrepresented tails.
collapse and underrepresented tails. While this com-
munication step makes our method not strictly ‘em- Combined,theseideassolvethefailuremodesdiscussed
barrassingly’ parallel, we argue it is necessary to avoid previously (Fig 2D).
Parallel MCMC Without Embarrassing Failures
Figure 2: Parallel active inference (PAI). A. Each log subposterior logp (θ) (black dashed line) is modeled
k
via Gaussian process surrogates (orange dashed line: mean GP; shaded area: 95% confidence interval) trained
on MCMC samples S (blue circles) and their log-density logp (S ). Here, MCMC sampling on the second
k k k
subposterior has missed a mode. B. Selected subsets of MCMC samples are shared across nodes, evaluated locally
and added to the GP surrogates. Here, sample sharing helps finding the missing mode in the second subposterior,
but the GP surrogate is now highly uncertain outside the samples. C. Subposteriors are refined by actively
selecting new samples (stars) that resolve uncertainty in the surrogates. D. Subposteriors are combined into the
full approximate log posterior (orange line); here a perfect match to the true log posterior (black dashed line).
3 PARALLEL ACTIVE INFERENCE First,wepickaninitialsubsetofn samplesS(0) ⊂S ,
0 k k
that we use to train an initial GP (details in Appendix
Inthissection,wepresentourframework,whichwecall C.1). Then, we iteratively select points θ? from S by
k
ParallelActiveInference(PAI),designedtoaddressthe maximizing the maximum interquantile range (MAX-
issues discussed in Section 2. The steps of our method IQR) acquisition function (Järvenpää et al., 2021):
are schematically explained in Fig 2 and the detailed
algorithm is provided in Appendix C. n (cid:16) (cid:17)o
θ? =argmax em(θ;S k (t))sinh u·s(θ;S k (t)) , (2)
θ
3.1 Subposterior modeling via GP regression
where m(θ;S(t)) and s(θ;S(t)) are, respectively, the
Asperstandardembarrassinglyparallelalgorithms, we k k
posterior latent mean and posterior latent standard
assume each node computes a set of samples S k and deviation of the GP at the end of iteration t ≥ 0;
their log density, logp
k
(S
k
), by running MCMC on
and sinh(z) = (exp(z)−exp(−z))/2 for z ∈ R is the
the subposterior p k . We model each log-subposterior hyperbolic sine. Eq. 2 promotes selection of points
L k (θ) ≡ logq k (θ) using GP regression (Fig 2A; see with high posterior density for which the GP surrogate
Rasmussen and Williams (2006); Nemeth and Sherlock
isalsohighlyuncertain,withthetrade-offcontrolledby
(2018); Görtler et al. (2019) and Appendix B for more
u>0,wherelargervaluesofufavorfurtherexploration.
information). We say that a GP surrogate model is
In each iteration t+1, we greedily select a batch of
trained on S as a shorthand for (S ,logp (S )).
k k k k n points at a time from S \S(t) using a batch
batch k k
version of MAXIQR (Järvenpää et al., 2021). We add
When building the GP model of the subposterior, it is
not advisable to use all samples S k because: (1) exact the selected points to the current training set, S k (t+1),
inferenceinGPsscalescubicallyinthenumberoftrain- and retrain the GP after each iteration (see Appendix
ing points (although see Wang et al. (2019)); (2) we C.1). After T iterations, our procedure yields a subset
want to limit communication costs when sharing sam- of points S0 ≡S(T) ⊆S that are highly informative
k k k
ples between nodes; (3) there is likely high redundancy about the shape of the subposterior.
inS abouttheshapeofthesubposterior. Nemethand
k
Sherlock (2018) simply choose a subset of samples by 3.2 Sample sharing
‘thinning’ a longer MCMC chain at regular intervals. Inthisstep,eachnodek sharestheselectedsamplesS0
k
Instead, we employ active subsampling as follows. withallothernodes(Fig2B).Thus,nodekgainsaccess
Daniel Augusto de Souza, Diego Mesquita, Samuel Kaski, Luigi Acerbi
to the samples S0 ≡ S S0. Importantly, S0 might importance sampling/resampling with an appropriate
\k j6=k j \k
containsamplesfromrelevantsubposteriorregionsthat (adaptive) proposal would also be feasible.
node k has has not explored. As discussed in Section
As a final optional step, after combining the subposte-
3.1, for efficiency we avoid adding all points S0 to the
current GP surrogate for subposterior k. Ins
\
t
k
ead, we
riors into the full approximate posterior q(θ), we can
add a sample θ? ∈S0 to the GP training set only if refine the solution using distributed importance sam-
the prediction of the \k current GP surrogate deviates pling (DIS)asproposedbyNemethandSherlock(2018)
and discussed in Section 2.2.
from the true subposterior logp (θ?) in a significant
k
way (see Appendix C.2 for details). After this step,
we obtain an expanded set of points S00 that includes 3.5 Complexity analysis
k
information from all the nodes, minimizing the risk of Similarly to conventional embarrassingly parallel
mode collapse (see Section 2.1). MCMC, we can split the cost of running PAI into
two main components. The first consists of local costs,
3.3 Active subposterior refinement which involve computations happening at individual
Sofar, theGPmodelshavebeentrainedusingselected computing nodes (i.e., model fitting and active refine-
subsets of samples from the original MCMC runs. In ment). The second are global (or aggregation) costs,
this step, we refine the GP model of each subposterior which comprise communication and sampling from the
by sampling new points (Fig 2C). Specifically, each combined approximate posterior.
node k actively selects new points by optimizing the
MAXIQR acquisition function (Eq. 2) over X ⊆ RD 3.5.1 Model fitting
(see Appendix C.3). New points are selected greedily Aftersamplingtheirsubposterior,eachcomputingnode
in batches of size n batch , retraining the GP after each k has to fit the surrogate model on the subset of their
iteration. This procedure yields a refined set of points samples,S0. Thesesubsetsaredesignedsuchthattheir
k
S000 which includes new points that better pinpoint sizeisO(D)(seeAppendixC).Thus,thecostoffitting
k
the shape of the subposterior, reducing the risk of the surrogate GP models in each of the K computing
catastrophic model mismatch and underrepresented nodes is O(D3). The same applies for S00 and S000.
k k
tails. The final log-subposterior surrogate model L is
k
the GP trained on S000. 3.5.2 Communication costs
k
Traditional embarrassingly parallel MCMC methods
3.4 Combining the subposteriors
only require two global communication steps: (1) the
Finally, weapproximatethefullposteriorlogp(θ|D)=
central server splits the N observations among K com-
PK k=1 logp k (θ)bycombiningallsubposteriorstogether puting nodes; (2) each node sends S subposterior sam-
(Fig 2D). Since each log-subposterior is approximated plesofdimensionD backtotheserver,assumingnodes
by a GP, the approximate full log-posterior is a sum of draw the same number of samples. Together, these
GPs and itself a GP, L(θ) = PK
k=1
L
k
(θ). Note that steps amount to O(N +KSD) communication cost.
L(θ), being a GP, is still a distribution over functions.
Wewantthentoobtainapointestimateforthe(unnor- PAI imposes another communication step, in which
malized) posterior density corresponding to expL(θ). nodes share their subposterior samples and incurring
Onechoiceistotaketheposteriormean,whichleadsto O(KSD)cost. Furthermore,supposingPAIacquiresA
the expectation of a log-normal density (Nemeth and active samples to refine each subposterior, the cost of
Sherlock, 2018). We prefer a robust estimate and use sendinglocalresultstoserversisincreasedbyO(KAD).
the posterior median instead (Järvenpää et al., 2021). PAI also incur a small additional cost for sending back
Thus, our estimate is the value of the GP hyperparameters O(KD). In sum,
since usually A (cid:28) S, the asymptotic communication
( K ) cost of PAI is equivalent to traditional methods.
X
q(θ)∝exp m (θ;S000) . (3)
k k
k=1 3.5.3 Active refinement costs
Inlowdimension(D =1,2),Eq. 3canbeevaluatedon Active learning involves GP training and optimization
agrid. Inhigherdimension,onecouldsamplefromq(θ) of the acquisition function, but only a small number of
using MCMC methods such as NUTS (Hoffman and likelihood evaluations. Thus, under the embarrassingly
Gelman,2014),asdonebyNemethandSherlock(2018). parallel MCMC assumption that likelihood evaluations
However, q(θ) is potentially multimodal which does are costly (e.g., due to large datasets), active learning
not lend itself to easy MCMC sampling. Alternatively, is relatively inexpensive (Acerbi, 2018). More impor-
Acerbi (2018) runs variational inference on q(θ) using tantly, as shown in our ablation study in Appendix
as variational distribution a mixture of Gaussians with D.1, this step is crucial to avoid the pathologies of
alargenumberofcomponents. Finally,formoderateD, embarrassingly parallel MCMC.
Parallel MCMC Without Embarrassing Failures
3.5.4 Sampling complexity with an appropriate proposal. We report results as
Sampling from the aggregate approximate posterior mean ± standard deviation across ten runs in which
q(θ) only requires evaluating the GP predictive mean the entire procedure was repeated with different ran-
for each subposterior and does not require access to dom seeds. For all metrics, lower is better, and the
the data or all samples. The sampling cost is linear best(statisticallysignificant)resultsforeachmetricare
in the number of subposteriors K and the size of each reported in bold. See Appendix D.2 for more details.
GP O(D). Even if K is chosen to scale as the size of
4.1 Multi-modal posterior
the actual data, each GP only requires a small training
set, making them comparably inexpensive. Setting. In this synthetic example, the data consist
of N =103 samples y ,...,y drawn from the follow-
1 N
ing hierarchical model:
4 EXPERIMENTS
We evaluate PAI on a series of target posteriors with θ ∼p(θ)=N(0,σ2I )
p 2
different challenging features. Subsection 4.1 shows re-
2 1
sults for a posterior with four distinct modes, which is y ,...,y ∼p(y |θ)= X N (cid:0) P (θ ),σ2(cid:1)
1 N n 2 i i l
pronetomodecollapse(Fig1A).Subsection4.2targets
i=1
a posterior with heavy tails, which can lead to under-
where θ ∈ R2, σ = σ = 1/4 and P ’s are second-
represented tails (Fig 1C). Subsection 4.3 uses a rare p l i
degree polynomial functions. By construction, the
event model to gauge how well our method performs
target posterior p(θ|y ,...,y )∝p(θ)QN p(y |θ) is
when the true subposteriors are drastically different. 1 N n=1 n
multimodalwithfourmodes. Werunparallelinference
Finally, Subsection 4.4 concludes with a real-world ap-
on K = 10 equal-sized partitions of the data. We
plication to a model and data from computational neu-
provide more details regarding P ,P in Appendix D.3.
roscience(Acerbietal.,2018). Weprovideimplementa- 1 2
tiondetailsinAppendixB.3andsourcecodeisavailable Results. Fig 3 shows the output of each parallel
at https://github.com/spectraldani/pai. MCMC method for a typical run, displayed as samples
from the approximate combined posterior overlaid on
Algorithms. We compare basic PAI and PAI with
top of the ground-truth posterior. Due to MCMC oc-
theoptionaldistributedimportancesamplingstep(PAI-
casionally missing modes in subposterior sampling, the
DIS) against six popular and state-of-the-art (SOTA)
combined posteriors from all methods except PAI lack
embarrassingly parallel MCMC methods: the paramet-
at least one mode of the posterior (mode collapse, as
ric, non-parametric and semi-parametric methods by
seen in Fig 1A). Other methods also often inappropri-
Neiswanger et al. (2014); PART (Wang et al., 2015);
ately distribute mass in low-density regions (as seen in
and two other GP-surrogate methods (Nemeth and
Fig 1B). In contrast, PAI accurately recovers all the
Sherlock, 2018), one using a simple combination of
high-density regions of the posterior achieving a near-
GPs (GP) and the other using the distributed impor-
perfect match. Table 1 shows that PAI consistently
tance sampler (GP-DIS; see Section 2.2).
outperforms the other methods in terms of metrics.
Procedure. For each problem, we randomly split
the data in equal-sized partitions and divide the tar-
Table 1: Multi-modal posterior.
get posterior into K subposteriors (Eq. 1). We run
Model MMTV W2 GsKL
MCMC separately on each subposterior using Stan
with multiple chains (Carpenter et al., 2017). The
Parametric 0.89
±0.12
1.08
±0.33
8.9 ±11×102
same MCMC output is then processed by the differ-
Semi-param. 0.81
±0.09
1.08
±0.12
5.6 ±1.3×101
Non-param. 0.81
±0.09
1.12
±0.09
5.0 ±1.8×101
ent algorithms described above, yielding a combined PART 0.55
±0.09
1.06
±0.33
7.3 ±14×102
approximate posterior for each method. To assess the GP 0.93
±0.16
1.01
±0.36
1.2 ±1.3×104
quality of each posterior approximation, we compute GP-DIS 0.87 ±0.18 1.04 ±0.34 4.8 ±14×1016
the mean marginal total variation distance (MMTV), PAI 0.037±0.011 0.028±0.011 1.6±1.7×10−4
the 2-Wasserstein (W2) distance, and the Gaussian- PAI-DIS 0.034±0.019 0.026±0.008 3.9±2.4×10−5
ized symmetrized Kullback-Leibler (GsKL) divergence
between the approximate and the true posterior, with Large datasets. To illustrate the computational
each metric focusing on different features. For each benefits of using PAI for larger datasets, we repeated
problem, we computed ground-truth posteriors using the same experiment in this section but with 105 data
numerical integration (for D ≤ 2) or via extensive points in each of the K =10 partitions. Remarkably,
MCMC sampling in Stan (Carpenter et al., 2017). For even for this moderate-sized dataset, we notice a 6×
all GP-based methods (PAI, PAI-DIS, GP, GP-DIS), speed-up – decreasing the total running time from 6
we sampled from the potentially multimodal combined hours to 57 minutes, (50 for subposterior sampling + 7
GP (Eq. 3) using importance sampling/resampling from PAI; see Appendix D.4). Overall, PAI’s running
Daniel Augusto de Souza, Diego Mesquita, Samuel Kaski, Luigi Acerbi
Figure 3: Multi-modal posterior. Each panel shows samples from the combined approximate posterior (red)
against the ground truth (blue). With exception of PAI, all methods completely miss at least one high-density
region. Moreover, PAI is the only method that does not assign mass to regions without modes.
where θ ∈ R2, ν = 5 is the degrees of freedom of
√
the Student’s t-distribution, σ = 1, and σ = 2.
p l
This model is a heavy-tailed variant of the Warped
Gaussian model studied in earlier work, e.g., Nemeth
and Sherlock (2018); Mesquita et al. (2019). As before,
we generate N =103 samples and split the data into
K =10 partitions for parallel inference.
Results. Fig 4 shows the full posterior and the
marginal posterior for θ obtained using the two best-
1
performing methods without DIS refinement, PAI and
GP (see Table 2). While PAI(-DIS) is very similiar to
GP(-DIS) in terms of metrics, Fig 4 shows that, unlike
GP(-DIS), PAI accurately captures the far tails of the
posterior which could be useful for downstream tasks,
avoiding failure mode III (Fig 1C).
Figure 4: Warped Student’s t. Top: Log marginal
posterior for θ . Bottom: Log posterior density.
1
Thanks to active sampling, PAI better captures de- Table 2: Warped Student’s t.
tails in the depths of the tails. Model MMTV W2 GsKL
Parametric 0.51
±0.01
0.71
±0.07
1.9 ±0.1×100
Semi-param. 0.50
±0.03
0.57
±0.05
1.1 ±0.2×101
time is in the same order of magnitude as the previous Non-param. 0.51
±0.02
0.59
±0.03
1.2 ±0.2×101
SOTA (e.g. Wang et al., 2015; Nemeth and Sherlock, PART 0.66 ±0.07 0.78 ±0.09 1.2 ±0.7×102
2018). However, only PAI returns correct results while GP 0.015±0.003 0.003±0.002 4.5 ±10.5×10−4
other methods fail. GP-DIS 0.018 ±0.004 0.002±0.001 6.6 ±5.8×10−5
PAI 0.015±0.003 0.002±0.001 1.2±0.8×10−5
4.2 Warped Student’s t PAI-DIS 0.018 ±0.003 0.002±0.001 3.8 ±3.4×10−5
Setting. We now turn to a synthetic example with
heavy tails. Consider the following hierarchical model: 4.3 Rare categorical events
Setting. To evaluate how our method copes with
θ ∼p(θ)=N(0,σ p 2I 2 ) heterogeneous subposteriors, we run parallel inference
y ,...,y ∼p(y |θ)=StudentT(cid:0) ν,θ +θ2,σ2(cid:1) for a synthetic example with Categorical likelihood
1 N n 1 2 l
Parallel MCMC Without Embarrassing Failures
and N =103 discrete observations split among three performs competitively, achieving a reasonably good
classes. To enforce heterogeneity, we make the first approximationofthetrueposterior(seeAppendixD.3).
two classes rare (true probability θ =θ =1/N) and All the other methods perform considerably worse; in
1 2
the remaining class much more likely (true probability particular the GP method without the DIS step is
θ =(N −2)/N). Since we split the data into K =10 among the worst-performing methods on this example.
3
equal-sized parts, some of them will not contain even
a single rare event. We perform inference over θ ∈∆2
(probability2-simplex)withasymmetricDirichletprior Table 4: Multisensory causal inference.
with concentration parameter α=1/3. Model MMTV W2 GsKL
Results. Fig5showsthesamplesfromthecombined
Parametric 0.40
±0.05
4.8
±0.6
1.2 ±0.4×101
approximate posterior for each method. In this exam-
Semi-param. 0.68
±0.07
9.7
±9.6
5.6 ±3.2×101
Non-param. 0.26
±0.02
0.52
±0.14
5.3 ±0.3×100
ple, PAI-DIS matches the shape of the target posterior
PART 0.24
±0.04
1.5
±0.5
8.0 ±5.4×100
extremely well, followed closely by GP-DIS (see also GP 0.49
±0.25
17
±23
6.3 ±8.9×101
Table 3). Notably, even standard PAI (without the GP-DIS 0.07±0.03 0.16±0.07 8.7 ±14×10−1
DIScorrection)producesaverygoodapproximationof
PAI 0.16
±0.04
0.56
±0.21
2.0 ±1.7×100
theposterior–afurtherdisplayoftheabilityofPAIof
PAI-DIS 0.05±0.04 0.14±0.13 2.9±3.6×10−1
capturing fine details of each subposterior, particularly
important here in the combination step due to the het-
5 RELATED WORKS
erogeneous subposteriors. By contrast, the other meth-
ods end up placing excessive mass in very-low-density While the main staple of embarrassingly parallel
regions (PART, Parametric, GP) or over-concentrating MCMC is being a divide-and-conquer algorithm, there
(Non-parametric, Semi-parametric). are other methods that scale up MCMC using more
intensive communication protocols. For instance, Ahn
Table 3: Rare categorical events. et al. (2014) propose a distributed version of stochatic
gradient Langevin dynamics (SGLD, Welling and Teh,
Model MMTV W2 GsKL
2011) that constantly passes around the chain state to
Parametric 0.26
±0.14
0.15
±0.19
1.1 ±1.4×100
computing nodes, making updates only based on local
Semi-param. 0.49
±0.21
0.27
±0.23
3.5 ±3.4×100
Non-param. 0.43
±0.17
0.19
±0.25
2.8 ±3.9×100 data. However, distributed SGLD tends to diverge
PART 0.31 ±0.14 0.08 ±0.13 8.6 ±10×10−1 from the posterior when the communications are lim-
GP 0.16
±0.09
0.04
±0.07
3.5 ±4.8×10−1
ited, an issue highlighted by recent work (El Mekkaoui
GP-DIS 0.011
±0.002
6.3 ±0.9×10−4 1.1 ±1.5×10−4
et al., 2021; Vono et al., 2022). Outside the realm of
PAI 0.028
±0.027
0.001
±0.002
8.0 ±16×10−3
MCMC, there are also works proposing expectation
PAI-DIS 0.009±0.002 5.4±0.8×10−4 4.3±2.1×10−5
propagationasaframeworkforinferenceonpartitioned
data (Vehtari et al., 2020; Bui et al., 2018).
4.4 Multisensory causal inference
Setting. Causal inference (CI) in multisensory per- Our method, PAI, builds on top of related work on
ception denotes the process whereby the brain decides GP-based surrogate modeling and active learning for
whether distinct sensory cues come from the same log-likelihoods and log-densities. Prior work used GP
source, a commonly studied problem in computational models and active sampling to learn the intractable
andcognitiveneuroscience(Kördingetal.,2007). Here marginal likelihood (Osborne et al., 2012; Gunter
we compute the posterior for a 6-parameter CI model et al., 2014) or the posterior (Kandasamy et al., 2015a;
given the data of subject S1 from (Acerbi et al., 2018) Wang and Li, 2018; Järvenpää et al., 2021). Recently,
(seeAppendixD.3formodeldetails). Thefittedmodel the framework of Variational Bayesian Monte Carlo
isaproxyforalargeclassofsimilarmodelsthatwould (VBMC) was introduced to simultaneously compute
strongly benefit from parallelization due to likelihoods both the posterior and the marginal likelihood (Acerbi,
that do not admit analytical solutions, thus requiring 2018, 2019, 2020). PAI extends the above works by
costly numerical integration. For this experiment, we dealingwithpartitioneddataintheembarrassinglypar-
run parallel inference over K = 5 partitions of the allel setting, similarly to Nemeth and Sherlock (2018),
N =1069 observations in the dataset. but with the key addition of active learning and other
algorithmic improvements.
Results. Table4showstheoutcomemetricsofparal-
lel inference. Similarly to the rare-events example, we
6 DISCUSSION
find that PAI-DIS obtains an excellent approximation
of the true posterior, with GP-DIS performing about In this paper, we first exposed several potential ma-
equally well (slightly worse on the GsKL metric). De- jor failure modes of existing embarrassingly parallel
spite lacking the DIS refinement step, standard PAI MCMC methods. We then proposed a solution with
Daniel Augusto de Souza, Diego Mesquita, Samuel Kaski, Luigi Acerbi
Figure 5: Rare categorical events. Each ternary plot shows samples from the combined approximate posterior
(red) on top of the true posterior (blue). Note that the panels are zoomed in on the relevant corner of the
probability simplex. Of all methods, PAI is the one that best captures the shape of the posterior.
our new method, parallel active inference (PAI), which foractivelearningwouldinvolveBayesiandeeplearning
incorporates two key strategies: sample sharing and techniques (e.g., Maddox et al., 2019).
active learning. On a series of challenging benchmarks,
As discussed before, our approach is not ‘embarrass-
we demonstrated that ‘vanilla’ PAI is competitive with
ingly’ parallel in that it requires a mandatory global
current state-of-the-art parallel MCMC methods and
communication step in the sample sharing part (see
dealssuccessfullywithscenarios(e.g.,multi-modalpos-
Section 3.2). The presence of additional communica-
teriors) in which all other methods catastrophically
tionstepsseeminevitabletoavoidcatastrophicfailures
fail. When paired with an optional refinement step
in parallel MCMC, and has been used before in the
(PAI-DIS), the proposed method consistently performs
literature (e.g., the DIS step of Nemeth and Sherlock,
onparwithorbetterthanstate-of-the-art. Ourresults
2018). Our method affords an optional final refinement
show the promise of the proposed strategies to deal
step (PAI-DIS) which also requires a further global
withthechallengesarisinginparallelMCMC.Still,the
communication step. At the moment, there is no au-
solution is no silver bullet and several aspects remain
tomated diagnostic to determine whether the optional
open for future research.
DIS step is needed. Our results show that PAI already
performs well without DIS in many cases. Still, future
6.1 Limitations and future work
work could include an analysis of the GP surrogate
Themajorlimitationofourmethod,acommonproblem
uncertainty to recommend the DIS step when useful.
to surrogate-based approaches, is scalability to higher
dimensions. Most GP-based approaches for Bayesian
Acknowledgments
posteriorinferencearelimitedtoupto∼10dimensions,
see e.g. Acerbi, 2018, 2020; Järvenpää et al., 2021. This work was supported by the Academy of Finland
Future work could investigate methods to scale GP (Flagship programme: Finnish Center for Artificial
surrogate modeling to higher dimensions, for example Intelligence FCAI and grants 328400, 325572) and
takinginspirationfromhigh-dimensionalapproachesin UKRI (Turing AI World-Leading Researcher Fellow-
Bayesian optimization (e.g., Kandasamy et al., 2015b). ship, EP/W002973/1). We also acknowledge the com-
putational resources provided by the Aalto Science-IT
More generally, the validity of any surrogate modeling
Project from Computer Science IT.
approachhingesontheabilityofthesurrogatemodelto
faithfully represent the subposteriors. Active learning
References
helps, but model mismatch in our method is still a
L. Acerbi. Variational Bayesian Monte Carlo. In Advances
potential issue that hints at future work combining
in Neural Information Processing Systems (NeurIPS),
PAI with more flexible surrogates such as GPs with
2018.
more flexible kernels (Wilson and Adams, 2013) or
L.Acerbi.VariationalBayesianMonteCarlowithnoisylike-
deep neural networks (Mesquita et al., 2019). For the
lihoods. In Advances in Neural Information Processing
latter, obtaining the uncertainty estimates necessary Systems (NeurIPS), 2020.
Parallel MCMC Without Embarrassing Failures
L. Acerbi, K. Dokka, D. E. Angelaki, and W. J. Ma. Konrad P Körding, Ulrik Beierholm, Wei Ji Ma, Steven
Bayesiancomparisonofexplicitandimplicitcausalinfer- Quartz,JoshuaBTenenbaum,andLadanShams. Causal
encestrategiesinmultisensoryheadingperception. PLoS inference in multisensory perception. PLoS one, 2(9):
Computational Biology, 14(7):e1006110, 2018. e943, 2007.
Luigi Acerbi. An exploration of acquisition and mean func- Wesley J Maddox, Pavel Izmailov, Timur Garipov,
tions in Variational Bayesian Monte Carlo. Proceed- Dmitry P Vetrov, and Andrew Gordon Wilson. A sim-
ings of The 1st Symposium on Advances in Approximate ple baseline for Bayesian uncertainty in deep learning.
Bayesian Inference (PMLR), 96:1–10, 2019. In Advances in Neural Information Processing Systems
(NeurIPS), volume 32, pages 13153–13164, 2019.
S.Ahn,B.Shahbaba,andM.Welling. Distributedstochas-
tic gradient MCMC. In International Conference on Wesley J Maddox, Sanyam Kapoor, and Andrew Gordon
Machine Learning (ICML), 2014. Wilson. When are iterative Gaussian processes reliably
accurate? 2021.
E. Angelino, M. J. Johnson, and R. P. Adams. Patterns of
scalable Bayesian inference. Foundations and Trends in D. Mesquita, P. Blomstedt, and S. Kaski. Embarrassingly
Machine Learning, 9(2-3):119–247, 2016. parallelMCMCusingdeepinvertibletransformations. In
Uncertainty in Artificial Intelligence (UAI), 2019.
M. Balandat, B. Karrer, D. Jiang, S. Daulton, B. Letham,
A.Wilson,andE.Bakshy.BoTorch: AFrameworkforEf- W. Neiswanger, C. Wang, and E. P. Xing. Asymptotically
ficient Monte-Carlo Bayesian Optimization. In Advances exact, embarrassingly parallel MCMC. In Uncertainty
in Neural Information Processing Systems (NeurIPS), in Artificial Intelligence (UAI), 2014.
2020.
C. Nemeth and C. Sherlock. Merging MCMC subposteri-
T.Bui,C.Nguyen,S.Swaroop,andR.Turner. Partitioned ors through Gaussian-process approximations. Bayesian
variational inference: A unified framework encompass- Analysis, 13(2):507–530, 2018.
ing federated and continual learning. ArXiv:1811.11206,
Michael Osborne, David K Duvenaud, Roman Garnett,
2018.
Carl E Rasmussen, Stephen J Roberts, and Zoubin
B.Carpenter,A.Gelman,M.Hoffman,D.Lee,B.Goodrich, Ghahramani. Active learning of model evidence using
M. Betancourt, M. Brubaker, J. Guo, P. Li, and A. Rid- Bayesian quadrature. Advances in Neural Information
dell. Stan: A probabilistic programming language. Jour- Processing Systems, 25:46–54, 2012.
nal of Statistical Software, 76(1), 2017.
H. Park and C. Jun. A simple and fast algorithm for k-
K. El Mekkaoui, D. Mesquita, P. Blomstedt, and S. Kaski. medoidsclustering. Expert Systems with Applications,36
Federated stochastic gradient Langevin dynamics. In (2):3336–3341, March 2009. ISSN 0957-4174.
Uncertainty in Artificial Intelligence (UAI), 2021.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
J.Gardner,G.Pleiss,D.Bindel,K.Weinberger,andA.Wil- James Bradbury, Gregory Chanan, Trevor Killeen, Zem-
son.Gpytorch: Blackboxmatrix-matrixGaussianprocess ingLin,NataliaGimelshein,LucaAntiga,etal. Pytorch:
inference with GPU acceleration. In Advances in Neural An imperative style, high-performance deep learning li-
Information Processing Systems (NeurIPS), 2018. brary. Advances in Neural Information Processing Sys-
tems, 32, 2019.
A. Gelman, J. B. Carlin, H. S. Stern, D. B. Dunson, A. Ve-
htari, and D. B. Rubin. Bayesian Data Analysis. CRC C. Rasmussen and C. K. I. Williams. Gaussian Processes
press, 2013. for Machine Learning. The MIT Press, 2006.
J. Görtler, R. Kehlbeck, and O. Deussen. A visual explo- C.RobertandG.Casella. Monte Carlo Statistical Methods.
ration of Gaussian processes. Distill, 4(4):e17, 2019. Springer Science & Business Media, 2013.
Tom Gunter, Michael A Osborne, Roman Garnett, Philipp C.P.Robert,V.Elvira,N.Tawn,andC.Wu. Accelerating
Hennig, and Stephen J Roberts. Sampling for inference MCMC algorithms. Wiley Interdisciplinary Reviews:
in probabilistic models with fast Bayesian quadrature. Computational Statistics, 10(5):e1435, 2018.
Advances in Neural Information Processing Systems, 27:
S. L. Scott, A. W. Blocker, F. V. Bonassi, H. A. Chip-
2789–2797, 2014.
man, E. I. George, and R. E. McCulloch. Bayes and big
M. D. Hoffman and A. Gelman. The No-U-Turn sampler: data: The consensus Monte Carlo algorithm. Interna-
adaptively setting path lengths in Hamiltonian Monte tional Journal of Management Science and Engineering
Carlo. Journal of Machine Learning Research, 15(1): Management, 11:78–88, 2016.
1593–1623, 2014.
M. Titsias. Variational learning of inducing variables in
M. Järvenpää, M. U. Gutmann, A. Vehtari, and P. Martti- sparse Gaussian processes. In Artificial intelligence and
nen. Parallel Gaussian process surrogate Bayesian infer- statistics, pages 567–574. PMLR, 2009.
encewithnoisylikelihoodevaluations.BayesianAnalysis,
A. Vehtari, A. Gelman, T. Sivula, P. Jylänki, D. Tran,
16(1):147–178, 2021.
S. Sahai, P. Blomstedt, J. P. Cunningham, D. Schimi-
K.Kandasamy,J.Schneider,andB.Póczos.Bayesianactive novich, and C. P. Robert. Expectation propagation as
learning for posterior estimation. In Proceedings of the a way of life: A framework for Bayesian inference on
24th International Conference on Artificial Intelligence, partitioneddata. Journal of Machine Learning Research,
pages 3605–3611, 2015a. 21(17):1–53, 2020.
KirthevasanKandasamy,JeffSchneider,andBarnabásPóc- M. Vono, V. Plassier, A. Durmus, A. Dieuleveut, and
zos. HighdimensionalBayesianoptimisationandbandits E. Moulines. QLSD: Quantised Langevin Stochastic
via additive models. In International Conference on Ma- Dynamics for Bayesian federated learning. In Artificial
chine Learning (ICML), pages 295–304. PMLR, 2015b. Intelligence and Statistics (AISTATS), 2022.
Daniel Augusto de Souza, Diego Mesquita, Samuel Kaski, Luigi Acerbi
Hongqiao Wang and Jinglai Li. Adaptive Gaussian pro-
cessapproximationforBayesianinferencewithexpensive
likelihood functions. Neural Computation, pages 1–23,
2018.
K.A.Wang,G.Pleiss,J.R.Gardner,S.Tyree,K.Q.Wein-
berger, and A. G. Wilson. Exact Gaussian processes on
amilliondatapoints. InAdvancesinNeuralInformation
Processing Systems (NeurIPS), 2019.
X. Wang, F. Guo, K. A. Heller, and D. B. Dunson. Paral-
lelizingMCMCwithrandompartitiontrees. InAdvances
in Neural Information Processing Systems (NeurIPS),
2015.
M. Welling and Y. Teh. Bayesian learning via stochastic
gradientLangevindynamics. InInternationalConference
on Machine Learning (ICML), 2011.
A. Wilson and R. Adams. Gaussian process kernels for
pattern discovery and extrapolation. In International
Conference on Machine Learning (ICML), pages 1067–
1075. PMLR, 2013.
Supplementary Material:
Parallel MCMC Without Embarrassing Failures
In this Supplement, we include extended explanations, implementation details, and additional results omitted
from the main text.
Code for our algorithm and to generate the results in the paper is available at: https://github.com/
spectraldani/pai.
A Failure modes of embarrassingly parallel MCMC explained
In Section2.1 of the main text we presented three major failure modes of embarrassingly parallelMCMC (Markov
Chain Monte Carlo) methods. These failure modes are illustrated in Fig 1 in the main text. In this section, we
further explain these failure types going through Fig 1 in detail.
A.1 Mode collapse (Fig 1A)
Fig 1A illustrates mode collapse. In this example, the true subposteriors p and p both have two modes (see
1 2
Fig 1A, top two panels). However, while in p the two modes are relatively close to each other, in p they are
1 2
farther apart. Thus, when we run an MCMC chain on p , it gets stuck into a single high-density region and is
2
unable to jump to the other mode (‘unexplored mode’, shaded area). This poor exploration of p is fatal to any
2
standard combination strategy used in parallel MCMC, erasing entire regions of the posterior (Fig 1A, bottom
panel). While in this example we use PART (Wang et al., 2015), Section 4.1 shows this common pathology in
several other methods as well. Our proposed solution to this failure type is sample sharing (see Section C.2).
A.2 Model mismatch (Fig 1B)
Fig 1B draws attention to model mismatch in subposterior surrogates. In this example, MCMC runs smoothly
on both subposteriors. However, when we fit regression-based surrogates – here, Gaussian processes (GPs) –
to the MCMC samples, the behavior of these models away from subposterior samples can be unpredictable.
In our example, the surrogate q hallucinates a mode that does not exist in the true subposterior p (‘model
2 2
hallucination’, shadedareainFig1B).Ourproposedsolutiontocorrectthispotentialissueistoexploreuncertain
areas of the surrogate using active subposterior sampling (see Section C.3).
In this example, we used a simple GP surrogate approach for illustration purposes. The more sophisticated
GP-based Distributed Importance Sampler (GP-DIS; Nemeth and Sherlock (2018)) might seem to provide an
alternative solution to model hallucination, in that the hallucinated regions with low true density would be
down-weighted by importance sampling. However, the DIS step only works if there are ‘good’ samples that cover
regions with high true density that can be up-weighted. If no such samples are present, importance sampling
will not work. As an example of this failure, Fig 3 in the main text shows that GP-DIS (Nemeth and Sherlock,
2018) does not recover from the model hallucination (if anything, the importance sampling step concentrates the
hallucinated posterior even more).
A.3 Underrepresented tails (Fig 1C)
Fig 1C shows how neglecting low-density regions (underrepresented tails) can affect the performance of parallel
MCMC. In the example, the true subposterior p has long tails that are not thoroughly represented by MCMC
2
samples. This under-representation is due both to actual difficulty of the MCMC sampler in exploring the tails,
and to mere Monte Carlo noise as there is little (although non-zero) mass in the tails, so the number of samples
is low. Consequently, the surrogate q is likely to underestimate the density in this unexplored region, in which
2
the other subposterior p has considerable mass. In this case, even though q is a perfect fit to p , the product
1 1 1
q (θ)q (θ)mistakenlyattributesnear-zerodensitytothecombinedposteriorinsaidregion(Fig1C,bottompanel).
1 2
In our method, we address this issue via multiple solutions, in that both sample sharing and active sampling
would help uncover underrepresentation of the tails in relevant regions.
Daniel Augusto de Souza, Diego Mesquita, Samuel Kaski, Luigi Acerbi
For further illustration of the effectiveness of our proposed solutions to these failure modes, see Section D.1.
B Gaussian processes
Gaussian processes (GPs) are a flexible class of statistical models for specifying prior distributions over unknown
functions f :X ⊆RD →R (Rasmussen and Williams, 2006). In this section, we describe the GP model used in
the paper and details of the training procedure.
B.1 Gaussian process model
In the paper, we use GPs as surrogate models for log-posteriors (and log-subposteriors), for which we use the
followingmodel. WerecallthatGPsaredefinedbyapositivedefinitecovariance,orkernelfunctionκ:X×X →R;
a mean function m:X →R; and a likelihood or observation model.
Kernel function. For simplicity, we use the common and equivalently-named squared exponential, Gaussian,
or exponentiated quadratic kernel,
(cid:20) 1 (cid:21)
κ(x,x0;σ2,‘ ,...,‘ )=σ2exp − (x−x0)Σ−1(x−x0)> withΣ =diag(cid:2) ‘2,...,‘2 (cid:3) , (S1)
f 1 D f 2 ‘ ‘ 1 D
where σ is the output length scale and (‘ ,...,‘ ) is the vector of input length scales. Our algorithm does not
f 1 D
hinge on choosing this specific kernel and more appropriate kernels might be used depending on the application
(e.g., the spectral mixture kernel might provide more flexibility; see Wilson and Adams, 2013).
Mean function. WhenusingGPsassurrogatemodelsforlog-posteriordistributions, itiscommontoassumea
negative quadratic meanfunctionsuchthatthesurrogateposterior(i.e., theexponentiatedsurrogatelog-posterior)
is integrable (Nemeth and Sherlock, 2018; Acerbi, 2018, 2019, 2020). A negative quadratic can be interpreted as
a prior assumption that the target posterior is a multivariate normal; but note that the GP can model deviations
from this assumption and represent multimodal distributions as well (see for example Fig 3 in the main text). In
this paper, we use
m(x;m ,µ ,...,µ ,ω ,...,ω )≡m − 1 X D (x i −µ i )2 , (S2)
0 1 D 1 D 0 2 ω2
i=1 i
where m denotes the maximum, (µ ,...,µ ) is the location vector, and (ω ,...,ω ) is a vector of length scales.
0 1 D 1 D
Observationmodel. Finally,GPsarealsocharacterizedbyalikelihoodorobservationnoisemodel. Throughout
the paper we assume exact observations of the target log-posterior (or log-subposterior) so we use a Gaussian
likelihood with a small variance σ2 =10−3 for numerical stability.
B.2 Gaussian process inference and training
Inference. Conditioned on training inputs X = {x ,...,x }, observed function values y = f(X) and GP
1 N
hyperparameters ψ, the posterior GP mean and covariance are available in closed form (Rasmussen and Williams,
2006),
f (x)≡E[f(x)|X,y,ψ]=κ(x,X)(cid:2) κ(X,X)+σ2I (cid:3)−1(y−m(X))+m(x)
X,y D (S3)
C (x,x0)≡Cov[f(x),f(x0)|X,y,ψ]=κ(x,x0)−κ(x,X)(cid:2) κ(X,X)++σ2I (cid:3)−1 κ(X,x0),
X,y D
where ψ is a hyperparameter vector for the GP mean, covariance, and likelihood (see Section B.1 above); and I
D
is the identity matrix in D dimensions.
Training. TrainingaGPmeansfindingthehyperparametervector(s)thatbestrepresentagivendatasetofinput
points and function observations (X,y). In this paper, we train all GP models by maximizing the log marginal
likelihood of the GP plus a log-prior term that acts as regularizer, a procedure known as maximum-a-posteriori
estimation. Thus, the training objective to maximize is
1
logp(ψ|X,y)=− (y−m(X;ψ))>(cid:2) κ(X,X;ψ)+σ2I (cid:3)−1(y−m(X;ψ))
2 D
(S4)
1
+ logdet(cid:0) κ(X,X;ψ)+σ2I (cid:1)+logp(ψ)+const,
2 D
where p(ψ) is the prior over GP hyperparameters, described below.
Parallel MCMC Without Embarrassing Failures
Hyperparameter Description Prior distribution
logσ2 Output scale —
f
(cid:16) q √ (cid:17)
log‘(i) Input length scale LogN log DL(i),log 103
6
m Mean function maximum SmoothBox(y ,y ,1.0)
0 min max
(cid:16) (cid:17)
x(i) Mean function location SmoothBox B(i) ,B(i) ,0.01
m min max
(cid:16) q √ (cid:17)
logω(i) Mean function scale LogN log DL(i),log 103
6
Table S1: Priors over GP hyperparameters. See text for more details
Priors. We report the prior p(ψ) over GP hyperparameters in Table S1, assuming independent priors over each
hyperparameter and dimension 1≤i≤D. We set the priors based on broad characteristics of the training set,
an approach known as empirical Bayes which can be seen as an approximation to a hierarchical Bayesian model.
In the table, B is the ‘bounding box’ defined as the D-dimensional box including all the samples observed by the
GP so far plus a 10% margin; L=B −B is the vector of lengths of the bounding box; and y and y
max min max min
are, respectively, the largest and smallest observed function values of the GP training set. LogN (µ,σ) denotes
the log-normal distribution and SmoothBox(a,b,σ) is defined as a uniform distribution on the interval [a,b] with
a Gaussian tail with standard deviation σ outside the interval. If a distribution is not specified, we assumed a flat
prior.
B.3 Implementation details
All GP models in the paper are implemented using GPyTorch1 (Gardner et al., 2018), a modern package for
GP modeling based on the PyTorch machine learning framework (Paszke et al., 2019). For maximal accuracy,
we performed GP computations enforcing exact inference via Cholesky decomposition, as opposed to the
asymptotically faster conjugate gradient implementation which however might not reliably converge to the exact
solution (Maddox et al., 2021). For parts related to active sampling, we used the BoTorch2 package for active
learning with GPs (Balandat et al., 2020), implementing the acquisition functions used in the paper as needed.
We used the same GP model and training procedure described in this section for all GP-based methods in the
paper, which include our proposed approach and others (e.g., Nemeth and Sherlock, 2018).
C Algorithm details
Algorithm S1 Parallel Active Inference (PAI)
Input: Data partitions D ,...,D ; prior p(θ); likelihood function p(D|θ).
1 K
1: parfor 1...K do . Parallel steps
2: S ← MCMC samples from p (θ)∝p(θ)1/Kp(D |θ)
k k k
3: S0 ← ActiveSubsample(S ) . See Sections 3.1 and C.1
4: se k nd S0 to all other nodes, r k eceive S0 = S S0 . Communication step
k \k j6=k j
5: S00 ← S0 ∪ SelectSharedSamples(S0,S0 ) . See Sections 3.2 and C.2
k k k \k
6: S000 ← S00∪ ActiveRefinement(S00) . See Sections 3.3 and C.3
k k k
7: train GP model L of the log subposterior on (S000,logp (S000))
k k k k
8: end parfor
9: combine subposteriors: logq(θ)=
PK
L (θ) . Centralized step, see Section 3.4
k=1 k
10: Optional: refine logq(θ) with Distributed Importance Sampling (DIS) . See Section 2.2
In this section, we describe additional implementation details for the steps of the Parallel Active Inference (PAI)
algorithm introduced in the main text. Algorithm S1 illustrates the various steps. Each function called by the
algorithm may involve several sub-steps (e.g., fitting interim surrogate GP models) which are detailed in the
following sections.
1https://gpytorch.ai/
2https://botorch.org/
Daniel Augusto de Souza, Diego Mesquita, Samuel Kaski, Luigi Acerbi
C.1 Subposterior modeling via GP regression
Here we expand on Section 3.1 in the main text. We recall that at this step each node k ∈{1,...,K} has run
MCMC on the local subposterior p , obtaining a set of samples S and their log-subposterior values logp (S).
k k k
The goal now is to ‘thin’ the samples to a subset which is still very informative of the shape of the subposterior,
so that it can be used as a training set for the GP surrogate. The rationale is that using all the samples for GP
training is expensive in terms of both computational and communication costs, and it can lead to numerical
instabilities. In previous work, Nemeth and Sherlock (2018) have used random thinning which is not guaranteed
to keep relevant parts of the posterior.3
The main details that we cover here are: (1) how we pick the initial subset of samples S(0) ⊆S used to train the
k k
initial GP surrogate; (2) how we subsequently perform active subsampling to expand the initial subset to include
relevant points from S .
k
Initial subset: To bootstrap the GP surrogate model, we use a distance clustering method to select the
initial subset of samples that we use to fit a first GP model. As this initial subset will be further refined, the
main characteristic for selecting the samples is for them to be spread out. For our experiments, we choose
a simple k-medoids method (Park and Jun, 2009) with n = 20 · (D + 2) medoids implemented in the
med
scikit-learn-extras library4. The output n medoids represent S(0).
med k
Activesubsampling: AfterhavingselectedtheinitialsubsetS(0),weperformT iterationsofactivesubsampling.
k
In each iteration t+1, we greedily select a batch of n points from the set S \S(t) obtained by maximizing a
batch k k
batch version of the maximum interquantile range (MAXIQR) acquisition function, as described by Järvenpää
et al. (2021); see also main text. The MAXIQR acquisition function has a parameter u which controls the tradeoff
betweenexploitationofregionsofhighposteriordensityandexplorationofregionswithhighposterioruncertainty
in the GP surrogate. To strongly promote exploration, after a preliminary analysis on a few toy problems, we set
u = 20 throughout the experiments presented in the paper. In the paper, we set n = D and the number
batch
of iterations T = 25, based on a rule of thumb for the total budget of samples required by similar algorithms
to achieve good performance at a given dimension (e.g., Acerbi 2018; Järvenpää et al. 2021). The GP model is
retrained after the acquisition of each batch. The procedure locally returns a subset of samples S0 =S(T).
k k
C.2 Sample sharing
In this section, we expand on Section 3.2 of the main text. We recall that at this step node k receives samples
S0 =S S0 fromtheothersubposteriors. Toavoidincorporatingtoomanydatapointsintothelocalsurrogate
\k j6=k j
model (for the reasons explained previously), we consider adding a data point to the current surrogate only if: (a)
the local model cannot properly predict this additional point; and (b) predicting the exact value would make a
difference. If the number of points that are eligible under these criteria is greater than n , the set is further
share
thinned using k-medoids.
Concretely, let θ? ∈S0 be the data point under consideration. We evaluate the true subposterior log density
\k
at the point, y? =logp (θ?), and the surrogate GP posterior latent mean and variance at the point, which are,
k
respectively, µ =f(θ ) and σ2 =C(θ?,θ?). We then consider two criteria:
? ? ?
a. First, we compute the density of the true value under the surrogate prediction and check if it is above a
certain threshold: N (cid:0)logy?;µ ,σ2(cid:1) >R, where R=0.01 in this paper. This criterion is roughly equivalent
? ?
to including a point only if |µ −y?|(cid:38)R0σ?, for an appropriate choice of R0 (ignoring a sublinear term in
?
σ?), implying that a point is considered for addition if the GP prediction differs from the actual value more
than a certain number of standard deviations.
b. Second, if a point meets the first criterion, we check if the value of the point is actually relevant for the
surrogate model. Let y be the maximum subposterior log-density observed at the current node (i.e.,
max
approximately, the log-density at the mode). We exclude a point at this stage if both the GP prediction and
the true value y? are below the threshold y −20D, meaning that the point has very low density and the
max
GP correctly predicts that it is very low density (although might not predict the exact value).
3Note that instead of thinning we could use sparse GP approximations (e.g., Titsias, 2009). However, the interaction
between sparse GP approximations and active learning is not well-understood. Moreover, we prefer not to introduce an
additional layer of approximation that could reduce the accuracy of our subposterior surrogate models.
4https://github.com/scikit-learn-contrib/scikit-learn-extra
Parallel MCMC Without Embarrassing Failures
Each of the above criteria is checked for all points in S0 in parallel (the second criterion only for all the points
\k
that pass the first one). Note that the second criterion is optional, but in our experiments we found that it
increases numerical stability of the GP model, removing points with very low (log) density that are difficult for
the GP to handle (for example, Acerbi (2018); Järvenpää et al. (2021) adopt a similar strategy of discarding very
low-density points). More robust GP kernels (see Section B.1) might not need this additional step.
If the number of points that pass both criteria for inclusion is larger than n , then k-medoids is run on the set
share
of points under consideration with n medoids, where n = 25D. The procedure run at node k locally
share share
returns a subset of samples S00.
k
C.3 Active subposterior refinement
Here we expand on Section 3.3 of the main text. We recall that up to this point, the local GP model of the
log-subposterior at node k was trained only using selected subset of samples from the original MCMC runs (local
and from other nodes), denoted by S00.
k
In this step, each node k locally acquires new points by iteratively optimizing the MAXIQR acquisition function
(Eq. 2 in the main text) over a domain X ⊆RD. For the first iteration, the space X is defined as the bounding
box of S S0 plus a 10% margin. In other words, X is initially the hypercube that contains all samples from all
k k
subposteriors obtained in the sample sharing step (Section C.2) extended by a 10% margin. The limits of X
at the first iteration are computed during the previous stage, without any additional communication cost. In
each subsequent iteration, X is iteratively extended to include the newly sampled points plus a 10% margin, thus
expanding the bounding box if a recently acquired point falls near the boundary.
New points are selected greedily in batches of size n =D, using the same batch formulation of the MAXIQR
batch
acquisition function used in Section C.1. The local GP surrogate is retrained at the end of each iteration, to
ensure that the next batch of points targets the regions of the log-subposterior which are most important to
further refine the surrogate. For the purpose of this work, we repeat the process for T = 25 iterations,
active
selected based on similar active learning algorithms (Acerbi et al., 2018; Järvenpää et al., 2021). Future work
should investigate an appropriate termination rule to dynamically vary the number of iterations.
The outcome of this step is a final local set of samples S000 and the log-subposterior GP surrogate model L
k k
trained on these samples. Both of these are sent back to the central node for the final combination step.
D Experiment details and additional results
In this section, we report additional results and experimental details omitted from the main text for reasons of
space.
D.1 Ablation study
As an ablation study, Fig S1 breaks down the effect of each step of PAI in the multi-modal posterior experiment
from Section 4.1 of the main text. The first panel shows the full approximate posterior if we were combining
it right after active subsampling (Section C.1), using neither sample sharing nor active refinement. Note that
this result suffers from Failure mode I (mode collapse; see Section A.1), as active subsampling only on the local
MCMC samples is not sufficient to recover the missing modes. The second panel incorporates sample sharing,
which covers the missing regions but now suffers from Failure mode II (model mismatch; see Section A.2) with an
hallucinated mode in a region where the true posterior has low density. Finally, the third panel shows full-fledged
PAI, which further applies active sampling to explore the hallucinated mode and corrects the density around it.
The final result of PAI perfectly matches the ground truth (as displayed in the fourth panel).
D.2 Performance evaluation
In this section, we describe in detail the metrics used to assess the performance of the methods in the main text,
how we compute these metrics, and the related statistical analyses for reporting our results.
Metrics. In the main text, we measured the quality of the posterior approximations via the mean marginal
total variation distance (MMTV), 2-Wasserstein (W2) distance, and Gaussianized symmetrized Kullback-Leibler
divergence (GsKL) between true and appoximate posteriors. For all metrics, lower is better. We describe the
three metrics and their features below:
Daniel Augusto de Souza, Diego Mesquita, Samuel Kaski, Luigi Acerbi
Figure S1: Ablation study for PAI on the multi-modal posterior. From left to right: The first panel
shows the approximate combined posterior density for our method without sample sharing and active learning;
the second panel uses an additional step to share samples (but no active refinement); the third shows results for
full-fledged PAI. The rightmost plot is the ground truth posterior. Note that both sample sharing and active
refinement are important steps for PAI: sample sharing helps account for missing posterior regions while active
sampling corrects model hallucinations (see text for details).
• The MMTV quantifies the (lack of) overlap between true and approximate posterior marginals, defined as
1 D Z ∞
MMTV(p,q)=
2D
X (cid:12) (cid:12)pM
d
(x
d
)−q
d
M(x
d
)(cid:12) (cid:12)dx
d
(S5)
d=1 −∞
where pM and qM denote the marginal densities of p and q along the d-th dimension. Eq. S5 has a direct
d d
interpretation in that, for example, a MMTV metric of 0.5 implies that the posterior marginals overlap by
50% (on average across dimensions). As a rule of thumb, we consider a threshold for a reasonable posterior
approximation to be MMTV < 0.2, that is more than 80% overlap.
• Wasserstein distances measure the cost of moving amounts of probability mass from one distribution to
the other so that they perfectly match – a commonly-used distance metric across distributions. The W2
metric, alsoknownasearth mover’s distance, isaspecialcaseofWassersteindistancethatusestheEuclidean
distance as its cost function. The W2 distance between two density functions p and q, with respective
supports X and Y is given by
(cid:20) Z Z (cid:21)1
2
W2(p,q)= inf kx−yk T(x,y)dxdy , (S6)
2
T∈T x∈X y∈Y
where T denotes the set of all joint density functions over X ×Y with marginals exactly p and q. In practice,
we use empirical approximations of p and q to compute the W2, which simplifies Eq. S6 to a linear program.
• The GsKL metric is sensitive to differences in means and covariances, being defined as
1
GsKL(p,q)= [D (N[p]||N[q])+D (N[q]||N[p])], (S7)
2 KL KL
where D (p||q) is the Kullback-Leibler divergence between distributions p and q and N[p] is a multivariate
KL
normal distribution with mean equal to the mean of p and covariance matrix equal to the covariance of p
(and same for q). Eq. S7 can be expressed in closed form in terms of the means and covariance matrices of
√
p and q. For reference, two Gaussians with unit variance and whose means differ by 2 (resp., 1) have a
2
GsKL of 1 (resp., 1). As a rule of thumb, we consider a desirable target to be (much) less than 1.
8
Computing the metrics. For each method, we computed the metrics based on samples from the combined
approximate posteriors. For methods whose approximate posterior is a surrogate GP model (i.e., GP, GP-DIS,
PAI, PAI-DIS in the paper), we drew samples from the surrogate model using importance sampling/resampling
(Robert and Casella, 2013). As proposal distribution for importance sampling we used a mixture of a uniform
distribution over a large hyper-rectangle and a distribution centered on the region of high posterior density (the
latter to increase the precision of our estimates). We verified that our results did not depend on the specific
choice of proposal distribution.
Parallel MCMC Without Embarrassing Failures
Statistical analyses. For each problem and each method, we reran the entire parallel inference procedure ten
times with ten different random seeds. The same ten seeds were used for all methods – implying among other
thingsthat,foreachproblem,allmethodsweretestedonthesametenrandompartitionsofthedataandusingthe
same MCMC samples on those partitions. The outcome of each run is a triplet of metrics (MMTV, W2, GsKL)
with respect to ground truth. We computed mean and standard deviation of the metrics across the ten runs,
which are reported in tables in the main text. For each problem and metric, we highlighted in bold all methods
whose mean performance does not differ in a statistically significant way from the best-performing method.
Since the metrics are not normally distributed, we tested statistical significance via bootstrap (n =106
bootstrap
bootstrapped datasets) with a threshold for statistical significance of α=0.05.
D.3 Model details and further plots
We report here additional details for some of the models used in the experiments in the main paper, and plots for
the experiment from computational neuroscience.
Multi-modal posterior. In Section 4.1 of the main text we constructed a synthetic multi-modal posterior
with four modes. We recall that the generative model is
θ ∼p(θ)=N(0,σ2I )
p 2
2 1
y ,...,y ∼p(y |θ)= X N (cid:0) y ;P (θ ),σ2(cid:1)
1 N n 2 n i i l
i=1
where θ ∈ R2, σ = σ = 1/4 and P ’s are second-degree polynomial functions. To induce a posterior with
p l i
four modes, we chose P (θ ) and P (θ ) to be polynomials with exactly two roots, such that, when the ob-
1 1 2 2
servations are drawn from the full generative model in Eq. D.3, each root will induce a local maximum of
the posterior in the vicinity of the root (after considering the shrinkage effect of the prior). The polynomi-
als are defined as P (x) = P (x) = (0.6 − x)(−0.6 − x), so the posterior modes will be in the vicinity of
1 2
θ? ∈{(0.6,0.6),(−0.6,0.6),(0.6,−0.6),(−0.6,−0.6)}.
Multisensory causal inference. In Section 4.4 of the main text, we modeled a benchmark visuo-vestibular
causal inference experiment (Acerbi et al., 2018; Acerbi, 2020) which is representative of many similar models and
tasksinthefieldsofcomputationalandcognitiveneuroscience. Inthemodeledexperiment,humansubjects,sitting
in a moving chair, were asked in each trial whether the direction of movement s matched the direction s of a
vest vis
looming visual field. We assume subjects only have access to noisy sensory measurements z ∼N (cid:0) s ,σ2 (cid:1),
vest vest vest
z ∼N (cid:0) s ,σ2 (c)(cid:1),whereσ isthevestibularnoiseandσ (c)isthevisualnoise,withc∈{c ,c ,c }
vis vis vis vest vis low med high
distinct levels of visual coherence adopted in the experiment. We model subjects’ responses with a heuristic
‘Fixed’ rule that judges the source to be the same if |z −z |<κ, plus a probability λ of giving a random
vis vest
response (Acerbi et al., 2018). Model parameters are θ =(σ ,σ (c ),σ (c ),σ (c ),κ,λ), nonlinearly
vest vis low vis med vis high
mapped to R6. In the paper, we fit real data from subject S1 of (Acerbi et al., 2018). Example approximate
posteriors for the PAI-DIS and GP-DIS methods, the best-performing algorithms in this example, are shown in
Fig S2.
D.4 Scalability of PAI to large datasets
In the main paper, as per common practice in the field, we used moderate dataset sizes (∼10k data points) to
easily calculate ground truth. This choice bears no loss of generality because increasing dataset size only makes
subposteriors sharper, which does not increase the difficulty of parallel inference (although more data would not
necessarily resolve multimodality, e.g. due to symmetries of the model). On the other hand, small datasets make
the reporting of run-times not meaningful, as they are dominated by overheads.
To assess the performance of PAI on large datasets, we ran PAI on the model of Section 4.1, but now with 1
million data points (K = 10 partitions). Average metrics for PAI were excellent and similar to what we had
before: MTV = 0.009, W2 = 0.005, GKL = 2e-05, while all other methods still failed. Moreover, run-times for
this experiment illustrate the advantages of using PAI in practice.
We ran experiments using computers equipped with two 8-core Xeon E5 processors and 16GB or RAM each.
Here, the total time for parallel inference was 57 minutes — 50 for subposterior MCMC sampling + 7 for all PAI
steps. By contrast, directly running MCMC on the whole dataset took roughly 6 hours.
Daniel Augusto de Souza, Diego Mesquita, Samuel Kaski, Luigi Acerbi
Figure S2: PAI-DIS and GP-DIS on the multisensory causal inference task. Each panel shows two-
dimensional posterior marginals as samples from the combined approximate posterior (red) against the ground
truth (blue). While PAI-DIS (top figure) and GP-DIS (bottom figure) perform similarly in terms of metrics,
PAI-DIS captures some features of the posterior shape more accurately, such as the ‘boomerang’ shape of the θ
3
marginals (middle row).

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Parallel MCMC Without Embarrassing Failures"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
