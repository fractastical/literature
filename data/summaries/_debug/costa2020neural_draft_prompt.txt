=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Neural dynamics under active inference: plausibility and efficiency of information processing
Citation Key: costa2020neural
Authors: Lancelot Da Costa, Thomas Parr, Biswa Sengupta

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2020

Abstract: Active inference is a normative framework for explaining behaviour under the free energy
principle – a theory of self-organisation originating in neuroscience. It specifies neuronal
dynamics for state-estimation in terms of a descent on (variational) free energy – a measure
of the fit between an internal (generative) model and sensory observations. The free energy
gradient is a prediction error – plausibly encoded in the average membrane potentials of
neuronal populations. Conversely, the expect...

Key Terms: dynamics, processing, descent, neural, information, efficiency, london, inference, neuronal, active

=== FULL PAPER TEXT ===

Neural dynamics under active inference
Neural dynamics under active inference:
plausibility and efficiency of information
processing
Lancelot Da Costa1,2, Thomas Parr2, Biswa Sengupta2,3,4, Karl Friston2
1Department of Mathematics, Imperial College London
2Wellcome Centre for Human Neuroimaging, Institute of Neurology,
University College London
3Core Machine Learning group, Zebra AI, London
4Department of Bioengineering, Imperial College London
Correspondence: Lancelot Da Costa l.da-costa@imperial.ac.uk
Abstract
Active inference is a normative framework for explaining behaviour under the free energy
principle – a theory of self-organisation originating in neuroscience. It specifies neuronal
dynamics for state-estimation in terms of a descent on (variational) free energy – a measure
of the fit between an internal (generative) model and sensory observations. The free energy
gradient is a prediction error – plausibly encoded in the average membrane potentials of
neuronal populations. Conversely, the expected probability of a state can be expressed in
terms of neuronal firing rates. We show that this is consistent with current models of
neuronal dynamics and establish face validity by synthesising plausible electrophysiological
responses. We then show that these neuronal dynamics
approximate natural gradient descent, a well-known optimisation algorithm from
information geometry that follows the steepest descent of the objective in information space.
We compare the information length of belief updating in both schemes, a measure of the
distance travelled in information space that has a direct interpretation in terms of metabolic
cost. We show that neural dynamics under active inference are metabolically efficient and
suggest that neural representations in biological agents may evolve by approximating
steepest descent in information space towards the point of optimal inference.
Keywords: active inference, free energy principle, process theory, natural gradient descent,
information geometry, variational Bayesian inference, Bayesian brain, self-organisation,
metabolic efficiency, Fisher information length
Introduction
Active inference is a normative framework for explaining behaviour under the free energy
principle, a theory of self-organisation originating in neuroscience [1–4] that characterises
1
Neural dynamics under active inference
certain systems at steady-state as having the appearance of sentience [5,6]. Active inference
describes agents’ behaviour as following the equations of motion of the free energy principle
so as to remain at steady-state, interpreted as the agent’s goal [7].
Active inference describes organisms as inference engines. This assumes that organisms
embody a generative model of their environment. The model encodes how the states external
to the agent influence the agent’s sensations. Organisms infer their surrounding environment
from sensory data by inverting the generative model through minimisation of variational free
energy. This corresponds to performing approximate Bayesian inference (also known as
variational Bayes) [2,3,8–11] or minimising the discrepancy between predictions and
sensations [1,12]. Active inference unifies many existing theories of brain function [13,14],
such as, for example, optimal control [15–18], the Bayesian brain hypothesis [19–21] and
predictive coding [19,22–24]. It has been used to simulate a wide range of behaviours in
neuropsychology, machine learning and robotics. These include planning and navigation [25–
31], exploration [32–36], learning of self and others [37,38], concept learning [39–41], playing
games [42–44], adapting to changing environments [45–47], active vision [48–51] and
psychiatric illness [42,52–55]. Active inference agents show competitive or state-of-the-art
performance in a wide variety of simulated environments [34,46,47,56].
The last decade has developed a theory of how the brain might be implementing active
inference consistently with neurology [1,7,49,57,58]. A small number of predictions of this
theory have been empirically validated, including the role of dopamine in decision-making
[59,60] and free energy minimisation in exploration and choice behaviour [61,62]. This
paper investigates the neural dynamics of this process theory from two complementary
standpoints. 1) Consistency with empirically driven models of neural population dynamics.
2) And the metabolic and computational efficiency of such dynamics.
Efficiency is an important aspect of neural processing in biological organisms [63–68] and an
obvious desideratum for artificial agents. Efficiency of neural processing in biological agents
is best seen in the efficient coding hypothesis by Horace Barlow [69–71], which has received
much empirical support [65,66,72–77] (see in particular [64] for energetic efficiency) and
has been a successful driver in computational neural modelling [75,78–80]. In brief, any
process theory of brain function should exhibit reasonably efficient neural processing.
Active inference formalises perception as inferring the state of the world given sensory data
through minimisation of variational free energy. This amounts to constantly optimising
Bayesian beliefs about states of the outside world in relation to sensations. By beliefs, we
mean probability distributions over states of the environment. These distributions score the
extent to which an agent trusts that the environment is, or not, in this or another state. From
an information theoretic viewpoint, a change of beliefs is a computation or a change in
information encoded by the agent.
This belief updating has an associated energetic cost. Landauer famously observed that a
change in information entails heat generation [81,82]. It follows that the energetics needed
for a change in beliefs may be quantified by the change in information encoded by the agent
over time (as the organism has to alter, e.g., synaptic weights or restore transmembrane
potentials) mathematically scored by the length of the path travelled by the agent’s beliefs in
information space. There is a direct correspondence between the (Fisher) information length
2
Neural dynamics under active inference
of a path and the energy consumed by travelling along that path [83,84]. To ensure metabolic
and computational efficiency, an efficient belief updating algorithm should reach the free
energy minimum (i.e., the point of optimal inference) via the shortest possible path on
average. Furthermore, since an agent does not know the free energy minimum in advance,
she must find it using only local information about the free energy landscape. This is a non-
trivial problem. Understanding how biological agents solve it might not only improve our
understanding of the brain, but also yield useful insights into mathematical optimisation and
machine learning.
In the first part of this work, we show that the dynamics prescribed by active inference for
state-estimation are consistent with current models of neural population dynamics. We then
show that these dynamics approximate natural gradient descent on free energy, a well-
known optimisation algorithm from information geometry that follows the steepest descent
of the objective in information space [85]. This leads to short paths for belief updates as the
free energy encountered in discrete state-estimation is convex (see Appendix A). These
results show that active inference prescribes efficient and biologically plausible neural
dynamics for state-estimation and suggest that neural representations may be collectively
following the steepest descent in information space towards the point of optimal inference.
The softmax activation function in neural population dynamics
This section rehearses a basic yet fundamental feature of mean-field formulations of neural
dynamics; namely, the average firing rate of a neural population follows a sigmoid function
of the average membrane potential. It follows that firing rates can be expressed as a softmax
function of average transmembrane potentials, when considering multiple coupled neural
populations, as the softmax is simply a generalisation of the sigmoid to vector inputs. This can
be seen by the fact that the sigmoid— respectively softmax— function is used in univariate—
respectively multivariate— logistic regression.
The sigmoid relationship between membrane potential and firing rate, was originally derived
by Wilson and Cowan [86], who showed that any unimodal distribution of thresholds within
a neural population, whose individual neurons are modelled as a Heaviside response unit,
results in a sigmoid activation function at the population level. This is because the
population’s activation function equals a smoothing (i.e., a convolution) of the Heaviside
function with the distribution of thresholds.
The assumption that the sigmoid arises from the distribution of thresholds in a neural
population remained unchallenged for many years. However, the dispersion of neuronal
thresholds is, quantitatively, much less important than the variance of neuronal membrane
potential within populations [87]. Marreiros and colleagues showed that the sigmoid
activation function can be more plausibly motivated by considering the variance of neuronal
potentials within a population [88], which is generally modelled by a Gaussian distribution
under the Laplace assumption in mean-field treatments of neural population dynamics [89].
Briefly, with a low variance on neuronal states, the sigmoid function that is obtained – as a
convolution of the Heaviside function – has a steep slope, which means that the neural
population as a whole, fires selectively with respect to the mean membrane potential, and
3
Neural dynamics under active inference
vice-versa. This important fact, which was verified experimentally using dynamic causal
modelling [88,90], means that the variance of membrane potentials implicitly encodes the
(inverse) precision of the information encoded within the population.
Currently, the sigmoid activation function is the most commonly used function to relate
average transmembrane potential to average firing rate in mean-field formulations of neural
population dynamics [91,92] and deep neural networks [93,94]. This relationship logically
extends to a softmax function when considering multiple coupled neural populations.
Neural dynamics of perceptual inference
Active inference formalises perception as inferring the state of the world given sensory data
through minimisation of variational free energy [7]. For state estimation on discrete state-
space generative models (e.g., partially observable Markov decision processes [95]), the free
energy gradient corresponds to a generalised a prediction error [7]. This means that to infer
the states of their environment, biological agents reduce the discrepancy between their
predictions of the environment and their observations, or maximise the mutual information
between them [63].
Variational free energy is a function of approximate posterior beliefs Q,
F(Q)! E ⎡lnQ(s)−lnP(o,s)⎤
Q(s)⎣ ⎦
= D ⎡Q(s)||P(s|o)⎤− lnP(o)
!K#L⎣ ##"###$ ⎦ !"#
≥0 Log-evidence
= D ⎡Q(s)||P(s)⎤− E ⎡lnP(o|s)⎤
!K#L⎣ ##"###$ ⎦ !Q#(s)# ⎣ "##$ ⎦
Complexity Accuracy
while P is the generative model: a probability distribution over hidden states (s) and
observations (o) that encodes the causal relationships between them. Only the observations
are directly accessible; hidden states can only be inferred. The symbol E means the
Q
expectation (i.e., the average) of its argument under the subscripted distribution. D is
KL
known as the Kullback-Leibler divergence or relative entropy [96–98] and is used as a non-
negative measure of the discrepancy between two probability distributions. Note that this is
not a measure of distance, as it is asymmetric. The second line here shows that minimising
free energy amounts to approximate the posterior over hidden states, which is generally
intractable to compute directly, with the approximate posterior. Exact Bayesian inference
requires the approximate and true posterior to be exactly the same, at which point free
energy becomes negative log model evidence (a.k.a., marginal likelihood). This explains why
the (negative) free energy is sometimes referred to as an evidence lower bound (ELBO) in
machine learning [93]. The final line shows a decomposition of the free energy into accuracy
and complexity, underlying the need to find the most accurate explanation for sensory
observations that is minimally complex (c.f., Horace Barlow’s principle of minimum
redundancy [69]).
4
Neural dynamics under active inference
When a biological organism represents some of its environment in terms of a finite number
of possible states (e.g., the locations in space encoded by place cells), we can specify its belief
updating about the current state in peristimulus time. Discrete state-estimation is given as a
(softmax) function of accumulated negative free energy gradients [57].
v!=−∇ F
s
s=σ(v)
In this equation, s is a softmax function and s represents the agent’s beliefs about states.
(These are the parameters of a categorical distribution Q over states). Explicitly, s is a vector
whose i-th component is the agent’s confidence (expressed as a probability) that it is in the i-
th state. The softmax function is the natural choice to map from free energy gradients to
beliefs as the former turns out to be a logarithm [7] and the components of the latter must
sum to one.
Just as neuronal dynamics involve translation from post-synaptic potentials to firing rates,
these dynamics involve translating from a vector of real numbers (v), to a vector where
components are bounded between zero and one (s). As such, we can interpret v as the voltage
potential of neuronal populations, and s as representing their firing rates (since these are
upper bounded thanks to neuronal refractory periods). Note the softmax function here plays
the same role as in mean-field formulations; it translates average potentials to firing rates.
On the one hand, this view is consistent with models of neuronal population dynamics. On
the other hand, it confers post-hoc face validity, as it enables to synthesise plausible local
field potentials (see Figure 1) and a wide range of other electrophysiological responses,
including repetition suppression, mismatch negativity, violation responses, place-cell activity,
phase precession, theta-gamma coupling, and more [57].
Figure 1: Active inference for state estimation (discrete state-space).
5
Neural dynamics under active inference
Panel 1a summarises the problem of finding the minimum of a function (e.g., the free energy). One possibility
would be taking the shortest path, which involves climbing up a hill, however in the nescience of the minimum,
a viable strategy consists of myopically taking the direction of steepest descent. In panel 1b, we depict an
example of a trajectory of an agent’s beliefs during the process of perception, which consists of updating beliefs
about the states of the external world to reach the point of optimal inference (i.e., free energy minimum). In
this example the state-space comprises only three states (e.g., three different locations in a room). As they are
probabilities over states, the components of s are non-negative and sum to one; hence, the agent’s beliefs
naturally live on a triangle in three-dimensional space. Mathematically, this object is called a (two-dimensional)
simplex. This constitutes the belief space, or information space, on which the free energy is defined. Technically,
this object is a smooth statistical manifold, which corresponds to the set of parameters of a categorical
distribution. To optimise metabolic and computational efficiency, agents must update their beliefs to reach the
free energy minimum via the shortest possible path on this manifold. In panel 1c we exhibit simulated local field
potentials that arise by interpreting the rate of change of v in terms of depolarisations, over a sequence of eight
observations (e.g., saccadic eye-movements). As the rate of change is given by the free energy gradients, the
decay of these local field potentials to zero coincides with reaching the free energy minimum (at which the
gradient is zero by definition). These were obtained during the first numerical simulation described in Figure 3.
For more details on the generation of simulated electrophysiological responses, see [57].
The idea that state-estimation is expressed in terms of firing rates is well-established when
the state-space constitutes an internal representation of space. This is the raison d’être of the
study of place cells [99], grid cells [100] and head-direction cells [101,102], where the states
inferred are physical locations in space. Primary afferent neurons in cats have also been
shown to encode kinematic states of the hind limb [103–105]. Most notably, the seminal work
of Hubel and Wiesel [106] showed the existence of neurons encoding orientation of visual
stimuli. In short, the very existence of receptive fields in neuroscience speaks to a carving of
the world into discrete states under an implicit discrete state generative model. While many
of these studies focus on single neuron recordings, the arguments presented above are
equally valid and generalise the case of ‘populations’ comprising of a single neuron.
In summary, the neuronal dynamics associated with state-estimation in active inference are
consistent with mean-field models of neural population dynamics. This view is strengthened
a posteriori, as this allows one to generate a wide range of plausible electrophysiological
responses. Yet, further validation remains to be carried out by testing these
electrophysiological responses empirically. We will return to this in the discussion.
A primer on information geometry and natural gradient descent
To assess the computational and metabolic efficiency of a belief trajectory, it becomes
necessary to formalise the idea of ‘belief space’. These are well-studied structures in the field
of information geometry [107–109], called statistical manifolds. In our case, these are
(smooth) manifolds, where each point corresponds to a parameterisation of the probability
distribution in consideration (see Figure 2). One is then licensed to talk about a change in
beliefs as a trajectory on a statistical manifold.
6
Neural dynamics under active inference
Figure 2: Statistical manifolds and information length.
Panels 2a-b illustrate the statistical manifolds associated with two well-known probability distributions: the
normal distribution and the categorical distribution, respectively. The statistical manifold associated with a
probability distribution is the set of all possible parameters that it can take. For the univariate normal
distribution, parameterised with mean µ and positive standard deviation V, the associated statistical manifold is
the upper half plane (panel 2a). For the categorical distribution, in the case of three possible states, the statistical
manifold is the 2-dimensional simplex (panel 2b). More generally, in the case of n possible states, the statistical
manifold of the categorical distribution is the set of all vectors with positive components that sum to one, i.e.,
the (n-1)-dimensional simplex. This is a higher-dimensional version of the triangle or the tetrahedron. In panels
2c-d we illustrate why the Euclidean distance is ill-suited to measure the information distance between
probability distributions. To show this we selected four distributions that correspond to points on the statistical
manifold of the normal distribution. One can see that the Euclidean distance between the modes of the red and
the blue distributions is the same as that from the orange and the green, however, the difference in information
of each respective pair is quite different. In panel 2c, the two distributions correspond to two drastically different
beliefs, since there is such little overlap; on the contrary, the beliefs in panel 2d are much more similar. This calls
for a different notion of distance that measures the difference in (Fisher) information between distributions;
namely, the information length.
Smooth statistical manifolds are naturally equipped with a different notion of distance, even
though they may be subsets of Euclidean space. This is because the Euclidean distance
measures the physical distance between points, while the information length measures
distance in terms of the (accumulated) change in (Fisher) information (see Figure 2) along a
path. The canonical choice of information length on a statistical manifold is associated with
the Fisher information metric tensor g [110–112]. Technically, a metric tensor is a smoothly
varying choice of symmetric, positive definite matrix at each point of the manifold. This
enables computation of the length of paths as well as the distance between points, by
measuring the length of the shortest path (see Appendix B). Mathematically, the Fisher
information metric can be defined the Hessian of the KL divergence between two
infinitesimally close distributions (see Appendix B). This means that the information length of
a trajectory on a statistical manifold is given by accruing infinitesimally small changes in the
KL divergence along it.
7
Neural dynamics under active inference
Amari’s natural gradient descent [85,113] is a well-known optimisation algorithm for finding
the minimum of functions defined on statistical manifolds such as the variational free energy.
It consists of preconditioning the vanilla gradient descent update rule with the inverse of the
Fisher information metric tensor:
s!=−∇ F → s!=−g−1(s)∇ F
s s
Preconditioning by the inverse of g means that the natural gradient privileges directions of
low information length. One can see this, since the directions of greatest (resp. smallest)
information length are the eigenvectors of the highest (resp. lowest) eigenvalues of g.
In fact, Amari proved that the natural gradient follows the direction of steepest descent of
the objective function in information space [85]. Technically, the natural gradient generalises
gradient descent to functions defined on statistical manifolds. As the free energy for discrete
state-estimation is convex (see Appendix A), this means that natural gradient descent will
always converge to the free energy minimum via a short path.
To summarise, agents’ beliefs naturally evolve on a statistical manifold towards the point of
optimal inference. These manifolds are equipped with a different notion of distance; namely,
the information length. In the case of discrete state-estimation, beliefs evolve on the simplex
towards the free energy minimum. Reaching the minimum with a short path translates into
higher computational and metabolic efficiency. One scheme that achieves short paths for
finding the minimum of the free energy is the natural gradient. In the next section, we will
show that the neuronal dynamics entailed by active inference approximate natural gradient
descent.
Active inference approximates natural gradient descent
Discretising the (neuronal) dynamics prescribed by active inference and natural gradient
descent give us the following state-estimation belief updates, respectively:
s(t)−εg−1(s(t))∇ F
s(t+1) ←σ ( lns(t)−ε ∇ F ) s(t+1) ← s(t)
s(t) ~
In these equations the logarithm is taken component-wise, e is the step-size used in the
discretisation and ~ denotes normalisation by the sum of the components, to ensure that s(t+1)
lies on the simplex1.
1 Natural gradient descent dynamics do not necessarily remain on the statistical manifold. This problem has
been the object of numerous works that supplemented the natural gradient update with a projection step [114–
118]. Here we choose the simplest projection step to ensure that the result remains on the simplex: normalising
with the sum of the components.
8
Neural dynamics under active inference
These dynamics are approximately the same. We can equate them under a first order Taylor
approximation of the exponential inside the softmax function:
s(t+1) ←σ ( lns(t)−ε ∇ F )
s(t)
exp⎡lns(t)−ε ∇ F⎤
=
⎣ s(t) ⎦
~
s(t)⊙exp⎡−ε ∇ F⎤
=
⎣ s(t) ⎦
~
"
s(t)⊙⎡1−ε ∇ F⎤
⎣ s(t) ⎦
!
~
s(t)−εs(t)⊙∇ F
=
s(t)
~
s(t)−εg−1(s(t))∇ F
=
s(t)
~
The symbol ⊙ denotes the Hadamard product (elementwise multiplication). The last line
follows since, on the simplex, the inverse of the Fisher information metric tensor is simply a
diagonal matrix whose diagonal is s (see Appendix C).
Although these dynamics are approximately the same, this does not guarantee that the paths
taken in the limit of infinitessimaly small time steps (which correspond to continuous-time
dynamics in physical and biological systems) will be the same. One can see this algebraically,
since the number of time steps needed to reach the free energy minimum increases as the
step-size decreases, thus the difference between paths, which equals the sum of the
differences at each timestep, is not guaranteed to converge to zero. Hence it is necessary to
verify that this approximation holds well in practice, by analysing the discrepancy between
paths using numerical simulations.
Numerical simulations
In this section we use numerical simulations of two tasks (i.e., a two-step maze and a rule
learning task) to assess to what extent the correspondence between active inference and
natural gradient descent holds true. Our simulations compared the information length of the
belief trajectories taken by both schemes which reflects their computational and metabolic
efficiency. See Figure 3 for details.
We observe that both schemes perform equally well on average across both tasks. It is
interesting to see that in some cases, the trajectories taken by both schemes are significantly
longer than the shortest path (i.e., the geodesic; see Appendix D). This is unsurprising since
9
Neural dynamics under active inference
agents’ beliefs evolve towards the free energy minimum using only local information about
the free energy landscape. Note that this occurred only in a minority of the trials in the
examples considered here.
Figure 3: Information length and belief trajectories of active inference and natural gradient.
We performed two simulations using both standard active inference and a modified active inference scheme
where perception is performed using natural gradient descent. We compare the information length of belief
updating of each scheme, with 128 agents across 24 trials and using a standard step size of e=0.25. The first
paradigm simulates a rat in a T-Maze (see panel 3a). The T-Maze has a reward which is placed either in the right
or left upper arm (in red). The bottom arm contains a cue (in blue) that specifies the location of the reward. The
rat’s initial location is the middle of the T-Maze, as shown in the picture. The initial conditions are specified such
that the rat believes that it will claim the reward and avoid the upper arm that does not have the reward. The
optimal strategy consists of collecting the cue (exploration) and then using this information to collect the reward
(exploitation). To achieve this, the rat must infer its location and the configuration of the maze, in addition to
the route it will take. In this simulation, there are two hidden states, which correspond to the location of the rat
in the T-Maze and the location of the reward, respectively. For details of this paradigm, the generative model
and ensuing simulation see [57]. The second paradigm was a more complex simulation of abstract rule learning,
with the same sort of generative model and belief updating mechanism as the first simulation. For more details
on the paradigm see [39]. The purpose of including this is that it includes a hidden state dimension with three
possible alternatives, facilitating a simple visual representation of the associated simplex in panel 3d. The
histograms show the information length of belief updating in active inference (in red) and natural gradient (in
blue) during the T-Maze and abstract rule learning tasks (resp. panel 3b, 3e). Specifically, this is the information
length accrued by each agent at each trial, averaged across agents. One can see that the performance of both
schemes as scored in terms of information length is almost identical across tasks. The reasons for systematic
variation in information length across trials is that 1) the task configuration varied from trial to trial and 2) the
generative models (i.e., representations of the environment) were themselves optimised (i.e., learned) over
trials. The boxplots (resp. panels 3c, 3f) illustrate the difference in information length of the histograms (resp.
panels 3b, 3e) by subtracting the information length of active inference from the information length of the
natural gradient. In the first paradigm, active inference mostly takes shorter paths, which is why the boxplot’s
values are mostly positive. However, we obtain the opposite pattern in the second simulation. In addition, the
differences in information length are marginal compared to the information length of each trial. Furthermore,
both schemes perform equally well across both tasks on average. This suggests that the differences between
the two schemes is small. Panel 3d shows an example of the belief trajectories taken during state estimation in
abstract rule learning. The red trajectory is standard active inference, the blue is natural gradient descent, and
the orange is the shortest path to the free energy minimum (i.e., the geodesic, see Appendix D). This example is
not representative of the average and was chosen for purely illustrative purposes as the trajectories are very
distinct, lengthy and do not coincide with the geodesic. The fact that both schemes take significantly longer
10
Neural dynamics under active inference
paths than the geodesic was expected to occur in some trials as beliefs evolve to the free energy minimum
myopically. Note that this apparent suboptimality was atypical in the tasks considered.
To summarise, our results suggest that state-estimation in active inference is a good
approximation to natural gradient descent on free energy. Natural gradient descent follows
the steepest descent of the objective in information space. Since the free energy optimised
during state-estimation is convex (see Appendix A), this means that neural dynamics under
active inference take short paths to the free energy minimum, which are the most
computationally and metabolically efficient.
Discussion
In the first part of this paper, we showed that neural dynamics under active inference are
consistent with mean-field models neural population dynamics. This construct validity is
further supported by the wide range of plausible electrophysiological responses that can be
synthesised with active inference [57]. Yet, to fully endorse this view, the electrophysiological
responses simulated during state-estimation need empirical validation. To do this, one would
have to specify the generative model that a biological agent employs to represent a particular
environment. This may be identified by comparing alternative hypothetical generative
models with empirical choice behaviour and computing the relative evidence for each model
(e.g., [119]). Once the appropriate generative model is found, one would need to compare
the evidence for a few possible practical implementations of active inference, which come
from various possible approximations to the free energy [27,120,121], each of which yields
different belief updates and simulated electrophysiological responses. Note that of possible
approximations to the free energy, the marginal approximation which was used in our
simulations currently stands as the most biologically plausible [120]. Finally, one would be
able to assess the explanatory power of active inference in relation to empirical
measurements and compare it with other process theories.
In the second part of this paper, we showed that the neuronal process theory associated with
active inference approximates natural gradient descent for state-estimation. Given that the
natural gradient follows the direction of steepest descent of the free energy in information
space [85] and that the free energy landscape at hand is convex (see Appendix A), this ensures
that agent’s beliefs reach the point of optimal inference via short trajectories in information
space. This means that active inference entails neuronal dynamics that are both
computationally and energetically efficient, an important feature of any reasonable process
theory of the brain.
In the case of simulated (i.e., discretised) belief dynamics, active inference and natural
gradient perform equally well on average. Performance is scored by the information length
accrued during belief updating, which measures efficiency. In some cases, however, the belief
trajectories taken by both schemes were significantly longer than the shortest path to the
point of optimal inference. This is unsurprising since agents’ beliefs move myopically to the
free energy minimum. In short, our analysis suggests that biological agents can perform
11
Neural dynamics under active inference
natural gradient descent in a biologically plausible manner. From an engineering perspective,
this means that we can relate variational message passing and belief propagation, two
inferential algorithms based on free energy minimisation [120,122,123], with the natural
gradient.
A general point is that the tools furnished by information geometry are ideally suited to
characterise and visualise inference in biological organisms as well as scoring its efficiency.
This paves the way for further applications of information geometry to analyse information
processing in biological systems.
Conclusion
In the first part, we showed the consistency between the generic, first-principles account of
brain function provided by active inference with the more detailed and empirically driven
mean-field models of neural population dynamics.
Then, we demonstrated that perception under active inference approximates natural
gradient descent on free energy. This suggests that the beliefs about states of the world of
biological agents evolve approximately according to a steepest descent on free energy. Since
the free energy landscape for discrete state-estimation is convex, the trajectories taken to
the point of optimal inference are short, which incurs minimal computational and metabolic
cost. This demonstrates the efficiency of neural dynamics under active inference, an
important feature of the brain as we know it.
Further testing of active inference as a process theory of brain function should focus on
extending the empirical evaluation of simulated neurophysiological responses.
Software availability
The belief updating process described in this article are generic and can be implemented using
standard routines (e.g., spm_MDP_VB_X.m). These routines are available as Matlab code in
the SPM academic software: http://www.fil.ion.ucl.ac.uk/spm/. Examples of simulations can
be found via a graphical user interface by typing DEM (e.g., DEM_demo_MDP_X.m for the T-
Maze task [57], rule_learning.m for the artificial curiosity and abstract rule learning task [39]).
Data accessibility
This article has no additional data.
Authors' contributions
12
Neural dynamics under active inference
All authors made substantial contributions to conception, design and writing of the article;
and approved publication of the final version.
Competing interests
We have no competing interests.
Funding
LD is supported by the Fonds National de la Recherche, Luxembourg (Project code:
13568875). TP is supported by the Rosetrees Trust (Award number: 173346). KF is funded by
a Wellcome Trust Principal Research Fellowship (Ref: 088130/Z/09/Z).
Appendix A. Convexity of the free energy
On discrete state-space generative models (e.g., partially observable Markov decision
processes), the free energy optimised during state-estimation can be expressed as [7]:
T t T
F(s ,...,s )=∑s ⋅lns −∑o ⋅ln(A)s −s ⋅lnD−∑s ⋅ln(B )s
π1 πT πτ πτ τ πτ π1 πτ π πτ−1
τ−1
τ=1 τ=1 τ=2
In this context, the neuronal dynamics described in the paper are:
v!(s ,...,s )=−∇ F(s ,...,s )
π1 πT s
πτ
π1 πT
s =σ(v)
πτ
Here τ corresponds to time (which is discretised), s corresponds to the beliefs about states
πτ
at timepoint τ, conditioned upon the fact that the agent is pursuing a certain sequence of
actions π. The particular meaning of the other variables is not important for our purposes;
the only important thing is that B are matrices, whose components are strictly contained
π
τ−1
between zero and one, and logarithms are taken component-wise.
Recall that a sum of convex functions is convex. Furthermore,
T
• x! xlnxis convex in the interval [0,1], which implies that ∑s ⋅lns is convex.
πτ πτ
τ=1
t
• −∑o ⋅ln(A)s −s ⋅lnD is a linear function, hence it is convex.
τ πτ π1
τ=1
T
• −ln(B ) only has positive components, hence −∑s ⋅ln(B )s is a positive
π τ−1 πτ π τ−1 πτ−1
τ=2
linear combination of polynomials of degree two, which is convex.
13
Neural dynamics under active inference
This implies that the free energy is convex.
Appendix B. Fisher information metric tensor, information length and
information distance
The Fisher information metric tensor is the canonical mathematical object that the enables
computation of (a certain kind of) information distance on a statistical manifold. Technically,
a metric tensor is a choice of symmetric, positive definite matrix at each point, that varies
smoothly on the statistical manifold. This is equivalent to specifying an inner product at each
point of the manifold and doing so smoothly.
Let p(x|s) be a probability distribution parameterised by s. The set of all possible choices of
s is the statistical manifold associated with p, which we will denote by M. This is (in the case
of classical probability distributions, which includes the scope of this paper) a smooth
manifold, where each point corresponds to a certain parameterisation of the probability
distribution, i.e., a (smooth) statistical manifold. We can then define the Fisher information
metric tensor as
g(s)=∇2 D ⎡p(x|s)|| p(x|θ)⎤
⎣ ⎦
θ KL θ=s
This is an n-by-n matrix where n is the dimensionality of s and q. There exist other equivalent
definitions [107,108].
This is nice, because a choice of an inner product at each point on the manifold enables to
compute the length of tangent vectors. Let v be such a tangent vector at a point s, then its
norm is given by
||v|| := vTg(s)v
g
This means that we can also compute the length of smooth curves. Let γ:[0,1]⊂!→ M be
such a curve. Its information length is given by
ℓ (γ):= ∫ 1 γ"(t)Tg( γ(t)) γ"(t)dt,
g 0
dγ
where γ!:= .
dt
We can trivially extend this definition to compute the information distance between points,
say s and s’. This is simply the information length of the shortest curve connecting the two
points
d (s,s′)=inf ℓ(γ).
g γ:[0,1]→M
γ(0)=s,γ(1)=s′
14
Neural dynamics under active inference
Where, inf denotes the infimum of the quantity subject to the constraints in the subscript.
Let us take a step back to see why these definitions are sensible.
Statistical manifolds are generally curved, therefore it is only possible to compute distances
locally, by deforming the small region of consideration into a portion of Euclidean space. This
is impractical and does not solve the problem of computing distances over larger scales. Even
if one did so, one would recover a deformed version of the Euclidean distance, which would,
generally speaking, not measure distance in terms of information. The raison d’être of the
metric tensor is to allow the computation of distances on the manifold in a consistent way,
and in our case consistently with the difference in Shannon information.
If one replaced g in the definitions above by the identity matrix (i.e., the metric tensor that is
used implicitly in Euclidean space), one recovers the classical notion of length of a vector (i.e.,
the square root of the inner product), the classical notion of the length of a curve, namely
1
ℓ(γ):= ∫ γ"(t) dt
0
The distance between two points is a little trickier as it involves proving that the shortest path
between two points is the straight line when the metric tensor is the identity. This involves
solving the geodesic equation (see Appendix D) for this metric tensor. Once this is done,
inserting a straight line in the above equation returns the usual Euclidean distance.
Appendix C. Fisher information metric tensor on the simplex
Suppose there are n+1 states S ={s ,...,s }. Then a categorical distribution p(x|s) over those
0 n
states is defined as p(s |s):=s . The statistical manifold of all possible parameters is the
i i
interior of the n-dimensional simplex which is defined as
Δn:={s=(s ,...,s )∈!n+1|s >0,∑s =1}
0 n i i
i
The Fisher information metric tensor can be defined as g(s)=∇2D ⎡p(x|s)|| p(x|θ)⎤ .
⎣ ⎦
θ KL θ=s
The KL-divergence between two categorical distributions is given by
p(x|s)
D ⎡p(x|s)|| p(x|θ)⎤=∑p(x|s)log
⎣ ⎦
KL p(x|θ)
x∈S
n p(s |s)
=∑p(s |s)log i
i p(s |θ)
i=0 i
n s
=∑s log i
i θ
i=0 i
We can take second derivatives
15
Neural dynamics under active inference
∂ ∂ ⎛ n s ⎞ s
∑s log i =δ k
∂θ ∂θ ⎝ ⎜ i θ⎠ ⎟ jkθ2
j k i=0 i k
Where δ is the Kronecker delta. Finally,
jk
s−1 0 ! 0
0
0 s−1 " #
g(s)= 1
# " " 0
0 ! 0 s−1
n
Technical remark: since the statistical manifold of interest is n-dimensional, it is best to view
this metric tensor as being defined on an n+1 dimensional neighbourhood of the simplex, e.g.,
the positive orthant of !
n+1
.
Appendix D. Geodesics on the simplex
The aim of this section is to find the expression of the shortest path (in information length)
between two points on the simplex.
As shown in Appendix C the metric tensor is given by
s−1 0 ! 0
0
0 s−1 " #
g(s)= 1
# " " 0
0 ! 0 s−1
n
Let s(0),s(1) be two points on the simplex. From standard differential geometry, the shortest
path g between two points satisfies the geodesic equation:
n
γ!! + ∑Γk(γ)γ!γ! ≡0
k ij i j
i,j=0
Where Γk are the Christoffel symbols of the Levi-Civita connection. These are real valued
ij
functions defined with respect to the metric:
1 n
Γk := ∑gkr( ∂ g +∂ g −∂ g )
ij 2 j ri i rj r ij
r=0
16
Neural dynamics under active inference
In this expression gkr is the (k,r) entry of the inverse metric tensor g−1 and ∂ is a shorthand
j
∂
for . In our case the only non-zero Christoffel symbols are given by
∂s
j
1
Γi (s)=−
ii 2s
i
This means that each component of the geodesic must satisfy the equation
2γγ!! −γ! 2 ≡0
i i i
By inspection, one can see that the differential equation admits a polynomial solution of
degree two. Solving with the boundary conditions γ(0)=s(0),γ(1)=s(1) and discarding those
solutions that leave the positive orthant of !
n+1
(c.f., last remark Appendix C) yields the
expression of the geodesic:
( )2
γ(t)= (1−t) s(0) +t s(1)
Appendix E. Information distance on the simplex
The distance between two points on a statistical manifold is given by the information length
of the shortest path (i.e., the geodesic) between the two. Given two points s(0),s(1) on the
simplex, we have seen in Appendix D that the geodesic between these points is
( )2
γ(t)= (1−t) s(0) +t s(1)
Furthermore, from Appendix B we have seen that the information distance between two
points is the information length of the geodesic between them
d (s(0),s(1))=ℓ (γ)= ∫ 1 γ!(t)Tg( γ(t)) γ!(t)dt
g g 0
Lastly, from Appendix C, the Fisher information metric tensor on the simplex is
s−1 0 ! 0
0
0 s−1 " #
g(s)= 1
# " " 0
0 ! 0 s−1
n
Therefore, expanding the expression inside the information distance
17
Neural dynamics under active inference
n γ! (t)2
γ!(t)Tg( γ(t)) γ!(t)=∑ i
γ (t)
i=0 i
γ! (t)2
It is possible to show that i is constant for each i. One can do this by taking the
γ (t)
i
derivative with respect to t and noting that the result vanishes. This means that one can
remove the integral and find a concise expression for the information distance:
d (s(0),s(1))= γ!(0)Tg( γ(0)) γ!(0)
g
n γ! (0)2
= ∑ i
γ (0)
i=0 i
2
n ( )
= 4∑ s(1) − s(0)
i i
i=0
=2 s(1) − s(0)
This expression is compelling, since it relates the information distance on the simplex to the
Euclidean distance on the n-dimensional sphere.
References
1. Bogacz R. A tutorial on the free-energy framework for modelling perception and
learning. Journal of Mathematical Psychology. 2017;76: 198–211.
doi:10.1016/j.jmp.2015.11.003
2. Friston K. The free-energy principle: a rough guide to the brain? Trends in Cognitive
Sciences. 2009;13: 293–301. doi:10.1016/j.tics.2009.04.005
3. Friston K, Kilner J, Harrison L. A free energy principle for the brain. Journal of Physiology-
Paris. 2006;100: 70–87. doi:10.1016/j.jphysparis.2006.10.001
4. Friston K. Life as we know it. Journal of The Royal Society Interface. 2013;10: 20130475.
doi:10.1098/rsif.2013.0475
5. Friston K. A free energy principle for a particular physics. arXiv:190610184 [q-bio]. 2019
[cited 29 Feb 2020]. Available: http://arxiv.org/abs/1906.10184
6. Parr T, Da Costa L, Friston K. Markov blankets, information geometry and stochastic
thermodynamics. Philosophical Transactions of the Royal Society A: Mathematical,
Physical and Engineering Sciences. 2020;378: 20190159. doi:10.1098/rsta.2019.0159
7. Da Costa L, Parr T, Sajid N, Veselic S, Neacsu V, Friston K. Active inference on discrete
state-spaces: A synthesis. Journal of Mathematical Psychology. 2020;99: 102447.
doi:10.1016/j.jmp.2020.102447
18
Neural dynamics under active inference
8. Friston K. A theory of cortical responses. Phil Trans R Soc B. 2005;360: 815–836.
doi:10.1098/rstb.2005.1622
9. Beal MJ. Variational Algorithms for Approximate Bayesian Inference. 2003; 281.
10. Jordan MI, Ghahramani Z, Jaakkola TS, Saul LK. An Introduction to Variational Methods
for Graphical Models. In: Jordan MI, editor. Learning in Graphical Models. Dordrecht:
Springer Netherlands; 1998. pp. 105–161. doi:10.1007/978-94-011-5014-9_5
11. Wainwright MJ, Jordan MI. Graphical Models, Exponential Families, and Variational
Inference. FNT in Machine Learning. 2007;1: 1–305. doi:10.1561/2200000001
12. Buckley CL, Kim CS, McGregor S, Seth AK. The free energy principle for action and
perception: A mathematical review. Journal of Mathematical Psychology. 2017;81: 55–
79. doi:10.1016/j.jmp.2017.09.004
13. Friston K. The free-energy principle: a unified brain theory? Nat Rev Neurosci. 2010;11:
127–138. doi:10.1038/nrn2787
14. Colombo M, Wright C. First principles in the life sciences: the free-energy principle,
organicism, and mechanism. Synthese. 2018 [cited 11 Aug 2019]. doi:10.1007/s11229-
018-01932-w
15. Kappen HJ, Gómez V, Opper M. Optimal control as a graphical model inference problem.
Mach Learn. 2012;87: 159–182. doi:10.1007/s10994-012-5278-7
16. Da Costa L, Sajid N, Parr T, Friston K, Smith R. The relationship between dynamic
programming and active inference: the discrete, finite-horizon case. arXiv:200908111
[cs, math, q-bio]. 2020 [cited 31 Jan 2021]. Available: http://arxiv.org/abs/2009.08111
17. Millidge B, Tschantz A, Seth AK, Buckley CL. On the Relationship Between Active
Inference and Control as Inference. arXiv:200612964 [cs, stat]. 2020 [cited 28 Jun
2020]. Available: http://arxiv.org/abs/2006.12964
18. Watson J, Imohiosen A, Peters J. Active Inference or Control as Inference? A Unifying
View. arXiv:201000262 [cs, stat]. 2020 [cited 27 Jan 2021]. Available:
http://arxiv.org/abs/2010.00262
19. Aitchison L, Lengyel M. With or without you: predictive coding and Bayesian inference in
the brain. Current Opinion in Neurobiology. 2017;46: 219–227.
doi:10.1016/j.conb.2017.08.010
20. Knill DC, Pouget A. The Bayesian brain: the role of uncertainty in neural coding and
computation. Trends in Neurosciences. 2004;27: 712–719.
doi:10.1016/j.tins.2004.10.007
21. Lake BM, Ullman TD, Tenenbaum JB, Gershman SJ. Building Machines That Learn and
Think Like People. arXiv:160400289 [cs, stat]. 2016 [cited 11 Aug 2019]. Available:
http://arxiv.org/abs/1604.00289
19
Neural dynamics under active inference
22. Rao RPN, Ballard DH. Predictive coding in the visual cortex: a functional interpretation of
some extra-classical receptive-field effects. Nat Neurosci. 1999;2: 79–87.
doi:10.1038/4580
23. Bastos AM, Usrey WM, Adams RA, Mangun GR, Fries P, Friston KJ. Canonical
Microcircuits for Predictive Coding. Neuron. 2012;76: 695–711.
doi:10.1016/j.neuron.2012.10.038
24. Friston K, Kiebel S. Predictive coding under the free-energy principle. Phil Trans R Soc B.
2009;364: 1211–1221. doi:10.1098/rstb.2008.0300
25. Kaplan R, Friston KJ. Planning and navigation as active inference. Biol Cybern. 2018;112:
323–343. doi:10.1007/s00422-018-0753-2
26. Pezzulo G. An Active Inference view of cognitive control. Front Psychology. 2012;3.
doi:10.3389/fpsyg.2012.00478
27. Schwöbel S, Kiebel S, Marković D. Active Inference, Belief Propagation, and the Bethe
Approximation. Neural Computation. 2018;30: 2530–2567. doi:10.1162/neco_a_01108
28. Matsumoto T, Tani J. Goal-Directed Planning for Habituated Agents by Active Inference
Using a Variational Recurrent Neural Network. Entropy. 2020;22: 564.
doi:10.3390/e22050564
29. Çatal O, Verbelen T, Nauta J, Boom CD, Dhoedt B. Learning Perception and Planning
With Deep Active Inference. ICASSP 2020 - 2020 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP). 2020. pp. 3952–3956.
doi:10.1109/ICASSP40776.2020.9054364
30. Çatal O, Wauthier S, Verbelen T, De Boom C, Dhoedt B. Deep Active Inference for
Autonomous Robot Navigation. arXiv:200303220 [cs]. 2020 [cited 22 May 2020].
Available: http://arxiv.org/abs/2003.03220
31. Sancaktar C, van Gerven M, Lanillos P. End-to-End Pixel-Based Deep Active Inference for
Body Perception and Action. arXiv:200105847 [cs, q-bio]. 2020 [cited 18 Sep 2020].
Available: http://arxiv.org/abs/2001.05847
32. Tschantz A, Seth AK, Buckley CL. Learning action-oriented models through active
inference. PLOS Computational Biology. 2020;16: e1007805.
doi:10.1371/journal.pcbi.1007805
33. Schwartenbeck P, Passecker J, Hauser TU, FitzGerald TH, Kronbichler M, Friston KJ.
Computational mechanisms of curiosity and goal-directed exploration. eLife. 2019; 45.
34. Tschantz A, Millidge B, Seth AK, Buckley CL. Reinforcement Learning through Active
Inference. ICLR. 2020. Available: http://arxiv.org/abs/2002.12636
35. Marković D, Goschke T, Kiebel SJ. Meta-control of the exploration-exploitation dilemma
emerges from probabilistic inference over a hierarchy of time scales. Cogn Affect
Behav Neurosci. 2020 [cited 27 Jan 2021]. doi:10.3758/s13415-020-00837-x
20
Neural dynamics under active inference
36. Friston K, Da Costa L, Hafner D, Hesp C, Parr T. Sophisticated Inference. arXiv:200604120
[cs, q-bio]. 2020 [cited 28 Jun 2020]. Available: http://arxiv.org/abs/2006.04120
37. Lanillos P, Cheng G. Adaptive robot body learning and estimation through predictive
coding. 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS). 2018; 4083–4090. doi:10.1109/IROS.2018.8593684
38. Lanillos P, Pages J, Cheng G. Robot self/other distinction: active inference meets neural
networks learning in a mirror. arXiv:200405473 [cs]. 2020 [cited 18 Sep 2020].
Available: http://arxiv.org/abs/2004.05473
39. Friston KJ, Lin M, Frith CD, Pezzulo G, Hobson JA, Ondobaka S. Active Inference, Curiosity
and Insight. Neural Computation. 2017;29: 2633–2683. doi:10.1162/neco_a_00999
40. Smith R, Schwartenbeck P, Parr T, Friston KJ. An Active Inference Approach to Modeling
Structure Learning: Concept Learning as an Example Case. Front Comput Neurosci.
2020;14. doi:10.3389/fncom.2020.00041
41. Wauthier ST, Çatal O, Verbelen T, Dhoedt B. Sleep: Model Reduction in Deep Active
Inference. 2020; 13.
42. Cullen M, Davey B, Friston KJ, Moran RJ. Active Inference in OpenAI Gym: A Paradigm
for Computational Investigations Into Psychiatric Illness. Biological Psychiatry:
Cognitive Neuroscience and Neuroimaging. 2018;3: 809–818.
doi:10.1016/j.bpsc.2018.06.010
43. Fountas Z, Sajid N, Mediano PAM, Friston K. Deep active inference agents using Monte-
Carlo methods. arXiv:200604176 [cs, q-bio, stat]. 2020 [cited 16 Jul 2020]. Available:
http://arxiv.org/abs/2006.04176
44. Ueltzhöffer K. Deep Active Inference. Biol Cybern. 2018;112: 547–573.
doi:10.1007/s00422-018-0785-7
45. Marković D, Reiter AMF, Kiebel SJ. Predicting change: Approximate inference under
explicit representation of temporal structure in changing environments. PLOS
Computational Biology. 2019;15: e1006707. doi:10.1371/journal.pcbi.1006707
46. Markovic D, Stojic H, Schwoebel S, Kiebel SJ. An empirical evaluation of active inference
in multi-armed bandits. arXiv:210108699 [cs]. 2021 [cited 26 Jan 2021]. Available:
http://arxiv.org/abs/2101.08699
47. Sajid N, Ball PJ, Friston KJ. Active inference: demystified and compared. arXiv:190910863
[cs, q-bio]. 2020 [cited 30 Apr 2020]. Available: http://arxiv.org/abs/1909.10863
48. Parr T, Friston KJ. Active inference and the anatomy of oculomotion. Neuropsychologia.
2018;111: 334–343. doi:10.1016/j.neuropsychologia.2018.01.041
49. Parr T. The computational neurology of active vision. Ph.D., University College London.
2019.
21
Neural dynamics under active inference
50. Mirza MB, Adams RA, Mathys CD, Friston KJ. Scene Construction, Visual Foraging, and
Active Inference. Front Comput Neurosci. 2016;10. doi:10.3389/fncom.2016.00056
51. Parr T, Friston KJ. Uncertainty, epistemics and active inference. J R Soc Interface.
2017;14. doi:10.1098/rsif.2017.0376
52. Adams RA, Stephan KE, Brown HR, Frith CD, Friston KJ. The Computational Anatomy of
Psychosis. Front Psychiatry. 2013;4. doi:10.3389/fpsyt.2013.00047
53. Smith R, Kirlic N, Stewart JL, Touthang J, Kuplicki R, Khalsa SS, et al. Greater decision
uncertainty characterizes a transdiagnostic patient sample during approach-avoidance
conflict: a computational modeling approach. PsyArXiv; 2020 Apr.
doi:10.31234/osf.io/t2dhn
54. Smith R, Schwartenbeck P, Stewart JL, Kuplicki R, Ekhtiari H, Investigators T, et al.
Imprecise Action Selection in Substance Use Disorder: Evidence for Active Learning
Impairments When Solving the Explore-Exploit Dilemma. PsyArXiv; 2020 Apr.
doi:10.31234/osf.io/a794k
55. Smith R, Kuplicki R, Feinstein J, Forthman KL, Stewart JL, Paulus MP, et al. A Bayesian
computational model reveals a failure to adapt interoceptive precision estimates
across depression, anxiety, eating, and substance use disorders. PLOS Computational
Biology. 2020;16: e1008484. doi:10.1371/journal.pcbi.1008484
56. Millidge B. Deep active inference as variational policy gradients. Journal of
Mathematical Psychology. 2020;96: 102348. doi:10.1016/j.jmp.2020.102348
57. Friston K, FitzGerald T, Rigoli F, Schwartenbeck P, Pezzulo G. Active Inference: A Process
Theory. Neural Computation. 2017;29: 1–49. doi:10.1162/NECO_a_00912
58. Parr T, Friston KJ. The Anatomy of Inference: Generative Models and Brain Structure.
Front Comput Neurosci. 2018;12. doi:10.3389/fncom.2018.00090
59. Schwartenbeck P, FitzGerald THB, Mathys C, Dolan R, Friston K. The Dopaminergic
Midbrain Encodes the Expected Certainty about Desired Outcomes. Cereb Cortex.
2015;25: 3434–3445. doi:10.1093/cercor/bhu159
60. FitzGerald THB, Dolan RJ, Friston K. Dopamine, reward learning, and active inference.
Front Comput Neurosci. 2015;9. doi:10.3389/fncom.2015.00136
61. Schwartenbeck P, FitzGerald T, Dolan RJ, Friston K. Exploration, novelty, surprise, and
free energy minimization. Front Psychol. 2013;4. doi:10.3389/fpsyg.2013.00710
62. Schwartenbeck P, FitzGerald THB, Mathys C, Dolan R, Kronbichler M, Friston K. Evidence
for surprise minimization over value maximization in choice behavior. Scientific
Reports. 2015;5: 16575. doi:10.1038/srep16575
63. Sengupta B, Stemmler MB, Friston KJ. Information and Efficiency in the Nervous
System—A Synthesis. Sporns O, editor. PLoS Comput Biol. 2013;9: e1003157.
doi:10.1371/journal.pcbi.1003157
22
Neural dynamics under active inference
64. Levy WB, Baxter RA. Energy Efficient Neural Codes. Neural Computation. 1996;8: 531–
543. doi:10.1162/neco.1996.8.3.531
65. Dan Y, Atick JJ, Reid RC. Efficient Coding of Natural Scenes in the Lateral Geniculate
Nucleus: Experimental Test of a Computational Theory. J Neurosci. 1996;16: 3351–
3362. doi:10.1523/JNEUROSCI.16-10-03351.1996
66. Lewicki MS. Efficient coding of natural sounds. Nat Neurosci. 2002;5: 356–363.
doi:10.1038/nn831
67. Chen BL, Hall DH, Chklovskii DB. Wiring optimization can relate neuronal structure and
function. PNAS. 2006;103: 4723–4728. doi:10.1073/pnas.0506806103
68. Raj A, Chen Y. The Wiring Economy Principle: Connectivity Determines Anatomy in the
Human Brain. PLoS One. 2011;6. doi:10.1371/journal.pone.0014832
69. Barlow H. Redundancy reduction revisited. Comput Neural Syst. 2001; 13.
70. Barlow HB. Possible Principles Underlying the Transformations of Sensory Messages.
The MIT Press; 1961. Available:
https://www.universitypressscholarship.com/view/10.7551/mitpress/9780262518420.
001.0001/upso-9780262518420-chapter-13
71. Binder MD, Hirokawa N, Windhorst U, editors. Efficient Coding Hypothesis. Encyclopedia
of Neuroscience. Berlin, Heidelberg: Springer; 2009. pp. 1037–1037. doi:10.1007/978-
3-540-29678-2_2901
72. Denève S, Machens CK. Efficient codes and balanced networks. Nature Neuroscience.
2016;19: 375–382. doi:10.1038/nn.4243
73. Chelaru MI, Dragoi V. Efficient coding in heterogeneous neuronal populations. PNAS.
2008;105: 16344–16349. doi:10.1073/pnas.0807744105
74. Kostal L, Lansky P, Rospars J-P. Efficient Olfactory Coding in the Pheromone Receptor
Neuron of a Moth. PLoS Comput Biol. 2008;4. doi:10.1371/journal.pcbi.1000053
75. Olshausen BA, Field DJ. Natural image statistics and efficient coding. Network. 1996;7:
333–339. doi:10.1088/0954-898X/7/2/014
76. Olshausen BA, O’Connor KN. A new window on sound. Nat Neurosci. 2002;5: 292–294.
doi:10.1038/nn0402-292
77. Simoncelli EP, Olshausen BA. Natural image statistics and neural representation. Annu
Rev Neurosci. 2001;24: 1193–1216. doi:10.1146/annurev.neuro.24.1.1193
78. Karklin Y, Simoncelli EP. Efficient coding of natural images with a population of noisy
Linear-Nonlinear neurons. In: Shawe-Taylor J, Zemel RS, Bartlett PL, Pereira F,
Weinberger KQ, editors. Advances in Neural Information Processing Systems 24.
Curran Associates, Inc.; 2011. pp. 999–1007. Available:
23
Neural dynamics under active inference
http://papers.nips.cc/paper/4384-efficient-coding-of-natural-images-with-a-
population-of-noisy-linear-nonlinear-neurons.pdf
79. Olshausen BA, Field DJ. Emergence of simple-cell receptive field properties by learning a
sparse code for natural images. Nature. 1996;381: 607–609. doi:10.1038/381607a0
80. Olshausen BA, Field DJ. Vision and the Coding of Natural Images: The human brain may
hold the secrets to the best image-compression algorithms. American Scientist.
2000;88: 238–245.
81. Bennett CH. Notes on Landauer’s principle, reversible computation, and Maxwell’s
Demon. Studies in History and Philosophy of Modern Physics 2003. 2003.
82. Landauer R. Irreversibility and Heat Generation in the Computing Process. IBM Journal
of Research and Development. 1961;5: 183–191. doi:10.1147/rd.53.0183
83. Ito S. Stochastic thermodynamic interpretation of information geometry. Phys Rev Lett.
2018;121: 030605. doi:10.1103/PhysRevLett.121.030605
84. Crooks GE. Measuring thermodynamic length. Phys Rev Lett. 2007;99: 100602.
doi:10.1103/PhysRevLett.99.100602
85. Amari S. Natural Gradient Works Efficiently in Learning. Neural Computation. 1998; 36.
86. Wilson HR, Cowan JD. A mathematical theory of the functional dynamics of cortical and
thalamic nervous tissue. Kybernetik. 1973;13: 55–80. doi:10.1007/BF00288786
87. Fricker D, Verheugen JAH, Miles R. Cell-attached measurements of the firing threshold
of rat hippocampal neurones. J Physiol. 1999;517: 791–804. doi:10.1111/j.1469-
7793.1999.0791s.x
88. Marreiros AC, Daunizeau J, Kiebel SJ, Friston KJ. Population dynamics: Variance and the
sigmoid activation function. NeuroImage. 2008;42: 147–157.
doi:10.1016/j.neuroimage.2008.04.239
89. Marreiros AC, Kiebel SJ, Daunizeau J, Harrison LM, Friston KJ. Population dynamics
under the Laplace assumption. NeuroImage. 2009;44: 701–714.
doi:10.1016/j.neuroimage.2008.10.008
90. Friston KJ, Harrison L, Penny W. Dynamic causal modelling. NeuroImage. 2003;19: 1273–
1302. doi:10.1016/S1053-8119(03)00202-7
91. Deco G, Jirsa VK, Robinson PA, Breakspear M, Friston K. The Dynamic Brain: From
Spiking Neurons to Neural Masses and Cortical Fields. Sporns O, editor. PLoS Comput
Biol. 2008;4: e1000092. doi:10.1371/journal.pcbi.1000092
92. Moran R, Pinotsis DA, Friston K. Neural masses and fields in dynamic causal modeling.
Front Comput Neurosci. 2013;7. doi:10.3389/fncom.2013.00057
93. Bishop CM. Pattern recognition and machine learning. New York: Springer; 2006.
24
Neural dynamics under active inference
94. Stone JV. Artificial Intelligence Engines: A Tutorial Introduction to the Mathematics of
Deep Learning. 2019.
95. Aström KJ. Optimal Control of Markov Processes with Incomplete State Information.
Journal of Mathematical Analysis and Applications. 1965;10.
96. Kullback S, Leibler RA. On Information and Sufficiency. Ann Math Statist. 1951;22: 79–
86. doi:10.1214/aoms/1177729694
97. Joyce JM. Kullback-Leibler Divergence. In: Lovric M, editor. International Encyclopedia of
Statistical Science. Berlin, Heidelberg: Springer; 2011. pp. 720–722. doi:10.1007/978-3-
642-04898-2_327
98. Rezende DJ. Short Notes on Divergence Measures. 2018. Available:
https://danilorezende.com/wp-content/uploads/2018/07/divergences.pdf
99. Stachenfeld KL, Botvinick MM, Gershman SJ. The hippocampus as a predictive map. Nat
Neurosci. 2017;20: 1643–1653. doi:10.1038/nn.4650
100. Hafting T, Fyhn M, Molden S, Moser M-B, Moser EI. Microstructure of a spatial map in
the entorhinal cortex. Nature. 2005;436: 801–806. doi:10.1038/nature03721
101. Chen LL, Lin L-H, Green EJ. Head-direction cells in the rat posterior cortex. Experimental
brain research. 1994; 16.
102. Taube J, Muller R, Ranck J. Head-direction cells recorded from the postsubiculum in
freely moving rats. I. Description and quantitative analysis. J Neurosci. 1990;10: 420–
435. doi:10.1523/JNEUROSCI.10-02-00420.1990
103. Stein RB, Weber DJ, Aoyagi Y, Prochazka A, Wagenaar JBM, Shoham S, et al. Coding of
position by simultaneously recorded sensory neurones in the cat dorsal root ganglion:
Coding of dorsal root ganglion neurones. The Journal of Physiology. 2004;560: 883–
896. doi:10.1113/jphysiol.2004.068668
104. Wagenaar JB, Ventura V, Weber DJ. State-space decoding of primary afferent neuron
firing rates. J Neural Eng. 2011;8: 016002. doi:10.1088/1741-2560/8/1/016002
105. Weber DJ, Stein RB, Everaert DG, Prochazka A. Decoding Sensory Feedback From Firing
Rates of Afferent Ensembles Recorded in Cat Dorsal Root Ganglia in Normal
Locomotion. IEEE Trans Neural Syst Rehabil Eng. 2006;14: 240–243.
doi:10.1109/TNSRE.2006.875575
106. Hubel DH, Wiesel TN. Receptive fields of single neurones in the cat’s striate cortex. The
Journal of Physiology. 1959;148: 574–591. doi:10.1113/jphysiol.1959.sp006308
107. Amari S. Information geometry and its applications. Springer; 2016.
108. Ay N, Jost J, Lê HV, Schwachhöfer L. Information Geometry. Cham: Springer
International Publishing; 2017. doi:10.1007/978-3-319-56478-4
25
Neural dynamics under active inference
109. Nielsen F. An elementary introduction to information geometry. arXiv:180808271 [cs,
math, stat]. 2018 [cited 11 Aug 2019]. Available: http://arxiv.org/abs/1808.08271
110. Cencov NN. Statistical Decision Rules and Optimal Inference. 1982.
111. Liang T, Poggio T, Rakhlin A, Stokes J. Fisher-Rao Metric, Geometry, and Complexity of
Neural Networks. arXiv:171101530 [cs, stat]. 2017 [cited 11 Aug 2019]. Available:
http://arxiv.org/abs/1711.01530
112. Cover TM, Thomas JA. Elements of Information Theory. Wiley; 2006.
113. Amari S, Douglas SC. Why natural gradient? Proceedings of the 1998 IEEE International
Conference on Acoustics, Speech and Signal Processing, ICASSP ’98 (Cat
No98CH36181). Seattle, WA, USA: IEEE; 1998. pp. 1213–1216.
doi:10.1109/ICASSP.1998.675489
114. Bernacchia A, Lengyel M, Hennequin G. Exact natural gradient in deep linear networks
and its application to the nonlinear case. Advances in Neural Information Processing
Systems. 2018;31: 5941–5950.
115. Zonghai S, Buhai S. The projection adaptive natural gradient online algorithm for SVM.
Proceedings of the 29th Chinese Control Conference. 2010. Available:
https://www.infona.pl//resource/bwmeta1.element.ieee-art-000005573523
116. Zhang L-, Cichocki A, Amari S. Natural gradient algorithm for blind separation of
overdetermined mixture with additive noise. IEEE Signal Processing Letters. 1999;6:
293–295. doi:10.1109/97.796292
117. Zhang Z, Sun H, Zhong F. Natural gradient-projection algorithm for distribution control.
Optimal Control Applications and Methods. 2009;30: 495–504.
doi:https://doi.org/10.1002/oca.874
118. Duan T, Anand A, Ding DY, Thai KK, Basu S, Ng A, et al. NGBoost: Natural Gradient
Boosting for Probabilistic Prediction. International Conference on Machine Learning.
PMLR; 2020. pp. 2690–2700. Available:
http://proceedings.mlr.press/v119/duan20a.html
119. Mirza MB, Adams RA, Mathys C, Friston KJ. Human visual exploration reduces
uncertainty about the sensed world. PLOS ONE. 2018;13: e0190429.
doi:10.1371/journal.pone.0190429
120. Parr T, Markovic D, Kiebel SJ, Friston KJ. Neuronal message passing using Mean-field,
Bethe, and Marginal approximations. Sci Rep. 2019;9: 1889. doi:10.1038/s41598-018-
38246-3
121. Yedidia JS, Freeman WT, Weiss Y. Constructing Free-Energy Approximations and
Generalized Belief Propagation Algorithms. IEEE Trans Inform Theory. 2005;51: 2282–
2312. doi:10.1109/TIT.2005.850085
26
Neural dynamics under active inference
122. Dauwels J. On Variational Message Passing on Factor Graphs. 2007 IEEE International
Symposium on Information Theory. Nice: IEEE; 2007. pp. 2546–2550.
doi:10.1109/ISIT.2007.4557602
123. Winn J, Bishop CM. Variational Message Passing. Journal of Machine Learning
Research. 2005; 34.
27

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Neural dynamics under active inference: plausibility and efficiency of information processing"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
