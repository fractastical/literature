=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: A coalgebraic perspective on predictive processing
Citation Key: baltieri2025coalgebraic
Authors: Manuel Baltieri, Filippo Torresan, Tomoya Nakai

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Key Terms: copy, process, environment, processing, coalgebraic, coalgebras, predictive, tokyo, than, perspective

=== FULL PAPER TEXT ===

A coalgebraic perspective on predictive processing
ManuelBaltieri1,2,†, FilippoTorresan1,2, TomoyaNakai1
1ArayaInc.,Tokyo,Japan
2UniversityofSussex,Brighton,UK
PredictiveprocessingandactiveinferencepositthatthebrainisasystemperformingBayesian
inferenceontheenvironment. Byvirtueofthis,aprominentinterpretationofpredictiveprocessing
statesthatthegenerativemodel(aPOMDP)encodedbythebrainsynchroniseswiththegenerative
process (another POMDP) representing the environment while trying to explain what hidden
propertiesoftheworldgenerateditssensoryinput. Inthisview,thebrainisthoughttobecome
a copy of the environment. This claim has however been disputed, stressing the fact that a
structural copy, or isomorphism as it is at times invoked to be, is not an accurate description
of this process since the environment is necessarily more complex than the brain, and what
matters is not the capacity to exactly recapitulate the veridical causal structure of the world. In
this work, we make parts of this counterargument formal by using ideas from the theory of
coalgebras,anabstractmathematicalframeworkfordynamicalsystemsthatbringstogetherwork
fromautomatatheory,concurrencytheory,probabilisticprocessesandotherfields. Todoso,wecast
generativemodelandprocess,intheformofPOMDPs,ascoalgebras,andusemapsbetweenthem
todescribeaformofconsistencythatgoesbeyondmerestructuralsimilarity,givingthenecessary
mathematicalbackgroundtodescribehowdifferentprocessescanbeseenasbehaviourally,rather
than structurally, equivalent, i.e. how they can be seen as emitting the same observations, and
thusminimisepredictionerror,overtimewithoutstrictassumptionsaboutstructuralsimilarity. In
particular,wewillintroducethreestandardnotionsofequivalencefromtheliteratureoncoalgebras,
evaluatingtheminthecontextofpredictiveprocessingandidentifyingtheoneclosesttoclaims
madebyproponentsofthisframework.
Keywords: active inference, action-oriented models, coalgebras, bisimulations, behavioural
equivalence
1. Introduction
Predictiveprocessing,anditsmoregeneralformulationknownasactiveinference,havebeenproposed
as a general computational theory to account for the functions of the nervous system [1, 2]. The
proposal’skeyclaimisthatonecanunderstandbrainactivityinitsvariousformsandmanifestations
as resulting from the single imperative of minimising (variational) free energy [3]. Thus, active
inferencepromisestobeaunifiedaccountofcognitionandsentientbehaviour,explaininginparticular
howkeycognitivefunctionssuchasperception,action,andlearningallemergefromasingleprinciple,
i.e. freeenergyminimisation[4,5,6,7,8,9,10].
In this view, the brain is described as a prediction machine [11, 6], and every living organism
is thought to be constantly trying to match or predict incoming sensory inputs produced by the
environment,describedasagenerativeprocess,thatarerelevanttoitself. Ifportionsofthesensorydata
†Correspondencee-mail:manuel_baltieri@araya.org
5202
guA
32
]CN.oib-q[
1v77861.8052:viXra
Acoalgebraicperspectiveonpredictiveprocessing
remainunaccountedfor,thenpredictionerrorsensue. Variationalinferenceisageneralframework
from probabilistic machine learning that, under certain assumptions, reduces to prediction error
minimisation. Variationalfreeenergyisameasurequantifyinghowmuchunexpectedcurrentsensory
inputsare,withrespecttothegenerativemodelanditsupdates,encodedbythenervoussystem(see
the next section for a more formal treatment of these notions). A generative model can be seen as
anapproximate,probabilisticrepresentationofthesurroundingenvironmentencodedbyanagent,
describinghowsensationsandobservationsariseforacertainorganismdependingoncontextand
behaviour (actions) [12, 13, 14, 1]. By encoding updates consistent with an implicit hierarchical
generativemodel,anagent’snervoussystemisthoughttoimplementpredictions,combinedwitha
setofpriorbeliefs,aboutthemostlikelysensoryinputsinacertainenvironment[15,16,17].
But what is the precise relation between the generative process and generative model? In the
literature,thisissomewhatunclear: standardtreatmentsofactiveinferenceandpredictiveprocessing
ofteninvokeanotionakintostructuralsimilaritybetweenthetwo,basedontheideathatanagent
ought to recapitulate the (statistical, or at times causal) structure of the environment [7, 18, 19, 15,
16, 20], and that the two must somehow synchronise [21, 22, 23, 24, 25, 26, 27, 28, 29, 30]. Other
works have on the other hand argued that structural similarity is not necessary, in the sense that
one can formulate action-oriented generative models that do not capture the structural richness of
theirrespectivegenerativeprocesses[11,6,31,32,33,34,35](seealso[26],statingthat“[h]iddenand
externalstatesmayormaynotbeisomorphic[...]”).
Tobetterexplaintheperspectiveinvokedbythelatter,largelybasedoninformalaccounts[11,6]
and simulation work [31, 32, 33, 34, 35], we reformulate the intuition behind it using the theory of
coalgebras,treatinggenerativeprocessandmodelsascoalgebras,andexploringtheirrelationinterms
ofspecificmapsbetweenthem. Coalgebrasareastandardtoolusedintheoreticalcomputerscience
andmathematics(especiallycategorytheory)[36,37]forthetreatmentofgeneraldynamicalsystems.
Their structural formulation puts an emphasis on objects (dynamical systems) and maps between
them(homomorphisms)thatmustsatisfycertainconditions. Thankstotheirabstractdefinitions,their
expressivepowerisquitebroadandwillallowustoeasilybringoutexamplesofgrowingdifficulty
for different transition types (with inputs, outputs or both, terminal states, etc.) and branching in
dynamicalsystems(deterministic,possibilisticandprobabilistic),focusingintheendontheparticular
implementationsrelevantforthiswork: POMDPsinactiveinference,andbehaviouralequivalence
betweenthem.
InSection2,weprovideastructuraloverviewofactiveinferencewithafocusonitscoreworking
parts and its applications to perception, i.e. predictive processing. Section 3 offers a short, self-
containedintroductiontocoalgebrasandtheirmaps,withanemphasisondefinitionsofbisimulation
(equivalence),kernelbisimulationandbehaviouralequivalenceandafewstandardexamplesoftheir
uses. InSection4,webringthesetwopartstogether,withPOMDPsinactiveinferenceformulatedas
coalgebras,andtheirrelationasakindofconsistencybetweencoalgebrasusingtheaforementioned
definitions.
2. Predictive processing, an overview
Whileafulltreatmentofactiveinferenceremainsoutsidethescopeofthepresentmanuscript,inthis
sectionweintroducethemainmotivationsbehindthisframework,whichwillallowustoformally
discussitsmainstructuralcomponents,showingconnectionsbetweenactiveinferenceandthetheory
ofcoalgebras. Forsometechnicaltreatmentsandreviews,seee.g.[38,39,40,41,26,42,29].
2
Acoalgebraicperspectiveonpredictiveprocessing
Fig.1: Activeinferencesetup. Brain-body-environmentfactorisationofanagent.
2.1. Thestructureofactiveinferenceproblems
Inastandardactiveinferencesetup,wehaveanagentactivelyinteractingwithanenvironment. The
agentcanbedescribedasasystemfactoredintotwocomponents: abrainandbody,asetuptypical
ofembodiedapproachestobiologyandcognitivescience[43,44],seeFig.1. Notethatinprinciple,
theseneednotbea“brain”anda“body”inastrictsense;onecanforinstanceimagineanE.Coli’s
signallingpathway[45,46]toplaytheroleofabraininthissetup.
Environment Focusingonthediscrete-timetreatments [40,41,47,26,42,29],therelevantpartof
theenvironmentwithwhichanagentinteracts,andwhichgeneratesobservationscomingthroughits
sensors,isreferredtoasgenerativeprocess. Agenerativeprocessisusuallyformulatedasapartially
observableMarkovdecisionprocess[48,49,50],andrepresentsthegroundtruthforanagent:
Definition1(PartiallyobservableMarkovdecisionprocess(POMDP)). ApartiallyobservableMarkov
decisionprocessisatuple(S,A,T,O,M),where:
• Sisthestatespace,
• Aistheactionspace,
• T : S×A → P(S)isthetransitionsdynamics,where P(S)isthesetofdistributionsoverSwith
finite support such that for a given state s and a , T(s ,a ) gives a probability distribution of
t t t t
statesanagentcantransitiontofromstates t whiletakingactiona t ,oftenwrittenas p(s t+1 | s t ,a t ),
• Oistheobservationspace,
• M : S → P(O)istheobservationmapwhere P(O)isthesetofdistributionsoverOwithfinite
supportsuchthatforagivenstates , M(s )givesaprobabilitydistributiononobservations p(o ).
t t t
Notethatthiscorrespondstoa“POMDPwithoutrewards”. AmoregeneraldefinitionofPOMDP
includesinfact:
• γ ∈ [0,1)isadiscountfactor,
• r : S×A → R ,amapgivingarewardeverytimeatransitionistaken.
3
Acoalgebraicperspectiveonpredictiveprocessing
This is because generative processes in active inference do not include a reward function (and
thus no discount factor either). One could say that rewards are effectively folded into O, seen as
observation-rewardpairsO := O′×R . However,itisstandardpracticeinactiveinferencetoavoid
rewardfunctions,infavourofpriorson(desired)outcomes/observations[51,52,53,54,47,29]. See
also [55] where, under the assumption that an agent has access to preferences that encode exactly
rewardingstates,aformulationcompatiblewithstandard(PO)MDPscanbederived.
Body Thebodyisoftenrepresentedasaninterface, equivalenttochannelsthatcoupletheagent
to the environment, in terms of sensors and actuators, forming what is usually referred to as an
action-perceptionlooporsensorimotorloop,seeforinstance[56,57]. Intheactiveinferenceliterature,
thesecomponentsconstitutetheso-called“Markovblanket”ofanagent[58,29,30],depictedinFig.1
asa“Fristonblanket”instead,followingargumentsfoundin[59](seealso[60,61,62,63]).
Brain The role of the brain in this framework is to perform inference about unknown properties
(states,parameters)oftheenvironment. Itactsasapredictionmachine,asisoftenhighlightedinthe
fieldofcognitive(neuro)science,generatingthesameobservationsitreceivesfromtheenvironment.
Thisisachievedbydescribingbrainstatesasparametrisingdistributionsofstatesandparameters
ofagenerativemodel,withtheirdynamicsbeingconsistentwithbeliefupdatesinaBayesianframe-
work. Generativemodelsareastandardcomponentof(Bayesian)machinelearningsetups[64,65],
consistingofajointprobabilitydistributionofobservationsemittedbytheenvironment,andhidden
variables/parameters that generated them. We note that, under this interpretation, the brain does
notcontain,have,orevenisagenerativemodel. Instead,brainstatesandtheirdynamicsimplement
a scheme consistent with update equations that could be interpreted [66, 67] as the brain implicitly
havingsuchamodel[68,69].
In this context, a generative model consists of another POMDP [40, 47, 26, 42, 29], whose goal
is to approximate (and in an ideal, perfect model scenario, to match) in some sense the generative
process,anotherPOMDP.ThisparticularPOMDPoughttohavethesameinterfaceasthegenerative
processPOMDP.Accordingtoactiveinference,anagent’sgoalisforitsgenerativemodeltohavethe
sameinputs,i.e. actionsgeneratedfortheenvironment,eitherbyhavingacopyofthem(efference
copy)orbyinferringthem(asinmorestandardactiveinferencesetups,see[10]andFig.1),andthe
sameoutputs,i.e. predictionsofobservationsthatminimisevariationalfreeenergy/predictionerror
(seeFig.1).
This setup is consistent with model-based approaches to reinforcement learning [50] since the
agentoperatesontheassumptionsinducedbythegenerativemodel. Computationally,however,the
goalsandalgorithmicimplementationsofthesetwoapproachesareusuallyquitedifferent,although
morerecentlyconvergingtosimilarideas[70,71,54,72,55,73],mainlythefreeenergy(andexpected
freeenergy)minimisation.
2.2. Freeenergyminimisationandpredictiveprocessing
Oneofthemaingoalsofanactiveinferenceagentistoinferthemostlikelyenvironment’sconfig-
urationgivenasequenceofobservations. ThetaskisformalisedintermsofapproximateBayesian
inference via variational Bayes. To see the core idea, consider the application of Bayes’ rule to a
POMDPwithintheactiveinferenceframework:
P(O |S ,π,A,B)P(S ,π,A,B)
P(S ,π,A,B|O ) = 1:T 1:T 1:T , (1)
1:T 1:T P(O )
1:T
4
Acoalgebraicperspectiveonpredictiveprocessing
whereπ isapolicy,whileA,Bareparametersforobservationandtransitionmaps,respectively. In
general,thiscannotbesolvedanalytically. Therefore,inactiveinference,theupdateoftheagent’s
probabilisticbeliefsisperformedviaanoptimisationprocedureinvolvingaquantitycalledvariational
freeenergy. Variationalfreeenergyisdefinedintermsofaprobabilitydistributionknownasthevaria-
tionalorapproximateposterior,Q(S ,π,A,B),whichapproximatesthetrueposteriorinequation1,
1:T
givingthefollowing:
(cid:104) (cid:105)
F (cid:2) Q(S ,π,A,B) (cid:3) := E logQ(S ,π,A,B)−logP(O ,S ,π,A,B) . (2)
1:T Q 1:T 1:T 1:T
It can be shown that minimising variational free energy with respect to the parameters of those
approximateposteriorsisequivalenttoperformingapproximateBayesianinference. Asaresult,the
variationalposteriorsbecomemorealignedwiththeexactposteriorsgiventhegenerativemodelthat
wewereaimingtocomputeinthefirstplace. Thisisexpressedmoreconciselybythefollowing:
(cid:16) (cid:13) (cid:17)
D Q(S ,π,A,B) (cid:13) P(S ,π,A,B|O ) = F (cid:2) Q(S ,π,A,B) (cid:3) +logP(O ), (3)
KL 1:T (cid:13) 1:T 1:T 1:T 1:T
showingthatminimisingvariationalfreeisequivalenttominimisingtheKLdivergencebetweenthe
variational posterior and the exact (analytical) posterior, at least up to another term: the negative
surprisal,logP(O ). Wenotethatthesurprisal−logP(O )canplaydifferentrolesdependingon
1:T 1:T
the setup. In (machine) learning and perception, it is assumed to be constant, given by a fixed set
ofobservationsO thatdonotchangeovertime. However,whenactionisintroduced,combining
1:T
perceptionandactionasinactiveinference,thesurprisalitselfcanchangebyselectivelysampling
observations O through the choice of particular action policies. In this work, we will focus on
1:T
predictive processing, which primarily involves perception and learning processes in which the
surprisalremainsconstantovertime[5,11,74,75],andleaveactiontofuturework.
Incontrasttothefullexpressionofequation1,freeenergyisaquantitythatcanbeevaluated,given
thespecificationofagenerativemodelandthevariationalposterior,thusallowinganagenttouse
collecteddata/observationstoupdateitsprobabilisticbeliefs. Equation(3)highlightsanimportant
aspect of this variational formulation: the starting point is the minimisation of the KL divergence
betweentheapproximateandrealposteriors,whichisachievedviaminimisingfreeenergyunderthe
assumptionthatthesurprisalisassumedisfixed. However,thisdoesnotyetspecifywhatkindof
relationoughttobeinplacebetweenthe(implicit)generativemodelencodedbythebrain,andthe
generativeprocessrepresentingthegroundtruthoftheenvironment.
Inparticular, minimisingthefreeenergyinEq.(2)onlyimpliesthattheapproximateposterior
ought to be as close as possible to the true posterior obtained from the generative model given
observationsO . HowdoweensurethenthatthisprocessofapproximateBayesianinferenceis,in
1:T
somesense,veridicalwithrespecttotheunderlyinggenerativeprocess,i.e. thatagenerativemodel
isappropriateforagenerativeprocess? Inotherwords,howdoweknowthatanagentminimising
variationalfreeenergywillsomehowbesuccessfulinagiventheenvironment?
Toanswerthis,westartwithasimpleobservation: ifagenerativemodelisinsomemeaningful
waywrong,thenfreeenergycannotbeminimisedtoasatisfactorylevel. Thismeansthatrepeated
attemptstoimplementprocessesofperception,learning,planning,andactionselection,combined
in a sensorimotor loop that aims to minimise free energy across time scales, are not sufficient for
an agent to fulfil its preferences. Active inference thus posits that further strategies should be in
placeinsuchcases,including,forinstance,modelselection[76]andstructurelearning[77],sothata
bettergenerativemodelcanbeobtained. Theformerreferstoselectingagenerativemodelfroma
5
Acoalgebraicperspectiveonpredictiveprocessing
pre-definedspaceofpossiblemodelsthatbettermatchtherequirementsofa“good”generativemodel,
onethatallowsanagenttofulfilitsgoals. Thelatterinvolvescombiningexpansionandreduction
processes, which respectively create and remove variables in a generative model until it can more
correctlybeusedtoachieveacertaingoal.
While the technical details are not particularly relevant in this work, the key message is that a
generativemodelisonlyasgoodastheperformanceitenablesanagenttoachieveinthepursuitofits
preferences. Inotherwords,thegenerativemodelmustbecloseenoughtothegenerativeprocessto
capturetherelevantaspectsoftheworldthataffecttheagent’sabilitytorealiseitspreferences. Only
thendoesitmakesensetospeakofminimisingfreeenergyforperception,learning,planning,and
actionselection. Nextweexaminehowthepredictiveprocessingliteraturehasinterpretedthisideaof
closeenoughindifferentways.
2.3. Synchronisationofgenerativemodelandgenerativeprocess?
Thereissomeconsensusintheactiveinferenceandpredictiveprocessingliteratureontheideathat
the generative model of the agent recapitulates the (statistical, or at times causal) structure of the
generativeprocessoftheenvironmentitrefersto[7,18,19,26,78]. However,theexactmeaningof
thisstatement,andtheextenttowhichthisoughttohold,arenotentirelyclear[79,31,34,80,59,62],
partlyduetotheambiguityconcerningwhatconstitutescausalstructure(see,e.g.[81]),andpartly
becauseofthedistinctionbetweeninternalandexternalstates[59,82]. Inanattempttoformalisethis
idea,differentworks[21,22,23,24,25,26,27,28,29,30]havearguedthat,undercertainassumptions,
mainlytheexistenceofasynchronisationmap[83],freeenergyminimisationentailsasynchronisation
between an agent’s internal states and external environment states. This synchronisation can be
regardedasakindofmirroringbetweeninternalandexternalstates.
Ontheotherhand,severalworkshaveargued[11,6]anddemonstrated[31,32,33,34,35]that
thisstructuralsynchronisationneednot,infact,hold. Onecanhavegenerativemodels,oftencalled
action-orientedinthisareaofresearch,thatdonotrecapitulatethestructuralrichnessoftheirrespective
generativeprocesses. Thisisalsonowlargelyacceptedbystandardtreatmentsofactiveinference,
e.g. [26], stating that “[h]idden and external states may or may not be isomorphic [...]” and that
“anagentusesitsinternalstatestorepresenthiddenstatesthatmayormaynotexistintheexternal
world”.
Toreconciletheseseeminglydivergentviews,namely,that1)agenerativemodelanditsrespective
generativeprocessmustsynchronise,andthat2)thestructureofthegenerativemodelcandifferfrom
thatofthegenerativeprocess,wetakeaperspectivedualtothestructuralone: aperspectivethatputs
behaviourfirst. Theterm“behaviour”tendstoassumedifferentconnotationsindifferentfields. In
psychology,forinstance,itisoftenassociatedwithbehaviouralresearch,focusingontheobservable
behaviour of a subject, its conditioning, and its interactions with the environment, in contrast to
cognitiveapproachesthatemphasiseinternalprocessessuchascognitionandemotion. Inthiswork,
wedonotengagewiththiskindofdebate,instead,weoperationaliseobservablebehaviourasoutputs
ofasystemovertime.
Thisoperationalisationstemsfromthewell-knowndualitybetweenstructureandbehaviourin
theoreticalcomputerscienceandmathematics[36,37],wherealgebrasaretakenasalanguagebest
suitedtodescribestructure,whilecoalgebrasasalanguageforthebehaviourofsystems. Whilethisis
bynomeanstheonlywaytoconceptualisesystemsandbehaviours,itisaconvenientapproachthat
clearlyhighlightshowtobuildabehaviouralunderstandingofasystem,startingfromstructural(i.e.
algebraic)notionsandadaptingthem(reversingarrows)bycategoricalduality[84]. Inthenextsection,
6
Acoalgebraicperspectiveonpredictiveprocessing
wewillprovideabriefoverviewofcoalgebras,andtheirroleasagenerallanguageforstate-based
systems. Thisincludesbothtransitiontype(dynamicsandtheirpossibleeffectssuchastermination,
outputs,andinputs)andbranching(e.g. deterministic,probabilistic,orpossibilisticsystems)within
thesameframeworkinaconvenientandformalway[85].
3. Categories, coalgebras and bisimulations
Inthissectionweprovideabriefoverviewofcategoricalideasthatleadtoanabstracttreatmentof
dynamicalsystemsand mapsbetweenthem. This frameworkwilllaterallowus toapplyexisting
notionsofbehaviouralconsistencytoprocessesofvariouskind,focusinginourparticularsetupon
POMDPs. Throughoutthiswork,wheneverwerefertoa“system”wedosoundertheassumption
thatasystemisacoalgebra. Whilethereareotherdoctrinesandtheoriesofsystemsbasedonvarious
otherdefinitions,seeforinstance[86,87,88],thesearebeyondthescopeofthecurrentworkandwill
notbecoveredhere.
3.1. Categoriesandfunctorsbetweenthem
Thefirstcrucialideainvolvesthedefinitionofcategory,anabstractcollectionofobjects,andmaps
betweenthemthatarerequiredtosatisfycertainrules[84,89].
Definition2(Category). AcategoryCconsistsof:
• aclassofobjects,ob(C),e.g. A,B,C,...,
• a class of maps, arrows or morphisms, hom(C), between objects (sources and targets), e.g.
f,g,h,...,usuallyrepresentedas f : A → B, g : B → C,h : C → D,...,
• foreachobjectofob(C),anidentitymorphismid : A → A,
A
• abinaryoperation representingthecompositionofmorphisms,e.g. giventhemorphisms f,g
above, f g = A →(cid:35) C,thatsatisfiesthefollowing1,
(cid:35)
– associativity,given f,g,habove, f (g h) = (f g) h,
(cid:35) (cid:35) (cid:35) (cid:35)
– left and right unit laws, for every pair of objects A,B and morphism f : A → B, id f =
A
f = f id . (cid:35)
B
(cid:35)
Ofparticularinterestforthisworkthen,aswellasforseveralotherresultsinthefieldofcategory
theory,istheconceptoffunctor,a“categorification”(theprocessofreplacingsetswithcategoriesto
generaliseset-theoreticdefinitions)ofthestandardnotionoffunctionbetweensets.
Definition3(Functor). LetCandDbecategories. A(covariant)functorF fromCtoDisamapping
that
• associateseachobject X inCtoanobjectF(X)inD,
• associateseachmorphism f : X → Y inCtoamorphismF(f) : F(X) → F(Y)inDsuchthat
thefollowingtwoconditionshold:
– F(id ) = id foreveryobject X inC(functorsmustpreserveidentity),
X F(X)
– F(f g) = F(f) F(f) for all morphisms f : X → Y and g : Y → Z in C (functors must
(cid:35) (cid:35)
preservecompositionofmorphisms).
1Notethat◦isalsooftenuseforcomposition,with f g = g◦ f,wedecidedhowevertoadopt aswethinkithelps
(cid:35) (cid:35)
followingtheorderofacomposition.
7
Acoalgebraicperspectiveonpredictiveprocessing
Forthetheoryofcoalgebras,whichwebrieflyoverviewnext,weneedtoconsideraspecialcaseof
functor,calledanendofunctor,i.e. afunctorfromacategoryCtoitself.
3.2. Processesascoalgebras
Coalgebras,aconstructionfromcategorytheoryrelatedtoalgebras(whicharetheirdualinacategori-
calsense[84]),haveinrecentyearsbecomeapopularapproachforthestudyofgeneraldynamical
systemsandautomata[36,37]. Technically,givenacategoryCandanendofunctorF : C → C,we
defineanF-coalgebra(orsimplycoalgebra,whenF isunderstood)asanobjectSofCtogetherwith
a map fS : S → F(S), represented as (S, fS), where F is the type or signature of the coalgebra, S
is the carrier, and fS is the (transition) structure map of the coalgebra. Given a category C and an
endofunctorF,wecanbuildacategoryofcoalgebraswithF-coalgebrasoftheform(S, fS)asobjects,
andmorphismsbetweenthemasmaps.
Definition 4 (Category of F-coalgebras). Coalg (F) is the category of F-coalgebras with objects
C
F-coalgebrasandmapscalledF-homomorphisms,coalgebrahomomorphismsorsimplyhomomor-
phismwhenthecontextallowsit. Givenobjects(S, fS),(S′, fS′)inCoalg (F),anF-homomorphism
C
ϕisamapthatmakesthefollowingdiagramcommute:
S ϕ S′
fS fS′ (4)
F(S) F(S′)
F(ϕ)
i.e. suchthat fS F(ϕ) = ϕ fS′ . TheidentityisgivenbythetrivialstructuremapS − id →S Sbetweenthe
(cid:35) (cid:35)
underlyingsets. Compositionisdefinedbyplacingcommutingsquaressidebyside,andassociativity
isestablishedbyverifyingthattheorderinwhichtheycommuteisnotrelevant.
Inthiswork,wefocusexclusivelyoncoalgebraswherethebasecategoryisC = Set,thecategory
whoseobjectsaresetsandwhosemorphismsarefunctions. Accordingly,ratherthanusingthesome-
whatbloatedCoalg (F),wewillsimplifythenotationtoCoalg(F)forthecategoryofcoalgebras
Set
fortheendofunctorF onthecategorySet. Asourtreatmentlargelyreliesonpre-existingknowledge
andintuitionsaboutsetsandfunctions,mostothertechnicaldetailsnecessaryforafullcategorical
accountofcoalgebraswillbeskipped.
Todevelopamoreintuitiveunderstandingofcoalgebrasforthepurposeofthiswork,i.e. discrete
dynamicalsystems,wenextintroduceafewstandardexamplesanddefinitionspresentedwithinthis
framework. Weinitiallyfollowintroductorytreatmentsatfirst[36,90],andlaterreferto[91]foran
examplespecificallyrelevanttothepresentwork(i.e. involvingprobabilities).
3.2.1. Deterministicsystemsascoalgebras
Deterministicsystemsconstitutethesimplestclassofdynamicalsystems. Whilethesesystemshavea
trivialbranching(i.e. onlyonepossiblenextstate),theirtransitiontypescanfollowdifferentrules.
Hereweconsiderbothclosedandopensystemsasillustrativeexamplesofhowdifferentinterfaces
canberepresented.
8
Acoalgebraicperspectiveonpredictiveprocessing
Example 5 (Category of closed systems as coalgebras). The category of closed systems as coalgebras
Coalg(Id),withcoalgebratypegivenbytheidentityfunctorId : Set → Set,has
• asobjects,coalgebrasoftheform(S, fS : S → S),and
• as morphisms between two coalgebras (S, fS) and (S′, fS′), homomorphisms in the form of functions
ϕ : S → S′ thatmakethefollowingdiagramcommute
S ϕ S′
fS fS′ (5)
S S′
ϕ
Notethat“closed”hererefertothefactthatthesesystemshavenoinputs/outputs,thatis,they
areautonomous(noinputs)andhavenoobservableoutputs,andthuscannotcommunicatewiththe
outsideworld(i.e. theirinterfacesaretrivial). This,however,doesnotpreventusfromdescribing
mapsthattrackconsistentupdatesbetweendifferentclosedsystems,mapsthatpreservetransitions,
i.e. (F-)coalgebrahomomorphisms.
Ontheotherhand,Mooremachinesareclassicalarchitecturesintheautomatatheoryliterature,
correspondingtosystemswithinputsandoutputs,oropendiscrete-timedynamicalsystems. Formally,
aMooremachineisrepresentedasaquintuplewithstates, inputs, outputs, transition, andoutput
functions(S,I,O,β : I×S → S,δ : S → O).
Example6(CategoryofMooremachinesascoalgebras). ThecategoryofMooremachinesascoalgebras
Coalg(Moore),withcoalgebratypegivenbythefunctorMoore : Set → SetsuchthatMoore(S) = O×SI,
has
• asobjects,coalgebrasoftheform(S, fS : S → O×SI),whereSI isthesetofallfunctions I → S(see
Moore
belowforsomeexplanationofthisnotation),withtransitions fS = ⟨outS ,trS ⟩givenby
Moore Moore Moore
outS : S → O
Moore
trS : S → SI, (6)
Moore
and
• as morphisms between two coalgebras (S, fS ) and (S′, fS′ ) with the same inputs and outputs,
Moore Moore
coalgebrahomomorphismsintheformoffunctions ϕ : S → S′ (sinceinputsandoutputsarethesame
forthetwosystems,therearesimpleidentityfunctionsbetweenthem)thatmakethefollowingdiagram
commute
S ϕ S′
fS fS′ (7)
Moore Moore
O×SI O×S′I
id
O
×(ϕ)I
Thenotationforthesetofallfunctions I → S,SI = {f | f : I → S},impliesthattrS sendsanelement
Moore
s ∈ Stoafunction f = trS (s) : I → Sassigninganextstates˜ := trS (s)(i) ∈ Stoeachinputi ∈ I,
Moore Moore
9
Acoalgebraicperspectiveonpredictiveprocessing
∼
see[90]. Importantly,asnotedby[36]andreferencestherein,thereisabijection{h | h : I×S → S} = {g |
g : S → SI}(cf. “currying”)2,andtherefore{h | h : I×S → O×S} ∼ = {h | h : S → O×SI}.3
3.2.2. Non-deterministicsystemsascoalgebras
Nondeterministicautomataprovideanexampleofnondeterministicopendiscrete-timedynamical
systems. Thesesystemshaveanon-trivialbranching: foreachstate,thereisasetofpossiblenextstates,
andthissetcanbeempty,meaningthatfromastatetherearenotransitionallowed. Theirtransition
typescanalsobeofdifferentkinds(closed,open,withorwithoutaterminalstate,etc.),butherewe
focusexclusivelyonopensystems(Mooremachines)thatincludefinalstates(statesfromwhichthere
arenopossibletransitions). Thisexampleisalsorelevantforthenextsection,wherewewillintroduce
yetanothertypeofbranching: aprobabilisticone. Alltheseexamplescanbeviewedasspecialcases
of“structuredMooremachines”[92],wherethetransitiontype(withinputsandoutputs,specifiedby
afixedfunctor, F)isthesame,whilethebranching(explainedintermsofamonad,T)canbedifferent.
TodefinepossibilisticMooremachines,werecallthedefinitionofthefinitepowersetfunctor. 4
Definition7(Finitepowersetfunctor). Thefinitepowersetfunctor,P : Set → Setisdefinedas
fin
P (X) = {U ⊆ X | U isfinite}. (8)
fin
Forafunction g : X → Y,the(pushforward)mapP (g) : P (X) → P (Y)isdefinedas
fin fin fin
P (g)(a) = g[U], (9)
fin
where g[U] = {g(a) | u ∈ U}.
Usingthisfunctor,wecangivethefollowingdefinition.
Example 8 (Category of nondeterministic Moore machines as coalgebras). The category of nonde-
terministic Moore machines as coalgebras Coalg(P Moore), with coalgebra type given by the functor
fin
P Moore : Set → SetsuchthatP Moore(S) = P (O)×P (S)I,has
fin fin fin fin
• asobjects,coalgebrasoftheform(S, fS : S → P (O)×P (S)I),withtransitions fS =
P Moore fin fin P Moore
fin fin
⟨trS ,outS ⟩givenby
P Moore P Moore
fin fin
outS : S → P (O)
P Moore fin
fin
trS : S → P (S)I, (10)
P Moore fin
fin
and
• asmorphismsbetweentwocoalgebras(S, fS )and(S′, fS′ )withthesameinputsandoutputs,
P Moore P Moore
fin fin
coalgebrahomomorphismsintheformoffunctions ϕ : S → S′ (sinceinputsandoutputsarethesame
2NotethatmapsI×S→SarenotoftypeS→F(S),whilemapsoftypeS→SIare,andsoarewelldefinedcoalgebras.
3ThisistrueforMooremachines,butnotforallkindsofopensystems,e.g.MealymachineswhereOalsodependsonI.
4Thefiniteversionofthisfunctorhasespeciallyniceproperties[37],andisinpracticemoreoftenadoptedinplaceofits
infinitecounterpart.
10
Acoalgebraicperspectiveonpredictiveprocessing
forthetwosystems,therearesimpleidentityfunctionsbetweenthem)thatmakethefollowingdiagram
commute
S ϕ S′
fS fS (11)
PfinMoore PfinMoore
P (O)×P (S)I P (O)×P (S′)I
fin fin fin fin
P
fin
(id
O
)×P
fin
(ϕ)I
Wenotethatthisformulationiscloselyrelatedtotheideaofpossibilistic[93]systems,definedfor
the nonempty powerset monad given by P +(X) = P (X)\{∅}, which exclude the case where
fin fin
therecanbenopossibletransitionfromagivenstate.
3.3. Comparingprocesses
Acorefeatureofthecoalgebraicapproachtodynamicalsystemsisitsfocusonmapsbetweensystems,
placingthematthecenterofthetheory’sdevelopment. Thesemapsarerelevant,forinstance,inthe
analysisofconcurrentprocessesintheoreticalcomputerscience[36,94],whereaprimarygoalisto
define notions of equivalence for processes based on their observable behaviour, contrasting with
standardconceptsofequivalencebasedonstructuralsimilarityofdifferentprocesses[95]. Theyare
alsoofinterestinotherfields,wheretheyappearinspecialisedformssuchas“homomorphisms”[96,
97],“coarse-grainings”[98],“variableaggregation”[99],“stateaggregation”[100],“lumpability”[101],
“modelreduction”[102]or“dynamicalconsistency”[103],amongothers.
Thesespecialisedmapscanbetracedbacktotheideaofamodel,inparticular,towhatitmeans
for a system to model another system. The standard definition of model relies on epimorphisms,
ageneralisationofsurjectivityforfunctionsbetweensets. Inthecaseofcoalgebras,ittellswhena
coalgebracanbespecifiedasacoarse-grainingofanother.
Definition9(Modelsincoalgebras). LetCoalg(F)beacategoryofcoalgebras. Ahomomorphism
ofcoalgebrasϕ : S → S′ from (S, fS : S → F(S))to(S′, fS′ : S′ → F(S′))isanepimorphisminthe
categoryCoalg(F)ifandonlyifϕisasurjectivefunctionbetweentheunderlyingsets [37,Theorem
3.3.4]. Followingtheconventionadoptedin[68],anepimorphismofcoalgebrasϕiscalledamodel,
whereas(S, fS)iscalledthereferent,i.e. whatthemodelrefersto,and(S′, fS′)thereferrer,i.e. what
referstothemodel.
Intuitively,thisdefinitionstatesthatdifferentelementsofthefirstcoalgebra,(S, fS),aremappedto
thesameelementofthesecondcoalgebra,(S′, fS′). Moreprecisely,theexistenceofamodelϕ : S → S′
impliesthatforeachstates′ ∈ S′,thereexistsasetofstatesϕ−1(s′) ∈ Softhereferent(S, fS),called
thefibreofs′,whichrepresentsasubsetofelementsofSthatareindistinguishablefromtheperspective
of the simpler coalgebra, the referrer (S′, fS′), as they all map to the same element s′ ∈ S′ via the
surjectivefunctionϕ. Furthermore,ass′ ∈ S′ variesalongthetransitionfunction f′,thisvariationis
consistentwiththevariationdescribedbythefunction f foreachelementsofthefibreϕ−1(s′)ofs′.
Anequivalentdefinition,see[37,Theorem3.3.4],canbegivenviatheuseofbisimulationequivalences,
buildingontheconceptsofspansandrelations.
Definition10(Spansandrelationsbetweensets). AspanbetweenthesetsXandYisatriple(V,p ,p )
1 2
where V isasetand p : V → X and p : V → Y aretwofunctionswiththesamedomain, V. The
1 2
pair(x,y) ∈ X×Y isrelatedby(V,p ,p )ifthereexistsav ∈ V suchthat p (v) = xand p (v) = y.
1 2 1 2
11
Acoalgebraicperspectiveonpredictiveprocessing
Arelation Risajointlymonicspan,i.e. aspanwhere p ,p arejointlyamonomorphism(aninjective
1 2
function in Set), R − (p −1− ,p →2 ) X×Y, given by v (cid:55)→ (p (v),p (v)), or in other words, given any two
1 2
functions f,g : W → V, f p = g p and f p = g p implythat f = g.
1 1 2 2
(cid:35) (cid:35) (cid:35) (cid:35)
Definition 11 (Bisimulation equivalence). Given an F-coalgebra (S, fS) an equivalence relation
B ⊆ S×S is said to be an F-bisimulation equivalence [36] if there exists an F-coalgebra structure
γB : B → F(B)suchthatthefollowingdiagramcommutes
S
πS
B
πS
S
fS ∃ γB fS (12)
F(S) F(B) F(S)
F(πS ) F(πS )
i.e. suchthattheprojection π isancoalgebrahomomorphism. Moregenerally,abisimulation
S
equivalenceisattimesdefinedasaspanofcoalgebras,i.e. aspan(B,π ,π )betweentheunderlying
S S
sets,SandS(itself),thatmakestheabovediagramcommute[37].
To get an intuition for how this relates to the modelling perspective of Definition 9, recall that
amodelisanepimorphismofcoalgebras, i.e. asurjectivefunctionbetweentheunderlyingsetsof
twocoalgebras(S, fS)and(S′, fS′). Recallalsothateverysurjectivefunction f : A → Binducesan
equivalencerelationon A, R ⊆ A×Aandconversely,everyequivalencerelation Ron Ainducesa
quotientmapping f : A → A/R,whichissurjective,where A/Risthequotientsetwithelements
R
equivalenceclassesofelementsof A. Inthissense,anequivalencerelationistoasurjectivefunction
what a bisimulation equivalence is to a coalgebra epimorphism: a bisimulation equivalence is an
equivalencerelation BonSwithextrastructure,i.e. anequivalencerelationthatpreservescoalgebra
transitions,andacoalgebraepimorphismisasurjectivefunctionbetweentheunderlyingsetsoftwo
coalgebras,SandS′,withextrastructure,i.e. asurjectivefunctionthatpreservescoalgebratransitions.
Thisinterpretationhasrecentlyreceivedincreasingattentioninmachineandreinforcementlearning,
seeforinstance[104,105,97],wherebisimulationequivalencesaresimplyreferredtoasbisimulations.
Following[106,107,37],weextendtheabovedefinitiontorelations(notequivalencerelations)of
theform B ⊆ S×S′ betweendifferentsetsofstates S and S′ andbuildadefinitionofbisimulation
betweendifferentprocessesformalisedascoalgebras.
Definition12(Bisimulation). Giventwo F-coalgebras (S, fS),(S′, fS′),arelation B issaidtobean
F-bisimulation[36]between(S, fS)and(S′, fS′)ifthereexistsanF-coalgebrastructureγB : B → F(B)
suchthatthefollowingdiagramcommutes
S πS B π S′ S′
fS ∃ γB fS′ (13)
F(S) F(B) F(S′)
F(πS ) F(π S′)
i.e. suchthatπ S ,π S′ arecoalgebrahomomorphisms. Again,wecangeneralisethisdefinitiontostate
thatabisimulationisaspanofcoalgebras.
12
Acoalgebraicperspectiveonpredictiveprocessing
Unlike the case of bisimulation equivalences, i.e. equivalence relations of type R ⊆ A× A
that correspond to models and coalgebra epimorphism given in Definition 9, for relations of type
R ⊆ A×Bthecorrespondenceislessobvious. Thisisprimarilybecauserelationsoftype R ⊆ A×B
donotsimplyinduceasurjectivefunction,seealsoDiscussionand[108].
An alternative approach to understanding the more general notion of bisimulation is through
thelensesofbehaviouralequivalence. However,toestablisharigorousnotionofbehaviouralequiva-
lence,wefirstneedtodefinecocongruences,kernelbisimulationsandfinalcoalgebras. Althoughin
someotherworks,“behaviouralequivalence”,“cocongruence”and“kernelbisimulation”areused
interchangeably,hereweadopttheformalcharacterisationgivenin[109],whichdistinguishesthese
notions by, roughly, stating that behavioural equivalence is a kernel bisimulation that uses a final
coalgebra, and kernel bisimulation is a cocongruence with an associated (pullback) relation. See
also[110,Chapter3.5]foradetailedbreakdown.
Thedefinitionofcocongruenceturnsouttobeequivalenttothatofbisimulationinoursetup,i.e.
using only weak pullback-preserving functors on Set as the base category [111] and [37, Theorem
4.5.3]. Cocongruencesin[112](orbehaviouralequivalencein[37])buildonthedual5 ofarelation(a
so-called“corelation”)andmoregenerally,thedualofaspan(a“cospan”).
Definition13(Cospansandcorelationsbetweensets). Acospanbetweenthesets X andY isatriple
(U,i ,i )whereUisasetandi : X → Uandi : Y → Uaretwofunctionswiththesamecodomain,U.
1 2 1 2
Thepair(x,y) ∈ X×Y isidentifiedby(U,i ,i )ifi (x) = i (y). AcorelationCisajointlyepiccospan,
1 2 1 2
i.e. acospanwherei ,i arejointlyanepimorphism(asurjectivefunctioninSet),X+Y − ( − i 1− ,i →2 ) C,given
1 2
by x (cid:55)→ i (x)andy (cid:55)→ i (y),orinotherwordsgivenanytwofunctionsh,k : U → T,i h = i kand
1 2 1 1
i h = i kimplythath = k. (cid:35) (cid:35)
2 2
(cid:35) (cid:35)
Definition 14 (Cocongruence). Given two F-coalgebras (S, fS),(S′, fS′), a corelation C is said to
be a cocongruence [112, 110] between (S, fS) and (S′, fS′) if there exists an F-coalgebra structure
γC : C → F(C)suchthatthefollowingdiagramcommutes:
S rS C r S′ S′
fS ∃ γC fS′ (14)
F(S) F(C) F(S′)
F(rS ) F(r S′)
i.e. suchthatr S ,r S′ arecoalgebrahomomorphisms. Moregenerally,cocongruencecanbecharacterised
asacospanofcoalgebras,withtheaboveasaspecialcase.
Akernelbisimulationis,inthiscontext,arelation Rassociated,whenitexists,toacocongruence.
Definition15(Kernelbisimulation). GivenacocongruencebetweentwoF-coalgebras(S, fS),(S′, fS′),
arelation R ⊆ S×S′ isakernelbisimulationifitisapullbackofthecospanS → C ← S′,i.e. ifthere
5Formalduality,inthesenseofcategorytheory,i.e.arrowreversal.
13
Acoalgebraicperspectiveonpredictiveprocessing
existmorphismss S : R → Sands S′ : R → S′ suchthatthefollowingdiagramcommutes:
R
sS s S′
S rS C r S′ S′
(15)
fS ∃ γC fS′
F(S) F(C) F(S′)
F(rS ) F(r S′)
Inacategoryofcoalgebrasforafunctorpreservingweakpullbacks,asisthecaseforallthefunctors
consideredinthiswork,withabasecategorywithpullbacks(suchasSet),cocongruenceandkernel
bisimulationsimplyeachother,i.e. everycocongruencehasanassociatedkernelbisimulation.
Afinalcoalgebra,whenitexists,isthefinalorterminalobjectinacategoryCoalg(F).
Definition16(Finalcoalgebra). AnF-coalgebra(Ω ,ω Ω)isfinalinthecategoryCoalg(F)ifforany
F-coalgebra(S, fS)thereexistsauniqueF-homomorphismbeh : (S, fS) → (Ω ,ω). Graphically,this
S
isequivalenttothefollowingdiagramcommuting:
S beh S Ω
fS ω Ω (16)
F(S) F(Ω)
F(beh )
S
A final coalgebra is said to capture the behaviour of a coalgebra [36, 37]. More precisely, the
elements of a final coalgebra (when it exists) are the possible observable behaviours of all objects
(includingitself)ofagivencategoryofcoalgebras. WerecallfromSection2.3that,inthetheoryof
coalgebras, behaviour was informally defined as “outputs of a system over time”. In coalgebraic
terms,however,therigorousdefinitionofbehaviourismorecomplicated,anddependsstrictlyonthe
typeoffunctorusedtobuildcoalgebras: behaviourscouldcorrespondtotraces(repeatedapplications
ofthecoalgebratransitionmap),trees,distributions,etc. Forthepurposesofthispaper,wewillonly
consideracoupleofstandardexamplesrelevanttopredictiveprocessinginSection4,andreferthe
readertostandardtreatmentssuchas[36,37]foramoreindepthdiscussionoffinalcoalgebrasand
theirsemantics.
Example17. ForthecategoryofclosedsystemsinExample5,thefinalcoalgebraistrivial,itistheoneelement
set{·}sinceallsystemslookthesamefromanexternalperspective,i.e. nothingcanbeobservedbecausethese
systemshavenooutputs.
Next,welookatthecaseofdeterministicMooremachines.
Example18. LetCoalg(Moore)bethecategoryofMooremachinesascoalgebrasfromExample6. Thefinal
coalgebrainCoalg(Moore)isgivenby
(OI∗ ,ωOI∗ : OI∗ → O×(OI∗ )I, (17)
14
Acoalgebraicperspectiveonpredictiveprocessing
where the notation ∗ is used to represent lists [36, 37]. Here, the carrier of the final coalgebra is OI∗ ,
and its elements ∈ OI∗ can be understood by defining state transitions from any initial state s ∈ S
0
for a given list of actions of arbitrary length n, ⟨i ,...,i ⟩ ∈ I∗. Using these, we can take n steps
1 n
from the initial state s , i.e. tr (...tr (s )(i ))...)(i ), and obtain an observation for each list
0 Moore Moore 0 1 n
out (...tr (s )(i ))...)(i )[37,Sec. 2.2.3],thusobtainingtreesrootedinsomeinitialstates with
Moore Moore 0 1 n 0
inputsasedgesbetweennodesrepresentedbythepossibleoutputsgiventhoseedges.
Wenowcombinethedefinitionsofkernelbisimulation,whichcanbebuiltfromallcocongruences
incategoriesofcoalgebrasusingonlyweakpullback-preservingfunctorsonSetasthebasecategory,
andfinalcoalgebratoobtainthefollowing[109].
Definition 19 (Behavioural equivalence). Given two F-coalgebras (S, fS),(S′, fS′), behavioural
equivalencebetweenthemisakernelbisimulation R(Definition15)foracocongruence(Ω ,γ Ω : Ω →
F(Ω))(Definition14)where(Ω
,γ
Ω)isthefinalcoalgebra(Definition16).
Twostates s ∈ S,s′ ∈ S′ arebehaviourallyequivalent, i.e. (s,s′) ∈ R, ifforr : S → C,r : S′ → C
S S
(see Definitions 14 and 15), r S (s) = r S′ (s′) (see [37, Theorem 3.3.3] for polynomial functors and its
generalisationtoincludethefinitepowersetfunctorandthedistributionfunctor[37,Theorem4.5.3]).
Inthenextsection,wewillusebehaviouralequivalencetoprovideastructuraldescriptionofthe
corepartsofpredictiveprocessing,andfreeenergyminimisation,underacoalgebraicframework. As
weshallsee,giventhelevelofabstractionwereached,wewillonlyneedtoapplyafewminorchanges
todefinitionsintroducedabovetounderstandtherelationthatoughttobeinplacebetweengenerative
processandgenerativemodelforasuccessfulpredictiveprocessingagent. Althoughwedonotfocus
onanyspecificalgorithmicimplementation,ourfinaldiscussionwillprovideaconnectiontoexisting
workonbothexactandapproximatebisimulations,showinghowsomeoftheideasintroducedinthe
nextsectioncouldbeimplementedinfuturework.
4. Predictive processing in coalgebraic terms
4.1. Generativemodelandgenerativeprocessascoalgebras
InSection2.1,wesawthat,inthepredictiveprocessingliterature,thetermsgenerativeprocessand
generativemodelhavebeenusedaslabelsforprobabilisticprocessesthatrepresentthegroundtruth
oftheenvironment,andamodelofitwhoseupdatesareencodedintheagent’sbrainstates,respec-
tively[26,35,29,47]. TheseprobabilisticprocessesareusuallypresentedasPOMDPs(seeDefinition1),
andintheliteratureoncoalgebras,correspondtoaprobabilisticversionofMooremachines(cf.Exam-
ple6). AsinthecaseofExample8,theseconstituteanotherexampleofbranching(probabilistic)given
thesametransitiontype(withinputsandoutputs)ofstructuredMooremachines[92]. Todefinethem,
wefirstrecallthefollowing.
Definition20(Distributionfunctor). Thedistributionfunctorfordiscreteprobability, P : Set → Set
isdefinedas:
(cid:40) (cid:12) (cid:41)
(cid:12)
P(X) = p : X → [0,1](cid:12)supp(p)isfiniteand ∑ p(x) = 1 , (18)
(cid:12)
(cid:12) x
where[0,1] ⊆ R istheunitintervalofrealnumbers,andsupp(p) ⊆ Xisthesupportofthedistribution,
i.e. thefinitesubsetof x ∈ X where p(x) ̸= 0. Forafunction g : X → Y themap P(g) : P(X) → P(Y)
15
Acoalgebraicperspectiveonpredictiveprocessing
(thepushforwardof palong g)isdefined,foranydistribution p ∈ P(X)andanyelementy ∈ Y,as:
∑ ∑
P(g)(p)(y) = p(x) = {p(x) | x ∈ supp(p)with g(x) = y}. (19)
x∈g−1(y) x
Usingthis,wecandefineprobabilisticMooremachinesandinterpretthemasPOMDPsincoal-
gebraicterms. Notethatweadoptacommonsimplifyingassumptionstatingthatobservationsand
transitionstothenextstateareindependent. Formoregeneraltreatmentsnotinvolvingcoalgebras,
seeforinstance[67,113,97].
Definition21(CategoryofPOMDPsascoalgebras(probabilisticMooremachinesin[91])). The
category of POMDPs as coalgebras Coalg(POMDP), with coalgebra type given by the functor
POMDP : Set → SetsuchthatPOMDP(S) = P(O)×P(S)A,has
• asobjects,coalgebrasoftheform(S, fS : S → P(O)×P(S)A),withtransitions fS =
POMDP POMDP
⟨outS ,trS ⟩givenby
POMDP POMDP
outS : S → P(O)
POMDP
trS : S → P(S)A, (20)
POMDP
and
• asmorphismsbetweentwocoalgebras(S, fS )and(S′, fS )withthesameinputs/ac-
POMDP POMDP
tionsandoutputs/observations,coalgebrahomomorphismsintheformoffunctionsϕ : S → S′
(sinceinputsandoutputsarethesameforthetwosystems,therearesimpleidentityfunctions
betweenthem,indicatedbyid forobservations,andbyhavingthesame Aonbothcoalgebras
O
foractions)thatmakethefollowingdiagramcommute:
S ϕ S′
fS fS′ (21)
POMDP POMDP
P(O)×P(S)A P(O)×P(S′)A
P(id
O
)×P(ϕ)A
Similarly,wedefinethecategoryofMDPsbelow,followinginthiscasepreviouswork[114],but
withoutincludingexplicitrewards.
Definition22(CategoryofMDPsascoalgebras). ThecategoryofMDPsascoalgebrasCoalg(MDP),
withcoalgebratypegivenbythefunctorMDP : Set → SetsuchthatMDP(S) = P(S)A,has
• asobjects,coalgebrasoftheform(S, fS : S → P(S)A)6,and
MDP
• asmorphismsbetweentwocoalgebras(S, fS )and(S′, fS′ )withthesameinputs,coalgebra
MDP MDP
homomorphisms in the form of functions ϕ : S → S′ (since inputs are the same, there is once
6NoticehowtherelationbetweenpartiallyandfullyobservableMDPsappears: fS isofthesametypeastrS .
MDP POMDP
16
Acoalgebraicperspectiveonpredictiveprocessing
againasimpleidentitybetweenthem)thatmakethefollowingdiagramcommute:
S ϕ S′
fS fS′ (22)
MDP MDP
P(S)A P(S′)A
P(ϕ)A
4.2. Comparing generative process and generative model: predictive processing as be-
haviouralequivalence
In Section 2.3 we saw how the literature on active inference and predictive processing contains
severalclaimsthattheprocessofminimisingvariationalfreeenergy,usedtoperformapproximate
Bayesianinferenceontheenvironment’sstatesthatgeneratesensoryinputs,canbeunderstoodin
termsofa“synchronisation”betweengenerativemodelandgenerativeprocess. Thismeansthat,fora
particulartask,thedynamicsofagenerativemodel,implicitlyencodedbybrainstatesrepresenting
(approximate) Bayesian updates given observations over time, becomes a model, ideally a perfect
one, of the generative process. Here, we provide three candidate, formal definitions of this idea
correspondingtothreeparticularformsofbehaviouralequivalencebetweengenerativemodeland
generative process. We will discuss their implications and possible shortcomings, focusing in the
endonwhatwebelievetobethebestcandidatetoreflectarelationbetweengenerativemodeland
generativeprocessthatgoesbeyondmerestructuralsimilarity,consistentwithpredictiveprocessing.
4.2.1. ComparingPOMDPs
To start off, we apply directly the definition of behavioural equivalence given in Definition 19 to
generativeprocessandgenerativemodelinthecategoryofPOMDPs(Definition21),which,aswe
know, has a final coalgebra, see [115, Section 7] and [37, Theorem 4.6.9]. As we will see shortly,
this has quite strong and perhaps undesirable implications, which are nevertheless important to
discuss. In what follows, we will make extensive use of Definition 19, but without visualising the
relationR(thekernelbisimulation)inourdiagrams,sinceitsexistenceisalwaysimpliedbyoursetup,
seeDefinition15.
Definition23(BehaviouralequivalenceofPOMDPs). WeapplyDefinition19forF = POMDP:
S beh S Ω beh S′ S′
f P S OMDP f P Ω OMDP f P S O ′ MDP (23)
P(O)×P(S)A P(O)×P(Ω)A P(O)×P(S′)A
P(id
O
)×P(beh
S
)A P(id
O
)×P(beh S′)A
Thiscorrespondstothefollowingconditions(formoredetailsseeAppendixA),whereforany
a ∈ Aandω ∈ Ω wehave:
p(o | s) = p(o | s ′) (condition1)
17
Acoalgebraicperspectiveonpredictiveprocessing
∑ p(s˜ | s,a) = ∑ p(s˜ ′ | s ′ ,a) (condition2) (24)
s˜∈beh−
S
1(ω) s˜′∈beh−
S′
1(ω)
Thisdefinitionstatesthat,giventhesameactions(byassumption),statessands′ arebehaviourally
equivalent only if they emit the same probabilistic observations (by condition 1), while creating
equivalenceclassesofindistinguishablegroundtruthstatesbyconsideringtheirprobabilistictransi-
tions(bycondition2). Webelievethisistoostricttoproperlydescribepredictiveprocessinginall
ofitsfacets,asthisrequiresprobabilistictransitionsof(equivalenceclassesof)groundtruthstates
of the generative process, s ∈ S, to be equal to probabilistic transitions of (equivalence classes of)
groundtruthstatesofthegenerativemodel,s′ ∈ S′,whileoneofthemainpointsofaction-oriented
generativemodelsisthattheydon’tneedtorecapitulatetheveridical,groundtruthstructureofthe
environment[11,6,31,32,33,34,35].
Moregenerally,adefinitionofbehaviouralequivalencebetweenprobabilisticprocessesisnotori-
ouslynon-trivial,andtheformulationprovidedaboveisnottheonlypossiblechoice(seee.g.[116]for
areviewofthisandotherpossiblechoices). Forprobabilisticprocesses,itisinfactoftendesirableto
focusonprobabilisticpropertiesratherthanoncharacteristicsofsampledtrajectoriesfromground
truthstatesasinDefinition23. Wearguethatthisisalsothecaseforpredictiveprocessing,whichis
basedontheminimisationofthedifferencebetweendistributionsencodedbythegenerativemodel
and generative process, rather than minimising the difference between trajectories sampled from
them.
4.2.2. ComparingbeliefMDPs
Next,wewilladaptthedefinitionofbeliefbisimulationequivalence[117,118],originallyrestricted
to a single system (hence the term “equivalence” (see Definition 11), to work between different
processes. In other words, we will define a belief bisimulation. This definition corresponds to a
standardbisimulation,thatis,aspanofcoalgebras(seeDefinition12)betweencoalgebrasencoding
beliefs,inaBayesiansense,asweshallseebelow,oftheoriginalprocesses. Thismeansthatthereis
acorrespondingnotionofbeliefbehaviouralequivalence(acorelationormoregenerallyacospan
of coalgebras, see Definition 19), which once again is implied and implies that of bisimulation by
workingwithwellbehavedfunctorsandSetasthebasecategory[115].
To apply the definition of belief behavioural equivalence, we start from a description of belief
MDPs[49]associatedto,orratherinducedbyPOMDPs. Thesearerelatedtotheseparationprinciple
of control [119] (see also [120] for a review of related ideas). Belief MDPs have previously been
formulatedinacoalgebraiccontextin[113,121],althoughtheyarenotexplicitlypresentedintermsof
MDPsinthoseworks.
Definition24(BeliefMDP). AbeliefMDPinducedbyaPOMDP(S,A,T,O,M)isanMDP(Z,A,T )
Z
where:
• Z is the space of belief states, sufficient statistics of histories H := (O× A)∗ ×O, given by
z : H → P(S),
• AisthespaceofactionsandcoincideswiththeonefromtheoriginalPOMDP,
• T Z : Z×A → P(Z)isthebelieftransitionsdynamics,definedforz t ,z t+1 ∈ Zand a t ∈ Aas
T
Z
(z
t
,a
t
) = p(z
t+1
| z
t
,a
t
)
∑
= p(z
t+1
| z
t
,o
t+1
,a
t
)p(o
t+1
| z
t
,a
t
), (25)
ot ∈O
18
Acoalgebraicperspectiveonpredictiveprocessing
where
(cid:40)
1 ifτ Z (z t ,o t+1 ,a t ) = z t+1 ,
p(z
t+1
| z
t
,o
t+1
,a
t
) = (26)
0 otherwise,
forτ : Z×O×A → ZdefinedbystandardBayesianfilteringupdatesofbeliefsz = p(s | h )
Z t t t
forh ∈ H ,see[49,97]andAppendixB.
t t
InabeliefMDP,beliefs,probabilitydistributionsoverthepossiblestates,serveasthestatesofa
standardMDP.Applyingthisconstructiontoboththegenerativemodelandgenerativeprocess,given
asPOMDPsorprobabilisticMooremachinesincoalgebraicform(seeDefinition21),producestwo
belief MDPs, describing the associated probability distributions and their transitions (obtained by
currying,seeExample6):
T : Z×A → P(Z) ↔ fZ : Z → P(Z)A (beliefgenerativeprocess)
Z MDP
T Z′ : Z ′×A → P(Z ′) ↔ f M Z′ DP : Z ′ → P(Z ′)A (beliefgenerativemodel). (27)
Usingthese,abeliefbehaviouralequivalencebetweenthemcanbedefinedasfollows:
Definition25(BeliefbehaviouralequivalenceofbeliefMDPs). ThisisadirectapplicationofDefini-
tion19(onceagain,withoutvisualising Rforsimplicity)forF = MDP:
Z beh Z Ω beh Z′ Z′
f M Z DP f M Ω DP f M Z′ DP (28)
P(Z)A P(Ω)A P(Z′)A
P(beh
Z
)A P(beh Z′)A
Thiscoincideswiththefollowingdefinition,i.e. condition2inEq.(24)withadifferentstatespace:
beliefs on states, rather than states, and without observations due to the Bayesian construction
inDefinition24(thiscanbeobtainedasinAppendixA,withtrivialobservations,i.e. O = 1),where,
forany a ∈ Aandω ∈ Ω ,wehave:
∑ p(z˜ | z,a) = ∑ p(z˜ ′ | z ′ ,a). (29)
z˜∈beh−
Z
1(ω) z˜′∈beh−
Z′
1(ω)
Thisconditionimpliesthatbeliefszandz′ arebehaviourallyequivalentifthedistributionsover
therespectivenextstates,belongingtothesameequivalenceclassrepresentedbyω,areequalgiven
thesameactions,butdoesnotmakeanystatementaboutobservations. Thisisaconsequenceofthe
definition of belief MDPs (Definition 24), in which observations are marginalised in the definition
of belief updates (see Eq. (25)). Such a condition could be relevant in situations where we require
the beliefs of two processes to be equivalent: their beliefs evolve in the same way, while the exact
implementations of these beliefs are not important. It is however problematic for another, crucial
reason: thefinalcoalgebraofthecategoryofMDPsistrivial,i.e. Ω istheone-elementset. 7 Thismeans
thatbeliefMDPsareactuallynotobservableinthecoalgebraicsense[37],sincethereisnonon-trivial
monomorphism(inoursetup,aninjectivemap)intothefinalcoalgebra(sinceithasonlyoneelement).
Wethusturntoanotherapproachtodescribesimilaritybetweenthegenerativemodelandgenerative
process.
7TheauthorswouldliketothankNathanielVirgoforpointingthisout,seealso[106,122]forrelatedresults.
19
Acoalgebraicperspectiveonpredictiveprocessing
4.2.3. ComparingliftedPOMDPs
In our final attempt to find a behavioural notion of similarity between the generative model and
generativeprocess,weturntoageneralisationofdistributionbisimulationequivalence[123,92,124,
125]betweenprocesses,i.e. distributionbisimulation. Undertheassumptionsofthiswork,thisalso
yieldsanotionofdistributionbehaviouralequivalence.
Notethatworkonthistypeofequivalenceisoftengroupedwithwhatwehavedescribedasbelief
bisimulationandbehaviouralequivalence(see,e.g.[116]). However,wewishtoemphasisethat,while
botharebehaviouralequivalencesonprobabilitydistributions,thecompareddistributionshavea
markedly different semantics. In belief equivalence, the distributions are constructed as Bayesian
beliefsonhiddenstatesofagivenprocess. Indistributionbisimulations,bycontrast,theycorrespond
toprobabilitydistributionsonhiddenstatesthatarenotnecessarilyupdatedusingBayes[92]. While
thedeterminisationofaPOMDPisanotherPOMDP,abelificationofaPOMDPisa(belief)MDP.We
furthernotethatthesearealsorelatedtotheunifilarisationdescribedin[66],butdonotexplorehere
thedetails.
We start by recalling the generalised determinisation construction of [92], focusing only on
POMDPs.
Definition 26 (Generalised determinisation of POMDPs). Given a POMDP as a coalgebra of type
(S, fS : S → P(O)×P(S)A),thegeneraliseddeterminisationof(S, fS )istheliftedcoalgebra
POMDP POMDP
♯
oftype(P(S), fS : P(S) → P(O)×P(S)A)suchthatthefollowingdiagramcommute:
POMDP
S
ηS
P(S)
⟨outS ,trS ⟩ (30)
POMDP POMDP ⟨outS ♯ ,trS ♯ ⟩
POMDP POMDP
P(O)×P(S)A
orinotherwords,suchthat:
♯
outS = η outS and
POMDP S POMDP
(cid:35)
♯
trS = η trS , (31)
POMDP S POMDP
(cid:35)
whereη : S → P(S)isamap8 that,givenS,returnsthe(Kronecker)deltadistributionofS,δ ,and
S S
♯ ♯
theliftedmaps9 ⟨outS ,trS ⟩aregiven,foranyw ∈ P(S),a ∈ A,ands,s˜ ∈ Sby:
POMDP POMDP
outS ♯ (w)(o) = ∑ w(s)outS (s)(o) and
POMDP POMDP
s∈S
trS ♯ (w)(a)(s˜) = ∑ w(s)trS (s)(a)(s˜) (32)
POMDP POMDP
s∈S
orinamoretraditionalnotation:
∑
p(o | w) = w(s) p(o | s) and
s∈S
8Forreadersfamiliarwithit, thisistheunitofdistributionmonad, since P isnotonlyafunctorbutafullfledged
monad[84].
9Forreadersfamiliarwithit,thisisjustaKleisliextension,i.e.giventhemultiplicationofthedistributionmonadµ and
X
amorphismg,wehaveg♯ =P(g) µ .
X
(cid:35)
20
Acoalgebraicperspectiveonpredictiveprocessing
∑
p(s˜ | w,a) = w(s) p(s˜ | s,a). (33)
s∈S
Determinisation allows us to work with a coalgebraic structure in which the transition map,
♯ ♯
⟨outS , trS ⟩,isfromaspaceofdistributionsoverstates, P(S),tothespaceofdistributions
POMDP POMDP
over the next state and observations, instead of being from the state space S itself. As we will see
next,inthisway,wecanthinkofthetransitiondynamicsinthePOMDPasatransitionfromonestate
distributiontoanother. Further,basedonthisnotionofdeterminisation,wecancomparePOMDPsin
termsofprobabilitydistributionsoverstatesandobservations,withoutinducingabeliefMDPthat
marginalisesobservations,seethebelieftransitiondynamicsinDefinition24. Toseethis,weproceed
todefinethenotionofliftedPOMDP.
Definition27(LiftedPOMDP). AliftedPOMDPinducedbyaPOMDP(S,A,T,O,M)viadetermini-
sation[92]isaPOMDP(W,A,T ,O,M )where:
W W
• W isthespaceofbeliefstates P(S)10,
• AisthespaceofactionsandcoincideswiththeonefromtheoriginalPOMDP,
• T W : W×A → W isthebelieftransitionsdynamics,definedforw t ,w t+1 ∈ W and a t ∈ Aas:
T
W
(w
t
,a
t
) = w
t+1
= w(s
t+1
) = p(s
t+1
| w
t
,a
t
)
∑
= w(s
t
) p(s
t+1
| s
t
,a
t
) (34)
st ∈S
andcorrespondtotheliftedtransitionmapinEq.(33),
• M : W → P(O)isthebeliefobservationmapdefinedforw ∈ W, a ∈ Aando ∈ Oas:
W t t t
M (w ) = p(o | w )
W t t t
∑
= w (s ) p(o | s ), (35)
t t t t
st ∈S
andcorrespondtotheliftedobservationmapinEq.(33).
WenotethatthisisaratherspecialkindofPOMDP,oneinwhichstatetransitionsaredeterministic. In
somesense,thisisadifferentgeneralisationofdeterministicMooremachines: probabilisticMoore
machinesinDefinition21makebothtransitionandobservationmapsstochastic,whilehereonlythe
observationmapisstochastic.
Applyingthegeneraliseddeterminisationof[92]tothegenerativeprocessandgenerativemodel
ascoalgebrasyieldsthefollowing,respectively(bycurrying,seeExample6):
⟨outW ,trW ⟩ : W → P(O)×WA (liftedgenerativeprocess)
POMDP POMDP
⟨outW′ ,trW′ ⟩ : W ′ → P(O)×W ′A (liftedgenerativemodel). (36)
POMDP POMDP
Usingthese,adistributionbehaviouralequivalencebetweenthemamountstothefollowing:
10NB:W ̸=Zingeneral,howevertheirrelationwon’tbeexploredfurtherhere.
21
Acoalgebraicperspectiveonpredictiveprocessing
Definition28(DistributionbehaviouralequivalenceofliftedPOMDPs). Thisisadirectapplication
ofDefinition19forF = POMDP,withPOMDPsgivenasliftedPOMDPsfromDefinition27(once
again,withoutvisualising Rforsimplicity):
W beh W Ω beh W′ W′
⟨outW ,trW ⟩ ⟨outΩ ,trΩ ⟩ ⟨outW′ ,trW′ ⟩ (37)
POMDP POMDP POMDP POMDP POMDP POMDP
P(O)×WA P(O)×ΩA P(O)×W′A
P(id
O
)×(beh
W
)A P(id
O
)×(beh W′)A
whichcorrespondstothefollowingconditions(seeagainAppendixA,consideringthattransitions
aredeterministicandhencedeltadistributions),whereforany a ∈ A,w ∈ W andw′ ∈ W′ wehave:
p(o | w) = p(o | w ′) (condition1)
beh W (cid:0) trW POMDP (w)(a) (cid:1) = beh W′ (cid:0) trW PO ′ MDP (w ′)(a)) (condition2) (38)
Condition1statesthattwobeliefs,wandw′,arebehaviourallyequivalentonlyiftheyproduce
thesameexpectedprobabilityofobservations(seeEq.(33)). Inotherwords,providedthatcondition
2 also holds, w and w′ are equivalent if their distributions on states “average out” to the same
observations. They may represent different internal (i.e. state) information, yet their expected
observationalconsequencesareidentical.
The second condition is recursive in nature, as is typical for bisimulations of deterministic sys-
tems[94]. Itstatesthat,fortwobeliefswandw′ tobebehaviourallyequivalent,theirpredictedfuture
beliefsmustalsobeequivalentforanygivenaction. Thisimpliesthattwobeliefsareindistinguishable
if they lead to the same beliefs (predictions) about the next state of the world, that is, equivalence
istestedonthebeliefthatresultsfrompuredynamicalprediction,withouttakingintoaccountnew
evidence,i.e. observations,whichareinsteadpartofcondition1. ThiscontrastswithbeliefMDPs,
wherethesetwoconditionsarecombinedintoaconditiononBayesianupdates(seeEq.(29)).
5. Discussion
InSection4.2,weintroducedthreedistinctnotionsofbehaviouralequivalencebuildingonDefini-
tion19: behaviouralequivalenceofPOMDPs(Definition23),beliefbehaviouralequivalenceofbelief
POMDPs(Definition25),anddistributionbehaviouralequivalenceofliftedPOMDPs(Definition28).
Translating the conditions of each definition into a more familiar form gives us some background
ontheirimplicationsandrelationstopredictiveprocessing. Wesummarisethishighlevelaccount
pictoriallyinFig.2.
Eq. (24) suggests that Definition 23 may be too strict, as it requires observations for particular
ground truth states to be equal. While states can be coarse grained based on transition dynamics,
the condition on observations seems too strong. In contrast, Definition 25 requires only Bayesian
beliefs to be equal, under the assumption that they can be coarse grained if their transitions allow
for it (see Eq. (29)). From a coalgebraic perspective, however, this condition is too loose: based on
thedefinitionofbehaviouralequivalencegiveninDefinition19,thisconditionmustbesatisfiedfor
elements of the final coalgebra of the category of MDPs (which contain belief MDPs). Such final
coalgebraishowevertrivial,aoneelementset,whichimpliesthatallbeliefMDPscanbesaidtobe
22
Acoalgebraicperspectiveonpredictiveprocessing
Fig.2: Thethreepossibledescriptionsofbehaviouralequivalenceintroducedinthiswork.
behaviourallyequivalentunderthisaccount,becausetheirstatesareneverexposedtoanexternal
observer (see Section 5.2 for further discussion). The third proposal seemingly hits a sweet spot,
andwebelieveitcanbeusedtoestablishsomeoftherelevantconnectionsbetweencoalgebras,their
languagetocomparesystems’behavioursandpredictiveprocessing.
Definition28translatessomeofthecoreprinciplesofpredictiveprocessingandactiveinferencein
acoalgebraiclanguage,clarifyingthegoal,themechanism,andthenatureofanagent’sgenerative
model. Condition1inDefinition28definesthegoalofpredictiveprocessingintermsofprediction
error minimisation, which is expressed as the idea that the agent’s model must generate the same
sensorypredictionsastheenvironment. Ifwetakew ∈ W torepresentprobabilitydistributionsover
statesoftheenvironmentencodedbythegenerativeprocess,Condition1demandsthattheagent’s
beliefs w′ ∈ W′, which,itshouldbenoted,aredistinctfromthoseinabeliefMDP,encodedbythe
implicitgenerativemodelproducetheexactsameprobabilitydistributionoverobservationso ∈ O
given by the environment. In active inference, an agent that has minimised its free energy to the
greatestpossibleextentisonethathassuccessfullytuneditsmodeltosatisfythiscondition.
23
Acoalgebraicperspectiveonpredictiveprocessing
Furthermore,anactiveinferenceagentneverhasdirectaccesstothetruestatesoftheworlds ∈ S,
butonlytoitsownprobabilisticbeliefsabouttheworld. Distributionbehaviouralequivalenceoperates
at the level of these beliefs, providing a mechanism to compare an agent’s beliefs, Q(S ,π,A,B),
1:T
whichcorrespond(D = 0inthelimit)totheexactposteriorP(S ,π,A,B|O )foragentsperform-
KL 1:T 1:T
ingexactinference(seeEq.(3)),withtheenvironment’sgroundtruthprobabilisticstate,ratherthan
comparinganagent’sinternalstateswiththeworld’struehiddenstates,cf.Definition23.
Thus, behavioural equivalence well captures what we believe to be the true nature of active
inference,proposingthatthebrain’simplicitgenerativemodelsneedonlybeaction-oriented,enabling
an agent to fulfil its goals but not necessarily proving to be perfect and exhaustive copies of the
generative processes of the world. In this light, a generative model, with state space S′, can be
vastly simpler than the environment’s generative process with state space S. As long as the two
systemssatisfytheconditionsinEq.(38), makingthesamepredictions(condition1)andevolving
theirbeliefsinindistinguishableways(condition2),theyarefunctionallyequivalent. Thisframework
thusdemonstrateswhenasimplermodelcanbe“goodenough”,byformalizingwhatitmeanstobe
observationallyindistinguishable.
5.1. Relatedwork
This perspective is based on a coalgebraic formulation of dynamical systems, describing both a
generativeprocessoftheenvironmentandanimplicitgenerativemodelencodedbybrainstates. The
applicationofthecoalgebraiclanguagetoMDPsisnotnew,see,forinstance[114],whichdefinesa
categoryofMDPsthatincludesrewards(cf.Definition22)and[113,121]. Coalgebrashavealsobeen
applied in the context of predictive processing (see for instance [126, 127]). However, these works
make no explicit reference to the structural reading of active inference and predictive processing
proposed here, highlighting the generative model and process as two distinct entities, clarifying
their role within the overall framework, and showing how to use maps of systems, particularly
bisimulations,tounderstandthepurportedsynchronisationofbrainandenvironment(although[127]
definesanotionof(quasi-)bisimulationstogettoageneraldefinitionofBayesianinversionsintheir
setup).
With the use of a coalgebraic framework, we have also seen how immediate it is to switch
betweenseeminglydifferentkindsofdynamicalsystems: deterministic(open)dynamicalsystems,
orMooremachines,possibilisticMooremachinesandprobabilisticMooremachinesare,infactall
examples of “structured Moore machines” as defined in [92]. That is to say, their transition types,
are thesame: givenaninput, a systemcombinesthecurrentstateandtheinput toobtainthenext
(one/possible/distributionof)state(s)andproduce(one/possible/adistributionof)output(s). What
changes is simply the content in the brackets, differentiating the kind of branching a system can
have, e.g. deterministic, possibilistic or probabilistic. It is thus, to some extent, not surprising to
seeimportantworkssuchas[128]highlightingthesimilaritiesbetweenformulationsofpredictive
processingandclassicalautomata. However,weshouldstressthat,following[92],onecannotsimply
expecttouseanotherkindofstructuredMooremachinetoobtainTuringmachines, sotheprecise
connectionto[128],ifitexistsatall,willbeinvestigatedinfuturework.
Webelievethatourworkbasedonbehaviouralequivalenceofgenerativemodelandgenerative
processisalsorelatedtothe“interpretationmap”approachin[66,67],howeverwedonotexplore
possible connections here. Our definition of lifted POMDP, see Definition 27, employs a notion of
beliefstatesthatisalsopotentiallyrelatedtothedefinitionofpredictivebeliefsin[97]. Thiscould
thenexplainitsrelationtothebeliefsinabeliefMDP(Definition24),whichwouldbetheirpostdictive
24
Acoalgebraicperspectiveonpredictiveprocessing
counterpart,however,thisremainsatpresentspeculative.11
5.2. Currentlimitationsandfuturework
Currently, we do not handle continuous probabilities (cf. Giry functor on measurable spaces [37]).
Whilediscreteprobabilitiesareahelpfulsimplificationandarealsousefulfordifferentpartsofactive
inferenceandpredictiveprocessing[26],theydonotcoverthewholeframework,whichalsoinvolves
largepartsusingcontinuousprobabilities[23,30]. Oneofthemainreasonstoleaveasimilartreatment
forcontinuousprobabilitiestofutureworkisthefactthatdefinitionsofbisimulationforcontinuous
probabilitiesaremoredifficulttohandleandrequiremorecarefulconsiderations[37,129,108].
We also do not currently discuss several other computational components of active inference,
including learning, policy selection, and action, which are a central part of this framework (for a
review, see [26]) but for which we lack a coalgebraic counterpart. The algorithmic part of active
inference is also currently not discussed, as our focus is on “fixed points”, that is, the behavioural
equivalence of generative model and generative process does not really include phases like the
transientsleadingtosynchronisation. Thisisinpartbecausethecurrentworkisfocusedonlaying
downthefoundationsforacoalgebraictreatmentofsomeaspectsofpredictiveprocessingandactive
inference,andinpartbecausewearenotcurrentlyawareofalgorithmicimplementationscompatible
with behavioural equivalence that don’t rely on an underlying bisimulation equivalence. This is
perhapsnotamajorlimitationsince,bisimulationsbetweentwocoalgebras(whichareinaone-to-one
correspondencetobehaviouralequivalencesundertherightconditions)areequivalenttobisimulation
equivalences12,forwhichvariousimplementationsoflearningalgorithms(i.e. exhibitingtransients)
existinmachinelearningandreinforcementlearningwherebisimulationequivalencesareacentral
researchtopic[104,130,97]. Inthesefields,theyareusedtodescribecompressions(seetheirrelation
tomodelsinDefinition9)ofstateorstate(-action)spacesofa(PO)MDP,oftenusingapproximation
suchas“bisimulationmetrics”[131].
Finally,ourdefinitionofacategoryofMDPs(Definition22)canbepotentiallyimproved: while
itdoeshaveafinalcoalgebra, thisistrivial(i.e. theone-elementset), whichrendersallnon-trivial
coalgebras in this category non observable in a coalgebraic sense, that is, their maps into the final
coalgebra are not monomorphisms. This is perhaps counter-intuitive from the perspective of rein-
forcementlearningandactiveinference,whereMDPsaretraditionallyseenasfullyobservable. This
observationpointstoapotentialinconsistencythatmayneedtobeaddressed. Technically,wecould
changethedefinitionofthecategoryofMDPssothatthesecoalgebrasproduceanoutput,exposing
theirstatesdirectly,i.e. MDP(S) = S×P(S)A. However,thiswouldproducesystemsthatexposeall
possiblestates S ratherthantheparticularstateataspecifictimestep.13 Itwouldbeaproblemfor
the traditional definition of bisimulation. In that case, we would either need to assume that given
coalgebras (S, fS),(S′, fS′), their states are equal (because their exposes outputs would need to be
equal,seeDefinition23whereinplaceof P(O)wewouldhaveS),orabandonthestandardnotion
ofbehaviouralequivalenceiftheiroutputsaretoremaindistinct. Alternatively,wecouldadoptthe
11Roughly:predictivebeliefsaresufficientstatisticsbuiltfromaBayesianpredictionstep,apushforwardofprobabilities
alongthedynamicsofthesystembeforeanewobservationiscollected,whilepostdictivebeliefsaresufficientstatistics
generatedaftertheobservationiscollectedfromaBayesianupdatestep.
12Followingstandardresults(seeforinstance[108,proposition3.44intheOctober8th2024version]),giventwocoalgebras
S = (S,fS),S′ = (S′,fS′), acospanbetweenS andS′ (“external”behaviouralequivalence)isequivalenttoacospan
betweenS+S′,thedisjointunion(orcoproduct)ofthetwooriginalcoalgebras,andS+S′,i.e.itself(“internal”behavioural
equivalence).
13TheauthorswouldliketothankNathanielVirgoforpointingthisout.
25
Acoalgebraicperspectiveonpredictiveprocessing
definition from [114], with coalgebras of the form S → R×P(S)A, where R is used to represent
rewards. Thiscategoryhasanon-trivialfinalcoalgebrabecauserewardsareobservable. However,
sinceactiveinferenceandpredictiveprocessingdonotusuallyrelyonrewards,thiswouldbeoutside
thescopeofthiswork.
6. Conclusion
Inthisworkwecapturedimportantpropertiesofagent-environmentcoupledsystemswhereagents
arethoughttoimplementaprocessoffreeenergyminimisationinpredictiveprocessing,usingthe
languageofcoalgebras[36,37]. Moreprecisely,weprovidedanewandmoreformalperspectiveon
theideathatanagent’simplicitgenerativemodelneednotinfactbeisomorphicinastructuralsense
(cf. algebras vs. coalgebras [36, 132, 133, 90]) to the generative process representing relevant parts
oftheenvironment[31,32,33,34,35]. Instead,wearguedthatwhattrulymattersisthebehaviourof
anagent’sbrain,orratherofthegenerativemodelasaPOMDPthatitimplicitlyencodes,whichin
coalgebraictermscorrespondstoitsoutputsovertime,i.e. predictionsofobservationsoverdifferent
modalities. In particular, it matters that these predictions are compatible with the observations
producedbytheenvironment’sgenerativeprocessonadistributionlevel,ratherthanforgroundtruth
statesofthebrainandtheenvironment(Definition28),whileremainingconsistentwithachievingthe
agent’soverallgoals.
Acknowledgments
M.B.andT.N.weresupportedbyJSTFORESTProgram(JPMJFR231V).T.N.wasalsosupportedby
JSPSKAKENHI(grantnumbersJP24H02172andJP24H01559). F.T.wassupportedbyJST,Moonshot
R&D,GrantNumberJPMJMS2012.
26
Acoalgebraicperspectiveonpredictiveprocessing
A. Concrete behavioural equivalence for POMDPs
Given two coalgebras (S, fS ) and (S′, fS′ ), two states s ∈ S and s′ ∈ S′ are behaviourally
POMDP POMDP
equivalentifthereexistsafinalF-coalgebra(Ω , f Ω = (tr Ω (s),out Ω (s)))andapairof
POMDP POMDP POMDP
F-coalgebramorphismsbeh S : (S, f P S OMDP ) → (Ω , f P Ω OMDP ) andbeh S′ : (S′, f P S O ′ MDP ) → (Ω , f P Ω OMDP )
such that beh S (s) = beh S′ (s′). This can be written in a more familiar form. To see that, let ω 0 =
beh
S
(s) = beh
S′
(s′). Themapsbeh
S
andbeh
S′
areF-coalgebramorphismsifboth:
f Ω (cid:0) beh (s) (cid:1) (a) = (cid:0) P(id )×P(beh ) (cid:1) fS (s)(a)
POMDP S O S POMDP
= (cid:0) P(id )×P(beh ) (cid:1)(cid:10) outS (s),trS (s)(a) (cid:11)
O S POMDP POMDP
= (cid:10) P(id )outS (s),P(beh )trS (s)(a) (cid:11)
O POMDP S POMDP
= (cid:10) outS (s),P(beh )trS (s)(a) (cid:11) (A.1)
POMDP S POMDP
and
f P Ω OMDP (cid:0) beh S′ (s ′) (cid:1) (a) = (cid:0) P(id O )×P(beh S′ ) (cid:1) f P S O ′ MDP (s ′)(a)
= (cid:0) P(id
O
)×P(beh
S′
) (cid:1)(cid:10) out
P
S
OMDP
(s ′),tr
P
S
OMDP
(s ′)(a) (cid:11)
= (cid:10) P(id
O
)out
P
S
OMDP
(s ′),P(beh
S′
)tr
P
S
OMDP
(s ′)(a) (cid:11)
= (cid:10) out P S OMDP (s ′),P(beh S′ )tr P S OMDP (s ′)(a) (cid:11) (A.2)
hold. Sincebeh S (s) = beh S′ (s′) = ω 0 ,thefollowingholds
f P Ω OMDP (cid:0) beh S (s) (cid:1) (a) = f P Ω OMDP (cid:0) beh S′ (s ′) (cid:1) (a) (A.3)
andtherefore,componentwise,
outS (s) = outS′ (s ′)
POMDP POMDP
P(beh S )tr P S OMDP (s)(a) = P(beh S′ )tr P S′ OMDP (s ′)(a). (A.4)
Thefirstconditionsaysthattheobservationsmustbeequal,whilethesecondonecorrespondstoequal-
ityofdistributionsover Ω ,i.e. foranyelementω ∈ Ω ,theprobabilityvalueP(beh )trS (s)(a)(ω)
S POMDP
isequaltotheprobabilityvalue P(beh S′ )tr P S′ OMDP (s′)(a)(ω). Usingthedefinitionofthedistribution
functorfordiscreteprobability(Definition20),wethenhavethat:
∑ trS (s)(a)(s˜) = ∑ trS′ (s ′)(a)(s˜ ′). (A.5)
POMDP POMDP
s˜∈beh−
S
1(ω) s˜′∈beh−
S′
1(ω)
Finally,were-expressthetwoconditionsinEq.(A.4)inamoretraditionalnotation,obtaining:
P(o | s) = P(o | s ′) (condition1)
∑ P(s˜ | s,a) = ∑ P(s˜ ′ | s ′ ,a) (condition2) (A.6)
s˜∈beh−
S
1(ω) s˜′∈beh−
S′
1(ω)
27
Acoalgebraicperspectiveonpredictiveprocessing
B. Bayesian filtering updates
Abeliefattimet,aprobabilitydistributionoverhiddenstatesattimet,z := z(s )isdefinedusing
t t
Bayesianfilteringupdatesoftypeτ : Z×O×A → Z,givenby
Z
z
t
:=τ
Z
(z
t−1
,o
t
,a
t−1
)
=p(s
t
| z
t−1
,o
t
,a
t−1
)
(cid:0) (cid:1)
=p(s t | h t−1 ,o t ,a t−1 ) = p(s t | h t ) (z t−1 isasufficientstatisticofh t−1 )
=p(s
t
| o
0...t−1
,a
0...t−2
,o
t
,a
t−1
)
=p(s
t
| o
0...t
,a
0...t−1
)
=
p(o
t
| s
t
,o
0...t−1
,a
0...t−1
)p(s
t
,o
0...t−1
,a
0...t−1
)
(Bayesianfiltering)
p(o
t
,o
0...t−1
,a
0...t−1
)
=
p(o
t
| s
t
,o
0...t−1
,a
0...t−1
)p(s
t
| o
0...t−1
,a
0...t−1
)
p(o
t
| o
0...t−1
,a
0...t−1
)
=
p(o
t
| s
t
)p(s
t
| o
0...t−1
,a
0...t−1
)
(Markovianity)
p(o
t
| o
0...t−1
,a
0...t−1
)
=
p(o
t
| s
t
)∑
st−1
p(s
t
| s
t−1
,a
0...t−1
)p(s
t−1
| o
0...t−1
,a
0...t−1
)
(Chapman-Kolmogorov)
p(o
t
| o
0...t−1
,a
0...t−1
)
=
p(o
t
| s
t
)∑
st−1
p(s
t
| s
t−1
,a
t−1
)p(s
t−1
| o
0...t−1
,a
0...t−2
)
(Markovianity)
p(o
t
| o
0...t−1
,a
0...t−1
)
= p(o t | s t )
p
∑
(
s
o
t−
t
1
|
p
h
(
t
s
−
t
1
|
,a
s t
t
−
−
1
1
,
)
a t−1 )z t−1 (Definitionsofz t−1 andh t−1 )
= ∑ p(s t ,o t | s t−1 ,a t−1 )z t−1 . (B.1)
st−1
p(o
t
| h
t−1
,a
t−1
)
References
[1] KarlJ.Friston. Learningandinferenceinthebrain. NeuralNetworks,16(9):1325–1352,2003.
[2] KarlJ.Friston. Atheoryofcorticalresponses. PhilosophicalTransactionsoftheRoyalSocietyof
London.SeriesB,Biologicalsciences,360(1456):815–836,2005.
[3] KarlJ.FristonandStefanJ.Kiebel. Predictivecodingunderthefree-energyprinciple. Philosophi-
calTransactionsoftheRoyalSocietyB:BiologicalSciences,364(1521):1211–1221,May2009.
[4] KarlJ.Friston. Thefree-energyprinciple: Aunifiedbraintheory? NatureReviewsNeuroscience,
11(2):127–138,2010.
[5] Andy Clark. Whatever next? Predictive brains, situated agents, and the future of cognitive
science. BehavioralandBrainSciences,36(3):181–204,2013.
[6] AndyClark. SurfingUncertainty: Prediction,Action,andtheEmbodiedMind. OxfordUniversity
Press,2015.
[7] JakobHohwy. ThePredictiveMind. OxfordUniversityPress,Oxford,2013.
[8] RickA.Adams,StewartShipp,andKarlJ.Friston. Predictionsnotcommands: Activeinference
inthemotorsystem. BrainStructureandFunction,218(3):611–643,2013.
28
Acoalgebraicperspectiveonpredictiveprocessing
[9] Thomas Parr and Karl J. Friston. Uncertainty, epistemics and active inference. Journal of the
RoyalSocietyInterface,14(136):20170376,2017.
[10] Filippo Torresan, Keisuke Suzuki, Ryota Kanai, and Manuel Baltieri. Active inference for
action-unawareagents,August2025.
[11] AndyClark. Radicalpredictiveprocessing. TheSouthernJournalofPhilosophy,2015.
[12] Thomas Parr and Karl J. Friston. The discrete and continuous brain: From decisions to
movement—andbackagain. NeuralComputation,30(9):2319–2347,2018.
[13] Thomas Parr, Noor Sajid, Lancelot Da Costa, M. Berk Mirza, and Karl J. Friston. Generative
ModelsforActiveVision. FrontiersinNeurorobotics,15:651432,April2021.
[14] Andy Clark. Dreaming the Whole Cat: Generative Models, Predictive Processing, and the
EnactivistConceptionofPerceptualExperience. Mind,121(483):753–771,July2012.
[15] AlexKieferandJakobHohwy. Contentandmisrepresentationinhierarchicalgenerativemodels.
Synthese,195(6):2387–2415,June2018.
[16] AlexKieferandJakobHohwy. RepresentationinthePredictionErrorMinimizationFramework.
InSarahRobins,JohnSymons,andPacoCalvo,editors,TheRoutledgeCompaniontoPhilosophy
ofPsychology,pages384–409.Routledge,Secondedition.|Abingdon,Oxon;NewYork,NY:
Routledge,Taylor&FrancisGroup,2020.,2edition,October2019.
[17] KarlJ.Friston,NelsonTrujillo-Barreto,andJeanDaunizeau. DEM:Avariationaltreatmentof
dynamicsystems. NeuroImage,41(3):849–885,2008.
[18] Paweł Gładziejewski. Predictive coding and representationalism. Synthese, 193(2):559–582,
February2016.
[19] Maxwell J. D. Ramstead, Michael D. Kirchhoff, and Karl J. Friston. A tale of two densities:
Activeinferenceisenactiveinference. AdaptiveBehavior,page1059712319862774,2019.
[20] MaxwellJ.D.Ramstead,CasperHesp,AlexanderTschantz,RyanSmith,AxelConstant,and
KarlFriston.Neuralandphenotypicrepresentationunderthefree-energyprinciple.Neuroscience
&BiobehavioralReviews,120:109–122,January2021.
[21] KarlJ.Friston. Lifeasweknowit. JournaloftheRoyalSocietyInterface,10(86):20130475,2013.
[22] BiswaSenguptaandKarlJ.Friston. SentientSelf-Organization: Minimaldynamicsandcircular
causality,May2017.
[23] KarlJ.Friston. Afreeenergyprincipleforaparticularphysics,June2019.
[24] Ensor Rafael Palacios, Takuya Isomura, Thomas Parr, and Karl J. Friston. The emergence of
synchronyinnetworksofmutuallyinferringneurons. ScientificReports,9(1):6412,April2019.
[25] ThomasParr,LancelotDaCosta,andKarlJ.Friston. Markovblankets,informationgeometry
andstochasticthermodynamics. PhilosophicalTransactionsoftheRoyalSocietyA:Mathematical,
PhysicalandEngineeringSciences,378(2164):20190159,2020.
[26] LancelotDaCosta, ThomasParr, NoorSajid, SebastijanVeselic, VictoritaNeacsu, andKarlJ.
Friston. Activeinferenceondiscretestate-spaces: Asynthesis. JournalofMathematicalPsychology,
99:102447,December2020.
[27] ThomasParr,NoorSajid,andKarlJ.Friston. ModulesorMean-Fields? Entropy,22(5):552,May
2020.
29
Acoalgebraicperspectiveonpredictiveprocessing
[28] KarlJ.Friston,ConorHeins,KaiUeltzhöffer,LancelotDaCosta,andThomasParr. Stochastic
ChaosandMarkovBlankets. Entropy,23(9):1220,September2021.
[29] ThomasParr,GiovanniPezzulo,andKarlJ.Friston. ActiveInference: TheFreeEnergyPrinciplein
Mind,Brain,andBehavior. MITPress,March2022.
[30] Karl J. Friston, Lancelot Da Costa, Noor Sajid, Conor Heins, Kai Ueltzhöffer, Grigorios A.
Pavliotis,andThomasParr. Thefreeenergyprinciplemadesimplerbutnottoosimple. Physics
Reports,1024:1–29,June2023.
[31] ManuelBaltieriandChristopherL.Buckley. Anactiveinferenceimplementationofphototaxis.
InEuropeanConferenceonArtificialLife2017,pages36–43.MITPress,2017.
[32] ManuelBaltieriandChristopherL.Buckley. Generativemodelsasparsimoniousdescriptionsof
sensorimotorloops. BehavioralandBrainSciences,42:e218,2019.
[33] ManuelBaltieriandChristopherL.Buckley. PIDcontrolasaprocessofactiveinferencewith
lineargenerativemodels. Entropy,21(3):257,2019.
[34] AlexanderTschantz,AnilK.Seth,andChristopherL.Buckley. Learningaction-orientedmodels
throughactiveinference. PLoSComputationalBiology,16(4):e1007805,April2020.
[35] FrancescoMannella,FedericoMaggiore,ManuelBaltieri,andGiovanniPezzulo. Activeinfer-
encethroughwhiskers. NeuralNetworks,144:428–437,2021.
[36] Jan J. M. M. Rutten. Universal coalgebra: A theory of systems. Theoretical Computer Science,
249(1):3–80,October2000.
[37] BartJacobs. IntroductiontoCoalgebra: TowardsMathematicsofStatesandObservation. Cambridge
UniversityPress,1edition,2017.
[38] R Bogacz. A tutorial on the free-energy framework for modelling perception and learning.
JournalofMathematicalPsychology,2017.
[39] ChristopherL.Buckley,ChangSubKim,SimonMcGregor,andAnilK.Seth. Thefreeenergy
principleforactionandperception: Amathematicalreview. JournalofMathematicalPsychology,
81:55–79,December2017.
[40] Karl J. Friston, Thomas Fitzgerald, Francesco Rigoli, Philipp Schwartenbeck, and Giovanni
Pezzulo. ActiveInference: AProcessTheory. NeuralComputation,29:1–49,2017.
[41] Martin Biehl, Christian Guckelsberger, Christoph Salge, Simón C. Smith, and Daniel Polani.
ExpandingtheActiveInferenceLandscape: MoreIntrinsicMotivationsinthePerception-Action
Loop. FrontiersinNeurorobotics,12:45,August2018.
[42] KarlJ.Friston,LancelotDaCosta,DanijarHafner,CasperHesp,andThomasParr. Sophisticated
Inference. NeuralComputation,33(3):713–763,March2021.
[43] Randall D. Beer. Dynamical approaches to cognitive science. Trends in Cognitive Sciences,
4(3):91–99,March2000.
[44] Randall D. Beer. The dynamics of brain–body–environment systems: A status report. In
HandbookofCognitiveScience,pages99–120.Elsevier,2008.
[45] Uri Alon, Michael G. Surette, Naama Barkai, and Stanislas Leibler. Robustness in bacterial
chemotaxis. Nature,397(6715):168–171,1999.
[46] BurtonW.Andrews,Tau-MuYi,andPabloA.Iglesias. OptimalNoiseFilteringintheChemo-
tacticResponseofEscherichiacoli. PLoSComputationalBiology,2(11):e154,2006.
30
Acoalgebraicperspectiveonpredictiveprocessing
[47] NoorSajid,PhilipJBall,ThomasParr,andKarlJ.Friston. Activeinference: Demystifiedand
compared. NeuralComputation,33(3):674–712,2021.
[48] MartinL.Puterman. MarkovDecisionProcesses: DiscreteStochasticDynamicProgramming. John
Wiley&Sons,2014.
[49] Leslie P. Kaelbling, Michael L. Littman, and Anthony R. Cassandra. Planning and acting in
partiallyobservablestochasticdomains. ArtificialIntelligence,101(1-2):99–134,May1998.
[50] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. Adaptive
ComputationandMachineLearningSeries.TheMITPress,Cambridge,Massachusetts,second
editionedition,2018.
[51] KarlJ.Friston,JeanDaunizeau,andStefanJ.Kiebel.ReinforcementLearningorActiveInference?
PLoSONE,4(7):e6421,July2009.
[52] KarlJ.Friston. Whatisoptimalaboutmotorcontrol? Neuron,72(3):488–498,2011.
[53] KarlJ.FristonandPingAo. Freeenergy,value,andattractors. ComputationalandMathematical
MethodsinMedicine,2012,2012.
[54] BerenMillidge,AlexanderTschantz,AnilK.Seth,andChristopherLBuckley.Ontherelationship
betweenactiveinferenceandcontrolasinference. InInternationalWorkshoponActiveInference,
pages3–11.Springer,2020.
[55] LancelotDaCosta,NoorSajid,ThomasParr,KarlJ.Friston,andRyanSmith. Rewardmaximiza-
tionthroughdiscreteactiveinference. NeuralComputation,35(5):807–852,2023.
[56] JakobVonUexküll. Astrollthroughtheworldsofanimalsandmen: Apicturebookofinvisible
worlds. Semiotica,89(4),1992.
[57] NihatAyandKeyanZahedi. OntheCausalStructureoftheSensorimotorLoop. InMikhail
Prokopenko,editor,GuidedSelf-Organization: Inception,volume9,pages261–294.SpringerBerlin
Heidelberg,Berlin,Heidelberg,2014.
[58] KarlJ.Friston,ErikD.Fagerholm,TaherehS.Zarghami,ThomasParr,InêsHipólito,LoïcMa-
grou,andAdeelRazi. Parcelsandparticles: Markovblanketsinthebrain. NetworkNeuroscience,
5(1):211–251,January2021.
[59] JelleBruineberg,KrzysztofDołe˛ga,JoeDewhurst,andManuelBaltieri. TheEmperor’sNew
MarkovBlankets. BehavioralandBrainSciences,45:e183,2022.
[60] MartinBiehl,FelixA.Pollock,andRyotaKanai. ATechnicalCritiqueofSomePartsoftheFree
EnergyPrinciple. Entropy,23(3):293,February2021.
[61] FernandoE.Rosas,PedroA.M.Mediano,MartinBiehl,ShamilChandaria,andDanielPolani.
Causalblankets: Theoryandalgorithmicframework. InInternationalWorkshoponActiveInference,
pages187–198.Springer,2020.
[62] Miguel Aguilera, Beren Millidge, Alexander Tschantz, and Christopher L. Buckley. How
particularisthephysicsofthefreeenergyprinciple? PhysicsofLifeReviews,40:24–50,March
2022.
[63] NathanielVirgo,FernandoE.Rosas,andMartinBiehl. Embracingsensorimotorhistory: Time-
synchronousandtime-unrolledMarkovblanketsinthefree-energyprinciple. Behavioraland
BrainSciences,45:e215,2022.
[64] Christopher M. Bishop. Pattern Recognition and Machine Learning. Information Science and
Statistics.Springer,NewYork,2006.
31
Acoalgebraicperspectiveonpredictiveprocessing
[65] Kevin P. Murphy. Machine Learning - A Probabilistic Perspective. Adaptive Computation and
MachineLearning.MITPress,Cambridge,2014.
[66] Nathaniel Virgo, Martin Biehl, and Simon McGregor. Interpreting Dynamical Systems as
BayesianReasoners. InJointEuropeanConferenceonMachineLearningandKnowledgeDiscoveryin
Databases.Springer,December2021.
[67] MartinBiehlandNathanielVirgo. InterpretingsystemsassolvingPOMDPs: Asteptowards
a formal understanding of agency. In International Workshop on Active Inference, pages 16–31.
Springer,2022.
[68] ManuelBaltieri,MartinBiehl,MatteoCapucci,andNathanielVirgo. ABayesianInterpretation
oftheInternalModelPrinciple,March2025.
[69] NathanielVirgo,MartinBiehl,ManuelBaltieri,andMatteoCapucci. A"goodregulatortheorem"
forembodiedagents,August2025.
[70] AlexanderTschantz,ManuelBaltieri,Anil.K.Seth,andChristopherL.Buckley. ScalingActive
Inference. In2020InternationalJointConferenceonNeuralNetworks(IJCNN),pages1–8,Glasgow,
UnitedKingdom,July2020.IEEE.
[71] AlexanderTschantz,BerenMillidge,AnilK.Seth,andChristopherL.Buckley. Reinforcement
LearningthroughActiveInference,February2020.
[72] AbrahamImohiosen,JoeWatson,andJanPeters. Activeinferenceorcontrolasinference? A
unifyingview. InInternationalWorkshoponActiveInference,pages12–19.Springer,2020.
[73] Parvin Malekzadeh and Konstantinos N. Plataniotis. Active inference and reinforcement
learning: Aunifiedinferenceoncontinuousstateandactionspacesunderpartialobservability.
NeuralComputation,36(10):2073–2135,2024.
[74] Georg B. Keller and Thomas D. Mrsic-Flogel. Predictive processing: A canonical cortical
computation. Neuron,100(2):424–435,2018.
[75] JakobHohwy. Newdirectionsinpredictiveprocessing. Mind&Language,2020.
[76] KlaasE.Stephan,WillD.Penny,JeanDaunizeau,RosalynJ.Moran,andKarlJ.Friston. Bayesian
modelselectionforgroupstudies. Neuroimage,46(4):1004–1017,2009.
[77] Ryan Smith, Philipp Schwartenbeck, Thomas Parr, and Karl J. Friston. An Active Inference
ApproachtoModelingStructureLearning: ConceptLearningasanExampleCase. Frontiersin
ComputationalNeuroscience,14:41,May2020.
[78] ThomasParr. Inferentialdynamics. PhysicsofLifeReviews,42:1–3,September2022.
[79] AndyClark. Embodiedprediction. OpenMIND,2015.
[80] MarcoFacchin. Structuralrepresentationsdonotmeetthejobdescriptionchallenge. Synthese,
199(3-4):5479–5508,December2021.
[81] FilippoTorresanandManuelBaltieri. Disentangledrepresentationsforcausalcognition. Physics
ofLifeReviews,51:343–381,December2024.
[82] JelleBruineberg,KrzysztofDołe˛ga,JoeDewhurst,andManuelBaltieri. TheEmperorIsNaked:
Repliestocommentariesonthetargetarticle. BehavioralandBrainSciences,45,2022.
[83] LancelotDaCosta,KarlJ.Friston,ConorHeins,andGrigoriosA.Pavliotis. Bayesianmechanics
forstationaryprocesses. ProceedingsoftheRoyalSocietyA:Mathematical,PhysicalandEngineering
Sciences,477(2256):20210518,December2021.
32
Acoalgebraicperspectiveonpredictiveprocessing
[84] Saunders Mac Lane. Categories for the Working Mathematician, volume 5 of Graduate Texts in
Mathematics. SpringerNewYork,NewYork,NY,1978.
[85] IchiroHasuo,BartJacobs,andAnaSokolova. Generictracesemanticsviacoinduction. Logical
MethodsinComputerScience,Volume3,Issue4,2007.
[86] DavidJazMyers. CategoricalSystemsTheory. 2021.
[87] MatteoCapucci. Notesoncategoricalsystemstheory. Technicalreport,2024.
[88] SophieLibkindandDavidJazMyers. Towardsadoubleoperadictheoryofsystems,May2025.
[89] EmilyRiehl. CategoryTheoryinContext. CourierDoverPublications,2017.
[90] Jan J. M. M. Rutten. The Method of Coalgebra: Exercises in coinduction. Technical report,
Amsterdam: CWI,2019.
[91] AlexandraSilva,FilippoBonchi,MarcelloM.Bonsangue,andJanJ.M.M.Rutten. Generalizing
thepowersetconstruction,coalgebraically. InIARCSAnnualConferenceonFoundationsofSoftware
Technology and Theoretical Computer Science (FSTTCS 2010), page 12 pages. Schloss Dagstuhl -
Leibniz-ZentrumfuerInformatikGmbH,Wadern/Saarbruecken,Germany,2010.
[92] AlexandraSilva,FilippoBonchi,MarcelloBonsangue,andJanJ.M.M.Rutten. Generalizing
determinizationfromautomatatocoalgebras. LogicalMethodsinComputerScience,Volume9,
Issue1:1087,March2013.
[93] Jean-PierreAubin,AlexandreM.Bayen,andPatrickSaint-Pierre. ViabilityTheory: NewDirections.
SpringerBerlinHeidelberg,Berlin,Heidelberg,2011.
[94] Davide Sangiorgi and Jan J. M. M. Rutten. Advanced Topics in Bisimulation and Coinduction,
volume52. CambridgeUniversityPress,2011.
[95] DavideSangiorgi. IntroductiontoBisimulationandCoinduction. CambridgeUniversityPress,1
edition,October2011.
[96] BalaramanRavindranandAndrewGBarto.SMDPHomomorphisms: AnAlgebraicApproachto
AbstractioninSemi-MarkovDecisionProcesses. InIJCAI’03: Proceedingsofthe18thInternational
JointConferenceonArtificialIntelligence,2003.
[97] Fernando Rosas, Alexander Boyd, and Manuel Baltieri. AI in a vat: Fundamental limits of
efficientworldmodellingforagentsandboxingandinterpretability. InProceedingsoftheSecond
ReinforcementLearningConference,April2025.
[98] WilliamG.Noid,Jhih-WeiChu,GaryS.Ayton,VinodKrishna,SergeiIzvekov,GregoryA.Voth,
AvisekDas,andHansC.Andersen. Themultiscalecoarse-grainingmethod.I.Arigorousbridge
betweenatomisticandcoarse-grainedmodels. TheJournalofChemicalPhysics,128(24):244114,
June2008.
[99] HerbertA.SimonandAlbertAndo. Aggregationofvariablesindynamicsystems. Econometrica:
JournaloftheEconometricSociety,29(2):111,1961.
[100] ZhiyuanRenandBruceKrogh. StateaggregationinMarkovdecisionprocesses. InProceedings
ofthe41stIEEEConferenceonDecisionandControl,volume4,page3824vol.4,January2003.
[101] JohnG.KemenyandJ.LaurieSnell. FiniteMarkovChains,volume26. vanNostrandPrinceton,
NJ,1969.
[102] B.Moore. Principalcomponentanalysisinlinearsystems: Controllability,observability,and
modelreduction. IEEETransactionsonAutomaticControl,26(1):17–32,February1981.
33
Acoalgebraicperspectiveonpredictiveprocessing
[103] PeterE.CainesandYuan-JunWei. Thehierarchicallatticesofafinitemachine. Systems&Control
Letters,25(4):257–263,1995.
[104] AmyZhang,ZacharyC.Lipton,LuisPineda,KamyarAzizzadenesheli,AnimaAnandkumar,
LaurentItti,JoellePineau,andTommasoFurlanello. LearningCausalStateRepresentationsof
PartiallyObservableEnvironments,February2021.
[105] AmyZhang. StateAbstractionsforGeneralizationinReinforcementLearning. PhDthesis,McGill
University,2021.
[106] E. P. de Vink and Jan J. M. M. Rutten. Bisimulation for probabilistic transition systems: A
coalgebraicapproach. TheoreticalComputerScience,221(1):271–293,June1999.
[107] JoséeDesharnais,AbbasEdalat,andPrakashPanangaden. BisimulationforLabelledMarkov
Processes. InformationandComputation,179(2):163–193,December2002.
[108] MartínSantiagoMoroniandPedroSánchezTerraf. Aclassificationofbisimilaritiesforgeneral
Markovdecisionprocesses,October2024.
[109] SamStaton. Relatingcoalgebraicnotionsofbisimulation. LogicalMethodsinComputerScience,
2011.
[110] GiorgioBacci. GeneralizedLabelledMarkovProcesses,Coalgebraically. PhDthesis,Universitàdegli
StudidiUdine,2013.
[111] Luís Soares Barbosa. Coalgebra for the working software engineer. Journal of Applied Logics,
2022.
[112] AlexanderKurz. LogicsforCoalgebrasandApplicationstoComputerScience. PhDthesis,Ludwig-
MaximilianUniversityofMunich,2001.
[113] NathanielVirgo. UnifilarmachinesandtheadjointstructureofBayesianmodels. InElectronic
ProceedingsinTheoreticalComputerScience,volume397,pages299–317,2023.
[114] FrankM.V.Feys, HelleHvidHansen, andLawrenceS.Moss. Long-TermValuesinMarkov
DecisionProcesses,(Co)Algebraically. InCorinaCîrstea,editor,CoalgebraicMethodsinComputer
Science,volume11202,pages78–99.SpringerInternationalPublishing,Cham,2018.
[115] LawrenceS.MossandIgnacioD.Viglizzo. Finalcoalgebrasforfunctorsonmeasurablespaces.
InformationandComputation,204(4):610–636,April2006.
[116] FilippoBonchi,AlexandraSilva,andAnaSokolova. DistributionBisimilarityviathePowerof
ConvexAlgebras. LogicalMethodsinComputerScience,Volume17,Issue3:6158,July2021.
[117] PabloSamuelCastro,PrakashPanangaden,andDoinaPrecup. EquivalenceRelationsinFully
andPartiallyObservableMarkovDecisionProcesses. InIJCAI2009-Proceedings,2009.
[118] DavidN.Jansen,FlemmingNielson,andLijunZhang. BeliefBisimulationforHiddenMarkov
Models: LogicalCharacterisationandDecisionAlgorithm. InNASAFormalMethods,volume
7226,pages326–340,Berlin,Heidelberg,2012.SpringerBerlinHeidelberg.
[119] KarlJ.Åström. IntroductiontoStochasticControlTheory. AcademicPress,1970.
[120] Jayakumar Subramanian, Amit Sinha, Raihan Seraj, and Aditya Mahajan. Approximate In-
formationStateforApproximatePlanningandReinforcementLearninginPartiallyObserved
Systems. JournalofMachineLearningResearch,2022.
[121] SimonMcGregor,timorl,andNathanielVirgo. Formalisingtheintentionalstance2: Acoinduc-
tiveapproach,January2025.
34
Acoalgebraicperspectiveonpredictiveprocessing
[122] Ana Sokolova. Probabilistic systems coalgebraically: A survey. Theoretical Computer Science,
412(38):5095–5110,September2011.
[123] SilviaCrafaandFrancescoRanzato.ASpectrumofBehavioralRelationsoverLTSsonProbability
Distributions. InJoost-PieterKatoenandBarbaraKönig,editors,CONCUR2011–Concurrency
Theory,volume6901,pages124–139,Berlin,Heidelberg,2011.SpringerBerlinHeidelberg.
[124] YuanFengandLijunZhang. WhenEquivalenceandBisimulationJoinForcesinProbabilistic
Automata. In FM 2014: Formal Methods, volume 8442, pages 247–262, Cham, 2014. Springer
InternationalPublishing.
[125] Holger Hermanns, Jan Krcˇál, and Jan Krˇetínský. Probabilistic Bisimulation: Naturally on
Distributions. InPaoloBaldanandDanieleGorla,editors,CONCUR2014–ConcurrencyTheory,
volume8704,pages249–265,Berlin,Heidelberg,2014.SpringerBerlinHeidelberg.
[126] TobyStClereSmithe. CompositionalActiveInferenceII:PolynomialDynamics.Approximate
InferenceDoctrines,August2022.
[127] TobyStClereSmithe. OpenDynamicalSystemsasCoalgebrasforPolynomialFunctors,with
Application to Predictive Processing. Electronic Proceedings in Theoretical Computer Science,
380:307–330,August2023.
[128] TakuyaIsomura. Tripleequivalencefortheemergenceofbiologicalintelligence. Communications
Physics,8(1):1–14,April2025.
[129] ClaudioHermida,UdayReddy,EdmundRobinson,andAlessioSantamaria. Bisimulationasa
logicalrelation. MathematicalStructuresinComputerScience,32(4):442–471,April2022.
[130] Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning
InvariantRepresentationsforReinforcementLearningwithoutReconstruction,April2021.
[131] NormanFernsandDoinaPrecup. BisimulationMetricsareOptimalValueFunctions. InUAI,
pages210–219,2014.
[132] LuísSoaresBarbosa. AlgebraicandCoalgebraicStructures. Technicalreport,Universidadedo
Minho,2005.
[133] YdeVenema. Algebrasandcoalgebras. StudiesinLogicandPracticalReasoning,2007.
35

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "A coalgebraic perspective on predictive processing"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
