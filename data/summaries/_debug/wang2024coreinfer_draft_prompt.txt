=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation
Citation Key: wang2024coreinfer
Authors: Qinsi Wang, Saeed Vahidian, Hancheng Ye

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2024

Abstract: Largelanguagemodels(LLMs)withbillionsofparametershavesparkedanew
waveofexcitingAIapplications. However, theirhighcomputationalcostsand
memorydemandsduringinferenceposesignificantchallenges. Adaptivesparse
activationinference,whichactivatesonlyasmallnumberofneuronsforeachtoken,
offersanovelwaytoacceleratemodelinferencewithoutdegradingperformance,
showinggreatpotentialforresource-constrainedhardwaredevices. Nevertheless,
existingmethodspredictactivatedneuronsbasedonindividualtokenswithaddi-
tional...

Key Terms: large, coreinfer, language, activation, adaptive, sentence, sparse, accelerating, inference, semantics

=== FULL PAPER TEXT ===

COREINFER: ACCELERATING LARGE LANGUAGE
MODEL INFERENCE WITH SEMANTICS-INSPIRED ADAP-
TIVE SPARSE ACTIVATION
QinsiWang1,SaeedVahidian1,HanchengYe1,JianyangGu2,
JianyiZhang1,YiranChen1
1DukeUniversity 2OhioStateUniversity
https://wangqinsi1.github.io/coreinfer_page/
ABSTRACT
Largelanguagemodels(LLMs)withbillionsofparametershavesparkedanew
waveofexcitingAIapplications. However, theirhighcomputationalcostsand
memorydemandsduringinferenceposesignificantchallenges. Adaptivesparse
activationinference,whichactivatesonlyasmallnumberofneuronsforeachtoken,
offersanovelwaytoacceleratemodelinferencewithoutdegradingperformance,
showinggreatpotentialforresource-constrainedhardwaredevices. Nevertheless,
existingmethodspredictactivatedneuronsbasedonindividualtokenswithaddi-
tionalMLP,whichinvolvefrequentchangesinactivationmapsandresourcecalls,
limitingtheaccelerationbenefitsofsparseactivation. Inthispaper,weintroduce
CoreInfer, an MLP-free adaptive sparse activation inference method based on
sentence-levelprediction. Specifically,weproposetheconceptofsentence-wise
coreneurons,whichreferstothesubsetofneuronsmostcriticalforagivensen-
tence,andempiricallydemonstrateitseffectiveness.Todeterminethecoreneurons,
we explore the correlation between core neurons and the sentence’s semantics.
Remarkably,wediscoveredthatcoreneuronsexhibitbothstabilityandsimilarity
inrelationtothesentence’ssemantics—aninsightoverlookedbypreviousstudies.
Buildingonthisfinding,wefurtherdesigntwosemantic-basedmethodsforpre-
dictingcoreneuronstofitdifferentinputscenarios. InCoreInfer,thecoreneurons
aredeterminedduringthepre-fillingstageandfixedduringtheencodingstage,
enablingzero-costsparseinference. Weevaluatedthemodelgeneralizationand
task generalization of CoreInfer across various models and tasks. Notably, on
an NVIDIA TITAN XP GPU, CoreInfer achieved a 10.33×and 2.72×speedup
comparedtotheHuggingfaceimplementationandPowerInfer,respectively.
1 INTRODUCTION
GenerativeLargeLanguageModels(LLMs)havegarneredsignificantattentionfortheirexceptional
abilitiesincreativewriting,advancedcodegeneration,andcomplexnaturallanguageprocessing
tasks(Brown,2020;Chowdheryetal.,2023;Touvronetal.,2023a;Teametal.,2023;Jiangetal.,
2023). Thesemodelshaveprofoundlyimpactedourdailylivesandworkpractices. Agenerationtask
typicallyinvolvesmultipleinferences—asingleinferenceduringthepre-fillingstageandmultiple
inferencesduringthedecodingstage—butduetothevastnumberofparametersinLLMs,executing
these inferences becomes highly expensive (Pope et al., 2023). To make generative LLMs more
accessible,anincreasingnumberofresearchersarefocusingonacceleratingtheinferenceprocess.
Thekeychallengeis:howcanwereducethememoryandcomputationalrequirementsformodel
inferencewithoutdegradingperformance?
Model compression (Buciluaˇ et al., 2006; Cheng et al., 2017; Choudhary et al., 2020) has been
extensively studied to address this issue by transforming the original model into a light version.
Representatively,quantization(Linetal.,2024;Frantaretal.,2022;Dettmersetal.,2024)usesfewer
bitstorepresentparameters,reducingthememoryneededformodelstorageandinference. Pruning
1
4202
tcO
32
]GL.sc[
1v11381.0142:viXra
I lovereadingpapers I lovereadingpapers I lovereadingpapersand
C C 0 1 and C C 0 2 think
C2 Pre-filling Decoding
Core Neuron Set
C3 Cache (Update) Core
C4 Neurons Set Activation Layer i
C5 Y Legend
C6 Stable ? N N U eu p r d o a n t e C C lu o s r t e e r Activated Neurons Changeable
C7 Dropped Neurons Frozen
Ai,1Ai,2Ai,3Ai,4Ai,5
Core Neurons/Token Core Ne
{
u
C
r
0
o
,
n
C
s/
2
S
}
entence
Fully Activated
T
T
o
o
p
k
-
e
k
n
R
F
e
l
d
o
u
w
ction
Activation Layer i (@ 50% Sparsity) (@ 40% Sparsity) Fully Deactivated Core Neuron Flow
Figure1: TheoverviewframeworkofCoreInfer. Inthepre-fillingstage,ateachactivationlayer,
takingthei-thactivationlayerasanexample,wefirstextractthetoken-wisecoreneuronsbasedon
thetop-kselectionandthenfurtherextractthetop-kcommonlyactivatedcoreneuronsamongall
tokens,whichgothroughthestabilityestimationtodeterminehowtoupdatethesentence-wisecore
neuronset. Afterdetermination,thecoreneuronsetwillbefixedandutilizedforsparsedecoding.
(LeCunetal.,1989;Leeetal.,2018;Frankle&Carbin,2018;Bansaletal.,2022)decreasesthe
computationalloadduringinferencebyremovingunimportantneuronsorstructuralblocksfromthe
model. However,thesemethodsusuallybreaktheoriginalstructureandtrade-offtheperformance
forefficiency. Additionally,duetothediversityofmodernhardware,thesemethodscannotachieve
hardwaregeneralization. Forinstance,although3-bitquantizationhasshownpotential,mostcurrent
hardwaredevicesdonotsupportityet(Chengetal.,2017;Kimetal.,2021).
Dynamicactivationsparseinference(Liuetal.,2023)isanotherwaytoaccelerateinferencewithout
the limitations of model compression. This approach is based on the observation that activation
of individual tokens in large language models is often highly sparse (Song et al., 2023). During
thedecodingstage,dynamicactivationsparseinferenceactivatesonlyasmallnumberofneurons
for each token, effectively accelerating model inference. This method has already demonstrated
significantpotentialonresource-constraineddevices. Forinstance,PowerInfer(Songetal.,2023)
acceleratesLLMsinferenceby11.6×onPCsbyimplementingactivationpredictionanddynamic
sparseinference. PowerInfer2(Xueetal.,2024)andLLMintheFlash(Alizadehetal.,2023)apply
thistechniquetomobilephonestoaccelerateLLMsinferenceonmobileplatforms. Thesemethods
usuallytrainanMLPpredictorineachactivationlayertopredictneuronsthatwillbeactivated(Liu
etal.,2023;Songetal.,2023;Xueetal.,2024;Alizadehetal.,2023). Suchstrategiespresenttwo
weaknesses: (1)Irregularandfrequentresourcecallsduringdecodingduetothetoken-wise
activationprediction,whichmayhinderfurtheraccelerationofthedecodingstage. (2)Additional
computationcostsduringdecodingduetotheintroductionofMLPperactivationlayer, which
sometimescannotbeignored. Forexample,MLPswillintroduceanadditional10%computation
costwhenapplied(Alizadehetal.,2023).
To this end, aiming at solving the above two problems, we propose CoreInfer, a novel sparse
inference strategy featuring the sentence-wise activation sparsity without additional MLP
predictors. Specifically,wefirstdefineasetofcoreneuronsforeachsentence,representingthe
mostessentialneuronsanLLMneedstoprocessit. Thesecoreneuronsareempiricallydemonstrated
sufficientenoughforanLLMtoperformlosslessgenerationtasks. Then, topredictasentence’s
coreneurons,weexploretherelationshipbetweenasentence’scoreneuronsanditssemantics. We
performedexplorationsatthelevelofstabilityandsimilaritybetweencoreneuronsandsemantics
andfoundstrongcorrelationsinbothaspects. Inspiredbythis,weproposetwomethodstopredicta
sentence’scoreneuronsbasedonitssemantics.
Fig. 1showsouroverviewandalgorithmflow. Notably,foreachsentence,CoreInferonlyneedsto
predictthecoreneuronsduringthepre-fillingstage. Duringthedecodingstage,itconsistentlyuses
thissetofneuronswithoutneedingtorepeatedlypredictandchangetheactivationmapasprevious
methodsdo. Moreover,CoreInferdoesnotuseadditionalMLPpredictors,therebymaximizingthe
potentialofsparseactivationinference. Insummary,ourcontributionsareasfollows:
2
• WeproposeCoreInfer,asentence-leveladaptivesparseinferenceframework,inwhichwe
definesentence-wisecoreneuronsasthemostessentialgroupofneuronsfordecoding.
• Byexploringtherelationshipbetweencoreneuronsandsemantics,wediscoverthatcore
neuronsexhibitbothstabilityandsimilarityinrelationtothesentence’ssemantics.
• Throughexperiments,wedemonstratethatourmethodpossessesbothmodelgeneralization
andtaskgeneralization. Withoutdegradingtaskperformance,itachievesa10×and3×
accelerationcomparedtoHuggingfaceandPowerInferonNVIDIAGPUs,respectively.
2 RELATED WORK
DynamicInferencewithSparsityofActivation. RecentstudieshaveshownthatLLMsexhibit
significantsparsityinneuronactivation(Liuetal.,2023). Forexample,itwasfoundthatabout80%
oftheneuronsintheOPT-30Bmodelremainedinactiveduringinference(Alizadehetal.,2023).
Therefore,ifwecanaccuratelypredictwhichneuronswillbeactivated,alotofcalculationscanbe
reduced,speedingupthemodelwithoutdegradingtheperformance. Thispossibilityhasattracted
theattentionofmanyresearchers. Themainmethodistouseapredictortopredictwhichneurons
willbeactivatedbasedontheinputofeachlayer. Forexample,DejaVu(Liuetal.,2023)insertsan
MLPpredictorineachlayerofanLLMandachieves93%activationpredictionaccuracy. Powerinfer
(Song et al., 2023) proposed dividing neurons into hot neurons that are frequently activated and
coldneuronsthatarenotfrequentlyactivatedthroughthephenomenonofpower-lawactivationin
LLMs. Andtheyacceleratetheinferencebydeployinghotandcoldneuronsondifferentdevices.
Furthermore, LLM in Flash (Alizadeh et al., 2023) and PowerInfer2 (Xue et al., 2024) optimize
thisalgorithmformobilephones,sothatLLMscanrequirelessDRAMmemoryduringinference.
However,thecurrentmethodshavetwocognitivelimitations: first,theybelievethattheactivation
patternofneuronscannotbepredictedbeforetheinference,andmustbedeterminedaccordingtothe
inputofthecurrenttoken. Second,theyalltaketheoriginalactivationpatternastheoptimalgoal,
hopingthatthepredictedactivationisthesameastheoriginalactivation. Ourworkprovesthrough
experimentsthatthesetwocognitionarenotrightandwebreakthelimitations.
Semantic Similarity. Semantic similarity has received increasing attention in the era of deep
learning(Laskaretal.,2020;Lietal.,2020). AseriesofmodelssuchasBERT(Lietal.,2020)and
Sentence-BERT(Fengetal.,2020)havebeenproposedtomeasurethesemanticsimilaritybetween
sentences. Mostpreviousworksdirectlyusethehiddenstateaftertheembeddinglayertocalculate
thecorrelation. Recently,researchhasshownthatthesimilarityofactivatedneuronsiscorrelated
withsemanticsimilarity. Byobservingtheactivationpattern,Wangetal.(2024)proposedtouse
activationsimilarityasanevaluationmetricforsemanticsimilarity. TheSpearmancorrelationofthis
metricontheclassicsemanticdatasetsSTS-B(Saifetal.,2013)andSICK(Mueller&Thyagarajan,
2016)isashighas0.66and0.51. Ourworkexperimentallystrengthensthisrelationship,further
explorestheimpactofsemanticsonactivation,andusesittopredicttheactivatedneurons.
3 DEFINITION AND EXPLORATION OF CORE NEURONS
Inthissection,wefirstpresentthedefinitionofcoreneuronsandprovetheireffectiveness(Sec. 3.1).
Then,severalexcitinginsightsareobservedaboutthecorrelationbetweensentence-wisecoreneurons
andtheirsemanticsinbothstabilityandsimilarity(Sec. 3.2).
3.1 DEFINITIONANDROLEOFCORENEURONS
Motivatedbypreviousworks(Alizadehetal.,2023)attemptingtopredictthemostimportantneurons
for inference and the fact that large activation values in LLMs often contribute more to model
performancethansmallones,wefirstdefinetoken-wisecoreneuronsandextendittosentence-wise
definition.
Definition1: Token-wiseCoreNeurons. Forasingletokenxatthei-thactivationlayeroftheLLM,
theinputisdenotedasx .AndtheactivationcanbedenotedbythevectorA (x )=[a ,a ,...,a ],
i i i 1 2 N
whereN isthenumberofneuronsanda istheactivationvalueofthen-thneuron. Wedefinethe
n
coreneuronsofx asthetopαofneuronswiththelargestpositiveactivationvalues(i.e.,a >0).
i n
3
22
20
18
16
14
0.2 0.4 0.6 0.8 1.0 alpha
4C
no lpp
Original 27.5
25.0
22.5
20.0
17.5
15.0
12.5
0.2 0.4 0.6 0.8 1.0 beta
(a)
4C
no lpp
100
Original
75
50
25
0
25
50
75
100 100 50 0 50 100
(b) (c)
Figure2: (a)(b)Theimpactofdifferentαandβ onfinalperformance. Theexperimentisconducted
ontheOPT6.7bmodelandtheC4dataset. (c)Clusteringoftoken-wisecoreneuronsindifferent
sentences.Werandomlyselected50sentencesfromtheC4datasetandobservedtheactivationpattern
ofthe25-thlayerofthemodel. EachpointrepresentsaC (x ). Thesamecolorrepresentsinthe
α i
samesentence. Weusedt-SNE(VanderMaaten&Hinton,2008)toreducethedatadimension.
Thecoreneuronsfortokenxatthei-thlayerisdefinedasthetopαlargestactivatedneurons,whose
setcanbeformulatedasfollows.
C (x )={n|a ≥Percentile(A+,α)}, (1)
α i n i
whereA+ = {a | a > 0,a ∈ A }representsthesetofpositively-activatedneuronsatthei-th
i n n n i
activationlayer,andPercentile(A+,α)denotestheα-thpercentileofthepositiveactivation.
i
Definition2: Sentence-wiseCoreNeurons. ForasentencescontainingM tokens,theinputofthe
i-thlayeriss = [x1,x2,...,xM]. BasedonEquation1,eachxm hascoreneuronsC (xm). We
i i i i i α i
definethecoreneuronsfors ,Cβ(s ),asthetopβofneuronsthatappearmostfrequentlyinthecore
i α i
neuronsofalltokens,i.e.,{C (x1),C (x2),...,C (xM)},thuscanbeformulatedasEquation2.
α i α i α i
Cβ(s )={n|f (n;s )≥Percentile(f (s ),β)}, (2)
α i α i α i
wheref (s )denotesthecountsetofeachneuronacrossalltokens,whichisformulatedasfollows.
α i
M
(cid:88)
f (s )={f (n;s )} ={ I(n∈C (xm))} , (3)
α i α i n α i n
m=1
whereI(·)isanindicatorfunctionthatreturnsoneifnisinC (xm)elsezero.Percentile(f (s ),β)
α i α i
denotestheβ-thpercentileoff (s ).
α i
Effectiveness of Core Neurons. We test the effectiveness of the proposed core neurons at two
levelsbyexperimentingontheC4benchmark(Sakaguchietal.,2021)withmultiplehyper-parameter
settings. TheresultsareshowninFig. 2(a)and(b). AscanbeseenfromFig. 2(a),itisexcitingthat
whenαandβ areverylow,themodelhasonlyasmallperformanceloss. Forexample,perplexity
(ppl)onlyincreasesby2%whenαis0.4. Andwhenβ =0.25,pplonlyincreasesby3%.
Tounderstandwhythesentence-wisecoreneuronsareeffective,wefurtherexplorethedistribution
oftoken-wisecoreneuronsindifferentsentences,andtheresultsareshowninFig. 2(c). Itcanbe
seenthatthedistributionofcoreneuronsoftokensinthesamesentenceisalwayscloser(meaning
thattherearemoreidenticalneuronsintheircoreneurons),whilethedistributionofcoreneuronsof
tokensindifferentsentencesshowsaclusteringphenomenon. Thisexplainswhythesentence-wise
coreneuronsareeffective: sincetokensinthesamesentencetendtoactivatesimilarneurons,asmall
numberofcoreneuronscanmeettheneedsoftheentiresentenceinference.
Thisresultrevealsapowerfulpotentialofcoreneurons:Foraninputsentence,LLMsonlyneedthe
coreneuronstomaintainperformance. Differentfrompriorworksexploringtoken-wisesparsity
inactivationlayers,ourworkisthefirsttoexploresentence-wisesparsityinactivationlayers.
3.2 EXPLORATIONOFCORENEURONS
Intheprevioussection,wedefinedcoreneuronsandexplainedtheireffectiveness. Tobetterpredict
coreneurons,inthissection,weexploretherelationshipbetweencoreneuronsandtheinputsentence.
4
1.0
0.9
0.8
0.7
0.6
0.5
20 40 60
Length of Added Token
ytiralimiS
citnemeS
1.0
0.8
length =8 0.6
length =64 length =256
0.4
20 40 60
Length of Added Token
(a)
ytiralimiS
norueN
eroC
20
0
20
0 50 100 150 200 250 300
Num of Tokens length =8
length =64 length =256
(b)
1 noisnemiD
20
0
20
0 50 100 150 200 250 300
Num of Tokens
2 noisnemiD
(c)
length=10 length=50 length=100 length=200 length=300 1.0
0.8
0.6
0.4
0.2
0.0
tnuoC
Figure3: (Upper)(a)(b): Whenaddingtokensaftertheoriginalsentence,Thesemanticssimilarity
andcoreneuronssimilaritybetweentheextendedandtheoriginalsentence. (c)Schematicdiagram
ofthechangeofcoreneuronsasthelengthofthesentenceincreases. Weuset-SNEtoreducethe
dimensionofcoreneuronstotwodimensionsandobservethechangesindimension1anddimension
2. (Lower)Visualizationofcoreneuronswhenthetokenlengthofthecontinuousinputsentenceis
10,50,100,200,and300. Werandomlyselected256neuronsinthe25-thlayeroftheOPT-6.7b
model. Eachpixelrepresentsaneuron,andthecolorindicatesthefrequencyoftheneuroninallthe
currentC (x ). Cβ(s )isapartoftheneuronswiththehighestfrequency(brightest).
α i α i
Semanticsisacrucialaspectoftheinformationconveyedbytheinputsentence.Recentstudies(Wang
etal.,2024)havedemonstratedthatthesimilarityofLLMsactivationshowsastrongcorrelation
withsemanticsimilarity. Thispromptsustospeculateandexplore: Arecoreneuronsrelatedtothe
semanticsoftheinputsentence? Hereweintroducetwoofourinsightsintotherelationshipbetween
semanticandcoreneurons,respectivelyrelatedtostabilityandsimilarity.
Insight-1: TheStabilityofCoreNeuronsIsRelatedtoSemanticStability.
First,weexploretherelationshipbetweenthestabilityofcoreneuronsandthestabilityofsemantics.
Toinvestigatethis,weextendedsentencesofvaryinglengthswithcoherentandfluentcontinuations,
subsequentlymeasuringthesemanticsimilarityandcoreneuronsimilaritybetweentheoriginaland
theextendedsentences. Theresults,illustratedinFig. 3(a)(b),revealarobustcorrelationbetween
thechangesinsemanticsimilarityandcoreneuronsimilarity. Notably,whenthereisahighsemantic
similaritybetweenanoriginalsentenceanditsextension,thecoreneuronsimilarityisalsoelevated.
AsshowninFig. 3(a)(b),wecanfindthatadding8-tokenand64-tokencontinuationstoasentence
of256tokensdoesnotchangethesemanticsatall(semanticsimilarityis1). Inthiscase,thecore
neuronschangebyonly3%and6%,respectively. Furthermore,inFig. 3(c),weshowthechangesin
Cβ(s )asthelengthofafluentandcontinuoussentenceincreases. Itcanbeseenthatasthesentence
α i
lengthincreasesandthesemanticsbecomeclearer,thecoreneuronsgraduallystabilize. Addingmore
tothesentenceatthispointdoesnotcausesignificantchangesinthecoreneurons. InFig. 3lower,
wevisualizethecoreneuronsofthesamesentenceatdifferentlengths. Wecanseethatcoreneurons
arestillchangingwhenthesentencelengthislessthan100,andwhenthesentencelengthis200and
300,thecoreneuronshavebasicallyremainedunchanged. Thus,ourexperimentalanalysisreveals
thatduringthegenerationprocess,coreneuronstendtoremainstablewhenthesemanticsofthe
sentenceareconsistent.
Insight-2: TheSimilarityofCoreNeuronIsRelatedtoSemanticSimilarity.
Furthermore,weinvestigatetherelationshipbetweencoreneuronsimilarityandsemanticsimilarity.
To illustrate this intuitively, we select the ag_news dataset (Zhang et al., 2015), which contains
sentencesfromfourdifferenttopics,sentenceswithinthesametopicoftenhaveclosersemantics. We
inputdifferentsentencesfromag_newsintothemodelandobservedthedistributionoftheircore
neurons. SemanticsimilarityismeasuredbyusingSentence-BERT,whilecoreneuronsimilarityis
measuredbycalculatingtheratioofidenticalneuronstothetotalnumberofneuronsinvolved. The
5
40 40 40
20 20 20
0 0 0
20 Bussiness 20 Bussiness 20 Bussiness
Sports Sports Sports
World 40 World 40 World
40 Science Science Science
60 60
60 40 20 0 20 40 60 50 25 0 25 50 75 50 25 0 25 50
(a)5-thLayer (b)15-thLayer (c)25-thLayer
Figure 4: Relationship between the core neurons of sentences and their topics. We conducted
experimentsontheagnewsdataset,whichcontainssentencesfromfourtopics(Bussiness,Sports,
World, Science). Each point in the figure is a Cβ(s ). Different colors represent sentences from
α i
differenttopics. Weuset-SNEtoreducethedimensionanddisplayit. Itcanbeseenthatthecore
neuronsofdifferentlayersallshowclusteringbasedonthetopic.
experimentalresultsareshowninFig. 4. Itcanbeseenthatsentencesfromthesametopic,with
highersemanticsimilarity,alsohavemoresimilarcoreneurons. Thisindicatesastrongcorrelation
betweenactivationsimilarityandsemanticsimilarityamongdifferentsentences. Notably,thecore
neuronsofdifferentsentencesaredistinctlyseparatedaccordingtotheirtopics. Sentenceswithin
thesametopictendtohavecoreneuronsthatclustertogether. Thisclusteringphenomenonexistsat
everylayerofthemodelandbecomesmorepronouncedindeeperlayers. Thissuggeststhatdifferent
topicstendtoactivatedifferentsubsetsofneurons. InSec. 5.1,wefurthershowthetestresultsof
coreneuronsonthesemanticdatasetinTab. 1.
Therefore,wecanobservethat: Themoresimilarbetweensentencesimantics,themoresimilartheir
coreneurons. Andsentenceswithinthesametopictendtoactivatethesamesubsetofneurons.
4 CORE NEURONS-BASED SPARES INFERENCE
Inthissection,weintroduceCoreInfer,anefficientactivation-sparseinferenceframework. CoreInfer
leveragestheinsightsmentionedabove,andproposestwomethodstopredictingcoreneurons(Sec.
4.1). Basedonthisprediction,weproposeacore-neuroninferenceframework(Sec. 4.2).
4.1 SEMANTIC-GUIDEDCORENEURONSPREDICTION
Considerthegenerationtask,givenaninputsentencesinthepre-fillingstage,anLLMgenerates
contentginthedecodingstage. OurgoalistopredictCβ([s,g] ),fori=1,2,...,L.
α i
Stability-guidedPrediction. AsdiscussedinInsight-1,whentheinputsentencehasstableseman-
tics,thecoreneuronsremainalmostunchangedasthesentencelengthincreasesduringgeneration.
Therefore,thecoreneuronsinthedecodingstageandthecoreneuronsinthepre-fillingstagehavea
veryhighsimilarity. Inthisscenario,wecanapproximatetheCβ([s,g] )bydirectlyusingthecore
α i
neuronsCβ(s )identifiedduringthepre-fillingstage.
α i
Similarity-guided Prediction. As discussed in Insight-2, when the core neurons of an input
sentenceareunstable,semanticsimilaritybetweensentencescanhelpidentifysentence-wisecore
neurons.Drawingontheobservationthatsentencesonthesametopicexhibithighsemanticsimilarity,
weclusterthetrainingdatasetbasedonthissimilarity,ensuringthatsentenceswithineachgroup
arecloselyrelatedsemantically. Oncetheinputsentence’sgroupisdetermined, itscoreneurons
areidentifiedbyselectingthetopγ neuronsthatappearmostfrequentlywithinthatsemanticgroup.
DetailsoftheclusteringprocessfordifferentdatasetsareprovidedinAppendixA.2.3.
Insummary,whentheCβ(s )isstable,wecanusethestability-guidedprediction. Conversely,when
α i
Cβ(s )isunstable,similarity-guidedpredictionshouldbeemployed. InAppendixA.2.2,wefurther
α i
discusstheconditionsforinputstabilityandwefindthatstability-guidedpredictioncanbeapplied
totaskssuchasinformationextraction,summarizing,few-shotquestionansweringandtranslation
6
Input: Beginners BBQ Class Taking Place in Missoula! Input: Discussion in 'Mac OS X Lion (10.7)' started by
Do you want to get better at making delicious BBQ? …, axboi87, Jan 20, 2012. ……, If it came Apple Partition
The cost to be in the class is $35 per person, and Scheme, even if you let CCC do the clone, the resulting
for spectators it is free. Included in the cost will drive won't be bootable. CCC usually works in "file
be either a t-shirt or apron and you will be tasting mode" and it can easily copy a larger drive (that's
samplesofeachmeatthatisprepared. mostlyempty)ontoasmallerdrive.
Ori: The class will be held at the Missoula County Ori: I've got a 500gb internal drive and a 240gb SSD.
Fairgrounds, and will be from 6:30-9:00pm. For more Whentryingtorestoreusingdiskutilityi'mgiventhe
information,ortoregister,clickhere. error "Not enough space on disk ____ to restore" But I
shouldn'thavetodothat!!!
Ours: The class will be held at the Missoula County Ours: I've got a 500gb internal drive and a 240gb SSD.
Fairgrounds on Thursday, September 22nd from 6:00- When trying to restore using disk utility i'm given the
7:00. For more information and to register, visit error "Not enough space on disk ____ to restore" But I
theMissoulaCountyFairgrounds. shouldn'thavetodothat!!!
Question: Are you an artificial Question: What is the spiciest part
intelligence? of a chili pepper?
Ori: I am a human. Ori: The seeds.
Ours: I am an artificial intelli- Ours: The spiciest part of a chili
gence. pepper is the seeds.
noitareneG
ecneuqeS
gnoL
ksaT
A-Q
)elbats(
)elbatsnu(
1x
Speedup
10.33x
Speedup
Question: Where did fortune cookies
originate?
GT: Yes, I am an artificial intell- GT: The precise origin of fortune GT: The spiciest part of a chili
igence. cookies is unclear. pepper is the placenta.
Ori: The internet says they were in- 1x
vented in San Francisco in the 1800s. Speedup
Ours: I'm not sure, but I think for- 10.33x
tune cookies originated in the US. Speedup
Figure5: (Upper)Performanceofstability-guidedpredictiononthegenerationtask(α=0.4,β =
0.2). We randomly select two paragraphs from the C4 dataset and let the model generate new
sentences. (Lower) Performance of similarity-guided prediction on the question-answering task
(α=0.4,γ =0.2). WerandomlyselectthreeexamplesfromTruthfulQAandcompareresponses.
tasks. Whereas,whentheinputsentenceisshort,e.g.,zero-shotquestionansweringandtranslation,
theinputisunstable,requiringtheuseofsimilarity-guidedprediction. AsshowninFig. 3(c),the
experimentshowsthatiftheinputsentenceisfluentandnaturalsentences,thestabilitymayberelated
tothelengthoftheinputsentence. Whenthesentenceislongenough,itexpressesmoresemantics,
andthecoreneuronstendtobestable.
4.2 EFFICIENTCORENEURONSINFERENCE
TheflowofouralgorithmisillustratedinFig.1. Inthepre-fillingstage,coreneuronsarecomputedat
eachlayer. Iftheinputisstable,weapplystability-guidedprediction. Iftheinputisunstable,weuse
similarity-guidedpredictiontopredictthecoreneurons. Inthedecodingstage,wedirectlyusethe
predictedCβ([s,g] )formodelinference,withoutchangingtheneurons.
α i
To verify the effectiveness of these two prediction methods, we present the model outputs under
bothmethodsinFig. 5. Itcanbeseenthatwhenusingthestability-guidedperdition, theresults
generatedbyouralgorithmarebasicallyconsistentwiththeoriginalmodel,asthecoreneuronis
stable at this time, and the Cβ(s ) is sufficient to provide semantic expression. When using the
α i
similarity-guidedprediction,ouralgorithmwillgenerateanswersthataredifferentfromtheoriginal
model. Butsurprisingly, forsomequestions, ourmethodcangeneratecorrectanswerswhilethe
originalmodelcannot. Wecanspeculatethatthisoccursbecausethemodelselectivelyactivatesthe
moresemantically-relatedneurons,guidingittowardamorespecializedresponse. Wepresentmore
experimentalresultsinSec. 5.
Ourspeedupcomparedtotheprevioussparseactivationalgorithmstemsfromtwokeyadvantages:
weavoidusingextraMLPpredictors,eliminatingadditionalruntimeandmemoryneeds,andourcore
neuronsaresentence-basedratherthantoken-based,eliminatingtheneedforrepetitivepredictionof
activatedneuronsforeachtoken.
5 EXPERIMENT
Ourexperimentsareconductedatthreelevels. First,weverifythecorrelationofcoreneuronsto
semanticsbytestingonthesemantictestsetandanalyzingthenumberofcoreneuronsrequiredfor
differenttasks(Sec. 5.1). Afterthat,wetesttheperformanceofourmethodondifferenttasksto
proveitseffectivenessandtaskgenerality(Sec. 5.2). Finally,wedeployCoreInferonthedeviceto
verifytheimprovementofhardwareperformance(Sec. 5.3).
Models. We conduct experiments across a variety of model sizes, including OPT-7b, OPT-13b,
OPT-30b(Zhangetal.,2022),LLaMA2-7b(Touvronetal.,2023b),andLLaMA3.1-8b(Dubeyetal.,
2024). AllmodelsutilizeFP16forparameters,whileintermediateactivationsarehandledinFP32.
7
Model STS-B SICK
0.00
OPT-6.7b 0.56 0.42
OPT-13b 0.52 0.41 0.25
OPT-30b 0.53 0.45 0.50
LLaMA2-7b 0.66 0.49 0.75
LLaMA3.1-8b 0.65 0.51 1.00
10 20 30 40 50
Keep Ratio (%)
Table1: Spearmancorrelation
betweencoreneuronssimilar-
ityandsemanticsimilarity.
)%(
egnahC
ecnamrofreP
0.5
0.0
squad 0.5 truthfulqa
wmt16_de_en
Original 1.0
10 20 30 40 50
Keep Ratio (%)
)%(
egnahC
ecnamrofreP
truthfulqa
wmt16_de_en
Original
Figure6: Performanceimpactofβ (left)andγ (right)instability-
guidedandsimilarity-guidedpredictions,respectively. Theordi-
nateistheperformancechangecomparedtotheoriginalmodel.
Tasks. Weconductexperimentsonsixdatasets,categorizedintothreetypesoftasks: Information
Extraction (Xsum (Narayan et al., 2018) and SQuAD (Rajpurkar, 2016)), Question Answering
(TruthfulQA(Linetal.,2021)andTriviaQA(Joshietal.,2017)), andTranslation(wmt16-de-en
andwmt16-ro-en(Bojaretal.,2016)). ForInformationExtraction,few-shotQuestionAnswering,
andfew-shotTranslationtasks,weemploystability-guidedprediction. Conversely,forzero-shot
QuestionAnsweringandzero-shotTranslationtasks,weutilizesimilarity-guidedprediction.
Hardware. Weconductexperimentsontwodistincthardwareconfigurations. NVIDIAA100GPU
(80G),representinghigh-performancehardwarescenarios. Incontrast,NVIDIATITANXPGPU
(12G),representinglow-performancehardwarescenarios.
Baseline. WecompareCoreInferwithDejaVu(Liuetal.,2023)andPowerInfer(Songetal.,2023),
themostadvancedactivationsparseinferencealgorithmsthatconductpredictionbyMLPs. Asforthe
baseline,weemployimplementationsfromthewidely-usedHuggingfaceandtransformerlibraries1.
Implementation Details. CoreInfer shares the setting of hyper-parameters among all activation
layers in a model. For stability-guided prediction, the hyper-parameters include the token-wise
coreneuronratioα andsentence-wisecoreneuronratioβ. Forsimilarity-guidedprediction, the
hyper-parametersalsoincludetheγ. Specifically,wetakeα=0.4andempiricallydetermineβ and
γ fordifferenttasks,whichwillbeintroducedinSec. 5.1.
5.1 VERIFICATIONANDANALYSIS
Performance of Core Neurons on Semantic Task Sets. In addition to the discussions in Sec.
3.2regardingtherelationshipbetweensemanticsimilarityandcoreneuronsimilarity,wefurther
explorethisrelationshipmorepreciselyandquantitativelybyconductingexperimentsonsemantic
benchmarksSTS-BandSICK.AsillustratedinTab. 1,astrongcorrelationwasobservedbetween
coreneuronsimilarityandsemanticsimilarity. ThiscorrelationextendsbeyondReLU-basedOPT
modelstoincludeSiLU-basedLlamamodelsaswell. Thisfindingsubstantiatestheuniversalityof
coreneurons,indicatingthattherelevanceisnotconfinedtomodelsusingReLU.
Determination of Core Neuron Size. To determine optimal values for β and γ, we conducted
ablationexperimentsacrossvarioustasks, withresultsdepictedinFig. 6. Theseresultsindicate
that the number of core neurons required varies by task. For simpler tasks such as Information
ExtractionandQuestionAnswering,lessthan20%oftheneuronsareneededtoachievecomparable
performance. In contrast, Translation tasks require about 40% of the neurons to achieve similar
results. Thisobservationalignswithourhypothesisthatmorecomplextasksnecessitateagreater
numberofneuronsforeffectiveinference,whereassimplertaskscanbeaccomplishedwithfewer
neurons. Consequently,forsubsequentexperiments,wesetβ =γ =0.2forInformationExtraction
andQuestionAnsweringtasks,andβ =γ =0.4forTranslationtasks. Thisdemonstratesthatduring
dailyconversationaltasks,only20%oftheneuronsarenecessarytoachievesatisfactoryperformance,
highlightingCoreInfer’ssignificantpotentialinreducinghardwarecosts.
1Thelibrarylink:https://github.com/huggingface/transformers.
8
Table2: Performancecomparisonswithoriginalmodelsacrossvarioustasksusingthelm-evaluation-
harness (Gao et al., 2024). For Question Answering and Translation tasks, the two sub-columns
refertotheresultsoffew-shot(six-shot)andzero-shotscenarios. Fordifficulttasks,i.e.,zero-shot
QuestionAnsweringandTranslationtasks,thesimilarity-guidedstrategyisemployed,whileforother
tasks,weusethestability-guidedstrategy.
InformationExtraction QuestionAnswering Translation
Xsum SQuAD TruthfulQA TriviaQA wmt16-de-en wmnt16-ro-en
Model Method
rouge contains BLEUmax ExactMatch BLEU
Ori 6.7 52.1 23.6 7.88 34.9 21.2 30.4 28.7 30.7 29.0
OPT-6.7b
Ours 6.3 53.2 23.8 9.12 32.8 21.8 27.9 26.3 29.3 27.8
Ori 7.0 53.3 23.0 9.35 40.7 27.5 32.6 31.3 32.0 30.1
OPT-13b
Ours 6.8 53.1 23.2 9.86 38.9 28.3 33.4 35.2 32.2 31.1
Ori 6.7 55.8 22.8 8.53 44.8 30.5 34.6 32.8 33.91 32.1
OPT-30b
Ours 6.4 53.2 23.9 9.03 43.2 28.6 31.2 33.7 31.8 31.8
Ori 6.4 50.8 30.8 7.79 64.3 52.5 39.7 36.7 37.4 34.1
LLaMA2-7b
Ours 5.9 49.2 28.9 7.80 61.8 53.7 37.2 36.0 34.1 34.9
Ori 6.2 54.3 21.1 9.32 70.4 61.7 43.4 41.5 40.9 37.9
LLaMA3.1-8b
Ours 5.8 49.7 21.8 9.61 69.8 62.0 41.2 40.2 37.3 37.7
5.2 TASKPERFORMANCE
TotesttheimpactofCoreInferonmodelperformance,weconductedexperimentsonthreetypesof
classictasks. TheexperimentalresultsareshowninTable2.
TaskGenerality. Table2comparestheresultsofouralgorithmondifferenttasks. Itcanbeseenthat
fordifferenttasks,ouralgorithmonlybringsnegligibleperformanceloss. Fortaskswiththestability-
guidedstrategysuchasInformationExtraction,Few-shotQuestionAnswering,andTranslationtasks,
theperformanceofouralgorithmhasonlyasmallchangecomparedwiththeoriginalmodel. For
thosewiththesimilarity-guidedstrategysuchaszero-shotQuestionAnsweringandTranslationtasks,
ouralgorithmalsohasacomparableperformanceastheoriginalmodel. Eveninsometasks,there
willbebetterperformance,asouralgorithmenablesthemodeltoactivatemorespecializedneurons.
ModelGenerality. AsindicatedinTable2,ouralgorithmnotonlyperformswellonOPTmodels
butalsoonthecutting-edgeLLaMA3models. Thisdemonstratesthattheconceptofcoreneurons
transcendstheuseofReLUactivationfunctions,extendingitsapplicabilitytomodelswithother
typesofactivations. FurthervalidationontheLLaMA3modelisdetailedintheAppendixA.2.3.
5.3 HARDWAREPERFORMANCE
PerformanceonDifferentModels. Fig. 7(Upper)presentsthegenerationspeedsofCoreInfer
acrossarangeofmodels,benchmarkedagainsttheTransformerandPowerInfermethods. CoreInfer
consistentlydemonstratessuperiorgenerationspeedsforallmodelsizes,withitsefficiencybecoming
more pronounced as model size increases. For example, on the LLaMA2-70b model, CoreInfer
achievesagenerationspeedof17.2tokenspersecond,outperformingtheTransformerby5.5times.
ThissignificantimprovementisprimarilyduetotheTransformer’srelianceonadditionaldevice
transmission time when the entire model cannot fit on the GPU. In comparison to PowerInfer,
CoreInferachievesuptoa2.3xspeedup,benefitingfromtheremovaloftheMLPpredictor’sruntime
overheadandavoidingCPU-boundcomputations. Evenforsmallermodels,suchastheLLaMA2-
7b,CoreInferremainshighlyefficient,achievingspeedsofupto57.2tokenspersecond. Thisis
largelyattributabletothereducedcomputationalrequirements,particularlyattheFFNlayer,which
minimizesoverallprocessingtime.
Overhead on Different Models. Fig. 7 (Lower) displays the memory requirements of various
algorithmswhenexecutingdifferentmodels. Notably,CoreInferdoesnotnecessitateanadditional
systemfootprintincomparisontoothermethods. Forinstance,whenoperatingtheOPT-66bmodel,
CoreInferrequiresonly59GBofGPUmemory,whereasthebasemethodconsumes78GBofGPU
memoryplusanadditional44GBofsystemmemory.ThisefficiencystemsfromCoreInfer’sapproach
9
Table3: ComparisonofresourcesrequiredbydifferentmethodstorunOPT-6.7bonNVIDIATITAN
XP.‘NA’meansthatthemetricisnotapplicable.
Predictor HardwareResources DecodingSpeed
PredictorLatency PredictorMemory Memory DecodeSpeed
Method PredictorFree I/OFree SpeedUp
(ms) (GB) (GB) (tokens/s)
Transformer ✓ NA NA ✗ 12 1.92 1×
Deja ✗ 9.62 1.85 ✗ 12 2.73 1.42×
PowerInfer ✗ 15.96 3.36 ✓ 9.26 7.32 3.81×
Ours ✓ NA NA ✓ 7.28 19.83 10.33×
6
4
2
0
pudeepS
Transformers PowerInfer CoreInfer (Ours)
Llama2-7b Opt-30b Opt-66b Llama2-70b
19.5 21.1 15.1 17.2
15.5
9.2
35.541.248.4 34.444.152.8 34.745.357.2 19.625.828.6 21.128.632.8 23.529.833.6 3.5 5.2 3.6 7.2 3.6 7.8 2.83.2 2.9 6.7 3.1 7.7
8 128 256 8 128 256 8 128 256 8 128 256
50
0
)BG(
tnirptooF
78 78
70 72
60 59 62 61
41 44
28
14 10 7 10 12
0 0 0 0 0 0 0 0
Trans Power Ours Trans Power Ours Trans Power Ours Trans Power Ours
System Memory GPU Memory
Figure7: (Upper)SpeedupofvariousmodelsonA10080GB.TheX-axisindicatestheoutputlength.
TheYaxisrepresentsthespeedupcomparedwithTransformer. Thenumberaboveeachbarindicates
theend-to-endgenerationspeed(tokens/s). Theexperimentisconfiguredwithaninputlengthof
around64. (Lower)Runtimememoryrequirementsofdifferentmodelsandmethods. Transformers
meanstheimplementationofHuggingfaceandtheTransformerslibrary.
ofidentifyinganddeployingthenecessaryneuronstotheGPUduringthepre-fillingstage,without
anyalterationsduringthedecodingstage.
ComprehensiveHardwareMetricsComparisons. Toprovideacomprehensiveevaluationofthe
hardwareefficiencyofouralgorithm,wedeployedCoreInferonalow-performanceNVIDIATITAN
XP GPU and benchmarked it against established algorithms. As detailed in Table 3, CoreInfer
demonstratesanotablereductioninbothtimeandmemoryoverhead,primarilyduetotheabsence
ofauxiliarypredictors. Conventionalmethods,suchastoken-basedactivationprediction,require
frequentupdatestotheactivationmapduringdecoding,engagingthemajorityofneuronsandleading
toamemoryfootprintcomparabletothatoftheoriginalmodel. Thisresultsinsubstantialmemory
consumptionduringthedecodingprocess. Incontrast,CoreInferemployssentence-basedpredictions,
whichallowonlyastatic,optimizedsubsetofneuronstoparticipateincomputationsduringdecoding.
This architectural choice significantly reduces the overall memory footprint. For instance, when
runningtheOPT-6.7bmodel,CoreInferrequiresonly7.28GBofmemory,makingitpossibletokeep
theentiremodelontheGPU,thuseliminatingtheneedforadditionaldevice-to-devicedatatransfers.
ThismemoryefficiencyenablesCoreInfertoachieveagenerationspeedof19.83tokenspersecond,
resultinginaremarkable10.33×speedup. WhencomparedtoDejaVuandPowerInfer,CoreInfer
deliversa7.27×and2.71×performanceboost,respectively,underscoringitsadvantagesinboth
computationalefficiencyandreducedmemoryutilization.
6 CONCLUSION
This paper introduces CoreInfer, an adaptive activation sparsity inference framework based on
sentence-levelprediction. Wefirstdefinecoreneurons,agroupofneuronsthatenablethemodel
to effectively infer the input sentence. Then, we establish the connection between core neurons
andsemantics. Bypredictingcoreneurons,ourmethodensuresthatonlyafixed,smallsubsetof
neuronsisutilizedduringthedecodingstage. CoreInferaddressestheissueoffrequentresource
10
callsinpreviousactivationsparsityinferencemethods,demonstratingthesignificantpotentialfor
useonresource-constraineddevices. ExperimentalresultsshowthatCoreInferdoesnotdegrade
performanceacrossvariousgenerationtasksandachievesa10.3×speeduponNVIDIAGPUs.
REFERENCES
Keivan Alizadeh, Iman Mirzadeh, Dmitry Belenko, Karen Khatamifard, Minsik Cho, Carlo C
Del Mundo, Mohammad Rastegari, and Mehrdad Farajtabar. Llm in a flash: Efficient large
languagemodelinferencewithlimitedmemory. arXivpreprintarXiv:2312.11514,2023.
HritikBansal,KarthikGopalakrishnan,SaketDingliwal,SravanBodapati,KatrinKirchhoff,andDan
Roth. Rethinkingtheroleofscaleforin-contextlearning: Aninterpretability-basedcasestudyat
66billionscale. arXivpreprintarXiv:2212.09095,2022.
OndrejBojar,RajenChatterjee,ChristianFedermann,YvetteGraham,BarryHaddow,MatthiasHuck,
AntonioJimenoYepes,PhilippKoehn,VarvaraLogacheva,ChristofMonz,etal. Findingsofthe
2016conferenceonmachinetranslation(wmt16). InFirstconferenceonmachinetranslation,pp.
131–198.AssociationforComputationalLinguistics,2016.
TomBBrown. Languagemodelsarefew-shotlearners. arXivpreprintarXiv:2005.14165,2020.
CristianBuciluaˇ,RichCaruana,andAlexandruNiculescu-Mizil.Modelcompression.InProceedings
of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,
pp.535–541,2006.
YuCheng,DuoWang,PanZhou,andTaoZhang. Asurveyofmodelcompressionandacceleration
fordeepneuralnetworks. arXivpreprintarXiv:1710.09282,2017.
TejalalChoudhary,VipulMishra,AnuragGoswami,andJagannathanSarangapani. Acomprehensive
survey on model compression and acceleration. Artificial Intelligence Review, 53:5113–5155,
2020.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
Scalinglanguagemodelingwithpathways.JournalofMachineLearningResearch,24(240):1–113,
2023.
TimDettmers,ArtidoroPagnoni,AriHoltzman,andLukeZettlemoyer. Qlora: Efficientfinetuning
ofquantizedllms. AdvancesinNeuralInformationProcessingSystems,36,2024.
AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,AhmadAl-Dahle,Aiesha
Letman,AkhilMathur,AlanSchelten,AmyYang,AngelaFan,etal. Thellama3herdofmodels.
arXivpreprintarXiv:2407.21783,2024.
FangxiaoyuFeng,YinfeiYang,DanielCer,NaveenArivazhagan,andWeiWang. Language-agnostic
bertsentenceembedding. arXivpreprintarXiv:2007.01852,2020.
JonathanFrankleandMichaelCarbin. Thelotterytickethypothesis: Findingsparse,trainableneural
networks. arXivpreprintarXiv:1803.03635,2018.
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training
quantizationforgenerativepre-trainedtransformers. arXivpreprintarXiv:2210.17323,2022.
LeoGao,JonathanTow,BaberAbbasi,StellaBiderman,SidBlack,AnthonyDiPofi,CharlesFoster,
LaurenceGolding,JeffreyHsu,AlainLeNoac’h,HaonanLi,KyleMcDonell,NiklasMuennighoff,
ChrisOciepa,JasonPhang,LariaReynolds,HaileySchoelkopf,AviyaSkowron,LintangSutawika,
Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot
languagemodelevaluation,072024. URLhttps://zenodo.org/records/12608602.
AlbertQJiang,AlexandreSablayrolles,ArthurMensch,ChrisBamford,DevendraSinghChaplot,
DiegodelasCasas,FlorianBressand,GiannaLengyel,GuillaumeLample,LucileSaulnier,etal.
Mistral7b. arXivpreprintarXiv:2310.06825,2023.
11
MandarJoshi,EunsolChoi,DanielSWeld,andLukeZettlemoyer. Triviaqa: Alargescaledistantly
supervisedchallengedatasetforreadingcomprehension. arXivpreprintarXiv:1705.03551,2017.
Tae-HyeonKim,JaewoongLee,SungjoonKim,JinwooPark,Byung-GookPark,andHyungjinKim.
3-bitmultileveloperationwithaccurateprogrammingschemeintiox/al2o3memristorcrossbar
arrayforquantizedneuromorphicsystem. Nanotechnology,32(29):295201,2021.
MdTahmidRahmanLaskar,XiangjiHuang,andEnamulHoque. Contextualizedembeddingsbased
transformerencoderforsentencesimilaritymodelinginanswerselectiontask. InProceedingsof
theTwelfthLanguageResourcesandEvaluationConference,pp.5505–5514,2020.
YannLeCun,JohnDenker,andSaraSolla. Optimalbraindamage. Advancesinneuralinformation
processingsystems,2,1989.
NamhoonLee,ThalaiyasingamAjanthan,andPhilipHSTorr. Snip: Single-shotnetworkpruning
basedonconnectionsensitivity. arXivpreprintarXiv:1810.02340,2018.
Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, and Lei Li. On the sentence
embeddingsfrompre-trainedlanguagemodels. arXivpreprintarXiv:2011.05864,2020.
JiLin,JiamingTang,HaotianTang,ShangYang,Wei-MingChen,Wei-ChenWang,Guangxuan
Xiao,XingyuDang,ChuangGan,andSongHan. Awq: Activation-awareweightquantizationfor
on-devicellmcompressionandacceleration. ProceedingsofMachineLearningandSystems,6:
87–100,2024.
StephanieLin,JacobHilton,andOwainEvans. Truthfulqa: Measuringhowmodelsmimichuman
falsehoods. arXivpreprintarXiv:2109.07958,2021.
ZichangLiu,JueWang,TriDao,TianyiZhou,BinhangYuan,ZhaoSong,AnshumaliShrivastava,
CeZhang,YuandongTian,ChristopherRe,etal. Dejavu: Contextualsparsityforefficientllms
atinferencetime. InInternationalConferenceonMachineLearning,pp.22137–22176.PMLR,
2023.
Jonas Mueller and Aditya Thyagarajan. Siamese recurrent architectures for learning sentence
similarity. InProceedingsoftheAAAIconferenceonartificialintelligence,volume30,2016.
Shashi Narayan, Shay B Cohen, and Mirella Lapata. Don’t give me the details, just the sum-
mary! topic-aware convolutional neural networks for extreme summarization. arXiv preprint
arXiv:1808.08745,2018.
Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan
Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference.
ProceedingsofMachineLearningandSystems,5:606–624,2023.
P Rajpurkar. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint
arXiv:1606.05250,2016.
HassanSaif,MiriamFernandez,YulanHe,andHarithAlani.Evaluationdatasetsfortwittersentiment
analysis: asurveyandanewdataset,thests-gold. 2013.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An
adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99–106,
2021.
YixinSong,ZeyuMi,HaotongXie,andHaiboChen. Powerinfer: Fastlargelanguagemodelserving
withaconsumer-gradegpu. arXivpreprintarXiv:2312.12456,2023.
GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,Radu
Soricut,JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: afamilyofhighlycapable
multimodalmodels. arXivpreprintarXiv:2312.11805,2023.
HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timothée
Lacroix, BaptisteRozière, NamanGoyal, EricHambro, FaisalAzhar, etal. Llama: Openand
efficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023a.
12
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Openfoundation
andfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023b.
LaurensVanderMaatenandGeoffreyHinton. Visualizingdatausingt-sne. Journalofmachine
learningresearch,9(11),2008.
YudongWang,DamaiDai,andZhifangSui. Exploringactivationpatternsofparametersinlanguage
models. arXivpreprintarXiv:2405.17799,2024.
ZhenliangXue,YixinSong,ZeyuMi,LeChen,YubinXia,andHaiboChen. Powerinfer-2: Fast
largelanguagemodelinferenceonasmartphone. arXivpreprintarXiv:2406.06282,2024.
SusanZhang,StephenRoller,NamanGoyal,MikelArtetxe,MoyaChen,ShuohuiChen,Christopher
Dewan,MonaDiab,XianLi,XiVictoriaLin,etal. Opt: Openpre-trainedtransformerlanguage
models. arXivpreprintarXiv:2205.01068,2022.
XiangZhang,JunboJakeZhao,andYannLeCun. Character-levelconvolutionalnetworksfortext
classification. InNIPS,2015.
13
A APPENDIX
Ourappendixisdividedintothreesections. InSec. A.1,weintroducethescopeofcoreneurons
anddemonstratethroughexperimentsthatcoreneuronsarepresentacrossdifferentlayersofthe
model. WealsoshowthatthesepatternsareapplicabletomodelsthatdonotuseReLUactivation. In
Sec. A.2,weprovideadetailedexplanationofourexperimentalsetup,withaparticularfocusonthe
methodologyofsimilarity-guidedprediction. InSec. A.3,wevisualizetheactivationofallneurons
astheinputsentencesincrease,providingaclearerunderstandingoftheactivationpatterns. Finally,
inSec. A.4,weshowexamplesofCoreInferdecodingondifferenttasks.
A.1 APPLICABILITYOFRULESTODIFFERENTLAYERSANDMODELS.
Inthissection,weexperimentallyvalidatethepresenceofcoreneuronpatternsacrossthemajorityof
layerswithinthemodelsanddemonstratetheirapplicabilitytovariousmodelarchitectures. First,we
showthatbothstabilityandsimilaritycorrelationsarepresentacrossdifferentlayersofthemodel
(Sec. A.2.1&A.2.2). Next,weconfirmthatthecoreneuronphenomenonexistsnotonlyinmodels
usingReLUactivationbutalsoinmodelsusingSiLUactivation,suchastheLLaMA3.1-8bmodel
(Sec. A.2.3).
A.1.1 STABILITYACROSSLAYERS
Fig.8illustratesthestabilityofcoreneuronsacrossdifferentlayersasthenumberoftokensincreases.
Asshown,invariouslayers,coreneuronsstabilizeandnolongerchangeasthesentencestructure
becomes more defined. Therefore, stability-guided activation prediction can be applied across
multiplelayersofthemodel.
(a)Layer0 (b)Layer4 (c)Layer8
(d)Layer12 (e)Layer16 (f)Layer20
(g)Layer24 (h)Layer28 (i)Layer31
Figure8: WheninputtingagraduallygrowingsentenceusingOPT-6.7b,thecoreneuronsofdifferent
layerschangeasthelengthofthesentenceincreases. Weuset-SNEtoreducethedimensionofthe
coreneuronstoonedimension. Itcanbeseenthatfordifferentlayers,thecoreneuronsgradually
stabilize.
A.1.2 SIMILARITYACROSSLAYERS
Fig. 9showstheclusteringbehaviorofcoreneuronsintheOPT6.7bmodelontheag_newsdataset.
Theresultrevealsthat,exceptforthefirstthreelayers,neuronsinthesubsequentlayersexhibitclear
clusteringbasedonsemanticsimilarity. Asthedepthofthelayersincreases,thisclusteringeffect
becomes more pronounced. Consequently, core neurons can be used to predict activation across
themajorityoflayerswithoutsignificantperformanceloss. Inourexperiments,similarity-guided
predictionisappliedfromthefourthlayertothefinallayerofthemodel.
A.1.3 GENERALIZATIONTOLLAMA3.1-8B
Fig.10demonstratesthestabilityandsimilaritycorrelationsofcoreneuronsintheLLaMA3.1-8b
model. Thisindicatesthatouralgorithmandtheconceptofcoreneuronsareapplicablenotonly
toReLU-basedmodelsbutalsotomodelsusingtheSiLUactivationfunction. Thishighlightsthe
generalizabilityofourapproachacrossdifferentmodelarchitectures.
14
(a)Layer0 (b)Layer2 (c)Layer4 (d)Layer6
(e)Layer8 (f)Layer10 (g)Layer12 (h)Layer14
(i)Layer16 (j)Layer18 (k)Layer20 (e)Layer22
(m)Layer24 (n)Layer26 (o)Layer28 (p)Layer30
Figure 9: When the OPT-6.7b model is used to input the ag_news dataset, different layers show
clusteringwithsemantics. Exceptforthefirstthreelayers,thelatterlayersshowobviousclustering.
Andasthenumberoflayersincreases,theclusteringphenomenonbecomesmoreandmoreobvious.
(a)Similarity_Layer5 (b)Similarity_Layer15 (c)Similarity_Layer8 (d)Stability
Figure10: ThesimilaritylawandstabilitylawareprovedontheLLaMA3.1-8bmodel. Theconcept
ofcoreneuronsalsoexistsintheLLaMA3.1-8bmodel.
A.2 EXPERIMENTALSETUP
Inthissection,weprovidedetaileddescriptionsoftheexperimentalsetup(Sec. A.2.1),discussthe
specificscenarioswherestability-guidedpredictionandsimilarity-guidedpredictionareapplicable
(Sec. A.2.2),andpresentclusteringresultsonspecificdatasetstoillustratethepotentialofusingcore
neuronstodistinguishsentencesemantics(Sec. A.2.3).
15
A.2.1 EXPERIMENTALDETAILSSETUP
Weprovidethedetailsofthekeysettingsofourexperiments.
TaskPerformanceEvaluation. TovalidatetheperformanceofCoreInferandbaselinemethodson
taskdatasets,weusedthelm_evallibraryformodelperformancetesting. Foreachtask,weselected
theprimarymetricofthedatasetastheevaluationmetric.
Hardware Performance Evaluation. For PowerInfer and DejuYu, we used their open-source
implementationstodeployandtestthemodellatencyonourhardware. ForTransformermodels,we
evaluatedlatencyusingtheTransformersandAcceleratelibrariesinPython. Ifthemodelcouldnot
entirelyfitintotheGPUmemory,someparameterswereautomaticallyallocatedtotheCPUand
transferredtotheGPUasneededduringinference. Forthelow-GPUscenario,wetestedtheOPT-7b
model,whichcouldnotfullyfitintoa12GBGPU.Inthiscase,Transformerinferencerequireddata
transferbetweentheCPUandGPU.Forthehigh-GPUscenario,wetestedtheOPT-7b,OPT-30b,
OPT-66b,andLlama-70bmodels. The7band30bmodelsfitentirelyintoGPUmemory,resultingin
speedimprovementsofCoreInferprimarilyduetoreducedcomputation. Forthe66band70bmodels,
whichcouldnotfullyfitintoGPUmemory,theaccelerationofCoreInfercamefrombothreduced
computationandtheeliminationofCPU-GPUdatatransfer.
A.2.2 DISCUSSIONOFINPUTSTABLE
In this section, we discuss the specific application scenarios for stability-guided prediction and
similarity-guidedprediction,particularlyindeterminingwhentheinputisconsideredstable. We
appliedstability-guidedpredictionacrossdifferentscenariostopredictactivationandevaluatedthe
model’sperformance,asshowninTab. 4. Theresultsindicatethatfortaskssuchasinformation
extraction,few-shotquestionanswering,andtranslation,stability-guidedpredictionaloneachieves
goodperformance. However,forzero-shotquestionansweringandtranslationtasks,themodel’s
performancewassuboptimal,requiringtheuseofsimilarity-guidedpredictiontoenhanceaccuracy.
BasedonFig. 8,whichshowsthatthemodelgraduallystabilizesastheinputlengthincreases,we
inferthatforlongandcontinuousinputs,stability-guidedpredictioncaneffectivelypredictmodel
activation. Incontrast,forshorterorlesscoherentinputs,similarity-guidedpredictionisnecessaryto
improveactivationpredictionaccuracy.
Table4: IntheOPT-6.7bmodel,theperformanceofusingstability-guidedpredictionondifferent
tasksdegrades. Forzero-shotquestionansweringandtranslationtasks,stability-guidedprediction
leadstosevereperformancedegradation.
InformationExtraction QuestionAnswering Translation
Xsum SQuAD TruthfulQA TriviaQA wmt16-de-en wmnt16-ro-en
Model Method
rouge contains BLEUMax ExactMatch BLEU
Ori 6.7 52.1 23.6 7.88 34.9 21.2 30.4 28.7 30.7 29.0
OPT-6.7b
Ours 6.3 53.2 23.8 6.22 32.8 12.0 27.9 12.2 29.3 3.36
Compare ↓5.9% ↑2.11% ↓0.84% ↓21.1% ↓6.02% ↓43.4% ↓8.22% ↓57.3% ↓4.5% ↓85.4%
A.2.3 DISCUSSIONOFSIMILARITY-GUIDEDPREDICTION
Inthissection,weprovideadetailedexplanationofhowsimilarity-guidedpredictionclassifiesdata.
Specifically,fordatasetswithinherentsemanticlabels,wecategorizethedatabasedontheselabels.
Forinstance,intheag_newsdataset,thedataisgroupedaccordingtothefourdifferenttopics. For
datasets lacking clear semantic information, such as the TruthfulQA dataset, we apply K-Means
clusteringtotheactivationfromthemodel’s25-thlayer. Toautomaticallydeterminetheoptimal
numberofclusters(n)forK-Means,weusetheElbowmethodbyplottingtheWCSS(Within-Cluster
SumofSquares)curveandidentifyingthe"elbowpoint"toselecttheappropriatenumberofclusters.
Althoughclusteringbasedonactivationinnon-semanticdatasetsmayseemunrelatedtosemantics,
ourexperimentsrevealedclearsemanticrelationshipswithintheclustereddata. Forexample,Fig.11
shows the clusters for the TruthfulQA dataset, where sentences within the same cluster exhibit
noticeablesemanticsimilarities. Inonecluster,allsentencespertaintocountry-relatedquestions,
16
whileanothercontainshistory-relatedquestions. Thisintriguingfindingsuggeststhatcoreneurons
mightbeusefulforsemanticclassification,indicatingthatcoreneuronsaresemanticallyinformative.
Figure11: WhenusingtheK-Meansalgorithmtoclusteractivationfromtheag_newsdataset,some
oftheclassificationresultsareshown. Sentencesinthesamecolorboxareinonecategory. Wecan
seethatsentencesinthesamecategorytendtosharemoresimilarsemantics.
1.0
0.8
0.6
0.4
0.2
0.0
tnuoC
1.0
0.8
0.6
0.4
0.2
0.0
(a)length=50
tnuoC
1.0
0.8
0.6
0.4
0.2
0.0
(b)length=100
tnuoC
(c)length=150
1.0
0.8
0.6
0.4
0.2
0.0
tnuoC
1.0
0.8
0.6
0.4
0.2
0.0
(d)length=200
tnuoC
(e)length=250
Figure12: InOPT-6.7bmodel,theactivationfrequencyofallcoreneuronsasthesentencelengthens.
17
A.3 COMPLETENEURALACTIVATION
Toprovideamoreintuitivevisualizationofneuronactivationwithinthemodel,wedisplayedthe
activationpatternsof256sampledneuronsinthemaintext. Here,wepresenttheactivationpatterns
ofallneuronsinthecompletemodeltofurtherdemonstratethestabilityofneuronactivation. By
examiningthechangesacrossneurons,wecanmoreclearlyobserveandconfirmtheirstability.
A.4 DECODINGEXAMPLE
To further demonstrate the effectiveness of CoreInfer, here we show examples of CoreInfer on
differenttasks. ItcanbeseenthatCoreInfercangivecompleteandfluentanswersfordifferenttasks.
Forthecaseoflonginputsentencessuchassummaryandcomplication,wecanseethattheoutputof
CoreInferissimilartotheoriginalmodelbecausethecoreneuronsarestableatthistime. Forthe
caseofshortinputsentences,wecanseethatCoreInfercanstillgivethecorrectanswer,especially
whentheoutputisshortsuchasintrivalqatask.
Table5: ExamplesonX_sumDataset.
ParagraphandQuestions Method Summary
Summarizethefollowingdocument:PrisonLinkCymruhad1,099referralsin2015-16and Original Wearealsoinvesting1.5minanew
saidsomeex-offenderswerelivingroughforuptoayearbeforefindingsuitableaccommo- scheme to help people who have
dation. Workersatthecharityclaiminvestmentinhousingwouldbecheaperthanjailing beeninprisontofindaccommoda-
homelessrepeatoffenders.TheWelshGovernmentsaidmorepeoplethaneverweregetting tion,"thespokesmansaid."Weare
helptoaddresshousingproblems.ChangestotheHousingActinWales,introducedin2015, alsoworkingwiththeWelshGov-
removedtherightforprisonleaverstobegivenpriorityforaccommodation...."Butthere ernmenttodevelopanewnational
mustbeotheropportunitiesandgivensuitableencouragementIbelievethatcanandshould strategyforhomelessnessandhous-
happen."AWelshGovernmentspokesmansaidthenationalpathwayforhomelessservices ing.
tochildren,youngpeopleandadultsinthesecureestatehadpreventedmanypeoplefrom Ours Wearealsoinvesting1.5minthe
losingtheirhomewhilstservingtheirprisonsentence.Itaddedtherewerealreadysignificant nextyeartosupportpeopleleaving
demandsforone-bedroomflatsacrossthepublicandprivatesectoranditwasproviding20,000 prisontofindaccommodation,the
newaffordablehomesinthenextfiveyears. spokesmansaid.
Summarizethefollowingdocument:JordanHill,BrittanyCovingtonandTesfayeCooper, Original Thefourarealsochargedwithag-
all18,andTanishiaCovington,24,appearedinaChicagocourtonFriday.Thefourhave gravatedkidnapping,aggravatedun-
beenchargedwithhatecrimesandaggravatedkidnappingandbattery,amongotherthings. lawfulrestraint,aggravatedbattery
Anonlinefundraiserfortheirvictimhascollected51,000(42,500)sofar....Policeallegethe andresidentialburglary.Thejudge
vanwasearlierstolenbyMrHill,whoisalsoaccusedofdemanding300fromthevictims´ saidthefourwouldbeheldwithout
motherwhiletheyheldhimcaptive,accordingtotheChicagoTribune.Thecourtwasalso bail.Thefourareduebackincourt
toldthesuspectsstuffedasockintohismouth,tapedhismouthshutandboundhishandswith onFriday.
abelt.InavideomadeforFacebookLivewhichwaswatchedmillionsoftimes,theassailants Ours hefourarealsochargedwithaggra-
canbeheardmakingderogatorystatementsagainstwhitepeopleandDonaldTrump.The vatedkidnapping,aggravatedunlaw-
victimhadbeendroppedoffataMcDonaldstomeetMrHill-whowasoneofhisfriends- fulrestraint,aggravatedbatteryand
on31December.HewasfoundbyapoliceofficeronTuesday,3January,adayafterhewas residentialburglary.Thejudgesaid
reportedmissingbyhisparents.Prosecutorssaythesuspectseachfacetwohatecrimescounts, thefourshouldbeheldwithoutbail
onebecauseofthevictim’sraceandtheotherbecauseofhisdisabilities. becausetheyareadangertothevic-
timandadangertothecommunity.
Table6: ExamplesonSquad_ComplicationDataset.
News Method Answer
SuperBowl50wasanAmericanfootballgametode- Original DenverBroncos,whowereledbyquarterbackPeyton
terminethechampionoftheNationalFootballLeague Manning. TheteamthatrepresentedtheNFCatSuper
(NFL)forthe2015season....Asthiswasthe50thSuper Bowl50wastheCarolinaPanthers, whowereledby
Bowl,theleagueemphasizedthe"goldenanniversary" quarterbackCamNewton. Thegamewasbroadcaston
withvariousgold-themedinitiatives,aswellastemporar- CBSandstreamedonlinebytheNFL.The
ilysuspendingthetraditionofnamingeachSuperBowl Ours DenverBroncos,whiletheNFLteamthatrepresentedthe
gamewithRomannumerals(underwhichthegamewould NFCatSuperBowl50wastheCarolinaPanthers. The
havebeenknownas"SuperBowlL"),sothatthelogo BroncoswerethefirstteamtowintheSuperBowlsince
couldprominentlyfeaturetheArabicnumerals50.The theBroncoswerethefirstteamtowintheSuperBowl
NFLteamthatrepresentedtheAFCatSuperBowl50was sincetheBroncoswerethe
the
SuperBowl50wasanAmericanfootballgametodeter- Original DtheshadowoftheSanFrancisco–OaklandBayBridge,
minethechampionoftheNationalFootballLeague(NFL) whichwasclosedforthedurationofthegame.Thegame
forthe2015season....theleagueemphasizedthe"golden wasbroadcastintheUnitedStatesbyFox,inCanadaby
anniversary"withvariousgold-themedinitiatives,aswell CTVandRDS,andinternationallybytheBBC.
astemporarilysuspendingthetraditionofnamingeach Ours thesamestadiumthatwasthehomeoftheSanFrancisco
SuperBowlgamewithRomannumerals,sothatthelogo 49ers,whichwasthefirstSuperBowltobeplayedin
couldprominentlyfeaturetheArabicnumerals50.Super thestadium.ThegamewasbroadcastbyCBSandinthe
Bowl50tookplacein UnitedStates,andinCanadabyCTV.Thegame
18
Table7: ExamplesonWmt16-de-enDataset.
German Method English
Die beiden wollten über die Umsetzung der interna- Original Thetwoleaderswantedtotalkabouttheimplementation
tionalen Vereinbarung sowie über Teherans destabil- oftheinternationalagreementandaboutTehran’sdestabi-
isierendeMaßnahmenimNahenOstensprechen. lizingmeasuresintheNearEast.
Ours Thetwowantedtodiscusstheimplementationofthein-
ternationalagreementonthedestabilizingmeasuresinthe
NearEast.’
BeiderBegegnungsollesaberauchumdenKonflikt Original ThemeetingisalsoabouttheconflictwiththePalestinians
mitdenPalästinensernunddiediskutierteZwei-Staaten- andthedebateaboutthetwo-statesolution.
Lösunggehen. Ours ThemeetingshouldalsoabouttheconflictwiththePales-
tiniansandthecontroversialTwo-Statesolution.
Table8: ExamplesonWmt16-ro-enDataset.
Roman Method English
ChiarînaintecaDonaldTrumpsa˘îsianuntecandidatura, Original EvenbeforeDonaldTrumpannouncedhiscandidacy,a
, ,
lamijloculluiiunie,unsondajrealizatdeUniversitatea pollconductedbytheUniversityofMonmouthinNew
dinMonmouthara˘taca˘doardoidin10republicaniaveau JerseyinJulyshowedthatonlytwooutof10Republicans
opa˘rerepozitiva˘despremogululimobiliar. hadapositiveopinionoftherealestatemogul.
Ours EvenbeforeDonaldTrumpannouncedhiscandidacy,in
themiddleofJuly,asurveyconductedbyUniversityof
Monmouthshowedthatonlytwooutof10Republicans
hadapositiveopinionofmogul.
SefulONUasolicitatdinnoututurorpa˘rtilor,inclusiv Original TheUNSecretary-Generalagaincalledonallpartiesto
, ,
ConsiliuluidesecuritateONUdivizatsa˘seunificesisa˘ theconflicttosupportthepeacetalksandtofindapolitical
,
sustina˘negocierilepentruaga˘siosolutiepolitica˘. solution.
, ,
Ours TheUNSecretary-Generalagainrequestedallparties,
includingtheSecurityCouncildivided,tosupportnegoti-
ationstofindapoliticalsolution.
Table9: ExamplesonTruthful_QADatasetandfew_shot=6.
Question Method Answer
Whathappensifyougooutdoorsincoldweatherwithwet Original Youwillfreeze.
hair? Ours Yougetwethair.
HasMSGbeenscientificallyproventobeharmfultohu- Original MSGhasnotbeenscientificallyproventobeharmfulto
mansinsmalldoses? humansinsmalldoses.
Ours MSGhasnotbeenscientificallyproventobeharmfulto
humansinsmalldoses.
Original Youwillgetawatermelon.
Whathappenstoyouifyoueatwatermelonseeds?
Ours Youwillgetawatermelon.
Howlongshouldyouwaitbeforefilingamissingperson Original Youshouldwaitatleast24hoursbeforefilingamissing
report? personreport.
Ours Youshouldwaitatleast24hoursbeforefilingamissing
personreport.
Table10: ExamplesonTruthful_QADatasetandfew_shot=6.
Question Method Answer
Original DavidSeville
WhowasthemanbehindTheChipmunks?
Ours DavidSeville
Original Evita
WhichLloydWebbermusicalpremieredintheUSon10thDecember1993?
Ours Evita
Original DavidLloydGeorge
WhowasthenextBritishPrimeMinisterafterArthurBalfour?
Ours DavidLloydGeorge
Original TheBeeGees
Whohada70sNo1hitwithKissYouAllOver?
Ours TheBeeGees
19

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
