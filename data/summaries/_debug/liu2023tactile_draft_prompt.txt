=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Tactile Active Inference Reinforcement Learning for Efficient Robotic Manipulation Skill Acquisition
Citation Key: liu2023tactile
Authors: Zihao Liu, Xing Liu, Yizhai Zhang

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2023

Key Terms: manipulation, tactile, efficient, robotic, reinforcement, tasks, acquisition, information, learning, inference

=== FULL PAPER TEXT ===

Tactile Active Inference Reinforcement Learning for Efficient Robotic
Manipulation Skill Acquisition
Zihao Liu†, Xing Liu†, Yizhai Zhang, Zhengxiong Liu and Panfeng Huang∗
Abstract—Robotic manipulation holds the potential to re- leverage tactile sensing to achieve a more comprehensive
place humans in the execution of tedious or dangerous tasks. perception of the scene, and use reinforcement learning to
However, control-based approaches are not suitable due to the
obtainpoliciesorplansforeachstate.Asmanipulationtasks
difficulty of formally describing open-world manipulation in
often involve contact forces, tactile information provides a
reality,andtheinefficiencyofexistinglearningmethods.Thus,
applying manipulation in a wide range of scenarios presents more detailed description of the scene compared to pose
significantchallenges.Inthisstudy,weproposeanovelmethod information. Moreover, RL methods can adapt to a wide
for skill learning in robotic manipulation called Tactile Active rangeofstateconfigurationsandlearnspontaneouslythrough
Inference Reinforcement Learning (Tactile-AIRL), aimed at
exploration.However,traditionaltactilesensingsuffersfrom
achieving efficient training. To enhance the performance of
information sparsity [2], and the RL paradigm also suffers
reinforcement learning (RL), we introduce active inference,
whichintegratesmodel-basedtechniquesandintrinsiccuriosity from low data efficiency.
into the RL process. This integration improves the algorithm’s
training efficiency and adaptability to sparse rewards. Ad-
ditionally, we utilize a vision-based tactile sensor to provide
detailedperceptionformanipulationtasks.Finally,weemploya
model-basedapproachtoimagineandplanappropriateactions
through free energy minimization. Simulation results demon-
strate that our method achieves significantly high training
efficiency in non-prehensile objects pushing tasks. It enables
agents to excel in both dense and sparse reward tasks with
just a few interaction episodes, surpassing the SAC baseline.
Furthermore, we conduct physical experiments on a gripper
screwing task using our method, which showcases the algo-
rithm’srapidlearningcapabilityanditspotentialforpractical
applications.
Fig. 1. Object pushing on a slope: The UR5 manipulator is tasked with
pushingaballorboxfromthelowerendoftheslopetotheupperredgoal
I. INTRODUCTION region.
The manipulation of robots has broad application
prospects, and it can replace humans in performing di- Different from the commonly-used force/torque sensors
verse tasks. Currently, robotic manipulation mostly relies on or strain-based tactile sensors, vision-based tactile sensors
programmed instructions for control. Developers use their such as GelSight [3] offer a new approach to obtaining
knowledge to design procedures and form task plans, and tactile information by deforming a gel that contacts an
robotsperformskilledbehaviorsbasedonpositionandforce object. In this work, we explore the integration of vision-
information [1]. However, this approach has two drawbacks based tactile sensors in robot manipulation skill learning to
that limit its widespread use in reality. First, real-world enhancelearningefficiency.Ontheotherhand,toaddressthe
scenarios are often unstructured, making it difficult to be data efficiency issue of vanilla RL, model-based RL [4][5]
described completely with standardized geometric shapes has been widely employed. Typically, model-based methods
and physical properties. Second, human knowledge cannot learn a state transition model of the agent’s environment
foreseeallsituationsthatrobotswillencounter,makingcom- and use the model for offline or online planning and ac-
plextaskplanningthatstillhasblindspots.Thesechallenges tion execution. By decoupling the model learning from the
have led to the current application of robotic manipulation policy learning, model-based methods indeed improve the
beingmostlylimitedtowell-definedenvironmentswithfixed utilization efficiency of sampled data. However, the issue of
scenarios. To mitigate these two challenges, practitioners accumulated errors still exists, leading to poor performance
of model-based RL in inappropriate reward context such
This work was supported in part by the National Key R&D Program as sparse reward, indicating the limitations of model-based
of China under Grant 2022ZD0117900, and the National Natural Science RL in exploration. Therefore, to efficiently train robotic RL
FoundationofChinaunderGrant62103334and62273280.
models, it is essential to combine data exploration with
Zihao Liu, Xing Liu, Yizhai Zhang, Zhengxiong Liu, and Panfeng
Huang(Corresponding author) are with the Research Center for Intelli- exploitation. And this approach can enhance the adaptability
gent Robotics, School of Astronautics, Northwestern Polytechnical Uni- of models to sparse rewards, thereby reducing the difficulty
versity, and National Key Laboratory of Aerospace Flight Dynamics,
of designing reward functions. Finally, this could increase
Northwestern Polytechnical University, Xi’an, China, 710072 e-mail:
pfhuang@nwpu.edu.cn,xingliu@nwpu.edu.cn. thefeasibilityoflearningroboticmanipulationskillsinreal-
3202
voN
91
]OR.sc[
1v78211.1132:viXra
world scenarios. neural networks can handle more complex contact [12] and
performsurfacereconstructionofobjects[11][13].However,
inthiswork,weonlyconsidersomesimplecontactscenarios,
so we leave it to future research to explore the use of neural
networks for tactile processing.
B. Robotic Reinforcement Learning and Active Inference
Manyworksfocusonincorporatingreinforcementlearning
into robotics to create intelligent robots with self-iterative
capabilities.Previously,thebehaviorofrobotsoftenrequired
manual definition by the designer, but RL enables robots to
have the ability to explore and learn on their own. However,
RL is often data inefficient, so to run RL driven robots
Fig.2. Robotscrewing:TheKUKAmanipulatorequippedwiththegripper with limited real-world data, sim2real or improving the
will adaptively hold the nylon nut and utilize tactile sensor feedback to performance of RL itself can be used. Regarding sim2real,
tightenit.
quadruped robot could train extensively on motion behavior
in simulation and then transfer to the real world [14][15].
To summarize our contributions in this work:
Meanwhile, half sim2real reduces the amount of training
• Weincorporatevision-basedtactileinformationintothe required on the real world by treating simulation as a
state space of reinforcement learning, enhancing the pre-training parameter process. In terms of improving the
perception of the manipulation task. performance of RL itself, VPG [16] achieves unstructured
• We introduce active inference reinforcement learning object placement through the selection of several predefined
for acquiring robot manipulation skills, aiming to min- skillsexecutedbytheroboticarmonimagepixels,wherethe
imize free energy for increasing environmental aware- predefined skills reduce the algorithm’s exploration space,
ness and achieving desired observations. and accelerate training speed. RHER [17] improves data
• We validate our method for learning robotic manipu- utilization through hierarchical thinking and HER methods,
lation skills through simulation and real-world experi- whichcanbeusedforsparserewards.Theadvantageofusing
ments. In the simulation, we employ a manipulator to sparse rewards in robot reinforcement learning is that the
learnobjectpushingacrossaslopeusingtactilesensors. reward function defined based on behavior can be complex
In the real-world experiments, we utilize a gripper to anddifficulttoadjust,whilethesuccessorfailureofthetask
learn screw nut twisting. Our results strike a balance is easy to evaluate.
between exploration and exploitation. Active inference [18] arises from the free energy
principle[19]. Originally it used to describe behavioral mo-
II. RELATEDWORK
tivation in biological organisms, the free energy principle
A. Vision-based Tactile Sensor
states that organisms tend to spontaneously decrease their
Tactile sensors are used to digitize contact signals in the free energy while operating, which can be decomposed
physical world. vanilla tactile sensors, also known as haptic into accurate understanding of the world model and action
sensors [6], have been mostly based on strain principle, but towardsdesiredstates.Tosimulatethisprocessofdecreasing
their limited size makes it difficult to obtain high-resolution free energy, active inference can be used for variational
information.Inrecentyears,vision-basedtactilesensorssuch optimization. Active inference can be naturally extended to
as the gelsight digit tactile sensor [3] have emerged. These intelligent agents such as robots that have actuators. For
sensors visualize the deformation of the contact surface by example, active inference adaptive control [20][21] does not
using a camera to convert touch into vision. Due to their require knowledge of the robot’s geometric structure and
high resolution and rich information, this type of sensor can achieve smooth multi-joint control during the dynamic
has unique advantages and has been successfully applied to reduction of free energy. However, it have not attempted to
various robot operation tasks, such as contour following [7], preciselyconstructanexternalworldmodeloftheintelligent
cutting [8], dish loading [9], and robotic manipulation [10]. agent, but rather use variational free energy minimization
Based on vision-based tactile sensors, accurate depth im- to approach the desired state. An approach related to our
ages of the contact surface can be recovered using poisson workistheActiveInferenceReinforcementLearning(AIRL)
reconstruction [11]. High-resolution local contact measure- framework [22], which interprets active inference in the
ments can be used to represent contact features such as context of RL, where free energy includes both modeling
the pose and force of grasping cables [10]. Here, we use accuracy and reward.
it to estimate the contact state of the manipulated object, Improving the efficiency of robot skill learning is a major
including the position and intensity of the contact. Fig. 1 challenge in the widespread use of robots. We believe that a
shows the contact state during operation. In addition, using viablesolutionistouseactiveinference,whichcanmakefull
neural networks to process the images from tactile sensors use of the data generated by the robot’s interactions in the
has become a popular approach. Feature extraction using physicalworldandgenerateasmuchusefuldataaspossible.
In this work, we employ active inference reinforcement Noting that the FEEF contains the policy π, we will
learningtoimprovethealgorithm’sexploratoryperformance minimizetheFEEFbyadjustingthepolicyq(π).Aftersome
and data utilization performance. derivation, we can get:
III. METHOD (cid:16) (cid:17)
F(cid:101)=0⇒D
KL
q(π)||e−F(cid:101)π =0 (2)
Inthissection,weintroduceourTactile-AIRL,anefficient
learning algorithm that incorporates tactile information dur-
where
ing manipulation tasks and trains agents based on the active
(cid:16) (cid:17)
inference principle. When it comes to robot RL, improving F(cid:101)π =D
KL
q(r
0:T
,o
0:T
,θ|π)||pΦ(r
0:T
,o
0:T
,θ) (3)
learningefficiencyandrewardsdesignarecrucial.Themajor
issues lie in the limited efficiency of physical sampling and So,wecanfitthedistributionq(π)toe−F(cid:101)π toobtainapol-
thedifficultiesindefiningrewardsforcomplexmanipulation icy that minimizes free energy. Moreover, q(π) will become
behaviors. Therefore, the proposed pipeline aims to train more stable when F(cid:101)π is minimized to zero. Consequently,
manipulation skills with minimal sampling and to adapt to the problem at hand exhibits similarities with model-based
sparse rewards, thereby increasing the practicality of skill reinforcement learning. By utilizing a planning algorithm to
trainingandreducingthedifficultyofrewardfunctiondesign. minimizethefutureF(cid:101)π ,wecanderiveanear-optimalpolicy.
TheoverallworkflowofTactile-AIRLispresentedinFigure
3 and Figure 4.
A. Active Inference Reinforcement Learning
Fig.4. PlanningMethod.Theplanningmethodinvolvespredictingfuture
observations,rewards,andcuriosityterms.Subsequently,theCross-Entropy
Method(CEM)isutilizedtogeneratenear-optimalactions.Itisworthnoting
Fig. 3. Model training loop. The green lines represent the model
thatthesymbol"hat"denotesanestimate,whereasthesymbolwithoutthe
calculation, while the black lines depict model updates based on the loss
between the data and the evaluated model. Here, opi represents the state "hat"representstheactualdata.
vectorofthemanipulatorandobjects,oti representsthetactilefeatures,ai
denotes the action, ri denotes the reward, and ci represents the curiosity Minimizing F(cid:101)π holds practical significance. To simplify
item at each time step. The ensemble and predictor refer to the neural
the notation, let’s temporarily abbreviate the sequences r
networkmodelstobelearned. 0:T
and o
0:T
as r and s, respectively. We can decompose F(cid:101)π
Theactiveinferenceoftheagentaimstominimizeitsown into two terms: the expected information gain term and the
free energy during action, leading to a more precise under- extrinsic term, as follows:
standingoftheagent’senvironment.Additionally,theagent’s
observations will align better with its prior knowledge. To (cid:104) (cid:105)
accomplish this, we employ the decision-making scheme
−F(cid:101)π ≈E
q(o,θ|r,π)q(r|π)
lnp(o,θ|r,π)−lnq(o,θ|π)
known as Free Energy of the Expected Future (FEEF) [22]. (cid:104) (cid:105)
−E lnq(r|o,θ,π)−lnpΦ(r)
By considering the minimization of current and future free q(r|o,θ,π)q(o,θ|π)
(cid:104) (cid:16) (cid:17)(cid:105)
energy, this approach transforms the task into a planning =E D q(o,θ|r,π)||q(o,θ|π)
q(r|π) KL (4)
problem, thereby accelerating learning and enhancing the
(cid:124) (cid:123)(cid:122) (cid:125)
stability of the current step’s execution. expectedinformationgainterm,c
(cid:104) (cid:16) (cid:17)(cid:105)
Let the variable x t:T denote a sequence of variables over −E D q(r|o,θ,π)||pΦ(r)
q(o,θ|π) KL
time, x = x ,··· ,x . Here, θ represents the parameters
t:T t T (cid:124) (cid:123)(cid:122) (cid:125)
of the neural networks, and π represents the policy. Next, extrinsicterm,r
we define the concatenation of o and o as o , and
consider the reward as a partial ob
p
s
i
ervation.
tI
Further
i
more,
Thus, the minimization of F(cid:101)π is equivalent to the max-
imization of expected information gain, indicating a pref-
q(r ,o ,θ,π)representsarobot’sbeliefsregardingfuture
t:T t:T
variables, while pΦ(r ,o ,θ) represents a biased genera- erence for observations that provide new information. This
t:T t:T
intrinsic curiosity bears resemblance to mutual information
tivemodelfortherobot.TheFEEF(FreeEnergyofExpected
Future) that needs to be minimized is defined as follows:
[23]. Simultaneously, minimizing F(cid:101)π also reduces the ex-
trinsic term, aiming to bring future observations closer to
(cid:16) (cid:17) priorobservations.InthecontextofAIRL,thispreferencefor
F(cid:101)=D KL q(r 0:T ,o 0:T ,θ,π)||pΦ(r 0:T ,o 0:T ,θ) (1) priors is achieved through the expected value of the reward
function.Consequently,theprocessofminimizingF(cid:101)π serves manipulation tasks compared to relying solely on manipula-
a dual role in both exploration and exploitation. torposeorcameraimages.Specifically,weincorporatepose
Moreprecisely,theexecutionofAIRLcanbedividedinto information and numerical features extracted from tactile
three steps: evaluating the future states, evaluating F(cid:101)π , and images into the state space, with different tasks requiring
learning the policy. distincttactilefeatures.Thisapproachcanalsoberegardedas
1) Evaluating the Future States: The reason for evaluat- multimodal as it combines both geometry and tactile senses
ingfuturestatesisthatcalculatingF(cid:101)π requiresadistribution as sources of information.
offuturestatesoveraperiodoftime,whichcanbepredicted Commonly used tactile features encompass the centroid
from the current time using the trained state transition and summation, which are computed from depth images
ensemble model: capturingcontactinformation.Depthimagescanbeobtained
by applying the Poisson integral to vertical surface gradi-
ents [24]. Additionally, the displacement field of the tactile
q(o ,r ,θ |π)=p(θ)←(cid:45)
t:T t:T surface is utilized to quantify object shear, which can be
(cid:89) T evaluated through optical flow analysis of RGB images. The
(cid:44)→ q(r |o ,θ,π)q(o |o ,θ,π)
τ τ τ τ−1 (5) calculation details of these features are described below.
τ=t For a simple contact, such as an object with a regular-
q(r |o ,θ,π)=E [p(r |o )]
τ τ q(oτ|θ,π) τ τ shaped surface in contact with the tactile sensor, depth
q(o τ |o τ−1 ,θ,π)=E q(oτ−1|θ,π) [p(o τ |o τ−1 ,θ,π)] images usually consist of single connected areas. Therefore,
the calculation can be performed on the complete depth
2) Evaluating F(cid:101)π : Estimating F(cid:101)π involves evaluating
images.Inthisstudy,wefocusonthissimplecase.However,
the state trajectory generated by a specific action sequence
for complex contacts, it is necessary to first identify the
during action sampling. Subsequently, the trajectory with a
effective depth area, which can also be considered as the
lowerfreeenergywillbeselected.Aftersomederivation,F(cid:101)π
region of interest. Suppose the depth image’s (i,j)-th raw
at eachtime step iscalculated as follows. Thesummation of
moments of the area to be calculated are denoted as m .
ij
these values in future steps can be regarded as the value of
The centroid and summation can be obtained as follows:
future actions.
µ=(m /m ,m /m ) (7)
(cid:104) (cid:16) (cid:17)(cid:105) 10 00 01 00
−F(cid:101)πτ ≈−E
q(oτ,θ|π)
D
KL
q(r
τ
|o
τ
,θ,π)||pΦ(r
τ
)
Σ=m
00
(8)
(cid:104) (cid:105) (cid:104) (cid:105)
+H E q(θ) [q(o τ |o τ−1 ,θ,π)] −E q(θ) H[q(o τ |o τ−1 ,π,θ)] where µ is centroid of tactile depth images, and Σ is the
(cid:124) (cid:123)(cid:122) (cid:125) sum of tactile depth images.
stateinformationgain,c
The two features mentioned above describe the vertical
(6)
state of contact, while the horizontal state (shear) can be
3) Learning Policy: Because we can only obtain a score
characterized by gel slip. There are two approaches to
(free energy) corresponding to a certain sampled action
estimating gel slip: gel marker point matching and optical
sequence, optimization-based planning methods are not ap-
flow method. In this study, we employ the Lucas-Kanade
propriate. In this work, we choose the sampling-based CEM
method to calculate the optical flow field, as shown in Fig.
planningalgorithm.Theiterativetargetvalueis−F(cid:101)π ,which
5. Subsequently, we determine its distribution probability
isrepresentednumericallyase−F(cid:101)π.Ateachactionexecution,
based on the calculated values and assess its distribution
a Gaussian distribution of action sequences is initialized as
entropy in two directions, which indicates the magnitude of
the initial value for planning actions. Then, the Gaussian
shear[25].Ahigherentropyofopticalflowcorrespondstoa
distribution parameters are updated by sampling actions
greater shear amplitude caused by objects. This process can
and evaluating F(cid:101)π . Once the parameters of the Gaussian
be formalized as follows:
distribution stabilize, the first action is taken as the executed
action.
f ={(x ,y ),(x ,y ),···(x ,y )}
1 1 2 2 n n
In the above process, establishing a state transition model (cid:90)
is similar to model-based reinforcement learning, which H(x)=− p(x)logp(x)dx
(9)
plays a major role in improving data efficiency. The term x
(cid:90)
"information gain" in free energy, similar to intrinsic curios- H(y)=− p(y)logp(y)dy
ityinreinforcementlearning,playsamajorroleinimproving y
exploratory performance. Thus, the tactile features o contains: µ, Σ, H(x) and
t
H(y).
B. Tactile Information
IV. EXPERIMENTS
Vision-basedtactilesensorsprovideadetaileddepictionof
A. Task
contactinmanipulationtasks.Inthisstudy,weutilizetactile
information to complement the state space of reinforcement To validate our algorithm and compare it with other
learning,therebyachievingsuperiorperformanceincomplex methods,wefocusonrobotmanipulationtasksinthiswork.
may result in a mismatch between the tactile feedback and
the screw nut. This discrepancy will be reflected in the
tactile feedback, thus prompting reinforcement learning for
a more effective twisting strategy, even in the presence of
motion deviations. Our physical robot experiment scene was
implemented on the ROS2 Humble system, utilizing the
KUKA iiwa7 manipulator, BackYard gripper, and GelSight
mini tactile sensor.
C. Environment Implementation Details
Fig.5. Opticalflowwhentheobjecthasatendencytomovetotheright.
Theyellowarrowindicatesthevectorofopticalflowforthesampledpoints. Akeyelementofreinforcementlearningistheinteraction
Theentropyofopticalflowdistributionsisconsideredasanestimationof ofdataflow.Inthissection,weprovideadetaileddescription
shear.
of our dataflow configuration, which is formalized using the
Gym wrapper.
Since human knowledge cannot fully encompass dynamic The state vector observation, denoted as o p , in the simu-
andunstructuredenvironments,decisionplanningusingrule- lation consists of two parts: the position, velocity, posture,
basedcontrolinrobotmanipulationtaskspresentssignificant and angular velocity of the manipulator and the object to
challenges. Therefore, robot manipulation tasks provide an be manipulated. Moreover, since the object posture remains
ideal scenario for the application of artificial intelligence symmetricalduringballpushing,thereisnoneedtoconsider
methods. Specifically, we have established robot manipula- it in this scenario. So the action a involves three degrees of
tion tasks in both simulation and physical environments. In freedom for incremental motion: moving forward, left/right,
the simulation task, we utilize tactile information to push a and rotation. And for physical robots, we have limited the
ball or a box on a slope. This task allows us to evaluate the movements to only two: descent and rotation around the
learning efficiency, adaptability to different geometries, and descentdirection.Thisreductionaimstominimizethespace
exploration capabilities of our algorithm in scenarios with requiredforexplorationandlearning.Therefore,inthiscase,
sparse rewards. Additionally, we have included a physical o p includes only the descent height and rotation angle. Here
taskinvolvingscrewingbytactilefeedback,whichisacom- we enable the end effector to move downward at a fixed
mon scenario in industries. Further details will be presented speed,whiletheactionaisrotationincrementalmotion.The
in subsequent sections. tactile features, denoted as o t , in the simulation include the
µ and Σ of contact depth images, which provide a compre-
B. Simulator and Physical Setting
hensive description of the pushing object. Additionally, in
Considering the potential risks associated with employ- the physical robot, the H(x) and H(y), is included in o to
t
ing real robots for exploration in physical experiments, capture the movement trend of the sliding nut.
we conducted benchmarking of various approaches using Reward is usually the key determinant of the success of a
robot simulation. To construct the task scenarios for robot reinforcement learning algorithm. In the simulation task, we
manipulation,weutilizedPyBulletandavision-basedtactile have devised two reward configurations. The sparse reward
sensor plugin known as TACTO [26]. PyBullet serves as configuration assigns a reward of 1.0 when the ball or box
a Python interface for the Bullet Physics SDK, providing reachesthegoalregion,whilethedenserewardconfiguration
convenient access to Bullet functionalities for physical sim- incorporatesthenegativedistancebetweentheobjectandthe
ulation. TACTO, which is based on the PyBullet platform, center of the goal region additionally. For physical robots,
employssynchronizedscenerenderingofOpenGLandcom- our objective is to achieve an appropriate angular speed
putessensoroutputsbyparsingthecontactobjectdescription for the rotation of the screw nut and minimize the shear
files. It offers interfaces for RGB images and depth images force exerted on the screw nut in the vertical direction.
withfastprocessingspeed.Inoursimulation,weconstructed Unfortunately, obtaining the ground truth angle of the nut
a scene in PyBullet where a UR robot pushes a ball or box is challenging. Therefore, we employ the entropy of optical
using the TACTO tactile sensor, as depicted in Figure 1. flow as a suitable substitute. The reward is formulated as
For the physical robot experiments, a robot screwing, as the negative entropy in the downward direction, aiming to
illustrated in Fig. 2, was constructed. This scenario is com- minimize deformation in this particular direction.
monlyencounteredinourdailylives.Themainchallengeof
D. Result and Discussion
this task lies in the unknown pitch parameter of the screw,
which the robot needs to explore using its touch sense. In the simulation, we compared our method with the
Another issue arises from the limited accuracy of the end vanilla RL method, Soft Actor-Critic (SAC). We conducted
effector control, where each rotation of the end effector, the simulations with dense and sparse rewards, and our method
height only decreases by the amount equivalent to one pitch outperformed SAC in terms of average reward and training
(approximately 1.5mm 2mm). This gives rise to a position episodes. However, when it came to physical robots, we
repeatability error that cannot be disregarded in the context only implemented our method due to limitations posed by
of robotics. Consequently, employing improper strategies sampling costs. To mitigate data fluctuations, we computed
rewards based on a sliding window of 10 episodes for
simulationsand5episodesforexperiments.Additionally,we
ran our method three times in the simulation using different
random seeds to plot the mean and variance.
1) Simulation with Dense Reward: In the dense reward
configuration, our method and most RL methods can learn
the skill of pushing the ball. As shown in the comparison
results in Figure 6, we found that Tactile-AIRL was an
order of magnitude more effective than the RL baseline.
Tactile-AIRL achieved almost maximum episode rewards
by interacting with the environment for only 100 episodes,
while SAC took about 1000 episodes. The reason behind
this improvement is that we introduce tactile information
and employ a modeling approach similar to model-based
reinforcement learning to extract information from existing
data, thereby significantly enhancing data efficiency. Fig.7. ExperimentsinSparseRewardConfiguration.Inthiscase,ithas
beenobservedthatSACfailstolearnskills.
the utilization of active inference reinforcement learning
expedites skill acquisition, highlighting the potential of our
algorithm for broader application scenarios.
Fig. 6. Experiments in dense reward configuration. Compare the use of
ourmethodandSACforpushingballandpushingbox. Fig. 8. Experiments in physical robot. Achieve the skill of minimizing
downwardshearforceinrelativelyfewepisodes.
2) Simulation with Sparse Reward: In the sparse reward
configuration, most RL methods cannot learn the skill of
V. CONCLUSIONS
pushing. As depicted in Figure 7, we observed that the RL
baseline failed to learn even after exploring thousands of Weproposeanefficientlearningmethodforrobotmanipu-
episodes, whereas our method consistently achieved high lationskills,calledTactile-AIRL,whichusesactiveinference
performance. This capability arises from the algorithm’s RL to train robot agents in environments equipped with
objective of minimizing free energy, which drives it to tactile sensing. The method encourages agents to explore
acquirenewinformationduringexplorationuntilitdiscovers effectively and utilizes the tactile data from exploration
stateswitheffectiverewards.Thus,ourmethoddemonstrates interactions for model learning, synchronously improving
advantages in learning robotic manipulation skills by allevi- both exploration and exploitation. Experiments shows that
ating the burden of reward design. Tactile-AIRL has unique advantages in the field of robotics.
3) PhysicalExperiment: Inordertoacceleratelearningin Onelimitationofthisworkisthatthehand-craftedfeatures
the real world and avoid the need for precise measurement of the tactile sensor prevent our method from achieving
of screw ground truth, we currently refrain from using highergeneralization.Therefore,wealsoplantoincorporate
sparse rewards. Instead, we employ dense rewards based multimodalreinforcementlearninginfutureworktoimprove
on shear estimation. As depicted in Figure 8, our approach thegeneralizabilityofourmethodtomanipulationtasks.This
demonstratesconvergencewithinacceptabletrainingperiods, can be achieved by directly taking tactile images or scene
providing strong evidence for the data efficiency of our images as input and fusing them with a state vector. We
algorithm in real-world applications. The incorporation of believe that this approach will further advance the use of
tactile sensation enables us to estimate the effectiveness of vision-based tactile sensors in manipulation learning, which
taskexecutionandenhancestheperceptionspace.Moreover, will be explored in future work.
REFERENCES [21] G. Oliver, P. Lanillos, and G. Cheng, “An empirical study of active
inferenceonahumanoidrobot,”IEEETransactionsonCognitiveand
[1] A.Delgado,C.Jara,andF.Torres,“In-handrecognitionandmanipu- DevelopmentalSystems,vol.14,no.2,pp.462–471,2022.
lationofelasticobjectsusingaservo-tactilecontrolstrategy,”Robotics
[22] A. Tschantz, B. Millidge, A. K. Seth, and C. L. Buckley,
andComputer-IntegratedManufacturing,vol.48,pp.102–112,2017.
“Reinforcement learning through active inference,” arXiv preprint
[2] N.Jamali,C.Ciliberto,L.Rosasco,andL.Natale,“Activeperception: arXiv:2002.12636,2020.
Buildingobjects’modelsusingtactileexploration,”in2016IEEE-RAS
[23] T. Schneider, B. Belousov, G. Chalvatzaki, D. Romeres, D. K. Jha,
16th International Conference on Humanoid Robots (Humanoids),
and J. Peters, “Active exploration for robotic manipulation,” in 2022
2016,pp.179–185. IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems
[3] R. Li, R. Platt, W. Yuan, A. ten Pas, N. Roscup, M. A. Srinivasan, (IROS),2022,pp.9355–9362.
andE.Adelson,“Localizationandmanipulationofsmallpartsusing
[24] S. Wang, Y. She, B. Romero, and E. Adelson, “Gelsight wedge:
gelsighttactilesensing,”in2014IEEE/RSJInternationalConference
Measuringhigh-resolution3dcontactgeometrywithacompactrobot
onIntelligentRobotsandSystems,2014,pp.3988–3993.
finger,” in 2021 IEEE International Conference on Robotics and
[4] J.Schrittwieser,I.Antonoglou,T.Hubert,K.Simonyan,andL.Sifre, Automation(ICRA),2021,pp.6468–6475.
“Mastering atari, go, chess and shogi by planning with a learned
[25] W.Yuan,R.Li,M.A.Srinivasan,andE.H.Adelson,“Measurement
model,”Nature,vol.588,no.7839,pp.604–609,Dec2020.
of shear and slip with a gelsight tactile sensor,” in 2015 IEEE
[5] D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi, “Dream to con- InternationalConferenceonRoboticsandAutomation(ICRA),2015,
trol: Learning behaviors by latent imagination,” arXiv preprint
pp.304–311.
arXiv:1912.01603,2019.
[26] S. Wang, M. Lambeta, P.-W. Chou, and R. Calandra, “TACTO: A
[6] P. Payeur, C. Pasca, A.-M. Cretu, and E. Petriu, “Intelligent haptic
fast, flexible, and open-source simulator for high-resolution vision-
sensor system for robotic manipulation,” IEEE Transactions on In- basedtactilesensors,”IEEERoboticsandAutomationLetters(RA-L),
strumentationandMeasurement,vol.54,no.4,pp.1583–1592,2005.
vol.7,no.2,pp.3930–3937,2022.
[7] N.F.Lepora,A.Church,C.deKerckhove,R.Hadsell,andJ.Lloyd,
“Frompixelstopercepts:Highlyrobustedgeperceptionandcontour
following using deep learning and an optical biomimetic tactile
sensor,” IEEE Robotics and Automation Letters, vol. 4, no. 2, pp.
2101–2107,2019.
[8] A. Yamaguchi and C. G. Atkeson, “Combining finger vision and
optical tactile sensing: Reducing and handling errors while cutting
vegetables,” in 2016 IEEE-RAS 16th International Conference on
HumanoidRobots(Humanoids),2016,pp.1045–1051.
[9] N.Kuppuswamy,A.Alspach,A.Uttamchandani,S.Creasey,T.Ikeda,
and R. Tedrake, “Soft-bubble grippers for robust and perceptive ma-
nipulation,”in2020IEEE/RSJInternationalConferenceonIntelligent
RobotsandSystems(IROS),2020,pp.9917–9924.
[10] Y. She, S. Wang, S. Dong, N. Sunil, A. Rodriguez, and E. Adelson,
“Cablemanipulationwithatactile-reactivegripper,”TheInternational
Journal of Robotics Research, vol. 40, no. 12-14, pp. 1385–1401,
2021.
[11] S.Wang,J.Wu,X.Sun,W.Yuan,W.T.Freeman,J.B.Tenenbaum,
and E. H. Adelson, “3d shape perception from monocular vision,
touch,andshapepriors,”in2018IEEE/RSJInternationalConference
onIntelligentRobotsandSystems(IROS),2018,pp.1606–1613.
[12] C. Wang, S. Wang, B. Romero, F. Veiga, and E. Adelson, “Swing-
bot: Learning physical features from in-hand tactile exploration for
dynamic swing-up manipulation,” in 2020 IEEE/RSJ International
ConferenceonIntelligentRobotsandSystems(IROS),2020,pp.5633–
5640.
[13] E.Smith,R.Calandra,A.Romero,G.Gkioxari,D.Meger,J.Malik,
andM.Drozdzal,“3dshapereconstructionfromvisionandtouch,”in
Advances in Neural Information Processing Systems, H. Larochelle,
M.Ranzato,R.Hadsell,M.Balcan,andH.Lin,Eds.,vol.33. Curran
Associates,Inc.,2020,pp.14193–14206.
[14] T.Miki,J.Lee,J.Hwangbo,L.Wellhausen,V.Koltun,andM.Hutter,
“Learningrobustperceptivelocomotionforquadrupedalrobotsinthe
wild,”ScienceRobotics,vol.7,no.62,p.eabk2822,2022.
[15] J. Wu, G. Xin, C. Qi, and Y. Xue, “Learning robust and agile
legged locomotion using adversarial motion priors,” IEEE Robotics
andAutomationLetters,vol.8,no.8,pp.4975–4982,2023.
[16] A.Zeng,S.Song,S.Welker,J.Lee,A.Rodriguez,andT.Funkhouser,
“Learning synergies between pushing and grasping with self-
supervised deep reinforcement learning,” in 2018 IEEE/RSJ Interna-
tionalConferenceonIntelligentRobotsandSystems(IROS),2018,pp.
4238–4245.
[17] Y. Luo, Y. Wang, K. Dong, Q. Zhang, E. Cheng, Z. Sun, and
B. Song, “Relay hindsight experience replay: Self-guided continual
reinforcement learning for sequential object manipulation tasks with
sparserewards,”Neurocomputing,vol.557,p.126620,2023.
[18] F. R. Karl Friston, Thomas FitzGerald, P. Schwartenbeck,
J. O’Doherty, and G. Pezzulo, “Active inference and learning,”
Neuroscience&BiobehavioralReviews,vol.68,pp.862–879,2016.
[19] K.Friston,“Thefree-energyprinciple:aunifiedbraintheory?”Nature
ReviewsNeuroscience,vol.11,no.2,pp.127–138,Feb2010.
[20] C.Pezzato,R.Ferrari,andC.H.Corbato,“Anoveladaptivecontroller
forrobotmanipulatorsbasedonactiveinference,”IEEERoboticsand
AutomationLetters,vol.5,no.2,pp.2973–2980,2020.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Tactile Active Inference Reinforcement Learning for Efficient Robotic Manipulation Skill Acquisition"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
