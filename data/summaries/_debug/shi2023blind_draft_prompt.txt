=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Blind CT Image Quality Assessment Using DDPM-derived Content and Transformer-based Evaluator
Citation Key: shi2023blind
Authors: Yongyi Shi, Wenjun Xia, Ge Wang

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2023

Key Terms: ddpm, quality, image, opinions, blind, content, transformer, biqa, metrics, assessment

=== FULL PAPER TEXT ===

1
Blind CT Image Quality Assessment Using
DDPM-derived Content and Transformer-based
Evaluator
Yongyi Shi, Wenjun Xia, Ge Wang, Xuanqin Mou
Abstract â€” Lowering radiation dose per view and images is assessed, radiologistâ€™s opinions serve as the gold
utilizing sparse views per scan are two common CT scan standard. Nonetheless, collecting these opinions is an expensive
modes, albeit often leading to distorted images and intricate process, making it impractical for real-time and
characterized by noise and streak artifacts. Blind image
large-scale IQA tasks. Consequently, peak signal-to-noise
quality assessment (BIQA) strives to evaluate perceptual
(PSNR) and structural similarity index measure (SSIM) have
quality in alignment with what radiologists perceive, which
plays an important role in advancing low-dose CT been widely used as surrogate metrics [11]. However, both
reconstruction techniques. An intriguing direction involves PSNR and SSIM have shown limited correlation with
developing BIQA methods that mimic the operational radiologistsâ€™ opinions on image quality, primarily due to their
characteristic of the human visual system (HVS). The reliance on mathematical models that do not account for the
internal generative mechanism (IGM) theory reveals that the
intricacies of human perception. Moreover, the requirement for
HVS actively deduces primary content to enhance
reference images to calculate these metrics poses challenges in
comprehension. In this study, we introduce an innovative
BIQA metric that emulates the active inference process of clinical environments; for example, obtaining high-quality
IGM. Initially, an active inference module, implemented as images is infeasible without increasing patient radiation
a denoising diffusion probabilistic model (DDPM), is exposure. To overcome these limitations, one direction is to
constructed to anticipate the primary content. Then, the develop no-reference image quality metrics that correlates well
dissimilarity map is derived by assessing the interrelation
with radiologistsâ€™ opinion on image quality.
between the distorted image and its primary content.
No-reference image quality assessment (NR-IQA), also
Subsequently, the distorted image and dissimilarity map
are combined into a multi-channel image, which is inputted known as blind IQA (BIQA), is widely used to evaluate the
into a transformer-based image quality evaluator. quality of natural images [12-13]. Traditional BIQA methods
Remarkably, by exclusively utilizing this transformer-based typically comprise three steps. First, some handcrafted
quality evaluator, we won the second place in the MICCAI descriptors are employed to extract quality-aware features of
2023 low-dose computed tomography perceptual image
training images. Then, the statistical distribution of the
quality assessment grand challenge. Leveraging the
extracted features serves as the guidance. Some approaches also
DDPM-derived primary content, our approach further
improves the performance on the challenge dataset. involve parameterizing this distribution through modeling.
Finally, a mapping function, such as support vector regression
Index Termsâ€”Blind image quality assessment, (SVR) [14], is designed to convert the distributions into a
denoising diffusion probabilistic model (DDPM), primary quality score. In traditional BIQA, features are derived from
content, transformer-based image quality evaluator.
various sources including discrete wavelet transform (DWT)
[15-17], discrete cosine transform (DCT) [18] and spatial
I. INTRODUCTION
domain measures [19-20] to predict the perceptual quality.
X- RAY image quality assessment (IQA) plays a crucial role Alternatively, it is assumed that the human visual system (HVS)
in computed tomography (CT) imaging, facilitating the gauges image quality by discerning features like gradient [21],
advancement of novel algorithms for low-dose CT luminance contrast [22] or local binary pattern [23]. Certain
reconstruction. Two strategies for low-dose CT scans are either approaches combine these extracted features to improve image
reducing the X-ray tube current or acquiring sparse views. quality assessment further [24-25]. Nevertheless, due to the
However, these strategies often introduce noise and/or streak complexities of image contents and distortion patterns, the
artifacts in the filtered backprojection (FBP) images. To address representation capabilities of handcrafted features are often
these problems, deep learning-based methods have been used unsatisfactory.
for low-dose CT image denoising [1-5] and sparse view CT In recent years, the convolutional neural network (CNN) has
reconstruction [6-10]. When the quality of these reconstructed gained major attention in BIQA tasks because of its potent
This work was supported in part by the National Institute of Biomedical Imaging and Bioengineering/National Institutes of Health under Grant R01
EB016977. The work of Xuanqin Mou was supported in part by the Natural Science Foundation of China, under Grant 62071375. (Corresponding
authors: Ge Wang; Xuanqin Mou).
Yongyi Shi, Wenjun Xia and Ge Wang are with the Department of Biomedical Engineering, Rensselaer Polytechnic Institute, Troy, NY 12180
USA (e-mail: shiy11@rpi.edu; xiaw4@rpi.edu; wangg6@rpi.edu).
Xuanqin Mou are with the Institute of Image Processing and Pattern Recognition, Xiâ€™an Jiaotong University, Xiâ€™an 710049, China (e-mail:
xqmou@mail.xjtu.edu.cn).
2 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. xx, NO. x, 2020
feature representation power [26-27] and started gaining and restoration [47- 49]. These qualities underscore a huge
momentum [28-33]. Rank-IQA utilizes synthetically generated potential of diffusion models in generating high-quality primary
distortions and a Siamese network to rank images based on their content, particularly in the context of low-dose CT imaging [50,
quality [29]. DBCNN demonstrates effectiveness with both 51] which is featured by uniquely intricate noise and streak
synthetic and authentic image distortions [30]. Hyper-IQA artifacts, quite different from what in natural images.
segregates features into low-level and high-level categories, Low-dose CT images frequently suffer from conspicuous
then transforms the latter to reshape the formerâ€™s influence [31]. noise and streak artifacts, being the primary culprits of the
Meta-IQA employes meta-learning to train the networks on image quality deterioration. Specifically, the streak artifacts
distinct types of distortions, thereby acquiring prior knowledge exhibit directional appearance contributing to globalized
[32]. More recently, vision transformers (ViTs) [33] have artifacts in a whole image [6]. This makes the existing datasets
emerged as competitive alternatives to CNNs. Their self- of natural images inadequate for accurately predicting quality
attention mechanism empowers ViT to grasp global contextual scores of low-dose CT images. On the other hand, given the
information from an entire image, ensuring a comprehensive scarcity of open datasets for low-dose CT BIQA, experiments
consideration of all image features of significance for task- have been conducted using disparate datasets, yielding results
specific prediction. This attribute aligns well with the that are challenging to compare and interpret [52, 53]. These
requirements of BIQA task, which involves predicting a quality challenges demand a standardized image quality metric in
index globally [34-40]. MSTIQA leverages a Swin transformer general, and for low-dose CT imaging in particular. Notably, a
to amalgamate features from multiple stages to enhance quality low-dose CT BIQA dataset has recently been unveiled for the
assessment [38]. MANIQA introduces a multi-dimensional MICCAI 2023 low-dose CT perceptual image quality
attention network for BIQA. This approach introduces the assessment grand challenge [54]. As low-dose CT images are
transposed attention block and the multiscale Swin transformer obtained by reducing the number of projections per rotation
block to strengthen global and local interactions [39]. and/or the X-ray tube current, the combination of sparse view
MAMIQA employs a lightweight attention mechanism that streaks and noise needs to be dealt with in the challenge so that
utilizes decomposed large kernel convolutions to extract the best-performing IQA model can be identified and made
multiscale features. Furthermore, it includes a feature applicable in real clinical environments.
enhancement module to enrich local fine-grained details and Given the globalized artifact patterns in low-dose CT images,
global semantic information at multi-scales [40]. Among the we utilized a transformer-based quality evaluator [39] and won
various transformer-based BIQA methods, MANIQA has not the second place in the MICCAI 2023 low-dose CT perceptual
only achieved the state-of-the-art performance in BIQA tasks image quality assessment grand challenge [54]. We found in
but also secured the first place in the no-reference track of our study that directional artifacts may sometimes be
NTIRE 2022 perceptual image quality assessment challenge misleading by resembling genuine anatomical structures. To
[41]. Intriguingly, the top three methods in this challenge all further improve the performance, inspired by IGM here we
rely on transformer-based techniques, highlighting the efficacy propose a novel approach utilizing a DDPM-based active
of the transformers in BIQA tasks. Nonetheless, the lack of inference for low-dose CT BIQA. At the outset, given DDPMâ€™s
reference information poses a challenge, preventing these capability to simultaneously eliminate noise and streak artifacts,
methods from aligning seamlessly with the HVS and may we employ DDPM to generate high-quality primary content,
adversely impacting their overall performance. closely mimicking the HVS. Recognizing the heightened
The theory of internal generative mechanisms (IGMs) sensitivity of HVS to structures, the dissimilarity map is
suggests that the HVS engages in an active process of deducing extracted from the distorted image and its primary content.
the primary content of an image during human evaluations [42- After that, incorporating such diverse prior information as input,
44]. In this process, IGM initially analyzes pixel correlations a multi-channel image is synthesized, amalgamating content,
within an input image. In conjunction with intrinsic prior distortion, and structural characteristics for comprehensive
knowledge, IGM deduces the corresponding primary content as quality prediction. The final step involves employing a
an active comprehension of the input image [45]. The primary transformer-based quality evaluator to predict the score.
content consists of essential scene information, representing the Empirical trials conducted on the low-dose CT BIQA challenge
structured, meaningful elements within the image, which is dataset systematically affirm the efficacy of our proposed
transported to the high level of HVS for interpretation [46]. method.
Thus, the ability to generate high-quality primary content In brief, the main contributions of this paper can be
becomes critical in emulating the HVS. Notably, recent summarized as follows:
successes in adapting and applying the denoising diffusion 1) We propose a conditional DDPM to emulate the active
probabilistic model (DDPM) and other diffusion models have inference process of IGM. The proposed DDPM-based active
showcased their amazing capabilities in both image generation inference module can effectively predict the primary content of
3 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. xx, NO. x, 2020
Figure 1. Workflow of the conditional DDPM that maps low-dose CT images to normal dose counterparts.
distorted images which contain intricate noise and streak
artifacts from both low-dose and sparse view CT imaging.
ğ‘(ğ’™
ğ‘¡
|ğ’™
ğ‘¡âˆ’1
)=ğ’©(ğ’™
ğ‘¡
;âˆš1âˆ’ğ›½
ğ‘¡
ğ’™
ğ‘¡âˆ’1
,ğ›½
ğ‘¡
ğ‘°) (2)
2) Based on the primary content, we introduce a transformer- ğ‘‡
based quality evaluator to predict the image quality on a low-
ğ‘(ğ’™ |ğ’™ )=âˆğ‘(ğ’™ |ğ’™ ) (3)
1:ğ‘‡ 0 ğ‘¡ ğ‘¡âˆ’1
dose CT BIQA dataset. Note that we secured the runner-up
ğ‘¡=1
position in the MICCAI 2023 low-dose CT perceptual image
quality assessment grand challenge by only employing this where ğ’™ 1 ,â‹¯,ğ’™ ğ‘‡ are latent variables of the same dimensionality
transformer-based quality evaluator. On that basis, we have as the sample ğ’™ 0 âˆ¼ğ‘(ğ’™ 0 ).
improved the image quality assessment performance even According to the properties of the Gaussian distribution, the
further, as explained in detail below. sampling result ğ’™ ğ‘¡ at an arbitrary timestep ğ‘¡ can be written in
The remainder of the paper is organized as follows. Section the following closed form:
II outlines the process of utilizing a conditional DDPM to
ğ‘(ğ’™ |ğ’™ )=ğ’©(ğ’™ ;âˆšğ›¼Ì… ğ’™ ,(1âˆ’ğ›¼Ì… )ğ‘°) (4)
acquire both the primary content and the dissimilarity map, ğ‘¡ 0 ğ‘¡ ğ‘¡ 0 ğ‘¡
while also detailing the transformer-based quality evaluator. where ğ›¼ =1âˆ’ğ›½ and ğ›¼Ì… =âˆğ‘¡ ğ›¼ .
ğ‘¡ ğ‘¡ ğ‘¡ ğ‘–=1 ğ‘–
Section III reports our evaluation results on the low-dose CT After the forward process, ğ’™ follows a standard normal
ğ‘‡
BIQA challenge dataset. Section IV discusses relevant issues distribution when ğ‘‡ is large enough. Thus, if we know the
and makes the conclusion. conditional distribution ğ‘(ğ’™ |ğ’™ ), we can use the reverse
ğ‘¡âˆ’1 ğ‘¡
process to get a sample under ğ‘(ğ’™ ) from ğ’™ ~ğ’©(ğŸ,ğ‘°).
0 ğ‘‡
II. METHODOLOGY However, ğ‘(ğ’™
ğ‘¡âˆ’1
|ğ’™
ğ‘¡
) depends on the entire data distribution,
which is hard to calculate. Hence, a neural network was
A. Conditional DDPM for Low-Dose CT
designed to learn a latent data distribution by gradually
To generate high-quality primary content for emulating the denoising a normal distribution variable, which corresponds to
HVS, a typical deep learning method trains a network to learn learning the reverse process of a fixed Markov Chain of length
a mapping from low-dose CT images to normal-dose CT ğ‘‡ conditioned on a low-dose CT image ğ’š. The reverse process
images. Let us assume that ğ’šâˆˆâ„ğ‘ and ğ’™âˆˆâ„ğ‘ are paired low- can be defined as:
dose CT and normal-dose CT images. The parameters of the
ğ‘ (ğ’™ |ğ’™ ,ğ’š)= ğ’©(ğ’™ ;ğ (ğ’™ ,ğ’š,ğ‘¡),ğœ2ğ‘°) (5)
network can be trained as follows: ğœƒ ğ‘¡âˆ’1 ğ‘¡ ğ‘¡âˆ’1 ğœƒ ğ‘¡ ğ‘¡
ğ‘‡
minâ€–ğ· (ğ’š)âˆ’ğ’™â€–2 (1)
ğœƒ ğœƒ 2 ğ‘ (ğ’™ |ğ’š)=ğ‘(ğ’™ )âˆğ‘ (ğ’™ |ğ’™ ,ğ’š) (6)
ğœƒ 0:ğ‘‡ ğ‘‡ ğœƒ ğ‘¡âˆ’1 ğ‘¡
where ğ· is the network parameterized by ğœƒ, which is the key ğ‘¡=1
ğœƒ
for producing high-quality images. where ğ‘(ğ’™ ) is the density function of ğ’™ . In Eq. (6),
ğ‘‡ ğ‘‡
Recently, DDPM has demonstrated superior performance in ğ (ğ’™ ,ğ’š,ğ‘¡) and ğœ2 are needed to solve ğ‘ (ğ’™ |ğ’™ ,ğ’š) .
ğœƒ ğ‘¡ ğ‘¡ ğœƒ ğ‘¡âˆ’1 ğ‘¡
generating high-quality images from their distorted According to the Bayes theorem, the posterior ğ‘(ğ’™ |ğ’™ ,ğ’™ )
ğ‘¡âˆ’1 ğ‘¡ 0
counterparts, particularly in complex scenarios involving are defined as
multiple distortions, such as CT imaging.
The architecture of the conditional DDPM is shown in Fig. 1. ğ‘(ğ’™ ğ‘¡âˆ’1 |ğ’™ ğ‘¡ ,ğ’™ 0 )=ğ’©(ğ’™ ğ‘¡âˆ’1 ;ğÌƒ ğ‘¡ (ğ’™ ğ‘¡ ,ğ’™ 0 ),ğœ ğ‘¡ 2ğ‘°) (7)
DDPM starts with a forward process that gradually adds noise
where
to the normal-dose CT image ğ’™ âˆ¼ğ‘(ğ’™ ) over the course of ğ‘‡
0 0
timesteps according to a variance schedule ğ›½ ,â‹¯,ğ›½ : âˆšğ›¼ (1âˆ’ğ›¼Ì… ) âˆšğ›¼Ì… (1âˆ’ğ›¼ )
1 ğ‘‡ ğÌƒ (ğ’™ ,ğ’™ )= ğ‘¡ ğ‘¡âˆ’1 ğ’™ + ğ‘¡âˆ’1 ğ‘¡ ğ’™ (8)
ğ‘¡ ğ‘¡ 0 1âˆ’ğ›¼Ì… ğ‘¡ 1âˆ’ğ›¼Ì… 0
ğ‘¡ ğ‘¡
4 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. xx, NO. x, 2020
ğœ2 = (1âˆ’ğ›¼Ì… ğ‘¡âˆ’1 )(1âˆ’ğ›¼ ğ‘¡ ) (9) Algorithm 1: Training DDPM
ğ‘¡ 1âˆ’ğ›¼Ì… ğ‘¡ ğœƒ â† Randomly Initialize parameters of DDPM
Since ğœ ğ‘¡ 2 is a constant, the most natural parameterization of ğ‘‡,ğ›½ ğ‘¡ â†Iinitialize time steps and variance schedule
ğ (ğ’™ ,ğ’š,ğ‘¡) is a neural network that predicts ğÌƒ (ğ’™ ,ğ’™ ) directly. repeat
ğœƒ ğ‘¡ ğ‘¡ ğ‘¡ 0
Alternatively, given that ğ’™ =âˆšğ›¼Ì… ğ’™ +âˆš1âˆ’ğ›¼Ì… ğ, ğ~ğ’©(ğŸ,ğ‘°), (ğ’™ ,ğ’š)â†Get minibatch from a training dataset
ğ‘¡ ğ‘¡ 0 ğ‘¡ 0
the posterior expectation in Eq. (8) can be expressed as ğ‘¡ â† Uniform ({1,2,â€¦,ğ‘‡ })
ğâ†ğ’©(ğŸ,ğ‘°)
1
ğÌƒ (ğ’™ ,ğ’™ )=ğÌƒ (ğ’™ , (ğ’™ âˆ’âˆš1âˆ’ğ›¼Ì… ğ)) gâ†âˆ‡â„’ Calculate gradient
ğ‘¡ ğ‘¡ 0 ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡
âˆšğ›¼Ì…
ğ‘¡ ğœƒ â†Update parameters
until Epoch is completed
1 1âˆ’ğ›¼
ğ‘¡
= (ğ’™ ğ‘¡ âˆ’ ğ) (10) return ğœƒ
âˆšğ›¼ âˆš1âˆ’ğ›¼Ì…
ğ‘¡ ğ‘¡
Since ğÌƒ
ğ‘¡
(ğ’™
ğ‘¡
,ğ’™
0
) can be represented by ğ, we can also use a
neural network model ğ· to predict the noise ğ, which has been
ğœƒ Algorithm 2: Inferencing with DDPM
proved to work well by Ho et al. [47]. Hence, the corresponding
ğœƒ â† Load parameters from the trained DDPM
objective can be simplified to
ğ‘‡,ğ›½ â†Iinitialize time steps and the variation schedule
â„’ =ğ”¼ ğ”¼ [
(1âˆ’ğ›¼ğ‘¡)2
â€– ğâˆ’ğ· (âˆšğ›¼Ì… ğ’™ +âˆš1âˆ’ğ›¼Ì… ğ,ğ’š,ğ‘¡)â€–
2
] ğ’™ ~
ğ‘¡
ğ’©(ğŸ,ğ‘°)
ğ’™,ğ’š ğ,ğ‘¡ 2ğœğ‘¡ 2ğ›¼Ì…Ì…Ì…(Ì…1Ì…Ì…âˆ’Ì…Ì…ğ›¼Ì…Ì…Ì…Ì…) ğœƒ ğ‘¡ 0 ğ‘¡
2 fo
ğ‘‡
r ğ‘¡ =1,2,â€¦,ğ‘‡
(11)
ğ’›~ğ’©(ğŸ,ğ‘°) ğ‘–ğ‘“ ğ‘¡ >1 ğ‘’ğ‘™ğ‘ ğ‘’ ğ’›=ğŸ
with ğ‘¡ uniformly sampled as {1,â‹¯,ğ‘‡}.
ğ’™ =
1
(ğ’™ âˆ’
1âˆ’ğ›¼ğ‘¡
ğ· (ğ’™ ,ğ’š,ğ‘¡))+ğœ ğ’›
In this study, the latent space is diffused into Gaussian noise ğ‘¡âˆ’1 âˆšğ›¼ğ‘¡ ğ‘¡ âˆš1âˆ’ğ›¼Ì…ğ‘¡ ğœƒ ğ‘¡ ğ‘¡
using ğ‘‡ =1000 steps. A U-Net [55] model ğ· ğœƒ is trained to end
predict the noise ğœ– in the latent space. To obtain high-quality
return Primary content
primary contents, samples can be computed as follows:
ğ’™ =
1
(ğ’™ âˆ’
1âˆ’ğ›¼ğ‘¡
ğ· (ğ’™ ,ğ’š,ğ‘¡))+ğœ ğ’› (12)
ğ‘¡âˆ’1 âˆšğ›¼ğ‘¡ ğ‘¡ âˆš1âˆ’ğ›¼Ì…ğ‘¡ ğœƒ ğ‘¡ ğ‘¡
where ğ’›~ğ’©(ğŸ,ğ‘°). For clarify, the pseudo codes of Algorithms
1 and 2 are presented for training and inference, respectively.
Through the emulation of IGM for active inference using
conditional DDPM, we successfully obtained the primary
content. It is important to note that conditional DDPM is not
intended to completely restore a distorted image to the reference
image, but the predicted primary content may present the
reference image much better than the distorted input. Therefore,
our goal is to estimate a dissimilarity map from this primary
content, rather than merely quantifying the disparity between
the distorted image and its primary content. The process for
generating this dissimilarity map is depicted in Fig. 2.
After we obtain the primary content with the conditional
DDPM, we calculate the SSIM map between the distorted
image and the primary content, which assigns values between 0
and 1 to each and every pixel, with 0 indicating the lowest
dissimilarity and 1 representing the perfect similarity. As
dissimilarity indicates the presence of distortion, a critical
factor in assessing image quality, we subtract the SSIM map
from an all-one matrix ğ‘° to obtain the dissimilarity weights,
where larger values signify more severe distortions. Finally, we
calculate the Hadamard product between the distorted image
and the dissimilarity weights to obtain the dissimilarity map.
With this dissimilarity map as an input, the subsequent image
quality evaluator can more effectively leverage the
Figure 2. Dissimilarity map computed from the distorted image and the
characteristics of IGM for BIQA. We call our method as corresponding primary content.
distortion-based BIQA or D-BIQA.
5 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. xx, NO. x, 2020
Figure 3. Transformer-based quality evaluator. a: general overview of the architecture; b: multi-scale vision transformer; c: transposed attention
block; d: scale Swin transformer block; e: patch-weighted quality prediction; and f: legend of the submodules.
denotes the batch size, ğ‘ represents channel size, ğ» and ğ‘Š
B. Transform-based Quality Evaluator ğ‘– ğ‘– ğ‘–
denote the dimensions of the i-th feature. Specifically, we
After obtaining the dissimilarity map, we integrate it with the utilize 4 layers to extract features with varying semantic
distorted image, resulting in a multi-channel image (a tensor in significance. Then, we concatenate ğ¹Ì‚ , where ğ‘– âˆˆ{7,8,9,10},
ğ‘–
general). Our primary goal is to create a quality evaluator that yielding a composite feature tensor denoted as ğ¹Ìƒ âˆˆ
can effectively process the multi-channel data tensor consisting
â„ğ‘Ã—âˆ‘ğ‘–ğ‘ğ‘–Ã—ğ»ğ‘–ğ‘Šğ‘–.
of these input images. To comprehensively utilize information
with both the spatial and channel dimensions, we introduce a 2) Transposed Attention Block
transformer-based model tailored for perceptual quality To enhance channel interaction within these concatenated
prediction based on MANIQA [39]. The overall architecture of features, we employ a transposed attention block. Unlike the
this transformer-based quality evaluator is depicted in Fig. 3a. conventional spatial attention, this block facilitates self-
The core components of the transformer-based quality attention across channels, effectively computing cross-
evaluator include: 1) vision transformer; 2) transposed attention covariance between channels to generate an attention map that
block; 3) scale Swin transformer block, and 4) patch-weighted encodes global contextual relations. Beginning with the feature
quality prediction. tensor ğ¹Ìƒ âˆˆâ„ğ‘Ã—âˆ‘ğ‘–ğ‘ğ‘–Ã—ğ»ğ‘–ğ‘Šğ‘–, the transposed attention block first
generates query (Q), key (K) and value (V) projections through
1) Vision Transformer
independent linear projections. These projections are utilized to
Fig. 3b shows the architecture of ViT. In the context of a given
encode pixel-wise cross-channel dependency. The query and
multi-channel image, denoted as ğ‘¬âˆˆâ„ğ»Ã—ğ‘ŠÃ—3, where ğ» and
key projections are reshaped for dot-product interaction,
ğ‘Š represent the height and width respectively, we employ the
producing a transposed-attention map Aâˆˆâ„ğ¶ÌƒÃ—ğ¶Ìƒ. Note that ğ¶Ìƒ is
ViT denoted by ğ‘“ with a learnable vector of parameters ğœ‘.
ğœ‘
equal to ğ‘ . It is noteworthy that the layer normalization and
From the ViT, we extract features ğ¹ âˆˆâ„ğ‘Ã—ğ‘ğ‘–Ã—ğ»ğ‘–ğ‘Šğ‘– ğ‘–
ğ‘– multi-layer perceptron components from the original
corresponding to the i-th layer, where ğ‘– ranges from 1 to 12, ğ‘
6 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. xx, NO. x, 2020
transformer structure are omitted. Mathematically, the 1,222 abdomen normal dose images from the 2016 low-dose
transposed attention block is defined as follows: CT grand challenge dataset and simulated 14,664 images with
different noise levels and numbers of projection views to train
ğ‘¿Ì‚ =ğ‘Šğ´ğ‘¡ğ‘¡ğ‘›(ğ‘¸Ì‚,ğ‘²Ì‚,ğ‘½Ì‚)+ğ‘¿ (13)
ğ‘ our DDPM model.
Attn(ğ‘¸Ì‚,ğ‘²Ì‚,ğ‘½Ì‚)=ğ‘½Ì‚âˆ™ğ‘†ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘²Ì‚ âˆ™ğ‘¸Ì‚/ğ›¼) (14) 2) Low-dose CT BIQA Dataset
The low-dose CT BIQA dataset comprises a total of 1,000
where ğ›¼ denotes the spatial dimension of Q, K and V. The
distorted abdominal images, which exhibit both noise and
specifics of the transposed attention block are illustrated in Fig.
streak artifacts. These images were generated at four different
3c.
dose levels: 100%, 50%, 25%, and 10%. Additionally, three
3) Scale Swin Transformer Block numbers of projection views were uniformly selected over a full
The scale Swin transformer block is shown in Fig. 3d. The scan, resulting in a total of 12 different types of image
scale Swin transformer block consists of Swin transformer degradation. Out of these images, 900 were allocated for the
layers and convolutional layers. Given an input feature ğ¹ , the training phase, while the remaining 100 were reserved for the
ğ‘–,0
scale Swin transformer block first encodes the feature tensor testing phase. In this dataset, CT values exceeding 350
through 2 Swin transformer layers: Hounsfield Units (HU) were capped at 350 HU, resulting in a
CT value range from -1000 HU to 350 HU. These CT values
ğ¹ =ğ» (ğ¹ ),ğ‘— =1,2, (15)
ğ‘–,ğ‘— ğ‘†ğ‘‡ğ¿ğ‘–,ğ‘— ğ‘–,ğ‘— were then transformed to a normalized range from 0 to 1. The
CT image quality assessment was performed using the
where ğ» (âˆ™) represents the j-th Swin transformer layer in
ğ‘†ğ‘‡ğ¿ğ‘–,ğ‘— abdominal soft-tissue window, defined by the width/level
the i-th stage, ğ‘– âˆˆ{1,2} . After the encoding process, a
setting of 350/40, and evaluated by five proficient radiologists.
convolutional layer is applied before the residual connection.
The ultimate human perceptual score for each image was
The output of the scale Swin transformer block is formulated as
determined by averaging the individual ratings by these
radiologists. The specific criteria for this assessment are
ğ¹ =ğ›¼âˆ™ğ» (ğ» (ğ¹Ìƒ ))+ğ¹Ìƒ (16)
ğ‘œğ‘¢ğ‘¡ ğ¶ğ‘‚ğ‘ğ‘‰ ğ‘†ğ‘‡ğ¿ ğ‘–,2 ğ‘–,0 detailed in Table I.
where ğ» ğ¶ğ‘‚ğ‘ğ‘‰ (âˆ™) is the convolutional layer, and ğ›¼ denotes a Table I. Image scoring criteria.
scale factor for the output of the Swin transformer layer. Score Quality Diagnostic quality criteria
0 Bad Desired features are not shown
4) Patch-Weighted Quality Prediction
1 Poor Diagnostic interpretation is impossible
A dual-branch structure for patch-weighted quality
2 Fair Suitable for compromised interpretation
prediction is shown in Fig. 3e. Given the feature tensor, we
generate weight and score projections, which are achieved 3 Good Good for diagnostic interpretation
through 2 independent linear projections. The final patch score 4 Excellent Anatomical features are clearly visible
of the distorted image is generated by multiplying the score and
weight of each patch, then the final score of the whole image is D. Network Training
generated by the summation of all the final patch scores.
For training the DDPM model, we set the total number of time
C. Dataset steps ğ‘‡ to 1,000. The model underwent training using the Adam
optimizer, with a learning rate of 1Ã—10âˆ’4. The training
1) DDPM Training Dataset
process demonstrated convergence after 5Ã—105 iterations on a
For training DDPM, we simulated images with varying levels
computing server equipped with four Nvidia Tesla V100 GPUs.
of dosage, by adding realistic noise to the dataset used in the
Upon completing the training process, we employed the trained
2016 low-dose CT grand challenge [56]. With the assumed use
DDPM to obtain the primary contents from the low-dose CT
of a monochromatic source, the projection measurements from
BIQA challenge dataset. Subsequently, we computed the
a CT scan follow the Poisson distribution, which can be
dissimilarity map, resulting in a multi-channel image.
expressed as
To train the transformer-based quality evaluator, we selected
the ViT-B/8 model as our pre-trained model. This model was
ğ‘› ~ğ‘ƒğ‘œğ‘–ğ‘ ğ‘ ğ‘œğ‘›{ğ‘ğ‘’âˆ’ğ‘™ğ‘– +ğ‘Ÿ}, ğ‘– =1,â‹¯,ğ¼ (17)
ğ‘– ğ‘– ğ‘– initially trained on ImageNet-21k and fine-tuned on ImageNet-
1k, using a patch size of 8. To accommodate the varying input
where ğ‘› is the measurement along the i-th ray path. ğ‘ are the
ğ‘– ğ‘– sizes of our datasets, we applied central cropping to resize the
air scan photons, ğ‘Ÿ denotes read-out noise. In Eq. (17), the
ğ‘– images to 448Ã—448, as the edges of the images typically
noise level can be controlled by ğ‘. This allowed us to obtain
ğ‘– represent air and have limited influence on the overall image
three additional noise levels equivalent to 50%, 25% and 10%
quality score. Furthermore, we resized these cropped images to
of the normal dose for the 2016 low-dose CT grand challenge
a final size of 224Ã—224. Then, we combined a dissimilarity map
dataset. Streak artifacts are generated using a similar pipeline to
and two duplicated distorted images to form a three-channel
noise insertion, but by reconstructing with different numbers of
image tensor.
projections. There are three different numbers of projection
The transformer-based quality evaluator consists of two
views used in our study, which are 720, 360, and 180 views
stages, each comprising 2 transposed attention blocks and 1
equiangularly distributed over a full scan. For each noise level
scale Swin transformer block. The dimensions of the hidden
we obtain 3 different sparse view, hence we obtained 12
layer, the number of heads, and the window size are set to 768,
reconstructed images for each normal dose scan. We selected
7 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. xx, NO. x, 2020
4, and 4 in each scale Swin transformer block. We set the scale
parameter to 0.8 in the scale Swin transformer block. For
training, we established the learning rate at 1Ã—10âˆ’5 and used
a batch size of 8. The Adam optimizer was used with a weight
decay of 1Ã—10âˆ’5 and cosine annealing for learning rate
scheduling. The selected loss function was the mean square
error (MSE). As our transformer-based quality evaluator was
built upon the foundation of MANIQA [39], we still call the
transformer-based quality assessment method without primary
content as MANIQA throughout this paper. Our experiments
were conducted on an NVIDIA RTX A4000 using PyTorch
2.0.1 and CUDA 11.8 for both training and testing.
E. Evaluation Criteria
We use Pearsonâ€™s linear correlation coefficient (PLCC), the
absolute value of the Spearmanâ€™s rank order correlation
coefficient (SROCC), and the Kendall rank-order correlation
coefficient (KROCC) as the metrics to evaluate the
performance of our models. The PLCC is defined as
âˆ‘ğ‘€ (ğ‘  âˆ’ğœ‡ )(ğ‘ Ì‚ âˆ’ğœ‡ )
ğ‘ƒğ¿ğ¶ğ¶ =
ğ‘–=1 ğ‘– ğ‘ ğ‘– ğ‘– ğ‘ Ì‚ğ‘–
(18)
âˆšâˆ‘ğ‘€ (ğ‘  âˆ’ğœ‡ ) 2 âˆšâˆ‘ğ‘€ (ğ‘ Ì‚ âˆ’ğœ‡ ) 2
ğ‘–=1 ğ‘– ğ‘ ğ‘– ğ‘–=1 ğ‘– ğ‘ Ì‚ğ‘–
where ğ‘  and ğ‘ Ì‚ respectively indicate the ground-truth and
ğ‘– ğ‘–
predicted quality scores of the i-th image, ğœ‡ and ğœ‡ indicate
ğ‘ ğ‘– ğ‘ Ì‚ğ‘–
their means, and ğ‘€ denotes the testing images. Let ğ‘‘ denote
ğ‘–
the difference between the ranks of the i-th test image in
ground-truth and the predicted quality score. The SROCC is
defined as
6âˆ‘ğ‘€ ğ‘‘2
ğ‘–=1 ğ‘–
ğ‘†ğ‘…ğ‘‚ğ¶ğ¶ =1âˆ’ (19)
ğ‘€(ğ‘€2âˆ’1)
The KROCC is defined as:
2(ğ‘€ âˆ’ğ‘€ )
ğ‘ ğ‘‘
ğ¾ğ‘…ğ‘‚ğ¶ğ¶ = (20)
ğ‘€(ğ‘€âˆ’1)
Figure 4. Visualization of the primary contents produced using different
where ğ‘€ is the count of data pairs sharing the same rank, and
ğ‘ reconstructed methods. (a1)â€“(a3) the input distorted images at different
ğ‘€ ğ‘‘ is the count of data pairs with different rank. We use the distortion levels, of which quality scores are 0.2, 2.0 and 4.0,
absolute values of all the metrics, PLCC, SROCC and KROCC, respectively; (b1)â€“(b3) the primary contents reconstructed by RED-
are all in the range of [0, 1]. A higher value indicates better CNN; (c1)â€“(c3) the primary contents reconstructed by SU-Net; and (d1)â€“
(d3) the primary contents reconstructed by DDPM. The display window
performance. The overall metric is defined as
(width/level) is set to 350/40 HU.
ğ‘‚ğ‘£ğ‘’ğ‘Ÿğ‘ğ‘™ğ‘™ =ğ‘ƒğ¿ğ¶ğ¶+ğ‘†ğ‘…ğ‘‚ğ¶ğ¶+ğ¾ğ‘…ğ‘‚ğ¶ğ¶ (21)
In Fig. 4, we present three representative samples from the
low-dose CT BIQA challenge dataset, with assigned quality
III. RESULTS
scores of 0.2, 2.0, and 4.0 respectively. It can be seen that the
A. Inspection of Primary Contents FBP-reconstructed image with a quality score of 0.2 exhibits
As obtaining high-quality primary contents is of paramount distortions caused by both noise and streak artifacts. Crucial
importance in mimicking the HVS, our initial focus is on features are not discernible. At a quality score of 2.0, the FBP-
evaluating primary contents across various quality scores and reconstructed image displays noticeable streak artifacts, but the
comparing the outcomes using distinct reconstruction methods. main structural features are still recognizable. In the FBP
We employed the four classic methods for this purpose: 1) FBP: reconstruction with a quality score of 4.0, the anatomical
Reconstructing the original image from the low-dose CT BIQA structure is highly visible, qualified as a reference image. The
challenge dataset using FBP; 2) RED-CNN: Utilizing CNN for RED-CNN method successfully mitigates noise artifacts but
low-dose CT reconstruction; 3) SU-Net: Combining the U-Net sometimes amplifies streak artifacts. This effect is particularly
architecture with the Swin transformer for low-dose CT pronounced in the image with a quality score of 0.2, exerting a
reconstruction; and 4) DDPM: Known for its powerful image negative impact on image quality. SU-Net effectively
generation capabilities in low-dose CT reconstruction. suppresses both noise and streak artifacts. However, the
8 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. xx, NO. x, 2020
resultant image appears overly smooth, deviating from the
authentic appearance of the real clinical counterpart. The
DDPM approach emerges as an accurate yet robust solution,
effectively suppressing noise and streak artifacts while
simultaneously preserving intricate details. Furthermore, the
distribution of the images closely aligns with that of real clinical
reference images. As a result, opting for DDPM to generate
high-quality primary contents proves feasible and
advantageous, facilitating an accurate emulation of the IGM
principle within the HVS.
B. Visualization of the Prior Information
By employing primary contents generated through DDPM,
we can glean insights into the proposed method. In Fig. 5, the
first row showcases distorted images across varying levels of
distortions. From the second to the last row, we present
corresponding primary contents and dissimilarity maps. The
effectiveness of the IGM becomes evident in its ability to
mitigate noise and infer high-quality primary content, as
evidenced in Fig. 5(d)-(f). Concurrently, as depicted in Fig.
5(g)-(i), when subjected to distinct levels of artifacts, the
dissimilarity map exhibits diverse patterns. Notably, Fig. 5(i)
demonstrates lower dissimilarity, implying less content
degradation in Fig. 5(c).
Figure 6. Visualization of several exemplary images from the test
dataset. From left to right are the distorted images, the weight maps, and
scores. From top to bottom are five representative example images with
the radiologist scores of 0.0, 1.0, 2.0, 3.0 and 4.0 and our model
predictions respectively.
C. Visualization of Exemplary Images
In Fig. 6, we visualize distorted images and weight maps for
five represetative examples from the low-dose CT BIQA test
dataset. These weight maps clearly highlight salient subjects,
which are significant areas that strongly influence human
perception. When humans view an image, features of interest
greatly affects our perception, and therefore these features
receive higher weights as depicted in Fig. 6. Furthermore, as the
score increases, the weight also becomes larger. Regarding the
predicted scores for the images, we observed some bias relative
to the radiologistsâ€™ scores, indicating that there is still room for
improvement.
D. Performance Comparison within the Low-dose CT
BIQA Challenge Dataset
Given the absence of reference images in the dataset, we
Figure 5. Generative prior information with DDPM. (a)â€“(c) the input
distorted images at different distortion levels, with image quality scores straightforwardly partition the dataset based on distorted
of 0.2, 2.0 and 4.0 respectively. The lower the quality score, the higher images. We compared our method against one traditional BIQA
the distortion level and the worse the perceptual quality; (d)â€“(f) the method and two CNN-based methods, along with one
primary contents generated by DDPM; and (g)â€“(i) the dissimilarity maps
transformer-based method. For the traditional approach, we
calculated by SSIM.
selected the natural image quality evaluator (NIQE) as a
representative method as implemented in the official code.
9 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. xx, NO. x, 2020
Among the CNN-based methods, we used DBCNN and Hyper- helpful to derive the primary content and guide our image
IQA as representative networks, using the implementations quality assessment, our goal is achieved.
provided in [56]. As for the transformer-based method, we In conclusion, we have introduced a novel D-BIQA model
employed MANIQA. Referring to Table II, limited by the for image quality assessment based on DDPM-driven active
handcrafted features, the traditional NIQE demonstrates the inference. Leveraging the amazing image synthesis capability
lowest performance. The CNN-based methods show of DDPM, our active inference module seems adeptly
remarkable improvements over the NIQE approach. Between emulating the IGM theory to forecast the primary contents from
these two CNN-based methods, DBCNN outperforms in PLCC distorted images. Through the amalgamation of information
and KROCC metrics, while Hyper-IQA excels in SROCC. The derived from distorted images and dissimilarity maps, a multi-
performance of these two CNN-based methods is closely channel image tensor is formed and fed into a transformer-
comparable. As a representative of transformer-based methods, based quality evaluator. A comprehensive set of experiments
MANIQA exhibits superior performance over the CNN-based substantiates the efficacy and superiority of our proposed
methods. Remarkably, our proposed D-BIQA method achieves approach. While we achieved the second place in the MICCAI
the highest performance. In summary, D-BIQA demonstrates 2023 low-dose CT perceptual image quality assessment grand
strong efficacy on the low-dose CT BIQA challenge dataset, challenge with our reported transformer-based quality
thereby affirming the effectiveness of our proposed approach. evaluator, this much-improved approach for D-BIQA has
further enhanced performance guided by the IGM theory, which
Table II. Comparisons on the Low-dose CT BIQA Dataset.
is the main innovation of this work.
Methods PLCC SROCC KROCC Overall
NIQE 0.9181 0.9335 0.7897 2.6414
ACKNOWLEDGMENTS
DBCNN 0.9716 0.9693 0.8692 2.8103
HyperIQA 0.9680 0.9694 0.8672 2.8045 The authors would like to express their gratitude to the
MANIQA 0.9789 0.9792 0.9041 2.8622 organizers of the MICCAI 2023 low-dose CT perceptual image
quality assessment grand challenge for providing the dataset.
D-BIQA 0.9814 0.9816 0.9122 2.8753
REFERENCES
IV DISCUSSIONS AND CONCLUSION
[1] H. Chen, Y. Zhang, M. K. Kalra, F. Lin, Y. Chen, P. Liao, J. Zhou, and
Given the widespread artifacts in low-dose CT images, we
G. Wang, â€œLow-dose CT with a residual encoder-decoder convolutional
have opted for a transformer-based approach to comprehend neural network,â€ IEEE Transactions on Medical Imaging, vol. 36, no. 12,
these artifacts globally. It is worth emphasizing the importance pp. 2524â€“ 2535, 2017.
[2] K. H. Jin, M. T. McCann, E. Froustey, and M. Unser, â€œDeep convolutional
of pretraining when utilizing transformers. In this study, we
neural network for inverse problems in imaging,â€ IEEE Transactions on
have chosen to use a pretrained ViT-B/8 model with a 224Ã—224
Image Processing, vol. 26, no. 9, pp. 4509â€“4522, 2017.
configuration for further training on a low-dose CT dataset. In [3] Q. Yang, P. Yan, Y. Zhang, H. Yu, Y. Shi, X. Mou, M. K. Kalra, Y.
typical natural image BIQA, researchers often employ a random Zhang, L. Sun, and G. Wang, â€œLow-dose CT image denoising using a
generative adversarial network with Wasserstein distance and perceptual
crop of 224Ã—224 to align with the requirement for training a
loss,â€ IEEE Transactions on Medical Imaging, vol. 37, no. 6, pp. 1348â€“
transformer. However, in the case of CT images, the peripheral
1357, 2018.
region of the image predominantly corresponds to air and [4] E. Kang, W. Chang, J. Yoo, and J. C. Ye, â€œDeep convolutional framelet
should not influence the quality assessment. As a result, we denoising for low-dose CT via wavelet residual network,â€ IEEE
Transactions on Medical Imaging, vol. 37, no. 6, pp. 1358â€“1369, 2018.
have cropped each original 512Ã—512 images down to a
[5] H. Shan, Y. Zhang, Q. Yang, U. Kruger, M. K. Kalra, L. Sun, W. Cong,
centralized 448Ã— 448 image, excluding the air and less and G. Wang, â€œ3-d convolutional encoder-decoder network for low-dose
important features from consideration. To ensure the quality of CT via transfer learning from a 2-d trained network,â€ IEEE Transactions
neural network training while simultaneously minimizing on Medical Imaging, vol. 37, no. 6, pp. 1522â€“1534, 2018.
[6] Y. Han, J. C. Ye, â€œFraming U-Net via deep convolutional framelets:
computational complexity, we have employed a common
Application to sparse-view CT,â€ IEEE Transactions on Medical Imaging,
down-sampling technique [57]. Specifically, all images were vol. 37, no. 6, pp. 1418-1429, 2018.
consistently down-sampled to a uniform size of 224Ã—224. This [7] Z. Zhang, X. Liang, X. Dong, Y. Xie, G. Cao, â€œA sparse-view CT
enables us to effectively address the globalized artifacts in low- reconstruction method based on combination of DenseNet and
deconvolution,â€ IEEE Transactions on Medical Imaging, vol. 37, no. 6,
dose CT images while losing little information in this medical
pp. 1407-1417, 2018.
imaging domain. [8] H. Zhang, B. Liu, H. Yu, B. Dong, â€œMetaInv-Net: Meta inversion network
Directional artifacts in low-dose CT images can occasionally for sparse view CT image reconstruction,â€ IEEE Transactions on Medical
mislead radiologists because they might resemble actual Imaging, vol. 40, no. 2, pp. 621-634, 2020.
[9] D. Hu, J. Liu, T. Lv, Q. Zhao, Y. Zhang, G Quan, et al., â€œHybrid-domain
anatomical structures. To address this issue, we have
neural network processing for sparse-view CT reconstruction,â€ IEEE
implemented a DDPM to mitigate these artifacts. While the Transactions on Radiation and Plasma Medical Sciences, vol. 5, no. 1,
DDPM has demonstrated the capability to generate high-quality pp. 88-98. 2020.
images that closely resemble reality, it is important to clarify [10] M. Lee, H. Kim, H. J. Kim, â€œSparse-view CT reconstruction based on
multi-level wavelet convolution neural network,â€ Physica Medica, vol.
that our intention is not to restore a perfectly pristine or
80, pp. 352-362, 2020.
distortion-free image. Our DDPM efforts may still retain [11] Z. Wang, A. C. Bovik, H. R. Sheikh, E. P. Simoncelli, â€œImage quality
compromised information when dealing with severely distorted assessment: from error visibility to structural similarity,â€ IEEE
images. Nevertheless, our primary objective is to emulate the Transactions on Image Processing, vol. 13, no. 4, pp. 600-612. 2004.
[12] V. Kamble, K. M. Bhurchandi, â€œNo-reference image quality assessment
active inference process employed by IGM to predict the vital
algorithms: A survey,â€ Optik, vol. 126, no. 11-12, pp. 1090-1097, 2015.
content within the image. As long as the DDPM results are
10 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. xx, NO. x, 2020
[13] S. Xu, S. Jiang, W. Min, â€œNo-reference/blind image quality assessment: a [36] S. A. Golestaneh, S. Dadsetan, K. M. Kitani, â€œNo-reference image quality
survey,â€ IETE Technical Review, vol. 34, no. 3, pp. 223-245, 2017. assessment via transformers, relative ranking, and self-consistency,â€
[14] A. J. Smola and B. SchÃ¶lkopf, â€œA tutorial on support vector regression,â€ Proceedings of the IEEE Winter Conference on Applications of Computer
Statistics Computing, vol. 14, no. 3, pp. 199â€“222, 2004. Vision, pp. 1220-1230, 2022.
[15] A. K. Moorthy and A. C. Bovik, â€œBlind image quality assessment: From [37] M. V. Conde, M. Burchi, R. Timofte, â€œConformer and blind noisy
natural scene statistics to perceptual quality,â€ IEEE Transactions on students for improved image quality assessment,â€ Proceedings of the
Image Processing, vol. 20, no. 12 2011 IEEE Conference on Computer Vision and Pattern Recognition, pp. 940-
[16] G. Wang, Z. Wang, K. Gu, L. Li, Z. Xia, L. Wu, â€œBlind quality metric of 950, 2022.
DIBR-synthesized images in the discrete wavelet transform domain,â€ [38] J. Wang, H. Fan, X. Hou, Y. Xu, T. Li, X. Lu, â€œMSTRIQ: No reference
IEEE Transactions on Image Processing, vol. 11, no. 29 pp. 1802-1814, image quality assessment based on Swin transformer with multi-stage
2019. fusion,â€ Proceedings of the IEEE Conference on Computer Vision and
[17] C. Deng, S. Wang, A. C. Bovik, G. B. Huang, B. Zhao, â€œBlind noisy Pattern Recognition, pp. 1269-1278, 2022.
image quality assessment using sub-band kurtosis,â€ IEEE Transactions [39] S. Yang, T. Wu, S. Shi, S. Lao, Y. Gong, M. Cao, et al., â€œMANIQA:
on Cybernetics, vol. 50, no. 3, pp. 1146-1156, 2019. Multi-dimension attention network for no-reference image quality
[18] M. A. Saad, A. C. Bovik, C. Charrier, â€œBlind image quality assessment: assessment,â€ Proceedings of the IEEE Conference on Computer Vision
A natural scene statistics approach in the DCT domain,â€ IEEE and Pattern Recognition, pp. 1191-1200, 2022.
Transactions on Image Processing, vol. 21, no. 8, pp. 3339-3352, 2012. [40] L. Yu, J. Li, F. Pakdaman, M. Ling, M. Gabbouj, â€œMAMIQA: No-
[19] A. Mittal, A. K. Moorthy, and A. C. Bovik, â€œNo-reference image quality Reference Image Quality Assessment Based on Multiscale Attention
assessment in the spatial domain,â€ IEEE Transactions on Image Mechanism with Natural Scene Statistics,â€ IEEE Signal Processing
Processing, vol. 21, no. 12, pp. 4695-4708, Dec. 2012. Letters, vol. 30, pp. 588-592, 2023.
[20] A. Mittal, R. Soundararajan, and A. C. Bovik, â€œMaking a â€˜completely [41] J. Gu, H. Cai, C. Dong, J. S. Ren, R. Timofte, Y. Gong et al., â€œNTIRE
blindâ€™ image quality analyzer,â€ IEEE Signal Processing Letter, vol. 22, 2022 challenge on perceptual image quality assessment,â€ Proceedings of
no. 3, pp. 209â€“212, 2013. the IEEE Conference on Computer Vision and Pattern Recognition, pp.
[21] M. Khan, I. F. Nizami, and M. Majid, â€œNo-reference image quality 951-967, 2022.
assessment using gradient magnitude and wiener filtered wavelet [42] G. Zhai, X. Wu, X. Yang, W. Lin, and W. Zhang, â€œA psychovisual quality
features,â€ Multimedia Tools and Applications, vol. 78, pp. 14485-14509, metric in free-energy principle,â€ IEEE Transactions on Image
2019. Processing, vol. 21, no. 1, pp. 41â€“52, Jan. 2012.
[22] Q. Li, W. Lin, J. Xu, and Y. Fang, â€œBlind image quality assessment using [43] J. Wu, W. Lin, G. Shi, and A. Liu, â€œPerceptual quality metric with internal
statistical structural and luminance features,â€ IEEE Transactions on generative mechanism,â€ IEEE Transactions on Image Processing, vol.
Multimedia, vol. 18, no. 12, pp. 2457â€“2469, 2016. 22, no. 1, pp. 43â€“54, Jan. 2013.
[23] Q. Wu, Z. Wang, H. Li, â€œA highly efficient method for blind image quality [44] K. Gu, G. Zhai, X. Yang, and W. Zhang, â€œUsing free energy principle for
assessment,â€ Proceedings of the IEEE International Conference on Image blind image quality assessment,â€ IEEE Transactions on Multimedia, vol.
Processing, pp. 339-343, 2015. 17, no. 1, pp. 50â€“63, Jan. 2015.
[24] M. Zhang, C. Muramatsu, X. Zhou, T. Hara, H. Fujita, â€œBlind image [45] J. Ma, J. Wu, L. Li, W. Dong, X. Xie, G. Shi, â€œBlind image quality
quality assessment using the joint statistics of generalized local binary assessment with active inference,â€ IEEE Transactions on Image
pattern,â€ IEEE Signal Processing Letters, vol. 22, no. 2, pp. 207-210, Processing,â€ vol. 30, pp. 3650-3663, 2021.
2014. [46] M. P. Eckert and A. P. Bradley, â€œPerceptual quality metrics applied to still
[25] W. Xue, X. Mou, L. Zhang, A. C. Bovik, X. Feng, â€œBlind image quality image compression,â€ Signal Processing, vol. 70, no. 3, pp. 177â€“200, 1998.
assessment using joint statistics of gradient magnitude and Laplacian [47] J. Ho, A. Jain, P. Abbeel, â€œDenoising diffusion probabilistic models,â€
features,â€ IEEE Transactions on Image Processing, vol. 23, no. 11, pp. Advances in Neural Information Processing Systems, vol. 33, pp. 6840-
4850-4862, 2014. 51, 2020.
[26] L. Kang, P. Ye, Y. Li, D. Doermann, â€œConvolutional Neural Networks for [48] C. Saharia, J. Ho, W. Chan, T. Salimans, D. J. Fleet, M. Norouzi, â€œImage
No-Reference Image Quality Assessment,â€ Proceedings of the IEEE super-resolution via iterative refinement,â€ IEEE Transactions on Pattern
Conference on Computer Vision and Pattern Recognition, pp. 1733-1740, Analysis and Machine Intelligence, vol. 45, no. 4, pp. 4713-4726, 2022.
2014. [49] P. Dhariwal, A. Nichol, â€œDiffusion models beat GANs on image
[27] W. Hou, X. Gao, D. Tao, X. Li, â€œBlind image quality assessment via deep synthesis,â€ Advances in Neural Information Processing Systems, vol. 34,
learning,â€ IEEE Transactions on Neural Networks and Learning Systems, pp. 8780-94, 2021.
vol. 26, no. 6, pp. 1275-1286, 2014. [50] W. Xia, Q. Lyu, G. Wang, â€œLow-Dose CT Using Denoising Diffusion
[28] X. Yang, F. Li, H. Liu, â€œA survey of DNN methods for blind image quality Probabilistic Model for 20x Speedup,â€ arXiv preprint arXiv:2209.15136,
assessment,â€ IEEE Access, vol. 7, pp. 123788-123806, 2019. 2022.
[29] X. Liu, J. Weijer, A. D. Bagdanov, â€œRank-IQA: Learning from Rankings [51] Y. Shi, G. Wang, â€œConversion of the Mayo LDCT Data to Synthetic
for No-Reference Image Quality Assessment,â€ Proceedings of the IEEE Equivalent through the Diffusion Model for Training Denoising Networks
International Conference on Computer Vision, pp. 1040-1049, 2017. with a Theoretically Perfect Privacy,â€ arXiv preprint arXiv:2301.06604,
[30] W. Zhang, K. Ma, J. Yan, D. Deng, Z. Wang, â€œBlind image quality 2023.
assessment using a deep bilinear convolutional neural network,â€ IEEE [52] Q. Gao, S. Li, M. Zhu, D. Li, Z. Bian, Q. Lyu, et al., â€œBlind CT image
Transactions on Circuits and Systems for Video Technology, vol. 30, no. quality assessment via deep learning framework,â€ Proceeding of the IEEE
1, pp. 36â€“47, 2018. Nuclear Science Symposium and Medical Imaging Conference, pp. 1-4,
[31] S. Su, Q. Yan, Y. Zhu, C. Zhang, X. Ge, J. Sun et al., â€œBlindly assess 2019.
image quality in the wild guided by a self-adaptive hyper network,â€ [53] S. Li, J. He, Y. Wang, Y. Liao, D. Zeng, Z. Bian, et al., â€œBlind CT image
Proceedings of the IEEE Conference on Computer Vision and Pattern quality assessment via deep learning strategy: initial study,â€ Proceeding
Recognition, pp. 3667-3676, 2020. of Society of Photo-Optical Instrumentation Engineers, vol. 10577, pp.
[32] H. Zhu, L. Li, J. Wu, W. Dong, G. Shi, â€œMeta-IQA: Deep meta-learning 293-297, 2018.
for no-reference image quality assessment,â€ Proceedings of the IEEE [54] Online available: https://ldctiqac2023.grand-challenge.org/.
Conference on Computer Vision and Pattern Recognition, pp. 14143- [55] O. Ronneberger, P. Fischer, T. Brox, â€œU-net: Convolutional networks for
14152, 2020. biomedical image segmentation,â€ In International Conference on Medical
[33] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Image Computing and Computer-Assisted Intervention, vol. 9351, pp.
Unterthiner, â€œAn image is worth 16x16 words: Transformers for image 234-241, 2015.
recognition at scale,â€ arXiv reprint arXiv:2010.11929, 2020. [56] C. Chen, Pytorch toolbox for image quality assessment, 6, 2022.
[34] J. You, J. Korhonen, â€œTransformer for image quality assessment,â€ [57] H. R. Sheikh, A. C. Bovik, G. D. Veciana, â€œAn information fidelity
Proceeding of the IEEE International Conference on Image Processing, criterion for image quality assessment using natural scene statistics,â€
pp. 1389-1393, 2021. IEEE Transactions on Image Processing, vol. 14, no. 12, pp. 2117-2128,
[35] J. Ke, Q. Wang, Y. Wang, P. Milanfar, F. Yang, â€œMUSIQ: Multi-scale 2005.
image quality transformer,â€ Proceedings of the IEEE International
Conference on Computer Vision, pp. 5148-5157, 2021.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Blind CT Image Quality Assessment Using DDPM-derived Content and Transformer-based Evaluator"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   âŒ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   âœ… GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   âŒ BAD: Repeating the same claim 3+ times with slight variations
   âœ… GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
