=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: A Concise Mathematical Description of Active Inference in Discrete Time
Citation Key: oostrum2024concise
Authors: Jesse van Oostrum, Carlotta Langer, Nihat Ay

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2024

Abstract: In this paper we present a concise mathematical description of active
inference in discrete time. The main part of the paper serves as a ba-
sic introduction to the topic, including a detailed example of the action
selection mechanism. Theappendixdiscussesthemoresubtlemathemat-
ical details, targeting readers who have already studied the active infer-
ence literature but struggle to make sense of the mathematical details
and derivations. Throughout, weemphasizeprecise andstandard mathe-
matical ...

Key Terms: discrete, action, details, main, description, introduction, inference, mathematical, active, selection

=== FULL PAPER TEXT ===

5202
yaM
71
]GL.sc[
4v62770.6042:viXra
A Concise Mathematical Description of Active
Inference in Discrete Time
Jesse van Oostrum, Carlotta Langer, Nihat Ay
Abstract
In this paper we present a concise mathematical description of active
inference in discrete time. The main part of the paper serves as a ba-
sic introduction to the topic, including a detailed example of the action
selection mechanism. Theappendixdiscussesthemoresubtlemathemat-
ical details, targeting readers who have already studied the active infer-
ence literature but struggle to make sense of the mathematical details
and derivations. Throughout, weemphasizeprecise andstandard mathe-
matical notation, ensuring consistency with existing texts and linking all
equations to widely used references on active inference. Additionally, we
provide Python code that implements the action selection and learning
mechanisms described in this paper and is compatible with pymdp envi-
ronments.
Introduction
Active inference is a theory that describes the action selection and learning
mechanismsofanagentinanenvironment. Weaimtopresentaconcisemathe-
maticaldescriptionofthe theorysothatreadersinterestedinthemathematical
details can quickly find what they are looking for. We have paid special atten-
tionto choosingnotationthatismoreinlinewithstandardmathematicaltexts
andisalsodescriptive,inthesensethatdependenciesaremadeexplicit. Hence,
the focus of this paper lies on the mathematical details and derivations rather
than verbal motivations and justifications.
The paper consists of a main text and an appendix. The main text provides
a clear introduction to active inference in discrete time, accessible for people
new to the topic. It is divided into two parts: inference, which assumes a
givengenerativemodel,andlearning,whichexplainshowtheagentacquiresthis
model. Themaintextconcludeswithaworkedexampleofactionselection. The
appendix delves into finer details and derivations, catering to readers familiar
with active inference who seek clarity on the mathematical aspects.
To complement our theoretical exposition, we provide a Python implementa-
1
tion1 of the action selection and learning mechanisms described in this paper,
which is compatible with pymdp environments. This code is more minimalistic,
which makes it easier to understand than other implementations such as SPM
and pymdp.
1 Set-up and notation
In this paper we consider an active inference agent acting in a discrete-time
setting witha finite-time horizon. This meansthat weconsidera sequenceofT
time steps and at every time step τ the agent receives an observation o , and
τ
performs an action a . We use τ for arbitrary time steps and the letter t to
τ
denote the current time step. We use the subscript to denote a sequence
τ:τ′
of variables, e.g. o =(o ,...,o ). A sequence of (future) actions is called a
τ:τ′ τ τ′
policy2 and is denoted by π = a , with π = π . We write a for actions
t t:T 1 1:t−1
that were performed in the past and π for future actions that still need to be
t
selected.
Theagentmodelsthedynamicsoftheenvironmentusinganinternalgenerative
model. This model uses a variable s , called an internal state, to represent the
τ
state of the environment3 at time step τ. The model is given by the following
probability distribution:
p(o ,s |a ,θ).
1:T 1:T 1:T−1
This probability distribution factorizes according to the graph in Figure 1. In
a a a
1 2 T−1
s s s s
1 2 3 T
o o o o
1 2 3 T
Figure 1: Graphical representation of the generative model
the first part of this paper we assume that this generative model is given and
neednotbelearned,andwethereforesuppressthedependenceontheparameter
θ. In the second part we discuss how the model is learned.
Suppose the agent is at time step t. It will have received observations o and
1:t
performedactionsa . Weuseq (s )todenotethe(approximate)posterior
1:t−1 t τ:τ′
1https://github.com/jessevoostrum/active-inference
2Notethatinareinforcementlearningcontexttheterm“policy”hasadifferentmeaning.
3Notethatthenumberofpossibleinternalstatesisusuallymuchsmallerthantheactual
numberofstates theenvironmentcanbein.
2
distribution of the generative model, p(s |o ,a ), and also refer to this
τ:τ′ 1:t 1:t−1
as the belief of the agent about the variable s .
τ:τ′
2 Inference
2.1 Action selection according to active inference
Let anagentbe at time step t, having receivedobservationso and performed
1:t
actions a . According to active inference an agent selects its next action by
1:t−1
sampling a policy π from the following distribution (equation (10) in [4]):
t
σ(−G(π |o ,a )) (1)
t 1:t 1:t−1
andselectingtheactiona correspondingtothatpolicy.4 Thefunctionσdenotes
t
the softmax function defined in (48) and G is the expected free energyfunction
given by
G(π |o ,a )=− E D q (s |o ,π )kq (s |π )
t 1:t 1:t−1 qt(ot+1:T|πt) KL t t+1:T t+1:T t t t+1:T t
(cid:18) (cid:20) (cid:16) (cid:17)(cid:21)
+E lnp (o ) .
qt(ot+1:T|πt) C t+1:T
h i(cid:19) (2)
Note that according to (1) the agent is more likely to sample policies π that
t
have a low expected free energy G(π ,o ,a ). In Section 2.2 we discuss
t 1:t 1:t−1
equivalent formulations and different interpretations of the expected free en-
ergy. The distributions q (o |π ), q (s |o ,π ), q (s |π ), needed
t t+1:T t t t+1:T t+1:T t t t+1:T t
for the calculation of G, are (approximate) posterior distributions of the gen-
erative model of the agent after having observed o and performed a . In
1:t 1:t−1
Section 2.3 we describe how the agent infers these posterior distributions. The
distribution p is a preference distribution over observations that we assume is
C
given to the agent. This distribution is distinct from the generative model p.
Remark 1. Note that in certain descriptions of active inference in discrete time
alsoavariationalfreeenergytermF appearsinthe distributioninequation(1)
(e.g. equation (B.9) in [8]). This term is only relevant in specific cases that
we will discuss in Remark 2 in Appendix A. Furthermore a habit term E is
sometimes included that is also considered in Appendix A, but discarded here
for simplicity.
4Note that at every time step a new policy is sampled, only the action at for the corre-
spondingtimesteptisexecuted, andtherestoftheactions inπt arediscarded.
3
2.2 Expected free energy
Recall that equation (2) gives the following expression for the expected free
energy function:
G(π |o ,a )=− E D q (s |o ,π )kq (s |π )
t 1:t 1:t−1 qt(ot+1:T|πt) KL t t+1:T t+1:T t t t+1:T t
(cid:18) (cid:20) (cid:16) (cid:17)(cid:21)
+E lnp (o ) .
qt(ot+1:T|πt) C t+1:T
h i(cid:19)
The first termbetween the bracketsonthe RHS is calledepistemic value orin-
formation gain. Itmeasurestheaveragechangeinbeliefaboutthefuturestates
s due to receiving future observations o . The second term, known
t+1:T t+1:T
as utility, quantifies the similarity between the expected future observationdis-
tribution and the preferred observation distribution. As previously mentioned,
the agentis more likely to sample policies with low expected free energy,which
correspond to high information gain and utility.
An equivalent formulation of expected free energy is given by
G(π |o ,a )= E H p(o |s )
t 1:t 1:t−1 qt(st+1:T|πt) t+1:T t+1:T
(3)
h (cid:2) (cid:3)i
+D q (o |π )kp (o ) .
KL t t+1:T t C t+1:T
(cid:16) (cid:17)
The first term on the RHS is referred to as ambiguity. It measures the average
uncertainty an agent has about its future observations given knowledge of its
futurestates. Thesecondtermiscalledexpectedcomplexity orrisk. Itrepresents
the divergence between expected and preferred future observations. The agent
favors policies with low ambiguity and risk.
In Appendix C we show that both expressions of the expected free energy are
equal.
In practice, often the following mean field approximations are made:
T
q (s )= q (s ),
t t+1:T t τ
τ=t+1
Y
T
p (o )= p (o ).
C t+1:T C τ
τ=t+1
Y
Equation (2) and (3) can then be written as follows:
T
G(π |o ,a )= G (π ,o ,a ), (4)
t 1:t 1:t−1 τ t 1:t 1:t−1
τ=t+1
X
4
with
G (π |o ,a )=− E D q (s |o ,π )kq (s |π ) (5)
τ t 1:t 1:t−1 qt(oτ|πt) KL t τ τ t t τ t
(cid:18) h (cid:16) (cid:17)i
+E lnp (o )
qt(oτ|πt) C τ
h i(cid:19)
=E H p(o |s ) +D q (o |π )kp (o ) .
qt(sτ|πt) τ τ KL t τ t C τ
h (cid:2) (cid:3)i (cid:16) (cid:17)
It is outside the scope of this paper to further derive or motivate the expected
free energy. We referthe readerto Appendix B.2.5 in[8] and[3, 10, 7]for more
details.
2.3 State inference
Inthissectionwedescribethesimplestformofstateinference,whichisobtained
byapplyingBayes’rule. Stateinferencemethodsasdescribedine.g.[8,6,9]can
be thought of as computationally efficient approximations of what is described
here. See Appendix A for more details on these methods.
Above we defined q to be the (approximate) posterior of the generative model
t
given o , a . In this section we make the conditioning variables explicit
1:t 1:t−1
and write q(·|o ,a ) instead.
1:t 1:t−1
Current and future state inference
We start by studying the generative model that is assumed to be given to the
agent. The generative model can be decomposed as follows: (see Figure 1)
p(o ,s |a )=p(s |a )p(o |s )
1:T 1:T 1:T−1 1:T 1:T−1 1:T 1:T
T
p(s |a )=p(s ) p(s |s ,a )
1:T 1:T−1 1 τ τ−1 τ−1
τ=2
Y
T
p(o |s )= p(o |s ).
1:T 1:T τ τ
τ=1
Y
At every time step the agent updates its belief about the current state it is in.
Before having performed any observations, its belief about the current state is
equal to the prior belief p(s ). After receiving o it will update its belief using
1 1
Bayes’ rule:
q(s |o )∝p(o |s )p(s ).
1 1 1 1 1
5
Subsequently, it will performan actiona (selected as described in Section 2.1)
1
and receive a next observation o . The belief about s is given by
2 2
q(s |o ,a )∝p(o |s ,o ,a )p(s |o ,a )
2 1:2 1 2 2 1 1 2 1 1
=p(o |s ) p(s |s ,o ,a )p(s |a ,o )
2 2 2 1 1 1 1 1 1
X
s1
=p(o |s ) p(s |s ,a )q(s |o ).
2 2 2 1 1 1 1
X
s1
For a general t the belief about s is updated as follows:
t
q(s |o ,a )∝p(o |s ) p(s |s ,a )q(s |o ,a ). (6)
t 1:t 1:t−1 t t t t−1 t−1 t−1 1:t−1 1:t−2
s Xt−1
For future time point τ >t, the belief about the state s is given by
τ
q(s |o ,a )= p(s |s ,a )q(s |o ,a ). (7)
τ 1:t 1:τ−1 τ τ−1 τ−1 τ−1 1:t 1:τ−2
s Xτ−1
Thebeliefaboutafuturestategivenafutureobservationiscalculatedasfollows:
q(s |o ,o ,a )∝p(o |s )q(s |o ,a ). (8)
τ τ 1:t 1:τ−1 τ τ τ 1:t 1:τ−1
Future observation inference
In order to compute G in (2), we need a posterior distribution q(o |o ,a )
τ 1:t 1:τ−1
over future observations o . This can be computed as follows:
τ
q(o |o ,a )= p(o |s )q(s |o ,a ). (9)
τ 1:t 1:τ−1 τ τ τ 1:t 1:τ−1
X
sτ
Past, current, and future state inference
In order to perform inference overstates in the past, presentand future (which
is needed for the learning of the generative model and for the computation of
thevariationalfreeenergyoverstates),the agentcanusethefollowingformula:
q(s |o ,a ,π )∝p(o |s ,a ,π )p(s |a ,π )
1:T 1:t 1:t−1 t 1:t 1:T 1:t−1 t 1:T 1:t−1 t
=p(o |s )p(s |a ,π )
1:t 1:t 1:T 1:t−1 t
t T
= p(o |s ) p(s ) p(s |s ,a ), (10)
τ τ 1 τ τ−1 τ−1
τ=1 τ=2
Y Y
where we use a in the last term for elements of both a and π .
τ−1 1:t−1 t
6
3 Learning
Inthe abovesection,wehaveassumedthatthe agenthasaccesstoagenerative
model p(o ,s |a ,θ). In this section we discuss how the parameter θ of
1:T 1:T 1:T−1
this model is learned. The generative model consists of three (conditional) cat-
egoricaldistributionsthatareparametrizedbyθ =(θD,θA,θB)inthefollowing
way:
p(s (j) |θD)=θD, (11)
1 j
p(o(i)|s(j),θA)=θA, (12)
τ τ ij
p(s(j)|s(k) ,a(l) ,θB)=θB , (13)
τ τ−1 τ−1 jkl
where we have enumerated the elements of the observation, action and state
spacewiththebracketedsuperscript(·). Inordertolearntheparametersofthe
generative model, we adopt a Bayesianbelief updating scheme with a Dirichlet
prior. (See Appendix B for details on this.) More specifically, the prior over θ
is parametrized by α=(αD,αA,αB) and is given by
p(θ|α)=p(θD|αD) p(θA|αA) p(θB |αB) (14)
•j •kl
j k,l
Y Y
p(θD|αD)∝ θD αD j −1 , (15)
j
j
Y(cid:0) (cid:1)
p(θA|αA)∝ θA αA ij −1 , (16)
•j ij
i
Y(cid:0) (cid:1)
p(θB |αB)∝ θB αB jkl −1 , (17)
•kl jkl
j
Y(cid:0) (cid:1)
where θ denotes the vector (θ ,...,θ ).
•j 1j nj
Now after performing actions a and receiving observations o we want
1:T−1 1:T
to update our belief about θ according to Bayes’rule. The true posteriors over
these parameters are not Dirichlet distributions. (See Appendix B for details.)
The active inference literature suggests to set the approximate posterior distri-
bution to be a Dirichlet distribution and update the hyperparameter α in the
followingway(equation(B.12)in[8] andequation(21),(A.6)and(A.7) in[4]):
αD′ =αD+q s(j) , (18)
j j T 1
T (cid:16) (cid:17)
αA′ =αA +
ij ij
τ=1 X
1
(o )q s(j) , (19)
o(i) τ T τ
(cid:16) (cid:17)
T
αB ′ =αB + q s(j) q s(k)
jkl jkl T τ T τ−1
X τ=2 (cid:16) (cid:17) (cid:16) (cid:17)
1
(a ). (20)
a(l) τ−1
7
The distributions q (s ),τ ∈ {1,...,T} are approximate posteriors obtained5
T τ
using the current version of the generative model (before θ has been updated).
In Appendix B we elaborate on the origin of this learning rule.
Note the similarity with the standard update rule for Dirichlet priors given
in (31). In the standard update rule the element α of the hyperparameter
i∗
corresponding to the observation x(i∗) is incremented by 1, which makes this
observation more likely in the updated distribution. In the updates (18)–(20)
the hyperparameters are incremented by the amount of posterior belief in that
state or state transition, e.g. αD is incremented by q s(j)|o ,a .
j 1 1:T 1:T−1
(cid:16) (cid:17)
In order to go from a Dirichlet distribution p(θ|α) to an actual value of the pa-
rameterthatcanbe usedforthegenerativemodel,themeanofthe distribution
can be used, which is given by
θˆ =E [θ ]
i p(θ|α) i
α
i
= . (21)
α
j j
For example, after the learning step, tPhe new distribution over s is given by
1
αD′
p(s(j)|θˆD)= j
.
1 αD′
j k
P
This concludes the discussion of learning in the context of the active inference
framework.
4 Example: T-maze example
In this section we discuss the action selection mechanism of an active inference
agent in the T-maze environment depicted in Figure 2. (See also Section 7.3 of
[8] and [5].) This illustrates the theory of action selection and state inference
that is presented in Section 2.
5SeeSection2.3.
8
? ¨ ? ¨
cue
Figure 2: T-maze environment
Description of the (internal) generative model of the agent
State, observation and action spaces
The internal state space of the agent6 S has two dimensions,7, Location and
Reward condition, and can be described as follows:
S =SL×SR,
SL ={center, right arm, left arm,cue location},
SR ={reward on right, reward on left}.
A typical element of the state space is written as s=(sL,sR).
The observation space O has three dimensions8,9, Location, Reward, and Cue,
and can be described as follows:
O=OL×OR×OC,
OL ={center, right arm, left arm, cue location},
OR ={no reward, reward, loss},
OC ={cue right, cue left}.
A typical element of the observation space is written as o=(oL,oR,oC).
6Inthisexamplethestateoftheenvironment(generativeprocess)isthesameasthestate
spaceoftheinternalworldmodeloftheagent(generativemodel). Notethatthisisingeneral
notthecase. Inamorerealisticsettingthestatespaceoftheenvironmentwillbemuchmore
complexthantheinternalstatespace.
7Thedimensionsofthestatespacearesometimesreferredtoasstatefactors.
8Thedimensionsoftheobservationspacearesometimesreferredtoasobservationmodal-
ities.
9Notethatwefollowherethedescriptionfrom[5]. In[8]thecueobservationisabsorbed
intothelocationobservation.
9
The space of actions A is described by
A={move to center, move to right arm,
move to left arm, move to cue location}.
Notethattheseactionsarealwaysavailable,independentofthecurrentlocation
of the agent. A typical element of the action space is written as a.
In the following we use [dir] as a placeholder for left and right, and [loc]
as a placeholder for the four locations center, right arm, left arm and cue
location.
Observation kernel p(o|s)
We now specify the observation kernel p(o|s) of the generative model of the
agent. First note that the observation dimensions are independent, that is
p(o|s)=p(oL|s)p(oR|s).
The beliefs about the location observation given the state are modelled as fol-
lows:
1 if oL =sL
p(oL|s)= ,
(0 otherwise
which implies that the location can be unambiguously inferred from the obser-
vation.
Thebeliefsabouttherewardobservationgiventhestatearemodelledasfollows:
p(oR =no reward|sL ∈{center,cue location},sR)=1,
p(oR =no reward|sL ∈/ {center,cue location},sR)=0,
p(oR =reward|sL =[dir] arm,sR =reward on [dir])=0.98,
p(oR =loss|sL =right arm,sR =reward on left)=0.98,
p(oR =loss|sL =left arm,sR =reward on right)=0.98.
This implies that the agent observes no reward when it is in location center
orcue location,itobservesrewardwhenitis inthe samearmasspecifiedby
the reward condition with high probability, and it observes loss when it is in
the opposite arm of the reward condition with high probability.
The beliefs about the cue observation given the state are modelled as follows:
p(oC =cue [dir]|sL =cue location,sR =reward on [dir])=1,
p(oC =cue [dir]|sL ∈SL\{cue location},sR)=0.5.
Thisimpliesthatthecueobservationiscompletelyinformativeaboutthereward
condition when the agent is at the cue location, and otherwise independent of
the actual reward condition.
10
Transition dynamics kernel p(s |s ,a )
τ+1 τ τ
We continue by describing the transition dynamics kernel p(s |s ,a ).
τ+1 τ τ
p(sL =[loc]|sL ∈{center,cue location},sR,a =go to [loc])=1,
τ+1 τ τ τ
p(sL =[dir] arm|sL =[dir] arm,sR,a )=1.
τ+1 τ τ τ
This implies that if the agent is in center or cue location it will be in the
locationspecified by the actionin the next time step. If it is in one of the arms
however,it will stay there, independent of the choice of action a .
τ
p(sR =reward on [dir]|sR =reward on [dir],sL,a )=1.
τ+1 τ τ τ
Thisimpliesthattherewardconditionstaysconstantthroughoutthetrajectory.
Preference distribution p and prior over states p
C D
Theunnormalizedpreferencedistributionp canbechosentofavorobservations
C
with reward and discourage observations with loss as follows:
p (([loc],no reward,cue [dir]))=2,
C
p (([loc],reward,cue [dir]))=3,
C
p (([loc],loss,cue [dir]))=1.
C
Finally we let the prior belief over states p be uniform.
D
Action selection procedure
We will now simulate the trajectory of an agent acting according to active
inference. Wesetthetime horizontoT =3,whichimpliesthatthepolicieswill
have length 2.
Time step 1
The agent starts by receiving an observation o = (center, no reward, cue
1
right). It now updates its beliefs about the current state such that
q (sL =center,sR =reward on [dir])=0.5.
t 1 1
Subsequently it computes its beliefs about future states and observations given
a policy using equation (7), (8), (9). For example for π∗ = (move to cue
1
location,move to left arm) we have
q (sL =cue location,sR =reward on [dir]|π =π∗)=0.5, (22)
1 2 2 1 1
q (sL =left arm,sR =reward on [dir]|π =π∗)=0.5,
1 3 3 1 1
11
and
q (oL =cue location,oR =no reward,oC =cue [dir]|π =π∗)=0.5,
1 2 2 2 1 1
q (oL =left arm,oR =reward,oC =cue [dir]|π =π∗)=0.25,
1 3 3 3 1 1
q (oL =left arm,oR =loss,oC =cue [dir]|π =π∗)=0.25,
1 3 3 3 1 1
and for example for o∗ = (cue location, no reward, cue left) we have
2
q (sL =cue location,sR =reward on left|o =o∗,π =π∗)=1.(23)
1 2 2 2 2 1 1
Noteherethereductionofuncertaintyabouts duetotheobservationo∗,repre-
2 2
sentedbythe epistemic valuedefined inSection2.2givenby the KL divergence
between the distributions (23) and (22).
The agent now computes G and plugs this into equation (1) and gets the fol-
lowing posterior distribution over policies:
a →
2 center right arm left arm cue location
a ↓
1
center 0.022 0.041 0.041 0.046
right arm 0.041 0.075 0.075 0.083
left arm 0.041 0.075 0.075 0.083
cue location 0.046 0.083 0.083 0.091
andinthisscenarioitsamplesapolicywithasfirstactionmove to cue location
with highest probability.
Time step 2
After having performed action a∗ = move to cue location, the next obser-
1
vation it receives is o∗ = (cue location, no reward, cue right). Its belief
2
about the current state is now given by
q (sL =cue location,sR =reward on right)=1.
2 2 2
For instance, when π∗ = (move to left arm), the beliefs about future states
2
are
q (sL =left arm,sR =reward on right|π =π∗)=1,
2 3 3 2 2
and the observation beliefs
q (oL =left arm,oR =reward,oC =cue [dir]|π =π∗)=0.01,
2 3 3 3 2 2
q (oL =left arm,oR =loss,oC =cue [dir]|π =π∗)=0.49.
2 3 3 3 2 2
Since all uncertainty has already been taken away by the last observation, con-
ditioning ono∗ = (left arm,reward,cue left)willmakeno difference to the
3
belief about the state s , i.e.
3
q (sL =left arm,sR =reward on right|o =o∗,π =π∗)=
2 3 3 3 3 2 2
q (sL =left arm,sR =reward on right|π =π∗)=1,
2 3 3 2 2
12
which will cause the epistemic value term in G to be zero.
The agent now calculates G again and obtains the following distribution over
policies:
a center right arm left arm cue location
2 ,
0.20 0.52 0.08 0.20
andwillthensamplethe policy(move to right arm)withhighestprobability.
Acknowledgements
TheauthorswouldliketothankThomasParr,ConorHeins,RyanSmith,Beren
Millidge, Pablo Lanillos, Sean Tull, Stephen Mann, Pradeep Kumar Banerjee,
Frank Ro¨der and Lance Da Costa for helpful discussions and comments and
acknowledgethe supportofthe Deutsche ForschungsgemeinschaftPriorityPro-
gramme “The Active Self” (SPP 2134).
References
[1] Christopher M Bishop and Nasser M Nasrabadi. Pattern recognition and
machine learning. Vol. 4. 4. Springer, 2006.
[2] David M Blei, Alp Kucukelbir, and Jon D McAuliffe. “Variational infer-
ence: A review for statisticians”. In: Journal of the American statistical
Association 112.518 (2017), pp. 859–877.
[3] Lancelot Da Costa et al. “Active inference as a model of agency”. In:
arXiv preprint arXiv:2401.12917 (2024).
[4] Lancelot Da Costa et al. “Active inference on discrete state-spaces: A
synthesis”. In: Journal of Mathematical Psychology 99 (2020), p. 102447.
issn: 0022-2496.doi: https://doi.org/10.1016/j.jmp.2020.102447.
[5] ConorHeins.ActiveInferenceDemo:T-MazeEnvironment.https://pymdp-rtd.readthedocs.io/en/la
[6] Conor Heins et al. “pymdp: A Python library for active inference in dis-
cretestate spaces”.In: The Journal of Open Source Software 7.73(2022),
p. 4098.
[7] BerenMillidge,AlexanderTschantz,andChristopherLBuckley.“Whence
the expected free energy?” In: Neural Computation 33.2 (2021), pp. 447–
482.
[8] ThomasParr,GiovanniPezzulo,andKarlJ Friston. Active inference: the
free energy principle in mind, brain, and behavior. MIT Press, 2022.
[9] Ryan Smith, Karl J Friston, and Christopher J Whyte. “A step-by-step
tutorial on active inference and its application to empirical data”. In:
Journal of mathematical psychology 107 (2022), p. 102632.
[10] Ran Wei. “Value of Information and Reward Specification in Active In-
ference and POMDPs”. In: arXiv preprint arXiv:2408.06542 (2024).
13
A Variational free energy minimization
Preliminaries
Inthissectionweusexforageneralobservedvariableandz foragenerallatent
variable.
Let p(x,z) = p(x|z)p(z) be a generative model. In order to perform inference
over the latent variables after making an observation x, one has to compute
the posterior p(z|x). This posterior is often hard to compute directly. One can
instead approximate this posterior by finding the distribution q (z) in a family
x
of distributions Q that minimizes the following function:
F(q˜|x)= q˜(z)(lnq˜(z)−lnp(x,z)),
z
X
calledthevariationalfreeenergy. Notethatwedistinguishnotationallyq˜,which
is a genericelement of Q anda variable in F, and q , which is the minimizer of
x
F for a fixed x, i.e.
q =argminF(q˜|x).
x
q˜∈Q
Note that when Q is large enough, for example when z is discrete and Q is the
set of all probability distributions over z, then the minimizer of the free energy
is equal to the exact posterior distribution and we have
q (z)=p(z|x).
x
We can replace lnp(x,z) in F by a general function f, i.e.
F (q˜|x)= q˜(z)(lnq˜(z)−f(x,z)).
f
z
X
The minimizer can be found by substituting g(x,z)=ef(x,z) as follows:
F (q˜,x)= q˜(z)(lnq˜(z)−lng(x,z))
f
z
X
g(x,z)
= q˜(z) lnq˜(z)−ln +ln g(x,z′).
g(x,z′)
z (cid:18) z′ (cid:19)! z′
X X
P
If Q is again large enough, the minimizer is given by:
g(x,z)
q (z)=
x
g(x,z)
z
ef(x,z)
P
=
ef(x,z)
z
=σ(f(x,z)), (24)
P
14
where σ is the softmax function defined in (48) and in the case that f(x,z) =
lnp(x,z) we have
q (z)=σ(lnp(x,z))
x
=p(z|x).
The variational free energy can be written as follows:
F(q˜|x)=D (q˜(z)kp(z|x))−lnp(x)
KL
≥−lnp(x),
whichshowsthatthenegativeofthevariationalfreeenergyisalowerboundon
theevidence(ELBO).ByminimizingF w.r.t.q˜wegetthefollowingapproximate
equality (equation (B.2) in [8]):
F(q |x)≈−lnp(x). (25)
x
Variational free energy minimization in active inference
Active inference adopts the perspective that perception, action selection and
learning can be interpreted as minimizing one single variational free energy
function F. In its complete form it can be written as follows:
F(q˜|o ,a )=E [lnq˜(s ,θ,π )−lnp(s ,o ,θ,π |a )].
1:t 1:t−1 q˜(s1:T,θ,πt) 1:T t 1:T 1:t t 1:t−1
The distributions in the family Q are assumed to factorize as follows:
T
q(s ,θ,π)=q(θD)q(θA)q(θB)q(π) q(s ),
1:T τ
τ=1
Y
which is sometimes referredto as the mean-fieldapproximation. Now F can be
written as follows:
T
F(q˜|o ,a )=E lnq˜(π )+lnq˜(θ)+ lnq˜(s )−lnp(π )
1:t 1:t−1 q˜(s1:T,θ,πt) t τ t
(cid:20) τ=1
X
t T
−lnp(θ)−lnp(s |θD)− lnp(o |s ,θA)− lnp(s |s ,a ,θB) ,
1 τ τ τ τ−1 τ−1
τ=1 τ=2 (cid:21)
X X
(26)
where we use a in the last term for elements of both a and π .
τ−1 1:t−1 t
Perception
For studying perception, equation (26) can be rewritten as follows:
F(q˜|o ,a )=E F (q˜|o ,a ) +C
1:t 1:t−1 q˜(πt) πt 1:t 1:t−1 \s
(cid:2) (cid:3)
15
where C is independent of q˜(s ) and
\s 1:T
F (q˜|o ,a )=
πt 1:t 1:t−1
E lnq˜(s )−E [lnp(s ,o |θ,a ,π )] . (27)
q˜(s1:T) 1:T q˜(θ) 1:T 1:t 1:t−1 t
(cid:2) (cid:3)
Perception according to active inference is minimizing F w.r.t. q˜(s ). The
πt 1:T
minimizer is written q (s |π )=q(s |π ,o ,a ).
t 1:T t 1:T t 1:t 1:t−1
We can use the factorizing properties to rewrite (27) as follows:
T
F (q˜|o ,a )= E [lnq˜(s )]−E [lnp(s |θD)]
πt 1:t 1:t−1 q˜(sτ) τ q˜(s1,θD) 1
τ=1
X
t T
− E p(o |s ,θA)− E p(s |s ,a ,θB),
q˜(sτ,θA) τ τ q˜(sτ,sτ−1,θB) τ τ−1 τ
τ=1 τ=2
X X
which is equivalent to equation (6) in [4].10 One can also get rid of the expec-
tations over θ by replacing them by an estimator θˆ. Then we get
T
F (q˜|o ,a )= E [lnq˜(s )]−E [lnp(s |θˆD)]
πt 1:t 1:t−1 q˜(sτ) τ q˜(s1) 1
τ=1
X
t T
− E p(o |s ,θˆA)− E p(s |s ,a ,θˆB),
q˜(sτ) τ τ q˜(sτ,sτ−1) τ τ−1 τ
τ=1 τ=2
X X
which is equivalent to (B.4) in [8].
Learning
Updating the θ parameter (learning)happens at the end of an episode (t=T).
The agenthas observedo andperformeda . The variationalfree energy
1:T 1:T−1
from equation (26) can be written as follows:
F(q˜|o ,a )=
1:T 1:T−1
E lnq˜(θ)−E [lnp(o ,s ,θ|a )] +C , (28)
q˜(θ) qT(s1:T) 1:T 1:T 1:T−1 \θ
(cid:2) (cid:3)
where C is independent of q˜(θ) and we have fixed q (s ) to be the approxi-
\θ T 1:T
mate posteriorover states, inferredusing the current (not-updated) belief p(θ).
Ifthecurrentbeliefp(θ)isaDirichletdistributionwithhyperparameterα,then
the minimizer q (θ) ofF willalsobe a Dirichletdistributionwith hyperparam-
T
eter α′ as given in (18)–(20). In Appendix B we derive this, and relate it to a
well known variational inference algorithm called coordinate ascent variational
inference (CAVI).
10Note that in [4] only the parameter θA is treated as a variable and θD and θB are
consideredfixed.
16
Action selection
Finally we can also view action selection as the minimization of the variational
free energy function (26). We can rewrite this function as follows:
F(q˜|o )=E lnq˜(π)−lnp(π)+E [lnq˜(s )−lnp(o ,s |π)] +C
1:t q˜(π) q˜(s1:T) 1:T 1:t 1:T \π
(cid:2) (cid:3)
=E [lnq˜(π)−lnp(π)+F (q˜(s )|o )]+C ,
q˜(π) π 1:T 1:t \π
where F is defined in (27), C is independent of q˜(π), and we replaced the
π \π
expectation over q˜(θ) by an estimator θˆ and suppress the dependence of the
generative model p on θˆin the notation. (This is equivalent to the second line
in equation (B.7) in [8].) What is important to note here, is that the agent is
trying to infer an action sequence (policy) π of both future and past actions.
We have therefore dropped the dependence on a in both F and F , and
1:t−1 π
instead π is a sequence of action starting at τ = 1 instead of τ =t. In Section
2, we alwaysfixedthe pastactions to the actions thatwere actuallyperformed,
which is no longer the case here.
We minimizeF w.r.t.q˜(π) andq˜(s )andusing(24)wegetforthe minimizers
1:T
respectively
q (π)=σ(−lnp(π)+F (q (s )|o )), (29)
t π t 1:T 1:t
and q (s ) is the minimizer of F . Now we can use
t 1:T π
p(π)=σ(lnE(π)−G(π |o ,a )),
t 1:t 1:t−1
which corresponds to the last line equation (B.7) in [8].11 The term E(π) is a
habit term, signifying what policies the agent is usually exercising. Plugging
this back into (29) gives
q (π)=σ(−lnE(π)+G(π |o ,a )+F (q |o )), (30)
t t 1:t 1:t−1 π t 1:t
which corresponds to equation (B.9) in [8].
Remark 2. We nowtry to interpretthis derivationconceptually. Note that due
to (25) we have the following approximate equality:
F(q˜|o )=E [lnq˜(π)−lnp(π)+F (q˜(s ),o )]+C
1:t q˜(π) π 1:T 1:t \π
≈E [lnq˜(π)−lnp(π)−lnp(o |π)]+C
q˜(π) 1:t \π
=E [lnq˜(π)−lnp(o ,π)]+C ,
q˜(π) 1:t \π
which implies that the minimizer q (π) is approximately equal to the posterior
t
p(π|o ). Inotherwords,thissaysthatweselectthepolicythatismostprobable
1:t
giventhepastobservations. Thatis,the agentforgetswhichpastactionsithas
11It can be argued that calling this a prior is incorrect, since it actually depends on the
observationso 1:t.
17
performed, and tries to infer these based on the past observations. Then it
tries to find the most likely sequence of future actions to go with this sequence
of past actions. Selecting future actions in this way however only makes sense
when certain past action sequences make certain future action sequences more
likely. For example, let our action space consist of two actions {left,right}
and policies consist of sequences of actions of length two. Now suppose that
the prior over policies dictates that the agent almost certainly performs the
policies (left,left) or (right,right). This implies that having inferred the
first actiongives the agent more informationabout the most likely next action.
However,ifbothnextactionsareequallylikelygivenafirstaction,accordingto
theprior,thenthelikelihoodtermp(o |π)doesnothaveanyinformationabout
1:t
the next action. Note that in the calculation of G the past actions are fixed to
theactionsthathavebeenperformed. ThereforeGwillnotmakecertainfuture
actionsequencesmorelikelybasedonpossiblepastactionsequences. Therefore,
thetermF in(30)onlybecomesrelevantwhenthehabittermE makescertain
π
future action sequences more likely based on past action sequences.
B Learning Preliminaries
Bayesian belief updating
The learning process of an active inference agent is formulated as Bayesian
belief updating over the parameters. In general, Bayesian belief updating can
be described as follows. Let θ be the parameter of a model p we want to learn
θ
and x the output of this model. We start with a prior belief p(θ|α) which is
parametrizedbythehyperparameterα. Nowourposteriorbeliefaboutθisgiven
by the distribution p(θ|x,α) which is obtained by Bayes’ rule. In some special
cases12, the posterior distribution belongs to the same parametrized family as
the prior, such that p(θ|x,α) = p(θ|α′). Then the learning can be summarized
by the update from α to α′.
Categorical model without latent variables
Nowweletthemodelbeacategoricaldistributionoverelements{x(1),...,x(n)}
parametrized by θ =(θ ,...,θ ). That is,
1 n
p(x(i)|θ)=θ , ∀i∈{1,...,n}.
i
ThepriorovertheparameterθisgivenbytheDirichletdistributionparametrized
by the hyperparameter α=(α ,...,α ). That is,
1 n
p(θ|α)∝ θαi−1.
i
i
Y
12Fordetails,seethetheoryofconjugatepriors.
18
After observing x∗, the posterior is given by
p(θ|x∗,α)∝p(x∗|θ)p(θ|α)
= θ
αi−1+
i
Y
1
x(i)(x∗)
.
i
Note that this is again a Dirichlet distribution with hyperparameter α′ such
that
α′ =α +
i i
1
(x∗), ∀i∈{1,...,n}. (31)
x(i)
That is, the hyperparameter corresponding to the observation x∗ is increased
byone. Thiswillmakethisobservationmorelikelyintheupdateddistribution.
Categorical model with latent variables
Exact posterior
Now we let the model be a joint distribution over the product space of obser-
vations x and latent states z, given by {x(1),...,x(n)}×{z(1),...,z(m)}. The
model is parametrized by θ =(θD,θA) as follows:
p(z(j)|θD)=θD, (32)
j
p(x(i)|z(j),θA)=θA. (33)
ij
The prior over the θ is defined as follows:
p(θ|α)=p(θD|αD) p(θA|αA) (34)
•j
j
Y
p(θD|αD)∝ θD αD j −1 , (35)
j
j
Y(cid:0) (cid:1)
p(θA|αA)∝ θA αA ij −1 , (36)
•j ij
i
Y(cid:0) (cid:1)
where θ denotes the vector (θ ,...,θ ). After observing x∗ = x(i∗), the
•j 1j nj
exact posterior is given by
p(θ|x∗,α)∝p(x∗|θ)p(θ|α)
= p(x∗|z(j),θA)p(z(j)|θD)p(θA|αA)p(θD|αD)
j
X
∝ θA θD θA αA i′j′−1 θD αD j′′−1
i∗j j  i′j′  j′′ 
j j′ i′ j′′
X YY(cid:0) (cid:1) Y(cid:0) (cid:1)
  
= θA αA i′j′−1+
 i′j′
j j′ i′
X YY(cid:0) (cid:1)

1 i∗(i′) 1 j(j′) θD αD j′′−1+
 j′′
j′′
Y(cid:0) (cid:1)

1 j(j′′) .


19
Note that this is no longer a Dirichlet distribution. Below we discuss how
the Dirichlet distribution shows up in an algorithm for approximating the true
posterior.
Mean-field approximation
We can also approximatethe posteriorover θ by minimizing the following vari-
ational free energy function:
F(q˜|x∗)=E [lnq˜(z,θ)−lnp(x∗,z,θ|α)], (37)
q˜(z,θ)
where we assume the approximate posterior over both θ and z factorizes as
follows:
q(z,θ)=q(z)q(θ).
The coordinate ascent variational inference (CAVI) algorithm [2, 1] updates
the distributions q(z) and q(θ) iteratively, each time holding one distribution
fixedwhileupdatingtheother. Morespecifically,wecaninitializeq(θ)withour
prior belief p(θ|α) and minimize F w.r.t. q˜(z). The variational free energy now
becomes
F(q˜|x∗,q(θ))=E lnq˜(z)−E [lnp(z|x∗,θ)] +C , (38)
q˜(z) q(θ) \z
where C
\z
is independent of q˜(z)(cid:2). The minimizer q(z) is pro(cid:3)portional to
q(z)∝exp E [lnp(z|x∗,θ)] . (39)
q(θ)
(See equation (24).) We then fix th(cid:0)is q(z) and optimi(cid:1)ze F w.r.t. q˜(θ) and get
F(q˜|x∗,q(z))=E lnq˜(θ)−E [lnp(x∗,z,θ|α)] +C , (40)
q˜(θ) q(z) \θ
where C
\θ
is independent of q˜(θ(cid:2)). The minimizer q(θ) is prop(cid:3)ortional to
q(θ)∝exp E [lnp(x∗,z,θ|α)] . (41)
q(z)
(cid:0) (cid:1)
These two steps are performed iteratively until the beliefs about θ and z have
converged.
Note that when p(x,z,θ|α) is a categorical model with Dirichlet priors, as de-
fined in (32)–(36), we can rewrite equation (41) as follows:
q(θ)∝exp E [lnp(x∗,z,θ|α)]
q(z)
=exp(cid:0) E
q(z)
lnp(x∗|z,θA)+(cid:1) lnp(z|θD)+lnp(θ|α)
(cid:0) (cid:2) (cid:3)(cid:1)
=exp q(z(j)) lnp(x∗|z(j),θA)+lnp(z(j)|θD) p(θ|α)
 
X j h i
=  θA q(z(j)) θD q(z(j)) θD αD j −1 θA αA ij −1 
i∗j j j ij
j i
Y(cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1) Y(cid:0) (cid:1)
= θD αD j +q(z(j))−1 θA αA ij +
j ij
j i
Y(cid:0) (cid:1) Y(cid:0) (cid:1)
1 i∗(i)q(z(j))−1
20
Note that this is again a Dirichlet distribution with updated parameter α′ =
(αD′ ,αA′
) given by
αD′ =αD+q(z(j)), (42)
j j
αA′ =αA +
ij ij 1
(i)q(z(j)). (43)
i∗
Update (42) makes latent states with high q(z) more likely in the updated dis-
tribution. Update(43)makessurethatlatentstatesaremorelikelytogenerate
the observation x(i∗), especially those with high q(z). Note the similarity with
the update rule (31) for categorical models without latent variables.
POMDP model
Nowweletpbethegenerativemodelfromanactiveinferenceagentasdescribed
in (11)–(17). Similar to (37) the variational free energy now becomes
F(q˜|o ,a )=
1:T 1:T−1
E [lnq˜(s ,θ)−lnp(o ,s ,θ|a ,α)]. (44)
q˜(s1:T,θ) 1:T 1:T 1:T 1:T−1
We use the mean-field approximation q(s ,θ) = q(s )q(θ). Equivalent to
1:T 1:T
(38)–(39) we can start by minimizing F w.r.t. q˜(s ) and get a minimizer
1:T
q (s )usingthe currentbeliefaboutθ. Then,similarto(40)–(41)wecanuse
T 1:T
this minimizer to write F as follows:
F(q˜|o ,a ,q (s ))=
1:T 1:T−1 T 1:T
E lnq˜(θ)−E [lnp(o ,s ,θ|a ,α)] +C , (45)
q˜(θ) qT(s1:T) 1:T 1:T 1:T−1 \θ
(cid:2) (cid:3)
21
and work out the minimizer. This gives
q(θ)∝exp E [lnp(o ,s ,θ|a ,α)]
qT(s1:T) 1:T 1:T 1:T−1
T
(cid:0) (cid:1)
=exp E lnp(o |s ,θA)+lnp(s |θD)
qT(s1:T) τ τ 1
"
τ=1
X
T
+ lnp(s |s ,a ,θB) p(θ|α)
τ τ−1 τ−1
#!
τ=2
X
T
=exp q (s ) lnp(o |s ,θA)+lnp(s |θD)
T 1:T τ τ 1
"
s X1:T τ X =1
T
+ lnp(s |s ,a ,θB) p(θ|α)
τ τ−1 τ−1
#!
τ=2
X
= θD qT(s 1 (j)) θA PT τ=1
j ij
j i
Y(cid:0) (cid:1) Y(cid:0) (cid:1)
1 o(i)(oτ)qT(s τ (j))
θB PT τ=2 qT(s τ (j))qT(s τ (k − ) 1 )
jkl
k
Y(cid:0) (cid:1)
1 a(l)(aτ−1) p(θ|α)
= θD αD j +qT(s 1 (j))−1 θA αA ij +PT τ=1
j ij
j i
Y(cid:0) (cid:1) Y(cid:0) (cid:1)
1 o(i)(oτ)qT(s τ (j))−1
θB αB jkl +PT τ=2 qT(s τ (j))qT(s τ (k − ) 1 )
jkl
k
Y(cid:0) (cid:1)
1 a(l)(aτ−1)−1 .
This gives the update rules for α given in (18)–(20).
Remark 3. Note that these update rules are actually just the first iteration of
the CAVI algorithm described above. It will therefore in general not minimize
the variational free energy in equation (44). Instead it minimizes the quantity
given in equation (28) and (45) where q (s ) is fixed. Note however that if
T 1:T
one would assume q (s ) to be given, the following variational free energy
T 1:T
would be the natural choice to minimize:
E lnq˜(θ)−ln E [p(o ,θ|s ,a ,α)] ,
q˜(θ) qT(s1:T) 1:T 1:T 1:T−1
(cid:2) (cid:0) (cid:1)(cid:3)
since this has as minimizer the exact posterior distribution.
22
C Further details
Equivalent formulations of expected free energy
Recall that the expected free energy in equation (2) is given by
G(π |o ,a )=− E D q (s |o ,π )kq (s |π )
t 1:t 1:t−1 qt(ot+1:T|πt) KL t t+1:T t+1:T t t t+1:T t
(cid:18) h (cid:16) (cid:17)i
+E lnp (o ) .
qt(ot+1:T|πt) C t+1:T
h i(cid:19)
We canderive the equivalent formulationfrom equation(3) as follows. We first
expand the KL divergence term to get
G(π |o ,a )=E lnq (s |π )−lnq (s |o ,π )
t 1:t 1:t−1 qt(st+1:T,ot+1:T|πt) t t+1:T t t t+1:T t+1:T t
h
−lnp (o ) .
C t+1:T
i (46)
Using Bayes’ rule we can rewrite
−lnq (s |o ,π )=−lnq (o |s ,π )−lnq (s |π )
t t+1:T t+1:T t t t+1:T t+1:T t t t+1:T t
+lnq (o |π ).
t t+1:T t
Plugging this into (46) and using that q (o |s ,π ) = p(o |s )
t t+1:T t+1:T t t+1:T t+1:T
we get
G(π |o ,a )=E lnp(o |s )+lnq (o |π )
t 1:t 1:t−1 qt(st+1:T,ot+1:T|πt) t+1:T t+1:T t t+1:T t
h
−lnp (o )
C t+1:T
i
= E H p(o |s )
qt(st+1:T|πt) t+1:T t+1:T
h (cid:2) (cid:3)i
+D q (o |π )kp (o ) ,
KL t t+1:T t C t+1:T
(cid:16) (cid:17)
where the last line is equal to equation (3).
Independence between state factors and observation modalities
Inordertomakethecomputationofstateinferencemoreefficient,theagentcan
use independencies between different state factors and observation modalities
(different dimensions of state and observation space). More specifically, we
assumethatgivenastate,the differentobservationmodalities areindependent,
which translates to:
p(o |s )= p(om|s ).
τ τ τ τ
m
Y
23
We use superscript m and f to denote a specific observation modalities and
state factors respectively. Furthermore, we assume a certain state factor to be
independent of allother state factors in the same and previoustime step, given
the same state factor in the previous time step and the last action, i.e.:
p(s |s ,a )= p(sf|sf ,a ).
τ τ−1 τ−1 τ τ−1 τ−1
f
Y
For a fixed state factor f equation (6) and (7) now become
q (sf)∝p(o |sf) p(sf|sf ,a )q (sf ) (47)
t t t t t t−1 t−1 t−1 t−1
sX f
t−1
q (sf|a )= p(sf|sf ,a )q (sf |a ),
t τ 1:τ−1 τ τ−1 τ−1 t τ−1 1:τ−2
sX f
τ−1
and equation (10) becomes
t T
q (s |π )∝ p(om|s ) p(s ) p(sf|sf ,a ).
t 1:T t τ τ 1 τ τ−1 τ−1
τ=1 m τ=2 f
YY YY
Fixed point iteration
Equation (47) involves the distribution p(o |sf). We can however not access
t t
this directly. To find an approximate solution, we can use fixed point iteration
as follows:
q(i+1)(sf)∝ q(i)(s\f )p(o |s ) p(sf|sf ,a )q (sf ),
t t t t t t t t−1 t−1 t−1 t−1
Xs\
t
f sX f
t−1
where \f denotes the set of all state factors apart from f.
Softmax function
Definition 1. LetS ={x(1),...,x(n)}beafinitesetandµ:S →Rafunction.
The softmax function σ is given by
eµ(x(i))
σ(µ(x(i)))= . (48)
eµ(x(j))
j
P
Notethatσ(µ(x(i)))>1and σ(µ(x(i)))=1. Thereforethesoftmaxfunction
S
can be used to turn µ into a probability distribution.
P
24

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "A Concise Mathematical Description of Active Inference in Discrete Time"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
