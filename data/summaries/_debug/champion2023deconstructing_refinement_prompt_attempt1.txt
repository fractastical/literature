=== IMPORTANT: ISOLATE THIS PAPER ===
You are revising a summary for ONLY the paper below. Do NOT reference or use content from any other papers.
Paper Title: Deconstructing deep active inference
Citation Key: champion2023deconstructing
REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Issues to fix:
1. CRITICAL: The current summary has severe repetition issues. You MUST eliminate all repeated sentences, phrases, and paragraphs. Each idea should be expressed only once. If you find yourself repeating content, remove the duplicates entirely. Focus on variety and uniqueness in your wording.
2. Severe repetition detected: Same phrase appears 361 times (severe repetition)

Current draft (first 2000 chars):
Okay, let's begin.**Deconstructing deep active inference**This document summarizes the key findings and arguments presented in the paper "Deconstructing deep active inference" (Champion et al.,2023). The paper investigates the challenges associated with implementing deep active inference (DAI) and proposes a framework for addressing these challenges. The authors highlight the importance of carefully considering the epistemic value component of the DAI framework, which is often overlooked.**1. Overview**The authors begin by outlining the core concept of deep active inference, which is to model the agent’s perception of the world as a generative model. This generative model is used to predict the agent’s sensory inputs, and the difference between the predicted and actual inputs is used to update the agent’s internal state. The authors argue that this approach is fundamentally different from traditional reinforcement learning, where the agent learns a policy directly from trial and error. The authors state: "The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: 'The authors state: ...

Key terms: united, kingdom, chmm, university, computing, kent, deep, deconstructing

=== FULL PAPER TEXT ===
Deconstructing deep active inference.
Th´eophile Champion tmac3@kent.ac.uk
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Marek Grze´s m.grzes@kent.ac.uk
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Lisa Bonheme lb732@kent.ac.uk
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Howard Bowman H.Bowman@kent.ac.uk
University of Birmingham, School of Psychology,
Birmingham B15 2TT, United Kingdom
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
University College London, Wellcome Centre for Human Neuroimaging (honorary)
London WC1N 3AR, United Kingdom
Editor: TO BE FILLED
Abstract
Active inference is a theory of perception, learning and decision making, which can be applied to
neuroscience, robotics, psychology, and machine learning. Recently, intensive reasearch has been
taking place to scale up this framework using Monte-Carlo tree search and deep learning. The
end-goal of this activity is to solve more complicated tasks using deep active inference. First,
we review the existing literature, then, we progresively build a deep active inference agent as
follows: (i) implement a variational auto-encoder (VAE), (ii) implement a deep hidden Markov
model (HMM), (iii) implement a deep critical hidden Markov model (CHMM), and (iv) imple-
ment a complete deep active inference agent (DAI). For the CHMM and DAI agents, we have
experimented with five definitions of the expected free energy and three different action selection
strategies. According to our experiments, the models able to solve the dSprites environment are
the ones that maximise rewards. Finally, we compare the similarity of the representation learned
by the layers of various models (e.g., deep Q-network, CHMM, DAI) using centered kernel align-
ment. Importantly, the CHMM maximising reward and the CHMM minimising expected free
energy learn very similar representations except for the last layer of the critic network (reflect-
ing the difference in learning objective), and the variance layers of the transition and encoder
networks. While performing further inspection of those (variance) layers, we found that the tran-
sition network of the reward maximising CHMMis a lot more certain thanthe transition network
of the CHMM minimising expected free energy. More precisely, the CHMM minimising expected
free energy is only confident about the world transition when performing action down. This sug-
gests that the CHMM minimising expected free energy always picks the action down, and does
not gather enough data for the other actions. In contrast, the CHMM maximising reward, keeps
on selecting the actions left and right, enabling it to successfully solve the task. The only differ-
ence between those two CHMMs is the epistemic value, which aims to make the outputs of the
transition and encoder networks as close as possible. Thus, the CHMM minimising expected free
3202
yaM
8
]IA.sc[
2v81610.3032:viXra
Champion et al.
energy repeatedly picks a single action (down), and becomes an expert at predicting the future
when selecting this action. This effectively makes the KL divergence between the output of the
transition and encoder networks small. Additionally, when selecting the action down the average
reward is zero, while for all the other actions, the expected reward will be negative. Therefore, if
the CHMM has to stick to a single action to keep the KL divergence small, then the action down
is the most rewarding. Thus, the appropriate formulation of the epistemic value in deep active
inference remains an open question.
Keywords: Deep Learning, Active Inference, Bayesian Statistics, Free Energy Principle, Rein-
forcement Learning
1. Introduction
Activeinferenceisaunifiedframeworkforperception, learning, andplanningthathasemergedfromtheo-
reticalneuroscience(Costaetal,2020a;Championetal,2021,2022b,a,c). Thisframeworkhassuccessfully
explained a wide range of brain phenomena (Friston et al, 2016; Itti and Baldi, 2009; Schwartenbeck et al,
2018; FitzGerald et al, 2015), and has been applied to a large number of tasks in robotics and artificial
intelligence (Fountas et al, 2020; Pezzato et al, 2020; Sancaktar et al, 2020; C¸atal et al, 2020; Cullen et al,
2018; Millidge, 2019).
A promising area of research revolves around scaling up this theoretical framework to tackle in-
creasingly complex tasks. Research towards this goal is generally driven from recent advances in machine
learning. Forexample,variationalauto-encoders(Doersch,2016;Higginsetal,2017;KingmaandWelling,
2014; Rezende et al, 2014) have been key to the integration of deep neural networks within active infer-
ence (Sancaktar et al, 2020; C¸atal et al, 2020; Millidge, 2020), and the Monte Carlo tree search algorithm
(Browne et al, 2012; Silver et al, 2016) has been used to improve planning efficency (Fountas et al, 2020;
Champion et al, 2022b,a,c,d).
Another closely related field is reinforcement learning (Mnih et al, 2013; van Hasselt et al, 2015;
Lample and Chaplot, 2016), which addresses the same kind of tasks, where an agent must interact with
its environment. A known challenge in this field is the correlation between the consecutive samples,
which violates the standard i.i.d. assumption on which most of machine learning relies. To break this
correlation, researchers proposed to store past experiences of the agent inside a replay buffer (Mnih et al,
2013). Experiences can then be re-sampled randomly from the buffer to train the Q-network, which is
used to approximate Q-values. The Q-network is trained to minimize the mean squared error between its
output and a target value, which is defined as:
(cid:104) (cid:105)
y(o ,a ) = E r +γ max Q (o ,a ) ,
t t ot+1∼E(ot,at) t
at+1∈A
θa t+1 t+1
where t is the present time step, A is the set of available actions, y(o ,a ) is the target Q-value to be
t t
predicted, E is the expectation w.r.t the observations received from the environment, r is the reward
t
obtained by the agent when performing action a in state1 o , o is the state reached when performing
t t t+1
action a in state o , E is the environment emulator from which o is sampled, γ is the discount factor
t t t+1
that discounts future rewards, and Q (o ,a ) is the output of the Q-network, i.e., the estimated
θa t+1 t+1
Q-value of performing action a in state o .
t+1 t+1
Unfortunatly,usingtheabovetargettotraintheQ-networkcanmakethetrainingunstable. Generally,
the problem is addressed by introducing a target network Qˆ (o ,a ), which is simply a copy of the
θˆ
a
t+1 t+1
Q-network. The weights of the target network are then synchronized with the weights of the Q-network
1. Note,weareusingthenotationo forthe(observable)stateat(anarbitrary)timestepτ,insteadofthemorestandard
τ
notation s . This is because we reserve the notation s for the (unobserved) states that arise in the context of active
τ τ
inference.
2
Deconstructing deep active inference.
every K (learning) iterations (Mnih et al, 2013). The new target is obtained by replacing the Q-network
by the target network, i.e.,
(cid:104) (cid:105)
y(o ,a ) = E r +γ max Qˆ (o ,a ) .
t t ot+1∼E(ot,at) t
at+1∈A
θˆ
a
t+1 t+1
In Section 2, we review the existing literature and present: the Deep Q-network (DQN) agent (Mnih et al,
2013), the deep active inference with Monte-Carlo methods (DAI ) agent by Fountas et al (2020), the
MC
deep active inference as variational policy gradients (DAI ) approach by Millidge (2020), the deep
VPG
active inference agent of rubber hand illusion (DAI ) by Rood et al (2020), the deep active inference
RHI
agent for humanoid robot control (DAI ) by Sancaktar et al (2020); Lanillos et al (2020); Oliver
HR
et al (2019), the deep active inference agent based on the free action objective (DAI ) by Ueltzh¨offer
FA
(2018), a deep active inference agent for partially observable Markov decision processes (DAI )
POMDP
by van der Himst and Lanillos (2020), as well as various methods for which the code is not available
online. We argue that while all these approaches illuminate important issues associated with realising a
deep active inference agent, a fully complete implementation has not yet been published. Consequently,
to systematically explore the construction of deep active inference agents, in Section 3, we incrementally
build such an agent. We start with a simple variational auto-encoder (VAE) composed of an encoder and
decoder network. Next, a transition network is added to create a deep hidden Markov model (HMM).
Then, a critic network is added to define a prior over actions, which leads to the critical HMM (CHMM).
Lastly,thepolicynetworkisaddedtoapproximatetheposterioroveractionsleadingtothefulldeepactive
inference (DAI) agent. Then, in Section 4, we discuss our findings regarding the abilities and limitations
of each intermediate step. This section also presents an analysis and discussion of the representations
learned by each intermediate model. Finally, Section 6 puts our findings in context and concludes this
paper.
2. Review of existing research
In this section, we discuss the DQN agent from the reinforcement learning literature, six agents from the
active inference literature for which the code is available online (DAI , DAI , DAI , DAI ,
MC VPG RHI HR
DAI , and DAI ), and a few other deep active inference agents for which the code is unavailable.
FA POMDP
Finally, we explain how the representations learned by the agents can be compared using centered kernel
alignment. Note, the notation used throughout this section is summarised in Appendix A.
2.1 DQN agent (Mnih et al, 2013)
Let us start with the DQN agent (Mnih et al, 2013), whose goal is to maximise the amount of reward
obtainedovertime. Ateachtimestepτ, theagentisobservinganimageo , andisallowedtoperformone
τ
action a ∈ A. After performing a when observing o , the agent receives a reward r . The Q-learning
τ τ τ τ
algorithm (Sutton et al, 1998) aims to maximise reward by computing the Q-values Q(o ,a ), for each
τ τ
state-action pair (o , a ). The Q-values represent the expected amount of rewards obtained by taking
τ τ
actiona instateo . ThisapproachisintractableforimagebaseddomainssuchasAtarigames, sinceone
τ τ
would need to store a vector of Q-values for each possible image. Instead, the DQN algorithm (illustrated
in Figure 1) has been developed, which uses a deep neural network Q to approximate the Q-values.
θa
More formally, Q maps any observation to a vector of size #A containing the Q-values of each possible
θa
action, and we denote by Q (o ,a ) the element at position a in the output vector predicted by Q
θa τ τ τ θa
when provided with the image o . As we discussed in the introduction, the training stability of the Q-
τ
networkisimprovedbyintroducingatargetnetworkQˆ , whichisstructurallyidenticaltotheQ-network
θˆ
a
and whose weights are synchronised with the weights of the Q-network every K (learning) iterations. The
Q-network’sweightsarethenoptimisedusinggradientdescenttominimisethemeansquareerrorbetween
3
Champion et al.
theoutputoftheQ-networkandatargetvalue,i.e.,θ
a
∗ = argmin
θa
MSE[Q
θa
(o
t
,•),y(o
t
,•)],wherey(o
t
,a
t
)
is the target Q-value for each state-action pair, and, as highlighted earlier, is defined as follows:
(cid:104) (cid:105)
y(o ,a ) = E r +γ max Qˆ (o ,a ) .
t t ot+1∼E(ot,at) t
at+1∈A
θˆ
a
t+1 t+1
o t+1 Qˆ θˆ
a
Qˆ θˆ
a
(o t+1 ,•) y(o t ,•)
r
t
MSE
γ
o t Q θa Q θa (o t ,•)
Figure 1: This figure illustrates the DQN agent. Briefly, the image o is fed into the Q-network, and
t
the image o is fed into the target network. The Q-network outputs the Q-values for each action at
t+1
time t, and the target network outputs the Q-values for each action at time t+1. Then, the reward, the
discount factor, and Q-values of each action at time t+1 are used to compute the target values y(o
t
,•).
Finally, the goal is to minimise the MSE between the prediction of the Q-network and the target values
by changing the weights of the Q-network.
2.2 DAI agent (Fountas et al, 2020)
MC
In this section, we review the DAI agent proposed by Fountas et al (2020), which represents the most
MC
ambitious and complete implementation of a deep active inference agent that accordingly adds important
new concepts to the field. The relevant code is available at the following URL: https://github.com/
zfountas/deep-active-inference-mc. TheDAI agentiscomposedoffourdeepneuralnetworks, as
MC
illustrated in Figures 2 and 3. The encoder E takes images as input, and outputs the mean and variance
φs
of the variational distribution over hidden states, i.e., Q (s ) = N(s ;µ,σ), where µ,σ = E (o ). The
φs t t φs t
decoder D takes a state as input, and outputs the parameters of a product of Bernoulli distributions,
θo
which can be interpreted as the expected (reconstructed) image oˆ, i.e.,
t
P (o |s ) = Bernoulli(o ;oˆ),
θo t t t t
where oˆ = D (s ) are the values predicted by the decoder, and Bernoulli(o ;oˆ) is a product of Bernoulli
t θo t t t
distributions defined as:
(cid:89)
Bernoulli(o ;oˆ) = Bernoulli(o [x,y];oˆ[x,y]),
t t t t
x,y
4
Deconstructing deep active inference.
oˆ oˆ
t t+1
Policy πˆ
Decoder Decoder
(cid:15)
sˆ sˆ
t t+1
˚µ
Transition ˚s t+1
(cid:15) µ lnσ ln˚σ (cid:15) µˆ lnσˆ
a
t
ω = a + d
t 1+exp(−b−Dt)
Encoder c Encoder
D = D [Q (a |s )||P(a )]
t KL φ t t t
a
o o
t t+1
Figure 2: This figure illustrates the DAI agent, which is composed of an encoder, a decoder, a
MC
transition network, and a policy network. The same VAE (encoder and decoder) is repeated in the figure
to reflect successive time-points.
whereBernoulli(•; •)isaBernoullidistributionoverthepossiblevaluesofthepixelo
t
[x,y],parameterized
by the parameter oˆ[x,y]. The transition network T takes a state-action pair as input, and outputs the
t θs
mean and variance of a Gaussian distribution over hidden states, i.e., P (s |s ,a ) = N(s ;˚µ, ˚σ ),
θs τ+1 τ τ τ+1 ωt
where ˚µ,˚σ = T (s ,a ), and ω is the top-down attention parameter modulating the precision of the
θs τ τ t
transitionmapping(seebelow). ThepolicynetworkP takesastateasinput, andoutputsadistribution
φa
over actions, i.e., Q (a |s ) = Cat(a ;πˆ), where πˆ = P (s ). Finally, the prior over actions is defined as
φa t t t φa t
follows:
(cid:88)
P(a ) = [π = a ]P(π), (1)
t t t
π∈Π
where Π is the set of all possible policies, π is the action precribed by policy π at time t, the square
t
brackets represent an indicator function that equals one if the condition within the bracket is satisfied
and zero otherwise, and P(π) is the prior over policies defined as:
P(π) = σ[−G(π)],
5
Champion et al.
where σ[•] is the softmax function, and G(π) is the expected free energy (EFE) of policy π, which is
defined as:
T T
(cid:88) (cid:88) (cid:104) (cid:105)
G(π) = G (π) = E lnQ(s ,θ|π)−lnP˜(o ,s ,θ|π) , (2)
τ Q˜ τ τ τ
τ=t τ=t
where Q˜ = Q(o ,s ,θ|π) = Q(θ|π)Q(s |θ,π)Q(o |s ,θ,π) is the predictive posterior, and P˜(o ,s ,θ|π) =
τ τ τ τ τ τ τ
P(θ|s ,o ,π)P(s |o ,π)P(o |π) is the target distribution. However, Equation 2 needs to be re-arranged
τ τ τ τ τ
to be computed in practice2, and Section 2.2.3 will present this derivation. Finally, as shown in Figure 2,
the top-down attention parameter is computed as follows:
a
ω = +d,
t 1+exp(−b−Dt)
c
where D = D [Q (a |s )||P(a )], and {a,b,c,d} are fixed hyperparameters. Intuitively, ω is high
t KL φa t t t t
when the posterior over actions (from the policy network) is close to the prior over actions (from the
expected free energy), and low when the posterior is far away from the prior. This, in turn, means
that extra uncertainty is introduced into the transition mapping (see paragraph before Equation 1) when
posterior over actions and prior over actions are very different. Finally, note that the number of terms
required to compute the prior over actions (defined in Equation 1) grows exponentially with the time
horizon of planning. Because this is intractable in practice, Fountas et al (2020) implemented a Monte-
Carlo tree search (MCTS) algorithm to evaluate the expected free energy of each action (see below).
Finally, action selection is performed by sampling from the following distribution:
N(sˆ,a )
P˜(a ) = t t ,
t (cid:80)
N(sˆ,aˆ )
aˆt t t
where sˆ is the current state of the environment, and N(s ,a ) is the number of times action a has been
t t t t
visited from state s during MCTS.
t
2.2.1 The Monte-Carlo tree search
Inthissection,wedescribetheplanningalgorithmusedbyDAI ,i.e.,Monte-Carlotreesearch(MCTS).
MC
MCTS is used to enhance the planning ability of the agent by allowing it to look into the future. At the
beginning of an action-perception cycle, the agent is provided with an image o . This image can be feed
t
intotheencodertogetthemeanvectorµoftheposterioroverthelatentstates,i.e.,Q (s ) = N(s ;µ,σ).
φs t t
Since µ is the mean of the Gaussian posterior, it can be interpreted as the maximum a posteriori (MAP)
estimate of the latent states at time step t. This MAP estimate will constitute the root node of the
Monte-Carlo tree search (MCTS).
The first step of the MCTS is to use the Upper Confidence bounds for Trees (UCT) criterion to
determine which node in the tree should be expanded. Let the tree’s root sˆ be called the current node,
t
which is denoted sˆ . If the current node has no children (i.e., no previously selected actions from the
τ
current node), then it is selected for expansion. Alternatively, the child with the highest UCT criterion
becomes the new current node and the process is iterated until we reach a leaf node (i.e. a node from
which no action has previously been selected). The UCT criterion (Browne et al, 2012) of the child of sˆ
τ
corresponding to action aˆ is given by:
τ
Q (a = aˆ |s = sˆ )
UCT(sˆ ,aˆ ) = −G¯(sˆ ,aˆ )+C · φa τ τ τ τ ,
τ τ τ τ explore
1+N(sˆ ,aˆ )
τ τ
2. By “in practice”, we mean “in the code” or equivalently “when implementing the approach”.
6
Deconstructing deep active inference.
Q (s )
φs τ
input: 64×64×#C
size: 31×31×32
kernel: 3
strides: (2,2)
ULeR
size: 15×15×32
kernel: 3
strides: (2,2)
ULeR
size: 31×31×32
kernel: 3
strides: (2,2)
ULeR
size: 7×7×64
kernel: 3
strides: (2,2)
ULeR
size: 3×3×64
kernel: 3
strides: (2,2)
ULeR
ULeR
size: 256
rate: 0.5
ULeR
size: 256
rate: 0.5
ULeR
P (o |s )
θo τ τ
input: #S
size: 256
rate: 0.5
size: #S + #S
ULeR
size: 256
rate: 0.5
ULeR
size: 256
rate: 0.5
ULeR
size: 256
rate: 0.5
ULeR
size: 256
rate: 0.5
size: 16×16×64
kernel: 3
strides: (2,2)
ULeR
size: 32×32×64
kernel: 3
strides: (2,2)
ULeR
size: 64×64×32
kernel: 3
strides: (2,2)
ULeR
P (s |s ,a )
θs τ+1 τ τ
input: #S + #A
size: 64×64×#C
kernel: 3
strides: (2,2)
ULeR
size: 512
rate: 0.5
ULeR
size: 512
rate: 0.5
ULeR
Q (a |s )
φa τ τ
input: #S
size: 512
rate: 0.5
size: #S + #S
ULeR
size: 128
ULeR
size: 128
size: #A
Figure 3: Neural network architectures of the DAI agent. Orange blocks correspond to convolutional
MC
layers, green blocks correspond to fully connected layers, blue blocks correspond to dropout, and yellow
blocks correspond to up-convolutional layers. For the dSprites environment, there are four actions (i.e.,
#A=4), tenstates(i.e., #S=10), andonlyonechannel(i.e., #C=1). FortheAnimal-AIenvironment,
there are three actions (i.e., #A = 3), ten states (i.e., #S = 10), and three channels (i.e., #C = 3). These
7
are all trained to minimize variational free energy.
Champion et al.
where G¯(sˆ ,aˆ ) is the average expected free energy of taking action aˆ in state sˆ , C is the explo-
τ τ τ τ explore
ration constant that modulates the amount of exploration at the tree level, N(sˆ ,aˆ ) is the number of
τ τ
times action aˆ was visited in state sˆ , and Q (a = aˆ |s = sˆ ) is the posterior probability of action aˆ
τ τ φa τ τ τ τ τ
in state sˆ as predicted by the policy network.
τ
Let˚s be the (leaf) node selected by the above selection procedure. The MCTS then expands one of
τ
the children of ˚s . The expansion uses the transition network to compute the mean ˚µ of P (s |s =
τ θs τ+1 τ
˚s ,a =˚a ), which is viewed as a MAP estimate of the states at time τ +1. Then, we need to estimate
τ τ τ
the cost of (virtually) taking action˚a . By definition, the cost is the expected free energy given by (2),
τ
and Monte-Carlo rollouts can be run to improve its estimation. The final step of the planning iteration is
to back-propagate the cost of the newly expanded (virtual) action toward the root of the tree. Formally,
we write the update as follows:
∀s ∈ A ∪{˚s }, G ← G +G , (3)
˚sτ τ s s ˚sτ
where˚s is the node that was selected for expansion, G is the expected free energy of s, and A is the
τ s ˚sτ
set of all ancestors of˚s in the tree. During the back propagation, we also update the number of visits as
τ
follows:
∀s ∈ A ∪{˚s }, N ← N +1. (4)
˚sτ τ s s
If we let Gaggr be the aggregated cost of an arbitrary node s obtained by applying Equation 3 after each
s
expansion, then we are now able to express G¯ formally as:
s
Gaggr
G¯ = s .
s
N
s
Importantly, if the node s corresponds to the state reached from state sˆ by performing action aˆ ,
τ τ
then G¯(sˆ ,aˆ ) = G¯ and N(sˆ ,aˆ ) = N . The planning procedure described above ends when the
τ τ s τ τ s
maximum number of planning iterations is reached, or when a clear winner has been identified, i.e.,
if max P(a ) − 1 > T where #A is the number of possible actions, and T is a (threshold)
at t #A dec dec
hyperparameter.
2.2.2 Derivation of the variational free energy
In this section, we provide a derivation for the variational free energy used by Fountas et al (2020).
This derivation was introduced in (Millidge, 2020), and can be adapted to derive the variational free
energy of the models presented in Section 3. Recall, the goal of the variational free energy as classically
presented is to make the approximate posterior Q (s ,a ) as close as possible to the true posterior3
φ t t
P(s ,a |o ,s ,a ), i.e.,
t t t t−1 t−1
Q∗(s ,a ) = argminD [Q (s ,a )||P(s ,a |o ,s ,a )].
φ t t KL φ t t t t t t−1 t−1
Q
φ
(st,at)
Using Bayes theorem, the linearity of expectation, and the fact that Q (s ,a ) integrates to one:
φ t t
Q∗(s ,a ) = argminD [Q (s ,a )||P(s ,a |o ,s ,a )]
φ t t KL φ t t t t t t−1 t−1
Q
φ
(st,at)
= argminD [Q (s ,a )||P(s ,a ,o ,s ,a )]+ lnP(o ,s ,a )
KL φ t t t t t t−1 t−1 t t−1 t−1
Q
φ
(st,at) (cid:124) (cid:123)(cid:122) (cid:125)
Constantw.r.tQ
φ
(st,at)
= argminD [Q (s ,a )||P(s ,a ,o ,s ,a )].
KL φ t t t t t t−1 t−1
Q
φ
(st,at)
3. Byposteriorwemeanaconditionaldistribution,wherethegivenvariablesarethoseforwhichweobserveaspecificvalue.
Note,thevalueofs isunknownbutcanbesampledfromtheposteriorovers (fromthepreviousaction-perception
t−1 t−1
cycle).
8
Deconstructing deep active inference.
Using the d-separation criteria (Koller and Friedman, 2009), it can be shown that:
P(s ,a ,o ,s ,a ) = P (o |s )P(a )P (s |s ,a )Q (s ,a ),
t t t t−1 t−1 θo t t t θs t t−1 t−1 φ t−1 t−1
where Q (s ,a ) is the variational posterior obtained through the inference process at the previous
φ t−1 t−1
time step. In the above equation, Q (s ,a ) was used to replace P(s ,a ), i.e., Q (s ,a )
φ t−1 t−1 t−1 t−1 φ t−1 t−1
was used as an empirical prior. Additionally, since Q (s ,a ) is a constant w.r.t Q (s ,a ), the above
φ t−1 t−1 φ t t
minimization problem reduces to:
Q∗(s ,a ) = argmin D [Q (s ,a )||P (o |s )P(a )P (s |s ,a )] (5)
φ t t KL φ t t θo t t t θs t t−1 t−1
Q
φ
(st,at) (cid:124) (cid:123)(cid:122) (cid:125)
variationalfreeenergy
(cid:104) (cid:105)
= argmin E D [Q (a |s )||P(a )] +D [Q (s )||P (s |s ,a )]
Q
φs
(st) KL φa t t t KL φs t θs t t−1 t−1
Q
φa
(at|st)Q
φs
(st)
(cid:104) (cid:105)
−E lnP (o |s ) .
Q
φs
(st) θo t t
BycomparingtheVFEin(5)andtheEFEin(2),onecanseeaninconsistency. Namely,theparametersare
seen as latent variables in the EFE definition, c.f., θ in (2), but they are regarded as parameters of neural
networks in the VFE, c.f., θ and θ in 5. Note, θ cannot be both a parameter (i.e, parameter, vector, or
s o
matrix) and a random variable, and if θ is a random variable, one must define its probability density, i.e.,
P(θ). Additionally, this inconsistency raises the question of whether the EFE is really the expectation of
the VFE. To sum up, the DAI agent is equipped with four deep neural networks modelling Q (a |s ),
MC φa t t
Q (s ), P (s |s ,a ), and P (o |s ). The weights of those networks are optimised using back-
φs t θs t t−1 t−1 θo t t
propagation to minimise the VFE given by (5). Note, (5) decomposes into two KL-divergence terms that
can be computed analytically, and the expectations can be approximated using a Monte-Carlo estimate.
Also, because P (o |s ) is modelled as a product of Bernoulli distributions, the logarithm of P (o |s )
θo t t θo t t
reduces to the binary cross entropy.
2.2.3 Independence assumptions and the expected free energy
The EFE as stated in Equation (2) needs to be re-arranged because it cannot be easily evaluated. We
therefore present the derivation proposed by Fountas et al (2020). Then, we highlight two independence
assumptions, i.e., s ⊥⊥ θ | π and s ⊥⊥ θ | π,o , used without explicitly presented proofs. Finally,
τ τ τ
we propose an alternative derivation that does not require these two assumptions and produces a sim-
pler result. Using the product rule of probability, one can see that Q(s ,θ|π) = Q(θ|s ,π)Q(s |π) and
τ τ τ
P˜(o ,s ,θ|π) = P(o |π)P(s |o ,π)P(θ|s ,o ,π). Using those two factorisations, the EFE given in (2),
τ τ τ τ τ τ τ
i.e.,
(cid:104) (cid:105)
G (π) = E lnQ(s ,θ|π)−lnP˜(o ,s ,θ|π) ,
τ Q˜ τ τ τ
where Q˜ = Q(o ,s ,θ|π), and can be re-arranged as follows:
τ τ
(cid:104) (cid:105)
G (π) =−E lnP˜(o |π)
τ Q˜ τ
(cid:104) (cid:105)
+E lnQ(s |π)−lnP˜(s |o ,π)
Q˜ τ τ τ
(cid:104) (cid:105)
+E lnQ(θ|s ,π)−lnP˜(θ|s ,o ,π) . (6)
Q˜ τ τ τ
Note, the above derivation follows the work of Fountas et al (2020).
9
Champion et al.
Re-arranging the second term of Equation (6) according to Fountas et al (2020)
First, the second term of Equation (6) is re-arranged into entropy terms for which an analytical solution
exists. In the supplementary material of (Fountas et al, 2020), the derivation proceeds as follows:
(cid:104) (cid:105) (cid:104) (cid:105)
E lnQ(s |π)−lnP˜(s |o ,π) =∆ E lnQ(s |π)−lnQ(s |o ,π) (7)
Q˜ τ τ τ Q˜ τ τ τ
(cid:104) (cid:105)
= E lnQ(s |π)−lnQ(s |o ,π)
Q(θ|π)Q(sτ|θ,π)Q(oτ|sτ,θ,π) τ τ τ
(cid:104) (cid:105)
= E E [lnQ(s |π)]−E [lnQ(s |o ,π)]
Q(θ|π) Q(sτ|θ,π) τ Q(sτ|θ,π)Q(oτ|sτ,θ,π) τ τ
(cid:104) (cid:105)
= E E [lnQ(s |π)]−E [lnQ(s |o ,π)] ,
Q(θ|π) Q(sτ|θ,π) τ Q(oτ|θ,π)Q(sτ,|oτ,θ,π) τ τ
where in the first line a distribution was renamed, i.e., P˜(s |o ,π) =∆ Q(s |o ,π). The next step in the
τ τ τ τ
derivation (c.f. supplementals of Fountas et al (2020)) re-arranges this final expression to the following:
(cid:104) (cid:105) (cid:104) (cid:105)
E lnQ(s |π)−lnQ(s |o ,π) = E E [H[Q(s |o ,π)]]−H[Q(s |π)] .
Q˜ τ τ τ Q(θ|π) Q(oτ|θ,π) τ τ τ
However, the above equation assumes that s ⊥⊥ θ | π and s ⊥⊥ θ | π,o . In other words, some of the
τ τ τ
conditioning on θ has been dropped, i.e.,
(cid:104) (cid:105)
E E [lnQ(s |π)]−E [lnQ(s |o ,π)] (last expression of derivation 7)
Q(θ|π) Q(sτ|θ,π) τ Q(oτ|θ,π)Q(sτ,|oτ,θ,π) τ τ
(cid:104) (cid:105)
(cid:54)= E E [lnQ(s |π)]−E [lnQ(s |o ,π)]
Q(θ|π) Q(sτ|π) τ Q(oτ|θ,π)Q(sτ|oτ,π) τ τ
(cid:104) (cid:105)
= E E (cid:2) H[Q(s |o ,π)] (cid:3) −H[Q(s |π)] .
Q(θ|π) Q(oτ|θ,π) τ τ τ
Whether this conditioning can be dropped or not depends on the factorisation of the distribution. In
other words, the two assumptions (i.e., s ⊥⊥ θ | π and s ⊥⊥ θ | π,o ) would have to be checked using the
τ τ τ
d-separationcriterion. However, thisisdifficulttodo, sinceasmentionedpreviously, theparametersθ are
latent variables in (2) but are regarded as parameters in (5), which makes the graphical model unclear.
Instead of attempting to prove that s ⊥⊥ θ | π and s ⊥⊥ θ | π,o , we propose an alternative derivation
τ τ τ
that does not require such independence assumptions.
Alternative derivation of the second term of Equation (6)
Restarting from the second term of Equation (6), we can re-arrange as follows:
(cid:104) (cid:105) (cid:104) (cid:105)
E lnQ(s |π)−lnP˜(s |o ,π) =∆ E lnQ(s |π)−lnQ(s |o ,π)
Q˜ τ τ τ Q˜ τ τ τ
(cid:104) (cid:105) (cid:104) (cid:105)
= E lnQ(s |π) −E lnQ(s |o ,π)
Q(sτ|π)Q(θ,oτ|sτ,π) τ Q(oτ|π)Q(sτ|oτ,π)Q(θ|sτ,oτ,π) τ τ
(cid:104) (cid:105) (cid:104) (cid:105)
= E lnQ(s |π) −E lnQ(s |o ,π)
Q(sτ|π) τ Q(oτ|π)Q(sτ|oτ,π) τ τ
(cid:104) (cid:105)
= E H[Q(s |o ,π)] −H[Q(s |π)],
Q(oτ|π) τ τ τ
where in the first line a distribution was renamed, i.e., P˜(s |o ,π) =∆ Q(s |o ,π), two different factoriza-
τ τ τ τ
tions of Q˜ are used going from the first to the second line, the linearity of expectations was used between
the first and second line, and the expectation w.r.t θ was dropped (between lines two and three) because
the expectation of a constant is the constant itself. Importantly, the above derivation does not make any
assumption of independence, and leads to a simpler result. This alternative line of reasoning is beneficial
as it produces a stronger derivation that relies upon fewer assumptions. The simpler result produced by
this derivation, also have a practical implication. Indeed, the expectation w.r.t. Q(θ|π) disappears and
the expectation w.r.t. Q(o |θ,π) is now w.r.t. Q(o |π). Those two changes sugguest that a different
τ τ
implementation of this term is required.
10
Deconstructing deep active inference.
Re-arranging the third term of Equation (6) from Fountas et al (2020)
For completeness, we now focus on the third term of (6), which can be re-arranged as follows:
(cid:104) (cid:105) (cid:104) (cid:105)
E lnQ(θ|s ,π)−lnP˜(θ|s ,o ,π) =∆ E lnQ(θ|s ,π)−lnQ(θ|s ,o ,π)
Q˜ τ τ τ Q˜ τ τ τ
(cid:104) (cid:105)
= E lnQ(o |s ,π)−lnQ(o |s ,θ,π) ,
Q˜ τ τ τ τ
where P˜(θ|s ,o ,π) was renamed as Q(θ|s ,o ,π), and Bayes theorem was used to get:
τ τ τ τ
Q(θ|s ,o ,π)Q(o |s ,π) Q(θ|s ,π) Q(o |s ,π)
τ τ τ τ τ τ τ
Q(θ|s ,π) = ⇔ = .
τ
Q(o |s ,θ,π) Q(θ|s ,o ,π) Q(o |s ,θ,π)
τ τ τ τ τ τ
Finally, by recalling that Q˜ = Q(o ,s ,θ|π) = Q(θ|π)Q(s |θ,π)Q(o |s ,θ,π), and using the linearity of
τ τ τ τ τ
expectation, we get:
(cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)
E lnQ(θ|s ,π)−lnQ(θ|s ,o ,π) = E lnQ(o |s ,π) +E H[Q(o |s ,θ,π)]
Q˜ τ τ τ Q(oτ,sτ|π) τ τ Q(θ|π)Q(sτ|θ,π) τ τ
(cid:104) (cid:105) (cid:104) (cid:105)
= E lnQ(o |s ,π) +E H[Q(o |s ,θ,π)]
Q(oτ|sτ,π)Q(sτ|π) τ τ Q(θ|π)Q(sτ|θ,π) τ τ
(cid:104) (cid:105) (cid:104) (cid:105)
= −E H (cid:2) Q(o |s ,π) (cid:3) +E H[Q(o |s ,θ,π)] ,
Q(sτ|π) τ τ Q(θ|π)Q(sτ|θ,π) τ τ
where because lnQ(o |s ,π) is a constant w.r.t θ, we have been able to use:
τ τ
E [lnQ(o |s ,π)] = E [lnQ(o |s ,π)].
Q(oτ,θ,sτ|π) τ τ Q(oτ,sτ|π) τ τ
Tosumup,thisderivationprovidesanexpressionbasedonthefollowingtwoentropyterms: H[Q(o |s ,π)]
τ τ
and H[Q(o |s ,θ,π)], which the authors claim can be estimated (c.f. Appendix B for more details). Note
τ τ
that our proposed alternative to the EFE (see below) uses this derivation for the third term of Equation
(6).
The EFE from Fountas et al (2020)
If one follows the derivation proposed by Fountas et al (2020), then the EFE is given by:
(cid:104) (cid:105)
G (π) =−E lnP˜(o |π)
τ Q˜ τ
(cid:104) (cid:105)
+E E (cid:2) H[Q(s |o ,π)] (cid:3) −H[Q(s |π)]
Q(θ|π) Q(oτ|θ,π) τ τ τ
(cid:104) (cid:105) (cid:104) (cid:105)
+E H[Q(o |s ,θ,π)] −E H (cid:2) Q(o |s ,π) (cid:3) . (8)
Q(θ|π)Q(sτ|θ,π) τ τ Q(sτ|π) τ τ
Note, in Section 2.2, we focused on presenting the approach given in Fountas et al (2020), with some
adjustments for consistency. More details about the implementation of Equation 8 are presented in
Appendix B, along with some discrepancies between the paper and the code.
Our proposed alternative to the EFE
If one follows our alternative derivation, then the EFE is given by:
(cid:104) (cid:105)
G (π) =−E lnP˜(o |π)
τ Q˜ τ
(cid:104) (cid:105)
+E H[Q(s |o ,π)] −H[Q(s |π)],
Q(oτ|π) τ τ τ
(cid:104) (cid:105) (cid:104) (cid:105)
+E H[Q(o |s ,θ,π)] −E H (cid:2) Q(o |s ,π) (cid:3) .
Q(θ|π)Q(sτ|θ,π) τ τ Q(sτ|π) τ τ
Finally, DAI does solve the dSprites environment.
MC
11
Champion et al.
2.3 DAI agent (Millidge, 2020)
VPG
In this section, we explain and discuss the approach of Millidge (2020). The code is available at the
following URL: https://github.com/BerenMillidge/DeepActiveInference. Note that, though the
mathematics in the paper are based on the formalism of a partially observable Markov decision process
(POMDP), the code does not implement an encoder/decoder architecture, which means that the code
implementsafullyobservabledecisionprocess, i.e., MDP.Additionally, theDAI iscomposedofthree
VPG
neural networks, as illustrated in Figures 4 and 5. The first is the transition network that predicts the
future observations based on the current observations and action, i.e., ˚o = T (o ,a ). The second is
τ+1 θo τ τ
thepolicynetworkthatmodelsthevariationaldistributionoveractionsQ (a |o ). Thethirdisthecritic
φa τ τ
network that predicts the expected free energy of each action given the current observation. Moreover,
Millidge (2020) defines the prior over actions as follows:
P(a |o ) = σ[−ζG(o ,a )],
τ τ τ τ
where ζ is the precision of the prior over actions, σ[•] is a softmax function, and G(o
τ
,a
τ
) is the expected
free energy (EFE) of taking action a when observing o . In the paper, the mathematics are based on
τ τ
the POMDP formalism. Therefore, G(o ,a ) is denoted G(s ,a ), and is defined as follows:
τ τ τ τ
G(s ,a ) = −r +D [Q(s )||Q(s |o )]+Gˆ (a ,s ), (9)
τ τ τ
(cid:124)
KL τ
(cid:123)(cid:122)
τ τ
(cid:125)
θˆ
a
τ+1 τ+1
intrinsicvalue
where r is the reward gathered by the agent at time step τ, and Gˆ (a ,s ) is the target network
τ θˆ
a
τ+1 τ+1
(i.e., a copy of the critic network whose weights are synchronised every K iterations of learning). Now,
remember that in the implementation, there is no encoder Q(s ) and no decoder P(o |s ). In other
τ τ τ
words, there are no hidden states s , raising some uncertainty about how the intrinsic value is com-
τ
puted. The code available on Github4 at the following URL: https://github.com/BerenMillidge/
DeepActiveInference, in the file active_inference_with_Tmodel.jl (see line 51) suggests that the
following equation is used:
(cid:88)(cid:104) (cid:105)2
intrinsic value = o [i]−˚o [i] , (10)
τ+1 τ+1
i
where o [i] is the i-th observation received at time step τ +1, and ˚o [i] is the (numerical) value of
τ+1 τ+1
the i-th observation (at time step τ +1) predicted by the transition network. More formally, the above
formulation for the intrisic value corresponds to the KL-divergence between two Gaussian distributions
both having an identity covariance matrix, i.e.,
(cid:88)(cid:104) (cid:105)2
intrinsic value = D [Q(o )||P(o |o ,a )] = o [i]−˚o [i] ,
KL τ+1 τ+1 τ τ τ+1 τ+1
i
where P(o |o ,a ) = N(o ;˚o ,I) and Q(o ) is a Gaussian distribution with mean vector o
τ+1 τ τ τ+1 τ+1 τ+1 τ+1
and an identity covariance matrix. However, note that (9) is the definition of the expected free energy in
the POMDP setting. As explained by Costa et al (2020b), the expected free energy in the MDP setting
is given by:
T
(cid:88)
G(a ,o ) ≈ D [P(o |a ,o )||P(o )],
t:T−1 t KL τ t:T−1 t τ
τ=t+1
4. We are referring to the version of the code that was available on github on the 6th of June 2022.
12
Deconstructing deep active inference.
where P(o ) are the prior preferences of the agent (related to rewards in reinforcement learning), and
τ
P(o |a ,o ) is the transition mapping. Importantly, this definition for the expected free energy does
τ t:T−1 t
not decompose into extrinsic and intrinsic terms as in (9). Thus, (as it stands) the implementation of
the DAI agent is a mixture between the POMDP and MDP setting, where the generative model
VPG
corresponds to an MDP, and the expected free energy is adapted from the POMDP setting.
We conclude this section by dicussing the training procedure of the transition, policy and critic net-
works. As explained in the paper, the transition network is trained to minimise the variational free enery.
Additionally, because of the Gaussian assumptions (with identity covariance matrices) mentioned above,
the KL-divergence reduces to the mean square error (MSE). Thus, the transition network is updated to
minimise the MSE between the observations made by the agent at time τ+1, and the observations (˚o )
τ+1
predicted by the transition network, i.e.,
(cid:104) (cid:105)
θ∗ = argminMSE o ,˚o ,
o τ+1 τ+1
θo
where ˚o = T (o ,a ). The policy network is trained to minimise the KL-divergence between the
τ+1 θo τ τ
variational posterior over actions Q (a |o ) and the prior over actions P(a |o ), i.e.,
φa τ τ τ τ
φ∗ = argminD [Q (a |o )||P(a |o )],
a KL φa τ τ τ τ
φa
which minimises the variational free energy. Finally, the critic is trained by minimising the MSE between
the target EFE as defined in (9) and the ouput of the critic G
θa
(o
τ
,•):
(cid:104) (cid:105)
θ
a
∗ = argminMSE G(o
τ
,•),G
θa
(o
τ
,•) .
θa
DAI is able to solve the CartPole environment.
VPG
T (o ,a )
θo τ τ
input: #O + #A
There is no encoder
Q(s ), and no
τ
decoder P(o |s ).
τ τ
ULeR
Q (a |o )
φa τ τ
input: #O
size: 100
size: #O
ULeR
G
θa
(o
τ
,•)
input: #O
size: 100
size: #A
ULeR
size: 100
size: #A
Figure4: NeuralnetworksarchitectureoftheDAI agent. Greenblockscorrespondtofullyconnected
VPG
layers. The first neural network is the transition network that takes as input the observation and action
at time step τ, and outputs the mean of a Gaussian distribution over observation at time step τ +1. The
second neural network is the policy network that models the variational posterior over actions. The third
neural network is the critic that takes as input an observation and outputs the expected free energy of
each action.
13
Champion et al.
Policy πˆ
o t Critic G
Transition ˚o t+1
a
t
Figure 5: This figure illustrates the DAI agent. The only new part is the policy network, which takes
VPG
as input the hidden state at time t and ouputs the parameters πˆ of the variational posterior over actions.
Importantly, the DAI takes actions based on the EFE.
VPG
2.4 DAI agent (Rood et al, 2020)
RHI
In this section, we explain and discuss the approach of Rood et al (2020). Put simply, this paper proposes
a variational auto-encoder (VAE), which is able to account for results that were observed in the context of
the rubber-hand illusion (RHI) experiment. In the experiment in Rood et al (2020), an agent (i.e., either
a human or a computer) is able to move an arm in a 3D space. However, the agent does not observe the
real position of the arm, instead, the agent sees an artificial hand placed in a different location. This can
be implemented using virtual reality (for humans) or within a simulator (for computers). Since, Rood
et al (2020) restricted themself to the context of a VAE, this approach cannot be considered as a complete
implementation of deep active inference. More precisely, the transition and critic (or policy) networks are
missing.
2.5 DAI agent (Sancaktar et al, 2020; Lanillos et al, 2020; Oliver et al, 2019)
HR
In this section, we explain and discuss the following approaches: Sancaktar et al (2020), Lanillos et al
(2020), and Oliver et al (2019). Briefly, those papers propose a free energy minimisation scheme based
on a single decoder network, which is used to control Nao, TIAGo, and iCub robots, respectively. Since,
14
Deconstructing deep active inference.
Sancaktar et al (2020); Lanillos et al (2020); Oliver et al (2019) restricted themself to the context of a
singledecoder,thisapproachcannotbeconsideredasacompleteimplementationofdeepactiveinference.
More precisely, the encoder, transition and critic (or policy) networks are missing.
2.6 DAI agent (Ueltzh¨offer, 2018)
FA
In this section, we review the approach proposed by Ueltzh¨offer (2018). The original code of this pa-
per is available on GitHub at the following URL: https://github.com/kaiu85/deepAI_paper. This
approach is composed of four deep neural networks. The encoder E models the approximate pos-
φs
terior over states Q (s |s ,o ) as a Gaussian distribution, i.e., Q (s |s ,o ) = N(s ;µ,σ) where
φs t t−1 t φs t t−1 t t
µ,σ = E (s ,o ). ThedecoderD modelsthelikelihoodmappingP (o |s )asaGaussiandistribution,
φs t−1 t θo θo τ τ
i.e., P (o |s ) = N(o ;µ ,σ ) where µ ,σ = D (s ). The transition network T models the transition
θo τ τ τ o o o o θo τ θs
mapping P (s |s ) as a Gaussian distribution, i.e., P (s |s ) = N(s ;˚µ,˚σ) where ˚µ,˚σ = T (s ).
θs τ τ−1 θs τ τ−1 τ θs τ−1
Note that, the transition network is only conditioned on the previous state. This is because the action
is contained in the observations predicted by the decoder. More precisely, the experiments were run in
the MountainCar environment, which means that the agent is observing the x position of the car ox.
τ
Additionally, according to the idea of proprioception, the agent observes its own action, i.e., oa = a
τ−1 τ−1
where a is the action performed by the agent at time τ −1. In what follows, we let o = (ox,oa ) be
τ−1 τ τ τ−1
the concatenation of the x position of the car and the action taken by the agent. Importantly, because
o contains oa , the latent space has to (implicitly) encode the action for the decoder to successfully
τ τ−1
predict the observations. Finally, the policy network P models the prior over actions P (a |s ) as a
θa θa τ τ
Gaussian distribution, i.e., P (a |s ) = N(a ;µ ,σ ) where µ ,σ = P (s ). Figure 6 illustrates the
θa τ τ τ a a a a θa τ
architectures of those deep neural networks. Then, Ueltzh¨offer (2018) defines the free action objective as
the cumulated variational free energy over time:
accuracy
T (cid:34) (cid:122) (cid:125)(cid:124) (cid:123) complexity (cid:35)
(cid:88) (cid:104) (cid:105) (cid:122) (cid:125)(cid:124) (cid:123)
FA(o ,φ,θ) = −E lnP (o |s ) +D [Q (s |s ,o )||P (s |s )] ,
1:T Q
φs
(sτ|sτ−1,oτ) θo τ τ KL φs τ τ−1 τ θs τ τ−1
τ=1 (cid:124) (cid:123)(cid:122) (cid:125)
VFEτ
where s = (0,...,0) is a vector of zeros representing the initial hidden state, T is the time horizon,
0
P (o |s ), Q (s |s ,o ), P (s |s ) are modeled using Gaussian distributions whose parameters
θo τ τ φs τ τ−1 t θs τ τ−1
are predicted by the decoder, encoder and transition network, respectively. Figure 7 illustrates the
computation of the free action objective, and the action-perception cycle of the agent. The first action-
perception cycle is initiated when the intital hidden state s is being fed into the policy network, which
0
outputs the parameters of a Gaussian distribution over actions. Then, an action aˆ is sampled from
0
this Gaussian, and executed in the environment leading to a new observation ox. Next, the action aˆ is
1 0
concatenated with ox to form o . The observation o and the state s are then fed into the encoder that
1 1 1 0
outputs the parameters of a Gaussian distribution over sˆ . Lastly, a state is sampled from this Gaussian
1
distributionandisusedasinputtothenextaction-perceptioncycle. Thisprocesscontinuesuntilreaching
the time horizon.
Within each action-perception cycle, the variational free energy of this time step is computed. To
compute VFE , the state s is fed into both the tansition network and the encoder. Both networks
τ τ
output the parameters of a Gaussian distribution over s . A state is sampled from the distribution
τ+1
predicted by the encoder, and is used as input to the decoder that outputs the parameters of a Gaussian
distribution over o . Finally, the parameters of the Gaussian distribution over o is used to compute
τ+1 τ+1
the accuracy term, and the parameters of the two Gaussian distributions over s are used to compute
τ+1
the complexity term.
We now focus on the prior preferences of the agent. Usually, prior preferences are part of the expected
free energy. However, Ueltzh¨offer (2018) takes a different approach. Recall, the latent variable s is
τ
15
Champion et al.
modeled using a multivariate Gaussian. The DAI agent reserves the first dimension of the latent space
FA
to the encoding of the prior preferences. Specifically, the transition network predicts the mean vector and
the diagonal of the covariance matrix (i.e., another vector) of a multivariate Gaussian over latent states.
The first element in the mean vector is clamped to the target x position, and the first element of the
variance vector is set to a relatively small value. This effectively propels the agent towards the target
location. Additionally, the encoder predicts another set of mean and variance vectors. The first element
of the mean vector predicted by the encoder is clamped to the current x position observed by the agent,
and the first element of the variance vector is set to a relatively small value. Note, clamping the value of
the first element of the mean and variance vectors predicted by the transition is uncontentious, i.e., this is
simply how the generative model is defined. However, clamping the value of the first element of the mean
and variance vectors predicted by the encoder may be debated. Specifically, the encoder is supposed to
predict the variational distribution, which is an approximation of the true posterior. However, clamping
the value of the first element of the mean and variance vectors predicted by the encoder is likely to push
the variational posterior further from the true posterior.
Q (s |o ,s )
φs t t t−1
input: #O + #S
ULeR
size: #S
ULeR
size: #S
SHT
P (o |s )
θo τ τ
input: #S
size: #S + #S
ULeR
size: #S
ULeR
size: #S
ULeR
size: #S
SL
P (s |s )
θs τ τ−1
input: #S
size: #O + #O
SHT
P (a |s )
θa τ τ
input: #S
size: #S + #S size: #S
SL size: #A + #A
Figure 6: Neural networks architecture of the DAI agent. Green blocks correspond to fully connected
FA
layers. The first neural network is the encoder that takes as input the state at time t − 1 and the
observation at time t, and outputs the parameters of a distribution over the state at time t. The second
neural network is the decoder that takes as input the state at time τ, and outputs the parameters of a
distribution over the observation at time τ. The third is the transition network that takes as input the
state at time step τ −1, and outputs the parameters of a distribution over the state at time step τ. The
fourth neural network is the policy network that models the prior over actions, i.e., the policy takes as
input a state at time τ and outputs the parameters of a distribution over the actions at time step τ.
Finally, THS stands for tangent hyperbolic and softplus, i.e., the tangent hyperbolic activation is over
the first half of the neurons and the softplus activation function is over the second half, and LS stands
for linear activation function and softplus, i.e., the linear activation is over the first half of the neurons
and the softplus activation function is over the second half.
There are a number of important aspects of the DAI agent. First, there is no expected free energy,
FA
instead the agent is trained to minimise the cumulated variational free energy over time. Second, this
approach unrolls the partially observable Markov decision process over time. In other words, the code
builds a huge computational graph containing the encoder, decoder, transition and policy networks for
each action-perception cycle. Therefore, the approach is computationally intensive and can become in-
16
Deconstructing deep active inference.
tractableforalargetimehorizon. Third, theDAI requiresthemodellertoencodethepriorpreferences
FA
within the distributions predicted by the encoder and transition network. This can limit the applicability
of the approach. Indeed, as previously explained, one can encode the prior preferences of the agent for
the MountainCar problem within the first dimension of the latent space.
However, manually encoding the prior preferences in the latent space has two major drawbacks. First,
the model needs to be modified from one environment to the next. This is because for each environment,
the prior preferences of the agent will be different. Second, for some environments, it is unclear how the
prior preferences may be defined. For example, when playing PacMan, the agent needs to eat all the dots,
while simultaneously avoiding the ghosts. How can this be encoded in the model’s latent space? This is
particularly challenging because the only observation made by the agent is an image of the game, i.e., the
agent does not directly have access to the positions of PacMan and the ghosts.
s
τ
P (s )
θa τ
(cid:15) µ a σ a
(cid:15)
E (o ,s )
aˆ
φs τ+1 τ
µˆ sˆ
τ τ+1
env.execute(aˆ )
τ
σˆ D (sˆ )
θo τ+1
ox
τ+1
µ σ
o o
T (s )
θs τ
˚µ VFE
τ
˚σ
Figure 7: Action-perception cycles (in black) and estimation of the free action objective (in red). Note,
˚µ, ˚σ, µˆ and σˆ are used to compute the complexity terms of the variational free energy, while µ and σ
o o
are used to compute the accuracy term of the variational free energy.
2.7 DAI agent (van der Himst and Lanillos, 2020)
POMDP
In this section, we review the approach proposed by van der Himst and Lanillos (2020). The code is avail-
ablehere: https://github.com/Grottoh/Deep-Active-Inference-for-Partially-Observable-MDPs.
The DAI agent is composed of five deep neural networks.
POMDP
The decoder D models P (o |s ) as a product of Bernoulli distributions, therefore: P (o |s ) =
θo θo τ τ θo τ τ
Bernoulli(o ;oˆ ) where oˆ = D (s ). The transition network T models P (s |s ,a ) as a Gaussian
τ τ τ θo τ θs θs τ+1 τ τ
distribution, i.e., P (s |s ,a ) = N(s |˚µ,˚σ) where ˚µ,ln˚σ = T (s ,a ). The critic G outputs a
θs τ+1 τ τ τ+1 θs τ τ θa
vector containing the predicted expected free energy of each action, which is used to define the prior over
action as P
θa
(a
τ
|s
τ
) = σ[−ζG
θa
(s
τ
,•)], where σ[•] is a softmax function, ζ is the precision of the prior over
actions, and G
θa
(s
τ
,•) is the expected free energy of each action as predicted by the critic network when
state s is provided as input. The variational posterior over states Q (s ) is a Gaussian distribution
τ φs t
modelled by the encoder E , i.e., Q (s ) = N(s ;µ,σ) where µ,lnσ = E (o ). The variational posterior
φs φs t t φs t
over actions Q (a |s ) is a categorical distribution modelled by the policy network P , i.e., Q (a |s ) =
φa t t φa φa t t
Cat(a ;πˆ) where πˆ = P (s ). Then, the agent is supposed to minimise the variational free energy defined
t φa t
17
Champion et al.
as follows:
Q∗(s ,a ) = argminD [Q (a |s )Q (s )||P (o |s )P (s |s ,a )P (a |s )]
φ t t KL φa t t φs t θo t t θs t t−1 t−1 θa t t
Q
φ
(st,at)
= argminD [Q (s )||P (s |s ,a )]+D [Q (a |s )||P (a |s )]−E [lnP (o |s )].
KL φs t θs t t−1 t−1 KL φa t t θa t t Q
φs
(st) θo t t
Q
φ
(st,at)
However, as explained in the paper, the KL-divergence (over states) is replaced by the mean square error
(MSE) as follows:
Q∗(s ,a ) = argminMSE(µ,˚µ)+D [Q (a |s )||P (a |s )]−E [lnP (o |s )],
φ t t KL φa t t θa t t Q φs (st) θo t t
Q
φ
(st,at)
where µ and ˚µ are the mean vectors predicted by the encoder and the transition network, respectively.
The paper justifies this substitution by saying that the maximum a posteriori (MAP) estimate is used to
compute the state prediction error, instead of using the KL-divergence over the densities. However, the
state prediction error and the KL-divergence over states are two different quantities, which are only equal
when the two densities over states are Gaussian distributions with identity covariance matrix. However,
the distribution predicted by the encoder network does not have an identity covariance matrix.
Put simply, in this context, the MSE and the KL-divergence between the densities over state are not
necessarilyequivalent. Asaresult, theDAI agentmaynotalwaysfollowthefreeenergyprinciple.
POMDP
2.8 DAI agent (van der Himst and Lanillos, 2020)
SSM
The deep active inference agent proposed by (C¸atal et al, 2020) is based on a state space model, and is
thereforecalledDAI . Thecodeofthisapproachwasnotavailableonline, butwewereabletoretrieve
SSM
it from the authors. Importantly, DAI is an offline approach meaning that the model is trained first
SSM
onafixeddatasetgatheredeitherbytakingrandomactionsintheenvironmentorbymanuallycontrolling
the robot. Then, when the model is trained, the expected free energy of different sequences of actions
can be estimated using imaginary rollouts, and the first action of the policy with the lowest expected free
energy is executed in the environment.
2.9 Active exploration for robotic manipulation (Schneider et al, 2022)
The last paper that we review (Schneider et al, 2022) is motivated from a reinforcement learning per-
spective, where the agent aims to maximise reward. However, instead of greedily maximising reward, the
agent also maximises the information gain between the model parameters and the expected states and
rewards, i.e.,
t+H
max E (cid:2) f(r) (cid:3) + βE (cid:2) D [P(θ|s,r,π,s )||P(θ)] (cid:3) , where: f(r) = (cid:88) r (11)
π
P(r|π) P(s,r|π,st) KL t τ
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) τ=t+1
Expectedreward Informationgain
where t is the current time step, H is the time horizon of planning, π is the agent policy, θ are the
parameters of the model, r = r is the sequence of rewards between time step t+1 and t+H,
t+1:t+H
s = s is the sequence of states between time step t+1 and t+H, and β is a hyper-parameter
t+1:t+H
modulatingtheimpactofinformationgain. Note,thedistributionoverthesequenceofrewardsr obtained
by the agent when behaving according to a policy π, i.e., P(r|π), is not deterministic. Indeed, the same
policy can produce different trajectories of states as the transition mapping is stochastic, and those
different states can produce different rewards. Notice, the expectation w.r.t P(r|π) is iterating over all
possible sequences of rewards, and passing each sequence to f(r). Then, the authors demonstrate the
relationship between Equation 11 and the expected free energy. This relationship is explained in more
details in (Schneider, N.D.), and relies on the the following assumptions:
18
Deconstructing deep active inference.
1. the states s and rewards r are latent variables for all τ ∈ {t+1,...,t+H}
τ τ
2. the generative model contains observed variables for states os and rewards or
τ τ
3. the likelihood mappings, i.e., P(or|r ) and P(os|s ), are delta distributions which effectively make
τ τ τ τ
thelatentvariablesfullyobservable,exceptforthemodelparametersθthataredistributedaccording
to a delta distribution at time step zero and remain fixed over time
4. the variance of the transition mapping P(s |s ,a ,θ) does not depend on the parameters θ,
τ+1 τ τ
e.g., P(s |s ,a ,θ) = N(s ;µ,σ) where µ = f (s ,a ) is predicted by a neural network with
τ+1 τ τ τ+1 θ τ τ
parameters θ, and σ = σI is a diagonal matrix whose diagonal elements are equal to σ.
Importantly, while the above assumptions allow the authors to derive the expected free energy from
Equation 11, these assumptions also impose a lot of constraints on the model. For example, the proof
does not hold if the likelihood mappings are not delta distributions. In this paper, we focus on such
models. To conclude, this approach is a great contribution to reinforcement learning applied to robotic
control, but cannot be considered as a complete deep active inference agent. The code of this approach
is available at the following URL: https://github.com/TimSchneider42/aerm.
2.10 Representational similarity with centered kernel alignment
The goal of representational similarity metrics is, as its name indicates, to measure the similarity between
two representations. In the context of deep learning, these representations correspond to Rn×p matrices
of activations, where n is the number of data examples and p the number of neurons in a layer. In this
paper, we aim to use such metrics to compare the representations learned by the deep learning models
described in Section 3 and the representations learned by a DQN.
For our analysis, we will use Centred Kernel Alignment (CKA) (Cortes et al, 2012; Cristianini et al,
2002), a normalised version of the Hillbert-Schmidt Independence Criterion (HSIC) (Gretton et al, 2005),
which measures the alignment between the n × n kernel matrices of two representations. Kornblith
et al (2019) have shown that for deep learning applications, linear kernels with centred layer activations
workedwell.WethusfocusonthelinearCKA,alsoknownasRV-coefficient(RobertandEscoufier,1976).
Moreover, it has been shown to provide results similar to other representational similarity metrics, while
being faster to compute (Bonheme and Grzes, 2022).
For conciseness, we will refer to linear CKA as CKA in the rest of this paper. We now define CKA
more formally. Given the centered layer activations x ∈ Rn×m and y ∈ Rn×p taken over n data examples,
CKA is defined as:
(cid:107)yTx(cid:107)2
CKA(x,y) = F ,
(cid:107)xTx(cid:107) (cid:107)yTy(cid:107)
F F
where (cid:107)·(cid:107) is the Frobenius norm, which is defined as:
F
(cid:118)
(cid:113) (cid:117) (cid:117)(cid:88) k (cid:88) l
(cid:107)a(cid:107) F = tr(aaT) = (cid:116) |a ij |2,
i=1 j=1
where a ∈ Rk×l is an arbitrary k×l matrix, and aT is the transpose of a.
Limitations of CKA While CKA leads to accurate results in practice, it can be overly sensitive to
differences in neural architectures (Maheswaranathan et al, 2019), and can thus underestimate the simi-
larity between activations coming from layers of different type (e.g., convolutional and deconvolutional).
Thus, we will only discuss the variation of similarity when analysing such cases. For example, we will not
compare CKA(a,b) and CKA(a,c) if a and b are convolutional layers but c is linear. We will, however,
compare CKA(a,c) and CKA(b,c).
19
Champion et al.
3. Incrementally building a deep active inference agent
All of the deep active inference models we have presented make important contributions, illustrating a
range of possible implementations. However, we do not feel that any of these approaches is a complete
and definitive realisation of deep active inference. We have highlighted limitations of these published
approaches throughout our presentation. Accordingly, in the remainder of this paper, we step back
to first principles and “build up” an agent component-by-component to determine which parts of a
“natural”deepactiveinferenceframeworkunderlieitscapacitytosolveorfailtosolveinferenceproblems.
Additionally, throughout this component-by-component investigation, we compare the different variants
of deep active inference that result with a standard (well-attested) approach: a deep Q-network (Mnih
etal,2013). Thus, inthissection, weprogresivelybuildadeepactiveinferenceagent. Section3.1presents
the dSprites environment in which all our simulations will be run. This environment was picked to test
whether an active inference agent is able to solved the dSprites problem, as explored in (Fountas et al,
2020). Section 3.2 describes how the agents introduced later in this paper interact with the environment.
Then, Section 3.3 introduces a variational auto-encoder (VAE) agent, Section 3.4 discusses a deep hidden
Markov model (HMM) agent, Section 3.5 presents a deep critical HMM (CHMM) agent, and finally,
Section 3.6 introduces a complete deep active inference agent. Note, the notation used throughout this
section are summarised in Appendix A.
3.1 dSprites environment
The dSprites environment is based on the dSprites dataset (Matthey et al, 2017), initially designed for
analysing the latent representation learned by variational auto-encoders (Doersch, 2016). The dSprites
dataset is composed of images of squares, ellipses and hearts. Each image contains one shape (square,
ellipse or heart) with its own scale, orientation, and (X,Y) position. In the dSprites environment, the
agent is able to move those shapes around by performing four actions (i.e., UP, DOWN, LEFT, RIGHT).
To make the task tractable, the action selected by the agent is executed eight times in the environment
beforethebeginningofthenextaction-perceptioncycle, i.e., theX orY positionisincreasedordecreased
byeightbetweentimesteptandt+1. Thegoaloftheagentistomoveallsquarestowardsthebottom-left
corner of the image and all ellipses and hearts towards the bottom-right corner of the image, c.f. Figure
8.
Figure 8: This figure illustrates the dSprites environment, in which the agent must move all squares
towards the bottom-left corner of the image and all ellipses and hearts towards the bottom-right corner
of the image. The red arrows show the behaviour expected from the agent.
20
Deconstructing deep active inference.
3.2 Agent-environment interaction
Inthissection,wepresenthowalltheagentsintroducedinthenextsectionsinteractwiththeenvironment.
Each agent was trained for N = 500K iterations. At the begining of a trial, the environment is reset to
a random state and the agent receives an observation5 o . Using o , the agent selects an action a , which
t t t
is then executed in the environment. This leads the agent to receive a new obervation o , a reward
t+1
r and a boolean done describing whether the trial is over or not. Then, the new experience (o , a ,
t+1 t t
o , r , done) is added to the replay buffer, from which a batch is sampled to train the various neural
t+1 t+1
networks of the agent. Finally, if the trial has ended, then the environment is reset to a random state
leading to a new observation o , otherwise o becomes the new o closing the action-perception cycle.
t t+1 t
Algorithm 1 summarises the agent-environment interaction.
Algorithm 1: The interaction between the agent and the environment.
Input: env the environment,
agent the agent,
buffer the replay buffer,
N the number of training iterations.
space
o = env.reset() // Get the initial observation from environment
t
repeat N times
a ← select action(o ) // Select an action
t t
o ,r ,done ← env.execute(a ) // Execute the action in the environment
t+1 t+1 t
buffer.push new experience(o , a , o , r , done) // Add the experience to the
t t t+1 t+1
replay buffer
agent.learn(buffer) // Perform one iteration of training
if done == True then
o ← env.reset() // Reset the environment when a trial ends
t
else
o ← o
t t+1
end
end
3.3 Variational auto-encoder
In this section, we present our first agent based on a variational auto-encoder. The agent is composed of
two deep neural networks, i.e., an encoder and a decoder. The encoder E takes as input an image o
φs t
and outputs the parameters of the variational posterior Q (s ) = N(s ;µ,σ), where µ is the mean vector
φs t t
of the Gaussian distribution, and σ are the diagonal elements of the covariance matrix. The decoder D
θo
models the likelihood mapping P (o |s ), which attributes a probability to each image o given a state
θo t t t
s , and is defined as:
t
P (o |s ) = Bernoulli(o ;oˆ),
θo t t t t
where oˆ = D (s ) are the values predicted by the decoder, and Bernoulli(o ;oˆ) is a product of Bernoulli
t θo t t t
distributions defined as:
(cid:89)
Bernoulli(o ;oˆ) = Bernoulli(o [x,y];oˆ[x,y]),
t t t t
x,y
whereBernoulli(•; •)isaBernoullidistributionoverthepossiblevaluesofthepixelo
t
[x,y],parameterized
by the parameter oˆ[x,y], which is predicted by the decoder network. The goal of the agent is to minimise
t
5. Eachobservationcontainsasequenceofthreeimages,i.e.,theimagecorrespondingtothecurrentstateoftheenvironment,
and the two images gathered during the previous two time steps.
21
Champion et al.
the variational free energy (VFE):
F = D [Q (s )||P (o ,s )] = D [Q (s )||P (o |s )P(s )],
KL φs t θo t t KL φs t θo t t t
where P(s ) = N(s ;0,I) is an isotropic (multivariate) Gaussian with variance one. The VFE can be
t t
re-arranged as follows:
F = D [Q (s )||P(s )]−E [lnP (o |s )],
KL φs t t Q
φs
(st) θo t t
where the KL-divergence between two Gaussian distributions can be computed using an analytical solu-
tion, and the expectation of the logarithm of P (o |s ) is approximated by a Monte-Carlo estimate using
θo t t
a single sample sˆ ∼ Q (s ). The sample sˆ is obtained using the reparameterisation trick as follows:
t φs t t
sˆ = µ+σ(cid:12)(cid:15)ˆ, where (cid:12) is an element-wise product between two vectors, and (cid:15)ˆ∼ N((cid:15);0,I).
t
To sum up, this agent takes random actions, and stores its experiences in a replay buffer (c.f. Section
3.2). Then, batches of experiences (o , a , o , r , done) are sampled from the replay buffer. The
t t t+1 t+1
observations at time step t are then fed into the encoder, which outputs the mean and log variance of
a Gaussian distribution Q (s ) = N(s ;µ,σ). A latent state is sampled from Q (s ) using the re-
φs t t φs t
parameterisation trick, and is then provided as input to the decoder which outputs the parameters of
Bernoulli distributions oˆ. The KL-divergence between Q (s ) and P(s ) is computed analytically, and
t φs t t
the logarithm of P (o |s ) reduces to the binary cross entropy (BCE) because P (o |s ) is a product of
θo t t θo t t
Bernoulli distributions. Next, the VFE is obtained by subtracting the BCE from the KL-divergence, and
back-propagation is used to update the weights of the encoder and decoder networks. Figure 9 illustrates
the VAE agent presented in this section. Note, this agent takes random actions.
(cid:15)
µ
o t Encoder sˆ t Decoder oˆ t
lnσ
Figure 9: This figure illustrates the VAE agent. From left to right, we have the input image o , the
t
encoder network, the layer of mean µ and log variance lnσ, the epsilon random variable used for the
reparameterisation trick, the latent state sˆ, the decoder network, and finally, the reconstructed image
t
oˆ. Note, there are no actions in this agent’s generative model. Therefore, the VAE agent takes random
t
actions.
3.4 Deep hidden Markov model
In this section, we present our second agent based on a hidden Markov model. Similarly to the VAE
agent, the HMM agent is composed of an encoder network modelling Q (s ), and a decoder network
φs τ
modelling P (o |s ). However, the prior over the hidden states at time step t+1 depends on the hidden
θo τ τ
states and action at time step t. This prior is modelled by the transition network T that predicts
θs
the parameters of the Gaussian distribution P (s |s ,a ) = N(s ;˚µ,˚σ), where ˚µ is the mean of the
θs t+1 t t t+1
Gaussian distribution, and˚σ are the diagonal elements of the covariance matrix. Recall, that the goal of
the inference process is to fit the approximate posterior Q (s ) to the true posterior P(s |o ,s ,a ).
φs t t t t−1 t−1
Formally,thisoptimisationcanbewrittenastheminimizationoftheKullback-Leiblerdivergencebetween
22
Deconstructing deep active inference.
the approximate and the true posterior, i.e.,
Q∗(s ) = argminD [Q (s )||P(s |o ,s ,a )].
t KL φs t t t t−1 t−1
Q
φs
(st)
Using a derivation almost identical to the one presented in Section 2.2.2, the VFE can be proven to be:
Q∗ (s ) = argmin D [Q (s )||P (o |s )P (s |s ,a )] (12)
φs t KL φs t θo t t θs t t−1 t−1
Q
φs
(st) (cid:124) (cid:123)(cid:122) (cid:125)
variationalfreeenergy
= argmin D [Q (s )||P (s |s ,a )]−E [P (o |s )].
KL φs t θs t t−1 t−1 Q
φs
(st) θo t t
Q
φs
(st)
The VFE of Equation 12 can be computed in a similar way to the VAE agent. Put simply, this agent
takes random actions, and stores its experiences in a replay buffer (c.f. Section 3.2). Then, batches of
experiences (o , a , o , r , done) are sampled from the replay buffer. The observations at time step
t−1 t−1 t t
t − 1 are feed into the encoder, which outputs the mean and log variance of a Gaussian distribution
Q (s ) = N(s ;µ,σ). A latent state is sampled from Q (s ) using the re-parameterisation trick,
φs t−1 t−1 φs t−1
and is then provided as input to the transition network along with action a . The transition network
t−1
outputs the parameters of the Gaussian distributions P (s |s ,a ) = N(s ;˚µ,˚σ). Additionally, the
θs t t−1 t−1 t
observations at time step t can be fed into the encoder to get the parameters of Q (s ) = N(s ;µˆ,σˆ).
φs t t
Sampling from Q (s ) using the reparameterisation trick gives a state sˆ that when given as input to
φs t t
the decoder produces the parameters of a product of Bernoulli distributions oˆ . The KL-divergence
t+1
between Q (s ) and P (s |s ,a ) is computed analytically, and the logarithm of P (o |s ) reduces
φs t θs t t−1 t−1 θo t t
to the binary cross entropy (BCE) because P (o |s ) is a product of Bernoulli distributions. Next, the
θo t t
VFEisobtainedbysubtractingtheBCEfromtheKL-divergence, andback-propagationisusedtoupdate
the weights of the encoder, decoder and transition networks. Figure 10 illustrates the HMM agent.
3.5 Deep critical HMM
In this section, we present our third agent that incorporates a critic network to the deep HMM presented
in the previous section. The resulting model is called a deep CHMM and is illustrated in Figure 11.
The CHMM is equipped with an encoder E modelling Q (s ), a decoder D modelling P (o |s ), a
φs φs τ θo θo τ τ
transitionnetworkT modellingP (s |s ,a ),andacriticnetworkG thatpredictstheexpectedfree
θs θs t t−1 t−1 θa
energy (see below) of each action. The critic is then used to define the prior over actions as: P (a |s ) =
θa t t
σ[−ζG
θa
(s
t
,•)], where ζ is the precision of the prior over actions, and G
θa
(s
t
,•) is the EFE of taking
each action in state s as predicted by the critic. The encoder, decoder and transition networks are all
t
trained in the same way as before to minimise the VFE. The critic however is trained to minimise the
smooth L1 norm between its output G
θa
(s
t
,•) and the target G-values y(•), i.e., the critic minimises
SL1[G
θa
(s
t
,•),y(•)]. Note, the SL1 was picked because it is less sensitive to outliers than the MSE, and
is defined as:
(cid:40) (x−y)2
0.5× if |x−y| < β
SL1[x,y] = β ,
|x−y|−0.5×β otherwise
where β is an hyper-parameter such that as β goes to zero, the SL1 loss converges to the L1 loss, and
when β tends to +∞, the SL1 loss converges to a constant zero loss. Intuitively, the SL1 loss uses a
squared term if the absolute element-wise error falls below β, and an L1 term otherwise. Addtionally, the
target G-values are defined as:
(cid:104) (cid:105)
y(a ) = G (a )+γE max Gˆ (s ,a ) ,
t t+1 t Q
φs
(st+1)
at+1∈A
θˆ
a
t+1 t+1
where Q (s ) can be computed by feeding the image o sampled from the replay buffer as input to
φs t+1 t+1
the encoder, G (a ) is the expected free energy at time t+1 after taking action a (see below), and γ is
t+1 t t
23
Champion et al.
oˆ oˆ
t t+1
Decoder Decoder
(cid:15)
sˆ sˆ
t t+1
˚µ
Transition ˚s t+1
(cid:15) µ lnσ ln˚σ (cid:15) µˆ lnσˆ
a
t
Encoder Encoder
o o
t t+1
Figure 10: This figure illustrates the HMM agent. On the left and right, one can see two auto-encoders,
i.e., one at time step t and one at time step t + 1. In the middle, the transition network takes as
input the state and action at time t, i.e., (sˆ,a ), and outputs the mean ˚µ and log variance ln˚σ of a
t t
Gaussian distribution. By sampling the latent variable (cid:15), and using the reparameterisation trick, we
get the latent state outputed by the transition network: ˚s = ˚µ +˚σ (cid:12) (cid:15)ˆ where (cid:15)ˆ is sampled from a
t+1
Gaussian distribution with mean zero and variance one. Importantly, the model seems to be composed
of two disconnected parts, however, the variational free energy will have a complexity term between the
Gaussian distributions outputed by the transition network and encoder at time t+1. Note, this agent
takes random actions.
a discount factor. Note, the above Equation is an application of Bellman’s equation (Bellman, 1952) to
the expected free energy. Also, as for the DQN agent, we improved the training stability by implementing
a target network Gˆ , which is structurally identical to the critic and whose weights are synchronised with
θˆ
a
the weights of the critic every K (learning) iterations. The last question to answer before focusing on the
subject of the EFE is: how does the CHMM select the action to be performed in the environment?
There are at least four possibilities: (i) select a random action, (ii) select the action that maximises
EFE according to the critic, i.e., a∗ = argmax G (s ,a ), (iii) sample an action from a softmax function
t at θa t t
oftheoutputofthecritic,i.e.,a∗
t
∼ σ[G
θa
(s
t
,•)]whereσ[•]isasoftmaxfunction,and(iv)usethe˚(cid:15)-greedy
24
Deconstructing deep active inference.
algorithm with exponential decay, i.e., select a random action with probabilty˚(cid:15) or select the best action
with probability 1−˚(cid:15) where˚(cid:15) starts with a high value and decays exponentially fast.
oˆ oˆ
t t+1
Critic G
Decoder Decoder
(cid:15)
sˆ sˆ
t t+1
˚µ
Transition ˚s t+1
(cid:15) µ lnσ ln˚σ (cid:15) µˆ lnσˆ
a
t
Encoder Encoder
o o
t t+1
Figure 11: This figure illustrates the CHMM agent. The only new part is the critic network, which takes
as input the hidden state at time t and ouputs the expected free energy of each action G. Importantly,
the CHMM takes actions based on the EFE.
3.5.1 Expected free energy
In this section, we discuss the definition of the expected free energy (EFE) before investigating various
ways to implement it in the context of deep active inference. Recently in (Parr and Friston, 2019), the
expected free energy was defined as:
T T
G(π) = (cid:88) G (π) = (cid:88) E (cid:2) lnQ(s |π)−lnP(o ,s ) (cid:3) , (13)
τ P(oτ|sτ)Q(sτ|π) τ τ τ
τ=t+1 τ=t+1
where P(o |s ) is the likelihood mapping, Q(s |π) is the variational distribution, and in the literature,
τ τ τ
P(o ,s ) is called the generative model but is better understood as a target distribution encoding the
τ τ
prior preferences of the agent. Indeed, assuming the standard generative model of active inference (i.e.,
25
Champion et al.
s t+1 Target Gˆ θˆ
a
(s t+1 ,•) y(•)
r
t
SL1
γ
s t Critic G θa (s t ,•)
Figure 12: This figure illustrates the computation of the critic’s loss function when the critic is only
maximising reward, i.e., when Equation 19 is used for the expected free energy. Briefly, the state s is
t
fed into the Critic, and the state s is fed into the target network. The critic outputs the G-values for
t+1
each action at time t, and the target network outputs the G-values for each action at time t+1. Then,
the reward, the discount factor, and G-values of each action at time t+1 are used to compute the target
values y(•). Finally, the goal is to minimise the SL1 between the prediction of the critic and the target
values by changing the weights of the critic.
a partially observable Markov decision process), the hidden states s should depend on s and a .
τ τ−1 τ−1
This point is important because it impacts the question of whether P(o ,s ) is indeed the generative
τ τ
model, and therefore whether the expected free energy is the expectation of the variational free energy.
According to the free energy principle, an active inference agent must minimise its (variational) free
energy. However, if such a relationship cannot be established between the expected and variational free
energy, thenonecannotclaimthatanagentminimisingexpectedfreeenergyalsominimisesitsvariational
free energy. Additionally, we need to re-arrange the definition of the EFE stated in (13) to allow rewards
to be incorporated:
G (π) = E (cid:2) lnQ(s |π)−lnP(o ,s ) (cid:3)
τ P(oτ|sτ)Q(sτ|π) τ τ τ
= E (cid:2) lnQ(s |π)−lnP(s |o )−lnP(o ) (cid:3)
P(oτ|sτ)Q(sτ|π) τ τ τ τ
≈ E (cid:2) lnQ(s |π)−lnQ(s )−lnP(o ) (cid:3)
P(oτ|sτ)Q(sτ|π) τ τ τ
= D [Q(s |π)||Q(s )]−E (cid:2) lnP(o ) (cid:3) , (14)
KL τ τ P(oτ|sτ)Q(sτ|π) τ
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
epistemicvalue extrinsicvalue
3.5.2 A principled estimate of the EFE at time t+1?
Now, the question is how to estimate (14), and we focus on the case where τ = t+1. Note, because
τ = t+1,thepolicyπcontainsonlyoneactiona ,i.e.,π = a . Inthetabularversionofactiveinference,the
t t
variational distribution is composed of a factor Q(s |π). However, in the deep active inference literature,
τ
26
Deconstructing deep active inference.
the variational distribution does not contain such a factor. Generally, a Monte-Carlo estimate is used as
follows:
N
Q(s |a ) = E (cid:2) P (s |s ,a ) (cid:3) ≈ 1 (cid:88) P (s |s = sˆi,a ), (15)
t+1 t Q φs (st) θs t+1 t t N θs t+1 t t t
i=1
where sˆi ∼ Q (s ). Importantly, for the expected free energy to be the expectation of the variational
t φs t
free energy, Q(s |a ) should be a factor of the variational distribution. However, (15) is estimated using
t+1 t
a factor of the generative model P (s |s = sˆi,a ). This is a conceptual issue, associated with current
θs t+1 t t t
deep active inference approaches, such as Fountas et al (2020). In what follows, we use N = 1 leading to
a simplified version of the estimate:
Q(s |a ) ≈ P (s |s = sˆ,a ). (16)
t+1 t θs t+1 t t t
Note, in the above equation, sˆi is denoted sˆ because there is only one sample, i.e., N = 1. At this point,
t t
we have an estimate for Q(s |a ) and Q (s ) is the variational distribution. The only missing piece is
t+1 t φs t
an estimate of the extrinsic value. In the tabular version of active inference, the preferences of the agent
can be related to the rewards from the reinforcement learning literature. In this paper, we follow (Costa
et al, 2020b) and define the prior preferences as:
exp(ψr [o ])
τ τ
P(o ) = ,
τ (cid:80)
exp(ψr [o ])
oτ τ τ
whereψistheprecisionofthepriorpreferences,andr [o ]istherewardobtainedwhenmakingobservation
τ τ
o . Taking the logarithm of the above equation leads to:
τ
(cid:88)
lnP(o ) = ψr [o ]−ln exp(ψr [o ])
τ τ τ τ τ
oτ
= ψr [o ]+C, (17)
τ τ
where we used the fact that the summation over all o is a normalisation term, i.e., a constant. Using
τ
(17), we can now create an estimate of the extrinsic value as follows:
M M
E (cid:2) lnP(o ) (cid:3) ≈ 1 (cid:88) lnP(o = oˆ ) = 1 (cid:88) ψr [o ]+C,
P θo (oτ|sτ)Q(sτ|at) τ M τ τ M τ τ
i=1 i=1
where oˆ ∼ P (o |s = sˆ ) and sˆ ∼ Q(s |a ). In what follows, we use M = 1 and discard the constant6,
τ θo τ τ τ τ τ t
which leads to a simplified version of the estimate:
E (cid:2) lnP(o ) (cid:3) =∆ ψr [o ] =∆ ψr ,
P
θo
(oτ|sτ)Q(sτ|at) τ τ τ τ
where we simplied the notation by deno in which all our simulations will be ting r [o ] as r . To conclude,
τ τ τ
we have the following estimate for the EFE at time τ = t+1:
G (a ) ≈ D [Q(s |a )||Q (s )]−E (cid:2) lnP(o ) (cid:3)
t+1 t KL t+1 t φs t+1 P
θo
(ot+1|st+1)Q(st+1|at) t+1
≈ D [P (s |s = sˆ,a )||Q (s )]−ψr , (18)
KL θs t+1 t t t φs t+1 t+1
where sˆ ∼ Q (s ), P (s |s ,a ) is known from the generative model, Q (s ) is known from the
t φs t θs t+1 t t φs t+1
variational distribution, the KL-divergence can be estimated using an analytical solution, ψ is a hyper-
parameter modulating the precision of the prior preferences, and r is the reward obtained at time step
t+1
t+1. As shown in Figure 12, the reward at time step t+1 is used to compute the target values that
must be predicted by the critic network.
6. Removing a constant does not influence which policy is the best. Indeed, π∗ =argmax G(π)=argmax G(π)−C.
π π
27
Champion et al.
3.5.3 Other definitions of the EFE at time t+1
In the previous section, we have presented what may be a principled way to estimate the EFE. As will be
discussed later in this paper, this estimate of the EFE was not very fruitful empirically. To explore the
range of alternatives, we also experimented with the following definitions, which contain relatively minor
perturbations of the epistemic value term:
G1 (a ) = H[Q (s )]−H[P (s |s = sˆ,a )]−ψr ,
t+1 t φs t+1 θs t+1 t t t t+1
G2 (a ) = H[P (s |s = sˆ,a )]−H[Q (s )]−ψr ,
t+1 t θs t+1 t t t φs t+1 t+1
G3 (a ) = D [Q (s )||P (s |s = sˆ,a )]−ψr ,
t+1 t KL φs t+1 θs t+1 t t t t+1
where all the entropy terms and the KL-divergence were computed analytically. Also, we experimented
with simply predicting the (negative) expected future reward as follows:
G4 (a ) = −ψr , (19)
t+1 t t+1
which is effectively making the job of the critic identical to the job of the Q-network in the DQN agent
(c.f. Section 2.1 for details). More precisely, they are identical, except for the fact that the Q-network is
taking observations as input, while the critic takes hidden states.
3.6 Deep active inference
In this section, we discuss the full deep active inference (DAI) agent illustrated in Figure 13. Put simply,
thisagentiscomposedoffivedeepneuralnetworks, i.e., theencoder, thedecoder, thetransition, thecritic
and the policy network. The decoder D models P (o |s ) as a product of Bernoulli distributions, i.e.,
θo θo τ τ
P (o |s ) = Bernoulli(o ;oˆ ) where oˆ = D (s ). The transition network T models P (s |s ,a ) as
θo τ τ τ τ τ θo τ θs θs τ+1 τ τ
a Gaussian distribution, i.e., P (s |s ,a ) = N(s |˚µ,˚σ) where ˚µ,ln˚σ = T (s ,a ). The critic G
θs τ+1 τ τ τ+1 θs τ τ θa
outputs a vector containing the predicted expected free energy of each action, which is used to define the
prior over action as P
θa
(a
τ
|s
τ
) = σ[−ζG
θa
(s
τ
,•)], where σ[•] is a softmax function and ζ is the precision
of the prior over actions. With this in mind, the full generative model of the agent is:
T T−1
(cid:89) (cid:89)
P (o ,s ,a ) = P(s ) P (o |s ) P (s |s ,a )P (a |s ),
θ o:T o:T o:T−1 0 θo τ τ θs τ+1 τ τ θa τ τ
τ=0 τ=0
where P(s ) = N(s ;µ ,σ ) is a Gaussian prior over initial hidden states. Let t be the present time
0 0 0 0
step. The DAI agent maintains posterior beliefs over the present states s and action a . The variational
t t
posterior over states Q (s ) is a Gaussian distribution modelled by the encoder E , i.e., Q (s ) =
φs t φs φs t
N(s ;µ,σ) where µ,lnσ = E (o ). The variational posterior over actions Q (a |s ) is a categorical
t φs t φa t t
distribution modelled by the policy network P , i.e., Q (a |s ) = Cat(a ;πˆ) where πˆ = P (s ). The
φa φa t t t φa t
full variational distribution is therefore defined as:
Q (a ,s ) = Q (a |s )Q (s ).
φ t t φa t t φs t
The variational free energy of the DAI agent is derived in a similar way to the VFE of Section 2.2.2, and
is defined as:
Q∗(s ,a ) = argminD [Q (s ,a )||P (o |s )P (s |s ,a )P (a |s )] (20)
φ t t KL φ t t θo t t θs t t−1 t−1 θa t t
Q
φ
(st,at)(cid:124) (cid:123)(cid:122) (cid:125)
variationalfreeenergy
= argmin E (cid:2) D [Q (a |s )||P (a |s )] (cid:3) +D [Q (s )||P (s |s ,a )]
Q
φs
(st) KL φa t t θa t t KL φs t θs t t−1 t−1
Q
φ
(st,at)
−E [lnP (o |s )].
Q
φs
(st) θo t t
28
Deconstructing deep active inference.
Policy πˆ
oˆ oˆ
t t+1
Critic G
Decoder Decoder
(cid:15)
sˆ sˆ
t t+1
˚µ
Transition ˚s t+1
(cid:15) µ lnσ ln˚σ (cid:15) µˆ lnσˆ
a
t
Encoder Encoder
o o
t t+1
Figure 13: This figure illustrates the DAI agent. The only new part is the policy network, which takes
as input the hidden state at time t and ouputs the parameters πˆ of the variational posterior over actions.
Importantly, the DAI takes actions based on the EFE.
The VFE is therefore a function of s , a , and o . Both a , and o can be obtained from the
t−1 t−1 t t−1 t
replay buffer, and s can be sampled from the variational distribution predicted by the encoder network
t−1
when observation o is provided as input. Also, the KL-divergences can be computed analytically,
t−1
the expectations w.r.t Q (s ) can be approximated using a Monte-Carlo estimate, and the logarithm of
φs t
the likelihood mapping reduces to the binary cross entropy because P (o |s ) is a product of Bernoulli
θo t t
distributions. Thus, all the VFE terms can be estimated, and the encoder, decoder, transition and policy
29
Champion et al.
networks can be trained to minimise the VFE using gradient descent. The critic’s weights are optimised
as in Section 3.5 using gradient descent to minimise the smooth L1 norm between the critic’s output
G
θa
(s
t
,•) and the target G-values y(•), i.e., SL1[G
θa
(s
t
,•),y(•)], where the target G-values are defined
as:
(cid:104) (cid:105)
y(a ) = G (a )+γE max Gˆ (s ,a ) ,
t t+1 t Q
φs
(st+1)
at+1∈A
θˆ
a
t+1 t+1
where Q (s ) can be computed by feeding the image o sampled from the replay buffer as input to
φs t+1 t+1
the encoder, γ is a discount factor, and G (a ) is the expected free energy received at time t+1 after
t+1 t
taking action a , i.e.,
t
G (a ) ≈ D [P (s |s = sˆ,a )||Q (s )]−ψr , (21)
t+1 t KL θs t+1 t t t φs t+1 t+1
where sˆ ∼ Q (s ), ψ is a hyperparamter modulating the precision of the prior preferences, and r is
t φs t t+1
the reward obtained at time step t+1. Note, we also experimented with other definitions of the EFE
at time t+1 as presented in Section 3.5.3. Finally, with regard to the action selection performed by the
DAI agent, there are at least four possibilities: (i) select a random action, (ii) select the action with the
highest posterior probability according to the policy network, i.e., a∗ = argmax Q (a |s ), (iii) sample
t at φa t t
an action from the posterior over actions, i.e., a∗ ∼ Q (a |s ), and (iv) use the˚(cid:15)-greedy algorithm with
t φa t t
exponential decay, i.e., random action with probabilty˚(cid:15) or best action with probability 1−˚(cid:15).
4. Results
Inthissection,wediscusstheresultsobtainedbytheDQNagentandeachmodelpresentedinSection3at
solving the dSprites problem. The code that can be used to reproduce all the experiements can be found
onGitHubatthefollowingURL:https://github.com/ChampiB/Challenges_Deep_Active_Inference.
Section 4.1 presents the results obtained by the DQN agent. Section 4.2 presents the VFE obtained by
the VAE agent, and shows the reconstructed images produced by the VAE. Section 4.3 shows the VFE of
the HMM agent as well as the generated sequences of images. Section 4.4 illustrates the VFE obtained
by the CHMM as well as the reward obtained by this model when using different action selection schemes
and different definitions for the EFE. Finally, Section 4.5 dicusses the VFE obtained by the DAI agent,
as well as the rewards obtained by this model. Note, each time CKA is used in the following sections, we
sampled 5K data examples, and we used them to compute all the CKA scores.
4.1 DQN agent
In this section, we report the results obtained from the DQN agent. As shown in Figure 14, the DQN was
able to accumulate a total amount of reward of around 50K. This result confirms the correctness of our
implementation, andgivesusabaselinewhichcanbeusedtoevaluatetheperformanceoftheCHMMand
DAI agents. To better understand the representations learned by a DQN, we compute the CKA scores
between the activations of its layers. We can see in Figure 15 that while the layers closer to the input
retain some similarity with each other, the last two layers learn highly specific representations.
30
Deconstructing deep active inference.
draweR
latoT
Training Iterations
Figure 14: This figure illustrates the cumulated rewards obtained by the DQN agent during the 500K
training iterations.
1_eulaV 2_eulaV 3_eulaV 4_eulaV 5_eulaV 6_eulaV
Value_1
Value_2
Value_3
Value_4
Value_5
Value_6
Model=DQN
NQD=ledoM
1.0
0.8
0.6
0.4
0.2
0.0
Figure 15: Value X labels the X-th layer of the value network, i.e., Value 1 is the closest to the input
and Value 6 is the output layer. We can see that the first three layers of the DQN learn very similar
representations (CKA is close to 1). The representations learned by the fourth layer start to diverge
(CKA is lower), and the last two layers learn highly specific representations that are very different from
the previous layers (CKA is close to 0), but slightly similar to each other.
4.2 VAE agent
In this section, we report the results obtained by the VAE agent. As shown in Figure 16, the VFE
decreases as training progresses. Also, at the end of the 500K training iterations, the VAE is able to
properly reconstruct the images, c.f., Figure 18. Additionally, since the VAE takes random actions in
the environment, the agent was unable to solve the task and accumulated a total amount of reward of
around -7K. Those results suggest our implementation is correct, and gives us a baseline for the amount
of rewards obtained under random play in the dSprites environment.
31
Champion et al.
EFV
Training Iterations
Figure 16: This figure illustrates the variational free energy of the VAE agent during the 500K iterations
of training.
1_redocnE 2_redocnE 3_redocnE 4_redocnE 5_redocnE naem_redocnE ecnairav_redocnE
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Model=VAE
EAV=ledoM
1.0
0.8
0.6
0.4
0.2
0.0
(a)
1_redocnE 2_redocnE 3_redocnE 4_redocnE 5_redocnE naem_redocnE ecnairav_redocnE
Value_1
Value_2
Value_3
Value_4
Value_5
Value_6
Model=VAE
NQD=ledoM
1.0
0.8
0.6
0.4
0.2
0.0
(b)
Figure17: Encoder XistotheX-thlayeroftheencodernetwork,i.e.,Encoder 1istheclosesttotheinput
and Encoder 5 is the one just before the mean and variance layers. Encoder mean and Encoder variance
are the mean and variance layers of the encoder network, respectively. (a) shows the similarity between
the representations learned by different layers of the encoder of a VAE. (b) shows the similarity between
the representations learned by a DQN and a VAE. Note, both the VAE and DQN take images as input
and need to process them to either learn a compact representation and reconstruct the images or predict
the cumulated reward, respectively. Thus, both learn to represent edges and combination of edges in their
first layers.
To observe the representations learned by VAEs, we compute the CKA scores between the activations of
the different layers of the encoder. We can see in Figure 17a that the representations are strongly similar
between all layers, with the exception of the mean and variance representations at the output end (last
two layers), similarly to what was observed in Bonheme and Grzes (2022), and is therefore expected. As
illustrated in Figure 17b, these representations are generally similar to those learned by the DQN in early
layers but the representations of the last two layers strongly differ, reflecting the difference of learning
objectives between the VAE and DQN.
32
Deconstructing deep active inference.
Reconstruction (R):
Ground Truth (GT):
Figure 18: This figure illustrates the reconstructed image produced by the VAE after 500K training
iterations. The columns alternate between the input images and the reconstructed images.
4.3 HMM agent
In this section, we report the results obtained by the HMM agent. As shown in Figure 19, the VFE
decreases as training progresses. By comparing Figure 16 and 19, one can see that the HMM has a
lower VFE than the VAE. This is because the agent has more flexibility regarding the prior, i.e., the
log-likelihood is the same between the two models but the complexity term is smaller for the HMM than
for the VAE. Also, at the end of the 500K training iterations, the HMM is able to properly generate se-
quences of images, c.f., Figure 21. Additionally, since the HMM takes random actions in the environment,
the agent was unable to solve the task and accumulated a total amount of reward of around -7K. These
results suggest that our implementation is correct, and comfirm our baseline for the amount of rewards
obtained under random play in the dSprites environment.
Similarly to VAEs, we are interested in observing the representations learned by the encoder of the
HMM, and also by its transition network. The representations learned by the encoder of the HMM fol-
low the same trend as those learned by VAEs with an even more marked dissimilarity between the log
variance of the HMM and the other representations learned by this model, as illustrated in Figures 20a
and 20b. We can further observe in Figure 20a that the transition network learns representations similar
to the mean representation (Encoder mean of HMM) in its first three layers, while the representations
learned by the last layer (Transition variance) are not similar to any other representations learned by the
transition or encoder networks. We can also see in Figure 20b that the mean and variance representa-
tions (Encoder mean and Encoder variance) learned by HMMs are different from those learned by VAEs,
possibly indicating that the transition network influences these two representations. Similarly to VAEs,
one can observe in Figure 20c, that the representations learned by the variance layers of the encoder and
transition networks (Encoder variance and Transition variance) are very different to the representation
learned by the DQN. In contrast, the first four layers of the encoder (Encoder 1 to Encoder 4) are similar
33
Champion et al.
to the representation learned by the first layers of the DQN (Value 1 to Value 4), but are very different
from the last two layers (Value 5 and Value 6).
EFV
Training Iterations
Figure 19: This figure illustrates the variational free energy of the HMM agent during the 500K iterations
of training.
1_redocnE 2_redocnE 3_redocnE 4_redocnE 5_redocnE naem_redocnE ecnairav_redocnE 1_noitisnarT 2_noitisnarT naem_noitisnarT ecnairav_noitisnarT
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Transition_1
Transition_2 Transition_mean
Transition_variance
Model=HMM
MMH=ledoM
1.0
0.8
0.6
0.4
0.2
0.0
(a)
1_redocnE 2_redocnE 3_redocnE 4_redocnE 5_redocnE naem_redocnE ecnairav_redocnE
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Transition_1
Transition_2 Transition_mean
Transition_variance
Model=VAE
MMH=ledoM
1.0
0.8
0.6
0.4
0.2
0.0
(b)
1_redocnE 2_redocnE 3_redocnE 4_redocnE 5_redocnE naem_redocnE ecnairav_redocnE 1_noitisnarT 2_noitisnarT naem_noitisnarT ecnairav_noitisnarT
Value_1
Value_2
Value_3
Value_4
Value_5 Value_6
Model=HMM
NQD=ledoM
1.0
0.8
0.6
0.4
0.2
0.0
(c)
Figure 20: Transition X is to the X-th layer of the transition network, i.e., Transition 1 is the closest
to the input and Transition 2 is the one just before the mean and variance layers. Transition mean and
Transition variancearethemeanandvariancelayersofthetransitionnetwork, respectively. (a)showsthe
similarity between the representations learned by different layers of the encoder and transition network
of an HMM. (b) shows the similarity between the representations learned by an HMM and a VAE. (c)
shows the similarity between the representations learned by a DQN and an HMM.
34
Deconstructing deep active inference.
Reconstruction (R):
Ground Truth (GT):
Figure 21: This figure illustrates the sequences of reconstructed images generated by the HMM after
500K training iterations. The columns alternate between the ground truth images and the reconstructed
images. Timepassesvertically(fromtoptobottom), andwithineachcolumn, thesameactionisexecuted
repeatedly.
4.4 CHMM agent
In this section, we report the results obtained by the CHMM agent, when using different action selection
strategies and different definitions of the expected free energy. Figure 22 presents the cumulated rewards
obtained by the CHMM agents using an ˚(cid:15)-greedy algorithm for action selection, as well as the total
rewards obtained by the DQN agent. The critic network of the CHMM agents were trained to predict the
five definitions of the expected free energy proposed in Section 3.5. Note, the DQN agent performs better
that any of the CHMM agents, and the only agent that solves the task is the CHMM maximising reward
only, i.e., without any information gain. Figure 23 presents the same experiement as Figure 22 except
that the CHMM agents were using softmax sampling for action selection. In this setting, none of the
CHMM agents were able to solve the task. Finally, Figure 24 presents yet again the same experiements
but this time the CHMM agents were selecting the best action according to the critic. In this setting,
only the CHMM maximising reward was able to solve the task. Also, by comparing Figures 22 and 24,
it becomes clear that the CHMM using an˚(cid:15)-greedy algorithm performs better than the CHMM selecting
the best action according to the critic. Put simply, the latter suffers from a lack of exploration that slows
down its learning.
Additionally, Figure 25 represents the variational free energy of the CHMM agent using the˚(cid:15)-greedy
algorithm. All the agents were able to minimise their variational free energy, except the one displayed in
orange whose VFE suddenly became equal to NaN (i.e., Not a Number); this agent was minimising the
expected free energy as defined by G1. Note, G1 is neither the reward nor the “principled” expected free
energy, G1 is one of definitions that we experimented with to explore alternative definitions. Also, the
35
Champion et al.
variational free energy of the CHMM agents using softmax sampling and best action selection are not
presented, because their results are very similar to the results shown in Figure 25.
Finally, Figure 26 shows examples of predicted trajectories after a CHMM (maximising reward) was
trained. By comparing with Figure 21, we see that the CHMM does not understand the dynamics of the
environment as well as the HMM agent. This sugguests a conflict between the two goals of the agent,
i.e., maximising reward7 (or expected free energy) and learning a model of the world. More precisely,
Figure 21 shows that an HMM agent taking random actions is able to gather a large diversity of training
examples and learns the dynamic of the environment beautifully, but does not solve the task. In contrast,
the CHMM maximising reward solves the task but learns a poor model of the environment, c.f., Figure
26.
Training iterations
draweR
latoT
DQN:
CHMM[G4]:
CHMM[G, G2, G3]:
CHMM[G1]:
Figure 22: This figure illustrates the total amount of reward gathered by the CHMM agents (with ˚(cid:15)-
greedy action selection) during the 500K iterations of training. The only two models that were able to
solve the task are the ones maximising reward (without information gain), i.e., the DQN agent in red and
the CHMM whose critic network was predicting only reward in gray.
7. As shown in Figure 12, the reward is used to compute the target values that must be predicted by the critic network.
36
Deconstructing deep active inference.
Figure23: ThisfigureillustratesthetotalamountofrewardgatheredbytheCHMMagents(withsoftmax
sampling) during the 500K iterations of training. The only model that was able to solve the task is the
DQN agent in red, and all CHMM agents failed.
Figure 24: This figure illustrates the total amount of reward gathered by the CHMM agents (with best
action selection) during the 500K iterations of training. The only two models that were able to solve
the task are the ones maximising reward (without information gain), i.e., the DQN agent in red and the
CHMM whose critic network was predicting only reward in pink.
37
Champion et al.
CHMM[G]:
CHMM[G1]:
CHMM[G2]:
CHMM[G3]:
CHMM[G4]:
Training iterations
EFV
Figure 25: This figure illustrates the variational free energy of the CHMM agents during the 500K
iterations of training. All the agents were able to minimise their variational free energy, except the one
displayed in orange which crashed; this agent was minimising the expected free energy as defined by G1.
Moreprecisely, thevariationalfreeenergytookthevalue“NotaNumber”(NaN),whichisvisiblebecause
of the thick horizontal line between 270K and 500K training iterations.
Reconstruction (R):
Ground Truth (GT):
Figure 26: This figure illustrates the sequences of reconstructed images generated by a CHMM (max-
imising reward) after 500K training iterations. The columns alternate between the ground truth images
and the reconstructed images. Time passes vertically (from top to bottom), and within each column, the
same action is executed repeatedly.
38
Deconstructing deep active inference.
4.4.1 How do CHMMs learn?
CHMM with ˚(cid:15)-greedy action selection As illustrated in Figure 22, only the CHMM whose critic
maximises the reward was able to solve the task. We could thus expect the representations learned by
this CHMM to be closer to those learned by the DQN than those learned by the other CHMMs. We can
see in Figure 28 that the last two layers of the critic network of the CHMM maximising the reward are
indeedabitmoresimilartotherepresentationsofthelasttwolayersoftheDQNthantherepresentations
learned when the critic is minimising the EFE (see intersection of Value 5 and Value 6 with Critic 3 and
Critic 4, i.e., bottom right corner of the matrix). However, the representations learned by the critic of
both CHMMs are still quite different from the last two layers of the DQN (CKA is lower than 0.4, bottom
right corner of the matrix, again). Interestingly, the first four layers of the CHMM maximising the reward
retain a high similarity with the earlier layers of the DQN (4×4 region at upper left), suggesting some
common representations between models. We can further see in Figure 27a that the CKA score between
the encoder, transition and critic networks is higher or equal to 0.6 (except for the variance layer of the
transition and the last layer of the critic), indicating that the transition and critic networks of the CHMM
maximising the reward retain some information from the encoder. The information retained by the first
three layers of the critic when the CHMM minimises the EFE is much lower, as illustrated in Figures 27b.
1_redocnE 2_redocnE 3_redocnE 4_redocnE 5_redocnE naem_redocnE ecnairav_redocnE 1_noitisnarT 2_noitisnarT naem_noitisnarT ecnairav_noitisnarT 1_citirC 2_citirC 3_citirC 4_citirC
Encoder_1
Encoder_2
Encoder_3 Encoder_4
Encoder_5
Encoder_mean Encoder_variance
Transition_1
Transition_2 Transition_mean
Transition_variance
Critic_1 Critic_2
Critic_3
Critic_4
Model=CHMM, Action=-greedy,
Gain=Reward
,ydeerg-=noitcA
,MMHC=ledoM
draweR=niaG
1.0
0.8
0.6
0.4
0.2
0.0
(a)
1_redocnE 2_redocnE 3_redocnE 4_redocnE 5_redocnE naem_redocnE ecnairav_redocnE 1_noitisnarT 2_noitisnarT naem_noitisnarT ecnairav_noitisnarT 1_citirC 2_citirC 3_citirC 4_citirC
Encoder_1
Encoder_2
Encoder_3 Encoder_4
Encoder_5
Encoder_mean Encoder_variance
Transition_1
Transition_2 Transition_mean
Transition_variance
Critic_1 Critic_2
Critic_3
Critic_4
Model=CHMM, Action=-greedy,
Gain=EFE
,ydeerg-=noitcA
,MMHC=ledoM
EFE=niaG
1.0
0.8
0.6
0.4
0.2
0.0
(b)
1_redocnE 2_redocnE 3_redocnE 4_redocnE 5_redocnE naem_redocnE ecnairav_redocnE 1_noitisnarT 2_noitisnarT naem_noitisnarT ecnairav_noitisnarT 1_citirC 2_citirC 3_citirC 4_citirC
Encoder_1
Encoder_2
Encoder_3 Encoder_4
Encoder_5
Encoder_mean Encoder_variance
Transition_1
Transition_2 Transition_mean
Transition_variance
Critic_1 Critic_2
Critic_3
Critic_4
Model=CHMM, Action=-greedy,
Gain=EFE
,ydeerg-=noitcA
,MMHC=ledoM
draweR=niaG
1.0
0.8
0.6
0.4
0.2
0.0
(c)
Figure27: (a)showsthesimilaritybetweentherepresentationslearnedbydifferentlayersoftheencoder,transition
andcriticnetworksofaCHMMwhosecriticmaximisesthereward(with˚(cid:15)-greedyselection). (b)showsthesimilarity
between the representations learned by different layers of the encoder, transition and critic networks of a CHMM
whosecriticminimisestheEFE(with˚(cid:15)-greedyselection)(c)showsthesimilaritybetweentherepresentationslearned
by two CHMMs, one whose critic optimises EFE and the other optimises reward (both with˚(cid:15)-greedy selection).
1_redocnE 2_redocnE 3_redocnE 4_redocnE 5_redocnE naem_redocnE ecnairav_redocnE 1_noitisnarT 2_noitisnarT naem_noitisnarT ecnairav_noitisnarT 1_citirC 2_citirC 3_citirC 4_citirC
Value_1
Value_2
Value_3
Value_4
Value_5
Value_6
Model=CHMM, Action=-greedy,
Gain=Reward
NQD=ledoM
1.0
0.8
0.6
0.4
0.2
0.0
(a)
1_redocnE 2_redocnE 3_redocnE 4_redocnE 5_redocnE naem_redocnE ecnairav_redocnE 1_noitisnarT 2_noitisnarT naem_noitisnarT ecnairav_noitisnarT 1_citirC 2_citirC 3_citirC 4_citirC
Value_1
Value_2
Value_3
Value_4
Value_5
Value_6
Model=CHMM, Action=-greedy,
Gain=EFE
NQD=ledoM
1.0
0.8
0.6
0.4
0.2
0.0
(b)
Figure 28: (a) shows the similarity between the representations learned by a CHMM whose critic maximises the
reward (with ˚(cid:15)-greedy selection) and a DQN; (b) shows the similarity between the representations learned by a
CHMM whose critic minimises the EFE (with˚(cid:15)-greedy selection) and a DQN
39
Champion et al.
CHMM with best action selection As illustrated in Figure 24, only the CHMM whose critic max-
imises the reward was able to solve the task. However, one can see in Figure 29 that the CHMM
whose critic minimises the EFE learns representations similar to those of the CHMM maximising the
reward in most layers, with the exception of the variance layer of the encoder and transition network
(Encoder variance and Transition variance). To better understand the differences between the represen-
tations learned by the variance layer of both models, we fed 5K state-action pairs through the transition
network, anddisplayedthedistributionofthevariancesoutputedbythetransitionnetwork. Thisanalysis
revealsthatthevariance(ofthevariancelayer)ofthetransitionnetworkisverysmallanddoesnotchange
much when maximising the reward but is larger and varies more when minimising the EFE as illustrated
in Figure 30. This reflects a higher uncertainty of the transitions for the CHMM minimising EFE. More
specifically, the CHMM minimising EFE seems to be confident for the action down, but is very uncertain
for all the other actions, which suggests that the CHMM minimising EFE always picks the action down
and does not gather enough data for the other actions.
1_redocnE 2_redocnE 3_redocnE 4_redocnE 5_redocnE naem_redocnE ecnairav_redocnE 1_noitisnarT 2_noitisnarT naem_noitisnarT ecnairav_noitisnarT 1_citirC 2_citirC 3_citirC 4_citirC
Encoder_1
Encoder_2
Encoder_3 Encoder_4
Encoder_5
Encoder_mean Encoder_variance
Transition_1
Transition_2 Transition_mean
Transition_variance
Critic_1 Critic_2
Critic_3
Critic_4
Model=CHMM, Action=Best Action,
Gain=Reward
,noitcA
tseB=noitcA
,MMHC=ledoM
draweR=niaG
1.0
0.8
0.6
0.4
0.2
0.0
(a)
1_redocnE 2_redocnE 3_redocnE 4_redocnE 5_redocnE naem_redocnE ecnairav_redocnE 1_noitisnarT 2_noitisnarT naem_noitisnarT ecnairav_noitisnarT 1_citirC 2_citirC 3_citirC 4_citirC
Encoder_1
Encoder_2
Encoder_3 Encoder_4
Encoder_5
Encoder_mean Encoder_variance
Transition_1
Transition_2 Transition_mean
Transition_variance
Critic_1 Critic_2
Critic_3
Critic_4
Model=CHMM, Action=Best Action,
Gain=EFE
,noitcA
tseB=noitcA
,MMHC=ledoM
EFE=niaG
1.0
0.8
0.6
0.4
0.2
0.0
(b)
1_redocnE 2_redocnE 3_redocnE 4_redocnE 5_redocnE naem_redocnE ecnairav_redocnE 1_noitisnarT 2_noitisnarT naem_noitisnarT ecnairav_noitisnarT 1_citirC 2_citirC 3_citirC 4_citirC
Encoder_1
Encoder_2
Encoder_3 Encoder_4
Encoder_5
Encoder_mean Encoder_variance
Transition_1
Transition_2 Transition_mean
Transition_variance
Critic_1 Critic_2
Critic_3
Critic_4
Model=CHMM, Action=Best Action,
Gain=EFE
,noitcA
tseB=noitcA
,MMHC=ledoM
draweR=niaG
1.0
0.8
0.6
0.4
0.2
0.0
(c)
Figure29: (a)showsthesimilaritybetweentherepresentationslearnedbydifferentlayersoftheencoder,transition
and critic networks of a CHMM whose critic maximises the reward (with best action selection). (b) shows the
similarity between the representations learned by different layers of the encoder, transition and critic networks
of a CHMM whose critic minimises the EFE (with best action selection) (c) shows the similarity between the
representationslearnedbytwoCHMMs,onewhosecriticoptimisesEFEandtheotherthatoptimisesreward(both
with best action selection).
Variance of the latent variable at index 9
(a) (b)
Figure30: (a)showsonelatentdimensionofthevariancelayerofthetransitionnetworkfortheCHMMmaximising
thereward. (b)showsonelatentdimensionofthevariancelayerofthetransitionnetworkfortheCHMMminimising
the EFE. Both figures are typical of the distributions of variance activations in the two models. Note, only the
action down has low variance for the CHMM minimising the EFE. This suggests that the CHMM minimising the
EFE always picks the action down, and does not gather enough data for the other actions.
40
Deconstructing deep active inference.
CHMM with softmax action selection As illustrated in Figure 23, none of the CHMMs with
softmaxactionselectionwereabletosolvethetask. Onceagain, thevariance(ofthevariancelayer)ofthe
transitionnetworkisverydifferentintheCHMMwhosecriticminimisestheEFEcomparedtotheCHMM
whose critic maximises the reward (see Figure 31c at the intersection of the two Transition variances).
Wefurtherobservethesametrendregardingtheuncertaintyoftheoutputofthetransitionnetworkwhen
optimising the EFE, as shown in Figure 32. While this may explain why the model optimising the EFE
does not solve the task, this does not indicate why the model maximising the reward cannot solve the
task, and we can hypothesise that those results may be attributed to the softmax action selection. More
precisely, if the values predicted by the critic network are very close to each other, then an agent using
softmax sampling may perform random actions.
1_redocnE 2_redocnE 3_redocnE 4_redocnE 5_redocnE naem_redocnE ecnairav_redocnE 1_noitisnarT 2_noitisnarT naem_noitisnarT ecnairav_noitisnarT 1_citirC 2_citirC 3_citirC 4_citirC
Encoder_1
Encoder_2
Encoder_3 Encoder_4
Encoder_5
Encoder_mean Encoder_variance
Transition_1
Transition_2 Transition_mean
Transition_variance
Critic_1 Critic_2
Critic_3
Critic_4
Model=CHMM, Action=Softmax,
Gain=Reward
,xamtfoS=noitcA
,MMHC=ledoM
draweR=niaG
1.0
0.8
0.6
0.4
0.2
0.0
(a)
1_redocnE 2_redocnE 3_redocnE 4_redocnE 5_redocnE naem_redocnE ecnairav_redocnE 1_noitisnarT 2_noitisnarT naem_noitisnarT ecnairav_noitisnarT 1_citirC 2_citirC 3_citirC 4_citirC
Encoder_1
Encoder_2
Encoder_3 Encoder_4
Encoder_5
Encoder_mean Encoder_variance
Transition_1
Transition_2 Transition_mean
Transition_variance
Critic_1 Critic_2
Critic_3
Critic_4
Model=CHMM, Action=Softmax,
Gain=EFE
,xamtfoS=noitcA
,MMHC=ledoM
EFE=niaG
1.0
0.8
0.6
0.4
0.2
0.0
(b)
1_redocnE 2_redocnE 3_redocnE 4_redocnE 5_redocnE naem_redocnE ecnairav_redocnE 1_noitisnarT 2_noitisnarT naem_noitisnarT ecnairav_noitisnarT 1_citirC 2_citirC 3_citirC 4_citirC
Encoder_1
Encoder_2
Encoder_3 Encoder_4
Encoder_5
Encoder_mean Encoder_variance
Transition_1
Transition_2 Transition_mean
Transition_variance
Critic_1 Critic_2
Critic_3
Critic_4
Model=CHMM, Action=Softmax,
Gain=EFE
,xamtfoS=noitcA
,MMHC=ledoM
draweR=niaG
1.0
0.8
0.6
0.4
0.2
0.0
(c)
Figure 31: (a) shows the similarity between the representations learned by different layers of the encoder,
transition and critic networks of a CHMM whose critic maximises the reward (with softmax action se-
lection). (b) shows the similarity between the representations learned by different layers of the encoder,
transitionandcriticnetworksofaCHMMwhosecriticminimisestheEFE(withsoftmaxactionselection).
(c) shows the similarity between the representations learned by two CHMMs, one whose critic optimises
EFE and the other that optimises reward (both with softmax action selection).
4.4.2 Degenerate behaviour with the expected free energy?
Up to now, we saw that the CHMM minimising expected free energy (EFE) was not able to solve the
task. Also, we discovered that the transition network is uncertain for the actions: up, left, and right,
which suggests that the CHMM minimising EFE always takes action down. Figure 33 corroborates this
story. Indeed, Figure 33a shows that the agent minimising EFE almost exclusively picked action down,
and Figure 33b shows that the entropy of the prior over actions very quickly converges to zero.
Incontrast,theCHMMmaximisingreward,keepsonselectingtheactionsrightandleft,whichenables
it to drag the shape towards the appropriate corner (see Figure 33c). Also, as shown in Figure 33d, the
entropy of the prior over actions remained a lot higher than zero. Note, the only difference between the
CHMM minimising EFE and the one maximising reward is the information gain, which is defined as the
KL divergence between the output of the transition and encoder networks. Since the EFE is minimised,
the output of those two networks need to be as close as possible to each other.
41
Champion et al.
Variance of the latent variable at index 9
Variance of the latent variable at index 9
in which all our simulations will be
(a)
(b)
Figure32: (a)showsonelatentdimensionofthevariancelayerofthetransitionnetworkfortheCHMMmaximising
thereward. (b)showsonelatentdimensionofthevariancelayerofthetransitionnetworkfortheCHMMminimising
the EFE. Both figures are representative of the distributions of variance activations in the two models. Note, only
the action down has low variance for the CHMM minimising the EFE. This suggests that the CHMM minimising
the EFE always pick the action down, and does not gather enough data for the other actions.
1.4
1.2
1.0
0.8
0.6
0.4
0.2
0.0
0 100000 200000 300000 400000 500000
Training iterations
(a)
snoitca
revo
roirp
fo
yportnE
(b)
1.38
Down
1.36
Right 1.34
1.32
Left 1.30
1.28
Up 1.26
0 100000 200000 300000 400000 500000 0 100000 200000 300000 400000 500000
Training iterations Training iterations
(c)
snoitca
revo
roirp
fo
yportnE
(d)
Figure 33: (a) shows the action taken for each planning iteration when the CHMM is minimising expected free
energy. (b) shows the entropy of the prior over actions when the CHMM is minimising the EFE. (c) shows the
action taken for each planning iteration when the CHMM is maximising reward. (d) shows the entropy of the prior
over actions when the CHMM is maximising reward.
42
Deconstructing deep active inference.
This suggests that the CHMM minimising EFE is picking a single action (down), and becomes an
expert at predicting the future when selecting this action. This effectively makes the KL divergence
between the output of the transition and encoder networks small. Additionally, when selecting the action
down, the average reward is zero, because (in the dSprites dataset) there are as many shapes on the left
of the image as on the right, and when crossing the bottom line, the agent receives a reward which is
linearly increasing (or decreasing) as a corner is approached and is zero at the center of the image. For all
the other actions, the expected reward will be negative because after 50 action-perception cycles without
crossing the bottom line, the trial is interrupted and the agent receive a reward of -1. Thus, if the CHMM
has to stick to a single action to keep the KL divergence small, then the best action it can choose is down,
i.e., action down has the highest expected reward.
Also, Figure 34 shows the impact of adding X% of the information gain into the objective function,
i.e., the agent starts by only maximising reward (c.f. Equation 19), and after 200K training iterations
minimises reward plus X% of the information gain. One can see that adding even 1% of the information
gain already dramatically decreases the amount of reward gathered.
To conclude, the same information gain that is intended to give an EFE minimising agent its explo-
ration behaviour, also prevents the agent from solving the dSprites environment. This is because the
agent is reduced to picking a single action, leading to a suboptimal policy.
CHMM[50%]:
CHMM[25%]:
CHMM[15%]:
CHMM[5%]:
CHMM[1%]:
CHMM[0%]:
Training iterations
draweR
latoT
Figure34: ThisfigureillustratesthetotalrewardaggregatedbyCHMMagentsduringthe500Kiterations
of training. All the agents start by only maximising reward, and after 200K training iterations, X% of
the information gain is added to the objective function. Note, even adding 1% of the information gain
is enough to drastically reduce the total reward aggregated by the agent. The differences in trajectories
before 200K are arbitrary, arising from differences in random initializations.
4.5 DAI agent
In this section, we report the results obtained by the DAI agent, when using different action selection
strategies and different definitions of the expected free energy. First, most of the fifteen DAI agents
crashed because of numerical instability, i.e., the VFE suddenly became “Not a Number”. The only DAI
agent that survived (i.e., did not crash) was maximising rewards while performing softmax sampling for
action selection. Figure 38 shows that the DAI agent successfully minimises its variational free energy,
but as shown in Figure 37, the DAI agent does not solve the task and performs as well as a random agent.
Finally, Figure 35 shows sequences of images produced by the DAI agent after 500K training iterations.
43
Champion et al.
Note, while the agent does not solve that task, it understands the dynamics of the environment pretty
well. However, the agent struggles with images representing hearts.
By comparing Figures 21, 26 and 35, we see that the DAI agent with softmax sampling has a better
reconstruction than the CHMM agent with the ˚(cid:15)-greedy algorithm (which is presented in Figure 26).
In contrast, the DAI agent does not reconstruct the sequences of images as well as the HMM agent
performing random actions (which is presented in Figure 21).
Reconstruction (R):
Ground Truth (GT):
Figure 35: This figure illustrates the sequences of reconstructed images generated by the DAI after
500K training iterations. The columns alternate between the ground truth images and the reconstructed
images. Timepassesvertically(fromtoptobottom), andwithineachcolumn, thesameactionisexecuted
repeatedly.
1_redocnE 2_redocnE 3_redocnE 4_redocnE 5_redocnE naem_redocnE ecnairav_redocnE 1_noitisnarT 2_noitisnarT naem_noitisnarT ecnairav_noitisnarT 1_citirC 2_citirC 3_citirC 4_citirC
Value_1
Value_2 Value_3
Value_4
Value_5
Value_6
Model=CHMM, Action=Softmax,
Gain=Reward
NQD=ledoM
1.0
0.8 0.6
0.4
0.2
0.0
(a)
1_redocnE 2_redocnE 3_redocnE 4_redocnE 5_redocnE naem_redocnE ecnairav_redocnE 1_noitisnarT 2_noitisnarT naem_noitisnarT ecnairav_noitisnarT 1_yciloP 2_yciloP 3_yciloP 4_yciloP 1_citirC 2_citirC 3_citirC 4_citirC
Value_1
Value_2 Value_3
Value_4
Value_5
Value_6
Model=DAI, Action=Softmax,
Gain=Reward
NQD=ledoM
1.0
0.8 0.6
0.4
0.2
0.0
(b)
1_redocnE 2_redocnE 3_redocnE 4_redocnE 5_redocnE naem_redocnE ecnairav_redocnE 1_noitisnarT 2_noitisnarT naem_noitisnarT ecnairav_noitisnarT 1_yciloP 2_yciloP 3_yciloP 4_yciloP 1_citirC 2_citirC 3_citirC 4_citirC
Encoder_1
Encoder_2
Encoder_3 Encoder_4 Encoder_5 Encoder_mean Encoder_variance
Transition_1
Transition_2 Transition_mean
Transition_variance
Critic_1 Critic_2
Critic_3
Critic_4
Model=DAI, Action=Softmax,
Gain=Reward
,xamtfoS=noitcA
,MMHC=ledoM
draweR=niaG
1.0
0.8 0.6
0.4
0.2
0.0
(c)
Figure36: (a)showsthesimilaritybetweentherepresentationslearnedbyaDQNandaCHMMmaximis-
ing the reward and using softmax action selection. (b) shows the similarity between the representations
learned by a DQN and a DAI maximising the reward and using softmax action selection. (c) shows the
similarity between the representations learned by a CHMM and a DAI. Both maximise the reward and
use softmax action selection.
44
Deconstructing deep active inference.
Aspreviouslymentioned,theonlyDAIthatdidnotcrashduringtrainingusedsoftmaxactionselection
andhadacriticmaximisingreward. WecanseeinFigures36band36athattherepresentationalsimilarity
between this DAI and DQN is very close to the representational similarity between a DQN and a CHMM
using the same action selection and maximising reward. This is further confirmed by a comparison
between the CHMM and the DAI model in Figure 36c. Interestingly, we can see that the policy and critic
network learn similar representations, indicating that the policy network is learning correctly. However,
we previously inferred that the softmax action selection may be suboptimal and this seems to hold true
for the DAI as well, given that it is unable to solve the task.
sdrawer
latoT
DQN
DAI[G4]
Training Iterations
Figure37: ThisfigureillustratesthetotalamountofrewardgatheredbyaDAIagentduringthe500Kiterationsof
training. This agent was maximising rewards while sampling actions from a softmax function of the policy network
output. Put simply, the DAI agent does not solve the task and performs at the level of a random agent.
EFV
TrainingIterations
Figure38: ThisfigureillustratesthevariationalfreeenergyoftheDAIagentduringthe500Kiterationsoftraining.
This agent was maximising reward while sampling actions from a softmax function of the policy network output.
The agent was able to minimise its variational free energy.
45
Champion et al.
5. Discussion of epistemic value
In this section, we discuss the decomposition of the expected free energy into epistemic and extrinsic
value. More precisely, intrigued by the poor results of deep active inference agents (e.g., CHMM and
DAI), we seek to understand the damaging impact of the epistemic value on performance. Assuming the
following definition for the epistemic value:
EV = E (cid:2) lnP(s |o )−lnQ(s |π) (cid:3) ,
P(oτ|sτ)Q(sτ|π) τ τ τ
we set up two experiments (c.f., Appendix C for details) in which all the disributions are stored in tables.
In other words, the following categorical distributions are represented using matrices: P(o |s ), Q(s |π),
τ τ τ
and P(s |o ).
τ τ
In the first experiement, the prior over states P(s ) is fixed, and the likelihood P(o |s ) is becoming
τ τ τ
more and more uniform. While this is happening, the true posterior P(s |o ) becomes more similar to
τ τ
the approximate posterior Q(s |π), and the epistemic value decreases; see left panel of Figure 39. This is
τ
the expected behaviour, and in this setting, the epistemic value encourages exploration.
In the second experiment, the likelihood P(o |s ) is fixed and has rather high entropy, while the
τ τ
prior over states P(s ) is shifted in one direction on the state axis. As a result of this, the true posterior
τ
P(s |o )becomesmoredifferenttotheapproximateposteriorQ(s |π), butthiscausestheepistemicvalue
τ τ τ
to decrease; see Figure 39, right panel. This is a degenerate behaviour, as in this setting, the epistemic
value discourages exploration.
To sum up, the expected free energy decomposition into epistemic and extrinsic value — as presented
in Equation (10) of Parr and Friston (2019) — seems to exhibit two very different behaviours depending
on how the distributions are defined, c.f., Figure 39. This is particularly important in the deep active
inference literature, which builds on this equation. For example, the graphs presented in Section 4.4.2
indicate that the CHMM agent minimising expected free energy is focusing almost exclusively on the
action “down”, i.e., it is not exploring, which leads to poor performance.
6. Conclusion
In this paper, we challenged the common assumption that deep active inference is a solved problem, by
highlighting the challenges that need to be resolved for the field to move forward. We reviewed eight
approaches implementing deep active inference: (a) DAI by Fountas et al (2020), (b) DAI by
MC VPG
Millidge (2020), (c) DAI by Rood et al (2020), (d) DAI by Sancaktar et al (2020), (e) DAI by
RHI HR FA
Ueltzh¨offer (2018), (f) DAI by van der Himst and Lanillos (2020), (g) DAI by C¸atal et al
POMDP SSM
(2020), and (h) the approach by Schneider et al (2022) for which the code was not available online.
Overall, those approaches brought interesting ideas such as: using deep neural networks to predict the
parameters of the distributions of interest, using Monte-Carlo tree search for planning in active inference,
and using a bootstrap estimate of the expected free energy to train a critic network. Yet, we struggled
to replicate some of the results claimed, e.g. training DAI on the animal AI environment, and we
MC
were unable to access code in some instances. Ideally, future research should draw inspiration from the
open science framework, by making the code that produced the claimed results open source. Also, the
definition of the expected free energy varied between papers. This suggests that additional research is
required to clarify the definition and justification of the expected free energy both in tabular and deep
active inference (c.f., Section 5 and Appendix C). To sum up, recent research on deep active inference
(Fountas et al, 2020; Millidge, 2020; Rood et al, 2020; Sancaktar et al, 2020; Ueltzh¨offer, 2018; van der
Himst and Lanillos, 2020; C¸atal et al, 2020) has made an important first step towards a complete deep
active inference agent, but a number of details still need to be honed, e.g., the definition and derivation
of the expected free energy, and reproducibilty of research. For more details, the reader is referred to
Section 2.
46
Deconstructing deep active inference.
eulaV
cimetsipE
Similarity of posteriors: P(s |o ) and Q(s |π)
τ τ τ
eulaV
cimetsipE
Similarity of posteriors: P(s |o ) and Q(s |π)
τ τ τ
Figure39: Theleft-mostfigureshowstheresultofthefirstexperimentwherethelikelihoodwasbecoming
more and more uniform. In this setting, the epistemic value encourages exploration, as maximising the
epistemic value makes the true posterior P(s |o ) as different as possible to the approximate posterior
τ τ
Q(s |π), leftward on x-axis. However, the right-most figure illustrates the result of the second experiment
τ
wheretheprioroverstateswaschanging. Inthiscase,theepistemicvaluedoesnotpromoteexploration,as
maximisingtheepistemicvaluemakesthetrueposteriorP(s |o )assimilaraspossibletotheapproximate
τ τ
posterior Q(s |π), rightward on x-axis. Indeed, in this case, maximising the epistemic value causes
τ
information to be lost.
After reviewing existing research, we tried to progressively implement a deep active inference agent.
First, we produced a variational auto-encoder agent that takes random actions. This agent was able to
learn a useful (latent) representation of the task. However, since the agent takes random actions, it was
unable to solve the task. Then, we added the transition network to create the hidden Markov model
(HMM) agent, which also takes random actions. The agent was able to learn a good represenation of the
task and of its dynamics, but was not able to solve the task.
Next, we tried to incorporate a critic network into the approach leading to the critical hidden Markov
model (CHMM). In this context, we experimented with several possible implementations of the expected
free energy. We also tried to remove the information gain and simply predict the reward. Additionally,
we implemented three types of action selection strategies, namely: best action according to the expected
free energy, softmax sampling, and epsilon-greedy.
When the epsilon-greedy algorithm was used, only the agent maximising reward was able to solve the
task. However,theagentrequiresmoretrainingiterationsthanasimpledeepQ-networktolearntheright
behaviour. This may be explained by the fact that the CHMM not only has to learn to solve the task,
but also learn the dynamics of the environment. When softmax sampling was used, all the agents failed
to solve the task. One of them perfoming even worse than an agent selecting random actions. Lastly,
when selecting the best action, only the reward maximising agent was able to solve the task. Importantly,
accordingtoourexperiments, theagentusingtheepsilon-greedyalgorithmreceivedthehighestamountof
cumulated rewards and learned to solve the task the fastest. Additionally, the reward maximising agents
properly solve the task, but the quality of their latent representation is not as good as that of an HMM
agent. This may be due to the fact that when performing reward maximising actions, the data available
to learn the model of the environment lacks diversity, i.e., not enough exploration.
Next, we tried to incorporate a policy network into the approach leading to a complete deep active
inference agent (DAI). As for the CHMM agent, we experiemented with several possible implementations
47
Champion et al.
of the expected free energy, tried to remove the information gain and simply predict the reward, and
implemented three types of action selection strategies, namely: best action according to the expected free
energy, softmax sampling, and epsilon-greedy algorithm. When the epsilon-greedy algorithm was used or
the best action was selected, all agents failed to solve the task. When using softmax sampling, most of
the agents were numerically unstable and crashed, and the remaining agents failed to solve the task.
Finally, we compared the similarity of the representation learned by the layers of various models (e.g.,
deep Q-network, CHMM, DAI, etc...) using centered kernel alignment. This reveals that the DQN learns
general features in its first few layers, and very specialised features in its last two layers. The VAE learns
similar features to the DQN in the first layers, but differs from the DQN in the last two layers, reflecting
the difference in learning objectives. Similarly, the HMM learns similar features to the DQN in the first
layers, but differs from the representation learned by the DQN in the last two layers. Also, the mean and
variance representations learned by the HMM are different from their VAE counterparts, which suggests
that the transition network influences the latent representation of the model.
Additionally, when using the˚(cid:15)-greedy algorithm for action selection, the representations learned by
the CHMM maximising reward is closer to the DQN than the CHMM minimising expected free energy is
to the DQN. Importantly, the critic network of the reward maximising CHMM retains more information
from the encoder than the CHMM minimising expected free energy. When the best action (according
to the critic network) is selected, the CHMM maximising reward and the CHMM minimising expected
free energy learn very similar representations except for the variance layers of the transition and encoder
network. While performing further inspection of those (variance) layers, we found that the transition
network of the reward maximising CHMM is a lot more certain than the transition network of the
CHMM minimising expected free energy. More precisely, the CHMM minimising expected free energy is
only confident about the world transition when performing action down. This suggests that the CHMM
minimising expected free energy always picks the action down, and does not gather enough data for the
other actions. Visualising the distribution of actions selected as training progresses corroborates this
story by showing that the agent minimising EFE almost exclusively picks action down. In contrast, the
CHMM maximising reward, keeps on selecting the actions left and right, which enables it to successfully
solve the task. The only difference between those two CHMMs is the epistemic value, which aims to make
the outputs of the transition and encoder network as close as possible. Thus, the CHMM minimising
expected free energy is picking a single action (down), and becomes an expert at predicting the future
when selecting this action. This effectively makes the KL divergence between the output of the transition
and encoder networks small. Additionally, when selecting the action down, the average reward is zero,
while for all the other actions, the expected reward will be negative. Therefore, if the CHMM has to stick
to a single action to keep the KL divergence small, then the action down is the most rewarding.
The same observations about the variance layers also applies to CHMMs using softmax sampling for
action selection. While this may explain why the model optimising the expected free energy does not
solve the task, it does not explain why the model maximising the reward cannot solve the task, and we
can hypothesise that those results may be due to the softmax action selection. More precisely, if the
values predicted by the critic network are very close to each other, then an agent using softmax sampling
may perform random actions. Note, increasing the gain parameter may help the agent to differentiate
between values close to each other.
In addition, the representational similarity between the DAI (maximising reward using softmax sam-
pling)andDQNisveryclosetotherepresentationalsimilaritybetweenaDQNwithaCHMM(maximising
reward using softmax sampling). Also, the DAI’s policy and critic network learn similar representations,
which indicates that the policy network is learning correctly. Thus, the fact that the DAI (maximising
reward using softmax sampling) fails in the dSprites environment is likely due to the softmax action
selection and not to the representation learned by the model.
Lastly, ourinvestigationsoftheexpectedfreeenergyoftenusedindeepactiveinference(Fountasetal,
2020), suggests that degenerate behaviour can arise from it, in certain situations. This could explain why
48
Deconstructing deep active inference.
adding epistemic value to our planning objective seems to have such a damaging impact on the agent’s
capacity to explore its environment and gain information. The use of this definition of the expected free
energy in the deep learning context may be at the core of our difficulty getting deep active inference to
work and may also explain some of the presentational uncertainties (e.g., additions of minus signs) found
in the deep active inference literature.
To conclude, the field of deep active inference has benefited from a large variety of ideas from the
reinforcement and deep learning literature. In the future, it would be valuable to provide an approach
that satisfies the following five desirata: (i) the approach is complete, i.e., it is composed of an encoder,
a decoder, a transition network, a policy network and (optionally) a critic network, (ii) the mathematics
underlyingtheapproachiserrorlessandconsistentwiththefreeenergyprinciple, (iii)theimplementation
is consistent with the mathematics, (iv) the code is publicly available so that the correctness of the
implementation can be verified and the results reproduced, and (v) the approach is able to solve tasks
with a large input space, e.g. image-based tasks. We believe that such an approach will benefit the field
of deep active inference by providing a strong and reproducible baseline against which future research
could benchmark.
Acknowledgments
We thank Karl J. Friston, Thomas Parr, Lancelot Da Costa, and Zafeirios Fountas for useful discussions
and feedback surronding this paper, as well as for pointing us towards important resources such as their
new book and several papers.
References
Bellman R (1952) On the theory of dynamic programming. Proc Natl Acad Sci U S A 38(8):716–719
Bonheme L, Grzes M (2022) How do Variational Autoencoders Learn? Insights from Representational
Similarity. arXiv e-prints arXiv:2205.08399, 2205.08399
Browne CB, Powley E, Whitehouse D, Lucas SM, Cowling PI, Rohlfshagen P, Tavener S, Perez D,
Samothrakis S, Colton S (2012) A survey of monte carlo tree search methods. IEEE Transactions on
Computational Intelligence and AI in Games 4(1):1–43
C¸atal O, Wauthier S, De Boom C, Verbelen T, Dhoedt B (2020) Learning generative state space models
for active inference. Frontiers in Computational Neuroscience 14:574,372
ChampionT,Grze´sM,BowmanH(2021)RealizingActiveInferenceinVariationalMessagePassing: The
Outcome-Blind Certainty Seeker. Neural Computation 33(10):2762–2826, DOI 10.1162/neco a 01422,
URL https://doi.org/10.1162/neco_a_01422, https://direct.mit.edu/neco/article-pdf/33/
10/2762/1963368/neco_a_01422.pdf
Champion T, Bowman H, Grze´s M (2022a) Branching time active inference: Empirical study and com-
plexityclassanalysis.NeuralNetworks152:450–466,DOIhttps://doi.org/10.1016/j.neunet.2022.05.010,
URL https://www.sciencedirect.com/science/article/pii/S0893608022001824
Champion T, Da Costa L, Bowman H, Grze´s M (2022b) Branching time active inference: The theory and
its generality. Neural Networks 151:295–316, DOI https://doi.org/10.1016/j.neunet.2022.03.036, URL
https://www.sciencedirect.com/science/article/pii/S0893608022001149
Champion T, Grze´s M, Bowman H (2022c) Branching Time Active Inference with Bayesian Filtering.
Neural Computation 34(10):2132–2144, DOI 10.1162/neco a 01529, URL https://doi.org/10.1162/
49
Champion et al.
neco_a_01529, https://direct.mit.edu/neco/article-pdf/34/10/2132/2042425/neco_a_01529.
pdf
ChampionT,Grze´sM,BowmanH(2022d)Multi-modalandmulti-factorbranchingtimeactiveinference.
DOI 10.48550/ARXIV.2206.12503, URL https://arxiv.org/abs/2206.12503
Cortes C, Mohri M, Rostamizadeh A (2012) Algorithms for Learning Kernels Based on Centered Align-
ment. J Mach Learn Res 13(1):795–828
Costa LD, Parr T, Sajid N, Veselic S, Neacsu V, Friston K (2020a) Active inference on discrete state-
spaces: a synthesis. 2001.07203
Costa LD, Sajid N, Parr T, Friston K, Smith R (2020b) The relationship between dynamic programming
and active inference: the discrete, finite-horizon case. 2009.08111
Cristianini N, Shawe-Taylor J, Elisseeff A, Kandola JS (2002) On Kernel-Target Alignment. In:
Dietterich TG, Becker S, Ghahramani Z (eds) Advances in Neural Information Processing Sys-
tems, vol 14, MIT Press, Vancouver, Canada, pp 367–373, URL http://papers.nips.cc/paper/
1946-on-kernel-target-alignment.pdf
Cullen M, Davey B, Friston KJ, Moran RJ (2018) Active inference in openai gym: A paradigm for
computational investigations into psychiatric illness. Biological Psychiatry: Cognitive Neuroscience
and Neuroimaging 3(9):809 – 818, DOI https://doi.org/10.1016/j.bpsc.2018.06.010, URL http://www.
sciencedirect.com/science/article/pii/S2451902218301617, computational Methods and Mod-
eling in Psychiatry
Doersch C (2016) Tutorial on variational autoencoders. 1606.05908
FitzGeraldTHB,DolanRJ,FristonK(2015)Dopamine,rewardlearning,andactiveinference.Frontiersin
ComputationalNeuroscience9:136,DOI10.3389/fncom.2015.00136,URLhttps://www.frontiersin.
org/article/10.3389/fncom.2015.00136
Fountas Z, Sajid N, Mediano PAM, Friston K (2020) Deep active inference agents using Monte-Carlo
methods. 2006.04176
Friston K, FitzGerald T, Rigoli F, Schwartenbeck P, Doherty JO, Pezzulo G (2016) Active inference and
learning.Neuroscience&BiobehavioralReviews68:862–879,DOIhttps://doi.org/10.1016/j.neubiorev.
2016.06.022
Gretton A, Bousquet O, Smola A, Sch¨olkopf B (2005) Measuring Statistical Dependence with Hilbert-
Schmidt Norms. In: Jain S, Simon HU, Tomita E (eds) Algorithmic Learning Theory, Springer Berlin
Heidelberg, pp 63–77
van Hasselt H, Guez A, Silver D (2015) Deep reinforcement learning with double Q-learning. 1509.06461
Higgins I, Matthey L, Pal A, Burgess C, Glorot X, Botvinick M, Mohamed S, Lerchner A (2017) beta-
VAE: Learning basic visual concepts with a constrained variational framework. In: 5th International
Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference
Track Proceedings, OpenReview.net, URL https://openreview.net/forum?id=Sy2fzU9gl
van der Himst O, Lanillos P (2020) Deep active inference for partially observable mdps. CoRR
abs/2009.03622, URL https://arxiv.org/abs/2009.03622, 2009.03622
50
Deconstructing deep active inference.
Itti L, Baldi P (2009) Bayesian surprise attracts human attention. Vision Research 49(10):1295 – 1306,
DOI https://doi.org/10.1016/j.visres.2008.09.007, URL http://www.sciencedirect.com/science/
article/pii/S0042698908004380, visualAttention: Psychophysics, electrophysiologyandneuroimag-
ing
Kingma DP, Welling M (2014) Auto-Encoding Variational Bayes. In: International Conference on Learn-
ing Representations, Banff, Canada, vol 2, URL http://arxiv.org/abs/1312.6114
Koller D, Friedman N (2009) Probabilistic graphical models: principles and techniques. MIT press
Kornblith S, Norouzi M, Lee H, Hinton G (2019) Similarity of Neural Network Representations Revisited.
In: Chaudhuri K, Salakhutdinov R (eds) Proceedings of the 36th International Conference on Machine
Learning, PMLR, Long Beach, USA, Proceedings of Machine Learning Research, vol 97, pp 3519–3529,
URL http://proceedings.mlr.press/v97/kornblith19a.html
Lample G, Chaplot DS (2016) Playing fps games with deep reinforcement learning. 1609.05521
Lanillos P, Cheng G, et al (2020) Robot self/other distinction: active inference meets neural networks
learning in a mirror. arXiv preprint arXiv:200405473
Maheswaranathan N, Williams A, Golub M, Ganguli S, Sussillo D (2019) Universality and individu-
ality in neural dynamics across large populations of recurrent networks. In: Wallach H, Larochelle
H, Beygelzimer A, d'Alch´e-Buc F, Fox E, Garnett R (eds) Advances in Neural Information Process-
ing Systems, Curran Associates, Inc., vol 32, URL https://proceedings.neurips.cc/paper/2019/
file/5f5d472067f77b5c88f69f1bcfda1e08-Paper.pdf
Matthey L, Higgins I, Hassabis D, Lerchner A (2017) dsprites: Disentanglement testing sprites dataset.
https://github.com/deepmind/dsprites-dataset/
Millidge B (2019) Combining active inference and hierarchical predictive coding: A tutorial introduction
and case study. URL https://doi.org/10.31234/osf.io/kf6wc
Millidge B (2020) Deep active inference as variational policy gradients. Journal of Mathematical Psychol-
ogy 96:102,348, DOI https://doi.org/10.1016/j.jmp.2020.102348, URL http://www.sciencedirect.
com/science/article/pii/S0022249620300298
Mnih V, Kavukcuoglu K, Silver D, Graves A, Antonoglou I, Wierstra D, Riedmiller M (2013) Playing
atari with deep reinforcement learning. 1312.5602
Oliver G, Lanillos P, Cheng G (2019) Active inference body perception and action for humanoid robots.
arXiv preprint arXiv:190603022
Parr T, Friston KJ (2019) Generalised free energy and active inference. Biological cybernetics 113(5-
6):495–513
Pezzato C, Hernandez C, Wisse M (2020) Active inference and behavior trees for reactive action planning
and execution in robotics. 2011.09756
Rezende DJ, Mohamed S, Wierstra D (2014) Stochastic Backpropagation and Approximate Inference in
Deep Generative Models. In: Xing EP, Jebara T (eds) Proceedings of the 31st International Conference
on Machine Learning, PMLR, Bejing, China, Proceedings of Machine Learning Research, vol 32, pp
1278–1286, URL http://proceedings.mlr.press/v32/rezende14.html
51
Champion et al.
Robert P, Escoufier Y (1976) A Unifying Tool for Linear Multivariate Statistical Methods: The RV-
Coefficient. Journal of the Royal Statistical Society Series C (Applied Statistics) 25(3):257–265, URL
http://www.jstor.org/stable/2347233
Rood T, van Gerven M, Lanillos P (2020) A deep active inference model of the rubber-hand illusion.
In: Verbelen T, Lanillos P, Buckley CL, De Boom C (eds) Active Inference, Springer International
Publishing, Cham, pp 84–91
Sancaktar C, van Gerven M, Lanillos P (2020) End-to-end pixel-based deep active inference for body
perception and action. 2001.05847
Schneider T (N.D.) Active inference for robotic manipulation, unpublished
SchneiderT,BelousovB,AbdulsamadH,PetersJ(2022)Activeinferenceforroboticmanipulation.arXiv
preprint arXiv:220610313
Schwartenbeck P, Passecker J, Hauser TU, FitzGerald THB, Kronbichler M, Friston K (2018) Com-
putational mechanisms of curiosity and goal-directed exploration. bioRxiv DOI 10.1101/411272,
URL https://www.biorxiv.org/content/early/2018/09/07/411272, https://www.biorxiv.org/
content/early/2018/09/07/411272.full.pdf
Silver D, Huang A, Maddison CJ, Guez A, Sifre L, van den Driessche G, Schrittwieser J, Antonoglou I,
Panneershelvam V, Lanctot M, Dieleman S, Grewe D, Nham J, Kalchbrenner N, Sutskever I, Lillicrap
TP, Leach M, Kavukcuoglu K, Graepel T, Hassabis D (2016) Mastering the game of go with deep
neural networks and tree search. Nature 529(7587):484–489, DOI 10.1038/nature16961, URL https:
//doi.org/10.1038/nature16961
Sutton RS, Barto AG, et al (1998) Introduction to reinforcement learning. MIT press Cambridge
Ueltzh¨offerK(2018)Deepactiveinference.BiolCybern112(6):547–573, DOI10.1007/s00422-018-0785-7,
URL https://doi.org/10.1007/s00422-018-0785-7
52
Appendix A: Notation
Symbol Meaning
s , o , r , a State, observation, reward and action at time step τ, respectively.
τ τ τ τ
An observation in which a reward has been encoded as explained in Figure 40, the hidden
or, sˆr,˚or state sampled from the encoder when feeding or as input, and the observation reconstructed
τ τ τ τ
by the decoder from the output of the transition network, respectively.
A state sampled from the encoder at time step τ when o is provided as input, and the image
sˆ , oˆ τ
τ τ reconstructed by the decoder at time τ when using sˆ as input, respectively.
τ
˚o The observations at time step τ predicted by the transition network.
τ
s , o , a Respectively, the set of states, observations, and actions between time step i and j (included).
i:j i:j i:j
A policy, i.e. a sequence of actions, the action prescribed by the policy at time step τ, and
π, π , π(cid:48)
τ another policy whose size is smaller or equal than the size of π, respectively.
A, Π The set of possible actions, and the set of possible policies, respectively.
A The set of all anscestors of a node s.
s
#A,#C,#S,#O The number of actions, channels, states and observations, respectively.
Q , Qˆ The Q-network parameterised by θ , and the target network parameterised by θˆ
θa θˆ
a
a a
G , Gˆ The critic network parameterised by θ , and the target network parameterised by θˆ .
θa θˆ
a
a a
E , D The encoder network parameterised by φ , and the decoder network parameterised by θ .
φs θo s o
P The policy network parameterised by φ .
φa a
T , T The transition network parameterised by θ or θ , respectively.
θs θo s o
θ, φ All the parameters of the generative model, and the variational distribution, respectively.
a, b, c, d Four hyperparameters involved in the computation of ω .
t
N(s ,a ) The number of times action a was explored in state s .
τ τ τ τ
t, γ The present time step, and the discount factor, respectively.
ζ, ψ The precision of the prior over actions, and the precision of the prior preferences.
(cid:15), (cid:15)ˆ The random variable used in the re-parameterisation trick, and a sample of epsilon.
˚(cid:15) The probability of selecting a random actions when using the˚(cid:15)-greedy algorithm.
T A hyperparameter defining the threshold value corresponding to a clear winner during MCTS.
dec
The expected free energy (EFE) of policy π, and the EFE received at time
G(π), G (π)
τ step τ when following policy π, respectively.
The average EFE, the aggregated EFE, the EFE, and the number of visits
G¯ , Gaggr, G , N
s s s s of a node s, respectively.
µ ,σ ,µ ,σ The mean and variance vectors predicted by the encoder and policy networks of the DAI .
o o a a FA
µ, σ The mean and variance of the Gaussian distribution over s predicted by the encoder.
t
˚µ,˚σ The mean and variance of the Gaussian distribution over s predicted by the transition.
t+1
µˆ, σˆ The mean and variance of the Gaussian distribution over s predicted by the encoder.
t+1
πˆ The parameters of the categorical distribution over a predicted by the policy network.
t
ω The top-down attention parameter modulating the precision of the transition mapping.
t
[condition] An indicator function that equals one if the condition is satisfied and zero otherwise.
σ[•] The softmax function.
Cat(x;φ ) A categorical distribution over x parameterised by φ .
x x
Bernoulli(x;φ ) A Bernoulli distribution over x parameterised by φ .
x x
Bernoulli(x;φ ) A product of Bernoulli distributions over x parameterised by φ .
x x
A multivariate Gaussian over x parameterised by a mean vector µ , and
N(x;µ ,σ ) x
x x a diagonal covariance matrix whose diagonal elements are σ .
x
i m
X → Y, X → Y X is fed as input to Y, and the mean of the distribution predicted by X is fed as input to Y.
s
X → Y, X → Y a sample from the distribution predicted by X is fed as input to Y, and X outputs Y.
Table 1: Notation of Sections 2 and 3.
Champion et al.
Appendix B: DAI discrepancies between the paper and the code
MC
In this section, we focus on the authors’ implementation of DAI available on GitHub: https://
MC
github.com/zfountas/deep-active-inference-mc/. First, according to a personal communication
with one of the authors, the code available on GitHub (on the 6th of June 2022) is not the same as the
one used to run the experiments of the paper. Below, we describe the discrepancies between the paper
and the code. For example, the computation of ω in the paper is as follows:
t
a
ω = +d,
t 1+exp(−b−Dt)
c
while the code uses the following formula:
(cid:32) (cid:33)
1
ω = a× 1− +d.
t
1+exp
(cid:0)
−
Dt−b(cid:1)
c
Also, the paper states that MCTS is perfomed to compute the prior over policies during training. How-
ever, in the code, MCTS is only used when testing the model, i.e., no MCTS when training the agent.
Additionally, the paper states that actions are selected by sampling from:
N(sˆ,a )
P˜(a ) = t t .
t (cid:80)
N(sˆ,aˆ )
aˆt t t
However, the code selects an entire sequence of actions π = (˚a ,˚a ,...,˚a ) recursively from the root
t t+1 t+n
node in the tree. At each step in the recursion, the node with the highest number of visits˚a is selected.
τ
Then, actions cancelling each other are removed from the sequence, e.g., if a = LEFT and a =
τ τ+1
RIGHT then both actions are removed from the sequence. This procedure generates a new sequence
of actions π(cid:48) of equal or smaller length. Finally, the entire sequence of actions π(cid:48) is performed in the
environment. This avoids the repetition of the planning process for each action-perception cycle (saving
computational time), however, this also requires domain knowledge (to remove actions that cancel each
other out).
Additionally, in the paper, experiments are run on both the dSprites environment and the animal AI
environment. However,thecodedoesnotallowthereplicationoftheresultsontheanimalAIenvironment,
i.e., the code handling the animal AI environment has been removed. In addition, the evaluation of the
expected free energy is non trivial (see below) and the details are not discussed in the paper. Before
explaining how the terms of the EFE are computed, we introduce notation that allows us to express those
computational steps concisely. For example, we note:
or → i Encoder → s Transition → m ˚sr ,
t t+1
meaning that or is used as input (→ i ) for the encoder, then a state is sampled (→ s ) from the distribution
t
m
predicted by the encoder and used as input for the transition network, finally, the mean (→) of the
distribution predicted by the transition network is used as a maximum aposteriori estimate of˚sr . Note,
t+1
the transition network takes two inputs (i.e., a state and an action), when using our concise notation we
implicitly assume that the actions prescribed by the policy8 π are provided as input to the transition
network. Also, for each time step τ, the reward r collected by the agent is encoded in the pixels of the
τ
image o as explained in Figure 40, leading to a new image or. As illustrated on the right of Figure 41,
τ τ
the encoder/decoder networks are trained to predict the resulting images or. The computation of the
τ
first term in equation (8) is illustrated on the left of Figure 41. Concisely, we have:
or → i Encoder → s Transition → s Decoder → m ˚or .
t t+1
8. π is the policy for which the expected free energy is being computed.
54
Deconstructing deep active inference.
Figure 40: This figure illustrates how the reward r ∈ [−1,1] is encoded in image o . On the left, the
τ τ
plus and minus signs shows where the reward will be encoded in the image if the reward is positive or
negative, respectively. In the middle, a positive reward is being encoded on the left side of the image. On
the right, a negative reward is being encoded on the right of the image.
Next, a matrix (˚r ) encoding the maximum reward that the agent can gather is used as parameter of
t+1
Bernoulli distributions to compute the logarithm of the probability (i.e., L) of the three first rows of
the reconstructed image˚or . Note, as explained in Figure 40, the first three rows contain the predicted
t+1
reward obtained at time t+1. Finally, the mean of L is then computed and is multiplied by ten to get
E [lnP˜(o |π)]. Similarly, the computation of H[Q(s |π)] proceeds as follows:
Q˜ τ τ
or → i Encoder → s Transition →˚µ,ln˚σ,
t
where Q(s |π) is equated with N(s ;˚µ,˚σ), and an analytical solution is used to compute the entropy of
τ τ
Q(s |π). Next, the computation of H[Q(o |s ,π)] proceeds as follows:
τ τ τ
or → i Encoder → s Transition → s Decoder → m ˚or ,
t t+1
where observation ˚or is equated to the parameters of the Bernoulli distribution Q(o |s ,π), and an
t+1 τ τ
analytical solution is used to compute H[Q(o |s ,π)]. Surprisingly, another observation ˚or sampled
τ τ t+1
exactly as before is equated to the parameters of the Bernoulli distribution Q(o |s ,θ,π), and the same
τ τ
analytical solution is used to compute H[Q(o |s ,θ,π)]. Finally, H[Q(s |o ,π)] is computed by feeding
τ τ τ τ
˚or back into the encoder to obtain the mean and log-variance of the Gaussian distribution Q(s |o ,π),
t+1 τ τ
and the analytical solution for the entropy of a Gaussian is used to compute H[Q(s |o ,π)].
τ τ
In summary, two samples of ˚or (sampled as described in Figure 40) have been equated to the
t+1
parameters of two diferent distributions, i.e., Q(o |s ,θ,π), and Q(o |s ,π). Additionally, a third sample
τ τ τ τ
of˚or (sampled in the same way) has also been used as input to the distribution P˜(o |π). Lastly, while
t+1 τ
Fountas et al (2020) defines the EFE as in (8), the code turns a plus into a minus, leading to the following
definition of the EFE:
(cid:104) (cid:105)
G (π) =−E lnP˜(o |π)
τ Q˜ τ
(cid:104) (cid:105)
− E E (cid:2) H[Q(s |o ,π)] (cid:3) −H[Q(s |π)]
Q(θ|π) Q(oτ|θ,π) τ τ τ
(cid:104) (cid:105) (cid:104) (cid:105)
+E H[Q(o |s ,θ,π)] −E H (cid:2) Q(o |s ,π) (cid:3) ,
Q(θ|π)Q(sτ|θ,π) τ τ Q(sτ|π) τ τ
where the red minus was a plus.
55
Champion et al.
perform action aˆ
t
env env
t t+1
r ∼U([−1,1]) r =env .compute reward()
t t+1 t+1
o o
t t+1
encode reward in image
or
t
def env .compute reward(): or
t+1 t+1
r =r ×0.95;
t+1 t
if agent.y ≥ 32:
Encoder if shape == “square”:
r = 15.5−agent.x
t+1 16
else: Encoder
r = agent.x−15.5
t+1 16
lnσ µ (cid:15)
The variational
auto-encoder
(cid:15) lnσˆ µˆ (cid:15)
at time t+1
sˆr
t is used to train
˚µ
Transition
˚sr sˆr the encoder
t+1 t+1
and decoder
ln˚σ
a networks.
t
Decoder
E Q˜ [lnP˜(o τ |π)]=mean(L)×10 Decoder
L=lnBernoulli(R(˚or );˚r )
t+1 t+1
˚r t+1 = 1 0 R(˚or t+1 ) ˚or t+1 oˆr t+1
Figure 41: This figure illustrates the computation of E [lnP˜(o |π)] in (the code of) Fountas et al (2020).
Q˜ τ
The environment at time t provides the agent with an image o and a reward r randomly sampled from
t t
theinterval[−1;1]. Then, actionaˆ isperformedintheenvironmentandtheagentobservesanimageo
t t+1
and a reward r , where r is computed according to the function presented in the center of the image.
t+1 t+1
Next, the reward at time t and t+1 are encoded in the images received at time t and t+1, respectively,
c.f., Figure 40 for details about the encoding. The encoded image at time t+1 (i.e., or ) is then fed into
t+1
the encoder, the re-parameterisation trick is then used to sample a state from the variational posterior.
This state is fed into the decoder which tries to reconstruct the image inputed into the encoder. Once
oˆr has been computed, the weights of the encoder and decoder are learned using back-propagation. On
t+1
the other hand, the encoded image at time t (i.e., or) is used to compute E [lnP˜(o |π)]. More precisely,
t Q˜ τ
the or is fed into the encoder, and a state is sampled from the variational posterior Q (s ). This state
t φs t
is then fed as input into the transition network along with the action prescribed by π at time t, i.e., a .
t
A state at time t + 1 can then be sampled from the distribution predicted by the transition network.
This state is then inputed into the decoder, which outputs˚or . Next, a matrix (i.e.,˚r ) encoding the
t+1 t+1
maximum reward that the agent can gather is used as a parameter of a Bernoulli distribution to compute
the logarithm of the probability (i.e., L) of the first three rows of oˆr , i.e., R(˚or ). The mean of L is
t+1 t+1
then computed and is multiplied by ten to obtain E [lnP˜(o |π)].
Q˜ τ
56
Deconstructing deep active inference.
Appendix C: Analysis of the epistemic value
In this appendix, we study the expected free energy decomposition into epistemic and extrinsic value
as given in Equation (10) of Parr and Friston (2019). A particular reason for being interrested in this
formulation is that it is the version of the expected free energy that has the most influenced the deep
active inference approaches, such as Fountas et al (2020). Starting with the definition of the expected
free energy, see Equation (14) in the main-body:
G (π) = E (cid:2) lnQ(s |π)−lnP(o ,s ) (cid:3)
τ P(oτ|sτ)Q(sτ|π) τ τ τ
= E (cid:2) lnQ(s |π)−lnP(s |o )−lnP(o ) (cid:3)
P(oτ|sτ)Q(sτ|π) τ τ τ τ
= −E (cid:2) lnP(s |o )−lnQ(s |π) (cid:3) −E (cid:2) lnP(o ) (cid:3)
P(oτ|sτ)Q(sτ|π) τ τ τ P(oτ|sτ)Q(sτ|π) τ
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Epistemicvalue Extrinsicvalue
In the rest of this appendix, we will focus on the epistemic value and report two experiments. In the first
experiment, the prior over states (c.f., left-most graph in Figure 45) is equal to the approximate posterior
(c.f., Figure 42), and the likelihood (c.f., Figure 43) is becoming more and more uniform (c.f., Figure 44).
Note, if the likelihood becomes uniform, then the true posterior P(s |o ) becomes equal to the prior over
τ τ
states P(s ), i.e.,
τ
1
P(s |o ) ∝ P(o |s )P(s ) = P(s ),
τ τ τ τ τ τ
|o |
τ
where |o | is the number of observations at time step τ, and after renormalisation P(s |o ) = P(s ). The
τ τ τ τ
left-most graph of Figure 39 shows that the epistemic value decreases as the likelihood becomes more
uniform, i.e., the epistemic value decreases as the true posterior P(s |o ) becomes more similar to the
τ τ
approximate posterior Q(s |π). This behaviour is to be expected, as the epistemic value is bigger when
τ
the true posterior P(s |o ) and the approximate posterior Q(s |π) are more different. Thus, maximising
τ τ τ
epistemic value will promote exploration and information gain.
Inthesecondexperiment, thelikelihoodhashighentropy(c.f., right-mostgraphinFigure44), andthe
prior over states is shifting from left to right (c.f., Figure 45). When the prior over states P(s ) and the
τ
approximate posterior Q(s |π) are different, the joint distribution P(o |s )Q(s |π) will be more similar
τ τ τ τ
to the approximate posterior Q(s |π) than it is to the true posterior P(s |o ). Indeed, as the likelihood
τ τ τ
is almost uniform, the true posterior P(s |o ) will almost be equal to the prior over states P(s ). At the
τ τ τ
same time, as the likelihood is almost uniform, it will not have much impact on the joint distribution
P(o |s )Q(s |π) and the approximate posterior Q(s |π) will dominate. To sum up, when the prior over
τ τ τ τ
states P(s ) and the approximate posterior Q(s |π) are different:
τ τ
• the true posterior will almost be equal to the prior over states P(s |o ) ≈ P(s )
τ τ τ
• the joint distribution P(o |s )Q(s |π) will be more similar to Q(s |π) than it is to P(s |o )
τ τ τ τ τ τ
Therefore, the joint distribution P(o |s )Q(s |π) will tend to be large when the difference within the
τ τ τ
expectation is negative (c.f., Figure 46). Indeed, as the joint is similar to the approximate posterior, it
means that the joint distribution is large when the approximate posterior is large and the true posterior is
smaller. This implies that the logarithm of true posterior will be very negative, while the logarithm of the
approximate posterior will be less negative, i.e., closer to zero. Then, the logarithm of the approximate
posterior will be substrated from the logarithm of the true posterior, i.e., a small positive number will be
added to a very negative number. Therefore, the result will be negative.
The right-most graph of Figure 39 shows that the epistemic value increases as the prior P(s ) and
τ
therefore the true posterior P(o |s ) becomes more similar to the approximate posterior Q(s |π). This
τ τ τ
behaviour should not be observed, as the epistemic value is bigger when the true posterior P(s |o ) and
τ τ
the approximate posterior Q(s |π) are more similar. Thus, maximising epistemic value will not promote
τ
exploration. In fact, it will recommend a strong focus on a single action as was observed in Figure 33.
57
Champion et al.
Figure 42: This figure illustrates the approximate posterior Q(s |π), which is distributed according to a
τ
binomial distribution corresponding to 6 trials with a probability of success of 0.5.
Figure 43: This figure provides two views of the same likelihood mapping P(o |s ), i.e., one from the
τ τ
front and one from behind. The likelihood was created by sliding a binomial distribution (corresponding
to 6 trials with a probability of success of 0.5) across the observation axis, each time the state increases
by one. Finally, when the binomial reached its most extreme position, it stays the same for the remaining
states.
Figure 44: During the first experiement, we changed the likelihood mapping P(o |s ) by making it more
τ τ
and more uniform. This change is shown in the figure from left to right.
58
Deconstructing deep active inference.
Figure 45: During the second experiement, we changed the prior over states P(s ) by making it more
τ
and more different to the approximate posterior Q(s |π). This change is shown in the figure from left to
τ
right.
Figure 46: To compute the epistemic value, we first computed the difference between the logarithm of the
trueposteriorlnP(s |o )andthelogarithmoftheapproximateposteriorlnQ(s |π)forallthevaluestaken
τ τ τ
by the observation o (c.f., left-most figure). Then, we computed the joint distribution P(o |s )Q(s |π)
τ τ τ τ
used in the expectation (c.f., middle figure). Next, we compute the element-wise product between the
matrices illustrated in the left-most and middle figures (c.f., right-most figure). Finally, the epistemic
value is obtained by summing up all the elememts of the element-wise product. In this instance, the
epistemic value will be strongly negative.
59

=== REVISE TO ===
PROFESSIONAL TONE: Begin directly with content - NO conversational openings like 'Okay, here's...'

1. Fix all issues above
2. Title: "Deconstructing deep active inference"
3. Include 10-15 quotes from paper text
   - Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
   - Use consistent quote formatting: 'The authors state: "quote"' or vary attribution phrases
   - Vary attribution phrases to avoid repetition
   - CRITICAL: Only extract quotes that actually appear in the paper text
4. ELIMINATE ALL REPETITION - each sentence must be unique
   - Check before each sentence: 'Have I already said this?' If yes, write something new
   - Vary attribution phrases - do NOT repeat 'The authors state' multiple times
5. Extract methodology, results with numbers, key quotes
6. 1000-1500 words, structured with ### headers

Generate COMPLETE revised summary.