=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Addressing the Subsumption Thesis: A Formal Bridge between Microeconomics and Active Inference
Citation Key: kuhn2025addressing
Authors: Noe Kuhn

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Key Terms: thesis, subsumption, expected, theory, addressing, bridge, formal, agency, inference, active

=== FULL PAPER TEXT ===

Addressing the Subsumption Thesis: A Formal
Bridge between Microeconomics and Active
Inference
Noé Kuhn
University of St. Gallen
Abstract. As a unified theory of sentient behaviour, active inference
is formally intertwined with multiple normative theories of optimal be-
haviour. Specifically, we address what we call the subsumption thesis:
Theclaimthatexpectedutilityfromeconomics,asanaccountofagency,
is subsumed by active inference. To investigate this claim, we present
multiple examples that challenge the subsumption thesis. To formally
comparethesetwoaccountsofagency,weanalyzetheobjectivefunctions
for MDPs and POMDPs. By imposing information-theoretic rationality
bounds(ITBR)ontheexpectedutilityagent,wefindthattheresultant
agency is equivalent to that of active inference in MDPs, but slightly
differentinPOMDPs.Ratherthanbeingstrictlyresolved,thesubsump-
tion thesis motivates the construction of a formal bridge between active
inference and expected utility. This highlights the necessary formal as-
sumptions and frameworks to make these disparate accounts of agency
commensurable.
Keywords: ActiveInference·ExpectedUtility·Information-Theoretic
Bounded Rationality · Microeconomics
1 Introduction
Sincethemiddleofthepreviouscentury,expectedutilityhasformedthebedrock
of the agency underwriting microeconomics. With early implementations dating
back to Bernoulli in 1713 [37], expected utility has undergone many augmenta-
tionsinordertoreflectrealisticdeliberatedecisionprocesses.Thecomprehensive
start of this lineage can be traced to the classic utility theorem [24]; [8], with
earlier applications found in [31]. Subsequent accounts include Bayesian Deci-
sion Theory [34]; [6], Bounded Rationality [36]; [26], Prospect Theory [19], and
many more flavours. The algorithmic implementation of expected utility theory
isfoundintheReinforcementLearningliterature[3].Whileseeminglydisparate,
practically all expected utility accounts of agency depict an agent making deci-
sions in a probabilistic setting to attain optimal reward – to pursue utility [5].
Coming from the completely different background of neuroscience, Active
Inference a comparatively new account of agency [12], positioning itself as “a
unifying perspective on action and perception [...] richer than the common op-
timization objectives used in other formal frameworks (e.g., economic theory
5202
raM
6
]HT.noce[
1v84050.3052:viXra
2 Noé Kuhn
and reinforcement learning)” [29, pg. 1;4]. Here the agent seeks to minimize
information-theoretic surprisal expressed as free energy (See Definition 3, 4).
Active inference allows for a realistic modeling of the very neuronal processes
underwriting biological agency [27].
Giventhebreadthofsuccessfulapplications[9]combinedwithitsstrongfun-
damental first principles [13], some proponents of active inference posited what
we call the Subsumption thesis: Expected utility theory as seen in economics is
subsumed by active inference – it is an edge case. A formulation in the same
vein posits: “Active inference [...] englobes the principles of expected utility the-
ory [...] it is theoretically possible to rewrite any RL algorithm [...] as an active
inference algorithm” [11]. So how does the subsumption thesis hold up in the
given examples? Is it possible to formally delineate how expected utility and
active inference differ? This paper then establishes a firm connection between
microeconomics and active inference, which has scarcely been explored before
[17].
Toformallycomparethetwoaccountsofagency,werequireacommensurable
spaceforagent-environmentinteractions:MDPsandPOMDPs(Definition1and
2). These agent-environment frameworks are the bread and butter of expected
utility applications [4], [3], [20]. Active inference agency has more recently also
been specified for the same frameworks [28], [9], [10], [11]. As such (PO)MDPs
provide a theoretical arena for the subsumption thesis to be evaluated.
What exactly is at stake that motivates this inquiry into the subsumption
thesis? Firstly, expected utility and active inference rest upon different first
principlestosubstantiatetheirrespectiveaccountofagency[11].Analysisofthe
formal relationship between these two accounts could provide insights into how
the first principles of one account might be a specification the other’s first prin-
ciples. Secondly, this inquiry will shed light on how each account handles the
exploration-exploitation dilemma [7]: How should an agent prioritise between
exploring an environment versus exploiting what they already know about the
environmentforutility?Finally,ifactiveinferencetrulysubsumedexpectedutil-
ity,thentheramificationsforwelfaristeconomicswouldbeenormous:Currently,
the formal mainstream understanding of welfare which informs economic policy
[30] is based on aggregating individual agents acting according to expected util-
ity [23, pg. 45] [32]. The subsumption thesis challenges foundations of ‘optimal’
economic policy if expected utility only captures a sliver of ‘optimal’ behaviour.
To investigate the subsumption thesis, the rest of the paper is structured as
follows. In section 2. the agent-environment frameworks are defined alongside
therelevantaccountsofagencyandbasicconceptsinmicroeconomics.Insection
3., some examples are investigated which challenge the subsumption thesis. In
section 4., the formal bridge between expected utility and active inference is
establishedviaInformationTheoreticBoundedRationality(ITBR)[26].Finally
section 5. provides some concluding and summarizing remarks.
Addressing the Subsumption Thesis 3
2 Preliminary Definitions and Microeconomics
2.1 Agent-Environment Frameworks
A finite Markov Decision Process (MDP) is a mathematical model that spec-
ifies the elements involved in agent-environment interaction and development
[3]. This formalization of sequential decision making towards reward maximiza-
tion originates in dynamic programming, and currently enjoys much popularity
in model-based Reinforcement Learning (RL). Although potentially reductive,
employingMDPsandPOMDPsallowsforformalcommensurabilitybetweendif-
ferent accounts of agency.
Definition 1 (Finite Horizon MDP). An MDP is defined according to the fol-
lowing given tuple: (S,A,P(s′|a,s),R(s′,a),γ =1,T)
– S is a finite set of states.
– A is a finite set of actions.
– P(s′|a,s) is the transition probability of posterior state s′ occurring upon
the agent’s selection of action a in the prior state s.
– R(s′,a)∈R+ is the reward function taking as arguments the agent’s action
and resulting state. For our purposes, the action taken will be irrelevant to
the resulting reward: R(s′,a)=R(s′).
– γ denotes the discount factor of future rewards. This is set to 1 as this
parameter is not commonly used in the cited active inference literature.
– T={1,2,...,t,...,τ,...,T}isafinitesetfordiscretetimeperiodswhereby
t<τ and the horizon is T.
Note that time period subscripts e.g, s are sometimes omitted when unneces-
τ
sary.
In a single-step decision problem, an expected reward-maximizing agent would
evaluate the optimal action a∗ as follows:
a∗ =argmax E R(s ) (1)
t
a∈A
P(sτ|at,st) τ
Further, a Partially Observable Markov Decision Process (POMDP) general-
izesanMDPbyintroducingobservationsothatcontainincompleteinformation
aboutthelatentstatesoftheenvironment[20,3].Theagentcanonlyinferlatent
statesviaobservations.Thus,POMDPsareidealformodelingaction-perception
cycles [13] with the cyclical causal graphical model a→s→o...
Definition 2 (Finite Horizon POMDP). A finite horizon POMDP further adds
two elements to the previously given MDP tuple: (O,P(o|s))
– O is a finite set of observations.
– P(o|s) is the probability of observation o occuring to the agent given the
state s.
4 Noé Kuhn
2.2 Active Inference Agency
With the environment-agent frameworks established, we can proceed to define
how an active inference agent approaches a (PO)MDP. Although fundamental
and interesting, the Variational Free Energy objective crucial to perception in
active inference will not be examined here; Inference on latent states is assumed
to occur through exact Bayesian inference [10, pg. 16]. The central objective
function for agency in active inference is the Expected Free Energy (EFE), the
formulation of which for (PO)MDPs we will take from [9],[11],[10],[28]. Essen-
tially,theagenttakestheactiontrajectoryπ ={a ,...,a }thatminimizesthe
τ T
cumulative expected free energy G, which is roughly the sum of the single-step
EFEs G . By inferring the resultant EFE of policies through Q(·), the optimal
τ
trajectoryπ∗ correspondstothemostlikelytrajectory–thepathofleastaction.
[13]. Formally:
π∗ =argmin G(π) (2)
π
T
(cid:88)
G(π)≈ G (π)
τ
τ
G (π)=G a )
τ ( t
We can then define the EFE for single-step for MDPs and POMDPs. Note that
this could also be scaled up to trajectories/vectors of the relevant elements e.g
s . For simplicity we will look at single-step formulations for the remainder of
t:T
the paper.
Definition 3: (EFE on MDPs). For an agent in an MDP with preference dis-
tribution P(s|C), the Expected Free Energy of an action for some given current
state s is defined as follows:
t
G (a )=D [P(s |a ,s )||P(s|C)] (3)
τ t KL τ t t
=− H[P(s |a ,s )] −E [logP(s|C)]
τ t t P(sτ|at)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Entropyoffuturestates ExpectedSurprise
As seen in the rearranged objective function of the second line, the agent seeks
to keep future options open while meeting preferences; The entropy of future
possible states is to be maximized while the information-theoretic surprisal ac-
cordingtothepreferencedistributionistobeminimized.Theconditionalisation
on C specifies a parameterized preference distribution [28].
Definition 4: (EFE in POMDPs). For an agent in a POMDP with preference
distribution P(s|C),P(o|C), the Expected Free Energy of an action for some
given current state s is defined as follows:
t
Addressing the Subsumption Thesis 5
G (a )=E H[P(o |s )]+D [P(s |a ,o )|P(s|C)] (4)
τ t P(sτ|ot,at) τ τ KL τ t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Ambiguity Risk
=−E [D [P(s |o )||P(s |a )]]−E [logP(o|C)]
P(oτ|at) KL τ τ τ t P(oτ,sτ|at)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
IntrinsicValue ExtrinsicValue
With some auxiliary assumptions [11, pg. 10] which are admissible for our pur-
poses,thetwoformulationsofEFEinaPOMDPareequivalent,andbothcontain
a curiosity inducing term and an exploitation term [15]. The first formulation
motivates the agent to minimize the expected entropy of observations given un-
knownstatesandtominimizethedivergencebetweenactualstatesandpreferred
states. The second formulation motivates the agent to maximize the expected
informationalvalueofobservationswhilealsomaximizingtheexpectedlogprob-
abilityofpreferredobservations–notehowtheunderbracedoesnotincludethe
minus.
2.3 Microeconomics
As this paper investigates an intersection between fields which are generally not
indirectcontact,abriefintroductiontoriskattitudesandlotteriesinmicroeco-
nomicsisprovided.Theoriginofthesestudiescanbetracedbacktothegambling
houses of the 18th century; As early as 1713, Bernoulli employed marginally de-
creasingutilityfunctionstoresolvethefamousSt.PetersburgParadox[37].This
paradoxaskswhatamountarationalagentwouldbewillingtopaytoenterlot-
terywithaninfiniteexpectedvalue.Toanswerthisquestion,weutilizelotteries
[8] and risk-attitudes [1] from microeconomics:
Definition 5: (Lottery). A (monetary) lottery is a probability distribution over
outcomes x that are the argument of the utility function. Therefore, a lottery
L can be modelled as an integrable random variable defined by the probability
space triplet consisting of a sample space, sigma algebra, and probability mea-
sure: (Ω,F,µ)
Adecisionmakerthenevaluatestheirpreferenceoverasetoflotteriesaccording
to their utility function U(x) ∈ R+ where x ∈ F. The expected utility of each
lotterytheninducesapreferenceorderingoverlotteries.Forexample,thestrong
preference relation L ≻L means that lottery L is more preferable to lottery
1 2 1
L . Classically, this ordering is in line with the von Neumann-Morgenstern ax-
2
ioms of completeness, transitivity, continuity, and independence [24]. Any such
preference ordering is also maintained for any positive affine transformation of
U(x) [8]; [24]. By juxtaposing the expected utility E[U(L)] of a lottery against
the utility of the expectation of the same lottery U(E[L]), risk aversion can be
defined.
Definition 6: (Risk Aversion). An agent with some utility function U(·) is con-
sidered risk averse if for some lottery the following preference relation holds:
U(E[L])≻U(L)
6 Noé Kuhn
This preference relation occurs if an agent’s utility function is concave, i.e the
marginal utility is decreasing. A risk loving agent conversely acts according to
a convex utility function, and a risk neutrality is associated with a linear utility
function. Accordingly, Bernoulli used lotteries and a log-utility function to re-
solvetheSt.Petersburgparadox,thesolutionofwhichisrelegatedtoAppendix
A for readers unfamiliar with the problem – the pertinent point is that concave
utility functions on set rewards are extensively studied in economics.
3 Subsumption Examples
Equipped with an understanding of marginal utility and lotteries, we can now
tackletwomanifestexhibitsofthesubsumptionthesisbyproponentsofactivein-
ference. Further, an illustrative MDP demonstrates the divergence in behaviour
between active inference and expected utility. The results of the simulated be-
haviour are directly taken from the discussed papers. These exhibits then moti-
vate the bridging in section 4 later.
The first [33] and second [11] exhibit both concern agency in a classical T-
maze: A simple forked pathway in which the agent can either go left or right
(See Figure 1 below). This environment is also called “Light” in POMDP lit-
erature [20]. The agent-environment dynamics are modeled using a POMDP;
Unbeknownst to the agent, the reward is either in the left or right arm. The
agent can also go down to observe a cue indicating the definite location of the
reward. Going down the ‘wrong’ arm of the fork leads to a punishment equal
to the negative reward, say −1. The performance of the agency is evaluated by
the reward attainment of the agent within a two period horizon. At this point
however, the setup of the first and second exhibit diverge crucially.
Fig.1. An agent in a T-Maze with unknown context. Illustration from [33]
In the first exhibit [33], the right and left fork are absorbing states – the
agent cannot leave them upon entry. As such, the agent cannot correct going
down the wrong arm in the first period by the second period. Given this setup,
the expected utility agent performs very poorly, while the active inference is
cue-seeking and therefore performs optimally [33, pg. 138]. The expected utility
agent performs so poorly because supposedly “the agent does not care about
Addressing the Subsumption Thesis 7
the information inferred and is indifferent about going to the cue location or
remaining at the central location”[33, pg. 137]. This appears reductive, as an
expected utility agent facing two lotteries will behave the same as the active
inference agent. Consider the risky lottery L which is the result of a gambling
1
andnon-informationseekingstrategy.ContrastthislotterywithL ,whichisthe
2
degenerate lottery of investigating the cue first and going to the reward in the
second period. Assuming even just a linear utility function U(R) = R(s), then
U(L ) = 0.5·1+0.5·−1 = 0 and U(L ) = 1·1. Clearly, the expected utility
1 2
agent holds a preference which motivates cue-seeking behaviour: L ≻L .
2 1
Regarding the second exhibit [11], there is a slight difference in the setup.
The arms of the fork are no longer absorbing states, which allows for mistake
correctionand acumulativereward of 2 over two periods. Now,the focusof[11]
isn’t anymore on performance comparison but instead achieving the desiderata
of risk-aversion and information sensitivity [11, pg. 10]. While the agency ac-
cording to active inference meets the desiderata, the expected utility agent does
not. However, risk aversion and the resulting information sensitivity can easily
be induced by using a concave utility function. Consider again the risky lottery
L and a cue-seeking lottery L . Assuming a utility function taking the reward
1 2
as argument U(R) = R(s)c where c ∈ R+, then U(L ) = 0.5·0+0.5·2c and
1
U(L )=1c. Accordingly if c<1, then L ≻L , and only if c=1, then indeed
2 2 1
the agent is indifferent L ∼ L . As is evident, it is the risk-neutral agent who
1 2
does not meet the desiderata.
Finally, consider the following single-step MDP created for illustrative pur-
poses. A paraglider stands at the foot of two steep mountains s ,s separated
1 2
by a chasm s and must decide which one to climb. While still risky, the path
3
upmountain1isfarmoresecurethanthepathupmountain3.However,moun-
tain 2 is taller than mountain 1 and therefore allows for a more enjoyable flight.
This decision process can aptly be modeled in an MDP (See Figure 2 below).
Note that the subscript here does not relate to the period. Taking a gives
1
{P(s |a ),P(s |a ),P(s |a )} = {0.6,0,0.4}, and a gives {P(s |a ),P(s |a ),
1 1 2 1 3 1 2 1 2 2 2
P(s |a )} = {0,0.4,0.6}. The height in kilometers gives the reward function
3 2
{R(s ),R(s ),R(s )} ={1,1.5,0}. With the MDP sufficiently specified, we can
1 2 3
compare the agency of an active inference agent and an expected utility agent.
See Appendix B for details on the resulting expected utility and free energy.
The active inference agent is indifferent between the two actions as both
actions result in the same EFE – equation (3). For expected utility however,
only the linear utility function agent is indifferent; The risk averse agent prefers
the safer mountain and the risk loving agent prefers the riskier mountain due
to the concavity or convexity of the utility function respectively. As such, this
simple but valid MDP provides a setup in which specified expected utility may
better meet the desiderata than the active inference agent.
Itshouldbeclearbynowthatwrappingautilityfunctionaroundtherewards
is a well-studied and principled approach which differs from simply including
“ad-hoc exploration bonuses in the reward function” [11, pg. 2]. Introducing
non-linearity over the rewards seems to lead to an impasse in the comparison
8 Noé Kuhn
Fig.2. ‘Paraglider’ MPD with states, actions, transition probabilities, and rewards
between expected utility. The most direct case for comparing and subsuming
expectedutility[10]onlyconsidersalinearutilityfunction(U(·)=R(·))forthe
expected utility agent. Even if non-linearity for expected utility were considered
in[10],itappearsuncleartousastohowtheresultingagency–morespecifically
theinducedcarefulandexplorativeaspect–couldbecomparedinageneralized
manner.
To resolve this issue of incommensurability, we would like to draw attention
to physical and biological constraints on agents which have motivated active
inference. For example, tractability is a central concern for active inference as
evidenced by the appeal to variational Bayes. Luckily, there already exists an
accountofagencywhichimbuesexpectedutilitywithconstraints:ITBR[26],[16].
The connection between ITBR and active inference in an MDP has briefly been
explored before [25]. We seek to now clearly establish this conceptual bridge
between microeconomics and active inference for both MDPs and POMDPs.
4 From expected utility to active inference via ITBR
4.1 In MDPs
Let us first establish the bridge between expected utility and active inference in
an MDP. Essentially, both objective functions can both be transformed into the
“Divergence Objective” [21]:
a∗ =argminD [P(s |a )||P∗(s)] (5)
KL τ t
a∈A
Wherea∗ istheoptimalactionandP∗(s)isapreferencedistributionoverstates,
for example, a softmax or Gibbs distribution. Note the immediate similarity to
the EFE objective function for MDPs (3) – here conditionalisation on the cur-
rent state s is omitted for brevity as we consider a single step.
t
To get there from expected utility, we can consider the following Lagrangian
Addressing the Subsumption Thesis 9
constraints on the utility objective function [16, pg. 3]. Let P(·) be the prior
distribution over relevant elements of the MDP, and Q(·) the posterior distri-
bution after a limited search or ‘bounded deliberation; see [26] for details. The
deliberationboundisgivenasaninformation-theoreticquantitye.gnatsorbits;
hence the name information-theoretic bounded rationality. Let K ∈ R+ nat –
although the information theoretic unit base nat is arbitrary:
D [Q(s |a )||P(s |a )]≤K (6)
KL τ t τ t
Theconstraintofequation6canbeinterpretedasaboundonthesearchforthe
optimal action. The second constraint of (6) means that the agent is uncertain
aboutthe‘true’transitionprobabilitiesintheMDP.Thisconstraintgivesusthe
following ITBR free energy objective function [25]:
(cid:18) (cid:19)
(cid:88) 1 Q(s|a)
F (Q)= Q(s|a) U(s,a)− log (7)
ITBR β P(s|a)
s
This functional is to be maximized (Q∗(s|a)) with given parameter β ∈R+. See
Appendix C for how the maximizing solution is derived. We can now use the
maximizing argument of the objective function (7) as a ‘goal’ for the agent, or
a preference distribution over states P∗(s). Like in active inference, we assume
thatthepreferencedistributionoverstatesisindependentoftheactiontakento
get there. This preference is given by the Gibbs distribution:
P(s|a)·eβU(s,a)
P∗(s|a)= →P∗(s) (8)
Z
β
Wecannowsolve(8)forU(s,a)andinputthisinto(7)toobtainthedivergence
objective (5):
a∗ =argmin−D [Q(s |a )||P∗(s)]+constant (9)
KL τ t
a∈A
Where the constant is irrelevant for optimization purposes. The details of this
derivation relegated to Appendix D. Evidently, the same optimal agency arises
inanMDPforanactiveinferenceandITBRagent.Next,letusbridgeexpected
utility to active inference in a POMDP.
4.2 In POMDPs
Analogously to the MDP setting, we can transform the ITBR objective to get
to the divergence objective function for POMDPs. Fortunately, this divergence
objective has previously been formulated as the “Free Energy of the Expected
Future” (FEEF):[22,pg.10].Again,thisobjectivefunctionmotivatesaminimal
posterior divergence from a preference distribution, now jointly over states and
observations:
a∗ =argminD [P(o ,s |a )||P∗(o,s)] (10)
KL τ τ t
a∈A
10 Noé Kuhn
Toattainthisexpression,wecanformulateanewITBRobjectiveinthePOMDP
framework [16] and transform it analogously to the MDP case before. We can
again consider the information-theoretic bound V ∈R+nat :
D [Q(s ,o |a )||P(s ,o |a )]≤V (11)
KL τ τ t τ τ t
Considering these constraints, we can express the ITBR Free energy objective
function again:
(cid:18) (cid:19)
(cid:88) 1 Q(o,s|a)
F (Q)= Q(o,s|a) U(o,s,a)− log (12)
ITBR β P(o,s|a)
s
Where the solution is again the Gibbs distribution:
P(o,s|a)eβU(o,s,a)
P∗(o,s|a)= (13)
Z
β
By combining (13) and (12) we get the resultant minimization objective, where
the resultant optimal agency is of course the same as that of the divergence
minimization objective (10):
a∗ =argmin−D [Q(o ,s |a )||P∗(o,s)]+constant (14)
KL τ τ t
a∈A
Which again intuitively motivates the agent to have the inferred posterior dis-
tributiongiventheactionbeascloseaspossibletothepriorpreferencedistribu-
tion over states. It is crucial to note however that this is not the same objective
function as EFE in POMDPs (4)! To get from the divergence objective (10)
for POMDPs to EFE (4), we can follow the steps taken in [22]; for a detailed
discussion of the relationship between the divergence objective and EFE, the
reader should also consult [21], [22]. Essentially, the divergence objective can
also be decomposed into an exploitative and explorative term. However, while
theexplorativetermisequaltothatofactiveinference,thedivergenceobjective
additionally further encourages the agent to increase posterior entropy of obser-
vations given latent states – to keep options open. Note that in the formulation
below, both objective functions below (15), (4) are to be minimized.
−F =D [Q(o,s|a)||P∗(o,s)] (15)
ITBR KL
=E [ D [Q(o|s)||P∗(o)] ]−E [ D [Q(s|o)||Q(s|a)] ]
Q(s|a) KL Q(o|a) KL
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ExtrinsicValue IntrinsicValue
G = −E [logP(o|C)]−E [ D [Q(s|o)||Q(s|a)] ] (4)
Q(o,s|a) Q(o|a) KL
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ExtrinsicValue IntrinsicValue
Whereby the relationship between G and −F is as follows:
ITBR
G−E H[Q(o|s)]=−F (16)
Q(o|a) ITBR
Comparingthenthe decomposeddivergenceobjectivetoactiveinference,inthe
pursuit of extrinsic value the boundedly rational utility agent seeks to addi-
tionally keep posterior options open compared to the active inference agent –
similarly to the agency in an MDP.
Addressing the Subsumption Thesis 11
4.3 The Bridge summarized
Let us reconsider the entire journey from expected utility to active inference so
as to not lose sight of the forest in front of all the trees. First, simply incor-
porate a utility function into the reward maximizing objective function (1) to
get an expected utility agent. Then, impose information-theoretic deliberation
constraints on the optimization process (6). Consequently, the agent faces a La-
grangian optimization problem (7). The solution to this optimization problem
is taken as a preference distribution for the agent. Combining the preference
distribution and the objective function results in the divergence objective [21],
which can then be compared with the active inference objective function. In an
MDP, the resultant agency is the exact same (5). However, in a POMDP, the
objective functions differ (16).
Bar this difference, one key aspect must be elucidated for the extrinsic value
terms in both MDPs and POMDPs. Although the intrinsic value term is the
same for the different objective functions, the two prior preference distributions
P∗(s) of ITBR (8) and P(s|C) of active inference (3) are not necessarily the
same. For β = 1, if we consider P(s|C) as a Gibbs distribution as per [10, pg.
9];[28, pg. 134], then the two preference distributions are only equal if either
the utility function is linear, or if active inference admits agent-specific utility
functions (17); An admission which prima facie seems irreconcilable with the
physicalist/nonsubjectivist philosophy behind active inference. This larger dis-
cussion is, however, to be relegated to a later paper.
eU(s) eR(s)
P∗(s)= and P(s|C)= (17)
(cid:80) eU(s) (cid:80) eR(s)
s s
Where optimal behaviour in an MDP, i.e a∗, is the same for both accounts of
agency only if U(s) is a positive affine transformation of R(s).
5 Conclusion
Having formalized the bridge from expected utility to active inference, we can
re-evaluatethesubsumptionthesis.Simplereward-orientedagency(U(·)=R(·))
can be effectively subsumed by active inference in MDPs, and if exact Bayesian
inference is used, also in POMDPs [10]. However, as shown in section 2., ex-
pected utility in microeconomics uses utility functions that take rewards as
arguments. As seen in section 3. then, there are various examples where the
subsumption argument does not hold up; Expected utility acts the same as ac-
tive inference, or under specific circumstances, may meet desiderata of agency
evenmore.Insection4.,weestablishtheformalbridgebetweenexpectedutility
andactiveinference.ByusingITBR[26],wecandirectlycomparetheobjective
functions of (bounded) expected utility and active inference. Upon considering
agent-environment assumptions, the divergence objective [21] is used as a refer-
ence point to comparethe two accounts of agency. It is demonstrated thatin an
12 Noé Kuhn
MDP, ITBR and active inference lead to the same agency [25]. In a POMDP,
ITBRisequivalenttothedivergenceobjective,whichhoweverdiffersfromtheac-
tiveinferenceobjectivefunction[22].Whiletheexplorative/information-seeking
terms are equal, the exploitative/reward-oriented term differs: E H[Q(o|s)]
Q(o|a)
must be subtracted from the active inference objective function, and the prefer-
ence distributions are not necessarily equal.
An area where expected utility cannot compete however is in the first prin-
ciples which motivate agency [12], [13] [14], [2], [11]. Still, the debate on what
objective function follows from the first principles is not yet sealed in this flour-
ishing field [22]. Perhaps more intriguing links between brain function and the
physicalinterpretationsofinformationtheorylurkunderneaththebridgeestab-
lished here. Furthermore, computational simulations [16] and empirical studies
[35] might flesh out the practical comparison between bounded expected utility
and active inference; Computational efficiency has not remotely been addressed
in this paper. Finally, it would be especially interesting for economics to under-
stand how an economy could develop from multiple ITBR or active inference
agents [18]. By integrating interdisciplinary approaches to agency, we aim to
foster a holistic understanding of agency that enriches the roles of both human
and artificial agents in society.
Addressing the Subsumption Thesis 13
Acknowledgments. I would like to express my immense gratitude to my supervisor
forallowingmetodelveintothistopicandlendinghissupportalongtheway.Further,
I want to thank the various researchers willing to so openly discuss the contents and
conceptsofthepaper.Onlythankstothosefruitfulexchangescouldtheseconnections
across varying fields even be grasped.
Disclosure of Interests. The author has no competing interests to declare that are
relevant to the content of this article.
References
1. Arrow, K.J.: Essays in the Theory of Risk Bearing. Markham Publishing Co,
Chicago (1971)
2. Barp, A., Da Costa, L., França, G., Friston, K., Girolami, M., Jor-
dan, M.I., Pavliotis, G.A.: Geometric methods for sampling, optimisa-
tion, inference and adaptive agents. Handbook of Statistics 46, 21–78
(2022). https://doi.org/10.48550/arXiv.2203.10592, https://doi.org/10.
48550/arXiv.2203.10592, arXiv:2203.10592v3 [stat.ML]
3. Barto,A.,Sutton,R.S.:ReinforcementLearning:AnIntroduction.TheMITPress,
2nd edn. (2018)
4. Bellman,R.:Amarkoviandecisionprocess.JournalofMathematicsandMechanics
6,679–684(1957).https://doi.org/10.1512/iumj.1957.6.56038,https://doi.
org/10.1512/iumj.1957.6.56038
5. Bentham,J.:AnIntroductiontothePrinciplesofMoralsandLegislation.Batoche
Books, Kitchener, 2000 edn. (1781)
6. Berger, J.O.: Statistical Decision Theory and Bayesian Analysis. Springer Series
in Statistics, Springer-Verlag, New York, 2nd edn. (1985). https://doi.org/10.
1007/978-1-4757-4286-2
7. Berger-Tal, O., Nathan, J., Meron, E., Saltz, D.: The exploration-exploitation
dilemma: A multidisciplinary framework. PLOS ONE 9(4), e95693 (April 2014).
https://doi.org/10.1371/journal.pone.0095693
8. Bonanno, G.: Decision making (2017), https://faculty.econ.ucdavis.edu/
faculty/bonanno/PDF/DM_book.pdf
9. Da Costa, L., Parr, T., Sajid, N., Veselic, S., Neacsu, V., Friston, K.: Active in-
ference on discrete state-spaces: a synthesis. Journal of Mathematical Psychol-
ogy 102447, 36 (2021). https://doi.org/10.1016/j.jmp.2020.102447, https:
//doi.org/10.48550/arXiv.2001.07203, submitted on 20 Jan 2020 (v1), last re-
vised 28 Mar 2020 (this version, v2)
10. Da Costa, L., Sajid, N., Parr, T., Friston, K., Smith, R.: Reward maximisation
through discrete active inference. arXiv preprint arXiv:2009.08111 v4, 18 pages
(2022), https://doi.org/10.48550/arXiv.2009.08111
11. Da Costa, L., Tenka, S., Zhao, D., Sajid, N.: Active inference as a model
of agency. arXiv preprint arXiv:2401.12917 (2024), https://doi.org/10.48550/
arXiv.2401.12917, accepted in RLDM2022 for the workshop ’RL as a model of
agency’
12. Friston,K.:Thefree-energyprinciple:Aroughguidetothebrain?TrendsinCog-
nitive Sciences 13(7), 293–301 (July 2009). https://doi.org/10.1016/j.tics.
2009.04.005
14 Noé Kuhn
13. Friston,K.,DaCosta,L.,Sajid,N.,Heins,C.,Ueltzhöffer,K.,Pavliotis,G.A.,Parr,
T.: The free energy principle made simpler but not too simple. Physics Reports
1024, 1–29 (June 2023). https://doi.org/10.1016/j.physrep.2023.07.001
14. Friston, K., Da Costa, L., Sakthivadivel, D.A., Heins, C., Pavliotis, G.A., Ram-
stead, M., Parr, T.: Path integrals, particular kinds, and strange things. Physics
of Life Reviews 47 (2023). https://doi.org/10.1016/j.plrev.2023.08.016,
https://doi.org/10.48550/arXiv.2210.12761
15. Friston, K., Rigoli, F., Ognibene, D., Mathys, C., Fitzgerald, T., Pezzulo, G.:
Active inference and epistemic value. COGNITIVE NEUROSCIENCE 6(4), 187–
224(2015).https://doi.org/10.1080/17588928.2015.1020053,http://dx.doi.
org/10.1080/17588928.2015.1020053
16. Genewein, T., Leibfried, F., Grau-Moya, J., Braun, D.A.: Bounded rationality,
abstraction,andhierarchicaldecision-making:Aninformation-theoreticoptimality
principle.FrontiersinRoboticsandAI2, 27(2015).https://doi.org/10.3389/
frobt.2015.00027, https://doi.org/10.3389/frobt.2015.00027, this article is
part of the Research Topic Theory and Applications of Guided Self-Organisation
in Real and Synthetic Dynamical Systems
17. Henriksen, M.: Variational free energy and economics: Optimizing with biases
and bounded rationality. Frontiers in Psychology 11 (November 2020). https://
doi.org/10.3389/fpsyg.2020.549187, https://doi.org/10.3389/fpsyg.2020.
549187
18. Hyland, D., Gavenciak, T., Da Costa, L., Heins, C., Kovarik, V., Gutierrez, J.,
Wooldridge,M.,Kulveit,J.:Multi-agentactiveinference.ForthcomingManuscript
in preparation
19. Kahneman, D., Tversky, A.: Prospect theory: An analysis of decision under risk.
Econometrica 47(2), 263–291 (March 1979)
20. Littman,M.:Atutorialonpartiallyobservablemarkovdecisionprocesses.Journal
of Mathematical Psychology 53(2), 119–125 (2009)
21. Millidge,B.,Seth,A.,Buckley,C.:Understandingtheoriginofinformation-seeking
explorationinprobabilisticobjectivesforcontrol.arXivpreprintarXiv:2103.06859
(2021), https://doi.org/10.48550/arXiv.2103.06859, submitted on 11 Mar
2021 (v1), last revised 24 Nov 2021 (this version, v7)
22. Millidge,B.,Tschantz,A.,Buckley,C.L.:Whencetheexpectedfreeenergy?Neural
Computation 33(2), 447–482 (February 2021). https://doi.org/10.1162/neco_
a_01354, https://doi.org/10.1162/neco_a_01354
23. Mongin, P.: A concept of progress for normative economics. Economics and Phi-
losophy 22, 19–54 (2006). https://doi.org/10.1017/S0266267105000696
24. von Neumann, J., Morgenstern, O.: Theory of Games and Economic Behavior.
Princeton University Press, Princeton, NJ (1953)
25. Ortega,P.A.,Braun,D.A.:Whatisepistemicvalueinfreeenergymodelsoflearn-
ing and acting? a bounded rationality perspective. Cognitive Neuroscience 6(4),
215–216 (2015). https://doi.org/10.1080/17588928.2015.1051525, https://
doi.org/10.1080/17588928.2015.1051525
26. Ortega,P.A.,Braun,D.A.,Dyer,J.,Kim,K.E.,Tishby,N.:Information-theoretic
bounded rationality. arXiv preprint arXiv:1512.06789 (2015), https://doi.org/
10.48550/arXiv.1512.06789, submitted on 21 Dec 2015
27. Parr, T., Markovic, D., Kiebel, S.J., Friston, K.J.: Neuronal message passing us-
ing mean-field, bethe, and marginal approximations. Scientific Reports 9(1), 1–18
(2019). https://doi.org/10.1038/s41598-019-50764-9
28. Parr, T., Pezzulo, G., Friston, K.J.: Active Inference: The Free Energy Principle
in Mind, Brain, and Behavior. The MIT Press (2022)
Addressing the Subsumption Thesis 15
29. Pezzulo, G., Parr, T., Friston, K.: Active inference as a theory of sentient be-
havior. Biological Psychology 186 (February 2024). https://doi.org/10.1016/
j.biopsycho.2023.108741, under a Creative Commons license
30. Pigou, A.C.: The Economics of Welfare. Macmillan and Co., Limited, London
(1920)
31. Ramsey,F.P.:Truthandprobability.In:Braithwaite,R.B.(ed.)TheFoundations
of Mathematics and Other Logical Essays, chap. VII, pp. 156–198. Kegan, Paul,
Trench,Trubner&Co.andHarcourt,BraceandCompany,LondonandNewYork
(1931), originally published in 1926
32. Ross,D.:PhilosophyofEconomics.PalgravePhilosophyToday,PalgraveMacmil-
lan London, 1 edn. (2014). https://doi.org/10.1057/9781137318756, https:
//doi.org/10.1057/9781137318756
33. Sajid, N., Da Costa, L., Parr, T., Friston, K.: Active inference, bayesian optimal
design, and expected utility. In: Cogliati Dezza, I., Schulz, E., Wu, C.M. (eds.)
The Drive for Knowledge: The Science of Human Information Seeking, pp. 124–
146. Cambridge University Press, Cambridge (2022)
34. Savage, L.J.: The Foundations of Statistics. Dover Publications, Inc., New York,
N.Y.,revisedandenlargededn.(1972),originallypublishedbyJohnWiley&Sons
in 1954
35. Schwartenbeck, P., FitzGerald, T.H.B., Dolan, R.J., Friston, K.J.: Evidence for
surprise minimization over value maximization in choice behavior. Scientific Re-
ports 5, 16575 (2015). https://doi.org/10.1038/srep16575, https://doi.org/
10.1038/srep16575
36. Simon, H.A.: Models of Man. John Wiley & Sons, New York (1957)
37. Szipro,G.:Risk,Choice,andUncertainty:ThreeCenturiesofEconomicDecision-
Making. Columbia University Press, New York City (2020)
16 Noé Kuhn
Appendix
A:
Resolving the St. Petersburg Paradox
Consider a lottery on the outcome of a fair coin toss. Starting at two dollars,
the stake doubles with every subsequent outcome of heads. The game ends once
tails comes up for the first time in the sequence. The expected payout E[L] of
the game is thus infinite:
∞
(cid:88) 1
E[L]= ·2i =∞
2i
i=1
Howmuchwouldsomeonepaytoparticipateinthisgame?Takingalinearutility
functiononthepayout,thegamblershouldbewillingtopayanyamounttoenter
thegame.DanielBernoullisuggestedalogarithmicutilityfunctionU(x)=ln(x).
Assumethecostofentryisx.Thentheexpectedutilityofthelotteryhasafinite
value; The amount the agent at most would be willing to enter the lottery:
∞
(cid:88) 1
E[U(L)]= ·ln(2i)=2·ln(2)
2i
i=1
Therefore the agent expects finite utility from the payout of the lottery due to
the concavity of the utility function. As such, only a finite amount will be paid
to enter the game.
B:
Expected Utility and Active Inference for the ’Paraglider’ MDP
The single-step MDP is specified as follows. Therefore note that the subscript
does not pertain to the period:
S=s ,s ,s
1 2 3
A=a ,a
1 2
{P(s |a ),P(s |a ),P(s |a )}={0.6,0,0.4}
1 1 2 1 3 1
{P(s |a ),P(s |a ),P(s |a )}={0,0.4,0.6}
1 2 2 2 3 2
{R(s ),R(s ),R(s )}={1,1.5,0}
1 2 3
Consider an expected utility agent with utility function U(R(s))=R(s)c where
c∈R+. As such,
E[U(a )]=0.6·1c
1
E[U(a )]=0.4·1.5c
2
For c<1→argmax E[U(a)]=a
1
a∈A
For c>1→argmax E[U(a)]=a
2
a∈A
Addressing the Subsumption Thesis 17
So a risk-averse expected utility agent will scale the smaller but safer mountain.
The active inference agent however is indifferent between the two actions. If we
assume the preference distribution to be a softmax on the rewards, then we can
ignore the normalizing denominator as it is constant w.r.t to action. Therefore
we can write the relevant objective function as:
(cid:88) (cid:88) 1
G(a )=− P(s |a )·R(s )− P(s |a )log
t τ t τ τ t P(s |a )
τ t
s s
G(a )=−0.6−0.3065−0.366=G(a )
1 2
→ argminG(a)={a ,a }
1 2
a∈A
Thereforetheoptimalactionoftherisk-averseexpectedutilityagentisasubset
of the optimal active inference agency.
C:
Preference distribution derivation
We maximize the ITBR objective function (7) via first order condition.
(cid:18) (cid:19)
δF 1 Q(s|a)
ITBR = U(s,a)− log +1 = ! 0
δQ(s|a) β P(s|a)
Solve for Q(s|a), and normalize to attain the Gibbs distribution
Q(s|a)=P(s|a)eβU(s,a)−1 ∝P(s|a)eβU(s,a)
P(s|a)eβU(s,a) P(s|a)eβU(s,a)
Q∗(s|a)= =
(cid:80) P(s|a)eβU(s,a) Z
β
s
Which gives us (8)
D:
Getting from ITBR to the divergence objective via the Gibbs distri-
bution.
Solve (8) for U(s,a):
P(s|a)eβU(s)
P∗(s|a)=
Z
β
1
ln(P∗(s|a)·Z )=U(s)
β β
18 Noé Kuhn
Plug this into the ITBR objective function (12) and consider the maximizing
argument a:
1 1 Q(s|a)
argmax E [lnP∗(s|a)+ln(Z )]− ln
a∈A β Q(s|a) β β P(s|a)
=argmax E [lnP∗(s|a)+ln(Z )−lnQ(s|a)+lnP(s|a)]
Q(s|a) β
a∈A
=argmin E [−lnP∗(s|a)−ln(Z )+lnQ(s|a)−lnP(s|a)]
Q(s|a) β
a∈A
=argmin D [Q(s|a)||P∗(s|a)]
KL
a∈A
WhichisthedivergenceobjectiveforMDPs(5).InaPOMDPsetting,thederiva-
tionproceedsanalogouslytoobtaintheFreeEnergyoftheExpectedFuture(10).

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Addressing the Subsumption Thesis: A Formal Bridge between Microeconomics and Active Inference"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
