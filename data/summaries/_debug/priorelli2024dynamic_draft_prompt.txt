=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Dynamic planning in hierarchical active inference
Citation Key: priorelli2024dynamic
Authors: Matteo Priorelli, Ivilin Peev Stoianov

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2024

Abstract: Bydynamicplanning,werefertotheabilityofthehumanbraintoinferandimposemotortrajectories
relatedtocognitivedecisions. Arecentparadigm,activeinference,bringsfundamentalinsightsinto
theadaptationofbiologicalorganisms,constantlystrivingtominimizepredictionerrorstorestrict
themselvestolife-compatiblestates. Overthepastyears,manystudieshaveshownhowhumanand
animalbehaviorscouldbeexplainedintermsofactiveinference–eitherasdiscretedecision-making
orcontinuousmotorcontrol–inspiringinnovativesolutionsinroboti...

Key Terms: planning, instituteofcognitivesciencesandtechnologies, dynamic, italy, nationalresearchcouncilofitaly, hierarchical, padova, inference, active, matteopriorelli

=== FULL PAPER TEXT ===

DYNAMIC PLANNING IN HIERARCHICAL ACTIVE INFERENCE
MatteoPriorelli IvilinPeevStoianov
InstituteofCognitiveSciencesandTechnologies InstituteofCognitiveSciencesandTechnologies
NationalResearchCouncilofItaly,Padova NationalResearchCouncilofItaly
SapienzaUniversityofRome,Italy Padova,Italy
matteo.priorelli@gmail.com ivilinpeev.stoianov@cnr.it
ABSTRACT
Bydynamicplanning,werefertotheabilityofthehumanbraintoinferandimposemotortrajectories
relatedtocognitivedecisions. Arecentparadigm,activeinference,bringsfundamentalinsightsinto
theadaptationofbiologicalorganisms,constantlystrivingtominimizepredictionerrorstorestrict
themselvestolife-compatiblestates. Overthepastyears,manystudieshaveshownhowhumanand
animalbehaviorscouldbeexplainedintermsofactiveinference–eitherasdiscretedecision-making
orcontinuousmotorcontrol–inspiringinnovativesolutionsinroboticsandartificialintelligence.
Still,theliteraturelacksacomprehensiveoutlookoneffectivelyplanningrealisticactionsinchanging
environments. Settingourselvesthegoalofmodelingcomplextaskssuchastooluse,wedelveinto
thetopicofdynamicplanninginactiveinference,keepinginmindtwocrucialaspectsofbiological
behavior: thecapacitytounderstandandexploitaffordancesforobjectmanipulation,andtolearnthe
hierarchicalinteractionsbetweentheselfandtheenvironment,includingotheragents. Westartfrom
asimpleunitandgraduallydescribemoreadvancedstructures,comparingrecentlyproposeddesign
choicesandprovidingbasicexamples. Thisstudydistancesitselffromtraditionalviewscenteredon
neuralnetworksandreinforcementlearning,andpointstowardayetunexploreddirectioninactive
inference: hybridrepresentationsinhierarchicalmodels.
1 Introduction
Threecharacteristicsofthebrainarerelevanttotasksinvolvingplanninginchangingenvironments,suchastooluse.
First,thebrain’sabilitytomaintainestimatesnotonlyofbodilystates,butalsoofexternalphysicalvariablesinrelation
totheself. StudieshaveshownhowthePosteriorParietalCortex(PPC)ofthemonkeybrainencodesobjectswith
sensorimotorrepresentationsreflectingthebodystructure[1,2]. Theserepresentationsareextremelyusefulforobject
manipulationsincetheyaccountefficientlyfortheactionpossibilitiesprovidedbytheobject,alsoknownasaffordances
[3]. Forinstance,onemightencodeacupindifferentways(powerversusprecisiongrips)basedonwhetheronewants
tothrowitordrinkfromit. Further,toacttimelyinadynamicenvironment,thePPCcanencodemultipleobjectsin
parallelduringsequencesofactions,evenwhenthereisaconsiderabledelaybetweendifferentsubgoals[4].
Asecondcharacteristicregardsflexibleanddeephierarchies. Hierarchicalstructuresaresopervasivethattheynotonly
existascausalrelationshipsbetweenphysicalpropertiesoftheenvironment,butarealsoinherenttohowbiological
organisms act over it. Even the most complex kinematic structures of animals follow a rigid hierarchical strategy,
whereby different limbs propagate from a body-centered reference frame. The hierarchical modularity of brain
functionalnetworksiswidelyrecognized[5,6],aswellastherepresentationofthebodyschemainsomatosensoryand
motorareas[7],andtheorganizationofhierarchicalmotorsequencesconcerningparietalandpremotorcortices[8]. In
particular,thebodyschemaisnotastaticentitybutchangesinconcurrencetothedevelopmentofthehumanbody
duringchildhoodandadolescence[9]. Surprisingly,thenervoussystemisabletorelateexternalobjectstotheselfina
waythat,althoughnotreflectingtheactualcausalrelationshipsbetweenthebodyandtheenvironment,isthemost
suitableforbetteroperatinginaspecificcontext. Physiologicalstudieshavedemonstratedthat,withextensivetooluse,
parietalandmotorareasofthemonkeybraingraduallyadapttomakeroomforthetool,increasingthelengthofthe
perceivedlimb[10,11]. Thisadaptationishighlyplastic,assimilatingobjectsinaveryshorttime[12]andinducing
alteredsomatosensoryrepresentationsofthebodymorphologythatpersistevenaftertooluse[13].
4202
voN
21
]IA.sc[
3v85611.2042:viXra
A third characteristic is the ability to construct a dynamic discretized plan based on continuous sensory evidence.
Complextasksinvolvedecision-making,whichthebrainisknowntorealizeviaseveralmethods[14]. Amongthem,
oneisparticularlyrelevant: planningfordeliberation,alsoknownasvicarioustrialsanderrors,wherebyanaction
isselectedafterseveralalternativeshavebeengeneratedandevaluated[15]. Oneofthemostintriguingaspectsof
humanplanningisthecapacitytoimagine,orendogenouslygeneratedynamicrepresentationsoffuturestates,including
potentialtrajectoriesandsubgoalsthatbringtosuchstates[16,17]. Thehippocampusisakeyneuralstructureknown
tosupporttrajectorygeneration,althoughplanningisaccomplishedinconcertwithotherareas[15].
Howdoesthehumanbraincapturethehierarchicalorganizationanddynamicsoftheselfandtheenvironmenttoafford
purposefulplanning?Onerecenttheoryisthatofpredictivecoding[18,19,20,21],whichhasbeenattractingincreasing
interestinrecentyearsandproposesitselfasaunifyingparadigmofcorticalfunction. Accordingtopredictivecoding,
livingbeingsmakesenseoftheworldbybuildinganinternalgenerativemodelthatimitatesthecausalrelationshipsof
theexternalgenerativeprocess. Fromahigh-levelhypothesisabouttheworld,acascadeofneuralpredictionstakes
place,eventuallyleadingtoalow-levelguessaboutsensoryevidence. Comparingthemodel’sguesswiththesensorium
triggersanothercascadeofpredictionerrorsthattravelbacktothedeepestcorticallevels. Themodeliterativelyrefines
itsstructureuntilallthepredictionerrorsareminimized,thatis,untilitcorrectlypredictswhatwillhappennext. This
optimizationdiffersfromthemoretraditionalviewofdeeplearning,inthatthemessagepassingislocalandwhat
climbs up the hierarchy does not signal the detection of a feature, but how much the model is surprised about its
prediction. Besideshavingstimulatedcognitiveandneuralstudiesunderseveralcircumstances[22,23,24,25],this
theoryhasalsoinfluencednoveldirectionsinmachinelearning: PredictiveCodingNetworks(PCNs)havebeenshown
togeneralizewelltoclassificationorregressiontasks[26,27],withkeyadvantagescomparedtoneuralnetworksand
stillapproximatingthebackpropagationalgorithm[28,29,30].
Whilepredictivecodingcanelucidateillusionsandvisualphenomenasuchasbinocularrivalry[31],itexplainsjust
thefirst(perceptual)halfofthestory. Morespecifically,itdoesnotexplainwhyinteractionswiththeenvironment
occur–aprocessthatresults,consideringtheaboveexample,inthemonkeybrainactivelydistortingitsbodyschema
during tool use. On this trail, a second innovative perspective has been proposed, aspiring to unveil a unified first
principle not just on cortical function, but on the behavior of all living organisms. This perspective, called active
inference [32, 33, 34, 35], is grounded on the same theoretical basis of predictive coding but further assumes two
keyaspectsofbiologicalbehavior. First,thatalivingbeingdoesnotmaintainstatichypothesesabouttheworldbut
alsoconstructsinternaldynamics–eitherasinstantaneoustrajectoriesorfuturestates–toanticipatetheunfolding
ofeventsoccurringatdifferenttimescales. Second, thatthesedynamichypothesescanbefulfilledbymovements.
Thelatterassumptionreplacesmodelswithagents,conveyingasomewhatcounterintuitivebutinsightfulimplication:
whileperceptionletstheagent’shypothesisconformtotheenvironment(asinpredictivecoding),actionforcesthe
environmenttoconformtothehypothesis–bysamplingthoseobservationsthatmakethehypothesistrue. Ifsuch
hypothesesorbeliefscorrespondtodesiredstatesdefined,e.g.,bythephenotype,cyclingbetweenactionandperception
ultimatelyallowstheagenttosurvive. Thisisthecoreoftheso-calledfreeenergyprinciple,whichstatesthatinorder
tomaintainhomeostasis,allorganismsmustconstantlyandactivelyminimizethedifferencebetweentheirsensory
statesandtheirexpectationsbasedonasmallsetoflife-compatibleoptions. Givingapracticalexample,ifIbelieveto
findmyselfwithatoolinhand,Iwilltrywithallmystrengthtoobservevisualimagesofthetoolinmyhand;indoing
this,acombinedreachingandgraspingactionhappens. Thisviewdistancesitselffromthestimulus-responsemapping
widelyestablishedinneuroscience,andevidenceindicatesthatitcouldbemorebiologicallyplausiblethanoptimal
controlandReinforcementLearning(RL)[36,37,38,39].
Activeinferenceimplementationscanbedividedintotwoframeworks,whichhavebeenusedtosimulatehumanand
animalbehaviorsunderthetwocomplementaryaspectsofmotorcontrol[40,41,42,43,44,45,46]anddecision-making
[47,48,49,50,51]. Inprinciple,activeinferencemightbekeyforunderstandinghowgoal-directedbehavioremerges
inthehumanbrain[52]. Forinstance, relevantobjectsusedformanipulationmaygraduallybecomepartofone’s
identitythroughaclosedloopbetweenmotorcommandsandsensoryevidence,meaningthattheboundaryoftheself
fromtheenvironmentincreaseswhenevertheagentpredictstheconsequencesofitsownmovements[53]. Additionally,
activeinferencemightleadtokeyadvanceswithcurrentartificialagents,takingforwardapromisingresearcharea
knownasplanningasinference[54,55,56]. Thethreecharacteristicsdelineatedabovearefundamentaltodesigning
activeinferenceagentsthatcantacklereal-lifeapplicationssuchastooluse. Buthowtocombinethemintoasingle
view? Inotherwords,howtoperformdynamicplanningwithhierarchicalstructuresofseveralobjects?
2
To answer this question, in this study we explore an alternative direction in active inference, i.e., toward hybrid
computationsinhierarchicalsystems. Weanalyzemanydesignchoicesthathavebeenappliedinthemotorcontrol
domain,withanin-depthlookatobjectaffordances,deephierarchies,andplanningwithcontinuoussignals. Asking
ourselves how to model tool use, we start from a simple unit and construct richer modules that can be linked in a
hierarchical fashion, exhibiting interesting high-level features. In Chapter 2, we consider a single-DoF agent and
explorehowtoaccountforaffordancesandrealizeamulti-stepbehaviorincontinuoustimeonly. InChapter3,we
analyzetheimplicationsofcombiningdifferentunitsinasinglenetwork,usingmorecomplexkinematicconfigurations
anddistinguishingbetweenintrinsicandextrinsicdynamics. InChapter4,wedescribetheadvantagesofusingdiscrete
decision-makingincontinuousenvironments,focusingonhybridstructuresanddrawingsomeparallelismsbetweenthe
twoworlds. Finally,intheDiscussionweelaborateonthebenefitsofaddressingdiscreteandcontinuousrepresentations
together,andgiveafewsuggestionsforfutureworkonthissubject.
2 Modelingaffordances
Inthischapter,weexplaintheinferencemechanismsofabasicunitincontinuoustime. Wethendiscussonebyonethe
changesandfeaturesthatweintroduce,inordertoachieveamulti-stepbehaviorinsimpletasksthatdonotrequire
deephierarchicalmodelingnoronlinereplanning.
Thecontinuous-timeactiveinferenceframework[57,36,40]–generallycomparedtothelow-levelsensorimotorloops
–makesuseofgeneralizedfiltering[58]tomodelinstantaneoustrajectoriesoftheselfandtheenvironment; these
trajectoriesareinferredbyminimizationofaquantitycalledvariationalfreeenergy,whichisthenegativeofwhatin
machinelearningisknownastheevidencelowerbound. Differentlyfromoptimalcontrol,motorcommandsinactive
inferencederivefromproprioceptivepredictionsthatarefulfilledbyclassicalspinalreflexarcs[38]. Thiseliminatesthe
needforcostfunctions–astheinversemodelmapsfromproprioceptive(andnotlatent)statestoactions–andreplaces
acontrolproblemwithaninferenceproblem[37].
Modelingofobjectsinactiveinferencehasbeenrecentlydoneinthecontextofactiveobjectreconstruction[59,60,61,
62]–whereanagentencodedindependentrepresentationsformultipleelements,andusedactiontomoreaccurately
inferitsdynamics; forsimulatingoculomotorbehavior[63]–wherethedynamicsofatargetbeliefwasbiasedby
ahiddenlocation;orforanalyzingepistemicaffordance[51],i.e.,thechangesinaffordanceofdifferentobjectsin
relationtotheagent’sbeliefs. Incontinuoustime, suchaffordancescanbeexpressedinintrinsicreferenceframes
correspondingtopotentialagent’sconfigurations,definingspecificwaystointeractwiththeobjects. Manipulating
theseadditionalbeliefsdependingontheagent’sintentions[64]permitseffectivelyoperatingindynamiccontexts,e.g.,
trackingatargetwiththeeyes[63],orgraspinganobjectonthefly[65]andplacingitatagoalposition[66].
2.1 Asimpleagent
ThemostelementaryunitisrepresentedinFigure1a. Thisisthesimplestformulationofacontinuous-timeactive
inferenceagent,wherewekeptonlythekeynodes. Thisallowsustoeasilydescribeavelocity-controlleddynamic
systemwiththefollowinglikelihoodg anddynamicsf:
p
o =g (x)+w
p p o,p
(1)
x′ =f(x)+w
x
wherexando arerespectivelycalledhiddenstatesandobservations(thesubscriptpindicatestheproprioceptive
p
domain),andtheletterwindicatesnoisetermssampledfromGaussiandistributions. Forsimplicity,weconsideredjust
twotemporalorders–althoughallthefeaturesweelucidateinthefollowingholdforasystemofgeneralizedcoordinates
[58]–andwedefinedalikelihoodfunctiononlyforasingletemporalorder.Weassumethatthecorrespondinggenerative
modelisfactorizedasinFigure1b,expressedintermsofprecisions(orinversevariances)Π. Notethatweintroduced
apriorη overthehiddenstates,whichisnotgenerallyusedincontinuous-timeformulations,butitisthekeyelement
x
connectingdifferentlevelsindiscrete-timeactiveinference[67]orPCNs[25]–aswillbeexplainedlater. Alsonote
thatweusedageneralizednotationforinstantaneoustrajectoriesorpaths,i.e.,x˜ =[x,x′],wherexwillbeindicated
inthefollowingasthe0thorder,andx′ asthe1storder. Wehighlightedingreenandredrespectivelytheinputand
outputoftheunit,namelythepriorη andtheobservationso .
x p
3
(a) (b)
Figure1: (a)Factorgraphofabasicunitforstaticreaching. Variablesandfactorsareindicatedbycirclesandsquares,
respectively. Hiddenstatesx(e.g.,thearmangle)generateobservationso (e.g.,thearmproprioception)throughthe
p
likelihoodfunctiong ,andtheir1stderivativesx′(e.g.,thearmvelocity)throughadynamicsfunctionf. Incontrastto
p
optimalcontrol,hereactionfollowsobservationpredictionerrorsarisingfromasimpleattractorρembeddedinthe
modeldynamics,orfromapriorbeliefη overthearmangle. (b)Agent’sgenerativemodel.
x
Exactcomputationoftheposteriorp(x˜|o )isunfeasiblesincetheevidencerequiresmarginalizingovereverypossible
p
(cid:82)
outcome,i.e.,p(o )= p(x˜,o )dx˜. Forthisreason,estimationofhiddenstatesx˜iscarriedoutthroughavariational
p p
approach[68],e.g.,byminimizingthedifferencebetweenaproperlychosenapproximateposteriorq(x˜)andthetrue
posterior. ThisdifferenceisexpressedintermsofaKullback-Leibler(KL)divergence:
(cid:90) q(x˜)
D [q(x˜)||p(x˜|o )]= q(x˜)ln dx˜ (2)
KL p p(x˜|o )
x˜ p
Thedenominatorp(x|o )stilldependsonthemarginalp(o ),buttheKLdivergencecanberewrittenintermsofthe
p p
logevidenceandaquantityknownasthefreeenergyF:
(cid:20) (cid:21) (cid:20) (cid:21)
q(x˜) q(x˜)
F = E ln = E ln −lnp(o ) (3)
q(x˜) p(x˜,o p ) q(x˜) p(x˜|o p ) p
SincetheKLdivergenceisalwaysnonnegative,thefreeenergyprovidesanupperboundonsurprise,i.e.,F ≥lnp(o ).
p
Hence, minimizing F achieves the dual objective of keeping surprise low while estimating the true distribution.
Assuming that the approximate posterior can be factorized into independent contributions, and further assuming
thateachcontributionisGaussian–i.e., q(x˜) = N(µ˜ ,P˜ ), withgeneralizedmeansµ˜ andprecisionsP˜ –the
x x x x
optimization process breaks down to the minimization of (precision-weighted) prediction errors in terms of the
approximateposterior–see[35]formoredetails:
ε =o −g (µ )
o,p p p x
ε =µ −η (4)
η,x x x
ε =µ′ −f(µ )
x x x
Then,theinferenceofthemeansµ˜ =[µ ,µ′](alsocalledbeliefs)oftheposterioroverthehiddenstatesisreduced
x x x
tothefollowingmessagepassing:
(cid:20) µ˙ (cid:21)  µ′ x −Π η,x ε η,x +∂ x g p TΠ o,p ε o,p +∂ x fTΠ x ε x 
µ˜˙ x = µ˙′ x =Dµ˜ x −∂ x˜ F =  (5)
x −Π ε
x x
4
whereDisanoperatorthatshiftseveryderivativebyone,i.e.,Dµ˜ =[µ′,0]. Thistermarisesbecausethegenerative
x x
modelmaintainsabeliefnotoverastaticpoint,butoveradynamictrajectory,andonlywhenthemotionofthemean
µ˜˙ equalsthemeanofthemotionDµ˜ ,isthefreeenergyminimized. Inshort,theinferentialprocessdoesnotinvolve
x x
matchingastate(asinPCNs)buttrackingapath[69]. UnpackingEquation5,wenotethatthe0thorderissubjecttoa
forwarderrorfromtheprior,abackwarderrorfromthelikelihood,andabackwarderrorfromthedynamicsfunction.
Ontheotherhand,the1storderisonlysubjecttothelatterbutintheformofaforwarderror. Thebeliefisthenupdated
viagradientdescent,i.e.,µ˜ =µ˜ +∆ µ˜˙ ,where∆ isatimeconstant.
x,t+1 x,t t x t
Howcanthisagentperformasimplereachingmovement? AshighlightedinFigure1a,wecanencodethearmangle
andvelocityasgeneralizedhiddenstates. Wewilltalklaterabouttherelationbetweenproprioceptiveandexteroceptive
domains;fornow,weconsiderasingleDoFthathasaunivocalmappingbetweenthejointangleofthearmandthe
Cartesianpositionofthehand. Indicatingthetargettoreachbyρ,wecandefinethefollowingdynamicsfunction:
f(x)=ρ−x (6)
expressingasimpleattractortowardthetarget[53,70,71,72,73]. Thesedynamicsdonotexistintheactualgenerative
process,anditisindeedthisdiscrepancythatforcestheenvironmenttoconformtotheagent’sbeliefs. Specifically,
Equation6meansthattheagentthinksitshandwillbepulledtowardthetargetwithastrengthproportionaltothe
precisionΠ . Infact,theattractoraffectsthebeliefupdatethroughthedynamicspredictionerrorε ,expressinga
x x
differencebetweentheestimatedvelocityµ′ andtheonepredictedbytheagentthroughthedynamicsfunctionf. Note
x
thatthiserrorappearsinbothtemporalorders: inbrief,ε imposesatrajectoryatthe1storderwhichinturnaffectsthe
x
0thorderdirectlythroughµ′,andindirectlythroughthegradient∂ f.
x x
(a) (b)
Figure2: (a)Inthistask,theagent(asingleDoF)hastoreachatargetanglerepresentedbytheredcircle. Estimated
andrealarmsaredisplayedincyanandblue,respectively. Here,π =0,ρ=120°,andµ wasinitializedto−40°.
η,x x
Thetimestepisindicatedinthebottomleftcornerofeachframe. Sincethebeliefwasinitializedatanegativevalue,
thelikelihoodinitiallypullsthearmtowardthewrongdirectionbeforeadaptingtothedynamicsattractor. (b)The
topgraphshowstheevolutionoftherealanglex,itsbeliefµ ,andthetargetangleρ. Themiddlegraphshowsthe
x
evolutionofthebeliefofthevelocityµ′ andthebeliefderivativeµ˙ . Thebottomgraphshowstheevolutionofallthe
x x
componentsthatcomprisethebeliefupdate: thebeliefofthevelocityµ′,thelikelihoodgradient∂ gTΠ ε ,the
x x o,p o,p
dynamicsgradient∂ fTΠ ε ,andtheweighteddynamicspredictionerror−Π ε . Thelatterhasbeenplottedto
x x x x x
compareitsmagnitudewiththeothercomponents,althoughaffectingthe1sttemporalorder.
TheinteractionsbetweenthesequantitiescanbebetterunderstoodfromFigure2,showingareachingmovementwith
thedefineddynamicsfunctionandthetrajectoriesoftheagent’sgenerativemodel. Here,thebeliefissubjecttotwo
differentforces: alikelihoodgradientpushingittowardwhatitiscurrentlyperceiving(i.e.,therealangle),andthe
othercomponentsthatsteerittowardthebiaseddynamics(i.e.,thetargetangleρ). Notehowinthethirdplot,ofthe
threecomponentsthatcomprisethebeliefupdate,thebackwarderror∂ fTΠ ε hasthesmallestamplitude. While
x x x
theexactinteractionsarisingfromthedynamicspredictionerrorhaveyettobeanalyzed,inthefollowingweassume
5
thatgoal-directedbehaviorisachievedthroughtheforwarderroratthe1storder−Π ε . Analternativewouldbeto
x x
directlycontrolthebackwarderrorwithoutmaintainingabeliefofthe1storder[70],whichhoweverrequirestakinga
gradientintoaccountandmaybemorechallengingwhendefiningappropriateattractors. Finallyseehow,inthemiddle
plot,theagenttriesateveryinstanttominimizethedifferencebetweenµ′ andµ˙ ,thustrackingtheactualpathofthe
x x
hiddenstates.
Buthowdoestheagentmoveinpractice? AsmentionedintheIntroduction,actionistheothersideofthecoinofthe
freeenergyprinciple,throughwhichtheagentsamplesthoseobservationsconformingtoitspriorbeliefs. Infact,in
additiontotheperceptualinferencetypicalofpredictivecoding,activeinferenceassumesthatorganismsminimizefree
energyalsobyinteractingwiththeenvironment;thisminimizationbreaksdowntoanevensimplerupdatethatonly
dependson(proprioceptive)predictionerrorsε . Sincethesepredictionerrorsaregeneratedfromtheagent’sbelief,
o,p
thismeansthatwheneverthelatterisbiasedtowardsomepreferredstate,movementnaturallyfollows. Thereisthusa
delicatebalancebetweenperception–inwhichpredictionerrorsclimbupthehierarchytobringthebeliefcloserto
theobservations–andaction–inwhichpredictionerrorsaresuppressedatalowlevelsothattheobservationsare
broughtclosertotheirpredictions. However,thereisanopenissueregardinghowactiveinferenceshouldbepractically
realizedincontinuoustime. Afewstudiesdemonstratedthatusingexteroceptiveinformationdirectlyforcomputing
motorcommandscouldresultinsmoothermovementsandresolutionofvisuo-proprioceptiveconflicts[32,44,64],and
infactsomeroboticimplementationseffectivelyusedthisapproach[70,71]. However,evidenceseemstoindicatethat
motorcommandsaregeneratedbysuppressionofproprioceptivesignalsonly[38,37],whichisalreadyintheintrinsic
referenceframeneededformovementandthusresultsineasierinversedynamics. Forthisreason,inthefollowingwe
assumethatmovementsarerealizedbyminimizingthefreeenergywithrespecttoproprioceptivepredictionerrors:
a˙ =−∂ F =−∂ gTΠ ε (7)
a a p o,p o,p
where ∂ g performs an inverse dynamics from proprioceptive predictions to motor commands a, likely to be
a p
implementedbyclassicalspinalreflexarcs. Asalastnote,theactionscanalsodependonmultipleorders–velocity,
acceleration,andsoon–allowingmoreefficientmovementandcontrol[74,75,76,77],butsinceitisbeyondour
scope,weonlyminimizethe0thorder. Nonetheless,1st-ordermovements–e.g.,maintainingaconstantvelocity–are
stillpossiblebyspecifyingappropriatedynamicsofthehiddenstates.
2.2 Trackingobjects
(a) (b)
Figure3: (a)Thetargetisnowencodedinthehiddencausesv,generatingadynamicattractorforobjecttracking. In
fact,bothhiddenstatesandhiddencausesgeneratepredictionsthroughproprioceptiveandvisuallikelihoodfunctions
g andg ,andbothconcurinestimatingthe1st-orderhiddenstatesx′. (b)Agent’sgenerativemodel.
p v
6
(a) (b)
Figure4: (a)Inthistask,theagenthastotrackatargetanglerotatingataconstantvelocity. Estimatedandrealtargets
aredisplayedinpurpleandred,respectively. Here,Π =0,Π =0,vwasinitializedto60°,andbothµ andµ
η,x η,v x v
wereinitializedto0°. Here,thebeliefofthehiddencausespullsthebeliefofthehiddenstateswithitwhileapproaching
therealtargetangle. (b)Thetopgraphshowstheevolutionoftherealanglex,itsbeliefµ ,thetargetanglev,and
x
itsbeliefµ . Themiddlegraphshowstheevolutionofthebeliefofthevelocityµ′ andthebeliefderivativeµ˙ ,as
v x x
before. The bottom graph shows the evolution of all the components that comprise the hidden causes update: the
likelihoodgradient∂ gTΠ ε ,andthedynamicsgradient∂ fTΠ ε . Notehow,inthemiddleplot,theestimated
v v o,v o,v v x x
1sttemporalorderstabilizestoanon-zerovalueastheagentrotateswithaconstantangularvelocity.
Thesimpleagentdefinedintheprevioussectioncanonlyrealizefixedtrajectoriesembeddedinthedynamicsfunction,
sohowcanittrackmovingobjects? Thisisusuallydonebyintroducingakeyconceptinactiveinference,thehidden
causesv,whichlinkhierarchicallevelsandspecifyhowthedynamicsfunctionevolves. Intheactiveinferenceliterature
of motor control, they are also used to encode the target to be reached [32, 78, 63, 79], as depicted in Figure 3a.
Consideringthetargetasacausalvariableforthehiddenstatesandsensoryobservationsmakessensefromanactive
perspectivewhereby“itisanobjectIwanttoreachthatgeneratesmymovements”. Now,theagent’sgenerativemodel
becomesthatofFigure3b. Notethattherearetwopriors,oneoverthehiddenstatesandanotheroverthehiddencauses,
respectivelydenotedbyη andη . Also,bothdynamicsandlikelihoodfunctionsdependonthehiddencauses,andwe
x v
assumedafurtherfactorizationforthelikelihood,whereo ando denotethearmandtargetobservations,respectively.
p v
Forsimplicity,weassumethatthevisuallikelihoodfunctiong isasimpleidentitygeneratingthetargetangle. Itis
v
throughtheconnectionbetweenhiddencausesandobservationsthattheagentcanoperateindynamicenvironments. In
fact,wecandefinethefollowingdynamicsfunction:
f(x,v)=v−x (8)
wherewejustreplacedthestatictargetρwiththehiddencauses. Asforthehiddenstates,wedefinea(Gaussian)
approximateposterioroverthehiddencausesq(v)=N(µ ,P ),withmeanµ andprecisionP . Then,themeanis
v v v v
updatedaccordingto:
µ˙ =−∂ F =−Π ε +∂ gTΠ ε +∂ fTΠ ε (9)
v v η,v η,v v v o,v o,v v x x
wherewedefinedthefollowingobservationandpriorpredictionerrorsintermsoftheapproximateposterior:
ε =o −g (µ )
o,v v v v
(10)
ε =µ −η
η,v v v
As evident, the hidden causes are subject to a prior prediction error, a backward dynamics error, and a backward
likelihooderror–similartotheupdateofthehiddenstates,exceptthatthisinferenceisoverastateandnotapath. Via
thebackwardlikelihooderror,theagentcancorrectlyestimatethetargetconfigurationwheneveritmoves,asshown
inthetrackingsimulationofFigure4. Concerningthedynamicspredictionerror,itcannowflowintotwodifferent
7
pathways: specifically,theroleofthegradients∂ f and∂ f aretorespectivelyinferthestateandthecausethatmay
x v
havegeneratedaparticularvelocity;theiractualrolewillbeclearinChapter4.
2.3 Intentionmodulationandobjectaffordances
Althoughcapableofoperatingindynamiccontexts,thelastapproachstillreproducesasimplescenarioinwhicha
target has no internal dynamics and always has the role of a cause for a hidden state. In other words, it does not
permitmodelingrealistictaskssuchasapick-and-placeoperation,whereanobjectisfirstthecauseofareachingand
graspingmovement,butthenitbecomestheconsequenceofanothercausesuchasagoalposition,resultinginaplacing
movement;critically,itdoesnotallowtomodelataskwhereinnotonlythedynamicsoftheself,butalsothedynamics
ofthetargetmustbelearned(e.g.,ifamovingobjectshouldbegraspedonthefly,theagentshouldinferitstrajectory
toanticipatewhereitwillfall).
Itfollowsthattooperateinacomplexenvironment,theagentmust(i)maintaincompleterepresentationsforeachentity
thatitwantstointeractwith,and(ii)flexiblyassigncausesandconsequencesforthenextmovementdependingonthe
currentcontext–inasimilarwaytopoliciesindiscretemodels,aswillbeexplainedlater. Therefore,wefirstencode
multipleenvironmentalentitiesaspotentialbodyconfigurationsinthehiddenstates,i.e.,x=[x ,x ,...,x ],where
0 1 N
x istheactualbodyconfiguration(asbefore),andN isthenumberofentities[64]. Consequently,thefactorized
0
likelihoodfunctiongeneratesaproprioceptiveobservationforthefirstcomponentx ,andvisualobservationsforeach
0
entity:
o=[o ,o ,...,o ]=[g (x ),g (x ),...,g (x )] (11)
p v,1 v,N p 0 v 1 v N
Here,visualobservationsareassumedtobeintheCartesiandomain,sothevisuallikelihoodfunctiong generates
v
thehandpositionsofthepotentialconfigurationsthroughforwardkinematics. Thisstructureissimilartotheprevious
model,exceptthatthetargetisnowembeddedinthehiddenstatesalongwiththehand,andthatthereisnoconnection
betweenhiddencausesandobservations. Wecoulddefineasimilarfactorizationforthehiddencausesanddynamics
function,suchthateachentitywouldhaveanindependentdynamicsbiasedbyaspecificcause(e.g.,wherethearmor
thetargetwillbeinthefuture);however,thisisoflimiteduseinapick-and-placeoperationthatdemandsinteraction
betweenentities. Wehencecomputeanintentionalstatewithasinglefunction,suchas:
i (x)=W x+b (12)
m m m
TheweightsW performalineartransformationofthehiddenstatesthatcombineseveryentity,whilethebiasb
m m
imposesastaticconfiguration[80]. Equation12canberealizedthroughsimpleneuralconnections,whereintheweights
areencodedassynapticstrengthsandthebiasrepresentsthethresholdneededtofireaspike,andbothcanbeknowna
prioriorlearnedviasensoryevidence. Anerroristhencomputedbetweenthisintentionalstateandthecurrentone:
e =i (x)−x (13)
i,m m
ThisvectorhasthesameroleastheattractorofEquation8,butnowitpointstowardafunctionofthehiddenstates.
Finally,wedefinethefollowingdynamicsfunction:
f (x,v )=v e (14)
m m m i.m
multiplyingtheerrorbyasingle-valuehiddencausev . Thus,thelatterisnotintendedasanexplicittrajectoryprior
m
overthehiddenstates(e.g.,encodingwheremyarmwillbeinthefuture),whoseroleisnowdelegatedtothebiasb ;
m
butasanattractorgain,wherebyahighvalueimpliesastrongforcetowardthefuturestate. Sincei isusedtodefinea
m
pathforthehiddenstatesaimingtoproduceadesiredconfiguration,wecallitanintention.
ThedynamicsfunctionofEquation14isnotcomposedofsegregatedpathwaysasforthelikelihood,butaffectsallthe
environmentalentitiesatonce–e.g.,itcomputesatrajectoryforthearmdependingonthetarget. Thestepsperformed
bytheagentduringareachingmovementarethefollowing: (i)the0thorderimposesatrajectorytothe1storderand
generatesasensoryprediction;(ii)the0thorderinferstheconsequencesofitspredictions,henceitisnowbiasedtoward
boththeintentionalstateandtheobservation;(iii)aproprioceptivepredictionisgeneratedfromthisnewbiasedposition,
eventuallydrivingaction. Thisapproachcanbeseenasageneralizationof[63]where,inthecontextofoculomotor
behavior,thetargetandthecenterofgazewereencodedashiddenstates,eachwiththeirowndynamicsandattracted
byahiddenlocation. Althoughlimitedcomparedtonon-lineardynamicsfunctions(e.g.,obstacleavoidancecanbe
realizedviarepulsivepotentials[80]),withthespecificformdefinedabove–alongwiththehiddenstatesfactorization–
8
(a) (b)
Figure5: (a)Factorgraphoftheunitwithobjectaffordances. Hiddenstatesarefactorizedintoindependentcomponents
thatencodetheactualbodilystatesandpotentialconfigurationsrelatedtotheobjects. Thefirstcomponentx generates
0
proprioceptivepredictions,whilethesuccessivecomponentsgeneratevisualpredictionsoftheobjects. Everyhidden
causev nowdefinesanattractorgainexpressingthestrengthofanagent’sintention(encodedasadistinctevolutionof
m
theworld). Thesearecombinedtoproduceatrajectoryη′,comprisingallthebodyconfigurationsx . Thetransition
x n
betweenintentionscanbeachievedbyahigher-levelprior,e.g.,abeliefoftactilesensations. TheweightsW ofthe
m
intentioncanbeused,e.g.,totrackmovingobjects,whilethebiasb realizesastaticconfiguration. See[64,66]for
m
moredetails. (b)Agent’sgenerativemodel.
thereisahighflexibilityforcomplexinteractions. Further,interpretingthehiddencausesasagainstillmakessense
fromanactiveinferenceperspectivebecausewhatisrepresentedatahigherlevelistheintentiontomoveatthetarget,
whilethetargetlocationisinferredatalowerlevel.
Takenalone,consideringahiddencauseasanattractorgainmaynotseemsohelpful. However,asdepictedinFigure
5a,wecancombineM intentionsinthefollowingway:
M M
(cid:88) (cid:88)
η′ =f(x,v)= f (x,v )= v (i (x)−x) (15)
x m m m m
m m
Inshort,theagententertainsM distinctdynamichypothesesofhowtheworldmayevolve. Trajectoriesf (x,v )
m m
areseparatelycomputedfromeachintentioni andwiththeirrespectivegains;then,thefinaltrajectoryη′ isfound
m x
bycombiningallofthem. Thereasonwhyweusedthepriornotationforthetrajectorypredictionswillbeclearin
Chapter4. Sinceeachattractore isproportionaltoitshiddencausev ,thelatterlendsitselftoaparallelismwith
i,m m
thepoliciesofdiscretemodels,aswillbeexplainedlater: ifv issetto1andalltheothersto0,thehiddenstates
m
willbesubjectonlytointentionm;conversely,ifmultiplehiddencausesareactive,thehiddenstateswillbepulled
towardacombinationofthecorrespondingintentions. Thismeansthatthehiddencausesactbothasattractorgains–
expressingtheabsolutestrengthbywhichthebeliefissteeredtowardthedesireddirection–andasintentionmodulators
–definingtherelativestrengthbetweeneachtrajectory. Asaresult,wehaveanadditionalmodulationthatcombines
withthedynamicsprecisionΠ ;theirinteractionswillbeexplainedinChapter4. Theresultingbehaviorissimilarto
x
theLotka-Volterradynamicsusedin[81]. Inthelatter,theagentrevisitsinsequencepointsinattractorspacelinkedto
specificlocations,andthedefineddynamicsensurethatonlyoneattractorisactiveatanytime. Thispermitsmodeling
itinerantmovementssuchashandwritingorwalking.
9
(a) (b)
Figure6: (a)Inthistask,theagenthastofirsttouchthemovingball(inred)andthentrackthemovingsquare(in
grey). Thetransitionisdonebyatactilebelief. Notehowthroughoutthetask, theagentmaintainspotentialbody
configurationsrelatedtothetwoobjects. See[64,66]formoredetails. (b)Thefirstgraphshowsthedifferencebetween
theestimatedandrealhandpositions(dottedblue),thedifferencebetweenthepotentialhandpositionrelatedtotheball
andtheactualballposition(dottedred),thedifferencebetweenthepotentialhandpositionrelatedtothesquareand
theactualsquareposition(dottedgreen),thedifferencebetweentherealhandandballpositions(soliddarkred),and
betweentherealhandandsquarepositions(soliddarkgreen). Thesecondgraphshowstheevolutionofthehidden
causesassociatedwiththetwointentions,µ andµ . Thethirdgraphshowstheevolutionoftheattractors
v,ball v,square
e ande ,andthedynamicsfunctionsf andf ,regardingthefirstjointangle. Thelastgraphshows
i,ball i,square ball square
theevolutionofthetactileobservationo ,anditsbeliefµ .
t t
ThegenerativemodelisshowninFigure5b. Theupdateruleforthe0th-orderhiddenstatesbecomes:
N
(cid:88)
µ˙ =µ′ −Π ε +∂ gTΠ ε + ∂ gTΠ ε +∂ fTΠ ε (16)
x x η,x η,x x p o,p o,p x v o,v,n o,v,n x x x
n
Inparticular,thedynamicspredictionerror:
ε =µ′ −η′ (17)
x x x
realizesanaveragetrajectorythattheagentpredictsforthecurrentsituation. Thisapproachiseffectivefortworeasons.
First,itallowsdefiningacompositemovementintermsofsimplersubgoals,whichcanbetackledseparately;thiscan
behelpful,forinstance,ifonehastoanalyzethebehaviorofanagentwhensubjecttotwoormoreopposingpriors[64].
Second,asimplemulti-stepbehaviorwithoutplanningcanbeachieved[66],whereinonejustneedstomodulatethe
hiddencauses. Transitionsbetweencontinuoustrajectoriescouldthenberealizedbyhigher-levelpriors,e.g.,abeliefof
tactilesensations. Third,andmostimportantly,itpermitsmaintaininginparallelpotentialbodyconfigurationsrelated
totheobjectstobemanipulated–thusprovidingefficienttransitionsbetweenmovements–andencodingtheobjects
accordingtotheiraffordancesandtheagent’sintentions(e.g.,graspingacupbythehandleorwiththewholehand).
ThesefeaturesareillustratedinthesimulationofFigure6,showingatwo-stepreachingtaskwithmovingobjects.
3 Hierarchicalmodels
Sofar,wehavediscussedseveralunitswithtwokindsofinputs–aprioroverthehiddenstatesandaprioroverthe
hiddencauses–andonekindofoutput–the0th-orderobservation. Inthischapter,wefocusoncombiningsuchunitsin
10
asinglenetworktoachieveamoreadvancedandefficientcontrol. Forthis,wewillmakeuseofthefirstinput,leaving
thediscussionaboutthesecondtothenextchapter.
Inhierarchicalactiveinferencemodels,unitsarearrangedinlayerssothattheoutputofonelayerprovidesinputtothe
subordinatelayers. Thisarchitecturepermitsrepresentingcomplexdata,suchasconvolutionsmodelsornonlineartime
series[57]. Inmotorcontrol,a(deep)hierarchyofgoalsintegratescontrolandmotivationalstreamsofthebrain[82].
Forrobotics,ahierarchicalkinematicmodelcanbedesignedincontinuoustime,whereineachunitencodesacertain
DegreeofFreedom(DoF)inintrinsicandextrinsicreferenceframes[80]. Thispermitsrealizingadvancedmovements
thatinvolvesimultaneouscoordinationofmultiplelimbs,e.g.,movingwithaglassinhand. Thishierarchicalstructure
canbegeneralizedtoperformhomogeneoustransformationsbetweenreferenceframes,e.g.,perspectiveprojections
[83].
3.1 Intrinsicandextrinsiccauses
Thelastunitpresentedaffordsamulti-stepbehaviorincontinuoustimethataccountsforobjectaffordancesand,to
someextent,fordynamicelementsoftheenvironment. However,itcanonlyestimatebodyconfigurations,whilein
real-lifeapplicationsweconstantlyplanmovementsinthespatialdomain. Further,itonlygeneratesvisualpredictions
of an object related to a single DoF (e.g., the hand), while we generally deal with much more complex kinematic
structureswithdifferentbranchessuchasthehumanbody. Asinoptimalcontrol,continuous-timeactiveinference
considersthreereferenceframesandtwoinversions: anextrinsicsignal(e.g.,encodingtheCartesianpositionofa
target)isfirsttransformedinanintrinsicsignal(e.g., encodingthejointanglesconfigurationcorrespondingtothe
handatthetarget)throughinversekinematics,whichisinturnconvertedtotheactualmotorcontrolsignals(e.g.,joint
torques)throughinversedynamics[84]. Thesetwoprocessesarealsoattributedtothehumanbrain[85,86,87],but
thereisasubstantialdifferencebetweenoptimalcontrolandactiveinferenceregardinghowtheyunfoldinpractice. As
mentionedinthepreviouschapter,inactiveinferencethemotorcommandsarereplacedbyproprioceptiveprediction
errorsthataresuppressedthroughspinalreflexarcs[38]. Asaconsequence,inversedynamicsbecomeseasierbecause
actionisputasideandtheagenthasjusttoknowthemappingfromproprioceptivestatestomotorcommands–see
Equation7.
Butwhataboutinversekinematics? Recalltheperspectivethatwementionedinthepreviouschapter,i.e.,that“itisan
objectIwanttoreachthatgeneratesmymovements”. Turningoptimalcontrolupsidedown,activeinferencepositsthat
actionisdrivenbytheproprioceptiveconsequences(e.g.,changesinmusclelengths)ofextrinsiccauses(e.g.,alimb
position)[37]. Intuitively,onecouldmodelanextrinsicmovementasinFigure7a,i.e.,withthefollowingdynamics
andlikelihoodfunctions:
f(x,v)=JT(v−T(x))
g (x)=x (18)
p
(cid:2) (cid:3)
g (x,v)= T(x) v
v
wherexarethearmjointangles, v isthetargetpositiontoreach, T istheforwardkinematicsreturningthehand
position,andJ isitsJacobianmatrix.
Thevisuallikelihoodfunctiong generatesvisualpredictionsforthehandandthetargetthroughforwardkinematics
v
and identity map, respectively. For goal-directed behavior, first an error between the target and hand positions is
generated;then,aninversekinematicmodelisembeddeddirectlyintothedynamicsfunction,e.g.,aJacobiantranspose
orapseudoinverse[70,71,72,53,32,81,79]. Inotherwords,anextrinsicreferenceframeisinvertedtogeneratean
intrinsicstate,whichisinturntransformedagaininthefirstdomaintobecomparedwithvisualobservations. Asa
result,forwardandinversekinematicsareperformedtwice,onceinthedynamicsfunctionandonceintheforwardand
backwardpassesofvisualinference,i.e.,whenbackpropagatingthevisualpredictionerrorε :
o,v
(cid:20) (cid:21)
∂gTε = J (o − (cid:2) T(x) v (cid:3) ) (19)
v o,v 1 v
Ifthepredictionsarenottemporarilystored,thisrequiresincreasedcomputationaldemandandmemory. Additionally,
thereisanissueregardingbiologicalplausibility: usingsensory-levelattractorswithinthedynamicsfunctionmeans
thataunitisawareofpartofthelikelihoodprediction–generallyassumedtogoalldowntothesensorium–andits
inversemapping,whicharelower-levelfeatures. Finally,withthemodelinFigure7atheagentcannoteasilyexpress
11
(a) (b)
(c)
Figure7: (a)Factorgraphofanactiveinferencemodelcommonlyusedforkinematics. Hiddencausesofasinglelevel
representatargettoreach,whilehiddenstatesdefinethejointanglesofthekinematicchain,generatingproprioceptive
predictions through g and visual predictions through g . Forward and inverse kinematics are duplicated, further
p v
requiringthelikelihoodg tobeembeddedintothedynamicsfunction. (b)Factorgraphofanalternativehierarchical
v
modelforkinematics. Twodifferentunits(leftandrightblocks)encodeinformationaboutarmjointanglesandtarget
position,respectivelygeneratingproprioceptiveandvisualpredictions. Thetwolevelsarelinkedbythelikelihood
functiong performingforwardkinematics. Inversekinematicsandgoal-directedbehaviorarisenaturallythrough
e
inference;moreover,bothlevelscanexpresstheirowndynamics,affordingmoreadvancedcontrol. (c)Generative
modelof(b).
pathsinextrinsiccoordinatesneeded,e.g.,forrealizinglinearorcircularmotions,orforimposingconstraintsinboth
intrinsicandextrinsicdomainssuchaswhenwalkingwithaglassinhand.
WecaninsteadexploitforwardandinversekinematicsofEquation19andfollowthenaturalflowofthegenerative
processtoavoidduplicatedcomputations,asdisplayedinFigure7b. Thealternativegenerativemodelisdisplayedin
Figure7c. Thismodelreliesontwohierarchicallevels,whereanintrinsicunit(encodingthearmjointangles)isplaced
atthetopandgeneratespredictionsthroughforwardkinematicsforanextrinsicunit(encodingtheCartesianpositionof
thetarget)[80]:
x =g (x )+w =T(x )+w (20)
e e i o,e i o,e
whilethevisuallikelihoodbecomesasimpleidentitymap,i.e.,g (x ) = x . Thegoal-directedbehaviorfromthe
v e e
dynamicsfunctionofEquation18arisesnaturallyviabackpropagationoftheextrinsicpredictionerrorε :
o,e
∂gTε =JT(µ −T(µ )) (21)
e o,e x,e x,i
12
(a) (b) (c)
Figure8: (a)Inthistask,theagent(a4-DoFarm)hastoreachtheredtargetwhileavoidingthegreenobstacle;this
ispossiblebyspecifyingtwo(attractiveandrepulsive)functionsattheextrinsiclevel. (b)Inthistask,theagenthas
toreachtheredtargetwhilemaintainingthesamehandorientation(suchaswhenwalkingwithaglassinhand);this
ispossiblebycombiningintrinsicandextrinsicconstraints. (c)Inthistask,theagenthastoperformlinear(top)and
circular(bottom)motions,possiblebydefiningattractorsatthe1sttemporalorderoftheextrinsichiddenstates. See
[80]formoredetails.
Havingacompleteunitthatdealswithextrinsicinformation–whichisthusnotembeddedintothehiddencausesof
theintrinsicunit–allowstheagenttospecifyitsdynamics,leadingtoanefficientdecompositionbetweenintrinsic
andextrinsicattractors–i.e., f (x ,v )andf (x ,v )–andbetweenproprioceptiveandvisualobservations–as
i i i e e e
exemplifiedinthesimulationsofFigure8. NotethesimilarityofEquation21withEquations18and19: ifinthemodel
ofFigure7awehadtwodifferentforwardandinversekinematicseitherforgoal-directedbehaviororforpredicting
currentobservations,inthenewmodelofFigure7bwhatiscomparedwiththeobservationsalreadycontainsabias
towardintentionalstates,withouttheneedforsensory-levelattractorswithinthedynamicsfunction. Theextrinsic
predictionerrorofEquation21becomeszeroonlywhentheextrinsichiddenstatesx matchthepredictionsofthe
e
intrinsicunit;ifsuchpredictionsarenotmet,ε willflowthroughthehierarchyeventuallygeneratinganaction. The
o,e
updaterulesforthe0thordersoftheintrinsicandextrinsichiddenstatesarethefollowing:
µ˙ =µ′ −Π ε +∂ gTΠ ε +∂ gTΠ ε +∂ fTΠ ε (22)
x,i x,i η,x,i η,x,i xi p o,p o,p xi e o,e o,e xii i x,i x,i
µ˙ =µ′ −Π ε +∂ gTΠ ε +∂ fTΠ ε (23)
x,e x,e o,e o,e xe v o,v o,v xe e x,e x,e
Althoughthegenerativemodelfollowstheforwardflowofoptimalcontrol,therelationshipbetweenproprioceptive
consequencesandextrinsiccausespeculiartoactiveinferencestillholdsbecausethekinematicinversionregardsa
high-levelprocessthatmanipulatesabstract(intrinsicorextrinsic)representations,andbothofthemconcurtogenerate
low-levelproprioceptivestates. AsAdamsandcolleaguesnote,“Thekeydistinctionisnotaboutmappingfromdesired
statesinanextrinsic(kinematic)frametoanintrinsic(dynamic)frameofreference,butthemappingfromdesired
states(ineitherframe)tomotorcommands”[38]. Havingsaidthis,thereisasignificantdifferencebetweenthetwo
modelsrepresentedinFigure7,whichcanbecomparedtothetwosupervisedlearningmodesofpredictivecoding[25]:
aforwardmodethatfixesthelatentstatestothelabelsandtheobservationstothedatacangeneratehighlyaccurate
imagesofdigits,whiletheinverseclassificationtaskismoredifficultasthereisnounivocalmappingbetweenlabels
anddata;instead,abackwardmodethatfixesthelatentstatestothedataandtheobservationstothelabelsachieves
highperformancesonclassificationbutfallsshortwhengeneratingimages. Basedonthis,wecaninterpretthemodel
ofFigure7aasabackwardmodethatwouldrapidlygenerateaproperkinematicconfigurationwiththehandatthe
target,butthatwouldhardlyinferfromproprioceptionthehandpositionneededtoplanmovements. Conversely,we
caninterpretthemodelofFigure7basaforwardmodethatwouldgeneratewithhighaccuracythehandposition,but
thatwouldfinditdifficulttoinferthekinematicconfigurationneededtoactuallyrealizemovement.
13
3.2 Amoduleforiterativetransformations
ThemodelinFigure7bintroducedahierarchicaldependencybetweentwo(intrinsicandextrinsic)levels,madepossible
byconnectinghiddenstates. Instead,thetypicalapproachincontinuous-timeactiveinferenceinvolvesconnections
between hidden states and causes of a level and hidden causes (and not hidden states) of the subordinate level, as
showninFigure9a. Whilethisallowsonetoimposeatrajectoryfortheunitbelow,specifyingfixedsetpointstothe
0th-orderhiddenstatesisnotasstraightforward,sincethedynamicspredictionerrorgeneratedfromthehiddencauses
wouldhavetotravelbacktotheprevioustemporalorders. AsclearfromFigure7b, aconnectionbetweenhidden
statesisofhighutilitywhendesigninghierarchicalmodels. Infact–asrepresentedinFigure9b–itisfundamentalin
definingtheinitialstatesofdifferenttemporalscalesindiscretemodels,e.g.,forpictographicreading[88]orlinguistic
communication [88]. Similar connections are used in standard PCNs, wherein each neuron of a level computes a
combinationofneuronsofthelevelabovepassedtoanactivationfunction[25]–asshowninFigure9c.
Recallthat,inthepreviouskinematicmodel,thepredictiong (x )oftheintrinsichiddenstatesactedasapriorforthe
e i
extrinsichiddenstates,whilethelatteractedasanobservationfortheintrinsichiddenstates. Followingthisexample,
weusetheobservationofaleveltobiasthe0th-orderhiddenstatesofthelevelbelowdirectly:
η(i+1) ≡g(i)(µ(i)) (24)
x x
o(i) ≡x(i+1) (25)
Asaresult,theobservationpredictionerrorε andthepriorpredictionerrorε ofEquation4isexpressedbythe
o η,x
samevariable:
ε(i) =ε(i+1) =µ(i+1)−g(i)(µ(i)) (26)
o η,x x x
wherethehierarchicallevelisindicatedwithasuperscriptandlowerlevelsaredenotedbyincreasingnumbers. Wecan
thendesignamultiple-inputandmultiple-outputsystemwhereinalevelimposesandreceivespriorsandobservations
toseveralindependentunits,asinFigure9d. ThecomputationofthefreeenergyinEquation3remainsunchanged,and
theupdateofthehiddenstatesturnsintothefollowing:
 
µ′(i,j)−Π(i−1)ε(i−1)+ (cid:80) ∂ g(i,l)TΠ(i,l)ε(i,l)+∂ f(i,j)TΠ(i,j)ε(i,j)
x o o l x(i,j) o o x(i,j) x x
µ˜˙(i,j) =  (27)
x  
−Π(i,j)ε(i,j)
x x
wherethesuperscriptnotation(i,j)indicatestheithhierarchicallevelandthejthelementwithinthesamelevel. As
evident,thisisasimilarconnectivitytohierarchicaldynamicalmodelsintemporalpredictivecoding[21,89]. Here,the
forwardpredictionerrorε(i−1)combinesthepredictionsfromtheunitsaboveandactsasprior(thegradientisabsent
o
sincetheyareexpressedinthesamedomainoftheunitconsidered),whilethebackwardpredictionerrorsε(i,l)contain
o
theobservationsfromtheunitsbelow.
Whatadvantagesdodeephierarchicalmodelscarrycomparedtoashallowagent? AlthoughthestructureofFigure7b
affordsamoreadvancedcontrolwithrespecttothemodelinFigure7a,itsusesarestilllimitedtosolvingsimpletasks,
e.g.,performingoperationswiththehand. Whilesimultaneouscoordinationofmultiplelimbsispossible,itwould
requirecomplexdynamicsfunctions,withcomplexityincreasingwiththenumberofjointsandramificationsofthe
kinematicchain. Critically,ashallowagentwouldnotbecapableofcapturingthehierarchicalcausalrelationships
inherent to the generative process, allowing one to predict and anticipate the local exchange of forces that would
unfoldduringmovement. AsmentionedintheIntroduction,adeepmodelisalsorequiredifonehastousetoolsfor
manipulation tasks. Besides roto-translations in forward kinematics, iterative transformations are also essential in
computervision–whereanimagecanbesubjecttoscaling,shearing,orprojection–and,moreingeneral,whenever
changingthebasisofacoordinatevector.
Forthesereasons,wecangeneralizethelastmodelandconstructanIntrinsic-Extrinsic(orIE)module[80,83,90].
Thismoduleiscomposedoftwounitsanditsroleistoperformiterativetransformationsbetweenreferenceframes. In
brief,aunitU(i−1)encodesasignalinanextrinsicreferenceframe,whileasecondunitU(i)containsagenericintrinsic
e i
transformation. ApplyingthelattertothefirstsignalreturnsanewextrinsicreferenceframeembeddedinaunitU(i).
e
14
(a) (b) (c) (d)
(e)
(f)
Figure9: (a)Factorgraphofthetypicalconnectionbetweentwocontinuouslevels. Hiddenstatesx(i) andhidden
causesv(i) ofleveligenerate–throughthelikelihoodfunctiong(i) –thehiddencausesv(i+1) ofleveli+1. (b)
Connections between two discrete levels. Hidden states s(i) of level i generate – through the prior matrix D(i) –
1
thehiddenstatess(i+1) ofleveli+1. Wewillcoverdiscretemodelsinthenextchapter. (c)Connectionsbetween
1
twolevelsinPCNs. Thelikelihoodfunctiong(i) performsasimplecombinationofneurons,passedtoanonlinear
activationfunction. StandardPCNsonlypermitrepresentingthecausalstructureofasystem,withoutmodelinginternal
dynamics. (d)Factorgraphofalevelwithmultipleinputsandoutputsfromindependentunits,withsimilarconnectivity
totemporalpredictivecoding. Theobservationo(i,j)becomesthe0th-orderhiddenstatex(i+1,j)ofthelevelbelow,
whilethepriorη(i,j)becomesthe0th-orderhiddenstatex(i−1,j)ofthelevelabove. (e)AnetworkofIEmodules. An
x
extrinsicx(i−1),alongwithanintrinsicsignalx(i,j)(e.g.,angleforrotationorlengthfortranslation),ispassedtoa
e i
functiong(i,j),generatinganewextrinsicsignalx(i,j). (f)Generativemodelof(e).
e e
15
(a) (b) (c)
Figure10: (a)Inthistask,theagent(a23-DoFhumanbody)hastoavoidamovingobstacle;thisispossiblebydefining
arepulsiveattractorforeachextrinsiclevel. (b)Inthistask,theagent(a28-DoFkinematictree)hastoreachfour
targetlocationswiththeextremitiesofitsbranches. See[80]formoredetails. (c)Inthistask,theagent(thetwoblue
eyesontheleftofthetopview)hastoinferthedepthoftheredobjectwhilefixatingonit. Theinferredposition(and
itstrajectory)isrepresentedinorange,whilethebluetrajectoryisthecenteroffixationoftheeyes. Thebottomtwo
framesshowtheobjectprojectionintheeyeplanes. See[83]formoredetails.
Then,wecandefinealikelihoodfunctiong suchthat:
e
x(i) =g(i)(x(i),x(i−1))+w(i) =T(i)(x(i))Tx(i−1)+w(i) (28)
e e i e o,e i e o,e
where w(i) is a noise term, while T(i) here indicates a linear transformation matrix, which can express forward
o,e
kinematicsasinthepreviousexamplesornon-affinetransformations. Backpropagatingtheextrinsicpredictionerror
ε(i) =µ(i) −g (µ(i),µ(i−1))leadstosimplebeliefupdates:
o,e x,e e x,i x,e
∂ g(i)Tε(i) =T(i)Tε(i)
xe (i−1) e o,e o,e
(29)
∂ g(i)Tε(i) =∂ T(i)⊙[ε(i)µ(i−1)T]
x(i) e o,e x(i) o,e x,e
i i
where⊙istheelement-wiseproduct. Theseequationsexpressthemostlikelyintrinsicandextrinsicstatesthatmay
havegeneratedthenewreferenceframe. AsshowninFigure9e,modulesarelinkedthroughtheextrinsicunitsU(i),
e
whileU(i)performsaninternaloperationanddoesnotcontributetothehierarchicalconnectivity. Formotorcontrol,
i
wecanrealizeahierarchicalmultiple-outputsystem,whereintheintrinsichiddenstatesx(i,j)ofalevelencodeapair
i
ofjointangleandlimblengthofasingleDoF.Iterativelyapplyingroto-translationstoanorigin(e.g.,body-centered)
referenceframex(0) –consistingofaCartesianpositionandanabsoluteorientation–willdeterminethekinematic
e
configurationoftheagentintermsofextrinsiccoordinates[80]. Thegenerativemodelfortheintrinsicandextrinsic
unitsofasingleIEmoduleatleveliaredisplayedinFigure9f. Comparedtothesingle-levelgenerativemodelofFigure
7ccomprisingeveryjointangleoftheagent’sbody,herewehaveadependencybetweenlevelsencodingdistinctjoint
angles. Inparticular,anadditionaltermthatlinkstheextrinsicunitwiththepreviouslevel,i.e.,p(x(i)|x(i),x(i−1)),
e i e
andmultipledistributionsp(x(i+1,l)|x(i+1,l),x(i))generatingextrinsicpredictionsfordifferentmodulesatthenext
e i e
level.
16
Atthispoint,wecaneasilyexpresshoweverysinglejointandlimbwouldevolve,affordingahighlyadvancedcontrol
asdemonstratedbythesimulationsofFigures10aand10b. Besidesmodelinglimbdynamics,theIEmodulescanalso
beappliedtootherlineartransformations,e.g.,perspectiveprojections. AsdisplayedinFigure10c,thiscanbeuseful
forestimatingthedepthofanobjectviaparallelpredictions(e.g.,fromtheeyesormultiplecameras)[83]–aprocess
thatactiveinferencecastsintermsoftargetfixationandhypothesistesting[78]. Themodularityofthisarchitecture
allowstheagenttodefinedynamicattractorsinthe2Dprojectedplanes,inthe3Dreferenceframesoftheeyes,oras
simplevergence-accommodationangles. ThisapproachalsohassomeanalogieswithActivePredictiveCoding[91]
andRecursiveNeuralPrograms[92],whichaddressedthepart-wholehierarchylearningproblemincomputervisionby
recursivelyapplyingreferenceframetransformationstopartsofascene.
3.3 Theself,theobjects,andtheothers
DescribingFigure7b,wepassedoveracriticalmechanismintroducedatthebeginning: thecharacterizationofobject
affordances. Recallthatthehiddenstatesencodedinparallelnotonlytheselfbutalsootherenvironmentalentities;
however,theagent’smodelcannowexpressthegenerativeprocesshierarchically. Thisisdescribedbythefollowing
likelihoodfunction:
(cid:104) (cid:105)
g(i)(x(i),x(i−1))= T(i)(x(i))Tx(i−1) T(i)(x(i))Tx(i−1) ... T(i)(x(i) )Tx(i−1) (30)
e i e i,0 e,0 i,1 e,1 i,N e,N
inwhicheveryIEmoduleofthehierarchyhasdistinctfactorsfortheselfandeveryentity. Fortheself, thishasa
simpleexplanation,i.e.,itjustgenerates,oneaftertheother,thepositionsofeverysegmentofthekinematicchain
dependingonitsjointangles. Concerninganobject,wecouldencodeitsCartesianpositionbyattachingitsvisual
observationtoasecondfactorofthehiddenstatesataspecificlevel. Ifthegenerativemodelhasthesamehierarchyfor
boththeselfandtheobject,backpropagatingtheextrinsicpredictionerrorsofthissecondcomponentwilleventually
inferapotentialagent’sconfigurationinrelationtotheobject,asbefore. Forinstance,iftheobjectislinkedtothelast
(i.e.,hand)level,thiswouldrepresentthehandattheobjectlocation,whileallthepreviouslevelswouldrepresent
appropriateintermediatepositionsandanglesgeneratingthatfinallocation. Inotherwords,theadditionalfactorizations
ofhiddenstatesandlikelihoodsreflectherea(deep)hierarchicalconfigurationoftheselfthattheagentthinkstobe
suitableforobjectmanipulation. Sinceeachlevelcanexpresssomedynamicsthroughitshiddencauses,theinference
of this potential configuration is steered to match the object’s affordances and the agent’s intentions. As will be
showninthenextchapter,thispermitsflexibleadaptationofthekinematicchaindependingonthecircumstances,as
wellasrepresentingthehierarchicalstructuresofobjects(e.g.,tools). Theinferredbeliefswouldbesubjectonlyto
exteroceptiveinformationfromtheobjects,whileproprioceptivestateswouldbeusedonlytoupdatetheagent’sbelief
ofitscurrentconfiguration.
Besides modeling object affordances, this strategy is also useful in multi-agent contexts. One could maintain a
hierarchicalgenerativemodelregardingthekinematicchainofanotheragent,whichwouldbeinferredbyexteroceptive
observationsaboutallitspositionsandjointangles,startingfromadifferentbody-centeredreferenceframe. Asshown
inFigure11,thegoal-directedmethodusedforexternalobjectsreflectsinthiscaseaswell: theagentcouldrepresent,
byaparallelhierarchicalpathway,asecondagentinrelationtoitself,expressingaparticularkindofinteraction(e.g.,
thehandofthesecondagentintermsofitsown,resultinginashakingaction). Thesetwocasescouldbeinterpreted,
fromabiologicalperspective,assimulatingthefunctioningofmirrorneurons,firingwheneverasubjectexecutesa
voluntarygoal-directedactionorwhenthatactionisperformedbyothersubjects[93]. Buildinganinternalmodel
withthekinematicchainoftheothers–bothperseandinrelationtotheself–couldbecriticaltopredict(thus,to
understand)theirintentions. Inthisview,neuralactivityresultsbecausetheagentmakesconstantpredictionsovertheir
kinematicstructuresdependingonitshypothesesandthecurrentcontext[94,81].
Therelationshipsbetweentheself,theobjects,andotheragentsunderactiveinferencemaybebetterunderstoodfrom
thesimulationofFigure12,showingtwoagentswithincompatiblegoalsthatdependoneachother. Here,bothagents
areabletoinferparallelrepresentationsofdifferentkinematicchains,usinganeffectivedecompositionofpotentialand
realconfigurations. Notehowone’scurrentbeliefisalwaysinbetweentheintentionalstatetoberealizedandtheactual
configuration;thisspeaksofoneofthefundamentalaspectsofactiveinference,i.e.,thatourbeliefsneverreallyreflect
thestateoftheaffairsoftheworld,butarealwaysbiasedtowardpreferredstates–eventuallydrivingaction. Ingeneral,
bodilystates,objects,orotheragentscanallbemanipulatedinreferenceframesappropriateforaspecificcontext;this
17
Figure11: Interactionsbetweenanagent(a2-DoFlight-bluearm),anotheragent(a3-DoFpurplearm),andanobject
(a red ball). Generative model of the first agent, composed of three parallel pathways representing the kinematic
structuresofbothagents. Forclarity,lateralconnectionsamongthemodelcomponentsofeachlevelarenotshown.
Boththeselfandtheother(orobject)inrelationtotheselfdependonthesamebody-centeredreferenceframe. Thefirst
componentissubjecttobothproprioception(yellowdottedlines)andexteroception(reddottedlines),whiletheother
twocomponentsareinferredviaexteroceptiononly. Inthiscase,theinteractionwiththesecondagentjustdependson
theobservationofitslastlevel,leading,e.g.,toahand-shakingaction.
isinlinewiththehypothesisthatcorticalcolumnsuseobject-centeredreferenceframestoencodeexternalelements
andmoreabstractentities[95].
4 Thehybridunit
Thecontinuous-timehierarchicalmodelspresentedsofarlackeffectiveusabilityintherealworld: althoughtheycan
representanyfuturetrajectory-whichimpliesadegreeofplanning-theydonotpossessanexplicitmodeloffuture
statesnorcantheychooseamongalternativetrajectories. Inthischapter,weturntotheproblemofhowtointegrate
discretedecision-makingintocontinuousmotorcontrol. Indoingthis,werevisitthebasicunitofthefirstchapter,
finallyusingthesecondinput–theprioroverthehiddencauses.
Active inference in discrete state-space [96, 97] – generally attributed to the cerebral cortex, especially prefrontal
areas[98],alongwithcorticostriatalloops–exploitsthestructureofPartiallyObservableMarkovDecisionProcesses
(POMDPs)toplanabstractactionsoverexpectedsensations. This(active)inferencereliesontheminimizationofthe
expectedfreeenergy,i.e.,thefreeenergythattheagentexpectstoperceiveinthefuture. Theexpectedfreeenergycan
beunpackedintotwotermsresemblingthetwoclassicalaspectsofcontroltheory,explorationandexploitation–which
herenaturallyarise;theserespectivelycorrespondtoanuncertainty-reducingterm,andagoal-seekingtermthatfindsa
sequenceofactionstowardtheagent’spriorbelief.
Further,so-calledmixedorhybridmodels[49,67]combinethepotentialitiesofadiscretemodelwiththeinferenceof
continuoussignals,allowingrobustdecision-makinginchangingenvironments. WhilethetheoryofBayesianmodel
reduction[99,100,101,102]providesefficientcommunicationbetweenthetwomodels,thisunifiedapproachhas
notenjoyedmanypracticalimplementationsforthetimebeing[35,49,67,103,104,105,78]. Anopenissueregards
howtodealwithhighlydynamicenvironments: hybridmodelsusuallyperformacomparisonbetweenstaticpriors,
18
(a) (b)
Figure 12: (a) In this task, two agents with different kinematic chains (respectively of 5 and 3 DoF) have two
incompatible goals: the first agent (in red) has to reach the elbow of the second agent (in blue), while the second
agenthastoreachthehandofthefirstagent. Notethatafteraninitialapproachingphasefromboth,thesecondagent
graduallyretractstryingtotouchthehandofthefirstagent. (b)Beliefsofbothagents,comparedwiththeiractual
configurations(displayedinlightredforthefirstagent,andinlightblueforthesecond). Thetopandbottomgraphs
respectivelyshowthebeliefsofthefirstandsecondagents. Specifically,thebeliefoftheactualconfigurationoftheself
(inorangeorcyan),thebeliefoftheotherinrelationtotheself(indarkredordarkblue),andthebeliefoftheother(in
greenorpurple). Notethebeliefsuccessionduringgoal-directedbehavior: thepotentialconfigurationpointingtoward
one’sintentionalstate(eithertheelboworthehand),followedbythecurrentbelief,andthenbytherealarm. Alsonote
aslightdelayabouttheinferenceoftheconfigurationoftheotheragent.
limitingtheagenttorealize,e.g.,multi-stepreachingmovementsthroughfixedpositions. In[65],ahybridmodelin
whichtheagent’shypothesesweregeneratedateachtimestepfromthesystemdynamicsallowedtorelatecontinuous
trajectorieswithdiscreteplans. Besidesthesemoreconventionalsolutions, manyotherhybridmethodshavebeen
proposedrecently. Onestudyaddressedtheproblemofrealisticrobotnavigationinactiveinference,makinguseof
bio-inspiredSLAMmethods[106]. Otherstudiesproposedhowtointegrateactiveinferencewithimitationlearningfor
autonomousvehicles,usingaDynamicBayesianNetwork(DBN)toexplaintheinteractionsbetweenanexpertagent
anddynamicobjects[107,108]. AnenhancedDBNconsistingoftwocontinuouslevelsandtwodiscretelevelswas
usedtomodelthebehaviorofaUAVatdifferenttimescalesforassistingwirelesscommunications[109,110,111,112].
Astudyinroboticscombinedactiveinferencewithbehaviortreesforreactiveactionplanningindynamicenvironments
[113]. Finally,ahybridmodelbasedonrecurrentswitchinglineardynamicalsystemsallowedtodiscovernon-grid
discretizationsofacontinuousMountainCartask. [114].
4.1 Dynamicinferencebymodelreduction
RecallthatintheunitofFigure5a,somesortofmulti-stepbehaviorwasachieved,basedonhigher-levelpriorsover
tactilesensations. Inmostcases, weneedtoswitchintentionsbasedonlower-levelinformation, affordingamore
dynamicbehavior. Consideringapick-and-placeoperation,anIEmodulewouldbemoreconfidentaboutthesuccessof
thefirstreachingmovementifitcouldrelynotjustonatactilebeliefbutalsoonitskinematicconfiguration. Inother
words,hiddencausesvshouldmanagetoeffectivelyusebothitspriorη andthedynamicspredictionerrorε . The
v x
latterassumestwodifferentrolesdependingonwhichpathwayitflowsinto: thegradientwithrespecttothehidden
statesinfersthepositionthatismostlikelytohavegeneratedthecurrenttrajectory,neededformovement;conversely,
thegradientwithrespecttothehiddencausesinfersthemostlikelycombinationofgainsv ,signalingthecurrent
m
statusofthetrajectoryandresultinginadynamicmodulationofintentions. However, sincethehiddencausesare
generatedbyGaussiandistributions,theydonotencodeproperprobabilities,andthegradient∂ f infersjustoneover
v x
manypossiblecombinationsofgains. Instead,whatweneedistoinferthemostlikelyintentiontohavegeneratedthe
19
currenttrajectory. Thus,toimplementacorrectintentionselection,weassumethatthehiddencausesaregenerated
fromacategoricaldistribution:
p(v)=Cat(H ) (31)
v
whereH isapriorpreferencelikeη . Inthisway,eachdiscreteelementofvrepresentstheprobabilitythataspecific
v v
continuoustrajectorywillberealized.
Conversionbetweendiscretehiddencausestocontinuoushiddenstates(andviceversa)isdoneviaBayesianmodel
reduction, a technique used to constrain the complexity of full posterior models into simpler and more restrictive
(formallycalledreduced)distributions[100,101]. Reducedmeansthatthelikelihoodofsomedataisequaltothatof
thefullmodelandtheonlydifferencerestsuponthespecificationofthepriors;hence,theposteriorofareducedmodel
mcanbeexpressedintermsoftheposteriorofthefullmodel:
p(x˜|m)p(o)
p(x˜|o,m)=p(x˜|o) (32)
p(x˜)p(o|m)
Inourcase,modelreductionmeanstoexplaintheinfinitevaluesofacontinuoussignalwithadiscretesetofhypotheses.
AsimplifiedversionofahybridactiveinferencemodelisshowninFigure13a. Wecancastthisprocedureintothe
usualmessagepassing,wheretop-downandbottom-upmessagesbetweenthetwodomainsperformaBayesianModel
Average(BMA)ofreducedpriorsandaBayesianModelComparison(BMC)ofreducedsensoryevidence,respectively.
Inconventionalhybridmodels,discretehiddenstatesgeneratepriorsforthecontinuoushiddencausesbyweighting
a specific reduced prior with the probability of each discrete state; hence, the reduced priors represent alternative
hypothesesoverthetruecausesofthesensorium[49]. Theposterioroverthehiddencausesisthencomparedwithsuch
reducedpriorstofindwhichoneamongthemcouldbethebestexplanationoftheenvironment,takingintoaccount
theirdiscreteprobabilitiesbeforeobservingsensoryevidence.
Becausetheagentcomparescontinuousalternativesthatarefixedanddetermineda-priori,itcannotcorrectlyoperate
inachangingenvironment. Forinstance,iftheagentthinkstofindanobjectinoneoftwolocations,itwillalways
reacheitheroneortheotherinitialguesses,eveniftheobjecthasbeenmovedtoathirdlocation. Howthentousethe
newlyavailableevidencetoupdateourreducedassumptions? Byconsideringthehiddencausesasgeneratedfroma
categoricaldistribution–asinEquation31–wecancomparetheposterioroverthehiddenstateswiththeoutputofthe
dynamicsfunctionsf ,whichthusactastheagent’sreducedpriors[65]. Moreformally,wedefineM reducedprior
m
probabilitydistributionsandafullpriormodel:
p(x′|x,m)=N(f (x),Π )
m x,m
(33)
p(x′|x,v)=N(η′,Π )
x x
whereη′ isthefullprior. NoteherethatthereducedpriorshavethesameformofEquation15butarenotdirectly
x
conditionedonthehiddencauses:
f (x)=e =i (x)−x (34)
m i,m m
Next,wedefinethecorrespondingposteriormodels:
q(x′|m)=N(µ′ ,P )
x,m x,m
(35)
q(x′)=N(µ′,P )
x x
Now,wecanfindthefullprioranditspredictionerrorbyaveragingthecontinuoustrajectorieswiththeirrespective
discreteprobabilities:
M M
(cid:88) (cid:88)
η′ = v f (x)= v e
x m m m i,m (36)
m m
ε =µ′ −η′
x x x
whichhavethesameformofEquations15and17. Infact,thehiddenstatesstillperceiveasingledynamicsprediction
errorcontainingthetotalcontributionofeveryintention. Concerningthebottom-upmessagesl,wefirstwritethefree
energyofeachreducedmodelintermsofthefullmodel. Asbefore,maximizingeachreducedfreeenergymakesit
20
(c)
(a)
(b) (d)
Figure13: (a)Aconventionalhybridmodel,composedofadiscretemodelatthetopandacontinuousmodelatthe
bottom. Forsimplicity,weassumethatthecontinuouspriorη isdirectlyconditionedondiscretehiddenstatess. We
v
willcoverdiscretemodelsinthefollowingsection. Here,top-downandbottom-upmessagesarecomputedbyBayesian
modelreductionofsomestaticpriors,withoutthepossibilityfordynamicplanning. (b)Asimplifiedgraphdisplaying
theexchangeoftop-down(red)andbottom-up(blueandgreen)messagesofthehybridcontrolin(c). (c)Factorgraph
oftheunitwithhybridcontrol. Thehiddencausesarenowgeneratedfromacategoricaldistribution,suchthatinstead
ofinferringacombinationofcontinuousattractorgains,themodelcorrectlyinfersthemostlikelyintentionassociated
withthecurrenttrajectory. ThisisdonebycomputingthefreeenergyE correspondingtoeachintention. Through
m
Bayesianmodelreductionbetweendiscretehiddencausesandcontinuoushiddenstates,theagentupdatesitsreduced
priorsateachtimestep. Generativemodelof(c).
approximatethelogevidence:
(cid:90) p(x˜|m)
F(m)=F −ln q(x˜)dx˜ ≈lnp(o|m) (37)
p(x˜)
Asaresult,thefreeenergyrelatedtoeachdynamicsfunctionf dependsontheapproximateposteriorq(x˜)ofthefull
m
model,avoidingthecomputationofthereducedposteriors. UnderaGaussianapproximation,themthreducedfree
energybreaksdowntoasimpleformulaandthebottom-upmessageslarefoundbyaccumulatingthelogevidence
21
(a) (b)
Figure14: (a)Inthisperceptiontask,theagenthastoinferwhichoneamongtwoobjects(aredballoragreysquare)
isfollowing. Thearmmovesalongacirculartrajectory,whilethetwoobjectsmovelinearlyinoppositedirections.
(a)Sequenceoftimeframes(left). Thedarkbluearrowrepresentstherealagent’strajectory,whiletheredandgreen
arrowsrepresentpotentialtrajectoriesassociatedwithdifferentreachingmovements. (b)Dynamicsofhiddencauses.
Thetwohiddencausesv andv areassociatedwiththetwopotentialtrajectories. Asthehandmovesawayfromthe
t1 t2
firsttargetandapproachesthesecondtarget,thedynamicaccumulationofevidenceleadstoanincreaseinthesecond
hiddencause. Here,weusedauniformpriorH ,sothehiddencausesonlyexpresstheaccumulatedlogevidencel.
v
Also,thecontinuoustimeT forevidenceaccumulationwassetto30timesteps. See[65]formoredetails.
associatedwitheveryintentionforacertainamountofcontinuoustimeT:
(cid:90) T
l = L dt
m m
0 (38)
1
L = (µ′T P µ′ −f (x)TΠ f (x)−µ′TP µ′ +η′TΠ η′)
m 2 x,m x,m x,m m x,m m x x x x x x
Then,aBMCturnsintocomputingthesoftmaxofavectorcomprisingthefreeenergyE ofeveryreducedmodel.
m
Thisquantitycomparesthepriorsurprise−lnH withtheaccumulatedlogevidence:
v
v =σ(−E)=σ(lnH +l) (39)
v
wherel=[l ,...,l ]. See[100,101]forafullderivationofBMCundertheLaplaceassumption,and[65]formore
1 M
detailsaboutthepresentedapproach. Equation39isthediscreteanalogousofEquation9, butnowthebottom-up
messageencodesaproperdiscretedistributionandcanbeusedtoinferthemostlikelyintentionassociatedwiththe
currenttrajectory.
Thefactorgraphofthismodel,whichwecallahybridunit,isdisplayedinFigure13c. Itsinferenceprocessateach
continuousstepisbetterunderstoodifweanalyzeseparatelythethreedifferentpathwaysshowninFigure13b: (i)
duringtheforwardpass,theunitreceivesadiscreteintentionpriorH ,performsaBMAwithpotentialtrajectories
v
f (x),andimposesapriorη′ overthe1storder;(ii)throughthefirstbackwardpass,theunitaccumulatesthemost
m x
likelyintentionrelatedtothecurrenttrajectorybycomparingittotheonesgeneratedbythedynamicsfunctions;(iii)in
thesecondbackwardpass,theunitpropagatesthedynamicspredictionerrorbacktothe0thordertoinferthemost
likely continuousstateassociated withthetrajectory, eventually generatingbiased observations. Aftera periodT,
theunitfinallycomputesthedifferencebetweenthediscretepriorandtheaccumulatedevidence, generatesanew
combinationofintentions,andtheprocessstartsover.
Nowhowthismechanismisself-reinforcing: adecisiongeneratessomemovement,whichinturninfersthedecision
itselftocomputethenextmovement. Inferringatargetfrombodymovementsisafundamentalaspectoftenneglected
indecision-makingstudies,whichcanresultincompletelydifferentbehaviors. Inparticular,thiscomponentproducesa
22
commitmenteffecttothedecisiontakenandstabilizestheactions,avoidingchangesofmindthatmayleadtotheloss
ofvalidopportunitiesindynamicenvironments[115,116]. Further,thiskindofdynamicinferencehasseveralutilities,
e.g.,itcanbeusedtoinferwhichoneamongmultipleobjectsanagentisfollowing–asexemplifiedinFigure14–by
generatingtrajectoriesfordifferentobjectsandcomparingthemwiththeoneperceived[65].
Analternativethatproducessimilarresultsisthemodelusedin[117]inthecontextofsocialexchange. Inthishybrid
solution, a "student" bird maintains several continuous generative models for every teacher conspecifics. Discrete
switching variables placed at the center of these hypotheses infer which teacher bird has generated the perceived
birdsong. Learningofthegenerativemodelsreliesontwocomplementarymethods,i.e.,Bayesianmodelaverageofall
possibleteacherbirds,orBayesianmodelselectionofaspecificbirdgeneratingasong.
Asalastnote,thedynamicsprecisionsofEquations33and38havehereaninterestinginterpretationequivalenttothe
observationprecisionsΠ . Predictivecodingassumesthatwheneveranagentperceiveshighnoiseaboutasensory
o
modality,theprecisionofthatgenerativemodelwilldecreasebecauseitcannotbetrustedforunderstandingthestate
ofaffairsoftheworld[19,20]. Inaddition,thedualismbetweenactionandperceptioninherenttothefreeenergy
principletellsusthattheoptimizationofprecisions–whicharethoughttobeencodedassynapticgains–couldplaya
crucialroleinattentionmechanismsthatselectivelysamplesensorydata[118,119]. Basedontheseassumptions,we
noteadualinterpretationof(reduced)dynamicsprecisions. AlowprecisionΠ relatedtoagraspingactioncould
x,m
meanthatitisunreliabletoexplainthecurrentcontext(e.g.,anobjectfarawayfromthehand). Inaddition,itcould
meanthattheagentdoesnotintendtorelyonittoachieveagoal(e.g.,graspinganobjectwhenitisoutofreach). This
perspectiveunveilsanadditionalmechanismbesidesthefastinferenceofhiddencausesthatwementionedbefore: a
slowlearningofreducedprecisionsthatletstheagentscore–and,crucially,focuson–thoseintentionsthatwouldbe
appropriateforaspecificscenario[65].
4.2 Adiscreteinterfacefordynamicplanning
Numerousstudieshavedemonstratedthatthebrainsofathletesaremarkedbyahigheractivationofposteriorand
subcorticalregionsthatinvolveslittleornoconsciousthinking,producingfluidtransitionsbetweendifferentmotions;
incontrast,thebrainofanovicerequiresahigherdemandofprefrontalcomputationsthatresultsinlowerperformances
[120,121,122]. Fromanactiveinferenceperspective,wecancomparetheproficiencyofathleteswiththecontinuous
modelofFigure5a,correspondingtothesubcorticalsensorimotorloops. Thismodelencodesatransitionmechanism
thatisnotveryflexible,butthatpreciselyforthiscanreactmuchmorerapidlytoenvironmentalstimuli,e.g.,when
graspingobjectsmovingathighspeed[66]. Ingeneral,thisstrategycanbeveryeffectivewhentheenvironmenthas
limiteduncertaintyandthetasktobesolvedcomprisesarigidsequenceofactions,whichtheagenthasalreadycorrectly
learned. However,supposethattheagentisintroducedtoanovelorcomplextaskthatrequirescarefulthinkingabout
theimminentfuture. Inthiscase,itshouldbecapableofreplanningthecorrectsequenceofactionsifsomethinggoes
unexpectedly,andahigh-levelbeliefalwaysproducingana-priori-determinedbehaviorforthehiddencauseswould
failtocompletethetask.
HavingreplacedthecontinuoushiddencausesofFigure5awithdiscretehiddencausesinFigure13c,wecannow
endowtheagentwithplanningcapabilitiesthroughadiscretemodelcomposedofthefollowingdistributions–asshown
inFigure15:
T
(cid:89)
p(s ,v ,π)=p(s )p(π) p(v |s )p(s |s ,π) (40)
1:T 1:T 1 τ τ τ τ−1
τ
where:
p(s )=Cat(D) p(v |s )=Cat(v |As )
1 τ τ τ τ
(41)
p(π)=σ(−G) p(s |s ,π)=Cat(s |B s )
τ τ−1 τ π,τ τ−1
Here,A,B,Darethelikelihoodmatrix,transitionmatrix,andpriorofthehiddenstates,πarethepolicies(which
arenotstate-actionmappingsasinRLbutsequencesofactions),s arethediscretehiddenstatesattimeτ. These
τ
quantities have strict analogies with their continuous counterparts of Figure 1b, i.e., the likelihood function g, the
dynamicsfunctionf,thepriorη ,hiddencausesv,andhiddenstatesx˜,withthedifferencethatthediscretehidden
x
statesdonotencodeinstantaneouspathsexpressedingeneralizedcoordinates,butsequencesoffuturestates. Crucially,
G istheexpectedfreeenergy,definedasthefreeenergythattheagentexpectstoperceiveinthefuture.
23
Figure15: Interfacebetweenadiscretemodel(atthetop)andseveralhybridunits(atthebottom). Forclarity,the
hiddenstatesfactorizationofeachunitisnotdisplayed. Thediscretehiddenstatessattimeτ +1arecomputedfrom
thecurrenthiddenstatess ,bychoosingsomepolicyπ relatedtoaspecifictransitiondistributionencodedinB. The
τ π
bestpolicyatanymomentisthesequenceofactionsthatmostminimizestheexpectedfreeenergyG. Asaresult,the
agentwilltrytosamplethoseobservationsthatconformtoitspreferenceC. Thediscretehiddencausesv(i)attimeτ
aredirectlygeneratedfromdiscretehiddenstatess throughlikelihoodmatricesA(i),thusaffordingdynamicplanning,
τ
synchronizedbehavior,andinferencewithmultipleevidences.
NoteherethatthelikelihoodmatrixAexpressesaconditionalprobabilityoverthediscretehiddencausesv . Asin
τ
conventionalhybridmodels,thediscretehiddenstatesarelinkedtothehiddencauses,butnowthelatterdirectlyactas
discreteoutcomesgeneratedbythelikelihoodmatrix,whichthusreplacesthepriorH inEquation31. Here,wewish
v
toinfertheposteriordistribution:
p(v |s ,π)p(s ,π)
p(s ,π|v )= 1:T 1:T 1:T (42)
1:T 1:T p(v )
1:T
Asbefore,thisrequirescomputingtheintractablemodelevidencep(v ),soweresorttoavariationalapproach: we
1:T
firstexpresstheapproximateposteriorbyitssufficientstatisticss andconditioninguponaspecificpolicy:
π,τ
T
(cid:89)
p(s |v ,π)≈q(s ,π)=q(π) q(s |π) (43)
1:T 1:T 1:T τ
τ
24
where:
q(π)=Cat(π)
(44)
q(s |π)=Cat(s )
τ π,τ
WethencomputethevariationalfreeenergyF ofthatpolicy:
π
T T T
(cid:88) (cid:88) (cid:88)
F = s lns − v lnAs −s lnD− s lnB s (45)
π π,τ π,τ τ π,τ π,1 π,τ π,τ π,τ−1
τ=1 τ=1 τ=2
DifferentiatingEquation45allowsustoinferthemostlikelydiscretehiddenstatesattimeτ forpolicyπ:
s =σ(lnB s +lnBT s +lnATv ) (46)
π,τ π,τ−1 π,τ−1 π,τ π,τ+1 τ
whereweappliedasoftmaxfunctiontoensurethatitisaproperprobabilitydistribution. Inordertoinferthepolicies,
weadditionallyconsiderunobservedoutcomesasrandomvariables,andthenconditionthejointprobabilityofthe
generativemodeluponsomepreferencesC. Inthisway,theagentcanpredictfutureoutcomesandselectthemost
likelysequenceofactionsthatwillleadittoitsdesiredstate. TheexpectedfreeenergyG underpolicyπconsistsofa
π
pragmaticorgoal-seekingtermtowardtheagent’spreferences,andanepistemicoruncertainty-reducingterm(see[96]
formoredetails):
T
(cid:88)
G ≈ D [q(v |π)||p(v |C)]+ E [H[p(v |s )]]
π KL τ τ τ τ
τ
q(sτ|sτ−1,π)
(47)
T
(cid:88)
= v (lnv −C )+s H
π,τ π,τ τ π,τ A
τ
where:
v =As C =lnp(v |C) H =−diag(AT lnA) (48)
π,τ π,τ τ τ A
Here,v isthepredictionofthehiddencausesattimeτ andunderpolicyπ,C isthelogarithmofthepriorpreference
π,τ τ
overthehiddencauses,andH istheambiguityofthelikelihoodmatrixA. Thepolicyπisthenfoundbyapplyinga
A
softmaxfunctiontotheexpectedfreeenergy,asdefinedinEquation41.
Usingthe(deep)hierarchicalmodeloutlinedintheprevioussection,wemayconnectseveralhybridunitstothediscrete
modelbyappropriatelikelihoodmatricesA(i). Eachofthemhasanindependentinterfacewherebythediscretemodel
computes different signals and waits for the next step τ +1, when it can infer its hidden states based on multiple
accumulatedevidences. RecallthatinthecombinedstructureofFigure15,theroleofthehybridunitwastopredicta
trajectoryfromadiscreteintentionprior,andtoinferthemostlikelyintentioninacontinuousperiodT. Butnowthe
intentionpriorisgeneratedfromthediscretemodel:
v =σ(lnAs +l ) (49)
τ τ τ
wherel isthebottom-upmessageattimeτ foundbyEquation38, andwecomputes bymarginalizingoverall
τ τ
(cid:80)
policies,i.e. s = π s .
τ π π π,τ
Insum,computingtheposteriorprobabilityoverpoliciesπturnsintofindingthebestactionthatconformstothedual
objectivedefinedbyG. Here,thediscreteactionsarenotintendedasactualmotorcommandssimilartoEquation7,but
asabstractactionsoverhigh-levelrepresentations. Infact,thehierarchicalnatureofdiscretemodelsinactiveinference
permitsperformingdecision-makingwithaseparationoftemporalscales,whereinaspecificlevelcangenerateand
inferthestatesandthepathsofthelevelbelow[123,124,125]. Furtherevaluatingtheconsequencesofanactionfora
longertimehorizonaffordsmoreadvancedplanningcalledsophisticatedinference[126]. Computingactionswiththe
expectedfreeenergyisdifferentfromthemotorcontrolofcontinuousmodels,whichonlyminimizethefreeenergyof
presentstates.
Inadditiontothepreviousagents,itisnowpossibletosynchronizethebehaviorofdifferentcontinuoussignalsbased
onthe samehigh-levelpolicy. Forinstance, one canrealizea pick-and-placeoperationwith amoving object–as
representedinFigure16–producingsmoothtransitionsbetweenreachingandgraspingactions,respectivelyperformed
25
Figure16: Inthistask,theagent(a4-DoFarmwithanadditional4-DoFhandcomposedoftwofingers)hastopicka
movingobject,andplaceitatagoalposition. TheagenthasashallowstructurewithasingleIEmodulecomputingthe
handpositionfromthearmjointangles. Besidesthespatialdimension,theagentcaneasilyintegratemultisensory
informationsuchastouch,dimensionsofobjects,andothermoreabstractproperties. Notethattheobjectbeliefis
rapidlyinferred,andassoonasthepickingactioniscomplete,thebeliefisgraduallypulledtowardthegoalposition,
resultinginasecondreachingmovement. Thetoprightpanelshowsthehand-objectdistanceovertime,whilethe
bottomrightpaneldisplaysthedynamicsofthediscreteactionprobabilitiesusedtoinferthenextdiscretestate. See
[66]formoredetails.
in extrinsic and intrinsic domains [66]. Note how an intermediate phase between the two actions naturally arises,
correspondingtoacompositeapproachingmovement. Inprinciple,thelearningofdynamicsprecisionsΠ might
x,m
shedlightonhowmotorskilllearningoccurs,viamessagepassingbetweencontinuoustrajectoriesanddiscretepolicies.
Moreover,throughthiskindofdynamicplanningtheagentcaninferandrealizeinstantaneoustrajectoriesevenfor
thesamecontinuousperiodT withinadiscretestepτ,useful,e.g.,forgraspingthemovingobjectwithoutwaitingfor
thesuccessivereplanningstep. Third,thisconfigurationpermitslearningadiscreterepresentationoftheenvironment
basedoncontinuousevidence: asshownin[127],learningthelikelihoodmatrixAinvolvescountingthecoincidences
betweentargetsandbodymovements,thusadaptingaresponsestrategy(riskyversusconservative)dependingonthe
difficultyofthecontext. Additionally,throughlearningofthepriorD,ahabitualbehaviortowardthechosendecision
emerges. Similarly, one could update preferred states encoded in the matrix C based on current observations and
actions.
Sincetheagentplanswithtrajectoriesandnotpositions,tocorrectlymaintainagoalstateweneedtointroduceahidden
causelooselycorrespondingtothestayactioncommonlyusedindiscretetasks[97]. Thishiddencausecanbelinkedto
anidentitymap,i.e.,i (x)=x,whichcanbeinterpretedastheagent’sdesiretomaintainthecurrentstateofthe
stay
world[65]. Thishiddencausealsorelatestotheinitialstateofthetask,andtranslatesintoaphaseofpureperceptual
inference.
4.3 Flexiblehierarchies
Figure17portraysadeephybridmodeldesignedforsolvingatool-usetask[90]. Itcombinestheexpressivityofa
(deep)hierarchicalformulation,theadvantagesofplanningtrajectoriesinherenttoahybridunit,andthepossibilityof
encodingobjectaffordancesandotheragents.AsinFigure15,theIEmodulescommunicatewithadiscretemodelatthe
top,butnowtheyarecombinedinahierarchicalfashionrecapitulatingtheagent’skinematicchain. Asaconsequence,
twodifferentgoal-directedstrategiesarise. Consideringasimplereachingmovement,anattractorimposedatthehand
levelwouldgenerateacascadeofextrinsicpredictionerrorsflowingbacktothepreviouslevelsandfindingasuitable
kinematicconfigurationwiththehandoverthetarget. Thiscorrespondstoahorizontalhierarchicaldepthoccurring
alongthehybridunits,andcanbecomparedtotheprocessofmotorbabblingtypicalofinfants[128],wherebyrandom
attractorsaregeneratedatdifferenthierarchicallevelstoidentifythecorrectbodystructure. Inadditiontothisnaive
strategy, since a discrete model can now generate trajectories for every IE module (in both intrinsic and extrinsic
domains),amoreadvancedbehaviorcanbeachievedonceinversekinematicsiscorrectlyperformed,whichimposesa
26
specificpathtothewholekinematicchain. Thiscorrespondstoaverticalhierarchicaldepthwithtwo(discreteand
continuous)temporalscales,steeringthelower-levelinferenceinadirectionthat,e.g.,avoidssingularitiesorgetsout
fromlocalminimageneratedbyrepulsiveattractors.
Figure17: Graphicalrepresentationofadeephybridmodelfortooluse,composedofadiscretemodelatthetopand
severalIEmodules. Everymoduleisfactorizedintothreeelements,whicharerespectivelylinkedtotheobservationsof
theagent’sarm(inblue),atool(ingreen),andaball(inred). Notethatthelastlevelonlyencodesthetool’sextremity
andtheball.
Consideringtheupdateruleforthe(extrinsic)hybridunitatleveli:
µ˙(i) ∝−Π(i−1)ε(i−1)+∂gTΠ(i)ε(i) +∂η′(i)TΠ(i)ε(i) (50)
x,e o,e o,e e o,e o,e x,e x,e x,e
where ∂η′(i)T is the gradient of the trajectory prior of Equation 36, we note a delicate balance between forward
x,e
andbackwardextrinsiclikelihood,andthetop-downmodulationofthediscretemodel. Fromthediscretemodel’s
perspective, the discrete hidden states produce a specific combination of hidden causes for each hybrid unit; this
combinationgeneratesacompositetrajectoryinthecontinuousdomainweightingdistinctpotentialtrajectories,taking
intoaccountdynamicelementsforthewholediscretestepτ. Afterthisperiod,evidenceisaccumulatedforeveryhybrid
unit,eventuallyinferringthemostlikelydiscretestatethatmayhavegeneratedtheactualtrajectory,relatedtotheself
andtheenvironment.
Anon-trivialissueexistsintasksrequiringtooluse,e.g.,reachingaballwiththeextremityofastick. Muchasother
agentsmayhavedifferentkinematicstructuresthantheself,atoolmayhaveitsownhierarchy(e.g.,evenasimplestick
isrepresentedbytwoCartesianpositionsandanangle)thatmustsomehowbeintegratedintotheagent’sgenerative
model. Specifically, reachinganobjectwiththeextremityofatoolmeansdefiningapotentialbodyconfiguration
augmentedbyanewvirtuallevel. Thisnewleveldoesnotexistinthegenerativeprocessandthebeliefoftheactual
bodyconfigurationx . Nonetheless,ifweconsiderthetwopotentialconfigurations,theagentthinksofthetoolasan
0
extensionofitsarm,henceflexiblymodifyingitsbodyschemaasdiscussedintheIntroduction. Thisispossibleby
linkingthetwovisualobservationsofthetooltothehandandvirtuallevelsinasecondpathwayofthehiddenstates,as
showninFigure17. SincetheintrinsicunitsoftheIEmodulesalsoencodeinformationaboutlimblengths,theagent
cannotonlyinferitskinematicstructurethroughvisualobservations,butalsotheactuallengthofthetool[129]. While
thissecondpathwayisstillmarkedbyacleardistinctionbetweenthetoolandthearmsincethehandlevelreceives
observationsfrombothelements,athirdpathwayisconstructedsuchthattheobservationoftheballisonlylinkedto
thevirtuallevel. Asaresult,thisnewpotentialconfigurationviewsthearmandthetoolaspartofthesamekinematic
chain. Theinteractionsbetweenthesethreepathways(showninFigure18)mayshedlightonhowtheremappingof
themotorcortexgraduallyoccurswithextensivetooluse[10,11],modifyingtheboundariesbetweentheselfandthe
environment.
27
Figure18: Inthistask,theagent(a4-DoFarm)hastograspagreentoolandtrackamovingredballwiththetool’s
extremity. Therealarmconfigurationisrepresentedinblue,whilethelightgreenandlightredarmscorrespondto
thepotentialagent’sconfigurationsinrelationtothetoolandtheball,respectively. Notethatthelasttwolevelsofthe
tool’sbeliefgraduallymatchtherealtool,whiletheball’sbeliefmakesnodistinctionbetweenthearmandthetool,
andisonlydefinedbythetool’sextremity. The(deep)hierarchicalfactorizationallowstheagenttoinferapotential
configurationfortheballevenduringthefirstreachingmovement. See[90]formoredetails.
5 Discussion
Despitethemanyadvancesthathavebeenmadeinactiveinference,withincreasingpopularityamongdifferentscientific
domains,acurrentdrawbackisthatstudiesaboutmotorcontrolanddecision-makinghavebeensomewhatseparatedso
far,makinguseoftwohighlysimilarbutdistinctframeworks. Asaresult,thereisnoconsensusonhowtoachieve
dynamicplanning(i.e.,howtoperformdecision-makinginconstantlychangingenvironments),andstate-of-the-art
solutionstotacklecomplextasksgenerallycoupleactiveinferencewithtraditionalmethodsinmachinelearningor
optimalcontrol. Fromatheoreticalperspective,afewworksprescribedanefficientandelegantwayforcombiningthe
capabilitiesofdiscreteandcontinuousrepresentationsintoasinglegenerativemodel[49,67];however,thishybrid
approachhasnotreachedasmuchmaturity,withtheconsequencethattherearefarfewerstudiesonthesubjectinthe
literature.
Forthisreason,wetriedheretogiveacomprehensiveviewofthisyetunexploreddirection,comparingseveraldesign
choices regarding tasks of increasing complexity, with the intent of bringing motor control and behavioral studies
closer. Asapracticalexample,wedescribedthemodelingoftooluse[90],ataskthatinevitablycallsforbothdiscrete
andcontinuousframeworks,andthatrequirestakingtwoadditionalaspectsintoaccount,i.e.,objectaffordancesand
hierarchicalrelationships. Inasimplescenario,consideringatargettoreachasthecauseofsomehiddenstatesisa
reasonableassumptionandpermitsoperatingindynamiccontexts. Butwhentherearemultipleobjects,howdoesthe
agentdecidewhatthecauseofaparticularactionwillbe? Howcanitaccountfordifferentobjectaffordances? And
whatifthetargetmovesalonganon-trivialpath? Thehiddenstatesmaybefactorizedintoindependentdistributions
encodingmultipleentitiesinintrinsiccoordinates,henceexpressingpotentialbodyconfigurations. Further,thehidden
causesmaybelinkedtopotentialtrajectoriesrelatedtotheagent’sintentions[64,80,66]. Beliefsovereachentity
wouldhavetheirowndynamics,allowingtheagenttopredict,e.g.,thetrajectoryofamovingball. Then,wedescribed
howthisunitcouldbescaleduptoconstructcomplex(deep)hierarchicalstructures,e.g.,forsimulatinghumanbody
kinematics[80],andtoperformmoregeneraltransformationsofreferenceframes,e.g.,perspectiveprojections[83]. A
hierarchicalfactorizationofthehiddenstatesnowassumesabroaderperspectivethatcanalsoaccountformulti-agent
interaction–anaspectthathasbeenanalyzedinthediscreteframeworkaswell[130]. Finally,wedescribedthedesign
ofahybridunitwithdiscretehiddencausesandcontinuoushiddenstates,affordingdynamicinferenceviaBayesian
modelreduction[65];then,ahigher-leveldiscretemodelmadeitpossibletosimulatedynamictasksinvolvingonline
28
planningofactions. Thisshowedfurtherparallelismsbetweentheinferenceoftrajectoriesincontinuousmodelsand
policiesindiscretemodels.
Onechallengeinmaintainingadeeprepresentationofthekinematicchainistheassociatedcomputationalcomplexity
and the time required to infer a body posture from visual input, both increasing with the DoF. This is because an
extrinsicpredictionerrorgeneratedatthedistal(e.g.,hand)levelswouldhavetoclimbthehierarchytowardthe(root)
body-centeredreferenceframe. Thisdirectlyaffectsbehavioralaccuracyandmovementtime,whichcriticallydepend
onthecorrectinferenceoftheintrinsicstate. Aperformancecomparisonduringinferenceandactionisshownin[80],
forincreasingDoF.Incontrast,handlingmultipleobjectsdoesnotleadtoasignificantincreaseininferencetime,since
the(deep)representationsarecomputedinparallel,andareonlylimitedbythecomplexityoftheobjectdynamics.
Communicationwiththediscretemodelisequallyefficient,asasinglediscretestatecaninferthebodytrajectories
basedonmultipleunitsconcurrently[65]. Still,thebehaviorofotherobjectsintheenvironmentrequirespredictingnot
onlyhowtheagentwillbehaveafteraspecificactionbutalsohowtheotherobjectswillbehaveandtheirinfluence
ontheagent’sbehavior. Thisrequiresarichdiscreterepresentationthatcanmodeltheinteractionsofeveryobject,
and evaluating complex policies may not be as effective for high-dimensional settings [131]. A last issue regards
thetimeT neededforaccumulatingcontinuousevidenceforasinglediscretestepτ. Asshownin[66],Figure5b,a
narrowsamplingtimeintervalpermitsefficientcontrolinhighlydynamicenvironmentsbutatthecostofincreased
computationaltime.
Alimitationofthemodelsreviewedhereistheirfixedstructure. Thecriticalquestion,then,ishowtoolusecanbe
realizedwithoutembeddingpriorknowledgeintotheagent’sgenerativemodel. Inotherwords, howanagentcan
perform using active inference when starting from a blank memory or under the assumption that the surrounding
environmentwillremainunchanged. Acommoncriticismofcontinuous-timeactiveinferencemodelsisthattheir
generative models are a-priori defined and fixed, with intricate and hardcoded dynamics functions that raise some
concernsaboutbiologicalplausibility. Incontrast,oneappealingcharacteristicofPCNsisthattheysimulatebrain
processingwithextremelysimplefunctionstypicaloftheconnectivityofneuralnetworks(e.g.,linearcombinationsof
weightsandbiasespassedtoanon-linearactivationfunction). ThisallowsPCNstoeasilyadapttohigh-dimensional
data,withafewcriticaladvantagescomparedtodeeplearning(e.g.,abouttop-downmodulation)[25]. Whilemuchof
theresearchwithPCNsinvolvesstaticrepresentations,somestudiesaddressedhowpredictivecodingcouldbeusedto
learntemporalsequences[132,89],ortosolveRLtasks[133,91,134]. Here,wedemonstratedhowgenerativemodels
inactiveinferencecouldberealizedbysimplelikelihoodanddynamicsfunctions,showingsomeanalogieswiththe
inferenceofPCNs. Basedonthesefindings,apromisingresearchdirectionwouldbetoimitatetheir(deep)hierarchical
architectures(asinFigure9d),sothatanagentcouldnotonlyflexiblyadaptitsbodyschematointeractwithobjects
withdifferenthierarchicalstructures,butalsolearnthesystemdynamicsandactoverthemtoconformtopriorbeliefs.
Learningpoliciesincontinuousenvironmentsisnotaneasychallenge,butaddressingitwithstrategiesdifferentfrom
traditionalmethodsmightbekeyforadvancingwithcurrentintelligentagents,realizingthefulltheoreticalpotentialat
thebasisofactiveinferenceandthefreeenergyprinciple. Onthismatter,thestate-of-the-artistoapproximatethe
likelihoodandtransitiondistributionsbydeepneuralnetworks[135,136,137,138,139,140,141,142]. Whileseveral
benefitsarisecomparedtodeepRL,thisstillrelegatesthedeepstructurewithintheneuralnetwork,generallyusinga
single-levelactiveinferenceagent. OnestudyusedamorebiologicallyplausiblePCNasagenerativemodel[134],but
reliedonasimilarapproach. Asextensivelyanalyzedin[57],neuralnetworkscanbeseenasstaticgenerativemodels
withinfinitelyprecisepriorsatthelastlevelandnohiddenstates. Thisarchitecturecanbeusedtoperformsparse
codingorPrincipalComponentsAnalysis(PCA);however,itfailstoaccountfordynamicvariables,asindeconvolution
problemsorfilteringinstate-spacemodels. Temporaldepth–eitherdiscreteorcontinuous–isthuskeytoinferringthe
mostaccuraterepresentationoftheenvironment,andindeeditseemsthatcorticalcolumnsareabletoexpressmodel
dynamics(e.g.,theprefrontalcortexisconstantlyinvolvedinpredictingfuturestates,andmotion-sensitiveneuronshave
beenrecordedintheearlyvisualcortexaswell[143]). Whileitistruethattemporalsequencescanbeeasilyhandled
bydeeparchitecturessuchasrecurrentneuralnetworksortransformers[144],theirpassivegenerativemechanism
couldstillbereflectedinthebehavioroftheactiveinferenceagent. IncontrasttosuchapassiveAI,beinggroundedon
sensorimotorexperiencesandactivelymodifyingtheenvironmentcouldbefundamentaltotheemergenceofgenuine
understanding[145]. Takentogether,thesefactssuggestthatahierarchyofactionsupongeneralizedcoordinatesof
motionordiscretefuturestatescouldbringseveraladvantagesinsolvingRLtasks. Forinstance,representinganagent
in a hierarchical fashion afforded highly advanced control over its whole body structure that would not havebeen
possiblebyasinglelevelgeneratingonlythehandposition[80,129].
29
How to learn dynamic planning in deep hierarchical models? The importance of being discrete when considering
structure learning has been stressed in [146]. Indeed, hierarchical discrete models afford much more expressivity
comparedtotheircontinuouscounterparts, aboveall, derivingfromthesimplicityofcomputingtheexpectedfree
energy. Nonetheless,asFristonandcolleaguesnote,whetherusingcontinuousordiscreterepresentationsdependson
themodelevidence. Specifically,theformermayhavebetterperformanceswhentheevidencehascontiguityproperties,
e.g., whendealingwithtimeseriesorwithEuclideanspace. Indeed, thetaskexemplifiedinFigure18iseffective
becausetheBayesianmodelreductionperformsadynamicevidenceaccumulationovertheextrinsicspaceinwhich
theagentoperates. Hence,couplingthehierarchicaldepthofthehybridunitsinFigure17withahierarchicaldiscrete
architecture(andnotjustasinglediscretelevel)couldbringefficientstructurelearningalsoinconstantlychanging
environments. AsuccessfulBayesianapproachtolearningdiscretestructuresinvolvesgrowingdiscretedistributions
usinganinfiniteDirichletprocessandtheChineserestaurantprior[147]. Thismethodassumesapotentiallyinfinite
mixtureofbasisdistributionsandbuildsastructurestartingfromanemptymodelwhereinnovelconfigurationsare
eitherassignedtopopularexistingstates,oroccasionally,usedtocreatenovelstates. Thisapproachhasshownsuccess
inlearningstructuresthatsupportcomplexgoal-directedbehavior[148],hierarchicalspatialorganization[17],and
spatialnavigation[16]amongotherapplications. Analternativetohierarchicaldiscretemodelswouldbetocombine
units composed of a joint discrete-continuous model – as in Figure 15 – which would allow to perform dynamic
planningwithineachsingleunit. Whilethissolutionmaynotbesupportedbyempiricalevidencefrombiological
agents,itcouldbeanencouragingdirectiontoexplorefromamachinelearningperspective,contrastingthehypothesis
ofcentraldiscretedecision-makingwithadistributednetworkoflocaldecisions.
Athirdinterestingtopicregardsmotorintentionality. Althoughmulti-steptasksaretypicallytackledatthediscrete
level,weshowedherethat,underappropriateassumptions,anon-trivialbehaviorcouldbeachievedandanalyzedalso
atthecontinuouslevel. Theflexibleintentionswedefinedcouldbecomparedtoanadvancedstageofmotorskill
learning,consistingofautonomousandsmoothmovementsthatdonotnecessitateconsciousdecision-making[66].
Still,eveninthiscasethemodelstructurewaspredefined. Howdosuchintentionsemergeduringrepeatedexposureto
thesametask? Howdoestheagentscorewhichintentionswillbeappropriateforaspecificcontext? Asmentioned
inthelastchapter,optimizationofdynamicsprecisionsislikelytoinvolvethefreeenergyofreducedmodels(see
Equation38). Thisprocessmayshedlightonhowdiscreteactionsarisefromlow-levelcontinuoustrajectoriesand,
conversely, howthelatteraregeneratedfromacompositediscreteaction. Last, afewstudiesproposedadditional
connectionsbetweenpoliciesunfoldingatdifferenttimescales,eitherdirectly[124,125]orthroughdiscretehidden
states[123]. Suchapproachescouldbeadoptedinhybridandcontinuouscontextsaswell,sothatflexibleintentions
couldbepropagatedvialocalmessagepassingbetweenhiddencausesalongthewholehierarchy.
Dataavailability
CodeanddataaredepositedinGitHub(https://github.com/priorelli/dynamic-planning).
Acknowledgments
ThisresearchreceivedfundingfromtheEuropeanUnion’sHorizonH2020-EIC-FETPROACT-2019Programmefor
Research and Innovation under Grant Agreement 951910 to I.P.S. The funders had no role in study design, data
collectionandanalysis,decisiontopublish,orpreparationofthemanuscript.
References
[1] Rossella Breveglieri, Claudio Galletti, Annalisa Bosco, Michela Gamberini, and Patrizia Fattori. Object
affordancemodulatesvisualresponsesinthemacaquemedialposteriorparietalcortex. JournalofCognitive
Neuroscience,27(7):1447–1455,July2015.
[2] Maria C. Romero, Pierpaolo Pani, and Peter Janssen. Coding of shape features in the macaque anterior
intraparietalarea. TheJournalofNeuroscience,34(11):4006–4021,March2014.
[3] Natsuki Yamanobe, Weiwei Wan, Ixchel G. Ramirez-Alpizar, Damien Petit, Tokuo Tsuji, Shuichi Akizuki,
ManabuHashimoto,KazuyukiNagata,andKensukeHarada.Abriefreviewofaffordanceinroboticmanipulation
research. AdvancedRobotics,31(19–20):1086–1101,October2017.
30
[4] DanielBaldauf,HeCui,andRichardA.Andersen. Theposteriorparietalcortexencodesinparallelbothgoals
fordouble-reachsequences. JournalofNeuroscience,28(40):10081–10089,2008.
[5] DavidMeunier. Hierarchicalmodularityinhumanbrainfunctionalnetworks. FrontiersinNeuroinformatics,3,
2009.
[6] ClausC.HilgetagandAlexandrosGoulas. ‘hierarchy’intheorganizationofbrainnetworks. Philosophical
TransactionsoftheRoyalSocietyB:BiologicalSciences,375(1796):20190319,February2020.
[7] NicholasP.HolmesandCharlesSpence. Thebodyschemaandmultisensoryrepresentation(s)ofperipersonal
space. CognitiveProcessing,5(2):94–105,June2004.
[8] AtsushiYokoiandJörnDiedrichsen. Neuralorganizationofhierarchicalmotorsequencerepresentationsinthe
humanneocortex. Neuron,103(6):1178–1190.e7,September2019.
[9] ChristineAssaiante,F.Barlaam,F.Cignetti,andM.Vaugoyeau. Bodyschemabuildingduringchildhoodand
adolescence: Aneurosensoryapproach. NeurophysiologieClinique=ClinicalNeurophysiology,44(1):3–12,
January2014.
[10] Atsushi lriki, Michio Tanaka, and Yoshiaki Iwamura. Coding of modified body schema during tool use by
macaquepostcentralneurones. NeuroReport,7(14):2325–2330,October1996.
[11] ShigeruObayashi,TetsuyaSuhara,KoichiKawabe,TakashiOkauchi,JunMaeda,YoshihideAkine,Hirotaka
Onoe,andAtsushiIriki. Functionalbrainmappingofmonkeytooluse. NeuroImage,14(4):853–861,2001.
[12] ThomasA.Carlson,GeorgeAlvarez,Daw-anWu,andFransA.J.Verstraten. Rapidassimilationofexternal
objectsintothebodyschema. PsychologicalScience,21(7):1000–1005,May2010.
[13] LucillaCardinali,FrancescaFrassinetti,ClaudioBrozzoli,ChristianUrquizar,AliceC.Roy,andAlessandro
Farnè. Tool-useinducesmorphologicalupdatingofthebodyschema. CurrentBiology,19(13):478,2009.
[14] GiovanniPezzulo,FrancescoDonnarumma,DomenicoMaisto,andIvilinStoianov. Planningatdecisiontime
andinthebackgroundduringspatialnavigation. CurrentOpinioninBehavioralSciences,29:69–76,2019.
[15] ADavidRedish. Vicarioustrialanderror. NatureReviewsNeuroscience,17:147–159,2016.
[16] IStoianov,CPennartz,CLansink,andGPezzulo. Model-basedspatialnavigationinthehippocampus-ventral
striatumcircuit: acomputationalanalysis. PlosComputationalBiology,14(9):1–28,2018.
[17] IvilinStoianov,DomenicoMaisto,andGiovanniPezzulo.Thehippocampalformationasahierarchicalgenerative
modelsupportinggenerativereplayandcontinuallearning. ProgressinNeurobiology,217:1–20,2022.
[18] RajeshP.N.RaoandDanaH.Ballard. Predictivecodinginthevisualcortex: Afunctionalinterpretationofsome
extra-classicalreceptive-fieldeffects. NatureNeuroscience,2(1):79–87,1999.
[19] JakobHohwy. ThePredictiveMind. OxfordUniversityPressUK,2013.
[20] AndyClark. SurfingUncertainty: Prediction,Action,andtheEmbodiedMind. OxfordUniversityPress,01
2016.
[21] KarlFristonandStefanKiebel. Predictivecodingunderthefree-energyprinciple. PhilosophicalTransactionsof
theRoyalSocietyB:BiologicalSciences,364(1521):1211–1221,2009.
[22] JakobHohwy. Newdirectionsinpredictiveprocessing. MindandLanguage,35(2):209–223,2020.
[23] AndyClark. Whatevernext? Predictivebrains,situatedagents,andthefutureofcognitivescience. Behavioral
andBrainSciences,36(3):181–204,2013.
[24] StewartShipp. Neuralelementsforpredictivecoding. FrontiersinPsychology,7,November2016.
[25] BerenMillidge,AnilSeth,andChristopherLBuckley. Predictivecoding: atheoreticalandexperimentalreview,
2022.
[26] AlexanderOrorbiaandDanielKifer. Theneuralcodingframeworkforlearninggenerativemodels. Nature
Communications,13(1),2022.
[27] TommasoSalvatori,AnkurMali,ChristopherL.Buckley,ThomasLukasiewicz,RajeshP.N.Rao,KarlFriston,
andAlexanderOrorbia. Brain-inspiredcomputationalintelligenceviapredictivecoding,2023.
[28] James C R Whittington and Rafal Bogacz. An approximation of the error backpropagation algorithm in a
predictivecodingnetworkwithlocalhebbiansynapticplasticity. NeuralComput,29(5):1229–1262,March2017.
31
[29] JamesC.R.WhittingtonandRafalBogacz. TheoriesofErrorBack-PropagationintheBrain. TrendsinCognitive
Sciences,23(3):235–250,2019.
[30] BerenMillidge,AlexanderTschantz,andChristopherL.Buckley. PredictiveCodingApproximatesBackprop
AlongArbitraryComputationGraphs. NeuralComputation,34(6):1329–1368,2022.
[31] JakobHohwy,AndreasRoepstorff,andKarlFriston. Predictivecodingexplainsbinocularrivalry: Anepistemo-
logicalreview. Cognition,108(3):687–701,September2008.
[32] Karl J. Friston, Jean Daunizeau, James Kilner, and Stefan J. Kiebel. Action and behavior: A free-energy
formulation. BiologicalCybernetics,102(3):227–260,2010.
[33] KarlFriston. Thefree-energyprinciple: Aunifiedbraintheory? NatureReviewsNeuroscience,11(2):127–138,
2010.
[34] ChristopherL.Buckley,ChangSubKim,SimonMcGregor,andAnilK.Seth. Thefreeenergyprinciplefor
actionandperception: Amathematicalreview. JournalofMathematicalPsychology,81:55–79,2017.
[35] ThomasParr,GiovanniPezzulo,andKarlJFriston. Activeinference: thefreeenergyprincipleinmind,brain,
andbehavior. Cambridge,MA:MITPress,2021.
[36] KarlJ.Friston,JeanDaunizeau,andStefanJ.Kiebel. Reinforcementlearningoractiveinference? PLoSONE,
4(7),2009.
[37] KarlFriston. Whatisoptimalaboutmotorcontrol? Neuron,72(3):488–498,2011.
[38] RickA.Adams,StewartShipp,andKarlJ.Friston. Predictionsnotcommands: Activeinferenceinthemotor
system. BrainStructureandFunction,218(3):611–643,2013.
[39] HarrietBrown,KarlFriston,andSvenBestmann. Activeinference,attention,andmotorpreparation. Frontiers
inPsychology,2(SEP):1–10,2011.
[40] MatteoPriorelli,FedericoMaggiore,AntonellaMaselli,FrancescoDonnarumma,DomenicoMaisto,Francesco
Mannella, Ivilin Peev Stoianov, and Giovanni Pezzulo. Modeling motor control in continuous-time Active
Inference: asurvey. IEEETransactionsonCognitiveandDevelopmentalSystems,pages1–15,2023.
[41] PabloLanillos,CristianMeo,CorradoPezzato,AjithAnilMeera,MohamedBaioumy,WataruOhata,Alexander
Tschantz,BerenMillidge,MartijnWisse,ChristopherL.Buckley,andJunTani. Activeinferenceinroboticsand
artificialagents: Surveyandchallenges. CoRR,abs/2112.01871,2021.
[42] TadahiroTaniguchi,ShingoMurata,MasahiroSuzuki,DimitriOgnibene,PabloLanillos,EmreUgur,Lorenzo
Jamone,TomoakiNakamura,AlejandraCiria,BrunoLara,andGiovanniPezzulo. Worldmodelsandpredictive
codingforcognitiveanddevelopmentalrobotics: frontiersandchallenges. AdvancedRobotics,37(13):780–806,
June2023.
[43] Corrado Pezzato, Riccardo Ferrari, and Carlos Hernández Corbato. A novel adaptive controller for robot
manipulatorsbasedonactiveinference. IEEERoboticsandAutomationLetters,5(2):2973–2980,2020.
[44] A.Maselli,P.Lanillos,andG.Pezzulo. Activeinferenceunifiesintentionalandconflict-resolutionimperatives
ofmotorcontrol. PLOSComput.Biol,18(6),2022.
[45] FrancescoMannella, FedericoMaggiore, ManuelBaltieri, andGiovanniPezzulo. Activeinferencethrough
whiskers. NeuralNetworks,144:428–437,2021.
[46] AjithAnilMeera,FilipNovicky,ThomasParr,KarlFriston,PabloLanillos,andNoorSajid. Reclaimingsaliency:
Rhythmicprecision-modulatedactionandperception. FrontiersinNeurorobotics,16:1–23,2022.
[47] Raphael Kaplan and Karl J. Friston. Planning and navigation as active inference. Biological Cybernetics,
112(4):323–343,2018.
[48] Rick A. Adams, Klaas Enno Stephan, Harriet R. Brown, Christopher D. Frith, and Karl J. Friston. The
computationalanatomyofpsychosis. FrontiersinPsychiatry,4,2013.
[49] KarlJ.Friston,ThomasParr,andBertdeVries. Thegraphicalbrain: Beliefpropagationandactiveinference.
1(4):381–414,2017.
[50] Riccardo Proietti, Giovanni Pezzulo, and Alessia Tessari. An active inference model of hierarchical action
understanding,learningandimitation. PhysicsofLifeReviews,46:92–118,September2023.
32
[51] FrancescoDonnarumma,MarcelloCostantini,EttoreAmbrosini,KarlFriston,andGiovanniPezzulo. Action
perceptionashypothesistesting. Cortex,89:45–60,April2017.
[52] GiovanniPezzulo,LeoD’Amato,FrancescoMannella,MatteoPriorelli,ToonVandeMaele,IvilinPeevStoianov,
and Karl Friston. Neural representation in active inference: using generative models to interact with – and
understand–thelivedworld. AnnalsoftheNewYorkAcademyofSciences,inpress2024.
[53] PabloLanillos, JordiPages, andGordonCheng. Robotself/otherdistinction: activeinferencemeetsneural
networkslearninginamirror. (Ecai),2020.
[54] MarcToussaintandAmosStorkey. ProbabilisticinferenceforsolvingdiscreteandcontinuousstateMarkov
DecisionProcesses. ACMInternationalConferenceProceedingSeries,148:945–952,2006.
[55] MarcToussaint. Probabilisticinferenceasamodelofplannedbehavior. KünstlicheIntelligenz,3/09:23–29,
2009.
[56] MatthewBotvinickandMarcToussaint. Planningasinference. TrendsinCognitiveSciences,16(10):485–488,
2012.
[57] KarlFriston. Hierarchicalmodelsinthebrain. PLoSComputationalBiology,4(11),2008.
[58] KarlFriston,KlaasStephan,BaojuanLi,andJeanDaunizeau. Generalisedfiltering. MathematicalProblemsin
Engineering,2010:ArticleID621670,34p.–ArticleID621670,34p.,2010.
[59] StefanoFerraro,ToonVandeMaele,PietroMazzaglia,TimVerbelen,andBartDhoedt. Disentanglingshape
andposeforobject-centricdeepactiveinferencemodels,2022.
[60] RubenS.vanBergenandPabloL.Lanillos. Object-basedactiveinference,2022.
[61] Toon Van de Maele, Tim Verbelen, Ozan undefinedatal, and Bart Dhoedt. Embodied object representation
learningandrecognition. FrontiersinNeurorobotics,16,April2022.
[62] ToonVandeMaele,TimVerbelen,PietroMazzaglia,StefanoFerraro,andBartDhoedt. Object-centricscene
representationsusingactiveinference,2023.
[63] RickA.Adams,EduardoAponte,LouiseMarshall,andKarlJ.Friston. Activeinferenceandoculomotorpursuit:
Thedynamiccausalmodellingofeyemovements. JournalofNeuroscienceMethods,242:1–14,2015.
[64] Matteo Priorelli and Ivilin Peev Stoianov. Flexible Intentions: An Active Inference Theory. Frontiers in
ComputationalNeuroscience,17:1–41,2023.
[65] M.PriorelliandI.P.Stoianov. Dynamicinferencebymodelreduction. bioRxiv,2023.
[66] MatteoPriorelliandIvilinPeevStoianov. Slowbutflexibleorfastbutrigid? discreteandcontinuousprocesses
compared. Heliyon,pagee39129,October2024.
[67] KarlJ.Friston,RichardRosch,ThomasParr,CathyPrice,andHowardBowman. Deeptemporalmodelsand
activeinference. NeuroscienceandBiobehavioralReviews,77(November2016):388–402,2017.
[68] DavidM.Blei,AlpKucukelbir,andJonD.McAuliffe. Variationalinference: Areviewforstatisticians. Journal
oftheAmericanStatisticalAssociation,112(518):859–877,April2017.
[69] MaxwellJ.D.Ramstead,DaltonA.R.Sakthivadivel,ConorHeins,MagnusKoudahl,BerenMillidge,Lancelot
DaCosta,BrennanKlein,andKarlJ.Friston. Onbayesianmechanics: aphysicsofandbybeliefs. Interface
Focus,13(3),April2023.
[70] CansuSancaktar,MarcelA.J.vanGerven,andPabloLanillos. End-to-EndPixel-BasedDeepActiveInference
for Body Perception and Action. In 2020 Joint IEEE 10th International Conference on Development and
LearningandEpigeneticRobotics(ICDL-EpiRob),pages1–8,2020.
[71] GuillermoOliver,PabloLanillos,andGordonCheng. Anempiricalstudyofactiveinferenceonahumanoid
robot. IEEETransactionsonCognitiveandDevelopmentalSystems,8920(c):1–10,2021.
[72] CristianMeoandPabloLanillos. MultimodalVAEactiveinferencecontroller. CoRR,abs/2103.04412,2021.
[73] ThomasRood,MarcelvanGerven,andPabloLanillos. Adeepactiveinferencemodeloftherubber-handillusion.
2020.
[74] MohamedBaioumy,PaulDuckworth,BrunoLacerda,andNickHawes. Activeinferenceforintegratedstate-
estimation,control,andlearning. arXiv,2020.
33
[75] Cristian Meo, Giovanni Franzese, Corrado Pezzato, Max Spahn, and Pablo Lanillos. Adaptation through
prediction: Multisensoryactiveinferencetorquecontrol. IEEETransactionsonCognitiveandDevelopmental
Systems,15(1):32–41,2023.
[76] FredBos,AjithAnilMeera,DennisBenders,andMartijnWisse. FreeEnergyPrincipleforStateandInput
EstimationofaQuadcopterFlyinginWind. Proceedings-IEEEInternationalConferenceonRoboticsand
Automation,pages5389–5395,2022.
[77] AjithAnilMeeraandMartijnWisse. Dynamicexpectationmaximizationalgorithmforestimationoflinear
systemswithcolorednoise. Entropy,23(10),2021.
[78] Thomas Parr and Karl J. Friston. Active inference and the anatomy of oculomotion. Neuropsychologia,
111(January):334–343,2018.
[79] LéoPio-Lopez,AngeNizard,KarlFriston,andGiovanniPezzulo. Activeinferenceandrobotcontrol: Acase
study. JournaloftheRoyalSocietyInterface,13(122),2016.
[80] MatteoPriorelli,GiovanniPezzulo,andIvilinPeevStoianov. Deepkinematicinferenceaffordsefficientand
scalablecontrolofbodilymovements. ProceedingsoftheNationalAcademyofSciencesoftheUnitedStatesof
America,120,2023.
[81] Karl J Friston, Jérémie Mattout, and James Kilner. Action understanding and active inference. Biological
cybernetics,104(1-2):137–60,feb2011.
[82] GiovanniPezzulo,FrancescoRigoli,andKarlJ.Friston. HierarchicalActiveInference: ATheoryofMotivated
Control. TrendsinCognitiveSciences,22(4):294–306,2018.
[83] M.Priorelli,G.Pezzulo,andI.P.Stoianov. Activevisioninbinoculardepthestimation: Atop-downperspective.
Biomimetics,8(5),2023.
[84] EmanuelTodorov. Optimalityprinciplesinsensorimotorcontrol. NatureNeuroscience,7:907–915,2004.
[85] MareikeFloegel,JohannesKasper,PascalPerrier,andChristianA.Kell.Howtheconceptionofcontrolinfluences
ourunderstandingofactions. NatureReviewsNeuroscience,24(May):313–329,2023.
[86] GiuseppeVallar,ElieLobel,GaspareGalati,AlainBerthoz,LuigiPizzamiglio,andDenisLeBihan. Afronto-
parietalsystemforcomputingtheegocentricspatialframeofreferenceinhumans. ExperimentalBrainResearch,
124(3):281–286,January1999.
[87] JamesR.Hinman,G.WilliamChapman,andMichaelE.Hasselmo. Neuronalrepresentationofenvironmental
boundariesinegocentriccoordinates. NatureCommunications,10(1),June2019.
[88] KarlJ.Friston,ThomasParr,YanYufik,NoorSajid,CatherineJ.Price,andEmmaHolmes. Generativemodels,
linguisticcommunicationandactiveinference. NeuroscienceandBiobehavioralReviews,118:42–64,November
2020.
[89] BerenMillidge,MahyarOsanlouy,andRafalBogacz. PredictiveCodingNetworksforTemporalPrediction.
pages1–59,2023.
[90] MatteoPriorelliandIvilinPeevStoianov. Deephybridmodels: inferandplanintherealworld. arXiv,2024.
[91] RajeshP.N.Rao,DimitriosC.Gklezakos,andVishwasSathish. Activepredictivecoding: Aunifiedneural
frameworkforlearninghierarchicalworldmodelsforperceptionandplanning,2022.
[92] AresFisherandRajeshPNRao. Recursiveneuralprograms: Adifferentiableframeworkforlearningcomposi-
tionalpart-wholehierarchiesandimagegrammars. PNASNexus,2(11),October2023.
[93] GiacomoRizzolattiandLailaCraighero. Themirror-neuronsystem. AnnuRevNeurosci,27:169–192,2004.
[94] JamesM.Kilner,KarlJ.Friston,andChrisD.Frith. Predictivecoding: anaccountofthemirrorneuronsystem.
CognitiveProcessing,8(3):159–166,April2007.
[95] JeffHawkins,SubutaiAhmad,andYuweiCui. Atheoryofhowcolumnsintheneocortexenablelearningthe
structureoftheworld. FrontiersinNeuralCircuits,11,2017.
[96] LancelotDaCosta,ThomasParr,NoorSajid,SebastijanVeselic,VictoritaNeacsu,andKarlFriston. Active
inferenceondiscretestate-spaces: Asynthesis. JournalofMathematicalPsychology,99,2020.
34
[97] Ryan Smith, Karl J. Friston, and Christopher J. Whyte. A step-by-step tutorial on active inference and its
applicationtoempiricaldata. JournalofMathematicalPsychology,107:102632,2022.
[98] ThomasParr,RajeevVijayRikhye,MichaelM.Halassa,andKarlJ.Friston. PrefrontalComputationasActive
Inference. CerebralCortex,30(2):682–695,2020.
[99] K.J.Friston,L.Harrison,andWillPenny. Dynamiccausalmodelling. NeuroImage,19(4):1273–1302,2003.
[100] KarlFristonandWillPenny. PosthocBayesianmodelselection. NeuroImage,56(4):2089–2099,2011.
[101] KarlFriston,ThomasParr,andPeterZeidman. Bayesianmodelreduction. pages1–32,2018.
[102] M.J.Rosa,K.Friston,andW.Penny. Post-hocselectionofdynamiccausalmodels. JournalofNeuroscience
Methods,208(1):66–78,June2012.
[103] ThomasParrandKarlJ.Friston. TheDiscreteandContinuousBrain: FromDecisionstoMovement—AndBack
AgainThomas. NeuralComputation,30:2319–2347,2018.
[104] T. Parr and K. J. Friston. The computational pharmacology of oculomotion. Psychopharmacology (Berl.),
236(8):2473–2484,August2019.
[105] A.Tschantz,L.Barca,D.Maisto,C.L.Buckley,A.K.Seth,andG.Pezzulo. Simulatinghomeostatic,allostatic
andgoal-directedformsofinteroceptivecontrolusingactiveinference. BiologicalPsychology,169:108266,
2022.
[106] OzanÇatal,TimVerbelen,ToonVandeMaele,BartDhoedt,andAdamSafron. Robotnavigationashierarchical
activeinference. NeuralNetworks,142:192–204,2021.
[107] SheidaNozari,AliKrayani,PabloMarin-Plaza,LucioMarcenaro,DavidMartinGomez,andCarloRegazzoni.
Activeinferenceintegratedwithimitationlearningforautonomousdriving. IEEEAccess,10:49738–49756,
2022.
[108] Sheida Nozari, Ali Krayani, Pablo Marin, Lucio Marcenaro, David Martin Gomez, and Carlo Regazzoni.
Exploringaction-orientedmodelsviaactiveinferenceforautonomousvehicles. July2023.
[109] AliKrayani,KhalidKhan,LucioMarcenaro,MarioMarchese,andCarloRegazzoni. Agoal-directedtrajectory
planningusingactiveinferenceinuav-assistedwirelessnetworks. Sensors,23(15):6873,August2023.
[110] Ali Krayani, Khalid Khan, Lucio Marcenaro, Mario Marchese, and Carlo Regazzoni. Self-supervised path
planninginuav-aidedwirelessnetworksbasedonactiveinference. InICASSP2024-2024IEEEInternational
ConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),volume10,page13181–13185.IEEE,April
2024.
[111] Felix Obite, Ali Krayani, Atm S. Alam, Lucio Marcenaro, Arumugam Nallanathan, and Carlo Regazzoni.
Intelligentresourceallocationforuav-basedcognitivenomanetworks: Anactiveinferenceapproach. In2023
IEEEFutureNetworksWorldForum(FNWF),volume182,page1–7.IEEE,November2023.
[112] AliKrayani,AtmS.Alam,LucioMarcenaro,ArumugamNallanathan,andCarloRegazzoni. Anovelresource
allocationforanti-jammingincognitive-uavs: Anactiveinferenceapproach. IEEECommunicationsLetters,
26(10):2272–2276,October2022.
[113] CorradoPezzato,CarlosHernándezCorbato,StefanBonhof,andMartijnWisse. Activeinferenceandbehavior
treesforreactiveactionplanningandexecutioninrobotics. IEEETransactionsonRobotics,39(2):1050–1069,
April2023.
[114] PoppyCollis,RyanSingh,PaulFKinghorn,andChristopherLBuckley. Learninginhybridactiveinference
models,2024.
[115] NathanF.LeporaandGiovanniPezzulo. Embodiedchoice: Howactioninfluencesperceptualdecisionmaking.
PLOSComputationalBiology,11(4):e1004110,April2015.
[116] MatteoPriorelli,IvilinPeevStoianov,andGiovanniPezzulo. Embodieddecisionsasactiveinference. June
2024.
[117] TakuyaIsomura,ThomasParr,andKarlFriston. Bayesianfilteringwithmultipleinternalmodels: Towarda
theoryofsocialintelligence. NeuralComputation,31(12):2390–2431,2019.
[118] HarrietFeldmanandKarlJ.Friston. Attention,uncertainty,andfree-energy. FrontiersinHumanNeuroscience,
4,2010.
35
[119] ThomasParr,DavidA.Benrimoh,PeterVincent,andKarlJ.Friston. Precisionandfalseperceptualinference.
FrontiersinIntegrativeNeuroscience,12,September2018.
[120] F. Fattapposta, G. Amabile, M. V. Cordischi, D. Di Venanzio, A. Foti, F. Pierelli, C. D’Alessio, F. Pigozzi,
A.Parisi,andC.Morrocutti. Long-termpracticeeffectsonanewskilledmotorlearning:Anelectrophysiological
study. ElectroencephalographyandClinicalNeurophysiology,99(6):495–507,1996.
[121] FrancescoDiRusso,SabrinaPitzalis,TeresaAprile,andDonatellaSpinelli. Effectofpracticeonbrainactivity:
Aninvestigationintop-levelrifleshooters. MedicineandScienceinSportsandExercise,37(9):1586–1593,
2005.
[122] AnnM.Graybiel. Habits,rituals,andtheevaluativebrain. AnnualReviewofNeuroscience,31:359–387,2008.
[123] KarlJ.Friston,ThomasParr,ConorHeins,AxelConstant,DanielFriedman,TakuyaIsomura,ChrisFields,Tim
Verbelen,MaxwellRamstead,JohnClippinger,andChristopherD.Frith. Federatedinferenceandbeliefsharing.
Neuroscience&BiobehavioralReviews,156:105500,2024.
[124] ToonVandeMaele,BartDhoedt,TimVerbelen,andGiovanniPezzulo. Integratingcognitivemaplearning
andactiveinferenceforplanninginambiguousenvironments. InActiveInference,pages204–217,Cham,2024.
SpringerNatureSwitzerland.
[125] Daria de Tinguy, Toon Van de Maele, Tim Verbelen, and Bart Dhoedt. Spatial and temporal hierarchy for
autonomousnavigationusingactiveinferenceinminigridenvironment. Entropy,26(1):83,January2024.
[126] KarlFriston,LancelotDaCosta,DanijarHafner,CasperHesp,andThomasParr. Sophisticatedinference. Neural
Computation,33(3):713–763,2021.
[127] Matteo Priorelli, Ivilin Peev Stoianov, and Giovanni Pezzulo. Learning and embodied decisions in active
inference. August2024.
[128] Daniele Caligiore, Tomassino Ferrauto, Domenico Parisi, Neri Accornero, Marco Capozza, and Gianluca
Baldassarre. Usingmotorbabblingandhebbrulesformodelingthedevelopmentofreachingwithobstaclesand
grasping. 2008.
[129] MatteoPriorelliandIvilinPeevStoianov. Efficientmotorlearningthroughaction-perceptioncyclesindeep
kinematicinference. InActiveInference,pages59–70.SpringerNatureSwitzerland,2024.
[130] DomenicoMaisto,FrancescoDonnarumma,andGiovanniPezzulo. Interactiveinference: Amulti-agentmodel
ofcooperativejointactions. IEEETransactionsonSystems,Man,andCybernetics: Systems,54(2):704–715,
2024.
[131] AswinPaul,NoorSajid,LancelotDaCosta,andAdeelRazi. Onefficientcomputationinactiveinference. Expert
SystemswithApplications,253:124315,November2024.
[132] LinxingPrestonJiangandRajeshP.N.Rao. Dynamicpredictivecoding: Amodelofhierarchicalsequence
learningandpredictionintheneocortex. bioRxiv,2023.
[133] AlexanderOrorbiaandAnkurMali. ActivePredictingCoding: Brain-InspiredReinforcementLearningfor
SparseRewardRoboticControlProblems. 2022.
[134] BerenMillidge. CombiningActiveInferenceandHierarchicalPredictiveCoding: aTutorialIntroductionand
CaseStudy. PsyArXiv,2019.
[135] KaiUeltzhöffer. DeepActiveInference. pages1–40,2017.
[136] OzanÇatal,JohannesNauta,TimVerbelen,PieterSimoens,andB.Dhoedt. Bayesianpolicyselectionusing
activeinference. ArXiv,abs/1904.08149,2019.
[137] StefanoFerraro,ToonVandeMaele,TimVerbelen,andBartDhoedt. Symmetryandcomplexityinobject-centric
deepactiveinferencemodels. InterfaceFocus,13(3),April2023.
[138] KaiYuan,KarlFriston,ZhibinLi,andNoorSajid. Hierarchicalgenerativemodellingforautonomousrobots.
ResearchSquare,2023.
[139] BerenMillidge. Deepactiveinferenceasvariationalpolicygradients. JournalofMathematicalPsychology,96,
2020.
[140] Zafeirios Fountas, Noor Sajid, Pedro A.M. Mediano, and Karl Friston. Deep active inference agents using
Monte-Carlomethods. AdvancesinNeuralInformationProcessingSystems,2020-Decem(NeurIPS),2020.
36
[141] ThéophileChampion,MarekGrzes´,LisaBonheme,andHowardBowman. Deconstructingdeepactiveinference.
2023.
[142] AlekseyZelenovandVladimirKrylov. Deepactiveinferenceincontroltasks. In2021InternationalConference
onElectrical,Communication,andComputerEngineering(ICECCE),pages1–3,2021.
[143] StephenGrossbergandPraveenK.Pilly. Temporaldynamicsofdecision-makingduringmotionperceptionin
thevisualcortex. VisionResearch,48(12):1345–1373,June2008.
[144] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,ŁukaszKaiser,
andIlliaPolosukhin. Attentionisallyouneed. InI.Guyon,U.VonLuxburg,S.Bengio,H.Wallach,R.Fergus,
S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30.
CurranAssociates,Inc.,2017.
[145] GiovanniPezzulo,ThomasParr,PaulCisek,AndyClark,andKarlFriston. Generatingmeaning: activeinference
andthescopeandlimitsofpassiveai. TrendsinCognitiveSciences,28(2):97–112,February2024.
[146] KarlJ.Friston,LancelotDaCosta,AlexanderTschantz,AlexKiefer,TommasoSalvatori,VictoritaNeacsu,
MagnusKoudahl,ConorHeins,NoorSajid,DimitrijeMarkovic,ThomasParr,TimVerbelen,andChristopherL
Buckley. Supervisedstructurelearning. 2023.
[147] AdamNSanborn,ThomasLGriffiths,andDJNavarro. Rationalapproximationstorationalmodels: alternative
algorithmsforcategorylearning. PsychologicalReview,117(4):1144–1167,2010.
[148] Ivilin Stoianov, Aldo Genovesio, and Giovanni Pezzulo. Prefrontal goal codes emerge as latent states in
probabilisticvaluelearning. JournalofCognitiveNeuroscience,28(1):140–157,2016.
37

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Dynamic planning in hierarchical active inference"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
