=== IMPORTANT: ISOLATE THIS PAPER ===
You are revising a summary for ONLY the paper below. Do NOT reference or use content from any other papers.
Paper Title: World Models and Predictive Coding for Cognitive and Developmental Robotics: Frontiers and Challenges
Citation Key: taniguchi2023world
REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Issues to fix:
1. CRITICAL: The current summary has severe repetition issues. You MUST eliminate all repeated sentences, phrases, and paragraphs. Each idea should be expressed only once. If you find yourself repeating content, remove the duplicates entirely. Focus on variety and uniqueness in your wording.
2. Severe repetition detected: Same phrase appears 5 times (severe repetition)

Current draft (first 2000 chars):
## World Models and Predictive Coding for Cognitive and Developmental Robotics: Frontiers and ChallengesThis paper investigates the application of world models and predictive coding frameworks in the context of cognitive and developmental robotics. The authors argue that developing robust and adaptable robots requires the ability to construct and utilize internal models of the world, enabling them to predict future states and react appropriately to changing environments.The authors state: “A key challenge in building intelligent robots is to develop robust and adaptable models of the world.” They note: “The ability to predict future states is crucial for autonomous navigation and interaction.” The paper argues: “The development of robust and adaptive models of the world is essential for creating intelligent robots.” According to the research, “The ability to predict future states is a fundamental requirement for autonomous robots.”The authors identify several key findings: First, the development of world models enables robots to predict future states and react appropriately to changing environments. Second, predictive coding provides a framework for integrating sensory information with prior knowledge. Third, the application of predictive coding to robotics has the potential to improve the robustness and adaptability of robots.The methodology employed by the authors involves the development of a novel architecture for cognitive robotics based on predictive coding. This architecture incorporates several key components, including a hierarchical representation of the world, a mechanism for updating the representation based on sensory input, and a mechanism for generating predictions about the future. The experimental setup involved testing the architecture in a simulated environment. The results demonstrated that the architecture was able to successfully navigate the environment and perform a variety of tasks.The results demonstrate that the proposed architecture is ca...

Key terms: world, developmental, main, challenges, models, italy, cognitive, japan

=== FULL PAPER TEXT ===
January18,2023 AdvancedRobotics main
ToappearinAdvancedRobotics
Vol.xx,No.xx,Month20xx,1–28
Survey Paper
World Models and Predictive Coding for Cognitive and Developmental Robotics:
Frontiers and Challenges
TadahiroTaniguchia∗,ShingoMuratab,MasahiroSuzukic,DimitriOgnibened,e,PabloLanillosf,g,EmreUgurh,
LorenzoJamonei,TomoakiNakamuraj,AlejandraCiriak,BrunoLaral,andGiovanniPezzulom
aDepartmentofInformationScienceandEngineering,RitsumeikanUniversity,
1-1-1Noji-Higashi,Kusatsu,Shiga,Japan
bKeioUniversity,Japan
cTheUniversityofTokyo,Japan
dUniversita` Milano-Bicocca,Italy
eUniversityofEssex,UK
fDondersInstituteforBrain,CognitionandBehaviour,Netherlands
gCajalInternationalNeuroscienceCenter,SpanishNationalResearchCouncil,Spain.
hBogaziciUniversity,Turkey
iQueenMaryUniversityofLondon,UK
jTheUniversityofElectro-Communications,Japan
kNationalAutonomousUniversityofMexico,Mexico
lUniversidadAuto´nomadelEstadodeMorelos,Mexico
mInstituteofCognitiveSciencesandTechnologies,NationalResearchCouncilofItaly,Italy
(v1.0releasedApril2021)
Creatingautonomousrobotsthatcanactivelyexploretheenvironment,acquireknowledgeandlearnskillscon-
tinuouslyistheultimateachievementenvisionedincognitiveanddevelopmentalrobotics.Importantly,iftheaim
istocreaterobotsthatcancontinuouslydevelopthroughinteractionswiththeirenvironment,theirlearningpro-
cessesshouldbebasedoninteractionswiththeirphysicalandsocialworldinthemannerofhumanlearningand
cognitivedevelopment.Basedonthiscontext,inthispaper,wefocusonthetwoconceptsofworldmodelsand
predictive coding. Recently, world models have attracted renewed attention as a topic of considerable interest
inartificialintelligence.Cognitivesystemslearnworldmodelstobetterpredictfuturesensoryobservationsand
optimizetheirpolicies,i.e.,controllers.Alternatively,inneuroscience,predictivecodingproposesthatthebrain
continuouslypredictsitsinputsandadaptstomodelitsowndynamicsandcontrolbehaviorinitsenvironment.
Bothideasmaybeconsideredasunderpinningthecognitivedevelopmentofrobotsandhumanscapableofcon-
tinualorlifelonglearning.Althoughmanystudieshavebeenconductedonpredictivecodingincognitiverobotics
andneurorobotics,therelationshipbetweenworldmodel-basedapproachesinAIandpredictivecodinginrobotics
hasrarelybeendiscussed.Therefore,inthispaper,weclarifythedefinitions,relationships,andstatusofcurrent
research on these topics, as well as missing pieces of world models and predictive coding in conjunction with
cruciallyrelatedconceptssuchasthefree-energyprincipleandactiveinferenceinthecontextofcognitiveand
developmentalrobotics.Furthermore,weoutlinethefrontiersandchallengesinvolvedinworldmodelsandpre-
dictivecodingtowardthefurtherintegrationofAIandrobotics,aswellasthecreationofrobotswithrealcognitive
anddevelopmentalcapabilitiesinthefuture.
Keywords:Worldmodel;cognitiverobotics;predictivecoding;free-energyprinciple;activeinference;deep
generativemodels
∗Correspondingauthor.Email:taniguchi@ci.ritsumei.ac.jp
1
3202
naJ
41
]OR.sc[
1v23850.1032:viXra
January18,2023 AdvancedRobotics main
1. Introduction
How can we develop robots that can autonomously explore the environment, acquire knowledge, and
learnskillscontinuously?Creatingautonomouscognitiveanddevelopmentalrobotsthatcanco-existin
our society has been considered an ultimate goal of cognitive and developmental robotics and artificial
intelligence(AI)sincetheinceptionofthesefields.Autonomousrobotsthatcandevelopintherealworld
andcollaboratewithusmayalsobecalledembodiedartificialgeneralintelligence(AGI).Therecentsuc-
cess of artificial intelligence depends primarily on large-scale human-annotated data. However, human
infants can acquire knowledge and skills from sensorimotor information through physical interactions
withtheirenvironmentandsocialinteractionswithothers(e.g.,theirparentsorcaregivers).Importantly,
the aim is to build robots that can continuously develop through embodied interactions, their learning
process must be strongly based on their own sensorimotor experiences. This autonomous learning pro-
cessthatoccursthroughoutdevelopmentisalsoreferredtoascontinualorlifelonglearning[1–3],andis
consideredthefoundationfortheemergenceofbothindividualandsocialabilitiesnecessaryforrobots
withadaptiveandcollaborativecapabilities.
Recently,worldmodelshaveattractedrenewedattentioninartificialintelligence[4–7].Now,theterm
“world” does not indicate the objective world but rather refers to a world understood from a robot’s
point of view1. This idea corresponds to that of Umwelt proposed by Uexku¨ll [9]. Umwelt, literally
around-world, meaning environment or surroundings, refers to the self-centered world of an organism
perceivedthroughitsspecies-specificsensors2.Therefore,notably,theworldmodelisdifferentfromthe
bird’s-eyemodeloftheworldthatwasaimedtobuildingood-old-fashionedAIandcriticizedlater[12]3.
A cognitive system learns a world model to predict its future sensory observations better and optimize
its policies, also referred to as controllers. Note that although typically the term “world model” is used
to denote the spatiotemporal dynamics of the external environment, it could also equally apply to bod-
ily dynamics (including interoceptive signals from inside the body) and the social environment. This
world-model view entails previous ideas and results, such as the effect of behavioral feedback on sen-
sory sampling and perceptual learning [14] and the resulting acquisition of self-centered, yet efficient,
representationsinducedbyanactiveperceptionstrategyonthepartofanagent[15].
Predictivecodingisanotherrelatedtheorythatrecentlyhasbecomemoreandmoreinfluential[16].It
isheavilyinfluencedbyHelmholtz’searlytheoriesofperceptionasaprocessdrivenbylearning,knowl-
edge,andinference[17].Predictivecodingproposesthatthebraininferstheexternalcausesofsensations
by continuously predicting its input through top-down signals and adapts to minimize prediction error
[18, 19]. This substantiates the idea that the brain might use an adaptive world model to support per-
ception.Thefreeenergyprinciple(FEP)alsoproposesasimilarvision.Itarguesthatourbrainsupports
bothperception(perceptualinference)andaction(activeinference)usingaformofvariationalBayesian
inference; in particular, using (variational) free energy, it assesses the quality of the prediction and its
conformitytopriorbeliefs[20].Theseideas,whicharecurrentlyinfluentialinneuroscienceandcogni-
tivescience,arealsousedincognitiveanddevelopmentalrobotics,neurorobotics[21–23],andartificial
intelligencetodevelopneurodynamicsrealizingadaptivebehaviorsandsocialperception[24].
Although such a learning-driven world model-based approach is promising in cognitive and devel-
opmental robotics, the many applications and studies of world models tend to be limited to simulation
studies or adopt an offline pretrained world model [25]. Meanwhile, many studies based on predictive
coding have been conducted in the field of cognitive robotics and neurorobotics. However, the rela-
tionship between the world model-based approach in AI and the predictive coding-based approach in
robotics has rarely been discussed in an integrated manner. We believe that clarifying the definition,
1Thisviewpointmaybecalledarobot’ssubjectivepointofviewoftheworld.Philosophically,however,whetherrobotscanhavea“subjective”
pointofviewremainscontroversial[8].Therefore,wedescribethispointofviewsimplyas“arobot’spointofview”.
2Importantly,therelationshipbetweenUmweltandworldmodelingwassuggestedinsemiotics.Sebeokpointedoutthattheclosestequivalent
of Umwelt in English is “model” [10]. An Umwelt is created and constructed through a functional cycle, which includes 1) anticipation
of a perceptual cue, 2) perception, 3) working out a relation between the perception and action (either simply executing a habit or using
representation,ormodelinganew),and4)action(operation)[11].
3Also,notably,theworld-modelapproachisdifferentfrombehavior-basedrobotics[13],whichdoesnotlearnworldmodels.
2
January18,2023 AdvancedRobotics main
Figure1. Overviewofchallengesandrelationshipsbetweentopicsdescribedinthissurvey.Arobot,similartoahuman,receivessensations
x,infersinternalstatesz,exhibitsactionsa,andaffectscausesEinthesocialandphysicalenvironment.Theinitialproblemisdeterminingthe
modelsandarchitecturethatarobotusestoefficientlyandeffectivelylearnlatentrepresentations.Anapproachtothisproblemusesaneuro-
symbolicpredictivemodel,whichcombinesneuralnetworkandsymbolicmodels.Thenotionofobjectaffordancehighlightstheimportanceof
object-centricrepresentationlearningandthecouplingofactionandperception.Socialinteractionwithotheragentsisalsoanimportantarea
ofresearch.Developingartificialintelligenceforcognitiveanddevelopmentalautonomousrobotsbasedonknowledgeofneuroscience,i.e.,
brain-inspiredworldmodels,ispromising.Creatingcognitivearchitectureanddevelopingandsharingsoftwareframeworksforthispurpose
willalsobeanimportantfrontier.
relationship, current state of the art, notable research gaps in work on world models, predictive cod-
ing, free-energy principle, and active inference in the context of cognitive and developmental robotics
isimportantforfurtherprogressinthisfield.Basedonthecurrentstatus,weelucidatethefrontiersand
challengestowardthisholygrailincognitiveanddevelopmentalrobotics.
Inthissurveypaper,weaimtobuildbridgesandclarifythechallengesandfrontiersofworldmodels
andpredictivecodingincognitiverobotics.Theremainderofthispaperisstructuredasfollows.Section
2 provides a working definition of each key concept. Section 3 describes prior works related to the
concepts and clarifies state of the art. Section 4 describes some notable challenges. Some additional
discussionisprovidedinSection5,andweconcludetheworkinSection6.
2. Workingdefinition
2.1 Worldmodel
Worldmodelsdescribetheinternalmodelsofanagent,whichencodeshowworldstatesevolve,respond
to agents’ actions, and relate to a given sensory input [4, 26]. The term world model dates back to the
beginnings of artificial intelligence and robotics [27]. Early research in machine learning studied how
anagentcouldindependentlyacquireandadaptaworldmodelto[28,29].Currently,itusuallyrefersto
predictivemodels[30],whicharemainlyencodedusingdeepneuralnetworks.
In recent years, advancements in the studies on deep neural networks have enabled self-supervised
(or unsupervised) learning4 of large-scale world models directly from observations (sensory in-
puts)[26,31,32],andthesemodelshavebeenappliedinvariousareasofartificialintelligence,including
4Self-supervised learning is a type of unsupervised learning that aims to accomplish a task by learning to predict or classify any part of
unsuperviseddatafromanyotherpart.Incontrasttoself-supervisedlearning,unsupervisedlearningincludesclustering.
3
January18,2023 AdvancedRobotics main
reinforcementlearning(RL).Worldmodelsallowagentstoperformasample-efficientpredictionofthe
presentstateoftheworldandenablethepredictionoffuturestates,whichfurtherenablesefficientplan-
ning (equivalent to model-based reinforcement learning or control). A compact internal representation
furtherenablesplanninginanefficientlow-dimensionalspace.
Thekeyelementsofworldmodelsarepredictionandinference5.Predictionistheprobabilisticprocess
ofgeneratingtheobservationxgiventhestate(orrepresentation)z,whereasinferenceistheprocessof
obtainingastaterepresentationzfromanobservationxinaprobabilisticmanner.Inreal-worldsettings,
observationsxarelarge(high-dimensional,e.g.,images)andprovideonlypartialinformationaboutthe
world (partial observability). At the same time, the latent representation z is assumed to represent the
internal state of the world. In a static case, these can be summarized as the generative processes of
probabilitydistributionsasfollows.
Prediction: x∼ p(x|z)
Inference: z∼q(z|x), (1)
where p is a generative model and q is an inference (or recognition) model6. These models are consid-
ered parameterized in deep neural networks. When these models are trained simultaneously, generative
approaches (e.g., variational autoencoders [38]) are employed [26]. There are also cases where only an
inference model is trained, in which case a discriminative approach (e.g., contrastive learning [39]) is
used7.
In the most common conditions, the state of the environment (and agent body) and the observations
evolveovertimeinresponsetotheagent’sactions.Insuchcases,thestatezisoftenassumedtosatisfy
Markovconditions.Inturn,duetotypicalsensorylimitationssuchaslimitedfieldofvieworocclusions,
the environment is assumed to follow a partially observable Markov decision process (POMDP) [34].
Thatis,whenthecurrentinternalstateoftheenvironmentisz (wherethesubscriptrepresentsadiscrete
t
time step), performing an action a causes the internal state to transition to z and the corresponding
t t+1
x isobserved.InthePOMDPcase,thepredictionandinferencemodelsaregivenasfollows.
t+1
Prediction(transition): z ∼ p(z |z ,a )
t t t−1 t−1
Prediction(generation): x ∼ p(x |z )
t t t
Inference: z ∼q(z |x ,a ), (2)
t t 1:t 1:t−1
where x denotes the set of observations from the first step (x ) to step t (x ). To learn a state space
1:t 1 t
model (SSM) on the time interval 1 to T, a variational approach can be adopted that maximizes the
followingobjective[6,30,41]:
logp(x |a )
1:T 1:T−1
T T
≥ ∑E [logp(x |z )]−E [D [q(z |x ,a )||p(z |z ,a )]]≡ ∑L ,
q(zt|x1:t,a1:t−1) t t q(zt−1|x1:t−1,a1:t−2) KL t 1:t 1:t−1 t t−1 t−1 t
t=1(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) t=1
(Negative) predictionerror Regularization
(3)
5Theuseofthesetermsissometimesincongruentintheliteratureonstatisticsandmachinelearning;thewordinferenceisalsousedtodescribe
predictionorsubstitutedbyotherconcepts,suchasencodinganddecodingintheliteratureon(variational)autoencoders.
6Inclassicalformulationsofcontroltheory[33],AI[34],andprobabilisticrobotics[35],theinferencestepisperformedbyexactlyinverting
thepredictionprobabilitypusingtheBayestheorem.However,thisposesseveralcomputationalchallengesandisoftenintractable.Fordealing
withsuchcomplexity,sampling-basedapproximateinferencecanbeperformedusingMonteCarlomethods[36,37].Inthecontextofworld
modelsandpredictivecoding,variationalBayesapproachesareoftenpreferred[20].VariationalBayesinvolvesthedefinitionofanapproximate
inferencedistributionqˆ.Amortizedinferenceallowsustoapproximatetheq(x)usinganeuralnetwork,whichiscalledaninferencenetwork,
andobtainaninferencemodelq(x|a)[38].
7However,recently,studieshavealsoshownthatcontrastivelearningcanbeinterpretedasagenerativeapproach[40].Therefore,theboundary
betweengenerativeanddiscriminativeapproachesis,toacertainextent,blurred.
4
January18,2023 AdvancedRobotics main
wherethefirstterminEq.(3)representsthepredictionerror(orreconstructionerror)oftheobservation,
andthesecondtermrepresentstheregularizationforthestaterepresentation(sothatthetransitionmodel
andtheinferencemodelyieldthesamestaterepresentation).
2.2 Predictivecodingandthefree-energyprinciple
The original predictive coding model provided by Rao and Ballard [18] was proposed as a model of
visual processing in the brain. The model assumes a hierarchically organized neural network, and top-
down and bottom-up interactions at each hierarchical level are considered. In the top-down process,
higherlevelsgeneratepredictionsaboutlower-levelneuralactivities,andthelowestlevelgeneratessen-
sory predictions. In the bottom-up process, residual errors between the predictions and actual activities
(or sensory inputs) are computed and used to correct the originally generated predictions at each level.
Predictivecodingmodelslearnspatialandtemporalstatisticalregularitiesateachlevelforefficientcod-
ing and to reduce the redundancy of the predicted activity of lower levels [42, 43]. The main principle
behindthishierarchicalpredictivecodingcorticalorganizationinthebrainispredictionerrorminimiza-
tion(PEM)[19,44,45].ThisideabasedontheprincipleofPEMhasbeenextendedtovariouscognitive
processes,andthisframeworkisusuallyreferredtoaspredictiveprocessing[16,46,47].Thisapproach
isbeingrecentlyusedalsoinmachinelearningtolearnrobustgenerativemodelsofdata[48].
The principle of PEM can be situated within a more general principle of free-energy minimization
becausetheamountofvariationalfreeenergy,thecoreinformationmeasureusedintheFEP,canbeun-
derstood,undersimplifyingassumptions,astheamountofpredictionerror[19,45,49].Thevariational
freeenergyF,whichisanupperboundonthesurprise−logp(x |x ,a ),isthenegativevalueof
t t 1:t−1 1:t−1
theevidencelowerbound(ELBO)L introducedinEq.(3)asfollows.
t
−L =F
t t
=−E [logp(x |z )]+E [D [q(z |x ,a )||p(z |z ,a )]
q(zt|x1:t,a1:t−1) t t q(zt−1|x1:t−1,a1:t−2) KL t 1:t 1:t−1 t t−1 t−1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Predictionerror Regularization
=D [q(z |x ,a )||p(z |x ,a )]−logp(x |x ,a )
KL t 1:t 1:t−1 t 1:t 1:t−1 t 1:t−1 1:t−1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Divergence Evidence
≥−logp(x |x ,a ). (4)
t 1:t−1 1:t−1
(cid:124) (cid:123)(cid:122) (cid:125)
Surprise
From the second line of Eq. (4), when observations are assumed to follow a Gaussian distribution with
a fixed variance, minimizing the variational free energy is equivalent to minimizing the sum of mean
squarederrorsandaregularizationterm.
The FEP is a mathematical formulation of how self-organizing systems, such as biological agents,
brains, and cells, are able to maintain an equilibrium with their environment by means of minimizing
variational free energy, or the surprise associated with sensations8 [44, 50, 51]. In the FEP, different
cognitive processes such as perception and action can be understood as different ways to minimize the
variational free energy in terms of probabilistic inference called active inference [52–54] as detailed in
thenextsubsection.
2.3 Activeinferenceandexploration
Active inference is a normative framework that derives from the FEP and provides a unifying account
for perception, control, and learning in terms of minimization of the variational free energy in the past,
8Incontrasttostatesofsurprise,free-energycanbemeasuredbecauseitisafunctionofsensoryinputandtheinferredstate[45]
5
January18,2023 AdvancedRobotics main
present, and future. This unification is important in neuroscience as it reflects neural mechanisms and
on a computational level because it offers new perspectives and the possibility of sharing algorithmic
solutions between all these functions and transforming them into sophisticated robotic behaviors [23].
Forexample,perceptionaimstominimizethevariationalfreeenergyinthepastandpresentbyinferring
thelatentrepresentationsofobservedsensoryinputs9,e.g.,whenanorangeappearsinthefieldofview
instead of the apple as currently encoded in the internal representation, the representation state can
changetowardthatofanorange [19].Conversely,actionstrytominimizethevariationalfreeenergyin
thepresentbyactivelysamplingsensoryinputs,e.g.,bymovingthegazeawayfromtheorangetoward
anapple.Inadditiontoselectinganactioninthepresent,agentscaninferasequenceoffutureactions(or
policy)thatelicitthemostplausiblefuturestates [53–56]byconsideringtheminimizationofexpected
freeenergy,asdetailedbelow.
Whileactiveinferenceintroducesanimportantperspectivetowardsanunderstandingofadaptiveand
autonomous behaviors, an obvious behavioral imperative, the exploration-exploitation dilemma, seems
in conflict with this idea because exploration, i.e., observing an uncertain aspect of the environment,
would result in obtaining an unpredictable outcome [57, 58]. Indeed exploration and active perception
have a central role in robot control and learning. Several tasks focus on robots’ ability to explore an
unknown environment [59–61]. Furthermore, in social contexts, unobservable factors such as others’
intentionsmustbeactivelyconsideredtoallowforefficienthuman-robotcollaboration[62–65].
However, in [20, 54, 66], the authors showed that active inference can easily support exploratory
behaviorsandthatitcanprovideanelegantformalsolutionfortheexploration-exploitationdilemma.
In fact, we must consider that planning behaviors for an extended period of time require anticipating
future data. More specifically, to infer the best action sequences (policies), one must also predict the
futureobservationstheywouldproduce.Thisisrealizedintheactiveinferenceframeworkbyminimizing
theexpectedfreeenergyoveratimeintervalT.Wecanexpressthisasthesumoftwoterms,including
i) the variational information gain term [62, 67–69], or epistemic value [54], defined as the expected
KL divergence between the distribution of the latent states conditioned on the expected observations
q(z |x ,a ) and the prior distribution on the latent states q(z |a ) that represents the
t+1:T t+1:T t:T−1 t+1:T t:T−1
reduction in uncertainty on the latent statesz provided by the expected observations x , and ii)
t+1:T t+1:T
theextrinsicorpragmaticvaluelogp(x |C),whereCdenotestheagent’spreferences.Thisresultsin
t+1:T
thefollowingexpression10.
G(a )=−E [D [q(z |x ,a )||q(z |a )]]
t:T−1 q(zt+1:T,xt+1:T|at:T−1) KL t+1:T t+1:T t:T−1 t+1:T t:T−1
(cid:124) (cid:123)(cid:122) (cid:125)
Epistemicvalue
−E [logp(x |C)]. (5)
q(xt+1:T|at:T−1) t+1:T
(cid:124) (cid:123)(cid:122) (cid:125)
Pragmaticvalue
Theepistemicvaluetermfavorsobtainingobservationsthatdisambiguatetheworldstatesuchasobtain-
ingtheaddressforthebestappleshopintown,versusobservationsthatcorrespondtomultiple(aliased)
state such as corridors in a mall. Without the factor of variational information gain, asking the address
of the shop would not be preferred to any other action that would not immediately result in obtaining
an apple. Thus, minimizing expected free energy corresponds to maximizing the sum of epistemic and
pragmaticvaluesoveranextendedperiodanddefinestheoptimaltrade-offbetweenexplorationandex-
ploitation.ThesimilaritybetweentheepistemicvalueterminEq.(5)andthedivergenceterminEq.(4)
with an inverted sign may be noted. This is due to the different role that observations play in expected
free-energyformulation,wheretheycomprisenotobserveddatabutexpectedobservations.Finally,the
9ThisperceptioncanberegardedasavariationalBayesianversionoftheoriginalpredictivecodingthatemploysamaximumaposteriori
(MAP)estimation[44].
10Notethatintheliteratureonactiveinference,asequenceofactionsat:T−1isreferredtoasapolicyπ.Thispolicyisdifferentfromapolicy
inreinforcementlearning,whereitrepresentsastatisticalmappingfromstatestoactions(π(at|st)).Usingthisnotationofthepolicyπ and
mean-fieldapproximation,thegeneralformulationoftheexpectedfreeenergycanbedescribedbythefollowingmorepracticalformulation.
G(π)=−∑T
τ=t+1
E
q(xτ|π)
[DKL[q(z
τ
|x
τ
,π)||q(z
τ
|π)]]−E
q(xτ|π)
[logp(x
τ
|C)],wherethetimestepτ>tusedhereisafuturetimestep.
6
January18,2023 AdvancedRobotics main
Figure2. Researchesofworldmodels.Left:Dreamer[30]andDreaming[78].Right:robotcontrolsystemusingNewtonianVAE[79].
close connection between variational free energy (Eq. (4)), expected free energy (Eq. (5)), used in this
context to define behaviors with exploration capabilities, and ELBO (Eq. (3)), used to model learning
processes objectives, shows the versatility of this type of formulation, the extension and refinement of
whichcurrentlyapromisingfieldofresearchthataimstodevelopanautonomoussystemwiththeability
to efficiently acquire and execute complex skills [69]. For a more advanced and detailed presentation,
wereferto[20,54,69,70].
Anotherimportantframeworkthatconsidersbehaviorsasinferenceisplanningorcontrolasinference
(CaI) [71–75]. The main difference between CaI and active inference is that CaI introduces a binary
optimalityvariableO thatrepresentswhetheranactiona instatez isoptimal(orpreferred)[75,76].If
t t t
therewardfortakingactiona instatez isr(z ,a ),theconditionaldistributionoftheoptimalityvariable
t t t t
isdefinedasfollows.
p(O =1|z ,a )≡exp(r(z ,a )). (6)
t t t t t
Thus, unlike active inference, CaI can introduce the value of the reward at each time explicitly and
independentlyoftheobservation’sgenerativemodel11.
CaI aims to obtain the optimal policy p(a |z ) for inference. If the variational inference is chosen
t t
as a solution to the intractability of exact inference (as with active inference), we seek the policy that
maximizesthefollowingELBO12.
(cid:34) (cid:35)
T
logp(O )≥E ∑r(z ,a )+H(p(a |z )) , (7)
1:T ∏
t
T
=1
p(at|zt)p(zt|zt−1,at−1) t t t t
t=1
whereH representstheentropy.Thiscorrespondstotheentropy-regularizedexpectedreward,andrein-
forcementlearningwiththisastheobjectiveiscalledentropy-regularizedreinforcementlearning[77].
3. Priorworks
3.1 Worldmodelsandmodel-basedreinforcementlearninginAIandrobotics
Inthissection,wedescribeworldmodelsusedinmodel-basedreinforcementlearninginthecontextof
artificialintelligenceandrobotics.
Time-series world models conditioned on behavior have been studied for policy learning for some
time.Schmidhuberproposedlearninganagent’spolicy(utility)viaanRNN-basedworldmodelobtained
11Therefore,unlikeactiveinference,CaIdoesnotrequiretheassumptionofPOMDP.
12Here,MDPisassumed,i.e.,statezt isanobservedvariableratherthanalatentvariable;ifPOMDPisassumed,inferenceandgenerative
modelsforobservationsareaddedtothisELBO.
7
January18,2023 AdvancedRobotics main
by self-supervised learning [28]. Based on this idea, Ha et al. introduced a large-scale world model
consisting of a VAE and an RNN that learned directly from observations (time-series images) from the
external world [5]. They showed that the policies of agents trained only on this world model, which
learns a game environment, can work properly in real game environments. Since this study, research
hasbeenconductedonself-supervisedlearningofmodelsofanenvironmentdirectlyfromobservations,
alongwithideasreferredtoas“worldmodels”.However,theauthorstrainedspatialcompression(VAE)
and temporal transitions (RNN) separately; thus, the perspective of learning state representations was
notconsidered.
Subsequently, VAE-based models that simultaneously learn time transitions and spatial compression
have been proposed. Kaiser et al. proposed a VAE-based world model with discrete latent variables
designed to predict the next frame and reward from the stacked frames of the previous four steps and
the current action and showed that model-based reinforcement learning using this model performed
adequatelyinanAtarivideogameenvironmentwithhighsampleefficiency[80].Onelimitationofthis
model is that it does not include RNNs and cannot account for long-term prediction. Moreover, the
measuresarelearnedfromtheobservationspace,sothelearnedrepresentationisnotfullyexploited.Ke
etal.showedthatlearninglong-termtransitionsusingastochasticRNN-basedworldmodelcontributes
to high performance on tasks that require long-term prediction [81]. All of these models, however, are
autoregressive,requiringthegenerationofahigh-dimensionalobservationspaceeverystepforlong-term
prediction,andareunabletotransitionwithinthelatentspace.
Recently,modelsthatlearntransitionsinlatentspacewithoutrequiringautoregressivegenerationhave
beenwidelyused.Hafneretal.introducedarecurrentstatespacemodel(RSSM)thatincludesRNNsin
SSMandshowedthatitcouldbeusedforlong-termpredictionandmodel-basedreinforcementlearning
withhigherperformancethanmodel-freelearning[41].
WhilePlaNet[41],thefirststudyusingRSSM,usedanexistingmodel-basedplanningmethod(cross-
entropymethod)forplanninginthelatentspace13,Dreamer[30],asubsequentmethod,explicitlymod-
eledthepolicyandvaluefunctioninneuralnetworksandlearnedaworldmodelthroughgradientsinan
actor-criticframework,resultinginabetterperformancethanPlaNet.Thismodelhasbeenfurtherdevel-
opedbyreplacingthelatentvariableswithdiscretevalues,whichsignificantlyoutperformedmodel-free
performanceinAtarigameenvironments(DreamerV2[6]),andbyusingcontrastivelearninginsteadof
reconstruction,whichresultedinhigherperformanceontasksthatweredifficulttoreconstruct(Dream-
ing[78],seetheleftsideofFig.2).Theywerealsocombinedandcompared(DreamingV2[83]).
In terms of obtaining a good state representation for control, enforcing explicit constraints on tran-
sitions is preferable. For example, NewtonianVAE was able to form PD-controllable state space [84].
However, to develop such a model, what kind of state representation the world model should acquire
(asagoodrepresentationforcontrol)shouldbeconsidered,whichremainsasyetrelativelyunclear(see
section4.1fordetails).
These world models have been shown to be effective in learning using real robots. Okumura et al.
successfully applied the NewtonianVAE to a robot and enabled it to perform a precise socket insertion
task[79](seetherightsideofFig.2).Wuetal.showedthatDreamerV2enabledrealrobotstoperform
onlinelearningwithveryhighsampleefficiencyandperformance,whichincludesapipelineofacquiring
datathroughinteractionwiththeexternalworld,learningaworldmodel,andcontrollingtherobotusing
the model [7]. However, all of these results are for a single environment and task, and what kinds of
worldmodelsshouldbeacquiredforrobotcontrolindiverseenvironmentsandtasksremainsunclear.
3.2 Predictivecodingandactiveinferenceincognitiveanddevelopmentalrobotics
In recent years, an increasing body of research has considered predictive coding models for percep-
tionandactioninrobotics.Recentcomprehensivereviewsonactiveinferenceandpredictiveprocessing
in robotics can be found in [21, 23], respectively. These ideas aim to provide a general mathematical
13PlaNetwasextendedtobeuncertainty-awareonthebasisofBayesianinference[82].
8
January18,2023 AdvancedRobotics main
Figure3. Researchonpredictivecodingandactiveinferenceincognitiverobotics.Left:adaptationtoenvironmentalchangesbyprediction
errorminimization[89].Right:bodyperceptionandactionbypixel-baseddeepactiveinference[99].
accountofbehavior.Importantly,theyincorporateadaptationandrobustnesstocurrentmethodsincog-
nitiveanddevelopmentalrobotics.
Since the early works of Tani et al. using hierarchically organized RNNs [85], a variety of methods
havebeenproposedtoexploitthisideaofprediction-error-minimizationorpropagation.“Higherlevels”
(internal representation) generate predictions about the dynamics of the “lower levels” up to the sen-
sorimotor level. Prediction errors at the sensorimotor level, given the observations, are then propagated
“upwards” in the hierarchy correcting the internal state and thus minimizing the errors. Extensions of
Tani’sapproachallowmultipletimescales[86–89](seetheleftsideofFig.3),stochasticity[90,91]and
stochastic latent representations [92]. In particular, a precision-weighting mechanism for the PEM en-
abledrobotstoextractstochasticorfluctuatingstructuresoftemporalsensorimotorsequencesandutilize
the extracted structures for their action generation [91]. Interestingly, this mechanism is related to the
precisionaccountinpsychiatricdisorders[93](especiallyautismspectrumdisorder[94,95])andseveral
works have proposed cognitive robot models based on aberrant-precision to model unusual perception
andaction[96–98].
Aside from hierarchical RNNs, active inference controllers for robotic manipulators [100, 101] and
humanoidrobots[22,99]havealsobeendevelopedbasedonLanillos’sinitialworkonpredictivecoding
adaptiveperceptionandlearning[102,103]forbothlow-dimensionalandhigh-dimensionalinputs(see
therightsideofFig.3).Thesemethodshavewidespreadapplicationssuchasobjectmanipulation[104],
imitation[104],languageacquisition[105],socialinteraction[106]andnavigation[107].
Cognitiverobotsbenefitfrompredictivecodingmechanismstoinferothers’actions[108].Thereuse
of common circuits for both movement generation and action estimation seems to be a key principle in
thesensorimotororganization.Recently,theauthorsof[109]proposeddeepmodalityblendingnetworks
(DMBN)designedtocreateacommonlatentspacefromthemulti-modalexperienceofarobotbyblend-
ingmulti-modalsignalswithastochasticweightingmechanism.Usingastate-of-the-artskill-encoding
system referred to as Conditional Neural Movement Primitives (CNMPs) [110], they showed that deep
learningcouldfacilitateactionrecognitionandproducestructurestosustainanatomical(mirror-like)and
effect-basedimitationcapabilitieswhencombinedwithanovelmodality-blendingscheme.
Current state-of-the-art research is focusing on scaling active inference in planning tasks [51] with
high-dimensional inputs [32, 111] and improving representation learning through multimodal common
latent space [109] or introducing structural inductive biases, such as objects [112]. Whilst active infer-
ence is a promising framework for robotics [113], current works are still limited to a particular aspect
ofcognitiveanddevelopmentalprocesses.Therefore,inadditiontoextendingthescalabilityofcompu-
tationalframeworks,continualorlifelonglearningfordevelopingabilitiesfromlow-levelsensorimotor
skillstohigher-ordercognitivefunctionsshouldalsobeconsidered.
9
January18,2023 AdvancedRobotics main
4. FrontiersandChallenges
4.1 Latentrepresentationsforactionplanning
One of the most important challenges in world-model approaches of any kind is that of efficiently
performing planning, in the sense of generating meaningful actions to solve a sequential task [114].
Working in the high-dimensional space of the sensorimotor manifold is very computationally expen-
siveandprovideslocaloptimasolutions[99].Infact,currentapproachesinplanninguseacompressed
encoded representation of the world dynamics, which aids in the process of predicting future states
and in action generation [26]. In reinforcement learning, state representation learning is tied to learned
tasks to achieve high performance because it depends on the actions needed to obtain the maximum
expectedreward[30].However,thissometimespreventsgeneralizationacrosstasks.Decoupledaction-
representationworldmodelsareaninterestingwork-around[39].Indeepactiveinference[23]amortized
methodshavealsobeenconsidered[32,115],inadditiontocontrastive[116]anditerativeamortizedin-
ferenceapproaches[112].
However,thekeyquestioncannotbenarroweddowntothatwhattypeofarchitectureormethodshould
be used. Rather, what type of information should be encoded in the latent representation and how this
informationisprocessedmustbeakeyfocussothatinformationisnotuncoupledfromthesensorimotor
process, particularly from motor control, which is a key limitation of existing endeavors in robotics.
There has been considerable discussion as to what would comprise an appropriate state representation
of a world model; that is, what inductive bias or prior knowledge should be given [117–119]. Here, we
listthepropertiesofthispriorknowledgeweconsiderimportant.
• Low dimensionality. Observations obtained from the environment are high-dimensional, and
compressing this information into a low-dimensional space is critical for efficient data handling,
abstraction,andplanning.Thisapproachisthemostfrequentlyconsideredinstaterepresentation
learning.Thechallengeishowbesttorepresentobservationsinalow-dimensionalencodingwhile
retaining the necessary task-dependant information. Recent literature focused on generative and
discriminativeapproachestotacklethis.
• Meaningful abstraction and disentanglement. Low-dimensional representations should have
scene-understanding and task meaning, such as objects [112], locations [120] and temporal
events[121].Representationdisentanglementproposesthatfactorsofvariationwithdifferentse-
manticsshouldbeseparated,contributingtotherequirementsforsufficiencyandefficiencyinstate
representation. Object-centric representation learning is related to this hypothesis, [112, 122], in
whicheveryobservedobjectisencodedindependently.
• Compositionality. Although disentanglement aims to separate independent factors, the agent
should also acquire their relationships and hierarchy. In the case of object representations, there
shouldalsoberelationsorimplicationrelationsamongobjects.Currently,methodssuchasthose
using graph neural networks are being considered, but they do not provide an essential solution.
Thisideaofthecompositionalityofrepresentationisalsorelevanttotheneuro-symbolicapproach.
• Dynamics prediction. These three properties are important not only for learning representations
in static environments but in dynamic worlds, e.g., they consider a transition model that depends
on external factors and agent actions. The best latent representation is one that allows transitions
to be easily predictable for given actions. Many recent models use RNNs to learn transitions,
which incorporate information on long-term dependence [30, 41]. One way to make transitions
more predictable is to incorporate prior knowledge of the physical world (e.g., dynamics follow-
ing Newton’s laws of motion [84]). Furthermore, by learning to separate representations that are
not related to control from state representations, representations that are easier to control can be
acquired[123].
• Valuesaresufficientlyencoded.Toperformreinforcementlearningonthestaterepresentationof
the world model, the value of the state representative to the agent must be known. For example,
a recurrent state representation learns to predict the reward from the state so that the reward is
embedded in the representation [30]. However, because the value of the state changes depending
10
January18,2023 AdvancedRobotics main
onthetask,itremainsunclearwhetherthishypothesisshouldbeintroducedinaworldmodelthat
should acquire a prediction model that is as task-independent as possible. Alternatively, in active
inference approaches, the agent value function cannot be modified, and it is defined by expected
free energy. Here, the challenge becomes learning the state preferences and being able to predict
thetransitionsthatmayyieldthosepreferences.
• Task-agnostic.Representationsshouldbeinformativetosolve narrowproblemswheretheagent
is trained but also sufficiently general to be reused in tasks with different kinds of variability or
newtasksthattheagenthasneverencountered.
• Fusion of multiple-types multimodal information. Robots inevitably face a variety of events
with their multimodal sensorimotor systems. Observations given to world models are from mul-
tiple sources (e.g., social non-social, sensorimotor purely sensorial, and linguistic and non-
linguistic). They can have different reliability and volatility and represent various aspects of the
world. Therefore, the world models must properly encode the internal representation in a stable
andefficientmanner.
These are some of the elements that we identified that a latent representation should be fulfilled to
provideasmoothconnectionwithreal-worldinteractionandprovidepowerforsolvingcognitivetasks.
Importantly, abstract representation and disentanglement, such as objects or events encoding, may be
important to achieve efficient planning, reducing the gap for neuro-symbolic solutions. However, the
connection between the low-dimensional (and hierarchical) encoding and the synchronization with the
sensorimotorcontrolremainsamajorchallenge.
4.2 Neuro-symbolicpredictivemodels
In this section, we provide an overview of state-of-the-art techniques in which symbols and rules are
discovered and used by robots through neuro-symbolic approaches. The term symbols, here, refers to
manipulative discrete representations used in symbolic AI and cognitive science. The neuro-symbolic
approachattemptstointegrateconventionalsymbolicandmodernneuralnetwork-basedAIs.
Both biological and artificial agents benefit from predictive coding mechanisms for reasoning,
decision-making, and planning. Predictive forward models are used to generate plans that involve a
sequence of actions. For example, chimpanzees are known to generate multi-step plans that include
stacking a number of boxes on top of each other, grabbing a long stick, climbing on top of a stack of
boxes,andusingthesticktoreachtheobjectthatwasinitiallyoutofreach[124,125].Whiletheunderly-
ingcognitivemechanismsforhigh-levelplanningremainunknown,differentspecificbrainregionshave
beenshowntobecomeactiveininductiveanddeductivereasoninginhumans[126]whilepredictingthe
effectsofactions[127].Inartificialagents,ontheotherhand,standardsearchandplanningrelyheavily
onmanuallycodedorlearnedstatetransitionsandpredictionmodels[128,Ch.3–6,10-11].
The seminal work of [129] addressed the learning of discrete representations of predictive models,
i.e., dynamic Bayesian networks, by discretizing the continuous features of the environment to plan
goal-directedarm/handcontrol.[130]showedtheunitsgeneratedbyslowfeatureanalysiswiththelow-
est eigenvalues resemble symbolic representations that highly correlate with high-level features, which
mightbeconsideredprecursorsforfullysymbolicsystems.[131,132]studiedmethodstodiscoveruse-
fulsymbolsthatcanbedirectlyutilizedinproblemanddomaindefinitionlanguage(PDDL)forvarious
agentsettings.Insimulationandtherealworld,thediscoveredsymbolsweredirectlyusedaspredicates
intheactiondescriptionstogeneratedeterministicandprobabilisticsymbolicplans.[133]learnedsym-
bolsintheego-centricframeoftheagenttotransferthelearnedsymbolsintonovelsettings.[134,135]
discoveredsymbolsinthecontinuousperceptualspaceoftherobotsforPDDL-basedmanipulationplan-
ningviacombiningseveralmachinelearningalgorithmssuchasX-meansclusteringandsupportvector
machine (SVM) classification. Although the symbols were discovered by the robot without any human
intervention,thecontinuousperceptualfeaturesweremanuallyencodedbytheauthors.Towardsanend-
to-endframework,[136]useddirectlyrawcameraimageandpixelvaluestodiscoversymbolsviaanovel
deeppredictivecodingneuralarchitecture.Indetail,theyproposedadeepencoder-decodernetworkwith
a binary bottleneck layer designed to take a camera image and an action as input and output the action
11
January18,2023 AdvancedRobotics main
effects in pixel coordinates. The binary activations in the bottleneck layer encode object symbols that
not only depend on the visual input but are also shaped based on action and effect. In other words, the
objectsthatprovidethesameaffordances[137]wereautomaticallygroupedtogetherasobjectsymbols.
To distill the knowledge represented by the neural network into rules useful for symbolic reasoning, a
decision tree was trained to reproduce its decoder function. Probabilistic rules were extracted from the
effectpredictor/neuraldecoderandencodedintheprobabilisticPDDL,whichcanbedirectlyusedby
the off-the-shelf AI planners. In follow-up work, [138] used a multi-head attention mechanism to learn
symbolstoencodeaffordancesofavaryingnumberofobjects.
Asaietal.implementedaneuralframeworkwhereastateautoencoderwithadiscretebottlenecklayer
was trained first, and preconditions and effects of actions were learned next [139]. In follow-up work,
[140]combinedtheprevioustwosystemsanddiscoveredactionpreconditionsandeffectstogetherwith
visual symbols. These works were realized in visual environments such as 2-D puzzles and achieved
visualized plan executions. An important aspect of pure visual neuro-symbolic systems and studies on
neuro-symbolic robotics is that in robotics, predictive coding over object symbols takes actions and
effects into account in addition to the features of objects and the environment, which facilitates the
formationofsymbolsthatarelikelytocaptureobjectaffordances[141–144].
However,inthecontextofworldmodels,methodstointegratepriorsymbolicknowledgeintoVAEand
SSM-basedworldmodelsarestillbeingexplored.Thebottom-upformationofsymbolicrepresentations
iscloselyrelatedtodisentanglementandcompositionalitydiscussedinSection4.1.Moreover,leveraging
linguisticknowledgeinworldmodelingisachallengeinrelationtoneuro-symbolicpredictivemodels.
4.3 Affordanceperception
Affordanceperceptionhasoftenbeendiscussedindependentlyofworldmodels,butinfact,itisclosely
related. According to the original definition provided by Gibson [141, 145, 146], an affordance is an
actionpossibilityofferedtotheagentbytheenvironment.Astablesurfacemayaffordtobetraversed;a
stonemayaffordthepossibilityofbeingusedasahammer;adoorhandlemayaffordthepossibilityof
beingopened.Theconceptofaffordancesandaffordanceperceptionhasthenbeenfurtheranalyzedand
revisited in psychology, neuroscience, cognitive science, artificial intelligence, and robotics (see [143]
forarecentsurvey).Theabilitytoperceiveaffordancesiscrucialforanybiologicalorartificialagentto
interactsuccessfullywiththeenvironment.
Centraltotheideaofaffordancesisthattheactionpossibilitiesdependonboththeagentandtheenvi-
ronment; the same environment would offer different action possibilities to different agents, depending
on their sensorimotor capabilities. A stable surface affords the possibility of traversal to an agent that
is able to locomote and whose body dimensions fit the size of the surface borders; a stone affords the
possibilityofbeingusedasahammertoanagentwhoisabletopickitandwhohasenoughforcetolift
it;adoorhandleaffordstoopentoanagentwhoknowshowtoopendoors,assumingitiswell-designed
[147].Therefore,thoseaffordancesmustbelearned autonomouslybytheagent.Infact,theagentmust
learn how to perceive them. The means by which agents perceive affordances are those of ecological
perception, powerfully illustrated by Eleanor Jack Gibson [148–150]: “narrowing down from a vast
manifold of (perceptual) information to the minimal, optimal information that specifies the affordance
of an event, object, or layout” [150, p.284]. The agent must learn what minimal information is to be
picked; this happens both through evolution and development, leveraging the sensorimotor exploration
oftheenvironmentbyphysicalinteraction.
Interestingly, while exploring the action possibilities, the agent can learn the effects of those actions
aswell.Thisiscrucialforbiologicalagentsandturnsouttobeextremelyusefulforartificialsystemsas
well. Infact, mostcomputational modelsof affordancesin roboticsrely on representationsthat include
not only the action but also the effects (or in other terms, the goal) of the action [108, 110, 151–162];
therefore, the perceived possibilities for actions (and for achieving certain effects) can be used for ac-
tionplanning,leadingtoproblem-solving[163].Suchcomprehensivemodelsofaffordancesare,infact,
worldmodels;theyareinternalmodelsofhowtheworldbehaves“intheeyes”ofthelearningagent,and
theycanbeusedbytheagenttomakepredictionsabouthowtheworldwillchangeifcertainactionsare
12
January18,2023 AdvancedRobotics main
performed.Therefore,itisnotsurprisingthatthecomputationaltechniquesusedforlearningaffordance
models often overlap with those used for learning world models [164]. It is worth noting that, in world
model approaches, a robot only receives raw sensory information and needs to extract the relevant se-
manticsfromsuchdataflow;therefore,tosuccessfullyintegrateaffordanceperceptioninthesesystems,
thechallengesofmeaningfulabstraction/disentanglementandobject-centricrepresentationlearning,de-
scribedinSection4.1,areparticularlyrelevant.
4.4 Socialinteraction
Robots’“worlds”donotconsistofphysicalobjectsalonebutalsoofsocialentities,i.e.,peoplewhogive
them social guidance and try to cooperate with them. World models should model and predict social
dynamicsinvolvingpeople’sbehaviors,andinfertheirlatentvariables,e.g.,intentionsandemotions,to
cooperatewiththem,thatis,tocontrolsocialphenomena.
Efficient and safe human-robot collaboration and interaction are some of the main research objec-
tives of robotics and have important practical applications [165–171]. Associating beliefs, intentions,
or mental states to other agents, theory of mind, or, in other words, trying to predict the internal state
of another agent’s world model to understand its activities and context [172], is an essential aspect of
humaninteraction[173–175]andhasattractedattentioninrobotics[176].
Mutual understanding using a world model in social interaction can play an important role when
complexinteractionsarechallengingtheperceptualsystemsoftheagents,inducingamismatchbetween
theirinterpretationofthecurrentcontext[177,178].Itisalsocrucialwhendifferentlevelsofknowledge
andexpertiseinducedifferentrepresentationsofadomain,aswellasdifferentpointsofview,whichmay
induce conflictual interactions [179] or different support strategies [180]. For example, the perspective
of an automotive mechanic and that of an ordinary user differ considerably, so collaboration may be
difficultifonecannotproperlyinfertheinternalstateofothers.Arobots’worldmodelcanplayacrucial
roleinitsoperationandfunctionality[181].
The recent progress in machine learning methods has resulted in substantial improvement in action
recognition methodologies [65, 182–184]. However, this approach has often focused on shallow and
purely perceptual representations of the observed activities resulting in limited flexibility in terms of
contexts, tasks, and observed actors demanding a substantial amount of difficult-to-collect data and re-
trainingtimetoapplythesysteminrelativelysimilarconditions[185].Approachessuchasgoalrecogni-
tionasplanningorinverseplanning[166,186–188],that,givenamodeloftheenvironment,understand
others’activitiesbycomputingplansthatwouldresultintheobservedactionshaveshowntheflexibility
advantage delivered in intention recognition by a world model. Several works have extended this ap-
proach.Theproblemofdealingwithbehaviorsgeneratedunderpartialobservability,whichmayrequire
inferring both the plan and the beliefs, the mental state [176, 178], of the observed actor, was studied
with both classical planning [189] and Bayesian approaches [177, 190]. The impact of missing obser-
vationsfortheobserveragenthasalsobeenanalyzed[189].Afurtherstephasbeenproposedbyactive
methods for activity recognition [62, 191, 192] that use the same world model both to interpret others’
actionsaswellasselectingactionsthatwouldimprovetherecognitionprocess,e.g.,bygivingaccessto
the most informative observations [63] and allow the completion of a joint task [180]. While even the
initialformulationsofthisapproachwerecomputationallyaware[186],theirefficiencyisoftenaffected
bythelengthoftheobservedbehaviorandtheenvironmentcomplexity,resultinginmethodsthatcansel-
dombeappliedonlineonarobot.Severalmodelsproposedapre-compileapproachthattransformedthe
world into a form that would allow efficient plan recognition [185]. The adoption of hierarchical world
model representations has also been considered to constrain the computational and modeling costs of
the process [193–195]. Precomputed and robust local plans, in the form of the same motor controllers
that the robot uses to perform its own actions, have also been adopted to allow active perception for
actionrecognitionandpredictiononhumanoidrobots[62,172].Oneofthemainissuesoftheapproach,
also related to computational efficiency considerations, is relying on specific algorithms for planning
thataimingfortheoptimalplanmaymisinterprettheboundedrationalbehaviorsthatcollaboratorsmay
perform.ThisproblemwasfacedbyusingonlineBayesianinferencein[196].
13
January18,2023 AdvancedRobotics main
The additional flexibility provided by world models in social interaction skills is likely relevant be-
yondactivityrecognition.Itiseasytoimaginethatpurelysupervisedmodelsmaybelimitedintermsof
perspective-taking and the ability to reason based on the world structure may help to adapt to partners
with different sensory systems [176, 197–199]. Similarly, world models are likely to help with imita-
tion learning by dealing with embodiment mismatch between the observed actor and the learner [200].
Finally,physicalcooperation[201,202]andsignaling[203,204]wouldalsobemoreflexiblewhenin-
tegrating world and partner models in the equation, for example, to account for the trust of the human
cooperator towards the robot [205]. Finally, a world model may also be learned through socially rich
experiences and sources of information (e.g., imitation [200] or verbal instructions) in addition to the
results of autonomous exploration. However, developing a robust, efficient, and flexible enough repre-
sentationmayprovetobeoneofthemainchallengesinthiseffort.
4.5 Brain-inspiredworldmodels
Incognitivescience,ithaslongbeenpostulatedthatthebrainlearnssmall-scalemodelsoftheworldand
usesthesemodelsforvariouscognitivefunctions,suchasperception,planning,andimagination[206].
For example, theories of perception-as-inference described perception as an inferential process, which
works by “inverting” a generative model of how the percepts are generated [207, 208]. As discussed
above, these ideas (and others) have been recently formalized under the label of the Bayesian Brain
[209]andextendedbyActiveInferencefromthedomainofperceptiontootherdomains,suchasaction
planningandinteroception[20].
In parallel, there have been many attempts to describe mathematically and to assess the neuronal
underpinnings of world models and of inference processes empirically (e.g., [210]). One question that
hasreceivedagreatdealofattentionishowthebrainmightencodeinternalworldmodelsintheneuronal
substrate. Given that the brain models are often assumed to be probabilistic, various formal schemes
have been proposed that describe plausible neuronal implementations of probabilistic variables and of
Bayesian inference over these variables, such as, for example, probabilistic population codes [211] and
sampling schemes [212]. These attempts show that (probabilistic, generative) world models could be
at least potentially implemented in neuronal substrate [213, 214] – and even updated after statistical
learning[215]–butthespecificscheme(s)thatthebrainmightuseforthisremaintobefullyassessed.
Another relevant question is what algorithms the brain might use to perform inference over world
models. A strong candidate in neuroscience is predictive coding [18, 19]. Several studies have aimed
tovalidateitskeyempiricalpredictions,showingthatundertheappropriateconditions,itispossibleto
observepredictions[216],predictionerrors[217]andothersignaturesofinferenceinbrainsignals[218]
andthatneuralactivityinlowervisualareasintheabsenceofbottom-upinputscouldbeexplainedbythe
top-down,feedbackdynamicspostulatedbypredictivecoding[219].Theseandotherstudies(see[220]
forarecentreview)lendsomesupportforpredictivecoding,butthetheoryremainsunderdevelopment.
Atyetanotherlevel,onemayaskwhatthesystems-levelarchitecturethatsupportsworldmodelsand
whether different parts of the brain might model different aspects of the world is. Anatomical consid-
erations suggest that the brain is not a monolithic entity but rather is composed of several areas and
networks [221]; however, the extent to which these areas or networks are modularized and how they
exactly influence each other are heavily discussed [222]. One interesting consideration is that cortical
brain areas in humans and monkeys appear to be organized along principal gradients (defined by func-
tional connectivity); in one of these gradients, heteromodal areas (e.g., prefrontal cortex) are placed at
the top, and unimodal areas (e.g., primary visual area) at the bottom, recapitulating the structure of a
putative hierarchical generative model [223]. Another interesting consideration is that there seems to
be a “division of labor” between brain pathways that perform complementary computations, such as
the two visual pathways for processing “what” and “where” information [224]. These anatomical and
functional separations might be potentially interpreted as useful factorizations of the brain generative
models. A whole-brain probabilistic generative model (WB-PGM) approach attempts to build a cog-
nitive architecture for cognitive and developmental robots integrating probabilistic generative model
(PGM)-basedmodulesreferringcomprehensiveknowledgeofhumanandanimalbrainarchitecturesand
14
January18,2023 AdvancedRobotics main
theiranatomy[225].
The above studies indicate that at a general level, both neuroscience and machine learning / AI con-
ceive world models and inference in similar ways. However, at a more detailed level, there might be
profound differences between the ways these two disciplines use the same concepts. Predictive coding
and other biological schemes proposed in neuroscience exploit top-down dynamics (and recurrences)
in ways that are rarely used in machine learning. Furthermore, brain information processing is heav-
ily based on spontaneous brain dynamics, which are largely absent in machine learning systems; see
[226–228]foradetaileddiscussionofputativecomputationalrolesofspontaneousdynamics.Moreover,
it is plausible to assume that different parts of the brain might be specialized (or might have different
inductivebiases)toprocessdifferentstatisticalregularities,renderingthemabletolearnandmodel(for
example) slower or faster dynamics of the visual scenes, one’s own body, the actions of other agents,
or extended temporal events [229]. It is worth highlighting here that, although prediction errors have
a central role in learning, there are other forms of statistical learning, such as those based on Hebbian
associative learning [230]. It remains to be understood how to best endow our more advanced machine
learningsystemswiththeabilityofthebraintoperform(apparently)specializedcomputationsbutalso
orchestratethemcoherently.Finally,itisimportanttorememberthatthebrainisanevolvedsystem,and
our more advanced cognitive abilities are grounded in (the neuronal mechanisms supporting) simpler
sensorimotorskills[231,232].Tryingtodevelopadvancedcognitivesystemswithoutthenecessaryre-
quirements for embodied interaction and “phylogenetic refinement” might lead to solutions that differ
completelyfromhowthebrainworks–orthatfailaltogether.
4.6 Cognitivearchitectures
Truly cognitive and developmental robots, i.e., embodied AGI, that behave autonomously and flexibly
intherealenvironmentwouldhaveawiderangeofsensorsandexhibitmultiplefunctions.Thatrequires
alarge-scaleworldmodelthatdealswithmultimodalsensoryobservationsandmultilayeredstaterepre-
sentations. Considering the discussion in Section 4.5, such word models may be factorized in a proper
mannerfromengineeringandbiologicalviewpoints.TorealizeembodiedAGIs,furtherframeworksand
architectures to factorize a total world model into cognitive modules and to integrate individual cogni-
tivecapabilitiesintoacognitivesystemarerequired.Theideaisrelatedtocognitivearchitectures,which
havebeenstudiedincognitivescience,artificialintelligence,androbotics[233,234].
In cognitive science, cognitive functionalities such as memory, perception, and decision-making are
implemented as modules in the cognitive architectures studied, and the specific task can be solved by
activating these modules coordinately. ACT-R [235] and Soar [236] are representatives of cognitive
architectures. It has been shown that the model implemented by ACT-R can explain the time to solve
the task by humans, and activation patterns of the brain can be predicted by activation patterns of the
modules[237].Furthermore,Soarhasbeenusedforcontrollingrobots[238]andlearninggames[239].
However,complexmachinelearningmethodsthathaverapidlyadvancedinadecadearenotintroduced
yet. Sigma [240, 241] is a newer cognitive architecture that introduces the generative flow graph, a
generalizedprobabilisticgraphicalmodel.Therefore,themodelcanbeimplementedusingprobabilistic
programming techniques [242–244]. Furthermore, the concept of the standard model of the mind is
discussed through a synthesis across these three cognitive architectures [222]. Particularly, cognitive
architectures based on first principles, e.g., with a general computation scheme, such as free energy
minimization[19],areespeciallyattractive.Thearchitectureforsocialcognitionhasalsobeenproposed
[245].Theauthorspointoutthatthesearchitecturesexplainedaboveareincompleteindealingwiththe
socialaspectofcognitionanddescribetheelementsofarchitectureforsocialcognition.Clarion[246]is
anothercognitivearchitecturebasedondualprocesstheory[247].Inthisarchitecture,eachsubsystemis
composedofexplicitandimplicitprocesses,anditisshownthattheinteractionbetweenimplicit-explicit
processescanexplainpsychologicalphenomena.
In robotics, several types of cognitive architecture have been proposed. One of them is ArmarX
[248],whichhasthreelayers,includingamiddlewarelayer,arobotframeworklayer,andanapplication
layer. This three-layered structure simplifies the development robotics software easier. (Neuro-)Serket
15
January18,2023 AdvancedRobotics main
[249, 250] is another approach to integrating cognitive modules14. In (Neuro-)Serket, modules are de-
scribed by the (deep) PGM and trained mutually by exchanging messages between modules. To make
it easy to develop large-scale models, the modules in Neuro-Serket are weakly connected through the
Serketinterface.(Neuro-)SERKETiscloselyrelatedtotheworldmodel-basedapproachbecauseSER-
KET requires each module to be a PGM, i.e., a model based on prediction and inference as Eq. (1),
and integrate modules into a large PGM. This architecture does not provide any restrictions regard-
ing the functionalities of modules. Therefore, it has high flexibility but brings high dimensional design
space at the same time. To reduce the large degree of freedom in the design space, a brain-inspired
approach,WBA-PGM,wasproposed[225].Inthisapproach,acognitivemodelwasconstructedbycon-
necting PGM-based modules utilizing knowledge from neuroscience. By referring to the brain studies,
WBA-PGMconstrainsthefunctionofmodulesandtheirconnectionandreducesthedesignspaceofthe
cognitivemodel.
Therearetwocrucialrequirementsforcognitivearchitectureforcognitiveanddevelopmentalrobots,
which can be used along with the approach based on world models and predictive coding. The first is
theengineeringaspectwhichisseenin(Neuro-)SerketandArmarX.Thescaleofcognitivemodelsthat
enables the robots to behave flexibly in the real environment is very large, and many modules must be
connected and work collaboratively. Furthermore, the model needs to introduce machine learning tech-
niques that are not only existing as well as those will be developed in rapid progress. The development
ofsuchamodelwouldrequireamassiveengineeringeffort,andthisisconsideredanotableobstacleto
realizing such robots. Therefore, architecture is needed to simplify development. Another requirement
isthatofthescientificaspectseeninACT-R,Soar,WBA-PGM,andClarion.DevelopingAGI,whichis
human-likeintelligence,referringtotheknowledgeregardinghumansobtainedincognitivescienceand
neuroscience, can accelerate its development. However, meeting these two aspects completely is very
challenging.Allmachinelearningtechniquesandmoduleconnectionsmightnecessarilybenotreason-
able from the point of view of cognitive science and neuroscience. On the other hand, entire humans
are not understood yet. Therefore, finding common ground between engineering and science aspects
anddevelopinganovelcognitivearchitectureisacurrentchallenge.Developingalarge-scalecognitive
architectureandovercomingtheproblemsdescribedintheprevioussubsectionsisalsoachallenge.
5. Discussion
Aswedescribed,worldmodelsandpredictivecodingarepromisingapproachesincognitiveanddevel-
opmental robotics. Before closing this paper, we will mention some remaining issues which have not
beenaddressedinthemainbodysufficiently.
Language and world models: Umwelts, i.e., worlds from first-person views, of biological systems
are not monolithic but have some sort of structure. Notably, language and symbolic systems have syn-
tactic structures. The interaction between high-level cognitive capabilities, e.g., language and reason-
ing, and low-level cognitive capabilities, e.g., perception and action, is essential in world modeling.
Recently, large-scale language models (LLMs) have been replacing many natural language processing
methods[251,252],includingreasoningtasks,whichhavebeenconductedsolelybysymbolicAIbythe
endof2010s[253,254].Recently,theuseofLLMsinroboticshasbeenattempted,e.g., [255].Itisclear
that language learning and understanding by robots is itself a frontier [256]. To leverage the symbolic
knowledgeinLLMs,integrationofLLMsandworldmodelswillbeanimportantchallenge.
This shift from models of artificial symbols in conventional AI to models of natural language, i.e., a
human symbol system, is resonating with the discussion in symbol emergence in cognitive and devel-
opmental systems[257–259]. An important topic is then considering not onlythe integration of human
language into robots’ world models in a top-down manner but also the bottom-up formation of symbol
systems,includinglanguageinrelationtoworldmodels.
Policy representations: How should the policies of robots be represented? Conventionally, policies
14Neuro-SERKETisanupdatedversionofSERKET.
16
January18,2023 AdvancedRobotics main
are described as feedback controllers π(z ,a )= p(a |z ) in reinforcement and imitation learning. Even
t t t t
though one direction of world model approaches is to explore task agnostic representations (Section
4.1), the decomposition of world modeling and policy learning can be controversial. In a conventional
approachofworldmodels,policies(π = p(a |z )ora )andworldmodels(p(z |z ,a )and p(o |z ))
t t t:T t+1 t t t t
are decoupled. In contrast, a series of studies about predictive coding in neuro-robotics have been in-
tentionallyentanglingpoliciesandworldmodelsandmakingrobotsdirectlylearn p(o ,a |o ,a )
t+1 t+1 1:t 1:t
andexhibitingmany successfulresultsinrobotics, e.g.,[24,260,261].As thenotionofaffordance also
suggests, actions and perceptions are not independent and entangled, generally. The question “to what
extentshouldwedecoupleworldmodelandpolicyrepresentations?”shouldbeinvestigated.
From artificial cognition to human cognition: Cognitive and developmental robotics are also con-
structive approaches to human developmental cognition. Not only learning from neuro-, cognitive and
developmental sciences but also provide with scientific feedback to them is also an important mission.
Buildingavirtuouscirclebetweenstudiesonhumanandartificialstudiesisachallenge.
Theconstructiveapproachmaygiveusanovelapproachtoscientificandphilosophicalhardproblems
like self-awareness [262] and consciousness. The relationship between the multimodal world model
andglobalworkspacetheorywassuggested[263].Extendingthediscussionbetweenworldmodelsand
consciousness using robots may be an exciting challenge. Moreover, the relationship between predic-
tive coding and emotion is worth exploring to build emotional robots and understand the emotions of
biologicalsystems[264–266].
Softwareframeworksforimplementation:Toacceleratethestudiesonworldmodelsandpredictive
codinginrobotics,thedevelopmentframeworkforcognitiveanddevelopmentalroboticsiscruciallyim-
portant. In robotics, not only AI “software” frameworks but also middle-ware are important. Recently,
ROShasbeenwidelyusedintheroboticscommunityforbridginghardwareandAIsoftwarelayers.De-
velopingandsharingsuchsoftwareframeworksasacommunitywillbeimportant,e.g.,[267].Moreover,
theworldmodelinvolvesmanytypesofknowledge,andtheknowledgecanbeusedforachievingmul-
tiplefunctionsviaactiveinference.Thesoftwareframeworkshouldallowtheworldmodeltoefficiently
organize the knowledge and to perform (cross-modal) active inference. A great initiative is the discrete
state-space active inference python library [268]. However, it can only be used for toy examples due to
scalabilityissuesandtobeusefulincognitiveanddevelopmentalrobotics.Itneedsfurtherdevelopment.
For instance, the support for high-dimensional input observations and the possibility of combining dis-
crete and continuous action and state representations are something that has been addressed in robotic
approaches[23].
Data-efficient and autonomous learning: A generalist agent called GATO was developed based on
Transformers and shown to be able to solve various tasks with one neural network [269]. Although the
approach is superficially different, the approach is really related to the world models and the predictive
codingapproach.However,thelearningsystemishugelydata-hungry.Itisveryquestionableifthemodel
can be regarded as a model of human intelligence. Moreover, to train the generalist agent, researchers
needtopreparealargedatasetandsimulationenvironment.Humanchildrencanautonomouslyexplore
theirenvironmentandacquiredatathroughactiveexploration.Moreover,theyuseheuristicsandbiases
intheirdevelopmentalprocess.Learningandconsideringthehumandevelopmentalprocesswillgiveus
theinspirationtobuildrealgeneralistagents.Developingadata-efficientautonomouslearningarchitec-
turewithworldmodelsandpredictivecodingatitscoreisthekeytoatrulycognitiveanddevelopmental
system.
Emergenceofbehaviors:Shouldanagenthaveacompletelyinternalmodelofitsworld?Lastly,we
raisefundamentalspeculationabouttheworldmodel-basedapproach.Behaviorsarenotexternalization
ofinternallydesignedtrajectoriesbutsomethingtoemergethroughtheinteractionbetweenthebodyand
theenvironment.Forexample,ithasbeenprovenbypassivewalkingmachinesthatthebehaviorofwalk-
ingemergesonlyfromtheinteractionbetweenthebodyandtheenvironment,withoutanycomputation
bythebrain[270].Aboutthreedecadesago,Brooksfamouslyadvocatedthephysicalgroundinghypoth-
esistogetherwithsubsumptionarchitecture,sayingtheworldisitsownbestmodel[271].Therobotsbe-
havedsmoothlyandflexiblywithoutanyexplicitworldmodels.Thisisalsoreferredtoasmorphological
computation,whichmeansthebodyitselfimplicitlyprocessesinformationdynamically[272,273].Soft
17
January18,2023 AdvancedRobotics main
robotics emphasizes these points nowadays. Combining the viewpoints of the emergence of behaviors
withcomplexphysicaldynamicsandtheworldmodel-basedapproachisanotherimportantchallenge.
6. Conclusion
Inthissurveypaper,wehaveaimedtoclarifythefrontiersandchallengesofworldmodelsandpredictive
codingincognitiveanddevelopmentalrobotics.Creatinganautonomousrobotthatcanactivelyexplore
therealenvironment,acquireknowledge,andlearnskillscontinuouslyistheultimategoalofcognitive
and developmental robotics. To make the robot continuously develop through active exploration, the
robot’s learning process should be based on sensorimotor information obtained through physical and
social interactions with the physical and social environment. Following the motivation, this paper re-
viewed studies related to world models and predictive coding in cognitive and developmental robotics
and related AI studies. We clarified the definition of world model and predictive coding in robotics, in
conjunction with those of FEP and active inference, and discussed the relationship between them. We
alsointroducedstate-of-the-artandresearchgapsofstudiesonworldmodelsandpredictiveinrobotics.
Wedescribedsixfrontiersandchallenges,i.e.,latentrepresentationsforactionplanning,neuro-symbolic
predictivemodels,affordanceperception,socialinteraction,brain-inspiredworldmodels,andcognitive
architecture.Throughthesurveyandclarificationofchallenges,weprovidedfuturedirectionsfordevel-
opingcognitiveanddevelopmentalrobotsbasedonworldmodelsandpredictivecoding.
Acknowledgements
This work was partially supported by JST Moonshot R&D, Grant Number JPMJMS2033 and JP-
MJMS2011, JST PRESTO Grant Number JPMJPR22C9, the BAGEP Award of the Science Academy,
TUBITAK ARDEB 1001 program (project number: 120E274), the European Union’s Horizon 2020
FrameworkProgrammeforResearchandInnovationunderSpecificGrantAgreementsNo.945539(Hu-
manBrainProjectSGA3),No.952215(TAILOR),andNo.824153(POTION),theEuropeanResearch
CouncilundertheGrantAgreementNo.820213(ThinkAhead),theHumanBrainProjectSpecificGrant
Agreement 3 grant (ID 643945539, for the SPIKEFERENCE project), Deepself project under the Pri-
ority Programme “The Active Self” (SPP 2134), the project ‘COURAGE - A social media companion
safeguarding and educating students’ (No. 95563 and No. 9B145), and the Volkswagen Foundation in-
sidetheinitiativeArtificialIntelligenceandtheSocietyoftheFuture.
References
[1] LungarellaM,MettaG,PfeiferR,SandiniG.Developmentalrobotics:asurvey.Connectionscience.2003;
15(4):151–190.
[2] LesortT,LomonacoV,StoianA,MaltoniD,FilliatD,D´ıaz-Rodr´ıguezN.Continuallearningforrobotics:
Definition, framework, learning strategies, opportunities and challenges. Information fusion. 2020;58:52–
68.
[3] De Lange M, Aljundi R, Masana M, Parisot S, Jia X, Leonardis A, Slabaugh G, Tuytelaars T. A contin-
ual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern analysis and
machineintelligence.2021;44(7):3366–3385.
[4] Friston K, Moran RJ, Nagai Y, Taniguchi T, Gomi H, Tenenbaum J. World model learning and inference.
NeuralNetworks.2021;144:573–590.
[5] HaD,SchmidhuberJ.Worldmodels.arXivpreprintarXiv:180310122.2018;.
[6] Hafner D, Lillicrap T, Norouzi M, Ba J. Mastering Atari with discrete world models. arXiv preprint
arXiv:201002193.2020;.
[7] WuP,EscontrelaA,HafnerD,GoldbergK,AbbeelP.Daydreamer:Worldmodelsforphysicalrobotlearn-
ing.arXivpreprintarXiv:220614176.2022;.
18
January18,2023 AdvancedRobotics main
[8] Kiverstein J. Could a robot have a subjective point of view? Journal of Consciousness Studies. 2007;
14(7):127–139.
[9] VonUexku¨llJ.Astrollthroughtheworldsofanimalsandmen:Apicturebookofinvisibleworlds.Semiot-
ica.1992;89(4):319–391.
[10] SebeokTA.Biosemiotics:Itsroots,proliferation,andprospects.Semiotica.2001;:61–78.
[11] KullK.Umweltandmodelling.In:Theroutledgecompaniontosemiotics.Routledge.2009.p.65–78.
[12] BrooksR.Intelligencewithoutrepresentation.ArtificialIntelligence.1991;47(1-3):139–159.
[13] ArkinRC,ArkinRC,etal..Behavior-basedrobotics.MITpress.1998.
[14] VerschurePF,VoegtlinT,DouglasRJ.Environmentallymediatedsynergybetweenperceptionandbehaviour
inmobilerobots.Nature.2003;425(6958):620–624.
[15] Ognibene D, Baldassare G. Ecological active vision: four bioinspired principles to integrate bottom–up
andadaptivetop–downattentiontestedwithasimplecamera-armrobot.IEEEtransactionsonautonomous
mentaldevelopment.2014;7(1):3–25.
[16] ClarkA.Whatevernext?Predictivebrains,situatedagents,andthefutureofcognitivescience.Behavioral
andBrainSciences.2013;36(3):181–204.
[17] HelmholtzHv.Handbuchderphysiologischenoptik,vol.III.AllgemeineEncyklopa¨diederPhysik,Leipzig:
LeopoldVoss.1867;.
[18] RaoRP,BallardDH.Predictivecodinginthevisualcortex:afunctionalinterpretationofsomeextra-classical
receptive-fieldeffects.NatureNeuroscience.1999;2(1):79–87.
[19] Friston K. A theory of cortical responses. Philosophical transactions of the Royal Society B: Biological
sciences.2005;360(1456):815–836.
[20] ParrT,PezzuloG,FristonKJ.Activeinference:thefreeenergyprincipleinmind,brain,andbehavior.MIT
Press.2022.
[21] CiriaA,SchillaciG,PezzuloG,HafnerVV,LaraB.Predictiveprocessingincognitiverobotics:areview.
NeuralComputation.2021;33(5):1402–1432.
[22] OliverG,LanillosP,ChengG.Anempiricalstudyofactiveinferenceonahumanoidrobot.IEEETransac-
tionsonCognitiveandDevelopmentalSystems.2021;.
[23] LanillosP,MeoC,PezzatoC,MeeraAA,BaioumyM,OhataW,TschantzA,MillidgeB,WisseM,Buck-
ley CL, et al.. Active inference in robotics and artificial agents: Survey and challenges. arXiv preprint
arXiv:211201871.2021;.
[24] TaniJ.Exploringroboticminds:actions,symbols,andconsciousnessasself-organizingdynamicphenom-
ena.OxfordUniversityPress.2016.
[25] IbarzJ,TanJ,FinnC,KalakrishnanM,PastorP,LevineS.Howtotrainyourrobotwithdeepreinforcement
learning:lessonswehavelearned.TheInternationalJournalofRoboticsResearch.2021;40(4-5):698–721.
[26] HaD,SchmidhuberJ.Recurrentworldmodelsfacilitatepolicyevolution.Advancesinneuralinformation
processingsystems.2018;31.
[27] NilssonNJ,etal..Shakeytherobot.1984.TechRep.
[28] SchmidhuberJ.Makingtheworlddifferentiable:Onusingself-supervisedfullyrecurrentneuralnetworks
fordynamicreinforcementlearningandplanninginnon-stationaryenvironments.Inst.fu¨rInformatik.1990.
[29] Sutton RS. Integrated architectures for learning, planning, and reacting based on approximating dynamic
programming.In:Machinelearningproceedings1990.Elsevier.1990.p.216–224.
[30] HafnerD,LillicrapT,BaJ,NorouziM.Dreamtocontrol:Learningbehaviorsbylatentimagination.arXiv
preprintarXiv:191201603.2019;.
[31] Levine S, Finn C, Darrell T, Abbeel P. End-to-end training of deep visuomotor policies. The Journal of
MachineLearningResearch.2016;17(1):1334–1373.
[32] vanderHimstO,LanillosP.Deepactiveinferenceforpartiallyobservablemdps.In:VerbelenT,Lanillos
P, Buckley CL, De Boom C, editors. Active inference. Cham: Springer International Publishing. 2020. p.
61–71.
[33] Kalman RE. A new approach to linear filtering and prediction problems. Journal of Basic Engineering
(TransactionsoftheAmericanSocietyofMechanicalEngineers).1960;:35–45.
[34] CassandraAR,KaelblingLP,LittmanML.Actingoptimallyinpartiallyobservablestochasticdomains.In:
Aaai.Vol.94.1994.p.1023–1028.
[35] ThrunS,BurgardW,FoxD.Probabilisticrobotics.MITPress.2005.
[36] Chen Z, et al.. Bayesian filtering: From kalman filters to particle filters, and beyond. Statistics. 2003;
182(1):1–69.
[37] ThrunS.Particlefiltersinrobotics.In:Proceedingsoftheeighteenthconferenceonuncertaintyinartificial
intelligence.2002.p.511–518.
19
January18,2023 AdvancedRobotics main
[38] KingmaDP,WellingM.Auto-encodingvariationalbayes.arXivpreprintarXiv:13126114.2013;.
[39] LaskinM,SrinivasA,AbbeelP.CURL:Contrastiveunsupervisedrepresentationsforreinforcementlearn-
ing.In:InternationalConferenceonMachineLearning(ICML).2020.p.5639–5650.
[40] Nakamura H, Okada M, Taniguchi T. Self-supervised representation learning as multimodal variational
inference.arXivpreprintarXiv:220311437.2022;.
[41] HafnerD,LillicrapT,FischerI,VillegasR,HaD,LeeH,DavidsonJ.Learninglatentdynamicsforplanning
frompixels.In:Internationalconferenceonmachinelearning(ICML).2019.p.2555–2565.
[42] HuangY,RaoRP.Predictivecoding.WileyInterdisciplinaryReviews:CognitiveScience.2011;2(5):580–
593.
[43] HogendoornH,BurkittAN.Predictivecodingwithneuraltransmissiondelays:areal-timetemporalalign-
menthypothesis.Eneuro.2019;6(2).
[44] Friston K, Kiebel S. Predictive coding under the free-energy principle. Philosophical Transactions of the
RoyalSocietyB:BiologicalSciences.2009;364(1521):1211–1221.
[45] FristonK.Thefree-energyprinciple:aunifiedbraintheory?NatureReviewsNeuroscience.2010;11(2):127–
138.
[46] ClarkA.Surfinguncertainty:Prediction,action,andtheembodiedmind.OxfordUniversityPress.2015.
[47] HohwyJ.Newdirectionsinpredictiveprocessing.Mind&Language.2020;35(2):209–223.
[48] OrorbiaA,KiferD.Theneuralcodingframeworkforlearninggenerativemodels.Naturecommunications.
2022;13(1):1–14.
[49] Friston K, Kilner J, Harrison L. A free energy principle for the brain. Journal of Physiology-Paris. 2006;
100(1-3):70–87.
[50] Friston K, Mattout J, Kilner J. Action understanding and active inference. Biological cybernetics. 2011;
104(1):137–160.
[51] BuckleyC,KimC,McGregorS,SethA.Thefreeenergyprincipleforactionandperception:Amathemat-
icalreview.JournalofMathematicalPsychology.2017;81:55–79.
[52] Friston KJ, Daunizeau J, Kilner J, Kiebel SJ. Action and behavior: a free-energy formulation. Biological
Cybernetics.2010;102(3):227–260.
[53] Friston K, Adams RA, Perrinet L, Breakspear M. Perceptions as Hypotheses: Saccades as Experiments.
FrontiersinPsychology.2012;3(May):151.
[54] FristonK,RigoliF,OgnibeneD,MathysC,FitzgeraldT,PezzuloG.Activeinferenceandepistemicvalue.
Cognitiveneuroscience.2015;6(4):187–214.
[55] KruglanskiA,JaskoK,FristonK.Allthinkingis“wishful”thinking.TrendsinCognitiveSciences.2020;.
[56] FristonK,SamothrakisS,MontagueR.Activeinferenceandagency:optimalcontrolwithoutcostfunctions.
BiologicalCybernetics.2012;106(8-9):523–541.
[57] Friston K, Thornton C, Clark A. Free-energy minimization and the dark-room problem. Frontiers in psy-
chology.2012;:130.
[58] SunZ,FirestoneC.Thedarkroomproblem.TrendsinCognitiveSciences.2020;24(5):346–348.
[59] ChaplotDS,GandhiD,GuptaS,GuptaA,SalakhutdinovR.Learningtoexploreusingactiveneuralslam.
In:InternationalConferenceonLearningRepresentations(ICLR).2020.
[60] Ramakrishnan SK, Jayaraman D, Grauman K. Emergence of exploratory look-around behaviors through
activeobservationcompletion.ScienceRobotics.2019;4(30):eaaw6326.
[61] Ammirato P, Poirson P, Park E, Kosˇecka´ J, Berg AC. A dataset for developing and benchmarking active
vision.In:IEEEInternationalConferenceonRoboticsandAutomation(ICRA).2017.p.1378–1385.
[62] OgnibeneD,DemirisY.Towardsactiveeventrecognition.In:Ijcai.2013.p.2495–2501.
[63] LeeK,OgnibeneD,ChangHJ,KimTK,DemirisY.Stare:Spatio-temporalattentionrelocationformultiple
structuredactivitiesdetection.IEEETransactionsonImageProcessing.2015;24(12):5916–5927.
[64] DonnarummaF,CostantiniM,AmbrosiniE,FristonK,PezzuloG.Actionperceptionashypothesistesting.
Cortex.2017;89:45–60.
[65] WangB,HuangL,HoaiM.Activevisionforearlyrecognitionofhumanactions.In:IEEE/CVFConference
onComputerVisionandPatternRecognition(CVPR).2020.
[66] SchwartenbeckP,PasseckerJ,HauserTU,FitzGeraldTH,KronbichlerM,FristonKJ.Computationalmech-
anismsofcuriosityandgoal-directedexploration.Elife.2019;8.
[67] DenzlerJ,BrownC.Informationtheoreticsensordataselectionforactiveobjectrecognitionandstateesti-
mation.IEEETransactionsonPatternAnalysisandMachineIntelligence.2002;24(2):145–157.
[68] Sommerlade E, Reid I. Information-theoretic active scene exploration. In: IEEE Conference on Computer
VisionandPatternRecognition(CVPR).2008.p.1–7.
[69] HafnerD,OrtegaPA,BaJ,ParrT,FristonK,HeessN.Actionandperceptionasdivergenceminimization.
20
January18,2023 AdvancedRobotics main
arXivpreprintarXiv:200901791.2020;.
[70] ParrT,FristonKJ.Generalisedfreeenergyandactiveinference.Biologicalcybernetics.2019;113(5):495–
513.
[71] AttiasH.Planningbyprobabilisticinference.In:Internationalworkshoponartificialintelligenceandstatis-
tics.2003.p.9–16.
[72] Toussaint M. Robot trajectory optimization using approximate inference. In: International conference on
machinelearning(ICML).2009.p.1049–1056.
[73] KappenHJ,Go´mezV,OpperM.Optimalcontrolasagraphicalmodelinferenceproblem.Machinelearning.
2012;87(2):159–182.
[74] BotvinickM,ToussaintM.Planningasinference.Trendsincognitivesciences.2012;16(10):485–488.
[75] MillidgeB,TschantzA,SethAK,BuckleyCL.Ontherelationshipbetweenactiveinferenceandcontrolas
inference.In:Internationalworkshoponactiveinference.Springer.2020.p.3–11.
[76] Van de Cruys S, Friston K, Clark A. Controlled optimism: Reply to sun and firestone on the dark room
problem.TrendsinCognitiveSciences.2020;24(9):1–2.
[77] HaarnojaT,ZhouA,AbbeelP,LevineS.Softactor-critic:Off-policymaximumentropydeepreinforcement
learningwithastochasticactor.In:InternationalConferenceonMachineLearning(ICML).2018.p.1861–
1870.
[78] Okada M, Taniguchi T. Dreaming: Model-based reinforcement learning by latent imagination without re-
construction.In:IEEEInternationalConferenceonRoboticsandAutomation(ICRA).2021.p.4209–4215.
[79] OkumuraR,NishioN,TaniguchiT.Tactile-sensitiveNewtonianVAEforhigh-accuracyindustrialconnector-
socketinsertion.In:IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems(IROS).2022.
[80] KaiserL,BabaeizadehM,MilosP,OsinskiB,CampbellRH,CzechowskiK,ErhanD,FinnC,Kozakowski
P,LevineS,etal..Model-basedreinforcementlearningforatari.arXivpreprintarXiv:190300374.2019;.
[81] KeNR,SinghA,TouatiA,GoyalA,BengioY,ParikhD,BatraD.Learningdynamicsmodelinreinforce-
mentlearningbyincorporatingthelongtermfuture.arXivpreprintarXiv:190301599.2019;.
[82] Okada M, Kosaka N, Taniguchi T. Planet of the Bayesians: Reconsidering and improving deep planning
networkbyincorporatingbayesianinference.In:Ieee/rsjinternationalconferenceonintelligentrobotsand
systems(IROS).2020.p.5611–5618.
[83] Okada M, Taniguchi T. DreamingV2: Reinforcement learning with discrete world models without recon-
struction.In:IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems(IROS).2022.
[84] Jaques M, Burke M, Hospedales TM. NewtonianVAE: Proportional control and goal identification from
pixels via physical latent spaces. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR).2021.p.4454–4463.
[85] TaniJ.Learningtogeneratearticulatedbehaviorthroughthebottom-upandthetop-downinteractionpro-
cesses.NeuralNetworks.2003;16(1):11–23.
[86] Yamashita Y, Tani J. Emergence of functional hierarchy in a multiple timescale neural network model: A
humanoidrobotexperiment.PLoSComputationalBiology.2008;4(11):e1000220.
[87] NishimotoR,TaniJ.Developmentofhierarchicalstructuresforactionsandmotorimagery:Aconstructivist
viewfromsyntheticneuro-roboticsstudy.PsychologicalResearch.2009;73(4):545–558.
[88] NamikawaJ,NishimotoR,TaniJ.Aneurodynamicaccountofspontaneousbehaviour.PLoSComputational
Biology.2011;7(10):e1002221.
[89] YamashitaY,TaniJ.Spontaneouspredictionerrorgenerationinschizophrenia.PloSOne.2012;7(5):e37843.
[90] Murata S, Namikawa J, Arie H, Sugano S, Tani J. Learning to Reproduce Fluctuating Time Series by In-
ferring Their Time-Dependent Stochastic Properties: Application in Robot Learning Via Tutoring. IEEE
TransactionsonAutonomousMentalDevelopment.2013;5(4):298–310.
[91] MurataS,YamashitaY,ArieH,OgataT,SuganoS,TaniJ.LearningtoPerceivetheWorldasProbabilistic
orDeterministicviaInteractionWithOthers:ANeuro-RoboticsExperiment.IEEETransactionsonNeural
NetworksandLearningSystems.2017;28(4):830–848.
[92] AhmadiA,TaniJ.ANovelPredictive-Coding-InspiredVariationalRNNModelforOnlinePredictionand
Recognition.NeuralComputation.2019;31(11):2025–2074.
[93] LanillosP,OlivaD,PhilippsenA,YamashitaY,NagaiY,ChengG.Areviewonneuralnetworkmodelsof
schizophreniaandautismspectrumdisorder.NeuralNetworks.2020;122:338–363.
[94] VandeCruysS,EversK,VanderHallenR,VanEylenL,BoetsB,De-WitL,WagemansJ.Preciseminds
inuncertainworlds:predictivecodinginautism.Psychologicalreview.2014;121(4):649–75.
[95] LawsonRP,ReesG,FristonKJ.Anaberrantprecisionaccountofautism.FrontiersinHumanNeuroscience.
2014;8(May):1–10.
[96] IdeiH,MurataS,ChenY,YamashitaY,TaniJ,OgataT.ANeuroroboticsSimulationofAutisticBehavior
21
January18,2023 AdvancedRobotics main
InducedbyUnusualSensoryPrecision.ComputationalPsychiatry.2018;2:164–182.
[97] Idei H, Murata S, Yamashita Y, Ogata T. Homogeneous Intrinsic Neuronal Excitability Induces Overfit-
ting to Sensory Noise: A Robot Model of Neurodevelopmental Disorder. Frontiers in Psychiatry. 2020;
11(August):1–15.
[98] IdeiH,MurataS,YamashitaY,OgataT.Paradoxicalsensoryreactivityinducedbyfunctionaldisconnection
inarobotmodelofneurodevelopmentaldisorder.NeuralNetworks.2021;138:150–163.
[99] SancaktarC,vanGervenM,LanillosP.End-to-endpixel-baseddeepactiveinferenceforbodyperception
andaction.In:JointIEEEInternationalConferenceonDevelopmentandLearningandEpigeneticRobotics
(ICDL-EpiRob).2020.
[100] Pezzato C, Baioumy M, Corbato CH, Hawes N, Wisse M, Ferrari R. Active inference for fault tolerant
controlofrobotmanipulatorswithsensoryfaults.In:Internationalworkshoponactiveinference.Springer.
2020.p.20–27.
[101] Meo C, Lanillos P. Multimodal vae active inference controller. In: IEEE/RSJ International Conference on
IntelligentRobotsandSystems(IROS).IEEE.2021.
[102] LanillosP,ChengG.Activeinferencewithfunctionlearningforrobotbodyperception.InternationalWork-
shop on Continual Unsupervised Sensorimotor Learning, IEEE Developmental Learning and Epigenetic
Robotics(ICDL-Epirob).2018;.
[103] LanillosP,ChengG.Adaptiverobotbodylearningandestimationthroughpredictivecoding.In:IEEE/RSJ
InternationalConferenceonIntelligentRobotsandSystems(IROS).IEEE.2018.p.4083–4090.
[104] Ito M, Noda K, Hoshino Y, Tani J. Dynamic and interactive generation of object handling behaviors by a
smallhumanoidrobotusingadynamicneuralnetworkmodel.NeuralNetworks.2006;19(3):323–337.
[105] SugitaY,TaniJ.LearningSemanticCombinatorialityfromtheInteractionbetweenLinguisticandBehav-
ioralProcesses.AdaptiveBehavior.2005;13(1):33–52.
[106] ChenY,MurataS,ArieH,OgataT,TaniJ,SuganoS.Emergenceofinteractivebehaviorsbetweentworobots
bypredictionerrorminimizationmechanism.In:JointIEEEInternationalConferenceonDevelopmentand
LearningandEpigeneticRobotics(ICDL-EpiRob).2016.p.302–307.
[107] C¸atalO,VerbelenT,VandeMaeleT,DhoedtB,SafronA.Robotnavigationashierarchicalactiveinference.
NeuralNetworks.2021;142:192–204.
[108] ImreM,OztopE,NagaiY,UgurE.Affordance-basedaltruisticroboticarchitectureforhuman–robotcol-
laboration.AdaptiveBehavior.2019;27(4):223–241.
[109] Seker MY, Ahmetoglu A, Nagai Y, Asada M, Oztop E, Ugur E. Imitation and mirror systems in robots
throughdeepmodalityblendingnetworks.NeuralNetworks.2022;146:22–35.
[110] Seker MY, Imre M, Piater J, Ugur E. Conditional neural movement primitives. In: Robotics Science and
Systems(RSS).2019.
[111] TschantzA,BaltieriM,SethAK,BuckleyCL.Scalingactiveinference.In:internationaljointconference
onneuralnetworks(IJCNN).2020.p.1–8.
[112] vanBergenRS,LanillosPL.Object-basedactiveinference.arXivpreprintarXiv:220901258.2022;.
[113] DaCostaL,LanillosP,SajidN,FristonK,KhanS.Howactiveinferencecouldhelprevolutioniserobotics.
Entropy.2022;24(3):361.
[114] Valenzo D, Ciria A, Schillaci G, Lara B. Grounding context in embodied cognitive robotics. Frontiers in
Neurorobotics.2022;16.
[115] Fountas Z, Sajid N, Mediano P, Friston K. Deep active inference agents using monte-carlo methods. Ad-
vancesinneuralinformationprocessingsystems.2020;33:11662–11675.
[116] MazzagliaP,VerbelenT,DhoedtB.Contrastiveactiveinference.AdvancesinNeuralInformationProcess-
ingSystems.2021;34:13870–13882.
[117] Bo¨hmerW,SpringenbergJT,BoedeckerJ,RiedmillerM,ObermayerK.Autonomouslearningofstaterepre-
sentationsforcontrol:Anemergingfieldaimstoautonomouslylearnstaterepresentationsforreinforcement
learningagentsfromtheirreal-worldsensorobservations.KI-Ku¨nstlicheIntelligenz.2015;29(4):353–362.
[118] AchilleA,SoattoS.Aseparationprincipleforcontrolintheageofdeeplearning.AnnualReviewofControl,
Robotics,andAutonomousSystems.2018;1:287–307.
[119] LesortT,D´ıaz-Rodr´ıguezN,GoudouJF,FilliatD.Staterepresentationlearningforcontrol:Anoverview.
NeuralNetworks.2018;108:379–392.
[120] StoianovI,MaistoD,PezzuloG.Thehippocampalformationasahierarchicalgenerativemodelsupporting
generativereplayandcontinuallearning.ProgressinNeurobiology.2022;217:102329.
[121] Gumbsch C, Butz MV, Martius G. Sparsely changing latent states for prediction and planning in partially
observabledomains.AdvancesinNeuralInformationProcessingSystems.2021;34:17518–17531.
[122] Greff K, Kaufman RL, Kabra R, Watters N, Burgess C, Zoran D, Matthey L, Botvinick M, Lerchner A.
22
January18,2023 AdvancedRobotics main
Multi-object representation learning with iterative variational inference. In: International Conference on
MachineLearning(ICML).2019.p.2424–2433.
[123] WangT,DuSS,TorralbaA,IsolaP,ZhangA,TianY.DenoisedMDPs:Learningworldmodelsbetterthan
theworlditself.arXivpreprintarXiv:220615477.2022;.
[124] Ko¨hlerW,WinterE.Thementalityofapes.Internationallibraryofpsychology,philosophy,andscientific
method.K.Paul,Trench,Trubner&Company,Limited.1925.
[125] SteedmanM.Plans,affordances,andcombinatorygrammar.LinguisticsandPhilosophy.2002;25(5-6):723–
753.
[126] Goel V, Gold B, Kapur S, Houle S. Neuroanatomical correlates of human reasoning. Journal of cognitive
neuroscience.1998;10(3):293–302.
[127] AlN,Nolen-HoeksemaS.Atkinsonandhilgard’sintroductiontopsychology.CengageLearning.2014.
[128] RussellSJ,NorvigP.Artificialintelligence:amodernapproach.PearsonEducationLimited,.2016.
[129] Mugan J, Kuipers B. Autonomous learning of high-level states and actions in continuous environments.
IEEETransactionsonAutonomousMentalDevelopment.2012;4(1):70–86.
[130] Ahmetoglu A, Ugur E, Asada M, Oztop E. High-level features for resource economy and fast learning in
skilltransfer.AdvancedRobotics.2022;36(5-6):291–303.
[131] KonidarisG,KaelblingLP,Lozano-PerezT.Constructingsymbolicrepresentationsforhigh-levelplanning.
In:AAAIConferenceonArtificialIntelligence(AAAI).2014.
[132] Konidaris G, Kaelbling L, Lozano-Perez T. Symbol acquisition for probabilistic high-level planning. In:
InternationalJointConferenceonArtificialIntelligence(IJCAI).2015.
[133] JamesS,RosmanB,KonidarisG.Learningportablerepresentationsforhigh-levelplanning.arXivpreprint
arXiv:190512006.2019;.
[134] Ugur E, Piater J. Bottom-up learning of object categories, action effects and logical rules: From contin-
uous manipulative exploration to symbolic planning. In: IEEE International Conference on Robotics and
Automation(ICRA).2015.p.2627–2633.
[135] UgurE,PiaterJ.Refiningdiscoveredsymbolswithmulti-stepinteractionexperience.In:2015ieee-ras15th
internationalconferenceonhumanoidrobots(humanoids).2015.p.1007–1012.
[136] AhmetogluA,SekerMY,PiaterJ,OztopE,UgurE.Deepsym:Deepsymbolgenerationandrulelearning
from unsupervised continuous robot interaction for planning. Journal of Artificial Intelligence Research.
2022;.
[137] S¸ahin E, Cakmak M, Dog˘ar MR, Ug˘ur E, U¨c¸oluk G. To afford or not to afford: A new formalization of
affordancestowardaffordance-basedrobotcontrol.AdaptiveBehavior.2007;15(4):447–472.
[138] AhmetogluA,OztopE,UgurE.Learningmulti-objectsymbolsformanipulationwithattentivedeepeffect
predictors.arXivpreprintarXiv:220801021.2022;.
[139] AsaiM,FukunagaA.Classicalplanningindeeplatentspace:Bridgingthesubsymbolic-symbolicboundary.
arXivpreprintarXiv:170500154.2017;.
[140] AsaiM,MuiseC.Learningneural-symbolicdescriptiveplanningmodelsviacube-spacepriors:Thevoyage
home(tostrips).arXivpreprintarXiv:200412850.2020;.
[141] GibsonJJ.Theecologicalapproachtovisualperception.Boston:HoughtonMifflin.1979.
[142] ZechP,HallerS,LakaniSR,RidgeB,UgurE,PiaterJ.Computationalmodelsofaffordanceinrobotics:a
taxonomyandsystematicclassification.AdaptiveBehavior.2017;25(5):235–271.
[143] Jamone L, Ugur E, Cangelosi A, Fadiga L, Bernardino A, Piater J, Santos-Victor J. Affordances in psy-
chology,neuroscienceandrobotics:asurvey.IEEETransactionsonCognitiveandDevelopmentalSystems.
2016;10(1):4–25.
[144] RenaudoE,ZechP,ChatilaR,KhamassiM.Computationalmodelsofaffordanceforrobotics.Frontiersin
Neurorobotics.2022;16:1045355.
[145] GibsonJJ.Thesensesconsideredasperceptualsystems.Boston:HoughtonMifflin.1966.
[146] GibsonJJ.Thetheoryofaffordances.Perceiving,Acting,andKnowing:Towardanecologicalpsychology.
Eds.LawrenceErlbaumAssociates.1977.
[147] NormanDA.Affordance,conventions,anddesign.Interactions.1999;6(3):38–42.
[148] GibsonEJ.Anodysseyinlearningandperception.MITPress.1994.
[149] Gibson EJ. Perceptual learning in development: Some basic concepts. Ecological Psychology. 2000;
12(4):295–302.
[150] GibsonEJ.Theworldissofullofanumberofthings:Onspecificationandperceptuallearning.Ecological
Psychology.2003;15(4):283–288.
[151] S¸ahin E, Cakmak M, Dog˘ar MR, Ug˘ur E, U¨c¸oluk G. To afford or not to afford: A new formalization of
affordancestowardaffordance-basedrobotcontrol.AdaptiveBehavior.2007;15(4):447–472.
23
January18,2023 AdvancedRobotics main
[152] MontesanoL,LopesM,BernardinoA,Santos-VictorJ.Learningobjectaffordances:Fromsensory–motor
coordinationtoimitation.IEEETransactionsonRobotics.2008;24(1):15–26.
[153] UgurE,S¸ahinE,OztopE.Unsupervisedlearningofobjectaffordancesforplanninginamobilemanipula-
tionplatform.In:IEEEInternationalConferenceonRoboticsandAutomation(ICRA).2011.p.4312–4317.
[154] Krueger N, Geib C, Piater J, Petrick R, Steedman M, Worgotter F, Ude A, Asfour T, Kraft D, Omrcen
D, Agostini A, Dillmann R. Object-action complexes: Grounded abstractions of sensory-motor processes.
RoboticsandAutonomousSystems.2011;59(10):740–757.
[155] SzedmakS,UgurE,PiaterJ.Knowledgepropagationandrelationlearningforpredictingactioneffects.In:
Ieee/rsjinternationalconferenceonintelligentrobotsandsystems(IROS).2014.p.623–629.
[156] Gonc¸alvesA,AbrantesJ,SaponaroG,JamoneL,BernardinoA.Learningintermediateobjectaffordances:
Towards the development of a tool concept. In: 4th international conference on development and learning
andonepigeneticrobotics.2014.p.482–488.
[157] Dehban A, Jamone L, Kampff AR, Santos-Victor J. Denoising auto-encoders for learning of objects and
tools affordances in continuous space. In: IEEE International Conference on Robotics and Automation
(ICRA).2016.p.4866–4871.
[158] DehbanA,JamoneL,KampffAR,Santos-VictorJ.Adeepprobabilisticframeworkforheterogeneousself-
supervised learning of affordances. In: IEEE-RAS 17th International Conference on Humanoid Robotics
(Humanoids).2017.p.476–483.
[159] Ugur E, Piater J. Emergent structuring of interdependent affordance learning tasks using intrinsic motiva-
tion and empirical feature selection. IEEE Transactions on Cognitive and Developmental Systems. 2017;
9(4):328–340.
[160] Sarathy V, Scheutz M. A logic-based computational framework for inferring cognitive affordances. IEEE
TransactionsonCognitiveandDevelopmentalSystems.2018;10(1):26–43.
[161] Stramandinoli F, Tikhanoff V, Pattacini U, Nori F. Heteroscedastic regression and active learning for
modeling affordances in humanoids. IEEE Transactions on Cognitive and Developmental Systems. 2018;
10(2):455–468.
[162] Tekden AE, Erdem A, Erdem E, Imre M, Seker MY, Ugur E. Belief regulated dual propagation nets for
learningactioneffectsongroupsofarticulatedobjects.In:IEEEInternationalConferenceonRoboticsand
Automation(ICRA).2020.p.10556–10562.
[163] Antunes A, Jamone L, Saponaro G, Bernardino A, Ventura R. From human instructions to robot actions:
Formulationofgoals,affordancesandprobabilisticplanning.In:Ieeeicra.2016.
[164] DehbanA,ZhangS,CauliN,JamoneL,Santos-VictorJ.Learningdeepfeaturesforroboticinferencefrom
physicalinteractions.IEEETransactionsonCognitiveandDevelopmentalSystems.2022;:1–1.
[165] ShenZ,WuY.Investigationofpracticaluseofhumanoidrobotsinelderlycarecentres.In:Proceedingsof
thefourthinternationalconferenceonhumanagentinteraction.2016.p.63–66.
[166] AlbrechtSV,StoneP.Autonomousagentsmodellingotheragents:Acomprehensivesurveyandopenprob-
lems.ArtificialIntelligence.2018;258:66–95.
[167] ElZaatariS,MareiM,LiW,UsmanZ.Cobotprogrammingforcollaborativeindustrialtasks:Anoverview.
RoboticsandAutonomousSystems.2019;116:162–180.
[168] HentoutA,AouacheM,MaoudjA,AkliI.Human–robotinteractioninindustrialcollaborativerobotics:a
literaturereviewofthedecade2008–2017.AdvancedRobotics.2019;33(15-16):764–799.
[169] MagriniE,FerragutiF,RongaAJ,PiniF,DeLucaA,LealiF.Human-robotcoexistenceandinteractionin
openindustrialcells.RoboticsandComputer-IntegratedManufacturing.2020;61:101846.
[170] OgnibeneD,FoulshamT,MarchegianiL,FarinellaGM.Activevisionandperceptioninhuman-robotcol-
laboration.FrontiersinNeurorobotics.2022;16.
[171] SemeraroF,GriffithsA,CangelosiA.Human–robotcollaborationandmachinelearning:Asystematicre-
viewofrecentresearch.RoboticsandComputer-IntegratedManufacturing.2023;79:102432.
[172] OgnibeneD,ChinellatoE,SarabiaM,DemirisY.Contextualactionrecognitionandtargetlocalizationwith
anactiveallocationofattentiononahumanoidrobot.Bioinspiration&biomimetics.2013;8(3):035002.
[173] Fotopoulou A, Tsakiris M. Mentalizing homeostasis: The social origins of interoceptive inference. Neu-
ropsychoanalysis.2017;19(1):3–28.
[174] Veissie`reSP,ConstantA,RamsteadMJ,FristonKJ,KirmayerLJ.Thinkingthroughotherminds:Avaria-
tionalapproachtocognitionandculture.Behavioralandbrainsciences.2020;43.
[175] SaxeR.Uniquelyhumansocialcognition.Currentopinioninneurobiology.2006;16(2):235–239.
[176] BiancoF,OgnibeneD.Functionaladvantagesofanadaptivetheoryofmindforrobotics:areviewofcurrent
architectures.ComputerScienceandElectronicEngineering(CEEC).2019;:139–143.
[177] BakerCL,Jara-EttingerJ,SaxeR,TenenbaumJB.Rationalquantitativeattributionofbeliefs,desiresand
24
January18,2023 AdvancedRobotics main
perceptsinhumanmentalizing.NatureHumanBehaviour.2017;1(4):1–10.
[178] BiancoF,OgnibeneD.Robotlearningtheoryofmindthroughself-observation:Exploitingtheintentions-
beliefssynergy.arXivpreprintarXiv:221009435.2022;.
[179] BianchiF,MarelliM,NicoliP,PalmonariM.Sweat:Scoringpolarizationoftopicsacrossdifferentcorpora.
arXivpreprintarXiv:210907231.2021;.
[180] Ognibene D, Mirante L, Marchegiani L. Proactive intention recognition for joint human-robot search and
rescue missions through monte-carlo planning in POMDP environments. In: International conference on
socialrobotics.Springer.2019.p.332–343.
[181] Heinze C. Modelling intention recognition for intelligent agent systems. DEFENCE SCIENCE AND
TECHNOLOGYORGANISATION.2004.TechRep.Availablefrom:http://www.dsto.defence.
gov.au/corporate/reports/DSTO-RR-0286.pdf.
[182] Roggen D, Calatroni A, Rossi M, Holleczek T, Fo¨rster K, Tro¨ster G, Lukowicz P, Bannach D, Pirkl G,
Ferscha A, et al.. Collecting complex activity datasets in highly rich networked sensor environments. In:
Internationalconferenceonnetworkedsensingsystems(INSS).2010.p.233–240.
[183] ZengM,NguyenLT,YuB,MengshoelOJ,ZhuJ,WuP,ZhangJ.Convolutionalneuralnetworksforhuman
activity recognition using mobile sensors. In: 6th international conference on mobile computing, applica-
tionsandservices.IEEE.2014.p.197–205.
[184] YangJ,NguyenMN,SanPP,LiXL,KrishnaswamyS.Deepconvolutionalneuralnetworksonmultichannel
time series for human activity recognition. In: Twenty-fourth international joint conference on artificial
intelligence.2015.
[185] Lee SU, Hofmann A, Williams B. A model-based human activity recognition for human–robot collabo-
ration. In: IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE. 2019. p.
736–743.
[186] Ram´ırezM,GeffnerH.Planrecognitionasplanning.In:Internationaljointconferenceonartificialintelli-
gence(IJCAI).2009.
[187] BakerCL,SaxeR,TenenbaumJB.Actionunderstandingasinverseplanning.Cognition.2009;113(3):329–
349.
[188] SohrabiS,RiabovAV,UdreaO.Planrecognitionasplanningrevisited.In:Internationaljointconferenceon
artificialintelligence(IJCAI).NewYork,NY.2016.p.3258–3264.
[189] RamirezM,GeffnerH.GoalrecognitionoverPOMDPs:Inferringtheintentionofapomdpagent.In:Inter-
nationaljointconferenceonartificialintelligence(IJCAI).2011.
[190] BakerC,SaxeR,TenenbaumJ.Bayesiantheoryofmind:Modelingjointbelief-desireattribution.In:Pro-
ceedingsoftheannualmeetingofthecognitivesciencesociety.Vol.33.2011.
[191] Shvo M, McIlraith SA. Active goal recognition. In: Proceedings of the AAAI Conference on Artificial
Intelligence.Vol.34.2020.p.9957–9966.
[192] AmatoC,BaiseroA.Activegoalrecognition.arXivpreprintarXiv:190911173.2019;.
[193] DemirisY,SimmonsG.Perceivingtheunusual:Temporalpropertiesofhierarchicalmotorrepresentations
foractionperception.NeuralNetworks.2006;19(3):272–284.
[194] Cardona-Rivera RE, Young RM. Toward combining domain theory and recipes in plan recognition. In:
Workshopsatthethirty-firstaaaiconferenceonartificialintelligence.2017.
[195] ProiettiR,PezzuloG,TessariA.Anactiveinferencemodelofhierarchicalactionunderstanding,learning
andimitation.PsyArXiv.2021;.
[196] Zhi-Xuan T, Mann J, Silver T, Tenenbaum J, Mansinghka V. Online bayesian goal inference for
boundedly rational planning agents. In: Larochelle H, Ranzato M, Hadsell R, Balcan M, Lin H, ed-
itors. Advances in neural information processing systems. Vol. 33. Curran Associates, Inc.. 2020.
p. 19238–19250. Available from: https://proceedings.neurips.cc/paper/2020/file/
df3aebc649f9e3b674eeb790a4da224e-Paper.pdf.
[197] Johnson M, Demiris Y. Perceptual perspective taking and action recognition. International Journal of Ad-
vancedRoboticSystems.2005;2(4):32.
[198] Pandey AK, Ali M, Alami R. Towards a task-aware proactive sociable robot based on multi-state
perspective-taking.InternationalJournalofSocialRobotics.2013;5(2):215–236.
[199] Fischer T, Demiris Y. Computational modeling of embodied visual perspective taking. IEEE Transactions
onCognitiveandDevelopmentalSystems.2019;12(4):723–732.
[200] TorabiF,WarnellG,StoneP.Recentadvancesinimitationlearningfromobservation.arXive-prints.2019;
:arXiv–1905.
[201] Mo¨rtlA,LawitzkyM,KucukyilmazA,SezginM,BasdoganC,HircheS.Theroleofroles:Physicalcooper-
ationbetweenhumansandrobots.TheInternationalJournalofRoboticsResearch.2012;31(13):1656–1674.
25
January18,2023 AdvancedRobotics main
[202] LiY,TeeKP,YanR,ChanWL,WuY.Aframeworkofhuman–robotcoordinationbasedongametheory
andpolicyiteration.IEEETransactionsonRobotics.2016;32(6):1408–1418.
[203] PezzuloG,DonnarummaF,DindoH,D’AusilioA,KonvalinkaI,CastelfranchiC.Thebodytalks:Sensori-
motorcommunicationanditsbrainandkinematicsignatures.Physicsoflifereviews.2019;28:1–21.
[204] Ognibene D, Giglia G, Marchegiani L, Rudrauf D. Implicit perception simplicity and explicit perception
complexityinsensorimotorcomunication.Physicsoflifereviews.2019;28:36–38.
[205] KokBC,SohH.Trustinrobots:Challengesandopportunities.CurrentRoboticsReports.2020;1(4):297–
309.
[206] CraikK.Thenatureofexplanation.Cambridge:CambridgeUniversityPress.1943.
[207] vonHelmholtzH.Handbuchderphysiologischenoptik.Leipzig:L.Voss.1867.
[208] Gregory RL. Knowledge in perception and illusion. Philosophical Transactions of the Royal Society B:
BiologicalSciences.1997;352:1121–1128.
[209] DoyaK,IshiiS,PougetA,RaoRPN,editors.Bayesianbrain:Probabilisticapproachestoneuralcoding.1st
ed.TheMITPress.2007.
[210] ShiffrinRM,BassettDS,KriegeskorteN,TenenbaumJB.Thebrainproducesmindbymodeling.Proceed-
ingsoftheNationalAcademyofSciences.2020;117(47):29299–29301.
[211] MaWJ,BeckJM,LathamPE,PougetA.Bayesianinferencewithprobabilisticpopulationcodes.NatNeu-
rosci.2006;9(11):1432–1438.Availablefrom:http://dx.doi.org/10.1038/nn1790.
[212] BuesingL,BillJ,NesslerB,MaassW.Neuraldynamicsassampling:amodelforstochasticcomputation
inrecurrentnetworksofspikingneurons.PLoSComputBiol.2011;7(11):e1002211.
[213] Sebastian S, Seemiller ES, Geisler WS. Local reliability weighting explains identification of partially
maskedobjectsinnaturalimages.ProceedingsoftheNationalAcademyofSciences.2020;117(47):29363–
29370.
[214] LynnCW,BassettDS.Howhumanslearnandrepresentnetworks.ProceedingsoftheNationalAcademyof
Sciences.2020;117(47):29407–29415.
[215] BerkesP,OrbanG,LengyelM,FiserJ.Spontaneouscorticalactivityrevealshallmarksofanoptimalinternal
modeloftheenvironment.Science.2011;331(6013):83–87.
[216] Ekman M, Kok P, de Lange FP. Time-compressed preplay of anticipated events in human primary visual
cortex.NatureCommunications.2017;8(1):1–9.
[217] DawND,GershmanSJ,SeymourB,DayanP,DolanRJ.Model-basedinfluencesonhumans’choicesand
striatalpredictionerrors.Neuron.2011;69(6):1204–1215.
[218] Go´mez CM, Arjona A, Donnarumma F, Maisto D, Rodr´ıguez Mart´ınez EI, Pezzulo G. Tracking the time
course of bayesian inference with event related potentials: a study using the central cue posner paradigm.
FrontiersinPsychology.2019;10:1424.
[219] Muckli L, De Martino F, Vizioli L, Petro LS, Smith FW, Ugurbil K, Goebel R, Yacoub E. Contextual
feedbacktosuperficiallayersofv1.CurrentBiology.2015;25(20):2690–2695.
[220] WalshKS,McGovernDP,ClarkA,O’ConnellRG.Evaluatingtheneurophysiologicalevidenceforpredic-
tiveprocessingasamodelofperception.AnnalsofthenewYorkAcademyofSciences.2020;1464(1):242–
268.
[221] Bullmore E, Sporns O. Complex brain networks: graph theoretical analysis of structural and functional
systems.Naturereviewsneuroscience.2009;10(3):186–198.
[222] Laird JE, Lebiere C, Rosenbloom PS. A standard model of the mind: Toward a common computational
frameworkacrossartificialintelligence,cognitivescience,neuroscience,androbotics.AIMagazine.2017;
38(4):13–26.
[223] Margulies DS, Ghosh SS, Goulas A, Falkiewicz M, Huntenburg JM, Langs G, Bezgin G, Eickhoff SB,
Castellanos FX, Petrides M, et al.. Situating the default-mode network along a principal gradient of
macroscalecorticalorganization.ProceedingsoftheNationalAcademyofSciences.2016;113(44):12574–
12579.
[224] UngerleiderL,HaxbyJ.“what”and“where”inthehumanbrain.CurrentOpinioninNeurobiology.1994;
4(2):157–65.
[225] TaniguchiT,YamakawaH,NagaiT,DoyaK,SakagamiM,SuzukiM,NakamuraT,TaniguchiA.Awhole
brain probabilistic generative model: Toward realizing cognitive architectures for developmental robots.
NeuralNetworks.2022;150:293–312.
[226] SingerW.Recurrentdynamicsinthecerebralcortex:Integrationofsensoryevidencewithstoredknowledge.
ProceedingsoftheNationalAcademyofSciences.2021;118(33):e2101043118.
[227] PezzuloG,ZorziM,CorbettaM.Thesecretlifeofpredictivebrains:what’sspontaneousactivityfor?Trends
inCognitiveSciences.2021;.
26
January18,2023 AdvancedRobotics main
[228] Gyo¨rgyBuzsa´kiM.Thebrainfrominsideout.OxfordUniversityPress.2019.
[229] PezzuloG,KemereC,vanderMeerM.Internallygeneratedhippocampalsequencesasavantagepointto
probefuture-orientedcognition.AnnalsoftheNewYorkAcademyofSciences.2017;1396:144–165.
[230] NazliI,FerrariA,Huber-HuberC,deLangeFP.Statisticallearningisnoterror-driven.bioRxiv.2022;.
[231] PezzuloG,CisekP.Navigatingtheaffordancelandscape:Feedbackcontrolasaprocessmodelofbehavior
andcognition.TrendsinCognitiveSciences.2016;20(6):414–424.
[232] CisekP.Resynthesizingbehaviorthroughphylogeneticrefinement.Attention,Perception,&Psychophysics.
2019;81(7):2265–2287.
[233] LietoA,BhattM,OltramariA,VernonD.Theroleofcognitivearchitecturesingeneralartificialintelligence.
2018.
[234] KotserubaI,TsotsosJK.40yearsofcognitivearchitectures:corecognitiveabilitiesandpracticalapplica-
tions.ArtificialIntelligenceReview.2020;53(1):17–94.
[235] AndersonJR.Howcanthehumanmindoccurinthephysicaluniverse?OxfordUniversityPress.2009.
[236] Laird JE. Extending the soar cognitive architecture. Frontiers in Artificial Intelligence and Applications.
2008;171:224.
[237] Anderson JR. Human symbol manipulation within an integrated cognitive architecture. In: Cognitive sci-
ence.Routledge.2005.p.313–341.
[238] Puigbo JY, Pumarola A, Te´llez RA. Controlling a general purpose service robot by means of a cognitive
architecture.In:Ceurworkshopproceedings.2013.p.45–55.
[239] MohanS,LairdJE.Learningtoplaymario.TechRepCCA-TR-2009-03.2009;.
[240] Rosenbloom PS, Demski A, Ustun V. The sigma cognitive architecture and system: Towards functionally
elegantgrandunification.JournalofArtificialGeneralIntelligence.2016;7(1):1–103.
[241] DamgaardMR,PedersenR,BakT.Towardanidiomaticframeworkforcognitiverobotics.Patterns.2022;
3(7):100533.
[242] BinghamE,ChenJP,JankowiakM,ObermeyerF,PradhanN,KaraletsosT,SinghR,SzerlipP,HorsfallP,
GoodmanND.Pyro:Deepuniversalprobabilisticprogramming.TheJournalofMachineLearningResearch.
2019;20(1):973–978.
[243] PaigeB,vandeMeentJW,DesmaisonA,GoodmanN,KohliP,WoodF,TorrP,etal..Learningdisentangled
representations with semi-supervised deep generative models. Advances in neural information processing
systems.2017;30.
[244] Tran D, Kucukelbir A, Dieng AB, Rudolph M, Liang D, Blei DM. Edward: A library for probabilistic
modeling,inference,andcriticism.arXivpreprintarXiv:161009787.2016;.
[245] Sandini G, Mohan V, Sciutti A, Morasso P. Social cognition for human-robot symbiosis—challenges and
buildingblocks.Frontiersinneurorobotics.2018;12:34.
[246] SunR.AnatomyoftheMind:ExploringPsychologicalMechanismsandProcesseswiththeClarionCogni-
tiveArchitecture.OxfordUniversityPress.2016.
[247] KahnemanD.Aperspectiveonjudgmentandchoice:mappingboundedrationality.Americanpsychologist.
2003;58(9):697.
[248] Vahrenkamp N, Wa¨chter M, Kro¨hnert M, Welke K, Asfour T. The robot software framework armarx. it-
InformationTechnology.2015;57(2):99–111.
[249] Nakamura T, Nagai T, Taniguchi T. Serket: An architecture for connecting stochastic models to realize a
large-scalecognitivemodel.FrontiersinNeurorobotics.2018;12:1–16.
[250] TaniguchiT,NakamuraT,SuzukiM,KuniyasuR,HayashiK,TaniguchiA,HoriiT,NagaiT.Neuro-serket:
developmentofintegrativecognitivesystemthroughthecompositionofdeepprobabilisticgenerativemod-
els.NewGenerationComputing.2020;:1–26.
[251] DevlinJ,ChangMW,LeeK,ToutanovaK.BERT:Pre-trainingofdeepbidirectionaltransformersforlan-
guageunderstanding.arXivpreprintarXiv:181004805.2019;.
[252] BrownT,MannB,RyderN,SubbiahM,KaplanJD,DhariwalP,NeelakantanA,ShyamP,SastryG,Askell
A,etal..Languagemodelsarefew-shotlearners.Advancesinneuralinformationprocessingsystems.2020;
33:1877–1901.
[253] WangX,WeiJ,SchuurmansD,LeQ,ChiE,ZhouD.Self-consistencyimproveschainofthoughtreasoning
inlanguagemodels.arXivpreprintarXiv:220311171.2022;.
[254] Kojima T, Gu SS, Reid M, Matsuo Y, Iwasawa Y. Large language models are zero-shot reasoners. arXiv
preprintarXiv:220511916.2022;.
[255] AhnM,BrohanA,BrownN,ChebotarY,CortesO,DavidB,FinnC,FuC,GopalakrishnanK,Hausman
K,HerzogA,HoD,HsuJ,IbarzJ,IchterB,IrpanA,JangE,RuanoRJ,JeffreyK,JesmonthS,JoshiN,
JulianR,KalashnikovD,KuangY,LeeKH,LevineS,LuY,LuuL,ParadaC,PastorP,QuiambaoJ,Rao
27
January18,2023 AdvancedRobotics main
K,RettinghouseJ,ReyesD,SermanetP,SieversN,TanC,ToshevA,VanhouckeV,XiaF,XiaoT,XuP,
Xu S, Yan M, Zeng A. Do as i can and not as i say: Grounding language in robotic affordances. In: arxiv
preprintarxiv:2204.01691.2022.
[256] TangiuchiT,MochihashiD,NagaiT,UchidaS,InoueN,KobayashiI,NakamuraT,HagiwaraY,Iwahashi
N,InamuraT.Surveyonfrontiersoflanguageandrobotics.AdvancedRobotics.2019;33(15-16):700–730.
[257] SteelsL.Thesymbolgroundingproblemhasbeensolved,sowhat’snext?In:Symbolsandembodiment:
Debatesonmeaningandcognition.OxfordUniversityPress.2008.p.223–244.
[258] TaniguchiT,UgurE,HoffmannM,JamoneL,NagaiT,RosmanB,MatsukaT,IwahashiN,OztopE,Piater
J,etal..Symbolemergenceincognitivedevelopmentalsystems:asurvey.IEEEtransactionsonCognitive
andDevelopmentalSystems.2018;11(4):494–516.
[259] TaniguchiT,NagaiT,NakamuraT,IwahashiN,OgataT,AsohH.Symbolemergenceinrobotics:Asurvey.
AdvancedRobotics.2016;30(11-12):706–728.
[260] JungM,MatsumotoT,TaniJ.Goal-directedbehaviorundervariationalpredictivecoding:Dynamicorgani-
zationofvisualattentionandworkingmemory.In:IEEE/RSJInternationalConferenceonIntelligentRobots
andSystems(IROS).2019.p.1040–1047.
[261] ItoH,YamamotoK,MoriH,OgataT.Efficientmultitasklearningwithanembodiedpredictivemodelfor
dooropeningandentrywithwhole-bodycontrol.ScienceRobotics.2022;7(65):eaax8177.
[262] HoffmannM,WangS,OutrataV,AlzuetaE,LanillosP.Robotinthemirror:towardanembodiedcomputa-
tionalmodelofmirrorself-recognition.KI-Ku¨nstlicheIntelligenz.2021;35(1):37–51.
[263] JulianiA,ArulkumaranK,SasaiS,KanaiR.Onthelinkbetweenconsciousfunctionandgeneralintelligence
inhumansandmachines.arXivpreprintarXiv:220405133.2022;.
[264] Seth AK. Interoceptive inference, emotion, and the embodied self. Trends in cognitive sciences. 2013;
17(11):565–573.
[265] BauermeisterJ,LanillosP.Theroleofvalenceandmeta-awarenessinmirrorself-recognitionusinghierar-
chicalactiveinference.arXivpreprintarXiv:220813213.2022;.
[266] HieidaC,NagaiT.Surveyandperspectiveonsocialemotionsinrobotics.AdvancedRobotics.2022;36(1-
2):17–32.
[267] ElHafiL,ZhengY,ShirouzuH,NakamuraT,TaniguchiT. Serket-SDE:AContainerizedSoftwareDevelop-
mentEnvironmentfortheSymbolEmergenceinRoboticsToolkit.In:IEEE/SICEInternationalSymposium
onSystemIntegration(SII).2023.
[268] HeinsC,MillidgeB,DemekasD,KleinB,FristonK,CouzinI,TschantzA.pymdp:Apythonlibraryfor
activeinferenceindiscretestatespaces.arXivpreprintarXiv:220103904.2022;.
[269] ReedS,ZolnaK,ParisottoE,ColmenarejoSG,NovikovA,Barth-MaronG,GimenezM,SulskyY,KayJ,
SpringenbergJT,etal..Ageneralistagent.arXivpreprintarXiv:220506175.2022;.
[270] McGeerT,etal..Passivedynamicwalking.IntJRoboticsRes.1990;9(2):62–82.
[271] BrooksRA.Elephantsdon’tplaychess.Roboticsandautonomoussystems.1990;6(1-2):3–15.
[272] PfeiferR,LungarellaM,IidaF.Self-organization,embodiment,andbiologicallyinspiredrobotics.science.
2007;318(5853):1088–1093.
[273] Pfeifer R, Go´mez G. Morphological computation–connecting brain, body, and environment. In: Creating
brain-likeintelligence.Springer.2009.p.66–83.
28

=== REVISE TO ===
PROFESSIONAL TONE: Begin directly with content - NO conversational openings like 'Okay, here's...'

1. Fix all issues above
2. Title: "World Models and Predictive Coding for Cognitive and Developmental Robotics: Frontiers and Challenges"
3. Include 10-15 quotes from paper text
   - Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
   - Use consistent quote formatting: 'The authors state: "quote"' or vary attribution phrases
   - Vary attribution phrases to avoid repetition
   - CRITICAL: Only extract quotes that actually appear in the paper text
4. ELIMINATE ALL REPETITION - each sentence must be unique
   - Check before each sentence: 'Have I already said this?' If yes, write something new
   - Vary attribution phrases - do NOT repeat 'The authors state' multiple times
5. Extract methodology, results with numbers, key quotes
6. 1000-1500 words, structured with ### headers

Generate COMPLETE revised summary.