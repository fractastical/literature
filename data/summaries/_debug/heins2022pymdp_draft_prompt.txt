=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: pymdp: A Python library for active inference in discrete state spaces
Citation Key: heins2022pymdp
Authors: Conor Heins, Beren Millidge, Daphne Demekas

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2022

Abstract: pymdp offers a suite of robust, tested, and modular routines for simulating active inference
agents equipped with partially observable Markov Decision Process (POMDP) generative
models. Mathematically, a POMDP comprises a joint distribution over observations o, hid-
den states s, control states u and hyperparameters φ: P(o,s,u,φ). This joint distribution
further factorizes into a set of categorical and Dirichlet distributions: the likelihoods and
priors of the generative model. With pymdp, one c...

Key Terms: konstanz, discrete, institute, collective, state, university, spaces, london, behavior, library

=== FULL PAPER TEXT ===

pymdp: A Python library for active inference
in discrete state spaces
Conor Heins∗1,2,3,4, Beren Millidge4,5, Daphne Demekas6, Brennan Klein4,7,8,
Karl Friston9, Iain D. Couzin1,2,3, and Alexander Tschantz∗4,10,11
1Department of Collective Behaviour, Max Planck Institute of Animal Behavior,
78457 Konstanz, Germany
2Centre for the Advanced Study of Collective Behaviour, 78457 Konstanz, Germany
3Department of Biology, University of Konstanz, 78457 Konstanz, Germany
4VERSES Research Lab, Los Angeles, California, USA
5MRC Brain Networks Dynamics Unit, University of Oxford, Oxford, UK
6Department of Computing, Imperial College London, London, UK
7Network Science Institute, Northeastern University, Boston, MA, USA
8Laboratory for the Modeling of Biological and Socio-Technical Systems,
Northeastern University, Boston, USA
9Wellcome Centre for Human Neuroimaging, Queen Square Institute of Neurology,
University College London, London WC1N 3AR, UK
10Sussex AI Group, Department of Informatics, University of Sussex, Brighton, UK
11Sackler Centre for Consciousness Science, University of Sussex, Brighton, UK
May 6, 2022
Statement of need
Active inference is an account of cognition and behavior in complex systems which brings
together action, perception, and learning under the theoretical mantle of Bayesian inference
[1, 2, 3, 4]. Active inference has seen growing applications in academic research, especially
in fields that seek to model human or animal behavior [5, 6, 7]. The majority of applications
have focused on cognitive neuroscience, with a particular focus on modelling decision-making
under uncertainty. Nonetheless, the framework has broad applicability and has recently been
applied to diverse disciplines, ranging from computational models of psychopathology [8, 9,
10, 11], control theory [12, 13, 14] and reinforcement learning [15, 16, 17, 18, 19], through
to social cognition [7, 20, 21, 22] and even real-world engineering problems [23, 24, 25].
While in recent years, some of the code arising from the active inference literature has been
∗Correspondence: conor.heins@gmail.com; tschantz.alec@gmail.com
1
2202
yaM
4
]IA.sc[
2v40930.1022:viXra
written in open source languages like Python and Julia [26, 27, 28, 29, 19], to-date, the most
popular software for simulating active inference agents is the DEM toolbox of SPM [30, 31].
SPM is a MATLAB library originally developed for the statistical analysis and modelling of
neuroimaging data, such as data collected by functional magnetic resonance imaging (fMRI)
or magneto- and electro-encephalographic (MEG/EEG) [32] methods. The DEM toolbox, a
sub-library of SPM, was originally developed to simulate and perform Bayesian estimation
of dynamical systems [30], but in the last decade it has been augmented with a series of
demonstrativescripts and simulationroutines related toactive inferenceand the Free Energy
Principle more broadly [33, 34, 35].
Simulations of active inference are commonly performed in discrete time and space [36,
3]. This is partially motivated by the mathematical tractability of performing inference with
discrete probability distributions, but also by the intuition of modelling choice behavior as
a sequence of discrete, mutually-exclusive choices, in e.g. psychophysics or decision-making
experiments. The most popular generative models – used to realize active inference in this
context–arepartially-observableMarkovDecisionProcessesorPOMDPs [37]. POMDPsare
state-space models that model the environment in terms of hidden states that stochastically
change over time, as a function of both the current state of the environment as well as
the behavioral output of an agent (control states or actions). Crucially, the environment is
partially-observable, i.e. the hidden states are not directly observed by the agent, but can
only be inferred through observations that relate to hidden states in a probabilistic manner,
suchthatobservationsaremodelledasbeinggeneratedstochasticallyfromthecurrenthidden
state.
In most POMDP problems, an agent is tasked with both inferring the hidden states
and selecting a sequence of control states or actions to change the hidden states in a way
that leads to desired outcomes (maximizing reward, or occupancy within some preferred
set of states). DEM contains a reliable, reproducible set of functions for simulating ac-
tive inference agents equipped with such generative models: they include spm_MDP_VB_X.m,
spm_MDP_game.m, and – recently introduced for simulating ‘sophisticated’ active inference –
spm_MDP_VB_XX.m [38]. Despite its robustness and widespread use among active inference
researchers, spm_MDP_VB_X.m is a single function, meaning that any active inference simu-
lation using the function has to comply with the constraints implied by its structure and
control flow. Although options can be specified that initiate particular sub-routines or vari-
ants of active inference, it is still not straightforward to construct a custom active inference
process from scratch. In practice, this means that novel, bespoke applications require re-
searchers to manually adapt parts of spm_MDP_VB_X.m for their own purposes, which limits
the general reproducibility and adaptability of academic active inference research, especially
for new practitioners. In addition, since all of DEM is written in MATLAB, using the tool-
box can be prohibitive due to the cost of a MATLAB license, especially for researchers who
are unaffiliated with institutions. Increasing interest in active inference, manifested both in
terms of sheer number as well as diversifying applications across scientific disciplines, has
thus created a need for generic, widely-available, and user-friendly code for simulating ac-
tive inference in open-source scientific computing languages like Python. The software we
2
present here, pymdp, represents a significant step in this direction: namely, we provide the
first open-source package for simulating active inference with discrete state-space generative
models. ThenamepymdpderivesfromthefactthatthepackageiswritteninthePythonpro-
gramming language and concerns discrete, Markovian generative models of decision-making,
which take the form of Markov Decision Processes or MDPs.
We developed pymdp to increase the accessibility and exposure of the active inference
framework to researchers, engineers, and developers with diverse disciplinary backgrounds.
Inthespiritofopen-sourcesoftware, wealsohopethatitspursnewinnovation, development,
and collaboration in the growing active inference community.
Summary
pymdp offers a suite of robust, tested, and modular routines for simulating active inference
agents equipped with partially observable Markov Decision Process (POMDP) generative
models. Mathematically, a POMDP comprises a joint distribution over observations o, hid-
den states s, control states u and hyperparameters φ: P(o,s,u,φ). This joint distribution
further factorizes into a set of categorical and Dirichlet distributions: the likelihoods and
priors of the generative model. With pymdp, one can build a generative model using a set
of prior and likelihood distributions, initialize an agent, and then link it to an external
environment to run active inference processes - all in a few lines of code. The agent and en-
vironment API is built according to the standardized framework of OpenAIGym commonly
used in reinforcement learning, where an Agent and Environment class recursively exchange
observations and actions over time [39].
In order to enhance the user-friendliness of pymdp without sacrificing flexibility, we have
built the library to be highly modular and customizable, such that agents in pymdp can be
specified at a variety of levels of abstraction with desired parameterizations. In the next
section, we provide an overview of the structure of the package.
Package structure
The Agent Class
The high-level API offered by pymdp is the Agent class. Instantiating an Agent allows the
user to abstract away the various optimization routines and sub-operations that make up an
active inference process, e.g. state estimation, action selection, and learning. The various
sub-routines of active inference are themselves abstracted as user-friendly methods of Agent
(such as self.infer_states(obs)), calls to which will run the corresponding function.
Modules
The methods of Agent themselves call functions from different sub-modules of pymdp. These
submodules can be roughly divided into three sorts of operations: perception, action and
learning. Anattractivefeatureofactiveinferenceisthatvariouscognitiveprocessesnaturally
3
emerge as different variants of Bayesian inference. For instance, instantaneous inference
about dynamically-changing hidden states is often analogized to perception (c.f. perception
as inference [40, 41, 42, 43]), whereas inference about slower-changing variables (statistical
regularities in the environment) is analogized to learning [44]. Moreover, action is treated
as a process of inference, where agents select actions by inferring a distribution over control
states or sequence of control states [4]. Each module of pymdp thus performs inference with
respect to different components of a POMDP generative model – we summarize them briefly
below.
The inference library of pymdp contains a set of functions for performing hidden state
inference or state estimation. These are the core functions that allow agents to update their
beliefs about the discrete hidden state of the environment, given observations. Functions
from this library are called by the self.infer_states() method of Agent. Specific argu-
ments can be passed into the Agent constructor to specify the type and parameterization of
the algorithm used to perform hidden state inference.
The control library of pymdp contains functions for inferring policies and sampling
actions from the posterior beliefs about control states.1 These functions are called internally
by the self.infer_policies() method of Agent.
Finally, the learning library of pymdp contains the functions necessary for the agent to
updatehyperparametersofitsgenerativemodel,i.e. Dirichletparametersoverthecategorical
prior and likelihood distributions. These functions are called internally by methods like
self.update_A(), self.update_B(), and self.update_D() of Agent.
For a more detailed overview of the functionality offered by each of pymdp’s modules,
please see Appendix B: Modules and Theory.
1 from pymdp.agent import Agent
2
3 # here you would set up your generative model
4 my A = ...
5 my_B = ...
6
7 # instantiate your agent with a call to the Agent() constructor
8 my_agent = Agent(A=my_A, B=my_B, C=my_C, D=my_D)
9
10 # define an environment
11 my_env = Env()
12
13 # set an initial action
14 action = initial_action
15
16 for t in range(T):
17 o_t = my_env.step(action)
18 my_agent.infer_states(o_t)
19 my_agent.infer_policies()
1In active inference ‘control states’ refer to the random variables in the generative model and approxi-
mate posterior, and can be thought of as the agent’s representation of its actions. Actions themselves are
realizations of these random variables, sampled from the posterior over control states.
4
20 action = my_agent.sample_action()
Example 1: Minimal example of running active inference with pymdp.
Usage
Specifying a generative model is central to active inference, and to Bayesian modelling in
general. Intuitively, a generative model is a probabilistic specification of how data or sensory
observations are generated. In the context of Bayesian agent-based models, the generative
model represents an agent’s probabilistic internal model of its environment, comprising a
set of structural assumptions about how the world generates observations and how action
changes the world. In discrete-state and -time active inference models, we typically assume
the generative model is a partially observable Markov Decision Process or POMDP, compris-
ing observations the agent receives, hidden states of the world, and actions the agent can
take to influence hidden states. Hidden states are called ‘hidden’ precisely because the agent
can never directly access them, but can only infer them via observations. The POMDP
structure assumes that at each timestep, the observation is generated by the current hidden
state, while the hidden state itself changes over time as a function of its current setting and
some control state (i.e. action). Mathematically, a generative model is usually expressed
as a joint probability distribution P(o,s,u,φ) over observations o, hidden states s, control
states u, and parameters φ. This joint distribution is called a generative model because it
can be used to sample (or generate) sequences of potential observations according to the
probabilistic structure encoded in the model.
Specifying the generative model in terms of discrete probability distributions is the first
step to building an active inference agent in pymdp. Below, we overview the steps involved
in building a generative model to provide intuition and illustrate its simplicity in the special
case of POMDPs.
The POMDP generative model
The POMDP generative model assumed by pymdp is a discrete-time and space generative
model that, like any probability distribution, can be factorized into a product of conditional
distributions (likelihoods) and marginals (priors). The most important of these distributions
–whenwritingdownagenerativemodelinpymdp–are1)theobservationlikelihoodP(o |s ),
τ τ
which represents the agent’s beliefs about how hidden states s generate observations o and 2)
the transition model P(s |s ,u ), which represents the agent’s beliefs about how hidden
τ τ τ−1
states at some time τ − 1 cause hidden states at the next time τ, conditioned on some
control states (actions) u . The agent also has a prior over initial hidden states P(s ),
τ−1 1
which represents the agent’s baseline belief, before gathering any observations, about the
probability of the different hidden states at the first timestep. For the sake of mathematical
convenience, we describe POMDP generative models with a finite time horizon T, but note
that in general pymdp does not require a finite time horizon, so active inference agents can be
5
theoretically run indefinitely (e.g. in streaming applications). Finally, there is an additional
prior distribution over observations, P(o ), which specifies an agent’s goals as a desired
1:T
distribution over observations. In active inference, goals and desires are encoded as a prior in
the generative model, such that the probability that the model assigns to some configuration
of observations, hidden states, control states and prior parameters is only maximized when
sampling preferred observations (e.g. if my model assigns high probability to observing body
temperature to be 37 degrees, prior preferences are realized when these observations are
sampled). As we will see in the following sections, active inference suggests that perception,
action and learning all work to maximize the marginal likelihood of observations. Prior
preferences only come into play during policy selection because only unobserved outcomes in
the future are random variables (see the section on control.py in Appendix B for details).
The local dependence in time, captured by one-step conditional dependence between
hidden states in the transition likelihood, is what renders POMDPs Markovian generative
models. As such, a general expression for the joint distribution over o, s, π and φ is as
follows:
T T
(cid:89) (cid:89)
P(o ,s ,π,φ) = P(φ)P(s )P(π) P(s |s ,π;φ) P(o |s ;φ) (1)
1:T 1:T 1 τ τ−1 τ τ
τ=2 τ=1
We have replaced u here with π to represent policies, or sequences of control states u, i.e.
π = {u ,u ,u ,...}. Control states can be formally related to policies by writing down an
i j k
additional likelihood, a ‘policy-to-control’ mapping P(u |π), that links a given policy to the
τ
control state it entails at time τ. Under active inference, agents also perform inference about
policies, which naturally entails goal-directed and uncertainty-resolving behavior (see the
section The Expected Free Energy in Appendix B for details on policy inference). Finally,
we capture any additional parameters as a single vector of hyperparameters φ, which might
correspond to the parameters of Dirichlet priors over the likelihood distributions P(o |s )
τ τ
and P(s |s ,π), for instance. In active inference, inference about these hyperparameters is
τ τ−1
often assumed to occur on a slower timescale than inference about hidden states and policies:
therefore, this process is referred to in the active inference literature as learning [44, 45] (see
learning.py for more details on hyperparameter inference).
Equipped with a generative model, active inference (and Bayesian inference more gener-
ally) entails inference over latent variables s, π and φ, given some observations o gathered
t
over time. For a description of the mathematical basis of this inference – and how it is im-
plemented algorithmically in pymdp – please refer to Appendix B: Modules and Theory. Now
that we have expressed the POMDP generative model formally, highlighting its important
components for active inference, we can move on to the representation of these distributions
in pymdp.
Building blocks
pymdp considers generative models of discrete states that evolve in discrete time. This means
that there are an integer number of discrete levels of both the states of the environment and
6
of the observations. Because of this fundamental discreteness, a natural way to represent
the distributions of the generative model is by using categorical distributions, which assign
a probability value between 0 and 1 to each discrete outcome level of the distribution’s
sample space, with the usual constraint that the sum of the probabilities over levels is 1.
Mathematically, we refer to categorical distributions with the notation P(x) = Cat(φ),φ ∈
{z ∈ Rn | z > 0, (cid:80) z = 1}. This means that the distribution over the random variable
i i i
x is described by a categorical distribution with an n-dimensional vector of parameters
φ, where n is the cardinality of the sample space of X. Numerically, these categorical
distributions can be represented as multidimensional arrays (also known as NDarrays or
tensors) that contain their parameters. These categorical distributions come in two flavors:
vector-valued marginal distributions (e.g. P(x)) usually playing the role of priors, and
conditional categorical distributions (e.g. P(y|x)) in the form of NDarrays (matrices and
tensors), playing the role of likelihoods in the generative model.
Marginal categorical distributions are encoded in pymdp as simple 1-D vectors, which
technically are instances of numpy.ndarrays, the core data structure for representing multi-
dimensional arrays in the Python array programming library, numpy [46]. One can easily
instantiate categorical distributions in numpy using calls to the array constructor, e.g.
prior_over_states = np.array([0.5, 0.5]). Conditional categorical distributions are
just collections of 1-D categorical vectors, with as many 1-D vectors as there are levels of the
conditioning variable. In pymdp, we encode these collections as matrices (2-D NDarrays) and
higher-order NDarrays. For instance, we would encode some discreteconditional distribution
relating two categorical variables P(y|x) as a matrix of size N ×M, where N is the number
of levels of the support random variable y and M is the number of levels of the conditioning
random variable x. We represent such conditional categorical distributions mathematically
with the notation P(y | x) = Cat(φ),φ ∈ Rn×m×p×..., where now φ is a matrix or tensor
of parameters, whose columns φ have the properties of a single categorical distribution,
•jkl...
i.e. φ ∈ {z ∈ Rm | z > 0, (cid:80) z = 1}.
•jkl... i i i
The core distributions of the generative model – that are encoded in this way – are
the observation likelihood P(o |s ) (also known as the observation model or the sensory
τ τ
likelihood) and the transition likelihood P(s |s ,u ) (also known as the transition model
τ τ−1 τ−1
or the dynamics model). For all the following descriptions, we borrow notation from the SPM
and DEM standards, which are summarized in papers like [3, 4, 31, 47]. In pymdp notation,
the observation likelihood is constructed as the A array. In simple generative models2, A will
be a O ×S matrix, where O is the number of outcomes or levels of the observations o, and
S is the number of levels of hidden states s. The entry A[i,j] encodes the probability of
seeing observation i given state j. In other words, each column of this matrix A[:,j] stores
a vector of categorical parameters that encodes the distribution P(o |s = s ). Similarly,
τ τ j
the transition likelihood is represented by the B array which is a S × S × U NDarray or
tensor, where U is the number of levels of the control states, and entry B[i,j,k] encodes
the probability transitioning to state i at time t from state j at time t − 1, when control
state or action k is taken by the agent. The general structure of numerical representations of
2See the section on Factorized representations for the more general form.
7
conditional distributions in pymdp can be expressed as follows: the first dimensions (or rows)
of the matrix or NDarray represent the support of the conditional distribution, while the
lagging dimensions (columns, slices, etc.) represent the random variables being conditioned
on. Thus, for the observation likelihood P(o |s ), the first dimension of A represents the
τ τ
support of o (and will have length O) while the second dimension represents the support of
s (and will have length S).
Beyond the A and B arrays, one can also specify an initial prior over states P(s ) – in
1
pymdp this is called the D vector (of length S) and represents the agent’s beliefs about the
distribution over hidden states at the first timestep of the time horizon (when τ = 1).
Finally, in order to achieve goal-directed behavior under active inference, it is necessary
to build a representation of some desired state or goal into the generative model. In rein-
forcement learning this is handled using reward functions but in active inference we instead
specify a prior distribution over observations, also known as the ‘prior preferences’ or ‘goal
distribution’ [1]. Inference over control states is then biased by this preference distribution,
leading agents to choose actions that bring them to states that (they expect) will lead to
preferred observations. In pymdp, this is represented by the C array of length O. The default
C array is a vector that is time-independent (the same C is used for all timesteps), but it is
also possible to specify a time-dependent C array. This can be used to represent goals that
change over time or the desire to reach a specific goal in a time-dependent manner .
After the generative model has been specified in terms of a set of likelihood and prior
distributions, one can build an active inference agent in a single line using the Agent()
constructor: e.g. my_agent = Agent(A=A, B=B, ...). The Agent() constructor requires
A and B arrays as mandatory input, while C and D vectors can be optionally included (the
defaults are uniform distributions for each).
The various methods of the resulting Agent instance can then be used to perform active
inference.
1 import numpy as np
2
3 import pymdp
4 from pymdp import utils, maths
5 from pymdp.agent import Agent
6
7 # create a simple model with one hidden state factor, and one
observation modality
8
9 n_obs = 3
10 n_states = 3
11
12 A = utils.obj_array(1)
13 A[0] = np.array([[1.0, 0.0, 0.0],
14 [0.0, 1.0, 0.0],
15 [0.0, 0.0, 1.0]])
16
17 # introduce uncertainty into one of the hidden states
18 inv_temperature = 0.5
19 A[0][:,2] = maths.softmax(inv_temperature* A[0][:,2])
8
20
21 # create a simple transition model with two possible actions
22
23 B = utils.obj_array(1)
24 B[0] = np.zeros((3, 3, 2))
25
26 # first action leads to first two states with uncertainty
27 B[0][:,:,0] = np.array([[0.5, 0.5, 0.5],
28 [0.5, 0.5, 0.5],
29 [0.0, 0.0, 0.0]])
30
31 # second action leads to last state with certainty
32 B[0][:,:,1] = np.array([[0.0, 0.0, 0.0],
33 [0.0, 0.0, 0.0],
34 [1.0, 1.0, 1.0]])
35
36 # specify prior preferences (C vector)
37 C = utils.obj_array_uniform([n_obs])
38
39 # specify prior over hidden states (D vector)
40 D = utils.obj_array(1)
41 D[0] = utils.onehot(1, n_states)
42
43 # instantiate your agent with a call to the ‘Agent()‘ constructor
44 my_agent = Agent(A=A, B=B, C=C, D=D)
45
46 # write a simple environment class, where state depends on the action
probabilistically, and observation is deterministic function of the
state except for state 2, where it’s randomly sampled
47
48 from pymdp.envs import Env
49
50 # sub-class it from the base Env class
51 class custom_env(Env):
52
53 def __init__(self):
54 self.state = 0
55
56 def step(self, action):
57
58 if action == 0:
59 self.state = 0 if np.random.rand() > 0.5 else 1
60 if action == 1:
61 self.state = 2
62
63 if self.state == 0:
64 obs = 0
65 elif self.state == 1:
66 obs = 1
67 elif self.state == 2:
68 obs = np.random.randint(3)
9
69
70 return obs
71
72 env = custom_env()
73
74 action = 0
75
76 T = 10 # length of active inference loop in time
77 for t in range(T):
78
79 # sample an observation from the environment
80 o_t = env.step(action)
81
82 # do active inference
83 qs = my_agent.infer_states([o_t]) # get posterior over hidden
states
84 my_agent.infer_policies()
85 action = my_agent.sample_action()
86
87 # convert action into int, for use with environment
88 action = int(action.squeeze())
Example 2: Detailed example of building and running an active inference process in pymdp.
Specifying an environment
For most use-cases of active inference, the agent will need to interface with some kind of
environment or external world. The minimal definition of an environment is just a class
or function that takes actions of the agent as input, updates the true hidden state of the
environment (but does not convey this information to the agent) and returns observations
generated by the updated hidden state. In the Bayesian modelling literature, this environ-
ment is also abstractly referred to as the ‘generative process’ or ‘data-generating process’.
What is important to note is that this generative process does not have to be identical to
the generative model – i.e. there is no requirement that an active inference agent with a
POMDP generative model is operating in a world with discrete, POMDP-like dynamics [48,
28]. All that matters is that the environment accepts the agent’s actions and returns obser-
vations that are discrete and are compatible with the support of the likelihood P(o |s ) of
τ τ
the agent’s generative model.
pymdp contains a library of pre-built environments which can be imported using from
pymdp import envs. Following the convention of OpenAI Gym [39], users can also write
their own environment class. This class is traditionally written to have a step() method
whichtakes anaction fromthe agentas inputand returns observations thatwill be processed
by the agent at the next timestep. In many reinforcement learning and control problem
contexts, the environment has its own internal state that is updated by the agent’s action,
and which determines (either stochastically or deterministically) the next observation.
10
Closing the action-perception loop
The typical ‘active inference loop’ consists of three main steps: 1) sampling an observation
from the environment; 2) updating the agent’s beliefs about states and policies using the
observation; and 3) choosing an action, based on the agent’s posterior over policies (see
Appendix B: Modules and Theory for more details on state and policy inference). In pymdp,
1) is implemented by calling the environment class env.step(); 2) is implemented using
the pymdp functions agent.infer_states() and agent.infer_policies() and 3) is im-
plemented using the pymdp function agent.sample_action(). Wrapping these three steps
into a loop over time entails the entire active inference process; see the full example using
the Agent class in Example 2.
Factorized representations
Although many simple POMDPs can be constructed with simple 2-D A matrices and 3-D B
arrays, most of the interesting applications of active inference require what are referred to as
‘factorized representations’. This requires building additional structure into the generative
model, such that observations o are divided into separate modalities and hidden states s into
separate factors. A multi-modality observation o and multi-factor hidden state s can be
t t
expressed as follows:
(cid:8) (cid:9) (cid:8) (cid:9)
o = o1,o2,...,oM s = s1,s2,...,sF
t t t t t t t t
where here the superscript refers to the index of the mth observation modality or fth hidden
state factor, respectively. This means that at any given time the agent receives a collection
of discrete observations, where each observation within the collection belongs to a distinct
‘modality’. Thenamemodalityisusedtoemphasizetheanalogytodifferentsensorychannels
(e.g. vision, audition, somatosensation) in biology that relay different sorts of information.
Likewise, in a factorized hidden state representation, the environment’s structure is repre-
sented through several hidden state factors, that may encode distinct features of the world,
each of which may have its own dimensionality, dynamics, and relationship to observations.
Importantly, with such factorized representations, the likelihood arrays A and B become
more complex. In both pymdp and SPM, we encode a multi-modality A array as a collection
of sub-arrays A[m], with one for each observation modality. Each modality-specific A array
thenrepresentstheconditionalprobabilityofobservationsformodalitym, giventhedifferent
configuration of hidden states, i.e., P(o |s). Note that the ‘first’ index of the larger A ar-
m
ray selects a particular modality from the collection, e.g. A_modality = A[m], whereas the
subsequent multi-index into the modality-specific A array selects conditional probabilities or
arrays of such probabilities, e.g., A_modality[0, 2, 3, ...]. Each A[m] thus encodes all
probabilistic dependencies between the different hidden state factors hidden states and ob-
servations for the mth modality: P(om|s) = P(om|s1,s2,...,sF). These complex conditional
relationships are encoded by accordingly higher-dimensional NDarrays in NumPy, with the
number of lagging dimensions encoding the number of hidden state factors that the obser-
vations depend on. Such factorized generative models require more involved belief updating
11
algorithms to achieve posterior inference, usually invoking a factorized approximate poste-
rior (e.g. a mean-field factorization), where the full posterior over all hidden state factors is
factorized into a product of marginals Q(s) = (cid:81)F Q(si), where each Q(si) is the posterior
i=1
for hidden state factor i. Fortunately, pymdp easily accommodates such higher-dimensional,
factorized generative models and will automatically perform message passing with respect to
such generative models with an arbitrary number of observation modalities and hidden state
factors. See Appendix A: Factorized Generative Models for more details on multi-factor
generative models.
Pedagogical materials & code
For more example code detailing how to use pymdp to simulate active inference in discrete
state-space environments, we refer the reader to the tutorials found in the official documen-
tation for the repository: https://pymdp-rtd.readthedocs.io/.
Customizability
pymdp offers a high degree of customizability in designing bespoke active inference processes,
such that the methods of the Agent class can be called in any particular order, depending
on the application, and furthermore they can be specified with various keyword arguments
that entail choices of implementation details at lower levels.
For instance, if one wanted to model a purely ‘perceptual’ task, i.e., where the agent
has no ability to act, but is only concerned with hidden state estimation, then one could
write an active inference loop where the Agent class only uses the infer_states() func-
tion. This offers an advantage over the main function used to perform active inference in
SPM, spm_MDP_VB_X.m where customization applications are limited and in practice are im-
plemented by modifying parts of the function by hand to suit one’s needs (e.g. commenting
out certain sections or adding in bespoke computations).
Moreover, by retaining a modular structure throughout the package’s dependency hier-
archy, pymdp also affords the ability to flexibly compose different low level functions. This
allows users to customize and integrate their active inference loops with desired inference
algorithms and policy selection routines. For instance, one could sub-class the Agent class
and write a customized step() function, that combines whichever components of active
inference one is interested in.
Related software packages
The DEM toolbox within SPM in MATLAB is the current gold-standard in active inference
modelling. In particular, simulating an active inference process in DEM consists of defining
the generative model in terms of a fixed set of matrices and vectors, and then calling the
spm_MDP_VB_X.m function to simulate a sequence of trials. pymdp, by contrast, provides a
12
user-friendly and modular development experience, with core functionality split up into dif-
ferent libraries that separately perform the computations of active inference in a standalone
fashion. Moreover, pymdp provides the user the ability to write an active inference process
at different levels of abstraction depending on the user’s level of expertise or skill with the
package – ranging from the high level Agent functionality, which allows the user to define
and simulate an active inference agent in just a few lines of code, all the way to specifying
a particular variational inference algorithm (e.g. marginal-message passing) for the agent to
use during state estimation. In SPM, this would require setting undocumented flags or else
manually editing the routines in spm_MDP_VB_X.m to enable or disable bespoke functionality.
pymdp has extensive, organized documentation and illustrative examples. While the DEM
toolbox is also replete with interesting examples that result in beautiful visualizations of
simulated behavior and synthetic neural responses, the available usage information for each
function remains limited to doc-strings in the source code. The closest to documentation
or an instruction manual for spm_MDP_VB_X.m is the comprehensive tutorial by Smith et
al. 2021 [31], which features a series of MATLAB tutorial scripts that walk through the
different aspects of active inference, with a focus on applications to modelling (behavioral
and neurophysiological) empirical data.
A recent related, but largely non-overlapping project is ForneyLab, which provides a
set of Julia libraries for performing approximate Bayesian inference via message passing
on Forney Factor Graphs [49]. Notably, this package has also seen several applications
in simulating active inference processes, using ForneyLab as the backend for the inference
algorithms employed by an active inference agent [27, 50, 51, 52]. While ForneyLab focuses
on including a rigorous set of message passing routines that can be used to simulate active
inference agents, pymdp is specifically designed to help users quickly build agents (regardless
of their underlying inference routines) and plug them into arbitrary environments to run
active inference in a few easy steps.
Funding Statement CH and IDC acknowledge support from the Office of Naval Research
grant(ONR,N00014-64019-1-2556),withIDCfurtheracknowledgingsupportfromtheEuro-
peanUnion’sHorizon2020researchandinnovationprogrammeundertheMarieSkłodowska-
Curie grant agreement (ID: 860949), the Deutsche Forschungsgemeinschaft (DFG, German
Research Foundation) under Germany’s Excellence Strategy-EXC 2117- 422037984, and the
Max Planck Society. KF is supported by funding for the Wellcome Centre for Human Neu-
roimaging (Ref: 205103/Z/16/Z) and the Canada-UK Artificial Intelligence Initiative (Ref:
ES/T01279X/1). CH, DD, and BK acknowledge the support of a grant from the John
Templeton Foundation (61780). The opinions expressed in this publication are those of the
author(s) and do not necessarily reflect the views of the John Templeton Foundation.
Acknowledgements The authors would like to thank Dimitrije Markovic, Arun Niranjan,
Sivan Altinakar, Mahault Albarracin, Alex Kiefer, Magnus Koudahl, Ryan Smith, Casper
Hesp, and Maxwell Ramstead for discussions and feedback that contributed to development
of pymdp. We would also like to thank Thomas Parr for pointing out a technical error
13
in an earlier version of the paper. Finally, we are grateful to the many users of pymdp
whose feedback and usage of the package have contributed to its continued improvement
and development.
References
[1] Karl J. Friston, Jean Daunizeau, and Stefan J. Kiebel. “Reinforcement learning or
active inference?” In: PLoS ONE 4.7 (2009), e6421. doi: 10.1371/journal.pone.
0006421.
[2] Karl J. Friston, Spyridon Samothrakis, and Read Montague. “Active inference and
agency: Optimal control without cost functions”. In: Biological Cybernetics 106.8-9
(2012), pp. 523–541. doi: 10.1007/s00422-012-0512-8.
[3] KarlJ.Friston,FrancescoRigoli,DimitriOgnibene,ChristophMathys,ThomasFitzger-
ald, and Giovanni Pezzulo. “Active inference and epistemic value”. In: Cognitive Neu-
roscience 6.4 (2015), pp. 187–214. doi: 10.1080/17588928.2015.1020053.
[4] Karl J. Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, and
Giovanni Pezzulo. “Active inference: A process theory”. In: Neural Computation 29.1
(2017), pp. 1–49. doi: 10.1162/NECO_a_00912.
[5] ThomasParr,RajeevVijayRikhye,MichaelMHalassa,andKarlJFriston.“Prefrontal
computation as active inference”. In: Cerebral Cortex 30.2 (2020), pp. 682–695.
[6] Emma Holmes, Thomas Parr, Timothy D Griffiths, and Karl J Friston. “Active infer-
ence,selectiveattention,andthecocktailpartyproblem”.In:Neuroscience & Biobehav-
ioral Reviews 131 (2021), pp. 1288–1304. doi: 10.1016/j.neubiorev.2021.09.038.
[7] Rick A Adams, Peter Vincent, David Benrimoh, Karl J Friston, and Thomas Parr.
“Everything is connected: Inference and attractors in delusions”. In: Schizophrenia
research (2021). doi: 10.1016/j.schres.2021.07.032.
[8] P. Read Montague, Raymond J. Dolan, Karl J. Friston, and Peter Dayan. “Compu-
tational psychiatry”. In: Trends in Cognitive Sciences 16.1 (2012), pp. 72–80. doi:
10.1016/j.tics.2011.11.018.
[9] Philipp Schwartenbeck, Thomas FitzGerald, Christoph Mathys, Ray Dolan, and Karl
J. Friston. “The dopaminergic midbrain encodes the expected certainty about desired
outcomes”. In: Cerebral Cortex 25.10 (2015), pp. 3434–3445. doi: 10.1093/cercor/
bhu159.
[10] Ryan Smith, Philipp Schwartenbeck, Jennifer L. Stewart, Rayus Kuplicki, Hamed
Ekhtiari, Martin P. Paulus, and Tulsa 1000 Investigators. “Imprecise action selection
in substance use disorder: Evidence for active learning impairments when solving the
explore-exploitdilemma”.In:Drug and Alcohol Dependence 215(2020),p.108208.doi:
10.1016/j.drugalcdep.2020.108208.
14
[11] Ryan Smith, Namik Kirlic, Jennifer L. Stewart, James Touthang, Rayus Kuplicki,
Sahib S. Khalsa, Justin Feinstein, Martin P. Paulus, and Robin L. Aupperle. “Greater
decision uncertainty characterizes a transdiagnostic patient sample during approach-
avoidance conflict: A computational modelling approach”. In: Journal of Psychiatry &
Neuroscience 46.1 (2021), E74. doi: 10.1503/jpn.200032.
[12] Manuel Baltieri and Christopher L. Buckley. “PID control as a process of active infer-
ence with linear generative models”. In: Entropy 21.3 (2019), p. 257. doi: 10.3390/
e21030257.
[13] Beren Millidge, Alexander Tschantz, Anil K. Seth, and Christopher L. Buckley. “On
the relationship between active inference and control as inference”. In: International
Workshop on Active Inference. Springer. 2020, pp. 3–11. doi: 10.1007/978-3-030-
64919-7_1.
[14] Mohamed Baioumy, Corrado Pezzato, Carlos Hernandez Corbato, Nick Hawes, and
Riccardo Ferrari. “Towards stochastic fault-tolerant control using precision learning
and active inference”. In: arXiv preprint arXiv:2109.05870 (2021). doi: 10.1007/978-
3-030-93736-2_48. url: https://arxiv.org/abs/2109.05870.
[15] Alexander Tschantz, Beren Millidge, Anil K. Seth, and Christopher L. Buckley. “Re-
inforcement learning through active inference”. In: Bridging AI and Cognitive Sci-
ence at the International Conference on Learning Representations. 2020. url: https:
//baicsworkshop.github.io/pdf/BAICS_37.pdf.
[16] Alexander Tschantz, Manuel Baltieri, Anil K. Seth, and Christopher L. Buckley. “Scal-
ing active inference”. In: 2020 International Joint Conference on Neural Networks
(IJCNN). IEEE. 2020, pp. 1–8. doi: 10.1109/IJCNN48605.2020.9207382.
[17] Noor Sajid, Philip J. Ball, Thomas Parr, and Karl J. Friston. “Active inference: De-
mystified and compared”. In: Neural Computation 33.3 (2021), pp. 674–712. doi: 10.
1162/neco_a_01357.
[18] Zafeirios Fountas, Noor Sajid, Pedro A.M. Mediano, and Karl J. Friston. “Deep ac-
tive inference agents using Monte-Carlo methods”. In: Advances in Neural Information
Processing Systems. 2020. url: https://proceedings.neurips.cc/paper/2020/
hash/865dfbde8a344b44095495f3591f7407-Abstract.html.
[19] Beren Millidge. “Deep active inference as variational policy gradients”. In: Journal of
Mathematical Psychology 96 (2020), p. 102348. doi: 10.1016/j.jmp.2020.102348.
[20] Inês Hipólito and Thomas van Es. Enactive-Dynamic Social Cognition and Active In-
ference. 2021. url: http://philsci-archive.pitt.edu/19653/.
[21] Nadine Wirkuttis and Jun Tani. “Leading or following? Dyadic robot imitative interac-
tion using the active inference framework”. In: IEEE Robotics and Automation Letters
6.3 (2021), pp. 6024–6031. doi: 10.1109/LRA.2021.3090015.
15
[22] Remi Tison and Pierre Poirier. “Communication as socially extended active infer-
ence: An ecological approach to communicative behavior”. In: Ecological Psychology
33 (2021), pp. 197–235. doi: 10.1080/10407413.2021.1965480.
[23] Ernesto C. Martínez, Jong Woo Kim, Tilman Barz, and Mariano N. Cruz Bournazou.
“Probabilistic modeling for optimization of bioreactors using reinforcement learning
with active inference”. In: Computer Aided Chemical Engineering 50 (2021), pp. 419–
424. doi: 10.1016/B978-0-323-88506-5.50066-8.
[24] Adrián Rocandio Moreno. PID control as a process of active inference applied to a
refrigeration system. 2021. url: https://projekter.aau.dk/projekter/files/
415131289/1034_PID_Control_as_Active_Inference.pdf.
[25] Stephen Fox. “Active inference: Applicability to different types of social organization
explained through reference to industrial engineering and quality management”. In:
Entropy 23.2 (2021), p. 198. doi: 10.3390/e23020198.
[26] KaiUeltzhöffer.“Deepactiveinference”.In:BiologicalCybernetics 112.6(2018),pp.547–
573. doi: 10.1007/s00422-018-0785-7.
[27] Thijs W. van de Laar and Bert de Vries. “Simulating active inference processes by
message passing”. In: Frontiers in Robotics and AI 6 (2019), p. 20. doi: 10.3389/
frobt.2019.00020.
[28] Alexander Tschantz, Anil K. Seth, and Christopher L. Buckley. “Learning action-
orientedmodelsthroughactiveinference”.In:PLoS Computational Biology 16.4(2020),
e1007805. doi: 10.1371/journal.pcbi.1007805.
[29] Ozan Çatal, Tim Verbelen, Johannes Nauta, Cedric De Boom, and Bart Dhoedt.
“Learning perception and planning with deep active inference”. In: IEEE Interna-
tional Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. 2020,
pp. 3952–3956. doi: 10.1109/ICASSP40776.2020.9054364.
[30] Karl J. Friston, N. Trujillo-Barreto, and Jean Daunizeau. “DEM: A variational treat-
ment of dynamic systems”. In: NeuroImage 41.3 (2008), pp. 849–885. doi: 10.1016/
j.neuroimage.2008.02.054.
[31] Ryan Smith, Karl J Friston, and Christopher J Whyte. “A step-by-step tutorial on
active inference and its application to empirical data”. In: Journal of Mathematical
Psychology 107 (2022), p. 102632. doi: 10.1016/j.jmp.2021.102632.
[32] William D. Penny, Karl J. Friston, John T. Ashburner, Stefan J. Kiebel, and Thomas
E. Nichols. Statistical parametric mapping: The analysis of functional brain images.
2007. isbn: 978-0-12-372560-8. doi: 10.1016/B978-0-12-372560-8.X5000-1.
[33] Karl J. Friston. “The free-energy principle: A unified brain theory?” In: Nature Reviews
Neuroscience 11.2 (2010), pp. 127–138. doi: 10.1038/nrn2787.
[34] Karl J. Friston. “Life as we know it”. In: Journal of the Royal Society Interface 10.86
(2013), p. 20130475. doi: 10.1098/rsif.2013.0475.
16
[35] Karl J. Friston. “A free energy principle for a particular physics”. In: arXiv (2019).
url: https://arxiv.org/abs/1906.10184.
[36] LancelotDaCosta,ThomasParr,NoorSajid,SebastijanVeselic,VictoritaNeacsu,and
Karl J. Friston. “Active inference on discrete state-spaces: A synthesis”. In: Journal of
Mathematical Psychology 99 (2020), p. 102447. doi: 10.1016/j.jmp.2020.102447.
[37] Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. “Planning and
acting in partially observable stochastic domains”. In: Artificial Intelligence 101.1-2
(1998), pp. 99–134. doi: 10.1016/S0004-3702(98)00023-X.
[38] Karl J. Friston, Lancelot Da Costa, Danijar Hafner, Casper Hesp, and Thomas Parr.
“Sophisticated inference”. In: Neural Computation 33.3 (2021), pp. 713–763. doi: 10.
1162/neco_a_01351.
[39] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
Jie Tang, and Wojciech Zaremba. “Openai gym”. In: arXiv preprint arXiv:1606.01540
(2016).
[40] Herman Von Helmholtz and JPC Southall. Treatise on physiological optics (Vol. 3).
1910. doi: 10.1037/13536-000.
[41] Richard Langton Gregory. “Perceptions as hypotheses”. In: Philosophical Transactions
of the Royal Society of London. B, Biological Sciences 290.1038 (1980), pp. 181–197.
doi: 10.1098/rstb.1980.0090.
[42] Geoffrey E. Hinton and Terrence J. Sejnowski. “Optimal perceptual inference”. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
Vol. 448. Citeseer. 1983. url: http://www.cs.toronto.edu/~hinton/absps/
optimal.pdf.
[43] Peter Dayan, Geoffrey E. Hinton, Radford M. Neal, and Richard S. Zemel. “The
Helmholtz Machine”. In: Neural Computation 7.5 (1995), pp. 889–904. doi: 10.1162/
neco.1995.7.5.889.
[44] Karl J. Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, John
O’Doherty, and Giovanni Pezzulo. “Active inference and learning”. In: Neuroscience &
Biobehavioral Reviews 68 (2016), pp. 862–879. doi: 10.1016/j.neubiorev.2016.06.
022.
[45] Philipp Schwartenbeck, Johannes Passecker, Tobias U. Hauser, Thomas FitzGerald,
Martin Kronbichler, and Karl J. Friston. “Computational mechanisms of curiosity and
goal-directed exploration”. In: Elife 8 (2019), e41703. doi: 10.7554/eLife.41703.
[46] Charles R. Harris, K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli
Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J.
Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew
Brett, Allan Haldane, Jaime Fernández del Río, Mark Wiebe, Pearu Peterson, Pierre
Gérard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi,
Christoph Gohlke, and Travis E. Oliphant. “Array programming with NumPy”. In:
17
Nature 585.7825 (Sept. 2020), pp. 357–362. doi: 10.1038/s41586-020-2649-2. url:
https://doi.org/10.1038/s41586-020-2649-2.
[47] Karl J. Friston, Marco Lin, Christopher D. Frith, Giovanni Pezzulo, J. Allan Hobson,
and Sasha Ondobaka. “Active inference, curiosity and insight”. In: Neural Computation
29.10 (2017), pp. 2633–2683. doi: 10.1162/neco_a_00999.
[48] Manuel Baltieri and Christopher L Buckley. “Generative models as parsimonious de-
scriptions of sensorimotor loops”. In: arXiv preprint arXiv:1904.12937 (2019).
[49] Marco Cox, Thijs van de Laar, and Bert de Vries. “A factor graph approach to auto-
mated design of Bayesian signal processing algorithms”. In: International Journal of
Approximate Reasoning 104 (Jan. 2019), pp. 185–204. issn: 0888-613X. doi: 10.1016/
j.ijar.2018.11.002. url: http://www.sciencedirect.com/science/article/
pii/S0888613X18304298 (visited on 11/16/2018).
[50] Mees Vanderbroeck, Mohamed Baioumy, Daan van der Lans, Rens de Rooij, and Tiis
van der Werf. “Active inference for robot control: A factor graph approach”. In: Student
Undergraduate Research E-journal! 5 (2019), pp. 1–5.
[51] Burak Ergul, Thijs van de Laar, Magnus Koudahl, Martin Roa-Villescas, and Bert
de Vries. “Learning Where to Park”. In: International Workshop on Active Inference.
Springer. 2020, pp. 125–132.
[52] Thijs van de Laar, Ismail Senoz, Ayça Özçelikkale, and Henk Wymeersch. “Chance-
Constrained Active Inference”. In: arXiv preprint arXiv:2102.08792 (2021). doi: 10.
1162/neco_a_01427.
[53] Mortimer Mishkin, Leslie G Ungerleider, and Kathleen A. Macko. “Object vision and
spatial vision: Two cortical pathways”. In: Trends in Neurosciences 6 (1983), pp. 414–
417. doi: 10.1016/0166-2236(83)90190-X.
[54] Semir Zeki, J.D. Watson, C.J. Lueck, Karl J. Friston, C. Kennard, and R.S. Frack-
owiak. “A direct demonstration of functional specialization in human visual cortex”.
In: Journal of Neuroscience 11.3 (1991), pp. 641–649. doi: 10.1523/JNEUROSCI.11-
03-00641.1991.
[55] RenaudJardri,SandrineDuverne,AlexandraS.Litvinova,andSophieDenève.“Exper-
imental evidence for circular inference in schizophrenia”. In: Nature Communications
8.14218 (2017), pp. 1–13. doi: 10.1038/ncomms14218.
[56] Pantelis Leptourgos, Charles-Edouard Notredame, Marion Eck, Renaud Jardri, and
Sophie Denève. “Circular inference in bistable perception”. In: Journal of Vision 20.4
(2020), p. 12. doi: 10.1167/jov.20.4.12.
[57] Thomas Parr, Dimitrije Markovic, Stefan J. Kiebel, and Karl J. Friston. “Neuronal
message passing using Mean-field, Bethe, and Marginal approximations”. In: Scientific
Reports 9.1 (2019), pp. 1–18. doi: 10.1038/s41598-018-38246-3.
[58] Thomas Parr, Noor Sajid, and Karl J. Friston. “Modules or mean-fields?” In: Entropy
22.5 (2020), p. 552. doi: 10.3390/e22050552.
18
[59] Matthew James Beal. Variational Algorithms for Approximate Bayesian Inference.
University of London, University College London, 2003. url: https://cse.buffalo.
edu/faculty/mbeal/thesis/.
[60] Martin J. Wainwright and Michael Irwin Jordan. Graphical models, exponential fam-
ilies, and variational inference. Vol. 1. Now Publishers Inc, 2008. doi: 10.1561/
2200000001.
[61] David J.C. MacKay. “Bayesian interpolation”. In: Neural Computation 4.3 (1992),
pp. 415–447. doi: 10.1162/neco.1992.4.3.415.
[62] Will D. Penny, Klaas E. Stephan, Andrea Mechelli, and Karl J. Friston. “Modelling
functional integration: A comparison of structural equation and dynamic causal mod-
els”. In: NeuroImage 23 (2004), S264–S274. doi: 10.1016/j.neuroimage.2004.07.
041.
[63] JonathanS.Yedidia,WilliamT.Freeman,andYairWeiss.“Generalizedbeliefpropaga-
tion”. In: Advances in Neural Information Processing Systems. Vol. 13. 2000, pp. 689–
695. url: https://dl.acm.org/doi/10.5555/3008751.3008848.
[64] John Winn and Christopher M. Bishop. “Variational message passing”. In: Journal
of Machine Learning Research 6.23 (2005), pp. 661–694. url: http://jmlr.org/
papers/v6/winn05a.html.
[65] Thomas Parr and Karl J. Friston. “Generalised free energy and active inference”. In:
Biological Cybernetics 113.5(2019),pp.495–513.doi:10.1007/s00422-019-00805-w.
[66] Beren Millidge, Alexander Tschantz, and Christopher L. Buckley. “Whence the ex-
pected free energy?” In: Neural Computation 33.2 (2021), pp. 447–482. doi: 10.1162/
neco_a_01354.
[67] BerenMillidge,AlexanderTschantz,AnilSeth,andChristopherBuckley.“Understand-
ingtheorigin of information-seekingexplorationinprobabilisticobjectivesforcontrol”.
In: arXiv (2021). url: https://arxiv.org/abs/2103.06859.
[68] Lancelot Da Costa. Personal communication. 2021.
19
Appendix A: Factorized Generative Models
In this appendix, we explain observation and state factorization by using an in-depth exam-
ple. Let’s imagine a scenario where you have to infer two simultaneous states of the world,
given some sensory data. The two facts you need to estimate are 1) what time of day it is
(morning, midday, or evening), and 2) whether it rained recently (yes or no). We can rep-
resent this in a generative model as an environment characterized by two discrete random
variables or hidden state factors. The first variable or factor we can call the time-of-day
state, which has three levels: Morning, Midday, or Evening; the second factor we can call
the did-it-rain state, which has two levels: Rained and Did-not-rain. We use the following
notation to denote these hidden state factors and their respective levels:
s = {stime-of-day,sdid-it-rain}
stime-of-day ∈ {Mo,Mid,Eve} sdid-it-rain ∈ {Rain,NoRain}
where we assign each state level a cardinal index, e.g. Mo = 0, Mid = 1, Eve = 2,
and Rain = 0,NoRain = 1. Let’s now augment our simple hidden state representation with
observations, which ahypothetical agent would use to infer both the time of day and whether
it rained recently, i.e. to obtain a posterior distribution over s: P(s | o). Our observations
will also be factorized, into two different modalities or information channels. Each of these
modalities is also a discrete-valued random variable. Let’s imagine our two modalities are:
1) the ambient light level (dark, cloudy, or sunny) and 2) humidity (dry or humid). We
denote the observations as follows:
o = {olight,ohum}
olight ∈ {Drk,Cld,Sun} ohumid ∈ {Dry,Hmd}
Having specified a factorized representation of both states and observations, we can
now consider how observations lend evidence for or against different states-of-affairs in the
environment. For example, if you notice it’s dark outside (i.e., olight = Drk), that provides
evidence to suggest that it’s night time, rather than being morning or midday. At the
same time, you might also notice that the air is humid through your humidity modality, i.e.
ohum = Hmd. We can imagine that the humidity observation provides no evidence for the
time of day it is, but it may suggest that it rained recently.
These probabilistic relationships between the observation modalities and the hidden state
factors, which are used to perform inference, are encoded in the observation likelihood
P(o |s), represented in pymdp as a modality-specific sub-array A[m]. Recall that each like-
m
lihood array encodes the conditional dependencies between each setting of the hidden states
s and the observations within modality m, i.e. A[m] = P(o |s ,s ,...,s ). Therefore the
m 1 2 F
dimensionality of a given A[m] array is O ×S ×...×S , where O is the dimensionality of
m 1 F m
modality m and S is the dimensionality of factor f. Following our simple example of infer-
f
ring the time of day and whether it just rained, the likelihood for the first modality would be
a 3D array that represents the likelihood distribution P(olight|stime-of-day,sdid-it-rain). Specifi-
cally, entry A[i,j,k] encodes the probability of observing olight = level i, given stime-of-day =
20
level j and sdid-it-rain = level k. The second observation modality accordingly has its own
likelihood NDarray, encoding the likelihood distribution P(ohum|stime-of-day,sdid-it-rain).
These higher-dimensional likelihood arrays enable complex, conjunctive relationships to
be encoded in the generative model. For instance, we might imagine that the olight obser-
vation depends on both the time of day and whether it just rained. For instance, all else
being equal we might expect the ambient lighting to be sunny, if the time of day is midday.
However, if it just rained then the probability of the ambient lighting being dark or cloudy
might be higher, even if the time of day is midday. This statement already requires a nonlin-
ear, conjunctive relationship between stime-of-day and sdid-it-rain. For example the probability
distribution over the 3 levels of olight, given stime-of-day is Mo and sdid-it-rain is Rain would be
encoded by the corresponding likelihood: P(olight|stime-of-day = Mo,sdid-it-rain = Rain). This
would then be easily encoded in the corresponding ‘slice’ of the high-dimensional A[light]
array: A[light][:,Mo,Rain], where light is the index of the A array that encodes the olight
likelihood, and Mo and Rain are the cardinal indices for those factor-specific state levels.
In the same way that the observation likelihood for each modality m is represented using
a single likelihood NDarray A[m], the transition likelihood for each hidden state factor f
is represented using a single likelihood NDarray B[f], where the size of the fth transition
array is of size S ×S ×U . So analogously to having a collection of A[m] arrays, one for
f f f
each observation modality, we also have a collection of B[f] arrays, one for each state factor.
Two important things to note are that: 1) constructing B matrices with this S ×S ×U
f f f
shape assumes that hidden state factors cannot influence each other dynamically, i.e. the
next state within a factor sf only depends on the past state for that factor sf and control
t t−1
state uf , and 2) that control states are factorized just like hidden states, such that for each
t−1
hidden state factor there is a corresponding control factor cf whose dimensionality is equal
to the number of control state levels or actions that can be taken upon hidden state factor
f. It is of course allowable to have uncontrollable hidden state factors - in which case we
simply set the dimensionality of the corresponding control factor to 1, i.e. U = 1. We often
f
refer to these as ‘trivial’ control factors, since they don’t actually encode any kind of control.
The factorized representation described above offers several advantages. First of all, if a
largehiddenstatespacecanbefactorizedintoacollectionofone-dimensionalrepresentations,
then the memory cost of storing the relevant probability distributions over hidden states can
begreatlyfinessed(e.g. P(s |s ), P(s), etc.). Forexample, ifyoucanrepresenttheidentity
t t−1
ofanobjectanditslocationindependently,withouthavingtoenumerateallthecombinations
of both its identity and its location together, then the amount of memory used to store the
factorized representation will be linear in the dimensionality of the two hidden state factors,
whereas the ‘enumerated’ representation will be polynomial. For example, if location is a
1000-dimensional vector, and identity is a 1000-dimensional vector, then storing two 1000-
dimensional vectors is considerably cheaper than storing a single 1000 × 1000-dimensional
vector.
Another advantage is the degree of interpretability and model transparency that fac-
torized representations afford; a particular factorization is ideally explicitly designed, such
that hidden state factors are directly mapped to intuitive features of the environment whose
21
relationships are easy to reason about. If a multi-factor model of, for example, 3 hidden
state factors (e.g. the location, identity, and time of some event) were fully enumerated
into a single, 1-dimensional hidden state, then each level of the single hidden state would
correspond to a unique combination of “what”, “where” and “when”. When it comes to
encoding probabilistic relationships in the generative model (e.g. the observation and tran-
sition models), it becomes harder to visualize and reason about the relationships between
such high-dimensional state combinations. Thus factorization also proves a useful tool when
designing generative models based on prior domain or task knowledge.
Interestingly, when one optimizes the factorial structure of a generative model, using
marginal likelihood or variational free energy, the best factorisation maximizes marginal
likelihood(a.k.a., modelevidence)byminimizingcomplexity: namely, thedegreesoffreedom
used to provide an accurate account of observations. This is an important aspect of active
inference; namely,thattoprovidethebestaccountofobservations–thatprecludesoverfitting
and ensures generalization – the (mean-field) factorisation should be as simple as possible
but no simpler.
Finally, inference also may take advantage of the factorized structure of the generative
model. In doing so, inference is not only more memory-efficient, but the belief-updating
algorithms have features like functional specialization [53, 54] and local message-passing
that have been linked to features of computation in the brain [55, 56]. It has been argued
that this affords factorized generative models a higher degree of biological plausibility [57,
58].
Appendix B: Modules and Theory
inference.py
In this section we provide an overview of the inference module of pymdp and then briefly
rehearse the mathematics of variational inference, both generally and as it is used in pymdp.
The inference.py file contains functions for performing variational inference about hid-
den states in discrete categorical generative models. Functions within this module are called
by the infer_states() method of Agent. The core functions of this module are:
• update_posterior_states(obs, A, prior=None, **kwargs): This function com-
putes the variational (categorical) posterior over hidden states at the current timestep
Q(s ). This function by default calls the standard or ‘vanilla’ inference algorithm of-
t
fered by pymdp, which estimates the marginal posteriors for each hidden state factor at
the current timestep Q(si) using fixed-point iteration. This function can be generically
t
applied as the inference step in any discrete POMDP model, as all it requires are some
observations obs, a likelihood array A and optionally a prior over hidden states prior,
which will have the same structure as the resulting posterior. The additional argu-
ments **kwargs contain parameters that will be passed to the run_vanilla_fpi()
function in algos/fpi.py (see the documentation for more details).
22
• update_posterior_states_full(A,B,prev_obs,policies, prev_actions=None,
prior=None, policy_sep_prior=True, **kwargs): Thisfunctioncomputesthevari-
ational (categorical) posterior over hidden states under all policies: Q(˜s|π). The nota-
tion ˜s represents a trajectory of hidden states over time. This is inspired by the ‘full
construct’ active inference process as implemented in spm_MDP_VB_X.m in DEM, where
the full posterior over both hidden states and policies is computed: Q(s,π). This func-
tion itself calls the run_mmp.py function within the algos library, which estimates the
marginal posteriors for hidden state factor i, at time point τ under policy j: Q(si | π ),
τ j
using marginal message passing [57] (for more details, see the algos.py summary be-
low). Thisfunctioncallsrun_mmp.pyonceperpolicy,estimatingthepolicy-conditioned
posterior over all timepoints of the horizon and for all hidden state factors. The num-
ber of timepoints over which inference occurs is not necessarily identical to the total
time horizon of the simulation: rather this time horizon is a function of the number
of previous observations (len(prev_obs)) and the temporal depth of the policy under
consideration (len(policies[j])). Thus the hidden state beliefs indexed by τ refer
to a finite time horizon that is relative to the current timestep t, where this horizon
[t − H ,t + H ] has a ‘lookback’ length H and a ‘planning horizon’ H . As argu-
0 1 0 1
ments, this function requires the observation (A) and transition (B) likelihoods of a
generative model, a list of previous (including current) observations (prev_obs), a list
of policies (policies), an optional list of actions taken up until the current timepoint
(prev_actions) an optional prior over hidden states at the start of the time horizon
(prior), and a keyword argument policy_sep_prior, which determines whether the
prior is itself conditioned on policies P(s |π) vs. unconditioned on policies P(s ). The
0 0
additional **kwargs contain parameters that will be passed to the run_mmp() function
in algos/mmp.py. For more details on run_mmp(), see the documentation.
Specific usage examples–in addition to descriptions of other specialized functions in
inference.py–are more extensively covered in the official documentation.
Bayesian and Variational Inference
A central task in statistics is to perform inference, which can be mathematically represented
as computing posterior distributions of one variable given another from a joint distribution.
For example, suppose we are given some observation o, and we then want to infer the likely
state s underlying that observation. Critical to achieving this task is the possession of a gen-
erative model, or joint distribution, P(o,s) that tells us how observations and hidden states
are related. We can formulate the problem of inferring s as finding a posterior distribution
over the states, given the observation: P(s|o). We can compute this posterior distribution
using our generative model and Bayes Rule:
P(o,s)
P(s|o) =
P(o)
The generative model P(o,s) is often factorized into a likelihood P(o|s) and a prior P(s),
while P(o) is known as the marginal likelihood or model evidence and can be computed by
23
solvingtheintegralP(o) = (cid:82) P(o,s)ds. Thisexpressestheideathatthemarginalprobability
of observations o in P(o) is the sum (or integral) over all the ways that that o depends on s,
for all possible settings of s.
While Bayes rule provides a simple formula for relating the posterior distribution to the
generative model, explicitly computing this distribution can often be difficult in practice due
to the computational expense involved in performing the integral over all states necessary
to compute the marginal likelihood P(o). However, a number of approximate Bayesian
inference methods have been developed which circumvent this computational difficulty at
the expense of only returning approximately correct posterior distributions.
Variational inference [59, 60] is a widely used and well understood approach for per-
forming approximate Bayesian inference. The central idea in variational inference is that
instead of directly computing the posterior, we instead optimize the parameters θ of an ar-
bitrary distribution Q(s;θ) so as to minimize the divergence between this distribution and
the true posterior. This arbitrary distribution is often named the approximate posterior,
because in the course of minimizing the divergence, the arbitrary distribution becomes an
approximation to the true posterior, i.e. Q(s;θ) ≈ P(s|o). In this way, variational inference
converts a challenging inference problem (involving computing intractable integrals) into a
relatively straightforward optimization problem, for which many powerful algorithms exist
in the optimization literature.
Ideally, variational inference would directly minimize the Kullback-Leibler divergence
between the approximate and true posteriors:
θ∗ = argmin D [Q(s;θ)||P(s|o)]
KL
θ
Inthepresentform„ thisobjectiveisalsointractablesinceitdependsonthetrueposterior
P(s|o), whoseapproximationisourgoal. However, bysupplementingtheKLdivergencewith
the log marginal likelihood, which does not depend upon Q(s;θ), we can convert the above
KL divergence into an upper bound on the log marginal likelihood, called variational free
energy (VFE) F. Crucially, this can be rearranged into a computable form:
θ∗ = argmin F
θ
F = D [Q(s;θ) (cid:107) P(s|o)]−lnP(o)
KL
= E [lnQ(s;θ)−lnP(s,o)]
Q
Thus, by minimizing F, we minimize the divergence between the approximate and true
posterior, thus forcing the approximate posterior to more closely resemble the true one.
Moreover, if this optimization finds the exact solution, such that KL[Q(s;θ)||P(s|o)] = 0
thenvalueofF = −lnP(o)providesthemarginallikelihood(P(o) ∝ e−F)whichcanthenbe
used for model selection and structure learning. More generally, F is known as an evidence
bound, because the KL divergence can never be less zero [61, 62].
24
Variational Inference in pymdp
Active inference agents in pymdp perform inference over both hidden states s and policies
π in a POMDP generative model. In this appendix, we only consider inference over states,
while inference over policies is treated in the following section control.py.
Recall the POMDP generative model is a joint distribution over observations, states,
policies, and parameters. For the purposes of hidden state inference, we will condition the
whole generative model on some fixed policy π, so that we can re-write it as follows:
T T
(cid:89) (cid:89)
P(o ,s ,φ | π) = P(φ)P(s ) P(s |s ,π,φ) P(o |s ,φ) (2)
1:T 1:T 1 τ τ−1 τ τ
τ=2 τ=1
Foranactiveinferenceagentequippedwiththisgenerativemodel,instantaneousinference
consists in optimizing an approximation to the posterior over the current hidden state:
P(s |s ,o ) given the past and future states s = {s ,s }, and the observations
τ \τ [1:τ] \τ [1:τ−1] [τ+1:T]
collectedupuntilthecurrenttimepointo . Mathematically,thisinferencecanbedescribed
[1:τ]
as minimizing the following free energy over trajectories with respect to the variational
parameters θ:
θ∗ = argminF
1:T
θ
F = E [lnQ(s ;θ)−lnP(o ,s ;φ | π)]
1:T Q 1:T 1:T 1:T
Thus the goal of hidden state inference is the optimization of variational parameters θ
which parameterise the approximate posterior over hidden states: Q(s ;θ). In the case of
1:T
our discrete POMDP generative model, the variational parameters θ are the sufficient statis-
tics of categorical distributions. Fortunately, these parameters are easy to interpret, since
they are identical to the probabilities of sampling each outcome level in the distribution’s
support: i.e. Q(s;θ) = Cat(θ). For example, the variational parameters of some categorical
distribution Q(s) = (cid:2) 0.1 0.4 0.5 (cid:3) would simply be θ = (cid:2) 0.1 0.4 0.5 (cid:3). In the equations to
follow, we therefore exclude the variational parameters θ when writing the variational pos-
terior, referring to it hereafter as simply Q(s). Below we describe the variational inference
methods currently offered by pymdp as of this document’s writing (November 2021).
The run_vanilla_FPI function (within algos/fpi.py) implements an inference algo-
rithm known as fixed-point iteration to optimize the posterior over hidden states Q(s ) at a
τ
given timestep τ. Central to this algorithm is the assumption of a factorized structure to the
variational posterior, such that the posterior at time τ = i is independent of the posterior
at any other timestep τ = j, where j (cid:54)= i. In addition to this temporal factorization, we
further assume the posterior at a given timestep Q(s ) is factorized across different hidden
τ
state factors: i.e. Q(sf) is independent of Q(sf(cid:48)) (see the section Factorized representations
τ τ
for more on multi-factor hidden states). This factorization is also known as a mean-field
25
approximation in the statistics and physics literatures and can be expressed as follows:
T F
(cid:89) (cid:89)
Q(s ) = Q(s ) Q(s ) = Q(sf)
[1:T] τ τ τ
τ=1 f=1
Given this factorization, the full free energy over trajectories now also factorizes into a
sum of free energies across time, which can be minimized independently of each other. Thus,
for a given time τ, we can write the time-dependent free energy3:
F = E [lnQ(s )−lnP(o |s )P(s |s ,u )] (3)
τ Q τ τ τ τ τ−1 τ−1
where we now use the bold notation s and o to express potentially multi-factor (or -modal)
hidden states (or observations) in the generative model, in the same way that the variational
posterior is factorized. Inference proceeds by optimizing Q(s ) in order to minimize the
τ
timestep-specific free energy F .
τ
Importantly, we can solve for the variational posterior analytically for a given timestep τ
and factor f by setting the derivative of the free energy F to 0 and solving for Q(sf). We
τ τ
express this partial derivative as ∂Fτ , and can express it as follows:
∂qf
∂F ∂ (cid:104)(cid:88) (cid:105)
τ
= Q(s )(lnQ(s )−lnP(o ,s )) = 0
∂qf ∂qf τ τ τ τ
= lnQ(sf)+1−E (cid:2) lnP(o |s )−ln (cid:0)E [P(s |s ,u )] (cid:1)(cid:3) = 0
τ qi\f τ τ P(sτ−1,uτ−1) τ τ−1 τ−1
(cid:16) (cid:17)
=⇒ lnQ(sf) = E [lnP(o |s )]+ln E [P(sf|sf ,uf )] −1
τ qi\f τ τ P(sf ,uf ) τ τ−1 τ−1
τ−1 τ−1
(cid:16) (cid:16) (cid:17)(cid:17)
=⇒ Q∗(sf) = σ E [lnP(o |s )]+ln E [P(sf|sf ,uf )] (4)
τ qi\f τ τ P(sf ,uf ) τ τ−1 τ−1
τ−1 τ−1
where the expectation E denotes an expectation with respect to all posterior marginals
qi\f
Q(si)besidesthemarginalQ(sf)currentlybeingoptimized,andσ(x) = ex isanormalized
τ τ (cid:80) ex
exponential or softmax function. The update equation for each marginalxposterior offers an
intuitive Bayesian interpretation, where the belief about the current state is the product of
an observation likelihood term P(o |s ) and a ‘prior’ term, E [P(s |s ,u )],
τ τ P(sτ−1,uτ−1) τ τ−1 τ−1
where the prior is dynamically determined by the previous state, previous action, and the
transition likelihood. Note that in practice we set the prior at any given timestep equal
to the posterior optimized at the previous timestep, i.e. P(s ,u ) ≡ Q∗(s ,u ) =
τ−1 τ−1 τ−1 τ−1
Q∗(s )Q∗(u ), where Q∗(u ) is a Dirac delta function over the action actually taken
τ−1 τ−1 τ−1
(the agent has perfect knowledge of the action it just took). This means that the prior
term in the last line of Equation (4) can be rewritten as E [P(sf|sf ,uf )] =
P(sf ,uf ) τ τ−1 τ−1
τ−1 τ−1
E [P(sf|sf ,uf )]. In the run_vanilla_fpi.py function of pymdp, the fixed
Q(sf ,uf ) τ τ−1 τ−1
τ−1 τ−1
pointequationissolvediterativelyforeachmarginalposteriorQ(sf),usingthelatestsolution
τ
for the other marginals Q(si\f) to compute the expected log-likelihood term for the marginal
τ
3For the remainder of the section we remove the hyperparameters φ from the generative model and
approximate posterior, but inference over these are treated in the section on learning.py
26
f currently being updated: E [lnP(o |s )]. Note that the prior P(sf|sf ,uf )P(sf )
qi\f τ τ τ τ−1 τ−1 τ−1
only depends on the marginal f currently being updated, because in pymdp the transition
likelihoods are assumed to be independent across hidden states, i.e. hidden states from
factor i do not determine the dynamics of hidden states of another factor j (see Appendix
A: Factorized Generative Models for details). Given enough iterations,4 the fixed point
equations converge to a unique solution of the variational posterior Q∗(s ).
τ
In pymdp, the approximate posterior Q(s) is represented as a collection of 1-D NumPy
arrays (e.g. qs), where individual elements of the collection (e.g. qs[f]) store the marginal
posterior for a particular hidden state factor. The likelihood distributions are represented by
A and B arrays. If we take the limiting case of a generative model and variational posterior
with a single hidden state factor, the update equation for the variational posterior at a given
timestep using fixed point iteration reduces to a single line of NumPy code:
qs_current = softmax(np.log(A[o,:])+ np.log(B[:,:,u_last].dot(qs_last)))
where utility functions like softmax are available from the utils.py and maths.py modules
of pymdp. In the default initialization of the Agent class, the infer_states() method will
call the update_posterior_states() function of the inference module; this in turns calls
upon fixed point iteration (via run_vanilla_fpi()) to update the variational posterior over
hidden states. Therefore all that is required for inference at a given timestep is to provide
some observation obs to infer_states(). The prior over hidden states is automatically
updated within the Agent class, where prior will either A) equal the initial belief about
hidden states (the D vector) in the case that τ = 1 ; or B) the dot product of the transi-
tion likelihood conditioned on the last action B[:,:,u_last], and the posterior at the last
timestep qs_last in the case that τ > 1: prior=B[:,:,u_last].dot(qs_last).
Another,morecomplexinferencealgorithmknownasmarginal message passing (MMP)is
also implemented in the run_mmp() function, also found within the algos module. Marginal
message passing makes weaker assumptions about the factorization of the variational pos-
terior, and incorporates the computational advantages of two well-known message passing
algorithms: belief propagation [63] and variational message passing [64]. In practice, using
marginal-message passing instead of standard fixed-point iteration enables more accurate
inference due to its less restrictive assumptions as to the form of the variational posterior,
at the expense of additional computational cost. For the purpose of brevity and since this
algorithm has been discussed in detail elsewhere (specifically, see Appendix C of [4] as well
a comprehensive treatment in [57]), we will not describe the mathematics behind marginal
message passing here. It is worth noting that for beginning users, a standard active infer-
ence simulation will not require marginal-message passing to achieve the desired behavior;
state inference achieved with instantaneous fixed-point iteration often suffices for practition-
ers interested in simulating a target behavior. However, cognitive neuroscientists are often
4The default number of iterations for run_vanilla_fpi() function is num_iter=10 but in many genera-
tive models with precise likelihood arrays (i.e. low entropy rows/columns), convergence is often achieved in
many fewer iterations. This is further controlled by a tolerance parameter that tracks the change in the free
energy across iterations.
27
interested in modelling neuronal responses based on estimated inferential dynamics. In this
case, more sophisticated schemes like run_mmp() may be required, where actual dynamics of
belief updating might be used as a forward model of hypothesized electrophysiological pro-
cesses (e.g. local field potentials or spiking activity). Finally, we also mention that in order
to achieve identical behavior to active inference agents simulated using spm_MDP_VB_X.m, it
is necessary to use run_mmp().
Inpractice, adesiredinferencealgorithmcanbespecifiedbypassingthenameofthealgo-
rithmintotheAgent()constructor, e.g. my_agent = Agent(...,inference_algo=‘MMP’).
For more detailed instructions on how to initialize an Agent with different customization op-
tions, please see the documentation.
control.py
The core functions implementing policy inference and action selection (i.e. control) in pymdp
can be found in the control.py file. As with inference, these functions are called by
methods of Agent like infer_policies() and sample_action(), but can also be directly
imported from the control library and used for custom applications. Below we briefly
summarize the core functions of the control module:
• update_posterior_policies(qs, A, B, C, policies, use_utility=True,
use_states_info_gain=True, use_param_info_gain=False, pA=None, pB=None,
E=None, gamma=16.0): This function computes the posterior over policies Q(π) using
aninitialposteriorbeliefabouthiddenstatesatthecurrenttimestepqs. Specifically, it
computes the expected free energy of each policy (discussed further in the next section)
by summing the expected free energies over a future path in the case of multi-timestep
or ‘temporally-deep’ policies. This function first loops over all policies, computes the
expected states and observations under each policy, and then sums the expected free
energies calculated from those predicted future states and observations. The expected
free energy for each policy is combined with its prior probability under the generative
model P(π) (in pymdp represented by the E vector) and softmaxed to determine the
posterior over policies Q(π) (in pymdp represented by q_pi). Optional Boolean pa-
rameters like use_utility and use_states_info_gain can be turned on and off to
selectively enable (disable) computation of components of the expected free energy (see
the section on The Expected Free Energy for information on these different expected
free energy terms).
• update_posterior_policies_full(qs_seq_pi, A,B,C, policies,use_utility=
True, use_states_info_gain=True, use_param_info_gain=False, prior=None,
pA=None, pB=None, F=None, E=None, gamma=16.0): This function computes the
posterior over policies Q(π) using a posterior belief over hidden states over multi-
ple timesteps under all policies. This version differs from the standard function,
update_posterior_policies(), in that the expected hidden states over future time-
points, under different policies, have already been computed in the input, the posterior
28
beliefs qs_seq_pi. This function for policy inference should thus be used in tandem
with the ‘advanced’ inference schemes (like marginal message passing) where posterior
beliefs over multiple timesteps, under all policies, are computed during the inference
step. As a consequence, this function only computes the expected observations under
each policy for all future timesteps, and then uses the expected states (already part
of the inputs) and expected observations under all policies to calculate the expected
free energy for each policy. This is integrated with prior belief about policies E and
the variational free energy of policies F (see the section on Policy Inference for more
information on the variational free energy of policies) to finally determine the poste-
rior over policies Q(π), often represented in pymdp as q_pi. This function’s remaining
arguments (e.g. use_utility) are identical to how they are used in the standard
update_posterior_policies() function.
• get_expected_states(qs, B, policy): This function computes a posterior distri-
bution over future states given a current state distribution (qs), a transition model
(B) and a policy (policy). Specifically, this function projects the current beliefs about
hidden states forward in time by iteratively taking the inner product of qs with the
action-conditioned B matrix, where the actions are those entailed by the policy.
• get_expected_obs(qs_pi, A): This function computes the observations expected un-
der a (policy-conditioned) hidden state distribution qs_pi. In the case of a sequence
of hidden states over time, qs_pi will be a list of hidden states distributions with one
element per timestep e.g. qs_pi[t]. This function only requires an expected state
distribution qs_pi and an observation model A.
• calc_expected_utility(qo_pi, C): This function computes the extrinsic value or
utility part of the expected free energy using the prior preferences or ‘goal distribution’
encoded by the C vector. The C is encoded in terms of relative log probabilities and
thus need not be a proper probability distribution.
• calc_states_info_gain(A, qs_pi): This function computes intrinsic value or infor-
mation gain part of the expected free energy, in particular the information gain or
epistemic value about hidden states s.
• calc_pA_info_gain(pA, qo_pi, qs_pi): This function computes the information
gain about the Dirichlet prior parameters over the observation model (A) , also known
as the ‘novelty’ term of the expected free energy [47]. It requires a Dirichlet prior
over the observation model pA, an expected observation distribution qo_pi and an
expected state distribution qs_pi. It is recommended to include this information gain
term in the expected free energy calculation, when also simultaneously performing A
array learning (i.e. inference over Dirichlet hyperparameters), since it leads to the
agent exploring regions which lead to the largest updates of the parameters of A .
• calc_pB_info_gain(pB, qs_pi, qs_prev, policy): Thisfunctioncomputesthein-
formation gain about the Dirichlet prior parameters over the transition model (B), also
29
known as the ‘novelty’ term of the expected free energy. It requires a Dirichlet prior
over the transition model pB, an expected state distribution under a policy qs_pi, an
initial state distribution qs_prev, and a policy policy. It is recommended to include
this information gain term in the expected free energy calculation, when also simulta-
neously performing B array learning (i.e. inference over Dirichlet hyperparameters).
• construct_policies(num_states, num_controls=None, policy_len=1,
control_fac_idx=None): This is a utility function which builds an array of policies
by combinatorially enumerating them from a set of actions and a time horizon. It can
be used to construct a full set of policies based on the time horizon and action space
of the environment, if the policy set is not explicitly stated by the user.
• sample_action(q_pi, policies, num_controls,
action_selection="deterministic", alpha=16.0): This function samples an ac-
tion, given the posterior distribution over policies and a desired sampling scheme. In
particular, this function computes the posterior over control states u by marginal-
ising the posterior over policies with respect to each control state, i.e. Q(u ) =
t
(cid:80) P(u | π)Q(π), where P(u | π) is the mapping between policies and control states.
π t t
To obtain an action, the most probable action is either A) selected deterministically
(action_selection="deterministic") as the most probable control state or B) an
action is sampled from the control posterior (action_selection="stochastic"), us-
ing a Boltzmann distribution with inverse temperature given by alpha.
Control in Active Inference
Policy inference consists in computing the ‘goodness’ or ‘quality’ of each policy, given the
ability to compute the expected consequences of each policy and the agent’s goals. In active
inference this is done by using a quasi-utility function known in the literature as the Expected
Free Energy (EFE) (often denoted G). Under active inference, agents are equipped with
a particular prior over policies P(π) that assumes policies are inversely proportional to the
free energy expected under their pursuit, i.e.:
P(π) = σ(−G) (5)
Equipped with this policy prior in the generative model, active inference agents perform
policy inference by optimizing Q(π), the variational posterior over policies. As we shall see in
the section Policy Inference, computing Q(π) entails computing the expected free energy of
each policy (the contribution from the prior) as well as the variational free energy of policies
(analogous to the ‘evidence’ for each policy).
The Expected Free Energy The expected free energy is the crucial component that
determines the behavior of active inference agents. The EFE is designed to be similar to the
VFE of standard variational inference but with two major modifications to enable its use as
30
an objective which, when minimized, will perform goal seeking behavior rather than simply
inference. Firstly, since the EFE ranks future performance, where future observations are
not known, it contains an expectation over future observations. Secondly, as there needs to
be a way to integrate the notion of goals or rewards into the inference procedure, the EFE
alters the generative model of the agent to be ‘biased’ in such a way that it predicts the agent
reaches rewarding or a priori preferred states [65]. Thus, performing inference to maximize
the likelihood of visiting these rewarding states naturally leads to policies that help the agent
achieve its goals. Moreover, an additional benefit is that minimizing the EFE also entails an
exploratory, inherently uncertainty-reducing component to behavior. This endows behavior
with an additional ‘epistemic drive’ which aids in computing the optimal long-term policies
[3]. For in-depth discussion of the nature of the EFE and the exploratory drive it induces
please see [3, 4, 66, 67].
The expected free energy is a function of observations, states, and policies, and is defined
mathematically as:
G(o ,s ,π) = E [lnQ(s ,π)−lnP ˜ (o ,s ,π)] (6)
1:T 1:T Q 1:T 1:T 1:T
where P ˜ represents a generative model ‘biased’ towards the preferences of the agent. We
can write this predictive generative model at a single timestep, under a given policy, as
P ˜ (o ,s |π) = P(s |o ,π)P ˜ (o ), where P ˜ (o ) represents a ‘predictive prior’ over observations,
τ τ τ τ τ τ
represented in pymdp with the C array. Given the factorization of the approximate posterior
Q(s,π) over time, the EFE for a single policy and timestep can also be defined as follows:
G (π) = E [lnQ(s |π)−lnP ˜ (o ,s |π)]
τ Q(oτ,sτ|π) τ τ τ
 
= E Q(oτ,sτ|π)lnQ(s
τ
|π)−lnP ˜ (o
τ
,s
τ
|π)+lnQ(s
τ
|o
τ
,π)−lnQ(s
τ
|o
τ
,π)
(cid:124) (cid:123)(cid:122) (cid:125)
=0
(cid:104) (cid:105)
= E Q(s |π)−Q(s |o ,π)−lnP ˜ (o ) +E [D [Q(s |o ) (cid:107) P(s |o ,π)]]
Q(oτ,sτ|π) τ τ τ τ Q(oτ|π) KL τ τ τ τ
(cid:124) (cid:123)(cid:122) (cid:125)
Expectedapproximationerror≥0
≥ −E [D [Q(s |o ,π) (cid:107) Q(s |π)]]−E [lnP ˜ (o )] (7)
Q(oτ|π) KL τ τ τ Q(oτ|π) τ
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
EpistemicValue Utility
where the first term, the epistemic value [3], encourages the pursuit of policies expected to
yield high information gain about hidden states, expressed here as the divergence between
the states predicted under a policy, with and without conditioning on observations. The
second term represents the degree to which expected outcomes under a policy will align with
prior preferences over observations. Since the prior over policies is inversely proportional
to the expected free energy, policies will thus be more likely if they visit states that resolve
uncertainty (maximize epistemic value) and satisfy prior preferences (maximize utility). The
epistemic value terms give active inference agents a degree of superior exploration capacity
compared to standard reinforcement learning agents. In pymdp, the EFE is computed using
exactly this decomposition into epistemic value and utility, where the expected approxima-
tion error (penultimate line of Equation (7)) is implicitly assumed to be 0, so the bound
31
becomes equality. The utility term is computed by the function calc_expected_utility()
while the epistemic value term (also known as the information gain) is computed by the
function calc_states_info_gain(). Both of these functions are found within control.py.
The computation of the utility term is particularly straightforward for categorical distribu-
tions, since it reduces to the dot product of the expected observations under a policy Q(o |π)
τ
with the log of the prior preferences or ‘goal vector’ P ˜ (o ), i.e. the C array.
τ
Parameter Information Gain In the case where the agent also maintains a variational
posterior over parameters Q(φ), the timestep- and policy-dependent EFE has an augmented
form, since it needs to account for the expected information gain over both hidden states
and parameters [47]:
G (π) = E [lnQ(s ,φ|π)−lnP ˜ (o ,s ,φ|π)]
τ Q(oτ,sτ|π) τ τ τ
≈ E [Q(s |π)−Q(s ,o |π)+Q(φ|π)−Q(φ,o |π)−lnP ˜ (o )]
Q(oτ,sτ|π) τ τ τ τ τ
= −E [D [Q(s |o ,π) (cid:107) Q(s |π)]]
Q(oτ|π) KL τ τ τ
(cid:124) (cid:123)(cid:122) (cid:125)
(State)EpistemicValue
−E [D [Q(φ|o ,π) (cid:107) Q(φ|π)]]−E [lnP ˜ (o )] (8)
Q(oτ|π) KL τ Q(oτ|π) τ
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
(Parameter)EpistemicValue Utility
So now the EFE is supplemented with an additional epistemic value, the so called
‘parameter’ epistemic value or ‘parameter information gain’. This additional term arises
when the approximate posterior includes variational beliefs about model hyperparameters:
Q(s ,φ) = Q(s )Q(φ). The optimization of the posterior over model parameters Q(φ) is
τ τ
handled in the next section on the learning.py module. This presence of this term in the
expected free energy mediates what’s also been referred to as ‘active learning’ or ‘model
exploration’, i.e. the drive to resolve uncertainty about the parameters of one’s generative
model [45].
In the discrete state space case implemented in pymdp, this parameter epistemic value
is computed with respect to the Dirichlet parameters (conjugate priors over categorical dis-
tributions) that parameterise the prior and approximate posterior over the likelihoods and
priors over the generative model, i.e. the A, B, C and D arrays. This is implemented as of the
time of writing (December 2021) for information gain about the parameters of the A array
and B array, parameterised respectively by the Dirichlet conjugate priors pA and pB. The
relevant functions for computing these information gains are calc_pA_info_gain(). and
calc_pB_info_gain().
Policy Inference Given the definition of the expected free energy in Equations (6) and
(7), we now are equipped to describe posterior inference over policies, i.e., how to obtain
Q(π).
We begin by expanding the variational free energy F as defined in Equation (2), dropping
parameters φ for simplicity:
32
F = E )[lnQ(s ,π)−lnP(o ,s ,π)]
1:T Q(s1:T,π 1:T 1:T 1:T
T
(cid:88)
= E [lnQ(π)+ lnQ(s |π)−lnP(π)−lnP(o ,s |π)]
Q(s1:T,π) τ 1:T 1:T
τ=1
= D [Q(π) (cid:107) P(π)]+E [F(π)] (9)
KL Q(π)
where the variational free energy of a particular policy F(π) is defined as follows:
F(π) = −E [lnP(o ,s |π)−H[Q(s |π)] (10)
Q(s1:T|π) 1:T 1:T 1:T
The optimal posterior that minimizes the full variational free energy F is found by taking
the derivative of F with respect to Q(π) and setting this gradient to 0, yielding the following
free-energy-minimizing solution for Q(π):
Q∗(π) = argmin F = σ(lnP(π)−F(π)) (11)
Q(π)
where the prior over policies P(π) is the softmax of the negative expected free energy
σ(−G(π)). Note that in the case of "temporally deep" or multi-timestep policies, the ex-
pected free energy of a given policy G(π) is the sum of the timestep-specific expected free
energies:
(cid:88)
G(π) = G (π) (12)
τ
τ
In pymdp and the DEM toolbox of MATLAB, one has the option of augmenting the prior
over policies with a ‘baseline policy’ or ‘habit vector’ P(π ), also referred to as the E vector.
0
This means the full expression for the optimal posterior can be written as (expanding lnP(π)
as lnP(π )−G(π)):
0
Q∗(π) = σ(−G(π)+lnP(π )−F(π)) (13)
0
This means the inferred policy distribution combines influences from the expected free
energyofeachpolicy(G ), abaselinepriorprobabilityassignedtoeachpolicy(lnP(π ))and
π 0
the variational free energy of each policy (F(π)). Numerically, policy inference is achieved
by computing the expected and variational free energies of each policy and then combining
them with the policy prior (the E vector) before softmaxing them. The expected free energy
is computed per policy as the integral of the timestep-specific expected free energies, as
shown in Equation (7). This is achieved by computing the ‘posterior predictive densities’
expected under each policy: Q(o ,s |π) and using those densities to compute and add
t:T t:T
33
together the epistemic value and utility for each policy. These posterior predictive densities
are simply the posterior beliefs at the current timestep t ‘multiplied through’ the transition
and observation models (in code: the A and B arrays) over the temporal horizon of the policy.
By doing this iteratively across policies, the G vector ends up storing a ‘cost’ for each policy,
which is then integrated with the policy prior and variational free energy of each policy to
determine the posterior probability of each policy, stored in Q(π). This boils down to the
following single line of NumPy code:
q_pi = softmax(-G + np.log(E) - F)
Inpymdp,thefunctionsupdate_posterior_policies()andupdate_posterior_policies
_full() of the control module perform the calculations needed for policy inference, and
themselves are called by the infer_policies method of Agent.
learning.py
In this section we will summarize the functions in the learning module and then derive the
update equations for updating model parameters of the likelihood and prior distributions
that comprise POMDP generative models.
The functions used to implement inference over model parameters can be found in
the learning.py file. These functions are called by methods of Agent like update_A(),
update_B(), and update_D(). We survey the most important functions of the learning
module below:
• update_obs_likelihood_dirichlet(pA, A, obs, qs, lr=1.0, modalities="all"):
This function computes the posterior Dirichlet parameters Q(A) over the A array or
observation model P(o |s ,A). As input arguments this function requires the current
τ τ
Dirichlet prior pA over the parameters of the A array, the current value of the categor-
ical A array (which is also the expected value of the Dirichlet prior pA), an observation
obs, the current posterior beliefs about hidden states qs, a learning rate lr and a list of
which observation modalities to update, modalities. The default setting is to update
theAarraysassociatedwithallobservationmodalities(modalities = "all"), butthis
extra argument allows one to only update specific sub-arrays of a larger multi-modality
A array. For example, modalities = [0, 1] would only update sub-arrays A[0] and
A[1]. The learning rate parameter scales the size of the update to the posterior over
the A array.
• update_state_likelihood_dirichlet(pB, B, actions, qs, qs_prev, lr=1.0,
factors="all"): This function computes the posterior Dirichlet parameters Q(B)
over the B array or transition model P(s |s ,u ,B). As input arguments this
τ τ−1 τ−1
function requires the current Dirichlet prior pB over the parameters of the B array, the
current value of the categorical B array, the posterior beliefs about hidden states at the
current timestep qs, the posterior beliefs about hidden states at the previous timestep
qs_prev, alearningratelrandalistofwhichhiddenstatefactorstoupdate, factors.
34
The default setting is to update the B arrays associated with all hidden state factors
(factors = "all"), but this extra argument allows you to only update specific sub-
arrays of a larger multi-factor B array. For example, factors = [0, 1] would only
update sub-arrays B[0] and B[1]. The learning rate parameter scales the size of the
update to the posterior over the B array.
• update_state_prior_dirichlet(pD, qs, lr=1.0, factors="all"): This function
computes the posterior Dirichlet parameters Q(D) over the D array or prior over initial
hidden states P(s |D). As input this function requires the current Dirichlet prior pD
0
over the parameters of the D array, the posterior beliefs about hidden states at the
current timestep qs, a learning rate lr and a list of which hidden state factors to
update, factors. The default setting is to update the D vectors associated with all
hidden state factors (factors = "all"), but this extra argument allows you to only
update specific sub-vectors of a larger multi-factor D array. For example, factors =
[0, 1] would only update sub-arrays D[0] and D[1]. The learning rate parameter
scales the size of the update to the posterior over the D array.
Inference of POMDP model parameters Under active inference, learning is cast as
inference about model parameters, and in the context of neuroscience is often analogized to
slower-scale changes to inter-neuronal synaptic weights (e.g. Hebbian learning) [36]. Param-
eter inference is referred to as ‘learning’ because it is often assumed to occur on a funda-
mentally slower timescale than hidden state and policy inference [44]. However, the update
equations for model parameters follow the exact same principles as hidden state inference -
namely, we optimize a variational posterior over model parameters Q(φ) by minimizing the
variational free energy F.
For the POMDP generative models used in pymdp, learning manifests as posterior infer-
ence over hyperparameters of the (categorical) likelihood and priors of the generative model.
We use Dirichlet distributions as conjugate priors for the categorical distributions5, meaning
that the hyperparameters φ become the parameters of Dirichlet distributions. This choice of
parameterization results in remarkably simple and biologically-plausible updates for the pos-
teriors over these parameters, wherein ‘fire-together-wire-together’-like Hebbian increments
are used to learn the parameters as a function of observations. Below we derive the update
rule for Dirichlet hyperparameters over the A, B, and D arrays.
To begin, we augment the POMDP generative model in (1) with the parameters of the
likelihood and prior categorical distributions and Dirichlet priors over each of them. In
order to do this, we divide the hyperparameters φ in the into subsets that correspond to the
categorical and Dirichlet parameters over the A, B, and D arrays:
5a prior is called conjugate to a likelihood when the resulting posterior is the same distribution family as
the prior
35
φ = {A,a,B,b,D,d}
P(o |s ,A) = Cat(A)
τ τ
(cid:89)
P(A) = P(A ), P(A ) = Dir(a )
•j •j •j
j
P(s |s ,u ,B) = Cat(B)
τ τ−1 τ−1
(cid:89)(cid:89)
P(B) = P(B ), P(B ) = Dir(b )
•ju •ju •ju
j u
P(s |D) = Cat(D)
1
P(D) = Dir(d) (14)
where the notation X denotes the jth column of a matrix X. Under this parameteri-
•j
sation, A, B, and D are arrays of categorical parameters (i.e. probabilities) that ‘fill out’
the entries of the A, B, and D arrays respectively. The Dirichlet parameters a, b, and d are
similarly the parameters of Dirichlet priors over these categorical distributions, and have
identical dimensionality to the distributions they parameterise. The Dirichlet parameters
are constrained to be positive real numbers (a,b,d ∈ R ) that score the prior probability
>0
of each entry of the categorical distribution they parameterize. Dirichlet values, like the
parameters of other common conjugate prior distributions, can be interpreted as ‘pseudo-
counts’ measuring how often a particular outcome level is expected a priori (e.g. the prior
probability assigned to a particular state-observation coincidence in the case of the A dis-
tribution). Note that for notational convenience we assume the generative model is not
factorized into multiple hidden state factors and observation modalities, but for generality
one could add in additional indices to capture multiple hidden state factors and observa-
tion modalities. For instance, the most general form of a (potentially multi-modality and
multi-factor) observation model would be:
P(o |s ,A) = {Cat(A1),Cat(A2),...,Cat(AM)}
τ τ
(cid:89)
P(Am) = P(Am ), P(Am ) = Dir(am )
•jk... •jk... •jk...
j,k,...
Given the introduction of the new Dirichlet priors in Equation (14), we can now write
down the augmented generative model, where the hyperparameters φ have been split into
individual priors over A, B, and D:
T T
(cid:89) (cid:89)
P(o ,s ,π,A,B,D) = P(A)P(B)P(D)P(s |D) P(s |s ,B) P(o |s ,A) (15)
[1:T] [1:T] 1 τ τ−1 τ τ
τ=2 τ=1
Given the new generative model with Dirichlet priors, we can now formulate learning as
approximate inference about these parameters, i.e. optimizing variational posteriors over the
36
likelihood and prior parameters. We begin by expanding our expression of the variational
posterior to include beliefs over the values of the A, B, and D distributions:
T
(cid:89)
Q(s ,π,A,B,D) = Q(A)Q(B)Q(D)Q(π) Q(s |π)
[1:T] τ
τ=1
(cid:89)
where Q(A) = Q(A ), Q(A ) = Dir(a )
•j •j •j
j
(cid:89)(cid:89)
Q(B) = Q(B ), Q(B ) = Dir(b )
•ju •ju •ju
j u
Q(D) = Dir(d)
where now the variational parameters a, b, and d are Dirichlet parameters of the approx-
imate posteriors Q(A), Q(B), and Q(D), respectively. Performing inference with respect to
A, B, and D thus amounts to optimizing the variational Dirichlet parameters in order to
minimize free energy. This is what is meant by ‘learning’ in active inference.
We will now step through the update rules for each of the Dirichlet posteriors over the
A, B, and D distributions. We begin by writing down the full variational free energy:
F = E [lnQ(s ,A,B,D,π)−lnP(o ,s ,π,A,B,D)]
1:T Q(s1:T,A,B,D,π) 1:T 1:T 1:T
= E [lnQ(A)−lnP(A)+lnQ(B)−lnP(B)
Q(s1:T,A,B,D,π)
+lnQ(π)−lnP(π)+lnQ(D)−lnP(s )
1
T T
(cid:88) (cid:88)
+ lnQ(s |π)− lnP(s |s ,π,B)
τ τ τ−1
τ=1 τ=2
T
(cid:88)
− lnP(o |s ,A)] (16)
τ τ
τ=1
Learning the observation model We begin with the update rule for the Dirichlet
parameters over Q(A), i.e. updating the parameters of the observation model or A array.
We can first isolate the components of the free energy that depend on Q(A) since we’re
interested in the gradients of the free energy with respect to a, the parameters of Q(A):
T
(cid:88)
F = D [Q(A) (cid:107) P(A)]− E [lnP(o |s ,A)]+... (17)
1:T KL Q(sτ,π)Q(A) τ τ
τ=1
WecanexpandtheKLdivergencebetweenQ(A)andP(A)asfollows, usingthedefinition
of the KL divergence between Dirichlet distributions and the independence of the Dirichlet
matrices across different columns (i.e. hidden state levels):
37
(cid:88)
D [Q(A) (cid:107) P(A)] = D [Dir(a ) (cid:107) Dir(a )]
KL KL •j •j
j
(cid:32) (cid:33)
(cid:88) (cid:88) (cid:88)
= lnΓ(a )− lnΓ(a )−lnΓ(a )+ lnΓ(a )
0j ij 0j ij
j i i
+(a−a)E [lnP(o |s ,A)] (18)
Q(A) τ τ
where the terms a and a are matrices whose entries store the column-wise sums of the
0 0
Dirichlet parameters of a and a, respectively, i.e. a = (cid:80) a . We can then combine the KL
0j i ij
divergence with the remaining term in the free energy that depends on Q(A) and take the
gradients of the free energy with respect to E [lnP(o |s ,A)], which we hereafter refer to
Q(A) τ τ
as lnA:
(cid:32) (cid:33)
(cid:88) (cid:88) (cid:88)
F = lnΓ(a )− lnΓ(a )−lnΓ(a )+ lnΓ(a )
1:T 0j ij 0j ij
j i i
T
(cid:88)
+(a−a)E [lnP(o |s ,A)]− E [lnP(o |s ,A)]+...
Q(A) τ τ Q(sτ,π)Q(A) τ τ
τ=1
T
∂F (cid:88)
=⇒ = a−a− o ⊗s
τ τ
∂lnA
τ=1
(cid:88)
where s = Q(s |π)Q(π) (19)
τ τ
π
where ⊗ denotes the outer product and s is also known as the Bayesian model average
τ
of hidden states, where the average is taken with respect to the posterior over policies, Q(π).
Thismoveiswhatallowsonetoconvertfromthefullposterioroverhiddenstatesandpolicies
Q(s ,π) to a hidden state representation that is not conditioned on policies.6 If we set the
1:T
gradient ∂F equal to 0 and solve for a, then we obtain the fixed-point solution for the
∂lnA
variational posterior:
T
(cid:88)
a∗ = a+ o ⊗s (20)
τ τ
τ=1
Note that the use of the gradient with respect to lnA instead of a directly is sufficient
for deriving the update rule [68]. This can be seen by rewriting lnA as ψ(a)−ψ(a ), where
0
ψ is the component-wise digamma function [44]. The digamma function is monotonically
increasing in a, meaning ∂F and ∂F have the same minima. This learning rule is imple-
∂lnA ∂a
mented by the function update_obs_likelihood_dirichlet() in the learning module,
which itself is wrapped by the update_A() method of Agent.
6The Bayesian model average can be computed using the function average_states_over_policies()
in the inference module.
38
Learning the transition model The updates for the Dirichlet posterior Q(B) are de-
rived similarly to those for Q(A), starting from the full expression for the variational free
energy, isolating those terms that only depend on b:
T
(cid:88)
F = D [Q(B) (cid:107) P(B)]− E [lnP(s |s ,π,B)]+...
1:T KL Q(sτ,sτ−1,π)Q(B) τ τ−1
τ=2
T
= D [Q(B) (cid:107) P(B)]− (cid:88) E (cid:2) Q(s |π)TE [lnP(s |s ,u ,B)]Q(s |π) (cid:3)
KL Q(π) τ Q(B) τ τ−1 τ τ−1
τ=2
(21)
where u is the action expected at time τ under Q(π). If we take the gradients of F
τ 1:T
with respect to lnB = E [lnP(s |s ,u ,B)] and solve for ∂F = 0, then we recover
Q(B) τ τ−1 τ ∂lnB
the variational solution for the posterior parameters b:
T
(cid:88)(cid:88)
b ∗ = b + Q(u |π)Q(π)(Q(s |π)⊗Q(s |π)) (22)
u u τ τ τ−1
τ=2 π
This update can be expressed intuitively as follows: a given timestep τ the B matrix is
updated using the outer product of the beliefs about states at τ and the beliefs about states
at τ −1. These updates are done in an action-conditioned sense, such that the update only
appliestotheuth ‘slice’ofthebDirichletparameters, dependingontheaction(s)expectedat
time τ under Q(π). In pymdp, we assume that the actions at past timesteps are known with
certainty, meaning that the term Q(u |π)Q(π) reduces to a delta function over the action
τ
actually taken, and the update for b only happens to one slice b at a time. The relevant
u
function for implementing learning Q(B) is update_transition_likelihood_dirichlet()
in learning and the update_B() method of Agent.
Learning the state prior Finally, we can write down the updates for the Dirichlet
posterior over initial hidden states Q(D) using the same formalism as used for Q(A) and
Q(B). First, we find the terms of the free energy that depend on d:
F = D [Q(D) (cid:107) P(D)]−E [Q(s |π)]TE [lnP(s |D)]+... (23)
1:T KL Q(π) 1 Q(D) 1
Taking the gradients of F with respect to lnD = E [lnP(s |D)] and setting the
Q(D) 1
gradient to 0 yields the following fixed-form solution for d:
d∗ = d+E [Q(s |π)] (24)
Q(π) 1
TherelevantfunctionforimplementinglearningQ(D)isupdate_state_prior_dirichlet()
in learning and the update_D() method of Agent.
For more complete versions of each of these derivations, we refer the reader to Section 8
and Appendix A.1 of Da Costa et al. 2021 [36].
39

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "pymdp: A Python library for active inference in discrete state spaces"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
