=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Lightweight error mitigation strategies for post-training N:M activation sparsity in LLMs
Citation Key: alanova2025lightweight
Authors: Shirin Alanova, Kristina Kazistova, Ekaterina Galaeva

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Abstract: The demand for efficient large language model (LLM) inference has intensified
the focus on sparsification techniques. While semi-structured (N:M) pruning is
well-establishedforweights,itsapplicationtoactivationpruningremainsunder-
exploreddespiteitspotentialfordynamic,input-adaptivecompressionandreduc-
tions in I/O overhead. This work presents a comprehensive analysis of methods
for post-training N:M activation pruning in LLMs. Across multiple LLMs, we
demonstrate that pruning activations enable...

Key Terms: pruning, activation, strategies, mitigation, post, error, sparsity, lightweight, equalcontribution, llms

=== FULL PAPER TEXT ===

September2025
LIGHTWEIGHT ERROR MITIGATION STRATE-
GIES FOR POST-TRAINING N:M ACTIVATION
SPARSITY IN LLMS
ShirinAlanova KristinaKazistova EkaterinaGalaeva AlinaKostromina
equalcontribution equalcontribution equalcontribution equalcontribution
VladimirSmirnov RedkoDmitry AlexeyDontsov MaximZhelnin
EvgenyBurnaev EgorShvetsov
ABSTRACT
The demand for efficient large language model (LLM) inference has intensified
the focus on sparsification techniques. While semi-structured (N:M) pruning is
well-establishedforweights,itsapplicationtoactivationpruningremainsunder-
exploreddespiteitspotentialfordynamic,input-adaptivecompressionandreduc-
tions in I/O overhead. This work presents a comprehensive analysis of methods
for post-training N:M activation pruning in LLMs. Across multiple LLMs, we
demonstrate that pruning activations enables superior preservation of generative
capabilities compared to weight pruning at equivalent sparsity levels. We eval-
uate lightweight, plug-and-play error mitigation techniques and pruning criteria,
establishing strong hardware-friendly baselines that require minimal calibration.
Furthermore,weexploresparsitypatternsbeyondNVIDIA’sstandard2:4,show-
ing that the 16:32 pattern achieves performance nearly on par with unstructured
sparsity. However,consideringthetrade-offbetweenflexibilityandhardwareim-
plementation complexity, we focus on the 8:16 pattern as a superior candidate.
Our findings provide both effective practical methods for activation pruning and
a motivation for future hardware to support more flexible sparsity patterns. Our
codeisavailablehere.
1 INTRODUCTION
TheexpandingcapabilitiesofLargeLanguageModels(LLMs)havedrivenenormousdemandfor
efficientAIinference.Acommonruleofthumbstatesthatinferenceservingspeedforadensemodel
√
withN parametersscalesas∝1/ N (Erdil,2025).Acceleratinginferencetypicallyinvolveseither
reducing numerical precision via quantization to mitigate memory bottlenecks and enable faster
arithmetic (Shvetsov et al., 2024) or sparsification to reduce the number of parameters (Maximov
etal.,2025). SparsityimprovesLLMefficiencyintwokeyways: (1)byreducingcomputation,and
(2)byreducingI/Otrafficfortransferringparametersbetweenmemoryandcomputeunits–amajor
bottleneckduringinference.
Weights vs. Activations. While sparse weights and activations offer identical theoretical FLOPs
(floating point operations count), their practical implications differ. Weight sparsity enables static
compression,efficientlyreducingmodelstorageandmemorybandwidth. However,itsstaticnature
often causes irreversible model degradation. In contrast, activation sparsity is dynamic and input-
adaptive,preservingmodelcapacity,whichwedemonstrateinourwork.
Sparsity in LLMs. Naturally, pruning is most effective when the values to be pruned already
havelowmagnitudesorarezero,whichisoftenthecaseforsomeLLMs’intermediaterepresenta-
tions (Liu et al., 2023), especially due to the ReLU activation function in MLP blocks (Mirzadeh
etal.,2023). Evenifamodelwastrainedwithanon-ReLUactivationfunction,wecanstillperform
1
5202
peS
62
]GL.sc[
1v66122.9052:viXra
September2025
relufication Song et al. (2024b) to induce sparsity. This makes activation sparsification a natural
choiceforenhancingthemodel’sperformance.
Accelerating LLMs with sparse activations. Indeed, recent results demonstrated that activation
sparsificationsupportedbyspecializedsoftwarecanincreasethemodel’svectorbymatrixmultipli-
cation during the LLM decoding stage up to 2×times (Song et al., 2024b;a; Liu et al., 2024; Lee
etal.,2024). Theideabehindsuchapproachesisrelativelystraightforward-ifavectorhasmany
zeroes,wecandisregardcorrespondingcolumnsinaweightmatrixW onthefly.However,asbatch
sizegrowsbeyondone, itbecomeschallengingtogainbenefits(Shresthaetal.,2025). Moreover,
mostpreviousmethodsmainlyrelyonsparsityinducedonlybyactivationfunctions(Mirzadehetal.,
2023)inMLPblocks.
Semi-structured N:M sparsity for activations, where each N elements out of a sub-vector of M
are zeroes, would provide computational gains beyond single vector×matrix computations. This
approachisatrade-offbetweentheperformanceretentionofunstructuredpruning(Zhuetal.,2016;
Paul et al., 2022) and the hardware-friendly acceleration of structured pruning (Liu et al., 2017;
Molchanov et al., 2019), motivating its use for efficient inference (Hubara et al., 2021). Several
worksdemonstratethat2:4sparsitypatternsforactivations,combinedwithSquared-ReLU,canac-
celerate transformer training (Haziza et al., 2025; Wang et al., 2024). Our work is most closely
alignedwithAmber-Pruner(Anetal.,2025b),whichapplies{2 : 4,4 : 8,8 : 16}semi-structured
activation sparsity patterns to already trained models. The authors propose a specific pruning cri-
terion - Amber Pruner and a layer-skipping strategy to avoid sensitive layers. We include Am-
ber Pruner’s criterion in our comparative analysis. While semi-structured weight-pruning is well-
establishedforinferenceefficiency(Hanetal.,2015;Frantar&Alistarh,2023;Kurtic´ etal.,2023),
dynamic activation sparsity remains under-explored despite its theoretical potential (Baykal et al.,
2022),andAmber-Pruner(Anetal.,2025b)currentlyistheonlypaperthatexplorespost-training
sparsificationofactivationswithvarioussparsitypatterns,whichistheprimaryfocusofourwork.
Focus and Motivation. Applying N:M sparsity patterns to activations, while conceptually
straightforward, raises several key questions: (Q1) Selection Strategy: As with weight sparsi-
fication, the criterion for selecting which activations to prune is crucial for maintaining perfor-
mance (Zhelnin et al., 2024). (Q2) Error Mitigation: Fine-tuning is a standard method for re-
coveringperformancelosttopost-trainingsparsification,butcanbeprohibitivelyexpensiveorrisk
compromisingmodelsafetyalignment(Kharinaevetal.,2025).Therefore,wefocusonlightweight,
plug-and-play and hardware-friendly approaches that require only minimal calibration data (e.g.,
WikiText)ornodataatalltoenhanceperformance. (Q3)Beyond2:4Patterns: While2:4isthe
onlyhardware-supportedsemi-structuredpattern,weexploreabroaderrange{2:4,4:8,8:16,16:32}
tomotivatefuturehardwaredesign. Whilewecannotdirectlycompareaccelerationgainsbetween
variouspatterns,ourworksolelyfocusesonmodelperformance.
OurContributions. Thispapermakesthefollowingkeycontributions:
• Activation vs. Weight Sparsity Superiority: We show that, at similar sparsity levels,
activation pruning consistently outperforms weight pruning across four diverse LLMs:
Llama2-7B,Llama3-8.1B,Qwen2.5-7B,andGemma3-4B.
• Lightweight Error Mitigation and Selection Criteria: We comprehensively evaluate
four plug-and-play error mitigation techniques, three evaluated for the first time in the
context of activation sparsity. We also evaluate three pruning selection criteria. These
methodsincludestatisticalcriteriasuchasmedianshiftandvariancecorrectionandprovide
strong,hardware-friendlybaselinesforactivationsparsification.
• ExplorationofNovelSparsityPatterns:Weevaluatetheperformanceofsparsitypatterns
2:4, 4:8, 8:16, 16:32. Wefindthatthemoreflexible16:32patternachievesperformance
closetounstructured50%sparsityandis3×betterthan2:4. However,consideringhard-
ware implementation, we advocate for the 8:16 pattern, which offers a 2× improvement
over2:4whileremaininghighlypractical.
2
September2025
2 HARDWARE AND COMPARISON OF 2:4 AND 8:16 SPARSITY
Oneoftheprimaryaimsofourworkistomotivatethedesignofhardwareformoreflexiblesparsity
patterns. Currently, only NVIDIA GPUs support structured sparsity, specifically the 2:4 pattern.
This pattern achieves 1.5–1.7x inference acceleration and 1.5x energy reduction for 7B models,
primarily from a 2x reduction in memory bandwidth (Fang et al., 2024; Lin et al., 2023). At the
sametime,anyN:Mpatternbeyond2:4,including8:16,canprovidethesame2xbandwidthreduc-
tionbutwithgreaterflexibility. InthegeneralN:M format,2:4sparsitypermits
(cid:0)4(cid:1)
= 6non-zero
2
patternsperblock,requiringlog (6)/4 ≈ 0.75bitsperelementformetadata. Incontrast,the8:16
2
patternallows
(cid:0)16(cid:1)
=12,870distinctconfigurations,demandinglog (12,870)/16≈0.875bitsper
8 2
element. Stacking four 2:4 blocks to form a 16-element unit yields only 64 = 1,296 configura-
tions—nearlyanorderofmagnitudefewerthanthe12,870offeredbyanative8:16block. Although
8:16requiresslightlymoremetadatastorage,itsvastlygreaterconfigurationalflexibilitymakesita
compellingcandidate. Themaintrade-offisimplementationcomplexity. Decodingsparsitymeta-
data and gathering non-zero values requires sophisticated circuitry. As the block size M grows,
these gather operations could impact performance. However, operating on larger, aligned blocks
couldalsoimprovecacheutilization,potentiallyamortizingthisoverhead.
Figure 1: Comparison of unstructured spar- Figure2: Comparisonofsparsitypatternswith
sity in activations (ACT) and weights (WT) unstructuredsparsity. 50%and70%correspond
averagedacrossfourdatasetsatvaryingsparsity tounstructuredsparsity. LowerisBetter. More
ratios. Higher is Better. More detailed results detailed results are presented in Appendix Ta-
arepresentedinAppendixTable9. ble6.
3 METHODS
3.1 PRELIMINARIES
ConsideralinearlayerofaLLMwithweightW,inputactivationsXandoutputYthen:
Y =XW⊤ (1)
Ouraimistofindasemi-structuredpruningmaskMfortheactivationsXbasedonsomepruning
metricS,suchthat:
(cid:26)
1, S(X )≥t
M = ij (2)
ij 0, S(X )<t
ij
where i,j denote the indices of the matrix elements, t is a threshold value chosen to ensure the
desiredlevelofsparsity.
3
September2025
Table1: Brieflydescribeevaluatedactivationpruningmetrics(top)andtransformations(bottom).
Abbreviations in bold with an asterisk (*) denote methods proposed here or first evaluated with
sparseactivations.
ShortName Method KeyMechanism
Pruningmetrics
ACT MagnitudePruning Selectsbasedonactivationmagnitude
WT Weight-basedPruning Selectsbycorrespondingweightmagnitude
CLACT* CosineLossActivation AmetricinspiredbycosinesimilarityfromMietal.(2025)
Amber-Pruner(2025a) WeightsImportance Ametricwhichaccountsforimportantweightsafteroutlierremovalandnormalization
Transformations
D-PTS* DynamicPer-TokenShift Batch-wisedynamiccenteringofactivationsbeforesparsification
S-PTS(2024) StaticPer-TokenShift Fixedcenteringofactivationsbeforesparsificationusingaper-tokenbiasvaluepre-collected
onWikiText-2
L-PTS* LearnablePer-TokenShift Fixedcenteringofactivationsbeforesparsificationusingper-tokenbiasvaluelearnedon
WikiText-2
VAR* VarianceCorrection Token-wisevariancenormalizationaftersparsification
VAR+L-PTS* Scaling+LearnableShift ApplyVARscaling,thenaddper-tokenbiasvaluelearnedonWikiText-2
R-Sparse(2025) Rank-AwareSparsity Combinessparseactivationswithweightlow-rankSVDfactorslearnedonWikiText-2
OncethemaskMiscomputed,theoutputofthelinearlayerwithprunedactivationsisgivenby:
Y =(X⊙M)W⊤ (3)
p
To build unstructured sparsity, we apply a global magnitude threshold to every element of the
activations(orweights)andzero-outthosebelowthethreshold.Insemi-structured2:4sparsity,we
spliteachrow(orcolumn)ofthematrixintonon-overlappingblocksoffourconsecutiveelements;
ineveryblock, wekeepthetwomostimportantentriesbyachosenimportancemetricandsetthe
rest to zero, thereby removing 50% of the elements (Hu et al., 2024). Likewise, we can construct
semi-structured 8:16 sparsity, achieving the same 50% density but providing greater flexibility
withminimalstorageoverhead(Maximovetal.,2025).
3.2 PRUNINGCRITERION
Inthisstudy,weprimarilyconsidersomepruningmetrics. Youcanfindbriefdescriptionsofthem
in1.
ACT:Thisisthemagnitudeactivationpruningmetric(ACT),definedastheabsolutevalueofthe
elementX :S (X )=|X |.
ij ACT ij ij
WT:Thisistheweight-basedpruningmetric(WT),definedastheabsolutevalueofthecorrespond-
ingweightW : S (W )=|W |.
ij WT ij ij
CLACT:Thismetricisinspiredby(Mietal.,2025),whichmeasuresthecosinesimilaritybetween
layeroutputs. Buildingonthisconcept,weproposeanovelmetriccalledCosineLossACTivation
(CLACT).ThisapproachapproximatesthecosinedistancebetweenanactivationX anditsfeature
ij
vector, quantifying each element’s importance within its activation row. Unlike magnitude-based
methods, CLACTidentifiesactivationsthatarecontextuallysignificantratherthanmerelylargein
absolutevalue.
(cid:118)
(cid:117) l
S CLACT (X ij )= (cid:113) |X ij | (cid:117) (cid:116) (cid:88) X2 pj , (4)
(cid:80)h X2 p=1
k=1 ik
wherehishiddendimension,lissequencelength,jisatokennumberandianelementwithingthis
token.
Amber-Pruner: ThismetricfromAnetal.(2025b)consistsofthefollowingsteps: (1)OutlierRe-
moval.Toreducetheinfluenceofextremevalues,weightsWoutsidethe0.5th−99.5thpercentiles
are discarded: W = {ω |Q (W)≤ω ≤Q (W)}. (2) Normalization. The remaining
k 0.005 k 0.995
weights are standardized using their mean and variance: Wˆ
ij
= (W
ij
− E[W])(Var[W])−1 2.
(3) Channel-wise Scoring. For each channel Wˆ , the ℓ norm is computed, and final scores are
:,j 2
assigned to activation elements, L(·) - denotes channel-wise ℓ norm: S (X ) = |X | ·
2 Amb.-Pr. ij ij
(cid:16) (cid:17)
L Wˆ .
:,j
4
September2025
Here, wehighlighta criticaldistinctionbetweenCLACTandAmber-Pruner. CLACTisa context
aware criterion, particularly useful during the prefill stage since with l = 1, it converges to the
L1-normcriterion,moreover,itdoesnotconsiderweightmagnitudes. Incontrast,Amber-Pruner
explicitly prioritizes weights based on their values and remains effective across both prefill and
generationstagesbutdoesnotconsidercontext.
3.3 ERRORMITIGATIONANDTRANSFORMATIONS
Since semi-structured pruning of activations can significantly disrupt the outputs of linear layers,
additional transformations can be applied to mitigate these negative effects and restore the pruned
activationstotheiroriginaldistribution. Youcanfindbriefdescriptionsofthemin1.
PCS:ThisapproachisinspiredbySmoothQuant(Xiaoetal.,2023)andhasbeenusedfor2:4weight
sparsification(Maximovetal.,2025)aswell.Itusesaper-channelsmoothing(PCS)factorsapplied
toactivationsXbeforepruning: Xˆ =diag(s(X,W))−1X. Tocompensateforthistransformation,
theweightmatrixiscorrespondinglyscaledintheoppositedirection. Asaresult,theoutputofthe
linearlayeriscomputedas: Y =(Xˆ ⊙M)(diag(s(X,W))W⊤). Thescalefactorsisestimated
p
(cid:113)
asXiaoetal.(2023): s= max|X |(max|W⊤|)−1
:,j :,j
D-/S-/L-PTS:Thenextapproachappliesadynamicper-tokenshift(D-PTS)totheinputactivations
Xusingabiastermη,suchthatXˆ = X−η. Thepurposeofthisshiftistocentermostelements
oftheactivationsnearzero. Asacompensatorytransformation,thebiasη isaddedtotheweights,
sotheoutputofthelinearlayerisgivenbyChuaetal.(2024): Y = ((Xˆ ⊙M)+η)W⊤,where
p
η =X. Arelatedvariantisthestaticper-tokenshift(S-PTS),wherethebiasηiscollectedduringa
shortwarm-upphaseandthenfixedforallsubsequentbatches.Thisreducescomputationaloverhead
whileprovidingsimilarcenteringeffects. Finally,weintroducealearnableper-tokenshift(L-PTS).
VAR:Incontrasttothepreviousapproach,thismethodappliesaper-tokenscalingfactorνtopruned
activations such that the output matrix is given as: Y = ν(X⊙M)W⊤, The scale factor ν is
p
determinedas
(cid:115)
Var[X]
ν = (5)
Var[X⊙M]
WerefertothismethodasvariancecorrectionVAR.
VAR+L-PTS:Thismethodisacombinationoftheper-tokenscalingwithfactorVARν isgivenby
Eq. 5andtheshiftL-PTS.
R-Sparse: Thismethodfrom (Kamiruletal.,2025)combinesactivationsparsitywithalow-rank
approximationoftheweightmatrix. WeprovidemoredetailsinAppendixA.
Throughout our experiments we use the WikiText-2 dataset for calibration and learning some pa-
rameters. For S-PTS we calibrate a single per-channel bias vector η and then keep it fixed for all
subsequentbatches. ForthelearnablevariantL-PTS(andthecombinedVAR+L-PTS)wetrainη
and fine-tune other weights. For R-Sparse we train in the same way the low-rank factors of the
weightadaptation. Inallothermethodsnoparametersarelearnedorcalibrated.
3.4 EVALUATION&MODELS
WebeginwithabroadfunnelofexperimentstoevaluateallmethodslistedinTable1. Atthisinitial
stage, we use the Core Datasets, Single-Choice and Multi-Choice benchmarks including BoolQ,
WinoGrande,PIQA,andARC-Easy. Oncethisexploratoryphaseiscomplete,wenarrowourfocus
tothemostpromisingapproaches,evaluatingthemprimarilyontwomodels: Qwen2.5-7B-Instruct
and Llama3-8B-Instruct. This more targeted evaluation phase not only refines the set of methods
underconsiderationbutalsoexpandstheevaluationtotheExtendedDatasets: HellaSwag,Open-
BookQA, RTE, MMLU, Lambada standard, Lambada openai, and IFEval. For calibration tasks,
when required, we use WikiText-2. This dataset selection follows established model compression
practices Egiazarian et al. (2024); Frantar et al. (2022); van Baalen et al. (2024). All evaluations
are performed using the LM Eval Harness Gao et al. (2023), and full dataset details are provided
in Table 8. For the models we focus on: Qwen2.5-7B-Instruct, Llama3-8B-Instruct, Llama2-7B,
5
September2025
Gemma3-4B-Instruct. Importantly,forQwen2.5-7B-Instruct,wedidnotsparsifythekey,query,or
value activations, as preliminary experiments showed severe model degradation when these layers
werepruned.
4 RESULTS
4.1 SPARSEWEIGHTSVS. SPARSEACTIVATIONS
In Figure 1 and Table 9, we demonstrate that unstructured weight sparsification causes greater
model degradation than unstructured activation sparsification at the same sparsity levels: {20%,
50%,70%,90%}. Forthisevaluation,wespecificallyuseunstructuredmagnitude-basedsparsifica-
tion, asitislessdamagingthansemi-structuredsparsificationandthusprovidesalowerboundon
performancedegradation.
4.2 OPTIMALSEMI-STRUCTUREDSPARSITYPATTERNS
Ourpreliminaryinvestigationdemonstratesthatwhilethe16:32patternachievesperformanceclose
tounstructuredsparsity(a5.4%dropversus4.5%for50%unstructured),itrequiresmoremetadata
and greater resources for gather operations, as discussed in Section 2. Therefore, we focus on the
8:16pattern,despiteitshigherperformancedropof7.38%. Forcomparison,the2:4patternresults
in a 14.35% drop. These results are shown in Figure 2 and Table 6 in Appendix, we used only
magnitude pruning to obtain these results. By demonstrating the superior model quality of 8:16
sparsity,ourworkincentivizeshardwaredesignerstoinvestin—oratleastconsider—supportingthe
8:16pattern.
4.3 EVALUATIONOFPRUNINGSELECTIONCRITERIAONSINGLE/MULTI-CHOICEDATASETS
WeevaluateCLACT,Amber-Pruner,andmagnitudepruningasabaseline. Themainresultsforthe
2:4and8:16sparsitypatternsarepresentedinTable2.Onaverage,bothCLACTandAmber-Pruner
outperformmagnitudepruningbyatleast2%,however,weobservenoclearwinnerbetweenthem.
AsnotedinSection3.2,thesecriteriaaredesignedfordifferentpurposes: CLACTadjustsbasedon
context, while Amber-Pruner adjusts based on weight magnitudes. Notably, for Llama3-8B under
the2:4sparsitypattern,simplemagnitudepruningoutperformsbothadvancedcriteria,underscoring
modelandarchitecture-specificsensitivitiestopruningstrategies.
4.4 EVALUATIONOFTRANSFORMATIONSONSINGLE/MULTI-CHOICEDATASETS
Our main results are presented in Table 2. Surprisingly, we find that simple methods such as dy-
namic and static per-token shifts (D-PTS, S-PTS) outperform most other approaches. The second
most effective methods are VAR and R-SPARSE. We also observe that increasing the number of
dimensionsinR-SPARSE(from64to128)leadstoworseperformance,whichmayindicateover-
fitting on the calibration data. Finally, we note that L-PTS, the approach with learnable per-token
shifts,significantlyunderperformscomparedtoitsstaticcounterpart,S-PTS.
4.5 EVALUATIONTRANSFORMATIONSWITHINSTRUCTION-FOLLOWINGTASKS
Table 3 presents instruction-following performance on the IFEval benchmark for Llama3-8B and
Qwen2.5-7B,evaluatedundertwosemi-structuredsparsitypatterns(2:4and8:16)andfouractiva-
tiontransformationmethods: S-PTS,D-PTS,R-Sparse,andVAR.Firstofall,weobserveastrong
model degradation on generative tasks. Second of all, we see that VAR is the strongest performer
overall,especiallyforLlama3-8B.S-PTS/D-PTSarecompetitiveandlightweight,andR-Sparselags
significantly,particularlyat2:4.Wespeculatethatwhilesemi-structuredpatternsaregoodforprefill
stageinLLMstheysignificantlydegradeperformanceduringdecodestage. However,asdiscussed
inSection1decodestageforsinglevectorcanbeacceleratedwithmoreflexibleapproaches.
6
September2025
Table2: Averagerelativeperformance(%)acrossfourdatasets. Valuesindicateperformancedrops
(lowerisbetter),negativevaluessignifyperformanceimprovement. Methodsmarkedwithanaster-
isk(*)areproposedinthispaper. Full,non-aggregatedresultsareavailableinAppendix10.
Models
Method Llama2-7B Qwen2.5-7B Gemma3-4B LLama3-8B AverageDrop(↓)
ActivationsUnstructured50%
ACT 2.31% 3.87% 4.80% 4.30% 3.82%
2:4WeightSparsity
WT 16.52% 12.96% 34.86% 33.63% 24.49%
2:4ActivationsSelectionCriteria
ACT 9.43% 4.95% 9.94% 14.35% 9.67%
CLACT* 8.32% −2.45% 8.01% 17.27% 7.79%
Amber-Pruner 11.70% −1.23% 5.91% 15.01% 7.85%
2:4ActivationsTransformations
VAR* 9.76% −1.48% 2.96% 13.11% 6.09%
D-PTS* 10.67% -6.46% 4.58% 14.59% 5.84%
S-PTS 10.37% −4.43% 3.93% 7.31% 4.29%
L-PTS* 13.13% 3.66% 4.21% 14.13% 8.79%
R-SPARSE(64) 12.90% −2.55% 5.17% 15.28% 7.70%
R-SPARSE(128) 12.23% −1.51% 5.16% 16.34% 8.05%
8:16WeightSparsity
WT 7.84% 9.54% 26.11% 27.26% 17.68%
8:16ActivationsSelectionCriteria
ACT 5.37% 4.38% 4.76% 7.38% 5.47%
CLACT* 3.98% −4.02% 0.60% 8.60% 2.29%
Amber-Pruner 5.32% −6.28% 0.08% 7.13% 1.56%
8:16ActivationsTransformations
VAR* 4.85% 1.93% -1.87% 8.30% 3.30%
D-PTS* 4.63% -8.28% 5.16% 6.79% 2.07%
S-PTS 3.87% −7.24% −1.54% 7.3 % 0.61%
L-PTS* 8.15% 1.71% 4.21% 7.19% 5.32%
R-SPARSE(64) 5.91% −6.91% −1.36% 8.44% 1.52%
R-SPARSE(128) 7.93% −5.44% −0.44% 8.44% 2.63%
Table 3: Evaluation with Instruction-Following (IFEval) for Llama3-8B and Qwen2.5-7B. PS de-
notesprompt-levelstrictacc,PLdenotesprompt-levellooseacc.
Original S-PTS D-PTS R-Sparse VAR
Model Pattern
PS PL PS PL PS PL PS PL PS PL
2:4 0.1682 0.1904 0.1941 0.2015 0.0869 0.0979 0.2237 0.2458
Llama3-8B 0.4455 0.4861
8:16 0.2995 0.3327 0.2828 0.3198 0.2089 0.2311 0.3161 0.3586
2:4 0.4325 0.5176 0.4399 0.5120 0.2736 0.3457 0.4565 0.5342
Qwen2.5-7B 0.7135 0.7394
8:16 0.5194 0.5804 0.5434 0.5989 0.3697 0.4196 0.5249 0.5896
7
September2025
Table4:Performancecomparisonofpruningmethodsunder50%and70%unstructuredsparsityfor
Llama3-8Bmodel.
Model ARCEasy BoolQ PIQA WinoGrande Avg. Drop(%)
Original 0.8207 0.8391 0.8003 0.7340 —
Unstructured50%
ACT 0.7770 0.8198 0.7714 0.6858 4.45
D-PTS 0.78577 0.8253 0.7807 0.68981 3.60
VAR 0.78409 0.8192 0.77584 0.7048 3.47
CLACT 0.7803 0.8253 0.7655 0.70007 3.89
Amber-Pruner 0.768 0.82018 0.7627 0.7016 4.45
Unstructured70%
ACT 0.55808 0.63119 0.6474 0.5477 25.32
D-PTS 0.56481 0.62415 0.6507 0.53433 25.68
VAR 0.61447 0.65107 0.67573 0.5319 22.66
CLACT 0.5551 0.6039 0.62676 0.52407 27.67
Amber-Pruner 0.48737 0.5938 0.5897 0.539 30.68
4.6 SELECTIONCRITERIAANDTRANSFORMATIONSFORUNSTRUCTUREDPRUNING
Although unstructured pruning offers limited efficiency gains compared to structured variants, it
servesasavaluableproxytaskforevaluatingtherobustnessandgeneralizationofproposedmethods.
WeevaluateD-PTS,VAR,andtwoselectioncriteriainthisexperiment: CLACTandAmber-Pruner
usingtheLlama3-8Bmodel. ResultsinTable4indicatethatVARisthemosteffectivetransforma-
tionunderunstructuredsparsity. Moreover,CLACToutperformsAmber-Prunerbyawidermargin
here than in our semi-structured pruning experiments. These findings suggest two key insights:
(1) No single method emerges as optimal for both unstructured and semi-structured sparsity. (2)
The methods proposed in this work, VAR and CLACT, demonstrate strong generalization and are
well-suitedforunstructuredactivationpruning.
4.7 COMBINATIONOFTRANSFORMATIONSANDPRUNINGCRITERIA
Next, we evaluated combinations of multiple approaches to explore potential performance gains.
ThesecombinationsandtheirresultsarepresentedinTable7.Asshown,noneoftheevaluatedcom-
binations outperforms any single method, highlighting the challenges of naively combining them.
Table5:AggregatedresultsonLlama3-8Bwith8:16activationsparsitywithdifferentlayerspruned.
ORIG. AVG. denotes original model performance. Layers indicates the subset of linear layers
where the method was applied. Non aggregated results are presented in Appendix 12. Drop is
computedwithoutaccountingforperplexity.
ORIG.AVG. Method Layers PPL AVG. Drop↓
LS+L-PTS all 9.6036 0.6047 10.90%
LS+L-PTS key,out,gate,down 8.3483 0.6385 5.43%
0.6726 LS+L-PTS key,value,gate,down 8.0821 0.6503 3.56%
LS+L-PTS+VAR all 9.4983 0.6056 10.60%
LS+L-PTS+VAR key,out,gate,down 8.2930 0.6422 4.64%
LS+L-PTS+VAR key,value,gate,down 8.0259 0.6516 3.36%
4.8 LAYERSENSITIVITY
Thenextquestionweinvestigateinthisworkislayersensitivitytoactivationsparsity. Forthisex-
periment,weusetheExtendedDatasets,theLlama3-8Bmodel,andthe8:16sparsitypattern. The
8
September2025
main results are presented in Table 5. Here, we focus on methods with learnable parameters, pri-
marilyL-PTSandLS(alearnable,diagonalscalingprojection). Whileinmostofourexperiments,
learnablemethodsunderperformedonaveragecomparedtostaticormagnitude-basedapproaches,
thebest-performingconfigurationsforLlama3-8Bunder8:16sparsitywere,infact,thoseemploy-
ing learnable parameters — as shown in Table 5. We find that activations from the up projection
(in the FFN) and the out projection1 are the most sensitive to sparsification, pruning these layers
leadstothemostsignificantperformancedrops,suggestingtheyshouldbepreservedortreatedwith
greatercareincompressionstrategies. Whilewecannotsaythisgeneralizesacrossalllayers, we
empiricallydemonstratethatsomelayersaremoreimportantthanothers.
5 DISCUSSION
Theperformancegapbetweenmultiple/single-choicebenchmarks(e.g.,BoolQ,PIQA)andIFEval
likely stems from differences in the inference stages they emphasize. Core QA benchmarks pri-
marilystresstheprefillphase,whereasIFEvalevaluatesbothprefillandautoregressivegeneration.
Ourevaluationremainsvalid: semi-structuredpatternslike2:4and8:16areespeciallyeffectiveat
acceleratingtheprefillstage,whichoftendominatesinferencelatency.
6 LIMITATIONS
Our work has three main limitations. First, evaluations rely on software emulation, precluding
real-world measurements of speedup or energy savings on hardware supporting sparsity beyond
2:4. Second, our layer sensitivity analysis is preliminary, while up-projection and out-projection
layersappearmostvulnerable,broaderarchitecturalstudiesareneeded. Third,Qwen2.5-7Bshows
anomalous gains on multiple-choice benchmarks under certain sparsification methods, an artifact
we attribute to benchmark limitations, as no such gains appear on IFEval. Critically, generative
performance degrades significantly, underscoring the need for evaluation beyond multiple-choice
QA.
7 CONCLUSION
Thisworkestablishesthatpost-trainingactivationpruningismoreaccuracypreservingthanweight
pruning for Large Language Models. Across four diverse LLMs, we demonstrate that activation
pruningconsistentlypreservesamodel’scapabilitiesbetterthanweightpruningatequivalentspar-
sitylevels,highlightingitspotentialfordynamic,input-adaptiveefficiencygains.
Our key novel contributions are twofold. First, we conduct the first comprehensive evaluation of
lightweight, plug-and-play error mitigation techniques for activation sparsity, including the intro-
ductionandevaluationofthreenewmethods: CosineLossActivation(CLACT)asacontext-aware
pruning criterion, Dynamic/Learnable Per-Token Shift (D-PTS/L-PTS) and Variance Correction
(VAR). We show that these simple methods, often outperforming more complex approaches, pro-
viding strong, hardware-friendly baselines for the community. Second, we systematically explore
semi-structuredsparsitypatternsbeyondthehardwarestandard2:4.Wefindthatthe8:16patternof-
fersmoreflexibilityand,ultimately,lowermodeldegradation. Althoughthe16:32patternperforms
evenclosertounstructuredsparsity,weadvocatefor8:16astheoptimaltargetforfuturehardware
designduetoitsbalanceofperformancegainandimplementationfeasibility.Insummary,thiswork
not only provides practical, high-performing methods for post-training activation pruning but also
creates grounded motivation for 8:16 structured sparsity support, which would unlock significant
efficiencygainsforLLMinferencewithminimaladdedcomplexity.
AcknowledgmentonLLMassistedwriting: ThispaperusedopenaccessQwen3-Max,insome
partsofthepaper,forproofreadingandtextrephrasinginaccordancewithformalstyle.
1Theoutputprojectionoftheattentionmechanism.Itcombinesoutputsfromallattentionheadsandprojects
thembacktothemodel’shiddendimension.
9
September2025
REFERENCES
TaiAn,RuwuCai,YanzheZhang,YangLiu,HaoChen,PengchengXie,ShengChang,YiwuYao,
andGongyiWang.Amberpruner:Leveragingn:Mactivationsparsityforefficientprefillinlarge
languagemodels. arXivpreprintarXiv:2508.02128,2025a.
TaiAn,RuwuCai,YanzheZhang,YangLiu,HaoChen,PengchengXie,ShengChang,YiwuYao,
andGongyiWang. Amberpruner: Leveragingn:mactivationsparsityforefficientprefillinlarge
languagemodels,2025b. URLhttps://arxiv.org/abs/2508.02128.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and
IdanSzpektor. Thesecondpascalrecognisingtextualentailmentchallenge. InProceedingsofthe
SecondPASCALChallengesWorkshop,2006.
CenkBaykal, NishanthDikkala, RinaPanigrahy, CyrusRashtchian, and XinWang. A theoretical
view on sparsely activated networks. Advances in Neural Information Processing Systems, 35:
30071–30084,2022.
YonatanBisk,RowanZellers,JianfengGao,andYejinChoi. Piqa: Reasoningaboutphysicalcom-
monsenseinnaturallanguage. InProceedingsofAAAI,2020.
VuiSengChua,YujiePan,andNileshJain. Post-trainingstatisticalcalibrationforhigheractivation
sparsity. arXivpreprintarXiv:2412.07174,2024.
ChristopherClark,KentonLee,Ming-WeiChang,TomKwiatkowski,MichaelCollins,andKristina
Toutanova. Boolq:Exploringthesurprisingdifficultyofnaturalyes/noquestions. InProceedings
ofNAACL-HLT,pp.2924–2936,2019.
PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,AshishSabharwal,CarissaSchoenick,and
OyvindTafjord. Thinkyouhavesolvedquestionanswering? tryarc,theai2reasoningchallenge.
arXiv:1803.05457v1,2018.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John
Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168,
2021.
Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment
challenge. InMachineLearningChallengesWorkshop,pp.177–190.Springer,2005.
VageEgiazarian,AndreiPanferov,DenisKuznedelev,EliasFrantar,ArtemBabenko,andDanAl-
istarh. Extremecompressionoflargelanguagemodelsviaadditivequantization. arXivpreprint
arXiv:2401.06118,2024.
EgeErdil. Inferenceeconomicsoflanguagemodels. arXivpreprintarXiv:2506.04645,2025.
Gongfan Fang, Hongxu Yin, Saurav Muralidharan, Greg Heinrich, Jeff Pool, Jan Kautz, Pavlo
Molchanov,andXinchaoWang. Maskllm: Learnablesemi-structuredsparsityforlargelanguage
models. AdvancesinNeuralInformationProcessingSystems,37:7736–7758,2024.
Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in
one-shot. InInternationalconferenceonmachinelearning,pp.10323–10337.PMLR,2023.
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training
quantizationforgenerativepre-trainedtransformers. arXivpreprintarXiv:2210.17323,2022.
LeoGao,JonathanTow,BaberAbbasi,StellaBiderman,SidBlack,AnthonyDiPofi,CharlesFos-
ter,LaurenceGolding,JeffreyHsu,AlainLeNoac’h,HaonanLi,KyleMcDonell,NiklasMuen-
nighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lin-
tang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework
forfew-shotlanguagemodelevaluation,122023. URLhttps://zenodo.org/records/
10256836.
10
September2025
SongHan, JeffPool, JohnTran, andWilliamJ.Dally. Learningbothweightsandconnectionsfor
efficientneuralnetworks,2015. URLhttps://arxiv.org/abs/1506.02626.
Daniel Haziza, Timothy Chou, Dhruv Choudhary, Luca Wehrstedt, Francisco Massa, Jiecao Yu,
GeonhwaJeong,SupriyaRao,PatrickLabatut,andJesseCai. Acceleratingtransformerinference
andtrainingwith2: 4activationsparsity. arXivpreprintarXiv:2503.16672,2025.
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and
Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint
arXiv:2009.03300,2020.
YuezhouHu,KangZhao,WeiyuHuang,JianfeiChen,andJunZhu. Acceleratingtransformerpre-
trainingwith2: 4sparsity. arXivpreprintarXiv:2404.01847,2024.
Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner, Joseph Naor, and Daniel Soudry. Accel-
eratedsparseneuraltraining: Aprovableandefficientmethodtofindn: mtransposablemasks.
Advancesinneuralinformationprocessingsystems,34:21099–21111,2021.
Kamirul Kamirul, Odysseas Pappas, and Alin Achim. R-sparse r-cnn: Sar ship detection based
on background-aware sparse learnable proposals, 2025. URL https://arxiv.org/abs/
2504.18959.
ArtyomKharinaev,ViktorMoskvoretskii,EgorShvetsov,KseniiaStudenikina,BykovMikhail,and
EvgenyBurnaev. Investigatingtheimpactofquantizationmethodsonthesafetyandreliabilityof
largelanguagemodels. arXivpreprintarXiv:2502.15799,2025.
Eldar Kurtic´, Elias Frantar, and Dan Alistarh. Ziplm: Inference-aware structured pruning of lan-
guagemodels. AdvancesinNeuralInformationProcessingSystems,36:65597–65617,2023.
Je-Yong Lee, Donghyun Lee, Genghan Zhang, Mo Tiwari, and Azalia Mirhoseini. Cats:
Contextually-awarethresholdingforsparsityinlargelanguagemodels,2024. URLhttps://arxiv.
org/abs/2404.08763,2024.
Bin Lin, Ningxin Zheng, Lei Wang, Shijie Cao, Lingxiao Ma, Quanlu Zhang, Yi Zhu, Ting Cao,
JilongXue, YuqingYang, etal. Efficientgpukernelsforn: M-sparseweightsindeeplearning.
ProceedingsofMachineLearningandSystems,5:513–525,2023.
JamesLiu,PragaashPonnusamy,TianleCai,HanGuo,YoonKim,andBenAthiwaratkun.Training-
freeactivationsparsityinlargelanguagemodels. arXivpreprintarXiv:2408.14690,2024.
ZhuangLiu,JianguoLi,ZhiqiangShen,GaoHuang,ShoumengYan,andChangshuiZhang. Learn-
ing efficient convolutional networks through network slimming. In Proceedings of the IEEE
internationalconferenceoncomputervision,pp.2736–2744,2017.
ZichangLiu,JueWang,TriDao,TianyiZhou,BinhangYuan,ZhaoSong,AnshumaliShrivastava,
CeZhang,YuandongTian,ChristopherRe,etal. Dejavu: Contextualsparsityforefficientllms
atinferencetime. InInternationalConferenceonMachineLearning,pp.22137–22176.PMLR,
2023.
EgorMaximov,YuliaKuzkina,AzamatKanametov,AlexanderPrutko,AlekseiGoncharov,Maxim
Zhelnin,andEgorShvetsov. From2: 4to8: 16sparsitypatternsinllmsforoutliersandweights
withvariancecorrection. arXivpreprintarXiv:2507.03052,2025.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models,2016.
Zhendong Mi, Zhenglun Kong, Geng Yuan, and Shaoyi Huang. Ace: Exploring activation co-
sine similarity and variance for accurate and calibration-efficient llm pruning. arXiv preprint
arXiv:2505.21987,2025.
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct
electricity? a new dataset for open book question answering. In Proceedings of EMNLP, pp.
2381–2391,2018.
11
September2025
Iman Mirzadeh, Keivan Alizadeh, Sachin Mehta, Carlo C Del Mundo, Oncel Tuzel, Golnoosh
Samei,MohammadRastegari,andMehrdadFarajtabar. Relustrikesback: Exploitingactivation
sparsityinlargelanguagemodels. arXivpreprintarXiv:2310.04564,2023.
PavloMolchanov,ArunMallya,StephenTyree,IuriFrosio,andJanKautz. Importanceestimation
forneuralnetworkpruning. InProceedingsoftheIEEE/CVFconferenceoncomputervisionand
patternrecognition,pp.11264–11272,2019.
Denis Paperno, Germa´n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi,
Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Ferna´ndez. The lambada dataset:
Wordpredictionrequiringabroaddiscoursecontext. arXivpreprintarXiv:1606.06031,2016.
MansheejPaul,FengChen,BrettWLarsen,JonathanFrankle,SuryaGanguli,andGintareKarolina
Dziugaite. Unmaskingthelotterytickethypothesis: What’sencodedinawinningticket’smask?
arXivpreprintarXiv:2210.03044,2022.
Keisuke Sakaguchi, Rowan Zellers, Ari Holtzman, and Yejin Choi. Winogrande: An adversarial
winogradschemachallengeatscale. InProceedingsofAAAI,2020.
Susav Shrestha, Brad Settlemyer, Nikoli Dryden, and Narasimha Reddy. Polar sparsity:
High throughput batched llm inferencing with scalable contextual sparsity. arXiv preprint
arXiv:2505.14884,2025.
EgorShvetsov,DmitryOsin,AlexeyZaytsev,IvanKoryakovskiy,ValentinBuchnev,IlyaTrofimov,
andEvgenyBurnaev. Quantnasforsuperresolution:Searchingforefficientquantization-friendly
architecturesagainstquantizationnoise. IEEEAccess,12:117008–117025,2024. doi: 10.1109/
ACCESS.2024.3446039.
YixinSong,ZeyuMi,HaotongXie,andHaiboChen.Powerinfer:Fastlargelanguagemodelserving
withaconsumer-gradegpu. InProceedingsoftheACMSIGOPS30thSymposiumonOperating
SystemsPrinciples,pp.590–606,2024a.
Yixin Song, Haotong Xie, Zhengyan Zhang, Bo Wen, Li Ma, Zeyu Mi, and Haibo Chen. Turbo
sparse: Achieving llm sota performance with minimal activated parameters. arXiv preprint
arXiv:2406.05955,2024b.
Mart van Baalen, Andrey Kuzmin, Markus Nagel, Peter Couperus, Cedric Bastoul, Eric Mahurin,
TijmenBlankevoort,andPaulWhatmough. Gptvq: Theblessingofdimensionalityforllmquan-
tization. arXivpreprintarXiv:2402.15319,2024.
HongyuWang, ShumingMa, RuipingWang, andFuruWei. Q-sparse: Alllargelanguagemodels
canbefullysparsely-activated. arXivpreprintarXiv:2407.10969,2024.
GuangxuanXiao,JiLin,MickaelSeznec,HaoWu,JulienDemouth,andSongHan. Smoothquant:
Accurate and efficient post-training quantization for large language models. In International
ConferenceonMachineLearning,pp.38087–38099.PMLR,2023.
RowanZellers, AriHoltzman, YonatanBisk, Ali Farhadi, and YejinChoi. Hellaswag: Cana ma-
chinereallyfinishyoursentence? InProceedingsofACL,2019.
MaximZhelnin,ViktorMoskvoretskii,EgorShvetsov,EgorVenediktov,MariyaKrylova,Aleksandr
Zuev, and Evgeny Burnaev. Gift-sw: Gaussian noise injected fine-tuning of salient weights for
llms. arXivpreprintarXiv:2408.15300,2024.
JeffreyZhou,TianjianLu,SwaroopMishra,SiddharthaBrahma,SujoyBasu,YiLuan,DennyZhou,
and Le Hou. Instruction-following evaluation for large language models, 2023. URL https:
//arxiv.org/abs/2311.07911.
Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. Trained ternary quantization. arXiv
preprintarXiv:1612.01064,2016.
12
September2025
APPENDIX
A R-SPARSE DETAILS
Finally, we include R-Sparse (Kamirul et al., 2025), which combines activation sparsity with a
low-rank approximation of the weight matrix. Instead of pruning solely by magnitude, R-Sparse
decomposes the computation into two parts: (i) sparse channels with high-magnitude activations,
and(ii)alow-rankcomponentobtainedviaSVDofWthatapproximatesthecontributionofpruned
activations.
Formally,thelinearlayer
Y =XW⊤ (6)
isapproximatedas
Y ≈Y +Y , (7)
s r
where
Y =σ (X)W⊤, Y =(X−σ (X))(A B )⊤. (8)
s t(s) r t(s) r r
Hereσ (·)denotessparsificationofactivationswiththresholdt(s), andA B⊤ istherank-r ap-
t(s) r r
proximationofWobtainedfromitstruncatedSVD.Thetrade-offbetweenY andY isdetermined
s r
byasparsitybudgetsandrankr,whichcanbeoptimizedviaevolutionarysearch.
B DIFFERENT PATTERNS FOR SEMI-STRUCTURE SPARSIFICATION
Table6:PerformancecomparisonofdifferentsparsitypatternsonLlama3-8Bacrossvariousbench-
marks. Values represent accuracy scores, with the last column showing the average performance
droprelativetotheoriginalmodel.
ARCEasy BoolQ PIQA WinoGrande AvgDrop(↓)
Original 0.8207 0.8391 0.8003 0.7340
2:4 0.6837 0.7261 0.7163 0.6110 14.35%
4:8 0.7272 0.7810 0.7529 0.6393 9.29%
8:16 0.7525 0.7969 0.7568 0.6551 7.38%
16:32 0.7698 0.8082 0.7688 0.6771 5.40%
50%unstructured 0.7820 0.8198 0.7714 0.6858 4.30%
70%unstructured 0.5580 0.6311 0.6474 0.5477 25.32%
B.1 COMPARISONOFDIFFERENTPATTERNS
Table 7: A comparison of combined approaches with 8:16 semi-structured sparsity. Average
relativeperformance(%)acrossfourdatasets. Valuesindicateperformancedrops(lowerisbetter),
negative values signify performance improvement. Full, non-aggregated results are available in
Appendix10.
Models
Method Llama2-7B Qwen2.5-7B Gemma3-4B LLama3-8B AverageDrop(↓)
CLACT+PTS 5.63% −5.06% 0.50% 8.55% 2.40%
CLACT+VAR 5.07% −2.90% 0.54% 8.59% 2.82%
Amber-Pruner+PTS 6.16% −3.47% 0.17% 7.42% 2.57%
Amber-Pruner+VAR 4.74% −3.63% −0.16% 8.39% 2.34%
L-PTS+VAR 6.87% 2.86% 3.41% 7.15% 5.07%
C DATASETS
13
September2025
Table 8: Datasets used to evaluate hypotheses. Prompt-level strict accuracy is the fraction of
promptsforwhichallverifiableinstructionsinthepromptarefollowedexactlyasstated.Instruction-
levelstrictaccuracyisthefractionofindividualinstructionsthatarefollowedexactlyasstated,av-
eragedacrossallinstructions.
Dataset Description Metric
WikiText-2 (Merity Acollectionofover100milliontokensextractedfromtheset Perplexity
etal.,2016) ofverifiedGoodandFeaturedarticlesonWikipedia.
ARC-Easy (Clark QAbenchmarkforgenuinegrade-schoollevel,multiple-choice Accuracy
etal.,2018) science questions. The dataset contains 2251 examples for
training,570fordevelopmentand2376fortesting.
ARC Challenge QA benchmark for more difficult grade-school level science Accuracy
(Clarketal.,2018) questions, part of the AI2 Reasoning Challenge. Designed to
requiredeeperreasoningthanARC-Easy.
BoolQ(Clarketal., QAbenchmarkforyes/noquestions.Thedatasetcontains9427 Accuracy
2019) examplesfortrainingand3270fortesting.
PIQA (Bisk et al., Physical commonsense QA benchmark for choosing the right Accuracy
2020) answerbetweentwooptions. Contains16Ktrain,2Kdev,and
3Ktestexamples.
WinoGrande (Sak- QA benchmark for pronoun resolution with adversarial filter- Accuracy
aguchietal.,2020) ing. Contains40Ktrain,1267dev,and1767testexamples.
HellaSwag (Zellers Commonsensereasoningbenchmarkforsentencecompletion, Accuracy
etal.,2019) designedtobeeasyforhumansbuthardformodels. Contains
70Ktrainand10Kvalidationexamples.
OpenBookQA (Mi- Open-bookquestionansweringdatasetrequiringretrievalofel- Accuracy
haylovetal.,2018) ementarysciencefacts. Contains59574-waymultiple-choice
questions.
RTE (Dagan et al., RecognizingTextualEntailmentdatasetsfromPASCALchal- Accuracy
2005; Bar-Haim lenges. Task is to classify if a hypothesis is entailed by a
etal.,2006) premise.
MMLU MassiveMultitaskLanguageUnderstandingbenchmarkcover- Accuracy
(Hendrycks et al., ing57subjectsacrossSTEM,humanities,andsocialsciences.
2020) Measuresmultitaskaccuracy.
Lambada Standard Wordpredictiontaskrequiringbroaddiscoursecontext. Target Accuracy
(Paperno et al., wordisunpredictablefromlocalcontextalone.
2016)
Lambada OpenAI LAMBADAtestsetpreprocessedbyOpenAIforstandardized Accuracy
(Paperno et al., evaluation. Taskremainsfinalwordpredictionwithlong-range
2016) dependencies.
Accuracy
(Strict)
GSM8K (Cobbe Gradeschoolmathwordproblemsrequiringmulti-stepreason-
Accuracy
etal.,2021) ing. Contains7.5Ktrainand1.3Ktestexamples.
(Flexible)
Accuracy
(Prompt-level)
IFEval(Zhouetal., Benchmarkwith541promptscontainingverifiableinstructions
Accuracy
2023) tomeasureinstruction-followingfidelity.
(Instruct-level)
14
September2025
D WEIGHTS VERSUS ACTIVATIONS
Table 9: The performance of models with applied unstructured activation pruning. We show that
evenwithseveresparsity(70-90%)modelswereabletoperformdecentlyonourbenchmarks. ACT
stands for activations pruning, WT — for weight pruning. OUT denotes values more than 23,
accordingaccuracyscoresmostlikelycorrespondtorandom.
Pruning WikiText2↓ ARCEasy BoolQ PIQA WinoGrande Drop(↓)%
Llama2-7B
Base 6.94 0.74 0.80 0.76 0.66 -
0.2ACT 6.96 0.74 0.80 0.77 0.66 -0.33%
0.2WT 7.49 0.72 0.80 0.76 0.66 0.68%
0.5ACT 7.53 0.70 0.78 0.75 0.66 2.32%
0.5WT 18.72 0.60 0.72 0.70 0.61 11.10%
0.7ACT 20.11 0.56 0.64 0.65 0.53 19.62%
0.7WT OUT 0.27 0.38 0.54 0.47 43.44%
0.9ACT OUT 0.26 0.38 0.52 0.49 43.39%
0.9WT OUT 0.27 0.38 0.53 0.48 43.39%
Qwen2.5-7B
Base 7.46 0.69 0.86 0.75 0.60 -
0.2ACT 7.48 0.69 0.86 0.74 0.61 2.37%
0.2WT 8.03 0.67 0.86 0.74 0.60 3.42%
0.5ACT 8.3 0.67 0.87 0.74 0.58 3.87%
0.5WT 43.6 0.56 0.80 0.68 0.57 3.42%
0.7ACT 18.7 0.6 0.81 0.70 0.58 3.87%
0.7WT OUT 0.28 0.38 0.54 0.48 12.12%
0.9ACT OUT 0.25 0.38 0.54 0.52 44.22%
0.9WT OUT 0.25 0.58 0.54 0.51 36.35%
Gemma3-4B
Base 17.29 0.72 0.84 0.72 0.62 -
0.2ACT 17.60 0.71 0.84 0.72 0.60 3.35%
0.2WT 18.93 0.68 0.84 0.72 0.59 4.74%
0.5ACT 22.39 0.71 0.83 0.72 0.57 4.80%
0.5WT 273 0.36 0.55 0.61 0.52 30.89%
0.7ACT 88 0.55 0.63 0.66 0.54 19.57%
0.7WT OUT 0.27 0.49 0.53 0.51 38.81%
0.9ACT OUT 0.26 0.38 0.54 0.50 42.64%
0.9WT OUT 0.25 0.45 0.52 0.52 40.60%
E MAIN EXTENDED RESULTS
In this section we present non - aggregated results for all of the methods and datasets., results are
presentedinTable11andTable10.
15
September2025
Table 10: Semi-Structured 2:4 Sparsification - performance Metrics, for calibration, when it is
required, and perplexity we use WikiText2. Average Drop is computed without accounting for
perplexity
Pruning WikiText2↓ ARCEasy BoolQ PIQA WinoGrande AverageDrop%
Llama2-7B 6.94 0.74 0.80 0.76 0.66 -
ACT 10.23 0.66 0.71 0.71 0.60 9.43%
WT 42.40 0.57 0.65 0.69 0.56 16.52%
D-PTS 9.38 0.64 0.68 0.71 0.61 10.67%
S-PTS 9.36 0.66 0.68 0.71 0.60 10.37%
VAR 8.31 0.67 0.69 0.72 0.59 9.76%
CLACT 8.23 0.65 0.72 0.71 0.63 8.32%
Amber-Pruner 9.24 0.64 0.68 0.69 0.60 11.70%
LPTS 8.89 0.65 0.60 0.72 0.59 13.13%
LPTS+VAR 8.39 0.67 0.63 0.72 0.60 11.47%
R-SPARSE(64) 9.19 0.66 0.63 0.69 0.59 12.90%
R-SPARSE(128) 9.29 0.65 0.65 0.70 0.59 12.23%
Llama3.1-8B 7.21 0.82 0.84 0.80 0.73 -
ACT 16.61 0.68 0.73 0.72 0.61 14.35%
WT 20.14 0.41 0.57 0.60 0.54 33.63%
PTS 16.4 0.69 0.73 0.72 0.60 14.59%
S-PTS(N-100) 16.5 0.67 0.74 0.72 0.60 14.61%
S-PTS(N-200) 16.5 0.68 0.73 0.72 0.61 14.31%
VAR 14.17 0.70 0.73 0.73 0.62 13.11%
CLACT 19.49 0.65 0.71 0.69 0.59 17.27%
WANDA 15.86 0.66 0.74 0.69 0.61 15.01%
L-PTS 12.77 0.71 0.71 0.73 0.59 14.13%
L-PTS+VAR 12.40 0.73 0.71 0.73 0.60 13.49%
R-SPARSE(64) 15.07 0.69 0.72 0.71 0.61 15.28%
R-SPARSE(128) 16.09 0.67 0.71 0.70 0.61 16.34%
Qwen2.5-7B 7.46 0.69 0.86 0.75 0.60 -
ACT 10.06 0.65 0.86 0.72 0.54 4.95%
WT 35.37 0.53 0.78 0.68 0.54 12.96%
D-PTS 10.07 0.79 0.86 0.76 0.66 -6.46%
S-PTS 10.74 0.78 0.84 0.74 0.65 -4.43%
VAR 13.95 0.74 0.83 0.74 0.61 -1.48%
CLACT 11.16 0.73 0.84 0.71 0.67 -2.45%
Amber-Pruner 10.64 0.74 0.84 0.70 0.64 -1.23%
LPTS 9.13 0.67 0.81 0.72 0.58 3.66%
LPTS+VAR 9.10 0.68 0.81 0.73 0.56 3.97%
R-SPARSE(64) 9.03 0.79 0.76 0.75 0.64 -2.55%
R-SPARSE(128) 9.12 0.77 0.77 0.75 0.63 -1.51%
Gemma3-4B 17.29 0.72 0.84 0.72 0.62 -
ACT 35.62 0.65 0.76 0.70 0.51 9.94%
WT 421.95 0.35 0.44 0.58 0.49 34.86%
D-PTS 35.94 0.70 0.76 0.70 0.60 4.58%
S-PTS 35.84 0.71 0.77 0.70 0.60 3.93%
VAR 33.25 0.60 0.76 0.63 0.54 5.04%
CLACT 39.22 0.66 0.74 0.67 0.59 8.01%
Amber-Pruner 35.56 0.67 0.76 0.68 0.61 5.91%
LPTS 19.55 0.65 0.73 0.70 0.55 9.19%
LPTS+VAR 19.13 0.65 0.74 0.71 0.53 9.82%
R-SPARSE(64) 17.04 0.69 0.76 0.69 0.60 5.17%
R-SPARSE(128) 16.17 0.68 0.75 0.70 0.61 5.16%
16
September2025
Table 11: Semi-Structured 8:16 Sparsification - performance Metrics, for calibration, when it
is required, and perplexity we use WikiText2. Average Drop is computed without accounting for
perplexity.
Pruning WikiText2↓ ARCEasy BoolQ PIQA WinoGrande AverageDrop%
Llama2-7B 6.94 0.74 0.80 0.76 0.66 -
ACT 8.12 0.69 0.75 0.73 0.63 5.37%
WT 20.47 0.64 0.76 0.72 0.61 7.84%
D-PTS 6.92 0.70 0.73 0.75 0.64 4.63%
S-PTS 6.93 0.70 0.73 0.75 0.66 3.87%
VAR 6.67 0.69 0.72 0.75 0.65 4.85%
CLACT 6.54 0.71 0.74 0.75 0.64 3.98%
CLACT+PTS 7.00 0.69 0.72 0.73 0.64 5.63%
CLACT+VAR 6.72 0.69 0.73 0.75 0.64 5.07%
R-SPARSE(64) 7.75 0.69 0.71 0.73 0.64 5.91%
R-SPARSE(128) 7.82 0.68 0.69 0.74 0.61 7.93%
Amber-Pruner 8.10 0.66 0.75 0.73 0.66 5.32%
Amber-Pruner+PTS 6.90 0.68 0.72 0.72 0.65 6.16%
Amber-Pruner+VAR 6.66 0.70 0.72 0.74 0.65 4.74%
LPTS 7.50 0.69 0.66 0.74 0.63 8.15%
LPTS+VAR 7.52 0.69 0.67 0.74 0.64 6.87%
Llama3.1-8B 7.21 0.82 0.84 0.80 0.73 -
ACT 10.32 0.75 0.80 0.76 0.66 7.38%
WT 22.56 0.51 0.64 0.63 0.54 27.26%
D-PTS 10.34 0.76 0.80 0.76 0.66 6.79%
S-PTS 10.31 0.76 0.80 0.75 0.66 7.30%
VAR 10.67 0.74 0.79 0.75 0.66 8.30%
CLACT 10.67 0.73 0.79 0.74 0.66 8.60%
CLACT+PTS 10.68 0.74 0.79 0.74 0.65 8.55%
CLACT+VAR 10.15 0.74 0.79 0.75 0.64 8.59%
R-SPARSE(64) 11.42 0.75 0.77 0.75 0.66 8.44%
R-SPARSE(128) 10.43 0.75 0.78 0.74 0.66 8.49%
Amber-Pruner 10.16 0.73 0.80 0.75 0.68 7.13%
Amber-Pruner+PTS 10.17 0.75 0.80 0.75 0.66 7.42%
Amber-Pruner+VAR 9.94 0.74 0.80 0.75 0.64 8.39%
LPTS 10.04 0.76 0.79 0.77 0.65 7.19%
LPTS+VAR 10.26 0.77 0.78 0.76 0.66 7.15%
Qwen2.5-7B 7.46 0.69 0.86 0.75 0.60 -
ACT 8.61 0.66 0.87 0.73 0.53 4.38%
WT 40.79 0.59 0.82 0.67 0.52 9.54%
D-PTS 8.61 0.80 0.87 0.77 0.68 -8.28%
S-PTS 8.84 0.80 0.86 0.76 0.67 -7.24%
VAR 11.91 0.69 0.72 0.75 0.65 1.93%
CLACT 8.94 0.77 0.85 0.73 0.65 -4.02%
CLACT+PTS 8.94 0.77 0.86 0.83 0.67 -5.06%
CLACT+VAR 8.87 0.76 0.84 0.71 0.65 -2.90%
R-SPARSE(64) 8.12 0.82 0.79 0.77 0.69 -6.90%
R-SPARSE(128) 8.24 0.80 0.79 0.77 0.67 -5.40%
Amber-Pruner 8.80 0.77 0.86 0.74 0.69 -6.20%
Amber-Pruner+PTS 8.79 0.77 0.85 0.73 0.64 -3.40%
Amber-Pruner+VAR 8.73 0.75 0.85 0.74 0.65 -3.60%
LPTS 8.23 0.69 0.83 0.75 0.57 1.70%
LPTS+VAR 8.21 0.70 0.83 0.73 0.56 2.80%
Gemma3-4B 17.29 0.72 0.84 0.72 0.62 -
ACT 25.31 0.70 0.81 0.71 0.55 4.76%
WT 198.53 0.39 0.60 0.62 0.52 26.11%
D-PTS 25.17 0.70 0.81 0.71 0.54 5.16%
S-PTS 25.40 0.75 0.82 0.74 0.63 -1.54%
VAR 23.93 0.75 0.81 0.73 0.65 -1.87%
CLACT 25.85 0.75 0.81 0.71 0.61 0.60%
CLACT+PTS 26.03 0.73 0.81 0.70 0.63 0.50%
CLACT+VAR 24.78 0.74 0.81 0.70 0.63 0.54%
R-SPARSE(64) 15.39 0.76 0.80 0.74 0.64 -1.36%
R-SPARSE(128) 14.55 0.74 0.80 0.73 0.63 -0.44%
Amber-Pruner 25.11 0.74 0.82 0.70 0.63 0.08%
Amber-Pruner+PTS 25.28 0.75 0.81 0.70 0.63 0.17%
Amber-Pruner+VAR 23.97 0.74 0.81 0.71 0.64 -0.16%
LPTS 15.73 0.70 0.79 0.73 0.56 4.21%
LPTS+VAR 15.68 0.71 0.79 0.72 0.57 3.41%
17
September2025
Table12:Llama3-8Bwith8:16activationsparsity.LS+L-PTSindicatesLearnableDiagonalScale+
LearnableShift,“Layers”indicatesthesubsetoflinearlayerswherethemethodwasapplied. Drop
iscomputedwithoutaccountingforperplexity.
Method Layers PPL BoolQ WinoGrande PIQA ARCEasy ARCChal. HellaSwag OpenBookQA RTE MMLU Lambadastand. Lambada(OpenAI) Drop%↓
ORIGINAL – – 0.8391 0.7340 0.8003 0.8207 0.5196 0.5905 0.3420 0.6859 0.6790 0.6569 0.7308 –
LS+L-PTS all 9.6036 0.7841 0.6638 0.7715 0.7647 0.4514 0.5294 0.2720 0.6318 0.5521 0.5686 0.6625 10.90%
LS+L-PTS k,o,gate,down 8.3483 0.8205 0.7111 0.7889 0.7887 0.4659 0.5591 0.3200 0.6462 0.6060 0.6123 0.7046 5.43%
LS+L-PTS k,v,gate,down 8.0821 0.8352 0.7174 0.7867 0.7992 0.4898 0.5651 0.3260 0.6643 0.6322 0.6262 0.7108 3.56%
LS+L-PTS+VAR all 9.4983 0.7872 0.6606 0.7606 0.7601 0.4334 0.5372 0.2880 0.6390 0.5532 0.5729 0.6689 10.60%
LS+L-PTS+VAR k,o,gate,down 8.2930 0.8116 0.7135 0.7851 0.7908 0.4838 0.5634 0.3300 0.6498 0.6095 0.6189 0.7079 4.64%
LS+L-PTS+VAR k,v,gate,down 8.0259 0.8306 0.7269 0.7851 0.7955 0.4863 0.5673 0.3260 0.6715 0.6327 0.6317 0.7143 3.36%
18

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Lightweight error mitigation strategies for post-training N:M activation sparsity in LLMs"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
