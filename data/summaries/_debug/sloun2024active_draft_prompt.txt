=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Active inference and deep generative modeling for cognitive ultrasound
Citation Key: sloun2024active
Authors: Ruud JG van Sloun

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2024

Key Terms: quality, image, sequences, sequence, deep, modeling, cognitive, generative, pulse, inference

=== FULL PAPER TEXT ===

ACCEPTEDPREPRINT,IEEETUFFC,2024 1
Active inference and deep generative modeling
for cognitive ultrasound
Ruud JG van Sloun Member, IEEE
Abstractâ€”Ultrasoundhastheuniquepotentialtoofferac-
cess to medical imaging to anyone, everywhere. Devices
havebecomeultra-portableandcost-effective,akintothe
stethoscope. Nevertheless, and despite many advances,
ultrasound image quality and diagnostic efficacy are still
highly operator- and patient-dependent. In difficult-to-
image patients, image quality is often insufficient for re-
liable diagnosis. In this paper, we put forth the idea that
ultrasoundimagingsystemscanberecastasinformation-
seekingagentsthatengageinreciprocalinteractionswith
theiranatomicalenvironment.Suchagentsautonomously
adapt their transmit-receive sequences to fully personal-
izeimagingandactivelymaximizeinformationgainin-situ.
To that end, we will show that the sequence of pulse-echo experiments that an ultrasound system performs
canbeinterpretedasaperception-actionloop:theactionisthedataacquisition,probingtissuewithacoustic
waves and recording reflections at the detection array, and perception is the inference of the anatomical
and or functional state, potentially including associated diagnostic quantities. We then equip systems with a
mechanism to actively reduce uncertainty and maximize diagnostic value across a sequence of experiments,
treating action and perception jointly using Bayesian inference given generative models of the environment
and action-conditional pulse-echo observations. Since the representation capacity of the generative models
dictates both the quality of inferred anatomical states and the effectiveness of inferred sequences of future
imagingactions,wewillbegreatlyleveragingtheenormousadvancesindeepgenerativemodelling(generative
AI),thatarecurrentlydisruptingmanyfieldsandsocietyatlarge.Finally,weshowsomeexamplesofcognitive,
closed-loop, ultrasound systems that perform active beamsteering and adaptive scanline selection, based on
deepgenerativemodelsthattrackanatomicalbeliefstates.
Index Termsâ€”Deep generative models, deep learning, active inference, ultrasound imaging, perception and
action,cognitiveimaging,computationalimaging,adaptivecompressedsensing.
I. INTRODUCTION i.e. patient geometry and user interaction. These factors thus
vary across exams, and within exams. Given this, it is rea-
ULTRASOUND (US) has the potential to revolution-
sonable to hypothesize that optimal imaging requires closed-
ize and democratize medical imaging due to its cost-
loop, goal-directed system behaviour, through sequential op-
effectiveness and portability. However, achieving consistent,
timal experiment (=transmit-receive) design via its reciprocal
precise and robust diagnostics remains a challenge. The diag-
interactions with the physical environment.
nostic performance of US is dependent on skilled operators
Based on this hypothesis, this paper proposes a brain-
and exams still fail frequently on hard-to-image patients. The
inspired paradigm for such a cognitive ultrasound transmit-
group of hard-to-image patients is moreover growing rapidly
receive control system. It recognizes that the cycle of ultra-
duetotherisingincidenceofobesityworldwide.Studiesshow
sounddataacquisitionandreconstructioncanbeinterpretedas
that reduced image quality leads to worse observer variabil-
aperception-actionloop:theactionistheacquisition,probing
ity, reproducibility, and accuracy of diagnostic parameters in
the anatomy, and the perception is the reconstruction that
ultrasound exams [1], [2].
infers what object most likely generated that acquired data.
Thebiggestadversariesforultrasoundimagequalityanddi-
This data acquisition cycle has associated costs (e.g. time and
agnosticaccuracystemfrompatient-anduser-specificfactors,
energy), and hence in practice one always deals with partial
observations of the time-varying object.
Â©2024IEEE.DOI:10.1109/TUFFC.2024.3466290.SubmittedonJuly
Perception-actionloopsarecommonlyusedinneuroscience
5,2024;acceptedonSeptember16,2024.Thisworkwassupportedby
theEuropeanResearchCouncil(ERC)undertheERCstartinggrantnr. and cybernetics to explain the behaviour of intelligent agents,
101077368(US-ACT),andtheDutchResearchCouncil(NWO)under whichalsotypicallydealwithpartialobservationsoftheworld
VIDIgrantnr.20381
around them. A now widely-accepted brain theory is that
RuudJGvanSlouniswiththeEindhovenUniversityofTechnology,
Eindhoven,TheNetherlands(e-mail:r.j.g.v.sloun@tue.nl) agents establish an internal generative model of the world in
4202
tcO
71
]PS.ssee[
1v01331.0142:viXra
2 ACCEPTEDPREPRINT,IEEETUFFC,2024
order to efficiently infer causes of their sensations, and plan aswellastheaction-valuefunctionsincomplexmodelswhen
useful future actions, all driven by an intrinsic motivation [3]: exact computation is intractable. Then, in section VII, we
minimization of uncertainty. will give some concrete examples that have tutorial value,
Through this lens, we will interpret the ultrasound imaging illustrating the potential utility of the presented approaches
system as an agent, more specifically an active perceiver [4], in the context of ultrasound. Finally, in section VIII, we
[5], which strives to perform actions and perceptions that will discuss future research directions, open questions, and
minimize uncertainty about the anatomical world across a outstanding challenges, and then conclude in section IX.
sequence of imaging experiments and resulting observations. Table I gives a glossary of some terms and symbols that are
To be effective at achieving this, the agent must understand used throughout this paper.
the environment, and the consequences that actions have on
TABLEI
observationsofthatenvironment.Wepostulatethatthiscanbe
GLOSSARYOFTERMSANDSYMBOLSUSEDINTHISPAPER.
achieved by equipping the agent with generative models that
Term Definition
governthesebeliefs.Throughprobabilisticinference,anagent x,y,a Randomvariables.
can then optimally plan its measurements, autonomously seek xâ€²,yâ€²,aâ€² Deterministicvariables.
value, and continuously update its imaging strategy based on yË†,aË† Datapoints.
xi,yi Samplesfromaprobabilitydensityfunction.
the incoming sensor data. In effect, full system behaviour is
p(x,y,a) A(probabilistic)generativemodelacrossstates,ob-
governedbythesingleholisticinformation-theoreticobjective servations and actions. We will use both generative
to minimize uncertainty. modelsbasedonfirstprinciples,anddeepgenerative
models,learnedfromdata.
The above ambition becomes practical only when the
p(x,y,aâ€²) A generative model across states and observations
agentâ€™s generative model is sufficiently expressive to reflect evaluated for a particular deterministic action aâ€².
diverse, yet plausible, anatomical states and the intricacies Shorthandnotationforp(x,y,a=aâ€²).
p(x|yË†,aË†) ABayesianposteriordistributionforthestatesgiven
of the relationship between an anatomical state, transmit-
pastandpresentobservationsandactions.Shorthand
receive parameters (e.g. transmit waveforms or receive com- notationforp(x|y=yË†,a=aË†).
pression/sampling), and the observations. This is not at all q(x|yË†,aË†) Anapproximateposteriordistribution.
trivial. Anatomical states (e.g. reflectivity) are typically repre- E q(x) f(x) The expected value of function f(x) when x âˆ¼
q(x).
sented on a spatial grid of tens- to hundreds of thousands of I(x,y|aâ€²) The mutual information between state x and ob-
pixels, and the relationship between these pixels is intricate, servation y, for a particular action aâ€². Reflects the
withstructureatvarioushierarchicalscales.Additionally,there
informationgainofanexperimentwithactionaâ€².
H(y|aâ€²) The marginal entropy of observations y, for a par-
isstructureintime,againatvariousscales.Moreover,sincethe
ticularactionaâ€².Reflectstheuncertaintyaboutout-
agentsâ€™ actions are determined by evaluating some expected comesofobservationsy foranactionaâ€².
observational value functional across hypotheses about the H(y|x,aâ€²) The conditional entropy of y given x, for a
particular action aâ€². Equal to the expectation
current (and future) state, it is critical that these hypotheses
E xâ€²âˆ¼q(x) H(y|x = xâ€²,a = aâ€²). Reflects the
are not only plausible and consistent (i.e. they agree with the remaininguncertaintyabouty whengiventhestate
observations done thus far), but that they cover all modes of x.
the true distribution to prevent collapse of the system into a s Î¸ (x) A neural-network-based approximation of the true
score function of a probability distribution p(x):
degenerate, â€œignorantâ€, mode. âˆ‡xlogp(x).Usedindiffusionmodelstodrawsam-
Deepgenerativemodels,suchasnormalizingflowsanddif- plesfromthedatadistribution.
|Î£| DeterminantofacovariancematrixÎ£,alsoreferred
fusionmodels,haverevolutionizedgenerativemodelingonall
toasthegeneralizedvariance.
these aspects (scale, plausibility, and diversity) in past years.
They are revolutionary in terms of sample fidelity and have
alreadybeenusedeffectivelytosolvechallengingimage-based II. PERCEPTION-ACTION LOOPS
inverse problems. We are currently seeing the very first use â€œEach movement we make by which we alter the appearance
casesforsuchmodelsinultrasoundimagereconstruction[6]â€“ of objects should be thought of as an experiment designed
[9], and will here also elucidate their potential in the context to test whether we have understood correctly the invariant
of active inference and closed-loop cognitive ultrasound. relations of the phenomena before usâ€ â€” Helmholtz [10]
Theremainderofthispaperisorganizedasfollows.Insec-
tions II and III we will introduce perception-action loops, and Intelligent agents, such as the brain, continuously engage
the rationale for using deep generative models, respectively. in interactions with their environment. These interactions
Wewillgivesomespecialattentiontodiffusionmodels.Then are reciprocal, i.e. agents take actions which affect their
insectionIV,wewillgiveaformaldescriptionofwhatexactly environment, and their environment in turn affects their
wemeanbyâ€œperceptualinferenceâ€,therationaleforchoosing observations. These observations solicit new actions, closing
aBayesianapproach,andvariousapproachestoexecutingthis the so-called perception-action loop. Useful actions are those
tractably. We will then, in section V, dive into the inference that lead to desired outcomes/observations. To be effective at
of actions (the active part of active inference), the design of achievingthis,anagentmustunderstandtheenvironment,and
criteria that measure the value of actions, and mechanisms to the consequences of actions on that environment. Rational
evaluateexpectedfuturevaluebasedongenerativepredictions agents thus pursue the understanding of their environment
of â€œwhat may happenâ€. Section VI gives an overview of through actions that lead to information gain, while at the
methods for approximating the generative density functions same time being goal-directed. Such behavior requires the
VANSLOUN:COGNITIVEULTRASOUND 3
ability to make predictions about the environment, and thus
agents embody a generative model.
This paper will be concerned only with perception-action
âˆ—
loops in which actions influence the observation/measurement Generative model ğ‘ğ‘1ğ‘¡ğ‘¡=ğ‘“ğ‘“ğ‘ğ‘ğ‘¡ğ‘¡ ğ‘ğ‘ ğ‘¥ğ‘¥ğ‘¡ğ‘¡:ğ‘‡ğ‘‡, ğ‘¦ğ‘¦ğ‘¡ğ‘¡:ğ‘‡ğ‘‡|ğ‘ğ‘ğ‘¡ğ‘¡,ğ‘ğ‘ï¿½0:ğ‘¡ğ‘¡âˆ’1,ğ‘¦ğ‘¦ï¿½0:ğ‘¡ğ‘¡âˆ’1
Active
of an environmental state, but not the state itself. The agent states
ğ‘ğ‘(ğ‘¦ğ‘¦0:ğ‘‡ğ‘‡,ğ‘¥ğ‘¥0:ğ‘‡ğ‘‡,ğ‘ğ‘0:ğ‘‡ğ‘‡) Action
is thus an active perceiver. As we will see, this has some Agent Environment
2
implications for the way we factorize generative models and (internal states) (external states)
Perception
their approximate posteriors. Some of the assumptions and Sensory
parameterizations used here will therefore deviate from what Approximate posterior states
3
is typical in the control/cybernetics literature, which assumes ğ‘¥ğ‘¥Ì‡ğ‘’ğ‘’=ğ‘“ğ‘“ğ‘’ğ‘’ ğ‘¥ğ‘¥ğ‘’ğ‘’,ğ‘ğ‘ğ‘¡ğ‘¡ +ğ‘›ğ‘›ğ‘’ğ‘’
ğ‘ğ‘ ğ‘¥ğ‘¥0:ğ‘¡ğ‘¡|ğ‘ğ‘ï¿½0:ğ‘¡ğ‘¡, ğ‘¦ğ‘¦ï¿½0:ğ‘¡ğ‘¡
actions impact states of the environment, rather than observa-
âˆˆğ‘„ğ‘„ ğ‘¦ğ‘¦ï¿½ğ‘¡ğ‘¡=ğ‘“ğ‘“ğ‘¦ğ‘¦ ğ‘¥ğ‘¥ğ‘’ğ‘’(ğ‘¡ğ‘¡),ğ‘ğ‘ğ‘¡ğ‘¡ +ğ‘›ğ‘›ğ‘¦ğ‘¦
tions thereof. We will further restrict ourselves to agents that Markov blanket
(ultrasound probe)
havenoexplicitpreferencesaboutoutcomes,otherthantoseek
information gain. They are â€œscientistsâ€ planning successive Fig.1. Attimepointt,anagentequippedwithagenerativemodelp
selects an action (1), which manifests in an excitation of the environ-
experiments to better understand what phenomenon causes
ment (2). The excitation â€œchangesâ€ the environment (e.g. it introduces
their observations. compressionalwaves).ThisinturnresultsinanewsensorystateyË†t.
ConfrontedwiththeupdatedsensorydatayË†0:t,theagentthenrevisits
The ultrasound action space its beliefs about the environment (including future states it may take,
and observations that may follow from that), and computes a new
(approximate)posteriorq(3).Theultrasoundprobecontainstheactive
At this point, it becomes useful to briefly discuss
and sensory states, and acts as a Markov blanket that separates the
what the action space of an ultrasound agent entails. agentfromitsenvironment;theyonlyinteractviatheactiveandsensory
Many of the controllable sensing parameters in ultra- states.Thedistinctionbetweenxe,theenvironmentalstates,andx,the
internalstates,istomakeexplicitthattheagentâ€™smodelisingeneralan
sound are similar to those commonly considered in
approximatemodelofthetruephysicalenvironment.
cognitive radar systems. They include both transmit
and receive parameters. On the transmit side, at the by which we make explicit that actions impact observations,
mostabstractlevel,theagentistaskedwiththedesign not states (active perception), and that the state impacts the
of an optimal, maximally informative, transmit code. distribution across actions (adaptivity). In section V we will
At the most fine-grained level, this code is governed show how the latter dependency manifests precisely in our
by a complete shaping of the transmit waveforms framework, but in a nutshell it comes down to the maximiza-
and timing for each of the transmitters, subject to tion of an information-theoretic action-value functional of the
the constraints of feasibility imposed by the analog conditionalgenerativemodelp(y ,x |a ).Notethatthe
0:T 0:T 0:T
transmit chain. A more coarse representation would active-perceptionmodelforegoesthatinreality,actionsdoim-
be to only control the delays and gains applied to pact the physical environment (they introduce compressional
each transmitter, shaping the transmit beam. Practical waves), and considers all these effects part of the observation
ultrasound systems typically execute a sequence of modelp(y |x ,a ).Thisdistinctionbetweentheagentâ€™s
0:T 0:T 0:T
such coded transmit events (e.g. a series of focused internal model, and the true environment is also made explicit
scan lines), and many practical codes will need to be in Figure 1.
consideredinthecontextofafullsequencetoevaluate Throughout this tutorial, we also refer to the generative
theimpacttheyhaveontheinformationgainachieved models given above as priors. Whenever these prior beliefs
by that sequence. are revised through exposure to information in the form of
data/measurements, we refer to the revised beliefs as posteri-
We define the following time-discretized perception-action ors. We refer to the updating of beliefs in the face of data as
loop. At time point t, an agent equipped with a generative perceptual inference, which will be treated in section IV. De-
model selects an action, which manifests in an excitation of veloping a sufficiently accurate generative model that enables
the environment. This in turn results in a new sensory state reasoningaboutfutureswiththedetailanddiversityneededfor
yË† t . Confronted with the updated sensory data yË† 0:t , the agent fulfillingtheagentsâ€™objectivesiskey.Ouragentistaskedwith
thenrevisitsitsbeliefsabouttheenvironment(includingfuture inferring high-dimensional, high-resolution images, for which
statesitmaytake,andobservationsthatmayfollowfromthat), the requirements are particularly challenging. We will now
and computes a new posterior belief about the state. Figure 1 briefly discuss how recent developments in deep generative
gives an overview of the perception-action loop. modelling alleviate some of these challenges, offering new
The agentâ€™s generative model over observations y 0:T , envi- opportunities for active inference in the context of high-
ronmental states x 0:T , and actions a 0:T is given by: dimensional data.
y ,x ,a âˆ¼p(y ,x ,a ), (1)
0:T 0:T 0:T 0:T 0:T 0:T
III. DEEP GENERATIVE MODELS
with
The potential gains of using accurate generative priors in
p(y ,x ,a )=p(y |x ,a )p(x ,a )
0:T 0:T 0:T 0:T 0:T 0:T 0:T 0:T ultrasoundperception-actionloopscanperhapsmosteasilybe
=p(y |x ,a )p(x )p(a |x ),
0:T 0:T 0:T 0:T 0:T 0:T understood when realizing that most combinations of pixels
(2)
thatformimagesarenotinterestingatall.Althougha5-second
4 ACCEPTEDPREPRINT,IEEETUFFC,2024
Deep generative
modeling
2
ğ‘ğ‘ğ‘§ğ‘§ ğ‘ğ‘
ğ’›ğ’›âˆˆâ„ ğ’™ğ’™âˆˆâ„
Low-dim manifold
ğ‘–ğ‘– ğ‘–ğ‘–
ğ’›ğ’› ğ’™ğ’™ embedded in
2
ğ‘ğ‘
ğ“œğ“œ â„
ğ‘ğ‘ğœƒğœƒ(ğ’™ğ’™|ğ’›ğ’›)
Simplğ‘–ğ‘–e tractable Highlyğ‘–ğ‘–-structured
latenğ’›ğ’›t diâˆ¼strğ‘ğ‘ib(uğ’›ğ’›ti)on datğ’™ğ’™a diâˆ¼striğ‘ğ‘b(uğ’™ğ’™tio)n
Fig.2. Real-worldhigh-dimensionaldatasuchasultrasoundimageslieonalow-dimensionalmanifoldembeddedinthathigh-dimensionalspace.
Thismanifoldistypicallyveryintricateandnon-smoothinthehigh-dimensionaldataspace,andimagesthatlieonitarehighlystructured.Deep
generative learning allows modeling of such highly-structured distributions and sampling of novel datapoints that lie on these low-dimensional
manifolds.Thisisenabledbytransformingsamplesfromatractabledistribution(suchasanisotropicGaussian)zi âˆ¼ RNz intosamplesfrom
thetruedatadistributionxi âˆ¼ RN2 .Therearemanywaysofachievingthis,e.g.viaaconditionaldistributionpÎ¸(x|z)trainedusingvariational
inference,gametheory(adversarialmodels),orjustmaximumlikelihoodforspecificinvertiblemodels(normalizingflows).Alternatively,iterative
samplingmethodslearntoestimatethegradientsofthetruedatadistributionatapluralityofnoisescalemanifoldsthatsuccessivelycorruptthedata
distributionintoatractableisotropicnormal.ThesegradientsthenallowforreversingthisprocessusingreversediffusionorLangevindynamics.
high-quality ultrafast video can in principle represent more is, we assume that there is a factorization
than a trillion unique instances, the vast majority of those (cid:89)
p(x)= p(x ), (3)
hypothetical instances is unstructured and unnatural. If one i
were to completely at random draw an instance vector x âˆˆ i
RNxÃ—NyÃ—Nt, the resulting sample would with overwhelming with factors p(x i ) governing the density functions for non-
probabilitybeclassifiedasâ€œnoiseâ€.Infact,onehastodrawan overlapping individual patches x i âˆˆRMxÃ—MyÃ—Mt.
extremely large amount of samples in this fashion (with this Given this, we can now fit a deep generative model for
amount scaling very quickly in the number of dimensions), p(x) to its data samples. Deep generative modelling comes
until any ultrasound image is sampled. Only a tiny fraction of in many flavours, and which technique to use is a design
that space is occupied by plausible ultrasound images. This choice. We refer the reader to the book by Tomczak [12]
is the manifold hypothesis: real-world high-dimensional data for an excellent overview. In this paper, we are mostly
(such as images) lie on low-dimensional manifolds embedded concerned with the generation of samples, i.e. xi âˆ¼ p(x),
within the high-dimensional space [11]. To use generative where p(x) is some complex data distribution (e.g. that of
models in perception-action loops is to use the structure of images). This is enabled by drawing samples from some
the natural world. It allows systems to track states on a low- tractable, simple distribution zi âˆ¼ p(z), and transforming
dimensionalmanifold,ratherthanintheveryhigh-dimensional those into samples from p(x). Using variational inference
data space. Deep generative models can be used to accurately [13] one can optimize an evidence lower bound to p(x)
describe the complex hierarchical structure in spatio-temporal by jointly learning the generative conditional distribution
imaging data (see Fig. 2). p (x|z) and a variational posterior q (z|x) that is amortized
Î¸ Ï•
across the training dataset, given a tractable prior e.g.
p(z) = N(0,I). Another alternative is to use game theory
In principle one could choose to learn the entire generative
and train models that play a game against a neural adversary
model in Eqn. (1) from data. Given a training dataset of L
trying to detect whether generated samples come from the
samples {(yË†1 ,xË†1 ,aË†1 ),...,(yË†L ,xË†L ,aË†L )}, one would
0:T 0:T 0:T 0:T 0:T 0:T true data distribution or are generated by the model [14].
then fit a deep generative model to the joint distribution.
We can also train models directly using maximum likelihood
Often we will use the factorization in Eqn. (2), and when
when using specific invertible models, i.e. normalizing flows
possible make use of modelling and physics to describe the
[15]. Iterative sampling methods instead learn to estimate the
conditional observation density p(y |x ,a ), which we
0:T 0:T 0:T gradients of the true data distribution at a plurality of noise
will often be able to further decompose into memoryless
scale manifolds that successively corrupt the data distribution
factors p(y |x ,a ). Many observation models are reasonably
t t t into a tractable isotropic normal. These gradients then allow
assumed to be Gaussian, with its mean being some (possibly
for reversing this process using reverse diffusion methods or
nonlinear) function of the state x and action a . This leaves
t t Langevin dynamics. We will now briefly review diffusion
us with the modelling of the prior p(x )
0:T models as they will later be used in our examples.
Howmuchstructure(inspaceandtime)shouldbeimposed Diffusion models: Diffusion models are state-of-the-art in
on x amounts to the classic bias-variance trade-off. One imageandvideogeneration.Anotablechallengethatdiffusion
0:T
way to reduce bias is to factorize the prior, e.g. by only models overcome is the need for explicit normalization of
(cid:82)
imposing structure locally in space or time (patches). That learned density functions (i.e. assuring that p(x)dx = 1).
VANSLOUN:COGNITIVEULTRASOUND 5
Such normalization is typically achieved by imposing specific extent required by the new information [21], [22]. Such belief
constraintsonthearchitecture(e.g.,usingflows[15])orusing updates in the face of data are given by Bayes rule:
variational approximate inference methods. Instead, diffusion
p(yË†Â¯ ,x ) p(yË†Â¯ |x )p(x )
models indirectly parameterize the (log) data distribution by p(x |yË†Â¯ )= 0:t 0:t = 0:t 0:t 0:t . (6)
0:t 0:t p(yË†Â¯ ) p(yË†Â¯ )
learning to approximate its gradient, the score function. 0:t 0:t
The score function appears when reversing a forward The numerator can be evaluated using the known genera-
stochastic differential equation (SDE) that progressively adds tive model Eqn. (1). Unfortunately, computing the marginal
Gaussianwhitenoisetosamplesdrawnfromthedatadistribu- p(yË†Â¯ 0:t ) = (cid:82) p(yË†Â¯ 0:t |x 0:t )p(x 0:t )dx 0:t quickly becomes infeasi-
tion p(x), such that eventually these samples become samples ble for high-dimensional states. In practice we will therefore
from a standard Normal distribution [16]. The forward SDE typically approximate the true posterior distribution, using an
is as follows: approximate posterior q(x 0:t |yË†Â¯ 0:t ). We will elaborate on this
in section VI.
dx=âˆ’ Î²(t s ) xdt + (cid:112) Î²(t )dw, (4) Perceptualinferencealsorevisitshypothesesaboutwhathas
2 s s yet to come. That is, our posterior beliefs about the past
where x(0) is an initial noise-free sample, t âˆˆ [0,T], Î²(t ) and present yield updated priors about the future. Using
s s
is the noise schedule, w is a standard Wiener process, and the generative model in Eqn. (2), and assuming that (i) at
x(T)âˆ¼N(0,I). As said, this SDE can be reversed using the timestep t, the actions aË† 0:t and observations yË† 0:t are known,
score function [17]: i.e. q(y 0:t ,a 0:t )=Î´(y 0:t âˆ’yË† 0:t )Î´(a 0:t âˆ’aË† 0:t ), (ii) actions only
influence observations and not states (active perception), and
(cid:20) (cid:21)
dx= âˆ’ Î²(t s ) xâˆ’Î²(t )âˆ‡ logp (x) dt + (cid:112) Î²(t )dwÂ¯, (iii) y t is only a function of x t and not of past states (the
2 s x ts s s observation model is memoryless), the approximate posterior
(5) projected also into future states and observations, for a de-
where wÂ¯ is a standard Wiener process running backwards. terministic sequence of hypothetical future actions aâ€² , is
t+1:T
Followingthenotationby[18],thediscretesettingoftheSDE given by:
isrepresentedusingx =x(t T/N),Î² =Î²(t T/N),Î± =
1âˆ’Î² ts ,Î±Â¯ ts = (cid:81)t s s =1 Î±
ts
s .
s ts s ts
q(x 0:T ,y t+1:T |aâ€² t+1:T ,yË†Â¯ 0:t )
The diffusion model performs reversal of the forward SDE ( = ii) p(y |x ,aâ€² )q(x |yË†Â¯ )
t+1:T 0:T t+1:T 0:T 0:t
by learning the score function using a neural network param-
eterised by Î¸, conditioned on the timestep t s , s Î¸ (x ts ,t s ) â‰ˆ (i = ii) p(y t+1:T |x t+1:T ,aâ€² t+1:T )q(x 0:T |yË†Â¯ 0:t )
âˆ‡ x logp ts (x)| x=xts .Theparametersarelearnedusingdenois- =p(y t+1:T |x t+1:T ,aâ€² t+1:T )p(x t+1:T |x 0:t )q(x 0:t |yË†Â¯ 0:t ).
ing score matching [19]. To sample from p(x), one can then (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
sample from a standard normal, and revert the SDE using observationmodel stateevolution currentposterior
(7)
the learned score function. This is an iterative process, with a
neuralnetworkfunctionevaluationateachiteration,andhence Note the explicit distinction between known forward gen-
its vanilla implementation is time-consuming. Accelerating erative processes, indicated by p(Â·), and approximations of
diffusion models is hence a very active research theme today, the exact Bayesian posteriors under the generative process,
with many solutions for fast sampling being developed [20]. indicated by q(Â·).
Equation (7) enables the generation of hypothetical state-
observation pairs in the future, which are revisited in-situ as
IV. PERCEPTUAL INFERENCE new data yË†Â¯ comes in:
Wewillnowmaketheperceptionstepinourperception-action q(x ,y |aâ€² ,yË†Â¯ )
t+1:T t+1:T t+1:T 0:t
loop explicit. Perception is the act of updating beliefs about =E p(y |x ,aâ€² )p(x |x ).
the states governed by the generative model in Eqn. (1) given q(x0:t|yË†Â¯0:t) t+1:T t+1:T t+1:T t+1:T 0:t
(8)
new information. Indeed, information is that which induces
a change of beliefs. Note that this is distinct from learning, We will now proceed with the selection of information-
whichistoinferthegenerativemodel(functionalformand/or optimal actions based on these hypothetical futures.
parameters) itself and occurs across longer timescales. In the
following, we will assume that the agent has established or V. ACTIVE INFERENCE AND INFORMATION GAIN
has access to a sufficiently accurate generative model. This â€œWhat is the first and most fundamental thing a newborn
may be obtained through past interactions and observations infant has to do? If one subscribes to the free energy
(training data) and/or injected knowledge of physics, that is principle, the only thing it has to do is to resolve uncertainty
available at t=0. about causes of [...] sensations.â€ â€“ Friston [23]
At time step t the agent first takes an action aË† , which
t
leads to observations yË†. Given the tuple of all observations The active inference paradigm postulates that an agentâ€™s
t
and actions performed thus far, which we denote as yË†Â¯ = desires are encoded by their generative models, which
0:t
(yË† ,aË† ), a rational agent will seek the minimal update attributemoremasstostatesandobservationsthatarealigned
0:t 0:t
of past/prior beliefs about x that satisfies the constraints with preferred outcomes [24]. Humans desire to stay alive
0:t
imposed by the new data - beliefs must only be revised to the and hence place a strong prior on equilibrium conditions
6 ACCEPTEDPREPRINT,IEEETUFFC,2024
manifesting in e.g. homeostasis, and subsequent observations This is often a reasonable assumption for active perceivers
such as â€œnot being hungry or too coldâ€. In active inference, and generally holds for observation models of the form y =
t
these priors subsequently drive inference of actions through f(x ,a )+n in which the remaining entropy in y given full
t t t t
the pursuit of observations and rational beliefs about hidden knowledge of x is solely determined by the noise n , and
t t
states that are not surprising. In doing so, agents actively thus independent of a .
t
resist the increase of posterior entropy.
As mentioned before, we will here restrict our treatment
VI. IMPLEMENTING PERCEPTION-ACTION LOOPS
to agents that take actions that are purely â€œscientificâ€ in
Estimating the entropies in Eqn. (12) is not trivial in practice,
nature, i.e. they maximize information gain, placing no ex-
especially for the flexible class of density functions needed to
plicitutilitarianpreferenceonparticularobservations.Wewill
accurately describe high-dimensional images and their (possi-
discuss agents that do not strive for survival but â€œjustâ€ seek
bly nonlinear) observations. It is worth reiterating that simple
to understand the world from the viewpoint of an external
linear models based on members of the exponential family
observer [3].
that do have closed-form expressions are insufficient for what
Westartbydefiningtheaction-valuefunctional,V(Â·),which
we try to achieve. Instead, we hypothesise that performing
evaluates the expected future value for each sequence of
approximateinferencewithaccurate(deep)generativemodels
actions aâ€² given a probability density function govern-
t+1:T is more fruitful than pursuing exact inference with overly
ing our present beliefs about future states and observations
V(aâ€² ;q(x ,y |aâ€² ,yË†Â¯ )).Forbrevity,wewill simple models. This confronts us with the daunting task
t+1:T t+1:T t+1:T t+1:T 0:t of performing reliable approximate inference using highly
let the conditioning on the probability density function be
nonlinear(deep)generativemodels.Forthesakeofbrevity,in
implicit in what follows. We will also use the shorthand
what follows we make the dependency of future observations
notation Ï„ = t + 1 : T. We seek to minimize uncertainty
on the actions aâ€² implicit and use shorthand notation yË†Â¯ to
about the state x , and hence our action-value-functional of Ï„
Ï„ indicate the tuple of all past observations and actions yË†Â¯ .
choice will be the negative expected posterior entropy: 0:tâˆ’1
V(aâ€²)â‰œâˆ’H(x |y ,aâ€²,yË†Â¯ ) (9)
Ï„ Ï„ Ï„ Ï„ 0:t A. Approximateposteriorinference
q(x ,y |aâ€²,yË†Â¯ )
=E log Ï„ Ï„ Ï„ 0:t , Acommonapproachtoapproximatingtheposteriordensity
q(xÏ„,yÏ„|aâ€² Ï„ ,yË†Â¯0:t) q(y Ï„ |aâ€² Ï„ ,yË†Â¯ 0:t ) functionistouseasetofsamples/particles{x1 ,...,xNp}âˆ¼
0:T 0:T
and the optimal action is: p(x |yË†Â¯) and weights {w ,...,w } that are proportional
0:T 1 Np
aâˆ— =argmax V(aâ€²). (10) to the probability of the sample belonging to the target
Ï„ aâ€² Ï„ distribution:
Ï„
Colloquially, the system will pursue actions that are ex- Np
(cid:88)
pected to â€œsharpenâ€ the posterior the most. As was already q(x 0:T |yË†Â¯)= w i Î´(x 0:T âˆ’xi 0:T ), (13)
shownbyLindley[25],minimizingexpectedposteriorentropy i=1
isequivalenttomaximizingtheconditionalmutualinformation with (cid:80) w = 1. Sampling methods make no explicit as-
i i
between state and future observations, such that we have: sumptions about the functional form of the density functions
aâˆ— =argmax I(x ,y |aâ€²,yË†Â¯ ) (11) and hence have low bias. This does come at the expense of
Ï„ aâ€² Ï„ Ï„ Ï„ 0:t relatively high variance, and resolving it requires paying a
Ï„
=argmaxE logp(y |x ,aâ€²) computational price, e.g. in terms of the number of samples
aâ€² Ï„ (cid:124)
q(xÏ„,yÏ„|aâ€²
Ï„
,yË†Â¯0:t)
(cid:123)(cid:122)
Ï„ Ï„ Ï„
(cid:125) required to get good (mode) coverage of the distribution.
âˆ’H(yÏ„|xÏ„,aâ€²
Ï„
,yË†Â¯0:t) Many methods allow for effective and/or efficient sampling
âˆ’E logq(y |aâ€²,yË†Â¯ ). (12) from a posterior. For instance, deep generative diffusion
q(yÏ„|aâ€²
Ï„
,yË†Â¯0:t) Ï„ Ï„ 0:t
(cid:124) (cid:123)(cid:122) (cid:125) models enable posterior sampling by integrating a data-
H(yÏ„|aâ€² Ï„ ,yË†Â¯0:t) consistency/likelihood step into the reverse diffusion process
Note that we again explicitly distinguish factors that are [18]. Another alternative is to use classic sequential Monte-
inferred based on past data (the approximate posteriors q(Â·)) Carlo methods (e.g. particle filters) [26]. These methods
and forward generative models p(Â·). The conditional mutual exploit the sequential structure of states to perform highly
information can also be interpreted as the expected prior- efficient tracking of the distribution under arbitrary (possibly
posterior gain, i.e. the expected divergence between the prior non-differentiable) nonlinear forward models. Such forward
andposterior,orcolloquially,â€œhowmuchithasbeenupdatedâ€. models can be physics-based, or learned from data, e.g. a
The larger the update, the more information has been gained pre-trained deep generative latent variable model that fits
(cid:82)
from the measurement. p(x ) = p(x |z)p(z)dz using variational inference [13].
t z t
From Eqn. (12) we can also appreciate that maximizing An extensive review of methods for approximating posterior
mutual information, or information gain, can be achieved distributions given nonlinear high-dimensional models is
by selecting the actions that maximize the marginal differ- beyond the scope of this paper. We refer the interested reader
ential entropy of future observations H(y |aâ€²,yË†Â¯ ), if the to [18], [27]â€“[29]. As before, we will however give some
Ï„ Ï„ 0:t
conditional entropy of the observations given the state is not special attention to diffusion models.
dependentontheactions,i.e.H(y |x ,aâ€²)=H(y |x ),âˆ€aâ€².
Ï„ Ï„ Ï„ Ï„ Ï„ Ï„
VANSLOUN:COGNITIVEULTRASOUND 7
Diffusion posterior sampling: The reverse diffusion pro- with Î£Ëœ (xi) being the sample covariance, estimated using
yÏ„ Ï„
cess can be conditioned on a measurement obtained us- N samples from {y1,...,yNp} âˆ¼ p(y |xi), and the equality
p Ï„ Ï„ Ï„ Ï„
ing a model y|x âˆ¼ p(y|x) to produce samples from the in Eqn. (19) being true if Î£Ëœ (xi)=Î£Ëœ for all xi.
yÏ„ Ï„ yÏ„ Ï„
posterior p(x|y). This is done by changing the uncondi- We now proceed with the marginal entropy H(y |yË†Â¯). Col-
Ï„
tional score function in (5) into a posterior score function loquially, this reflects the diversity of futures that may be
âˆ‡ xts logp ts (x ts |y). However, the posterior score is not trivial observedforeachoftheactions.Ultimately,onlyoneobserva-
toevaluate,asrefactoringitintoa(data-conditional)likelihood tion will manifest, but it is the diversity in hypothetical future
score, âˆ‡ xts logp ts (y|x ts ), and the original (unconditional) observations that measures the uncertainty that one has about
prior score, requires computing the intractable likelihood of theresultofanimagingexperiment.Approximatingitrequires
noise-perturbed states. This has led to various approximate choosing a reasonable marginal model, which is not obvious.
guidance schemes [18], [30], [31], most of which exploit In general, the marginal distribution over future observations
Tweedieâ€™s formula, which can be thought of as a one-step p(y |yË†Â¯) may be multimodal, as the posterior distribution
Ï„
denoising process from t s â†’ 0 using our trained diffusion p(x Ï„ |yË†Â¯) is likely multimodal too for high-dimensional states
modeltoestimatethetruefully-denoisedsamplex 0 asfollows: and partial observations. Nevertheless, in the examples of
section VII we will see that approximating the marginal as
xË† =E[x |x ]
0 0 ts a multivariate Gaussian is often a reasonable choice from a
1
pragmatic perspective.
â‰ˆ (x +(1âˆ’Î±Â¯(t ))s (x ,t )). (14)
(cid:112)
Î±Â¯(t s )
ts s Î¸ ts s
Toillustratethis,considerthefollowingthoughtexperiment.
Anagentevaluatestheexpectedvalueoftwocandidateactions
Diffusion Posterior Sampling (DPS) uses (14) to approximate
bysamplingexpectedobservationsinR1forbothofthem.The
âˆ‡ logp(y|x ) â‰ˆ âˆ‡ logp(y|xË† ), which for (non)linear
xts ts xts 0 firstactionresultsinaunimodalempiricaldistributionoverhy-
Gaussian likelihood models leads to a guidance gradient
pothetical observations of which all samples have some small
âˆ‡ ||y âˆ’ f(xË† )||2 that can be straightforwardly evaluated.
Th x i t s s measurem 0 ent 2 -guidance gradient step is then alternated nonzero â„“ 2 distance to each other. The second action results
in a bimodal distribution over hypothetical observations, with
with the standard reverse diffusion steps using the uncondi-
the â„“ distance among samples within each mode being close
tional score. 2
to zero, but the â„“ distance between the modes being very
2
large. Fitting a (unimodal) Gaussian entropy model to both
B. Approximateentropymodels distributions would lead to a higher marginal entropy for the
latter:thelargeâ„“ distancebetweenthemodeswoulddominate
2
Givenasetofposteriorsamples,thenextstepistoestimate
the variance. As such the agent would select the latter action.
the entropy terms in our action-value function. Recall that
In many risk-sensitive applications, such as medical imaging,
our action-value function is the action-conditional mutual
selecting the latter action would arguably indeed be preferred,
information between state and future observations, and de-
as it would discriminate between two equally plausible but
composes into two entropies: The marginal entropy of the
(semantically)verydifferentmodes.TheentropyofaGaussian
observations, H(y |yË†Â¯), and the state-conditional entropy of
Ï„ marginalisareasonablesurrogateforthisâ€œdistanceâ€,although
the observations, H(y |x ,yË†Â¯) (see Eqn. (12)). In the most
Ï„ Ï„ the model fit itself can be poor (as in the bimodal example
general case, both depend on the action, although there are
given above). The analogy is less intuitive for observations in
many practical examples for which the latter dependency is RN, but it is clear that in that case one should also model the
negligible (recall the discussion at the end of Section V).
correlation across the elements in the observation vector.
We nevertheless start with the expected conditional entropy
Assuming that the marginal observations indeed follow a
(the remaining uncertainty about y if x were known) as
Ï„ Ï„ multivariate Gaussian pdf, i.e. q(y |yË†Â¯) = N(Âµ ,Î£ ), and
choosing a reasonable entropy model is more straightforward.
Ï„ y|yË†Â¯ y|yË†Â¯
that we are given posterior samples {y1,...,yNp} âˆ¼ q(y |yË†Â¯),
Assuming a forward measurement model with additive Gaus- Ï„ Ï„ Ï„
we have:
sian noise n , i.e. y = f(x ,a ) + n , the conditional
Ï„ Ï„ Ï„ Ï„ Ï„
entropy model is also a Gaussian. Given posterior samples H(y |yË†Â¯)â‰ˆ constant+ 1 log|Î£Ëœ |, (20)
{x1,...,xNp}âˆ¼p(x |yË†Â¯) with uniform weights, we then have Ï„ 2 y|yË†Â¯
Ï„ Ï„ Ï„
that: where Î£Ëœ is the sample covariance, estimated using N
y|yË†Â¯ p
H(y |x ,yË†Â¯)=E âˆ’logp(y |x ) (15) samples from p(y Ï„ |yË†Â¯). This result is intuitive: the determinant
Ï„ Ï„ q(xÏ„,yÏ„|yË†Â¯) Ï„ Ï„ |Î£Ëœ | is also called the generalized variance, and is equal to
=âˆ’E E logp(y |x ) (16) y|yË†Â¯
q(xÏ„|yË†Â¯) p(yÏ„|xÏ„) Ï„ Ï„ the product of the eigenvalues of Î£Ëœ
y|yË†Â¯
.
1 (cid:88)
Np
Alternatively, we can start from the approximate posterior
=âˆ’ E logp(y |xi) (17)
N p p(yÏ„|xi Ï„ ) Ï„ Ï„ familyinEqn.(13)toderiveamodelforthemarginal.Again,
i=1 assuming Gaussian measurements y = f(x ,a )+n , we
Ï„ Ï„ Ï„ Ï„
â‰ˆ constant+ 1 (cid:88) Np 1 log|Î£Ëœ (xi)| (18) obtain that q(y Ï„ |yË†Â¯) is a mixture of Gaussians, i.e.:
N 2 yÏ„ Ï„
p i=1 Np
(cid:88)
= âˆ— constant+ 1 log|Î£Ëœ |, (19) q(y Ï„ |yË†Â¯)= w i N(Âµ i ,Î£ i ), (21)
2 yÏ„ i=1
8 ACCEPTEDPREPRINT,IEEETUFFC,2024
(a)
TX âˆ— Action â€²
sequence ğ‘ğ‘ğ‘¡ğ‘¡=ğ¼ğ¼(ğ’šğ’šğ‘¡ğ‘¡,ğ’™ğ’™ğ‘¡ğ‘¡|Gğ‘ğ‘eğ‘¡ğ‘¡,nğ’šğ’šï¿½ï¿½er0a:ğ‘¡ğ‘¡tâˆ’iv1e) predictions
1
â€²
steer ğ‘ğ‘ ğ’™ğ’™ğ‘¡ğ‘¡, ğ’šğ’šğ‘¡ğ‘¡|ğ‘ğ‘ğ‘¡ğ‘¡, ğ’šğ’šï¿½ï¿½0:ğ‘¡ğ‘¡âˆ’1
Agent
Probe Fetal heart
2 Sequential Monte Carlo
RX Doppler
processing ğ’šğ’šï¿½ğ‘¡ğ‘¡ Perception ğ‘ğ‘ ğ’™ğ’™0:ğ‘¡ğ‘¡| ğ’šğ’šï¿½ï¿½0:ğ‘¡ğ‘¡
3
Heart rate
estimation
ğ”¼ğ”¼
Poğ‘ğ‘s(ğ’™ğ’™teğ‘¡ğ‘¡|r iğ’šğ’šï¿½ï¿½o0r:ğ‘¡ğ‘¡ m)( e
ğ’™ğ’™
ağ‘¡ğ‘¡n
)
Time [min]
]dar[
elgnA
(c)
Time [min]
]mpb[
etar-traeH
(b)
GT GT Estimates Estimates
Fig.3. Example:ActivebeamsteeringusingsequentialMonte-Carlo[32].(a)Dopplertargettrackingusingcognitiveultrasound.(1)Theagent
selectstheaction(beamsteeringangleÎ¸
t
tx)thathasthehighestexpectedinformationgaingivengenerativepredictionsq(xt,yt|aâ€²
t
,yË†0:tâˆ’1).(2)
ThisactionpromptsanewDopplerobservationyË†t,whichinturntriggersanupdateoftheposteriorq(x0:t|yË†0:t),implementedusingasequential
Monte Carlo method. (3) Finally, the posterior mean fetal heart location at that timestep is communicated to a heart rate estimation module
alongsidethereceivedDopplerdata.(b)Real-timelabsetupmimickingthescenariodescribedin(a),usingaphasedarraytransducermountedon
atranslationstagethattransmitsafocusedbeamcontrolledbytheagenttotrackaâ€œbeatingâ€chickenheart.(c)Positionaltrackingandheartrate
estimation(red)fromadaptivelysteeredfocusedbeams,againstgroundtruth(blue).
where Âµ = f(xi,a ). A variational approximation to the A. ActivebeamsteeringusingsequentialMonte-Carlo
i Ï„ Ï„
marginal differential entropy is then given by [33]:
1) Generativemodel: We start with an agent that is tasked
Np Np with the sequential selection of optimal focused transmit
(cid:88) (cid:88)
H(y
Ï„
|yË†Â¯)â‰ˆâˆ’ w
i
log w
j
eâˆ’KL[N(Âµi,Î£i)||N(Âµj,Î£j)]
beams for tracking the position of a moving Doppler target
i=1 j=1 x = (Î¸ ,z ,Ï‰ ), with Î¸ its angular position, z its axial
t t t t t t
(cid:88)
Np position,andÏ‰
t
itsDopplerfrequencyattimestept[32]. The
+ w i H[N(Âµ i ,Î£ i )], observation at timestep t, yË† t âˆˆ RNÎ¸, is an angular power
i=1 Doppler profile. It is computed by pixel-based receive beam-
(22)
formingandsubsequentDopplerprocessingofanensembleof
whichcanbeevaluatedinclosedform.Ifoneassumesthatthe channeldatacomingfromfocused transmitswithafixedfocal
measurement noise has constant diagonal isotropic covariance depth ztx and steering angle aË† = Î¸tx. The agentâ€™s generative
t t
Î£ i =Î£ j =Ïƒ y 2I, Eqn. (22) further simplifies to: model given a series of actions aâ€² 0:T is given by:
(cid:88)
Np
(cid:88)
Np
âˆ’
||Âµiâˆ’Âµj||2
2 p(y ,x |aâ€² )=p(y |x ,aâ€² )p(x ), (24)
H(y
Ï„
|yË†Â¯)â‰ˆconstantâˆ’ w
i
log w
j
e 2Ïƒy 2 , (23) 0:T 0:T 0:T 0:T 0:T 0:T 0:T
i=1 j=1
with
which illustrates the intuitive connection between distance
among the samples in observation space and the differential (cid:89) T
p(x )=p(x ) p(x |x ), (25)
entropy. 0:T 0 t tâˆ’1
We now have all the ingredients to close the ultrasound t=1
perception-action loop: (1) perceptual inference of anatomical assuming x has sequential Markovian structure, with lin-
state posteriors using deep generative models, followed by ear Gaussian state transition dynamics p(x |x ) =
t tâˆ’1
(2) action selection through maximization of an approximate N(Ax ,Î£ ), and a Gaussian memoryless nonlinear obser-
tâˆ’1 x
action-conditional mutual information between future states vationmodelp(y |x ,a )=N(f(x ;a ),Î£ ). Here,f(Â·;a ):
t t t t t y t
and future observations. R3 â†’ RNÎ¸ is a forward observation function that maps the
target state to a simulated power Doppler profile, using an
VII. EXAMPLES
approximate action-conditional transmit beamprofile.
We will now illustrate the concepts described above in the
context of concrete ultrasound imaging applications. From 2) Perception: We here track the posterior using particle
sections VII-A, to VII-C, we will cover (A) active inference filtering [26], a sequential Monte-Carlo method based on the
with low-dimensional generative models based on first princi- posterior family in Eqn. (13). We use the state transition prior
ples, (B) perceptual inference with complex data-driven deep as the proposal distribution (bootstrap particle filter), i.e. we
generative models in high dimensions, and finally, (C) active draw proposals as xi âˆ¼ p(xi|xi ), and update the weights
t t tâˆ’1
inference using deep generative models. of the resulting N particles by their likelihood given the new
p
VANSLOUN:COGNITIVEULTRASOUND 9
(a) input (b)
ğ’šğ’š
GT EchoNet dehazed EchoNet input
RNCg
dehazed Input NCSNv2 BM3D Ours
ğ’™ğ’™ï¿½
0.90
0.85
0.80
0.75
0.70
4 5 6 7 8 9
FWHMlat[px]
Fig.4. Example:Multipathhazesuppressionusingdiffusionmodels.(a)ExampleinputyanddehazedoutputxË†(top)alongwithautomaticleft-
ventricular(LV)segmentationsbyEchoNet[34](bottom).NotehowtheLVareaisunderestimatedonthehazy,cluttered,inputdata,andhowthisis
improvedafterdehazing.(b)Tradeoffbetweentissuecontrast(gCNR)andlateralresolution(FWHM),showinghowdehazingbygenerativediffusion
models(ours)greatlyimprovesthegCNRwhilecompromisingmuchlessonresolutionthandenoisingmethodssuchasBM3D.Italsocompares
favourablyagainstadiscriminativeneuralnetworkusingsupervisedtrainingonphantomdata(NCSNv2).Imageadaptedfrom[6].
observations, i.e.: At each timestep t, we perform a Doppler transmit sequence
(ensemble length: 2000 frames, PRF: 1.5 kHz, center fre-
wË†i =wi p(yË† |xi,aâ€²), (26)
t tâˆ’1 t t t quency: 3.125 MHz) with an adaptively-steered focused beam
a p n ra d ct t i h c e e n , w no e rm ad a d li i z ti e on s a u l c ly h t p h e a r t fo w rm t i = res (cid:80) am wË† i t i w p Ë† l t i i . ng As of pe p r ar c ti o c m le m s o to n a in t t Î¸ e t t g x r . a A l f a t c e r r o p s i s x t e h l- e b d a e se p d th re a c n e d iv D e o b p e p a l m er fo a r x m es in t g o , g w e e t c a o n m o p b u s t e e rv t e h d e
power-Doppler angular profile yË† . The information-seeking
avoiddegeneracy[26].Thisposteriorupdatethenallowssam- t
pling updated futures from p(y |yË† ,aâ€² ), using the par- agent accurately tracks the moving target and retains precise
ticles yi âˆ¼ p(y |xi ,aâ€² t+ ) 1 , w 0 i : t t h x t+ i 1 âˆ¼ p(xi |xi). downstream heart-rate estimates by adequately steering the
t+1 t+1 t+1 t+1 t+1 t+1 t beam and maintaining high Doppler SNR. Performance was
These hypothetical futures are used to drive action selection.
also quantified for various Doppler SNR levels in-silico, with
3) Action: At each timestep t, the agent greedily selects the SNR defined relative to the peak Doppler signal under ideal
next beamsteering action aâˆ— by maximizing the expected steering. These results show that heart rate estimation using
t+1
information gain I(y ,x |aâ€² ,yË† ) across a discrete cognitiveadaptivesteeringremainedaccurate(within5bpmof
t+1 t+1 t+1 0:t
set of candidate actions aâ€² âˆˆ S , which in light of the thegroundtruth)atabout20dBlowerSNRlevelscomparedto
t+1 a
aforementioned generative model is equivalent to maximizing non-adaptivesteering,adirectresultofthedramaticreduction
the marginal differential entropy of the observations: the in beamforming gain when the target moved out of the static
conditional entropy is constant for all actions, and solely beam.
determined by the receiver noise covariance Î£ . We further
y
assume a marginal multivariate Gaussian distribution and B. Multipathhazesuppressionusingdiffusionmodels
approximate the differential entropy using N p samples from 1) Generative model: The previous example shows how
the marginal {y1 ,...,yNp }âˆ¼p(y |yË† ,aâ€² ) as: cognitive ultrasound imaging improves target tracking given a
t+1 t+1 t+1 0:t t+1
physics-basedmodeloftheobservationsandalinearGaussian
1
aâˆ— t+1 =argmax 2 log|Î£Ëœ(aâ€² t+1 ,yË† 0:t )|. (27) state transition function in R3. To move to more complex and
aâ€² t+1 âˆˆSa high-dimensional states, such as high-resolution reflectivity
This action yields a new observation yË† , which in turn maps in RNÃ—N, we will first need to expand the modelling.
t+1
prompts an update of the posterior distribution (perception), Wewillthereforetrainagenerativediffusionmodelfromdata.
closing the loop. We consider an agent that is tasked with the suppression
of multipath clutter (â€œhazeâ€) from (linearly) beamformed RF
4) Results: Figure 3 shows a real-time implementation data patches y âˆˆRNÃ—N in cardiac imaging [6]. To that end,
of the perception-action loop for a dynamic Doppler target we define the following generative model, assuming linear
mimicking a beating fetal heart in a lab setup. We use an scattering:
s5-1 phased array transducer (Philips) mounted on a motion
p(y,x,h)=p(y|x,h)p(x)p(h), (28)
stage, and connected to the Verasonics Vantage 256 system.
10 ACCEPTEDPREPRINT,IEEETUFFC,2024
Perception
(a) Past observations (b)
Conditional Temporal
Unobserved anatomy Diffusion Model Reconstructions
Posterior sampling
âˆ— ğ‘ ğ‘ ğœƒğœƒ(ğ‘¥ğ‘¥ï¿½ğœğœ,ğœğœ)
ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’1
(ğ‘–ğ‘–)
ğ‘¥ğ‘¥ï¿½ğ‘¡ğ‘¡âˆ’1
Acquire scanğ‘¦ğ‘¦ï¿½lğ‘¡ğ‘¡inâˆ’eğ‘‡ğ‘‡s+1:ğ‘¡ğ‘¡âˆ’1 Iterative denoising ğ‘¥ğ‘¥ğ‘¡ğ‘¡ ~ğ‘ğ‘(ğ‘¥ğ‘¥ğ‘¡ğ‘¡|ğ‘¦ğ‘¦ï¿½ï¿½ğ‘¡ğ‘¡âˆ’ğ‘‡ğ‘‡+1:ğ‘¡ğ‘¡)
Maximum
likelihood
âˆ—
ğ‘¥ğ‘¥ğ‘¡ğ‘¡
ğ‘¦ğ‘¦ï¿½ğ‘¡ğ‘¡=A ğ‘ğ‘ğ‘¡ğ‘¡
âˆ—
ğ‘¥ğ‘¥ğ‘¡ğ‘¡
âˆ— ğ‘¥ğ‘¥ï¿½ğ‘¡ğ‘¡
ğ‘¥ğ‘¥ğ‘¡ğ‘¡ âˆ— +1 A(ğ‘ğ‘ğ‘¡ğ‘¡ âˆ— ) ğ‘ğ‘ğ‘¡ğ‘¡ âˆ— =argmax ğ¼ğ¼(ğ‘¦ğ‘¦ğ‘¡ğ‘¡,ğ‘¥ğ‘¥ğ‘¡ğ‘¡|ğ‘ğ‘ğ‘¡ğ‘¡ â€² ) ğ‘¥ğ‘¥ï¿½ğ‘¡ğ‘¡+1
Select scanlines
Action
emit
dezitercsiD
ğ‘¡ğ‘¡
â€² ğ‘ğ‘ğ‘¡ğ‘¡âˆˆğ‘†ğ‘†ğ‘ğ‘
Fig.5. Example:Activesubsamplingusingtemporaldiffusionmodels.(a)Overviewoftheperception-actionloopforultrasoundscanlineselection.
Attimestept,theagentacquireskscanlinesthatmaximizeexpectedinformationgain.Itthencombinesthemwiththepastk(T âˆ’1)scanlines
acquiredattimestepstâˆ’T +1:tâˆ’1,andperformperceptualinferenceviadiffusionposteriorsamplingtoyieldsamplesx(i).Themostlikely
t
sampleisselectedasthefinalreconstruction.Next,theposteriorsamplesareusedtoestimateexpectedinformationgainattimestept+1,and
theactionaâˆ— thatmaximizesitisusedtoacquirethenextscanlines.(b)Meanabsolutereconstructionerror(MAE)ofacognitiveagent(Max
t+1
informationSampling)vsarandomagent(RandomSampling).Bothusetheconditionaltemporaldiffusionmodelforinference.Eachbluedotisa
hold-outtestsequencefromtheCAMUSdataset.Pointsabovethereddashedlinearepointsforwhichcognitiveimagingoutperformsarandom
scanlineselection.
wherexâˆˆRNÃ—N isthebeamformeddirectpathcontribution, p(yËœ |x ,h ) = N(x +h ,Ï2I), with yËœ âˆ¼ q(y |y )
ts ts ts ts ts ts ts 0
and h âˆˆ RNÃ—N is the multipath component. The generative being a noise-perturbed observation.
model assumes that the direct and multipath RF data are
statistically independent, governed by their respective priors 3) Results: Weevaluateperformanceon1020cardiacultra-
soundRFframesfromtwodifficult-to-imagevolunteersacross
p(x) and p(h). We further assume that p(y|x,h) is a linear
Gaussian model of the form N(x + h,Ïƒ2I). We perform 4 cardiac views (three chamber, four chamber, parasternal
n
long axis and parasternal short axis), acquired using an X5-
denoising score matching on (unpaired) training data samples
{xË†1,...,xË†L}, and {hË†1,...,hË†L} to learn two score functions, 1C matrix transducer connected using a Philips EPIQ scanner
in harmonic imaging mode. Figure 4a qualitatively shows
conditioned on the SDE timestep t : âˆ‡ logp (x ) â‰ˆ
s xts Ï„ ts
how dehazing significantly boosts tissue contrast, as well as
s (x ,t ) and âˆ‡ logp (h ) â‰ˆ s (h ,t ). For architec-
Î¸ ts s hts ts ts Ï• ts s
improving the downstream left-ventricular segmentation. In
tural and training details see [6].
Fig. 4b, we quantitatively evaluate performance, showing that
2) Perception: We then perform diffusion posterior sam- our deep generative dehazing model strikes a good balance
pling x ,h âˆ¼ p(x ,h |y) through the formulation of a between tissue contrast and resolution, comparing favourably
ts ts ts ts
jointconditionaldiffusionprocess{x ,h |y} ,inturn to other methods.
ts ts tsâˆˆ[0,T]
producing a joint conditional reverse-time SDE:
d(x ,h )= (cid:2) f(t )(x ,h )âˆ’... C. Activesubsamplingusingtemporaldiffusionmodels
ts ts s ts ts
g(t )2âˆ‡ logp(x ,h |y) (cid:3) dÏ„ +g(t )dwÂ¯ , 1) GenerativeModel: Our final example concerns an agent
s xts ,hts ts ts s ts
(29) thatperformsadaptivecompressivesensing[35],[36]through
active subsampling of ultrasound scanlines. We will use the
where the posterior score at SDE timestep t ,
s approach by Nolan et al. [37] to design subsampling masks
âˆ‡ logp(x ,h |y), is approximated as:
xts ,hts ts ts that maximize information gain given a diffusion-based gen-
ï£± ï£´ï£´ï£² âˆ‡ xts logp(x ts ,h ts |y)â‰ƒs Î¸ (x ts
+
,t
âˆ‡
s )
xts logp(y|x ts ,h ts )
,
e
d
r
iv
at
e
i
r
v
g
e
in
m
g
o
w
d
a
e
v
l.
es
S
,
u
o
b
r
sa
o
m
th
p
e
l
r
in
w
g
ise
tr
)
an
is
sm
ty
i
p
t
i
e
c
v
a
e
ll
n
y
ts
us
(
e
b
d
e
t
i
o
t s
m
ca
in
n
i
l
m
in
i
e
z
s
e
,
ï£´ï£´ï£³ âˆ‡ hts logp(x ts ,h ts |y)â‰ƒs Ï• (h ts
+
,
âˆ‡
t s )
logp(y|x ,h )
c
O
o
u
s
r
ts
a
a
g
s
e
s
n
o
t
ci
i
a
s
te
ta
d
s
w
ke
i
d
th
w
da
it
t
h
aa
th
c
e
qu
r
i
e
s
c
it
o
io
n
n
st
,
r
s
u
u
c
c
ti
h
on
as
o
a
f
cq
a
u
s
is
e
i
q
ti
u
o
e
n
nc
ti
e
m
o
e
f
.
hts ts ts
(30) T ultrasound frames x 1:T âˆˆ RNzÃ—NyÃ—T from goal-directed
inwhichweusethegenerativemodelinEqn.(28),Bayesrule, partial observations y 1:T âˆˆ RNzÃ—kÃ—T, with a budget of k <
andthatthegradientofthemarginalp(y)withrespecttoxand N y focused scanlines per frame. We formulate the following
hiszero.AsnotedinsectionVI-A,theconditionaldistribution observation model:
of y given noise-perturbed states x and h , p(y|x ,h ),
ts ts ts ts y
t
=A(a
t
)x
t
, (31)
is generally intractable, unlike the known data likelihood
factor in the generative model p(y|x ,h ). Following Song with A(a ) being the time-varying (structured) subsampling
0 0 t
et al. [19], we therefore approximate it as p(y|x ,h ) â‰ˆ matrixthatgivenactiona selectsallsamplesbelongingtoK
ts ts t
VANSLOUN:COGNITIVEULTRASOUND 11
scanlinesfromthevectorizedframex .Theagentâ€™sgenerative imaging as an active inference problem [24], with clear ties
t
model is then given by: to other fields that have a long tradition, such as Bayesian
experiment design [40], active perception [4], active hypoth-
p(y ,x |aâ€² )=p(y |x ,aâ€² )p(x ), (32)
1:T 1:T 1:T 1:T 1:T 1:T 1:T esis testing [41], and adaptive compressed sensing [36]. We
(cid:89) T postulate that recent advances in deep generative modelling
=p(x ) p(y |x ,aâ€²), (33)
1:T t t t unlock the potential of these approaches for high-dimensional
t=1 imaging [37], [42], and specifically ultrasound.
assuming a Gaussian memoryless linear observation model Active inference also shares similarities with other fields,
p(y t |x t ,aâ€² t )=N(A(aâ€² t ),Î£ y ) that can be factorized in time. such as reinforcement learning (RL) and stochastic optimal
We will again use a diffusion model to establish the spa- control [43]. In stochastic optimal control, policies are de-
tiotemporal generative prior p(x 1:T ). To that end, we sample signedtominimizeanexpectedKLdivergencetoagoalprior.
sequences of T = 4 frames xË†Â¯l t = [xË†l tâˆ’T+1 ,...,xË†l t ] from the Like in active inference, actions are selected based on real-
CAMUSdataset[38],andconvertthemintopolarcoordinates. timesimulations(inference)offuturestatesbasedonpastand
We use denoising score-matching to train the (now spatiotem- present observations. In both active inference and stochastic
poral) score function s Î¸ (xÂ¯ ts ,t s ) â‰ˆ âˆ‡ xÂ¯ts logp ts (xÂ¯ ts ) using optimal control, an optimal policy can be specified as some
a U-Net architecture based on [39]. We use 400 videos for explicit functional of a probability density function about the
training and 50 for testing. One video contained about 19 expected future, e.g. an explicit information measure (such
frames on average. as the mutual information), a free energy, or a divergence
suchastheKLdivergence.IndeepRL,optimizedpoliciesare
2) Perception: At each timestep t, we consider the T
derivedfromdatabasedon(delayed)rewardscomputedusing
most recent measurements yË† , and use diffusion pos-
tâˆ’T+1:t simulations of actions and resulting observations (episodes).
terior sampling to generate a total of 16 posterior samples
In some variants of RL, it is the action-value function that is
{xÂ¯(1),...,xÂ¯(16)} âˆ¼ p(xÂ¯ |yË† ), tracking a distribution
t t t tâˆ’T+1:t learned from data (Q learning), and the policy is to execute
over plausible anatomical state sequences. To that end,
the action that maximizes this action-value function [44].
we again exploit Tweedyâ€™s formula and perform likelihood-
The examples given in this paper illustrate the concept of
guidance of the reverse diffusion process making use of the
cognitive ultrasound, but also have limitations. In example
factorization in Eqn. (33). The posterior sample with the
VII-C, we considered focused adaptive beamsteering actions
highest likelihood is used to produce reconstruction xË† which
t that lead to individual beamformed lines, while improved res-
is compared with the ground truth xâˆ—.
t olution could be achieved using software-based retrospective
transmitbeamforming.Forinstance,transmitsequencescanbe
3) Action: Using the generative forward model, we project
the posterior samples x(i) into hypothetical future obser- designed to optimize recovery of the full multistatic dataset
t
vations y |aâ€² , and greedily choose the k lines that that is in turn used for retrospective transmit beamforming
t+1 t+1
[45].Moreover,thecognitiveultrasoundparadigmextendsto-
maximize expected information gain at timestep t + 1, i.e.
aâˆ— =argmax I(y ,x |aâ€² ,yË† ), using a pixel- wardsoptimizationofothertransmitparameters,includingthe
t+1 t+1 t+1 t+1 tâˆ’T+1:t
aâ€²
t+1
âˆˆSa waveform.Itsefficacyhingesuponanaccurateandsufficiently
wise Gaussian marginal entropy model. As in the first exam- fast model that predicts the consequences of changing such
ple,the conditionalentropy isconstant acrossthe actions,and parameters on the received channel data, based on all the
solely determined by Î£ . In effect, the agent will thus select channel data received in the past. Neural approximations of
y
those lines that have the highest generalized variance across physics-based acoustics simulators may allow flexible trade-
the posterior samples, in order to resolve that uncertainty. offs between inference speed and accuracy.
Figure 5a gives an overview of the approach. Using cognitive imaging, a system might in the future
perform very resource-efficient 3D scanning, e.g. using com-
4) Results: The model successfully tracks the anatomical
binations of focused and unfocused transmissions in 3D that
dynamics over time and generates accurate reconstructions
jointly maximise the information gain. Moreover, a cognitive
preserving important anatomical details using only 12.5% of
ultrasound system might optimize the full transmitted wave-
the scan lines per frame. We compare the cognitive sampling
fieldtominimizetheimpactoftheacousticwindowandother
strategy to random sampling and find that maximum informa-
patient-specific challenges in difficult-to-image patients. This
tion sampling almost always outperforms random sampling
again requires modeling and inclusion of multipath effects.
(see Figure 5b). We also find that maximum information
Multipath could either be modeled as stochastic structured
sampling with 4 lines outperforms random sampling with 6
noise(aswedidforthede-hazingexample),orasadetermin-
lines, indicating that cognitive sampling, in this case, is worth
istic element of the forward model. In the stochastic setting,
50% more measurements.
a cognitive system could e.g. strive to shape the statistics of
the multipath signal such that it is easily separated from the
VIII. DISCUSSION
direct path.
This paper gives a new interpretation of the ultrasound While we see a lot of opportunities and possible appli-
transmit-receive cycle as a perception-action loop, in which cations, there are also many open questions and challenges.
pulse-echo experiments are adaptively designed to maximize First, using deep generative models introduces a bias that
informationgaingivenagenerativemodel.Ittreatsultrasound is highly desired (the structure of the natural world), but it
12 ACCEPTEDPREPRINT,IEEETUFFC,2024
potentially also introduces undesired biases e.g. arising from REFERENCES
inadequate or biased sampling of the true data distribution
[1] G. D. Cole, N. M. Dhutia, M. J. Shun-Shin, K. Willson, J. Harrison,
whentraining.Caremustbetakennottomakepriorstooinfor- C.E.Raphael,M.Zolgharni,J.Mayet,andD.P.Francis,â€œDefiningthe
mativeandrestrictive.Second,accurateposteriorestimationin real-world reproducibility of visual grading of left ventricular function
and visual estimation of left ventricular ejection fraction: impact of
complex models comes with high computational complexity,
imagequality,experienceandaccreditation,â€Theinternationaljournal
and balancing this accuracy with efficiency for real-time im- ofcardiovascularimaging,vol.31,pp.1303â€“1314,2015.
plementations is not trivial, potentially requiring hierarchical [2] Y.Nagata,K.Yuichiro,O.Takeshi,O.Kyoko,N.Akemi,O.Yutaka,and
T.Masaaki,â€œImpactofimagequalityonreliabilityofthemeasurements
inferencewithadaptivecomplexity.Third,inferringsequences
ofleftventricularsystolicfunctionandgloballongitudinalstrainin2d
of future actions across longer time horizons (instead of our echocardiography,â€EchoResearch&Practice,vol.5,no.1,pp.28â€“39,
current greedy approach) is computationally daunting, with 2018.
[3] M. Biehl, C. Guckelsberger, C. Salge, S. C. Smith, and D. Polani,
naive implementations growing exponentially in complexity
â€œExpanding the active inference landscape: more intrinsic motivations
with time. This will likely require clever sequential reduction in the perception-action loop,â€ Frontiers in neurorobotics, vol. 12,
of the action space. Future work should be geared towards p.387187,2018.
[4] R.Bajcsy,â€œActiveperception,â€ProceedingsoftheIEEE,vol.76,no.8,
addressing these challenges.
pp.966â€“1005,1988.
[5] R.Bajcsy,Y.Aloimonos,andJ.K.Tsotsos,â€œRevisitingactivepercep-
IX. CONCLUSION tion,â€AutonomousRobots,vol.42,pp.177â€“196,2018.
[6] T. S. Stevens, F. C. Meral, J. Yu, I. Z. Apostolakis, J.-L. Robert, and
Ultrasound systems engage in repeated reciprocal interactions R. J. Van Sloun, â€œDehazing ultrasound using diffusion models,â€ IEEE
with the anatomical environment that they probe, and this TransactionsonMedicalImaging,2024.
[7] Y. Zhang, C. Huneau, J. Idier, and D. Mateus, â€œUltrasound image
cycle of probing and receiving data can be interpreted as a
reconstructionwithdenoisingdiffusionrestorationmodels,â€inInterna-
perception-action loop. Active inference offers a probabilis- tionalConferenceonMedicalImageComputingandComputer-Assisted
tic framework for developing agents that implement such Intervention,pp.193â€“203,Springer,2023.
[8] H. Lan, Z. Li, Q. He, and J. Luo, â€œFast sampling generative model
perception-action loops, which when augmented with deep
forultrasoundimagereconstruction,â€arXivpreprintarXiv:2312.09510,
generative models that govern anatomical beliefs, unlocks 2023.
closed-loop high-dimensional imaging. We here showed how [9] H. Asgariandehkordi, S. Goudarzi, A. Basarab, and H. Rivaz, â€œDeep
ultrasounddenoisingusingdiffusionprobabilisticmodels,â€in2023IEEE
these principles can be used for cognitive ultrasound systems
InternationalUltrasonicsSymposium(IUS),pp.1â€“4,IEEE,2023.
that maximize information gain by changing their probing of [10] F. Hermann Ludwig, Selected Writings of Hermann Von Helmholtz.
the environment. The promise that this holds for ultrasound Conn,WesleyanUniversityPress,1971.
[11] C. Fefferman, S. Mitter, and H. Narayanan, â€œTesting the manifold
imaging is significant; it may spur a paradigm shift in the
hypothesis,â€ Journal of the American Mathematical Society, vol. 29,
design of systems, akin to cognitive radar systems, that au- no.4,pp.983â€“1049,2016.
tonomously strive to maximize diagnostic value in-situ. [12] J.M.Tomczak,DeepGenerativeModeling. Springer,2021.
[13] D.P.KingmaandM.Welling,â€œAuto-encodingvariationalbayes,â€arXiv
preprintarXiv:1312.6114,2013.
ACKNOWLEDGMENT [14] A.Creswell,T.White,V.Dumoulin,K.Arulkumaran,B.Sengupta,and
A. A. Bharath, â€œGenerative adversarial networks: An overview,â€ IEEE
The author would like to thank Tristan Stevens, OisÂ´Ä±n Nolan,
signalprocessingmagazine,vol.35,no.1,pp.53â€“65,2018.
Wessel van Nierop, and Beatrice Federici, for their contribu- [15] I. Kobyzev, S. J. Prince, and M. A. Brubaker, â€œNormalizing flows:
tions in particular to section VII. Anintroductionandreviewofcurrentmethods,â€IEEEtransactionson
pattern analysis and machine intelligence, vol. 43, no. 11, pp. 3964â€“
3979,2020.
[16] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and
B. Poole, â€œScore-based generative modeling through stochastic differ-
entialequations,â€arXivpreprintarXiv:2011.13456,2020.
[17] B. D. Anderson, â€œReverse-time diffusion equation models,â€ Stochastic
ProcessesandtheirApplications,vol.12,no.3,pp.313â€“326,1982.
[18] H.Chung,J.Kim,M.T.Mccann,M.L.Klasky,andJ.C.Ye,â€œDiffusion
posterior sampling for general noisy inverse problems,â€ arXiv preprint
arXiv:2209.14687,2022.
[19] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and
B. Poole, â€œScore-based generative modeling through stochastic differ-
Ruud J.G. van Sloun is an Associate Profes- entialequations,â€inInternationalConferenceonLearningRepresenta-
sor at the Department of Electrical Engineer- tions,2021.
ing at the Eindhoven University of Technology. [20] C.Meng,R.Rombach,R.Gao,D.Kingma,S.Ermon,J.Ho,andT.Sal-
He received the M.Sc. and Ph.D. degree (both imans, â€œOn distillation of guided diffusion models,â€ in Proceedings of
cum laude) in Electrical Engineering from the theIEEE/CVFConferenceonComputerVisionandPatternRecognition,
EindhovenUniversityofTechnology,Eindhoven, pp.14297â€“14306,2023.
The Netherlands, in 2014, and 2018, respec- [21] E.T.Jaynes,â€œOntherationaleofmaximum-entropymethods,â€Proceed-
tively. From 2019-2020 he was a Visiting Pro- ingsoftheIEEE,vol.70,no.9,pp.939â€“952,1982.
fessorwiththeDepartmentofMathematicsand [22] A.Caticha,â€œEntropicdynamics,timeandquantumtheory,â€Journalof
ComputerScienceattheWeizmannInstituteof Physics A: Mathematical and Theoretical, vol. 44, no. 22, p. 225303,
Science, Rehovot, Israel, and from 2020-2023 2011.
he was a kickstart AI fellow at Philips Research. He received an [23] K. J. Friston, â€œSelf-evidencing babies: Commentary on â€œmentalizing
ERC starting grant, an NWO VIDI grant, an NWO Rubicon grant, homeostasis: The social origins of interoceptive inferenceâ€ by fo-
andaGoogleFacultyResearchAward.Hiscurrentresearchinterests topoulou & tsakiris,â€ Neuropsychoanalysis, vol. 19, no. 1, pp. 43â€“47,
includeclosed-loopimageformation,deepgenerativelearningforsignal 2017.
processing and imaging, active signal acquisition, model-based deep [24] K.Friston,T.FitzGerald,F.Rigoli,P.Schwartenbeck,G.Pezzulo,etal.,
learning, compressed sensing, ultrasound imaging, and probabilistic â€œActiveinferenceandlearning,â€Neuroscience&BiobehavioralReviews,
signalandimagereconstruction. vol.68,pp.862â€“879,2016.
VANSLOUN:COGNITIVEULTRASOUND 13
[25] D. V. Lindley, â€œOn a measure of the information provided by an
experiment,â€ The Annals of Mathematical Statistics, vol. 27, no. 4,
pp.986â€“1005,1956.
[26] O. CappeÂ´, S. J. Godsill, and E. Moulines, â€œAn overview of existing
methods and recent advances in sequential monte carlo,â€ Proceedings
oftheIEEE,vol.95,no.5,pp.899â€“924,2007.
[27] N. Chopin, O. Papaspiliopoulos, et al., An introduction to sequential
MonteCarlo,vol.4. Springer,2020.
[28] C.Zhang,J.BuÂ¨tepage,H.KjellstroÂ¨m,andS.Mandt,â€œAdvancesinvari-
ationalinference,â€IEEEtransactionsonpatternanalysisandmachine
intelligence,vol.41,no.8,pp.2008â€“2026,2018.
[29] T. S. Stevens, H. van Gorp, F. C. Meral, J. Shin, J. Yu, J.-L. Robert,
andR.J.vanSloun,â€œRemovingstructurednoisewithdiffusionmodels,â€
arXivpreprintarXiv:2302.05290,2023.
[30] J. Song, A. Vahdat, M. Mardani, and J. Kautz, â€œPseudoinverse-guided
diffusionmodelsforinverseproblems,â€inInternationalConferenceon
LearningRepresentations,2022.
[31] L.Rout,N.Raoof,G.Daras,C.Caramanis,A.Dimakis,andS.Shakkot-
tai, â€œSolving linear inverse problems provably via posterior sampling
withlatentdiffusionmodels,â€AdvancesinNeuralInformationProcess-
ingSystems,vol.36,2024.
[32] B.Federici,R.J.vanSloun,andM.Mischi,â€œActiveinferenceforclosed-
looptransmitbeamsteeringinfetaldopplerultrasound,â€arXivpreprint
arXiv:2410.04869,2024.
[33] J.R.HersheyandP.A.Olsen,â€œApproximatingthekullbackleiblerdi-
vergencebetweengaussianmixturemodels,â€in2007IEEEInternational
Conference on Acoustics, Speech and Signal Processing-ICASSPâ€™07,
vol.4,pp.IVâ€“317,IEEE,2007.
[34] D. Ouyang, B. He, A. Ghorbani, M. P. Lungren, E. A. Ashley, D. H.
Liang, and J. Y. Zou, â€œEchonet-dynamic: a large new cardiac motion
video data resource for medical machine learning,â€ in NeurIPS ML4H
Workshop,pp.1â€“11,2019.
[35] H.VanGorp,I.Huijben,B.S.Veeling,N.Pezzotti,andR.J.VanSloun,
â€œActivedeepprobabilisticsubsampling,â€inInternationalConferenceon
MachineLearning,pp.10509â€“10518,PMLR,2021.
[36] G.Braun,S.Pokutta,andY.Xie,â€œInfo-greedysequentialadaptivecom-
pressedsensing,â€IEEEJournalofselectedtopicsinsignalprocessing,
vol.9,no.4,pp.601â€“611,2015.
[37] O.Nolan,T.S.Stevens,W.L.vanNierop,andR.J.vanSloun,â€œActive
diffusionsubsampling,â€arXivpreprintarXiv:2406.14388,2024.
[38] S. Leclerc, E. Smistad, J. Pedrosa, A. Ã˜stvik, F. Cervenansky, F. Es-
pinosa, T. Espeland, E. A. R. Berg, P.-M. Jodoin, T. Grenier, et al.,
â€œDeep learning for segmentation using an open large-scale dataset in
2dechocardiography,â€IEEEtransactionsonmedicalimaging,vol.38,
no.9,pp.2198â€“2210,2019.
[39] A.Beres,â€œDenoisingdiffusionimplicitmodels,â€2022.
[40] K.ChalonerandI.Verdinelli,â€œBayesianexperimentaldesign:Areview,â€
Statisticalscience,pp.273â€“304,1995.
[41] M. Naghshvar and T. Javidi, â€œActive sequential hypothesis testing,â€
2013.
[42] K. C. van de Camp, H. Joudeh, D. J. Antunes, and R. J. van Sloun,
â€œActive subsampling using deep generative models by maximizing
expected information gain,â€ in ICASSP 2023-2023 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP),
pp.1â€“5,IEEE,2023.
[43] K. Rawlik, M. Toussaint, and S. Vijayakumar, â€œOn stochastic optimal
controlandreinforcementlearningbyapproximateinference,â€2013.
[44] T. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul, B. Piot,
D. Horgan, J. Quan, A. Sendonaris, I. Osband, et al., â€œDeep q-
learningfromdemonstrations,â€inProceedingsoftheAAAIconference
onartificialintelligence,vol.32,2018.
[45] J. Spainhour, K. Smart, S. Becker, and N. Bottenus, â€œOptimization
of array encoding for ultrasound imaging,â€ Physics in Medicine and
Biology,2024.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Active inference and deep generative modeling for cognitive ultrasound"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   âŒ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   âœ… GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   âŒ BAD: Repeating the same claim 3+ times with slight variations
   âœ… GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
