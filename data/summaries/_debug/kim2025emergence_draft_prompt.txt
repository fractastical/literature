=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Emergence of Goal-Directed Behaviors via Active Inference with Self-Prior
Citation Key: kim2025emergence
Authors: Dongmin Kim, Hoshinori Kanazawa, Naoto Yoshida

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Abstract: Infantsoftenexhibitgoal-directedbehaviors,suchasreachingforasensorystimu-
lus,evenwhennoexternalrewardcriterionisprovided. Theseintrinsicallymo-
tivatedbehaviorsfacilitatespontaneousexplorationandlearningofthebodyand
environmentduringearlydevelopmentalstages. Althoughcomputationalmodeling
canofferinsightintothemechanismsunderlyingsuchbehaviors,manyexisting
studiesonintrinsicmotivationfocusprimarilyonhowexplorationcontributesto
acquiringexternalrewards. Inthispaper,weproposeanoveldensitymodelfora...

Key Terms: emergence, kyoto, behaviors, prior, japan, directed, inference, active, goal, tokyo

=== FULL PAPER TEXT ===

Emergence of Goal-Directed Behaviors
via Active Inference with Self-Prior
DongminKim HoshinoriKanazawa‚àó YasuoKuniyoshi‚àó
GraduateSchoolofInformationScienceandTechnology
TheUniversityofTokyo
Tokyo,Japan
{d-kim,kanazawa,kuniyosh}@isi.imi.i.u-tokyo.ac.jp
NaotoYoshida
GraduateSchoolofInformatics
KyotoUniversity
Kyoto,Japan
yoshida.naoto.8x@kyoto-u.ac.jp
Abstract
Infantsoftenexhibitgoal-directedbehaviors,suchasreachingforasensorystimu-
lus,evenwhennoexternalrewardcriterionisprovided. Theseintrinsicallymo-
tivatedbehaviorsfacilitatespontaneousexplorationandlearningofthebodyand
environmentduringearlydevelopmentalstages. Althoughcomputationalmodeling
canofferinsightintothemechanismsunderlyingsuchbehaviors,manyexisting
studiesonintrinsicmotivationfocusprimarilyonhowexplorationcontributesto
acquiringexternalrewards. Inthispaper,weproposeanoveldensitymodelforan
agent‚Äôsownmultimodalsensoryexperiences,calledthe‚Äúself-prior,‚Äùandinvesti-
gatewhetheritcanautonomouslyinducegoal-directedbehavior. Integratedwithin
anactiveinferenceframeworkbasedonthefreeenergyprinciple,theself-prior
generatesbehavioralreferencespurelyfromanintrinsicprocessthatminimizesmis-
matchesbetweenaveragepastsensoryexperiencesandcurrentobservations. This
mechanismisalsoanalogoustotheacquisitionandutilizationofabodyschema
throughcontinuousinteractionwiththeenvironment. Weexaminethisapproachin
asimulatedenvironmentandconfirmthattheagentspontaneouslyreachestoward
atactilestimulus. Ourstudyimplementsintrinsicallymotivatedbehaviorshaped
bytheagent‚Äôsownsensoryexperiences,demonstratingthespontaneousemergence
ofintentionalbehaviorduringearlydevelopment.
ResearchHighlights
‚ÄìSuggestsacomputationalmodelforearlyintentionalbehavior,integratingbody-schemaformation
andgoal-directedactionsunderthefreeenergyprinciple.
‚Äì Introduces a self-prior as an internal density model that drives the emergence of goal-directed
behaviorswithoutexternalrewards.
‚ÄìDemonstratesspontaneousreachingforastickerbyminimizingmismatchesbetweenobserved
multimodalsensoryinputsandtheempiricallyacquiredself-prior.
‚àóCo-correspondingauthors:HoshinoriKanazawa,YasuoKuniyoshi
Codeisavailableathttps://github.com/kim135797531/self-prior.
Preprint.Underreview.
5202
voN
11
]IA.sc[
2v57011.4052:viXra
1 Introduction
Infantsspontaneouslyexhibitgoal-directedbehaviors,suchasreachingforasensorystimulusor
activelyexploringtheirsurroundings,evenintheabsenceofexternalrewardsessentialforsurvival.
Thesebehaviorsarehighlymotivatedbyinternalsatisfactionandarereferredtoasintrinsicallymoti-
vatedbehaviors(Czikszentmihalyi,1990;Ryan&Deci,2000).Suchbehaviorsfacilitateunsupervised
learningwithoutexplicitexternalrewardsorpunishments,whichisknowntooffervariousadvantages
inearlydevelopmentalstages(Gopnik,2009;Kanazawaetal.,2023;White,1959;Zaadnoordijk
etal.,2022). Forexample,sensoryexperiencesarisingfromaninfant‚Äôsspontaneousmovements,such
asself-touch,notonlypromotelearningaboutone‚Äôsownbody(e.g.,acquiringbodyrepresentations)
(Hoffmannetal.,2017),butalsocontributetotheformationofanearlysenseofself(Rochat,1998).
Traditionalobservationalstudiesandneurosciencemethodologieshavebeenemployedtoinvestigate
themechanismsunderlyingintrinsicallymotivatedbehavioraldevelopment(DiDomenico&Ryan,
2017). Inparticular,computationalmodelinghasalsobeenutilizedtoquantitativelyinterpretand
predictbehavioraldevelopment(Oudeyer&Kaplan,2009;Shultz,2013). Experimentsusingcom-
putersimulationsallowresearcherstofreelymanipulateandcontrolspecificvariablestoinvestigate
variousscenarios,makingthemadvantageousforuncoveringlatenteffectsandinteractionsthatmay
bedifficulttoobservethroughconventionalbehavioralstudiesalone.
Althoughcomputationalmodelingisausefulapproachforstudyinginfantbehavioraldevelopment,
manymodelsproposedinmodernroboticsandmachinelearningtendtotreatintrinsicmotivation
primarilyasanexploratorymechanismforimprovingrewardacquisitionefficiencyinenvironments
withsparseexternalrewards(Aubretetal.,2023).Indeed,suchapproachescanenhancereinforcement
learningagents‚Äôabilitytoimproveexplicittaskperformance,suchasmaximizingvideogamescores.
However, they do not sufficiently address how entirely new behaviors emerge when no explicit
external reward objective is set, meaning that no performance criterion or information gain is
provided.
Toaddressthislimitation,weproposeanovelcomputationalmodelofintrinsicmotivationbasedon
theexpectedfreeenergy(Friston,2010;Fristonetal.,2016)(Figure1). Weintroduceaninternal
densitymodel,termedtheself-prior,whichenablesanagenttolearnastatisticalrepresentationof
incomingmultimodalsensorysignals. Whenamismatchbetweenthelearnedprobabilisticmodeland
actualobservationsisdetected,goal-directedbehavioremergeswithoutanyexplicitrewardcriterion.
Our approach combines several key features that, to our knowledge, have not been integrated in
previouscomputationalmodelsofdevelopmentalbehavior:(i)thebehavioralsetpointisautonomously
formedfromtheagent‚Äôsownexperienceratherthanexternallypredefined;(ii)specificgoal-directed
behaviorssuchasreachingemergeindependently,ratherthanservingmerelyasauxiliarymechanisms
for reward exploration; (iii) behaviors persist even without information gain, as long as there is
a mismatch with familiar states; and (iv) the mechanism is theoretically integrable with existing
extrinsicandintrinsicmotivationswithintheactiveinferenceframework. Weimplementasimulated
agentthatperceivesastimulus(suchasasticker)onitsarmandspontaneouslyexhibitsreaching
behavior toward it through the self-prior mechanism. In the latter part of this paper, we discuss
howtheproposedmechanismmaycontributetodevelopmentalscience,positioningthisstudyasa
computationalinvestigationintotheearlydevelopmentofintentionalbehaviorsininfants.
1.1 FreeEnergyPrinciple
The free energy principle posits that an agent can learn to perceive and act by minimizing the
"surprise" induced by sensory inputs (Friston, 2010). This surprise refers to the prediction error
thatariseswhentheagentencountersunexpectedsensoryobservations,andreducingthissurprise
becomestheagent‚Äôsprimaryobjective.
Ateachtimestept,theagentreceivesinformationfromtheworldthroughitssensoryorgans. We
denotethissensoryinputbyo ,referringtoitastheobservation. Althougho isalltheagentcan
t t
directlyobserve,theremayexistunderlyingfactorsintheworldthatgeneratetheseobservations.
Suchfactorsarecalledhiddenstates,denotedbys attimet. Forexample,givenaproperlylearned
t
model, an agent can infer from a tactile pattern on its arm (observation o ) that there is a sticker
t
attachedtoitsarm(hiddenstates ).
t
2
(a) (b) Initial Acquired
Sensory data ùëú Self-prior Self-prior
ùëù‡∑§( & ) = Low! ùëú ( )
ùëù‡∑§
ùëú
Time
Proprioceptive Tactile
Sensory Experiences
(c)
Expected Free Energy
Sticker
Action Current Future Planning
Figure1: Emergenceofreachingbehaviorviatheself-priorandactiveinference. (a)Whenasticker
isplacedontheleftarmofthesimulatedagent,itdetectsamismatchwithitspriorexperienceof
nothavingasticker,andreachestowardthestickerwithitsrighthandtominimizethediscrepancy.
(b) Development of the self-prior through experience: as sensory experiences are collected, the
probabilitydistributionoversensorypatternsgraduallydevelops. (c)Theactiveinferenceprocessin
whichtheagentplansfutureactionstominimizeexpectedfreeenergybyaligningsensoryinputs
withthelearnedself-prior. Asaresult,theagentperformsareachingactiontowardthesticker. A
full-bodyinfantillustrationisusedforclarity,theactualexperimentwasconductedinapseudo-3D
environment.
Consideranagentthatmustsatisfycertainbiologicalcriteriaforsurvivalinastablemanner. Todoso,
theagentmustaccuratelyunderstanditsenvironment,whichamountstoinferringp(o ). According
t
tothefreeenergyprinciple,anagentmaintainsitsownprobabilisticmodelofthesehiddenstates
because doing so enables it to approximate p(o ) and thus make inferences about the world. In
t
practice,weoftenmaximizethelogofp(o )(i.e.,logp(o ))forcomputationalconvenience,whichis
t t
referredtoasmaximizingthemodelevidence. Minimizingthenegativeterm,‚àílogp(o ),knownas
t
surprise,isthereforeequivalenttomaximizingthismodelevidence.
In reality, calculating p(o ) requires marginalization over hidden states (i.e., p(o ,s )), which is
t t t
often intractable. The variational free energy principle tackles this by introducing a variational
approximationofBayesianinference,whereintheagentindirectlyreducessurprisebyminimizing
thevariationalfreeenergyF,anupperboundon‚àílogp(o ). Formally,F isdefinedasfollows(see
t
(Parretal.,2022)fordetails):
‚àílogp(o )‚â§F =E [logq(s )‚àílogp(o ,s )]
t q(st) t t t
=D [q(s )‚à•p(s )]‚àíE [logp(o |s )] (1)
KL t t q(st) t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
complexity accuracy
where D (¬∑‚à•¬∑) denotes the Kullback‚ÄìLeibler divergence, and q(¬∑) represents the variationally
KL
approximateddistribution. Thatis,q(s )isthevariationaldistributionthatapproximatestheposterior
t
distribution over hidden states. Therefore, to reduce surprise at time t, the agent must keep the
posteriorandpriordistributionsclose(reducingcomplexity)whilemaximizingthelikelihoodof
observations(improvingaccuracy).
1.2 ActiveInferenceandExpectedFreeEnergy
Whilethefreeenergyprincipledescribeshowanagentupdatesitsmodeltoreducesurprisebased
oncurrentsensoryobservations,theagentcanalsodirectlychangeitsobservationsthroughactions
a tomakethemalignbetterwithitsinternalmodel. Thisistheessenceofactiveinference, and
t
moreconcretely,theapproximateposteriorq(s )isextendedtoincludeactionsasq(s ,a )=q(a |
t t t t
s )q(s ).
t t
3
To obtain optimal actions a for both present and future steps, a method has been proposed that
t
computesexpectedfreeenergybyconsideringtheexpectedfreeenergyatfuturetimepoints(Friston
etal.,2016). Thatis,theagentpredictsmultiplepossiblefuturestatesandselectsthetrajectorythat
isexpectedtominimizefreeenergymosteffectively.
However,sincefutureobservationscannotbeobtainedbeforethecorrespondingtimeactuallyarrives,
the agent cannot directly compute the free energy of future states. Therefore, the expected free
energyassumesthattheagentpossessesapreferredpriorpÀú(o )overdesiredobservations,which
t
actsasasetpoint. Asaresult,p(o ,s )isextendedtoincludeactionsandisbiasedtowardpreferred
t t
observations,expressedaspÀú(o ,s ,a )=p(a )p(s |o )pÀú(o ). Thus,theexpectedfreeenergyG is
t t t t t t t
definedasfollows(see(Mazzagliaetal.,2021;Millidgeetal.,2020)fordetails):
G =E [logq(s ,a )‚àílogpÀú(o ,s ,a )]
q(ot,st,at) t t t t t
‚âà‚àíE [logpÀú(o )]‚àíE [D [q(s |o )‚à•q(s )]]‚àíE [H(q(a |s ))] (2)
q(ot) t q(ot) KL t t t q(st) t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
extrinsicvalue intrinsicvalue actionentropy
whereHdenotestheinformationentropy. Thatis,toreducesurpriseatafuturetimet,theagentmust
(i)increasetheprobabilityofpreferredobservations,(ii)obtainhighinformationgainfromthose
observations,and(iii)maintaindiverseactions. Asshownabove,expectedfreeenergyintegrates
bothpragmatic(extrinsic)andepistemic(intrinsic)valueintoasingleexpression,therebynaturally
resolvingtheexploration‚Äìexploitationdilemma.
2 Self-Prior: AcquiredPreferencetoInduceGoal-DirectedBehavior
Undertheexpectedfreeenergy,agentsdonotrequirescalarrewardstobeexplicitlydefinedbythe
environment.Thisisbecausetheagentisassumedtopossessinformationaboutpreferredobservations
asapreferredpriordistribution,whichservesasthesetpointforactionplanning.
However,sincethepreferredprioristypicallygiveninafixedform,theremovalofexternalreward
signalsdoesnotimplythattheagenthasautonomouslygenerateditsgoal-directedbehavior.Therefore,
inthisstudy,weallowtheagenttoautonomouslydeterminethesetpointforitsactionplanningby
additionallylearningadensitymodel,whichwerefertoastheself-prior.
Theself-priorrepresents"theprobabilitydensityoverthefrequencyofobservedsensationsthatthe
agenthasexperienced,"andisdefinedasavariablepreferredpriorthatinducesbehavioraltendencies
fortheagenttomaintainorre-experiencefamiliarstates. Unlikefixedpreferredpriors,thismeans
thatmodelswiththesamestructurecandevelopdifferentpriorsdependingondifferentexperiences,
leadingtotheemergenceofdifferentbehaviors. Toprovideanintuitiveunderstandingofhowthe
self-prioroperates,thispaperpresentstheexampleofspontaneousreachingtowardandremovalof
astickerattachedtoone‚Äôsownbody. Thisissimilartotheexperimentalsetupof(Bigelow,1986),
whereasilenttoywasplacedonthebodyofablindinfanttoexaminereachingbehavior. Here,we
interpretthereasonwhytheinfantagentremovesastickerattachedtoitsbodyasbeingduetoa
mismatchbetweenthepriorof"oneselfwithoutasticker"and"thecurrentlyobservedself."Moreover,
thispriorhasbeenautonomouslyandgraduallydevelopedbytheagentthrougheverydayexperiences
(in which no sticker was attached). Since this self-prior does not contain specific instructions to
removethesticker,itisdistinguishedfromapproachesthatdirectlycommandreachingbyproviding
fixedsetpoints.
The implementation of the self-prior in the active inference framework is derived from a simple
modificationtothestandardexpectedfreeenergyformulation,separatingtheself-priortermfromthe
originalpreferredpriorterm. Specifically,thisisachievedbyredefiningeachpreferredobservation
ointermsoftwocomponents: (i)anextrinsicobservationoE thatmustbemaintainedtosatisfy
survivalrequirements,and(ii)anintrinsicobservationoI relatedtotheself-prior. Here,oI refersto
bodilystatesorsensationssuchasatactilepatternonthearm,thatcanvarywithoutthreateningthe
agent‚Äôsessentialfunctions. Consequently,wedecomposethepreferredpriorpÀú(o)intopÀú(oE,oI),and
inthisstudy,weassumeindependencesothatpÀú(oE,oI)‚âàpÀú(oE)pÀú(oI):
Preferredprior: pÀú(o)=pÀú(oE,oI)‚âàpÀú(oE) pÀú(oI)
(3)
(cid:124)(cid:123)(cid:122)(cid:125)
Self-Prior
4
Inourapproach,wechoosetodefinetheself-prioroverobservationsoratherthaninternalstatess.
Thisisbecause,asmentionedearlier,theself-priorcanbederivedthroughsimplemanipulationofthe
standardexpectedfreeenergyformulation,enablingnaturalintegrationoffixedextrinsicpreferences
andexperiencebasedintrinsicpreferenceswithinthesameframework. Thefactthattheself-prioris
relatedtointrinsicmotivationyetcanemergefromexternalsensorydatamayseemcounterintuitive.
Thisconfusionstemsfrominconsistentuseof"external-extrinsic"and"internal-intrinsic"terminology.
Toresolvethisconfusionandrigorouslyclassifyintrinsicmotivation,weadoptthecomputational
classificationofintrinsicmotivationproposedby(Oudeyer&Kaplan,2009):
‚Ä¢ Externalvs.Internal: Referringtowhethertherewardcomputationoccursintheenviron-
mentorwithintheagent.
‚Ä¢ Extrinsicvs.Intrinsic: Referringtowhethertherewardcriterionisdeterminedexternally
orinternally.
Accordingtothesedefinitions,anyrewardprovidedfromtheexternalenvironment,suchasagame
score,qualifiesasextrinsicmotivation. Meanwhile,internalmotivationcanbeeitherextrinsicor
intrinsic. Aninternalmotivationisconsideredintrinsiciftheagentsetsitsrewardcriterionaccording
toitsinternalmodel(e.g.,measuringinformationgainbasedonalearnedworldmodel). Bycontrast,
aninternalyetextrinsicmotivationisoneinwhichtheagentchecksitsinternalstateandcomputesa
rewardbasedonanexternallyfixedcriterion(e.g.,"maintainingabatterylevelof50%").
Therefore,thekeytodistinguishingextrinsicandintrinsicmotivationisnotthesourceofinformation,
butratherwhosetstherewardcriterion. Inthissense,expectedfreeenergyisaninternalframework
sincetheagentitselfcomputesbothextrinsicandintrinsiccomponents. Furthermore,theself-prior
proposedinthispaperqualifiesasinternalintrinsicmotivationbecausetheagent‚Äôsinternalmodel
directlycalculatesandupdatestherewardcriterion.
Additionally, (Oudeyer & Kaplan, 2009) proposed that motivation can also be distinguished as
homeostaticvs.heterostatic.Forexample,ifthereisanintrinsicmotivationtopursuenewinformation
acquisition,thisisheterostatic,whereasifthereisanintrinsicmotivationtomaintainafamiliarstate,
thisisdefinedashomeostatic.
Inactiveinference,thepreferredpriorhastraditionallybeeninterpretedasafixedsetpointrepresenting
observationsessentialforsurvival(e.g.,bloodsugarlevel,batterylevel). Thesesetpointshavebeen
consideredextrinsicvaluessincetheyareexternallyspecified, butweregardthistermasstrictly
representing a homeostatic value characteristic. Therefore, the decomposition in Equation (3)
separatesthehomeostaticvalueterminEquation(2)intoextrinsicandintrinsicterms:
‚àíE [logpÀú(o )]=‚àíE [logpÀú(oE)]‚àíE [logpÀú(oI)]
q(ot) t q(oE
t
) t q(oI
t
) t
(4)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
homeostaticvalue extrinsic(homeostatic) intrinsic(homeostatic)
Note that unlike Equation (2), where E [logpÀú(o )] is labeled as extrinsic value, Equation (4)
q(ot) t
labelsitashomeostatic. Thus,theexpectedfreeenergyincorporatingtheself-priorbecomes:
G ‚âà‚àíE [logpÀú(oE)]‚àí E [logpÀú(oI)] ‚àíE [D [q(s |o )‚à•q(s )]]‚àíH‚Ä≤
q(oE
t
) t q(oI
t
) t q(ot) KL t t t
(5)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
extrinsic(homeo-,fixed) intrinsic(homeo-,familiarity) intrinsic(hetero-,novelty)
Here,H‚Ä≤denotestheentropyoftheactiondistribution.
In summary, the self-prior in this study aims to "maintain or restore a reference pattern" learned
fromtheagent‚Äôssensoryexperiences,thusqualifyingasinternalhomeostaticintrinsicmotivation.
Thismostcloselyresemblesdistributionalfamiliaritymotivation(DFM)amongtheclassificationsof
intrinsicmotivationproposedby(Oudeyer&Kaplan,2009). Table1summarizesthepositioningof
theself-priorwithinthismotivationaltaxonomy.
Asanalternativeapproachtoimplementingtheself-priorconcept,therealsoexistapproachesthat
constructpreferencesoverinternalstates. Thisistheapproachproposedby(Sajidetal.,2021),butit
couldnotexplicitlyintegrateextrinsicandintrinsicmotivations. Anotherapproachcouldconsider
hierarchicalmodelswhereininternalrepresentationslearnedathigherlevelsaretransformedinto
self-priorsforlowerlevels. WerevisitthesepotentialfutureextensionsintheDiscussionsection.
5
Table1: Positioningoftheself-priorwithinataxonomyofmotivations
Extrinsic Intrinsic
Homeostatic Heterostatic
External Scoreinvideogame (N/A) (N/A)
Gapofsensorydata Gapofsensorydata Informationgain
Internal
fromfixedvalue fromself-prior bysensorydata
3 ComputationalModelofSelf-Prior
Inthissection,weintroducetheconcreteimplementationofourmodelforcomputationalsimulations
based on the active inference framework incorporating the self-prior. The overall framework is
constructedusingsmall-sizedmatricesinthediscreteenvironmenttoallowdirectinspectionofmodel
behavior,anddeepneuralnetworksusingtechniquesdevelopedindeepreinforcementlearninginthe
continuousenvironment.
Weimplementthisasageneral-purposestructureapplicabletoawiderangeofbehaviorsbeyond
specifictasks,usinganend-to-endlearningmodelthatrequiresnopriorpreparationsuchasfiltering
specific parts of sensory data for the self-prior. Specifically, in the discrete environment model,
we learn maximum likelihood estimation via empirical frequency normalization (plug-in MLE),
whileinthecontinuousenvironmentmodel,wetrainNormalizingFlowbaseddensityestimatorsvia
maximumlikelihood(Durkanetal.,2019).
3.1 ModelforDiscreteEnvironment
The model for the discrete environment uses categorical distributions as the generative model,
handlingdiscreterandomvariables:
Prior: P(s |s ,a )=Cat(B )
t t‚àí1 t‚àí1 at‚àí1
Posterior: Q(s )=Cat(œï ) (6)
t t
Likelihood: P(o |s )=Cat(A)
t t
Here,uppercaselettersP, Qdenotediscretedistributions,andA, B, œï areparametermatricesand
t
vectorsofcategoricaldistributions. Bimplementsthetransitionmodel,withseparateB matrices
a
foreachactiona. Usingthisnotation,minimizingvariationalfreeenergyyieldstheoptimalposterior
distributionparameterœï forthecurrentobservationo throughtheSoftmaxnormalizationfunction
t t
œÉasfollows(derivationprocessseeAppendixA.1.1):
œï ‚âàœÉ(log(A¬∑o )+log(B œï )) (7)
t t at‚àí1 t‚àí1
WeassumethatthelikelihooddistributionCat(A)andthepriordistributionCat(B)havealready
beenlearnedaccurately. Thatis,theparameterAperfectlyknowswhichobservationo corresponds
t
toeachpossiblehiddenstates ,andBperfectlyknowstheoutcomeofeachpossibleactiona for
t t
eachstate. Thissimplifyingassumptionisintendedtoexcludesideeffectsthatmayarisefromother
partsofthegenerativemodelwheninvestigatingchangesinbehaviorduetochangesintheself-prior
(thelearningofbothlikelihoodandpriordistributionsisincludedinthemodelforthecontinuous
environmentdiscussedlater).
The distribution PÀú(o ) that serves as the setpoint for future observations is given in the form of
t
a categorical distribution Cat(C), where C is a column vector with a row for each possible o .
t
AccordingtothedefinitioninEq.(3),thepreferredpriorpÀú(o )shoulddecomposeintoanextrinsic
t
component pÀú(oE) and an intrinsic component pÀú(oI), but in this study, we excluded the extrinsic
t t
component in order to investigate purely intrinsic motivational behavior driven by the self-prior.
Therefore,allcomponentsoftheobservationvectoro correspondtointrinsicobservationsoI,and
t t
theentireCat(C)isdefinedastheself-priorPÀú(oI):
t
Self-prior: PÀú(oI)=Cat(C) (8)
t
6
Theagentautonomouslylearnstheself-priorbyrecordingthefrequencyofeachobservationexperi-
encedsofarandstoringthecorrespondingobservationprobabilitiesintheparameterC. Theinitial
valueofCissetsothatallobservationshaveequalprobability. Todeterminetheoptimalactiona
t
basedonthis,wecomputetheexpectedfreeenergyforeachaction(derivationprocessseeAppendix
A.1.2):
G ‚âà(B œï )¬∑H[A]+D [AB œï ‚à•C] (9)
at t KL at t
Toevaluatefuturepolicies,wepropagatethecurrentbeliefœï throughthetransitionmodelBinto
t
future beliefs œï along "imagined time": œï = B œï . The optimal policy is
t+1:t+N t+œÑ at+œÑ‚àí1 t+œÑ‚àí1
selected by summing expected free energies for each candidate policy œÄ = {a ,...,a }. The
t t+N
policyisexpressedasacategoricaldistribution,fromwhichactionsaresampled:
Policy: P(œÄ)=œÉ(‚àíG) (10)
3.2 ModelforContinuousEnvironment
In prior research attempting to apply the free energy principle to (high-dimensional) continuous
environments,manyimplementationsusejointanglesdirectlyasinternalstatesandgenerateactions
throughanalyticallyderivedbackpropagation(e.g.,(Priorellietal.,2023;Sancaktaretal.,2020)).
While these approaches offer the advantage of analytical computation, they are limited in that
therepresentationisconstrainedtojointangles, makingitdifficulttoimplicitlyencodecomplex
sensorimotorinformationinhigh-dimensionallatentspacesandchallengingtoapplytocomplex
tasksrequiringlong-termplanning.
Incontrast,approachesutilizingdeepneuralnetworksdonotdirectlymimicbiologicaldetails,but
theyprovideapracticalmethodtoextendthecorecomputationalprinciplesofactiveinferenceto
high-dimensionalproblems(Millidge,2020). Thefocusofthisstudyistoproposeanddemonstrate
theconceptualmechanismoftheself-prior,whichisnotboundtoanyspecificneuralimplementation.
Therefore,wefollowthedeepneuralnetworkbasedapproachtoenablerepresentationofcomplexsen-
sorimotorinformationinhigh-dimensionallatentspacesandtoperformfutureplanning. Specifically,
inthisstudy,wefollowtheapproachofpriorresearchthatinterpretstheRecurrentState-SpaceModel
(RSSM)inPlaNet(Hafneretal.,2019)asoptimizingvariationalfreeenergy(√áataletal.,2020),and
interpretspolicygradientmethodsasminimizingexpectedfreeenergy(Millidge,2020). TheRSSM
isessentiallyanextensionoftheVariationalAutoencoder(VAE,(Kingma,2013))architectureto
sequentialdata,directlyimplementingtheprincipleofvariationalfreeenergyminimization.
Whereasthediscretemodelfixedtheparametersofthelikelihoodandpriordistributionsinvariational
freeenergy,thecontinuousmodelfollowsthestandardRSSMstructureandtrainstheparametersœï
usingdeepneuralnetworks. TheRSSMseparatesthedeterministiclatentstateh andthestochastic
t
latentstates ,enablingpropersequentialdecomposition:
t
Deterministicstate: h =f (h ,s ,a )
t œï t‚àí1 t‚àí1 t‚àí1
Stochasticprior: p (sÀÜ |h )
œï t t
(11)
Stochasticposterior: q (s |h ,o )
œï t t t
Likelihood: p (o |h ,s )
œï t t t
ThedeterministicstatecapturestemporaldependenciesthroughaGRUcell,whilethestochastic
prior and posterior each generate Gaussian distribution parameters from the deterministic states
(and observations). This separation ensures proper variational decomposition at each time step
(implementationdetailsinAppendixA.2.2).
Thecontinuousmodelalsoinvestigatespurelyintrinsicmotivationalbehavior,thuspÀú(o )=pÀú(oI).
t t
The self-prior is learned using Neural Spline Flows (NSF, (Durkan et al., 2019)) to maximize
observation log-likelihood. Thisisdonebysamplingobservations ofromareplaybufferD that
storesrecentlyexperiencedtrajectories:
Self-prior: pÀú (o )
Œæ t
argminL =argminE [‚àílogpÀú (o)] (12)
self o‚àºD Œæ
Œæ Œæ
7
Current Future Planning Current Future Planning
(Minimize ) (Minimize ) (Minimize ) (Minimize )
Entropy
KL Div.
Self-Prior Self-Prior
Log Prob. Log Prob.
Preferred Prior (Preferred Prior)
(a)Originalactiveinference (b)Activeinferencewithself-prior
Figure2: Graphicalmodelofactiveinferenceusingdeepneuralnetworksthatminimizevariational
free energy F and expected free energy G. The self-prior pÀú(oI) is trained to maximize the log-
t
likelihood of observations oI. In the expected free energy calculation (highlighted in blue), the
t
learnedself-priorservesasthebehavioralsetpointalongsidethefixedpreferredprior. Althoughthe
preferredpriorandself-priorcantheoreticallybeappliedsimultaneously,weuseonlytheself-prior
inthisstudyforclarityofexposition;thus,thepreferredpriorisshownfadedinthefigure.
Sinceactioncandidatescannotbeenumeratedincontinuousenvironments,weusepolicygradient
methods. Following(Millidge,2020),wetrainapolicynetworkthatpredictsactionsfromstatesand
avaluenetworkthatestimatesexpectedfreeenergyoveraninfinitehorizon:
Policy: q (a |s )
Œ∏ t t
(13)
Expectedutility: g (s )
œà t
ThisisasimilarapproachtoSoftActor-Critic(Haarnojaetal.,2018),andapproximatesexpected
utilitythroughGAE(Œª)estimation(Schulmanetal.,2015). Trainingdetailsandhyperparametersare
providedinAppendixA.2.3and(Mazzagliaetal.,2021).
4 ExperimentsandResults
Inthissection,wevalidatetheemergenceofgoal-directedbehaviordrivenbytheself-prior,whichis
learnedfrommultimodalsensoryexperiences,throughcomputersimulations. Asbrieflymentioned
earlier,ourexperimentisbasedonthefollowingassumptions: (i)sinceaninfanttypicallydoesnot
havestickersattachedtoitsbody,itacquiresaself-priorthat"therearenostickersonmybody";(ii)
whenastickerisattached,themismatchbetweensensoryobservationsandtheself-priorincreases
freeenergy; and(iii)toreducefreeenergy, theagentspontaneouslygeneratesreachingbehavior
aimedatthesticker. Importantly,wedidnotpresetanypreferredstatessuchasremovingstickers
oradoptingspecificpostures;allgoal-directedbehaviorsemergefromtheself-priorautonomously
formedfromthesensationstheagentexperiencesthroughmotorbabbling.
Throughtheseexperiments,we(i)verifytheprocessbywhichtheself-prior,representingtheagent‚Äôs
ownpreference,isautonomouslyformed,(ii)confirmtheemergenceofreachingbehaviorwithin
thefreeenergyminimizationframeworkincorporatingtheself-prior,and(iii)inexperimentsinthe
discreteenvironment,additionallyshowthatbehaviorcanvaryforthesamestimulusdependingon
changesintheself-prior.
4.1 SimulationinDiscreteEnvironment
In the discrete environment, the agent receives multimodal observations composed of discrete
proprioceptiveandtactileinputs(Figure3). Theagent‚Äôsrighthandcanoccupyoneoffivepositions,
labeled0,1,2,3,or4,representingitsproprioceptivestate. Theleftarmhasthreetactilesensors,
locatedatpositions1,2,and3intherighthand‚Äôscoordinateframe,andeachsensorcanbeactivated
independently,yielding23 =8possibletactilestates. Consequently,theobservationvariableo can
t
8
Sticky note Right hand
4
Left arm
Table 2: Possible combinations of sensory
1 [3] observationsinthediscreteenvironment
ùëé
ùë°
0 2 o Touch Handposition
t
(000‚àº111) (0‚àº4)
1 1
0 000 0
1 000 1
0
. . .
. . .
ùëú . . .
ùë°
<Tactile> <Proprioceptive> 28 101 3
Skin touch (000~111) Joint position (0~4)
. . .
. . .
Figure 3: Overview of the discrete environ- . . .
ment. Therighthandcanmoveleftorright 39 111 4
eitherabovetheleftarmoroutsideofit,and
tactileinputoccurseitherwheretherighthand
islocatedorwherethestickerisattached.
take5√ó8=40distinctvaluesandisrepresentedasa40-dimensionalone-hotcolumnvector(Table
2).
Acaregivercanattachastickertotheagent‚Äôsleftarm. Ifastickerisplacedataspecificposition(e.g.,
positionx),theagentcontinuouslyreceivesatactilesignalfrompositionx,regardlessoftheright
hand‚Äôsposition. Iftherighthandexactlymatchesthesticker‚Äôslocation,theagentperceivesasingle
tactilesignal.
Ateachtimestep,theagentcanchooseoneofthreeactionsa : (i)remainstationary,(ii)moveone
t
steptotheleft,(iii)moveonesteptotheright. Thus,a isrepresentedasa3-dimensionalone-hot
t
columnvector. Inthediscreteenvironmentexperiment,thereare34 =81possiblepoliciesœÄ,which
aresequencesofactionsupto4stepsintothefuture,andareprobabilisticallydeterminedbyEquation
10accordingtothesumofexpectedfreeenergiesforeach.
At the beginning of the experiment, the self-prior was uniformly initialized across all possible
observations. This makes the expected free energy equal for all policies, causing random policy
selection. Thatis,nospecialresponseoccursevenwhenastickerisattached(Figure5a).
Subsequently,welettheagentperformmotorbabblingwithoutconstraintsforasufficientlylong
period(10,000steps). Inthecaseofrealinfants,suchexperiencesmaycorrespondtoavarietyof
everydayexperiencesthatfulfillextrinsicpreferences. However,forsimplicity,weperformedmotor
babblinginwhichtheagentrandomlyselectsitsactionsa. Throughthisprocess,thepatternthat"a
singletactilesignaloccurswhentherighthandisovertheleftarm"islearned,andasaresult,"no
tactilesignaloccurswhentherighthandisofftheleftarm"(LeftofFigure4).
Atthispoint,whenacaregiverattachesastickertoposition1ontheleftarm,atactilesignaloccurs
atalocationdifferentfromthecurrenthandposition(position4). Accordingtothenewlylearned
self-prior,thisisalow-probabilitysituation,resultinginanincreaseinfreeenergy(i.e.,attention
isdrawnduetothemismatchwiththemodel). Accordingly,theagentbeginsplanningactionsto
reduceexpectedfreeenergyandconcludesthattouchingthestickeralignsbestwithitsself-prior. As
aresult,theagentshowsgoal-directedmovementtowardposition1(Figure5b). Similarly,whenthe
caregivermovesthestickertoposition3,theagentdetectsthisnewsignalasincongruentwithits
modelandimmediatelyreachestowardposition3.
Next, welettheagentundergoanotherroundofmotorbabbling(20,000steps)whilethesticker
remainedatposition3. Overtime,theself-priorwasgraduallyupdatedsothattheprobabilityof
tactilesignalatposition3increasedrelativetowhenthestickerwasabsent. Inotherwords,theagent
cametoexpecttactileinputatposition3regardlessofitshandposition(RightofFigure4).
After this extended babbling, if the sticker was moved to position 1, the agent still noticed the
mismatch(asatactilesignalatposition1isinconsistentwiththeself-prior)andreachedouttotouch
it. However,ifthestickerwasplacedbackatposition3,theagentnolongerexhibitedconsistent
9
39
0.25
0.2
0.15
0.1
0.05
0
0 10k 20k 30k
Time
DI
noitavresbO
Put sticker
Figure4: Changeinself-priorovertime. Beforethestickerisattached,theprobabilityincreasesfor
situationswherenostickerispresentonthearm(t<10,000). Afterthestickerisattached,theagent
graduallyadaptstothenewsituationwherethestickerispresent(t‚â•10,000).
40
35
30
25
20
15
10
5
0
0 5 10 15 20 25
Time
ygrene
eerf
detcepxE
4
3
2
1
0
0 5 10 15 20 25
Time
noitisop
.vnE
Sticker on 1 Sticker on 3 Sticker on 1
40
35
30
25
20
15
10
5
0
0 5 10 15 20 25
Time
(a)Beforeacquiringtheself-prior(t=0)
ygrene
eerf
detcepxE
4
3
2
1
0
0 5 10 15 20 25
Time
noitisop
.vnE
Sticker on 1 Sticker on 3 Sticker on 1
(b)Afteracquiringtheself-prior(t=10,000)
Figure5: Comparisonoftheagent‚Äôsbehaviorbeforeandafteracquiringtheself-prior. Thetoppanel
illustrates environmental changes over time: the red line indicates the hand position, the yellow
dashedlineindicatesthestickerposition. Whiteareasdenotewheretactilefeedbackoccurred,black
areasindicatenotactilefeedback, andgrayareasrepresentregionsoutsidethearmwheretactile
feedbackneveroccurs. Thebottompanelshowsexpectedfreeenergyovertime. Eachgreendot
representsthefreeenergyofacandidatepolicy,withlowerfreeenergypoliciesbeingmorelikelyto
beselected. Theredlineconnectstheactuallyselectedpolicies. (a)Beforeacquiringtheself-prior,
the agent does not respond even when a sticker is attached to the arm. (b) After acquiring the
self-prior,goal-directedbehavioremerges: theagentmovesitshandtothesticker‚Äôslocationwhenit
appearsonthearm.
goal-directed behavior (Figure 6). Because the self-prior had already adapted to position 3, no
additionalmismatcharose. Consequently,thedifferenceinexpectedfreeenergybetweenreachingor
notwasnegligible,sotheagentsimplybehavedrandomly.
This situation, in which the self-prior does not induce any motivation, allows us to isolate the
influenceofthetraditionalinformationgaincomponentofintrinsicmotivationintheexpectedfree
energyframework. Inthisexperiment,sincetheparametersAandBarealreadyfixedinafully
accuratestate,thereisnonewinformationtobegainedfromobservations. Asaresult,noepistemic
(information-seeking) drive emerges, and all policies yield nearly identical expected free energy,
causingtheagenttobehaverandomly.
10
40
35
30
25
20
15
10
5
0
0 5 10 15 20 25
Time
ygrene
eerf
detcepxE
4
3
2
1
0
0 5 10 15 20 25
Time
noitisop
.vnE
Sticker on 1 Sticker on 3 Sticker on 1
Figure6: Agent‚Äôsbehaviorwhenastickerhaspersistentlybeenattachedtoposition3(t=30,000).
Whenthestickerisplacedonotherpositions,theagentstillreachestowardthem; however,itno
longershowsinterestwhenthestickerisplacedatposition3.
Incontrast,weobservedthatactionsarisingfromtheself-priorcouldbegeneratedevenwhenno
additionalinformationgainwaspossible. Inotherwords,becausetheparametersoftheself-prior,C,
continuouslyevolvedwithexperience,intrinsicmotivationrelatedtofamiliaritybasedonexperience
wasstillinduced. Asaresult,theagentwasdriventoactinwaysthatwouldalignthenewlyupdated
self-priorwiththeobservations.
4.2 SimulationinContinuousEnvironment
Uptothispoint,wehaveintroducedthecoreideathatgoal-directedbehaviorcanemergefromthe
self-priorusingasimplediscretesetup.Wenowextendthistoanenvironmentwithcontinuous-valued
variables,inspiredbytheenvironmentsetupof(Marceletal.,2022)(Figure7).
Initially,theagent‚Äôstwoarmslieonthexy-plane. Thelengthsoftheleftforearm,leftarm,torso,
rightarm,andrightforearmare80,70,140,70,and80mm,respectively. Theleftforearmfeaturesa
30mm-widetwo-dimensionaltactilesensorarray,placedlaterallyrelativetotheforearmitself. At
theendpointoftherightforearmisacircular"righthand"ofradius6mm,whichcangeneratetactile
inputontheleftforearm.
Alljointanglesareexpressedinradians. TheleftelbowandleftshoulderanglesarefixedatœÄ/3
and2œÄ/3,whiletherightshoulderandrightelbowangleshaverespectiverangesof[0, 2œÄ/3]and
[0, 3œÄ/4]. Additionally,therighthandcanmovealongapseudo-z(height)axiswithintheinterval
[0, 20]mm.
Similar to the discrete case, the agent perceives both proprioception and touch; however, these
quantitiesarenowcontinuous. Theproprioceptiveinputisathree-dimensionalreal-valuedvector
(shoulder,elbow,hand),whiletactileinputisrepresentedbyan80√ó30real-valuedmatrixnormalized
to[0, 1]. Whenevertherighthandisabovetheleftforearmataheightof‚â§10mm,thetactilesensor
ontheleftforearmisactivatedwithintensityinverselyproportionaltothehand‚Äôsheight.
Torepresentbothtactileandproprioceptivemodalitiesasaunifiedobservationo inthemodelfor
t
thecontinuousenvironment,weconverteachmodalityintoanembeddingofthesamedimensionality
and integrate them. The tactile matrix is transformed into an embedding using a CNN, and the
proprioceptivevectoristransformedintoanembeddingusingaMLP.Wecombinethetwoembeddings
viaelement-wiseadditiontoyieldthefinalintegratedobservationembeddingo . Todecodefromthe
t
embeddingbacktotheoriginalsensations,weuseinversetransformationmodules(seeAppendix
A.2.1fordetailedhyperparameters).
Acaregivercanattachastickerofradius4mmtotheleftforearmatafixedheightof0mm. Asa
simplifiedenvironmentalruletodemonstratethecoreself-priormechanism,theagentcanremovethe
stickerifthecenterofitsrighthandremainswithinthesumofthehand‚Äôsandsticker‚Äôsradiifor10
11
Sticker
30mm
Hand 80mm
80mm [0~3ùúã/4]
2ùúã/3
70mm 70mm
ùúã/3 [0~2ùúã/3]
70mm 70mm
Figure7: Overviewofthecontinuousenvironment. Asinthediscreteenvironment,therighthand
canmoveoverandaroundtheleftarm,andtactilesensationsaregeneratedwherethehandorthe
stickerislocated.
consecutivetimesteps. Thisrepresentsasimplifiedassumptionwhereinsustainedcontactbetween
thehandandstickerleadstoobjectremoval,withoutmodelingcomplexmanipulationactionssuchas
graspingorpushing.
Ateverytimestep,therightshoulderandrightelbowanglescaneachrotatebetween‚àí0.05and
+0.05rad,andthehand‚Äôsheightcanshiftby‚àí0.5to+0.5mm. Tokeepthehandneartheforearm,
weimposeanadditionalconstraintthatthehand‚Äôscentermustremainwithin15mmoftheforearm‚Äôs
surface. If a proposed movement would place the hand outside this region, the agent‚Äôs action is
disregardedanditspreviouspositionisretained.
WeconductedthecontinuousenvironmentexperimentsonamachinerunningUbuntu22.04.5LTS64-
bit(Linux5.15.0-1066)withanIntelXeonE5-2698v4CPUandanNVIDIATeslaV100-SXM2GPU,
usingPython3.11.10andPyTorch2.5.1. Forournormalizingflowsbasedself-prior,weemployed
the‚Äúzuko‚Äùlibrary(version1.3.0),andwebasedmuchofouroverallalgorithmicimplementationon
Dreamer(Hafneretal.,2020)andContrastiveActiveInference(Mazzagliaetal.,2021).
Totrainandevaluateourdeepmodel,westoredepisodesinareplaybufferandrandomlysampled
them during model training. Each episode was collected either by using random actions or by
followingtheagent‚Äôspolicyfromactiveinference,chosenwithequalprobability(50%). Likewise,
forhalfoftheepisodes,astickerwasplacedatarandomlocationonthearm;fortheotherhalf,no
stickerwasused.
Aftercollectinganinitialbatchof100episodes,wetrainedthemodelfor100epochsattheendof
eachepisode.Ineachepoch,wesampledB =50trajectoriesoflengthL=50fromthereplaybuffer,
andgeneratedimaginedrolloutswithaplanninghorizonH =15forpolicylearning. Forvariational
freeenergyminimization,trajectoriesweresampledfromallavailabledata,regardlessofthepresence
ofastickerorwhetherthebehaviorwasrandomorpolicy-driven. Thiswasintendedtoensurethe
acquisitionofanaccurateworldmodel,similartotheassumptioninthediscreteenvironmentthatA
andBwerealreadyknown. Likewise,alldatawereusedforexpectedfreeenergyminimization,so
thattheagentcouldlearnwhichactionsreducefreeenergyeffectivelyacrossvariousconditions.
Forself-priorlearning,weconstructedthedatasetsuchthatonlyabout5%oftheepisodesincluded
asticker,whiletherestweresampledwithoutanystickerpresent. Consequently,asinthediscrete
environmentexperiment,theself-priorcametoassignhighprobabilitytoobservationslike‚Äúasingle-
pointtouchoccurswhentherighthandisabovetheleftarm‚Äùand‚Äúnotouchoccurswhentheright
handisaway‚Äù.
Aftercollectingandtrainingfor2,000episodes,weattachedastickertotheagent‚Äôsleftarm. Asa
result,theagentexhibitedgoal-directedbehaviorofreachingtowardthesticker(Figure8a). This
occurredbecausetouchingthestickeralignedwiththeself-prior,minimizingfreeenergy(Figure8b).
Moreover, after reaching the sticker, the agent continued touching it and eventually removed it,
indicatingthatithadapersistentmotivationtoreducethemismatchratherthansimplylosinginterest
aftercontact.
Interestingly,theagentexhibitedreachingbehaviortowardthestickerevenwhentherighthandwas
notinitiallytouchingthearm. Iftheself-priorhadbeenlearnedbasedsolelyontactileinformation,
(i)thecasewheretheleftarmistouchedbytherighthandand(ii)thecasewhereastickerisattached
12
t = 0
t = 5
t = 10
t = 15
t = 20
t = 25
t = 30
t = 35
t = 40
t = 45
t = 50
t = 55
(a)Timeseriesofreachingbehavior (b)Changeinexpectedfreeenergy
Figure8: Agentbehaviorwhenastickerisplacedinthecontinuousenvironment. (a)Whenasticker
(bluecircle)isattached,thehand(redcircle)movestowardit,illustratinggoal-directedreaching
behavior. Thefigurevisualizestheagent‚Äôstactilematrix,wheregrayscaletactiledataisoverlaidwith
coloredmarkersforthehandandstickerforclarity. (b)Reachingforthestickerreducesexpected
freeenergy,andremovingthestickerleadstoitsminimization. Shadedareasrepresentthestandard
deviationacross64experimentsusing8modeltrainingseeds(0~7)testedon8environmentseeds
(0~7). Thefigureusesseed4,selectedforclearlydemonstratingthereachingbehavior.
might not be distinguishable, since both yield a single tactile point on the arm. In that case, no
reachingbehaviortowardthestickerwouldemerge. However,theagentdidreachtowardthesticker,
suggestingthattheself-priorwaslearnedbyintegratingbothtactileandproprioceptiveinputs. In
thissense,oursystem,whichutilizesaself-priorlearnedfrommultimodalsensoryinputstogenerate
reachingbehavior,resemblestheconceptofabodyschemamentionedin(Gallagher,1986;Hoffmann,
2021),whichsupportsactionplanningandcontrol.
Additionally,evenifextratouchesarosemid-trajectory(e.g.,fromtherighthanditself)thusmo-
mentarilyincreasingmismatchrelativetoasingle-pointself-prior,theagentacceptedthisshort-term
deviationtoachievetheultimategoalofminimizingoverallfreeenergy.Thisrobust,noise-tolerantse-
quenceofgoal-directedactiondemonstratesthatouragentmaintainedacoherentintentionthroughout
itsinteractions.
5 Discussion
Weproposedtheself-priorasanintrinsicpreferencethatisautonomouslyformedfromanagent‚Äôs
sensory experiences. We then applied it to a simulated infant agent within the active inference
frameworkandconfirmedtheemergenceofgoal-directedbehavior,specificallyreachingforasticker.
Inthissection,toclarifythecontributionsofourworktodevelopmentalscience,weexplainhowour
approachdiffersfrompreviousstudiesthatattemptedtogeneratespontaneousbehaviors,propose
theroleoftheself-priorasapotentialoriginofearlyintentionalbehavior,andfinallydiscussthe
limitationsofthisworkandfutureresearchdirections.
5.1 ComputationModelsofIntrinsicMotivation
Approachesthataimtocomputationallyaddressintrinsicmotivationhaveprimarilybeenexploredin
reinforcementlearning,wheredesigninghigh-qualityrewardfunctionsremainsasignificantchallenge.
Asaresult,intrinsicmotivationhasbeenproposedasameansforagentstomoreefficientlyexplore
theenvironmentandbetterinferrewardfunctions. However,afundamentalquestionpersists: how
cananagentautonomouslyformmotivationandgeneratebehaviorinenvironmentswherenogoals
13
are given, and what kind of value guides theagent‚Äôs actions under such conditions (Juechems &
Summerfield,2019)?
ExtrinsicHomeostaticReinforcementLearning Analternativeapproachishomeostaticrein-
forcementlearning, inwhichtheagentcangeneratearangeofbehaviorssimplybymaintaining
certainsensorychannelstospecifiedthresholds,withoutenvironmentalgoal(Keramati&Gutkin,
2011). Recentstudieshaveextendedthisconcepttocomplexsettingsusingdeepneuralnetworks,
examining how internal physiological states drive behavior (Yoshida et al., 2024). Under active
inference, a similar mechanism emerges: by minimizing free energy with respect to an internal
setpoint(expressedasapreferredprior),anagentcangenerateasinglegoal-directedactlikereaching
foratarget(Oliveretal.,2022)ororchestraterepeatedbehaviors(Matsumotoetal.,2022).
However, although these approaches appear intrinsic because behaviors occur without external
rewards,mosthavealimitationinthattheexperimenterfixesthereferencevalues(e.g.,setpointsor
preferredpriors)inadvance.Thatis,eventhoughtheagentcalculatesrewardsinternally,thereference
itselfisexternallyprovided,soitultimatelyhasanextrinsicnature. Therefore,theseapproaches
donotfullyresolvethefundamentalquestionof"Wheredoesvaluecomefrom,andcantheagent
establishitbyitself?"
Reward-auxiliaryIntrinsicMotivation Toallowtheagenttoestablishitsownvaluefunction
independently,neitherexternalrewardsnorinternalsetpointscanbepredetermined. Butwithout
these metrics, it is difficult to track learning progress or judge the utility of behaviors, which is
whymanystudiesonintrinsicmotivationstilltendtoanalyzeitprimarilyasameanstofacilitate
explorationforextrinsictasks(Aubretetal.,2023). Thatis,theprocessbywhichagentsgenerateand
maintaingoalsthemselveshasbeenrelativelylessilluminated.
HeterostaticIntrinsicMotivation Someworksdoinvestigatethebehaviorsarisingpurelyfrom
intrinsicmotivationandqualitativelyevaluatetheminscenarioswithnoexternallydefinedreward
(Eysenbachetal.,2018;Pathaketal.,2017). Whilethesehighlighttheagent‚Äôsacquisitionofdiverse
skillsorbehaviors,theytypicallyinvolveaheterostaticapproach,continuouslyseekingnovelstates.
Consequently,onceuncertaintyissufficientlyreduced,furtherexplorationtendstodiminish,and
fewernewbehaviorsemerge.
Activeinferenceoffersaunifiedtheoreticalframeworkthatcanintegratebothextrinsicandintrinsic
motivationsunderthesinglequantitativemeasureoffreeenergy(Parretal.,2022). Furthermore,
(Biehletal.,2018)showedthatmanyexistingmethodsforintrinsicmotivationcanbecastwithin
activeinferenceasprocessesforimprovingtheposterior,aperspectivereminiscentof(Schmidhuber,
2010),whodefinedintrinsicmotivationintermsofseeking‚Äúbetterworldmodels.‚ÄùHowever,such
posterior-improvingmotivesarelikewiseheterostatic: theypromoteexplorationuntiluncertaintyis
resolved,afterwhichtheyelicitfewernotableactions.
HomeostaticIntrinsicMotivation Recently,studieshavebeguntoemergethataddresshomeostatic
intrinsicmotivation,inwhichagentsautonomouslydefinetheirownsetpointsbutaimtopreserve
alreadylearnedstatesorpastexperiences,ratherthancontinuouslyseekingnovelty. Thesestudies
havemadeimportantcontributionsbydemonstratingthatgoal-directedbehaviorscanemergewithout
externalrewards.Forexample,(Marceletal.,2022)proposedaprocessinwhichlatentrepresentations
ofself-touchobservationsareusedassetpointstoinducereachingbehavior,and(Kimetal.,2023;
Takemuraetal.,2018)demonstratedthatforwardandinversemodelslearnedfrommotorbabbling
cangeneratereachingtowardtargetpositionswithoutexplicitrewards. Mostsimilartoourapproach,
(Sajidetal.,2021)introducedtheconceptof"learnedpreferences"withinthefreeenergyframework
andsemanticallyanalyzedhowadrivetoreducemismatchesbetweenobservationandpreferencecan
leadtonewbehaviors. Whilethesestudieshavedemonstratedthepotentialofhomeostaticintrinsic
motivation,theyoperatewithinindependentframeworksfocusedonasingleintrinsicmotivation
mechanism,andintegrationwithextrinsicmotivationsremainsafuturechallenge.
Summary Insummary,ourapproachisdifferentiatedfromexistingresearchinthefollowingways:
1. Intrinsic: Theagentautonomouslyestablishescriteriabasedonitsownexperiencerather
thanadheringtoexternallyfixedcriteria.
14
2. Independentbehaviorgeneration: Itindependentlygeneratesspecificgoal-directedbe-
haviorssuchasreaching,ratherthanservinganauxiliaryroletofacilitateexploration.
3. Homeostatic: Eveninsituationswithoutinformationgain,behaviorspersistwithoutstop-
pingaslongasthereisamismatchwithfamiliarstates.
4. FEPwithactiveinference: Definedwithintheactiveinferenceframework,itcanbetheo-
reticallyintegratedwithexistingextrinsicmotivationsandheterostaticintrinsicmotivations
(informationgain).
5.2 OriginofEarlyIntentionalBehaviors
Infantsinitiallydependheavilyonreflexmechanismsforaction. Overtime,however,theygradually
developwhat(Mele&Moser,1994)describesas‚Äúintentionalbehavior,‚Äùcomprising(i)amotivational
system that judges how to handle incoming stimuli and (ii) the capability to execute genuinely
goal-directedactions. Accordingto(Zaadnoordijk&Bayne,2020),stimulus-drivenintentionsarise
first,andthen,withaccumulatingexperience,moreendogenousintentionsemerge. Forinstance,
newbornsrelyonreflexessuchasrootingtoacquirefeedingexperience;later,merelyseeingabottle
promptsthemtoreachoutanddrink(stimulus-drivenintention). Astheybecomeawareofinternal
stateslikehunger,theybeginactivelysearchingforthebottlethemselves(endogenousintention).
Over time, to recognize themselves as independent entities, infants must undergo abundant ex-
periences featuring multimodal redundancy and temporal‚Äìspatial contingencies (Rochat, 1998).
Sequentiallydevelopingintentionalactionsmayspontaneouslyprovideopportunitiesforsuchexperi-
ences. (Thelenetal.,1993)similarlynotedthatinfantsinitiallylackboththeabilityandtheintention
to perform reaching, and that the formation of stable intentions, coupled with active exploration
drivenbythoseintentions,facilitatesthelearningofreaching.
However,priorstudieshavenotdiscussedhowintentions,includingstimulus-drivenintentions,and
intentional behaviors develop through what structure. Even (Priorelli & Stoianov, 2023), which
implementedswitchableintentionswithintheactiveinferenceframework,usedintentionsthatwere
discretelypredefinedratherthanself-developing. Meanwhile,neurophysiologicalevidencesuggests
thattheanteriorcingulatecortexprimarilydealswithreward-relatederrorsinindividualdecisions,
whereas the posterior cingulate cortex processes discrepancy signals related to autobiographical
informationsuchasone‚Äôsownbodyorpastexperiences,andtriggersnewbehaviors(Breweretal.,
2013;Pearsonetal.,2011).
Ourproposedself-prioralignswiththisnotionof‚Äúself-relevantpredictions,‚Äùindicatingthatwhen
newinputfailstosatisfythesepredictions,thesysteminitiatesaction. Inotherwords,evenwithout
anyexternallydesignedgoal,anintrinsicdrivetopreserveaself-priorlearnedfrompastexperiences
canmanifestbehaviorsakintoaninfantpersistentlytouchinganewlydiscoveredstickerontheir
arm. ThismirrorsPiaget‚Äôsconceptofprimarycircularreactions,whererepeatedattemptscanrefine
intentionsfurther,offeringatheoreticalfoundationforcomputationalstudiesontheoriginsofinfant
intentionalbehavior.
Inourexperiments,wefixedtheself-prior‚Äôslearningrate. However,anexcessivelyhighratecould
leadtooverreactions(compulsive-likeresponses)tominorchanges,whereasanoverlylowratemight
causeindifference(apathy)towardnewstimuli. Futureworkcouldinvestigatehowmoreflexible
learningratesshapedifferentbehavioralstylesviaself-relevantprediction-errorprocessing.
5.3 LimitationsandFutureDirections
Ourstudyemploysaconstructivistapproach,focusingondemonstratinghowtheself-priormechanism
operatesunderminimalconditions. Theinfantsticker-reachingtaskservesasoneexampletoclearly
illustratethismechanism,showingwhatbehaviorsemergeunderthespecificconditionoftactileand
proprioceptivebodyreaching. Sincetheself-priorismodality-independentinprinciple,thesame
mechanismcanbeappliedtovarioussensorymodalities. Forexample,usingvisualmodalitywould
beexpectedtoproducereachingbehaviortowardrarelyseenexternalobjectsoutsidetheagent‚Äôs
body. Indeed,severalphenomenaobservedinactualinfantdevelopmentmayalsobeexplainedby
theself-priormechanism. Forinstance,accordingto(Eskandarietal.,2020),preterminfantsnested
inboundariesshowedasignificantreductioninunstableextensionmovementsandmaintainedstable
flexedpostures,whichcanbereinterpretedastheinfantspreferringuterine-likeenvironmentsand
15
maintainingstablebehavioralstatestoresolvemismatcheswithaself-priorlearnedfromtheflexed
postureandtactileboundarysensationsexperiencedduringfetalstages.
Furthermore,behaviorsthatemergeinmorecomplexcognitivecontextsbeyondsimplesensorimotor-
leveladjustments,suchasattentionorsocialinteractions,mayalsobeexplainedbytheself-prior.
(Warneken&Tomasello,2006)presentedtasksinwhichinfantshelpadultswhoarehavingdifficulty
achievinggoals,includingresultswhereinfantsopenedadoorforanadultwhowashavingdifficulty
puttingamagazineintoacloset. Reinterpretingthisintermsoftheself-priorapproach,itispossible
thattheinfantopenedthedoortoresolveamismatchwithaself-priorlearnedfromvisualinformation
thatadultsnormallyopentheclosetdoorandputmagazinesin. Thus,thesameprincipleoflearning
theprobabilitydensityoverexperiencedobservationscanexplainbehaviorsacrossdiversecontexts.
Ourapproachhasonlybeenvalidatedindiscreteenvironmentsandlow-degree-of-freedom(3-DoF)
continuousenvironments,butrealinfantsorrobotsexhibitfarmorecomplexandvariedsensorimotor
interactions. Therefore, it is necessary to extend this study to high-dimensional state and action
environments. In particular, by utilizing simulations of early human sensorimotor experiences
includingfetalstages(Kimetal.,2022),itwouldbepossibletoexplorehowrichsensorimotorinputs
providedinuteroshapetheself-priorandgoal-directedbehaviors,therebyrevealingwhatbehaviors
candevelopunderphysicalconstraints(Kuniyoshi,2019).
In such complex environments, achieving generalizations such as reusing priors acquired across
diverse contexts would require integrating all available modalities, learning various scenes over
extendedperiods,andemployinghigh-dimensionalself-priormodelscapableofprobabilistically
modeling all of these. Such models could potentially be achieved through hierarchical active
inference(Fristonetal.,2017),ordeepgenerativemodelslikeTransformersforhandlinglongtime
series(Chenetal.,2022;Vaswanietal.,2017). Sincetheself-priorlearnsexperiencedsensationsina
densitymodelwithoutpriorknowledge,suchextensionsarepossibleinprinciplethroughscalingup
generativemodels.
Forexample,generalizationssuchastransferringreachingbehaviorlearnedthroughtouchtovisual
stimuli,generalizingresponsestoaspecificshapeofstickertoothershapes,orextendingbehaviors
towardobjectsattachedtoone‚Äôsownbodytoexternalobjectsareallexpectedtobepossiblethrough
large-scale generative models capable of learning statistical regularities across diverse sensory
experiences. Suchgeneralizationcanbeunderstoodnotsimplyasrepetitionofthesameaction,but
asaprocessofrecognizingandutilizingstructuralsimilaritiesamongself-priorsformedindifferent
contexts. Additionally,weshowedbehaviorsdrivensolelybyfamiliaritybasedintrinsicmotivation
byexcludingexternallyimposedrewardcriteria,butinreality,infantsorrobotsmustbalancemultiple
motivationssuchasenergyhomeostasisandnovelinformationexploration. Therefore,designing
experimentsthatsystematicallyassesshowthesemotivationscomplementorconflictandorganize
behaviorinlarge-scaletaskswillbeanimportanttaskforfutureresearch.
Meanwhile,ourstudydemonstratedthat,byseparatinganintrinsichomeostatictermusingalearnable
self-priorfromtheextrinsictermusingafixedpreferredpriorthatwascommonlyprovidedinprior
studies using expected free energy, an agent can not only generate actions aimed at achieving
predefined goals but also spontaneously generate goal-directed behaviors. However, a current
limitationisthatwhiletheuseoftheself-priorisintegratedintopolicygenerationviaexpectedfree
energy,thelearningoftheself-priorisperformedseparatelyfromthelearningofvariationalfree
energy.
Onepotentialcluetoresolvingthisseparationmayalsolieinahierarchicalarchitecture. Specifically,
a higher-level hidden state that reflects long-term experiences maybe gradually learned, and the
likelihooddistributioninferredfromitcouldserveastheself-priorforthelowerlevel. Thisdiffers
fromconventionalhierarchicalinference,wherethehigherlevelsimplylearnspriorsanddetermines
actionsonaslowertimescaletoinfluencethelowerlevel(e.g., P(s |s ,a ) = Cat(B )).
t t‚àí1 t‚àí1 at‚àí1
Instead,itcontinuouslyreceivesinformationfromthelowerleveltolearnaprior,servingonlytoform
aself-prioroveralongduration(e.g.,P(s)=Cat(B)). Thishierarchicalapproachisalsoexpected
tobecloselyconnectedtobuildingpreferencesoverinternalstates. Whilethecurrentimplementation
learnstheself-priorinobservationspace,throughahierarchicalapproach,thereisapossibilitythat
higher-levelinternallatentrepresentationsmaybeformedthroughexperience.
Anotherpossibilityisthatthelearningoftheself-priorishandlednotbythecortexbutbyaseparate
memory system such as the hippocampus, and thus may require structural separation from the
16
inference system proposed under the free energy principle. Concrete validation of these open
hypothesesisleftforfuturework.
Finally,inthediscreteenvironment,wecouldobservethatastimepassedwithastickerattached,the
self-priorgraduallychangedand,accordingly,behavioralsochanged. Bycontrast,inthecontinuous
environment,weconductedtheexperimentbyfixingtheprobabilityofstickersbeingincludedinthe
observationsusedforself-priorlearningat5%. Thiswasbecausetheself-priormodelstoredpastdata
inareplaybufferandlearnedwithoutconsideringtemporalorder,makingitdifficulttoinvestigate
gradualchangesintheself-prioraccordingtochangesinobservationfrequency. Toreproducethe
processbywhichstimulus-drivenintentiongraduallystrengthensintoamoreendogenousformof
intentioninrealinfantsorrobots,aself-priorlearningmechanismthatconsiderstemporalorderis
necessary. Thiscouldpotentiallybeimplementedthroughthehierarchicalarchitecturementioned
earlier,wherehigherlevelsintegratelong-termexperiences.
6 Conclusion
Buildingonthefreeenergyprincipleandtheactiveinferenceframework,wehavedemonstrated
thatanagent‚Äôsintrinsicdrivetolearnandmaintaina"self-prior",whichisakintoabodyschema,
caninducegoal-directedbehaviorsevenintheabsenceofexternallyspecifiedrewards. Inparticular,
whereastheconventionalactiveinferenceliteraturehasemphasizedheterostaticintrinsicmotivation
(i.e.,seekingnewinformation),ourworkhighlightsahomeostaticformofintrinsicmotivation,one
thatactivelystrivestopreservefamiliarsensoryexperiences.
Using a simulated infant touching and examining a sticker on its arm as an illustrative example,
we showed how an internally derived drive to resolve mismatches between the self model and
incomingdatacanmanifestinbehaviorssuchasreachingandstickerremoval,allwithoutanyexplicit
reward criteria. This offers a computational interpretation of stimulus-driven intentional actions
indevelopmentalpsychologyandsuggestspotentialrelevancetotheneurobiologicalliteratureon
posteriorcingulatecortex,whichprocesseserrorslinkedtoautobiographicalinformation.
Futureworkshouldextendourapproachtocomplexphysicalenvironmentssuchasfull-scaleinfant
simulationsorhigh-DoFrobotstoinvestigatethelong-termformationandrefinementofintentions
and behaviors. Additionally, we must experimentally validate how multiple motivations interact
withinasinglefreeenergyframework,incorporatingnotonlyfamiliaritybasedmotivationbutalso
extrinsicandinformation-seekingmotivations. Suchresearchwillplayanimportantroleinanalyzing
thespontaneousdevelopmentaltrajectoryofintentionalbehavior.
AcknowledgmentsandDisclosureofFunding
ThisworkwassupportedbyJST,PRESTOGrantNumberJPMJPR23S4,Japan.
References
Aubret,A.,Matignon,L.,&Hassas,S.(2023). AnInformation-TheoreticPerspectiveonIntrinsic
MotivationinReinforcementLearning: ASurvey. Entropy,25(2),327.
Biehl,M.,Guckelsberger,C.,Salge,C.,Smith,S.C.,&Polani,D.(2018). ExpandingtheActive
InferenceLandscape: MoreIntrinsicMotivationsinthePerception-ActionLoop. Frontiersin
neurorobotics.
Bigelow,A.E.(1986). Thedevelopmentofreachinginblindchildren. BritishJournalofDevelop-
mentalPsychology,4(4),355‚Äì366.
Brewer,J.,Garrison,K.,&Whitfield-Gabrieli,S.(2013). Whataboutthe‚ÄúSelf‚ÄùisProcessedinthe
PosteriorCingulateCortex? FrontiersinHumanNeuroscience,7.
Chen,C.,Wu,Y.-F.,Yoon,J.,&Ahn,S.(2022). ReinforcementLearningwithTransformerWorld
Models. arXiv:2202.09481.
Chung,J.,Gulcehre,C.,Cho,K.,&Bengio,Y.(2014). EmpiricalEvaluationofGatedRecurrent
NeuralNetworksonSequenceModeling. arXiv:1912.01603.
17
Czikszentmihalyi,M.(1990). Flow: Thepsychologyofoptimalexperience. NewYork: Harper&
Row.
DiDomenico,S.I.&Ryan,R.M.(2017). TheEmergingNeuroscienceofIntrinsicMotivation: A
NewFrontierinSelf-DeterminationResearch. FrontiersinHumanNeuroscience,11.
Durkan,C.,Bekasov,A.,Murray,I.,&Papamakarios,G.(2019). Neuralsplineflows. Advancesin
neuralinformationprocessingsystems,32.
Eskandari,Z.,Seyedfatemi,N.,Haghani,H.,Almasi-Hashiani,A.,&Mohagheghi,P.(2020). Effect
ofnestingonextensormotorbehaviorsinpreterminfants: Arandomizedclinicaltrial. Iranian
JournalofNeonatology,11(3),64‚Äì70.
Eysenbach,B.,Gupta,A.,Ibarz,J.,&Levine,S.(2018). DiversityisAllYouNeed: LearningSkills
withoutaRewardFunction123. arXiv:1802.06070.
Friston,K.(2010). Thefree-energyprinciple: aunifiedbraintheory? NatureReviewsNeuroscience,
11(2),127‚Äì138.
Friston,K.,FitzGerald,T.,Rigoli,F.,Schwartenbeck,P.,O‚ÄôDoherty,J.,&Pezzulo,G.(2016). Active
inferenceandlearning. Neuroscience&BiobehavioralReviews,68,862‚Äì879.
Friston,K.J.,Parr,T.,&deVries,B.(2017). Thegraphicalbrain: Beliefpropagationandactive
inference. NetworkNeuroscience,1(4),381‚Äì414.
Gallagher,S.(1986). BodyImageandBodySchema: AConceptualClarification. TheJournalof
MindandBehavior,7(4),541‚Äì554.
Gopnik,A.(2009). Thephilosophicalbaby: Whatchildren‚Äôsmindstellusabouttruth,love&the
meaningoflife. RandomHouse.
Haarnoja, T., Zhou, A., Abbeel, P., & Levine, S. (2018). Soft actor-critic: Off-policy maximum
entropy deep reinforcement learning with a stochastic actor. In International conference on
machinelearning(pp.1861‚Äì1870).: PMLR.
Hafner,D.,Lillicrap,T.,Ba,J.,&Norouzi,M.(2020). DreamtoControl: LearningBehaviorsby
LatentImagination. arXiv:1912.01603.
Hafner,D.,Lillicrap,T.,Fischer,I.,Villegas,R.,Ha,D.,Lee,H.,&Davidson,J.(2019). Learning
latentdynamicsforplanningfrompixels. InInternationalconferenceonmachinelearning(pp.
2555‚Äì2565).: PMLR.
Hoffmann,M.(2021). Bodymodelsinhumans,animals,androbots: Mechanismsandplasticity.
InBodyschemaandbodyimage: Newdirections.(pp.152‚Äì180).NewYork, NY,US:Oxford
UniversityPress.
Hoffmann,M.,Chinn,L.K.,Somogyi,E.,Heed,T.,Fagard,J.,Lockman,J.J.,&O‚ÄôRegan,J.K.
(2017). Developmentofreachingtothebodyinearlyinfancy:Fromexperimentstoroboticmodels.
In 2017 Joint IEEE International Conference on Development and Learning and Epigenetic
Robotics(ICDL-EpiRob)(pp.112‚Äì119).
Juechems, K. & Summerfield, C. (2019). Where Does Value Come From? Trends in Cognitive
Sciences,23(10),836‚Äì850.
Kanazawa,H.,Yamada,Y.,Tanaka,K.,Kawai,M.,Niwa,F.,Iwanaga,K.,&Kuniyoshi,Y.(2023).
Open-endedmovementsstructuresensorimotorinformationinearlyhumandevelopment. Proceed-
ingsoftheNationalAcademyofSciences,120(1),e2209953120.
Keramati,M.&Gutkin,B.(2011). AReinforcementLearningTheoryforHomeostaticRegulation.
InAdvancesinNeuralInformationProcessingSystems,volume24: CurranAssociates,Inc.
Kim,D.,Kanazawa,H.,&Kuniyoshi,Y.(2022). SimulatingaHumanFetusinSoftUterus. In2022
IEEEInternationalConferenceonDevelopmentandLearning(ICDL)(pp.135‚Äì141).
Kim,D.,Kanazawa,H.,&Kuniyoshi,Y.(2023). EmergenceofReachingusingPredictiveLearning
asSensorimotorDevelopmentinComplexDynamics. InThe11thInternationalSymposiumon
AdaptiveMotionofAnimalsandMachines(AMAM2023)(pp.144‚Äì145).: AdaptiveMotionof
AnimalsandMachinesOrganizingCommittee.
Kingma,D.P.(2013). Auto-encodingvariationalbayes. arXiv:1312.6114.
18
Kuniyoshi,Y.(2019). Fusingautonomyandsocialityviaembodiedemergenceanddevelopmentof
behaviourandcognitionfromfetalperiod. PhilosophicalTransactionsoftheRoyalSocietyB:
BiologicalSciences,374(1771),20180031.
Marcel,V.,O‚ÄôRegan,J.K.,&Hoffmann,M.(2022).Learningtoreachtoownbodyfromspontaneous
self-touchusingagenerativemodel. In2022IEEEInternationalConferenceonDevelopmentand
Learning(ICDL)(pp.328‚Äì335).
Matsumoto,T.,Ohata,W.,Benureau,F.C.Y.,&Tani,J.(2022). Goal-DirectedPlanningandGoal
UnderstandingbyExtendedActiveInference: EvaluationthroughSimulatedandPhysicalRobot
Experiments. Entropy,24(4),469.
Mazzaglia,P.,Verbelen,T.,&Dhoedt,B.(2021). ContrastiveActiveInference. Advancesinneural
informationprocessingsystems,34,13870‚Äì13882.
Mele,A.R.&Moser,P.K.(1994). IntentionalAction. No√ªs,28(1),39‚Äì68.
Millidge,B.(2020). Deepactiveinferenceasvariationalpolicygradients. JournalofMathematical
Psychology,96,102348.
Millidge,B.,Tschantz,A.,Seth,A.K.,&Buckley,C.L.(2020). OntheRelationshipBetweenActive
InferenceandControlasInference. InT.Verbelen,P.Lanillos,C.L.Buckley,&C.DeBoom
(Eds.),ActiveInference,CommunicationsinComputerandInformationScience(pp.3‚Äì11).Cham:
SpringerInternationalPublishing.
Oliver,G.,Lanillos,P.,&Cheng,G.(2022). AnEmpiricalStudyofActiveInferenceonaHumanoid
Robot. IEEETransactionsonCognitiveandDevelopmentalSystems,14(2),462‚Äì471.
Oudeyer, P.-Y.&Kaplan, F.(2009). Whatisintrinsicmotivation? Atypologyofcomputational
approaches. Frontiersinneurorobotics,1,6.
Parr,T.,Pezzulo,G.,&Friston,K.J.(2022). ActiveInference: TheFreeEnergyPrincipleinMind,
Brain,andBehavior. TheMITPress.
Pathak, D., Agrawal, P., Efros, A.A., &Darrell, T.(2017). Curiosity-drivenexplorationbyself-
supervisedprediction. InInternationalconferenceonmachinelearning(pp.2778‚Äì2787).: PMLR.
Pearson,J.M.,Heilbronner,S.R.,Barack,D.L.,Hayden,B.Y.,&Platt,M.L.(2011). Posterior
cingulatecortex: adaptingbehaviortoachangingworld. TrendsinCognitiveSciences, 15(4),
143‚Äì151.
Priorelli,M.,Pezzulo,G.,&Stoianov,I.P.(2023). Deepkinematicinferenceaffordsefficientand
scalablecontrolofbodilymovements. ProceedingsoftheNationalAcademyofSciences,120(51),
e2309058120.
Priorelli,M.&Stoianov,I.P.(2023). Flexibleintentions: Anactiveinferencetheory. Frontiersin
ComputationalNeuroscience,Volume17-2023.
Rochat, P. (1998). Self-perception and action in infancy. Experimental Brain Research, 123(1),
102‚Äì109.
Ryan,R.M.&Deci,E.L.(2000). IntrinsicandExtrinsicMotivations: ClassicDefinitionsandNew
Directions. ContemporaryEducationalPsychology,25(1),54‚Äì67.
Sajid,N.,Tigas,P.,Zakharov,A.,Fountas,Z.,&Friston,K.(2021). Explorationandpreference
satisfactiontrade-offinreward-freelearning. arXiv:2106.04316.
Sancaktar, C., van Gerven, M. A. J., & Lanillos, P. (2020). End-to-end pixel-based deep active
inferenceforbodyperceptionandaction. In2020JointIEEE10thInternationalConferenceon
DevelopmentandLearningandEpigeneticRobotics(ICDL-EpiRob)(pp.1‚Äì8).
Schmidhuber,J.(2010). FormalTheoryofCreativity,Fun,andIntrinsicMotivation(1990‚Äì2010).
IEEETransactionsonAutonomousMentalDevelopment,2(3),230‚Äì247.
Schulman,J.,Moritz,P.,Levine,S.,Jordan,M.,&Abbeel,P.(2015). High-dimensionalcontinuous
controlusinggeneralizedadvantageestimation. arXiv:1506.02438.
Shultz,T.R.(2013). ComputationalModelsinDevelopmentalPsychology. InP.D.Zelazo(Ed.),
TheOxfordHandbookofDevelopmentalPsychology,Vol.1: BodyandMind.OxfordUniversity
Press.
19
Takemura,N.,Inui,T.,&Fukui,T.(2018). Aneuralnetworkmodelfordevelopmentofreachingand
pointingbasedontheinteractionofforwardandinversetransformations. DevelopmentalScience,
21(3),e12565.
Thelen,E.,Corbetta,D.,Kamm,K.,Spencer,J.P.,Schneider,K.,&Zernicke,R.F.(1993). The
TransitiontoReaching: MappingIntentionandIntrinsicDynamics. ChildDevelopment,64(4),
1058‚Äì1098.
Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,L.,&Polosukhin,
I.(2017). AttentionIsAllYouNeed. arXiv:1706.03762.
Warneken,F.&Tomasello,M.(2006). Altruistichelpinginhumaninfantsandyoungchimpanzees.
Science,311(5765),1301‚Äì1303.
White,R.W.(1959). Motivationreconsidered: theconceptofcompetence. Psychologicalreview,
66(5),297.
Yoshida,N.,Daikoku,T.,Nagai,Y.,&Kuniyoshi,Y.(2024). Emergenceofintegratedbehaviors
throughdirectoptimizationforhomeostasis. NeuralNetworks,177,106379.
Zaadnoordijk,L.&Bayne,T.(2020). TheOriginsofIntentionalAgency. psyArXiv:wa8gb.
Zaadnoordijk,L.,Besold,T.R.,&Cusack,R.(2022). Lessonsfrominfantlearningforunsupervised
machinelearning. NatureMachineIntelligence,4(6),510‚Äì520.
√áatal,O.,Wauthier,S.,DeBoom,C.,Verbelen,T.,&Dhoedt,B.(2020). LearningGenerativeState
SpaceModelsforActiveInference. FrontiersinComputationalNeuroscience,14.
20
A Appendix
A.1 DetailedDescriptionoftheModelfortheDiscreteEnvironment
A.1.1 VariationalFreeEnergyDerivation
Inthediscretemodel,observationo andhiddenstates areone-hotcolumnvectorswithN and
t t o
N rows,respectively. Categoricaldistributionparametersareimplementedasmatrices: Aisan
s
N √óN matrix, andeachB isanN √óN squarematrix. Forinstance, ifactionsare{LEFT,
o s a s s
STOP,RIGHT},separatematricesB ,B ,B exist.
LEFT STOP RIGHT
ThevariationalfreeenergyF expandsas:
F =E [logQ(s )‚àílogP(o ,s )]
Q(st) t t t
=E [logQ(s )‚àílogP(s )]‚àíE [logP(o |s )] (14)
Q(st) t t Q(st) t t
=œï ¬∑(logœï ‚àílog(B œï ))‚àíœï ¬∑(log(A¬∑o ))
t t at‚àí1 t‚àí1 t t
Theparameterœïthatminimizesfreeenergyisderivedbydifferentiation:
‚àÇF
=logœï ‚àílog(B œï )+1‚àílog(A¬∑o )=0
‚àÇœï t at‚àí1 t‚àí1 t
t
(15)
‚à¥logœï ‚âàlog(A¬∑o )+log(B œï )
t t at‚àí1 t‚àí1
œï ‚âàœÉ(log(A¬∑o )+log(B œï ))
t t at‚àí1 t‚àí1
ThiscorrespondstoEq.(7)inthemaintext.
A.1.2 ExpectedFreeEnergyDerivation
Theexpectedfreeenergycanbederivedthroughstep-by-stepapproximationsintoacomputationally
tractableform:
G =E [logQ(s |a )‚àílogPÀú(o ,s |a )]
Q(st+1,ot+1|at) t+1 t t+1 t+1 t
=E [logQ(s |a )‚àílogP(s |o ,a )‚àílogPÀú(o )]
Q(st+1,ot+1|at) t+1 t t+1 t+1 t t+1
(16)
‚âàE [logQ(s |a )‚àílogQ(s |o ,a )‚àílogPÀú(o )]
Q(st+1,ot+1|at) t+1 t t+1 t+1 t t+1
=E [logQ(o |a )‚àílogQ(o |s ,a )‚àílogPÀú(o )]
Q(st+1,ot+1|at) t+1 t t+1 t+1 t t+1
The approximation in the third line replaces the true posterior P(s | o ,a ) with the ap-
t+1 t+1 t
proximate posterior Q(s | o ,a ). The fourth line is derived by applying Bayes‚Äô rule:
t+1 t+1 t
Q(s,o)=Q(o|s)Q(s)=Q(s|o)Q(o). Reformulatingforcomputation:
G ‚âàE [‚àílogQ(o |s ,a )]
Q(st+1|at)Q(ot+1|st+1,at) t+1 t+1 t
+E [logQ(o |a )‚àílogPÀú(o )]
Q(ot+1|at) t+1 t t+1
=E [‚àílogP(o |s )]
Q(st+1|at)P(ot+1|st+1) t+1 t+1
(17)
+E [logQ(o |a )‚àílogPÀú(o )]
Q(ot+1|at) t+1 t t+1
=E [H[P(o |s )]]+D [Q(o |a )‚à•PÀú(o )]
Q(st+1|at) t+1 t+1 KL t+1 t t+1
=(B œï )¬∑H[A]+D [AB œï ‚à•C]
at t KL at t
ThiscorrespondstoEq.(9)inthemaintext. Inoursetup,sinceweassumethatAandBarealready
knownaccurately,theactionentropytermforinformationgainaboutparametersisomittedinthe
discreteenvironmentexperiment(see(Parretal.,2022)formoredetailsontheaboveequations).
A.2 DetailedDescriptionoftheModelfortheContinuousEnvironment
A.2.1 EmbeddingTransformationArchitectureforContinuousEnvironmentModel
Torepresentbothtactileandproprioceptivemodalitiesasaunifiedobservationo inthemodelfor
t
thecontinuousenvironment,weconverteachmodalityintoanembeddingofthesamedimensionality.
21
The tactile matrix (80√ó30) is transformed into an embedding using a Conv2D module, and the
proprioceptivevector(3-dimensional)istransformedintoanembeddingusingaMLP.Wecombine
thetwoembeddingsviaelement-wiseadditiontoyieldthefinalintegratedobservationembeddingo .
t
Todecodefromtheembeddingbacktotheoriginalsensations,weuseaConvTranspose2Dfortactile
andaMLPforproprioception.
A.2.2 RSSMComputation
Thedeterministicstateh iscomputedasfollows:
t
1. Encodepreviousstochasticstates andactiona intoasinglevectorviaalinearlayer
t‚àí1 t‚àí1
2. Feed the encoded vector together with previous deterministic state h into a GRU
t‚àí1
cell(Chungetal.,2014)
3. TheGRUcelloutputbecomesthenewdeterministicstateh
t
Thestochasticpriorp (sÀÜ |h )passesh throughaMLPtooutputmeanandvarianceparametersof
œï t t t
amultivariateGaussian,fromwhichthestochasticstateissampled.
Thestochasticposteriorq (s |h ,o )receivesaconcatenatedvectorofdeterministicstateh and
œï t t t t
embeddedobservationo . TheconcatenatedvectorispassedthroughaMLPtooutputmeanand
t
varianceparametersofamultivariateGaussian,fromwhichthestochasticstateissampled.
Thelikelihoodp (o |h ,s )transformstheconcatenatedh ands intoanembeddedo througha
œï t t t t t t
linearlayer,whichisthendecodedintoindividualmodality-specificobservations.
AllparametersœïconstitutingthisRSSMstructureareoptimizedviagradientdescenttominimizethe
followingvariationalfreeenergy:
F =E [logq(s )‚àílogp(o ,s )]
q(st) t t t
=E [logq(s )‚àílogp(s )]‚àíE [logp(o |s )]
q(st) t t q(st) t t
=D [q(s )‚à•p(s )]‚àíE [logp(o |s )]
KL t t q(st) t t
‚à¥argminF =argmin (cid:2) D [q (s |h ,o )‚à•p (s |h )]‚àíE [logp (o |h ,s )] (cid:3)
KL œï t t t œï t t qœï(st|ht,ot) œï t t t
œï œï
(18)
A.2.3 PolicyandValueNetworkTraining
Boththepolicynetworkq (a |s )andutility(value)networkg (s )takethestochasticstates as
Œ∏ t t œà t t
input,useaMLPtooutputmultivariateGaussianparameters,andsamplefromthisdistribution.
Thetrainingobjectivesare:
(cid:88)
argminL =argmin GŒª
policy t
Œ∏ Œ∏ t
(cid:88)
argminL =argmin (g (s )‚àíGŒª)
œà utility œà t œà t t (19)
(cid:26) (1‚àíŒª)g (s )+ŒªGŒª , ift<H
GŒª =G(s )+Œ≥ œà t+1 t+1
t t t g (s ), ift=H
œà H
Here,GŒªistheGAE(Œª)-estimatedexpectedutilityapproximation. Œ≥ isthediscountfactorandH is
t t
thesimulationhorizon.
A.3 HyperparametersforContinuousEnvironmentExperiments
Table3presentsallhyperparametersusedinthecontinuousenvironmentexperiments.
22
Table3: Hyperparametersforcontinuousenvironmentexperiments
Parameter Value Description
Environment&DataCollection
Episodelength(L) 1000 Timestepsperepisode
Initialrandomepisodes 100 Randomepisodesbeforetraining
Explorationnoise 0.3 Noiseaddedtopolicyduringtraining
TrainingSchedule
Replaybuffersize 450 Maxepisodesstoredinmemory
Trainingepochsperepisode 100 Trainingiterationsaftereachepisode
Batchsize(B) 50 Numberoftrajectoriessampledperepoch
Trajectorylength(L) 50 Timestepspersampledtrajectory
Planninghorizon(H) 15 Futurepredictionhorizonforpolicylearning
Gradientclipping 100.0 Maximumgradientnorm
Networkoptimizer AdamW Optimizerfortraining
ObservationEmbedding
CNNencoderchannels 4,8,16,16 Outputchannelsforeachencoderlayer
CNNdecoderchannels 16,8,4,3 Outputchannelsforeachdecoderlayer
CNNencoderkernelsizes 4,4,4,2 Kernelsizeforeachencoderlayer
CNNdecoderkernelsizes 2,4,4,4 Kernelsizeforeachdecoderlayer
CNNstride 2 Stridevalueforalllayers
MLPhiddenlayersize 32 UnitsinMLPhiddenlayer
MLPhiddenlayers 2 NumberofhiddenlayersinMLP
Integratedembeddingsize 64 Finalembeddingdimension
Activationlayer ELU Activationlayer
WorldModel(RSSM)
Deterministicstatesize(dim(h )) 200 DimensionofGRUhiddenstate
t
Stochasticstatesize(dim(s )) 30 Dimensionofstochasticlatentvariable
t
Hiddenlayersize 200 Unitsinhiddenlayers
Numberofhiddenlayers 1 Numberofhiddenlayers
Activationlayer ELU Activationlayer
Learningrate(Œ± ) 10‚àí3 Learningrateforworldmodelparameters
œï
Freenats 3.0 MinimumthresholdforKLdivergence
KLscale(Œ≤) 2.0 KLdivergenceweight(Œ≤-VAE)
Policy&ValueNetworks
Hiddenlayersize 200 Unitsinhiddenlayers
Numberofhiddenlayers 3 Numberofhiddenlayers
Activationlayer ELU Activationlayer
Learningrate(Œ± ,Œ± ) 10‚àí3 Learningrateforparameters
Œ∏ œà
Entropytemperature 10‚àí4 Entropyregularizationcoefficient
Discountfactor(Œ≥) 0.99 Discountrateforfuturerewards
GAElambda(Œª) 0.95 SmoothingparameterforGAEestimation
Self-Prior(NeuralSplineFlow)
Numberofsplinetransforms 3 Numberoftransformlayersintheflow
Hiddenlayersize 64 Unitsinsplinenetworkhiddenlayers
Activationlayer ReLU Activationlayer
Learningrate(Œ± ) 10‚àí3 Learningrateforself-priorparameters
Œæ
23

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Emergence of Goal-Directed Behaviors via Active Inference with Self-Prior"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ‚ùå BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ‚úÖ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ‚ùå BAD: Repeating the same claim 3+ times with slight variations
   ‚úÖ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
