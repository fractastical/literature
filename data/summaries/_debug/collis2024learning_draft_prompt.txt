=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Learning in Hybrid Active Inference Models
Citation Key: collis2024learning
Authors: Poppy Collis, Ryan Singh, Paul F Kinghorn

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2024

Key Terms: brighton, schoolofengineeringandinformatics, models, poppycollis, learning, inference, hybrid, active, kinghorn, universityofsussex

=== FULL PAPER TEXT ===

Learning in Hybrid Active Inference Models
PoppyCollis ⋆1,†,RyanSingh1,2,†,PaulFKinghorn1,andChristopherLBuckley1,2
1 SchoolofEngineeringandInformatics,UniversityofSussex,Brighton,UK
{pzc20, rs773, p.kinghorn, c.l.buckley}@sussex.ac.uk,
2 VERSESAIResearchLab,LosAngeles,California,USA
Abstract. An open problem in artificial intelligence is how systems can flexibly learn discrete
abstractionsthatareusefulforsolvinginherentlycontinuousproblems.Previousworkincompu-
tationalneurosciencehasconsideredthisfunctionalintegrationofdiscreteandcontinuousvariables
duringdecision-makingundertheformalismofactiveinference[13,29].However,theirfocusison
theexpressivephysicalimplementationofcategoricaldecisionsandthehierarchicalmixedgenerative
modelisassumedtobeknown.Asaconsequence,itisunclearhowthisframeworkmightbeextended
tothelearningofappropriatecoarse-grainedvariablesforagiventask.Inlightofthis,wepresenta
novelhierarchicalhybridactiveinferenceagentinwhichahigh-leveldiscreteactiveinferenceplanner
sitsabovealow-levelcontinuousactiveinferencecontroller.Wemakeuseofrecentworkinrecurrent
switchinglineardynamicalsystems(rSLDS)whichlearnmeaningfuldiscreterepresentationsof
complexcontinuousdynamicsviapiecewiselineardecomposition[22].Therepresentationslearntby
therSLDSinformthestructureofthehybriddecision-makingagentandallowusto(1)liftdecision-
makingintothediscretedomainenablingustoexploitinformation-theoreticexplorationbonuses
(2)specifytemporally-abstractedsub-goalsinamethodreminiscentoftheoptionsframework[34]
and(3)‘cache’theapproximatesolutionstolow-levelproblemsinthediscreteplanner.Weapply
ourmodeltothesparseContinuousMountainCartask,demonstratingfastsystemidentification
viaenhancedexplorationandsuccessfulplanningthroughthedelineationofabstractsub-goals.
Keywords: hybridstate-spacemodels,decision-making,piecewiseaffinesystems
1 Introduction
Inaworldthatisinherentlyhigh-dimensionalandcontinuous,thebrain’scapacitytodistilandreason
aboutdiscreteconceptsrepresentsahighlydesirablefeatureinthedesignofautonomoussystems.Humans
areabletoflexiblyspecifyabstractsub-goalsduringplanning,therebyreducingcomplexproblemsinto
manageablechunks[26,16].Indeed,translatingproblemsintodiscrete spaceoffersdistinctadvantagesin
decision-making systems. For one, discrete states admit the direct implementation of classical techniques
fromdecisiontheorysuchasdynamicprogramming[21].Furthermore,wealsofindthecomputationallyfea-
sibleapplicationofinformation-theoreticmeasures(e.g.information-gain)indiscretespaces.Suchmeasures
(generally)requireapproximationsincontinuoussettingsbutthesehaveclosed-formsolutionsinthediscrete
case[12].While theprevailingmethodfortranslatingcontinuousvariablesintodiscreterepresentations
involvesthesimplegrid-baseddiscretisationofthestate-space,thisbecomesextremelycostlyasthedimen-
sionalityincreases[7,24].Wethereforeseektodevelopaframeworkwhichisabletosmoothlyhandlethe
presenceofcontinuousvariableswhilstmaintainingthebenefitsofdecision-makinginthediscretedomain.
⋆ Correspondingauthor
-†Equalcontribution
4202
peS
2
]IA.sc[
1v66010.9042:viXra
2 P.Collis,R.Singh,P.F.KinghornandC.L.Buckley
Fig.1:Previous discrete-continuous active inference models have focused on the physical implemen-
tation of categorical decisions in continuous space.Here,outcomesfromthehigh-levelactiveinference
plannerselectfromasetofdiscretemodelsofcontinuousdynamics,specifiedbyapriorovertheirhiddencauses.
Thismixedgenerativemodeleffectivelygeneratesdiscretesequencesofshortcontinuoustrajectoriesdefinedin
terms of their generalised coordinates of motion. The discrete planner is formulated as a standard POMDP
generativemodel(seeSec.3.3)withdiscretestatess andobservationso .Thefirstactiona oftheselected
τ τ τ
policy(seeSec.3.2)isthenpasseddowntothecontinuousactiveinferencecontrollerviatheexpectedobservation
q(o |a ).Thisdistributionweightsasetoffixedpointmeans{η }M−1 whichmapthemdiscretelatentstates
τ+1 τ m m=0
(cid:80)
intocontinuousstatespace.Theresultingweightedaverage,η = η ·q(o |a),servesasthemeanofa
τ m m τ+1,m
prioroverhiddencauses,p(ν)=N(η ,π−1),whichdrivesthedynamicsofthelow-levelcontinuouslatentstates
τ τ
x˜ ={x,x′,x′′} and observations y˜ ={y,x′,x′′} represented in generalised coordinates. Inherently, there is a
t t t t t t t t
separation of timescales in this open-loop control setup: the discrete controller sends an action down to the
continuous controller which is then executed in a ballistic manner over several timesteps. After this low-level
inner-loopcompletes,aprocessofBayesianmodelselectionisusedtoinferthecurrentdiscretestatedescription
ofthelow-levelsystemgiventhetrajectoryofcontinuousobservationsy˜.Thisisthengivenasanobservationfor
t
thediscreteplanneratthetop.Forafulltreatmentofthismodel,see[13].
1.1 Hybrid Active Inference
Here,wedrawonrecentworkinactiveinference(AIF)whichhasforegroundedtheutilityofdecision-
making in discrete state-spaces [8,12]. Additionally, discrete AIF has been successfully combined with
low-levelcontinuousrepresentationsandusedtomodelarangeofcomplexbehaviourincludingspeech
production,oculomotionandtooluse[13,29,14,30,31].Asdetailedin[13],suchmixedgenerativemodels
focusonthephysicalimplementationofcategoricaldecisions.Thistreatmentbeginswiththepremisethat
theworldcanbedescribedbyasetofdiscretestatesevolvingautonomouslyanddrivingthelow-level
LearninginHybridActiveInferenceModels 3
Fig.2:Recurrent switching linear dynamical systems (rSLDS) discover meaningful discrete states
and explain how their switching behaviour depends on continuous latent states.Thisclassofhybrid
statespacemodelincludesarecurrentdependencyofthediscretelatentstatez onthecontinuouslatentstatex
t+1 t
andcontrolinputu.AsinastandardSLDS,thecontinuouslatentdynamicsareconditionallylinear(dependent
t
on the current discrete state z) and generate observations y. Note that this figure shows the recurrent-only
t t
formulationoftherSLDS(seeSec.3.1)inwhichthediscretelatentz havenodependencyonz asispresent
t t−1
initscanonicalform.
continuous states by indexing a set of attractors (c.f. subgoals) encoded through priors which have
beenbuiltintothemodel(seeFig.1).Whiletheemphasisoftheaboveworkisonmappingcategorical
decision-makingtothecontinuousphysicalworld,here,weapproachthequestionoflearningthegenerative
model.Specifically,weseekthecompletelearningofappropriatediscreterepresentationsoftheunderlying
dynamicsandtheirmanifestationincontinuousspace.Importantly, unlike the previousworkmentioned
here,wefocusoninstancesinwhichthemappingbetweenthediscretestatesandthecontinuousstates
is not assumed to be known. In this case, however, the assumption that higher-level discrete states
autonomouslydrivelower-levelcontinuousstates(i.e.downwardcausation)becomesproblematic.Any
failureofthecontinuoussystemtocarryoutadiscretepreferencemustbetreatedasanautonomousfailure
atthediscretelevel.Althoughusefulforplanning,thisdecouplingofthediscretefromthecontinuous
componentsmakesitdifficulttorepresentcomplexdynamics,whichinturncreatesdifficultiesinlearning.
1.2 Recurrent Switching Systems
Previous work has demonstrated that models involving autonomous switching systems are often not
sufficientlyexpressivetoapproximaterealisticgenerativeprocesses[22].Theystudythisprobleminthe
context of a class of hybrid state-space model known as switching linear dynamical systems (SLDS).
These models have been shown to discover meaningful behavioural modes and their causal states via
thepiecewiselineardecompositionofcomplexcontinuousdynamics[15,11].Theauthorsof[22]remedy
the problem associated with limited expressivity by introducing recurrent switching linear dynamical
systems (rSLDS) (see Fig. 2). These models importantly include a dependency from the underlying
continuous variables in the high-level discrete transition probabilities. By providing an understanding
ofthecontinuouslatentcausesofswitchesbetweenthediscretestatesviathisadditionaldependency,the
authorsdemonstrateimprovedgenerativecapacityandpredictiveperformance.Weproposethisricher
4 P.Collis,R.Singh,P.F.KinghornandC.L.Buckley
representationcanbeusefulfordecisionmakingandcontrol.Thisrecurrenttransitionstructurecanbe
exploitedsuchthatcontinuouspriorscanbeflexiblyspecifiedforalow-levelcontrollerinordertodrive
thesystemintoadesiredregionofthestatespace.Usingstatisticalmethodstofitthesemodelsnotonly
liberatesusfromtheneedtoexplicitlyspecifyamappingbetweendiscreteandcontinuousstatesapriori,
butenableseffectiveonlinediscoveryofusefulnon-griddiscretisationsofthestate-space.
1.3 Emergent descriptions for planning
Unfortunately, the inclusion of recurrent dependencies also destroys the neat separation of discrete
planningfromcontinuouscontrol,creatinguniquechallengesinperformingroll-outs.Ourcentralinsight
is to re-instate the separation by lifting the dynamical system into the discrete domain only during
planning. We do this by approximately integrating out the continuous variables, naturally leading to
spatio-temporallyabstractedactionsandsub-goals.Ourdiscreteplannerthereforeoperatespurelyatthe
levelofare-descriptionofthediscretelatents,modellingnothingoftheautonomoustransitionprobabilities
butratherreflecting transitions thatarepossiblegiven thediscretisationofthe continuousstate-space.
Inshort,wedescribeanovelhybridhierarchicalactiveinferenceagent[28]inwhichadiscreteMarkov
decisionprocess(MDP),informedbytherepresentationsofanrSLDS,interfaceswithacontinuousactive
inferencecontrollerimplementingclosed-loopcontrol.Wedemonstratetheefficacyofthisalgorithmby
applyingittotheclassiccontroltaskofContinuousMountainCar[27].Weshowthattheexploratory
bonusesaffordedby theemergentdiscrete piecewisedescriptionofthetask-spacefacilitatesfastsystem
identification.Moreover,thelearntrepresentationsenabletheagenttosuccessfullysolvethisnon-trivial
planningproblembyspecifyingaseriesofabstractsubgoals.
2 Related work
SuchtemporalabstractionsarethefocusofHierarchicalreinforcementlearning(HRL),wherehigh-level
controllersprovidethemeansforreasoningbeyondtheclock-rateofthelow-levelcontrollersprimitive
actions.[10,34,9,18].ThemajorityofHRLmethods,however,dependondomainexpertisetoconstruct
tasks, often through manually predefined subgoals as seen in [35]. Further, efforts to learn hierarchies
directlyinasparseenvironmenthavetypicallybeenunsuccessful[36].Incontrast,ourabstractionsare
anaturalconsequenceofliftingtheproblemintothediscretedomainandcanbelearntindependently
ofreward.Inthecontextofcontrol,hybridmodelsintheformofpiecewiseaffine(PWA)systemshave
been rigorously examined and are widely applied in real-world scenarios [33,3,6]. Previous work has
appliedavariantonrSLDS(recurrentautoregressivehiddenMarkovmodels)totheoptimalcontrolof
generalnonlinear systems[2,1].The authorsuse thesemodelsto theapproximateexpertcontrollersin
aclosed-loopbehaviouralcloningcontext.Whiletheiralgorithmfocusesonvaluefunctionapproximation,
incontrast,welearnonlinewithoutexpertdataandfocusonflexiblediscreteplanning.
3 Framework
The following sections detail the components of our Hierachical Hybrid Agent (HHA). For additional
information,pleaserefertoAppendix.A.
LearninginHybridActiveInferenceModels 5
3.1 Generative Model: rSLDS(ro)
Intherecurrent-only(ro)formulationoftherSLDS(seeFig.2),thediscretelatentstatesz ∈{1,2,...,K}
t
aregeneratedasafunctionofthecontinuouslatentsx ∈RM andthecontrolinputu ∈RN (specified
t t
bysomecontroller)viaasoftmaxregressionmodel,
P(z |x ,u )=softmax(W x +W u +r) (1)
t+1 t t x t u t
whereby W ∈RK×M and W ∈RK×N are weight matrices and r is abias ofsize RK. Thecontinuous
x u
latentstatesx evolveaccordingtoalineardynamicalsystemindexedbythecurrentdiscretestatez .
t t
x |x ,u ,z =A x +B u +b +ν ,
t+1 t t t zt t zt t zt t
(2)
ν ∼N(0,Q )
t zt
y |x =C x +ω , ω ∼N(0,S ) (3)
t t zt t t t zt
A isthestatetransitionmatrix,whichdefineshowthestatex evolvesintheabsenceofinput.B is
zt t zt
thecontrolmatrixwhichdefineshowexternalinputsinfluencethestateofthesystemwhileb isanoffset
zt
vector.Ateachtime-stept,weobserveanobservationy ∈RM producedbyasimplelinear-Gaussianemis-
t
sionmodelwithanidentitymatrixC .Boththedynamicsofthecontinuouslatentsandtheobservations
zt
areperturbedbyzero-meanGaussiannoisewithcovariancematricesofQ andS respectively.
zt zt
Inferencerequiresapproximatemethods giventhatthe recurrentconnectionsbreak conjugacyrender-
ing the conditional likelihoods non-Gaussian. Therefore, a Laplace Variational Expectation Maximisation
(EM)algorithmisusedtoapproximatetheposteriordistributionoverthelatentvariablesbyamean-field
factorisationintoseparatedistributionsforthediscretestatesq(z)andthecontinuousstatesq(x).The
discretestateisupdatedviaacoordinateascentvariationalinference(CAVI)approachbyleveragingthe
forward-backwardalgorithm.ThecontinuousstatedistributionisupdatedusingaLaplaceapproximation
aroundthemodeoftheexpectedlogjointprobability.Thisinvolvesfindingthemostlikelycontinuous
latentstatesbymaximizingtheexpectedlogjointprobabilityandcomputingtheHessiantoapproximate
theposterior.FulldetailsoftheLaplaceVariationalEMusedforlearningaregivenin[37].
The rSLDS is initialised according to the procedure outlined in [22]. In order to learn the rSLDS
parameters using Bayesian updates, conjugate matrix normal inverse Wishart (MNIW) priors are placed
ontheparametersofthedynamicalsystemandrecurrenceweights.Welearntheparametersonlinevia
observingthebehaviouraltrajectoriesoftheagentandupdatingtheparametersinbatches(every1000
timestepsoftheenvironment).
3.2 Active Inference
Equippedwithagenerativemodel,activeinferencespecifieshowanagentcansolvedecisionmakingtasks
[28].Policyselection isformulatedasa search procedurein which afreeenergy functionalofpredicted
statesisevaluatedforeachpossiblepolicy.Formally,weuseanupperboundontheexpectedfreeenergy
(G)givenby:
G (π)≤−E [D [Q(s|o,π)∥Q(s|π)]]
1:T Q(o|π) KL
(cid:124) (cid:123)(cid:122) (cid:125)
StateInformationGain
−E [D [Q(θ|o,π)∥Q(θ|π)]]
Q(o|π) KL
(4)
(cid:124) (cid:123)(cid:122) (cid:125)
ParameterInformationGain
−E [lnp˜(o)].
Q(o|π)
(cid:124) (cid:123)(cid:122) (cid:125)
Utility
6 P.Collis,R.Singh,P.F.KinghornandC.L.Buckley
Wheres={s ,...,s }ando={o ,...,o }arethestatesandobservationsbeingevaluatedunderaparticular
1 T 1 T
policyorsequenceofactions,π=a .Theintegrationofrewardsintheinferenceprocedureisachieved
1:T
bybiasingtheagent’sgenerativemodelwithanoptimisticprioroverobservingdesiredoutcomesp˜(o).
Actionselectiontheninvolvesconvertingthisintoaprobabilitydistributionoverthesetofpoliciesand
samplingfromthisdistributionaccordingly.
3.3 Discrete Planner
In order to create approximate plans at the discrete level, we derive a high-level planner based on a
re-description of the discrete latent states found by the rSLDS by approximately ‘integrating out’ the
continuousvariablesandthecontinuousprior.Thisprocessinvolvescalculatingtheexpectedfreeenergy
(G)foracontinuouscontrollertodrivethesystemfromonemodetoanother.Importantly,thestructure
of the lifted discrete state transition model has been constrained by the polyhedral partition of the
continuous state space extracted from the parameters of the rSLDS 3: invalid transitions are assigned
zeroprobabilitywhilevalidtransitionsareassignedahighprobability.Inordertogeneratethepossible
transitionsfromtherSLDS,wecalculatethesetofactiveconstraintsforeachregionfromthesoftmax
representation, p(z|x)=σ(Wx+b). Specifically, to check that the region i is adjacent to region j, we
verifythesolutionusingalinearprogram,
−b =min(W −W )x (5)
j i j
s.t.(W −W )x≤(b −b )∀k∈[K] (6)
i k i k
s.t.x∈(x ,x ) (7)
lb ub
where (x ,x ) are bounds chosen to reflect realistic values for the problem. This ensures we only lift
lb ub
transitionstothediscretemodeliftheyarepossible.Afterintegration,weareleftwithadiscreteMDP
which contains averaged information about all of the underlying continuous quantities. This includes
information about the transitions that the structure of the task space allows, and their corresponding
approximatecontrolcosts(seeA.2).NotethataftereachbatchupdateoftherSLDSparameters,this
discreteplannermustberefittedaccordingly.
The lifted discrete generative model has all the components of a standard POMDP in the active
inferenceframework:
(cid:89)
P(o ,s ,A,B,π)=P(π)P(A)P(B)P(s ) P(s |s ,B,π)P(o |s ,A) (8)
1:T 1:T 0 t t−1 t t
t
along with prior over policies P(π)=Cat(E), and preference distribution P˜(o )=Cat(C). Specifically
t
ourliftedP(π)reflectstheapproximatecontrolcostsofeachcontinuoustransitionandP˜(o )reflectsthe
t
rewardavailableineachmode.Weassumeanidentitymappingbetweenstatesandobservationmeaning
thestateinformationgainterminEq.4collapsesintoamaximumentropyregulariser,whilewemaintain
Dirichlet priors over the transition parameters B, facilitating directed exploration. Due to conjugate
structureBayesianupdatesamounttoasimplecount-basedupdateoftheDirichletparameters[25].At
eachtimestep,thediscreteplannerselectsapolicybysamplingfromthefollowingdistribution:
Q(π)=softmax(−G(π)+lnP(π)). (9)
3 Foravisualisationofthispartitioningofthestatespace,seeFig.4(a)
LearninginHybridActiveInferenceModels 7
Fig.3:OurHybridHierarchicalAgentlearnsemergentcoarse-graineddescriptionsofthecontinuous
state-space for planning and control.Likepreviousworkonmixedgenerativemodelsinactiveinference
showninFig.1,wehaveadiscreteactiveinferenceplannersittingabovealow-levelcontinuousactiveinference
controller.ThediscreteplannerisconstructedasastandardPOMDPgenerativemodel(seeSec.3.3)withdiscrete
statess andobservationso .However,ourmodeldepartsfrom[13]inthegenerationofcoarse-grainedvariables
τ τ
whichinsteademergefromthetheunderlyingrSLDSgenerativemodel.Here,thestatesS oftheplannerare
essentiallyare-descriptionofthediscretestatesZ foundbytherSLDS.Thetransitionmodelprobabilitiesare
thenconstrainedtoreflecttheadjacencystructureofthepolyhedralpartitionsofthestate-spacefoundbythe
softmaxregressioncomponentofrSLDS.Thechosenactiona fromthehigh-levelplannerselectsfromadiscrete
τ
setofcontinuousactiveinferencecontrollersbasedonboththelineardynamicsofthecurrentdiscretestatez
τ
andacontrolpriorforthedesirednextdiscretestate.UsingtherSLDSgenerativemodel,thispriorisaflexibly
specified continuous point in the state-space that is in the discrete region the agent wishes to move into (see
Eq.10).Unlikethemodelsin[13],theactiona istemporallyabstractedwithnopre-definedtimescaleatthe
τ
lower level. Instead, the discrete planner is only re-triggered when the system enters a new discrete state (i.e.
z ≠ z ).Atwhichpoint,theplannerobservesthenewdiscretestatez ofthesystemandconstructsaplan
t t−1 τ
accordingly.
The policy is then communicated to the continuous controller. Specifically, the first action of the selected
policy is a requested transition i→j and is translated into a continuous control prior p˜(x)∼N(x ,Σ )
j j
8 P.Collis,R.Singh,P.F.KinghornandC.L.Buckley
viathefollowinglinkfunction,
x =argmaxP(z=j|x,u) (10)
j
x
wherebywenumericallyoptimiseforapointinspaceuptosomeprobabilitythreshold,T (fordetailson
thisoptimisation,seeA.6).Thesepriorsrepresentsanapproximatelycentralpointinthedesireddiscrete
regionj requestedbytheactionaj.Notethatthesepriorsonlyneedtobecalculatedonceperrefitofthe
rSLDS.Thediscreteplannerinfersitscurrentstates fromobservingz .Importantly,thediscreteplanneris
τ t
onlytriggeredwhenthesystemswitchesintoanewmode4.Inthissense,discreteactionsaretemporallyab-
stractedanddecoupledfromcontinuousclock-timeinamethodreminiscentoftheoptionsframework[34].
3.4 Continuous controller
Continuousclosed-loopcontrolishandledbyasetofcontinuousactiveinferencecontrollers.Forcontrolling
thetransitionfrommodeitomodej (x tox ),theobjectiveofthecontrolleristominimisethefollowing
i j
(discrete-time)expectedfreeenergyfunctional 5:
S
(cid:88)
G (π)=E [(x −x )TQ (x −x )+ uT(R−Πu)u ]+lndetΠ (11)
ij q(·|x0=xi,π) S j f S j t t t
t=0
4 Oramaximumdwell-time(hyperparameter)isreached.
5 Asshownin[20]linearstatespacemodelsprecludestateinformationgaintermsleavingthesimplifiedform
seenhere.
Fig.4:HHA solves nonlinear problems via specifying abstract sub-goals in state-space. (a)Piecewise
lineardynamicsoftheContinuousMountainCarstate-spacefoundbyrSLDSrepresentedasavectorplotwhere
magnitudeofthearrowsindicateshowfastthestateischangingatthatpoint.Rewardlocationshown(black
triangle).WhiletherSLDSretrives5modesintotal,hereweplotonlythemodesseenintheposition-velocity(x)
spacewithoutshowingthecontrolinput(u)axis.(b)Exampletrajectory(segmentscolouredbymode)showing
theHHAconsistentlynavigatingtothegoal.(c)Continuouscontrolinput(colouredbydiscreteactionspecified
byplannerandarrowsizeindicatingmagnitudeanddirection)oversameexampletrajectoryin(b).
LearninginHybridActiveInferenceModels 9
WhereS isthefinitetimehorizonandthequadratictermsderivefromGaussianpreferencesaboutthe
finalstatep˜ (x )∼N(x ,Q−1)andtimeinvariantcontrolinputpriorp(u )∼N(0,R−1)(A.4).Importantly
j S j f t
wedesignthecontrolpriorssuchthatthecontrolleronlyprovidessolutionswithintheenvironmentsgiven
constraints (for further discussion, see Sec. 5). The approximate closed-loop solution to each of these
sub-problemsiscomputedofflineeachtimetherSLDSisrefitted(seeA.3)usingtheupdatedparameters
ofthelineardynamicalsystems,allowingforfastdiscrete-onlyplanningwhenonline.
4 Results
Toevaluatetheperformanceofour(HHA)model,weappliedittotheclassiccontrolproblemofContinu-
ousMountainCar.Thisproblemisparticularlyrelevantforourpurposesduetothesparsenatureofthe
rewards,necessitatingeffectiveexplorationstrategiestoachievegoodperformance.WefindthattheHHA
findspiecewiseaffineapproximations ofthetask-spaceandusesthesediscretemodeseffectivelyto solve
thetask.Fig.4showsthatwhiletherSLDShasdividedupthespaceaccordingtoposition,velocityand
Fig.5:HHA with information-gain explored a wider range of the state-space.State-spacecoveragein
ContinuousMountainCarafter10,000stepsandbestof3runsfor(a)HHAwithinformation-gaindrive,(b)HHA
withoutinformationgaindriveand(c)randomlysampledcontinuousactionsbaseline.HHAwithinformation-gain
drivealsoshowscomparableperformanceto(d)aDeepQ-NetworkwithModel-BasedExploration(DQN-MBE)
onthe(comparablyeasier)DiscreteMountainCartask[17].ExactparametersforDQN-MBEaregiveninTable3
inA.8.
10 P.Collis,R.Singh,P.F.KinghornandC.L.Buckley
controlinput,theusefulmodesforsolvingthetaskarethosefoundinthepositionspace.Oncethegoal
andagoodapproximationtothesystemhasbeenfound,theHHAsuccessfullyandconsistentlynavigates
to the reward. This can be seen in the example trajectories (in Fig.4b) where the agent starts at the
centralposition[0,0]andproceedstorockbackandforthwithinthevalleyuntilenoughmomentumis
gainedfor the cartoreach theflagposition ataposition of0.5.Theepisodeterminatesonce thereward
hasbeenreachedandtheagentisre-spawnedattheoriginbeforerepeatingthesamesuccessfulsolution.
Fig.5showsthattheHHAperformsacomprehensiveexplorationofthestate-spaceandsignificantgains
inthestate-spacecoverageareobservedwhenusinginformation-gaindriveinpolicyselectioncomparedto
without.Indeed,ourmodelcompeteswiththestate-spacecoverageachievedbymodel-basedalgorithms
withexploratoryenhancementsinthediscreteMountainCartask,whichisinherentlyeasiertosolve.
WecomparetheperformanceoftheHHAtomodel-freereinforcementlearningbaselines(Actor-Critic
and Soft Actor-Critic) and find that the HHA both finds the reward and capitalises on its experience
significantlyquickerthantheothermodels(seeFig.6).Givenboththesparsenatureofthetaskandthe
poorexploratoryperformanceofrandomactioninthecontinuousspace,theseRLbaselinesstruggletofind
thegoalwithin20episodeswithouttheimplementationofreward-shapingtechniques.Withreferencetothe
highsamplecomplexityofthesealgorithms,ourmodelsignificantlyoutperformsotherbaselinesinthistask.
Fig.6: HHA both finds the reward and captilises on its experience significantly quicker than
model-free RL baselines.Averagereward(+/-std)over6runsforContinuousMountainCar(20episodes,
maxepisodelengthof200steps)forHHA(ourmodel),Soft-ActorCritic(with2Q-functions),andActor-Critic
models.Notethatafter20episodes,SACandACareyettofindtherewardandconvergeonasolution.
5 Discussion
Theemergenceofnon-griddiscretisationsofthestate-spaceallowsustoperformfastsystemsidentifi-
cationviaenhancedexploration,andsuccessfulnon-trivialplanningthroughthedelineationofabstract
sub-goals. Hence, the time spent exploring each region is not based on euclidean volume which helps
mitigatethecurseofdimensionalitythatothergrid-basedmethodssufferfrom.Interestingly,evenwithout
information-gain,theareacoveredbyourhybridhierarchicalagentisstillnotablybetterthanthatof
the random continuous action control (see Fig. 5c). This is because the agent is still operating at the
levelofthenon-griddiscretisationofthestate-spacewhichactstosignificantlyreducethedimensionality
ofthesearchspaceinabehaviourallyrelevantway.
LearninginHybridActiveInferenceModels 11
Suchapiecewiseaffineapproximationofthespacewillincursomelossofoptimalityinthelongrunwhen
pittedagainstblack-boxapproximators.Thisisduetothenatureofcachingonlyapproximateclosed-loop
solutionstocontrolwithineachpiecewiseregion,whilstthediscreteplannerimplementsopen-loopcontrol.
However, this approach eases the online computational burden for flexible re-planning. Hence, in the
presenceofnoiseorperturbationswithinaregion,thecontrollermayadaptwithoutanynewcomputation.
Thisisincontrasttoothernonlinearmodel-basedalgorithmslikemodel-predictivecontrolwherereacting
todisturbancesrequiresexpensivetrajectoryoptimisationateverystep[32].Byusingthepiecewiseaffine
framework,we maintainfunctionalsimplicity andinterpretabilitythroughstructured representation.We
thereforesuggestthatthismethodisamenabletofuturealignmentwithacontrol-theoreticapproachto
safetyguaranteesforensuringrobustsystemperformanceandreliability.Indeed,suchuseofdiscreteapprox-
imationstocontinuoustrajectorieshasbeenshowntoimprovetheabilitytohandleuncertainty.Evidence
oftheefficacyofthiskindofapproachinmachinelearningapplicationshasbeenexhibitedinrecentwork
by[5],whichexaminedtheproblemofcompoundingerrorinimitationlearningfromexpertdemonstration.
Theauthorsdemonstratedthatapplyingasetofprimitivecontrollerstodiscreteapproximationsoftheex-
perttrajectoryeffectivelymitigatedtheaccumulationoferrorbyensuringlocalstabilitywithineachchunk.
We acknowledgethere may bebetter solutionstodealingwithcontrolinput constraints than theone
giveninSec.3.4.Differentapproacheshavebeentakentotheproblemofimplementingconstrained-LQR
control,suchasfurtherpiecewiseapproximationbasedondefiningreachabilityregionsforthecontroller[4].
6 Conclusion
Insummary,thesuccessfulapplicationofourhybridhierarchicalactiveinferenceagentintheContinuous
MountainCarproblemshowcasesthepotentialofrecurrentswitchinglineardynamicalsystems(rSLDS)for
enhancingdecision-makingandcontrolincomplexenvironments.ByleveragingrSLDStodiscovermeaning-
fulcoarse-grainedrepresentationsofcontinuousdynamics,ourapproachfacilitatesefficientsystemidentifi-
cationandtheformulationofabstractsub-goalsthatdriveeffectiveplanning.Thismethodrevealsapromis-
ingpathwayfortheend-to-endlearningofhierarchicalmixedgenerativemodelsforactiveinference,provid-
ingaframeworkfortacklingabroadrangeofdecision-makingtasksthatrequiretheintegrationofdiscrete
andcontinuousvariables.Thesuccessofouragentinthiscontroltaskdemonstratesthevalueofsuchhybrid
modelsinachievingbothcomputationalefficiencyandflexibilityindynamic,high-dimensionalsettings.
Acknowledgements ThisworkwassupportedbyTheLeverhulmeTrustthroughthebe.AIDoctoral
ScholarshipProgrammeinBiomimeticEmbodiedAI.Additionally,thisresearchreceivedfundingfromthe
EuropeanInnovationCouncilviatheUKRIHorizonEuropeGuaranteeschemeaspartoftheMetaTool
project.Wegratefullyacknowledgebothfundingsourcesfortheirsupport.
Disclosure of Interests Theauthorshavenocompetingintereststodeclarethatarerelevanttothe
contentofthisarticle.
12 P.Collis,R.Singh,P.F.KinghornandC.L.Buckley
A Appendix
A.1 Framework
Optimal Control To motivate our approximate hierarchical decomposition, we adopt the optimal
controlframework,specificallyweconsiderdiscretetimestatespacedynamicsoftheform:
x =f(x ,u ,η ) (12)
t+1 t t t
withknowninitialconditionx ,andnoiseη drawnfromsometimeinvariantdistributionη ∼D,where
0 t t
we assume f to be p(x |x ,u ) and is a valid probability density throughout. We use c :X×U→R
t+1 t t t
forthecontrolcostfunctionattimetandletUbethesetofadmissible(non-anticipative,continuous)
feedback control laws, possibly restricted by affine constraints. The optimal control law for the finite
horizonproblemisgivenas:
T
(cid:88)
J(π)=E [ c (x ,u )] (13)
x0,π t t t
t=0
π∗=argminJ(π) (14)
π∈U
PWA Optimal Control Thefactwedonothaveaccesstothetruedynamicalsystemf motivates
theuseofapiecewiseaffine(PWA)approximation.Alsoknownashybridsystems:
x =Ax +Bu +ϵ (15)
t+1 i t i t t
when(x ,u )∈H (16)
t t i
Where H={H :i∈[K]} is a polyhedral partition of the space X×U. In the case of a quadratic cost
i
function, it can be shown the optimal control law for such a system is peicewise linear. Further there
exist many completeness (universal approximation) typetheorems for peicewise linearapproximations
implyingiftheoriginalsystemiscontrollable,therewillexistapeicewiseaffineapproximationthrough
whichthesystemisstillcontrollable[3,6].
Relationship to rSLDS WeperformacanonicaldecompositionofthecontrolobjectiveJ interms
ofthecomponentsormodesofthesystem.Byslightabuseofnotation[x =i]:=[(x ,u )∈H]represent
t t t i
theIversonbracket.
(cid:90)
(cid:88)
J(π)= p (x |x ,u )c (x ,u )dx dx (17)
π t t−1 t t t t t t−1
t
(cid:90)
(cid:88) (cid:88)
= [x =i]p (x |x ,u )c (x ,u )dx dx (18)
t−1 π t t−1 t t t t t t−1
t i∈[K]
Now let z be the random variable on [K] induced by Z =i if [x =i] we can rewrite the above more
t t t
conciselyas,
(cid:90)
(cid:88) (cid:88)
J(π)= p (x ,z =i|x ,u )c (x ,u )dx dx (19)
π t t−1 t−1 t t t t t t−1
t i∈[K]
(cid:90)
(cid:88)(cid:88)
= p (x ,z =i|x ,u )c (x ,u )dx dx (20)
π t t−1 t−1 t t t t t t−1
i∈[K] t
(cid:88)(cid:88)
= E [c (x ,u )] (21)
πi t t t
i∈[K] t
LearninginHybridActiveInferenceModels 13
which isjust the expectation undera recurrentdynamical system with deterministic switches. Later (see
A.5),weexploitthenon-deterministicswitchesofrSLDSinordertodriveexploration.Eq.21demonstrates
the global problem can be partitioned solving problems within each region (inner expectation), and a
globaldiscreteproblemwhichdecideswhichsequenceofregionstovisit.Inthenextsection,weintroduce
anewsetofvariableswhichallowsustoapproximatelydecoupletheproblems.
A.2 Hierarchical Decomposition
Our aim was to decouple the discrete planning problem from the fast low-level controller. In order to
break down the control objective in this manner, we first create a new discrete variable which simply
tracksthetransitionsofz,thisallowsthediscreteplannertofunctioninatemporallyabstractedmanner.
DecouplingfromclocktimeLettherandomvariable(ζ ) recordthetransitionsof(z ) i.e.let
s s>0 t t>0
τ (τ )=min{t:z ≠ z ,t>τ },τ =0 (22)
s s−1 t+1 t s−1 0
be the sequence of first exit times, then ζ is given by ζ =z . With these variables in hand, we frame
s τs
asmallsectionoftheglobalproblemasafirstexitproblem.
Low level problems Considerthefirstexitproblemforexitingregioniandenteringj definedby:
π (x )=argminJ (π,x ,S) (23)
ij 0 ij 0
π,S
S
(cid:88)
J (π,x ,S)=E [ c(x ,u )] (24)
ij 0 π,x0 t t
t=0
s.t.(x ,u )∈H (25)
t t i
s.t.c(x,u)=0when(x,u)∈∂H (26)
ij
(cid:84)
where ∂H is the boundary H H . Due, to convexity of the polyhedral partition, the full objective
ij i j
admitsthedecompositionintermsofthesesubproblems,
(cid:88)
J(π)= J (π,x ,t −t ) (27)
ζ(s+1),ζ(s) ts s+1 s
s
Ideally, we would like to simply solve all possible subproblems {J∗(x):i,j∈[K]×[K]} and then find a
ij
sequenceofdiscretestates,ζ(1),...,ζ(S),whichminimisesthesumofthesub-costs,howevernoticeeachsub-
costdependsonthestartingstate,andfurtherthisisdeterminedbythefinalstateofthepreviousproblem.
Apureseparationintodiscreteandcontinuousproblemsisnotpossiblewithoutasimplifyingassumption.
Slow and fast mode assumption The goal is to tackle the decomposed objectives individually,
howeverthehiddenconstraintthatthetrajectorieslineuppresentsacomputationalchallenge.Herewe
maketheassumptionthatthedifferenceincostinducedbydifferentstartingpositionswithinaregion
ismuchlessthanexpecteddifferenceincostofstartinginadifferentregion.Thisassumptionjustifies
usinganaveragecostforthelow-levelproblemstocreatethehigh-levelproblem.
HighlevelproblemweletJ∗=min (cid:82) J (π,x )p(x )betheaveragecostofeachlow-levelproblem.
ij π x0 ij 0 0
WeformaMarkovdecisionprocessbyintroducingabstractactionsa∈[K]:
p (a)=P(ζ =k|ζ =i,a=j,π∗) (28)
ik s+1 s ij
14 P.Collis,R.Singh,P.F.KinghornandC.L.Buckley
andletp betheassociateddistributionovertrajectoriesinducedbysomediscretestatefeedbackpolicy,
πd
alongwiththediscretestateactioncostc (a=j,ζ=i)=J∗ wemaywritethehighlevelproblem:
d ij
π∗=minJ (π,ζ ) (29)
d d 0
πd
S
(cid:88)
J (π,ζ )=Ep [ c (a ,ζ )] (30)
d 0 πd,ζ0 d s s
s=0
Our overall approximate control law is then given by choosing the action of the continuous controller
π (x) suggested by the discrete policy π (i(x)), or more concisely, π(x)=π (x), where i is
ij d i(x),π∗◦i(x)
d
calculatesthe discretelabel(MAPestimate) forthecontinuousstate x.Inthe nextsectionswedescribe
themethodsusedtosolvethehighandlowlevelproblems.
A.3 Offline Low Level Problems: Linear Quadratic Regulator (LQR)
Rather than solve the first-exit problem directly, we formulate an approximate problem by finding
trajectoriesthatendatspecific‘controlpriors’(seeA.6).Recallthelowlevelproblemgivenby:
π (x )=argminJ (π,x ,S) (31)
ij 0 ij 0
π,S
S
(cid:88)
J (π,x ,S)=E [ c(x ,u )] (32)
ij 0 π,x0 t t
t=0
s.t.(x ,u )∈H (33)
t t i
s.t.c(x,u)=0when(x,u)∈∂H (34)
ij
InordertoapproximatethisproblemwithonesolvablebyafinitehorizonLQRcontroller,we adopta
fixedgoalstate,x∗∈H .Imposingcostsc (x ,u )=uTRu andc (x ,u )=(x−x∗)Q (x−x∗).Formally
j t t t t t S S S f
wesolve,
π (x )=argminJ (π,x ,S) (35)
ij 0 ij 0
π,S
S−1
(cid:88)
J (π,x ,S)=E [(x −x∗)TQ (x −x∗)+ uTRu ] (36)
ij 0 π,x0 S f S t t
t=0
(37)
byintegratingthediscreteRicattiequation backwards.Numerically,wefoundoptimisingoverdifferent
time horizons made little difference to the solution, so we opted to instead specify a fixed horizon
(hyperparameter).Thesesolutionsarerecomputedofflineeverytimethelinearsystemmatriceschange.
Designing the cost matrices Insteadofimposingthestateconstraintsexplicitly,werecordahigh
costwhichinformsthediscretecontrollertoavoidthem.Inordertoapproximatetheconstrainedinputwe
chooseasuitablylargecontrolcostR=rI.Weadoptedthisapproachforthesakeofsimplicity,potentially
acceptingagooddealofsub-optimality.However,webelievemoreinvolvedmethodsforsolvinginput
constrainedLQRcouldbeusedinfuture,e.g.[3],especiallybecausewecomputethesesolutionsoffline.
LearninginHybridActiveInferenceModels 15
A.4 Active Inference Interpretation
Expected Free Energy Hereweexpressthefully-observedcontinuous(discretetime)activeinference
controller, without mean-field assumptions, and show it reduces to a continuous quadratic regulator.
Supposewehavealinearstatespacemodel:
x =Ax +Bu +ϵ (38)
t+1 t t t
and a prior preference over trajectories p˜(x )∼N(x ;x ,Q−1), active inference specifies the agent
1:T T f f
minimises
G(π)=E [−lnp˜(x ,u )+lnq(x ,u ;π)] (39)
q(x1:T,u1:T;π) 1:T 1:T 1:T 1:T
Note,sinceallstatesarefullyobservedwehave noambiguityterm.Wherep˜(x ,u )∝p˜(x )p(x |
1:T 1:T 1:T 1:T
u )p(u ), the central term is the dynamics model and the prior over controls is also gaussian,
1:T 1:T
p(u )= (cid:81) N(u ;0,R−1). Finally, we adopt q(x ,u ;π)=p(x |u ) (cid:81) π (u |x ), where we
1:T t t 1:T 1:T 1:T 1:T t t t t
parametrise the variational distributions as π ∼N(u ;K x,Σq) (where K ,Σ) are parameters to be
t t t t t t
optimised).Theexpectedfreeenergythussimplifiesto:
(cid:88)
G(π)=E [(x −x )TQ (x −x )− uT(R+Πu)u]+lndetΠ (40)
q(x1:T,u1:T;π) T F f T F t t
t
Dynamic Programming (HJB) Weproceedbydynamicprogramming,letthe‘value’functionbe
T
(cid:88)
V(x )=minE [(x −x )TQ (x −x )+ uT(R+Πu)u ]+lndetΠ (41)
k
π
q(xk+1:T,uk:T|xk,π) T f f T f t t t
t=k
Asusualthevaluefunctionsatisfiesarecursiveproperty:
V(x )=minE [uT(R+Πu)u +V(x )]+lndetΠ (42)
k
π
q(xk+1,uk|xk,π) k k k k+1
WeintroducetheansatzV(x )=xTS x leadingto,
k k k k
xTS x =minE [uT(R+Πu)u +xT S x ]+lndetΠ (43)
k k k
π
q(xk+1,uk|xk,π) k k k k+1 k+1 k+1
Finallywetakeexpectations,whichareavailableinclosedform,andsolveforΣ andK :
k k
xTS x = min xTKTRK x +tr(Σu(R+Πu)) (44)
k k k k k k k k t
Kk,Πk
+xT(A+BK )TS (A+BK )x +tr(ΣxS )+lndetΠ (45)
k k k+1 k k k k+1
SolvingforΣ andsubstituting,
k
Σq=(R+Πu)−1 (46)
k k
=⇒S =minKTRK +(A+BK )TS (A+BK ) (47)
k k k k k+1 k
Kk
K =−(R+BTS B)−1BTS A (48)
k K+1 K+1
WhereS followsthediscretealgebraicRiccatiequation(DARE).
k
Thus we recover π (u | x) ∼ N(K x,Σ ) where K is the traditional LQR gain, and Σ solves
t t k t t
Σ =(R+Π )−1.Hereweusethedeterministicmaximum-a-posterori‘MAP’controllerK x.Howeverthe
k k t
collectionofposteriorvarianceestimatesaddsadifferenttotalcostdependingonthevarianceinherent
inthedynamicswhichcanbeliftedtothediscretecontroller.
16 P.Collis,R.Singh,P.F.KinghornandC.L.Buckley
AsBeliefPropagation Adifferentperspectiveisasmessagepassing:wewishtocalculatethemarginals
p(x ) and p(x ,u ) tilted by the preference distribution p˜(x ) and control prior p(u) for this we can
k k k k
integratebackwardsusingtherecursiveformula
(cid:90)
b(x )= b(x ,u )dx (49)
k k k k
(cid:90)
b(x ,u )= p˜(x )p(x |x ,u )p(u )b(x )dx (50)
k k k k+1 k k k k+1 k+1
fromwhichwecanextractthecontrollawp(u |x )=b(x ,u )/b(x ).Toproceedweusethevariational
k k k k k
methodtomarginalise:
−lnb(x )=minE [−lnp˜(x ,u )p(x |x ,u )b(x )+lnq(x ,u |x )] (51)
k q k k k+1 k k k+1 k+1 k k
q
making the same assumption as above about variational distributions, and introducing the ansatz
b(x )∼N(x ;0,S )leadstothesameequationas43uptoirrelevantconstants.
k k k
A.5 Online high level problem
ThehighlevelproblemisadiscreteMDPwitha‘known’model,sotheusualRLtechniques(approximate
dynamicprogramming,policyiteration)apply.Here,howeverwechoosetouseamodel-basedalgorithm
witharecedinghorizoninspiredbyActiveInference,allowingustoeasilyincorporateexplorationbonuses.
Let the Bayesian MDP be given by M =(S,A,P ,R,P ) be the MDP, where p (s |s ,a ,θ)∼
B a θ a t+1 t t
Cat(θ )andp(θ )∼Dir(α).Weestimatetheopen-looprewardplusoptimisticinformation-theoretic
as as
explorationbonuses.
ActiveInferenceconversionWeadopttheActiveInferenceframeworkfordealingwithexploration.
Accordinglyweadoptthenotationlnp˜(s ,a )=R(s ,a )andrefertothis‘distribution’asthegoalprior
t t t t
[23],andoptimiseoveropenlooppoliciesπ=(a ,...,a ).
0 T
T
(cid:88)
G(a ,s )=E[ R(s ,a )+IG +IG |s ,a ] (52)
1:T 0 t t p s 0 1:T
t=0
whereparameterinformation-gainisgivenbyIG =D [p (θ)||p (θ)],withp (θ)=p(θ|s ).Inother
p KL t+1 t t 0:t
words, we add a bonus when we expect the posterior to diverge from the prior, which is exactly the
transitionswehaveobservedleast[19].
We also have a state information-gain term, IG =D [p (s )||p (s )]. In this case (fully
s KL t+1 t+1 t t+1
observed),p (s )=δ isaone-hotvector.LeavingthetermE [−lnp (s )]leadingtoamaximum
t+1 t+1 s t t t+1
entropyterm[19].
We calculate the above with Monte Carlo sampling which is possible due to the relatively small
number of modes. Local approximations such as Monte Carlo Tree Search could easily be integrated
inordertoscaleuptomorerealisticproblems.Alternatively,forrelativelystationaryenvironmentswe
couldinsteadadoptapproximatedynamicprogrammingmethodsformorehabitualactions.
A.6 Generating continuous control priors
Inorder togeneratecontrolpriors fortheLQR controller which correspondto each ofthediscrete states
wemustfindacontinuousstatex whichmaximisestheprobabilityofbeinginadesiredz:
i
LearninginHybridActiveInferenceModels 17
x =argmaxP(z=i|x,u) (53)
i
x
For this we perform a numerical optimisation in order to maximise this probability. Consider that
thisprobabilitydistributionP(z=i|x)isasoftmaxfunctionforthei-thclassisdefinedas:
exp(v)
σ(v)= i ,v =w ·x+r (54)
i (cid:80) exp(v ) i i i
j j
wherew isthei-throwoftheweightmatrix,xistheinputandr isthei-thbiasterm.Theupdate
i i
functionusedinthegradientdescentoptimisationcanbedescribedasfollows:
x←x+η∇ σ(v) (55)
x i
whereη isthelearningrateandthegradientofthesoftmaxfunctionwithrespecttotheinputvector
xisgivenby:
∂σ(v) ∂v
∇ σ(v)= i · =σ(v)(e −σ(v))·W (56)
x i ∂v ∂x i i
in which σ(v) is the vector of softmax probabilities, and e is the standard basis vector with 1 in
i
thei-th positionand0 elsewhere.The gradientdescentprocess continuesuntilthe probabilityP(z=i|x)
exceeds a specified threshold θ which we set to be 0.7. This threshold enforces a stopping criterion which
isrequiredforthecasesinwhichtheregionz isunbounded.
A.7 Model-free RL baselines
Table1:SummaryoftheSoftActor-CriticalgorithmwithmultipleQ-functions.
Component Input
Q-network 3×256×256×256×2
Policy network 2×256×256×256×2
Entropy regularization coeff 0.2
Learning rates (Qnet + Polnet) 3e-4
Batchsize 60
18 P.Collis,R.Singh,P.F.KinghornandC.L.Buckley
Table2:SummaryoftheActor-Criticalgorithm
Component Input
Feature Processing StandardScaler, RBF Kernels (4 × 100)
Value-network 4001 parameters (1 dense layer)
Policy network 802 parameters (2 dense layers)
Gamma 0.95
Lambda 1e-5
Learning rates (Policy + Value) 0.01
A.8 Model-based RL baseline
Table3:SummaryofDQN-MBEalgorithm[17]
Component Input
Q-network 1 hidden-layer, 48 units, ReLU
Dynamics Predictor Network (Fully Connected) 2 hidden-layers (each 24 Units), ReLU
ϵ minimum 0.01
ϵ decay 0.9995
Reward discount 0.99
Learning rates (Qnet / Dynamics-net) 0.05 / 0.02
Target Q-network update interval 8
Initial exploration only steps 10000
Minibatch size (Q-network) 16
Minibatch size (dynamics predictor network) 64
Number of recent states to fit probability model 50
LearninginHybridActiveInferenceModels 19
References
1. Abdulsamad, H., Peters, J.: Hierarchical decomposition of nonlinear dynamics and control for system
identificationandpolicydistillation.In:Bayen,A.M.,Jadbabaie,A.,Pappas,G.,Parrilo,P.A.,Recht,B.,
Tomlin,C.,Zeilinger,M.(eds.)Proceedingsofthe2ndConferenceonLearningforDynamicsandControl.
ProceedingsofMachineLearningResearch,vol.120,pp.904–914.PMLR(10–11Jun2020)
2. Abdulsamad,H.,Peters,J.:Model-basedreinforcementlearningviastochastichybridmodels.IEEEOpen
JournalofControlSystems 2,155–170(2023)
3. Bemporad,A.,Borrelli,F.,Morari,M.:Piecewiselinearoptimalcontrollersforhybridsystems.In:Proceedings
ofthe2000AmericanControlConference.ACC(IEEECat.No.00CH36334).vol.2,pp.1190–1194vol.2(2000)
4. Bemporad, A., Morari, M., Dua, V., Pistikopoulos, E.N.: The explicit linear quadratic regulator for
constrainedsystems.Automatica 38(1),3–20(2002)
5. Block,A.,Jadbabaie,A.,Pfrommer,D.,Simchowitz,M.,Tedrake,R.:Provableguaranteesforgenerative
behaviorcloning:Bridginglow-levelstabilityandhigh-levelbehavior(2023)
6. Borrelli,F.,Bemporad,A.,Fodor,M.,Hrovat,D.:Anmpc/hybridsystemapproachtotractioncontrol.IEEE
TransactionsonControlSystemsTechnology 14(3),541–552(2006)
7. Coulom,R.:Efficientselectivityandbackupoperatorsinmonte-carlotreesearch.In:vandenHerik,H.J.,
Ciancarini,P.,Donkers,H.H.L.M.J.(eds.)ComputersandGames.pp.72–83.SpringerBerlinHeidelberg,
Berlin,Heidelberg(2007)
8. DaCosta,L.,Parr,T.,Sajid,N.,Veselic,S.,Neacsu,V.,Friston,K.:Activeinferenceondiscretestate-spaces:
Asynthesis.JournalofMathematicalPsychology 99,102447(2020)
9. Daniel, C., van Hoof, H., Peters, J., Neumann, G.: Probabilistic inference for determining options in
reinforcementlearning.MachineLearning 104(2),337–357(Sep2016)
10. Dayan,P.,Hinton,G.E.:Feudalreinforcementlearning.In:Hanson,S.,Cowan,J.,Giles,C.(eds.)Advances
inNeuralInformationProcessingSystems.vol.5.Morgan-Kaufmann(1992)
11. Fox,E.,Sudderth,E.,Jordan,M.,Willsky,A.:Nonparametricbayesianlearningofswitchinglineardynamical
systems. In: Koller, D., Schuurmans, D., Bengio, Y., Bottou, L. (eds.) Advances in Neural Information
ProcessingSystems.vol.21.CurranAssociates,Inc.(2008)
12. Friston,K.,DaCosta,L.,Tschantz,A.,Kiefer,A.,Salvatori,T.,Neacsu,V.,Koudahl,M.,Heins,R.,Sajid,
N.,Markovic,D.,Parr,T.,Verbelen,T.,Buckley,C.:Supervisedstructurelearning(122023)
13. Friston,K.J.,Parr,T.,deVries,B.:Thegraphicalbrain:beliefpropagationandactiveinference.Network
neuroscience 1(4),381–414(2017)
14. Friston,K.J.,Sajid,N.,Quiroga-Martinez,D.R.,Parr,T.,Price,C.J.,Holmes,E.:Activelistening.Hearing
research 399,107998(2021)
15. Ghahramani,Z.,Hinton,G.E.:VariationalLearningforSwitchingState-SpaceModels.NeuralComputation
12(4),831–864(042000)
16. Gobet,F.,Lane,P.,Croker,S.,Cheng,P.,Jones,G.,Oliver,I.,Pine,J.:Chunkingmechanismsinhuman
learning.Trendsincognitivesciences 5,236–243(072001)
17. Gou, S.Z., Liu, Y.: DQN with model-based exploration: efficient learning on environments with sparse
rewards.CoRR abs/1903.09295 (2019)
18. Hafner,D.,Lee,K.H.,Fischer,I.,Abbeel,P.:Deephierarchicalplanningfrompixels(2022)
19. Heins, C., Millidge, B., Demekas, D., Klein, B., Friston, K., Couzin, I., Tschantz, A.: pymdp: A python
libraryforactiveinferenceindiscretestatespaces.arXivpreprintarXiv:2201.03904(2022)
20. Koudahl, M.T., Kouw, W.M., de Vries, B.: On Epistemics in Expected Free Energy for Linear Gaussian
StateSpaceModels.Entropy 23(12), 1565(Dec2021)
21. LaValle,S.M.:PlanningAlgorithms,chap.2.CambridgeUniversityPress,Cambridge(2006)
22. Linderman,S.W.,Miller,A.C.,Adams,R.P.,Blei,D.M.,Paninski,L.,Johnson,M.J.:Recurrentswitching
lineardynamicalsystems(2016)
23. Millidge, B., Tschantz, A., Seth, A.K., Buckley, C.L.: On the relationship between active inference and
controlasinference(2020)
20 P.Collis,R.Singh,P.F.KinghornandC.L.Buckley
24. Mnih,V.,Kavukcuoglu,K.,Silver,D.,Rusu,A.A.,Veness,J.,Bellemare,M.G.,Graves,A.,Riedmiller,M.A.,
Fidjeland,A.K.,Ostrovski,G.,Petersen,S.,Beattie,C.,Sadik,A.,Antonoglou,I.,King,H.,Kumaran,D.,
Wierstra,D.,Legg,S.,Hassabis,D.:Human-levelcontrolthroughdeepreinforcementlearning.Nature 518,
529–533(2015)
25. Murphy,K.P.:Machinelearning:aprobabilisticperspective.MITpress(2012)
26. Newell,A.,Simon,H.A.:HumanProblemSolving.Prentice-Hall,EnglewoodCliffs,NJ(1972)
27. OpenAI:Continuousmountaincarenvironment(2021),accessed:2024-05-25
28. Parr,T.,Pezzulo,G.,Friston,K.:ActiveInference:TheFreeEnergyPrincipleinMind,Brain,andBehavior.
MITPress(2022)
29. Parr, T., Friston, K.J.: The discrete and continuous brain: from decisions to movement—and back again.
Neuralcomputation 30(9),2319–2347(2018)
30. Parr, T., Friston, K.J.: The computational pharmacology of oculomotion. Psychopharmacology 236(8),
2473–2484(2019)
31. Priorelli,M.,Stoianov,I.P.:Hierarchicalhybridmodelingforflexibletooluse(2024)
32. Schwenzer,M.,Ay,M.,Bergs,T.,Abel,D.:Reviewonmodelpredictivecontrol:anengineeringperspective.
TheInternationalJournalofAdvancedManufacturingTechnology 117(5),1327–1349(Nov2021)
33. Sontag,E.:Nonlinearregulation:Thepiecewiselinearapproach.IEEETransactionsonAutomaticControl
26(2),346–358(1981)
34. Sutton,R.S.,Precup,D.,Singh,S.:Betweenmdpsandsemi-mdps:Aframeworkfortemporalabstraction
inreinforcementlearning.ArtificialIntelligence 112(1),181–211(1999)
35. Tessler, C., Givony, S., Zahavy, T., Mankowitz, D., Mannor, S.: A deep hierarchical approach to lifelong
learninginminecraft.ProceedingsoftheAAAIConferenceonArtificialIntelligence 31(1)(Feb2017)
36. Vezhnevets,A.S.,Osindero,S.,Schaul,T.,Heess,N.,Jaderberg,M.,Silver,D.,Kavukcuoglu,K.:FeUdal
networksforhierarchicalreinforcementlearning.In:Precup,D.,Teh,Y.W.(eds.)Proceedingsofthe34th
International Conference on Machine Learning. Proceedings of Machine Learning Research, vol. 70, pp.
3540–3549.PMLR(06–11Aug2017)
37. Zoltowski,D.M.,Pillow,J.W.,Linderman,S.W.:Unifyingandgeneralizingmodelsofneuraldynamicsduring
decision-making(2020)

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Learning in Hybrid Active Inference Models"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
