=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Bio-Inspired Artificial Neural Networks based on Predictive Coding
Citation Key: casnici2025bioinspired
Authors: Davide Casnici, Charlotte Frenkel, Justin Dauwels

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Key Terms: updates, predictive, neural, artificial, network, weight, networks, casnici, inspired, coding

=== FULL PAPER TEXT ===

Bio-Inspired Artificial Neural Networks based on Predictive
Coding
Davide Casnici, Charlotte Frenkel*, Justin Dauwels*
*Equal contributions
Backpropagation (BP) of errors is currently the backbone training algorithm for
artificial neural networks (ANNs). It works by updating the network weights through
gradient descent to minimize the value of a loss function, which represents the mismatch
between the network’s prediction and the desired output. BP relies on the chain rule
from calculus to propagate the loss gradient backward through the network’s hierarchy,
allowing each weight to be efficiently and precisely updated based on its contribution
to the output error. However, this process constrains the weight updates at every layer
to rely on a global error signal generated at the extreme of the hierarchy. In contrast,
the Hebbian model of synaptic plasticity in the brain states that weight updates should
be local, determined only by the activity of pre- and post-synaptic neurons. According
to Hebb’s model, it is therefore unlikely that biological brains directly implement BP.
Recently, an alternative algorithm for training ANNs called Predictive Coding (PC) is
gaining interest, appearing as a more biologically plausible alternative that updates the
network weights using only local information. Originating from P. Elias’s 1950s work on
signal compression [1], PC was later proposed in neuroscience as a model of the visual
This work has been submitted to the IEEE for possible publication.
Copyright may be transferred without notice, after which this version may no longer be accessible.
Davide Casnici, Charlotte Frenkel and Justin Dauwels are with the Microelectronics Depart-
ment (EEMCS Faculty), Delft University of Technology, 2628 CD Delft, Netherlands (e-mail:
d.casnici@tudelft.nl).
1
5202
guA
21
]LM.tats[
1v26780.8052:viXra
cortexbyRaoandBallard[2]. Successively,K.Fristonformalizeditunderthefreeenergy
principle (FEP) [3, 4], grounding PC within the frameworks of Bayesian inference and
dynamical systems. PC weight updates rely only on pre- and post-synaptic information,
eliminatingBP’sdependenceonaglobalerrorsignal. Moreover, ittheoreticallyprovides
features beyond those of standard BP, such as the ability to automatically scale each
gradient based on the associated uncertainty.
This Lecture Notes column offers a novel, tutorial-style introduction to PC, focus-
ing on its formulation, derivation, and connections to well-established optimization and
signal processing algorithms such as BP and the Kalman Filter (KF). It aims to pro-
vide accessible support to the existing literature, guiding readers from the mathematical
foundations underlying PC to its practical implementation, including computational ex-
amples in Python using the PyTorch framework.
Relevance
While BP has enabled significant advances in training large-scale models, it lacks bio-
logical plausibility. Weight updates in early layers of the network rely on error signals
generated far away in the hierarchy, violating locality principle central to Hebbian plas-
ticity. PC offers a biologically plausible alternative, enabling parameter updates using
only locally available information. Notably, recent studies have shown that, under cer-
tain conditions, PC can approximate or even exactly match BP gradients [5, 6, 7].
Furthermore, PC has been framed as Bayesian filtering and shown to approximate the
closed-form Kalman Filter (KF) solution when recurrent connections are included [8, 9].
These properties make PC a promising algorithm for next-generation biologically plausi-
ble ANNs, linking biological plausibility to already well-established artificial intelligence
(AI) and signal processing algorithms.
In this Lecture Notes column, we will guide step-by-step readers through the mathe-
2
maticalfoundationsofPC,providingclearandintuitiveexplanationsofallrequiredtools
and key concepts. We will also briefly cover its relations with BP and KF algorithms,
and conclude by discussing some trade-offs with a computational example.
Prerequisites
The content of this Lecture Notes column is meant to be as self-contained as possible;
however,readersmustbefamiliarwithcalculus,linearalgebra,probability,andBayesian
statistics. A basic knowledge of variational inference and KF is helpful but not essential.
Problem statement and solution
Problem statement
A key question in the fields of neuroscience and AI is how the brain processes and makes
sense of sensory signals, giving rise to perceptions. When sensing some stimuli, the
brain has to infer and interpret the physical setting in the real world that has produced
them, resulting in what we experience as perception. This task is particularly complex
when considering that synaptic changes in the brain rely exclusively on local plasticity.
Therefore, how could this perception problem be solved by relying only on local learning
rules, and what kind of connections might the resulting mathematical model share with
AI and system identification algorithms?
Predictive Coding approach
Helmholtz’s principle states that when there is a significant deviation from random-
ness, structure becomes apparent to us, and perception is thus an unconscious inference
process about statistical regularities of sensory stimuli. PC suggests that the brain rep-
resents these statistical regularities through neural activity, aiming to predict and infer
3
the causes of unforeseen stimuli. The brain is therefore assumed to employ a generative
model of the world, and to infer the causes of sensory stimuli by inverting the generative
process [4]. Here, “inversion” refers to the process of inferring causes from stimuli rather
than generating stimuli from causes. These causes are encoded as neural activity at
varying abstraction levels, such as edges or more complex shapes. For example, while
reading this manuscript, the brain infers the most likely physical state of the world that
causesthelightdetectedbytheeyes(theblackshapesoflettersonawhitebackground).
Its predictions are compared with the actual sensory input, and any mismatch generates
error feedback that suppresses inaccurate beliefs. This results in a final neural activity
pattern that we experience as the visual perception of this manuscript. Therefore, PC
suggests that the visual cortex actively predicts sensory inputs rather than passively
processing them. Indeed, PC networks are characterized by top-down prediction signals
that predict sensory inputs and bottom-up error signals that transmit the residuals of
these predictions, conveying only the unpredicted information [2]. Having summarized
the core idea behind PC, we now introduce the key mathematical concepts essential for
the later sections.
Information Measures
Inthissection,wewillcoverthebasicmathematicalconceptsandtoolsfrominformation
theory (IT) that will be the bedrock for later derivations and understanding of PC.
GivenarandomvariableX definedoveradiscretesupportX = {x ,...,x },wemay
1 n
beinterestedinquantifyingthesurprisal, orinformationcontent, I(x)ofitsrealizations.
According to Shannon’s definition [10], the information content of a realization x ∈ X
of a random variable X with probability mass function p(x), where X ∼ p(x) and
p : X → [0,1], is given by
I(x) ≜ −log p(x), (1)
2
4
where p(x) denotes the probability of X taking the specific value x. While it is usually
denotedasP(X = x), forthesakeofbrevitywewillusethesimplifiednotationthrough-
out this manuscript. The I(·) operator allows us to quantify the expected information
content of a random variable, referred to as Shannon’s entropy, defined as
(cid:88)
H(X) ≜ E[I(x)] = − p(x)log p(x). (2)
2
x∈X
Its unit depends on the logarithm’s base; with base 2, it is measured in bits. Using
the natural logarithm, as we will later, quantifies information in nats, where 1nat =
log ebits. The entropy of a random variable can be interpreted as a measure of the
2
uncertainty of its outcomes, or equivalently, as the minimum average number of bits
needed to encode a sample from the random variable’s distribution losslessly. As shown
in the left pane of Figure 1, the more peaked a distribution is around some values, the
lower its entropy, and vice versa, since most likely outcomes can be encoded with fewer
bits than unlikely ones. For a continuous random variable distributed according to a
probability density function X ∼ f(x), x ∈ X ⊆ R, the summation in (1) is replaced
by an integral and the probability mass function is replaced by the probability density
function; thus, the entropy is defined as
(cid:90)
H(X) ≜ E[I(X)] = − f(x)log f(x) dx. (3)
2
X
It is important to highlight that in the continuous case, Shannon’s entropy is no longer
rigorously defined, and it loses its original interpretation as an information measure. In
the discrete case, it is a non-negative quantity representing the expected information
content of a random variable. However, in the continuous case, entropy can result in
negative values, and this interpretation no longer holds [10]. Through the manuscript,
the entropy of a random variable X ∼ p(x) will be denoted as H(p).
Another quantity of interest is the cross entropy. It measures the average number
5
of bits needed to encode the outcomes of a random variable distributed according to
a reference probability distribution p, under another arbitrary probability distribution
q. Intuitively, it has high values when events that are likely according to the true
distribution p are assigned a low probability under the distribution q, as shown by the
right pane of Figure 1. That is, the cross entropy between two probability distributions
p and q defined over the same discrete support X = {x ,...,x } is defined as
1 n
(cid:88)
H(p,q) ≜ −E [log q(x)] = − p(x)log q(x). (4)
p 2 2
x∈X
The cross entropy can also be expressed in terms of the reference probability distribu-
tion’s entropy added to the Kullback–Leibler divergence (D ) between the reference
KL
and arbitrary probability distributions:
H(p,q) = H(p)+D (p∥q). (5)
KL
The D is a measure of dissimilarity between an arbitrary probability distribution and
KL
a reference distribution, defined in the discrete case as
(cid:18) (cid:19)
(cid:88) p(x)
D (p∥q) ≜ p(x)log . (6)
KL 2 q(x)
x∈X
Both cross entropy and D are asymmetric measures, that is D (q||p) ̸= D (p||q)
KL KL KL
and H(p,q) ̸= H(q,p). For continuous random variables, the summation is replaced by
an integral in both definitions. From the right pane in Figure 1, we can observe that
as the reference distribution and the arbitrary distribution become more similar, their
cross entropy decreases, and vice versa.
Now that we have introduced the essential tools from IT, we will cover how they
connect to the variational inference (VI) framework, the backbone of PC.
6
Figure 1: The left pane illustrates how the shape of a Gaussian distribution varies with
its entropy, providing an intuitive understanding of this information-theoretic concept.
TherightpanecomparesfourcolouredGaussiandistributionstoadashed-linereference;
the closer their means and variances match those of the reference distribution, the lower
the cross-entropy, and vice versa.
Variational Inference
In this section, we will cover VI, a technique that allows us to work with complex or
intractable probability distributions by approximating them with arbitrary distributions
that we can work with.
When working with Bayesian inference, it is common to encounter integrals that are
too complex to be solved both numerically and analytically. For instance, let us suppose
wearegivenanobservations ∈ S ⊆ Rrandomlygeneratedbyastatisticalmodelp(s;θ),
parametrized by some parameter θ ∈ Θ ⊆ R. Assuming this generative model has some
latent variable x ∈ X ⊆ R that influences our observation s, with this relationship
governed by the model parameter, we can represent the joint distribution as p(s,x;θ).
Inthiscase,wemightbeinterestedininferringthemostlikelyvalueofthelatentvariable
x given the observed data s and the model parameter θ. Using Bayes’ theorem, we can
compute the posterior distribution of the latent variable given the observed data as
7
p(s|x;θ)p(x)
p(x|s;θ) = , (7)
p(s;θ)
where p(s;θ), known as the model evidence or marginal likelihood, is the normalizing
constant obtained by marginalizing over all possible values of x. However, when dealing
with high-dimensional multivariate distributions with a large number of latent variables
involved,computingthemodelevidenceoftenbecomesintractable. Ifweassumealatent
variable x to be a high-dimensional array of parameters, x ∈ Xm ⊆ Rm, (7) is written
as
p(s|x;θ)p(x)
p(x|s;θ) = , (8)
(cid:82) (cid:82)
··· p(s|x;θ)p(x)dx ...dx
X X 1 m
where computing the integral in the denominator may require an intractable amount of
computation, making it unfeasible to evaluate the posterior distribution p(x|s;θ). The
core idea of VI is to approximate the intractable posterior distribution with an arbitrary
tractable distribution. The latter is referred to as the variational posterior, denoted by
q. To allow for some degrees of flexibility, it is usually parametrized by some parameters
ϕ ∈ Φ ⊆ Rm, that can be learnt. To quantify the difference between the posterior and
variational distributions p and q, we use the previously defined D :
KL
(cid:90) (cid:18) (cid:19)
q(x;ϕ)
D (q∥p) ≜ q(x;ϕ)ln dx, (9)
KL
p(x|s;θ)
X
where we avoid expanding the integral as in (8) for brevity. The top-right pane of
Figure 2 visually illustrates (9), with the red area representing the integrand of the
D . To minimize the difference between the two distributions, we can modify the
KL
parameters of our variational posterior according to the D gradient, but this would
KL
still require computing the intractable posterior. To overcome this, we can substitute
(7) into (9) obtaining:
8
(cid:90) (cid:18) (cid:19)
q(x;ϕ)
D (q∥p) = q(x;ϕ)ln p(s;θ) dx. (10)
KL
p(s|x;θ)p(x)
X
Continuingbyapplyingthelogarithmssumrule,wecandevelop(10)asfollowstoobtain
a more familiar equivalent form:
(cid:90) (cid:18) (cid:19) (cid:90)
q(x;ϕ)
D (q∥p) = q(x;ϕ)ln dx+lnp(s;θ) q(x;ϕ)dx
KL
p(s,x;θ)
X X (11)
(cid:90) (cid:90)
= q(x;ϕ)lnq(x;ϕ)dx− q(x;ϕ)lnp(s,x;θ)dx+lnp(s;θ).
X X
This decomposition of the D is sometimes referred to as the energy-entropy decom-
KL
position, due to its similarity with Helmholtz’s free energy in statistical physics. By
looking closer at (11), we can easily recognize two familiar terms: the cross entropy and
the entropy. Readers may find the term cross entropy referred to as the ‘energy’ in the
FEP or PC literature. Proceeding by applying definitions in (3) and (4), we can write
(11) as
D (q∥p) = −H(q)+H(q,p)+lnp(s;θ). (12)
KL
Since we wish to minimize the divergence between the distributions by making the
variationalposteriorfitthetrueposteriorascloselyaspossible, wecomputethegradient
of D with respect to ϕ, obtaining:
KL
(cid:8)(cid:8)
∂H(q) ∂H(q,p) ∂lnp((cid:8)s;θ)
∇ D (q∥p) = − + + (cid:8) , (13)
ϕ KL (cid:8)
∂ϕ ∂ϕ (cid:8) ∂ϕ
since p(s;θ) does not depend on ϕ and therefore its gradient is the null vector. In the
case where s is a discrete random variable, the following inequality holds:
D (q∥p) = −H(q)+H(q,p)+lnp(s;θ) ≤ −H(q)+H(q,p)
KL
(14)
≤ F,
9
where F = H(q,p)−H(q) is called variational free energy, or simply free energy (FE).
Since the D is a non-negative quantity, and lnp(s;θ) ≤ 0 ∀s ∈ S = {s ,...,s }, it
KL 1 n
follows that the D is always upper-bounded by the FE. This establishes a tractable
KL
bound on an otherwise intractable quantity, thereby allowing an intractable inference
problem to be addressed as a tractable optimization one. This relationship is illustrated
in the bottom-left pane of Figure 2. However, it is important to note that when s is
continuous, this property no longer holds. This is because density functions can take
values greater than one, resulting in positive logarithm values. However, (13) holds
regardless when dealing with continuous random variables, still allowing us to optimize
the intractable distance measure.
Another property of FE is that, when its sign is negated, it provides a bound on
the model’s evidence. The negative free energy (NFE) is therefore also known as the
evidence lower bound (ELBO), a common terminology in the field of ML and statistics.
This can be shown by evaluating the evidence logarithm
(cid:18)(cid:90) (cid:19)
q(x;ϕ)
lnp(s;θ) = ln p(s,x;θ)dx , (15)
q(x;ϕ)
X
wherewehavemultipliedanddividedbythesamequantityq(x;ϕ). ByapplyingJensen’s
inequality, we can write
(cid:90) (cid:18) (cid:19)
p(s,x;θ)
ln(p(s;θ)) ≥ q(x;ϕ)ln dx
q(x;ϕ)
X
(cid:90) (cid:90)
≥ q(x;ϕ)lnp(s,x;θ)dx− q(x;ϕ)lnq(x;ϕ)dx (16)
X X
≥ F˜,
where F˜ is the NFE. As shown in Figure 2, we can now appreciate how the (N)FE
simultaneously bounds two essential but intractable quantities, D and the model log
KL
evidence, thereby recasting an intractable inference problem into a tractable optimiza-
10
tion problem. Indeed, optimizing the (N)FE with respect to the variational posterior
parameters yields an approximation of the true posterior, which can then be used as a
proxy to update the model parameters and reduce surprise on the observation. This al-
ternating procedure of optimizing the variational posterior and updating the generative
parameters corresponds to the Expectation-Maximization (EM) algorithm: in the first
phase, the approximate posterior over the latent variables is refined for fixed generative
parameters; in the second phase, the generative parameters are updated based on the
current posterior estimate. Repeatedly alternating these phases iteratively improves the
estimates until convergence is reached.
11
Figure2: Thetopleftpaneillustratesthedistributionofarandomvariableconditioned
onalatentone,bothfollowingGaussiandistributions. Thetoprightillustratesinredthe
area representing D between the real posterior (given observation s) and a Gaussian
KL
variational posterior. The bottom left compares D and the NFE across variational
KL
parameters, showing NFE as a lower bound. The bottom right displays model surprise
(log-evidence) versus the NFE, showing how the latter acts as a lower bound also for
the log-evidence.
Predictive Coding Network
Withthenecessarymathematicalbackgroundintroduced,wecannowproceedtodevelop
a PC network. This section is divided into three subsections. The first one covers the
modelspecification, fittingintheprecedentformulastheactualmodelsthatcharacterize
PC. The two subsequent subsections will focus on deriving the update rules for the
variational and generative parameters, respectively.
12
Model specification
In this subsection, we replace the earlier abstract framework with concrete functional
forms to fully specify the model. Specifically, we define a Gaussian generative model
and approximate the posterior distribution using a Dirac delta variational posterior.
All modelling assumptions are summarized in Box 1. We conclude with the resulting
expression for the NFE under these assumptions.
Box 1 - Summary of the Main Assumptions in Predictive Coding
• Gaussian Generative Model: Neurons are assumed to be continuous
random variables distributed according to a Gaussian density.
• Hierarchical Generative Structure: ThemeanoftheGaussiandensities
at each level is a function parametrized by neurons in the layer above in the
hierarchy and by the synapses connecting them.
• Gaussian Variational Posterior: The variational posterior distribution
for each neuron is assumed to be Gaussian. In this work, we consider the
limiting case where it reduces to a Dirac delta distribution.
• Mean Field Approximation: All the variational factors are assumed to
be independent.
To account for the hierarchical structure of the visual cortex and the complexity of
real-world stimuli, we need to extend the generative model introduced in the numerator
of (7) to a hierarchical formulation with high-dimensional latent variables and stimuli.
We define x ℓ ∈ Rd ℓ as the d ℓ -dimensional vector of latent variables representing the
brain’s neural activity encoding the most likely causes of sensory stimuli at layer ℓ,
with s ∈ Rk denoting the input sensed at the bottom layer of the hierarchy. The
conditional distributions are now parametrized by parameter matrices Θ ℓ ∈ Rd ℓ−1 ×d ℓ,
13
representing the synaptic connections between neurons and providing the model with
greater flexibility to learn and refine the relationships between latent causes and sensory
observations. Thus, the generative model is defined as
L−1
(cid:89)
p(x ;Θ ) = p(x ) p(x | x ;Θ ), (17)
L:0 L:1 L ℓ ℓ+1 ℓ+1
ℓ=0
where the latent variables at each layer are conditionally dependent on those from the
layer above, with x representing the prior belief at the top, and x = s denoting the
L 0
sensory stimulus at the bottom of the hierarchy. PC assumes Gaussian latent variables,
resulting in the following generative model:
L−1
(cid:89)
p(x ;Θ ) = N(x ;µ¯,Σ¯) N(x ;µ ,Σ ). (18)
L:0 L:1 L ℓ ℓ ℓ
ℓ=0
Each Gaussian’s mean is given by a parametrized function µ = f (x ), with
ℓ Θ ℓ+1 ℓ+1
f Θ : Rd ℓ+1 → Rd ℓ representing the forward mapping from layer ℓ + 1 to layer ℓ.
ℓ+1
Note that the generative model is defined as a forward model with indices decreasing
from top to bottom layers, following the indexing convention inspired by the visual
cortex in PC literature [2, 3, 4], which contrasts the standard increasing layer indexing
used in conventional ANNs literature. Successively, we define the variational posterior
q(x ;ϕ ),whichweassumetobecomposedofvariationalfactorsthatareindependent
L:0 L:0
from each other, taking the form:
L
(cid:89)
q(x ;ϕ ) = q(x ;ϕ ). (19)
L:0 L:0 ℓ ℓ
ℓ=0
Each variational factor is parametrized by some parameters ϕ
ℓ
∈ Rd ℓ, providing the
necessary degrees of freedom to better approximate the true posterior. As we will see
later, depending on the settings, the latent variables in the first and/or last layers are
fixed to specified values (inputs and targets) and therefore do not require inference.
14
The variational posterior is often modelled as a Gaussian distribution in the FEP
and PC literature. While the resulting maths is more involved [4], a limiting case using
a Dirac delta variational posterior simplifies the derivation and yields nearly identical
update rules in practice. The Dirac delta function can be seen as the limit of a Gaussian
as its variance approaches zero:
δ(x−µ) = lim √
1 e−(x
2
−
σ
µ
2
)2
. (20)
σ→0 2πσ
The motivation behind this choice is that we are mainly interested in the modes of
the posterior distribution, i.e., the most likely values for the causes given the stimuli.
Using a Gaussian variational posterior requires a Taylor’s expansion around the mode of
the log-joint lnp(s,x;Θ) in the NFE’s cross entropy term, making the assumption that
the log-joint is tightly peaked around the mode. Using the Dirac delta, we assume no
variance in the variational posterior, resulting in a deterministic approximation of the
log-joint modes. This allows us to avoid Taylor expansion since the expected value of a
function under a Dirac distribution centred at µ equals the function evaluated at that
concentration point:
(cid:90)
E [f(x)] = δ(x−µ)f(x)dx = f(µ). (21)
δ
χ
This holds in the multivariate case x,µ ∈ Rd as well:
(cid:90) d
(cid:89)
E [f(x)] = δ(x −µ )f(x)dx = f(µ) (22)
δ i i
χ
i=1
where again the multiple integral has not been expanded for brevity of notation. We
consider a Dirac delta distribution parametrized by ϕ ∈ Rd ℓ, with its density centred at
the modes of the underlying generative model. This can be expressed as δ(x ;ϕ ), where
ℓ ℓ
15
ϕ = µ . Consequently, the joint distribution is defined as
ℓ ℓ
L
(cid:89)
q(x ;ϕ ) = δ(x −ϕ ). (23)
L:0 L:0 ℓ ℓ
ℓ=0
BeforefittingourmodelsintotheNFE,wemustaddresstheentropyterm. Inthediscrete
case, the Dirac delta’s entropy is well-behaved and equals zero, but in the continuous
case, the differential entropy tends to −∞. However, it can be considered as a constant
andthusstilldisregardedintheoptimizationprocess. Consideringonlythecrossentropy
term, we then have
F˜ = −H(q,p)+(cid:8)H(cid:8)(q (cid:8) )
(cid:90) L (cid:32) L−1 (cid:33)
(cid:89) (cid:89)
= q(x ;ϕ )ln p(x ) p(x |x ;Θ ) dx
ℓ ℓ L ℓ ℓ+1 ℓ+1 (24)
X
ℓ=0 ℓ=0
(cid:90) L (cid:32) L−1 (cid:33)
(cid:89) (cid:89)
= δ(x −ϕ )ln N(x ;µ¯,Σ¯) N(x ;µ ,Σ ) dx,
ℓ ℓ L ℓ ℓ ℓ
X
ℓ=0 ℓ=0
and by applying the logarithms sum property we obtain
(cid:90) L (cid:34) L−1 (cid:35)
F˜ = (cid:89) δ(x −ϕ ) ln (cid:0) N(x ;µ¯,Σ¯) (cid:1) + (cid:88) ln(N(x ;µ ,Σ )) dx. (25)
ℓ ℓ L ℓ ℓ ℓ
X
ℓ=0 ℓ=0
We can now apply the Dirac delta’s property described in (22), obtaining:
L
(cid:88)
F˜ = ln(N(ϕ ;µ ,Σ )), (26)
ℓ ℓ ℓ
ℓ=0
resulting in a sum of Gaussians evaluated at the Dirac’s center of mass, where µ = µ¯
L
and Σ = Σ¯. Proceeding with expanding the NFE, we have
L
16
L
F˜ = (cid:88) − d ℓ ln(2π)− 1 ln|Σ |− 1 (ϕ −µ )TΣ−1(ϕ −µ )
2 2 ℓ 2 ℓ ℓ ℓ ℓ ℓ
ℓ=0
(27)
(cid:34) L (cid:35)
1 (cid:88)
≈ − ln|Σ |+(ϕ −µ )TΣ−1(ϕ −µ ) ,
2 ℓ ℓ ℓ ℓ ℓ ℓ
ℓ=0
where the constants have been discarded, as they do not depend on the parameters and
thus do not affect the optimization process. Crucially, the objective function consists
onlyoflayer-wiseprecision-weightederrortermsbetweenthevariationalposteriorvalues
andthegenerativemodelpredictions,expressedasµ = f (ϕ ),followingfrom(22).
ℓ Θ ℓ+1 ℓ+1
Variational parameters optimization
In this subsection, we focus on optimizing the NFE with respect to the variational
parameters to approximate (the modes of) the true posterior distribution. We also show
how this optimization results in local update rules, enabling the neural activity to be
updated using only information from adjacent layers.
PCdistinguishesbetweentwotypesofneurons: errorneuronsandvalueneurons. The
former, represented as red triangles in the right pane of Figure 3, encode the precision-
weighted prediction errors at each layer. The latter, represented as coloured circles,
encode the modes of the approximated posterior distribution. The network shown in
Figure 3 illustrates the generative setting, where the input stimuli are clamped to the
value neurons at the lowest layer, which typically corresponds to the output layer of a
neural network. The highest layer, which usually corresponds to the input layer, is left
free to converge to the most likely configuration whose forward pass would produce the
clamped stimuli given the current network parameters.
Buildingonthisframework,wedefinetheerrornodesatalayerℓasϵ ≜ Σ−1(ϕ −µ ),
ℓ ℓ ℓ ℓ
and the layer-wise energy as ξ ≜ (ϕ −µ )TΣ−1(ϕ −µ ). Using these definitions, we
ℓ ℓ ℓ ℓ ℓ ℓ
can express the gradient of the NFE with respect to ϕ as follows:
ℓ
17
Figure 3: The left pane illustrates PC’s probabilistic graphical model, showing hierar-
chical dependencies between variables. The model’s prediction, based on the most likely
value for each variable, corresponds to a standard neural network forward pass. At the
hierarchy’s bottom, sensed stimuli are compared to predictions, generating error signals.
During neural activity optimization, the lowest layer is clamped to the stimuli, while the
other layers update to match these stimuli according to the generative model’s forward
pass, as illustrated in the right pane. Red triangles denote error nodes, which receive
signals from circle-shaped value neurons from the layer below and from the layer above.
The figure assumes the covariance matrix is fixed to the identity and is therefore omit-
ted.
∂F˜ 1 (cid:34) ∂ (cid:88) L (cid:35)
= − ln|Σ |+(ϕ −µ )TΣ−1(ϕ −µ )
∂ϕ 2 ∂ϕ ℓ ℓ ℓ ℓ ℓ ℓ
ℓ ℓ
ℓ=0 (28)
(cid:18) (cid:19)
1 ∂ξ ∂ξ
ℓ ℓ−1
= − + ,
2 ∂ϕ ∂ϕ
ℓ ℓ
as ϕ affects only layers ℓ and ℓ−1. We proceed by computing the gradient of the two
ℓ
energy terms separately, starting from the first one:
∂ξ ∂
ℓ = (ϕ −f (ϕ ))TΣ−1(ϕ −f (ϕ )). (29)
∂ϕ ∂ϕ ℓ Θ ℓ+1 ℓ+1 ℓ ℓ Θ ℓ+1 ℓ+1
ℓ ℓ
Since the inverse covariance matrix is symmetric, we can apply the following quadratic
form derivative rule from matrix calculus:
18
∂
(x−s)T W(x−s) = 2W(x−s), (30)
∂x
resulting in
∂ξ
ℓ = 2Σ−1(ϕ −f (ϕ )) = 2ϵ , (31)
∂ϕ ℓ ℓ Θ ℓ+1 ℓ+1 ℓ
ℓ
where we have applied the error node definition. To compute the gradient of the second
energy term, we use another derivative rule for the quadratic form:
∂
(x−s)T W(x−s) = −2W(x−s). (32)
∂s
By applying it, we obtain
∂ξ ∂
ℓ−1 = (ϕ −f (ϕ ))TΣ−1 (ϕ −f (ϕ ))
∂ϕ ∂ϕ ℓ−1 Θ ℓ ℓ ℓ−1 ℓ−1 Θ ℓ ℓ
ℓ ℓ
(33)
(cid:18)
∂
(cid:19)T
= −2 f (ϕ ) ϵ .
∂ϕ Θ ℓ ℓ ℓ−1
ℓ
Now we can substitute (31) and (33) inside (28), obtaining:
∂F˜ (cid:18) ∂ (cid:19)T
= f (ϕ ) ϵ −ϵ , (34)
∂ϕ ∂ϕ Θ ℓ ℓ ℓ−1 ℓ
ℓ ℓ
where f Θ ℓ (ϕ ℓ ) : Rd ℓ → Rd ℓ−1, and the Jacobian of f, denoted by ∂ ∂ ϕ ℓ f Θ ℓ (ϕ ℓ ), belongs to
Rd ℓ−1 ×d ℓ, consistently with ϵ ℓ−1 ∈ Rd ℓ−1 and ϵ ℓ ∈ Rd ℓ. During the NFE optimization
with respect to the variational parameters, called the inference phase, the lowest and/or
highest layers are typically clamped to the stimuli, the observation, or both, depending
onthetask. Forexample,inthegenerativesettingillustratedinFigure3,onlythelowest
layers are clamped. In the classification setting, both the highest and lowest layers are
clamped to the input stimuli and output labels, respectively. Consequently, their values
are known and do not require inference. The variational parameters can be modelled
19
either as a discrete-time system with ∆t = 1, or as a continuous-time dynamical system,
i.e., dϕ ℓ ∝ ∂F˜ , resulting in a set of differential equations that can be solved using an
dt ∂ϕ
ℓ
Euler scheme:
∂F˜
ϕ (t+∆t) = ϕ (t)+α∆t , (35)
ℓ ℓ
∂ϕ
ℓ
where ∆t defines the time resolution, and α is the learning rate, which scales the step
size (the latter can be incorporated in the former, and vice versa). Now, by looking
at the right pane of Figure 3 and (34), we can understand how the dynamics of the
variational parameters depend on inhibitory feedforward signals from the error nodes
above and from excitatory feedback signals from the error nodes below, where the latter
carry information about the wrong information in the prediction. This optimization
should be interpreted as climbing the dashed black function in the bottom-left pane of
Figure 2, and is run until convergence, where we have
ϕ∗ = argmaxF˜. (36)
ϕ
The process of optimizing the NFE with respect to neural activity is illustrated in the
top row of Figure 4, with the final approximation shown in the bottom-right pane. Note
that a Gaussian variational posterior is used there to illustrate the general PC scenario.
When a Dirac delta is used instead, the purple distribution collapses to a point mass,
and the NFE is maximized as this point converges to the mode of the true posterior.
Generative parameters optimization
In this subsection, we focus on optimizing the NFE with respect to the generative pa-
rameters, usingthevariationalposteriorapproximationasaproxyforthetrueposterior.
We will also demonstrate how these updates are biologically plausible by relying solely
on locally available information.
20
The process of updating the generative parameters to improve the network’s pre-
dictions is referred to as the learning phase. It requires evaluating the gradient of the
NFE with respect to Θ , for which we adopt the notation f (Θ ) instead of f (ϕ ) to
ℓ ϕ ℓ ℓ Θ ℓ ℓ
emphasize that Θ is the variable of interest. This results in:
∂F˜ 1 (cid:34) ∂ (cid:88) L (cid:35)
= − ln|Σ |+ξ
ℓ ℓ
∂Θ 2 ∂Θ
ℓ ℓ
ℓ=0 (37)
(cid:20) (cid:21)
1 ∂
= − (ϕ −f (Θ ))TΣ−1 (ϕ −f (Θ )) .
2 ∂Θ ℓ−1 ϕ ℓ ℓ ℓ−1 ℓ−1 ϕ ℓ ℓ
ℓ
As Σ−1 is symmetric, we can apply the chain rule of calculus and (32), obtaining:
∂F˜ 1 (cid:20) (cid:18) ∂ (cid:19)(cid:21)
= − −2Σ−1 (ϕ −f (Θ )) f (Θ )
∂Θ 2 ℓ−1 ℓ−1 ϕ ℓ ℓ ∂Θ ϕ ℓ ℓ
ℓ ℓ (38)
= f′ (Θ )⊙ϵ ϕT,
ϕ ℓ ℓ−1 ℓ
ℓ
assuming f is a linear transformation followed by a nonlinearity, i.e., f (Θ ) = f(Θ ϕ ).
ϕ ℓ ℓ ℓ
ℓ
As with the variational parameters, we have f ϕ ℓ (Θ ℓ ) : Rd ℓ → Rd ℓ−1, with f ϕ ′ ℓ (Θ ℓ ),ϵ ℓ−1 ∈
Rd ℓ−1 and ϕ⊤ ∈ R1×d ℓ.
ℓ
To optimize (27), the generative parameters can be updated proportionally to this
derivative, using the optimal values ϕ∗ found for the variational parameters as a proxy
fortherealmaximumaposteriorivalue. PCalternatesbetweenupdatingthevariational
parameters to approximate the true posterior given the current generative parameters,
and then using these variational parameters as a proxy for the posterior to optimize the
generative parameters. This exactly follows the EM algorithm described at the end of
the VI section of this manuscript. However, unlike the standard EM algorithm, only a
small gradient step ∆Θ is taken during generative parameters optimization, to avoid
ℓ
overfitting a specific set of stimuli:
(cid:12)
Θ ℓ (t+∆t) = Θ ℓ (t)+η
∂ F˜(cid:12)
(cid:12) , (39)
∂Θ (cid:12)
ℓ ϕ=ϕ∗
21
where η is the learning rate of the generative parameters. As a result, unexpected
but rare inputs caused by noise have a weak impact on the generative parameters, while
consistent statistical regularities produce stronger parameter updates. As for the neural
activity, alternatively to a discrete gradient step, the generative parameters can also be
formulated as a continuous-time system. In this case, the generative parameters evolve
(cid:12)
over a much shorter period of time according to dΘ ℓ ∝ ∂ F˜(cid:12) (cid:12) . This perspective
dt ∂Θ ℓ (cid:12)
ϕ=ϕ∗
enforces a separation of timescales, where the variational parameters can be interpreted
asrapidneuralactivitychanges,whilethegenerativeparametersundergoslowersynaptic
adjustments. Limiting convergence to a short period is crucial to prevent overfitting,
similar to the role of a small learning rate in the discrete case.
The generative model’s covariance matrices Σ enable PC networks to modulate
ℓ
gradients based on the associated uncertainty. Although this modulation of errors can
beseenasanadvantageofPCoverBP,itremainsarelativelyunexploredresearchtopic.
Furthermore, learning the covariance matrices involves two challenges arising from the
necessity of computing their inverse during gradient evaluation. First, computing the
inverse requires a neuron to access information from neurons to which it is not directly
connected, such as the variance of other units, which violates the principle of local
computation. Second, if some components of the covariance matrix tend to zero, their
inverse diverges, potentially causing the gradient to explode and leading to numerical
instability during training. For instance, in the case of a diagonal covariance matrix, the
inverse has entries 1 on the diagonal; therefore, if neurons exhibit variance approaching
σ2
zero, the inverse will diverge, as the gradients computed in (34) and (38).
Nevertheless, for the sake of completeness, we include the derivation of a possible
learning rule for Σ based on the NFE gradient, analogous to those for ϕ and Θ
ℓ ℓ ℓ
22
Figure 4: The figures summarize the steps involved in PC. The upper-left pane shows
a prediction error when the perceived signal differs from the brain’s prediction. Due
to the intractability of the true posterior, a variational posterior is used for approxima-
tion, with the mismatch quantified by the Kullback-Leibler divergence (red area in the
upper-center pane). The upper-right pane illustrates how optimizing a tractable lower
boundimprovesthevariationalposterior. Thelower-rightpaneshowstheimprovedpos-
terior approximation computed during the inference phase, and the lower-center pane
illustrates how this approximation is used to adjust generative model parameters, opti-
mizing the network prediction
discussed earlier:
dΣ ℓ ∝ ∂F˜ = − 1 ∂ (cid:88) L (ϕ −µ )TΣ−1(ϕ −µ )+ln|Σ |
dt ∂Σ 2∂Σ ℓ ℓ ℓ ℓ ℓ ℓ
ℓ ℓ
ℓ=1 (40)
(cid:20) (cid:21)
1 ∂
= − (ϕ −µ )TΣ−1(ϕ −µ )+Σ−1 .
2 ∂Σ ℓ ℓ ℓ ℓ ℓ ℓ
ℓ
Given the following matrix derivative rule, which assumes that A is a symmetric, invert-
ible, and square matrix, such that the identity A−1 = (A−1)T holds:
∂ (cid:0) xTA−1x (cid:1) = −A−1xxTA−1 = −(A−1x)(A−1x)T, (41)
∂A
23
we can write:
∂F˜ = − 1 (cid:2) −(Σ−1(ϕ −µ ))(Σ−1(ϕ −µ ))T +Σ−1(cid:3)
∂Σ 2 l l l l l l l
l (42)
= 1 (cid:2) ϵ ϵT −Σ−1(cid:3) .
2 l l l
which would allow us to define an update rule analogous to (35) and (39).
Box 2 - Summary of Predictive Coding Updating Rules
1. Update rule for Variational Parameters:
ϕ (t+∆t) = ϕ (t)+∆t (cid:0) f′(Θ ϕ )⊙ΘTϵ −ϵ (cid:1)
ℓ ℓ ℓ ℓ ℓ−1 ℓ
2. Update rule for Generative Parameters:
Θ (t+∆t) = Θ (t)+∆t (cid:0) f′(Θ ϕ )⊙ϵ ϕT(cid:1)
ℓ ℓ ℓ ℓ ℓ−1 ℓ
Σ (t+∆t) = Σ (t)+ ∆t(cid:0) ϵ ϵT −Σ−1(cid:1)
ℓ ℓ 2 ℓ ℓ ℓ
Predictive Coding and Backpropagation
In this section, we analyze and compare the key differences between PC and the BP
algorithm, following the approach of [5], and refer to other relevant studies for interested
readers.
Let us assume we have an ANN whose loss function L is the least squares function
1
L = ∥y −yˆ∥2, (43)
2 L 2
whereyˆdenotesthetargetvectorandy theoutputvectorofthenetwork. Theobjective
L
is to optimize this loss function by iteratively updating the ANN’s parameters in order
to obtain predictions that better match the desired targets. The ANN forward pass is
24
modelled as linear transformation followed by a nonlinearity:
y = f(W y ) (44)
ℓ+1 ℓ ℓ
for 0 ≤ ℓ < L, where f(·) represents any differentiable and continuous activation func-
tion, and W denotes the parameter matrix of layer ℓ−1. The parameter updates are
ℓ−1
performed by evaluating the loss gradient using the chain rule of calculus:
(cid:32)ℓ+2 (cid:33)
∂L ∂L (cid:89) ∂y k ∂z k ∂y ℓ+1 ∂z ℓ+1
= , (45)
∂W ∂y ∂z ∂y ∂z ∂W
ℓ L k k−1 ℓ+1 ℓ
k=L
where z = W y . This expression can be reformulated recursively by introducing the
ℓ+1 ℓ ℓ
error terms δℓ, defined as:


 (y
ℓ
−yˆ) , ℓ = L
δ = (46)
ℓ
  f′(W
ℓ
y
ℓ
)⊙W
ℓ
Tδ
ℓ+1
, 0 < ℓ < L,
resulting in the following gradient:
∂L
= δ yT. (47)
∂W ℓ+1 ℓ
ℓ
Interestingly, a recursive formulation can also be derived in a particular case of PC.
Such recursion arises by setting the gradient in (34) to zero, meaning the NFE is maxi-
mized with respect to the variational parameters, and the network’s activity has reached
equilibrium:
∂F˜
= 0
∂ϕ
ℓ (48)
ϵ = f′(Θ ϕ )⊙ΘTϵ .
ℓ ℓ ℓ ℓ−1
To simplify the comparison between the two optimization algorithms, we invert the
indexing of the PC model, treating the highest layer as the 0-th and the lowest as the
25
L-th, in contrast to Figure 3. For a more direct comparison, we consider PC in the
supervised learning setting. In this context, the inputs are clamped to the 0-th layer,
while the labels are clamped to the L-th layer. The PC error equation can now be
written recursively as follows:

  Σ−
L
1(yˆ−µ
L
) , ℓ = L
ϵ = (49)
ℓ
  f′(Θ
ℓ
ϕ
ℓ
)⊙ΘT
ℓ
ϵ
ℓ+1
, 0 < ℓ < L,
asinthelastlayerofthehierarchy, theerroristheprecision-weighteddifferencebetween
the model’s prediction and the clamped target vector yˆ. Comparing (46) and (51), we
can see an explicit similarity, allowing us to show how PC can approximate BP gradi-
ents, assuming the two models to be initialized with the same generative parameters,
and the covariance matrix Σ fixed to the identity matrix. After both networks predicts
L
the output for a given input, the PC network starts from an equilibrium state, satisfying
(50). Then, its output neurons are clamped to the target values. If the initial prediction
was incorrect, this clamping perturbs the error neurons at layer L generating feedback
error signals. These signals cause changes in the activity of PC model’s neurons accord-
ing to (35), resulting in deviations from the forward-pass activity, which was initially
equivalent in both BP and PC models. Consequently, even if (50) is satisfied, the weight
updates may deviate from those obtained via BP. Therefore, to approximate BP gradi-
ents, we need to mitigate the change in the PC model’s neural activity. To achieve this,
the covariance (identity) matrix in the final layer can be scaled up by a constant factor.
As the matrix is inverted, this proportionally reduces the magnitude of the final layer’s
error neurons ϵ in (51). As a result, for large rescaling constants, PC neuron activity
L
remains nearly unchanged and closely aligns with BP, as both models share initial pa-
rameters and forward pass. Consequently, the error terms in (51) approximate those of
BP, scaled by the rescaling factor. Thus, while the gradients of the two models point in
similar directions, their magnitudes differ. To match the weight updates, PC gradients
26
must be rescaled by the same factor used to suppress the error signals, compensating for
the magnitudes difference. For readers interested in a more detailed discussion on this
property, we refer to [5]. Some studies have also demonstrated that PC can compute ex-
act BP gradients with additional minor modifications, such as updating parameters only
at specific timestamps [7] or that this property can be extended to any graph topology,
and not being restricted to ANN-like structures [6]. For those particularly interested
in the comparison between BP and PC, [11] provides a critical review, formulating PC
as a steady-state implementation of BP. The authors argue that this reinterpretation
compromises essential aspects of traditional PC, including its variational Bayesian inter-
pretation and its capacity to represent uncertainty over latent states, and they critically
assess the efficiency of such approaches.
Predictive Coding and Kalman Filtering
In the previous sections, we showed how a Bayesian inference problem can be reformu-
lated as an optimization problem, resulting in a bio-inspired algorithm with spatially
local updates. In this section, we show that this locality property can also be extended
temporally by introducing recurrent connections into the model.
By augmenting a PC network with recurrent connections, it can process temporal
data while maintaining local parameter updates in both space and time [9]. In this set-
ting, PC becomes analogous to recursively estimating an unknown probability density
function over time using incoming measurements and previous estimates. This approach
is equivalent to Bayesian filtering. A particular case of the latter arises when the system
is a linear Markov process and the noise is assumed to be Gaussian. In this scenario, the
process reduces to the well-known KF algorithm. KF provides a closed-form solution
for the Bayesian-optimal estimate of the system’s next state, along with the associated
uncertainty. This connection is particularly interesting, as it links neural processing to
Bayesian filtering, thereby supporting the Bayesian brain hypothesis. This hypothesis
27
proposes that the brain constructs beliefs and perceptions by integrating sensory in-
put with prior knowledge and expectations, an assumption that aligns closely with the
principles of the PC framework. We will first provide a brief overview of KF and then
show how a specific configuration of recurrent PC, illustrated in Figure 5, results in the
optimization the same objective [8]. Note that, as the focus of this manuscript is on PC,
which concerns perception, we omit the KF’s control parameter.
KF addresses the problem of trying to estimate the state x ∈ Rn of a linear dynamic
system that is assumed to evolve accordingly to
x = Ax +w, w ∼ N(0,Σ ), (50)
t+1 t w
where A ∈ Rn×n is the state transition matrix of the process from state at time t to time
t+1, x ∈ Rn is our expected state vector at time t, and w ∈ Rn is a random vector
t
representing the process noise. Therefore, we have that our estimate of the system’s
next state is distributed as follows:
x ∼ N(E[x ],Σ ), (51)
t+1 t+1 xt+1
The Gaussian is centred at the expected projected state, which is equal to the sys-
tem transition matrix multiplied by the maximum a posteriori (MAP) of the previous
timestamp
E[x ] = E[Ax +w] = Aµ . (52)
t+1 t t
Assuming zero covariance between x and w, the covariance matrix of the projected state
value is given by
Σ = Cov(Ax +w) = ACov(x )AT +Cov(w) = AΣ AT +Σ . (53)
xt+1 t t xt w
The measurements y ∈ Rm performed to update our posterior belief on the state at a
28
given time t+1 are modelled by
y = Cx +z, z ∼ N(0,Σ ), (54)
t+1 t+1 z
where C ∈ Rm×n is the emission matrix, mapping the state vector into the measurement
vector, and z ∈ Rm is a random vector representing the measurement noise. There-
fore, we can model the measurement vector y as distributed according to a Gaussian
t+1
distribution with mean Cx , and covariance Σ :
t+1 z
y ∼ N(Cx ,Σ ). (55)
t+1 t+1 z
The state projection in (50) acts as a prior distribution over the system, while the
measurement in (54) acts as a likelihood term, leading to a posterior probability distri-
bution that combines the predicted and observed estimates:
p(x |y ,µ ) ∝ p(y |x )p(x |µ ), (56)
t+1 t+1 t t+1 t+1 t+1 t
where µ is the MAP estimate from the previous time step. The posterior distribution
t
is then used to predict again the evolution of the system, generating the prior belief
for the subsequent timestamp. KF gives a closed-form exact solution to this problem,
which is optimal in a Bayesian sense as it minimizes the mean square error (MSE) in
the estimated parameters, finding the next state MAP
µ = argmax {p(x |y ,µ )}. (57)
t+1 t+1 t+1 t
xt+1
This inference problem can also be formulated from an optimization perspective [8]. To
do so, as we are mainly interested in finding the MAP of the system’s state posterior
distribution,wecanagainapproximatetheposteriordistributionbyaDiracdeltacentred
29
at the posterior’s mode δ(x −µ ). This involves the same steps we already covered
t+1 t+1
before, starting by lower-bounding the D between the variational posterior and the
KL
true posterior by the NFE:
F˜ = −H(q,p)+(cid:8)H(cid:8)(q (cid:8) )
= −E (cid:2) −ln (cid:0) N(y ;Cx ,Σ )N(x ;Aµ ,Σ ) (cid:1)(cid:3) (58)
δ t+1 t+1 z t+1 t xt+1
(cid:0) (cid:1)
= ln N(y ;Cµ ,Σ )N(µ ;Aµ ,Σ ) .
t+1 t+1 z t+1 t xt+1
Note that here µ has to be interpreted as the variational parameter, analogous to ϕ in
the PCN section, to emphasize that as it approaches the NFE maximum, it converges to
the value in (57), corresponding to the posterior modes. Indeed, maximizing the NFE
with respect to µ is equivalent to finding the x value maximizing the posterior
t+1 t+1
distribution in (59), as the maximum of the log-joint is the same as the maximum of the
joint distribution:
argmax{p(x |y ,µ )} = argmax{p(y |x )p(x |µ )}
t+1 t+1 t t+1 t+1 t+1 t
xt+1 xt+1
(cid:8) (cid:0) (cid:1)(cid:9)
= argmax ln N(y ;Cx ,Σ )N(x ;Aµ ,Σ ) .
t+1 t+1 z t+1 t xt+1
xt+1
(59)
By expanding the two Gaussian distributions and then applying the properties of loga-
rithms, we obtain
1 (cid:104) (cid:105)
F˜ ≈ − (y −Cµ )TΣ−1(y −Cµ )+(µ −Aµ )TΣ−1 (µ −Aµ ) ,
2 t+1 t+1 z t+1 t+1 t+1 t xt+1 t+1 t
(60)
where the normalization constants have not been considered, as they do not affect the
optimization with respect to the modes. We can now use gradient ascent to iteratively
approximate the most likely state of the system, after performing the prediction and
measurement steps. The update rules derived from this approach are again biologically
plausible updating rules that could be implemented within a neural circuit such as the
30
one proposed in Figure 5.
Figure 5: Graphical representation of the circuit derived from the PC approximation of
KF. For consistency of notation, µ is renamed ϕ, and matrices A and C are relabelled
Θ and Θ . Dashed lines indicate how each estimate serves as the prior for the next
x y
timestamp. At each timestamp, a new observation is clamped to the y layer (KF mea-
surement), and the most likely state is inferred by integrating prior and observation,
repeating the process over time.
Aligned with the error definition discussed in the previous sections, we introduce the
projectionandmeasurementerrornodes,ϵ ≜ Σ−1(y −Cµ )andϵ ≜ Σ−1 (µ −Aµ ),
y z t+1 t+1 x xt+1 t+1 t
respectively. Proceeding by computing the gradient of F˜ with respect to the variational
posterior parameters µ , we apply the derivative rules in (30) and (32) obtaining:
t+1
31
∂F˜ = − 1 ∂ (cid:104) (cid:0) (y −Cµ )TΣ−1(y −Cµ ) (cid:1) + (cid:16) (µ −Aµ )TΣ−1 (µ −Aµ ) (cid:17)(cid:105)
∂µ 2∂µ t+1 t+1 z t+1 t+1 t+1 t xt+1 t+1 t
t+1 t+1
= CTΣ−1(y −Cµ )−Σ−1 (µ −Aµ )
z t+1 t+1 xt+1 t+1 t
= CTϵ −ϵ .
y x
(61)
This results in a gradient with the same form as the one obtained in (34), up to the
nonlinearity. Similarly, the matrices A and C can be learned by treating them as pa-
rameters and computing the gradient of F˜ with respect to them. Starting with the state
transition matrix A, by applying the rule in (41), we can write:
∂F˜ = − 1 ∂ (cid:104) (cid:0) (y −Cµ )TΣ−1(y −Cµ ) (cid:1) + (cid:16) (µ −Aµ )TΣ−1 (µ −Aµ ) (cid:17)(cid:105)
∂A 2∂A t+1 t+1 z t+1 t+1 t+1 t xt+1 t+1 t
= ϵ µT.
x t
(62)
This results in the same gradient obtained for the PCN parameters Θ in (38), up to the
linearity of µ. The same applies to the emission matrix C:
∂F˜ = − 1 ∂ (cid:104) (cid:0) (y −Cµ )TΣ−1(y −Cµ ) (cid:1) + (cid:16) (µ −Aµ )TΣ−1 (µ −Aµ ) (cid:17)(cid:105)
∂C 2∂C t+1 t+1 z t+1 t+1 t+1 t xt+1 t+1 t
= ϵ µT .
y t+1
(63)
Moreover, it is also possible to refine our uncertainty about the system state by opti-
mizing F˜ with respect to the state covariance matrix, similarly to (42). Importantly, the
considerations raised in the PCN section regarding covariance updates remain relevant
here and represent an active area of research in the PC community.
This variational treatment of FK under FE optimization thus allows us to rein-
terpret the problem as a particular case of PC, enhanced with recurrent connections.
Importantly, unlike KF, this approach results in an approximation of the exact solu-
tion. However, it emphasizes how PC with recurrent connections can deal with time
32
series data. Additionally, KF and its intricate linear algebra equations are unlikely to be
implemented by the brain. This perspective instead suggests that it could be approxi-
mated by a biologically plausible algorithm, potentially bridging the brain’s mechanisms
of perception with Bayesian filtering and inference [9].
Computational Example
This section presents a comparative analysis of ANNs trained with PC and BP on stan-
darddeeplearningtaskscommoninthePCliterature[12],specificallydataclassification
and compression. Experiments focus on standard benchmarking datasets for PC net-
works [12], such as MNIST, FashionMNIST, and CIFAR10, employing multilayer per-
ceptrons (MLPs) and convolutional neural networks (CNNs) to evaluate performance
across deep learning architectures. All models were trained using an NVIDIA RTX
A6000 GPU, with model implementation and training performed in Python using the
PyTorch framework. For reproducibility of the experiments, the complete implementa-
tion setup, including additional details regarding the network architectures and other
experiments such as approximating BP gradients using PC, is available on our GitHub
repository: https://github.com/cogsys-tudelft/PredictiveCodingTutorial.
In data compression tasks, the key difference between the two optimization algo-
rithms lies in their architectural structure. BP autoencoders are composed of two sepa-
rate networks: an encoder and a decoder. The encoder processes the input to produce a
lower-dimensional latent representation, which the decoder uses to attempt reconstruct-
ing the original input. The reconstruction error is successively computed and back-
propagated through both modules, forcing the encoder to learn compact yet informative
representations, and the decoder to accurately reconstruct the input. In contrast, PC
achieves the same objective using a single network, as illustrated in Figure 3, effectively
acting as an autoencoder folded onto itself. During the PC inference phase, the input is
33
clampedtothebottom-layerneurons, andthenetwork’sneuralactivityisiterativelyup-
dated to minimize the FE according to (34). Once convergence is reached, the top layer
encodes the compressed representation of the input. The PC decoder functionality is
then executed by computing the generative forward pass from this latent code, resulting
in the reconstructed input. Importantly, during training, the PC generative parameters
are also updated after the inference phase, reducing the mismatch between the recon-
struction and the original input. At test time, only the inference phase is performed.
In classification tasks, both BP and PC share the same network architecture and size.
Here, the input is clamped to the top layer of the PC network, and the model infers the
corresponding label. Once a prediction is made, the true label is clamped to the output
layer. Thus, in classification settings, both the first and last layers of a PC network are
clamped to fixed values. After clamping the target label, PC inference phase proceeds
to propagate eventual feedback errors. Upon convergence, thegenerative parameters are
updated to minimize the FE, as in the compression setting.
Task Dataset Model Test Metric Epoch Time (s) Gen. Params Var. Params FLOPs
noisserpmoC
BP (9.39±0.06)·10−3 7.01±0.10 652k – 4M
MNIST
PC (1.10±0.15)·10−2 17.34±0.24 326k 448 138M
BP (1.34±0.14)·10−2 7.00±0.19 652k – 4M
F-MNIST
PC (1.34±0.33)·10−2 17.56±0.57 326k 448 138M
BP (1.03±0.34)·10−2 8.09±0.13 2,173k – 11M
CIFAR10
PC (1.29±0.61)·10−2 39.47±0.55 1,088k 12k 226M
noitacfiissalC
BP 98.31%±0.07p.p. 11.64±0.75 135k – 0.74M
MNIST
PC 98.12%±0.05p.p. 17.40±1.20 135k 384 5.2M
BP 89.46%±0.15p.p. 10.94±0.22 135k – 0.74M
F-MNIST
PC 88.82%±0.08p.p. 16.61±0.27 135k 384 5.2M
BP 77.56%±0.45p.p. 11.95±0.09 4,762k – 48.7M
CIFAR10
PC 77.89%±0.14p.p. 28.61±0.06 4,762k 43k 313M
Table 1: Comparison of BP and PC across compression and classification tasks.
Table 1 provides a comparative summary of the performance and resource utilization
of both PC and BP models, averaged across five independent runs for each of the three
34
datasets. For MNIST and FashionMNIST, in both classification and compression tasks,
anMLPnetworkisused. ForCIFAR10, aconvolutionalarchitectureisemployed. While
performance in the classification task is evaluated using accuracy, the metric of interest
for the compression task is the MSE loss.The FLOPs estimate refers to the total amount
of floating point operations required by each algorithm to complete a full parameters
update. For BP, this includes the forward pass, loss computation, error backpropaga-
tion, and parameters update. For PC, it accounts also for the inference phase. The
latter was performed over 35 steps for the compression task and over 10 steps in the
classification one. FLOPs estimates were obtained using the PyTorch Profiler. Table
1 highlights how, in the compression task, PC requires only half the generative param-
eters compared to BP, while maintaining comparable performance and relying only on
local and bio-plausible learning rules. However, PC consistently requires greater com-
putational resources due to its inner optimization process, leading to more FLOPs and
longer training times compared to BP. The classification section in Table 1 shows how
PC needs exactly the same number of generative parameters as BP, while having also
a small percentage of additional variational parameters. Furthermore, even if in clas-
sification settings the inference phase usually requires fewer steps, it still introduces a
non-negligible overhead in terms of computation. While PC is competitive with BP for
small networks and datasets, it tends to underperform relative to BP when applied to
larger datasets and more complex architectures. This limitation is discussed in detail
in [12], where the authors present a comprehensive comparison of state-of-the-art PC
algorithmsacrossvariousdeeplearningarchitecturesanddatasets, benchmarkedagainst
theirBPcounterparts. Inparticular, forverydeepnetworksandmorecomplexdatasets,
BPconsistentlyoutperformsPC.Thesefindingshighlightthescalabilitylimitationscur-
rently affecting the the latter. Although the framework offers appealing features from
bothcomputationalandbiologicalperspectives,improvingitsefficiencyandperformance
on large-scale tasks remains an open and actively researched challenge.
35
Acknowledgments
The authors would like to thank Martin Lefebvre for constructive feedback on the
manuscript. ThispublicationispartoftheprojectSynergAIwithfilenumberNGF.1607.22.010
of the AiNed Fellowship research programme, which is financed by the Dutch Research
Council (NWO) and the Microelectronics Department of TU Delft.
Authors
Davide Casnici (d.casnici@tudelft.nl) received his B.Sc. degree in Information Engi-
neering from the University of Modena and Reggio Emilia, Italy, in 2021. He succes-
sively obtained his M.Sc. degree in Artificial Intelligence from the University of Lugano,
Switzerland, in 2023. Since 2023, he has been a Ph.D. student at Delft University of
Technology, the Netherlands. His research interests include neuromorphic computing,
bio-inspired AI, machine learning, statistics, and information theory.
Charlotte Frenkel (c.frenkel@tudelft.nl)receivedherPh.D.fromUniversit´ecatholique
de Louvain in 2020 and was a post-doctoral researcher at the Institute of Neuroinfor-
matics, UZH, and ETH Zu¨rich, Switzerland. She is now an Assistant Professor at Delft
University of Technology, The Netherlands. Her research aims at bridging the bottom-
up (bio-inspired) and top-down (engineering-driven) design approaches toward neuro-
morphic intelligence, with a focus on digital neuromorphic processor design, embedded
machine learning, and brain-inspired on-device learning. She co-leads the NeuroBench
initiative and the Edge AI Foundation working group on neuromorphic engineering.
Justin Dauwels (j.h.g.dauwels@tudelft.nl) obtained his PhD degree in electrical engi-
neering at the Swiss Polytechnical Institute of Technology (ETH) in Zurich in December
2005. Moreover, he was a postdoctoral fellow at the RIKEN Brain Science Institute
36
(2006-2007) and a research scientist at the Massachusetts Institute of Technology (2008-
2010). He is an Associate Professor in AI at TU Delft, co-director of the Safety and
Security Institute, and scientific lead of the Model-Driven Decisions Lab, collaborating
with the Netherlands police. His research focuses on machine learning, generative AI,
and their applications to autonomous systems, as well as the analysis of human behavior
and physiology. His academic lab has fostered four startups spanning industries such as
health tech and autonomous vehicles.
References
[1] P.Elias. Predictivecoding–i. IRE Transactions on Information Theory, 1(1):16–24,
1955.
[2] R. PN. Rao and D. H. Ballard. Predictive coding in the visual cortex: a functional
interpretation of some extra-classical receptive-field effects. Nature neuroscience,
2(1):79–87, 1999.
[3] K. Friston. A theory of cortical responses. Philosophical transactions of the Royal
Society B: Biological sciences, 360(1456):815–836, 2005.
[4] K. Friston and S. Kiebel. Predictive coding under the free-energy principle. Philo-
sophical transactions of the Royal Society B: Biological sciences, 364(1521):1211–
1221, 2009.
[5] J.CR.WhittingtonandR.Bogacz. Anapproximationoftheerrorbackpropagation
algorithm in a predictive coding network with local hebbian synaptic plasticity.
Neural computation, 29(5):1229–1262, 2017.
[6] B. Millidge, A. Tschantz, and C. L. Buckley. Predictive coding approximates back-
prop along arbitrary computation graphs. Neural Computation, 34(6):1329–1368,
2022.
37
[7] Y. Song, T. Lukasiewicz, Z. Xu, and R. Bogacz. Can the brain do
backpropagation?—exact implementation of backpropagation in predictive coding
networks. Advances in neural information processing systems, 33:22566–22579,
2020.
[8] B. Millidge, A. Tschantz, Anil Seth, and C. Buckley. Neural kalman filtering. arXiv
preprint arXiv:2102.10021, 2021.
[9] B. Millidge, M. Tang, M. Osanlouy, N. S. Harper, and R. Bogacz. Predictive coding
networks for temporal prediction. PLOS Computational Biology, 20(4):e1011183,
2024.
[10] C. Marsh. Introduction to continuous entropy. Department of Computer Science,
Princeton University, 1034, 2013.
[11] U. Zahid, Q. Guo, and Z. Fountas. Predictive coding as a neuromorphic alternative
to backpropagation: A critical evaluation. Neural Computation, 35(12):1881–1909,
2023.
[12] LucaPinchetti,ChangQi,OlehLokshyn,GaspardOlivers,CorneliusEmde,Mufeng
Tang,AmineM’Charrak,SimonFrieder,BayarMenzat,RafalBogacz,etal. Bench-
markingpredictivecodingnetworks–madesimple. arXivpreprintarXiv:2407.01163,
2024.
38

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Bio-Inspired Artificial Neural Networks based on Predictive Coding"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
