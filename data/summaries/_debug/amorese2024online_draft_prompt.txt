=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Online Pareto-Optimal Decision-Making for Complex Tasks using Active Inference
Citation Key: amorese2024online
Authors: Peter Amorese, Shohei Wakayama, Nisar Ahmed

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2024

Key Terms: decision, pareto, task, trade, user, preferences, tasks, rack, optimal, inference

=== FULL PAPER TEXT ===

1
Preprint submitted to the IEEE Transactions on Robotics journal.
Online Pareto-Optimal Decision-Making for
Complex Tasks using Active Inference
Peter Amorese†, Shohei Wakayama†, Nisar Ahmed, and Morteza Lahijanian
Abstract—When a robot autonomously performs a complex Risk : Pareto points
task, it frequently must balance competing objectives while : User preference
maintaining safety. This becomes more difficult in uncertain
environmentswithstochasticoutcomes.Enhancingtransparency Put jar on the floor
and pitcher on the rack
in the robot’s behavior and aligning with user preferences are
Drying rack
alsocrucial.Thispaperintroducesanovelframeworkformulti-
Time
objectivereinforcementlearningthatensuressafetaskexecution,
optimizes trade-offs between objectives, and adheres to user
preferences. The framework has two main layers: a multi-
objective task planner and a high-level selector. The planning
layer generates a set of optimal trade-off plans that guarantee
satisfaction of a temporal logic task. The selector uses active
inference to decide which generated plan best complies with
Dishwasher Towel Floor
user preferences and aids learning. Operating iteratively, the
frameworkupdatesaparameterizedlearningmodelbasedoncol-
Fig. 1: Motivating robotic dishwashing scenario: a robotic
lected data. Case studies and benchmarks on both manipulation
manipulator needs to compare execution time of a task and
and mobile robots show that our framework outperforms other
methodsand(i)learnsmultipleoptimaltrade-offs,(ii)adheresto inherentrisksofdroppingfragiledishes(thejarismorefragile
a user preference, and (iii) allows the user to adjust the balance than the blue pitcher). The user’s preferred trade-off between
between (i) and (ii). time and risk is incorporated for transparent behaviors.
Index Terms—Multi-Objective Decision Making, Active Infer-
ence, Formal Synthesis the dishwasher, then properly drying them by either placing
directly up on the rack, or placing on the towel and moving
I. INTRODUCTION them to the floor. In such a scenario, the robot should be both
efficient(minimizethetimeofexecution)andrisk-awarewhen
THE demand for robots to autonomously perform haz-
carrying fragile dishes high above the ground. However, due
ardous and repetitive tasks is steadily increasing [1, 2].
tothenatureofsampling-basedmethods,thecharacteristicsof
Suchtasksofteninvolvemultiple,possiblycompeting,quanti-
actions(timeandpath-height)areunknownapriori,andhence
tative objectives, e.g., time and energy, requiring optimization
therobotneedstoconstructaccuratemodelsofthesequantities
fortrade-offsbetweentheobjectives,knownasParetooptimal.
during deployment. The robot can either quickly place dishes
In unknown environments, like the Martian surface or civilian
up on a drying rack (low-time, high-risk for fragile objects),
households, however, such quantities are often stochastic and
or manually dry dishes on the ground (high-time, low-risk).
unknown a priori. Fortunately, they are measurable and can
Considering a human user may prefer one trade-off over the
be learned online. Then, the robot should make decisions to
other, the robot must both learn the quantitative models, and
learnalloptimaltrade-offs,orfocusonlearningasingleuser-
decideamongstoptimaltrade-offactionswhileadheringtothe
preferred trade-off. Although highly desirable, accomplishing
user’s preference. Such decision making problems involving
bothsimultaneouslyischallengingsincethetrade-offselection
competing objectives can be formulated as multi-objective
must balance exploring unknown trade-offs with exploiting
reinforcement learning (MORL) [5, 6, 7, 8].
preferred trade-offs. This paper focuses on this challenge
and aims to develop a framework that simultaneously (i) Embeddingthesatisfactionofataskintothemulti-objective
learnsmultipleoptimaltrade-offs,(ii)embedsanintuitiveuser reward function often aims to capture both qualitative prop-
preferenceoveradesiredtrade-off,and(iii)allowstheuserto erties of the task (i.e. whether or not the robot completed
adjust the balance between (i) and (ii). the task), as well as quantitative properties (i.e. how well the
robot completed the task) [9]. However, properly representing
Example 1. Consider the robotic dishwashing scenario
qualitative task completion through a reward function is a
in Figure 1. The robotic manipulator is equipped with a
challenging problem, often requiring the use of methods such
sampling-based motion planner (e.g., RRT and PRM [3, 4])
as inverse reinforcement learning (IRL) [10]. In uncertain
that solves A-to-B planning queries. The robot must repeti-
environments,itisnotalwayspossibletoobtainlargeamounts
tively complete a complex task of inserting the dishes into
of human teaching data. Furthermore, these learning-based
methods often fail to provide safety guarantees on the quali-
†Authorscontributedequallytothiswork
AuthorsarewiththeUniversityofColoradoBoulder. tative behavior of the robot. Nevertheless, in scenarios where
4202
nuJ
71
]OR.sc[
1v48911.6042:viXra
2
qualitative aspects of the robotic model and task are known [6,8,18],alongwithMarssurfaceexplorationsimulation
beforehand, e.g., manipulation problems, formal methods [11] and a robotic hardware diswashing experiment.
provide alternative approaches to both task representation and A detailed discussion on related work along with an
plan synthesis using temporal logic specifications, such as overview of AIF are provided in Sec. V.
Linear Temporal Logic (LTL) [12] and LTL over finite traces
(LTLf)[13].Theseplanningmethodscansolvethequalitative
II. PRELIMINARIESANDPROBLEMFORMULATION
portionofthelearningproblemandprovideguaranteesonthe
This paper studies a framework that enables a robot to (i)
completion of tasks and safety while avoiding the need for
repetitively complete a given high-level complex task, and (ii)
IRL.Formalsynthesisapproacheshavealsobeenusedtosolve
learn the most efficient and user-aligned means of completing
multi-objective quantitative planning problems [14]; however,
the task. These two goals separate nicely into qualitative
those approaches assume a perfectly accurate quantitative
behavior,i.e.,whetherornottherobotcompletesthetask,and
model, which are often unavailable.
quantitative behavior, i.e., how well the robot completes the
Unlike single-objective RL, MORL agents must select
task. We restrict our attention to problems where qualitative
amongpotentiallymanyoptimaltrade-offs.Thenumericalval-
aspects of the model (i.e. robotic states, state transitions, task
uationsoftheseoptimaltrade-offsarereferredtoasthePareto
related observations, etc.) are known, but quantitative aspects
front. The Multi-Objective Multi-Armed Bandit (MOMAB)
(i.e.costsofexecutingroboticactions)areunknownandmust
community emphasizes learning all optimal trade-off actions,
be learned. Below, we first formally define a robot model and
yielding valuable insight into the trade-off analysis of the
LTLf task specifications. We then connect these definitions
learned results [15, 16, 17, 18]. Alternatively, one can use
with cost uncertainty and user preferences to be able to fully
scalarization to convert the multi-objective problem into a
formulate the problem.
single objective problem based on a preference trade-offs
[5,6,7,8].Furtherdescriptionofthesemethodscanbefound
in Sec. V. To the best of our knowledge, no MORL selec- A. Robot Model
tion methods simultaneously propose mathematically sound We consider a robotic system modeled as a deterministic
adherencetoapreferredtrade-offwhilelearningandexploring transition system (DTS), an abstraction used in many formal
portions of the true Pareto front. approaches to robotic systems, including both mobile robots
In light of these gaps, this study proposes a novel MORL and robotic manipulators [11, 23, 24, 25, 26, 27, 28]. To cap-
framework centered around viewing optimal trade-off selec- ture uncertainty in quantitative values, we augment classical
tion as its own high-level decision making under uncertainty DTS with a stochastic cost function as defined below.
problem. Specifically, we employ active inference (AIF), re-
Definition 1 (DTS-sc). A labeled Deterministic Transition
cently explored as a sound approach for sequential decision-
System with a stochastic cost function (DTS-sc) is a tuple
making under uncertainty [19, 20, 21, 22] that minimizes an
T =(S,A,δ ,C,AP,L), where
information-theoretic quantity known as surprise. We exploit T
the decision making power of AIF to adhere to a user’s ideal • S is a finite set of states,
trade-off while exploring and learning localized portions of • A is a finite set of actions,
the whole Pareto front. This, however, introduces intractable • δ T :S×A(cid:55)→S is a deterministic transition function,
computationalcomplexityinoptimizingforsurpriseforfinite- • C : S × A (cid:55)→ D(RN) is a stochastic cost function
horizon tasks. Our framework integrates multi-objective plan- that maps each state-action pair to an N-dimensional
ningforacomplexLTLfspecificationwithhigh-levelselection probability distribution, where N ∈N and D(RN) is the
of an optimal trade-off by using a Bayesian approach to set of all probability distributions over RN,
learning the quantitative model. We extend AIF to reasoning • AP is a set of atomic propositions related to the robot
over finite horizon planning via upper-bounding the surprise tasks, and
with expected free energy (EFE). We derive computationally • L:S (cid:55)→2AP is a labeling function that assigns to each
tractabletechniquesforoptimizingEFEforLTLftasks.Using state s ∈ S the subset of propositions in AP that are
our framework, we illustrate the performance trade-offs be- true in s.
tween biasing towards a user’s preferred Pareto point versus
A robot plan π ∈ A∗ is a finite sequence of ac-
learningtheentireParetofront.Additionally,weshowcasethe
tions π = a a ...a that induces a trajectory τ(s ,π) =
utility of our approach using robotic case studies, including a 0 1 m 0
s s ...s s from a current state s ∈ S, where s ∈
hardware demonstration of the scenario in Example 1. 0 1 m m+1 0 k
S and, for all 0 ≤ k ≤ m + 1, s = δ (s ,a ).
The major contributions of this work are three-fold: k+1 T k k
The observation of τ(s ,π), denoted L(s ,π), is a trace
• A novel MORL framework that combines formally guar- 0 0
L(s ,π)=o o ...o where o =L(s ).
anteed task plan synthesis with efficient learning of 0 0 1 m+1 k k
Pareto-optimal behavior, Example 2. Consider a simplified DTS-sc model in Fig. 2 of
• Derivation of a tractable approximation of a free energy the dishwashing scenario described in Fig. 1 with five states
formulation for finite horizon planning over an uncertain capturing the locations of each dish (J : jar on floor, P :
f r
quantitative model, pitcher on rack, etc.), and two actions load and unload.
• Benchmarking the efficacy of the proposed AIF-based Eachstateobserveseither{dry}or{wash}.Eachactionfrom
selection strategy against the state-of-the-art methods a state takes time and incurs some risk when transporting the
3
{dry} {dry} SincetrueΘareunknownbeforehand,welearntheparameters
Jr,Pf Jr,Pr from experience. We denote the learned parameters by Θ˜.
unload_2 load
B. Robotic Task
p(c|s,a;θs,a)
load unload_3
The robot is given a complex task that can be completed in
finite time. To specify such tasks in a formal manner, we use
unload_1 load
Jd,Pd Linear Temporal Logic over Finite Traces (LTLf) [13], which
{dry} {dry}
{wash} combine propositional logic with temporal operators.
load unload_4
Jf,Pf Jf,Pr
Definition 2 (LTLf Syntax). A Linear Temporal Logic over
Finite Traces (LTLf) formula over AP is recursively defined
Fig. 2: The DTS-sc T described in Example 2 is shown, with as
a plan (red) that satisfies the task described in Example 3.
ϕ:=true|o|¬ϕ|ϕ∧ϕ|Xϕ|ϕUϕ,
jar, modeled with a unique multivariate cost distribution for whereo∈AP,¬(negation)and∧(conjunction)areBoolean
each state-action pair. operators,andX (next)andU (until)aretemporaloperators.
When robot takes action a∈A at state s∈S, it endures
N distinct types of costs c s,a = (c 1 ,...,c N ) ∈ RN, e.g., The commonly-used temporal operators “eventually” (F) and
energy expenditure and time of execution. This cost vector “globally” (G) can be defined as Fϕ ≡ trueUϕ and Gϕ ≡
can generally be stochastic, i.e., the cost value of executing a ¬F¬ϕ, respectively.
atscandifferatdifferentinstancesdueto,e.g.,changesinthe The semantics of LTLf are defined over finite traces [13].
environmentoruseofrandomizedalgorithms(sampling-based That is, an LTLf formula ϕ can be satisfied with a finite trace
planners).Hence,c s,a isarandomvariablewhosedistribution w ∈(2AP)∗ [13], denoted w |=ϕ. We define the language of
is given by C(s,a), i.e., c s,a ∼ C(s,a). With an abuse of ϕ to be the set of finite traces that satisfy ϕ for the first time,
notation, we use c as the cost random variable for all state- i.e., let |w| be the length of w and w[k] for 1 ≤ k ≤ |w| be
action pairs, and hence (c|s,a)=c s,a . the prefix of w with length k; the language of ϕ is defined as
We treat C(s,a) as a conditional distribution of a proba-
L ={w ∈(2AP)∗ |w |=ϕ∧w[k]̸|=ϕ ∀k <|w|}.
bilistic transition-cost generative model, parameterized by the ϕ
set of parameters Θ, i.e., Wesayplanπ ∈A∗ executedfromssatisfiesϕiftheresulting
C(s,a)=p(c|s,a;Θ). trace L(π,s)∈L ϕ .
For ease of presentation, we denote (s,a) specific parameters Example 3. Following Example 2, we specify the task ϕ =
by θ s,a ⊆Θ such that Θ=∪ s∈S,a∈A θ s,a . F(wash∧F(dry)) stating “the robot should first wash the
We assume robot model T is a Markov process. That is, c dishes, and then dry the dishes.”
foreachstate-actionpairisindependent,i.e.,p(c|s,a;θ )is
s,a Wewanttherobottoexecutetaskϕrepeatedly.Specifically,
independent from p(c|s′,a′;θ ) for all (s,a) ̸= (s′,a′) ∈
s′,a′
weareinterestedinsynthesizingplansthatepisodicallyrepeat
S×A.Further,weassumeeachp(c|s,a;θ )isamultivariate
s,a ϕ. That is, once a plan is complete (ϕ is satisfied), a new plan
normal (MVN) distribution N(µ ,Σ ) paramerized by a
s,a s,a hastobesynthesizedfromthecurrentstatetosatisfyϕagain.
non-negativemeanµ ∈RN andcovarianceΣ ∈RN×N,
s,a ≥0 s,a Weassumethatsucharepetitionisalwaysphysicallypossible
i.e., θ = {µ ,Σ }. Note that this assumption allows
s,a s,a s,a fortherobot,i.e.,therobot alwaysendsinastatefromwhich
for correlation between types of costs. For instance, if the
there exists a plan that satisfies ϕ. For instance, in the above
executionofaroboticactiontakesalongtime,itislikelythat
example, after drying dishes, the robot always ends in a state
it also uses more energy.
from which it can start washing a new pile of dirty dishes.
Inthiswork,weconsidertherealisticcasewhereΘareun-
Thus,wefocusonthesetofsatisfyingplansfromeverystate,
known, i.e., both µ and Σ are unknown for every (s,a).
s,a s,a and refer to it as the ϕ-satisfying plan set.
Oneofthegoalsofthisworkistolearntheseparametersfrom
data collected on c during execution. Definition 3 (ϕ-Satisfying Plan Set). Given DTS-sc robot
s,a
Givenaplanπ anditsinducedtrajectoryτ ,thecumulative model T, state s∈S, and LTLf task formula ϕ, ϕ-Satisfying
cost of executing π from state s , denoted C , is the sum Plan Set Π(s,ϕ) is defined as
of the state-action costs along τ
0
, i.e., C =
s0,
(cid:80)
π
m c .
s0,π k=0 sk,ak Π(s,ϕ)={π ∈A∗ |L(π,s)∈L }.
Sinceeachc isarandomvariable,C isalsoarandom ϕ
sk,ak s0,π
variable,anditsdistributionisgivenbytheconvolutionofthe
c distributions, i.e.,
sk,ak
C. Optimization Objectives
p(C|s ,π;θ )=p(c|s ,a ;Θ)∗...∗p(c|s ,a ;Θ), (1)
0 s0,π 0 0 m m
Ateachplanning(decision)instancefromstates,weaimto
where θ is constructed by Θ. Recall that p(c|s ,a ;Θ) select a plan π⋆ ∈Π(s,ϕ) that, in addition to completing task
s0,π k k
is assumed to be a MVN; p(C|s ,π;θ ) is also a MVN. ϕ, optimizes for the expected cumulative cost E[C|s,π; Θ].
0 s0,π
4
Since C is N-dimensional, it is in fact an N-objective high-time,low-riskalternative,bothofwhichcanbeinformed
s,π
optimization problem. Moreover, the cost objectives may be by past observations of how humans wash the dishes. If the
competing,i.e.,byoptimizingoneobjective,anotherbecomes user is interested in learning other optimal trade-offs, or is
sub-optimal. Hence, we want to optimize for the trade-offs uncertain how accurately they have estimated the mean, they
betweentheobjectives,whichisknownasParetooptimaland can expand and orient the covariance around regions they
best defined by the notion of vector dominance. wouldliketherobottoexploreinformationrichopportunities.
Definition 4 (Vector Dominance). Let C,C′ ∈ RN be two
vectorsandC(i)bethei-thelementofC.VectorC dominates Tomeetuser’spreference,weseekaParetopointthatisnot
C′,denotedbyC ≺C′,iffC(i)≤C′(i)forevery1≤i≤N “suprising.”Theinformationtheoreticdefinitionofsurpriseis
and there exists an i such that C(i)<C′(i). the negative log of model evidence [30]. Similarly, we define
the surprise of an observation obtained by following a plan π
Definition 5 (Pareto Optimal). For state s ∈ S and LTLf
as:
formula ϕ, plan π ∈Π(s,ϕ) is called Pareto optimal if
Surprise(π)=−logp(C|π). (2)
∄π′ ∈Π(s,ϕ) s.t. E[C|s,π′; Θ]≺E[C|s,π; Θ].
This quantity can be derived by marginalizing the following
The set of all Pareto optimal plans from s is denoted by
generative model:
Π⋆(s,ϕ) ⊆ Π(s,ϕ). Then, for each π⋆ ∈ Π⋆(s,ϕ), E[C |
(cid:90) (cid:90)
s,π⋆; Θ] is called a Pareto point, and the set of all Pareto (2)=−log p(s,θ˜ ,C|π)dθ˜ ds,
s,π s,π
points is called the Pareto front, denoted Ω(s,ϕ). s θ˜ s,π
(cid:90) (cid:90)
At every planning instance (episode), we desire to select a =−log p(s,θ˜ s,π |C,π)p(C|π)dθ˜ s,π ds. (3)
Pareto optimal plan π⋆ ∈ Π∗(s,ϕ) which ensures that the s θ˜ s,π
expected cost is not dominated by another plan. However, In active inference (AIF), one can inject a bias towards
since Θ are not available, it is impossible to compute for π⋆. seeingcertainoutcomesbysubstitutingthepredictiveoutcome
Instead,wecanuseΘ˜ tocomputealearnedsetofϕ-satisfying distribution p(C|π) with the prior preference p pr (C) (a.k.a.
Pareto optimal plans denoted Π˜⋆(s,ϕ) ⊆ Π(s,ϕ). Similarly, “evolutionaryprior”)thatisindependentofanyplan [21,31].
let Ω˜(s,ϕ) denote the set of expected cost vectors of each We refer to the biased surprise as surprise w.r.t. p pr :
π ∈Π˜⋆(s,ϕ). (cid:90) (cid:90)
Surprise(π,p )=−log p(s,θ˜ |C,π)p (C)dθ˜ ds.
pr s,π pr s,π
s θ˜ s,π
D. User Preference (4)
There are possibly many Pareto optimal plans that perform In essence, under AIF, an agent acts to see desired observa-
ϕ. We are interested in picking a plan π⋆ that achieves the tions. An overview of AIF is provided in Sec. V-B.
pr
user’smost-preferredParetopointineveryinstance.Giventhat
our goal is autonomous operation, we require this preference E. Problem Statement
to be provided as an input before deployment. This poses
The problem we consider is as follows.
a challenge because the Pareto front is unknown a priori,
and the user cannot know the exact Pareto point they prefer. Problem 1. Consider a DTS-sc robot model T with unknown
Instead,assuggested inrecentliterature[22,29], theusercan cost distributions, an LTLf task specification ϕ that the robot
provide a probability density function (pdf) that serves as an is to repeatedly complete, and a user preference distribution
expressive way of capturing preferred observed outcomes. We p pr (C)overN costobjectives.LetK ∈Ndenotethenumber
extend this notion to the multi-objective domain, and leverage of times (instances) the robot achieves task ϕ, and s K ∈S be
theexpressivepdfpreferenceasatie-breakerbetweenoptimal thestartstateoftherobotintheK-thepisode.Then,foreach
trade-off candidates. K, synthesize a ϕ-satisfying plan π K ∈Π(s K ,ϕ) such that
We specifically focus on user preference as a normal dis- (i) π minimizes Surprise(π,p ) in (4),
K pr
tribution p (C) = N(µ ,Σ ) over the objectives since it (ii) as K → ∞, π becomes Pareto optimal, i.e., π ∈
pr pr pr K K
canbefullydefinedbyonlyspecifyingameanµ ∈RN and Π⋆(s ,ϕ), and
pr K
covariance Σ ∈ RN×N, respectively. Thus, by providing (iii) as K → ∞, the resulting Pareto front Ω˜(s ,ϕ) con-
pr K
µ and Σ , the user can not only express a desired trade-off verges to the true Pareto front Ω(s ,ϕ).
pr pr K
region in the objective space, but also specify the willingness
Notethat,inProblem1,satisfactionofϕ(qualitativeobjec-
to explore alternative trade-offs (Pareto points) that allow the
tive)ateachinstanceisahardconstraint,andtheoptimization
robot to learn more quickly, offering means to address the
(quantitative) objectives in (i)-(iii) are soft constraints. Hence,
exploration-exploitation trade-off problem.
by incorporating safety requirements in ϕ, safety satisfaction
Example4. SupposethedishwashingrobotdescribedinFig.1 is guaranteed throughout the process. Furthermore, from the
is being deployed in a restaurant. If the restaurant is going to perspective of multi-objective decision making, Problem 1 is
be busy, the user may define a preference distribution with particularly challenging since adhering to a user’s preference
mean in a low-time high-risk part of the objective space, and learning the Pareto front are arguably perpendicular ef-
whereas if the restaurant will be slow, the user may opt for a forts. In our approach, we address this challenge by using
5
Risk : Pareto points
: Trajectory
: User preference
s , a s , a
32 32 32 32
Put jar on the floor and
p(θ|c) ∝ p(c|θ)p(θ)
pitcher on the rack ・
s , a s , a
0 0 0 0
φ = F(load ∧ F(dry_rack ∨ dry_floor))
Time
Planning Selection Execution Update
Fig.3:Proposedmulti-objectivesafereinforcementlearningarchitecture:Asthefourphasesinvolving1)Planning,2)Selection,
3) Execution, and 4) Update are repeatedly executed, the robot gradually learns the state-action cost parameters and selects
the user preferred optimal task plan.
freeenergyminimizationtonaturallybalanceexploitationand B. Planning
exploration over the Pareto front.
At the start of each planning (decision-making) instance,
we employ a multi-objective task planner to compute the set
III. APPROACH of approximate Pareto-optimal plans Π˜⋆(s ,ϕ) described in
K
OurapproachtoProblem1isaniterativeMORLframework Def. 5. To this end, we formulate a correct-by-construction
withfourphases:1)planningParetooptimalplans,2)selection multi-objective shortest-path graph search problem, similar to
of the preferred Pareto optimal plan, 3) execution of the [14], which can be solved with the following procedure.
selected plan, and 4) update to the executed state-action cost 1) Product Graph Construction: An LTLf formula can be
parameters, as illustrated in Fig. 3. Planning, selection, and translated into a deterministic finite automaton (DFA) [13], a
update all are dependent on the learning status of the robot. finite state-machine that precisely captures the language of ϕ.
Hence, we start by formalizing our approach to modeling Θ˜.
Definition 6 (Deterministic Finite Automaton). A DFA
constructed from LTLf formula ϕ is a tuple Q =
A. Learning the Transition Cost Model ϕ
(Γ,γ ,W,δ ,F) where Γ is a set of states, γ ∈ Γ is the
Recall that the cost c s,a of each (s,a) is distributed as initia 0 lstate, Q W ϕ =2AP isthealphabet,δ :Γ× 0 W (cid:55)→Γisa
p(c|s,a;θ )=N(µ ,Σ ). However, θ are unknown
Qϕ
s,a s,a s,a s,a deterministic transition function, and F is a set of accepting
and must be learned using experience. We take a Bayesian
states.
approach to learning θ by modeling with random variable
s,a
θ˜ s,a . Specifically, we assume that the robot collects a realized A trace w = w 0 ...w m−1 ∈ (2AP)∗ induces a finite run
sample cˆ drawn from p(c|s,a;θ s,a ) each time a is executed τQϕ = γ 0 γ 1 ...γ m on Q ϕ , where γ k+1 = δ Qϕ (γ k ,w k ). Run
from s. Let D s,a ={cˆ 1 ,...cˆ l } be the set of l realizations of τQϕ is called accepting if γ m ∈F. If τQϕ is accepting, then
c s,a .Wemaintainabeliefoverθ˜ s,a usingtheNormalInverse- trace w is accepted by Q ϕ and satisfies ϕ, i.e., w |=ϕ.
Wishart (NIW) distribution, i.e., Using T to capture the physical capability of the robotic
system, along with Q to capture the temporal attributes of
p(θ˜ )=NIW (λ ,κ ,Λ ,ν ). ϕ
s,a s,a 0 0 0 0 ϕ, we can construct a product automaton that intersects the
NIW is chosen because of the property of conjugacy for esti- restrictions of both T and Q ϕ .
matingunknownmeanandcovarianceofaMVNdistribution,
Definition7(ProductAutomaton). GivenaDTS-scT,current
i.e., the posterior is also represented analytically as a NIW
state s ∈ S, and DFA Q , a Product Automaton is a tuple
K ϕ
[32]
P =(P,ρ ,A,δ ,v,F ), where
p(θ˜ |D )=NIW (λ,κ,Λ,ν). K P P
s,a s,a s,a • P =S×Γ is a set of states,
Note that some works assume the covariance is known a • ρ K = (s K ,γ 0 ′), where γ 0 ′ = δ Qϕ (γ 0 ,L(s K )), is the
priori [33], limiting the fidelity of the model. NIW strikes a starting state in the current instance,
balance between an over simplified parametric model (known • A is the same action set in T,
covariance) and a sample inefficient non-parametric model • δ P : P × A (cid:55)→ P is a transition function, where for
[34]. We detail the the update procedure in Sec. III-D. If the states ρ = (s,γ) and ρ′ = (s′,γ′) and action a ∈ A,
user has an informed guess about the mean and covariance transition ρ′ =δ (ρ,a) exists if s′ =δ (s,a) and γ′ =
P T
of a certain state-action distribution, they can set λ and Λ δ (γ,L(s′)),
0 0 Qϕ
respectively,reducingthenumberofdecisionmakinginstances • v : P ×A (cid:55)→ RN ≥0 , is a transition cost-vector function,
required to learn Θ˜. and
6
• F P =S×F is a set of accepting states.
π3
Note that unlike stochastic cost function C in T, cost-vector Jr,Pf Jr,Pr
function v(ρ,a)∈RN is not a distribution. We elaborate on
≥0 π4
v further below.
π8 π9
P
S
w
i
h
m
e
i
r
l
e
ar
ρ
to T
=
, a
δ
pl
(
a
ρ
n π
,a
in
)
d
.
u
R
c
u
e
n
s a
τP
ru
i
n
s
τ
ac
P
ce
=
pt
ρ
in
0
g
..
if
.
f
ρ
ρ
m+1 o
∈
n π1=load,unload_
π
2
2 π6
π5
k+1 P k k m+1
F . Therefore, by construction, the set of ϕ-satisfying plans
P Jd,Pd
Π(s
K
,ϕ) is equal to the set of all plans that induce paths on π7 π10
P from ρ to a ρ∈F .
2) Tran
K
sitionCost-Ve
P
ctorFunction: Weaimtodefinecost-
p(C|s,π;θs,π) Jf,Pf Jf,Pr
vector function v and accordingly formulate a multi-objective Fig. 4: By treating plans π as macro-actions, the selection
graphsearchproblemonP thatenablesthecomputationofthe decision making problem becomes sequential. The example
set of Pareto optimal ϕ-satisfying plans Π∗(s K ,ϕ). Consider plan highlighted in Fig. 2 is embedded into π 1 , as shown.
current state ρ =(s ,γ ), plan π ∈Π(s ,ϕ), and induced
K K 0 K
path τ(ρ ,π) = ρ ...ρ . Ideally, for every ρ =
K K,0 K,m+1 parameters E[θ˜ ]. To maintain optimality guarantees of the
(s,γ) ∈ P and a ∈ A, we want v((s,γ),a) = E[c|s,a;Θ] s,a
aforementionedgraphsearchmethod,werectifyeachelement
because the total cost of induced path τP(ρ ,π) becomes
K of v to be non-negative. The cost-lower confidence bound
m+1 m+1 (LCB) cost vector v(ρ,a) is computed element-wise as:
(cid:88) (cid:88)
v(ρ ,π )= E[c|s ,π ;Θ]=E[C|s ,π;θ ].
K,i i K,i i K sK,π
i=0 i=0 v (ρ,a)=max (cid:26) 0, E[c|s,a;E[θ˜ ]] −α (cid:113) log(k )/n(s,a) (cid:27)
Therefore, by assigning v(ρ,a) = E[c|s,a;Θ], the multi- i s,a i g
objective Pareto front of the graph search problem on P
where α is a “confidence” hyperparameter, k is the current
is exactly equal to Ω(s ,ϕ) (see Def. 5). Then, existing g
K
global time step across all decision instances, and n(s,a) is
algorithms such as a multi-objective variant of Dijkstra’s
algorithmorA∗[35]canbeemployedtocomputeΩ.Notethat the number of times action a has been executed from state s.
these multi-objective graph search algorithms are exponential Note that many multi-objective multi-armed bandit con-
in the worst case, however the average branching factor for fidence bound approaches provide theoretical regret bounds
a N-dimensional vector belonging to a set of size G, i.e. that are logarithmic with respect to the number of decision
the set of all unique cost vector edge weights, is reduced to instances. It is not straight forward to extend these regret
O((log|G|)N−1),withcertainconditionsontheordering[35]. bounds to this framework due to the exponential nature of the
However, Θ is unknown. A naive approach simply uses planningspaceΠ(s K ,ϕ)whichhassizeO(|A||P|).Hence,we
learned estimates E[Θ˜] for v instead, but that suffers from leave theoretical regret bounds to future research.
biasing the solution towards taking transitions that have low Usingthetechniquesdescribedabove,wecannowcompute
estimatecost,regardlessofhowaccurateΘ˜ is.Toaddressthis, the approximate set of Pareto optimal plans Π˜⋆(s K ,ϕ) while
we interweave exploration into the graph search problem, as accountingforexplorationoftheenvironment.Theexploration
detailed below. in planning aids in the effort in addressing goals (ii) and (iii)
3) Pareto-Regret: The efficacy of learning can be analyzed ofProblem1,however,toupholdproperlearningofthePareto
withthenotionofcumulative“Pareto-regret”,quantifyinghow front (iii), the selection must also explore among trade-offs.
sub-optimal each plan is with respect to the true Pareto front
Ω(s ,ϕ). Formally, given π with true mean cumulative cost
K
µ =E[C|s ,π;Θ], the Pareto regret for a given instance C. Pareto Point Selection
sK,π K
is defined as
For the selection problem, one can think of candidate
r(s ,π)= plans π ∈ Π˜⋆(s ,ϕ) computed by the planner as abstract
K K
min{µ −ϵ1|µ⋆ ̸≺µ −ϵ1 ∀µ⋆ ∈Ω(s ,ϕ)}, (5) macro-actions, such that high-level decision making can be
ϵ
sK,π sK,π K
done sequentially for each instance. Fig. 4 provides a visual
where 1 is a vector of ones with appropriate dimension. Note representationoftheabstractedselectionproblemwithrespect
that if π ∈ Π⋆(s ,ϕ), then r(s ,π) = 0. The cumulative to the evolution of the robotic system over each instance.
K K
regretuptoinstanceK issimplythesummedregretforevery AsdescribedinSec.II-D,ideally,theagentshouldselectan
instance,i.e.R=
(cid:80)K
r(s ,π ).Tominimizecumulativere- action minimizing the surprise of outcomes. However, since it
i=0 i i
gret,weaugmentthetransitioncost-vectorwithanexploration is analytically intractable to marginalize the joint distribution
strategy. to derive the surprise, its upper bound, i.e., free energy, is
4) ParetoCost-LCB: WeadaptthewellestablishedPareto- minimized. Yet, the outcomes C cannot be observed until a
UCB1 [15, 36] strategy for computing an estimate Pareto planπisactuallyexecuted,thus,theagentsendupminimizing
front that balances the current best estimate cost model (ex- the so-called expected variational free energy (EFE) in active
ploitation) with a bonus reduction in cost (exploration). The inferencebaseddecision-making.InthisParetopointselection
learned mean transition-cost is calculated using the estimated problem,theEFEisdescribedasfollows(seeAppendixAfor
7
detailed derivation). E[θ˜ ]. This certainty-equivalence approximation makes
sK,π⋆
(cid:104) (cid:105) q(C|π⋆) a simple multivariate normal distribution
EFE(s K ,π⋆,p pr )=−E q(C|π⋆) logp pr (C) (cid:90)
q(C|π⋆)= q(C,θ˜ |π⋆)dθ˜
−E q(C|π⋆)
(cid:104)
D KL
(cid:16)
q(s K |C,π⋆)||q(s K |π⋆)
(cid:17)(cid:105)
θ˜ sK,π⋆
sK,π⋆ sK,π⋆
(cid:104) (cid:16) (cid:17)(cid:105) ≈q(C|π⋆;E[θ˜ ]). (10)
−E D q(θ˜ |C,s ,π⋆)||q(θ˜ |s ,π⋆) , sK,π⋆
q(C|π⋆) KL sK,π⋆ K sK,π⋆ K
The first term can then be calculated as follows
(6) (cid:104) (cid:105)
−E logp (C) ≈log((2π)−k/2|Σ |−1/2)
q(C|π⋆) pr pr
where q(·) represents a proposal distribution. EFE is a varia-
tionalquantity[37],yieldingfreedominthechoiceofproposal + 1(cid:0) µT Σ−1µ +tr(Σ−1Σ )+µT Σ−1µ (cid:1)
distribution q(θ˜ |s ,π⋆). As q(θ˜ |s ,π⋆) more closely 2 pr pr pr pr sK,π⋆ sK,π⋆ pr sK,π⋆
resembles p(θ˜
sK
sK
,π
,π |C, K s
K
,π⋆), the u s p K p , e π r b K ound on (expected) −µT pr Σ− pr 1µ sK,π⋆ , (11)
surprise decreases. The first term of EFE in (6) represents where µ and Σ are the mean vector and the covariance
pr pr
how much the predicted cost distribution q(C|π⋆) aligns with matrix of the user’s preference distribution (see Appendix B
p (C) (i.e. exploitation), and the second and third terms for detailed derivation).
pr
representhowmuchtheuncertaintiesofs andθ˜ canbe 2) Calculating the Second Term of (7): The convolved dis-
reduced by following π⋆ and measuring
K
C (i.e. e
sK
xp
,π
lo
⋆
ration). tributionp(θ˜ sK,π⋆ |s K ,π⋆)isanalyticallyintractable.However,
Since the user does not know the true Pareto-optimal region we can leverage the CLT to determine a suitable proposal
a priori, EFE cannot be used to scalarize any plan in Π, distributionq(·)thatcloselymodelsp(·).Sinceeachp(θ˜ )is
s,a
as it may often be sub-optimal. Therefore, as described in NIW-distributed, both the mean and variance are well-defined
Sec. V-A, the preferred plan π =argmin EFE(π,p ) is when κ > 0 and ν > N + 1. Therefore, under the CLT,
pr π∈Π pr
not guaranteed to also be an element in Π⋆ and vice versa. p(θ˜ sK,π⋆ |s K ,π⋆) tends towards a MVN distribution as the
Consequently, in order to ensure that the user’s preference plan length |π⋆| becomes large [41].
does not bias the system away from optimal behavior, this Consider the following vectorization of the parameters θ
s,a
preference is only expressed over Pareto-optimal candidates.
vec(θ )=(µ ,...,µ ,σ2 ,...,σ2 ), (12)
So, Π⋆(s ,ϕ) is scalarized via EFE in order to adhere to p s,a 1 N 1,1 N,N
K pr
and reduce uncertainties in the belief over θ˜ sK,π⋆ . Since T where µ 1 ,...,µ N represent the components of µ s,a , and
models deterministic transitions, the second term of (6) can σ2 ,...,σ2 are the unique upper triangular elements of
1,1 N,N
be ignored. Expanding the third term of (6) yields Σ . In total, the vectorization has dimension N(N +3)/2.
s,a
(cid:104) (cid:105) Due to the linearity of expectation and independence between
EFE(s ,π⋆,p )=−E logp (C)
K pr q(C|π⋆) pr state-action pairs,
(cid:104) (cid:105)
−H[q(θ˜ |s ,π⋆)]+E H[q(θ˜ |C,s ,π⋆)] , m
sK,π K q(C|π⋆) sK,π⋆ K E[θ˜ ]= (cid:88) E[θ˜ ], (13)
(7) sK,π sk,ak
k=0
whereH[·]representstheentropyofapdf.Then,thepreferred var[θ˜ ]= (cid:88) m var[θ˜ ]. (14)
plan is sK,π sk,ak
π⋆ = argmin EFE(s ,π⋆,p ). (8) k=0
pr K pr
π⋆∈Π⋆(sK) Therefore, the distribution p(θ˜
sK,π⋆
|s
K
,π⋆) can be approx-
DuetotheMarkovpropertyofDTS-scand(1),µ and imately represented by multivariate normal proposal distri-
sK,π⋆
Σ of the convolved distribution p(C|s ,π⋆;θ ) are bution parameterized by the vectorized cumulative mean and
pa s r K am ,π e ⋆ terized by (cid:80) µ and (cid:80) Σ K of each sK c , o π s ⋆ t dis- variance
k sk,ak k sk,ak
tributionrespectively.Accordingtotheestimatetransitioncost q(θ˜ |s ,π⋆)≡N (cid:0)(cid:88) E[vec(θ˜ )], (cid:88) var[vec(θ˜ )] (cid:1) .
model, θ˜ ={µ˜ ,Σ˜ } are random variables themselves; sK,π⋆ K sk,ak sk,ak
s,a s,a s,a
k k
therefore,
(15)
p(θ˜ sK,π⋆ |s K ,π⋆)=p(θ˜ s0,a0 )∗...∗p(θ˜ sm,am ). (9) Since q(θ˜
sK,π⋆
|s
K
,π⋆) is MVN, the entropy of the proposed
As described in Section III-A, each p(θ˜ ) is a unique distribution (second term) can be represented analytically
s,a
rewritten as follows.
NIW, which makes (9) analytically intractable. To enable
computation for π⋆ in (8), we leverage both the functional H[q(θ˜ |s ,π⋆)]= 1 log (cid:0) det (cid:0)(cid:88) var[vec(θ˜ )] (cid:1)(cid:1)
pr sK,π K 2 sk,ak
freedominchoosingproposaldistributionsq(·)aswellasthree
k
statistical approximations of (7): certainty equivalence of the N(N +3)
+ (1+log2π). (16)
predicted observation distributions [38], central limit theorem 4
(CLT) [39], and Monte Carlo sampling [40]. 3) Approximating the Third Term of (7): Using both the
1) Approximating the First Term of (7): Recall certainty equivalence approximation in (10) and the CLT
p(θ˜ |s ,π⋆) is the belief over parameters of the approximation in (15), the remaining third term can be ap-
sK,π⋆ K
predicted observation distribution q(C|π⋆). To address the proximated using Monte Carlo sampling of the expectation.
intractability, we can extract the current estimate parameters To sample the expectation, a cumulative cost vector must
8
be sampled from q(C|π⋆). Using (10), this sample (denoted is a NIW parameterized as follows.
cˆ) can be drawn from the MVN Cˆ ∼ q(C|π⋆;E[θ˜ ]).
However, note that the posterior p(θ˜ |C,s ,π⋆) s i K s , i π ts ⋆ elf λ= κ 0 λ 0 +lc¯ , (18)
sK,π⋆ K κ +l
aconvolvedNIWconditionedonc.Toconstructananalytical 0
κ=κ +l, (19)
approximation of the posterior, we must instead expand the 0
κ l
sampling procedure to each state-action distribution. Instead Λ=Λ + 0 (c¯−λ )(c¯−λ )T+ (20)
ofsamplingCˆ directlyfromq(C|π⋆;E[θ˜ sK,π⋆ ]),cˆcaninstead 0 κ 0 +l 0 0
be constructed from the observation distribution for each state l
(cid:88)
(cˆ −c¯)(cˆ −c¯)T,
action pair j j
j=0
ν =ν +l, (21)
0
Cˆ = (cid:88) cˆ sk,ak , (17) By iteratively performing each described phase, the agent
k can safely complete the task, guided by the user’s preference.
Through the iterative combination of exploratory planning,
surprised-based selection, execution, and Bayesian update of
t w h h e er r e es cˆ p s e k c ,a ti k v ∼ e p p ( o c s | t s e k ri , o a r k ; p E (θ [ ˜ θ˜ sk,ak |cˆ ]). For ) ea a c re h s c a o m m p p l u e te cˆ d sk, u ak s- , c a o ll st th d r i e st e rib g u o t a i l o s ns d , e t s h c e ri p b r e o d po in sed Pr M ob O . R 1 L . f N ra o m w e , w w o e rk e c v a a n lu a a c te hie th v e e
sk,ak sk,ak
ing the equations found in Sec. III-D. The distribution empirical efficacy through experiments.
p(θ˜ |cˆ ) is the conjugate NIW posterior. Therefore,
sk,ak sk,ak
using the same process described in Sec. III-C2, an approx-
IV. EXPERIMENTS
imation of H[q(θ˜ |Cˆ,s ,π⋆)] can be calculated. This
sK,π K
To evaluate the effectiveness of our MORL framework for
sampling procedure is repeated n times to compute the third
s
autonomous robotic decision making in unknown environ-
term of (7). Using the aforementioned techniques, we can
ments,weperformedthreecasestudies:(i)anillustrativesim-
optimize for π⋆ in (8). While this sampling procedure is
pr ulation study, (ii) two numerical benchmarking experiments
computationally burdensome, the selection procedure runs in
O(n m|Π˜⋆|), linear in the size of number of candidate Pareto forcomparisonagainstthestate-of-the-art,and(iii)ahardware
s
experiment to demonstrate the real-world applicability of the
optimal plans.
method1.
Note that unlike the prior distribution in the second term
of (7), the proposal posterior q(θ˜ |C,s ,π⋆) is not a
sK,π⋆ K
A. Simulated Mars Surface Exploration Study
variational proposal distribution. In fact, the approximation
of true posterior p(θ˜ |C,s ,π⋆) with q(θ˜ |C,s ,π⋆) 1) Motivation: Suppose a Mars rover is tasked to collect
sK,π⋆ K sK,π⋆ K
is an approximation inherent to active inference [31], as scientifically interesting minerals from target sample sites
p(θ˜ |C,s ,π⋆) is often not analytically tractable. This (orange region in Fig. 5), designated based on past mission
sK,π⋆ K
approximation intuitively relies on the assumption that the in- data [42]. Due to the limited number of sample tubes, it is
ternalmodeloftheagentisaccurateenoughtowellpredictthe required to go back and forth between the deposit location
true posterior. Our empirical evaluations in Appendix C show (green region) and the target site. Since the closer sampling
that the approximation of convolved NIW with convolved regionisinthesun,therovermusttrade-offbetweenlow-time,
MVNagreeswiththeCLT.Thatis,theapproximationisfairly high-radiation for efficient science data collection, or high-
accurate for the typical plan length seen in the experiments time, low-radiation to avoid long-term damage. Therefore, to
in Sec. IV, and increases as the plan length increases (Fig. protect itself while gaining sufficient amount of science, it
9a) and more data is collected (Fig. 9b). Additionally, refer to muststrikeabalancebetweenthetwo.Moreover,althoughthe
AppendixDforanerroranalysisoftheMonteCarlosampling roverknowsthequalitativeinformationabouttheenvironment
procedure described in Sec. III-C3 with respect to the number (e.g., where sand, base, washing station, etc. are located), it is
of samples n , plan length, and collected data. still necessary to estimate online the time and radiation cost
s
of collecting a sample. Informed by past mission data and
scientific expertise, the user can tune an appropriate p .
pr
2) Simulation Setup: For the sake of simplicity, the rover
can travel in cardinal directions, and may not move through
obstacles (black). The task specification is given as
D. Execution and Parameter Update
ϕ =F(sample ∧ F(deposit))∧
MR
After obtaining the preferred optimal trade-off plan π⋆ , G(sand→(¬baseUwash));
pr
it is executed on the robotic platform (or in simulation).
in English, “collect a sample, then deposit it, and, if sand
During the execution, measurements for the cost of each
is visited then wash before returning to base.” Each transition
state-action cˆ are collected. Recall, the cost parameters are
distributed as p(θ˜ |D )=NIW (λ,κ,Λ,ν). Given a set
s,a s,a s,a 1To see a video of the simulation and hardware experiments, visit https:
of measurements D s,a = {cˆ 0 ,...cˆ l }, the conjugate posterior //youtu.be/tCRJwqeT-f4
9
Instance 3 Instance 25 Instance 35 Instance 150
Base
Deposit
Sun
Wash
Sample
Sand
Candidate
Chosen
50 Candidate
LCB
Chosen
40 True
ppr
30
20
10
0
0 25 50 75 100
Time (m)
)yGµ(
noitaidaR
50 50 50
40 40 40
30 30 30
20 20 20
10 10 10
0 0 0
0 25 50 75 100 0 25 50 75 100 0 25 50 75 100
Time (m) Time (m) Time (m)
(a) Instance 3 (b) Instance 25 (c) Instance 35 (d) Instance 150
Fig. 5: Simulated rover sample collection: A rover is tasked with repetitively collecting a sample and delivering it to deposit.
Moving takes time and collects radiation (in the sun). The user prefers that the sample is collected in roughly 90 minutes with
about 6 µGy of radiation, represented with p . Top row figures show the computed plans, and the bottom row figures show
pr
the estimated Pareto points. The true optimal trade-off plans are shown for comparison in four instance (episode) snapshots.
Video: https://youtu.be/tCRJwqeT-f4.
takestime(objective1)measuredinminutes,andtransitionsin explore a less preferred, information rich trade-off. Finally,
the sun accumulate harmful radiation (objective 2) measured after 150 instances, seen in Fig. 5d, the robot has obtained
in micrograys. The true mean costs for transitions in each a fairly accurate estimate of the true Pareto front, and has
region are given as follows: converged to often selecting the most preferred trade-off.
µ sample(left) =(6,16) (in the sun), B. Benchmarks
µ sample(right) =(6,0) (out of the sun), We evaluate the performance of our framework against the
µ =(3,7) (slows the rover in the sun), state-of-the-art in two benchmark problems. The goal is to
sand
assess in each instance: (i) how close the selected plan π⋆ is
µ wash =(11,31) (long duration of radiation exposure), pr
to being Pareto optimal, and (ii) how well Π⋆ is represented
µ sun = (1,1), and all other transitions have µ = (1,0). by Π˜⋆. Metric cumulative Pareto-regret directly evaluates (i).
Covariance values are omitted for brevity. The user informs To properly evaluate (ii), we define Pareto-bias metric
the prior transition cost for all transitions to be λ 0 =(0.5,0). for a true Pareto front Ω and an estimate Pareto front Ω˜ as
For thelongevity ofthe rover, theuser aimsto endure asmall 1 (cid:88) 1 (cid:88)
B= min(d(C ,C ))+ min(d(C ,C )),
a
in
m
r
o
o
u
u
n
g
t
h
o
ly
f r
o
a
n
d
e
iat
a
i
n
o
d
n
a
an
h
d
al
p
f
re
h
f
o
e
u
rs
rs
t
.
h
T
at
o
th
ex
e
p
m
re
i
s
s
s
sio
th
n
is
i
,
s
t
c
h
o
e
m
c
p
h
l
o
e
s
t
e
e
n
d |Ω|
Ci∈Ω
Cj∈Ω˜ i j |Ω˜|
Ci∈Ω˜
Cj∈Ω j i
(22)
preference distribution is
where
(cid:16) (cid:17)
N(µ pr ,Σ pr ), µ pr =(90,6) d(C ,C )=W p(C |s ,π ;θ ),q(C |s ,π ;E[θ˜ ])
i j 2 i K i sK,πi j K j sK,πj
σ =140, σ =σ =−2, σ =70,
1,1 1,2 2,1 2,2
is the Wasserstein-2 (W ) distance. The first term in (22)
2
where σ is the i-j element of Σ . Soon after deployment quantifies the estimate Pareto front’s coverage of the true
i,j pr
(instance 3), the planner only finds candidate plans that go Pareto front, and the second term penalizes outlier/excess
through sand to collect the sample, and then must wash off, trade-offsintheestimateParetofront.IfeachtrueParetopoint
seen in Fig. 5a. The robot quickly learns to avoid sand as is exactly similar to an estimate Pareto point, and visa versa,
washing takes very long and collects lots of radiation. The then B=0, otherwise B>0.
robot gradually discovers more optimal behavior seen by We benchmarked our active inference (AIF) selection
instance 25 in Fig. 5b. In instance 35, seen in Fig. 5c, the method against other state-of-the-art selection methods: uni-
rover has roughly learned the true Pareto front, and elects to form random selection (fair selection), TOPSIS [8] (objective
10
600
400
200
0
0 50 100 150 200 250 300
tergeR
evitalumuC
50
40
30
20
10
0
0 50 100 150 200 250 300
Instance
saiB
evitalumuC
1000
750
500
250
0
0 100 200 300
1e4
TOPSIS
Weights
AIF (no var)
AIF (small var)
AIF (medium var)
AIF (large var)
Uniform
(a) 10 true Pareto points
tergeR
evitalumuC
300
200
100
0
0 100 200 300
Instance
saiB
evitalumuC
1000
750
500
250
0
0 100 200 300
1e4
(b) 32 true Pareto points
tergeR
evitalumuC
150
100
50
0
0 100 200 300
Instance
saiB
evitalumuC
1e4
(c) 72 true Pareto points
Fig. 6: Specific environment benchmark: Within our framework, we benchmark our active inference (AIF) selection method
against other state-of-the-art methods with respect to cumulative Pareto-regret (top) and Pareto-bias (bottom) for three fixed
environments with varying numbers of true Pareto points. The mean is represented by the solid line, with 1-σ variance shown
by the shaded region. The dark shaded bar aids in distinguishing overlapping variance bands.
preference),andlinearscalarization,i.e.“weights”(subjective wecanobservethatAIFmethodsstartbecomingveryselective
preference). Refer to Sec. V for further description of the once the preferred region (which can be varied by the size of
compared methods. variance) on the Pareto front is discovered.
1) Fixed Environment Case Study: Three grid world envi- 2) Random Environment Case Study: The behavior of
ronments were designed to challenge the learning of different our AIF selection method can vary widely across different
sizeParetofronts.Allexperimentswererunfor300instances, scenarios depending on the diversity of the Pareto front
with a LCB confidence parameter α = 0.1, and n s = 300 relative to p pr . Therefore, we illustrate the general behavior
Monte Carlo samples, selected via cross validation. See Ap- of our approach using randomized grid worlds. Each trial, a
pendix D for more details on the accuracy with respect to 20×20 grid world robot model is generated with randomized
the choice of n s . Fig. 6 shows benchmarks against each envi- propositionlocationsandtruecostdistributionparameters.The
ronment.Generally,methodsthataccumulatelessbiasrequire data was analyzed across 750 randomized trials.
learningmoreofthetransitioncostmodelviaexploration,and
Fig. 7 demonstrates the general trade-off between bias
henceaccumulatemoreregret.WhenlearningthesmallPareto
and regret. Note that the standard deviation in performance
front,seeninFig.6a,AIFperformedbetterthanbothuniform
among trials is very large, since each randomly generated
and the biased methods (weights and TOPSIS) in terms of
environment is diverse. When the variance of p is large,
pr
bias, with comparable regret to uniform. This is due to the
activeinferencetendstoperformsimilarlytorandomselection
advantageous information-gain-based exploration. When the
(Uniform) in terms of both regret and bias. With a moderate
number of Pareto points on the Pareto front increases, seen in
variance, active inference tends to have better regret and
Fig.6b,usingsmallandno2varianceselectsforonlyaportion
bias performance than other methods due to the intelligent
of the true Pareto front, while the medium and large variance information theoretic exploration when the estimate plans Π˜⋆
stillenvelopetheentireParetofront.Withthesmallervariance
are uninformed. Uniform, AIF (large var), and AIF (medium
AIFmethods,onceagoodestimatecandidateisfound,thefirst
var)stabilizeincumulativebias,meaningtheyeffectivelylearn
termof(7)dominatestheselection,andthusabandonstheless
the entire Pareto front. The remaining methods converge to a
promising options that are beyond the variance. With an even
non-constantslopecumulativebias,indicatingthatportionsof
larger Pareto front seen in Fig. 6c, this effect is exacerbated.
the Pareto front have not been properly learned. From this
In this case, the no variance method sees a decrease in regret
experiment, we can observe that AIF methods outperform
duetotheheavymyopicbiasafterfindingacandidatesolution
other state-of-the-art methods in terms of both cumulative
that agrees with the preference. Thus, from this experiment,
regrets and biases as long as the size of variance is not too
large.
2Iftheuseronlywantstospecifyadesiredpointinobjective-space,they
canmakethepreferencedistributioncovarianceverysmall(novariance) Akin to other expected free energy approaches, the en-
11
2500
2000
1500
1000
500
0
0 100 200 300
tergeR
evitalumuC
12.5
10.0
7.5
5.0
2.5
0.0
0 100 200 300
Instance
saiB
evitalumuC
repetitively load and unload the dishwasher with two items, a
durable pitcher, and a fragile glass jar. The LTLf specification
is
ϕ=ϕ ∧ϕ ,
l s
where
ϕ =F(P ∧J ∧lid ∧F(P ∨(P ∧FP )∧
l dw dw on rack dry floor
F(J ∨(J ∧FJ ))),
rack dry floor
ϕ =G(¬lid →F(P ∧J )),
s aside dw dw
whichcanbeinterpretedas“putthepitcherandthejarinthe
1e4 dishwasher, then put the lid on, then for both the pitcher and
jar, place on the drying rack, or dry the item and place it on
TOPSIS
Weights theground”.Theperformanceoftherobotisevaluatedbythe
AIF (no var) cumulativeexecutiontime(objective1),aswellascumulative
AIF (small var) risk (objective 2) measured for a given action primitive,
AIF (medium var)
AIF (large var)
(cid:40)(cid:82)
h2dt if holding jar,
Uniform risk(s,a)= t J (23)
0 otherwise,
whereh istheheightofthejarabovetheground.Asampling
J
based motion planner (PRM∗ [47]) is used to realize motion
actionprimitives,inducingunknownandrandomtimeandrisk
costs. Note that placing the jar on the drying rack is likely
to have high risk, whereas manually drying the jar on the
Fig. 7: Randomized environment benchmark: We benchmark ground will likely take more time. We trained the transition
our method against other selection methods for randomly costmodelinsimulationfor500instances,usingapreference
generated scenarios of varying complexity. distribution of µ = (350,0.5) and σ2 = 400, σ2 = 2,
pr 1,1 2,2
σ σ =0, which selects for a high-time, low-risk alternative.
1 2
hanced reasoning power comes at a computational cost com- TheconvergedbehaviorisshowninFig.8.Ascanbeseen,the
pared to other methods [20, 31]. Uniform selection has com- robot correctly loads the dishwasher, then unloads the pitcher
plexityO(1),weightshasO(|Π˜⋆|),andTOPSIShasO(|Π˜⋆|2)
on the rack, and decides to manually dry the jar, all while not
[43]. We benchmarked the aforementioned random environ- performinganysub-optimal/unnecessaryactions.Thevideoof
ment case studies with respect to both planning time and the execution is included in the supplementary material and
selection time per instance on a computer with AMD Ryzen visually represents the applicability of our MORL framework
7 3800X 3.9 GHz 8-Core Processor. The average planning to a variety of robotic models with complex tasks.
time is across 400 experiments with 100 instances each is
483±443ms (1-σ) with a median planning time of 369ms.
V. RELATEDWORK
The average AIF selection time (across all four preference
A. Multi-Objective Reinforcement Learning
distribution variances) is 610±1051ms (1-σ) with a median
selection time of 179ms. The compared selection procedures The goal of MORL is to maximize a multi-objective value
completed in less than 1ms. or utility function [7]. MORL extends single-objective RL
methodsbyselectingamongPareto-optimalcandidateactions.
One method of selection is casting valuations of candidate
Remark 1. For larger-scale robotic models, the exact multi-
actions to a single-objective by scalarizing, in turn requiring
objective graph search can be replaced with approximate
a sound and interpretable utility function.
multi-objectivesearch,suchasA∗pex[44],toadmitasmaller
Scalarization of utility functions can be classified as sub-
(approximate) Pareto front in less time. Additionally, a clus-
jective, scalarizing based on an exogenous user’s preferred
tering algorithm [45, 46] can further reduce the size of the
trade-off [6], or non-subjective, scalarizing without external
Pareto front. We expect both adaptations to our framework to
preferences[5,8].Linearscalarizationisastandardsubjective
greatlyenhancecomputationspeedatthecostofPareto-regret
methodthatcombinesobjectivesobjectivesthrougharelative-
and bias performance, however we leave this to future work.
importance weighted sum. On the contrary, non-subjective
methods such as TOPSIS [8] and hyper-volume indicators [5]
relyonEuclidiandistancemetricsinobjectivespace.However,
C. Hardware Experiment
in many robotic applications where objectives have different
To demonstrate the efficacy and diversity of application of units of measurement (e.g. time in seconds and energy in
ourframework,westudiedacomplexdishwashingexperiment joules), these methods use the addition of quantities with
usingaroboticmanipulatordescribedinFig.1.Therobotmust different units, which is generally mathematically ad-hoc and
12
Fig.8:Hardwaredishwashingexperiment:Aroboticmanipulatormustloadbothadurablepitcherandafragileglassjarintoa
dishwasher,then mustunloadeachitem byeitherplacing itonthedrying rack,ormanually dryingitonthe ground.Therobot
learns to minimize time and risk (holding the jar high above the ground). The user prefers a low-risk, higher-time trade-off.
After 500 instances, the system performs optimally (left to right, top to bottom), and decides to quickly place the pitcher on
the rack, and spend time to manually dry the jar. Video: https://youtu.be/tCRJwqeT-f4.
lacks interpretation. Linear scalarization can potentially com- by diversity in selection and learning of all actions on a
bat this issue by interpreting weights as reciprocal measures possibly non-convex Pareto-front. To learn the entire Pareto
(e.g. 1/second), at the cost of requiring a user to have a front,work[15]extendsthesingle-objectiveUpperConfidence
rigorous understanding of how to compare the importance Bound (UCB) algorithm [36] to multiple objectives, then rely
of each objective and properly assign weights. Furthermore, on random selection among the Pareto front. While random
re-interpreting units of measurement (e.g. seconds instead of selection effectively learns the entire Pareto front, it cannot
milliseconds)altersEuclidiandistance,whichcandramatically accountfororconvergetoapreferredtrade-off.Toaddressthis
altertheselectedsolution,whennofundamentalaspectsabout shortcoming,ourformulationofthehigh-levelactiveinference
the problem have changed. On the contrary, our method decision making agent naturally balances this exploration vs.
avoids this issue by only describing the scalarization through exploitation trade-off.
operations on probability density functions.
Many MORL methods do not distinguish between optimiz-
B. Active Inference and Expected Free Energy
ingmulti-objectivevaluefunctionandselectingthebestaction
from the Pareto-optimal candidate actions [6, 7]. Instead, Activeinferenceisanuncertainty-basedsequentialdecision
these methods scalarize using a monotonic utility function, making scheme that applies the free energy principle (FEP)
which guarantees that an optimal (preferred) action with [48] to the behavioral norms of physical agents [19, 20, 21].
respect to the single-objective utility function must also be According to the FEP, agents intend to minimize the surprise
Pareto-optimal in objective space. The use of such a utility (in our problem it is denoted as −logp(C|π)) to maintain
functioneffectivelyoptimizesasingleobjective,whichcastsa their homeostases. However, calculating the surprise directly
completeorderontherewardspaceandeliminatestheneedfor requires to sum/integrate over all possible states/parameters
explicit computation of the Pareto front beforehand. However, in their internal generative models. When these quantities
when a non-monotonic utility function is used, or the agent are continuous, depending on the prior and/or the likelihood
is concerned with learning the set of all optimal trade-off function, the integral does not have a closed-form solution.
solutions [15], the Pareto front must be explicitly computed Thus, some approximation methods are required to calculate
before selection, as seen in many MOMAB approaches. Our the surprise.
framework computes the Pareto front using a multi-objective One of the approaches is to employ variational Bayesian
task-planner. inference in statistical machine learning [40]. The core idea
MOMAB problems are concerned with minimizing Pareto- of this methodology is to introduce a proposal/surrogate dis-
Regret [15], which uses ε-dominance to measure how “far” tribution,whichiseasilymodeled,andoptimizethefunctional
the chosen action is from being Pareto-optimal with respect ofthisdistributiontomakeitresembleatrueintractabledistri-
to the true unknown reward distributions. Besides Pareto- bution. More specifically, in our problem, the following free-
regret, the MOMAB performance can also be evaluated by energy (FE, i.e. negative evidence lower bound) is minimized
fairness, which quantifies how evenly Pareto-optimal actions such that the Kullback-Leibler Divergence (KLD) between
wereselected.FairnessisusedtocompareMOMABselection the proposal distribution q(s,θ˜|π) and the true distribution
13
p(s,θ˜|C,π) approaches zero (Note that here we used the and we examine the resulting behavior through robotic simu-
simplified notations seen in Appendices A and B). lation and hardware experiments, and numerical benchmarks
that compare against other state-of-the-art selection methods.
(cid:90) q(s,θ˜|π)
FE = q(s,θ˜|π)log dθ˜ds Ourapproachmarriesthebenefitsofauser-providedpreferred
s,θ˜
p(s,θ˜,C|π)
optimaltrade-offwithexplorationrequiredtolearnportionsof
(cid:90) q(s,θ˜|π) the entire Pareto front. We introduce a new Pareto-bias metric
= q(s,θ˜|π)log dθ˜ds−logp(C|π)
s,θ˜
p(s,θ˜|C,π) that, coupled with traditional Pareto-regret, elucidates the
(cid:16) (cid:17) trade-off between accurately learning the diverse Pareto front
=D q(s,θ˜|π)||p(s,θ|C,π) −logp(C|π)
KL versus quickly learning a single optimal trade-off. Notably,
≥Surprise(π). (24) the balance between the aforementioned behaviors can be
controlled via the user’s preference.
As can be seen from this equation, since KLD is greater or This work gives rise to a few future directions of research
equal to zero, FE is the upper bound of the surprise, i.e. suchasextensiontostochastictransitionmodels.Additionally,
minimizing FE leads to minimize the surprise. integrating cost-bounds for certain objectives by embedding a
ComputationofFEisperformedgivenanobservedoutcome pruning procedure [14] into the multi-objective graph search
observation C, which is not known until the plan is executed. algorithm is an interesting extension. Since the costs are not
Hence, in active inference, the expected free energy (EFE) known a priori, we expect the Pareto cost upper confidence
is instead minimized, and similar to FE, if the proposal and bound to be an effective choice of pruning constraint. We
the true hidden posterior distributions match, the EFE term are also interested in generalizing the cost distribution to a
reduces to the following, Gaussian mixture to account for multi-modality. Other future
directions may include convergence analysis and finite time
EFE →−E [logp (C)], (25)
q(C|π) pr regret analysis, for example, adapting the Upper Credible
which is “expected” surprise. In summary, FE allows to Limit algorithm [33] for Gaussian multi-armed bandits to the
make the surprise minimization tractable for continuous multi-objective setting.
state/parameter systems, and EFE allows for the minimization
FE without knowing the received observation a priori.
REFERENCES
Additionally, as explained in (6), the EFE comprises of an
[1] Yang Gao and Steve Chien. Review on space robotics:
utility term governed by a prior agent’s desired observation
toward top-level science through space exploration. Sci-
distribution, and an information gain term that evaluates how
ence Robotics, 2(7), 2017.
much a candidate action would reduce the uncertainty of hid-
[2] Jaeseok Kim, Anand Kumar Mishra, Raffaele Limosani,
den states/parameters. Hence, by minimizing EFE, the agents
Marco Scafuro, Nino Cauli, Jose Santos-Victor, Barbara
can naturally balance between two modes of behavior that are
Mazzolai, and Filippo Cavallo. Control strategies for
essential for autonomous decision making under uncertainty;
cleaning robots in domestic applications: A comprehen-
exploitation (i.e. focusing on the current best action) and
sive review. International Journal of Advanced Robotic
exploration(i.e.tryinglessexecutedactions).Furthermore,the
Systems, 16(4):1729881419857432, 2019.
degreeofpreferenceforeithermodecanbeeasilyadjustedby
[3] LydiaEKavraki,PetrSvestka,J-CLatombe,andMarkH
varying the prior preference distribution. For instance, using
Overmars. Probabilistic roadmaps for path planning in
a prior preference with a large variance will cause the agents
high-dimensional configuration spaces. IEEE transac-
to provide more priority to exploration, and vice versa. These
tionsonRoboticsandAutomation,12(4):566–580,1996.
characteristics are particularly useful when transparent agent
[4] SertacKaramanandEmilioFrazzoli. Sampling-basedal-
behavior is desired [49]. Using active inference for selection
gorithms for optimal motion planning. The international
marriesthebenefitsofpreferredversusfairselectiondescribed
journal of robotics research, 30(7):846–894, 2011.
in Sec. V-A through localized information-theoretic explo-
[5] WeijiaWangandMiche`leSebag. Multi-objectiveMonte-
ration around the prior preference distribution. Additionally,
Carlo tree search. In Steven C. H. Hoi and Wray
specifying a preference distribution over desired trade-offs
Buntine, editors, Proceedings of the Asian Conference
maybeparticularlyintuitive,explainable,andevenstatistically
on Machine Learning, volume 25 of Proceedings of
informed with past data. Throughout this work, we study the
Machine Learning Research, pages 507–522, Singapore
benefits of using EFE in active inference for Pareto point
Management University, Singapore, 04–06 Nov 2012.
selection in MORL with two statistical techniques when an
PMLR.
intractable convolved NIW is used to represent the hidden
[6] Axel Abels, Diederik M. Roijers, Tom Lenaerts, Ann
parameters.
Nowe´, and Denis Steckelmacher. Dynamic weights in
multi-objectivedeepreinforcementlearning.InKamalika
VI. CONCLUSIONANDFUTUREWORK
Chaudhuri and Ruslan Salakhutdinov, editors, Proceed-
In this work we study a systematic approach to learn- ings of the 36th International Conference on Machine
ing Pareto-optimal behavior on an unknown multi-objective Learning, ICML 2019, 9-15 June 2019, Long Beach,
stochastic cost model. In particular we examine an active California, USA, volume 97 of Proceedings of Machine
inference-based approach to multi-objective decision making Learning Research, pages 11–20. PMLR, 2019.
14
[7] ConorF.Hayes,RoxanaRa˘dulescu,EugenioBargiacchi, page 3080–3086. AAAI Press, 2019.
Johan Ka¨llstro¨m, Matthew Macfarlane, Mathieu Rey- [19] Karl Friston, Francesco Rigoli, Dimitri Ognibene,
mond, Timothy Verstraeten, Luisa M. Zintgraf, Richard Christoph Mathys, Thomas FitzGerald, and Giovanni
Dazeley, Fredrik Heintz, Enda Howley, Athirai A. Iris- Pezzulo. Activeinferenceandepistemicvalue. Cognitive
sappane, Patrick Mannion, Ann Nowe´, Gabriel Ramos, neuroscience, 02 2015.
Marcello Restelli, Peter Vamplew, and Diederik M. Roi- [20] RaphaelKaplanandKarlJ.Friston.Planningandnaviga-
jers. A practical guide to multi-objective reinforcement tion as active inference. Biol. Cybern., 112(4):323–343,
learning and planning. Autonomous Agents and Multi- aug 2018.
Agent Systems, 36(1), apr 2022. [21] Karl J Friston, Maxwell JD Ramstead, Alex B Kiefer,
[8] Mohammad Mirzanejad, Morteza Ebrahimi, Peter Vam- AlexanderTschantz,ChristopherLBuckley,MahaultAl-
plew, and Hadi Veisi. An online scalarization multi- barracin, Riddhi J Pitliya, Conor Heins, Brennan Klein,
objective reinforcement learning algorithm: Topsis q- Beren Millidge, Dalton AR Sakthivadivel, Toby St Clere
learning. The Knowledge Engineering Review, 37:e7, Smithe, Magnus Koudahl, Safae Essafi Tremblay, Capm
2022. Petersen, Kaiser Fung, Jason G Fox, Steven Swanson,
[9] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, Dan Mapes, and Gabriel Rene´. Designing ecosystems of
George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, intelligence from first principles. Collective Intelligence,
Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft 3(1):26339137231222481, 2024.
actor-critic algorithms and applications. arXiv preprint [22] Shohei Wakayama and Nisar Ahmed. Active inference
arXiv:1812.05905, 2018. for autonomous decision-making with contextual multi-
[10] Saurabh Arora and Prashant Doshi. A survey of in- armed bandits. In 2023 IEEE International Conference
verse reinforcement learning: Challenges, methods and on Robotics and Automation (ICRA), pages 7916–7922,
progress. Artificial Intelligence, 297:103500, 2021. 2023.
[11] Hadas Kress-Gazit, Morteza Lahijanian, and Vasumathi [23] Marius Kloetzer and Calin Belta. A fully automated
Raman. Synthesis for robots: Guarantees and feedback framework for control of linear systems from temporal
for robot behavior. Review of Control, Robotics, and logic specifications. IEEE Transactions on Automatic
Autonomous Systems, 1:211–236, May 2018. Control, 53(1):287–297, 2008.
[12] Christel Baier and Joost-Pieter Katoen. Principles of [24] Georgios E Fainekos, Hadas Kress-Gazit, and George J
Model Checking (Representation and Mind Series). The Pappas. Temporal logic motion planning for mobile
MIT Press, 2008. robots. In Proceedings of the 2005 IEEE International
[13] Giuseppe De Giacomo and Moshe Y Vardi. Linear tem- Conference on Robotics and Automation, pages 2020–
poral logic and linear dynamic logic on finite traces. In 2025. IEEE, 2005.
IJCAI’13 Proceedings of the Twenty-Third international [25] ShuoYang,XiangYin,ShaoyuanLi,andMajidZamani.
joint conference on Artificial Intelligence, pages 854– Secure-by-construction optimal path planning for linear
860. Association for Computing Machinery, 2013. temporal logic tasks. In 2020 59th IEEE Conference on
[14] Peter Amorese and Morteza Lahijanian. Optimal cost- Decision and Control (CDC), pages 4460–4466. IEEE,
preference trade-off planning with multiple temporal 2020.
tasks. In 2023 IEEE/RSJ International Conference on [26] Georgios E Fainekos, Hadas Kress-Gazit, and George J
IntelligentRobotsandSystems(IROS),pages2071–2077, Pappas. Hybridcontrollersforpathplanning:Atemporal
2023. logicapproach. InProceedingsofthe44thIEEEConfer-
[15] Madalina M. Drugan and Ann Nowe. Designing multi- ence on Decision and Control, pages 4885–4890. IEEE,
objective multi-armed bandits algorithms: A study. In 2005.
The 2013 International Joint Conference on Neural Net- [27] Keliang He, Morteza Lahijanian, Lydia E Kavraki, and
works (IJCNN), pages 1–8, 2013. Moshe Y Vardi. Towards manipulation planning with
[16] Ro´bert Busa-Fekete, Bala´zs Szo¨re´nyi, Paul Weng, and temporallogicspecifications. In2015IEEEinternational
Shie Mannor. Multi-objective bandits: Optimizing the conference on robotics and automation (ICRA), pages
generalized gini index. In Proceedings of the 34th 346–352. IEEE, 2015.
InternationalConferenceonMachineLearning-Volume [28] Keliang He, Morteza Lahijanian, E Kavraki, Lydia, and
70, ICML’17, page 625–634. JMLR.org, 2017. Y Vardi, Moshe. Automated abstraction of manipulation
[17] Eralp Turgay, Doruk O¨ner, and Cem Tekin. Multi- domainsforcost-basedreactivesynthesis. IEEERobotics
objective contextual bandit problem with similarity in- and Automation Letters, 4(2):285–292, Apr. 2019.
formation. In Amos J. Storkey and Fernando Pe´rez- [29] Pablo Lanillos, Cristian Meo, Corrado Pezzato,
Cruz, editors, International Conference on Artificial In- Ajith Anil Meera, Mohamed Baioumy, Wataru Ohata,
telligenceandStatistics,AISTATS2018,9-11April2018, Alexander Tschantz, Beren Millidge, Martijn Wisse,
Playa Blanca, Lanzarote, Canary Islands, Spain, vol- Christopher L. Buckley, and Jun Tani. Active inference
ume 84 of Proceedings of Machine Learning Research, in robotics and artificial agents: Survey and challenges.
pages 1673–1681. PMLR, 2018. CoRR, abs/2112.01871, 2021.
[18] Shiyin Lu, Guanghui Wang, Yao Hu, and Lijun Zhang. [30] Thomas M. Cover and Joy A. Thomas. Elements of
Multi-objective generalized linear bandits. IJCAI’19, InformationTheory(WileySeriesinTelecommunications
15
and Signal Processing). Wiley-Interscience, USA, 2006. the pareto optimal front in multi-objective optimization.
[31] Ryan Smith, Karl J. Friston, and Christopher J. Whyte. Computation, 10(3):37, 2022.
A step-by-step tutorial on active inference and its ap- [47] SM LaValle. Planning Algorithms, volume 2. 2006.
plication to empirical data. Journal of Mathematical [48] K. Friston. The free-energy principle: A unified brain
Psychology, 107:102632, 2022. theory? Nature Reviews Neuroscience, 11(2):127–138,
[32] Kevin P. Murphy. Conjugate bayesian analysis of the 2010.
gaussian distribution. 2007. [49] Shohei Wakayama and Nisar Ahmed. Observation-
[33] Paul B Reverdy, Vaibhav Srivastava, and Naomi Ehrich augmented contextual multi-armed bandits for robotic
Leonard. Modeling human decision making in gener- exploration with uncertain semantic data. arXiv,
alized gaussian multiarmed bandits. Proceedings of the (2312.12583), 2023.
IEEE, 102(4):544–571, 2014. [50] Robert A. Stine. Explaining normal quantile-quantile
[34] Yen-Chi Chen. A tutorial on kernel density estimation plots through animation: The water-filling analogy. The
and recent advances. Biostatistics & Epidemiology, American Statistician, 71(2):145–147, 2017.
1(1):161–187, 2017. [51] Zafeirios Fountas, Noor Sajid, Pedro Mediano, and Karl
[35] Lawrence Mandow and Jose´ Luis Pe´rez De La Cruz. Friston. Deep active inference agents using monte-carlo
Multiobjectivea*searchwithconsistentheuristics. Jour- methods. Advances in neural information processing
nal of the ACM (JACM), 57(5):1–25, 2008. systems, 33:11662–11675, 2020.
[36] Peter Auer, Nicolo` Cesa-Bianchi, and Paul Fischer. [52] Alexander Tschantz, Manuel Baltieri, Anil. K. Seth, and
Finite-time analysis of the multiarmed bandit problem. Christopher L. Buckley. Scaling active inference. In
Mach. Learn., 47(2–3):235–256, may 2002. 2020InternationalJointConferenceonNeuralNetworks
[37] David M Blei, Alp Kucukelbir, and Jon D McAuliffe. (IJCNN), pages 1–8, 2020.
Variational inference: A review for statisticians. Journal [53] Domenico Maisto, Francesco Gregoretti, Karl J. Friston,
of the American statistical Association, 112(518):859– and Giovanni Pezzulo. Active tree search in large
877, 2017. pomdps. ArXiv, abs/2103.13860, 2021.
[38] HoriaMania,StephenTu,andBenjaminRecht.Certainty
equivalence is efficient for linear quadratic control. Ad-
vances in Neural Information Processing Systems, 32,
APPENDIXA
2019.
DERIVATIONOFEFEFORTHEPARETOPOINTSELECTION
[39] Sang Gyu Kwak and Jong Hae Kim. Central limit As mentioned in Sec. III-C, marginalizing the joint distri-
theorem: the cornerstone of modern statistics. Korean butionisanalyticallyintractable.Therefore,itsbound,i.e.free
journal of anesthesiology, 70(2):144–156, 2017. energy, is minimized instead. Yet, the outcomes C cannot be
[40] Christopher M. Bishop. Pattern Recognition and Ma- observeduntilaplanπ⋆ isactuallyexecuted,sotheagentends
chine Learning (Information Science and Statistics). up minimizing EFE. Hereafter, for the sake of simplicity, we
Springer-Verlag, Berlin, Heidelberg, 2006. denote s as s, π⋆ as π, and θ˜ as θ˜.
K s,π
[41] WilliamLDunnandJKennethShultis. Exploringmonte
EFE(π)
carlo methods. Elsevier, 2022.
(cid:90) (cid:90) q(s,θ˜|π)
[42] Kenneth A Farley, Kenneth H Williford, Kathryn M = q(s,θ˜|π) q(C|s,θ˜,π)log dCdθ˜ds,
Stack,RohitBhartia,AlChen,ManueldelaTorre,Kevin s,θ˜ C p(s,θ˜|C,π)p pr (C)
Hand, Yulia Goreva, Christopher DK Herd, Ricardo (26)
Hueso,etal.Mars2020missionoverview.SpaceScience where p (C) is the prior preference for outcomes and
pr
Reviews, 216:1–41, 2020. q(s,θ˜|π) is the proposal distribution for s and θ˜given a plan
[43] HamdaniHamdaniandRetantyoWardoyo.Thecomplex- π. Eq. (26) can be decomposed into three parts as follows.
ity calculation for group decision making using topsis (cid:90)
algorithm. InAIPconferenceproceedings,volume1755.
(1st)=− q(s,θ˜|π)q(C|s,θ˜,π)logp
pr
(C)dCdθ˜ds,
s,θ˜,C
AIP Publishing, 2016. (cid:90) (cid:110)(cid:90) (cid:111)
[44] Han Zhang, Oren Salzman, TK Satish Kumar, Ariel Fel- =− logp (C) q(s,θ˜,C|π)dθ˜ds dC,
pr
ner, Carlos Herna´ndez Ulloa, and Sven Koenig. A* pex: C s,θ˜
(cid:104) (cid:105)
Efficient approximate multi-objective search on graphs. =−E logp (C) . (27)
q(C|π) pr
In Proceedings of the International Conference on Auto-
mated Planning and Scheduling, volume 32, pages 394– (cid:90) q(s|π)
403, 2022.
(2nd)= q(s,θ˜|π)q(C|s,θ˜,π)log dCdθ˜ds,
[45] Enrico Zio and Roberta Bazzo. A clustering procedure
s,θ˜,C p(s|C,π)
(cid:90) (cid:110)(cid:90) (cid:111) q(s|π)
for reducing the number of representative solutions in = q(s,θ˜,C|π)dθ˜ log dCds,
theparetofrontofmultiobjectiveoptimizationproblems. s,C θ˜ p(s|C,π)
European Journal of Operational Research, 210(3):624– (cid:90) (cid:90) q(s|C,π)
=− q(C|π) q(s|C,π)log dsdC,
634, 2011. q(s|π)
C s
[46] Lilian Astrid Bejarano, Helbert Eduardo Espitia, and (cid:104) (cid:16) (cid:17)(cid:105)
=−E D q(s|C,π)||q(s|π) . (28)
Carlos Enrique Montenegro. Clustering analysis for q(C|π) KL
16
Following (13) and (14), p(·) and q(·) must have the same
(cid:90) q(θ˜|s,π) meanandvariance,therefore,itsufficestoanalyzethenormal-
(3rd)= q(s,θ˜|π)q(C|s,θ˜,π)log dCdθ˜ds,
s,θ˜,C
p(θ˜|s,C,π) ity of Monte Carlo samples of p(·) for determining how well
q(·)approximatesp(·).ThisanalysisisdoneusingaQuantile-
(cid:90) (cid:110)(cid:90) q(θ˜|s,C,π) (cid:111)
=− q(s,C|π) q(θ˜|s,C,π)log dθ˜ dCds, Quantile(Q-Q)plot[50].TheprocedureforgeneratingMonte
s,C θ˜
q(θ˜|s,π)
Carlo samples used in the Q-Q plot is outlined as follows:
(cid:104) (cid:16) (cid:17)(cid:105)
=−E q(s,C|π) D KL q(θ˜|s,C,π)||q(θ˜|s,π) . (29) 1) Generate M number of samples of θ ˆ˜ s,a from each
respective NIW p(θ˜ ). For high fidelity, we used
By combining these parts, (6) is finally derived. sk,ak
M =5000 samples.
APPENDIXB 2) SumeachrespectivesampletogetM samplesofθ
ˆ˜
,
APPROXIMATIONFORTHEFIRSTTERMOF(7) (empirically representing samples from p(θ˜|s,C,π s ) K ), ,π
Inordertoapproximatethefirsttermof(7),logp (C)can 3) Perform a whitening transform to all θ
ˆ˜
such that
pr sK,π
be at first simplified as follows. the such that the mean is zero and the covariance is
logp (C)=log (cid:16) Z exp (cid:0) − 1 (C−µ )TΣ−1(C−µ ) (cid:1)(cid:17) , normalized and uncorrelated,
pr pr 2 pr pr pr 4) Produce a Q-Q plot to assess the normality of the
=log(Z )− 1 (C−µ )TΣ−1(C−µ ) (cid:1) , (30) whitened empirical distribution.
pr 2 pr pr pr TheresultingQ-QplotsareshowninFig.9.Thedistributions
where Z pr =(2π)−N/2|Σ pr |−1/2. Recall, the certainty equiv- ofthemeanparametersµ sK,π (Fig.9a,toprow)aregenerally
alenceapproximationforthepredictedobservationdistribution well approximated by q(·). For short plans |π| = 10, p(·)
q(C|π) ≈ q(C|π;E[θ]) is a MVN distribution q(C|π) = thecovarianceparameterdistributionsdifferfromq(·)slightly
N(µ s,π ,Σ s,π ). By substituting this result into (7), the first (Fig. 9a, bottom left), however, due to the Central Limit
term of (7) can be further transformed as Theorem (CLT), they converge to normal as the length of the
plan increases (Fig. 9a, bottom right). Similarly, as more data
(cid:104) (C−µ )TΣ−1(C−µ )(cid:105)
(1st)=−E q(C|π;E[θ]) log(Z pr )− pr 2 pr pr . is supplied to each state-action pair, the distribution becomes
morenormalduetotheInverseWishartdistributionbeingless
(31)
right-skewed(Fig.9b,bottomlefttobottomright).Therefore,
Since (C−µ )TΣ−1(C−µ ) is expanded as CTΣ−1C− the MVN replacement approximation introduces less error
pr pr pr pr
2µT Σ−1C+µT Σ−1µ , the second term of (31) is turned to as the scenarios become more complex, and more data is
pr pr pr pr pr
collected.
(2nd of (31))=
1(cid:90)
q(C|π;E[θ]) (cid:0) CTΣ−1C (cid:1) dC (32)
2 pr
C APPENDIXD
+
1(cid:90)
q(C|π;E[θ]) (cid:0) µT Σ−1µ (cid:1) dC (33) EMPIRICALANALYSISOFERRORINTRODUCEDVIA
2 C pr pr pr MONTECARLOSAMPLING
(cid:90)
− q(C|π;E[θ]) (cid:0) −µT Σ−1C (cid:1) dC (34) Fig. 10 shows the percent error introduced via sampling.
pr pr
C Note that n =300 was used for the experiments and bench-
s
Finally, (32) can be reduced to marks shown in the manuscript as well as the computation
timebenchmarks.Althoughthespecificsamplingprocedureis
1(cid:16) (cid:17)
tr(Σ−1Σ)+µTΣ−1µ slightly different, there are multiple studies using the Monte
2 pr pr
Carlo sampling to evaluate the EFE [51, 52, 53].
where E[θ]={µ,Σ}. Also, (33) is simply
µT Σ−1µ ,
pr pr pr
and (34) can be reduced to
−µT Σ−1µ.
pr pr
APPENDIXC
EMPIRICALANALYSISOFERRORINTRODUCEDVIA
NIW-MVNSUBSTITUTION
To evaluate the choice of approximate posterior described
in Sec. III-C3, we numerically demonstrated the efficacy of
substituting intractable convolved NIW distributions (denoted
p(·)) with convolutions of MVN (denoted q(·)). Additionally,
since the same substitution technique is used for choosing the
prior proposal distribution in (15), this analysis also yields
insight into the quality of the variational upper bound.
17
4
4
2
2
0
0
-2
-2
-4
-4
8
6
6
4
4
2
2
0
0
-2
-2
-4
-5 0 5-5 0 5-5 0 5 -5 0 5-5 0 5-5 0 5
(a) Q-Q plot varying length of plan (m = |π|), while each (s,a) (b) Q-Q plots of varying number of collected samples (l) from
has10datasamples(l).Asthelengthoftheplanincreases,thetrue execution for each state-action, while m = 20. As more samples
distribution becomes more normal (i.e. the effect of the CLT). are collected, the true distribution becomes more normal.
Fig. 9: Q-Q Plot assessing the normality of Monte Carlo approximated true posterior p(θ˜|s,C,π). For brevity, only µ and
1
σ are shown. The closer the samples (blue) are to the quantile line (red), the more normal the empirical distribution.
1,1
0.014
0.012
0.010
0.008
0.006
0.004
0.002
0.000
0 100 200 300 400 500
Monte Carlo Samples
rorrE
tnecreP
m = 10
0.014
m = 20
m = 50
0.012
0.010
0.008
0.006
0.004
0.002
0.000
0 100 200 300 400 500
Monte Carlo Samples
(a) Error for varying length of plan (m).
rorrE
tnecreP
l = 1
l = 2
l = 10
l = 20
(b) Error for varying number of collected samples (l).
Fig. 10: Percent error introduced via sampling.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Online Pareto-Optimal Decision-Making for Complex Tasks using Active Inference"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
