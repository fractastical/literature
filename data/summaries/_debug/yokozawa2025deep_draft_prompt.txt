=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Deep Active Inference with Diffusion Policy and Multiple Timescale World Model for Real-World Exploration and Navigation
Citation Key: yokozawa2025deep
Authors: Riko Yokozawa, Kentaro Fujii, Yuta Nomura

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Key Terms: navigation, world, timescale, energy, multiple, deep, real, inference, framework, active

=== FULL PAPER TEXT ===

JOURNALOFLATEXCLASSFILES,VOL.18,NO.9,SEPTEMBER2020 1
Deep Active Inference with Diffusion Policy and Multiple Timescale
World Model for Real-World Exploration and Navigation
Riko Yokozawa, Kentaro Fujii, Yuta Nomura, and Shingo Murata, Member, IEEE
to unexpected situations [7]–[9]. Recent advances in learning-
Abstract—Autonomous robotic navigation in real-world envi- basedmethods,includingtransformer-basedpolicies[10]–[13]
ronmentsrequiresexplorationtoacquireenvironmentalinforma- and diffusion-based policies [14]–[19], have enabled diverse
tionaswellasgoal-directednavigationinordertoreachspecified
action generation and shown promising results in navigation
targets.Activeinference(AIF)basedonthefree-energyprinciple
tasks. However, these methods rely on explicit planners or
provides a unified framework for these behaviors by minimizing
the expected free energy (EFE), thereby combining epistemic extensive task-specific supervision to balance exploration and
and extrinsic values. To realize this practically, we propose a navigation, limiting their flexibility in real-world settings.
deep AIF framework that integrates a diffusion policy as the Meanwhile,activeinference(AIF)basedonthefree-energy
policymodelandamultipletimescalerecurrentstate-spacemodel
principle (FEP) [20] offers a unifying framework for explo-
(MTRSSM) as the world model. The diffusion policy generates
ration and goal-directed behavior by minimizing the expected
diversecandidateactionswhiletheMTRSSMpredictstheirlong-
horizonconsequencesthroughlatentimagination,enablingaction free energy (EFE) [21]–[23]. EFE comprises two terms: epis-
selectionthatminimizesEFE.Real-worldnavigationexperiments temic value, which naturally encourages exploration; and ex-
demonstrated that our framework achieved higher success rates trinsic value, which accounts for goal-directed behavior. Prior
and fewer collisions compared with the baselines, particularly
work has demonstrated the potential of AIF-based navigation
in exploration-demanding scenarios. These results highlight how
in simulation environments [24]–[26]; however, applications
AIFbasedonEFEminimizationcanunifyexplorationandgoal-
directed navigation in real-world robotic settings. in real-world robotic systems remain relatively limited, due
mainly to challenges in scaling AIF to complex and uncertain
Index Terms—Active inference, autonomous navigation, diffu-
environments.
sion policy, free-energy principle, mobile robot, world model
Building on this theoretical foundation, we aim to en-
hance the scalability of AIF for real-world robotic navigation.
I. INTRODUCTION Achievingsuchscalabilityrequirestheabilitytobothgenerate
diverse action sequences depending on the situation and to
AUTONOMOUS robotic navigation in real-world envi-
predict long-horizon state transitions under uncertainty. These
ronments requires exploration to acquire environmental
capabilities can be realized by leveraging advances in deep
information as well as goal-directed navigation in order to
generative models, which offer flexible policy representations
reach designated targets efficiently. Achieving an adaptive
and powerful predictive dynamics. In this work, we propose
balance between these two behaviors remains a fundamental
a deep AIF framework that integrates a diffusion policy [27]
challenge in machine learning and robotics [1]–[3]. In many
as the policy model and a multiple timescale recurrent state-
real-world situations, a robot cannot determine its position
space model (MTRSSM) [28] as the world model [29]–
or the surrounding structure from current observation alone.
[31] (Fig. 1). The diffusion policy flexibly generates diverse
For example, in visually similar areas such as corridors or
candidate actions according to the situation [14], while the
intersections, visually similar observations might correspond
MTRSSM predicts their long-horizon consequences through
to multiple possible locations, creating uncertainty in self-
latent imagination [28]. Together, these components enable
localization [4], [5]. In such cases, exploration plays a crucial
principled action selection under EFE minimization, thereby
role by actively collecting additional information to resolve
balancing exploration and goal-directed navigation without
this uncertainty [4]–[6]. When sufficient knowledge has been
relying on handcrafted planners.
acquired,therobotmustshiftitsfocustogoal-directednaviga-
The main contributions of this paper are summarized as
tioninordertoreachtargetsefficiently.Thus,bothexploration
follows:
and navigation are indispensable, and autonomous systems
must be able to flexibly balance between them according to • We advance the theoretical foundation of AIF by for-
malizing how deep generative models can extend the
the situation.
scalability of AIF for real-world navigation, highlighting
TraditionalapproachessuchasSLAM-basednavigationand
the role of policy models in generating diverse candidate
handcraftedplannerscanprovidereliablegoal-reachingstrate-
actions as well as that of world models in supporting
giesbuttendnottogeneralizetounseenenvironmentsoradapt
long-horizon predictive dynamics.
This work was supported by the Japan Science and Technology Agency • WerealizethisapproachbyusingadeepAIFarchitecture
(PRESTOGrantNumberJPMJPR22C9)andJSPSKAKENHIGrantNumber that integrates a diffusion policy as the policy model
JP24K03012.(Correspondingauthor:ShingoMurata) and an MTRSSM as the world model, thereby enabling
The authors are with the School of Integrated Design Engineering,
principled action selection via EFE minimization while
Keio University, Yokohama, Kanagawa 223-8522, Japan (e-mail: mu-
rata@elec.keio.ac.jp). balancing exploration and goal-directed behavior.
©ThisworkhasbeensubmittedtotheIEEEforpossiblepublication.Copyrightmaybetransferredwithoutnotice,afterwhichthisversionmaynolonger
beaccessible.
5202
tcO
72
]OR.sc[
1v85232.0152:viXra
JOURNALOFLATEXCLASSFILES,VOL.18,NO.9,SEPTEMBER2020 2
Fig. 1. Overview of the proposed deep active inference (AIF) framework. The framework integrates a diffusion policy and a multiple timescale recurrent
state-spacemodel(MTRSSM).Thediffusionpolicygeneratesdiversecandidateactionsequences,andtheMTRSSMpredictstheresultingstatetransitions.
The expected free energy (EFE) is evaluated for each candidate sequence, and the action with the lowest EFE is selected for execution in the real-world
environment.
• We validate the proposed framework on a real mobile explorationandnavigationisdeterminedmanually,depending
robot,showingthatitachievesbothexplorationandgoal- on whether a goal image is provided. More broadly, balanc-
directed navigation in uncertain environments without ing these two modes remains a fundamental challenge for
relying on handcrafted planners. diffusion-based methods.
Theremainderofthispaperisstructuredasfollows.Section In summary, policy learning for navigation has advanced
II reviews related work on learning-based navigation policies, throughtransformer-anddiffusion-basedapproaches,butchal-
world models, and AIF. Section III introduces the proposed lenges remain in terms of achieving both exploratory and
methodology, including the formulation of EFE and its inte- goal-directed behaviors without relying on manually designed
gration with a diffusion policy and an MTRSSM. Section IV modules. In the present work, we adopt a diffusion policy
describestheexperimentalsetup.SectionVreportstheresults. as the policy model, motivated by its ability to generate
Section VI discusses the findings, and Section VII concludes diverse behaviors that can support both exploration and goal-
the paper and outlines future directions. directed navigation within a single policy. By integrating this
policywithAIF,weaimtoaddressthechallengeofbalancing
exploration and navigation in real-world environments.
II. RELATEDWORK
Researchonautonomousnavigationhasinvestigatednumer-
B. World Models
ousapproaches,rangingfrompolicylearningtoworldmodels
World models have been studied as a means to capture
and AIF frameworks. This section reviews the representative
environmental dynamics and predict future states without
studies in these domains, with a particular focus on their ap-
requiring direct interaction with the environment [29]–[31].
plicabilitytoreal-worldnavigationaswellastheirlimitations.
A representative example is the recurrent state-space model
(RSSM), which combines deterministic and stochastic latent
A. Policy Models
variables in order to learn compact dynamics.
In recent years, policy learning approaches for navigation World models can be leveraged in two main ways. First,
have increasingly leveraged powerful sequence models such theycanfacilitatepolicylearning,whereimaginedrolloutsare
as transformers [10]–[13], [32] and diffusion models [14]– used to train policies efficiently [33]–[35]. Second, they can
[19], [27]. Transformer-based methods have demonstrated support action planning, where imagined rollouts are utilized
strong performance in both simulation [10], [11] and real- to evaluate candidate action sequences before execution [1],
world environments [12], [13], often generating actions by [36]–[38].
conditioning on subgoals or return signals. Although these Regardingnavigationtasks,worldmodelshavebeenstudied
approaches can generalize to real-world scenarios with large- for both policy learning [39]–[41] and action planning [42]–
scale datasets, they typically rely on external planners for [44]. For policy learning, some methods incorporate seman-
subgoal specification and are thus less suited for generating tic information [39] or contrastive representation learning
exploratory behaviors. [40]. For action planning, navigation world models employ
In contrast, diffusion-based policies have been applied to conditional generative dynamics to imagine trajectories for
both exploration and navigation, demonstrating robust per- multiple action candidates and evaluate them by comparing
formance across diverse environments, including real-world the imagined outcomes with goal observations [42]. Recent
settings[14]–[19].ArepresentativeexampleisNoMaD,which work has also developed generative models for autonomous
employs diffusion policy to generate diverse action samples driving, aiming to address large-scale real-world challenges
for both exploration and goal-directed navigation within a [45], [46].
single policy model. Although this illustrates the flexibility Nevertheless, world models face persistent limitations. Pre-
of diffusion-based policies, NoMaD still requires additional diction errors accumulate over long horizons, making robust
components.Specifically,high-levelplannersareusedtoguide long-horizon imagination difficult, especially in partially ob-
action selection during navigation, and the switching between servable real-world environments. To mitigate this, hierarchi-
JOURNALOFLATEXCLASSFILES,VOL.18,NO.9,SEPTEMBER2020 3
calextensionssuchastheMTRSSM[28]havebeenproposed, policies supporting both exploration and navigation. World
capturing dynamics at both fast and slow timescales in order models enable dynamics prediction for learning and planning,
to improve long-horizon prediction. and hierarchical variants such as MTRSSM have improved
In summary, although world models provide versatility for long-horizon prediction. AIF provides a unified method of
both learning and planning, their deployment in real-world exploration and goal-directed behavior by minimizing EFE,
navigation remains challenged by error accumulation in long- butmoststudiesremainlimitedtoperformingsimulationsand
horizon prediction. In the present work, we leverage a world emphasize only one mode, limiting real-world applicability.
model within the AIF framework, using it to provide the In contrast, our approach extends AIF to complex real-
environmentalstatepredictionsrequiredforEFEcomputation. world navigation by (i) employing a diffusion policy to gen-
To address long-horizon error accumulation, we adopt the erate diverse candidate actions that support both exploration
MTRSSM,whichcapturestemporaldependenciesacrossmul- and goal-directed navigation, (ii) leveraging an MTRSSM
tiple timescales, thereby improving long-horizon prediction. to capture temporal dependencies and provide long-horizon
state predictions for EFE computation, and (iii) integrating
C. Active Inference these components within the AIF framework for principled
action selection. This combination enables efficient balancing
AIF is a biologically inspired framework grounded in the
of exploration and goal-directed navigation in uncertain envi-
free-energyprinciple,whichexplainslearning,perception,and
ronments.
action in biological agents [20]–[23]. Under this framework,
observations are assumed to be generated by hidden states
of the environment, and the agent maintains a generative
III. METHODOLOGY
modeltoinferthesehiddenstatesfromobservationsandselect
actions accordingly. While prediction errors on observations A. Overview
(termed “surprise”) are minimized through variational free
The proposed deep AIF framework enables autonomous
energy (VFE), actions are selected to minimize EFE, which
navigationbyintegratingadiffusionpolicyasthepolicymodel
represents future uncertainty and goal-directed preferences.
and the MTRSSM as the world model. The architecture of
By selecting actions that minimize EFE, the agent can
the framework is illustrated in Fig. 2. The diffusion policy
effectivelyintegratebothexplorationandgoal-directedbehav-
generates diverse candidate action sequences conditioned on
ior. In the context of mobile robot navigation, this property
past observations, while the MTRSSM predicts the corre-
enablesAIFtoserveasaunifiedframeworkforhandlingboth
sponding state transitions. For each candidate sequence, the
explorationandgoal-directednavigation,enablingtherobotto
EFEiscalculatedusinglatentimagination[36],andtheaction
acquire additional information when necessary and to reach
sequence with the lowest EFE is executed in the real-world
specified targets efficiently.
environment.
In machine learning and robotics, deep neural networks
havebeenemployedtoimplementprobabilisticrepresentations
within AIF, including applications to mobile robot navigation
B. Formulation of Active Inference
[24]–[26],[47]–[49].Previousresearchhastendedtofocuson
either the epistemic (exploration-driven) value or the extrinsic In the free-energy principle, perception and learning are
(goal-directed) value in isolation. For instance, some studies formalized as the minimization of VFE. At time t, and given
have considered only the epistemic value needed to explore hidden states s and observations o under the generative
t t
and build topological maps [24], [48], whereas others have model p(o ,s ), the VFE F serves as an upper bound on
t t t
emphasized the extrinsic value needed to achieve navigation the surprise [20], [50]:
objectives in simulation or with real robots [26], [47]. Addi-
tionally,hierarchicalextensionsofstate-transitionmodelshave F =E [logq(s )−logp(o ,s )]
t q(st) t t t
been incorporated into AIF [24]–[26], [48], [49]. =D [q(s )||p(s )]−E [logp(o |s )]
In summary, despite these advances, most experiments re-
KL t t q(st) t t
(1)
=D [q(s )||p(s |o )]−logp(o )
main confined to simulation, and few studies have simulta- KL t t t t
neously integrated exploration and navigation in real-world ≥−logp(o t ).
scenarios [48], [49]. Incorporating powerful policy and world
Incontrast,decision-makingandactionselectionaredriven
models based on deep generative modeling into AIF might
by minimization of the EFE. For a future time step τ under
open the door to scaling AIF to more complex real-world
policy π, the EFE is defined as follows [50]:
environments. In the present study, we take a step in this
direction by integrating policy and world models into AIF for
G (π)=E [logq(s |π)−logp(o ,s |π)]. (2)
real-world navigation tasks. τ q(oτ,sτ|π) τ τ τ
Note that although the VFE is computed after receiving an
D. Summary
observation and therefore requires only the expectation over
Recent advances in autonomous navigation span policy hidden states, the EFE considers future time steps before
learning, world models, and AIF. Transformer- and diffusion- observations are available, and thus involves expectations
basedpolicieshaveimprovedactiongeneration,withdiffusion over both hidden states and observations. Following standard
JOURNALOFLATEXCLASSFILES,VOL.18,NO.9,SEPTEMBER2020 4
Fig.2. ArchitectureoftheproposeddeepAIFframework.Theprocesscomprisesthreesteps:(1)Sample Actions:thediffusionpolicygeneratesmultiple
candidate action sequences of length TF, conditioned on past observations; (2) Simulate States: the MTRSSM performs latent imagination by simulating
the state transitions for each candidate action sequence, sampling high- and low-level latent states in order to estimate epistemic and extrinsic values; (3)
CalculateEFE:theEFEiscalculatedforeachcandidateactionsequence,combiningepistemicandextrinsicterms,andtheactionsequencewiththelowest
EFEisexecutedinthereal-worldenvironment.
derivations, and given preference C, the EFE can be decom- At the time of inference, candidate action sequences are
posed into epistemic and extrinsic values as follows [50]: generatedbyiterativelydenoisingaGaussiannoisesampleaK
t
through K reverse-diffusion steps, yielding a fully denoised
G (π)≈−E [D [q(s |o ,π)||q(s |π)]]
τ q(oτ|π) KL τ τ τ sequence a0. The reverse diffusion at step k is defined as:
(cid:124) (cid:123)(cid:122) (cid:125) t
epistemicvalue (3) 1 (cid:18) 1−α (cid:19)
−E q(oτ|π) [logp(o τ |C)]. a t,k−1 = √ α a t,k − √ 1−α¯ k ϵ θ (o t ,a t,k ,k) +ϵ k , (7)
(cid:124) (cid:123)(cid:122) (cid:125) k k
extrinsicvalue
where ϵ ∼ N(0,σ2I) is Gaussian noise and σ denotes the
k k k
C. Policy Model standard deviation at step k.
At deployment, only the first T steps of the generated
To enable action selection under AIF, the policy model a
sequence a are executed, while T >T facilitates planning
must represent a diverse set of candidate actions for a given t F a
further ahead.
situation. In this work, we adopt a diffusion policy [27] as
the policy model. The diffusion policy models the conditional
distribution of future action sequences a t based on past D. World Model
observation sequences o , using a diffusion model [51]; that
t To predict the action-conditioned state transitions, we used
is, p(a |o ). Specifically, we define the action sequence as
t t an MTRSSM [28], which extends the standard RSSM [30]
consisting of the two most recent actions followed by T
F byincorporatingtemporalhierarchies.Thishierarchicaldesign
future steps, and the observation sequence as the two most
enables the model to capture both fast and slow dynamics in
recent observations:
the environment.
(cid:40)
a t =a t−1:t+TF . (4) Letdh t anddl t denotedeterministicstatesathigherandlower
o =o levels, respectively, and sh and sl denote stochastic states at
t t−1:t t t
each level. The overall latent state becomes
Diffusion models are generative models that are trained to
iteratively denoise data corrupted by Gaussian noise. During z ={zh,zl}, zh ={dh,sh}, zl ={dl,sl}. (8)
t t t t t t t t t
training, clean data samples are perturbed with noise, and the
Thedeterministicstatesevolveaccordingtoseparaterecurrent
model is optimized to predict this noise. After training, new
functions at each timescale as follows:
data can be generated by starting from a noise sample and
progressively denoising it through multiple steps.
dh =fh(dh ,sh ;τh), (9)
Duringtraining,Gaussiannoiseisaddedtotheground-truth t ϕ t−1 t−1
actionsequencea t ,andthenetworkϵ θ isoptimizedtopredict dl t =f ϕ l(z t l −1 ,sh t ,a t−1 ;τl), (10)
the added noise via the following objective:
where fh and fl are implemented by multiple timescale
ϕ ϕ
L (θ)=MSE(ϵ ,ϵ (o ,a ,k)), (5) recurrentneuralnetworks(MTRNNs)[52]–[54]forthehigher
DP k θ t t,k
√ √ andlowerlevels,respectively.Thehigherleveldhwithalarger
a = α¯ a + 1−α¯ ϵ , (6) t
t,k k t,0 k k time constant τh updates slowly in order to capture long-
where k is the diffusion step, ϵ ∼N(0,I) is Gaussian noise, horizon dependencies, while the lower level dl with a smaller
k t
and α¯ = (cid:81)k α denotes the cumulative product of the timeconstantτl updatesrapidlyinorder toencodeshort-term
k s=1 s
noise-scheduling coefficients. transitions.
JOURNALOFLATEXCLASSFILES,VOL.18,NO.9,SEPTEMBER2020 5
The stochastic states are sampled from either the prior
distributions ph,pl or the approximate posterior distributions
ϕ ϕ
qh,ql, defined as follows:
ϕ ϕ
sˆh ∼ph(sh|dh), sˆl ∼pl (sl|dl), (11)
t ϕ t t t ϕ t t
sh ∼qh(sh|dh,dl), sl ∼ql(sl|dl,o ). (12)
t ϕ t t t t ϕ t t t
Observations o are encoded into low-dimensional features
t
via a convolutional neural network (CNN) encoder before be-
ingfedintothemodel.TheMTRSSMistrainedbyminimizing
the following VFE-based loss:
T
L
WM
= (cid:88)(cid:110) βD
KL
(cid:2) q
ϕ
l(sl
t
|dl
t
,o
t
) (cid:13) (cid:13)pl
ϕ
(sl
t
|dl
t
) (cid:3)
t=1
+βD KL (cid:2) q ϕ h(sh t |dh t ,dl t ) (cid:13) (cid:13)ph ϕ (sh t |dh t ) (cid:3) (13)
−E (cid:2) logpl (o |zh,zl) (cid:3)
qϕ(sl
t
|dl
t
,ot) ϕ t t t
−E (cid:2) logph(dl |zh ) (cid:3)(cid:111) .
qϕ(sh
t
|dh
t
,dl
t
) ϕ t t−1
This loss combines the KL regularization at both hierarchies
Fig.3. ComputationofEFEforcandidateactionsequences.Eachcandidate
with the reconstruction losses of observations and dynamics, actionsequenceat issimulatedbytheMTRSSM,usinglatentimagination.
makingitpossibleforthemodeltolearnbothshort-andlong- At each time step, the process unfolds as follows: (1) the higher-level
deterministic state dh is updated; (2) the higher-level stochastic state sh is
horizon dependencies. sampled;(3)thelow τ er-leveldeterministicstatedl isupdated;(4)thelow τ er-
τ
level prior q ϕ (sl τ |dl τ ) is predicted and the stochastic state sˆl τ is sampled;
E. Active Inference with EFE Minimization
(
p
5
o
)
st
a
er
p
i
r
o
e
r
di
q
c
ϕ
te
(
d
sl τ
obs
|
er
d
v
l τ
at
,
io
oˆ
n
τ)
oˆτ
is
is
in
g
f
e
e
n
r
e
re
ra
d
te
u
d
s
;
i
a
n
n
g
d
oˆ
(
τ
6)
.
t
A
he
t
l
t
o
h
w
e
er
l
-
o
l
w
ev
e
e
r
ls
le
to
v
c
e
h
l,
as
t
t
h
i
e
c
Atthetimeofinference,thepolicymodelgeneratesmultiple epistemicvalueiscomputedastheKLdivergencebetweentheposteriorand
theprior,whiletheextrinsicvalueiscomputedasthefeature-spacedistance
candidate action sequences a , each of which is simulated
t betweenthepredictedobservationoˆτ andthegoalobservationog.Combining
using the MTRSSM through latent imagination [36]. Future these two terms yields the EFE Gτ(at), which is used to select the action
states sˆl are sampled from the prior distribution, and the sequencethatbalancesexplorationandgoal-directednavigation.
τ
correspondingpredictedobservations oˆ aredecoded. Ateach
τ
time step, M high-level latent states sh are sampled from the
τ observations o . To balance these two terms, the precision
posterior distribution of the higher level, and for each one, g
(inverse variance) 1/σ2 is designed as a time-decaying coeffi-
N low-level latent states sl are sampled from the prior of the τ
τ cient,wheretheepistemicvaluedominatesintheearlierphase
lowerlevel.ThisresultsinM×N predictedobservationsoˆi,j.
τ when self-localization uncertainty is high, while the extrinsic
Usingthesepredictedobservationstogetherwiththecandidate
value gradually becomes dominant in the later phase once the
action and the current state, the posterior of the lower level is
robot has localized itself.
recomputedateachtimestepandpropagatedforwardthrough
Finally, an action sequence a∗ is selected according to the
the MTRSSM. The KL divergence between this posterior and t
following equation:
the corresponding prior constitutes the epistemic value term.
The EFE for a candidate action sequence is approximated as
a∗ =argminG (a ). (15)
follows: t τ t
at
M N (cid:40)
1 (cid:88)(cid:88) This action-selection mechanism based on the EFE minimiza-
G (a )≈−
τ t MN tion enables the robot to balance exploration driven by epis-
i=1j=1
D
KL
(cid:104) q
ϕ
l(sl
τ
|dl
τ
,oˆi
τ
,j) (cid:13) (cid:13)pl
ϕ
(sl
τ
|dl
τ
) (cid:105) (14) t
v
e
a
m
lu
i
e
c
.
value with goal-directed navigation guided by extrinsic
(cid:41)
+ 1 MSE (cid:0) f(oˆi,j),f(o ) (cid:1) ,
σ2 τ g
τ IV. EXPERIMENTS
where f(·) denotes a CNN encoder that maps observations
A. Hardware Setup
onto a feature space. The first term corresponds to the epis-
temic value, and the second term corresponds to the extrinsic The proposed deep AIF framework was implemented with
value in (3). Details of the EFE computation are illustrated in a TurtleBot 4 (Clearpath Robotics), as shown in Fig. 1.
Fig. 3. The robot is controlled via two velocity commands: linear
In this work, the epistemic value is computed only at the velocity and angular velocity. A wide-angle RGB camera
lower level of the MTRSSM, while the extrinsic value is (CMS-V43BK, Sanwa Supply) was mounted on the top plate
defined as the temporal average of a feature-space distance of the robot. The captured images were resized to 240×320
betweenthepredicted(imagined)observationsoˆ andthegoal pixels and used as observations.
τ
JOURNALOFLATEXCLASSFILES,VOL.18,NO.9,SEPTEMBER2020 6
Fig. 4. Experimental environment. (a)Overhead view ofthe indoorroom. (b) Top–downmap ofthe environment. (c)Representative observationsat nine
designated location–orientation patterns (1)–(9) on the map. In the real-world navigation tasks, the initial position–orientation pairs and goal images were
selectedfromamongthesepatterns,resultingin18experimentalcases.
B. Experimental Environment F. Implementation Details
The experiments were conducted in an indoor room (ap- 1) Training Details:
proximately 4.7 m × 6.3 m), the layout and representative a) Diffusion policy: The observation encoder was com-
observation images of which are shown in Fig. 4. Three posed of a 3-layer CNN followed by a spatial softmax,
sides of the room (excluding the entrance wall) were lined producing 32 keypoints that conditioned the diffusion model.
with desks and black desk chairs, and the center contained TheCNNusedconvolutionallayerswithchannels(8,16,32),
a meeting table surrounded by dining chairs. Additionally, kernel sizes (3, 3, 3), strides (2, 2, 1), and paddings (1, 1,
four colored stools were placed as landmarks: two red, one 1). The diffusion model was implemented as a 1D-UNet [55]
black, and one green. A key challenge is that, because of with a DDIM sampler [56]. The action sequence length was
the similarity of images across different locations, especially set to T =64, and the number of diffusion steps was set to
F
along the wall side with black desk chairs, localization based K =100.
solelyonthecurrentobservationcanbeambiguous.Therefore, b) MTRSSM: The observation encoder consisted of a 3-
the robot must actively explore the environment to reduce layer CNN with channel sizes (16, 32, 64), kernel sizes (3,
uncertainty and realize goal-directed navigation. 3, 3), strides (1, 2, 2), and paddings (1, 1, 1), producing a
128-dimensional embedding. The deterministic states of the
MTRSSM were modeled using an MTRNN [52]–[54] with
C. Data Collection
both higher-level deterministic states dh ∈ R32 and lower-
t
Data were collected by manually teleoperating the robot level deterministic states dl ∈ R128. The time constants
t
within the environment. In total, 15 sequences of 2,000 time were set to τh = 64 for the higher level and τl = 4 for
steps each were recorded at 5 Hz, for a total of 30,000 time the lower level. The higher-level stochastic states sh were
t
steps. Each dataset contained velocity commands and RGB
represented by a categorical distribution over 4×4 classes,
observation images. while the lower-level stochastic states sl were represented by
t
acategoricaldistributionover8×8classes.Theimagedecoder
D. Dataset for Policy Model Training wasimplementedasaCNNwitheightresidualblocksandtwo
pixel-shuffle layers [57], [58]. Training employed truncated
For policy model training, the observation images were
backpropagation through time [59] with a window size of 50
resizedto120×160pixels.Eachsequencewassegmentedinto
steps.
128-step windows every 32 steps, yielding 885 subsequences.
2) Experiment Details:
From each, 64-step subsequences were randomly sampled to
a) Diffusion Policy: At test time, the policy generated
ensure that all observation images would appear as conditions
action sequences of T =64 steps but executed only the first
during training. F
T =32 steps. The number of diffusion steps was reduced to
a
K = 10 for real-time execution, and eight candidate action
E. Dataset for World Model Training
sequences were sampled per inference.
For world model training, the observation images were b) MTRSSM: For EFE computation defined in (14), we
downsampledto60×80pixels.Eachsequencewassegmented sampled M = 5 higher-level latent states and N = 5 lower-
into 600-step trajectories every 100 steps, resulting in 225 level latent states at each time step. The precision for the
trajectories.Fromeachtrajectory,500-stepsubsequenceswere extrinsic value in (14) was formulated as a smooth, sigmoidal
randomly extracted. function of the inference iteration n, which was incremented
JOURNALOFLATEXCLASSFILES,VOL.18,NO.9,SEPTEMBER2020 7
once every T time steps. Specifically, it was defined as
a
follows:
1 3.0−0.08
=0.08+ . (16)
σ2 1+exp[−0.6(n−10)]
τ
This formulation gradually shifts the weighting between epis-
temic and extrinsic values, with the epistemic term domi-
nating in the early phase, and the extrinsic term becoming
increasingly influential as the inference iteration progresses.
The feature encoder f(·) comprised a three-layer CNN with
channel sizes (16, 32, 64), kernel sizes (3, 3, 3), strides
(2, 2, 2), and paddings (1, 1, 1), followed by a three-layer
fully connected network with 1,024 hidden units. The feature
encoderwastrainedsothatthefeature-spacedistancebetween
two observations reflects their spatial distance, as estimated
from accumulated actions or odometry.
G. Navigation Task with Real-World Robot
The main evaluation was conducted using robot navigation
experiments in the environment described above. Three initial
positions were prepared, and each was tested under the fol-
lowing two facing directions: (1) facing toward the interior of
the room, where the current location is visually distinct (2)
facing the wall side, where the similarity of chairs along the
threewallsintroducesperceptualaliasing,makinglocalization
highly uncertain Accordingly, six distinct initial states were
defined in total.
For each initial position, three different goal images were
specified, yielding 6 × 3 = 18 task cases. Each case was
executed twice, for a total of 36 trials. A trial was terminated
after 1,000 time steps if the robot had not reached the
Fig.5. Representativeactionsequencesgeneratedbythediffusionpolicyin
goal. Success was defined as entering a predefined goal area threescenarios:(a)clearpath,(b)obstacleahead,and(c)approachingacorner.
(manually determined to correspond to the region where the Thepolicyadaptsitsactionproposalstoeachsituationandgeneratesdiverse
behaviors such as forward movement and turns, illustrating its flexibility in
goal image was observable). In addition to the success rate,
handlingdifferentenvironmentalcontexts.
the number of collisions with obstacles was also recorded.
H. Baseline Methods context-dependent action sequences. We then assess the pre-
dictive capability of the world model through imagination
We consider two baselines, which also served as ablations
experiments. Finally, we demonstrate the overall navigation
for our framework.
performanceonarealmobilerobot,comparingourframework
The first baseline replaces the MTRSSM with a standard
with baseline methods.
RSSM. Although RSSM has been widely used in model-
based reinforcement learning to capture short-term dynamics,
it lacks hierarchical temporal structure, making long-horizon A. Policy Model Evaluation
prediction more prone to error.
We first evaluated the diffusion policy to assess its ability
The second baseline, Only Extrinsic, minimizes only the
to generate diverse and context-dependent action sequences.
extrinsiccomponentofthe EFE,ignoringtheepistemicvalue.
Fig. 5 shows representative cases illustrating how the policy
This corresponds to purely goal-directed navigation without
responds under different environmental conditions.
exploration,similartoconventionalplanning-orreward-driven
Fig. 5(a) shows the case of a clear path ahead, where the
methods.
corridor in front of the robot was unobstructed. The policy
By including these ablations as baselines, we can directly
generatedawidevarietyofforward-directedactions,including
assess the contributions of multiple timescale modeling and
driving straight ahead or turning left/right.
epistemic value to overall performance.
Fig. 5(b) illustrates a case in which a black desk chair was
positioned directly in front of the robot. The generated action
V. RESULTS
sequences included turns to both the left and right, as well
This section presents the experimental results of the pro- as slight forward adjustments to navigate around the obstacle.
posed framework. We begin by evaluating the policy model, This indicates that the policy successfully incorporated the
examining whether it is capable of generating diverse and perceived obstacle into its proposals.
JOURNALOFLATEXCLASSFILES,VOL.18,NO.9,SEPTEMBER2020 8
Fig. 6. Examples of real observations and imaginations generated by the MTRSSM. (a, b) Successful cases. (c) Failure case. In the successful cases, the
imaginationscapturedtheoverallscenedynamics,althoughsubtletemporallagsappearedaftertimestept=180.Inthefailurecase,temporallagsoccurred
after time step t = 120, followed by a spatial discontinuity at time step t = 180. Despite these deviations, the imagined sequence remained coherent for
morethan100steps,demonstratingthemodel’slong-horizonpredictioncapability.
Fig. 5(c) presents the case of approaching a room corner. by collecting the corresponding states or features across the
The policy produced turning actions that aligned with the entire training dataset and projecting them into three dimen-
room’s geometry, suggesting that it can propose adaptive sions,usingprincipalcomponentanalysis.Inthevisualization,
trajectories consistent with the layout of the environment. alldataareshowningray,andthetrajectorycorrespondingto
Overall, these examples demonstrate that the diffusion pol- the room loop is highlighted with a time-encoded colormap.
icy is capable of flexibly generating diverse actions suited to Only the higher-level states dh formed a smooth closed loop,
t
the current situation. While this evaluation highlights adapt- which represents the global room structure and acts as an
abilitytodifferentsituations,theroleofexploratoryandgoal- attractor in the state space. This indicates that the higher-
directed behaviors will be further examined in the subsequent level dynamics of the MTRSSM successfully captured the
real-world experiments. environmentalgeometry,enablingconsistentimaginationeven
in the absence of observation inputs.
Overall, these results demonstrate that the MTRSSM can
B. World Model Evaluation
leverage its hierarchical structure to generate coherent long-
Next, we evaluated the MTRSSM, examining its predictive horizon predictions. This predictive capability is essential for
capability in long-horizon imagination. The model received evaluating candidate action sequences in AIF and will be
observationinputsforthefirst20timestepsandthengenerated combined with the diffusion policy in the subsequent real-
predictions for 200 steps without further observations. world experiments.
Figs. 6(a) and 6(b) show successful imaginations in
which the imaginations closely matched the real observations
C. Real-World Experiments
Fig. 6(c) shows a case in which the imagination diverged
fromtheactualobservations,generatingawall-sideblackdesk Finally, we evaluated the proposed deep AIF framework
chair instead of a corridor with a red stool. Nevertheless, in real-world navigation tasks, using the setup described in
the imagined sequence remained coherent for more than 100 Sec. IV-G. In total, 36 trials were conducted.
timesteps,demonstratingthemodel’sabilitytomaintainlong- Table I summarizes the quantitative results in terms of
horizon predictions. success rate and collisions. Overall, our framework achieved
Furtheranalyzingtheseresults,Fig.7illustratestheinternal a success rate of 75%, outperforming RSSM (64%) and
statedynamicsduringalooparoundtheroom.Fig.7(a)shows OnlyExtrinsic(53%).Theimprovementwasmostpronounced
the higher-level deterministic states dh, Fig. 7(b) shows the in exploration-demanding scenarios, where our framework
t
lower-level deterministic states dl, and Fig. 7(c) shows the reached 78% compared with 61% for RSSM and only 28%
t
image features. For each panel, the trajectories were obtained for Only Extrinsic. In the no-exploration cases, all methods
JOURNALOFLATEXCLASSFILES,VOL.18,NO.9,SEPTEMBER2020 9
Fig. 7. Internalstate dynamics during a loop around the room. (a)Higher-level deterministic states dh. (b) Lower-level deterministic states dl. (c) Image
t t
features. For each panel, trajectories were obtained by collecting the corresponding states or features across the entire training dataset and projecting them
intothreedimensionsusingprincipalcomponentanalysis.Graypointsrepresentalldatafromthetrainingset,andthetrajectorycorrespondingtotheroom
loop is highlighted with a color gradient from light blue to pink, indicating temporal progression. Only the higher-level states dh formed a smooth closed
t
loop, which reflected the global room structure and demonstrated that the higher-level dynamics of the MTRSSM successfully captured the environmental
geometry,enablingconsistentimaginationevenintheabsenceofobservationinputs.
TABLEI localization. Candidate actions included staying in place as
NAVIGATIONSUCCESSRATES(%)ANDAVERAGECOLLISIONSFOR wellasturninginplace.Theimaginedobservationsatthelast
OVERALL,EXPLORATION(EXP),ANDNON-EXPLORATION(NOEXP)
time step revealed that the turning action (Sample 1 in the
TRIALS.
figure) would expose additional information beyond the line
SuccessRate(%) Collisions of chairs along the wall. Our framework selected this turning
Method
Overall Exp NoExp Overall Exp NoExp action because the EFE assigned it a high epistemic value,
Ours 75 78 72 0.806 0.778 0.833 reflecting the potential information gain. This illustrates how
RSSM 64 61 67 0.806 1.056 0.556 epistemic considerations guide the robot toward exploratory
OnlyExtrinsic 53 28 78 1.000 1.667 0.333
behavior, enabling it to reduce uncertainty and improve sub-
sequent localization.
performed relatively well, with Only Extrinsic achieving the In contrast to the early stage shown in Fig. 8, Fig. 9
highest rate (78%) and our framework achieving 72%. For illustrates the later stage of the same trial, when the robot
collisions, our framework matched RSSM overall (0.806) was already near the goal. In the current observation, a green
and clearly reduced collisions compared with Only Extrinsic stool—also present in the goal observation—appears on the
(1.000), particularly under exploration-demanding scenarios left side, providing a clear visual cue. At this point, some
(0.778 vs. 1.667). candidateactionswouldcontinuepastthegoal,whereasothers
wouldturntowardandapproachit.Theimaginedobservations
These quantitative results highlight two key factors under-
for the goal-approaching actions (Samples 1 and 3) closely
lying the effectiveness of our framework. First, the use of
matched the goal observation. In this case, the EFE was
the MTRSSM contributed to improved success rates by en-
dominatedbytheextrinsicvalue,favoringactionsthatbrought
abling more accurate long-horizon predictions compared with
the robot closer to the goal.
a single-layer RSSM. Second, the incorporation of epistemic
value in the EFE formulation enabled the robot to actively Finally,Fig.10comparesourframeworkwiththeOnlyEx-
explore and resolve uncertainty, which was especially bene- trinsic baselinein the same scenario thatrequired exploration.
ficial in exploration-demanding scenarios. these contributions Note that the first situation (t = 10) in our framework corre-
are further illustrated by the following qualitative examples, sponds to the example shown in Fig. 8. The figure presents
which show how epistemic and extrinsic values guide action the sampled candidate actions together with their EFE values.
selection in practice. The action with the lowest EFE was selected and executed,
Fig.8and9providequalitativeexamplesofactionselection. leading to a change in the robot’s observations. The upper
Here, we focus on a trial whose initial and goal observations row (our framework) demonstrates active exploration, where
correspondtoimages(4)and(5)inFig.4,respectively.Fig.8 therobotobtainedincreasinglydiverseobservationsovertime,
shows the early stage of the trial, whereas Fig. 9 shows the whereasthelowerrow(OnlyExtrinsic)showstherobotlargely
later stage near the goal. These examples illustrate how the remaining in place without gaining new information. These
robot selected actions at different phases of navigation in the resultsindicatethatincorporatingepistemicvalueintotheEFE
trial. formulationencouragedexploratoryactionsthatproducednew
InFig.8,therobotinitiallyfacedablackdeskchair,andbe- observations,therebyenablingtherobottoresolveuncertainty
cause similar chairs were placed throughout the environment, and achieve self-localization, whereas the baseline relying
thecurrentobservationalonewasinsufficientforreliableself- solely on extrinsic value failed to do so.
JOURNALOFLATEXCLASSFILES,VOL.18,NO.9,SEPTEMBER2020 10
Fig.8. Actionselectionduringearly-stagenavigation.Therobotevaluatescandidateactions,suchasstayinginplaceorturning,whilethecurrentobservation—
showing a black desk chair that appears at multiple locations—is insufficient for precise localization. Imagined observations corresponding to each action
suggestthatturningrevealsadditionalenvironmentalinformation.Inthissituation,theEFEisdominatedbytheepistemicvalue,leadingtherobottoselect
theturningactionthatreducesuncertaintyandimproveslocalization.
Fig. 9. Action selection near the goal. The robot evaluates candidate actions, some of which would pass the goal while others approach it. Imagined
observationscorrespondingtogoal-approachingactionscloselymatchthegoalimage.Inthissituation,theEFEisdominatedbytheextrinsicvalue,leading
therobottoselectactionsthatmoveitclosertothegoal.
VI. DISCUSSION incorporating the epistemic value. This finding highlights the
potential of AIF as a principled framework for navigation
The central finding of this study is that AIF can effectively under partial observability and perceptual aliasing.
unify exploration and goal-directed navigation in a real-world A key contribution of this work lies in its integration of
setting by minimizing EFE. Although previous research has a diffusion policy into the AIF framework. Prior studies have
tended to emphasize either the epistemic value to drive explo- shownthatdiffusionpoliciesarecapableofgeneratingdiverse
ration[24],[48]ortheextrinsicvaluetopursuenavigationob- and adaptive action sequences in navigation tasks [14]–[19].
jectives [26], [47], few studies have simultaneously integrated However, they often rely on additional modules such as high-
both aspects in physical robots [49]. As shown in Table I, level planners or manual switching mechanisms to balance
our framework achieved higher success rates, particularly in explorationandnavigation.Forexample,NoMaD[14]demon-
exploration-demanding tasks, clearly indicating the benefit of strated the flexibility of diffusion-based action generation but
JOURNALOFLATEXCLASSFILES,VOL.18,NO.9,SEPTEMBER2020 11
Fig.10. ComparisonofactionselectionbetweenourdeepAIFframework(upperrow)andanOnlyExtrinsicbaseline(lowerrow)inanexplorationscenario.
Ourframeworkdemonstratesactiveexploration,acquiringdiverseobservationsovertime,whereasthebaselinelargelyremainsstationary.Thefirstactionin
ourframeworkcorrespondstotheexampleinFig.8,highlightingtheroleoftheepistemicvalueindrivingexploratorybehavior.
still required external guidance to select between exploratory asaunifyingcriterionselectingactionsthatbalanceepistemic
and goal-directed behaviors. In contrast, our formulation em- and extrinsic considerations. This synergy was most apparent
beds the diffusion policy within the AIF framework, enabling in uncertain initial states, as highlighted in Table I, where our
exploratory trajectories to be considered naturally alongside framework outperformed the Only Extrinsic baseline by ac-
goal-directed ones. This effect is visible in Fig. 8, where tivelyselectingexploratoryactions.Similarly,Fig.9illustrates
the epistemic value drove the robot to turn and expose new how extrinsic value dominates once the robot approaches the
observations, thereby reducing localization uncertainty. goal, ensuring efficient convergence.
Equally important is the role of the MTRSSM. Recurrent
From a broader perspective, the proposed framework con-
world models such as RSSM [30] have been widely adopted
tributes to extending the scalability of AIF in real-world
for imagination-based planning, yet they often suffer from
robotics. Previous applications of AIF in navigation have
error accumulation in long-horizon predictions. As shown
largely been confined to simulation environments [24]–[26].
in Fig. 7, only the higher-level deterministic states of the
Even when deployed on real robots, these studies often ad-
MTRSSM formed smooth closed loops, reflecting the global
dressed simplified tasks or considered exploration and nav-
room structure. This indicates that MTRSSM successfully
igation in isolation [48], [49]. By leveraging advances in
capturedslow-varyingdynamics,enablingrobustlong-horizon
generative modeling, the present work demonstrates that AIF
imagination. Such stability is essential for reliable estimation
can be scaled to more complex and uncertain real-world
of epistemic and extrinsic values.
scenarios.Withitsabilitytoflexiblygenerateactions,perform
The integration of diffusion policy and MTRSSM within stable long-horizon imagination, and balance epistemic and
AIF reveals a synergistic effect. Although diffusion policy extrinsic values, AIF represents a competitive alternative to
provides a broad set of candidate actions, MTRSSM supplies traditional methods. In particular, our framework provides a
reliable long-horizon predictions that enable these candidates unifiedmechanismthatdoesnotrequireexplicitmappingasin
to be meaningfully evaluated. The EFE formulation then acts SLAM-based navigation [7]–[9] or handcrafted planners [12],
JOURNALOFLATEXCLASSFILES,VOL.18,NO.9,SEPTEMBER2020 12
[14]. [5] A. Siddique, W. N. Browne, and G. M. Grimshaw, “Frames-of-
Nevertheless, several limitations must be acknowledged. reference-based learning: Overcoming perceptual aliasing in multistep
decision-making tasks,” IEEE Transactions on Evolutionary Computa-
First, the experimental environment was limited to a single
tion,vol.26,no.1,pp.174–187,2022.
indoor room, which limited the diversity of the conditions [6] D. Shah, B. Eysenbach, N. Rhinehart, and S. Levine, “Rapid
tested. Second, although MTRSSM reduced long-horizon pre- Exploration for Open-World Navigation with Latent Goal Models,” in
5th Annual Conference on Robot Learning, 2021. [Online]. Available:
dictionerrorscomparedwithRSSM,deviationssuchasspatial
https://openreview.net/forum?id=d SWJhyKfVw
discontinuitystilloccurred(Fig.6(c)).Third,theprecisionfor [7] C. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira,
the extrinsic term was heuristically scheduled as in (16) for I. Reid, and J. J. Leonard, “Past, present, and future of simultaneous
localization and mapping: Toward the robust-perception age,” IEEE
numericalstability,limitingthetheoreticalcompletenessofthe
TransactionsonRobotics,vol.32,no.6,pp.1309–1332,2016.
active inference formulation. [8] A. Kadian, J. Truong, A. Gokaslan, A. Clegg, E. Wijmans, S. Lee,
In summary, this discussion highlights how the integration M. Savva, S. Chernova, and D. Batra, “Sim2real predictivity: Does
evaluation in simulation predict real-world performance?” IEEE
of diffusion policy and MTRSSM within AIF enables prin-
Robotics and Automation Letters, vol. 5, no. 4, p. 6670–6677, Oct.
cipled action selection that balances exploration and goal- 2020.[Online].Available:http://dx.doi.org/10.1109/LRA.2020.3013848
directed navigation. Our findings demonstrate that recent [9] T. Gervet, S. Chintala, D. Batra, J. Malik, and D. S. Chaplot,
“Navigating to objects in the real world,” Science Robotics,
advances in deep generative modeling can be leveraged to
vol. 8, no. 79, p. eadf6991, 2023. [Online]. Available: https:
addressthelong-standingchallengeofscalingAIFtocomplex //www.science.org/doi/abs/10.1126/scirobotics.adf6991
robotictasks.Bysituatingourcontributionsinrelationtoprior [10] Q. Chen, R. Wang, M. Lyu, and J. Zhang, “Transformer-based
reinforcement learning for multi-robot autonomous exploration,”
work, and by referencing the empirical evidence presented in
Sensors, vol. 24, no. 16, 2024. [Online]. Available: https://www.mdpi.
Figs.5,6,8,9,andTableI,weemphasizeboththetheoretical com/1424-8220/24/16/5083
significance of unifying epistemic and extrinsic values under [11] H. Wang, A. H. Tan, and G. Nejat, “Navformer: A transformer ar-
chitecture for robot target-driven navigation in unknown and dynamic
EFE and the technical advances that made this integration
environments,”IEEERoboticsandAutomationLetters,vol.9,no.8,pp.
feasible. 6808–6815,2024.
[12] D.Shah,A.Sridhar,N.Dashora,K.Stachowicz,K.Black,N.Hirose,
and S. Levine, “ViNT: A foundation model for visual navigation,” in
VII. CONCLUSION 7th Annual Conference on Robot Learning, 2023. [Online]. Available:
https://arxiv.org/abs/2306.14846
In this paper, we proposed a deep AIF framework for real-
[13] D.LawsonandA.H.Qureshi,“Controltransformer:Robotnavigationin
world navigation that incorporates a diffusion policy as the unknownenvironmentsthroughprm-guidedreturn-conditionedsequence
policy model and an MTRSSM as the world model. The modeling,” in 2023 IEEE/RSJ International Conference on Intelligent
RobotsandSystems(IROS),2023,pp.9324–9331.
diffusionpolicyenabledthegenerationofdiverseandcontext-
[14] A.Sridhar,D.Shah,C.Glossop,andS.Levine,“Nomad:Goalmasked
dependent action candidates, while the MTRSSM provided diffusion policies for navigation and exploration,” in 2024 IEEE In-
stablelong-horizonpredictionsthatpreservedtheenvironmen- ternationalConferenceonRoboticsandAutomation(ICRA),2024,pp.
63–70.
tal structure. The integration of the policy and world models
[15] J. Liang, A. Payandeh, D. Song, X. Xiao, and D. Manocha, “Dtg :
withintheEFEformulationenabledtheepistemicandextrinsic Diffusion-based trajectory generation for mapless global navigation,”
values to be exploited effectively, resulting in improved nav- 2024.[Online].Available:https://arxiv.org/abs/2403.09900
[16] Y.Cao,J.Lew,J.Liang,J.Cheng,andG.Sartoretti,“Dare:Diffusion
igation performance compared with baseline methods. These
policy for autonomous robot exploration,” in Submission to IEEE
findings demonstrate that recent advances in deep generative InternationalConferenceonRoboticsandAutomation(ICRA),2025.
modeling can substantially enhance the scalability of AIF in [17] W.Yu,J.Peng,H.Yang,J.Zhang,Y.Duan,J.Ji,andY.Zhang,“Ldp:
A local diffusion planner for efficient robot navigation and collision
real-world robotic systems.
avoidance,” in 2024 IEEE/RSJ International Conference on Intelligent
Looking ahead, we identify three promising directions for RobotsandSystems(IROS),2024,pp.5466–5472.
future research. First, incorporating natural language instruc- [18] B. Liao, S. Chen, H. Yin, B. Jiang, C. Wang, S. Yan, X. Zhang,
X. Li, Y. Zhang, Q. Zhang, and X. Wang, “Diffusiondrive: Truncated
tions as goal specifications might enable more flexible and
diffusion model for end-to-end autonomous driving,” arXiv preprint
intuitive navigation. Second, leveraging pretrained foundation arXiv:2411.15139,2024.
models might facilitate adaptation to entirely unseen environ- [19] W. Cai, J. Peng, Y. Yang, Y. Zhang, M. Wei, H. Wang, Y. Chen,
T. Wang, and J. Pang, “Navdp: Learning sim-to-real navigation
ments. Finally, extending the framework to manipulation and
diffusionpolicywithprivilegedinformationguidance,”2025.[Online].
integrating it with navigation might open the way for deep Available:https://arxiv.org/abs/2505.08712
AIF-based mobile manipulation in real-world robotics. [20] K. Friston, “Friston, k.j.: The free-energy principle: a unified brain
theory?nat.rev.neurosci.11,127-138,”Naturereviews.Neuroscience,
vol.11,pp.127–38,022010.
REFERENCES [21] K. Friston, FitzGerald, Thomas, Rigoli, Francesco, Schwartenbeck,
Philipp, Pezzulo, and Giovanni, “Active inference: A process theory,”
[1] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra, “Planning Neural Computation, vol. 29, no. 1, pp. 1–49, 01 2017. [Online].
and acting in partially observable stochastic domains,” Artificial Available:https://doi.org/10.1162/NECO a 00912
Intelligence, vol. 101, no. 1, pp. 99–134, 1998. [Online]. Available: [22] T.Parr,G.Pezzulo,andK.J.Friston,ActiveInference:TheFreeEnergy
https://www.sciencedirect.com/science/article/pii/S000437029800023X Principle in Mind, Brain, and Behavior. The MIT Press, 03 2022.
[2] S. Thrun, W. Burgard, and D. Fox, Probabilistic Robotics (Intelligent [Online].Available:https://doi.org/10.7551/mitpress/12441.001.0001
RoboticsandAutonomousAgents). TheMITPress,2005. [23] K. Friston, F. Rigoli, D. Ognibene, C. Mathys, T. Fitzgerald,
[3] S.LevineandD.Shah,“Learningroboticnavigationfromexperience: and G. Pezzulo, “Active inference and epistemic value,” Cognitive
principles, methods and recent results,” Philosophical Transactions of Neuroscience, vol. 6, no. 4, pp. 187–214, 2015, pMID: 25689102.
the Royal Society B: Biological Sciences, vol. 378, no. 1869, Dec. [Online].Available:https://doi.org/10.1080/17588928.2015.1020053
2022.[Online].Available:http://dx.doi.org/10.1098/rstb.2021.0447 [24] de Tinguy, Daria and Verbelen, Tim and Dhoedt, Bart, “Exploring
[4] M. Nowakowski, C. Joly, S. Dalibard, N. Garcia, and F. Moutarde, and learning structure : active inference approach in navigational
“Topological localization using wi-fi and vision merged into fabmap agents,” in Active inference : 5th international workshop, IWAI
framework,”092017,pp.3339–3344. 2024, revised selected papers, Buckley, Christopher L. and Cialfi,
JOURNALOFLATEXCLASSFILES,VOL.18,NO.9,SEPTEMBER2020 13
Daniela and Lanillos, Pablo and Pitliya, Riddhi J. and Sajid, Noor [45] A. Hu, L. Russell, H. Yeo, Z. Murez, G. Fedoseev, A. Kendall,
and Shimazaki, Hideaki and Verbelen, Tim and Wisse, Martijn, J. Shotton, and G. Corrado, “Gaia-1: A generative world model for
Ed., vol. 2193. Springer, 2025, pp. 105–118. [Online]. Available: autonomousdriving,”arXivpreprintarXiv:2309.17080,2023.[Online].
http://doi.org/10.1007/978-3-031-77138-5 7 Available:https://arxiv.org/abs/2309.17080
[25] D. de Tinguy, T. Van de Maele, T. Verbelen, and B. Dhoedt, [46] X.Wang,Z.Zhu,G.Huang,X.Chen,J.Zhu,andJ.Lu,“Drivedreamer:
“Spatialandtemporalhierarchyforautonomousnavigationusingactive Towardsreal-world-drivenworldmodelsforautonomousdriving,”arXiv
inferenceinminigridenvironment,”Entropy,vol.26,no.1,p.83,Jan. preprintarXiv:2309.09777,2023.
2024.[Online].Available:http://dx.doi.org/10.3390/e26010083 [47] O.C¸atal,S.Wauthier,T.Verbelen,C.D.Boom,andB.Dhoedt,“Deep
[26] E. Delavari, J. Moore, J. Hong, and J. Kwon, “Perceptual motor active inference for autonomous robot navigation,” arXiv preprint
learning with active inference framework for robust lateral control,” arXiv:2003.03220,2020.[Online].Available:https://arxiv.org/abs/2003.
2025.[Online].Available:https://arxiv.org/abs/2503.01676 03220
[27] C.Chi,Z.Xu,S.Feng,E.Cousineau,Y.Du,B.Burchfiel,R.Tedrake, [48] O. C¸atal, T. Verbelen, T. Van de Maele, B. Dhoedt, and A. Safron,
and S. Song, “Diffusion policy: Visuomotor policy learning via action “Robot navigation as hierarchical active inference,” Neural Networks,
diffusion,”TheInternationalJournalofRoboticsResearch,2024. vol. 142, pp. 192–204, 2021. [Online]. Available: https://www.
[28] K. Fujii and S. Murata, “Hierarchical latent dynamics model with sciencedirect.com/science/article/pii/S0893608021002021
multiple timescales for learning long-horizon tasks,” in 2023 IEEE [49] D. de Tinguy, T. Verbelen, E. Gamba, and B. Dhoedt, “Bio-inspired
InternationalConferenceonDevelopmentandLearning(ICDL),2023, topological autonomous navigation with active inference in robotics,”
pp.479–485. 2025.[Online].Available:https://arxiv.org/abs/2508.07267
[29] D. Ha and J. Schmidhuber, “Recurrent world models facilitate policy [50] R. Smith, K. J. Friston, and C. J. Whyte, “A step-by-step
evolution,” in Proceedings of the 32nd International Conference on tutorial on active inference and its application to empirical data,”
NeuralInformationProcessingSystems,ser.NIPS’18. RedHook,NY, Journal of Mathematical Psychology, vol. 107, p. 102632, 2022.
USA:CurranAssociatesInc.,2018,p.2455–2467. [Online]. Available: https://www.sciencedirect.com/science/article/pii/
[30] D. Hafner, T. P. Lillicrap, M. Norouzi, and J. Ba, “Mastering S0022249621000973
atari with discrete world models,” in International Conference [51] J.Ho,A.Jain,andP.Abbeel,“Denoisingdiffusionprobabilisticmod-
on Learning Representations, 2021. [Online]. Available: https: els,” in Proceedings of the 34th International Conference on Neural
//openreview.net/forum?id=0oabwyZbOu InformationProcessingSystems,ser.NIPS’20. RedHook,NY,USA:
[31] T. Taniguchi, S. Murata, M. Suzuki, D. Ognibene, P. Lanillos, CurranAssociatesInc.,2020.
E. Ugur, L. Jamone, T. Nakamura, A. Ciria, B. Lara, and [52] Y. Yamashita and J. Tani, “Emergence of functional hierarchy in
G. Pezzulo, “World models and predictive coding for cognitive a multiple timescale neural network model: A humanoid robot
and developmental robotics: frontiers and challenges,” Advanced experiment,”PLOSComputationalBiology,vol.4,no.11,pp.1–18,11
Robotics, vol. 37, no. 13, pp. 780–806, 2023. [Online]. Available: 2008.[Online].Available:https://doi.org/10.1371/journal.pcbi.1000220
https://doi.org/10.1080/01691864.2023.2225232 [53] A.AhmadiandJ.Tani,“Anovelpredictive-coding-inspiredvariational
[32] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. rnnmodelforonlinepredictionandrecognition,”NeuralComputation,
Gomez, L. u. Kaiser, and I. Polosukhin, “Attention is all you vol. 31, no. 11, pp. 2025–2074, 11 2019. [Online]. Available:
need,” in Advances in Neural Information Processing Systems, https://doi.org/10.1162/neco a 01228
I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, [54] S. Murata, Y. Yamashita, H. Arie, T. Ogata, S. Sugano, and J. Tani,
S. Vishwanathan, and R. Garnett, Eds., vol. 30. Curran Associates, “Learningtoperceivetheworldasprobabilisticordeterministicviain-
Inc., 2017. [Online]. Available: https://proceedings.neurips.cc/paper teractionwithothers:Aneuro-roboticsexperiment,”IEEETransactions
files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf onNeuralNetworksandLearningSystems,vol.28,no.4,pp.830–848,
[33] D.HaandJ.Schmidhuber,“Worldmodels,”2018.[Online].Available: 2017.
https://zenodo.org/record/1207631 [55] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional
[34] P. Wu, A. Escontrela, D. Hafner, K. Goldberg, and P. Abbeel, networks for biomedical image segmentation,” 2015. [Online].
“Daydreamer: World models for physical robot learning,” 2022. Available:https://arxiv.org/abs/1505.04597
[Online].Available:https://arxiv.org/abs/2206.14176 [56] J. Song, C. Meng, and S. Ermon, “Denoising diffusion implicit
[35] R.Sekar,O.Rybkin,K.Daniilidis,P.Abbeel,D.Hafner,andD.Pathak, models,”2022.[Online].Available:https://arxiv.org/abs/2010.02502
“Planningtoexploreviaself-supervisedworldmodels,”inICML,2020. [57] K.He,X.Zhang,S.Ren,andJ.Sun,“Deepresiduallearningforimage
[36] D. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and recognition,”in2016IEEEConferenceonComputerVisionandPattern
J. Davidson, “Learning latent dynamics for planning from pixels,” Recognition(CVPR),2016,pp.770–778.
2019.[Online].Available:https://arxiv.org/abs/1811.04551 [58] W. Shi, J. Caballero, F. Husza´r, J. Totz, A. P. Aitken, R. Bishop,
[37] J. S. V, S. Jalagam, Y. LeCun, and V. Sobal, “Gradient-based D. Rueckert, and Z. Wang, “Real-time single image and video super-
planning with world models,” 2023. [Online]. Available: https: resolutionusinganefficientsub-pixelconvolutionalneuralnetwork,”in
//arxiv.org/abs/2312.17227 2016 IEEE Conference on Computer Vision and Pattern Recognition
[38] J. Chun, Y. Jeong, and T. Kim, “Sparse imagination for efficient (CVPR),2016,pp.1874–1883.
visual world model planning,” 2025. [Online]. Available: https: [59] C.TallecandY.Ollivier,“Unbiasingtruncatedbackpropagationthrough
//arxiv.org/abs/2506.01392 time,” arXiv preprint arXiv:1705.08209, 2017. [Online]. Available:
[39] W. Liu, H. Zhao, C. Li, J. Biswas, B. Okal, P. Goyal, Y. Chang, and https://arxiv.org/abs/1705.08209
S. Pouya, “X-mobility: End-to-end generalizable navigation via world
modeling,”arXivpreprintarXiv:2410.17491,2024.[Online].Available:
https://arxiv.org/abs/2410.17491
[40] R. P. Poudel, H. Pandya, S. Liwicki, and R. Cipolla, “ ReCoRe:
Regularized Contrastive Representation Learning of World Model ,”
in 2024 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR). Los Alamitos, CA, USA: IEEE Computer
Society, Jun. 2024, pp. 22904–22913. [Online]. Available: https:
//doi.ieeecomputersociety.org/10.1109/CVPR52733.2024.02161
[41] Y. Duan, W. Mao, and H. Zhu, “Learning world models for
unconstrained goal navigation,” 2024. [Online]. Available: https:
//arxiv.org/abs/2411.02446
[42] A.Bar,G.Zhou,D.Tran,T.Darrell,andY.LeCun,“Navigationworld
models,” arXiv preprint arXiv:2412.03572, 2024. [Online]. Available:
https://arxiv.org/abs/2412.03572
[43] D.Nie,X.Guo,Y.Duan,R.Zhang,andL.Chen,“Wmnav:Integrating
vision-languagemodels intoworld modelsforobject goalnavigation,”
2025.[Online].Available:https://arxiv.org/abs/2503.02247
[44] B. Kayalibay, A. Mirchev, P. van der Smagt, and J. Bayer, “Tracking
and planning with spatial world models,” 2022. [Online]. Available:
https://arxiv.org/abs/2201.10335

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Deep Active Inference with Diffusion Policy and Multiple Timescale World Model for Real-World Exploration and Navigation"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
