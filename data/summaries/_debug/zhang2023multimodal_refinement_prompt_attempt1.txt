=== IMPORTANT: ISOLATE THIS PAPER ===
You are revising a summary for ONLY the paper below. Do NOT reference or use content from any other papers.
Paper Title: Multi-Modal MPPI and Active Inference for Reactive Task and Motion Planning
Citation Key: zhang2023multimodal
REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Issues to fix:
1. Too short: 151 words (minimum 200)

Current draft:
### OverviewThis summary of “Multi-Modal MPPI and Active Inference for Reactive Task and Motion Planning” synthesizes the paper’s key contributions, methods, and findings. The research addresses the challenge of robust task execution in dynamic environments by combining an Active Inference planner with a Model Predictive Path Integral controller. This approach enables reactive action selection and low-level motion planning during task execution, allowing the system to adapt to unforeseen geometric constraints and disturbances.### MethodologyThe authors introduce a novel Multi-Modal MPPI (M3P2I) controller that samples in parallel different plan alternatives to achieve a goal. The AIP generates alternative symbolic plans, each linked to a cost function for M3P2I. The cost functions are evaluated by sampling control input sequences using importance sampling. 

The M3P2I controller then computes one coherent control input that minimizes the given costs. 

The AIP computes N alternative symbolic plans based on the current symbolic state and the available symbolic actions

Key terms: task, execution, planning, motion, reactive, mppi, level, different

=== FULL PAPER TEXT ===
IEEEROBOTICSANDAUTOMATIONLETTERS.PREPRINTVERSION. 1
Multi-Modal MPPI and Active Inference for
Reactive Task and Motion Planning
Yuezhe Zhang, Corrado Pezzato, Elia Trevisan, Chadi Salmi, Carlos Herna´ndez Corbato, Javier Alonso-Mora
Abstract—TaskandMotionPlanning(TAMP)hasmadestrides scene knowledge. Additionally, scenarios like pick-and-place
in complex manipulation tasks, yet the execution robustness of taskswithdynamicobstaclesandhumandisturbancesdemand
the planned solutions remains overlooked. In this work, we
varied grasping poses for different objects and obstacles,
propose a method for reactive TAMP to cope with runtime
requiring TAMP algorithms to adapt to such configurations
uncertainties and disturbances. We combine an Active Inference
planner (AIP) for adaptive high-level action selection and a dynamically.
novel Multi-Modal Model Predictive Path Integral controller Weaddressthesechallengesbyproposingacontrolscheme
(M3P2I) for low-level control. This results in a scheme that that jointly achieves reactive action selection and robust
simultaneously adapts both high-level actions and low-level mo-
low-level motion planning during execution. We propose a
tions. The AIP generates alternative symbolic plans, each linked
high-level planner capable of providing alternative actions to
to a cost function for M3P2I. The latter employs a physics
simulatorfordiversetrajectoryrollouts,derivingoptimalcontrol achieve a goal. These actions are translated to different cost
by weighing the different samples according to their cost. This functions for our new Multi-Modal Model Predictive Path
ideaenablesblendingdifferentrobotskillsforfluidandreactive Integral controller for motion planning. This motion planner
plan execution, accommodating plan adjustments at both the
leverages a physics simulator to sample parallel motion plans
highandlowlevelstocope,forinstance,withdynamicobstacles
that minimize the given costs and computes one coherent
or disturbances that invalidate the current plan. We have tested
our approach in simulations and real-world scenarios. Project control input that effectively blends different strategies. To
website: https://autonomousrobots.nl/paper websites/m3p2i-aip achieve this, we build upon two of our recent works: 1)
an Active Inference planner (AIP) [7] for symbolic action
Index Terms—Task and Motion Planning, Manipulation Plan-
ning selection, and 2) a Model Predictive Path Integral (MPPI)
controller [10] for motion planning. The AIP computes a
sequenceofactionsandstatetransitionsthroughbackchaining
I. INTRODUCTION
toachieveasub-goalspecifiedinagivenBehaviorTree(BT).
TASK and Motion Planning (TAMP) is a powerful class The BT guides the search and allows real-time high-level
of methods for solving complex long-term manipulation planning within the AIP framework [7]. In this work, we
problems where logic and geometric variables influence each extend the previous AIP to plan possible alternative action
other.TAMP[1]–[3]hasbeensuccessfullyappliedtodomains plans, and we propose a new Multi-Modal Model Predictive
such as table rearrangement, stacking blocks, or solving the Path Integral controller (M3P2I) that can sample in parallel
Hanoi tower. However, the plan is often executed in open- these alternatives and smoothly blend them considering the
loop in static environments. Recent works [4]–[6] recognized geometric constraints of the problem.
the importance of robustifying the execution of TAMP plans
to be able to carry them out in the real world reliably. But
A. Related work
they either rely only on the adaptation of the action sequence
in a plan [6]–[9] or only on the motion planning problem in a To robustly operate in dynamic environments, reactive mo-
dynamicenvironmentgivenafixedplan[4],[5].Unliketypical tion planners are necessary. In [4], the authors provided a
TAMP planners that focus on solving static and complex reactiveModelPredictiveControl(MPC)strategytoexecutea
tasks offline and then execute the solution, this paper aims to TAMP plan as a given linear sequence of constraints. The re-
achieve reactive execution by simultaneously adapting high- active nature of the approach allows coping with disturbances
level actions and low-level motions. and dynamic collision avoidance during the execution of a
Reactive TAMP faces the challenge of accommodating TAMPplan.Authorsin[5]formulatedaTAMPplaninobject-
unforeseen geometric constraints during planning, such as centricCartesiancoordinates,showinghowthisallowscoping
the need to pull rather than push a block when it’s in a with perturbations such as moving a target location. However,
corner, complicating high-level planning without complete both[4],[5]donotconsideradaptationatthesymbolicaction
level if a perturbation invalidates the current plan.
This research was supported in part by Ahold Delhaize; by the Nether- Several papers focused on adapting and repairing high-
landsOrganizationforScientificResearch(NWO),domainScience(ENW),
level action sequences during execution. In [11], robot task
TRILOGY project; and by the European Union through ERC, under Grant
101041863(INTERACT).(Correspondingauthor:YuezheZhang.) plans are represented as robust logical-dynamical systems to
TheauthorsarewithCognitiveRoboticsDepartment,TUDelft,TheNether- handlehumandisturbances.Similarly,[12]coordinatescontrol
lands yuezhezhang_bit@163.com, {corrado.pezzato,
chains for robust plan execution through plan switching and
salmi.chadi}@gmail.com, {e.trevisan,
c.h.corbato, j.alonsomora}@tudelft.nl controller selection. A recent paper [13] suggests employing
4202
luJ
01
]OR.sc[
2v82320.2132:viXra
2 IEEEROBOTICSANDAUTOMATIONLETTERS.PREPRINTVERSION.
Monte Carlo Tree Search with IsaacGym to accelerate task We refer the interested reader to the original articles [7], [22],
planning for multi-step object retrieval from clutter involving [23] for a more in-depth understanding of the techniques.
intricate physical interactions. While promising, [13] only
supports high-level reasoning with predefined motions in an
A. Active Inference Planner (AIP)
open loop. Recent works [6], [14] combined BTs and linear
temporal logic to adapt the high-level plan to cope with coop- AIP is a high-level decision-making algorithm that relies
erativeoradversarialhumanoperators,environmentalchanges, on symbolic states, observations, and actions [7]. Each in-
or failures. In our previous work [7], AIP and BT were dependent set of states in AIP is a factor, and the planner
combined to provide reactive action selection in long-term contains a total of n f factors. For a generic factor f j where
tasks in partially observable and dynamic environments. This j ∈J ={1,...,n f }, it holds:
methodachievedhierarchicaldeliberationandcontinualonline
(cid:104) (cid:105)⊤
planning, making it particularly appealing for the problem s(fj) = s(fj,1),s(fj,2),...,s(fj,m(fj)) ,
of reactive TAMP at hand. In this paper, we extend [7] by S = (cid:8) s(fj)|j ∈J (cid:9) (1)
bridgingthegaptolow-levelreactivecontrolbyplanningcost
functions instead of symbolic actions. where m(fj) is the number of mutually exclusive symbolic
At the lower level, MPC is a widely used approach [15]– values a state factor can have, each entry of s(fj) is a real
[17].However,manipulationtasksofteninvolvediscontinuous value between 0 and 1, and the sum of the entries is 1. This
contacts that are hard to differentiate. Sampling-based MPCs, represents the current belief state.
such as MPPI [18], [19], can handle non-linearities, non- The continuous state of the world x ∈ X is discretized
convexities, or discontinuities of the dynamics and costs. through a symbolic observer such that the AIP can use it.
MPPI relies on sampling control input sequences and forward Discretized observations o are used to build a probabilistic
system dynamics simulation. The resulting trajectories are belief about the symbolic current state. Assuming one set of
weighted according to their cost to approximate an optimal observations per state factor with r(fj) possible values:
controlinput.In[20],theauthorsproposedanensembleMPPI
(cid:104) (cid:105)⊤
to cope with model parameters uncertainty. Sampling-based o(fj) = o(fj,1),o(fj,2),...,o(fj,r(fj)) ,
MPCs are generally applied for single-skill execution, such
as pushing or reaching a target point. As pointed out in the
O = (cid:8) o(fj)|j ∈J (cid:9) (2)
future work of [21], one could use a high-level agent to
The robot has a set of symbolic actions that can act then
set the cost functions for the sampling-based MPC for long-
their corresponding state factor:
horizon cognitive tasks. We follow this line of thought and
propose a method to reactively compose cost functions for a ∈α(fj) = (cid:8) a(fj,1),a(fj,2),...,a(fj,k(fj))(cid:9) ,
τ
long-horizon tasks. Moreover, classical MPPI approaches can A= (cid:8) α(fj)|j ∈J (cid:9) (3)
onlykeeptrackofonecostfunctionatatime.Thismeansthe
task planner should propose a single plan to solve the task. where k(fj) is the number of actions that can affect a specific
However, some tasks might present geometric ambiguities for state factor f
j
. Each generic action a(fj,·) has associated a
which multiple plans could be effective, and selecting what symbolic name, parameters, pre- and postconditions:
strategy to pursue can only be determined by the motion
planner based on the geometry of the problem. Actiona(fj,·) Preconditions Postconditions
action_name(par) prec post
a(fj,·) a(fj,·)
B. Contributions
where prec and post are first-order logic predi-
The main contribution of this work is a reactive task and
a(fj,·) a(fj,·)
cates that can be evaluated at run-time. A logical predicate is
motion planning algorithm based on the following:
aboolean-valuedfunctionB :X →{true,false}.Finally,
• A new Multi-Modal MPPI (M3P2I) capable of sampling wedefinethelogicalstatel(fj) asaone-hotencodingofs(fj).
in parallel plan alternatives to achieve a goal, evaluating The AIP computes the posterior distribution over p plans π
them against different costs. This enables the smooth through free-energy minimization [7]. The symbolic action to
blending of alternative solutions into a coherent behavior be executed by a robot in the next time step is the first action
instead of switching based on heuristics. of the most likely plan, denoted with π :
ζ,0
• An enhanced Active Inference planner (AIP) capable of
generating alternative cost functions for M3P2I. ζ =max([π 1 ,π 2 ,...,π p ]), a τ=0 =π ζ,0 . (4)
(cid:124) (cid:123)(cid:122) (cid:125)
We demonstrate the method in several scenarios in simula-
π⊤
tionsandrealrobotsforpushing,pulling,picking,andplacing
objects under disturbances.
B. Model Predictive Path Integral Control (MPPI)
MPPI is a method for solving optimal stochastic problems
II. BACKGROUND
in a sampling-based fashion [22], [23]. Let us consider the
In this section, we present the background knowledge
following discrete-time systems:
about the Active Inference planner and Model Predictive Path
Integral Control to understand the contributions of this paper. x =f(x ,v ), v ∼N(u ,Σ), (5)
t+1 t t t t
ZHANGetal.:MULTI-MODALMPPIANDACTIVEINFERENCEFORREACTIVETASKANDMOTIONPLANNING 3
where f, a nonlinear state-transition function, describes how manuallysetorbeencodedastheskeletonsolutionofaBTas
the state x evolves over time t with a control input v . u and previous work [7]. The AIP computes N alternative symbolic
t t
Σ are the commanded input and the variance, respectively. plans based on the current symbolic state and the available
K noisy input sequences V are sampled and then applied symbolic actions. The symbolic actions are encoded as action
k
to the system to forward simulate K state trajectories Q , templates with pre-post conditions that Active Inference uses
k
k ∈ [0,K −1], over a time horizon T. Given the state tra- to construct action sequences to achieve the desired state.
jectories Q and a designed cost function C to be minimized, After the plans are generated, the plan interface links the first
k
the total state-cost S of an input sequence V is computed action a , i = 0...N −1 of each plan to a cost function
k k 0,i
by evaluating S = C(Q ). Finally, each rollout is weighted C . The cost functions are sent to M3P2I, which samples
k k i
by the importance sampling weights w . These are computed N ·K different control input sequences. The input sequences
k
through an inverse exponential of the cost S with tuning are forward simulated using IsaacGym, which encodes the
k
parameter β and normalized by η. For numerical stability, the dynamics of the problem [10]. The resulting trajectories are
minimumsampledcostρ=min S issubtracted,leadingto: evaluatedagainsttheirrespectivecosts.Finally,animportance
k k
sampling scheme calculates the approximate optimal control
(cid:18) (cid:19) K
w = 1 exp − 1 (S −ρ) , (cid:88) w =1 (6) u∗. All processes are running continuously during execution
k η β k k 0
at different frequencies. The action planner runs, for instance,
k=1
The parameter β is called inverse temperature. The impor- at 1Hz while the motion planner runs at 25Hz. An overview
tance sampling weights are finally used to approximate the can be found in Algorithm 1.
optimal control input sequence U∗:
Algorithm 1 Overview of the method
K
U∗ = (cid:88) w V (7) 1: Input: action templates and inputs from Algorithm 2 to 4
k k
2: AIP.task =AIP.agent(ActionTemplates)
k=1
The first input u∗ of the sequence U∗ is applied to the 3: while task not completed do
system, and the proc 0 ess is repeated. At the next iteration, U∗ 4: o←GetSymbolicObservation(x)
5: /* Get current desired state */
isusedasawarm-start,time-shiftedbackwardofonetimestep.
Specifically, the second last input in the shifted sequence is 6: AIP.s d ←BT(o) or be manually set ▷ from [7]
7: /* Get current action plans from Active Inference */
also propagated to the last input. In this work, we build upon
8: P ←AIP.parall act sel(o) ▷ Algorithm 2
ourpreviousMPPIapproaches[10],[24],whereweemployed
9: /* Translate action plan to cost function */
IsaacGym as a dynamic model to forward simulate trajectory
10: C ←Interface(P)
rollouts and allow for arbitrary sampling distributions.
11: /* Compute motion commands */
III. METHODOLOGY 12: M3P2I.command(C) ▷ Algorithm 4
13: end while
The proposed method is depicted in Fig. 1. After a general
overview, we discuss the three main parts of the scheme:
action planner, motion planner, and plan interface.
B. Action planner - Active Inference Planner (AIP)
A. Overview
In contrast to our previous work [7] where only one action
The proposed scheme works as follows. First, the symbolic a for the next time step is computed, we modify the AIP to
τ
observers translates continuous states x into discretized sym- generate action alternatives. In particular, instead of stopping
bolicobservationso,whicharethenpassedtotheactionplan- the search for a plan when a valid executable action a is
τ
ner. The current desired state s d for Active Inference can be found,werepeatthesearchwhileremovingthatsamea τ from
the available action set A. This simple change is effective
because we are looking for alternative actions to be applied
Action planner
Behavior at the next step, and the AIP builds plans backward from the
symbolic plans Tree Symbolic
Observer desired state [7]. The pseudocode is reported in Algorithm 2.
Active
Inference The algorithm will cease when no new actions are found,
System
Plan interface Motion planner returning a list of possible plans P. This planner is later
Se C le o c s t t or M3P2I integratedwithM3P2Itoevaluatedifferentalternativesinreal-
time.Thisincreasestherobustnessatrun-timeand,atthesame
Set of costs time, reduces the number of heuristics to be encoded in the
actionplanner.Specifically,onedoesnotneedtoencodewhen
topreferasymbolicactionoveranotherbasedonthegeometry
Hard-coded knowledge Utilities Low frequency of the problem.
Planning algorithms IsaacGym High frequency
Fig.1. Proposedscheme.Givensymbolicobservationsooftheenvironment, C. Motion planner - Multi-Modal MPPI (M3P2I)
theactionplannercomputesN differentplanalternativeslinkedtoindividual
cost functions Ci. M3P2I samples control input sequences and uses an We propose a Multi-Modal MPPI capable of sampling
importancesamplingschemetoapproximatetheoptimalcontrolu∗ 0 . different plan alternatives from the AIP. Traditional MPPI
4 IEEEROBOTICSANDAUTOMATIONLETTERS.PREPRINTVERSION.
Algorithm2GeneratealternativeplansusingActiveInference Algorithm 3 Update inverse temperature β
i
1: Input: available action set: A 1: Input: parameters: η l ,η u
2: a τ ←AIP.act sel(o) ▷ from [7] 2: while η i ∈/ [η l ,η u ] do
3: Set P ←∅ 3: ρ i ←min k∈κ(i) S i (V k ) ▷ (11)
(cid:16) (cid:17)
4 5 : : wh P il . e ap a p τ e ! n = d( n a o τ n ) e do 5 4 : : i η f i η ← i > (cid:80) η k u ∈κ th (i e ) n exp −Si(V β k i ) ▷ −ρ g i reater than upper ▷ bo ( u 1 n 0 d )
6: A=A\{a τ } 6: β i =0.9∗β i
7: a τ ←AIP.act sel(o) ▷ from [7] 7: else if η i <η l then ▷ smaller than lower bound
8: end while 8: β i =1.2∗β i
9: Return P 9: end if
10: end while
11: Return ρ i ,η i ,β i
approaches consider one cost function and one sampling
distribution. In this work, we propose keeping track of N
separate control input sequences corresponding to N different
to denote the action sequence of plan i over a time horizon
plan alternatives/costs. This is advantageous because it offers
µ = [µ ,µ ,...,µ ]. Each sequence is weighted by
ageneralapproachtoexploringdifferentstrategiesinparallel. i i,0 i,1 i,T−1
the corresponding weights leading to:
We perform N separate sets of importance weights, one for
(cid:88)
eachalternative,andonlyultimately,wecombinetheweighted µ = ω (V )V (12)
i i k k
controlinputsinonecoherentcontrol.Thisallowsthesmooth
k∈κ(i)
blending of different strategies. Assume we consider N alter-
At every iteration, we add to µ the sampled noise from
native plans, a total of N ·K samples. Assume the cost of i
Halton splines [19]. Then, we forward simulate the state
plan i,i∈[0,N) to be formulated as:
trajectories Q (V ) using IsaacGym as in [10]. Finally, given
i k
T (cid:88) −1 the state trajectories corresponding to the plan alternatives,
S (V )= γtC (x ,v ) (8)
i k i t,k t,k we need to compute the weights and mean for the overall
t=0 control sequence. To do so, we concatenate the N state-costs
∀k ∈ κ(i) where κ(i) is the integer set of indexes ranging S (V ),i ∈ [0,N) and represent it as S˜(V). Therefore, we
i k
from i·K to (i+1)·K −1. State x and control input calculate the weights for the whole control sequence as [19]:
t,k
v t,k are indexed based on the time t and trajectory k. The 1 (cid:18) 1 (cid:16) (cid:17)(cid:19)
randomcontrolsequenceV =[v ,v ,...,v ]defines ω˜(V)= exp − S˜(V)−ρ (13)
k 0,k 1,k T−1,k η β
the control inputs for trajectory k over a time horizon T. The
trajectory Q (V )=[x ,x ,...,x ] is determined by Similarly, η,ρ are computed as in (10) and (11) but consid-
i k 0,k 1,k T−1,k
the control sequence V and the initial state x . C is the eringS˜(V)instead.Theoverallmeanactionovertimehorizon
k 0,k i
cost function for plan i. Finally, γ ∈[0,1] is a discount factor T is denoted as u=[u ,u ,...,u ]. For each timestep t:
0 1 T−1
that evaluates the importance of accumulated future costs.
N·K−1
As in classical MPPI approaches, given the costs S (V ), we (cid:88)
i k u =(1−α )u +α ω˜ (V)v (14)
t u t−1 u k t,k
cancomputetheimportancesamplingweightsassociatedwith
k=0
each alternative as:
where α is the step size that regularizes the current solution
(cid:18) (cid:19) u
1 1
ω (V )= exp − (S (V )−ρ ) ,∀k ∈κ(i) (9) to be close to the previous u t−1 . The optimal control is set
i k η i β i i k i to u∗ 0 = u 0 . Note that through (13), we can smoothly fuse
(cid:18) (cid:19)
(cid:88) 1 different strategies to achieve a goal in a general way.
η = exp − (S (V )−ρ ) (10)
i β i k i The pseudocode is summarized in Algorithm 4. After the
i
k∈κ(i)
initialization, we sample Halton splines and forward simulate
ρ = min S (V ) (11)
i i k the plan alternatives using IsaacGym to compute the costs
k∈κ(i)
(Lines 8-18). The costs are then used to update the weights
We use the insight in [10] to 1) sample Halton splines for each plan and update their means (Lines 20-24). Finally,
instead of Gaussian noise for smoother behavior, 2) auto- the mean of the overall action sequence is updated (Line 28),
matically tune the inverse temperature β i to maintain the and the first action from the mean is executed.
normalization factor η within certain bounds. The latter is
i
helpful since η indicates the number of samples to which
i
D. Plan interface
significant weights are assigned. If η is close to the number
i
of samples K, an unweighted average of sampled trajectories The plan interface is a component that takes the possible
willbetaken.Ifη iscloseto1,thenthebesttrajectorysample alternative symbolic actions in P and links them to their
i
will be taken. We observed that setting η between 5% and corresponding cost functions, forwarding the latter to M3P2I.
i
10% of K generates smooth trajectories. As opposed to [10], Foreverysymbolicactionarobotcanperform,westoreacost
we update η within a rollout to stay within bounds instead function in a database that we can query at runtime, bridging
of updating it once per iteration, see Algorithm 3. We use µ the output of the action planner to the motion planner.
i
ZHANGetal.:MULTI-MODALMPPIANDACTIVEINFERENCEFORREACTIVETASKANDMOTIONPLANNING 5
Algorithm 4 Multi-Modal Model Predictive Path Integral This scenario is depicted in Fig. 2. One object has to be
Control (M3P2I) placed to a goal, situated in one of the corners of an arena.
1: Input: cost functions: C i ,∀i∈[0,N) The object can have different initial locations, for instance, in
2: Parameters: N,K,T the middle of the arena or on one of the corners. There are
3: Initial sequence: µ i =0,u=0,∈RT ∀i∈[0,N) also static and dynamic obstacles, and the robot can push or
4: while task not completed do pull the object. We define the following action templates for
5: x←GetStateEstimate() AIP and the cost functions for M3P2I.
6: InitIsaacGym(x) 1) ActiontemplatesforAIP: TheAIPforthistaskrequires
7: /* Begin parallel sampling of alternatives */ one state s(goal) and a relative symbolic observation o(goal)
8: for i=0 to N −1 do thatindicateswhenanobjectisatthegoal.Thisisdefinedas:
9: for k ∈κ(i) do (cid:40)
0,||p −p ||≤δ
10: S i (V k )←0 o(goal)) = G O (15)
1,||p −p ||>δ
11: Sample noise E k ←SampleHaltonSplines() G O
12: µ i ←BackShift(µ i ) wherep G ,p O representthepositionsofthegoalandtheobject
13: for t=0 to T −1 do ina3Dcoordinatesystem.δisaconstantthresholddetermined
14: Q i (V k )←ComputeTrajIsaacGym(µ i +E k ) by the user. The mobile robot can either push, pull, or move.
15: S i (V k )←UpdateCost(C i ,Q i (V k )) ▷ (8) These skills are encoded in the action planner as follows:
16: end for
Actions Preconditions Postconditions
17: end for
push(obj,goal) - l(goal)=[10]⊤
18: end for pull(obj,goal) - l(goal)=[10]⊤
19: /* Begin computing trajectory weights */
20: for i=0 to N −1 do The postcondition of the action push(obj, goal) is
21: ρ i ,η i ,β i ←UpdateInvTemp(i) ▷ Algorithm 3 thattheobjectisatthegoal,similarlyforthepullaction.Note
(cid:16) (cid:17)
22: ω i (k) (cid:80) ← η 1 i exp − β 1 i (S i (V k )−ρ i ) ,∀k ▷ (9) t r h el a a t t w io e ns d i o n n t o h t e a t d as d k c p o l m an p n le e x rt h o eu d r e i t s e t r i m cs in to e e w n h c e o n de to th p e us g h eo o m rp e u tr l i l c ;
23: µ i = k∈κ(i) ω i (V k )V k ▷ (12)
instead,wewillexploitparallelsamplinginthemotionplanner
24: end for
later. The desired state s of this task is set as a preference
25: /* Begin control update */ d
26: ω˜(V)= 1 exp (cid:16) −1 (cid:16) S˜(V)−ρ (cid:17)(cid:17) ▷ (13) for l(goal) = [1 0]⊤. The BT would contain more desired
η β states for pushing or pulling several blocks. Our approach
27: for t=0 to T −1 do
28: u t =(1−α u )u t−1 +α u (cid:80)N j= ·K 0 −1ω˜ k v t,k ▷ (14) c in a s n ta b n e ce e , x a te n n d d a e c d co to m m m u o l d ti a p te le m o o b r j e ec i t n s v i o n lv d e i d ff p er r e e n -p t o l s o t c c a o ti n o d n i s t , io f n o s r
29: end for
and fallbacks since it has the same properties as in [7].
30: ExecuteCommand(u∗ 0 =u 0 ) 2) CostfunctionsforM3P2I: Weneedtospecifyacostfor
31: u=BackShift(u)
each symbolic action. The cost function for pushing object O
32: end while
to the goal G is defined as:
C (R,O,G)=C (R,O)+C (O,G)+C (O,G)
push dist dist ori
IV. EXPERIMENTS +C align push (R,O,G)
(16)
Weevaluatetheperformanceofourmethodintwodifferent
whereminimizingC (O,G)=ω ·||p −p ||makesthe
scenarios. The first is a push-pull scenario for non-prehensile dist dist G O
object O close to the goal G. C (O,G)=ω ·ϕ(Σ ,Σ )
manipulation of an object with an omnidirectional robot. The ori ori O G
defines the orientation cost between the object O and goal G.
secondisaobjectstackingscenariowitha7-DOFmanipulator
We define ϕ for symmetric objects as:
with dynamic obstacles and external disturbances at runtime.
ϕ(Σ ,Σ )= min (2−||⃗u ·⃗v ||−||⃗u ·⃗v ||) (17)
u v 1 i 2 j
i,j∈{1,2,3}
A. Push-pull scenario where Σ ={⃗u ,⃗u ,⃗u },Σ ={⃗v ,⃗v ,⃗v } form the orthog-
u 1 2 3 v 1 2 3
onal bases of two coordinates systems. Minimizing this cost
makes two axes in the coordinate systems of the object and
Push-pull scenario
goalcoincide.Theorientationcostforasymmetricobjectscan
Walls Robot Goal be extended from (17) by aligning the corresponding axes.
Object to push The align cost C (R,O,G) is defined as:
align push
Fix. Obst. C (R,O,G)=ω ·h(cos(θ)), (18)
align push align push
Movable Obj. (p −p )·(p −p )
Dyn. Obst. cos(θ)= R O G O , (19)
||p −p ||·||p −p )||
R O G O
(cid:40)
Fig. 2. Push-pull scenario. The dark purple object has to be placed on the 0, cos(θ)≤0
h(cos(θ))= (20)
greenarea.Therobotcanpullorpushtheobjectwhileavoidingdynamicand
cos(θ), cos(θ)>0
fixedobstacles.Theobjectsandgoalscanhavedifferentinitialpositions.
6 IEEEROBOTICSANDAUTOMATIONLETTERS.PREPRINTVERSION.
This makes the object O lie at the center of robot R and or combine the two through our M3P2I. The AIP plans for
goal G so that the robot can push it, as illustrated in Fig. 3. the two alternatives, pushing and pulling, and forwards the
solution to the plan interface. Then, M3P2I starts minimizing
Object the costs until the AIP observes the completion of the task.
General case Weperformed20trialspercase,perarenaconfiguration,fora
Goal total of 120 simulations. By only pulling an object, the robot
cannot tightly place it on top of the goal in the corner; on
the other hand, by only pushing, the robot cannot retrieve the
object from the corner. Using multi-modal motion, we can
Push configuration Pull configuration complete the task in every tested configuration. Table I shows
that the multi-modal case outperforms push and pull in both
Fig.3. Pushandpullidealconfigurations.TherobotRhastopushorpull
theobjectO tothegoalG. arenaconfigurations.Itpresentslowerpositionandorientation
errors and a shorter planning and execution time.
Similarly, the cost function of making the robot R pull
TABLEI
object O to the goal G can be formulated as: SIMULATIONRESULTSOFPUSHANDPULL
Mean(std) Mean(std) Mean(std)
C pull (R,O,G)=C dist (R,O)+C dist (O,G)+C ori (O,G) Case Skill poserror orierror time(s)
+C align pull (R,O,G)+C act pull (R,O,G) Push 0.1061 0.0198 6.2058
(21) (0.0212) (0.0217) (6.8084)
Middle- 0.1898 0.0777 25.1032
Pull
where the align cost C (R,O,G) makes the robot R corner (0.0836) (0.1294) (13.7952)
align pull
Multi- 0.1052 0.0041 3.7768
liebetweentheobjectOandgoalG,seeFig.3.Whilepulling,
modal (0.0310) (0.0045) (0.8239)
we simulate a suction force in IsaacGym, and we are only
7.2679 0.0311
allowed to sample control inputs that move away from the Push time-out
(3.2987) (0.0929)
object through C (R,O,G). Mathematically: Corner- 0.3065 0.1925 32.8838
act pull Pull
corner (0.1778) (0.2050) (7.9240)
C (R,O,G)=ω ·h(−cos(θ)) (22) Multi- 0.1375 0.0209 9.9473
align pull align pull
modal (0.0091) (0.0227) (3.4591)
(p −p )·⃗u
C (R,O,G)=ω ·h( O R )
act pull act pull ||p −p ||·||⃗u||
O R
(23) B. Object stacking scenario
An example can be seen in Fig. 4. We also consider We address the challenge of stacking objects with external
an additional cost C (R,D) to avoid collisions with taskdisruptions,necessitatingadaptiveactionslikere-grasping
dyn obs
(dynamic) obstacles while operating. The dynamic obstacle is with different pick configurations (e.g., top or side picking in
assumed to move in a certain direction with constant velocity. Fig. 5). We showcase the robot’s ability to rectify plans by
We use a constant velocity model to predict the position of repeating actions or compensating for unplanned occurrences,
the dynamic obstacle D in the coming horizon and try to such as unexpected obstacles obstructing the path. We bench-
maximize the distance between the latter and the robot: mark against the cube-stacking task outlined in [25].
C
dyn obs
(R,D)=ω
dyn obs
·e−||pR−pDpred || (24)
where p is the predicted position of dynamic obstacle.
Dpred
Cube to pick
Rollouts
Dyn. Obst.
Place on top of
Goal
Pick from table Pick from shelf
Object Robot Wall Fig. 5. Pick-place scenarios. The red cube has to be placed on top of the
green cube. The red cube can be either on the table or a constrained shelf,
Pulling example Pushing example requiringdifferentpickstrategiesfromthetoportheside,respectively.
Fig. 4. Illustrative example of pulling and pushing a block to a goal. The 1) Action templates for AIP: For this task, we define the
strategy differs accordingto the object, goal location,and dynamic obstacle following states s(reach), s(hold), s(preplace), s(placed), and
position.Whatactiontoperformisdecidedatruntimethroughmulti-modal
sampling. theircorrespondingsymbolicobservations.Therobothasfour
symbolic actions, summarized below:
3) Results: We test the performance of our approach in
Actions Preconditions Postconditions
two configurations: a) the object is in the middle of the arena,
reach(obj) - l(reach)=[10]⊤
and the goal is to one corner, and b) both the object and the pick(obj) reachable(obj) l(hold)=[10]⊤
goals are in different corners. For each arena configuration, prePlace(obj) holding(obj) l(preplace)=[10]⊤
we test three cases: the robot can either only push, only pull, place(obj) atPreplace(obj) l(placed)=[10]⊤
ZHANGetal.:MULTI-MODALMPPIANDACTIVEINFERENCEFORREACTIVETASKANDMOTIONPLANNING 7
The symbolic observers to estimate the states are defined agent shows a slightly lower position error in the vanilla case,
as follows. To estimate whether the gripper is close enough our method outperforms it in the reactive task. Planning and
to the cube, we define the relative observation o(reach). We execution time for smooth pick-and-place with our method is
set o(reach) =0 if δ ≤δ, where δ =||p −p || measures approximately 5 to 10s.
r r ee O
the distance between the end effector ee and the object O. TABLEII
o(reach) = 1 otherwise. To estimate whether the robot is
SIMULATIONRESULTSOFREACTIVEPICKANDPLACE
holding the cube of size 0.06m, we define:
Training Mean(std)
Task Method
(cid:40) epochs poserror
0,δ <0.06+δ and δ ≥0.06−δ
o(hold) = f f (25) Ours 0 0.0075(0.0036)
Vanilla
1,δ f ≥0.06+δ or δ f ≤0.06−δ RL 1500 0.0042(0.0019)
Ours 0 0.0117(0.0166)
whereδ =||p −p ||measuresthedistancebetweenthe Reactive
f ee l ee r RL 1500 0.0246(0.0960)
twogripper’sfingers.Toestimatewhetherthecubereachesthe
pre-place location, we define: 4) Results-multi-modalgrasping: Inthiscase,weconsider
(cid:40) grasping the object with different grasping poses by sampling
0,C (O,P)<δ and C (O,P)<δ
o(preplace) = dist ori two alternatives in parallel. That is, pick from the top or the
1,C (O,P)≥δ or C (O,P)≥δ
dist ori side to cover the cases when the object is on the table or
(26) the constrained shelf with an obstacle above. To do so, we
use the proposed M3P2I and incorporate the cost functions of
where C (O,P) and C (O,P) measure the distance and
dist ori
C (ee,O,ψ = 0) and C (ee,O,ψ = 1) as shown in
theorientationbetweentheobjectOandthepre-placelocation reach reach
(27).Thisallowsforasmoothtransitionbetweentopandside
P asin(16).Thepre-placelocationisafewcentimetershigher
grasp according to the geometry of the problem, see Fig. 6.
thanthetargetcubelocation,directlyontopofthegreencube.
We use the same logic as (26) for o(placed) where the place
location is directly on top of the cube location. The desired Dyn. Obstacle
state for this task is set to be l(placed) =[1 0]T, meaning the
cube is correctly placed on top ofthe other. Note thatin more
complex scenarios, such as rearranging many cubes, the BT
can guide the AIP as demonstrated in [7].
2) CostfunctionsforM3P2I: Atthemotionplanninglevel, Shelf
Top Pick from table Side Pick from shelf
the cost functions for the four actions are formulated as:
C (ee,O,ψ)=ω ·||p −p ||
reach reach ee O Fig.6. Exampleofdifferentpickingstrategiescomputedbyourmulti-modal
(cid:18) ||⃗z ·⃗z || (cid:19) (27) MPPI.Theobstacleontopoftheshelfcanbemoved,simulatingadynamic
+ω · ee O −ψ obstacle.
tilt ||⃗z ||·||⃗z ||
ee O
5) Results - real-world experiments: Our real-world vali-
dation of reactive pick-and-place, depicted in Fig. 7, involves
C (ee)=ω ·l (28)
pick gripper gripper
avoiding a moving stick and disturbances such as movement
C (O,P)=C (O,P)+C (O,P) (29)
preplace dist ori and theft of the cube. M3P2I enables smooth execution and
C place (O,P)=ω gripper ·(1−l gripper ) (30) recovery while using different grasp configurations.
C (ee,O,ψ) moves the end effector close to the object
reach
with a grasping tilt constraint ψ. As ψ approaches 1, the Top Pick from table
gripper becomes perpendicular to the object; as it nears 0,
Side Pick from shelf
the gripper aligns parallel to the object’s supporting plane.
3) Results - reactive pick and place: We first consider the
pick-and-place under disturbances. We model disturbances by
changingthepositionofthecubesatanytime.Wecomparethe
performance of our method with the off-the-shelf RL method
[25].ThisisareadilyavailableActor-CriticRLexamplefrom
IsaacGym, which considers the same tabletop configuration
and robot arm. We compare the methods in a vanilla task
Dynamic Obstacle
without disturbances and a reactive task with disturbances.
It should be noticed that the cube-stacking task in [25] only
considers moving the cube on top of the other cube while
neglecting the action of opening the gripper and releasing Displacing target Stealing the cube
the cube. In contrast, our method exhibits fluent transitions
between pick and place and shows robustness to interferences
Fig.7. Real-worldexperimentsofpickingacubefromthetableortheshelf
suchasrepickduringthelong-horizontaskexecution.Results whileavoidingdynamicobstaclesandrecoveringfromtaskdisturbances.
areavailableinTableII,with50trialspercase.WhiletheRL
8 IEEEROBOTICSANDAUTOMATIONLETTERS.PREPRINTVERSION.
V. DISCUSSION [4] M.Toussaint,J.Harris,J.-S.Ha,D.Driess,andW.Ho¨nig,“Sequence-
of-constraints mpc: Reactive timing-optimal control of sequential ma-
In this section, we discuss key aspects of our solution nipulation,” IEEE International Conference on Intelligent Robots and
and potential future work. The main strength of M3P2I is Systems,2022.
[5] T.MigimatsuandJ.Bohg,“Object-centrictaskandmotionplanningin
its ability to reason over discrete alternative actions at the
dynamicenvironments,”IEEERoboticsandAutomationLetters,vol.5,
motion planning level. This is enabled by sampling different no.2,2020.
controlsequencesforeachalternativesymbolicactionandthen [6] S. Li, D. Park, Y. Sung, J. A. Shah, and N. Roy, “Reactive task
and motion planning under temporal logic specifications,” in IEEE
blendingthemthroughimportancesampling.Wethusalleviate
InternationalConferenceonRoboticsandAutomation,2021.
the task planning burden by eliminating logic heuristics to [7] C. Pezzato, C. Hernandez, S. Bonhof, and M. Wisse, “Active infer-
switch between these actions. Sampling alternatives at the ence and behavior trees for reactive action planning and execution in
robotics,”IEEETransactionsonRobotics,2023.
motion planning level increases robustness during execution,
[8] N.Castaman,E.Pagello,E.Menegatti,andA.Pretto,“Recedinghorizon
at the price of slightly degrading the performance since the task and motion planning in changing environments,” Robotics and
controldistributionisalsoslightlybiasedtowardslesseffective AutonomousSystems,vol.145,2021.
[9] M. Colledanchise, D. Almeida, M, and P. O¨gren, “Towards blended
strategies, as shown in [24]. The performance of M3P2I
reactiveplanningandactingusingbehaviortree,”inIEEEInternational
also depends on the weight tuning of the cost functions. In ConferenceonRoboticsandAutomation,2019.
this case, implementing auto-tuning techniques can reduce [10] C. Pezzato, C. Salmi, M. Spahn, E. Trevisan, J. Alonso-Mora, and
C. Herna´ndez, “Sampling-based model predictive control leveraging
manual effort [26]. The cost functions also need to capture
parallelizablephysicssimulations,”arXivarXiv:2307.09105,2023.
the essence of the skills. The AIP requires manually defined [11] C.Paxton,N.Ratliff,C.Eppner,andD.Fox,“Representingrobottask
symbolic action templates and a set of discrete states. The plans as robust logical-dynamical systems,” in IEEE/RSJ International
ConferenceonIntelligentRobotsandSystems,2019.
discrete desired states need to be encoded in a sequence in
[12] J.Harris,D.Driess,andM.Toussaint,“FC3:Feasibility-basedcontrol
a BT or can be as simple as encoding the end state for a chaincoordination,”inIEEE/RSJInternationalConferenceonIntelligent
task, as in our examples. To transfer from simulation to the RobotsandSystems,2022.
[13] B.Huang,A.Boularias,andJ.Yu,“Parallelmontecarlotreesearchwith
real world, we considered randomization of object properties
batched rigid-body simulations for speeding up long-horizon episodic
in the rollouts [10]. Online system identification could be robot planning,” in IEEE/RSJ International Conference on Intelligent
added to achieve better performance with uncertain model RobotsandSystems,2022.
[14] Z.Zhou,D.J.Lee,Y.Yoshinaga,S.Balakirsky,D.Guo,andY.Zhao,
parameters [20].
“Reactive task allocation and planning for quadrupedal and wheeled
robot teaming,” in IEEE International Conference on Automation Sci-
enceandEngineering,2022.
VI. CONCLUSION [15] M. Bangura and R. Mahony, “Real-time model predictive control for
quadrotors,”IFACProceedingsVolumes,vol.47,no.3,2014.
Inthispaper,toaddresstheruntimegeometricuncertainties
[16] N. Scianca, D. De Simone, L. Lanari, and G. Oriolo, “Mpc for
and disturbances, we proposed a method to combine the humanoidgaitgeneration:Stabilityandfeasibility,”IEEETransactions
adaptabilityofanActiveInferenceplanner(AIP)forhigh-level onRobotics,2020.
[17] M.Spahn,B.Brito,andJ.Alonso-Mora,“Coupledmobilemanipulation
action selection with a novel Multi-Modal Model Predictive
via trajectory optimization with free space decomposition,” in IEEE
Path Integral Controller (M3P2I) for low-level control. We InternationalConferenceonRoboticsandAutomation,2021.
modified the AIP to generate plan alternatives that are linked [18] G.Williams,N.Wagener,B.Goldfain,P.Drews,J.M.Rehg,B.Boots,
and E. A. Theodorou, “Information theoretic mpc for model-based
to costs for M3P2I. The motion planner can sample the plan
reinforcementlearning,”inIEEEInternationalConferenceonRobotics
alternatives in parallel, and it computes the control input andAutomation,2017.
for the robot by smoothly blending different strategies. In a [19] M.Bhardwaj,B.Sundaralingam,A.Mousavian,N.D.Ratliff,D.Fox,
F.Ramos,andB.Boots,“Storm:Anintegratedframeworkforfastjoint-
push-pulltask,wedemonstratedhowourproposedframework
spacemodel-predictivecontrolforreactivemanipulation,”inConference
can blend both push and pull actions, allowing it to deal onRobotLearning. PMLR,2022.
with corner cases where approaches only using a single plan [20] I. Abraham, A. Handa, N. Ratliff, K. Lowrey, T. D. Murphey, and
D.Fox,“Model-basedgeneralizationunderparameteruncertaintyusing
fail. With a simulated manipulator, we showed our method
path integral control,” IEEE Robotics and Automation Letters, vol. 5,
outperforming a reinforcement learning baseline when the en- no.2,2020.
vironment is disturbed while requiring no training. Simulated [21] T. Howell, N. Gileadi, S. Tunyasuvunakool, K. Zakka, T. Erez, and
Y. Tassa, “Predictive sampling: Real-time behaviour synthesis with
and real-world experiments demonstrated how our approach
mujoco,”arXivarXiv:2212.00541,2022.
solvesreactiveobjectstackingtaskswithamanipulatorsubject [22] G. Williams, A. Aldrich, and E. A. Theodorou, “Model predictive
to severe disturbances and various scene configurations that pathintegralcontrol:Fromtheorytoparallelcomputation,”Journalof
Guidance,Control,andDynamics,vol.40,no.2,Feb.2017.
require different grasp strategies.
[23] G.Williams,P.Drews,B.Goldfain,J.M.Rehg,andE.A.Theodorou,
“Information-theoreticmodelpredictivecontrol:Theoryandapplications
toautonomousdriving,”IEEETransactionsonRobotics,2018.
REFERENCES [24] E. Trevisan and J. Alonso-Mora, “Biased-MPPI: Informing sampling-
based model predictive control by fusing ancillary controllers,” IEEE
[1] C.R.Garrett,R.Chitnis,R.Holladay,B.Kim,T.Silver,L.P.Kaelbling, RoboticsandAutomationLetters,vol.9,no.6,2024.
and T. Lozano-Pe´rez, “Integrated task and motion planning,” Annual [25] V.Makoviychuk,L.Wawrzyniak,Y.Guo,M.Lu,K.Storey,M.Macklin,
reviewofcontrol,robotics,andautonomoussystems,vol.4,2021. D. Hoeller, N. Rudin, A. Allshire, A. Handa et al., “Isaac gym: High
[2] M. Toussaint, “Logic-geometric programming: An optimization-based performance gpu-based physics simulation for robot learning,” arXiv
approach to combined task and motion planning,” in Twenty-Fourth arXiv:2108.10470,2021.
InternationalJointConferenceonArtificialIntelligence,2015. [26] M. Spahn and J. Alonso-Mora, “Autotuning symbolic optimization
[3] C. R. Garrett, T. Lozano-Pe´rez, and L. P. Kaelbling, “Pddlstream: fabricsfortrajectorygeneration,”inIEEEInternationalConferenceon
Integrating symbolic planners and blackbox samplers via optimistic RoboticsandAutomation,2023,pp.11287–11293.
adaptive planning,” in Proceedings of the International Conference on
AutomatedPlanningandScheduling,vol.30,2020.

=== REVISE TO ===
PROFESSIONAL TONE: Begin directly with content - NO conversational openings like 'Okay, here's...'

1. Fix all issues above
2. Title: "Multi-Modal MPPI and Active Inference for Reactive Task and Motion Planning"
3. Include 10-15 quotes from paper text
   - Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
   - Use consistent quote formatting: 'The authors state: "quote"' or vary attribution phrases
   - Vary attribution phrases to avoid repetition
   - CRITICAL: Only extract quotes that actually appear in the paper text
4. ELIMINATE ALL REPETITION - each sentence must be unique
   - Check before each sentence: 'Have I already said this?' If yes, write something new
   - Vary attribution phrases - do NOT repeat 'The authors state' multiple times
5. Extract methodology, results with numbers, key quotes
6. 1000-1500 words, structured with ### headers

Generate COMPLETE revised summary.