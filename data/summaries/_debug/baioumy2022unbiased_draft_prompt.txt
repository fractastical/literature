=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Unbiased Active Inference for Classical Control
Citation Key: baioumy2022unbiased
Authors: Mohamed Baioumy, Corrado Pezzato, Riccardo Ferrari

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2022

Key Terms: computational, state, active, neuroscience, inference, framework, control, classical, mathematical, goal

=== FULL PAPER TEXT ===

Unbiased Active Inference for Classical Control
Mohamed Baioumy∗1, Corrado Pezzato∗2, Riccardo Ferrari3, Nick Hawes1
Abstract—Active inference is a mathematical framework
that originated in computational neuroscience. Recently, it has
been demonstrated as a promising approach for constructing
goal-driven behavior in robotics. Specifically, the active
inference controller (AIC) has been successful on several
continuous control and state-estimation tasks. Despite its
relative success, some established design choices lead to a
numberofpracticallimitationsforrobotcontrol.Theseinclude
having a biased estimate of the state, and only an implicit
model of control actions. In this paper, we highlight these
limitations and propose an extended version of the unbiased
active inference controller (u-AIC). The u-AIC maintains all
the compelling benefits of the AIC and removes its limitations.
Simulation results on a 2-DOF arm and experiments on a real
7-DOF manipulator show the improved performance of the Fig. 1: Manipulation of delicate items in a human-shared
u-AICwithrespecttothestandardAIC.Thecodecanbefound store environment.
at https://github.com/cpezzato/unbiased_aic.
I. INTRODUCTION real 7-DOF robot manipulator, which was previously only
performed in simulation.
Active inference is a mathematical framework promi-
In this paper, we identify the drawbacks of the AIC
nent in computational neuroscience [1]. It aims to explain
and connect them to two root causes. First, in the AIC
decision-making in biological agents using free-energy min-
the estimated state (or belief) is biased toward the current
imization. Recent work has led to many schemes based
goal/target state through a goal prior. This leads to reduced
on this framework, for an overview see [2]. Nevertheless,
quality in state-estimation [4] but also influences precision
active inference for robot control is still a relatively new
learning (learning the precision/covariance of the sensory
field. It is thus of particular importance to understand the
model) making the model parameters converge to a biased
limits of such methods. In this paper, we present an analysis
value[4],[5].Arangeofissuesalsoarisesinfaultdiagnosis
of the drawbacks associated with specific design choices
and fault-tolerant control as a result of the goal prior, such
withintheactiveinferencecontroller(AIC).Additionally,we
as false-positive fault detection [6], [3]. Second, the control
propose an extended and improved unbiased AIC (u-AIC),
action is not explicitly modeled as a random variable in the
which we initially presented in [3] solely for fault-tolerant
generative model of the AIC. This causes a range of issues
control. The effort in understanding the limits of the AIC
on the control side. In real-world control applications, the
for robot control started with our previous work [3], where
integral control law typical of the AIC can cause saturation
we specifically analyzed the effect of joint state-estimation
problems for the actuators when the agent fails to reach a
andcontrolforfaulttolerance.Thecurrentpaperprovidesan
target. This can happen for instance because of a collision.
in-depth analysis of the performance and limits of the AIC
Other limitations from a control perspective are that the
more generally (not just fault-tolerant control). Additionally,
AIC does not naturally allow for the incorporation of feed-
we formalize the u-AIC. This includes 1) a derivation of
forward control signals [3] which could improve the overall
the new control architecture and possible extensions, 2) a
performance of the system. Finally, the motion can be jerky
proof of convergence for both the state estimation and the
in practice.
controlled system, and 3) the application of the u-AIC to a
Interestingly, all these limitations are present in the AIC
but are not intrinsically part of active inference as a general
∗ Authorswithequalcontribution
1Mohamed Baioumy and Nick Hawes are with the framework. One can design different controllers that stay
Oxford Robotics Institute, Oxford University, [mohamed, true to the principles of active inference while mitigating
nickh]@robots.ox.ax.uk
thelimitations.Tothisend,wepresenttheu-AIC,originally
2CorradoPezzatoiswiththeCognitiveRoboticsDepartment,TUDelft,
introduced in [3]. We demonstrate the properties of the u-
c.pezzato@tudelft.nl
3Riccardo Ferrari is with the Department of Systems and Control, TU AIC in Section IV, as well as the convergence of both the
Delft,r.ferrari@tudelft.nl state estimation and control which is still missing for the
This research was partially supported by Ahold Delhaize. All content
AIC [7].
represents the opinion of the author(s), which is not necessarily shared or
endorsedbytheirrespectiveemployersand/orsponsors. The contributions of this paper are twofold: 1) we con-
2202
luJ
72
]OR.sc[
1v90431.7022:viXra
cretely demonstrate the limitations of the AIC using both in [9]. In this section, we only report the crucial equations
theoretical results and empirical demonstrations. 2) We for- to understand the limitations of the AIC in Section III, an
malize the u-AIC and show how it overcomes such limita- interested reader is referred to [7] for all the details.
tions, providing proof of convergence and extensions to the
A. The generative model
controller. We believe that understanding the properties and
Active Inference considers an agent in a dynamic envi-
the boundaries of the AIC is important for researchers that
ronment that receives observations y about states x at every
want to apply it for robot control, such that better and safer
time-step t. The generative model of the agent can then be
systems can be built using this framework.
expressed as:
A. Related work
p(x,y)= p(y|x) p(x). (1)
Active inference has been proposed in neuroscience as a
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124)(cid:123)(cid:122)(cid:125)
generaltheoryofthebrain[8]andwasconciselyreportedin observationmodel prior
[9] using a notation and language closer to engineering and Avisualrepresentationofthismodelispresentedinfig.2
control. We refer to the active inference controller in [9] as (right).Theprobabilitydistributionp(y|x)hasameang(x),
AIC, from which the first real-world applications for robot whichisamappingfromstatetoobservation.Thepriorp(x)
control[7],[10]werederived.Recently,activeinferencehas has a mean f(x), which is a function that encodes the goal
beenappliedforrobotcontrolonabroadervarietyofsettings state state µ . The agent aims to infer the posterior p(x|y)
g
andtasks.Thisincludesworkonrobuststate-estimation[11], given a model of the agent’s world. This means finding the
[12],adaptivecontrol[7],[4],fault-tolerantcontrol[3],[13], belief µ over the state x given the observations. This can be
reinforcementlearning[14],planningunderuncertainty[15], achievedbyminimizingtheso-calledvariationalfreeenergy.
[16],human-robotinteraction[17],[18]andmore.Therecent If all distributions in eq. (1) are Gaussian, the free energy
surveyin[2]providesadetailedoverviewofactiveinference becomes a sum of least square terms [7].
for robot control to date.
B. Free-energy
Particularly interesting in the context of this paper, is that
several recent approaches in robotics have taken inspiration The AIC performs joint state estimation and control by
from the free-energy principle and applied it to continu- minimizing the free-energy F through a gradient descent
ous control and state estimation. Recent works focused on scheme. F is defined as [7]:
chance-constrained active inference [19], LQG control [20]
1
n (cid:88)d−1
(cid:104) (cid:105)
and model-predictive control [21]. F(y,µ)= ε(i)(cid:62)Σ−1 ε(i)+ε(i)(cid:62)Σ−1 ε(i) +K.
2 y y(i) y µ µ(i) µ
However, while the area of application of active inference i=0
(2)
inengineeringsettingsiscontinuouslygrowing,worksonin-
The free energy is a weighted sum of prediction errors up
depthanalysisoftheAICfromacontroltheoreticperspective
to a constant K resulting from the derivations [9]. The
are limited. Work in [22], [4] has shown the relationship
terms Σ−1 and Σ−1 are precision matrices representing
between the AIC and PID controllers while the relationship y(i) µ(i)
theconfidenceaboutsensoryinputandinternalbeliefs.These
between the AIC, LQR control, and Kalman filters is dis-
canbeseenastuningparameters.Thetermn representsthe
cussed in [23], [20]. Finally, in [6], [3], some limitations d
number of derivatives considered in the control problem. . If
of the AIC were discussed limited to fault-tolerant control,
we assume as in most cases [7], [10], [3] that n =2, then
whichmotivatedtheintroductionoftheu-AIC[3].Thiswork d
state estimation and control are performed on position and
focusesonathoroughanalysisofthelimitsoftheAICfrom
velocity. These quantities are internally represented as the
a control point of view and proposes an extended version of
beliefs µ(0) =µ for positions, and µ(1) =µ(cid:48) for velocities.
the u-AIC to address them.
The terms ε(i) = (µ(i+1) − f(i)(µ)) and ε(i) = (y(i) −
µ y
B. Structure of the paper g(i)(µ)) are respectively the state and sensory prediction
errors.
The remainder of this paper is organized as follows.
The function g(µ) represents the mapping between states
After providing the necessary mathematical background in
and sensory observations. In case of robot control with
Section II, Section III details the limitations of the AIC
position and velocity sensors, this is the identity mapping,
when applied to robot control. Section IV presents the u-
so g(µ) = µ. The function f(µ) = µ −µ specifies the
AIC scheme to overcome these limitations. In Section V the g
desiredevolutionofthedynamicsofthesystem.Inthiscase,
properties of the u-AIC and its relation with the AIC are
[7], the AIC will make the system behave like a first-order
highlighted.InSectionVIthetheoreticalclaimsarevalidated
linear system with desired goal position µ . By definition
in simulation and on a real 7-DOF Franka Emika Panda g
[9], [7], it holds:
manipulator. Finally, we draw conclusions in Section VII.
∂g ∂f
g(i) = µ(i), f(i) = µ(i), g(0) =g, f(0) =f. (3)
II. PRELIMINARIES ∂µ ∂µ
In this section, we concisely report the AIC formulation Finally, the terms Σ and Σ are diagonal covariance
µ(i) y(i)
as used in previous work (e.g. [7], [4], [24], [10]), which matrices. In the scalar case, these are simply represented as
specified the AIC for robot control according to the theory the variances σ and σ .
µ y
C. State estimation A. Limitation #1: biased state estimation
Before formally analyzing the controller, we provide a vi-
StateestimationintheAICisachievedbygradientdescent
sualexplanationusingfactorgraphs.Infig.2,thegenerative
on the free-energy [9], [25], [7] with the update rule:
modeloftheAICineq.(1)isdepicted.Eachrandomvariable
d ∂F is represented by a circle, and each probability distribution
µ˜˙ = µ˜−κ , (4)
dt µ∂µ˜ by a black square. By inspecting this graph, we see that the
state x is connected to two distributions. The distribution
where µ˜ = [µ, µ(cid:48)], and t refers to time. The term κ µ is a p(y|x) moves the belief closer to the observed value y.
tunable learning rate. This distribution is represented in the free-energy F by the
term ε(cid:62)Σ−1ε where ε =(y−µ). This is considering, as
y y y y
D. Control commonly done, g(·) as the identity mapping,
Theseconddistributionisthepriorp(x).Thisdistribution
In the AIC, the control input u is also computed through
is Gaussian with the function f(µ) = µ −µ as its mean.
g
gradient descent on F. As can be seen by eq. (2), however,
The function encodes the goal state µ . This term moves
g
F does not depend on u explicitly. On the other hand, the
the belief µ towards the goal state. The belief of the AIC
actions indirectly influence F by making the state evolve
is thus always biased towards the goal state. This is by
over time and thus changing the sensory output. We can
design.Thebenefitofthisisthatnowwecanalsocomputea
compute the actions using the chain rule as [9], [7]:
controlactionthatmovestheagenttothegoal(usingeq.(6)).
However, the drawback is that state estimation is inaccurate.
∂y˜∂F
u˙ =−κ (5)
a∂u ∂y˜
wherey˜=[y, y(cid:48)],andκ isthetuningparameter.Theterm
a
∂y˜,requiresaforwarddynamicmodelandisgenerallyhard
∂u
to compute in closed-form for non-linear systems. In most
previouswork[7],[10],[3],[4]suchneedhasbeenobviated
by introducing a linear approximation. The expression can
alternatively be written as a sum for every sensor y in y˜.
This relationship shows how the control action is directly
related to the sensory inputs and the number of sensors as
seen in the following expression:
∂F (cid:88)∂F Fig. 2: Illustration of the u-AIC (left) and the AIC (right)
u˙ ≈−κ =−κ . (6)
a∂y˜ a ∂y for a one-dimensional problem. Circles indicate random
y
variables. Black boxes indicate probability distributions.
As an example, for a system with a position sensor y ,
q
and velocity sensor y this control law would be: 1) Convergence to a biased belief: We will now analyze
q˙
the convergence of the beliefs in the AIC. Let us consider
u˙ =−κ (cid:2) Σ−1(y −µ)+Σ−1(y −µ(cid:48)) (cid:3) . (7) a generic form of the free-energy, as in eq. (2). For the
a yq q yq˙ q˙ simplest linear and scalar case with n
d
=1 (i.e. one sensor
and one actuator, with observable state), this expression can
III. LIMITATIONSOFTHEAIC
be reduced to:
In this section, we discuss the limitations of the AIC for 1 (cid:20) (y−g(µ))2 (µ(cid:48)−f(µ))2(cid:21)
F = + +K (8)
robot control. We note that these limitations are not inherent 2 σ σ
y µ
in the active inference framework but rather in how the AIC
Let us consider the generative model of the state dynamics
is constructed. In Section III-A we address Limitation #1:
as a first order linear system with unitary time constant, so
in AIC the belief over the current state is biased toward the
f(µ)=µ −µ,andsincethestateisobservable,sog(µ)=µ
g
target the agent aims to reach by means of goal prior. This
[7]. At steady state, it holds that
means that the agent’s belief is never accurate, except when
∂F (y−µ) (µ(cid:48)−µ +µ)
the target is reached. In Section III-B we address Limitation =− + g =0. (9)
#2:thecontrolactionisnotexplicitinthegenerativemodel. ∂µ σ y σ µ
This means, that one cannot minimize the free energy with Additionally, when solving for µ we can show that
respect to the actions directly and instead use the chain rule
σ y+σ µ −σ µ(cid:48)
as in eq. (5), making assumptions about the linearity of the µ= µ y g y . (10)
σ +σ
system. This can cause a saturation problem and does not µ y
allow for the incorporation of feed-forward control signals From eq. (10), one can notice that the belief µ is a
to improve performance. weighted sum of the sensory measurement and the goal
state. The belief is thus always biased towards the goal. 3) Limitation in fault-tolerant control: The AIC was
We demonstrate incorrect state estimation in simulation usedin[6]forfaulttolerantcontrol,wherethemainideawas
(sectionVI-A),whichisparticularlyevidentwhencollisions thatthesensorypredictionerrorsinthefree-energycouldbe
happenwiththeenvironment.Thebiasinthestateestimation used as residuals for fault detection, removing the need for
leads to two other issues. These are apparent when one tries moreadvancedresidualgenerators.Despitethesimplicityof
tooptimizemodelparametersthroughprecisionlearning(or themethodfordetectingandrecoveringfrombrokensensors,
inversecovariancelearning),andwhenapplyingtheAICfor the use of prediction errors with biased state estimation can
fault tolerant control. We expand upon these claims in the cause several false positives. This is explained in detail in
following sections. [3], where a first version of the u-AIC has been proposed.
2) Biasedprecisionlearning: Pastworkonrobotcontrol We refer the reader to [3] for additional details.
such as [7], [10] assumed that the precision σ−1 of the
y
observation model is known. Intuitively, this quantifies the B. Limitation #2: implicit modelling of actions
confidence about how noisy the sensor is, and can be
determinedbeforethedeploymentofthecontroller.However, The control law for the AIC is computed using gradient
forcertainapplicationswheresensorynoiseisuncertain,one descent on F, as in eq. (5). Since the action is not explicitly
might need to estimate the precision of the sensor online. modeled, one resorts to the chain rule. This introduces
This is the case in many robotics applications [26], [27]. additional complexity. The term dy/dµ is generally hard
There exists a body of work related to active inference to compute, and researchers approximated it by the identity
which explores precision learning., e.g. methods such as matrix [5], [7]. Despite the promising results, linearizing the
DynamicExpectationMaximizationandgeneralizedfiltering forward dynamics necessarily limits the performance in the
[28], [29]. However, these methods do not perform control. presence of non-linear dynamics. Additionally, the control
When precision learning is done in conjunction with the law is driven by sensory prediction errors weighted by the
active inference controller, we cannot estimate the true precision, see eq. (7), and it is de-facto an integrator. This
precisionofthesensor.Workin[5],[4]showshowprecision causes three issues in practice.
learning in the AIC can be used to find the optimal gains of First, the control law in the AIC is directly dependent on
thecontroller.Inthiscontext,however,theobtainedprecision the observation. Eq. (6) shows that the control law is a sum
of the sensor loses its physical meaning. of sensory prediction errors. This means that, if a system
As explained earlier, the belief over the current state is is not observable, it is not controllable. More specifically,
biased, and this propagates to the precision of the system. under the assumptions explained in [4] and [5], the active
Consider again the simplest linear and scalar case of the inference controller is equivalent to a PI Controller i.e. PID
free-energy in eq. (8). The constant K can be expanded to with D = 0, a P gain of κ u Σ− y(cid:48) 1 and an I gain of κ u Σ− y 1.
include the variances as (see [9]): This assume that the function f(µ) = (µ −µ)τ−1 has a
g
1 τ → 0. In that case, the belief will approach the goal state
K = 2 lnσ y σ µ . µ→µ g . In eq. (6), we see that the control law of the AIC
is the sum of sensory prediction errors. Every term, in this
Considering these terms time-varying, we can take the gra-
case, is responsible for an error term in the PID control law.
dient with respect to the variances as:
A position sensor is responsible for the I gain, a velocity
∂F (y−µ)2 1
sensor will result in a P gain, and an acceleration sensor
=− + . (11)
∂σ y 2σ y 2 2σ y results in a D gain.
Setting this expression to zero, we obtain: Second, integral control has a few general drawbacks. For
instance, if the goal cannot be reached due to a collision,
σ =(y−µ)2. (12)
y the magnitude of the control action u will monotonically
The updated variance depends on µ which we showed increaseuntilsaturation.ThisisshowninsectionVIforboth
is biased. Thus the precision will also be biased. This is simulated and real robots, where we point out that a vanilla
because the agent is estimating the precision as the average implementation of the AIC leads to integrator windup.
squaredistancebetweenthemeanµandeverymeasurement Third,thecontrolactioncannotbenaturallysupplemented
y. If µ is biased, then the agent is estimating the variance withafeed-forwardsignal.Incaseonehasinformationabout
around a value that is not the true mean. A special case is the system, designing a feed-forward signal to supplement
when the agent already starts at the goal. In that setting, the feedback controller consistently improves performance.
µ would represent the true mean and the precision would
be estimated around the true mean resulting in an accurate IV. UNBIASEDACTIVEINFERENCECONTROLLER
precision estimate. Finally, learning the precision of the
observation model can be used in the context of control, but A first version of the u-AIC has been introduced in
the obtained precision no longer represents the noise level [3] in the context of fault-tolerant control. In this section,
of the sensor. Instead, it is a combination of the noise level, we generalize, extend, and formally analyze the previously
and how aggressive the controller will act, as shown in [21]. proposed u-AIC for generic control settings, and show how
it overcomes the limitations of the AIC.
A. Derivation of the u-AIC where I represents a unitary matrix of suitable size. This
formassumesthatthepositionofeachjointisthuscomputed
Inthissection,wedescribetheu-AICasintroducedin[3],
as the discrete-time integral of the velocity, using a first-
to which an interested reader is referred for more details on
order Euler scheme. This approximation can be avoided if
the derivations of the following equations. Let us consider
a better dynamic model of the system is available, and in
x = [q,q˙](cid:62) and let us define a probabilistic model where
that case, predictions can be made using the model itself.
actions are modelled explicitly:
Finally, by choosing the distribution p(u|x) to be Gaussian
p(x,u,y)=p(u|x) p(y|x) p(x) (13) with mean f∗(µ x ,µ g ), we can steer the system towards the
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124)(cid:123)(cid:122)(cid:125) target µ without biasing the state estimation. This results
g
control observationmodel prior
in ε =(µ −f∗(µ ,µ )).
u u x g
A visual representation of the u-AIC can be seen in fig. 2 In the u-AIC state estimation and control are achieved
(left). Note that with the u-AIC the information about the using gradient descent along the free energy. This leads to:
desired goal to be reached is encoded in the distribution
∂F ∂F
p(u|x). This fundamentally removed the bias of the current µ˙ =−κ , µ˙ =−κ , (17)
u u∂µ x µ∂µ
state, as will be shown. In this paper, as in [4], we assume u x
thatanaccuratedynamicmodelofthesystemisnotavailable where κ u and κ µ are the gradient descent step sizes. The
to keep the solution system agnostic and to highlight once gradient on control can be computed as:
again the adaptability of the controller. ∂F
=Σ−1(µ −f∗(µ ,µ )) (18)
Theu-AICaimsatfindingtheposterioroverstatesaswell ∂µ u u x g
u
as the posterior over actions p(x,u|y). Since the posteriors
B. Proof of convergence
can be difficult to compute exactly, they are approximated
using a variational distribution Q(x,u). We can make use For the simple 1-dimensional case, the expression of the
of the mean-field assumption (Q(x,u) = Q(x)Q(u)) and free energy for the u-AIC can be reduced to:
the Laplace approximation, and assume the posterior over 1 (cid:20) (y−µ )2 (µ −f∗(·))2 (µ −xˆ)2(cid:21)
the state x is Gaussian with mean µ [30]. Similarly for F = x + u + x +K
x 2 σ y σ u σ x
the actions, the posterior u is assumed Gaussian with mean (19)
µ u .BydefiningtheKullback-Leiblerdivergencebetweenthe The belief over the state µ x and over the control action µ u
variational distribution and the true posterior, one can derive will converge when:
an expression for the free-energy F as [3]:
∂F ∂F
=0, =0, (20)
F =−lnp(µ u ,µ x ,y)+C (14) ∂µ u ∂µ x
At steady state, it holds that µ = f∗(µ ,µ ). Considering
Consideringeq.(13)andassumingGaussiandistributions,F u x g
this result, one can show that µ converges to
becomes: x
σ y+σ xˆ
1 µ = x y . (21)
F = 2 (ε(cid:62) y Σ− y 1ε y +ε(cid:62) x Σ− x 1ε x (15) x σ x +σ y
+ε(cid:62)Σ−1ε +ln|Σ Σ Σ |)+C, We can thus see that the control action converges to the
u u u u y x
function f∗, which can be chosen for instance as a PID
The terms ε = y −µ, ε = y −µ(cid:48) are the sen-
yq q yq˙ q˙ controller. We can also show that belief over the state is
sory prediction errors respectively for position and velocity a weighted average of the sensory measurement y and the
sensoryinputs.Thecontrollerrepresentsthestatesinternally predictionxˆ.UnliketheAIC,thispredictiondoesnotdepend
as µ = [µ,µ(cid:48)](cid:62). The relation between internal state and
x on the goal, see eq. (10).
observationisexpressedthroughthegenerativemodelofthe
sensory input g = [g , g ]. Position and velocity encoders C. Extensions of the u-AIC for control
q q˙
directlymeasurethestate,thusg q andg q˙ arelinear(identity) The u-AIC can be extended for richer control by modify-
mappings. ing the probabilistic model in eq. (13). This is because the
Additionally, ε is the prediction error on the control control action u is explicitly modeled as a random variable,
u
actionwhileε isthepredictionerroronthestate.Thelatter andthuswecanaddpriorprobabilitydistributionstoit.This
x
is computed considering a prediction of the state xˆ at the canbedoneinmultiplewaystoachievedifferentobjectives.
currenttime-stepsuchthatε =(µ −xˆ).Thepredictionis 1) Adding a feed-forward controller (open-loop): So far
x x
a deterministic value xˆ = [qˆ,qˆ˙](cid:62) which can be computed theonlytermdependingonthecontrolactionnowisp(u|x);
in the same fashion as the prediction step of, for instance, however, a prior p(u) can be also added. In that case, the
a Kalman filter. The prediction is approximated propagating generative model would be:
forward in time the current state belief using the following
1
simplified discrete time model: p(u)p(u|x)p(y|x)p(x) (22)
α
(cid:20) (cid:21)
I I∆t where α is a normalization constant. Note that, this
xˆ = µ (16)
k+1 0 I x,k denominator does not need to be computed explicitly. To
achieve state estimation and control, we simply need to B. Control law
minimize the free energy (which maximizes the likelihood
Intheu-AIC,actionsareexplicitlymodelledandthetarget
of the model). We do not need to exactly compute the free state is encoded in the term p(u|x). Explicit actions allow
energy or the likelihood. us to directly perform gradient descent on F with respect to
This prior can encode a feed-forward signal (open-loop u.ThisisnotpossibleinthegeneralAICcaseandthechain
control law). Assuming the prior to be Gaussian with mean
rulehadtobeutilized(seeeq.(5)).Theeventualcontrollaw
f (x) and variance σ (‘ol’ stands for open-loop), we can
ol ol of the AIC is also directly dependent on the measurement
show that the control law will converge to:
and the number of measurements eq. (6). Thus if part of
f (µ )σ +f∗(µ ,µ )σ the system is unobserved, it can not be controlled. The u-
µ u = ol x u σ +σ g x ol. (23) AIC on the contrary has a control law irrespective of the
ol u number of sensors eq. (21) and allows us to encode control
This is the weighted sum of the PID control law (after
costs, smoothing effects and feed-forward control signals
applying a filter) and the open-loop control law. Note previ-
(see eq. (23)).
ously, the value of σ did not matter, now the ratio between
u
σ and σ does contribute. Additionally, the expression for C. General architecture
u ol
F, would contain a quadratic term for the open-loop control The general architecture (for state estimation and control)
law. for the AIC and u-AIC is also different. In the case of the
2) Adding control costs: Alternative to adding feed- AIC,stateestimationisdependentonthegoalstate.Thegoal
forward control law, we might add a control cost. This can isencodedintheprior,whichbiasesthestateestimation.The
be achieved by adding the control prior p cc (u) with a mean controller, on the other hand, is dependent on the (biased)
of 0 and variance of σ cc . This creates a quadratic term in F beliefandmeasurements.Thisisunusualinclassicalcontrol,
of (µ u −0)2/σ cc . This is equivalent to a quadratic control see fig. 3. A discussion on the role of modularity in AIC
cost as seen in classical LQR controllers. and the general control architecture can be found in [32].
3) Smoothingthecontrolaction: TheAICoftenexhibits The u-AIC on the other hand has a more traditional flow
jerky motion. This can be mitigated in the u-AIC by adding
a smoothing prior. Let u be a random variable referring to
the current control action being executed. We then define
u as the control action executed at the time previous time-
p
step. A distribution p(u|u ) can be added to the generative
p
model. This will add a control cost of (µ −u )2/σ to F.
u p p
Minimizing this quantity nudges the control action u toward
the value of the last control action u creating a smoothing Fig. 3: Control diagram of the AIC
p
effect.ThisquadraticlossissimilartoapproachesinModel-
predictive control (MPC) [31].
of information. State estimation requires the measurements
AcomparisonofthestandardAICandu-AICagainstMPC
y. Then the (unbiased) belief µ and the goal µ are fed
x g
and impedance control can be found in [24]. Future work
into the controller which produces the control action. This
could address the comparison of the proposed extensions of
is illustrated in fig. 4.
the u-AIC against other classical controllers.
V. RELATIONSHIPBETWEENTHEAICANDU-AIC
A. Convergence of beliefs
The AIC considers the relationship between states and
observationswithoutexplicitlymodeling thecontrolactions.
Inclassicalfilters,suchastheKalmanfilter,thepriorcomes
from a prediction step that relies on the previous state and Fig. 4: Control diagram of the u-AIC.
action. In the case of the AIC, the prior essentially predicts
theagenttomovetowardsthetarget.Thisresultsinabiased
state estimate as seen in eq. (10). In contrast, the u-AIC has VI. EXPERIMENTALEVALUATION
anunbiasedbeliefoverthestateasseenineq.(21).Notethat We now showcase the limitations of the AIC explained in
both expressions are almost identical. The AIC converges Section III and compare the performance with the u-AIC,
to the weighted average between the sensory measurement both in simulation and in the real world.
and the goal. The u-AIC on the other hand converges to a
A. Simulation
weighted average between the sensory measurement and the
predicted state (using a model or Euler integration). Note The simulation scenario is depicted in fig. 5. The robot
that, in the AIC, the goal state is encoded in the prior over has to reach a target in configuration space. During motion,
thestatep(x),whileintheu-AICitisencodedinaseparate we suppose that at a certain time a collision occurs which
distribution p(u|x) (see fig. 2). prevents the robot from moving further. We assume the
collision persists for t = 3s, then the robot is free to
Scenarios ess [rad] ts [s] os[%] RMSE [rad]
c
proceed.Thisallowsustoshowtheincorrectstateestimation AIC 2.82e-05 3.70 0.12 0.042
u-AIC 0.0058 5.15 8.44 4.43e-4
andtheovershootduetotheintegralcontrollawoftheAIC.
AIC+coll. 6.07e-05 5.04 43.90 0.1695
In the u-AIC, we observe none of these while maintaining u-AIC+coll 0.0053 5.86 11.65 4.92e-04
similar performance.
TABLE I: Simulation results of AIC and u-AIC without and
with a random collision of random duration, averaged over
100 randomized reaching tasks.
while the AIC presents the lowest e due to the prominent
ss
integration scheme, the RMSE for state estimation is two
ordersofmagnitudehigherthanfortheu-AIC.Incorrectstate
Fig. 5: Scenarios considered to illustrate the incorrect state
estimationcanalsocauseseveralfalsepositivesasdescribed
estimationduetothebiastowardsthetarget(reddot).In(b)
indetailin[3].ThesensorypredictionerrorsfortheAICcan
an obstacle occludes the way and blocks the arm.
in fact increase also due to collisions with the environment
(see the last row of plots in fig. 6). The u-AIC is instead
The 2-DOF robot arm is equipped with position and
velocity sensors y , y ∈ R2 for the two joints affected insensitive.
q q˙
2) Monotonicincreaseofcontrolinput: Thesecondrow
by zero mean Gaussian noise, as in fig. 5. We define y =
of the plots in fig. 6 displays the control action of one joint.
[y , y ](cid:62). The states x to be controlled are set as the joint
q q˙
During a collision, the control input computed by the AIC
positions q = [q ,q ](cid:62) of the robot arm. The simulation
1 2
keeps increasing due to the integral nature of the control
results for AIC and u-AIC are reported in fig. 6.
law employed (see eq. (7)). After the collision is removed,
the controller necessarily overshoots. In the u-AIC, one can
saturate only the integral term, and not the entire control
law. On average, the AIC has four times the overshoot after
a collision with a comparable settling time to the u-AIC.
B. 7-DOF Panda arm
On the real Panda arm, we compared 1) the reference
trackinginconfigurationspace,and2)thecollisionbehavior
intermsofovershootandcommandedcontrolactionsresult-
ing from holding the robot arm away from its desired set
point. The behaviors of the controllers are best appreciated
in the accompanying video1.
1) Reference tracking: The performance in terms of
Fig. 6: Simulation results of a 2-DOF robot arm collision
referencetrackingofasinusoidalwavearereportedinfig.7.
(orangearea)fort =3s.Forreadability,weonlyreportthe
c TheAICneverconvergestothereferencetrajectory,andthis
first joint since the behavior is similar for the second one.
is independent of the initial conditions. We highlight this by
plotting the response of the two controllers after running
Thecollisionscenarioinfig.5highlightsafewlimitations: them for 4 seconds.
1) Incorrect state estimation: Let us consider the first
row of the plots in fig. 6. When the AIC is not able to
reach the desired target due to a collision blocking the path,
the belief µ does not follow the trend of the real state x.
The belief converges to a value between the sensory reading
and the desired goal. For instance, at time t = 4s the
sensory reading is −0.355 [rad]. Given the current µ =
g
−0.2 [rad] as goal for the first joint, σ = σ = 1, and
y µ
µ(cid:48) = 0.053 [rad/s] the belief converges to −0.304 [rad],
which is in accordance with eq. (10). On the other hand, the Fig. 7: Experiments with real Panda arm on reference track-
u-AICconvergestothetruestate.Forstatisticalsignificance, ing. For readability, we only report the first joint.
we ran 100 reaching tasks for each controller, with random
blocking collisions of duration between ∈ [1,3]s at time
TheAICperformspoorlyintermsoftrackingerrors.This
∈[0,3]s.TableIreportssteady-stateerror(e ),settlingtime
ss is due to three main reasons: 1) the AIC as defined in [9],
(t )afterremovingthecollision,overshoot(os),andRMSE
s [7] only allows for position reference through its generative
between beliefs and joint positions in case of blocking the
armtemporarily,averagedoverthe100trials.Ascanbeseen, 1https://www.youtube.com/watch?v=jI-zX8XvfgI
model, while the u-AIC can perform position and velocity [7] C.Pezzato,R.Ferrari,andC.H.Corbato,“Anoveladaptivecontroller
tracking;2)amoreaggressivetuningoftheAICwouldresult forrobotmanipulatorsbasedonactiveinference,”IEEERoboticsand
AutomationLetters,vol.5,no.2,pp.2973–2980,2020.
inhighovershootsincaseofcollision,compromisingsafety.
[8] K. J. Friston, “The free-energy principle: a unified brain theory?”
2) Collision behavior: The collision behavior is reported NatureReviewsNeuroscience,vol.11(2),pp.27–138,2010.
in fig. 8. The robot is commanded to keep an initial position [9] C. L. Buckley, C. S. Kim, S. McGregor, and A. K. Seth, “The free
energy principle for action and perception: A mathematical review,”
while a person is pushing, pulling, and holding the robot.
JournalofMathematicalPsychology,vol.81,pp.55–79,2017.
The AIC shows high overshoot which might be dangerous [10] G. Oliver, P. Lanillos, and G. Cheng, “An empirical study of active
or damage delicate products such as fruits and vegetables inferenceonahumanoidrobot,”IEEETransactionsonCognitiveand
DevelopmentalSystems,2021.
in our setting. The u-AIC is instead well-behaved during
[11] P. Lanillos and G. Cheng, “Adaptive robot body learning and esti-
interaction (see the attached video for a visualization of the mation through predictive coding,” in 2018 IEEE/RSJ International
results). The same behavior is observed when the robot is ConferenceonIntelligentRobotsandSystems(IROS),2018.
[12] A. Meera and M. Wisse, “Free energy principle based state and
disturbed while performing a complicated trajectory.
inputobserverdesignforlinearsystemswithcolorednoise,”in2020
AmericanControlConf.(ACC),2020,pp.5052–5058.
[13] M. Baioumy, C. Pezzato, C. H. Corbato, N. Hawes, and R. Ferrari,
“Towardsstochasticfault-tolerantcontrolusingprecisionlearningand
activeinference,”inIWAI. Springer,2021.
[14] A. Tschantz, B. Millidge, A. K. Seth, and C. L. Buckley,
“Reinforcement learning through active inference,” arXiv preprint
arXiv:2002.12636,2020.
[15] L. Da Costa, N. Sajid, T. Parr, K. Friston, and R. Smith, “The
relationshipbetweendynamicprogrammingandactiveinference:The
discrete,finite-horizoncase,”arXivpreprintarXiv:2009.08111,2020.
[16] M.Baioumy,P.Duckworth,B.Lacerda,andN.Hawes,“Onsolving
a stochastic shortest-path markov decision process as probabilistic
Fig. 8: Experiments with real Panda arm during unwanted
inference,”inIWAI. Springer,2021.
interaction (orange area). For readability we only report the [17] H.F.Chame,A.Ahmadi,andJ.Tani,“Ahybridhuman-neurorobotics
thirdjoint,beingthemostaffectedone.Fasterconvergenceto approach to primary intersubjectivity via active inference,” Frontiers
inPsychology,vol.11,p.3207,2020.
zerosteady-stateerrorfortheu-AICcanbeachievedthrough
[18] W. Ohata and J. Tani, “Investigation of the sense of agency in
tuning of the integral action. socialcognition,basedonframeworksofpredictivecodingandactive
inference: A simulation study on multimodal imitative interaction,”
FrontiersinNeurorobotics,vol.14,Sep2020.
VII. CONCLUSION [19] T.vandeLaar,˙I.S¸eno¨z,A.O¨zc¸elikkale,andH.Wymeersch,“Chance-
constrainedactiveinference,”NeuralComputation,2021.
In this paper, we discussed the fundamental limitations of
[20] T. van de Laar, A. O¨zc¸elikkale, and H. Wymeersch, “Application of
the AIC for robot control and how the u-AIC overcomes thefreeenergyprincipletoestimationandcontrol,”IEEETransactions
them. These limitations arise from the fact that the state onSignalProcessing,vol.69,pp.4234–4244,2021.
[21] M. Baioumy, M. Mattamala, and N. Hawes, “Variational inference
estimation is biased towards the goal, and that the control
forpredictiveandreactivecontrollers,”inBAIN-PILworkshop,ICRA,
action is not explicitly modeled in the generative model. Paris,France,2020.
These cause degraded state estimation and can lead to large [22] M.BaltieriandC.L.Buckley,“AprobabilisticinterpretationofPID
controllers using active inference,” in Int. Conf. on Simulation of
overshoots during human-robot interaction. We thoroughly
AdaptiveBehavior. Springer,2018,pp.15–26.
discussedandextendedtheu-AICprovidingamissingproof [23] ——, “On kalman-bucy filters, linear quadratic control and active
of convergence. We theoretically demonstrated the limita- inference,”arXivpreprintarXiv:2005.06269,2020.
[24] C.MeoandP.Lanillos,“Multimodalvaeactiveinferencecontroller,”
tionsoftheAICandprovidedexperimentalevidenceofhow
in2021IEEE/RSJInternationalConferenceonIntelligentRobotsand
theu-AICovercomesthem,bothinsimulationandinthereal Systems(IROS). IEEE,2021.
world with a 7-DOF robot manipulator. [25] K. Friston, K. Stephan, B. Li, and J. Daunizeau, “Generalised filter-
ing,”MathematicalProblemsinEngineering,2010.
REFERENCES [26] W. Vega-Brown and N. Roy, “Cello-em: Adaptive sensor models
without ground truth,” in 2013 IEEE/RSJ International Conf. on
[1] K.Friston,T.FitzGerald,F.Rigoli,P.Schwartenbeck,andG.Pezzulo, IntelligentRobotsandSystems. IEEE,2013,pp.1907–1914.
“Active inference: a process theory,” Neural computation, vol. 29, [27] T. Pfeifer, S. Lange, and P. Protzel, “Dynamic covariance estima-
no.1,pp.1–49,2017. tion—a parameter free approach to robust sensor fusion,” in 2017
[2] P.Lanillos,C.Meo,C.Pezzato,A.A.Meera,M.Baioumy,W.Ohata, IEEE International Conf. on Multisensor Fusion and Integration for
A. Tschantz, B. Millidge, M. Wisse, C. L. Buckley, et al., “Active IntelligentSystems(MFI). IEEE,2017,pp.359–365.
inference in robotics and artificial agents: Survey and challenges,” [28] K. J. Friston, N. Trujillo-Barreto, and J. Daunizeau, “Dem: a varia-
arXivpreprintarXiv:2112.01871,2021. tionaltreatmentofdynamicsystems,”Neuroimage,2008.
[3] M. Baioumy, C. Pezzato, R. Ferrari, C. H. Corbato, and N. Hawes, [29] K. Friston, K. Stephan, B. Li, and J. Daunizeau, “Generalised filter-
“Fault-tolerantcontrolofrobotmanipulatorswithsensoryfaultsusing ing,”MathematicalProblemsinEngineering,vol.2010,2010.
unbiasedactiveinference,”inEuropeanControlConf.(ECC),2021.
[30] K.Friston,J.Mattout,N.Trujillo-Barreto,J.Ashburner,andW.Penny,
[4] M. Baioumy, P. Duckworth, B. Lacerda, and N. Hawes, “Active “VariationalfreeenergyandtheLaplaceapproximation,”Neuroimage,
inferenceforintegratedstate-estimation,control,andlearning,”inProc
vol.34(1),pp.220–234,2007.
ofIEEEInt.Conf.onroboticsandautomation(ICRA),2021.
[31] L. E. Olivier and I. K. Craig, “Fault-tolerant nonlinear mpc using
[5] M. Baltieri and C. L. Buckley, “PID control as a process of active particlefiltering,”IFAC-PapersOnLine,2016.
inferencewithlineargenerativemodels,”Entropy,vol.21,no.3,2019.
[32] M.BaltieriandC.L.Buckley,“Themodularityofactionandpercep-
[6] C. Pezzato, M. Baioumy, C. H. Corbato, N. Hawes, M. Wisse, and tionrevisitedusingcontroltheoryandactiveinference,”arXivpreprint
R.Ferrari,“Activeinferenceforfaulttolerantcontrolofrobotmanip- arXiv:1806.02649,2018.
ulatorswithsensoryfaults,”in1stInt.WorkshoponActiveInference,
ECML PKDD, ser. Communications in Computer and Information
Science,Springer,Ed.,vol.1326,2020.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Unbiased Active Inference for Classical Control"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
