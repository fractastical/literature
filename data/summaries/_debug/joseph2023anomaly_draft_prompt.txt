=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Anomaly Detection via Learning-Based Sequential Controlled Sensing
Citation Key: joseph2023anomaly
Authors: Geethu Joseph, Chen Zhong, M. Cenk Gursoy

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2023

Key Terms: decision, anomaly, process, anomalies, learning, sensing, binary, sequential, detection, controlled

=== FULL PAPER TEXT ===

3202
voN
03
]GL.sc[
1v88000.2132:viXra
IEEESENSORSJOURNAL,VOL.XX,NO.XX,XXXX2024 1
Anomaly Detection via Learning-Based
Sequential Controlled Sensing
Geethu Joseph,Chen Zhong,M.Cenk Gursoy,SenemVelipasalar,and
Pramod K. VarshneyLife Fellow,IEEE
Process 1
Process 2
.
.
.
Process N
noitubirtsid
tnioJ
Abstract—In this paper, we address the problem of
detecting anomalies among a given set of binary pro- Environment
cessesvialearning-basedcontrolledsensing.Eachpro-
cess is parameterized by a binary random variable in-
dicating whether the process is anomalous. To identify Dynamic
the anomalies, the decision-makingagent is allowed to sensor selection
Our learning
observea subsetof theprocessesateachtime instant.
Also,probingeachprocesshasanassociatedcost.Our algorithm
Noisy sensor
objectiveis to design a sequentialselectionpolicy that
dynamically determines which processes to observe at measurements Output
each time with the goal to minimize the delay in mak-
ing the decision and the total sensing cost. We cast Anomalous
thisproblemasasequentialhypothesistestingproblem
processes’ indices
withintheframeworkofMarkovdecisionprocesses.This
formulationutilizesbothaBayesianlog-likelihoodratio-basedrewardandanentropy-basedreward.Theproblemisthen
solvedusingtwoapproaches:1)adeepreinforcementlearning-basedapproachwherewedesignbothdeepQ-learning
andpolicygradientactor-criticalgorithms;and2)adeepactiveinference-basedapproach.Usingnumericalexperiments,
wedemonstratetheefficacyofouralgorithmsandshowthatouralgorithmsadapttoanyunknownstatisticaldependence
patternoftheprocesses.
Index Terms—Active hypothesis testing, anomaly detection, active inference, quickest state estimation, sequential
decision-making,sequentialsensing.
I. INTRODUCTION served condition may get flipped from the actual condition
with a certain probability. This paradigm is encountered in
Sequential controlled sensing refers to a stochastic frame-
manypracticalapplicationssuchasremotehealthmonitoring,
work in which an agent sequentially controls the process
assembly lines, structural health monitoring and Internet of
of acquiring observations. The goal here is to minimize the
Things(IoT).In such applications,the objectiveis to identify
cost of making observations while satisfying the inference
the anomalies among a given set of different(not necessarily
objectives. We consider a sequential controlled sensing prob-
independent)functionalities of a system [1], [2]. Each sensor
lem in the context of anomaly detection wherein there are
monitorsadifferentfunctionalityandsendsobservationstothe
N processes, each of which can be in either a normal or
agent over a communication link. The received observation
an anomalous condition. Our goal is to identify the anoma-
may be distorted due to the unreliable nature of the sensor
lies among the given processes. To this end, the decision-
hardware and/or the noisy link (e.g., a wireless channel)
making agent sequentially chooses a subset of processes (or
between the sensor and the agent. Hence, the agent needs
equivalentlysensorsmonitoringtheseprocesses)ateverytime
to probe each process multiple times before declaring one or
instant, probesthem, and obtainestimates of their conditions.
moreoftheprocessesanomalouswiththedesiredconfidence.
The agent generally obtains noisy observations, i.e., the ob-
Repeatedly probing all the processes allows the agent to find
ThisworkwassupportedinpartbytheNationalScienceFoundation anypotentialsystem malfunctionoranomalyquickly,butthis
undergrantsENG60064237.Thematerialinthispaperwaspresented incursa higherenergyconsumptionthat reducesthe life span
in part at the IEEE International Workshop on Signal Processing Ad-
of the network. Therefore, we address the question of how
vancesinWirelessCommunications,May2020,Atlanta,GA,USA,and
theIEEEGlobalCommunicationsConference,December2020,Taipei, the agent must sequentially choose the subset of processes to
Taiwan. accurately detect the anomalies while minimizing the delay
G.JosephiswiththefacultyofElectricalEngineering,Mathematics,
and cost of making observations.
and Computer Science, Delft Technical University, 2628 XE, Nether-
lands(email:g.joseph@tudelft.nl). We startwith a briefliteraturereview.A classicalapproach
C. Zhong, M. C. Gursoy, S. Velipasalar, and P. K.Varshney to solvethe sequentialprocessselectionproblemforanomaly
are with the Department of Electrical and Computer
detection is based on the active hypothesis testing frame-
Engineering, Syracuse University, New York, 13244, USA
(emails:{czhong03,mcgursoy,svelipas,varshney}@syr.edu.) work [3], [4]. Here, the decision-making agent constructs a
2 IEEESENSORSJOURNAL,VOL.XX,NO.XX,XXXX2024
hypothesis corresponding to each of the possible conditions of finding an optimal policy that maximizes the long-
oftheprocessesanddetermineswhichoneofthesehypotheses termrewardoftheMDPsubjecttotheconditionthatthe
is true. The goal of active hypothesis testing is to infer confidencelevelontheestimateexceedsaspecificvalue.
the true hypothesis by collecting relevant data sequentially • RL algorithms: In Section IV, we develop the deep
until sufficiently strong evidenceis gathered.In [5], Chernoff RL framework through which an optimal policy that
proposedarandomizedstrategyandestablisheditsasymptotic maximizes the discounted cumulative reward is learned.
optimality. This seminal work in [5] was followed by several We develop two deep RL algorithms, based on the Q-
other studies that investigated active hypothesis testing un- learning and actor-critic frameworks.
der different settings [6]–[10]. These studies investigated the • Active inference: In Section V, we present an alternative
theoreticalaspectsoftheproblemandpresentedafewmodel- solution strategycalled activeinference.Here, we define
basedalgorithmstosolvetheproblem.However,thesemodel- thenotionoffree-energybasedontheentropyassociated
based algorithms are designed under simplified modeling with the estimate of the states of the processes and
assumptions. This has motivated the researchers to design the sensing cost and reformulate the anomaly detection
data-drivendeeplearningalgorithmsforactivehypothesistest- problem as an active inference problem to minimize the
ing [3], [4], [11]–[15]. These algorithms are not only model freeenergy.Theresultingalgorithmisimplementedusing
free and thus, more flexible than the traditional algorithms, deep neural networks which are relatively less explored
but they also possess reduced computational complexity. It for active inference.
shouldbenotedthattheclassicalsequentialhypothesistesting • Empiricalvalidation:Viaournumericalresultspresented
frameworkdoesnotincorporatethe sensingcost in the detec- inSectionVI,weinvestigatetheperformanceofdifferent
tion problem and assumes that the decision-maker chooses frameworks and algorithms in terms of detection accu-
thesamefixednumberofprocessesateverytimeinstant.Our racy, delay, and sensing cost. We show that the active
problem setting is different from these models. Specifically, inference algorithm is more robust to the variations in
the decision-maker can choose any number of processes at the system parameters and adapts better to statistical
each time instant, and this choice is determined by the cost dependence among the processes. Further, we observe
associated with the observations. We also account for the that as the statistical dependence between the states of
potential statistical dependence among the processes. theprocessesincreases,thedelayinstateestimationgets
Our anomaly detection problem is different from the se- diminished.Thisresultimpliesthatunlikethe traditional
quential parameter estimation. Sequential parameter estima- Chernoff test, our algorithms are able to learn and
tion refers to estimating the random parameter of a pro- exploit any underlying statistical dependence among the
cess [16]–[18].Althoughthis goalis similar to ours, the con- processes to reduce the number of observations.
trolled sequential selection of processes makes our problem
In summary, we use the model-based posterior updates to
fundamentallydifferentfromsequentialparameterestimation.
tackle the uncertainties in the observations and the data-
Inparticular,onlyoneBernoulliprocessisconsideredin[16]–
driven neural networks to handle the underlying statistical
[18], and at every time instant, the decision-maker only
dependencebetweentheprocesses,balancingthemodel-based
decides whether or not to continue collecting observations.
and the data-driven approaches.
Hence, the results in these studies apply to our setting only
Furthermore,inthispaper,comparedtotheconferencever-
if we consider the set of all N processes as a single random
sions, we conducta morecomprehensiveand unifiedanalysis
processwith2N statesandchoosethesub-optimalstrategyof
of deep learning-based anomaly detection and make several
observing all the processes all the time.
newcontributions:1)WedesignanewRLalgorithmbasedon
the deepQ-learningalgorithmwhich we implementusingthe
A. OurContributions dueling architecture; 2) in addition to the LLR-based reward,
we introduceanentropy-basedreward(newlyappliedin deep
To the best of our knowledge, ours is the first work
RL algorithms), and we mathematically show that the two
that formulated the anomaly detection problem as an active
reward functions encourage the agent to achieve the desired
hypothesis testing problem and developed specific solutions
confidence level as quickly as possible (see Proposition 1);
for the problem. Our specific contributions are as follows:
3) we derive the Chernoff test for the anomaly detection
• Formulation of anomaly detection as a Markov decision problem and compare its performance with our algorithms;
process: In Section III, we formulate the anomaly de-
4) we present a detailed numerical study that compares the
tection problem as a Markov decision process (MDP).
different algorithms when the cost and flipping probabilities
We define the posterior belief vector on the conditions
are different across the processes.
of the processes as the state of the MDP, the subset of
The remainder of the paper is organized as follows. We
processes chosen by the decision-making agent as the
presentthe system modelin SectionII and describethe MDP
action,andtwodifferenttypesofrewardfunctions:anav-
problem in Section III. In Sections IV and V, we present our
erage Bayesian log-likelihood ratio (LLR) based reward
RL and active inference algorithms, respectively. We provide
and an entropy-based reward. The rewards are designed
the simulation results in Section VI and offer our concluding
such that the optimal policy of the MDP minimize the
remarks in Section VII.
sensing cost and the delay in decision-making. Finally,
theprocess/sensorselectionisformulatedastheproblem
G.JOSEPHetal.:ANOMALYDETECTIONVIALEARNING-BASEDSEQUENTIALCONTROLLEDSENSING 3
II. ANOMALY DETECTION PROBLEM
x
We consider a set of N processes wherein each process
is in one of the two conditions: normal (denoted by 0) or
anomalous(denotedby1). Theconditionof the i th processis
denoted by the i th entry x of a random vector x∈{0,1}N. y A(1) (1) y A(2) (2) ... y A(T) (T)
i
The vector x can take M , 2N possible values denoted by
h(i), i=1,2,...,M . The conditions of these processes
( n entries of x) can be p o otentially statistically dependent. This A(1) A(2) ... A(T)
dependence is captured by the prior distribution of x that is
denoted using π(0)∈[0,1]M whose i th entry π (0) is
i
π i (0)=P x=h(i) . (1) π(0) π(1) π(2) ... π(T)
n o
Our goal is to identify the anomalous processes out of the
N processes, which is equivalent to estimating the random
vectorx.Toestimatex,thedecision-makingagentprobesone Fig.1:Graphicalmodelwithposteriors(MDPstates),actions
ormoreprocessesateverytimeinstantandobtainspotentially and observations
erroneousobservationsof the correspondingentries of x. Let
the set of processes probed at time t be A(t)∈P(N) where
P(N) denotes the power set of {1,2,...,N} without the 2) stopping rule: a mechanism to determine when to stop
null set (i.e., |P(N)| = M − 1). Also, let the observation taking observations and declare xˆ.
corresponding to the i th process at time t be denoted as
We next present a novel anomaly detection algorithm that
y (t). Dependingon the condition,y (t) obeysthe following
i i wederivebycastingthedetectionasalearningproblemusing
probabilistic model:
an MDP framework.
x with probability 1−p
y (t)= i i (2)
i (1−x
i
with probability p
i
,
III. MDP-BASED LEARNING PROBLEM FORMULATION
where p ∈ [0,0.5] is called the cross-over (or flipping)
i This section describes the MDP framework that models
probability of the i th process. We also assume that given x,
the anomaly detection problem. An MDP represents a se-
the observations are jointly (conditionally) independent:
quential decision making problem in stochastic environments
t N wherein the state of the environment depends on the action
P {y(τ)} t τ=1 x = P[y k (τ)|x], (3) of a decision-maker. The goal of the decision-maker is to
h (cid:12) i τ Y =1k Y =1 sequentially decide which action to choose while in a given
for any t > 0 where y (cid:12) (cid:12)(τ) ∈ {0,1}N and its k th entry is state to maximize the reward which is the same as finding
y (τ).Further,probingthei th processincursacostofc >0. a mapping from the states to the actions. This mapping is
k i
In short, at every time instant t, the decision-maker probes referred to as a policy and it can be either deterministic (i.e.,
the processes indexed by A(k), obtains the corresponding aone-to-onemapping)orstochastic(describedbyconditional
observations denoted by y (t) ∈ {0,1}|A(t)|, and incurs probability distributions over actions given the states).
A(t)
a sensing cost of c .
k∈A(t) k
In this setting, the three performance metrics associated
with the decision P making are the following: A. StateandAction
1) stopping time denoted by T which is the time instant Inthecontextofouranomalydetectionproblem,wedefine
whenthedecision-makerendstheobservationacquisition
the state of the MDP at time t as the posterior belief vector
phase and yields its estimate xˆ of x; π(t) on the random vector x∈{0,1}N. The i th entry of the
2) detection accuracy given by the conditional probability posterior belief vector π(t)∈[0,1]M is defined as
T
P xˆ =x y (t) ;
3) se (cid:20) nsingco (cid:12) (cid:12)sntg A iv ( e t n ) by o t= T 1 (cid:21) c whichrepresents π i (t)=P x=h(i) y A(τ) (τ) t . (4)
thetotalc (cid:12) (cid:12)ostincurreddu t r = in 1 gth k e ∈A ob (t s ) er k vationacquisition. (cid:20) (cid:12) (cid:12)n o τ=1 (cid:21)
P P Further, the action at time t ref(cid:12)ers to the set of processes to
The strategy of probing all the processes at all times may (cid:12)
be observed, A(t)∈P(N).
lead to the most accurate and fastest decision, but at the
We next establish the connections between the states, ac-
expense of a higher sensing cost. Therefore, the decision-
tions, and corresponding observations which can be repre-
maker sequentially chooses a subset of processes A(t) ∈
sentedusingaprobabilisticgraphicalmodeldepictedinFig.1.
P(N)tobalancethetrade-offsbetweenthethreeperformance
metrics. Ourdecision-makingalgorithmhastwo components:
We first note that at time t, the data available to the agent is
1) sequential process selection: a mechanism to choose y A(τ) (τ),τ =1,2,...,k using which the posterior belief
A(t)∈P(N) at every time t, nvector π(t)∈[0,1]M can boe computed in closed form. From
4 IEEESENSORSJOURNAL,VOL.XX,NO.XX,XXXX2024
(4), π (t) is computed recursively from (3) and (1) as
i
π (t−1) P y (t) x=h(i) i k
π i (t)= k∈YA(t) h (cid:12) (cid:12) i , (5)
M (cid:12)
π (t−1) P y (t) x=h(j) j k
X j=1 k∈YA(t) h (cid:12) i
(cid:12)
where we obtain from (2) that (cid:12)
P y (t) x=h(i) k
h (cid:12)=(1−pi)
(cid:12) k (cid:12) 1
+p
n yk(t)=h k (i) o k 1
, (6)
n yk(t)6=h k (i) o
1
encourages the decision-maker to move away from the non-
informative posterior π(t) = 1/M1 and move towards the
posterior π(t)∈{e } M . Here, 1 denotes the all-ones vector i i=1
and e ∈ {0,1}M denotes the i th column of the M × M i
identitymatrix.Twofunctionsthatachievethisgoalaregiven
by the following proposition [19].
Proposition 1: Let L,H : [0,1]M → R be two functions
defined, respectively, as
M
π
L(π)= π log i (9) i 1−π
i
i=1
X
M
H(π)=− π logπ . (10)
i i
is the indicator function, and h(i) ∈{0,1} is the k th entry i=1
k X of h(i). As a result, given the previous posterior π(t −1), These functions satisfy
the action A(t) and the observation y , we can exactly 1
A(t) argmin L(π)= argmax H(π)= 1
compute the updated posterior belief vector π(t) using (5). π∈[0,1]M π∈[0,1]M M
Se B ct e io fo n re II w th e at d o e u fi r ne go t a h l e is no to tio a n ch o ie f ve re a wa b r a d l , an w ce e b r e e t c w al e l en fro th m e P M i=1 πi=1 P M i=1 πi=1
argmax L(π)= argmin H(π)={e } M ,
detection accuracy, stopping time, and sensing cost. We can i i=1
π∈[0,1]M π∈[0,1]M
u ac s c e u t r h a e cy po v s i t a er t i h o e rs s o to r p t p h i e ng MD ru P le st u a s t i e n s g to a c p o a n r t a r m ol et t e h r e π detectio ∈ n P M i=1 πi=1 P M i=1 πi=1
upper where 1 denotes the all-ones vector and e ∈[0,1]M denotes
(0,1). The parameter π represents the threshold on the i
upper the i th column of the M ×M identity matrix.
largest value among the beliefs on the different values of x,
Proof: From the log-sum inequality, we have
max π i (T)≥π upper . (7) M a M M a
i=1,2,...,m a log i ≥ a log i=1 i ,
Having definedthe stoppingrule (one of the two components i=1 i (cid:18) b i(cid:19) i=1 i ! P M i=1 b i!
X X
n
o
e
f
x
t
t
he
fo
a
c
l
u
g
s
or
o
it
n
hm
th
),
e
w
tr
h
a
i
d
c
e
h
-o
c
f
o
f
n
b
tr
e
o
t
l
w
s
e
t
e
h
n
e d
th
e
e
tec
st
t
o
io
p
n
pi
a
n
c
g
cu
t
r
im
ac
e
y,
a
w
nd
e for any set {a
i
≥0,b
i
≥0} M
i=1
, and equaPlity holds only if
a =αb ,forsomeconstantα>0.Thus,foranyπ ∈[0,1]M,
i i
sensing cost. This trade-off is determined by the sequential
M
process selection (the other algorithm component), which we 1 π (M −1)
L(π)−L 1 = π log i
derive using the notion of reward and policy. M i 1−π
(cid:18) (cid:19) i=1 i
X
M M
π
B. T R he ew re a w rd ard r(t) at time t is a function of the posterior ≥ X i=1 π i ! log P M i= i= 1 1 M 1− − π i 1 i ! =0.
beliefs π(t) and π(t−1), and the selected processes A(t). Hence,L(π)≥L M 1 1 andequalityholPdsonlyifπ i =1/M.
The reward indicates the intrinsic desirability of choosing the We next look at the maximum of L(π) and we see that if
(cid:0) (cid:1)
subset of processes as a function of the posterior belief. For π i ∈ [0,1) for all values of i, L(π) < ∞. Therefore, L(π)
a given reward function, the policy µ : [0,1]M → P(N) attainsthe maximumvalue if and onlyif at least oneentry of
is a mapping from the posterior belief vector π(t − 1) to π is 1. Hence, the desired maxima is achieved at {e i } M i=1 .
the processes to be probed A(t). The policy represents the We can compute the maxima and minima of H(π) using
similar arguments and thus, the proof is complete.
sequential process selection part of our algorithm, and it is
The above proposition implies that the functions L and H
designed to maximize the long-termreward given as follows:
are two good choices for ξ. We note that in (9), the term
T log πi is the likelihood ratio (LLR) of the two hypotheses
R(T)= E{r(t)}. (8) 1−πi
namely, x = h(i) and x 6= h(i). Consequently, L(π) is the
t=1
X Bayesian LLR obtained by applying the logit function on
Here,theexpectationisovertheuncertaintyinthe valueofx
the posterior belief. Further, maximizing the Bayesian LLR
andtheobservationsy (t)whenA(t)followsthepolicyµ.
A(t) increases the posterior belief on the true value of x. Also, H
The reward function balances the trade-off between the
is the entropy of the distribution π, and thus, minimizing H
stopping time and sensing cost. Here, the sensing cost can be
reduces the uncertainty in estimation and builds the posterior
quantified as a function of A(t) using the term k∈A(t) c k . belief on the true value of x.
However, we also need a term in the reward which forces Having defined the function1 ξ, we formulate the instanta-
P
the policy to build the posterior belief on the true value of
x as quickly as possible, and thus minimize the stopping 1Thealgorithmic development isindependent ofthechoice ofthereward
time T. We represent this term using ξ : [0,1]M → R function(LLRandentropy-based). Therefore,intheremainderofthepaper,
weuse ξ(·)in the reward ofthe MDP which can either beL(·) defined in
which is a function of the posterior beliefs. We seek ξ that (9)or−H(·)definedin(10).
G.JOSEPHetal.:ANOMALYDETECTIONVIALEARNING-BASEDSEQUENTIALCONTROLLEDSENSING 5
neous reward of the MDP as a weighted sum of ξ and the distant future relative to those in the immediate future, i.e., a
sensing cost: reward received j time steps in the future is worth only γj−1
timeswhatitwouldbe worthif itwere receivedimmediately.
r(t)=ξ(π(t))−ξ(π(t−1))+λ c , (11)
k Hence, this approach encourages the agent to minimize the
k∈XA(t) stopping time.
where λ > 0 is the weighing parameter that dictates the To maximize R¯(t), the RL algorithms make process selec-
balance between the stopping time and the total sensing cost. tion based on the value functions of the posterior-action pair
Thus,from(8)and(11),thelong-termexpectedrewardofthe and the posterior. For a given policy µ, these value functions
MDP up to time t is given by are
t Q µ (π,A)=E R¯ k π(t−1)=π,A(t)=A (14)
R(t)=E  ξ(π(t))−ξ(π(0))−λ c k  , V µ (π)=E A(cid:8)∼µ((cid:12)π) R¯ k π(t−1)=π , (cid:9) (15)
 τ X =1k∈XA(τ)  where the expectationsar (cid:12) e e(cid:8)valu(cid:12)ated given tha(cid:9)t the agent fol-
where the expe  ctation is over the distribution of x  and the lowsthepolicyµ forallfuturea (cid:12) ctions.Intuitively,theaction-
observations y A(t) (t) given A(t). The MDP objective is to value function (referred to as the Q-function), Q µ (π,A), in
find a policy or sequence of actions {A(t)∈P(N)} T that (14)indicatesthelongtermdesirabilityofchoosingaparticu-
t=1
maximizes the long-term average sum of the rewards. Hence, laractionwhentheposteriorbeliefvectorisπ.Also,thestate-
a policy that maximizes the long-term reward improves the value function (referred to as the value function), V (π), in
µ
accuracy of the estimate (quantified by ξ(π(t))) as soon as (15)specifiestheexpectedrewardwhenstartingwithposterior
possible while minimizing the overall sensing cost (the last belief vector π and following the policy µ thereafter. An RL
term in R(t)). Further, the agent continues to take observa- agentmakestheactionchoicesbyevaluatingtheoptimalvalue
tions until it declares an estimate with the desired level of estimates, Q-function or the value function, or both. If we
confidence given by π (i.e., t = T). Therefore, if λ have the optimalvaluesof the functions,then the actionsthat
upper
is small, the reward ensures that the agent chooses actions appearbestafteraone-stepsearcharetheoptimalactions[20].
with a significant change ξ(π(t)) − ξ(π(0)), leading to a Inthefollowing,wepresenttwo differentRL approaches,the
shorter stopping time. On the other hand, with a large λ, the Q-learningand actor-critic algorithms, and describe how they
agent tries to minimize the sensing cost by probing a few estimate these functions to arrive at the optimal policy.
processes at every time instant which increases the stopping
time.Therefore,λcontrolsthestoppingtimeandtotalsensing A. DuelingDeepQ-learning
cost.
The Q-learning approach is a popular RL algorithm where
Further, the Bayesian LLR L is unbounded unlike the
theagentestimatestheQ-functionandchoosestheactionA(t)
entropy satisfying H(π)≤logM for any π ∈[0,1]M. Also,
thatmaximizestheQ-functiongiventheposteriorbeliefvector
M π(t−1) [21]. Further,in the deep Q-learning framework,the
L(π)=−H(π)− π
i
log(1−π
i
)≥−H(π). (12) unknownQ-functionis modeledusing a neuralnetwork [22],
i=1 and the dueling deep Q-learning framework refers to the
X
Therefore,for the same value of λ, the Bayesian LLRreward implementation of this neural network using a model called
function gives a higher weight to the accuracy than the the dueling architecture [23].
cost. As a result, the sensitivity of the trade-off between the This architecture consists of a single Q-network and relies
accuracyandsensingcostdiffersforthetworewardfunctions. on a quantity called the advantage function: A µ (π,A) =
We discuss this point in detail in Section VI. Q µ (π,A)−V µ (π),whichisameasureofhowmuchQ µ (π,A)
This completes our discussion on the reward function. deviates from the expected value over all the actions, V µ (π),
Using this formulation, we next present the deep learning and therefore, specifies the relative preference of each action
algorithms to obtain policies that maximize the long-term for a given posterior belief vector. The dueling architecture
reward of the MDP. We use two approaches: the deep RL- estimates both V µ (π) and A µ (π,A) separately and combines
based approach presented in Section IV and deep active them to obtain Q µ (π,A). The input to the Q-network is the
inference-based approach presented in Section V. posteriorbelief vectorπ ∈RM andthe outputisan (M−1)-
length vector whose i th entry corresponds to Q (π,·) of the
µ
i th possible action. The network parameter θ is obtained
IV. ANOMALY DETECTION USING DEEP RL DQN
by optimizing the loss function [23]:
ALGORITHMS
Our RL algorithms are designed to maximize the expected
discounted return R¯(t) defined as L DQN (θ DQN )=E π,A,π′ ( r(t)+γ A′ m ∈P a ( x N) Q(π′,A′;θ D − QN )
T
R¯(t)= lim γjr(t+j), (13) −Q(π,A;θ ) , (16)
DQN
T→∞ j=0 )
X
where 0 < γ < 1 (which is generally close to 1) is the where θ− is the current network parameter estimate ob-
DQN
discount factor. This parameter γ weighs the rewards in the tained in the previous time. We update the Q-function using
6 IEEESENSORSJOURNAL,VOL.XX,NO.XX,XXXX2024
bootstrapping by basing its update in part on an current of the temporal-difference(TD) error as given by
estimate Q(π′,A′;θ− ). Also, to ensure that the learned
DQN δ(t)=r(t)+γV (π(t))−V (π(t−1)),
value of the Q-function convergesto the optimal Q-function, µ µ
we use the greedy policy to choose the successor action A′. for a given policy µ(·) with
Using the learned Q-function, we next describe how to
obtain the optimal policy. We derive the policy using a V µ (π)=E π′,A∼µ(π) {r(t)+γV µ (π′)|π(t−1)=π}.
combination of the decaying-epsilon greedy algorithm [23], If the TD error is positive, the probability of choosing A(t)
[24] and the Gibbs softmax method [25]. At every time step, in the future is increased, and vice versa. Therefore, the
the agent takes action using the Gibbs softmax method with input to the critic network is the posterior belief vectors
a probability of 1−ǫ and a random action with a probability π ∈[0,1]M andtheoutputisthecorrespondingvaluefunction
of ǫ. Here, ǫ is a parameter that decays with time. Also, the V(π;θ ) ∈ R where θ represents the neural network
critic critic
Gibbs softmax method refers to choosing the action A(t) ∼ parameters.
σ(Q(π(t),·;θ )) ∈ [0,1]M−1 where σ(·) is the softmax We next describe how to learn the two sets of network
DQN
function. The approachensures that the entire action space is parameters: θ of the actor and θ of the critic. Since
actor critic
exploredwhileexploitingthebestactionwithhighprobability. the goal of the critic network is to fit a model to estimate the
Thealgorithmchoosesactionsintheabovefashionuntilthe optimal value function, its parameter update is equivalent to
posterior belief distribution satisfies the stopping rule in (7). minimizingthemodelmismatchbetweentherewardobtained
We presentthe pseudo-codefor our duelingdeep Q-learning- at the currenttime step and the learned value function. Thus,
based detection algorithm in Algorithm 1. the critic network updatesits parameter θ by minimizing
critic
the square of the TD error given by
Algorithm 1 Dueling Q-learning algorithm for anomaly de-
δ(t)=r(t)+γV(π(t);θ )−V(π(t−1);θ− ), (17)
tection critic critic
Parameters: Priordistributionπ(0),discountrateγ ∈(0,1), where θ c − ritic is the current critic network parameter estimate
and confidence level π ∈(0,1] obtained in the previous time instant. On the other hand, the
upper
Initialization: Q-network parameter θ arbitrarily goaloftheactornetworkistofindapolicythatmaximizesthe
DQN
1: repeat valuefunction.Thus,itsparameterupdateisviamaximization
2: Time index t=1 of the value function. The actor updatesits parameter[20] as
3: while maxπ i (t−1)<π upper do θ =θ− +δ(t)∇ [logµ (π(t−1),A(t);θ )],
i actor actor θactor AC actor
4: Choose action A(t) using the policy derived from (18)
Q(π(t),·;θ DQN ) where θ a − ctor is the current actor network parameter estimate
5: Generate observations y (t) obtained in the previous time instant and δ(t) given by (17)
A(t)
6: Compute π(t) using (5) and r(t) using (11) is obtained from the critic network.
7: Update θ DQN by minimizing L DQN (θ DQN ) in (16) The learned policy is straightforward in the case of the
8: Increase time index t=t+1 actor-critic framework, as the actor network directly learns
9: end while the policy. Hence, at every time step, the agent chooses an
10: Declare xˆ =h k
(i∗)
where i∗ =argmaxπ i (t−1) action based on the output of the actor network and receives
i the reward for updating both actor and critic networks. The
11: until
agentstops collecting observationsand returnsan estimate of
x when the confidence level exceeds the desired level, i.e.,
when(7)holds.Thepseudo-codeofouralgorithmis givenin
B. DeepActor-critic Algorithm 2.
Thedeepactor-criticalgorithmisanotherRLalgorithmthat
V. ANOMALY DETECTION USING DEEP ACTIVE
directly learns the policy. This principle differs from that of
INFERENCE
the dueling deep Q-learning algorithm, which learns the Q-
functionandderivesapolicybasedonthelearnedQ-function. The active inference frameworkis an alternate approachto
solving the MDP problem described in Section III. It is in-
The actor-criticarchitectureconsistsof two separateneural
spiredbyanormativetheoryofbrainfunctionbasedonitsper-
networks, actor and critic, with no shared features. The actor
ceptionoftheMDP,i.e.,theactiveinferenceagentmaintainsa
learns the policy, which chooses the action based on the
posterior probabilities. Thus, its input is π ∈ RM and the generativemodelthatrepresentsitsperception[26]–[28].This
outputisµ (π,·;θ )∈[0,1]M−1whereθ represents generative model φ(·) comprises a joint probability distribu-
AC actor actor
tionontheposterior,theactions,andthecorrespondingobser-
the neural network parameters. The policy returned by the
actor network is a stochastic policy which chooses an action vations: π(t−1),A(t),y (t),t>0 .Themodelassigns
A(t)
according to A ∼ µ AC (π,·;θ actor ). The critic refers to the higher pr n obabilities to the posteriors and o actions favorable to
learned value function, which is an estimate of how good the the agent. Given a generative model, the agent inverts the
policy learned by the actor is and hence, essentially provides model to find the conditional distribution of the action A(t)
anevaluationofthatpolicy.TheevaluationoftheactionA(t) corresponding to the posterior π(t−1). However, since di-
taken corresponding to the posterior π(t−1) takes the form rectlycomputingthemarginalsisdifficult,weusethemethod
G.JOSEPHetal.:ANOMALYDETECTIONVIALEARNING-BASEDSEQUENTIALCONTROLLEDSENSING 7
Algorithm 2 Actor-critic RL for anomaly detection divergencebetweenthevariationaldistributionµ (·) andthe
AI
Parameters: Priordistributionπ(0),discountrateγ ∈(0,1), generative model φ(·):
and confidence level π ∈(0,1]
upper
F(t)= µ (π(t−1),A(t))
Initialization: Actor and critic neural network parameters AI
θ
actor
and θ
critic
arbitrarily A(t)X∈P(N)
1: repeat ×log µ AI (π(t−1),A(t)) . (21)
2: Time index t=1 φ(A(t),y |π(t−1))
A(t)(t)
3: while maxπ i (t−1)<π upper do Thus,theagentconstructsthegenerativemodelusingtheEFE,
i
4: Choose action A(t) using the policy derived from obtainsthe optimum policy by minimizing the EFE of all the
µ AC (π(t−1),·;θ actor ) pathsintothefuture,andchoosesanactionthatminimizesthe
5: Generate observations y A(t) (t) EFE. In other words, determining the optimal policy reduces
6: Compute π(t) using (5) and r(t) using (11) to computing and optimizing the EFE.
7: Update θ critic by minimizing the squared temporal Next,we presenttheneuralnetworkarchitectureandlearn-
error δ2(t) in (17) ing of the neural network parameters. The deep active infer-
8: Update θ actor using (18) encealgorithmconsistsoftwoneuralnetworks:thepolicyand
9: Increase time index t=t+1 EFE.Thepolicynetworkdirectlylearnstheprocessselection,
10: end while and therefore, it takes the posterior belief vector π(t − 1)
11: Declare xˆ =h k
(i∗)
where i∗ =argmaxπ i (t−1) as the input. Its output is the stochastic selection policy
12: until i µ AI (π(t − 1),·;θ policy ) ∈ [0,1]M−1 which is a probability
distribution on P(N). Here, θ denotes the neural net-
policy
work parameters. The EFE network represents EFE’s learned
value, which estimates how close the learned policy is to
of approximate Bayesian inference. To this end, it defines
the generative model. Thus, the input of the EFE network
a variational distribution µ (π,A) that is controlled by the
AI is the posterior π ∈[0,1]M, and the output is the EFE value
agent.Thedistributionµ (·)is optimizedbyminimizingthe
AI G(π,·;θ ) ∈ RM−1, representing the EFE values of each
Kullback-Leibler (KL) divergence between the distributions EFE
action A ∈ P(N) and the posterior π. Here, θ denotes
µ (·) and φ(·). Therefore, a stochastic policy that chooses EFE
AI
the parameters of the EFE network.
actions according to the distribution µ (·) maximizes the
AI
The EFE can be approximated as follows [29]:
obtained reward. The KL divergence between the variational
distribution and the generative model is called the variational G(A(t),π(t−1))≈E{−r(t)+G(A(t+1),π(t))},
free energy. In other words, the goal of the active inference
where we use (19). Therefore,we learn the parametersof the
agent is to find the stochastic policy µ (·) which minimizes
AI
its expected free energy (EFE). EFE networkby optimizingthe modelmismatch betweenthe
learned EFE value and the reward obtained:
From(5), weknowthattheposteriorbeliefvectorπ(t) can
be exactly inferred using the knowledge of the action A(t),
L (θ )=E G(A(t),π(t−1);θ )+r(t)
observation y and posterior belief vector π(t−1). This EFE EFE EFE
A(t)
relationship, along with the Markov property, enables us to n(cid:0)−G A(t+1),π(t);θ− 2 , (22)
EFE
completely define the generative model using the distribution
φ(A(t),y (t)|π(t−1)). This distribution is where θ− denotes the cu(cid:0)rrent estimate of the(cid:1)n(cid:1)etw o ork pa-
A(t) EFE
rameterobtainedintheprevioustimestepandtheexpectation
φ(y (t),A(t)|π(t−1)) isoverthe actiondistributionA(t+1)∼µ (π(t),·;θ ).
A(t) AI policy
=φ(y |A(t),π(t−1))φ(A(t)|π(t−1)). To update the policy network, we minimize the variational
A(t)
free energy defined in (21):
Thegenerativemodelisbiasedtowardshighrewards,encoded
intothegenerativemodelasthepriorprobabilityofthebelief, F(θ )= µ (π(t−1),A;θ ))
policy AI policy
A∈XP(N)
φ(y (t)|A(t),π(t−1))=σ(r(t)), (19)
A(t) µ (π(t−1),A;θ )
×log AI policy . (23)
where we recall that σ(·) is the softmax function and r(t) σ(r(t))σ(−G(π(t−1),A;θ EFE ))
denotes the instantaneous reward of the MDP at time t. Since r(t) is independent of θ , the loss function is
policy
We nextcomplete the constructionof the generativemodel
by specifying the distribution φ(A(t)|π(t)). Since the agent L (θ )=−H(µ (π(t−1),·;θ ))
AI policy AI policy
tries to minimize the total free energy of the expected trajec-
− µ (π(t−1),A)logσ(G(π(t−1),A;θ )),
AI EFE
tories into the future, it is encoded into the generative model
as
A∈XP(N)
(24)
φ(A(t)|π(t)) =σ(−G(A(t),π(t−1))), (20)
where H(·) is given by (10).
whereG(·)isthetotalfreeenergyoftheexpectedtrajectories Asinthecaseoftheactor-criticalgorithm,the policyto be
into the future, and the variational free energy is the KL followed by the agent is directly obtained from the (policy)
8 IEEESENSORSJOURNAL,VOL.XX,NO.XX,XXXX2024
neuralnetworkoutput.Theagentfollowsthispolicytochoose state given the previous state, action, and observation [29].
an action at every time instant, collects the correspondingre- However, in our case, (19) and (5) define the distributions
ward,andupdatesthetwoneuralnetworksusingtheobtained learned by the observation network and the state transition
reward. The algorithm is summarized in Algorithm 3. network.Hence,theactiveinferencealgorithmcomprisesonly
two neural networks. In other words, the active inference
Algorithm3Activeinferencealgorithmforanomalydetection algorithm naturally allows the algorithm to incorporate any
knowledge of the environment’s statistics into the model.
Parameters: Prior distribution π(0) and confidence level
2) Policy optimization: The actor-critic algorithm uses the
π ∈(0,1]
upper
value functions directly to learn the policy. In contrast, the
Initialization: Policy and EFE network parameters θ
policy
active inferencealgorithm relies on the generativeprobability
and θ arbitrarily
EFE
distribution derived from softmax over the variational free
1: repeat
energyasdefinedin(20).Therefore,theactor-criticalgorithm
2: Time index t=1
maximizestheexpectedrewardfunctioninthefuture,whereas
3: while maxπ i (t−1)<π upper do
i the active inference algorithm reduces the surprise in the
4: Choose action A(t) using the policy derived from
future by learning the probabilistic model. Moreover, the
µ (π(t−1),·;θ )
AI policy objective function of the actor-critic algorithm depends only
5: Generate observations y (t)
A(t) on the samples generated using the actions that the agent
6: Compute π(t) using (5) and r(t) using (11)
took within the episode as opposed to the active inference
7: Update θ EFE by minimizing L EFE (θ EFE ) in (22) algorithm, which averages the objective function over all
8: Update θ policy by minimizing the variational free possible actions in the next step (see the summation in (24)).
energy L (θ ) in (24)
AI policy Since the active inference algorithm computes the expected
9: Increase time index t=t+1
value,itmayleadtoreducedvarianceandbetterperformance.
10: end while
11: Declare xˆ =h k
(i∗)
where i∗ =argmaxπ i (t−1)
12: until i B. ComparisonWithChernoffTest
The Chernoff test, a standard algorithm for active hypoth-
esis testing [5], sequentially chooses actions that build the
posterior belief on the true value of x as quickly as possible.
A. ComparisonWithRLMethods
However, it does not take the sensing cost into account. It
The active inference approach has many similarities to follows the stochastic policy µ ,
Chernoff
RL-based algorithms, such as learning probabilistic models,
T
exploring and exploiting various actions, and efficient plan- µ Chernoff (π(t−1),·)= argmax min q d(x¯(t−1),xˆ),
ning. In particular, the active inference algorithm closely
q∈[0,1]M−1xˆ∈{0,1}M
resemblesthepolicygradientmethods(forexample,theactor-
Piqi=1 xˆ6=x¯(t−1)
(25)
critic algorithm)since both approachestry to learn the policy where we define the i th entry of d(·)∈RM−1 as
directly.Werecallthattheactor-criticandtheactiveinference
methodshavetwoseparateneuralnetworks.Theactornetwork d (x¯(t),xˆ),KL(p(y (t)|x=x¯(t))kp(y (t)|x=xˆ))
i Ai Ai
of the actor-critic algorithm and the policy network of the x¯(t)=h(˜i∗); ˜i∗ =argmaxπ (t).
i
active inference algorithm learn the policy to be followed. In
i
contrast, the other network estimates a function (TD error or
Here, KL denotes the KL divergence between the distribu-
EFE) used to evaluate and optimize the policy. However, the tions, andA denotesthati th elementof setP(N). However,
two algorithms are derived based on different principles, and i
for any xˆ, the KL divergence term is maximized when all
themaindifferencesbetweentheRLframeworkandtheactive
the processes are selected. Thus, we have d (x¯(t),xˆ) ≤
inference framework are as follows: i
d (x¯(t),xˆ) with A = {1,2,...,N} denoting the action of
1) Model-free and model-based: The traditional RL uses 1 1
selecting all processes. Therefore, we arrive at
a model-free approach where the algorithm aims at reward
maximization based on the Q function or the value function, T
max min q d(x¯(t−1),xˆ)
or both. The algorithm does not try to explicitly learn the q∈[0,1]M−1xˆ∈{0,1}M
probabilistic model that governs the state transition or the Piqi=1 xˆ6=x¯(t−1)
generation of the observations of the MDP. However, the ac- ≤ min d (x¯(t),xˆ),
1
tiveinferencemodelreliesonahierarchicalgenerativemodel,
xˆ∈{0,1}M
xˆ6=x¯(t−1)
which is based on variational free energy. It explicitly learns
the model consisting of states, actions, and observations of and equality holds when q = 1 0...0 ∈ [0,1]M−1.
the MDP. Tobe specific, in the mostgeneralsetting,the deep Therefore, the Chernoff test always chooses the action A
1
(cid:2) (cid:3)
active inference algorithm consists of four neural networks: with probability one. This policy is expected as the Chernoff
policy network; EFE network; observation network to learn test does not optimize the sensing cost, and thus, it achieves
the distributionof the observationsgiventhe state and action; a smallstoppingtime whileincurringa highsensing cost.On
andstatetransitionnetworktolearnthedistributionofthenext the contrary, our formulation balances the trade-off between
G.JOSEPHetal.:ANOMALYDETECTIONVIALEARNING-BASEDSEQUENTIALCONTROLLEDSENSING 9
1
0.8
0.6
0.4
0.2
0
0 1 2 3 4 5 6
Time step t
)t(
ytilibaborp
roiretsoP
i
Processes chosen/observations
(1/0,2/0)
(1/0,2/1,3/1
(
)
1/0,3/1)
(3/1) (1/0)
(1/1,2/0,3/1)
x=000 x=100
x=001 x=101
x=010 x=110
x=011 x=111
(a) Actor-critic’s policy when the process state is
[001]
1
0.8
0.6
0.4
0.2
0
0 1 2 3 4 5 6
Time step t
)t(
ytilibaborp
roiretsoP
i
We implement all the neural networks (the Q-network of
deep Q-learning,the actor and critic networks,and the policy
and the bootstrapped EFE networks of active inference) with
three layers and the ReLU activation function between each
consecutivelayer.Toupdatethenetworkparameters,weapply
the Adam Optimizer. Also, we set γ = 0.9 for the RL
algorithms, and ǫ values linearly decrease from 0.4 to 0.05.
We train the neural networks over multiple episodes (real-
izations)where,foreachepisode,wechoosetheprocessstates
from the prior distribution mentioned above, and the number
of time slots for each episode is fixed as 50. The actor-critic
and activeinferencealgorithmsconvergeafter1000episodes,
whereasthedeepQ-learningalgorithmrequires2000episodes
to achieve a stable policy. So, the deep Q-learning algorithm
requiresmoreextendedtrainingthantheothertwoalgorithms.
Processes chosen/observations
(1/1,3/0) (1/0,2/1)
(2/0) (1/0) (2/0)
(1/0,2/0,3/0)
w
th
i
e
t
A
h
v
f
a
t
t
e
w
r
r
i
o
at
t
i
h
i
o
l
e
l
n
u
t
s
o
r
t
a
f
r
i
a
n
t
t
h
i
i
o
n
e
n
g
b
s
e
p
i
l
h
n
ie
a
f
F
se
i
v
g
,
e
.
c
w
2
to
e
.
r
T
t
π
e
h
s
(
e
t
k
y
)
th
s
,
h
e
s
o
e
a
w
n
l
s
g
o
t
o
h
r
r
e
i
s
t
e
h
r
l
e
m
e
a
c
l
s
t
i
.
i
z
o
a
W
n
ti
e
o
A
n
s
(
s
t
k
a
o
)
r
f
t
,
andthecorrespondingobservationsy (k)overtimekuntil A(k)
the stopping time. Fig. 2a shows the sensor selection of the
actor-criticalgorithmwhenthe truehypothesis(processstate)
x=000 x=100
x=001 x=101 is [0 0 1]. Here, the posterior probability corresponding to
x=010 x=110 the wrong process state [0 0 0] was high initially due to the
x=011 x=111 prior distribution. Since the true process state [001] and the
state [0 0 0] differ only in the state of the third process, the
posterior probability corresponding to the true process state
[0 0 1] is not high until the third process is observed at
(b) Active inference’s policy when the process state k = 2. Note that at k = 2, the selected processes and the
is [000]
corresponding observations are described by (1/0,2/1,3/1),
indicating that all three processes have been chosen and the
Fig. 2: A single realization of the variation of the belief
noisy observations are [0 1 1]. As the probability of the
vector π(t), sensor selection A(t), and the corresponding
true process state [0 0 1] increases (at time k = 2), the
observations y (t) over time t. We choose π =0.94,
A(t) upper
algorithm observes the third process more often. Finally, at
ρ = 0.8, and λ = 0.2. The curves represent the evolution
time k =6, the posterior probability of the true process state
of the posterior probabilities of different hypotheses. The se-
[0 0 1] exceeds π = 0.94 and the algorithm stops. We
lected processes(sensors)andthe correspondingobservations upper
can make similar observations from Fig. 2b where the true
at different times are depicted at the top of the figure.
process state is [000]. Due to the error in observations from
the first process at k = 1 and the second process at k = 2,
the posteriorprobabilityof the state [110] increasesinitially.
the stoppingtime and sensing cost via λ and exploitsthe sta-
Then,thealgorithmobservesthese two processesmoreoften.
tistical dependencein x modeled using π(0). We corroborate
Thispolicyallowsthealgorithmtoobservethattheprobability
this point using results in Section VI (e.g., see Fig. 7).
of the state [1 1 0] decreases, and the probability of the true
process state [000] exceeds π at k =6.
VI. NUMERICAL RESULTS upper
Next, we show the performance of the algorithms. Like
In this section, we present numerical results comparing
the training phase, for each episode of the testing phase,
the performances of deep RL and deep active inference we choose the process states from the prior distribution. The
algorithms. We choose the number of processes as N = 3 threeperformancemetricsweuseforcomparisonaredetection
and, thus, M = 2N = 8. The prior probability of a process accuracy,stoppingtime,andtotalcost,asdefinedinSectionII.
being normal is taken as q = 0.8. Here, the first and second Ifthe estimatedhypothesisisthesame asthetruehypothesis,
processes are assumed to be statistically dependent, and the the (instantaneous) detection accuracy is one, and otherwise,
thirdisindependentoftheothertwo.Thecorrelationbetween
itiszero.Also,stoppingtimeistheshortesttimeatwhichthe
the dependent processes is captured by the parameter ρ ∈ stoppingcriteriain(7)ismet.Theaveragedetectionaccuracy,
[0,1]: stopping time, and total cost obtained using the 104 episodes
P x= 0 0 =q2+ρq(1−q) areshowninFigs.3through7.Likethetrainingphase,during
testing for each episode, we choose the process states from
P x= 0 1 =P x= 1 0 =q(1−q)(1−ρ).
(cid:8) (cid:2) (cid:3)(cid:9) the prior distribution mentioned above. In Figs. 4 to 6, we
Also(cid:8), we a(cid:2)ssume(cid:3)(cid:9)that the(cid:8)maxi(cid:2)mum(cid:3)n(cid:9)umber of time slots for also show bar plots where the heights are proportionalto the
each episode (trial or run) is T =5000. fraction of times each process is chosen. In the figures, we
max
10 IEEESENSORSJOURNAL,VOL.XX,NO.XX,XXXX2024
1
0.9
0.8
0.7
0.6
0.5
0.8 0.85 0.9 0.95 1
Confidence level
upper
ycarucca
noitceteD
20
18
16
14
12
10
8
AC: LLR
AI: LLR 6
DQN: LLR
4
0.8 0.85 0.9 0.95 1
Confidence level
upper
K
emit
gnippotS
4
AC: LLR
AI: LLR
DQN: LLR 3.5
3
2.5
2
1.5
1
0.8 0.85 0.9 0.95 1
Confidence level
upper
tsoc
latoT
AC: LLR
AI: LLR
DQN: LLR
1
0.9
0.8
0.7
0.6
0.5
0 0.2 0.4 0.6 0.8 1
Correlation parameter
ycarucca
noitceteD
13
12
11
10
9
8
7
6
AC: Entropy
AI: Entropy 5
DQN: Entropy
4
0 0.2 0.4 0.6 0.8 1
Correlation parameter
K
emit
gnippotS
3
AC: Entropy
AI: Entropy
DQN: Entropy
2.5
2
1.5
1
0 0.2 0.4 0.6 0.8 1
Correlation parameter
tsoc
latoT
AC: Entropy
AI: Entropy
DQN: Entropy
1
0.9
0.8
0.7
0.6
0.5
0 0.5 1 1.5 2 2.5
Tradeoff parameter
ycarucca
noitceteD
10
8
6
4
AC: LLR
AC: Entropy
AI: LLR 2
AI: Entropy
DQN: LLR
DQN: Entropy
0
0 0.5 1 1.5 2 2.5
Tradeoff parameter
K
emit
gnippotS
2.5
2
1.5
AC: LLR
AC: Entropy 1
AI: LLR
AI: Entropy
DQN: LLR
DQN: Entropy
0.5
0 0.5 1 1.5 2 2.5
Tradeoff parameter
tsoc
latoT
AC: LLR
AC: Entropy
AI: LLR
AI: Entropy
DQN: LLR
DQN: Entropy
Fig. 3: Performance of the actor-critic, active inference, and deep Q-learning algorithms for two different reward functions.
Unless otherwise mentioned in the plot, we choose π =0.8, ρ=0.8, and λ=1.
upper
compare the three algorithms (label names in brackets), deep and requires the algorithms to collect more observations
Q-learning(DQN), actor-critic(AC),and activeinference(AI) beforetheydecideonanomalousprocesses.Also,theaccuracy
algorithms considering both Bayesian LLR-based (LLR) and levels achieved by all the algorithms are comparable in all
entropy-based(Entropy)rewardfunctions.Ourobservations the settings because the common π sets the desired
upper
from the numerical results are presented next. confidence level of detection.
1) Confidence level πupper: The variations in the perfor- 2) Correlation parameter ρ: The second row of Fig. 3
mance of different algorithms with π are shown in the illustrates the performances with varying ρ. The accuracy
upper
first row of Fig. 3, and Figs. 4 to 6. All three performance is insensitive to ρ as it is decided by the confidence level
metrics increase with π in all cases. This observation is π . On the other hand, the stopping time and total cost
upper upper
intuitive as a higher value of π implies higher accuracy decrease with ρ. This decrease is expected because when the
upper
G.JOSEPHetal.:ANOMALYDETECTIONVIALEARNING-BASEDSEQUENTIALCONTROLLEDSENSING 11
0.5
0.4
0.3
0.2
0.1
0
AC: LLR AI: LLR DQN: LLR
noitceles
rosnes
fo
noitcarF
8
Sensor 1 7.5
Sensor 2
Sensor 3 7
6.5
6
5.5
5
4.5
4
0.8 0.85 0.9 0.95 1
Confidence level
upper
K
emit
gnippotS
11
AC: LLR 10 AI: LLR DQN: LLR
9
8
7
6
5
4
3
2
0.8 0.85 0.9 0.95 1
Confidence level
upper
tsoc
latoT
AC: LLR AI: LLR DQN: LLR
Fig.4: Performanceof the actor-critic, active inference, and deep Q-learning algorithmswhen the sensing costs differ: c =2
1
and c =c =0.2. We choose ρ=λ=1, p =0.2 for i=1,2,3, and for the bar plot, we set π =0.94.
2 3 i upper
0.5
0.4
0.3
0.2
0.1
0
AC: LLR AI: LLR DQN: LLR
noitceles
rosnes
fo
noitcarF
13
12
Sensor 1 Sensor 2 11
Sensor 3
10
9
8
7
6
5
4
0.8 0.85 0.9 0.95 1
Confidence level
upper
K
emit
gnippotS
4
AC: LLR AI: LLR
DQN: LLR 3.5
3
2.5
2
1.5
0.8 0.85 0.9 0.95 1
Confidence level
upper
tsoc
latoT
AC: LLR AI: LLR
DQN: LLR
Fig.5: Performanceof the actor-critic, active inference,and deep Q-learningalgorithmswhen the flipping probabilitiesdiffer:
p =0.45 and p =p =0.2. We choose ρ=λ=1, c =0.2 for i=1,2,3, and for the bar plot, we set π =0.94.
1 2 3 i upper
0.5
0.4
0.3
0.2
0.1
0
AC: LLR AI: LLR DQN: LLR
noitceles
rosnes
fo
noitcarF
8
Sensor 1
Sensor 2 7
Sensor 3
6
5
4
3
0.8 0.85 0.9 0.95 1
Confidence level
upper
K
emit
gnippotS
10
AC: LLR
AI: LLR
9
DQN: LLR
8
7
6
5
4
3
2
0.8 0.85 0.9 0.95 1
Confidence level
upper
tsoc
latoT
AC: LLR
AI: LLR
DQN: LLR
Fig.6: Performanceof the actor-critic, active inference and deep Q-learning algorithmswhen both sensing costs are different:
c = 2 and c = c = 0.2; and p = 0.02, and p = p = 0.2. We choose ρ = λ = 1, and we set π = 0.94 for the bar
1 2 3 1 2 3 upper
plot.
12 IEEESENSORSJOURNAL,VOL.XX,NO.XX,XXXX2024
1
0.9
0.8
0.7
0.6
0.5
0 0.2 0.4 0.6 0.8
Correlation parameter
ycarucca
noitceteD
10
8
6
4
AC: LLR AI: LLR 2
DQN: LLR
Chernoff
0
0 0.2 0.4 0.6 0.8
Correlation parameter
K
emit
gnippotS
3
2.5
2
1.5
AC: LLR AI: LLR
DQN: LLR
Chernoff
1
0 0.2 0.4 0.6 0.8
Correlation parameter
tsoc
latoT
AC: LLR
AI: LLR DQN: LLR
Chernoff
Fig.7: Comparison of our algorithms with the Chernoff test when π =0.82, λ=0 and c =p =0.2 for i=1,2,3.
upper i i
correlation increases, an observation corresponding to one of cost and flipping probability under three settings: 1) nonuni-
the dependent processes gives more information about the form sensing costs and uniform flipping probabilities (see
other.Consequently,thealgorithmsrequirefewerobservations Fig. 4); 2) uniform sensing costs and nonuniform flipping
andashorterstoppingtimetoreachthesameconfidencelevel. probabilities (see Fig. 5); and 3) nonuniform sensing costs
and flipping probabilities (see Fig. 6) across the processes.
3) Tradeoffparameterλ: The last row of Fig. 3 depicts the
From Fig. 4, the deep Q-learning algorithm is more sensitive
changesin the algorithmperformanceswith λ. As in the case
ofρ,theaccuracyandtotalcostdonotvarysignificantlywith to differentcostvaluesc i . In all settingsconsideredin Fig. 4,
the deepQ-learningagentchoosesthe firstprocessless often,
λ for a fixed value of π and ρ. This behavior is because
upper
when ρ is fixed,we need the same numberof observationsto leading to the lowest total cost and best performance. The
achieve the same confidence level. However, as λ increases, actor-critic algorithm also adapts to the varying cost, while
the active inference algorithm is relatively less insensitive to
each observation becomes costlier, and the stopping time
increases. We notice that the stopping time of the actor-critic the different costs. Similarly, when we increase the flipping
probabilityofthefirstprocessinFig.5(withuniformsensing
algorithm is more sensitive to λ compared to the deep Q-
costs),weseethatallalgorithmsadapttheirpolicies.However,
learning and active inference algorithms. One reason for this
could be that the temporal error, which is a function of only thepolicyofferedbytheactiveinferencealgorithmhasshorter
stopping times than the other algorithms for comparable
theposterior,ismoresensitivetothe parameterλthantheQ-
function learned by the deep Q-learning algorithm and EFE values of the total cost. The differences in the policies of
learned by the active inference algorithm, which are both the three algorithmsare moreevidentin Fig. 6 when we vary
bothsensingcostandflippingprobabilities.Inthissetting,the
functions of the posterior belief and action.
deepQ-learningalgorithmchoosesthe first processless often
4) Rewardfunctions: From Fig. 3, we infer that all the al-
despiteitssmallerflippingprobability.AsinthecaseofFig.5,
gorithmsprovidesimilarperformancelevelswithbothchoices
active inference is more sensitive to the flipping probability
of the reward function.However,the actor-critic and deep Q-
than the cost, and as a result, it gives the shortest stopping
learning algorithms slightly underperform with the entropy-
times at the price of a higher total cost. The performance
based reward function. Since the two reward functions have
ofthe actor-criticalgorithmis betweenthose ofthe othertwo
λkA(t)k in common, as λ increases, the performance differ-
algorithms.Theactor-criticalgorithmprovidesstoppingtimes
ence also grows, as observed from the last row of Fig. 3. In
comparable to those of the active inference algorithm while
other words, the performance gap is the largest when λ=0,
incurring a smaller total sensing cost.
and the two reward functions become identical as λ goes to
∞. Further, we recall from Section III-B that for the same 6) Competing algorithms: We first note that all the algo-
value of λ, the Bayesian LLR reward functions give more rithms have similar detection accuracy due to the common
weighttotheaccuracythanthecost(see(12)).Asaresult,the stoppingcriteriain(7),i.e.,theystoponlywhenthedetection
performance with the entropy-based function for a particular accuracy of the algorithm always exceeds π . So, the
upper
value of λ is similar to that with the Bayesian LLR reward choiceofthe bestlearningalgorithmdependsonthestopping
for a larger value of λ. For example, the sudden change in time and total cost. We first look at the algorithm perfor-
the stopping time of the actor-critic algorithm with λ occurs mancesfortheuniformcostandfillipingprobabilitycasefrom
at λ=0.05 for the entropy-basedfunction,whereas it occurs Figs.3and7.Forsmallvaluesofλ,theactor-criticalgorithm
at λ = 0.2 for the Bayesian LLR-based reward. We observe offers the best stopping time but has a slightly higher cost
similar behavior for the deep Q-learning algorithm as well. than the other algorithms. As λ increases, its stopping time
5) Sensingcostciandflippingprobabilitypi: We analyzethe also increases, and the active inference algorithm provides
dependence of the algorithms’ performance on the sensing the best stopping time for a comparable total cost. Also, the
G.JOSEPHetal.:ANOMALYDETECTIONVIALEARNING-BASEDSEQUENTIALCONTROLLEDSENSING 13
active inference algorithm offers slightly better performance [4] G.Joseph,M.C.Gursoy,andP.K.Varshney,“Anomalydetectionunder
thandeepQ-learning.However,ourexperimentsshowthatthe controlled sensing using actor-critic reinforcement learning,” in Proc.
IEEEInter.WorkshopSPAWC,May2020.
Q-learning algorithm requires more episodes in the training
[5] H. Chernoff, “Sequential design of experiments,” Ann. Math. Stat.,
phase than the other algorithms to achieve a stable policy. vol.30,no.3,pp.755–770, Sep.1959.
ThememoryreplayintheQ-learningalgorithmalsomakesits [6] S. A. Bessler, “Theory and applications of the sequential design of
experiments,k-actions andinfinitelymanyexperiments.partI.theory,”
trainingphasefurtherlongerthantheotheralgorithms.There-
StanfordUnivCAAppliedMathematicsandStatisticsLabs,Tech.Rep.,
fore, the actor-critic algorithm is more suitable for sensing 1960.
cost-critical applications, and for time-sensitive applications, [7] M.NaghshvarandT.Javidi,“Extrinsicjensen-shannondivergencewith
application in active hypothesis testing,” in Proc. ISIT, Jul. 2012, pp.
werecommendtheactiveinferencealgorithmoverQ-learning.
2191–2195.
Next,we lookatthenonuniformsettinginFigs. 4to6.We [8] M. Naghshvar, T. Javidi et al., “Active sequential hypothesis testing,”
notice that the deep Q-learning algorithm is more sensitive Ann.Stat.,vol.41,no.6,pp.2703–2738, 2013.
[9] M. Franceschetti, S. Marano, and V. Matta, “Chernoff test for strong-
to the nonuniform sensing cost, whereas the active inference
or-weakradarmodels,”IEEETrans.SignalProcess.,vol.65,no.2,pp.
algorithm is more sensitive to the nonuniform flipping prob- 289–302,Oct.2016.
ability. So, in the nonuniform setting, we prefer Q-learning [10] B. Huang, K. Cohen, and Q. Zhao, “Active anomaly detection in
heterogeneous processes,” IEEETrans. Inf. Theory, vol. 65, no.4, pp.
for cost-critical applicationsand active inference for stopping
2284–2301, Aug.2018.
time-sensitive applications. These observations further justify [11] D.Kartik,E.Sabir,U.Mitra,andP.Natarajan,“Policydesignforactive
our joint analysis of different learning-based methods. sequential hypothesis testing using deep learning,” in Proc. Allerton,
Oct.2018,pp.741–748.
7) Comparison with Chernoff test: Fig. 7 compares our
[12] G.Joseph,C.Zhong,M.C.Gursoy,S.Velipasalar,andP.K.Varshney,
algorithmswith theclassical Chernofftest. Thestoppingtime “Anomaly detection via controlled sensing and deep active inference,”
andthetotalsensingcostoftheChernofftestarerelativelyin- inProc.IEEEGlobecom, Dec.2020.
[13] G. Joseph, M. C. Gursoy, and P. K. Varshney, “Temporal detection
sensitivetothevariationinρ.Incontrast,ouralgorithms,par-
of anomalies via actor-critic based controlled sensing,” in Proc. IEEE
ticularly the active inference algorithm, adapt their stopping Globecom, Dec.2021,pp.1–6.
time and total sensing cost to ρ. This observation is intuitive [14] ——, “A scalable algorithm for anomaly detection via learning-based
controlled sensing,”inProc.ICC,Jun.2021,pp.1–6.
as the policy followed by the Chernoff test does not depend
[15] G. Joseph, C. Zhong, M. C. Gursoy, S. Velipasalar, and P. K. Varsh-
on ρ or λ, and it assumes that the processes are independent. ney, “Scalable and decentralized algorithms for anomaly detection via
Therefore,(25)leadstotheoptimumperformancewhenρ=0 learning-based controlled sensing,” IEEE Trans. Signal Inf. Process.
Netw.,toappear.
but deteriorates as ρ increases.
[16] T. Yaacoub, G. V. Moustakides, and Y. Mei, “Optimal stopping for
intervalestimationinBernoullitrials,”IEEETrans.Inf.Theory,vol.65,
VII. CONCLUSION no.5,pp.3022–3033,2018.
[17] P.Grambsch,“SequentialsamplingbasedontheobservedFisherinfor-
This paper considered the anomaly detection problem,
mationtoguaranteetheaccuracyofthemaximumlikelihoodestimator,”
where the goal is to identify the anomalies among a given Ann.Stat.,pp.68–77,1983.
set of processes. We modeled the problem of anomaly de- [18] P.J.BickelandJ.A.Yahav,“Asymptotically pointwiseoptimalproce-
dures insequential analysis,” in Proc.Fifth Berk. Symp. Math. Statist.
tection as an MDP problem aiming at the detection accuracy
Probab,vol.1,1967,pp.401–413.
exceedingadesiredvaluewhileminimizingthedelayandtotal [19] T. M. Cover, Elements of information theory. John Wiley & Sons,
sensingcost.Tothisend,wedesignedtwoobjectivefunctions 1999.
[20] R.S.SuttonandA.G.Barto,Reinforcementlearning:Anintroduction.
based on Bayesian LLR and entropy and presented two deep
MITpress,2018.
RL-based algorithms and a deep active inference algorithm. [21] C.J.C.H.Watkins,“Learning fromdelayed rewards,” Ph.D.disserta-
Through simulation results, we compared our algorithms and tion,Psychology Department, King’sCollege, Cambridge, UK,1989.
[22] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
showed that all algorithms perform similarly in the detection
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski
accuracyfor the same confidencelevel. However,the dueling et al., “Human-level control through deep reinforcement learning,”
deepQ-learningalgorithmrequiredamoreprolongedtraining Nature,vol.518,no.7540,pp.529–533,2015.
[23] Z.Wang,T.Schaul,M.Hessel,H.Hasselt,M.Lanctot,andN.Freitas,
phase, and the active inference algorithm is more robust to
“Dueling network architectures for deep reinforcement learning,” in
the trade-off parameter and adapts better to the correlation Proc.ICML,2016,pp.1995–2003.
parameter.Wealsoinferredthatthepolicyoftheduelingdeep [24] A.Ostovar,O.Ringdahl,andT.Hellstro¨m,“Adaptiveimagethreshold-
ing of yellow peppers for a harvesting robot,” Robotics, vol. 7, no. 1,
Q-learning algorithm always led to more negligible sensing
p.11,2018.
costs. In contrast, the active inference algorithm is more [25] W. Kong, W. Krichene, N. Mayoraz, S. Rendle, and L. Zhang,
sensitivetotheflippingprobabilities.Extendingouralgorithm “Rankmax:Anadaptiveprojectionalternative tothesoftmaxfunction,”
Adv.NeuralInf.Process.Syst.,vol.33,pp.633–643, 2020.
to track any changes in the behavior of the processes over a
[26] K.Friston,T.FitzGerald, F.Rigoli,P.Schwartenbeck, andG.Pezzulo,
more extended time period is an exciting future direction. “Active inference: A process theory,” Neural Comput., vol. 29, no. 1,
pp.1–49,Jan.2017.
REFERENCES [27] K. J. Friston, M. Lin, C. D. Frith, G. Pezzulo, J. A. Hobson, and
S.Ondobaka,“Activeinference,curiosityandinsight,”NeuralComput.,
[1] W.-Y. Chung and S.-J. Oh, “Remote monitoring system with wireless vol.29,no.10,pp.2633–2683,Oct.2017.
sensorsmoduleforroomenvironment,”SensorsActuatorsB:Chemical, [28] K. Friston, F. Rigoli, D. Ognibene, C. Mathys, T. Fitzgerald, and
vol.113,no.1,pp.64–70,Jan.2006. G.Pezzulo,“Activeinferenceandepistemicvalue,”J.Cogn.Neurosci.,
[2] A. Bujnowski, J. Ruminski, A. Palinski, and J. Wtrorek, “Enhanced vol.6,no.4,pp.187–214,Oct.2015.
remotecontrol providing medical functionalities,” inProc.Inter.Conf. [29] B.Millidge, “Deep active inference asvariational policy gradients,” J.
PervasiveComput.TechHealthc. Workshops,May2013,pp.290–293. Math.Psychol.,vol.96,p.102348,Jan.2020.
[3] C. Zhong, M. C. Gursoy, and S. Velipasalar, “Deep actor-critic rein- [30] A.Dargazany,“Model-basedactor-critic:GAN+DRL(actor-critic)=>
forcement learning for anomaly detection,” in Proc. Globecom, Dec. AGI,”arXivpreprintarXiv:2004.04574, 2020.
2019.
Process 1
Process 2
.
.
.
Process N
noitubirtsid
tnioJ
Environment
Dynamic
sensor selection
Our learning
algorithm
Noisy sensor
measurements Output
Anomalous
processes’ indices
This figure "jsenga.png" is available in "png"(cid:10) format from:
http://arxiv.org/ps/2312.00088v1

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Anomaly Detection via Learning-Based Sequential Controlled Sensing"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
