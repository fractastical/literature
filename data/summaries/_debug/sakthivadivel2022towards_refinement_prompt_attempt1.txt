=== IMPORTANT: ISOLATE THIS PAPER ===
You are revising a summary for ONLY the paper below. Do NOT reference or use content from any other papers.
Paper Title: Towards a Geometry and Analysis for Bayesian Mechanics
Citation Key: sakthivadivel2022towards
REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Issues to fix:
1. CRITICAL: The current summary has severe repetition issues. You MUST eliminate all repeated sentences, phrases, and paragraphs. Each idea should be expressed only once. If you find yourself repeating content, remove the duplicates entirely. Focus on variety and uniqueness in your wording.
2. Severe repetition detected: Same sentence appears 54 times (severe repetition)

Current draft (first 2000 chars):
Okay, let’s begin.**Towards a Geometry and Analysis for Bayesian Mechanics**This summary extracts key elements from the paper “Towards a Geometry and Analysis for Bayesian Mechanics” by D.A. Sakthivadivel, focusing on the core arguments and findings.**1. Overview**The paper presents a novel approach to understanding complex systems, particularly those exhibiting non-equilibrium dynamics, through the lens of geometry and information theory. It introduces a “constraint geometry” for inference and integration, offering a framework for modelling systems with emergent properties, such as those found in biological and chemical systems. The core of the approach is based on the concept of “free energy” as a measure of the system’s potential to minimise its energy, and the use of Bayesian inference to determine the most probable state of the system.**2. Methodology**The authors’ methodology centres around the concept of a “constraint geometry” – a geometric representation of the constraints imposed on a system’s dynamics. This geometry is constructed by mapping the system’s state space onto a geometric space, where the constraints are represented as geometric objects. The authors utilise the concept of “free energy” as a measure of the system’s potential to minimise its energy, and the use of Bayesian inference to determine the most probable state of the system. The authors utilise a variational approach to minimise the free energy, and the use of maximum entropy to determine the most probable distribution of states. The authors utilise a combination of techniques to model the system, including:***Maximum Entropy:** Used to determine the most probable distribution of states, given the constraints of the system.***Constraint Geometry:** The constraints are represented as geometric objects, allowing for a geometric representation of the system.***Bayesian Inference:** Used to determine the most probable state of the system, given the constraints.The authors utilise a variation...

Key terms: system, systems, geometry, mechanics, theory, analysis, constraints, gauge

=== FULL PAPER TEXT ===
TOWARDS A GEOMETRY AND ANALYSIS FOR BAYESIAN
MECHANICS
DALTON A R SAKTHIVADIVEL
Abstract. In this paper, a simple case of Bayesian mechanics under the free energy
principle is formulated in axiomatic terms. We argue that any dynamical system with
constraintsonitsdynamicsnecessarilylooksasthoughitisperforminginferenceagainst
these constraints, and that in a non-isolated system, such constraints imply external
environmental variables embedding the system. Using aspects of classical dynamical
systems theory in statistical mechanics, we show that this inference is equivalent to a
gradient ascent on the Shannon entropy functional, recovering an approximate Bayesian
inference under a locally ergodic probability measure on the state space. We also use
some geometric notions from dynamical systems theory—namely, that the constraints
constituteagaugedegreeoffreedom—toelaborateonhowthedesiretostayself-organised
can be read as a gauge force acting on the system. In doing so, a number of results of
independent interest are given. Overall, we provide a related, but alternative, formalism
to those driven purely by descriptions of random dynamical systems, and take a further
step towards a comprehensive statement of the physics of self-organisation in formal
mathematical language.
Contents
1. Introduction 2
2. Preliminaries 3
2.1. Bayesian mechanics and the free energy principle 4
2.2. Inference of distributions and of dynamics 10
2.3. Dualities in categories and in inference 13
3. Constraints as specifications of system-like states 16
4. Optimisation as self-evidencing 19
4.1. Geometry and analysis 28
4.2. Technical remarks on random dynamics 31
4.3. On the idea of ‘as if’ inference 35
5. Maximum entropy as the description of a field theory 36
6. Constraints as gauge symmetries 37
6.1. A review of gauge theory 37
6.2. Sample paths in stationary stochastic processes 39
6.3. Regarding non-equilibria 43
7. Conclusion 46
References 46
Date: 27th April 2022.
2020 Mathematics Subject Classification. 37K58, 37L40, 51P05; 46N55, 60K35, 70S15.
1
2202
rpA
52
]hp-htam[
1v00911.4022:viXra
2 DALTONARSAKTHIVADIVEL
1. Introduction
Thetheoryofcomplexsystemsis,bymostaccounts,inaGalileanage. Whilstavisionary
himself, to whom the shadows of classical mechanics and relativity were known, Galileo’s
physics was pre-Newtonian. He made observations, and ascribed to them theoretical ideas
as a modelling effort—but, the higher principles organising the phenomena he observed
were inaccessible to him, as neither the calculus nor the geometry to fully explain them
were known.
Comparably, many examples of complex systems are known to us, and there are effect-
ive, if phenomenological, accounts of their dynamics. Biological systems are a particular
exampleofthechallengeofstudyingcomplexadaptivesystems, whereinwearelearningin-
creasingly more about underlying mechanisms—why things behave the way they do—but
still know little about organising principles—why things exist the way they do. What
largely does not exist yet is a formal approach to the theory of complex systems. This
is despite the fact that there are physical foundations for complexity, found primarily in
statistical physics, and mathematical foundations for physics, found primarily in geometry
and analysis.
Here, we wish to investigate what this foundational picture might look like. There is
a fairly regular assignment of data to any physical theory, consisting of a principle being
operationalised by a field,1 a field being modelled by a geometry, and some mechanics as
theconfinementofafieldtotheworld-lineofasystem. Afieldtheoryisthusathingwhich
takesaprincipleandproducesamechanicaltheory(aprescriptionofsomedynamics)when
restricted to an individual system, rather than an entire field. So, if we wish to develop
a formal theory of complex dynamics, we might begin by finding the principle and the
accompanying field theory that produces such dynamics, and seeing what axioms follow.
Recentworkhassuggestedthattherightprincipleisaninferentialone, establishedasan
account of what it must mean to self-organise—and thus, to remain far from equilibrium,
whence most all features of complexity arise. In particular, the loop between formal math-
ematics and physical dynamics suggested above has already been partially closed by the
freeenergyprinciple(FEP)forstochasticdynamicalsystems,appealingtoamathematical,
functional formulation of what complex systems do, using the fundamental optimisation
principles known to underlie most of physics [RBF18, Fri19, MSB21, FCS+22]. This is an
excellentexampleofsuchamathematicaltheoryofcomplexity; evenso,therearequestions
surrounding the core articulation and accomplishments of the FEP, many of which remain
unresolved (see [AMTB22] for a key paper in this regard). Laying a geometric founda-
tion for parts of the FEP, and providing a collection of formal structures surrounding it,
motivates the results described here.
Inspired by the revolutionary treatment of physical ideas by formal mathematics, this
paper will lay out the beginnings of a theory formulating FEP-theoretic ideas in the lan-
guage of differential geometry, functional calculus, and category-theoretic elements. An
emphasis is placed on building the manner in which one might give a formal, mathemat-
ical delivery of the free energy principle, as a physical idea relevant to dynamical systems
exhibiting features of complexity.
1By field, we mean an assignment of some quantity to points in some base space—for instance, values
of an electromagnetic force at points in space, generated by electric charges, comprise a classical electro-
magnetic field. In quantum electrodynamics, this field consists of photon states at every point in space.
TOWARDS A GEOMETRY AND ANALYSIS FOR BAYESIAN MECHANICS 3
We begin by giving some remarks about alternative formulations of the free energy prin-
ciple—in particular, in Section 2.3, we sketch out how the free energy of a system’s beliefs
is the dual function to the entropy of a system’s states. We give the context for looking
at entropy in Section 3; there, we introduce the idea that constraints in maximum entropy
subsume the role of system-ness in the free energy principle, witnessing the equivalence
betweenthetwoprinciples. FarandawaythelongestsectionisSection4, whichestablishes
formally that constrained maximum entropy is an equivalent language in which to discuss
the free energy principle. By transforming the free energy principle into a statement about
the maximisation of self-entropy, we are given an opportunity to ground the FEP in some
conventional mathematics and physics. The remainder of that section makes use of this
fact to prove older, and newer, statements about the FEP.
In the following sections, we use the principle of maximum entropy to establish a field-
theoretic language for the free energy principle. In Section 5, we discuss how maximum
entropycanbethoughtofastheactionofasemi-classicalfieldtheory,andinparticular,one
thatinteractswithagaugefield. ThesecondmajorsectionofthepaperisSection6, where
we discuss previous results on a gauge symmetry in maximum entropy, and show in detail
how this is like a gauge force shaping probabilistic dynamics. Important results are given
in Section 6.3, concerning how this formalism sheds light on the nature of non-equilibrium
steady states.
More specifically, we will elaborate on three key ideas, rephrasing the ideas about infer-
ence put forth by the free energy principle in geometric and analytic terms, and suggesting
a formal, proof-based path towards Bayesian mechanics:
(1) approximate Bayesian inference is equivalent to the maximisation of entropy under
a particular constraint (Theorems 4.1 and 4.2),
(2) this constraint serves as a potential for a system whose description is given by a
gradient ascent on entropy (Theorems 4.3 and 6.2), and
(3) such a constraint shapes the dynamics of an inferential process in the same way as
a gauge field interacting with a matter field (Theorem 6.1).
Alongside the existing frameworks surrounding the free energy principle, these results
move us towards a formulation of the physics of complex systems and non-equilibrium
randomness, and advance a view of living things as field theories.
Acknowledgements. The author is grateful for discussions with the following people:
Miguel Aguilera, Mel Andrews, Lancelot Da Costa, James F Glazebrook, Alex B Kiefer,
MagnusKoudahl,MichaelLevin,BerenMillidge,andJeffYoshimi. Theauthoralsothanks
the members of the VERSES Research Lab for valuable conversations. Special thanks are
owed to Christopher L Buckley and Karl J Friston, and in particular, the author thanks
Maxwell J D Ramstead.
2. Preliminaries
This paper aims to be at least partially self-contained, and so, overviews of both max-
imum entropy and the free energy principle are given below. An account of gauge field
theory and its mathematical objects is largely left to other resources, with some slightly
un-pedagogical commentary in Section 6. Good references include [RW02, Nak03, Wit18].
We assume—without much loss of generality—that the systems we discuss and their state
4 DALTONARSAKTHIVADIVEL
spaces are low-dimensional and free of singularities, but not necessarily linear. In key
analysis-oriented sections, we assume the underlying noise process is a Wiener process,
with some implicit regularity of the operators defined. Throughout, we suppose that we
can fully or effectively characterise what it means for a system to be a given system, in the
sense of what properties it satisfies, what states it can occupy, and what sorts of dynamics
it possesses. Whilst only accessible in principle, invoking the existence of a well-defined
‘system-ness’ stipulating the essential properties of a given system is a necessary basis for
any formal investigation of this topic.
2.1. Bayesian mechanics and the free energy principle. Self-organisation is the
notion that complex systems, especially adaptive systems, seem to exist in stable regimes
of states enforced by the very repertoire of dynamics that appear complex—i.e., non-linear
or chaotic—fashioning a certain order out of disorder. In particular, many features of
complexity can be understood as emerging from a sort of synergy between the components
of a complex system and their dynamical couplings (see [Hey08] for a review). First
described in 2006 as a framework in which to understand perception and representation
in the human brain as Bayesian estimation [FKH06, Fri10], the free energy principle is an
accountofhowcomplexsystemsbecome, andstay, organised. Itrecapitulatesseveralolder
theories pertaining to perception, learning, autopoiesis, morphogenesis, and the physics of
life [FCS+22]. At first only being applied to the human brain, it has since been expanded
to cover a much broader domain, including physical systems in general. It is posited as
a comprehensive theory of non-equilibria in stochastic dynamics [Fri19, PDCF20]. Good
overviews of the principle are found in [Fri12] and [RBF18], laying out its mathematical,
as well as conceptual, claims; this mathematical content is later examined in detail in
[Fri19], especially its third section, and exemplified in a mechanical construction found
in [DCFHP21]. Additional worked examples, with commentary on the implied physical
dynamics, exist in [AMTB22] and [FHU+21]. An extension of the inference encoded in the
FEP to more dynamical situations is effectively summarised in [RKF20]. The most recent
comprehensivereviews(atthetimeofwriting)oftheFEParecontainedin[And21,MSB21,
FCS+22]. Our primary references are [Fri19] and [DCFHP21]. This work is in some sense
a sequel—or perhaps more properly, a prequel—to those two papers, specifically. More
than a response to critics of the FEP, it is an exercise in clarifying the formal structure
and mathematics of the FEP.2 In effect, we do not aim explicitly to defend the FEP, nor
even to patch it—instead, we assume it is correct, and aim to elevate it, by going back to
basics.
The FEP is motivated by an attempt to say what it means for a system to be a system,
in the sense of being and staying a cohesive whole. The consequence of this, that an
organised system ought to be—and remain—independent of its environment, is where the
FEPbeginsitswork. Assuch,itisnotexclusivelyatheoryofnon-equilibria,butatheoryof
howstructurespersist. Inparticular, itisatheoryofwhatitlooksliketoremainorganised
in a stable fashion on some time-scale despite the assault of entropy, and a prescription
of what the dynamics of such a system must look like. Whilst this is most informative in
the case of complex non-equilibrium systems, whose structures actively resist equilibrium
drives, any system with some conception of distinct internal and external states satisfies
2Atthetimeofwriting,severalresponsesto[AMTB22],akeyexpositionofweaknessesintheformulation
presentin[Fri19],aretoappearinvolume41ofPhysicsofLifeReviews. Assuchwedonotaddressspecific
points of critique contained in [AMTB22]; this will occur in those responses.
TOWARDS A GEOMETRY AND ANALYSIS FOR BAYESIAN MECHANICS 5
the FEP in principle. This includes essentially anything that is a stable physical structure
(i.e., not a quantum vacuum), and excludes only those idealised systems which are isolated
from their environment or embedded in no environment. More pertinently, any structured
system which has not lost its structure and joined its environment via some decay can be
modelledasresistanttoentropy, evenifthisisonlyvalidonsomeverybrieftime-scale, and
cannotbereasonablycalled‘resistance’inanenactive3sense. Thedualviewpointthatany
system which is embedded in an environment will reflect the statistical structure of that
environment via couplings between the two is equally valid, and seems just as tautological.
These points are elaborated on in Remark 2.1 and Section 2.3, respectively.
We assume a system with internal states µ is situated in an external environment with
its own set of states η, and couples to this external environment via dynamical interactions
across some interface between the two. We further assume that this interface consists of
‘blanket states’ b, which are nothing but a distinct subset of system states called a Markov
blanket,interactingdirectlywiththeenvironment. Finally,wesupposethereisaninjective
function σ : µ (cid:55)→ η relating a given pair of internal and external states across a shared
b b
blanket state. We model the noise inherent in these states as some stochastic differential
equation, such that fluctuations in these states are possible; thus, in particular, σ maps
an expected internal state given a blanket state to the expected external state given that
blanket state, and can be thought of loosely as injective on pairs b (cid:55)→ (µˆ ,ηˆ ).4 Moreover,
b b
interactions of the system with external states are observations of the outcomes of random
variables satisfying a stochastic dynamical system, with hidden generative processes.
It is important to the chain of reasoning in the FEP that a system interfaces with
causalfactorsaffectingitsstates, butcannot‘know’thosefactorsdirectly, andsoinfersthe
externalstatessuperveningonitsowninternalstates.5 Theseinferencesmeanthatinternal
states carry beliefs about the environment, by reflecting the optimal structure mapping
onto a particular suite of environmental states; this is true of any embedded dynamical
system in at least a trivial sense [VBM22]. Suppose that the expected external state ηˆ
is a sufficient statistic for the density describing the environment, or, some variational
approximation thereof. We say the states internal to the system play the role of encoding
probabilities of external states, such that internal states parameterise a probability density
q(η;σ(µˆ))ofthelikelyexternalstatescausingparticularinternalstates. Thisdensityneeds
to match a true or optimal belief over states, p(η,µ,b), which occurs when µ = µˆ such that
q(η;σ(µˆ)) = q(η;ηˆ) = p(η,µ,b) [DCFHP21]. From the viewpoint of an agent, it acquires a
semantic theory about the content and meaning of external states, which it uses to parse
sensorystreams—i.e.,howtheenvironmentactsonit[RFH20]. Whenthisdensitymatches
the actual probabilities over external environmental states, the system is in harmony with
its environment, and as such—in the enactive case—can respond to and resist the sort of
environmental fluctuations that cause system dissipation.
The relationship between internal states, a ‘recognition density’ q(η;σ(µˆ)), and an op-
timal belief model of the environment, p(η,µ,b), is enough to cast the FEP as a theory
3Roughly, actively or with intention, cf. [RKF20, RFH20]. We do not discuss explicit connections to
enactive cognitive science here.
4Note, however, that σ is not constructed as a tuple-valued function in [PDCF20] or [DCFHP21], and
it would be a type error to say so; indeed, it is sufficient that b(cid:55)→µˆ and b(cid:55)→ηˆ be injections separately,
b b
precisely because σ factorises. See Lemma 4.3 for more on this point.
5Orcanbemodelledassuch. Theentirediscussionrevolvesaroundthisdistinction,anotalwaysexplicit
assumption about the normativity of the FEP; see [And21] for an account of this problem.
6 DALTONARSAKTHIVADIVEL
of inference. Interestingly, in even trivial cases of structures at equilibrium, we have all
the ingredients for the FEP: internal states caused by external states, boundaries across
which these interactions occur, and most importantly, the statistical structure of the en-
vironment reflected in the internal states of the system. We give a case of this in Example
2.1. This suggests we should be able to extract some insights about truly complex systems
from much simpler ones in a well-defined fashion, which is a central motivation for our
approach—ultimately, we seek to extrapolate insights about the kinds of systems the FEP
aims to describe, from a discussion of simpler systems and the more rigorously understood
mathematics of those systems.
Wecannowofferashortlemmaaboutthe‘mode-matching’behaviourintheFEP,which
was defined above via the synchronisation map σ.
Lemma 2.1. A system is optimal when it occupies the expected internal state for a given
blanket state.
Proof. Suppose the system embodies beliefs about its environment, and carries a recog-
nition density such that a belief about the probability of an external state, q(η), is con-
ditioned on a particular internal state q(η | µ). We claim a system is able to maintain a
non-equilibrium steady state if and only if q(η | µ = µˆ ) such that q(η;σ(µ)) = q(η;ηˆ ),
b b
which equals p(η,µ,b) by assumption. (cid:3)
We will sometimes denote q(η | µ) as q (η) to emphasise that we are conditioning on
µ
an event, rather than a random variable, in the interest of using it as a parameter for
the sufficient statistics of q. This notational choice also appears in the FEP literature, for
presumably similar reasons.
Importantly, the proof in Lemma 2.1 is purely heuristic: our assumption in the final
line cannot be met, since p(η,µ,b) is formally quite different from q(η;ηˆ). Nonetheless, we
begin what is now identified as the FEP by defining the (also heuristic) free energy:
Definition 2.1. Free energy is the quantity
(1) E (cid:2) ln{q(η | µ)}−ln{p(η,µ,b | m)} (cid:3) ,
q
which is in fact the positive relative entropy between two probability densities over external
states, where p(η,µ,b | m) is the true joint density over internal states, observations,
and their causes, and the density q(η | µ) is the belief carried by the system, describing a
couplingbetweenexternalandinternalstates. Theformof (1)islikenedtoaKLdivergence
between the two densities, which is minimised when q(η | µ) = p(η,µ,b | m) almost surely.
In this case, we condition the density p(η,µ,s) on the existence of a random dynamical
system m, entailing a model of how blanket data is related to external states. This could
be thought of as a representation of the blanket, or the model induced by the blanket
[Fri12]. Note that the expectation is with respect to the density q(η | µ). Due to linearity
in the expectation operator, (1) takes the form of an internal energy less an entropy (cf.
cross entropy), and hence is an analogue of thermodynamical free energy. Indeed, it is
the energy ‘available’ to the system to enact changes in itself or the environment, and
much like a thermodynamical system, stable configurations are tautologically free energy
minima, where there is no work to be done. That is, when the free energy of the system
approaches zero from above, the system goes from being changed by the environment to
being stable. Thus, free energy can be seen as minimised with respect to internal states.
Furthermore, the system can decrease free energy by changing its environment, to change
TOWARDS A GEOMETRY AND ANALYSIS FOR BAYESIAN MECHANICS 7
the surprisal of blanket states. This comprises a physically-motivated connection of the
FEP to organisation and self-organisation. The ‘internal energy’ in F is the average of the
intrinsic surprisal of systemic states, so that the surprisal of any such set of states admits
an interpretation as a sort of configuration energy. This quantity must be matched by the
belief about that state.
We emphasise that the expression in Definition 2.1 is not truly a thermodynamical free
energy, but is generically a control-theoretic or information-theoretic quantity [UDCF21,
And21]. We also emphasise (once more) that it is not formally meaningful, but is useful as
a motivation for the entire FEP: like other variational inference methods, (1) is supposedly
controlled by how optimal the system’s dynamics are. In particular, the internal state
µ—which, we remind ourselves, is in fact an internal state given a blanket state—is a
control parameter for (1). Suspending our disbelief temporarily, (1) is identically zero if µ
is the expected internal state given that blanket state, µ = µˆ . The blanket dependence of
b
this relationship is a subtle but important feature of the FEP, sometimes obscured by the
shorthand notation used here and in other papers. It allows us to define the variational
free energy, an approximation to (1) which does make sense:
Definition 2.2. The variational free energy arises from a factorisation of (1), relying
on an expansion of the joint density p(η,µ,b | m) into p(η | µ,b,m)p(µ,b | m). Using
properties of the logarithm and noting that −ln{p(µ,b | m)} is independent of η, this
results in
E (cid:2) ln{q(η | µ)}−ln{p(η | µ,b,m)} (cid:3) −ln{p(µ,b | m)},
q
such that the KL divergence defined in the first term is a variational approximation to
(1). The full expression is equivalent to (1), and we refer to the divergence herein as the
variational free energy.
ItisimportantthatwetakethesteptoformulatethefreeenergyintermsofaproperKL
divergence—also called a relative entropy—for various results here. We lose no generality
in doing so. Note that in the literature, the full expression above is sometimes called the
variational free energy, and thus, so is (1). We use the terminology indicated in Definition
2.2 for the purposes of results like Lemma 4.2.
Suppose the blanket consists of sensors of some sort. It is apparent that the vari-
ational free energy of a system’s observations bounds the surprisal of those observations
and resultant internal states, −ln{p(µ,b | m)}, from above. Conceptually, the free energy
principle can be summarised by the fact that the surprisal of observed states of the en-
vironment—given the model of the environment, q(η | µ), encoded in internal states—is
minimised when that model is close to an actual model of the environment. Thus, the
principle that ‘self-organising systems minimise their free energy’ is the principle accord-
ing to which systems remain unsurprised by observations—in other words, that changes in
the environment are predictable, carry little to no new information, and are not allowed to
threaten system integrity. Indeed, the expected internal state minimises free energy pre-
cisely by mapping to the expected external state, as above. In doing so, we conclude that
the probability measure on the state space remains localised around preferable states, the
average of which is the expected internal state given some relationship between external
and internal states, and take this as self-organisation.
Insummary: mathematically,theminimisationoffreeenergyminimisesinternalentropy,
and thus maximises log-evidence, for the system’s own states. It is in this sense that
the FEP is essentially a self-evidencing [Hoh16] of the current and continued existence
8 DALTONARSAKTHIVADIVEL
of a ‘thing,’ since a consequence of the maximisation of self-evidence is that the non-
equilibrium probability density over system-like states is resistant to dispersion. That is
to say, things that continue to exist—away from equilibrium, in particular—are resistant
to dispersion, and must therefore be maximisers of their self-evidence. More generally
than that, the surprisal given above is a measure of blanket (and thus system) integrity,
in that unexpected changes to blanket states register as surprising—as such, minimising
the surprisal of blanket states means the blanket separating internal and external states
remains in place.
The active qualities of some of this language obscure the tautology at the heart of their
statements, which are generalised from equilibrium systems with structure to describe
systems with a stronger sense of adaptivity. Many systems do not assume a model of
how the environment causes their states, per se, but simply embody evidence of their own
existence in virtue of existing [RFH20]. In fact, this is consistent with the simplification
to the equilibrium case. Equilibrium systems with definite structure evidence by reflecting
theirenvironment,whichoccursduetothefreesupervenienceofenvironmentalstatisticson
internal states at equilibrium—many such systems will even have states of activity, when
those internal states affect external degrees of freedom. In that case, trivially, internal
states will on average couple to the average external state, but one could describe the lack
of adaptivity in these systems as a result of an extremely poor (i.e., inexpressive) model
of the external causes of blanket states.6 On the other hand, systems that remain at non-
equilibrium for long periods of time need to ‘know’ what equilibrium is to avoid it—or,
know what fixed point their environment will force them towards, in order to self-evidence
by avoiding the changes to internal states intended by the environment. Such systems
maintain a model, but return to an equilibrium structure, if they fail to minimise free
energy for a given set of beliefs. This loss of agency could be described as inertness. In
living systems, it is a state known as death. See Theorem 4.4 for more on the ‘passing
down’ of Markov blankets.
The notion of iterated inference as a proxy for changing statistics, such as non-adaptive
systems that fail to evidence after a change in the environment, is mentioned in several
places. Identifying goal-directed actions with changing the environmental states determ-
ining the likelihood of internal states, these actions are an exercise in the preservation of
a structure identified with existing internal states. Separately, those internal states can
change, to conform or be described by a density of minimum free energy. We will go on to
say that the latter is performing inference against a set of constraints on possible internal
states.
Remark 2.1. States of activity play an interesting role in the context of the passing down
of Markov blankets, underlying a distinction that will be used later in our framework.
In standard treatments of the FEP, blanket states are typically separated into sensory
and active states, coupled to internal and external states in such a way as to preserve
conditionalindependencies. Viaactivestates, thesystemcanchangeenvironmentalstates.
As mentioned, many systems (like a stone irradiating heat) will have trivial active states,
and so the distinction between adaptive and non-adaptive systems is more like how those
active states are—or are not, respectively—used to maintain a given structure in the face
of a dynamic environment. On the other hand, having well-defined internal states entails a
6Forexample,astoneirradiatingheatinthesunmayonlyhaveoneinternaldegreeoffreedomtracking
external states: its temperature. In the form of synaptic weights, a human may have millions.
TOWARDS A GEOMETRY AND ANALYSIS FOR BAYESIAN MECHANICS 9
definition of the system’s structure and key properties, and is enforced by sampling from a
steady state density of minimum variational free energy; this description fits both adaptive
and non-adaptive systems, as mentioned before. We speak more about the example of a
stoneinExample2.1,whichwerefertoasaninertsystem—i.e.,asystemwithnon-adaptive
action states.
Again, despite the active language used, inert physical systems do not actually calculate
derivatives. As such, many principles of optimisation—including the least action principle,
the principle of maximum entropy, and the FEP—instead proceed variationally, such that
theoptimisationbeinginvokedcanbeattributedtoagradientflowwhichmightrealistically
be implemented by a simple object. Many systems—a diffusing particle, for instance—are
describedwellbysomequantitywhichalwaysdecreasesalongthespatiotemporalevolution
ofthesystem. Aposthoc interpretationofthisphenomenonisthatthesystemiscalculating
and then flowing towards the stationary point of that quantity, but the truth is much
simpler: this quantity is a Lyapunov or H-function that controls the dynamics of the
system by a basic and quite natural minimisation process—or even more simply, varies in
concert with these dynamics in an insightful way.
As a result, the probability distribution encoded by internal states must flow towards
that over external states by some sort of loss-driven dynamics. This is usually spoken
of in terms of the KL divergence between the two distributions, which—in the enactive
case—propagates some sort of error signal to the rest of the system, motivating a self-
correction.7 For inert systems, the moment error begins to propagate is the moment that
structure ceases to exist. For adaptive systems, this gradient flow is the mechanical theory
induced by the principle that systems minimise (1), in that free energy decreases along the
dynamics of a complex system. Indeed, the tendency to minimise free energy is formally a
Lyapunov function that any Bayesian mechanical system flows along.
Example 2.1. Consider a stone as an example of an inert FEP system, having an organised
crystalline structure, but being at equilibrium and not being enactive. The stone has no
dynamics—there are no goal-directed states of activity originating from internal states,
such as rolling away from a threat. Likewise, the stone cannot adapt its internal states to
reflect changes in the environment—a stone will never soften itself to withstand blows of
the hammer and remain a stone. All the same, so long as it exists, the stone does minimise
its free energy from time-point to time-point, and evidence follows from existence. Here,
the internal states of the stone parameterise a set of possible external states (the ambient
temperature,forinstance),whereexternalstatescauseinternalstatesandare‘observed’by
the cortex of the stone. Following Remark 2.1, the existence of this structure is equivalent
tosystem-ness,buttheinertnessofthestoneprecludesanyenactiveoradaptiveproperties.
In the context of the above example, after being crushed, the description of the stone
goesfromacrystallinestructuretoapowderofmineral-basedparticles. Eachsuchparticle
has its own internal states and environmental boundary, and so the FEP still applies. This
is what we refer to as iterated inference, which accounts for the changing statistics of a
model of a failed system. Forthcoming work by Friston, Da Costa, and Parr8 re-derives
7Under a notion of system-ness as consisting of the key properties and states that make a system,
identifying ‘entering constrained states as drifting away from allostasis’ with ‘propagation of error by
dopaminergic signalling’ recovers the implementation of the FEP in the human brain known as predictive
processing [Fri10]. Parallels to other theories like interoception [ALPF19] are also obvious.
8See “the free energy principle: particular kinds and strange things,” in preparation.
10 DALTONARSAKTHIVADIVEL
essential results from the FEP in the absence of Markov blanket assumptions, where the
distinction we have made between what sorts of partitions—active, sensory, coordinated
activity, purely environmental—are possible is also crucial. Like these results, in order to
speak of the internal states of something like a stone, it is necessary to specify what sort
of content those states could realistically have under the FEP.
Bayesian mechanics should be regarded as a consequence of the free energy principle,
in the same sense as classical mechanics is a consequence of the least action principle.
Principles are prescriptions of how some theory ought to look; mechanical theories are
consequences of a principle, which in turn give us the dynamics of a specific system when
applied to that system. In this sense, they are synonymous, as a system minimising its free
energy will exhibit Bayesian mechanics. This usage of the term is somewhat idiosyncratic,
but is consistent with [DCFHP21] and [Ram21], for example.
2.2. Inference of distributions and of dynamics. By fashioning a model of a process,
inferential things can either consider the probability of an observed state, invert it to
estimate the true state in the absence of noise, or more dynamically, solve the master
equation associated to a stochastic differential equation. Maximum entropy is a procedure
for inference which posits that the best model for any data, and hence the best model of
(the outputs of) a process generating that data, is that which maximises the differential
Shannon entropy subject to prior constraints J(x) on the possible probabilities of states
[Jay57, JB03, PGLD13b]. Here, by a maximum entropy ‘model’ we mean a probability
density encoding the statistics of the underlying noisy dynamics as though a sampling
process. This type of model is thought of momentarily as being distinct from what is
supposed by the free energy principle, but we will see later that they coincide in a precise
sense.
Definition 2.3. Let γ(x,t), denoted γ for brevity, be an X-valued random variable sat-
t
isfying some stochastic differential equation, with labelled instances γ = x . Moreover, let
t t
the process in question generating γ be stationary, such that γ is distributed like p(x) for
t t
every t, and let J : X → R be a scalar measurable function. The Shannon entropy is the
action functional
(cid:90) (cid:18)(cid:90) (cid:19)
(cid:88)
(2) S[p;J] = − ln{p(x)}p(x)dx− λ J (x)p(x)dx−C ,
k k k
X X
k
where any C = E[J (γ )] such that the final term is zero.
k k t
In the remainder of this paper, we will forego notating sums, assuming J is a linear
combination of J ’s when necessary. We may also write (2) as
k
(cid:90)
S[p;J] = − ln{p(x)}p(x)+λJ(x)p(x)dx+λC
X
to emphasise that there exists a Lagrangian
(cid:0) (cid:1)
(3) L = ln{p(x)}+λJ(x) p(x)
in this action functional, of the more general form
(cid:0) (cid:1)
L = f(φ)+V φ
for φ, f(φ), and V an arbitrary field, function, and potential, respectively. This is used to
effect in Sections 4 and 6, in that it provides a direct analogy to the variational calculus
of classical mechanics under the least action principle.
TOWARDS A GEOMETRY AND ANALYSIS FOR BAYESIAN MECHANICS 11
Constraints are usually derived from external knowledge of the data, and often cor-
respond to known values of observables quantities. Inferentially, a constraint is both a
function on states weighting the relative probability of a state, and, an expression of some
knowledge of what this process ought to look like, as a rule that we know the data must be
consistentwith. Hence, aconstraintisnothingbutasynthesisationofrelevantinformation
from the data, guiding the likelihood of a given solution. This is reflected in the fact that
maximum entropy without constraints is a knowledge-less density, the uniform density,
whilst constrained maximum entropy produces a density with no information other than
the constraints. Indeed, a crucial aspect of maximum entropy is that a constraint can be
read as a prior specification of what states are likely [Sak22a].
Due to the way in which (2) is defined, the constraints on states J(x) become moments
that the model must satisfy. For instance, suppose we know the mean of the random
variables we have observed, such that the density describing Y must have expectation
(cid:90)
xp(x)dx = C.
X
Here, the constraint on a state is proportional to the state, J(x) = x. Maximum entropy
can then be constrained such that generated p(x) must produce this given expectation and
must be normalised. Let C be the rescaled constant λ−1C arising from the Lagrange mul-
λ
tiplier in (2). The density generated for this constraint is the exponential distribution with
mean C , given by C exp{−C x}. Indeed, the probability of x decreases as x increases
λ λ λ
in this density, consistent with the constraint function. Denote by J(x) = j the value of
J on a state x. Maximum entropy provides us the fact that p(x) is a model of the data,
and more strictly, becomes a generative model conditioned on the constraints on states,
p(x | J = j). In fact, it is not just a generative model, but the maximum likelihood model
of γ given the constraints. Note again that we can take constraints as an expression of
the preferability of a particular state, rather than data about the probability density itself;
hence we can make sense of p(x | J), if J(x) is structured as some observed data about x.
Throughout,wemakeanimplicitdistinctionbetweeninferenceanddynamicalinference.
Inference, broadly construed, is the process of finding a model of some data; this is either
in the sense of the distribution generating this data, or the distribution from which some
process generating the random variables being observed samples. Dynamical inference, on
the other hand, ought to fashion a model of the data-generating process as a dynamical
system, giving probabilities over the evolution of data—sample paths, in other words. In
virtue offocussing ona simple caseof the theory, i.e., stationarilysystem-likesystemswith
only brief comments on actions, we discuss primarily distributional inference, and possibly
iterated distributional inference to encode changes in statistics over time.
Correspondingly, we implicitly refer to the following lemma throughout the paper:
Lemma 2.2. A process samples from a steady state density if and only if its statistics are
stationary.
Proof. Suppose there exists a density p(x,t) describing the system. Both statements are
equivalent to ∂ p(x,t) = 0. (cid:3)
t
AconsequenceofLemma2.2isthat,allelseequal,thedifferencebetweenanequilibrium
and non-equilibrium steady state consists of strictly physical desiderata (e.g., the presence
of detailed balance and the nature of the underlying energy flows). Indeed, detailed bal-
ance is a stronger condition on the existence of a probability density than stationarity,
12 DALTONARSAKTHIVADIVEL
and it is easy to construct stationary Markov processes with asymmetric transitions. On
the physical side, maximising constrained entropy is not truly maximising entropy, leaving
the possibility of extracting further work from the system by dissipating it. This differ-
ence in entropy is measured precisely by the free energy, the difference between a density
exp{−V(x)} and a true p∗(x), as suggested earlier by formulating a free energy as a KL
divergence (Definitions 2.1 and 2.2).
Remark 2.2. By a consequence of de Finetti’s theorem, when the statistics of a process are
stationary on some relevant time-scale, the two notions of distributional and dynamical
inference coincide on that time-scale. In particular, in the stationary case, each random
variable in a temporally-indexed sequence of random variables samples from the same
distribution.
Using Lemma 2.2 and Remark 2.2, we take stationary processes as a sufficient funda-
mentalmodelofanFEPsystem, asanyprocesswhichiscontrolledisstationary, andhence
has a steady state dictated by the FEP. In fact, these conditions are also necessary, which
is discussed in Section 4.2. The stability of internal variables in the presence of fluxes at
non-equilibrium has been proposed as a hallmark of self-organisation in complex systems
in previous literature (see [Pok20] for an extended overview in the context of interacting
thermodynamical systems), and the principle of maximum entropy can be employed to
describe stable systems in non-equilibrium settings (see [End17] for an explicit construc-
tion of such a model and related discussion, [ME15, ME17, HZE17] for justifications of
coarse-graining non-equilibrium steady state dynamics, or Theorem 6.2 in this paper). As
such, the resultant ‘effective equilibrium’ of controlled dynamics with a non-equilibrium
steady state is used to justify focussing on the simple case of maximum entropy; this is
takenasopposedtomaximumcalibre,amoreappropriategeneralisationtonon-stationary,
non-equilibrium, path-dependent systems. Such a generalisation is left to future work. We
are able to speak about specific flow properties of some non-equilibrium systems in Sec-
tion 6.3, but focus strongly on grounding current results and attitudes within the FEP in
equilibrium-like cases where we can ignore underlying flows.
If the sufficient statistics µˆ of q(η | µ), which engage in the mode-matching beha-
viour described in Section 2.1, are fixed, then the density prescribed by the FEP is a
stationary density with no goal-directed action component. This precludes a description
of intentionally evolving systems, as suggested above. Action can now be introduced as
non-stationarity—a temporally-indexed change in these statistics deriving from a change
in the matched external state. This is a signature of an iterated maximum entropy pro-
cess, describing a system whose steady state might suddenly change, rather than a highly
controlled process. Note that this non-stationarity appears to include adaptive systems
and inert systems, both of whose embodied evidence changes when the system changes
(undergoes changes in, respectively) its form. This means we have partitioned the class of
systems into four subclasses under the FEP, two of which are stationary (experience no
effective energy flows), and two of which are adaptive (have active states), where no such
subclass is empty.
Example 2.2. We provide examples of the sorts of systems contained in each subset here.
(1) A stone at a single point in time is a stationary, inert process.
(2) A control circuit is a stationary, adaptive process.
TOWARDS A GEOMETRY AND ANALYSIS FOR BAYESIAN MECHANICS 13
(3) A stone over multiple points in time is a non-stationary, inert process.
(4) A life-like system is a non-stationary, adaptive process.
The difference between stationary and non-stationary adaptivity (Examples 2.2.2 and
2.2.4) is whether or not the attractor characterising system-like states wanders: is the
system changing its environment to maintain a fixed, stable regime of states, or, does it
change its statistics in concert with the environment? In contrast, the difference between
inertandadaptivestationarity(Examples2.2.1and2.2.2)iswhetherthesystemisresisting
equilibrium drives or not. We have also introduced systems which are non-stationary and
inert. We could contrast Examples 2.2.3 and 2.2.4 by asking whether any changes in the
system originate from the system, or if they are all environmental. These are the sorts of
systems that the FEP only applies to vacuously—dissipating systems that change because
they can’t help themselves—but which nonetheless can be discussed in the language of
the FEP, because on some time-scale they appear like Example 2.2.1. The difference
between the two could be characterised as a sort of meta-stability, or stability in the face
of environmental perturbations predicated on appropriate active responses.
Concomitantly, we take the position that the FEP—as written, or perhaps re-written,
here—speaks of system-ness in the broadest sense possible, covering things which are both
atandoutofequilibriumfromtime-pointtotime-point. WefocusonsystemslikeExample
2.2.1,generalisingtosystemslikeExample2.2.2wherepossible. Asstated,thepointofthis
istosuggestwhatmathematicallywell-understoodfeaturescanbeexpectedofmoregeneral
systemsstill,likethoseofExample2.2.4. SincesystemslikeExample2.2.2—otherexamples
include systems like Turing patterns, reaction-diffusion systems, and the morphogenesis of
differentiating cells—can be regarded as performing inference [KFGL20, FGL21], termed
‘cognition in unconventional substrates’ in [Lev22], this is an approach that offers ready
insights to life-like systems. Moreover, as we will discuss in Section 3, even living systems
like humans have certain static bounds on what constitutes such a system. This should
allow us to fashion models of life-like things using the formalism defined here, albeit coarse
ones which may not be very informative.
2.3. Dualities in categories and in inference. An adjunction, as found in category
theory, istheexistenceofapairofmapswhichareadjoint; thatis, oneisdualtotheother.
Dualisation maintains key intrinsic properties of objects, but reverses the directions of the
relationshipsbetweenobjects. Dualmapsarethuspreciseopposites: eachmapisoneoftwo
sides of a shared coin. The relationships encoded by adjunctions are of critical importance
in mathematics, especially via the application of universal properties, and most every non-
trivial mathematical structure arises from a duality of this sort [Law69, Lan98, Awo06].9
In this spirit, within the free energy principle there is a symmetry between a system
and the environment embedding it, referred to as ‘synchrony’ across a Markov blanket
[FCS+22]. This is evident in the way environments tend to model agents just as well as
agents model their environments, a kind of niche construction [CRV+18]. Moreover, the
FEPnaturallyinducesaso-called‘dualinformationgeometry’bydefiningthesimultaneous
evolution of internal states and external states [PDCF20]. This allows us to speak intelli-
gently about the agent in the context of the coupling between agent and environment, or
moregenerally,aboutagentandenvironmentassystemsrelatedbyacoupling(see[RFH20,
9Indeed, the stated slogan of [Lan98] is “adjoint functors arise everywhere.” Mac Lane also makes the
claim that “all concepts are Kan extensions,” which come in left and right dual pairs.
14 DALTONARSAKTHIVADIVEL
section4.2]). Anon-trivialformulationofthebidirectionalityofsuchinteractions(oftenfo-
cussingontheflowofrelevantquantities)hasbeenessentialtootherrigorousdiscussionsof
opensystems[Ros58a,HY10,HY11,Sie18,Pok20,VBM22], especiallythoserootedincat-
egorytheory[Ros58b,Fon16,BFP16,BM20,Cou20,CGHR21,Smi21,BGMS21,BCG+21],
suggesting it should play a key role in understanding the FEP. In fact, formulating sys-
tems with bidirectional information flow necessitates an explicit adjunction between two
interactingagents—acovariantandcontravariantpairofmorphisms[BHZ19]—furthersug-
gestingthatweshouldexplorethissymmetryfromwithintheFEPasadualisationofagent
into environment.
In summary, the fact that a Markov blanket encodes a kind of adjunction—between
whichstatestrackwhich—suggeststhatagiven(outward-orinward-facing)dynamicacross
aMarkovblanketisultimatelythedualofsomeother,adjointdynamic,describedbyfacing
in the opposite direction. The content of this paper is in large part preoccupied with what
this dual perspective has to say about the problem of self-organisation, and what value it
may have as a technical and conceptual tool.
The FEP works by focussing not on the system, per se, but on the coupling between
the environment and the system, depicting what environmental forces look like from the
perspective of the system.10 The FEP has a natural control-theoretic interpretation as a
statement that the dynamics of a system can be described as responses to perturbations
from the system’s surrounding environment (this is present throughout the literature, but
is arguably exemplified best in [MTSB20], a detailed account of the similarities between
control and temporally-extended FEP-type inference), with a system’s dynamical reper-
toire being more akin to a set of tools for coping with a variable environment, rather than
a defining functional or phenotypical feature of the system. It is a consequence of this
framework, favouring the viewpoint of the environmental states being predicted or reacted
to, that the dynamics of a system can be understood as minimising free energy. In other
words, the FEP is the principle prescribing what the dynamics of a complex system ought
to look like, in that the maintenance of a steady state far from equilibrium requires an
active flow against some gradient. Under this framework, the system embodies or even
carries a generative model of environmental states, simply by representing what it is to
exist in a given environment. In this sense, embedded dynamical systems in general are
like agents of inference: we can, in principle, ‘read off’ the beliefs about the environment
encoded by the states of a system, and thus, the semantic content stored in the internal
states of the system (see [VBM22] for an example). This is another facet of the same
symmetry: as outside observers situated in the environment surrounding the agent, we
can fashion a model of how the agent behaves, which is dual to the model carried by the
agent about how the environment behaves. Dualisation introduces the agent as an entity
which exists from the point of view of the universe, by asking what it means for an agent
to be a cohesive region of states distinguished from its surroundings. This idea is intro-
duced formally in Section 3 as a set of constraints on what the system can be, such that
10It would be appropriate to note here that a Markov blanket is not strictly a separation of the system
fromitsenvironment,despitewhatappearancesmighthaveusthink. Rather,itisacouplingbetweentwo
distinctsubsetsofanentirelocaleofstates,withonebeingacohesivewholethatwecallthesystem. This
distinction is critical to the philosophical effectiveness of the FEP, in that it does not falsely presuppose
that open embedded systems make themselves into closed systems; this is not even possible in principle,
and in fact, the FEP leverages this non-closure to define a mechanical theory predicated on the coupling.
See also Theorem 4.6.
TOWARDS A GEOMETRY AND ANALYSIS FOR BAYESIAN MECHANICS 15
constraints enforcing a systemic ideal are adjoint to a system having ideal explanations of
its environment. Being equivalent to the principle of maximum entropy as it was described
in Section 2.2, this move allows us to speak about constrained self-entropy rather than the
free energy of beliefs. In this sense, dualisation is little more than a technical tool that
relates the FEP to the entropy functional, but it offers an attractive alternative viewpoint
on the FEP.
Spelling out the formulation of this adjoint pair in greater detail, we have the following:
take a set of possible parameter values for q(η;µ) equivalently as defining a conditional
distribution q(η | µ). Formally, the surprisal is an evidence bound for the model q(η |
µ). That expected internal states minimise (1) prescribes that non-equilibrium systems
must create and climb positive probability gradients, thereby staying self-organised by
assuming a model concordant with what it is to exist in a particular external environment.
This is the density p(η | µ,b) that FEP-type systems embody by way of internal states,
where those states model some belief about external states via a map across the Markov
blanket; it corresponds to estimating likely external states given what the system looks
like, which is how the system remains system-like in the face of a changing environment.
The minimisation of variational free energy is precisely the leveraging of this map to make
embodied beliefs model true descriptions of the environment.
This says little of self-evidencing, though, and could (albeit na¨ıvely) be construed as the
opposite: we understand systems more directly as things which model their environments,
rather than things which collect evidence about themselves; it is only the fact that the
latter is a consequence of the former in the variational case of the FEP that evinces self-
evidencing. Moreover, there is immediately a suggestion that the Bayesian inversion of
p(η | µ,b) is the more canonical viewpoint. In a sense, we are secretly asking about what
a system probably looks like, or is allowed to look like under a mutual understanding of
the defining qualities of that system, conditioned on what the environment looks like—and
this is p(µ | η ), where we have absorbed blanket states into internal states. From a certain
b
pointofview,this iswhatreallydefinesself-organisation,inthatwearedirectlyestimating
the probability density of states given the aforementioned external perturbations to those
states. Initially, it seems like this flips the entire story on its head. Now, we are interested
in the dynamical repertoire of the system for what it says about the system, whilst we are
no longer interested in the generative model encoded by the system. We have changed
our perspective to focus directly on what the system looks like from the perspective of
environmental forces. And yet, in both contexts, the tendency to minimise free energy is a
means to understand system-ness, whether by our own model or the system’s model. The
fact that the minimisation of free energy is recognisable in both ‘narratives’ is, in fact, a
key feature of the FEP. Our intuition is clearly wrong: this new interpretation is not so
radically different from the original, even if its mathematical formulation has reason to
change.
To determine what this dual formulation—focussing on what internal states look like
from the perspective of the bath they are situated in—ought to look like, we can exploit
the symmetry of the Markov blanket. The adjunction between external states supervening
on internal states, and internal states responding to external states, is a very literal—in
the sense of Bayesian inversion over a Markov blanket—duality between system and envir-
onment. This paper takes the route of formally establishing that maximising constrained
entropy over internal states tends to minimise free energy, such that localising the probab-
ility density around optimal states implies the minimisation of the free energy of internal
16 DALTONARSAKTHIVADIVEL
states. We are able to prove this largely by relying on that duality, the foundation for
which is established in the following section.
3. Constraints as specifications of system-like states
Asectiondevotedtothespecialroleconstraintsplayinself-evidencingiscontainedhere,
preempting the technical step that makes many of these results possible: passing from free
energy to self-entropy, which requires us to formulate the problem in terms of constraints.
Constraints are most easily understood in terms of preferences for certain internal states,
perhaps reflective of the naturally adjoint structure of the problem.
We are principally interested in what the evidence of self-ness is, or, what is the actual
definition of a system that a system can be understood as meeting by existing. For a
system to be a system means that it obeys its own ‘system-ness’—whilst initially nonsense,
what this means is that there are a small number of characteristically system-like states,
which are occupied with high probability, such that the system is and remains whatever
sort of system it is. This is what we refer to using terms like organisation and self-
organisation—that the system exists means that it remains in a regime of states which
give it the properties it has. This is what we term an ‘ontological potential,’ defining a set
of states with key properties. Byfalling into an ontological potential, a system is organised
into occupying system-like states.
In turn, this is quite closely connected to the fact that observations of unexpected en-
vironmental states can cause transitions to unpreferable internal states, compromising the
system’s system-ness. In adaptive systems, self-evidencing entails enforcing this distinc-
tion—and thus persisting on a long time-scale—by modelling the environment embedding
the system, and then responding to those changes. Even in the adaptive case this contains
the rather more trivial, special case that systems specialise to exist in the environment
they exist in. This is certainly a tautology, and itself includes the simple structure-at-
equilibrium case, where a system takes on the statistics of its environment via uninhibited
energy flow. As in Example 2.1, a stone does not self-evidence, but it does evidence, in
virtue of existing with a ‘model’ that captures the free supervenience of external states
on its own states. It would begin to evidence an entirely new set of states were it to get
smashed to pieces (again, we point the reader to Theorem 4.4 for a formalisation of this).
Essential to the definition of system-ness is obviously a set of constraints being placed
on what it means to exist as a thing. For humans, this consists of allostatic set-points
comprising healthy system dynamics—the amount of energy, fluids, and waste product in
the body, for instance. These shape the dynamics of the thing and enforce system-ness
when they are obeyed—eating when hungry, staunching bleeding when injured, excreting
waste when it builds up. Constraints can thus be read as preferences, in that states with
low constraints are ‘good’ states for the system. Given some notion of system-hood—i.e.,
the key variables being constrained—we can think of constraints as inducing an attractor
for the system, in that they define a region of states that a system can sample from or
occupyandstillbelikethatsystem(undersomefundamentalnotionofwhatsortofsystem
it is). The constraints on the system are the ontological potential defining how we might
model a system. Crucially, it is also not necessary to have the entire essence of a thing
written down; we need only define a collection of key internal state variables which are
independent of key external state variables. These include what are termed ‘existential
variables’ in [And21], which manifest self-evidencing by a sort of contradiction—these are
TOWARDS A GEOMETRY AND ANALYSIS FOR BAYESIAN MECHANICS 17
the variables for which the system surely dissipates if not kept in allostatic bounds, or in
other words, any variable whose obedience of some bound is necessary in order for the
system to be defined. The constraints on these quantities constitute the potential, and the
maintenance of their values in a regime which is amenable to self-organisation is allostasis.
As seen in maximum entropy, constraints on what states are accessible to a system
enforce a probability density taking into account the relative probability of such states.
Hence, any state sampled by the system can be understood as occuring with a probability
dictated by the constraints on that system—definitionally, systems do not sample non-
systemlikestates,buttheremaybesomeflexibilitywithintheallostaticboundsmentioned
before. Maximising entropy thus gives us a route to claim that internal states model
external states, contingent on pre-specifying the desired internal states constraining the
system.
TheviewpointofconstraintsdemandssomethingofareinterpretationoftheFEP,which
discards the more computationally useful Markov blanket, invokes some platonic repres-
entation of what it is to be a given system, and then claims that we can understand a
systems’s system-ness in light of maximising entropy against system-ness. This becomes
evidencing, in that we begin with what the system is and ask how it is that way—or self-
evidencing in the adaptive case, where we begin with what the system wants to be and
ask how it gets there. In other words, ours is consistent with the recent, more deflationary
view of the FEP, in that we presuppose what the system ought to be and then describe
how it attains system-ness [RFH20, And21].
Remark 3.1. It is interesting that many such constraints will be variance constraints, or, a
J function which is quadratic in x, in the sense that states which are too high or too low
ought to be penalised equally under the definition of a controlled system. Occupying these
states, which lie outside of some sort of set-point, for long periods of time would certainly
violate negative feedback principles in systems biology [Wie48]. In this case, the resulting
maximum entropy probability distribution would be of a Gaussian shape, and the idea of
‘peaking’aroundsystem-likestatesistakenliterally. Thislendsaconcrete,controlsystems
interpretation to the observation that increasing the variance of p(x) and sampling more
surprising states is the dispersion of the system’s system-ness. The physical role of free
energy minimisation as a definition of system identity is also discussed in detail in [Kie20].
MuchhasbeenmadeoftheMarkovblanketutilisedintheFEPliterature(see[RVB+21]
for a detailed critique). The viewpoint emphasising constraints appears to avoid some of
these problems, at the expense of being less useful for general calculations. It will be
apparent in Theorem 4.2 that the Markov blanket plays at most an auxiliary role, and the
constraintsimplyaboundarybetweensystemandenvironmentalstateswithoutreferencing
the statistical structure of either. This is, in fact, an advantage to our approach: consider
that anything which persists as a thing falls vacuously under the FEP, and that Markov
blankets ought to be just as fundamental as a result. Note that in the following, we mean
well-defined in the technical sense of a property which is independent of some choice.
Proposition 3.1. There is no well-defined, maximal proper subclass of stationary embed-
ded systems with the property that each such system is equipped with a Markov blanket.
Proof. We suppose that every embedded system either has a Markov blanket or does
not, and assume there exists a generic thing with fixed internal states which admits a
representation of those states that is not conditionally independent of external states.
18 DALTONARSAKTHIVADIVEL
In other words, we assume a distinguished subclass of blanketed systems exists, which
is neither empty nor equal to the entire set of stationary embedded systems. Now, for
that generic thing, choose a partition of the statistically fixed internal states into blanket
states. By including sufficient information in the form of latent blanket states to predict
internal states independently of external states, a Markov blanket can be constructed for
that system. The claim follows by contradiction. (cid:3)
Hence, for any collection of internal states—even a stone—we may construct a Markov
blanket on those states. This proposition is a special case of the more general fact that
any system can be made Markovian by conditioning states on adequately many hidden
variables. This may be one explanatory variable or many; either way, conditional inde-
pendence is somewhat arbitrary, and some sort of blanket can be constructed for arbitrary
systems.11 Now, note how we began this line of reasoning: declaring there exist distinct
internal states within some bath characterising the system of interest implies a coupling
that automatically leads to some manner of conditional independence. As such, constrain-
ing the system to remain in a certain regime of internal states that allows those states to
cohere and persist is equivalent to asking that a Markov blanket exist between internal
and external states.12
It must be stated again that this restores the validity of the FEP as a foundational
approach to every ‘thing,’ since it is in fact the case that distinct internal states of any
kind ought to be sufficient for a boundary of some sort. As a further remark, note that (2)
has no preferred length- or time-scale—since the action is scale-free, the theory applies to
the dynamics of most any system. The Markov blanket construction itself is scale-free, as
has been described in [FFZ+21] using a renormalisation-type method; dually, any system
can certainly be constrained to be system-like.
On the other hand, it preserves the fact that the FEP is not so general as to be useless.
WhilstthesetrivialMarkovblanketsexist, theyarenothinginteresting. Inertsystems, like
the crystalline structure of a stone, are not controlled (see Example 2.2.3), and so do not
truly have a Markov blanket. Proposition 3.1 is the sort of sleight of hand that allows us
to talk about a simplified version of the FEP; in reality, the FEP has a far less trivial heart
that applies only to very particular systems with complexity on the order of Examples
2.2.2 or 2.2.4. The Markov blanket is thus more special than Proposition 3.1 would lead
one to believe.
An interesting case study for this construction, and in particular, the fact that gener-
alisations of constraints generalise blankets, is that of a flame. Consider one of the key
issues raised in [RVB+21]: that of a ‘wandering blanket.’ This is the idea that an evolving
definition of a system, especially a system whose states fluctuate too quickly to maintain
the renormalisation-type relationships yielding the statistical independencies defining the
11Despiteappearingtobeanesotericstatement,itisactuallyfairlystraightforward: allnon-Markovian
dynamicsadmitarepresentationasahiddenMarkovmodel,andonceweincludehiddenvariablesasstate
information,thesystemismadeconditionallyindependentofsomeotherquantity. Agoodexampleisthat
internal states are independent of external states when conditioned on the variable of control. Since we
assume control equates to the presence of a Markov blanket, stationarity implies a Markov blanket.
12Note how this proposition does not tell us how to find the Markov blanket for any system, nor if
it is meaningful from the point of view of physical calculations—Proposition 3.1 is merely an existence
proof,asopposedtoanalgorithm. AchoiceofwhatcountsasausefulconditionfortheMarkovblanketis
the element of well-definiteness given in the statement, in that more restrictive ideas of a blanket can be
introduced that carry stricter conceptual content.
TOWARDS A GEOMETRY AND ANALYSIS FOR BAYESIAN MECHANICS 19
Markov blanket, fail to accomodate a Markov blanket. The example often used is a flame,
which converts itself from a fluctuating plasma-like substance to carbon gas too quickly
to have a blanket. However, in the process of discussing why the Markov blanket fails, we
have just sketched out what the flame is and what the flame is not. Evolving systems more
generally have a formal home in the notion of dynamical constraints, given that internal
states can be defined or approximated by a dynamical system at some scale of observation
with some salient state variables, no matter how quickly that changes. We might, for
instance, model a flame’s essential flame-ness as the maintenance of a temperature above
the ignition point of the substance being burned, a non-zero flux of oxygen around the
material, as well as the occurrence of some particular chemical reactions related to com-
bustion, without any one of which there is no flame. These are the existential variables for
the flame. Suppose the flame gets snuffed out; we have moved the flame to non-flame-like
states, and this can be regarded as a failure to evidence. The fact that the flame is not
adaptiveiscapturedbytheflamenotself-evidencing, andsimplybeing evidentbyexisting.
That is to say, it does not enforce its own existence. The fact that the state of the flame
fluctuateswithintheseconstraintsiscapturedbythefactthatthemaximisationofentropy
around said constraints provides a certain amount of exploration in the state space. In
this case, maximum entropy is a post hoc description of the system’s continued existence,
rather than a mechanism for self-preservation, so to speak.
A key to the distinction above is that we don’t need to consider the difference between
evidencing and self-evidencing: from the perspective of the environment, these look the
same. Inbothcaseswearereferringtoacoherentcollectionofthingswhichtogetherdefine
a particular system. In the absence of perturbations from the environment, these notions
coincide. Much will be made of this mental framework for the internal states of an agent,
in that this view of constrained system-ness underlies the entire construction in this paper.
Dynamical constraints are not discussed here, but follow from the framework laid out,
and—conjecturally—solve issues related to non-ergodicity in itinerant systems with chan-
ging blankets. See Theorem 4.5 for the relationship between constraints and ergodicity,
and Theorem 6.1 for the relationship between constrained ergodic densities and attractors,
which together suggest that dynamical constraints will yield a rigorous notion of wander-
ing attractors. The adjoint view—that systems can apply dynamical constraints to their
models of the environment—has been described in the literature before [Yos11], and may
be related to the expected free energy construction [PF19, PDCF20, MTB21]. As such,
this is a promising future direction for the modelling of genuine non-equilibria.
4. Optimisation as self-evidencing
The content of this section first establishes how the principle of maximum entropy sub-
ject to a particular constraint is equivalent to the free energy principle, in that it leads
to self-evidencing of self-organising systems (or an evidencing of organised systems) and
induces an information geometry on a system’s possible models. The equivalence between
these theories is best understood in light of constraints on the possible states of the sys-
tem, motivating the dualisation described in Sections 2.3 and 3. We go on to leverage
this new construction, using maximum entropy to discuss some formal mathematics for
self-organisation in Sections 4.1 and 4.2. In Section 4.3, we zoom back out, to discuss what
maximising entropy entails in a philosophical, agent-based context.
20 DALTONARSAKTHIVADIVEL
Recall that a maximum entropy probability density is a representation of its constraints
(see Section 2.2). We assert the following claim:
Claim 1. The right interpretation of the FEP is that non-equilibrium dynamical systems
are data-generating processes whose data is the maximum likelihood model of the system
given the constraints on what that system can be. This likelihood is self-evidence, and
maximising likelihood is self-evidencing. As such, FEP-type systems maximise constrained
self-entropy.
This claim leads to various features of complexity, by way of non-equilibrium dynamics,
purely as a result of representing and responding to constraints. It can be read as a
statement about nestedness—anything that exists, exists in some environment, and is
a microcosm of that environment in virtue of reflecting just what it is to exist in that
environment. As such, we can model a system as a thing in an environment which is
constrained not to join that environment.
Motivating Claim 1 is the observation that, as well as its important roles in probability
andphysics, themaximisationofentropyunderliesmuchofmachinelearning. Featuringin
Boltzmann machines [KOG+19] and forms of energy-based learning [LCH+07, FCAL16],
physical descriptions of inference lend themselves to entropic descriptions in particular.
The‘metalogical’aimofthispaperisalsotoshowthattheFEPistruewhenevermaximum
entropy under a particular constraint is true, simultaneously demystifying it and giving it
a solid foundation.
Expounding upon Claim 1, we obtain the construction we need, as follows. Recall that
minimising free energy implies the presence of an attractor of states—a region where the
probability of occupying any state contained therein is high. Saying the dispersion of
this probability density is low, and hence it is low entropy, is ultimately misleading: in
reality, we maximise entropy under a certain constraint against dispersion, leading to a
high probability of occupying states in that region. Moreover, minimising free energy does
not construct that attractor explicitly, but returns the internal states that parameterise an
optimal belief about the environment.
For these reasons we pass to constrained entropy over beliefs and then constrained self-
entropy, a transformation happening in two steps: first, we equate the minimisation of
free energy to the maximisation of a particular entropy functional, and then we use the
adjunction sketched out in Section 2.3 to change our perspective. This will allow us to
relatetheself-evidencingaspectsoftheFEPtotheimplicitattractoronstates, and, allows
us to invoke results on the entropy functional to understand the FEP.
We denote by q (η) the conditional density q(η | µ). To begin, observe that maximising
µ
thefirsttermin(1)minimisestheoverallexpression. Thismotivatesustochoosetheq (η)
µ
forwhichS[q (η)]isgreatest,undertheconstraintthatq (η)characterisesitssurroundings
µ µ
well.
Proposition 4.1. For a fixed density p(η,µ,b | m), the minimisation of (1) occurs if and
only if S[q (η)] is maximised under the constraint that the optimal belief about the agent-
µ
environment loop is on average unsurprising, J(η) = −ln{p(η,µ,b | m)} and E [J(η)] = 0.
q
Proof. Negating (1) and rewriting it as
(cid:90) (cid:90)
− ln{q(η | µ)}q(η | µ)dη+ ln{p(η,µ,b | m)}q(η | µ)dη,
TOWARDS A GEOMETRY AND ANALYSIS FOR BAYESIAN MECHANICS 21
bythelogicof (2), wehaveamaximumentropyproblem. Sincemaximisingthisexpression
is equivalent to minimising its negation, we have the intended result. (cid:3)
It should be noted that we could have constructed this relationship via the Legendre
transform, but that this is largely unnecessary. A particular statement of one direction of
Proposition 4.1 should be made:
Corollary 4.1. Maximising S[q (η)] under J(η) = −ln{p(η,µ,b | m)} implies the optimal
µ
probability density q∗(η) is equal to p(η,µ,b | m).
µ
Proof. This follows from maximum entropy, which yields q∗(η) = exp{−J(η)}. (cid:3)
µ
AconsequenceofProposition4.1isthatminimisingparticularfreeenergyaswrittenne-
cessarilyimpliesthatinternalstatesmodelexternalstates. Likewise,thisuniqueconstraint
issufficientforamaximumentropydensitytodescribeasystemmodelledbytheFEPinthe
context of particular free energy, in that it implies the expected surprisal of external states
under q is zero. Both are summarised effectively by Corollary 4.1, q∗(η) = p(η,µ,b | m),
µ
which is the core of the FEP. In [Sei12] this constraint is an effective potential for a non-
equilibrium system, which is explored more in Section 6.3.
ThehypothesisinCorollary4.1isaparticularspecialcasewherethesystemisinperfect
harmony with its environment. A consequence of this corollary is that only obtaining an
identically zero minimum for (1), and the surprisal, recovers strict equality. In [Fri19], it is
stated that this constraint is not always satisfiable in a strict sense, in that p(η,µ,b | m) is
not always known.13 Moreover, the attentive reader will note that our foregoing discussion
(especially Definition 1) defined a relative entropy functional for densities with differing
supports, which formally can be nothing more than a heuristic. Observe that (1) factorises
into
(4) F(µ,b) = E (cid:2) ln{q(η | µ)}−ln{p(η | µ,b)} (cid:3) −ln{p(µ,b)},
q
implying that, in fact, the free energy is the surprisal when the bound vanishes. This
observation is why the FEP takes the view of bounding surprisal by minimising free energy
variationally, rather than what is supposed by Corollary 4.1, per se. In addition to the
connection between free energy and surprisal being more evident, (4) is well-defined on
states η, since supp(q) ⊆ supp(p) when we take parameter values as conditional events.
So, for both technical and conceptual reasons, we postulate a variational approximation of
p(η,µ,b | m), where the approximation is controlled by the parameter of internal states.
We refer to the above functional in the ideal case as the particular free energy. For more
on the distinction, see sections four and six of [MSB21].
Lemma 4.1. The minimisation of free energy takes place if internal states model external
states.
Proof. Assume b is a Markov blanket between µ and η such that p(η | µ,b) = p(η | b),14
and let µˆ = E [µ | b] (likewise for η). As stated, we also postulate that internal states
b p(µ|b)
parameterise external states by mapping to a sufficient statistic for p(η) given a value of b,
such that p(η | b) is modelled by q(η;σ(µˆ )) under σ(µˆ ) = ηˆ . This is sufficient for all of
b b b
13From now on we will neglect to write the model m, taking its existence as implicit everywhere in
virtue of the system being defined.
14This is given in [DCFHP21].
22 DALTONARSAKTHIVADIVEL
the results shown—observe that if σ(µ) is chosen to match the parameters of p(η | b) and
thus p(η | µ,b), then the KL divergence term of (4) is identically zero. (cid:3)
We also prove the following important result about internal states, elaborating on
Lemma 2.1 and completing Lemma 4.1. By proving that µˆ is the minimiser of variational
b
free energy, and bounds the surprisal, we can go on to prove the approximate Bayesian
inference lemma given in [Fri19].
Lemma 4.2. Taking the expected internal state as the ideally free energy minimising state
yields
E [µ | b] = µˆ = argmin[F(µ,b)],
p(µ|b) b
µ
b
such that the variational free energy F˜(µ,b) = E (cid:2) ln{q(η | µ)}−ln{p(η | µ,b)} (cid:3) bounds
q
both the free energy and the surprisal from above.
Proof. Since we assume the particular free energy F (µ,b) reduces to the surprisal under
q∗
µ
a control parameter µ, we have
F˜(µ,b) ≥ F(µ,b) ≥ F (µ,b),
q∗
µ
which in fact follows from (4). The intended result then follows from the Bogoliubov
inequality. (cid:3)
Lemmas 4.1 and 4.2 suggest internal states ought to be chosen such that q(η | µ) models
p(η | µ,b), making µ a control parameter for the agent-environment loop (given a map
σ(µˆ ) = ηˆ ). The mathematical theory of variational approximation has been discussed
b b
previously by the author in [Sak22b], where a proof of the role the Bogoliubov inequality
plays in mean field theory appears. The above inequalities are typically strict—we do not
observe identity unless the surprisal is zero, in which case F˜(µ,b) = F(µ,b) ≥ 0, or q is
optimal, in which case we have the reduction mentioned before.
Importantly, Lemma 4.2 states that—to learn the causes of internal states and bound
the surprisal of those states, it is sufficient to do variational inference. This is a key result
in [Fri19], shown there in a similar fashion. We now prove that there is an equivalent
statement in the context of constrained entropy:
Proposition 4.2. Under the constraint that external states are on average as informat-
ive as blanket states, Corollary 4.1 holds for (4) such that q∗(η) = p(η | µ,b), which is
µ
approximately p(η,µ,b).
Proof. Let J(η) = −ln{p(η | µ,b)} and E [J(η)] = −ln{p(µ,b)}, such that the expected
q
surprisal of external states given internal and blanket states is equal to the surprisal of
internal and blanket states. This yields the expression
(cid:90) (cid:18) (cid:90) (cid:19)
− ln{q(η | µ)}q(η | µ)dη− − ln{p(η | µ,b)}q(η | µ)dη+ln{p(µ,b)} ,
recovering (4) after negation. By the same argument as that of Proposition 4.1, this leads
to maximising entropy, which yields q∗(η) = p(η | µ,b) as in Corollary 4.1. (cid:3)
µ
Remark 4.1. Iftheblanketconsistsonlyofsensors,thisistheconstraintthatexternalstates
contain no new information on average, which is equivalent to q (η) explaining external
µ
states. Moreover, this optimal sampling is equivalent to a Markov blanket remaining in
place, in virtue of self-organisation under those explanations; this is apparent in that J(η)
TOWARDS A GEOMETRY AND ANALYSIS FOR BAYESIAN MECHANICS 23
is also directly equivalent to a bound on the average surprisal of blanket states, and thus,
implies some average structural integrity.
We can now prove the approximate Bayesian inference lemma of [Fri19].
Theorem 4.1 (approximate Bayesian inference). When conditioned on blanket states,
the internal states of a self-evidencing system on average perform approximate Bayesian
inference on external states, via a minimisation of variational free energy.
Proof. This follows from Lemmas 4.1 and 4.2, in that internal states minimise free energy
to reduce the average surprisal of external states to some lower bound. By Proposition 4.2,
this is formally equivalent to maximising entropy, and thus is Bayesian inference over the
optimal assignment of probability to any given external state. As was shown in Corollary
4.1, the optimal such probability is the optimal belief over states. (cid:3)
Lemma 4.1 and Proposition 4.2 were particularly important for this result. We also
implicitly used the fact that maximising entropy is formally Bayesian inference against an
unconstrained reference density serving as a uniform prior (see Jeffrey conditioning, for
instance). We used this to suggest that the mode matching prescribed by the FEP is a
maximum entropy inference of the optimal assignment of probabilities to states against
some constraint.
Wenowgivethefirstmajorcontributionofthispaper, inthiscase, totheFEP.Previous
results here established that the recognition density q(η | µ) is a maximum entropy prob-
abilitydensity, andthatalloftherelatedFEP-theoreticstatementsaboutq(η | µ)areboth
sensible and have statements in the language of maximum entropy. It is initially clear that
we can derive the free energy principle from maximum entropy at a conceptual level, given
the fact that the free energy principle aims to produce an attractor defining a system, and
claims that the system self-evidences to inhabit that region of state space [Fri12, RBF18].
Indeed, the duality between free energy over beliefs and entropy over beliefs is a bit trivial,
consisting of (in Proposition 4.1) a negation and a compensatory maximisation. Moreover,
that free energy can be derived from constraints on belief updating has been discussed
in the literature before [GB20], and free energy and entropy are easily related via convex
analysis.
Self-organisation frames the question in terms of self-entropy, however, and seeks to de-
termine where such an attractor actually comes from. This perspective is not immediately
apparent from (1), in that it asks about the behaviour of internal states, whilst in the
FEP, the behaviour of internal states follows from an analysis of the environmental states
embedding the system. Even in (4), the surprisal of internal states is only incidental to
the fashioning of a model of external states. What we want is effectively to dualise the
entire construction—to ask not what external states look like from the perspective of the
system, but what the system looks like from the perspective of the environment. Thus,
we leverage the duality defined previously: we maintain the structure of a model of states,
but it becomes a model of internal states, contingent on external states, dictating what
internal states are likely given the dynamics of the system and the environment. In effect,
in order to treat the system itself as more canonical, we ask about the Bayesian inversion
of the recognition density, q(µ | η).
Wemaysketchaproofofthisequivalenceasfollows: assumethereisamaximumentropy
probability density q(µ) constrained to be localised around system-like states, or, states
that are amenable to organisation according to some notion of the system. This could
24 DALTONARSAKTHIVADIVEL
be a stone remaining in crystalline states, or a human remaining in allostatic states. The
presence of blanket states, being a proxy for surprisal, implies that to every external state
and internal state is associated a blanket state, such that external states cause blanket
states, which are paired at any time with possible internal states q(µ | η). From there,
the pairing of highly likely internal states with low surprisal blanket states is equivalent to
the FEP. Conceptually, this follows by tautology: if a state µ satisfies σ−1(ηˆ ), it models a
b
densityoverexternalstates,meaningitisastatethatmakesorganisationpossible,andthus
is system-like. If a state is system-like, it draws from a system-like density over states.
The other direction of implication easily follows from this argument, since minimising
surprisal on external causes of changes in internal states maintains a locus of system-like
states. Explicitly, we have the following technical lemmas (elaborations on results found
in [DCFHP21]; see there for a relevant proof and expository material) and theorem:
Definition 4.1. Let [−,−] be the set of all maps between two objects, and let there be an
evaluation map ev taking a mapping set and evaluating a map therein on an input tensored
into the arguments of ev, e.g., ev([X,Y] ⊗ X) → Y by taking f : X → Y and passing
into it some x ∈ X, yielding f(x) = y ∈ Y. The evaluation of maps to produce objects
is adjoint to the composition of objects, which produces maps. Thus, given appropriate
symmetry assumptions, one exists if and only if the other does.
Recall that a synchronisation map σ exists, taking the expected internal state given a
blanket state to the corresponding expected internal state for that blanket state. We prove
some qualities of the synchronisation map now; we do so in category-theoretic language
following our description in Section 2.3 of the categorical structure of synchronisation.
Lemma 4.3. The synchronisation map σ exists injectively if and only if the factorisation
σ = η◦µ−1 exists and µ and η are injections.
Proof. We have constructed σ such that it sends µˆ to ηˆ . To define such a function we
b b
take the evaluation map of [µˆ ,b] and a fixed µˆ , getting b out of it; we follow this by
b b
feeding that b to [b,ηˆ ]. Overall we have
b
[b,ηˆ ]⊗([µˆ ,b]⊗µˆ ) → [b,ηˆ ]⊗b → ηˆ ,
b b b b b
which accepts and returns the same quantities as σ. Applying tensor-hom adjunction, this
relation is equivalent to the composition of two maps,
[b,ηˆ ]⊗[µˆ ,b] → [µˆ ,ηˆ ].
b b b b
Hence, σ is equivalent to the factorisation indicated, proving one part of the claim. Now,
restricting an injection to its image set produces a bijection, and the composition of two
bijections is again a bijection. Since this factorisation is possible, clearly σ is injective if
and only if both η and µ. (cid:3)
In effect, Lemma 4.3 allows us to claim that σ assigns each blanket state to a map
between an expected internal and expected external state. Since σ = η ◦µ−1, these two
sets are identical. It is important to note that, due to the compositional structure of σ, it
is not explicitly a function of blanket states. Only internally (i.e., within the sequence of
maps) does the blanket state matched to the expected internal state get changed into the
expected external state given that blanket state.
To make the above statement clear, we can expand the adjunction in Lemma 4.3 to
∼
[[b,ηˆ ]⊗[µˆ ,b],[µˆ ,ηˆ ]] = [[b,ηˆ ],[[µˆ ,b],[µˆ ,ηˆ ]]],
b b b b b b b b
TOWARDS A GEOMETRY AND ANALYSIS FOR BAYESIAN MECHANICS 25
which gives η ◦ µ−1 = σ on the left and reconstructs σ equivalently on the right as a
function which generates ηˆ out of (µˆ ,b) by an internal function η : b → ηˆ . In other
b b b
words, η (cid:55)→ (µ−1 (cid:55)→ σ). If we so choose, we can assign a fixed b to σ, instead of producing
it internally from evaluating µ−1 as we did above. This changes the definition of σ,
but interestingly, it produces the same result. The same adjunction on this function,
∼
[b⊗µˆ ,ηˆ ] = [b,[µˆ ,ηˆ ]], constructs a function of pairs, ξ(b,µˆ ); for an unknown secondary
b b b b b
input µˆ , fixing15 b produces σ:
b
b : b (cid:55)→ ξ(b,−), ξ(b,µˆ ) = σ(µˆ ) = ηˆ .
b b b
This obscures the compositional structure of σ, however, making it of little use to us. The
shared structure of functions of opposite inputs under the tensor product is a reflection of
braiding. Indeed, by checking braiding relationships, then reapplying tensor-hom adjunc-
tion and de-evaluating µˆ and ηˆ , it is straightforward to prove that these definitions of σ
b b
imply each other.
The non-trivial structure of σ can be seen as a symptom of the highly structured rela-
tionship encoded by σ, namely, the assignment of external states to internal states via an
assignment of identical blanket states to both.
Lemma 4.4. There exists a map taking the marginally expected internal state µˆ to the
marginally expected external state ηˆ. Moreover, this map is equivalent to σ(µˆ) = ηˆ.
Proof. Let µ : b (cid:55)→ µˆ and η : b (cid:55)→ ηˆ injectively, and assume σ(µ(b)) = η(b), factorising
b b
µ−1 η
into σ : µˆ −−→ b −→ ηˆ . By the partition theorem,
b b
E [µ] = E [E [µ | b]],
p(µ) p(b) p(µ|b)
which we have denoted as µˆ and E [µˆ ], respectively (and likewise for ηˆ and E [ηˆ ]).
p(b) b p(b) b
Thus, we seek to construct a map
σ˜(E [E [(µ | b)]]) = E [E (η | b)].
p(b) p(µ|b) p(b) p(η|b)
By construction, η and µ are linear functions of blanket states, and since the composition
of linear functions is again linear, σ is linear in b. As such, we have the following identity,
which holds in the sense of the Bochner integral:
E [σ(µˆ )] = σ(E [µˆ ]),
p(b) b p(b) b
implying that E [σ(µˆ )] = E [ηˆ ] if and only if σ(E [µˆ ]) = E [ηˆ ]. (cid:3)
p(b) b p(b) b p(b) b p(b) b
If the fairly reasonable assumptions of the above lemma are not met—say, if µ is not
Bochner integrable as a random variable, or σ is not continuous—we can still construct a
marginalsynchronisationfunctionσ˜ : µˆ (cid:55)→ ηˆquiteeasily. Lemma4.4issimplyanidealcase
where our existing σ does the work for us. Establishing as much is an important technical
step for the proof of Theorem 4.2, which proves that systems which perform approximate
Bayesian inference under a Markov blanket maximise constrained self-entropy, in line with
Claim 1. We first prove the given statement for a fixed blanket state, and then prove it
marginally, suchthateachµˆ samplesfromsomesystem-likeq(µ); invokingσ˜ isimperative
b
to this notion of system-ness.
15Evaluatingafunctionofproductsonasingleinputisreferredtoascurrying intypetheoryandparts
of functional analysis. The author thanks Dan Abramov for this reference.
26 DALTONARSAKTHIVADIVEL
Theorem 4.2. Given a fixed set of constraints, the probability measure in the state space
is the constrained maximum entropy probability distribution on internal states if and only
if the surprisal term in (4) is on average equal to σ−1(ηˆ).
Proof. We begin by defining a target density over states p(µ | η,b) = p(µ | b) and a current
density over states q(µ | η ), taking roles dual to the optimal belief and the recognition
b
density, respectively. We moreover we suppose we have defined a stationary sufficient
statistic for p(µ | η,b), given by µˆ . We can construct a dualised maximum entropy density
b
constrained such that E [µ | b] = σ−1(ηˆ ),
p(µ|b) b
q∗(µ | η ) = argmax (cid:2) S[q(µ | η)]−λ (cid:0)E [µ | b]−σ−1(ηˆ ) (cid:1)(cid:3) ,
b p(µ|b) b
which exists if and only if q∗(µ | η ) = p(µ | b). To show that this implies −ln{p(µ,b)}
b
equals σ−1(ηˆ ) on average, note we have
b
−ln{p(µ,b)} = −ln{p(µ | b)δ(b−¯b)} = −ln{p(µ |¯b)}
for a given blanket event ¯b. The desired result is now a consequence of an exponential
distribution having its mean as its sufficient statistic, i.e., that
E [−ln{p(µ | b)}] = E [µ | b] = σ−1(ηˆ ).
p(µ|b) p(µ|b) b
ThisproducesonedirectionofthedualresulttotheapproximateBayesianinferencelemma
(Theorem 4.1, here)—that if we maximise self-entropy, we bound surprisal, and perform
inferenceoverwhattheoptimalq(µ | η)is. Theotherdirectionofimplicationfollowseasily
from the converse of the claim: if p(µ | b) satisfies E [−ln{p(µ | b)}] = σ−1(ηˆ ), then
p(µ|b) b
by definition of a sufficient statistic, it is automatically the maximum entropy probability
densitygiventhatconstraint. Finally, usingLemma4.4, wecandeducethemarginalresult
(5) q∗(µ | η) = argmax[S[q(µ | η)]−λ (cid:0)E [µ]−σ−1(ηˆ) (cid:1) .
p(µ)
This admits the same argument as the first portion of the proof, in virtue of the more
general fact that total conditionalisation and marginalisation preserve identities. (cid:3)
A slightly weaker statement than that of Theorem 4.2 is that an upper bound for sur-
prisal exists, in which case, the maximum entropy density under the constraint given is not
necessarily the best description of the system; the total constraint on any given state may
be greater or less than what is required. An alternative proof of Theorem 4.2 constructs
a sampling process that converges in law to q(µ | η); to see why, and to deconstruct the
technical portions of the above theorem, we can revisit the example of the stone:
Example 4.1. Tomodelastoneonabeach, wemightchoosetomodelhowthetemperature
changes in the environment affect internal states via a particular blanket state. During the
day, when the blanket is in a state of receiving the sun’s rays, the expected temperature
at a point on the interior of the stone is given by whatever function corresponds to the
heat flow into that material given the intensity of sunlight on its boundary. Conversely,
at night, the blanket state is no longer that of ‘receiving sunlight,’ and so the internal
temperature of the stone at a point is given by the diffusion of heat out of the stone given
the coolness of the environment surrounding it—say, the temperature and speed of the
land breeze making contact with the stone. In neither case is the stone entering non-stone-
like states. In both cases, the stone is (by construction) occupying the expected state for
that blanket state on average. Now, suppose the day time heat becomes so intense the
stone melts. That the stone still goes to the expected internal state given the blanket
TOWARDS A GEOMETRY AND ANALYSIS FOR BAYESIAN MECHANICS 27
state is the tautology of evidencing—but, the fact that the expected internal state is no
longer stone-like is a result of the blanket being passed down to a different set of states,
rather than the initial set of stone-like blanket states. That is to say, suddenly, blanket
states are surprising. In this case, ours is now a poor model of the stone. Clearly that
is because it is no longer stone-like, but also, because we fail to meet the condition that
E [−ln{p(µ,b)}] = σ−1(ηˆ ) for that b; in fact, the surprisal of b (here, joint with µ) is
p(µ|b) b
much greater. This reflects the fact that our old Markov blanket does not exist anymore,
and has moved to a new regime of states. Correspondingly, a new model for which those
states are not surprising is possible, but requires re-calibrating the probabilities of blanket
states.
WhenwediscussattractorstatesforstonesinSection6.2,wegiveanotherusefulexample
(Example 6.1).
These results clarify the intuitively un-obvious statement of Claim 1—that minimising
free energy over beliefs maximises constrained self-entropy. This is crucial to relating the
FEP to self-organisation. In fact, we do this by focussing on the surprisal, rather than
re-dualising the construction, which would have been completely uninsightful. It is fitting
thattheFEPalsofocussesonitsvariationalcase,drawingfrominsightsaboutthesurprisal
of internal states, to comment on self-organisation.
Concomitantly, theseresultscanbeinformallystatedas: forasystemwhichisoptimally
system-like, external interventions—and thus their observations—cannot be surprising. If
they are, then this enacts changes to unpreferable states, such that the system fails to
remain organised—or, in the adaptive case, must act to re-organise. If they are not, states
being expected means the system draws from the maximum entropy p(µ | η) under σ.16 As
such, the maximum entropy view follows from both the interpretation and definition of the
FEP. In theory, it is now possible to—without appealing to external conceptualisations of
what is system-like about a system—summarise a sufficient set of constraints as the set of
constraints that makes internal states unsurprising, according to the above identification.
This returns us to the idea of existential variables discussed in Section 3 and [And21].
Remark 4.2. Importantly, we could have constrained (5) and any of the other results here
by the distance of a state away from the intended mean, as in Remark 3.1. This would
introduce a Laplace approximation to the true density p, which—fittingly—appears in
[DCFHP21] after introducing additional covariance information to σ. See Theorem 6.2 for
moreonhowthisisimplicatedinnon-equilibriumsteadystates. Thisadditionalconstraint
is often important for equilibria alike, in that it contains information about the physics
of a situation—this can help clarify the FEP’s role in modelling specific systems, which is
broadly separate from its mathematical character. Again, we speak more about this later,
in Section 6.3 especially.
We end the beginning of Section 4 by discussing one further idea. In the following
corollary, we reference a result due to [Pin99]—namely, that a non-polynomial function of
apolynomialcanapproximateanycontinuousscalarfunctionarbitrarilywell(seeTheorem
3.1 there).
16Accordingly,notehowTheorem4.2—aswellasitsdualonq (η)—clearlybothfailifσisnotinjective.
µ
This is an elementary property of functions which are left-cancellative, which we require by taking σ−1; it
has also been noted in [PDCF20, DCFHP21, AMTB22].
28 DALTONARSAKTHIVADIVEL
Corollary 4.2. Maximum entropy can approximate any process to arbitrary precision
under FEP-theoretic constraints.
Proof. Let C0(Rn;R) be the space of R-valued continuous functions on an n-dimensional
realdomain. SupposeJ(µ)canbeexpandedintoapowerseriesofconstraintsonacceptable
states. The exponentiation of such polynomials is dense in C0(Rn;R). Thus, it is a
universal approximator for the classification of states of µ-valued systems when µ ∈ Rn.
ClassifyingµbytheprobabilityofobservingµsolvestheFokker-Planckequationdescribing
p(µ). (cid:3)
The above corollary recalls the architecture of a neural network by showing that our
model of FEP-type systems is capable of computing the probability density over states of
thesystemtoarbitrarilysmallerror. Intranslatingfromformalideastophysicalideas,itis
important—it implies there are situations where the constraint viewpoint is only precise if
we have arbitrarily detailed information about the system. The tractability of constraints
in practice thus depends highly on how complicated such a system is.
4.1. Geometryandanalysis. Thiscompletesthefirstsetofresultsinthispaper. Having
established that the FEP is equivalent to the principle of maximum entropy, which puts
Bayesianmechanicsinthelanguageofprobabilitytheoryandstochasticdynamicalsystems
theory,wewillnowdiscusstheaccompanyinggeometricandanalytictheoryavailabletous.
There is a sense in which the FEP could only work if there existed some deeper connection
between the gradient flows of Lagrangian mechanics and the information geometries of
statistical mechanics, establishing both a geometry and functional calculus for Bayesian
mechanics. The following material relies on some obvious, but largely un-investigated,
connectionsbetweeninformationgeometryinstochasticdynamicalsystemsandsymplectic
geometry in classical dynamical systems. We will build up the theory as necessary, but
leave a complete account of the symplectic geometry of statistical physics to future work.
The following theorem is the first significant result in this portion of Section 4. The
statement is intuitively obvious, owing to the shared roles of potential and constraint
functions in shaping the dynamics of a process. In the simple case of a Wiener process, we
have a set of useful formal results from the analysis of functionals and partial differential
equations. We will make particular use of results due originally to Bakry and E´mery,
and Markowich and Villani. Many of these results use the minimisation of the positive
entropy rather than the maximisation of Shannon entropy, for technical reasons; clearly,
these are equivalent. In fact, we used this observation to prove Proposition 4.1 and other
results earlier in this section. In that spirit, all of these statements hold after adjunction,
to describe a maximum entropy model of external states as well. Due to this symmetry,
we typically need not specify variables of interest; as such, to follow references [MV00] and
(cid:82)
[Led11], we will switch to the functional notation plnp for entropy.
Theorem 4.3. In the equilibrium case of a Wiener process, the constraint on a maximum
entropy problem is a potential function for the dynamics of the underlying sampling process.
Proof. Suppose the noise is a Weiner process with the diffusion matrix set to the identity,
e.g., γ is given by
t
dγ = −∇V(γ )dt+dW .
t t t
The associated Fokker-Planck equation
∂p
(6) = ∆p−∇V ·∇p
∂t
TOWARDS A GEOMETRY AND ANALYSIS FOR BAYESIAN MECHANICS 29
converges exponentially quickly to the stationary solution exp{−V}. Taking the gradient
descent of the functional
(cid:90) (cid:90)
plnp−λ Jp+λC
yields
(cid:90)
p(∂ p+∇·(pλJ)−∆p) = 0
t
after integrating by parts, which is a Fokker-Planck equation with λJ(x) = V(x). (cid:3)
In effect, we have proven that the diffusion process which maximises entropy diffuses in
a potential given by the constraints. Intuitively this is rather obvious, since the potential
function changes the shape of the gradient of the entropy.
We proceed with this set of analytic results by demonstrating a related idea from the
functional analysis of entropy. First, we prove a modified logarithmic Sobolev inequality.
See [Led11] for a review of the actual log-Sobolev inequality.
Lemma 4.5. For a process sampling from a measure of Gibbs-type, the classical kinetic
action of its density dynamics bounds the positive entropy of its density from above.
Proof. Let a measure of Gibbs-type be a probability density p = Zexp{−V} for some
potential V which is positive on the support of p, and a fractional constant Z. In other
words, a maximum entropy probability density under a constraint function V. Since, by
physicalarguments,thekineticenergyofasystemisalwayspositive,thekineticaction—an
integral of a positive quantity—is also always positive; in contrast, the Shannon entropy
of a process is generally positive, such that the positive entropy is negative. This inspires
the ansatz
(cid:90) (cid:90)
1
plnpdx ≤ |∂ p|2dt.
t
2
For a Gibbs-type measure, the positive entropy evaluates to:
(cid:90) (cid:90) (cid:18) (cid:19)
1
Ze−V lnZe−V dx = − Ze−V ln +V dx.
Z
Let Z = k−1 for some k > 1. Since Zexp{−V} > 0 and lnk+V ≥ 0, the overall integral
is negative. Now, since |∂ p|2 is always positive, we have the relation
t
(cid:90) (cid:90)
plnpdx ≤ 0 ≤ C |∂ p|2dt
t
for any C ≥ 0, which proves the claim. (cid:3)
We make no undue claim to novelty by providing Lemma 4.5, since this result offers no
greater analytic insight than the true log-Sobolev inequality, beyond implying it whenever
|∂ p|2 ≤ ∇p·∇p. What thus is insightful is that we can deduce the log-Sobolev inequality
t
purely by physical arguments about the behaviour of the entropy functional.
Corollary 4.3. If a diffusion process minimises its classical kinetic action, then it bounds
its free energy from above.
Proof. Let ∂ p = 0. Since the classical kinetic action is quadratic in velocities, this is
t
an absolute minimum for the upper bound in Lemma 4.5. Thus, enforcing a stationary
probability density bounds the positive entropy from above, which is a lower bound on
the negative entropy; by Theorem 4.2, this places an upper bound on the free energy. In
30 DALTONARSAKTHIVADIVEL
(cid:82) (cid:82)
particular, evaluating plnp for a Gibbs-type measure yields − Vp which is a constraint
on a maximum entropy problem. (cid:3)
This proves one direction of the particular free energy lemma that FEP-type systems
are least action systems. The result recalls that of Lemma 2.2—the model of thing-ness
provided by the FEP is valid for any ‘thing,’ given a definition of a thing as an object
with some key invariant characteristics. We take the position that this generality is both
a boon and a bane. The FEP is a powerful modelling framework, but (as written here)
has nothing special to say about any particular system being modelled. See also remarks
in Section 4.3.
Using some well-known identities relating to the entropy functional, all of which are
noted or proven in Markowich and Villani’s seminal paper [MV00], we can also prove
that maximising entropy under the above equivalences induces an information geometry.
Critically, the following proposition says very little of sample paths or dynamics on that
statistical manifold, only proving that it exists asymptotically.
Proposition 4.3. Stationary points of the free energy functional are minima of the Fisher
information.
Proof. For any convex function ψ : R → R and p satisfying (6), the functional
(cid:90)
ψ(p)p
isaLyapunovfunctionfor(6). Choosingψ(p) = lnp−V,wehavebothconstrainedpositive
entropy and free energy,
(cid:90) (cid:90)
(7) ln{p}p− Vp.
ThisfunctionalmetrisesaspaceofprobabilitymeasuresbytheCsisz´ar-Kullbackinequality.
Moreover, the induced flow of p by this Lyapunov function obeys the Fisher information,
since
d (cid:18)(cid:90) (cid:90) (cid:19) d (cid:90) (cid:110) p (cid:111) (cid:90) (cid:12) (cid:110) p (cid:111)(cid:12)2
plnp− Vp = ln p = − (cid:12)∇ln (cid:12) p.
dt dt e−V (cid:12) e−V (cid:12)
When p = exp{−V}, for appropriate boundary conditions, both quantities are zero. (cid:3)
(cid:82) (cid:12) √ (cid:12)2
Note that the Fisher information is equivalent to (cid:12)∇ p(cid:12) , calculated by properties of
the logarithm. This provides us with an equivalent formulation of the actual log-Sobolev
inequality [Led11]. As such, this also proves a weak formulation of the other direction of
the particular free energy lemma, by establishing an equivalence between least action and
free energy.
We can give some further geometric results. The information geometric aspect of the
flow of p is an important property of both the FEP and the maximisation of entropy,
especially when we discuss the sort of iterated inference that characterises non-stationary
systems. Inmaximisingentropy, wehavea(potentiallysmall)numberofsystem-likestates
occupied with high probability. A system which is ideally itself is ideal with respect to
the set of constraints that characterises that system. This means that if a system fails
to self-evidence, it may appear to obey a different set of constraints. The correspondence
allows us to prove another theorem:
TOWARDS A GEOMETRY AND ANALYSIS FOR BAYESIAN MECHANICS 31
Theorem 4.4. Determining the optimal p(µ;J(cid:48)) after a change in constraints J (cid:55)→ J(cid:48)
induces an information geometry.
Proof. Assume the KL divergence can be made symmetric, for instance, by taking its
second-order Taylor expansion, and that p(µ;J) and p(µ;J(cid:48)) have the same support. We
show that the stochastic channel created by changing a set of constraints metrises a space
of probability densities, thus constructing a moduli space P containing points p with
J
distances D (p ,p ). Suppose the system has already optimised for J such that its
KL J J(cid:48)
generative model is p(µ;J). Under J (cid:55)→ J(cid:48), we can combine the two actions and define an
invariant measure such that
(cid:90) (cid:26) p(µ;J(cid:48)) (cid:27)
S[p;J(cid:48)] = ln p(µ;J(cid:48))dµ.
p(µ;J)
This is the relative entropy, which, upon maximising its negation, minimises the KL diver-
gence between the current p(µ;J) and the target p(µ;J(cid:48)). (cid:3)
Theconsequenceofthistheoremisthatadaptiveself-evidencingisamovementin‘thing-
ness’ space, wherein a system must minimise the distance between one density paramet-
erised by one set of J and the intended density parameterised by J(cid:48). This is done by
re-maximising entropy against the target set of constraints.
Corollary 4.4. Failure to self-evidence according to one set of constraints leads to a new
set of constraints prescribing the optimality of the resultant structure.
Proof. This follows from Theorems 4.2 and 4.4. (cid:3)
This perspective formalises arguments in the literature commonly referred to as the
‘passing down’ of Markov blankets. Refer to Example 2.1 to see how this describes a phase
transition in the possible dynamics of the system. In humans, this is the transition from
life-likestatestoastructurewhichisinert,afterdriftingtoofarawayfromtheallostaticset-
points dictated by J. Such a structure is still organised, and thus still evidences according
to some new set of constraints J(cid:48).
Atthispointwepauseanddeferanyfurthergeometricideas, whilstwetakeanextended
detour into the geometry and physics of quantum field theory in Sections 5 and 6. We
will take up the FEP again later in Section 6. Before that, we take the opportunity to
address some measure-theoretic questions from the analysis of random variables, as well
as a philosophical point about these results.
4.2. Technical remarks on random dynamics. Some apparent technical issues arising
in [Fri19] are easily patched in light of these results, as maximum entropy has strong
connectionstoformalprobabilitytheoryandtheanalysisofrandomfunctions;additionally,
we can begin to give an explicit characterisation of the attractor of system-like states we
are interested in. One key problem in this context is that of ergodicity assumptions in
the FEP. In the following results, we assume the state space has a σ-algebra17 of open
sets F, preferably the set of Borel sets admitted by X, collecting states as observable
events. Moreover, we assume (X,F,P) is a complete measure space or extends to one.
Throughout, x is an arbitrary state variable. We begin with the following corollary:
17To avoid any confusion, we point out that this has nothing to do with the synchronisation map of
Sections 2.1 and 4, and is so named as a convention within probability theory.
32 DALTONARSAKTHIVADIVEL
Corollary 4.5. It is necessary that the target density p(x) be stationary for particular free
energy to be minimised.
Proof. Observe that the maximum entropy Lagrangian has no velocity coordinate, and
thus assumes that ∂ p(x,t) = 0 on any level set of S[p]. Moreover, the constraining
t
environmental density is necessarily fixed for fixed constraints. (cid:3)
This affirms the equivalence of free energy minimising systems and systems at steady
state given in Lemma 2.2, clarifying the role of fixed constraints in defining system-ness.
Definition 4.2. Let A ∈ F. An ergodic probability measure is a probability measure for
which no subset P(A) is invariant under a measure-preserving function T : X → X; that
is, P(T−1(A)) = P(A) and P(A) = 1 for all A up to sets of measure zero.
Taking T as a shift map inducing the flow of a random dynamical system [Arn98],
Definition 4.2 is intuitively that ‘every meaningful state will certainly be visited by the
system’—thatis,system-likestatesareobservedalmostsurely. Basedonthis,weintroduce
a notion of local ergodicity, described below. The idea of local ergodicity is not completely
vacuous, in that states of probability measure zero are not impossible—thus, they are
contained in the state space—but there is not a meaningful sense in which the system
occupies those states, which is certainly (tautologically) true if they are not system-like.
Taking the Bayesian interpretation of probability, our model of the system places no belief
innon-system-likestates. Moreover,aneventwhichispossiblebuthasmeasurezeroshould
be surrounded by other possible events, but we demand that an entire open set around
any such event has measure zero, which is a slightly stronger claim.
Since σ-algebras are closed under complement and intersection, locally ergodic measures
may be constructed for systems that are ergodic on a subset of states.
Definition 4.3. A locally ergodic system is a system that possesses an invariant probability
measure on a subset of the state space, such that the system occupies a distinct open subset
of states almost surely.
The following is a minor technical lemma, a shortcut to the disintegration theorem:
Lemma4.6. Theretractionofmeasurablesets(cid:96)(C) = Aissuchthatameasurablefunction
f on X restricts to A under f((cid:96)(C)) = f (C) = f(A). Moreover, F is closed under
A
pullback by (cid:96).
Proof. Let C and A be sets in F with A ⊆ C, and (cid:96) a map retracting C to A by the
left-inverse of the inclusion ι : A (cid:44)→ C. Pulling back f by (cid:96) defines f(A), the restriction of
f to A. The algebra F is, by definition, closed under complement. Thus we can topologise
any retraction as a subset of X. (cid:3)
We refer to the map (cid:96) as a lariat. Its intuitive role in the following proposition is that of
a rope tightening around system-like states; it will allow us to construct a locally ergodic
measure explicitly.
Theorem 4.5. A system minimises its free energy if and only if it is locally ergodic, given
that it visits all system-like states.
Proof. Using the convex duality of constraints on states in (2), we strengthen the con-
straints on the subset of system-like states A until non-system-like states comprise a sub-
(cid:82)
set of measure zero under p(x)dx = P(A) for x ∈ A. Assume that A occurs almost
A
TOWARDS A GEOMETRY AND ANALYSIS FOR BAYESIAN MECHANICS 33
surely—that every system-like state is visited eventually. By Lemma 4.6, it follows that
P(A) is an ergodic target distribution under P(A) = 1 and P(F \ A) = 0, and under
Definition 4.3, it is a locally ergodic measure on A. By Theorem 4.2, the existence of P(A)
is equivalent to minimising free energy. By Theorem 4.4, the logical inverse also holds. (cid:3)
Note that, since we have assumed we are working in a complete measure space, every
subset of F\A is also a set of measure zero. Never do we show that subsets of measurable
sets are measurable: this is clearly false in general.
Remark 4.3. Much like ergodicity itself, this can be stated heuristically as ‘system-like
states are certain and non-system-like states are meaningless given the system is a system,’
which is precisely the idea of self-evidencing. In the highly determined case, the trivial
ergodic measure on a fixed point c is often denoted δ . Such a measure can be constructed
c
under the retraction of C to the point, as in Lemma 4.6. We note that the local ergodicity
in Theorem 4.5 is more properly thought of as a definition of system-ness, since enforcing
an identically zero measure on non-system-like states is technically valid under (cid:96)’s re-
topologisation of X, but is not very instructive, and fails if the constraints cannot be
strengthened as required in Lemma 4.6. Thus, local ergodicity is equivalent to a well-
defined system-ness. The definition of local ergodicity as ergodicity on system-like states
is consistent with current FEP literature [RBF18].
Toreviewtheseresults: aconsequenceoftheCorollary4.5isthatwerequirethestatistics
of any such process to be time-wise equivalent after a single maximisation in order for
the derived p∗(x) to remain valid—this is intuitively sensible, but it poses a challenge to
generalising the FEP to non-stationary environments. The inclusion of certain complex
systems possessing learning or memory forces us into ad hoc methods, such as the iteration
of inference, or the use of effective equilibrium for stationarity (cf. Remark 2.2); however,
the sought generalisation should be possible in the future. Non-ergodicity appears to be
a non-issue given we accept stationarity, due to the lariat argument in Theorem 4.5. The
central tautology at the heart of the FEP—that things which exist, exist—has already
been discussed as a consequence of Lemma 2.2. Consistent with the consequences of that
lemma, maximum entropy makes no detailed balance assumptions—detailed balance must
be specified as a structural constraint on states in maximum entropy [CDC15], and so a
priori nothing is said of the requirement or dismissal of detailed balance. However, note
that in Remark 6.1 we will discuss how local detailed balance is indeed required to model
any such system.
An interesting physical consequence of these results is that adaptive systems cannot be
said to violate the second law of thermodynamics, ∂ S[p(x)] ≥ 0.
t
Remark 4.4 (the second law of thermodynamics). A consequence of Proposition 4.1, re-
flected in the maximisation of the entropy of a density over beliefs about external states, is
that adaptive systems do not avoid the second law of thermodynamics; rather, they lever-
age it, offloading the increase in entropy to their environments, and changing their beliefs
accordingly. Much like self-evidencing is native to the constrained entropic view but is still
apparent in the free energy view, this disordering is a signature of free energy present on
the constraint-based side of the adjunction, in that this is what creates the aforementioned
ontologicalpotentialwhichorganisesthesystemintoitself. Itisthusthecasethatadaptive
systems are engines which ‘eat’ order and produce entropy, disordering their environments
to keep themselves organised. It has indeed been argued that complex systems are in fact
34 DALTONARSAKTHIVADIVEL
statistically favoured due to their role as vehicles of dissipation [Uel20, UDCCF21]. An
example of this dissipation is seen in the conversion of edible substances to heat, such as
what occurs in human metabolism. Dually, Theorem 4.2 states that organised systems
do indeed maximise self-entropy over system-like states, accepting the second law up to
what is allowable within the confines of system-ness. This lack of determination of what
constitutes a system-like state is important for the flexibility and itinerancy character-
ising adaptive systems, and more generally, models an inexorable tendency for agents to
fluctuate and explore.
In fact, a stronger statement is possible, based on this duality. We have established
that an agent increases the entropy of its beliefs to within some constraints on what is an
accurate belief. Non-agentially, the statistical structure of an object reflects the ever in-
creasing entropy of its surroundings, under the constraint that this environment must still
allow that object to exist (Proposition 4.2). An interesting feature of adjunctions is that
there are typically signatures of either view on each side—for instance, the constraints on
surprisalthatappearinbothLemma4.2andTheorem4.2. TheaccuracyconstraintofPro-
position 4.2 is a signature of the maximum self-entropy view on the side of beliefs—recall,
adjointly, we have established that we can model an object by maximising the entropy of
our own beliefs about the system, assuming the system can be understood as maximising
its entropy against some constraints on what it means to be such a system. In both views
we suggest a belief corresponds to a physical maximisation of entropy, in that the believed
and actual probabilities of states disperses. For instance, in Remark 4.4, we suggest this
belief includes or reflects the process of environmental disorder.
Separately from the above results, it has been shown that correlations between coupled
subsystem can (and should) be incorporated as a structural constraint on maximum en-
tropy, leading to subadditivity [PGLD13a]. This motivates the following result, again
elaborating on the role of constrained maximum entropy in self-organisation:
Theorem 4.6. Under the adjunction defined in Theorem 4.2, the joint entropy of the
agent-environment loop is bounded by the total entropy of the agent-environment loop.
Proof. Under the coupling induced by the Markov blanket, the mutual information
I[µ;η] = S[µ]+S[η]−S[µ,η] > 0
such that joint agent-environment entropy is subadditive,
S[µ,η] < S[µ]+S[η].
When both systems maximise their entropies subject to a constraint that implies the
presence of a Markov blanket, the total entropy of the joint agent-environment system
should be bounded strictly by the sum of the entropies for each individual system, such
that introducing constraints which imply a coupling decreases the joint entropy. (cid:3)
Since the entropy of the agent-environment loop is controlled by the individual compon-
ents of the loop, and introducing constraints that imply a coupling decreases that joint
entropy, the integrity of the loop is precisely dependent on the integrity of its components.
This statement is in some sense a statement that, by maximising constrained entropy both
ways—or identically, by leveraging the coupling between agent and environment—self-
organisation is possible. In so doing, it reveals the importance of the Markov blanket
formulation, as a symmetric statistical relationship between the system and its environ-
ment. Likewise, this joint entropy decrease out of constrained entropy maximisation can
TOWARDS A GEOMETRY AND ANALYSIS FOR BAYESIAN MECHANICS 35
be taken as a feature of self-organisation, in that no loop exists if the system joins its en-
vironment. Interesting results are given on the role of joint entropies in control in [TL04],
which appear consistent with Theorem 4.6.
Finally, on the subject of this coupling, note that blanket states play an auxiliary role in
the statement and ultimate proof of Theorem 4.2. The Markov blanket itself is subsumed
under the constraints determining what states are system-like and what states are not.
This has other useful consequences, like the idea of time-varying constraints generalising
the FEP to systems with wandering Markov blankets, which was discussed in Section 3.
Whilst equivalent, the different viewpoint sheds some light on the nature of blankets—the
same idea, that definitional constraints on system dynamics effectively subsume a Markov
blanket, appears in a slightly different fashion in [RKLM22].
4.3. On theidea of ‘asif’ inference. Ashasbeenpointedout,throughouttheliterature
(see [And21] for an account of this idea) as well as this paper, the free energy principle
holds in most generality when systems look ‘as if’ they are performing inference against
their own system-ness. This paper has implicitly put forth the idea that this is the best
summarisation of the FEP: an account of the sufficient conditions for a non-equilibrium
steady state to exist for a system. This is where its generality in the theory of random
dynamical systems comes from, and its description of complex self-organising systems is
ultimately a consequence of more mathematical aspirations. This leads to a challenge,
however: the FEP is not necessarily married to the supposed physical content of the
FEP. A similar sentiment was raised in Section 3—we model the existence of a system as
the performance of inference on system-ness, ultimately an abuse of model interpretation,
enabled by the tautology that a particular system carries evidence of being system-like by
existing as the system. This has been called ‘organism-centred fictionalism’ in [RFH20],
emphasising that it is an account—or indeed, a model—of how agents behave, given that
the system can be understood as being coupled bidirectionally with an environment. This
is in contrast to the framework being strictly an explanation or a mechanism.
WealsorevisitimportantcommentsinSection2.3,wherewerefertotheuseofmaximum
entropy as a modelling tool, capable of understanding an agent from the outside looking
in. This is where a key difference between the viewpoints of maximising constrained
entropy and minimising free energy emerges. It does not matter whether the agent is truly
performinginference: weareperforminginferenceabouthowtheagentbehaves, byplacing
a probability density over its likely states under a semi-arbitrary but informed definition
of system-ness. Suppose for some philosophical reason we wish to say that systems do
not intentionally maximise their entropy but are modelled by the principle of maximum
entropy. Thisisinlinewithpreviousdiscussionsaboutsystemswhichdonotself-evidence,
but simply evidence, e.g., Example 2.1. Maximum entropy obviously accommodates this;
thiswas,infact,theoriginalpointofJaynes’deploymentofmaximumentropyinstatistical
physics. We ought to then ask why it is that the formalism here extrapolates to systems
with a greater adaptive sense, such as in Example 2.2.2 (or even Example 2.2.4 when
iterated appropriately). The fact that it is still a model is key. Neglecting to explicitly
construct actions—i.e., those originating from active states of the system—is consistent
with the purpose of maximising entropy as a description of some system. This is not a
reinforcementlearningmodelofhowagentschooseactions, andfromtheperspectiveofthe
heat bath the agent is situated in, there is no policy. That is, the environment is agnostic
as to the motivational structure for an agent—there is simply a normative account of what
36 DALTONARSAKTHIVADIVEL
the system is doing at any given time. This is the adjoint of free energy minimisation.
We do away with the generative model that the agent’s recognition density matches, place
a density q(µ) on what internal states do—without reference to why, necessarily, they
are doing it—and this matches some prescribed internal states via the maximisation of
entropy. We emphasise once more that the lack of action is not simply an artefact of this
description—it is fully a consequence of it.
5. Maximum entropy as the description of a field theory
Recall our motivation is to develop the FEP as the principle underlying a field theory.
We assume that the probability density on X has a sufficiently constructive description
underfield-theoreticaxiomstolicenseaninterpretationasareal-valuedscalarpre-quantum
field theory admitting a coupling to an abelian gauge field. That is, p(x) describes a field
with probabilistic degrees of freedom, obeys wave mechanics, and has a gauge symmetry.
Intuitively, this is fairly clear: the probability measure on statistical microstates, itself
dictated by a classical equation of motion, is indeed a central part of statistical field theory
(SFT). We take this in a slightly unconventional direction, and invoke this probability
density as a field itself. To that end, we will check some key properties, but leave any
fuller axiomatic field theory of probabilistic inference to future work. Since we use classical
devicestounderstandtheFokker-Planckequation,anyformalequivalencewouldbemostly
for book-keeping purposes.
Mathematically, an object-valued field theory is simply an assignment of an object state
toeverypointonsomebasespace. Forinstance,atitsmathematicallypurest,18aquantum
field theory (QFT) is a function assigning a space of quantum states or operators to points
or temporal slices of a space-time manifold [HK64, Ati88], and the same is true of classical
fields [BFR19]. Generically, a field is a section of a vector bundle, mapping each point on
an input manifold to a space of possible outputs at that point [Nak03].
It is clear that a probability distribution over some state space is one such function.
Moreover, its evolution is given by a Lagrangian corresponding to a particular wave equa-
tion,andunderassumptionsofaWienerprocess,thatwaveequationcanbeunderstoodvia
a path integral representation. This allows us to treat p(x) as a real-valued, semi-classical
scalar field theory over probabilistic degrees of freedom (rather than, say, a measure on
X).
Remark 5.1. The notion of model reification as a logical fallacy, introduced in [And21]
and [And22], arises from transferring the conceptual content of a model to new domains
alongside the model itself. One must in general hesitate to treat equivalent mathematical
structuresasidenticalobjectsextendingoneoranotherphysicalinterpretation. Conversely,
this allows us to use the fact that common mathematical models do not imply common
conceptual content, to forget anything but the relevant mathematical structure in a shared
model. Whilst perhaps initially unnatural, this is used to effect throughout physics: con-
sider the virtual particles of Feynman diagrammes or the phonons of condensed matter,
or the conceptual differences between thermodynamic and information entropy. Here, the
claim is not precisely that anything which does inference is a field theory, but rather, that
it looks like one and thus can be modelled the same way. Conversely, as it is not a physical
18Inparticular,thisistheso-called‘algebraicapproach’toQFT,andincludesthecobordismhypothesis
thatcertainquantumfieldtheoriesareclassifiedbyfunctors(maps)fromcobordisms(space-timeevolutions)
to vector spaces (containing quantum states) [BD95].
TOWARDS A GEOMETRY AND ANALYSIS FOR BAYESIAN MECHANICS 37
field theory, we do not necessarily expect to see fully consistent physical features of this
theory. This equivalence of models is hence an identification of structures and not neces-
sarily of interpretations. The physical interpretation of this theory is any case complicated
by the threefold role of the constraints: J(x) is simultaneously an observable on X whose
expectation is C, a constraining potential on the acceptability of states in X, and a free
choice of gauge over X. On the other hand, these roles are context-dependent, and can
be understood classically as any function of state being both an observable and a poten-
tial, and the addition of arbitrary functions on the boundary of X factorising away in the
variation of a Lagrangian.
The behaviour of a field theory generally follows from some optimisation principle: laws
like the least action principle map onto the fundamental tendency of systems to take
trajectories of minimal cumulative energy usage. We preserve this data when we identify
p(x) with a field theory, since probability densities minimise the functional
(cid:90)
ln{p(x)}p(x)dx,
i.e., they obey the principle of maximum entropy [Jay57].
6. Constraints as gauge symmetries
The geometric portion of these results are an application of more general results in pre-
vious work by the author [Sak22a]. We provide a brief account of those results to motivate
theresultsshownhere,butreferthetechnically-mindedreadertothecitedpaperforproofs
of these claims. We also briefly motivate gauge theory itself. The gauge-theoretic results
induce a flow on the state space describing the sampling dynamics under a given steady
state density. This will allow us to show that, for a specific class of non-equilibrium steady
state admitting a potential-based description, we can use the entropy-based mathematics
we have established for the inert or equilibrium case.
In previous results, we have taken the idea of an ontological potential seriously, as a
geometric feature of a system. A potential function is something that appears to shape
the flows of a dynamical system in a state space, as if that space were curved. The results
indicated begin from the viewpoint that there ought to be a duality between deformations
of the geometry of a state space and constraints on the dynamics in that state space,
in such a way that maximum entropy translates between the two. Since gauge theory
encodes the way that dynamics in a state space reflect the geometry of the state space,
the equivalence seems natural; indeed, it is possible to give a new interpretation to the
principle of maximum entropy using features of classical gauge field theories.
6.1. A review of gauge theory. Inkeepingwiththeattitudeintheintroduction,theaim
is to situate maximum entropy—and hence the FEP—as the principle dictating how a geo-
metric foundation determines a dynamical theory. Gauge theory is an especially prescient
example. It has a rich geometric framework already in place, in terms of iterated inference
on statistical manifolds, where connections to gauge theory [Ama01, STC+16, SF17] have
already been made. Likewise, separate connections to gauge forces have appeared in the
FEP literature [RBF18], and the Helmholtz decomposition used in the FEP have their
roots in the geometric features of simple gauge theories [Gra77, Sak22a]. Going beyond
these initial examples, most everything—from things as small as strings, to things as large
as black holes—and in particular, the statistical mechanics of condensed matter theories
38 DALTONARSAKTHIVADIVEL
[LN92]—have gauge symmetries or connections to gauge theory. That inference or rudi-
mentary forms of cognition should be free of the explanatory scope of gauge theory is
unlikely, and certainly unsatisfying. Moreover, many gauge symmetries arise in a certain
sense out of constraints on the features of a theory [LN92, HT94, LW05, GT12, Wit18]. It
seems appropriate that we should be able to derive some insights for the FEP from gauge
theory.
A matter field is a physical field described by an action, as in Section 5. A gauge theory
is characterised by a matter field theory whose action, or the variation of that action, is
invariant under a certain quantity: this quantity is called a gauge [RW02]. A gauge is a
function on space-time coupled to the matter field, in the same way as a potential function
couples to the dynamics of a system. A choice of gauge induces a gauge field, akin to the
gradient of the gauge potential, keeping track of how that choice of gauge varies across
space-time. Thegeometricformulationofaclassicalgaugefieldisviaageneralisedfunction
called a section, corresponding to a choice of gauge and a matter field configuration. The
correspondinggeneralisationofthederivativeofafunctionisencapsulatedinaconnection,
whichgivesusanotionoftangentvectorsfortheimageofasection[Nak03]. Itfollowsthat
gaugefieldsareconnectionsinaprincipalbundle,spaceswhichgiverisetosuchgeneralised
functions.
We take the principle of constrained maximum entropy as a gauge theory, under p(x) a
matter field and J(x) a gauge field. This symmetry is rooted in Jaynes’ original claims.
Prosaically, the fact that there exists a large class of systems which are all described by
maximum entropy, with constraints that can be arbitrarily fixed to produce the partic-
ularities of specific systems, makes these constraints a gauge symmetry. We will briefly
describe this in more detail, but for full technical constructions, we again refer the reader
to [Sak22a].
Gauge invariance of S[p] is essential to the construction. This is easily shown in analogy
to gauge theories like quantum electrodynamics: a short calculation shows that a gauge
transformation, a particular sort of change in J(x) compatible with both an assumed
coupling of J(x) to p(x) and the geometric structure of the problem, factorises away at
some point in the calculation of p(x). This yields an arbitrary choice of gauge as well
as an arbitrary change in that choice, leaving the action invariant under any gauge. In
contrast, the field equation (here, the probability density) is typically gauge covariant,
being expressed in a particular choice of gauge. The interaction with p(x) as J(x) varies is
given by parallel transport with respect to J(x), which generalises a dynamical system by
means of a covariant derivative. The specific technology of gauge theory, and in particular,
relating J(x) to a gauge field coupled to p(x), is motivated by our desire for something
that allows us to discuss how varying one quantity varies another in a well-defined way.
This is given by the covariant derivative, whose solution is termed parallel transport.
The content of Theorem 1 in [Sak22a] is that the solution to this dynamical system,
parallel transport, is the stationary point of the entropy functional under a given choice of
gauge. This is readily seen by the equation for parallel transport of a field p(x) in a gauge
field ∂ J(x)dxi, which is
i
∂ p(x) = −∂ J(x)p(x)
i i
when expressed in Rn, i ∈ {1,...,n}. Like a separable ODE, this integrates to
−ln{p(x)}−J(x) = 0,
TOWARDS A GEOMETRY AND ANALYSIS FOR BAYESIAN MECHANICS 39
which is the stationary point of entropy. Whilst initially trivial, it reveals a subtle gauge-
theoretic structure to our problem, by the nature of parallel transport.
Definition 6.1. A gauge theory consists of a space of matter field states E, a space of
choices of gauge P, and a base space X (generally a space-time manifold like R3,1). The
spaces E and P are fibre bundles, copies of the space of possible states at every point in
the base space. The bundle P is associated to E such that a choice of points in P frames
the expression of points in E, and changes in that choice re-frame points in E. A choice
of points in a bundle is called a section, a sort of internal function assigning bundle points
to base points; hence a section defines a field theory. The fact that the matter field is
frame-dependent is where gauge covariance comes from. The space that P collates is a
symmetry group that acts invariantly on the action of the theory, reproducing invariance.
In a bundle there exists a connection that tells us how a section varies across the base,
leading to a conceptualisation of the derivative in a bundle. The gauge field is the pullback
of the connection to the base, reproducing the effects of the choice of gauge on the evolution
of fermionic particles on space-time.
As a corollary to the above definition, we can construct a gauge theory for maximum
entropy: we take the choice of gauge as g(x) = exp{J(x)}, which acts on an exponential
density p(x) by multiplication to re-frame it. The local connection ωg is expressed as
i
g−1(x)∂ g(x) in the basis ei∂ , such that ωg = ∂ J(x). This defines our gauge field. The
i i i i
effect of the gauge field on the evolution of points p(x)—where we mean the application
of p to a point x, such that this is a scalar changing over space-time—is given by parallel
transport.
Paralleltransportisaspecialcaseofmotioninagaugetheorywhichfollowsthepathpre-
scribedbythegaugefieldexactly,inthesensethatwhenaparticlemovesinparallelfashion,
there is no force acting on particles deflecting them from parallel paths. We can also have
motion that experiences a gauge force by curving away from these parallel paths. This
splitting of dynamics into trajectories which are parallel to the base—termed horizontal
paths—andtrajectorieswhichcurveupwardsordownwards,likechargedparticles—vertical
paths—is characteristic of a gauge theory [Ble81]. In the FEP, it takes on a renewed mean-
ing, whichweshallseebelow. Usingagauge-theoreticandmaximally-entropicformulation
of the splitting, we can explicitly construct the attractor over states which we sought in
earlier results, and, comment on the Helmholtz decomposition in itinerant or life-like sys-
tems.
6.2. Sample paths in stationary stochastic processes. Thus far we have focussed on
a relatively simple theory: that of an effectively equilibrium system. In particular, we have
not covered the dynamical sample paths admitted by the evolution of states under p(x),
which are not, themselves, stationary; in fact, invoking maximum entropy as a model of a
system in Section 4.3, we have intentionally ignored sampling dynamics. We can no longer
avoid this, given the content of parallel transportation, but we still constrain ourselves to
this simple case.
We begin by revisiting results from [Sak22a] in the context of stochastic systems which
self-organise. Using the results onestimationas a gauge force, inferencecanbe understood
as weighting the states of a process in such a way that inference converges to a stationary
distribution over some data by constraining inference to the set of most likely states. We
begin the argument by giving a formal statement of the idea that a system has a system-
like attractor. We are then able to prove that the gauge theory induced by the constraints
40 DALTONARSAKTHIVADIVEL
enforces the convergence of sample paths to that attractor under strong constraints, and
thatthisbuildsamaximumentropyprobabilitydistributionoverstatesbypurelygeometric
considerations. In the Gaussian case, an attractive intuitive picture is that of a funnel,
guiding paths to a low-constraint region, such that this region is an attractor where states
are observed with high probability. Under this interpretation, we return to the idea of
the constraints being a potential function, guiding the dynamics of a process. In fact, we
assume p(x) is a Gaussian serving as the Laplace approximation to an underlying density.
We also discuss splitting behaviours of stochastic dynamics under this gauge potential,
referring implicitly to Proposition 4 of [Sak22a].
The general, formal definition of a ‘geometry,’ a motif dating back to Klein’s Erlangen
programme and which features heavily in work as recent as Lurie’s derived algebraic geo-
metry [Lur09] and Freed and Hopkins’ H -structures [FH21], is that of a topological space
n
equipped with some additional structure, especially that which derives from a group or a
group action. A Euclidean geometry, for instance, is an n-dimensional real space equipped
with a Euclidean metric and isometries thereof. A Riemannian geometry is a smooth
manifold equipped with a Riemann structure. Previously, we have introduced the idea of
a constraint geometry as a geometric formulation of the covariance of probabilities and
constraints in maximum entropy. The fixing of constraints in maximum entropy induces
a potential function that affects flows in that space, reflecting an extra structure in what
states are accessible by a flow and what states are not. We offer the following semi-formal
definition to that end:
Definition 6.2. A constraint geometry is a topological setting in which dynamics in some
space interact with a constraint function in that space determining the shape of any such
flow. Constraint geometries have a natural interpretation as the result of a gauge fixing
procedure in a manifestly G-covariant field theory for some Lie group G, and hence admit a
formulation in terms of gauge theory. As such, a constraint geometry is a principal bundle
structure over a space of interest with a given choice of connection, and an associated
bundle giving some ensemble property of the possible flows.
Notice what we have done without much further elaboration: claim that constraints tell
us what states can and cannot be accessed, rather than what states can be accessed and
withwhatprobability. Intuitively,wecanrelatetheideaofsamplingunderadensitytothe
frequenciesofsampledstates, asthoughhistogrammingtheoccupationtimeofoursystem.
Under very strong constraints this means that flows will occupy constrained states very
briefly, if at all, which returns us to the idea of the locally ergodic measure constructed
in Theorem 4.5. In fact, we can formalise this using aspects of ergodic dynamical systems
theory.
Definition 6.3. An ergodic probability measure implies that the proportion of time spent
in an area of state space A is equivalent to P(A), i.e.,
1 (cid:90) T
(8) lim 1 (x )dt = P(A)
A t
T→∞ T 0
for 1 an indicator function on states x ∈ A and x a state at time t.
A t
Definition 6.3 leads to the celebrated equivalence of temporal and spatial averages for
ergodic measures, due originally to Birkhoff. Here, it allows us to claim that an adequately
system-like system which visits all system-like states defined under some set of constraints
TOWARDS A GEOMETRY AND ANALYSIS FOR BAYESIAN MECHANICS 41
will almost surely be observed in that region A. Correspondingly, in the limit T → ∞
when system-like events become typical, we can interpret the locally ergodic measure for
a constrained system as a demand that the system spend negligible time outside of A.
Theorem 6.1. Let the open set A be some region of the state space. The following state-
ments are true, and under Theorem 4.5, arise from a constraint geometry:
(1) Approximate Bayesian inference arises from a gauge force acting on a locally er-
godic system in the direction of an optimal control parameter.
(2) AlocallyergodicsystemoccupiesanystateinAwithprobabilityexp{−J(A)}, which
is equivalent to exploring A in parallel fashion.
(3) For a locally ergodic system, there exists an attractor A.
Proof. To prove Statement 6.1.1, we need only note that, on a Gaussian probability dens-
ity, under a gauge potential, there exists a vertical flow towards the expected internal
state. By Theorem 4.3, this potential is a constraint on the maximum entropy probability
density, which is approximate Bayesian inference by Theorem 4.2. As such, maximising
p(x) to find the expected internal state in A is a vertical flow, which we identify as the
flow of charged particles under a gauge force. To prove Statement 6.1.2, we recall results
from [Sak22a]; namely, that under a gauge potential, there exists a horizontal flow of equi-
probable states which explores that locus of equiprobable states. The set of all such level
sets is exp{−J(A)}. Now we prove Statement 6.1.3. Firstly, for a locally ergodic system
whose occupation of system-like states has measure zero, the solenoidal flow degenerates
on the complement of A such that states are confined to A. By Poincar´e recurrence, the
set of points in A that escapes A has measure zero if P(A) > 0, such that states return
to this confining region; since A is an open set, this is finally a trapping region for the
system, and thus an attractor. Now we can prove that all three statements are implied in
a constraint geometry. Statements 6.1.1 and 6.1.2 are clearly equivalent to the existence of
a gauge potential, without which the splitting of flows under constraints on inference does
not exist. Under Theorem 4.5, our use of Poincar´e recurrence necessitates a constraint
geometry, and is equivalent to strong constraints on F\A. As such, Statement 6.1.3 holds
if and only if we maximise entropy in a constraint geometry. This proves the primary
claim. (cid:3)
We take a moment to comment on the results in Theorem 6.1 more fully—the region of
high probability A constructed in this result is the system-like attractor that we initially
sought,inSection2.1. TheintuitiveideaisthattheFEPsaysourmodelofasystemshould
be that the system’s evolution returns to this region of system-like states, in virtue of it
being such a system. The fact that the system samples from these states is described by
the steady state density over A, which here has become an ergodic density for a trapping
region A. When constraints J(x) exist such that the complement of A, F\A, has measure
zero, we can rigorously say that the system spends no time in that region—otherwise, we
must enforce a semi-arbitrary variance cutoff defining what counts as an instance of that
system and what is leaving system-like states.
Example 6.1. Consideradissipativedynamicalsystemandagivenblanketstateforit,such
as a stone for a given weather condition. A faithful model of a stone must constrain itself
such that it proceeds directly to the expected internal state given that blanket state; less
normatively, we can expect a stone to be defined as an inert system, which definitionally,
42 DALTONARSAKTHIVADIVEL
does not have the ability to vary its states. As such, the ontological potential of a stone
(conditioned on a blanket state) is highly constrained about that expected internal state.
In this case, the solenoidal component of the flow degenerates, whilst the gauge force we
have referred to enforces the thing-ness of the stone by driving it to exist at µˆ . Here
b
the attractor given a blanket state is µˆ , and the conditional system-like density is the
b
Dirac measure on µˆ ; the full set of system-like µ, A ⊂ X, is the set of all such possible
b
stone-like states, with individual probabilities based on the constraints we place on what
it means to be a stone. The systemic constraints on a stone would likely be constant
within A, since there is no preference to be expressed, and hence would produce uniform
probabilities over A and zero probabilities elsewhere. The resulting probability density
could be updated were we to observe some states more often than others, which would
place further accuracy constraints on our model in addition to constraints definitional of
the system.
In particular, such constraints could include a hard cut-off of temperature at whatever
the stone’s melting point is, as well as a constraint on the internal energy of the molecular
arrangement of the stone, enforcing its structure as, say, a silicate mineral. Some of these
states will be observed less often, suggesting we should change the shape of the constraints
until we get the desired sampling relation on A yielded by (8). This could be predicated
on certain expected internal states being observed more often because their blanket states
are observed more often, changing the density p(b) that we marginalised over in Theorem
4.2.
A consequence of formulating these results as parallel transport is that the way this
sampling is enforced is by splitting the dynamics under such a descriptive steady state
density:
Corollary 6.1. Under a choice of gauge g(x) = exp{−J(x)} and an induced field equation
p(x) = exp{−J(x)}, a solenoidal flow over the state space exists.
Proof. For a gauge theory where g(x) = p(x) and a differential acting in the standard basis
of Rn, the parallel transport equation satisfies the following logarithmic derivative:
(9) ∂ p(x) = −∂ ln{p(x)},
x x
conventionally written as g−1dg. Moreover, Statement 6.1.1 holds in the vertical direction
wherethisgradientisnon-zero,andStatement6.1.2holdsinthehorizontaldirectionwhere
thisgradientiszero. Sincetheliftofapathϕ = {x ,...,x }isgivenbyp◦ϕ,thepullback
t 1 n
to X under quadratic constraints gives circular paths for horizontal ϕ and flows towards
xˆ for vertical ϕ. This yields flows in X along the surprisal. (cid:3)
As an addendum to the above corollary, note that the derivative on the left-hand
side, ∂ p(x), measures how the evaluation of the section p at a point changes; hence
x
it is analogous to a particle moving on the probability density treated as a hypersur-
face. As such, under Theorem 6.1, Corollary 6.1 proves that particles which perform
approximate Bayesian inference perform a gradient descent on surprisal, as suggested in
[Fri19, PDCF20, DCFHP21]. See also Example 1 of [Sak22a].
Note what it does not prove: that those particles seek the mode xˆ. They merely match
it. Moreover, note that the velocity of the flow—the parameterisation of the isocon-
tours, in other words—is arbitrary. Supposing a matrix operator of the form given in
[Fri19, DCFHP21, BDCF+22] acts on the flow to yield a well-defined velocity along a
TOWARDS A GEOMETRY AND ANALYSIS FOR BAYESIAN MECHANICS 43
curve recovers those results. However, this must be specified beforehand. Reintroducing a
Lagrange multiplier to (9) and choosing the value of λ based on some physical data to be
matchedmaybeamoreprincipledwayofgeneratingthosematrices, butaworkedexample
should be provided in the future.
As was mentioned, the paths we describe are relatively simple, given that (by construc-
tion) they converge to a stationary attractor. A proper theory of paths, which extends to
moving attractors and even focusses on the paths themselves—and thus, describes adapt-
ive, truly complex systems—requires a broad generalisation of these results, but one which
is within reach.
We now discuss an application of these results to non-equilibrium situations. Some
connections to the splitting introduced above are mentioned.
6.3. Regarding non-equilibria. In this section, we are able to gesture towards an ap-
plication of these results to non-equilibrium regimes. Thus far we have been able to show
that the relationship between the technology of the FEP and the nature of equilibrium
dynamics rests on fairly solid ground.
We have previously referred to the lack of ‘effective’ energy flows in non-equilibrium
steady state. It is apparent that, due to stationarity playing a fundamental role in both
equilibrium and non-equilibrium steady state, the only difference between these systems is
the presence of some underlying flux.
Example 6.2. Following Example 6.1, when the locally ergodic measure for a system which
performs approximate Bayesian inference is δ(x−x¯), and the resultant attractor is a fixed
point x¯, the horizontal flow degenerates and there is no solenoidal component. Thus, we
often need to make a choice as a modeller as to whether or not it makes sense to describe a
system with some spread in its state space. For this reason, an underlying horizontal flow
is more like a non-equilibrium condition than an equilibrium one; in the limit, equilibrium
systems will go to x¯ under any sort of dissipation. Indeed, if the horizontal flow has a
meaningful sense of direction—if a flow in one direction is preferable to a flow in another,
and as such, if forward trajectories are more probable than their inverses—then detailed
balance is broken, making this a non-equilibrium steady state. However, as we mentioned,
thisisstrictlyextra(physical)dataaboutthehorizontalflows. Correspondingly,maximum
entropy can accommodate both under appropriate constraints.
Coarse-graining non-equilibrium systems at steady state, where those fluxes cancel out,
implies that non-equilibrium steady state looks like equilibrium steady state with spread
and can be modelled the same way. Ignoring certain physical degrees of freedom, we can
use maximum entropy to treat non-equilibrium steady state and still derive insights. This
is fully consistent with maximum entropy as a model (cf. Sections 2.3 and 4.3). Our model
of the system’s characteristic probability density is agnostic to the sampling dynamics
and actions taken by the system under that density: we simply do not evaluate how a
non-equilibrium steady state is maintained, only whether it is. As such, the underlying
sampling dynamics and physical degrees of freedom can be ignored to get an effective
equilibrium. This is another sleight of hand that is necessary to extend our simple picture
to the less trivial heart of the FEP, but which is possible in any case.
Remark 6.1 (limitations of effective equilibria). It should be noted that, realistically, this
framework is only insightful near equilibrium, where the non-zero probability flux due to
non-equilibrium can actually be neglected [ZS07, HZE17]. Consider also that we do not
44 DALTONARSAKTHIVADIVEL
want divergences in entropy, but only presume it is maximised within what is allowable by
constraints. As such, the work done by underlying flows cannot be arbitrary—it must be
bounded—another signature of near-equilibrium. On the other hand, assuming the system
has reset its energy levels to whatever is induced by the constraint is effectively designing
a new equilibrium for the system [Sei12], which is what we do as modellers who coarse
grain our observation of the system’s maintenance of a stationary state. Some further
technical conditions should be stated: by assuming our system’s sampling dynamics are
related to entropy production and reflect a coupling to an environmental heat bath, this
requires at least local detailed balance, purely for our model to be valid [Mae21]. Luckily,
we lose very little by these simplifying assumptions, in that the production of entropy as
a characteristic of life (cf. Remark 4.4) is a hallmark of the non-equilibrium steady states
of even complex adaptive systems [JPR19, Uel20].
On this basis, we have suggested that this construction applies to systems with out of
equilibrium dynamics which can be described by a potential. An example of this, and a
justification of the use of maximum entropy on that system, is constructed in this section.
In particular, we prove an effective equilibrium theorem below, which constructs a generic
system like Example 2.2.2 that is described by maximum entropy, thereby proving such a
class of system exists.
Proposition 6.1. Non-equilibrium systems with a steady state as an initial condition
maximise entropy at any other steady state.
Proof. Defineastochasticentropyfunctionalass[x] = −ln{p(x(t));κ(t))}, whichissimply
thesurprisalonatrajectoryforsomecontrolparameterκ(t). Atsteadystate,thestochastic
entropy can be averaged into the Shannon entropy, since any p(x(t)) = p(x). Now suppose
p(x;κ) = exp{−s(x)}. This is the stationary point of the Shannon entropy. (cid:3)
The stochastic entropy in Proposition 6.1 can be taken, rather straightforwardly, as a
constraint that the system ought to meet a particular non-equilibrium steady state and re-
main there. In the following, we use a different constraint, assuming the potential captures
both the driving force towards an equilibrium density and the minimum energy necessary
to maintain a density favouring states with statistics out of equilibrium. In so doing, we
extend the above proof to systems with control parameters, an important viewpoint on
non-equilibrium systems for our purposes.
Theorem 6.2. Non-equilibrium systems at steady state under a potential function are
systems which maximise entropy under a constraint and minimise variational free energy.
Proof. We prove this by constructing an example of such a system. Take a controlled
system in the presence of non-zero flux of energy. Suppose the system constrains itself
to fluctuate around a mean state xˆ. Moreover, suppose xˆ (cid:54)= xˆ . This system is at non-
eq
equilibrium. Finally, suppose that xˆ is both a fixed point and a sufficient statistic for
the sampling density, such that it is stationary. There is a potential function E(x − xˆ)
measuring the deviation from x∗ such that on average, E(x − xˆ) = 0. This potential
functionisacoarse-grainingoftheunderlyingenergyfluxinducedbydrivesanddissipation
in the system. These fluxes increase entropy production; as such the system is a non-
equilibriumsteadystatewhichmaximisesentropyundertheconstraintE[E(x−xˆ)] = 0. By
constrainingitselftoxˆ, thissystemisequivalenttoasystemwhichengagesinapproximate
Bayesian inference (Theorem 4.2). (cid:3)
TOWARDS A GEOMETRY AND ANALYSIS FOR BAYESIAN MECHANICS 45
Attheveryleast,E canbeconsideredaquadraticform,andthustheLaplaceapproxima-
tionofanon-equilibriumsystemrepresentingthedistancefromthemode. Consistentwith
the formalism laid out in [Sei12], a non-equilibrium steady state p(x;xˆ) assumed to have
relaxedfromaninitialnon-equilibriumsteadystate—ratherthanatransientstate—admits
a potential based on a control parameter κ, which is in general equivalent to −ln{p(x;κ)}.
Invoking Theorem 4.2 allows us to pose this as a problem of maximising constrained en-
tropy,anddually,oneofminimisingfreeenergy. In[Niv09,Niv10]asimilarpotential-based
description of a non-equilibrium steady state, formulated in terms of maximising entropy
without explicit reference to the underlying fluxes, is given.
We raise this problem now to provide some comments about the solenoidal flows written
ofintheFEP.TheresultsfromtheprevioussectioncombinedwithTheorem6.2constitute
a proof that, for a certain class of non-equilibrium systems, a solenoidal flow exists. The
generality of this decomposition for stationary systems has been proven rigorously before
[BTB+21], and it is formulated in detail in [DCFHP21, BDCF+22].19
Using the splitting of dynamics induced by a gauge-theoretic structure on our state
space, we are able to state that a solenoidal flow on states exists. The decomposition of
flows on the surface (x,p(x)) induces an equivalent decomposition of the flows on the X,
the state space, by the pullback one-form (the local gauge field). It is in this manner that
wereproducesolenoidalanddissipativedynamics, whichexploreanddrivetoafixedpoint,
respectively; this accommodates the flow decomposition referred to in [Fri19]. The spread
of the density is seen as a fluctuation-based, exploratory flow, whilst the dissipative flow
is a mode-matching flow which corresponds to evidencing, or the control that maintains
a non-equilibrium steady state. In inert cases this fixed point is nothing quite special,
and simply represents the average state of the system contingent on the average external
state—such a mode exists by evidencing, but has no special content, since the system does
not self-evidence. Contrast this with other systems that pursue a mode that corresponds
to a desired belief, and we recover a sort of adaptivity.
Note what we have not claimed: that there are any dynamics to the expected states.
Moreover,byignoringtheformofflowsonthestatespace,wearenotabletoreproducekey
results of the monograph [Fri19], which are non-trivial [AMTB22] assumptions about the
decomposition of flows under the FEP. What results we have been able to reproduce hold
identically at equilibrium and non-equilibrium in virtue of their connection to stationarity,
and have little connection to the underlying flows or couplings; they simply arise from
a natural gauge-theoretic interpretation of the FEP. Parameterising the change in the
density on a level set gives us curves of arbitrary velocity in (9), but there is still no
explicit connection to the sampling happening under that density. It is interesting that
the solenoidal flow surely exists, but approximate Bayesian inference is possible without
it, as in Theorem 4.1. From that perspective, that the existence and interpretation of this
flow is noted in [Fri19] with the fact that this flow does not necessarily imply FEP-type
inference in [AMTB22] may be resolved.
There are some further caveats to these results. For instance, the decomposition is
also not unique—the choice of horizontal tangent space can be reparameterised or chosen
differently. This is consistent with the literature, in the sense that there is a large equival-
ence class of dynamics that fall under the same non-equilibrium steady state density and
19It has been known for some time that such a decomposition exists for Stratonovich stochastic differ-
ential equations, perhaps first being described in [Gra77]; the papers and results discussed here focus on
its applications to the FEP. The author thanks Lancelot Da Costa for suggesting this reference.
46 DALTONARSAKTHIVADIVEL
hence the potential for any such density is non-unique. The splitting exists, but the flows
underlying that splitting are arbitrary. That is, just knowing the non-equilibrium steady
state is not sufficient to produce unique forms for the flows. In fact, this too is consist-
ent with modelling philosophies about using the FEP: we can’t access precise information
about internal states of a system, we can only model what it does at the level of ensemble
statistics. What we can say is that the flows go towards that region (Theorem 6.1) and
that this flow, at some ensemble level, looks like it consists of solenoidal and dissipative
components—these local vector fields must go towards the trapping region, and that there
is a splitting of those fields.
It is possible that some of these issues will be addressed in a more dynamical formalism
than simply maximising entropy, such as maximising path entropy, which—by Proposition
6.1—is already an attractive generalisation to truly life-like systems.
7. Conclusion
This paper has reformulated ideas fundamental to the free energy principle in the lan-
guage of maximum entropy and gauge theory. We have established that, at minimum, a
slightly simplified version of the FEP is a sound mathematical principle, the implications
of which are verifiable, and which have an additional gauge-theoretic interpretation.
Throughout, we have leveraged a few extensions or relaxations of more traditional con-
cepts. Namely, it would that the FEP satisfies properties like ergodicity, but only locally
so; maximises entropy, but only under constraints; applies at non-equilibrium, but only
because it applies to stationary systems in a scale-free fashion. The FEP rests on several
such results—places or equations where it bends a definition just enough to work in a
rigorous framework. This could be taken as a sign that the FEP maps a territory lying
towards the edge of what can be formalised in conventional mathematics. Correspond-
ingly, it is apparent that the FEP is—at some basic level of description, and as suggested
here, certainly more generally—essentially correct. Rather than being riddled with ‘global
errors’ in conceptualisation and application, as some critics have suggested, its failings are
limited to ‘local errors’ in the precision of mathematical statements and techniques; these
reflect a global correctness, but a technical subtlety, to the theory—and with a bit of work,
they are easily smoothened out.
Recalling the rich history of physically-inspired mathematics and mathematically for-
mulated physics, the framework laid out in this paper builds the FEP a concrete, formal
foundation, with some results being of more general interest to the mathematical theory of
inference and stochastic dynamical systems. In the future, this foundation will allow easier
and more effective generalisations of the mathematics contained in the FEP, to describe
more complicated systems just as rigorously.
References
[ALPF19] Micah Allen, Andrew Levy, Thomas Parr, and Karl J Friston. In the body’s eye: the compu-
tational anatomy of interoceptive inference. 2019. BioRxiv preprint 10.1101/603928.
[Ama01] Shun-ichiAmari.Informationgeometryonhierarchyofprobabilitydistributions.IEEETrans-
actions on Information Theory, 47(5):1701–1711, 2001.
[AMTB22] Miguel Aguilera, Beren Millidge, Alexander Tschantz, and Christopher L Buckley. How par-
ticular is the physics of the free energy principle? Physics of Life Reviews, 40:24–50, 2022.
[And21] Mel Andrews. The math is not the territory: navigating the free energy principle. Biology &
Philosophy, 36(30), 2021.
TOWARDS A GEOMETRY AND ANALYSIS FOR BAYESIAN MECHANICS 47
[And22] Mel Andrews. Making reification concrete: a response to Bruineberg et al. 2022. PhilSci-
Archive preprint 20095.
[Arn98] LudwigArnold.Random Dynamical Systems.SpringerMonographsinMathematics.Springer
Berlin Heidelberg, 1998.
[Ati88] Sir Michael Francis Atiyah. Topological quantum field theory. Publications Math´ematiques de
l’IHE´S, 68:175–186, 1988.
[Awo06] Steven M Awodey. Category Theory. Oxford Logic Guides. Ebsco Publishing, 2006.
[BCG+21] Dylan Braithwaite, Matteo Capucci, Bruno Gavranovi´c, Jules Hedges, and Eigil Fjeldgren
Rischel. Fibre optics. 2021. Preprint arXiv:2112.11145.
[BD95] John C Baez and James Dolan. Higher-dimensional algebra and topological quantum field
theory. Journal of Mathematical Physics, 36(11):6073–6105, 1995.
[BDCF+22] Alessandro Barp, Lancelot Da Costa, Guilherme Franc¸a, Karl J Friston, Mark Girolami, Mi-
chael I Jordan, and Grigorios A Pavliotis. Geometric methods for sampling, optimisation,
inference and adaptive agents. 2022. Preprint arXiv:2203.10592.
[BFP16] John C Baez, Brendan Fong, and Blake S Pollard. A compositional framework for markov
processes. Journal of Mathematical Physics, 57(3):033301, 2016.
[BFR19] Romeo Brunetti, Klaus Fredenhagen, and Pedro Lauridsen Ribeiro. Algebraic structure of
classicalfieldtheory: kinematicsandlinearizeddynamicsforrealscalarfields.Communications
in Mathematical Physics, 368:519–584, 2019.
[BGMS21] John C Baez, Fabrizio Genovese, Jade Master, and Michael Shulman. Categories of nets. In
2021 36th Annual ACM/IEEE Symposium on Logic in Computer Science (LICS),pages1–13.
IEEE, 2021.
[BHZ19] Joe Bolt, Jules Hedges, and Philipp Zahn. Bayesian open games. 2019. Preprint
arXiv:1910.03656.
[Ble81] DavidDBleecker.GaugeTheoryandVariationalPrinciples.Globalanalysis.Addison-Wesley,
1981.
[BM20] JohnCBaezandJadeMaster.Openpetrinets.MathematicalStructuresinComputerScience,
30(3):314–341, 2020.
[BTB+21] Alessandro Barp, So Takao, Michael Betancourt, Alexis Arnaudon, and Mark Giro-
lami. A unifying and canonical description of measure-preserving diffusions. 2021. Preprint
arXiv:2105.02845.
[CDC15] GregorChliamovitch,AlexandreDupuis,andBastienChopard.Maximumentropyraterecon-
struction of Markov dynamics. Entropy, 17(6):3738–3751, 2015.
[CGHR21] MatteoCapucci,BrunoGavranovi´c,JulesHedges,andEigilFjeldgrenRischel.Towardsfound-
ations of categorical cybernetics. 2021. Preprint arXiv:2105.06332.
[Cou20] Kenny Courser. Open systems: a double categorical perspective. 2020. Preprint
arXiv:2008.02394.
[CRV+18] Axel Constant, Maxwell J D Ramstead, Samuel P L Veissi`ere, John O Campbell, and Karl J
Friston. A variational approach to niche construction. Journal of the Royal Society Interface,
15(141):20170685, 2018.
[DCFHP21] LancelotDaCosta,KarlJFriston,ConorHeins,andGrigoriosAPavliotis.Bayesianmechanics
for stationary processes. Proceedings of the Royal Society A, 477(2256):20210518, 2021.
[End17] Robert G Endres. Entropy production selects nonequilibrium states in multistable systems.
Scientific Reports, 7(1):1–13, 2017.
[FCAL16] Chelsea Finn, Paul Christiano, Pieter Abbeel, and Sergey Levine. A connection between gen-
erative adversarial networks, inverse reinforcement learning, and energy-based models. 2016.
Preprint arXiv:1611.03852.
[FCS+22] Karl J Friston, Lancelot Da Costa, Noor Sajid, Conor Heins, Kai Ueltzho¨ffer, Grigorios A
Pavliotis,andThomasParr.Thefreeenergyprinciplemadesimplerbutnottoosimple.2022.
Preprint arXiv:2201.06387.
[FFZ+21] Karl J Friston, Erik D Fagerholm, Tahereh S Zarghami, Thomas Parr, Inˆes Hipo´lito, Lo¨ıc
Magrou, and Adeel Razi. Parcels and particles: Markov blankets in the brain. Network Neur-
oscience, 5(1):211–251, 2021.
48 DALTONARSAKTHIVADIVEL
[FGL21] Chris Fields, James F Glazebrook, and Michael Levin. Minimal physicalism as a scale-free
substrate for cognition and consciousness. Neuroscience of Consciousness, 2021(2):niab013,
2021.
[FH21] DanielSFreedandMichaelJHopkins.Reflectionpositivityandinvertibletopologicalphases.
Geometry and Topology, 25:1165–1330, 2021.
[FHU+21] KarlJFriston,ConorHeins,KaiUeltzho¨ffer,LancelotDaCosta,andThomasParr.Stochastic
chaos and Markov blankets. Entropy, 23(9):1220, 2021.
[FKH06] KarlJFriston,JamesKilner,andLeeHarrison.Afreeenergyprincipleforthebrain.Journal
of Physiology-Paris, 100(1):70–87, 2006.
[Fon16] Brendan Fong. The algebra of open and interconnected systems. 2016. Preprint
arXiv:1609.05382.
[Fri10] KarlJFriston.Thefreeenergyprinciple: aunifiedbraintheory? NatureReviewsNeuroscience,
11(2):127–138, 2010.
[Fri12] KarlJFriston.Afreeenergyprincipleforbiologicalsystems.Entropy,14(11):2100–2121,2012.
[Fri19] KarlJFriston.Afreeenergyprincipleforaparticularphysics.2019.PreprintarXiv:1906.10184.
[GB20] Sebastian Gottwald and Daniel A Braun. The two kinds of free energy and the Bayesian
revolution. PLoS Computational Biology, 16(12):e1008420, 2020.
[Gra77] Robert Graham. Covariant formulation of non-equilibrium statistical thermodynamics. Zeits-
chrift fu¨r Physik B Condensed Matter, 26(4):397–405, 1977.
[GT12] Dmitri Gitman and Igor V Tyutin. Quantization of Fields with Constraints. Springer, 2012.
[Hey08] FrancisHeylighen.Complexityandself-organization.EncyclopediaofLibraryandInformation
Sciences, 3:1215–1224, 2008.
[HK64] Rudolf Haag and Daniel Kastler. An algebraic approach to quantum field theory. Journal of
Mathematical Physics, 5(7):848–861, 1964.
[Hoh16] Jakob Hohwy. The self-evidencing brain. Nouˆs, 50(2):259–285, 2016.
[HT94] MarcHenneauxandClaudioTeitelboim.QuantizationofGaugeSystems.PrincetonUniversity
Press, 1994.
[HY10] ScottHottonandJeffYoshimi.Thedynamicsofembodiedcognition.InternationalJournalof
Bifurcation and Chaos, 20(4):943–972, 2010.
[HY11] Scott Hotton and Jeff Yoshimi. Extending dynamical systemstheory to model embodied cog-
nition. Cognitive Science, 35(3):444–479, 2011.
[HZE17] JordanMHorowitz,KevinZhou,andJeremyLEngland.Minimumenergeticcosttomaintain
a target nonequilibrium state. Physical Review E, 95(4):042102, 2017.
[Jay57] Edwin T Jaynes. Information theory and statistical mechanics. Physical Review, 106(4):620–
630, 1957.
[JB03] EdwinTJaynesandGLarryBretthorst.ProbabilityTheory: TheLogicofScience.Cambridge
University Press, 2003.
[JPR19] Kate Jeffery, Robert Pollack, and Carlo Rovelli. On the statistical mechanics of life:
Schr¨odinger revisited. Entropy, 21(12), 2019.
[KFGL20] Franz Kuchling, Karl J Friston, Georgi Georgiev, and Michael Levin. Morphogenesis as
Bayesianinference: avariationalapproachtopatternformationandcontrolincomplexbiolo-
gical systems. Physics of Life Reviews, 33:88–108, 2020.
[Kie20] AlexBKiefer.Psychophysicalidentityandfreeenergy.JournalofTheRoyalSocietyInterface,
17(169):20200370, 2020.
[KOG+19] RitheshKumar,SherjilOzair,AnirudhGoyal,AaronCourville,andYoshuaBengio.Maximum
entropy generators for energy-based models. arXiv preprint arXiv:1901.08508, 2019.
[Lan98] Saunders Mac Lane. Categories for the Working Mathematician. Graduate Texts in Mathem-
atics. Springer New York, 1998.
[Law69] F William Lawvere. Adjointness in foundations. Dialectica, 23:281–296, 1969. Available from
Reprints in Theory and Applications of Categories, 16:1–16, 2006, https://www.emis.de/
journals/TAC/reprints/articles/16/tr16abs.html.
[LCH+07] YannLeCun,SumitChopra,RaiaHadsell,MarcAurelioRanzato,andFuJieHuang.Atutorial
on energy-based learning. In Predicting Structured Data. MIT Press, 2007.
[Led11] Michel Ledoux. Analytic and geometric logarithmic Sobolev inequalities. Journ´ees E´quations
aux D´eriv´ees Partielles, pages 1–15, 2011.
TOWARDS A GEOMETRY AND ANALYSIS FOR BAYESIAN MECHANICS 49
[Lev22] Michael Levin. Technological approach to mind everywhere: an experimentally-grounded
framework for understanding diverse bodies and minds. Frontiers in Systems Neuroscience,
16, 2022.
[LN92] PatrickALeeandNaotoNagaosa.Gaugetheoryofthenormalstateofhigh-T superconduct-
c
ors. Physical Review B, 46(9):5621–5641, 1992.
[Lur09] JacobLurie.DerivedalgebraicgeometryV:structuredspaces.2009.PreprintarXiv:0905.0459.
[LW05] Michael A Levin and Xiao-Gang Wen. String-net condensation: a physical mechanism for
topological phases. Physical Review B, 71(4):045110, 2005.
[Mae21] Christian Maes. Local detailed balance. SciPost Physics Lecture Notes, 32:1–17, 2021.
[ME15] RobertMarslandandJeremyLEngland.Far-from-equilibriumdistributionfromnear-steady-
state work fluctuations. Physical Review E, 92(5):052120, 2015.
[ME17] Robert Marsland and Jeremy L England. Limits of predictions in thermodynamic systems: a
review. Reports on Progress in Physics, 81(1):016601, 2017.
[MSB21] Beren Millidge, Anil K Seth, and Christopher L Buckley. A mathematical walkthrough and
discussion of the free energy principle. 2021. Preprint arXiv:2108.13343.
[MTB21] Beren Millidge, Alexander Tschantz, and Christopher L Buckley. Whence the expected free
energy? Neural Computation, 33(2):447–482, 2021.
[MTSB20] Beren Millidge, Alexander Tschantz, Anil K Seth, and Christopher L Buckley. On the rela-
tionshipbetweenactiveinferenceandcontrolasinference.InFirstInternationalWorkshopon
Active Inference 2020, pages 3–11, 2020. Preprint arXiv:2006.12964.
[MV00] Peter A Markowich and C´edric Villani. On the trend to equilibrium for the Fokker-Planck
equation: an interplay between physics and functional analysis. Matema´tica Contemporaˆnea
(SBM), 19:1–29, 2000.
[Nak03] Mikio Nakahara. Geometry, Topology and Physics. Taylor & Francis, second edition, 2003.
[Niv09] RobertKNiven.Steadystateofadissipativeflow-controlledsystemandthemaximumentropy
production principle. Physical Review E, 80(2):021113, 2009.
[Niv10] Robert K Niven. Minimization of a free-energy-like potential for non-equilibrium flow sys-
tems at steady state. Philosophical Transactions of the Royal Society B: Biological Sciences,
365(1545):1323–1331, 2010.
[PDCF20] Thomas Parr, Lancelot Da Costa, and Karl J Friston. Markov blankets, information geo-
metry and stochastic thermodynamics. Philosophical Transactions of the Royal Society A,
378(2164):20190159, 2020.
[PF19] Thomas Parr and Karl J Friston. Generalised free energy and active inference. Biological
Cybernetics, 113(5):495–513, 2019.
[PGLD13a] Steve Press´e, Kingshuk Ghosh, Julian Lee, and Ken A Dill. Nonadditive entropies yield
probability distributions with biases not warranted by the data. Physical Review Letters,
111(18):180604, 2013.
[PGLD13b] Steve Press´e, Kingshuk Ghosh, Julian Lee, and Ken A Dill. Principles of maximum entropy
and maximum caliber in statistical physics. Reviews of Modern Physics, 85(3):1115–1141, Jul
2013.
[Pin99] Allan Pinkus. Approximation theory of the MLP model in neural networks. Acta Numerica,
8:143––195, 1999.
[Pok20] Vladimir N Pokrovskii. Thermodynamics of Complex Systems. IOP Publishing, 2020.
[Ram21] MaxwellJDRamstead.Theempirestrikesback: someresponsestoBruinebergandcolleagues.
2021. Preprint arXiv:2112.15528.
[RBF18] Maxwell J D Ramstead, Paul B Badcock, and Karl J Friston. Answering Schro¨dinger’s ques-
tion: a free energy formulation. Physics of Life Reviews, 24:1–16, 2018.
[RFH20] MaxwellJDRamstead,KarlJFriston,andInˆesHipo´lito.Isthefree-energyprincipleaformal
theory of semantics? From variational density dynamics to neural and phenotypic represent-
ations. Entropy, 22(8):889, 2020.
[RKF20] Maxwell J D Ramstead, Michael D Kirchhoff, and Karl J Friston. A tale of two densities:
active inference is enactive inference. Adaptive Behavior, 28(4):225–239, 2020.
[RKLM22] Wiktor Rorot, Tomasz Korbak, Piotr Litwin, and Marcin Mil(cid:32)kowski. Enough blanket meta-
physics, time for data-driven heuristics. 2022. PsyArXiv preprint 10.31234/osf.io/9r4np.
50 DALTONARSAKTHIVADIVEL
[Ros58a] Robert Rosen. A relational theory of biological systems. The Bulletin of Mathematical Bio-
physics, 20(3):245–260, 1958.
[Ros58b] Robert Rosen. The representation of biological systems from the standpoint of the theory of
categories. The Bulletin of Mathematical Biophysics, 20(4):317–341, 1958.
[RVB+21] VicenteRaja,DineshValluri,EdwardBaggs,AnthonyChemero,andMichaelLAnderson.The
Markov blanket trick: on the scope of the free energy principle and active inference. Physics
of Life Reviews, 39:49–72, 2021.
[RW02] ValeryRubakovandStephenSWilson.ClassicalTheoryofGaugeFields.PrincetonUniversity
Press, 2002.
[Sak22a] DaltonARSakthivadivel.Aconstraintgeometryforinferenceandintegration.2022.Preprint
arXiv:2203.08119.
[Sak22b] Dalton A R Sakthivadivel. Magnetisation and mean field theory in the Ising model. SciPost
Physics Lecture Notes, 35:1–16, 2022.
[Sei12] Udo Seifert. Stochastic thermodynamics, fluctuation theorems and molecular machines. Re-
ports on Progress in Physics, 75(12):126001, 2012.
[SF17] BiswaSenguptaandKarlJFriston.ApproximateBayesianinferenceasagaugetheory.2017.
Preprint arXiv:1705.06614.
[Sie18] Ivo Siekmann. An applied mathematician’s perspective on Rosennean complexity. Ecological
Complexity, 35:28–38, 2018.
[Smi21] Toby St Clere Smithe. Compositional active inference I: Bayesian lenses. Statistical games.
2021. Preprint arXiv:2109.04461.
[STC+16] Biswa Sengupta, Arturo Tozzi, Gerald K Cooray, Pamela K Douglas, and Karl J Friston.
Towards a neuronal gauge theory. PLOS Biology, 14(3):1–12, 03 2016.
[TL04] Hugo Touchette and Seth Lloyd. Information-theoretic approach to the study of control sys-
tems. Physica A: Statistical Mechanics and its Applications, 331(1-2):140–172, 2004.
[UDCCF21] KaiUeltzho¨ffer,LancelotDaCosta,DanielaCialfi,andKarlJFriston.Adrivetowardsthermo-
dynamicefficiencyfordissipativestructuresinchemicalreactionnetworks.Entropy,23(9):1115,
2021.
[UDCF21] KaiUeltzho¨ffer,LancelotDaCosta,andKarlJFriston.Variationalfreeenergy,individualfit-
ness,andpopulationdynamicsunderacutestress: commenton“DynamicandThermodynamic
Models of Adaptation” by Alexander N Gorban et al. Physics of Life Reviews, 37:111–115,
2021.
[Uel20] Kai Ueltzho¨ffer. On the thermodynamics of prediction under dissipative adaptation. 2020.
Preprint arXiv:2009.04006.
[VBM22] Nathaniel Virgo, Martin Biehl, and Simon McGregor. Interpreting dynamical systems as
Bayesian reasoners. In Second International Workshop on Active Inference 2021, pages 726–
762, 2022. Preprint arXiv:2112.13523.
[Wie48] NorbertWiener.Cybernetics or Control and Communication in the Animal and the Machine.
The MIT Press, 2019 edition, 1948.
[Wit18] Edward Witten. Symmetry and emergence. Nature Physics, 14(2):116–119, 2018.
[Yos11] Jeff Yoshimi. Phenomenology and connectionism. Frontiers in Psychology, 2:288, 2011.
[ZS07] Royce K P Zia and Beate Schmittmann. Probability currents as principal characteristics in
the statistical mechanics of non-equilibrium steady states. Journal of Statistical Mechanics:
Theory and Experiment, 2007(7):P07012, 2007.
Departments of Mathematics, Physics and Astronomy, and Biomedical Engineering, Stony
Brook University, Stony Brook, NY, USA, 11794-3651
VERSES Research Lab and Spatial Web Foundation, Los Angeles, CA, USA, 90016
Email address: dalton.sakthivadivel@stonybrook.edu
URL: https://darsakthi.github.io

=== REVISE TO ===
PROFESSIONAL TONE: Begin directly with content - NO conversational openings like 'Okay, here's...'

1. Fix all issues above
2. Title: "Towards a Geometry and Analysis for Bayesian Mechanics"
3. Include 10-15 quotes from paper text
   - Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
   - Use consistent quote formatting: 'The authors state: "quote"' or vary attribution phrases
   - Vary attribution phrases to avoid repetition
   - CRITICAL: Only extract quotes that actually appear in the paper text
4. ELIMINATE ALL REPETITION - each sentence must be unique
   - Check before each sentence: 'Have I already said this?' If yes, write something new
   - Vary attribution phrases - do NOT repeat 'The authors state' multiple times
5. Extract methodology, results with numbers, key quotes
6. 1000-1500 words, structured with ### headers

Generate COMPLETE revised summary.