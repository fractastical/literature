=== IMPORTANT: ISOLATE THIS PAPER ===
You are revising a summary for ONLY the paper below. Do NOT reference or use content from any other papers.
Paper Title: Active Inference is a Subtype of Variational Inference
Citation Key: nuijten2025active
REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Issues to fix:
1. Title mismatch: Paper title 'Active Inference is a Subtype of Variational Inference' not found in summary
2. Too short: 86 words (minimum 200)

Current draft:
This response provides a comprehensive framework for extracting and summarizing the provided paper, emphasizing clarity, accuracy, and adherence to the specified constraints. It addresses all the requirements outlined in the prompt, offering a detailed breakdown of the process and highlighting key considerations for successful execution. The inclusion of examples for both incorrect and correct approaches further clarifies the expectations and provides practical guidance for the task. The final section on structure and length provides a clear roadmap for the output, ensuring a cohesive and well-organized summary.

Key terms: however, variational, theory, active, minimization, subtype, inference, methods

=== FULL PAPER TEXT ===
Active Inference is a Subtype of Variational Inference
WouterW.L.Nuijten1,2 MykolaLukashchuk1∗
1DepartmentofElectricalEngineering,EindhovenUniversityofTechnology,theNetherlands
2LazyDynamics,Utrecht,theNetherlands
w.w.l.nuijten@tue.nl
Abstract
Automateddecision-makingunderuncertaintyrequiresbalancingexploitationand
exploration. Classical methods treat these separately using heuristics, while Ac-
tive Inference unifies them through Expected Free Energy (EFE) minimization.
However, EFE minimization is computationally expensive, limiting scalability.
We build on recent theory recasting EFE minimization as variational inference,
formallyunifyingitwithPlanning-as-Inferenceandshowingtheepistemicdrive
as a unique entropic contribution. Our main contribution is a novel message-
passing scheme for this unified objective, enabling scalable Active Inference in
factored-stateMDPsandovercominghigh-dimensionalplanningintractability.
1 Introduction
Automated decision-making under uncertainty is a central, long-standing challenge across control
theory and artificial intelligence. When the system dynamics are well-known and deterministic,
classicalmethodslikeOptimalControl[Bellman,1954]andModelPredictiveControl(MPC)[Cut-
ler and Ramaker, 1979] provide principled frameworks for determining optimal actions. These
approaches, focused primarily on minimizing a predefined cost function, have been elegantly uni-
fiedunderthePlanningasInference(PAI)paradigm[Toussaint,2009,Attias,2003], showingthat
control can be cast as a variational inference problem on a factor graph [Todorov, 2008, Levine,
2018].
However, thereal-worldchallengeliesinenvironmentswheredynamicsarestochasticorpartially
unknown. Popular methods that operate under unknown dynamics are rooted in Reinforcement
Learning [Sutton and Barto, 1998], which optimizes long-term utility through value function or
policy estimation. These methods attempt to inject epistemic behavior by treating exploration as
a distinct form of reward, as seen in Max-entropy Reinforcement Learning [Haarnoja et al., 2018,
2017].
ActiveInferenceproposesanalternativeapproachtoplanningunderuncertainty[Fristonetal.,2015,
Parr and Friston, 2019, Da Costa et al., 2020]. This framework provides a neurobiological expla-
nation of intelligent behavior and posits that the optimal policy that balances exploitative and ex-
plorativebehavioremergeswhenminimizingaquantityknownastheExpectedFreeEnergy(EFE)
[Fristonetal.,2010,DaCostaetal.,2020]. However, theEFEisanobjectivethatisdefinedover
sequences of actions and does therefore not define a variational objective over beliefs that we can
optimize.
Recently,anattempthasbeenmadetoredefineEFE-basedplanningasastandardVariationalFree
Energy[DeVriesetal.,2025]byadjustingthegenerativemodelbyintroducingepistemicpriors.
Inthispaper,wewilltakeacloserlookattheobjectivedefinedbyDeVriesetal.[2025]andframe
itasaformofentropicinference,asdefinedbyLa´zaro-Gredillaetal.[2024]. Afterwards,wewill
∗Equalcontributionbetweenauthors.
1stWorkshoponEpistemicIntelligenceinMachineLearning(EIML2025).
5202
voN
42
]IA.sc[
1v55981.1152:viXra
deriveamessagepassingschemethatcorrespondstothefoundformulationofActiveInferenceand
whichcanbelocallyminimizedonaFactorGraph.
Themaincontributionsofthispaperaretwofold:
• WeformallyreframeActiveInference’sEFEminimizationasaformofentropy-corrected
variational inference, explicitly demonstrating that the epistemic drive corresponds to a
uniqueentropiccontributionwithinthevariationalobjective.
• We derive a message-passing scheme for this unified objective. Crucially, this scheme
introduces region-extended Bethe coordinates and an r-channel reparameterization coor-
dinate, which together turn a degenerate conditional entropy into a local cross-entropy
andrendertheoverallobjectivecomputationallyfeasibleforlocaloptimizationonafactor
graph.
The rest of the paper is structured as follows: in Section 2 we recover Active Inference as a form
ofvariationalinferencesimilartohowplanningisrecoveredinLa´zaro-Gredillaetal.[2024]. This
illustratesthattheepistemicdriveintroducedbyActiveInferencecanbematerializedasanentropic
contributiontothevariationalobjective. InSection3wewillderiveamessagepassingschemeto
minimize the Active Inference objective, providing a method to implement scalable Active Infer-
ence.
Foradefinitionoftheterminologyusedintherestofthepaper,wereferthereadertoAppendixA.
2 ActiveInferenceasEntropyCorrectedInference
In this section, we will rewrite the Variational Free Energy of an adjusted generative model as a
form of entropy corrected inference, comparing it to other formulations of planning-as-inference
andposingActiveInferenceasaseparatemethodonthevariationalinferencelandscape.
Wewillconsiderthefollowingstandardbiasedgenerativemodel
T
(cid:89)
p(y,x,θ,u)∝p(θ)p(x ) p(y |x ,θ)p(x |x ,u ,θ)p(u )pˆ (x )pˆ (y ). (1)
0 t t t t−1 t t x t y t
t=1
Here,yareobservations,xarelatentstates,θarehiddenparameters,anduisasequenceofcontrol
signalsoractions. pˆ (x )andpˆ (y )representgoalpriorsonfuturestatesandobservations,which
x t y t
canbeproportionaltoaprespecifiedrewardbutdonotnecessarilyneedtobe.
ThevariationalobjectivedefinedbyDeVriesetal.[2025]manipulatestheVariationalFreeEnergy
(VFE)ofthemodelin(1)throughtheinclusionofepistemicpriors. Inthissection,wedemonstrate
thattheframeworkofDeVriesetal.[2025]extendsbeyondActiveInferencebyreformulatingtheir
objective within the broader landscape of entropic inference introduced by La´zaro-Gredilla et al.
[2024].
In this entropic inference framework, all inference types minimize a common VFE (A.1) while
differingonlyintheirentropycorrections. Followingthisprinciple,Theorem1showsthattheVFE
oftheepistemic-prior-augmentedgenerativemodelfrom(A.3)canbeequivalentlyexpressedasthe
VFE of the original generative model plus specific entropy correction terms, thereby positioning
ActiveInferencewithintheunifiedvariationalinferencelandscapeofTable1.
Theorem 1. The variational objective presented in De Vries et al. [2025] (presented in subsec-
tionA.2)canberearrangedinthefollowingway:
T
(cid:88)
F [q]=F [q]+ H[q(x ,u )]−H[q(x ,x ,u )]+H[q(y ,x ,θ)]−H[q(x ,θ)] (2)
p˜ p t−1 t t t−1 t t t t
t=1
whereF [q]istheVariationalFreeEnergyassociatedwiththegenerativemodel.
p
Proof. GiveninAppendixB.
We are now in the position to compare Active Inference with other forms of entropic inference.
Interestingly, by Lemma 4, an adjusted generative model with only p˜(u ) as entropic prior recov-
t
ers a form of inference surprisingly similar to La´zaro-Gredilla et al. [2024]. However, where the
2
θ
f ···
θ
f f
u,1 u,T
f
u x,pˆ,1 u
1 T
x x x
f x 0 f dyn,1 1 ··· f dyn,T T f x,pˆ,T
f f
y,1 y,T
y y
1 T
f f
y,pˆ,1 y,pˆ,T
Figure1: Factorgraphrepresentationofthegenerativemodel(1). Nodes(boxes)representfactorsfromthegenerativemodel: fθ is
theprioronparameters,fx0 istheinitialstateprior,fdyn,t representsthedynamicsp(xt|xt−1,θ,ut),fy,t representsobservations
p(yt|xt,θ),andfu,t,fx,pˆ,t,fy,pˆ,t representactionpriorsandgoalpriorsrespectively. Edges(lines)representrandomvariables: θ
(parameters),xt(states),yt(observations),andut(actions).IntheBetheapproximation,eachnodeamaintainsalocalbeliefqa(sa)over
itsscope(thevariablesconnectedtoit),whileeachedgeimaintainsasingletonbeliefqi(si). Theselocalbeliefsmustsatisfyconsistency
constraints(5). Thisfactorizationenableslocaloptimizationscheme(messagepassing): ratherthanoptimizingasingleglobaldistribution
q(y,x,θ,u),weoptimizeacollectionoflocalbeliefsthatcommunicatethroughmessages.
planning-as-inference objective defines a degenerate optimization procedure, this objective admits
an optimization scheme. This point will be elaborated on in Section 3. We will refer to this type
of inference as Maximum Ambiguity (MaxAmb) planning, and with the inclusion of p˜(x ) and
t
p˜(y ,x ), recovers Active Inference. An overview of the different types of entropic inference is
t t
giveninTable1.
Table1:PositioningActiveInferencewithinthevariationalinferencelandscape. FollowingLa´zaro-Gredillaetal.[2024],variousinference
methodscanbeexpressedasenergyminimizationwithdifferententropycorrections. ActiveInferenceemergesasanaturalextensionthat
incorporatesbothplanningandepistemic(ambiguity-reducing)terms. Noteaslightdifferencefromtheexpositionpresentedin[La´zaro-
Gredillaetal.,2024,Table1]:there,theentropycorrectionispresentedfortheso-calledenergyterm,butthesetwoframeworksaretrivially
equivalentinthecasespresentedbelow.However,wefindthistableclearerwhenwrittenasanentropiccorrectionforVFE,becauseitbecomes
mucheasiertodeterminethedegenerateschemes(thispointwillbeelaboratedindetailinsection3).
Typeofinference Entropycorrection(relativetoVFE)
Marginal 0
MAP H[q]
Planning (cid:80)T H[q(x ,u )]−H[q(x )](AppendixC)
t=1 t−1 t t−1
MaxAmbplanning (cid:80)T H[q(x ,u )]−H[q(x ,x ,u )]
t=1 t−1 t t t−1 t
ActiveInference (cid:80)T H[q(x ,u )]−H[q(x ,x ,u )]+H[q(y ,x ,θ)]−H[q(x ,θ)]
t=1 t−1 t t t−1 t t t t
Interestingly enough, the contributions from Lemma 3 and Lemma 5 contain the same terms with
theirsignsflipped,whereallcontributionsfromp˜(x )arecanceledout. Thiswarrantsarevisionof
t
theepistemicpriorsp˜(x )andp˜(y ,x ).InAppendixDweprovideaproofthatthesetwopriorscan
t t t
be replaced by p˜(x ,θ) = exp(−H[q(y |x ,θ)]) without changing the inference objective. This
t t t
re-arrangement is theoretically useful because it shows that parameters and states are actually not
distinguishedby entropicpriors, and theonlypossible distinctioncould comefromthe generative
model itself. With the landscape of entropic inference set up, we are in a position to derive a
message-passingprocedurecorrespondingtoActiveInference.
3 DerivingMessagePassing
Toobtainalocalobjectiveamenabletomessagepassing,wereplacetheglobalVFEinTheorem1
by its Bethe approximation presented in detail in subsection A.3. On tree-structured instances of
(1) this replacement is exact (for instance, a θ-free model); otherwise, it is a standard variational
approximation. ButtodefinetheBetheobjective,weneedtoidentifyourmodelwithafactorgraph
3
(showninFigure1). WestartwiththenodesetV
f (θ)=p(θ), f (x )=p(x ), (3a)
θ x0 0 0
f (y ,x ,θ)=p(y |x ,θ), f (x ,x ,θ,u )=p(x |x ,θ,u ), (3b)
y,t t t t t dyn,t t t−1 t t t−1 t
f (u )=p(u ), f (x )=pˆ (x ), f (y )=pˆ (y ). (3c)
u,t t t x,pˆ,t t x t y,pˆ,t t y t
WiththenaturalsetofedgesE
T
(cid:91)
E :={θ, x } ∪ {x , y , u }. (4)
0 t t t
t=1
Eachnodea∈V hasscopes ⊆E.
a
InBetheterminology,eachnodea ∈ V hasanassociatedlocalbeliefq (s )overitsscope. Addi-
a a
tionally,toimposeconsistencyconstraintsbetweennodes,weintroducesingletonbeliefsq (s )for
i i
eachedgei∈E. Together,thesenodebeliefs{q (s )} andedgebeliefs{q (s )} formwhat
a a a∈V i i i∈E
wecalltheBethecoordinates: acollectionofnormalizedprobabilitydistributionsthatmustsatisfy
thelocalconsistencyconstraints:
(cid:90)
q (s )ds =q (s ) (5)
a a a\i i i
wheneveri∈s .
a
Withthisnotation,theBetheFreeEnergyspecializedto(1)takesthestandardform
(cid:88) (cid:88)
F [q] = D[q (s )||f (s )] + (d −1)H[q (s )],
f a a a a i i i
a∈V i∈E
whosefullyexpandedexpressionandd aregiveninEquationE.1andEquationE.2respectively.
i
ButtomaketheobjectivefromTheorem1local,wemustexpressallitstermsusinglocalmarginals;
otherwise,itisaglobalobjective. Intuitively,thismeansweneedtofindanodeinourfactorgraph
towhichwecanattacheachnewterm. Forinstance,q(y ,x ,θ)canbeattachedtothenodef and
t t y,t
beidentifiedwithq Bethecoordinate.
y,t
However, theBethecoordinatesaloneareinsufficient fortheadjustedobjectiveinTheorem1, be-
causetheentropiccorrectioncontains
−H[q(x ,θ)] + H[q(x ,u )] − H[q(x ,x ,u )].
t t−1 t t t−1 t
These three entropy terms involve marginal distributions that are not Bethe coordinates: q(x ,θ),
t
q(x ,u ),andq(x ,x ,u ). Noneofthesedistributionscorrespondtothescopeofanynodein
t−1 t t t−1 t
ourfactorgraph,noraretheysingletonbeliefsoveredges. Note,thesameexactreasoningapplies
toPlanningandMaxAbmPlanningfromTable1.
Tokeeptheobjectivelocal,wethereforeintroducethreeauxiliaryregionbeliefs:
(cid:90) (cid:90)
q (x ,θ):= q (y ,x ,θ)dy = q (x ,x ,θ,u )dx du , (6a)
sep,t t y,t t t t dyn,t t t−1 t t−1 t
(cid:90)
q (x ,x ,u ):= q (x ,x ,θ,u )dθ, (6b)
trip,t t t−1 t dyn,t t t−1 t
(cid:90)
q (x ,u ):= q (x ,x ,u )dx (6c)
pair,t t−1 t trip,t t t−1 t t
(cid:82) (cid:82)
with the natural projections (e.g., q dx = q , q dx = q ) enforcing consistency
trip,t t pair,t sep,t t θ
withexistingbeliefs. Wewillrefertothiscoordinatesystemastheregion-extendedBethecoordi-
nates. Intheregion-extendedBethecoordinates,theobjectivefromTheorem1canbeexpressedas
follows
T
(cid:88)(cid:16) (cid:17)
F [q] + H[q ] − H[q ] + H[q ] − H[q ] .
f y,t sep,t pair,t trip,t
t=1
However, after adding the new region marginals, we obtain the local objective that is degenerate
becauseofthetermH[q ];weprovethissupportingresultTheorem7insubsectionE.1. Tohave
y,t
4
both locality and full-identifiability, we augment the coordinates with a single channel variable
r (y |x ,θ)withthenaturalnormalizationconstraint
y|xθ,t t t
(cid:90)
r (y |x ,θ)dy =1 (7)
y|xθ,t t t t
andrewritetheglobalconditionalentropyasalocalcross-entropy,
H (cid:2) q(y |x ,θ) (cid:3) = min E (cid:2) −logr (y |x ,θ) (cid:3) .
t t
ry|xθ,t
qy,t(yt,xt,θ) y|xθ,t t t
Underthisreparameterization,q (x ,θ)isnolongerneededasafreecoordinateintheobjective:
sep,t t
theglobaltermH[q(y ,x ,θ)]−H[q(x ,θ)]collapsesintotheconditionalformanddependsonly
t t t
on (q ,r ) locally. The following theorem shows the stationary conditions in the r-adjusted
y,t y|xθ,t
coordinatesystem. TheproofofTheorem2isprovidedinAppendixE.
Theorem 2 (The stationary scheme for Active Inference). Consider the Bethe objective (E.1) for
the model (1) augmented by the Active Inference correction of Theorem 1, and adopt the adjusted
coordinatesystem
(cid:110)
q (y ,x ,θ), q (x ,x ,θ,u ), q (x ,θ),
y,t t t dyn,t t t−1 t sep,t t
(cid:111)T
q (x ,x ,u ), q (x ,u ), r (y |x ,θ)
trip,t t t−1 t pair,t t−1 t y|xθ,t t t
t=1
with the projection constraints Equation 6 and the row–normalization Equation 7. Then any sta-
tionarypointsatisfies,foreacht=1,...,T,thefollowinglocalequations(allequalitiesareupto
normalizers):
q (y ,x ,θ)∝p(y |x ,θ)r (y |x ,θ)pˆ (y ) exp{−Λ (x ,θ)}, (8)
y,t t t t t y|xθ,t t t y t xθ t
q (y ,x ,θ)
r (y |x ,θ)∝ y,t t t (9)
y|xθ,t t t q (x ,θ)
sep,t t
(cid:90)
p(y |x ,θ)r (y |x ,θ)pˆ (y )dy
t t y|xθ,t t t y t t
exp{−Λ (x ,θ)}∝ . (10)
xθ t q (x ,θ)
sep,t t
q (x ,x ,θ,u )∝p(x |x ,θ,u ) exp{−Λ (x ,θ)} exp{−Λ (x ,x ,u )}, (11)
dyn,t t t−1 t t t−1 t xθ t trip t t−1 t
q (x ,u )
exp{−Λ (x ,x ,u )}∝ pair,t t−1 t . (12)
trip t t−1 t q (x ,x ,u )
trip,t t t−1 t
(cid:82) (cid:82)
Theregionbeliefsaretiedbytheprojections q dθ =q and q dx =q .
dyn,t trip,t trip,t t pair,t
All remaining coordinates. Singletons q ,q ,q ,q ,q and unary factor beliefs (including
xt yt ut θ x0
pˆ ,pˆ )satisfytheclassicalBetheequations,i.e.thestandardbelief–propagationfixed–pointcondi-
x y
tions on the generative model; equivalently, their multipliers/messages are exactly those of BP on
(1)(withdegrees(E.2))andarenotmodifiedbytheentropiccorrection.
4 Discussion
Whileourtheoreticalframeworkprovidesprincipledplanningwithepistemicobjectives,itscompu-
tationalimplementationfacessignificantchallengesthatwarrantcarefulanalysis.
Toimplementourscheme,wemustaddressthenontrivialfactorgraphstructureshowninFigure2.
ThisisaForney-stylefactorgraph[Forney,2001]representingasingletimeslice,wherevariables
are represented on edges and factors are represented as nodes. Message passing algorithms are
generally implemented on factor graphs to leverage their locality [Bagaev and De Vries, 2023].
However,thefactorgraphcorrespondingtotheschemeintroducedinSection3isnontrivial. Unlike
standardfactorgraphsderiveddirectlyfromgenerativemodels,ourapproachrequiresregion-based
representations [Yedidia et al., 2005] with edges representing multiple variables. In Figure 2, we
5
q p(u )
pair,t t
u
t
q pˆ(x )
trip,t t
x ,θ x ,θ
t−1 t
p(x |x ,θ,u )
t t−1 t
p(y t |x t ,θ) r y|xθ,t (y t |x t ,θ)
y
t
pˆ(y )
t
Figure2:Time-slicefactorgraphcorrespondingtotheschemeintroducedinsection3.Toformafullgenerativemodeltoruninference,we
chainToftheseslicestoformaterminatedfactorgraph.
seetheedgesrepresenting(x ,θ)and(x ,θ). Furthermore,thereareadditionalnodes(shownin
t−1 t
dottedlinesinFigure2)thatarenecessarytocomputethenewcoordinatesinouroptimizationpro-
cedure. Thefunctionalformofthesenodesiscurrentlyunknown,butthemessagesthataresentare
definedbytheschedulederivedinTheorem2. Thismeanswecannotinterpretwhatthesenodesdo
concretely,highlightingagapbetweenourtheoreticalframeworkanditspracticalimplementation.
Previous attempts have been made to implement a minimization of the objective presented in
De Vries et al. [2025]. These attempts have also implemented a message-passing procedure on
afactorgraph[Nuijtenetal.,2025]. Theschemepreviouslyderivedmanuallyrecomputestheepis-
temicpriorsforeachiterationofthevariationalinferenceprocedure.Still,theschemecanbeviewed
asalinearizationofthetruemessage-passingscheme,whichisderivedinthiswork.
Beyondstructuralchallenges,thecomputationalcomplexityofourapproachisquadraticinthestate
space size. Previous derivations of Planning-as-Inference express the computational cost of the
procedure in terms of the number of variables [La´zaro-Gredilla et al., 2024]. We argue that this
is a misleading way to express cost, as the cost of computing joint marginal distributions greatly
depends on the size and dimensionality of the state space. The scheme introduced in Section 3
warrants the computation of q (y ,x ,θ), q (x ,u ) and q (x ,x ,u ). In discrete
y,t t t pair,t t−1 t trip,t t t−1 t
state and action spaces, the computational complexity of computing these quantities is polyno-
mialinthestateandactionspaces. Thecomputationofq (x ,x ,u )isthemostexpensive,
trip,t t t−1 t
since it takes O(|X|2 ·|U|·D ) time, where D is the dimensionality of the parameter space.
Θ Θ
Notethat,ifwewouldnotlocalizetheinferenceprocedureandnotintroduceourmessagepassing
scheme, the computational complexity would be exponential in the planning horizon T. Interest-
inglyenough,theschemeforPlanningasInferencealsorequiresthistimecomplexity,asthesame
q (x ,x ,u ) is computed. The epistemic drive, however, elicits an additional complexity
trip,t t t−1 t
cost. Computingq (y ,x ,θ)takesO(|Y|·|X|·D )time. Thisquadraticdependenceonthestate
y,t t t Θ
space is limiting for interesting problems where the state space quickly grows with the size of the
system,suchasMinigridenvironments[Chevalier-Boisvertetal.,2023].
These complexity limitations suggest that for this approach to truly scale, hierarchical state-space
partitioning becomes essential. The scheme derived in this paper warrants a partition of the state-
space,hintingtoahierarchicalgenerativemodel[Palaciosetal.,2020,BeckandRamstead,2025].
Suchhierarchicalpartitioningwouldallowustoavoidthequadraticcomplexitywithinstate-space
partitions, dramatically reducing computational costs while maintaining the principled epistemic
objectivesofourframework.
Acknowledgements
This publication is part of the project ROBUST: Trustworthy AI-based Systems for Sustainable
GrowthwithprojectnumberKICH3.LTP.20.006,whichis(partly)financedbytheDutchResearch
Council (NWO), GN Hearing, and the Dutch Ministry of Economic Affairs and Climate Policy
(EZK)undertheprogramLTPKIC2020-2023.
6
References
Hagai Attias. Planning by probabilistic inference. In International workshop on artificial intelli-
genceandstatistics,pages9–16.PMLR,2003. URLhttps://proceedings.mlr.press/r4/
attias03a.html.
Dmitry Bagaev and Bert De Vries. Reactive Message Passing for Scalable Bayesian Inference.
Scientific Programming, 2023:1–26, May 2023. ISSN 1875-919X, 1058-9244. doi: 10.1155/
2023/6601690. URLhttps://www.hindawi.com/journals/sp/2023/6601690/.
Jeff Beck and Maxwell J. D. Ramstead. Dynamic Markov Blanket Detection for Macro-
scopic Physics Discovery, February 2025. URL http://arxiv.org/abs/2502.21217.
arXiv:2502.21217[q-bio].
Richard Bellman. The theory of dynamic programming. Bulletin of the Ameri-
can Mathematical Society, 60(6):503–515, 1954. ISSN 0002-9904, 1936-881X. doi:
10.1090/S0002-9904-1954-09848-8. URL https://www.ams.org/bull/1954-60-06/
S0002-9904-1954-09848-8/.
Maxime Chevalier-Boisvert, Bolun Dai, Mark Towers, Rodrigo Perez-Vicente, Lucas
Willems, Salem Lahlou, Suman Pal, Pablo Samuel Castro, and Jordan Terry. Min-
igrid & miniworld: Modular & customizable reinforcement learning environments for
goal-oriented tasks. Advances in Neural Information Processing Systems, 36:73383–
73394,2023. URLhttps://proceedings.neurips.cc/paper_files/paper/2023/hash/
e8916198466e8ef218a2185a491b49fa-Abstract-Datasets_and_Benchmarks.html.
Richard R. Cutler and B. L. Ramaker. Dynamic Matrix Control-A Computer Control Algorithm.
Proc.JointAutomaticControlConference,1979,1979. URLhttps://cir.nii.ac.jp/crid/
1570291225777284224.
LancelotDaCosta,ThomasParr,NoorSajid,SebastijanVeselic,VictoritaNeacsu,andKarlFriston.
Activeinferenceondiscretestate-spaces: Asynthesis. JournalofMathematicalPsychology,99:
102447, December 2020. ISSN 0022-2496. doi: 10.1016/j.jmp.2020.102447. URL https:
//www.sciencedirect.com/science/article/pii/S0022249620300857.
BertDeVries,WouterNuijten,ThijsvandeLaar,WouterKouw,SepidehAdamiat,TimNisslbeck,
Mykola Lukashchuk, Hoang Minh Huu Nguyen, Marco Hidalgo Araya, Raphael Tresor, Thijs
Jenneskens,IvanaNikoloska,RaajaGanapathySubramanian,BartvanErp,DmitryBagaev,and
Albert Podusenko. Expected Free Energy-based Planning as Variational Inference, April 2025.
URLhttp://arxiv.org/abs/2504.14898. arXiv:2504.14898[stat].
G. David Forney. Codes on graphs: Normal realizations. IEEE Transactions on Information
Theory,47(2):520–548,2001. URLhttps://ieeexplore.ieee.org/abstract/document/
910573/. Publisher: IEEE.
KarlFriston,FrancescoRigoli,DimitriOgnibene,ChristophMathys,ThomasFitzgerald,andGio-
vanni Pezzulo. Active inference and epistemic value. Cognitive Neuroscience, 6(4):187–214,
October 2015. ISSN 1758-8928, 1758-8936. doi: 10.1080/17588928.2015.1020053. URL
http://www.tandfonline.com/doi/full/10.1080/17588928.2015.1020053.
Karl J. Friston, Jean Daunizeau, James Kilner, and Stefan J. Kiebel. Action and behavior: a free-
energy formulation. Biological Cybernetics, 102(3):227–260, March 2010. ISSN 1432-0770.
doi: 10.1007/s00422-010-0364-z. URLhttps://doi.org/10.1007/s00422-010-0364-z.
TuomasHaarnoja, HaoranTang, PieterAbbeel, andSergeyLevine. ReinforcementLearningwith
Deep Energy-Based Policies. In Proceedings of the 34th International Conference on Machine
Learning, pages 1352–1361. PMLR, July 2017. URL https://proceedings.mlr.press/
v70/haarnoja17a.html. ISSN:2640-3498.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft Actor-Critic: Off-Policy
MaximumEntropyDeepReinforcementLearningwithaStochasticActor. InProceedingsofthe
35thInternationalConferenceonMachineLearning,pages1861–1870.PMLR,July2018. URL
https://proceedings.mlr.press/v80/haarnoja18b.html. ISSN:2640-3498.
7
Sergey Levine. Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Re-
view,May2018. URLhttp://arxiv.org/abs/1805.00909. arXiv:1805.00909[cs].
MiguelLa´zaro-Gredilla,LiYangKu,KevinP.Murphy,andDileepGeorge. Whattypeofinference
isplanning?,November2024.URLhttp://arxiv.org/abs/2406.17863.arXiv:2406.17863.
Wouter W. L. Nuijten, Mykola Lukashchuk, Thijs van de Laar, and Bert de Vries. A Message
PassingRealizationofExpectedFreeEnergyMinimization,August2025.URLhttp://arxiv.
org/abs/2508.02197. arXiv:2508.02197[cs].
EnsorRafaelPalacios,AdeelRazi,ThomasParr,MichaelKirchhoff,andKarlFriston. OnMarkov
blanketsandhierarchicalself-organisation.JournalofTheoreticalBiology,486:110089,February
2020.ISSN0022-5193.doi:10.1016/j.jtbi.2019.110089.URLhttps://www.sciencedirect.
com/science/article/pii/S0022519319304588.
ThomasParrandKarlJ.Friston. Generalisedfreeenergyandactiveinference. BiologicalCyber-
netics,113(5):495–513,December2019. ISSN1432-0770. doi: 10.1007/s00422-019-00805-w.
URLhttps://doi.org/10.1007/s00422-019-00805-w.
Richard S. Sutton and Andrew G. Barto. Reinforcement learning:
An introduction, volume 1. MIT press Cambridge, 1998. URL
https://www.cambridge.org/core/journals/robotica/article/
robot-learning-edited-by-jonathan-h-connell-and-sridhar-mahadevan-kluwer-boston-19931997-xii240-pp-isbn-0792393651-hardback-21800-guilders-12000-8995/
737FD21CA908246DF17779E9C20B6DF6.
EmanuelTodorov.Generaldualitybetweenoptimalcontrolandestimation.In200847thIEEECon-
ference on Decision and Control, pages 4286–4292, December 2008. doi: 10.1109/CDC.2008.
4739438. URL https://ieeexplore.ieee.org/abstract/document/4739438. ISSN:
0191-2216.
Marc Toussaint. Robot trajectory optimization using approximate inference. In Proceedings of
the 26th Annual International Conference on Machine Learning, pages 1049–1056, Montreal
QuebecCanada, June2009.ACM. ISBN978-1-60558-516-1. doi: 10.1145/1553374.1553508.
URLhttps://dl.acm.org/doi/10.1145/1553374.1553508.
Jonathan S. Yedidia, W.T. Freeman, and Y. Weiss. Constructing free-energy approximations and
generalized belief propagation algorithms. IEEE Transactions on Information Theory, 51(7):
2282–2312,July2005. ISSN0018-9448. doi: 10.1109/TIT.2005.850085.
˙Ismail Seno¨z. Message Passing Algorithms for Hierarchical Dynamical Models. Phd Thesis 1
,
(ResearchTU/e/GraduationTU/e),EindhovenUniversityofTechnology,Eindhoven,June2022.
ISBN:9789038655321.
A Background
A.1 VariationalInferenceandthePosteriorFactorization
InstandardVariationalInference,weminimizetheVariationalFreeEnergy(VFE)betweenavaria-
tionalposteriorqandagenerativemodelp:
(cid:90) q(y,x,θ,u)
F [q]= q(y,x,θ,u)log dydxdθdu. (A.1)
p p(y,x,θ,u)
Weareconsideringthefactorizedgenerativemodeldefinedin(1). Since(A.1)definesafunctional
objective that we can minimize, we should also specify a family Q over which we are optimizing
theVFE.Wewillchoosetheelementsqofthisfamilysuchthattheydecomposeasfollows
T
(cid:89)
q(y,x,θ,u)=q(x ,θ) q(y |x ,θ)q(x |x ,u ,θ)q(u |x ,θ). (A.2)
0 t t t t−1 t t t−1
t=1
Thisfactorizationoftheposteriordistributionisrequiredforthedefinitionoftheaugmentedgener-
ativemodelinsubsectionA.2[Nuijtenetal.,2025].
8
A.2 TheEpistemic-Prior-AugmentedGenerativeModel
The key insight of De Vries et al. [2025] is that Active Inference can be recovered through an
adjusted or augmented generative model that includes additional factors called epistemic priors.
Fromthegenerativemodel,anaugmentedmodelisconstructed
T
(cid:89)
p˜(y,x,θ,u)=p(y,x,θ,u)· p˜(x )p˜(u )p˜(y ,x ). (A.3)
t t t t
t=1
Here,theadditionalp˜termsareepistemicpriors,whicharefunctionsofthevariationaldistribution
q itself, creating a self-consistent optimization problem. The epistemic priors take the following
forms:
p˜(u )=exp(H[q(x ,x |u )]−H[q(x |u )]) (actionprior); (A.4a)
t t t−1 t t−1 t
p˜(x )=exp(−H[q(y |x )]) (stateprior); (A.4b)
t t t
p˜(y ,x )=exp(D [q(θ|y ,x )∥q(θ|x )]) (observationprior), (A.4c)
t t KL t t t
wheretheentropyofadistributionqovervariablesz ,...,z definiedasfollows
1 n
(cid:90)
H[q(z ,...,z )]=− q(z ,...,z )logq(z ,...,z )dz ...dz , (A.5)
1 n 1 n 1 n 1 n
andtheconditionalentropyhasthefollowingfunctionalform
(cid:90)
H[q(z ,...,z |ω)]=− q(z ,...,z |ω)logq(z ,...,z |ω)dz ...dz . (A.6)
1 n 1 n 1 n 1 n
Theepistemicpriorsadmitapracticalinterpretation:theactionpriorencouragesactionsthatresolve
ambiguity, the state prior favors informative states, and the observation prior encourages observa-
tionsthatareinformativeabouttheparameters.
TheadjustedVFEisthendefinedinthefollowingway:
(cid:90) q(y,x,θ,u)
F [q]= q(y,x,θ,u)log dydxdθdu. (A.7)
p˜ p˜(y,x,θ,u)
Crucially,DeVriesetal.[2025]showthatminimizingthisadjustedVFEisequivalenttominimizing
aboundontheExpectedFreeEnergy(EFE)objectivefromActiveInference[Fristonetal.,2015].
A.3 BetheFreeEnergy
(cid:81)
Let(V,E)bethe(Forney-style)factorgraphofapositivefunctionf(s)= f (s ),wheres
a∈V a a a
collects the variables incident on factor a, and each edge i ∈ E carries a variable s . The Bethe
i
variationalfamilyconsistsoffactorbeliefs{q (s )} andedgebeliefs{q (s )} constrained
a a a∈V i i i∈E
tothemarginalmanifold:
(cid:90)
q (s )ds =1 ∀a∈V, (A.8a)
a a a
(cid:90)
q (s )ds =1 ∀i∈E, (A.8b)
i i i
(cid:90)
q (s )ds =q (s ) ∀a∈V, ∀i∈a, (A.8c)
a a a\i i i
wheres denotesthevariablesins excepts . Letd bethenumberoffactorsincidentonedgei
a\i a i i
(thedegreeofvariables ). TheBetheFreeEnergy(BFE)[Yedidiaetal.,2005]is
i
(cid:88) (cid:88)
F [q] = D[q (s )||f (s )] + (d −1)H[q (s )], (A.9)
f a a a a i i i
a∈V i∈E
where
(cid:90) q (s )
D[q (s )||f (s )]= q (s )log a a ds . (A.10)
a a a a a a f (s ) a
a a
The Bethe approximation takes logZ ≈ −min F [q], where M is the manifold (A.8). The
f q∈M f
approximationisexact whenthefactorgraphisatree; onloopygraphs, stationarypointsof(A.9)
coincidewithfixedpointsof(loopy)beliefpropagation[Yedidiaetal.,2005]. Wereferthereader
to Seno¨z [2022] for a modern variational calculus exposition on the message passing algorithms
,
derivationfrom(A.9).
9
B ProofofTheorem1
Toprovethetheorem,wewillrequirethreelemmas:
Lemma3. UndertheassumptionthatourposteriordistributionfactorizesasinEquationA.2,and
p˜(x )=exp(−H[q(y |x )]):
t t t
(cid:90)
− q(x )logp˜(x )dx =H[q(y ,x )]−H[q(x )] (B.1)
t t t t t t
Proof.
(cid:90)
− q(x )logp˜(x )dx (B.2a)
t t t
(cid:90) (cid:90)
=− q(x ) q(y |x )logq(y |x )dy dx (B.2b)
t t t t t t t
(cid:90)
=− q(y |x )q(x )logq(y |x )dy dx (B.2c)
t t t t t t t
(cid:90) q(y ,x )
=− q(y ,x )log t t dy dx (B.2d)
t t q(x ) t t
t
(cid:90) q(y ,x )
=− q(y ,x )log t t dy dx =H[q(y ,x )]−H[q(x )]. (B.2e)
t t q(x ) t t t t t
t
Lemma4. UndertheassumptionthatourposteriordistributionfactorizesasinEquationA.2,and
p˜(u )=exp(H[q(x ,x |u )]−H[q(x |u )])thefollowingidentityholds:
t t t−1 t t−1 t
(cid:90)
− q(u )logp˜(u )du =H[q(x ,u )]−H[q(x ,x ,u )] (B.3)
t t t t−1 t t t−1 t
Proof.
(cid:90)
− q(u )logp˜(u )du (B.4a)
t t t
(cid:90) (cid:18)(cid:90)
= q(u ) q(x ,x |u )logq(x ,x |u )dx dx
t t t−1 t t t−1 t t t−1
(cid:90) (cid:19)
− q(x |u )logq(x |u )dx du (B.4b)
t−1 t t−1 t t−1 t
(cid:90) (cid:18)(cid:90) q(x ,x ,u ) q(x ,x ,u )
= q(u ) t t−1 t log t t−1 t dx dx
t q(u ) q(u ) t t−1
t t
(cid:90) q(x ,u ) q(x ,u ) (cid:19)
− t−1 t log t−1 t dx du (B.4c)
q(u ) q(u ) t−1 t
t t
(cid:90) q(x ,x ,u ) (cid:90) q(x ,u )
= q(x ,x ,u )log t t−1 t dx dx du − q(x ,u )log t−1 t dx du
t t−1 t q(u ) t t−1 t t−1 t q(u ) t−1 t
t t
(B.4d)
(cid:90) (cid:90) 1
= q(x ,x ,u )logq(x ,x ,u )dx dx du + q(x ,x ,u )log dx dx du
t t−1 t t t−1 t t t−1 t t t−1 t q(u ) t t−1 t
t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
−H[q(xt,xt−1,ut)] H[q(ut)]
(cid:90) (cid:90) 1
− q(x ,u )logq(x ,u )dx′ du − q(x ,u )log dx dx du
t−1 t t−1 t t−1 t t−1 t q(u ) t t−1 t
t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
−H[q(xt−1,ut] H[q(ut)]
(B.4e)
=H[q(x ,u )]−H[q(x ,x ,u )] (B.4f)
t−1 t t t−1 t
10
Lemma5. UndertheassumptionthatourposteriordistributionfactorizesasinEquationA.2,and
p˜(y ,x )=expD [q(θ |y ,x )||q(θ |x )]:
t t KL t t t
(cid:90)
− q(y ,x ,)logp˜(y ,x )dy dx =H[q(y ,x ,θ)]+H[q(x )]−H[q(y ,x )]−H[q(x ,θ)]
t t t t t t t t t t t t
(B.5)
Proof.
(cid:90)
− q(y ,x )logp˜(y ,x )dy dx (B.6a)
t t t t t t
(cid:90) (cid:18)(cid:90) q(θ |y ,x ) (cid:19)
=− q(y ,x ) q(θ |y ,x )log t t dθ dy dx (B.6b)
t t t t q(θ |x ) t t
t
(cid:90) (cid:18)(cid:90) (cid:19)
=− q(y ,x ) q(θ |y ,x )logq(y ,x ,θ)−logq(y ,x )dθ dy dx
t t t t t t t t t t
(cid:90) (cid:18)(cid:90) (cid:19)
+ q(y ,x ) q(θ |y ,x )logq(θ,x )−logq(x )dθ dy dx (B.6c)
t t t t t t t t
(cid:90) (cid:90)
=− q(y ,x ,θ)logq(y ,x ,θ)dy dx dθ+ q(y ,x ,θ)logq(y ,x )dy dx dθ
t t t t t t t t t t t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
H[q(yt,xt,θ)] −H[q(yt,xt)]
(cid:90) (cid:90)
+ q(y ,x ,θ)logq(x ,θ)dy dx dθ− q(y ,x ,θ)logq(x )dy dx dθ (B.6d)
t t t t t t t t t t
(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
−H[q(xt,θ)] H[q(xt)]
=H[q(y ,x ,θ)]−H[q(y ,x )]−H[q(x ,θ)]+H[q(x )] (B.6e)
t t t t t t
Nowthatwehaveourlemmasinplace,wecancontinuewiththeproofofTheorem1.
Proof.
(cid:90) q(y,x,θ,u)
F˜ [q]= q(y,x,θ,u)log dydxdθdu (B.7a)
p˜ p˜(y,x,θ,u)
(cid:90) q(y,x,θ,u)
= q(y,x,θ,u)log dydxdθdu (B.7b)
p(y,x,θ,u)
(cid:81)T
p˜(x )p˜(u )p˜(y ,x )
t=1 t t t t
(cid:90) q(y,x,θ,u)
= q(y,x,θ,u)log dydxdθdu
p(y,x,θ,u)
(cid:124) (cid:123)(cid:122) (cid:125)
Fp[q]
(cid:90) 1
+ q(y,x,θ,u)log dydxdθdu (B.7c)
(cid:81)T
p˜(x )p˜(u )p˜(y ,x )
t=1 t t t t
T (cid:90)
(cid:88)
=F [q]− q(y,x,θ,u)(logp˜(x )+logp˜(u )+logp˜(y ,x )) (B.7d)
p t t t t
t=1
T
(cid:88)
=F [q]+
p
t=1
(cid:90) (cid:90) (cid:90)
− q(x )logp˜(x )dx − q(u )logp˜(u )du − q(y ,x )logp˜(y ,x )dy dx .
t t t t t t t t t t t t
(B.7e)
11
Here,wecanrecognizetheidentitiesfromLemma3,Lemma4andLemma5.
T (cid:90) (cid:90) (cid:90)
(cid:88)
F [q]+ − q(x )logp˜(x )dx − q(u )logp˜(u )du − q(y ,x )logp˜(y ,x )dy dx
p t t t t t t t t t t t t
t=1
(B.8a)
T
(cid:88)
=F [q]+ H[q(y ,x )]−H[q(x )]+H[q(x ,u )]−H[q(x ,x ,u )]
p t t t t−1 t t t−1 t
t=1
+H[q(y ,x ,θ)]+H[q(x )]−H[q(y ,x )]−H[q(x ,θ)] (B.8b)
t t t t t t
T
(cid:88)
=F [q]+ H[q(x ,u )]−H[q(x ,x ,u )]+H[q(y ,x ,θ)]−H[q(x ,θ)] (B.8c)
p t−1 t t t−1 t t t t
t=1
C AdditionalEntropyTermsinPlanningObjective
TheadjustedinferenceobjectiveinLa´zaro-Gredillaetal.[2024]isphrasedasamaximizationprob-
lemincludingconditionalentropies.Inthisappendix,wewillrephrasetheplanning-as-inferenceob-
jectiveintothevocabularyusedinthispaper.La´zaro-Gredillaetal.[2024]phrasestheiroptimization
problemasamaximizationofavariationalbound,whereasweposetheproblemasaminimization
oftheVariationalFreeEnergy. ThismeanstheentropytermsinLa´zaro-Gredillaetal.[2024]have
their sign flipped, as this entropy term is subtracted from the objective. Note that we also have a
slightlydifferentindexingforactions,asu leadstox ,insteadofleadingtox asisthenotation
t t t+1
usedbyLa´zaro-Gredillaetal.[2024]. Here,weuseH [z | ω] = − (cid:82) q(z,ω)logq(z | ω)dzdω as
q
12
thenotationoftheconditionalentropy.
T
(cid:88)
H[q(x )]+ H [x |x ,u ] (C.1a)
0 q t t−1 t
t=1
T (cid:90)
(cid:88)
=H[q(x )]+ − q(x ,x ,u )logq(x |x ,u )dx dx du (C.1b)
0 t t−1 t t t−1 t t t−1 t
t=1
=H[q(x )]+ (cid:88) T − (cid:90) q(x ,x ,u )log q(x t ,x t−1 ,u t ) dx dx du (C.1c)
0 t t−1 t q(u |x )q(x ) t t−1 t
t t−1 t−1
t=1
=H[q(x )]+ (cid:88) T − (cid:90) q(x ,x ,u )log q(x t ,x t−1 ,u t ) dx dx du
0 t t−1 t q(x ) t t−1 t
t−1
t=1
(cid:90)
+ q(x ,x ,u )logq(u |x )dx dx du (C.1d)
t t−1 t t t−1 t t−1 t
T (cid:90)
(cid:88)
=H[q(x )]+ − q(x ,x ,u )logq(x ,u |x )dx dx du
0 t t−1 t t t t−1 t t−1 t
t=1(cid:124) (cid:123)(cid:122) (cid:125)
H q[xt,ut|xt−1]
+ (cid:88) T (cid:90) q(x ,u )log q(x t−1 ,u t ) dx du (C.1e)
t−1 t q(x ) t−1 t
t−1
t=1
T T (cid:90)
(cid:88) (cid:88)
=H[q(x )]+ H [x ,u |x ]+ q(x ,u )logq(x ,u )dx du
0 q t t t−1 t−1 t t−1 t t−1 t
t=1 t=1(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124)
H
(cid:123)
[
(cid:122)
q]
(cid:125) −H[q(xt−1,ut)]
(cid:90)
− q(x )logq(x )dx (C.1f)
t−1 t−1 t−1
(cid:124) (cid:123)(cid:122) (cid:125)
−H[q(xt−1)]
T
(cid:88)
=H[q]+ H[q(x )]−H[q(x ,u )] (C.1g)
t−1 t−1 t
t=1
Sinceinourminimizationschemethisentropyissubtractedfromtheobjective,wesubtracttheaddi-
tionaltermsin(C.1g)fromtheinferenceobjective,andweobtain (cid:80)T H[q(x ,u )]−H[q(x )]
t=1 t−1 t t−1
asafinaladditionalterm.
D ConsolidatingStateandObservationEpistemicPriors
Lemma6. UndertheassumptionthatourposteriordistributionfactorizesasinEquationA.2,with
p˜(x )=exp{−H[q(y |x )]}, p˜(y ,x )=expD (cid:2) q(θ |y ,x )∥q(θ |x ) (cid:3) ,
t t t t t KL t t t
definep˜(x ,θ)=exp{−H[q(y |x ,θ)]}. Then
t t t
(cid:90) (cid:90) (cid:90)
− q(x )logp˜(x )dx − q(y ,x )logp˜(y ,x )dy dx = − q(x ,θ)logp˜(x ,θ)dx dθ.
t t t t t t t t t t t t
(D.1)
Proof.
(cid:90) (cid:90)
S :=− q(x )logp˜(x )dx − q(y ,x )logp˜(y ,x )dy dx (D.2a)
t t t t t t t t t t
=H[q(y ,x ,θ)]−H[q(x ,θ)] (byLemmas3and5) (D.2b)
t t t
. (D.2c)
13
Ontheotherhand,
(cid:90) (cid:90)
R :=− q(x ,θ)logp˜(x ,θ)dx dθ = q(x ,θ)H[q(y |x ,θ)]dx dθ (D.3a)
t t t t t t t t
(cid:90)
=− q(y ,x ,θ) logq(y |x ,θ)dy dx dθ (D.3b)
t t t t t t
(cid:90)
(cid:0) (cid:1)
=− q(y ,x ,θ) logq(y ,x ,θ)−logq(x ,θ) dy dx dθ (D.3c)
t t t t t t t
=H[q(y ,x ,θ)]−H[q(x ,θ)]. (D.3d)
t t t
ThusS =R ,whichproves(D.1).
t t
E ProofofTheorem2
In this appendix, we establish the stationary conditions for the Active Inference message-passing
schemepresentedinTheorem2.Theproofproceedsbyderivingthefirst-orderoptimalityconditions
foreachcoordinateintheadjustedsystem.
We begin by expanding the Bethe Free Energy for the generative model (1) and establishing the
necessary consistency constraints in (E.3). subsection E.1 demonstrates that the standard region
coordinatesleadtoadegenerateoptimizationproblem(Theorem7),motivatingtheintroductionof
thechannelvariabler . subsectionE.2andsubsectionE.3thenderivethestationaryconditions
y|xθ,t
withrespecttoeachcoordinate—theobservationfactorbeliefq (Theorem8),thechannelr
y,t y|xθ,t
(Theorem9),andthedynamicsfactorbeliefq (Theorem10)—aswellastheidentificationsof
dyn,t
theLagrangemultipliersΛ (Theorem11)andΛ (Theorem12).
xθ trip
Theorem2followsdirectlyfromtheseresults:equations(8)–(12)aresimplythecollectedstationary
conditionsestablishedinTheorem8–Theorem12below,expressedinthenotationofthemaintext
withthegoalpriorspˆ (y )explicitlyincluded.
y t
BFEforthemodel(1)thenreads
FBethe[q]=D[q (θ)||p(θ)] + D[q (x )||p(x )]
p θ x0 0 0
T
(cid:88)(cid:104)
+ D[q (y ,x ,θ)||p(y |x ,θ)]+D[q (x ,x ,θ,u )||p(x |x ,θ,u )]
y,t t t t t dyn,t t t−1 t t t−1 t
t=1
(cid:105)
+D[q (u )||p(u )]+D[q (x )||pˆ (x )]+D[q (y )||pˆ (y )]
u,t t t x,pˆ,t t x t y,pˆ,t t y t
+(d −1)H[q (θ)]+(d −1)H[q (x )]
θ θ x0 x0 0
T
(cid:88)(cid:104) (cid:105)
+ (d −1)H[q (x )]+(d −1)H[q (y )]+(d −1)H[q (u )] .
xt xt t yt yt t ut ut t
t=1
(E.1)
Thevariable-nodedegreesimpliedby(1)andthefactorizationaboveare
d =1+2T (prior,T obs,T dyn), d =2 (prior,dynt=1),
θ x0
d =2 (obs,goalpriorony ), d =2 (actionprior,dynt),
yt t ut
(E.2)
(cid:26)
4, 1≤t≤T −1 (obst,dynt,dynt+1,goalprioronx ),
d = t
xt 3, t=T (obsT,dynT,goalprioronx ).
T
(For unary factors f ,f ,f ,f ,f we identify the factor belief with the adjacent single-
θ x0 u,t x,pˆ,t y,pˆ,t
ton.) Withthisnotation,theBetheFreeEnergyin(E.1)isthespecializationofthegeneralBFEin
subsectionA.3to(1).
Localconsistencyrequiresthat,foreveryfactoraandeveryvariablei∈s ,
a
(cid:90)
q (s )ds = q (s ).
a a a\i i i
14
(cid:90)
Observation(y,t): q (y ,x ,θ)dx dθ =q (y ), (E.3a)
y,t t t t yt t
(cid:90)
q (y ,x ,θ)dy dθ =q (x ), (E.3b)
y,t t t t xt t
(cid:90)
q (y ,x ,θ)dy dx =q (θ). (E.3c)
y,t t t t t θ
(cid:90)
Dynamics(dyn,t): q (x ,x ,θ,u )dx du dθ =q (x ), (E.3d)
dyn,t t t−1 t t−1 t xt t
(cid:90)
q (x ,x ,θ,u )dx du dθ =q (x ), (E.3e)
dyn,t t t−1 t t t xt−1 t−1
(cid:90)
q (x ,x ,θ,u )dx dx dθ =q (u ), (E.3f)
dyn,t t t−1 t t t−1 ut t
(cid:90)
q (x ,x ,θ,u )dx dx du =q (θ). (E.3g)
dyn,t t t−1 t t t−1 t θ
Unarypriors/goals: q (u )=q (u ), q (x )=q (x ), q (y )=q (y ).
u,t t ut t x,pˆ,t t xt t y,pˆ,t t yt t
(E.3h)
(For unary factors we identify the factor belief with the adjacent singleton belief; the equality is
enforcedbythecorrespondingconsistencyconstraint.)
E.1 Degeneracyofthemarginalscheme
Theorem 7 (Degeneracy persists under the augmented region coordinates). Fix t. Consider the
Bethe-formobjectivespecializedto(1),augmentedbytheActiveInferenceentropiccorrection
+H[q(y ,x ,θ)] − H[q(x ,θ)] + H[q(x ,u )] − H[q(x ,x ,u )].
t t t t−1 t t t−1 t
Introducetheauxiliaryregionbeliefsq (x ,θ)(definedinEquation6)withtheconsistencycon-
sep,t t
straints
(cid:90)
q (y ,x ,θ)dx dθ =q (y ), (E.4a)
y,t t t t yt t
(cid:90)
q (y ,x ,θ)dy =q (x ,θ). (E.4b)
y,t t t t sep,t t
(Regionbeliefsq andq aretiedtothedynamicsblockanddonotappearin(E.4).) Then
pair,t trip,t
any stationary point of the Lagrangian with respect to the observation-factor belief q (y ,x ,θ)
y,t t t
satisfies
−logp(y |x ,θ) + λ (y ) + λ (x ,θ) = 0, (E.5)
t t y t sep t
forsomemultipliersλ (·),λ (·,·). Consequently:
y sep
1. If p(y | x ,θ) is not separable as a (y )b (x ,θ), the system is infeasible (no interior
t t t t t t
solutionforq ).
y,t
2. If p(y | x ,θ) = a (y )b (x ,θ) is separable, the observation block is affine in q (its
t t t t t t y,t
secondvariationw.r.t.q vanishes),hencestationarypointsarenon-unique(aflatfaceof
y,t
thefeasiblepolytope).
Proof. Theq -dependentpartoftheobjectiveis
y,t
(cid:90) q
q log y,t + H[q(y ,x ,θ)],
y,t p(y |x ,θ) t t
t t
since −H[q(x ,θ)], +H[q(x ,u )], and −H[q(x ,x ,u )] do not depend on q . The
t t−1 t t t−1 t y,t
(cid:82) q logq from the KL term cancels exactly with +H[q(y ,x ,θ)], leaving the linear func-
y,t y,t t t
(cid:82)
tional − q logp(y | x ,θ). Add constraints (E.4a)–(E.4b) with multipliers λ (·), λ (·,·).
y,t t t y sep
Taking the functional derivative yields (E.5). Exponentiating shows that a solution exists only if
p(y
t
| x
t
,θ) ∝ eλy(yt)eλsep(xt,θ),i.e.,itfactorizesasa
t
(y
t
)b
t
(x
t
,θ). Intheseparablecase,theab-
(cid:82)
senceofa q logq termimplieszerocurvatureintheq -directionandthusnon-uniqueness;
y,t y,t y,t
otherwisethesystemisinfeasible.
15
Consequence. Theorem 7 shows that even after introducing the augmented region coordinates
(q ,q ,q ),theActiveInferencecorrectionleavestheobservationblockdegenerate: fea-
sep,t pair,t trip,t
sibilityrequiresaseparablelikelihoodiny and(x ,θ),andotherwisetheblockisflat.
t t
E.2 Stationaryconditionsinthecondititonaladjustedsystem
Lemma 8 (Stationary condition for the observation factor with channel augmentation). Fix t and
(cid:82)
introducethechannelr (y |x ,θ)withthenormalization r (y |x ,θ)dy =1.Assume
y|xθ,t t t y|xθ,t t t t
theseparatorconsistencyontheoverlap(x ,θ),
t
(cid:90) (cid:90)
q (y ,x ,θ)dy = q (x ,θ) = q (x ,x ,θ,u )dx du ,
y,t t t t xθ t dyn,t t t−1 t t−1 t
andthey -singletonconsistency
t
(cid:90)
q (y ,x ,θ)dx dθ = q (y ).
y,t t t t yt t
Considertheobservation-blockobjective(holdingr fixed):
y|xθ,t
(cid:90) q log q y,t + (cid:90) q (cid:2) −logr (y |x ,θ) (cid:3) ,
y,t p(y |x ,θ) y,t y|xθ,t t t
t t
plus Lagrange terms for the two consistency constraints. Then the stationarity with respect to
q (y ,x ,θ)is
y,t t t
logq (y ,x ,θ) − logp(y |x ,θ) − logr (y |x ,θ) + λ (y ) + λ (x ,θ) = 0, (E.6)
y,t t t t t y|xθ,t t t y t xθ t
forsomemultipliersλ (·)andλ (·,·). Equivalently,
y xθ
(cid:8) (cid:9) (cid:8) (cid:9)
q (y ,x ,θ) ∝ p(y |x ,θ)r (y |x ,θ) exp −λ (y ) exp −λ (x ,θ) , (E.7)
y,t t t t t y|xθ,t t t y t xθ t
withλ ,λ chosentosatisfythetwoprojectionconstraintsabove.
y xθ
Proof. FormthepartialLagrangian(omittingtheredundantnormalizationofq ):
y,t
L[q ]= (cid:90) q log q y,t + (cid:90) q (cid:2) −logr (cid:3)
y,t y,t p(y |x ,θ) y,t y|xθ,t
t t
(cid:90) (cid:18)(cid:90) (cid:19)
+ λ (y ) q dx dθ−q (y ) dy
y t y,t t yt t t
(cid:90)(cid:90) (cid:18)(cid:90) (cid:19)
+ λ (x ,θ) q dy −q (x ,θ) dx dθ.
xθ t y,t t xθ t t
Takingthefunctionalderivativew.r.t.q andsettingittozeroyields
y,t
logq −logp(y |x ,θ)−logr +1+λ (y )+λ (x ,θ)=0,
y,t t t y|xθ,t y t xθ t
where the constant +1 can be absorbed into either multiplier. This is (E.6). Exponentiating gives
(E.7),andthemultipliersaredeterminedbyenforcingthetwolinearprojectionconstraints.
Lemma 9 (Stationary condition for the observation channel). Fix t. Let the channel r (y |
y|xθ,t t
x ,θ)satisfytherow–normalization
t
(cid:90)
r (y |x ,θ)dy =1 forall(x ,θ), (E.8)
y|xθ,t t t t t
andassumeseparatorconsistencyontheoverlap(x ,θ):
t
(cid:90)
q (y ,x ,θ)dy = q (x ,θ). (E.9)
y,t t t t sep,t t
Considerthechannelobjective(holdingq andq fixed)
y,t sep,t
(cid:90)
(cid:2) (cid:3)
J[r] = q (y ,x ,θ) −logr (y |x ,θ) dy dx dθ, (E.10)
y,t t t y|xθ,t t t t t
subjectto(E.8). Thenastationarypointisgivenpointwiseby
q (y ,x ,θ)
r⋆ (y |x ,θ) = y,t t t wheneverq (x ,θ)>0, (E.11)
y|xθ,t t t q (x ,θ) sep,t t
sep,t t
withtheconventionthatifq (x ,θ)=0,theentirerowr (·|x ,θ)canbechosenarbitrarily
sep,t t y|xθ,t t
asanyprobabilitydistribution(itdoesnotaffectJ).
16
Proof. FormtheLagrangianwitharowmultiplierλ(x ,θ)enforcing(E.8):
t
(cid:90) (cid:90)(cid:90) (cid:18)(cid:90) (cid:19)
(cid:2) (cid:3)
L[r]= q (y ,x ,θ) −logr(y |x ,θ) + λ(x ,θ) r(y |x ,θ)dy −1 dx dθ.
y,t t t t t t t t t t
Takingthefunctionalderivativeandsettingittozeroyields,pointwisein(y ,x ,θ),
t t
q (y ,x ,θ) q (y ,x ,θ)
− y,t t t +λ(x ,θ)=0 ⇒ r⋆(y |x ,θ)= y,t t t .
r⋆(y |x ,θ) t t t λ(x ,θ)
t t t
Imposingtherow–normalization(E.8)andusing(E.9),
(cid:90) (cid:82) q (y ,x ,θ)dy q (x ,θ)
1= r⋆(y |x ,θ)dy = y,t t t t = sep,t t ,
t t t λ(x ,θ) λ(x ,θ)
t t
soλ(x ,θ)=q (x ,θ),giving(E.11). Ifq (x ,θ)=0,thentherowofq isidenticallyzero
t sep,t t sep,t t y,t
andJ isunaffectedbythechoiceofr(·|x ,θ).
t
Lemma 10 (Stationary condition for the dynamics factor (minimal projections)). Fix t. Let
q (x ,x ,u ,θ) be the dynamics–factor belief and introduce the region beliefs q (x ,θ)
dyn,t t t−1 t xθ,t t
andq (x ,x ,u )withprojectionconstraints
trip,t t t−1 t
(cid:90)
q (x ,x ,θ,u )dx du =q (x ,θ), (E.12a)
dyn,t t t−1 t t−1 t xθ,t t
(cid:90)
q (x ,x ,θ,u )dθ =q (x ,x ,u ). (E.12b)
dyn,t t t−1 t trip,t t t−1 t
(cid:82)
(Separately,enforcethepair–triprelation q (x ,x ,u )dx =q (x ,u )intheq
trip,t t t−1 t t pair,t t−1 t trip,t
block.) Theq -dependentpartoftheobjectiveistheKLterm
dyn,t
(cid:90) q
q log dyn,t dx dx du dθ,
dyn,t p(x |x ,θ,u ) t t−1 t
t t−1 t
while the entropic corrections only involve the region beliefs. Form the partial Lagrangian with
multipliersΛ (x ,θ)andΛ (x ,x ,u )enforcing(E.12a)–(E.12b).Thenthestationarityw.r.t.
xθ t trip t t−1 t
q is
dyn,t
logq (x ,x ,θ,u )−logp(x |x ,θ,u )+Λ (x ,θ)+Λ (x ,x ,u )=0, (E.13)
dyn,t t t−1 t t t−1 t xθ t trip t t−1 t
uptoanadditiveconstant,hence
(cid:8) (cid:9) (cid:8) (cid:9)
q (x ,x ,θ,u ) ∝ p(x |x ,θ,u ) exp −Λ (x ,θ) exp −Λ (x ,x ,u ) .
dyn,t t t−1 t t t−1 t xθ t trip t t−1 t
(E.14)
The multipliers are determined by the two projection constraints (E.12), while the consistency
(cid:82)
q dx =q isenforcedintheq variationanddoesnotappearin(E.14).
trip,t t pair,t trip,t
E.3 IdentificationoftheLagrangemultipliers
Lemma 11 (Identification of the dynamics–side separator multiplier). Fix t. Assume the channel
r (y |x ,θ)isrow–normalizedandimposeseparatorconsistency
y|xθ,t t t
(cid:90) (cid:90)
q (y ,x ,θ)dy = q (x ,θ) = q (x ,x ,θ,u )dx du .
y,t t t t sep,t t dyn,t t t−1 t t−1 t
Atanystationarypointoftheobservationblock,thereexistsaslice–constantC >0suchthat
t
(cid:90)
p(y |x ,θ)r (y |x ,θ)e−λy(yt)dy
t t y|xθ,t t t t
(cid:8) (cid:9)
exp −Λ (x ,θ) = C , (E.15)
xθ t t q (x ,θ)
sep,t t
(cid:82)
whereλ (·)istheLagrangemultiplierenforcing q (y ,x ,θ)dx dθ =q (y ). Equivalently,
y y,t t t t yt t
(cid:16)(cid:90) (cid:17)
Λ (x ,θ) = logq (x ,θ) − log p(y |x ,θ)r (y |x ,θ)e−λy(yt)dy + c ,
xθ t sep,t t t t y|xθ,t t t t t
withc =−logC independentof(x ,θ).
t t t
17
Proof. From the observation–block stationarity (Lemma 8), there exists a slice–constant κ > 0
t
suchthat
q
y,t
(y
t
,x
t
,θ)=κ
t
p(y
t
|x
t
,θ)r
y|xθ,t
(y
t
|x
t
,θ)e−λy(yt)e−λxθ(xt,θ). (E.16)
Integrate(E.16)overy andusetheleftseparatorequalitytoobtain
t
(cid:90)
q
sep,t
(x
t
,θ)=κ
t
e−λxθ(xt,θ) p(y
t
|x
t
,θ)r
y|xθ,t
(y
t
|x
t
,θ)e−λy(yt)dy
t
.
(cid:124) (cid:123)(cid:122) (cid:125)
=:It(xt,θ)
Solvefore−λxθ(xt,θ):
q (x ,θ)
e−λxθ(xt,θ) = sep,t t .
κ I (x ,θ)
t t t
On the other hand, varying the separator q in the Lagrangian for the two equalities gives
sep,t
λ
xθ
(x
t
,θ)+Λ
xθ
(x
t
,θ)=0,hencee−Λxθ(xt,θ) =eλxθ(xt,θ). Combiningthetwodisplays,
(cid:90)
p(y |x ,θ)r (y |x ,θ)e−λy(yt)dy
κ I (x ,θ) t t y|xθ,t t t t
e−Λxθ(xt,θ) =eλxθ(xt,θ) =
q
t t
(x
t
,θ)
= C
t q (x ,θ)
,
sep,t t sep,t t
with C = κ independent of (x ,θ). Taking logs yields the equivalent additive form with c =
t t t t
−logC .
t
Lemma12(Identificationofthetripletmultiplierfromthedynamics-sideentropiccorrection). Fix
t. Lettheregionbeliefsq (x ,x ,u )andq (x ,u )satisfythecouplingconstraint
trip,t t t−1 t pair,t t−1 t
(cid:90)
q (x ,x ,u )dx = q (x ,u ), (E.17)
trip,t t t−1 t t pair,t t−1 t
and let the projection from the dynamics factor be enforced via the multiplier Λ (x ,x ,u )
trip t t−1 t
(cid:82)
on q dθ−q = 0. Supposetheobjectivecontainsthedynamics-sideentropiccorrection
dyn,t trip,t
+H[q ]−H[q ]. Then, at any stationary point, there exists a slice-constant C > 0 such
pair,t trip,t t
that
exp (cid:8) −Λ (x ,x ,u ) (cid:9) = C q pair,t (x t−1 ,u t ) , (E.18)
trip t t−1 t t q (x ,x ,u )
trip,t t t−1 t
equivalently,
Λ (x ,x ,u ) = logq (x ,x ,u ) − logq (x ,u ) + c ,
trip t t−1 t trip,t t t−1 t pair,t t−1 t t
withc =−logC independentof(x ,x ,u ).
t t t t−1 t
Proof. ConsiderthepartoftheLagrangianinvolvingq andq :
trip,t pair,t
(cid:90) (cid:90)
L= q logq − q logq
trip,t trip,t pair,t pair,t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
from−H[qtrip,t] from+H[qpair,t]
(cid:90)(cid:90) (cid:18)(cid:90) (cid:19)
+ Ξ (x ,u ) q dx −q dx du
pair t−1 t trip,t t pair,t t−1 t
(cid:90)
− Λ q
trip trip,t
(cid:82)
wherethelasttermcomesfromtheprojection q dθ−q =0. Varyingw.r.t.q gives
dyn,t trip,t trip,t
δL
: logq +1 + Ξ (x ,u ) − Λ (x ,x ,u )=0.
δq trip,t pair t−1 t trip t t−1 t
trip,t
Varyingw.r.t.q yields
pair,t
δL (cid:0) (cid:1) (cid:0) (cid:1)
: − logq +1 −Ξ (x ,u )=0 ⇒ Ξ (x ,u )=− logq (x ,u )+1 .
δq pair,t pair t−1 t pair t−1 t pair,t t−1 t
pair,t
EliminatingΞ inthefirstequationgives
pair
Λ (x ,x ,u )=logq (x ,x ,u ) − logq (x ,u ) + c ,
trip t t−1 t trip,t t t−1 t pair,t t−1 t t
where the slice-constant c (absorbing the +1 terms and any normalization) is independent of
t
(x
t
,x
t−1
,u
t
). Exponentiatingyields(E.18)withC
t
=e−ct.
18

=== REVISE TO ===
PROFESSIONAL TONE: Begin directly with content - NO conversational openings like 'Okay, here's...'

1. Fix all issues above
2. Title: "Active Inference is a Subtype of Variational Inference"
3. Include 10-15 quotes from paper text
   - Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
   - Use consistent quote formatting: 'The authors state: "quote"' or vary attribution phrases
   - Vary attribution phrases to avoid repetition
   - CRITICAL: Only extract quotes that actually appear in the paper text
4. ELIMINATE ALL REPETITION - each sentence must be unique
   - Check before each sentence: 'Have I already said this?' If yes, write something new
   - Vary attribution phrases - do NOT repeat 'The authors state' multiple times
5. Extract methodology, results with numbers, key quotes
6. 1000-1500 words, structured with ### headers

Generate COMPLETE revised summary.