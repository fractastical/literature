=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: A Neural Network Implementation for Free Energy Principle
Citation Key: liu2023neural
Authors: Jingwei Liu

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2023

Key Terms: principle, energy, implementation, neural, helmholtz, machine, brain, active, network, free

=== FULL PAPER TEXT ===

A Neural Network Implementation for Free
Energy Principle⋆
Jingwei Liu1[0009−0004−3160−9326]
University of California San Diego, La Jolla CA 92092, USA
Abstract. Thefreeenergyprinciple(FEP),asanencompassingframe-
work and a unified brain theory, has been widely applied to account for
various problems in fields such as cognitive science, neuroscience, social
interaction, andhermeneutics. Asa computationalmodeldeeply rooted
in math and statistics, FEP posits an optimization problem based on
variational Bayes, which is solved either by dynamic programming or
expectationmaximizationinpractice.However,thereseemstobeabot-
tleneckinextendingtheFEPtomachinelearningandimplementingsuch
modelswithneuralnetworks.Thispapergivesapreliminaryattemptat
bridgingFEPandmachinelearning,viaaclassicalneuralnetworkmodel,
the Helmholtz machine. As a variational machine learning model, the
Helmholtzmachineisoptimizedbyminimizingitsfreeenergy,thesame
objective as FEP. Although the Helmholtz machine is not temporal, it
gives an ideal parallel to the vanilla FEP and the hierarchical model of
the brain, under which the active inference and predictive coding could
be formulated coherently. Besides a detailed theoretical discussion, the
paperalsopresentsapreliminaryexperimenttovalidatethehypothesis.
By fine-tuning the trained neural network through active inference, the
model performance is promoted to accuracy above 99%. In the mean-
time, the data distribution is continuously deformed to a salience that
conforms to the model representation, as a result of active sampling.
Keywords: Helmholtz machine · Free energy Principle · Active infer-
ence · Hierarchical model
1 Introduction
FreeEnergyPrinciple(FEP)asanencompassingframeworkandaunifiedbrain
theory [10] has been widely applied to account for various phenomena in many
cognition, humanity-related fields such as psychology[3], music[21], linguistic
communication[15],culturalnicheconstruction[5],embodiment[1],autopoiesis[20],
emotion recognition[7]. In the meanwhile, as a computational model deeply
rootedinmathandstatistics,FEPpositsanoptimizationproblembasedonvari-
ational Bayesian inference, which is solved either by dynamic programming[16]
or expectation maximization[9]. However, there seems to be a bottleneck in ex-
tending FEP to the fields of machine learning, which have been the hotspots
⋆ This work is done in Prof. Shlomo Dubnov’s experimental seminar.
3202
nuJ
11
]EN.sc[
1v29760.6032:viXra
2 J. Liu
for solving statistical and engineering problems in recent years. There are a few
works that bridge active inference and reinforcement learning[26], and here is a
surveyonseekingthecommongroundforactiveinferenceanddeeplearning[23].
However,asavariational-basedmethod,FEPcannotbeseenequally,orbegen-
eralized trivially, to reinforcement learning, a reward-based training method.
This work gives a preliminary attempt at bridging FEP and machine learning,
via a classical neural network model, the Helmholtz machine. There are three
features of using the Helmholtz machine to study FEP under neural network
settings:
1. TheHelmholtzmachineusesvariationalfreeenergyasitsobjective,whichis
in accord with the FEP. In other words, we maintain the variational essence
ofFEPbyusingacorrespondingvariationalmachine-learningmethod,which
minimizes the free energy.
2. Ifreinforcementlearningisconsideredascapturingthequalitiesofexpected
freeenergy,whichinvolvesplanningandfutureoutcomesofsequentialevents,
thentheHelmholtzmachineisaperfectparallelforthefreeenergy.Although
theHelmholtzmachineisnottemporal,inmanyaspects,it’sanidealproto-
typeforimplementingFEPandactiveinferenceinaneuralnetworkfashion,
which will be argued extensively in this paper.
3. TheHelmholtzmachinealsopresentsasatisfactorysimulationforthehierar-
chicalmodelofthebrain.Theforwardandbackwardconnectionsandhierar-
chical message passing are inherent in the implementation of the Helmholtz
machine.
This paper includes two main sections. Section 2 gives a theoretical account
to the interrelationship of the free energy principle and the Helmholtz machine
fromaspectsofmathematicalformulation,modeltrainingandparameterupdat-
ing,biologicalinterpretabilityandplausibility.Itprovidesatheoreticalbasisfor
generalizing FEP via the Helmholtz machine to broader model-fitting schemas
in the neural networks for machine learning. Section 3 presents a preliminary
experiment we designed to test the model. The model performs pretty well as
the theoretical analysis indicates. In training stage I, the Helmholtz machine
achieves an accuracy of 0.94 under traditional data fitting; In training stage II,
we apply the active inference in FEP to actively sample the input sensations
as salience. After a few rounds of fine-tuning, the model accuracy was boosted
above 0.99, which presents high generation accuracy while keeping generation
diversity at a satisfactory level.
2 Free Energy Principle and Helmholtz Machine
2.1 Variational Inference for Statistics
Herewegiveabriefoverviewofvariationalinference(VI)[2]inBayesianstatistics
and show how this concept is linked to the free energy principle and Helmholtz
A Neural Network Implementation for Free Energy Principle 3
machine. To maintain the notational consistency, we use the standard notations
in [6].
ThegoalistodeterminetheposteriorP(α|d),whereddenotestheobservable
data, and α is variously referred to as hidden causes, latent variables, or hidden
states. According to Bayes’ rule,
P(α,d) P(α,d)
P(α|d)= = (cid:82) (1)
P(d) P(α,d)dα
α
The integral over underlying causes is usually intractable (either unavailable
in closed form or requires exponential time to compute), so the true posterior
P(α|d) cannot be computed directly.
Remark 1. We give a separate account for the conditional data d in P(α|d).
In classical variational inference[2], d represents the entire dataset, thus the
latent distribution P(α|d) is independent of single data point, and all model
properties resort to latent local and global variables. Therefore, we can see that
anapproximateposteriorQ(α)isusedtoapproximatethetrueposteriorP(α|d),
where Q is not conditioned on the observations. This formulation is widely used
instatisticalvariationalinferenceandallresourcesI’vereadaboutthefreeenergy
principle such as [11] [13] [25].
However, in many other settings, we frequently see that another form of
approximate posterior Q(α|d) is used, the most prominent case is in VAE [19].
In the Helmholtz machine, this conditional Q(α|d) is also used as approximate
posterior(it’snotexplicitlygivenin[6],butin[18],it’sclearlystated).Themain
reason is that the data d is treated point-wisely in these models, thus the latent
cause distributions conditioned on individual data points vary from each other.
However, as the distribution Q(α|d) is parameterized by ϕ, which is amortized
toalldatapoints,theapproximateposteriorisstilltractableandworksasimilar
way as in the unconditioned case.
AstheHelmholtzmachineisthemodelweadopt,wewillusetheconditioned
approximate posterior in the following discussions. In spirit, it differs little from
the vanilla version as the formula deduction unfolds. To recap, we use an ap-
proximate posterior Q (α|d) to approximate the true posterior P(α|d), where
ϕ
Q (α|d)belongstoaparameterizedfamilyQ ofprobabilitydensities.Ourgoal
ϕ ϕ
is to find the member of this family that minimizes Kullback-Leibler (KL) di-
vergence to the exact posterior,
Q∗(α|d)= argmin D [Q (α|d)||P(α|d)] (2)
ϕ KL ϕ
Qϕ(α|d)∈Q
ϕ
The variational method kicks in when we decompose the true posterior in
the KL-divergence term,
D [Q (α|d)||P(α|d)]=E [logQ (α|d)]−E [logP(α|d)] (3)
KL ϕ Q ϕ Q
=E [logQ (α|d)]−E [logP(α,d)]+logP(d) (4)
Q ϕ Q
4 J. Liu
ThedistributionP(α,d)iscalledthegenerativemodel,asitdenotesthejointdis-
tributionofthelatentandobservablevariables.Thegenerativemodelisusually
assumed as known in VI and FEP, as the way environment generates observa-
tions from causes is innate.
Now the problem falls on the third term in Equation (4), what we call the
log-evidence, or negative surprisal, logP(d). This term is again intractable, so
to circumvent it, we use the nonnegativity of KL-divergence, rewrite (4) as
logP(d)≥E [logP(α,d)]−E [logQ (α|d)] (5)
Q Q ϕ
where the right-hand side term E [logP(α,d)]−E [logQ (α|d)] is called the
Q Q ϕ
evidence lower bound (ELBO). By maximizing ELBO we implicitly maximize
the log-evidence logP(d). The free energy is given by the negative ELBO,
F =E [logQ (α|d)]−E [logP(α,d)]=D [Q (α|d)||P(α,d)] (6)
Q ϕ Q KL ϕ
whichistheultimateminimizationgoalinVI,FEP,andtheHelmholtzmachine.
Remark 2. The minimization term F is seen as a compromise in VI. Since we
cannot minimize D [Q (α|d)||P(α|d)] directly, we find some cheap approxi-
KL ϕ
mation that we can compute. However, I claim it’s not the case for generative
models. In generative models which use generation as an organic component of
model construction, the generative density P(α,d) is a necessity instead of the
posterior P(α|d), since we are not only finding the best set of parameters in a
density family that approximates a given distribution, but also using generated
samplestoregulatetherecognitionprocess(Helmholtzmachine,VAE)oractive
sampling the generations to improve accuracy (FEP).
InHelmholtzmachine,insteadofpre-definingagenerativemodelP(α,d),in
a more realistic way (since the generative density is usually unknown in real-life
problems),weparameterizethisdistributionbyθ,andconstructthefreeenergy
minimization goal
F =D [Q (α|d)||P (α,d)] (7)
KL ϕ θ
Byjointlyoptimizingthetwosetsofparametersϕandθ inanEM(expectation-
maximization) manner, we minimize the free energy of the system.
In FEP, the free energy is reformulated as the two equations,
F =D [Q (α|d)||P(α|d)]−logP(d) (8)
KL ϕ
=D [Q (α|d)||P(α)]−E [logP(d|α)] (9)
KL ϕ Q
Equation (8) is interpreted from its first term as optimizing the recognition
density of the brain to approximate the true distribution of the world, which
shares the same goal as VI, minimizing D [Q (α|d)||P(α|d)]; Equation (9) is
KL ϕ
interpretedmoreinclinedtoitssecondterm,E [logP(d|α)],asawaytoactively
Q
sample the sensory inputs that conform to the current representations, thus
improving accuracy (please refer to [10] for more details). In this work, we will
integratetheclassicalHelmholtzmachinewhichistrainedunderminimizationof
A Neural Network Implementation for Free Energy Principle 5
variational free energy with the active inference in FEP. Besides the parameter
optimization, the model also performs active inference by a selective sampling
of the environment, which entails a modulation of attention reflected in the
distribution of evidence.
2.2 Neural Network for Machine learning
This work explores a way of implementing FEP using neural networks in ma-
chine learning. Traditionally, problems formulated under FEP are either solved
by DEM (dynamic expectation-maximization)[12] or MDP (Markov decision
process)[25]. Although the Helmholtz machine is not a temporal model, it uses
real neurons specified by modern neural networks and updates its parameters
via gradient descent. Under the current world trend, we believe it’s imperative
to extend FEP to the machine learning field and to solve problems using neural
network architectures.
Fig.1. The Helmholtz Machine. The Helmholtz Machine is a fully connected feed-
back neural network with hierarchical architecture. The solid lines show the bottom-
up recognition process parameterized by ϕ, and the dashed lines show the top-down
generation process parameterized by θ. The activity of each neuron is computed from
the activities of all neurons in its previous layer. The activation functions are given in
the text.
The structure of the Helmholtz machine is shown in Fig. 1. It’s a layered
hierarchicalmodelcomposedofstochasticbinaryneurons,connectedbybottom-
up recognition weights ϕ and top-down generative weights θ. In this work, we
did two major modifications to the original model in [6],
1. Theactivityofthestochasticbinaryneuronischangedfrom{0,1}to{−1,1}.
6 J. Liu
2. The bias is added when computing the linear activation of each neuron.
ThefirstmodificationisdoneduetothederivativeformofF withrespectto
its parameters, for example, ∂F = −sm+1(sm −pm). The neuron activity
∂θm+1,m k n n
k,n
fromthepreviouslayerisusedasamultiplieronthederivatives,whichmeansif
sm+1 =0, the gradient equals zero, thus no updating will be performed for this
k
parameter θm+1,m by gradient descent. Therefore, the parameter updating will
k,n
be half paralyzed when neuron activities alternate between 0 and 1. To make
the learning more efficient, we replace the activity value 0 with −1, thus zero
gradients won’t occur unless pm approaches sm.
n n
The Helmholtz machine is fully connected. The activation of each neuron is
a linear combination of all the neurons from its previous layer plus bias,
(cid:88)
am(θ,sm+1)= θm+1,msm+1+bm+1,m (10)
n k,n k n
k
where the activation of the n-th neuron in layer m is computed by a weighted
sumofallactivitiessm+1 inlayerm+1weightedbyitscorrespondingparameter
k
θm+1,m,plusthebiasbm+1,mforthisneuron.Herethepreviouslayeristheupper
k,n n
layer m+1, which corresponds to the top-down generative process indicated by
dashed lines in Fig. 1. We added the bias term to the original formulation to
expand the parameter set, thus endowing more freedom for the neural network
to fit the data.
The probability is calculated by the sigmoid function σ(x)=1/(1+e−x) of
activation, which nonlinearly compresses the range into (0,1).
(cid:88)
pm(θ,sm+1)=σ(am)=σ( θm+1,msm+1+bm+1,m) (11)
n n k,n k n
k
Similarly, the activation probability of a neuron in the bottom-up recognition
process is computed as
(cid:88)
qm(ϕ,sm−1)=σ( ϕm−1,msm−1+bm−1,m) (12)
n k,n k n
k
where the previous layer is the lower layer m−1, and the probability is denoted
with q. The notations are consistent with our notations in Equation (7). The
recognition density Q (α|d) and the generative density P (α,d) are computed
ϕ θ
by the product of the probabilities of all neurons, namely
Q ϕ (α|d)= (cid:89) (cid:89) [q n m(ϕ,sm−1)] 1+ 2 sm n [1−q n m(ϕ,sm−1)] 1− 2 sm n (13)
m>1 n
P θ (α,d)= (cid:89) (cid:89) [pm n (θ,sm+1)] 1+ 2 sm n [1−pm n (θ,sm+1)] 1− 2 sm n (14)
m≥1 n
Each neuron gives a Bernoulli distribution as the activity of sm takes value −1
n
or 1.
A Neural Network Implementation for Free Energy Principle 7
As the explicit form of the free energy F =D [Q (α|d)||P (α,d)] is given
KL ϕ θ
by equations (13) (14), now we should consider how to compute the derivatives
of F thus minimizing this term by gradient descent. If we refer back to the
ϕ,θ
structure of the Helmholtz machine in Fig. 1, we can see the neurons are con-
nected recurrently. Besides that, the change of activities in one layer will affect
the behavior of all neurons in higher layers, which makes backpropagation ex-
tremely difficult. To tackle this problem, a customized algorithm is designed for
training this system, which is the wake-sleep algorithm [18].
The wake-sleep algorithm disentangles the recognition parameters ϕ and the
generative parameter θ by separating the training into two phases. In the wake
phase,thebottom-uprecognitionprocessisperformedusingthecurrentweights
ϕ to get an instance of complete neuron assignments α by sampling activities
from {−1,1} based on the probability of each neuron qm. Now we update the
n
generative parameters θ based on the complete neuron activities encoded in α,
whichmakesthetargetvaluesavailablefortrainingthehiddenunitslocally,thus
disentangling them from the multi-layer coupling. In the sleep phase, the recog-
nitionweights ϕ are turnedoff anda randominstance isgeneratedbased onthe
current top-down weights θ. Alternatively, we update ϕ according to the neu-
ron assignments of this generated instance while keeping the generative weights
fixed. By iterating the wake and sleep phases, updating ϕ and θ alternatively,
the objective function F is minimized in an EM manner.
ϕ,θ
Now let’s compute the derivatives of F with respect to the generative
ϕ,θ
weights. We can write out Equation (7) as
F =E [logQ (α|d)]−E [logP (α,d)] (15)
Qϕ ϕ Qϕ θ
Since ϕ and θ are decoupled, the first term in Equation (15) is constant when
computing ∂F, thus we write out the second term as
∂θ
(cid:88)
E [logP (α,d)]= Q (α|d)logP (α,d) (16)
Qϕ θ ϕ θ
α
As the latent cause α is generated by random sampling as a single instance, the
summation over all possible hidden causes in Equation (16) is ignored with the
weighting term Q (α|d), thus the objective is further simplified as
ϕ
(cid:88) (cid:88) 1+sm 1−sm
logP (α,d)= ( n log[pm(θ,sm+1)]+ n log[1−pm(θ,sm+1)])
θ 2 n 2 n
m≥1 n
(17)
If we plug in Equation (11) and calculate its derivatives with respect to θ and
b, we easily derive the local delta rule,
∂F
=−sm+1(sm−pm) (18)
∂θm+1,m k n n
k,n
∂F
=−(sm−pm) (19)
∂bm+1,m n n
n
8 J. Liu
Toupdatetherecognitionweightsϕinthesleepphase,weexchangetherelative
positions of P and Q, using F˜ =E [logP (α,d)]−E [logQ (α|d)] as a mod-
Pθ θ Pθ ϕ
ified objective function, then the similar deductions follow which give the same
local delta rules for recognition weights. Finally, all the parameters are updated
by gradient descent x=x−γ∂f, where γ is the learning step size.
∂x
Remark 3. From the derivation of the local delta rule we can tell, by decou-
plingtheforwardandbackwardpasses,andupdatingbasedonasinglesampled
instance,theobjectivefunctionissimplifiedtoomuchtobeconsideredasvaria-
tional or free-energy. However, this working algorithm is computationally cheap
andefficient,anditapproximatesthetrueobjectiveinanensemblesense,which
means the system minimizes the variational free energy after enough rounds of
iterationswithsufficientaccuracy(probablyconvergestoalocalminimainstead
of the global minima).
Remark 4. The local delta rule is the simplest updating rule we could derive
fromEquation(15).IfwekeeptheweightingtermQ (α|d)inEquation(16),the
ϕ
local delta rule will also be weighted by this term, which gives what we call the
weighted local delta rule. In [6], another rule is given by replacing the stochastic
neuronactivitieswiththeirholisticmean,whichisthecalculatedprobabilitiesof
neuronactivations.Inourpreliminaryexperiments,westartedwiththeclassical
local delta rules. The comparison and evaluation of all updating rules will be
reserved for future work.
Here we present a brief account of the two-phase training mechanism of the
HelmholtzmachineincomparisontotheVAE.TheHelmholtzmachineiswidely
acknowledgedasthepredecessorofVAEandbothneuralnetworksresorttovari-
ational machine learning. Instead of alternative two-phase training, VAE uses
a single objective function that jointly optimizes the recognition and genera-
tive processes. The advantage of the wake-sleep algorithm, for our application
purposes, mainly lies in three areas:
1. The decoupling of recognition and generation spares a whole lot of space
forcreativemanipulationandsubtlemediationbetweenthesetwoprocesses,
thusavailablethegroundforstudyingcomputationalcreativityandforaging
artistic usage of the model (future directions).
2. The idea of analysis-by-synthesis and inverting the hierarchical model in
real-time make active inference and real-time modifications possible. One of
the biggest differences between the Helmholtz machine and the VAE is that
thegenerationintheHelmholtzmachineisunconstrained.It’snotsubjectto
the maximum likelihood and any generated sample could be used to update
themodelparameters.Inotherwords,theHelmholtzmachinepresentsmore
flexibility in the training process, a desired feature to serve our purposes.
3. Thehierarchicalstructureandforward-backwardconnectionsintheHelmholtz
machine are a good parallel for the cortical hierarchies in our brain. This
point be illustrated in more detail in the following arguments.
A Neural Network Implementation for Free Energy Principle 9
2.3 Hierarchical Model for the Brain
It’s contended extensively in FEP-related works that the cortical responses in
our brain observe a hierarchical model with forward and backward connections,
where the forward driving connections convey prediction errors from a lower
areatoahigherarea,andnonlinearbackwardconnectionsconstructpredictions
[8] [9] [12]. The brain trying to infer the causes of its sensory inputs and gener-
ating corresponding sensations is also a prevailing idea in FEP. As the author
doesn’t have a real background in neurobiology, the statements in this subsec-
tioncouldn’tbepresentedwithtoomuchprecisionordetail.Butingeneral,the
ideaofanalysis-by-synthesisiswelldemonstratedintheHelmholtzmachine,and
Hinton pointed out that one of their motivations for developing the Helmholtz
machine is inspired by Friston’s cortical hierarchy theory [17]. In [12], Friston
also cited the paper on the Helmholtz machine [6]. By this cross-referencing, we
believe there is enough evidence to link the brain architecture to the Helmholtz
machine,andstudythebrainfunctionssuchaspredictivecoding,messagepass-
ing,andpredictiveprocessingviatheHelmholtzmachine,atleastatareasonable
metaphorical level. Two preliminary comments could be made at this stage by
theworkingmechanismoftheHelmholtzmachine,whilethevalidationandmore
systematicdiscussionscouldonlyberealizedafterthenumericalexperimentsare
carried out and a detailed examination is applied in future work.
1. In predictive processing [4], the processing of sensory inputs goes both ways
within the hierarchical model. Besides the bottom-up passive pattern recog-
nition of the stimuli, the brain also actively constructs the predictions via
top-down connections. The backward connections regulate the precision of
prediction errors that allow the system to cope with noisy and ambiguous
sensory inputs. In other words, the separate treatment of forward and back-
ward connections, which correspond to the wake and sleep phases in the
Helmholtz machine, is imperative to study PP (predictive processing) and
PC(predictivecoding)relatedbrainfunctions.Besides,duetothefunctional
asymmetry of forward and backward connections in the brain[12], where
backward connections are more modulatory or nonlinear in their effects on
neuronalresponses,it’salsoadvantageoustodecouplethetwoprocessesand
treat them separately, which allow more flexibility.
2. In hierarchical message passing, the synapses are characterized by their lo-
cal efficacy. It means that the prediction errors are resolved by each neuron
locally, which only receives responses from neurons at its current and pre-
cedinglevels.Thislocalefficacywellcorrespondstothelocaldeltaupdating
rulesderivedintheHelmholtzmachine.Thisconditionisimportantbecause
it permits a biologically plausible implementation, where the connections
driving inference run only between neighboring levels [8].
3 Experiment
The preliminary experiment is designed by a 4-layer Helmholtz machine, with
10,8,5,3neuronsineachlayerrespectively(seeFig.2).Thissectionwillpresent
10 J. Liu
a detailed experimental setup with data design and two-stage training, as well
as the experimental result which preserves adequate generative diversity, while
boosting the generation accuracy above 0.99 in the meantime (please see the
codes in my GitHub).
3.1 Training Stage I: Experimental Setup
Fig.2.HelmholtzMachinewithActiveInference.Theexperimentisimplementedwith
a 4-layer Helmholtz machine, with 10,8,5,3 neurons in each layer ascendingly. In the
sleep phase, activities in layer 4 are generated by generative bias from unity. The
training is implemented in two stages. In stage I, as the single lines connecting the
training set and the model on the left side indicate, the inputs are from the well-
formedregionandthegenerationsareunconstrained,whichfallsanywhereintheentire
space; in stage II, the parameters are fine-tuned by restricting the generations within
the well-formed region while actively deforming the input distribution based on the
modelgenerations(doublelinesontherightside),resultingintheselectedregionthat
conforms to the latent model representations.
A Neural Network Implementation for Free Energy Principle 11
The data design is inspired by the phenotypic boundary discussed in [14].
For a phenotype to exist it must possess defining characteristics or traits. These
traits essentially limit the agent to a bounded region in the space of all states
it could be in. Once outside these bounds, it ceases to possess that trait (cf, a
fishoutofwater).Thisboundedregioncorrespondstothewell-formedregionin
Fig. 2 and the entire space represents all possible states of the world.
To construct a valid subset of all 1024 possibilities of the combination of
binary-valued first-layer data neurons (210), we devise three well-formed rules
inspired by the musical gestalt. Metaphorically, we consider the 10 neurons as
a 10-note sequence, which represents a rhythmic pattern – 0 or −1 denotes the
rest (we will use 0 for discussion convenience but in numerical implementation
it’s always replaced by −1) and 1 denotes a percussive attack. Then the rules
entail, what is a valid rhythmic pattern?
Rule 1 The sequence always starts with 1.
Rule 2 Forbid single event that’s strongly isolated from other groups (00100),
andalsoavoidisolatedeventatthebeginning(100)andtheend(001)ofthe
sequence.
Rule 3 Forbid the extended break (0000).
Wewon’tgivefurtherexplanationsforthedesigninglogicfortheserules,butwe
referthereaderswhoareinterestedto[22]forabetterunderstandingofmusical
grouping structures.
Theadvantageofrule-basedgenerationisthat,wehaveametrictoassessthe
goodness of the generated samples by simply checking them against the rules,
thus the model performance could be measured with certainty. In training stage
I, we use this generated well-formed set as inputs, and update the recognition
weights with arbitrarily generated instances (see the single lines connecting the
data and machine on the left side in Fig. 2). As explained in [18], the model
aims to find the economical latent representations that prescribe the minimum
description length. After sufficient iterations, the generation accuracy reached
0.94±0.01, and couldn’t be further improved by repeating the current training.
3.2 Training Stage II: Active Inference
IntrainingstageII,wefine-tunethemodeltrainedinstageIbyactiveinference,
which boosted the generation accuracy to 0.99+ with only 200 rounds of itera-
tions.Inthisstage,thegeneratedinstancesinthesleepphasearefilteredbythe
well-formed rules, thus only valid generations within the phenotypic bounds are
acceptedtotraintherecognitionweights.Inthemeantime,thevalidgenerations
aremaintainedtoactivelymodifythedistributionoftheinputset.Themorean
instance is generated, the more salient it becomes in the evidence distribution.
This distribution modification could be viewed either as salience[24], in a
similar way of executing eye movements to sample the sensations that conform
to the agent’s expectations; or as niche construction[27], that renders real mod-
ifications on the environment such as the ”desire path”. Either way, the data
12 J. Liu
distribution changes due to active inference, thus the given input data (data
in the well-formed region) becomes actively sampled data (data in the selected
region) that reflects the current representations in the ”brain” (see the double
lines on the right side connecting the data and machine in Fig. 2).
Fig.3.ReconfiguredDataDistributionbyActiveSampling.Thewell-formedsetgives
auniforminitialdistributionoverallvaliddatapoints(inblue),whichiscontinuously
deformed to the distribution in orange by active sampling.
The input data distribution is described in Fig. 3. The FEP-based active
training continuously deforms the uniform equal-probability distribution of the
initial dataset (in blue) to the active sampled distribution (in orange) that fits
the internal representation and capacity of the machine. After this second-stage
fine-tuning, the Helmholtz machine is able to generate almost 100% accurate
samples with a more focused range within all possibilities of the well-formed set
while keeping generative diversity to a satisfactory degree.
References
1. Allen, M., Friston, K.J.: From cognitivism to autopoiesis: towards a computa-
tional framework for the embodied mind. Synthese 195(6), 2459–2482 (2018).
https://doi.org/10.1007/s11229-016-1288-5
2. Blei, D.M., Kucukelbir, A., McAuliffe, J.D.: Variational Inference: A
Review for Statisticians. arXiv e-prints arXiv:1601.00670 (Jan 2016).
https://doi.org/10.48550/arXiv.1601.00670
3. Carhart-Harris, R.L., Friston, K.J.: The default-mode, ego-functions and free-
energy: a neurobiological account of freudian ideas. Brain 133(4), 1265–1283
(2010). https://doi.org/10.1093/brain/awq010
4. Clark,A.:Radicalpredictiveprocessing.SouthernJournalofPhilosophy53,3–27
(2015). https://doi.org/10.1111/sjp.12120
A Neural Network Implementation for Free Energy Principle 13
5. Constant, A., Ramstead, M.J., Veissiere, S.P., Campbell, J.O., Friston, K.J.: A
variational approach to niche construction. Journal of the Royal Society Interface
15(141), 20170685 (2018). https://doi.org/10.1098/rsif.2017.0685
6. Dayan,P.,Hinton,G.E.,Neal,R.M.,Zemel,R.S.:Thehelmholtzmachine.Neural
computation 7(5), 889–904 (1995). https://doi.org/10.1162/neco.1995.7.5.889
7. Demekas, D., Parr, T., Friston, K.J.: An investigation of the free energy principle
for emotion recognition. Frontiers in Computational Neuroscience 14, 30 (2020).
https://doi.org/10.3389/fncom.2020.00030
8. Friston, K.: A theory of cortical responses. Philosophical transactions
of the Royal Society B: Biological sciences 360(1456), 815–836 (2005).
https://doi.org/10.1098/rstb.2005.1622
9. Friston, K.: Hierarchical models in the brain. PLoS computational biology 4(11),
e1000211 (2008). https://doi.org/10.1371/journal.pcbi.1000211
10. Friston,K.:Thefree-energyprinciple:aunifiedbraintheory?Naturereviewsneu-
roscience11(2),127–138(2010).https://doi.org/https://doi.org/10.1038/nrn2787
11. Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., ODoherty, J., Pezzulo,
G.: Active inference and learning. Neuroscience and Biobehavioral Reviews 68,
862–879 (2016). https://doi.org/https://doi.org/10.1016/j.neubiorev.2016.06.022,
https://www.sciencedirect.com/science/article/pii/S0149763416301336
12. Friston, K., Kiebel, S.: Predictive coding under the free-energy principle. Philo-
sophicaltransactionsoftheRoyalSocietyB:Biologicalsciences364(1521),1211–
1221 (2009). https://doi.org/10.1098/rstb.2008.0300
13. Friston, K., Rigoli, F., Ognibene, D., Mathys, C., Fitzgerald, T., Pezzulo, G.:
Activeinferenceandepistemicvalue.CognitiveNeuroscience6(4),187–214(2015).
https://doi.org/10.1080/17588928.2015.1020053, pMID: 25689102
14. Friston, K.J., Daunizeau, J., Kiebel, S.J.: Reinforcement learning or active infer-
ence? PloS one 4(7), e6421 (2009). https://doi.org/10.1371/journal.pone.0006421
15. Friston, K.J., Parr, T., Yufik, Y., Sajid, N., Price, C.J., Holmes, E.: Generative
models, linguistic communication and active inference. Neuroscience & Biobehav-
ioral Reviews 118, 42–64 (2020). https://doi.org/10.1016/j.neubiorev.2020.07.005
16. Friston, K.J., Rosch, R., Parr, T., Price, C., Bowman, H.: Deep temporal models
and active inference. Neuroscience & Biobehavioral Reviews 90, 486–501 (2018).
https://doi.org/10.1016/j.neubiorev.2017.04.009
17. Hinton, G.E.: Lecture 13.4 - the wake sleep algorithm (2017), https:
//www.youtube.com/watch?v=FBkhbqrFyo4&list=PLLssT5z_DsK_gyrQ_
biidwvPYCRNGI3iv&index=63
18. Hinton, G.E., Dayan, P., Frey, B.J., Neal, R.M.: The” wake-sleep” algo-
rithm for unsupervised neural networks. Science 268(5214), 1158–1161 (1995).
https://doi.org/10.1126/science.7761831
19. Kingma, D.P., Welling, M.: Auto-Encoding Variational Bayes. arXiv e-prints
arXiv:1312.6114 (Dec 2013). https://doi.org/10.48550/arXiv.1312.6114
20. Kirchhoff, M., Parr, T., Palacios, E., Friston, K., Kiverstein, J.: The
markov blankets of life: autonomy, active inference and the free energy
principle. Journal of The royal society interface 15(138), 20170792 (2018).
https://doi.org/10.1098/rsif.2017.0792
21. Koelsch, S., Vuust, P., Friston, K.: Predictive processes and the pe-
culiar case of music. Trends in cognitive sciences 23(1), 63–77 (2019).
https://doi.org/10.1016/j.tics.2018.10.006
22. Lerdahl, F., Jackendoff, R.S.: A Generative Theory of Tonal Music, reissue, with
a new preface. MIT press (1996)
14 J. Liu
23. Mazzaglia, P., Verbelen, T., C¸atal, O., Dhoedt, B.: The free energy principle for
perception and action: A deep learning perspective. Entropy 24(2), 301 (2022).
https://doi.org/10.3390/e24020301
24. Parr, T., Friston, K.J.: Attention or salience? Current opinion in psychology 29,
1–5 (2019). https://doi.org/10.1016/j.copsyc.2018.10.006
25. Parr, T., Friston, K.J.: Generalised free energy and active inference. Biological
cybernetics 113(5-6), 495–513 (2019). https://doi.org/10.1007/s00422-019-00805-
w
26. Tschantz, A., Millidge, B., Seth, A.K., Buckley, C.L.: Reinforcement
learning through active inference. arXiv preprint arXiv:2002.12636 (2020).
https://doi.org/10.48550/arXiv.2002.12636
27. Veissi`ere, S.P.L., Constant, A., Ramstead, M.J.D., Friston, K.J., Kir-
mayer, L.J.: Thinking through other minds: A variational approach to
cognition and culture. Behavioral and Brain Sciences 43, e90 (2020).
https://doi.org/10.1017/S0140525X19001213

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "A Neural Network Implementation for Free Energy Principle"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
