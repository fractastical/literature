=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms
Citation Key: podusenko2021aida
Authors: Albert Podusenko, Bart van Erp, Magnus Koudahl

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2021

Abstract: In this paper we present AIDA, which is an active inference-based agent that iteratively designs
apersonalizedaudioprocessingalgorithmthroughsituatedinteractionswithahumanclient. The
targetapplicationofAIDAistoproposeon-the-spotthemostinterestingalternativevaluesforthe
tuningparametersofahearingaid(HA)algorithm,wheneveraHAclientisnotsatisfiedwiththeir
HA performance. AIDA interprets searching for the "most interesting alternative" as an issue of
optimal(acoustic)context-awareBayesiantrialdesign....

Key Terms: audio, aida, processing, design, inference, active, agent, eindhoven, algorithms, thenetherlands

=== FULL PAPER TEXT ===

AIDA: AN ACTIVE INFERENCE-BASED DESIGN AGENT FOR
AUDIO PROCESSING ALGORITHMS
APREPRINT
AlbertPodusenko∗†,1,BartvanErp†,1,MagnusKoudahl†,1,2,BertdeVries1,3
1BIASlab,DepartmentofElectricalEngineering,EindhovenUniversityofTechnology,Eindhoven,TheNetherlands
2NestedMindsSolutions,Liverpool,England
3GNHearing,Eindhoven,TheNetherlands
∗a.podusenko@tue.nl
January11,2022
ABSTRACT
In this paper we present AIDA, which is an active inference-based agent that iteratively designs
apersonalizedaudioprocessingalgorithmthroughsituatedinteractionswithahumanclient. The
targetapplicationofAIDAistoproposeon-the-spotthemostinterestingalternativevaluesforthe
tuningparametersofahearingaid(HA)algorithm,wheneveraHAclientisnotsatisfiedwiththeir
HA performance. AIDA interprets searching for the "most interesting alternative" as an issue of
optimal(acoustic)context-awareBayesiantrialdesign. Incomputationalterms,AIDAisrealizedas
anactiveinference-basedagentwithanExpectedFreeEnergycriterionfortrialdesign. Thistypeof
architectureisinspiredbyneuro-economicmodelsonefficient(Bayesian)trialdesigninbrainsand
impliesthatAIDAcomprisesgenerativeprobabilisticmodelsforacousticsignalsanduserresponses.
Weproposeanovelgenerativemodelforacousticsignalsasasumoftime-varyingauto-regressive
filters and a user response model based on a Gaussian Process Classifier. The full AIDA agent
hasbeenimplementedinafactorgraphforthegenerativemodelandalltasks(parameterlearning,
acousticcontextclassification,trialdesign,etc.) arerealizedbyvariationalmessagepassingonthe
factorgraph. Allverificationandvalidationexperimentsanddemonstrationsarefreelyaccessibleat
ourGitHubrepository.
Keywords Activeinference,Bayesiantrialdesign,Hearingaids,Noisereduction,Probabilisticmodeling,Source
separation,Speechenhancement,VariationalMessagepassing
1 Introduction
Hearingaids(HA)areoftenequippedwithspecializednoisereductionalgorithms. Thesealgorithmsaredevelopedby
teamsofengineerswhoaimtocreateasingleoptimalalgorithmthatsuitsanyuserinanysituation. Takingaone-size-
fits-allapproachtoHAalgorithmdesignleadstotwoproblemsthatareprevalentthroughouttoday’shearingaidindustry.
First,modelingallpossibleacousticenvironmentsissimplyinfeasible. ThedailylivesofHAusersarevariedandthe
differentenvironmentstheytraverseevenmoreso. Givendifferingacousticenvironments,asinglestaticHAalgorithm
cannotpossiblyaccountforalleventualities-evenwithouttakingintoaccounttheparticularconstraintsimposedbythe
HAitself,suchaslimitedcomputationalpowerandallowedprocessingdelays[1]. Secondly,hearinglossishighly
personalandcandiffersignificantlybetweenusers. EachHAuserconsequentlyrequirestheirown,individuallytuned
HAalgorithmthatcompensatesfortheiruniquehearinglossprofile[2,3,4]andsatisfiestheirpersonalpreferencesfor
parametersettings[5].ConsideringthatHAsnowadaysoftenconsistofmultipleinterconnecteddigitalsignalprocessing
unitswithmanyintegratedparameters,thetaskofpersonalizingthealgorithmrequiresexploringahigh-dimensional
searchspaceofparameters, whichoftendonotyieldaclearphysicalinterpretation. Thecurrentmostwidespread
approachtopersonalizationrequirestheHAusertophysicallytraveltoanaudiologistwhomanuallytunesasubsetof
allHAparameters. Thisisaburdensomeactivitythatisnotguaranteedtoyieldanimprovedlisteningexperiencefor
theHAuser.
2202
naJ
01
]SA.ssee[
2v66331.2112:viXra
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms Podusenkoetal.
Fromthesetwoproblems,itbecomesclearthatweneedtomovetowardsanewapproachforhearingaidalgorithm
designthatempowerstheuser. Ideally,usersshouldbeincontroloftheirownHAalgorithmsandshouldbeableto
changeandupdatethematwillinsteadofhavingtorelyonteamsofengineersthatoperatewithlongdesigncycles,
separatedfromtheusers’livingexperiences.
Thequestionthenbecomes,howdowemovetheHAalgorithmdesignawayfromengineersandintothehandsofthe
user? WhileanaiveimplementationthatallowsfortuningHAparameterswithsliderson,forexample,asmartphoneis
trivialtodevelop,evenasmallnumberofadjustableparametersgivesrisetoalarge,high-dimensionalsearchspace
thattheHAuserneedstolearntonavigate. Thisputsalargeburdenontheuser,essentiallyaskingthemtobetheirown
trainedaudiologist. Clearly,thisisnotatrivialtaskandthisapproachisonlyfeasibleforasmallsetofparameters,
whichcarryaclearphysicalinterpretation. Instead,wewishtosupporttheuserwithanagentthatintelligentlyproposes
newparametertrials. Inthissetting,theuserisonlytaskedtocast(positiveornegative)appraisalsofthecurrentHA
settings. Basedontheseappraisal,theagentwillautonomouslytraversethesearchspacewiththegoalofproposing
satisfyingparametervaluesforthatuserunderthecurrentenvironmentalconditionsinasfewtrialsaspossible.
Designinganintelligentagentthatlearnstoefficientlynavigateaparameterspaceisnottrivial. Inthesolutionapproach
inthispaper,werelyonaprobabilisticmodelingapproachinspiredbythefreeenergyprinciple(FEP)[6]. TheFEPisa
frameworkoriginallydesignedtoexplainthekindsofcomputationsthatbiological,intelligentagents(suchasthehuman
brain)mightbeperforming.RecentyearshaveseentheFEPappliedtothedesignofsyntheticagentsaswell[7,8,9,10].
AhallmarkfeatureofFEP-basedagentsisthattheyexhibitadynamictrade-offbetweenexplorationandexploitation
[11,12,13],whichisahighlydesirablepropertywhenlearningtonavigateanHAparameterspace. Concretely,the
FEPproposesthatintelligentagentsshouldbemodeledasprobabilisticmodels. Thesetypesofmodelsdonotonly
yieldpointestimatesofvariables,butalsocaptureuncertaintythroughmodelingfullposteriorprobabilitydistributions.
Furthermore,userappraisalsandactionscanbenaturallyincorporatedbysimplyextendingtheprobabilisticmodel.
Takingamodel-basedapproach alsoallows forfewerparametersthanalternativedata-driven solutions, as wecan
incorporatefield-specificknowledge,makingitmoresuitableforcomputationallyconstrainedhearingaiddevices. The
noveltyofourapproachisrootedinthefactthattheentireproposedsystemsisframedasaprobabilisticgenerative
modelinwhichwecanperform(active)inferencethrough(expected)freeenergyminimization.
In this paper we present AIDA1, an active inference-based design agent for the situated development of context-
dependentaudioprocessingalgorithms,whichprovidestheuserwithherowncontrollableaudioprocessingalgorithm.
ThisapproachembodiesanFEP-basedagentthatoperatesinconjunctionwithanacousticmodelandactivelylearns
optimalcontext-dependenttuningparametersettings. Afterformallyspecifyingtheproblemandsolutionapproachin
Section2wemakethefollowingcontributions:
1. Wedevelopamodularprobabilisticmodelthatembodiessituated,(acoustic)scene-dependent,andpersonalized
designofitscorrespondinghearingaidalgorithminSection3.1.
2. Wedevelopanexpectedfreeenergy-basedagent(AIDA)inSection3.2,whoseproposalsfortuningparameter
settingsarewell-balancedintermsofseekingmoreinformationabouttheuser’spreferences(explorativeagent
behavior)versusseekingtooptimizetheuser’ssatisfactionlevelsbytakingadvantageofpreviouslylearned
preferences(exploitativeagentbehavior).
3. InferenceintheacousticmodelandAIDAiselaborateduponinSection4andtheiroperationsareindividually
verified through representative experiments in Section 5. Furthermore, all elements are jointly validated
throughademonstratorapplicationinSection 5.4.
WehaveintentionallypostponedamorethoroughreviewofrelatedworktoSection7aswedeemitmorerelevantafter
theintroductionofoursolutionapproach. Finally,Section6discussesthenoveltyandlimitationsofourapproachand
Section8concludesthispaper.
2 Problemstatementandproposedsolutionapproach
2.1 Automatedhearingaidtuningbyoptimization
Inthispaperweconsidertheproblemofchoosingvaluesforthetuningparametersuofahearingaidalgorithmthat
processes an acoustic input signal x to output signal y. In Figure 1, we sketch an automated optimization-based
approachtothisproblem. Assumethatwehaveaccesstoageneric“signalquality”modelwhichratesthequalityofa
1Aidaisagirl’snameofArabicorigin,meaning“happy”.Weusethisnameasanabbreviationforan"ActiveInference-based
DesignAgent"thataimstomakeanenduser“happy”.
2
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms
data base group model
optimizer
Figure1: Aschematicoverviewoftheconventionalapproachtohearingaidalgorithmtuning. Heretheparametersof
thehearingaiduareoptimizedwithrespecttosomegenericuserratingmodelr(y)foralargedatabase ofinput
X
datax.
HAoutputsignaly =f(x,u),asafunctionoftheHAinputxandparametersu,byaratingr(x,u)(cid:44)r(y). Ifwe
runthissystemonarepresentativesetofinputsignalsx ,thenthetuningproblemreducestotheoptimizationtask
∈X
(cid:88)
u∗ =argmax r(x,u). (1)
u
x∈X
Unfortunately,incommercialpractice,thisoptimizationapproachdoesnotalwaysresultinsatisfactoryHAperformance,
because of two reasons. First, the signal quality models in the literature have been trained on large databases of
preferenceratingsfrommanyusersandthereforeonlymodeltheaverageHAclientratherthananyspecificclient
[14,15,16,17,18,19]. Secondly,theoptimizationapproachaveragesoveralargesetofdifferentinputsignals,soit
willnotdealwithacousticcontext-dependentclientpreferences. Byacousticcontext,weconsidersignalpropertiesthat
dependonenvironmentalconditionssuchasbeinginside,outside,inacaroratthemall. Generally,clientpreferences
forHAtuningparametersarebothhighlypersonalandcontext-dependent. Therefore,thereisaneedtodevelopa
personalized,context-sensitivecontrollerfortuningHAparametersu.
2.2 Situatedhearingaidtuningwiththeuserin-the-loop
Inthispaper,wewilldevelopapersonalized,context-awaredesignagent,basedonthearchitectureshowninFigure2.
IncontrasttoFigure1,theoutsideworld(ratherthanadatabase)producesaninputsignalxundersituatedconditions
thatisprocessedbyahearingaidalgorithmtoproduceanoutputsignaly. Aparticularhumanhearingaidclientlistens
tothesignaly andisinvitedtocastatanytimebinaryappraisalsr 0,1 aboutthecurrentperformanceofthe
∈ { }
hearingaidalgorithm,where1and0correspondtotheuserbeingsatisfiedandunsatisfied,respectively. Context-aware
trialsforHAtuningparametersareprovidedbyAIDA.Ratherthananofflinedesignprocedure,thewholesystem
designscontinuallyundersituatedconditions. TheHAdeviceitselfhousesacustomhearingaidalgorithm,basedon
stateinferenceinagenerativeacousticmodel. Theacousticmodelcontainstwosub-models: 1)asourcedynamics
modeland2)acontextdynamicsmodel.
Inferenceintheacousticmodelisbasedontheobservedsignalxandyieldstheoutputyandcontextc. Basedonthis
contextsignalcandprevioususerappraisalsr,AIDAwillactivelyproposenewparameterstrialsuwiththegoalof
makingtheuserhappy. Technically,theobjectiveisthatAIDAexpectstoreceivefewernegativeappraisalsinthefuture,
relativetonotmakingparameteradaptations,seeSection3.2fordetails.
ThedesignofAIDAisnon-trivial. Forinstance,sincethereisapriorinopersonalizedmodelofHAratingsforany
particularuser,AIDAwillhavetobuildsuchamodelon-the-flyfromthecontextcanduserappraisalsr. Sincethe
systemoperatesundersituatedconditions,wewanttoimposeaslittleburdenontheenduseraspossible. Asaresult,
mostuserswillonlyonceinawhilecastanappraisalandthiscomplicatesthelearningofapersonalizedHArating
model.
Tomakethisdesireforverylight-weightinteractionsconcrete,wenowsketchhowweenvisionatypicalinteraction
betweenAIDAandaHAclient. AssumethattheHAclientisinaconversationwithafriendatarestaurant. Thesignal
ofinterest,inthiscase,isthefriend’sspeechsignalwhiletheinterferingsignalisanenvironmentalbabblenoisesignal.
TheHAalgorithmtriestoseparatetheinputsignalxintoitsconstituentspeechandnoisesourcecomponents,then
appliesgainsutoeachsourcecomponentandsumstheseweightedsourcesignalstoproduceoutputy. IftheHA
3
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms Podusenkoetal.
acoustic model
source dynamics
context
dynamics
user response
model
AIDA
Figure2: AschematicoverviewoftheproposedsituatedHAdesignloopcontainingAIDA.Anincomingsignalx
entersthehearingaidandisusedtoinferthecontextoftheuserc. Basedonthiscontextandprevioususerappraisals,
AIDA proposes a new set of parameters u for the hearing aid algorithm. Based on the input signal, the proposed
parametersandthecurrentcontext,theoutputy ofthehearingaidisdetermined,whichareusedtogetherwiththe
contextinthehearingaidalgorithm. TheparametersuareactivelyoptimizedbyAIDA,basedontheinferredcontext
cfromtheinputsignalxandappraisalsr fromtheuserintheloop. Allindividualsubsystemsrepresentpartsofa
probabilisticgenerativemodelasdescribedinSection3,wherethecorrespondingalgorithmsfollowsfromperforming
probabilisticinferenceinthesemodelsasdescribedinSection4.
clientishappywiththeperformanceofherHA,shewillnotcastanyappraisals. Afterall,sheisinthemiddleofa
conversationandhasnoimperativetochangetheHAbehavior. However,ifshecannotunderstandherconversation
partner,theclientmaycovertlytapherwatchormakeanothergesturetoindicatethatsheisnothappywithhercurrent
HAsettings. Inresponse,AIDA,whichmaybeimplementedasasmartwatchapplication,willreplyinstantaneouslyby
sendingatuningparameterupdateutothehearingaidalgorithminanefforttofixtheclient’scurrenthearingproblem.
Sincetheclient’spreferencesarecontext-dependent,AIDAneedstoincorporateinformationabouttheacousticcontext
fromHAinputx. Asanexample,theHAusermightleavetherestaurantforawalkoutside. Walkingoutsidepresentsa
differenttypeofbackgroundnoiseandconsequentlyrequiresdifferentparametersettings.
Crucially,wewouldlikeHAclientstobeabletotunetheirhearingaidswithoutinterruptionofanyongoingactivities.
Therefore,wewillnotdemandthattheclienthastofocusvisualattentiononinteractingwithasmartphoneapp. At
most, we want the client to apply a tap or make a simple gesture that does not draw any attention away from the
ongoingconversation. Asecondcriterionisthatwedonotwanttheconversationpartnertonoticethattheclientis
interactingwiththeagent. Theclientmayactuallybeinasituation(e.g.,abusinessmeeting)whereitisnotappropriate
todemonstratethatherprioritieshaveshiftedtotuningherhearingaids. Inotherwords,theinteractionsmustbevery
light-weightandcovert. Athirdcriterionisthatwewanttheagenttolearnfromasfewappraisalsaspossible. Notethat,
iftheHAhas10tuningparametersand5interestingvalues(verylow,low,middle,high,veryhigh)perparameter,
thenthereare510(about10million)parametersettings. Wedonotwanttheclienttogetengagedinanendlessloopof
disapprovingnewHAproposalsasthiswillleadtofrustrationanddistractionfromtheongoingconversation. Clearly,
thismeansthateachupdateoftheHAparameterscannotbeselectedrandomly: wewanttheagenttoproposethemost
interestingvaluesforthetuningparameters,basedonallobservedpastinformationandcertaingoalcriteriaforfuture
HAbehavior. InSection4.2,wewillquantifywhatmostinterestingmeansinthiscontext.
Inshort,thegoalofthispaperistodesignanintelligentagentthatsupportsuser-drivensituateddesignofapersonalized
audioprocessingalgorithmthroughaverylight-weightinteractionprotocol.
4
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms
Inordertoaccomplishthistask,wewilldrawinspirationfromthewayhowhumanbrainsdesignalgorithms(e.g.,for
speechandobjectrecognition,ridingabike,etc.) solelythroughenvironmentalinteractions. Specifically,webasethe
designofAIDAontheActiveInference(AIF)framework. Originatingfromthefieldofcomputationalneuroscience,
AIFproposestoviewthebrainasapredictionenginethatmodelssensoryinputs. Formally,AIFaccomplishesthis
throughspecifyingaprobabilisticgenerativemodelofincomingdata. PerformingapproximateBayesianinference
inthismodelbyminimizingfreeenergythenconstitutesaunifiedprocedureforbothdataprocessingandlearning.
Toselecttuningparametertrials,anAIFagentpredictstheexpectedfreeenergyinthenearfuture,givenaparticular
choiceofparametersettings. AIFprovidesasingle,unifiedmethodfordesigningallcomponentsofAIDA.Thedesign
ofaHAsystemthatiscontrolledbyanAIF-baseddesignagentinvolvessolvingthefollowingtasks:
1. Classificationofacousticcontext
2. Selectingacousticcontext-dependenttrialsfortheHAtuningparameters.
3. ExecutionoftheHAsignalprocessingalgorithm(thatiscontrolledbythetrialparameters).
Task 1 (context classification) involves determining the most probable current acoustic environment. Based on a
dynamiccontextmodel(describedinSection3.1.2),weinferthemostprobableacousticenvironmentasdescribedin
Section4.1.
Task2(trialdesign)encompassesproposingalternativesettingsfortheHAtuningparameters. Sections3.2and4.2
describetheuserresponsemodelandexecutionofAIDA’strialselectionprocedurebasedonexpectedfreeenergy
minimization,respectively.
Finally,task3(hearingaidalgorithmexecution)concernsperformingvariationalfreeenergyminimizationwithrespect
tothestatevariablesinagenerativeprobabilisticmodelfortheacousticsignal. InSection3.1wedescribethegenerative
acousticmodelunderlyingtheHAalgorithmandSection4.3describestheinferredHAalgorithmitself.
Crucially, intheAIFframework, allthreetaskscanbeaccomplishedbyvariationalfreeenergyminimizationina
generativeprobabilisticmodelforobservations. Sincewecanautomatevariationalfreeenergyminimizationbya
probabilisticprogramminglanguage,theonlyremainingtaskforthehumandesigneristospecifythegenerativemodels.
Thenextsectiondescribesthemodelspecification.
3 Modelspecification
In this section, we present the generative model of the AIDA controlled HA system, as illustrated in Figure 2. In
Section3.1,wedescribeagenerativemodelfortheHAinputandoutputsignalsxandyrespectively. Inthismodel,the
hearingaidalgorithmfollowsthroughperformingprobabilisticinference,aswillbediscussedinSection4. Partofthe
hearingaidalgorithmisamechanismforinferringthecurrentacousticcontext. InSection3.2weintroduceamodelfor
agentAIDAthatisusedtoinfernewparametertrials. Aconcisesummaryofthegenerativemodelisalsopresentedin
AppendixBandanoverviewofthecorrespondingsymbolsisgiveninTable2.
Throughoutthissection,wewillmakeuseoffactorgraphsforvisualizationofprobabilisticmodels. Inthispaperwe
focusonForney-stylefactorgraphs(FFG),asintroducedin[20]withnotationalconventionsadoptedfrom[21]. FFGs
representfactorizedfunctionsbyundirectedgraphs,whosenodesrepresenttheindividualfactorsoftheglobalfunction.
Thenodesareconnectedbyedgesrepresentingthemutualargumentsofthefactors. InanFFG,anodecanbeconnected
toanarbitrarynumberofedges,butedgesareconstrainedtohaveamaximumdegreeoftwo. Amoredetailedreview
ofprobabilisticmodelingandfactorgraphshasbeenprovidedinAppendixA.
3.1 Acousticmodel
Ouracousticmodeloftheobservedsignalandhearingaidoutputconsistsofamodelofthesourcedynamicsofthe
underlyingsignalsandamodelforthecontextdynamics.
3.1.1 Modelofsourcedynamics
Weassumethattheobservedacousticsignalxconsistsofaspeechsignal(ormoregenerally,atargetsignalthattheHA
clientwantstofocuson)andanadditivenoisesignal(thattheHAclientisnotinterestedin),as
x =s +n (2)
t t t
where x R represents the observed signal at time t, i.e. the input to the HA. The speech and noise signals are
t
represente ∈ dbys Randn R,respectively. Atthispoint,thesourcedynamicsofs andn needtobefurther
t t n t
∈ ∈
5
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms Podusenkoetal.
specified. Herewechoosetomodelthespeechsignalbyatime-varyingauto-regressivemodelandthenoisesignalbya
context-dependentauto-regressivemodel. Theremainderofthissubsectionwillelaborateonboththesesourcemodels
andwillfurtherspecifyhowthehearingaidoutputisgenerated. AnFFGvisualizationofthedescribedacousticmodel
isdepictedinFigure3.
Historically,Auto-Regressive(AR)modelshavebeenwidelyusedtorepresentspeechsignals[22,23]. Asthedynamics
ofthevocaltractexhibitnon-stationarybehavior,speechisusuallysegmentedintoindividualframesthatareassumed
tobequasi-stationary. Unfortunately,thesignalisoftensegmentedwithoutanypriorinformationaboutthephonetic
structureofthespeechsignal. Thereforethequasi-stationarityassumptionislikelytobeviolatedandtime-varying
dynamicsaremorelikelytooccurinthesegmentedframes[24]. Toaddressthisissue,wecanuseatime-varyingprior
forthecoefficientsoftheARmodel,leadingtoatime-varyingAR(TVAR)model[25]
θ (θ , ωI ) (3a)
t t−1 M
∼N
s (A(θ )s , V (γ)) (3b)
t t t−1
∼N
whereθ =[θ ,θ ,...,θ ] (cid:124) RM,s =[s ,s ,...,s ] (cid:124) RM arethecoefficientsandstatesofanM-th
t 1t 2t Mt ∈ t (cid:124) t t−1 t−M+1 ∈
orderTVARmodelforspeechsignals =e s . Weuse (µ,Σ)todenoteaGaussiandistributionwithmeanµand
t 1 t N
covariancematrixΣ. Inthismodel,theARcoefficientsθ arerepresentedbyaGaussianrandomwalkwithprocess
t
noisecovarianceωI ,withI denotingtheidentitymatrixofsize(M M),scaledbyω R . γ R represents
M M >0 >0
× ∈ ∈
theprocessnoiseprecisionmatrixoftheARprocess. Here,wehaveadoptedthestate-spaceformulationofTVAR
(cid:124)
modelsasin[26],whereV(γ)=(1/γ)e e createsacovariancematrixwithasinglenon-zeroentry. Weusee to
1 1 i
denoteanappropriatelysizedCartesianstandardunitvectorthatrepresentsacolumnvectorofzeroswhereonlytheith
entryis1. A(θ)denotesthecompanionmatrixofsize(M M),definedas
×
(cid:20) θ(cid:124) (cid:21)
A(θ)= . (4)
I 0
M−1
Multiplicationofastatevectorbythiscompanionmatrix,suchasA(θ )s ,basicallyperformstwooperations: an
(cid:124) t t−1
innerproductθ s andashiftofs byonetimesteptothepast.
t t−1 t−1
Theacousticmodelalsoencompassesamodelforbackgroundnoise,suchasthesoundsatabarortrainstation. Many
ofthesebackgroundsoundscanbewellrepresentedbycolorednoise[27],whichinturncanbemodeledbyalow-order
ARmodel[28,29]
n (A(ζ )n , V (τ )), fort=t−,t−+1,...,t+ (5)
t k t−1 k
∼N
whereζ = [ζ ,ζ ,...,ζ ] (cid:124) RN,n = [n ,n ,...,n ] (cid:124) RN arethecoefficientsandstatesofanAR
modelo
k
forder
1
N
k 2k
N+fo
N
rn
k
ois ∈ esignaln
t
=e
(cid:124)t
n
t
.
−
τ
1
R
t−N
de
+
n
1
otes ∈ theprocessnoiseprecisionoftheARprocess.
∈ t 1 t k ∈ >0
Incontrasttothespeechmodel, weassumetheprocessesζ andτ tobestationarywhentheuserisinaspecific
k k
acousticenvironmentorcontext. Tomakeclearthatcontextualstateschangemuchslowerthatrawacousticdatasignals,
weindextheslowerparametersattimeindexk,whichisrelatedtoindextby
(cid:24) (cid:25)
t
k = . (6)
W
Here, denotestheceilingfunctionthatreturnsthelargestintegersmallerorequalthanitsargument,whileW isthe
(cid:100)·(cid:101)
windowlength. TheaboveequationmakessurethatkisintuitivelyalignedwithsegmentsoflengthW,i.e. t [1,W]
∈
correspondstok =1. Todenotethestartandendindicesofthetimesegmentcorrespondingtocontextindexk,we
definet− =(k 1)W +1andt+ =kW asanimplicitfunctionofk,respectively. Thecontextcanbeassumedtobe
−
stationarywithinalongerperiodoftimecomparedtothespeechsignal. However,abruptchangesinthedynamicsof
backgroundnoisemayoccasionallyoccur. Forexample,iftheusermovesfromatrainstationtoabar,theparameters
oftheARmodelthatareattributedtothetrainstationwillnowinadequatelydescribethebackgroundnoiseofthe
newenvironment. Todealwiththesechangingacousticenvironments,weintroducecontext-dependentpriorsforthe
backgroundnoise,usingaGaussianandGammamixturemodel:
L
(cid:89)
ζ (µ ,Σ )clk (7a)
k l l
∼ N
l=1
L
(cid:89)
τ Γ(α ,β )clk (7b)
k l l
∼
l=1
The context at time index k, denoted by c , comprises a 1-of-L binary vector with elements c 0,1 , which
k lk
(cid:80) ∈ { }
are constrained by c = 1. Γ(α,β) represents a Gamma distribution with shape and rate parameters α and
l lk
6
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms
T = context
ck−1 Cat = = ck
GMM
ΓMM
ζk τk
←−ν(θ) →−ν(θ) ←−ν(γ) →−ν(γ)
nt−1 AR = nt ↑ ↓ ↑ ↓
ωI
A V
θt−1
N
= θt →−ν(x) A(θ) V(γ) ←−ν(y)
→ ←
γ = × N
←−ν←(x) →−ν→(y)
st−1 AR = st AR
e(cid:62)
1
e(cid:62)
1
+
xt
Figure3: (left)AForney-stylefactorgraphrepresentationoftheacousticsourcesignalsmodelasspecifiedby(3)-(2)
attimeindext. Theobservationx isspecifiedasthesumofalatentspeechsignals andalatentnoisesignaln .
t t t
Thespeechsignalismodeledbyatime-varyingauto-regressiveprocess,whereitscoefficientsθ aremodeledbya
t
Gaussianrandomwalk. Thenoisesignalisacontext-dependentauto-regressiveprocess,modeledbyGaussian(GMM)
andGammamixturemodels(ΓMM)fortheparametersζ andτ ,respectively. Theselectionvariableofthesemixture
k k
modelsrepresentsthecontextc . Themodelforthecontextdynamicsisenclosedbythedashedbox. Thecomposite
k
ARfactornoderepresentstheauto-regressivetransitiondynamicsspecifiedby(3b). (right)ThecompositeARnode
thatconcealsitsinternaloperationsfromtherestofthegraph[30]. Thearrowsshowthedirectionofincomingand
outgoingmessages.
β,respectively. Thehyperparametersµ ,Σ ,α andβ definethecharacteristicsofthedifferentbackgroundnoise
l l l l
environments.
Nowthatanacousticmodeloftheenvironmenthasbeenformallyspecified,wewillextendthismodelwiththegoal
of obtaining a HA algorithm. The principal goal of a HA algorithm is to improve audibility and intelligibility of
acousticsignals. Audibilitycanbeimprovedbyamplifyingthereceivedinputsignal. Intelligibilitycanbeimprovedby
increasingtheSignal-to-NoiseRatio(SNR)ofthereceivedsignal. Assumingthatwecaninfertheconstituentsource
signalss andn fromreceivedsignalx ,thedesiredHAoutputsignalcanbemodeledby
t t t
y =u s +u n , fort=t−,t−+1,...,t+ (8)
t sk t nk t
whereu = [u ,u ] (cid:124) [0,1]2 representsavectorof2tuningparametersorsource-specificgainsforthespeech
k sk nk
∈
andbackgroundnoisesignal,respectively. Inthisexpressiontheoutputofthehearingaidismodeledbyaweighted
sumoftheconstituentsourcesignals. Thegainscontroltheamplificationoftheextractedspeechandnoisesignals
individuallyandthusallowstheusertoperformsource-specificfiltering,alsoknownassoundscaping[31]. Because
ofimperfectionsduringinferenceofthesourcesignals(seeSection4),thegainssimultaneouslyreflectatrade-off
betweenresidualnoiseandspeechdistortion. WeshowtheFFGrepresentationoftheHAoutputinFigure4. Finding
goodvaluesforthegainsucanbeadifficulttaskbecausethepreferredparametersettingsmaydependonthespecific
listenerandontheacousticcontext.
Next,wedescribetheacousticcontextmodelthatwillallowAIDAtomakecontext-dependentparameterproposals.
3.1.2 Modelofcontextdynamics
AsHAclientsmovethroughdifferentacousticbackgroundsettings,suchasbeinginacar,doinggroceries,watchingTV
athome,etc.) thepreferredparametersettingsforHAalgorithmstendtovary. Thecontextsignalallowstodistinguish
betweenthesedifferentacousticenvironments.
7
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms Podusenkoetal.
nt = nt
st = st
usk × × unk
+
yt
Figure4: AForney-stylefactorgraphrepresentationofthehearingaidoutputmodelof(8). Theoutputofthehearing
aidismodeledbyreweighingtheseparatedspeechandbackgroundnoisesignals.
Thehiddencontextstatevariablec attimeindexk isa1-of-Lencodedbinaryvectorwithelementsc 0,1 ,
k lk
(cid:80) ∈ { }
whichareconstrainedby c =1. Thiscontextisresponsiblefortheoperationsofthenoisemodelin(7). Context
l lk
transitionsaresupportedbyadynamicmodel
c Cat(Tc ), (9)
k k−1
∼
wheretheelementsoftransitionmatrixT,aredefinedasT = p(c = 1 c = 1),whichareconstrainedby
ij ik j,k−1
T [0,1]and
(cid:80)L
T =1. WemodeltheindividualcolumnsofTbyaD
|
irichletdistributionas
ij ∈ j=1 ij
T Dir(α ), (10)
1:L,j j
∼
whereα denotesthevectorofconcentrationparameterscorrespondingtothejth columnofT. Thecontextstateis
j
initializedbyacategoricaldistributionas
L L
(cid:89) (cid:88)
c Cat(π)= πcl0 suchthat π =1, (11)
0 ∼ l l
l=1 l=1
wherethevectorπ =[π ,π ,...,π ] (cid:124)containstheeventprobabilities,whoseelementscanbechosenasπ =1/Lif
1 2 L l
theinitialcontextisunknown. AnFFGrepresentationofthecontextdynamicsmodelisshowninthedashedboxin
Figure3.
3.2 AIDA’suserresponsemodel
ThegoalofAIDAistocontinuallyprovidethemost“interesting”settingsfortheHAtuningparametersu ,where
k
interestinghasbeenquantitativelyinterpretedbyminimizationofExpectedFreeEnergy. ButhowdoesAIDAknow
what the client wants? In order to learn the client’s preferences, she is invited to cast at any time her appraisal
r ∅,0,1 ofcurrentHAperformance. Tokeeptheuserinterfaceverylight,wewillassumethatappraisalsare
k
∈{ }
binary, encodedbyr = 0fordisapprovalandr = 1indicatingapositiveexperience. Ifauserdoesnotcastan
k k
appraisal,wewilljustrecordamissingvalue,i.e.,r =∅. Thesubscriptkforr indicatesthatwerecordappraisalsat
k k
thesamerateasthecontextdynamics.
Ifaclientsubmitsanegativeappraisalr =0,AIDAinterpretsthisasanexpressionthattheclientisnothappywith
k
thecurrentHAsettingsu inthecurrentacousticcontextc (andviceversaforpositiveappraisals). Tolearnclient
k k
preferencesfromtheseappraisals,AIDAholdsacontext-dependentgenerativemodeltopredictuserappraisalsand
updatesthismodelafterobservingactualappraisals. Inthispaper,weoptforaGaussianProcessClassifier(GPC)
modelasthegenerativemodelforbinaryuserappraisals. AGaussianProcess(GP)isaveryflexibleprobabilisticmodel
andGPCshavesuccessfullybeenappliedtopreferencelearninginavarietyoftasksbefore[32,33,34]. Foranin-depth
discussiononGPs,wereferthereaderto[35]. Specifically,thecontext-dependentuserresponsemodelisdefinedas
L
(cid:89)
v () GP(m (),K (, ))clk (12a)
k l l
· ∼ · · ·
l=1
r Ber(Φ(v (u ))). ifr 0,1 (12b)
k k k k
∼ ∈{ }
8
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms
ck
uk
GPM
vk
Φ Ber rk
Figure5: AForney-stylefactorgraphrepresentationoftheuserresponsemodelspecifiedby(12). Thecontextstatec
k
ispassedtotheGPMixture(GPM)nodeasaselectorvariableforitsarguments.
In (12a), v () is a latent function drawn from a mixture of GPs with mean functions m () and kernels K (, ).
k l l
· · · ·
Evaluating v () at the point u provides an estimate of user preferences. Without loss of generality, we can set
k k
·
m ()=0. Sincec isone-hotencoded,raisingtothepowerc servestoselecttheGPthatcorrespondstotheactive
l k lk
cont · ext. Φ()denotestheGaussiancumulativedistributionfunction, definedasΦ(x) = √1 (cid:82)x exp (cid:0) t2/2 (cid:1) dt.
· 2π −∞ −
Thismapin(12b)castsv (u )toaBernoulli-distributedvariabler . AnFFGrepresentationoftheuserresponse
k k k
modelisshowninFigure5.
4 Solvingtasksbyprobabilisticinference
ThissectionelaboratesonsolvingthethreetasksofSection2.2: 1)contextclassification,2)trialdesignand3)hearing
aidalgorithmexecution. Alltaskscanbesolvedthroughprobabilisticinferenceinthegenerativemodelspecifiedby
(2)-(12b)inSection3. Inthissection,theinferencegoalsareformallyspecifiedbasedonthepreviouslyproposed
generativemodel.
Fortherealizationoftheinferencetaskswewillusevariationalmessagepassinginafactorgraphrepresentationofthe
generativemodel. Messagepassing-basedinferenceishighlyefficient,modularandscaleswelltolargeinferencetasks
[36,37]. Withmessagepassing,inferencetasksinthegenerativemodelreducetoautomatableproceduresrevolving
aroundlocalcomputationsonthefactorgraphs.
Athoroughdiscussiononmessagepassingandrelatedtopicsisomittedhereforreadability,butmadeavailablein
AppendixAtoserveasreference.
4.1 Inferenceforcontextclassification
Theacousticcontextc describesthedynamicsofthebackgroundnoisemodelthrough(5)and(7). Fordeterminingthe
k
currentenvironmentoftheuser,thegoalistoinferthecurrentcontextbasedontheprecedingobservations. Technically
we are interested in determining the marginal distribution p(c x ), where the index range over t of x takes
k
|
1:t+
intoaccounttherelationbetweentandkasdefinedin(6). Inouronlinesetting,wewishtocalculatethismarginal
distributioniterativelybysolving
(cid:90)
p(c x ) p(z ,Ψ ,x z ,c ) p(c ,T c )
k
|
1:t+
∝
t−:t+ k t−:t+
|
t−−1 k k
|
k−1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
posterior observationmodel contextdynamics (13)
p(c ,z x ) dz dΨ dc dT.
·
k−1 t−−1
|
1:t−−1 t−−1:t+ k k−1
(cid:124) (cid:123)(cid:122) (cid:125)
prior
TheobservationmodelisfullyspecifiedbythemodelspecificationinSection3,similarlyasthecontextdynamics.
Thepriordistributionisajointresultoftheiterativeexecutionofboth(13)and(18),wherethelatterreferstotheHA
algorithmexecutionfromSection4.3. Thecalculationofthismarginaldistributionrendersintractableandtherefore
exactinferenceofthecontextisnotpossible. Thisisaresultof1)theintractabilityresultingfromtheautoregressive
model as described in the previous subsection and of 2) the intractability that is a result of performing message
passingwithmixturemodels. In(7)themodelstructurecontainsaNormalandGammamixturemodelfortheAR-
coefficientsandprocessnoiseprecisionparameter,respectively. Exactinferencewiththesemixturemodelsquickly
leadstointractableinferencethroughmessagepassing,especiallywhenmultiplebackgroundnoisemodelsareinvolved.
Therefore,weneedtoresorttoavariationalapproximationwheretheoutputmessagesofthesemixturemodelsare
constrainedtobewithintheexponentialfamily.
Althoughvariationalinferencewiththemixturemodelsisfeasible[30,38,39],itispronetoconvergetolocalminima
oftheBethefreeenergy(BFE)formorecomplicatedmodels. Thevariationalmessagesoriginatingfromthemixture
9
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms Podusenkoetal.
modelsareconstrainedtoeitherNormalorGammadistributions,possiblylosingimportantmulti-modalinformation,
andasaresulttheycanleadtosuboptimalinferenceofthecontextvariable. Becausethecontextisvitalfortheabove
underdeterminedsourceseparationstage,wewishtolimittheamountof(variational)approximationsduringcontext
inference. Atthecostofanincreasedcomputationalcomplexity,wewillremovethevariationalapproximationaround
themixturemodelsandinsteadexpandthemixturecomponentsintodistinctmodels. Asaresult,eachdistinctmodel
nowcontainsoneofthemixturecomponentsforagivencontextandnowresultsinexactmessagesoriginatingfromthe
priorsofξ andτ . Thereforeweonlyneedtoresorttoavariationalapproximationfortheauto-regressivenode. By
k k
expandingthemixturemodelsintodistinctmodelstoreducethenumberofvariationalapproximations,calculation
of the posterior distribution of the context p(c x ) reduces to an approximate Bayesian model comparison
k
|
1:t+
problem,similarlyasdescribedin[31]. AppendixC.1givesamorein-depthdescriptiononhowweuseBayesianmodel
comparisonforsolvingtheinferencetaskin(13).
4.2 InferencefortrialdesignofHAtuningparameters
ThegoalofproposingalternativeHAtuningparametersettings(task3)istoreceivepositiveuserresponsesinthefuture.
FreeenergyminimizationoverdesiredfutureuserresponsescanbeachievedthroughaprocedurecalledExpectedFree
Energy(EFE)minimization[11,40].
EFEasatrialselectioncriterioninducesanaturaltrade-offbetweenexplorative(informationseeking)andexploitative
(rewardseeking)behaviour. InthecontextofsituatedHApersonalization, thisisdesirablebecausesolicitinguser
feedbackcanbeburdensomeandinvasive,asdescribedinSection2.2. Fromtheagent’spointofview,thismeansthat
strikingabalancebetweengatheringinformationaboutuserpreferencesandsatisfyinglearnedpreferencesisvital. The
EFEprovidesawaytotacklethistrade-off,inspiredbyneuro-scientificevidencethatbrainsoperateunderasimilar
protocol[11,41]. TheEFEisdefinedas[11]
(cid:20) (cid:21)
q(v u)
G [q]=E ln | , (14)
u q(r,v|u) p(r,v u)
|
wherethesubscriptindicatesthattheEFEisafunctionofatrialu. TheEFEcanbedecomposedinto[11]
(cid:20) (cid:21) (cid:20) (cid:21)
q(v u,r)
G [q] E lnp(r) E ln | , (15)
u ≈− q(r|u) − q(r,v|u) q(v u)
|
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Utilitydrive Informationgain
whichcontainsaninformationgaintermandautility-driventerm. MinimizationoftheEFEreducestomaximizationof
boththeseterms. Maximizationoftheutilitydrivepushestheagenttowardsmatchingpredicteduserresponsesq(r u)
|
withagoalprioroverdesireduserresponsesp(r). Thisgoalpriorallowsencodingofbeliefsaboutfutureobservations
thatwewishtoobserve. Settingthegoalpriortomatchpositiveuserresponsesthendrivestheagenttowardsparameter
settingsthatitbelievesmaketheuserhappyinthefuture. Theinformationgaintermin(15)drivesagentsthatoptimize
theEFEtoseekoutresponsesthataremaximallyinformativeaboutlatentstatesv.
Toselectthenextsetofgainsutoproposetotheuser,weneedtofind
(cid:18) (cid:19)
u∗ =argmin minG [q] . (16)
u
u q
Intuitively,onecanthinkof(16)asatwostepprocedurewithaninnerandanouterloop. Theinnerloopfindsthe
approximateposteriorqusing(approximate)Bayesianinference,conditionedonaparticularactionparameteru. The
outerloopevaluatestheresultingEFEasafunctionofuandproposesanewsetofgainstobringtheEFEdown. For
ourexperimentsweconsideracandidategridofpossiblegains. ForeachcandidatewecomputetheresultingEFEand
thenselectthelowestscoringproposalasthenextsetofgainstobepresentedtotheuser.
TheprobabilisticmodelusedforAIDAisamixtureGPC.ForsimplicitywewillrestrictinferencetotheGPcorre-
spondingtotheMAPestimateofc . Betweentrials,thecorrespondingGPneedstobeupdatedtoadapttothenewdata
k
gatheredfromtheuser. Specifically,weareinterestedinfindingtheposterioroverthelatentuserpreferencefunction
(cid:90)
p(v∗ u ,r )= p(v∗ u ,u ,v)p(v u ,r )dv. (17)
1:k 1:k−1 1:k−1 k 1:k−1 1:k−1
| | |
whereweassumeAIDAhasaccesstoadatasetconsistingofpreviousqueriesu andappraisalsr andweare
1:k−1 1:k−1
queryingthemodelatu . WhilethisinferencetaskintheGPCisintractable,thereexistanumberoftechniquesfor
k
approximateinference,suchasvariationalBayesianmethods,ExpectationPropagation,andtheLaplaceapproximation
[35]. AppendixC.2describestheexactdetailsoftheinferencerealizationoftheinferencetasksofAIDA.
10
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms
4.3 Inferenceforexecutingthehearingaidalgorithm
Themaingoaloftheproposedhearingaidalgorithmistoimproveaudibilityandintelligibilitybyre-weighinginferred
sourcesignalsintheHAoutputsignal. Inourmodeloftheobservedsignalin(2)-(7)weareinterestediniteratively
inferringthemarginaldistributionoverthelatentspeechandnoisesignalsp(s ,n x ). Thisinferencetaskisin
t t 1:t
|
literaturesometimesreferredtoasinformedsourceseparation[42]. Inferringthelatentspeechandnoisesignalstriesto
optimallydisentanglethesesignalsfromtheobservedsignalbasedonthesub-modelsofthespeechandnoisesource.This
requiresustocomputetheposteriordistributionsassociatedwiththespeechandnoisesignals. Todoso,weperform
probabilisticinferencebymeansofmessagepassingintheacousticmodelof(2)-(7). Theposteriordistributionscanbe
calculatedinanonlinemannerusingsequentialBayesianupdatingbysolvingtheChapman-Kolmogorovequation[43]
p(z ,Ψ x )
t k 1:t
|
(cid:124) (cid:123)(cid:122) (cid:125)
posterior
(cid:90) (18)
p(x z ) p(z z ,Ψ )p(z ,Ψ x )dz , fort=t−,t−+1,...,t+
t t t t−1 k t−1 k 1:t−1 t−1
∝ | | |
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
observation statedynamics prior
where z and Ψ denote the sets of dynamic states and static parameters z = θ ,s ,n and Ψ = γ,τ ,ζ ,
t k t t t t k k k
{ } { }
respectively. Here,thestatesandparameterscorrespondtothelatentARandTVARmodelsof(3)and(5). Furthermore,
weassumethatthecontextdoesnotchange,i.e. kisfixed. Whenthecontextdoeschange(18)willneedtobeextended
by integrating over the varying parameters. Unfortunately, the solution of (18) is not analytically tractable. This
happensbecauseof1)theintegrationoverlargestatespaces,2)thenon-conjugateprior-posteriorpairing,and3)the
absenceofaclosed-formsolutionfortheevidencefactor[44]. Tocircumventthisissue,weresorttoahybridmessage
passingalgorithmthatcombinesstructuredvariationalmessagepassing(SVMP)andloopybeliefpropagationforthe
minimizationofBethefreeenergy[45]. AppendixAdescribestheseconceptsinmoredetail.
For the details of the SVMP and BP algorithms, we refer the reader to Appendix A and [45, 46]. Owing to the
modularityofthefactorgraphs,themessagepassingupdaterulescanbetabulatedandonlyneedtobederivedonce
foreachoftheincludedfactornodes. Thederivationsofthesum-productupdaterulesforelementaryfactornodes
canbefoundin[36]andthederivedstructuredvariationalrulesforthecompositeARnodecanbefoundin[44]. The
variationalupdatesinthemixturemodelscanbefoundin[30,39]. Therequiredapproximatemarginaldistribution
ofsomevariablez canbecomputedbymultiplyingtheincomingandoutgoingvariationalmessagesontheedges
correspondingtothevariablesofourinterestasq(z) (cid:126)ν(z)
∝ ·
(cid:126) ν(z).
Basedontheinferredposteriordistributionsofs andn ,thesesignalscanbeusedforinferringthehearingaidoutput
t t
through(8)toproduceapersonalizedoutputwhichcompromisesbetweenresidualnoiseandspeechdistortion.
5 Experimentalverification&validation
Inthissection,wefirstverifyourapproachforthethreedesigntasksofSection2.2. Specifically,inSection5.1we
evaluatethecontextinferenceapproachbyreportingtheclassificationperformanceofcorrectlyclassifyingthecontext
correspondingtoasignalsegment. InSection5.2weevaluatetheperformanceofourintelligentagentthatactively
proposeshearingaidsettingsandlearnsuserpreferences. Theexecutionofthehearingaidalgorithmisverifiedin
Section5.3byevaluatingthesourceseparationperformance. Toconcludethissection,wepresentademonstratorfor
theentiresysteminSection5.4.
AllalgorithmshavebeenimplementedinthescientificprogramminglanguageJulia[47]. Probabilisticinferencein
ourmodelisautomatedusingtheopensourceJuliapackageReactiveMP2[48]. Alloftheexperimentspresentedin
thissectioncanbefoundatourAIDAGitHubrepository3.
5.1 Contextclassificationverification
ToverifythatthecontextisappropriatelyinferredthroughBayesianmodelselection,wegeneratedsyntheticdatafrom
thefollowinggenerativemodel:
c Cat(Tc ) (19a)
k k−1
∼
2ReactiveMP[48]isavailableathttps://github.com/biaslab/ReactiveMP.jl.
3TheAIDAGitHubrepositorywithallexperimentsisavailableathttps://github.com/biaslab/AIDA.
11
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms Podusenkoetal.
Table1: Theparametersofautoregressiveprocessesthatareusedforgeneratingatimeserieswithsimulatedcontext
dynamics.
ARorder ζ τ−1
1 -0.308 1.0
2 0.722 -0.673 2.0
3 -0.081 0.079 -0.362 0.5
4 -1.433 -0.174 0.757 0.466 1.0
AR-5
AR-4
AR-3
AR-2
AR-1
AR-0
200 210 220 230 240 250 260 270 280 290 300
frame
sledom
activecontext
selectedcontext
Figure6: Trueandinferredevolutionofcontextsfromframes200to300. Eachframeconsistsof100datapoints.
Circlesdenotetheactivecontextsthatwereusedtogeneratetheframe. Crossesdenotethemodelthatachievesthe
lowestBethefreeenergyforaspecificframe.
withpriors
c Cat(π) (20a)
0
∼
T Dir(α ), (20b)
1:L,j j
∼
wherec ischosentohavelengthL=4. Theeventprobabilitiesπandconcentrationparametersα aredefinedas
o j
π = [0.25,0.25,0.25,0.25] (cid:124) andα = [1.0,1.0,1.0,1.0] (cid:124),respectively. Wegeneratedasequenceof1000frames,
j
eachcontaining100samples,suchthatwehave100x1000datapoints. Eachframeisassociatedwithoneofthe4
differentcontexts. EachcontextcorrespondstoanARmodelwiththeparameterspresentedinTable1.
Forverificationofthecontextclassificationprocedure,wewishtoidentifywhichmodelbestapproximatestheobserved
data. Todothat,4modelswiththesamespecificationsaswereusedtogeneratethedatasetwereemployed. Weused
informativepriorsforthecoefficientsandprecisionofARmodels. Additionally,weextendedoursetofmodelswith
anAR(5)modelwithweaklyinformativepriorsandaGaussiani.i.d. modelthatcanbeviewedasanARmodelof
zeroth order, i.e. AR(0). The individual frames containing 100 samples each were processed individually and we
computedtheBethefreeenergyforeachofthedifferentmodels. TheBethefreeenergyisintroducedinAppendixA.4.
ByapproximatingthetruemodelevidenceusingtheBethefreeenergyasdescribedinAppendixC.1,weperformed
approximateBayesianmodelselectionbyselectingthemodelwiththelowestBethefreeenergy. Thismodelthen
correspondstothemostlikelycontexthatwearein. WehighlighttheobtainedinferenceresultinFigure6.
WeevaluatetheperformanceofthecontextclassificationprocedureusingapproximateBayesianmodelselectionby
computingthecategoricalaccuracymetricdefinedas
tp+tn
acc= (21)
N
12
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms
1
0.8
0.6
0.4
0.2
0
0 0.2 0.4 0.6 0.8 1
u
s
u
n
User preference function
�
1
0.8
0.6
0.4
0.2
0
Figure7: Simulateduserpreferencefunctionp(r =1 u ). Thecoloringcorrespondstotheprobabilityoftheuser
k k
givingapositiveappraisalforthesearchspaceofgains | u =[u ,u ] (cid:124).
k sk nk
wheretp,tnarethenumberoftruepositiveandtruenegativevalues,respectively. N correspondstothenumberoftotal
observations,whichinthisexperimentissettoN =1000. Inthiscontextclassificationexperiment,wehaveachieved
acategoricalaccuracyofacc=0.94.
5.2 Trialdesignverification
Evaluatingtheperformanceoftheintelligentagentisnottrivial. Becausetheagentadaptivelytradesoffexploration
andexploitation,accuracyisnotanadequatemetric. Therearereasonsfortheagenttoveerawayfromwhatitbelieves
istheoptimumtoobtainmoreinformation. Asaverificationexperimentwecaninvestigatehowtheagentinteractswith
asimulateduser. Oursimulatedusersamplesbinaryappraisalsr basedontheHAparametersu as
k k
(cid:18) (cid:19)
2
r k ∼ Ber 1+exp (cid:0) (u u∗)TΛ (u u∗) (cid:1) , (22)
k user k
− −
whereu∗ denotestheoptimalparametersetting,u isthesetofparametersproposedbyAIDAattimek,Λ isa
k user
diagonal weighing matrix that controls how quickly the probability of positive appraisals decays with the squared
distancetou∗. Theconstant2ensuresthatwhenu =u∗,theprobabilityofpositiveappraisalsis1insteadof0.5. For
k
ourexperiments,wesetu∗ =[0.8,0.2] (cid:124)andthediagonalelementsofΛ to0.004. Thisresultsintheuserpreference
user
functionp(r =1 u )asshowninFigure7. ThekernelusedforAIDAisasquaredexponentialkernel,givenby
k k
|
(cid:26) u u(cid:48) 2(cid:27)
K(u,u(cid:48))=σ2exp (cid:107) − (cid:107)2 , (23)
− 2l2
where l and σ are the hyperparameters of this kernel. Intuitively, σ is a static noise parameter and l encodes the
smoothnessofthekernelfunction. Bothhyperparameterswereinitializedtoσ =l=0.5,whichisuninformativeonthe
scaleoftheexperiment. Welettheagentsearchfor80trialsandupdatehyperparametersevery5thtrialusingconjugate
gradientdescentasimplementedinOptim.jl[49]. Weconstrainbothhyperparameterstothedomain[0.1,1]toensure
stabilityoftheoptimization. Aswewillsee,forlargepartsofeachexperimentAIDAonlyreceivesnegativeappraisals.
The generative model of AIDA is fundamentally a classifier and unconstrained optimization can therefore lead to
degenerateresultswhenthedatasetonlycontainsexamplesofasingleclass. Forallexperiments,thefirstproposal
ofAIDAwasarandomlysampledparameterfromtheadmissiblesetofparameters,becausetheAIDAhasnoprior
knowledgeabouttheuserpreferencefunction. Thisrandominitialproposal,leadtodistinctbehaviourforallsimulated
agents.
WeprovidetwoverificationexperimentsforAIDA.First,wewillthoroughlyexamineasingleruninordertoinvestigate
howAIDAswitchesbetweenexploratoryandexploitativebehaviour. Secondly,weexaminetheaggregateperformance
ofanensembleofagentstotesttheaverageperformance. Toassesstheperformanceforasinglerun,wecanexamine
13
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms Podusenkoetal.
theevolutionofthedistincttermsintheEFEdecompositionof(15)overtime. WeexpectthatwhenAIDAisprimarily
exploring,theutilitydriveisrelativelylowwhiletheinformationgainisrelativelyhigh. WhenAIDAisprimarily
engagedinexploitation,weexpecttheoppositepattern. WeshowthesetermsseparatelyinFigure8.
0
5 −
10
−
15
−
20
−
25
−
30
−
0 10 20 30 40 50 60 70 80
k
eulav
0 Informationgain
0.2 −
0.4
−
0.6
−
0.8
−
Utilitydrive 1
−
0 10 20 30 40 50 60 70 80
k
Figure8: Evolutionoftheutilitydriveandnegativeinformationgainafterthroughoutasingleexperiment.
Figure8showsthattherearedistinctphasestotheexperiment. Inthebeginning(k <5)AIDAseesasharpdecrease
inutilitydriveandinformationgainterms. Thisindicatesasaturationofthesearchspacesuchthatnopointspresent
good options. This happens early due to uninformative hyperparameter settings in the GPC. After trial 5, these
hyperparametersareoptimizedandtheagentnolongerthinksithassaturatedthesearchspace,whichcanbeexplained
bythejumpsinFigure8fromtrial5to6. Fromtrial6throughout15weobservearelativelyhighinformationgainand
relativelylowutilitydrive,meaningthattheagentisstillexploringthesearchspaceforparametersettingswhichyield
apositiveuserappraisal. Theagentobtainsitsfirstpositiveappraisalatk =16,asdenotedbythejumpinutilitydrive
anddropininformationgain. Thisfirstpositiveappraisalisfollowedbyaperiodofoscillationsinbothterms,where
theagentisrefiningitsparameters. FinallyAIDAsettlesdowntopredominantlyexploitativebehaviourstartingfrom
41st trial. Toexaminethefirsttransition,wecanvisualizetheEFElandscapeatk = 5andk = 6,theupperrowof
Figure9.
RecallthatAIDAisminimizingEFE.Therefore,itislookingforthelowestvaluescorrespondingtoblueregionsand
avoidingthehighvaluescorrespondingtoredregions. Betweenk =5andk =6weperformthefirsthyperparameter
update,whichdrasticallychangestheEFElandscape. Thisindicatesthatinitialparametersettingswerenotinformative,
aswedidnotcoverthemajorityofthesearchspacewithinthefirst5iterations. Theyellowregionsatk =6indicates
regionscorrespondingtopreviousproposalsofAIDAthatresultedintonegativeappraisals. Wecanvisualizesnapshots
of the exploration phase starting from k = 6 in a similar manner. The second row of Figure 9 displays the EFE
landscapeattwodifferenttimeinstancesduringtheexplorationphase. Itshowsthatoverthecourseoftheexperiment,
AIDAgraduallybuildsarepresentationoverthesearchspace. Intrial16thistakestheformofpatternsofconnected
regionsthatdenoteareasthatAIDAbelievesareunlikelytoresultsinpositiveappraisals.
OnceAIDAreceivesitsfirstpositiveappraisalatk =16,itswitchesfromexploringthesearchspacetofocusingonly
onthelocalregion. IfweexamineFigure8,weseethatatthistimetheinformationgaintermisstillreasonablyhigh.
Thisindicatesasubtlepoint: onceAIDAreceivesapositiveappraisal,itstartswithlocalexplorationaroundwhere
theoptimummightbelocated. However,theagentwaslocatedneartheboundaryoftheoptimumandnextreceivesa
negativeappraisal. Thereforeintrials18to22AIDAqueriespointswhichitdeemsmostinformative. Attime23the
positionofAIDAinthesearchspace(blackdotinthethirdrowofFigure9)returnstotheedgeoftheuserpreference
functioninFigure7. ThiscausesAIDAtoreceiveamixtureofpositiveandnegativeappraisalsinthefollowingtrials,
leadingtotheoscillationsseeninFigure8. Finally,wecanexaminethelandscapeafterAIDAhasconfidentlylocated
theoptimumandswitchedtopurelyexploitativebehavior. Thishappensatk =42wheretheutilitydrivegoesto0and
theinformationgainconcentratesaround 1.
−
ThelastrowofFigure9showsthatonceu∗isconfidentlylocated,AIDAdisregardstheremainderofthesearchspace
infavourofprovidinggoodparametersettings. Finally,iftheusercontinuestosupplydatatoAIDA,itwillgradually
extendthepotentialregionofsamplesaroundtheoptimum. Thisindicatesthatifauserkeepsrequestingupdated
parameters,AIDAwillonceagainperformlocalexplorationaroundtheoptimum. ThisfurtherindicatesthatAIDA
accommodatesgradualretrainingasuser’shearinglossprofilechangesovertime.
HavingthoroughlyexaminedanexamplerunandinvestigatedthetypesofbehaviorproducedbyAIDA,wecannow
turnourattentiontoaggregateperformanceoveranensembleofagents. Tothatendwerepeattheexperiment80times
withidenticalhyperparameters,butwithdifferentinitialproposals. Themetricwearemostinterestedinishowquickly
AIDAisabletolocatetheoptimumandproduceapositiveappraisal.
14
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms
1
0.8
0.6
0.4
0.2
0
0 0.2 0.4 0.6 0.8 1
us
nu
EFElandscapeattime5
EFE
37 1
Target
Current
36 0.8
35
0.6
34
0.4
33
0.2
32
0
0 0.2 0.4 0.6 0.8 1
us
nu
EFElandscapeattime6
EFE
Target
35
Current
30
25
20
1
0.8
0.6
0.4
0.2
0
0 0.2 0.4 0.6 0.8 1
us
nu
EFElandscapeattime15
EFE
1
Target
35
Current
0.8
30 0.6
0.4
25
0.2
20
0
0 0.2 0.4 0.6 0.8 1
us
nu
EFElandscapeattime16
EFE
Target 35
Current
30
25
20
15
10
1
0.8
0.6
0.4
0.2
0
0 0.2 0.4 0.6 0.8 1
us
nu
EFElandscapeattime22
EFE
1
Target
35
Current
0.8
30 0.6
0.4
25
0.2
20
0
0 0.2 0.4 0.6 0.8 1
us
nu
EFElandscapeattime23
EFE
Target 35
Current
30
25
20
15
10
1
0.8
0.6
0.4
0.2
0
0 0.2 0.4 0.6 0.8 1
us
nu
EFElandscapeattime42
EFE
1
Target
Current
30 0.8
0.6
20
0.4
10
0.2
0
0 0.2 0.4 0.6 0.8 1
us
nu
EFElandscapeattime80
EFE
Target
Current
30
20
10
Figure9: SnapshotofEFElandscapeatdifferenttimepointsasafunctionofgainsu andu . Theblackdotdenotes
s n
thecurrentparametersettingsandthegreendotdenotesu∗.
Figure10showsaheatmapofwheneachagentobtainspositiveresponses. Positiveresponsesareindicatedbyyellow
squaresandnegativeresponsesbyblacksquares. EachrowcontainsresultsforasingleAIDA-agentandeachcolumn
15
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms Podusenkoetal.
80
60
40
20
20 40 60 80
TimeIndex
rebmuntnegA
PositiveAppraisals
14
12
10
7
5
3
1
10 27.5 45 62.5 80
TimeIndex
sesnopserevitsoPforebmuN
Ensemble Performance
Figure10:(Left)Heatmapshowingensembleperformanceover80agents. Positiveandnegativeresponsesareindicated
withyellowandblacksquares,respectively. (Right)Histogramshowingtimeindiceswheretheagentsreceivetheir
firstpositiveresponse. Therightmostcolumnindicatesagentsthatfailedtoobtainapositiveappraisal. Intotal,66/80
agentssolvethetask,correspondingtoasuccessrateof82.5%.
indicatesatimestepoftheexperiment. Consistentwiththeresultsforasingleagent,weseethateachexperimentstarts
withaperiodofexploration. Alargenumberofrowsalsoshowayellowsquarewithinthefirst35trials,indicatingthat
theoptimumwasfound. Interestingly,noagentsreceiveonlypositiveresponses,evenafterlocatingtheoptimum. This
followsfromAIDAactivelytradingoffexplorationandexploitation. Whenexploring,AIDAcanselectparameters
thataresuboptimalwithrespecttoelicitingpositiveuserresponses,togathermoreinformation. Figure10alsoshows
ahistogramindicatingwheneachagentobtainsitsfirstpositiveappraisal. Theveryrightcolumnshowsagentsthat
failedtolocatetheoptimumwithinthedesignatednumberoftrials. Intotal, 66/80agentscorrectlysolvethetask,
correspondingtoasuccessrateof82.5%. Disregardingunsuccessfulruns,onaverage,AIDAobtainsapositiveresponse
in37.8trialswithamedianof29.5trials.
5.3 Hearingaidalgorithmexecutionverification
Toverifytheproposedinferencemethodologyforthehearingaidalgorithmexecution,wesynthesizeddatabysampling
fromthefollowinggenerativemodel:
θ (θ , ωI ) (24a)
t t−1 M
∼N
s (A(θ )s , V (γ)) (24b)
t t t−1
∼N
n (A(ζ)n , V (τ)) (24c)
t t−1
∼N
x =s +n , (24d)
t t t
withpriors
θ (0,ωI ) (25a)
0 M
∼N
ζ (0,I ) (25b)
N
∼N
γ Γ(1.0,1e 4) (25c)
∼ −
τ Γ(1.0,1.0) (25d)
∼
ω =1e 4 (25e)
−
whereM andN aretheordersofTVARandARmodels,respectively,andwhereM N holds,asweassumethatthe
≥
noisesignalcanbemodeledbyalowerARorderincomparisontothespeechsignal. Weuseanuninformativepriorfor
theoutputofthehearingaidy asinFigure3topreventinteractionsfromthatpartofthegraph. Wegenerated1000
t
distincttimeseriesoflength100. Foreachgeneratedtimeseries,the(TV)ARordersM andN weresampledfromthe
discretedomains[4,8]and[1,4],respectively. WeresampledthepriorsthatinitiallyresultedintounstableTVARand
ARprocesses.
16
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms
Thegeneratedtimeserieswereusedinthefollowingexperiment. Wefirstcreatedaprobabilisticmodelwiththesame
specificationsasthegenerativemodelin(24). However,weusednon-informativepriorsforthestatesandparameters
ofthemodelthatcorrespondstotheTVARprocessin(24b). Toensuretheidentifiabilityoftheseparatedsources,we
usedweaklyinformativepriorsfortheparametersoftheARprocessin(24c). Specifically,themeanofthepriorforζ
wascenteredaroundtherealARcoefficientsthatwereusedinthedatagenerationprocess. Thegoalsoftheexperiment
are1)toverifythattheproposedinferenceprocedurerecoversthehiddenstatesθ ,s andn foreachgenerateddataset
t t t
and2)toverifyconvergenceoftheBFEasconvergenceisnotguaranteed,becauseourgraphcontainsloops[50]. Fora
typicalcase,theinferenceresultsforthehiddenstatess andn areshowninthetoprowofFigure11. Thebottomrow
t t
5
0
5 −
0 20 40 60 80 100
t
eulav
Dataset999 Dataset999 Dataset999
6
4
4
2 2
0
2 0
−
4
observations − 2 TVAR(5) TVAR(5) − AR(4)
AR(4) − 6 inferred inferred
0 20 40 60 80 100 0 20 40 60 80 100
t t
0.5
0
−0.5
0 20 40 60 80 100
t
eulav
Coefficient1
1
0.8
0.6
0.4
0.2
hidden 0
inferred
0 20 40 60 80 100
t
eulav
Coefficient2
0
−0.2
−0.4
−0.6
hidden −0.8
inferred
0 20 40 60 80 100
t
eulav
Coefficient3
0
−0.2
−0.4
−0.6
hidden −0.8
inferred
0 20 40 60 80 100
t
eulav
Coefficient4
0.6
0.4
0.2
0
hidden −0.2
inferred
0 20 40 60 80 100
t
eulav
Coefficient5
hidden
inferred
h
Figure11: (Top)Inferenceresultsforthehiddenstatess andn ofcoupled(TV)ARprocessondataset999. (left)
t t
Thegeneratedobservedsignalx withunderlyinggeneratedsignalss andn . (center)Thelatentsignals andits
t t t t
correspondingposteriorapproximation. (right)Thelatentsignaln anditscorrespondingposteriorapproximation. The
t
dashedlinescorrespondstothemeanoftheposteriorestimates. Thetransparentregionsrepresentthecorresponding
remaining uncertainty as plus-minus one standard deviation from the mean. (Bottom) Inference results for the
coefficientsθ ofdataset999. ThesolidlinescorrespondtothetruelatentARcoefficients. Thedashedlinescorrespond
t
tothemeanoftheposteriorestimatesofthecoefficientsandthetransparentregionscorrespondtoplus-minusone
standarddeviationfromthemeanoftheestimatedcoefficients.
ofFigure11showsthetrackingofthetime-varyingcoefficientsθ . Thisplotdoesnotshowthecorrelationbetween
t
theinferredcoefficients,whereasthisactuallycontainsvitalinformationformodelinganacousticsignal. Namely,the
coefficientstogetherspecifyasetofpoles,whichinfluencethecharacteristicsofthefrequencyspectrumofthesignal.
AninterestingexampleisdepictedinFigure12. Wecanseethattheinferenceresultsforthelatentstatess andn are
t t
swappedwithrespecttothetrueunderlyingsignals. Thisbehaviourisundesirableinstandardalgorithmswhenthe
outputoftheHAisproducedbasedonhard-codedgains. However,thepresenceofourintelligentagentcanstillfind
theoptimalgainsforthissituation. Theautomationofthehearingaidalgorithmandintelligentagentwillrelievethis
burdenonHAclients. AscanbeseenfromFigure13,theBethefreeenergyaveragedoverallgeneratedtimeseries
monotonicallydecreases. Notethateventhoughtheproposedhybridmessagepassingalgorithmresultsinastationary
solution,itdoesnotprovideconvergenceguarantees.
5.4 Validationexperiments
ForthevalidationoftheproposedHAalgorithmandAIDA,wecreatedaninteractivewebapplication4 todemonstrate
thethejointsystem. Figure14showstheinterfaceofthedemonstrator.
Theuserlistenstotheoutputofthehearingaidalgorithmbypressingthe"output"button. Thebuttons"speech"and
"noise"correspondtothebeliefsofAIDAabouttheconstituentsignalsoftheHAinput. Notethatinrealitytheuser
doesnothaveaccesstothisinformationandcanonlylistentoHAoutput. Afterlisteningtotheoutputsignal,theuser
4AwebapplicationofAIDAisavailableathttps://github.com/biaslab/AIDA-app/.
17
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms Podusenkoetal.
6
4
2
0
2
−
4
−
6
−
0 20 40 60 80 100
t
eulav
Dataset42 Dataset42 Dataset42
6
4 5
2
0 0
2
−
observations
TVAR(6) 4 TVAR(6) 5 AR(3)
AR(3) − inferred − inferred
0 20 40 60 80 100 0 20 40 60 80 100
t t
Figure12: Inferenceresultsforthehiddenstatess andn ofcoupled(TV)ARprocessondataset42. Inthisparticular
t t
case it can be noted that the inferred states are swapped with respect to the true underlying signals. However, the
accompanyingintelligentagentisabletocopewiththesekindsofsituations,suchthattheHAclientsdonotexperience
anyproblemsasaresult.
370
360
350
340
330
320
20 0 20 40 60 80 100 120 140 160 180 200 220 240 260 280 300 320 340
−
iteration
]stan[ygreneeerfehteB
BFE
Figure13: EvolutionoftheBethefreeenergyforthecoupledautoregressivemodelaveragedoverallgeneratedtime
series. Theiterationindexspecifiesthenumberofmarginalupdatesforalledgesinthegraph.
isinvitedtoassesstheperformanceofthecurrentHAsetting. Theusercansendpositiveandnegativeappraisalsby
pressingthethumbuporthumbdownbuttonsrespectively. Oncetheappraisalissent,AIDAupdatesitsbeliefsabout
theparameters’spaceandprovidesnewsettingsfortheHAalgorithmtomaketheuserhappy. AsAIDAmodelsuser
appraisalsusingaGPC,weprovideanadditionalbuttonthatforcesAIDAtooptimizetheparametersofGPC.This
couldbeusefulwhenAIDAhasalreadycollectedsomefeedbackfromtheuserthatcontainsbothpositiveandnegative
appraisals.
Thedemonstratorworksintwoenvironments: syntheticandreal. Thesyntheticenvironmentallowstheusertolisten
to a spoken sentence with two artificial noise sources, i.e. either interference from a sinusoidal wave or a drilling
machine. In the synthetic environment the hearing aid algorithm exploits the knowledge about acoustic contexts,
i.e,itusesinformativepriorsfortheARmodelthatcorrespondstonoise. Therealenvironmentusesthedatafrom
NOIZEUSspeechcorpus5. Inparticular,therealenvironmentconsistsof30sentencespronouncedintwodifferent
noiseenvironments. Heretheuseriseitherexperiencingsurroundingnoiseatatrainstationorbabblenoise. Inthe
realenvironment,theHAalgorithmusesweaklyinformativepriorsforthebackgroundnoisewhichinfluencesthe
performanceoftheHAalgorithm. BoththeHAalgorithmandAIDAdeterminetheacousticcontextbasedonthe
Bethefreeenergyscore,whichisalsoshowninthedemonstrator. ThecontextwiththelowerBethefreeenergyscore
correspondstotheselectedacousticcontext.
5TheNOIZEUSdatabaseisavailableathttps://ecs.utdallas.edu/loizou/speech/noizeus/.
18
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms
Active Inference Design Agent
Environment
SYNTHETIC REAL RESET
Hearing Aid
input
0.3 speech
noise
0.2 output
mplitude0.
0
1
a
−0.1
−0.2
−0.30 5k 10k 15k 20k
NEXT
INPUT SPEECH NOISE OUTPUT
EFE Agent Classifier
2 −160 Babble
25 Train
−170
1.5 24
noise
gain
1
2
2
2
3
BFE
[nats]
−
−
1
1
9
8
0
0
−200
0.5 21
−210
20
0
0 0.5 1 1.5 2 0 1 2 3 4
speech gain
Figure14: ScreenshotoftheinteractivewebapplicationofAIDA.Thedashboardconsistsoffourdistinctcells. Thetop
cellEnvironmentallowstheusertochangetheinterferingnoisesignalfromageneratednoisesignal(synthetic)toa
realnoisesignal. Furthermoreitcontainsaresetbuttonforresettingtheapplication. TheHearingAidcellprovidesan
interactiveplotoftheinput,separatedspeech,separatednoise,andgeneratedoutputwaveformsignals. Eachwaveform
canbeplayedwhenthecorrespondingbuttonispressed. TheNEXTbuttonloadsanewaudiofileforevaluation. The
thumbs-upandthumbs-downbuttonscorrespondtoprovidingAIDAwithpositiveandnegativeappraisals,respectively.
ThebrainbuttonstartsoptimizationoftheparametersofGPC.TheEFEAgentcellreflectstheagent’sbeliefsabout
optimalparametersfortheuserasanEFEheatmap. TheClassifiercellshowstheBethefreeenergy(BFE)scorefor
thedifferentmodels,correspondingtothedifferentcontexts. Fortherealnoisesignal,thealgorithmautomatically
determineswhetherwearesurroundedbybabblenoise,orbynoisefromatrainstation.
19
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms Podusenkoetal.
6 Discussion
Wehaveintroducedadesignagentthatiscapableoftuningthecontext-dependentparametersofahearingaidalgorithm
byincorporatinguserfeedback. Throughoutthepaper,wehavemadeseveraldesignchoiceswhoseimplicationswe
shortlyreviewinthissection.
TheaudiomodelintroducedinSection3.1describesthedynamicsofthespeechsignalperturbedbycolorednoise.
Despitethefactthattheproposedinferencealgorithmallowsforthedecompositionofsuchsignalsintospeechand
noisecomponents,thereareafewlimitationsthatmustbehighlighted. First,theidentifiabilityofthecoupledAR
modeldependsontheselectedpriors. Non-informativepriorscanleadtopoorsourceestimation[51,52]. Totacklethe
identifiabilityissue,weuseinformativecontext-dependentpriors. Inotherwords,foreachcontext,weuseadifferentset
ofpriorsthatbetterdescribethedynamicsoftheacousticsignalinthatcontext. Secondly,throughoutourexperiments
weusedfixedordersofTVARandARmodels. Inreality,wedonothavepriorinformationabouttheactualorderof
theunderlyingsignals. Therefore,tocontinuouslyupdateourmodelsoftheunderlyingsourcesweneedtoperform
activeorderselection,whichcanberealizedusingBayesianmodelreduction[53,54]. Thirdly,ourmodelassumes
thatthehearingaiddeviceonlyhasaccesstoamonauralinput,whichmeansthattheobservedsignaloriginatesfrom
singlemicrophone. Asaresultwedonotuseanyspatialinformationaboutanacousticsignalthatcouldhavebeen
obtainedusingmultiplemicrophones. Thisassumptionismostlyinfluencedbyourdesiretofocusontheconceptof
designinganovelclassofhearingaidalgorithmsratherthanbuildingreal-worldHAengine. Fortunately,theproposed
frameworkallowsfortheeasysubstitutionofsourcemodelswithmoreversatilemodelsthatmightbebettersuitedfor
speech. Forinstance,onecanuseseveralmicrophones,ascommonlydoneinbeamforming[55],oruseafrequency
decompositionforimprovingthesourceseparationperformance[56,57,58]. Inevitably,amorecomplexmodelwill
alsolikelyresultinahighercomputationalburden. Hence,theimplementationofthisalgorithmonanembeddeddevice
remainsachallenge.
Thepoweroftheagentcomesfromthechoiceoftheobjectivefunction. Sincetheobjectiveisindependentofthe
generativemodel,astraightforwardapproachtoimprovingtheagentistoadaptthegenerativemodel. Inparticular,
aGPCisanonparametricmodelwithveryfewassumptionsontheunderlyingfunction. Placingconstraintsonthe
preferencefunction,suchaswasdonein[59,60],islikelytoimprovedataefficiencyoftheagent. Arguablyacore
moveof[59,60]istoacknowledgethatuserpreferencesarelikelytobepeakedaroundoneorafewoptima. Even
ifthetruepreferencefunctionhasmultiplemodes,assumingasinglepeakfortheagentissafesinceitonlyneedsto
locateoneofthemodestoprovidegoodparametersettings. Makingthisassumptionallowstheauthorstoworkwitha
parametricmodeloveruserpreferences. Workingwithalessflexiblemodelpredictablyleadstohigherdataefficiency,
whichcanaidperformanceoftheagent. GiventhatthetargetdemographicforAIDAconsistsofHAusers,itisof
paramountimportancethattheagentisabletolearnanadequaterepresentationofuserpreferencesinasfewtrialsas
possibletoavoidinconveniencingtheuser.
DuringmodelspecificationinSection3.2,wemakesomeassumptionsonthecontrolvariableu anduserappraisalsr .
k k
First,wesetthedomainoftheelementsofcontrolvariableu to[0,1]. Notethatthisisanarbitraryconstraintwhich
k
weuseforillustrativepurposes. Thedomaincanbeeasilyrescaledwithoutlossofgenerality. Forexample,inour
demonstrator,weusethedefaultdomainofu [0,2]2.Secondly,weoptforbinaryuserappraisals,i.e.r ∅,0,1 .
k k
∈ ∈{ }
ThisdesignchoicefollowsfromtherequirementofallowinguserstocommunicatecovertlytoAIDA.Binaryuser
appraisalcanmoreeasilybelinkedtoforexamplecovertwristmovementswhenwearingasmartwatchtoupdatethe
controlvariables. Withcontinuoususerappraisals,e.g. r [0,1],orpairwisecomparisonteststheconvergenceof
k
∈
AIDAcanbegreatlyimprovedastheseappraisalsyieldmoreinformationperappraisal. However,providingAIDAwith
theseappraisalsrequiresmoreattention,whichisundesirableincertaincircumstances,forexampleduringbusiness
meetings.
Real-worldtestingofAIDAhasnotbeenincludedinourworkasperformanceevaluationwithhumanHAclientsis
notstraightforward. TheperformanceofAIDAshouldbeevaluatedbymeansofarandomizedcontrolledtrial(RCT)
whereHAclientsshouldberandomlyassignedtoeitheranexperimentalgrouporacontrolgroup. Unfortunately,our
implementationiscurrentlynotabletoachievereal-timeperformanceandhencecannotbetestedintheproperRCT
setting. Nonetheless,weprovideademothatsimulatesAIDAandcanbetestedfreely.
7 Relatedwork
The problem of hearing aid personalization has been explored in various works. In [3] the HA parameters are
tunedaccordingtoapairwiseuserassessmenttests,duringwhichtheuser’sperceptionisencodedusingGaussian
processes. Theintractableposteriordistributioncorrespondingtotheuser’sperceptionisthencomputedusingaLaplace
approximationwithExpectedImprovementastheacquisitionfunctionusedtoselectthenextsetofgains. Ouragent
20
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms
improvesupon[3]intwoconcreteways. Firstly,AIDAplacesalowercognitiveloadontheuserbynotrequiring
pairwisecomparisons. ThismeanstheuserdoesnotneedtokeepinhermemorywhattheHAsoundedlikeatthe
previoustrialbutonlyneedstoconsiderthecurrentHAoutput. AIDAaccomplishesthiswithoutrequiringmoretrials
fortraining. Infact,sinceAIDAdoesnotrequirepre-trainingbutcanbetrainedfullyonlineunderin-situconditions,
AIDArequireslessdatatolocateoptimalgains. Secondly,AIDAcanbetrainedandretrainedinacontinuallearning
fashion. Incasetheuserspreferenceschangeovertime,forinstancebyachangeinthehearinglossprofile,AIDAcan
smoothlyaccommodatetheuseraslongasshecontinuestoprovidetheagentwithfeedback. UsingEFEasacquisition
functionmeanstheagentwillengageinlocalexplorationoncetheoptimumislocated,leadingtheagenttonaturally
learnshiftsintheuserspreferencesbybalancingexplorationandexploitation. In[4],personalizationofthehearing
aidcompressionalgorithmisframedintermsofdeepreinforcementlearning. Onthecontrary,inourworkwetake
inspirationfromtheactiveinferenceframeworkwhereagentsacttomaximizemodelevidenceoftheirunderlying
generativemodel. Importantly,thisdoesnotrequireustoexplicitlyspecifyalossfunctionthatdrivesexploitativeand
epistemicbehaviour. Intherecentworkof[59],thehearingaidpreferencelearningalgorithmisimplementedthrough
sequentialBayesianoptimizationwithpairwisecomparisons. Theirhearingaidsystemcomprisestwosubsystems
representingauserwiththeirpreferencesandtheagentthatguidesthelearningprocess. However,[59]focusonlyon
explorationthroughmaximisinginformationgainwithaparametricmodel. TheEFEadditionallyaddsagoaldirected
termthatensurestheagentwillstayneartheoptimumoncelocated,evenifotherparametersettingsprovidemore
information. Extendingthemodelof[59]toemploythefullEFEisanexcitingpotentialdirectionforfuturework.
Finallyneither[3]nor[59]takescontextdependenceintoaccount.
[61]introducesActiveListening(AL),whichperformsspeechrecognitionbasedontheprinciplesofactiveinference.
In[61],theyregardlisteningasanactiveprocessthatislargelyinfluencedbylexical,speakerandprosodicinformation.
[61]distinguishesitselffromconventionalaudioprocessingalgorithms,becauseitexplicitlyincludestheprocessof
wordboundaryselectionbeforewordclassificationandrecognition,andthattheyregardthisasanactiveprocess. Word
boundariesareselectedfromagroupofcandidatewordboundaries,basedonBayesianmodelselection,bychoosing
thewordboundarythatoptimizestheVFEduringclassification. Inthefuture,weseethepotentialofincorporating
theALapproachintoAIDA.Activeinferenceissuccessfullyappliedinthework[62]thatstudiestomodelselective
attentioninacocktailpartylisteningsetup.
TheaudioprocessingcomponentsofAIDAessentiallyperforminformedsourceseparation[42],wheresourcesare
separatedbasedonpriorknowledge. Eventhoughblindsourceseparationapproaches[63,64]alwaysusesomedegree
ofpriorinformation,wedonotfocusonthisdirectionandinsteadweactivelytrytomodeltheunderlyingsources
basedonvariationsofauto-regressiveprocesses. Foraudioprocessingapplicationssourceseparationhasoftenbeen
performedinthelog-powerdomain[56,57,58]. However,theinteractionofthesignalsinthisdomainisnolonger
linear. Theintractabilitythatresultsfromperformingexactinferenceinthismodelisoftenresolvedbysimplifyingthe
interactionfunction[65,66]. Althoughthisapproachhasshowntobesuccessfulinthepast,itsperformanceislimited
becauseofthenegligenceofphaseinformation.
8 Conclusions
ThispaperhaspresentedAIDA,anactiveinferencedesignagentfornovelsituation-awarepersonalizedhearingaid
algorithms. AIDAandthecorrespondinghearingaidalgorithmarebasedonprobabilisticgenerativemodelsthatmodel
theuserandtheunderlyingspeechandcontext-dependentbackgroundnoisesignalsoftheobservedacousticsignal,
respectively. Throughprobabilisticinferencebymeansofmessagepassing,weperforminformedsourceseparationin
thismodelandusetheseparatedsignalstoperformsource-specificfiltering. AIDAthenlearnspersonalizedsource-
specific gains through user interaction, depending on the environment that the user is in. Users can give a binary
appraisal after which the agent will make an improved proposal, based on expected free energy minimization for
encouragingbothexploitativeandepistemicbehaviour. AIDA’soperationsarecontext-dependentandusesthecontext
fromthehearingaidalgorithm,whichisbasedonBayesianmodelselection. Experimentalresultsshowthathybrid
messagepassingiscapableoffindingthehiddenstatesofthecoupledARmodelthatareassociatedwiththespeech
andnoisecomponents. Moreover,Bayesianmodelselectionhasbeentestedforthecontextinferenceproblemwhere
eachsourceismodelledbyARprocess. Theexperimentsonpreferencelearningshowedthepotentialofapplying
expectedfreeenergyminizationforfindingtheoptimalsettingsofthehearingaidalgorithm. Althoughreal-world
implementationsstillpresentchallenges,thisnovelclassofaudioprocessingalgorithmshasthepotentialtochange
theleadingapproachtohearingaidalgorithmdesign. FutureplansencompassdevelopingAIDAtowardsreal-time
applications.
21
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms Podusenkoetal.
Acknowledgments
ThisworkwaspartlyfinancedbyGNAdvancedScience,whichistheresearchdepartmentofGNHearingA/S,andby
researchprogramsZEROandEDLwithprojectnumbersP15-06andP16-25,respectively,whichare(partly)financed
bytheNetherlandsOrganisationforScientificResearch(NWO).TheauthorswouldalsoliketothanktheBIASlab
teammembersforinsightfuldiscussionsonvarioustopicsrelatedtothiswork.
References
[1] JamesKatesandKathrynArehart. MultichannelDynamic-RangeCompressionUsingDigitalFrequencyWarping.
EURASIPJournalonAppliedSignalProcessing,18:3003–3014,2005.
[2] Thijs van de Laar and Bert de Vries. A Probabilistic Modeling Approach to Hearing Loss Compensation.
IEEE/ACMTransactionsonAudio,Speech,andLanguageProcessing,24(11):2200–2213,November2016.
[3] J.B.B.Nielsen, J.Nielsen, andJ.Larsen. Perception-BasedPersonalizationofHearingAidsUsingGaussian
ProcessesandActiveLearning. IEEE/ACMTransactionsonAudio,Speech,andLanguageProcessing,23(1):162–
173,January2015.
[4] NasimAlamdari,EdwardLobarinas,andNasserKehtarnavaz. PersonalizationofHearingAidCompressionby
Human-in-the-LoopDeepReinforcementLearning. IEEEAccess,8:203503–203515,2020.
[5] C. Karadagur Ananda Reddy, N. Shankar, G. Shreedhar Bhat, R. Charan, and I. Panahi. An Individualized
Super-GaussianSingleMicrophoneSpeechEnhancementforHearingAidUsersWithSmartphoneasanAssistive
Device. IEEESignalProcessingLetters,24(11):1601–1605,November2017.
[6] KarlFriston,JamesKilner,andLeeHarrison. Afreeenergyprincipleforthebrain. JournalofPhysiology,Paris,
100(1-3):70–87,September2006.
[7] ThijsvandeLaarandBertdeVries. SimulatingActiveInferenceProcessesbyMessagePassing. Frontiersin
RoboticsandAI,6:20,2019.
[8] ThijsvandeLaar,AyçaÖzçelikkale,andHenkWymeersch.ApplicationoftheFreeEnergyPrincipletoEstimation
andControl. arXivpreprintarXiv:1910.09823,2019.
[9] BerenMillidge. DeepActiveInferenceasVariationalPolicyGradients. arXiv:1907.03876[cs],July2019. arXiv:
1907.03876.
[10] AlexanderTschantz,ManuelBaltieri,AnilK.Seth,andChristopherL.Buckley. Scalingactiveinference. In2020
InternationalJointConferenceonNeuralNetworks(IJCNN),pages1–8.IEEE,2020.
[11] KarlFriston,FrancescoRigoli,DimitriOgnibene,ChristophMathys,ThomasFitzgerald,andGiovanniPezzulo.
Activeinferenceandepistemicvalue. CognitiveNeuroscience,6(4):187–214,March2015.
[12] Lancelot DaCosta, Thomas Parr, Noor Sajid, Sebastijan Veselic, Victorita Neacsu, and Karl Friston. Active
inferenceondiscretestate-spaces: asynthesis. arXiv:2001.07203[q-bio],January2020. arXiv: 2001.07203.
[13] KarlFriston,LancelotDaCosta,DanijarHafner,CasperHesp,andThomasParr. SophisticatedInference. Neural
Computation,33(3):713–763,March2021.
[14] AntonyW.Rix,JohnG.Beerends,MichaelP.Hollier,andAndriesP.Hekstra. Perceptualevaluationofspeech
quality(PESQ)-anewmethodforspeechqualityassessmentoftelephonenetworksandcodecs. In2001IEEE
InternationalConferenceonAcoustics, Speech, andSignalProcessing, 2001.Proceedings., volume2, pages
749–752.IEEE,2001.
[15] JamesM.KatesandKathrynH.Arehart. Thehearing-aidspeechqualityindex(HASQI). JournaloftheAudio
EngineeringSociety,58(5):363–381,2010.
[16] Cees H. Taal, Richard C. Hendriks, Richard Heusdens, and Jesper Jensen. An Algorithm for Intelligibility
PredictionofTime–FrequencyWeightedNoisySpeech. IEEETransactionsonAudio,Speech,andLanguage
Processing,19(7):2125–2136,September2011.
[17] JohnG.Beerends,ChristianSchmidmer,JensBerger,MatthiasObermann,RaphaelUllmann,JoachimPomy,
andMichaelKeyhl. PerceptualObjectiveListeningQualityAssessment(POLQA),TheThirdGenerationITU-T
Standard for End-to-End Speech Quality Measurement Part I—Temporal Alignment. Journal of the Audio
EngineeringSociety,61(6):366–384,July2013. Publisher: AudioEngineeringSociety.
[18] AndrewHines,JanSkoglund,AnilCKokaram,andNaomiHarte. ViSQOL:anobjectivespeechqualitymodel.
EURASIPJournalonAudio,Speech,andMusicProcessing,2015(1):13,December2015.
22
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms
[19] MichaelChinen,FeliciaS.C.Lim,JanSkoglund,NikitaGureev,FeargusO’Gorman,andAndrewHines.ViSQOL
v3: AnOpenSourceProductionReadyObjectiveSpeechandAudioMetric. arXiv:2004.09584[cs,eess],April
2020.
[20] G.DavidForney.Codesongraphs:normalrealizations.IEEETransactionsonInformationTheory,47(2):520–548,
February2001.
[21] Hans-AndreaLoeliger.Anintroductiontofactorgraphs.SignalProcessingMagazine,IEEE,21(1):28–41,January
2004.
[22] O.KakushoandM.Yanagida. HierarchicalARmodelfortimevaryingspeechsignals. InICASSP’82.IEEE
InternationalConferenceonAcoustics,Speech,andSignalProcessing,volume7,pages1295–1298,Paris,France,
May1982.
[23] K. Paliwal and A. Basu. A speech enhancement method based on Kalman filtering. In ICASSP ’87. IEEE
InternationalConferenceonAcoustics,Speech,andSignalProcessing,volume12,pages177–180,Dallas,TX,
USA,April1987.
[24] J.Vermaak,C.Andrieu,A.Doucet,andS.J.Godsill. ParticlemethodsforBayesianmodelingandenhancementof
speechsignals. IEEETransactionsonSpeechandAudioProcessing,10(3):173–185,March2002.
[25] DanielRudoy,ThomasF.Quatieri,andPatrickJ.Wolfe. Time-VaryingAutoregressionsinSpeech: Detection
TheoryandApplications. IEEETransactionsonAudio,Speech,andLanguageProcessing,19(4):977–989,May
2011.
[26] AlbertPodusenko,WouterM.Kouw,andBertdeVries. OnlineVariationalMessagePassinginHierarchical
AutoregressiveModels. In2020IEEEInternationalSymposiumonInformationTheory(ISIT),pages1337–1342,
LosAngeles,CA,USA,June2020. ISSN:2157-8117.
[27] D.C.PopescuandI.Zeljkovic. Kalmanfilteringofcolorednoiseforspeechenhancement. InProceedingsofthe
1998IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing,ICASSP’98,volume2,pages
997–1000,Seattle,WA,USA,May1998. ISSN:1520-6149.
[28] S.Gannot,D.Burshtein,andE.Weinstein. IterativeandsequentialKalmanfilter-basedspeechenhancement
algorithms. IEEETransactionsonSpeechandAudioProcessing,6(4):373–385,July1998.
[29] J.D. Gibson, B. Koo, and S.D. Gray. Filtering of colored noise for speech enhancement and coding. IEEE
TransactionsonSignalProcessing,39(8):1732–1742,August1991.
[30] Albert Podusenko, Bart van Erp, Dmitry Bagaev, Ismail Senoz, and Bert de Vries. Message Passing-Based
InferenceintheGammaMixtureModel. In2021IEEE31stInternationalWorkshoponMachineLearningfor
SignalProcessing(MLSP),pages1–6,GoldCoast,Australia,October2021.IEEE.
[31] BartvanErp,AlbertPodusenko,TanyaIgnatenko,andBertdeVries. ABayesianModelingApproachtoSituated
DesignofPersonalizedSoundscapingAlgorithms. AppliedSciences,11(20):9535,October2021. Number: 20
Publisher: MultidisciplinaryDigitalPublishingInstitute.
[32] NeilHoulsby,FerencHuszár,ZoubinGhahramani,andMátéLengyel.BayesianActiveLearningforClassification
andPreferenceLearning. arXiv:1112.5745[cs,stat],December2011.
[33] WeiChuandZoubinGhahramani. PreferencelearningwithGaussianprocesses. InProceedingsofthe22nd
internationalconferenceonMachinelearning,ICML’05,pages137–144,NewYork,NY,USA,August2005.
AssociationforComputingMachinery.
[34] FerencHuszar. AGPclassificationapproachtopreferencelearning. InNIPSWorkshoponChoiceModelsand
PreferenceLearning,page4,SierraNevada,Spain,2011.
[35] CarlEdwardRasmussenandChristopherK.IWilliams. GaussianProcessesforMachineLearning. MITPress,
2006.
[36] Hans-AndreaLoeliger,JustinDauwels,JunliHu,SaschaKorl,LiPing,andFrankR.Kschischang. TheFactor
GraphApproachtoModel-BasedSignalProcessing. ProceedingsoftheIEEE,95(6):1295–1322,June2007.
[37] MarcoCox,ThijsvandeLaar,andBertdeVries. AfactorgraphapproachtoautomateddesignofBayesiansignal
processingalgorithms. InternationalJournalofApproximateReasoning,104:185–204,January2019.
[38] ChristopherM.Bishop. PatternRecognitionandMachineLearning. Springer-VerlagNewYork,Inc.,2006.
[39] ThijsvandeLaar.AutomatedDesignofBayesianSignalProcessingAlgorithms.PhDthesis,EindhovenUniversity
ofTechnology,Eindhoven,TheNetherlands,2019.
[40] NoorSajid,PhilipJ.Ball,ThomasParr,andKarlJ.Friston. ActiveInference: DemystifiedandCompared. Neural
Computation,33(3):674–712,March2021.
23
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms Podusenkoetal.
[41] ThomasParrandKarlJ.Friston. Uncertainty,epistemicsandactiveinference. JournalofTheRoyalSociety
Interface,14(136):20170376,November2017.
[42] KevinH.Knuth. InformedSourceSeparation: ABayesianTutorial. arXiv:1311.3001[cs,stat],November2013.
arXiv: 1311.3001.
[43] SimoSärkkä. BayesianFilteringandSmoothing. CambridgeUniversityPress,London;NewYork,October2013.
[44] AlbertPodusenko,WouterM.Kouw,andBertdeVries. MessagePassing-BasedInferenceforTime-Varying
AutoregressiveModels.Entropy,23(6):683,June2021.Number:6Publisher:MultidisciplinaryDigitalPublishing
Institute.
[45] ˙Ismail S¸enöz, Thijs van de Laar, Dmitry Bagaev, and Bert de Vries. Variational Message Passing and Local
Constraint Manipulation in Factor Graphs. Entropy, 23(7):807, 2021. Publisher: Multidisciplinary Digital
PublishingInstitute.
[46] Justin Dauwels. On Variational Message Passing on Factor Graphs. In IEEE International Symposium on
InformationTheory,pages2546–2550,Nice,France,June2007.
[47] J.Bezanson,A.Edelman,S.Karpinski,andV.Shah. Julia: AFreshApproachtoNumericalComputing. SIAM
Review,59(1):65–98,January2017.
[48] DmitryBagaevandBertdeVries. ReactiveMessagePassingforScalableBayesianInference. 2022. Submitted
totheJournalofMachineLearningResearch.
[49] PatrickKMogensenandAsbjørnNRiseth. Optim: AmathematicaloptimizationpackageforJulia. Journalof
OpenSourceSoftware,3(24):615,April2018.
[50] KevinP.Murphy,YairWeiss,andMichaelI.Jordan. Loopybeliefpropagationforapproximateinference: An
empiricalstudy.InProceedingsoftheFifteenthconferenceonUncertaintyinartificialintelligence,pages467–475.
MorganKaufmannPublishersInc.,1999.
[51] TeshengHsiao. IdentificationofTime-VaryingAutoregressiveSystemsUsingMaximumaPosterioriEstimation.
IEEETransactionsonSignalProcessing,56(8):3497–3509,August2008.
[52] FrankKleibergenandHenkHoek. BayesianAnalysisofARMAmodelsusingNoninformativePriors. CentER
DiscussionPaper,1995-116:24,1995.
[53] KarlFristonandWillPenny. PosthocBayesianmodelselection. Neuroimage,56(4-2):2089–2099,June2011.
[54] KarlFriston,ThomasParr,andPeterZeidman. Bayesianmodelreduction. arXiv:1805.07092[stat],May2018.
arXiv: 1805.07092.
[55] A.OzerovandC.Fevotte. MultichannelNonnegativeMatrixFactorizationinConvolutiveMixturesforAudio
SourceSeparation. IEEETransactionsonAudio,Speech,andLanguageProcessing,18(3):550–563,March2010.
[56] StevenRennie,TraustiKristjansson,PederOlsen,andRameshGopinath. Dynamicnoiseadaptation. In2006
IEEEInternationalConferenceonAcousticsSpeechandSignalProcessingProceedings,volume1,pages1–4,
Toulouse,France,2006.IEEE.
[57] S.J.Rennie,J.R.Hershey,andP.A.Olsen. Single-channelspeechseparationandrecognitionusingloopybelief
propagation. InIEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing,2009.ICASSP2009,
pages3845–3848,Taipei,Taiwan,April2009.
[58] BrendanJFrey,LiDeng,AlexAcero,andTraustiKristjansson. ALGONQUIN:IteratingLaplace’sMethodto
RemoveMultipleTypesofAcousticDistortionforRobustSpeechRecognition. InProceedingsoftheEurospeech
Conference,pages901–904,Aalborg,Denmark,September2001.
[59] TanyaIgnatenko,KirillKondrashov,MarcoCox,andBertdeVries. OnSequentialBayesianOptimizationwith
PairwiseComparison. arXiv:2103.13192[cs,math,stat],March2021. arXiv: 2103.13192.
[60] MarcoCoxandBertdeVries. AparametricapproachtoBayesianoptimizationwithpairwisecomparisons. In
NIPSWorkshoponBayesianOptimization(BayesOpt2017),pages1–5,LongBeach,USA,December2017.
[61] KarlJ.Friston,NoorSajid,DavidRicardoQuiroga-Martinez,ThomasParr,CathyJ.Price,andEmmaHolmes.
Activelistening. HearingResearch,399(Stimulus-specificadaptation,MMNandpredictingcoding):107998,
January2021.
[62] EmmaHolmes,ThomasParr,TimothyD.Griffiths,andKarlJ.Friston. Activeinference,selectiveattention,and
thecocktailpartyproblem. NeuroscienceandBiobehavioralReviews,131:1288–1304,October2021.
[63] Y. Laufer and S. Gannot. A Bayesian Hierarchical Model for Blind Audio Source Separation. In 2020 28th
EuropeanSignalProcessingConference(EUSIPCO),pages276–280,January2021. ISSN:2076-1465.
24
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms
[64] S.Xie,L.Yang,J.Yang,G.Zhou,andY.Xiang. Time-FrequencyApproachtoUnderdeterminedBlindSource
Separation. IEEETransactionsonNeuralNetworksandLearningSystems,23(2):306–316,February2012.
[65] JohnRHershey,PederOlsen,andStevenJRennie. SignalInteractionandtheDevilFunction. InProceedingsof
theInterspeech2010,pages334–337,Makuhari,Chiba,Japan,2010.
[66] M.H.Radfar,A.H.Banihashemi,R.M.Dansereau,andA.Sayadiyan. Nonlinearminimummeansquareerror
estimatorformixture-maximisationapproximation. ElectronicsLetters,42(12):724–725,June2006.
[67] JudeaPearl. ReverendBayesonInferenceEngines: ADistributedHierarchicalApproach. InProceedingsofthe
SecondAAAIConferenceonArtificialIntelligence,AAAI’82,pages133–136,Pittsburgh,Pennsylvania,1982.
AAAIPress.
[68] FrankR.Kschischang,BrendanJ.Frey,andH.-A.Loeliger. Factorgraphsandthesum-productalgorithm. IEEE
Transactionsoninformationtheory,47(2):498–519,2001.
[69] JonathanSYedidia,WilliamTFreeman,andYairWeiss. Bethefreeenergy,Kikuchiapproximations,andbelief
propagationalgorithms. Advancesinneuralinformationprocessingsystems,13:24,2001.
[70] ThomasMinka. DivergenceMeasuresandMessagePassing. Technicalreport,MicrosoftResearch,2005.
25
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms Podusenkoetal.
A Factorgraphs,freeenergyandmessagepassing-basedinference
Thisappendixintroducesthebasicsoftheprobabilisticmodelingapproach,whichunderliesallcomputationsinthis
paper.First,inSectionA.1wedescribefactorgraphsasausefultoolforvisualizingprobabilisticmodels.InSectionA.2
we describe how exact probabilistic inference can be performed through message passing. We then introduce the
variationalfreeenergyinSectionA.3andtheBethefreeenergyinSectionA.4forquantifyingtheperformanceof
the probabilistic model when exact inference is not possible. By adding constraints to the approximate posterior
distributions, we end up with hybrid message passing algorithms, such as variational message passing, as will be
describedinSectionA.5.
A.1 Forney-stylefactorgraphs
Fortheremainderofthissectionconsiderthefactorizedfunction
(cid:89)
f(z)= f (z ), (26)
a a
a∈V
where f a denotesthesetoffactorsf (z )indexedbya fromthesetofvertices andzdenotestheset
a a a
{ | ∈V} ∈V V
ofvariablesz = z i ,whichareindexedbyi fromthesetofedges . Thevariablez representsthesetof
i a
{ | ∈E} ∈E E
allneighbouringvariablesoffactorf .
a
Factorgraphsareaspecifictypeofprobabilisticgraphicalmodels. InthispaperwewillfocusonForney-stylefactor
graphs(FFG)asintroducedin[20]withnotationalconventionsadoptedfrom[21]. FFGsvisualizefactorizedfunctions
asundirectedgraphs,whosenodesrepresenttheindividualfactorsoftheglobalfunction. Thenodesareinterconnected
byedgesrepresentingthemutualargumentsofthefactors. InFFGs,anodecanbeconnectedtoanarbitrarynumberof
edges,butedgesareconstrainedtohaveamaximumdegreeoftwo. Asanexample,considerthefactorizedfunction
f(z ,z ,z ,z ,z )=f (z )f (z ,z ,z )f (z ,z ,z )f (z ). (27)
1 2 3 4 5 a 1 b 1 2 3 c 3 4 5 d 4
TheFFGrepresentationofthisfunctionisvisualizedinFigure15.
When a variable occurs in more than three factors, this constraint can be satisfied by introducing equality factors,
definedasf (z,z(cid:48),z(cid:48)(cid:48))=δ(z z(cid:48))δ(z z(cid:48)(cid:48)). Herez(cid:48)andz(cid:48)(cid:48)arevariablecopiesofz,whoseposteriordistributions
=
− −
areconstrainedtobeidenticalasaresultoftheequalityfactor. Foramoreextensiveoverviewoffactorgraphs,werefer
theinterestedreaderto[21,36].
A.2 Sum-productmessagepassing
Probabilisticinferenceconcernsthecalculationoftheposteriordistributioninourmodel. Theposteriordistributionof
thefactorizedfunctionin(26)isdefinedas
f(z)
p(z)= , (28)
Z
(cid:82)
whereZ = f(z)dz isthenormalizationconstant. Hereandthroughouttherestofthissection,weassumetobe
dealingwithcontinuousvariablesforgenerality. Fordiscretevariables,thisintegrationsimplyreducestoasummation.
Furthermore we implicitly assume that the factors f are appropriate (possibly unnormalized) probability density
a
functions,meaningthattheirmappingisspecifiedasf
a
:R|za| R
≥0
,where z
a
denotesthecardinalityofsetz
a
.
→ | |
Computingthemarginaldistributionsofthisposteriorrequiresintegrationoverallnuisancevariables. Becauseofthe
conditional(in)dependenciesinafactorizedmodel,thiscomputationallycomplexglobalintegrationcanbeperformed
throughasetofsmallerlocalcomputations. Thisapproachisbetterknownasthesum-productalgorithmorbelief
propagation[67,68]. Theresultsofthelocalcomputationsaretermedmessagesµandtheypropagateovertheedgesof
thecorrespondingFFG.Inordertodistinguishbetweenthemessagesintheforwardandbackwarddirection,theedges
inthecorrespondingFFGaremadearbitrarilydirectedasshowninFigure15. Nowthemessagesµ(cid:126) and (cid:126) µarespecified
topropagateinandagainstthedirectionoftheedge,respectively. Thesum-productmessageµ(cid:126)(z )[68]flowingoutof
i
somenodef withz z isdefinedas
a i a
∈
(cid:90)
(cid:89)
µ(cid:126)(z ) f (z ) µ(cid:126)(z )dz , (29)
i a a j a\i
∝
zj∈z a\i
wherethenotationz referstothesetz excludingtheelementz ,formallydefinedasz = z z z / z .
a\i a i a\i a i
{ ∈ | ∈{ }}
Themessagesµ(cid:126)(z )aretheincomingmessagestothefactornode. Propagatingthesemessagesthroughoutthegraph
j
26
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms
z z
1 3
f f f
a b c
µ(cid:126)→(z1) µ(cid:126)→(z3)
f
d
(cid:126) µ←(z3)
z
4
(cid:126)
z
5
z
2
µ(z4)
↑
Figure 15: A Forney-style factor graph representation of the factorized function in (27). The edges are arbitrarily
directedtodistinguishbetweenforwardandbackwardmessagesonthegraph. Thedrawnmessagescanberegardedas
summariesofthedashedboxes,usedforsolving(31).
allowsustodeterminethemarginaldistributionsofsomevariablez astheproductofthemessagespropagatingonthat
i
respectiveedgeas
p(z ) µ(cid:126)(z )
i i
∝ ·
(cid:126) µ(z ). (30)
i
Toclarifytheabovepoints,supposethatweareinterestedincalculatingthemarginaldistributionofs fromthemodel
3
in(27). Thisdistributioncanbecalculated(uptoascalingconstant)as
(cid:90)
p(z )= p(z)dz
3 \3
(cid:90)(cid:90) (cid:90)(cid:90)
f (z )f (z ,z ,z )dz dz f (z )f (z ,z ,z )dz dz
a 1 b 1 2 3 1 2 d 4 c 3 4 5 4 5
∝ ·
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
µ(cid:126)(z3) (cid:126)
(31)
.
µ(z3)
Fromthisderivationthemarginaldistributionofz canbecalculatedastheproductoftwotermsthateachsummarizea
3
differentpartofthemodel. Figure15visualizestheexamplemodelof(27),nowwithdirectededges,andvisualizes
thecorrespondingmessagesassummariesofthedashedpartsofthegraph. Intheexamplethepriordistributionsover
z andz canberegardedasmessagesthemselves,meaningthatµ(cid:126)(z )=f (z )and
1 4 1 a 1
(cid:126) µ(z )=f (z ). Furthermore,
4 d 4
theedgescorrespondingtoz andz aredangling,meaningthattheyonlyreceiveinformationfromoneside. The
2 5
messagesinthereversedirectionaredefinedtobeuninformativeas (cid:126) µ(z ) =
2
(cid:126) µ(z ) = 1,becausethentheposterior
5
distributionisfullyderivedbythemessagesintheforwarddirectionase.g. p(z ) µ(cid:126)(z )
2 2
∝
(cid:126) µ(z )=µ(cid:126)(z ).
2 2
A.3 Variationalfreeenergy
ThecalculationofthenormalizationconstantZ in(28)isoftendifficultorevenintractable. Similarly,alsothemarginal
distributionsmightbeunobtainableinclosedform. Toresolvethis,wewillapproximatetheposteriordistribution
p(z)of(28)withanapproximatedistributionq(z). Usingthisapproximationwecandefinethevariationalfreeenergy
(VFE)functionas
(cid:20) (cid:21)
q(z)
F[q,f]=E ln =KL[q(z) p(z)] lnZ lnZ, (32)
q(z) f(z) (cid:107) − ≥−
whichprovidesanupper-boundonthenegativelog-normalizationconstantandisusedforapproximatingthemodel
performanceinintractablemodels. ThisboundisattainedattheminimumoftheVFEwhentheapproximateposterior
distributionequalstheexactposteriordistributionsq(z)=p(z). However,toallowfortractableinference,q(z)often
hastobeconstrainedtosomefamilyofdistributions asq(z) undersomefactorization.
Q ∈Q
A.4 Bethefreeenergy
TheBetheassumption
(cid:89) (cid:89)
q(z)= q (z ) q (z )−1. (33)
a a i i
a∈V i∈E
isausefulconstraintontheapproximateposteriorq(z),[69].
HerewemadeuseofthefactthatalledgesintheFFGhaveamaximumdegreeoftwo,whichcanbestrictlyenforced
byaddinguninformativepriorsp(z ) = 1todanglingedges. UndertheBetheassumption,theVFEreducestothe
i
27
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms Podusenkoetal.
Bethefreeenergy(BFE)
(cid:88) (cid:88) (cid:88)
F [q,f]= E [lnf (z )] H[q (z )]+ H[q (z )], (34)
B
−
q(za) a a
−
a a i i
a∈V a∈V i∈E
whichequalstheVFEforacyclicgraphs(i.e. trees). TheBFEdecomposestheVFEintoasumofnode-localfree
energiescontributionsandedge-specificentropiesH.
A.5 Variationalandhybridmessagepassing
Underthevariationalapproximationwecanemployvariationalinferenceinthemodel,whichiterativelyfindsstationary
points on the BFE by fixing all approximate posterior distributions besides the one that is being optimized. This
inferenceprocedurecanbecasttoamessagepassingparadigmandiscalledvariationalmessagepassing[46]. Herethe
exactmessageupdateruleof(29)thenreducestothevariationalmessageupdaterule[46]
(cid:110) (cid:111)
(cid:126)ν(z ) exp E [lnf (z )] (35)
i ∝ q(z a\i ) a a
where(cid:126)ν(z )denotestheoutgoingvariationalmessageonedgez . Theapproximatemarginaldistributionsarethen
i i
iterativelyupdatedas
q (z ) (cid:126)ν(z )
i i i
∝ ·
(cid:126) ν(z ). (36)
i
Thecalculationsofvariationalmessagesandapproximatemarginaldistributionsaretheniterativelyrepeateduntil
convergenceoftheVFEisreached.
InadditiontothestructureimposedbytheBetheapproximation,additionalconstraintscanbeenforced. Depending
on these localconstraints differentinference algorithmsnaturally follow [45]. [45]showsthat amongstothers the
sum-productalgorithm[67,68],variationalmessagepassing[46]andexpectationpropagation[70]canberecovered.
Bycombiningdifferentlocalconstraintswecanachievehybridmessagepassing-basedinferenceintheprobabilistic
model. Wehighlyrecommendtheinterestedreadertheworkof[45]foranextensiveoverviewofhybridmessage
passingschemes.
28
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms
B Probabilisticmodeloverview
ThisappendixgivesaconciseoverviewofthegenerativemodeloftheacousticmodelandAIDA.Thepriordistributions
areuninformativeunlessstatedotherwiseinSection5.
B.1 Acousticmodel
Theobservedsignalx isthesumofaspeechandnoisesignalas
t
x =s +n
t t t
(cid:124)
Thespeechsignals =e s ismodeledbyatime-varyingauto-regressiveprocessas
t 1 t
s (A(θ )s , V (γ))
t t t−1
∼N
Theauto-regressivecoefficientsofthespeechsignalaretime-varyingas
θ (θ , ωI )
t t−1 M
∼N
(cid:124)
Thenoisesignaln =e n isalsomodeledbyanauto-regressiveprocess
t 1 t
n (A(ζ )n , V (τ )) fort=t−,t−+1,...,t+
t k t−1 k
∼N
Theparametersofthenoisemodeldependonthecontext
L
(cid:89)
ζ (µ ,Σ )clk
k l l
∼ N
l=1
L
(cid:89)
τ Γ(α ,β )clk
k l l
∼
l=1
Thecontextc evolvesoveradifferenttimescaleindexedbykas
k
c Cat(Tc )
k k−1
∼
Thetransitionmatrixofthecontextismodeledas
T Dir(α )
1:L,j j
∼
Finally,theoutputofthehearingaidalgorithmy isformedastheweightedsumofthespeechandnoisesignalsas
t
y =u s +u n fort=t−,t−+1,...,t+
t sk t nk t
B.2 AIDA’suserresponsemodel
TheuserresponsesaremodeledbyaBernoullidistributioncontainingaGaussiancumulativeprobabilitydistribution
thatenforcestheoutputv (u )tothealloweddomainfortheargumentoftheBernoullidistribution
k k
r Ber(Φ(v (u ))) ifr 0,1
k k k k
∼ ∈{ }
v (u )encodesourbeliefsabouttheuserresponsefunction(evaluatedatu ), modeledbyamixtureofGaussian
k k k
processesas
L
(cid:89)
v GP(m (),K (, ))clk
k l l
∼ · · ·
l=1
whosekernelfunctionisdefinedas
(cid:26) u u(cid:48) 2(cid:27)
K(u,u(cid:48))=σ2exp (cid:107) − (cid:107)2
− 2l2
whereσdenotesnoiseandlthelengthscaleofthekernel.
29
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms Podusenkoetal.
Table2: SummaryofthenotationalconventionsusedthroughoutSection3inthispaper.
Notation Definition Explanation
x (2) Observedsignalattimeindext.
t
y (8) Outputofthehearingaidattimeindext.
t
s Latentspeechsignalattimeindext.
t
n Latentnoisesignalattimeindext.
t
s (3b) Vectorwithhiddenstatesoftheautoregressivemodelofthespeechsignalattimeindext.
t
n (5) Vectorwithhiddenstatesoftheautoregressivemodelofthenoisesignalattimeindext.
t
θ (3a) Vectorofautoregressivecoefficientsofthespeechsignalattimeindext.
t
ω Covariancematrixscalingofthedynamicsoftheautoregressivecoefficientsofthespeech
signal.
I Identitymatrixofsize(M M).
M
×
M Autoregressiveorderofthespeechsignal.
N Autoregressiveorderofthenoisesignal.
W Samplingratiobetweentandk.
A(θ) (4) Statespacetransitionmatrixofanautoregressiveprocesswithcoefficientsθ.
γ Processnoiseprecisionoftheautoregressivemodelforthespeechsignal.
V(γ) Covariancematrixoftheautoregressivemodelforthespeechsignal,containingonlyzeros
exceptforthefirstelement,whichis1/γ.
e Appropriatelysizedcartesianstandardunitvectorcontainingonlyzeros,exceptfortheith
i
entry,whichis1.
0 Appropriatelysizedvectorofzeros.
(µ,Σ) NormaldistributionwithmeanµandcovariancematrixΣ.
N
Γ(α,β) Gammadistributionwithshapeandrateparametersαandβ,respectively.
ζ (7a) Vectorofauto-regressivecoefficientsofthenoisesignalatcontexttimescaleindexk.
k
τ (7b) Processnoiseprecisionoftheautoregressivemodelofthenoisesignalatcontexttimescale
k
indexk.
L Numberofcontexts.
c (9) Thecontextatcontexttimescalek,containinga1-of-Lbinaryvectorwithelementsc
k lk
∈
0,1 .
{ }
π Vectorofeventprobabilities.
T (10) Transitionmatrixrepresentingthediscretecontextdynamics.
α ConcentrationparametersfortheprioroverthejthcolumnofT.
j
u Gainofthespeechsignalatcontexttimeindexk.
sk
u Gainofthenoisesignalatcontexttimeindexk.
nk
u Vectoroftuningparametersatcontexttimeindexkdefinedas[u ,u ] (cid:124).
k sk nk
v () (12a) LatentfunctiondrawnfromamixtureofGPs.
k
·
r (12b) Binaryuserresponseattimeindexk.
k
Φ() StandardGaussiancumulativedistributionfunction.
m ( · ) MeanfunctionofthelthGP.
l
K (, · ) KernelfunctionofthelthGP.
l
· ·
30
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms
C Inferencerealization
ThisappendixdescribesindetailhowtheinferencetasksofSections4.1and4.2arerealized. Theinferencetaskof
Section4.3isperformedbyautomatedmessagepassingusingtheupdaterulesof[44].
C.1 Realizationofinferenceforcontextclassification
Theinferencetaskforcontextclassificationof(13)rendersintractableasdiscussedinSection(4.1). Tocircumventthis
problem,wewillsolvethistaskasaBayesianmodelcomparisontask.
InaBayesianmodelcomparisontask,weareinterestedincalculatingtheposteriorprobabilityp(m x)ofsome
l
|
modelm afterobservingdatax. Theposteriormodelprobabilityp(m x)canbecalculatedusingBayes’ruleas
l l
|
p(x m )p(m )
p(m l | x)= (cid:80) p(x | m l )p(m l ) , (39)
j | j j
wherethedenominatorrepresentstheweightedmodelevidencep(x),i.e.themodelevidenceobtainedfortheindividual
modelsp(x m ),weightedbytheirpriorsp(m ).
l l
|
ToformulateourinferencetaskasaBayesianmodelcomparisontask,thedistinctmodelsm firsthavetobespecified.
l
Inordertodoso,wefirstnotethatweobtainthepriorsofc andz in(13)separately,andthereforeweimplicitly
k−1 t−−1
assumeafactorizationofourpriorp(c ,z x )as
k−1 t−−1
|
1:t−−1
p(c ,z x )=p(c x )p(z x ). (40)
k−1 t−−1
|
1:t−−1 k−1
|
t−−1 t−−1
|
1:t−−1
Asaresult(13)canberewrittenas
(cid:90)
p(c x ) p(c ,T c )p(c x )dTdc
k
|
1:t+
∝
k
|
k−1 k−1
|
1:t−−1 k−1
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:90)
µ(cid:126)(ck)
(41)
p(z ,Ψ ,x z ,c)p(z x )dz dΨ .
·
t−:t+ k t−:t+
|
t−−1 t−−1
|
1:t−−1 t−−1:t+ k
(cid:124) (cid:123)(cid:122) (cid:125)
p(x t−:t+|x 1:t−−1 ,ck)
Thefirsttermµ(cid:126)(c )canberegardedastheforwardmessagetowardsthecontextc originatingfromtheprevious
k k
context. Itgivesusanestimateofthenewcontextsolelybasedonthecontextdynamicsasstipulatedbythetransition
matrixT. Thesecondtermp(x x ,c )canberegardedastheincrementalmodelevidenceundersome
t−:t+
|
1:t−−1 k
givencontextc . Comparisonof(41)and(39)allowsustoformulateourinferenceproblemin(13)intoaBayesian
k
modelcomparisonproblembydefining
p(m )=µ(cid:126)(c =e ), (42a)
l k l
p(x m )=p(x x ,c =e ). (42b)
|
l t−:t+
|
1:t−−1 k l
Wecanthereforedefineamodelm byclampingthecontextvariableingenerativemodelasc =e . Thismeansthat
l k l
eachmodelonlyhasoneactivecomponentforboththeGaussianandGammamixturenodesandthereforethemessages
originatingfromthesenodesareexactanddonotrequireavariationalapproximation.
Despitetheexpansionofthemixturemodels,theincrementalmodelevidencep(x x ,c =e )cannotbe
t−:t+
|
1:t−−1 k l
computedexactlyastheauto-regressivesourcemodelsleadtointractableinference. Asaresultweapproximatethe
modelevidencein(42b)usingtheBethefreeenergy,asdefinedin(34)inAppendixA,as
p(x m ) exp F [q,m ] , (43)
l B l
| ≈ {− }
whereF [q,m ]denotestheBethefreeenergyobservedafterconvergenceoftheinferencealgorithmformodelm .
B l l
Similarlythecalculationof(42a)isintractable. Thereforewewillapproximatethemodelpriorwiththevariational
messagetowardsc insteadas
k
p(m ) (cid:126)ν(c =e ). (44)
l k l
≈
C.2 Realizationofinferencefortrialdesign
ProbabilisticinferenceinAIDAencompasses2tasks: 1)optimalproposalselectionand2)updatingoftheGaussian
processclassifier(GPC).Herewespecifyhowtheseinferencetasksareexecutedinmoredetail.
31
AIDA:AnActiveInference-basedDesignAgentforAudioProcessingAlgorithms Podusenkoetal.
Optimalproposalselection
Aclosed-formexpressionoftheEFEdecompositionin(15)canbeobtainedfortheGPCasshownin[32].
Thefirstterminthedecomposition,thenegativeutilitydrive,resemblesthecross-entropylossbetweenourgoalprior
andposteriormarginal. Sinceuserresponsesarebinary,wecanevaluatethisbinarycross-entropytermas[32]
    
E
q(r|u)
[lnp(r)]=Φ(cid:113) µ u,D lnE
p(r)
[r]+1 Φ(cid:113) µ u,D ln (cid:0) 1 E
p(r)
[r] (cid:1) , (45)
− σ2 +1 − σ2 +1 −
u,D u,D
whereµ andσ2 denotetheposteriormeanandvariancereturnedbytheGPCwhenqueriedatthepointugiven
u,D u,D
somedatasetD = u ,r ,respectively. Moreconcretely,theGPCreturnsaGaussiandistributionfrom
1:k−1 1:k−1
whichtheposteriorm { eanandvarian } ceareextractedasv(u)= (µ ,σ2 ). Φ()denotesthestandardGaussian
N u,D u,D ·
cumulativedistributionfunction. p(r)denotestheBernoulligoalprioroverdesireduserfeedback. histhebinary
(cid:113)
entropyfunctionandC = πln2. Forbrevity,wedenotethedatasetofparametersandmatchinguserresponses
2
collectedsofarasD.
Thesecondterminthedecomposition,the(negative)informationgain,describeshowmuchinformationwegainby
observinganewuserappraisal. Thisinformationgainterm(IG)canbeexpressedinaGPCas[32]
    
µ C
µ2
IG[r,v D,u] hΦ(cid:113) u,D  (cid:113) exp (cid:16) u,D (cid:17) , (46)
| ≈ σ2 +1 − σ2 +C2 − 2 σ2 +C2
u,D u,D u,D
(cid:113)
wheretheconstantC isdefinedasC = πln2 andwhereh()isdefinedash(p)= pln(p) (1 p)ln(1 p).
2 · − − − −
InferenceintheGaussianprocessclassifier
For our experiments we use Laplace approximation as described in [35, Chapter 3.4] for performing inference in
theGPC.TheLaplaceapproximationisatwo-stepprocedure,whereweapproximatetheposteriordistributionbya
Gaussiandistribution. Wefirstfindthemodeoftheexactposterior,whichresemblesthemeanoftheapproximated
Gaussiandistribution. ThenweapproximatethecorrespondingprecisionasthenegativeHessianaroundthemode.
Findingtheexactposteriorp(v D)amountstocalculating
|
p(r v)p(v u )
p(v D)=
1:k−1
| |
1:k−1
(47a)
| p(r u )
1:k−1 1:k−1
|
p(r v)p(v u ). (47b)
1:k−1 1:k−1
∝ | |
Takingthelogarithmof(47b)anddifferentiatingtwicewithrespecttovgives
lnp(v D)= lnp(r v) K−1v (48a)
v v 1:k−1
∇ | ∇ | −
lnp(v D)= lnp(r v) K−1 = W K−1 (48b)
v v v v 1:k−1
∇ ∇ | ∇ ∇ | − − −
where denotesthegradientwithrespecttov,K =K(u ,u )isthekernelmatrixoverthequeriesu
v 1:k−1 1:k−1 1:k−1
∇
andW = lnp(r v)isadiagonalmatrixsincethelikelihoodfactorizesoverindependentobservations.
v v 1:k−1
−∇ ∇ |
Atthemodevˆ(48a)equalszerowhichimplies
vˆ=K lnp(r vˆ). (49)
v 1:k−1
∇ |
Directly solving (49) is intractable, because of the recursive non-linear relationship. Instead we can estimate vˆ
using Newton’s method, where we perform iterations with an adaptive step size. We omit the computational and
implementationdetailshereandinsteadreferto[35,Algorithm3.1]. Wedeterminethestepsizeusingalinesearchas
implementedinOptim.jl[49]. Havingfoundthemodevˆ,wecanconstructourposteriorapproximationas
p(v D) (cid:16) vˆ, (cid:0) K−1+W (cid:1)−1 (cid:17) , (50)
| ≈N
whereW isevaluatedatv =vˆ.IfwenowrecallthatevaluatingaGPatanyfinitenumberofpointsresultsinaGaussian,
weseethatundertheLaplaceapproximationthesolutioncanbeobtainedusingstandardresultsformarginalization
ofjointlyGaussianvariables. WedefinetheshorthandK(u ,u ) = K andK(u ,u ) = K andfindthe
k 1:k−1 1:k k k k
posteriormeanµ as[35,p. 44]
u
µ =K (cid:124) K−1vˆ=K (cid:124) lnp(r vˆ). (51)
u,D 1:k 1:k∇ 1:k−1 |
Theposteriorcovarianceσ2 isgivenby[35,p. 44]
u,D
σ2 =K K (cid:124) (cid:0) K+W−1(cid:1)−1 K . (52)
u,D k − 1:k 1:k
32

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
