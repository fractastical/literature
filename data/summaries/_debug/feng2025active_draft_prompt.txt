=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Active Multimodal Distillation for Few-shot Action Recognition
Citation Key: feng2025active
Authors: Weijia Feng, Yichen Zhu, Ruojia Zhang

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Key Terms: multimodal, distillation, action, shenzhen, hangzhou, china, information, recognition, performance, shot

=== FULL PAPER TEXT ===

5202
nuJ
61
]VC.sc[
1v22331.6052:viXra
Active Multimodal Distillation for Few-shot Action Recognition
WeijiaFeng1, YichenZhu1, RuojiaZhang1, ChenyangWang2,3∗,
FeiMa3, XiaobaoWang4 and XiaobaiLi5,6
1CollegeofComputerandInformationEngineering,TianjinNormalUniversity,Tianjin,China
2CollegeofComputerScienceandSoftwareEngineering,ShenzhenUniversity,Shenzhen,China
3GuangdongLaboratoryofArtificialIntelligenceandDigitalEconomy(SZ),Shenzhen,China
4CollegeofIntelligenceandComputing,TianjinUniversity,Tianjin,China
5TheStateKeyLaboratoryofBlockchainandDataSecurity,ZhejiangUniversity,Hangzhou,China
6HangzhouHigh-TechZone(Binjiang)InstituteofBlockchainandDataSecurity,Hangzhou,China
weijiafeng@tjnu.edu.cn,{zhuyiiichen,zrj20001127}@163.com,chenyangwang@ieee.org,
mafei@gml.ac.cn,wangxiaobao@tju.edu.cn,xiaobai.li@zju.edu.cn
Abstract isoftencostlyandtime-intensive,especiallyforpracticalac-
tion recognition applications. To address this issue, recent
Owing to its rapid progress and broad application research[Yangetal.,2023]hasfocusedonenhancingtheef-
prospects,few-shotactionrecognitionhasattracted ficiency of video understanding through lightweight models
considerable interest. However, current methods thatcanprocesstemporalinformationefficientlywhilemini-
are predominantly based on limited single-modal mizingfew-shotactionrecognitioncosts.
data, which does not fully exploit the potential The existing methods [Feng et al., 2024; Wang et al.,
of multimodal information. This paper presents 2024a] have shown significant performance. They mainly
a novel framework that actively identifies reliable
relyonsingle-modaldata(suchasRGBframes),andhuman
modalitiesforeachsampleusingtask-specificcon-
actionscomprisemodalitiessuchasRGB,opticalflow,skele-
textual cues, thus significantly improving recog-
talnodes,anddepthinformation. Thesemethodscannotcap-
nition performance. Our framework integrates an
ture the global information of human actions, and their per-
ActiveSampleInference(ASI)module,whichuti-
formancewilldecreasetoacertainextentwhenencountering
lizesactiveinferencetopredictreliablemodalities
complex actions. Due to the limitations inherent in single-
based on posterior distributions and subsequently modalityapproaches[Maetal.,2024a],thepotentialofmul-
organizesthemaccordingly. Unlikereinforcement
tiplemodalitieshasattractedextensivescholarlyattentionand
learning, active inference replaces rewards with been empirically validated as effective [Xue et al., 2024;
evidence-based preferences, making more stable Ma et al., 2024b]. Several state-of-the-art methods have
predictions. Additionally, we introduce an active begun to combine multiple modalities [Wu et al., 2025;
mutual distillation module that enhances the rep- Wangetal.,2025;Maetal.,2024c],suchasvisualdata,op-
resentation learning of less reliable modalities by
ticalflow,andaudio,toprovidecomplementaryrecognition.
transferring knowledge from more reliable ones.
However, these methods cannot identify which modality is
Adaptivemultimodalinferenceisemployedduring
importantandwhichothermodalitiesarenotimportantinthe
the meta-test to assign higher weights to reliable
current sample. These methods may assign extremely high
modalities. Extensiveexperimentsacrossmultiple
weightstounimportantmodalitieswhenrecognizingactions,
benchmarks demonstrate that our method signifi-
resultinginunsatisfactoryrecognitionresults.
cantlyoutperformsexistingapproaches.
Inspired by Active Inference [Tschantz et al., 2020], this
paperproposesanActiveMultimodalFew-ShotInferencefor
Action Recognition (AMFIR) to address the limitations of
1 Introduction
single-modal data in few-shot action recognition. The pro-
Over the past few years, video action recognition has wit- posed AMFIR significantly improves the accuracy and effi-
nessedsubstantialadvancements,largelyduetotherapidde- ciency of inference by actively identifying the most domi-
velopment of deep learning technologies. Although tradi- nantmodalityofeachquerysample. Thisframeworkadopts
tionalapproaches,including2Dand3DConvolutionalNeu- ameta-learningparadigm, whereeachlearningunitconsists
ralNetworks(CNNs),haveshownremarkableproficiencyin oflabeledsupportsamplesandunlabeledquerysamples. In
capturing both spatial and temporal features within videos, themeta-trainingphase,weuseamodality-specificbackbone
they typically demand extensive labeled data for effective networktoextractfeaturerepresentationsbasedonactivein-
training. Theacquisitionofsuchlarge-scalelabeleddatasets ference, and divide the query samples into an RGB domi-
nant group and an optical-flow dominant group. We further
∗Correspondingauthor:ChenyangWang. designed a bidirectional distillation mechanism to guide the
learning of unreliable modes through reliable modes. In the on alignment and strives to evaluate the similarity between
meta-testphase,ActiveMultimodalInference(AMI)dynam- querysamplesandsupportsamplesthroughsynchronization
icallyfusesposteriordistributionsofdifferentmodalities,as- frames or periods in the temporal or spatial domain [Cao et
signing higher weights to more reliable modalities to opti- al.,2020;Wangetal.,2022;Wuetal.,2022]. TheM2-CLIP
mizeinferenceresults.Overall,themaincontributionsofthis framework[Wangetal.,2024b]utilizesmultimodaladapters
articlecanbesummarizedasfollows: and multitasking decoders to improve video action recogni-
tion while maintaining a strong zero sample generalization
• This article utilizes the natural complementarity be-
ability.
tween different modalities to select the most dominant
modality for each query sample through active infer-
ence, thereby significantly improving the performance 2.3 ActiveInference
offew-shotactionrecognition.
• A mutual refinement strategy has been proposed to Active Inference (AIF) is a Bayesian framework explaining
transfer task-related knowledge learned from reliable howorganismsminimizeuncertaintyandsurprisebypredict-
modes to representation learning of unreliable modali- ing and evaluating sensory inputs. It originates from Karl
ties,leveragingthecomplementarityofmultiplemodal- Friston’s Free Energy Principle (FEP) and emphasizes how
itiestoenhancetheabilitytoidentifyunreliablemodal- organismspredictfuturestatesandadjustbehaviortoachieve
ities. goalsthroughgenerativemodelsduringenvironmentalinter-
actions. Forexample, [Sedlaketal.,2024]exploredtheap-
• We have designed an adaptive multimodal few-sample
plicationofactivereasoninginedgecomputing,demonstrat-
inference method that combines the results of specific
ing its potential in adaptive flow processing to meet service
modalities and assigns higher weights to more reliable
level objectives (SLOs) and real-time system management.
modalitiestooptimizerecognitionperformancefurther.
Similarly, [Pezzulo et al., 2023] highlighted the close re-
lationship between active reasoning and Generative AI, em-
2 RelatedWork phasizing its role in achieving higher-level intelligence and
2.1 Few-shotLearning understanding through active perception and action. These
advancementssuggestthatactivereasoningprovidesanovel
Few-shot learning (FSL) focuses on recognizing novel con-
perspective for designing and implementing artificial intel-
cepts with minimal labeled training data. In recent years,
ligence, fostering progress in related fields. Furthermore,
few-shot learning (FSL) has made significant strides in var-
in hyperspectral image classification (HSIC), the Active In-
ioustraditionaldomains,suchasimageclassification[Bateni
ference Transfer Convolutional Fusion Network (AI-TFNet)
et al., 2020], object detection [Fan et al., 2020; Ma et al.,
proposedby[Wangetal.,2023]utilizesapseudo-labelprop-
2024c],andsegmentation[Liuetal.,2020;Fengetal.,2025].
agationalgorithmtoenhancetheavailabilityoftrainingsam-
Despitetheseachievements,mostexistingFSLmethodspri-
plesandoptimizeclassificationperformance.
marily concentrate on single-modality data, with relatively
limitedexplorationofmultimodalapproaches. Forinstance,
sometechniques[Pahdeetal.,2019]enhancelow-shotvisual 2.4 KnowledgeDistillation
embeddingsbyincorporatingauxiliarytextdataduringtrain-
ing, therebyboostingperformanceinfew-shotimageclassi- Knowledge distillation serves as an effective knowledge
fication. Others [Dong et al., 2018] focus on modeling the transfer technique, extracting information from teacher net-
interplay between visual and textual information to address works and conveying it to student models. Recent re-
taskslikefew-shotimagedescriptionandvisualquestionan- search[Hinton,2015;Parketal.,2019;TungandMori,2019]
swering. Similarly, [Tsimpoukelli et al., 2021] leverages has highlighted its potential in cross-modal tasks. For in-
pre-trainedlanguagemodelstoextendfew-shotlearningca- stance, [Gupta et al., 2016] developed a method to trans-
pabilities to downstream multimodal tasks, including visual fer supervision across different modalities, using represen-
questionanswering. Collectively, thesestudieshighlightthe tations learned from well-labeled modalities as guidance for
potentialofmultimodaldatatoaddressthelimitationsinher- trainingnew,unlabeledmodalities. Similarly, [Garciaetal.,
entinunimodalFSLmethods. 2018]proposedatechniquefortrainingmultimodalvideoac-
tionrecognitionmodelsusingbothdepthandRGBdata,ad-
2.2 Few-shotActionRecognition
dressingchallengessuchasnoiseandmissingmodalitiesdur-
Two promising few-shot action recognition techniques have ingtests. TheMARSframework[Crastoetal., 2019]simu-
been proposed in the study. The first type utilizes data aug- lates optical flow through RGB frame training, avoiding op-
mentation to support robust representation learning by cre- tical flow computation at test time. It integrates appearance
ating supplementary training data [Kumar Dwivedi et al., andmotioninformationbycombiningfeaturelossandcross-
2019], self-supervised cues [Zhang et al., 2020], or auxil- entropy loss. Additionally, [Dai et al., 2021] introduced a
iary information [Fu et al., 2020; Wu et al., 2022]. Premier knowledge distillation framework for action detection, en-
TACO[Zhengetal.,2024]hasimprovedtheefficiencyofthe hancingRGBrepresentationsbyleveragingknowledgefrom
few-shot learning strategy through multi-task feature repre- othermodalitieslikeopticalflowand3Dpose.Thisapproach
sentationandnegativesampleselectionmechanism, demon- achieves performance comparable to dual-stream networks
stratingsignificantperformance. Thesecondmethodfocuses usingonlyRGBdataduringthetest.
Figure1:Illustrationofourproposedframeworkinthe3-way3-shotsetting.
3 Method tionsofquerysamples{qm}M andtheprototypesofsupport
i i=1
samples {tm}N for each modality. Subsequently, we cal-
3.1 ProblemDefinition i k=1
culate the modality-specific posterior distributions for each
Thispaperemploysameta-learningframeworkforfew-shot
querysamplebasedonthedistancesbetweenquerysamples
multimodal action recognition, which includes two primary
and prototypes in the modality-specific feature space. Dur-
phases: meta-training and meta-test. In the meta-training
ing the meta-training phase, we introduce an Active Sample
phase,weutilizeamultimodalvideodatasetD thaten-
train Inference (ASI) module, which takes the modality-specific
compasses base action classes C . We construct multi-
train posterior distributions as inputs and performs reliability in-
ple meta-tasks (often termed episodes) from D to train
train ference to assess the reliability of each modality (RGB and
ameta-learnercapableofgeneralizingtonewactionclasses.
opticalflow)foreachquerysample. Thisprocesscategorizes
Each meta-task τ consists of a query set Q ⊂ D and a
train thesamplesintotwogroups:theopticalflowdominantgroup
supportsetS ⊂D train .WithintheN−wayK−shotmeta- Gf andRGBdominantgroupGr. Fortheselectedsamplesin
learningsetting,thequerysetQ={(xr
i
,xf
i
,y
i
)}M
i=1
includes Gf andGr,weimplementActiveMutualDistillationtotrans-
M multimodalquerysamples.Here,xrandxf representtwo fertask-specificknowledgefromreliablemodalitiestounreli-
i i
modalities(RGBandopticalflow)ofthei−thquerysample, ableonesviaabidirectionaldistillationmechanism, thereby
andy ∈ {1,2,··· ,N}denotestheclasslabelofthei−th enhancing the representation learning of the less dominant
i
query sample. The support set S = {(xr,xf,y )}M+NK modality. Inthemeta-testphase,weemployadaptivemulti-
containsK multimodalsamplesforeachof i the i N c i las i= se M s. + I 1 n modalinference,whichleveragesmodality-specificposterior
the meta-test phase, we employ a multimodal dataset D , distributions to make adaptive fusion decisions, prioritizing
test
which includes novel action classes C that are disjoint themorereliablemodalities.
test
fromthetrainingclasses(C ∩C =⊘).Similartothe
test train
3.3 ActiveSampleInference
meta-trainingphase, thesupportandquerysetsforeachtest
taskareconstructedinthesamemanner. Akeypointtohigh- Inthismodule,weusetheideaofAIFtoactivelypredictthe
lightisthattheclasslabelsofthequerysamplesareconcealed most dominant modality of each sample, and this predicted
duringthemeta-test. Themeta-learnermustaccuratelyclas- modalitywillbeconsideredasthesample-specificdominant
sify each sample in the query set, relying exclusively on the modality of the corresponding sample. Before introducing
labeledsamplesprovidedinthesupportset. thismodule,wefirstneedtoreviewthemethodofAIF,which
aimstomaximizetheBayesianmodelevidenceforanagent’s
3.2 Overview
generativemodelinthecontextofPartiallyObservedMarkov
The overall architecture of the proposed AFMIR is depicted Decision Processes (POMDP). Formally, a POMDP is de-
inFigure1. Foreachepisode,weutilizeabackbonenetwork fined by a tuple ⟨S,A,O,P,Θ⟩, where S denotes the true
ϕm(Q,S;θm),m ∈ (r,f) to extract the feature representa- state of the environment and A denote agent’s action space.
During each time step t, the agent transitions to a new state discrepancy in reliability between the two modalities. Con-
s , which is calculated through P(s |s ,a ), where the sequently, the queried samples are meticulously categorized
t t t−1 t−1
transitionisinformedbys ∈S anda ∈Aexecutedat intothefollowingdistinctgroups:
t−1 t−1
theprecedingtimestept−1. Agentsmightnothavedirect
accesstotheactualstateoftheenvironment. Instead,theyre- Gm ={(xr,xf)|(xr,xf)∈G, Fm >Fn}, (3)
i i i i i i
ceiveobservations,denotedaso ,sampledfromadistribution
t where m,n ∈ {r,f}, m ̸= n, and Gm denotes the group
P(o | s ) that depends on the true environmental state s .
t t t dominated by modality m, comprising query samples for
Giventhislimitation,agentsmustbasetheiroperationsonan
whichthemodalitymexhibitshighercertaintythanmodality
inferredbeliefaboutthetruestate,sˆ,representingtheiresti-
t n in the few-shot task. Specifically, when m = f, Gm rep-
mationof s basedonthe receivedobservations. Within the
t resentstheFlow-dominantgroup. Conversely,whenm = r,
framework of a specified generative model, agents perform
Gm correspondstotheRGB-dominantgroup,indicatingthat
approximate Bayesian inference through the encoding of an
theRGBmodalityismorereliablefordistinguishingbetween
arbitrary probability distribution q(s,θ), which is optimized
querysamples.
throughtheminimizationofvariationalfreeenergyF˜:
F˜ =D (cid:0) q(s,θ)∥pΦ(o,s,θ) (cid:1) , (1) 3.4 ActiveMutualDistillation
KL
In this section, we first introduce an active mutual distilla-
where o ∈ O denotes the agent’s observations and θ ∈ Θ
tion method, which enhances the representation learning of
representsmodelparameters.
less reliable modes by utilizing task-specific discriminative
To establish the active inference model, we input the rep-
knowledge from more reliable modes. Traditional knowl-
resentationsofquerysamples{qm}M andtheprototypesof
i i=1 edgeextractionmethodstypicallyinvolveusingwell-trained
support samples {tm}N into the module. We consider the
k k=1 teachermodelstoguidestudentmodellearningthroughcon-
dominantmodalityforeachquerysampleastheonethatcan
sistencyconstraints,suchasKLdivergencecalculatedbased
reflect more task-specific discriminative features. However,
onlogits:
thedominantmodalityforeachquerysampleisnotfixed,as
thecontributionofaspecificmodalitylargelydependsonthe N
contextual information of the query and support samples in D (pt∥ps)= (cid:88) pt(cid:0) logpt−logps(cid:1) , (4)
KL i i i
eachtask. Toaddressthis,weproposetoinferthereliability
i=1
of different modalities based on modality-specific posterior
distributions. This approach aligns with the core principles wherept andps representthelogitsproducedbytheteacher
ofactiveinference,wherethemodeldynamicallyassessesthe and student models, respectively. In conventional methods,
reliabilityofdifferentmodalitiestooptimizeitspredictiveca- teacher-studentdistillationisconsistentlyappliedtoindivid-
pabilities. Foreachquerysample,themodality-specificpos- ualsamples.
teriordistributionpm i canbeformulatedas: Based on this, we define the absolute certainty cm i as the
maximum value of the modality-specific posterior distribu-
pm(k|xm)= e−ψ(q i m,tm k ) , (2) tion, which quantifies the model’s highest confidence in any
i i (cid:80)N
k′=1
e−ψ(q
i
m,tm
k′
) classpredictionformodalitym:
where k ∈ {1,...,N},m ∈ {r,f}, and q i m denotes the cm i =m k axpm i (k|xm i ), (5)
modality-specificrepresentationofthei-thquerysample,tm
k
denotes the prototype of the k-th class, and ψ is a distance where pm(k|xm) denotes the posterior distribution over
i i
measurementfunction(e.g.,Euclideandistanceinthiswork). classes for the i-th query sample in modality m. Absolute
The posterior distribution reflects the model’s belief in the certaintycmreflectsthehighestbeliefofthemodelinitspre-
i
classlabelsgiventhespecificmodality,consistentwithmini- diction for that modality. This dynamic assessment allows
mizingpredictionerrorstooptimizebeliefs. Bydynamically us to adaptively evaluate the reliability of different modali-
evaluating the reliability of each modality in the context of tiesbasedoncontextualinformationineachtask,thusbetter
the current task, the model can select the most informative accommodatingthespecificdemandsofdifferenttasks.
modalitytoreducepredictionerrors. To exploit the complementarity between different modal-
After obtaining the modal-specific posterior distribution ities and improve few-shot action recognition, we refine the
pm for the ith query sample, we use it as input to construct approachbytreatingmodelstrainedononemodalityasteach-
i
the observation space O and the state space S. The relia- ersandthosetrainedonanothermodalityasstudents. How-
bility of each modality for a given sample can be assessed ever,determiningwhichmodalityshouldactastheteacheris
by the free energy derived from Eq. (1). To elaborate, the challenging, as the contribution of each modality varies be-
modality with the lower free energy value would be consid- tween samples and depends heavily on the contextual infor-
eredmorereliableaccordingtothecalculationsbasedonthe mation of the few-shot task. To address this, we previously
providedformula. Thisapproachenablesaquantitativecom- introduced active inference to dynamically infer the impor-
parison between different modalities, facilitating a more ac- tance of different modalities for each sample. We actively
curatedeterminationoftheirrespectivereliabilities. Inorder assignmorereliablemodalitiesasteacherstotransferknowl-
tosystematicallyinvestigatethecross-modalcomplementar- edge.Specifically,weconstrainthelearningoftwomodality-
ity,aselectioncriterionisestablishedbasedonthesignificant specific models by actively transferring query-to-prototype
relationshipknowledgebetweendifferentmodalities: The parameters of the modality-specific backbone net-
works θm,m ∈ {r,f} are optimized via distinct weighted
1 (cid:88)
L (θn)= cmD (pm,pn), (6) combinationsoftheircorrespondinglosses:
m→n (cid:80) cm i KL i i
(xn i ,xm i ) i (xn i ,xm i ) θm ←θm−γ∇ θm(Lm ce (θm)+λ (cid:88) Ln →Lm(θm)).
wherepmdenotesthemodality-specificposteriordistribution n̸=m∈{r,f}
i (11)
forthei-thquerysampleinmodalitym,asdefinedinEq.(2),
Theparameterupdateforeachmodalityθm isperformedby
cmrepresentstheabsolutecertaintyforthei-thquerysample
i minimizing the cross-entropy loss Lm(θm) and the knowl-
inmodalitym,asdefinedinEq. (5),andm,n∈{r,f}with ce
edge distillation loss from the other modality n ̸= m. The
m̸=n.
learningrateisdenotedbyγ,andλbalancesthecontribution
3.5 ActiveMultimodalInference ofthedistillationlosses.
In the first two sub-sections, we explored how different
4 Experiments
modalitiesofeachsamplecontributetometa-traininganddis-
cussedtheirroleinthemeta-testphase.Specifically,weintro- 4.1 ValidationProtocol
duceamethodforadaptivelyintegratingmultimodalpredic- Datasets. Weassessourproposedmethodonfourprominent
tionstoformthefinaldecisioninfew-shotinference. Given and challenging benchmarks for few-shot action recogni-
the varying reliability of modalities for each query sample, tion: Kinetics-400[Kayetal.,2017],Something-Something
wedesignanadaptivemultimodalfusionstrategyasfollows: V2 [Goyal et al., 2017], HMDB51 [Wang et al., 2015], and
UCF101 [Peng et al., 2018]. These datasets are augmented
P(k|xm)=
e−αr
i
ψ(q
i
r,tr
k
)−αf
i
ψ(q
i
f,tf
k
)
, (7) toincludemulti-modaldatabygeneratingopticalflowframe
i (cid:80)N
k′=1
e−αr
i
ψ(q
i
r,tr
k′
)−αf
i
ψ(q
i
f,tf
k′
) s
a
e
lg
q
o
u
r
e
i
n
th
c
m
es
.
fromtheoriginalvideosusingadenseopticalflow
where ψ(·) represents the Euclidean distance function, and Modality-specific Backbones. For the RGB modality, a
αr and αf are the adaptive fusion weights for the RGB and pre-trainedResNet-50backboneisemployedtoextractvisual
i i
opticalflowmodalitiesofthei-thquerysample,respectively. featuresattheframelevel. Fortheopticalflowmodality,an
Sincemodality-specificposteriordistributionsmaynotal- I3Dmodelpre-trainedontheCharadesdatasetisusedtocap-
ways be accurate during meta-tests, and relative certainty ture motion features from individual frames. Subsequently,
doesnotdirectlyreflectthesimilaritybetweenquerysamples video-level features for both modalities are derived by ag-
andclassprototypes,weusetheabsolutecertaintyvaluescm gregating these enhanced frame-level features. Ultimately,
i
tocomputetheadaptivefusionweights: thequery-specificprototypeisgeneratedbyconsolidatingthe
video-level features from the support samples of the corre-
cm
αm = i , (8) spondingactionclass.
i cr+cf Parameters. In the meta-training phase, the balance
i i
weight (λ, specified in Eq. 9) is uniformly set to 1.0 across
wherem ∈{r,f}indicatesthemodality,whererrepresents all benchmarks. Training is conducted using the SGD op-
theRGBmodalityandf representstheopticalflowmodality. timizer. For both RGB and optical flow modalities, the re-
spective networks are iteratively updated by minimizing a
3.6 Optimization
combinedweightedlossfunction,whichincludesbothcross-
The proposed AFMIR can be optimized with the following entropyanddistillationlosses,untilconvergenceisachieved.
function: Thelearningrateγ issetas10−3.
L= (cid:88) Lm(θm)+λ (cid:88) Lm →Ln(θn), (9)
ce
m∈{r,f} m̸=n∈{r,f}
where Lm(θm) denotes the cross-entropy loss for modality
ce
m, and Lm → Ln(θn) represents the knowledge distilla-
tion loss from modality m to modality n. The parameter λ
balances the contribution of the distillation losses. Besides,
theLm(θm)isfurtherusedtoconstrainthemodality-specific
ce
predictions,i.e.,
M N
(cid:88)(cid:88) (a) SSv2 (b) Kinetics-400
Lm(θm)= pm(k)logpm(k), (10)
ce i i
i=1k=1
Figure2:Thechangeoffreeenergy.
where m ∈ {r,f} indicates the modality, r represents the
RGB modality and f represents the optical flow modality.
Thecross-entropylossLm(θm)iscomputedforeachmodal- 4.2 FreeEnergy
ce
ity based on the predicted probabilities pm(k) of the query Theexperimentalresultsshowthat,ontheSSv2andKinetics-
i
samplesbelongingtoeachclassk. 400datasets,thereisasignificantfluctuationintheinitialfree
Kinetics SSv2 HMDB51 UCF101
Modality Method
1-shot5-shot 1-shot5-shot 1-shot5-shot 1-shot5-shot
MatchingNet 53.3 78.9 - - - - - -
TRX 63.6 85.9 42.0 64.6 - 75.6 - 96.1
RGB
HyRSM 73.7 86.1 54.3 69.0 60.3 78.0 83.9 94.7
STRM - 86.7 - 68.1 - 77.3 - 96.9
ProtoNet-F 45.2 69.5 32.9 51.1 43.7 65.0 69.7 89.6
Flow TRX-F 44.8 69.7 30.7 52.4 43.0 67.6 65.6 90.6
STRM-F 47.8 69.7 36.3 55.7 52.2 67.9 79.7 91.6
ProtoNet-EC 63.8 84.1 33.0 49.5 56.9 73.8 78.3 93.9
ProtoNet-EA 61.7 83.9 31.1 50.5 53.2 46.3 76.7 94.3
Multimodal STRM-EC 68.3 87.4 45.5 66.7 59.3 78.3 87.4 96.3
AFMAR 80.1 92.6 61.7 79.5 73.9 87.8 91.2 99.0
AFMIR(ours) 82.8 96.1 70.6 92.3 83.7 90.0 94.9 99.1
Table 1. Comparison with state-of-the-art few-shot action recognition methods. The best results are in bold. For multi-modal methods
extended from existing unimodal methods, “EC” represents cascaded early fusion schemes, “EA” represents collaborative attention early
fusion.The‘-’indicatesthattheresultisnotavailableinpublishedworks.
energyduringtraining. However, astrainingprogresses, the (ASI)intheframework,whichdynamicallyselectsthemost
free energy gradually decreases and tends to stabilize, even- reliable mode to reduce uncertainty, the active mutual dis-
tually converging to around -4.0 on both datasets (as shown tillation module (AMD) improves the representation ability
inFigure2). Thistrendindicatesthatthemodeldynamically of unreliable modes through bidirectional knowledge distil-
evaluatesmodalreliabilitythroughactiveinferencemodules, lation, and the Active Multimodal Inference module (AMI)
prioritizingtheselectionofmodeswithlowerfreeenergyfor optimizing the complementarity of modes through adaptive
inference, which significantly optimizes the modal selection fusion,thusfullyutilizingthepotentialofmultimodaldata.
strategy. Inaddition,theknowledgedistillationanddynamic
weighting mechanisms between multiple modalities further ASI Kinetics SSv2
AMD AMI
reducetheuncertaintyofmodalselection,enhancetaskadap- RD FD 1-shot 5-shot 1-shot 5-shot
tationcapability,andimprovethestabilityofcross-modalin- × ✓ ✓ ✓ 64.10 68.94 58.03 66.81
✓ × ✓ ✓ 59.63 75.27 62.91 66.06
ference. Theseresultsvalidatethecrucialroleofactiveinfer-
✓ ✓ × ✓ 71.64 89.05 61.81 83.37
ence and multimodal interaction in few-shot action recogni-
✓ ✓ ✓ × 59.34 65.78 62.59 85.33
tion, enablingthemodeltomoreefficientlyandreliablyuti- ✓ ✓ ✓ ✓ 82.85 96.11 70.59 92.32
lizemodalinformationtocompletetasks.
Table2. Comparisonofresultsacrossdifferentconfigurations. The
4.3 ComparativeExperiments
bestareinbold.
WeselectedfourRGB-basedalgorithms: MatchingNet[Zhu
andYang,2018],TRX[Perrettetal.,2021],HyRSM[Wang
etal.,2022],andSTRM[Thatipellietal.,2022].Thesemeth- 4.4 AblationStudy
ods leverage metric learning, temporal relationships, spa- Theimpactofkeycomponents. Theexperimentalresultsin
tiotemporal modeling, and hybrid relationships to advance Table2havebeenvalidatedthroughablationstudies,demon-
few-shotactionrecognition.Weretrainedvision-basedmeth- strating that the complete framework has achieved state-of-
ods like TRX-F and STRM-F for optical flow data due to the-art performance on both Kinetics and SSv2. When the
the lack of existing methods. We extended visual methods ASI module uses only RGB as the dominant modality (RD)
for multimodal baselines via early fusion (EC, EA) and late or only optical flow as the dominant modality, the accuracy
fusion (LF). Additionally, the chosen AFMAR [Wanyan et willsignificantlydecrease. RemovingAMDwillreducethe
al., 2023] algorithm enhances few-shot recognition by ac- thermalaccuracyofKinetics5by7.06%.Comparedwithdy-
tivelyselectingreliablemodalities,distillingknowledgebidi- namics,disablingAMIresultedinagreaterdecreaseinSSv2,
rectionally,andadaptivelyfusingmultimodaldata. indicatingthenecessityoftime-sensitiveadaptivefusion.
As shown in Table 1, our AMFIR framework signifi-
4.5 FurtherRemarks
cantly outperforms existing methods in Few Shot Action
Recognition tasks on datasets such as SSv2, HMDB51, Performance with Different Numbers of Support Sam-
UCF101, and Kinetics-400. The accuracy of 1-shot and ples. The experimental results in Figure 3 show that the
5-shot tasks reaches 70.6% and 92.3% (SSv2), 83.7% and performance of AMFIR steadily improves as the number of
90.0%(HMDB51),94.9%and99.1%(UCF101),and82.8% support samples changes from 1-shot to 5-shot. The accu-
and96.1%(Kinetics-400),respectively.Thisoutstandingper- racyontheSSv2andKineticsdatasetsincreasesfromabout
formanceisattributedtotheactivesampleinferencemodule 70% and 82% to about 92% and 96%, respectively, which
is significantly better than other methods, especially under
few-shot conditions. The reason for these results is that the
ASI module reduces uncertainty by dynamically selecting
reliable modalities, enabling the model to have high initial
performance at 1-shot. The AMD module utilizes bidirec-
tionalknowledgedistillationtoenhancethecollaborativeef-
fectbetweenmodalities,supportingfurtherperformanceim-
provement as the sample size increases. The AMI module
adaptively adjusts modal weights and optimizes the integra-
tion strategy of multimodal data. The synergistic effect of
(a) SSv2 (b) Kinetics-400
these modules fully utilizes the complementarity of multi-
modaldata,resultingintheexcellentperformanceofAMFIR
Figure4:Comparisonwithconventionaldistillationstrategiesina5-
inbothfew-shotandmulti-samplescenarios.
way1-shotsetting.T-RGB(orT-Flow)denotesdistillation,whereas
RGB(opticalflow)isconsistentlyregardedastheteacher.
exhibitstrongrobustnessandgeneralization,evenwhentask
complexity increases, verifying its excellent performance in
small-samplemulti-classificationtasks.
(a) SSv2 (b) Kinetics
Figure 3: Comparison results with different numbers of support
samplesin5-wayK-shotsetting.
InfluenceofDifferentDistillationStrategies. Theexper-
imentalresultsinFigure4showthatthebidirectionaldistil- (a) SSv2 (b) Kinetics
lation strategy (AMFIR) achieved accuracies of 70.6% and
82.8%inthe5-way1-shottaskontheSSv2andKinetics-400
Figure5:N-way1-shotperformanceonSSv2andKinetics.
datasets, respectively, significantly outperforming the unidi-
rectionaldistillationstrategy(T-RGBandT-Flow)andtheno-
distillationstrategy(NoDistillation). Thereasonisthatbidi- 5 Conclusion
rectionaldistillationeffectivelytransmitstask-relatedknowl-
We have proposed the Active Multimodal Few-Shot Infer-
edge of reliable modalities through the mechanism of mu-
ence for Action Recognition (AMFIR) framework that sig-
tual teaching, enhances the representation ability of unreli-
nificantly enhances few-shot action recognition by actively
able modalities, and further optimizes the learning effect of
identifyingandutilizingthemostreliablemodalitiesforeach
reliable modalities. In addition, this strategy fully utilizes
sample.Byintegratingactivemutualdistillationandadaptive
the complementarity between RGB and optical flow modal-
multimodal inference, AMFIR effectively improves the rep-
ities and combines active inference to dynamically evaluate
resentationlearningofunreliablemodalitiesandoutperforms
themodalreliability, flexiblyoptimizingthedistillationpro-
existing methods across multiple benchmarks. This frame-
cess. Thisdesigndemonstratesthesignificantadvantagesof
work highlights the potential of active inference and knowl-
bidirectionaldistillationinfew-shotmultimodallearning.
edge distillation in advancing multimodal few-shot learning
N-way Few-Shot Classification. In the N-way Few-Shot
through uncertainty-driven modality selection, bidirectional
Classificationtask, theAMFIRframeworksignificantlyout-
knowledgetransfer,andcontext-awarefusion. Extensiveex-
performsexistingmethodssuchasSTRM,STRM-F,STRM-
periments validate its robustness in handling sensor noise,
LF,andAMFARontheSSv2andKineticsdatasets,achieving
motionambiguity,andextremedatascarcitywhilemaintain-
thebestaccuracyin5-wayto10-wayclassificationtasks.The
ingcomputationalefficiency.
experimentalresultsareshowninFigure5. Inthe5-waytask
oftheSSv2dataset,AMFIRachievedanaccuracyofaround
Acknowledgements
70%, while other methods were below 60%. In the Kinet-
icsdataset,AMFIRstillachievedanaccuracyofaround70% This study is funded in part by the National Key Research
inthe10-waytask,significantlyaheadofothermethods. Its and Development Plan: 2019YFB2101900, NSFC (Natu-
advantages are mainly due to the complementarity between ral Science Foundation of China): 61602345, 62002263,
RGB and optical flow modalities, the dynamic adaptive ad- 62302333, TianKai Higher Education Innovation Park En-
justmentoftheactiveinferencemodule,andtheenhancement terprise R&D Special Project: 23YFZXYC00046, and 2024
oftherepresentationofunreliablemodalitiesbythebidirec- China University Industry-Academia-Research Innovation
tionaldistillationstrategy.Thisenablestheframeworktostill Fund: 2024HY015.
References Yianilos, Moritz Mueller-Freitag, et al. The” something
something”videodatabaseforlearningandevaluatingvi-
[Batenietal.,2020] Peyman Bateni, Raghav Goyal, Vaden
sualcommonsense. InProceedingsoftheIEEEinterna-
Masrani, Frank Wood, and Leonid Sigal. Improved few-
tional conference on computer vision, pages 5842–5850,
shotvisualclassification.InProceedingsoftheIEEE/CVF
2017.
conference on computer vision and pattern recognition,
pages14493–14502,2020. [Guptaetal.,2016] Saurabh Gupta, Judy Hoffman, and Ji-
tendra Malik. Cross modal distillation for supervision
[Caoetal.,2020] Kaidi Cao, Jingwei Ji, Zhangjie Cao,
transfer. In Proceedings of the IEEE conference on com-
Chien-YiChang,andJuanCarlosNiebles.Few-shotvideo
puter vision and pattern recognition, pages 2827–2836,
classification via temporal alignment. In Proceedings of
2016.
theIEEE/CVFconferenceoncomputervisionandpattern
recognition,pages10618–10627,2020. [Hinton,2015] GeoffreyHinton. Distillingtheknowledgein
aneuralnetwork. arXivpreprintarXiv:1503.02531,2015.
[Crastoetal.,2019] Nieves Crasto, Philippe Weinzaepfel,
[Kayetal.,2017] Will Kay, Joao Carreira, Karen Si-
Karteek Alahari, and Cordelia Schmid. Mars: Motion-
augmentedrgbstreamforactionrecognition. InProceed- monyan, Brian Zhang, Chloe Hillier, Sudheendra Vijaya-
ingsoftheIEEE/CVFconferenceoncomputervisionand narasimhan, Fabio Viola, Tim Green, Trevor Back, Paul
patternrecognition,pages7882–7891,2019. Natsev, et al. The kinetics human action video dataset.
arXivpreprintarXiv:1705.06950,2017.
[Daietal.,2021] Rui Dai, Srijan Das, and Franc¸ois Bre-
[KumarDwivedietal.,2019] Sai Kumar Dwivedi, Vikram
mond. Learning an augmented rgb representation with
Gupta,RahulMitra,ShuaibAhmed,andArjunJain. Pro-
cross-modal knowledge distillation for action detection.
togan: Towards few shot learning for action recognition.
InProceedingsoftheIEEE/CVFInternationalConference
InProceedingsoftheIEEE/CVFinternationalconference
onComputerVision,pages13053–13064,2021.
oncomputervisionworkshops,pages0–0,2019.
[Dongetal.,2018] Xuanyi Dong, Linchao Zhu, De Zhang,
[Liuetal.,2020] WeideLiu,ChiZhang,GuoshengLin,and
YiYang,andFeiWu. Fastparameteradaptationforfew-
FayaoLiu. Crnet: Cross-referencenetworksforfew-shot
shot image captioning and visual question answering. In
segmentation.InProceedingsoftheIEEE/CVFconference
Proceedingsofthe26thACMinternationalconferenceon
oncomputervisionandpatternrecognition, pages4165–
Multimedia,pages54–62,2018.
4173,2020.
[Fanetal.,2020] Qi Fan, Wei Zhuo, Chi-Keung Tang, and
[Maetal.,2024a] Fei Ma, Yukan Li, Yifan Xie, Ying He,
Yu-Wing Tai. Few-shot object detection with attention-
Yi Zhang, Hongwei Ren, Zhou Liu, Wei Yao, Fuji Ren,
rpn and multi-relation detector. In Proceedings of the
Fei Richard Yu, et al. A review of human emotion syn-
IEEE/CVF conference on computer vision and pattern
thesis based on generative technology. arXiv preprint
recognition,pages4013–4022,2020.
arXiv:2412.07116,2024.
[Fengetal.,2024] WeijiaFeng,RuojiaZhang,YichenZhu,
[Maetal.,2024b] FeiMa,YuchengYuan,YifanXie,Hong-
Chenyang Wang, Chuan Sun, Xiaoqiang Zhu, Xiang Li,
wei Ren, Ivan Liu, Ying He, Fuji Ren, Fei Richard Yu,
andTarikTaleb. Exploringcollaborativediffusionmodel
andShiguangNi. Generativetechnologyforhumanemo-
inferring for aigc-enabled edge services. IEEE Trans-
tion recognition: A scoping review. Information Fusion,
actions on Cognitive Communications and Networking,
page102753,2024.
2024.
[Maetal.,2024c] Huiying Ma, Dongxiao He, Xiaobao
[Fengetal.,2025] Weijia Feng, Xinyu Zuo, Ruojia Zhang, Wang,DiJin,MengGe,andLongbiaoWang.Multi-modal
Yichen Zhu, Chenyang Wang, Jia Guo, and Chuan Sun. sarcasmdetectionbasedondualgenerativeprocesses. In
Federated deep reinforcement learning for multimodal Proceedings of the Thirty-Third International Joint Con-
content caching in edge-cloud networks. IEEE Transac- ferenceonArtificialIntelligence,pages2279–2287,2024.
tionsonNetworkScienceandEngineering,2025.
[Pahdeetal.,2019] Frederik Pahde, Oleksiy Ostapenko,
[Fuetal.,2020] YuqianFu,LiZhang,JunkeWang,Yanwei Patrick Ja¨ Hnichen, Tassilo Klein, and Moin Nabi. Self-
Fu, and Yu-Gang Jiang. Depth guided adaptive meta- paced adversarial training for multimodal few-shot learn-
fusion network for few-shot video recognition. In Pro- ing. In2019IEEEWinterConferenceonApplicationsof
ceedings of the 28th ACM International Conference on ComputerVision(WACV),pages218–226.IEEE,2019.
Multimedia,pages1142–1151,2020.
[Parketal.,2019] WonpyoPark, DongjuKim, YanLu, and
[Garciaetal.,2018] Nuno C Garcia, Pietro Morerio, and MinsuCho.Relationalknowledgedistillation.InProceed-
Vittorio Murino. Modality distillation with multiple ingsoftheIEEE/CVFconferenceoncomputervisionand
stream networks for action recognition. In Proceedings patternrecognition,pages3967–3976,2019.
oftheEuropeanConferenceonComputerVision(ECCV),
[Pengetal.,2018] YuxinPeng,YunzhenZhao,andJunchao
pages103–118,2018.
Zhang. Two-stream collaborative learning with spatial-
[Goyaletal.,2017] RaghavGoyal,SamiraEbrahimiKahou, temporal attention for video classification. IEEE Trans-
Vincent Michalski, Joanna Materzynska, Susanne West- actions on Circuits and Systems for Video Technology,
phal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter 29(3):773–786,2018.
[Perrettetal.,2021] TobyPerrett, AlessandroMasullo, Tilo [Wangetal.,2024b] Mengmeng Wang, Jiazheng Xing,
Burghardt,MajidMirmehdi,andDimaDamen.Temporal- Boyuan Jiang, Jun Chen, Jianbiao Mei, Xingxing Zuo,
relational crosstransformers for few-shot action recogni- GuangDai,JingdongWang,andYongLiu.Amultimodal,
tion. In Proceedings of the IEEE/CVF conference on multi-task adapting framework for video action recogni-
computer vision and pattern recognition, pages 475–484, tion. InProceedingsoftheAAAIConferenceonArtificial
2021. Intelligence,volume38,pages5517–5525,2024.
[Pezzuloetal.,2023] Giovanni Pezzulo, Thomas Parr, Paul [Wangetal.,2025] XiaobaoWang,YujingWang,Dongxiao
Cisek, Andy Clark, and Karl Friston. Generating mean- He,ZheYu,YawenLi,LongbiaoWang,JianwuDang,and
ing:Activeinferenceandgenerativeai. PsyArXiv.June,8, DiJin. Elevatingknowledge-enhancedentityandrelation-
2023. shipunderstandingforsarcasmdetection. IEEETransac-
[Sedlaketal.,2024] Boris Sedlak, Victor Casamayor Pu- tionsonKnowledgeandDataEngineering,2025.
jol, Andrea Morichetta, Praveen Kumar Donta, and [Wanyanetal.,2023] Yuyang Wanyan, Xiaoshan Yang,
Schahram Dustdar. Adaptive stream processing on Chaofan Chen, and Changsheng Xu. Active exploration
edge devices through active inference. arXiv preprint ofmultimodalcomplementarityforfew-shotactionrecog-
arXiv:2409.17937,2024. nition. In Proceedings of the IEEE/CVF Conference on
[Thatipellietal.,2022] AnirudhThatipelli,SanathNarayan, Computer Vision and Pattern Recognition, pages 6492–
Salman Khan, Rao Muhammad Anwer, Fahad Shahbaz 6502,2023.
Khan, and Bernard Ghanem. Spatio-temporal relation [Wuetal.,2022] Jiamin Wu, Tianzhu Zhang, Zhe Zhang,
modelingforfew-shotactionrecognition. InProceedings FengWu,andYongdongZhang. Motion-modulatedtem-
oftheIEEE/CVFConferenceonComputerVisionandPat- poral fragment alignment network for few-shot action
ternRecognition,pages19958–19967,2022. recognition. InProceedingsoftheIEEE/CVFConference
[Tschantzetal.,2020] AlexanderTschantz,BerenMillidge, onComputerVisionandPatternRecognition,pages9151–
Anil K Seth, and Christopher L Buckley. Reinforce- 9160,2022.
ment learning through active inference. arXiv preprint [Wuetal.,2025] Sheng Wu, Dongxiao He, Xiaobao Wang,
arXiv:2002.12636,2020. LongbiaoWang,andJianwuDang. Enrichingmultimodal
[Tsimpoukellietal.,2021] Maria Tsimpoukelli, Jacob L sentimentanalysisthroughtextualemotionaldescriptions
Menick,SerkanCabi,SMEslami,OriolVinyals,andFelix ofvisual-audiocontent. InProceedingsoftheAAAICon-
Hill. Multimodalfew-shotlearningwithfrozenlanguage ferenceonArtificialIntelligence,volume39,pages1601–
models. AdvancesinNeuralInformationProcessingSys- 1609,2025.
tems,34:200–212,2021. [Xueetal.,2024] Haiwei Xue, Xiangyang Luo, Zhanghao
[TungandMori,2019] Frederick Tung and Greg Mori. Hu,XinZhang,XunzhiXiang,YuqinDai,JianzhuangLiu,
Similarity-preservingknowledgedistillation. InProceed- ZhensongZhang,MingleiLi,JianYang,etal.Humanmo-
ings of the IEEE/CVF international conference on com- tionvideogeneration:Asurvey.AuthoreaPreprints,2024.
putervision,pages1365–1374,2019. [Yangetal.,2023] Taojiannan Yang, Yi Zhu, Yusheng Xie,
[Wangetal.,2015] Limin Wang, Yu Qiao, and Xiaoou Aston Zhang, Chen Chen, and Mu Li. Aim: Adapting
Tang. Action recognition with trajectory-pooled deep- imagemodelsforefficientvideoactionrecognition. arXiv
convolutionaldescriptors.InProceedingsoftheIEEEcon- preprintarXiv:2302.03024,2023.
ferenceoncomputervisionandpatternrecognition,pages [Zhangetal.,2020] HongguangZhang,LiZhang,Xiaojuan
4305–4314,2015.
Qi,HongdongLi,PhilipHSTorr,andPiotrKoniusz.Few-
[Wangetal.,2022] Xiang Wang, Shiwei Zhang, Zhiwu shot action recognition with permutation-invariant atten-
Qing, Mingqian Tang, Zhengrong Zuo, Changxin Gao, tion. In Computer Vision–ECCV 2020: 16th European
Rong Jin, and Nong Sang. Hybrid relation guided set Conference,Glasgow,UK,August23–28,2020,Proceed-
matchingforfew-shotactionrecognition. InProceedings ings,PartV16,pages525–542.Springer,2020.
oftheIEEE/CVFconferenceoncomputervisionandpat- [Zhengetal.,2024] Ruijie Zheng, Yongyuan Liang, Xiyao
ternrecognition,pages19948–19957,2022.
Wang, Shuang Ma, Hal Daume´ III, Huazhe Xu, John
[Wangetal.,2023] Jianing Wang, Linhao Li, Yichen Liu, Langford,PraveenPalanisamy,KalyanShankarBasu,and
Jinyu Hu, Xiao Xiao, and Bo Liu. Ai-tfnet: Active in- FurongHuang. Premier-tacoisafew-shotpolicylearner:
ference transfer convolutional fusion network for hyper- Pretraining multitask representation via temporal action-
spectralimageclassification.RemoteSensing,15(5):1292, driven contrastive loss. In Forty-first International Con-
2023. ferenceonMachineLearning,2024.
[Wangetal.,2024a] Chenyang Wang, Hao Yu, Xiuhua Li, [ZhuandYang,2018] LinchaoZhuandYiYang.Compound
Fei Ma, Xiaofei Wang, Tarik Taleb, and Victor CM Le- memory networks for few-shot video classification. In
ung. Dependency-aware microservice deployment for Proceedings of the European conference on computer vi-
edgecomputing: Adeepreinforcementlearningapproach sion(ECCV),pages751–766,2018.
with network representation. IEEE Transactions on Mo-
bileComputing,2024.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Active Multimodal Distillation for Few-shot Action Recognition"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
