=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: ALGO: Object-Grounded Visual Commonsense Reasoning for Open-World Egocentric Action Recognition
Citation Key: kundu2024algo
Authors: Sanjoy Kundu, Shubham Trehan, Sathyanarayanan N. Aakur

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2024

Key Terms: world, action, grounded, object, auburnuniversity, reasoning, egocentric, recognition, visual, open

=== FULL PAPER TEXT ===

ALGO: Object-Grounded Visual Commonsense Reasoning for Open-World
Egocentric Action Recognition
SanjoyKundu ShubhamTrehan SathyanarayananNAakur
AuburnUniversity AuburnUniversity AuburnUniversity
Auburn,Alabama,USA Auburn,Alabama,USA Auburn,Alabama,USA
szk0266@auburn.edu szt0113@auburn.edu san0028@auburn.edu
Abstract egocentric activity understanding. We define an activity
as a complex structure whose semantics are expressed by
Learning to infer labels in an open world, i.e., in an a combination of actions (verbs) and objects (nouns). To
environment where the target “labels” are unknown, is an recognize an activity, one must be cognizant of the object
important characteristic for achieving autonomy. Founda- label, action label, and the possibility of any combination
tionmodelspre-trainedonenormousamountsofdatahave sincenotallactionsareplausibleforanobject. Supervised
shownremarkablegeneralizationskillsthroughprompting, learning approaches [10, 19, 24, 28] have been the domi-
particularly in zero-shot inference. However, their perfor- nantapproachtoactivityunderstandingbutaretrainedina
mance is restricted to the correctness of the target label’s “closed”world,wherethereisanimplicitassumptionabout
searchspace.Inanopenworld,thistargetsearchspacecan the target labels. The videos during inference will always
beunknownorexceptionallylarge,whichseverelyrestricts belong to the label space seen during training. Zero-shot
theperformanceofsuchmodels. Totacklethischallenging learning approaches [5, 18, 33, 34] relax this assumption
problem, we propose a neuro-symbolic framework called by considering disjoint “seen” and “unseen” label spaces
ALGO - Action Learning with Grounded Object recogni- wherealllabelsarenotnecessarilyrepresentedinthetrain-
tion that uses symbolic knowledge stored in large-scale ingdata. Thissetupisaknownworld, wherethetargetla-
knowledge bases to infer activities in egocentric videos belsarepre-definedandawareduringtraining.Inthiswork,
withlimitedsupervisionusingtwosteps. First,wepropose we define an open world to be one where the target labels
a neuro-symbolic prompting approach that uses object- are unknown during both training and inference. The goal
centricvision-languagemodelsasanoisyoracletoground is to recognize elementary concepts and infer the activity.
objectsinthevideothroughevidence-basedreasoning.Sec- Weproposetotacklethisproblemusinganeuro-symbolic
ond,drivenbypriorcommonsenseknowledge,wediscover frameworkthatleveragesadvancesinmulti-modalfounda-
plausibleactivitiesthroughanenergy-basedsymbolicpat- tion models to ground concepts from symbolic knowledge
tern theory framework and learn to ground knowledge- bases, such as ConceptNet [26], in visual data. The over-
basedaction(verb)conceptsinthevideo. Extensiveexper- allapproachisshowninFigure1. Usingtheenergy-based
imentsonfourpubliclyavailabledatasets(EPIC-Kitchens, pattern theory formalism [2, 4, 12] to represent symbolic
GTEA Gaze, GTEA Gaze Plus) demonstrate its perfor- knowledge, wegroundobjects(nouns)usingCLIP[22]as
manceonopen-worldactivityinference. anoisyoracle. Drivenbypriorknowledge,novelactivities
(verb+noun)areinferred,andtheassociatedaction(verb)is
groundedinthevideotolearnvisual-semanticassociations
fornovel,unseenactions.
1.Introduction
The contributions of this work are three-fold: (i) We
Humans display a remarkable ability to recognize unseen present a neuro-symbolic framework to leverage composi-
concepts (actions, objects, etc.) by associating known tional properties of objects to prompt CLIP for evidence-
concepts gained through prior experience and reasoning based grounding. (ii) We propose object-driven activity
over their attributes. Key to this ability is the notion discovery as a mechanism to reason over prior knowledge
of “grounded” reasoning, where abstract concepts can be and provide action-object affinities to constrain the search
mappedtotheperceivedsensorysignalstoprovideevidence space. (iii) We demonstrate that the inferred activities can
to confirm or reject hypotheses. In this work, we aim to be used to ground unseen actions (verbs) from symbolic
create a computational framework that tackles open-world knowledge in egocentric videos, which can generalize to
4202
nuJ
9
]VC.sc[
1v22750.6042:viXra
Gaze-driven ROI Selection
Concept Search Space
Noisy Grounding Model
(CLIP)
stcejbO biscuit, bowl, bread, broccoli, carrot, cereal, cheese, chocolate, coke,
cup, Fanta, fork, fork box, honey, jam, ketchup, knife, pepper, burner,
container, drawer, egg, freezer, fridge, microwave, milk, ...
snoitcA
Evidence-Based
Object Grounding
close, cut, compress, crack, open, pour, put, read, take, turn, eat, cook,
spread, slice, measure, wash, dip, store, roast, wipe, clean, ...
Commonsense
Prior-driven Object-driven Concept Knowledgebase
Prompting Contextualization
Action Action-Object
Grounded Object Search Space Search Space Affinity Prior
CLIP Feature Vegetable Energy-based Pattern Update Action Visual Semantic
Theory Inference Action Grounding
IsA Prior
IsA IsA
Jalapeno Pepper Green Pepper
CLIP Feature Vegetable
IsA
IsA IsA CLIP Feature Appliance
Temporal
Jalapeno Green Pepper
IsA Smoothing
IsA AtLocation
Oven Microwave Counter Object-Driven Activity Video Feature
Top-K Activity Interpretations Discovery
oTdetaleR
CLIP Feature Counter
Pepper
AtLocation Appliance IsA IsA /
Oven Microwave
IsA IsA
HasProperty HasProperty
RelatedTo UsedFor
Take Food Video Feature Open Door
Figure1.Overallarchitectureoftheproposedapproach(ALGO)isillustratedhere.Usingatwo-stepprocess,wefirstgroundtheobjects
withinagaze-drivenROIusingCLIP[22]asanoisyoraclebeforereasoningovertheplausibleactivitiesperformedinthevideo.
unseenandunknownactionspaces. of curated pre-training data to learn semantic associations
Egocentric video analysis has been extensively ex- among concepts. Neuro-symbolic models [4, 15, 21, 30]
ploredincomputervisionliterature,havingapplicationsin show promise in reducing the increasing dependency on
virtual reality [13] and human-machine interaction. While data. We extend the idea of neuro-symbolic reasoning to
Supervised learning has been the dominant approach such addressegocentric,open-worldactivityrecognition.
as [27], [19], [29, 35], [23] along with some zero-shot
learning approaches [24, 33], KGL [4] is one of the first 2.ProposedFramework: ALGO
workstoaddresstheproblemofopen-worldunderstand-
ing. They represent knowledge elements derived from Problem Formulation.Our task is to recognize unknown
ConceptNet [26], using pattern theory [2, 9, 12]. Their activitiesinegocentricvideoswithinanopen-worldsetting.
method depends on an object detector to link objects in We aim to develop a framework that identifies elementary
a source domain before translating concepts to the target concepts,establishessemanticassociations,andeffectively
space via ConceptNet-based semantic connections. How- combinesthesetointerprettheobservedactivity. Activities
ever, this approach has drawbacks: (i) false alarms may are formed by combining concepts from two distinct sets:
arise if the initial object detector misses the object, resort- anobject(nouns)(G obj )andanaction(verbs)(G act )drawn
ing to the closest object instead, and (ii) it relies on Con- fromapredefinedsearchspace.
ceptNet for correspondences, potentially disregarding ob- Overview. Our proposed framework ALGO (Action
jects with zero corresponding probabilities. The develop- LearningwithGroundedObjectrecognition),asillustrated
ment of object-centric foundation models has enabled im- in Figure 1 tackles the problem of discovering novel ac-
pressive capabilities in zero-shot object recognition in im- tionsinanopenworld.Itstartsbyhypothesizingtheplausi-
ages, as demonstrated by CLIP [22], DeCLIP [17], and bleobjectsthroughevidence-basedobjectgrounding(Sec-
ALIGN[14]. Recentworks,suchasEGO-VLP[18],Hier- tion 2.1) by exploring prior knowledge from a symbolic
VL[5],LAVILLA[34],andCoCa[31]haveexpandedthe knowledge base. An energy-based inference mechanism
scopeofmultimodalfoundationmodelstoincludeegocen- (Section 2.2) then identifies the plausible actions on these
tric videos and have achieved impressive performance in objects. We leverage visual-semantic action grounding to
zero-shotgeneralizationwhichrequiressubstantialamounts discoveractivitieswithoutexplicitsupervisionbyemploy-
ingtoolslikeCLIP[22]andConceptNet[26],respectively. (go), its ungrounded evidence generators (g¯o), an action
i j
Knowledge Representation. We use Grenander’s pat- generator (ga), and related ungrounded generators, struc-
k
tern theory [12] to represent the knowledge, integrating turedbyagraphderivedfromConceptNet. Theenergyofa
neural and symbolic elements in a unified, energy-based configurationc isexpressedas:
i
representation. We refer the reader to Aakur et al. [2] and
E(c)=ϕ(p(go|g¯o,I ,K ))+ϕ(p(ga,go|K ))
deSouzaetal. [9]foradeeperexplorationofknowledge i j t CS k i CS (1)
representationinpatterntheory. +ϕ(p(ga|I ))
k t
2.1.Evidence-basedObjectGrounding Hence, activity inference becomes an optimization over
Equation 1 to find the configuration (or activity interpre-
The first step to assess the plausibility of object concepts
(generators {go,go,...go} ∈ G ) by grounding them in tation)withtheleastenergy.
1 2 i obj
theinputvideoV i . Groundinggathersevidencetosupport 2.3.Visual-SemanticActionGrounding
or reject a concept’s presence. To enhance object recog-
nition accuracy, we propose a neuro-symbolic mechanism In this step we aim to map inferred action verbs into a
thatleveragescompositionalpropertiesfromConceptNetto semantic embedding space provided by ConceptNet Num-
compute the likelihood of an object’s presence. This in- berbatch, using a linear projection to translate visual fea-
volvesconstructinganego-graphforeachobjectandusing tures from the video to 300-dimensional semantic vectors
CLIP to evaluate the likelihood of ungrounded generators, (R1×300). This process involves training a mapping func-
using prior knowledge to assess the presence of grounded tionψ(g i a,f V ),primarilyusingameansquarederror(MSE)
objectgenerators. loss, to ground actions recognized in the video within the
Given this set of ungrounded generators ({g¯o}∀go ∈ broadersemanticcontextofConceptNet.
i i
G ),wethenpromptCLIPtoprovidelikelihoodsforeach TemporalSmoothingWeimplementtemporalsmooth-
obj
ungrounded generator p(g¯o|I ) to compute the evidence- ingbyfirstaggregatingactionpredictionsattheframelevel.
i t
based likelihood for each grounded object generator go as For each frame, we compute the top five actions based on
defined by the probability p(go|g¯o,I ,K ) = p(go|I i )∗ theirenergylevels,thenaveragetheseacrossthecliptosta-
(cid:13) i (cid:13) i 2 t CS i t bilizethelearningprocess. Thisaggregateddataformsthe
(cid:13) (cid:13) (cid:80) ∀g¯ i o p(g i o,g¯ i o|E g i o )∗p(g¯ i o)|I t )(cid:13) (cid:13) , where p(g i o,g¯ i o|E g i o ) basisfortrainingthemappingfunctionψ(g i a,f V ),focusing
istheedgeweightfromtheedgegraphE go (sampledfrom onthemostfrequentandenergeticallyconsistentactions.
i
a knowledge graph K ) that acts as a prior for each un- Posterior-basedActivityRefinement.Thefinalstepin-
CS
grounded evidence generator g¯o, and p(g¯o)|I ) is the like- volves an iterative refinement process that updates the ac-
i i t
lihoodfromCLIPforitspresenceineachframeI . Tofo- tion concept priors based on predictions from the visual-
t
cus on relevant objects, we use the human gaze to select semantic grounding mechanism (Section 2.3). We adjust
a specific region for analysis, leveraging object-grounding theactionpriorsintheenergycomputation(Equation1),re-
insights. ranking activity interpretations to reflect clip-level dynam-
icsbetter.Therefinementcyclealternatesbetweenupdating
2.2.Object-drivenActivityDiscovery
posteriorprobabilitiesandre-trainingtheactiongrounding
The next step focuses on identifying plausible activ- modeluntilgeneralizationerrorsaturates.
ities in the video by considering object affordances
3.ExperimentalEvaluation
and the compatibility of action-object pairs using
prior knowledge. The probability of an activity (de-
Data. We evaluate the approach on GTEA Gaze [11],
fined by an action generator ga and a grounded
i GTEA GazePlus [16], and EPIC-Kitchens-100 [7, 8]
object generator go) is given by p(ga,go|K ) =
j i j CS datasets, whichcontainegocentric, multi-subjectvideosof
(cid:80)
a
E
rg
is
m
t
a
h
x
e
∀E
co
∈
l
K
le
C
c
S
tion
(g
o
¯m
f
,
a
g¯n
ll
)∈
p
E
at
w
hs
k
b
∗
e
K
tw
C
e
S
e
(
n
g¯ m
g i a
,g¯
a
n
n
)
d
.
go j
wh
in
ere
a
m
ve
e
r
a
b
l
s
p
a
r
n
e
d
pa
3
r
8
ati
n
o
o
n
u
a
n
c
s
ti
(
v
s
i
e
t
a
ie
r
s
c
.
h
T
s
h
p
e
ac
G
e
T
o
E
f
A
38
G
0
a
a
z
c
e
t
d
iv
a
i
t
t
a
ie
se
s)
t
,
h
w
as
hi
1
l
0
e
commonsenseknowledgegraphK ,w isaweightdrawn GTEA GazePlus has 15 verbs and 27 nouns (search space
CS i
from an exponential decay function based on the distance of 405), Charades-Ego has 33 verbs and 38 nouns (search
of the node g¯ from ga. After filtering for compositional space of 1254), and Epic-Kitchens has 97 verbs and 300
n i
properties, the path with the maximum weight is chosen nouns(searchspaceof29100).
withtheoptimalaction-objectaffinity. Baselines.Wecompareagainstbothclosed-worldlearn-
Energy-based Activity Inference. To infer activities, ingandopen-worldsetup(KGL)[4].Wealsocreateabase-
weassignanenergytermtoeachlabelusingconfigurations linecalled“KGL+CLIP”byaugmentingKGLwithCLIP-
composedofgeneratorsconnectedbyaffinity-basedbonds. based grounding by including CLIP’s similarity score for
Each configuration includes a grounded object generator establishing semantic correspondences. We compare with
GTEAGaze GTEAGazePlus
Approach Search VLM?
Space Object Action Activity Object Action Activity
Two-StreamCNN[25] Closed ✗ 38.05 59.54 53.08 61.87 58.65 44.89
IDT[28] Closed ✗ 45.07 75.55 40.41 53.45 66.74 51.26
ActionDecomposition[33] Closed ✗ 60.01 79.39 55.67 65.62 75.07 57.79
Random Known ✗ 3.22 7.69 2.50 3.70 4.55 2.28
ActionDecompositionZSL[33] Known ✗ 40.65 85.28 39.63 43.44 27.68 15.98
ALGOZSL(Ours) Known ✗ 49.47 74.74 27.34 47.67 29.31 16.68
KGL[4] Open ✗ 5.12 8.04 4.91 14.78 6.73 10.87
KGL+CLIP[4] Open ✗ 10.36 8.15 9.21 20.49 9.23 14.86
ALGO(Ours) Open ✗ 13.07 17.05 15.05 26.23 11.44 18.84
EgoVLP[18] Open ✓ 10.17 8.45 9.31 29.43 17.17 23.30
LaViLa[34] Open ✓ 6.07 23.07 14.57 28.27 25.47 26.87
ALGO+EgoVLP Open ✓ 8.61 4.64 6.63 20.48 20.48 20.48
ALGO+LaViLa Open ✓ 17.50 26.60 22.05 30.74 27.00 28.87
Table1. Open-worldactivityrecognitionperformanceontheGTEAGazeandGTEAGazePlusdatasets. Wecompareapproacheswith
aclosedsearchspace,thosewithaknownsearchspace,andthosewithapartiallyopenone. Accuracyisreportedforpredictedobjects,
actions,andactivities.VLM:Vision-LanguageModelpre-trainedonegocentricvideodata.*indicatestrainingon“seen”classesfromthe
samedataset(s)andleave-one-action-outevaluation.
Approach VLM? Action Object Activity 17.05% on Gaze and 11.44% on Gaze Plus, outperform-
ing KGL (8.04% and 6.73%). Adding action priors from
Random ✗ 1.03 0.33 0.68
LaViLa (ϕ(p(ga|I )) in Equation 1) allows us to improve
KGL[4] ✗ 3.89 2.56 3.23 k t
theperformancefurther,asindicatedbyALGO+LaViLa.
KGL+CLIP[4] ✗ 5.32 4.67 4.99
ALGO(Ours) ✗ 10.21 6.76 8.48 WealsoevaluateourapproachontheEpic-Kitchens-100
dataset, a larger-scale dataset with a significantly higher
EgoVLP[18] ✓ 10.77 19.51 15.14 number of concepts (actions, verbs, and activities). Ta-
LaViLa[34] ✓ 11.16 23.25 17.21 ble 2 summarizes the results. We significantly outperform
ALGO+LaViLa ✓ 12.54 22.84 17.69 non-VLM models while offering competitive performance
to the VLM-based models. We see that even without any
Table 2. Evaluation on the EPIC-Kitchens-100 dataset. VLM: video-based training data, we achieve an action accuracy
Vision-Language pre-training on egocentric data. Accuracy for of 10.21% and object accuracy of 6.76%, indicating that
actions,objects,andactivityarereported.
wecanlearnaffordance-basedrelationshipsfordiscovering
andgroundingnovelactionsinegocentricdata.
supervisedlearningmodelsaswellaszero-shotversionsof
Action Decomposition and large vision-language models,
such as EGO-VLP [18], HierVL [5], and LAVILA [34] in 4.Discussion,Limitations,andFutureWork
bothzero-shotandopen-worldsettings.
Inthiswork,weproposedALGO,aneuro-symbolicframe-
work for open-world egocentric activity recognition that
3.1.OpenWorldActivityRecognition
aims to learn novel action and activity classes without
Table 1 summarizes the evaluation results under the open- explicit supervision. While showing competitive perfor-
world inference setting. Top-1 prediction results are re- mance, there are two key limitations: (i) it is restricted
ported for all approaches. As can be seen, CLIP-based to ego-centric videos due to the need to navigate clutter
groundingsignificantlyimprovestheperformanceofobject by using human attention as a contextual cue for object
recognitionforKGL,asopposedtotheoriginallyproposed, grounding, and (ii) it requires a knowledge base such as
prior-only correspondence function. However, our neuro- ConceptNet to learn associations between actions and ob-
symbolic grounding mechanism (Section 2.1) improves it jects. In the future, we aim to explore attention-based
further, achieving an object recognition performance of mechanisms[1,20]toextendtheframeworktothird-person
13.07% on Gaze and 26.23% on Gaze Plus. Similarly, videos and using abductive reasoning [3, 32] with neural
theposterior-basedactionrefinementmodule(Section2.3) knowledgebase completion models [6] to integrate visual
helps achieve a top-1 action recognition performance of commonsenseintothereasoning.
Acknowledgements. This research was supported in formersforimagerecognitionatscale.InInternationalCon-
part by the US National Science Foundation grants IIS ferenceonLearningRepresentations. 1
2348689, andIIS2348690. WethankDr. AnujSrivastava [11] AlirezaFathi,YinLi,andJamesMRehg. Learningtorec-
(FSU) and Dr. Sudeep Sarkar (USF) for their thoughtful ognizedailyactionsusinggaze. InEuropeanConferenceon
feedbackduringthediscussionabouttheproject’sproblem ComputerVision,pages314–327.Springer,2012. 3
[12] UlfGrenander.Elementsofpatterntheory.JHUPress,1996.
formulationandexperimentalanalysisphase.
1,2,3
[13] ShangchenHan,BeibeiLiu,RandiCabezas,ChristopherD
References
Twigg,PeizhaoZhang,JeffPetkau,Tsz-HoYu,Chun-Jung
[1] SathyanarayananAakurandSudeepSarkar. Actor-centered Tai, Muzaffer Akbay, Zheng Wang, et al. Megatrack:
representations for action localization in streaming videos. monochromeegocentricarticulatedhand-trackingforvirtual
In Computer Vision–ECCV 2022: 17th European Confer- reality. ACMTransactionsonGraphics(ToG),39(4):87–1,
ence, Tel Aviv, Israel, October 23–27, 2022, Proceedings, 2020. 2
PartXXXVIII,pages70–87.Springer,2022. 4 [14] ChaoJia,YinfeiYang,YeXia,Yi-TingChen,ZaranaParekh,
HieuPham,QuocV.Le,YunhsuanSung,ZhenLi,andTom
[2] Sathyanarayanan Aakur, Fillipe de Souza, and Sudeep
Duerig.Scalingupvisualandvision-languagerepresentation
Sarkar. Generatingopenworlddescriptionsofvideousing
learningwithnoisytextsupervision,2021. 2
common sense knowledge in a pattern theory framework.
[15] JindongJiangandSungjinAhn. Generativeneurosymbolic
QuarterlyofAppliedMathematics,77(2):323–356,2019. 1,
machines. In Advances in Neural Information Processing
2,3
Systems,pages12572–12582.CurranAssociates,Inc.,2020.
[3] Sathyanarayanan N Aakur and Sudeep Sarkar. Leveraging
2
symbolic knowledge bases for commonsense natural lan-
[16] YinLi,AlirezaFathi,andJamesMRehg.Learningtopredict
guageinferenceusingpatterntheory. IEEETransactionson
gazeinegocentricvideo. InProceedingsoftheIEEEInter-
PatternAnalysisandMachineIntelligence,2023. 4
nationalConferenceonComputerVision,pages3216–3223,
[4] SathyanarayananNAakur,SanjoyKundu,andNikhilGunti.
2013. 3
Knowledge guided learning: Open world egocentric action
[17] YangguangLi,FengLiang,LichenZhao,YufengCui,Wanli
recognitionwithzerosupervision. PatternRecognitionLet-
Ouyang, Jing Shao, Fengwei Yu, and Junjie Yan. Su-
ters,156:38–45,2022. 1,2,3,4
pervision exists everywhere: A data efficient contrastive
[5] Kumar Ashutosh, Rohit Girdhar, Lorenzo Torresani, and
language-imagepre-trainingparadigm,2022. 2
Kristen Grauman. Hiervl: Learning hierarchical video-
[18] KevinQinghongLin,JinpengWang,MattiaSoldan,Michael
languageembeddings,2023. 1,2,4
Wray,RuiYan,EricZXU,DifeiGao,Rong-ChengTu,Wen-
[6] AntoineBosselut,HannahRashkin,MaartenSap,Chaitanya
zhe Zhao, Weijie Kong, et al. Egocentric video-language
Malaviya,AsliCelikyilmaz,andYejinChoi. Comet: Com-
pretraining.AdvancesinNeuralInformationProcessingSys-
monsensetransformersforautomaticknowledgegraphcon-
tems,35:7575–7586,2022. 1,2,4
struction. In Proceedings of the 57th Annual Meeting of
[19] MinghuangMa,HaoqiFan,andKrisMKitani.Goingdeeper
theAssociationforComputationalLinguistics,pages4762–
intofirst-personactivityrecognition. InIEEE/CVFConfer-
4779,2019. 4
enceonComputerVisionandPatternRecognition(CVPR),
[7] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, pages1894–1903,2016. 1,2
SanjaFidler,AntoninoFurnari,EvangelosKazakos,Davide [20] Ramy Mounir, Ahmed Shahabaz, Roman Gula, Jo¨rn
Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Theuerkauf, and Sudeep Sarkar. Towards automated
MichaelWray. Theepic-kitchensdataset: Collection,chal- ethogramming: Cognitively-inspiredeventsegmentationfor
lenges and baselines. IEEE Transactions on Pattern Anal- streamingwildlifevideomonitoring. InternationalJournal
ysisandMachineIntelligence(TPAMI),43(11):4125–4141, ofComputerVision,pages1–31,2023. 4
2021. 3
[21] MaxwellNye,MichaelTessler,JoshTenenbaum,andBren-
[8] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, denMLake.Improvingcoherenceandconsistencyinneural
Antonino Furnari, Jian Ma, Evangelos Kazakos, Davide sequencemodelswithdual-system,neuro-symbolicreason-
Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and ing. Advances in Neural Information Processing Systems,
Michael Wray. Rescaling egocentric vision: Collection, 34:25192–25204,2021. 2
pipelineandchallengesforepic-kitchens-100. International [22] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
JournalofComputerVision(IJCV),130:33–55,2022. 3 Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
[9] Fillipe DMdeSouza, Sudeep Sarkar, AnujSrivastava, and AmandaAskell,PamelaMishkin,JackClark,etal. Learn-
JingyongSu. Patterntheoryforrepresentationandinference ingtransferablevisualmodelsfromnaturallanguagesuper-
ofsemanticstructuresinvideos.PatternRecognitionLetters, vision. In International Conference on Machine Learning,
72:41–51,2016. 2,3 pages8748–8763.PMLR,2021. 1,2,3
[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, [23] Michael S. Ryoo, Brandon Rothrock, and Larry Matthies.
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Pooledmotionfeaturesforfirst-personvideos. InProceed-
MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl- ingsoftheIEEEconferenceonComputerVisionandPattern
vain Gelly, et al. An image is worth 16x16 words: Trans- Recognition(CVPR),2015. 2
[24] GunnarASigurdsson,AbhinavGupta,CordeliaSchmid,Ali
Farhadi, and Karteek Alahari. Actor and observer: Joint
modelingoffirstandthird-personvideos. InProceedingsof
theIEEEconferenceonComputerVisionandPatternRecog-
nition,pages7396–7404,2018. 1,2
[25] KarenSimonyanandAndrewZisserman. Two-streamcon-
volutional networks for action recognition in videos. Ad-
vancesinNeuralInformationProcessingSystems,27,2014.
4
[26] RobynSpeer,JoshuaChin,andCatherineHavasi. Concept-
net5.5: Anopenmultilingualgraphofgeneralknowledge.
InProceedingsoftheAAAIConferenceonArtificialIntelli-
gence,2017. 1,2,3
[27] SwathikiranSudhakaran,SergioEscalera,andOswaldLanz.
Lsta: Longshort-termattentionforegocentricactionrecog-
nition.InProceedingsoftheIEEE/CVFConferenceonCom-
puterVisionandPatternRecognition(CVPR),2019. 2
[28] HengWangandCordeliaSchmid. Actionrecognitionwith
improved trajectories. In IEEE/CVF International Confer-
enceonComputerVision(ICCV),pages3551–3558, 2013.
1,4
[29] XiaohanWang,LinchaoZhu,HengWang,andYiYang. In-
teractive prototype learning for egocentric action recogni-
tion. InProceedingsoftheIEEE/CVFInternationalConfer-
enceonComputerVision(ICCV),pages8168–8177, 2021.
2
[30] Tailin Wu, Megan Tjandrasuwita, Zhengxuan Wu, Xuelin
Yang,KevinLiu,RokSosic,andJureLeskovec. Zeroc: A
neuro-symbolicmodelforzero-shotconceptrecognitionand
acquisitionatinferencetime. InAdvancesinNeuralInfor-
mationProcessingSystems,pages9828–9840.CurranAsso-
ciates,Inc.,2022. 2
[31] JiahuiYu,ZiruiWang,VijayVasudevan,LeggYeung,Mo-
jtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive
captionersareimage-textfoundationmodels,2022. 2
[32] RowanZellers, YonatanBisk, AliFarhadi, andYejinChoi.
Fromrecognitiontocognition:Visualcommonsensereason-
ing. InProceedingsoftheIEEE/CVFConferenceonCom-
puterVisionandPatternRecognition(CVPR),2019. 4
[33] YunCZhang,YinLi,andJamesMRehg.First-personaction
decompositionandzero-shotlearning. InIEEEWinterCon-
ferenceonApplicationsofComputerVision(WACV),pages
121–129,2017. 1,2,4
[34] YueZhao,IshanMisra,PhilippKra¨henbu¨hl,andRohitGird-
har. Learning video representations from large language
models. InCVPR,2023. 1,2,4
[35] Yang Zhou, Bingbing Ni, Richang Hong, Xiaokang Yang,
and Qi Tian. Cascaded interactional targeting network for
egocentric video analysis. In Proceedings of the IEEE
conference on Computer Vision and Pattern Recognition
(CVPR),2016. 2

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "ALGO: Object-Grounded Visual Commonsense Reasoning for Open-World Egocentric Action Recognition"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
