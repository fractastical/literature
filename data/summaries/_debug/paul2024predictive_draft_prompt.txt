=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: On Predictive planning and counterfactual learning in active inference
Citation Key: paul2024predictive
Authors: Aswin Paul, Takuya Isomura, Adeel Razi

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2024

Abstract: Giventherapidadvancementofartificialintelligence,understandingthefoundationsofintelligent
behaviourisincreasinglyimportant. Activeinference,regardedasageneraltheoryofbehaviour,
offersaprincipledapproachtoprobingthebasisofsophisticationinplanninganddecision-making.
In this paper, we examine two decision-making schemes in active inference based on â€™planningâ€™
andâ€™learningfromexperienceâ€™. Furthermore,wealsointroduceamixedmodelthatnavigatesthe
data-complexity trade-off between these strategies, lever...

Key Terms: mumbai, counterfactual, planning, india, learning, data, inference, active, predictive, making

=== FULL PAPER TEXT ===

ON PREDICTIVE PLANNING AND COUNTERFACTUAL LEARNING
IN ACTIVE INFERENCE
APREPRINT
AswinPaul1,2,3,TakuyaIsomura4andAdeelRazi1,5,6
1TurnerInstituteforBrainandMentalHealth,SchoolofPsychologicalSciences,MonashUniversity,Clayton3800,Australia
2IITB-MonashResearchAcademy,Mumbai,India
3DepartmentofElectricalEngineering,IITBombay,Mumbai,India
4BrainIntelligenceTheoryUnit,RIKENCenterforBrainScience,Wako,Saitama,Japan
5WellcomeTrustCentreforHumanNeuroimaging,UniversityCollegeLondon,WC1N3ARLondon,UnitedKingdom
6CIFARAzrieliGlobalScholarsProgram,CIFAR,Toronto,Canada
March20,2024
ABSTRACT
Giventherapidadvancementofartificialintelligence,understandingthefoundationsofintelligent
behaviourisincreasinglyimportant. Activeinference,regardedasageneraltheoryofbehaviour,
offersaprincipledapproachtoprobingthebasisofsophisticationinplanninganddecision-making.
In this paper, we examine two decision-making schemes in active inference based on â€™planningâ€™
andâ€™learningfromexperienceâ€™. Furthermore,wealsointroduceamixedmodelthatnavigatesthe
data-complexity trade-off between these strategies, leveraging the strengths of both to facilitate
balanceddecision-making. Weevaluateourproposedmodelinachallenginggrid-worldscenariothat
requiresadaptabilityfromtheagent. Additionally,ourmodelprovidestheopportunitytoanalyze
theevolutionofvariousparameters,offeringvaluableinsightsandcontributingtoanexplainable
frameworkforintelligentdecision-making.
Keywords ActiveinferenceÂ·DecisionmakingÂ·Data-complexitytrade-offÂ·Hybridmodels
1 Introduction
Definingandtherebyseparatingtheintelligentâ€œagentâ€fromitsembodiedâ€œenvironmentâ€,whichthenprovidesfeedback
totheagent,iscrucialtomodelintelligentbehaviour. Popularapproaches,likereinforcementlearning(RL),heavily
employsuchmodelscontainingagent-environmentloops,whichboilsdowntheproblemtoagent(s)tryingtomaximise
rewardinthegivenuncertainenvironmentSuttonandBarto[2018].
Active inference has emerged in neuroscience as a biologically plausible framework Friston [2010], which adopts
a different approach to modelling intelligent behaviour compared to other contemporary methods like RL. In the
activeinferenceframework,anagentaccumulatesandmaximisesthemodelevidenceduringitslifetimetoperceive,
learn,andmakedecisionsDaCostaetal.[2020],Sajidetal.[2021],Millidgeetal.[2020]. However,maximisingthe
modelevidencebecomeschallengingwhentheagentencountersahighlyâ€™entropicâ€™observation(i.e. anunexpected
observation)concerningtheagentâ€™sgenerative(world)modelDaCostaetal.[2020],Sajidetal.[2021],Millidgeetal.
[2020]. Thisseeminglyintractableobjectiveofmaximisingmodelevidence(orminimisingtheentropyofencountered
observations)isachievablebyminimisinganupperboundontheentropyofobservations,calledvariationalfreeenergy
DaCostaetal.[2020],Sajidetal.[2021]. Giventhisgeneralfoundation,activeinferenceFristonetal.[2017]offers
excellentflexibilityindefiningthegenerativemodelstructureforagivenproblemandhasattractedmuchattentionin
variousdomainsKuchlingetal.[2020],Deaneetal.[2020].
Inthiswork,wedevelopanefficientdecision-makingschemebasedonactiveinferencebycombiningâ€™planningâ€™and
â€™learningfromexperienceâ€™. Afterageneralintroductiontogenerativeworldmodelsinthenextsection, wetakea
4202
raM
91
]IA.sc[
1v71421.3042:viXra
OnpredictiveplanningandcounterfactuallearninginAIF APREPRINT
closerlookatthedecision-makingaspectofactiveinference. Then,wesummarisetwodominantapproachesinactive
inferenceliterature: thefirstbasedonplanning(Section2.3.1),andthesecondbasedoncounterfactuallearning(cf.
Section2.3.2). Wecomparethecomputationalcomplexityanddataefficiency(cf. Section3.2)ofthesetwoexisting
activeinferenceschemesandproposeamixedorhybridmodelthatbalancesthesetwocomplementaryschemes(Section
3.3). Ourproposedhybridmodelnotonlyperformswellinanenvironmentthatdemandsadaptability(inSection3.5)
butalsoprovidesinsightsregardingtheexplainabilityofdecision-makingusingmodelparameters(inSection4.1).
2 Methods
2.1 Agent-environmentloopinactiveinference
Generativemodelsarecentraltoestablishingtheagent-environmentloopinanactiveinferencemodel. Theagentis
assumedtoholdascaled-downmodeloftheexternalworldthatenablestheagenttopredicttheexternaldynamics
andfutureobservations. Theagentcanthenuseitsavailableactionstopursuefutureoutcomes,ensuringsurvival. We
sticktoapartiallyobservedMarkovdecisionprocess(POMDP)basedgenerativemodelKaelblingetal.[1998]inthis
paper. POMDPsareageneralcaseofMarkovdecisionprocesses(MDPs),whicharecontrollableMarkovchainsaptfor
modellingstochasticsystemsinadiscretestatespace. Inthefollowingsection,weprovidethespecificdetailsofa
POMDP-basedgenerativemodel.
2.2 POMDP-basedgenerativemodels
Inactiveinference,agentslearnthegenerativemodelaboutexternalstatesandoptimisetheirdecisionsbyminimising
variational free energy. The POMDP is a universal framework to model discrete state-space environments, where
thelikelihoodandstatetransitionareexpressedastractablecategoricaldistributions. Thus,weadoptedthePOMDP
as our agentâ€™s generative model. The POMDP-based generative model is formally defined as a tuple of finite sets
(S,O,T,U,B,A,D,E)suchthat:
â—¦ s âˆˆS :statesands isagiveninitialstate.
t 1
â—¦ o âˆˆO :whereo =s inthefullyobservablesetting,ando =f(s )inapartiallyobservablesetting.
t t t t t
â—¦ T âˆˆN+,isafinitetimehorizonavailableperepisode.
â—¦ u âˆˆU :actions,fore.g.,U={Left,Right,Up,Down}.
t
â—¦ B :encodesone-steptransitiondynamics,suchthatP(s |s ,u ,B)istheprobabilitythatactionu
t tâˆ’1 tâˆ’1 tâˆ’1
takenatstates attimetâˆ’1resultsins attimet.
tâˆ’1 t
â—¦ A:encodesthelikelihooddistribution,P(o |s ,A)forthepartiallyobservablesetting.
t t
â—¦ D:prioraboutthestate(s)atthestartingtimepointusedfortheBayesianinferenceofstate(s)attimet=1.
â—¦ E:prioraboutaction-selectionusedtotakeactioninthesimulationsattimet=1.
InthePOMDP,hiddenstates(s)generateobservation(o)throughthelikelihoodmapping(A)intheformofacategorical
distribution,P(o |s ,A)=Cat(A). Thestatessaredeterminedbythetransitionmatrix(B)giventheagentâ€™saction
t t
(u),P(s |s ,u ,B)=Cat(B(s âŠ—u )). Thus,thegenerativemodelinquestionisgivenas:
t tâˆ’1 tâˆ’1 tâˆ’1 tâˆ’1
t t
(cid:89) (cid:89)
P(o ,s ,u )=P(A)P(B)P(D)P(E) P(o |s ,A) P(s |s ,u ,B). (1)
1:t 1:t 1:t Ï„ Ï„ Ï„ Ï„âˆ’1 Ï„âˆ’1
Ï„=1 Ï„=2
Underthemean-fieldapproximation,anapproximateposteriordistribution(concerninghidden-statess)isgivenas:
ï£« ï£¶
Q(s
t+1
)=Ïƒï£­logP(s
t+1
)+log(o
t+1
Â·As
t+1
)ï£¸, (2)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Posterior Prior Likelihood
wheretheposteriorbeliefsaboutstatesandparametersareexpressedascategoricaldistribution,Q(s )=Cat(s )and
t t
Dirichletdistribution,Q(A)=Dir(a),respectively. Hence,underthisPOMDPsetup,variationalfreeenergyisgiven
as:
2
OnpredictiveplanningandcounterfactuallearninginAIF APREPRINT
(cid:88)
F = Q(s )[logQ(s )âˆ’logP(o |s )âˆ’logP(s )]
1:t 1:t 1:t 1:t 1:t
s1:t
(cid:88)
+ Q(u )[logQ(u )âˆ’logP(u |s )]+D [Q(Î¸)||P(Î¸)]. (3)
1:t 1:t 1:t 1:t KL
u1:t
Variations of F give appropriate posterior expectations about states and parameters. Some optional parameters,
dependingonthespecificdecision-makingschemeused,are:
â—¦ C :priorpreferencesoveroutcomes,P(o|C). Here,Cisthepreferenceforthepredefinedgoalstate. This
parameterisgenerallyusedintheplanning-basedactiveinferencemodelsSajidetal.[2021],Pauletal.[2021].
â—¦ Î“(t): Atime-specificriskparameterthattheagentmaintainstoupdatethestate-actionmappingCLintheCL
schemeasinIsomuraetal.[2022].
â—¦ Î²(s,t): Astate-dependentbiasparameterusedinthemixedmodelproposedinthispaper.
Theseareusedtoparameterisethedistributionofactionsu,andactionsareoptimisedthroughvariationalfreeenergy
minimisation. Furtherdetailsareexplainedinthesubsequentsections.
2.3 Decision-makingschemesinactiveinference
Decision-makingunderactiveinferenceisformulatedasminimisingthe(expected)variationalfreeenergyoffuture
timestepsKaplanandFriston[2018],Fristonetal.[2009],Friston[2012]. Thisenablesanagenttodeployaplanning-
baseddecision-makingscheme,whereanagentpredictspossibleoutcomesandmakesdecisionstoattainstatesand
observationsthatminimiseexpectedfreeenergy(EFE).Classically,activeinferenceoptimisespoliciesâ€“i.e.,sequences
ofactionsintimeâ€“insteadofastate-actionmappinginmethodslikeQ-LearningSuttonandBarto[2018]inRLto
choosethepolicythatminimisesEFESajidetal.[2021].However,suchformulationslimitagentstosolveenvironments
onlywithlow-dimensionalstate-spaceSajidetal.[2021],Pauletal.[2021].
Severalimprovementstotheframeworkfollowed,includingtherecentsophisticatedinferenceschemeFristonetal.
[2021]thatusesarecursiveformoffreeenergytoeasethecomputationalcomplexityofpolicysearch.Thesophisticated
inferencemethodusesaforwardtreesearchintimetoevaluateEFE;however,itrestrictstheplanningdepthofagents
Fristonetal.[2021]duetocomputationalcomplexity. Moreinnovativealgorithmslikedynamicprogrammingcanbe
usedtolinearisetheplanningPauletal.[2023],DaCostaetal.[2020]. Theproposedlinearisedplanningmethodwas
calledDynamicprogramminginexpectedfreeenergy(DPEFE)inPauletal.[2023]. ThisDPEFEalgorithmperforms
atparwithbenchmarkreinforcementlearningmethodslikeDyna-QinenvironmentssimilartogridworldtasksPaul
etal.[2021](SeeSection2.3.1fortechnicaldetailsofthismethod). AgeneralisationoftheDPEFEalgorithmwas
recentlyproposedasâ€˜inductive-inferenceâ€™tomodelâ€™intentionalbehaviourâ€™inagentsFristonetal.[2023].
Anotherrecentworkdeviatesfromthisclassicalapproachofpredictiveplanningandemploysâ€œlearningfromexperienceâ€
todetermineoptimaldecisionsIsomuraetal.[2022]. Thisschemeismathematicallyequivalenttoaparticularclassof
neuralnetworksaccompaniedbysomeneuromodulationsofsynapticplasticityIsomuraandFriston[2020],Isomura
etal.[2022]. Itusescounterfactuallearning(theCLmethodinthispaper)toaccumulateameasureofâ€˜riskâ€™over
time-basedonenvironmentalfeedback. Subsequentworkthatvalidatesthisschemeexperimentallyusingin-vitroneural
networkshasalsoappearedrecentlyIsomuraetal.[2023].
Thefollowingsummarisesthecriticalalgorithmicdetailsofbothschemes: DPEFEinSec.2.3.1andCLschemein
Sec.2.3.2. BothschemesareproposedbasedonconventionalPOMDPs.
2.3.1 DPEFEschemeandactionprecision
TheDPEFEschemeinthispaperisbasedontheworkinPauletal.[2021]. ThisschemewasgeneralisedtoaPOMDP
setting in the paper Paul et al. [2023]. The model parameters used are as given in Sec.2.2. The action-perception
loopintheDPEFEschemecomprisesperception(i.e.,identifyingstatesthatcauseobservations),planning,action
selection,andlearningmodelparameters. Inthispaper,allenvironmentsarefullyobservablesinceourfocusison
decision-makingratherthanperception,henceO =S.
TheactionselectionintheDPEFEschemeisimplementedasfollows: Afterevaluatingtheexpectedfreeenergy(EFE,
G) of future observations using dynamic programming (cf. Paul et al. [2023]), the agent evaluates the probability
distributionforselectinganactionuas:
3
OnpredictiveplanningandcounterfactuallearninginAIF APREPRINT
P (u|s)=Ïƒ(âˆ’Î±G(u|s)). (4)
DPEFE
Here,Ïƒistheclassicalsoftmaxfunction,renderingactionswithsmallerEFEbeingselectedwithlargerprobabilities.
Theactionprecisionparameter(Î±)maybetunedtoincrease/decreasetheagentâ€™sactionselectionconfidence. Fora
detaileddescriptionoftheevaluationoftheEFE(G)andtheDPEFEalgorithm,refertoPauletal.[2023](Section5).
2.3.2 CLmethodandriskparameter
InsteadofattemptingtominimisetheEFEdirectly, inthecounterfactuallearning(CL)method, theagentlearnsa
state-actionmappingCL. Thisstate-actionmappingislearnedthroughanupdateequationmediatedbyaâ€™riskâ€™termÎ“
t
asdefinedinIsomuraetal.[2022]:
CLâ†CL+tâŸ¨(1âˆ’2Î“ )âŸ¨u âŠ—s âŸ©âŸ©. (5)
t t tâˆ’1
Here,âŸ¨Â·âŸ©referstotheaverageovertime,andâŠ—istheKronecker-productoperator. Giventhestate-actionmappingCL,
theagentsamplesactionsfromthedistribution,
P(u|s) =Ïƒ(ln CLÂ·s ). (6)
CL tâˆ’1
Inthesimulations,Î“ withthefollowingfunctionalformisused: Whentheagentisatthestartpositionâ€”orwhen
t
the agentâ€™s action causes a â€œhigh riskâ€ â€” the value of 0.9 is substituted, i.e., Î“ â† 0.9. Otherwise, Î“ decreases
t t
continuouslyfollowingtheequation
1
Î“ â†Î“ âˆ’ . (7)
t t T âˆ’t
goal
Here,T iswhentheagentreceivesapositiveenvironmentalreward. So,thesoonertheagentcomestothedesirable
goal
state,thequickertheÎ“ (i.e.,risk)convergestozero1.
t
Alltheupdaterulesdefinedinthepapercanbederivedfromthepostulatethattheagenttriestominimisethe(variational)
freeenergy(Eq. 3)w.r.tthegenerativemodelPauletal.[2023],Isomuraetal.[2022]. Intherestofthepaper,we
investigate the performance of the two schemes â€” i.e. the DPEFE and the CL method â€” and consider a scheme
combiningthem. Thefollowingsectionexploreshowthesetwoschemesperforminagivenenvironment.
3 Results
Wenowtesttheperformanceoftwodecision-makingschemes(DPEFEandCL)inbenchmarkenvironmentssuchas
theCartPole-v1(Fig. 1)fromOpenAIGym.
3.1 CartPole-v1(OpenAIGymtask)
InaCartPole-v1environment,anagentisrewardedforbalancingthepoleupright(withinanacceptablerange)by
movingthecartsideways(Fig.1(A)).Anepisodeterminateswhenthepoleorcartcrossestheacceptablerange(Â±12
degreesforthepoleandÂ±2.4unitsframesizeforthecart,Fig.1(B)).Thisproblemisinherentlyspontaneous,without
theneedforplanningfromthecontroller,wheretheagentmustreacttothecurrentsituationofthecartandthepole.
Wethentesttheactiveinferenceinamutatingsetup,wheretheenvironmentmutatestoamorechallengingversionwith
halftheacceptablerangeforboththepoleandcartposition(Â±6degreesforthepoleandÂ±1.2unitsframesizeforthe
cart). TheperformanceoftheactiveinferenceagentswithdifferentplanningissummarisedinFig.2(A).
Asexpected,theCLmethodagentoutperformsotheractiveinferenceschemes(Astheproblemdemandsspontaneous
control,favouringastate-actionmappingoverplanning). Theagentsquicklylearnthenecessarystate-actionmapping
andbalancethepolemoreeffectivelythanotherplanning-basedschemes. Weobservethisalsoafterthemutationin
theenvironmentatepisodenumber100. TheimprovedperformanceoftheCLmethodagentaftermutationwarrants
additionalinvestigation;however,itcanbeattributedtotheincreasedfeedbackfrequencyduetotheincreasedfailure
rateaftermutation.
InFig.2(B),weseetheevolutionoftheriskterm(Î“). TheriskÎ“settlestoavaluelessthan0.5astheagentlearnsmore
abouttheenvironment. ItisinterestingtonotetheincreaseinÎ“whenfacedwithamutationintheenvironmentinFig.2
1Fortheexactformofthegenerativemodelandfreeenergy,refertoIsomuraandFriston[2020].
4
OnpredictiveplanningandcounterfactuallearninginAIF APREPRINT
Modality Range Episode
termination
Cart Position -4.8 to 4.8 Â±2.4
Cart Velocity -inf to inf NA
Pole Angle -24Â°to 24Â° Â±12Â°
Pole Angular -inf to inf NA
Velocity
A
B
Figure1: A:AsnapshotfromtheCartPole-v1environment(fromOpenAIGym),B:Environmentsummary: The
objectiveistobalancethepole(brown)uprightaslongaspossiblewithoutmeetingtheepisodeterminationcriteria,i.e.
withoutthepoleandcartcrossingpole-angleandcart-positionthresholdsrespectively.
A
B
Figure2: A:Performanceofactiveinferenceagentswithdifferentdecision-makingschemesinthemutatingCartPole-
v1(withamutationatepisode100). AfterEpisode100,theenvironmentmutatestoaharderversion,whichtheagents
mustadaptto. B:Evolutionoftheriskparameter(Î“ )oftheCLmethodagentwhenembodiedintheMutatingCart
t
Poleproblem. Wecanobservethespikeatepisode100,consistentwithmutation,andthereducedriskresultingin
improvedperformanceinthesecondhalfofthetrial.
5
OnpredictiveplanningandcounterfactuallearninginAIF APREPRINT
A
B
Figure3: A:Asnapshotofthe900-stategridworld(maze)environment. B:Theoptimalsolutionforthemazeis
showninA.Thisisacomplexmaze,aswhenactionsaretakenrandomly,ittakesaround9000stepstonavigatethe
gridagainsttheoptimalroutewith47steps.
(B)asexpected. Therisk-reducingbehaviourcorrelateswiththeincreaseinperformanceafterepisodenumber100,
highlightingtheexplainabilityoftheactiveinferenceframework. Next,wetesttheagentsinafundamentallydifferent
environmentâ€“amazetaskâ€“whichwarrantstheneedforplanningforthefuture.
3.2 Complexmazetaskanddata-complexitytrade-off
Tocomparetheperformanceofthetwoagentsinastrategictask,wesimulatetheperformanceinastandardgridworld
taskasshowninFig.3(A).TheoptimalsolutiontothisgridproblemisdemonstratedinFig.3(B).Thisisacomplex
gridworld,whichisnon-trivialcomparedtogridworldtasksusedinthepastliteraturetosolveSajidetal.[2021],as
itwilltakearoundninethousandstepsforanagenttoreachthegoalstateifactionsaretakenrandomlyagainstthe
optimalroutewithlength47.
Theperformanceisevaluatedregardinghowsoontheagentcanfinishanepisode(i.e.,thelengthofanepisode(lower
thebetter)forreachingthegoalstate). ThesimulationresultsshowingtheperformanceofDPEFEandCLagents
areplottedinFig.4(A).Theseresultsshowthatthepredictiveplanning-basedDPEFEagentcanlearnquickly(i.e.,
withintenepisodes)tonavigatethisgrid. Inthesimulations,theactionprecisionusedbytheDPEFEagentisÎ±=1
substitutedin(4). Theagenttendstonavigateinevenlowertimestepsforahigheractionprecision(Ïƒ),alwayssticking
tooptimalactions. Additionally,weobservethattheCLmethodagenttakeslongertolearntheoptimalpath. This
result(Fig.4(A))showsthattheCLagentneedsmoreexperienceintheenvironment(i.e. moredata)tosolveit.
In Fig.4 (B), we compare major active inference algorithmsâ€™ computational complexity associated with planning
fordecision-making. TheDPEFEalgorithmiscomputationallyefficientcomparedtootherpopularactiveinference
schemesSajidetal.[2021],Fristonetal.[2021]. PleasenotethatthisfigurealsoemphasiseshowtheCLmethodhas
nocomputationalcomplexityassociatedwithplanning. So,itisclearthattheCLmethodagentiscomputationally
cheaper than the DPEFE agent as there is no planning component. The computational complexity of the DPEFE
agentisassociatedwiththeplanningdepth(timehorizonofplanning,T),asseeninFig.4(B).Thisdemonstratesa
data-complexitytrade-offbetweenboththeseschemes.
Thisrealisationmotivatesustowardsamixedmodel,whereweproposetodevelopanagentthatcanbalancethetwo
schemesaccordingtotheresourcesavailabletotheagent.Thismakesmuchsensefromtheneuro-biologicalperspective,
asbiologicalagentscontinuallytrytobalanceresourcestolearnandplanforthefutureversustheexperiencethey
alreadyhave. Thisideaalsorelatestotheclassicexploration-exploitationdilemmainreinforcementlearningTriche
etal.[2022].
6
OnpredictiveplanningandcounterfactuallearninginAIF APREPRINT
A
B
Figure4: A:PerformancecomparisonofDPEFEandCLagentsinthe900-stategridschemewith300episodes. The
DPEFEagentlearnstonavigatethegridfaster(Withalowerepisodelength)thantheCLmethodagent. B:Comparison
of computational complexity between state-of-the-art active inference algorithms Sajid et al. [2021], Friston et al.
[2021],theDPEFEmethodPauletal.[2021]andCLmethodIsomuraetal.[2022]. Pleasenotethatthey-axisisinthe
logscale. Thecomputationalcomplexitywascalculatedforthealgorithmstoimplementplanninginastandardgrid
likeinFig.3.
3.3 Integratingthetwodecision-makingapproaches
Toenabletheagenttobalanceitsabilitytopredictfutureoutcomesandusepriorexperience,weintroduceastate-
dependentbiasparameterthatevolveswithexperience(Î²(s,t) âˆˆ [0,1])tothemodel. Thisadditionismotivatedby
thehypothesisthatanagentmaintainsasenseofbias,quantifyingitsconfidenceintheexperienceofdeciding(inthe
past)inthatparticularstate.
When exposed to a new environment, an agent starts with an equal bias for DEEFE (predictive planning) and CL
schemes,representedbyapriorbiasparameterÎ² =0.5.
prior
Over the episodes, the agent will have the probability distributions for decision-making from both models. These
distributionsenabledecision-makinggiventhepresentstate(s). Inafullyobservableenvironment(MDP),sisknown
totheagent(i.e. O=S,orA=I,theidentitymapping). Inthepartiallyobservablecase(POMDP),theagentinfersthe
(hidden)state(s)fromobservation(o)byminimisingvariationalfreeenergyDaCostaetal.[2020],Sajidetal.[2021].
Giventhestateestimation,P(u|s) andP(u|s) arethedistributionsusedforsamplingdecision-makingcorre-
DPEFE CL
spondingtotheDPEFEschemeandCLmethodrespectively(SeeSection2.3.1andSection2.3.2fordetails).
Giventhesedistributions,theagentcannowevaluatehowâ€™usefulâ€™theyareusingtheirShannonentropy(H(X)).This
measure is beneficial as it represents how â€™sureâ€™ that particular distribution is regarding a decision in that state(s).
Namely,iftheagenthasconfidenceinaspecificaction,theactiondistributiontendstobeaone-hotvectorfavouring
theconfidentaction;hence,theentropyofthedistributiontendstozero,incontrasttotheuniformdistribution(not
favouringanyaction)withmaximumentropy. Thus,comparingthisquantityenablestheselectionofthemostconfident
strategyfromthepoolofdifferentschemes.
Basedonthisobservation,overtime,theagentcanusethisentropymeasuretoupdatethevalueofÎ²(s,t)asfollows:
Î²(s )â†Î²(s )+Î±(H(P (u|s ))âˆ’H(P (u|s ))). (8)
t t CL t DPEFE t
Here,Î±isanormalisationparameterstabilisingtheupdatedvalue,andwemakesurethatÎ² âˆˆ [0,1]byre-calibrating
Î² < 0asÎ² = 0andÎ² > 1asÎ² = 1. FromaBayesianinferenceperspective, onemayviewtheupdatedbeliefÎ²
inEq.(8)asaposteriorbeliefrepresentinghowlikelytheDPEFEmodelisselected,similartotheBayesianmodel
selectionschemes.
7
OnpredictiveplanningandcounterfactuallearninginAIF APREPRINT
ğ”» â„‚ ğ”¾ ğ« ğ”¼
s A o ğ”¸ =
1 1 s
1
P
DPEFE
u B ğ”¹ u
1 u u 1
Î“(t)
s A o ğ”¸ =
2 2
s
2
u 2 B ğ”¹ â„‚ğ•ƒ P MM u 2
u u
s
3
A o
3
ğ”¸
s
= â„‚ğ•ƒ
3
s A o ğ”¸ = â„‚ğ•ƒ
T-1 T-1
s
T-1
u
u T-1 B u T-1
s A o
T T
ssecorp
evitareneG
Generative
model
DPEFE
Real world Perception CL method Decision making
Planning
Figure5: Flowdiagramoftheagent-environmentloopintheproposedmixedmodelcombiningplanningandcoun-
terfactuallearning. Thereisakeydistinctionbetweenthegenerativeprocessandthegenerativemodelintheactive
inferenceframework. InaPOMDP,weassumethattheobservationsaregeneratedbythegenerativeprocess(â€˜Real
worldâ€™)byâ€˜hidden-statesâ€™(s )throughastate-observationmapping(A),bothbeinginaccessibletotheagent. Inthe
t
generativemodel,theagentuseso tomaintainanoptimalbeliefaboutthehiddenstates (â€˜Perceptionâ€™). Subsequently,
t t
theagentusestheplanningmethod(DPEFE)andCounterfactual(CL)methodtocombinetheactiondistributionsusing
themodel-biasparameterÎ² fordecision-making. Thedecisionattimetinfluencesthehiddenstateoftheâ€˜Realworldâ€™
atthenextstep,completingtheloop. Thegenerativeprocesscanbethoughtofastheenvironmenttheagenttriesto
survivein,whereasthegenerativemodeliscompletelypartoftheagentandcanbeinterpretedastheâ€˜imaginaryâ€™world
theagentassumesitsurvives.
UsingthismeasureofbiasÎ²(s ),theagentcannowevaluateanewdistributionfordecision-making,P ,whereMM
t MM
standsforthemixedmodelas:
P(u|s ) =P(u|s )1âˆ’Î²(st)Â·P(u|s )Î²(st) . (9)
t MM t CL t DPEFE
Theflowdiagramdescribingtheproposedmixedmodelâ€™sPOMDP-basedâ€œagent-environmentâ€loopisgiveninFig.5. 2.
3.4 Derivingupdateequationsforthemixedmodelfromvariationalfreeenergy
Eqs.8and9canbederivedfromvariationalfreeenergyminimisationunderaPOMDPgenerativemodel.Thevariational
freeenergyforthemixedmodelisdefinedas:
2Foradetaileddescriptionofvariousparametersinthehybridmodel,refertoSection2.2,Section2.3.1,andSection2.3.2.
8
OnpredictiveplanningandcounterfactuallearninginAIF APREPRINT
t
(cid:88)
F = s Â·{lns âˆ’lnAÂ·o âˆ’lnBs }
Ï„ Ï„ Ï„ Ï„âˆ’1
Ï„=1
t
(cid:88)
+ u Â·{lnu +Î²Î±Â·G âˆ’(1âˆ’Î²)(1âˆ’2Î“ )lnCLs }
Ï„ Ï„ Ï„ t Ï„âˆ’1
Ï„=1
+D [Q(Î²)||P(Î²)]+D [Q(Î¸)||P(Î¸)] (10)
KL KL
WhenÎ“ =0andÎ² =0.5,thederivativeofF withrespecttoÎ²=E[Î²]givestheposteriorexpectationasfollows:
t prior
(cid:32) t t (cid:33)
(cid:88) (cid:88)
Î²=sig âˆ’ u Â·Î±Â·G âˆ’ u Â·lnCLs . (11)
Ï„ Ï„ Ï„ Ï„âˆ’1
Ï„=1 Ï„=1
Interestingly,thisposteriorexpectationcanberewrittenusingtheentropiesofDPEFEandCL.TheaboveF becomes
variationalfreeenergy(Eq. 3)forDPEFEorCLwhenÎ² =1or0,respectively.
Thus,minimisingF withrespecttou yields:
Ï„
u =Ïƒ(âˆ’Î±Â·G ), (12)
Ï„ Ï„
forDPEFEand,
u =Ïƒ(lnCLs ), (13)
Ï„ Ï„âˆ’1
forCL(notethatÎ“ =0isusuallysupposedinCLwhengeneratingactions).
t
Thus,fromthedefinitionoftheShannonentropy,weobtain
t t
(cid:88) (cid:88)
H =âˆ’ u Â·lnu = u Â·Î±Â·G , (14)
DPEFE Ï„ Ï„ Ï„ Ï„
Ï„=1 Ï„=1
and,
t t
(cid:88) (cid:88)
H =âˆ’ u Â·lnu =âˆ’ u Â·lnCLs . (15)
CL Ï„ Ï„ Ï„ Ï„âˆ’1
Ï„=1 Ï„=1
Hence,Î²canberewrittenas:
Î²=sig(âˆ’H +H ). (16)
DPEPE CL
When|H âˆ’H |<<1,Eq.8approximatesEq.16. MinimisationofF furtheryieldsEq.9asitisanexpression
CL DPEPE
usingtheprobabilitydistributionandequivalenttotheposteriorexpectation:
u =Ïƒ(âˆ’Î²Î±Â·G +(1âˆ’Î²)lnCLs ). (17)
Ï„ Ï„ Ï„âˆ’1
Therefore,theupdaterulesforthemixedmodel(Eqs.8and9)canbeformallyderivedfromvariationalfreeenergy
minimisation.
3.5 Performanceofmixed-modelinamutatingmazeenvironment
Wenowexaminetheproposedmixedschemewithagentsofdifferentplanningpower(i.e. differentplanningdepths,N
3)inasimilarenvironment. ThecomputationalcomplexityoftheDPEFEschemeislinearlydependentontheplanning
timehorizon(planningdepth),i.e. T,andholdsforthemixed-modelagentaswell(seeFig.4). Thus,anagentwith
planningdepthN =50takesuptwicethecomputationalresourceswhileplanningcomparedtoanagentwithN =25.
Weuseamutatinggridenvironmenttotesttheperformanceofthemixedmodel-basedagent. Thismutatinggridscheme
isillustratedinFig.6. Theagentstartsinamoreaccessiblegridversionwithanoptimalpathoffoursteps(Fig.6,(A)).
After300episodes,theenvironmentmutatestothecomplexversionofthegridshownintheprevioussection(See
Fig.6(B)).Thissetupalsoenablesustostudyhowadaptabletheagentistonewenvironmentalchanges.
The performance is summarised in Fig.7. We observed that all three mixed model agents (with varying levels of
planningability)learnedtonavigatethemoreaccessiblegridwithinthefirsttenepisodes(Fig.7: A).However,when
3Werefertotheplanninghorizonofthemixed-modelasN,andtheDPEFEmethodasT toavoidconfusion.
9
OnpredictiveplanningandcounterfactuallearninginAIF APREPRINT
A
B
Figure6: Themutatinggridschemeusedforstudyingagentâ€™sadaptability. Theagentlearnstonavigatetheeasygrid
(A)inthefirsthalf(300episodes)andfacesenvironmentmutationandshouldlearntosolvethehardgrid(B).
A
B
Figure 7: A: Performance of mixed model agents with different planning depths in the mutating grid scheme, B:
Performanceofmixedmodelagentswithdifferentplanningdepthsinthehardmazesimulatedseparately.
10
OnpredictiveplanningandcounterfactuallearninginAIF APREPRINT
A
B
Figure8:A:EvolutionoftheRiskparameter(Î“)ofthemixed-modelagentwhenembodiedinthemutatinggridscheme,
B:Evolutionofthemodelmixingparameter(Î²)ofthemixed-modelagentwhenembodiedinthemutatinggridscheme.
theenvironmentmutatedtotherigidgridinepisodenumber300,theagentslearnedsimilartotheperformancewe
observewhennavigatingthatgridalone,Fig.7: B,(i.e.,complexgridwith900states).
Wealsoobservedthattheagentwithhigherplanningabilitylearnedtonavigatethegridfasterandmoreconfidently
than the other two. This result demonstrates that the proposed mixed model enables agents to balance the two
decision-makingapproachesintheactiveinferenceframework.
Itisconsideredthatthebrainofbiologicalorganismsalsoemploysmechanismstoswitchmultiplestrategies.Ourmodel
ispotentiallyhelpfulforunveilingefficientdecision-makingmechanismsinthebrainandtheirneuronalsubstratesand
developingcomputationallyefficientbio-mimeticagents.
4 Discussion
4.1 Explainabilityoftheactiveinferencemodels
Anadditionaladvantageofthemixedmodelproposed(andthePOMDP-basedgenerativemodels)isthatwecanprobe
themodelparameterstounderstandthebasisofintelligentbehaviourdemonstratedbyagentsthroughthelensofactive
inference. Modelsthatrelyonartificialneuralnetworks(ANNs)toscaleupthemodelsFountasetal.[2020]have
limitedexplainabilityregardinghowagentsmakedecisions,especiallywhenfacedwithuncertainty.
InFig.8: (A),wecanprobetoseetheevolutionoftherisk(Î“ )inthemodel(associatedwiththeCLmethodscheme
t
asdefinedinIsomuraetal.[2022]). Wecanobservethatthemodelâ€™sriskquicklytendstozerowhentheeasygridis
presentedandsolved;however,itshootsupwhenfacedwiththeenvironmentmutation.
Similarly,theevolutionofthebiasparameter(thatbalancestheDPEFEandCLmethodinthemixedmodel)isshown
inFig.8: (B).Here,wealsoobservehowtheagentconsistentlymaintainsahigherbiastotheDPEFEmodelwhenit
hasahigherplanningability(i.e. theagentwithaplanningdepthofN =50comparedtobiasinagentswithN =25,
andN =5).
Weshouldnotethatthevalueofthebiasparameterneverincreasesmorethan0.5,evenwhentheDPEFEagentis
planningatT =50. Inthesimulations,westartwithabiasÎ² =0.5andupdateÎ²accordingto(8). Thisshowshowthe
agenteventuallylearnstorelyonthemixedmodelâ€™sCLscheme(i.e.,experience). Still,theDPEFEcomponent(i.e.
planning)accelerateslearningandperformancetoaiddecision-making. Suchinsightsintotheexplainabilityofthe
agentâ€™sbehaviourviamodelparametershelpstudythebasisofnatural/syntheticintelligence.
11
OnpredictiveplanningandcounterfactuallearninginAIF APREPRINT
4.2 Conclusions
Thispapercomparedandcontrastedtwodecision-makingschemesintheactiveinferenceframework. Observingthe
prosandconsofbothapproaches,weexaminedthemontasksthatdemandspontaneous(CartPole-v1)andstrategic
(maze) decision-making, thereby testing a hybrid approach. The insights observed in this work will help improve
algorithmsusedforcontrol,giventheexcitementaroundusingactiveinferenceschemesDaCostaetal.[2022].
Weleavethedetailedanalysisofbehaviouraldependenceonparametersandmodelexpansioninmoredemanding
environmentstofuturework. SystematiccomparisonwithANNs(ArtificialNeuralNetworks)aidedmodelslikeinthe
resultsofFountasetal.[2020]isalsoapromisingdirectiontopursue.
5 Softwarenote
Thegridenvironmentandagents(DPEFE,CLandMixed-modelschemes)werecustom-writteninPython. Allscripts
areavailableatthefollowinglink: https://github.com/aswinpaul/aimmppcl_2023.
6 Acknowledgments
AP acknowledges research sponsorship from IITB-Monash Research Academy, Mumbai and the Department of
Biotechnology,GovernmentofIndia. TIisfundedbytheJapanSocietyforthePromotionofScience(JSPS)KAKENHI
(Refs:JP23H04973&JP23H03465)andtheJapanScienceandTechnologyAgency(JST)CREST(Ref:JPMJCR22P1).
AR is funded by the Australian Research Council (Ref: DP200100757) and the Australian National Health and
MedicalResearchCouncilInvestigatorGrant(Ref: 1194910). ARisaffiliatedwithTheWellcomeCentreforHuman
Neuroimaging,supportedbycorefundingfromWellcome[203147/Z/16/Z].ARisalsoaCIFARAzrieliGlobalScholar
intheBrain,Mind&ConsciousnessProgram.
References
RichardS.SuttonandAndrewG.Barto. ReinforcementLearning: AnIntroduction. TheMITPress,secondedition,
2018. URLhttp://incompleteideas.net/book/the-book-2nd.html.
KarlFriston. Thefree-energyprinciple: aunifiedbraintheory? NatureReviewsNeuroscience,11(2):127â€“138,2010.
ISSN1471-0048. doi:10.1038/nrn2787. URLhttps://doi.org/10.1038/nrn2787.
Lancelot Da Costa, Thomas Parr, Noor Sajid, Sebastijan Veselic, Victorita Neacsu, and Karl Friston. Active in-
ference on discrete state-spaces: A synthesis. Journal of Mathematical Psychology, 99:102447, 2020. ISSN
0022-2496. doi:10.1016/j.jmp.2020.102447. URL https://www.sciencedirect.com/science/article/
pii/S0022249620300857.
NoorSajid,PhilipJ.Ball,ThomasParr,andKarlJ.Friston. Activeinference: Demystifiedandcompared. Neural
Computation,33(3):674â€“712,January2021. ISSN0899-7667. doi:10.1162/neco_a_01357. URLhttps://doi.
org/10.1162/neco_a_01357.
BerenMillidge,AlexanderTschantz,andChristopherLBuckley. Whencetheexpectedfreeenergy?,2020.
KarlJ.Friston,ThomasParr,andBertdeVries. Thegraphicalbrain: Beliefpropagationandactiveinference. Network
Neuroscience,1(4):381â€“414,2017. doi:10.1162/NETN_a_00018. URLhttps://doi.org/10.1162/NETN_a_
00018.
FranzKuchling,KarlFriston,GeorgiGeorgiev,andMichaelLevin. Morphogenesisasbayesianinference:Avariational
approach to pattern formation and control in complex biological systems. Physics of Life Reviews, 33:88â€“108,
2020. ISSN1571-0645. doi:https://doi.org/10.1016/j.plrev.2019.06.001. URLhttps://www.sciencedirect.
com/science/article/pii/S1571064519300909.
GeorgeDeane,MarkMiller,andSamWilkinson. Losingourselves:Activeinference,depersonalization,andmeditation.
Frontiers in Psychology, 11, 2020. ISSN 1664-1078. doi:10.3389/fpsyg.2020.539726. URL https://www.
frontiersin.org/articles/10.3389/fpsyg.2020.539726.
LesliePackKaelbling,MichaelL.Littman,andAnthonyR.Cassandra. Planningandactinginpartiallyobservable
stochasticdomains.ArtificialIntelligence,101(1):99â€“134,1998.ISSN0004-3702.doi:https://doi.org/10.1016/S0004-
3702(98)00023-X. URLhttps://www.sciencedirect.com/science/article/pii/S000437029800023X.
AswinPaul,NoorSajid,ManojGopalkrishnan,andAdeelRazi. Activeinferenceforstochasticcontrol. InMachine
LearningandPrinciplesandPracticeofKnowledgeDiscoveryinDatabases,pages669â€“680,Cham,2021.Springer
InternationalPublishing. ISBN978-3-030-93736-2. doi:https://doi.org/10.1007/978-3-030-93736-2_47.
12
OnpredictiveplanningandcounterfactuallearninginAIF APREPRINT
Takuya Isomura, Hideaki Shimazaki, and Karl J. Friston. Canonical neural networks perform active inference.
Communications Biology, 5(1):55, 2022. ISSN 2399-3642. doi:10.1038/s42003-021-02994-2. URL https:
//doi.org/10.1038/s42003-021-02994-2.
Raphael Kaplan and Karl J. Friston. Planning and navigation as active inference. Biological Cybernetics, 112
(4):323â€“343, 2018. ISSN 1432-0770. doi:10.1007/s00422-018-0753-2. URL https://doi.org/10.1007/
s00422-018-0753-2.
KarlJ.Friston,JeanDaunizeau,andStefanJ.Kiebel. Reinforcementlearningoractiveinference? PLOSONE,4(7):
1â€“13,072009. doi:10.1371/journal.pone.0006421. URLhttps://doi.org/10.1371/journal.pone.0006421.
KarlFriston. Afreeenergyprincipleforbiologicalsystems. Entropy(Basel,Switzerland),14:2100â€“2121,112012.
doi:10.3390/e14112100.
KarlFriston,LancelotDaCosta,DanijarHafner,CasperHesp,andThomasParr. Sophisticatedinference. Neural
Computation,33(3):713â€“763,February2021. ISSN0899-7667. doi:10.1162/neco_a_01351. URLhttps://doi.
org/10.1162/neco_a_01351.
AswinPaul, NoorSajid, LancelotDaCosta, andAdeelRazi. Onefficientcomputationinactiveinference. arXiv
preprintarXiv:2307.00504,2023.
Karl J. Friston, Tommaso Salvatori, Takuya Isomura, Alexander Tschantz, Alex Kiefer, Tim Verbelen, Mag-
nus T. Koudahl, Aswin Paul, Thomas Parr, Adeel Razi, Brett J. Kagan, Christopher L. Buckley, and Maxwell
James D. Ramstead. Active inference and intentional behaviour. ArXiv, abs/2312.07547, 2023. URL https:
//api.semanticscholar.org/CorpusID:266191299.
Takuya Isomura and Karl Friston. Reverse-Engineering Neural Networks to Characterize Their Cost Functions.
NeuralComputation, 32(11):2085â€“2121,112020. ISSN0899-7667. doi:10.1162/neco_a_01315. URLhttps:
//doi.org/10.1162/neco_a_01315.
TakuyaIsomura,KiyoshiKotani,YasuhikoJimbo,andKarlJ.Friston. Experimentalvalidationofthefree-energyprinci-
plewithinvitroneuralnetworks. NatureCommunications,14(1):4547,2023. ISSN2041-1723. doi:10.1038/s41467-
023-40141-z. URLhttps://doi.org/10.1038/s41467-023-40141-z.
Anthony Triche, Anthony S. Maida, and Ashok Kumar. Exploration in neo-hebbian reinforcement learning:
Computational approaches to the explorationâ€“exploitation balance with bio-inspired neural networks. Neu-
ral Networks, 151:16â€“33, 2022. ISSN 0893-6080. doi:https://doi.org/10.1016/j.neunet.2022.03.021. URL
https://www.sciencedirect.com/science/article/pii/S0893608022000995.
ZafeiriosFountas,NoorSajid,PedroA.M.Mediano,andKarlFriston. DeepactiveinferenceagentsusingMonte-Carlo
methods. arXiv:2006.04176[cs,q-bio,stat],June2020.
Lancelot DaCosta, Pablo Lanillos, NoorSajid, KarlFriston, andShujhat Khan. How activeinference could help
revolutioniserobotics. Entropy,24(3),2022. ISSN1099-4300. doi:10.3390/e24030361. URLhttps://www.mdpi.
com/1099-4300/24/3/361.
13

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "On Predictive planning and counterfactual learning in active inference"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   âŒ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   âœ… GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   âŒ BAD: Repeating the same claim 3+ times with slight variations
   âœ… GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
