=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Toward Design of Synthetic Active Inference Agents by Mere Mortals
Citation Key: vries2023toward
Authors: Bert de Vries

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2023

Abstract: The theoretical properties of active inference agents are impressive,
but how do we realize effective agents in working hardware and software
on edge devices? This is an interesting problem because the computa-
tional load for policy exploration explodes exponentially, while the com-
putational resources are very limited for edge devices. In this paper, we
discussthenecessaryfeaturesforasoftwaretoolboxthatsupportsacom-
petentnon-expertengineertodevelopworkingactiveinferenceagents. We
introduce a...

Key Terms: graph, bert, agents, toward, software, technology, design, toolbox, mere, inference

=== FULL PAPER TEXT ===

Toward Design of
Synthetic Active Inference Agents
by Mere Mortals
Bert de Vries
Eindhoven University of Technology
Eindhoven, the Netherlands
bert.de.vries@tue.nl
July 27, 2023
Abstract
The theoretical properties of active inference agents are impressive,
but how do we realize effective agents in working hardware and software
on edge devices? This is an interesting problem because the computa-
tional load for policy exploration explodes exponentially, while the com-
putational resources are very limited for edge devices. In this paper, we
discussthenecessaryfeaturesforasoftwaretoolboxthatsupportsacom-
petentnon-expertengineertodevelopworkingactiveinferenceagents. We
introduce a toolbox-in-progress that aims to accelerate the democratiza-
tion of active inference agents in a similar way as TensorFlow propelled
applications of deep learning technology.
1 Introduction
Thispositionpaperaimstocomplementarecentwhitepaperondesigningfuture
intelligent ecosystems where autonomous Active InFerence (AIF) agents learn
purposeful behavior through situated interactions with other AIF agents [11].
The white paper states that these agents ‚Äú... can be realized via (variational)
message passing or belief propagation on a factor graph‚Äù [11, abstract]. Here,
we discuss the computational requirements for a factor graph software toolbox
that supports this vision. Noting that the steep rise of commercialization op-
portunities for deep learning systems was greatly facilitated by the availability
ofprofessional-leveltoolboxessuchasTensorFlowandsuccessors,weclaimthat
a high-quality AIF software toolbox is needed to realize the proposition in [11].
Therefore, in this paper, we ask the question: what properties should a fac-
tor graph toolbox possess that enable a competent engineer to develop relevant
AIFagents? Thequestionisimportantsincethenumberofapplicationsforau-
1
3202
luJ
62
]LM.tats[
1v54141.7032:viXra
tonomous AIF agents is expected to vastly outgrow the number of world-class
experts in AIF and robotics.
Asanillustratingexample,consideranengineer(Sarah)whoneedstodesign
a quad-legged robot that is tasked to enter a building and switch off a valve.
We assume that Sarah is a competent engineer with an MS degree and a few
years of experience in coding and control systems. She has some knowledge of
probabilistic modeling but is not a top expert in those fields.
In order to relieve Sarah from designing every detail of the robot, we expect
that the robot possesses some ‚Äúintelligent‚Äù adaptation capabilities. Firstly, the
robot should be able to define sub-tasks and solve these tasks autonomously.
Secondly, since we do not know a-priori the inside terrain of the building, the
robotshouldbecapableofadaptingitswalkingandotherlocomotiveskillsunder
situatedconditions. Thirdly,weexpectthattherobotperformsrobustly,inreal-
time, and cleverly manages the consumption of its computational resources.
All these robot properties should be supported seamlessly by Sarah‚Äôs AIF
software toolbox. For instance, she should not need to know the specifics of
how to implement robustness in her algorithms or how many time steps the
robotneedstolookaheadinanygivensituationforeffectiveplanningpurposes.
We want a toolbox that enables competent engineers to develop effective AIF
agents, notatoolboxforaselectgroupofworld-classmachinelearningexperts.
WedoexpectthatSarahiscapableofdescribingherbeliefsaboutdesiredrobot
behavior through the high-level specification of a probabilistic (world or gener-
ative) model or, at least, the prior preferences or constraints that underwrite
behavior.
After reviewing some motivating agent properties that follow immediately
from committing to free energy minimization (section 2), we proceed to discuss
why message passing in a factor graph is the befitting framework for imple-
menting AIF agents (section 3.1). More specifically, we argue that a reactive
programming-based implementation of message passing will be the standard in
professional-levelAIFtools(section3.2). Incomparisontotheusualprocedural
codingstyle,reactivemessagepassingleadstoincreasedrobustness(section3.3),
lower power consumption (section 3.5), hard real-time processing (section 3.4),
andsupportforcontinualmodelstructureadaptation(section4). Insection5.3
we introduce RxInfer, a toolbox-in-progress for developing AIF agents that
robustly minimize free energy in real-time by reactive message passing.
2 The Free Energy Principle and Active Infer-
ence
2.1 FEP for synthetic AIF agents
TheFreeEnergyPrinciple(FEP)describesself-organizingbehaviorinpersistent
naturalagents(suchasabrain)astheminimizationofaninformation-theoretic
2
functional that is known as the variational Free Energy (FE).1 Essentially, the
FEP is a commitment to describing adaptive behavior by Hamilton‚Äôs Principle
of Least Action [14]. The process of executing FE minimization in an agent
that interacts with its environment through both active and sensory states is
calledActiveInference (AIF).Crucially,theFEPclaimsthat,innaturalagents,
FE minimization is all that is going on. While engineering fields such as signal
processing,control,andmachinelearningareconsidereddifferentdisciplines,in
nature these fields all relate to the same computational mechanism, namely FE
minimization.
For an engineer, this is good news. If we wish to design a synthetic AIF
agentthatlearnspurposefulbehaviorsolelythroughself-directedenvironmental
interactions, we can focus on two tasks:
1. Specificationoftheagent‚Äôsmodelandinferenceconstraints. Thisisequiv-
alent to the specification of a (constrained) FE functional.
2. A recipe to continually minimize the FE in that model under situated
conditions, driven by environmental interactions.
We are interested in the development of an engineering toolbox to support
these two tasks.
2.2 FEM for simultaneous refinement of problem repre-
sentation and solution proposal
An important quality of the robot will be to define tasks for itself and solve
these tasks autonomously. Here, we shortly discuss how the FEP supports this
objective.
Consider a generative model p(x,s,u), where x are observed sensory inputs,
u are latent control signals and s are latent internal states. For notational ease,
we collect the latent variables by z = {s,u}. The variational FE for model
p(x,z) and variational posterior q(z) is then given by
(cid:88) q(z)
F[q,p]=‚àílogp(x)+ q(z)log (1a)
p(z|x)
(cid:124) (cid:123)(cid:122) (cid:125)
z
surprise (cid:124) (cid:123)(cid:122) (cid:125)
bound
(cid:88) q(z) (cid:88)
= q(z)log ‚àí q(z)logp(x|z) . (1b)
p(z)
z z
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
complexity accuracy
The FE functional in (1a) can be interpreted as the sum of surprise (negative
log-evidence)andanon-negativeboundthatistheKullback-Leiblerdivergence
1For reference, we use the following abbreviations in this paper: Active Inference (AIF),
ConstrainedBetheFreeEnergy(CBFE),ExpectedFreeEnergy(EFE),(variational)FreeEn-
ergy(FE),FreeEnergyPrinciple(FEP),FreeEnergyMinimization(FEM),MessagePassing
(MP),ReactiveMessagePassing(RMP).
3
between the variational and the optimal (Bayesian) posterior. The first term,
surprise, can be interpreted as a performance score for the problem represen-
tation in the model. This term is completely independent of any inference
performance issues. The second term (the bound) scores how well actual solu-
tions are inferred, relative to optimal (Bayesian) inference solutions. In other
words, the FE functional is a universal cost function that can be interpreted as
the sum of problem representation and solution proposal costs. FE minimiza-
tion leads toward improving both the problem representation and solving the
problemthroughinferenceoverlatentvariables. Inparticular,FEminimization
overaparticularmodelstructurepshouldleadtonestedsub-modelsthatreflect
the causal structure of the sensory data. Sub-tasks are solved by FE minimiza-
tion in these sub-models. Hence, both creation of subtasks and solving these
subtasks are driven solely by FE minimization.
In conclusion, a high-end toolbox should be capable to minimize FE both
over(beliefsover)latentvariablesthroughadaptationofq(z)(leadingtobetter
solution proposals for the current model p), and over the model structure p
(leading to a better problem representation).
As an aside, an interesting consequence of the FE decomposition into prob-
lem plus solution costs is that a relatively poor problem representation with
a superior inference process may be preferred (evidenced by lower FE), over
a model with a good problem representation (high Bayesian evidence) where
inference costs are high. The notion that the model with the largest Bayesian
evidence may not be the most useful model in a practical application, casts an
interesting light on the common interpretation of FE as a mere upper bound
on Bayesian evidence. We argue here that FE is actually a more principled
performance score for a model, since in addition to Bayesian model evidence,
FE also scores the performance loss in a model due to an inaccurate inference
process.
2.3 AIF for smart data sets and resource management
Ifwewanttherobottocopewithunknownphysicalterrainconditions, itisnot
sufficient to pre-train the robot offline on a large set of relevant examples. The
robot must be able to acquire relevant new data and update its model under
real-world conditions.
FE minimization in the generative model‚Äôs roll-out to the future results in
theminimizationofacostfunctionalknownastheExpectedFreeEnergy(EFE).
ItcanbeshownthattheEFEdecomposesintoasumofpragmatic(goal-driven,
exploitation) and epistemic (information-seeking, exploration) costs [9]. As a
result, inferred actions balance the need to acquire informative data (to learn a
better predictive model) with the goal to reach desired future behavior.
In contrast to the current AI direction towards training larger models on
larger data sets, an active inference process elicits an optimally informative,
small (‚Äúsmart‚Äù) data set for training of just ‚Äúgood-enough‚Äù models to achieve
a desired behavior. AIF agents adapt enough to accomplish the task at hand
while minimizing the consumption of resources such as energy, data, and time.
4
ùëù , ùëù - ùëù .
ùúá‚Éó (ùë• ) ùúá‚Éñ (ùë• )
$ $ % %
ùëã " ùëã # ùëã ‚Äô
ùëã
ùëù ùëù $ ùëù ùëù
( ) * +
ùëã ùëã ùëã
! % &
ùúá‚Éñ (ùë• )
$ $
Figure 1: Forney-style Factor Graph representation of the factorization (2).
The trade-off between data accuracy and resource consumption is driven by
the decomposition in (1b) of FE as a measure of complexity minus accuracy.
According to this decomposition, more accurate models are only pursued if the
increase in accuracy outweighs the resource consumption costs.
In short, AIF agents that are driven solely by FE minimization will inher-
ently manage their computational resources. These agents automatically infer
actions that elicit appropriately informative data to upgrade their skills toward
good-enough performance levels. Since both the agent and environment mutu-
allyaffecteachotherinareal-timeinformationprocessingloop,itwouldnotbe
possible to acquire the same data set through the sampling of the environment
without the agent‚Äôs participation.
3 FE Minimization by Reactive Message Passing
3.1 Why message passing-based inference?
Up to this point, our arguments strongly supported AIF as an information
processing engine for the robot. Unfortunately, the computational demands
for simulating a non-trivial synthetic AIF agent are extreme. For comparison,
consider the human brain that minimizes in real-time, for less than 20 watts,
a highly time-varying FE functional (visual data rate about of about a million
bits per second) over about 100 trillion latent variables (synapses). It has been
estimated that the human brain consumes about a million times less energy
than a high-tech silicon computer on quantitatively comparable information
processing tasks. [17].
Clearly, thehumanbrainminimizesFEinaverydifferentwaythanisavail-
ableinstandardoptimizationtoolboxes. Inthissection,wewillarguefordevel-
oping a FE minimization toolbox based on reactive message passing in a factor
graph.
First, we shortly recapitulate why message passing in factor graphs is an
effective inference method for large models. Consider a factorized multivariate
5
function
p(x ,x ,...,x )
1 2 7
=f (x )f (x )f (x ,x ,x )f (x )f (x ,x ,x )f (x )f (x ,x ,x ) (2)
a 1 b 2 c 1 2 3 d 4 e 3 4 5 f 6 g 5 6 7
Assume that we are interested in inferring (the so-called marginal distribution)
(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)
p(x )= p(x ,x ,...,x ) (3)
3 1 2 7
x1 x2 x4 x5 x6 x7
If each variable x in (3) has about 10 possible values, then the sum contains
i
about 1 million terms. However, making use of the factorization (2) and the
distributive law [7], we can rewrite this sum as
‚Üí‚àí
¬µ3(x3)
(cid:18)(cid:122) (cid:125)(cid:124) (cid:123)(cid:19)
(cid:88)(cid:88)
p(x )= f (x )f (x )f (x ,x ,x ) ¬∑
3 a 1 b 2 c 1 2 3
x1 x2
‚Üê‚àí
¬µ5(x5)
(cid:18) (cid:122) (cid:125)(cid:124) (cid:123) (cid:19)
¬∑ (cid:88)(cid:88) f (x )f (x ,x ,x ) (cid:0)(cid:88)(cid:88) f (x )f (x ,x ,x ) (cid:1) (4)
d 4 e 3 4 5 f 6 g 5 6 7
x4 x5 x6 x7
(cid:124) (cid:123)(cid:122) (cid:125)
‚Üê‚àí
¬µ3(x3)
Thecomputationin(4),whichrequiresonlyafewhundredsummationsand
multiplications, is clearly preferred from a computational load viewpoint. To
execute (4), we need to compute intermediate results ‚Üí‚àí ¬µ (x ) and ‚Üê ¬µ ‚àí (x ) that
i i i i
affordaninterpretationoflocalmessagesinaForney-styleFactorGraph(FFG)
representation of the model, see Fig. 1.
Variational FE minimization can also be executed by message passing in a
factorgraph. Infact,nearlyallknowneffectivevariationalinferencemethodson
factorizedmodelscanbeinterpretedasminimizationofaso-called‚Äúconstrained
BetheFreeEnergy‚Äù (CBFE)functional[16]. Inthisformulation, posteriorvari-
ationalbeliefsarefactorizedintobeliefsoverboththenodesandtheedgesofthe
graph. It is possible to add constraints to these local beliefs such as requiring
that a particular variational posterior is expressed by a Gaussian distribution.
In general, CBFE minimization by message passing in a factor graph supports
local adaptation of a plethora of constraints to optimize accuracy vs resource
consumption. [16, 1]
Useful dynamic models for real-time processing ofdata streams with a large
numberoflatentvariablesarenecessarilysparselyconnectedbecauseotherwise,
real-timeinferencewouldnotbetractable. Insparsemodels,thecomputational
complexity of inference can be vastly reduced by message passing in a factor
graph representation of the model. In particular, automated CBFE minimiza-
tion by message passing in a factor graph supports refined optimization of the
accuracy vs resource consumption balance.
6
3.2 Reactive vs procedural coding style
Next, we discuss a key technological component for a synthetic AIF agent,
namely the requirement to execute FE minimization through a reactive pro-
gramming paradigm.
A crucial feature of all MP-based inference is that the inference process
consists entirely of a (parallelizable) series of small steps (messages) that in-
dividually and independently contribute to FE minimization. As a result, a
messagepassing-basedFEminimizationprocesscanbeinterruptedat any time
without loss of important intermediate computational results.
In a practical setting, it is very important that an ongoing inference process
canberobustly(withoutcrashing)interruptedatanytimewitharesult. These
intermediateinferenceresultscanonlybereliablyretrievediftheinferencepro-
cess iteratively updates its beliefs in small steps, or, in other words, by message
passing. Moreover, the inference process should not be subject to a prescribed
control flow that contains for-loops. Rather, if we were to write code for an
anytime-interruptable inference process in a programming language, we should
use a reactive rather than the more common procedural programming style. In
areactivelycodedinferenceengine,thereisnocodeforcontrolflow,suchas‚Äúdo
first this, then that‚Äù, but instead only a description of how a processing module
(a factor graph node) should react to changes in incoming messages. We will
call this process Reactive Message Passing (RMP) [2]. In an RMP inference
process,thereisnoprescribedscheduleforpassingmessagessuchastheViterbi
or Bellman algorithm. Rather, an RMP inference process just reacts by FE
minimization whenever FE increases due to new observations.
In Fig. 2, we display the consequences of choosing a reactive programming
style for an application engineer like Sarah. The procedural programming style
in Algorithm-1 requires Sarah to provide the control flow (the ‚Äúprocedure‚Äù) for
theinferenceprocess. Sarahneedstowritecodeforwhentocollectobservations,
when to update states, etc. The specific control flow in Algorithm-1 is just an
example and there exists literature that aims to improve the efficiency of the
control flow [5, 10]. In order to write an efficient inference control flow recipe
for a complex AIF agent, Sarah needs to be an absolute expert in this field.
Consider in contrast the code for reactive inference in Algorithm-2. In a
reactive programming paradigm, there is no control flow. Rather, the only
inference instruction is for the agent to react to any opportunity to minimize
FE. When FE minimization is executed by a reactive message passing toolbox,
the application engineer only needs to specify the model.
Aside from lowering the competence bar for application engineers to design
effective AIF agents, the procedural style of implementing FE minimization
is fundamentally inappropriate. The control flow in Algorithm-1 necessarily
contains many design choices that only become known during deployment. For
instance,howfarshouldtheagentrolloutitsmodeltothefutureforcomputing
theEFE?Thiskindofinformationishighlycontextualandnotavailabletothe
application engineer. In contrast, the application engineer‚Äôs code for reactive
inference("reacttoanyFEMopportunity")worksforanymodelinanycontext.
7
Algorithm 1 Procedural AIF Algorithm 2 Reactive AIF
1: Specify model p(x,s,u,Œ∏) 1: Specify model p(x,s,u,Œ∏)
2: for t=1,2,... do ‚ñ∑ Deploy 2: while true do ‚ñ∑ Deploy
3: Collect new observation x t 3: React to any FEM opportunity
4: Update state q(s t |x 1:t ) 4: end while
5: Update desired futurepÀú(x >t )
Upd. candidate policies
6:
{œÄ(i)}
7: for all œÄ(i) do
Predict future
8:
p(x |s ,œÄ(i))
>t t
9: Compute EFE G(œÄ(i))
10: end for
11: Select œÄ‚àó =argmin G(œÄ)
œÄ‚àà{œÄ(i)}
12: end for
Figure2: Pseudo-codeforproceduralandreactivecodingstylesforAIFagents.
In a reactive inference setting, the appropriate planning horizon is going to be
continually updated (inferred) with contextual information. In other words, it
is the reactive FEM process itself that leads to optimizing the inference control
flow.
3.3 RMP for robustness
Since an AIF agent executes under situated conditions, it must perform the FE
minimization process robustly in real-time. Consider an agent whose computa-
tional resources are represented by a graph and FE minimization results from
executing MP-based inference on that graph. Any MP schedule that visits the
nodes in the graph in a prescribed fixed order (as would be the case in a proce-
dural approach to FE minimization) is vulnerable to malfunction in any of the
nodes in the schedule. In principle, the FE minimization process needs to stop
after such a malfunction and proceed to compute a new MP schedule. Since
FEminimizationistheonly ongoingcomputationalprocess,therobotbasically
moves blindfolded after a reset. Clearly, for robustness, we need a system that
continues to minimize FE, even after parts of the graph break down over time.
In a reactive inference framework, collapse of a component is simply a switch
toanalternativemodelstructure. Thenewmodelmayperformbetterorworse
at FE minimization, but there is no reason to stop processing.
3.4 RMP for real-time, situated processing
An ongoing RMP process can always be interrupted when computational re-
sourceshaverunoutonagivenplatform. Inthisway,bytradingcomputational
8
complexity (i.e., the number of messages) for accuracy, any RMP-based infer-
ence process can be scaled down to a real-time processing procedure, where of
courseapredictionaccuracypricemayhavetobepaid,dependingontheavail-
able computational resources. In short, FE minimization in any model can be
executed in real-time on any computational platform if we implement inference
by RMP in a factor graph.
3.5 RMP for low power consumption
Similarly, an ongoing RMP process can always be terminated if the expected
improvement in accuracy does not outweigh the expected computational load
that additional messages would incur.2 Note that, since FE decomposes as
computationalcomplexityminusaccuracy,interruptinganRMP-basedinference
process for this reason is fully consistent with the goal of FE minimization.
Interrupting an ongoing MP process by any of the above-mentioned rea-
sons (e.g., node malfunction, running out of computational resources, expected
processing costs outweighing expected accuracy gains, etc.), in principle always
leads to sacrificing some prediction accuracy in favor of saving computational
costs. Crucially,theseinterruptswillnotcauseasystem-widecrashinareactive
system.
4 Model Structure Adaptation
Insection2.2,wetoucheduponthenotionthatFEminimizationshouldideally
drive the generative model p to evolve to structurally segregated but communi-
cating sub-models that reflect the causal structure of the environment. Techni-
cally, this is due to the drive for a lower surprise (‚àílogp(x)).
Thereisanotherreasonwhyonlinestructuraladaptationisimportant. Free
energyminimizationoverthestructureofpshouldalsoleadtoamodelstructure
for which inference costs D [q(z)||p(z|x)] are lower by moving p(z|x) closer to
KL
q(z). Consider again the procedural and reactive inference code in Fig. 2. The
control flow in the procedural code aims to cleverly steer the inference process
toward maximal inference accuracy for minimal computational costs. In con-
trast, the reactive code just declares that the system should react (by message
passing)toanyFEminimizationopportunity. Inthereactiveframework,clever
inference is learned over time by continual minimization over all movable parts
of the CBFE, i.e., by FEM over states, parameters, structure (adaptation of p),
and constraints (adaptation of the structure of q). To learn the most effective
paths for inference, the toolbox should support structural adaptation over both
p and q.
Unfortunately, online structural adaptation during the deployment of the
robot is still an ongoing research issue, e.g., [8, 15, 3]. One technical difficulty
2The computational load and complexity can only be equated in the absence of a Von
Neumannbottleneck(i.e.,withmortalcomputationorin-memoryprocessing). Thisisbecause
energyandtimeare‚Äòwasted‚Äôbyreadingandwritingtomemory.
9
is that an efficient inference control flow (which states are updated at what
time, etc.) may change if the structure of the generative model changes. In a
procedural programming style, we would need to reset the system and repro-
gram the inference code in Algorithm-1 (in Fig. 2). This is incompatible with
the demand that the agent adapts during deployment. As discussed above, a
reactiveprogrammingstylesolvesthisissuesincetheapplicationinferencecode
(Algorithm-2 in Fig. 2) is independent of the model structure.
5 Discussion
5.1 Review of arguments
Weshortlysummarizeourviewonaprofessional-levelsupportingsoftwaretool-
box for the design of relevant AIF agents, see also Table 1. In section 2, we
discussedafewextraordinaryfeaturesthatfollowstraightawayfromcommitting
to free energy minimization as the sole computational mechanism for a future
AI ecosystem as proposed in Friston et al. [11]. First, the FE functional in an
AIF agent can be interpreted as a universal performance criterion that applies
in principle to all problems. If FEM can be extended to structural model adap-
tation, then an AIF agent is naturally able to create and solve sub-problems.
Moreover, by virtue of the decomposition of EFE into a sum of information-
and goal-seeking costs, AIF agents naturally seek out small "smart" data sets.
In terms of FEM implementation, we asserted that useful models are highly
factorized and sparse. Efficient inference in factorized models can always be
described as message passing in a factor graph. In particular, nearly all known
variants of highly efficient message passing algorithms for FEM can be formu-
lated in a single framework as minimizing a Constrained Bethe Free Energy
(CBFE).
Wethenclaimedthatareactive ratherthanproceduralprocessingstrategyis
essential. Reactive message passing-based (RMP) inference is always interrupt-
ible with an inference result, thus supporting guaranteed real-time processing,
which is a hard requirement for AIF agents in the real world. In comparison
realization technology benefits
1 FEP, AIF one solution approach;
smart data
2 reactive message passing low power;
robustness;
real-time
3 structural adaptation problem refinement;
clever inference
Table1: Summaryofbenefitsforsupportingreactivemessagepassingandstruc-
tural adaptation in an AIF agent.
10
to the more common procedural programming approach to FEM, reactive pro-
cessing also improves robustness, resource consumption, and the capability to
make structural changes without the need for resetting the inference process.
This latter feature, support for online structural adaptation is also a vital
feature of a high-quality AIF toolbox. Online structural adaptation leads to
both continual problem representation refinement (by lowering surprise) and to
a more efficient inference process.
5.2 Review of existing tools
Currently, there exists a small but vibrant research community on the devel-
opment of open-source tools for simulating synthetic AIF agents. In this com-
munity, a few supporting packages have been released, including SPM [12],
PyMDP [13] and ForneyLab [6]. The SPM toolbox was originally written by
Karl Friston and colleagues, and has developed into a very large set of tools
and demonstrations for experimental validation of the scientific output of the
UCL team and collaborators. PyMDP is a more recent Python package for
simulating discrete-state POMDP models by Conor Heins, Alexander Tschantz
and a team of collaborators. ForneyLab.jl is a Julia package from BIASlab
(http://biaslab.org) for simulating FE minimization by message passing in
Forney-style factor graphs. Unfortunately, none of the above-mentioned tools
support reactive message passing-based inference. Therefore, we believe that
these tools will serve the community well as AIF prototyping and validation
tools, but they will not scale to support real-time, robust simulation of AIF
agents with commercializable value.
5.3 Reactive message passing with RxInfer
More recently, BIASlab has released the open-source Julia package RxInfer
(http://rxinfer.ml) to support an engineer at Sarah‚Äôs level to develop com-
merciallyrelevantAIFagentsthatminimizeFEbyautomatedreactivemessage
passing in a factor graph [2]. Julia is a modern open-source scientific program-
ming language with roughly the syntax of MATLAB and out-of-the-box speed
of C [4].
The development process of RxInfer focuses on the following priorities:
1. model space coverage
‚Ä¢ RxInfer aims to support reactive message passing-based FEM for a
very large set of freely definable relevant probabilistic models.
2. user experience
‚Ä¢ RxInfer aims to support a busy, competent researcher or developer
who understands probabilistic modeling (but doesn‚Äôt know Julia) to
design and deploy an AIF agent into the world. In particular, a
user-friendlyspecificationofnestedAIFagentsshouldbesupported.
11
3. adaptation
‚Ä¢ RxInfer aims to support continual adaptation by automated FEM
over all movable parts of the CBFE functional, including states, pa-
rameters, structure, and variational constraints.
4. real-time
‚Ä¢ RxInfer aims to process data streams in ‚Äúhard‚Äù real-time, under
situatedconditions,evenforlargemodels. Largermodelsmayleadto
lessaccurateinference(intermsofKL-divergencebetweenvariational
and Bayesian posteriors), but no crashes.
5. low-power
‚Ä¢ RxInferaimstoprocessdatastreamsonany, possiblytime-varying,
power budget. Lower power budgets may lead to less accurate infer-
ence but no crashes.
At the time of writing this paper, RxInfer supports fast and robust auto-
mated CBFE minimization by reactive message passing for states and parame-
ters in a large set of freely definable models. RxInfer processes streaming data
very fast, but not yet guaranteed in hard real-time. User-friendly specifications
of AIF agents will be released this summer. Model structure adaptation is sup-
ported by NUV priors (normal priors with unknown variance) [15], but not yet
by online Bayesian model reduction [3, 8]. RxInfer comes with a large set of
examples and is slated to support the above priority list in the future.
6 Conclusions
SupportedbyRxInferorasimilartoolbox,futureAIengineerswillnolongerde-
sign end-product algorithms, but will instead design the designers (AIF agents)
of production algorithms in short and easy-readable code scripts. Along with
[11], we think that the potential benefits of shared intelligence in ecosystems of
communicating AIF agents are hard to overstate. As we have argued in this
position paper, the required underlying technology for realizing this vision is
very demanding and currently not yet available. Still, we also think it is not
out of reach and is one of the most exciting ongoing research threads in the AI
field.
6.0.1 Acknowledgments
I would like to acknowledge my colleagues at BIASlab (http://biaslab.org)
forthestimulatingworkenvironmentandtheanonymousreviewersforexcellent
feedbackonthedraftversion. Somewordinginthisdocument,suchasfootnote
2, comes straight from a reviewer.
12
References
[1] SemihAkbayrak,IvanBocharov,andBertdeVries.‚ÄúExtendedVariational
MessagePassingforAutomatedApproximateBayesianInference‚Äù.In:En-
tropy 23.7 (July 2021). Number: 7 Publisher: Multidisciplinary Digital
Publishing Institute, p. 815. issn: 1099-4300. doi: 10.3390/e23070815.
url: https://www.mdpi.com/1099-4300/23/7/815 (visited on
05/26/2023).
[2] DmitryBagaevandBertdeVries.‚ÄúReactiveMessagePassingforScalable
BayesianInference‚Äù.In:ScientificProgramming 2023(May27,2023).Pub-
lisher: Hindawi, e6601690. issn: 1058-9244. doi: 10.1155/2023/6601690.
url: https://www.hindawi.com/journals/sp/2023/6601690/ (visited
on 05/28/2023).
[3] JimBeckersetal.PrincipledPruningofBayesianNeuralNetworksthrough
Variational Free Energy Minimization. Oct. 17, 2022. doi: 10.48550/
arXiv.2210.09134. arXiv: 2210.09134[cs,eess]. url: http://arxiv.
org/abs/2210.09134 (visited on 05/26/2023).
[4] Jeff Bezanson et al. ‚ÄúJulia: A Fresh Approach to Numerical Computing‚Äù.
In: SIAM Review 59.1 (Jan. 1, 2017). Publisher: Society for Industrial
and Applied Mathematics, pp. 65‚Äì98. issn: 0036-1445. doi: 10.1137/
141000671. url: https://epubs.siam.org/doi/10.1137/141000671
(visited on 02/03/2022).
[5] Th√©ophileChampion,MarekGrze≈õ,andHowardBowman.‚ÄúRealizingAc-
tive Inference in Variational Message Passing: The Outcome-Blind Cer-
tainty Seeker‚Äù. In: Neural Computation 33.10 (Sept. 16, 2021), pp. 2762‚Äì
2826. issn: 0899-7667. doi: 10.1162/neco_a_01422. url: https://doi.
org/10.1162/neco_a_01422 (visited on 05/26/2023).
[6] MarcoCox,ThijsvandeLaar,andBertdeVries.‚ÄúAfactorgraphapproach
to automated design of Bayesian signal processing algorithms‚Äù. In: Inter-
national Journal of Approximate Reasoning 104 (Jan. 1, 2019), pp. 185‚Äì
204.issn:0888-613X.doi:10.1016/j.ijar.2018.11.002.url:http://
www.sciencedirect.com/science/article/pii/S0888613X18304298
(visited on 11/16/2018).
[7] Distributiveproperty.In:Wikipedia.PageVersionID:1124679546.Nov.29,
2022.url:https://en.wikipedia.org/w/index.php?title=Distributive_
property&oldid=1124679546 (visited on 05/26/2023).
[8] Karl Friston, Thomas Parr, and Peter Zeidman. ‚ÄúBayesian model reduc-
tion‚Äù. In: arXiv:1805.07092 [stat] (May 18, 2018). arXiv: 1805.07092.
url: http://arxiv.org/abs/1805.07092 (visited on 05/28/2018).
[9] Karl Friston et al. ‚ÄúActive inference and epistemic value‚Äù. In: Cogni-
tive Neuroscience 0 (ja Feb. 17, 2015), null. issn: 1758-8928. doi: 10.
1080/17588928.2015.1020053. url: http://dx.doi.org/10.1080/
17588928.2015.1020053 (visited on 02/22/2015).
13
[10] KarlFristonetal.‚ÄúSophisticatedInference‚Äù.In:Neural Computation 33.3
(Mar. 1, 2021), pp. 713‚Äì763. issn: 0899-7667. doi: 10.1162/neco_a_
01351. url: https://doi.org/10.1162/neco_a_01351 (visited on
02/14/2022).
[11] Karl J. Friston et al. Designing Ecosystems of Intelligence from First
Principles. Dec. 2, 2022. doi: 10.48550/arXiv.2212.01354. arXiv:
2212.01354[nlin]. url: http://arxiv.org/abs/2212.01354 (vis-
ited on 12/08/2022).
[12] KarlJ.Fristonetal.SPM12toolbox,http://www.fil.ion.ucl.ac.uk/spm/software/.
2014.
[13] Conor Heins et al. ‚Äúpymdp: A Python library for active inference in dis-
cretestatespaces‚Äù.In:arXiv:2201.03904[cs,q-bio] (Jan.11,2022).arXiv:
2201.03904. url: http://arxiv.org/abs/2201.03904 (visited on
02/03/2022).
[14] Cornelius Lanczos. The Variational Principles of Mechanics. 4th Revised
ed. edition. New York: Dover Publications, Mar. 1, 1986. 464 pp. isbn:
978-0-486-65067-8.
[15] Hans-Andrea Loeliger et al. ‚ÄúOn sparsity by NUV-EM, Gaussian mes-
sage passing, and Kalman smoothing‚Äù. In: 2016 Information Theory and
Applications Workshop (ITA).2016InformationTheoryandApplications
(ITA). La Jolla, CA, USA: IEEE, Jan. 2016, pp. 1‚Äì10. isbn: 978-1-5090-
2529-9. doi: 10.1109/ITA.2016.7888168. url: http://ieeexplore.
ieee.org/document/7888168/ (visited on 07/21/2021).
[16] ƒ∞smail ≈ûen√∂z et al. ‚ÄúVariational Message Passing and Local Constraint
Manipulation in Factor Graphs‚Äù. In: Entropy (Basel, Switzerland) 23.7
(June 24, 2021), p. 807. issn: 1099-4300. doi: 10.3390/e23070807.
[17] Lena Smirnova et al. ‚ÄúOrganoid intelligence (OI): the new frontier in
biocomputing and intelligence-in-a-dish‚Äù. In: Frontiers in Science (2023).
Publisher: Frontiers.
14

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Toward Design of Synthetic Active Inference Agents by Mere Mortals"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ‚ùå BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ‚úÖ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ‚ùå BAD: Repeating the same claim 3+ times with slight variations
   ‚úÖ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
