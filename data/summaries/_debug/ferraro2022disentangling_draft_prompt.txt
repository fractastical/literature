=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Disentangling Shape and Pose for Object-Centric Deep Active Inference Models
Citation Key: ferraro2022disentangling
Authors: Stefano Ferraro, Toon Van de Maele, Pietro Mazzaglia

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2022

Key Terms: shape, energy, ferraro, object, disentangling, state, centric, stefano, space, models

=== FULL PAPER TEXT ===

Disentangling Shape and Pose for
Object-Centric Deep Active Inference Models
Stefano Ferraro, Toon Van de Maele, Pietro Mazzaglia,
Tim Verbelen, and Bart Dhoedt
IDLab, Department of Information Technology
Ghent University - imec
Ghent, Belgium
stefano.ferraro@ugent.be
Abstract. Activeinferenceisafirstprinciplesapproachforunderstand-
ingthebraininparticular,andsentientagentsingeneral,withthesingle
imperative of minimizing free energy. As such, it provides a computa-
tional account for modelling artificial intelligent agents, by defining the
agent’sgenerativemodelandinferringthemodelparameters,actionsand
hidden state beliefs. However, the exact specification of the generative
model and the hidden state space structure is left to the experimenter,
whose design choices influence the resulting behaviour of the agent. Re-
cently,deeplearningmethodshavebeenproposedtolearnahiddenstate
spacestructurepurelyfromdata,alleviatingtheexperimenterfromthis
tedious design task, but resulting in an entangled, non-interpreteable
state space. In this paper, we hypothesize that such a learnt, entangled
state space does not necessarily yield the best model in terms of free
energy, and that enforcing different factors in the state space can yield
a lower model complexity. In particular, we consider the problem of 3D
object representation, and focus on different instances of the ShapeNet
dataset.Weproposeamodelthatfactorizesobjectshape,poseandcat-
egory, while still learning a representation for each factor using a deep
neuralnetwork.Weshowthatmodels,withbestdisentanglementproper-
ties,performbestwhenadoptedbyanactiveagentinreachingpreferred
observations.
Keywords: ActiveInference·ObjectPerception·DeepLearning·Dis-
entanglement.
1 Introduction
In our daily lives, we manipulate and interact with hundreds of objects without
eventhinking.Indoingso,wemakeinferencesaboutanobject’sidentity,location
in space, 3D structure, look and feel. In short, we learn a generative model of
howobjectscomeabout[24].Robotshoweverstilllackthiskindofintuition,and
struggle to consistently manipulate a wide variety of objects [2]. Therefore, in
thiswork,wefocusonbuildingobject-centricgenerativemodelstoequiprobots
2202
peS
61
]VC.sc[
1v79090.9022:viXra
2 S. Ferraro et al.
with the ability to reason about shape and pose of different object categories,
and generalize to novel instances of these categories.
Active inference offers a first principles approach for learning and acting us-
ing a generative model, by minimizing (expected) free energy. Recently, deep
learning techniques were proposed to learn such generative models from high
dimensional sensor data [33,7,27], which paves the way to more complex ap-
plication areas such as robot perception [14]. In particular, Van de Maele et
al.[16,18]introducedobject-centric,deepactiveinferencemodelsthatenablean
agenttoinfertheposeandidentityofaparticularobjectinstance.However,this
modelwasrestrictedtoidentifyuniqueobjectinstances,i.e.“thissugarboxver-
sus that particular tomato soup can”, instead of more general object categories,
i.e. “mugs versus bottles”. This severely limits generalization, as it requires to
learn a novel model for each particular object instance, i.e. for each particular
mug.
In this paper, we further extend upon this line of work, by learning object-
centric models not by object instance, but by object category. This allows the
agent to reduce the number of required object-centric models, as well as to
generalizetonovelinstancesofknownobjectcategories.Ofcourse,thisrequires
theagenttonotonlyinferobjectposeandidentity,butalsothedifferentshapes
that comprise this category. An important research question is then how to
define and factorize the generative model, i.e. do we need to explicitly split
the different latent factors in our model (i.e. shape and pose), or can a latent
structure be learnt purely from data, and to what extent is this learnt latent
structure factorized?
In the brain, there is also evidence for disentangled representations. For in-
stance,processingvisualinputsinprimatesconsistsoftwopathways:theventral
or“what”pathway,whichisinvolvedwithobjectidentificationandrecognition,
and the dorsal or “where” pathway, which processes an object’s spatial loca-
tion [22]. Similarly, Hawkins et al. hypothesize that cortical columns in the neo-
cortexrepresentanobjectmodel,capturingtheirposeinalocalreferenceframe,
encoded by cortical grid cells [8]. This fuels the idea of treating object pose as a
first class citizen when learning an object-centric generative model.
In this paper, we present a novel method for learning object-centric models
for distinct object categories, that promotes a disentangled representation for
shape and pose. We demonstrate how such models can be used for inferring
actions that move an agent towards a preferred observation. We show that a
better pose-shape disentanglement indeed seems to improve performance, yet
further research in this direction is required. In the remainder of the paper we
first give an overview on related work, after which we present our method. We
present some results on object categories of the ShapeNet database [3], and
conclude the paper with a thorough discussion.
Disentangling Shape and Pose 3
2 Related work
Object-centric models.Manytechniqueshavebeenproposedforrepresenting
3D objects using deep neural networks, working with 2D renders [5], 3D voxel
representations [32], point clouds [15] or implicit signed distance function repre-
sentations [23,20,21,28]. However, none of these take “action” into account, i.e.
there is no agent that can pick its next viewpoint.
Disentangled representations. Disentangling the hidden factors of variation
ofadatasetisanlongsoughtfeatureforrepresentationlearning[1].Thiscanbe
encouragedduringtrainingbyrestrictingthecapacityoftheinformationbottle-
neck [9], by penalizing the total correlation of the latent variables [11,4], or by
matchingmomentsofafactorizedprior[13].Ithasbeenshownthatdisentangled
representationsyieldbetterperformanceondown-streamtasks,enablingquicker
learning using fewer examples [29].
Deep active inference. Parameterizing generative models using deep neural
networks for active inference has been coined “deep active inference” [30]. This
enables active inference applications on high-dimensional observations such as
pixelinputs[33,7,27].Inthispaper,weproposeanovelmodelwhichencourages
a disentangled latent space, and we compare with other deep active inference
models such as [33] and [17]. For a more extensive review, see [19].
3 Object-Centric Deep Active Inference Models
Inactiveinference,anagentactsandlearnsinordertominimizeanupperbound
onthenegativelogevidenceofitsobservations,givenitsgenerativemodelofthe
worldi.e.thefreeenergy.Inthissection,wefirstformallyintroducethedifferent
generativemodelsconsideredforouragentsforrepresenting3Dobjects.Nextwe
discuss how we instantiate and train these generative models using deep neural
networks, and how we encourage the model to disentangle shape and pose.
Generative model. We consider the same setup as [18], in which an agent
receives pixel observations o of a 3D object rendered from a certain camera
viewpoint v, and as an action a can move the camera to a novel viewpoint. The
action space is restricted to viewpoints that look at the object, such that the
object is always in the center of the observation.
Figure 1 depicts different possible choices of generative model to equip the
agent with. The first (1a) considers a generic partially observable Markov deci-
sion process (POMDP), in which a hidden state s encodes all information at
t
timestep t to generate observation o . Action a determines together with the
t t
currentstates howthemodeltransitionstoanewstates .Thisamodelcan
t t+1
beimplementedasavariationalautoencoder(VAE)[25,12],asshownin[33,7].A
second option (1b) is to exploit the environment setup, and assume we can also
observe the camera viewpoint v . Now the agent needs to infer the object shape
t
s which stays fixed over time. This resembles the architecture of a generative
query network (GQN), which is trained to predict novel viewpoints of a given a
4 S. Ferraro et al.
Fig.1: Different generative models for object-centric representations, blue nodes
are observed. (a) A generic POMDP model with a hidden state s that is tran-
t
sitioned through actions and which generates the observations. (b) The hidden
statesencodestheappearanceoftheobject,whileactionstransitionthecamera
viewpoint v which is assumed to be observable. (c) Similar as (b), but without
accesstothecameraviewpoint,whichinthiscasehastobeinferredasaseparate
pose latent variable p .
t
scene [6,17]. Finally, in (1c), we propose our model, in which we have the same
structureas(1b),butwithoutaccesstothegroundtruthviewpoint.Inthiscase,
the model needs to learn a hidden latent representation of the object pose in
view p . This also allows the model to learn a different pose representation than
t
a 3D pose in SO(3), which might be more suited. We call this model a VAEsp,
as it is trained in similar vein as (1a), but with a disentangled shape and pose
latent.
VAEsp.Ourmodelisparameterizedbythreedeepneuralnetworks:anencoder
q ,atransitionmodel p ,andadecoder p ,asshowninFigure2.Observations
φ χ ψ
oi ofobjectinstanceiareprocessedbytheencoderq ,thatoutputsabeliefover
φ
a pose latent q (pi|oi) and a shape latent q (si|oi). From the pose distribution
φ t t φ t t
a sample pi is drawn and fed to the transition model p , paired with an action
t χ
a . The output is a belief p (pi |pi,a ). From the transitioned belief a sample
t χ t+1 t t
pi is again drawn which is paired with a shape latent sample si and input to
t+1
the decoder p (oi|pi,si). The output of the decoding process is again an image
ψ t t
oˆi . These models are jointly trained end-to-end by minimizing free energy,
t+1
or equivalently, maximizing the evidence lower bound [18]. More details on the
model architecture and training hyperparameters can be found in Appendix A.
Enforcing disentanglement. In order to encourage the model to encode ob-
ject shape features in the shape latent, while encoding object pose in the pose
latent,weonlyoffertheposelatentp asinputtothetransitionmodel,whereas
t
the decoder uses both the shape and pose. Similar to [10], in order to further
Disentangling Shape and Pose 5
Fig.2:TheproposedVAEsparchitectureconsistsofthreedeepneuralnetworks:
an encoder q , a transition model p , and a decoder p . By swapping the shape
φ χ ψ
latentsamples,weenforcethemodeltodisentangleshapeandposeduringtrain-
ing.
disentangle,werandomlyswaptheshapelatentcodefortwoobjectinstancesat
train time while keeping the same latent pose, refer to Figure 2.
4 Experiments
WetrainourmodelonasubsetoftheShapeNetdataset[3].Inparticular,weuse
renders of 15 instances of the ‘mug’, ‘bottle’,‘bowl’ and ‘can’ categories, train a
separate model for each category, and evaluate on unseen object instances. We
compare our VAEsp approach against a VAE model [33] that has equal amount
oflatentdimensions,butwithoutashapeandposesplit,andaGQN-likemodel
[17], which has access to the ground truth camera viewpoint.
Weevaluatetheperformanceofthethreeconsideredgenerativemodels.First
we look at the reconstruction and prediction quality of the models for unseen
object instances. Next we investigate how good an agent can move the camera
to match a preferred observation by minimizing expected free energy. Finally,
we investigate the disentanglement of the resulting latent space.
One-step Prediction. First, we evaluate all models on prediction quality over
a test set of 500 observations of unseen objects in unseen poses. We provide
eachmodelwithaninitialobservationwhichisencodedintoalatentstate.Next
we sample a random action, predict the next latent state using the transition
model, for which we reconstruct the observation and compare with a ground
truth.Wereportbothpixel-wisemeansquarederror(MSE)andstructuralsim-
ilarity (SSIM) [31] in Table 1. In terms of MSE results are comparable for all
6 S. Ferraro et al.
Table1:One-steppredictionerrors,averagedovertheentiretestset.MSE(lower
the better) and SSIM (higher the better) are considered.
bottle bowl can mug
MSE ⇓ GQN 0.473±0.0874 0.487±0.141 0.707±0.1029 0.656±0.0918
VAE 0.471±0.0824 0.486±0.1487 0.693±0.1103 0.646±0.0886
VAEsp 0.480±0.0879 0.485±0.1486 0.702±0.1108 0.626±0.0915
SSIM ⇑GQN 0.748±0.0428 0.814±0.0233 0.868±0.0203 0.824±0.0279
VAE 0.828±0.0238 0.907±0.0178 0.844±0.0361 0.874±0.0323
VAEsp0.854±0.0190 0.902±0.0291 0.880±0.0176 0.814±0.0348
theproposedarchitectures.IntermsofSSIMhowever,VAEspshowsbetterper-
formance for ‘bottle’ and ‘can’ category. Performance for ‘bowl’ category are
comparabletothebestperformingVAEmodel.Forthe‘mug’category,theneg-
ative gap over the VAE model is consistent. Qualitative results for all models
are shown in Appendix B.
Reaching preferred viewpoints. Next, we consider an active agent that is
tasked to reach a preferred observation that was provided in advance. To do so,
the agent uses the generative model to encode both the preferred and initial
observation and then uses Monte Carlo sampling to evaluate the expected free
energy for 10000 potential actions, after which the action with the lowest ex-
pectedfreeenergyisexecuted.Theexpectedfreeenergyformulationiscomputed
as the negative log probability of the latent representation with respect to the
distribution over the preferred state, acquired through encoding the preferred
observation. This is similar to the setup adopted by Van de Maele et al. [18],
with the important difference that now the preferred observation is an image of
a different object instance.
To evaluate the performance, we compute the pixel-wise mean squared error
(MSE) between a render of the target object in the preferred pose, and the
render of the environment after executing the chosen action after the initial
observation. The results are shown in Table 2. VAEsp performs on par with the
other approaches for ‘bowl’ and ‘mug’, but significantly outperforms the GQN
on the the ‘bottle’ and ‘can’ categories, reflected by p-values of 0.009 and 0.001
Table 2: MSE for the reached pose through the minimization of expected free
energy. For each category, 50 meshes are evaluated, where for each object a
random pose is sampled from a different object as preferred pose, and the agent
should reach this pose.
bottle bowl can mug
GQN 0.0833±0.0580 0.0888±0.0594 0.0806±0.0547 0.1250±0.0681
VAE 0.0698±0.0564 0.0795±0.0599 0.0608±0.0560 0.1247±0.0656
VAEsp0.0557±0.0404 0.0799±0.0737 0.0487±0.03810.1212±0.0572
Disentangling Shape and Pose 7
(a) (b)
Fig.3: Two examples of the experiment on reaching preferred viewpoints for a
‘bottle’(a)anda‘mug’(b).Firstcolumnshowsthetargetview(top)andinitial
viewgiventotheagent(bottom).Next,forthethreemodelsweshowtheactual
reached view (top), versus the imagined expected view of the model (bottom).
for these respective objects. The p-values for the comparison with the VAE are
0.167 and 0.220, which are not significant. A qualitative evaluation is shown
in Figure 3. Here we show the preferred target view, the initial view of the
environment, as well as the final views reached by each of the agents, as well as
what each model was imagining. Despite the target view being from a different
object instance, the agent is able to find a matching viewpoint.
Disentangled latent space.Finally,weevaluatethedisentanglementofshape
and pose for the proposed architecture. Given that our VAEsp model outper-
forms the other models on ‘bottle‘ and ‘can‘, but not on ‘bowl‘ and ‘mug‘, we
hypothesize that our model is able to better disentangle shape and pose for the
first categories, but not for the latter. To evaluate this, we plot the distribution
ofeachlatentdimensionwhenencoding50randomshapesinafixedpose,versus
50randomposesforafixedshape,asshownonFigure4.Weseethatindeedthe
VAEsp model has a much more disentangled latent space for ‘bottle‘ compared
to ‘mug‘, which supports our hypothesis. Hence, it will be interesting to further
experimenttofindacorrelationbetweenlatentspacedisentanglementandmodel
performance.Moreover,wecouldworkonevenbetterenforcingdisentanglement
when training a VAEsp model, for example by adding additional regularization
losses [11,4]. Also note that the GQN does not outperform the other models,
although this one has access to the ground truth pose factor. This might be due
to the fact that an SO(3) representation of pose is not optimal for the model to
process, and it still encodes (entangled) pose information in the resulting latent
space, as illustrated by violin plots for GQN models in Appendix C. Figure 5
qualitatively illustrates the shape and pose disentanglement for our best per-
formingmodel(bottle).Weplotreconstructionsoflatentcodesconsistingofthe
shape latent of the first column, combined with the pose latent of the first row.
8 S. Ferraro et al.
(a) VAEsp bottle
(b) VAEsp mug
Fig.4: Violin plots representing the distribution over the latent dimension when
keeping either the pose or shape fixed. For the bottle model (a) the pose latent
dimensions (0-7) vary when only varying the pose, whereas the shape latent
dimensions (8-23) don’t vary with the pose. For the mug model (b) we see the
shape and pose latent are much more entangled.
Fig.5: Qualitative experimentation for the bottle category. Images are recon-
structed from the different pairings of the pose latent and shape latent of the
first row and column respectively.
Disentangling Shape and Pose 9
5 Conclusion
In this paper, we proposed a novel deep active inference model for learning
object-centric representations of object categories. In particular, we encourage
the model to have a disentangled pose and shape latent code. We show that
the better our model disentangles shape and pose, the better the results are on
prediction,reconstructionaswellasactionselectiontowardsapreferredobserva-
tion.Asfuturework,wewillfurtherourstudyontheimpactofdisentanglement,
and how to better enforce disentanglement in our model. We believe that this
lineofworkisimportantforroboticmanipulationtasks,i.e.wherearobotlearns
to pick up a cup by the handle, and can then generalize to pick up any cup by
reaching to the handle.
References
1. Bengio,Y.,Courville,A.,Vincent,P.:Representationlearning:Areviewandnew
perspectives. IEEE transactions on pattern analysis and machine intelligence 35,
1798–1828 (08 2013). https://doi.org/10.1109/TPAMI.2013.50
2. Billard,A.,Kragic,D.:Trendsandchallengesinrobotmanipulation.Science364,
eaat8414 (06 2019). https://doi.org/10.1126/science.aat8414
3. Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z.,
Savarese, S., Savva, M., Song, S., Su, H., Xiao, J., Yi, L., Yu, F.: ShapeNet:
An Information-Rich 3D Model Repository. Tech. Rep. arXiv:1512.03012 [cs.GR],
Stanford University — Princeton University — Toyota Technological Institute at
Chicago (2015)
4. Chen, R.T.Q., Li, X., Grosse, R., Duvenaud, D.: Isolating sources of disentan-
glement in vaes. In: Proceedings of the 32nd International Conference on Neural
Information Processing Systems. p. 2615–2625. NIPS’18, Curran Associates Inc.,
Red Hook, NY, USA (2018)
5. Dosovitskiy, A., Springenberg, J.T., Tatarchenko, M., Brox, T.: Learning to
generate chairs, tables and cars with convolutional networks. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence 39(4), 692–705 (2017).
https://doi.org/10.1109/TPAMI.2016.2567384
6. Eslami, S.M.A., Jimenez Rezende, D., Besse, F., Viola, F., Morcos, A.S., Gar-
nelo, M., Ruderman, A., Rusu, A.A., Danihelka, I., Gregor, K., Reichert, D.P.,
Buesing, L., Weber, T., Vinyals, O., Rosenbaum, D., Rabinowitz, N., King,
H., Hillier, C., Botvinick, M., Wierstra, D., Kavukcuoglu, K., Hassabis, D.:
Neural scene representation and rendering. Science 360(6394), 1204–1210 (Jun
2018). https://doi.org/10.1126/science.aar6170, https://www.science.org/doi/
10.1126/science.aar6170
7. Fountas,Z.,Sajid,N.,Mediano,P.,Friston,K.:Deepactiveinferenceagentsusing
monte-carlo methods. In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.F.,
Lin, H. (eds.) Advances in Neural Information Processing Systems. vol. 33, pp.
11662–11675. Curran Associates, Inc. (2020)
8. Hawkins, J., Ahmad, S., Cui, Y.: A Theory of How Columns in the Neocor-
tex Enable Learning the Structure of the World. Frontiers in Neural Circuits
11, 81 (Oct 2017). https://doi.org/10.3389/fncir.2017.00081, http://journal.
frontiersin.org/article/10.3389/fncir.2017.00081/full
10 S. Ferraro et al.
9. Higgins, I., Matthey, L., Pal, A., Burgess, C.P., Glorot, X., Botvinick, M.M., Mo-
hamed, S., Lerchner, A.: beta-vae: Learning basic visual concepts with a con-
strainedvariationalframework.In:5thInternationalConferenceonLearningRep-
resentations,ICLR2017,Toulon,France,April24-26,2017,ConferenceTrackPro-
ceedings (2017)
10. Huang, X., Liu, M.Y., Belongie, S., Kautz, J.: Multimodal unsupervised image-
to-image translation. In: Proceedings of the European Conference on Computer
Vision (ECCV) (September 2018)
11. Kim,H.,Mnih,A.:Disentanglingbyfactorising.In:Dy,J.,Krause,A.(eds.)Pro-
ceedings of the 35th International Conference on Machine Learning. Proceedings
of Machine Learning Research, vol. 80, pp. 2649–2658. PMLR (10–15 Jul 2018)
12. Kingma,D.P.,Welling,M.:Auto-EncodingVariationalBayes.arXiv:1312.6114[cs,
stat] (May 2014), http://arxiv.org/abs/1312.6114, arXiv: 1312.6114
13. Kumar, A., Sattigeri, P., Balakrishnan, A.: Variational inference of disentangled
latent concepts from unlabeled observations. In: 6th International Conference on
LearningRepresentations,ICLR2018,Vancouver,BC,Canada,April30-May3,
2018, Conference Track Proceedings (2018)
14. Lanillos,P.,Meo,C.,Pezzato,C.,Meera,A.A.,Baioumy,M.,Ohata,W.,Tschantz,
A., Millidge, B., Wisse, M., Buckley, C.L., Tani, J.: Active inference in robotics
and artificial agents: Survey and challenges (2021)
15. Lin, C.H.,Kong,C.,Lucey, S.:Learningefficient point cloudgenerationfordense
3d object reconstruction. In: Proceedings of the Thirty-Second AAAI Conference
on Artificial Intelligence and Thirtieth Innovative Applications of Artificial In-
telligence Conference and Eighth AAAI Symposium on Educational Advances in
Artificial Intelligence. AAAI’18/IAAI’18/EAAI’18, AAAI Press (2018)
16. Van de Maele, T., Verbelen, T., Catal, O., Dhoedt, B.: Disentangling What
and Where for 3D Object-Centric Representations Through Active Inference.
arXiv:2108.11762 [cs] (Aug 2021), http://arxiv.org/abs/2108.11762, arXiv:
2108.11762
17. Van de Maele, T., Verbelen, T., C¸atal, O., De Boom, C., Dhoedt, B.: Active
Vision for Robot Manipulators Using the Free Energy Principle. Frontiers in
Neurorobotics15,642780(Mar2021).https://doi.org/10.3389/fnbot.2021.642780,
https://www.frontiersin.org/articles/10.3389/fnbot.2021.642780/full
18. Van de Maele, T., Verbelen, T., C¸atal, O., Dhoedt, B.: Embodied ob-
ject representation learning and recognition. Frontiers in Neurorobotics 16
(2022). https://doi.org/10.3389/fnbot.2022.840658, https://www.frontiersin.
org/article/10.3389/fnbot.2022.840658
19. Mazzaglia, P., Verbelen, T., C¸atal, O., Dhoedt, B.: The free energy princi-
ple for perception and action: A deep learning perspective. Entropy 24(2)
(2022). https://doi.org/10.3390/e24020301, https://www.mdpi.com/1099-4300/
24/2/301
20. Mescheder, L., Oechsle, M., Niemeyer, M., Nowozin, S., Geiger, A.: Occupancy
Networks: Learning 3D Reconstruction in Function Space. arXiv:1812.03828 [cs]
(Apr 2019), http://arxiv.org/abs/1812.03828, arXiv: 1812.03828
21. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R.,
Ng, R.: NeRF: Representing Scenes as Neural Radiance Fields for View Synthe-
sis.arXiv:2003.08934[cs](Aug2020),http://arxiv.org/abs/2003.08934,arXiv:
2003.08934
22. Mishkin, M., Ungerleider, L.G., Macko, K.A.: Object vision and spa-
tial vision: two cortical pathways. Trends in Neurosciences 6, 414–417
Disentangling Shape and Pose 11
(Jan1983).https://doi.org/10.1016/0166-2236(83)90190-x,https://doi.org/10.
1016/0166-2236(83)90190-x
23. Park, J.J., Florence, P., Straub, J., Newcombe, R., Lovegrove, S.: DeepSDF:
Learning Continuous Signed Distance Functions for Shape Representation.
arXiv:1901.05103 [cs] (Jan 2019), http://arxiv.org/abs/1901.05103, arXiv:
1901.05103
24. Parr, T., Sajid, N., Da Costa, L., Mirza, M.B., Friston, K.J.: Genera-
tive Models for Active Vision. Frontiers in Neurorobotics 15, 651432 (Apr
2021). https://doi.org/10.3389/fnbot.2021.651432, https://www.frontiersin.
org/articles/10.3389/fnbot.2021.651432/full
25. Rezende, D.J., Mohamed, S., Wierstra, D.: Stochastic Backpropagation and Ap-
proximate Inference in Deep Generative Models. arXiv:1401.4082 [cs, stat] (May
2014), http://arxiv.org/abs/1401.4082, arXiv: 1401.4082
26. Rezende, D.J., Viola, F.: Taming VAEs. arXiv:1810.00597 [cs, stat] (Oct 2018),
http://arxiv.org/abs/1810.00597, arXiv: 1810.00597
27. Sancaktar,C.,vanGerven,M.A.J.,Lanillos,P.:End-to-endpixel-baseddeepactive
inferenceforbodyperceptionandaction.2020JointIEEE10thInternationalCon-
ference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)
(Oct 2020). https://doi.org/10.1109/icdl-epirob48136.2020.9278105, http://dx.
doi.org/10.1109/ICDL-EpiRob48136.2020.9278105
28. Sitzmann, V., Martel, J.N.P., Bergman, A.W., Lindell, D.B., Wetzstein, G.:
SIREN: Implicit Neural Representations with Periodic Activation Functions.
arXiv:2006.09661[cs,eess](Jun2020),http://arxiv.org/abs/2006.09661,arXiv:
2006.09661
29. van Steenkiste, S., Locatello, F., Schmidhuber, J., Bachem, O.: Are dis-
entangled representations helpful for abstract visual reasoning? In: Wallach,
H., Larochelle, H., Beygelzimer, A., d'Alch´e-Buc, F., Fox, E., Garnett, R.
(eds.) Advances in Neural Information Processing Systems. vol. 32. Curran
Associates, Inc. (2019), https://proceedings.neurips.cc/paper/2019/file/
bc3c4a6331a8a9950945a1aa8c95ab8a-Paper.pdf
30. Ueltzh¨offer, K.: Deep active inference. Biol. Cybern. 112(6), 547–573 (Dec
2018). https://doi.org/10.1007/s00422-018-0785-7, https://doi.org/10.1007/
s00422-018-0785-7
31. Wang, Z., Bovik, A., Sheikh, H., Simoncelli, E.: Image quality assessment: from
error visibility to structural similarity. IEEE Transactions on Image Processing
13(4), 600–612 (2004). https://doi.org/10.1109/TIP.2003.819861
32. Wu, J., Zhang, C., Xue, T., Freeman, B., Tenenbaum, J.: Learning a prob-
abilistic latent space of object shapes via 3d generative-adversarial mod-
eling. In: Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., Garnett, R.
(eds.) Advances in Neural Information Processing Systems. vol. 29. Curran
Associates, Inc. (2016), https://proceedings.neurips.cc/paper/2016/file/
44f683a84163b3523afe57c2e008bc8c-Paper.pdf
33. C¸atal, O., Wauthier, S., De Boom, C., Verbelen, T., Dhoedt, B.: Learning Gen-
erative State Space Models for Active Inference. Frontiers in Computational
Neuroscience14,574372(Nov2020).https://doi.org/10.3389/fncom.2020.574372,
https://www.frontiersin.org/articles/10.3389/fncom.2020.574372/full
12 S. Ferraro et al.
A Model and training details
This paper compares three generative models for representing the shape and
poseofanobject.Eachofthemodelshasalatentdistributionof24dimensions,
parameterized as a Gaussian distribution and has a similar amount of total
trainable parameters.
VAE: The VAE baseline is a traditional variational autoencoder. The en-
coder consists of 6 convolutional layers with a kernel size of 3, a stride of 2 and
padding of 1. The features for each layer are doubled every time, starting with
4 for the first layer. After each convolution, a LeakyReLU activation function is
applied to the data. Finally, two linear layers are used on the flattened output
from the convolutional pipeline, to directly predict the mean and log variance
of the latent distribution. The decoder architecture is a mirrored version of the
encoder. It consists of 6 convolutional layers with kernel size 3, padding 1 and
stride 1. The layers have 32, 8, 16, 32 and 64 output features respectively. After
eachlayertheLeakyReLUactivationfunctionisapplied.Thedataisdoubledin
spatial resolution before each such layer through bi-linear upsampling, yielding
a 120 by 120 image as final output. A transition model is used to predict the
expectedlatentafterapplyinganaction.Thismodelisparameterizedthrougha
fully connected neural network, consisting of three linear layers, where the out-
putfeaturesare64,128and128respectively.Theinputistheconcatenationofa
latentsample,anda7Drepresentationoftheaction(coordinateandorientation
quaternion). The output of this layer is then again through two linear layers
transformed in the predicted mean and log variance of the latent distribution.
This model has 474.737 trainable parameters.
GQN: The GQN baseline only consists of an encoder and a decoder. As the
modelisconditionedontheabsoluteposeofthenextviewpoint,thereisnoneed
for a transition model. The encoder is parameterized exactly the same as the
encoder of the VAE baseline. The decoder is now conditioned on both a latent
sample and the 7D representation of the absolute viewpoint (coordinate and
orientation quaternion). These are first concatenated and transformed through
a linear layer with 128 output features. This is then used as a latent code for
the decoder, which is parameterized the same as the decoder used in the VAE
baseline. In total, the GQN has 361.281 trainable parameters.
VAEsp: Similar to the VAE baseline, the VAEsp consists of an encoder,
decoder and transition model. The encoder is also a convolutional neural net-
work, parameterized the same as the encoder of the VAE, except that instead
of two linear layers predicting the parameters of the latent distribution, this
model contains 4 linear layers. Two linear layers with 16 output features are
used to predict the mean and log variance of the shape latent distribution, and
two linear layers with 8 output features are used to predict the mean and log
variance of the pose latent distribution. In the decoder, a sample from the pose
andshapelatentdistributionsareconcatenatedanddecodedthroughaconvolu-
tional neural network, parameterized exactly the same as the decoder from the
VAEbaseline.Thetransitionmodel,onlytransitionstheposelatent,aswemake
the assumption that the object shape does not change over time. The transition
Disentangling Shape and Pose 13
model is parameterized the same as the transition model of the VAE, with the
exception that the input is the concatenation of the 8D pose latent vector and
the 7D action, in contrast to the 24D latent in the VAE. The VAEsp model has
464.449 trainable parameters.
Allmodelsaretrainedusingaconstrainedloss,whereLagrangianoptimizers
are used to weigh the separate terms [26]. During training, we tuned the recon-
struction tolerance for each object empirically. Respectively to ’bottle’, ’bowl’,
’can’ and ’mug’ categories, MSE tolerances are: 350, 250, 280 and 520. Regular-
ization terms are considered for each latent element. For all models, the Adam
optimizer was used to minimize the objective.
B Additional qualitative results
(a) bottle (b) bowl
(c) can (d) mug
Fig.6: One-step prediction for different object categories.
C Latent disentanglement
In Figures 7, 8, 9 and 10, we show the distribution over the latent values when
encodingobservationwhereasingleinputfeaturechanges.Theblueviolinplots
14 S. Ferraro et al.
representthedistributionoverthelatentvaluesforobservationswheretheshape
is kept fixed, and renders from different poses are fed through the encoder. The
orange violin plots represent the distribution over the latent values for observa-
tions where the pose is kept fixed, and renders from different shapes within the
object class are encoded through the encoder models.
In these figures, we can clearly see that the encoding learnt by the VAE is
not disentangled for any of the objects as the latent dimensions vary for both
the fixed shape and pose cases. With the GQN, we would expect that the latent
dimensionswouldremainstaticforthefixedshapecase,astheposeisanexplicit
external signal for the decoder, however we can see that for a fixed shape, the
variationoverthelatentvaluestillvariesalot,insimilarfashionasforthefixed
pose. We conclude that the encoding of the GQN is also not disentangled. For
the VAEsp model, we can see that in Figures 7 and 8, the first eight dimensions
are used for the encoding of the pose, as the orange violins are much denser
distributedforthefixedposecase.However,inFigures9and10,weseethatthe
model still shows a lot of variety for the latent codes describing the non-varying
feature of the input. This result also strokes with our other experiments where
for these objects both reconstruction as well as the move to perform worse.
In this paper, we investigated the disentanglement for the different consid-
ered object classes. We see that our approach does not yield a disentangled
representationeachtime.Furtherinvestigationandresearchwillfocusonbetter
enforcing this disentanglement.
Disentangling Shape and Pose 15
(a) VAE bottle
(b) GQN bottle
(c) VAEsp bottle
Fig.7: Distribution of the latent values for the different models (VAE, GQN
and VAEsp) for objects from the “bottle” class. In this experiment, 50 renders
from a fixed object shape with a varying pose (fixed shape, marked in blue) are
encoded.Theorangeviolinplotsrepresentthedistributionoverthelatentvalues
for 50 renders from the same object pose, with a varying object shape.
16 S. Ferraro et al.
(a) VAE can
(b) GQN can
(c) VAEsp can
Fig.8:Distributionofthelatentvaluesforthedifferentmodels(VAE,GQNand
VAEsp) for objects from the “can” class. In this experiment, 50 renders from a
fixedobjectshapewithavaryingpose(fixedshape,markedinblue)areencoded.
The orange violin plots represent the distribution over the latent values for 50
renders from the same object pose, with a varying object shape.
Disentangling Shape and Pose 17
(a) VAE mug
(b) GQN mug
(c) VAEsp mug
Fig.9:Distributionofthelatentvaluesforthedifferentmodels(VAE,GQNand
VAEsp) for objects from the “mug” class. In this experiment, 50 renders from a
fixedobjectshapewithavaryingpose(fixedshape,markedinblue)areencoded.
The orange violin plots represent the distribution over the latent values for 50
renders from the same object pose, with a varying object shape.
18 S. Ferraro et al.
(a) VAE bowl
(b) GQN bowl
(c) VAEsp bowl
Fig.10: Distribution of the latent values for the different models (VAE, GQN
and VAEsp) for objects from the “bowl” class. In this experiment, 50 renders
from a fixed object shape with a varying pose (fixed shape, marked in blue) are
encoded.Theorangeviolinplotsrepresentthedistributionoverthelatentvalues
for 50 renders from the same object pose, with a varying object shape.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Disentangling Shape and Pose for Object-Centric Deep Active Inference Models"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
