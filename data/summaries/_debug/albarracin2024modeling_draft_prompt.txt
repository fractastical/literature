=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Modeling Sustainable Resource Management using Active Inference
Citation Key: albarracin2024modeling
Authors: Mahault Albarracin, Ines Hipolito, Maria Raffa

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2024

Key Terms: sustainable, university, resource, modeling, behavior, management, inference, active, agent, sustainability

=== FULL PAPER TEXT ===

1
Modeling Sustainable Resource Management
using Active Inference
Mahault Albarracin1 , Ines Hipolito2 , Maria Raffa3 , Paul
Kinghorn4
1Université du Québec à Montréal, Montréal, Canada
2Macquarie University, Sydney, Australia
3IULM University, Milan, Italy
4Department of Informatics and Engineering, University of Sussex, Brighton, UK
Abstract. Active inference helps us simulate adaptive behavior and
decision-makinginbiologicalandartificialagents.Buildingonourprevi-
ousworkexploringtherelationshipbetweenactiveinference,well-being,
resilience, and sustainability, we present a computational model of an
agentlearningsustainableresourcemanagementstrategiesinbothstatic
anddynamicenvironments.Theagent’sbehavioremergesfromoptimiz-
ingitsownwell-being,representedbypriorpreferences,subjecttobeliefs
aboutenvironmentaldynamics.Inastaticenvironment,theagentlearns
to consistently consume resources to satisfy its needs. In a dynamic en-
vironment where resources deplete and replenish based on the agent’s
actions, the agent adapts its behavior to balance immediate needs with
long-term resource availability. This demonstrates how active inference
cangiverisetosustainableandresilientbehaviorsinthefaceofchanging
environmental conditions. We discuss the implications of our model, its
limitations, and suggest future directions for integrating more complex
agent-environment interactions. Our work highlights active inference’s
potential for understanding and shaping sustainable behaviors.
Keywords: Active Inference · Sustainability · Generative model.
1 Introduction
For the past decade, we have shown that the Free Energy Principle can serve
as a foundational concept in predicting and modeling the present and future
behaviors of a system. Under this principle, the behavior of a system aims to
maintainequilibriumandsustaininglifethroughtheminimizationoffreeenergy
[Parr et al., 2022, Parr and Friston, 2019, Stubbs and Friston, 2024]. Systems,
particularly biological ones, act to minimize the difference between their repre-
sentation of the world, encoded in internal states and the external environment.
Byreducingthisdiscrepancy,asquantifiedbyfreeenergy,thesystemachievesa
state of balance and effectively adapts to its surroundings [Friston et al., 2017,
2023,Parretal.,2023].Wecanthusunderstanditasameasureofuncertaintyor
4202
nuJ
11
]IA.sc[
1v39570.6042:viXra
2
surprise, used such that agents are driven to more predictable and stable states.
Using free energy, systems can slowly make adjustments and adaptations, and
thus maintain homeostasis in a changing environment [Ramstead et al., 2018,
Kirchhoff et al., 2018, Karl, 2012, Da Costa et al., 2023, Pezzulo et al., 2024].
This modeling approach has been used for various types of systems, from neu-
ral processes and cognitive functions to broader ecological and social dynamics
[Friston et al., 2010, Da Costa et al., 2024, Solymosi and Schulkin, 2024, Al-
barracin et al., 2024a, Matsumura et al., 2023, Montgomery and Hipólito, 2023,
Ramstead et al., 2020, Pezzulo et al., 2024].
Whileitisawidelyheldassumptionthatallsystemswillinvariablyminimize
free energy (FE), this is not always a simple linear process. To understand this,
we have to consider the system’s goals and constraints. These goals can some-
times result in behaviors that do not align perfectly with immediate free energy
minimization. This is partly what can make a system, given a specific scale of
measurement,somewhatunsustainable.Notallsystemsarecapableofeffectively
minimizing free energy. They may indeed have constraints in their structure or
function. Think of certain pathological conditions impeding a system’s ability
to minimize free energy efficiently. These conditions can lead to maladaptive
behaviors or states that deviate significantly from what would be predicted by
the FEP. For example, constraints in structure or function: in neurological dis-
orders such as schizophrenia, the brain’s ability to minimize free energy can be
impaired. As it distorts connectivity, it may also alter perceptions and thoughts
- no longer fully related to the external world. Someone with Schizophrenia can
struggle to reduce uncertainty about its environment, resulting in maladaptive
behaviors [Friston et al., 2016, Harikumar et al., 2023, Zarghami et al., 2023].
Free energy minimization can also be influenced by external perturbations
and environmental factors. The very nature of the environment is unpredictable
dynamics, which temporarily disrupt a system’s meta-stable states. The system
transientlyincreasesfreeenergyasitadaptstonewconditions.Albarracinetal.
(2024)explorehowsystemsmustdealwithexternalshocksandstresses(pertur-
bations) to maintain sustainability, resilience and well-being. They suggest that
resilience means absorbing shocks and stresses from the environment, while sus-
tainabilityrequiresenduringcapacitytostayresilient,butwithoutcausingaloss
ofresilienceoftheenvironmentsuper-system.Inthisparadigm,externalpertur-
bations are central to developing better strategies to maintain well-being across
system strata. Since these perturbations can be unpredictable, the temporality
ofstrategiescanchange.Long-termstrategiescanweatherslightincreasesinfree
energytemporarilytoachievemorestableandfavorableconditionsinthefuture.
Thisisthecasewewillbetestingandpresentinginthispaper:long-termstrate-
gies involving temporary increases in free energy. When an agent learns that it
does not have to satisfy its greed immediately, even if it is very hungry, because
the aim is to maintain a balance between itself and the environment (such as a
room with food) over time, the agent can resist the urge for immediate gratifi-
cation and managing its resources judiciously. And thus, the agent can endure
3
short-term discomfort (increased free energy) to ensure long-term stability and
sustainability.
To do so, we test out two cases, detailed in the Methods section. Case 1
acts as a baseline scenario, and involves a static environment where the agent
decides whether to eat food or not. In Case 2, the environment is dynamic, and
the agent must learn to moderate its consumption behavior over time. Food
increases when the agent does not eat, introducing a dynamic aspect to the en-
vironment. This study is important for two reasons. First, we must understand
adaptive strategies to properly predict when systems will achieve long-term sta-
bility and sustainability. It will help us predict when systems choose to balance
short-term needs with long-term goals. But this will also help us identify po-
tential vulnerabilities, such that we can identify areas where intervention may
be needed to prevent collapse or dysfunction. The FEP dictates that behavior
should align with minimizing free energy. But we have to better understand the
variabilityinpathswherethisprincipleisn’tconsistentlyupheldatagivenscale
of measurement.
2 Methods
2.1 Case 1: Static environment
We build this simulation using the PyMDP package, by Conor Heins and col-
leagues [Heins et al., 2022]. In Case 1, we consider a static environment where
the agent’s goal is to maintain satiety by deciding whether to eat the available
food. The generative model for Case 1 (detailed in Table 1, and visualized in
figure 1) includes hidden states for food availability and agent satiety, observa-
tions that directly correspond to these hidden states, and actions to eat or not
eat. The Likelihood Matrix (A) assumes an identity mapping between hidden
statesandobservations,meaningthattheagentdirectlyobservesthetruestates
of food availability and its own satiety. Mathematically, this is represented as
P(o | s ,A) = Cat(A), where A is an identity matrix. This means the proba-
t t
bility of an observation given a state is 1 if they correspond and 0 otherwise:
(cid:40)
1 if i=j
P(o =i|s =j)=
t t 0 if i̸=j
The agent performs variational inference to optimize an approximate poste-
rior Q(s ) over hidden states at each timestep, using the expected log likelihood
t
of observations E[logP(o |s )]=Q(s )T logA.
t t t
The Transition Matrix (B) specifies that the "eat" action leads to satiety
when food is present, while food remains constantly available regardless of the
agent’sactions.The"don’teat"actionleadstohunger.Thetransitionlikelihood
B is represented as a set of matrices B[f], one for each hidden state factor f,
withdimensions S ×S ×U ,where S isthenumberoflevels forfactorf and
f f f f
U is the number of control states or actions for that factor.
f
4
Fig.1:Theagent’sgenerativemodelencodesbeliefsaboutthecausalstructureof
theenvironmentandhowitsactionsaffectthestateoftheworld.Thetruestate
of the environment is represented by two hidden state factors - the availability
of food (s1) and the agent’s satiety (s2). The prior preference C matrix specifies
the agent’s innate drives or goals, in this case a strong preference for being
satiated. The starting conditions are specified by the initial state distribution,
D. Here, food is initially present but the agent is not satiated. The agent has
two observation modalities - the presence of food (o1) and its own satiety level
(o2). The agent can select between two actions at each time step - "eat" or "do
not eat". We have two hidden state factors: food left and satiety. For the "do
not eat" action, for Case 1 the B matrix is an identity matrix, as this action
does not change the state, while for Case 2 it changes, since not eating leads to
an increase in available food. When the agent chooses the "eat" action, if food
is present, the states will transition by reducing "food left" by 1 (down to a
minimum of 0) state and by increasing "satiety" by 1 (to a maximum of 2).
5
The entry B[f][i,j,k] represents the probability of transitioning from state
j to state i for factor f, given action k: P(sf = i | sf = j,uf = k). In this
t+1 t t
case, the "eat" action (k = 0) would have a high probability of leading to the
"satiated" state (i) when the current state is "food available" (j), while the
"don’t eat" action (k =1) would likely lead to the "hungry" state.
Importantly, the transition matrices B[f] are assumed to be conditionally
independentacrossfactors,meaningthatthenextstateoffactorf onlydepends
on the current state and action for that factor, and not on the states of other
factors: P(sf | sf,uf) = P(sf | s ,u ). This simplifies the computation of
t+1 t t t+1 t t
the joint transition probability.
The Preference Vector (C) encodes a strong preference to observe satiation
and food present. The agent’s goals and preferences are represented as a prior
distribution over observations, P(o ). The C vector encodes these preferences
1:T
as a categorical distribution, where higher values correspond to preferred obser-
vations.Theagentaimstomaximizetheprobabilityofsamplingthesepreferred
observations.
The Initial State Distribution (D) is not specified, so that it is a uniform
distribution where each state has an equal probability of being the initial state.
In the PyMDP framework, the initial state distribution is represented as a cate-
goricaldistributionoverhiddenstatesatthefirsttimestep,P(s |D)=Cat(D).
1
Ifnotspecified,itdefaultstoauniformdistribution,assigningequalprobability
to all possible initial states.
Duringthegenerativeprocess,theagentinteractswiththeenvironment,and
its actions affect the state transitions according to the generative process. If the
agent chooses not to eat, the state of the environment remains unchanged. If
the agent chooses to eat and food is present, the agent becomes satiated, but
food remains available due to the static nature of the environment. We then
instantiate the simulation loop. First, the agent performs state inference based
onthecurrentobservation,evaluatingpoliciestomaximizeexpectedfreeenergy,
andselectinganactionthatminimizesfreeenergyandalignswithitspreferences.
The selected action is applied to the environment, resulting in state transitions
andnewobservations,andtheloopcontinueswiththeagentupdatingitsbeliefs,
inferring policies, and selecting actions to achieve its goals. In the extension to
Case 1, we introduce variations to test the agent’s adaptability. In Case 1.1, we
setincorrectAandBmatrices,introducingflawedperceptionsandbeliefsabout
state transitions. This extension is intended for us to validate that the behavior
oftheagentisinfactpredicatedonitsappropriateappraisaloftheenvironment.
2.2 Case 2: Dynamic Environment
In Case 2 (laid out in table 2), we extend our model to a dynamic environment
wheretheagent’sactionshaveconsequencesontheavailabilityoffoodresources
over time. The goal is to study how the agent adopts a sustainable behavior,
balancing its immediate need for satiety with the long-term availability of food.
The generative model for Case 2 (Figure 1) builds upon the previous model by
introducingmoregranularityinthestatesandobservations,allowingforawider
6
Component Values Description
Represents the true state of
Food availability: present
Hidden States foodavailabilityintheenvi-
(1), absent (0)
ronment
Agent satiety: hungry (0), Represents the true state of
satiated (1) the agent’s satiety
Corresponds directly to the
Observations Observed food availability
food availability state
Corresponds directly to the
Agent’s perceived satiety
agent satiety state
The actions available to the
Actions Eat (1), Don’t Eat (0)
agent
Assumes the agent directly
Likelihood Matrix (A) Identity mapping
observes the true states
Specifies the state transi-
"Eat"(1)leadstosatiety(1)
Transition Matrix (B) tions based on the agent’s
when food is present (1)
actions
"Don’t Eat" (0) leads to
hunger (0)
Strong preference for sati- Encodes the agent’s goals
Preference Vector (C)
ated(1)andfoodpresent(1) and drives its behavior
Initial State Distribution Sets the starting conditions
Uniform Distribution
(D) for the simulation
Table 1: Components of the generative model for Case 1
range of behaviors and interactions between the agent and the environment.
Both the observations and hidden states are expanded to have three levels each:
food left (0: none, 1: some, 2: abundant) and satiety (0: not satiated, 1: some-
whatsatiated,2:fullysatiated).Inthismodel,weassumethattheagentdirectly
observes the true environmental states with some variations across different lev-
els of food availability and satiety. In this dynamic environment, the transitions
dependonboththecurrentstateandtheactiontakenbytheagent.Iftheagent
does not eat, food availability increases over time, while if the agent eats, food
availabilitydecreasesorremainsdepleted.Forthesatietystate,iftheagentdoes
not eat, satiety decreases over time, while if the agent eats, satiety increases. In
Case2,thepreferencesaredesignedtobalancebetweenmaintainingsatietyand
ensuringasustainablefoodsupply,encouragingtheagenttomaximizeitssatiety
while also considering the long-term availability of food resources. Specifically,
the agent has a strong preference for being satiated, while flat preference over
food left. The agent interacts with the dynamic environment over multiple time
steps, updating its beliefs and actions based on the observed states and the
changing dynamics of the environment, and it learns to not eat even if it is not
fullysatiated.TheagentisinitializedwiththegenerativemodelspecifiedinCase
2, using an extended policy length to plan multiple time steps ahead and antic-
ipate future consequences. In the simulation loop, the environment starts with
7
food fully available and the agent being half satisfied. The agent can plan over
multiple time steps (policy length of 3), and thus has the opportunity to bal-
anceimmediateconsumptionwithlong-termsustainability.Theextendedpolicy
lengthallowstheagenttoanticipatefuturestatesandavoidgreedybehaviorthat
could lead to starvation. The agent’s policies are restricted to ensure consistent
and sustainable actions across all time steps for both observation modalities.
Component Values Description
Represents the true state of
Food left: none (0), some
Hidden States foodavailabilityintheenvi-
(1), abundant (2)
ronment
Agent satiety: not satiated
Represents the true state of
(0), somewhat satiated (1),
the agent’s satiety
fully satiated (2)
Corresponds to the food
Observations Observed food availability availability state with some
variability
Corresponds to the agent
Agent’s perceived satiety satietystatewithsomevari-
ability
The actions available to the
Actions Eat (1), Don’t Eat (0)
agent
High probability of correct Defines the probability of
Likelihood Matrix (A) observations, lower for adja- observations given the true
cent states hidden states
Specifies the state transi-
“Eat” (1): food left de-
Transition Matrix (B) tions based on the agent’s
creases, satiety increases
actions and current state
“Don’tEat” (0):foodleftin-
creases, satiety decreases
Strongpreferenceforsatiety.
Balances maintaining sati- Encodes the agent’s goals
Preference Vector (C)
ety and sustainable food and drives its behavior
supply
Initial State Distribution Sets the starting conditions
Uniform Distribution
(D) for the simulation
Allows the agent to plan
Policy Length 3 time steps ahead and consider long-
term effects
Table 2: Components of the generative model for Case 2
In Case 2.1, we extend the dynamic environment setup from Case 2 by in-
troducing a learning mechanism for the agent. The key change is that the agent
now starts with a random B matrix and updates it based on its experiences in
the environment. The agent starts with a random B matrix instead of a prede-
8
fined one, which will be updated as the agent interacts with the environment.
The B matrix is initially random, and the agent updates this matrix based on
observed transitions between states. The A matrix remains the same as in Case
2,mappingthehiddenstatestoobservations,andtheCvector,representingthe
agent’spreferencesoverobservations,remainsunchangedfromCase2.Tolearn,
the agent starts with a randomly initialized B matrix, which does not initially
capture the correct state transitions. The random initialization is done using a
Dirichlet distribution to ensure valid probability values. At each time step, the
agent receives observations, infers states, infers policies, and samples actions,
similar to Case 2. After executing an action and receiving the next observation,
theagentupdatesitsBmatrix.Theagentnotesthetransitionfromtheprevious
statetothecurrentstategiventheactiontaken.TheBmatrixisupdatedusing
a learning rate to adjust the probabilities of the observed transitions. For states
that depend on a single factor, the transition probability is updated directly by
increasing the probability of the observed transition by the learning rate. For
states that depend on two factors (e.g., satiety depends on both food left and
previous satiety), the transition probabilities are updated based on the depen-
dencies specified. After updating, the B matrix is normalized to ensure that the
probabilities sum to 1, maintaining a valid probability distribution. We also in-
troduce several extended case variations for Case 2 (with and without learning)
to explore the agent’s behavior and performance under different conditions. We
test the agent’s robustness by initializing the B matrix with incorrect values set
toveryhigh(1)orverylow(0).Theagent’sperformanceisexpectedtodegrade,
demonstrating the importance of accurate transition models, and avoiding iner-
tia. We examine the agent’s behavior when it has different prior preferences by
modifying the C vector to represent a strong preference for food being present.
The agent prioritizes actions that ensure food is present, potentially at the ex-
pense of satiety. We test the agent in an environment where food increases at
a slower rate (0.5 units per step, compared to 1 unit per step previously) when
not eating and decreases at a faster rate (1 unit per step) when eating. Satiety
decreases faster when not eating (0.2 units per step, compared to 1 previously)
and increases at a different rate when eating (0.8 units per step, compared to 1
previously). The agent needs to adapt its strategy to account for these specific
changes in the environment dynamics. Its performance may be lower compared
to Case 2 due to the increased difficulty in balancing food and satiety levels, as
the food depletes more quickly when eating and satiety decreases more rapidly
whennoteating.Wefinallyassesstheimpactofplanninghorizonontheagent’s
performance by comparing agents with different planning horizons (1 time step
vs.3timesteps).Agentswithalongerplanninghorizonareexpectedtoperform
better, as they can anticipate future states more effectively and make decisions
that lead to more sustainable resource management.
9
3 Results
In Case 1, the agent is in a static environment where food is always available,
and its task is to maintain satiety by deciding whether to eat. The agent consis-
tently chooses to eat at every time step, reflecting its understanding that food
is always available and that eating maximizes its satiety (Appendix, figure 3,
first row). Food availability remains constant throughout the simulation, as ex-
pected in a static environment where food does not deplete (Appendix, figure
3, second row). The agent’s satiety increases as it eats and remains at a high
level,indicatingsuccessfullearningandadaptationtomaintainitsinternalstate
optimally (Appendix, figure 3, third row). This setup demonstrates the agent’s
abilitytoperformoptimallyinanenvironmentwithconstantresources.InCase
1.1, we introduce errors in the A and B matrices to test the agent’s resilience
and adaptability when its internal model does not accurately represent the en-
vironment.Theagent’sactionsshowamoreerraticpattern,reflectingconfusion
oruncertaintyduetotheincorrectmatrices.Despitetheconfusedmatrices,food
availability remains constant as in the standard case (Appendix, figure 4, sec-
ond row). The agent’s satiety fluctuates more compared to the standard case,
indicating that the agent’s ability to maintain a consistent internal state is im-
paired by the incorrect perception and planning models. This case shows how
deviations from accurate environmental models can affect an agent’s behavior
andperformance,leadingtolessoptimaldecisions.WithCase1results,wehave
shown that the agent has a degree of validity, and that it does in fact show how
theagentreactstomodelfitness,andchoosesthebestactionsrelativetoitsown
survival.InCase2,theenvironmentisdynamic,withfooddepletingwheneaten
andreplenishingifnotconsumed.Theagentmustbalanceitseatingbehaviorto
avoid starvation and resource depletion. It is equipped with a strong preference
over satiety = 2, and flat preference over food left (Appendix, figure 5). Over
multiple runs with a policy length of 3 time steps, the agent tends to avoid eat-
ing, leading him to die of starvation (Appendix, figure 6 left), or eats too much,
leadinghimtodeathaswell(Appendix,figure6right)ashisfoodgetsdepleted.
In Case 2 with learning, the agent starts with a randomly initialized B matrix
andupdatesitthroughinteractionswiththedynamicenvironment.Theagent’s
actions fluctuate regularly between "Eat" and "Do Not Eat," suggesting that it
has learned a strategy to balance its actions, so that it manages to survive the
whole time of the run and keeps satiety between 0 and 1 (Appendix, figure 7).
The survival time plot for each run shows that the agent consistently survives
for the maximum number of time steps after the initial learning phase, indicat-
ing that it quickly learns an effective strategy to avoid starvation and maintain
survival (Figure 2).
Compared to Case 2 without learning, the case with learning shows the
agent’s ability to learn from its interactions with the environment and develop
more effective strategies for survival and resource management. The extended
Case 2 variations explore the agent’s behavior and performance under different
conditions, such as incorrect transition models, altered prior preferences, and
varyingplanninghorizons.Withan incorrectBmatrix,wherethe valuesare set
10
Fig.2: Case 2, showing the survival time of agents with both learning and no
learning when the agent starts with a random B matrix. The survival time plot
for each run averaged over 10 agents, shows that the agents can quickly learn to
survive by acting in a sustainable way.
11
to extremes from the start (1 and 0, rather than lower probabilities), the agent
consistently chooses to eat in every time step, leading to suboptimal behavior
and eventual starvation (Appendix, figure 8, left). Under certain conditions, the
agent was unable to learn, being stuck in the inertia of its transitions. But over-
all, with learning enabled, the agent was able to pull itself out of high values
and was able to survive - highlighting the value of plasticity to get out of bad
bootstraps.
In the case of strong preferences on both states (high satiety and high food
left - without learning), the agent initially chooses to eat but then stops eating
asfoodbecomesscarce,demonstratingtheinfluenceofstrongpreferencesonthe
agent’s actions (Appendix, figure 9, left). This leads the agent to die over most
of its runs quite quickly. With learning enabled (Appendix, figure 9, right), the
agent is able to balance its actions again and can survive longer, balancing its
preferences and the environmental demands.
When the environment rate of change changes (Appendix, figure 10), the
agent’s performance declines compared to the previous case, but learning still
provides a significant advantage. With learning enabled, the agent adapts its
strategy - eating less often to conserve food, maintaining higher average food
levels, and sustaining satiety more effectively. This allows the agent to consis-
tentlysurvivethefullruntimestepswhenlearning,whileitonlysurvivesaround
3timestepswithoutlearning.Althoughthetougherenvironmentdynamicsmake
it more challenging, as the agent must plan in a different way and possibly over
longertimescales,theagentdemonstratesanimpressiveabilitytoadjustitspol-
icy through learning to match the new rate of change in food and satiety levels.
Learning is critical for the agent to find the right balance and survive in this
more complex scenario.
Withapolicylengthof1,theagentdoesverypoorlywithoutlearning,dying
basically after the first step. With learning, it takes the agent a little bit of
time to learn how to survive, but it eventually does. Its actions are a little more
erratic, but it does find a short term strategy (Appendix, figure 11). However,
even with this short term strategy, it is unable to survive for very long, truly
highlighting the need to focus on longer term strategies.
4 Discussion
Our sustainable agent demonstrates how active inference can give rise to sus-
tainableresourcemanagementstrategiesatthelevelofanindividualagent.The
agent’s behavior emerges from the interaction between its model of the world,
prior preferences, and the environmental dynamics. It seeks to optimize for im-
mediateneeds(e.g.,hunger)andlong-termoutcomes(e.g.,consistentfoodavail-
ability),learningtobalanceconsumptionandresourcereplenishmenttopromote
sustainability. Our findings align with our previous formalization of sustainabil-
ity, resilience, and well-being within the active inference framework.
In Case 1, the static environment allowed the agent to exhibit inertia, main-
tainingaconsistentconsumptionpatternwithoutconsideringlong-termresource
12
availability. While this behavior was adequate for the given context, it lacked
the flexibility needed for sustainable outcomes in more dynamic environments.
Case 2 introduced environmental variability, requiring the agent to demonstrate
elasticityandplasticity.Theagent’sabilitytoadaptitseatinghabitsinresponse
to changing food availability exemplifies elasticity, as it temporarily endures in-
creases in free energy (i.e., hunger) to ensure long-term stability. The agent’s
capacity to learn and update its model of the world based on new information
reflectsplasticity,enhancingitsresilienceinthefaceofenvironmentalshifts.The
agent’sadaptivebehaviorinCase2reflectsresilience,asitadjustsitsactionsto
maintain well-being under changing resource availability. The dynamic coupling
between agent and environment in the study of sustainable resource manage-
ment was critical, even at the level of a single agent. In the extended cases, we
can see the issues with inertia, and the possibility for even adaptive agents to
get stuck in difficult policies.
The agent’s actions optimized its own well-being and contributed to the re-
silience of the environment by preventing complete resource depletion. This re-
ciprocal relationship between the agent and its environment is a fundamental
aspect of sustainability, as the generative models of different layers in a hierar-
chical system are inherently linked through niche construction Albarracin et al.
[2024b]. However, the model’s simplicity also reveals its limitations. The single-
agent,single-resourcesetupdoesnotcapturethecomplexinterdependenciesand
feedback loops present in real-world systems. Future research should explore
multi-agent scenarios with competing interests and shared resources, as well as
environments with multiple, interconnected resource types and more sophisti-
cated replenishment dynamics. Additionally, the model does not consider the
possibility of permanent resource depletion, which would require conditioning
the environment’s survival on the maintenance of certain values. In the future,
we need to incorporate this aspect to understand the long-term implications of
resource management strategies. To further advance the application of active
inferenceinsustainableresourcemanagement,futureworkshouldfocusoninte-
gratingnetworktheoryanddynamicalsystemstheorytomodelandquantifythe
interdependenciesbetweenresourcesandtheirimpactonoverallsystemsustain-
ability. Optimizing precision or learning rates could also help foster the elastic
and plastic resilience necessary for long-term sustainability and abundance. We
wouldneedtoexplorethisavenuefurther.Ourpaperpresentsaproof-of-concept
modeldemonstratinghowactiveinferencecaninformsustainableresourceman-
agementattheindividuallevel.Weconsidertherelationshipbetweenagentand
environment to highlight the importance of resilience, adaptability, and long-
term planning in achieving sustainable outcomes. While the model’s simplicity
limits its direct applicability to real-world systems, it provides a foundation for
futureresearchexploringthecomplexdynamicsofsustainableresourcemanage-
ment through the lens of active inference.
Bibliography
T.Parr,G.Pezzulo,andK.J.Friston. Activeinference:thefreeenergyprinciple
in mind, brain, and behavior. MIT Press, 2022.
T.ParrandK.J.Friston.Generalisedfreeenergyandactiveinference.Biological
Cybernetics, 113(5):495–513, 2019.
G.StubbsandK.Friston. Thepolicehunch:thebayesianbrain,activeinference,
and the free energy principle in action. Frontiers in Psychology, 15:1368265,
2024.
K. Friston, T. FitzGerald, F. Rigoli, P. Schwartenbeck, and G. Pezzulo. Active
inference: a process theory. Neural Computation, 29(1):1–49, 2017.
K. Friston, L. Da Costa, N. Sajid, C. Heins, K. Ueltzhöffer, G. A. Pavliotis, and
T. Parr. The free energy principle made simpler but not too simple. Physics
Reports, 1024:1–29, 2023.
T. Parr, K. Friston, and G. Pezzulo. Generative models for sequential dynamics
in active inference. Cognitive Neurodynamics, pages 1–14, 2023.
M. J. D. Ramstead, P. B. Badcock, and K. J. Friston. Answering schrödinger’s
question: A free-energy formulation. Physics of Life Reviews, 24:1–16, 2018.
M. Kirchhoff, T. Parr, E. Palacios, K. Friston, and J. Kiverstein. The markov
blanketsoflife:autonomy,activeinferenceandthefreeenergyprinciple. Jour-
nal of The Royal Society Interface, 15(138):20170792, 2018.
F. Karl. A free energy principle for biological systems. Entropy, 14(11):2100–
2121, 2012.
L.DaCosta,N.Sajid,T.Parr,K.Friston,andR.Smith. Rewardmaximization
through discrete active inference. Neural Computation, 35(5):807–852, 2023.
G. Pezzulo, T. Parr, and K. Friston. Active inference as a theory of sentient
behavior. Biological Psychology, page 108741, 2024.
K. J. Friston, J. Daunizeau, J. Kilner, and S. J. Kiebel. Action and behavior: a
free-energy formulation. Biological Cybernetics, 102:227–260, 2010.
L. Da Costa, S. Tenka, D. Zhao, and N. Sajid. Active inference as a model of
agency. arXiv preprint arXiv:2401.12917, 2024.
T. Solymosi and J. Schulkin. Creative resilience. flourishing and valuation
throughsocialallostasisandactiveinference.EuropeanJournalofPragmatism
and American Philosophy, 16(XVI-1), 2024.
M.Albarracin,G.Bouchard-Joly,Z.Sheikhbahaee,M.Miller,R.J.Pitliya,and
P. Poirier. Feeling our place in the world: an active inference account of self-
esteem. Neuroscience of Consciousness, 2024(1):niae007, 2024a.
T. Matsumura, K. Esaki, S. Yang, C. Yoshimura, and H. Mizuno. Active infer-
ence with empathy mechanism for socially behaved artificial agents in diverse
situations. Artificial Life, pages 1–21, 2023.
C. Montgomery and I. Hipólito. Resurrecting gaia: harnessing the free energy
principle to preserve life as we know it. Frontiers in Psychology, 14:1206963,
2023.
14
M. J. Ramstead, K. J. Friston, and I. Hipólito. Is the free-energy principle a
formal theory of semantics? from variational density dynamics to neural and
phenotypic representations. Entropy, 22(8):889, 2020.
K. Friston, H. R. Brown, J. Siemerkus, and K. E. Stephan. The dysconnection
hypothesis (2016). Schizophrenia Research, 176(2-3):83–94, 2016.
A.Harikumar,K.P.Solovyeva,M.Misiura,A.Iraji,S.M.Plis,G.D.Pearlson,
..., and V. D. Calhoun. Revisiting functional dysconnectivity: A review of
threemodelframeworksinschizophrenia.CurrentNeurologyandNeuroscience
Reports, 23(12):937–946, 2023.
T. S. Zarghami, P. Zeidman, A. Razi, F. Bahrami, and G. A. Hossein-Zadeh.
Dysconnection and cognition in schizophrenia: A spectral dynamic causal
modeling study. Human Brain Mapping, 44(7):2873–2896, 2023.
Conor Heins, Beren Millidge, Daphne Demekas, Brennan Klein, Karl Friston,
Iain Couzin, and Alexander Tschantz. pymdp: A python library for active
inference in discrete state spaces. arXiv preprint arXiv:2201.03904, 2022.
M. Albarracin, M. Ramstead, R. J Pitliya, I. Hipolito, L. Da Costa, M. Raffa,
A.Constant,andS.G.Manski.Sustainabilityunderactiveinference.Systems,
12(5):163, 2024b.
5 Appendix 1 - Figures
15
Fig.3: Case 1. The three plots above show the expected behavior. At time step
0, food is available (food left = 0), and the satiety level is low (satiety = 0).
Due to the agent’s strong preference for high level of satiety, it keeps eating at
subsequent time steps and the satiety increases. Since the environment is static,
the food is always present.
16
Fig.4:Case1.1-wheretheagentisgivenincorrectAandBmatrices,introducing
errorsinitsperceptionandbeliefsaboutstatetransitions.Thetopplotshowsthe
agent’s actions overtime. The pattern is more erratic compared to the standard
Case1,astheagentisconfused.Themiddleplotshowsthefoodleftobservations.
Food availability remains constant at 1 throughout the simulation since the
environment is static. The bottom plot shows the agent’s satiety over time.
Satiety level fluctuates more. This indicates that the agent’s ability to maintain
a stable, high satiety state is impaired by the incorrect perception and planning
models.
17
Fig.5: Prior preference for Case 2. The agent’s preferences are changed so that,
unlike case 1 and 1.1 it no longer has a preference over food left. Its only non-
uniform preference is to have a preference over satiety.
18
Fig.6: Case 2 - Dynamic Environment. Without learning, the agent either does
not eat (as shown in the three plots on the left) or eats too much and therefore
allows food in the environment to go to 0 (as shown in the three plots on the
right). As a result, the agent dies.
19
Fig.7:Case2-DynamicEnvironment.Examplerunwithlearningonandpolicy
length = 3. With this depth of policy, the agent is able to plan further in time,
andwithlearningitmanagestosurviveforthewholelengthoftherun.Over10
timestepstheagentisabletoplanitsbehavioursothatitneverreachessatiety
= 0, and always has food left.
20
Fig.8: Example run from Case 2 without learning enabled on the left and with
learning enabled on the right, but starting with an extreme B matrix setting
(probabilities set to 1 or 0, on the left three plots and middle three plots, and
high but non-extreme values on the right). The agent dies quickly, just as the
randomly set values of the B matrix in plot 6, and is able to learn on the right.
21
Fig.9: Case 2 with strong prior preferences on both high satiety and high food
leftstates.Onthethreetopleftplots,theagenthasnolearning,andonthetop
right, the agent has learning. On the bottom, we can see that survival time is
vastlydifferentwithandwithoutlearning,asthepreferencesaffectthebehavior
of the agent.
22
Fig.10: Case 2 in a changing environment where food and satiety change at
differenttimerates.Thethreetopleftplotsshowtheresultswithoutlearningoff,
andthethreetoprightplotsshowtheresultswithlearningon.Thebottomplot
represents the comparison between the survival time over 10 time steps. Food
increases at a slower rate (0.5 units per step) when not eating and decreases at
a faster rate (1 unit per step) when eating. Satiety decreases faster when not
eating(0.2unitsperstep)andincreasesatadifferentratewheneating(0.8units
per step).
23
Fig.11: Case 2 example runs runs with policy length = 1, left plot without
learning, right plot with learning, and survival time on the bottom.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Modeling Sustainable Resource Management using Active Inference"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
