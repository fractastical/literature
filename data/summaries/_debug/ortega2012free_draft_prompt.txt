=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Free Energy and the Generalized Optimality Equations for Sequential Decision Making
Citation Key: ortega2012free
Authors: Pedro A. Ortega, Daniel A. Braun

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2012

Abstract: Thefreeenergyfunctionalhasrecentlybeenproposedasavariationalprincipleforbounded
rationaldecision-making,since it instantiates a natural trade-offbetween utility gains and
information processing costs that can be axiomatically derived. Here we apply the free
energy principle to general decision trees that include both adversarial and stochastic en-
vironments. We derive generalized sequential optimality equations that not only include
theBellmanoptimalityequationsasalimitcase,butalsoleadtowell-kn...

Key Terms: decision, optimality, energy, equations, sequential, institute, planck, include, free, generalized

=== FULL PAPER TEXT ===

2102
yaM
71
]LM.tats[
1v7993.5021:viXra
JMLR:WorkshopandConferenceProceedingsvol:1–10,2012 EWRL2012
Free Energy and the Generalized Optimality Equations for
Sequential Decision Making
Pedro A. Ortega pedro.ortega@tuebingen.mpg.de
Max Planck Institute for Biological Cybernetics
Max Planck Institute for Intelligent Systems
ORAND S.A.
Daniel A. Braun daniel.braun@tuebingen.mpg.de
Max Planck Institute for Biological Cybernetics
Max Planck Institute for Intelligent Systems
Editor: Marc Deisenroth, Csaba Szepesvari, Jan Peters
Abstract
Thefreeenergyfunctionalhasrecentlybeenproposedasavariationalprincipleforbounded
rationaldecision-making,since it instantiates a natural trade-offbetween utility gains and
information processing costs that can be axiomatically derived. Here we apply the free
energy principle to general decision trees that include both adversarial and stochastic en-
vironments. We derive generalized sequential optimality equations that not only include
theBellmanoptimalityequationsasalimitcase,butalsoleadtowell-knowndecision-rules
suchasExpectimax,MinimaxandExpectiminimax. Weshowhowthesedecision-rulescan
be derived from a single free energy principle that assigns a resource parameter to each
node in the decision tree. These resource parameters express a concrete computational
cost that can be measured as the amount of samples that are needed from the distribu-
tion that belongs to eachnode. The free energyprinciple therefore providesthe normative
basis for generalizedoptimality equations that accountfor both adversarialand stochastic
environments.
Keywords: Foundations of AI, free energy,Bellmanoptimality equations,bounded ratio-
nality.
1. Introduction
Decision trees are a ubiquitous tool in decision theory and artificial intelligence research to
represent a wide range of decision-making problems that include the classic reinforcement
learningparadigmaswellascompetitivegames (OsborneandRubinstein,1999; Russelland
Norvig, 2010). Depending on the kind of system one is interacting with, there are different
decision rules one has to apply—the most famous ones being Expectimax, Minimax and
Expectiminimax—see Figure 1. When an agent interacts with a stochastic system, the
agent chooses its decisions based on Expectimax. Essentially, Expectimax is the dynamic
programming algorithm that solves the Bellman optimality equations, thereby recursively
maximizing expected future reward in a sequential decision problem (Bellman, 1957).
In two-player zero-sum games where strictly competitive players make alternate moves,
an agent shouldusetheMinimax strategy. Themotivation underlyingminimax decisions is
(cid:13)c 2012P.A.Ortega&D.A.Braun.
Ortega Braun
that the agent wants to optimize the worst-case gain as a means of protecting itself against
the potentially harmful decisions made by the adversary. Finally, there are games that mix
PSfrag replacements
the two previous interaction types. For instance, in Backgammon, the course of the game
depends on the skill of the players and chance elements. In these cases, the agent bases its
decisions on the Expectiminimax rule (Michie, 1966).
Expectimax Minimax Expectiminimax
max max E
E min max
max max E
E min min
Figure 1: Illustration of Expectimax, Minimax and Expectiminimax in decision trees rep-
resenting three different interaction scenarios. The internal nodes can be of three
possible types: maximum (△), minimum (▽) and expectation (◦). The optimal
decision is calculated recursively using dynamic programming.
What is common to all of these decision-making schemes is that they presupposea fully
rational decision-maker that is able to compute all of the required operations with absolute
precision. In contrast, a bounded rational decision-maker trades off expected utility gains
against the cost of the required computations (Simon, 1984). Recently, the free energy
has been suggested as a normative variational principle for such bounded rational decision-
making that takes the computational effort into account (Ortega and Braun, 2011; Braun
and Ortega, 2011; Ortega, 2011). This builds on previous work on efficient computation of
optimal actions that trades off the benefits obtained from maximizing the utility function
against the cost of changing the uncontrolled dynamics given by the environment (Kappen,
2005; Todorov, 2006, 2009; Kappen et al., 2012). The aim of this paper is to extend these
results to generalized decision trees such that Expectimax, Minimax, Expectiminimax, and
bounded rational acting can all be derived from a single optimization principle. Moreover,
this framework leads to a natural measure of computational costs spent at each node of the
decision tree. All the proofs are given in the appendix.
2. Free Energy
2.1. Equilibrium Distribution
In Ortega and Braun (2011) and in Ortega (2011) it was shown that a bounded rational
decision-making problem can be formalized based on the negative free energy difference
between two information processing states represented by two probability distributions P
and Q. The decision process then transforms the initial choice probability Q into a final
2
Generalized Optimality Equations
choice probability P by taking into account the utility gains (or losses) and the transfor-
mation costs. This transformation process can be formalized as
1
P(x)= Q(x)eαU(x), where Z = Q(x)eαU(x). (1)
Z
Xx
Accordingly, the choice pattern of the decision-maker is predicted by the equilibrium dis-
tribution P. Crucially, the probability distribution P extremizes the following functional
(Callen, 1985; Keller, 1998):
Definition 1 (Negative Free Energy Difference) Let Q be a probability distribution
and let U be a real-valued utility function over the set X. For any α ∈ R, define the
negative free energy difference F [P] as
α
1 P(x)
F [P] := P(x)U(x)− P(x)log . (2)
α
α Q(x)
Xx Xx
The parameter α is called the inverse temperature.
Although strictly speaking, the functional F [P] corresponds to the negative free energy
α
difference, we will refer to it as the “free energy” in the following for simplicity. When
inserting the equilibrium distribution (1) into (2), the extremum of F yields:
α
1
log Q(x)eαU(x) . (3)
α (cid:18) (cid:19)
Xx
For different values of α, this extremum takes the following limits:
lim 1 logZ = maxU(x) (maximum node)
α→∞ α x
lim 1 logZ = Q(x)U(x) (chance node)
α
α→0
Xx
lim 1 logZ = minU(x) (minimum node)
α→−∞ α x
Thecase α → ∞ corresponds to the perfectly rational agent, the case α → 0 correspondsto
the expectation at a chance node and the case α → −∞ anticipates the perfectly rational
opponent. Therefore, thesingleexpression 1 logZ canrepresentthemaximum,expectation
α
and minimum depending on the value of α.
The inspection of (2) reveals that the free energy encapsulates a fundamental decision-
theoretic trade-off: itcorrespondstotheexpected utility, penalized—orregularized—by the
information cost of transforming the base distribution Q into the final distribution P. The
inverse temperatureplays therole of theconversion factor between unitsof information and
units of utility.
Ifwewanttochangethetemperatureαtoβ whilekeepingtheequilibriumandreference
distributions equal, then we need to change the corresponding utilities from U to V in a
manner given by the following theorem. Temperature changes will be important for the
application of the free energy principle to the general decision trees in Section 3.
3
Ortega Braun
Theorem 2 Let P be the equilibrium distribution for a given inverse temperature α, utility
function U and reference distribution Q. If the temperature changes to β while keeping P
and Q fixed, then the utility function changes to
P(x)
V(x) = U(x)− 1 − 1 log .
α β Q(x)
(cid:16) (cid:17)
2.2. Resource Costs
Consider the problem of picking the largest number in a sequence U ,U ,U ,... of i.i.d.
0 1 2
data, where each U ∈ U is drawn from a source with probability distribution M. After α
i
draws the largest number will be given by max{U ,U ,...,U }. Naturally, the larger the
1 2 α
number of draws, the higher the chances of observing a large number.
Theorem 3 Let X be a finite set. Let Q and M be strictly positive probability distributions
over X. Let α be a positive integer. Define M as the probability distribution over the max-
α
imum of α samples from M. Then, there are strictly positive constants δ and ξ depending
only on M such that for all α,
Q(x)eαU(x)
−M (x) ≤e−(α−ξ)δ.
(cid:12)
(cid:12) x′
Q(x′)eαU(x′) α (cid:12)
(cid:12)
(cid:12) (cid:12)
(cid:12)P (cid:12)
(cid:12) (cid:12)
Consequently, one can interpret the inverse temperature as a resource parameter that
determines how many samples are drawn to estimate the maximum. Note that the distri-
bution M is arbitrary as long as it has the same support as Q. This interpretation can
be extended to a negative α, by noting that αU(x) = (−α)(−U(x)), i.e. instead of the
maximum we take the minimum of −α samples.
3. General Decision Trees
A generalized decision tree is a tree where each node corresponds to a possible interaction
history x ∈ Xt, where t is smaller or equal than some fixed horizon T, and where edges
≤t
connect two consecutive interaction histories. Furthermore, every node x has an associ-
≤t
ated inverse temperature β(x ); and every transition has a base probability Q(x |x ) of
≤t t <t
moving from state x to state x = x x representing the stochastic law the interactions
<t ≤t <t t
follow when it is not controlled, and an immediate reward R(x |x ). The objective of the
t <t
T
agent is to make decisions such that the sum R(x |x ) is maximized subject to the
t=1 t <t
temperature constraints. P
3.1. Free Energy for General Decision Trees
The free energy principle is stated above for one decision variable x. If x represents a tuple
of (possibly dependent) random variables x ,...,x , then the free energy principle can
1 T
be applied in a straightforward manner to the corresponding tree. However, all nodes of
the tree will have the same inverse temperature assigned to them and, therefore, the same
amount of computational resources will be spent at each node of the tree. This allows for
4
PSfragreplacements
Generalized Optimality Equations
U(ε) V(ε)
S(0|ε) α S(1|ε) R(0|ε) β(ε) R(1|ε)
U(0) U(1) V(0) V(1)
α α
S(0|0) S(1|0) S(0|1) S(1|1) R(0|0) β(0) R(1|0) R(0|1) β(1) R(1|1)
Figure 2: The free energy formalism can only be applied in a straightforward manner to
trees with uniform resource allocation (left). In order to apply it to general trees
thathavedifferentresourceparametersateachnode(right), weneedtotransform
the utilities as described in (4) to preserve the equilibrium distribution.
example deriving the formalisms of path integral control and KL control (Todorov, 2009;
Braun and Ortega, 2011; Kappen et al., 2012).
In the case of general decision trees the assumption of uniform temperatures has to be
relaxed (Figure 2). In general, we can then dedicate different amounts of computational
resources to each node of the tree. However, this requires a translation between a tree
with a single temperature and to a tree with different temperatures. This translation can
be achieved using Theorem 2. Define a reward as the change in utility of two subsequent
nodes. Then, the rewards of the resulting decision tree are given by
P(x |x )
R(x |x ) := V(x )−V(x ) = U(x )−U(x ) − 1 − 1 log t <t . (4)
t <t ≤t <t ≤t <t α β(x<t) Q(x |x )
(cid:16) (cid:17) t <t
(cid:2) (cid:3) (cid:2) (cid:3)
This allows introducing a collection of node-specific (not necessarily time-specific) inverse
temperatures β(x ), allowing for a greater degree of flexibility in the representation of
<t
information costs. The next theorem states the connection between the free energy and the
general decision tree formulation.
Theorem 4 The free energy of the whole trajectory can be rewritten in terms of rewards:
1 P(x )
≤T
F [P] = P(x ) U(x )− log
α ≤T ≤T
(cid:26) α Q(x )(cid:27)
xX
≤T
≤T
T
1 P(x |x )
t <t
= U(ε)+ P(x ) R(x |x )− log . (5)
≤T t <t
(cid:26) β(x ) Q(x |x )(cid:27)
xX
≤T
Xt=1 <t t <t
This translation allows applying the free energy principle to each node with a different
resource parameter β(x ). By writing out the sum in (5), one realizes that this free energy
<t
has a nested structure where the latest time step forms the innermost variational problem
and all other variational problems of the previous time steps can be solved recursively by
working backwards in time. This then leads to the following solution:
5
Ortega Braun
Theorem 5 The solution to the free energy in terms of rewards is given by
1 1
P(x |x ) = Q(x |x )exp β(x ) R(x |x )+ logZ(x ) ,
t <t t <t <t t <t ≤t
Z(x ) β(x )
<t n
(cid:2)
≤t
(cid:3)
o
where Z(x )= 1 and where for all t < T
≤T
1
Z(x ) = Q(x |x )exp β(x ) R(x |x )+ logZ(x ) .
<t t <t <t t <t ≤t
β(x )
Xxt n (cid:2) ≤t (cid:3) o
3.2. Generalized Optimality Equations
Theorem5togetherwiththepropertiesofthefreeenergyextremum(3)suggestthefollowing
definition.
Definition 6 (Generalized Optimality Equations)
1
V(x ) = log Q(x |x )exp β(x ) R(x |x )+V(x ) .
<t t <t <t t <t ≤t
β(x ) (cid:26) (cid:27)
<t Xxt n (cid:2) (cid:3) o
By virtue of our previous analysis, this equation tells us how to recursively calculate the
value function (i.e. the utility of each node) given the computational resources allocated in
each node.
It is immediately clear that the three kinds of decision trees mentioned in the introduc-
tion are special cases of general decision trees. In particular, the three classical operators
are obtained as limit cases:
max{R(x |x )+V(x )} if β(x )= ∞,
t <t ≤t <t
xt

V(x <t ) =   E{R(x t |x <t )+V(x ≤t )} if β(x <t )= 0,


min{R(x |x )+V(x )} if β(x )= −∞.
t <t ≤t <t
 xt



The familiar Bellman optimality equations for stochastic systems are obtained by consider-
ing an agent decision node followed by a random decision node:
V(x ) = max R(x |x )+V(x )
<t t <t ≤t
xt n o
= max R(x |x )+E R(x |x )+V(x ) .
t <t t+1 ≤t ≤t+1
xt n o
(cid:2) (cid:3)
4. Discussions & Conclusions
Boundedrationaldecision-makingschemesbasedonthefreeenergygeneralizeclassicdecision-
making schemes by taking into account information processing costs measured by the
Kullback-Leibler divergence (Wolpert, 2004; Todorov, 2009; Peters et al., 2010; Ortega
and Braun, 2011; Kappen et al., 2012). Ultimately, these costs are determined by Lagrange
multiplierconstraints givenbytheinversetemperatureplayingtheroleofaresourceparam-
eter. Here we generalize this approach to general decision trees where each node can have
a different resource allocation. Consequently, we obtain generalized optimality equations
6
Generalized Optimality Equations
for sequential decision-making that include the well-known Bellman optimality equation as
well as Expectimax-, Minimax- and Expectiminimax-decision rules depending on the limit
values of the resource parameters. The resource parameters themselves are amenable to
interesting computational, statistical and economic interpretations. In the first sense they
measurethenumberofsamplesneededfromadistributionbeforeapplyingthemaxoperator
and therefore correspond directly to computational effort. In the second sense they reflect
the confidence of the estimate of the maximum and therefore they can also express risk
attitudes. Finally, the resource parameters reflect the control an agent has over a random
variable. These different ramifications need to be explored further in the future.
Appendix A. Proofs
A.1. Proof of Theorem 2
Proof
1 P(x) 1 P(x)
P(x)U(x)− P(x)log = P(x)V(x)− P(x)log
α Q(x) β Q(x)
Xx Xx Xx Xx
SincetheequilibriumandreferencedistributionsP(x)andQ(x)areconstantbutarbitrarilychosen,
it must be that
1 P(x) 1 P(x)
U(x)− log =V(x)− log .
α Q(x) β Q(x)
Hence,
P(x)
V(x)=U(x)− 1 − 1 log .
α β Q(x)
(cid:16) (cid:17)
A.2. Proof of Theorem 3
Proof Let x ,x ,...,x be the ordering of X such that U(x ),U(x ),...,U(x ). It is well
1 2 N 1 2 N
known that the distribution over the maximum of α samples is equal to F (x) = F(x)α, where
α
F is the cumulative distribution F(x ) = M(x ). Defining F(x ) := 0, one has M (x ) =
n k≤n k 0 α n
F(x
n
)α−F(x
n−1
)α. Hence, the probabilityPcan be bounded as 0≤M
α
(x
n
)≤F(x
n
)α, or
0≤M (x )≤e−αγ(xn), (6)
α n
if we use F(x )=e−γ(xn) where γ(x )≥0. The Boltzmann distribution can be bounded as
n n
Q(x )eαU(xn) Q(x )eαU(xn)
n n
0≤ ≤ .
k
Q(x
k
)eαU(xk) Q(x
N
)eαU(xN)
P
The upper bound is obtained by dropping all the summands in the expectation but the largest. In
exponential form, the bounds are written as
Q(x )eαU(xn)
0≤ n ≤e−αδ(xn)+c(xn), (7)
k
Q(x
k
)eαU(xk)
P
where δ(x ) := U(x )−U(x ), c(x ) := −logQ(x )+logQ(x ). Note that δ(x ) is positive.
n N n n N n n
Subtracting the inequalities (6) from (7) yields
Q(x )eαU(xn)
−e−αγ(xn) ≤ n −M (x )≤e−αδ(xn)+c(xn).
k
Q(x
k
)eαU(xk) α n
P
7
Ortega Braun
Choosing ξ(x )=c(x )/δ(x )≥0 allowsrewriting the upper bound and changingthe lower bound
n n n
to
Q(x )eαU(xn)
−e−(α−ξ(xn))γ(xn) ≤ n −M (x )≤e−(α−ξ(xn))δ(xn).
k
Q(x
k
)eαU(xk) α n
P
Finally, choosing ξ := max {ξ(x )} and δ = max{max {δ(x )},max {γ(x )}} yields the bounds
n n n n n n
of the theorem
Q(x )eαU(xn)
−e−(α−ξ)δ ≤ n −M (x )≤e−(α−ξ)δ.
k
Q(x
k
)eαU(xk) α n
P
A.3. Proof of Theorem 4
Proof The free energy of the whole trajectory with inverse temperature α is given by
1 P(x )
≤T
P(x ) U(x )− log .
≤T ≤T
(cid:26) α Q(x )(cid:27)
xX
≤T
≤T
T
Using a telescopic sum (a −a )=a −a for the utilities yields
t=1 t t−1 T 0
P
T
1 P(x |x )
t <t
U(ε)+ P(x ) U(x )−U(x ) − log .
≤T ≤t <t
(cid:26) α Q(x |x )(cid:27)
xX ≤T Xt=1 (cid:2) (cid:3) t <t
Using the definition of rewards (4), one gets the result
T
1 P(x |x )
t <t
U(ε)+ P(x ) R(x |x )− log .
≤T t <t
(cid:26) β(x ) Q(x |x )(cid:27)
xX
≤T
Xt=1 <t t <t
A.4. Proof of Theorem 5
Proof The inner sum of the free energy
T
1 P(x |x )
t <t
U(ε)+ P(x ) R(x |x )− log .
≤T t <t
(cid:26) β(x ) Q(x |x )(cid:27)
xX
≤T
Xt=1 <t t <t
can be expanded as
1 P(x )
1
U(ε)+ P(x ) R(x )− log
1 1
(cid:26) β(ε) Q(x )
Xx1 1
1 P(x |x )
2 1
+ P(x |x ) R(x |x )− log
2 1 2 1
(cid:26) β(x ) Q(x |x )
Xx2 1 2 1
+···
1 P(x |x )
T <T
+ P(x |x ) R(x |x )− log ··· .
T <T T <T
(cid:26) β(x ) Q(x |x )(cid:27) (cid:27)(cid:27)
XxT <T T <T
8
Generalized Optimality Equations
This can be solved by induction, starting with the innermost sums and then recursively solving the
outer sums. The innermost sums
1 P(x |x )
T <T
P(x |x ) R(x |x )− log
T <T T <T
(cid:26) β(x ) Q(x |x )(cid:27)
XxT <T T <T
are maximized when
1
P(x |x )= Q(x |x )exp β(x )R(x |x ) .
T <T T <T <T T <T
Z(x )
<T n o
This can be seen by noting that for probabilities p and positive numbers r > 0, the quantity
i i
p log(p /r ) is minimized by choosing p = 1r , where Z = r is just a normalizingconstant.
i i i i i Z i i i
PSubstituting this solution yields the outer sums P
1 P(x |x ) 1
t <t
P(x |x ) R(x |x )− log + logZ(x )
t <t t <t ≤t
(cid:26) β(x ) Q(x |x ) β(x ) (cid:27)
Xxt <t t <t ≤t
where
1
Z(x )= Q(x |x )exp β(x ) R(x |x )+ logZ(x ) .
<t t <t <t t <t ≤t
β(x )
Xxt n (cid:2) ≤t (cid:3)o
These sums are then maximized by choosing
1 1
P(x |x )= Q(x |x )exp β(x ) R(x |x )+ logZ(x ) .
t <t t <t <t t <t ≤t
Z(x ) β(x )
<t n (cid:2) ≤t (cid:3)o
References
R.E. Bellman. Dynamic programming, 1957.
D. A. Braun and P. A. Ortega. Path integral control and bounded rationality. In IEEE
Symposium on adaptive dynamic programming and reinforcement learning, pages 202–
209, 2011.
H.B. Callen. Thermodynamics and an introduction to thermostatistics. John Wiley & Sons,
New York, 1985.
H.J. Kappen. A linear theory for control of non-linear stochastic systems. Physical Review
Letters, 95:200201, 2005.
H.J. Kappen, V. Go´mez, and M. Opper. Optimal control as a graphical model inference
problem. Machine Learning, 1:1–11, 2012.
G. Keller. Equilibrium States in Ergodic Theory. London Mathematical Society Student
Texts. Cambridge Univeristy Press, 1998.
D. Michie. Game-playing and game-learning automata. Advances in Programming & Non-
Numerical Computation, pages 183–200, 1966.
9
Ortega Braun
P. Ortega. A unified framework for resource-bounded autonomous agents interacting with
unknown environments. PhD thesis, Department of Engineering, University of Cam-
bridge, UK, 2011.
P.A. Ortega andD.A. Braun. Information, utility and boundedrationality. InLecture notes
on artificial intelligence, volume 6830, pages 269–274, 2011.
M.J. Osborne and A. Rubinstein. A Course in Game Theory. MIT Press, 1999.
J. Peters, K. Mu¨lling, and Y. Altun. Relative entropy policy search. In AAAI, 2010.
S.J. Russell and P. Norvig. Artificial Intelligence: A Modern Approach. Prentice-Hall,
Englewood Cliffs, NJ, 3rd edition edition, 2010.
H. Simon. Models of Bounded Rationality. MIT Press, Cambridge, MA, 1984.
E.Todorov. Linearlysolvablemarkovdecisionproblems. InAdvances inNeural Information
Processing Systems, volume 19, pages 1369–1376, 2006.
E. Todorov. Efficient computation of optimal actions. Proceedings of the National Academy
of Sciences U.S.A., 106:11478–11483, 2009.
D.H. Wolpert. Complex Engineering Systems, chapter Information theory - the bridge
connecting bounded rational game theory and statistical physics. Perseus Books, 2004.
10

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Free Energy and the Generalized Optimality Equations for Sequential Decision Making"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
