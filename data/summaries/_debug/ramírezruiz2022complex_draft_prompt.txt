=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Complex behavior from intrinsic motivation to occupy action-state path space
Citation Key: ramírezruiz2022complex
Authors: Jorge Ramírez-Ruiz, Dmytro Grytskyy, Chiara Mastrogiuseppe

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2022

Abstract: Most theories of behavior posit that agents tend to maximize some form of reward or utility. However,
animals very often move with curiosity and seem to be motivated in a reward-free manner. Here we
abandontheideaofrewardmaximization,andproposethatthegoalofbehaviorismaximizingoccupancy
of future paths of actions and states. According to this maximum occupancy principle, rewards are
the means to occupy path space, not the goal per se; goal-directedness simply emerges as rational ways
of searching...

Key Terms: path, future, action, state, motivation, space, intrinsic, behavior, goal, complex

=== FULL PAPER TEXT ===

Complex behavior from intrinsic motivation to occupy future
action-state path space
Jorge Ramírez-Ruiz1, Dmytro Grytskyy1, Chiara Mastrogiuseppe1, Yamen Habib1, and
Rubén Moreno-Bote1,2
1Center for Brain and Cognition, and Department of Information and Communication Technologies, Universitat
Pompeu Fabra, Barcelona, Spain 08005
2Serra Húnter Fellow Programme, Universitat Pompeu Fabra, Barcelona, Spain
February 27, 2024
Abstract
Most theories of behavior posit that agents tend to maximize some form of reward or utility. However,
animals very often move with curiosity and seem to be motivated in a reward-free manner. Here we
abandontheideaofrewardmaximization,andproposethatthegoalofbehaviorismaximizingoccupancy
of future paths of actions and states. According to this maximum occupancy principle, rewards are
the means to occupy path space, not the goal per se; goal-directedness simply emerges as rational ways
of searching for resources so that movement, understood amply, never ends. We find that action-state
path entropy is the only measure consistent with additivity and other intuitive properties of expected
future action-state path occupancy. We provide analytical expressions that relate the optimal policy
and state-value function, and prove convergence of our value iteration algorithm. Using discrete and
continuous state tasks, including a high–dimensional controller, we show that complex behaviors such
as ‘dancing’, hide-and-seek and a basic form of altruistic behavior naturally result from the intrinsic
motivationtooccupypathspace. Allinall,wepresentatheoryofbehaviorthatgeneratesbothvariability
and goal-directedness in the absence of reward maximization.
1 Introduction
Natural agents are endowed with a tendency to move, explore and interact with their environment [1, 2]. For
instance,humannewbornsunintentionallymovetheirbodyparts[3],and7to12-monthsinfantsspontaneously
babble vocally [4] and with their hands [5]. Exploration and curiosity are major drives for learning and
discovery through information-seeking [6–8]. These behaviors seem to elude a simple explanation in terms
of extrinsic reward maximization. However, these intrinsic motivations push agents to visit new states by
performing novel courses of action, which helps learning and the discovery of even larger rewards in the long
run [9, 10]. Therefore, it has been argued that exploration and curiosity could arise as a consequence of
seeking extrinsic reward maximization by endowing agents with the necessary inductive biases to learn in
complex and ever-changing natural environments [11, 12].
Whilemosttheoriesofrationalbehaviordopositthatagentsarerewardmaximizers[13–16], veryfewofus
would agree that the sole goal of living agents is maximizing money gains or food intake. Indeed, expressing
excessive emphasis on these types of goals is usually seen as a sign of psychological disorders [17, 18]. Further,
settingarewardfunctionbydesignasthegoalofartificialagentsis,moreoftenthannot,arbitrary[14,19–21],
leading to the recurrent problem faced by theories of reward maximization of defining what rewards are
[22–26]. In some cases, like in artificial games, rewards can be unambiguously defined, such as number of
collected points or wins [27]. However, in most situations defining rewards is task-dependent, non-trivial
and problematic. For instance, a vacuum cleaner robot could be designed to either maximize the weight or
volume of dust collected, energy efficiency, or a weighted combination of them [28]. In more complex cases,
companies can aim at maximizing profit, but without a suitable innovation policy profit maximization can be
self-defeating [29].
1
4202
beF
42
]IA.sc[
2v61301.5022:viXra
Here,weabandontheideathatthegoalismaximizingextrinsicrewardsandthatmovementoverspaceisa
means to achieve this goal. Instead, we adopt the opposite view, inspired by the nature of our intrinsic drives:
we propose that the objective is to maximally occupy action-state path space, understood in a broad sense, in
the long term. We call this principle the maximum occupancy principle (MOP), which posits that the goal
of agents is to generate all sort of behaviors and occupy, on average, as much space (action-state paths) as
possible in the future. According to MOP, extrinsic rewards serve to obtain the energy necessary to move in
order to occupy action-state space, they are not the goals per se. The usual exploration–exploitation tradeoff
[30] therefore disappears: agents that seek to occupy space “solve” this issue naturally because they care about
rewards only as means to an end. Furthermore, in this sense, surviving is only preferred because it is needed
to keep visiting action-state space. Our theory provides a rational account of exploratory and curiosity-driven
behavior where the problem of defining a reward function vanishes, and captures the variability of behavior
[31–36] by taking it as a principle.
In this work, we model a MOP agent interacting with the environment as a Markov decision process
(MDP) where the intrinsic, immediate reward is the occupancy of the next action-state visited, which is
largest when performing an uncommon action and visiting a rare state –there are no extrinsic rewards (i.e., no
task is defined) that drive the agent. We show that (weighted) action-state path entropy is the only measure
of occupancy consistent with additivity per time step, positivity and smoothness. Due to the additivity
property, the value of being in a state, defined as the expected future time-discounted action-state path
entropy, can be written in the form of a Bellman equation, which has a unique solution that can be found with
an iterative map. Following this entropy objective leads to agents that seek variability, while being sensitive to
the constraints imposed by the agent-environment interaction on the future path availability. We demonstrate
in various simulated experiments with discrete and continuous state and action spaces that MOP generates
complex behaviors that, to the human eye, look genuinely goal-directed and playful, such as hide-and-seek in
a prey-predator problem, dancing of a cartpole, a basic form of altruism in an agent-and-pet example, and
rich behaviors in a high-dimensional quadruped.
MOP builds over an extensive literature on entropy-regularized reinforcement learning (RL) [37–47] or
pure entropic objectives [48–53]. This body of work emphasizes the regularization benefits of entropy for
learning, butextrinsicrewardsstillserveasthemajordriveofbehavior, andarbitrarymixturesofaction-state
entropy are rarely considered [47]. Our work also relates to reward-free theories of behavior. These minimize
predictions errors [54–59], seek novelty [60–62], or maximize data compression [63], and therefore the major
behavioral driver depends on the agent’s experience with the world. On the other hand, MOP agents find the
action-states that lead to high future occupancy “interesting”, regardless of experience. There are two other
approaches that sit closer to this description, one maximizing mutual information between actions and future
states (empowerment, MPOW) [20, 64–66], and the other minimizing the distance between the actual and a
desired state distribution (free energy principle, FEP) [67, 68]. We show that both MPOW and FEP tend
to collapse to deterministic policies with little behavioral variability. In contrast, MOP results in lively and
seemingly goal-directed behavior by taking behavioral variability and the constraints of embodied agents as
principles.
2 Maximum occupancy principle
2.1 Entropy measure of path space occupancy
We model an agent as a finite action-state MDP in discrete time. The policy π describes the probability
π(a|s) of performing action a, from some set A(s), given that the agent is at state s at some time step, and
p(s′|s,a) is the transition probability from s to a successor state s′ in the next time step given that action a
is performed. Starting at t=0 in state s , an agent performing a sequence of actions and experiencing state
0
transitions τ ≡(s ,a ,s ,...,a ,s ,...) gets a return defined as
0 0 1 t t+1
∞ ∞
R(τ)= (cid:88) γtR(s ,a )=− (cid:88) γtln (cid:0) πα(a |s )pβ(s |s ,a ) (cid:1) , (2.1)
t t t t t+1 t t
t=0 t=0
with action and state weights α>0 and β ≥0, respectively, and discount factor 0<γ <1. A larger return is
obtained when, from s , a low-probability action a is performed and followed by a low-probability transition
t t
2
Figure 1: MOP agents maximize action-state path occupancy. (a) A MOP agent (grey triangle) in the middle of two
roomshasthechoicebetweengoingleftorright. Whenthenumberofactions(blackarrows)ineachroomisthesame,
theagentprefersgoingtotheroomwithmorestatetransitions(bluearrowsindicaterandomtransitionsafterchoosing
moving right or moving left actions, and pink arrow width indicates the probabilities of those actions). (b) When the
states transitions are the same in the two rooms, the MOP agent prefers the room with more available actions. (c) If
there are many absorbing states in the room where many actions are available, the MOP agent avoids it. (d) Even if
there are action and state-transition incentives (in the left room), a MOP agent might prefer a region of state space
where it can reliably get food (right room), ensuring occupancy of future action-state paths. See Supplemental Fig. 8
for a more formal example.
to a state s . Therefore, maximizing the return in Eq. (2.1) favors ‘visiting’ action-states (a ,s ) with a
t+1 t t+1
low transition probability. From s , another low-probability action-state transition is preferred and so on,
t+1
such that low-probability trajectories τ are more rewarding than high-probability ones. Thus, the agent is
pushed to visit action-states that are rare or ‘unoccupied’, implementing the intuitive notion of MOP. Due to
the freedom to choose action a given state s and the uncertainty of the resulting next state s , apparent
t t t+1
in Eq. (2.1), the term ‘action-states’ used here is more natural than ‘state-actions’. We stress that this return
is purely intrinsic, namely, there is no extrinsic reward that the agent seeks to maximize. We define intrinsic
rewards as any reward signal that depends on the policy or the state transition probability, and therefore it
can change with the course of learning as the policy is improved, or the environment is learnt. An extrinsic
reward is the complementary set of reward signals: any function R(s,a) that is both policy-independent and
transition probability-independent, and therefore it does not change with the course of improving the policy
or learning the state transition probability of the environment.
The agent is assumed to optimize the policy π to maximize the state-value V (s), defined as the expected
π
return
(cid:34) (cid:88) ∞ (cid:12) (cid:35)
V (s)≡E [R(τ)|s =s]=E γt(αH(A|s )+βH(S′|s ,a ))(cid:12)s =s (2.2)
π at∼π,st+1∼p 0 at∼π,st+1∼p t t t (cid:12) 0
t=0
given the initial condition s =s and following policy π, that is, the expectation is over the a ∼π(·|s ) and
0 t t
s ∼p(·|s ,a ), t≥0. In the last identity, we have rewritten the expectations of the terms in Eq. (2.1) as a
t+1 t t
(cid:80)
discountedandweightedsumofactionandsuccessorstateconditionalentropiesH(A|s)=− π(a|s)lnπ(a|s)
a
and H(S′|s,a)=− (cid:80) p(s′|s,a)lnp(s′|s,a), respectively, averaged over previous states and actions.
s′
3
We define a MOP agent as the one that optimizes the policy to maximize the state-value in Eq. (2.2).
The entropy representation in Eq. (2.2) of MOP has several implications. First, agents prefer regions of state
space that lead to a large number of successor states (Fig. 1a) or larger number of actions (Fig. 1b). Second,
death (absorbing) states where only one action-state (i.e., “stay”) is available forever are naturally avoided
by a MOP agent, as they promise zero future action and state entropy (Fig. 1c). Therefore, our framework
implicitly incorporates a survival instinct. Finally, regions of state space where there are “rewarding” states
that increase the capacity of the agent to visit further action-states (such as filling an energy reservoir) are
more frequently visited than others (Fig. 1d).
We found that maximizing the discounted action-state path entropy in Eq. (2.2) is the only reasonable
way of formalizing MOP, as it is the only measure of action-state path occupancy in Markov chains consistent
with the following intuitive conditions (Supplemental Sec. A.1): if a path τ has probability p, visiting it
results in an occupancy gain C(p) that (i) decreases with p and (ii) is first-order differentiable. Condition (i)
implies that visiting a low probability path increases occupancy more than visiting a high probability path,
and our agents should tend to occupy ‘unoccupied’ path space; condition (ii) requires that the measure should
be smooth. We also ask that (iii) the occupancy of paths, defined as the expectation of occupancy gains over
paths given a policy, is the sum of the expected occupancies of their subpaths (additivity condition). This
last condition implies that agents can accumulate occupancy over time by keeping visiting low-probability
action-states, but the accumulation should be consistent with the Markov property of the decision process.
These conditions are similar but not exactly the same as Shannon’s information measure [69] (Supplemental
Sec. A.1).
2.2 Optimal policy and state-value function
The state-value V (s) in Eq. (2.2) can be recursively written using the values of successor states through the
π
standard Bellman equation
(cid:88) (cid:88)
V (s) = αH(A|s)+β π(a|s)H(S′|s,a)+γ π(a|s)p(s′|s,a)V (s′)
π π
a a,s′
(cid:88)
= π(a|s)p(s′|s,a)(−αlnπ(a|s)−βlnp(s′|s,a)+γV (s′)), (2.3)
π
a,s′
where the sum is over the available actions a from state s, A(s), and over the successor states s′ given
the performed action at state s. The optimal policy π∗ that maximizes the state-value is defined as
π∗ =argmax V and the optimal state-value is
π π
V∗(s)=maxV (s), (2.4)
π
π
where the maximization is with respect to the {π(·|·)} for all actions and states. To obtain the optimal policy,
we first determine the critical points of the expected return V (s) in Eq. (2.3) using Lagrange multipliers
π
(Supplemental Sec. A.2). The optimal state-value V∗(s) is found to obey the non-linear self-consistency set of
equations
(cid:34) (cid:32) (cid:33)(cid:35)
(cid:88) (cid:88)
V∗(s)=αlnZ(s)=αln exp α−1βH(S′|s,a)+α−1γ p(s′|s,a)V∗(s′) , (2.5)
a s′
where Z(s) is the partition function, defined by substitution, and the critical policy satisfies
(cid:32) (cid:33)
1 (cid:88)
π∗(a|s)= exp α−1βH(S′|s,a)+α−1γ p(s′|s,a)V∗(s′) . (2.6)
Z(s)
s′
We find that the solution to the non-linear system of Eqs. (2.5) is unique and, moreover, the unique solution
is the absolute maximum of the state-values over all policies (Supplemental Sec. A.3).
To determine the actual value function from such non-linear set of equations, we derive an iterative
map, a form of value iteration that exactly incorporates the optimal policy at every step. Defining z =
i
4
exp (cid:0) α−1γV(s ) (cid:1) , p =p(s |s ,a ) and H =α−1βH(S′|s ,a ), Eq. (2.5) can be turned into the iterative
i ijk j i k ik i k
map
 γ
z i (n+1) = (cid:88) w ik eHik (cid:89)(cid:16) z j (n) (cid:17)pijk  (2.7)
k j
for n ≥ 0 and with initial conditions z(0) > 0. Here, the matrix with coefficients w ∈ {0,1} indicate
i ik
whether action a is available at state s (w =1) or not (w =0), and j extends over all states, with the
k i ik ik
understanding that if a state s is not a possible successor from state s after performing action a then
j i k
p = 0. We find that the infinite series z(n) defined in Eq. (2.7) converges to a finite limit z(n) → z∞
ijk i i i
regardless of the initial condition in the positive first orthant, and that V∗(s )=αγ−1lnz∞ is the optimal
i i
state-value function, which solves Eq. (2.5) (Supplemental Sec. A.3). Iterative maps similar to Eq. (2.7)
have been studied before [37, 70], subsequently shown to have uniqueness [71] and convergence guarantees
[45, 72] in the absence of state entropy terms. A summary of results and particular examples can be found in
Supplemental Sec. A.4.
We note that in the definition of return in Eq. (2.2) we could replace the absolute action entropy terms
(cid:80)
H(A|s) by relative entropies of the form −D (π(a|s)||π (a|s)) = π(a|s)ln(π (a|s)/π(a|s)), as in KL-
KL 0 a 0
regularization[37,41,46,70],butintheabsenceofanyextrinsicrewards. Inthiscase,oneobtainsanequation
identicalto(2.7)wherethecoefficientsw aresimplyreplacedbyπ (a |s ),onetoone. Thisapparentlyminor
ik 0 k i
variation undercovers a major qualitative difference between absolute and relative action entropy objectives:
(cid:80)
as w ≥1, absolute entropy-seeking favors visiting states with a large action accessibility, that is, where
k ik
(cid:80) (cid:80)
the sum w and thus the argument of Eq. (2.7) tends to be largest. In contrast, as π (a |s )=1,
k ik k 0 k i
maximizingrelativeentropiesprovidesnopreferenceforstatesswithlargenumberofaccessibleactions|A(s)|.
This happens even if the default policy is uniform in the actions, as then the immediate intrinsic return
becomes −D (π(a|s)||π (a|s))=H(A|s)−ln|A(s)|, instead of H(A|s). The negative logarithm penalizes
KL 0
visiting states with large number of actions, which is the opposite goal to occupying action-state path space
(see details in Supplemental Sec. A.6).
3 Results
3.1 MOP agents quickly fill physical space
In very simple environments with high symmetry and little constraints, like open space, maximizing path
occupancy amounts to performing a random walk that chooses at every step any available action with equal
probability. However, in realistic environments where space is not homogeneous, where there are energetic
limitations for moving, or where there are absorbing states, a random walk is no longer optimal. To illustrate
howinterestingbehaviorsarisefromMOPinthesecases, wefirsttestedhowaMOPagentmovingina4-room
and 4-food-sources environment (Fig. 2a) compares in occupying physical space to a random walker (RW)
and to a reward seeking agent (R agent). The definition of the three agents are identical in most ways. They
have nine possible movement actions, including not moving; they all have an internal state corresponding to
the available energy, which reduces one unit at every time step and gets increased by a fixed amount (food
gain) whenever a food source is visited; and they can move as long as their energy is non-zero. The total
state space is the Cartesian product between physical space and internal energy. The agents differ however in
their objective function. The MOP agent has a reward-free objective and implements MOP by maximizing
path action entropy, Eq. (2.2). In contrast, the R agent maximizes future discounted reward (in this case,
food), and displays stochastic behavior through an ϵ-greedy action selection, with ϵ matched to the survival of
the MOP agent (Supplemental Sec. A.5 and Fig. 9a). Finally, the random walker is simply an agent that in
each state takes a uniformly random action from the available actions at that state.
We find that the MOP agent generates behaviors that can be dubbed goal-directed and curiosity-driven
(Video 1). First, by storing enough energy in its reservoir, the agent reaches far, entering the four rooms in
the long term (Fig. 2b, left panel), and visiting every location of the arena except when food gain is small
(Fig. 2d, blue line). In contrast, the R agent lingers over one of the food sources for most of the time (Fig. 2b,
middle panel; Video 1). Although its ϵ-greedy action selection allows for brief exploration of other rooms,
the R agent does not on average visit the whole arena (Fig. 2d, orange line). Finally, the random walker
5
Figure 2: Maximizing future path occupancy leads to high occupancy of physical space. (a) Grid-world arena. The
agents have nine available actions (arrows, and staying still) when alive (internal energy E larger than zero) and
away from walls. There are four rooms, each with a small food source in a corner (green diamonds). (b) Probability
of visited spatial states for a MOP agent, an ϵ-greedy reward (R) agent that survives as long as the MOP agent,
and a random walker. Food gain =10 units, maximum reservoir energy =100, episodes of 5×104 time steps, and
(α,β)=(1,0) for the MOP agent. All agents are initialized in the middle of the lower left room. (c) Optimal value
function V∗(s) over locations when energy is E =5. Black arrows represent the optimal policy given by Eq. 2.6; their
length is proportional to the probability of each action. The size of red dots is proportional to the probability of the
do nothing action. (d) Fraction of locations of the arena visited at least once per episode as a function of food gain.
Error bars correspond to s.e.m over 50 episodes. (e) Noisy room problem. The bottom right room of the arena was
noisy, such that agents in this room jump randomly to neighboring locations regardless of their actions. Food gain
equals maximum reservoir energy =100. Histogram of visited locations for an episode as long as in (b) for a MOP
agent with β =0.3 (left) and time fraction spent in the noisy room (right) show that MOP agents with β >0 can
either be attracted to the room or repelled depending on γ.
dies before it has time to visit a large fraction of the physical space (Fig. 2b, right panel). These differences
hold for a large range of food gains (Fig. 2d). The MOP agent, while designed to generate variability, is also
capable of deterministic behavior: when its energy is low, it moves toward the food sources with little to no
variability, a distinct mark of goal-directedness (Fig. 2c, corner spots show that only one action is considered
by optimal policy).
We next considered a slightly more complex environment where actions in one of the rooms lead to
uniformly stochastic transitions to any of the neighboring locations (noisy room –a spatial version of the noisy
TVproblem[57,73]). AMOPagentwithβ >0(seeEq. (2.2))hasapreferenceforstochasticstatetransitions,
and a priori it could get attracted and stuck in the noisy room, where actions do not have any predictable
effect. Indeed, we see that for larger β, which measures the strength of the state entropy contribution to the
agent’s objective, the attraction to the noisy room increases (Fig. 2e, right panel). However, MOP agents also
care about future states, and thus getting stuck in regions where energy cannot be predictably obtained is
avoided by sufficiently long-sighted agents, as shown by the reduction of the time spent in the noisy room
with increasing γ (Fig. 2e; Supplemental Sec. A.5.3). This shows how MOP agents can tradeoff immediate
with future action-state occupancy.
6
Figure 3: Complex hide-and-seek and escaping strategies in a prey-predator example. (a) Grid-world arena. The agent
has nine available actions when alive and far from walls. There is a small food source in a corner (green diamond).
A predator (red, down triangle) is attracted to the agent (gray, up triangle), such that when they are at the same
location, the agent dies. The predator cannot enter the locations surrounded by the brown border. Arrows show a
clockwise trajectory. (b) Histogram of visited spatial states across episodes for the MOP and R agents. The vector
field at each location indicates probability of transition at each location. Green arrows on R agent show major motion
directions associated with its dominant clockwise rotation. (c) Fraction of clockwise rotations (as in panel (a)) to total
rotations as a function of food gain, averaged over epochs of 500 timesteps. Error bars are s.e.m. (d) Optimal value
functions for different energy levels, and same predator position; black arrows indicate optimal policy, as in Fig. 2c.
3.2 Hide and seek in a prey-predator interaction
More interesting behaviors arise from MOP in increasingly complex environments. To show this, we next
considered aprey andapredator ina gridworldwith asafe area(a “home”)and asinglefood source(Fig. 3a).
The prey (a “mouse”, gray up triangle) is the agent whose behavior is optimized by maximizing future action
path entropy, while the predator (a “cat”, red down triangle) acts passively chasing the prey. The state of the
agent consists of its location and energy level, but it also includes the predator’s location being accurately
perceived. The prey can move as in the previous 4-room grid world and it also has a finite energy reservoir.
For simplicity, we only considered a food gain equal to the size of the energy reservoir, such that the agent
fully replenishes its reservoir each time it visits the food source. The predator has the same available actions
as the agent and is attracted to it stochastically, i.e. actions that move the predator towards the agent are
more probable than those that move it away from it (Supplemental Sec. A.5.4).
MOPgeneratescomplexbehaviors, notlimitedtovisitingthefoodsourcetoincreasetheenergybufferand
hide at home. In particular, the agent very often first teases the cat and then performs a clockwise rotation
around the obstacle, which forces the cat to chase it around, leaving the food source free for harvest (Fig.
3a, arrows show an example; Video 2, MOP agent). Importantly, this behavior is not restricted to clockwise
rotations, as the agent performs an almost equal number of counterclockwise rotations to free the food area
(Fig. 3c, MOP agent, blue line). The variability of these rotations in the MOP agent are manifest in the
lack of virtually any preferred directionality of movement in the arena at any single position. Indeed, arrows
pointing toward several directions indicate that on average the mouse moves following different paths to get to
the food source (Fig. 3b, MOP agent). Finally, the optimal value function and optimal policy show that the
MOP agent can display deterministic behaviors as a function of internal state as well as distance to the cat
(Fig. 3d): for instance, it prefers running away from the cat when energy is large (right), and it risks getting
7
Figure 4: Dancing of a MOP cartpole. (a) The cart (brown rectangle) has a pole attached. The cartpole reaches an
absorbingstateifthemagnitudeoftheangleθ exceeds36deg oritspositionreachestheborders. Thereare5available
actions when alive: a big and a small force to either side (arrows on cartpole), and doing nothing (full circle). (b)
Time-shifted snapshots of the pole in the reference frame of the cart as a function of time for the MOP (top) and R
(bottom) agents. (c) Position and angle occupation for a 2×105 time step episode. (d) Here, the right half of the
arena is stochastic, while the left remains deterministic. In the stochastic half, the intended state transition due to an
applied action (force) succeeds with probability 1−η (and thus zero force is applied with probability η). (e) Fraction
of time spent on the right half of the arena increases as a function of β, regardless of the failure probability η. (f)
The fraction has a non-monotonic behavior as a function of η when state entropy is important for the agent (β =1),
highlighting a stochastic resonance behavior. When the agents do not seek state entropy (β =0) the fraction of time
spent by the agent on the right decreases with the failure probability, and thus they avoid the stochastic right side.
γ =0.99 for panels (e,f).
caught to avoid starvation if energy is small (left), both behaviors starkly opposite to stochastic actions.
The behavior of the MOP agent was compared with an R agent that receives a reward of 1 each time it
is alive and 0 otherwise. To promote variable behavior in this agent as well, we implemented an ϵ-greedy
action selection (Supplemental Sec. A.5.4), where ϵ was chosen to match the average lifetime of the MOP
agent (Supplemental Fig. 9b). The behavior of the R agent was strikingly less variable than that of the
MOP agent, spending more time close to the food source (Fig. 3b, R agent). Most importantly, while the
MOP agent performs an almost equal number of clock and counterclockwise rotations, the R agent strongly
prefers the clockwise rotations, reaching 90% of all observed rotations (Video 3, R-agent; Fig. 3c, orange line).
This shows that the R agent mostly exploits only one strategy to survive and displays a smaller behavioral
repertoire than the MOP agent.
3.3 Dancing in an entropy-seeking cartpole
In the previous examples, complex behaviors emerge as a consequence of the presence of obstacles, predators
and limited food sources, but the actual dynamics of the agents are very coarse-grained. Here, we considered
a system with physically realistic dynamics, the balancing cartpole [74, 75], composed of a moving cart with
an attached pole free to rotate (Fig. 4a). The cartpole is assumed to reach an absorbing state when either it
hits a border, or when the pole angle exceeds 36 degrees. Thus, we consider a broad range of angles that
makes the agents reach a larger state space than in standard settings [76]. We discretized the state space and
used a linear interpolation to solve for the optimal value function in Eq. (2.4), and to implement the optimal
8
Figure 5: Modelling altruism through an optimal tradeoff between own action entropy and other’s state entropy. (a)
An agent (gray up triangle) has access to nine movement actions (gray arrows and doing nothing), and open or close a
fence (dashed blue lines). This fence does not affect its movements. A pet (green, down triangle) has access to the
same actions, and chooses one randomly at each timestep, but is constrained by the fence when closed. Pet location is
part of the state of the agent. (b) As β in Eq. (2.2) is increased, the agent tends to leave the fence open for a larger
fraction of time. This helps its pet reach other parts of the arena. Error bars correspond to s.e.m. (c) Occupation
heatmaps for 2000 timestep-episodes for β =0 (left) and β =1 (right). In all cases α=1.
policy in Eq. (2.6), (Supplemental Sec. A.5.5). The MOP agent widely occupies the horizontal position, and
more strikingly it produces a wide variety of pole angles, constantly swinging sideways as if it were dancing
(Video 4, MOP agent; Fig. 4b,c).
We compared the behavior of a MOP agent with that of an R agent that receives a reward of 1 for being
alive and 0 otherwise. The R agent gets this reward regardless of the pole angle and cart position within the
allowed broad ranges, so that behaviors of the MOP and R agents can be better compared without explicitly
favoring in any of them any specific behavior, such as the upright pole position. As expected, the R agent
maintains the pole close to the balanced position throughout most of a long episode (Fig. 4b, bottom),
because it is the furthest to the absorbing states and thus the safest. Therefore, the R agent produces very
little behavioral variability (Fig. 4c, right panel) and no movement that could be dubbed ‘dancing’ (Video 4,
R agent). Although both MOP and R agents use a similar strategy which keeps the pole pointing towards
the center for substantial amounts of time (Fig. 4c, positive angles correlate with positive positions in both
panels), the behavior of the R agent is qualitatively different, and is best described as a bang-bang sort of
control for which the angle is kept very close to zero while the cart is allowed to travel and oscillate around
the origin, which is more apparent in the actual paths of the agent (see trajectories in phase space in Video
5). We also find that the R agent does not display much variability in state space even after using an ϵ-greedy
action selection (Supplemental Fig. 10, Video 6), with ϵ chosen to match average lifetimes between agents
(Supplemental Fig. 9c). This result showcases that the MOP agent exhibits the most appropriate sort of
variability for a given average lifetime.
Wefinallyintroducedaslightvariationtotheenvironment, wheretherighthalfofthearenahasstochastic
state transitions. Here, when agents choose an action (force) to be executed, a state transition in the desired
direction occurs with probability 1−η, and a transition corresponding to zero force occurs with probability η
(Fig. 4d). Therefore, a MOP agent that seeks state entropy (β >0) will show a preference for the right side,
where there is in principle higher state entropy resulting from the stochastic transitions over more successor
statesthanontheleftside. Indeed, wefindthatMOPagentsspendmoretimeontherightsideasβ increases,
regardless of the probability η (Fig. 4e). For fixed γ, spending more time on the right side can bring the life
expectancy to decrease significantly depending on β and η (Supplemental Fig. 9 d-e). Interestingly, for β >0
there is an optimal value of the noise η that maximizes the fraction of time spent on the right side (Fig. 4f),
which is a form of stochastic resonance. Therefore, for different β, qualitatively different behaviors emerge as
a function of the noise level η.
3.4 MOP agents can also seek entropy of others
Next, we considered an example where an agent seeks to occupy path space, which includes another agent’s
location as well as its own. The agent can freely move (Fig. 5a; grey triangle) and open or close a fence by
pressing a lever in a corner (blue triangle). The pet of the agent (green triangle) can freely move if the fence is
9
open, but when the fence is closed the pet is confined to move in the region where it is currently located. The
pet moves randomly at each step, but its available actions are restricted by its available space (Supplemental
Sec. A.5.6).
To maximize action-state path entropy, the agent ought to trade off the state entropy resulting from
letting the pet free with the action entropy resulting from using the open-close action when visiting the lever
location. The optimal tradeoff depends on the relative strength of action and state entropies. In fact, when
state entropy weighs as much as action entropy (α=β =1), the fraction of time that the agent leaves the
fence open is close to 1 (rightmost point in Fig. 5b) so that the pet is free to move (Fig. 5c, right panel; β =1
MOP agent). However, when the state entropy has zero weight (α=1,β =0), the fraction of time that the
fence remains open is close to 0.5 (leftmost point in Fig. 5b) and the pet remains confined to the right side
for most of the time (Fig. 5c, left panel; β =0 MOP agent), the region where it was initially placed. As a
function of β the fraction of time the fence is open increases. Therefore, the agent gives more freedom to
its pet, as measured by the pet’s state entropy, by curtailing its own action freedom, as measured by action
entropy, thus becoming more "altruistic".
3.5 MOP compared to other reward-free approaches
One important question is how MOP compares to other reward-free, motivation-driven theories of behavior.
Here we focus on two popular approaches: empowerment and the free energy principle. In empowerment
(MPOW) [20, 64–66] agents maximize the mutual information between n-step actions and the successor
states resulting from them [20, 77], a measure of their capability to perform diverse courses of actions with
predictable consequences. MPOW formulates behavior as greedy maximization of empowerment [20, 64], such
that agents move to accessible states with the largest empowerment (maximal mutual information), and stay
there with high probability.
We applied MPOW to the gridworld and cartpole environments (Fig. 6). In the gridworld, MPOW agents
(5-step MPOW, see Supplemental Sec. A.7.1) prefer states from where they can reach many distinct states,
such as the middle of a room. However, due to energetic constraints, they also gravitate towards the food
source when energy is low, and they alternate between these two locations ad nauseam (Fig. 6a, middle;
Video7). Inthecartpole, MPOWagents(3-stepMPOW[64], seeSupplementalSec. A.7.1)favourtheupright
position because, being an unstable fixed point, it is the state with highest empowerment, as previously
reported [64, 78]. Given the unstable equilibrium, the MPOW agent gets close to it but needs to continuously
adjust its actions when greedily maximizing empowerment (Fig. 6b, middle; Video 8). The paths traversed by
MPOW agents in state space are highly predictable, and they are similar to the ones of the R agent (see Fig.
4c). The only source of stochasticity comes from the algorithm, which approximately calculates empowerment,
and thus a more precise estimation of empowerment leads to even less variability.
In the free energy principle (FEP), agents seek to minimize the minus log probability, called surprise, of
a subset of desired states via the minimization of an upper bound, called free energy. This minimization
reduces behavioral richness by making a set of desired (homeostatic) states highly likely [67, 68], rendering
this approach almost opposite to MOP. In a recent MDP formalization, FEP agents aim to minimize the
(future) expected free energy (EFE) [79], which equals the future cumulative KL divergence between the
probability of states and the desired (target) probability of those states (see Supplemental Sec. A.7.2 for
details). Even though this objective contains the standard exploration entropy term on state transitions
[68, 80], we prove that the optimal policy is deterministic (see Supplemental Sec. A.7.2).
As a consequence, we find that in both the gridworld and cartpole environments, the behavior of the
EFE agent (receding horizon H =200) is much less variable than the MOP agent in general (Fig. 6a, right
panel for the gridworld, Video 7; and b, right panel, for the cartpole, Video 8). The only source of action
variability in the EFE agent is due to the degeneracy of the expected free energy, and thus behavior collapses
to a deterministic policy as soon as the target distribution is not perfectly uniform (see Supplemental Sec.
A.7.2.2 for details). We finally prove that under discounted infinite horizon, and assuming a deterministic
environment, the EFE agent is equivalent to a classical reward maximizer agent with reward R=1 for all
non-absorbing states and R = 0 for the absorbing states (Supplemental Sec. A.7.2). In conclusion, MOP
generates much more variable behaviors than MPOW and FEP.
10
Figure6: Empowerment(MPOW)andFreeEnergyPrinciple(FEP)lackrobustoccupationofaction-states. (a)Inthe
grid-world environment, MPOW and expected free energy (EFE) only visit a restricted portion of the arena. Initial
position was the center of a room (x,y)=(3,3). (b) In the cartpole environment, both MPOW and EFE shy away
from large angles, producing a limited repertoire of predictable behaviors.
3.6 MOP in continuous and large action-state spaces
Theexamplessofarcanbesolvedexactlywithnumericalmethods, withoutrelyingonfunctionapproximation
of the value function or the policy, which could obscure the richness of the resulting behaviors. However, one
important question is whether our approach scales up to large continuous action-state spaces where no exact
solutions are available. To show that MOP generates rich behaviors even in high-dimensional agents, we used
a quadruped from Gymnasium [81] without imposing any explicit fine-tuned reward function (Fig. 7a). The
only externally imposed conditions are the absorbing states, which are reached when either the agent falls
(given by the torso touching the ground), or the torso reaches a maximum height [81].
We first trained the MOP agent by approximating the state-value function, Eq. (2.5), using the soft-actor
critic (SAC) architecture [40] with zero rewards, which corresponds to the case α=1 and β =0. The MOP
agent learns to stabilize itself and walk around, sometimes jumping, spinning and moving up and down the
legs, without any instructions to do so (Video 9). The MOP agent exhibits variable and long excursions over
state space (Fig. 7b,c blue) and displays a broad distribution of speeds (Fig. 7d, blue). We compared the
MOP agent with an R agent that obtains a reward of R=1 whenever it is alive and R=0 when it reaches
an absorbing state. As before, we add variability to the R agent with an ϵ-greedy action selection, adjusting
ϵ so that the average lifetime of the R agent matched that of the MOP agent (Supplemental Fig. 11a). In
contrast to the MOP agent, the R agents exhibit much shorter excursions (Fig. 7b,c yellow) and a velocity
distribution that peaks around zero, indicating prolonged periods spent with no translational movement (Fig.
7d, yellow). When visually compared, the behavior for MOP and R agents shows stark differences (Video 9).
While the MOP agent elicits variable behaviors, it is also capable of generating deterministic, goal-directed
behaviors when needed. To show this, we added a food source in the arena and extended the state of the
agent with its internal energy. Now the agent can also die of starvation when the internal energy hits zero
(Fig. 7e). As expected, when the initial location of the MOP quadruped is far from the food source, it directly
moves to the food source to avoid dying from starvation (Fig. 7f). After the food source is reached for the
first time, the MOP quadruped generates random excursions away from the food source. During these two
phases, the agent displays very different speed distributions (Fig. 7g), showing also quantitative differences in
the way it moves (see a comparison with the R agent in Supplemental Fig. 11, and Video 10).
Finally, we modified the environment by adding state transition noise of various magnitudes in one half of
thearena(x>0), whiletheotherhalfremaineddeterministic. Wefindthattheagent’sbehaviorismodulated
by β, which controls the preference of state transition entropy (see details in Supplemental Sec. A.5.7). As
expected, for fixed α and positive noise magnitude, MOP agents show increasing preference toward the noisy
11
Figure 7: MOP in high-dimensional states generates variable and goal-directed behaviors. (a) The quadruped
environment, adapted from Gymnasium, serves as the testing environment. The x,y dimensions are unbounded. (b)
Trajectories of the center of mass of the torso of the MOP (left panel) and R (right) agents. MOP occupies more
space for approximately the same survival time (see Supplemental Fig. 11a). (c-d) Distribution of the planar distance
d from the origin (c) and planar speed v (d) for MOP (blue) and R (yellow) agents. (e) In a new environment, a
xy
food source (green ball) is available so that the MOP agent can replenish its internal energy to avoid starvation. (f)
Trajectories of the MOP agent before (left) and after (right) getting to the food source. Colormap defined by the
energy level of the agent. (g) Distribution of the planar speed showcasing changes before (dark blue) and after (light
blue) the MOP agent reaches the food source for the first time. Distributions computed only on the tests where the
quadruped finds the food source.
side as β increases (Supplemental Fig. 12). However, as noise magnitude increases, and for fixed β, MOP
agents tend to avoid the noisy side to prevent them from falling. This shows that MOP agents can exhibit
approach and avoidance behaviors depending on the environment’s stochasticity and their β hyperparameter.
4 Discussion
Often, the success of agents in nature is not measured by the amount of reward obtained, but by their
ability to expand in state space and perform complex behaviors. Here we have proposed that a major goal of
intelligence is to ‘occupy path space’. Extrinsic rewards are thus the means to move and occupy action-state
path space, not the goal of behavior. In an MDP setting, we have shown that the intuitive notion of path
occupancy is captured by future action-state path entropy, and we have proposed that behavior is driven
by the maximization of this intrinsic goal –the maximum occupancy principle (MOP). We have solved the
associated Bellman equation and provided a convergent iterative map to determine the optimal policy.
In several discrete and continuous state examples we have shown that MOP, along with the agent’s
constraintsanddynamics,leadstocomplexbehaviorsthatarenotobservedinothersimplerewardmaximizing
agents. Quick filling of physical space by a moving agent, hide-and-seek behavior and variable escaping
routes in a predator-prey example, dancing in a realistic cartpole dynamical system, altruistic behavior in
an agent-and-pet duo and successful, vigorous movement in a high-dimensional quadruped are all behaviors
that strike as being playful, curiosity-driven and energetic. To the human eye, these behaviors look genuinely
goal-directed, like approaching to the food source when the energy level is low or escaping from the cat when
12
it gets close to the mouse (see Figs. 2c and 3d). Although MOP agents do not have any extrinsically designed
goal, like eating or escaping, they generate these deterministic, goal-directed behaviors whenever necessary so
that they can keep moving in the future and maximize future path action-state entropy (see Supplemental Sec.
A.8). These results show that the presence of internal states (e.g. energy) and absorbing states (e.g, having
zero energy or being eaten) are critical for generating interesting behaviors, as getting close to different types
of absorbing states triggers qualitatively different behaviors. This capability of adapting variability depending
oninternalstateshasbeenoverlookedintheliteratureandisessentialtoobtainingthegoal-directedbehaviors
we have shown here. In parallel, when basic energy and safety conditions are met, behaviors are lively and
somewhat risky, like when the cartpole gets close to the borders of the arena, and therefore our approach can
lead to novel ways of thinking about risk–seeking behaviors [82].
A related set of algorithms, known as empowerment, have also proposed using reward-free objectives as
the goal of behavior [20, 64, 66]. In this approach, the mutual information between a sequence of actions and
the final state is maximized. This makes empowerment agents prefer states where actions lead to large and
predictable changes, such as unstable fixed points [64]. We have shown that one drawback is that empowered
agents tend to remain close to those states without producing diverse behavioral repertoires (see Fig. 6b
and Video 8), as it also happens in causal entropy approaches [83]. Another difference is that empowerment
is not additive over paths because the mutual information of a path of actions with the path of states is
not the sum of the per-step mutual information, and thus it cannot be formalized as a cumulative per-step
objective (Supplemental Sec. A.9) [64, 66, 72, 84], in contrast to action-state path entropy. We note, however,
that an approximation to empowerment having the desired additive property could be obtained from our
framework by putting β <0 in Eq. (2.2), such that more predictable state transitions are preferred. Similarly
to empowerment, we have also shown that agents following the free energy principle [67, 79] collapse behavior
to deterministic policies in known environments (see Fig. 6b and Video 8). Other reward-free RL settings
and pure exploration objectives have been proposed in the past [48, 50, 52, 58, 85–88], but this body of
work typically investigates how to efficiently sample MDPs to construct near-optimal policies when reward
functions are introduced in the exploitation phase. More importantly, this work differs from ours in that the
goal-directedness that MOP displays entails behavioral variability at its core, even in known environments
(see examples above). Finally, other overlapping reward-free approaches focus on the unsupervised discovery
of skills, by encouraging diversity [26, 89–91]. While the motivation is similar, they focus on skill-conditioned
policies, whereas our work demonstrates that complex sequences of behaviors are possible working from the
primitive actions of agents, although a possible future avenue for MOP is to apply it to temporally extended
actions [92]. In addition, these works define tasks based on extrinsic rewards, whereas we have shown that
internal state signals are sufficient to let agents define sub-tasks autonomously.
Our approach is conceptually different as well to hybrid approaches that combine extrinsic rewards
with action entropy or KL regularization terms [37, 38, 41, 43, 93] for two main reasons. First, entropy
seeking behavior does not pursue any form of extrinsic reward maximization. But most importantly, using
KL-regularization using a default policy π (a|s) in our framework would be self-defeating. This is because
0
the absolute action entropy terms H(A|s) in the expected return in Eq. (2.2) favor visiting states where a
large set of immediate and future action-states are accessible. In contrast, using relative action entropy (KL)
precludes this effect by normalizing the number of accessible actions, as we have shown above. Additionally,
minimizing the KL divergence with a uniform default policy and without extrinsic rewards leads to an optimal
policy that is uniform regardless of the presence of absorbing states, equivalent to a random walker, which
shows that a pure KL objective does not lead to interesting behaviors (Supplemental Sec. A.6, Supplemental
Fig. 13). The idea of having a variable number of actions that depend on the state is consistent with the
concept of affordance [94]. While we do not address the question of how agents get the information about
the available actions, an option would be to use the notion of affordances as actions [95]. Secondly, while
previous work has studied the performance benefits of either action [40], state [42, 48] or equally weighted
action-state [44, 96] steady-state entropies, our work proposes mixing them arbitrarily through path entropy,
leading to a more general theory without any loss in mathematical tractability [47]. This arbitrary weight
mixing lets us model more diverse phenomena. For example, for the right combination of state entropy weight
β, and lookahead horizon, controlled by γ, MOP agents could get stuck in a noisy TV, consistent with the
observation that humans have a preference for noisy TVs under particular conditions [97]. However, it can
also capture the avoidance of noisy TVs for sufficiently-long-sighted agents (see Fig. 2e).
We have also shown that MOP is scalable to high-dimensional problems and when the state-transition
13
matrix is unknown, using the soft-actor critic architecture [98] to approximate the optimal policy prescribed
by MOP. Nevertheless, several steps remain to have a more complete MOP theory with learning. Previous
related attempts have introduced Z-learning [37, 70] and G-learning [99] using off-policy methods, so our
results could be extended to learning following similar lines. Other possibilities are using transition estimators
using counts or pseudo-counts [60], or hashing [61], for the learning of the transition matrices. One potential
advantage of our framework is that, as entropy-seeking behavior obviates extrinsic rewards, those rewards do
not need to be learned and optimized, and thus the learning problem reduces to transition matrices learning.
In addition, modeling and injecting prior information could be particularly simple in our setting in view
that intrinsic entropy rewards can be easily bounded before the learning process if action space is known.
Therefore, initializing the state-value function to the lower or upper bounds of the action-state path entropy
could naturally model pessimism or optimism during learning, respectively.
Allinall,wehaveintroducedMOPasanoveltheoryofbehavior,whichpromisesnewwaysofunderstanding
goal-directedness without reward maximization, and that can be applied to artificial agents to discover by
themselves ways of surviving and occupying action-state space.
Acknowledgments
This work is supported by the Howard Hughes Medical Institute (HHMI, ref 55008742), ICREA Academia
2022 and MINECO (Spain; BFU2017-85936-P) to R.M.-B, and MINECO/ESF (Spain; PRE2018-084757) to
J.R.-R. We thank Jose Apesteguia, Luca Bonatti, Ignasi Cos, and Benjamin Hayden for very useful comments.
Code and data availability
ThecodetogeneratetheresultsandvariousfiguresisavailableasPythonandJuliacodealongwithguidednote-
books to reproduce the figures at this public GitHub repository:
https://github.com/jorgeerrz/occupancy_max_paper. All data are in this public repository, except
for the specific data for the ant experiment, which are available upon request.
References
[1] Richard M Ryan and Edward L Deci. Intrinsic and extrinsic motivations: Classic definitions and new
directions. Contemporary educational psychology, 25(1):54–67, 2000. Publisher: Elsevier.
[2] Pierre-Yves Oudeyer, Frdric Kaplan, and Verena V Hafner. Intrinsic motivation systems for autonomous
mental development. IEEE transactions on evolutionary computation, 11(2):265–286, 2007. Publisher:
IEEE.
[3] Karen E Adolph and Sarah E Berger. Motor development. Handbook of child psychology, 2, 2007.
Publisher: Wiley Online Library.
[4] Peter F MacNeilage and Barbara L Davis. On the origin of internal structure of word forms. Science,
288(5465):527–531, 2000. Publisher: American Association for the Advancement of Science.
[5] Laura Ann Petitto and Paula F Marentette. Babbling in the manual mode: Evidence for the ontogeny of
language. Science, 251(5000):1493–1496, 1991. Publisher: American Association for the Advancement of
Science.
[6] Arne Dietrich. The cognitive neuroscience of creativity. Psychonomic bulletin & review, 11(6):1011–1026,
2004. Publisher: Springer.
[7] Celeste Kidd and Benjamin Y Hayden. The psychology and neuroscience of curiosity. Neuron, 88(3):
449–460, 2015. Publisher: Elsevier.
[8] Jacqueline Gottlieb, Pierre-Yves Oudeyer, Manuel Lopes, and Adrien Baranes. Information-seeking,
curiosity, and attention: computational and neural mechanisms. Trends in cognitive sciences, 17(11):
585–593, 2013. Publisher: Elsevier.
14
[9] John Gittins, Kevin Glazebrook, and Richard Weber. Multi-armed bandit allocation indices. John Wiley
& Sons, 2011.
[10] Bruno B Averbeck. Theory of choice in bandit, information sampling and foraging tasks. PLoS
computational biology, 11(3):e1004164, 2015. Publisher: Public Library of Science San Francisco, CA
USA.
[11] Bradley B Doll, Dylan A Simon, and Nathaniel D Daw. The ubiquity of model-based reinforcement
learning. Current opinion in neurobiology, 22(6):1075–1081, 2012. Publisher: Elsevier.
[12] Maya Zhe Wang and Benjamin Y Hayden. Latent learning, cognitive maps, and curiosity. Current
Opinion in Behavioral Sciences, 38:1–7, 2021. Publisher: Elsevier.
[13] JohnVonNeumannandOskarMorgenstern. Theoryofgamesandeconomicbehavior. Princetonuniversity
press, 2007.
[14] Richard S Sutton, Andrew G Barto, and others. Introduction to reinforcement learning. 1998. Publisher:
MIT press Cambridge.
[15] Daniel Kahneman and Amos Tversky. Prospect theory: An analysis of decision under risk. In Handbook
of the fundamentals of financial decision making: Part I, pages 99–127. World Scientific, 2013.
[16] David Silver, Satinder Singh, Doina Precup, and Richard S. Sutton. Reward is enough. Artifi-
cial Intelligence, 299:103535, 2021. URL https://www.sciencedirect.com/science/article/pii/
S0004370221000862. Publisher: Elsevier.
[17] Carla J Rash, Jeremiah Weinstock, and Ryan Van Patten. A review of gambling disorder and substance
use disorders. Substance abuse and rehabilitation, 7:3, 2016. Publisher: Dove Press.
[18] TamásÁgh,GáborKovács,DylanSupina,ManjiriPawaskar,BarryKHerman,ZoltánVokó,andDavidV
Sheehan. A systematic review of the health-related quality of life and economic burdens of anorexia
nervosa, bulimia nervosa, and binge eating disorder. Eating and Weight Disorders-Studies on Anorexia,
Bulimia and Obesity, 21(3):353–364, 2016. Publisher: Springer.
[19] John M McNamara and Alasdair I Houston. The common currency for behavioral decisions. The
American Naturalist, 127(3):358–378, 1986. Publisher: University of Chicago Press.
[20] Alexander S Klyubin, Daniel Polani, and Chrystopher L Nehaniv. Empowerment: A universal agent-
centric measure of control. In 2005 ieee congress on evolutionary computation, volume 1, pages 128–135.
IEEE, 2005.
[21] Joel Lehman and Kenneth O Stanley. Abandoning objectives: Evolution through the search for novelty
alone. Evolutionary computation, 19(2):189–223, 2011. Publisher: MIT Press.
[22] Satinder Singh, Richard L Lewis, and Andrew G Barto. Where do rewards come from. In Proceedings of
the annual conference of the cognitive science society, pages 2601–2606. Cognitive Science Society, 2009.
[23] TonyZhang,MatthewRosenberg,PietroPerona,andMarkusMeister. Endotaxis: AUniversalAlgorithm
for Mapping, Goal-Learning, and Navigation. bioRxiv, 2021. Publisher: Cold Spring Harbor Laboratory.
[24] Jürgen Schmidhuber. A possibility for implementing curiosity and boredom in model-building neural
controllers. In Proc. of the international conference on simulation of adaptive behavior: From animals to
animats, pages 222–227, 1991.
[25] Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart J Russell, and Anca Dragan. Inverse reward
design. Advances in neural information processing systems, 30, 2017.
[26] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need:
Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018.
15
[27] JulianSchrittwieser,IoannisAntonoglou,ThomasHubert,KarenSimonyan,LaurentSifre,SimonSchmitt,
Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, and others. Mastering atari, go, chess
and shogi by planning with a learned model. Nature, 588(7839):604–609, 2020. Publisher: Nature
Publishing Group.
[28] TBAsafa, TMAfonja, EAOlaniyan, andHOAlade. Developmentofavacuumcleanerrobot. Alexandria
engineering journal, 57(4):2911–2920, 2018. Publisher: Elsevier.
[29] StephenJKlineandNathanRosenberg. Anoverviewofinnovation. Studies on science and the innovation
process: Selected works of Nathan Rosenberg, pages 173–203, 2010. Publisher: World Scientific.
[30] Robert C Wilson, Elizabeth Bonawitz, Vincent D Costa, and R Becket Ebitz. Balancing exploration and
exploitation with information and randomization. Current opinion in behavioral sciences, 38:49–56, 2021.
Publisher: Elsevier.
[31] Rubén Moreno-Bote, David C Knill, and Alexandre Pouget. Bayesian sampling in visual perception.
Proceedings of the National Academy of Sciences, 108(30):12491–12496, 2011. Publisher: National Acad
Sciences.
[32] StefanoRecanatesi,UlisesPereira-Obilinovic,MasayoshiMurakami,ZacharyMainen,andLucaMazzucato.
Metastable attractors explain the variable timing of stable behavioral action sequences. Neuron, 110(1):
139–153, 2022. Publisher: Elsevier.
[33] Abel Corver, Nicholas Wilkerson, Jeremiah Miller, and Andrew Gordus. Distinct movement patterns
generate stages of spider web building. Current Biology, 31(22):4983–4997, 2021. Publisher: Elsevier.
[34] PauleDagenais,SeanHensman,ValérieHaechler,andMichelCMilinkovitch. Elephantsevolvedstrategies
reducingthebiomechanicalcomplexityoftheirtrunk. Current Biology,31(21):4727–4737,2021. Publisher:
Elsevier.
[35] Gabriela Mochol, Roozbeh Kiani, and Rubén Moreno-Bote. Prefrontal cortex represents heuristics that
shape choice bias and its integration into future behavior. Current Biology, 31(6):1234–1244, 2021.
Publisher: Elsevier.
[36] Fanny Cazettes, Masayoshi Murakami, Alfonso Renart, and Zachary F Mainen. Reservoir of decision
strategies in the mouse brain. bioRxiv, 2021. Publisher: Cold Spring Harbor Laboratory.
[37] Emanuel Todorov. Efficient computation of optimal actions. Proceedings of the national academy of
sciences, 106(28):11478–11483, 2009. Publisher: National Acad Sciences.
[38] Brian D Ziebart. Modeling purposeful adaptive behavior with the principle of maximum causal entropy.
Carnegie Mellon University, 2010.
[39] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement Learning with Deep
Energy-Based Policies, July 2017. URL http://arxiv.org/abs/1702.08165. arXiv:1702.08165 [cs].
[40] TuomasHaarnoja, AurickZhou, PieterAbbeel, andSergeyLevine. Softactor-critic: Off-policymaximum
entropy deep reinforcement learning with a stochastic actor. In International conference on machine
learning, pages 1861–1870. PMLR, 2018.
[41] John Schulman, Xi Chen, and Pieter Abbeel. Equivalence between policy gradients and soft q-learning.
arXiv preprint arXiv:1704.06440, 2017.
[42] Gergely Neu, Anders Jonsson, and Vicenç Gómez. A unified view of entropy-regularized markov decision
processes. arXiv preprint arXiv:1705.07798, 2017.
[43] Karol Hausman, Jost Tobias Springenberg, Ziyu Wang, Nicolas Heess, and Martin Riedmiller. Learning
anembeddingspacefortransferablerobotskills. InInternationalConferenceonLearningRepresentations,
2018.
16
[44] Naftali Tishby and Daniel Polani. Information theory of decisions and actions. In Perception-action
cycle, pages 601–636. Springer, 2011.
[45] Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between value
and policy based reinforcement learning. Advances in neural information processing systems, 30, 2017.
[46] Alexandre Galashov, Siddhant M Jayakumar, Leonard Hasenclever, Dhruva Tirumala, Jonathan Schwarz,
Guillaume Desjardins, Wojciech M Czarnecki, Yee Whye Teh, Razvan Pascanu, and Nicolas Heess.
Information asymmetry in KL-regularized RL. arXiv preprint arXiv:1905.01240, 2019.
[47] Dmytro Grytskyy, Jorge Ramírez-Ruiz, and Rubén Moreno-Bote. A general Markov decision process
formalism for action-state entropy-regularized reward maximization, February 2023. URL http://arxiv.
org/abs/2302.01098. arXiv:2302.01098 [cs].
[48] Elad Hazan, Sham Kakade, Karan Singh, and Abby Van Soest. Provably efficient maximum entropy
exploration. In International Conference on Machine Learning, pages 2681–2691. PMLR, 2019.
[49] Hao Liu and Pieter Abbeel. Behavior from the void: Unsupervised active pre-training. Advances in
Neural Information Processing Systems, 34:18459–18473, 2021.
[50] Mirco Mutti, Lorenzo Pratissoli, and Marcello Restelli. Task-agnostic exploration via policy gradient of a
non-parametric state entropy estimate. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 35, pages 9028–9036, 2021. Issue: 10.
[51] Younggyo Seo, Lili Chen, Jinwoo Shin, Honglak Lee, Pieter Abbeel, and Kimin Lee. State entropy
maximization with random encoders for efficient exploration. In International Conference on Machine
Learning, pages 9443–9454. PMLR, 2021.
[52] ChuhengZhang,YuanyingCai,LongboHuang,andJianLi. ExplorationbymaximizingRényientropyfor
reward-free RL framework. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35,
pages 10859–10867, 2021. Issue: 12.
[53] Susan Amin, Maziar Gomrokchi, Harsh Satija, Herke van Hoof, and Doina Precup. A Survey of
Exploration Methods in Reinforcement Learning. arXiv preprint arXiv:2109.00157, 2021.
[54] Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network
distillation. arXiv preprint arXiv:1810.12894, 2018.
[55] Joshua Achiam and Shankar Sastry. Surprise-based intrinsic motivation for deep reinforcement learning.
arXiv preprint arXiv:1703.01732, 2017.
[56] Zafeirios Fountas, Noor Sajid, Pedro Mediano, and Karl Friston. Deep active inference agents using
Monte-Carlo methods. Advances in neural information processing systems, 33:11662–11675, 2020.
[57] Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and Alexei A. Efros. Large-
Scale Study of Curiosity-Driven Learning. In ICLR, 2019.
[58] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by
self-supervised prediction. In International conference on machine learning, pages 2778–2787. PMLR,
2017.
[59] Danijar Hafner, Pedro A. Ortega, Jimmy Ba, Thomas Parr, Karl Friston, and Nicolas Heess. Action
and Perception as Divergence Minimization, February 2022. URL http://arxiv.org/abs/2009.01791.
arXiv:2009.01791 [cs, math, stat].
[60] Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.
Unifying count-based exploration and intrinsic motivation. Advances in neural information processing
systems, 29, 2016.
17
[61] Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John Schul-
man, Filip DeTurck, and Pieter Abbeel. # exploration: A study of count-based exploration for deep
reinforcement learning. Advances in neural information processing systems, 30, 2017.
[62] ArthurAubret, LaetitiaMatignon, andSalimaHassas. AnInformation-TheoreticPerspectiveonIntrinsic
Motivation in Reinforcement Learning: A Survey. Entropy, 25(2):327, February 2023. ISSN 1099-4300.
doi: 10.3390/e25020327. URL https://www.mdpi.com/1099-4300/25/2/327. Number: 2 Publisher:
Multidisciplinary Digital Publishing Institute.
[63] Juergen Schmidhuber. Driven by Compression Progress: A Simple Principle Explains Essential Aspects
of Subjective Beauty, Novelty, Surprise, Interestingness, Attention, Curiosity, Creativity, Art, Science,
Music, Jokes, April 2009. URL http://arxiv.org/abs/0812.4360. arXiv:0812.4360 [cs].
[64] Tobias Jung, Daniel Polani, and Peter Stone. Empowerment for continuous agent—environment systems.
Adaptive Behavior, 19(1):16–39, 2011. Publisher: SAGE Publications Sage UK: London, England.
[65] Susanne Still and Doina Precup. An information-theoretic approach to curiosity-driven reinforcement
learning. Theory in Biosciences, 131(3):139–148, 2012. Publisher: Springer.
[66] Shakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation for intrinsically
motivated reinforcement learning. Advances in neural information processing systems, 28, 2015.
[67] Karl Friston, James Kilner, and Lee Harrison. A free energy principle for the brain. Journal of
Physiology-Paris, 100(1-3):70–87, July 2006. ISSN 09284257. doi: 10.1016/j.jphysparis.2006.10.001. URL
https://linkinghub.elsevier.com/retrieve/pii/S092842570600060X.
[68] Christopher L. Buckley, Chang Sub Kim, Simon McGregor, and Anil K. Seth. The free energy principle
for action and perception: A mathematical review. Journal of Mathematical Psychology, 81:55–79,
December 2017. ISSN 0022-2496. doi: 10.1016/j.jmp.2017.09.004. URL https://www.sciencedirect.
com/science/article/pii/S0022249617300962.
[69] Claude Elwood Shannon. A mathematical theory of communication. The Bell system technical journal,
27(3):379–423, 1948. URL https://ieeexplore.ieee.org/abstract/document/6773024/. Publisher:
Nokia Bell Labs.
[70] EmanuelTodorov. Linearly-solvableMarkovdecisionproblems. Advancesinneuralinformationprocessing
systems, 19, 2006.
[71] Jonathan Rubin, Ohad Shamir, and Naftali Tishby. Trading value and information in MDPs. In Decision
Making with Imperfect Decision Makers, pages 57–74. Springer, 2012.
[72] Felix Leibfried, Sergio Pascual-Díaz, and Jordi Grau-Moya. A Unified Bellman Optimality Principle
Combining Reward Maximization and Empowerment. In Advances in Neural Information Processing
Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_
files/paper/2019/hash/13384ffc9d8bdb21c53c6f72d46f7866-Abstract.html.
[73] Jürgen Schmidhuber. Curious model-building control systems. In Proc. international joint conference on
neural networks, pages 1458–1463, 1991.
[74] Andrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike adaptive elements that can
solve difficult learning control problems. IEEE transactions on systems, man, and cybernetics, (5):
834–846, 1983. Publisher: IEEE.
[75] Razvan V Florian. Correct equations for the dynamics of the cart-pole system. Center for Cognitive and
Neural Studies (Coneural), Romania, 2007. Publisher: Citeseer.
[76] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
18
[77] R. Blahut. Computation of channel capacity and rate-distortion functions. IEEE Transactions on
Information Theory, 18(4):460–473, July 1972. ISSN 1557-9654. doi: 10.1109/TIT.1972.1054855.
Conference Name: IEEE Transactions on Information Theory.
[78] Alexander S. Klyubin, Daniel Polani, and Chrystopher L. Nehaniv. Keep Your Options Open: An
Information-Based Driving Principle for Sensorimotor Systems. PLOS ONE, 3(12):e4018, December 2008.
ISSN 1932-6203. doi: 10.1371/journal.pone.0004018. URL https://journals.plos.org/plosone/
article?id=10.1371/journal.pone.0004018. Publisher: Public Library of Science.
[79] Lancelot Da Costa, Noor Sajid, Thomas Parr, Karl Friston, and Ryan Smith. Reward Maximization
Through Discrete Active Inference. Neural Computation, 35(5):807–852, April 2023. ISSN 0899-7667.
doi: 10.1162/neco_a_01574. URL https://doi.org/10.1162/neco_a_01574.
[80] Alexander Tschantz, Beren Millidge, Anil K. Seth, and Christopher L. Buckley. Reinforcement learning
through active inference. arXiv preprint arXiv:2002.12636, 2020.
[81] Mark Towers, Jordan K. Terry, Ariel Kwiatkowski, John U. Balis, Gianluca de Cola, Tristan Deleu,
Manuel Goulão, Andreas Kallinteris, Arjun KG, Markus Krimmel, Rodrigo Perez-Vicente, Andrea Pierré,
Sander Schulhoff, Jun Jet Tai, Andrew Tan Jin Shen, and Omar G. Younis. Gymnasium, March 2023.
URL https://zenodo.org/record/8127025.
[82] YingjieFei,ZhuoranYang,YudongChen,ZhaoranWang,andQiaominXie. Risk-SensitiveReinforcement
Learning: Near-Optimal Risk-Sample Tradeoff in Regret. In Advances in Neural Information Processing
Systems, volume 33, pages 22384–22395. Curran Associates, Inc., 2020. URL https://proceedings.
neurips.cc/paper/2020/hash/fdc42b6b0ee16a2f866281508ef56730-Abstract.html.
[83] Alexander D Wissner-Gross and Cameron E Freer. Causal entropic forces. Physical review letters, 110
(16):168702, 2013. Publisher: APS.
[84] Nicola Catenacci Volpi and Daniel Polani. Goal-Directed Empowerment: Combining Intrinsic Motivation
and Task-Oriented Behaviour. IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL
SYSTEMS.
[85] Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric Xing, Sergey Levine, and Ruslan Salakhutdinov.
Efficient exploration via state marginal matching. arXiv preprint arXiv:1906.05274, 2019.
[86] Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu. Reward-free exploration for
reinforcement learning. In International Conference on Machine Learning, pages 4870–4879. PMLR,
2020.
[87] Mirco Mutti and Marcello Restelli. An intrinsically-motivated approach for learning highly exploring and
fast mixing policies. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages
5232–5239, 2020. Issue: 04.
[88] Benjamin Eysenbach and Sergey Levine. Maximum entropy rl (provably) solves some robust rl problems.
arXiv preprint arXiv:2103.06257, 2021.
[89] Karol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational Intrinsic Control, November
2016. URL http://arxiv.org/abs/1611.07507. arXiv:1611.07507 [cs].
[90] Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-Aware
Unsupervised Skill Discovery. International Conference on Learning Representations, April 2020. MAG
ID: 2995736683 S2ID: ae3b2768b0a3c73410bce0d2ae03feaf01f6f864.
[91] Seohong Park, Kimin Lee, Youngwoon Lee, and Pieter Abbeel. Controllability-Aware Unsupervised Skill
Discovery, February 2023. URL http://arxiv.org/abs/2302.05103. arXiv:2302.05103 [cs].
[92] Richard S. Sutton, Doina Precup, and Satinder Singh. Between MDPs and semi-MDPs: a
framework for temporal abstraction in reinforcement learning. Artificial Intelligence, 112(1):
181–211, August 1999. doi: 10.1016/s0004-3702(99)00052-1. MAG ID: 2109910161 S2ID:
0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d.
19
[93] Jordi Grau-Moya, Felix Leibfried, Tim Genewein, and Daniel A. Braun. Planning with Information-
Processing Constraints and Model Uncertainty in Markov Decision Processes, April 2016. URL http:
//arxiv.org/abs/1604.02080. arXiv:1604.02080 [cs].
[94] James J. Gibson. The Ecological Approach to Visual Perception: Classic Edition. Psychology Press,
November 2014. ISBN 978-1-317-57937-3. Google-Books-ID: QReLBQAAQBAJ.
[95] Khimya Khetarpal, Zafarali Ahmed, Gheorghe Comanici, David Abel, and Doina Precup. What can i do
here? atheoryofaffordancesinreinforcementlearning. InInternational Conference on Machine Learning,
pages 5243–5253. PMLR, 2020. URL https://proceedings.mlr.press/v119/khetarpal20a.html.
[96] Jan Peters, Katharina Mulling, and Yasemin Altun. Relative entropy policy search. In Proceedings
of the AAAI Conference on Artificial Intelligence, volume 24, pages 1607–1612, 2010. URL https:
//ojs.aaai.org/index.php/AAAI/article/view/7727. Issue: 1.
[97] Alireza Modirshanechi, Wei-Hsiang Lin, He A. Xu, Michael H. Herzog, and Wulfram Gerstner. The curse
of optimism: a persistent distraction by novelty, June 2023. URL https://www.biorxiv.org/content/
10.1101/2022.07.05.498835v2. Pages: 2022.07.05.498835 Section: New Results.
[98] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft Actor-Critic Algorithms
and Applications, January 2019. URL http://arxiv.org/abs/1812.05905. arXiv:1812.05905 [cs, stat].
[99] Roy Fox, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via soft updates.
arXiv preprint arXiv:1512.08562, 2015.
20
A Appendix
A.1 Entropy measures the occupancy of action-state paths
In this section, we show that entropy is the only measure of action-state path occupancy that obeys some
basic intuitive notions of occupancy. We first list the intuitive conditions in mathematical form, present the
main theorem and then discuss some implications through some corollaries.
We consider a time-homogeneous Markov decision process with finite state set S and finite action set A(s)
for every state s∈S. Henceforth, the action-state x =(a ,s ) is any joint pair of one available action a
j j j j
and one possible successor state s that results from making that action under policy π ≡{π(a|s)} from the
j
action-state x =(a ,s ). By assumption, the availability of action a depends on the previous state s alone,
i i i j i
not on a . Thus, the transition probability from x to x in one time step is p =π(a |s )p(s |s ,a ), where
i i j ij j i j i j
p(s |s ,a ) is the conditional probability of transitioning from state s to s given that action a is performed.
j i i i j j
Although there is no dependence of the previous action a on this transition probability, it is notationally
i
convenient to define transitions between action-states. We conceive of rational agents as maximizing future
action-state path occupancy. Any measure of occupancy should obey the intuitive Conditions 1-4 listed below.
Intuitive Conditions for a measure of action-state occupancy:
1. Occupancy gain of action-state x from x is a function of the transition probability p , C(p )
j i ij ij
2. Performingalowprobabilitytransitionleadstoahigheroccupancygainthanperformingahighprobability
transition, that is, C(p ) decreases with p
ij ij
3. The first order derivative C′(p ) is continuous for p ∈(0,1)
ij ij
4. (Definition: the action-state occupancy of a one-step path from action-state x is the expectation over
i
occupancy gains of the immediate successor action-states, C(1) ≡ (cid:80) p C(p ))
i j ij ij
The action-state occupancy of a two-steps path is additive,
C(2) ≡ (cid:80) p p C(p p )=C(1)+ (cid:80) p C(1)
i jk ij jk ij jk i j ij j
for any choice of the p and initial x
ij i
Condition 1 simply states that occupancy gain from an initial action-state is defined over the transition
probabilitiestosuccessoraction-statesinasamplespace. Condition2impliesthatperformingalowprobability
transition leads to a higher occupancy of the successor states than performing a high probability transition.
Thisisbecauseperformingararetransitionallowstheagenttooccupyaspacethatwasleftinitiallyunoccupied.
Condition 3 imposes smoothness of the measure.
InCondition4wehavedefinedtheoccupancyofthesuccessoraction-states(one-steppaths)intheMarkov
chain as the expected occupancy gain. Condition 4 is the central property, and it imposes that the occupancy
of action-states paths with two steps can be broken down into a sum of the occupancies of action-states at
each time step. Note that the action-state path occupancy can be written as
C(2) ≡ (cid:88) p p C(p p )= (cid:88) p C(p )+ (cid:88) p p C(p )= (cid:88) p p (C(p )+C(p )),
i ij jk ij jk ij ij ij jk jk ij jk ij jk
jk j jk jk
(cid:80)
which imposes a strong condition on the function C(p). Note also that the sum p p C(p p ) extends
jk ij jk ij jk
the notion of action-state to a path of two consecutive action-states, each path having probability p p due
ij jk
to the (time-homogeneous) Markov property. The last equality is an identity. While here we consider paths of
length equal to 2, further below we show that there is no difference in imposing additivity to paths of any
fixed or random length (Corollary 2).
Theorem 1. C(p)=−klnp with k >0 is the only function that satisfies Conditions 1-4
Corollary 1. The entropy C(1) =−k (cid:80) p lnp is the only measure of action-state occupancy of successor
i j ij ij
action-states x from x with transition probabilities p consistent with Conditions 1-4.
j i ij
21
Proof. Put p = 1 and p = 0 for j ̸= 1. Then, Condition 4 reads C(1) = C(1)+C(1) when the initial
1,1 1,j
action-state is x , which implies C(1)=0.
1
Now, take a Markov chain with p = 1, p = 1−t > 0, p = t > 0, p = p = 0, p = 1/n for
0,0 1,0 1,2 2,0 2,1 2,j
j =3,...,n+2andn>0,andp =1fork =3,...,n+2. Inthischain,thestate0isabsorbingandallothers
k,0
are transient (here action-states are simply referred to as states). Starting from state 1, transition to the
transient state 2 happens with probability t and to the absorbing state 0 with probability 1−t. From state 2
a transition to states j =3,...,n+2 happens with equal probability. From any of those states, a deterministic
transition to 0 ensues. (These last transitions can only happen in the third time step, and although it will be
relevant later on, it is no used in the current proof, which only uses additivity on paths of length two.) Then,
Condition4withinitialstate1readstC(t/n)+(1−t)C(1−t)=tC(t)+(1−t)C(1−t)+tC(1/n)+(1−t)C(1),
and hence C(t/n)=C(t)+C(1/n) for any 0<t<1 and integer n>0. By Condition 3 and taking derivative
with respect to t in both sides, we obtain C′(t/n) = nC′(t), and multiplying in both sides by t we obtain
tC′(t)=tC′(t). By replacing t with nt, we get tC′(t)=ntC′(nt), provided that nt<1.
n n
We will now show that tC′(t) is constant. In the last equation replace t by t/m by integer m>0 to get
the last equivalence in tC′(t)= t C′( t )= ntC′(nt) (the first equivalence is obvious). These equivalences
m m m m
are valid for positive t<1 and nt<1. Let 0<s<1 and n=⌊ms/t⌋ be the largest integer smaller than
m
ms/t. Therefore, as m increases nt<1 and approaches s as close as desired. By Condition 3 the function
m
xC′(x) is continuous, and therefore lim ntC′(nt)=sC′(s). The basic idea is that we can first compress
m→∞ m m
t as much as needed by the integer factor m and then expand it by the integer factor n so that nt/m is as
close as desired to s. This shows that sC′(s)=tC′(t) for s,t∈(0,1), and therefore tC′(t) is constant.
Assume that tC′(t)=−k. Then, by integrating we obtain C(t)=−klnt+a, but a=0 due to C(1)=0,
and k >0 due to Condition 2. Together with the above, we can now proof the theorem by noticing that the
solution satisfies Condition 4 for any choice of the p .
ij
Remark: We have found that entropy is the measure of occupancy. The famous derivation of entropy as a
measure of information [69] uses similar elements, but some differences are worthy to be mentioned. First, our
proof uses the notion of additivity of occupancy on MDPs of length two (our Condition 4), while Shannon’s
notion of additivity uses sequences of random variable of arbitrary length (his Condition 3), and therefore his
condition is in a sense stronger than ours. Second, our proof enforces continuous derivative of the measure,
while Shannon enforces continuity of the measure, rendering our Condition 3 stronger. Finally, we enforce a
specific form of the measure as an average over occupancy gains (our Condition 4 again), because it intuitively
captures the notion of occupancy, while Shannon does not enforce this structure in his information measure.
Corollary 2. Condition 4 can be replaced by the stronger condition that requires additivity of paths of any
finite length n with no change in the above proof. We first introduce some notation: the probability of path
i ,i ,...,i is p p ...p , where i refers to the state visited at step t and i is the initial state. Then
0 1 n i0,i1 i1,i2 in−1,in t 0
the new Condition 4 reads in terms of the action-state occupancy of paths of length n as
C(n) = (cid:88) p p ...p C (cid:0) p p ...p (cid:1)
i0 i0,i1 i1,i2 in−1,in i0,i1 i1,i2 in−1,in
i1,i2,...,in
(cid:88) (cid:88) (cid:88) (cid:0) (cid:1)
= p C(p )+ p p C(p )+...+ p p ...p C p
i0,i1 i0,i1 i0,i1 i1,i2 i1,i2 i0,i1 i1,i2 in−1,in in−1,in
i1 i1,i2 i1,i2,...,in
(cid:88) (cid:0) (cid:1)
= p p ...p C(p )+C(p )...+C(p ) ,
i0,i1 i1,i2 in−1,in i0,i1 i1,i2 in−1,in
i1,i2,...,in
for any time-homogeneous Markov chain. By choosing the particular chains used in Theorem 1, we arrive
again to the same unique solution C(p)=−klnp after using C(1)=0 repeated times, which obviously solves
the above equation for any chain and length path. Indeed, note that for the second chain in Theorem 1, from
initial state 1 the absorbing state is reached in three time steps with probability one, and thus the above sum
contains all C(1) starting from the third terms, which contribute zero to the sum.
The above entropy measure of action-state path occupancy can be extended to the case where there is a
discount factor 0 < γ < 1. To do so, we assume now that the paths can have a random length n ≥ 1 that
follows a geometric distribution, p =γn−1(1−γ). In this case, the occupancy of the paths is
n
22
(cid:88) (cid:88)
C = (1−γ) p C(p )+γ(1−γ) p p C(p p )
global i0,i1 i0,i1 i0,i1 i1,i2 i0,i1 i1,i2
i1 i1,i2
(cid:88)
+γ2(1−γ) p p p C(p p p )+... (A.1)
i0,i1 i1,i2 i2,i3 i0,i1 i1,i2 i2,i3
i1,i2,i3
wherethen-thterminthesumistheexpectedoccupancygainofpathsoflengthnweightedbytheprobability
of a having a path with exactly such a length.
Equivalently, a path in course can grow one step further with probability γ or be extinguished with
probability 1−γ. Therefore, the occupancy in Eq. (A.1) should also be equal to the sum of the expected
occupancy gains of the local states along the paths, defined as
(cid:88) (cid:88) (cid:88)
C = p C(p )+γ p p C(p )+γ2 p p p C(p )+... (A.2)
local i0,i1 i0,i1 i0,i1 i1,i2 i1,i2 i0,i1 i1,i2 i2,i3 i2,i3
i1 i1,i2 i1,i2,i3
where the first term is the expected occupancy gain given by the initial condition, the second term is the
expected occupancy gain in the next step weighted by the probability of having a path length of at least two
steps, and so on.
Eqs. (A.1-A.2), after using the Markov chain in Corollary 2, reduce to
(cid:88) (cid:88)
C = (1−γ) p C(p )+γ(1−γ) p p C(p p )
global i0,i1 i0,i1 i0,i1 i1,i2 i0,i1 i1,i2
i1 i1,i2
(cid:88)
+γ2(1−γ) p p C(p p )+...
i0,i1 i1,i2 i0,i1 i1,i2
i1,i2
(cid:88) (cid:88)
= (1−γ) p C(p )+γ p p C(p p )
i0,i1 i0,i1 i0,i1 i1,i2 i0,i1 i1,i2
i1 i1,i2
and
(cid:88) (cid:88)
C = p C(p )+γ p p C(p ),
local i0,i1 i0,i1 i0,i1 i1,i2 i1,i2
i1 i1,i2
where we have used p =1 because all transitions in the third step are deterministic.
i2,i3
(cid:80)
Equality of these two quantities leads to Condition 4, specifically, p p C(p p ) =
(cid:80) (cid:80)
i1,i2 i0,i1 i1,i2 i0,i1 i1,i2
p C(p )+ p p C(p ). Therefore, the only consistent measure of occupancy with
i1 i0,i1 i0,i1 i1,i2 i0,i1 i1,i2 i1,i2
temporal discount is the entropy. Obviously, the equality of global and local time-discounted occupancies
measured by entropy holds for any time-homogeneous or inhomogeneous Markov chain.
A.2 Critical policies and critical state-value functions
Here, the expected return following policy π in Eq. (A.2), known as the state-value function, is written
recursively using the Bellman equation. Then, we find a non-linear system of equations for the critical policy
and critical state-value function by taking partial derivatives with respect to the policy probabilities (Theorem
2).
Using Eq. (A.2) and Theorem 1 with k =1, we define the expected return from state s under policy π as
(cid:88) (cid:88) (cid:88)
V (s)=− p lnp −γ p p lnp −γ2 p p p lnp +... (A.3)
π s,i1 s,i1 s,i1 i1,i2 i1,i2 s,i1 i1,i2 i2,i3 i2,i3
i1 i1,i2 i1,i2,i3
where p is the transition probability from state s to action-state x =(a ,s ). Note that in Eq. (A.2) we
s,i1 i1 i1 i1
have replaced the initial action-state i by the initial state s alone, as the previous action that led to it does
0
no affect the transition probabilities in the Markov decision process setting. The expected returns satisfy the
standard recurrence relationship [14]
23
V (s) = (cid:88) p (cid:0) −lnp +γV (s′) (cid:1)
π s,(a,s′) s,(a,s′) π
a,s′
(cid:88)
= π(a|s)p(s′|s,a)(−lnπ(a|s)p(s′|s,a)+γV (s′)). (A.4)
π
a,s′
Here, we have unpacked the sum over the action-state i into a sum over (a,s′), where a is the action made
1
in state s and s′ is its successor. The second equation shows, in a more standard notation, the explicit
dependence of the expected return on the policy. It also highlights that the intrinsic immediate reward takes
the form R (s,a,s′)=−lnπ(a|s)p(s′|s,a), which is unbounded.
intrinsic
From Eq. (A.3) it is easy to see that the expected return exists (is finite) for any policy π if the Markov
decision process has a finite number of actions and states. Due to the properties of entropy, Eq. (A.3) is a
sum of non-negative numbers bounded by H =ln(|A| |S|) (|A| is the maximum number of available
max max max
actions from any state) weighted by the geometric series, which guarantees convergence of the infinite sum for
−1<γ <1. An obvious, but relevant, implication of the above is that the expected return is non-negative
and bounded, 0≤V (s)≤H /(1−γ), for any state and policy.
π max
While in Eq. (A.4) the immediate intrinsic reward is the sum of the action and state occupancies,
R (s,a,s′)=−lnπ(a|s)p(s′|s,a)=−lnπ(a|s)−lnp(s′|s,a), we can generalize this reward to consider
intrinsic
any weighted mixture of entropies as R (s,a,s′) = −αlnπ(a|s)−βlnp(s′|s,a) for any two numbers
intrinsic
α > 0 and β ≥ 0. In particular, for (α,β) = (1,1) we recover the action-state occupancy of Eq. (A.4),
and for (α,β) = (1,0) and (α,β) = (0,1) we only consider action or state occupancy, respectively. The
case (α,β)=(0,1) is understood as the limit case where α becomes infinitely small. We note that the case
(α,β)=(1,0) has often been used along with an external reward with the aim of regularizing the external
reward objective [37, 38, 40, 41, 43]. We also note that the case (α,β)=(1,−1), with negative β, constitutes
an approximation to empowerment [20, 64]: the agent tries to maximize action entropy while minimizing
state entropy conditioned to the previous action-state, which favors paths where there is more control on the
resulting states. However, we do not consider this case in this paper.
Under the more general intrinsic reward, the expected return obeys
V (s)= (cid:88) π(a|s)p(s′|s,a) (cid:0) −lnπα(a|s)pβ(s′|s,a)+γV (s′) (cid:1) . (A.5)
π π
a,s′
Our goal is to maximize the expected return over the policy probabilities π ={π(a|s):a∈A(s),s∈S} to
obtain the optimal policy. Note that for α>0 and β ≥0 the expected return is non-negative, V (s)≥0.
π
Theorem 2. The critical values Vc(s) of the expected returns V (s) in equation (A.5) with respect to the
π
policy probabilities π ={π(a|s):a∈A(s),s∈S} obey
 (cid:32) (cid:33)
(cid:88) (cid:88)
Vc(s)=αlnZ(s)=αln exp α−1βH(S′|s,a)+α−1γ p(s′|s,a)Vc(s′)  (A.6)
a∈A(s) s′
where H(S′|s,a)=− (cid:80) p(s′|s,a)lnp(s′|s,a) is the entropy of the successors of s after performing action a,
s′
and Z(s) is the partition function.
The critical points (critical policies) are
(cid:32) (cid:33)
1 (cid:88)
πc(a|s)= exp α−1βH(S′|s,a)+α−1γ p(s′|s,a)Vc(s′) , (A.7)
Z(s)
s′
one per critical value, where the partition function Z(s) is the normalization constant.
Defining z = exp (cid:0) α−1γVc(s ) (cid:1) , p = p(s |s ,a ) and H = α−1βH(S′|s ,a ), Eq. (A.6) can be
i i ijk j i k ik i k
compactly rewritten as
z
i
γ−1 = (cid:88) w
ik
eHik (cid:89) z
j
pijk (A.8)
k j
24
where the matrix with coefficients w ∈{0,1} indicates whether action a is available at state s (w =1)
ik k i ik
or not (w = 0), and j extends over all states, with the understanding that if a state s is not a possible
ik j
successor from state s and action a then p =0.
i k ijk
Note that the we simultaneously optimize |S| expected returns, one per state s, each with respect to the
set of probabilities π ={π(a|s):a∈A(s),s∈S}.
Proof. We first note that the expected return in Eq. (2.2) is continuous and has continuous derivatives with
respect to the policy except at the boundaries (i.e., π(a|s)=0 for some action-state (a,s)). Choosing a state
s, we first take partial derivatives with respect to π(a|s) for each a∈A(s) in both sides of (A.5), and then
evaluate them at a critical point πc to obtain the condition
λ(s,s) = (cid:88) p(s′|s,a) (cid:0) −ln(πc(a|s))αpβ(s′|s,a)+γVc(s′) (cid:1) −α+γ (cid:88) πc(b|s)p(s′|s,b)λ(s′,s)
s′ b,s′
(cid:88)
= −αlnπc(a|s)−β p(s′|s,a)lnp(s′|s,a)−α
s′
(cid:88) (cid:88)
+γ p(s′|s,a)Vc(s′)+γ πc(b|s)p(s′|s,b)λ(s′,s), (A.9)
s′ b,s′
where we have defined the partial derivative at the critical point ∂Vπ(s′)| ≡λ(s′,s) and used the fact that
∂π(a|s) πc
this partial derivative should be action-independent. To understand this, note that the critical policy should
lie in the simplex (cid:80) π(a|s)=1, π(a|s)≥0, and therefore the gradient of V (s′) with respect to the π(a|s)
a π
at the critical policy should be along the normal to the constraint surface, i.e., the diagonal direction (hence,
action-independent), or be zero. Indeed, the action-independence of the λ(s′,s) also results from interpreting
them as Lagrange multipliers: λ(s′,s) is the Lagrange multiplier corresponding to the state-value function at
s′, V (s′), associated to the constraint (cid:80) π(a|s)=1, π(a|s)≥0, defining the simplex where the probabilities
π a
{π(a|s):a∈A(s)} lie.
Noticing that the last term of Eq. (A.9) does not depend on a, we can solve for the critical policy πc(a|s)
to obtain equation (A.7). Eq. (A.7) implicitly relates the critical policy with the critical value of the expected
returns from each state s. Inserting the critical policy (A.7) into Eq. (A.5), we get (A.6), which is an implicit
non-linear system of equations exclusively depending on the critical values.
It is easy to verify that the partial derivatives of V (s) in Eq. (A.5) with respect to π(a′|s′) for s̸=s′ are
π
(cid:88)
λ(s,s′)=γ p(s′′|s)λ(s′′,s′),
s′′
and thus they provide no additional constraint on the critical policy. 1
We finally show that the optimal expected returns, as defined from the Bellman optimality equation
V∗(s)= max (cid:88) π(a|s)p(s′|s,a) (cid:0) −lnπα(a|s)pβ(s′|s,a)+γV∗(s′) (cid:1) , (A.10)
π(·|s)
a,s′
obey the same Eq. (A.6) as the critical values of Eq. (A.5) do. To see this, note that after taking partial
derivatives with respect to π(a|s) for each a∈A(s) on the right-hand side of Eq. (A.10) we get
(cid:88) (cid:88)
0=−αlnπ(a|s)−β p(s′|s,a)lnp(s′|s,a)+γ p(s′|s,a)V∗(s′)−α+λ(s), (A.11)
s′ s′
1ThissetofequationsalongwithEq. (A.9)generatesalinearsystemofS2 equationsfortheS2 unknownsλ(s,s′). Inthe
next section we show that the critical values Vc(s) and critical policy πc(a|s) exists and are unique, and thus the system of
equationsforλ(s,s′)isofthetypeΛ=γP⊺Λ+F,withuniquematricesΛ
ss′
=λ(s,s′),P
s′s
=p(s′|s)≡(cid:80)
a
πc(a|s)p(s′|s,a)
andF s′s isadiagonalmatrixwithFss=Vc(s)−α. BecauseP isastochasticmatrix,itdoesnothaveeigenvalueslargerthan
one. Therefore the matrix I−γP⊺ with γ <1 does not have zero eigenvalues, and thus it is invertible. The solution to the
systemisthenuniqueandgiventhenbyΛ=(I−γP⊺)−1F.
25
(cid:80)
where λ(s) is the Lagrange multiplier associated to the constraint π(a|s)=1. This equation, except for
a
the irrelevant action-independent Lagrange multipliers, is identical to Eq. (A.9). Eq. (A.6) follows from
inserting the resulting optimal policy into the Bellman optimality equation.
A.3 Unicity of the optimal value and policy, and convergence of the algorithm
We now prove that the critical value Vc(s) is unique, in other words, equation (A.6) admits a single solution
(Theorem 3). We later prove that the solution is the optimal expected return (Theorem 4).
Theorem 3. With the definitions in Theorem 2, the system of equations
z
i
γ−1 = (cid:88) w
ik
eHik (cid:89) z
j
pijk (A.12)
k j
with 0<γ <1, α>0 and β ≥0 has a unique solution in the positive first orthant z >0, provided that for
i
all i there exists at least one k such that w =1. The solution satisfies z ≥1.
ik i
Moreover, given any initial condition z(0) >0 for all i, the infinite series z(n) defined through the iterative
i i
map
 γ
z i (n+1) = (cid:88) w ik eHik (cid:89)(cid:16) z j (n) (cid:17)pijk  (A.13)
k j
for n≥0 converges to a finite limit z∞ ≥1, and this limit is the unique solution of equation (A.12)
i
Note that the condition that for all i there exists at least one k such that w =1 imposes virtually no
ik
restriction, as it only asks for the presence of at least one available action in each state. For instance, in
absorbing states, the action leads to the same state.
Importantly, proving that the map (A.13) has a single limit regardless of the initial condition in the
positive first orthant z(0) >0 suffices to prove that equation (A.12) has a unique solution in that region, as
i
then no other fix point of the map can exist. Additionally, since the solution is unique and satisfies z∞ ≥1,
i
the critical state-value function that solves equation (A.6) is unique, and Vc(s )=αγ−1lnz∞ ≥0, consistent
i i
with its properties.
The map (A.13) provides a useful value-iteration algorithm used in examples shown in the Results section,
and empirically is found to rapidly converge to the solution.
Proof. We call the series z(n) with initial condition z(0) =1 for all i the main series. We first show that the
i i
main series is monotonic non-decreasing.
For n=1, we get
 γ
z
i
(1) = (cid:88) w
ik
eHik (cid:89) (1)pijk ≥1=z
i
(0) (A.14)
k j
for all i, using that there exists k for which, w = 1, w is non-negative for all i and k, H ≥ 0 and the
ik ik ik
power function xγ is increasing with its argument.
Assume that for some n>0, z(n) ≥z(n−1) for all i. Then
i i
 γ  γ
z i (n+1) = (cid:88) w ik eHik (cid:89)(cid:16) z j (n) (cid:17)pijk  ≥ (cid:88) w ik eHik (cid:89)(cid:16) z j (n−1) (cid:17)pijk  =z i (n) (A.15)
k j k j
using the same properties as before, which proves the assertion for all n by induction.
Now let us show that the main series is bounded. Define H =max H , and obviously H ≥0.
max ik ik max
For n=1 we have
(cid:32) (cid:33)γ
z
i
(1) = (cid:88) w
ik
eHik ≤ (cid:0) |A|
max
eHmax (cid:1)γ ≡cγ (A.16)
k
26
(remember that |A| is the maximum number of available actions from any state).
max
For n=2,
 γ  γ
z i (2) =  (cid:88) w ik eHik (cid:89)(cid:16) z j (1) (cid:17)pijk  ≤ (cid:88) w ik eHik (cid:89) cγpijk
k j k j
(cid:32) (cid:33)γ (cid:32) (cid:33)γ
= (cid:88) w
ik
eHikcγ =cγ2 (cid:88) w
ik
eHik ≤cγ+γ2
k k
(cid:80)
using the standard properties, p =1 and Eq. (A.16).
j ijk
Assume that for some n>1 we have z(n) ≤cγ+γ2+...+γn. We have just showed that this is true for n=2.
i
Then
 γ (cid:32) (cid:33)γ
z i (n+1) =  (cid:88) w ik eHik (cid:89)(cid:16) z j (n) (cid:17)pijk  ≤ (cid:88) w ik eHikcγ+...+γn
k j k
(cid:32) (cid:33)γ
= cγ2+...+γn+1 (cid:88) w
ik
eHik ≤cγ+...+γn+1
k
and therefore it is true for all n≥0 by induction.
Therefore the series z(n) is bounded by c1/(1−γ). Together with the monotonicity of the series, we have
i
now proved that the limit z∞ of the series exists. Moreover, z∞ ≥z0 =1.
i i i
Theaboveresultscanbeintuitivelyunderstood: the‘allones’initialconditionofthemainseriescorresponds
to an initial guess of the state-value function equal to zero everywhere. The iterative map corresponds to
state-value iteration to a more optimistic value: as intrinsic reward based on entropy is always non-negative,
the z-values monotonically increase after every iteration. Finally, the z-values reach a limit because the
state-value function is bounded.
We now show the central result that the series obtained by using the iterative map starting from any
initial condition in the positive first orthant can be bounded below and above by two series that converge to
the main series. Therefore, by building ‘sandwich’ series we will confirm that any other series has the same
limit as the main series.
Let the y(0) = u > 0 be the initial condition of the series y(n) obeying the iterative map (A.13), and
i i i
define u =min u and u =max u . Obviously, u >0 and u >0. Applying the iterative map
min i i max i i min max
once, we get
 γ  γ
y i (1) =  (cid:88) w ik eHik (cid:89)(cid:16) y j (0) (cid:17)pijk  ≤ (cid:88) w ik eHik (cid:89) (u max )pijk
k j k j
(cid:32) (cid:33)γ (cid:32) (cid:33)γ
= (cid:88) w
ik
eHiku
max
=uγ
max
(cid:88) w
ik
eHik =uγ
max
z
i
(1)
k k
where in the last step we have used the values of the main series in the first iteration. We can similarly
lower-bound y(1) to finally show that it is both lower- and upper-bounded by z(1) with different multiplicative
i i
constants,
uγ z(1) ≤y(1) ≤uγ z(1) (A.17)
min i i max i
Now, assume that
uγn z(n) ≤y(n) ≤uγn z(n) (A.18)
min i i max i
27
is true for some n>0. Then, for n+1 we get
 γ  γ
y i (n+1) =  (cid:88) w ik eHik (cid:89)(cid:16) y j (n) (cid:17)pijk  ≤ (cid:88) w ik eHik (cid:89)(cid:16) uγ m n ax z i (n) (cid:17)pijk 
k j k j
 γ
= uγ m n a + x 1  (cid:88) w ik eHik (cid:89)(cid:16) z i (n) (cid:17)pijk  =uγ m n a + x 1 z i (n+1)
k j
(cid:80)
by simply extracting the common factor in the fourth expression, remembering that p =1, and using
j ijk
the definition of the main series in the last one. By repeating the same with the lower bound, we finally find
that (A.18) holds also for n+1, and then, by induction, for every n>0.
The proof concludes by noticing that the limit of both uγn and uγn is 1, and therefore using (A.18) the
max min
limit y∞ of the series y(n) equals the limit of the main series, y∞ =z∞.
i i i i
Note that the iterative map (A.13) is not necessarily contractive in the Euclidean metric, as it is possible
that, depending on the values of u and u and the changes in the main series, the bounds in Eq. (A.18)
min max
initially diverge to finally converge in the limit.
Theorem 4. The (unique) critical value Vc(s) is the optimal expected return, that is, the one that attains the
maximum expected return at every state for any policy, and we write Vc(s)=V∗(s)
Proof. To show that Vc(s) is the optimal expected return, we note that the maximum of the functions V (s)
π
with respect to policy π should be at the critical policy or at the boundaries of the simplices defined by
(cid:80)
π(a|s) = 1 with 0 ≤ π(a|s) ≤ 1 for every a and s, as the expected return V (s) is continuous and has
a π
continuous derivatives with respect to the policy except at the boundaries. At the policy boundary, there
exists a non-empty subset of states s and a non-empty set of actions a for which π(a |s )=0. Computing
i k k i
the critical value of the expected return along that policy boundary is identical to moving from the original to
a new problem where we replace the graph connectivity matrix w in Eq. (A.12) by a new one v such that
ik ik
v ≤w (remember that at the boundary there should be an action a that were initially available from
ik ik k
state s , w = 1, that at the policy boundary is forbidden, v = 0). We now define the convergent series
i ik ik
z(n) and y(n) for the original and new problems respectively by using the iterative map (A.13) with initial
i i
conditions equal to all ones. We prove now that z(n) ≥y(n) for all i for n=1,2,..., and thus their limits obey
i i
z∞ ≥y∞.
i i
For n=1, we get
 γ  γ
z
i
(1) = (cid:88) w
ik
eHik (cid:89) (1)pijk ≥ (cid:88) v
ik
eHik (cid:89) (1)pijk =y
i
(1) (A.19)
k j k j
for all i, using that w ≥v and that the power function xγ is increasing with its argument.
ik ik
Assuming that z(n) ≥y(n) for all i for some n>0, then
i i
 γ  γ
z i (n+1) = (cid:88) w ik eHik (cid:89)(cid:16) z j (n) (cid:17)pijk  ≥ (cid:88) v ik eHik (cid:89)(cid:16) y j (n) (cid:17)pijk  =y i (n+1) (A.20)
k j k j
using the same properties as before, which proves the assertion for all n by induction.
Remembering that the expected return V(s ) is increasing with z , we conclude that the expected return
i i
obtained from policies restricted on the boundaries of the simplices is no better than the original critical value
of the expected return.
28
A.4 Particular examples
Here we summarize the main results and specialize them to specific cases. We assume 0 < γ < 1, α > 0
and β ≥ 0 and use the notation z = exp (cid:0) α−1γV∗(s ) (cid:1) , where V∗(s) is the optimal expected return,
i i
p =p(s |s ,a ) and H =α−1βH(S′|s ,a ), where H(S′|s,a)=− (cid:80) p(s′|s,a)lnp(s′|s,a).
ijk j i k ik i k s′
A.4.1 Action-state entropy maximizers
Agents that seek to maximize the discounted action-state path entropy follow the optimal policy
 
π∗(a
k
|s
i
)=
Z
1 w
ik
eHik (cid:89) z
j
pijk (A.21)
i
j
with
Z
i
= (cid:88) w
ik
eHik (cid:89) z
j
p ijk′ (A.22)
k j
The matrix with coefficients w ∈{0,1} indicate whether action a is available at state s (w =1) or not
ik k i ik
(w =0).
ik
The expected return (state-value function) in terms of the z variables obeys
z
i
γ−1 = (cid:88) w
ik
eHik (cid:89) z
j
pijk (A.23)
k j
A.4.2 Action-only entropy maximizers
Agents that ought to maximize the time-discounted action path entropy correspond to the above case with
β =0, and therefore the optimal policy reads as
 
π∗(a
k
|s
i
)=
Z
1 w
ik
(cid:89) z
j
pijk (A.24)
i
j
with
Z =
(cid:88)
w
(cid:89)
z
pijk
(A.25)
i ik j
k j
The state-value function in terms of the z variables obeys
zγ−1
=
(cid:88)
w
(cid:89)
z
pijk
(A.26)
i ik j
k j
A.4.3 Entropy maximizers in deterministic environments
In a deterministic environment p =1 for successor state j =j(i,k), and zero otherwise. In this case,
i,j(i,k),k
at every state i we can identify an action k with its successor state j. Therefore, the optimal policy is
w z
π∗(a |s )= ij j (A.27)
k i Z
i
with
(cid:88)
Z = w z (A.28)
i ij j
j
The state-value function in terms of the z variables reads
zγ−1
=
(cid:88)
w z (A.29)
i ij j
j
29
Figure 8: MOP agents determine stochastic policies that maximize occupancy of future action-state paths. In all
panels, the three successive dots indicate that the future looks the same for all the states or actions involved from
that point onwards. (a) At time t, the agent is faced with determining the optimal policy at state s. Given that
taking action a can stochastically lead to two distinct states s′ and s′, the optimal policy gives action a twice the
1 1 2 1
probability weight than to action a (which only induces a deterministic transition to state s′). From time t+1, the
2 3
future looks the same from all three states s′. (b) If the future does not look the same, and actually there are many
i
more actions available at state s′ compared to s′ and s′, then more weight should be given to action a than if the
3 1 2 2
future was the same. (c) If, however, all the actions available at state s′ lead you to an absorbing state, almost zero
3
weight should be given to action a .
2
A.5 Experiments
In this subsection, we present the details for the numerical simulations performed for the different experiments
in the manuscript. First, we discuss the construction of the MOP and R agents, and afterwards we present
the details of each particular experiment.
A.5.1 MOP agent
In all the experiments presented, we introduce the MOP agent, whose name comes from the usual notation
for using H to denote entropy. Therefore, the objective function that this agent maximizes in general is
Eq. (2.2). As described in section A.4, the α and β parameters control the weights of action and next-state
entropies to the objective function, respectively. Unless indicated otherwise, we always use α=1,β =0 for
the experiments. It is important to note, as we have done before, that if the environment is deterministic,
30
then the next-state entropy H(S′|s,a)=− (cid:80) p(s′|s,a)lnp(s′|s,a)=0, and therefore β does not change the
s′
optimal policy, Eq. (2.6).
We have implemented the iterative map, Eq. (2.7), to solve for the optimal value, using z(0) =1 for all i
i
as initial condition. Theorem (3) ensures that this iterative map finds a unique optimal value regardless of
the initial condition in the first orthant. To determine a degree of convergence, we compute the supremum
norm between iterations,
δ =max|V(n+1)−V(n)|,
i i
i
where V = αlog(z ), and the iterative map stops when δ <10−3.
i γ i
A.5.2 R agent
We also introduce a reward-maximizing agent in the usual RL sense. In this case, the reward is r = 1 for
living and r =0 when dying. In other words, this agent maximizes life expectancy. Additionally, to emphasize
the typical reward-seeking behavior and avoid degenerate cases induced by the tasks, we introduced a small
reward for the Four-room grid world (see below). In all other aspects, the modelling of the R agent is identical
to the MOP agent. To allow for reward-maximizing agents to display some stochasticity, we used an ϵ-greedy
policy, the best in the family of ϵ-soft policies [14]. At any given state, a random admissible action is chosen
with probability ϵ, and the action that maximizes the value is chosen with probability 1−ϵ. Given that
the world models p(s′|s,a) are known and the environments are static, this ϵ-greedy policy does not serve
the purpose of exploration (in the sense of learning), but only to inject behavioral variability. Therefore, we
construct an agent with state-independent variability, whose value function satisfies the optimality Bellman
equation for this ϵ-greedy policy,
(cid:88) ϵ (cid:88)
V (s)=(1−ϵ)max p(s′|s,a)(r+γV (s′))+ p(s′|s,a)(r+γV (s′)), (A.30)
ϵ a ϵ |A(s)| ϵ
s′ a,s′
where |A(s)| is the number of admissible actions at state s. To solve for the optimal value in this Bellman
equation, we perform value iteration [14]. The ϵ-greedy policy for the R agent is therefore given by
(cid:40) 1−ϵ+ ϵ , if a=argmax (cid:80) p(s′|s,a′)(r+γV (s′))
π(a|s)= |A(s)| a′ s′ ϵ
ϵ , otherwise
|A(s)|
where ties in argmax are broken randomly. Note that if ϵ=0, we obtain the usual greedy optimal policy that
maximizes reward.
A.5.3 Four-room grid world
A.5.3.1 Environment The arena is composed of four rooms, each having size 5×5 locations where the
agent can be in. From each room, the agent can go to two adjacent rooms through small openings, each
located in the middle of the wall that separates the rooms. At each of these rooms, there is a food source
located in the corner furthest from the openings. See Fig. 2 for a graphic description. Unless indicated
otherwise, the discount factor is set to γ =0.99.
A.5.3.2 States The states are the Cartesian product between (x,y) location and internal state u, which
is simply a scalar value between a minimum of 0 and a maximum capacity of 100. All states such that
(x,y,u=0) are absorbing states, independently of the location (x,y). The particular internal state u=100 is
the maximum capacity for energy, such that even when at a food source, this internal state does not change.
Therefore, the number of states in this experiment is |S|=104external states×101internal states=10504.
A.5.3.3 Actions Theagenthasamaximumof9actions: up, down, left, right, up left, up right,
down left, down right,andnothing. Whenevertheagentisclosetoawall,thenumberofavailableactions
decreases such that the agent cannot choose to go into walls. Finally, whenever the agent is in an absorbing
state, only nothing is available.
31
Figure 9: Survivability for the experiments considered in the manuscript. (a) Survivability of the various agents tested
inthefour-roomgridworld. Ateach5E4timestepepisode,werecordedthesurvivedtimeandaveragedacrossepisodes.
(b) Survivability of the mouse for both MOP and R agents. (c) Survivability for the cartpole (Sec. A.5.5) in the
deterministic arena for the MOP agent and the ϵ-greedy R agents, γ =0.98. (d) Survivability for cartpole (Sec. A.5.5)
in the stochastic arena for the β =0 and the β =1 MOP agents. γ =0.99. (e) Survivability of the cartpole (Sec.
A.5.5) MOP agents as a function of β, for various values of η. γ =0.99
32
A.5.3.4 Transitions At any transition, there is a cost of 1 unit of energy for being alive. On the other
hand,whenevertheagentislocatedatafoodsource,thereisanincreaseinenergythatwevaryparametrically
that we call food gain g. For example, if the agent is in location (2,1) at time t and moves towards (1,1)
(where food is located), the change in energy would be ∆u =−1, given that the change in internal energy
t
depends only on the current state and action. If the agent decides to stay in (1,1) at time t+1, then
∆u =−1+g.
t+1
A.5.3.5 R agent As stated above, in this experiment we introduced an extra reward for the R agent
when it reaches the food source. The magnitude is small compared to the survival reward (1E−5 smaller)
and it mainly serves to break the degeneracy of the value function. The variability of the R agent is thus
coming purely from the ϵ-greedy action selection.
A.5.3.6 Survivability To allow for the maximum uniform variability for the R agent, we tested various
values for ϵ and observed the survivability of the agents as a function of ϵ, across all the food gains tested (see
Results section). The value of ϵ for which the R agent still survives as much as the MOP agent is ϵ=0.45
(see Figure 9a).
A.5.3.7 Noisy room In this variation for the experiment, there is a room (the bottom right room) where
transitions are uniformly random for all actions, across all possible neighboring locations. That is, for any
location s in the noisy room, and any a available at that location, given that it has n(s ) total neighbours
nr nr
(including the same location),
(cid:40)
1 for s′ ∈neighbours
p(s′|s
nr
,a)= n(snr)
0 otherwise
A.5.4 Predator-prey scenario
Here we provide all details of the simulated experiments. Results are shown in Fig. 3.
A.5.4.1 Environment Theenvironmentissimilartothatoneusedforthe4-roomgridworlddescribedin
A.5.3. Apart from the agent (prey), there is also another moving subject (predator) with a simple predefined
policy. The grid world consists of a “home” area, a rectangle 2x3 where the agent may enter, but the predator
cannot. This home area has a small opening that leads to a bigger 4x7 rectangle arena available for both the
agent and the predator. The only food source is located at the bottom-right corner of the common part of the
arena, so that the agent needs to leave its home to boost its energy. Additionally, there is an obstacle which
separates the arena in two parts with two openings, above and under the obstacle. This obstacle allows the
agent to “hide” from the predator behind it.
A.5.4.2 States The location of the predator is part of the agent’s state, such that a particular state
consists of the position of the agent, the position of the predator and the amount of energy of the agent. For
this case, we set the maximum amount of energy F equal to the food gain. Positions are 2-dimensional, and
therefore the states are 5-dimensional. In the used arena there are 33 possible locations for the agent and 26
ones for the predator, so that the total number of states ranges from 11154 for F =13 to 17160 for F =20.
A.5.4.3 Actions The agent has the same actions as in the four-room grid world. The maximum number
of available actions is therefore 9. Moving towards obstacles or walls is not allowed.
A.5.4.4 Transitions The agent loses one unit of energy every time step and increases the amount of
energy up to a given maximum capacity level F only at the food source. If the position of both the agent and
the predator are the same, then the agent is "eaten" and moves to the absorbing state of death as well as in
the case of energy equal to 0. After entering the absorbing state the agent stays there forever.
The predator also moves as the agent (horizontally, vertically, diagonally on one step or to stay still).
Steps of the agent and the predator happen synchronously. The predator is “attracted” to the agent: the
33
probability of moving to some direction is an increasing function on the cosines cosα of the angle α between
k k
this direction of motion k and the direction of the radius vector from the predator to the agent. In particular,
this probability is
pc =C−1exp(κcosα ) (A.31)
k k
(cid:80)
where κ is the inverse temperature of the predator and C = exp(κcosα ) is a normalization factor. These
k k
probabilities are computed only for motions available at the current location of the predator, so that e.g. for
the location at the wall the motions along the wall are taken into account, but not the motion towards the
wall.
A.5.4.5 Goal The goal of the MOP agent is to maximize discounted action entropy, and thus to find
the optimal state-value function using the iterative map in Eq. (2.7) with H = 0 (β = 0). While using
ik
the iterative map, we take advantage of the fact that given an action the physical transition of the agent is
deterministic, but the physical transition of the predator is stochastic. Therefore, the sum over successor
states j in Eq. (2.7) is simply a sum over the predator successor states.
A.5.4.6 Parameters γ = 0.98, F = 15 (if another value between 13 and 20 not mentioned), κ = 2.
Simulation time is 5000 steps.
A.5.4.7 Counting rotations We define a clockwise (counterclockwise) half-rotation as the event when
the agent came from the left part of the arena to the right part over the field above (under) the wall and
from the right part to the left one over the field under (above) the wall without crossing the vertical line of
the wall in between. One full rotation consists of two half-rotations in the same directions performed one
after another. We counted the number of full rotations in both directions in 70 episodes of 500 time steps
each for both MOP and R agents for different values of the food gain F. Error bars were computed based on
these 70 repetitions. The fraction of clockwise rotations to total rotations (sum of clockwise and anticlockwise
rotations) for different values of F is shown at Fig. 3.
A.5.4.8 Survivability The ϵ-greedy R agents display some variability that depends on ϵ. To select this
parameter, we matched average lifetimes (measured in simulations of 5000 steps length) between the MOP
and R agents, separately for every F. Lifetimes are plotted in Figure 9b.
A.5.4.9 Videos We have generated one video for the MOP agent (Video 2) and another for the R agent
(Video 3), both for F = 15, κ = 2, and ϵ = 0.06 for the R agent so as to match their average lifetimes as
described above. In the videos, green vertical bar indicates the amount of energy by the agent at current
time. When the agent makes at least one full rotation around the wall, it is indicated by the written phrase
“clockwise rotation” or “anticlockwise rotation”. Black vertical arrow indicates direction (‘up’ for clockwise
and ‘down’ for anticlockwise directions) of the half-rotation in the part of arena left from the wall.
A.5.5 Cartpole
A.5.5.1 Environment A cart is placed in a one-dimensional track with boundaries at |x|=1.8. It has a
pole attached to it, that rotates like an inverted pendulum with its pivot point on the cart.
A.5.5.2 States The dynamical system can be described by a four-dimensional external state (x,v,θ,ω),
where x is the position of the cart, v is its linear velocity, θ is the angle of the pole with respect to the vertical
which grows counterclockwise, and ω is its angular velocity. In this case, we model the internal state u simply
with the binary variable alive, dead, where the agent enters the absorbing state dead if its position exceeds
the boundaries, or if its angle exceeds 36 degrees. This amplitude of angles is larger than that typically
assumed (12 degrees in [76]), and therefore our system is allowed to be more non-linear and unstable. The
statespaceis[−1.8,1.8]×(−∞,∞)×[−36,36]×(−∞,∞)×{0,1}. TosolveforthestatevaluefunctioninEq.
(2.7), we discretize the state space by setting a maximum value for the velocities. Given all the parameters
(allowed x and θ, magnitude of the forces, masses of cart and pole, length of pole and gravity, below), we
34
Figure 10: Histogram of angles and locations visited for the cartpole, as in Fig. 4 of the main manuscript, for the
MOP agent (left) and ϵ-greedy R agent (right), with ϵ chosen such that MOP and R agents’ lifetimes are similar (see
Fig. 9c).
empirically set the maximum values for |v|=6 and |ω|=3, which the cart actually never exceeds. Therefore,
we computed the state value function in a 31×31×31×31×2 grid (number of states = 1.8×106).
A.5.5.3 Actions Any time the agent is alive, it has 5 possible actions: forces of {−40,−10,0,10,40},
where zero force is understood as nothing. If the agent is dead, then only nothing is allowed.
A.5.5.4 Transitions This dynamical system is a standard task in reinforcement learning, namely the
cartpole-v0 system of the OpenAI gym [76]. The solution of this dynamical system is given in Ref. [75],
where we use a frictionless cartpole. The equations for angular and linear accelerations are thus
(cid:16) (cid:17)
−gsin(θ)+ cos(θ) −F +mθ˙2lsin(θ)
θ¨= M+m (A.32)
(cid:16) (cid:17)
l 4 − mcos2(θ)
3 M+m
(cid:18) (cid:19)
1 4
x¨= lθ¨−gsin(θ) . (A.33)
cos(θ) 3
Given a force F, a deterministic transition can be computed from these dynamical rules, and a real-valued
state transition is observed by the agents.
A.5.5.5 R agent The reward signal is 1 each time the agent is alive and 0 otherwise. To allow for some
variability in the action selection of the R agent, we implement an ϵ-greedy action selection as described above.
For exposition purposes, in the manuscript we set ϵ=0.0, but we also compared to an R agent with ϵ chosen
such that average lifetimes between MOP and R agents are matched (see Fig. 9c and Fig. 10).
A.5.5.6 Parameters Mass of the cart M = 1, mass of the pole m = 0.1, length of the pole l = 1,
acceleration due to gravity g =9.81, time discretization ∆t=0.02. Unless specified differently, the discount
factor was set to γ =0.98.
A.5.5.7 Value interpolation The observed external state is a continuous four-dimensional variable,
so we need to approximate the value function. In order to do so, we simply discretized the state space as
described above, and use value iteration as described in Eq. (2.5) in these grid points by performing a linear
value interpolation for the successor states at each iteration. During a particular episode, the observed states
might not be the same as the ones in the grid, so in order to compute the optimal policy at these states, we
perform the same type of value interpolation as in the value iteration stage.
35
A.5.5.8 Stochastic arena We introduced a slight variation to the environment, where the x>0 half
of the arena is noisy: agents choose an action (force), but the intended state transition of applying such an
action fails with probability η and succeeds with probability 1−η. This is implemented as follows: given
any state-action pair (s,a) for which x>0, there are two possible successor states, one corresponding to the
intended action (force) chosen, and the other one corresponding to a zero force action:

1, if x<0 and s′ ←(s,a)

p(s′|s,a)= 1−η, if x>0 and s′ ←(s,a) (A.34)
η, if x>0 and s′ ←(s,0)
This stochasticity lets us differentiate between action path occupancy maximizers and action-state path
occupancy maximizers by choosing any positive real value of β in Eq. (2.1), because β >0 agents will have a
natural tendency to prefer x>0 locations.
A.5.6 Agent-pet scenario
An agent and a pet move in an arena with degrees of freedom that depend on the actions made by the agent,
as explained next in detail.
A.5.6.1 Environment A 5×5 arena. The middle column of arena can be blocked by a fence, a vertical
obstacle that the pet cannot cross. The agent can cross it freely regardless of whether it is open or closed.
The agent can open or close the fence by performing the corresponding action when visiting the lever location,
at the left bottom corner.
A.5.6.2 States The system’s state consists of the Cartesian product of agent’s location, pet’s location
and binary state of the fence. So, the number of states is 1250. For the sake of simplicity there is no internal
states for the energy, and thus there are not absorbing states. The initial states of the agent and pet at the
start of each episode are the middle of the second column and the right lower corner of the arena, respectively.
A.5.6.3 Actions As in Sec. A.5.4 the agent’s actions are movements to one of the 8 neighbour locations
as well as staying on the current one. Additionally, if the agent is on the “lever” location, an additional action
is available, namely to open or close the fence, depending on its previous state.
A.5.6.4 Transitions The pet has the same available movements as the agent when the fence is open. The
pet performs a random transition to any of the neighbour locations, or stays still, with the same probability.
If the agent closes the fence, then the pet can only move on the side where it lies when closed. For simplicity,
if the fence is closed by the agent when the pet lies in the middle column, then the pet can only move to the
right or left locations such that it will be at one side of the fence in the next time step.
A.5.6.5 Goal The goal of the MOP agent is to maximize discounted action-state entropy using the
iterative map in Eq. (2.7) with α=1 and β ∈[0,1], parameters that measure the weight of action and state
entropies, respectively. As in the prey-predator example, we take advantage of the fact that given an action
the physical transition of the agent is deterministic, while the physical transition of the pet is stochastic.
Thus, the product over successor states j in Eq. (2.7) is a product over the pet successor states.
A.5.6.6 Simulation details We ran simulations for several values of β, from 0 to 1 in 0.1 steps, to
interpolate between pure action entropy (β =0) and action-state entropy (β =1). We measured the fraction
of time the gate was open using episodes of 2000 steps averaged over 70 simulations for each β, shown in Fig
5. Heat-maps in that figure correspond to the occupation probability by the pet for β =0 (left panel) and
β =1 (right panel) using an episode of 5000 steps.
36
Figure11: ComparisonbetweentheMOPagentandtheRagentinthehigh-dimensionalquadruped(ant)environment
from Gymnasium. (a) In both experiments, ϵ for the R agent is chosen as to match the average survival time of the
MOP agent. (b) Both MOP and R agents are able to reach the food source in most of the test runs. (c) Probability
density function of the travel time, defined as the time the agent spends before encountering the food source for the
first time. (d) Probability density function of the projection of all points in a trajectory, for all trajectories, onto the
line perpendicular to the shortest path connecting the origin and the food source during travel time (main diagonal).
(e) Display of 20 randomly chosen trajectories for the R agent before (left) and after (right) finding the food source
for the first time. Colorcode defined by the energy level of the agent. (f) Probability density function of the planar
speed before and after finding the food for the first time for the R agent. Both distributions show a peak at very low
velocities, indicating prolonged periods of time in which the ant performs very little translational movements.
A.5.7 Quadruped-Ant
Our goal is to show that MOP also works in high-dimensional, continuous action-state spaces. We employ the
Ant-v4 environment from OpenAI’s Gymnasium as our testing ground. We benchmark the performance of
our entropy-maximizing agent against agents using an ϵ-greedy strategy with rewards R=1 for every step
except for absorbing states, where R = 0 (R agent). All the relevant hyperparameters used to train these
agents are provided in Table 1.
In the first experiment, we study the behavioral variability of our agents and their average lifetime. The
agentbeginsatthe(x,y)coordinate(0,0)andfollowsitsdesignatedpolicyalgorithm. Theagentisconsidered
"dead" if either it takes a step that results in the z-coordinate of its torso falling outside the range [0.3,1.0],
or when the episode concludes.
In the second experiment, the agent possesses an energy value, represented as a scalar. The agent dies
once it consumes all its energy. Each step taken by the agent consumes one energy point. It commences
with an initial energy of 200, and its maximum energy capacity is set at 400. A food source is situated at
the (2,2) coordinate within the arena. Should the agent approach this source within a distance less than
0.5 from the center of its torso, it receives an energy boost of 25. The permissible z-coordinate range for
the agent’s torso remains consistent with the first experiment. In addition to the state vector provided by
the OpenAI Gym environment, we incorporate the agent’s energy level, its absolute position, and the food
source’s position. Interestingly, we found that the MOP agent travels to the food source much faster than
the R agent (Supplemental Fig. 11c, travel time distribution for the MOP agent is shifted towards short
times), seemingly appearing less risky, given the stochastic nature of both agents’ action selection. Even when
37
Table 1: Hyperparameters for Ant environment
Parameter Value
optimizer Adam
learning rate 3×10−4
discount (γ) 0.999
replay buffer size 106
number of hidden layers (all networks) 2
number of hidden units per layer 256
number of samples per minibatch 100
number of training epoch 300
steps per epoch 10000
initial random steps 20000
maximum episode length 5000
nonlinearity ReLU
target smoothing coefficient (τ) 0.005
number of agents 5
test runs 100
the MOP agent travels faster, its trajectories are more variable compared to the R agent (Supplemental Fig.
11d, projection of trajectories on a line perpendicular to the straight line that joins the origin and the food
source). In this second experiment, we find one out of five R agents not being able to reach the food source in
a significant percentage of the test runs. For this reason, we excluded that agent from the analyses.
Inthethirdexperiment, thex>0portionofthearenaproducesstatetransitionnoise. Intheunperturbed
case (first experiment), given a state s and action a, the agent transitions deterministically to a state
sp = step(s,a) (given by the Gymnasium package). For this experiment, we apply discrete noise to the
resulting state s′ in the following way: the i-th coordinate of the new state s′ now independently transitions
i
with probability 1/2 to either of two states that are close to the unperturbed transition sp. Specifically,

1/2 if x>0 and s′ =(1+u)sp
 i i
p(s′|s,a)= 1/2 if x>0 and s′ =(1−u)sp , (A.35)
i i i
1 if x≤0 and s′ =sp
i i
where u is the noise magnitude parameter, which makes the transition noisy with probability 1/2 (note that if
u = 0, the transition is deterministic, and corresponds to the unperturbed transition). The perturbations
thus scale with respect to the unperturbed transition sp, so that each coordinate gets noise proportional to
its magnitude. We apply this noise only to the coordinates given by the 27-dimensional observation vector
provided by Gymnasium, so we do not apply noise to the x,y coordinates directly (see details at Ref. [81]).
We do not implement an energy constraint in this experiment. The parameter α is set to a constant equal to
1. Note that the intrinsic reward for the next-state transition obtained from being in x>0 is independent of
u>0, as −βlog(p(s′|s,a))=βlog2, given that the stochasticity of the transition does not depend on the
action. Following Eq. (A.35), when u=0, this intrinsic reward vanishes.
We found that β ≥ 0 MOP agents are sensitive to the added noise, and they all managed to survive
for almost the whole duration of the episodes after training (Fig. 12a). First of all, when there is no noise
(u=0), the transition is deterministic, and agents do not show any preference to either side of the arena (Fig.
12b, grey line, c, first row). When noise magnitude is finite but small (u=0.01), β =0 MOP agents do not
show a significant preference between halves of the arena (Fig. 12b,c). However, β >0 MOP agents show a
preference for the half of the arena that produces state transition noise (Fig. 12b blue line, c second row). If
the noise magnitude is larger (u=5%,7%), small β MOP agents (including β =0) avoid the noisy half of the
arena, given that noise can more easily cause the agent to fall (Fig. 12b,c). Crucially, with increasing β >0
we see an increasing preference for the noisy half of the arena (Fig. 12b, increasing curves), without much
effect on survival rates (Fig. 12a).
38
Figure 12: Ant shows flexible preferences for stochastic transitions in the half plane x > 0 as a function of the β
parameter, which controls the preference for state transition entropy, for a constant α=1. Averages are across 1000
episodesforeachofthe5differentrandomseeds. (a)Averagesurvivaltimesfortheagentsshowthatallagentslearned
to approximately survive 1000 step episodes. (b) Mean of x position of the ant, as a function of next-state entropy
weight β, for various noise magnitudes u. (c) Position heatmaps of all agents for each combination of parameters.
39
Figure 13: Fixing the number of actions for non-absorbing states in the gridworld environment, instead of having it be
variable across states. We fix this number of actions at 9 for non-absorbing states. (a) MOP agent with fixed actions,
(b) R agent with fixed actions, (c) KL regularization agent with fixed actions.
A.6 Differences with KL regularization
Giventhesimilarityofourobjective,Eq. (2.1)toaKLregularizationscheme[70],herewecontrastpredictions
of using a KL divergence (relative entropy) compared to an absolute entropy objective. A relative entropy
objective would look like a maximization of the cumulative immediate reward given by the negative KL
divergence between a behavioral policy π(a|s) and a default policy π (a|s),
0
−D (π(a|s)||π (a|s))= (cid:88) π(a|s)ln π 0 (a|s) (A.36)
KL 0 π(a|s)
a
=H(π(a|s))−ln(|A(s)|), (A.37)
where the second equation comes from considering a default policy that is a uniform over actions, conveying
the idea that we want to be as close to a uniform policy as possible. However, the lack of an extrinsic reward
makes this case degenerate, in the sense that all states are equally preferred. We can see this by realizing
that the highest possible immediate intrinsic reward in this case is zero, given that KL divergence is always
non-negative. Thus, the optimal policy at all states is uniform over all available actions in each state. For
instance, for absorbing states, where only one action is available, Eq. (A.37) is zero, making a “KL agent" be
equally attracted to non-absorbing and absorbing states, completely opposite to the motivation of our work,
and illustrated in Supplemental Fig. 13c, where the agent dies very quickly. Furthermore, having a variable
or a fixed number of actions for non-absorbing states is the same for a KL agent, since the relative entropy
regularizes over the number of actions. This is in stark contrast with MOP, which intrinsically prefers states
with a high number of actions. Having a fixed number of actions for non-absorbing states affects the behavior
of MOP agents, which we can see in our gridworld experiment, comparing Fig. 2b and Fig. 13.
Finally, one could imagine setting up a default policy for a KL agent with a different set of available
actions than the behavioral policy. In particular, we can set the action set for the default uniform policy
to be fixed everywhere, including absorbing states. This amounts to shifting the immediate reward by a
scalar everywhere, resulting in an equivalent objective as MOP However, it is hard to see how one can justify
allowing the default policy to have a different set of actions than the behavioral policy, especially because
of the sum over actions in Eq. (A.37) implies that we sum over all (im)possible actions and implicitly set
the probability π(a|s) of the behavioral policy to be zero for actions that are not in its support. In contrast,
MOP does not have to deal with this problem, and can easily handle constant or variable number of actions
for non-absorbing states.
40
A.7 Comparison to Empowerment and Active Inference
A.7.1 Empowerment
In this subsection we compare the behaviors attained by the MOP and empowered (MPOW) agents. We
implemented empowerment for the 4-room gridworld and cartpole experiments.
A.7.1.1 4-room gridworld
For the 4-room gridworld, we implemented empowerment in its original discrete formulation [20]. That is, we
take the definition of empowerment of a particular state s at time t as the channel capacity between the
t
agent’s n-step actions an =(a ,a ,...,a )∈An at this state, and the resulting states s ,
t t t+1 t+n−1 t+n
C(s )= max (cid:88) p(s |s ,an)p(an|s )log (cid:18) p(s t+n |s t ,an t ) (cid:19) , (A.38)
t p(an t |st) An,S t+n t t t t (cid:80) An p(s t+n |s t ,an t )p(an t |s t )
where p(an|s ) is the probability distribution of n-step actions that mutual information is maximized over,
t t
and p(s |s ,an) is the n-step world model, computed as
t+n t t
n−1
(cid:89)
p(s |s ,an)=p(an|s ) p(s |s ,a )
t+n t t t t t+τ+1 t+τ t+τ
τ=0
.
This maximization procedure is done via the Blahut-Arimoto algorithm [77], with a tolerance of 1×10−12
for ∥p (an|s )−p (an|s )∥, where k is the iteration number of the algorithm. The initial condition for the
k+1 t t k t t
n-step action probabilities is uniform over actions, and for this particular environment, very few iterations
were needed for convergence (typically 3 or 4).
We initialize an agent at a particular location (in the center of a room, (x,y)=(3,3)), with an internal
energy of E = 30, so that the initial state is s = (E,x,y) = (30,3,3). The agent looks ahead at all
possible immediately successor states s , computes their empowerment, and greedily chooses the action
t+1
that corresponds to the successor state with highest empowerment (environment is deterministic). In our
particular formulation, we allowed for a stochastic choice of action in case of empowerment ties between
successor states. Note that the behavioral policy (greedy maximization of empowerment) and the probability
of the n-step actions over which mutual information is maximized are different [20].
Given the nature of the arena, we implemented 5-step empowerment, to give the agent enough lookahead
to consider going into other rooms, while keeping the computations tractable, given the large amount of
1-step actions (9 for center cells). Usually, empowerment assumes a fixed amount of actions across states, and
simply considers inconsequential actions to end in the current state, such as running into a wall resulting in
staying in the same place. We implemented this original formulation of empowerment, although it is possible
to implement state-dependent action sets, as for our formulation of MOP. This would still be meaningful for
empowerment, as having more actions available results in more distinct successor states, producing similar
predictions as in the original formulation of empowerment.
A.7.1.2 Cartpole
For the case of the cartpole experiment, we implemented continuous-state empowerment, as developed in [64],
C(s )= max (cid:88) p(an|s ) (cid:90) p(s |s ,an)log (cid:18) p(s t+n |s t ,an t ) (cid:19) ds , (A.39)
t p(an t |st) An t t S t+n t t (cid:80) An p(s t+n |s t ,an t )p(an t |s t ) t+n
where p(s |s ,an) is now a probability density over successor states s .
t+n t t t+n
In order to have enough lookahead without needing high n, we used 3-step empowerment with each action
in the 3-step action held constant for k =10 time steps, in order for the computation of empowerment to be
meaningfully different between states. Following [64], we constructed a Gaussian process from where successor
states s can be drawn for each of the actions, only in the computation of empowerment (real dynamics are
t+n
still deterministic). The standard deviation of the noise that blurs successor states was σ =0.01I , as in
4×4
[64], independent of the action. The number of Monte Carlo samples needed to be drawn to approximate the
high dimensional integral in Eq. (A.39) was N =300. The computation of empowerment is done similarly
MC
41
Figure 14: In our grid-world environment, the expected free energy (EFE) agent only visits a restricted portion of
the arena, as long as the target distribution is not perfectly uniform (see Supplemental Sec. A.7.2.2). In the limit of
infinite temperature (λ=0), the EFE degenerates to a survival maximization and the stochasticity is due to the Free
Energy degeneracy across actions.
as in the gridworld, through a Blahut-Arimoto algorithm described in [64]. Similarly, the agent looks ahead
at successor states, computes their empowerment and greedily chooses the action that corresponds to the
state with the highest empowerment.
A.7.2 Active Inference
Second, we compared with an active inference approach [79]. Note that our experiments assume full
observability of states, although the partial observability condition has often been studied under active
inference [80]. The Expected Free Energy (EFE) is defined as the quantity
G (s )= (cid:88) p (s¯ ,a¯ |s )log p(s¯ t+1 |a¯ t ,s t ) , (A.40)
π,t t π t+1 t t q(s¯ )
t+1
s¯t+1,a¯t
which is to be minimized as a function of the policy π, which is allowed to change as a function of the
state. Here s¯ =(s ,s ,...,s ) and a¯ =(a ,a ,...,a ), that is, the sequence of future states and
t+1 t+1 t+2 T t t t+1 T−1
actions respectively from time t up to some finite time T given that the initial state at time t is s . Thus,
t
p (s¯ ,a¯ |s )andp(s¯ |a¯ ,s )refertothejoinprobabilityoffuturestatesandactions, andtheirconditional,
π t+1 t t t+1 t t
respectively, given the initial state. The quantity q(s¯ ) factorizes as q(s¯ )=
(cid:81)T−1q(s
), where q(s) is
t+1 t+1 τ=t τ+1
a time-independent probability describing the "desired" states of the agent, capturing the idea that desired
states are independent of time. Note that G (s ) is the expectation over actions given a policy π of the KL
π,t t
divergence between p(s¯ |a¯ ,s ) and q(s¯ ), that is, G (s )=E KL(p(s¯ |a¯ ,s )||q(s¯ )). Note that
t+1 t t t+1 π,t t a¯t∼π t+1 t t t+1
because the time horizon is finite, here we need to consider time-dependent policies, so π(a |s ) is understood
t t
as the probability of selecting action a at time t given that the state at time t is s . Time-independent
t t
policies will be suboptimal in general in finite horizon MDPs.
Minimizing the objective in Eq. (A.40) is similar to MOP in that state transition entropy is being
maximized, butitdiffersinthatthereisnoactionentropyandthereisaregularizingdistributionq(s)towards
which states should converge on the long run. The latter distinction highlights a difference in focus of the
EFE and MOP approaches, but they can be made similar by just taking q(s) to be uniform in state space.
However, the former difference is essential: the optimal policy of the EFE will be deterministic (see Sec.
A.7.2), while the optimal policy of MOP is stochastic. Therefore, one expects to find much larger behavioral
variability under MOP than under EFE with uniform preference over all states.
ByvirtueoftheMarkovproperty,wehavep (s¯ ,a¯ |s )=
(cid:81)T−1π(a
|s )p(s |s ,a )andp(s¯|a¯ ,s )=
π t+1 t t τ=t τ τ τ+1 τ τ t t t
(cid:81)T−1p(s
|s ,a ). Therefore, the objective in Eq. (A.40) can be recursively written as
τ=t τ+1 τ τ
(cid:20) (cid:21)
G (s )= (cid:88) π(a |s )p(s |s ,a ) log p(s t+1 |s t ,a t ) +G (s ) (A.41)
π,t t t t t+1 t t q(s ) π,t+1 t+1
t+1
st+1,at
for t<T −1, while the terminal value is
G (s )= (cid:88) π(a |s )p(s |s ,a )log p(s T |s T−1 ,a T−1 ) , (A.42)
π,T−1 T−1 T−1 T−1 T T−1 T−1 q(s )
T
sT,aT−1
42
Figure 15: (Left) Entropy of distribution of visited state space as a function of horizon shows EFE agent is never as
good as the MOP agent in generating variability. (Right) Increasing the horizon lookahead makes EFE agent similar
to the R agent described in the main manuscript (see Fig. 4)
as at time T the episode terminates.
Note that the above formalization slightly generalizes EFE [79] by allowing the possibility that the optimal
policy is stochastic. Next we show that the optimal policy is deterministic.
To find the optimal policy, we proceed backwards in time [14]. At time T −1 the optimal policy is
deterministic because Eq. (A.42) is linear in the policy. The only exception is that there could be ties between
several actions having the same value of the objective, in which case one can be always chosen arbitrarily, or
they can be chosen randomly. Therefore, the optimal action is
a∗ (s )=argmin (cid:88) p(s |s ,a)log p(s T |s T−1 ,a) (A.43)
T−1 T−1 T T−1 q(s )
a T
sT
and define the optimal return at time T −1 as
G∗ (s )= (cid:88) p(s |s ,a∗ (s ))log p(s T |s T−1 ,a∗ T−1 (s T−1 )) , (A.44)
T−1 T−1 T T−1 T−1 T−1 q(s )
T
sT
Proceeding backwards, with t = T −2,T −3,..., we find that again for all times the optimal policy is
deterministic, and that the optimal action is
(cid:20) (cid:21)
a∗(s )=argmin (cid:88) p(s |s ,a) log p(s t+1 |s t ,a) +G∗ (s ) , (A.45)
t t t+1 t q(s ) t+1 t+1
a t+1
st+1
where the optimal return is recursively computed as
G∗(s )= (cid:88) p(s |s ,a∗(s )) (cid:20) log p(s t+1 |s t ,a∗ t (s t )) +G∗ (s ) (cid:21) . (A.46)
t t t+1 t t t q(s ) t+1 t+1
t+1
st+1
A.7.2.1 Discounted infinite-horizon sophisticated inference is identical to reward maximization
under deterministic dynamics
Here, we show that under an infinite horizon, a discounted expected free energy that considers state-
dependent policies in the future is equivalent to reward maximization under deterministic dynamics. We start
with the same assumption as before that minimizing EFE optimally needs to consider future states where the
agent is minimizing EFE. In a discounted, infinite horizon case, this becomes
(cid:88) (cid:20) p(s′|s,a) (cid:21)
G (s)= π(a|s)p(s′|s,a) log +γG (s′) , (A.47)
π q(s′) π
s′,a
where γ <1. Under deterministic dynamics p(s′|s,a)=1 for only one state s′, i.e. s′ =s′(s,a). So we can
rewrite the EFE as
(cid:88)
G (s)= π(a|s)[−log(q(s′(s,a)))+γG (s′(s,a))]. (A.48)
π π
a
43
Asking to minimize G is equivalent to maximizing −G, which means that the optimal Bellman equation for
sophisticated active inference in this case turns to
G∗(s)=max[log(q(s′(s,a)))+γG∗(s′(s,a))]. (A.49)
a
Simply rewriting log(q(s′(s,a)))=r(s′(s,a)) gives us the typical Belllman equation for MDPs.
In particular, when the preferred distribution is uniform on a finite portion of state space, under the
presence of absorbing states outside this portion, this scheme is identical to survival maximization. This
is because we can define q(s′(s,a)) = 1/V, where V is the volume of the portion of state space that
is not absorbing, for s′(s,a) that stays in this portion. For states outside this region, we can establish
q(s′(absorbing)) ≪ q(s′(alive)), such that log(q) is bounded. Therefore, for long horizons, we expect the
EFE agent to behave identically to our previously defined R agent that maximizes survival. We confirm this
expectation in Supplemental Fig. 15.
A.7.2.2 Details of simulations
Onecandefineatargetdistributionq(s)throughaBoltzmanndistribution,insteadofahardmaximization
of rewards, as similar to what is done in soft RL [39, 40]. The target distribution q(s) can be defined as
1
q (s)= exp(λR(s)), (A.50)
λ Z
λ
where λ is an inverse temperature, which expresses how motivated the agent is to maximize reward [79].
A.7.2.3 Grid world We take R=δ for being in the food and R=0 otherwise. For a large temperature,
λ is small, and thus q (s) is very close to an uniform distribution –it has a little bump on the reward location.
λ
EvenatinybumpbreaksthesymmetryoftheEFEagentindeterministicenvironmentssuchthatitabsolutely
prefers the food source location, and thus behavior collapses to the occupancy of that single state (see Fig. 6).
A.7.2.4 Cartpole We define the rewards similar to the R agent, R = 1 for non-absorbing states and
R=0 for absorbing states. This amounts to a uniform target distribution q(s) over non-absorbing states.
A.8 Relationship to Maximum Entropy Reinforcement Learning and goal direct-
edness
The objective of maximizing action-state path entropy in Eq. (2.2) for the special case β =0 can be obtained
from the maximum entropy reinforcement learning (MaxEnt RL) formulation [37, 38, 40]
(cid:34) (cid:88) ∞ (cid:12) (cid:35)
V (s)=E γt(r(s ,a )+αH(π(·|s )))(cid:12)s =s , (A.51)
π π t t t (cid:12) 0
t=0
bysettingtherewardr(s,a)=0forallstatesandactions, andthereforethereisnodifferencebetweenthetwo
approachesinthisparticularcase. However,thisreductionobscuresthefactthatwecangenerategoal-directed
behaviors in H-agents without the need of specifying rewards –indeed, this is one of the main accomplishment
of our work. To see this, we first quantify how a MaxEnt RL agent gets reward in the four-room grid world
defined in Supplemental Sec. A.5.3, as a function of the temperature parameter α. In this case, a sensible
goal is “eating food” (that is, defining r(s,a)=1 at the food locations, and zero everywhere else). Trivially,
when α ≪ 1 in Eq. (A.51), the goal is simply to maximize the future expected reward, equivalent to the
ϵ-greedy R agent defined in Supplemental Sec. A.5.2, for ϵ=0 (Figure 16a, leftmost points). In contrast, for
α ≫ 1, we recover the MOP agent in practice (due to the environment being deterministic). In this case,
the agent mostly focuses on maximizing future expected entropy, and getting small eating rate (Figure 16a,
rightmost points). Therefore, the temperature α quantifies how “goal directed” the agent should be, where the
goal here is understood as getting food, and the entropy term is understood as a regularizer that promotes
exploration of the arena.
44
Figure 16: Reward is not necessary for "goal-directed" behavior. (a) Eating rate as a function of the temperature
parameter α in Equation (A.51) for a MaxEnt RL agent in the four-room grid world. (b) Eating rate as a function of
the capacity for a MOP agent in the four-room grid world.
To aid in showing our central result that an extrinsic reward is not necessary for “goal directed behavior”,
we take the MOP agent and vary its energy capacity (see Supplemental Sec. A.5.3). For large capacities, the
MOP agent can largely ignore the food most of the time, obtaining small eating rate (Figure 16b, right-most
points). This is because food is conceived as the means to accomplish the goal of maximizing future path
occupancy. In contrast, when capacity is small, the MOP agent needs to get the food much more frequently to
avoidtheabsorbingstate,thusgettingmuchhighereatingrates(Figure16b,leftmostpoints). Theremarkably
strong qualitative similarities between the two panels in the figure show that by reinterpreting the concept of
reward, one can forego the need of specifying a reward function, and focus on more universal principles of
behavior.
A.9 Non-additivity of mutual information and channel capacity
Here we show that mutual information over Markov chains does not obey the additive property. It suffices to
prove our statement for paths of length two. Thus, we ask whether the mutual information between actions
(a ,a ) and states (s ,s ) given initial state s
0 1 1 2 0
MI = (cid:88) p(a ,s ,a ,s |s )ln p(a 0 ,s 1 ,a 1 ,s 2 |s 0 )
global 0 1 1 2 0 p(a ,a |s )p(s ,s |s )
0 1 0 1 2 0
a0,a1,s1,s2
equals the sum of the per-step mutual information
MI = (cid:88) p(a ,s |s )ln p(a 0 ,s 1 |s 0 ) + (cid:88) p(a ,s ,a ,s |s )ln p(a 1 ,s 2 |s 1 )
local 0 1 0 p(a |s )p(s |s ) 0 1 1 2 0 p(a |s )p(s |s )
0 0 1 0 1 1 2 1
a0,s1 a0,a1,s1,s2
where p(a ,s ,a ,s |s ) = π(a |s )p(s |s ,a )π(a |s )p(s |s ,a ) and p(a ,s |s ) = π(a |s )p(s |s ,a ).
0 1 1 2 0 0 0 1 0 0 1 1 2 1 1 0 1 0 0 0 1 0 0
Using Bayes’ rule and the Markov property, the above quantities can be rewritten as
45
MI = (cid:88) p(a ,s ,a ,s |s )ln p(a 0 ,a 1 |s 0 ,s 1 ,s 2 )
global 0 1 1 2 0 p(a ,a |s )
0 1 0
a0,a1,s1,s2
= (cid:88) p(a ,s ,a ,s |s )ln p(a 0 |s 0 ,s 1 )p(a 1 |s 1 ,s 2 )
0 1 1 2 0 p(a ,a |s )
0 1 0
a0,a1,s1,s2
= (cid:88) p(a ,s ,a ,s |s )ln p(a 0 |s 0 ,s 1 )p(a 1 |s 1 ,s 2 )
0 1 1 2 0 π(a |s )p(a |s ,a )
0 0 1 0 0
a0,a1,s1,s2
= (cid:88) p(a ,s ,a ,s |s )ln p(a 0 |s 0 ,s 1 )p(a 1 |s 1 ,s 2 )
0 1 1 2 0 π(a |s ) (cid:80) π(a |s)p(s|s ,a )
a0,a1,s1,s2 0 0 s 1 0 0
= (cid:88) p(a ,s |s )ln p(a 0 |s 0 ,s 1 ) + (cid:88) p(a ,s ,a ,s |s )ln p(a 1 |s 1 ,s 2 )
0 1 0 π(a |s ) 0 1 1 2 0 (cid:80) π(a |s)p(s|s ,a )
a0,s1 0 0 a0,a1,s1,s2 s 1 0 0
and
MI = (cid:88) p(a ,s |s )ln p(a 0 |s 0 ,s 1 ) + (cid:88) p(a ,s ,a ,s |s )ln p(a 1 |s 1 ,s 2 )
local 0 1 0 π(a |s ) 0 1 1 2 0 π(a |s )
0 0 1 1
a0,s1 a0,a1,s1,s2
The quantities MI and MI are remarkable similar except for the denominator in the ln of the last
global local
term in each expression. Therefore, equality between MI and MI holds iff
global local
(cid:88) (cid:88) (cid:88)
p(a ,s ,a ,s |s )ln π(a |s)p(s|s ,a )= p(a ,s ,a ,s |s )lnπ(a |s ),
0 1 1 2 0 1 0 0 0 1 1 2 0 1 1
a0,a1,s1,s2 s a0,a1,s1,s2
which is not true for all choices of policy and transitions probabilities. To see this, take a Markov chain where
the action a =0 from s =0 is deterministic, but results in two possible successor states s =1 or s =2
0 0 1 1
with equal probability 1/2. From s = 1 the policy takes actions a = 1 and a = 2 with probability 1/2.
1 1 1
From s =2 the policy is deterministic, that is, a =3 with probability 1. A simple calculation shows that
1 1
the left side equals −3ln2, while the right side equals a different quantity, −1ln2.
2 2
A.10 Video captions
A.10.0.1 Video 1 Animation of a portion of an episode comparing the behaviors of the MOP agent and
the ϵ-greedy R agent for the four-room grid world environment (see main text, Fig. 2, for more details).
A.10.0.2 Video 2 Animation of a portion of an episode of the MOP agent (mouse) behaving in the
predator-prey scenario detailed in Fig. 3.
A.10.0.3 Video 3 Animation of a portion of an episode of the R agent (mouse) behaving in the predator-
prey scenario detailed in Fig. 3.
A.10.0.4 Video 4 Animation of a portion of an episode comparing the behaviors of the MOP agent and
the R agent for the cartpole experiment detailed in Fig. 4.
A.10.0.5 Video 5 Animation of the state space trajectories (in an angle-position projection) traveled by
the MOP and the R agents from Video 4, sped up four times the original frame rate.
A.10.0.6 Video 6 Animation of a portion of an episode comparing the behaviors of the MOP agent and
the lifetime-matching ϵ−greedy R agent for the cartpole experiment detailed in Fig. 10.
A.10.0.7 Video 7 Animation of a portion of an episode comparing the behaviors of the MOP agent
and the MPOW and EFE agents for the four-room grid world environment (see main text, Fig. 6, for more
details).
46
A.10.0.8 Video 8 Animation of a portion of an episode comparing the behaviors of the MOP agent and
the MPOW and EFE agents for the cartpole experiment (corresponding to Fig. 6).
A.10.0.9 Video 9 Animation of a portion of an episode comparing the behaviors of the MOP agent and
the R agent for the quadruped experiment without energetic constraints (corresponding to upper row of Fig.
7).
A.10.0.10 Video 10 Animation of a portion of an episode comparing the behaviors of the MOP agent
and the R agent for the quadruped experiment with energetic constraints (corresponding to lower row of Fig.
7).
47

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Complex behavior from intrinsic motivation to occupy action-state path space"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
