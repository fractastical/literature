=== IMPORTANT: ISOLATE THIS PAPER ===You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.Do NOT mix information from different papers. Only use information from THIS specific paper.Paper Title: A unified theory of learningCitation Key: katayose2022unifiedAuthors: Taisuke KatayoseYear:2022Abstract: Recently machine learning using neural networks (NN) has been developed, and many newmethods have been suggested. Thesemethodsareoptimizedforthetypeofinputdataandworkvery effectively, but they cannot be used with any kind of input data universally. On the otherhand, the human brain is universal for any kind of problem, and we will be able to constructartificial general intelligence if we can mimic the system of how the human brain works. Weconsider how the human brain learns things uniformly, and find that the essence of learning isthe compression of information. We suggest a toy NN model which mimics the system of thehuman brain, and we show that the NN can compress the input information without ad hoctreatment, only by setting the loss function properly. The loss function is expressed as the sumoftheself-informationtorememberandthelossoftheinformationalongwiththecompression,and its minimum corresponds to the self-information of the original data. To evaluate the self-information to remember, we provided the concept of memory. The memory expresses thecompressed information, and the learning proceeds by referring to previous memories. Thereare many similarities between this NN and the human brain, and this NN is a realization ofthe free-energy principle which is considered to be a unified theory of the human brain. Thiswork can be applied to any kind of data analysis and cognitive science.Key Terms: taisuke, human, theory, information, learning, data, katayose, brain, unified, kind=== FULL PAPER TEXT ===OU-HET-1138A unified theory of learning∗Taisuke KatayoseDepartment of Physics, Osaka University, Toyonaka560-0043, JapanAbstractRecently machine learning using neural networks (NN) has been developed, and many newmethods have been suggested. Thesemethodsareoptimizedforthetypeofinputdataandworkvery effectively, but they cannot be used with any kind of input data universally. On the otherhand, the human brain is universal for any kind of problem, and we will be able to constructartificial general intelligence if we can mimic the system of how the human brain works. Weconsider how the human brain learns things uniformly, and find that the essence of learning isthe compression of information. We suggest a toy NN model which mimics the system of thehuman brain, and we show that the NN can compress the input information without ad hoctreatment, only by setting the loss function properly. The loss function is expressed as the sumoftheself-informationtorememberandthelossoftheinformationalongwiththecompression,and its minimum corresponds to the self-information of the original data. To evaluate the self-information to remember, we provided the concept of memory. The memory expresses thecompressed information, and the learning proceeds by referring to previous memories. Thereare many similarities between this NN and the human brain, and this NN is a realization ofthe free-energy principle which is considered to be a unified theory of the human brain. Thiswork can be applied to any kind of data analysis and cognitive science.∗taisuke.katayose@het.phys.sci.osaka-u.ac.jp2202rpA42]GL.sc[2v14961.3022:viXraContents1 Introduction22 Consideration about human learning32.1 General definition of learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .32.2 Correlation and probability bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .32.3 Probability bias and redundancy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .42.4 Detection of redundancy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .