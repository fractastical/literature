=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Investigating the impact of free energy based behavior on human in human-agent interaction
Citation Key: horibe2022investigating
Authors: Kazuya Horibe, Yuanxiang Fan, Yutaka Nakamura

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2022

Key Terms: people, principle, osaka, energy, investigating, impact, sense, behavior, unity, agent

=== FULL PAPER TEXT ===

Investigating the impact of free energy based behavior on human
in human-agent interaction
KazuyaHoribe†∗,YuanxiangFan∗,YutakaNakamura andHiroshiIshiguro
DepartmentofSystemsInnovation,OsakaUniversity,Osaka,Japan
(E-mail: horibe@irl.sys.es.osaka-u.ac.jp)
∗Theseauthorscontributedequallytothiswork.
Abstract: Humanscommunicatenon-verballybysharingphysicalrhythms,suchasnoddingandgestures,toinvolveeachother.
This sharing of physicality creates a sense of unity and makes humans feel involved with others. In this paper, we developed
a new body motion generation system based on the free-energy principle (FEP), which not only responds passively but also
promptshumanactions. Theproposedsystemconsistsoftwomodules,thesamplingmodule,andthemotionselectionmodule.
We conducted a subjective experiment to evaluate the ”feeling of interacting with the agent” of the FEP based behavior. The
resultssuggestedthatFEPbasedbehaviorsshowmore”feelingofinteractingwiththeagent”. Furthermore,weconfirmedthat
theagent’sgestureselicitedsubjectgestures. Thisresultnotonlyreinforcestheimpressionoffeelinginteractionbutcouldalso
realizationofagentsthatencouragepeopletochangetheirbehavior.
Keywords: non-verbalcommunication,human-agentinteraction,entrainment,freeenergyprincipal
1. INTRODUCTION changetheirbehavior.
Peopleinteractwitheachotherbydrawingineachother’s
2. FREE ENERGY PRINCIPLE BASED AC-
physicalrhythms,suchasnon-verbalgestures[5,10,11].The TION SELECTION FOR INPUT-OUTPUT
synchronization of physical rhythms through attraction cre- HIDDENMARKOVMODEL
ates a sense of unity in people and plays an important role
infacilitatingdialogueintermsofinterpersonalimpressions
and conversation satisfaction[7]. The sense of unity during
adialoguehasbeenquantifiedbymeasuringthefeatureval-
ues of the movements of two people during a dialogue. It
hasbeenshownthatthenumberofdimensionsofthefeature
values of the movements of two people decreases during a
dialogue, suggesting that it is easier to predict each other’s
movementsduringadialogue[12].
Agents that interact with people daily have been studied
extensively for purposes such as guiding people at the re-
ception, serving customers, and recommending products in
stores,andhouseholdpets[1,3,4]. Sincemostagentsusethe
other person’s state as input for deciding their next action,
theydonotrecognizetheirmovementsandtheotherperson’s
movements as a set, and it is difficult to obtain the sense of
unitythatcanbeachievedthroughthemutualdirectionalat-
tractionbetweenpeoplethroughagent-humaninteraction.
Inthisstudy,wedesignedanagentthatgeneratesthenext
gestureusingbothitsownandtheotherperson’smovements.
Togeneratethegesture,weusedFriston’sfreeenergyprinci-
ple[6]. Theagenttraineditsgenerativemodelusingtheges-
turesoftwopeoplewhowerefacingeachotherandinteract-
ingbeforehandandgeneratedgesturesthattookintoaccount
bothitsownandtheotherperson’smovements. Asaresult,
thesubjects’impressionsofthelife-likenessandhuman-like Fig. 1: Schematic of the gesture generation model. (a) Ac-
natureoftheagentwereimproved.Notonlythat,weshowed tionselectionmodule,whereDisa“identifier”functionthat
that the subjects paid attention to the agent’s gestures and returns a decision from the current state s(t) as to whether
weredrawnintotheinteraction. Thisproposedmodelisex- or not it is interacting with the person in front. (b) Action
pectedtoopenthedoortoanewtypeofhuman-agentinter- alternativesarerandomlysampledfromaroadmapofaction
action that actively engages people and encourages them to sequences. Thefreeenergyofeachsampledseriesiscalcu-
latedandthesmallestactionseriesisselected.
† KHisthepresenterofthispaper.
2202
naJ
52
]CH.sc[
1v46101.1022:viXra
Symbol Definition
ActionsA(t) A(t)=[a(1),a(2),...,a(t)],theinputsequencefromtimestep1tot
LatentstateS(t) S(t)=[s(1),s(2),...,s(t)],thelatentstatesequencefromtimestep1tot.
OutputsO(t) O(t)=[o(1),o(2),...,o(t)],theoutputsequencefromtimestep1tot
LikelihoodL(θ,O(t),A(t)) ThelikelihoodofIO-HMMgivingtheoutputsequenceO(t)andtheinputsequenceA(t)
IO-HMMparametersθ θ=(θ ,θ ,θ ),whicharetheparametersofinitial,transition,andemissionmodel
IN TR EM
Table1: SymboldefinitionofIO-HMM
Inthischapter,wedescribethegesturegenerationmodel The parameter θ for initial model is a matrix with the
IN
oftheproposedagent(Fig.1). Thegesturegenerationmodel ith row θi being the coefficients for the initial state being
IN
consists of two components. (1) Sampling module: a prob- in state s and is modeled by using multinomial logistic re-
i
abilistic roadmap to generate gesture sequences (Fig. 1(a)). gression. Foraisdefinedasatimesequencedata, θi a(1)
IN
(2)Actionselectionmodule:atimeevolutionmodelthatcal- represented the inner product of features transformed from
culatesthefreeenergyoftheseriesobtainedfromthegesture a(1)andvectorθi .
IN
roadmap and selects the gesture with the minimum free en- Thetransitionprobabilitymodelisdefinedas
ergymodeledbyPartiallyObservableMarkovDecisionPro-
Pr(x(t)=x |x(t−1)=x ,u(t);θ )=
j i TR
cesses(POMDP)(Fig.1(b)). ThePOMDPmodeledthetime
transitionwithIO-HMM(Table.1)andusedfreeenergyasa eθ T ij R a(t) (cid:88) eθ T ik R u(t). (3)
strategyforactionselection. k
The parameter θ for transition model is a set of matrices
2.1. Action selection module: Free Eenegy Principle TR
withthejthrowoftheithmatrixθij beingthecoefficients
basedactionselectionforIO-HMM TR
forthenextstartbeinginstates giventhecurrentstatebeing
j
To inclement POMDP model for generating motion, we
ins andmodeledusingmultinomiallogisticregression. For
i
proposed to combine the free energy principle (FEP) and aisdefinedasatimesequencedata,θij a(t)representedthe
Input-output Hidden Markov Model (IO-HMM). For a set TR
inner product of features transformed from a(t) and vector
ofpossibleactions, thefreeenergyF iscalculatedforeach θij .
action,andthepolicyistoselecttheactionwithminimalfree TR
Theemissionprobabilitymodelisdefinedas
energy. ThearchitectureofIO-HMMisshowninFig. 1(b)
1
rightside. Thebluenodesrepresenttheobservedvariables, Pr(o(t)=1|s(t)=s ;θ )= . (4)
whilethewhitenodesrepresentthelatentvariables. Thetop i EM 1+e−θ E i M
row contains the input variables a(t); The middle row con- The parameter θ for emission model is a set of array
EM
tains the latent variables s(t); The bottom row contains the whereθi denotethecoefficientswhenthehiddenstateis
EM
outputvariableso(t). TheoutputofIO-HMMisthefreeen- s .
i
ergy,i.e.,theentropyofthepredictedstateatthenexttime,
2.2. Samplingmodule:Probabilisticroadmapformotion
giventhebeliefstatecalculatedfromtheobservedseriesso
sampling
farandanaction.
The dynamics is solved by maximizing parameter like- To obtain a choice of action sequences, we used a prob-
lihood L(θ,O(T), A(T)) after input data sequence A(T) and abilistic roadmap for motion sampling (Fig. 1(b) left side).
output data sequence O(T) has been given. The likelihood one node represents the posture of the upper body, and the
canbecalculatedby edgesrepresenttheprobabilityoftransitioningfromonepos-
turetoanother. Asinglenoderepresentsanupperbodypos-
(cid:88) ture,andanedgerepresentstheprobabilityoftransitionfrom
L(θ,O(T),A(T))= Pr(s(1)|a(1);θ )
IN oneposturetoanother. Thechoicesofactionsequencesare
s randomlysampledactionsequencesofafixedtimestep.
T
(cid:89)
Pr(s(t−1),a(t);θ TR ) (1) 3. DATACOLLECTIONAND
t=2 MODULETRAINING
T
(cid:89)
Pr(o(t)|s(t);θ ), 3.1. Interactionmotiondatacollection
EM
t=2 Werecordedbyanomnidirectionalcamera(Insta360Air,
where θ , θ and θ denotes the parameters of initial ShenzhenArashiVision,China). Theparticipantsareasked
IN TR EM
modelPr(s(1)|a(1);θ ),transitionmodelPr(s(t)|s(t−1), to watch an interesting video then talk about it with the ex-
IN
a(t); θ ) and emission model Pr(o(t)|s(t); θ ) respec- perimentersonebyone.Anomnidirectionalcamerabetween
TR EM
tively. themisusedtorecordvideodataofparticipantsandexper-
Theinitialprobabilitymodelisdefinedas imenters. The video data is recorded in the 3d frame per
second. Theresolutionoftherecordedimageis1536×768.
eθ
I
i
N
a(1)
Thetotallengthofrecordedvideoisaround80minute(Fig.
Pr(s(1)=s |a(1);θ )= . (2)
i IN (cid:80)
k
eθ
I
k
N
a(1) 2(a)).
vector R τ) stands for the joint angles of dataset R at time
(
pointτ.
In learning phase, graph G = {n, l} is initialized by con-
nectingnodesaccordingtotheconsecutivenessoftimepoint
(e.g. from R(τ −1) to R(τ)) as shown in Fig.2(c). Prob-
abilistic connections is established based on joint angular
velocity V(τ) and joint angular acceleration W(τ). The
joint angular velocity V(τ) and joint angular acceleration
W(τ) is calculated by the difference of joint angles (e.g.
R(τ −1)−R(τ))andthedifferenceofjointangularveloci-
Fig.2: Datapreparation. (a)Skeletonkeypointsextractedby ties(e.g. V(τ)−V(τ −1))). Sincethehumanmotionmust
OpenPose. Participants (left side) and experi- menter (right befluent, thevariationbetweentworeachablenodescanbe
side)(b)Result of 3d posture estimation(c)Process of initial- consideredlowerthansomelimitations.Therelationshipbe-
izingRoadmap(d)ProcessoffusingnodesinRoadmap tweenjointangularvelocityandjointangularaccelerationis
modeledbylinearregression.
Openpose[2,13,14], is used to extract 2d skeleton key-
3.2. Observationmodel: discriminatortocalculateo(t)
points sequences of human body from the videos. The out-
Interaction motion is defined as Irτ) containing L coor-
( put of OpenPose is quite noisy, so We remove outlier and
dinatesvectorsfromtimepointτ −Ltoτ. Thediscrimina-
resampledatato8fpsbylinearinterpolation. Sincethehu-
tor is implemented by convolutional neural network (CNN)
manmotionduringinteractcanbeconsideredalwayswithin
trained by semi-supervise learning method, where fake in-
somespecificfrequency,Weusealowpassfilterwithcutoff
teraction data If is generated from real interaction data Ir
frequencyof4Hztosmoothdata.
bynegativesampling. Thentherealinteractionandfakein-
The dataset C is consists of skeleton keypoints sequence
teraction data is mixed for training. We used the network
ofexperimentersC andparticipantsC . C τ)standsforthe
p e (
structure of CNN, which is Fully-connected layer (On 2nd
coordinates vector at time point τ, which contains the key-
axis 56 to 48), conv1(channel=16, kernel=[3,3], stride = 1,
points coordinates of participants C (τ) and experimenters
p
padding = [0,0]), conv2(channel=32, kernel=[3,3], stride =
C (τ). Theyarealsoshownintheleftsideandrightsideof
e
2, padding = [1,0]), conv3(channel=64, kernel=[3,3], stride
Fig.2(a)respectively.
= 2, padding = [1,0]), Fully-connected layer (3200 to 128,
dropout=0.5),andFC(128to1). 3.4. TrainingIO-HMMthoroughEMalgorithm
Before training the network, We normalized the mixed
The input sequence A(T) and the output sequence O(T)
dataset to between 0 and 1, then augment data by adding
is necessary to estimate parameters in Input-output Hidden
noise in the temporal axis and spatial axis. Fake interac-
MarkovModel(IO-HMM)(Table.1). Datausedintraining
tion data If can be generated from real interaction data Ir
IO-HMMisskeletonkeypointssequenceofexperimenterC
e
byshiftingthetimeofIr(τ)orIr(τ)randomly. Theobjec-
e p andthepredictionofinteractionmotiondiscriminatorF d (C)
tive of training a Discriminator is to minimize binary cross
givendatasetCasinputsequenceA(T)andoutputsequence
entropy.
O(T). The parameter θ for emission model is a set of
EM
The objective of training a Discriminator is to minimize array where θi denote the coefficients when the hidden
EM
binarycrossentropy.
state is s . The parameters θ of this probabilistic model is
i
estimated to maximize likelihood L(θ,O(t), A(t)|C) under
Loss=−[ylogF (I)+(1−y)log(1−F (I))] (5)
d d thedatasetCbyExpectation-Maximization(EM)algorithm.
where F (I) is the prediction on interaction motion I, y Fig. 3(b)showsthelearningcurveoftrainingIO-HMMand
d
is the ground truth of whether interaction motion I is real this figure suggests that this probabilistic model can fit our
data or not, y = 1 when I is real data, else y = 0. Fig. 3(a) dataset.
shows that the interaction motion discriminator can achieve
4. EXPERIMENTS
theaccuracyof60.5%,whichisbetterthanussinceWecan
nottellmostthefakedatawhenwewatchthemmanually.
4.1. ImpressionontheFEPbasedbehavior
3.3. ProbabilisticRoadmapforMotionSampling To the purpose of evaluating impression on the Free En-
In order to apply the sampled motion to robots and vir- ergy Principle (FEP) based behavior in human-perceptible
tual agent, it is necessary to construct PRM by 3D skele- level, We gather some experiment subjects and the impres-
ton keypoints. We use a 3D posture estimation method to sionismeasuredbythescoretheygavetothegeneratedmo-
estimate 3D skeleton keypoints for each 2D skeleton key- tion. The experiment subjects are indicated to bring an ob-
points[9]. The outcome is shown in Fig.2(b). Roadmap is ject back from the front of the projected human-size virtual
constructed by motion of experimenter-side in the dataset agent and evaluate the impression on the agent. An omni-
C . Sincerobotsaretypicallycontrolledbyjointangles,We directionalcameraisusedforcapturingthesensoryinputof
e
constructdatasetRbycalculatingthejointrotationforeach thevirtualagent. Thesnapshotsofevaluationexperimentfor
frame in the 3D skeleton keypoints sequence. Coordinates eachconditionisshowninFig. 4.
instance,theintensityofmotiongeneratedby“PerlinNoise”
and “Sampling based on PRM” is limited when comparing
to“Minimizingfreeenergy”.
Fig. 3: Learning curve (a)Learning curve of discriminator.
Orange: Train accuracy, Blue: Test Accuracy (b)Learning
curveofIO-HMM
Fig.4: Evaluationexperimentforthreeconditions
Theimpressionisevaluatedbythreeindexes,i)“lifelike-
ness”, ii) “human-likeness” and iii) “feeling of interacting
with agent”. Three motion generation methods are com- 4.2. Agentschangehumansbehavior
pared in this experiment, i) “Minimizing free energy”, ii) Then, to investigate whether or not the agent causes the
“Sampling based on PRM” and iii) “Perlin Noise”. Experi- pull-in, we gathered new subjects and experimented in the
mentsubjectneedstoanswerthreequestionsaftereachtime same environment as Fig. 4. This time, we experimented
they bring the object back. The order of motion generation withtwoconditions:onewitharecordingagentandtheother
methodisdeterminedrandomlybeforehand. withanagentinteractinginreal-time. Fortherecordingcon-
Motiongeneratedby“Minimizingfreeenergy”isimple- dition, we used a video of the author interacting with the
mentedasmotiongenerationsystemexplainedinsection3. agentbeforehand. Threesubjectswereusedforeachcondi-
Ateachtimestep, asetofpossiblemotionissampledfrom tion. Subjectsenteredaroomwhereahuman-sizeagentwas
ProbabilisticRoadmap(PRM)andtheonewithminimalfree projectedandwereaskedtorecordthenamesof10fruitsinto
energy is selected. The motion generated by “Sampling amicrophoneinfrontofthem. Thesubjectswereinstructed
based on PRM” is implemented as sampling motion from tostartrecordingattheirtimingandtoleavetheroomwhen
PRMrandomlyexplainedinchapter3.Themotiongenerated they were finished and were not told that an agent was pro-
from “Perlin Noise” is implemented as adding noise to idle jected inside. We filmed the agent and the subject during
posture. Inrobotics,“PerlinNoise”hasbeenused,addedto theirstayintheroomusingavideocamera.
jointangles,toincreasethelifelikenessofrobotmovements Thetimespentbythesubjectintheroomwheretheagent
andtogenerateidlemotion. Theexperimentisconductedin was projected was measured. One of the subjects who in-
alaboratoryroomoftheuniversity. Theexperimentsubjects teracted with the agent in real-time showed interest in the
arebachelorstudents,masterstudentsanddoctoralstudents agent’sgesturesandwaslisteningtotheagent’sgesturesev-
of robotics major. We gathered 8 experiment subjects and ery time the name of fruit was mentioned during the task.
repeat5timesforeachcondition. After the task, the agent responded to the sound. After the
task was over, the subject was observed vocalizing into the
The subjective impression on three motion generation
microphoneandmimickingtheagent’sgesturestoseeifthe
methodsisshowninFig.5. Asexplainedinthelastsection,
agentrespondedtothesound.
experimentsubjectsscorethegeneratedmotionfrom1to7.
To express the difference between three motion generation
5. DISCUSSION
method intuitively, the box plot on the left side of each fig-
uresshowstheminimumvalue,firstquartile,medianvalue, Weimplementedafree-energybasedagentthatmakesac-
third quartile value and maximum value of score for each tion choices based on a set of self and opponent gestures.
motiongenerationmethod. Ontherightsideofeachfigure, This agent improved the impression of interaction for the
histograms describe how many people gave a specific score subject,aswellastheimpressionofhuman-likenessandlife-
for motion generated by “Minimizing free energy”, “Sam- likeness. Inaddition,wefoundthatsubjectswhoconfronted
plingbasedonPRM”and“PerlinNoise”. theagentpaidmoreattentiontothegesturesgeneratedbythe
ByusingMann-Whitney’sUtest[8],Theresultssuggest agent and spent more time observing them, and some sub-
thatmotiongeneratedby“Minimizingfreeenergy”showsa jectsweredrawnintonon-verbalinteractionbyimitatingthe
more“feelingofinteractingwithagent”thanboth“Sampling gesturesthemselves.
based on PRM” and “Perlin Noise” (p < 0.01) (Fig. 5(a)). In human-agent interaction, there have been many stud-
Moreover, data indicate that motion generated by “Mini- ies of agents that take only the other person’s actions as in-
mizingfreeenergy”showsmore“lifelikeness”and“human- put. The proposed model generates gestures by taking into
likeness” than both “Sampling based on PRM” and “Perlin account the set of actions of both the agent and the other
Noise” (p < 0.01) (Fig. 5(b)(c)). By watching the recorded person,sothatthemutualattractionthatoccursduringinter-
videoofevaluationexperiment,itcanbefoundthattheinten- action between humans can be realized in human-agent in-
sityofmotionmayalsoaffectthesubjectiveimpression. For teraction. Theagent’sgestureswereabletoinduceachange
Fig.5: ImpressionontheFEPbasedbehavior(a)Feelingofinteraction. (b)Human-lileness(c)Life-likeness
inthebehavioroftheperson. Itisexpectedtoopenthedoor International Conference on Computer Vision, pages
to the exciting challenge of agents that actively work with 2640–2649,2017.
peopleandencouragethemtochangetheirbehavior. [10] David McNeill. So you think gestures are nonverbal?
Psychologicalreview,92(3):350,1985.
6. ACKNOWLEDGMENTS [11] DavidMcNeill. Handandmind. DeGruyterMouton,
2011.
This work was supported by JST Moonshot R&D Grant [12] Yusuke Nishimura, Yutaka Nakamura, and Hiroshi
NumberJPMJMS2011. Ishiguro. Human interaction behavior modeling us-
inggenerativeadversarialnetworks. NeuralNetworks,
REFERENCES 132:521–531,2020.
[13] TomasSimon,HanbyulJoo,IainMatthews,andYaser
[1] Timothy Bickmore, Laura Pfeifer, and Daniel Schul-
Sheikh.Handkeypointdetectioninsingleimagesusing
man. Relationalagentsimproveengagementandlearn-
multiview bootstrapping. In Proceedings of the IEEE
ing in science museum visitors. In International
conference on Computer Vision and Pattern Recogni-
Workshop on Intelligent Virtual Agents, pages 55–67.
tion,pages1145–1153,2017.
Springer,2011.
[14] Shih-EnWei, VarunRamakrishna, TakeoKanade, and
[2] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Yaser Sheikh. Convolutional pose machines. In Pro-
Sheikh. Realtime multi-person 2d pose estimation us- ceedings of the IEEE conference on Computer Vision
ing part affinity fields. In Proceedings of the IEEE andPatternRecognition,pages4724–4732,2016.
conferenceoncomputervisionandpatternrecognition,
pages7291–7299,2017.
[3] GinevraCastellano,IolandaLeite,Andre´ Pereira,Car-
losMartinho, AnaPaiva, andPeterWMcowan. Mul-
timodal affect modeling and recognition for empathic
robotcompanions. InternationalJournalofHumanoid
Robotics,10(01):1350010,2013.
[4] SoumiaDermoucheandCatherinePelachaud. Genera-
tivemodelofagent’sbehaviorsinhuman-agentinterac-
tion. In2019InternationalConferenceonMultimodal
Interaction,pages375–384,2019.
[5] DavidEfron. Gestureandenvironment. 1941.
[6] Karl Friston, Francesco Rigoli, Dimitri Ognibene,
Christoph Mathys, Thomas Fitzgerald, and Giovanni
Pezzulo. Active inference and epistemic value. Cog-
nitiveneuroscience,6(4):187–214,2015.
[7] SpencerDKelly,DaleJBarr,RBreckinridgeChurch,
andKatherynLynch. Offeringahandtopragmaticun-
derstanding: The role of speech and gesture in com-
prehensionandmemory. JournalofmemoryandLan-
guage,40(4):577–592,1999.
[8] Henry B Mann and Donald R Whitney. On a test of
whether one of two random variables is stochastically
largerthantheother.Theannalsofmathematicalstatis-
tics,pages50–60,1947.
[9] Julieta Martinez, Rayat Hossain, Javier Romero, and
James J Little. A simple yet effective baseline for 3d
human pose estimation. In Proceedings of the IEEE

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Investigating the impact of free energy based behavior on human in human-agent interaction"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
