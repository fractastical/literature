=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Benchmarking Dynamic SLO Compliance in Distributed Computing Continuum Systems
Citation Key: lapkovskis2025benchmarking
Authors: Alfreds Lapkovskis, Boris Sedlak, Sindri Magnússon

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Key Terms: continuum, systems, distributed, sedlak, sindri, computing, compliance, dynamic, benchmarking, lapkovskis

=== FULL PAPER TEXT ===

Benchmarking Dynamic SLO Compliance in
Distributed Computing Continuum Systems
Alfreds Lapkovskis∗, Boris Sedlak†, Sindri Magnu´sson∗, Schahram Dustdar†‡, and Praveen Kumar Donta∗
∗Department of Computer Systems and Sciences (DSV), Stockholm University, SE-106 91 Stockholm, Sweden
alfreds.lapkovskis, praveen, sindri.magnusson @dsv.su.se
{ †Distributed Systems Group, TU Wien, Vienna 1040, Au } stria
b.sedlak,dustdar @dsg.tuwien.ac.at
‡ICREA, U { niversitat Pompeu Fabra } Barcelona, Barcelona 08002, Spain
Abstract—Ensuring Service Level Objectives (SLOs) in large- capacity, cost, and priority, enhancing real-time processing
scale architectures, such as Distributed Computing Continuum and minimizing latency. Unlike traditional edge computing,
Systems(DCCS),ischallengingduetotheirheterogeneousnature
they offer fault tolerance by reallocating tasks to other avail-
and varying service requirements across different devices and
able servers in case of device failure, ensuring uninterrupted
applications.Additionally,unpredictableworkloadsandresource
limitations lead to fluctuating performance and violated SLOs. computation [7], [8]. Additionally, DCCS prioritize resource
ToimproveSLOcomplianceinDCCS,onepossibilityistoapply efficiency, enabling scalable and adaptive computing across
machinelearning;however,thedesignchoicesareoftenlefttothe the continuum. They maintain a high Quality of Experience
developer. To that extent, we provide a benchmark of Active In-
(QoE)despitechangingsystemrequirementsandenvironmen-
ference—an emerging method from neuroscience—against three
tal uncertainties by effectively managing resources [9], [10].
establishedreinforcementlearningalgorithms(DeepQ-Network,
Advantage Actor-Critic, and Proximal Policy Optimization). We Simultaneously, DCCS are complex, open systems, vul-
consider a realistic DCCS use case: an edge device running nerable to workload spikes, evolving requirements, and dy-
a video conferencing application alongside a WebSocket server namicinfrastructurechanges[6].Thesechallengesnecessitate
streamingvideos.Usingoneoftherespectivealgorithms,wecon-
adaptive capabilities to maintain optimal performance and
tinuously monitor key performance metrics, such as latency and
resilience. However, anticipating all possible system config- bandwidth usage, to dynamically adjust parameters—including
the number of streams, frame rate, and resolution—to optimize urations and environmental conditions is often impractical.
servicequalityanduserexperience.Totestalgorithms’adaptabil- Therefore, DCCS require pervasive intelligence across the
itytoconstantsystemchanges,wesimulatedynamicallychanging entire continuum to ensure seamless integration, optimal per-
SLOs and both instant and gradual data-shift scenarios, such as
formance, and robust management. This intelligence enables
network bandwidth limitations and fluctuating device thermal
the system to respond to fluctuations in workload and chang-
states. Although the evaluated algorithms all showed advantages
and limitations, our findings demonstrate that Active Inference ing conditions dynamically, ensuring reliability and efficiency
isapromisingapproachforensuringSLOcomplianceinDCCS, in real-time processing and resource allocation [11]. Thus,
offering lower memory usage, stable CPU utilization, and fast Service Level Objectives (SLOs) [12]–[15] are introduced to
convergence.
provide a structured approach for monitoring, predicting, and
Index Terms—distributed computing continuum systems, ser-
managingoradaptingsystembehavioracrossdiversecomput-
vice level objectives, active inference, reinforcement learning,
quality of service, quality of experience ing environments. By establishing clear performance targets,
SLOs enable adaptive mechanisms that respond dynamically
to fluctuations in workload and changing conditions, ensuring
I. INTRODUCTION
that the system meets predefined performance standards.
Over the decades, computing environments have evolved In the literature, various mechanisms have been explored
cyclically, shifting between centralized, decentralized, and for the dynamic adaptation of workloads through effective
distributed models based on technological advancements and orchestration strategies [16]. While these topics are gradu-
organizational needs [1]–[3]. Recently, distributed environ- ally addressed by applying different machine learning (ML)
ments, particularly those incorporating edge computing and mechanisms, e.g., [17], [18], these works fail to provide a
IoT, have gained prominence due to their advantages, such as fundamental understanding of which ML techniques to apply
lowlatencyandenhancedprivacy[4],[5].Inthisongoingevo- to ensure SLOs. For example, with Octopus [18], the authors
lution, Distributed Computing Continuum Systems (DCCS) createdanSLO-awareinferenceschedulerbasedonAdvantage
have emerged as a powerful approach, efficiently integrating Actor-Critic (A2C). For evaluation, the common scheme here
multiple computational tiers into a cohesive ecosystem that is to use baselines designed for a different use case or
ensures trade-off between cost, Quality of Service (QoS), and have a completely different architecture. Within Octopus, the
resource utilization at scale [6]. In DCCS, tasks are allocated open question is whether other ML techniques, e.g., Proximal
dynamically based on multiple criteria, including proximity, Policy Optimization (PPO), would have performed superiorly.
5202
raM
5
]CD.sc[
1v47230.3052:viXra
To provide a profound understanding of how different ML The remaining sections of this paper are organized as
techniques rank for ensuring SLOs in a DCCS application, follows: Section II provides an overview of AIF and com-
this paper provides benchmarks that target various aspects of mon RL algorithms. Section III presents a detailed use case,
the techniques. While there exist benchmarking solutions for SLO design, and algorithm implementation. In Section IV,
pure Edge computing [19], the evaluated solutions were not we provide a detailed discussion of the various criteria and
intended for the DCCS. Also, runtime adaptations for stream scenarios used for evaluating the benchmarks, along with the
processing [20] have a long history, but they are designed experimental setup. Section V offers extensive results and
for static requirements, while SLOs, derived from a business discussions,alongwithasummaryoflimitationsandpotential
context, are an evolving system property. extensions. Finally, we conclude the paper in Section VI.
Meanwhile, Active Inference (AIF) [21]—a concept from
II. RELATEDWORK
neuroscience—isgainingsignificantattentionduetoitsability
to efficiently predict and adapt to changing conditions. AIF Although the merits of DCCS are openly discussed in
attempts to explain the behavior and learning of sentient crea- recent research [3], [27], the heterogeneity and dynamism of
tures;toraisethelevelofintelligenceinDCCS,AIFisalsoin- DCCS are open challenges. To enhance the rigor and repro-
creasinglyadoptedincomputerscience.Recentliterature[22], ducibility of our benchmarks, we compare AIF against three
[23] has shown that AIF agents can effectively ensure SLO well-established RL techniques, which serve as baseline ap-
compliance, maintain high QoS and QoE, and continuously proaches.Eachtechniqueoffersdistinctadvantagesinensuring
learn and adapt to dynamically changing environments and SLO compliance. In this section, we provide a brief overview
requirements. The promising results of AIF in DCCS inspire of how these methods were applied in the context of dynamic
further exploration and also draw our curiosity to evaluate SLO management and analyze their respective strengths and
its performance against Reinforcement Learning (RL)-based limitations. To effectively highlight the differences between
algorithms(DeepQ-Network(DQN)[24],A2C[25],andPPO AIF and RL-based approaches, we categorize the three RL
[26]),whichhaverecentlygainedpopularityanddemonstrated techniques—DQN, A2C, and PPO.
significant benefits across various applications.
A. Reinforcement Learning
Tothebestofourknowledge,wearethusthefirsttoprovide
Dynamic processing environments often suffer from fluc-
abenchmarkingsolutionfordynamicSLOcompliance.During
tuating workload patterns or multiple competing SLOs. To
our study, we found that many existing works are conducted
ensure SLO compliance under these circumstances, RL has
under simplified assumptions, lacking the complexity of real-
been applied for proactive orchestration, e.g., using A2C to
world application scenarios. Hence, we simulate realistic
adjust to client pattern [28] or using DQN to find a trade-
video conferencing applications to rigorously test the afore-
off between scaling actions [17]. Particularly for autoscaling,
mentioned algorithms, ensuring a comprehensive evaluation
DQN are applied by numerous researchers, e.g., for ensuring
of their performance and adaptability. In this context, our
high utilization in the Cloud [29] or adjusting the size of
contributions are threefold, as outlined below:
serverless containers [30]. However, the authors chose their
1) To evaluate algorithms in a realistic environment, we
respective RL algorithms based on expert knowledge and
implement a custom DCCS use case that contains (i) an
experience, i.e., not supported by empirical evidence.
edge device running a video conferencing application
Within the RL family, the three algorithms (i.e., DQN,
and (ii) a WebSocket server streaming videos to the
A2C, and PPO) have characteristic strengths and weaknesses
edge device. To ensure device SLOs, the server hosts
in terms of sample efficiency and stable convergence. As we
an intelligent agent that optimizes the number of video
will see later in the results, this proves critical at cold starts
streams, frames per second (FPS), and video resolution.
with few training samples or during distribution shifts.
2) We provide a benchmark for dynamic QoS and QoE
fulfillment in DCCS. This simplifies the design choice B. Active Inference
for stakeholders by providing insights into the different
Although AIF is not as widely applied for ensuring SLO
capabilities of AIF and common RL algorithms.
compliance, it has found its way from neuroscience, over
3) To further compare the algorithms’ robustness against
robotics, to computing systems [31]. In contrast to RL, the
dynamic changes in environment or system require-
challenge is not to maximize the expected reward but to
ments, we perform a series of experiments, where:
minimize free energy, a measure of the uncertainty in the
a) We introduce an instant distribution shift by sig- environment. More precisely, AIF agents must constantly
nificantly limiting network bandwidth. balance between actions that improve its understanding of
b) We introduce a gradual distribution shift by sim- the environment and such that ensure high pragmatic value,
ulating an overheating device. This shows if algo- i.e., fulfill SLOs. One option for creating a model of the
rithms can differentiate between dynamic system environment is to train knowledge graphs from observations,
evolution and environmental noise. as done in [22], [32]. While training these structures poses an
c) We change SLO thresholds to see whether algo- overhead, they improve trustworthiness because the behavior
rithms can dynamically adapt to new objectives. of agents can be traced empirically.
However, to the best of our knowledge, there exist no
...
scientific works that performed extensive evaluations between New
Connect configuration
RL and AIF techniques, which again leaves the choice with
the developer according to personal benefit. To support stake-
Stream
holders in making this design choice, the benchmarks created
Client Server Agent
in this paper will provide a profound idea of the advantages
Metrics
of each technique. In the following, we describe how these
different algorithms are incorporated into our methodology. Preprocessed
metrics
Configurations
III. METHODOLOGY
This section introduces an extensible benchmarking plat- Fig. 1: Overview of the Streaming Process
form designed to ensure SLO compliance in dynamic DCCS
environments. We begin by presenting a real-time video
conferencing use case, incorporating a realistic environment ofSLOsmustbefulfilled;incasetheyareviolated,theserver
setup. Then, we provide a comprehensive discussion on SLO can act by changing the streaming configuration.
composition, considering various quality metrics. Finally, we Metrics: To quantify SLO compliance, train the agent, and
detailtheimplementationofkeyalgorithms—AIF,DQN,A2C, inferthenextsystemconfigurations,theclientcollectsvarious
and PPO—while highlighting their primary hyperparameters. performance metrics, including CPU usage (M ), memory
CPU
usage (M ), throughput (M ), average latency (M ),
mem tp lat
A. Use Case
average render scale factor (M ) and thermal state (M ).
rs ts
We consider a real-time video communication service as a
Consider a system with dynamically changing configura-
case study to compare the algorithms in realistic conditions. tions, indexed by configuration timesteps c N+. At each
Our simulated environment has two components: ∈
configurationtimestepc,thereisasetofvideostreamsindexed
1) Client:Thisisavideo-conferencingapplicationthatruns by i 1,...,N . Each video stream i emits video frames
c
on an edge device (e.g., iPhone). By using the applica- index ∈ ed { by frame } timesteps t N+, where each frame
∈
tion, a user may join a conference with N participants, has a size (in bytes) denoted as b (t). These frames may
i
where each participant provides a video stream. Each be captured at different real-world timestamps τ (t). Every
i
video stream is characterized by resolution and FPS. second, for each configuration c applied during that time
2) Server: Provides a configurable video stream to clients. interval,theclientreceivesT setsofvideoframes,represented
c
ToensurehighQoSandQoEforclients,theserverhosts as F (t) i Tc . In practice, configurations do not change
{ i |∀ }t=1
an intelligent agent that continuously learns an optimal that frequently. Based on this setup, the average latency is
policy (i.e., streaming configuration) through one of the measured as shown in Eq. (1)
compared algorithms.
Streaming Process: Initially, the client connects to the (cid:88)
Tc
(cid:88)
Nc
M =α β (τ (t) τ (t 1)) (1)
server, which is set up with a default configuration and SLOs lat i − i −
t=2 i=1
(refer to Table II). The server begins to stream videos to the
client, as visualized in Fig. 1. While rendering the streams, whereα=1/(T 1)andβ =1/N .Wecalculatethroughput
c c
the client locally collects performance metrics and transmits as the average am − ount of data received over the time period
themtotheserveratfixedintervals.Theserverthenusesthese T , as shown in Eq. (2)
c
metrics to train its agent and infer a client configuration that
shouldimproveSLOcompliance.Theclientistheninstructed (cid:88) Tc (cid:88) Nc b i (t)
tooperatewiththisnewstreamingconfiguration.Thisprocess M tp =α β . (2)
τ (t) τ (t 1)
i i
is repeated throughout the entire lifetime of the client-server t=2 i=1 − −
connection.
The average render scale factor is given by:
It is important to note that while local decision-making
i
s
s
erv
p
e
re
r
fe
to
rre
a
d
c
,
ce
w
le
e
rat
i
e
mp
si
l
m
em
u
e
la
n
t
t
io
l
n
ea
e
r
x
n
p
in
e
g
rim
a
e
n
n
d
ts
in
th
fe
r
r
o
e
u
n
g
c
h
e
p
o
a
n
ral
t
l
h
e
e
l M =α (cid:88)
Tc
β (cid:88)
Nc (cid:115)
W i (t) × H i (t) (3)
rs
w (t) h (t)
execution in the cloud using pre-collected metrics and to t=2 i=1 i × i
enhance reproducibility. Additionally, server-side implemen-
where w (t) and h (t) are the pixel width and height of the
tation allows us to leverage stable and reliable libraries. i i
videostream,andW (t)andH (t)representthecorresponding
i i
B. SLO Composition dimensions of the rendered area on the client device screen at
Withtheusecaseset,itremainstodescribehowtheDCCS timestep t. The CPU usage is calculated according to Eq. (4).
application will be monitored and configured. For this, we
captureasetofmetricsthatgiveinsightsintotheperformance (cid:88)
Tc
U act (t)
M =α (4)
CPU
andefficiencyofthestreamingpipeline.Duringruntime,aset U
ref
t=2
TABLE I: Hyperparameters
whereU (t)istheactualCPUusageattimestept,andU
act ref
is the expected maximum CPU usage, which we set to 200%. Hyperparameter AIF DQN A2C PPO
Similarly, memory usage is computed as shown in Eq.(5) surprise threshold factor 2.0 – – –
weight of past data 0.6 – – –
(cid:88) Tc R act (t) initial additional surprise 1.0 – – –
M mem =α (5) graph max indegree 8 – – –
R
ref hill climb epsilon 1.0 – – –
t=2
input size 32 1 1 1
whereR act (t)denotesactualmemoryusageattimestept,and batch size 32 128 64 128
R the expected maximum memory usage, which we set to learning rate – 10−4 10−4 10−4
ref
exploration initial eps – 1.0 – –
200Mb. Finally, the thermal state of the device is determined
exploration final eps – 0.05 – –
as: exploration fraction – 0.1 – –
M = max Θ(t) (6) train freq – 4 – –
ts
t∈{2,...,Tc} grad steps – 4 – –
target update interval – 10000 – –
where Θ(t) 0,...,3 represents the device’s thermal state
∈{ } neurons – [128,128] [128,128] [64,64]
at timestep t, with 0 indicating a nominal state, 1 indicates gamma – 0.99 0.99 0.99
a fair state, 2 indicates a serious state and 3 representing a gae lambda – – 0.9 0.95
vf coef – – 0.75 0.25
critical state.
ent coef – – 0.01 0.01
To ensure high QoE and QoS, the intelligent agent is normalize advantage – – TRUE TRUE
continuously learning system configurations that comply with n steps – – – 1280
n epochs – – – 10
thefollowingSLOs.ForanymetricM ,wherexrepresentsa
x clip range – – – 0.2
placeholder for any metric type, variables Mmax and Mmin
x x
denote the upper and lower SLO thresholds, respectively. For
an overview of possible assignments, refer to Table II. Actions: To maximize SLO compliance, the server can
1) Averagerenderscalefactor(M r m s ax):Weaimtodisplay take action, namely, change the streaming configuration. The
streams of a sufficient resolution on a client device, en- respective policy—which configuration to choose to optimize
suring that M rs ≤ M r m s ax. Maintaining M rs within this SLOs—is learned over time. The system configuration itself
range enhances the streaming experience and ensures is characterized by the following streaming parameters:
that video content does not appear blurred.
1) Number of streams: The number of streams the client
2) Stream fulfillment (Mmin): To fulfill its purpose, the
sf receives and subsequently renders on the screen.
videoconferencingapplicationshouldshowaminimum
2) Resolution: Pixel dimensions of video streams.
number of streams from connected participants.
3) FPS: Frame rate of video streams.
3) Average latency (Mmax): To ensure high QoE, our
lat
services should provide a pleasant streaming latency for We constrain the parameters to the following possible
viewers. Therefore, we wish to maintain a sufficiently values: streams = 1,2,5,10,15,20 , resolution =
A { } A
low latency, averaged over all received video streams. 180,360,720 (equal widths and heights) and fps =
{ } A
4) Throughput (Mmax): High network usage drains the 5,10,15,20,25,30 . This forms an action (configuration)
tp { }
smartphonebatteryandputsexcessiveloadontheserver, space = streams resolution fps of size 108.
A A ×A ×A
and hence, may turn out costly. Therefore, we aim to
constrain the device’s throughput to a certain limit. C. Algorithm Implementations
5) Thermal state (Mmax): Excessive resource usage and a
ts
In this subsection, we discuss the implementations of the
hot environment may heat the device dangerously. Our
evaluated algorithms. To achieve stable performance and high
application should respond to it by adjusting the system
SLOcompliance,weoptimizetheirkeyhyperparametersusing
configuration to facilitate cooling.
a grid search approach; the selected values are summarized in
We calculate SLO-compliance level S [0,1] for any
x
∈ Table I. When multiple hyperparameter assignments showed
given metric M according to the following formula:
x
(cid:40) identical performance, we preferred the most efficient assign-
S = min(1,M x max/M x ), if M x max is defined (7) ment in terms of CPU consumption time and memory usage.
x min(1,M x /M x min), if M x min is defined 1) Active Inference: Our implementation of the AIF agent
is based on [22] with several adjustments, reflecting the
Furthermore, we calculate the SLO-compliance levels for
increasedcomplexityofourevaluationenvironmentandfacil-
QoE and QoS SLOs according to Eq. (8) and Eq. (9). Then,
itating fair comparison with RL methods. Specifically, we in-
we calculate the overall SLO compliance according to Eq.
(10). Each SLO-compliance level is in the range [0,1], with 1 troducechangestothemetricpre-processingandcomputation
and interpolation of a pragmatic value (pv) and risk assigned
being ideal.
S =(S +S )/2 (8) (ra). These values, together with information gain (ig), serve
QoE rs sf
as criteria for selecting the next system configuration.
S =(S +S +S )/3 (9)
QoS lat tp ts In the original approach, all metrics are discretized, with
S =(S +S )/2 (10) SLO-relatedmetricsbeingconvertedtobinaryvalues,accord-
QoE QoS
ing to Eq. (11). where µ(s˜) = min(1,0.2s˜+0.1) approximates a true SLO-
(cid:40) compliance value. This approach allows AIF to differentiate
1 if S =1
f(M )= x (11) more precisely between partially SLO-complying system con-
x
0 otherwise figurations, which is critical for fair comparison with RL.
Additionally, in Sedlak et al. [22], the configuration space
These discrete and binary values are used to learn the
is two-dimensional, and they artificially increase initial ig
structure and parameters of a generative model used by AIF.
values for key configurations to facilitate the interpolation of
Consequently, pv and ra are computed as a joint probability
pv and ra matrices for unexplored parameters. In our work,
of SLO compliance with respect to all QoE and QoS SLOs,
the configuration space is three-dimensional, therefore, we
accordingly. For example, let A be a random variable taking
use tensors. Similarly, we increase initial ig values for 8
values from the action space . Given a configuration a ,
A ∈A configurations,locatedinthecornersofthisthree-dimensional
pv and ra are computed according to Eq. (12) and Eq. (13),
space. These represent all possible combinations of 1,20
respectively. { }
streams, 180,720 resolutions and 5,30 FPS values.
{ } { }
pv a =P(S QoE =1A=a) (12) 2) ReinforcementLearning: ToopposeAIF,webenchmark
|
it with three well-established RL algorithms, namely, DQN,
ra =P(S =1A=a) (13)
a QoS
| A2C, and PPO. We use the implementations from Stable
Our baseline approach has certain limitations; for example, Baselines3 (SB3) [33], which are well-documented and stan-
it does not consider the partial SLO compliance in system dardized for this purpose.
configurations. Each SLO variable is in the continuous range We standardize continuous metrics (CPU usage, memory
[0,1] (refer to Eq. (7)), which is not captured by binary usage, throughput, average latency and average render scale
variables.Further,theremaybeahighprobabilitythatatleast factor) based on pre-collected metrics data (see Section IV),
one of the QoS and QoE SLOs is fulfilled, but this is ignored and one-hot encode a categorical metric (thermal state) and
when considering joint distribution only. These assumptions systemconfigurationparameters.Westacktogethertheresult-
lead the algorithm to treat both suboptimal and partially ingfeaturesintoavectorofsize24.Thisvectorcorrespondsto
compliant configurations equivalently. While this may suffice anobservationprocessedbyDQN,A2C,andPPOalgorithms.
for straightforward scenarios, it introduces considerable bias For each observation, we compute a reward , corresponding
R
in more complex environments, particularly when achieving to a SLO-compliance level for this observation as in Eq.
full SLO compliance (i.e., S =1) is infeasible. Therefore, we (10) (i.e., S). With such observations and rewards,
R ≡
introduce several modifications to the AIF implementation to RL algorithms have access to all available information and
enhanceitscompetitivenesswithRL,enablingittoeffectively can differentiate between partially complying configurations,
distinguishbetweenvaryinglevelsofSLOcompliancethrough which enables fair comparison with AIF.
reward mechanisms.
First, we continue discretizing non-SLO metrics, namely,
IV. EXPERIMENTALDESIGNANDTESTCASES
CPU time and memory usage. We partition these vari- We conduct a series of experiments to evaluate the perfor-
ables into a set of ordered, non-overlapping intervals mance and efficiency of the benchmark algorithms in terms
[0,0.2],(0.2,0.4],(0.4,0.6],(0.6,0.8],(0.8,+ ) and as- of SLO compliance within dynamic and uncertain DCCS
{ ∞ }
sign each observation a discrete value from 1,...,5 based environments. Initially, we pre-collect a dataset (refer to Data
{ }
on an interval it falls in. However, we also do a similar Availability) containing various system performance metrics
procedure with SLO-compliance levels, but with intervals gathered from controlled experimental conditions prior to
[0,0.2),[0.2,0.4),[0.4,0.6),[0.6,0.8),[0.8,1), 1 and la- simulation. This static dataset serves as the foundation for
{ { }}
bels 0,...,5 , respectively. In this case, we define this accurately modeling the dynamics of our environment. We
mapp { ing as th } e discretization operator : R N. This sequentially accumulate metrics for each possible system
D →
formulates pv and ra more precisely and approximates expec- configuration over several minutes in real time. The dataset
tations over S and S . We calculate the true expected used in experiments encompasses 512 records of metrics per
QoE QoS
SLO compliance given a configuration a through configuration. During experiments, when an agent selects a
∈A
1 (cid:88) (cid:90) 1 newconfiguration,wesampleabatchofcorrespondingmetrics
f (a)=E[S A=a]= s p(s a)ds (14) from the dataset as if they were collected in real time. This
x x i i i
| |
|S x |Si∈Sx 0 approach makes training and evaluation incomparably faster
and facilitates the reproducibility of results as we publish
where for pv we use f (a) with = S ,S and
QoE QoE sf rs
S { } our data. Our experiments are evaluated under both certain
for ra f (a) with = S ,S ,S , respectively.
QoS QoS lat tp ts
S { } and uncertain conditions, including basic preferences, instan-
However, as we discretize SLOs, calculating true expectation
taneous and gradual distribution shifts, and changing SLO
is impossible; therefore, we approximate this function as
requirements, which are detailed below.
shown in Eq. (15):
1) Basic Performance and Efficiency Evaluation: Initially,
1 (cid:88) (cid:88)
f˜(a)= µ(s˜)P(s˜a) (15) we perform a basic experiment where the algorithms operate
x
x | inregularenvironmentconditionsandarerequiredtomeetthe
|S |Si∈Sxs˜∈D(Si)
TABLE II: Experiment SLOs
a Newton’s law of cooling equation [35] to calculate the
Experiment M t m p ax M l m at ax M s m f in M r m s ax M t m s ax temperature at the next environment state T t+1 :
Basic 10Mb/s 1/15s 5 1.6 1
InstantShift 10Mb/s 1/15s 5 1.6 1 T =T∗+(T T∗) exp( k) (17)
t+1 t
GradualShift#1 10Mb/s 1/15s 5 1.6 1 − × −
GradualShift#2 10Mb/s 1/15s 5 1.6 1
ChangingSLOs#1 256Kb/s 1/30s 20 0.25 1 wherek representsthecoolingconstant,andwearbitrarilyset
ChangingSLOs#2 5Mb/s 1/15s 10 1.0 1 itsvalueto0.03or0.07toevaluatethebehaviorofalgorithms
under different temperature changing speeds, i.e., we conduct
this experiment twice with different k. Finally, we linearly
SLOs in Table II. During this experiment, we run algorithms
for 1.28 106 environment steps (RL processes individual map T t+1 to a discrete value Θ ∈ { 0,...,3 } to represent
× the device’s thermal state. We do this to make the simulation
observations, while AIF batches of 32), each 6,400 steps per-
closer to reality, as iOS exposes temperature as a similar
forming evaluation. Evaluation involves executing inference
discrete value.
deterministically, without learning in a separate copy of the
Similartothepreviousexperiment,wecontinuerunningthe
environment. The evaluation sequence is repeated eight times
simulation on the pre-trained models discussed in Subsection
at once, each time for 640 steps, starting from one of eight
IV-1. Thus, we can simulate dynamically heating/cooling the
corner positions in our action space, . This approach allows
A device and introduce strong temporal dependence into our
ustosummarizetheperformanceofthealgorithmswithmean
system to evaluate the proactivity of the algorithms.
and standard deviation across various starting points. In this
experiment, we evaluate the performance and efficiency of 4) Changing SLOs: In DCCS, devices may be exposed to
thealgorithmsbymeasuringSLOcompliance,CPUtime,and changing requirements, which require the intelligent agents
memory usage. to adapt according to the circumstances. To evaluate this, we
2) Instant Distribution Shift: DCCS are subject to unex- do exactly that—dynamically change SLOs. We conduct two
pected network spikes, dynamically changing topology, and similar experiments in which we modify the SLOs such that
variousfailures.Hence,itiscriticalthatalgorithmscandetect fullcompliancebecomesunattainable.Thedistinctionbetween
and handle such challenges. To assess the adaptable capa- thetwoexperimentsliesinthefeasibilityoftheobjectives:one
bilities of the algorithms, we conduct this experiment where experiment features objectives that are less feasible than the
a client is suddenly facing a significantly reduced network other (see Table II). Similarly to the previous two experiment
bandwidth (1 Mb/s). To simulate this, we implement this types, we continue this evaluation on the pre-trained models.
limitation in our video streaming server and pre-collect the
respective metrics. Then, we proceed with the training and A. Execution Setup
evaluation of the pre-trained models (building on the basic
We generate metrics datasets for experiments using an
preferencesandperformancemetricsdiscussedintheprevious
iPhone 16 simulator with iOS 18.2, available in Xcode IDE
subsection)usingthisnewdatasetwhilemaintainingthesame
(16.2), installed on MacBook M2 Pro with macOS 15.0.1.
SLOs as outlined in Table II.
The application runs in the foreground in portrait device
3) Gradual Distribution Shift: Often, environmental dy-
orientation, as shown in Fig. 1. We execute hyperparameter
namics may evolve more gradually. In such circumstances,
tuning and the experiments on a machine with two 32 core
an intelligent algorithm within DCCS should capture the tra-
Intel(R) Xeon(R) Gold 8358 CPU @ 2.6GHz with 512 GiB
jectory of system metrics and act accordingly. This proactive
RAM.
property—crucial for DCCS [6], [10], [34]—is inspected in
thisexperiment.Supposeauserisattendinganonlinemeeting
viaamobiledeviceandsuddenlytheyexposethedevicetoan V. RESULTSANDDISCUSSION
additionalworkload.Thisraisesthedevicetemperature,poten-
TocomparetheSLOcomplianceofRLandAIFalgorithms,
tially to a point that damages the device. To facilitate cooling,
we ran experiments simultaneously and collected the respec-
applications should reduce their resource consumption.
tive metrics. Recall that RL algorithms operate on individual
To simulate device heating, at each environment step, we
calculate a target temperature T∗, which represents the tem- observations, whereas AIF requires batches of 32. Hence, to
present the results of the uniform scale, we compute averages
perature to which the device tends to heat up. We calculate it
of subsequences of 32 metrics for each RL algorithm. To
as a function of throughput (as shown in Eq. (16)), as in our
cover distribution shifts during runtime, we decided to learn
case, it is a straightforward proxy to network, CPU, and other
and improve the streaming configuration continuously; this
resource utilization:
(cid:20) (cid:18) (cid:19)(cid:21) potentially presents an overhead, so we monitor CPU and
λM
T∗ =min 1,κ exp tp (16) memory utilization. However, we present SLO-compliance
× 1024 1024
× metrics from evaluation to avoid noise introduced by learning
where the coefficients κ = 0.364 and λ = 0.05 were chosen and exploration. The curves and transparent regions in SLO-
to decrease the number of optimal configurations yet remain compliance charts represent means and standard deviations
feasible for full SLO compliance. Then, we plug this into over batches of 32 observations from eight evaluation se-
quences performed. Means are computed according to Eq.
(18):
T N 1 (cid:88)(cid:88)
µ= S (t) (18) i T N
× t=1 i=1
our experiments assume N = 8 is a number of evaluation
sequences, T = 32 is a number of environment steps in a
batch and S (t) is a SLO-compliance level for the t-th step
i
of i-th evaluation. In turn, standard deviations are computed
according to the law of total variance:
(cid:112)
σ = E[Var(S T)]+Var(E[S T]) (19)
| |   2
T N N 1 (cid:88)(cid:88) 1 (cid:88)
=
TN
S
i
(t)
− N
S
j
(t)
t=1 i=1 j=1
T (cid:34) N T N (cid:35)21/2
1 (cid:88) 1 (cid:88) 1 (cid:88)(cid:88)
+ S i (t) S i (t)  .
T N − TN
t=1 i=1 t=1 i=1
For visual interpretability, we smooth the curves by averaging
over the last 15 batches. Additionally, since full SLO com-
pliance is infeasible in some experiments, we plot their lines
denoted by ”Exp.” that represent an average SLO compliance
of the most optimal configuration based on pre-collected
metrics used for the corresponding experiment.
A. Basic Preferences
Fig. 2 shows that all four algorithms were able to achieve
decently high SLO fulfillment rates under certain conditions.
Specifically, AIF showed remarkable sample efficiency by
convergingsignificantlyfasterthanotheralgorithms.However,
its solution is mildly suboptimal, which may be partially
attributed to environmental noise. In their turn, PPO and A2C
wereabletoconvergetoanoptimalconfigurationbutrequired
multiples of AIF training time, especially A2C.
On the contrary, DQN showed the lowest SLO compliance
and high instability. Although the initial evaluation cycles
1.0
0.9
0.8
0.7
0.6
0 1000 2000 3000 4000
Batch
ecnailpmoCOLS
104
103
5 102 2 3 × × 1 1 0 02 2
×102
2 101
×101
AIF DQN A2C PPO
Zoom
1.00
0.98
0.96 AIF A2C
DQN PPO
0 150 300
Fig.2:SLOComplianceduringBasicPerformanceEvaluation
)sm(emiTUPC
3000
2500
2000
1500
1000
0 10000 20000 30000 40000
Batch
(a)CPUtime
)bM(yromeM
AIF
DQN A2C
PPO
(b)Memoryusage
Fig. 3: Resource Utilization during Training
showed high performance, it progressively declined over time
until around batch 2300, where improvement was observed.
Overall, the results suggest that in the base case AIF, PPO
and A2C perform similarly, whereas DQN is distinguished by
its instability, sample-inefficiency, and subpar performance.
Intermsofefficiency,Fig.3ademonstratesthatAIFrequires
approximately equivalent CPU time as DQN on average, i.e.,
304msvs.289ms,respectively.However,thereareoccurrences
whereDQNdrasticallyexceedsthisvalue,reachingupto2.04s
perbatch.Incontrast,AIFdemonstratesnumerouscaseswhere
it uses substantially less CPU time, up to 15ms.
A2C and PPO utilize approximately 2.3–2.5 times less
CPU time than DQN and AIF on average, i.e., 121ms and
122ms,accordingly.However, theypresentconsiderablymore
outliers, reaching up to 6.39s for A2C and 1.13s for PPO per
batch. Although occasional violations of our CPU utilization
expectations in our setup are negligible, they can be critical
in some applications, and this should be considered when
choosing the right algorithm.
Overall, it is clear that the algorithms are comparable in
terms of CPU utilization, with A2C and PPO being more
efficient on average, though they occasionally introduce sig-
nificant overhead. In contrast, AIF uses surprisingly less CPU
time for some batches, demonstrating a more efficient use of
resources.
Regarding memory utilization, Fig. 3b suggests that all RL
algorithms exhibit similar memory utilization dynamics, close
to linear, with memory for DQN growing slightly faster than
others. In contrast, AIF showcases a vastly lower memory
footprint, which is rather logarithmic. The memory usage of
theAIFprocessincreasesbyonly372MBfromthebeginning
to the end of training, whereas the three RL methods exhibit
a significantly higher increase of 2.12 GB, 1.86 GB, and 1.90
GB for DQN, A2C, and PPO, respectively. Furthermore, by
the end of training, the AIF process requires approximately
2.1–2.3timeslessmemorythanRLmethods.ThismakesAIF
more advantageous for deploying to edge devices that have a
tightly limited memory capacity.
B. Distribution Shifts
1) Instant distribution shift: Fig. 4 shows that PPO and
A2C successfully detected the instant distribution shift and
actedaccordinglytoreachnearoptimalSLO-compliancelevel.
A2C converged slower but to a slightly better configuration.
1.05
1.00
0.95
0.90
0.85
0.80
0.75
0.70
0.65
0 1000 2000 3000 4000
Batch
ecnailpmoCOLS
tation at k = 0.07. Contrary to expectations, A2C displays
AIF A2C Exp. increased fluctuations following aconstant pattern, suggesting
DQN PPO Shift its inability to detect temperature changing trends. This be-
haviorimpliesthatdynamicallychangingdiscretetemperature
poses a challenge for A2C, and it is worth exploring alter-
native techniques, such as utilizing continuous temperature.
However, the reason for seemingly the same problem for AIF
is different—the current implementation lacks capabilities to
model relationships between consecutive states.
As a result, it cannot effectively predict the long-term con-
sequences of actions. This is also the reason why Fig. 5a–5c
show very similar curves for experiments with temperature—
AIF’s generative model simply cannot capture dependencies
oftemperatureonothervariables,so,fromAIFstandpointthe
datadistributiondidnotchange,withexceptionthatfulfillment
Fig. 4: SLO Compliance after an Instant Distribution Shift
of the thermal state SLO became virtually random. Therefore,
to be generalizable to such non-stationary systems, AIF im-
plementations should have the capacity to model transitions
Conversely,DQNdemonstratesadeclineinperformanceearly
between states and plan series of actions to achieve long-term
onbutcontinuestoimproveafterbatch2000,eventuallyreach-
SLO compliance.
ingSLOcomplianceclosetoPPO.Surprisingly,AIFperforms
poorly. According to our investigation, this is caused by an C. Changing SLOs
optimization employed to reduce its computational overhead Similarly to the case of instant distribution shift (Fig. 4), in
by limiting executions of structure and parameter learning of bothcasesofchangingSLOs(Fig.7aandFig.7b)AIFsuffers
the generative model in [22], the model’s parameters are re- thesameissue—gettingstuckwithasuboptimalconfiguration
learned only if c > ˜ 10 , where c is the surprise caused due to experiencing insufficient surprise to trigger model
by the current b
ℑ
atch of
ℑ
data and
˜ℑ
10 is the median surprise updates. However, relieving the limitations of model update
ℑ
over the last 10 batches; meanwhile, the model’s structure frequenciesshowsevenlessstableandperformantresults(Fig.
is re-learned if c > ˜ 10 h, where h is some factor. 5a), which again prompts the need for exploration of more
ℑ ℑ ×
While these constraints significantly accelerate execution in advancedtechniquesforhandlingdistributionshifts.Although,
the long run, they may also cause prolonged stagnation in a Fig. 5c shows some improvement (even with less stability)
suboptimal configuration—one that is sufficiently surprising in the case #1 of SLO change, this correlates with other
to be explored, yet not surprising enough to trigger model observations in Fig. 5c, where we see greater variance than
updates. in other figures, which stems from both, re-learning model
Another issue with the AIF implementation is the incapa- every batch and having a small buffer with observations.
bility to forget outdated experiences. Fig. 5a shows that AIF Nonetheless, AIF can maintain a high SLO-compliance level
converged to a more optimal configuration after pre-training in case #2 and, with greater stability, surpass DQN and even
on128,000insteadof1,280,000observations.Simultaneously, PPO and A2C until around the 600th and 2500th batch,
Fig.5bdemonstratesAIFbehaviorwhenre-learningthemodel respectively.
structure every batch. After the instant distribution shift, there DQNremainsconsistentwithpreviousexperiments,exhibit-
aremorefluctuationswithoutperformanceimprovements.This ing high variance, but simultaneously, its performance slowly
suggests that mixing new data distribution with the outdated improves, and it approaches near optimal SLO compliance.
one is not an effective strategy. Naively limiting the obser- Interestingly, both PPO and A2C initially remain on the
vation buffer does not contribute to improving results in this same SLO-compliance level and eventually can detect SLO
case (Fig. 5c and Fig 5d). change. In a simpler case (#2), both algorithms converge to a
2) Gradual distribution shifts: In this scenario, Fig. 6 near optimal configuration, and PPO converges significantly
shows that most algorithms struggle to effectively capture the faster. However, in case #1 A2C converges to an optimal
patternofthermalstatevariations.Increasingtheparameterk, configuration, while PPO improves insufficiently. However,
which corresponds to a faster update of thermal state, allows in case #2, both converge to a near optimal configuration,
RL algorithms to consider a greater number of temperature and PPO converges significantly faster. This may stem from a
changing cycles. Consequently, we expected higher k values higher stability of the PPO algorithm, influenced by methods
to improve stability and performance. As anticipated, DQN like gradient clipping and the Kullback-Leibler divergence
demonstrates better performance and greater stability when term,whichhindersitsexploration.Significantobjectiveshifts
k = 0.03 (Fig. 6a) compared to k = 0.07 (Fig. 6b), though couldcauselargergradientupdates,whichfosteredexploration
its overall performance remains suboptimal. PPO successfully in A2C, but in PPO, these gradients were clipped, and drastic
adapts to different settings but exhibits a more gradual adap- policy changes were penalized.
1.0
0.5
0.0
0 250 500 750
Batch
ecnailpmoCOLS
1.0
0.5
Inst.sft. SLOs#1
Grad.(k=.03) SLOs#2
Grad.(k=.07) Change
0.0
0 250 500 750
Batch
(a)largebuffer,h=2(default)
ecnailpmoCOLS
1.0
0.5
Inst.sft. SLOs#1
Grad.(k=.03) SLOs#2
Grad.(k=.07) Change
0.0
0 250 500 750
Batch
(b)largebuffer,h=0
ecnailpmoCOLS
1.0
0.5
Inst.sft. SLOs#1
Grad.(k=.03) SLOs#2
Grad.(k=.07) Change
0.0
0 250 500 750
Batch
(c)smallbuffer,h=2
ecnailpmoCOLS
Inst.sft. SLOs#1
Grad.(k=.03) SLOs#2
Grad.(k=.07) Change
(d)smallbuffer,h=0
Fig.5:SLOComplianceofAIFineachExperimentunderDifferentStructureLearningConditions(pre-trainedononly128,000
observations)
1.0
0.9
0.8
0.7
0 1000 2000 3000 4000
Batch
ecnailpmoCOLS
1.0
0.9
0.8
AIF A2C Exp.
DQN PPO Shift
0.7
0 1000 2000 3000 4000
Batch
(a)k=0.03
ecnailpmoCOLS
AIF A2C Exp.
DQN PPO Shift
(b)k=0.07
Fig. 6: SLO Compliance under Changing Thermal States
1.0
0.8
0.6
0.4
0 1000 2000 3000 4000
Batch
ecnailpmoCOLS
AIF A2C Exp. 1.0
DQN PPO Change
0.9
0.8
0.7
0 1000 2000 3000 4000
Batch
(a)Case#1
ecnailpmoCOLS
tive model and minimizing computational latency from
frequent learning to avoid hindering the exploration of
potentially better configurations.
3) AIF should more effectively utilize the accumulated
experience to minimize negative impact caused by dis-
crepanciesindistributionsofnewandpastobservations.
It is important to note that our study, particularly experi-
ments for efficiency comparison, are limited to concrete algo-
rithmimplementationsused.Otherlibrariesorimplementation
details may impact CPU and memory differently.
VI. CONCLUSION
AIF A2C Exp.
DQN PPO Change
ThispaperbenchmarkstheAIFmethodforSLOcompliance
inDCCSandcomparesitsperformancewithvariousRLalgo-
rithms,includingDQN,A2C,andPPO.Wefocusonadapting
to dynamic resource scaling and fluctuating workloads, which
often introduce performance and efficiency challenges. To
evaluatetheseapproaches,wesimulatearealisticvideoconfer-
(b)Case#2
encingapplicationonanedgedeviceandmonitorkeymetrics
Fig. 7: SLO Compliance after Changing SLOs such as latency and bandwidth, to ensure service quality
by adjusting stream parameters. The experiments incorporate
both instantaneous and gradual data shifts, such as network
D. Limitations and Future Work
limitations and device overheating, as well as SLO changes
Based on our observations, AIF remains a promising ap- to comprehensively assess the adaptability of each algorithm.
proach in the context of DCCS, demonstrating lower memory Our results indicate that PPO and A2C achieve high and
footprint than other algorithms, fair and predictable CPU uti- stable performance across various scenarios, whereas DQN
lization,andfastconvergencetonearoptimalSLO-compliance suffers from instability and sample inefficiency. Meanwhile,
level in our standard scenario. Although PPO and A2C al- AIFdemonstrateslimitedresourceconsumptionandthefastest
gorithms achieved high scores and stability in many cases, convergence in a stable scenario, making it a promising
AIF is also inherently explainable and, via the use of Markov approachforDCCS.Inthefuture,wewillfocusonenhancing
blankets, allows us to infer system configurations, discarding AIF for DCCS by planning a series of actions, balancing
irrelevant factors and thus accelerating inference speed. This generativemodelrelevancewithcomputationalefficiency,and
makesitattractiveforembeddingintohighlycomplexsystems exploiting accumulated experience to mitigate distribution
that require dependability. With that said, we identify several discrepancies in observations.
issues with the current AIF implementation for DCCS that
should be addressed in future work:
ACKNOWLEDGMENT
1) AIF should capture relationships between consecutive
observations and plan series of actions to predict ef- ThisworkispartiallyfundedbytheSvenskaInstitutetunder
fectively optimal system configurations in environments the ’SI Baltic Sea Neighbourhood Programme 2024’ (Project
where observations exhibit interdependencies. No. 31005669) and the Swedish Research Council (Project
2) AIF should more effectively balance updating a genera- No. 2024-04058).
DATAAVAILABILITY [17] B.Sedlak,A.Morichetta,P.Raith,V.C.Pujol,andS.Dustdar,“Towards
Multi-dimensionalElasticityforPervasiveStreamProcessingServices,”
Theimplementationofourserver,agents,andpre-collected
in 2025 IEEE International Conference on Pervasive Computing and
metrics are publicly available in the following GitHub repos- CommunicationsWorkshopsandotherAffiliatedEvents(PerComWork-
itory1. The code of our client application is also available at shops),2025.
the other repository2. [18] Z. Zhang, Y. Zhao, and J. Liu, “Octopus: SLO-Aware Progressive
Inference Serving via Deep Reinforcement Learning in Multi-tenant
REFERENCES EdgeCluster,”inService-OrientedComputing,Cham,2023.
[19] I. Cˇilic´, P. Krivic´, I. Podnar Zˇarko, and M. Kusˇek, “Performance
[1] D. Kimovski, N. Saurabh, M. Jansen, A. Aral, A. Al-Dulaimy, A. B.
Evaluation of Container Orchestration Tools in Edge Computing En-
Bondi, A. Galletta, A. V. Papadopoulos, A. Iosup, and R. Prodan,
vironments,”Sensors,vol.23,no.8,p.4008,Jan.2023.
“Beyond von neumann in the computing continuum: Architectures,
applications,andfuturedirections,”IEEEInternetComputing,vol.28, [20] V. Cardellini, F. Lo Presti, M. Nardelli, and G. R. Russo, “Runtime
no.3,pp.6–16,2024. AdaptationofDataStreamProcessingSystems:TheStateoftheArt,”
[2] J.L.King,“Centralizedversusdecentralizedcomputing:Organizational ACMComput.Surv.,vol.54,no.11s,pp.237:1–237:36,Sep.2022.
considerations and management options,” ACM Computing Surveys
[21] T.Parr,G.Pezzulo,andK.J.Friston,“Activeinference:Thefreeenergy
(CSUR),vol.15,no.4,pp.319–349,1983.
principleinmind,”Brain,andBehavior,vol.10,2022.
[3] P.K.Donta,I.Murturi,V.CasamayorPujol,B.Sedlak,andS.Dustdar,
“Exploringthepotentialofdistributedcomputingcontinuumsystems,” [22] B. Sedlak, V. C. Pujol, P. K. Donta, and S. Dustdar, “Equilibrium in
Computers,vol.12,no.10,p.198,2023. thecomputingcontinuumthroughactiveinference,”FutureGeneration
[4] T. Meuser, L. Love´n, M. Bhuyan, S. G. Patil, S. Dustdar, A. Aral, ComputerSystems,2024.
S. Bayhan, C. Becker, E. d. Lara, A. Y. Ding, J. Edinger, J. Gross,
[23] ——, “Active inference on the edge: A design study,” in 2024 IEEE
N. Mohan, A. D. Pimentel, E. Rivie`re, H. Schulzrinne, P. Simoens,
InternationalConferenceonPervasiveComputingandCommunications
G. Solmaz, and M. Welzl, “Revisiting Edge AI: Opportunities and
Workshops and other Affiliated Events (PerCom Workshops). IEEE,
challenges,”IEEEInternetComputing,vol.28,no.4,pp.49–59,2024.
2024,pp.550–555.
[5] P. Beckman, J. Dongarra, N. Ferrier, G. Fox, T. Moore, D. Reed, and
M. Beck, “Harnessing the computing continuum for programming our [24] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
world,”FogComputing:TheoryandPractice,pp.215–230,2020. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski
[6] S. Dustdar, V. C. Pujol, and P. K. Donta, “On distributed computing et al., “Human-level control through deep reinforcement learning,”
continuumsystems,”IEEETransactionsonKnowledgeandDataEngi- nature,vol.518,no.7540,pp.529–533,2015.
neering,vol.35,no.4,pp.4092–4105,2022.
[25] V. Mnih, “Asynchronous methods for deep reinforcement learning,”
[7] P. K. Donta, B. Sedlak, V. Casamayor Pujol, and S. Dustdar, “Gover-
arXivpreprintarXiv:1602.01783,2016.
nance and sustainability of distributed continuum systems: A big data
approach,”JournalofBigData,vol.10,no.1,p.53,2023. [26] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
[8] V. C. Pujol, B. Sedlak, P. K. Donta, and S. Dustdar, “On causality in “Proximalpolicyoptimizationalgorithms,”arXiv:1707.06347,2017.
distributedcontinuumsystems,”IEEEInternetComputing,2024.
[27] V. Cardellini, P. Dazzi, G. Mencagli, M. Nardelli, and M. Torquati,
[9] V. Casamayor Pujol, A. Morichetta, I. Murturi, P. Kumar Donta, and
“Scalable compute continuum,” Future Generation Computer Systems,
S.Dustdar,“Fundamentalresearchchallengesfordistributedcomputing
vol.166,p.107697,May2025.
continuumsystems,”Information,vol.14,no.3,p.198,2023.
[10] V. C. Pujol, P. K. Donta, A. Morichetta, I. Murturi, and S. Dustdar, [28] H.Qiu,S.S.Banerjee,S.Jha,Z.T.Kalbarczyk,andR.K.Iyer,“FIRM:
“Edge intelligence—research opportunities for distributed computing AnIntelligentFine-grainedResourceManagementFrameworkforSLO-
continuumsystems,”IEEEInternetComputing,vol.27,no.4,2023. Oriented Microservices,” in 14th USENIX Symposium on Operating
[11] P.K.Donta,B.Sedlak,I.Murturi,V.Casamayor-Pujol,andS.Dustdar, SystemsDesignandImplementation(OSDI20),2020,pp.805–825.
“Human-based distributed intelligence in computing continuum sys-
[29] S. Yalles, M. Handaoui, J.-E. Dartois, O. Barais, L. d’Orazio, and
tems,”IEEEInternetComputing,vol.29,no.2,092025.
J. Boukhobza, “RISCLESS: A Reinforcement Learning Strategy to
[12] Z.Zhao,Y.Hu,G.Yang,Z.Gong,C.Shen,L.Zhao,W.Li,X.Liu,and
Guarantee SLA on Cloud Ephemeral and Stable Resources,” in 2022
W.Qu,“SLOpt:Servingreal-timeinferencepipelinewithstrictlatency
30th Euromicro International Conference on Parallel, Distributed and
constraint,”IEEETransactionsonComputers,pp.1–14,2025.
Network-basedProcessing(PDP),Mar.2022,pp.83–87.
[13] B.Sedlak,V.C.Pujol,P.K.Donta,andS.Dustdar,“Diffusinghigh-level
SLOinmicroservicepipelines,”in2024IEEEInternationalConference [30] N. Filinis, I. Tzanettis, D. Spatharakis, E. Fotopoulou, I. Dimolitsas,
onService-OrientedSystemEngineering(SOSE). IEEE,2024. A. Zafeiropoulos, C. Vassilakis, and S. Papavassiliou, “Intent-driven
[14] J.Lin,M.Li,S.Q.Zhang,andA.Leon-Garcia,“Murmuration:On-the- orchestration of serverless applications in the computing continuum,”
flydnnadaptationforslo-awaredistributedinferenceindynamicedge FutureGenerationComputerSystems,May2024.
environments,”inProceedingsofthe53rdInternationalConferenceon
[31] D. Vyas, M. d. Prado, and T. Verbelen, “Towards smart and adaptive
ParallelProcessing,2024,pp.792–801.
agentsforactivesensingonedgedevices,”Jan.2025.
[15] V. Casamayor Pujol, B. Sedlak, Y. Xu, P. K. Donta, and S. Dustdar,
“Deepslos for the computing continuum,” in Proceedings of the 2024 [32] B. Sedlak, V. C. Pujol, A. Morichetta, P. K. Donta, and S. Dustdar,
WorkshoponAdvancedTools,ProgrammingLanguages,andPLatforms “Adaptivestreamprocessingonedgedevicesthroughactiveinference,”
for Implementing and Evaluating algorithms for Distributed systems, arXivpreprintarXiv:2409.17937,2024.
2024,pp.1–10.
[33] A.Raffin,A.Hill,A.Gleave,A.Kanervisto,M.Ernestus,andN.Dor-
[16] H. Kokkonen, L. Love´n, N. H. Motlagh, A. Kumar, J. Partala,
mann,“Stable-baselines3:Reliablereinforcementlearningimplementa-
T. Nguyen, V. C. Pujol, P. Kostakos, T. Leppa¨nen, A. Gonza´lez-
tions,”JournalofMachineLearningResearch,vol.22,no.268,2021.
Gil et al., “Autonomy and intelligence in the computing continuum:
Challenges, enablers, and future directions for orchestration,” arXiv [34] A.Morichetta,V.C.Pujol,andS.Dustdar,“Aroadmaponlearningand
preprintarXiv:2205.01423,2022. reasoning for distributed computing continuum ecosystems,” in 2021
IEEEInternationalConferenceonEdgeComputing(EDGE),2021.
1https://github.com/AlfredsLapkovskis/VideoStreamEnv.git [35] R. Winterton, “Newton’s law of cooling,” Contemporary Physics,
2https://github.com/AlfredsLapkovskis/SmartVideoStream.git vol.40,no.3,pp.205–212,1999.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Benchmarking Dynamic SLO Compliance in Distributed Computing Continuum Systems"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
