=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: A Theory of Human-Like Few-Shot Learning
Citation Key: jiang2023theory
Authors: Zhiying Jiang, Rui Wang, Dongbo Bu

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2023

Key Terms: theory, learn, deep, recognition, learning, species, shot, like, human, under

=== FULL PAPER TEXT ===

A Theory of Human-Like Few-Shot Learning
Zhiying Jiang1, Rui Wang2, Dongbo Bu2, Ming Li1∗
1DavidCheritonSchoolofComputerScience,UniversityofWaterloo,
200UniversityAveW,Waterloo,ONN2L3G1,Canada
2InstituteofComputingTechnology,ChineseAcademyofScience,Beijing,China
∗Towhomcorrespondenceshouldbeaddressed;E-mail: mli@uwaterloo.ca
Weaimtobridgethegapbetweenourcommon-sensefew-samplehumanlearn-
ing and large-data machine learning. We derive a theory of human-like few-
shotlearningfromvon-Neuman-Landauer’sprinciple. Modellinghumanlearn-
ing is difficult as how people learn varies from one to another. Under com-
monlyaccepteddefinitions,weprovethatallhumanoranimalfew-shotlearn-
ing,andmajormodelsincludingFreeEnergyPrincipleandBayesianProgram
Learning that model such learning, approximate our theory, under Church-
Turingthesis. Wefindthatdeepgenerativemodellikevariationalautoencoder
(VAE)canbeusedtoapproximateourtheoryandperformsignificantlybetter
than baseline models including deep neural networks, for image recognition,
lowresourcelanguageprocessing,andcharacterrecognition.
Introduction
During the past decade, fast progress in deep learning (1) has empowered computer speech
recognition,imageprocessing,naturallanguageprocessing,proteinfolding,gameplayingand
manyotherapplications. However,thesegreatprogressesfellshort whenwetrytounderstand
ourownlearningmechanism: Howtomodelhumanlearning(2),(3),(4)?
Species in nature learn quickly to survive. When a dragonfly is hatched, within hours it
firmsupitswingsandthenfliestocatchmosquitoes;anewborndoesnotneedtonsofrepeated
examples or transfer learning to identify an apple. Most human or animal learning exhibits a
mixture of inherited intelligence, few-shot learning without prior knowledge, as well as long
term many-shot learning. It is interesting to note that these learning programs are encoded in
our genomes but they are not all the same, even for individuals within the same species. The
diversityoftheselearningalgorithmsisvividlyexpressedbySpearman’s"g"factor(2).
Workinprogress.
1
3202
naJ
3
]GL.sc[
1v74010.1032:viXra
Unlikedata-laden,model-heavy,andenergy-hungrydeeplearningapproaches,mosthuman
learning appear to be simple and easy. Merelyscaling up current deep learning approaches may
not be sufficient for achieving human level intelligence. We miss certain major components
whenmodellinghumanoranimallearning.
Diversity is one of the missing part when modelling human or animal few-shot learning.
Thereare eightbillion peopleon earth,each withaunique few-shotlearningmodel (5). Evenif
wejustwanttomodeloneperson,asinglepersonoftenusesdifferentparameters,features,and
perhapsdifferent algorithmsto dealwithdifferent learningtasks. Ideally wewant aframework
that can cover the diversity in human and animal few-shot learning. Facing such a seemingly
formidabletask,traditionalthinkinginmachinelearningwillonlyleadustovarioustraps. To
avoidsuchtrapsweneedtogobacktotheveryfirstprinciplesofphysics.
Specifically,westartfromanagreed-uponlawinthermodynamics,toformallyderiveour
model for few-shot learning, and prove this is the optimalmodel within our framework in the
sense that all other models including human ones may be viewed as approximations to our
framework. Weshowadeepconnectionbetweenourframeworkandthefreeenergyprinciple(3)
andtheBayesianProgramLearningmodel(4). Bytheendofthisprocess,acomponentofdata
compressionduringtheinferencephaseoflearningemergesasakeycomponentofallfew-shot
learningmodels.
First, we formalize our intuitive and commonly accepted concept of human-like few-shot
learning. For example, our definition below is consistent with what is used in (4), and in the
samespiritof(3).
Definition 1. Consider a universe Ω, partitioned into H disjoint concept classes: C , h =
h
1,2,...,H. Few-shot(k-shot)learningisdescribedasfollows:
1. nelementsinoroutsideΩaregivenasunlabelledsamplesy ,...,y ;
1 n
2. Therearek labelledexamplesforeachclassC ,forsmallk;
h
3. Thelearningprogram,usingacomputablemetricM,few-shotlearnsC ,h = 1,2,...H,
h
if it uses the n unlabelled samples and k labelled samples and minimizes the objective
function:
H |C h |
(cid:88)(cid:88)
M(x ,core ) | y ,...,y ,x ∈ C ,
i h 1 n i h
h=1 i=1
where core = ψ(k samplesof C ) representing a transformed representation of the k
h h
labelledsamplesfromC .
h
This definition covers most of our common sense few-shot learning scenarios and other
studies. Inparticular,thisisusedinone-shotlearningby(4). Aseachindependentindividual,
we do not all use a same metric, or even similar metric, to few-shot learning. For example,
MN Hebart et al (6) identified 49 highly reproducible dimensions to 1854 objects to measure
2
theirsimilarity. Differentpeoplecanbeequippedtobetterobservesomeofthesedimensional
features.
We explain the intuition behind Definition 1 via a simple example. A human toddler may
havealready seenmanyunlabelledsamplesoffruitswhich,forexample,containstwoclasses:
applesandpears. Then givenanewlabelled sample fromeachclass,thetoddlerlearnshow to
differentiatebetweenthesetwofruits. Thenumberoflabelleddatarequiredforonetoclassify
mayvaryaspeoplehavedifferentlearningalgorithms.
Currentdeep learningbasedapproachesfor few-shotlearninggenerallydepend on1)many
auxiliary labelled training samples or task-specific data augmentation for transfer learning or
metalearning(7);or2)verylargescaleself-supervisedpre-training(8). Theseapproachesthus
fallshorttomodelfew-shotlearninginnaturebyhumansandanimalsastheycanhardlyaccount
for the diversity in learning algorithms and they either neglect the unsupervised scenario that
humansaremostlyexposedtoorusethescaleofunlabelleddataandtrainingparametersthat
arefarbeyondcreaturesneed.
Many attemptshave beenmade to understand human learning through cognitive, biological,
andbehaviorsciences. Somestudieshaveestablishedbasicprinciplesahumanlearningmodel
shouldobey. Onetheoryisthetwo-factortheoryofintelligencebyCharlesSpearmanin1904(2),
wherethe “g”factorisan indicatoroftheoverall cognitiveability, andthe“s”factor standsfor
theaptitudethatapersonpossessesinspecificareas. As“g”factorisgenetically-related(9),it
indicatesthenecessityofalearningtheorythatcanaccountforthediversityincreatures’learning
ability. Another theory is the Free Energy Principle by Karl Friston (3) that human (and all
biologicalsystems)learningtendstominimizethefreeenergybetweeninternalunderstandingin
thesenseofBayesian(underinternalperceiveddistributionp)andthatoftheenvironmentalevent
(underdistributionq),measuredbyKL-divergence(10). Inasimilarspirit,Lake,Salakhutdinov
andTenenbaum(4)proposedaBayesianprogramlearning(BPL)model,learningaprobabilistic
modelforeachconceptandachievehuman-levelperformance. TwoarticlesbySchmidhuber(11)
andbyChaterandVitanyi(12)linkedsimplicitytohumancognitionandappreciationofarts.
Instead of exploring a biological basis for few-shot learning, we think it is possible to
mathematically derive an optimal framework that can unify the above theories. We further
demonstrate by experiments that our new model indeed works significantly better than other
classicaldeeplearningneuralnetworksforfew-shotlearning. Asabyproductofournewmodel,
a new concept class of "interestingness" is learned; this class implies where our appreciation
of art, music, science and games comes from. Extending this observation, some aspects of
consciousness may be modelled as a set of few-shot learned concepts. Consequently, we
hypothesizetheabilityoflabellinginputdatabecomesakeysteptoacquiringsomeaspectsof
consciousness.
3
A theory of few-shot learning
Wemathematicallyderiveanoptimalfew-shotlearningmodelforDefinition1thatiseffective
andisabletocoverenormousdiversitiesexistedindifferentspecies. Thetaskmayappeartobe
formidablebecauseofconflictingandseeminglyverygeneralgoals: eachindividualisallowed
to have a different learning model, yet our model has just one program to model everybody;
we do not yet exactly know the complete underlying biological mechanisms, yet we need to
implement the right functionality; there are infinite number of models, yet we need to choose
onethatisoptimal;wearenotreallyinterestedin"proposingmodels"outofblue,yetwewish
our model to be a mathematicalconsequence of some basic laws of physics; the model needs to
betheoreticallysound,yetpracticallyuseful.
Forsimplicityandreadability,we beginwithone-shotlearning, k = 1inDefinition1. Thus,
core inDefinition1isjustthesinglelabelledsamplex . Forlargerk,core canbesomeform
h h h
ofaverageofthek samples. AsDefinition1defined,someunlabelledobjectsareassumedand
it’salsopossibletoextendthedefinitionbyaddingdistribution,learntfromeitherunlabelledor
labelleddata,toΩ. UsingmetricMthatisresponsiblefork-shotlearningofanindividual,the
learningsystemseekstominimizetheenergyfunction
H |C h |
(cid:88)(cid:88)
M(x ,x |y ,...,y ),
i h 1 n
h=1 i=1
or, assuming H(y ,...,y ) is a pre-trained model of y ,...,y , or other labelled samples,
1 n 1 n
capturingthedistribution.
H |C h |
(cid:88)(cid:88)
M(x ,x |H(y ,...,y )),
i h 1 n
h=1 i=1
Nowthequestionis,whatsortofMshouldweuse? Indeed,thisvariesfrompersontoperson.
Canweunifyallsuchmeasures,algorithmsandinferences? Let’sgobacktothefundamentals.
Principle1(von-Neuman-LandauerPrinciple). Irreversiblyprocessing1bitofinformationcosts
1kT;reversiblecomputationisfree.
Then for two objects x,y, the minimum energy needed to convert between x and y in our
brainis:
E (x,y) = min{|p| : U(x,p) = y,U(y,p) = x},
U
whereU is auniversalTuring machine or our brain, assumingChurch-Turingthesis. Since we
canproveatheoremshowingallUniversalTuringmachinesareequivalentmoduloaconstantand
efficiency,wewilldroptheindexU (see(13)). Tointerpret,E(x,y)isthelengthoftheshortest
program that reversibly converts between x and y. These bits used in the shortest program p
when they are erased will cost |p|kT of energy, according to the John von Neuman and Rolf
Landuaer’slaw. Thisleadsustoafundamentaltheorem(14):
4
... degree
... degree
Figure1: BipartiteGraph
Theorem1. E(x,y) = max{K(x|y),K(y|x)}+O(1).
K(x|y)isthe Kolmogorov complexityofxgiveny, orinformally, thelength ofthe shortest
program that outputs x given input y (details are shown in (13)). As this theorem was proved
thirtyyearsagoanditisvitalinourtheory,tohelpourreaders,wewillprovideanintuitivebut
lessformalproofhere.
Proof. By the definition of E(x,y), it follows E(x,y) ≥ K(x|y) and E(x,y) ≥ K(y|x), thus
wehaveE(x,y) ≥ max{K(x|y),K(y|x)}.
To prove the other direction E(x,y) ≤ max{K(x|y),K(y|x)}, we need to construct a
program p such that p outputs y on input x and p outputs x on input y, and length of p is
boundedbymax{K(x|y),K(y|x)}+O(1).
Let k = K(x|y), and k = K(y|x). Without loss of generality, assume k ≤ k . We first
1 2 1 2
defineabipartitegraph{X,Y,E},whereX,Y = {0,1}∗,asshowninFigure1andE isafinite
setofedgesdefinedbetweenX andY asfollows:
E = {{u,v},u ∈ X,v ∈ Y,K(u|v) ≤ k ,K(v|u) ≤ k }
1 2
Notethataparticularedge(x,y)isinE. Ifwefindedge(x,y),thengivenx,pcanoutputy,
andviceversa. SotheideaoftheproofistopartitionE properlysothatwecanidentify(x,y)
easily. Two edges are disjoint if they do not share nodes on either end. A matching in graph
theoryisasetofdisjointedgesinE.
Claim. E canbepartitionedintoatmost2k2+2 matchings.
ProofofClaim. Consideredge(u,v) ∈ E. Thedegreeofanodeu ∈ X isboundedby2k2+1
becausethereareatmost2k2+1 differentstringsv suchthatK(v|u) ≤ k ,accumulatingpossible
2
strings from i = 1 to i = k gives us (cid:80)i=k2 = 2k2+1 − 2. Hence u belongs to at most 2k2+1
2 i=1
matchings. Similarly,node v ∈ Y belongsto atmost2k1+1 matchings. Wejustneed toputedge
(u,v)inanunusedmatching. (EndofProofofClaim)
LetM bethematchingthatcontainsedge(x,y)Wenowconstructourprogramp. poperates
i
asfollows:
• Generate M following the proof of Claim, i.e. enumerating the matchings. This uses
i
informationk ,k ,andi. K(i) ≤ k +O(1)
1 2 2
5
• Givenx,pusesM tooutputy,andgiveny,pusesM tooutputx.
i i
A conditional version of Theorem 1, using information in Definition 1, can be obtained
E(x,y|y ,...,y ) = max{K(x|y,y ,...,y ),K(y|x,y ,...,y )},conditioningonunlabelled
1 n 1 n 1 n
samplesy ,...,y . Accordingto(14),thisdistanceisuniversal,inthesensethatE(x,y)isthe
1 n
minimumamonganyothercomputabledistances:
Theorem2. ForanycomputablemetricD,thereisaconstantc,suchthatforallx,y,E(x,y) ≤
D(x,y)+c.
This theorem implies: if D metric finds some similarity between x and y, so will E. Thus,
theabovetheoremimplies,uptosomeconstantO(H)
H |C h | H |C h |
(cid:88)(cid:88) (cid:88)(cid:88)
E(x ∈ C ,core |y ,...,y ) ≤ M(x ∈ C ,core |y ,...,y ).
i h h 1 n i h h 1 n
h=1 i=1 h=1 i=1
Whenunlabelledsamplesy ,...,y plusotherirrelevanthistoricallabelledsamplesaremodeled
1 n
by some model H such as a generative model (e.g., VAE), then the above inequality can be
rewrittenas:
H |C h | H |C h |
(cid:88)(cid:88) (cid:88)(cid:88)
E(x ∈ C ,core |H) ≤ M(x ∈ C ,core |H). (1)
i h h i h h
h=1 i=1 h=1 i=1
Thus, E gives optimal metric for few-shot learning algorithm. Other algorithms satisfied
Definition1aretheapproximationtothisoptimalsolution. 1
In addition, we show that our theory’s deep connection to two well-established principles
oflearning inneuroscience andpsychology. Friston’s FreeEnergy Principle(FEP)(3), derived
fromBayesianbrainhypothesis(15),statesthatbrainseekstominimizesurprises. Specifically,
it assumes the brain has its internal state (a.k.a. generative model) that implicitly models the
environment according to the sensory data. Hidden (latent) variables need to be defined for
the internal state, which are drawn from prior beliefs. Ideally, these prior knowledge is also
modelled,whichismadepossiblebyhierarchicalgenerativemodels. Thefreeenergyprinciple
(FEP) is often interpreted as Bayesian optimization, using the Evidence LowerBound (ELBO)
asELBO = logp(x;θ)−D(q(z)(cid:107)p(z|x;θ)optimizationfunction. Heretheevidencelogp(x;θ)
is the encoding length ofx under probabilityp, and the Kullback-Leibler divergence term is the
p-expectedencodinglengthdifference. ThisishalfofTheorem1andFEPisasymmetricifwe
viewit asadistance. However,thesymmetry isimportanttofew-shot learning. For example,a
scarletkingsnake maylooklikeacoral snake,butthelatter certainly hasmoredeadlyfeatures
theformerlacks, onewaycompression K(ScarletKingSnake|CoralSnake)isnotsufficient to
1NotethatE isametric: itissymmetric,andsatisfiestriangleinequality
6
Test Instance
Compressor
Unlabeled Data Distribution
Figure2: Illustrationofourframework,dashedlineindicatesoptionalcomponentwhenlearning.
distinguishthetwo. DespiteofthefactH.influnzawithgenomesize1.8millionandE.coliwith
genomesize5million theyaresisterspeciesbutE.coli wouldbemuchcloser toaspecieswith
zerogenomeG orjustacovid-19genomewiththisasymmetricmeasure(K(G |E.coli)than
0 0
withH.influnza(K(H.influnza|E.coli)). AsymmetricinterpretationofFriston’sFEPcanbe
derivedbyrequiringminimumconversionenergyasweshowinTheorem1.
Different individuals may use different compression algorithms to do data abstraction and
inference. ItcanbeviewedthatthesealgorithmsallapproximateE(x,y). Somearemoreefficient
than others in different situations. The individuals with better compression algorithms have
bigger“g”factor. Diversifiedcompressionalgorithmsalsoguaranteebettersurvivalchancesofa
community when facing apandemic. As compression neural networks are genetically encoded,
the “g” factor is thus inheritable. This can be seen via Figure 2, compression algorithms vary
from one to another. The distribution of the data to be learnt is either implicitly or explicitly
capturedbycreatures. Thosewhocanbetterutilizeunlabelleddatatocapturedistributionmay
haveamoreefficientcompressionalgorithm.
Experimental Results
Image Experiments
To approximate our universal few-shot learning model, we use a hierarchical VAE as our
underlyingmodelHinInequality1tomodeltheunlabelledsamplesy ,...,y . Thishierarchical
1 n
structure coincides with our visual cortex and brain structure (16). According to integrated
information theory (17), an input y may come from all sensing terminals: vision, hearing,
smell, taste, sensation. Often, creatures are exposed to an unsupervised environment where
objects are unknown and unlabelled. Revisiting the negative ELBO, we can see it can be
interpreted as changing perceptions to minimize discrepancy (minimize KL divergence) or
changing observations to maximize evidence, in the context of FEP. When the creatures are
7
exposed to a “tree” and they do not fully realize what it is, the sensory information of the
objectsare internalizedwithhidden states(innerbelief) thatcan describeshow itbelievesthe
generation process of a “tree”. This process of generation, helps the creatures to identify the
latent similarities among objects that belong to the same category, without the full awareness.
This process of "unconsciously" training to generate helps the creatures to better categorize in
future. Whentheidentityofa“tree”isfinallyrevealed,theycangeneralizequickly. Thisexplains
our rationale of using a VAE to process unlabelled samples. Consequently, the Kolmogorov
complexitytermsinInequality1arenaturallyapproximatedbyaVAEbasedcompressor(18).
To test the hypothesis, we carry out the experiment on five datasets, MNIST, KMNIST,
FashionMNIST, STL-10 and CIFAR-10. We first train a hierarchical VAE on unlabelled data
to learn to generate xˆ that’s as close to x as possible. This corresponds to the time when
creatures exposed to a environment without knowing the object, implicitly learning the latent
representationamongobjects. Whentheidentityofobjectsarerevealed,aVAEbaseduniversal
compressor can be used to identify the new objects. Specifically, after training a hierarchical
VAE unsupervisedly, we compare the E energy function between a labelled image and a test
image, as in Definition 1. In our experiment, we use 5 labelled samples per class to test the
accuracyofclassification. TheenergyfunctionE reliesonacompressortoapproximate. Wethus
usethebits-backargumenttodirectlyuseourtrainedVAEforthecompressorin(18). Ourresult
shows that using only 5 samples, our method outperforms traditional supervised models like
SVM,CNN,VGGandVisionTransformer(ViT)onallfivedatasets. Thesesupervisedmethods
are chosen to represent different model complexity with wide range of number of parameters.
As we can see, when labelled data are scarce, supervised methods are not effective: complex
models like VGG cannot perform better than SVM and this tendency is more obvious on ViT
withoutpre-training. Theimprovementthatourmethodbringsismoreobviousonmorecomplex
datasetslikeSTL-10and CIFAR-10. Similarresult isalsoobtained inthe recentwork, across
differentshotsettings(19).
Wealsocomparewithusinglatentrepresentationdirectlywithk-Nearest-Neighborclassifier,
labelled as“Latent” in thetable. The architectureand training procedurefor “Latent” method
is exactly the same to our method — we train on unlabelled data to generate the sample and
then take the latent representation for classification. We can see using latent representation
outperforms all supervised methods on four out of five datasets. But the accuracy is still way
lowerthanourmethod,indicatingourmethodcanbetterutilizethegenerativemodels.
Text Experiments
Our theory is generally applicable, even without pre-training on unlabelled data. Here, we
demonstratesignificantadvantagesofourapproachwithasimplecompressorgzipoverlower
resourcelanguages.
Languages with Abundant Resources We first test our method on datasets with abundant
resources. Specifically,wecomparewiththreedatasets—AGNews,SogouNewsandDBpedia.
8
MNIST KMNIST FashionMNIST STL-10 CIFAR-10
SVM 69.4±2.2 40.3±3.6 67.1±2.1 21.3±2.8 21.1±1.9
CNN 72.4±3.5 41.2±1.9 67.4±1.9 24.8±1.5 23.4±2.9
VGG 69.4±5.7 36.4±4.7 62.8±4.1 20.6±2.0 22.2±1.6
ViT(disc) 58.8±4.6 35.8±4.1 61.5±2.2 24.2±2.5 22.3±1.8
Latent 73.6±3.1 48.1±3.3 69.5±3.5 31.5±3.7 22.2±1.6
Ours 77.6±0.4 55.4±4.3 74.1±3.2 39.6±3.1 35.3±2.9
Table1: 5-shotimageclassificationaccuracyonfivedatasets.
AGNews SogouNews DBpedia
fasttext 27.3±2.1 54.5±5.3 47.5±4.1
Bi-LSTM+Attn 26.9±2.2 53.4±4.2 50.6±4.1
HAN 27.4±2.4 42.5±7.2 35.0±1.2
W2V 38.8±18.6 14.4±0.5 32.5±11.3
BERT 80.3±2.6 22.1±4.1 96.4±4.1
Ours 58.7±4.8 64.9±6.1 62.2±2.2
Table2: 5-shottextclassificationaccuracyonthreedatasets.
Similartoimageclassification,wecomparewithbothsupervisedmethods,includingfasttext(20),
BiLSTM(21)withattentionmechanism(22)andHierarchicalAttentionNetwork(HAN)(23),
andnon-parametricmethods thatuseWord2Vec(W2V)(24)asrepresentation. We alsocompare
withpre-trainedlanguagemodelslikeBERT(25)Weusefivelabelleddataforeachclass(5-shot)
forallthemethods.
Surprisingly, even without any pre-training and with a simple compressor like gzip, our
method outperforms all non-pretrained supervised methods and non-parametric methods in
low data regime. This indicates that compressor serves as an efficient method to capture the
regularity and our information distance is effective in comparing the similarity based on the
essential information. When comparing with pre-trained models like BERT, we can see our
methodissignificantlyhigheronSogouNews,aspecialdatasetthatincludesPinyin—aphonetic
romanization of Chinese, which can be viewed as an Out-Of-Distributed (OOD) dataset as it
usesthesamealphabetasenglishcorpus.
Low-ResourceLanguages Sufficientlypre-trainedlanguagemodelsareexceptionalfew-shot
learners(8). However,whenfacedwithlowresourcedataordistributionsthataresignificantly
different from any pre-trained data, those pre-trained language models lose their advantages
to our method. We compare our method with BERT on four different low-resource language
datasets-Kinyarwanda,Kirundi,SwahiliandFilipino. Thesedatasetsarecurated
tohavetheLatinalphabets,sameasenglishcorpus. BERThasperformedextremelywellas
9
Kinnews Kirnews Swahili Filipino
BERT 24.0±6.0 38.6±10.0 39.6±9.6 40.9±5.8
mBERT 22.9±6.6 32.4±7.1 55.8±16.9 46.5±4.8
Ours 45.8±6.5 54.1±5.6 62.7±7.2 65.2±4.8
Table3: 5-shottextclassificationaccuracyonlow-resourcedatasets
showninTable2duetopre-trainingonbillionsoftokens. However,whenfacinglow-resource
datasets, BERT perform significantly worse than our method only using gzip as we can see
inTable3,nomatterusingmultilingualpre-trainedversionortheoriginalone. NotethatmBERT
ispre-trainedon104languagesincludingSwahiliandTagalog(onwhichFilipinoisbased
on). AswecanseeonSwahiliandFilipino,mBERTperformsbetterthanBERT,butstill
significantlylowerthanourmethod.
Omniglot one-shot-classification dataset
In (4), a one-shot learning framework
Bayesian program learning (BPL) was pro-
posed. It learns a simple probabilistic model
foreachconcept. Takinganegativelogarithm
converts a Bayesian formula to a description
length paradigm, hence BPL can be viewed
asoneparticularapproximationtoourtheory.
Here we provide another simple approxima-
tionofourtheoryfortheOmniglotone-shot-
classificationdatasetof(4).
Oursystemfirstdecomposeagivenchar-
acter into strokes, then compute E(a,b) be-
tweencharactersaandb,usingalltheirpossi-
ble strokedecomposition. Weprovide how to
calculateE(a,b)here and detailsof decompo-
sitionprogramisgiveninAppendixA. Figure3: DistancebetweentwoBeziercurves
1. FitastrokebyaBeziercurve;
2. Ensurethenumberofpointsontwocurvesaresame. Thisalgorithmutilizeequallysplit
methodtoselectcertainsamenumberofpointsoneachcurveFigure3;
3. Ensure the area of the convex hull and the barycenter of the compared charactersare the
same;
10
4. Usemax Cartesiandistance betweenparallelpoints ontwo Beziercurvesto approximate
theminimumencodingdistancebetweentwoBeziercurves,asshowninFigure3;
5. Choosethecharacterwithminimumdistance.
Thissimple implementationachieves 92.25% accuracy20-way-1-shotonthis dataset. The
point here is to demonstrate various approximations of our theory that work rather than com-
paringaccuracy. At96.75%(4)or at92.25%mightbe twodifferent individualswith different
compressionalgorithms.
Unification
Ourframeworkcanunifyotherpopulardeepneuralnetworksforfew-shotlearning.
Siamese Network: Siamesenetwork usestwinsubnetwork torankthesimilarity between
two inputsin orderto learnuseful features. Mhere isoften acontrastive loss. This framework
showsstrongperformanceinone-shotimagerecognition(26).
PrototypicalNetwork: Prototypicalnetworks(27)proposetooptimizethedistancemetric
M directly by learning core in representation space. core are represented as the mean of
h h
embeddedsupportsamples.
Bi-Encoder: In the context of natural language processing, one of the dominant structure
is theBi-Encoderdesign witheach encoderbeing apre-trained languagemodel. Forexample,
in information retrieval, Dense Passage Retrieval (DPR), with two encoders encoding query
anddocumentrespectively,hasbecomethenewstateoftheart. Tocapturesemanticsimilarity,
sentenceBERT(28)alsoadoptsthebi-encoderdesignandbecomingoneofthemostprevalent
methods for semantic textual similarity. M in both cases can either be cosine similarity or
Euclideandistancebetweentherepresentationlearnedthroughpre-trainedmodels.
Information Distance basedMethods: Hundredsofalgorithmswerepublished, beforethe
deeplearningera,onparameter-freedatamining,clustering,anomalydetection,classification
using information distance E (29–34), with a comprehensive list in (13). Recently (19) have
discoveredusinginformationdistancewithdeepneuralnetworksandleveragethegeneralizability
of few-shot imageclassification. This work shows that with the help of deep generative models,
unlabelleddatacanbebetterutilizedforfew-shotlearningunderourframework.
Conclusion and a discussion on consciousness
We have defined human-like few-shot learning and derived an optimal form of such few-shot
learning. Note there is an interesting difference between our theory and classical learning
theory. In classical learning theory, it is well-known that if we compress training data to a
smaller consistent description, whether it is a classical Bayesian network or a deep neural
networks(13,35),wewouldachievelearning. Inthispaper,wedemonstratethatintheinference
11
stage, compression is also important, especially when there are not enough labelled data to
train a small model. On the biological side, compression circuits using predictive coding in
human cortex has been studied by (36). Experiments have also strongly supported our theory.
We expect to see more practical systems approximating our theory can be implemented to
solvecommonplacefew-shotlearningproblemswhenlargeamountsoflabelleddatafordeep
learningislacking. Wenowwishtoexploretwoconsequencesofourfew-shotlearningmodel,
toconsciousness.
A binary classifier of interestingness
Our few-shotlearning model has a by-product. We haveproved compression is auniversal goal
thatfew-shotlearningalgorithms approximate. Thusthisimplies immediatelya(subconscious)
binaryclassifier: ifsomethingiscompressed,thensomethinginterestinghappens,andattention
isgiven. Itturnsoutthatthis"Interestingness"hasbeentheoreticallystudiedaslogicaldepthfirst
proposedbyCharlesBennett(13). AccordingtoBennett,astructureisdeepifitissuperficially
randombutsubtlyredundant. Whenfew-shotlearninghappens,significantcompressionhappens,
and these deep objects gain attention. Such a binary classifier might explain our appreciation
of arts, music, games, and science, since these all share a common feature of dealing with
non-triviallycompressibleobjects: whetheritisashorterdescriptionofthedatathatgivesrise
ofNewton’slaws(13),orapieceofartormusicthatitselfiscompressibleorthatremindsusof
somethingwehave experiencedbefore,hencevery compressible,wefeelwe understanditand
henceappreciateit. Scienceisnothingbutcompressingdataintosimplerdescriptionsofnature.
Consciousness and the ability of labelling data
Doother specieshave consciousness? It isdifficult toanswerthis questionasconsciousness is
not testable. Thomas Nagel (37) made a comment: We will never know if a bat is conscious
becausewearenotbats.
Consideranalternativedata-drivenapproachbyaskingwhataspeciescandoinsteadofhow
theyfeel. Thatis,ifwetreatsomeaspectsofconsciousnessasacollectionoflearnedconcepts,
thengivenacompressionnetwork,theabilityofacquiringtherelevantconceptsbecomesamatter
of labelling relevant data. We know learning and consciousness are both located at posterior
cortexregion(38). Thisisinagreementwithsomeinjuredpatientswhentheylostconsciousness.
Thisisalsoinagreementwith“bistableperception”trainingresultswithmonkeys(39).
Varietiesofconsciousnessarebeingpragmaticallystudied(40). Theseinclude: 1)theability
of consciouslyperceivethe environment; 2)the abilityof evaluatingconscious emotions; 3)the
ability of having a unified conscious experience; 4) the ability of integrating across time as a
continuous stream, one moment flowing into the next; 5) the conscious awareness of oneself
asdistinctfromtheworldoutside. Manyoftheseabilitiesmaybeseenasafew-shotlearnable
concepts,givenproperlylabelleddata.
12
Differentanimalshavevariouslevelsofsomeofsuchconsciousnessbypassingcertaintests.
Forexample,chimpanzees,dolphins,Asianelephants,andmagpiescanrecognizethemselves
by passing some mirror-mark tests. The corvids display some emotions, and are able to plan
ahead. Octopushavepowerfulperceptualfacilitiesobtainingandprocessingdataindependently
witheachtentacle. Experimentally,awarenessemergeswheninformationtravelsbackandforth
betweenbrainareas(41)insteadofalinearchainofcommand.
Accordingtoourtheory,thebrainreallyonlyneedstouseauniversalcompressortocompress
information,regardlessofoneprocessorintheheadorafewprocessorsinthetentacle(incase
of Cephalopods). Thus we can conjecture that "consciousness” then is a matter of ability of
labellingthedatafromsensoryterminals. Foodorenemyintheenvironmentareeasytolabel.
Emotional labelling requires some level of abstraction. Self-awareness of “me” and “others”
thusisjustanotherbinaryclassifiertrainabledependingonifthespeciesisabletodo“displaced
reference”mentallabelling. Otherthanthehumanbeings,onlyorangutansareknowntohave
limiteddisplacedreferenceability(42).
Thus we have just reduced the non-testable question of whether an animal has consciousness
insomeaspectstoifitisabletolabelthecorrespondingdataproperly.
Acknowledgement
WethankDr. HangLiforsuggestionsandbringing(43)toourattentionandDr. AmySunfor
bringing(44)toourattention. TheworkissupportedinpartbyCanada’sNSERCoperatinggrant
OGP0046506,CanadaResearchChairProgram,andthe Leading InnovativeandEntrepreneur
teamsprogramofZhejiang,number2019R02002,andNSFCgrant61832019.
References and Notes
1. Y.LeCun,Y.Bengio,G.Hinton,Nature521,436(2015).
2. C.Spearman(1961).
3. K.Friston,NatureReviewNeuroscience11,21(2010).
4. B.M.Lake,R.Salakhutdinov,J.B.Tenenbaum,Science350,1332(2015).
5. E.Stern,npjScienceofLearning2,1(2017).
6. M.Hebart,C.Zheng,F.Pereira,C.Baker,Nature,HumanBehaviour pp.1173–1185(2020).
7. C. Finn, P. Abbeel, S. Levine, International conference on machine learning (PMLR, 2017),
pp.1126–1135.
8. T.Brown,etal.,Advancesinneuralinformationprocessingsystems33,1877(2020).
13
9. T.J.BouchardJr,AnnalsofHumanBiology36,527(2009).
10. M.N.Bernstein,mbernste.github.io/posts/elbo/ .
11. J.Schmidhuber,arXiv:0812.4360v2[cs.AI](2009).
12. N.Chater,P.Vitányi,TrendsinCognitiveSciences7,19(2003).
13. M.Li,P.Vitányi,AnIntroductiontoKolmogorovComplexityandItsApplications(Springer-
Verlag,1993,1997,2008,2019).
14. C. Bennett, P. Gács, M. Li, P. Vitányi, W. Zurek, IEEE Trans. Inform. Theory 44, 1407
(1998).
15. D.C.Knill,A.Pouget,TRENDSinNeurosciences27,712(2004).
16. K.Friston,PLoScomputationalbiology4,e1000211(2008).
17. C.Koch,G.Tononi,ScientificAmerican304(2011).
18. J. Townsend, T. Bird, D. Barber, International Conference on Learning Representations
(2018).
19. Z.Jiang,Y.Dai,J.Xin,M.Li,J.Lin,AdvancesinNeuralInformationProcessingSystems
(2022).
20. A.Joulin,E.Grave,P.B.T.Mikolov,EACL2017 p.427(2017).
21. M.Schuster,K.K.Paliwal,IEEEtransactionsonSignalProcessing45,2673(1997).
22. Y. Wang, M. Huang, X. Zhu, L. Zhao, Proceedings of the 2016 conference on empirical
methodsinnaturallanguageprocessing(2016),pp.606–615.
23. Z. Yang, et al., Proceedings of the 2016 conference of the North American chapter of
the association for computational linguistics: human language technologies (2016), pp.
1480–1489.
24. T.Mikolov,K.Chen,G.Corrado,J.Dean,arXivpreprintarXiv:1301.3781(2013).
25. J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human
LanguageTechnologies,Volume1(LongandShortPapers)(2019),pp.4171–4186.
26. G.Koch,R.Zemel,R.Salakhutdinov,etal.,ICMLdeeplearningworkshop(Lille,2015),
vol.2,p.0.
14
27. J. Snell, K. Swersky, R. Zemel, Advances in neural information processing systems 30
(2017).
28. N. Reimers, I. Gurevych, Proceedings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th International Joint Conference on Natural
LanguageProcessing(EMNLP-IJCNLP)(2019),pp.3982–3992.
29. M.Li,etal.,Bioinformatics17,149(2001).
30. E. Keogh, S. Lonardi, C. A. Ratanamahatana, Proceedings of the tenth ACM SIGKDD
internationalconferenceonKnowledgediscoveryanddatamining(2004),pp.206–215.
31. C.H.Bennett,M.Li,B.Ma,ScientificAmerican288,76(2003).
32. M.Nykter,etal.,Physicalreviewletters100,058702(2008).
33. D.Benedetto,E.Caglioti,V.Loreto,PhysicalReviewLetters88,048702(2002).
34. M.Nykter,etal.,ProceedingsoftheNationalAcademyofSciences105,1897(2008).
35. Y.Bengio,etal.,Foundationsandtrends®inMachineLearning2,1(2009).
36. R.P.Rao,D.H.Ballard,Natureneuroscience2,79(1999).
37. T.Negel,Readingsinphilosophyofpsychology(1974).
38. C.Koch,ScientificAmerican.(2018).
39. G.Miller,Science309,79(2005).
40. J.Birch,A.Schnell,N.Clayton,Trendsincognitivesciences(2020).
41. M.Boly,etal.,Science332(May,2011).
42. H.Lyn,etal.,AnimalCognition17(2014).
43. Y.Ma,D.Tsao,H.Shum(2022).
44. F.Scherr,C.Stöckl,W.Maass,BioRxiv(2020).
15
A Algorithm for extracting strokes from a character
Repeatuntilallpixelsofacharacteraremarked,bydepth-firstsearch:
(1)Extractitsskeletonsothatthestrokewidthis1pixelpoint. Thenconverttheimageto
a graph and shrink adjacent cross points. (2) Randomly select an endpoint as starting point,
endpointattoplefthasagreaterchanceofbeingselected. Walkuntilacrosspointorendpoint.
Ifthereisacirclethenselectacrosspointofatopleftpointifthereisnocrosspoint. Record
this stroke and mark it on the character. Allow small number of marked pixel points to make
thedecompositionmorenatural. (3)Whenmeetingacrosspoint,thenenumeratetwosituations
ofpen-upandturning, randomly. Pen-upmeansendofastroke, gotostep(2)withthemarked
graph. Turningmeanscontinuationhencerepeatstep(2). Ifwalkingtoanendpoint,thenattempt
to turn by going back to find a new unmarked pixels within some small number of pixels or
directlyendthestrokeandrepeatstep(2)withmarkedgraph.
16

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "A Theory of Human-Like Few-Shot Learning"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
