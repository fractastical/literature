=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Dissociated Neuronal Cultures as Model Systems for Self-Organized Prediction
Citation Key: yaron2025dissociated
Authors: Amit Yaron, Zhuo Zhang, Dai Akita

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Abstract: Dissociated neuronal cultures provide a simplified yet effective model system for investigating
self-organized prediction and information processing in neural networks. This review
consolidates current research demonstrating that these in vitro networks display fundamental
computational capabilities, including predictive coding, adaptive learning, goal-directed
behavior, and deviance detection. We examine how these cultures develop critical dynamics
optimized for information processing, detail t...

Key Terms: systems, energy, organized, prediction, university, computing, information, dissociated, cultures, neuronal

=== FULL PAPER TEXT ===

Dissociated Neuronal Cultures as Model Systems for Self-Organized
Prediction
Amit Yaron1, Zhuo Zhang2, Dai Akita2, Tomoyo Isoguchi Shiramatsu2, Zenas Chao1,
Hirokazu Takahashi1,2*
¹ International Research Center for Neurointelligence (WPI-IRCN), The University of Tokyo
Institutes for Advanced Study (UTIAS), The University of Tokyo, Tokyo, Japan
² Department of Mechano-Informatics, Graduate School of Information Science and Technology,
The University of Tokyo, Tokyo, Japan
* Correspondence:
Corresponding Author
Hirokazu Takahashi (takahashi@i.u-tokyo.ac.jp)
Keywords: Dissociated neuronal cultures, Predictive coding, Self-organized criticality,
Neuromorphic computing, Goal-directed behavior, Free energy principle
Abstract
Dissociated neuronal cultures provide a simplified yet effective model system for investigating
self-organized prediction and information processing in neural networks. This review
consolidates current research demonstrating that these in vitro networks display fundamental
computational capabilities, including predictive coding, adaptive learning, goal-directed
behavior, and deviance detection. We examine how these cultures develop critical dynamics
optimized for information processing, detail the mechanisms underlying learning and memory
formation, and explore the relevance of the free energy principle within these systems. Building
on these insights, we discuss how findings from dissociated neuronal cultures inform the design
of neuromorphic and reservoir computing architectures, with the potential to enhance energy
efficiency and adaptive functionality in artificial intelligence. The reduced complexity of
neuronal cultures allows for precise manipulation and systematic investigation, bridging
theoretical frameworks with practical implementations in bio-inspired computing. Finally, we
highlight promising future directions, emphasizing advancements in three-dimensional culture
techniques, multi-compartment models, and brain organoids that deepen our understanding of
hierarchical and predictive processes in both biological and artificial systems. This review aims
1
to provide a comprehensive overview of how dissociated neuronal cultures contribute to
neuroscience and artificial intelligence, ultimately paving the way for biologically inspired
computing solutions.
1 Introduction
The brain's remarkable ability to process information, learn from experience, and adapt to
changing environments emerges from the dynamic interactions of billions of neurons.
Understanding how these capabilities arise from neural network organization represents a
fundamental challenge in neuroscience (Friston et al., 2006; Friston, 2010; Bastos et al., 2012;
Keller and Mrsic-Flogel, 2018). Dissociated neuronal cultures—simplified systems where
neurons are isolated from their native environment and allowed to self-organize—provide a
powerful experimental platform for investigating these processes. These cultures retain core
capabilities for network formation, information processing, and adaptation while offering
unprecedented access for manipulation and observation (Maeda et al., 1995; Kamioka et al.,
1996; Potter and DeMarse, 2001; Marom and Shahaf, 2002).
The study of neuronal cultures has evolved dramatically since Ross Granville Harrison first
demonstrated nerve fiber growth in vitro in 1910 (Harrison, 1910). Harrison's pioneering work
established the foundation for modern neurobiology by enabling direct observation of neural
development. A transformative advance came with the introduction of microelectrode array
(MEA) technology (Figure 1A) (Thomas et al., 1972; Gross et al., 1977; Pine, 1980). MEAs
revolutionized the field by enabling long-term, non-invasive recording from multiple neurons
simultaneously, providing unprecedented insight into network dynamics and development (Pine,
2006; Bakkum et al., 2013; Müller et al., 2015; Obien et al., 2015). Early MEA platforms
allowed researchers to monitor network formation in dissociated cultures, revealing spontaneous
activity and plasticity (Figures 1B and 1C). High-density CMOS microelectrode arrays now
enable recording from thousands of neurons with unprecedented spatial and temporal resolution
(Berdondini et al., 2005; Frey et al., 2007; Ballini et al., 2014; Müller et al., 2015). These
systems (Figure 1D) facilitate detailed investigations of both localized interactions and long-
range network dynamics. They provide subcellular resolution, as illustrated by the precise
alignment of neurons with individual electrodes (Figure 1E) and allow spatial mapping of
extracellular spikes overlaid on neuronal morphology to track activity sources and connectivity
2
(Figure 1F). Research using these systems has revealed several fundamental properties of neural
network organization and function.
Research using these systems has revealed several fundamental properties of neural network
organization and function. As cultures develop, they demonstrate a remarkable capacity for self-
organization, evolving from random collections of cells into functional networks that exhibit
critical dynamics optimized for information (Beggs and Plenz, 2003; Levina et al., 2007;
Millman et al., 2010; Friedman et al., 2012; Yada et al., 2017; Kossio et al., 2018). These
networks show robust capabilities for learning and memory formation, as demonstrated through
studies of synaptic plasticity and adaptive responses to electrical stimulation (Jimbo et al., 1998,
1999; Shahaf and Marom, 2001; Le Feber et al., 2010, 2014, 2015; Dranias et al., 2013; Dias et
al., 2021)
Neuronal cultures have proven effective for studying goal-directed behavior in closed-loop
systems. Potter et al. (1997) introduced the 'Animat in a Petri Dish' concept, establishing a
paradigm where network activity controlled a simulated animal ('animat') while receiving
sensory feedback through electrical stimulation. This foundational work led to numerous studies
demonstrating that cultured networks can adapt to control external devices (DeMarse et al.,
2001; Potter et al., 2003; Bakkum et al., 2008b; Chao et al., 2008a; Tessadori et al., 2012;
Masumori et al., 2020; Yada et al., 2021; Kagan et al., 2022), advancing our understanding of
neural adaptation and control while suggesting new approaches for brain-machine interfaces and
neuroprosthetics.
The computational capabilities of neuronal cultures extend to more sophisticated information
processing tasks. These networks exhibit predictive coding and deviance detection, supporting
theoretical frameworks such as the free energy principle (Rao and Ballard, 1999; Friston, 2010;
Huang and Rao, 2011; Isomura et al., 2015; Isomura and Friston, 2018; Lamberti et al., 2023).
Their ability to perform complex computations while maintaining remarkable energy efficiency
has important implications for neuromorphic computing and artificial intelligence (Marković et
al., 2020; Smirnova et al., 2023). Insights from neuronal cultures can influence the development
of new computing architectures, particularly in areas such as reservoir computing and adaptive
3
neural networks (Dockendorf et al., 2009; Kubota et al., 2019, 2021; Tanaka et al., 2019;
Subramoney et al., 2021; Cai et al., 2023; Sumi et al., 2023).
The development of three-dimensional culture techniques and brain organoids offers new
opportunities to study neural organization in more physiologically relevant contexts (Hogberg et
al., 2013; Lancaster et al., 2013; Clevers, 2016; Smirnova and Hartung, 2024). These advances,
combined with sophisticated analysis techniques and theoretical frameworks, are providing new
insights into how neural networks self-organize for efficient information processing and
adaptation.
This review synthesizes current research on dissociated neuronal cultures, examining their
contributions to our understanding of neural network organization and function. We begin by
exploring network development and the emergence of critical dynamics (Chapter 2), followed by
detailed analysis of learning and memory formation in these systems (Chapter 3). We then
examine how neuronal cultures exhibit deviance detection and predictive processing (Chapter 4),
and their remarkable capacity for goal-directed behavior when coupled with external systems
(Chapter 5). The relationship between these empirical findings and theoretical frameworks,
particularly the free energy principle, is explored in Chapter 6. Chapter 7 discusses how insights
from neuronal cultures inform the development of artificial neural networks and neuromorphic
computing systems. Finally, we consider future directions for the field, including advances in
three-dimensional culture techniques, brain organoids, and their implications for both
neuroscience and artificial intelligence (Chapter 8).
By examining how these simplified neural systems self-organize for prediction and adaptation,
we aim to illuminate fundamental principles of neural computation while highlighting their
practical applications in bio-inspired computing and neuroprosthetics. This understanding may
ultimately guide the development of more efficient and adaptive artificial systems while
deepening our knowledge of biological neural network function.
2. Network Development and Self-Organized Criticality
The transformation of dissociated neuronal cultures from random collections of neurons into
sophisticated, functionally organized systems is a remarkable feat of biological self-organization.
This chapter explores the key processes and principles underlying network development in these
4
cultures, followed by an introduction to the concept of Self-Organized Criticality (SOC) and its
relevance to understanding network maturation.
Network development in neuronal cultures progresses through several distinct stages, each
characterized by increasingly complex patterns of activity. In the earliest stages, neurons exhibit
seemingly chaotic, independent firing patterns. Kamioka et al. (1996) observed that this apparent
randomness quickly gives way to more organized activity as the culture matures. The transition
from independent firing to coordinated activity is heavily dependent on NMDA receptor
activation and is influenced by external factors such as calcium concentrations (Segev et al.,
2001). As development continues, the network establishes stable, recurring patterns of
synchronized activity. Van Pelt et al. (2004) documented the emergence of network bursting as a
hallmark of culture maturation. Further refinement of connections leads to more sophisticated
firing patterns, including what Wagenaar et al. (2006) termed "superbursts" - periods of intense,
coordinated activity that reflect the increasing complexity of network interactions. As shown by
Yada et al. (2017), these patterns exhibit state-dependent properties, with different
spatiotemporal patterns appearing successively and periodically, suggesting organized
fluctuations in neural activity propagation. Figure 2 illustrates these developmental transitions
using data from high-density CMOS microelectrode arrays. Figure 2A displays spatial maps of
action potential amplitudes recorded at different developmental stages, while Figure 2B
highlights changes in spike waveforms at selected electrodes over time. Figure 2C depicts the
progression of spontaneous spiking activity, showcasing the emergence of synchronized bursts.
Figure 2D visualizes the shift in neuronal avalanche size distributions, from exponential at early
stages (4 DIV) to power-law distributions indicative of SOC by 16 DIV. Lastly, Figure 2E
presents the integration-fragmentation model explaining SOC emergence, highlighting the role
of synaptic pruning and balanced excitation-inhibition dynamics in this transition. The structural
and functional organization of the network evolves in parallel with these changes in activity
patterns. Over time, synaptic connections become more stable, as evidenced by metrics like
conditional firing probabilities (Le Feber et al., 2007). Soriano et al. (2008) demonstrated that
maturing networks form modular structures with hierarchical organization, which is crucial for
supporting coherent and synchronized activity. This organization has been shown to develop into
rich-club topology, which supports coordinated dynamics and information processing (Schroeter
et al., 2015; Nigam et al., 2016). Baruchi et al. (2008) characterized how mutual synchronization
5
emerges between coupled networks, demonstrating that despite engineering similarity,
spontaneous asymmetries emerge in both activity propagation and functional organization.
Intriguingly, Orlandi et al. (2013) discovered that intrinsic noise plays a constructive role in
network formation. Through a process they termed "noise focusing," weak, randomly distributed
synaptic connections are enhanced, facilitating the transition from disorganized activity to
coherent network dynamics.
As researchers sought to understand the principles governing these complex developmental
dynamics, the concept of Self-Organized Criticality (SOC) emerged as a powerful explanatory
framework. Introduced by Per Bak (Bak et al., 1987; Bak, 1996) in the context of physical
systems, SOC describes how complex systems naturally evolve toward a critical state
characterized by scale-invariant behavior. Further theoretical work has expanded our
understanding of SOC in neural systems, highlighting its ubiquity across different scales of brain
organization and its functional implications (Muñoz, 2018; Plenz et al., 2021). Networks at
criticality exhibit maximized dynamic range, optimally responding to the broadest range of
stimulus intensities (Shew et al., 2009). This concept has been widely applied to neural systems,
offering insights into network development and function (Chialvo, 2010; Beggs and Timme,
2012; Shew and Plenz, 2013; Bilder and Knudsen, 2014). In the context of neuronal networks,
SOC is most notably manifested in the phenomenon of neuronal avalanches - cascades of
spontaneous activity that follow power-law size distributions. Beggs and Plenz (2003) were
among the first to observe and characterize these avalanches in neuronal cultures, followed by
confirmations in various neural systems (Mazzoni et al., 2007; Pasquale et al., 2008; Petermann
et al., 2009; Friedman et al., 2012). Experimental evidence for the development of SOC has
come from several studies using advanced recording techniques. Yada et al. (2017) used high-
density CMOS microelectrode arrays to capture the progression of avalanche dynamics across
three distinct phases: an initial exponential distribution, a transitional bimodal distribution, and a
final power-law distribution characteristic of a critical state. This observed sequence supports a
gradual expansion model of network development, where neural connections are extended
incrementally over time. Kayama et al. (2019) revealed the formation of functional clusters
within maturing cultures, showing how these clusters exhibit diverse and repeatable patterns of
synchronized firing, indicating the development of specialized subnetworks within the larger
network structure. These findings complement earlier observations (Baruchi et al., 2008) about
6
the emergence of mutual synchronization in coupled networks, demonstrating how spontaneous
asymmetries arise in both activity propagation and functional organization.
The emergence of SOC in neuronal cultures involves multiple mechanisms developing over
time. Vreeswijk and Sompolinsky (1996) demonstrated the importance of balanced excitation
and inhibition in neural networks, showing its relationship with chaotic dynamics. Abbott and
Rohrkemper (2007) proposed a growth-based mechanism where neurons add or remove synapses
based on their activity levels. Both short-term and long-term plasticity contribute to the
network's evolution toward criticality (Levina et al., 2007; Millman et al., 2010). Vogels et al.
(2011) showed how inhibitory plasticity maintains excitation-inhibition balance in memory
networks, and Hennequin et al. (2017) synthesized how inhibitory synaptic plasticity acts as a
crucial control mechanism for network stability and computation. The interaction between
plasticity mechanisms is particularly important: excitatory STDP with an asymmetric time
window destabilizes the network toward a bursty state, while inhibitory STDP with a symmetric
time window stabilizes the network toward a critical state (Sadeh and Clopath, 2020). Structural
changes, such as axonal elongation and synaptic pruning, also shape the network's critical
dynamics (Tetzlaff et al., 2010; Kossio et al., 2018). Kuśmierz et al. (2020) demonstrated that
networks with power-law distributed synaptic strengths exhibit a continuous transition to chaos.
The relationship between criticality and the edge of chaos represents another important
regulatory point in neural networks, associated with the balance between excitation and
inhibition. SOC, the edge of chaos, and excitation-inhibition balance serve as complementary
homeostatic set points in well-tuned networks, each contributing to the optimization of
computation and memory formation. Ikeda et al. (2023) have shown how the interplay between
environmental noise and spike-timing-dependent plasticity can drive networks toward criticality,
emphasizing the importance of optimal noise levels in this process. Theoretical modeling by
Kern et al. (2024 (under review)) has emphasized the crucial role of inhibitory circuitry,
demonstrating how the density and range of inhibitory synaptic connections significantly
influence the development of critical dynamics.
The study of network development through the lens of SOC has provided valuable insights into
the fundamental principles governing the maturation of neuronal systems. It offers a framework
for understanding how complex, functional network structures emerge from initially disordered
7
collections of neurons, and how these networks maintain a balance between stability and
flexibility as they mature. This self-organized development toward criticality, supported by
various plasticity mechanisms and carefully regulated by inhibitory circuits, enables neuronal
networks to achieve optimal information processing capabilities while maintaining adaptability.
The resulting networks exhibit a rich repertoire of dynamics that supports their computational
functions while preserving the ability to respond to changing environmental demands.
3. Adaptive Learning and Memory Formation
Dissociated neuronal cultures offer a simplified system for studying learning and memory,
providing insight into how neural networks adapt in response to external stimuli. This chapter
reviews key findings demonstrating that these cultures exhibit learning behaviors and explores
the mechanisms that enable memory formation and adaptation in these systems.
Early studies laid the foundation for understanding learning in dissociated cultures. Jimbo et al.
(1999) showed that localized tetanic stimulation could induce potentiation and depression in
specific pathways, highlighting the network's capacity to modify connections based on stimuli.
Shahaf and Marom (2001) demonstrated that networks could be trained to produce specific
responses through low-frequency electrical stimulation, without the need for external reward
mechanisms, suggesting that learning can emerge from simple, self-organizing principles. Ruaro
et al. (2005) further established the computational capabilities of these cultures, showing they
could perform pattern recognition tasks through targeted electrical stimulation. Their work
demonstrated how biological neurons could be trained to recognize specific spatial patterns, with
responses enhanced through long-term potentiation mechanisms.
Later work explored how network dynamics could be controlled and shaped through stimulation.
Wagenaar et al. (2005) demonstrated that closed-loop, distributed electrical stimulation could
effectively transform burst-dominated activity into dispersed spiking patterns more characteristic
of in vivo activity. Le Feber et al. (2010) showed that adaptive electrical stimulation—where
stimulation is adjusted based on network feedback—was more effective at inducing long-lasting
connectivity changes compared to random stimulation. This highlighted the role of feedback in
shaping the learning process.
8
Memory formation in dissociated cultures was further investigated by Le Feber et al. (2015),
who found that repeated stimulation could create multiple parallel memory traces. This indicated
that these cultures could handle complex memory storage tasks, with distinct stimuli producing
stable patterns of connectivity. Additionally, Bakkum et al. (2008a) demonstrated that even when
synaptic transmission was blocked, changes in action potential propagation still occurred,
suggesting that non-synaptic mechanisms contribute to network adaptation.
Short-term memory processes were explored by Dranias et al. (2013), who identified two types
of STM in these networks: "fading memory," reliant on reverberating neural activity, and
"hidden memory," which persists through changes in synaptic strength even after neural activity
has ceased. Ju et al. (2015) expanded on these findings, demonstrating that dissociated networks
possess an intrinsic capacity for spatiotemporal memory lasting several seconds and can classify
complex temporal patterns. Their work highlighted the importance of short-term synaptic
plasticity and recurrent connections in enabling these computational capabilities.
Further studies have provided more detail on the molecular and network dynamics underlying
memory and learning. Dias et al. (2021) found that memory consolidation in these cultures was
influenced by network state, with low cholinergic tone enhancing memory formation. Ikeda et al.
(2021) demonstrated the flexibility of dissociated networks, showing that low-frequency
stimulation could initially induce depression but later lead to potentiation, revealing the dynamic
nature of learning.
These findings demonstrate that dissociated neuronal cultures are capable of both learning and
memory formation through various mechanisms, including synaptic plasticity, non-synaptic
adaptations, and network state-dependent processes. As we further explore predictive processing,
these adaptive behaviors provide an essential foundation for understanding how simplified neural
networks manage information and anticipate future events.
4. Prediction, Deviance Detection, and the Free Energy Principle
The free energy principle and predictive coding framework propose that neural systems maintain
internal models to minimize prediction errors about their sensory inputs. Under this framework,
neural responses represent prediction errors - the difference between expected and actual inputs.
Organisms actively minimize prediction errors through two complementary processes: updating
9
internal models to better predict sensory inputs and selecting actions that confirm these
predictions. This principle helps explain phenomena like mismatch negativity (MMN), where the
brain produces enhanced responses to stimuli that violate statistical regularities, representing
prediction error signals in sensory processing hierarchies.
In dissociated neuronal cultures, evidence for predictive processing comes from multiple
experimental approaches. Early evidence for differential processing of frequent and rare stimuli
came from Eytan et al. (2003), who showed that cortical networks could selectively adapt to
different stimulation patterns using multi-electrode arrays. Their work demonstrated that neurons
attenuated responses to frequent stimuli while enhancing responses to rare events. Through
careful pharmacological manipulations, they showed this selective adaptation depended on both
excitatory synaptic depression and GABAergic inhibition, though their findings likely primarily
reflect stimulus-specific adaptation (SSA) mechanisms rather than true prediction error
signaling. The distinction between SSA and genuine deviance detection became clearer through
subsequent work. While SSA reflects passive reduction in responses to repeated stimuli through
synaptic depression, true deviance detection requires active comparison between predicted and
actual inputs. Kubota et al. (2021a) provided preliminary evidence for genuine prediction error
detection using high-density CMOS arrays. By implementing both oddball paradigms and many-
standards control conditions, they demonstrated that deviant responses were enhanced beyond
what SSA alone would predict. These paradigms and their results are summarized in Figure 3,
which illustrates the experimental setup and neuronal responses. Figure 3A shows the electrode
map of the high-density CMOS microelectrode array, highlighting the spatial distribution of
stimulating and recording sites. Figure 3B details the stimulation protocols used in the oddball
and many-standards control paradigms, demonstrating how the alternation of standards and
deviants elicits differential responses. Figure 3C compares neural responses to standard and
deviant stimuli, with raster plots and population peristimulus time histograms (p-PSTHs)
revealing that deviant stimuli elicit stronger and more widespread responses than standards,
particularly in the late response phase. Recent work (Zhang et al. 2025, in press) has solidified
these findings using additional controls and larger sample sizes to confirm that the enhanced
mismatch responses are not artifacts of simpler mechanisms like stimulus-specific adaptation.
These findings were particularly robust in demonstrating mismatch responses dependent on
NMDA receptor function, mirroring their role in MMN generation in intact brains and
10
highlighting the critical role of synaptic plasticity in neural prediction. Additionally, this study
showed that cultured networks can detect violations of complex statistical regularities, providing
further evidence for their sophisticated mismatch responses and sensitivity to sequence
predictability, similar to capabilities previously observed only in intact cortex (Yaron et al.,
2012). The findings suggest these basic networks possess intrinsic capabilities for statistical
learning and prediction. The mechanistic basis for deviance detection has been illuminated
through computational modeling. Kern and Chao (2023) demonstrated that the interaction
between two forms of short-term plasticity—synaptic short-term depression (STD) and threshold
adaptation (TA)—can explain how neural networks achieve deviance detection. Their work
showed that threshold adaptation alone enables basic deviance detection by reducing responses
to frequent stimuli while maintaining sensitivity to unexpected inputs. However, the combination
of TA with synaptic short-term depression produces enhanced deviance detection through
synergistic effects: local synaptic fatigue from STD amplifies the global recovery mediated by
TA. This mechanism allows networks to effectively encode predictable patterns while
maintaining heightened sensitivity to novel stimuli, providing a computational foundation for
understanding how neural circuits implement prediction error detection.
Strong evidence for predictive processing in cultured networks comes from studies
demonstrating Bayesian inference capabilities. Isomura et al. (2015) showed that cortical
neurons in culture could perform blind source separation using a microelectrode array (MEA)
system. By delivering mixed stimuli containing distinct patterns, they demonstrated that rat
cortical neurons could develop selective responses to specific stimulus aspects through Hebbian
plasticity, distinguishing individual sources within the mixed inputs. This work provided initial
support for free energy minimization in simplified neural circuits. Building on this foundation,
Isomura and Friston (2018) explored how neuronal cultures perform inference about hidden
causes in their sensory environment. By stimulating cortical neurons with probabilistic input
patterns, they observed neurons developing functional specialization - selectively responding to
certain hidden sources within mixed stimuli. This selective response pattern aligned with
Bayesian inference under the free energy principle, as neurons refined their responses based on
accumulated evidence regarding the sources generating their inputs. Recent work Isomura et al.
(2023) provided the most direct evidence yet by demonstrating that dissociated neuronal
networks perform variational Bayesian inference. Using an MEA to deliver structured stimuli
11
composed of two hidden sources, they observed that neuronal networks adapted their responses
through synaptic adjustments, functioning as probabilistic beliefs about the sources. Notably,
pharmacological manipulation of network excitability altered these "prior beliefs," offering direct
evidence for variational free energy minimization in simplified neural systems.
The relationship between prediction and memory formation has been illuminated by (Lamberti et
al., 2023), who demonstrated that focal electrical stimulation generates more effective long-term
memory traces compared to global stimulation. Using detailed analysis of network responses,
they showed that spatially specific activation patterns enhance the network's ability to predict
future inputs. This suggests that localized stimulation allows networks to build more accurate
predictive models through targeted synaptic modifications. Their follow-up study (Lamberti et
al., 2024) provided mechanistic insights by revealing that NMDA receptor activity is crucial for
stabilizing these memory traces and improving prediction, demonstrating how synaptic plasticity
enables networks to build and refine their predictive models.
These findings demonstrate that even simplified neuronal networks can implement core aspects
of predictive processing - from basic prediction error detection to sophisticated Bayesian
inference. While the exact mechanisms may differ from intact brains, the evidence suggests that
prediction is a fundamental feature of neural computation that can be studied effectively in
reduced preparations. Understanding how these basic circuits implement prediction may inform
both theories of brain function and development of artificial systems incorporating similar
principles.
5. Goal-Directed Behavior
Dissociated neuronal cultures, when integrated with embodied systems, provide a powerful
model for studying goal-directed behavior. Potter et al. (1997) pioneered this field by introducing
the 'Animat in a Petri Dish' concept, combining cultured neural networks with real-time
computing environments. Using multi-electrode arrays (MEAs) and advanced imaging
techniques, they established a paradigm where network activity controlled a simulated animal
('animat') while receiving sensory feedback through electrical stimulation. This groundbreaking
work demonstrated the potential for studying learning and memory in simplified neural networks
through feedback-driven interaction with their environment.
12
DeMarse et al. (2001) built upon this foundation by demonstrating that cultured networks could
control a simulated aircraft's pitch and roll in a virtual environment, showing that these cultures
could learn to maintain flight stability over time. Potter et al. (2004) further advanced the field by
introducing "Hybrots" (hybrid neural-robotic systems), where cultured networks served as
"brains" for robotic systems. This approach addressed limitations of traditional in vitro systems
by providing sensory inputs and motor outputs through closed-loop interaction.
A systematic investigation of these systems emerged through a series of complementary studies.
Chao et al. (2005) demonstrated that random background stimulation could stabilize synaptic
weights after tetanization in both simulated and living networks, preventing spontaneous bursts
from disrupting learned patterns. They developed novel analytical tools, further refined in Chao
et al. (2007), including the Center of Activity Trajectory (CAT) to better detect and analyze
network plasticity. This work provided the methodological foundation for more complex
behavioral studies.
Chao et al. (2008a, 2008b) demonstrated how simulated neural networks could be shaped for
adaptive, goal-directed behavior. Using leaky integrate-and-fire neurons inspired by cortical
cultures, they created a closed-loop system where an animat learned to move and remain within
specific target areas. Their work revealed several key principles: random background stimulation
was crucial for maintaining network stability, successful adaptation required stimuli that evoked
distinct network responses, and long-term plasticity through STDP was essential for learning.
Building on these insights, (Bakkum et al., 2008b) made the crucial advance of implementing
these principles in living neural networks. Using multi-electrode arrays, they showed how real
biological networks could be trained to perform goal-directed behavior through a structured
combination of context-control probing sequences (CPS), patterned training stimulation (PTS),
and random background stimulation (RBS). Their success in training cultures to guide an animat
toward predefined areas demonstrated that biological neural circuits could be shaped for adaptive
control in real-world applications, establishing a foundation for developing neuroprosthetics and
therapeutic interventions. This work provided definitive evidence that living neuronal networks
could be systematically trained to perform specific behaviors through carefully designed
stimulation protocols.
13
Tessadori et al. (2012) further explored modular network architectures, showing that
hippocampal neurons divided into distinct compartments could enhance goal-directed behavior.
Their virtual robot avoided obstacles in an arena by interfacing with the neuronal culture, with
tetanic stimulation applied to reinforce successful movements. Modular networks exhibited more
structured and selective neural activity, improving the robot’s performance compared to random
networks.
Recent advances have explored new computational paradigms in these systems. Masumori et al.
(2020) introduced the concept of "neural autopoiesis," showing how networks can regulate self-
boundaries through stimulus avoidance behaviors. Their work revealed how networks adaptively
distinguish between controllable and uncontrollable inputs, providing insights into neural self-
organization and adaptation. Yada et al. (2021) demonstrated physical reservoir computing with
FORCE learning in living neuronal cultures. Figure 4 illustrates this closed-loop system, where
cortical neurons cultured on a microelectrode array (MEA) generate spiking activity processed
via FORCE learning to create coherent signals. Figure 4A shows the system’s design, including
optical stimulation using a digital micromirror device (DMD) for feedback. Figure 4B
demonstrates the robot navigation task, where neuronal activity controls a robot navigating
through a maze toward a goal (highlighted in yellow), with electrical stimulation applied when
obstacles are encountered. Feedback from the environment guides the robot’s trajectory,
highlighting how intrinsic neural dynamics, coupled with real-time learning algorithms, enable
adaptive task performance. This work underscores the potential of embodied neuronal networks
for solving goal-directed tasks without additional external learning mechanisms.
Kagan et al. (2022) made a significant advance by demonstrating that dissociated neuronal
cultures could rapidly adapt to controlling a paddle in a simplified “Pong” game. Using a high-
density multi-electrode array (HD-MEA) with 26,400 electrodes, the system provided real-time
feedback to neurons, which were able to adjust their firing patterns within minutes. Their latest
work (Khajehnejad et al., 2024) compared the learning efficiency of biological neurons with
deep reinforcement learning (RL) algorithms, revealing that neurons could learn faster in
environments with limited training data, highlighting their unique adaptability.
14
Moving forward, these works open new opportunities for exploring more complex tasks in
embodied neural systems, though questions about the intelligence or sentience of these behaviors
remain (Balci et al., 2023). Further research could involve more intricate feedback systems and
multi-compartment setups, to deepen our understanding of neuronal plasticity and prediction in
embodied systems, with potential applications in neuroprosthetics, robotics, and bio-hybrid
systems.
6. Insights for Artificial Neural Networks and Neuromorphic Systems
Research into dissociated neuronal cultures has become increasingly relevant for designing
neuromorphic computing systems that address traditional computing limitations. The scale of
this challenge is striking: Marković et al. (2020) highlight that training a single state-of-the-art
natural language processing model on conventional hardware consumes energy equivalent to
running a human brain for six years. In contrast, biological neural networks perform complex
computations with remarkable energy efficiency, requiring approximately 20 W for the entire
human brain. Beyond energy savings, neuronal cultures offer a paradigm where computation and
memory coexist within the same substrate, which may interface directly with biological systems
(Gentili et al., 2024). The computational properties of neuronal cultures, detailed in earlier
chapters, suggest principles for artificial system design. From their self-organization toward
critical states optimizing information flow (Chapter 2), to their demonstrations of adaptability
and learning (Chapter 3), deviance detection, and predictive coding (Chapter 4), these networks
display capabilities crucial for efficient information processing, adaptation, and prediction.
Early studies revealed fundamental aspects of temporal processing in neural systems.
Buonomano and Maass (2009) demonstrated how cortical networks process spatiotemporal
information by encoding temporal sequences through transient activity patterns. They showed
how recurrent connections and short-term synaptic plasticity enable sequence recognition and
prediction. Nikolić et al. (2009) revealed that neurons in the visual cortex retain fading memories
of stimuli for several hundred milliseconds, using multi-electrode recordings to show that this
supports sequential processing. Later work by Enel et al. (2016) extended this by demonstrating
reservoir computing properties in the prefrontal cortex, showing how high-dimensional dynamics
allow adaptive decision-making through mixed selectivity, while Seoane (2019) examined
15
reservoir computing from an evolutionary perspective. Various approaches have emerged for
implementing neural computation in artificial systems. Abbott et al. (2016) tackled challenges in
building functional spiking networks, emphasizing stable excitation-inhibition balance and
scalable training mechanisms. Learning strategies in artificial systems have also drawn from
these findings: Diehl and Cook (2015) demonstrated unsupervised learning in spiking networks
with STDP, using spike rates to classify MNIST digits with competitive accuracy, while Nicola
and Clopath (2017) introduced FORCE training, stabilizing chaotic network dynamics to
reproduce complex temporal sequences like oscillations and trajectories. Reservoir computing
applications in neuronal cultures have revealed increasing sophistication in computational
capabilities. Dockendorf et al. (2009) demonstrated that cultured networks could act as liquid
state machines, effectively separating input patterns with high-frequency stimulation. Kubota et
al. (2019) identified the echo state property in cultured networks, which is crucial for
maintaining short-term memory and processing temporal information. They demonstrated that
cultured networks could hold transient states that preserve past inputs while enabling flexible
processing of new information. Using high-density multielectrode arrays, they systematically
tested various inter-pulse intervals (IPIs) and found that the optimal range, particularly between
20 and 30 ms, maximized reproducibility and differentiation of neural responses. This optimal
timing reflects the networks' ability to encode and process information with minimal interference
or loss. (Kubota et al., 2021) expanded on this work by quantifying the networks' information
processing capacity (IPC), a comprehensive metric capturing their computational versatility.
Their analysis revealed how memory capacity and generalization ability in these networks
depend on specific stimulation patterns, bridging physical reservoir computing with living neural
systems. (Suwa et al., 2022) demonstrated that dissociated cortical cultures possess both first-
order IPC (linear memory of past inputs) and second-order IPC (interactions of past inputs),
enabling them to perform arithmetic and logical operations on previous stimuli. Using high-
density CMOS microelectrode arrays, they quantified the computational capabilities of these
cultures, showing how spatiotemporal neural activity supports advanced processing tasks. Their
findings underscore the capacity of neuronal cultures to serve as living reservoirs for complex
computations, bridging the gap between biological and artificial systems while providing insights
into the intrinsic mechanisms underlying neural computation. Ikeda et al. (2023) further refined
these insights by investigating the dynamic interaction between evoked and spontaneous
16
activities. They highlighted the importance of evoked response intensity and identified
conditions, such as a 30-ms IPI, that optimize IPC. These findings collectively underscore the
potential of cultured networks to act as robust and adaptable computational substrates, providing
critical benchmarks for designing bio-inspired computing architectures. (Subramoney et al.,
2019, 2021) proposed the "Learning-to-Learn" framework, enabling spiking neural networks to
adapt rapidly to new tasks by leveraging meta-learning strategies. This framework fine-tunes
internal structures using task-specific feedback, optimizing performance across diverse
computational challenges. It exemplifies a scalable and efficient approach for building
neuromorphic systems that can generalize and adapt to novel inputs. Ishikawa et al. (2024)
integrated predictive coding principles with reservoir computing in spiking neural networks,
advancing the capacity for dynamic temporal processing. Predictive coding allows networks to
anticipate future inputs by leveraging past patterns, optimizing processing efficiency and
reducing computational redundancy. Their work demonstrates how combining these principles
enhances spiking neural networks' ability to process sensory information in real-time,
particularly in tasks requiring temporal predictions and interaction with dynamic environments.
The presence of noise in biological neural systems represents not a limitation but a crucial
computational resource that enables energy-efficient processing. Unlike digital computers, which
require high signal margins (e.g., 0 vs 5V) to maintain adequate signal-to-noise ratios, biological
networks harness noise for computation. Early studies revealed fundamental principles:
Matsumoto and Tsuda (1983) showed that noise stabilizes chaotic systems by reshaping
trajectories into periodic orbits, while Kirkpatrick et al. (1983) showed how noise-based
optimization through simulated annealing could solve complex problems. Gassmann (1997)
demonstrated noise-induced transitions between chaos and order, and Gammaitoni et al. (1998)
showed how stochastic resonance could enhance weak signal detection. Anderson et al. (2000)
revealed noise's role in maintaining visual contrast invariance, demonstrating how seemingly
random fluctuations support robust sensory processing. A comprehensive review by Faisal et al.
(2008) documented noise's pervasive and often beneficial role throughout nervous systems.
Subsequent work demonstrated specific computational advantages: Habenschuss et al. (2013)
showed how cortical circuits harness noise for stochastic computation, and Maass (2014)
established noise as a resource for learning in spiking networks. This noise-harnessing
17
computation represents an evolutionary adaptation that allows the brain to operate in harsh
biochemical environments while maintaining energy efficiency. Recent studies have revealed
specific mechanisms by which noise shapes neural computation in biological and artificial
systems. (Ikeda et al., 2023) demonstrated that noise interacts with spike-timing-dependent
plasticity to drive self-organized criticality in spiking neural networks. They showed that
moderate noise levels optimize critical dynamics while maintaining stable synaptic structures,
whereas excessive noise disrupts network stability and computational capacity. Ikeda et al.
(2024) further revealed how noise-driven spontaneous activity serves broader computational
functions, maintaining criticality and supporting memory consolidation through homeostatic
processes. These findings suggest that incorporating controlled noise in neuromorphic systems
might improve their adaptability and computational efficiency, paralleling the sophisticated use
of noise observed in biological neural networks.
These developments continue to drive advances in neuromorphic computing. The computational
properties observed in neuronal cultures—from criticality to noise-harnessing computation—
provide blueprints for energy-efficient, adaptive architectures. The field moves toward systems
that may operate synergistically with living neural tissue, combining the advantages of biological
and artificial computation.
7. Conclusions and Future Directions
Dissociated neuronal cultures serve as powerful, simplified model systems for examining
fundamental neural processes. As detailed in this review, these cultures exhibit complex
dynamics characteristic of self-organized criticality and adaptive computation (Chapter 2),
demonstrate learning and memory formation through various plasticity mechanisms (Chapter 3),
and show predictive processing and deviance detection capabilities consistent with theoretical
frameworks like the free energy principle (Chapter 4). They have also demonstrated the capacity
for goal-directed behaviors in controlled, closed-loop environments, further illustrating their
potential as computational models (Chapter 5). These discoveries not only enhance our
understanding of biological neuronal function but also provide insights that could influence the
design of future artificial neural networks and computational architectures due to the unique
blend of simplicity, adaptability, and controllability found in these in vitro systems (Chapter 6).
18
While our current understanding of dissociated neuronal cultures is robust, several avenues
remain open for deepening our knowledge and refining the practical applications of these
systems. Continued advancements in microelectrode array (MEA) technology are expected to
enable more precise recordings and manipulations of neuronal activity in dissociated cultures,
allowing for an even deeper exploration of their computational properties. Ravula et al. (2007)
pioneered early work in this direction by developing a microfabricated compartmentalized
culture system that leveraged microfluidics for precise spatial and temporal control over neuronal
microenvironments. This design improved upon traditional compartment-based methods by
facilitating reliable fluid isolation and collagen-guided axonal growth, thus enabling
simultaneous electrophysiological recordings and drug exposures. Improvements in the spatial
and temporal resolution of MEAs may further clarify how specific patterns of connectivity and
synaptic plasticity underlie adaptive computations and dynamic behavior in neuronal networks.
New MEA designs featuring modularity—such as multi-well MEAs that physically and
functionally separate different neuronal populations—also hold the potential to recreate
hierarchical and modular network structures in vitro. Bisio et al. (2014), for instance,
demonstrated how modular networks grown on polydimethylsiloxane (PDMS) structures can
exhibit higher firing rates during early development and display unique synchronization
properties compared to uniform networks, shedding light on hierarchical organization. Building
on this foundation, Joo and Nam (2019) introduced an agarose-based microwell patterning
method, enabling the recording of slow-wave activity from micro-sized neural clusters while
preserving high-frequency spiking information. Negri et al. (2020) refined protocols for multi-
well MEA experiments—providing a spike-sorting pipeline and statistical methodologies to
improve reproducibility—while also highlighting the importance of proper experimental design.
More recent work by (Gladkov et al., 2017, 2021) and Duru et al. (2022) has further extended the
engineering of biological neural networks by integrating microstructures with high-density
CMOS arrays. These approaches not only confine axonal outgrowth to specific channels,
creating reproducible unidirectional connectivity, but also offer subcellular-resolution recordings
of directed spike propagation. Finally, (Sumi et al., 2023) revealed how increasing network
modularity enhances reservoir computing performance in biological neuronal networks, enabling
improved classification accuracy in both spatial and temporal tasks. Their findings suggest that
19
structured designs can foster dynamic states conducive to advanced processing and short-term
memory.
Looking ahead, research into three-dimensional neuronal culture systems and brain organoids
may reveal how increasing the complexity of these in vitro models affects network organization
and computation. By introducing additional layers of structural and functional complexity,
researchers can investigate how hierarchical connectivity and layered processing influence
predictive coding, learning, and memory. Such 3D cultures and organoids more closely mimic
the architecture of in vivo brain tissue, potentially providing deeper insights into complex
cognitive functions and developmental processes. However, even as complexity increases, the
fundamental simplicity and controllability of these cultures remain advantageous, allowing
precise manipulation and observation of the network’s activity (Hogberg et al., 2013; Lancaster
et al., 2013; Clevers, 2016; Smirnova and Hartung, 2022, 2024).
Future studies will likely explore how the principles uncovered in dissociated neuronal cultures
generalize to more complex neural systems. While introducing 3D structures and organoids adds
realism, it is the balance between complexity and controllability that makes these models so
valuable. Researchers will need to maintain the simplicity that allows for precise control and
manipulation, ensuring that the systems remain tractable for in-depth investigations of network
function. By carefully scaling complexity, it is possible to examine how additional layers of
organization and connectivity influence predictive processing and adaptive computation without
losing the crucial benefits of simplicity. There is also significant potential for an increased
synergy between experimental neuroscience and computational modeling. As our ability to
record and manipulate neuronal activity improves, so does our capacity to develop and refine
computational models that can predict network behavior. These models can, in turn, guide
experimental interventions, allowing researchers to probe network function more systematically.
This iterative process between experimentation and modeling may help identify the principles
underpinning self-organization, learning, and prediction in neural networks and aid in translating
these insights into artificial systems.
In summary, dissociated neuronal cultures remain an invaluable model system for exploring
fundamental aspects of neuronal function and computation, particularly the mechanisms
20
underlying self-organized prediction. They have proven essential in examining how networks
self-organize, learn, and adapt, providing a simplified and controllable environment to study
complex neural phenomena that underlie predictive processing. As researchers continue to
balance the simplicity of these systems with increasing complexity—our understanding will
deepen further. These insights not only elucidate how biological brains function through
prediction and adaptation but also inspire the next generation of computational architectures and
neurotechnological applications.
Figure legends
21
Figure 1: Evolution of microelectrode array (MEA) technology for studying neuronal
networks. (A) The MEA system, featuring a transparent glass substrate with 60 microelectrodes
spaced at 200 µm. This design provides sufficient spatial resolution for capturing network-level
neuronal activity and allows for optical imaging of the culture. The system is capable of both
extracellular recording and stimulation for long-term culture studies. (B) Bright-field microscopy
of dissociated neuronal cultures grown on the MEA platform. The electrode array beneath the
neuronal layer supports the self-organization of functional networks while enabling the
simultaneous observation of culture morphology and recording of extracellular signals. Scale bar
22
= 100 µm. (C) Extracellular spike recordings from MEA, demonstrating its capacity to capture
neuronal activity from multiple electrodes simultaneously. The recording resolution and
electrode layout enable the analysis of network activity patterns and dynamic behaviors. (D)
High-dense CMOS-based MEA system (MaxOne), incorporating 26,400 platinum electrodes
with a 17.5 µm pitch. This CMOS-MEA provides subcellular spatial resolution for recording and
stimulation, enabling the detailed investigation of localized neuronal activity and network
interactions. (E) Schematic overlay of a neuron (green) interacting with electrodes (red) on a
CMOS- MEA. The figure illustrates how neuronal somas and processes align with the electrode
array. The red electrodes in close proximity to the soma demonstrate the ability of high-density
CMOS arrays to monitor and stimulate activity at a single-cell resolution. The scale bar indicates
the high spatial resolution provided by this system, with electrodes spaced at approximately 17.5
µm. (F) CMOS-MEA monitoring an action potential generated from the soma. Immunostaining
image of a neuron on the CMOS MEA is overlaid with spatially localized extracellular spike
sources. The high-density electrode array enables the resolution of neuronal activity at
subcellular precision, revealing fine-scale functional properties of single neurons and their
interactions with the network. Scale bar = 30 µm.
23
24
Figure 2: Developmental transition toward self-organized criticality (SoC) in dissociated
neuronal cultures. (A) Spatial maps of action potential amplitudes recorded using high-density
CMOS MEAs at different developmental stages: 4 days in vitro (DIV), 7 DIV, and 16 DIV.
Black circles mark recording sites, and the heatmap represents voltage amplitudes (color scale:
−400 to 100 µV). Scale bar = 200 µm. (B) Representative spike waveforms recorded at selected
electrodes (indicated by black circles in (A)) across developmental stages. Grey lines depict raw
spike traces, while red lines indicate averaged spike waveforms. Scale bars = 1 ms, 100 µV. (C)
Raster plots of spontaneous spiking activity from 120 s of recorded data for the same cultures at
4, 7, and 16 DIV, illustrating the emergence of synchronized bursts over time. (D) Log-log plots
of neuronal avalanche size distributions at 4, 7, and 16 DIV. Exponential distributions dominate
early development (4 DIV), while bimodal distributions emerge at 7 DIV, and power-law
distributions characteristic of SoC appear by 16 DIV. Fitted red lines represent power-law
distributions, and blue lines indicate exponential fits. (E) Schematic representation of the
integration-fragmentation model for SoC emergence. Initially, neurons form weak excitatory
connections, generating exponential distributions. Large-scale avalanches emerge as connectivity
strengthens, leading to a bimodal distribution. Finally, synaptic pruning and the balance of
excitation and inhibition result in diverse avalanche sizes distributed according to a power-law.
Figure reproduced from Yada et al. (2017), "Development of neural population activity toward
self-organized criticality," Neuroscience, 343, 55–65.
25
Figure 3: Experimental paradigm and deviance detection responses in neuronal networks.
(A) Electrode map from a high-density CMOS microelectrode array showing the spatial
distribution of stimulating electrodes (red, blue, and green dots for Stim A, Stim B, and Stim C,
respectively) and recording sites (light blue dots). Stimuli were delivered at specific locations to
investigate network responses. (B) Stimulation protocols used in the oddball and many standards
control (MSC) paradigms. In the oddball paradigm, Stim A and Stim B were alternated as
standard (std) and deviant (dev) stimuli. In the MSC paradigm, multiple stimuli (Stim A, Stim B,
Stim C, etc.) were presented in random order to eliminate expectations of repetition. (C) Top:
Raster plots showing neural responses to standard (top) and deviant (bottom) stimuli. Each row
corresponds to a recording site, and black dots indicate spike times relative to the stimulus onset.
Deviant stimuli elicited stronger and more widespread responses compared to standards. Bottom:
Population peristimulus time histograms (p-PSTHs) comparing the number of spikes per time bin
across conditions. Deviant stimuli (red line) evoke higher firing rates and longer-lasting
responses than standard (black line) and MSC (blue line) conditions, particularly in the late
response phase (30-100 ms). Figure modified from Kubota et al. (2020).
26
Figure 4. Closed-loop system for goal directed behavior using a living neuronal culture. (A)
Schematic representation of the closed-loop system. Cortical neurons cultured on a
microelectrode array (MEA) generate spiking activity, which is recorded and processed via
FORCE learning to create a coherent signal. For FORCE learning, the feedback to the neuronal
network is provided via optical stimulation (using a digital micromirror device, DMD). (B)
Robot navigation task. Representative trajectories of a robot in a maze with obstacles toward a
designated goal (target zone highlighted in yellow) are shown. The robot's movements are
controlled by neuronal activity, with FORCE learning enabling adaptive task performance.
Electrical stimulation is applied when the robot hit an obstacle. Feedback from the
27
environment—through optical and electrical stimulation—guides the robot's trajectory toward
the goal. Figure adapted from Yada et al. (2021), "Physical reservoir computing with FORCE
learning in a living neuronal culture," Applied Physics Letters, 119, 173701.
Ethics statement
This study is a review of previously published research and does not involve any experiments
with human participants or animals performed by the authors. All referenced studies have
adhered to their respective ethical standards as stated in their publications.
Author Contributions
AY drafted the manuscript and organized its overall structure. ZZ conducted literature review
and provided critical feedback. DA contributed feedback and refinements across multiple
sections of the manuscript. TIS contributed feedback and refinements on deviant detection. ZC
provided substantial input throughout the manuscript, with significant contributions to the
section on goal-directed behavior. HT supervised the project, provided key references,
contributed to the theoretical framework, prepared the figures, and ensured the manuscript's
coherence and finalization. All authors contributed to the work and approved the final version of
the manuscript.
Funding
This work is partly supported by JSPS KAKENHI (23H03465, 23H04336, 24H01544,
24K20854), AMED (24wm0625401h0001), JST (JPMJPR22S8), the Asahi Glass Foundation,
and the Secom Science and Technology Foundation.
Acknowledgments
The authors acknowledge the use of ChatGPT (OpenAI, version 4o) for assistance in language
editing.
Conflict of Interest
The authors declare that the research was conducted in the absence of any commercial or
financial relationships that could be construed as a potential conflict of interest.
References
Abbott, L. F., DePasquale, B., and Memmesheimer, R. M. (2016). Building functional networks of
spiking model neurons. Nature Neuroscience 2016 19:3 19, 350–355. doi: 10.1038/nn.4241
Abbott, L. F., and Rohrkemper, R. (2007). A simple growth model constructs critical avalanche
networks. Prog Brain Res 165, 13–19. doi: 10.1016/S0079-6123(06)65002-4
28
Anderson, J. S., Lampl, I., Gillespie, D. C., and Ferster, D. (2000). The contribution of noise to
contrast invariance of orientation tuning in cat visual cortex. Science (1979) 290, 1968–1972.
doi: 10.1126/SCIENCE.290.5498.1968/ASSET/1A33D046-13E4-469C-923B-
7333817099B2/ASSETS/GRAPHIC/SE4709036004.JPEG
Bak, P. (1996). How Nature Works. How Nature Works. doi: 10.1007/978-1-4757-5426-1
Bak, P., Tang, C., and Wiesenfeld, K. (1987). Self-organized criticality: An explanation of the 1/f
noise. Phys Rev Lett 59, 381–384. doi: 10.1103/PHYSREVLETT.59.381
Bakkum, D. J., Chao, Z. C., and Potter, S. M. (2008a). Long-term activity-dependent plasticity of
action potential propagation delay and amplitude in cortical networks. PLoS One 3. doi:
10.1371/JOURNAL.PONE.0002088
Bakkum, D. J., Chao, Z. C., and Potter, S. M. (2008b). Spatio-temporal electrical stimuli shape
behavior of an embodied cortical network in a goal-directed learning task. J Neural Eng 5, 310–
323. doi: 10.1088/1741-2560/5/3/004
Bakkum, D. J., Frey, U., Radivojevic, M., Russell, T. L., Müller, J., Fiscella, M., et al. (2013). Tracking
axonal action potential propagation on a high-density microelectrode array across hundreds of
sites. Nat Commun 4. doi: 10.1038/NCOMMS3181
Balci, F., Ben Hamed, S., Boraud, T., Bouret, S., Brochier, T., Brun, C., et al. (2023). A response to
claims of emergent intelligence and sentience in a dish. Neuron 111, 604–605. doi:
10.1016/j.neuron.2023.02.009
Ballini, M., Muller, J., Livi, P., Chen, Y., Frey, U., Stettler, A., et al. (2014). A 1024-channel CMOS
microelectrode array with 26,400 electrodes for recording and stimulation of electrogenic cells
in vitro. IEEE J Solid-State Circuits 49, 2705–2719. doi: 10.1109/JSSC.2014.2359219
Baruchi, I., Volman, V., Raichman, N., Shein, M., and Ben-Jacob, E. (2008). The emergence and
properties of mutual synchronization in in vitro coupled cortical networks. European Journal of
Neuroscience 28, 1825–1835. doi: 10.1111/J.1460-9568.2008.06487.X
Bastos, A. M., Usrey, W. M., Adams, R. A., Mangun, G. R., Fries, P., and Friston, K. J. (2012).
Canonical microcircuits for predictive coding. Neuron 76, 695–711. doi:
10.1016/j.neuron.2012.10.038
Beggs, J. M., and Plenz, D. (2003). Neuronal Avalanches in Neocortical Circuits. Journal of
Neuroscience 23, 11167–11177. doi: 10.1523/JNEUROSCI.23-35-11167.2003
Beggs, J. M., and Timme, N. (2012). Being critical of criticality in the brain. Front Physiol 3 JUN. doi:
10.3389/FPHYS.2012.00163/PDF
Berdondini, L., Van Der Wal, P. D., Guenat, O., De Rooij, N. F., Koudelka-Hep, M., Seitz, P., et al.
(2005). High-density electrode array for imaging in vitro electrophysiological activity. Biosens
Bioelectron 21, 167–174. doi: 10.1016/J.BIOS.2004.08.011
29
Bilder, R. M., and Knudsen, K. S. (2014). Creative cognition and systems biology on the edge of
chaos. Front Psychol 5. doi: 10.3389/FPSYG.2014.01104/PDF
Bisio, M., Bosca, A., Pasquale, V., Berdondini, L., and Chiappalone, M. (2014). Emergence of
Bursting Activity in Connected Neuronal Sub-Populations. PLoS One 9, e107400. doi:
10.1371/JOURNAL.PONE.0107400
Buonomano, D. V., and Maass, W. (2009). State-dependent computations: spatiotemporal
processing in cortical networks. Nat Rev Neurosci 10, 113–125. doi: 10.1038/NRN2558
Cai, H., Ao, Z., Tian, C., Wu, Z., Liu, H., Tchieu, J., et al. (2023). Brain organoid reservoir computing
for artificial intelligence. Nat Electron 6, 1032–1039. doi: 10.1038/S41928-023-01069-W
Chao, Z. C., Bakkum, D. J., and Potter, S. M. (2007). Region-specific network plasticity in simulated
and living cortical networks: comparison of the center of activity trajectory (CAT) with other
statistics. J Neural Eng 4, 294–308. doi: 10.1088/1741-2560/4/3/015
Chao, Z. C., Bakkum, D. J., and Potter, S. M. (2008a). Shaping embodied neural networks for
adaptive goal-directed behavior. PLoS Comput Biol 4. doi: 10.1371/JOURNAL.PCBI.1000042
Chao, Z. C., Bakkum, D. J., and Potter, S. M. (2008b). Shaping Embodied Neural Networks for
Adaptive Goal-directed Behavior. PLoS Comput Biol 4, e1000042. doi:
10.1371/JOURNAL.PCBI.1000042
Chao, Z. C., Bakkum, D. J., Wagenaar, D. A., and Potter, S. M. (2005). Effects of Random External
Background Stimulation on Network Synaptic Stability After Tetanization: A Modeling Study.
Neuroinformatics 3, 263. doi: 10.1385/NI:3:3:263
Chialvo, D. R. (2010). Emergent complex neural dynamics. Nature Physics 2010 6:10 6, 744–750.
doi: 10.1038/nphys1803
Clevers, H. (2016). Modeling Development and Disease with Organoids. Cell 165, 1586–1597. doi:
10.1016/J.CELL.2016.05.082/ASSET/D0C13FDE-B890-47EA-B2CF-
50D1A6D46953/MAIN.ASSETS/GR4.JPG
DeMarse, T. B., Wagenaar, D. A., Blau, A. W., and Potter, S. M. (2001). The neurally controlled
animat: Biological brains acting with simulated bodies. Auton Robots 11, 305–310. doi:
10.1023/A:1012407611130/METRICS
Dias, I., Levers, M. R., Lamberti, M., Hassink, G. C., Van Wezel, R., and Le Feber, J. (2021).
Consolidation of memory traces in cultured cortical networks requires low cholinergic tone,
synchronized activity and high network excitability. J Neural Eng 18, 046051. doi:
10.1088/1741-2552/ABFB3F
Diehl, P. U., and Cook, M. (2015). Unsupervised learning of digit recognition using spike-timing-
dependent plasticity. Front Comput Neurosci 9, 149773. doi:
10.3389/FNCOM.2015.00099/BIBTEX
30
Dockendorf, K. P., Park, I., He, P., Príncipe, J. C., and DeMarse, T. B. (2009). Liquid state machines
and cultured cortical networks: The separation property. Biosystems 95, 90–97. doi:
10.1016/J.BIOSYSTEMS.2008.08.001
Dranias, M. R., Ju, H., Rajaram, E., and VanDongen, A. M. J. (2013). Short-term memory in
networks of dissociated cortical neurons. J. Neurosci. 33, 1940–53. doi:
10.1523/jneurosci.2718-12.2013
Duru, J., Küchler, J., Ihle, S. J., Forró, C., Bernardi, A., Girardin, S., et al. (2022). Engineered
Biological Neural Networks on High Density CMOS Microelectrode Arrays. Front Neurosci 16.
doi: 10.3389/FNINS.2022.829884
Enel, P., Procyk, E., Quilodran, R., and Dominey, P. F. (2016). Reservoir Computing Properties of
Neural Dynamics in Prefrontal Cortex. PLoS Comput Biol 12, e1004967. doi:
10.1371/JOURNAL.PCBI.1004967
Eytan, D., Brenner, N., and Marom, S. (2003). Selective adaptation in networks of cortical neurons. J
Neurosci 23.
Faisal, A. A., Selen, L. P. J., and Wolpert, D. M. (2008). Noise in the nervous system. Nature
Reviews Neuroscience 2008 9:4 9, 292–303. doi: 10.1038/nrn2258
Frey, U., Sanchez-Bustamante, C. D., Ugniwenko, T., Heer, F., Sedivy, J., Hafizovic, S., et al.
(2007). Cell recordings with a CMOS high-density microelectrode array. Annual International
Conference of the IEEE Engineering in Medicine and Biology - Proceedings, 167–170. doi:
10.1109/IEMBS.2007.4352249
Friedman, N., Ito, S., Brinkman, B. A. W., Shimono, M., Deville, R. E. L., Dahmen, K. A., et al.
(2012). Universal critical dynamics in high resolution neuronal avalanche data. Phys Rev Lett
108, 208102. doi: 10.1103/PHYSREVLETT.108.208102/FIGURES/4/MEDIUM
Friston, K. (2010). The free-energy principle: a unified brain theory? Nature Reviews Neuroscience
2010 11:2 11, 127–138. doi: 10.1038/nrn2787
Friston, K., Kilner, J., and Harrison, L. (2006). A free energy principle for the brain. J Physiol Paris
100, 70–87. doi: 10.1016/j.jphysparis.2006.10.001
Gammaitoni, L., Hänggi, P., Jung, P., and Marchesoni, F. (1998). Stochastic resonance. Rev Mod
Phys 70, 223. doi: 10.1103/RevModPhys.70.223
Gassmann, F. (1997). Noise-induced chaos-order transitions. Phys Rev E 55, 2215. doi:
10.1103/PhysRevE.55.2215
Gentili, P. L., Zurlo, M. P., and Stano, P. (2024). Neuromorphic engineering in wetware: the state of
the art and its perspectives. Front Neurosci 18. doi: 10.3389/FNINS.2024.1443121
31
Gladkov, A., Pigareva, Y., Kolpakov, V., Mukhina, I., Bukatin, A., Kazantsev, V., et al. (2021).
Bursting activity interplay in modular neural networks in vitro. Proceedings - 3rd International
Conference “Neurotechnologies and Neurointerfaces”, CNN 2021, 23–25. doi:
10.1109/CNN53494.2021.9580356
Gladkov, A., Pigareva, Y., Kutyina, D., Kolpakov, V., Bukatin, A., Mukhina, I., et al. (2017). Design of
Cultured Neuron Networks in vitro with Predefined Connectivity Using Asymmetric Microfluidic
Channels. Scientific Reports 2017 7:1 7, 1–14. doi: 10.1038/s41598-017-15506-2
Gross, G. W., Rieske, E., Kreutzberg, G. W., and Meyer, A. (1977). A new fixed-array multi-
microelectrode system designed for long-term monitoring of extracellular single unit neuronal
activity in vitro. Neurosci Lett 6, 101–105. doi: 10.1016/0304-3940(77)90003-9
Habenschuss, S., Jonke, Z., and Maass, W. (2013). Stochastic Computations in Cortical Microcircuit
Models. PLoS Comput Biol 9, e1003311. doi: 10.1371/JOURNAL.PCBI.1003311
Harrison, R. G. (1910). The outgrowth of the nerve fiber as a mode of protoplasmic movement.
Journal of Experimental Zoology 9, 787–846. doi: 10.1002/JEZ.1400090405
Hennequin, G., Agnes, E. J., and Vogels, T. P. (2017). Inhibitory Plasticity: Balance, Control, and
Codependence. Annu Rev Neurosci 40, 557–579. doi: 10.1146/ANNUREV-NEURO-072116-
031005
Hogberg, H. T., Bressler, J., Christian, K. M., Harris, G., Makri, G., O’Driscoll, C., et al. (2013).
Toward a 3D model of human brain development for studying gene/environment interactions.
Stem Cell Res Ther 4. doi: 10.1186/SCRT365
Huang, Y., and Rao, R. P. N. (2011). Predictive coding. Wiley Interdiscip Rev Cogn Sci 2, 580–593.
doi: 10.1002/WCS.142
Ikeda, N., Akita, D., and Takahashi, H. (2023). Noise and spike-time-dependent plasticity drive self-
organized criticality in spiking neural network: Toward neuromorphic computing. Appl Phys Lett
123. doi: 10.1063/5.0152633/18034850/023701_1_5.0152633.AM.PDF
Ikeda, N., Takahashi, H., Ikeda, N., and Takahashi, H. (2021). Learning in Dissociated Neuronal
Cultures by Low-frequency Stimulation. ITEIS 141, 654–660. doi: 10.1541/IEEJEISS.141.654
Ishikawa, Y., Shinkawa A, T., Sumi, T., Kato A, H., Yamamoto, H., and Katori, Y. (2024). Integrating
predictive coding with reservoir computing in spiking neural network model of cultured neurons.
Nonlinear Theory and Its Applications, IEICE 15, 432–442. doi: 10.1587/NOLTA.15.432
Isomura, T., and Friston, K. (2018). In vitro neural networks minimise variational free energy. Sci
Rep 8. doi: 10.1038/S41598-018-35221-W
Isomura, T., Kotani, K., and Jimbo, Y. (2015). Cultured Cortical Neurons Can Perform Blind Source
Separation According to the Free-Energy Principle. PLoS Comput Biol 11. doi:
10.1371/JOURNAL.PCBI.1004643
32
Isomura, T., Kotani, K., Jimbo, Y., and Friston, K. J. (2023). Experimental validation of the free-
energy principle with in vitro neural networks. Nature Communications 2023 14:1 14, 1–15. doi:
10.1038/s41467-023-40141-z
Jimbo, Y., Robinson, H., and Kawana, A. (1998). Strengthening of synchronized activity by tetanic
stimulation in cortical cultures: Application of planar electrode arrays. IEEE Trans Biomed Eng
45.
Jimbo, Y., Tateno, T., and Robinson, H. P. C. (1999). Simultaneous induction of pathway-specific
potentiation and depression in networks of cortical neurons. Biophys J 76, 670–678. doi:
10.1016/S0006-3495(99)77234-6
Joo, S., and Nam, Y. (2019). Slow-wave recordings from micro-sized neural clusters using multiwell
type microelectrode arrays. IEEE Trans Biomed Eng 66, 403–410. doi:
10.1109/TBME.2018.2843793
Ju, H., Dranias, M. R., Banumurthy, G., and Vandongen, A. M. J. (2015). Spatiotemporal Memory Is
an Intrinsic Property of Networks of Dissociated Cortical Neurons. Journal of Neuroscience 35,
4040–4051. doi: 10.1523/JNEUROSCI.3793-14.2015
Kagan, B. J., Kitchen, A. C., Tran, N. T., Habibollahi, F., Khajehnejad, M., Parker, B. J., et al. (2022).
In vitro neurons learn and exhibit sentience when embodied in a simulated game-world. Neuron
110, 3952-3969.e8. doi: 10.1016/J.NEURON.2022.09.001
Kamioka, H., Maeda, E., Jimbo, Y., Robinson, H. P. C., and Kawana, A. (1996). Spontaneous
periodic synchronized bursting during formation of mature patterns of connections in cortical
cultures. Neurosci Lett 206, 109–112. doi: 10.1016/S0304-3940(96)12448-4
Kayama, A., Yada, Y., and Takahashi, H. (2019). Development of network structure and
synchronized firing patterns in dissociated culture of neurons. Electronics and Communications
in Japan 102, 3–11. doi: 10.1002/ECJ.12199
Keller, G. B., and Mrsic-Flogel, T. D. (2018). Predictive Processing: A Canonical Cortical
Computation. Neuron 100, 424–435. doi: 10.1016/J.NEURON.2018.10.003
Kern, F. B., and Chao, Z. C. (2023). Short-term neuronal and synaptic plasticity act in synergy for
deviance detection in spiking networks. PLoS Comput Biol 19, e1011554. doi:
10.1371/JOURNAL.PCBI.1011554
Khajehnejad, M., Habibollahi, F., Paul, A., Razi, A., and Kagan, B. J. (2024). Biological Neurons
Compete with Deep Reinforcement Learning in Sample Efficiency in a Simulated Gameworld.
Available at: https://arxiv.org/abs/2405.16946v1 (Accessed November 15, 2024).
Kirkpatrick, S., Gelatt, C. D., and Vecchi, M. P. (1983). Optimization by Simulated Annealing.
Science (1979) 220, 671–680. doi: 10.1126/SCIENCE.220.4598.671
33
Kossio, F. Y. K., Goedeke, S., Van Den Akker, B., Ibarz, B., and Memmesheimer, R. M. (2018).
Growing Critical: Self-Organized Criticality in a Developing Neural System. Phys Rev Lett 121,
058301. doi: 10.1103/PHYSREVLETT.121.058301/FIGURES/4/MEDIUM
Kubota, T., Nakajima, K., and Takahashi, H. (2019). Echo State Property of Neuronal Cell Cultures.
Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence
and Lecture Notes in Bioinformatics) 11731 LNCS, 137–148. doi: 10.1007/978-3-030-30493-
5_13/FIGURES/5
Kubota, T., Takahashi, H., and Nakajima, K. (2021). Unifying framework for information processing
in stochastically driven dynamical systems. Phys Rev Res 3, 043135. doi:
10.1103/PHYSREVRESEARCH.3.043135/FIGURES/10/MEDIUM
Kuśmierz, Ł., Ogawa, S., and Toyoizumi, T. (2020). Edge of Chaos and Avalanches in Neural
Networks with Heavy-Tailed Synaptic Weight Distribution. Phys Rev Lett 125. doi:
10.1103/PHYSREVLETT.125.028101
Lamberti, M., Putten, M. J. A. M. van, Marzen, S., and Feber, J. le (2024). The role of NMDA
receptors in memory and prediction in cultured neural networks. bioRxiv, 2024.02.01.578348.
doi: 10.1101/2024.02.01.578348
Lamberti, M., Tripathi, S., van Putten, M. J. A. M., Marzen, S., and le Feber, J. (2023). Prediction in
cultured cortical neural networks. PNAS Nexus 2. doi: 10.1093/PNASNEXUS/PGAD188
Lancaster, M. A., Renner, M., Martin, C. A., Wenzel, D., Bicknell, L. S., Hurles, M. E., et al. (2013).
Cerebral organoids model human brain development and microcephaly. Nature 2013 501:7467
501, 373–379. doi: 10.1038/nature12517
Le Feber, J., Postma, W., de Weerd, E., Weusthof, M., and Rutten, W. L. C. (2015). Barbed
channels enhance unidirectional connectivity between neuronal networks cultured on multi
electrode arrays. Front Neurosci 9, 159060. doi: 10.3389/FNINS.2015.00412/BIBTEX
Le Feber, J., Rutten, W. L. C., Stegenga, J., Wolters, P. S., Ramakers, G. J. A., and Van Pelt, J.
(2007). Conditional firing probabilities in cultured neuronal networks: A stable underlying
structure in widely varying spontaneous activity patterns. J Neural Eng 4, 54–67. doi:
10.1088/1741-2560/4/2/006
Le Feber, J., Stegenga, J., and Rutten, W. L. C. (2010). The Effect of Slow Electrical Stimuli to
Achieve Learning in Cultured Networks of Rat Cortical Neurons. PLoS One 5, e8871. doi:
10.1371/JOURNAL.PONE.0008871
Le Feber, J., Stoyanova, I. I., and Chiappalone, M. (2014). Connectivity, excitability and activity
patterns in neuronal networks. Phys Biol 11. doi: 10.1088/1478-3975/11/3/036005
Levina, A., Herrmann, J. M., and Geisel, T. (2007). Dynamical synapses causing self-organized
criticality in neural networks. Nature Physics 2007 3:12 3, 857–860. doi: 10.1038/nphys758
34
Maass, W. (2014). Noise as a resource for computation and learning in networks of spiking neurons.
Proceedings of the IEEE 102, 860–880. doi: 10.1109/JPROC.2014.2310593
Maeda, E., Robinson, H. P. C., and Kawana, A. (1995). The mechanisms of generation and
propagation of synchronized bursting in developing networks of cortical neurons. Journal of
Neuroscience 15, 6834–6845. doi: 10.1523/JNEUROSCI.15-10-06834.1995
Marković, D., Mizrahi, A., Querlioz, D., and Grollier, J. (2020). Physics for neuromorphic computing.
Nature Reviews Physics 2020 2:9 2, 499–510. doi: 10.1038/s42254-020-0208-2
Marom, S., and Shahaf, G. (2002). Development, learning and memory in large random networks of
cortical neurons: lessons beyond anatomy. Q Rev Biophys 35, 63–87. doi:
10.1017/S0033583501003742
Masumori, A., Sinapayen, L., Maruyama, N., Mita, T., Bakkum, D., Frey, U., et al. (2020). Neural
Autopoiesis: Organizing Self-Boundaries by Stimulus Avoidance in Biological and Artificial
Neural Networks. Artif Life 26, 130–151. doi: 10.1162/ARTL_A_00314
Matsumoto, K., and Tsuda, I. (1983). Noise-induced order. J Stat Phys 31, 87–106. doi:
10.1007/BF01010923/METRICS
Mazzoni, A., Broccard, F. D., Garcia-Perez, E., Bonifazi, P., Ruaro, M. E., and Torre, V. (2007). On
the Dynamics of the Spontaneous Activity in Neuronal Networks. PLoS One 2, e439. doi:
10.1371/JOURNAL.PONE.0000439
Millman, D., Mihalas, S., Kirkwood, A., and Niebur, E. (2010). Self-organized criticality occurs in non-
conservative neuronal networks during ‘up’ states. Nature Physics 2010 6:10 6, 801–805. doi:
10.1038/nphys1757
Müller, J., Ballini, M., Livi, P., Chen, Y., Radivojevic, M., Shadmani, A., et al. (2015). High-resolution
CMOS MEA platform to study neurons at subcellular, cellular, and network levels. Lab Chip 15,
2767–2780. doi: 10.1039/C5LC00133A
Muñoz, M. A. (2018). Colloquium: Criticality and dynamical scaling in living systems. Rev Mod Phys
90, 031001. doi: 10.1103/REVMODPHYS.90.031001/FIGURES/6/THUMBNAIL
Negri, J., Menon, V., and Young-Pearse, T. L. (2020). Assessment of Spontaneous Neuronal Activity
In Vitro Using Multi-Well Multi-Electrode Arrays: Implications for Assay Development. eNeuro 7.
doi: 10.1523/ENEURO.0080-19.2019
Nicola, W., and Clopath, C. (2017). Supervised learning in spiking neural networks with FORCE
training. Nat Commun 8, 1–15. doi: 10.1038/s41467-017-01827-3
Nigam, S., Shimono, M., Ito, S., Yeh, F. C., Timme, N., Myroshnychenko, M., et al. (2016). Rich-Club
Organization in Effective Connectivity among Cortical Neurons. Journal of Neuroscience 36,
670–684. doi: 10.1523/JNEUROSCI.2177-15.2016
35
Nikolić, D., Usler, S. H., Singer, W., and Maass, W. (2009). Distributed Fading Memory for Stimulus
Properties in the Primary Visual Cortex. PLoS Biol 7, e1000260. doi:
10.1371/JOURNAL.PBIO.1000260
Obien, M. E. J., Deligkaris, K., Bullmann, T., Bakkum, D. J., and Frey, U. (2015). Revealing neuronal
function through microelectrode array recordings. Front Neurosci 9, 423. doi:
10.3389/FNINS.2014.00423/BIBTEX
Orlandi, J. G., Soriano, J., Alvarez-Lacalle, E., Teller, S., and Casademunt, J. (2013). Noise focusing
and the emergence of coherent activity in neuronal cultures. Nature Physics 2013 9:9 9, 582–
590. doi: 10.1038/nphys2686
Pasquale, V., Massobrio, P., Bologna, L. L., Chiappalone, M., and Martinoia, S. (2008). Self-
organization and neuronal avalanches in networks of dissociated cortical neurons.
Neuroscience 153, 1354–1369. doi: 10.1016/J.NEUROSCIENCE.2008.03.050
Pelt, J. van, Wolters, P., Corner, M., Rutten, W., and Ramakers, G. (2004). Long-term
characterization of firing dynamics of spontaneous bursts in cultured neural networks. IEEE
Trans Biomed Eng 51.
Petermann, T., Thiagarajan, T. C., Lebedev, M. A., Nicolelis, M. A. L., Chialvo, D. R., and Plenz, D.
(2009). Spontaneous cortical activity in awake monkeys composed of neuronal avalanches.
Proc Natl Acad Sci U S A 106, 15921–15926. doi:
10.1073/PNAS.0904089106/SUPPL_FILE/0904089106SI.PDF
Pine, J. (1980). Recording action potentials from cultured neurons with extracellular microcircuit
electrodes. J Neurosci Methods 2, 19–31. doi: 10.1016/0165-0270(80)90042-4
Pine, J. (2006). A History of MEA Development. Advances in Network Electrophysiology: Using
Multi-Electrode Arrays, 3–23. doi: 10.1007/0-387-25858-2_1
Plenz, D., Ribeiro, T. L., Miller, S. R., Kells, P. A., Vakili, A., and Capek, E. L. (2021). Self-Organized
Criticality in the Brain. Front Phys 9, 639389. doi: 10.3389/FPHY.2021.639389/BIBTEX
Potter, S. M., and DeMarse, T. B. (2001). A new approach to neural cell culture for long-term
studies. J Neurosci Methods 110, 17–24. doi: 10.1016/S0165-0270(01)00412-5
Potter, S. M., DeMarse, T., Bakkum, D., Booth, M. C., Brumfield, J., Chao, Z. C., et al. (2004).
HYBROTS: HYBRIDS OF LIVING NEURONS AND ROBOTS FOR STUDYING NEURAL
COMPUTATION.
Potter, S. M., Fraser, S., and Caltech, J. P. (1997). Animat in a petri dish: cultured neural networks
for studying neural computation. In Proc. 4th Joint Symposium on Neural Computation, UCSD
(pp. 167-174).
Potter, S. M., Wagenaar, D. A., Madhavan, R., and DeMarse, T. B. (2003). Long-Term Bidirectional
Neuron Interfaces for Robotic Control, and In Vitro Learning Studies. Annual International
36
Conference of the IEEE Engineering in Medicine and Biology - Proceedings 4, 3690–3693. doi:
10.1109/IEMBS.2003.1280959
Rao, R. P. N., and Ballard, D. H. (1999). Predictive coding in the visual cortex: a functional
interpretation of some extra-classical receptive-field effects. Nature Neuroscience 1999 2:1 2,
79–87. doi: 10.1038/4580
Ravula, S. K., Wang, M. S., Asress, S. A., Glass, J. D., and Bruno Frazier, A. (2007). A
compartmented neuronal culture system in microdevice format. J Neurosci Methods 159, 78–
85. doi: 10.1016/J.JNEUMETH.2006.06.022
Ruaro, M. E., Bonifazi, P., and Torre, V. (2005). Toward the neurocomputer: Image processing and
pattern recognition with neuronal cultures. IEEE Trans Biomed Eng 52, 371–383. doi:
10.1109/TBME.2004.842975
Sadeh, S., and Clopath, C. (2020). Inhibitory stabilization and cortical computation. Nature Reviews
Neuroscience 2020 22:1 22, 21–37. doi: 10.1038/s41583-020-00390-z
Schroeter, M. S., Charlesworth, P., Kitzbichler, M. G., Paulsen, O., and Bullmore, E. T. (2015).
Emergence of rich-club topology and coordinated dynamics in development of hippocampal
functional networks in vitro. J Neurosci 35, 5459–5470. doi: 10.1523/JNEUROSCI.4259-
14.2015
Segev, R., Shapira, Y., Benveniste, M., and Ben-Jacob, E. (2001). Observations and modeling of
synchronized bursting in two-dimensional neural networks. Phys Rev E 64, 011920. doi:
10.1103/PhysRevE.64.011920
Seoane, L. F. (2019). Evolutionary aspects of reservoir computing. Philosophical Transactions of the
Royal Society B 374. doi: 10.1098/RSTB.2018.0377
Shahaf, G., and Marom, S. (2001). Learning in networks of cortical neurons. J Neurosci 21, 8782–
8788. doi: 10.1523/JNEUROSCI.21-22-08782.2001
Shew, W. L., and Plenz, D. (2013). The functional benefits of criticality in the cortex. Neuroscientist
19, 88–100. doi: 10.1177/1073858412445487
Shew, W. L., Yang, H., Petermann, T., Roy, R., and Plenz, D. (2009). Neuronal Avalanches Imply
Maximum Dynamic Range in Cortical Networks at Criticality. Journal of Neuroscience 29,
15595–15600. doi: 10.1523/JNEUROSCI.3864-09.2009
Smirnova, L., Caffo, B. S., Gracias, D. H., Huang, Q., Morales Pantoja, I. E., Tang, B., et al. (2023).
Organoid intelligence (OI): the new frontier in biocomputing and intelligence-in-a-dish. Frontiers
in Science 1, 1017235. doi: 10.3389/FSCI.2023.1017235
Smirnova, L., and Hartung, T. (2022). Neuronal cultures playing Pong: First steps toward advanced
screening and biological computing. Neuron 110, 3855–3856. doi:
10.1016/j.neuron.2022.11.010
37
Smirnova, L., and Hartung, T. (2024). The Promise and Potential of Brain Organoids. Adv Healthc
Mater 13, 2302745. doi: 10.1002/ADHM.202302745
Soriano, J., Martínez, M. R., Tlusty, T., and Moses, E. (2008). Development of input connections in
neural cultures. Proc Natl Acad Sci U S A 105, 13758–13763. doi:
10.1073/PNAS.0707492105/SUPPL_FILE/0707492105SI.PDF
Subramoney, A., Scherr, F., and Maass, W. (2019). Reservoirs learn to learn. Natural Computing
Series, 59–76. doi: 10.1007/978-981-13-1687-6_3
Subramoney, A., Scherr, F., and Maass, W. (2021). Reservoirs Learn to Learn. Natural Computing
Series, 59–76. doi: 10.1007/978-981-13-1687-6_3/FIGURES/4
Sumi, T., Yamamoto, H., Katori, Y., Ito, K., Moriya, S., Konno, T., et al. (2023). Biological neurons
act as generalization filters in reservoir computing. Proc Natl Acad Sci U S A 120,
e2217008120. doi: 10.1073/PNAS.2217008120/SUPPL_FILE/PNAS.2217008120.SAPP.PDF
Suwa, E., Kubota, T., Ishida, N., and Takahashi, H. (2022). 神経細胞の分散培養系の情報処理容量.
電気学会論文誌Ｃ（電子・情報・システム部門誌） 142, 578–585. doi:
10.1541/IEEJEISS.142.578
Tanaka, G., Yamane, T., Héroux, J. B., Nakane, R., Kanazawa, N., Takeda, S., et al. (2019). Recent
advances in physical reservoir computing: A review. Neural Networks 115, 100–123. doi:
10.1016/J.NEUNET.2019.03.005
Tessadori, J., Bisio, M., Martinoia, S., and Chiappalone, M. (2012). Modular neuronal assemblies
embodied in a closed-loop environment: Toward future integration of brains and machines.
Front Neural Circuits 6, 33917. doi: 10.3389/FNCIR.2012.00099/ABSTRACT
Tetzlaff, C., Okujeni, S., Egert, U., Wörgötter, F., and Butz, M. (2010). Self-Organized Criticality in
Developing Neuronal Networks. PLoS Comput Biol 6, e1001013. doi:
10.1371/JOURNAL.PCBI.1001013
Thomas, C. A., Springer, P. A., Loeb, G. E., Berwald-Netter, Y., and Okun, L. M. (1972). A miniature
microelectrode array to monitor the bioelectric activity of cultured cells. Exp Cell Res 74, 61–66.
doi: 10.1016/0014-4827(72)90481-8
Van Vreeswijk, C., and Sompolinsky, H. (1996). Chaos in neuronal networks with balanced
excitatory and inhibitory activity. Science 274, 1724–1726. doi:
10.1126/SCIENCE.274.5293.1724
Vogels, T. P., Sprekeler, H., Zenke, F., Clopath, C., and Gerstner, W. (2011). Inhibitory plasticity
balances excitation and inhibition in sensory pathways and memory networks. Science 334,
1569–1573. doi: 10.1126/SCIENCE.1211095
38
Wagenaar, D. A., Madhavan, R., Pine, J., and Potter, S. M. (2005). Controlling Bursting in Cortical
Cultures with Closed-Loop Multi-Electrode Stimulation. Journal of Neuroscience 25, 680–688.
doi: 10.1523/JNEUROSCI.4209-04.2005
Wagenaar, D. A., Pine, J., and Potter, S. M. (2006). An extremely rich repertoire of bursting patterns
during the development of cortical cultures. BMC Neurosci 7. doi: 10.1186/1471-2202-7-11
Yada, Y., Mita, T., Sanada, A., Yano, R., Kanzaki, R., Bakkum, D. J., et al. (2017). Development of
neural population activity toward self-organized criticality. Neuroscience 343, 55–65. doi:
10.1016/J.NEUROSCIENCE.2016.11.031
Yada, Y., Yasuda, S., and Takahashi, H. (2021). Physical reservoir computing with FORCE learning
in a living neuronal culture. Appl Phys Lett 119. doi: 10.1063/5.0064771/1065020
Yaron, A., Hershenhoren, I., and Nelken, I. (2012). Sensitivity to Complex Statistical Regularities in
Rat Auditory Cortex. Neuron 76, 603–615. doi: 10.1016/J.NEURON.2012.08.025
39

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Dissociated Neuronal Cultures as Model Systems for Self-Organized Prediction"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
