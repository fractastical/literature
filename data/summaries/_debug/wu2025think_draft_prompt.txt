=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Think How Your Teammates Think: Active Inference Can Benefit Decentralized Execution
Citation Key: wu2025think
Authors: Hao Wu, Shoucheng Song, Chang Yao

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Abstract: Inmulti-agentsystems,explicitcognitionofteammatesâ€™de-
How Teammate
cision logic serves as a critical factor in facilitating coordi-
nation. Communication (i.e., â€œTellâ€) can assist in the cog- Thinks
nitive development process by information dissemination,
yet it is inevitably subject to real-world constraints such Perception Belief Action
as noise, latency, and attacks. Therefore, building the un-
derstandingofteammatesâ€™decisionswithoutcommunication
remains challenging. To address this, we propo...

Key Terms: execution, process, your, belief, action, perception, teammate, beijing, inference, benefit

=== FULL PAPER TEXT ===

Think How Your Teammates Think: Active Inference Can Benefit
Decentralized Execution
HaoWu1,2*,ShouchengSong1,2*,ChangYao1,2,ShengHan1,2,
HuaiyuWan1,2,YoufangLin1,2,KaiLv1,2â€ 
1SchoolofComputerScience&Technology,BeijingJiaotongUniversity,Beijing,China
2BeijingKeyLaboratoryofTrafficDataMiningandEmbodiedIntelligence,Beijing,China
{wuhao ,insis songsc,yaochang,shhan,hywan,yflin,lvkai}@bjtu.edu.cn
Abstract
Inmulti-agentsystems,explicitcognitionofteammatesâ€™de-
How Teammate
cision logic serves as a critical factor in facilitating coordi-
nation. Communication (i.e., â€œTellâ€) can assist in the cog- Thinks
nitive development process by information dissemination,
yet it is inevitably subject to real-world constraints such Perception Belief Action
as noise, latency, and attacks. Therefore, building the un-
derstandingofteammatesâ€™decisionswithoutcommunication
remains challenging. To address this, we propose a novel Agent i
non-communicationMARLframeworkthatrealizesthecon-
structionofcognitionthroughlocalobservation-basedmod-
eling(i.e.,â€œThinkâ€).Ourframeworkenablesagentstomodel
teammatesâ€™ active inference process. At first, the proposed
methodproducesthreeteammateportraits:perception-belief-
Figure 1: Modeling the Active Inference Process of the
action.Specifically,wemodeltheteammateâ€™sdecisionpro-
Teammate. In this scenario, agent i models the perception-
cess as follows: 1) Perception: observing environments; 2)
belief-actioninvolvedinitsteammateâ€™sactiveinferencepro-
Belief: forming beliefs; 3) Action: making decisions. Then,
we selectively integrate the belief portrait into the decision
cesswhenfacingthegoalkeeper.Thisallowsitoobtainits
process based on the accuracy and relevance of the percep- teammateâ€™sdecision-relevantinformationandachieveeffec-
tionportrait.Thisenablestheselectionofcooperativeteam- tivecollaboration.
matesandfacilitateseffectivecollaboration.Extensiveexper-
imentsontheSMAC,SMACv2,MPE,andGRFbenchmarks
demonstratethesuperiorperformanceofourmethod.
bandwidth, high latency, and significant noise (Tung et al.
2021; Hu et al. 2023; Song et al. 2025). Therefore, we ex-
Introduction
plorethemethodfordevelopingthecognitionofteammatesâ€™
Multi-agent reinforcement learning (MARL) has garnered decisionsintheabsenceofcommunication.
significant attention due to its wide applications in fields
In contrast to directly telling an agent how its team-
suchasautonomousdriving(Kiranetal.2021),smartgrids
mates act, we advocate that agents could engage in â€œThink
(Roesch et al. 2020), and transportation (Lee, Chung, and
howyourteammatesthinkâ€.Specifically,theagentactively
Sohn2019).Indecentralizedsystems,thelackofcognition
builds comprehension of teammatesâ€™ decisions. To achieve
regardingteammatesâ€™decisionlogicmayinducemiscoordi-
this,thedirectmethodistomodelteammatesâ€™decisionpro-
nationamongagentsandresultinsuboptimalpolicies.
cess.However,existingagentmodelingmethodsfailtoful-
To address the mentioned issue, one intuitive approach
fill this. On the one hand, some methods rely on access to
is â€œTell agent how its teammates actâ€, which can be im-
other agentsâ€™ trajectories during the modeling (Rabinowitz
plemented through communication mechanisms. The com-
et al. 2018; Zintgraf et al. 2021), which is unavailable dur-
munication (i.e., â€œTellâ€) methods can facilitate the under-
ingdecentralizedexecution.Ontheotherhand,somemeth-
standing of teammatesâ€™ behaviors by exchanging decision-
ods typically only enable a single agent to model other
relevantmessages(Wangetal.2019;Yuanetal.2022;Sun
agents that possess fixed parameters (Xie et al. 2021; Pa-
etal.2023a,2024).However,communicationeffectiveness
poudakis,Christianos,andAlbrecht2021;Yu,Jiang,andLu
may be limited under some conditions, including limited
2024). This configuration imposes an upper bound on the
systemâ€™s collaborative efficiency, preventing the team from
*Theseauthorscontributedequally.
â€ CorrespondingAuthor:KaiLv(lvkai@bjtu.edu.cn). learningmoreoptimalpolicies.Furthermore,thesemethods
CopyrightÂ©2026,AssociationfortheAdvancementofArtificial only model incomplete decision components (e.g., behav-
Intelligence(www.aaai.org).Allrightsreserved. iors or intentions), risking inaccuracies from discrepancies
5202
voN
42
]AM.sc[
1v16781.1152:viXra
betweenthemodelandtheactualsituation.Toaddressthis, well (Tung et al. 2021; Hu et al. 2023; Song et al. 2025).
we rely solely on local observations to model teammatesâ€™ Furthermore,thecommunicationattackmayintroducema-
complete decision processes, reducing modeling inaccura- liciousinformation,disruptingagentsâ€™decision-makingand
ciesduringdecentralizedexecution. hinderingcollaboration(Xueetal.2021;Zhu,Dastani,and
Inspired by human brain decision-making mechanisms Wang 2024). In comparison, our method adopts an alter-
and active inference theory (Friston et al. 2016), we model nativeperspective.Weproposeanovelcommunication-free
theteammateâ€™sdecisionprocessasanactiveinferencepro- frameworkthatmodelsteammatesâ€™activeinferencetocom-
cess comprising perception-belief-action. In this process, prehensivelyunderstandtheirdecisionlogic.
the teammate perceives environments, forms beliefs, and
then takes actions by integrating perceptions and beliefs. AgentModelingMethods
Consequently, we employ local observation-based model- Methodsbasedonagentmodelingtypicallyacquireagentsâ€™
ing(i.e.,â€œThinkâ€)toacquireteammatesâ€™perception-belief- behaviors, beliefs, or intentions. For example, (Rabinowitz
action (i.e., â€œHow your teammates thinkâ€). Figure 1 pro- etal.2018;Yangetal.2018;Tianetal.2019;Zintgrafetal.
videsanillustrativeexampleofthismodelingprocess. 2021; Zhai et al. 2023) model agentsâ€™ psychological state
In this paper, we propose a novel non-communication and beliefs using the Theory of Mind and Bayesian rea-
frameworkformodelingtheActiveInferenceofteammates soning. (He et al. 2016; Raileanu et al. 2018) infer agentsâ€™
in MARL (AIM). The framework consists of two parts: At policy based on the modeled agentsâ€™ observations and ac-
first, we develop a modeling method to model teammatesâ€™ tions.(PapoudakisandAlbrecht2020)modelagentsâ€™policy
threeportraits:perception-belief-action,solelybasedonlo- as a latent distribution. However, these methods rely on a
calobservations.Meanwhile,theperceptionandbeliefpor- strong assumption that agents can directly access the mod-
traitsareoptimizedbyminimizingthediscrepancybetween eled agentâ€™ trajectories, which is infeasible during the de-
predictedandactualactions.Then,weproposeadual-filter centralizedexecution.
mechanism to enhance teammatesâ€™ cognition utilization. Although (Papoudakis, Christianos, and Albrecht 2021;
This mechanism features selective collaboration by choos- Xieetal.2021;Yu,Jiang,andLu2024)modelotheragents
ing teammates whose modeled portraits have high accu- solely based on local observations, they maintain team-
racy. Additionally, by considering the perception relevance matesâ€™ policies as fixed parameters. Nevertheless, collabo-
amongagents,weadoptanattentionmoduletodynamically rationperformancebecomesconstrainedbyfixedteammate
integrateteammatesâ€™beliefportraits,therebyoptimizingthe parameters, preventing team policy optimization. More-
decisionprocess.Ourproposedmethoddemonstratessignif- over, the above methods only partially model other agentsâ€™
icantimprovementintaskswithinSMAC(Samvelyanetal. decision-makingprocesses,introducinginevitableinaccura-
2019), SMACv2 (Ellis et al. 2024), MPE (Mordatch and ciesandnegativelyimpactingcooperativeperformanceow-
Abbeel2018)andGRF(Kurachetal.2020). ing to mismatches between the model and reality. In con-
Ourcontributionsareoutlinedasfollows: trast,ourframeworkmodelsteammatesâ€™completedecision-
making process through three key aspects: perception-
â€¢ We replace â€œcommunication (i.e., Tell)â€ with â€œmodel-
belief-actiontoincreasemodelingaccuracy.Inthisway,our
ing (i.e., Think)â€, enabling agents to construct the cog-
methodenablesagentstounderstandhowtheythink.
nitionofteammatesâ€™decisionlogicwithoutcommunica-
tionduringdecentralizedexecution.
Background
â€¢ We propose an active inference framework to model
A fully cooperative multi-agent task can be modeled as a
teammatesâ€™ three portraits: perception-belief-action, to
Decentralized Partially Observable Markov Decision Pro-
understandhowtheythink.
cess (Dec-POMDP) (Oliehoek, Amato et al. 2016), repre-
â€¢ We introduce a dual filter that leverages the accuracy sented as a tuple âŸ¨I,S,{A }N ,{â„¦ }N ,O,T,R,Î³âŸ©. In
i i=1 i i=1
and relevance of perception portraits to select coopera- thismodel,I ={1,...,N}isthesetofagentsandN isthe
tiveteammates. numberofagents.Sisthestatespace.ForeachagentiâˆˆI,
â€¢ WeconductexperimentsonSMAC,SMACv2,MPE,and A i istheindividualactionspace,andA = A 1 Ã—Â·Â·Â·Ã—A N
GRF.Theresultsshowthatourmethodachievesoptimal is the joint action space. â„¦ i is the observation space for
ornear-optimalperformanceinmostscenarios. agent i. O(o i | s,i) is the observation function over local
observations o âˆˆ â„¦ given state s âˆˆ S and agent i. The
i i
RelatedWorks statetransitionfunctionT(sâ€² | s,a)definestheprobability
of transitioning to state sâ€² âˆˆ S given current state s âˆˆ S
CommunicationinMARL
andjointactiona âˆˆ A.TherewardfunctionR(s,a)gives
Several communication methods, such as (Das et al. 2019; the immediate reward, and Î³ âˆˆ [0,1) is the discount fac-
Ding, Huang, and Lu 2020; Yuan et al. 2022; Sun et al. tor, used to balance long-term and short-term rewards. At
2023b; Sun 2024; Li et al. 2025; Yao et al. 2025), design each time step t, agent i âˆˆ I receives a local observation
communication networks that enable agents to exchange o âˆ¼ O(Â· | s,i),selectsanactiona âˆˆ A ,andthejointac-
i i i
decision-relevant messages during the decentralized exe- tiona = âŸ¨a ,...,a âŸ©resultsinastatetransitionandare-
1 N
cution. However, in some real-world scenarios, limitations wardR(s,a).Thegoalofmulti-agentsystemalgorithmsis
such as high noise, high latency, and low bandwidth often tofindajointpolicyÏ€tomaximizetheexpectedcumulative
prevent these communication algorithms from performing reward,formulatedas:VÏ€(s)=E [ (cid:80)âˆ Î³tR(s ,a )].
Ï€ t=0 t t
(a) Training Framework (b) Active Inference (c) Dual Filter
ğ“› ğ‘»ğ‘« ğ‘„ ğ‘¡ğ‘œğ‘¡ (ğ‘ ,ğ’‚) â„ ğ‘– ğ‘¡ ğ‘„ ğ‘– ğ‘™ğ‘œğ‘ğ‘ğ‘™(ğ‘’ ğ‘– ğ‘¡,â„ ğ‘– ğ‘¡,ğ‘ ğ‘– ğ‘¡)
ğ§ ğ“› ğ’ğ’Š ,ğ“› ğ’„ğ’
ğ‘„ âˆ’ ğ‘™ğ‘œ ğ‘– ğ‘  ğ‘ğ‘ ğ‘¡ ğ‘™ Mixing Ne ğ‘„ t ğ‘– ğ‘™ w ğ‘œğ‘ o ğ‘ r ğ‘™ k ğ¨ ğ¢ğ­ğš ğ¯ ğ« ğ ğ¬ ğ› ğ« ğ ğ ğ¨ ğœ ğ§ ğ„ ğ‘–ğ‘‘ â„ âˆ’ ğ‘– ğ‘¡ ğ‘– ğŸ ğ ğ¢ğ¥ğ ğ ğ« ğ ğ ğ¨ ğœ ğ§ ğ„ ğ‘§ âˆ’ ğ‘¡ ğ‘– ğ›¼ âˆ’ğ‘– ğ‘’ ğ‘– ğ‘¡
ğ
Belief Portrait
Dual Filter Q K V
ğ‘§ğ‘¡
â„ ğ‘– ğ‘¡ ğ‘§ âˆ’ ğ‘¡ ğ‘– â„à·  âˆ’ ğ‘¡ ğ‘– ğ‘œ ğ‘– ğ‘¡ ğ‘ğ‘¡ğ‘Ÿğ‘¢ğ‘’ ğ“› ğ’„ğ’† ğ‘à·œ Argmax ğ‘„à·  ğŒğ‹ğ âˆ’ğ‘– â„ ğ‘– ğ‘¡ â„à·  ğ‘˜ ğ‘¡ ğ‘§ ğ‘˜ ğ‘¡
âˆ’ğ‘– âˆ’ğ‘– âˆ’ğ‘–
Active â„à· ğ‘¡ â„à·  âˆ’ ğ‘¡ ğ‘– ğ‘§ âˆ’ ğ‘¡ ğ‘–
Action Portrait âˆ’ğ‘–
Inference
Relevance Filter
Agent ğ‘–
ğ‘œğ‘¡ ğ‘œà·œğ‘¡ ğ‘œà·œğ‘¡
ğ‘– ğ‘— ğ‘–ğ‘—
ğ‘œ ğ‘– ğ‘¡ ğ“› ğ’”ğ’š ,ğ“› ğ’”ğ’† ğ‘»ğ’ğ’‘_ğ’Œ
ğ‘§ âˆ’ ğ‘¡ C ğ‘– onc B G at e r e a l n i d e a i f e te E nt m B b a e c d k D d w i o n a t g r P d roduct ğ‘— ğ‘– Imagine ğ‘– ğ‘— ğ‘œà·œ ğ‘– ğ‘¡ ğ‘— ğ§ ğ¨ ğ¢ğ­ ğ© ğ ğ« ğ ğ ğœ ğ« ğ ğ ğ¨ ğœ ğ§ ğ„ â„à·  âˆ’ ğ‘¡ ğ‘– â„à·  â„ âˆ’ ğ‘¡ ğ‘– ğ‘¡ ğ‘– ğ’‡
ğ‘’ğ‘¡ FusionEmbedding Perception Portrait Accuracy Filter
ğ‘–
Figure2:TheoverallframeworkofAIM.(a)Thetrainingframeworkcomprisestheagentnetworkandthemixingnetwork;(b)
Theactiveinferencemodule,whichincludesperceptionportrait,beliefportrait,andactionportrait;(c)Thedualfiltermodule,
consistingoftheaccuracyfilterandtherelevancefilter.
Method perspective.Indetail,foragenti,theperceptionoË†t ofteam-
j
matej isconstructedbasedoniâ€™sownobservationsot.The
To build the cognition of teammatesâ€™ decision logic dur- i
specificprocessinvolvessettingeachteammateâ€™spositionas
ing decentralized execution, our core idea is to model their
theoriginandrecalculatingthepositionsoftheotheragents
completedecision-makingprocess.Ontheonehand,wein-
relativetothisorigin,whilealsoaddingotherrelevantinfor-
troduce active inference to construct teammatesâ€™ three por-
mation.AsshowninFigure2,weonlyselecttheportionof
traits:perception-belief-action,whichrepresentâ€œhowteam-
ot thatintersectswithoË†t astheperceptionportraitoË†t .This
matesthinkâ€.Ontheotherhand,duetomodelingerrorsand i j ij
processisessentiallyaviewpointtransformationoperation.
the diversity of teammates, we devise a dual filter that dy-
More details about the transformation can be found in Ap-
namicallyintegratesteammatesâ€™beliefportraitsbasedonthe
pendixB.WethenuseoË†t astheinputandapplyaGRUto
accuracyandrelevanceoftheirperceptionportraits. ij
obtainthehistoricaltrajectoryinformationhË†t oftheteam-
ij
TeammatePortraitviaActiveInference matej.Subsequently,weacquirehË†t ofallteammates.
âˆ’i
Inactiveinference,theteammateâ€™sdecision-makingprocess
BeliefPortrait Beliefservesasahigher-levelabstraction
is divided into three stages: perception of the environment,
of actions, serving as the core basis for policies. However,
beliefformation,andactionexecution.Perceptionservesas
unliketheperceptionportrait,whichisobjective,thebelief
thefoundationalstep,capturingthechangeofentities,which
portrait typically exhibits high subjectivity and variability.
supports the following processes. Beliefs represent agentsâ€™
In scenarios with limited observation, these characteristics
deeperunderstandingoftheenvironment,servingastheba-
become more pronounced, making it challenging to model
sis for deriving action. The final decision, which is the ac-
beliefsfromteammatesâ€™perspectivesaccurately.
tion output, is based on both perception and belief and can
beusedasposteriorinformationtooptimizetheperceptions Therefore, we construct the belief portrait of teammates
andbeliefs.Here,wegivethedetailsofthetriplemodelpro- from the agentâ€™s perspective. For agent i, we use its trajec-
cessinAIM. toryht i andteammatesâ€™indexid âˆ’i formodeling,asshownin
Figure2.Theinput(ht,id )isfedintothebeliefencoder
i âˆ’i
Perception Portrait To understand teammatesâ€™ behavior, to obtain the belief distribution N(Âµt,Î´t). Through repa-
i i
agents should first understand what they have experienced. rameterization, we obtain the teammatesâ€™ belief portraits
AIM aims to construct the world in teammatesâ€™ eyes, gen- zt whichshouldexhibittwocharacteristics:(1)Decision-
âˆ’i
erating their local observations from a teammate-centered supportability;(2)Stabilityovertheshortterm.
To enhance the decision-support ability, we get inspired Accuracy Filter Among triple portraits, the perception
by(Yuanetal.2022)andmaximizethemutualinformation portrait is derived through perspective transformation, ren-
betweenteammatesâ€™actionsandbeliefportraitszt ,condi- deringitinherentlypartial.Therefore,anevaluationmethod
âˆ’i
tionedonhtandid .Throughvariationalinference,maxi- isrequiredtoassesstheaccuracyofperceptionportraits.
mizingmut i ualinfo âˆ’ rm i ationcanbetransformedintothefol- Specifically,welearnamappingf : Rh (cid:55)â†’ Rthatmaps
lowingloss.ThederivationcanbefoundinAppendixA. the perception portrait to an accuracy score. At time t, we
simultaneouslyprocesstheNÃ—N portraits,constructingthe
L =E(cid:2) D (cid:0) p(zt |ht,id )âˆ¥ evaluationmatrixCt.N representsthenumberofagents.ct
mi KL âˆ’i i âˆ’i ij
q (zt |ht,at ,id ) (cid:1)(cid:3) , (1) refers to the accuracy score of agent iâ€™s perception portrait
Î¾ âˆ’i i âˆ’i âˆ’i hË†t toagentj,formedas:
ij
where D KL (...||...) represents the Kullback-Leibler di- ct =softmax(f(hË†t )), (5)
vergence,q (zt |ht,at ,id )isusedasavariationaldis- ij ij
Î¾ âˆ’i i âˆ’i âˆ’i
tributiontoapproximatetheconditionaldistributionp(zt | where f(Â·) represents a two-layer MLP network. A higher
âˆ’i value of c signifies that the perception portrait of agent j
ht,id ).MinimizingL isequivalenttomaximizingthe ij
i âˆ’i mi byagentiismoreaccurate.TheevaluationmatrixCtshould
relevancebetweenthebeliefportraitsandselectedactions.
possessthefollowingcharacteristics.
To improve the stability of zt , we calculate the cosine
âˆ’i (1)Mutualevaluationissimilar.Foragentsiandj,per-
similaritybetweenthezt atadjacenttimestep,formalized
âˆ’i ception portraits of each other are essentially different per-
as:
spectivesonthesameintersectionoftheirobservations.
(cid:34) (cid:35)
ztâˆ’1Â·zt (2)Self-evaluation is highest. The perception portrait of
L =E âˆ’ âˆ’i âˆ’i . (2)
cn âˆ¥ztâˆ’1âˆ¥âˆ¥zt âˆ¥ anagentâ€™sownisguaranteedtobeaccurate,sothenetworkâ€™s
âˆ’i âˆ’i evaluationofthisresultshouldbethehighest.
(3)High similarity gets a high score. If an agentâ€™s per-
ActionPortrait Actionisthemostdirectoutcomeofac-
ceptionportraitissimilartotherealobservationofanyother
tiveinference.Theaccuracyoftheactionservesasacritique
agent,itindicateshighaccuracy.
ofthemodelingprocess,especiallythecoherenceofthebe-
TosatisfytheMutualevaluationissimilar,weintroduce
lief portrait. Moreover, joint actions induce environmental
a symmetry loss L to optimize the evaluation matrix C,
statetransitions,whichinturninfluenceteammatesâ€™percep- sy
whichisformalizedas:
tionportraits.Hence,weoptimizetheperceptionandbelief
portraitbyleveragingposterioractionpredictions.InAIM, L =âˆ¥Câˆ’CTâˆ¥ , (6)
sy F
weconcatenatebeliefportraitsz âˆ’ t i andhistoricalperception whereCT isthetransposeofthematrixC,andâˆ¥Â·âˆ¥ denotes
F
information hË†t as the input of action prediction network, theFrobenius-norm.
âˆ’i
and obtain the imagined action distribution aË† âˆ’i . By mini- ToenhanceSelf-evaluationishighest,weutilizeadiag-
mizingthecross-entropylossbetweenaË† âˆ’i andthetrueac- onallossL se tomaximizetheaccuracyscoreoftrueht ii .
tionsat
âˆ’
r
i
ue,AIMoptimizestheactionmodelingnetworkand
L =âˆ’ (cid:88) c . (7)
theperception-beliefportrait. se ii
i
(cid:88)
L =âˆ’ atruelogaË† . (3) RegardingHighsimilaritygetsahighscore,deepneu-
ce âˆ’i âˆ’i
ralnetworksexhibitthepropertythatsimilarinputsproduce
i
similar outputs. Combined with characteristic (2), percep-
Thecompletelossfortheteammateportraitviaactivein- tionportraitssimilartothetrueht canbemappedtohigher
ii
ferenceisexpressedas: scores, thereby satisfying characteristic (3) without the ad-
ditionallossfunction.
L MD =Î» mi L mi +Î» cn L cn +Î» ce L ce , (4) HavingtheevaluationmatrixC,wesamplethetop k in-
dicesforeachagentbasedontheaccuracytoselectpotential
where Î» , Î» , and Î» represent the hyperparameters of
mi cn ce teammatesforthenextfilter.
thelossfunction,whichareusedtobalancetheireffects.
RelevanceFilter Buildingontheaccuracyfilter,we also
need to apply the relevance filter from a decision-making
DualFilterofAccuracyandRelevance
perspective,selectingteammatesmorerelevanttotheagent
Duetolocalobservation,errorsintheactiveinferencepro- foreffectivecollaboration.Giventhatcollaborationinmulti-
cess are unavoidable. Indiscriminately utilizing erroneous agentsystemsisoftenlocalized,weusetheperceptionpor-
portraitsofteammatescandirectlydistorttheagentâ€™scom- trait as a proxy for relevance. As for how to utilize team-
prehension of the current environment, resulting in non- matesâ€™ portraits to aid decision-making, the most intuitive
cooperative behavior. Furthermore, since collaboration in approachistocombineteammatesâ€™actionportraitstomiti-
multi-agentsystemsisoftenlocalized,itisredundanttoin- gatethelackofcognitionaboutteammates.However,dueto
corporatetheportraitsofallteammatesindecision-making. local observation, accurately modeling actions is challeng-
Hence,weproposeadualfiltermechanismforselectingco- ing. Therefore, we instead combine belief portraits, lever-
operativeteammates,focusingontwoaspects:theaccuracy aging higher-level behavioral bases to dilute the impact of
andtherelevanceofportraits. single-stepmodelingerrors.
100
80
60
40
20
0
0 1M 2M
%
etaR
niW
tseT
(a) 8m_vs_9m (b) 3M_vs_2z5m (c) 2s3z_vs_2s4z (d) 3s5z_vs_3s6z (e) corridor
100 100 100 100
80 80 80 80
60 60 60 60
40 40 40 40
20 20 20 20
0 0 0 0
0 1M 2M 0 1M 2M 0 1.5M 3M 0 1M 2M
100
80
60
40
20
0
0 2M 4M
%
etaR
niW
tseT
(f) 6h_vs_8z (g) protoss_5_vs_5 (h) terran_5_vs_5 (i) zerg_5_vs_5 (j) protoss_10_vs_10
80 80 80 80
60 60 60 60
40 40 40 40
20 20 20 20
0 0 0 0
0 1M 2M 0 1M 2M 0 1M 2M 0 1M 2M
80
60
40
20
0
0 1M 2M
%
etaR
niW
tseT
(k) terran_10_vs_10 (l) zerg_10_vs_10
80 50
40
60
30
40
20
20 10
0 0
0 1M 2M 0 1M 2M
naeM
nruteR
tseT
QMIX RODE QPLEX SIRD COLA OMG MAIC T2MAC AIM (Ours)
(m) PD (2v1) (n) PP (6v2) (o) PP (9v3)
1500 3500
1200 2800
900 2100
600 1400
300 700
0 0
0 1M 2M 0 1M 2M
Figure3:PerformancecomparisonbetweenAIMandbaselinesonSMAC,SMACv2,andMPE.(a)-(f)Sixrepresentativemaps
onSMAC.(g)-(l)SixtasksonSMACv2.(m)-(o)ThreetasksonMPE.
Here,weapplytheattentionmechanism(Vaswani2017) OverallTrainingObjective
toachievetherelevancefilterandfusionofbeliefportraits. Followingthefusionofteammatesâ€™beliefportraits,weuti-
InAIM,foragenti,weadoptthetrueperceptionhistoryht
i lize the basic CTDE training framework QMIX (Rashid
as the query, the historical perception hË†t of the selected k etal.2020)fortraining.Meanwhile,AIMremainscompat-
k
teammatesasthekey,andadoptthebeliefportraitrepresen- iblewithothervaluedecompositionmethods,suchasVDN
tationszt ofthekteammatesasthevalue.Wethenfusethe (Sunehag et al. 2017) or QPLEX (Wang et al. 2020). All
k
zt,withtheattentionscorerepresentedas: modelparametersareupdatedbyminimizingtheL :
k TD
(cid:104) (cid:105)
(cid:18) (cid:19) L =E (yâˆ’Q (Ï„,a))2 , (10)
exp âˆš1 (htW )Â·(hË†tW )T TD tot
Î± i,k = (cid:18) dkey i Q k K (cid:19) , (8) where y = r+Î³max aâ€² QË† tot (Ï„â€²,aâ€²) is the target network
(cid:80)k exp âˆš1 (htW )Â·(hË†tW )T of the joint action-value function. By integrating the triple
j=1 dkey i Q j K portrait and the dual filter, the complete training loss is as
follows:
whereW Q andW K arelearnableweightmatricesappliedto L tot =L TD +L MD +L DF . (11)
thequeryht andthekeyhË†t,whiled denotesthedimen-
i k key
sionofthekeyvector.TheattentionscoreÎ± representsthe Experiments
i,k
relevance between agent i and its collaborative teammates. Weselectseveralmethodsasourprimarybaselines,includ-
The combined result is et = (cid:80)k Î± Â·zt, which is con- ingQMIX(Rashidetal.2020),QPLEX(Wangetal.2020),
i j=1 i,j j
catenated with ht and subsequently processed by a linear RODE (Wang et al. 2021), COLA (Xu et al. 2023), and
i
networktocomputethelocalQ-valueQlocal. SIRD(Zeng,Peng,andLi2023).Toevaluatetheefficacyof
i ourcommunication-freemethodinconstructingteammatesâ€™
Theoptimizationobjectiveofthissectionis:
decisionlogic,weincludetwocommunication-basedmeth-
L =Î» L +Î» L , (9) ods:MAIC(Yuanetal.2022)andT2MAC(Sunetal.2024)
DF sy sy se se
assupplementarybaselines.Furthermore,wesetOMG(Yu,
whereÎ» andÎ» representthehyperparameteroftheloss Jiang, and Lu 2024) with all teammates being trainable,
sy se
function. servingasabaselineforagentmodeling.
50
40
30
20
10
0
0 1M 2M
naeM
nruteR
tseT
(a) PD (2v1)
100
80
60
40
20
0
0 1.5M 3M
%
etaR
niW
tseT
AIM_w/o_Belief AIM_w/o_Filter AIM_w/o_CN AIM_w/o_SY AIM
AIM_w/o_Action AIM_w/o_MI AIM_w/o_CE AIM_w/o_SE
(b) 3s5z_vs_3s6z (c) 3M_vs_2z5m (d) 2s3z_vs_2s4z
100 100
80 80
60 60
40 40
20 20
0 0
0 1M 2M 0 1M 2M
Figure 4: Ablation studies. (a)-(b) illustrate module-wise ablation. (c)-(d) present loss-wise ablation. â€œAIM w/o Beliefâ€ re-
movesbeliefportrait,â€œAIM w/o Actionâ€removesactionportrait,whileâ€œAIM w/o Filterâ€removesdualfilter.
PerformanceComparisons tions,AIMachievesperformancecomparabletoorsurpass-
We conduct experiments on four MARL benchmarks: ingcommunication-basedbaselines.Thisvalidatestheeffi-
SMAC(Samvelyanetal.2019),SMACv2(Ellisetal.2024), cacy of AIM in assisting agents to understand teammatesâ€™
theMulti-agentParticle-Environment(MPE)(Mordatchand decisionsandenhancingcooperation.
Abbeel2018)andtheGoogleResearchFootball(GRF)(Ku-
MPE The MPE provides a 2D physics-based environ-
rach et al. 2020). Notably, although GRF provides a fully
mentsupportingbothcontinuousanddiscreteactionspaces
observable environment, the soccer task demands frequent
(Papoudakis et al. 2020). We evaluate the performance of
collaborationtoachievescoringgoals,posingchallengesto
AIM in the following three tasks with discrete actions: (1)
theagentmodelingmethods.Forevaluation,weusefivedif-
Physical-Deception(PD)(2v1), (2) Predator-Prey(PP)(6v2),
ferentrandomseedsandplottheaveragetestresultsasbold
and (3) Predator-Prey(PP)(9v3). Since the original MPE is
lines. Due to page limitations, we leave the GRF experi-
fully observable, we modify it to evaluate the effectiveness
mentsinAppendixD.1.
of the perception portrait. Specifically, we set the agentsâ€™
SMAC AsthewidelyusedbenchmarkinMARL,SMAC viewradiusto0.8.AsdemonstratedinFigure3(m-o),AIM
requires agents to master micro-level policies such as â€œfo- effectivelyinfersteammatesâ€™decisionlogicthroughmodel-
cus fire,â€ â€œkite,â€ and â€œdraw aggroâ€ to defeat enemies un- ing,enablingcollaborationevenunderstrictpartialobserv-
der relatively concentrated initial positions. Figure 3 (a-f) abilityconditions.
presents the performance of AIM on six tasks, demonstrat-
AblationStudies
ingoptimalresultsonmostmaps.Particularlyonmapsthat
requireacleardivisionoflaborandtheselectionofthebest Inthissection,weanalyzetheimpactofactiveinferenceand
collaborators, such as 3s5z vs 3s6z, corridor, and dual filter modules on overall performance using ablation
6h vs 8z, AIM even outperforms communication-based experiments. We progressively remove these key modules
baselines. This demonstrates that when agents are nearby, andevaluatetheireffects.
communicationexertsnegligibleeffectsondecision-making
Active Inference Module We conduct ablation studies
quality. In contrast, AIM enables agents to select the most
to validate the active inference moduleâ€™s efficacy. Since
cooperative teammates by modeling teammatesâ€™ active in-
the different sight ranges among units of SMACv2 have
ference processes. This capability enhances task allocation
already validated the efficacy of the perception portrait,
efficiency and contributes to achieving team objectives. In
we focus on analyzing the belief and action portrait.
our OMG setup, each allied agent independently models
We compare AIM with three ablation configurations: 1)
teammates, failing to capture inter-agent coordination pat-
AIM w/o Belief,whichonlyremovesthebeliefportrait;
ternsandthusimpairingcollaboration.
2)AIM w/o Action,whichonlyremovestheactionpor-
SMACv2 AsanextensionofSMAC,SMACv2introduces trait; 3) AIM w/o Filter, which removes the dual filter
additionalrandomnessintheinitialpositionsandunittypes, of accuracy and relevance. The ablation results presented
withdispersedagentdistributions.Italsoemploystrueunit in Figure 4 (a)-(b) demonstrate that the removal of any in-
attackandsightranges.TheperformanceofAIMisshown dividual module leads to significant performance degrada-
in Figure 3 (g-l). Due to the difficulty in adapting to the tion. This highlights that: (1) the comprehensive modeling
environmental randomness, the baseline methods perform of teammatesâ€™ decision processes is essential for maintain-
poorly. In contrast, AIM demonstrates superior adaptabil- ing accurate models, and (2) the dual filters are crucial for
itytothisuncertainenvironment.Theresultsshowthatthe obtainingoptimalteammatesâ€™information.
proposedperceptionportraitmoduleinAIMeffectivelyad- ToanalyzetheimpactofdifferentlossfunctionsinAIM,
juststochangesintheagentsâ€™sightranges,validatingitsef- we conduct ablation studies on each loss function. As
ficacy.Furthermore,evenwithinitiallydispersedagentposi- demonstrated in Figures 4 (c)-(d), the elimination of any
100
80
60
40
20
0
0 1M 2M
%
etaR
niW
tseT
3M_vs_2z5m 2s3z_vs_2s4z 3s5z_vs_3s6z 6h_vs_8z
100 100 100
k=1 k=1 k=2 k=4
k=3 80 k=3 80 k=6 80 k=6
k=4 k=4 k=8 k=2 (Ours)
k=5 60 k=5 60 k=4 (Ours) 60
k=2 (Ours) k=2 (Ours)
40 40 40
20 20 20
0 0 0
0 1M 2M 0 1.5M 3M 0 2M 4M
Figure5:Analysisoftheparameterkinselectivecollaboration.Differentvaluesofkhavevaryingimpactsonperformance.
100
80
60
40
20
0
0 1M 2M
%
etaR
niW
tseT
2c_vs_64zg 8m_vs_9m
100 250
MAPPO MAPPO
AIM+MAPPO 80 AIM+MAPPO 200
60 150
40 100
20 50
0 0
0 1M 2M 0 1M 2M
naeM
nruteR
tseT
PP (3v1) PP (6v2)
250
MADDPG MADDPG
AIM+MADDPG 200 AIM+MADDPG
150
100
50
0
0 1M 2M
Figure 6: Scalability Experiments. AIM + MAPPO denotes the extension of AIM to the MAPPO framework, while AIM +
MADDPGrepresentstheadaptationofAIMtotheMADDPGframeworkforcontinuousactionspaces.
Maps QMIX AIM(Teammate) AIM(Ours) thatthechoiceofksignificantlyinfluencestheperformance
8m vs 9m 87.2 89.1 94.0 ofAIM.Askincreases,collaborationwithmoreteammates
3M vs 2z5m 32.3 84.4 93.3 enhances task success rates due to increased information,
3s5z vs 3s6z 00.0 50.4 58.7 but surpassing a certain threshold introduces redundancy,
6h vs 8z 00.0 53.6 67.6 hindering decision-making. Therefore, the dual filter mod-
uleoptimizestheteammateselection,effectivelypreventing
Table1:Analysisofbeliefportraitsfromdifferentperspec- informationredundancyandmisleadingdecisions.
tives. â€œAIM (Teammate)â€ models belief portraits from the
teammatesâ€™ perspective, while AIM adopts the agentâ€™s per- ScalabilityAcrossPolicy-BasedFrameworks
spectiveformodeling.Boldindicatesthebestperformance.
TofurtherinvestigatethescalabilityofAIMacrossdifferent
policy-basedframeworks,weextendAIMtobothMAPPO
(Yuetal.2022)andMADDPG(Loweetal.2017).Wecon-
single loss function results in significant performance de- ductevaluationsinbothSMACandcontinuous-actionMPE.
terioration. Furthermore, the performance decline is more TheresultsinFigure6demonstrateAIM+MAPPOsurpass-
pronouncedwhenL cn orL se isremovedalone.Thishigh- ing MAPPO and AIM + MADDPG exceeding MADDPG.
lightsthecriticalimportanceofmaintainingcontinuousbe- Experimental results validate the dual scalability capabil-
lief modeling of teammates while preserving self-focused ities of AIM: (1) extensibility within policy-based frame-
decisioncognitionforenhancingcomplextasksuccessrates. works, and (2) adaptability to continuous action space en-
Subsequently,wecomparetheconstructionofbeliefpor- vironments.
traitsfromtheagentâ€™sperspectivewiththatfromtheteam-
mateâ€™sperspective.Table1showstheAIM (Teammate) ConclusionandDiscussion
models belief portraits based on inaccurate perception por-
In this paper, we propose AIM, a novel framework that re-
traits constructed from teammatesâ€™ perspectives. This ap-
placesexplicitcommunication(i.e.,â€œTellâ€)withagentmod-
proachintroducesmodelingbiasesthathindercollaboration.
eling(i.e.,â€œThinkâ€)toconstructthecognitionofteammatesâ€™
In contrast, AIM constructs belief portraits of teammates
decision logic. AIM models teammatesâ€™ active inference
fromtheagentâ€™sperspective,effectivelyreducinginaccura-
throughperception,belief,andactionportraits,andemploys
ciesandimprovingcooperativeperformance.
adualfiltermoduletoexcludeinaccurateandirrelevantpor-
Dual Filter Module We analyze the impact of the pa- traits.ExperimentsvalidatetheeffectivenessofAIM.How-
rameter k selected from the perception evaluation matrix, ever, in scenarios where selective collaboration is required,
with a focus on its role in filtering redundant information. AIMselectsafixedsetoftop kteammatesascollaborators.
We conduct experiments using four super-hard maps from Thedynamicselectionofcollaborativeteammatesbasedon
SMAC, and the results are shown in Figure 5. It is evident portraitsremainsanopenissueandisleftforfuturework.
Acknowledgments Mordatch,I.;andAbbeel,P.2018. Emergenceofgrounded
compositionallanguageinmulti-agentpopulations. InPro-
This work was supported by the National Natural Sci-
ceedings of the AAAI conference on artificial intelligence,
ence Foundation of China (Grant No. 62576029) and the
volume32.
Aeronautical Science Foundation of China (Grant No.
202300010M5001). Oliehoek,F.A.;Amato,C.;etal.2016. Aconciseintroduc-
tiontodecentralizedPOMDPs,volume1. Springer.
References Papoudakis, G.; and Albrecht, S. V. 2020. Variational Au-
toencodersforOpponentModelinginMulti-AgentSystems.
Das,A.;Gervet,T.;Romoff,J.;Batra,D.;Parikh,D.;Rab-
CoRR,abs/2001.10829.
bat,M.;andPineau,J.2019. Tarmac:Targetedmulti-agent
communication. In International Conference on machine Papoudakis, G.; Christianos, F.; and Albrecht, S. 2021.
learning,1538â€“1546.PMLR. Agent modelling under partial observability for deep rein-
forcement learning. Advances in Neural Information Pro-
Ding, Z.; Huang, T.; and Lu, Z. 2020. Learning individ-
cessingSystems,34:19210â€“19222.
ually inferred communication for multi-agent cooperation.
Papoudakis, G.; Christianos, F.; SchaÂ¨fer, L.; and Albrecht,
Advances in Neural Information Processing Systems, 33:
S. V. 2020. Benchmarking multi-agent deep reinforcement
22069â€“22079.
learning algorithms in cooperative tasks. arXiv preprint
Ellis,B.;Cook,J.;Moalla,S.;Samvelyan,M.;Sun,M.;Ma-
arXiv:2006.07869.
hajan,A.;Foerster,J.;andWhiteson,S.2024. Smacv2:An
Rabinowitz, N.; Perbet, F.; Song, F.; Zhang, C.; Eslami,
improvedbenchmarkforcooperativemulti-agentreinforce-
S.A.;andBotvinick,M.2018. Machinetheoryofmind. In
mentlearning. AdvancesinNeuralInformationProcessing
International conference on machine learning, 4218â€“4227.
Systems,36.
PMLR.
Friston, K.; FitzGerald, T.; Rigoli, F.; Schwartenbeck, P.;
Raileanu, R.; Denton, E.; Szlam, A.; and Fergus, R. 2018.
Pezzulo,G.;etal.2016.Activeinferenceandlearning.Neu-
Modelingothersusingoneselfinmulti-agentreinforcement
roscience&BiobehavioralReviews,68:862â€“879.
learning. InInternationalconferenceonmachinelearning,
He,H.;Boyd-Graber,J.;Kwok,K.;andDaumeÂ´III,H.2016. 4257â€“4266.PMLR.
Opponent modeling in deep reinforcement learning. In In-
Rashid, T.; Samvelyan, M.; De Witt, C. S.; Farquhar, G.;
ternational conference on machine learning, 1804â€“1813.
Foerster,J.;andWhiteson,S.2020. Monotonicvaluefunc-
PMLR.
tionfactorisationfordeepmulti-agentreinforcementlearn-
Hu, G.; Zhu, Y.; Zhao, D.; Zhao, M.; and Hao, J. ing. JournalofMachineLearningResearch,21(178):1â€“51.
2023. Event-Triggered Communication Network With
Roesch, M.; Linder, C.; Zimmermann, R.; Rudolf, A.;
Limited-Bandwidth Constraint for Multi-Agent Reinforce-
Hohmann, A.; and Reinhart, G. 2020. Smart grid for in-
mentLearning. IEEETransactionsonNeuralNetworksand
dustry using multi-agent reinforcement learning. Applied
LearningSystems,34(8):3966â€“3978.
Sciences,10(19):6900.
Kiran, B. R.; Sobh, I.; Talpaert, V.; Mannion, P.; Al Sal- Samvelyan, M.; Rashid, T.; De Witt, C. S.; Farquhar, G.;
lab, A. A.; Yogamani, S.; and PeÂ´rez, P. 2021. Deep rein- Nardelli, N.; Rudner, T. G.; Hung, C.-M.; Torr, P. H.; Fo-
forcementlearningforautonomousdriving:Asurvey. IEEE erster,J.;andWhiteson,S.2019. Thestarcraftmulti-agent
Transactions on Intelligent Transportation Systems, 23(6): challenge. arXivpreprintarXiv:1902.04043.
4909â€“4926.
Song, S.; Lin, Y.; Han, S.; Yao, C.; Wu, H.; Wang, S.; and
Kurach,K.;Raichuk,A.;StanÂ´czyk,P.;Zajac,M.;Bachem, Lv,K.2025. CoDe:CommunicationDelay-TolerantMulti-
O.;Espeholt,L.;Riquelme,C.;Vincent,D.;Michalski,M.; AgentCollaborationviaDualAlignmentofIntentandTime-
Bousquet,O.;etal.2020. Googleresearchfootball:Anovel liness. InProceedingsoftheAAAIConferenceonArtificial
reinforcementlearningenvironment. InProceedingsofthe Intelligence,39(22):23304â€“23312.
AAAIconferenceonartificialintelligence,volume34,4501â€“
Sun,C.;Zang,Z.;Li,J.;Li,J.;Xu,X.;Wang,R.;andZheng,
4510. C.2024. T2MAC:TargetedandTrustedMulti-AgentCom-
Lee,J.;Chung,J.;andSohn,K.2019. Reinforcementlearn- munication through Selective Engagement and Evidence-
ingforjointcontroloftrafficsignalsinatransportationnet- DrivenIntegration. InProceedingsoftheAAAIConference
work. IEEE Transactions on Vehicular Technology, 69(2): onArtificialIntelligence,volume38,15154â€“15163.
1375â€“1387. Sun,X.2024. AssessingModelRobustnessinComplexVi-
Li,D.;Lou,N.;Xu,Z.;Zhang,B.;andFan,G.2025. Effi- sual Environments. Ph.D. thesis, The Australian National
cientCommunicationinMulti-AgentReinforcementLearn- University(Australia).
ingwithImplicitConsensusGeneration. InProceedingsof Sun, X.; Leng, X.; Wang, Z.; Yang, Y.; Huang, Z.; and
the AAAI Conference on Artificial Intelligence, volume 39, Zheng, L. 2023a. Cifar-10-warehouse: Broad and more
23240â€“23248. realistic testbeds in model generalization analysis. arXiv
Lowe,R.;Wu,Y.I.;Tamar,A.;Harb,J.;PieterAbbeel,O.; preprintarXiv:2310.04414.
and Mordatch, I. 2017. Multi-agent actor-critic for mixed Sun,X.;Yao,Y.;Wang,S.;Li,H.;andZheng,L.2023b.Al-
cooperative-competitive environments. Advances in neural ice Benchmarks: Connecting Real World Re-Identification
informationprocessingsystems,30. withtheSynthetic. arXivpreprintarXiv:2310.04416.
Sunehag,P.;Lever,G.;Gruslys,A.;Czarnecki,W.M.;Zam- the AAAI Conference on Artificial Intelligence, volume 36,
baldi, V.; Jaderberg, M.; Lanctot, M.; Sonnerat, N.; Leibo, 9466â€“9474.
J. Z.; Tuyls, K.; et al. 2017. Value-decomposition net- Zeng, X.; Peng, H.; and Li, A. 2023. Effective and stable
works for cooperative multi-agent learning. arXiv preprint role-based multi-agent collaboration by structural informa-
arXiv:1706.05296. tion principles. In Proceedings of the AAAI conference on
Tian, Z.; Wen, Y.; Gong, Z.; Punakkath, F.; Zou, S.; and artificialintelligence,volume37,11772â€“11780.
Wang, J. 2019. A regularized opponent model with maxi- Zhai,Y.;Peng,P.;Su,C.;andTian,Y.2023. DynamicBe-
mumentropyobjective. arXivpreprintarXiv:1905.08087. liefforDecentralizedMulti-AgentCooperativeLearning.In
Tung, T.-Y.; Kobus, S.; Roig, J. P.; and GuÂ¨nduÂ¨z, D. 2021. Elkind, E., ed., Proceedings of the Thirty-Second Interna-
Effective Communications: A Joint Learning and Commu- tionalJointConferenceonArtificialIntelligence,344â€“352.
nicationFrameworkforMulti-AgentReinforcementLearn- Zhu,C.;Dastani,M.;andWang,S.2024.Asurveyofmulti-
ingOverNoisyChannels. IEEEJournalonSelectedAreas agentdeepreinforcementlearningwithcommunication.Au-
inCommunications,39(8):2590â€“2603. tonomousAgentsandMulti-AgentSystems,38(1):4.
Vaswani, A. 2017. Attention is all you need. Advances in Zintgraf, L.; Devlin, S.; Ciosek, K.; Whiteson, S.; and
NeuralInformationProcessingSystems. Hofmann, K. 2021. Deep interactive bayesian rein-
Wang, J.; Ren, Z.; Liu, T.; Yu, Y.; and Zhang, C. 2020. forcement learning via meta-learning. arXiv preprint
Qplex: Duplex dueling multi-agent q-learning. arXiv arXiv:2101.03864.
preprintarXiv:2008.01062.
Wang, T.; Gupta, T.; Mahajan, A.; Peng, B.; Whiteson, S.;
and Zhang, C. 2021. {RODE}: Learning Roles to Decom-
pose Multi-Agent Tasks. In International Conference on
LearningRepresentations.
Wang,T.;Wang,J.;Zheng,C.;andZhang,C.2019. Learn-
ing nearly decomposable value functions via communica-
tionminimization. arXivpreprintarXiv:1910.05366.
Xie, A.; Losey, D.; Tolsma, R.; Finn, C.; and Sadigh, D.
2021. Learning latent representations to influence multi-
agent interaction. In Conference on robot learning, 575â€“
588.PMLR.
Xu, Z.; Zhang, B.; Li, D.; Zhang, Z.; Zhou, G.; Chen, H.;
andFan,G.2023.Consensuslearningforcooperativemulti-
agent reinforcement learning. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 37, 11726â€“
11734.
Xue, W.; Qiu, W.; An, B.; Rabinovich, Z.; Obraztsova, S.;
andYeo,C.K.2021. Mis-spokeormis-lead:Achievingro-
bustnessinmulti-agentcommunicativereinforcementlearn-
ing. arXivpreprintarXiv:2108.03803.
Yang, T.; Meng, Z.; Hao, J.; Zhang, C.; Zheng, Y.; and
Zheng, Z. 2018. Towards efficient detection and optimal
response against sophisticated opponents. arXiv preprint
arXiv:1809.04240.
Yao,C.;Lin,Y.;Song,S.;Wu,H.;Ma,Y.;Han,S.;andLv,
K. 2025. From General Relation Patterns to Task-Specific
Decision-Making in Continual Multi-Agent Coordination.
arXivpreprintarXiv:2507.06004.
Yu,C.;Velu,A.;Vinitsky,E.;Gao,J.;Wang,Y.;Bayen,A.;
and Wu, Y. 2022. The surprising effectiveness of ppo in
cooperative multi-agent games. Advances in Neural Infor-
mationProcessingSystems,35:24611â€“24624.
Yu,X.;Jiang,J.;andLu,Z.2024.Opponentmodelingbased
onsubgoalinference. AdvancesinNeuralInformationPro-
cessingSystems,37:60531â€“60555.
Yuan,L.;Wang,J.;Zhang,F.;Wang,C.;Zhang,Z.;Yu,Y.;
andZhang,C.2022. Multi-agentincentivecommunication
via decentralized teammate modeling. In Proceedings of

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Think How Your Teammates Think: Active Inference Can Benefit Decentralized Execution"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   âŒ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   âœ… GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   âŒ BAD: Repeating the same claim 3+ times with slight variations
   âœ… GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
