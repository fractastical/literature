=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: A framework for the use of generative modelling in non-equilibrium statistical mechanics
Citation Key: friston2024framework
Authors: Karl J Friston, Maxwell J D Ramstead, Dalton A R Sakthivadivel

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2024

Key Terms: statistical, systems, energy, equilibrium, mechanics, object, dynamics, models, generative, framework

=== FULL PAPER TEXT ===

A framework for the use of generative modelling in non-equilibrium statistical
mechanics
Karl J Friston,1 Maxwell J D Ramstead,1 and Dalton A R Sakthivadivel2,∗
1Queen Square Institute of Neurology, University College London, London, UK
2Department of Mathematics, CUNY Graduate Center, New York, NY 10016, USA
(Dated: 15th October 2025)
We discuss an approach to mathematically modelling systems made of objects that are
coupled together, using generative models of the dependence relationships between states
(or trajectories) of the things comprising such systems. This broad class includes open or
non-equilibrium systems and is especially relevant to self-organising systems. The ensuing
variationalfreeenergyprinciple(FEP)hascertainadvantagesoverusingrandomdynamical
systemsexplicitly, notably, bybeingmoretractableandofferingaparsimoniousexplanation
ofwhythejointsystemevolvesinthewaythatitdoes,basedonthepropertiesofthecoupling
between system components. The FEP is a method whose use allows us to build a model of
the dynamics of an object as if it were a process of variational inference, because variational
free energy (or surprisal) is a Lyapunov function for its dynamics. In short, we argue that
using generative models to represent and track relations amongst subsystems leads us to
a particular statistical theory of interacting systems. Conversely, this theory enables us to
construct nested models that respect the known relations amongst subsystems. We point
outthatthefactthataphysicalobjectconformstotheFEPdoesnotnecessarilyimplythat
this object performs inference in the literal sense; rather, it is a useful explanatory fiction
which replaces the ‘explicit’ dynamics of the object with an ‘implicit’ flow on free energy
gradients—a fiction that may or may not be entertained by the object itself.
CONTENTS
1. Introduction 3
2. Summary remarks on the free energy principle 5
2.1. Fundamentals 5
2.2. Analysis and interpretation 8
3. Empirical validation 10
∗ dsakthivadivel@gc.cuny.edu
5202
tcO
41
]hcem-tats.tam-dnoc[
4v03611.6042:viXra
2
3.1. Cellular morphogenesis 10
3.2. Periodically-firing cells 13
4. Some preliminary philosophical considerations 22
5. A map of that part of the territory that behaves as if it were a map 24
6. Conclusion 26
References 27
Acknowledgements
This paper more fully develops the theory outlined in a previous manuscript [1], and we are
grateful to those listed there for their contributions to preliminary discussions on the topic. The
accounthereislargelybasedonaseriesoftalksgivenbythethirdnamedauthorattheDepartment
ofPhilosophyoftheUniversityofVienna, theLauferCenterforPhysicalandQuantitativeBiology,
the2024ConferenceonComplexSystems,andtheinformaldiscussionsthatfollowed. Wethankthe
organisersandaudiencesofboth. DARSacknowledgessupportfromtheEinsteinChairprogramme
at the Graduate Centre of the City University of New York. KJF and DARS acknowledge support
from the VERSES Research Lab.
Funding declaration
There is no specific funding to be acknowledged.
Author contributions
The list of authors has been ordered alphabetically by surname. The CRediT contributions are
asfollows: KJF(formalanalysis,software,writing—reviewandediting),MJDR(conceptualisation,
writing—review and editing), DARS (conceptualisation, formal analysis, software, visualisation,
writing—original draft).
3
1. INTRODUCTION
In statistics, generative models are joint probability distributions that model the relationships
between different variables—most often, observations and their causes, or data and a set of labels.
In the study of open and interacting systems, the use of statistical models—capable of describing
such relationships—seems to be particularly apt. One approach to making such models, using
variational (approximate Bayesian) inference [2–4], describes how a physical object reflects the
properties of its environment and vice versa. In this paper, we argue that generative models
provide an especially useful foundation in scientific modelling generally, and especially in the study
of systems exhibiting adaptation, morphogenesis, and other non-equilibrium phenomena. More
specifically, we argue that generative modelling—premised on the use of variational inference to
model the causes of the signals that a coupled system is receiving [5]—is particularly apt for the
practice of modelling the activity of modelling itself; for instance, in modelling sophisticated self-
organising systems, where we write down a representation of the model that we assume is in play
in prediction and action. We use the ensuing variational free energy principle to illustrate how
implementing models which (i) formalise how one object keeps track of another, and (ii) codify
these relationships, can increase the fidelity of our mathematical modelling.
Our proposed free energy principle (FEP) has been explored in some depth in prior literature,
from the viewpoint of modelling control systems using the property that they generally look as if
they track or infer the statistics of perturbations to their setpoints. In this paper we will make a
more general case for its utility in statistical mechanics at large. After reviewing prior work on the
FEP, we identify a particular issue that the FEP is able to uniquely address: the FEP keeps track
of how a model depends on aspects of the thing that is modelled in a particularly insightful way,
offering advantages in terms of tractability and explanation, by using gradients of variational free
energy to describe the dynamics of interacting systems.
One of the points this paper will emphasise is that using this framework does not necessarily
presuppose that a system actually performs inference on its environment. It is a truism that phys-
ical systems need not explicitly calculate their trajectories of motion, to be modelled as pursuing
such trajectories. We need not assume that the particle itself is literally performing inference when
we build models under our framework. What is at stake is an ‘as if’ description: we simply as-
sume that there exists a quantity that varies systematically with the dynamics of the system; it
so happens that surprisal (or, equivalently, variational free energy) is a good choice of quantity
for physical reasons, and that the minimisation of this quantity is mathematically equivalent to
4
inference, in the sense of being an estimator. The fact that it is our model of the system which
does inference and not necessarily the system itself is both a boon and a bane. It enables us to
model the system using the more tractable gradient of variational free energy and the mathematics
of statistical inference, rather than studying the possibly highly non-linear coupling between the
dynamics of the two systems. However, it is not necessarily a faithful representation of the system
in the literalist sense. (See also [6].)
To that end, after describing the advantages of this FEP we will then argue that there is no
conflation of the modelled system and our scientific model of it—in fact, we describe how they
are carefully distinguished. This argument gives way to a more general theme of the FEP, which
is that capturing interactions and dependence relations between things using generative models is
insightful. We discuss how using the FEP gives us a flexible model of the properties of the object
that look like they perform inference, but crucially, without inherently reifying the inferential
aspects of the model, attributing them to the real-world object. That is to say, a model of an
object can always be written as if it were modelling or inferring the structure of its environment.
To make this argument, we appeal to two distinctions: one between the generative model (which
is our scientific model) and the physical object being modelled (the real ‘thing’ in the world), and
the other, between the generative model and the variational density.
The structure of the paper is as follows. We first present a conceptual introduction to the
FEP—andthewayitleveragestherelationshipsencodedbyagenerativemodeltobuildascientific
modelofthedynamicsofphysicalobjectsusingfreeenergygradients. Thisispossiblebymodelling
the property of physical systems that they look as if they engage in modelling to an external
observer. To sketch the argument: the FEP provides us with tools from dynamical systems and
information theory, allowing us to model any coupled random dynamical systems as statistical
estimators, i.e., as engaging in the statistical estimation of some quantity pertaining to the set of
objects with which it interacts. Under the FEP, one object that is coupled to any other can thus
be read as a representation of the coupled or estimated object, albeit of a deflationary, qualified
sort (discussed in [7]; see also [3, 8]). More formally, the FEP applies to generative models that
contain a partition of the states or paths of a joint system (a so called “particular” partition, i.e.,
into objects or particles). This partition induces a statistical boundary called a Markov blanket
[9], which captures conditional independence relations amongst subsets of the system [2, 10]. The
FEP says that, when such Markov blanketed subsets of the system exist, those subsystems track
each others’ statistical properties (dynamically) across the blanket [11]. Essentially, the FEP is a
modelling framework which captures the following fact: models of coupled particles or factorised
5
systems can be written as parameter estimators, and estimating the parameters of a distribution
is a form of Bayesian inference.
We then argue that there is no ambiguity between model and modelled real-world system in
this formalism. We can think of the FEP, metaphorically, as a map of the part of the territory that
behaves as if it were a map—or looks as such to an external observer. Accounts such as [6] have
encouraged practitioners of the FEP to be explicit about this point. Viewing the theory this way,
not only is there no conflation between our FEP-theoretic model of the system and the system
itself, but in fact we arrive at a novel modelling method for coupled random dynamical systems. In
particular, we do not claim that the objects in the system themselves must literally be performing
inference. Rather, we describe the dynamics as doing inference, in the sense of being a statistical
estimator whose states parameterise a variational density.
In summary we propose that the FEP provides a map of any possible map whatsoever—of, or
held by—a physical system. (This includes the use of the FEP itself by a modeller.) We suggest
that this leads to constraints on all possible maps, or models of physical things, arising from
what it means to be a map or model of a physical process at all. In the fashion of Jaynes, who
proposed that doing statistical mechanics is doing inference about the probabilities of microstates
given knowledge of macrostates [12, 13], the FEP is a natural expression of statistical physics as a
process of making models. More broadly we claim that this echoes the sentiments of Wittgenstein,
providing a constraint on modelling in terms of logically necessary preconditions for sense-making
and the parsing of sensory streams [7].
For those already familiar with the FEP, this paper can be thought of as going back to ba-
sics, emphasising its foundation in inverting generative models (written as SDEs in generalised
coordinates), using a path integral scheme for minimising variational free energy [14].
2. SUMMARY REMARKS ON THE FREE ENERGY PRINCIPLE
2.1. Fundamentals
Here we will describe the proposed free energy principle (also directing the reader to prior
literature such as [3, 4, 11] for a more extended look). The FEP aims to describe the physical
properties of a system in terms of dynamics on a manifold of probability distributions (i.e., as
variational inference) [15], resting on the useful property that physical objects generally look as if
they track or infer the statistics of things to which they are coupled [3, 11, 16]. Formulated most
6
generally, the FEP says that the average path taken by an object that is sparsely coupled to its
environment is characterised by a minimisation of a variational free energy functional.
We consider a particle (also referred to as a ‘system’) with states µ coupled to another system
with states η (henceforth, the ‘environment’) through a set of variables B with values b, and a
steady state density p(η,b,µ) such that µ is conditionally independent of η given B. After Pearl
(see [9]) we refer to B as the Markov blanket of the system. Typically, B will itself consist of two
different kinds of states—sensory states, providing the system signals about its environment, and
active states, providing interventions of the system on the environment. The coupling at play puts
us in a non-equilibrium situation, where a system can be driven by fluxes of energy or matter from
other systems in its environment [17].
The FEP begins by saying that if there exists a boundary in the form of a Markov blanket, then
there exists a function σ relating (the dynamics of) internal states to the environment across the
boundary, and in particular, such that σ(µˆ ) = ηˆ , where ˆ· denotes the conditional expectation
b b b
given the blanket state b [16, Lemma 4.3]. Following that, the aim of the FEP is to understand
what the generative model p(η,b,µ) says about the dynamics of the subsystem with states µ,
beyond just its marginal distribution. In particular: if we assume that ηˆ parametrises p(η | b),
b
then we have made µ into an estimator of the statistics of the environment.
The FEP is simplest when we have a non-equilibrium steady state density, in which case the
following claim holds: if the system remains on some attractor of states, it looks like it minimises
variational free energy. This is true in the following sense. It is possible to express the dynamics
of a random variable with a non-equilibrium steady state density
dX = f(X )dt+D(X )dW
t t t t
in terms of a gradient flow on the surprisal weighted by a particular matrix field [18–21], having
the form
dX = −(Q(X )−Γ(X ))∇ logp∗(X )dt+D(X )dW (2.1.1)
t t t x t t t
where Q(x) is skew-symmetric everywhere, Γ(x) is positive (semi-)definite everywhere and satisfies
2Γ(x) = D(x)D⊤(x), and p∗(x) is the non-equilibrium steady state density. This decomposition
holds whenever there exists a Q(x) such that
∇×[Q(x)+Γ(x)]−1f(x) = 0
and Γ is divergence free—otherwise, in the It¯o picture, a correction is necessary due to the diver-
gence of Γ. For an equilibrium steady state, Q(x) is identically zero and f is a gradient on the
7
nose, with steady state proportional to exp{−∇f(x)}.
Moreover, the log-probability is able to describe the probability of all fluctuations of the system
in terms of their distance from the most likely state, as well as physically-derived quantities [17,
22, 23], making this a meaningful equation. (See [24] where this is applied to reproduce certain
aspects of stochastic thermodynamics.)
Let us make the Laplace assumption such that the most likely state is the expectation. Then
our generic SDE (2.1.1) tends towards the fixed point solution
µˆ = argmax logp(µ,b).
b
To make use of this first observation, we now postulate a conditional density over external states,
q(η;ηˆ ), where we have parameterised the variational density by the conditional mode ηˆ of the
b b
environment. Under the Laplace assumption, in a neighbourhood of its peak the posterior is
approximatelyGaussian,andhenceuniquelydeterminedbyitsmeanandvariance. Thisguarantees
that finding the right value of ηˆ is sufficient for q to equal the posterior almost surely. More
b
generally, under the maximum entropy assumption of a Gibbs density, any constraint function
suffices. Once again, this implies that the system’s average behaviour encodes an inference about
its environment, in the sense of parameter estimation.
Inference by parameter estimation implies the KL divergence between the posterior and the
true density vanishes as the parameter varies. We will write
(cid:90) (cid:90)
F(µ,b) := q(η;σ(µ))logq(η;σ(µ))dη− q(η;σ(µ))logp(η | b)dη−logp(µ,b).
Now observe that since the divergence term is non-negative, F(µ,b) is an upper bound on the
surprisal −logp(µ,b),
F(µ,b) = D (q(η;σ(µ))∥p(η | b))−logp(µ,b) ⩾ −logp(µ,b) (2.1.2)
KL
withequalitypreciselywhentheKLtermvanishes. Weconcludethattheminimisationofsurprisal
impliesthevanishingofthedivergenceterm. LetusapplytheLaplaceapproximation[25]suchthat
theexpectedstateisapproximatelythemodeofthedensity. Itfollowsthatthemostprobablestate
is the least surprising one. Hence, for physical reasons, we expect the surprisal to be minimised.
From that reasoning, (2.1.1) has the equivalent form
dµ = −(Q−Γ)∇ F(µ,b)+dW . (2.1.3)
t µ t
Evidently, when the inference gap is minimised, we have
−(Q−Γ)∇ logp(µ,b)+dW = −(Q−Γ)∇ F(µ,b)+dW
µ t µ t
8
on the nose. Our reasoning so far implies this suffices as an approximate description of all Markov
blanketed systems.
What we have done is used the fact that the dynamics of a (steady state) system minimise
surprisalonaverage,andarguedthatwheneverequippedwithaMarkovblankettheyalsominimise
variational free energy; and hence, the dynamics of a system with a particular partition can always
be written as a gradient flow on variational free energy. This gradient flow is what we identify
as actually being the process (i.e., the performance) of inference. Explicit examples of this can
be found in [11, 24, 26]. The particular partition becomes crucial here, since this is what we
leverage in our model to incorporate some unknown quantity (external states beyond the blanket)
that the system can do inferences about at all. Variational free energy, being a tractable (i.e.,
easily computed) upper bound on surprisal, already makes this a useful tool to have—however we
will go further and claim supposing the existence of a minimised variational free energy, with its
interpretation as inferring a target density by approximating it with a variational density, has a
use in interpreting models in statistical physics in a new way.
We will remark that much of this story can be told for the trajectories of a system and distribu-
tions over path spaces, but with many modifications [4, 11], none of which are necessary to make
the point sought in this paper.
2.2. Analysis and interpretation
We have, in the previous subsection, introduced a vanishing KL divergence term to our SDE-
based model of a physical system, which we have then stipulated to vanish. Though seemingly
vacuous, this move opens a world of interesting interpretations for models built according to this
reasoning, by connecting the physically-given dynamics of the mode of the system to inference
performed by the mode of an estimator. For example: by existing a particular way inside the joint
state space, the particle can be seen as encoding (via its mode) an inference about a generative
modelovertheentirejointstatespace, resultingfromcarvingoutsome‘niche’withinit. Thatisto
say, the environment must likely be some certain way for the system to exist the way it does. On
the other hand, if free energy (or surprisal) remains irreducibly high, it is likely that the particle
occupies surprising (i.e., implausible or uncharacteristic) states; this would render it non-existent
over some timescale, in the sense that it would be occupying states that were not characteristic of
the thing in question (e.g., a fish out of water). Correspondingly, its inference about the generative
model would be incorrect, concentrating mass in the wrong areas of the joint state space. These
9
statementstietogetherthereasoningthatthingswhichexistinanenvironmentreflectthestatistics
of their environment in a particular way, based on the way they are coupled to their environment;
or, they do not exist that way.
Here, variational free energy is a Lyapunov function—a function that decreases monotonically
along the trajectories of a (deterministic) dynamical system. The utility of Lyapunov functions
is that they guarantee stability, allowing us to understand the dynamical system as flowing on
the gradient of the Lyapunov function. Variational free energy arises as a Lyapunov function for
the straightforward reason that the most probable state is the least surprising one, and that the
log-probability is able to describe the probability of all fluctuations of the system in terms of their
distance from that most likely state [17, 22, 23]—combined with the inequality in (2.1.2). Since
the dynamics towards the mode induce a gradient flow on a divergence between the estimated
parametric distribution and the true distribution, the FEP moreover proposes that these dynamics
(in virtue of the coupling) can be modelled as a variational inference process, following a gradient
flow on the variational free energy. Said another way, the crucial idea is that the dynamics of
a system do inference when written as minimising a free energy functional, which is itself the
consequenceofreadingsurprisalasaLyapunovfunctionforthosedynamics,followinge.g. Freidlin–
Wentzell, and noting that the KL divergence is stipulated to be minimised by a near-tautology that
the system will be at its average state on average. In that case, we can map these parameters to
σ(µˆ ), meaning that for as long as the particle is at state µˆ , the particle models its environment
b b
by estimating the parameters of its distribution.
Ultimately this simply means that, in virtue of being coupled (via blanket states) to its envir-
onment, the particle reflects data about its environment—or, what it believes that its environment
is like. Under the assumption that a particle which exists in an environment models its envir-
onment, free energy is implicitly a measurement of coherence. If the particle stops modelling its
environment in a particular way (i.e., for a particular σ), it must have ceased to exist with the
mean that σ was constructed for; conversely, if a particle ceases to exist, it will stop modelling its
environment. The surprisal being considered is, much like probability, surprisal given a model, in
that it quantifies how surprising it would be for an observer—equipped with a model—to find the
system (such as itself) in some non-system-like state or configuration. Minimising surprisal implies
that the variational free energy is minimised, (i.e., the object-as-model is a good estimator of its
environment [11]), and likewise, minimising the variational free energy places an upper bound on
the surprisal.
Similarly, note the direction of the implication in our claims. In the context of modelling
10
persistent systems, the FEP is a sort of inversion of the typical reasoning one sees: instead of
saying an object persists (as a thing) because it is stable, we say an object is stable (as what it
is) because it persists (as that thing) [16]. We stipulate that the KL divergence vanishes—because
it is a tautology that p should equal q almost surely—and because physics only ‘works’ in a given
regime if this is the case (see [11, §IIB] where this argument is spelt out). This happens in the
background, so to speak. Then we can use the FEP to describe the dynamics as a gradient flow on
surprisal—which can be done by a simple mathematical fact and independently of the FEP—as a
process which has performed inference in virtue of existing. Adding this ‘inflationary’ KL term, the
non-equilibriumsteadystatedensityofthesystemcanbeunderstoodasamodeloftheenvironment
under this stipulation. All this is made possible by the existence of a particular partition. Hence,
we arrive at a deflationary account of intelligence through an inflationary mathematical technique.
However—note that when the Markov blanket consists of sensory states and active states, this
inference can immediately be written as part of a control problem [27–33], lending it an even more
sophisticated interpretation.
From the viewpoint of model building, since internal states are by construction unobservable,
we can in principle tell any story that we like about them—provided that this story is supported
well enough by observable evidence. One such story is that those internal states look as if they
parameterise a variational density over external states. Taking this explanation seriously, and
leveraging it as an approach to modelling physical systems, is applying the FEP.
3. EMPIRICAL VALIDATION
In this section we bolster the claim that modelling physical systems with variational free en-
ergy is advantageous by reviewing some simulations of self-organisation and self-assembly. Two
toy models will be studied—cellular morphogenesis, and an ensemble of excitable cells. In these
simulations we will see that introducing the generative model allows us to model the system self-
organising as if it ‘knew its place’ within a larger system—inferring the unknowable external states
by the minimisation of variational free energy.
3.1. Cellular morphogenesis
Starting from an ensemble of undifferentiated cells at the same location, we wanted to simulate
theirmigrationanddifferentiationusingtheminimisationofvariationalfreeenergy. Thesimulation
11
here exploits the fact one can express a Markov blanketed system’s marginal Lyapunov function in
terms of a variational free energy function, such that its attracting set can be prescribed in terms
of the generative model that defines the free energy. In essence, the systems dynamics rest upon
each cell inferring its unique identity (in relation to all others) and behaving in accord with those
inferences.1
A group of eight cells was simulated with states consisting of the physical locations on a two-
dimensional grid (ψ ) of each cell and the cell type of each cell (measured by its cell signalling
x
profile, a four-dimensional vector of signals expressed ψ ). For cell i, (ψ ,ψ ) is an internal state
c xi ci
µ, and the remaining tuples (ψ ,ψ ) constitute external states η.
xj cj
Each cell was equipped with active states (a ,a ) corresponding to modulations of its position
x c
and chemotactic signalling profile, respectively. We assume the active states fluctuate quickly
enough that from the viewpoint of the environment their dynamics are negligible, i.e. ψ ≈ a,
constituting an adiabatic approximation
τ∂ ψ = a−ψ
t
with large time constant τ. In this way the cell’s active states become the external states offered
to other cells. (Note that this assumption is purely for convenience, simplifying the analytic
expressions for certain quantities; it is by no means necessary in the following discussion, and the
example to follow will not use it. What is at stake in terms of the realism of the simulation is: the
adiabatic assumption excludes the possibility of modelling cell migration or signal expression as a
function of action.)
We will additionally assume an exponential decay in space of extracellular signal concentration
as a function of the cell’s own signalling. This means the cell can sense
(cid:88)
λ (ψ ,ψ ) = C ψ e|ψxi−ψxj|, C ∈ [0,1].
i x c cj
j
Each cell also has sensory states detecting the cell’s current location and the concentration of
chemical signals produced by the cell. We will assume these are simply noisy estimates of (ψ ,ψ )
xi ci
with noise distributed like a Gaussian of precision Π(1) = e−16, for the full sensory state
s = [ψ +ω,ψ +ω,λ (ψ ,ψ )+ω].
i xi ci i x c
Insummary,theactivestatesare(i)whereacellchoosestobeand(ii)whatacellchoosestorelease.
Those actions are the external states being offered to other cells under the adiabatic assumption,
and through the kernel above they superpose into a shared field which every cell samples.
1 Thesesimulationswerebasedon[34]andusetheDEMtoolboxintheMATLABsoftwareSPM12(https://www.
fil.ion.ucl.ac.uk/spm/). Theycanbereproducedusingthemorphogenesisfeatureandthedataprovidedthere.
12
A point in the state space (ψ∗,ψ∗) was designated as a target morphology, giving prescribed
x c
locations and cell types of all cells. This preferred morphology was encoded in a prior and specified
desired positions and signal expressions for each clone or cell in terms of the cell’s expectations
regarding sensory signals and spatial configurations. This point consisted of the physical form of a
head, body, and tail, and with different cell types in each component of the organism. The target
sensation under λ is plotted in Figure 1a. In real situations one can imagine a genetic encoding
of the target morphology, with epigenetic dynamics being active inference. A possible example of
this is given in Figure 1b where the target is encoded in binary codons for location and expression.
We then equipped each cell with a generative model, specifying it as follows. The mean was given
in terms of how environmental states are mapped to sensory states,
 
ψ∗
x
exp(ψ )  
g(ψ) i = (cid:80) exp( i ψ )   ψ c ∗  
j j  
λ(ψ∗,ψ∗)
x c
with the softmax function returning the expected identity of each cell. In Figure 1c we show the
softmax function, interpreted as the posterior beliefs that each cell (column) occupies a particular
place in the ensemble (rows). A precision Π(2) = e−2 was also set over external states. A prediction
error for how the generative model matched with sensations was given in the form
ε = [s −g(ψ) ]
i i i
The model was operationalised by following the procedure in [14]. A Taylor expansion of tra-
jectories was taken and the statistics of each order of approximation (ψ,ψ′,ψ′′,...) were modelled
at each point in time. A derivative operator D was defined for ordinary differential equations on
this space. The entire trajectory was given a coordinate in function space
ψ˜= [ψ,ψ′,ψ′′,...].
Now by the decomposition (2.1.1), we have the equations of motion
dµ˜ = −(Q −Γ )∇ logp∗(s˜,a˜,µ˜)dt+dW˜ µ,
t µµ µµ µ˜ t
(3.1.1)
da˜ = −(Q −Γ )∇ logp∗(s˜,a˜,µ˜)dt+dW˜ a
t aa aa a˜ t
for each cell.2 This is where the interpretation as inference which we supply as modellers comes
into play. We cannot compute the solution to (3.1.1) since the surprisal term yields an intractable
2 This follows from the marginal flow lemma of [35].
13
integral. However, under the generative model we have equipped our system with, we can instead
treat the dynamics of the cell under (3.1.1) as maximising log-evidence for the sensations of the
cell,andhenceasavariationalproblem—asposedthroughout§2. Wewillintroducethevariational
distribution
q(ψ˜| µ˜) = N (cid:0) µ˜;−∇ logp(µ˜,s˜,a˜,µ˜) (cid:1) ,
µ˜µ˜
a Gaussian with expectation being the coordinate of the internal trajectory in function space µ˜
and variance being the curvature of the surprisal—computed at ψ˜= µ˜—and free energy
F(a˜,s˜,ψ˜) = −logp(s˜,a˜,µ˜)+D (cid:0) q(ψ˜| µ˜)∥p(ψ˜| s˜,a˜,µ˜) (cid:1) .
KL
Treating (3.1.1) as a Bayesian filter of trajectories, from the derivation outlined in [14, equation
3.6] the final drift terms for the equations of motion for the i-th cell given the generative model
were obtained as (again denoting the internal state ψ as µ)
i
f (a˜ ,s˜,µ˜) = (Q −Γ )∇ F(a˜ ,s˜,µ˜) = Dµ˜−∇ ε˜ ·Π(1)ε˜ −Π(2)µ˜
µ˜ i i µµ µµ µ˜ i i µ˜ i i
(3.1.2)
f (a˜ ,s˜,µ˜) = (Q −Γ )∇ F(a˜ ,s˜,µ˜) = Da˜ −∇ s˜ ·Π(1)ε˜
a˜i i i aiai aiai a˜i i i i a˜i i i
with the following explicit equations for (positional and signalling, respectively) active states
∂ a˜ = Da˜ −∇ s˜ ·Π(1)ε˜ +∇ s˜ ·Π (1) ε˜
t x x x x x x x λ λ λ
∂ a˜ = Da˜ −Π(1)ε˜ +Π (1) ε˜ .
t c c c c λ λ
As before, if the inference gap is minimised, for instance, in the neighbourhood of the mode, then
(3.1.2) is equal to (3.1.1) on the nose.
We then simulated the dynamics of each cell. In Figure 2 we have shown the expected positions
and releases of each cell over time, extracted from the solution to (3.1.2), and the decrease in free
energy as the cells evolve towards the target state.
In summary, the cell carries a generative model that predicts what signals it should sense if it
were the ‘correct’ type of cell at the ‘correct’ place in the target morphology. Those predictions
are built from the target locations and target signal combinations, and the model maps internal
beliefs about identity through the generative model to predicted sensory consequences. The final
outcome of this process is shown in Figure 3.
3.2. Periodically-firing cells
A similar but more complicated example is a ring of excitable, gap-junction-coupled cells whose
generative model encodes a periodic target waveform. Each cell’s internal states infer the current
14
(a)
2 4 6 8
cell
(b)
llec
1
2
3
4
5
6
7
8
(c)
Figure 1: Plots of the target extracellular gradients (1a), encoding of the target signal in the cells
(1b) and softmax expectations of the identities of each cell (1c). In Figure 1a colours denote
different signal expressions and black dots denote no data, with the final locations of the cells
starred.
phase of that cycle from its membrane sensors, whilst active states like ion-channel gating act to
makesensedsignalsmatchthepredictedperiodictrajectory. Thismodelismoreexpressivebecause
the prior over hidden states is oscillatory, such that the free energy minimum is a limit cycle: the
population converges to a stable circulating orbit in state space rather than to a fixed point.
We will define external η as a stimulus in the world, s as a sensory state, µ as the internal
t t t
state estimating the phase of the cell, and a as the state of some actuator or motor acting on the
t
environment to make observations conform to predictions. For simplicty we will assume s tracks
t
15
0 5 10 15 20 25 30
time
noitacol
4
3
2
1
0
-1
-2
-3
-4
5 10 15 20 25 30
time
(a) Migration of cells in time. As the cells
differentiate they are marked with red, yellow, green,
or blue, according to the signal they follow, with the
gradient denoting the level of how strongly a signal is
expressed.
ygrene
eerf
-240
-260
-280
-300
-320
-340
(b) Decrease of free energy in time. A decrease
of free energy in time is noted as the cells migrate to
the final states corresponding to the sensed target
signal.
Figure 2: Here morphogenesis is demonstrated by cells migrating to align with expectations
based on sensory input, modelled by a concomitant decrease of free energy in time.
η and that sensor noise is standard white in time, i.e. that the likelihood satisfies
t
(cid:26) (s−η)2(cid:27)
p(s | η) ∝ exp −
2σ2
s
at steady state. We will also introduce a likelihood over outcomes of actions
(cid:26) (η−a)2(cid:27)
p(η | a) ∝ exp −
2σ2
η
in place of the simplifying adiabatic assumption used in the previous example.
The system’s predictions of sensations given a phase is the cosine function s(µ) = Acosµ and
we introduce a predictive control prior making actions prefer to create the encoded preference for
the waveform,
(cid:26) (a−Acosµ)2(cid:27)
p(a | µ) ∝ exp − .
2σ2
a
There will finally be a uniform prior on µ as a circle-valued variable.
In total we have a one-step generative model (conditioned on the internal phase):
s | η ∼ N(η,σ2), η | a ∼ N(a,σ2), a | µ ∼ N(Acosµ,σ2).
s η a
16
4
3
2
1
n
o
it 0
a
c
o
l
-1
-2
-3
-4
-4 -3 -2 -1 0 1 2 3 4
location
Figure 3: Dynamics of each cell, with final locations starred. The gradients here match
those of Figure 2a. It can be seen that (speaking somewhat heuristically) each cell moves to fulfil
its expectations about the signals it should encounter, whilst expressing the signals associated
with its current beliefs about its place in the target ensemble.
The generative model is linear in (s,η,a) and mildly nonlinear in µ through the cosine function.
To keep this example brief—and to make the surprisal analytically soluble, such that some direct
comparisons can be made between (2.1.1) and (2.1.3)—we will not create any hidden states to
make inferences about beyond the environmental state, so our generative model is not hierarchical
and generalised coordinates of motion need not be used.
In that case, using conditional independence, we can write at steady state (up to additive
17
constants)
(s−η)2 (η−a)2 (a−Acosµ)2
−logp(x) = + + .
2σ2 2σ2 2σ2
s η a
Now specify a parametrisation of Q and Γ,
 
0 q q 0
µs µa
 
−q 0 0 0 
 µs 
Q =  , Γ = diag(γ µ ,γ s ,γ a ,γ η ).
 
−q 0 0 q
 µa aη
 
0 0 −q 0
aη
Noticetherearenocouplingsintheflowoverthestatespacebetweenµandη. Thisisanindication
of the Markov blanket.
Since the generative model is somewhat simple, it is straightforward to write the equations of
motion:
∂ µ = q ∂ L+q ∂ L−γ ∂ L+ξµ
t t µs s µa a µ µ t
(cid:18) (cid:19)
s−η a−η a−Acosµ Asinµ
= q +q + −γ (a−Acosµ)+ξµ
µs σ2 µa σ2 σ2 µ σ2 t
s η a a
∂ s = −q ∂ L−γ ∂ L+ξs
t t µs µ s s t
Asinµ s−η
= −q (a−Acosµ)−γ +ξs
µs σ2 s σ2 t
a s
∂ a = −q ∂ L+q ∂ L−γ ∂ L+ξa
t t µa µ aη η a a t
(cid:18) (cid:19) (cid:18) (cid:19)
Asinµ η−s η−a a−η a−Acosµ
= −q (a−Acosµ)+q + −γ + +ξa
µa σ2 aη σ2 σ2 a σ2 σ2 t
a s η η a
∂ η = −q ∂ L−γ ∂ L+ξη
t t aη a η η t
(cid:18) (cid:19) (cid:18) (cid:19)
a−η a−Acosµ η−s η−a
= −q + −γ + +ξη
aη σ2 σ2 η σ2 σ2 t
η a s η
(3.2.1)
where ξi are independent Brownian motions. Notice the solenoidal couplings between sensory
t
states and environmental states preclude any adiabatic assumption.
Using properties of marginals of jointly Gaussian random variables we can compute the steady
state
(cid:90)
p∗(s,a,µ) ∝ e−L(µ,s,a,η)dη = 1 N (cid:0) a;Acosµ,σ2(cid:1) N (cid:0) s;a,σ2+σ2(cid:1)
2π a s η
i.e. i.e. a uniform phase prior, a Gaussian action prior around Acosµ, and a sensor likelihood
concentrated around the action with variance σ2 +σ2. The interpretation of this density is that
s η
18
the triple (µ,s,a) forms a ribbon-shaped non-equilibrium steady state in S1×R2 aligned with the
subspace {a = Acosµ,s = a}. The solenoidal component makes the probability circulate along
this ribbon; however, the shape of the ribbon is fixed by the generative terms specified above.
The system can treated as a linear Gaussian chain, with Gaussian likelihoods for s | η and
η | s,a, providing an exact expression for the posterior
(cid:26) (s−η)2 (η−a)2(cid:27)
p(η | s,a) ∝ exp − − .
2σ2 2σ2
s η
Completing the square in η yields p(η | s,a) = N(m ,v ) where
post post
(cid:18)
1 1
(cid:19)−1 (cid:18)
s a
(cid:19)
v = + , m = v +
post σ2 σ2 post post σ2 σ2
s η s η
so that the posterior is parametrised by a variance of twice the inverse precision. Now let the
internal states parametrise a Gaussian posterior expressing beliefs about the world, q(η;µ,τ2) =
N(η;Acosµ,τ2). The free energy can be expressed analytically as
F(s,a,µ) = E [logq(η;µ,τ2)]−E [logp(η | s,a)]−logp(s,a,µ)
q q
= − 1 log2πτ2− 1 + 1 log2πv + 1 (cid:0) τ2+(Acosµ−m )2(cid:1) −logp(s,a,µ)
post post
2 2 2 2v
post
or, collecting like terms,
1 (cid:18) v τ2+(Acosµ−m )2 (cid:19)
post post
log + −1 −logp(s,a,µ)
2 τ2 v
post
so that when τ2 = v and m = Acosµ, the KL term vanishes.
post post
Taking gradients of the free energy yields the equations of motion
∂ µ = q ∂ F +q ∂ F −γ ∂ F +ξµ
t t µs s µa a µ µ t
(cid:18) (cid:19)
s−Acosµ 1 1
= q +q (a−Acosµ) +
µs σ2 µa σ2 σ2
s η a
(cid:18) (cid:19)
s−Acosµ a−Acosµ a−Acosµ
−γ Asinµ + + +ξµ
µ σ2 σ2 σ2 t
s η a
(3.2.2)
∂ a = −q ∂ F −γ ∂ F +ξa
t t µa µ a a t
(cid:18) (cid:19)
s−Acosµ a−Acosµ a−Acosµ
= −q Asinµ + +
µa σ2 σ2 σ2
s η a
(cid:18) (cid:19)
1 1
−γ (a−Acosµ) + +ξa.
a σ2 σ2 t
η a
We then simulated the system as a Kalman update for a linear Gaussian state space channel
η → s with input a.3 In practice, this means when simulating the system forwards, we set the
3 The python code for this simulation is available at https://github.com/DARSakthi/periodically_firing_
cells/.
19
parameters of the variational density to those of the posterior such that the variational density q
became the (instantaneous) Kalman posterior over the latent world state η and the instantaneous
precision-weighted prediction errors drove the gradient descent on F.
In Figure 4 sampled trajectories of µ under (3.2.1) and (3.2.2) are compared (Figure 5, a , re-
t t
spectively). Close correlation is observed between the two flow regimes. A corresponding tendency
towards zero of the sample mean of the KL term is observed in Figure 6. Plotted per-timepoint
sample means of the free energy under both flow regimes are contained in Figure 7. By adding the
precision-weighted prediction error (KL) term to the internal updates, F has a faster decrease and
a greater magnitude of decrease under (3.2.2); this is evidently from the prediction error driving
the flow of ∂ F to align q(η;µ) with p(η | s,a).
µ
Figure 4: Evolution of the internal state under L and under F. The dynamics of internal
states in two regimes, plotted on a polar spiral showing the evolution in the phase in time. The
dynamics under a gradient descent on L and on F show excellent agreement over all timescales.
20
Figure 5: Evolution of the active state under L and under F. The dynamics of the active
state in two regimes were also plotted; again, excellent agreement is observed.
Figure 6: Decrease of the KL divergence under the flow on F. The sample mean
(N = 200) of the KL divergence along trajectories of (3.2.2) is shown.
21
Figure 7: Decrease of the free energy under trajectories in both flow regimes. The
sample mean (N = 200) of the variational free energy along trajectories of (3.2.1) and along
trajectories of (3.2.2) are compared. Both drive the free energy towards zero, however, the flow
under F decreases more rapidly and attains a lesser minimum closer to zero.
The reason an inferential interpretation is useful here is in the coding of stimuli in neurones
and neuronal ensembles. Minimising variational free energy, and the resultant model of internal
and active states behaving as if they were performing Bayesian inference, endows the phase of
the oscillation with the interpretation of a posterior belief about expected incoming signals in the
environment and the Kalman-like updates as modulations of gain to minimise precision-weighted
predictionerror,ratherthanmerelyapositiononalimitcycle. Undertherelationshiptostochastic
thermodynamics forged in e.g. [24], quantitative predictions can be made about the cost of main-
taining such beliefs—and with an explicit low-dimensional representation of such signals given by
the concentration of the non-equilibrium steady state density, we have an organically emerging
neural manifold from those beliefs.
22
4. SOME PRELIMINARY PHILOSOPHICAL CONSIDERATIONS
We will now enter a more philosophical discussion concerning the use of the FEP, guiding
its application and interpretation. The preceding technical arguments establish the FEP as an
explanatory principle that can be applied to any thing (particle, person, or population) around
which one can draw a boundary or Markov blanket. This means that the FEP is what it says on
the tin: it is a principle. Like the principles of least action or of maximum entropy, the FEP is a
machinery that produces models [3].
Throughout, ‘as if’ marks an interpretive equivalence: internal flows that minimise variational
free energy can be read as variational inference without presupposing explicit, propositional mod-
els inside the system. Where specific mechanisms are identified (e.g., precision-weighted message
passing), this reading coincides with literal implementation; otherwise, it remains an explanatory
stance. How might one apply the FEP to make a model of a particular particle, person or popula-
tion? The answer is straightforward: the observer is trying to infer the generative model—which
contains unobservable internal states of an object—that best explains the observed dynamics of
some thing. What is observed is simply the blanket states of some ‘thing’ that shield internal
states from direct observation. If a generative model that best explains an object’s behaviour can
beidentified,thenbyconstructiontheFEPaffordsthemodelleracomplete(butnotover-complete)
description of the object’s internal dynamics, even though they can never be observed. As noted
above, technical term for this relationship between real-world system and generative model is “en-
tailment”: a system that conforms to the dependence structure of the generative model is said to
entail that generative model, with non-entailment of a known generative model being surprising.
If we now move to a meta-theoretical perspective and consider one particle (e.g., a scientist or
philosopher) observing another particle, we have the interesting situation where the modeller may
impute a particular generative model that best explains the mechanics of the observed particle. Is
thismeta-modelamaporaterritory?4 Usingagenerativemodeltocapturetheformalstructureof
the environment—and to distinguish it from the model that is carried by particles (the variational
density)—canbereadasanewtakeonthenouvelleAI ideathatphysicalsystemsaretheirownbest
models; orasanewtakeonthegoodregulatortheorem. Forinstance,inworkonroboticsfollowing
the tradition of embodied cognition [38, 39], practitioners have eschewed the construction of agents
4 This meta-theoretical move is commonplace in practical applications of the FEP and is sometimes referred to
as meta-Bayesian inference or, more simply, observing the observer [36]. Practically, it involves optimising the
parametersofagenerativemodel,suchthatundertheFEP(theidealBayesianobserverassumption)theobservable
behaviouroftheparticleisrenderedthemostlikely. ThisapplicationoftheFEPisoftendescribedascomputational
phenotyping [37].
23
with internal representations of some environment. In these systems, the physics and geometry of
the situation were sufficient to endow these agents with the capacity to couple to an environment.
The world is, on this view, its own best representation. In our view, this approach dovetails nicely
with FEP-theoretic modelling, where the generative model in FEP-theoretic constructions is just
a joint probability density defined over all the states (or paths) of a system. In other words, the
generative model is just our representation of the world as we believe it to be; and this model
need not be encoded directly in the particle or agent. Given a generative model or Lagrangian
of the appropriate sort, we can show that the system evinces a particular partition (i.e., contains
particles); and we can show that subsets of the system track each other, where tracking means
inferring or becoming the sufficient statistics of probabilistic beliefs about their external states.
We interpret this tracking as a form of inference, namely, variational or approximate Bayesian
inference under a generative model (i.e., the map).
In the context of this meta-question, one may ask how an observer could come to consciously
think an observed system is manipulating a model if the observer’s own brain is simply flowing
down a free-energy gradient. Here, an observer’s conscious report that ‘X is using an internal
model’ may arise from some higher-order generative model over another agent’s (or one’s own)
internal states, i.e., beliefs about the other system’s blanket-to-internal mappings (what we have
called ‘observing the observer’). Conscious thought in this context is a higher-level subset of those
internal states which summarises beliefs about others (and the self) within a hierarchical model.
Mechanistically,thisreportisimplementedbytheobserver’sownfree-energy-minimisingdynamics.
Inshort, ifweequatetheterritorywiththefulljointsystem(crucially, includingtheunobserved
internal states or dynamics of the observed thing), then the generative model plays the role of a
map that best describes that (partially observed) territory. Note here, that the map only exists in
relation to the observer—and, crucially, that the observer could be observing itself. In addition,
applications of the FEP enables one to map out the parts of a territory that are unobservable in
another sense; namely, in the sense that one cannot observe every state a given thing has been, or
willbein. Evenifonecould, theinternalstateswouldbeforeversequesteredbehindblanketstates.
Applications of the FEP place the generative model centre-stage as explanations for the behaviour
of things—in terms of a map of some territory that can only be observed through its impressions
on its Markov blanket. Here both observed and observer are described at the mechanistic level
by flows on free energy flows; at the informational level, the observer’s higher-order model licenses
attributions like ‘the observed system is manipulating an internal model’.
24
5. A MAP OF THAT PART OF THE TERRITORY THAT BEHAVES AS IF IT WERE
A MAP
Interestingly, because it incorporates this information about things that are coupled and their
coupling, it has been suggested that our FEP conflates the metaphorical map (our mathematical
modelofthethingmodelled)andtheterritory(thethingmodelled). Theso-calledmodel reification
or map-territory fallacy is a general critique of the practice of scientific modelling—in physics,
principally due to Cartwright [40, 41]. Some have referenced this fallacy in relation to prior work
on the FEP [42, 43]. In particular, it has been suggested that FEP-theoretic modelling conflates
the metaphorical “map”—(i.e., the scientific model that scientists use to make sense of some real-
world phenomenon)—and “territory,” i.e., the actual physical system that is being modelled. (See
[3, 6, 44, 45] for related critical discussions of such claims.) According to this line of thinking,
using the FEP to claim that we can model objects as themselves engaging in inference about
the statistics of their environment constitutes a case of model reification. The allegation is that,
in describing the dynamics of physical objects as implementing a form of inference—as opposed
to considering the inferential aspects of the explanation as pertaining to our scientific models of
those objects—the FEP theorist mistakenly conflates their metaphorical “map” of the territory
(i.e., the scientific, FEP-theoretic model) with the “territory” (the real-world system itself). This
constitutes a reification of our model by assuming some aspect of the model or “map” is a real
feature of the physical world or “territory” [6, 44]. By clarifying that this potential difficulty does
not directly and necessarily apply to FEP-theoretic modelling, we are able to turn the story on its
head: namely, we point out that the FEP provides a set of ultimate explanatory constraints on
what counts as a model (or map) of any thing that exists in the physical world.
We argue that the FEP can be understood, metaphorically, a “map” of sorts: a map of that
part of the territory which behaves or looks as if it were a map. In other words, the FEP provides
us with tools to understand the mathematically striking property of self-organising systems: that
they look as if they infer, track, reflect, or represent the statistics of their environment. Even more
simply, it gives us tools to model systems that look as if they are modelling the world.
HowdoesFEP-theoretictechnologyallowustomodelself-organisingsystemsasmodellingtheir
world? The generative model or joint probability p(η,b,µ) plays a central role. This generative
model can be read as a probabilistic specification of the states or paths characteristically occupied
by the joint particle-environment system. In other words, our scientific model (or map) includes
thegenerativemodelentailedbythesystem(i.e., theterritory). Inthissense, thegenerativemodel
25
we hypothesise—to account for observations of an object—allows us to represent the constraints
on the kinds of states the joint system can be found in. In other words, the generative model is a
modeller’smapofthewhole“territory”thatcharacterisestheobservedsystemanditsenvironment
(including the modeller, as an observer in the environment).
The technology of the FEP allows us to say that certain components of the system being
considered look as if they track other components, and that this is a feature of our map contained
in a generative model under a particular partition. In providing us with a calculus allowing us
to model things as modelling other things, the FEP provides us with a map (a scientific model)
of any possible map or tracking relationship between things (namely, a mapping between internal
and external states, mediated by blanket states).5
Does our ability to always describe some dynamics as a form of inference imply that those
dynamics are literally a form of inference, as opposed to being a manner of describing those
dynamics? Here, it is important to defuse a possible (but in our view na¨ıve) objection. It is
a truism that physical systems need not explicitly calculate their trajectories of motion, to be
modelled as pursuing such trajectories. In developing an inferential account of the dynamics of
systems, we need not assume that the particle itself is literally performing inference.6 What is at
stake with the FEP is an ‘as if’ description. We simply take the fact that surprisal varies with the
dynamics of the system, which implies free energy is minimised; the minimisation of this quantity
is mathematically equivalent to inference, in the sense of being an estimator.
In summary, our scientific, FEP-theoretic model is, metaphorically, a map that allows us to say
thatsome(internal)subsetofthesystemlooksasifitpossessesamap; whichweinterpretformally
as tracking the statistics of another (external) subset. Our map (as scientists and modellers) can
be identified in an unproblematic way as the generative model of the system, which FEP-theoretic
technology enables us to write down. It is by construction that our model is a model of the
modelling capacities of subsets of the real-world system: that is, our map is precisely a map of how
objects behave as if they were maps, allowing us to construct scientific models of the maps encoded
(or looking as if they were encoded) by the internal subset of the system considered. This is an
answer to allegations that FEP-theoretic modelling commits the map-territory fallacy: namely, it
shows that there is no conflation of the map and the territory, in the sense of the map we postulate
as being embodied by the system within the mathematical machinery of the FEP. We remark that
5 Here, we are assuming representationalist accounts of scientific practice. For alternatives, see [6].
6 Ways of defending stronger, increasingly literal versions of this positions are available as well. For a defence of
the claim that physical systems are quite literally in the business of inference, see [46, 47]. Related to this, views
of physical systems implementing computations has been articulated [48, 49], as well as views where systems
self-organise by computing predictions [23, 26, 50, 51].
26
we do not claim to invalidate the map-territory fallacy itself. That is, we do not suggest that the
map and the territory are never conflated; they obviously are in cases of genuine model reification
(as discussed cogently by [44]). Models built using the FEP are still scientific models, and the
properties of FEP-theoretic models also run the risk of reification. Instead, we are addressing
the claim that the FEP is limited in its effectiveness or engages in model reification because real
physical things cannot be modelled as themselves modelling their environments by minimising free
energy.
Taking all of the above, we might say that the FEP ultimately provides us with a map of any
possible map whatsoever of, or held by, a physical system. This echoes seminal work in philosophy
byWittgenstein[52]. InhisfamousTractatus Logico-Philosophicus, Wittgensteinsetouttodelimit
the domain of sensible propositions by determining the general form of a proposition, an utterance
able to carry a truth value. This set ultimate constraints on what is sensible, and bounded what
is meaningful “from inside” of language itself. Similarly, the FEP provides ultimate constraints
on possible maps or modelling relations, which arise from what it means to be a map or model
of a physical process at all. Indeed, any modelling process consistent with the laws of physics
must conform to the FEP, such that the FEP sets ultimate limits on what can count as a map or
model—starting from within the technology of mapping or modelling itself.
In sum, the generative model is a kind of scientific model, which harnesses what we know about
the dependency structure of the system. A variational density, on the other hand, is a probability
density over external states or paths, implemented in terms of the sufficient statistics of external
states given blanket states. It is an explicitly inferential representation of the manner in which
subsets of a system with a particular partition track each other: in which case, we are describing
a part of the territory that, as seen in our map, also behaves as if it were a map.
6. CONCLUSION
We argued that, in describing ‘things’ or ‘particles’ as estimators or (deflated) representations
of the systems to which they are coupled, the FEP-theoretic apparatus does not commit the
map territory fallacy, i.e., it is false that applications the FEP necessarily reify aspects of the
metaphorical map (i.e., our scientific model), mistakenly taking them to be part of the territory
(i.e., the real-world system that we wish to model); although this remains possible in principle.
We have further argued that this allegation itself constitutes a map-territory fallacy fallacy. In
distinguishing the generative model and the variational density—and the generative model and
27
the generative process from the real-world system that we aim to model—the technology of FEP-
theoretic modelling allows us to construct a map of that part of the territory which looks to an
external observer as if or behaves as if it were a map, without committing model reification. The
FEP is thus, metaphorically, a map of any possible map whatsoever of, or held by, a physical
system. One ought to celebrate this ‘territory-map mapping,’ and the FEP is ultimately, at its
core, a principled approach to the formalisation of this mapping.
[1] Maxwell J D Ramstead, Dalton A R Sakthivadivel, and Karl J Friston. On the map-territory fallacy
fallacy. 2022. Preprint arXiv:2208.06924.
[2] Karl J Friston. A free energy principle for a particular physics. 2019. Preprint arXiv:1906.10184.
[3] Maxwell J D Ramstead, Dalton A R Sakthivadivel, Conor Heins, Magnus Koudahl, Beren Millidge,
Lancelot Da Costa, Brennan Klein, and Karl J Friston. On Bayesian mechanics: a physics of and by
beliefs. Interface Focus, 13(3):20220029, 2023.
[4] Karl Friston, Lancelot Da Costa, Dalton A R Sakthivadivel, Conor Heins, Grigorios A Pavliotis, Max-
well Ramstead, and Thomas Parr. Path integrals, particular kinds, and strange things. Physics of Life
Reviews, 47:35–62, 2023.
[5] Matthew J Beal. Variational algorithms for approximate Bayesian inference. PhD thesis, University
of London, University College London, 2003.
[6] MelAndrews. Themathisnottheterritory: navigatingthefreeenergyprinciple. Biology&Philosophy,
36(3):30, 2021.
[7] Maxwell J D Ramstead, Karl J Friston, and Inˆes Hip´olito. Is the free-energy principle a formal theory
of semantics? From variational density dynamics to neural and phenotypic representations. Entropy,
2020.
[8] Maxwell J D Ramstead, Michael D Kirchhoff, and Karl J Friston. A tale of two densities: active
inference is enactive inference. Adaptive Behavior, 28(4):225–239, 2020.
[9] Judea Pearl. Graphical Models for Probabilistic and Causal Reasoning, pages 367–389. Springer, 1998.
[10] Dalton A R Sakthivadivel. Weak Markov blankets in high-dimensional, sparsely-coupled random dy-
namical systems. 2022. Preprint arXiv:2207.07620.
[11] Dalton A R Sakthivadivel. A worked example of the Bayesian mechanics of classical objects. In Active
Inference: Third International Workshop, IWAI 2022, Grenoble, France, September 19, 2022, Revised
Selected Papers, pages 298–318. Springer, 2023.
[12] Edwin T Jaynes. Information theory and statistical mechanics. Physical Review, 106(4):620, 1957.
[13] Edwin T Jaynes. Probability Theory: The Logic of Science. Cambridge University Press, 2003.
[14] Karl Friston, Klaas Stephan, Baojuan Li, and Jean Daunizeau. Generalised filtering. Mathematical
Problems in Engineering, 2010(1):621670, 2010.
28
[15] Lancelot Da Costa, Karl J Friston, Conor Heins, and Grigorios A Pavliotis. Bayesian mechanics for
stationary processes. Proceedings of the Royal Society A, 477(2256), 2021.
[16] Dalton A R Sakthivadivel. Towards a geometry and analysis for Bayesian mechanics. 2022. Preprint
arXiv:2204.11900.
[17] Udo Seifert. Stochastic thermodynamics, fluctuation theorems and molecular machines. Reports on
Progress in Physics, 75(12):126001, 2012.
[18] Ping Ao. Potential in stochastic differential equations: novel construction. Journal of Physics A:
Mathematical and General, 37(3):L25, 2004.
[19] Yi-AnMa,TianqiChen,andEmilyFox. AcompleterecipeforstochasticgradientMCMC. InAdvances
in Neural Information Processing Systems, volume 28, pages 2917–2925, 2015.
[20] Ruoshi Yuan, Ying Tang, and Ping Ao. SDE decomposition and A-type stochastic interpretation in
nonequilibrium processes. Frontiers of Physics, 12:1–9, 2017.
[21] LancelotDaCostaandGrigoriosAPavliotis. Theentropyproductionofstationarydiffusions. Journal
of Physics A: Mathematical and Theoretical, 56(36):365001, 2023.
[22] Mark I Freidlin and Alexander D Wentzell. Random Perturbations of Dynamical Systems, volume 260
of Grundlehren der mathematischen Wissenschaften. Springer, 1998.
[23] Udo Seifert. From stochastic thermodynamics to thermodynamic inference. Annual Review of Con-
densed Matter Physics, 10:171–192, 2019.
[24] Thomas Parr, Lancelot Da Costa, and Karl Friston. Markov blankets, information geometry and
stochastic thermodynamics. Philosophical Transactions of the Royal Society A, 378(2164):20190159,
2020.
[25] Karl Friston, J´er´emie Mattout, Nelson Trujillo-Barreto, John Ashburner, and Will Penny. Variational
free energy and the Laplace approximation. NeuroImage, 34(1):220–234, 2007.
[26] KaiUeltzh¨offer,LancelotDaCosta,DanielaCialfi,andKarlJFriston. Adrivetowardsthermodynamic
efficiency for dissipative structures in chemical reaction networks. Entropy, 23(9):1115, 2021.
[27] KarlFriston,SpyridonSamothrakis,andReadMontague. Activeinferenceandagency: optimalcontrol
without cost functions. Biological Cybernetics, 106:523–541, 2012.
[28] Giovanni Pezzulo, Francesco Rigoli, and Karl Friston. Active inference, homeostatic regulation and
adaptive behavioural control. Progress in Neurobiology, 134:17–35, 2015.
[29] L´eo Pio-Lopez, Ange Nizard, Karl Friston, and Giovanni Pezzulo. Active inference and robot control:
a case study. Journal of The Royal Society Interface, 13(122):20160616, 2016.
[30] Giovanni Pezzulo, Francesco Rigoli, and Karl J Friston. Hierarchical active inference: a theory of
motivated control. Trends in Cognitive Sciences, 22(4):294–306, 2018.
[31] Manuel Baltieri and Christopher L Buckley. PID control as a process of active inference with linear
generative models. Entropy, 21(3):257, 2019.
[32] Beren Millidge, Alexander Tschantz, Anil K Seth, and Christopher L Buckley. On the relationship
between active inference and control as inference. In Active Inference: First International Workshop,
29
pages 3–11. Springer, 2020.
[33] Mohamed Baioumy, Paul Duckworth, Bruno Lacerda, and Nick Hawes. Active inference for integ-
rated state-estimation, control, and learning. In 2021 IEEE International Conference on Robotics and
Automation, pages 4665–4671. IEEE, 2021.
[34] KarlFriston,MichaelLevin,BiswaSengupta,andGiovanniPezzulo.Knowingone’splace: afree-energy
approach to pattern regulation. Journal of the Royal Society Interface, 12(105):20141383, 2015.
[35] Karl J Friston. A free energy principle for a particular physics. 2019. Preprint arXiv:1906.10184.
[36] Jean Daunizeau, Hanneke E M Den Ouden, Matthias Pessiglione, Stefan J Kiebel, Klaas E Stephan,
andKarlJFriston. Observingtheobserver(I):meta-Bayesianmodelsoflearninganddecision-making.
PLoS ONE, 5(12):e15554, 2010.
[37] Philipp Schwartenbeck and Karl J Friston. Computational phenotyping in psychiatry: a worked ex-
ample. eNeuro, 3(4), 2016.
[38] Randall D Beer. Computational and dynamical languages for autonomous agents. In Mind as Motion:
Explorations in the Dynamics of Cognition, pages 121–147. 1996.
[39] Rodney A Brooks. Intelligence without representation. Artificial Intelligence, 47(1-3):139–159, 1991.
[40] Nancy Cartwright. How the Laws of Physics Lie. Oxford University Press, 1984.
[41] Nancy Cartwright. Nature’s Capacities and their Measurement. Oxford University Press, 1989.
[42] Jelle Bruineberg, Krzysztof Dolega, Joe Dewhurst, and Manuel Baltieri. The emperor’s new Markov
blankets. Behavioral and Brain Sciences, pages 1–63, 2020.
[43] Thomas van Es. Living models or life modelled? On the use of models in the free energy principle.
Adaptive Behavior, 2020.
[44] Mel Andrews. Making reification concrete: a response to Bruineberg et al. Behavioral and Brain
Sciences, 45:e186, 2022.
[45] Michael D Kirchhoff, Julian Kiverstein, and Ian Robertson. The literalist fallacy and the free en-
ergy principle: model-building, scientific realism, and instrumentalism. The British Journal for the
Philosophy of Science, 2022.
[46] Alex B Kiefer. Literal perceptual inference. In Philosophy and Predictive Processing, 2017.
[47] Alex B Kiefer. Psychophysical identity and free energy. Journal of the Royal Society Interface,
17(169):20200370, 2020.
[48] Clare Horsman, Susan Stepney, Rob C Wagner, and Viv Kendon. When does a physical system
compute? Proceedings of the Royal Society A, 470(2169):20140182, 2014.
[49] Scott Aaronson. Guest column: NP-complete problems and physical reality. ACM SIGACT News,
36(1):30–52, 2005.
[50] Susanne Still, David A Sivak, Anthony J Bell, and Gavin E Crooks. Thermodynamics of prediction.
Physical Review Letters, 109(12):120604, 2012.
[51] NikolayPerunov,RobertAMarsland,andJeremyLEngland.Statisticalphysicsofadaptation.Physical
Review X, 6(2):021036, 2016.
30
[52] Ludwig Wittgenstein. Tractatus Logico-Philosophicus. 1922. 2013 edition, Routledge.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "A framework for the use of generative modelling in non-equilibrium statistical mechanics"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
