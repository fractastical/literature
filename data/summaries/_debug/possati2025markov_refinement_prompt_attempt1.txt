=== IMPORTANT: ISOLATE THIS PAPER ===
You are revising a summary for ONLY the paper below. Do NOT reference or use content from any other papers.
Paper Title: Markov Blanket Density and Free Energy Minimization
Citation Key: possati2025markov
REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Issues to fix:
1. CRITICAL: The current summary has severe repetition issues. You MUST eliminate all repeated sentences, phrases, and paragraphs. Each idea should be expressed only once. If you find yourself repeating content, remove the duplicates entirely. Focus on variety and uniqueness in your wording.
2. Severe repetition detected: Same sentence appears 4 times (severe repetition)

Current draft (first 2000 chars):
Okay, let’s begin.### OverviewThis document provides a concise summary of the paper “Markov Blanket Density and Free Energy Minimization.” The paper investigates the relationship between the concept of a Markov blanket – a set of variables that renders a system Markovian – and the free energy minimization framework commonly used in statistical physics and machine learning. The authors propose a novel approach to understanding and modeling complex systems by explicitly considering the density of Markov blankets, offering a more nuanced and potentially more accurate representation of the underlying dynamics.### MethodologyThe core methodology presented in the paper centers around the concept of “Markov blanket density” (ρ(x)) – a measure of how tightly connected a given node (x) is to its Markov blanket. The authors employ a variant of the Maximum Entropy Method (MaxEnt) to estimate this density, leveraging the statistical properties of the system. Specifically, they construct a probability distribution over the Markov blanket, given the observed state of the node of interest. This approach allows for the quantification of the uncertainty and dependencies within the system, providing a more granular representation of the information flow. The authors utilize a Kraskov-Stögbauer-Grassberger (KSG) estimator, a non-parametric technique, to estimate the mutual information between the node and its Markov blanket. This estimator is based on the distance to the k-th nearest neighbor, providing a robust and accurate estimate of the mutual information.### ResultsThe authors demonstrate that the estimation of Markov blanket density is crucial for accurate free energy minimization. They show that the accuracy of the free energy minimization depends directly on the density of the Markov blanket. Specifically, they show that the error in estimating the free energy decreases proportionally to the square root of the number of nodes in the Markov blanket. They also demonstrate that t...

Key terms: blanket, treat, markov, energy, density, independence, spatially, minimization

=== FULL PAPER TEXT ===
Markov Blanket Density and
Free Energy Minimization
Luca M. Possati
July 2025
Abstract
Thispaperintroducesanovelgeometricalframeworkforunderstandingthecon-
ditionsunderwhichtheFreeEnergyPrinciple(FEP)canbemeaningfullyapplied.
Departing from formulations that treat the FEP as a universal variational imper-
ative governing all self-organizing systems, we define a spatially continuous field
called the Markov Blanket Density ρ(x) ∈ [0,1]. This quantity measures, at each
point in a spatial domain Ω ⊂ Rn, the degree of conditional independence be-
tween internal and external states given a local blanket. Rather than assuming
the existence of a Markov blanket as a binary structural feature of the system, we
treat conditional independence as a graded and spatially variable property. We
show that the standard components of the FEP—including generative modeling,
variational inference, and action as free energy minimization—are only definable
in regions where ρ(x) < 1, i.e., where conditional independence fails and mutual
information across the internal-external divide remains non-zero. In regions where
ρ(x)=1,noinferentialcouplingispossible,andthevariationalfreeenergybecomes
ill-definedorvacuous. ThisimpliesthattheFEPisnotagenerallawbutalocally
validphenomenologicalregularity,contingentontheinformationalgeometryofthe
environment. The model introduces a differential-geometric structure on the infor-
mationalfieldρ(x),defininganinformation-theoreticmetric,aLaplaciancurvature
operator, and a log-transformed free energy landscape. In this setting, epistemic
dynamicsarereinterpretedasgradientflowsonρ(x),andagentbehaviorbecomesa
responsetoinformationaltopography. Thisapproachprovidesanewfoundationfor
the FEP as a situated, spatially constrained phenomenon, and offers a principled
framework for modeling inference in distributed, multi-agent, or weakly bounded
systemswhereclassicalblanketstructuresareunstableorundefined. Theresultisa
shiftintheexplanatory architectureoftheFEP:fromafoundational principleim-
posedonallsystems,toanemergentregimeofbehaviorpermittedbythestructure
of the environment. We argue that this reframing offers a more falsifiable, flexible,
and geometrically grounded interpretation of inference and self-organization, with
implicationsforformalepistemology,situatedcognition,andthedesignofepistemic
systems in both natural and artificial contexts.
1 Introduction
The Free Energy Principle (FEP) provides a powerful framework to understand how
agents (i.e., self-organizing systems such as living systems) maintain their structure by
minimizingvariationalandexpectedfreeenergy[1,2,3,4,9,20,31,37]. CentraltoFEP
is the Markov blanket, traditionally viewed as a discrete boundary separating internal
states from external environmental states. However, this binary view limits our ability
to model nuanced interactions and spatial dynamics.
In this paper, we introduce "Markov blanket density" as a continuous scalar field
quantifying the degree of conditional independence between internal and external states
at every spatial point relative to an observer and their scale of observation. Blanket
strength is thus measured by how effectively bkanket states mediate interactions, struc-
turing space into continuous gradients of coupling. Preferred states become regions of
optimal coupling rather than purely internal homeostatic targets. Although the fun-
damental idea—that agents naturally move towards regions of lower Markov blanket
1
5202
guA
8
]CN.oib-q[
5v49750.6052:viXra
density (greater coupling)—is intuitive, the paper offers originality through: (a) Shifting
from discrete partitions to a continuous scalar field, allowing nuanced spatial modeling;
(b) Rigorous mathematical formalization capturing precise, verifiable dynamics through
spatial gradients; (c) Practical applicability, providing a robust framework for empirical
predictions and novel simulations.
We think that classic active inference fails to properly account for the spatial di-
mension, collapsing it into a notion of space as an empty, passive, and predictable “en-
vironment.” In doing so, active inference cannot fully grasp the concept of affordance,
reducing it to a set of predictions about the environment. Essentially, active inference
remainscaptivetoalab-basedperspective,wherespaceadaptstohypothesesratherthan
hypotheses adapting to space. The point is that space is complex, as are affordances.
The very unity of perception and action depends on that complexity.
Through detailed mathematical analysis, this paper demonstrates how free energy
minimization dynamics depend on variations in Markov blanket (MB) density, including
scenarios that invert typical inference dynamics. By bridging ecological and embodied
perspectives with formal variational inference, this work advances our understanding of
the embodied mind as actively embedded within dynamically structured informational
environments. The purpose of this paper is to introduce an informational geometry of
inferential space: not only “what an agent does,” but “where it does it,” how much it can
infer, at what speed, and with what access to information. The central thesis is that
the FEP should not be treated as a universal law, but rather as an emergent regime of
behavior permitted by the geometry of ρ(x). We formalize this framework by construct-
ing an informational metric, a free energy landscape defined via log-transform of ρ(x),
and a dynamic formulation in which agent trajectories follow informational gradients.
ThisreinterpretationpositionstheFEPnotasaprincipletobeimposeda priori, butas
a local affordance of the informational structure of the environment—one that may fail
to hold, shift over time, or vary across systems. In so doing, we aim at extendind the
explanatory power of the FEP to a broader class of systems, including weakly bounded
agents, distributed cognitive architectures, and informationally heterogeneous environ-
ments. OuraimisnottorejecttheFEP,buttorecoveritasaspecialcasewithinamore
general geometric and epistemic framework—one that better accommodates the spatial,
graded, and context-sensitive nature of real-world inference. In other words, the FEP
emerges as a local property of regions with good informational permeability.
As regards the structure of this paper, Section 2 introduces some key concepts from
active inference and the notion of the Markov blanket. Section 3 introduces the central
thesis of the paper, connecting it to existing literature and offering several conceptual
clarifications. BeginningwithSection4,themathematicalframeworkislaidout. Aclear
definition of Markov blanket density is provided, followed by an operational definition
in Section 4.4—namely, an algorithm for identifying and computing the Markov blanket
density at each point in a hypothetical space. Three theorems (Sections 5–9) follow,
illustrating the relationship between free energy minimization and Markov blanket den-
sity. Sections 10–11 present a fourth theorem, which demonstrates how the temporal
evolution of free energy minimization depends on and is continually modulated by the
Markov blanket density. Section 12 addresses the problem of how an active inference
agent learns the space and how Markov blanket density is incorporated into the agent’s
inferential dynamics. Section 13 offers a complete axiomatic derivation of the Markov
blanket density concept, showing how the FEP can be seen as a local emergent effect of
this density.
Throughoutthepaper,aseriesoffiguresillustratestheresultsofvarioussimulations.
The appendices cover technical details.
2 The Free Energy Principle
2.1 A basic outline
The FEP is a mathematical framework rooted in statistical physics, information theory,
andvariationalinferencetechniquesfrommachinelearning[9,22]. Itprovidesaunifying
account of self-organizing systems by interpreting their dynamics in terms of the mini-
2
mization of variational free energy. In particular, consider a random dynamical system
that satisfies the following conditions:
• it exhibits a degree of ergodicity, allowing time-averaged behavior to approximate
ensemble statistics;
• it possesses a pullback attractor, that is, a set of states toward which the system
tends over time — its "preferred" or most frequently occupied states;
• it admits an ergodic density that probabilistically characterizes long-term state
occupancy; and
• it maintains a degree of separation from its environment, such that internal and
external states can be distinguished (e.g., via a Markov blanket structure).
Undertheseassumptions,thesystem’sbehaviorcanbeinterpretedasperformingapprox-
imate Bayesian inference by minimizing a quantity known as variational free energy. In
this context, the flow of states (e.g., internal states, active states) follows a gradient de-
scentonvariationalfreeenergy,whichservesasanupperboundonthesystem’ssurprisal
(or self-information, see Table 1) about its sensory states. That is, even in the absence
of an explicit model, the system behaves as if it were inferring the causes of its sensory
inputs and acting to maintain itself within a bounded set of preferred states — thereby
resisting disorder and preserving its structural and functional integrity. As mentioned,
Self-information I(x)
Surprise or informativeness of a specific outcome x. High for rare events, zero for certain
ones.
Formal definition: I(x) = −log p(x), where p(x) ∈ (0,1] and b is typically 2 (bits), e
b
(nats), or 10 (Hartleys).
Entropy H(X)
Expected uncertainty or average surprise over all outcomes of a random variable X.
Formal definition: H(X)=− (cid:80) p(x)log p(x)
x∈X b
Kullback–Leibler divergence D (P ∥Q)
KL
Information lost when using distribution Q to approximate the true distribution P.
(cid:16) (cid:17)
Formal definition: D (P ∥ Q) = (cid:80) p(x)log p(x) , defined only if p(x) > 0
KL x∈X b q(x)
implies q(x)>0.
Table 1: Essentialdefinitionsofself-information,entropy,andKLdivergenceusedintheFEP
framework.
theFEPisamathematicalmodelingframework. Itisnotatheoryseekingempiricalval-
idation, but rather a mathematical-physical formalism that can be used to generate new
hypotheses or analyze data. In itself, however, it remains a purely theoretical construct,
without predictive aims.
Whenappliedtolivingsystems(e.g.,thebrain),theFEPgivesrisetowhatisknown
as active inference [30, 9]. In this framework, a living system maintains its structural
and functional integrity by resisting the natural tendency toward disorder—that is, by
remaining within a bounded set of preferred states despite environmental volatility (i.e.,
any living system tends, on average, to move along the gradient that leads toward its
attracting set, i.e., the pullback attractor). To do so, the system must possess a hierar-
chical generative model of the hidden causes of its sensory inputs—a probabilistic model
that is continuously tested and updated through Bayesian inference, i.e., a generative
model. Since exact inference is generally intractable in realistic conditions, the system
performs approximate variational inference: it selects an approximate posterior distri-
bution and updates its parameters iteratively to minimize the divergence from the true
posterior. This optimization process is formally equivalent to maximizing the Evidence
3
Lower Bound (ELBO) in machine learning. The objective of this process is to minimize
a quantity known as variational free energy. Variational free energy serves as an upper
boundonsurprisal,orthenegativelogmodelevidence,whichquantifieshowunexpected
sensory inputs are under the model. In formal terms, free energy is decomposed into the
sum of a Kullback–Leibler divergence (between the approximate and true posterior) and
a term representing log evidence (see Table 2). Minimizing free energy thus corresponds
to maximizing model evidence. This process of continuously updating beliefs and ac-
tionstoreducefreeenergyenablesthesystemtomaintaincoherenceandadaptivityina
changingenvironment. Inthissense,self-organizationisreframedasself-evidencing—the
system acts in ways that confirm its own model of the world. Finally, the FEP asserts
that "all biological systems maintain their integrity by actively reducing the disorder or
dispersion (i.e., entropy) of their sensory and physiological states by minimising their
variational free energy" [4].
1. Free energy as a bound on surprise
F(o)≥−logp(o)
Free energy upper-bounds the surprisal (negative log model evidence) of sensory input.
Minimizing it helps explain perception as evidence maximization.
2. Free energy as a variational bound
F(q)=KL(q(s)∥p(s|o)) − logp(o)
Free energy is minimized when the approximate posterior q(s) matches the true posterior
p(s|o). This is the essence of variational Bayesian inference.
3. Free energy as energy minus entropy
F(q)=E [−logp(o,s)] + E [logq(s)]
q q
Freeenergyisthesumofexpectedpredictionerrorandthecomplexityoftheapproximate
posterior. It balances accuracy and complexity.
Table 2: Three equivalent formulations of variational free energy.
It should be emphasized that the recent literature on the FEP and active infer-
ence—from essentially [1, 6, 7, 8, 40]—has introduced and developed a new formulation
of the FEP, shifting from a state-based formulation to a path-based one that no longer
requires the concept of a NESS. In some of the more advanced FEP formulations, it
is shown how the FEP can be derived simply from Jaynes’s principle of maximum en-
tropyormaximumcaliberundertheconstraintthatthereareboundariesofcertainkinds.
TheseformulationsshowthattheFEPisjustmaximumcaliberwithexplicitboundaries.
Whereas the classical formulations (Friston 2019) start from the equations of statistical
physics (the Langevin or Fokker–Planck equation) and arrive at an equivalent descrip-
tion of the system in information-theoretic terms (a gradient descent over a free-energy
functional), themoreadvancedapproachesinsteadbeginwithmaximumcalibertoshow
how, once boundaries are imposed, it becomes the FEP. This addresses criticisms that
the FEP depends on physical presuppositions (e.g., the NESS).
2.2 Markov blankets
The concept of Markov blanket is crucial in the formulation of the FEP. In fact, "we
assume that for something to exist it must possess (internal or intrinsic) states that can
be separated statistically from (external or extrinsic) states that do not constitute the
thing" [1]. The existence of things implies the existence of Markov blanket, namely, "a
set of states that render the internal and external states conditionally independent" [1].
But what does it mean "separation" here? If the space in which the active inference
agentmovesiscomposedofnestedMarkovblankets, howtheagentpassesthroughthese
blankets and their permeability? "States of things are constituted by their Markov
blanket, while the Markov blanket comprises the states of smaller things with Markov
blankets within them – and so on ad infinitum" [1].
A Markov blanket is “a statistical partitioning of a system into internal states and
externalstates,wheretheblanketitselfconsistsofthestatesthatseparatethetwo” [7,1].
TheMarkovblanketdividesthesystemintothreegroupsofstatisticalvariables: internal
states, external states, and blanket states. As Friston claims, “the dependencies induced
4
byMarkovblanketscreateacircularcausalitythatisreminiscentoftheaction-perception
cycle” [1]. Circular causality here means that “external states cause changes in internal
states, via sensory states, while the internal states couple back to the external states
through active states, such that internal and external states influence each other in a
vicariousandreciprocalfashion” [1]. Consequently, theinternalandexternalstatestend
to synchronize over time (i.e., coupling), much like two pendulums attached to opposite
ends of a wooden beam gradually swinging in unison.
The Markov blanket thus allows a certain statistical boundary to be defined between
internalstatesandexternalstates,whicharemediatedsolelybyactivestatesandsensory
states [38]. This means that, given the Markov blanket—that is, the sensory and active
states—the internal and external states are conditionally independent. In other words,
oncetheblanketisknown,knowingadditionalinformationabouttheexternalstatesdoes
not further constrain or inform the internal states. This structure ensures that internal
and external states remain independent while being connected only through the active
and sensory states. Active and sensory states “shield” the internal states by creating
a statistical boundary [2, 5, 6, 7]. Put simply, internal states cannot directly affect
external states but can do so indirectly by influencing active states. Likewise, external
statescannotdirectlyimpactinternalstatesbutcandosoindirectlybyaffectingsensory
variables (see Table 3).
Free energy is a functional—that is, a function of a function—that quantifies the
probability distribution encoded by the internal states of the system. Importantly, this
differs from surprise, which is a function of the sensory and active states on the Markov
blanket itself. Put differently: free energy is a function of probabilistic beliefs (i.e.,
internal states) about external states—that is, expectations about the likely causes of
sensory input. When these beliefs match the true Bayesian posterior, variational free
energy becomes equal to surprise. Otherwise, it serves as a tractable upper bound on
surprise. This is why self-organizing systems can be characterized as minimizing varia-
tionalfreeenergy,andtherebyminimizingsurprise,throughthecontinuousoptimization
of their beliefs about what lies beyond their Markov blanket. Finally, the FEP tells us
"howthequantitiesthatdefineMarkovblanketschangeasthesystemmovestowardsits
variational free energy minimum" [4].
Element Symbol Description
Internal states I Hidden states of the system that encode beliefs about ex-
ternal causes; not directly influenced by external states.
External states E Statesintheenvironmentthatinfluencesensorystatesbut
are not directly influenced by internal states.
Sensory states S Statesthatreceiveinputfromexternalstatesandinfluence
internal states; part of the Markov blanket.
Active states A Statesinfluencedbyinternalstatesthatactuponexternal
states; part of the Markov blanket.
Markov blanket B =S∪A The boundary of the system that mediates interactions
between internal and external states through sensory and
active channels.
Conditional independence — GiventheblanketB,internalandexternalstatesarecon-
ditionally independent: p(I,E |B)=p(I |B)p(E |B).
Table 3: Formal components of a Markov blanket in active inference.
3 The Space as a Continuous Gradient of Markov Blan-
ket Strengths
Inthissectionweintroducethecentralphilosophicalthesisofthispaper. Inthefollowing
one we develop a formal demonstration. It all stems from a rather naive and abstract
question: Whatwouldaspacebelikeifeverypointwerecomposedofinternalandexternal
5
states, i.e. had a Markov blanket? And how would an agent with a blanket of their own
move in this space?
Free energy minimization is generally described in temporal terms: “Strictly speak-
ing, free energy is only ever minimized diachronically—that is, over some discrete time
span—as a process” [2]. What role does space play in this process? The space through
whichanactiveinferenceagentmovesisnotanemptyoruniformcontainer—itisinstead
a structure composed by nested Markov blankets: “[...] we should be able to describe
the universe in terms of Markov blankets of Markov blankets—and Markov blankets all
the way up, and all the way down” [2]. The key issue is how we conceptualize Markov
blankets and the statistical boundaries they define.
Classic works on active inference fails to properly account for the spatial dimension,
treating space as a passive and predictable “environment.” By doing so, it cannot fully
grasptheconceptofaffordance,reducingittoasetofpredictionsaboutthatenvironment.
Inthisview,affordancesarenotinherentlypartoftheenvironmentitself;theydependon
thepredictionsandknowledgeoftheagentinteractingwithit. Inactiveinference, space
plays no active role in shaping the agent’s trajectories—and this is not compatible with
Gibson’s view of affordance [29]. In essence, active inference remains confined to a lab-
basedperspective,wherespaceadaptstohypothesesratherthanhypothesesadaptingto
space. Thepointisthatspaceiscomplex,asareaffordances—theycannotbereducedto
the agent’s predictions. Affordances “are not simply static features of the environment,
independent of the presence and engagement of an agent, nor are they states of the
cognitive agent alone” [28]. The very unity of perception and action depends on that
complexity. In a nutshell, space is not entirely predictable, and above all, space shapes
anddistortsourpredictions. Aswehopetoshow,sincetheblanket-densityfactordirectly
modulates how strongly sensory evidence can update internal beliefs (and therefore the
generative model), it does in effect “shape” the model the agent uses.
Atthispoint,thenextquestionbecomes: Howcanwereconceptualizespaceindepen-
dentlyofanagent’spredictions,thatis,itsgenerativemodel? Thehypothesiswewantto
propose and test here is that the space inhabited by active inference agents is populated
with Markov blankets that can vary (along a spectrum) in their degree of permeability
or porosity—that is, Markov blankets that are more or less “strong,” exhibiting higher
or lower degrees of separation relative to an observer and their scale of observation. The
strengthofaMarkovblanket(i.e.,howwelltheblanketinsulatestheinside)isthedegree
to which it enforces conditional independence between internal and external states, via
themediatingsensoryandactivestates. Therefore, thespaceisstructuredbyacontinu-
ousgradientofMarkovblanketstrengths. Fromthisspatialperspective,preferredstates
can be reinterpreted as configurations of optimal coupling—zones of dynamic synchro-
nization with other Markov blankets—rather than purely internal homeostatic targets.
Thus, from this perspective, every point in the space through which the active in-
ference agent moves is associated with a Markov blanket that separates internal and
external variables. Based on this, we define the Markov blanket density, which varies
continuously, like a spectrum, and quantifies the local degree of informational isolation
between those internal and external variables.
3.1 Connection to the Literature
Thispaperbuildsonsomefindingsfrompreviousliteratureandaimstounifyandextend
them.
[7] advances the FEP by translating its abstract notions of conditional independence
and “things” into a concrete, unsupervised learning algorithm. Recognizing that any
identifiable object must correspond to a partition—internal, boundary, external—their
variational Bayesian expectation maximization framework treats each microscopic ele-
ment as governed by one of several low dimensional latent processes. During inference,
elements are dynamically assigned to roles by maximizing an ELBO, and a “Bayesian
attention” mechanism tracks how the inferred boundary can move, split, or merge over
time. Through case studies as diverse as Newton’s cradle, a propagating combustion
front, and the Lorenz attractor, they demonstrate that their method reliably uncovers
the intuitive interfaces that simplify a system’s macroscopic description. See also [5, 6].
6
[8] complements this algorithmic advance with a rigorous, asymptotic guarantee for
the existence of blankets in high-dimensional stochastic systems. By defining a “blan-
ket index” to measure the strength of cross-couplings between internal and external
variables, the paper models these interactions as independent, bounded random vari-
ables and employs large-deviation techniques to show that, as the system’s dimension
grows without bound, almost all such couplings vanish. This result proves that “weak”
Markovblankets—whereconditionalindependenceholdsuptovanishinglysmallinterac-
tions—emergealmostsurelyintheinfinite-dimensionallimit,therebygroundingFriston’s
sparse-couplingconjectureinabroadclassofItôstochasticdifferentialequations. While
this theorem confirms that blankets are not an ad hoc or exceptional phenomenon but
a generic feature of complex systems, it remains silent on how to measure the varying
strengthsoftheseblanketsinfinite,real-worldsettingsorhowtheymightsteeranagent’s
behavior. On Bayesian mechanics, see also [26, 27].
The present paper is also related to [1]. Both works share the same foundational
insight: any system at a non-equilibrium steady state can be partitioned into internal,
sensory, active, and external components via a Markov blanket, and internal states ap-
peartoperformBayesianinferencebyminimizingvariationalfreeenergy. However,while
Friston treats this boundary as a sharply defined, discrete set of sensory and active vari-
ables that uniformly insulates internal states from external states—demonstrating how
this partition underlies phenomena from quantum dynamics through classical stochastic
processes to living systems—the present paper explicitly extends this approach by al-
lowing that “insulating” effect to vary continuously across space. In other words, where
Friston envisions a crisp frontier separating inside and outside, the present research pro-
poses a continuous scalar field that quantifies, at each location, how strongly internal
and external states are decoupled. This permits intermediate regions where external
influences partially penetrate, rather than assuming each point is either fully inside or
fully outside the Markov blanket.
However, the present research does not stop at proposing this shift in perspective;
it also provides a concrete algorithmic recipe—based on information-theoretic estima-
tors and nearest-neighbor sampling—to measure local blanket strength from observed
data. In contrast, Friston’s treatment, although highly ambitious and formally rich
across multiple scales, remains largely conceptual with regard to how one might detect
or manipulate the blanket in real systems. Specifically, Friston [1] illustrates his theory
with idealized “active soup” simulations and outlines the mathematical links between
free energy, steady-state densities, and inference, but he does not detail how to estimate
blanket strength in, for example, a spatially extended neural system or an agent navi-
gating a heterogeneous environment. By combining these two perspectives, the present
research neither contradicts nor undermines Friston’s core theorems regarding a discrete
Markov blanket. Rather, by embedding Friston’s boundary within a gradient of insulat-
ingstrength,itshowshowfree-energyminimizationcanbemodulatedbylocalvariations
incouplingbetweeninternalandexternalstates. Inthisview, agentsnaturallygravitate
towardregionswherecouplingisstrongest—wheretheblanketisweakest—becausethose
regions offer richer sensory information. However, this also means that the MB density
imposes limits on free energy minimization. In summary, the present paper takes Fris-
ton’shigh-level,multiscaleframeworkandgivesitconcretespatialtexture: showinghow
blanket strength can ebb and flow across space and, in turn, shape an agent’s inferential
and behavioral trajectories.
3.2 Some clarifications
We use here the term "coupling" to describe the degree of statistical and causal in-
terdependence between an agent and its environment. This is formalized in terms of
conditional mutual information, but also interpreted dynamically: strong coupling im-
plies that the agent’s sensory states carry information about external causes, and that
its actions can affect those causes. In our model, low MB density corresponds to higher
potential for coupling, which in turn enables more effective free energy minimization.
This is perfectly in line with [1].
However, we acknowledge that this use of "density" introduces a metaphorical shift:
7
we are interpreting space not as geometrically partitioned, but as structured by the sta-
tistical architecture of interaction. This raises ontological and epistemological questions.
Is MB density a real property of physical space, or is it a modeling construct used to
represent the agent’s epistemic relation to its surroundings? In this paper, we remain
agnostic: we treat MB density as a tool for expressing how the spatial environment
constrains inferential dynamics, rather than making strong claims about its physical in-
stantiation. We steer clear of the more strictly philosophical debates on the ontological
implications of the concept of Markov blankets [38]—at least, from my point of view,
Markov blankets are good modelling tools, but at the same time, they are only good
heuristics. This doesn’t mean avoiding philosophical debates about the relationship be-
tween the map (Markov blankets) and the territory (reality) — quite the opposite, in
fact. It means recognizing the importance of the problem (beyond an armchair philos-
ophy approach), and therefore focusing first on the robustness of the map and what we
candowithit—anecessaryconditionforunderstandingitsrelationshiptotheterritory,
especially since the map is itself part of the territory. And we are also convinced that
(unlike [40]) instrumentalism does not necessarily imply blind belief in the usefulness of
the model—quite the contrary, in fact.
Morover,MBdensityinitselfisnotaprobabilitydensity. MBdensityisaninformation-
theoretic measure (ranging from 0 to 1) of how effectively an agent’s boundary blocks
informationflowbetweenitsinternalandexternalstatesatapointx, estimatedviacon-
ditionalandunconditionalmutualinformations. Itisnotnormalizedoverthestatespace
and directly modulates the speed of gradient-descent on free energy (when MB density
= 1, updates freeze). By contrast, a probability density p(x) is a normalized function
(integrating to one) that assigns relative likelihoods to values of x, without any notion
of informational blocking or direct influence on free-energy descent.
TheMBdensityρ(x)isamodelingindexoflocalinformationalshielding,notaprob-
ability density; it modulates dynamics via x˙ =−(1−ρ(x))∇F(x). Because conditional
mutual information can exceed mutual information in the presence of synergy, the raw
ratio
I(I;E |B)
ρ (x)=1−
raw I(I;E)+ε
may fall outside [0,1]. Operationally, we adopt an ε > 0 for numerical stability and
clip estimates to ρ(x) ∈ [δ,1−δ] during inference (Algorithm 1, Sec. 4.5), ensuring a
well-defined mobility factor while preserving the statistical meaning of the ratio. An
extended exploratory variant lifts the clipping and allows ρ > 1, which flips the sign of
the prefactor and induces local ascent of F (see Sec. 10).
Another important caveat. The paper never claims that the agent “tends toward a
point where MB density = 0” (i.e., complete elimination of any boundary). When it
says that free-energy minimization follows trajectories into regions of low MB density,
it really means cases where the MB density is reduced but not zero: in those regions,
the blanket is “thin” enough to allow faster information exchange between internal and
external states, speeding up the descent of free energy. That does not imply that the
agent is drifting toward entropy and dissipation. The agent’s skill is precisely in staying
in areas where MB density is high enough to maintain its internal structure, yet not
so high as to block necessary coupling. Consequently, there is no contradiction in the
paper’sthesis. LowMBdensitymeanstheblanketisjustenoughtoseparateinternaland
external states, but weak enough to permit rapid information flow such that free-energy
descent is effective; by contrast, MB density = 0 is a theoretical limit where the blanket
no longer exists, and at that point the model no longer describes adaptive behavior but
instead total informational extinction (i.e., the agent dissolves). Moving into regions of
low MB density does not automatically cause an overall increase in entropy. In the free-
energy minimization framework, “low MB density” simply means that the information
boundary between agent and environment is more “porous,” allowing the internal state
toupdatemorequicklybasedonsensorydata. Thatdoesnotequatetoalossofinternal
order or a slide into chaos. Again, this is perfectly in line with [1]. In fact, "nearly every
system encountered in the real world is self-organizing to a greater or lesser degree" [1].
8
4 Thesis
We aim to demonstrate the following claim:
Free energy minimization tends to follow trajectories leading toward regions of lower
MBdensity. Theseregionscorrespondtostrongeragent-environmentcouplingandgreater
synchronization potential.
4.1 Definitions and Assumptions
Let Ω⊂Rn denote a spatial domain.
For each point x ∈ Ω, assume the presence of a local Markov blanket B(x) that
mediates interactions between internal states I, external states E, and blanket states B.
Define the Markov blanket strength at point x as:
I(I;E |B)
S(x):=1− (1)
I(I;E)
where I(I;E | B) is the conditional mutual information between internal and external
states given the blanket.
This yields:
• S(x)=1: perfect conditional independence (strong MB).
• S(x)=0: no conditional independence (no effective MB).
Informational separation is at its highest degree when
I(I;E |B)=0,
thatis, when, onceB isknown, knowingfurtherdetailsaboutE doesnothelptoinform
I.
Define the Markov blanket (MB) density ρ(x) as the field of MB strengths over
Ω:
ρ(x):=S(x), ρ(x)∈[0,1] (2)
Thisfieldquantifieshowinsulatedeachpointinspaceiswithrespecttointernal-external
separation.
4.2 Clarification on Continuous MB Density vs. Discrete Condi-
tional Independence
A potential concern is that our MB density ρ(x), defined as a continuous scalar field,
might be conflated with the classical, discrete property of conditional independence.
We resolve this by distinguishing carefully between the structural and the quantitative
aspects, and by invoking precise measure-theoretic language and manifold geometry.
4.2.1 Measurability and Measure-Theoretic Foundations
Let (Ω,F,P) be a probability space and let I,E,B be random variables with joint
distribution absolutely continuous w.r.t. Lebesgue measure on a parameter manifold X.
By the Radon–Nikodym theorem there exists a density
dP
p(i,e,b|x) = (i,e,b|x) for almost every x∈X.
dλ
Define the total and residual mutual informations at x by
(cid:90)(cid:90) p(i,e|x)
I (x)= p(i,e|x) ln dide,
tot p(i|x)p(e|x)
(cid:90)(cid:90)(cid:90) p(i,e|b,x)
I (x)= p(i,e,b|x) ln didedb.
res p(i|b,x)p(e|b,x)
9
Since each is an integral functional of measurable densities, both I (x) and I (x) are
tot res
themselvesmeasurablefunctionsofx,definedforalmosteveryx∈X. Wethenintroduce
I (x)
ρ(x) = 1 − res , 0 ≤ ρ(x) ≤ 1, (1)
I (x)
tot
with all equalities understood to hold almost everywhere (i.e., off a P–null set where
I (x)=0).
tot
Importantly, the discrete Markov blanket remains the primary structural object
enforcing
I ⊥E |B ⇐⇒ p(I,E |B) = p(I |B)p(E |B).
Only after specifying that blanket and computing the integrals above do we obtain the
scalar summary ρ(x). In the limiting cases,
I (x)=0 =⇒ ρ(x)=1, I (x)=I (x) =⇒ ρ(x)=0,
res res tot
recovering perfect separation or total coupling, respectively, but never conflating struc-
ture with measure.
4.2.2 Geometry of the Underlying State Space
We regard X as the statistical manifold of our generative model. Explicitly,
x∈X ⇐⇒ x parametrizes p(i,e,b|x).
EquippedwithaFisher–Raometric(oranysmoothatlascompatiblewiththedensities),
ρ(x) becomes a smooth scalar field on X.
In applications where x denotes a point in physical space, one first constructs a local
kernel approximation
1 (cid:90)
p(i,e,b|x) = p(i,e,b|y)dy,
|B(x,r)|
B(x,r)
and then applies the same definitions. Thus ρ(x) may equally be viewed as a field on a
physical manifold, provided the generative kernel is smooth.
4.2.3 Key Takeaways
• ρ(x) is a measurable function on X (a.e.), derived from well-defined integrals
(Radon–Nikodym, Lebesgue differentiation).
• ItquantifiesthedegreeofconditionalindependenceenforcedbyaclassicalMarkov
blanket; it does not redefine or replace that structural concept.
• Its domain is a statistical manifold of model parameters (or, via kernel lifts, a
physical manifold).
• In the discrete limit ρ∈{0,1}, one recovers the ordinary, binary Markov blanket.
4.3 Local vs Regional ρ(x) Interpretations
Twoclarificationsareinorderregardingtheapparenttensionbetweenthelocaldefinition
of blanket density and its regional or global usage in the simulations.
First,thediscrepancyisonlyconceptualandresultsfromthedualperspectiveadopted
in this work. On the one hand, the agent’s behavior is analyzed from the point of view
of the environment: here, the space (and the field ρ(x)) is taken as given — which
is standard in most dynamical models, where the landscape precedes the agent. On
the other hand, the model also addresses the subjective perspective of the agent, for
whom the structure of the space is initially unknown and must be inferred through
interaction. Thus, while ρ(x) appears as a pre-defined field in the simulations, it should
beunderstoodasarepresentationthattheagentprogressivelyconstructsthroughactive
10
MB 5
MB 1
P(x)
P(x)
P(x)
P(x)
P(x)
MB 6
MB 3
MB 2
MB 4
Figure 1: Walking through Markov blankets. A schematic and intuitive representation
of the path of an active inference agent (orange line) in a space “filled” with Markov blankets
(MB) and touching points with different densities or porosities. Obviously, the agent also has
its own Markov blanket, and therefore its movement is conditioned by the coupling with the
other blankets and thus by the MB density.
inference. These two views are not contradictory, but complementary: one external, the
other internal. This point is explicitly addressed in Section 12.
Second, the supposed tension between local and regional definitions of ρ(x) dissolves
whenweassumethediscretenatureoftheagent’smovement. Ateachtimestep,theagent
is located at a single point in space and therefore interacts only with the local blanket
densityρ(x)atthatpoint. ThisispreciselywhatthealgorithmintroducedinSection4.4
formalizes: alocal,data-drivenestimationofρ(x)basedonafiniteneighborhood. Hence,
although ρ(x) is conceptually related to spatial regions (via the mutual information over
internal, blanket, and external zones), its operative meaning remains pointwise. The
algorithm, by grounding this estimation in local data, resolves the apparent conflict
between locality and regionality.
Afinalclarificationconcernstheinterpretationoftheblanketdensityfieldρ(x). While
itassignsavaluetoeverypointinspace,thisdoesnotimplythattheunderlyingMarkov
blanket is associated with the same system or agent across the entire domain. In fact,
a given point may be part of different Markov blankets under different conditions —
dependingonthescale,temporalresolution,orinferentialcontextadopted. Forinstance,
alocationmightfallwithintheblanketofoneagentduringagiveninteraction,andlater
within that of a different system, or none at all. What ρ(x) captures, then, is not a
fixed assignment of spatial regions to specific agents, but the local degree of conditional
coupling between internal and external states — regardless of which system is involved.
This perspective reinforces the interpretation of ρ(x) as a context-sensitive measure of
informational structure, rather than a mapping of fixed systemic boundaries.
4.4 Technical Assumptions on Space and Measures
Let X ⊆Rd be a nonempty, bounded, open set with Lipschitz boundary, equipped with
the Lebesgue measure dx. We assume the joint distribution of internal, external, and
blanket variables admits a strictly positive, C2 density
p(i,e,b|x) w.r.t. didedbdx,
where i ∈ I, e ∈ E, and b ∈ B range over compact subsets of Euclidean spaces. Con-
cretely, we require:
(i) Uniform bounds: there exist constants 0<m≤M <∞ such that
m ≤ p(i,e,b|x) ≤ M, ∀(i,e,b,x).
11
(ii) Smooth marginals and conditionals: all marginal and conditional densities p(i,e),
p(i,e|b), etc., depend in a C2-fashion on x.
(iii) Regularity of information maps: the mappings
x(cid:55)→I(I;E) and x(cid:55)→I(I;E |B(x))
I(I;E |B)
are C1 on X, and hence ρ(x)=1− is C1 as well.
I(I;E)
Underthesehypotheses, allintegrals, gradients, andcontinuityargumentsinsubsequent
sections are well-defined and satisfy the smoothness conditions required by the main
theorems.
4.5 Operational Definition of MB Density
To render the blanket density field ρ(x) operational in continuous systems, we parti-
tion the state–space around each point x using two radii, r < r . Variables within
1 2
distance r of x form the internal set I(x); those at distances in [r ,r ) form the blan-
1 1 2
ket B(x); and the remainder form the external set E(x). We estimate the conditional
mutual information I(I;E | B) and the marginal mutual information I(I;E) using the
Kraskov–Stögbauer–Grassberger (KSG) k-nearest–neighbors estimator. To avoid divi-
sion by zero, we introduce a small regularizer ε and constrain ρ(x) ∈ [δ, 1−δ]. See
[21]. The computational cost scales as O(NlogN) using KD-trees or similar structures;
Algorithm 1 Estimation of Blanket Density ρ(x)
Require: Dataset D ={(y ,s )}N , radii r ,r , neighbor count k, regularizer ε, bound
i i i=1 1 2
δ.
Ensure: Blanket density ρ(x) for each sample x∈{y }.
i
1: for each sample x∈{y i } do
2: I ←{s i |∥y i −x∥<r 1 }
3: B ←{s i |r 1 ≤∥y i −x∥<r 2 }
4: E ←{s i |∥y i −x∥≥r 2 }
5: Estimate I(I;E |B) via KSG_conditional(k,I,E,B)
6: Estimate I(I;E) via KSG_mutual(k,I,E)
I(I;E |B)
7: S(x)←1−
I(I;E)+ε
8: ρ(x)←min{max{S(x),δ}, 1−δ}
9: end for
10: return {ρ(x)} x∈{yi}
further speedups are possible via grid-based subsampling. In our simulations we used
r =0.1, r =0.2, k =5, ε=10−6, δ =10−3, N =104.
1 2
A Python implementation of the KSG estimator to compute MB density from sample
datacanbefoundhere: https://github.com/DesignAInf/MB-density. SeealsoAppendix
B. On the project’s GitHub page, there are two separate repositories serving different
purposes. The first one (mbdensity-sim) is a demonstrative sandbox — a minimal,
self-contained environment designed to showcase the core idea of modeling a spatially
varying Markov blanket density ρ(x). It uses a single synthetic field, a simplified mu-
tual information estimator, and a basic 2D gradient-descent dynamic, making it easy
to test and understand the concept without extra complexity. The second repository
(mbdensity-paper-sims)isafullreproductionofallsimulationsandvisualizationsfrom
the manuscript (Figures 2–8). It implements the exact scenarios described in the main
text and Appendix A. In short: the first repo is a stripped-down prototype for explo-
ration, while the second is a faithful, figure-by-figure replication of the paper’s experi-
ments.
12
5 Free Energy and Modulated Gradient Descent
Let the variational free energy field F(x) be defined over space Ω:
F(x):=E [logq (s(x))−logp(s(x),η(x))] (3)
qµ(s(x)) µ
where q is the internal (variational) distribution of the agent, s(x) are sensory states at
µ
location x, and η(x) are environmental (hidden) states at x.
The agent minimizes F(x) via gradient descent, modulated by MB density:
x˙ =−M(x)∇F(x) (4)
whereM(x):=(1−ρ(x))I,andI istheidentitymatrix. Ifρ(x)=1,inferenceisblocked
(no coupling), so x˙ = 0; if ρ(x) = 0, there is full coupling and maximal inference is
possible [10, 11, 14, 15, 16].
6 FEP and MB Density
Theorem 1 (Simultaneous Descent of Free Energy and Blanket Density under the Gra-
dient-Ratio Condition). Let Ω ⊂ Rn be a compact domain with C2 boundary, and let
F ∈ C2(Ω) be the free-energy function. An agent’s trajectory x(t) evolves according to
the modulated gradient descent
x˙(t) = −[1−ρ (x(t))]∇F(x(t)),
N
where ρ (x) is the empirical blanket-density estimator derived from a dataset of N sam-
N
ples.
Assume the following hold on an open set D ⊂Ω where ∇F(x)̸=0:
(a) Smoothness of information functions.
The true unconditional mutual information
v(x) = I (I(x);E(x)), u(x) = I (I(x);E(x)|B(x))
true true
are C1 functions of x.
(b) Gradient alignment.
Define
A(x) = ∇u(x)·∇F(x), B(x) = ∇v(x)·∇F(x),
and assume A(x) > 0, B(x) > 0, i.e. moving down ∇F decreases both u(x) and
v(x).
(c) Gradient-Ratio Condition.
The proportional rate of decrease of v(x) along ∇F strictly exceeds that of u(x):
B(x) A(x)
> .
v(x) u(x)
Conclusion. Undertheseassumptions, withprobabilitytendingto1asN →∞(sothat
∇ρ →∇ρuniformly),theagent’strajectorybothdescendsthefree-energylandscapeand
N
movestowardregionsofstrictlydecreasingblanketdensity. Inparticular, foreverytwith
x(t)∈D,
d
ρ (x(t))=∇ρ (x(t))·∇F(x(t)) > 0.
dt N N
Note. We choose the sign so that a positive directional derivative ∇ρ·∇F >0 implies ρ
decreases as F decreases.
13
From this point of view, free-energy minimization is fundamentally and universally
aligned with seeking regions of stronger informational coupling (lower MB density). The
formerprocessnaturallygivesrisetothelatter. Thealignmentbetweenfree-energymin-
imization and stronger informational coupling is contingent on the local informational
geometry. It holds only when the Gradient-Ratio Condition is met—i.e. when the infor-
mational benefit of reducing total uncertainty outweighs the cost of residual uncertainty
along the current path. Thus, the agent’s dynamics emerge from a direct competition
between these two informational gradients.
This revised framework opens up a richer, more nuanced model of active inference.
Activeinferenceisnotasimple,guaranteeddescentdownasmoothhillbutthenavigation
ofacomplex“epistemiclandscape,” determinedbytheinterplaybetweenthefree-energy
gradient ∇F and the informational field ρ(x). The directional derivative of the true
blanket density
u(x)
ρ (x)=1 −
true v(x)
along the agent’s path has sign proportional to
[v(x)]2u(x)B(x) − v(x)A(x)u(x),
whichispositivepreciselywhentheGradient-RatioConditionholds, ensuringalignment
offree-energydescentwithadecreaseinρ(x). Thisimmediatelyraisesacriticalquestion:
What happens when the condition is violated? This leads to the concept of an epistemic
trap: aregioninthestatespacewheretheinformationalgeometryis"perverse,"causing
the Gradient-Ratio Condition to fail. In such a region:
If
B(x) A(x)
≤ ,
v(x) u(x)
anepistemic trap arises: despitefollowingfree-energyminimization,theagentmaymove
toward higher ρ(x), becoming more informationally isolated. Such traps offer a first-
principles model of maladaptive behaviors (e.g. social anxiety) and underscore the ne-
cessity of stochasticity or curiosity-driven exploration to escape basins that violate the
Gradient-Ratio Condition. This discussion of epistemic traps serves as a qualitative
bridgetoTheorem2, wherethesetrapsarerigorouslydefinedandtheirdynamicalprop-
erties analyzed.
There are at least two implications of this point:
• Modeling Maladaptive States. Epistemictrapsformalizehowanagentcanget
stuck in high-ρ regions despite lower free-energy alternatives elsewhere.
• Justification for Exploration. Deterministic descent may fail; random pertur-
bationsorexplicitexploratorydrivesarenormativelyjustifiedtoovercomeperverse
informational geometries.
6.1 Proof of the Gradient-Alignment Condition
Here we provide the complete technical details required to justify the claim
∇ρ (x) · ∇F(x) > 0 on D,
N
with high probability as N →∞. Recall that
I(cid:98)(I(x);E(x)|B(x)) + ε(N)
ρ (x) = 1 − , ε(N)=C N−α.
N 0
I(cid:98)(I(x);E(x)) + ε(N)
The proof proceeds in several steps:
14
Step 1: Consistency and C1 Convergence of the MI Estimators
Under the choice of radii r (N), r (N) satisfying
1 2
r (N) → 0, r (N)=cr (N), NVol(Ball(x; r (N))) → +∞,
2 1 2 2
the KSG–kNN estimators
I(cid:98)(I(x);E(x)), I(cid:98)(I(x);E(x)|B(x))
converge in probability to their true values
I (I(x);E(x)), I (I(x);E(x)|B(x)),
true true
uniformly on every compact K ⊂ D. Moreover, if the true mutual informations are C1
and the underlying noise is sub-Gaussian (or sub-Exponential), then I(cid:98) converges to I
true
in C1-norm on compacts:
sup|I(cid:98)(I(x);E(x))−I
true
(I(x);E(x))|=O
p
(N−α),
x∈K
sup|∇
x
I(cid:98)(I(x);E(x))−∇
x
I
true
(I(x);E(x))|=O
p
(N−α).
x∈K
and similarly for I(cid:98)(I(x);E(x)|B(x)). The exponent α>0 depends on the data dimen-
sion d and the chosen k. In particular, for sufficiently large N, with probability at least
1−δ, one has
∥I(cid:98)(I(·);E(·))−I
true
(I(·);E(·))∥
C1(K)
< η(N),
∥I(cid:98)(I(·);E(·)|B(·))−I
true
(I(·);E(·)|B(·))∥
C1(K)
< η(N),
where η(N)→0 as N →∞.
Step 2: Definition of the “True” Blanket Density ρ (x)
true
Define
I (I(x);E(x)|B(x))
ρ (x) = 1 − true .
true I (I(x);E(x))
true
SinceI (I(x);E(x))>0forallx∈D,ρ (x)iswell-definedandliesstrictlyin(0,1).
true true
By hypothesis, I (·;·) and I (·;·|·) are C1, so ρ (x)∈C1(Ω). A straightforward
true true true
differentiation yields
1 I (I(x);E(x)|B(x))
∇ρ (x) = − ∇I (I(x);E(x)|B(x))+ true ∇I (I(x);E(x)).
true I true (I(x);E(x)) true [I true (I(x);E(x))]2 true
Since, by the above assumptions, both
∇I (I(x);E(x)|B(x)) · ∇F(x) > 0, ∇I (I(x);E(x)) · ∇F(x) > 0 ∀x∈D,
true true
and because I (I(x);E(x)|B(x))<I (I(x);E(x)), it follows that
true true
∇I (I(x);E(x)|B(x)) · ∇F(x)
∇ρ (x) · ∇F(x)=− true
true I (I(x);E(x))
true
I (I(x);E(x)|B(x))[∇I (I(x);E(x)) · ∇F(x)]
+ true true < 0.
[I (I(x);E(x))]2
true
Hence
∇ρ (x) · ∇F(x) < 0 =⇒ ∇ρ (x) · ∇[−F(x)] > 0.
true true
Equivalently,
∇ρ (x) · ∇F(x) > 0 ∀x∈D.
true
15
Step 3: Uniform C1 Convergence Implies Gradient Alignment for
ρ
N
Since
∥I(cid:98)(I(·);E(·))−I
true
(I(·);E(·))∥
C1(K)
=O
p
(N−α),
∥I(cid:98)(I(·);E(·)|B(·))−I
true
(I(·);E(·)|B(·))∥
C1(K)
=O
p
(N−α).
and ε(N) = C N−α, one deduces that ρ (x) → ρ (x) uniformly in C1(K) over any
0 N true
compact K ⊂D. In particular, for sufficiently large N, with probability at least 1−δ,
sup∥∇ρ (x)−∇ρ (x)∥< η(N), where η(N)→0 as N →∞.
N true
x∈K
Since ∇ρ (x)·∇F(x) is strictly positive and bounded away from zero on K, there
true
exists N such that for all N ≥N ,
0 0
∇ρ (x) · ∇F(x) = ∇ρ (x) · ∇F(x) + [∇ρ (x)−∇ρ (x)] · ∇F(x) > 0,
N true N true
∀x∈K,
with probability at least 1−δ. Covering D by a finite collection of such compact sets
yields the uniform positivity of ∇ρ (x)·∇F(x) on all of D, with probability →1.
N
Step 4: Conclusion—Monotonic Decrease of ρ along the Trajec-
N
tory
Let x(t) solve
x˙(t) = −[1−ρ (x(t))]∇F(x(t)), x(0)=x ∈D.
N 0
Then, wherever x(t)∈D,
d
ρ (x(t))=∇ρ (x(t)) · x˙(t)=−[1−ρ (x(t))][∇ρ (x(t)) · ∇F(x(t))].
dt N N N N
Since0<ρ (x)<1implies1−ρ (x)>0,andfromStep3wehave∇ρ (x)·∇F(x)>0
N N N
for all x∈D with high probability, it follows that
d
ρ (x(t)) < 0, whenever x(t)∈D.
dt N
Hence ρ (x(t)) is strictly decreasing along the agent’s path so long as x(t) remains in
N
D. This completes the proof.
□
The agent is driven by free energy minimization to move toward regions of lower
Markov blanket density—i.e., where boundaries are weak, coupling is strong, and inter-
action with the environment is richer. This provides a formal justification for the thesis:
freeenergyminimizationinspacetendstodeformtowardtopologiesoflowMarkovblanket
density.
For more details, see Figure 2-8 and Appendix A. As said, you can find the full code
of the simulations, detailed parameter settings, and usage instructions in the GitHub
repository: https://github.com/DesignAInf/MB-density.
6.2 An Empirical Diagnostic for Gradient Alignment
We provide here an operational, local diagnostic to test whether the alignment assump-
tion
⟨∇ρ(x),∇F(x)⟩>0
holdsindata. Thediagnosticismodularandcomesinthreecomplementaryvariants: (A)
directional differences, (B) local gradient estimation with bootstrap, and (C) a dynamic
test when trajectories are observed. For a reference Python implementation of the em-
pirical alignment diagnostic, including local gradient estimation, directional derivatives,
and bootstrap testing, see https://github.com/DesignAInf/MB-density.git.
16
6.2.1 Variant A: Directional Differences (fast and robust)
Let ρ
(cid:98)
be an estimator of ρ and ∇(cid:100)F(x) an estimate of the free-energy gradient (e.g., via
autodiff on the model or via SPSA/finite differences). Define the unit direction
∇(cid:100)F(x)
u (x):= ,
F
∥∇(cid:100)F(x)∥
provided∥∇(cid:100)F(x)∥>0. ForasetofstepsizesE ={ε
1
,ε
2
,ε
3
},wecomputethesymmetric
directional derivative of ρ along u :
(cid:98) F
ρ(x+εu )−ρ(x−εu )
D (x;ε):= (cid:98) F (cid:98) F .
ρ|F 2ε
Aggregate across scales via the median:
D (x):=median D (x;ε).
ρ|F ε∈E ρ|F
Decision rule. For a tolerance τ > 0 (set from noise, see Sec. 6.2.4): (i) if D (x) >
ρ|F
τ, declare positive alignment (⟨∇ρ,∇F⟩ > 0); (ii) if D (x) < −τ, declare negative
ρ|F
alignment; (iii) otherwise, inconclusive.
Symmetriccross-check. Ifanestimate∇(cid:99)ρ(x)isavailable,defineu
ρ
(x):=∇(cid:99)ρ(x)/∥∇(cid:99)ρ(x)∥
and compute
D (x;ε):=
F(cid:98)(x+εu
ρ
)−F(cid:98)(x−εu
ρ
)
.
F|ρ 2ε
ConsistencybetweenthesignsofD andthemedianofD strengthenstheevidence.
ρ|F F|ρ
6.2.2 Variant B: Local Gradient Estimation with Bootstrap (CIs and p-
values)
We estimate both gradients locally and assess uncertainty by resampling.
Local gradients. Within a ball B (x) of radius r centered at x, fit a first-order local
r
polynomial (local linear regression) for ρ
(cid:98)
and F(cid:98) using a kernel weight (e.g., tricube or
Gaussian). This yields
ρ
(cid:98)
(z)≈a
ρ
+b⊤
ρ
(z−x), F(cid:98)(z)≈a
F
+b⊤
F
(z−x),
with ∇(cid:99)ρ(x) := b
ρ
and ∇(cid:100)F(x) := b
F
. When model derivatives are inaccessible, SPSA
with two-point perturbations can be used to obtain ∇(cid:100)F(x) (and analogously ∇(cid:99)ρ(x)).
Test statistic.
T(x):=∇(cid:99)ρ(x)⊤∇(cid:100)F(x).
Optionally, consider the cosine similarity c(x) := T(x)/(∥∇(cid:99)ρ(x)∥·∥∇(cid:100)F(x)∥) to reduce
scale effects.
Bootstrapuncertainty. ResamplewithreplacementthepointsinB (x)(block-bootstrap
r
if temporal dependence is present), re-estimate gradients, and recompute T∗(b)(x) for
b=1,...,B. Let
CI (x)=[q , q ]
1−α α/2 1−α/2
betheempirical(1−α)confidenceintervalfrombootstrapquantiles. Aone-sidedp-value
for H :⟨∇ρ,∇F⟩≤0 vs. H :>0 is
0 1
1+#{b:T∗(b)(x)≤0}
p(x)= .
B+1
Decision rule. (i) If CI (x) lies entirely above 0 (and p(x)<α), declare positive
1−α
alignment. (ii) If it lies entirely below 0, declare negative alignment. (iii) Otherwise,
inconclusive.
17
6.2.3 Variant C: Dynamic Test Along Trajectories
Suppose we observe a trajectory x evolving under x˙ = −M(x )∇F(x ) with M(x)
t t t t
positive definite (e.g., M ≈I). Then
d
ρ(x )=∇ρ(x )⊤x˙ =−∇ρ(x )⊤M(x )∇F(x ).
dt t t t t t t
IfM ≈I, thesignof⟨∇ρ,∇F⟩isthenegativeofthesignofρ˙. Inpractice, filterx (e.g.,
t
Savitzky–Golay), compute ρ
(cid:98)
(x
t
) and its central finite-difference derivative (cid:98)ρ˙
t
. Estimate
the fraction
T
1 (cid:88)
π := ⊮{(cid:98)ρ˙ <0},
T t
t=1
and form block-bootstrap confidence intervals for π. If the CI is strictly above 0.5, we
declarepositivealignmentonaverage alongtheobservedpath(adjustingforM ifknown).
6.2.4 Hyperparameters, Quality Checks, and Practical Defaults
Neighborhood size. Choose r so that the local sample size N ∈ [50,200]; report
r
stability across two or three values of r. Directional step sizes. Use E = {0.5,1,2}×
the median nearest-neighbor distance. Tolerance. Set τ to one or two MADs of the
bootstrapped D
ρ|F
(x;ε). Flat regions. If ∥∇(cid:100)F(x)∥ or ∥∇(cid:99)ρ(x)∥ fall below a threshold,
label the test uninformative. Cosine similarity. Report bootstrap CIs for c(x) as a
scale-invariant summary.
6.2.5 Limitations
The diagnostic is local and sensitive to neighborhood size r and step sizes E. In high
dimension, gradient estimates and MI-based ρ may be noisy; we therefore recommend
(cid:98)
reporting sensitivity analyses and bootstrap CIs. In flat regions where ∥∇F∥ or ∥∇ρ∥ is
small, the test is uninformative by design.
7 Implications
This result calls fora redefinition of active inference concepts in terms ofspatially struc-
tured MB density. Markov blankets are no longer discrete boundaries, but a graded
field ρ(x) across space. Free energy becomes a spatial field F(x), whose minimization is
modulatedbythisfield. Perceptionandactionemergeasspatiallyconstrainedprocesses,
more effective in low-MB-density regions. Expected free energy can be redefined as a
trajectory-dependent integral:
(cid:90)
G(π)= (1−ρ(x (t)))F(x (t))dt (5)
π π
τ
This framework generalizes active inference beyond fixed, agent-centered models. It
accommodates proto-agents, emergent structures, and distributed cognition. It also
grounds the role of movement, curiosity, and exploration in the topology of inference:
agents seek regions where inference is possible and fruitful. It aligns naturally with eco-
logical and enactive theories of cognition, and opens the door to applications in swarm
robotics, architecture, and cognitive development.
8 MB Density and the Limits on the Free Energy Min-
imization
ThenexttwotheoremselaborateontherelationshipbetweenMBdensityandfreeenergy
minimization. Theorem 2 formalizes that as MB density rises toward 1, agent’s mecha-
nisms by which it reduces free energy—namely, its movements and belief updates—slow
downwithoutboundand,atfulldensity,stopaltogether,sothatregionsofhighblanket-
density effectively lock the agent in place and prevent any further action or inference
[12, 13, 17].
18
Theorem 2. Let
1. F :Ω→R be a continuously differentiable (C1) function on an open set Ω⊆Rn.
2. ρ:Ω→[0,1]beacontinuousblanket-densityfield. Ateachpointx∈Ω,assumethe
agent’s spatial (or parametric) coordinates evolve according to the continuous-time
dynamics
x˙ = −(1−ρ(x))∇F(x).
3. There exist two positive constants:
• G such that ∥∇F(x)∥ ≤ G for all x ∈ Ω. In other words, F has a globally
bounded gradient on Ω.
• m such that
m = inf ∥∇F(x)∥2 > 0,
x∈Ω
Ftarget≤F(x)≤F(x0)
where x is the initial point (with F(x )=F ) and F <F is the desired
0 0 0 target 0
(strictly lower) “target” value of free energy.
Under these assumptions, the following statements hold:
1. Exact Blocking at ρ=1.
If, for some open neighborhood U ⊆ Ω, ρ(x) = 1 for every x ∈ U, then for all
x∈U:
x˙ =−(1−ρ(x))∇F(x)=−(1−1)∇F(x)=0,
and therefore
d
F(x(t)) = ∇F(x)·x˙ = 0.
dt
In other words, whenever ρ(x)≡1 on some region, the agent is completely immo-
d
bilized there: it cannot move (x˙ =0) and cannot reduce free energy ( F =0).
dt
2. Quantitative Slowing When ρ Is Close to 1.
Fix an arbitrary point x∈Ω. Because
d
F(x(t)) = ∇F(x)·x˙ = −(1−ρ(x))∥∇F(x)∥2,
dt
one sees immediately that if ρ(x)≥1−δ for some 0<δ ≪1, then
0 ≤ 1−ρ(x) ≤ δ,
and hence
d
− F(x)=(1−ρ(x))∥∇F(x)∥2 ≤ δ∥∇F(x)∥2 ≤ δG2.
dt
Equivalently,
d
F(x) ≥ −δG2.
dt
Thus, at any point where ρ(x)≥1−δ, the instantaneous decrease of F is at most
δG2. In particular:
• Ifonedemandsthattherateofdecreaseoffreeenergybeatleastsomepositive
threshold α>0, i.e.
d
− F(x) ≥ α,
dt
then it is necessary that
α α
(1−ρ(x))∥∇F(x)∥2 ≥ α ⇐⇒ 1−ρ(x) ≥ ≤ .
∥∇F(x)∥2 G2
Hence
α
ρ(x) ≤ 1− .
G2
Inshort, anypointxatwhichρ(x)exceeds1− α cannotdecreasefreeenergy
G2
faster than α.
19
α
• Conversely, if ρ(x)≤1− , then
G2
d α
− F(x)=(1−ρ(x))∥∇F(x)∥2 ≥ ∥∇F(x)∥2 ≥ 0.
dt G2
d
Buttoensure F(x)≤−α,onemustalsorequire∥∇F(x)∥2 notbetoosmall.
dt
d
The precise condition for F(x)≤−α is
dt
α
(1−ρ(x))∥∇F(x)∥2 ≥ α ⇐⇒ 1−ρ(x) ≥ .
∥∇F(x)∥2
α
Since ∥∇F(x)∥2 ≤G2, a sufficient condition is 1−ρ(x)≥ .
G2
In summary, whenever ρ(x) lies in the interval
α
1− < ρ(x) ≤ 1,
G2
the descent of free energy is either very slow (bounded by δG2 with δ = 1−ρ) or
completely blocked (if ρ = 1). As ρ(x) → 1, the instantaneous free-energy-descent
d
rate | F(x)|→0.
dt
3. Lower Bound on the Time to Decrease F by ∆.
Suppose we start at x(0) = x , with F(x ) = F , and we want to reach any point
0 0 0
x(t) such that F(x(t)) ≤ F = F −∆ for some fixed ∆ > 0. Assume that,
target 0
along the entire trajectory x(t) from t = 0 until the first hitting time T of {x :
F(x)≤F −∆}, it holds that
0
1−ρ(x(t)) ≥ δ for all t∈[0,T],
for some δ >0. Then
d
F(x(t))=−(1−ρ(x(t)))∥∇F(x(t))∥2 ≤ −δ∥∇F(x(t))∥2.
dt
By hypothesis, on the level set {x:F ≤F(x)≤F }, we have ∥∇F(x)∥2 ≥m.
target 0
Hence
d
F(x(t)) ≤ −δm,
dt
and integrating from 0 to T gives
(cid:90) T
F(x(T)) − F(x ) ≤ [−δm]dt = −δmT.
0
0
Since F(x(T))=F −∆, we conclude
0
∆
−∆ ≤ −δmT =⇒ T ≥ .
δm
Thus, if the agent is “stuck” in regions where 1−ρ(x)≥δ (i.e. ρ(x)≤1−δ), then
it will take at least T = ∆/(δm) units of time to reduce F by ∆. As δ → 0, this
lower bound T →+∞.
4. Implication for Learning Rates of Internal Parameters θ.
Supposetheagentalsohasinternalparameters(beliefs)θ ∈Rp thatevolveaccording
to
∂F(x,θ)
θ˙ = −(1−ρ(x)) .
∂θ
At any x such that ρ(x) ≥ 1−δ, the magnitude of the instantaneous update of θ
is bounded by
(cid:13)∂F(cid:13) (cid:13)∂F(cid:13)
∥θ˙∥=(1−ρ(x))(cid:13) (cid:13) ≤ δ(cid:13) (cid:13).
(cid:13)∂θ(cid:13) (cid:13)∂θ(cid:13)
20
Therefore, if one demands a minimum learning rate ∥θ˙∥ ≥ α > 0, then it is
θ
necessary that
α α
1−ρ(x) ≥ θ ⇐⇒ ρ(x) ≤ 1 − θ .
∥∂F/∂θ∥ ∥∂F/∂θ∥
α
Hence any location x satisfying ρ(x)>1− θ will force ∥θ˙∥<α , meaning
∥∂F/∂θ∥ θ
that the agent’s ability to update its beliefs is dramatically reduced when ρ is close
to 1.
Proof Sketch. 1. Since F ∈ C1(Ω) and x(t) evolves via x˙ = −(1−ρ(x))∇F(x), one
computes
d (cid:104) (cid:105)
F(x(t))=∇F(x(t))·x˙(t)=∇F(x)· −(1−ρ(x))∇F(x) =−(1−ρ(x))∥∇F(x)∥2,
dt
establishing the exact expression for the instantaneous change of F.
2. If ρ(x) = 1, then x˙ = 0 and hence dF/dt = 0. This immediate calculation shows
that any region where ρ≡1 blocks both motion and free-energy reduction.
3. If ρ(x)≥1−δ, then 1−ρ(x)≤δ. Therefore
d
− F(x)=(1−ρ(x))∥∇F(x)∥2 ≤δ∥∇F(x)∥2 ≤δG2,
dt
d
whichimplies F(x)≥−δG2. Requiring−dF/dt≥αforces1−ρ(x)≥α/∥∇F(x)∥2,
dt
and since ∥∇F(x)∥2 ≤ G2, a sufficient condition is 1−ρ(x) ≥ α/G2, so ρ(x) ≤
1−α/G2.
d
4. Supposealongthetrajectory1−ρ(x(t))≥δ. Then F(x(t))≤−δ∥∇F(x(t))∥2 ≤
dt
−δm. Integrating from t=0 to t=T and using F(x(T))=F −∆ yields
0
∆
F(x(T))−F ≤ −δmT =⇒ T ≥ .
0 δm
Hence, to reduce by ∆, at least T =∆/(δm) time is needed.
5. Becauseθ˙ =−(1−ρ(x))∂F/∂θ,ifρ(x)≥1−δthen∥θ˙∥≤δ∥∂F/∂θ∥. Toguarantee
∥θ˙∥≥α , one needs 1−ρ(x)≥α /∥∂F/∂θ∥, i.e. ρ(x)≤1−α /∥∂F/∂θ∥.
θ θ θ
8.1 Numerical Example (One-Dimensional Case)
Consider:
F(x)=x2, x∈R.
Then ∇F(x)=2x, so ∥∇F(x)∥2 =4x2.
1. Let x =1, so F =1. Choose F =0.04. Then ∆=F −F =0.96.
0 0 target 0 target
2. On the level set {x:0.04≤x2 ≤1}, one has |x|≥0.2. Thus
∥∇F(x)∥2 =4x2 ≥ 4(0.2)2 =0.16,
so we can take m=0.16. On |x|≤1, ∥∇F(x)∥2 ≤4, hence G=2.
3. If everywhere along the continuous trajectory we have ρ(x) ≤ 0.9 (so δ = 0.1),
Theorem 2 says
∆ 0.96
T ≥ = =60.
δm 0.1×0.16
If instead ρ(x)≤0.99 (δ =0.01), then
0.96
T ≥ =600.
0.01×0.16
If ρ(x)≤0.999, then T ≥6000. As ρ→1, T →∞.
21
4. Instantaneous descent at x=0.5: ∥∇F(0.5)∥2 =4(0.5)2 =1.
• If ρ(0.5)=0.95 (δ =0.05), then
(cid:12)
dF(cid:12)
− (cid:12) =(1−0.95)×1=0.05.
dt (cid:12)
x=0.5
• If ρ(0.5)=0.99 (δ =0.01), then
(cid:12)
dF(cid:12)
− (cid:12) =(1−0.99)×1=0.01.
dt (cid:12)
x=0.5
• If ρ(0.5)=0.999 (δ =0.001), then
(cid:12)
dF(cid:12)
− (cid:12) =0.001.
dt (cid:12)
x=0.5
Hence “ρ near 1” throttles the instantaneous descent.
1
5. Internal-parameterupdate: letF(x,θ)=x2+ θ2. At(x,θ)=(0.5,0.5),∥∂F/∂θ∥=
2
0.5.
• If ρ=0.95, then δ =0.05, so ∥θ˙∥≤0.05×0.5=0.025.
• If ρ=0.99, then ∥θ˙∥≤0.01×0.5=0.005.
Again, higher ρ means slower learning.
8.2 Discrete-Time Corollary
Proof. Suppose we implement the gradient-descent-like update:
x =x − ∆t(1−ρ(x ))∇F(x ), k =0,1,2,...,
k+1 k k k
with a fixed time-step ∆t>0. Assume:
• ∥∇F(x)∥≤G for all x∈Ω.
• On the level set {x:F ≤F(x)≤F(x )}, ∥∇F(x)∥2 ≥m>0.
target 0
1
• 0<∆t≤ .
2G2
Then each iterate satisfies
1
F(x ) ≤ F(x ) − ∆t(1−ρ(x ))G2.
k+1 k 2 k
If along all iterates 1−ρ(x )≥δ, then
k
1
F(x ) ≤ F(x ) − ∆tδG2, k =0,1,...
k+1 k 2
To reduce F by at least ∆>0, one needs at least
2∆
K ≥
δG2∆t
iterations. As δ =1−ρ(x )→0, K →∞, demonstrating that “almost-perfect blankets”
k
stall discrete-time descent as well.
The following theorem formalizes how the FEP remains operative in realistically
heterogeneoussettings,wheretheinformational“shielding” ofanagent’sMarkovblankets
varies randomly across space. By showing that the expected rate of free energy descent
is proportional to (1−ρ¯), it quantifies exactly how much average permeability of the
Markovblanket(ρ¯<1)isrequiredtoguaranteenetminimization. Inpractice,thisresult
isessential: ittellsusthat—evenifsomeregionsarenearlyopaque(ρcloseto1)—aslong
as the overall environment provides enough “leakiness” or "porosity," the agent can still
reduce free energy. Without this balance theorem, we would lack a principled criterion
for when and where active inference can succeed in complex, non-uniform worlds [18].
22
Theorem 3. Let Ω ⊂ R3 be a compact domain with smooth boundary. Define a twice
continuously differentiable free-energy function
F :Ω −→ R,
satisfying
1. ∥∇F∥ :=sup ∥∇F(x)∥<+∞,
∞ x∈Ω
2. min∥∇F(x)∥2 = m ≥ 0,
x∈Ω
1 (cid:90)
3. G := ∥∇F(x)∥2dx = E [∥∇F(x)∥2].
Vol(Ω) x∼Uniform(Ω)
Ω
Assume ∥D2F∥≤L everywhere on Ω, so that F is Lipschitz with constant ∥∇F∥ and
F ∞
has Hessian bounded by L .
F
Next, let
ρ:Ω × Θ −→ [0,1]
be a random field on a probability space (Θ,F,P), satisfying:
(i) (Boundedness)
0 ≤ ρ(x,θ) ≤ 1, ∀x∈Ω, ∀θ ∈Θ.
(ii) (Spatial Stationarity in the Weak Sense) For every x∈Ω,
E[ρ(x)] = µ∈[0,1), Var[ρ(x)] = σ2.
(iii) (Covariance with ∥∇F∥2) For each x∈Ω,
Cov(ρ(x), ∥∇F(x)∥2) = C,
a constant independent of x. Equivalently,
E[ρ(x)∥∇F(x)∥2] = µG + C.
(iv) (Exponential Decay of Spatial Correlations)Thereexistsacorrelationlength
ℓ>0 such that, for all x,y ∈Ω,
(cid:16) ∥x−y∥(cid:17)
|Cov(ρ(x), ρ(y))| ≤ σ2 exp − .
ℓ
Consider the stochastic dynamics
x˙ = −(1−ρ(x ))∇F(x ), x(0)=x ∈Ω.
t t t 0
Theorem 4 (Free Energy Descent under a Stochastic ρ Field [19]).
A. Free-Energy Descent in Expectation Define
ϕ(x) = (1−ρ(x))∥∇F(x)∥2.
Taking expectation over both the random field ρ and (ergodically) over x in Ω, we
t
have
d (cid:104) (cid:105)
E[F(x )]=E[∇F(x )·x˙ ]=−E (1−ρ(x ))∥∇F(x )∥2 .
dt t t t t t
Since
E[ϕ(x)]=E[∥∇F(x)∥2] − E[ρ(x)∥∇F(x)∥2]=G−(µG+C)=(1−µ)G − C,
it follows that
(cid:16) C(cid:17) d
If (1−µ)G−C > 0 ⇔ µ<1− , then E[F(x )]=−((1−µ)G−C)<0, ∀t≥0.
G dt t
Consequently, for any finite T >0,
E[F(x )] ≤ E[F(x )] − ((1−µ)G−C)T.
T 0
23
B. Free-Energy Descent with High Probability (Pointwise Uniform Control)
Define
m := min((1−µ)∥∇F(x)∥2−C).
0
x∈Ω
Assume m >2ε for some ε>0. Also fix a finite grid {x(1), x(2), ..., x(N)}⊂Ω
0
such that max min ∥x−x(i)∥ ≤ δ. Since each ϕ(x) = (1−ρ(x))∥∇F(x)∥2 is
x∈Ω i
bounded in [0,K2], Hoeffding’s inequality implies, for each fixed i,
(cid:16) (cid:17) (cid:16) 2ε2(cid:17)
P |ϕ(x(i))−E[ϕ(x(i))]|≥ε ≤ 2 exp − .
K4
Taking a union bound over all N grid points,
(cid:16) (cid:17) (cid:16) 2ε2(cid:17)
P ∃isuch that|ϕ(x(i))−E[ϕ(x(i))]|≥ε ≤ 2N exp − .
K4
Choose N (or refine the grid) so that
(cid:16) 2ε2(cid:17)
2N exp − ≤ δ,
K4
for a prescribed small δ >0. Moreover, by continuity of ϕ(x), the maximum oscil-
lation between ϕ(x) and ϕ(x(i)) for any x within δ of x(i) can be made arbitrarily
small by choosing δ sufficiently small.
Therefore, with probability at least 1−δ,
sup|ϕ(x)−E[ϕ(x)]|<ε,
x∈Ω
and since E[ϕ(x)]≥m for every x, one obtains
0
ϕ(x)=(1−ρ(x))∥∇F(x)∥2 ≥ E[ϕ(x)]−ε ≥ m −ε > 2ε−ε=ε > 0, ∀x∈Ω.
0
Hence, with probability at least 1−δ, for every t≥0,
F˙(x ) = −ϕ(x ) < −ε < 0.
t t
In other words, the free energy F(x ) decreases uniformly (at least at rate ε) for
t
all t, with probability at least 1−δ.
C. Existence of a Deterministic Descent Path Suppose there exists a continuous,
connected curve
γ :[0,1] −→ Ω, γ(0)=x , γ(1)=x∗,
0
where x∗ is a global minimizer of F, such that
1. sup ρ(γ(s)) ≤ ρ <1,
max
s∈[0,1]
2. inf ∥∇F(γ(s))∥2 = m′ >0.
s∈[0,1]
Define a deterministic “descent” velocity along γ by
γ˙(s) = −(1−ρ )∇F(γ(s)), 0≤s≤1,
max
with γ(0)=x . Then for each s∈[0,1],
0
d
F(γ(s))=∇F(γ(s))·γ˙(s)=−(1−ρ )∥∇F(γ(s))∥2 ≤ −(1−ρ )m′ < 0.
ds max max
Hence F(γ(s)) strictly decreases from F(x ) down to F(x∗) as s ranges from 0 to
0
1. In particular, γ does not “get stuck”: the factor 1−ρ is strictly positive, and
max
∥∇F∥ remains bounded below by m′ >0. Therefore, γ is a valid monotone descent
path for F.
24
D. Finite-Sample Estimates and Confidence Intervals In practice, one does not
knowµ,G,andC exactly. Instead,onedrawsafinitesampleofN pointsx ,x ,...,x
1 2 N
(uniformly from Ω or according to the stationary distribution of x ), and defines
t
the empirical estimates:
N N
µˆ = 1 (cid:88) ρ(x ), Gˆ = 1 (cid:88) ∥∇F(x )∥2,
N i N i
i=1 i=1
N
C(cid:98) =
N
1 (cid:88) ρ(x
i
)∥∇F(x
i
)∥2 − µˆGˆ.
i=1
By Hoeffding’s or Bernstein’s inequality, for any confidence level 1−δ, there exist
(cid:114)
ln(1/δ)
error bounds ε ,ε ,ε =O( ) such that, with probability at least 1−δ,
1 2 3 N
|µˆ−µ| ≤ ε
1
, |Gˆ−G| ≤ ε
2
, |C(cid:98)−C| ≤ ε
3
.
Define conservative bounds:
µ = µˆ+ε
max 1
9 Temporal Expected Free Energy and Its Dependence
on Spatial Fields
In this section we introduce another theorem showing that the temporal side of the
FEP depends on MB density. The theorem provides a mathematical foundation for
understanding free energy minimization as a spatiotemporal process. It embeds the
familiartemporalversionoftheFEPwithinabroaderframeworkwherebothfreeenergy
andMarkovblanketstrengthvarycontinuouslyacrossspace. Thisinsightnotonlyunifies
“beliefupdating” and“movement” underasingleinformationallensbutalsoopenstheway
to apply the FEP in settings where spatial coupling is partial, graded, or heterogeneous.
(Some redundancy with the previous sections is necessary for the completeness of the
argument).
9.1 Definitions and Setup
1. Spatial Free Energy F(x). For each location x ∈ Ω, define the variational free
energy
(cid:104) (cid:105)
F(x) = E logq (s|x) − logp(s, η |x) ,
qµ(s|x) µ
where
• q (s|x) is the agent’s approximate posterior density over sensory data s if it were
µ
at x.
• p(s,η | x) is the generative model (joint likelihood) of sensory data s and hidden
external states η at location x.
• The expectation E is taken with respect to q (s|x).
qµ µ
Intuitively, F(x) quantifies the discrepancy between what the agent expects to see at x
andwhattheenvironmentactually encodesatx. Inthisway,F(x)istheusualvariational
free energy functional indexed by spatial location (cf. Eq. (3)).
2. MBDensityρ(x). Insteadofahard,binaryMarkovblanket,wedefineacontinuous
blanket-density
I(I(x); E(x) | B(x))
ρ(x) = 1 − ,
I(I(x); E(x)) + ε
where
25
• I(x) denotes the agent’s internal variables within a small radius r around x.
1
• B(x) denotes the “blanket” (sensory/active) variables in the annulus between radii
r and r .
1 2
• E(x) denotes the external (hidden) variables beyond radius r .
2
• I(·;·) is the Shannon mutual information; ε > 0 is a small regularizer to avoid
division by zero.
Hence:
• IfI(I;E |B)=0exactly(perfectshieldingbyB),thenρ(x)=1(aperfectMarkov
blanket).
• If I(I;E | B) = I(I;E) (conditioning on B does not reduce dependence), then
ρ(x)=0 (no blanket; maximal coupling).
• Ingeneral,ρ(x)∈[0,1]measureshow“porous” thelocalstatisticalboundaryis(cf.
Eq. (2) and §5.4).
3. SpatialDynamics. Theagent’spositionx(t)∈Ωevolvesaccordingtothethrottled
gradient-descent:
x˙(t) = −[1−ρ(x(t))]∇F(x(t)). (6)
Concretely:

−∇F(x), ρ(x)=0,
x˙ = and for ρ(x)∈(0,1), x˙ =−(1−ρ(x))∇F(x).
0, ρ(x)=1,
Thus:
• ρ(x)=0: Theblanketisfullytransparent,sotheagentperformsordinarygradient
descent on F.
• ρ(x)≈1: The agent is nearly insulated and x˙ ≈0; free-energy descent stalls.
• Intermediate values of ρ “throttle” the descent speed proportionally to (1−ρ).
Equation (6) is precisely Eq. (4).
9.2 Expression for Temporal EFE
Theorem 5. Let π = {x(t)}τ be any (piecewise-continuous) trajectory in Ω. Then
t=0
the temporal expected free energy along π is
(cid:90) τ
G(π) = [1−ρ(x(t))] × F(x(t)) dt. (7)
0 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
couplingfactor spatialfreeenergy
In other words, G(π) is exactly the time-integral of the “accessible” free energy (1 −
ρ(x))F(x) at each location x(t).
Proof of Theorem 5. At any instant t, if the agent is located at x = x(t), the accessible
portion of the spatial free energy is
F(x) × [1−ρ(x)] .
(cid:124)(cid:123)(cid:122)(cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
totalfreeenergy couplingfactor
Indeed:
• If ρ(x) = 0, the blanket is transparent and the agent can fully exploit F(x) to
update beliefs ⇒ the accessible free energy is F(x).
26
• If ρ(x)=1, the blanket is opaque ⇒ the accessible free energy is 0.
• For ρ(x) ∈ (0,1), the fraction (1 − ρ(x)) measures how much of F(x) remains
available for reduction.
Hence, over an infinitesimal time interval [t, t+dt], the agent can reduce at most
[1−ρ(x(t))]F(x(t)) dt.
Integrating from t=0 to t=τ yields exactly
(cid:90) τ
G(π) = [1−ρ(x(t))]F(x(t))dt,
0
which is Equation (7). This completes the proof.
Remark. Equation (7) recovers Eq. (5) verbatim and is exactly what is referred to as
Theorem 5.
9.3 Evolution of the Instantaneous Integrand Γ(x)
Define the instantaneous integrand
Γ(x(t)) := [1−ρ(x(t))]F(x(t)).
Since G(π)= (cid:82)τ Γ(x(t))dt, understanding how G evolves is equivalent to computing the
0
time-derivative dΓ(x(t)) along the agent’s trajectory.
dt
4.1. Computing ∇Γ(x). Observe that
Γ(x) = (1−ρ(x))F(x).
Taking the spatial gradient:
∇Γ(x)=∇[(1−ρ(x))F(x)]=−F(x)∇ρ(x) + (1−ρ(x))∇F(x). (8)
4.2. Agent’s Dynamics. By assumption (Equation (6)),
x˙(t) = −[1−ρ(x(t))]∇F(x(t)).
Substituting ∇Γ(x) from (8) and x˙(t) yields
d
Γ(x(t))=∇Γ(x(t)) · x˙(t)
dt
(cid:104) (cid:105) (cid:104) (cid:105)
= −F(x(t))∇ρ(x(t)) + (1−ρ(x(t)))∇F(x(t)) · −(1−ρ(x(t)))∇F(x(t))
=−(1−ρ(x(t)))2 ∥∇F(x(t))∥2 − F(x(t))(1−ρ(x(t)))[∇ρ(x(t))·∇F(x(t))].
Hence, for brevity dropping the (x(t)) arguments,
d
Γ(x) = −(1−ρ)2∥∇F∥2 − F (1−ρ)[∇ρ · ∇F]. (9)
dt
Equation (9) displays two terms:
(A) Throttled Descent Term:
−(1−ρ(x))2 ∥∇F(x)∥2.
• If ρ(x) < 1, this term is strictly negative (unless ∇F(x) = 0), ensuring Γ(x)
(and thus G) decreases.
27
• As ρ(x) → 1, the factor (1−ρ(x))2 → 0, so this negative term vanishes and
no descent occurs. In particular, if ρ(x) = 1, then x˙ = 0 and Γ(x) = 0, so
d
Γ=0. This is precisely the “exact blocking” result (Theorem 2).
dt
(B) Gradient-Alignment Correction:
−F(x)(1−ρ(x))[∇ρ(x)·∇F(x)].
• If ∇ρ(x)·∇F(x)>0, then this term is strictly negative, further accelerating
Γ’s decrease.
• If ∇ρ·∇F <0, it could partially oppose descent.
• The gradient-alignment assumption requires ∇ρ·∇F >0 over an open set D.
Underthatassumption,(9)impliesΓdecreasesstrictly,showingsimultaneous
descent of F and “leakage” 1−ρ. This recovers Theorem 1.
9.4 Corollaries: Theorems 1 and 2
Corollary 1 (Exact Blocking, Theorem 2). Ifρ(x(t))=1atsomepointx(t), then
x˙(t)=−(1−ρ)∇F =0, so x(t) remains fixed. Moreover, Γ(x(t))=(1−ρ)F =0, and
from (9),
d
Γ(x(t)) = 0.
dt
Hence the agent is “frozen” and cannot reduce any free energy once it enters a perfect-
blanket region.
Corollary 2 (Gradient Alignment, Theorem 1). If, over an open set D ⊂Ω, the
gradient-alignment condition
∇ρ(x) · ∇F(x) > 0 and ρ(x)<1, F(x)>0 for all x∈D
holds, then from (9), both terms on the right-hand side are strictly negative, so
d
Γ(x(t)) < 0 whenever x(t)∈D.
dt
Thus Γ (and therefore the accessible free energy) strictly decreases as long as the agent
remains in D. Consequently, the agent’s trajectory simultaneously descends F and de-
creases ρ, driving it toward regions of stronger coupling and lower free energy.
9.5 Interpretation
Taken together, Theorem 5 and its corollaries paint a vivid picture:
• The temporal EFE G(π) is not an independent objective; it is exactly the time-
integral of the spatial free energy F(x), gated by the local blanket density ρ(x).
• The agent’s spatiotemporal dynamics are determined by the interplay between the
shape of F(x) and the “porosity” ρ(x).
• Exact blocking: Regions where ρ = 1 act as walls: the agent cannot traverse
them nor reduce any free energy within them.
• Gradient alignment: IfspatialgradientsofρandF alignpositively, theagentis
guaranteedtomovetotilesof(F,ρ)thataresimultaneouslylower, therebyforging
a path of ever-stronger coupling and lower surprise.
This theorem makes “space” a first-class player in active inference. In this way, one
obtains a unified description of how movement (spatial navigation) and belief updating
(free energy minimization) are two sides of the same informational coin.
28
10 Inversion of Free Energy Minimization via Extended
MB Density
In the previous parts of this paper, the blanket-density field ρ(x) is constrained to lie in
[0,1], ensuring that the “throttled” gradient flow
x˙ = −[1−ρ(x)]∇F(x)
always points downhill on the free energy F. Consequently, an agent following these
dynamics strictly minimizes F. Here, we relax the requirement ρ(x) ≤ 1 and allow
ρ(x) to exceed unity in certain regions. In that case, the prefactor [1−ρ(x)] becomes
negative, and the flow reverses direction—driving the system uphill on F. Theorem 6
below formalizes this phenomenon.
Theorem 6 (Inversion of Free Energy Flow under ρ > 1). Let Ω ⊂ Rn be an open set,
and let F:Ω→R be a C1 function. Suppose we define an extended blanket-density field
ρ:Ω −→ R
and an open subset U ⊂Ω such that
ρ(x) > 1 for all x∈U.
Consider the modified dynamics
x˙ = −[1−ρ(x)]∇F(x), x(0)∈U.
Then for every x∈U, the following statements hold:
1. Original normalization of ρ. In the original framework, ρ(x) was defined by
I (I(x); E(x)|B(x)) + ε
ρ(x) = true ∈ [0, 1],
I (I(x); E(x)) + ε
true
because unconditional mutual information I(I;E) is always at least as large as
conditional mutual information I(I;E | B). Therefore 1−ρ(x) ≥ 0 ensured x˙
pointed downward on F.
2. Extended definition allowing ρ>1. Topermitρ(x)>1,replacethenormalized
I (I;E |B)
ratio true by a more general mapping
I (I;E)
true
(cid:16) (cid:17)
ρ(x) = f I (I;E |B)(x), I (I;E)(x) ,
true true
where f:R+×R+ →R is chosen so that f(x)>1 on U. Examples include:
• Shifted ratio:
I (I;E |B)(x) + ε
ρ(x) = true + α, α>0,
I (I;E)(x) + ε
true
which lies in [α, 1+α].
• Weighted excess information:
I (I;E |B)(x) (cid:16) I (I;E |B)(x)(cid:17)
ρ(x)= true + β 1− true , β >1,
I (I;E)(x) I (I;E)(x)
true true
which can exceed β when I (I;E |B)≪I (I;E).
true true
In either construction, ρ(x) may exceed 1 for all x∈U.
29
3. Gradient-ascent when ρ > 1. Whenever ρ(x) > 1, the coefficient [1−ρ(x)] is
strictly negative. Thus for x∈U,
x˙ =−[1−ρ(x)]∇F(x)=[ρ(x)−1]∇F(x),
which is the gradient-ascent flow on F instead of gradient-descent.
4. Free-energy increase formula. Along any trajectory x(t) satisfying x˙(t) =
(ρ(x(t))−1)∇F(x(t)) with x(t)∈U, one obtains
d
F(x(t))=∇F(x(t)) · x˙(t)=(ρ(x(t))−1)∥∇F(x(t))∥2 > 0,
dt
since ρ(x(t))−1>0 and ∥∇F(x(t))∥2>0 except at critical points. Consequently,
F(x(t)) strictly increases as long as x(t)∈U.
5. Separatrix at ρ=1 and illustrative example. The level set {x:ρ(x)=1} is
a hypersurface on which x˙ =0. It separates:
• {ρ(x)<1}: descent on F.
• {ρ(x)>1}: ascent on F.
For a concrete example, let ρ(x) be a C1 function such that

0.8, ∥x∥≤1,

ρ(x)= 1.2, 1<∥x∥≤2,
0.5,
∥x∥>2,
with smooth transitions at ∥x∥=1 and ∥x∥=2. Then:
• For ∥x∥≤1, ρ(x)=0.8<1: the agent follows gradient-descent on F.
• For 1<∥x∥≤2, ρ(x)=1.2>1: the agent follows gradient-ascent on F.
• For ∥x∥>2, ρ(x)=0.5<1: gradient-descent on F resumes.
This construction can produce limit-cycle or oscillatory behavior: the agent de-
scends in ∥x∥≤ 1, then ascends in 1 < ∥x∥≤ 2, and descends again for ∥x∥> 2,
repeatedly.
If the blanket-density factor ρ(x) stays between 0 and 1, then
x˙ =−[1−ρ(x)]∇F(x)
always points in the direction of decreasing free energy. In contrast, whenever ρ(x)>1,
the multiplier [1−ρ(x)] becomes negative and
x˙ =(ρ(x)−1)∇F(x)
points in the direction of increasing free energy. Thus, in regions where ρ(x) > 1, the
agent climbs up the free energy landscape instead of descending it. The level set
{x:ρ(x)=1}
forms a boundary separating “descent” regions (ρ < 1) from “ascent” regions (ρ > 1).
Crossing this boundary reverses the agent’s objective from minimizing free energy to
maximizing it. In reality, this is not a simple abstract extension of the initial model.
The “shift” we have inserted to make ρ>1 can be interpreted as a perturbation. Or, for
example, interpreting the human brain as a blanket-density field, the “shift” can be seen
as a form of psychopathology.
30
11 Joint Inference of an Unknown MB Density
How can an active inference agent learn about and move around in space? In the earlier
sections,weassumedthattheagenthaspriorknowledge—atleastinstatisticalform—of
the MB density ρ(x) or its empirical approximation ρ (x). In more realistic settings,
N
however, the agent does not know ρ (x) in advance. It must, instead, infer the blanket
N
density while simultaneously minimizing variational free energy. We now show how an
active inference agent can learn the spatial profile of ρ(x) on the fly and prove that its
estimate converges to the true field.
11.1 Belief Model and Coupled Dynamics
Let Ω⊂Rn be compact, and let ρ (x)∈C1(Ω) denote the true blanket density. The
true
agent maintains a parametric family {ρ (x)}, θ ∈ Θ ⊂ Rp, with Θ compact and each
θ
ρ (x) lying in [0,1]. Initially, the agent possesses a prior density p (θ)>0 on Θ.
θ 0
As the agent moves, it collects sensory data s(x(t)) at positions x(t). From each
sample, it constructs a likelihood L(s(x);ρ (x)) that measures how well ρ (x) explains
θ θ
the observed coupling between internal and external states. Under standard regularity
(continuityofρ inbotharguments,boundedlikelihoods,andidentifiabilityofθ ),the
θ true
posterior
t
(cid:89)
p (θ) ∝ p (θ) L(s(x(τ)); ρ (x(τ)))
t 0 θ
τ=0
concentrates on the true parameter θ satisfying ρ (x)=ρ (x).
true θtrue true
Simultaneously, the agent uses the current point estimate
ρˆ(x) = ρ (x), θˆ =argmaxp (θ),
t θˆ
t
t
θ
t
to drive free-energy descent:
x˙(t) = −[1−ρˆ(x(t))]∇F(x(t)).
t
Thus, the agent interleaves Bayesian updating of θˆ and state evolution under a scaled
t
gradient flow.
11.2 Consistency of Blanket-Density Learning
Theorem 7 (Convergence of Joint Inference and Descent). Assume:
1. Eachρ (x)isC1 onΩ×Θ,takesvaluesin[0,1],andthemapθ (cid:55)→ρ (x)isinjective
θ θ
for every x∈Ω.
2. The likelihood L(s;ρ (x)) is Lipschitz in both s and ρ (x), and identifies θ such
θ θ true
that ρ =ρ .
θtrue true
3. The prior p (θ)>0 in a neighborhood of θ .
0 true
4. The descent flow under the true blanket density,
x˙ =−[1−ρ (x)]∇F(x),
true
is ergodic on Ω: it visits every open set infinitely often.
5. F ∈C2(Ω) has ∥∇F(x)∥>0 except at finitely many isolated minima.
Thenast→∞, theagent’spointestimateρˆ(x)convergesuniformlytoρ (x)onevery
t true
compact K ⊂Ω, almost surely:
t→∞
sup|ρˆ(x)−ρ (x)| −−−→ 0.
t true
x∈K
31
Proof. Because the true descent flow visits every open neighborhood infinitely often, the
agent samples sensory data at arbitrarily many positions throughout Ω. Under assump-
tions(1)–(3),classicalconsistencyforinput-drivenparameterestimationguaranteesthat
p (θ) concentrates on θ , so θˆ →θ and hence ρˆ(x)→ρ (x) uniformly on com-
t true t true t true
pacts.
We must ensure that using ρˆ in place of ρ does not break ergodicity. Since
t true
ρˆ →ρ uniformly, for large t the perturbed vector field
t true
−[1−ρˆ(x)]∇F(x)
t
differs from the true flow by at most a small uniform error. Because F has no plateaus,
thissmallperturbationleavesthetrajectoryqualitativelyunchanged: itcontinuestovisit
each open neighborhood infinitely often. Hence the learning process remains ergodic,
validating the consistency argument ad infinitum.
Theorem 7 shows that an active inference agent, without prior knowledge of ρ(x),
can learn the blanket density while descending free energy. Ergodicity ensures the agent
gathersenoughdatatoidentifyθ almostsurely,andfreeenergyminimizationprevents
true
collapseintoazero-blanketstate. Consequently,ρ(x)becomesunpredictableinadvance:
the agent must discover its own coupling constraints by moving through and sampling
the environment. This extension embeds the blanket into the agent’s inference process,
yieldingafullyself-consistentactiveinferencemodelonanunknownstochasticlandscape.
12 MB Density: Axiomatization and Derivation of the
FEP
In this section we provide a rigorous, assumption-light derivation of free energy min-
imization as a necessary dynamical consequence of the information geometry induced
by the MB Density ρ(x). Our strategy is: (i) postulate minimal axioms on ρ (regu-
larity and informational meaning), plus symmetry/isotropy assumptions that rule out
dynamically uninformative tangential drifts; (ii) deduce that any admissible dynamics
must be colinear with ∇ρ with magnitude depending only on the level ρ(x); (iii) show
that this structure forces the existence of a scalar potential F such that the dynamics
is a gradient flow x˙ = −∇F. Classical choices (e.g. F = −log(1−ρ+ε)) then appear
ascorollaries—monotonereparameterizationsoftheemergentpotential—ratherthanad
hoc postulates. We also prove that the FEP has operational content precisely on the
regime ρ<1, while it is informationally vacuous on ρ=1.
We work on an open domain Ω ⊂ Rn with the Euclidean metric; x ∈ Ω denotes the
agent’s state.
12.1 Setup and standing assumptions
For each x ∈ Ω, let (I,B,E) be a measurable partition of the variables of a generative
model, inducing a local data distribution p . Define
x
MI(x):=I(I;E) , CMI(x):=I(I;E |B) . (10)
x x
Axiom 8 (positivity of MI). There exists an open set U ⊆ Ω such that MI(x) > 0 for
almost every x∈U.
Axiom 9 (regularity). The maps x(cid:55)→MI(x) and x(cid:55)→CMI(x) are C1 on U.
Definition 12.1 (MB Density). For x∈U define
CMI(x)
ρ(x):=1− ∈(0,1]. (11)
MI(x)
Extend by continuity with ρ=1 where CMI=0. Assume ρ∈C1(U).
Interpretation. ρ(x) = 1 iff the blanket perfectly screens I and E (I ⊥ E | B); ρ ↓ 0
corresponds to maximal conditional coupling.
32
12.2 Operability of inference
Theorem 10 (Operability iff ρ < 1). For x ∈ U, ρ(x) = 1 ⇔ I(I;E | B) = 0. In this
x
case, any variational free energy functional
F(x)=E [logq (s)−logp (o,s)] (12)
qx x x
has zero first variation with respect to external perturbations transmitted through B: the
“outer” gradient vanishes and inference is informationally vacuous.
Proof. If ρ = 1 then CMI = 0, hence p (i | e,b) = p (i | b) (conditional independence).
x x
Therefore changes in e cannot affect F once b is fixed, and the outer gradient is zero.
The converse is by Definition 12.1.
12.3 Symmetry, isotropy, and the form of admissible dynamics
Let v :U →Rn be the dynamical field, x˙ =v(x).
Axiom11(level-setsymmetry). ForeverydiffeomorphismΦpreservingρ(i.e.ρ◦Φ=ρ),
the dynamics is equivariant: DΦ(x)v(x)=v(Φ(x)).
Axiom12(notangentialpreference). OneachregularlevelsetL :={x∈U :ρ(x)=c},
c
the tangential component of v vanishes for a.e. x: v(x) is colinear with ∇ρ(x) wherever
∇ρ̸=0.
Lemma 13 (colinearity). Under Axioms 11 and 12, there exists a scalar function α :
U →R such that v(x)=−α(x)∇ρ(x) for a.e. x∈U.
Proof. By Axiom 12, v(x) has no tangential component on L , hence it is normal to
c
L , i.e. colinear with ∇ρ. The sign is chosen so that flow proceeds towards increasing
c
information accessibility (monotone decrease of screening).
Axiom 14 (levelinvarianceofmagnitude). αisconstantalongeachlevelsetofρ: there
exists a continuous f :[0,1]→[0,∞) with α(x)=f(ρ(x)).
Axiom 15 (stall at perfect screening). f(1)=0 and f(ρ)>0 for ρ<1.
Combining Lemma 13, Axiom 14 and Axiom 15 we obtain the forced form of the
dynamics:
x˙ =v(x)=−f(ρ(x))∇ρ(x). (13)
12.4 Integrability and the emergent Free Energy potential
Lemma16(integrability). Definew(x):= 1 ∇ρ(x)ontheregularset{x:∇ρ(x)̸=
f(ρ(x))
0}. Then w is irrotational; hence, there exists F ∈C2 such that ∇F =w.
Proof. Since w = ψ(ρ)∇ρ with ψ(ρ) := 1/f(ρ), choose any C2 primitive Ψ of ψ, i.e.
Ψ′ =ψ. Then w =∇(Ψ◦ρ) is a gradient field.
Definition 12.2 (Emergent Free Energy). Fix a constant C ∈R and define
(cid:90) ρ(x) 1
F(x):=Ψ(ρ(x))+C = du+C. (14)
f(u)
Theorem 17 (Emergence of FEP). Under Axioms 8–15, the dynamics (13) is exactly
a gradient flow of the potential F in (14):
x˙ =−∇F(x). (15)
Moreover, F is a Lyapunov function:
d
F(x(t))=∇F ·x˙ =−∥∇F∥2≤0, (16)
dt
with equality iff ∇ρ=0 or ρ=1.
33
Proof. ByLemma16,∇F = 1 ∇ρ. Multiplyingbothsidesby−f(ρ)yields−f(ρ)∇F =
f(ρ)
−∇ρ. Comparing with (13) gives x˙ = −∇F. The Lyapunov property follows from
standardgradient-flowcalculus;vanishingoccursonlyatcriticalpointsofF (i.e.∇ρ=0)
or where f(1)=0 (i.e. ρ=1).
Corollary 18 (class of representatives and uniqueness up to reparameterization). If
F˜ = ϕ◦F with ϕ strictly increasing, then x˙ = −∇F˜ generates the same trajectories
up to time-rescaling. Conversely, any gradient potential producing the same trajectories
must be a strictly monotone reparameterization of F. In particular, the classical choice
f(ρ)=1−ρ+ε (ε>0) yields
F(x)=−log(1−ρ(x)+ε)+C, (17)
as a corollary of Theorem 17.
12.5 Link to standard variational free energy
Let F(x) denote a standard variational free energy (VFE) for the recognizer q .
x
Axiom 19 (informational alignment). There exists a continuous κ:[0,1]→[0,∞) with
κ(1)=0 and κ(ρ)>0 for ρ<1, such that
∇F(x)=κ(ρ(x))∇ρ(x). (18)
Proposition 12.1 (directional equivalence). Under Axiom 19, there exists a strictly
increasingϕsuchthatthegradientflowofF coincideswiththatofϕ◦F;hence,aftertime-
rescaling,theVFE-descentisdynamicallyequivalenttotheemergentFEPofTheorem17.
Proof. From ∇F =κ(ρ)∇ρ and ∇F =(1/f(ρ))∇ρ, define ϕ′◦F :=κ(ρ)f(ρ). Since ρ is
a strictly monotone function of F by (14) and Axiom 15, ϕ is well-defined and strictly
increasing. Then ∇(ϕ◦F) = ϕ′(F)∇F = κ(ρ)∇ρ = ∇F. A constant time-rescaling
aligns the flows.
12.6 Limits, regularity, completeness
• Critical points of ρ. Where ∇ρ = 0, the flow stalls; F has stationary points
(minima, maxima, or saddles).
• Frontier ρ = 1. By Axiom 15, f(1) = 0; we may normalize F| = 0. At ρ = 1
ρ=1
the FEP is informationally vacuous by Theorem 10.
• Domain issues. The set where MI = 0 has measure zero in U by Axiom 8; the
extension ρ=1 is continuous there.
• Uniqueness (strong). Under Axioms 11–15, any admissible dynamics must take
theform(13);henceapotentialF existsnecessarily andtheclassofrepresentatives
is exactly {ϕ◦F :ϕ↑}.
Conclusion. The MBD ρ together with symmetry and isotropy axioms forces the
dynamics to be a gradient flow of an emergent Free Energy potential defined by (14).
Thus,theFEPisnotprimitivebutaderivative structureprevailingontheinformational
regime ρ<1 and becoming vacuous at ρ=1.
13 Limitations and the Risk of Circularity
InTheorem1,itisassumedthatthemutualinformation(bothmarginalandconditional)
is C1 and that their gradients align with ∇F over an open set D. We recognize that
this requirement of “gradient alignment” is extremely strong and likely does not hold in
many real-world applications (biological or engineering), where ∇F and ∇I may point
in very different directions.
34
In Theorems 3 and 4, the assumption of “constant covariance”
Cov(ρ(x), ∥∇F(x)∥2) = C
is an artificial simplification, which is difficult to justify in practical situations where
both ρ(x) and ∇F(x) can vary spatially in complex ways.
ThekNN–KSGestimator,onwhichtheestimationofρ relies,requireshigh-dimensional
N
datasetsandsuffersfromthecurseofdimensionality. Ifthedatas havedimensiond≫1,
i
obtaining a sufficiently accurate mutual information estimate to guarantee convergence
in the C1 norm becomes practically infeasible.
Alloftheseregularityandstationarityassumptionslimitthepracticalapplicabilityof
thesetheorems: ifonetrulywantstousethemtoexplainneuralorbehavioralphenomena,
itisnecessarytodemonstratethatthebasicassumptions(alignment,constantcovariance,
exponential decay of correlations) are at least approximately satisfied on real data.
Another limitation concerns the possible risk of circularity of the overall argument.
Saying “the agent moves to regions of low p” can be read as “the agent moves where it
is already well coupled,” which is arguably just restating “the agent moves to reduce free
energy” inspatialterms. Theapparentcircularitydissolvesonceonerecognizesthathere
ρ(x)isdefineda priori asanexternalfieldofconditional-informationestimates—derived
from raw sensory-environment samples—rather than as a byproduct of an agent’s free-
energydescent. Inotherwords,onefirstsamplesthejointstatisticsofinternal,external,
andblanketvariablestobuildρ(x)independentlyofanyinferenceprocess;onlythendoes
the agent navigate according to ∇F and the precomputed ρ. Because ρ(x) is not recom-
puted from the agent’s current beliefs but estimated from external data, minimizing free
energy does not “chase its own tail” but rather follows a fixed landscape of informational
permeability, rendering any notion of tautological self-reference illusory.
14 Conclusions
The core idea of this paper is to reconceptualize the FEP not merely as an internal
ruleforbeliefupdatingbutasgenuinespatialnavigationthroughacontinuouslyvarying
MB density field. Instead of treating the informational boundary between internal and
externalstatesasabinarycondition,weintroduceafunctionρ(x),definedateverypoint
x in a continuous domain, which quantifies how “insulated” that location is. Values of
ρ(x) near zero indicate that internal and external states are strongly coupled (minimal
insulation),whereasvaluesclosetooneindicatethatalocationisalmostentirelyisolated
(maximal insulation).
To make this precise, the paper ties ρ(x) to an information-theoretic measure: it
is the ratio between conditional mutual information I(I;E | B), which measures how
much information about external states E remains once boundary states B are known,
and unconditional mutual information I(I;E), which captures overall coupling. Because
conditioningcannotincreasemutualinformation,thatratioalwaysliesbetweenzeroand
one. Consequently,whenρ(x)isnearzero,mostofthemutualinformationbetweeninter-
nal and external states bypasses the boundary, and when ρ(x) is near one, conditioning
on the boundary removes almost all of the coupling. In other words, ρ(x) serves as a
continuous gauge of how effectively the environment can inform the agent at location x.
The formulation of the FEP presented here, grounded in the concept of a spatially
distributed MB density ρ(x), offers several theoretical advances relative to traditional
approaches. In contrast to standard formulations that assume the FEP as a universal
law governing all systems equipped with a Markov blanket, our approach introduces
a continuous informational field that modulates the very applicability of the principle.
Specifically, the FEP becomes locally definable only in regions where ρ(x) < 1, that is,
where conditional dependence between internal and external states—mediated by the
blanket—is sufficiently high to permit inferential coupling. This perspective yields four
distinct advantages. First, it introduces a topological criterion of inferential viability,
replacing the binary presence or absence of a blanket with a scalar field that continu-
ouslymeasuresthestrengthofinformationalseparation. Thismakesitpossibletostudy
inference not as a global property, but as a situated phenomenon, dependent on the
35
agent’s position within an informational manifold. Second, it reorients the explanatory
architecture of the FEP: rather than positing inference as an intrinsic drive of the sys-
tem, it treats inference as an affordance of the environment—possible only where the
topology of ρ(x) permits it. Third, this framework offers a geometrization of inference.
By defining informational gradients, curvature, and metric structure on the field ρ(x),
we provide tools to analyze cognitive processes as forms of motion through an infor-
mational landscape. This opens new paths for modeling agent-environment interaction,
includingmulti-agentsystems,distributedcognition,andecologicallearning. Finally,the
concept of MB density allows us to extend the reach of the FEP to systems where the
blanket structure is ambiguous, dynamic, or absent—such as in decentralized networks,
social systems, or collective organisms. In such contexts, ρ(x) can serve as a measure of
inferential accessibility even in the absence of well-defined agent boundaries.
36
Figure 2: Agent trajectories shaped by Markov blanket density. Thisfigurecompares
two systems governed by the same free energy minimization equation x˙ = −(1−ρ(x))∇F(x),
where ρ(x) is the spatially distributed Markov blanket density. Left: In the case of an infant
engagedinsocialinteraction,theMBdensityislow,allowingforstrongcouplingwiththeenvi-
ronment. Theagentfollowsthefreeenergygradientefficiently,resultinginasmoothanddirected
trajectory. Right: In the bureaucratic system, high blanket density inhibits coupling. Despite
non-zero free energy gradients, the trajectory is shallow and constrained, demonstrating how
strong informational boundaries block adaptive inference. The color maps represent the local
coupling potential (1−ρ), highlighting the spatial modulation of active inference. Parameters:
theFiguredescribesthetrajectoriesofanagentonthefree-energylandscapeF(x,y)=x2+y2
undertwodifferentblanketdensities. Theagentstartsat(0.8,0.8)inthesquare[−1,1]×[−1,1]
and evolves for 100 explicit-Euler steps with time step ∆t = 0.02. Its velocity at each step
is given by x˙ = −(1−ρ)∇F(x), with ρ = 0.2 (blue curve, “Infant”) or ρ = 0.8 (red curve,
“Bureaucracy”), plotted in 2D with equal aspect ratio to illustrate how lower blanket density
permits faster descent toward the origin.
Figure 3: MB density as an informational topology. This 3D surface plot visualizes
the spatial distribution of MB density ρ(x) in a high-density regime (e.g., a bureaucratic sys-
tem). Regionsofhighρ(x)indicatestronginformationalboundaries—zonesoflimitedcoupling
between internal and external states. Such topologies constrain active inference by inhibiting
access to meaningful sensory feedback. This figure illustrates how the geometry of ρ(x) can
serve as an inferential landscape that shapes the success or failure of free energy minimization.
37
Figure4: Whenvariationalfreeenergyminimizationisobstructedbyinformational
structure. Thisfigureillustratesatheoreticalconflictcentraltothepaper. Anagent(reddot)
beginsinaregionofhighMBdensity(dashedredcontour,shadedredarea). Althoughtheagent
is embedded in a free energy landscape (blue gradient), and a global minimum of variational
free energy is present (black dot), the high local value of ρ(x) inhibits coupling between the
agent’sinternalstatesandexternalcauses. Asaresult,theagentcannotexploitthefreeenergy
gradient: adaptive inference is blocked not by the absence of a minimization path, but by the
statisticalopacityofthesurroundingspace. Thefiguredemonstratesthattheabilitytominimize
freeenergyiscontingentuponlocalinformationalaccessibility. Parameters: Contourplotofthe
free-energy landscape F(x,y) = x2 +y2 obstructed by a high-density barrier in ρ(x,y). On
the same 100×100 grid over [−1,1]2, F is contoured at 20 levels using the “Blues” palette. A
circularregioncenteredat(0.5,0.5)withradius0.2isassignedρ=0.95,whiletheremainderof
the grid has ρ=0.05; the ρ=0.5 boundary is overlaid as a red dashed contour. The starting
point(0.2,0.2)ismarkedwithareddotandtheglobalminimumlocation(0.5,0.5)withablack
dot.
38
Figure5: EffectofMarkovblanketdensityonagentmovementacrossinformational
landscapes. This two-panel 3D visualization illustrates how the spatial distribution of MB
density ρ(x) modulates an agent’s ability to perform gradient descent on free energy. In both
panels,anagentattemptstofollowthesameepistemicimperative—minimizationofvariational
freeenergy—bymovingthroughalandscapeshapedbyρ(x). (A)Inaregionofweak MBdensity
(lowρ(x)),theinformationalcouplingbetweenagentandenvironmentisstrong. Theagentcan
descend the surface efficiently, adapting its trajectory to the available gradient field. (B) In
contrast, in a region of strong MB density (high ρ(x)), the agent is epistemically insulated.
Coupling is weak and movement is suppressed: although gradients still exist, the agent cannot
access or respond to them effectively. These simulations demonstrate that the capacity to
minimizefreeenergyisshapednotonlybyinternaldynamicsbutalsobytheexternaltopology
of informational boundaries. Parameters: Side-by-side 3D depictions of free-energy surfaces
modulated by low versus high blanket-density fields, with corresponding agent trajectories. In
each panel, F(x,y) = x2 +y2 is plotted over a 100×100 grid on [−1,1]2 using an alpha
x+1
of 0.7 and stride 4. In panel A, ρ(x,y) = 0.2+0.3 (range [0.2,0.5]) and in panel B,
2
x+1
ρ(x,y) = 0.8+0.2 (range [0.8,1.0]). From the initial point (0.8,−0.8), each trajectory
2
is simulated for 80 explicit-Euler steps with ∆t = 0.02 using x˙ = −(1−ρ¯)∇F(x), where ρ¯is
the mean density over the panel; trajectories are drawn as red lines with markers against the
translucent energy surface.
39
Figure 6: Algorithm Agent Navigation in a Dynamic 3D Blanket-Density Field
(Section 5.4). The agent (red during the noisy phase, blue during the deterministic phase)
navigates a 3D barrier field ρ(x,y,z,t) that evolves over time due to two moving Gaussian
coupling regions. In the first half of the simulation, strong noise allows the agent to penetrate
thickbarriers; inthesecondhalf,withoutnoise,theagentsmoothlysteersaroundhigh-ρzones
and converges on its target at (2,2,2). For more details, see Appendix A.
40
Figure7: AdvancedAgentNavigationinNonstationary3DEnvironment. Snapshots
ofthethree-dimensionalblanketdensityρ(x,y,z,t)(displayedwithaplasmaheatmap)atthree
horizontal slices z ≈ −1.5, 0.0, 1.5 for times t = 0, 400, and 699. At each slice, the white,
black, and gray contour lines correspond to ρ = 0.2, ρ = 0.5, and ρ = 0.8, indicating regions
of low, medium, and high barrier strength. Red dots (sized proportionally to instantaneous
speed) mark the agent’s visits during the noisy phase (t < 400), often penetrating even the
darkest, high-barrier contours, while blue dots show the agent’s path during the deterministic
phase(t≥400),huggingjustoutsidethestrongestbarriersdespiteoccasionalperceptionnoise.
In the lower right, the full 3D trajectory is plotted: the red segment (t = 0...400) wanders
through overlapping, rotating ellipsoidal obstacles due to colored movement noise, whereas the
blue segment (t=400...699) smoothly navigates around the gray isosurface clouds at t=400
(where ρ > 0.8). Black and yellow markers denote the agent’s start at (−2.5,−2.5,−2.5) and
the final position of the moving helix target. This composite visualization demonstrates how
an inertial agent with AR(1) movement and perception noise first plows through dynamically
changing, anisotropic obstacles and then transitions to informed, barrier-avoiding navigation
toward a moving goal. For more details, see Appendix A.
41
Figure8: InertialAgentNavigationinUltra-Complex3DBarriers. Eachpanelinthis
3×3 grid shows a horizontal slice of the ultra-complex, nonstationary barrier field ρ(x,y,z,t)
(plotted with a plasma colormap) at z ≈−1.5, 0.0, and +1.5 for times t=0, 350, and 599. In
each slice, white contours mark ρ = 0.2 (low barriers), black contours mark ρ = 0.5 (medium
barriers),andgraycontoursmarkρ=0.8(strongbarriers). Overlaidreddots—sizedinpropor-
tiontoinstantaneousspeed—representtheagent’svisitsduringthenoisyphase(t<350),often
penetratingeventhehighest-barrier(gray)regionsbecausecoloredmovementnoiseoverwhelms
thebarrier. Bluedotscorrespondtothedeterministicphase(t≥350),wheretheinertialagent,
subject to AR(1) perception noise, hugs just outside the strongest barrier contours and weaves
throughlower-ρcorridors. Inthelowerrightpanel,thecomplete3Dtrajectoryisshown: thered
segment (t = 0...350) meanders through overlapping, rotating ellipsoidal Gaussians, AR(1)-
driftingmicro-obstacles,andatime-varyingrandomFourierfield. Whenmovementnoiseceases
att=350,thebluesegment(t=350...599)smoothlynavigatesaroundthelayeredisosurfaces
at t=350, where 0.5<ρ≤0.8 (light gray) and ρ>0.8 (dark gray). Black and yellow markers
denotetheagent’sstartinglocation(−2.5,−2.5,−2.5)andthefinalpositionofthemovinghelix
target, respectively. For more details, see Appendix A.
42
Appendix A: Detailed Objectives and Methodology of
Figures 6-7-8
Figure 6
ThegoalofFigure6istoconstructandvisualizeacontinuousthree-dimensionalblanket-
density field ρ(x,y,z,t), to demonstrate how high-noise perturbations enable an agent
to traverse thick Markov blankets, and then to show how, once the noise is removed, the
agent’s dynamics
x˙ = −(1−ρ(x,t))∇F(x), F(x)=∥x−(2,2,2)∥2,
cause it to avoid high-ρ zones and find a low-resistance path toward a fixed target at
(2,2,2).
At each time step t = 0,1,...,399 on a uniform 35×35×35 grid over [−3,3]3, we
define two moving Gaussian coupling regions:
c (t)=(1.5cos(0.02t), 1.5sin(0.02t), 0), σ =0.7,
1 1
c (t)=(1−0.01t, −1+0.01t, 0.5sin(0.015t)), σ =0.5.
2 2
Foreachgridpointx=(x,y,z),thetotalcouplinga(x,t)isthesumofthetwoGaussian
values
(cid:16) ∥x−c (t)∥2(cid:17) (cid:16) ∥x−c (t)∥2(cid:17)
a(x,t) = exp − 1 + exp − 2 .
2σ2 2σ2
1 2
We then form
a(x,t)2 1
ρ (x,t) = , I(x,t) = − ln[1−ρ (x,t)2],
corr a(x,t)2+0.92 2 corr
and normalize I(x,t) over the entire grid to obtain ρ(x,t)∈[0,1].
The agent’s trajectory is computed in two phases:
• Noisy Phase (0≤t<200): At each step,
x(t+∆t) = x(t) + ∆tv(t) + η(t), ∆t=0.03,
where
v(t)=−(1−ρ(x(t),t))∇F(x(t)), η(t)∼N(0, 0.92I ).
3
• Deterministic Phase (200≤t<400): Noise is removed, and
(cid:104) (cid:105)
x(t+∆t) = x(t) + ∆t −(1−ρ(x(t),t))∇F(x(t)) .
Figure6isdisplayedasa3×3gridoftwo-dimensionalheatmaps. Columnscorrespond
to t = 0, 200, 399, and rows correspond to slices at z ≈ −1.0, 0.0, 1.0. Each heatmap
uses a plasma colormap to show ρ(x,y,z,t), with white and black contours at ρ = 0.3
and ρ = 0.6 indicating moderate and strong barrier levels. Overlaid red dots show the
agent’s positions during the noisy phase (for each slice, points where |z −z |<0.1
agent slice
overt±10), whilebluedotsshowpositionsduringthedeterministicphase. Inthelower-
right panel, the full 400-step trajectory is shown in three dimensions: a red segment
for 0 ≤ t ≤ 200 (noisy phase), which penetrates high-ρ regions, and a blue segment
for 200 ≤ t ≤ 399 (deterministic phase), which winds around gray points (grid cells at
t = 200 satisfying ρ > 0.6). A black dot at (−2.5,−2.5,−2.5) marks the agent’s start,
and a yellow dot at (2,2,2) marks the target.
43
Figure 7
Figure 7 demonstrates an advanced agent navigating an even more nonstationary 3D
environment. The environment is a 40×40×40 grid over [−3,3]3 containing:
1. Four rotating, anisotropic ellipsoidal Gaussians with parameters
σ ={(0.7,0.4,0.3), (0.5,0.5,0.6), (0.6,0.3,0.7), (0.4,0.6,0.5)}.
blobs
• Blob1: Center(1.5cos(0.02t), 1.5sin(0.02t), 0),covariancediag(0.72, 0.42, 0.32)
rotated about z by 0.01t.
• Blob2: Center(1−0.01t, −1+0.01t, 0.5sin(0.015t)),covariancediag(0.52, 0.52, 0.62)
rotated about y by 0.015t.
• Blob3: Center(0.5cos(0.03t), 0.5sin(0.03t), cos(0.02t)),covariancediag(0.62, 0.32, 0.72)
rotated about x by 0.012t.
• Blob4: Center(2cos(0.01t), 2sin(0.01t), −1+0.005t),covariancediag(0.42, 0.62, 0.52)
rotated about z by 0.02t.
2. Ten micro-obstacles of width σ = 0.3 whose centers {m (t)} follow an AR(1)
obs i
process with ϕ =0.9. Each micro-obstacle contributes
micro
(cid:16) ∥x−m (t)∥2(cid:17)
0.3exp − i
2(0.3)2
to the total coupling field a(x,t).
3. Aspatio-temporalrandomFourierfieldbuiltasthesumoffifteensinusoidalwaves
15
(cid:88)
sin(w ·(x,y,z)+ω t+ϕ ),
k k k
k=1
withrandomwavevectorsw ∼N(0,1.52I ),frequenciesω ∼Uniform(0.005,0.02),
k 3 k
and random phases ϕ ∼[0,2π]. This field is normalized to [0,1] and scaled by 0.5
k
before adding to a(x,t).
Thus, at each time 0≤t<700,
a(x,t)= (cid:88) 4 exp[− 1 (x−c (t))⊤Σ (t)−1(x−c (t))]+0.3 (cid:88) 10 exp (cid:104) − ∥x−m i (t)∥2(cid:105) +0.5f (x,t).
2 j j j 2(0.3)2 RF
j=1 i=1
We compute
a(x,t)2 1
ρ (x,t) = , I(x,t)=− ln[1−ρ (x,t)2],
corr a(x,t)2+1.02 2 corr
and normalize I(x,t) across all grid points to obtain ρ(x,t)∈[0,1].
The agent pursues a moving helix target
g(t)=(2cos(0.005t), 2sin(0.005t), 2−0.002t)
using second-order dynamics (mass m = 1.2, damping γ = 0.8, time step ∆t = 0.02).
Its perceived barrier strength is the average of ρ over the local neighborhood of radius
1.0 on the 403 grid, corrupted by AR(1) perception noise (ϕ = 0.6, σ = 0.15).
perc perc
During 0 ≤ t < 400 (the noisy phase), movement noise follows AR(1) with ϕ = 0.7
move
and σ = 0.7; for 400 ≤ t < 700 (deterministic phase), movement noise is removed
move
but perception noise remains.
44
Figure 8
This figure extends complexity by using a 30 × 30 × 30 grid over [−3,3]3 and three
rotating, anisotropic ellipsoidal blobs:
σ ={(0.8,0.5,0.3), (0.6,0.4,0.7), (0.5,0.6,0.4)}.
blobs
• Blob1: Center(1.5cos(0.015t), 1.5sin(0.015t), 0.5sin(0.01t)),covariancediag(0.82, 0.52, 0.32)
rotated about z by 0.02t.
• Blob2: Center(−1.2cos(0.018t), 1.2sin(0.018t), −0.5cos(0.012t)),covariancediag(0.62, 0.42, 0.72)
rotated about x by 0.017t.
• Blob3: Center(0.5cos(0.02t), −0.5sin(0.02t), 1.5sin(0.015t)),covariancediag(0.52, 0.62, 0.42)
rotated about y by 0.013t.
Eight micro-obstacles of width σ = 0.3 drift via an AR(1) process with ϕ =
obs micro
0.85. Aspatio-temporalrandomFourierfield(sumof15sinusoidswithrandomwavevec-
tors k ∼ N(0,1.52I ), frequencies in [0.005,0.02], and random phases) is normalized to
3
[0,1] and scaled by 0.5. At each 0≤t<600, the total coupling a(x,t) is the sum of the
threeanisotropicGaussians,eightmicro-Gaussians(scaledby0.25),and0.5×therandom
Fourier field. We then compute
a(x,t)2 1
ρ (x,t) = , I(x,t) = − ln(1−ρ (x,t)2),
corr a(x,t)2+1.02 2 corr
normalize I(x,t) over the 303 grid to obtain ρ(x,t)∈[0,1].
The agent uses second-order dynamics (mass m = 1.0, damping γ = 0.6, time step
∆t=0.02) to chase a moving helix target
g(t)=(2cos(0.008t), 2sin(0.008t), 2−0.0015t).
Its perceived barrier strength is the average of ρ over a spherical neighborhood of radius
1.0(vianearest-neighboraveragingonthe303 grid)plusAR(1)perceptionnoise(ϕ =
perc
0.65, σ = 0.12). During 0 ≤ t < 350 (noisy phase), movement noise is AR(1) with
perc
ϕ = 0.75, σ = 0.7; for 350 ≤ t < 600 (deterministic phase), movement noise is
move move
removed.
Appendix B: Estimating MB Density via KSG Mutual
Information
This appendix explains how to estimate MB density from data using a practical method
basedonnearest-neighborstatistics. Theapproach, basedontheKSGestimator, letsus
compute mutual information directly from samples, without needing to guess the shape
of the underlying distributions [23, 24, 25, 32, 33, 34, 35, 36].
Motivation and Theoretical Framework
We define the MB density ρ(x) as the conditional mutual information:
ρ(x):=I(s :s |s =x)
int ext blanket
This quantity expresses the degree to which blanket states mediate information flow. A
lowρ(x)indicatesstrongcoupling(porousblanket),whileahighρ(x)indicatesstatistical
insulation. Estimating ρ(x) from samples requires non-parametric tools, for which we
adopt the KSG estimator.
45
The KSG Estimator
Given samples (x ,y ), the mutual information estimator is:
i i
N
Iˆ (X;Y)=ψ(k)+ψ(N)− 1 (cid:88) [ψ(n (i)+1)+ψ(n (i)+1)]
KSG N x y
i=1
where ψ(·) is the digamma function, k is the neighbor order, and n (i),n (i) count
x y
neighbors in the marginal spaces within joint-space neighborhoods.
To estimate conditional mutual information:
I(X;Y |Z)≈I(X;[Y,Z])−I(X;Z)
This can be computed by applying the KSG estimator to (X,[Y,Z]) and (X,Z).
Simulation Example
We simulate the following generative process:
s ∼U(0,1)
ext
s =sin(2πs )+η , η ∼N(0,0.05)
blanket ext 1 1
s =cos(2πs )+η , η ∼N(0,0.05)
int blanket 2 2
Thisstructureintroducesnonlinear,noise-perturbedcouplingbetweenthevariables. Us-
ing 300 samples and k =5, the estimated mutual information values were:
I(s ;s ,s )≈1.88 nats, I(s ;s )≈0.44 nats
int ext blanket int blanket
So the estimated Markov blanket density is:
ρ(x)=I(s :s |s )≈1.44 nats
int ext blanket
By constructing empirical spatial maps of ρ(x) across agent-environment interfaces,
onecancomputegradientsandsimulateagentdynamicsbasedonvariationalflows. These
flowsreflectmovementtowardregionsofmaximalinformationexchangeandsupportthe
main claim of this paper: that active inference policies emerge from navigating MB
density fields. Therefore, by developing a KSG-based algorithm to estimate ρ(x) from
sampled data, we open a research path for applying active inference to real sensory
trajectories—where one must learn the blanket density from streaming measurements.
One can now ask questions such as: How does estimation error in ρ(x) affect planning?
Whataretheoptimalsamplingstrategiestoreduceuncertaintyinblanketdensity? How
does noisy or partial observation of state-variables bias inference of ρ(x)?
Error Propagation from KSG to Free-Energy Descent (with p )
true
In this subsections, we developed an active-inference framework in which an agent navi-
gatesacontinuousstatespacebydescendingthevariationalfreeenergyF(x)=−lnp (s(x),η(x)).
true
Central to this dynamic is the MB density ρ(x), which the agent must estimate in real
timeviaaKSGmutual-informationestimator,yieldingρ (x). Becauseρ (x)inevitably
N N
differs from the true blanket density ρ (x), our goal here is to quantify precisely how
true
the resulting pointwise error ∆ρ (x) slows the agent’s descent of F(x). Concretely, we:
N
1. Bound ∆ρ (x) uniformly over Ω, showing it scales like N−1/(d+1) with high prob-
N
ability.
2. Translatethaterrorintoaperturbationoftheagent’svelocityx˙ =−[1−ρ(x)]∇ F(x),
x
yielding a local slowdown proportional to ∆ρ (x).
N
3. Integrate these local effects along a finite descent path to derive an asymptotic
bound on the total “convergence delay” T −T, proving it is O (N−1/(d+1)).
N p
46
TheseresultsmakeexplicithowKSGestimationaccuracy,samplesizeN,andambient
dimension d jointly determine the agent’s performance in minimizing free energy.
In this subsection, we incorporate the notation
p (s(x),η(x)) = the true joint density of sensory signals s(x) and latent states η(x),
true
which underlies the variational free energy
F(x) = −lnp (s(x),η(x)),
true
evaluated at each position x∈Ω. We also recall the notation
I (I(x);E(x)|B(x))
ρ (x) = 1 − true ,
true I (I(x);E(x))
true
which is the ground-truth MB density at x. Here:
• I (I(x);E(x)) is the true mutual information between internal variables I(x)
true
(within a ball of radius r around x) and external variables E(x) (outside a ball of
1
radius r ).
2
• I (I(x);E(x)|B(x))isthetrueconditionalmutualinformationwhencondition-
true
ing on blanket variables B(x) in the shell r ≤∥y−x∥<r .
1 2
We estimate ρ (x) via the KSG-estimator from N samples, defining
true
I(cid:98)(I(x);E(x)|B(x))
ρ (x) = 1 − .
N
I(cid:98)(I(x);E(x))
Our goal is to quantify how the pointwise estimation error ∆ρ (x) = ρ (x)−ρ (x)
N N true
propagates through the active-inference dynamics
x˙ = −[1−ρ(x)]∇ F(x),
x
andinparticularhowitdelaysthetimetodescendtoagivenfree-energylevel. Through-
out, we assume:
(i) Regularity of F. F(x) = −lnp (s(x),η(x)) is C2 on Ω, and there exist con-
true
stants 0<m≤L such that
F
m ≤ ∥∇ F(x)∥≤ L for all x∈Ω
x F
(away from any isolated critical points).
(ii) Regularity of ρ . ρ ∈ C1(Ω) with ∥∇ρ (x)∥ ≤ L uniformly, and 0 ≤
true true true ρ
ρ (x)≤1−δ on the regions traversed by the agent (for some δ >0).
true
(iii) KSG estimation error. Forradiir (N), r (N)∼N−1/(d+1),theKSGestimator
1 2
of each mutual-information term satisfies, uniformly on Ω,
1 1
− −
|I(cid:98)(I;E)(x)−I
true
(I;E)(x)|=O
p
(N d+1), |I(cid:98)(I;E |B)(x)−I
true
(I;E |B)(x)|=O
p
(N d+1).
Under these conditions, we prove:
Lemma 20 (Uniform Consistency of ρ ). If r (N),r (N) ≈ cN−1/(d+1), then there
N 1 2
exists a constant C >0 such that, with probability tending to 1,
1
sup|ρ N (x)−ρ true (x)| ≤ C 1 N− d+ 1 1.
x∈Ω
47
Proof Sketch. Write
ρ (x)=1−
I(cid:98)c (x)
, ρ (x)=1−
I
c
(x)
,
N I(cid:98)u (x) true I u (x)
whereI
u
(x)=I
true
(I(x);E(x)),I
c
(x)=I
true
(I(x);E(x)|B(x)),I(cid:98)u (x)=I(cid:98)(I(x);E(x)),I(cid:98)c (x)=
I(cid:98)(I(x);E(x)|B(x)). Then
ρ (x)−ρ (x)=
I
c
(x)[I(cid:98)u (x)−I
u
(x)]
−
I(cid:98)c (x)−I
c
(x)
.
N true
I
u
(x)I(cid:98)u (x) I(cid:98)u (x)
Since I
u
(x)≥c
I
>0 and I(cid:98)u (x)→I
u
(x) uniformly, both denominators remain bounded
below. Hence
1
|ρ
N
(x)−ρ
true
(x)|≤
|I
c
c
(
2
x)|
|I(cid:98)u (x)−I
u
(x)| +
|I(cid:98)c (x)
c
−I
c
(x)|
+ o
p
(N
−
d+1 ).
I I
By assumption each numerator is O (N−1/(d+1)) uniformly on Ω, so sup |ρ (x) −
p x N
ρ (x)|=O (N−1/(d+1)).
true p
Lemma 21 (Local Velocity Perturbation). Fix a point x ∈ Ω with ∥∇ F(x)∥≥ m > 0.
x
Define the “ideal” velocity
x˙ = −[1−ρ (x)]∇ F(x),
true true x
and the “noisy” velocity
x˙ = −[1−ρ (x)]∇ F(x).
N N x
Then
∥x˙ −x˙ ∥=|ρ (x)−ρ (x)|∥∇ F(x)∥.
N true N true x
In particular, if sup |ρ (x)−ρ (x)|≤ε , then
x N true N
∥x˙ −x˙ ∥≤ L ε , ∥x˙ −x˙ ∥≥ m|ρ (x)−ρ (x)|.
N true F N N true N true
Proof. Subtracting gives
x˙ −x˙ =−[1−ρ (x)]∇ F(x)+[1−ρ (x)]∇ F(x)=−[ρ (x)−ρ (x)]∇ F(x).
N true N x true x N true x
Taking norms yields the desired bounds.
Lemma 22 (Convergence Delay over a Finite Curve). Let x (t) solve
true
x˙ =−[1−ρ (x)]∇ F(x), x (0)=x ,
true true x true 0
and let x (t) solve
N
x˙ =−[1−ρ (x)]∇ F(x), x (0)=x .
N N x N 0
Suppose that, over t ∈ [0,T], the ideal trajectory x (t) travels a curve of length ℓ and
true
satisfies ∥∇ F(x)∥≥m>0. If
x
sup|ρ (x)−ρ (x)|≤ε , ρ (x)≤1−δ (δ >0) along that region,
N true N N
x∈Ω
then there exists C >0, depending on m,L ,δ,ℓ, such that for sufficiently large N,
2 F
|T −T| ≤ C ε .
N 2 N
1
−
Hence T −T =O (N d+1 ).
N p
48
Proof Sketch. (1) Ideal descent rate. For t∈[0,T],
d
F(x (t))=−[1−ρ (x)]∥∇ F(x)∥2 ≤ −βm2,
dt true true x
where β =inf (1−ρ (x))>0. Thus F(x ) decreases at least at rate βm2.
x∈Ω true true
(2) Noisy descent rate. Similarly,
d
F(x (t))=−[1−ρ (x)]∥∇ F(x)∥2=−[1−ρ (x)]∥∇ F(x)∥2−[ρ (x)−ρ (x)]∥∇ F(x)∥2.
dt N N x true x N true x
Hence
(cid:12)d d (cid:12)
(cid:12) F(x )− F(x )(cid:12)=|ρ (x)−ρ (x)|∥∇ F(x)∥2 ≤ L2 ε .
(cid:12)dt N dt true (cid:12) N true x F N
(3) Gap at time T. Let α=F(x (T)). Define
true
h(t)=F(x (t))−F(x (t)), h(0)=0.
N true
Then
dh d d
= F(x )− F(x ) ≤ L2 ε .
dt dt N dt true F N
Integrating from 0 to T yields
F(x (T))−α=h(T) ≤ T L2 ε .
N F N
Therefore F(x (T))≤α+T L2 ε .
N F N
(4) Extra time to reach α. At t=T, F(x (T)) exceeds α by at most T L2 ε . Since
N F N
along x ,
N
(cid:12)d (cid:12)
(cid:12) F(x )(cid:12)=[1−ρ (x)]∥∇ F(x)∥2 ≥ δm2,
(cid:12)dt N (cid:12) N x
it takes at most
T L2 ε
∆T ≤ F N = C ε
δm2 2 N
additionaltimeforF(x )todropfromα+T L2 ε toα. Hence|T −T|≤C ε .
N F N N 2 N
(5) Scaling of ε . From Lemma 20, ε =O (N−1/(d+1)). Consequently,
N N p
1
−
T −T =O (N d+1 ).
N p
Asymptotic Conclusion. From Lemmas 20–22, we obtain a high-probability bound
on the convergence delay:
1
(cid:16) − (cid:17)
|T −T| = O N d+1 .
N p
• Dependence on d. The rate N−1/(d+1) reflects the curse of dimensionality: more
samples are needed in higher d to achieve the same ε and delay T −T.
N N
• Role of r ,r . Choosing r (N) ∝ N−1/(d+1) balances bias and variance in KSG
1 2 i
estimates; deviating worsens ε and lengthens T −T.
N N
• Uniformity on Ω. Since ε is uniform over Ω, no path can bypass this bound,
N
provided ∥∇ F∥≥m>0 and ρ (x)≤1−δ along it.
x N
• High-probability vs. expectation. All bounds hold with probability →1. With mild
moment conditions, E[|T −T|]=O(N−1/(d+1)) follows.
N
49
In summary, any local estimation error ρ (x)−ρ (x) of order ε induces a slow-
N true N
down in x˙ of order O(ε ), leading to a total convergence delay of order O(ε ). Since
N N
ε = O (N−1/(d+1)), we conclude that an agent must collect N ∼ ϵ−(d+1) samples to
N p
bound its convergence delay by ϵ > 0. This quantifies precisely how KSG estimation
accuracy, sample size, and dimensionality jointly determine the agent’s performance in
free-energy minimization.
Figure 9: Simulation of Conditional Dependencies Mediated by a Markov Blanket.
Synthetic generative structure: (left) external input s ; (center) mediated blanket variable
ext
s ; (right) internal response s . This figure illustrates the informational structure of
blanket int
a synthetic agent-environment system composed of three variables: an external state s , a
ext
blanketstates ,andaninternalstates . Theleftpanelshowsthemappingfroms to
blanket int ext
s , which is generated by a sinusoidal function with added noise. The structured, curved
blanket
distribution reflects a strong but noisy dependence between external and blanket states. The
middle panel displays the relationship between s and s , also nonlinear and structured,
blanket int
indicating that internal states are tightly coupled to the blanket dynamics. The right panel
showsthedirectrelationshipbetweens ands ,whichappearsmorediffuse. Althoughsome
ext int
dependencyremains,thestructureissignificantlyweaker,becausetheinternalstateisinfluenced
bytheexternalstateonlyindirectlythroughtheblanket. Together,theseplotsdemonstratethe
mediating role of the blanket state in shaping the flow of information from the external to the
internal system. This functional mediation is the defining property of a Markov blanket. The
figure supports the idea that this mediation can vary in strength across space, and that such
variationcanbeformallyquantifiedasMBdensity. UsingestimatorssuchastheKSGmethod,
this density can be empirically estimated from data, allowing for simulation and validation of
the theoretical framework presented in the main text.
Appendix D: Mathematical Background
Overview
In this appendix, we collect and summarize the principal mathematical concepts, defi-
nitions, and results that underpin the main text. The goal is to provide a concise but
self-contained exposition of the background material required to follow the formal argu-
ments and proofs in this paper. We assume that the reader is familiar with basic real
analysis and elementary probability theory; we then introduce, in turn:
• The notions of entropy, mutual information, and conditional mutual information
in both the discrete and continuous settings.
• ThenonparametricestimationofmutualinformationviatheKraskov–Stögbauer–Grassberger
(KSG) k-nearest-neighbors (kNN) estimator.
• The concept of convergence in the norm C1(K) for functions defined on compact
subsets K ⊂Rn.
• The theory of gradient flows and ordinary differential equations (ODEs) of the
formx˙(t)=−g(x(t))∇F(x(t)),includingexistence,uniqueness,andbasicstability
estimates.
• Lipschitz continuity, differentiability classes (Ck), and regularity properties for
functions on Euclidean spaces.
50
• Basic ideas from the theory of random fields or stochastic processes indexed by
space, including covariance functions, stationarity, and concentration inequalities
(in particular Hoeffding’s inequality).
• Notions from geometric measure theory regarding compact domains with smooth
boundary, volume (Lebesgue measure), and balls Ball(x;r) in Rn.
Throughout, we adopt the following notational conventions:
• Rn denotes n–dimensional Euclidean space, with the standard Euclidean norm
∥x∥= (cid:112) x2+···+x2.
1 n
• Ω⊂Rn will denote a compact set with C2 boundary, or more generally a domain
(open connected set) whose closure Ω is compact.
• Given a probability density p(x) on Rn, H(p) denotes its (differential) entropy,
and I(X;Y) denotes mutual information between random variables X,Y (possibly
vector-valued).
• For an open set U ⊂Rn, Ck(U) is the space of k–times continuously differentiable
real-valued functions on U, and ∥f∥ denotes the C1—norm on a compact
C1(K)
K ⊂U.
• For random variables indexed by points in space (a “random field”), we often write
ρ(x,θ) where θ is a point in some probability space (Θ,F,P).
Entropy and Mutual Information
Discrete Entropy
Let X be a discrete random variable taking values in a finite or countable set X, with
probability mass function (pmf) p (x)=P(X =x). The Shannon entropy of X is
X
(cid:88)
H(X) = − p (x) lnp (x),
X X
x∈X
where throughout ln denotes the natural logarithm. Entropy H(X) measures the ex-
pected “surprisal” of X and satisfies 0≤H(X)≤ln|X| when |X|<∞.
Differential Entropy
When X is a continuous random vector in Rd with probability density function (pdf)
p (x), its differential entropy is defined as
X
(cid:90)
h(X) = − p (x) lnp (x)dx,
X X
Rd
provided the integral exists (i.e., p is absolutely continuous and (cid:82) p |lnp |< ∞).
X X X
Unlike discrete entropy, differential entropy can be negative and is not invariant under
change of variable.
Mutual Information
Giventworandomvariables(orvectors)X andY,withjointdistributionp (x,y)and
X,Y
marginals p (x), p (y), the mutual information between X and Y is defined by
X Y
(cid:90) (cid:16) p (x,y) (cid:17)
I(X;Y) = p (x,y) ln X,Y dxdy
Rd×Rd′ X,Y p X (x)p Y (y)
in the continuous case, or the analogous sum in the discrete case. Equivalently, in the
discrete setting:
I(X;Y) = H(X) + H(Y) − H(X,Y),
51
and in the continuous setting:
I(X;Y) = h(X) + h(Y) − h(X,Y).
Mutualinformationisalwaysnonnegative, i.e.I(X;Y)≥0, andvanishespreciselywhen
X and Y are independent. It can also be written as the Kullback–Leibler divergence
I(X;Y) = D (p ∥p ⊗p ).
KL X,Y X Y
Conditional Mutual Information
For three random variables (vectors) X, Y, and Z, the conditional mutual information
of X and Y given Z is
(cid:104) (cid:105)
I(X;Y |Z) = E D (p ∥p ⊗p ) .
Z KL X,Y|Z X|Z Y|Z
Equivalently, in terms of (differential) entropies:
I(X;Y |Z) = H(X |Z)+H(Y |Z)−H(X,Y |Z),
or in the continuous case
I(X;Y |Z) = h(X |Z)+h(Y |Z)−h(X,Y |Z).
An equivalent formula in continuous form is
(cid:90) (cid:90) (cid:90) p (x,y|z)
X,Y|Z
I(X;Y |Z)= p (x,y,z) ln dxdydz.
X,Y,Z p (x|z)p (y|z)
X|Z Y|Z
ConditionalmutualinformationmeasurestheresidualstatisticaldependencebetweenX
and Y once Z is known. In our context, I (internal states), B (blanket) and E (external
states) play the roles of X, Z, and Y respectively.
Normalized “Blanket Strength”
In the paper, the Markov blanket strength is defined at a point x by
I(I;E | B)
S(x) = 1 − ,
I(I;E)
and the associated MB density by ρ(x)=S(x). Here I(I;E) and I(I;E |B) denote the
marginal and conditional mutual information restricted to the subsets I(x), B(x), and
E(x) around x. One must therefore be fluent in all of the foregoing definitions.
Nonparametric Estimation of Mutual Information via KSG–kNN
Nearest-Neighbor Distances and Entropy Estimation
Given a sample {z }N ⊂Rd, consider the distance to the k-th nearest neighbor:
i i=1
ε (i) = min{r >0: |{j ̸=i:∥z −z ∥≤r}|≥k}.
k j i
TheclassicalKozachenko–Leonenko (KL)estimatorforthe(differential)entropyh(Z)is
N
hˆ (Z)=ψ(N)−ψ(k)+ln(c )+ d (cid:88) lnε (i),
KL d N k
i=1
where ψ(·) is the digamma function, c =πd/2/Γ(d+1) is the volume of the unit ball in
d 2
Rd, and ε (i) is half the distance to the k-th nearest neighbor when using the maximum
k
norm (or Euclidean norm if appropriate correction is made).
52
Kraskov–Stögbauer–Grassberger (KSG) Estimator
Kraskov, Stögbauer, and Grassberger (2004) generalized the KL estimator to estimate
mutual information between two continuous random vectors X ∈ Rdx and Y ∈ Rdy .
Given samples {(x ,y )}N , for each i define
i i i=1
ε (i)=min{max{∥x −x ∥ , ∥y −y ∥ }:1≤j ≤N, j ̸=i, rank(j)=k},
k j i ∞ j i ∞
i.e. the distance (in the maximum norm) to the k-th nearest neighbor in the joint space
Rdx+dy . Then count
n (i)=|{j ̸=i:∥x −x ∥ ≤ε (i)}|, n (i)=|{j ̸=i:∥y −y ∥ ≤ε (i)}|.
x j i ∞ k y j i ∞ k
The KSG estimator for mutual information is
N
Iˆ (X;Y) = ψ(k)− 1 +ψ(N)− 1 (cid:88) [ψ(n (i)+1)+ψ(n (i)+1)],
KSG k N x y
i=1
where ψ is again the digamma function. Under mild regularity conditions on the joint
density p , this estimator is (asymptotically) unbiased and consistent for large N. An
X,Y
analogousprocedurecanbeappliedtoestimateconditionalmutualinformationI(X;Y |
Z) by conditioning on Z in a similar nearest-neighbor scheme.
Convergence Properties and Conditions
ToemployKSG–kNNestimationwithinatheoreticalanalysis,oneoftenneedsmorethan
mere pointwise consistency Iˆ− p →I . In the paper’s arguments, the key requirement is
true
convergence in the norm
∥Iˆ(·)−I (·)∥ = O (N−α),
true C1(K) p
for some α>0 and any compact K in the domain. Convergence in C1(K) means:
1. sup |Iˆ(x)−I (x)|=O (N−α),
x∈K true p
2. sup ∥∇Iˆ(x)−∇I (x)∥=O (N−α),
x∈K true p
where∇denotesthegradientwithrespecttothespatialcoordinatex∈Rn. Establishing
such rates typically requires:
• Assumptions that the true densities are bounded away from zero and infinity on
K, with Lipschitz (or Hölder) continuous derivatives.
• Control of the bias and variance of the kNN–KSG estimator and uniformity over
x∈K.
• Strong concentration inequalities for the nearest-neighbor distances and counts
n (i), n (i).
x y
Convergence in the Norm C1(K)
Function Spaces of Class C1
Let U ⊂Rn be an open set and K ⊂U a compact subset. We say a function f :U →R
belongstoC1(U)ifitiscontinuouslydifferentiable,i.e.,allfirstpartialderivatives∂f/∂x
i
exist and are continuous on U. The restriction f| is then in C1(K) in the sense that f
K
and its gradient ∇f are continuous on the compact set K.
Definition of the C1–Norm
For f ∈C1(U) and a compact K ⊂U, define
∥f∥ = sup|f(x)| + sup∥∇f(x)∥.
C1(K)
x∈K x∈K
If ∥f −g∥ → 0 as some parameter (e.g., sample size N) grows, we say f converges
C1(K)
to g in the C1(K) norm. This implies uniform convergence of both f and ∇f on K.
53
Implications for Gradient-Based Dynamics
Convergence in C1(K) is crucial when one studies ODEs of the form
x˙(t)=−[1−ρ (x(t))]∇F(x(t)),
N
when ρ (x) → ρ (x) in C1(K). Under such convergence, one can pass to the limit
N true
in the vector fields and deduce that solutions to the “estimated” flow approach those of
the “true” flow, provided standard conditions of Lipschitz continuity hold. In particular,
if ρ → ρ and ∇ρ → ∇ρ uniformly on K, then the direction of descent −[1−
N true N true
ρ ]∇F converges uniformly to −[1−ρ ]∇F. This is one of the stepping stones in the
N true
proof of Theorem 1.
Gradient Flows and Ordinary Differential Equations
Gradient Descent in Rn
Given a continuously differentiable function F : Ω ⊂ Rn → R, the gradient flow is the
ODE
x˙(t)=−∇F(x(t)),
withinitialconditionx(0)=x ∈Ω. Underthestandardassumptionthat∇F isglobally
0
Lipschitz (or at least locally Lipschitz on Ω), the Picard–Lindelöf theorem guarantees
the existence and uniqueness of a solution defined on a maximal interval. Moreover, if
Ω is compact and ∇F is continuous, then the flow exists for all t ≥ 0 and F(x(t)) is
d
nonincreasing (since F(x(t))=∇F(x)·x˙ =−∥∇F(x)∥2≤0).
dt
Modified Gradient Flow with Mobility Function
In the paper, the dynamics are modified as
x˙(t)=−M(x(t))∇F(x(t)),
where the mobility (or “coupling”) function is
M(x)=1−ρ(x), 0≤ρ(x)≤1.
Hence,
F˙(x(t)) = ∇F(x)·x˙ = −[1−ρ(x)]∥∇F(x)∥2 ≤ 0.
This shows that F(x(t)) is nonincreasing along trajectories. If ρ(x)=1 at some x, then
M(x)=0 and x˙ =0, so the flow is frozen at that point.
Existence and Uniqueness under Lipschitz Conditions
SupposeF ∈C2(Ω),so∇F isLipschitzcontinuousonanycompactK ⊂ΩwithLipschitz
constant L . If, in addition, ρ(x) is C1 on K, then M(x)=1−ρ(x) is also Lipschitz on
F
K. Consequently, the vector field v(x)=−M(x)∇F(x) satisfies
∥v(x)−v(y)∥= ∥M(x)∇F(x)−M(y)∇F(y)∥ ≤ ∥M(x)(∇F(x)−∇F(y))∥+∥∇F(y)∥|M(x)−M(y)|.
Since M and ∇F are each bounded and Lipschitz on K, v(x) is Lipschitz. Hence by the
Picard–Lindelöf theorem, for each x ∈ K there is a unique solution x(t) ∈ K for some
0
maximal interval of existence. If K = Ω is compact and v does not push trajectories
outsideΩ(e.g.,vistangentattheboundary),thesolutionexistsforallt≥0andremains
in Ω.
Lipschitz Continuity and Differentiability Classes
Ck Function Spaces
Let U ⊂Rn be open. We denote by Ck(U) the set of functions f :U →R whose partial
derivatives up to order k exist and are continuous on U. In particular:
54
• C0(U) is the set of continuous functions.
• C1(U) consists of continuously differentiable functions; i.e., all first partials exist
and are continuous.
• C2(U) consists of twice continuously differentiable functions, etc.
If Ω is compact with C2 boundary, and F ∈C2(Ω), then ∇F is Lipschitz on Ω (since a
continuouslydifferentiablemaponacompactsetisautomaticallyLipschitz). Specifically,
there exists L >0 such that
F
∥∇F(x)−∇F(y)∥≤ L ∥x−y∥ ∀x,y ∈Ω.
F
Lipschitz Continuity
A function f : K → R defined on a metric space (K,d) is Lipschitz continuous if there
exists a constant L≥0 such that
|f(x)−f(y)|≤ Ld(x,y) ∀x,y ∈K.
Iff ∈C1(K)foracompactK ⊂Rn,thenthemeanvaluetheoremimpliesf isLipschitz
with Lipschitz constant
L = sup∥∇f(x)∥.
f
x∈K
Analogously, avectorfieldv :K →Rn isLipschitzifsup ∥Dv(x)∥<∞, whereDv(x)
x∈K
is the Jacobian matrix and ∥·∥ is the operator norm.
Random Fields and Concentration Inequalities
Random Fields on a Compact Domain
A random field on a compact set Ω⊂Rn is a collection of real-valued random variables
{ρ(x)} defined on a common probability space (Θ,F,P). We denote ρ(x,θ) when
x∈Ω
emphasizing the dependence on the random element θ ∈ Θ. Conditions often imposed
on ρ(x,θ) in the paper include:
• Boundedness: 0≤ρ(x,θ)≤1 for all x,θ.
• Stationarity of mean: E [ρ(x,θ)]=µ is constant for all x∈Ω.
θ
• Constant covariance with a deterministic field: Cov(ρ(x), ∥∇F(x)∥2)=C is inde-
pendent of x.
• Decay of spatial correlations: There exists a length-scale ℓ>0 such that
|Cov(ρ(x), ρ(y))| ≤ σ2exp(−∥x−y∥/ℓ) ∀x,y ∈Ω.
The latter condition is a form of exponential mixing or exponential decay of correlations
and ensures that values of ρ at distant points become nearly independent.
Hoeffding’s Inequality for Bounded Random Variables
Suppose Z ,...,Z are independent random variables with a ≤ Z ≤ b almost surely.
1 N i i i
Then for any ε>0,
(cid:16) 1 (cid:88) N (cid:17) (cid:16) 2N2ε2 (cid:17)
P | Z − E[Z ]|≥ε ≤ 2exp − .
N i i (cid:80)N (b −a )2
i=1 i=1 i i
In the paper, to derive a uniform high-probability bound on
φ(x)=[1−ρ(x)]∥∇F(x)∥2
55
over all x ∈ Ω, one discretizes Ω by a finite grid {x(1),...,x(N)} of mesh δ. Then
Hoeffding’s inequality yields, for each grid point x(j),
(cid:16) (cid:17)
P |φ(x(j))−E[φ(x(j))]|≥ε ≤ 2exp(−Cε2)
for some constant C > 0 if φ is bounded (since 0 ≤ φ ≤ ∥∇F∥2 ). A union bound over
∞
all grid points (and then controlling the remainder of Ω by Lipschitz continuity) yields
(cid:16) (cid:17)
P sup|φ(x)−E[φ(x)]|≥ε ≤ N ·2exp(−Cε2),
x∈Ω
so that with high probability the random field φ(x) is uniformly close to its expectation.
Compact Domains and Volume in Rn
Compact Sets with Smooth Boundary
Let Ω ⊂ Rn be a bounded open set whose boundary ∂Ω is a C2 hypersurface. Such an
Ω is said to be a compact domain with C2 boundary if Ω is compact and ∂Ω is a C2
manifold. In particular:
• Thereexistcoordinatecharts(U ,ψ )covering∂Ωsuchthatψ (U )isopeninRn−1
i i i i
and ∂Ω is locally given by x =ϕ (x ,...,x ) for some C2 function ϕ .
n i 1 n−1 i
• Ω satisfies an interior sphere condition: every point on ∂Ω has a ball of positive
radius contained in Ω tangent to ∂Ω at that point.
These properties guarantee that standard PDE and ODE results (e.g., existence of flows
that remain in Ω with vector fields tangent at the boundary) apply.
Volume of Balls
The n-dimensional Lebesgue measure (volume) of a ball of radius r >0 in Rn is
Vol(Ball(x;r))=Vol(Ball(0;r))=c rn,
n
where
πn/2
c = .
n Γ(n +1)
2
InmanyconsistencyargumentsforkNNestimators,onerequiresthatNVol(Ball(x;r (N)))→
2
∞asN →∞,whichensuresthat,onaverage,thereareinfinitelymanysamplepointsin
an r (N)-neighborhood of any given x. Usually r (N) is chosen so that Nr (N)n →∞
2 2 2
but r (N)→0.
2
Gradient Alignment and Monotonicity Conditions
Gradient Conditions for Theorem 1
In Theorem 1, one assumes that there exist continuous functions
I (I(x);E(x)), I (I(x);E(x)|B(x)) : D ⊂Ω −→ R,
true true
which are C1 on an open set D, and satisfy
∇[I (I(x);E(x)_]()
true
Acknowledgements
I thank Kobus and Stefan Esterhuysen for their assistance in developing the technical
aspects of this paper.
56
References
[1] Friston, K., A Free energy Principle for a Particular Physics, arXiv preprint
arXiv:1906.10184, 2019.
[2] Friston, K., “The Free-energy Principle: A Rough Guide to the Brain?” Trends
in Cognitive Sciences 13(7): 293–301. 2009.
[3] Friston,K.,“LifeasWeKnowIt.” Journal of the Royal Society Interface 10(86):
20130475. 2013.
[4] Ramstead, M., Badcock, P., Friston, K., “Answering Schrödinger’s Question: A
Free Energy Formulation.” Physics of Life Reviews (24):1–16. 2018.
[5] Kirchhoff, M., T. Parr, E. Palacios, K. Friston, Kiverstein, J., “The Markov
Blankets of Life.” Journal of the Royal Society Interface 15:20170792. 2018.
[6] Ramstead,M.,Sakthivadivel,D.,Heins,C.,Koudahl,M.,Millidge,B.,DaCosta,
L.,Klein,B.,Friston,K.,“OnBayesianMechanics.” InterfaceFocus 13:20220029.
2023.
[7] Beck, J., Ramstead, M. "Dynamic Markov Blanket Detection for Macroscopic
Physics Discovery" arXiv:2502.21217 [q-bio.NC] 2025.
[8] Sakthivadivel, D. “Weak Markov Blankets in High-Dimensional, Sparsely-
Coupled Random Dynamical Systems” arXiv:2207.07620 2025
[9] Parr, T., Pezzulo, G., Friston, K. Active Inference. The Free Energy Principle
in Mind, Brain, and Behavior, MIT Press. 2022.
[10] Amari, S. Information Geometry and Its Applications, Springer. 2016.
[11] Cover, T. M., Thomas, J. A.Elements of Information Theory (2nd ed.), Wiley.
2006.
[12] Fleming, W. H., Rishel, R. W.Deterministic and Stochastic Optimal Control,
Springer. 1975.
[13] Bertsekas, D. P. Nonlinear Programming (2nd ed.), Athena. 1999.
[14] Jaynes, E. T. "Information theory and statistical mechanics" Physical Review,
106(4), 620.
[15] Amari,S."Informationgeometryonhierarchyofprobabilitydistributions"IEEE
Transactions on Information Theory, 47(5), 1701–1711. 2001.
[16] Pearl, J. Causality: Models, Reasoning and Inference, Cambridge. 2009.
[17] Crroks, G. "Entropy production fluctuation theorem and the nonequilibrium
work relation for free energy differences" Physical Review E, 60(3), 2721. 1999.
[18] Amari, S. "Natural gradient works efficiently in learning" Neural Computation,
10(2), 251–276. 1998.
[19] Li, W., Montúfar, G. "Natural gradient via optimal transport" Information Ge-
ometry, 1(2), 181–214. 2018.
[20] Parr, T., Friston, K. "Generalised free energy and active inference." Biological
Cybernetics, 113(5), 495–513. 2019.
[21] Kraskov, A., Stögbauer, H., Grassberger, P. "Estimating mutual information."
Physical Review, 69(6), 066138. 2004.
[22] Smith, R., Friston, K., Whyte, C. “A Step-by-Step Tutorial on Active Inference
and Its Application to Empirical Data.” Journal of Mathematical Psychology,
107: 102632. 2022.
57
[23] Paninski, L. "Estimationof entropyand mutual information." Neural Computa-
tion, 15(6), 1191–1253. 2003.
[24] Gao, W., Oh, S., Viswanath, P. "Demystifying information-theoretic estima-
tors." arXiv:1708.00065. 2017.
[25] Lizier, J. T. "JIDT: An information-theoretic toolkit for studying the dynamics
of complex systems." Frontiers in Robotics and AI, 1, 11. 2014.
[26] Da Costa, L., Friston, K., Heins, C., Pavliotis, G.A. "Bayesian mechanics for
stationary processes." Proceedings Royal Society, A 477: 20210518. 2021.
[27] Parr, T., Da Costa, L., Friston, K. "Markov blankets, information geometry
and stochastic thermodynamics." Philosophical Transactions Royal Society A,
378:20190159. 2019.
[28] Veissière S., Constant A., Ramstead M., Friston K., Kirmayer L. “Thinking
through Other Minds.” Behavioral and Brain Sciences, (43): 1–75. 2020.
[29] Gibson, J. J. The ecological approach to visual perception (1st ed). Mifflin and
Company, 1979.
[30] Badcock,P.,Davey,C.,Whittle,S.,Allen,N.,Friston,K."TheDepressedBrain:
An Evolutionary Systems Theory." Trends in Cognitive Science, (21): 182-194.
2017.
[31] Friston K, FitzGerald T, Rigoli F, Schwartenbeck P, Pezzulo G. "Active Infer-
ence: A Process Theory." Neural Computations, 29(1):1-49. 2017.
[32] Frenzel, S., Pompe, B. “Partial mutual information for coupling analysis of mul-
tivariate time series.” Physical Review Letters, 99:204101. 2007.
[33] Gao, W., Kannan, S., Oh, S., Viswanath, P. “Estimating mutual information
for discrete–continuous mixtures.” Advances in Neural Information Processing
Systems, 30:—. 2017.
[34] Hahsler,M.,Hornik,K.“TSP—Infrastructureforthetravelingsalespersonprob-
lem.” Journal of Statistical Software, 23:1–21. 2007.
[35] Darbellay, G. A., Vajda, I. “Estimation of the information by an adaptive par-
titioning of the observation space.” IEEE Transactions on Information Theory,
45:1315–1321. 1999.
[36] Singh, S., Poczos, B., Wasserman, L. “Exponential concentration of a den-
sity functional estimator.” Neural Information Processing Letters and Reviews,
9:123–132. 2016.
[37] Clark, A. Surfing Uncertainty: Prediction, Action, and the Embodied Mind, Ox-
ford University Press. 2016.
[38] Bruineberg J, Dołęga K, Dewhurst J, Baltieri M. "The Emperor’s New Markov
Blankets." Behavioral and Brain Sciences", 45:e183. 2022.
[39] Colombo, M. "Nothing but a useful tool? (F)utility and the free energy princi-
ple." In Bruineberg J, Dołęga K, Dewhurst J, Baltieri M. "The Emperor’s New
Markov Blankets." Behavioral and Brain Sciences", 45:e183. 2022.
[40] Friston, K, Da Costa, L., Sakthivadivel, D., Heins, C., Pavliotis, G., Ramstead,
M., Parr, T. "Path Integrals, Particular Kinds, and Strange Things." Physics of
Life Reviews", 47, 2023.
58

=== REVISE TO ===
PROFESSIONAL TONE: Begin directly with content - NO conversational openings like 'Okay, here's...'

1. Fix all issues above
2. Title: "Markov Blanket Density and Free Energy Minimization"
3. Include 10-15 quotes from paper text
   - Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
   - Use consistent quote formatting: 'The authors state: "quote"' or vary attribution phrases
   - Vary attribution phrases to avoid repetition
   - CRITICAL: Only extract quotes that actually appear in the paper text
4. ELIMINATE ALL REPETITION - each sentence must be unique
   - Check before each sentence: 'Have I already said this?' If yes, write something new
   - Vary attribution phrases - do NOT repeat 'The authors state' multiple times
5. Extract methodology, results with numbers, key quotes
6. 1000-1500 words, structured with ### headers

Generate COMPLETE revised summary.