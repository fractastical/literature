=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Active Inference for Stochastic Control
Citation Key: paul2021active
Authors: Aswin Paul, Noor Sajid, Manoj Gopalkrishnan

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2021

Key Terms: monashuniversity, mumbai, australia, active, india, optimal, inference, control, vixra, stochastic

=== FULL PAPER TEXT ===

1202
guA
72
]GL.sc[
1v54221.8012:viXra
Active Inference for Stochastic Control
AswinPaul1,2,3,NoorSajid4,ManojGopalkrishnan2,andAdeelRazi3,4,5,6
1 IITB-MonashResearchAcademy,Mumbai,India
2 DepartmentofElectricalEngineering,IITBombay,Mumbai,India
3 TurnerInstituteforBrainandMentalHealth,MonashUniversity,Australia
4 WellcomeTrustCentreforHumanNeuroimaging,UCL,UnitedKingdom
5 MonashBiomedicalImaging,MonashUniversity,Australia
6 CIFARAzrieliGlobalScholarsProgram,CIFAR,Toronto,Canada
Abstract. Active inference has emerged as an alternative approach to control
problems given itsintuitive (probabilistic) formalism. However, despite itsthe-
oretical utility, computational implementations have largely been restricted to
low-dimensional,deterministicsettings.Thispaperhighlightsthatthisisaconse-
quenceoftheinabilitytoadequatelymodelstochastictransitiondynamics,partic-
ularlywhenanextensivepolicy(i.e.,actiontrajectory)spacemustbeevaluated
duringplanning.Fortunately,recentadvancementsproposeamodifiedplanning
algorithmforfinitetemporalhorizons.Webuilduponthisworktoassesstheutil-
ityofactiveinferenceforastochasticcontrol setting.Forthis,wesimulatethe
classicwindygrid-worldtaskwithadditionalcomplexities,namely:1)environ-
mentstochasticity;2)learningoftransitiondynamics;and3)partialobservabil-
ity.Ourresultsdemonstratetheadvantageofusingactiveinference,comparedto
reinforcementlearning,inbothdeterministicandstochasticsettings.
Keywords: Active inference · Optimal control · Stochastic control · Sophisti-
catedinference
1 Introduction
Active inference, a corollary of the free energy principle, is a formal way of describ-
ingthebehaviourofself-organisingsystemsthatinterfacewiththeexternalworldand
maintainaconsistentformovertime[1,2,3].Despiteitsrootsinneuroscience,activein-
ferencehassnowballedtomanyfieldsowingtoitsambitiousscopeasageneraltheory
ofbehaviour[4,5,6].Optimalcontrolisonesuchfield,andseveralrecentresultsplace
active inference as a promising optimal control algorithm [7,8,9]. However, research
in the area has largely been restricted to low-dimensional and deterministic settings
where defining, and evaluating, policies (i.e., action trajectories) is feasible [9]. This
follows from the active inference process theory that necessitates equipping agents a
priori with sequences of actions in time. For example, with 8 available actions and a
time-horizonof15,thetotalnumberof(definable)policiesthatwouldneedtobecon-
sidered→3.5×1013.
This becomes more of a challenge in stochastic environmentswith inherently un-
certain transition dynamics, and no clear way to constrain the large policy space to a
2 A.Pauletal.
smaller subspace. Happily,recentadvancementslike sophisticatedinference[10] pro-
posea modifiedplanningapproachforfinite-temporalhorizons[11]. Briefly,sophisti-
cated inference [10], compared to the earlier formulation[12,9], providesa recursive
formoftheexpectedfreeenergythatimplementsadeeptreesearchoveractions(and
outcomes)inthefuture.WereservefurtherdetailsforSection3.2.
In this paper, we evaluate the utility of active inference for stochastic control us-
ingthesophisticatedplanningobjective.Forthis,weutilisethewindygrid-worldtask
[13],andassess ouragent’sperformancewhenvaryinglevelsofcomplexityareintro-
ducede.g.,stochasticwind,partialobservability,andlearningthetransitiondynamics.
Throughthesenumericalsimulations,wedemonstratethatactiveinference,compared
toaQ-learningagent[13],providesapromisingapproachforstochasticcontrol.
2 Stochastic control inawindy grid-world
Inthissection,wedescribethewindygrid-worldtask,withadditionalcomplexity,used
for evaluatingour activeinferenceagent(Section3). Thisis a classic grid-worldtask
fromreinforcementlearning[13],withapredefinedstart(S)andgoal(G)states(Fig.1).
Theaimistonavigateasoptimally(i.e.,withina minimumtimehorizon)aspossible,
takingintoaccounttheeffectofthewindalongtheway.Thewindrunsupwardthrough
the middleof the grid,and the goalstate is located in onesuch column.Thestrength
of the wind is noted under each column in Fig. 1, and its amplitude is quantified by
the numberof columnsshifted upwardsthat were unintendedby the agent. Here, the
agentcontrolsitsmovementthrough8availableactions(i.e.,theKing’smoves):North
(N), South (S), East (E), West (W), North-West (NW), South-West (SW), South-
East(SE),andNorth-East(NE).Everyepisodeterminateseitherattheallowedtime
horizon,orwhentheagentreachesthegoalstate.
2.1 Grid-worldcomplexity
To testtheperformanceofouractiveinferenceagentin acomplexstochasticenviron-
ment, we introduced different complexity levels to the windy grid-world setting (Ta-
ble1).
Table1:Fivecomplexitylevelsforthewindygrid-worldtask
Transition
Level Wind Observability Dynamics
1 Deterministic Full(MDP) Known
2 Stochastic Full(MDP) Known
3 Deterministic Full(MDP) Learned
4 Stochastic Full(MDP) Learned
5 Stochastic Partial(POMDP) Known
ActiveInferenceforStochasticControl 3
Fig.1: Windy grid-world task. Here, S and G denote starting and goal locations.
On the x-axis, the wind amplitude is shown. This is quantified as the number of
unintended additional columns the agent moves during each action e.g., any ac-
tion in column four results in one unintended shift upwards. There are 8 actions:
N,S,E,W,NW,SW,SE,NE.We plotsamplepathsfromthestartto the goalstate
inlightanddarkblue.Notice,theindirectjourneytothegoalisa consequenceofthe
wind.
Windproperties Inadeterministicsetting,theamplitudeofthewindremainsconstant.
Conversely,instochastic setting,forwindycolumnstheeffectvariesbyonefromthe
mean values. We consider two settings: medium and high stochasticity. For medium
stochasticity, the mean value is observed 70% of the time and similarly 40% of the
timeinthehighstochasticcase(Table2).Theadjacentwindvaluesareobservedwith
remainingprobabilities.Here, stochasticity is notexternallyintroducedto the system,
butitisinbuiltinthetransitiondynamicsB(Section3)oftheenvironment.
Table2:Stochasticnatureofwind
Level Windamplitudestatic Windamplitude±1
Medium 70%ofthetime 15%eachfor±1
High 40%ofthetime 30%eachfor±1
Observability In the fully observable setting, the agent is aware of the current state
i.e., there is no ambiguity about the states of affair. We formalise this as a Markov
decisionprocesses(MDP).Whereasinthepartiallyobservableenvironment,theagent
measures an indirect function of the associated state i.e., current observation. This is
usedtoinferthecurrentstate oftheagent.We formalisethisasapartiallyobservable
MDP(POMDP).Specificdetailsofoutcomemodalitiesusedinthetaskarediscussed
inAppendixB.
4 A.Pauletal.
Transition dynamics known to agent In the known set-up, the agent is equipped
withthetransitionprobabilitiesbeforehand.However,ifthesearenotknown,theagent
begins the trials with a uninformative (uniform) priors and updates its beliefs (Eq.9)
usingrandomtransitions.Briefly,randomactionsaresampledandtransitiondynamics
updatedto reflectthe best explanationfor the observationsat hand. Here, the learned
dynamicsareusedforplanning.
3 Activeinference on finitetemporal horizons
3.1 Generativemodel
Thegenerativemodelisformallydefinedasatupleoffinitesets(S,O,T,U,B,C,A):
◦ s∈S :stateswhereS ={1,2,3,...,70}ands isapredefined(fixed)startstate.
1
◦ o ∈ O : whereo = s, in the fullyobservablesetting, and in partialobservability
o=f(s)7.
◦ T ∈N+,andisafinitetimehorizonavailableperepisode.
◦ a∈U :actions,whereU ={N,S,E,W,NW,SW,SE,NE}.
◦ B : encodesthe transitiondynamics,P(s |s ,a ,B)i.e., the probabilitythat
t t−1 t−1
actiona takenatstates attimet−1resultsins attimet.
t−1 t−1 t
◦ C :priorpreferencesoveroutcomes,P(o|C).Here,C preferenceforthepredefined
goal-state.
◦ A : encodes the likelihood distribution, P(o |s ,A) for the partially observable
τ τ
setting.
Accordingly, the agents generative model is defined as the following probability
distribution:
P(o ,s ,a ,A,B,C)= (1)
1:T 1:T 1:T−1
T T
P(A)P(B)P(C)P(s ) P(s |s ,a ,B) P(o |s ,A) (2)
1 Y τ τ−1 τ−1 Y τ τ
τ=2 τ=1
3.2 Fullobservability
Perception: During full observability, states can be directly accessed by agent with
known or learned transition dynamics. Then the posterior estimates, Q(s |a ,s ),
τ+1 τ τ
canbedirectlycalculatedfromB[11].
Q(s |a ,s )=P(s |a ,s ,B). (3)
τ+1 τ τ τ+1 τ τ
7Here,outcomesintroduceambiguityfortheagentassimilaroutcomesmaptodifferent(hid-
den)states.SeeAppendixB,TableB.1forimplementationdetails.
ActiveInferenceforStochasticControl 5
Planning: In active inference,expectedfree-energy(G) [9] is used forplanning.For
finite temporal horizons, the agent acts to minimise G [11]. Here, to calculate G we
using the recursive formulation introduced in [10]. This is defined recursively as the
immediateexpectedfreeenergyplustheexpectedfreeenergyforfutureactions:
G(a |s )=G(a |s )=D [Q(s |a ,s )||C(s )] (4)
τ τ T−1 T−1 KL T T−1 T−1 T
forτ =T −1and,
G(a |s )=D [Q(s |a ,s )||C(s )] + E G(nextstep) (5)
τ τ KL τ+1 τ T−1 τ+1 Qh i
forτ =1,...,T −2.InEq.5,thesecondtermiscalculatedas,
E G(nextstep) =E [G(a |s )]. (6)
Qh i Q(aτ+1,sτ+1|sτ,aτ) τ+1 τ+1
Priorpreferenceoverstatesareencodedsuchthattheagentpreferstoobserveitself
in the goalstate ateverytime-step.C(o = goal) = 1, and0 otherwise.Inthe matrix
form,theithelementofC,correspondstoithstateinS.
Action selection: A distribution for action selection Q(a |s ) > 0 is defined using
τ τ
expectedfreeenergysuchthat,
Q(a |s )=σ(−G(U|s )). (7)
τ τ τ
Here,σisthesoftmaxfunctionensuringthatcomponentssumtoone.Ateachtime-step,
actionsaresamplesfrom:
a ∼Q(a |s ). (8)
t t t
Learningtransitiondynamics: Welearnthetransitiondynamics,B,acrosstimeusing
conjugacyupdaterules[14,12,9]:
t
b =b + δ Q(a)(s ⊗s ). (9)
a a XX a,aτ a,τ a,τ−1
τ=2aǫU
Here, b ∼ Dir(b;α) is the learnedtransition dynamicsupdatedover time, Q(a)
a
is the probability of taking action a, s is the state at time τ as a consequence of
a,τ
actiona,s isthestate-vectorattimeτ−1takingactiona,and⊗istheKronecker-
a,τ−1
product of the corresponding state-vectors. Furthermore, we also assessed the model
accuracy obtained after a given number of trials to update B, when random actions
wereemployedtoexploretransitiondynamics.Theselearnedtransitionswereusedfor
controlinLevel-3andLevel-4oftheproblem.
3.3 Partialobservability
We formalise partial observability as a partially observed MDP (POMDP). Here, the
agents have access to indirectobservationsaboutthe environment.Specific details of
6 A.Pauletal.
outcome modalities used in this work are discussed in Appendix B. These outcome
modalitiesaresameformanystatesfore.g.,thestates2and11havethesameoutcome
modalities (see AppendixB, Table B.1). Here, we evaluate the ability of active infer-
ence agent to perform optimal inference and planning in the face of ambiguity. The
critical advancement with sophisticated inference [10] compared to the classical for-
mulation[9]allowsustoperformdeep-treesearchforactionsinthefuture.Theagent
infers the hidden-statesby minimising a functionalof its predictive distribution (gen-
erative model) of the environment called the variational free-energy. This predictive
distributioncanbedefinedas,
T
Q(~s|~a,o˜):= Q(s |a ,s ,o˜). (10)
Y τ τ−1 τ−1
τ=1
To infer hidden-states from partial observations, thr agent engages in minimising
variationalfreeenergy(F)functionalofQusingvariational(Bayesian)inference.For
arigoroustreatmentofit,pleasereferto[10,11].Inthisscheme,actionsareconsidered
asrandomvariablesateachtime-step,assumingsuccessiveactionsareconditionallyin-
dependent.Thiscomeswithacostofhavingtoconsidermanyactionsequencesintime.
The search for policies in time is optimised both by restricting the search over future
outcomes which has a non-trivial posterior probability (Eg: > 1/16) as well as only
evaluating policies with significant prior probabilities (Eg: > 1/16) calculated from
theexpectedfreeenergy(i.e.,Occam’swindow).Inthepartiallyobservablesetting,the
expectedfreeenergyaccommodatesambiguityinfutureobservationsprioritisingboth
preferenceseekingaswellasambiguityreductioninobservations[10].
4 Results
Wecomparetheperformanceofouractiveinferenceagentwithapopularreinforcement
learning algorithm, Q-learning [13], in Level 1. Q-Learning is a model-free RL algo-
rithmthatoperatesbylearningthe’value’ofactionsataparticularstate.Itiswellsuited
forproblemswithstochastictransitionsandrewarddynamicsduetoitsmodel-freepa-
rameterisation.Q-Learningagentsareextensivelyusedinsimilarproblemsettingsand
exhibitstate-of-the-art(SOTA) performances[13]. To train the Q-learningagents, we
usedanexplorationrate of0.1,learningrate of0.5anddiscountfactorof1.Training
wasconductedusing10differentrandomseedstoensureunbiasedresults.Thetraining
depthforQ-Learningagentswereincreasedwithcomplexityoftheenvironment.
WeinstantiatetwoQ-learningagents,onetrainedfor500time-steps(QLearning500)
and anotherfor 5000time-steps(QLearning5K)in Level-1.Both the active inference
agentandtheQLearning5Kagentdemonstrateoptimalsuccessrateforthetime-horizon
T =8+(seeAppendixA,Fig.A.1).
Usingthesebaselinesfromthedeterministicenvironmentwithknowntransitiondy-
namics,wecomparedtheperformanceoftheagentinacomplexsettingwithmedium
andhighlystochasticwind(Level2;Table.2).Here,theactiveinferenceagentisclearly
superior against the Q-Learning agents (Fig. 2 top row). Moreover,they demonstrate
ActiveInferenceforStochasticControl 7
bettersuccessratesforshortertime-horizons,and’optimal’actionselection.Note,suc-
cess rate is the percentageof trials for which the agent successfully reached the goal
withintheallowedtime-horizon.
Next,weconsideredhowlearningthetransitiondynamicsimpactedagentbehaviour
(Level3and4).Here,weusedEq.9forlearningthetransitiondynamics,B.First,the
algorithm learnt the dynamics by taking random actions over X steps (for example,
X is 5000 time steps in ’SophAgent(5K B-updates)’, see Fig. 2 middle row). These
learnedtransitiondynamicsBwereused(seeFig.3)bytheactiveinferenceagenttoes-
timatetheactiondistributioninEq.8.Resultsforlevel3arepresentedinAppendixA,
Fig. A.2. Here, the Q-Learning algorithm with 5,000 learning steps shows superior
performancetotheactiveinferenceagents.Howeverwithlongertimehorizons,theac-
tiveinferenceagentshowscompetitiveperformance.Importantly,theactiveinference
agent used self-learned, and imprecise transition dynamics B in these levels. Level 4
results for mediumand highlystochastic setting are presentedin Fig. 2 (middlerow).
Formediumstochasticity,theQLearning10Kexhibitedsatisfactoryperformance,how-
everitfailedwithzerosuccessrateinthehighlystochasticcase.Thisshowstheneed
forextensivetrainingforalgorithmslikeQ-Learninginhighlystochasticenvironments.
However,theactiveinferenceagentdemonstratedat-parperformance.Remarkably,the
performancewasachievedusingimprecise(comparedtotrue-model),self-learnedtran-
sitiondynamics(B)(seeFig.3).
Theactiveinferenceagentshowssuperiorperformanceinthehighlystochasticen-
vironmentevenwithpartialobservability(Fig.2,lastrow).Conversely,excessivetrain-
ingwasrequiredfortheQ-Learningagenttoachieveahighsuccessrateinamedium
stochasticenvironment,buteventhistrainingdepthledtoazerosuccessratewithhigh
stochasticity.Theseresultspresentactiveinference,witharecursivelycalculatedfree-
energy,asapromisingalgorithmforstochasticcontrol.
5 Discussion
Weexploredtheutilityoftheactiveinferencewithplanninginfinitetemporal-horizons
for five complexity levels of the windy grid-world task. Active inference agents per-
formed at-par, or superior, when compared with well-trained Q-Learning agents. Im-
portantly,inthehighlystochasticenvironmentstheactiveinferenceagentshowedclear
superiorityovertheQ-Learningagents.Thehighersuccessratesatlowertimehorizons
demonstrated the ’optimality’ of actions in stochastic environments presented to the
agent. Additionally,this performanceis obtainedwith no specificationsof acceptable
policies.Thetotalnumberofacceptablepoliciesscaleexponentiallywiththenumberof
availableactionsandtime-horizon.Moreover,theLevel4&5resultsdemonstratethe
needforextensivetrainingfortheQ-Learningagentswhenoperatinginstochasticen-
vironments.Wealsodemonstratedtheabilityoftheactiveinferenceagentstoachieve
high success rate even with self-learned, but sub-optimal, transition dynamics. Meth-
ods to equip the agent to learn both transition-dynamicsB and outcome-dynamicsA
forapartiallyobservablesettinghavebeenpreviouslyexplored[14,9].Forastochastic
setting,weleavetheirimplementationforfuturework.
8 A.Pauletal.
1.0
0.8
0.6
0.4
0.2
0.0
5 10 15 20 25
Time-horizon
slairt
401
revo
etar
sseccuS
Medium stochastic case (Level 2)
1.0 SophAgent
QLearning5K
QLearning20K 0.8
0.6
0.4
0.2
0.0
5 10 15 20 25 30
Time-horizon
slairt
401
revo
etar
sseccuS
High stochastic case (Level 2)
SophAgent
QLearning5K
QLearning20K
1.0
0.8
0.6
0.4
0.2
0.0
5 10 15 20 25
Time-horizon
slairt
301
revo
etar
sseccuS
Medium stochastic case (Level 4)
1.0
SophAgent (5K B-updates)
SophAgent (10K B-updates)
QLearning10K 0.8
0.6
0.4
0.2
0.0
5 10 15 20 25 30
Time-horizon
slairt
401
revo
etar
sseccuS
High stochastic case (Level 4)
SophAgent (5K B-updates)
SophAgent (10K B-updates)
QLearning10K
QLearning20K
0.5
0.4
0.3
0.2
0.1
0.0
2 3 4 5 6 7 8 9 10
Time-horizon
slairt
301
revo
etar
sseccuS
Medium stochastic case (Level 5)
SophAgent
QLearning5K 0.25
QLearning10K
0.20
0.15
0.10
0.05
0.00
2 3 4 5 6 7 8 9 10
Time-horizon
slairt
301
revo
etar
sseccuS
High stochastic case (Level 5)
SophAgent
QLearning10K
QLearning20K
Fig.2: Stochastic environments: Performance comparison of agents in Level-2 (top
row), Level-4 (middle row), and Level-5 (last row) of windy grid-world task for
medium-stochastic(left column)and high-stochastic(rightcolumn)environments,re-
spectively.Here,x-axisdenotestimehorizonandy-axisthesuccessrateovermultiple-
trials.’SophAgent’representstheactiveinferenceagent,’QLearning5K’representsQ-
learningagenttrainedfor5,000time-steps,’QLearning10K’forthe’Q-learningagent
trainedfor10,000time-steps,and’QLearning20K’fortheQ-learningagenttrainedfor
20,000 time-steps. Each agent was trained using 10 differentrandom seeds. ’SophA-
gent(5KB-updates)’andSophAgent(10KB-updates)referstoactiveinferenceagent
usingself-learnedtransitiondynamicsBwith5000and10000updatesrespectively.
ActiveInferenceforStochasticControl 9
20
18
16
14
12
10
5 10 15 20 25
Time-horizon
)||eurtB−denraeLB||(
noitaived
ledoM
Model deviation (Medium Stochastic case)
20
Model deviation 5K B-updates
Model deviation 10K B-updates
18
16
14
12
5 10 15 20 25 30
Time-horizon
)||eurtB−denraeLB||(
noitaived
ledoM
Model devia ion (High S ochas ic case)
Model devia ion 5K B-updates
Model deviation 10K B-updates
Fig.3:Accuracyoflearneddynamicsintermsofdeviationfromtruetransitiondynam-
icsinLevel-4A:MediumstochasticcaseB:Highstochasticcase
The limitation yet to be addressed is the time consumed for trials in active infer-
ence.Largerun-timerestrictedanalysisforlongertimehorizonsinLevel5.Deeplearn-
ing approaches using tree searches, for representing policies were proposed recently
[15,16,17],maybeusefulinthissetting.We leaverun-timeanalysisandoptimisation
formoreambitiousenvironmentsforfuturework.Also,comparingactiveinferenceto
modelbasedRLalgorithmslikeDyna-Q[13]andcontrolasinferenceapproaches[18]
isapromisingdirectiontopursue.
Weconcludethattheaboveresultsplaceactiveinferenceasapromisingalgorithmfor
stochastic-control.
Softwarenote TheenvironmentsandagentswerecustomwritteninPythonforfully
observablesettings. The script ’SPM_MDP_VB_XX.m’available in SPM12 package
was used in the partially observable setting. All scripts are available in the following
link:https://github.com/aswinpaul/iwai2021_aisc.
Acknowledgments APacknowledgesresearchsponsorshipfromIITB-MonashResearchAcademy,Mumbaiand
DepartmentofBiotechnology,GovernmentofIndia.ARisfundedbytheAustralianResearchCouncil(Refs:DE170100128
&DP200100757)andAustralianNationalHealthandMedicalResearchCouncilInvestigatorGrant(Ref:1194910).ARisa
CIFARAzrieliGlobalScholarintheBrain,Mind&ConsciousnessProgram.ARandNSareaffiliatedwithTheWellcome
CentreforHumanNeuroimagingsupportedbycorefundingfromWellcome[203147/Z/16/Z].
References
1. Friston, K.: The free-energy principle: a unified brain theory?. Nat Rev Neuroscience 11,
127–138(2010).
2. Kaplan,RaphaelandFriston,KarlJ:Planningandnavigationasactiveinference.Biological
cybernetics112(4),323–343(2018).
3. Kuchling, Franz and Friston, Karl and Georgiev, Georgi and Levin: Morphogenesis as
Bayesian inference: A variational approach to patternformation and control incomplex bi-
ologicalsystems.Physicsoflifereviews,(2019)
4. Oliver,GuillermoandLanillos,PabloandCheng,Gordon:Activeinferencebodyperception
andactionforhumanoidrobots.arXivpreprintarXiv:1906.03022,(2019)
10 A.Pauletal.
5. Rubin,SergioandParr,ThomasandDaCosta,LancelotandFriston,Karl:Futureclimates:
Markovblanketsandactiveinferenceinthebiosphere.JournaloftheRoyalSocietyInterface
17(172),(2020)
6. Deane, George and Miller, Mark and Wilkinson, Sam: Losing Ourselves: Active Inference,
Depersonalization,andMeditation.FrontiersinPsychology,(2020)
7. FristonKJ,DaunizeauJ,KiebelSJ.:ReinforcementLearningorActiveInference?PLoSONE
4(7):e6421,(2009)https://doi.org/10.1371/journal.pone.0006421
8. Friston,KarlandSamothrakis,SpyridonandMontague,Read:Activeinferenceandagency:
optimalcontrolwithoutcostfunctions.Biologicalcybernetics106(8),523-541(2012)
9. NoorSajid,PhilipJ.Ball,ThomasParr,KarlJ.Friston.:ActiveInference:Demystifiedand
Compared.NeuralComputation33(3),674–712(2021)
10. KarlFriston,LancelotDaCosta,DanijarHafner,CasperHesp,ThomasParr:Sophisticated
Inference.NeuralComput2021;33(3),713–763(2021).
11. LancelotDaCostaandNoorSajidandThomasParrandKarlFristonandRyanSmith:,The
relationshipbetweendynamicprogrammingandactiveinference:thediscrete,finite-horizon
case.:,arXiv.2009.08111,(2020).
12. DaCosta,L.,Parr,T.,Sajid,N.,Veselic,S.,Neacsu,V.,andFriston,K.:Activeinferenceon
discretestate-spaces:asynthesis”,arXive-prints,(2020).
13. Sutton,R.,Barto,A.:ReinforcementLearning:AnIntroduction.MITPress(2018).
14. Friston,KarlandFitzGerald,ThomasandRigoli,FrancescoandSchwartenbeck,Philippand
Pezzulo,Giovanni:Activeinference:aprocesstheory.Neuralcomputation29(1),1–49(2017)
15. Fountas,ZafeiriosandSajid,NoorandMediano,PedroAMandFriston,Karl:Deepactive
inferenceagentsusingMonte-Carlomethods.arXivpreprintarXiv:2006.04176,(2020)
16. Çatal,OzanandNauta,JohannesandVerbelen,TimandSimoens,PieterandDhoedt,Bart:
Bayesianpolicyselectionusingactiveinference.arXivpreprintarXiv:1904.08149,(2019)
17. van der Himst, Otto Lanillos, P.: Deep Active Inference for Partially Observable MDPs.
In: Verbelen, Tim and Lanillos, Pablo and Buckley, Christopher L. and De Boom,
Cedric (eds.), Active Inference, pp. 61–71, Springer International Publishing (2020).
https://doi.org/10.1007/978-3-030-64919-7
18. Millidge,BerenandTschantz,AlexanderandSeth,AnilK.andBuckley,ChristopherL.:On
theRelationshipBetweenActiveInferenceandControlasInference.In:Verbelen,Timand
Lanillos,PabloandBuckley,ChristopherL.andDeBoom,Cedric(eds.),ActiveInference,pp.
3–11,SpringerInternationalPublishing(2020).https://doi.org/10.1007/978-3-030-64919-7
ActiveInferenceforStochasticControl 11
Supplementary information
A ResultsLevel-1 and Level-3 (Non-stochasticsettings)
1.0
0.8
0.6
0.4
0.2
0.0
2 4 6 8 10 12 14
Time-horizon
slairt
501
revo
etar
sseccuS
Results Level-1 Results Level-1
QLearning500 1.0 RandomAgent
SophAgent QLearning5K
0.8
0.6
0.4
0.2
0.0
2 4 6 8 10 12 14
Time-horizon
Fig.A.1:PerformancecomparisonofagentsinLevel-1ofwindygrid-worldtask.’Ran-
domAgent’referstoanaive-agentthattakesallactionswithequalprobabilityatevery
timestep.
1.0
0.8
0.6
0.4
0.2
0.0
2 4 6 8 10 12 14
Time-horizon
slairt
301
revo
etar
sseccuS
Determinstic case (Level 3)
22
20
SophAgent (5K B-updates)
SophAgent (10K B-Updates) 18
QLearning5K
16
14
12
2 4 6 8 10 12 14
Time-horizon
)||eurtB−denraeLB||(
noitaived
ledoM
Model de iation (Non-Stochastic case)
Model deviation 5K B-updates
Model deviation 10K B-updates
Fig.A.2:A:PerformancecomparisonofactiveinferenceagentswithlearnedB using
5000and10000updatesrespectivelytoQ-LearningagentinLevel-3.’Q-Learning5K’
standsforQ-Learningagenttrainedfor5000timestepsusing10differentrandomseeds.
B:Accuracyoflearneddynamicsintermsofdeviationfromtruedynamics.
12 A.Pauletal.
B Outcomemodalities forPOMDPs
Inthepartiallyobservablesetting,weconsideredtwooutcomemodalitiesandbothof
themwerethefunctionof’side’and’down’coordinatesdefinedforeverystateinFig.
1.Examplesofthecoordinatesandmodalitiesaregivenbelow.Firstoutcomemodality
isthesumofco-ordinatesandsecondmodalityistheproductofcoordinates.
TableB.1:Outcomemodalitiesspecifications
Down Side Outcome-1 Outcome-2
State
coordinate(C1) coordinate(C2) (C1+C2) (C1*C2)
1 1 1 2 1
2 1 2 3 2
. . . . .
11 2 1 3 2
. . . . .
31 4 1 5 4
38 4 8 12 32
. . . . .
Theseoutcomemodalitiesaresimilarformanystates(fore.g.,states2and11have
thesameoutcomemodalities(seeTab.B.1)).Theresultsdemonstratestheabilityofac-
tiveinferenceagenttoperformoptimalinferenceandplanninginthefaceofambiguity.
Oneoftheoutputfrom’SPM_MDP_VB_XX.m’is’MDP.P’.’MDP.P’returnstheac-
tionprobabilitiesanagentwilluseforagivenPOMDPasinputateachtime-step.This
distribution was used to conduct multiple trails to evaluate success rate of the active
inferenceagent.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Active Inference for Stochastic Control"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
