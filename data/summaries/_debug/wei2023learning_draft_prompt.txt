=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Learning An Active Inference Model of Driver Perception and Control: Application to Vehicle Car-Following
Citation Key: wei2023learning
Authors: Ran Wei, Anthony D. McDonald, Alfredo Garcia

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2023

Key Terms: vehicle, driver, upon, agent, state, perception, control, observations, application, learning

=== FULL PAPER TEXT ===

1
Learning An Active Inference Model of Driver
Perception and Control: Application to Vehicle
Car-Following
Ran Wei, Alfredo Garcia, Anthony McDonald, Gustav Markkula, Johan Engstrom, Matthew O’Kelly
Abstract—In this paper we introduce a general estima- [5]. In contrast to reinforcement learning (wherein the goal
tionmethodologyforlearningamodelofhumanperception is to identify a control policy based upon reward and state
and control in a sensorimotor control task based upon a
observations), the goal of IRL is to estimate the reward
finitesetofdemonstrations.Themodel’sstructureconsists
functionandtransitionprobabilitiesfromobservedtrajectories
of (i) the agent’s internal representation of how the envi-
ronmentandassociatedobservationsevolveasaresultof ofstate-actionpairs[6]–[8].Theestimatedrewardprovidesan
controlactionsand(ii)theagent’spreferencesoverobserv- interpretation of the agent’s behavior and the reward function
able outcomes. We consider a model’s structure specifi- can be used to design policies in domains where manual
cationconsistentwithactiveinference,atheoryofhuman
reward specification is difficult, e.g., in autonomous driving
perceptionandbehaviorfromcognitivescience.According
[9]. There is a significant literature on identification and
to active inference, the agent acts upon the world so as
tominimizesurprisedefinedasameasureoftheextentto estimation of models of human control when the state is
which an agent’s current sensory observations differ from observable [10]–[12]. In contrast, model identification and
its preferred sensory observations. We propose a bi-level estimation when the state is only partially observable (as
optimizationapproachtoestimationwhichreliesonastruc-
in models accounting for human perception) has received
tural assumption on prior distributions that parameterize
lessattention.Notableexceptionsinclude[13]–[16].However,
thestatistical accuracyofthe humanagent’smodel ofthe
environment. To illustrate the proposed methodology, we the environments considered in these papers are either low
present the estimation of a model for car-following behav- dimensional as in [13], [14], restricted to a linear-quadratic
ior based upon a naturalistic dataset. Overall, the results control [15] or customized for a specific control task [16].
indicate that learning active inference models of human
Inthispaper,weintroduceaBayesianestimationmethodol-
perceptionandcontrolfromdataisapromisingalternative
ogy for learning a structural model of perception and control
toblack-boxmodelsofdriving.
ingeneralcontroltasksinhigherdimensionalsettings.Weuse
Index Terms—Human perception and action, Partially theformalismofaPartiallyObservableMarkovDecisionPro-
ObservableMarkovDecisionProcess,Activeinference,in-
cess (POMDP) in which the agent’s preferences are modeled
versereinforcementlearning.
by a reward function and the agent’s internal representation
I. INTRODUCTION of the environment consists of observation and transition
probabilities. However, a POMDP model of a human agent’s
IN many control tasks requiring mind and motor resources
perception and control policy based solely on demonstrations
by a human agent, the observation space can be high-
isingeneralnon-identifiable,i.e.theremaybeseveraldifferent
dimensional and complex. Empirical evidence indicates that
combinations of reward and internal model of the environ-
humansagentsuseasimpler,lowerdimensionalrepresentation
ment that rationalize the same demonstrations dataset. This
of the environment in sensorimotor control tasks [1], [2].
is because in planning control tasks, different combinations
The Bayesian brain hypothesis [3], [4] posits that the human
of reward and internal dynamics model could result in the
brain uses the information provided by sensory data to update
same inter-temporal reward trade-offs. To address this issue,
a representation of the world in the form of a conditional
we make a structural assumption on prior distributions that
probabilitydistribution.Toaccountforthestructureofhuman
parameterize the statistical accuracy of the human agent’s
perception and control in a task involving motor and mind
model of the environment. Specifically, we assume (i) the
resources, a model must separately describe (i) the agent’s
agent’spreferencesandmodeloftheenvironmentareindepen-
internalrepresentationoftheworldastheenvironmentevolves
dent and (ii) the distribution of the model of the environment
as a result of control actions and (ii) the agent’s preferences
parameters concentrates on values with higher fit to the data
over observable outcomes. Equipped with data in the form of
(i.e. higher log-likelihood). In words, this assumption restricts
demonstrations (i.e. sequences of recorded observation-action
our estimation to agents with reasonably accurate models of
pairs), the learning task is to estimate the agent’s preferences
the environment whose preferences over states of the world
as well as its internal representations leading to a behavior
are not determined by their perception of the environment.
policy that best fits data.
ThisallowsustoformulatetheMaximumAPosteriori(MAP)
In machine learning, this estimation problem is known as
estimator as the solution to a bi-level optimization problem.
inverse reinforcement learning (IRL) in an off-line setting
The upper-level problem is the maximization of the posterior
distribution and the lower-level problem is the computation
ThisworkwassupportedinpartbyArmyResearchOfficeAROunder
grantW911NF-22-1-0213. of optimal policy for the given reward and model of the
5202
yaM
1
]GL.sc[
2v10251.3032:viXra
2
environment. We approximate the solution to this bi-level based upon active inference, a novel framework for cognition
optimization problem by a stochastic gradient algorithm with and behavior. In section V, we describe the application of the
a nested policy optimization step. proposed estimation algorithm to obtain an active inference
To illustrate the proposed methodology, we consider an model for car-following behavior by human drivers. We com-
applicationtohighwaydrivingusinganaturalisticdataset.We pare the active inference model with Behavior Cloning (BC)
specify the structure of the model in accordance to “active and the Intelligent Driver Model (IDM). Finally, in section
inference” [17]–[19], a novel framework for modeling human VI, we close with concluding remarks about the promise and
perception and behavior in sensorimotor control tasks [20]. challengesoflearningperception&controlmodelsbasedupon
Active inference is related to the Bayesian Brain hypothesis naturalistic datasets.
thatpositspredictionasthefundamentaltaskofcognition[21],
II. A POMDP MODEL OF PERCEPTION AND CONTROL
[22], i.e. the brain minimizes prediction error by updating
beliefs about the states of the world consistent with data. WestartbyprovidingadescriptionofapartiallyObservable
Activeinferencetakesaconceptualleapfromthisviewinthat MarkovDecisionProcess(POMDP)ofperceptionandcontrol.
minimizingpredictionerrorcanalsobeattainedbybothupdat- This encompasses neuroscience modeling frameworks for hu-
ingbeliefsandactingupontheworldtoapproximatelyinduce manperceptionandactioninsensorimotortaskssuchasactive
a preferred distribution of the states of the world. The active inference [17] and the expected value of control (EVC) [32].
inference framework is summarized by a principle of free In the proposed POMDP framework (see Figure 1), the
energy minimization: forward (action) and backward (belief) human agent maintains an internal model of the world (or
updating processes work in tandem to minimize “surprise” representation) so that high-dimensional observations o t ∈
with respect to a preferred belief distribution about the states O ⊂ Rn (sensory stimuli) are represented with a lower
of the world. dimensional hidden state s t ∈ S ⊂ Rm where S is the state
Ourultimategoalistoprovideamodelthatisinterpretable space and m << n. If the hidden state is s t and a t ∈ A
–in terms of a cognitive model of perception and action– is implemented, the agent accrues a reward r(s t ,a t ). The
and that exhibits a statistical performance that is similar or agent’s internal representation includes state dynamics, i.e. a
arguably superior to other black-box models. To this end, transition to a new state s t+1 takes place with probability
we compare the learned structural model of perception and T(s t+1 |s t ,a t ) and a new observation o t+1 is obtained with
action(withanactiveinferencerewardspecification)withtwo probability O(o t+1 |s t+1 ).
After t > 0 time periods, the observable history of obser-
baseline models based on Behavior Cloning (BC), a common
vations and actions is denoted by
machinelearningapproachtodrivingbehaviormodeling[23]–
[26], and the Intelligent Driver Model (IDM), a class of h :={o ,...,o ,a ,...,a }∈H ⊂Ot+1×At.
t t 0 t−1 0 t
rule-based models widely used by traffic simulation software
We consider randomized or stochastic policies π that are
[27]–[29]. It is important to emphasize that the naturalistic
adapted to the history of the process, i.e. given history h
dataset does not include collisions and only includes rela- t
action a ∈ A is implemented with probability π(a|h ) ∈
tively few extreme observations (e.g. extremely low relative (cid:80) t
[0,1],a∈A and π(a|h )=1 for all h ∈H .
distance or high relative velocity). Thus, our model only a∈A t t t
In the proposed POMDP model of perception and action,
provides an account of driver perception and control behavior
the human agent aims to maximize the expected value of
in average conditions and our testing of model performance
discounted reward net of information processing costs:
focuses on aggregate measures. The results indicate that the
(cid:104)(cid:88) (cid:105)
active inference-based model outperforms those obtained by U (h )≜ supE γt−τ[r(s ,a )−c(π(·|h ))] (1)
τ τ t t t
imitation learning-based models from the machine learning π∈Π t≥τ
literature.However,thelearnedmodelisinaccurateinextreme where γ ∈ (0,1) is the discount factor, Π is the set of
scenarios that are poorly covered in the dataset and does
randomized policies that are adapted to the history process
exhibit higher collision rates than IQM when tested online. and c(π(·|h )) is a per-period information processing cost.
t
Thisisduetothelimitationsofthedatasetwhichpoorlycovers
As human agents may differ in their ability to process task-
extreme scenarios so that the learned model does poorly in
relevantinformationortoattendtothetaskathand[33]–[37],
extreme scenarios due to distribution shift [30], [31]. Overall, thecostc(π(·|h ))modelsthefactthatlowentropybehavioral
t
the results indicate that learning active inference models from policies are consistent with high information processing effort
data is a promising alternative to black-box models of driving
or attention.
asitprovidesawaytotracedrivingbehaviorsbacktoahuman The combination of additive reward structure and Marko-
drivers’ perception and preferences. vian dynamics allows for a recursive characterization of the
The structure of this paper is as follows. In section II, we optimal policy as follows:
start by describing a Partially Observable Markov Decision (cid:40)
Process (POMDP) of perception and control. In section III, (cid:88)(cid:88)
U (h )= max r(s ,a )b (s )π(a |h )−c(π(·|h ))
t t t t t t t t t
we describe the inverse estimation problem, i.e. based upon π(·|ht)
at st
sequences of observations and implemented actions to esti- (cid:41)
(cid:88)(cid:88)
mate the primitives of the POMDP model (reward, partially +γ P(o |h ,a )π(a |h )U (h ) .
t+1 t t t t t+1 t+1
observable state transition and observation probabilities). In ot+1 at
section IV, we describe the specification of reward function (2)
3
Theorem 2.2: (a)B :Q→Qisacontractionmappingwith
Observations o t−1 o t o t+1 modulus γ ∈(0,1) with unique fixed point Q∗, i.e.
(cid:88)
Q∗(b,a)= r(s,a)b(s)+
InternalModel s t−1 s t s t+1 ···
s
··· b t−1 b t b t+1 b t+2 γ
(cid:88)
σ(o′|b,a)αlog
(cid:16)(cid:88)
π0(a′|b′)exp
(cid:0)1
Q∗(b′,a′)
(cid:1)(cid:17)
,
α
o′ a′
a a a (b)
Action t−1 t t+1
Fig.1: Graphical Model of Perception and Control. V∗(b)= max (cid:104)(cid:88) πˆ(a|b)Q∗(b,a)−αD KL (cid:0) πˆ(·|b)||π0(·|b) (cid:1)(cid:105)
πˆ(·|b)
Let b ∈ ∆S denote the Bayes updated belief distribution a
on the s t tate, i.e. b (s) := P(s = s|h ), where ∆S is the set =αlog (cid:88) π0(a|b)exp (cid:0)1 Q∗(b,a) (cid:1)
t t t α
of probability distributions on state space S. a
Let us denote by σ(o |b ,a ) the probability of recording
t+1 t t (c) the optimal policy is of the form:
observation o when action a is implemented and the
t+1 t
current belief distribution is b t , i.e. π0(a|b)exp (cid:0)1Q∗(b,a) (cid:1)
σ(o |b ,a ):= (cid:88)(cid:88) O(o |s )T(s |s ,a )b (s ) π∗(a|b)= (cid:80) π0(a′|b)ex α p (cid:0)1Q∗(b,a′) (cid:1) . (5)
t+1 t t t+1 t+1 t+1 t t t t a′∈A α
st+1 st
UsingstandardPOMDPargumentsinProposition1belowwe Proof: See Appendix.
show that with no loss of optimality, the search for optimal Remark 1: Note that as α→+∞, information processing
policy can be restricted to Markovian policies, say ΠM ⊂ effortisarbitrarilycostlyandinthelimit,theagentimplements
Π that only depend b t , the current Bayes updated belief as the default policy π∗ → π0. Conversely, as α → 0+, we
opposed to the whole history h t ∈H t . recover the optimal solution without information processing
Proposition 2.1: Let V t (b) be recursively defined as fol- cost since V∗(b)→max a∈A Q∗(b,a).
lows: Remark 2: In the remainder of the paper we shall use
(cid:110)(cid:88)(cid:88)
α = 1 and the default policy is the uniformly random policy
V (b)= max r(s,a)π(a|b)b(s)−c(π(·|b))
t π(·|b) π0(a|b) = 1 . With these choices the optimal policy takes
s a |A|
(cid:88)(cid:88) (cid:111) the form:
+γ σ(o′|b,a)π(a|b)V (b′)
t+1
a o′
π∗(a|b)=
expQ∗(b,a)
. (6)
whereb′(s)=P(s =s|h ∪(a,o′)),i.e.theresultingBayes (cid:80) expQ∗(b,a′)
t+1 t a′∈A
update after action a is implemented and observation o′ are
recorded. Then, the Bayes updated belief b = P(·|h ) is a Remark 3 (Finite Horizon): It can be easily verified that
t t
sufficient statistic for solving (2), i.e. U (h ) = V (b ) for all Proposition 1 and Theorem 1 continue to hold for the case
t t t t
h . in which the controller is solving a finite horizon problem.
t
Proof: See Appendix. Evidently, the results in this case require that the state-action
We now state and prove the soft Bellman equation for the function Q t and the conditional choice probabilities π t are
value function V (b) when the information processing cost time-dependent t. Formally, for a planning horizon of length
t
H >0, the optimal policy at time t∈{0,1,...,H} is of the
is proportional to the Kullback-Leibler divergence between
the control policy and a default policy π0, i.e. c(π(·|b )) = form:
t
αD KL (π(·|b t )||π0(·|b t )) where: expQ∗ (b,a)
π∗ (a|b)= t,H (7)
(cid:88) π(a|b) t,H (cid:80) expQ∗ (b,a′)
D (π(·|b)||π0(·|b)):= π(a|b)log . (3) a′∈A t,H
KL π0(a|b)
a∈A
and
and α>0.
(cid:88) (cid:88)
Let Q be the Banach space of bounded, measurable func- Q∗ (b,a)= r(s,a)b(s)+ σ(o′|b,a)V∗ (b′) (8)
t,H t+1,H
tions Q:∆S →R under the supremum norm ||.||. Define the
s o′
soft Bellman operator B :Q→Q by
where b′ is the resulting Bayes update after action a and
(cid:88)
[BQ](b,a):= r(s,a)b(s)+ observation o′ are recorded and
s
γ (cid:88) σ(o′|b,a)αlog (cid:16)(cid:88) π0(a′|b′)exp (cid:0)1 Q(b′,a′) (cid:1)(cid:17) , (4) V t ∗ +1,H (b′)=log
(cid:16)(cid:88)
expQ∗ t+1,H (b′,a′)
(cid:17)
t≤H −1
α
a′
o′ a′
(9)
where b′ is the resulting Bayes update after action a and
observation o′ are recorded. and V∗ =0.
H+1,H
4
III. ESTIMATION METHODOLOGY (parameterizedbyθ
1
).Underassumption1(b)onthepriordis-
tribution, parameter values θ with higher fit to the sequences
Equipped with a model of perception and action as de- 1
of observations in the data are more likely. Increasing values
scribed in the previous section, we consider the estimation of
of λ imply the agent has (a priori) an increasingly accurate
the primitives based upon demonstrations, that is, sequences
model of the environment.
of observations and implemented actions of the form τ =
AssumingauniformpriorP(θ )onacompactsubsetΘ ⊂
{(o ,a ),(o ,a ),...,(o ,a )}. We shall denote by D the 2 2
0 0 2 1 T T Rp, the log of the posterior distribution can be written as:
finitedatasetofdistinctsequencesofobservation-actionpairs. 2
The primitives of the perception & control model are
parametrized as follows: logP(θ|D)=logP(D|θ)+logP(θ
1
)+constant
• P of er h ce id p d ti e o n n: s W tat e e a d ss y u n m am e i t c h s ea a g n e d nt o ’s bs i e n r t v er a n ti a o l n re p p r r o e b se a n b t i a li t t i i o e n s =E D (cid:104) log (cid:88) T π θ ∗(at|b θ1,t ) (cid:105) |D|+
is parametrized with θ ∈ Rp so that the likelihood t=0
1 1
o
σ
f o
(o
bserv
|b
at
,
io
a
n
).
o
t+1
given beliefs b
t
and action a
t
is
λE
D
(cid:104)(cid:88) T
logσ
θ1
(o
t+1
|b
θ1,t
,at)
(cid:105)
|D|+constant
θ1 t+1 t t
t=0
• Preferences: A reward function r θ2 (b,a) which is (14)
parametrized by θ ∈Rp.
2 2 We are ready to formulate the estimation problem as the
Assuming the data is generated by an agent who uses a
following bi-level optimization problem:
receding horizon plan with horizon H according to (7), the
log-likelihood of a sequence τ ∈D can be written as (cid:104) (cid:88) T (cid:88) T (cid:105)
max E log π∗(a |b )+λ logσ (o |b ,a )
P(τ|θ)= (cid:89) T (cid:16) π∗(a |b )P(cid:0) o |h ∪{a } (cid:1)(cid:17) (θ1,θ2) D t=0 θ t θ1,t t=0 θ1 t+1 θ1,t t
θ t θ1,t t+1 t t (15)
t=0 (cid:104) (cid:88) (cid:105)
where to alleviate notation we write π∗(·|b) to refer to the s.t. π θ ∗ =arg max E [r θ (b h ,a h )−logπ(·|b h )]
θ π∈ΠH
first-period optimal policy with a planning horizon H > h≤H
0. P(o |h ∪ {a }) is the external observation-generating
t+1 t t where here again we write π∗(·|b) to refer to the first-period
distributionthatisindependentoftheagent’sinternalrepresen- θ
tation. Hence, the log-likelihood of dataset D can be written optimal policy with a planning horizon H > 0 with initial
as: belief b. The algorithm for approximating a solution, say θ(cid:98), to
(cid:89) (15) is described in Algorithm 1 below. The estimated model
logP(D|θ)=log P(τ|θ)
structure is summarized as follows:
τ∈D
T
=E τ∼D (cid:104)(cid:88) log (cid:16) π θ ∗(at|b θ1,t )P(cid:0) o t+1 |ht∪{at} (cid:1)(cid:17)(cid:105) |D| Structural Model of Perception and Control
t=0 Perception
(10)
Observations O (o |s )
=E τ∼D (cid:104)(cid:88) T logπ θ ∗(at|b θ1,t ) (cid:105) |D|+constant (11) Transitions T θ(cid:98) θ(cid:98) 1 1 (s t t +1 t |s t ,a t )
t=0 Generative Model σ θ(cid:98)1 (o t+1 |b t ,a t )
where the expectation is taken with respect to the empirical Control
measure P¯(τ)= π∗ | ( D 1 a | |b a ) n = d expQ∗ θ (b,a) (12) P C r o e n fe tr r o e l nc P e o s li ( c r y eward) r π θ(cid:98) θ(cid:98) 2 ( ( a s t t | , b a t ) t ) = (cid:80) a′ e ∈ x A pQ ex ∗ θ(cid:98) p (b Q t ∗ θ , (cid:98) a ( t b ) t,a′)
θ (cid:80) expQ∗(b,a′)
a′∈A θ
Condition (12) imposes model in the form of the first period Algorithm 1 Bayesian MAP Estimation of Perception &
policy of a receding horizon plan. Control Model
We take a Bayesian approach to finding an estimator and Require: Dataset D = {τ}, perception model σ (o′|b,a),
θ1
make an additional assumption on the structure of the prior preference model r (b,a), initial value θ =(θ ,θ ),
θ2 0 1,0 2,0
distribution of parameters denoted by P(θ): hyperparameter λ>0 and learning rate ρ>0.
Assumption 1: (a) P(θ) = P(θ 1 )P(θ 2 ). (b) The distribu- 1: for k =0:K do
tion of θ 1 is of the form: 2: Compute the optimal policy π θ ∗ k using value-iteration
P(θ )∝exp (cid:16) λE (cid:2)(cid:88) T logσ (o |b ,a ) (cid:3) |D| (cid:17) 3: (14) E . valuate the log posterior logP(θ k |D) according to
1 τ∼D θ1 t+1 θ1,t t
t=0 4: Compute the gradient of ∇ θ logP(θ k |D)
(13) 5: Perform parameter update
for some λ>0. θ =θ +ρ∇ logP(θ |D)
k+1 k θ k
Assumption 1(a) restricts our estimation to agents whose
6: end for
preferences (parameterized by θ ) over states of the world
2
are not determined by their perception of the environment
5
IV. AN ACTIVE INFERENCE SPECIFICATION behaviorbyhumandrivers.1 Computationalmodelsofhuman
In this section we describe a specification of the reward performance in such task have been amply studied by traffic
functionconsistentwithactiveinference[17].Activeinference engineersandpsychologists,seee.g.,[41]–[44].However,our
is a novel framework for cognition and behavior according to goal here is to learn a model that is motivated by cognitive
whichtheagentjointlyperceivesandactsupontheworldsoas science (active inference) based upon a naturalistic dataset of
to maximize the match between perceived vs preferred states task demonstrations. In this sense, the closest paper to our
of the world. work is [16] which assumes the agent’s decisions are based
The process of matching the perceived vs preferred distri- upon a state estimate (speed, relative speed and distance) and
bution of the states of the world follows a principle of free a predictive model of the lead vehicle. In contrast, in the
energy minimization: forward (action) and backward (belief) proposed POMDP model, the variables speed, relative speed
updating processes work in tandem to minimize a measure and distance are observations which are used by the agent
of “surprise” or free energy. For backward (belief) updating, to form current and future beliefs about the states of the
free energy is minimized when the agent’s belief distribution environment which are discrete.2. In addition, the policy in
b corresponds to the Bayes updated belief distribution on the [16] is deterministic and the model is not based on agent’s
t
state s . For forward (action) selection processes, surprise is preferences.
t
measuredwithrespecttoapreferreddistributionP˜(s )over We compare the active inference model, referred to as
t+1
statesoftheenvironment.Theimmediate“surprise”associated Active Inference Driving Agent (AIDA), with two baseline
with action a when current beliefs are b is quantified by the models: Behavior Cloning (BC), a common machine learning
t t
expected free energy defined as: approach to driving behavior modeling [23]–[26], and the
Intelligent Driver Model (IDM), a rule-based model used
EFE(b ,a )=E(cid:2) D (cid:0) b ||P˜(cid:1)(cid:3) +E(cid:2) H(O(·|s )) (cid:3)
t t KL t+1 t+1 by most traffic simulation software [27]–[29]. We begin by
(16)
describingthebaselinemodelsandthedatasetusedtoestimate
where the expectation is taken with respect to o ∼ the parameters of the models. We then describe the protocols
t+1
O(·|s ),s ∼ (cid:80) T(·|s,a )b (s) with for evaluating the models’ ability to replicate human driving
t+1 t+1 s t t
behaviorinthedataset.Lastly,wepresentthemodelevaluation
b (s)=P(s =s|h ∪{a ,o })
t+1 t+1 t t t+1 results and demonstrate AIDA’s interpretability advantages.
and H(O(·|s )) is the entropy of the resulting generative Implementation details are provided in Appendix VII-C.
t+1
model of observations, i.e.:
A. Modelsandparameterization
(cid:88) (cid:16) (cid:17)
H(O(·|s )):=− O(o′|s )log O(o′|s ) .
t+1 t+1 t+1 Behavior Cloning: BC trains neural networks to map
o′ observations or a history of observations to control actions
The first term in (16) quantifies the extent to which the belief in the dataset. The policy parameters, denoted with θ, are
distribution on the states of the world b t+1 (resulting from estimatedusingmaximumlikelihoodestimationofthedataset
implementingactiona t andrecordingobservationo t+1 )differs actions:
from the preferred distribution of the states of the world P˜(·).
This term is usually referred to as “risk” because of its rela- (cid:34) (cid:88) T (cid:35)
maxL(θ)=E logπ (a |h ) (17)
tionship to the deviation from an agent’s goal [38]. Selecting D θ t t
θ
policies that generate preferred observations minimizes risk. t=0
The second term in (16) is a measure of the observation
We implement two BC approaches in this work, a standard
uncertainty induced by action a . This term is referred to
t multi-layer neural network approach (BC-MLP) and a recur-
as “ambiguity” and represents the value of obtaining reliable
rent neural network approach (BC-RNN). These approaches
information that may help to resolve uncertainty about future
are strong baselines for simulated driving agents. [25], [26],
states[18],[38].Definingambiguityhingesonhavingamodel
[46], [47].
of the world.
Intelligent Driver Model: The IDM [27] accepts obser-
In [39] an interpretation of active inference (when the
vations of vehicle speed v, relative speed to the lead vehicle
state is observable) is given in terms of Markov decision
∆v, and distance headway to the lead vehicle d as inputs and
processes. In a similar manner, by setting the reward function
outputs acceleration a (the control action) using the following
as r(b ,a ) := −EFE(b ,a ), the active inference model
t t t t rule:
can be seen as a particular instance of the class of POMDP
models described in section II [40]. However, the ability to  (cid:16)v (cid:17)4 (cid:32) d˜ (cid:33)2 
consider trade-offs between the described measures of risk a t =a max1− v˜ t − d  (18)
vs. ambiguity presents an advantage of the active inference t
formulation compared to traditional RL/IRL formulations.
1Thesourcecodeisavailableathttps://github.com/ran-weii/
V. APPLICATION: LEARNING A MODEL OF PERCEPTION interactive_inference.
AND CONTROL IN CAR FOLLOWING BEHAVIOR 2In this sense, the generative model in the proposed POMDP model can
beseenascategorical.Thereisevidencetosupportthecategoricalnatureof
In this section, we describe the application of Algorithm
humanperceptioninpsycho-physicalexperiments[45]thataresimplerthan
1 to estimate an active inference model for car-following theoneconsideredinthispaper
6
where v˜ is a desired speed and d˜ is the desired distance
headway defined as:
v ∆v
d˜=d +v τ − √t t (19)
0 t
2 a b
max
The IDM has the following parameters: v˜ the desired
Fig.2: Top-down view of the roadway explored in the analy-
speed, a the maximum acceleration rate which can be
max
sis. The west-bound lanes (blue) have denser traffic and more
implementedbythedriver,d theminimumallowabledistance
0
stop-and-go behavior whereas the east-bound lanes (orange)
headway, τ the desired headway time, and b the maximum
have sparser traffic and higher speed. We trained the models
decelerationrate.Tocaptureheterogeneityindata,wespecify
to emulate the behavior of the blue cars and evaluated the
an “ensemble” of IDM models, i.e. a distribution over actions
models’ ability to predict the behavior of the blue and orange
givenobservations.Theensemblemodelparameters,i.e.mean
cars. Grey cars in the merging lanes were excluded.
and variance of action given observations, are obtained by
maximizing the likelihood of the ensemble model predictions
to the dataset of observed actions subject to the mean action
review of a subset of trajectories. We also removed all trajec-
satisfying (18).
tories with length shorter than 10 seconds for the dense lanes
Active Inference Driving Agent: We parameterized the
and 5 seconds for the sparse lanes, leaving a total of 1,254
state transition probability distributions T (s′|s,a) as cate-
θˆ 1 trajectoriesinthedenselanesand290trajectoriesinthesparse
gorical distributions and the observation probability distribu-
lanes with an average length of 14 seconds. We only used the
tions O (o|s) using normalizing flows [48], specifically, a
θˆ 1 dense lane data for training models.
sharedinverseautoregressiveflowwithasetofGaussianbase
1) Feature Computation: The input features to the IDM
distributions [49]. Thus, observations that have high density
are defined in (18) and (19). For BC and the AIDA, we
under the conditional distribution of each state represent the
used d and ∆v but excluded v to prevent the models from
“prototypical” observation for that state. The active inference
achieving spuriously high training accuracy by computing
preference distribution P˜ (s) is parameterized using a cate-
θˆ
2
acceleration predictions from past ego velocities, a well-
goricaldistribution.Wethenobtainedthefinitehorizonpolicy
known phenomenon reported in prior studies [25], [26], [53].
in (12) by computing the value function in (8) using a finite
Furthermore, we included an additional feature τ−1 in BC
number of value iterations steps using the QMDP method
and AIDA which is a visual estimate of inverse time-to-
[50]. By optimizing the policy log likelihood (15), both the
collision defined as the rate of change of the visual angle
preferences and prototypical observations are fitted to explain
of the lead vehicle from the ego driver’s seat position divided
actions in the dataset.
by the angle itself [54]. This feature was chosen to account
B. Dataset for speed control and puts the information contained in the
inputs to BC and the AIDA on a similar level to the IDM as
We trained and evaluated AIDA, BC, and IDM using the
theIDMimplicitlyaccountsfortime-to-collisioninitsdesired
INTERACTION dataset [51], a publicly available driving
distanceheadwaycomputationin(19).Italsomakesourmodel
dataset recorded using drones on fixed road segments in the
consistent with recent family of driver models [55]–[57].
USA, Germany, and China. The dataset provides a lanelet2
We computed all features in the Frenet frame (i.e., lane-
format map [52] and a set of time-indexed trajectories of the
centric coordinates [58]), by first transforming vehicle posi-
positions,velocities,andheadingsofeachvehicleinthescene
tions, velocities, and headings using the current lane center
in the map’s coordinate system at a sampling frequency of 10
line as the reference path and then computing the features
Hz,and thevehicle’slength andwidthfor eachroadsegment.
from the transformed positions and velocities. We obtained
The dataset contains a variety of traffic behaviors, including
the drivers’ instantaneous longitudinal control inputs (i.e.,
car following, free-flow traffic, and merges.
accelerations) from the dataset by differentiating the Frenet
Due to our emphasis on modeling longitudinal control
frame longitudinal velocities. For BC and the AIDA, we
behavior in car following, we selected a subset of the data
discretized the continuous control inputs into discrete actions
to include car following data from a two-way, seven-lane
using a Gaussian mixture model of 15 Gaussian components
highway segment in China with a total distance of 175 m.
with mean and variance parameters chosen with the Bayesian
We focused on vehicles in the four middle lanes shown in
Information Criteria [59].
Figure 2, where the blue west-bound lanes have denser traffic
C. ModelEvaluationandComparison
and more stop-and-go behavior and the orange east-bound
lanes have sparser traffic at higher speed. We further filtered We evaluated and compared our models’ ability to generate
the remaining vehicles according to two criteria: 1) there behavior similar to the human drivers in the dataset using
was a lead vehicle with a maximum distance headway of 60 both open-loop offline predictions and closed-loop online
m, and 2) the ego vehicle was not performing a merge or simulations. In both cases, we evaluated the models (15 seeds
lane change. This focus facilitates algorithm comparisons by foreachmodelclass)ontwodifferentheld-outtestingdatasets.
removing environmental artifacts. We identified merging and The first dataset includes vehicles from the same dense lanes
lane change behavior using an automated logistic regression- as the training dataset. This dataset tests whether the models
basedapproachandvalidatedtheclassificationswithamanual cangeneralizetounseenvehiclesinthesametrafficcondition.
7
We obtained this dataset by dividing trajectories in the dense and online control performance using the interquartile mean
lanes using a 7-3 train-test ratio. The second dataset includes (IQM) of the offline MAEs and online ADEs. The IQMs
vehicles from the sparse lanes. This dataset tests whether are computed by 1) ranking all tested trajectories by their
the models can generalize to unseen vehicles in novel traffic respective performancemetrics and 2)computing themean of
conditions, since the traffic in the east-bound lanes have on the performance metrics ranked in the middle 50%. Collision
average higher speed and less density. rates are computed as the percentage of testing runs that
1) OfflineEvaluation: Thegoaloftheofflineevaluationwas resulted in a collision. It should be noted that IQM makes the
to assess each model’s ability to predict a driver’s next action differencebetweeneachmodel’sperformancecentraltendency
based on the observation-action history recorded in the held- more salient at the expense of removing the tails of the
outtestingdataset.Thistaskevaluatesthemodels’abilitytobe performance distribution. Thus, we also provide the average
used as a short-horizon predictor of other vehicles’ behavior performance results in appendix (VII-E). To compare the
inanon-boardtrajectoryplanner[60].Wemeasuredamodel’s centralperformancedifferencebetweentheAIDAandbaseline
predictive accuracy using Mean Absolute Error (MAE) of the models, we performed two-sided Welch’s t-tests with 5 per-
predicted control inputs (unit=m/s2) on the entire held-out cent rejection level on the MAE-IQM and ADE-IQM values
testingdatasets.FortheIDM,thepredictedcontrolinputswere computed from different random seeds with the assumption
given by the IDM rule, i.e., we discarded the variance used that the performance distributions between two models may
for model fitting. For BC and the AIDA, predicted action is have different variances [62], [63].
produced by first sampling a discrete action from the action
D. ResultsandDiscussion
distribution predicted by the models and then sampling the
mean of the selected Gaussian component from the Gaussian 1) Offline Performance Comparison: Figure 3 shows the
mixture model used to perform action discretization. MAE offline evaluation results for each model with the model type
of each dataset action was calculated as the average of 30 on the x-axis and the IQMs of acceleration prediction MAEs
samples. averaged across the testing dataset on the y-axis. The color
2) Online Evaluation: Rather than predicting instantaneous of the points in the figure represents the testing condition and
actions, the goal of the online evaluation was to assess eachpointcorrespondstotheresultofamodelinitializedfrom
the models’ ability to generate trajectories similar to human a different random seed. The points are randomly distributed
drivers such that they can be used as simulated agents in around each x-axis label for clarity. Dispersion on the y-axis
automatedvehicletrainingandtestingenvironments[24].This indicates sensitivity in the model to initial training conditions.
isfundamentallydifferentfromofflinepredictionsbecausethe The plot illustrates that the AIDA had the lowest MAE-IQM
models need to choose actions based on observation-action in the sparse-lane tests, followed by BC-RNN, IDM, and BC-
history generated by its own actions rather than those stored MLP. The corresponding pairwise Welch’s t-test results in
in the fixed, offline dataset. This can introduce significant Table V (Appendix VII-F) show that the differences between
distributionshift[61]sometimesresultinginsituationsoutside AIDA and baseline models are significant. The difference
the model’s training data, which can lead to poor action between IDM and BC-RNN was surprisingly small and BC-
selection. MLP had substantially larger MAE. This was likely because
We built a single-agent simulator where the ego vehicle’s theIDMrulewaswell-suitedtocapturebehaviorinthistraffic
longitudinal acceleration is controlled by the trained models condition,whereastheaccuracyofBC-MLPwasrestrictedby
and its lateral acceleration is controlled by a feedback con- the features it had access to and action discretization. In the
troller for lane-centering. The lead vehicle simply plays back sparse-lane tests, AIDA performed similarly to BC models
the trajectory recorded in the dataset. Other vehicles do not with a few seeds substantially better than BC models. IDM
haveanyeffectontheegovehicle,givenourobservationspace performed substantially worse and also with much higher
does not contain other vehicle related features. We tested the variance across different seeds. Given IDM trained from
models on 100 randomly chosen trajectories in each of the different initializations converged to similar final parameters,
dense-lane and sparse-lane settings. thisresultwasmostlikelyduetothedistributionshiftbetween
Following [23], we measured the similarity between the trainingandtestingsetsandIDMrule’slackofadaptabilityto
generated trajectories and the true trajectories using the fol- different traffic conditions. However, the poor performance of
lowing metrics: IDM may be specific to the dataset considered in this paper
1) Averagedeviationerror(ADE;unit=m):deviationofthe (see Appendix, Section VII-E).
FrenetFramepositionfromthedatasetaveragedoverall To understand each model’s actual behavior, Figure 4 com-
time steps in the trajectory. pares the predicted actions of each model’s best performing
2) Lead vehicle collision rate (LVCR; unit=%): percentage seedversusthegroundtruthonarandomlyselectedtrajectory
of testing trajectories containing collision events with in the dense-lane (left) and sparse-lane (right) settings, re-
the lead vehicle. A collision is defined as an overlap spectively, where shading corresponds to 1 standard deviation
between the ego and lead vehicles’ bounding boxes. of the predictive distribution represented by 30 samples as
3) StatisticalEvaluation: Followingtherecommendationsin described in section V-C.1. In the dense-lane setting, all
[62], [63] for evaluating learned control policies in stochastic models captured the variation of actions in the dataset, i.e.,
environments with a finite number of testing runs, we rep- acceleration first decreased and then increased. However, the
resented the central tendency of a model’s offline prediction acceleration magnitudes predicted by IDM were substantially
8
modelshadADE-IQMvaluesbetween1.8mand2.8m,which
is less than the length of a standard sedan (≈ 4.8 m; [64]).
Among all models, BC-MLP achieved the lowest ADE values
forboththedense-laneandsparse-laneconditions,followedby
the AIDA, IDM, and BC-RNN. Furthermore, both the AIDA
and BC models achieved lower ADE-IQM in the sparse lane
settings compared to the dense-lane setting, however the IDM
achieved higher ADE-IQM in the sparse-lane setting. The
Welch’s t-test results in Table VI show that AIDA’s online
test performances are significantly different from all baseline
models in both the dense-lane and sparse-lane settings (P ≤
0.01). These findings confirm that the AIDA and BC models
Fig.3: Offline evaluation MAE-IQM. Each point corresponds
generalized better to the sparse-lane setting than the IDM and
toarandomseedusedtoinitializemodeltraininganditscolor
suggest that the AIDA’s average online trajectory-matching
corresponds to the testing condition of either dense-lane or
ability is on average better than IDM and BC-RNN, although
sparse-lane.
BC-MLPisbetterthantheAIDA.However,itshouldbenoted
smaller than the ground truth. In the sparse-lane setting, the thatthetail-endbehaviorofAIDAandBC-RNNcanbeworse
prediction interval of all BC models and AIDA were able to when evaluated under average ADE (i.e., without IQM; see
cover ground truth actions. However, IDM predictions were Figure 12 in Appendix VII-E) where the worst AIDA seed
substantially lower than the ground truth. These patterns are performed approximately equal to the worst BC-RNN seed,
consistent with the aggregate measures in Figure 3. both of which would increase online ADE by 1 m.
Fig.5: Online evaluation ADE-IQM. Each point corresponds
toarandomseedusedtoinitializemodeltraininganditscolor
corresponds to the testing condition of either dense-lane or
sparse-lane.
To understand how trajectory deviations were generated,
Figure 6 shows the ADE of the best seed of each model
averaged over all testing episodes for each time step in
the dense-lane (left) and sparse-lane (right) scenarios. We
truncated the plots at 10 s and 4 s because there are very
few trajectories longer than those horizons making the curves
highly oscillatory. The amount of deviations generated by
Fig.4:Exampleofflinepredictionsinthedense-lane(top)and
different models are consistent with the prior study [46]. The
sparse-lane(bottom)settings.Eachlineexceptfortheblueline
ranking of model performance is also consistent with the
represents the mean prediction of the corresponding model.
aggregated measures in Figure 5. In the dense-lane settings
Shading represents 1 standard deviation of prediction interval.
(Figure 6 left), model performance started to differentiate
The prediction intervals for BC and AIDA are computed by
around 4 s but the differences were not substantial (i.e., up
drawing 30 samples from the models’ predictive distributions.
to 2 m). In contrast, in the sparse-lane setting, IDM generated
IDM has no prediction interval because it’s deterministic.
substantially larger deviations from the beginning and BC-
MLP and AIDA had nearly matching ADE at all time steps.
2) Online Performance Comparison: Figure 5 shows the Figure 7 shows the lead vehicle collision rates for each
IQM of each model’s ADEs from data set trajectories in random seed and model using the same format as Figure
the online evaluations using the same format as the offline 5. The figure illustrates that in the dense-lane condition, the
evaluation results. In the dense-lane testing condition, all randomseedsforBC-MLP,BC-RNN,andtheAIDAhadmore
9
Fig.7: Lead vehicle collision rate in online evaluation. Each
point corresponds to a random seed used to initialize model
training and its color corresponds to the testing condition of
either dense-lane or sparse-lane.
theAIDA’sdecisionsareemittedfromatwo-stepprocess,i.e.,
(1) forming beliefs about the environment and (2) selecting
control actions that realized preferred states (i.e., minimize
free energy), the model’s interpretability depends on the two
sub-processesbothindependentlyandjointly.Thus,weexam-
ined the learned input-output mechanism by visualizing the
Fig. 6: Online evaluation ADE for each time step averaged
components (i.e., the observation, transition, and preference
over all online testing episodes for the dense-lane (top) and
distributions) of the best performing AIDA seed and verified
sparse-lane (bottom) settings by the best seed of each model.
them against expectations guided by driving theory [67]–[69].
Wethen examinedthe jointbelief-actionprocess byreplaying
the AIDA beliefs and diagnosing its predictions of recorded
collisionsthantheIDM(0%collisionrateacrossallseeds).In human drivers in the offline setting and its own decisions in
particular, BC-RNN and the AIDA had substantial differences the online setting.
across random seeds compared to the other models. However, 4) AIDAComponentInterpretability: Initial insights into the
the minimum collision rates for BC-MLP, BC-RNN, and the model input and output connections can be gained by visu-
AIDAwereconsistent(lessthanorequalto1%).Inthesparse- alizing the AIDA components, specifically its policy (Figure
lane condition, the collision rate was 0% for all four models. 8b), observation distribution (Figure 8c), and preference dis-
Thehighercollisionratesinthedense-lanedataarelikelydue tribution (Figure 8d). These figures show 200 random “pro-
tothetrafficdensityandcomplexity,whichwerehigherinthe totypical” samples from the observation distribution O(o|s)
dense-lane condition compared to the sparse-lane condition. of each state, plotted on each pair of observation modalities.
This is also due to the way we defined a collision in section Thetoprowshowsthesamplesusingdistanceheadway(d;x-
V-C.2 as any overlapping of vehicle bounding boxes. As we axis) by relative velocity to the lead vehicle (∆v; y-axis), the
showlaterinsectionV-D.5,manycollisioneventsweredueto middle row shows distance headway by τ−1, and the bottom
insufficient braking magnitude despite correct braking intent, rowshowsrelativevelocitybyτ−1.Colorisusedtohighlight
part of which can be attributed to discrete belief and action relevant quantities of interest. We further used samples drawn
spaces.Thisputsegovehicle’sstoppingpositionslightlyahead from the INTERACTION dataset, plotted in Figure 8a and
oftheno-collisionpositionwithoutgeneratingalargeposition coloredbytherecordedaccelerations,tofacilitateinterpreting
deviationascommonlyseeninmachine-learneddrivingagents the the AIDA samples. The shape of the sampled points
[46]. matches the contour of the empirical dataset (Figure 8a),
3) AIDAInterpretabilityAnalysis: Theprevioussectionssug- particularly in the middle and bottom visualizations, which
gest that the AIDA can capture driver car following behav- suggests that the model’s learned observation model aligns
iorcomparably if not better than baseline models. However, with the recorded observations in the dataset. However, the
the findings have yet addressed the interpretability of the learned distributions also showed longer tails at the edge of
AIDA. Interpretability represents the ability to understand the the data distribution. This was expected because the dataset
relationship between model input and output and is a crucial does not contain samples that correspond to these extreme
element of model deployment success [65]. While there is no conditions.Thusthemodelcouldnotlearnaccuratekinematics
establishedmetricformodelinterpretability,Ra¨ukuret.al.[66] in these regions. Nevertheless, this does not affect the inter-
recommend assessments based on the ease of comprehending pretability analysis.
the connection between model input and output and tracing Figure8billustratestheobservationsamplesbythemodel’s
modelpredictiveerrorstointernalmodeldynamics.Giventhat chosencontrolactions.Darkergreenandredcolorscorrespond
10
toppanel.ThemiddleandbottompanelsshowthattheAIDA’s
categorization of high τ magnitude states (blue and cyan
1
clusters) have a larger span than that of low τ−1 magnitude
states. These patterns further establish that the AIDA has
learned a representation of the environment consistent with
the dataset. At the same time, it can be interpreted as a form
of satisficing in that the model represents low urgency large
distance headway states with less granularity [70].
Figure 8d shows the observation samples by the log of
its preference probability, P˜(o) = (cid:80) P˜(s)O(o|s), where
s
higher preference probability (i.e., desirability) corresponds
to brighter colors (e.g., yellow) and lower desirability corre-
sponds to darker colors (e.g., purple). The figure shows that
the highest preference probability corresponds to observations
of zero τ−1, zero relative velocity, and a distance headway of
18m(seethecenterregionofthemiddlechart,andtheyellow
circle at the left-center of the top chart). This aligns with the
task-difficultyhomeostasishypothesisthatdriverspreferstates
inwhichthecrashriskismanageable[67]andnotincreasing.
ItisalsoconsistentwiththeobserveddriverbehaviorinFigure
8a where drivers tend to maintain low accelerations (light
(a) (b) (c) (d)
yellow points) within the same regions.
Fig. 8: Visualizations of the best performing AIDA seed. In Overall, these results show a clear mapping between the
panel(a),weplottedobservationssampledfromthedataset.In AIDA’s perceptual (Figure 8c) and control (Figure 8d and
panels(b),(c),and(d)weillustrateAIDA’slearnedpolicy,ob- 8b) behavior that is both consistent with the observed data
servation model, and preference model via 200 “prototypical” and straightforwardly illustrated using samples from the fitted
samples from each state’s conditional observation distribution modeldistributions.Thismappingfacilitatespredictionsofthe
O(o|s) and plotted the samples for each pair of observation AIDA’s reaction to observations without querying the model,
featurecombinations.Thepointsineachpanelarecoloredby: which is an important dimension of interpretability in real
(a) accelerations from the dataset, (b) predicted accelerations world model validation [66].
upon observing the sampled signals from a uniform prior 5) AIDADecisionDiagnostics: While the previous analysis
belief, (c) state indices (d) log probabilities of the preference illustratestheinterpretabilityofindividualmodelcomponents,
distribution. the overall model interpretability is also contingent upon
understandingtheinteractionbetweencomponents.Toaddress
to larger acceleration and deceleration magnitudes, respec- this, we analyzed two dense-lane scenarios where the AIDA
tively, and light yellow color corresponds to near zero control madesub-optimaldecisionsinthemodeltestingphase—one
inputs. The color gradient at different regions in Figure 8b is fromtheofflineevaluationswheretheAIDA’spredictionshad
consistent with that of the empirical dataset shown in Figure the largest MAE and one from the online evaluations where
8a. This shows that the model learned a similar control rule theAIDAgeneratedarear-endcollisionwiththeleadvehicle.
(i.e., observation to action mapping) as the empirical dataset. We first visualized the AIDA’s beliefs and policies as the
The control rule can be interpreted as the tendency to choose model generated actions and then used those visualizations to
negative accelerations when the relative speed and τ−1 are demonstrate how the transparent input-output mechanism in
negative and the distance headway is small, and positive the AIDA can be used to mitigate the sub-optimal decisions.
accelerationsintheoppositecase.Furthermore,thesensitivity The chosen offline evaluation trajectory is visualized in
of the red and green color gradients with respect to distance Figure 9. The left column charts show the data of the three
headway shows that the model tends to accelerate whenever observation features over time. The right column charts show
there is positive relative velocity, regardless of the distance the time-varying ground truth action probabilities over time
headway. However, it tends to input smaller deceleration at (top), action probabilities predicted by the AIDA over time
large distance headway for the same level of relative speed. (middle), and environment state probabilities P(s|h) inferred
Figure 8c shows the observation samples colored by their by the AIDA over time (bottom). In the right-middle and
associated discrete states. The juxtaposition of color clusters right-bottom charts, the action and belief state indices are
in the top panel shows that the AIDA learned to categorize sorted by the mean acceleration and τ−1 value of each state
observations by relative speed and distance headway and its to facilitate alignment with the left and top-right charts. We
categorization for relative speed is more fine-grained at small labeled the actions by the corresponding means but not the
distance headways and spans a larger range of values. The belief states because they represent multi-dimensional obser-
middle and bottom panels show that its categorization of vation categorizations (see Figure 8c). The bottom-right chart
relative speed is highly correlated with τ−1 as the ordering shows that the inferred belief patterns closely followed the
of colors along the y-axis is approximately the same as in the observed relative speed and τ−1 in the left-middle and left-
11
bottom charts with high precision, i.e., close to probability
of 1. The predicted action probabilities in the right-middle
chart followed the trend of the ground truth actions, however,
they exhibited substantially higher uncertainty at most time
steps and multi-modality at t = 1 s and t = 12 s, where
one of the predicted modes coincided with the true actions.
Given the inferred beliefs were precise, uncertain and multi-
model actions were likely caused by inter-driver variability in
the dataset, where drivers experienced similar belief states but
Fig. 10: Visualizations of a dense-lane online evaluation tra-
selected different actions. Alternatively, this uncertainty may
jectorywheretheAIDAgeneratedarear-endcollisionwiththe
becausedbyactualdrivershavinghighlydifferentbeliefsafter
lead vehicle. This figure shares the same format as Figure 9.
experiencing similar observations, In either case, the error in
Theredsquareinthebottom-leftchartrepresentstheduration
AIDA predictions can be attributed to inconsistency between
oftherear-endcrasheventwherethevehiclecontrolledbythe
the belief trajectories and action predictions.
AIDA had overlapping bounding box with the lead vehicle.
thetransitionandobservationdistributions)toproperlyrecog-
nizenear-crashobservationsignalscanlikelyavoidthecurrent
crash.
The analyses in this section show that the decision making
structure in the AIDA enables modelers to reason about the
training dataset’s effect on the learned model behavior. To the
bestofourknowledge,thisanalysisisnotpossiblewithneural
network BC models using existing interpretability tools. Thus
Fig. 9: Visualizations of a dense-lane offline evaluation tra-
AIDA represents a significant step forward for interpretable
jectory where the AIDA had the highest prediction MAE.
perception and control models of human control behavior.
The charts in the left column show distance headway, relative
speed, and τ−1 signals observed by the model over time.
The binary heat maps in the right column show the ground
VI. CONCLUSIONS
truth action probabilities (top), action probabilities predicted We consider the problem of learning a model of human
by the AIDA (middle), and the corresponding belief states perception and control based on data in the form of ob-
(bottom) over time (x-axis), where darker colors correspond servations and implemented actions. We posit a POMDP
to higher probabilities. The belief state and action indices are model and formulated a bi-level optimization formulation of
sorted by the mean τ−1 and acceleration value of each state, Maximum A Posteriori (MAP) estimate for the primitives
respectively. of the model. To illustrate the estimation methodology we
develop a model of driver behavior (AIDA) with the reward
The chosen online evaluation trajectory which resulted in specification motivated by the active inference framework
a rear-end collision with the lead vehicle is shown in Figure from cognitive science. Using car following data, we showed
10 plotted using the same format as Figure 9. The duration of that the AIDA performed comparably and in certain cases
thecrasheventishighlightedbytheredsquareinthebottom- betterthantherule-basedIDManddata-drivenneuralnetwork
left chart, where the sign of τ−1 values instantly inverted benchmarks.Usinganinterpretabilityanalysis,weshowedthat
when overlapping bounding boxes between the ego and lead the structure of the AIDA provides superior transparency of
vehiclefirstoccurredandeventuallyended.TheAIDAinitially its input-output mechanics than the neural network models.
made the correct and precise decision of braking, however, its Future work should focus on training with data from more
predictions for high magnitude actions became substantially diversedrivingenvironmentsandexaminingmodelextensions
less precise prior to the collision (t > 1 s; see right middle that can capture heterogeneity across human agents.
chart). This led to the model failing to stop fully before
colliding with the lead vehicle. The belief pattern shows that ACKNOWLEDGEMENTS
theAIDAtrackedtheinitialdecreasingvaluesofrelativespeed Support for this research was provided in part by the
andτ−1butdidnotfurtherrespondtoincreasingmagnitudeof U.S. Department of Transportation (DOT), University Trans-
τ−1 3 seconds prior to the crash (starting at t=1.6 s). These portation Centers Program to the Safety through Disruption
findings show that the model exhibited the correct behavior UniversityTransportationCenter(451453-19C36)andtheUK
of being “shocked” by out-of-sample near-crash observations, EngineeringandPhysicalSciencesResearchCouncil(EPSRC;
however, the learned categorical belief representation was EP/S005056/1). Thanks to advisers, J. Engstrom and M.
not able to extrapolate beyond the data from the crash-free O’Kelly,fromWaymo,whohelpedsetthetechnicaldirection,
INTERACTION dataset. identified relevant published research, and advised on the
The analysis of the near-crash AIDA beliefs suggests that scope and structuring of this publication, independent of the
editingtheAIDA’slearnedenvironmentdynamicsmodel(i.e., support this research received from USDOT.
12
REFERENCES [24] M.Igl,D.Kim,A.Kuefler,P.Mougin,P.Shah,K.Shiarlis,D.Anguelov,
M.Palatucci,B.White,andS.Whiteson,“Symphony:Learningrealistic
and diverse agents for autonomous driving simulation,” arXiv preprint
arXiv:2205.03195,2022.
[25] F.Codevilla,E.Santana,A.M.Lo´pez,andA.Gaidon,“Exploringthe
[1] D. Badre, A. Bhandari, H. Keglovits, and A. Kikumoto, “The dimen- limitations of behavior cloning for autonomous driving,” in Proceed-
sionality of neural representations for control,” Current Opinion in ings of the IEEE/CVF International Conference on Computer Vision,
BehavioralSciences,vol.38,pp.20–28,2021. pp.9329–9338,2019.
[2] H.OpdeBeeck,J.Wagemans,andR.Vogels,“Inferotemporalneurons [26] M.Zhou,X.Qu,andX.Li,“Arecurrentneuralnetworkbasedmicro-
representlow-dimensionalconfigurationsofparameterizedshapes,”Na- scopiccarfollowingmodeltopredicttrafficoscillation,”Transportation
tureneuroscience,vol.4,pp.1244–52,012002. researchpartC:emergingtechnologies,vol.84,pp.245–264,2017.
[3] D.C.KnillandA.Pouget,“TheBayesianbrain:theroleofuncertainty [27] M. Treiber, A. Hennecke, and D. Helbing, “Congested traffic states in
in neural coding and computation,” Trends in Neurosciences, vol. 27, empiricalobservationsandmicroscopicsimulations,”PhysicalreviewE,
no.12,pp.712–719,2004. vol.62,no.2,p.1805,2000.
[28] M. Treiber and A. Kesting, “Microscopic calibration and validation
[4] K.Friston,“ThehistoryofthefutureoftheBayesianbrain,”NeuroIm-
of car-following models–a systematic approach,” Procedia-Social and
age,vol.62,no.2,pp.1230–1233,2012.
BehavioralSciences,vol.80,pp.922–939,2013.
[5] T.Osa,J.Pajarinen,G.Neumann,J.A.Bagnell,P.Abbeel,andJ.Peters,
[29] A.Kesting,M.Treiber,andD.Helbing,“Agentsfortrafficsimulation,”
AnAlgorithmicPerspectiveonImitationLearning,vol.7ofFoundations
Multi-agentsystems:Simulationandapplications,vol.5,2009.
andTrendsinRobotics. 2018.
[30] J. Quionero-Candela, M. Sugiyama, A. Schwaighofer, and N. D.
[6] A. Y. Ng, S. J. Russell, et al., “Algorithms for inverse reinforcement
Lawrence,DatasetShiftinMachineLearning. TheMITPress,2009.
learning.,”inIcml,vol.1,p.2,2000.
[31] S. Levine, A. Kumar, G. Tucker, and J. Fu, “Offline reinforcement
[7] T. Osa, J. Pajarinen, G. Neumann, J. A. Bagnell, P. Abbeel, J. Peters, learning: Tutorial, review, and perspectives on open problems,” arXiv
et al., “An algorithmic perspective on imitation learning,” Foundations preprintarXiv:2005.01643,2020.
andTrends®inRobotics,vol.7,no.1-2,pp.1–179,2018.
[32] A. Shenhav, M. M. Botvinick, and J. D. Cohen, “The expected value
[8] N. Ab Azar, A. Shahmansoorian, and M. Davoudi, “From inverse ofcontrol:Anintegrativetheoryofanteriorcingulatecortexfunction,”
optimalcontroltoinversereinforcementlearning:Ahistoricalreview,” Neuron,vol.79,no.2,pp.217–240,2013.
AnnualReviewsinControl,vol.50,pp.119–138,2020. [33] N.TishbyandD.Polani,“Informationtheoryofdecisionsandactions,”
[9] T.Phan-Minh,F.Howington,T.-S.Chu,M.S.Tomov,R.E.Beaudoin, inPerception-actioncycle,pp.601–636,Springer,2011.
S.U.Lee,N.Li,C.Dicle,S.Findler,F.Suarez-Ruiz,B.Yang,S.Omari, [34] P.A.OrtegaandD.A.Braun,“Thermodynamicsasatheoryofdecision-
andE.M.Wolff,“Driveirl:Driveinreallifewithinversereinforcement making with information-processing costs,” Proceedings of the Royal
learning,” in 2023 IEEE International Conference on Robotics and SocietyA:Mathematical,PhysicalandEngineeringSciences,vol.469,
Automation(ICRA),pp.1544–1550,2023. no.2153,p.20120683,2013.
[10] E. Boer and R. Kenyon, “Estimation of time-varying delay time in [35] F.MateˇjkaandA.McKay,“Rationalinattentiontodiscretechoices:A
nonstationary linear systems: an approach to monitor human operator new foundation for the multinomial logit model,” American Economic
adaptation in manual tracking tasks,” IEEE Transactions on Systems, Review,vol.105,pp.272–98,January2015.
Man, and Cybernetics - Part A: Systems and Humans, vol. 28, no. 1, [36] D. Fudenberg, R. Iijima, and T. Strzalecki, “Stochastic choice and
pp.89–99,1998. revealedperturbedutility,”Econometrica,vol.83,no.6,pp.2371–2409,
[11] K.vanderEl,D.M.Pool,H.J.Damveld,M.R.M.vanPaassen,and 2015.
M.Mulder,“Anempiricalhumancontrollermodelforpreviewtracking [37] L.P.HansenandJ.Miao,“Aversiontoambiguityandmodelmisspecifi-
tasks,” IEEE Transactions on Cybernetics, vol. 46, no. 11, pp. 2609– cationindynamicstochasticenvironments,”ProceedingsoftheNational
2621,2016. AcademyofSciences,vol.115,no.37,pp.9163–9168,2018.
[12] F.M.Drop,D.M.Pool,M.R.M.vanPaassen,M.Mulder,andH.H. [38] A.Tschantz,A.K.Seth,andC.L.Buckley,“Learningaction-oriented
Bu¨lthoff,“Objectivemodelselectionforidentifyingthehumanfeedfor- modelsthroughactiveinference,”PLoScomputationalbiology,vol.16,
ward response in manual control,” IEEE Transactions on Cybernetics, no.4,p.e1007805,2020.
vol.48,no.1,pp.2–15,2018. [39] J. Y. Shin, C. Kim, and H. J. Hwang, “Prior preference learning from
[13] C.Baker,J.Jara-Ettinger,R.Saxe,andJ.Tenenbaum,“Rationalquanti- experts: Designing a reward with active inference,” Neurocomputing,
tativeattributionofbeliefs,desiresandperceptsinhumanmentalizing,” vol.492,pp.508–515,2022.
Nature:HumanBehavior,no.4,pp.1–10,2017. [40] R.Wei,“Valueofinformationandrewardspecificationinactiveinfer-
[14] Y. Chang, A. Garcia, Z. Wang, and L. Sun, “Structural Estimation of enceandpomdps,”arXivpreprintarXiv:2408.06542,2024.
Partially Observable Markov Decision Processes,” IEEE Transactions [41] M. Taieb-Maimon and D. Shinar, “Minimum and comfortable driving
onAutomaticControl,vol.68,no.8,pp.5135–5141,2023. headways: Reality versus perception,” Human Factors, vol. 43, no. 1,
[15] D. Straub and C. A. Rothkopf, “Putting perception into action with pp.159–172,2001. PMID:11474761.
inverse optimal control for continuous psychophysics,” eLife, vol. 11, [42] S.H.Hamdar,M.Treiber,H.S.Mahmassani,andA.Kesting,“Modeling
p.e76635,sep2022.
driverbehaviorassequentialrisk-takingtask,”TransportationResearch
Record,vol.2088,no.1,pp.208–217,2008.
[16] J. Pekkanen, O. Lappi, P. Rinkkala, S. Tuhkanen, R. Frantsi, and
[43] S. H. Hamdar, H. S. Mahmassani, and M. Treiber, “From behavioral
H.Summala,“Acomputationalmodelfordriver’scognitivestate,visual
psychology to acceleration modeling: Calibration, validation, and ex-
perceptionandintermittentattentioninadistractedcarfollowingtask,”
ploration of drivers’ cognitive and safety parameters in a risk-taking
RoyalSocietyOpenScience,vol.5,no.9,p.180194,2018.
environment,”TransportationResearchPartB:Methodological,vol.78,
[17] K.Friston,“Thefree-energyprinciple:aunifiedbraintheory?,”Nature
pp.32–53,2015.
reviewsneuroscience,vol.11,no.2,pp.127–138,2010.
[44] F. W. Siebert, M. Oehl, F. Bersch, and H.-R. Pfister, “The exact
[18] T.Parr,G.Pezzulo,andK.Friston,ActiveInference:TheFreeEnergy
determinationofsubjectiveriskandcomfortthresholdsincarfollowing,”
PrincipleinMind,Brain,andBehavior. MITPress,2022.
Transportation Research Part F: Traffic Psychology and Behaviour,
[19] D. Maisto, F. Donnarumma, and G. Pezzulo, “Interactive Inference: A
vol.46,pp.1–13,2017.
Multi-Agent Model of Cooperative Joint Actions,” IEEE Transactions [45] S.K.Reed,“Patternrecognitionandcategorization,”CognitivePsychol-
onSystems,Man,andCybernetics:Systems,pp.1–12,2023.
ogy,vol.3,no.3,pp.382–407,1972.
[20] J. Engstro¨m, R. Wei, A. D. McDonald, A. Garcia, M. O’Kelly, and [46] R. Bhattacharyya, B. Wulfe, D. Phillips, A. Kuefler, J. Morton,
L.Johnson,“Resolvinguncertaintyonthefly:modelingadaptivedriving R. Senanayake, and M. Kochenderfer, “Modeling human driving be-
behaviorasactiveinference,”FrontiersinNeurorobotics,vol.18,2024. haviorthroughgenerativeadversarialimitationlearning,”arXivpreprint
[21] A. Clark, Surfing Uncertainty: Prediction, Action, and the Embodied arXiv:2006.06412,2020.
Mind. OxfordUniversityPress,2015. [47] A. Kuefler, J. Morton, T. Wheeler, and M. Kochenderfer, “Imitating
[22] J.Hohwy,ThePredictiveMind. OxfordUniversityPress,2015. driver behavior with generative adversarial networks,” in 2017 IEEE
[23] S. Suo, S. Regalado, S. Casas, and R. Urtasun, “Trafficsim: Learning IntelligentVehiclesSymposium(IV),pp.204–211,IEEE,2017.
to simulate realistic multi-agent behaviors,” in Proceedings of the [48] G. Papamakarios, E. T. Nalisnick, D. J. Rezende, S. Mohamed, and
IEEE/CVF Conference on Computer Vision and Pattern Recognition, B. Lakshminarayanan, “Normalizing flows for probabilistic modeling
pp.10400–10409,2021. andinference.,”J.Mach.Learn.Res.,vol.22,no.57,pp.1–64,2021.
13
[49] D. P. Kingma, T. Salimans, R. Jozefowicz, X. Chen, I. Sutskever, and [73] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
M.Welling,“Improvedvariationalinferencewithinverseautoregressive T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al., “Pytorch: An
flow,”Advancesinneuralinformationprocessingsystems,vol.29,2016. imperative style, high-performance deep learning library,” Advances in
[50] M.L.Littman,A.R.Cassandra,andL.P.Kaelbling,“Learningpolicies neuralinformationprocessingsystems,vol.32,2019.
forpartiallyobservableenvironments:Scalingup,”inMachineLearning [74] S. Hoogendoorn and R. Hoogendoorn, “Calibration of microscopic
Proceedings1995,pp.362–370,Elsevier,1995. traffic-flowmodelsusingmultipledatasources,”PhilosophicalTransac-
[51] W.Zhan,L.Sun,D.Wang,H.Shi,A.Clausse,M.Naumann,J.Kum- tions of the Royal Society A: Mathematical, Physical and Engineering
merle, H. Konigshof, C. Stiller, A. de La Fortelle, et al., “Interaction Sciences,vol.368,no.1928,pp.4497–4517,2010.
dataset: An international, adversarial and cooperative motion dataset
in interactive driving scenarios with semantic maps,” arXiv preprint VII. APPENDIX
arXiv:1910.03088,2019.
[52] F.Poggenhans,J.-H.Pauls,J.Janosovits,S.Orf,M.Naumann,F.Kuhnt, A. Proofs
and M. Mayr, “Lanelet2: A high-definition map framework for the
futureofautomateddriving,”in201821stinternationalconferenceon 1) Proof of Proposition 1: Proof: The proof is by
intelligenttransportationsystems(ITSC),pp.1672–1679,IEEE,2018. induction. Assume U (h )=V (b ), then
t+1 t+1 t+1 t+1
[53] P.DeHaan,D.Jayaraman,andS.Levine,“Causalconfusioninimitation
(cid:40)
learning,”AdvancesinNeuralInformationProcessingSystems,vol.32, (cid:88)(cid:88)
2019. U t (h t )= max r(s,a)P(s t =s|h t )π(a|h t )−c((π(·|h t ))
[54] D.N.Lee,“Atheoryofvisualcontrolofbrakingbasedoninformation π(·|ht)
a s
abouttime-to-collision,”Perception,vol.5,no.4,pp.437–459,1976. (cid:41)
[55] M. Sva¨rd, G. Markkula, J. Ba¨rgman, and T. Victor, “Computational +γ (cid:88)(cid:88) P(o |h ,a)π(a|h )V (b )
modelingofdriverpre-crashbrakeresponse,withandwithoutoff-road t+1 t t t+1 t+1
glances: Parameterization using real-world crashes and near-crashes,” a ot+1
AccidentAnalysis&Prevention,vol.163,p.106433,2021. (cid:40)
[56] J.Engstro¨m,G.Markkula,Q.Xue,andN.Merat,“Simulatingtheeffect (cid:88)
= max r(s,a)b (s)π(a|b )−c((π(·|b ))
ofcognitiveloadonbrakingresponsesinleadvehiclebrakingscenarios,” t t t
IETIntelligentTransportSystems,vol.12,no.6,pp.427–433,2018.
π(·|bt)
a
(cid:41)
[57] A. D. McDonald, H. Alambeigi, J. Engstro¨m, G. Markkula, T. Vo-
(cid:88)(cid:88)
gelpohl,J.Dunne,andN.Yuma,“Towardcomputationalsimulationsof +γ σ(o |s ,a)π(a|b )V (b )
t+1 t t t+1 t+1
behaviorduringautomateddrivingtakeovers:areviewoftheempirical
andmodelingliteratures,”Humanfactors,vol.61,no.4,pp.642–688,
a ot+1
2019. =V (b )
t t
[58] M. Werling, J. Ziegler, S. Kammel, and S. Thrun, “Optimal trajectory
generationfordynamicstreetscenariosinafrenetframe,”in2010IEEE where b
t+1
(s) = P(s
t+1
= s|h
t
∪{a,o
t+1
}) and the second
International Conference on Robotics and Automation, pp. 987–993, equality follows from
IEEE,2010.
[59] K.P.Murphy,Machinelearning:aprobabilisticperspective.MITpress, P(o |h ,a)= (cid:88)(cid:88) O(o |s )T(s |s ,a)b (s )
2012. t+1 t t+1 t+1 t+1 t t t
[60] D. Sadigh, S. Sastry, S. A. Seshia, and A. D. Dragan, “Planning for st st+1
autonomouscarsthatleverageeffectsonhumanactions.,”inRobotics: =σ(o |b ,a)
t+1 t
Scienceandsystems,vol.2,pp.1–9,AnnArbor,MI,USA,2016.
[61] J.Spencer,S.Choudhury,A.Venkatraman,B.Ziebart,andJ.A.Bagnell, B. ProofofTheorem1
“Feedback in imitation learning: The three regimes of covariate shift,”
To prove (a), let Q ,Q ∈Q and ϵ=∥Q −Q ∥. Then
arXivpreprintarXiv:2102.02872,2021. 1 2 1 2
[62] C. Colas, O. Sigaud, and P.-Y. Oudeyer, “A hitchhiker’s guide to (cid:32) (cid:33)
statistical comparisons of reinforcement learning algorithms,” arXiv log (cid:88) π0(a|b)exp (cid:0)1 Q (b,a) (cid:1)
preprintarXiv:1904.06979,2019. α 1
[63] R.Agarwal,M.Schwarzer,P.S.Castro,A.C.Courville,andM.Belle- a
(cid:32) (cid:33)
m pr a e r c e i , pic “ e D ,” ee A p dv r a e n in ce fo s r i c n em ne e u n r t al le i a n r f n o i r n m g ati a o t n t p h r e oc e e d s g si e ng o s f ys t t h e e ms s , ta v t o is l. ti 3 c 4 al , ≤log (cid:88) π0(a|b)exp (cid:0)1 Q (b,a)+ϵ (cid:1)
pp.29304–29320,2021. α 2
a
[64] “Sedan vehicle dimensions.” https://www.mathworks.com/ (cid:32) (cid:33)
help/driving/ref/sedan.html,2022. Accessed:2022-12-15. =log exp(ϵ) (cid:88) π0(a|b)exp (cid:0)1 Q (b,a) (cid:1)
[65] C.Rudin,“Stopexplainingblackboxmachinelearningmodelsforhigh α 2
stakesdecisionsanduseinterpretablemodelsinstead,”NatureMachine a
Intelligence,vol.1,no.5,pp.206–215,2019. (cid:32) (cid:33)
[66] T. Ra¨ukur, A. Ho, S. Casper, and D. Hadfield-Menell, “Toward trans- =ϵ+log (cid:88) π0(a|b)exp (cid:0)1 Q (b,a) (cid:1)
parent ai: A survey on interpreting the inner structures of deep neural α 2
networks,”arXivpreprintarXiv:2207.13243,2022. a
[67] R. Fuller, “Towards a general theory of driver behaviour,” Accident Similarly, we have
analysis&prevention,vol.37,no.3,pp.461–472,2005.
(cid:32) (cid:33)
[68] P J. ic E c n in g i s n t i r , o¨ a m n , d J T . . B V a¨ i r c g t m or a , n “ , G D re . at N e i x ls p s e o c n t , at B io . n S s: ep a p p e r l e t, di G ct . iv M e a p r r k o k c u e l s a s , in G g . a B c- . log (cid:88) π0(a|b)exp (cid:0)1 Q (b,a) (cid:1)
countofautomobiledriving,”Theoreticalissuesinergonomicsscience, α 1
a
vol.19,no.2,pp.156–194,2018. (cid:32) (cid:33)
[69] H.Summala,“Hierarchicalmodelofbehaviouraladaptationandtraffic ≥−ϵ+log (cid:88) π0(a|b)exp (cid:0)1 Q (b,a) (cid:1)
accidents,” Traffic and transport psychology. Theory and application, α 2
1997. a
[70] P. Hancock, “Is car following the real question–are equations the
Hence, we obtain that
answer?,” Transportation research part F: traffic psychology and be-
haviour,vol.2,no.4,pp.197–199,1999.
∥BQ −BQ ∥≤γ∥Q −Q ∥=γϵ.
[71] A.Tamar,Y.Wu,G.Thomas,S.Levine,andP.Abbeel,“Valueiteration 1 2 1 2
networks,”Advancesinneuralinformationprocessingsystems,vol.29,
To prove (b), consider the policy of the form:
2016.
[72] P. Karkus, D. Hsu, and W. S. Lee, “Qmdp-net: Deep learning for π0(a|b)exp (cid:0)1Q∗(b,a) (cid:1)
planning under partial observability,” Advances in neural information π∗(a|b)= α .
processingsystems,vol.30,2017. (cid:80) π0(a′|b)exp (cid:0)1Q∗(b,a′) (cid:1)
a′∈A α
14
where Q∗ is the unique fixed point of B. We first note that 1) BC Implementation: For BC-MLP, we used a two-layer
MLP network with ReLU activation and 40 hidden units in
1
logπ∗(a|b)=logπ0(a|b)+ Q∗(b,a)
each layer. For BC-RNN, we used a two-layer MLP network
α
−log (cid:88) π0(a′|b)exp (cid:0)1 Q∗(b,a′) (cid:1) o an n d to 3 p 0 of hi a dd s e in n gl u e n -l i a ts ye i r n G e R a U ch ne la tw ye o r r . k T w h i e th G R R e U LU la a y c e ti r va o ti n o l n y
α
a′ takes in past observations but not past actions. We found that
Thus, larger number of hidden units in the BC-RNN model led to
significant overfitting. Both BC-MLP and BC-RNN receive 3
1(cid:104)(cid:88) (cid:105)
α
Q∗(b,a)π∗(a|b)−αD
KL
(π∗(·|b)||π0(·|b))inputobservationsandoutputprobabilitydistributionsover15
a discrete actions.
= (cid:88) π∗(a|b) (cid:104) log (cid:88) π0(a′|b)exp (cid:0)1 Q∗(b,a′) (cid:1) +log π∗(a|b)(cid:105) 2) AIDA Implementation: The AIDA implementation fol-
α π0(a|b) lows the value-iteration network and QMDP network [71],
a a′
−D (π∗(·|b)||π0(·|b)) [72] to enable end-to-end training in Pytorch [73]. We used a
KL
state dimension of 20, action dimension of 15, and a planning
=log (cid:88) π0(a′|b)exp (cid:0)1 Q∗(b,a′) (cid:1) .horizon of 30 steps (3 seconds). Discrete state transition
α
a′ probabilities are parameterized using categorical distributions.
(20) The continuous observation distributions are parameterized
Moreover, for any policy π ̸=π∗, it holds that using a set of Gaussian distributions, one for each discrete
state, and a shared noramlizing flow network to transform
1(cid:104)(cid:88)
Q∗(b,a)π(a|b)−αD (π(·|b)||π0(·|b))
(cid:105)
the base Gaussian distributions into more flexible density
α KL estimators. Specifically, we use inverse autoregressive flow
a
(cid:88) 1 π(a|b) [49] parameterized by a two-layer MLP network with ReLU
= π(a|b)[ Q∗(b,a)−log ]
α π0(a|b) activation and 30 hidden units in each layer.
a For each mini-batch of observation-action sequences, we
(cid:88) π∗(a|b)
= π(a|b)[log first computed the likelihood of the observations at all time
π0(a|b)
steps and compute the belief at each time step as:
a
+log (cid:88) π0(a′|b)exp (cid:0) α 1 Q∗(b,a′) (cid:1) −log π π 0 ( ( a a | | b b ) ) ] b(s )= P(o t |s t ) (cid:80) st−1 P(s t |s t−1 ,a t−1 )b(s t−1 ) . (22)
a′ t (cid:80) P(o |s ) (cid:80) P(s |s ,a )b(s )
=−D (π(·|b)||π∗(·|b))+log (cid:88) π0(a′|b)exp (cid:0)1 Q∗(b,a′) (cid:1)
st t t st−1 t t−1 t−1 t−1
KL α We then computed the value function (8) for the EFE reward
a′
and the resulting optimal policy in (7) for each inferred belief
(21)
using the QMDP approximation method [50]. The QMDP
Since D KL (π(·|b)||π∗(·|b)) ≥ 0 we conclude from (20) and method assumes the belief-action value can be approximated
(21) that as a weighted-average of the state-action value:
αlog (cid:88) π0(a′|b)exp (cid:0)1 Q∗(b,a′) (cid:1) Q∗(b ,a )= (cid:88) b (s )Q∗(s ,a ), (23)
t t t t t t
α
a′ st
(cid:88)
= Q∗(b,a)π∗(a|b)−αD KL (π∗(·|b)||π0(·|b)) where
a
= max[ (cid:88) Q∗(b,a)π(a|b)−αD (π(·|b)||π0(·|b))]=V∗(b) Q∗(s t ,a t )=r(s t ,a t )+logπ(a t |s t )+
KL
π(·|b) a + (cid:88) P(s |s ,a )V∗(s ), (24)
t+1 t t t+1
To prove (c), we apply (20) and (21) to π to conclude that st+1
V∗(b)= max[ (cid:88) Q∗(b,a)π(a|b)−αD (π(·|b)||π0(·|b))] r(s t ,a t )=EFE(s t ,a t )=D KL (P(s t+1 |s t ,a t )||P˜(s t+1 ))
π(·|b) a KL +E P(st+1|st,at) [H(P(o t+1 |s t+1 ))]. (25)
(cid:88)(cid:88)
= max[ r(s,a)b(s)π(a|b)−αD KL (π(·|b)||π0(·|b))) and ∀s ∈ S,Q∗(s t+H+1 ) = 0. We further approximate the
π(·|b)
a s observation entropy using the entropy of the Gaussian base
(cid:88)(cid:88)
+γ σ(o′|b,a)π(a|b)V∗(b′)] distributions of the normalizing flows which can be computed
a o′ in closed form.
where b′ is the updated Bayes belief distribution after action The combination of QMDP approximation and computing
a is implemented and observation o′ is recorded and the the observation entropy in (25) using the Gaussian base
optimality of π∗ follows from Proposition 1. distributions reduced the model’s ability to evaluate state
uncertainty. However, given the low state uncertainty shown
in Figure 9 and Figure 10 (i.e., the nearly deterministic belief
C. Implementationdetails
states in the lower right charts), these approximations do not
Thesourcecodeisavailableathttps://github.com/ significantly impact the current results while providing the
ran-weii/interactive_inference. benefit of computational tractability.
15
3) Training and hyperparameters: We trained all models D. Simulationplatform
usingtheAdamoptimizerforafixednumberofepochswhich
We built a single-agent simulator for online evaluation of
are selected upon visual inspection of convergence, i.e., the
the trained models. The simulator plays back lead vehicle
loss function no longer changes significantly. For all models,
trajectoriesfromthedatasetwhichisconvertedintotheFrenet
we use a batch size of 100. For AIDA, we use a smaller
frame to compute LV related observations for the ego vehicle.
learning rate of 0.001 for the normalizing flow network than
Wemodeltheeffectofegocontrolactionsonitsownposition
therestofthemodelbecauseofitssensitivitytolargelearning
and velocity in the Frenet frame using linear dynamics:
rates. Additional hyperparameters are reported in table I.
 x′  1 0 ∆t 0  x   0.5∆t2 0 
TABLEI: Training hyperparameters   y′ =   0 1 0 ∆t    y +   0 0.5∆t2  (cid:20) a x (cid:21)
v x ′  0 0 1 0 v x  ∆t 0  a y
Hyperparameter IDM BC-MLP BC-RNN AIDA v′ 0 0 0 1 v 0 ∆t
y y
Learningrate 0.005 0.001 0.001 0.01
(26)
Trainingepochs 300 500 500 500
Since the models only control longitudinal action a , we
x
used a simple feedback controller for lateral actions with
TABLEII: Model input features position and velocity gain [−0.01,−0.2].
Feature IDM BC-MLP BC-RNN AIDA
Distanceheadway(d) Yes Yes Yes Yes
Relativespeed(∆v) Yes Yes Yes Yes
Speed(v) Yes No No No
τ−1 No Yes Yes Yes
TABLEIII: Model parameter counts
IDM BC-MLP BC-RNN AIDA
Count 6 4125 6465 7670
TABLEIV: Fitted IDM parameters: mean and standard devia-
tions across 15 seeds.
v˜ τ d0
12.2±0.2 0.83±0.03 1.07±0.07
amax b σ
0.21±0.006 2.68±0.19 0.46±0.004
TABLE V: Two-sided Welch’s t-test results of offline MAE-
IQM against baseline models. Asterisks indicate statistical
significance with α=0.05.
Baseline Comparison t(df=14) p-value
Fig.11:ComparisonofofflineMAEbyeachmodelwithIQM
IDM dense-lane t=16.38 p<0.001*
BC-MLP dense-lane t=29.74 p<0.001* (top) and without IQM (bottom).
BC-RNN dense-lane t=16.03 p<0.001*
IDM sparse-lane t=29.11 p<0.001*
BC-MLP sparse-lane t=0.44 p=0.66
BC-RNN sparse-lane t=-0.04 p=0.97 E. ComparisonofaveragemetricsandIQM
This section compares model offline (Figure 11) and online
(Figure 12) evaluation performance with and without IQM. In
TABLE VI: Two-sided Welch’s t-test results of online ADE-
the offline evaluation setting, IQM did not substantially affect
IQM against baseline models. Asterisks indicate statistical
model performance, e.g., the IDM MAE in the sparse-lane
significance with α=0.05. settingwasonlyreducedby0.1m/s2.Intheonlineevaluation
Baseline Comparison t(df=14) p-value setting,IQMreducedtheaverageADEofBC-RNNandAIDA
IDM dense-lane t=3.05 p<0.01* by1mandsubstantiallyreducedtheuppertailofAIDAseeds.
BC-MLP dense-lane t=-5.46 p<0.001*
However, IDM also increased the difference between IDM’s
BC-RNN dense-lane t=8.73 p<0.001*
IDM sparse-lane t=58.18 p<0.001* ADE in the dense-lane and sparse-lane settings, which is
BC-MLP sparse-lane t=-3.77 p<0.001* consistent with IDM having worse offline prediction accuracy
BC-RNN sparse-lane t=6.87 p<0.001*
in the sparse-lane setting as shown in Figure 3. Overall, IQM
did not change the ranking of models. It should be noted that
16
Fig.12: Comparison of online ADE of each model with IQM
(top) and without IQM (bottom).
in the estimated IDM model, the parameter estimate a is
max
significantly lower than that reported in [74].
F. Statisticaltestingresults
Statistical tests using the two-sided Welch’s t-test with 5
percentrejectionlevelareshowninTableVandVIforoffline
and online evaluations, respectively.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Learning An Active Inference Model of Driver Perception and Control: Application to Vehicle Car-Following"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
