=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: On efficient computation in active inference
Citation Key: paul2023efficient
Authors: Aswin Paul, Noor Sajid, Lancelot Da Costa

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2023

Abstract: Despitebeingrecognizedasneurobiologicallyplausible,activeinferencefacesdifficultieswhenem-
ployedtosimulateintelligentbehaviourincomplexenvironmentsduetoitscomputationalcostand
thedifficultyofspecifyinganappropriatetargetdistributionfortheagent.Thispaperintroducestwo
solutionsthatworkinconcerttoaddresstheselimitations. First,wepresentanovelplanningalgo-
rithmforfinitetemporalhorizonswithdrasticallylowercomputationalcomplexity.Second,inspired
byZ-learningfromcontroltheoryliterature,wesimplifythep...

Key Terms: computation, mumbai, efficient, india, inference, active, defining, first, vixra, distribution

=== FULL PAPER TEXT ===

3202
luJ
2
]GL.sc[
1v40500.7032:viXra
ON EFFICIENT COMPUTATION IN ACTIVE INFERENCE
A PREPRINT
AswinPaul1,2,3,NoorSajid4,LancelotDaCosta4,5,6andAdeelRazi1,4,7
1TurnerInstituteforBrainandMentalHealth,MonashUniversity,Clayton3800,Australia
2IITB-MonashResearchAcademy,Mumbai,India
3DepartmentofElectricalEngineering,IITBombay,Mumbai,India
4WellcomeCentreforHumanNeuroimaging,UniversityCollegeLondon,WC1N3ARLondon,UnitedKingdom
5DepartmentofMathematics,ImperialCollegeLondon,London,SW72AZ,UK
6VERSESResearchLab,LosAngeles,CA90016,USA
7CIFARAzrieliGlobalScholarsProgram,CIFAR,Toronto,Canada
July4,2023
ABSTRACT
Despitebeingrecognizedasneurobiologicallyplausible,activeinferencefacesdifficultieswhenem-
ployedtosimulateintelligentbehaviourincomplexenvironmentsduetoitscomputationalcostand
thedifficultyofspecifyinganappropriatetargetdistributionfortheagent.Thispaperintroducestwo
solutionsthatworkinconcerttoaddresstheselimitations. First,wepresentanovelplanningalgo-
rithmforfinitetemporalhorizonswithdrasticallylowercomputationalcomplexity.Second,inspired
byZ-learningfromcontroltheoryliterature,wesimplifytheprocessofsettinganappropriatetarget
distribution for new and existing active inferenceplanning schemes. Our first approachleverages
thedynamicprogrammingalgorithm,knownforitscomputationalefficiency,tominimizethecost
functionusedinplanningthroughtheBellman-optimalityprinciple. Accordingly,ouralgorithmre-
cursivelyassessestheexpectedfreeenergyofactionsinthereversetemporalorder. Thisimproves
computationalefficiency by ordersof magnitude and allows precise modellearning and planning,
evenunderuncertainconditions. Ourmethodsimplifiestheplanningprocessand showsmeaning-
fulbehaviourevenwhenspecifyingonlytheagent’sfinalgoalstate. Theproposedsolutionsmake
defining a targetdistribution from a goalstate straightforwardcomparedto the more complicated
task of defining a temporally informed target distribution. The effectiveness of these methods is
tested and demonstrated throughsimulations in standard grid-worldtasks. These advancescreate
newopportunitiesforvariousapplications.
Keywords Activeinference·Dynamicprogramming·Stochasticcontrol·Reinforcementlearning
1 Introduction
How should an organismperceive, learn, and act to ensure survival when born into a new world? How do ‘agents’
eventuallylearntoexhibitsentientbehaviourinnature,suchashuntingandnavigation?
A prominentframeworkthat approachesthese questions is stochastic optimal control(SOC), which determines the
best possible set of decisions—given a specific criterion—at any given time and in the face of uncertainty. The
fundamentalproblemthatSOCaddressescanbedefinedasfollows: Whenbornattimet = 1andahead,an‘agent’
receivesobservationsfromits surrounding‘environment’. This‘agent’notonlypassivelyreceivesobservationsbut
also is capableof respondingwith ‘actions’. Additionally,itmay receiveinformationorhasinbuiltrewardsystems
that quantifyits chanceof survivaland progress. So, this processmay be summarisedas a stream of data from the
agent’sperspective:(o ; a ),(o , r ; a ),...,(o , r ). Here,o standsfortheobservationattimet,a standsforthe
1 1 2 2 2 t t t t
agent’sactionattimet,andr standsforthe‘reward’attimetfromtheexternalenvironmentoragent’sinbuiltreward
t
structure.Inthissetting,theprimarygoalofanagentisto
Onefficientcomputationinactiveinference APREPRINT
t
Maximise:Score= r .1 (1)
t
X1
Eq.1 is an optimisation problem, and due to its general structure, it has a vast scope in various disciplines across
the sciences. Several fields of research grew aroundthis idea in the past decades, like reinforcementlearning (RL)
[SuttonandBarto, 2018], controltheory[Todorov,2006,2009], gametheory[FudenbergandTirole, 1991,Luetal.,
2020],andeconomics[Mookherjee,1984,VonNeumannandMorgenstern,1944]. Butinfact,formulatingdecision-
makingas utility maximisationoriginatedmuchearlier in theethicaltheoryofutilitarianism in 18th-centuryphilos-
ophy[Bentham, 1781, Mill, 1870], and was later appliedby Pavlov in the early 20th centuryto accountfor animal
conditioning [Pavlov, 1927]. Many current engineering methods, such as Q-learning [WatkinsandDayan, 1992],
build uponthe Bellman-optimalityprinciple to learn properobservation-actionmappingsthat maximise cumulative
reward. Model-basedmethodsinRL,likeDyna-QPengandWilliams[1993],employaninternalmodelofthe‘envi-
ronment’toacceleratethisplanningprocessSuttonandBarto[2018].Similarly,efficientmethods,e.g.,whichlinearly
scaleswiththeproblemdimensions,emergedinclassicalcontroltheorytocomputeoptimalactionsinsimilarsettings
[Todorov,2006,2009].
Another critical and complementary research direction is studying systems showing ‘general intelligence’, which
aboundsinnature. Indeed,weseeaspectrumofbehaviourinthenaturalworldthatmayormaynotbeaccountable
bytherathernarrowgoalofoptimisingcumulativereward. Bylearningmoreabouthowthebrainproducessentient
behaviour,wecanhopetoacceleratethegenerationofartificialgeneralintelligence[Goertzel,2014,Gershman,2023].
Thisoutlookmotivatesus to lookinto the neuralandcognitivesciences, whereanintegraltheoryisthe freeenergy
principle(FEP),whichbringstogetherHelmholtz’searlyobservationsofperceptionwithmorerecentideasfromsta-
tistical physicsand machinelearning[Feynman, 1998, Dayanetal., 1995] to attempta mathematicaldescriptionof
brainfunctionandbehaviourintermsofinferencethathasthepotentialofunifyingmanyprevioustheoriesonthesub-
ject,includingbutnotlimitedtocumulativerewardmaximisation[Friston,2010,Fristonetal.,2022,DaCostaetal.,
2023].
In the last decade, the FEP has been applied to model and generate biological-like behaviour under the banner of
activeinference[DaCostaetal.,2020]. Activeinferencehassincepercolatedintomanyadjacentfieldsowingtoits
ambitiousscopeasageneralmodellingframeworkforbehaviour[Pezzatoetal.,2023,Oliveretal.,2022,Deaneetal.,
2020,Rubin,2020,Fountasetal.,2020,Matsumotoetal.,2022].Inparticular,severalrecentexperimentspositactive
inferenceasapromisingapproachtooptimalcontrolandexplainableandtransparentartificialintelligenceFristonetal.
[2009],Friston[2012],Sajidetal.[2021a],Mazzagliaetal.[2022],Millidgeetal.[2020],Albarracinetal.[2023]. In
thisarticle,weconsideractiveinferenceasanapproachtostochasticcontrol,itscurrentlimitations,andhowtheycan
beovercomewithdynamicprogrammingandtheadequatespecificationofatargetdistribution.
In the following three sections, we consider the active inference framework, discuss existing ideas accounting for
perception,planninganddecision-making—andidentifytheirlimitations. Next,inSection5,weshowhowdynamic
programming can address these limitations by enabling efficient planning and can scale up existing methods. We
formalisetheseideasinapracticalalgorithmforpartiallyobservedMarkovdecisionprocesses(POMDP)inSection5.1.
Then we discuss the possibility of learning the agent’s preferencesby building upon Z-learning[Todorov, 2006] in
Section6. WeshowcasetheseinnovationswithillustrativesimulationsinSection8.
2 Activeinference as biologicallyplausibleoptimalcontrol
Theactiveinferenceframeworkisaformalwayofmodellingthebehaviourofself-organisingsystemsthatinterface
with the external world and maintain a consistent form over time Fristonetal. [2021], KaplanandFriston [2018],
Kuchlingetal.[2020].Theframeworkassumesthatagentsembodygenerativemodelsoftheenvironmenttheyinteract
with,onwhichtheybasetheir(intelligent)behaviour[Tschantzetal.,2020,ParrandFriston,2018]. Theframework,
however, does not impose a particular structure on such models. Here, we focus on generative models in the form
of partially observed Markov decision processes (POMDPs) for their simplicity and ubiquitous use in the optimal
controlliteratureLovejoy[1991],Shanietal.[2013],Kaelblingetal.[1998]. Inthenextsection,wediscussthebasic
structureofPOMDPsandhowtheactiveinferenceframeworkusesthem.
1Rewardscoresthedesirabilityforaparticularoutcomeorstate;akintosomecostfunction.Briefly,itcanbeexplicitlydefined
bythe’external’environment(extrinsicreward)orinternallybytheagentitself(intrinsicreward).
2
Onefficientcomputationinactiveinference APREPRINT
2.1 GenerativemodelsusingPOMDPs
Assumingagentshaveadiscreterepresentationoftheirsurroundingenvironment,weturntothePOMDPframework
[Kaelblingetal.,1998].POMDPsofferafairlyexpressivestructuretomodeldiscretestate-spaceenvironmentswhere
parameterscan be expressedas tractable categoricaldistributions. The POMDP-basedgenerativemodelcan be for-
mallydefinedasatupleoffinitesets(S,O,U,B,A):
◦ s∈S :S isasetofhiddenstates(s)causingobservationso.
◦ o ∈ O : O is a set of observations, where o = s, in the fully observablesetting. In a partiallyobservable
setting,o=f(s).
◦ u∈U :U isasetofactions(u)Eg:U ={Left,Right,Up,Down}.
◦ B :encodestheone-steptransitiondynamics,P(s |s ,u )i.e.,theprobabilitythatwhenactionu is
t t−1 t−1 t−1
takenwhilebeinginstates (attimet−1)resultsins attimet.
t−1 t
◦ A:encodesthelikelihoodmapping,P(o |s )forthepartiallyobservablesetting.
τ τ
◦ D:Encodestheprioroftheagentaboutthehiddenstatefactors.
◦ E:Encodestheprioroftheagentaboutactionsu.
In a POMDP, the hidden states (s) generate observations (o) through the likelihood mapping (A) in the form of a
categorical distribution, P(o |s ) = Cat(A ×s ). B is a collection of square matrices B , where B represents
τ τ τ u u
transitiondynamicsP(s |s ,u = u): Thetransitionmatrix(B)determinesthedynamicsofsgiventheagent’s
t t−1 t−1
actionuasP(s |s ,u )=Cat(B ×s ). In[A×s ]and[B ×s ],s isrepresentedasaone-hotvector
t t−1 t−1 ut−1 t−1 τ uτ τ τ
thatismultipliedthroughregularmatrixmultiplication2. TheMarkovianityofPOMDPsmeansthatstatetransitions
areindependentofhistory(i.e. states onlydependsuponthestate-actionpair(s ,u )andnots , u etc.).
t t−1 t−1 t−2 t−2
Insummary,thegenerativemodelcanbesummarisedasfollows,
t t
P(o ,s ,u )=P(A)P(B)P(D)P(E) P(o |s ,A) P(s |s ,u ,B). (2)
1:t 1:t 1:t τ τ τ τ−1 τ−1
τY=1 τY=2
So, fromthe agent’sperspective,whenencounteringa stream of observationsin time, such as(o ,o ,o ,...,o ), as
1 2 3 t
a consequence of performinga stream of actions (u ,u ,u ,...,u ), the generative model quantitativelycouples
1 2 3 t−1
andquantifiesthecausalrelationshipfromactiontoobservationthroughsomeassumedhiddenstatesoftheenviron-
ment. These are called‘hidden’states because, in POMDPs, the agentcannotobservethem directly. Based onthis
representation,anagentcannowattemptto optimiseits actionstokeepreceivingpreferredobservations. Currently,
the generativemodelhasno conceptof ‘preference’and ‘goal’[Bruinebergetal., 2018]. Rather thanattemptingto
maximise cumulative reward from the environment, active inference agents minimise the ‘surprise’ of encountered
observations[Sajidetal.,2021a,b]. Welookatthisideacloselyinthenextsection.
2.2 Surpriseandfreeenergy
Thesurpriseofagivenobservationinactiveinference[Friston,2019,Sajidetal.,2021a]isdefinedthroughtherelation
S(o)=−log(P(o)). (3)
Please note that the agent does not have access to the true probability of an observation: P (o). However, the
true
internal generativemodel expectsan observationwith a certain probabilityP(o), which quantifies surprise in Eq.3.
Minimisingsurprisedirectlyrequiresthemarginalisationofthegenerativemodel,i.e.,P(o) = P(o,s),whichis
s
oftencomputationallyintractableduetothelargesizeofthestate-space[Bleietal.,2017,SajidPetal.,2022a]. Since
f(x)=log(x)isaconvexfunction,wecansolvethisproblembydefininganupper-boundtosurpriseusingJensen’s
inequality3:
P(o,s)
S(o)=−log P(o,s)≤− Q(s)log =F[Q]. (4)
Q(s)
Xs Xs
2One-hotisagroupofbitsamongwhichthelegalcombinationsofvaluesareonlythosewithasinglehigh(1)bitandallthe
otherslow(0).Here,thebit(1)isallocatedtothestates=sτ
3Jensen’sinequality:IfXisarandomvariableandψisaconvexfunction,ψ(E[X])≤E[ψ(X)].
3
Onefficientcomputationinactiveinference APREPRINT
ThenewlyintroducedtermQ(s)isofteninterpretedasan(approximateposterior)beliefaboutthe(hidden)state: s.
Thisupperbound(F)iscalledthevariationalfreeenergy(VFE)(itisalsocommonlyknownasevidencelowerbound
–ELBO[Bleietal.,2017]4). So,byoptimisingthebeliefQ(s)tominimisethevariationalfreeenergy(F),anagent
iscapableofminimisingthesurpriseS(o)=−log(P(o))oratleastmaintainitboundedatlowvalues.
How is this formulation useful for stochastic control? Imagine the agent embodies a biased generative model
with‘goal-directed’expectationsforobservations.ThegoalthenbecomestominimiseF,whichcanbedonethrough
the conjunctionof perception,i.e., optimisingthebelief Q(s), or action, i.e., controllingthe environmentto sample
observationsthatleadtoalowerF [Tschantzetal.,2020].So,insteadofpassivelyinferringwhatcausedobservations,
the agent starts to ‘actively’ infer, exerting control over the environmentusing available actions in U. The central
advantageofthisformalismisthatthereisnowonlyonesinglecostfunction(F)tooptimiseallaspectsofbehaviour,
such as perception, learning, planning, and decision-making (or action selection). There are related works in the
reinforcement literature noting the use of similar information-theoretic metrics for control Rhinehartetal. [2021],
Bersethetal. [2019]. Thefollowingsectiondiscussesthisfeaturein detailandfurtherdevelopstheactiveinference
framework.
3 Perception andlearning
3.1 Perception
Fromtheagent’sperspective,perceptionmeans(Bayesoptimally)maintainingabeliefabouthiddenstatesscausing
theobservationso. Inactiveinference,agentsoptimisethebeliefsQ(s)tominimiseF. TheVFE mayberewritten
(fromEq.4),usingtheidentityP(o,s)=P(s)P(o|s),as:
F = Q(s)[logQ(s)−logP(o|s)−logP(s)]. (5)
Xs
DifferentiatingF w.r.tQ(s)andsettingthederivativetozero,weget(seeSupplementaryA),
δF
= 1+logQ(s)−logP(o|s)−logP(s)=0. (6)
δQ(s)
Xs
Usingtheaboveequation,wecanevaluatetheoptimalQ(s)thatminimises5F using,
logQ∗(s)=logP(s)+logP(o|s). (7)
Thisequationprovidesthe(Bayesian)beliefpropagationscheme,givenby
Q(s
t+1
)=σlogP(s
t+1
)+log(o
t+1
·As
t+1
). (8)
Posterior  Prior Likelihood 
| {z } | {z } | {z }
Here, σ is the softmax function; that is, the exponentialof the input that is then normalised so that the output is a
probabilitydistribution.Givenareal-valuedvectorV inRK,thei-thelementofσ(V)reads:
expVi
σ(V)i = , (9)
K expVj
j=1
P
whereVi correspondstothei-thelementofV. WeestimatethefirsttermofEq.8,i.e. thepriorusingbeliefQ(s )at
t
timet,andtheactionu takenattimet. UsingthetransitiondynamicsoperatorB ,wewrite:
t ut
P(s )=B ·Q(s ). (10)
t+1 ut t
4Theconnectionbetweenthetwoliesinthefactthattheyareessentiallyequivalentuptoaconstant(thelogevidence),butwith
oppositesigns. Inotherwords,minimizingVFEisequivalenttomaximizingtheELBO.Formallythisis: VFE = −ELBO+
constant
5ThesecondderivateofEq.6w.r.ttoQ(s)isgreaterthanzerowhichcorrespondstolocalminimaofF w.r.ttoQ(s).
4
Onefficientcomputationinactiveinference APREPRINT
At the first time step, i.e. t = 0, we use a knownprioraboutthe hiddenstate D to substitute for the term P(s ).
t+1
Similarly, the second term in Eq.8, i.e., the estimate of the hidden state from the observationwe gatheredfrom the
environmentattimet+1canbeevaluatedasthedotproductbetweenthelikelihoodfunctionAandtheobservation
gatheredattime t+1. The beliefpropagationschemehereisshowninthe literatureto havea degreeofbiological
plausibilityinthesensethatitcanbeimplementedbyalocalneuronalmessage-passingscheme[deVriesandFriston,
2017]. Thefollowingsectiondiscussesthelearningofthemodelparameters.
3.2 Learning
The parameterlearningrulesof ourgenerativemodelaredefinedintermsofthe optimisedbeliefaboutstates Q(s).
In ourarchitecture, the agentuses belief propagation6 to best estimate Q(s), the beliefabout(hidden)states in the
environment.Giventhesebeliefs,observationssampled,andactionsundertakenbytheagent,theagenthopestolearn
theunderlyingcontingenciesoftheenvironment.Thelearningrulesofactiveinferenceconsistofinferringparameters
ofA,B,andDataslowertimescale. Wediscusssuchlearningrulesindetailinthefollowing.
3.2.1 Transitiondynamics
Agents learn the transition dynamics, B, across time by maintaining a concentration parameter b , using conjugate
u
updateruleswelldocumentedintheactiveinferenceliterature[Fristonetal.,2017,DaCostaetal.,2020,Sajidetal.,
2021a]suchas:
b ←b +Q(u )·(Q(s )⊗Q(s )), (11)
u u t−1 t t−1
whereQ(u)istheprobabilityoftakingactionu,Q(s )isbeliefthestateattimetasaconsequenceofactionuatt−1,
t
andQ(s )⊗Q(s )representsasquarematrixofKroneckerproductbetweentwovectorsQ(s )andQ(s ).
t t−1 t t−1
EverycolumnofthetransitiondynamicsB ,canbeestimatedfromb column-wiseas,
u u
col(B ) =Dir[col(b ) ]. (12)
u i u i
Here,col(X) isthei-thcolumnofX.Dir(b )representsthemeanoftheDirichletdistribution7withparameterb .
i u u
3.2.2 Likelihood
SimilartotheconjugacyupdateinEq.11,theDirichletparameter(a)forthelikelihooddynamics(A)islearnedover
timewithintrialsusingtheupdaterule,
a←a+o ⊗Q(s ). (13)
t t
Here,o istheobservationgatheredfromenvironmentattimet,andQ(s )≈P(s |o )istheapproximateposterior
t t t 1:t
beliefaboutthehidden-state(s)[Fristonetal.,2017,DaCostaetal.,2020].
Likeperceptionandlearning,decision-makingandplanningcan alsobe formulatedaroundthecostfunctionF and
beliefQ. Inthenextsection,wereviewindetailexistingideas[Fristonetal.,2021,Sajidetal.,2022b]forplanning
anddecision-making.Wethenidentifytheirlimitationsand,next,proposeanimprovedarchitecture.
4 Planning and decisionmaking
4.1 Classicalformulationofactiveinference
Traditionally, planning and decision-making by active inference agents revolve around the goal of minimising the
variational free energy of the observations one expects in the future. To implement this, we define a policy space
6We sticktothebelief propagation scheme for perception inthispaper. However, general schemes likevariational message
passingmaybeusedtoestimateQ(s).
7Dirichletdistributionsarecommonly usedaspriordistributionsinBayesianstatisticsgiven thattheDirichletdistributionis
theconjugatepriorofthecategoricaldistributionandmultinomialdistribution. WeusethemeanoftheDirichletdistributionhere
becauseweareperformingaBayesianupdate. Briefly,themeanofaDirichletdistributionwithparametersb = (b1,...,bK)is
b
given by µ = (µ1,...,µK) where µk =
PK
j=
k
1
b
j
. So, in this case, each entry in the estimated transition probabilities is the
correspondingentryinbu,dividedbythesumofallentriesinthecorrespondingcolumnofbu.Thisnormalizationensuresthatthe
columnsofB usumto1.
5
Onefficientcomputationinactiveinference APREPRINT
comprisingsequencesofactionsintime. Thepolicyspaceinclassicalactiveinference[Sajidetal.,2021a]isdefined
asacollectionofpoliciesπ
n
Π={π ,π ,...,π }, (14)
1 2 N
which are themselves sequences of actions indexed in time; that is, π = (u ,u ,...,u ), where u is one of the
1 2 T t
available action in U, and T is the agent’s planning horizon. N is the total number of unique policies defined by
permutationsofavailableactionsuoveratimehorizonofplanningT.
Toenablegoal-directedbehaviour,weneedawaytoquantifytheagent’spreferenceforsampleobservationso. The
priorpreferenceforobservationsisusuallydefinedasacategoricaldistributionoverobservations,
C=Cat(o). (15)
So,ifthevaluecorrespondingtoanobservationinCisthehighest,itisthemostpreferredobservationfortheagent.
Giventhesetwoadditionalparameters(ΠandC),wecandefineanewquantitycalledtheexpectedfreeenergy(EFE)
ofapolicyπsimilartothedefinitionin[Sajidetal.,2021a,Schwartenbecketal.,2019,ParrandFriston,2019]as,
T
G(π)= D Q(o |πt)||C +E [H[P(o |s )]]. (16)
KL t Q(st|st−1,πt−1) t t
Xt=1 (cid:2) (cid:3)
Risk Expectedambiguity
| {z } | {z }
InEq.(16)above,πt isthet-thelementinπ,i.e. theactioncorrespondingtotimetforpolicyπ. Theterm,Q(o |πt)
t
representsthe most likely observationcaused by the policyπ at time t. D stands forthe KL-divergence,which,
KL
when minimised, forcesthe distributionQ(o |πt) closer towardsC. This term is also called the "Risk" term, repre-
t
sentingthegoal-directedbehaviouroftheagent. TheKL-divergencebetweentwodistributions,P andQ,isdefined
as:
P(i)
D (P||Q)= P(i)log , (17)
KL
Q(i)
Xi
andP =QifandonlyifD (P||Q)=0.
KL
InthesecondtermofEq.16,H[P(o |s )]standsforthe(Shannon)entropyofP(o |s )definedas,
t t t t
H(P(o))=− P(o)logP(o). (18)
XoǫO
Thesecondtermisalsocalledthe‘Expectedambiguity’term. WhentheexpectedentropyofP(o |s )w.r.tthebelief
t t
Q(s |s ,πt−1)isless,theagentismoreconfidentofthestate-observationmapping(i.e.,A)initsgenerativemodel.
t t−1
Hence, by choosing the policy π to make decisions that minimise G, the agent minimises ‘Risk’ at the same time
andalsoits‘Ambiguity’aboutthestate-observationmapping. Hence,inactiveinference,decision-makingnaturally
balancesthe exploration-exploitationdilemma[Tricheetal., 2022]. We alsonote thatthe agentisnotoptimisingG
butonlyevaluatingandcomparingvariousGoverdifferentpoliciesπ inthepolicyspaceΠ. Oncethebestpolicyπ
isidentified,themostsimpledecision-makingrulefollowsbychoosingactionsu =πtattimet,whereπtisthet-th
t
elementofπ.
Itmayalreadybeevidentthattheaboveformulationhasonefundamentalproblem:inthestochasticcontrolproblems
thatarecommonlyencounteredinpractice,thesizeofpossibleactionspacesU andthetimehorizonsofplanningT
makethepolicyspacetoolargetobecomputationallytractable. Forexample,witheightavailableactionsinU anda
timehorizonofplanningT = 15,thetotalnumberof(definable)policiesthatneedtobeconsideredare(3.5∗1013)
35trillion. Evenforthisrelativelysmall-scaleexample,thispolicyspaceisnotcomputationallytractabletosimulate
agentbehaviour(unlessadditionaldecisiontreesearchmethodsareconsidered[Fountasetal.,2020,Championetal.,
2021a,b]orpolicyamortisationFountasetal.[2020],Çataletal.[2020])orbyeliminatingimplausiblepolicytrajec-
tories using Occam’s principle. We now turn to an improved scheme that redefines policy space and planning all
together.
4.2 Sophisticatedinference
Graduating from the classical definition of policy as a sequence of actions in time, sophisticated inference
[Fristonetal., 2021] attempts to evaluate the EFE of observation-action pairs at a given time t, G(o ,u ). Given
t t
6
Onefficientcomputationinactiveinference APREPRINT
thisjointdistribution, an agentcan sample actionsusing the conditionaldistributionQ(u |o ) whenobservingo at
t t t
timet,
u ∼Q(u |o )=σ[−G(o ,u )]. (19)
t t t t t
Giventheprior-preferencedistributionof anagentP(s), in termsofhiddenstatess, the expectedfreeenergyof an
observation-actionpairisdefinedas[Fristonetal.,2021],
G(o ,u )=E logQ(s |u )−logP(s )−logP(o |s )+ (20)
t t P(ot+1|st+1)Q(st+1|u<t+1) t+1 <t+1 t+1 t+1 t+1
 Risk Ambiguity 
 
| {z }| {z }
EFEofactionattimet
| {zE [G(o ,u })]. (21)
Q(ut+1|ot+1)Q(ot+1|u≤t) t+1 t+1
EFEoffutureactions
| {z }
WerewritethisequationinafamiliarfashiontoEq.16. Intheaboveequation,theagentholdsevaluatedbeliefsabout
futurehiddenstatesgivenallpastactionsinthetermQ(s |u ). Beliefsabouthiddenstatescanbeextrapolated
t+1 <t+1
toobservationsusingthelikelihoodmapping(A)as
P(o |s )Q(s |u )=Q(o |u ). (22)
t+1 t+1 t+1 <t+1 t+1 <t+1
Also,thepriorpreferenceoftheagentisdefinedintermsofhiddenstatessinEq.20. NowtheEq.20canberewritten
usingmappingslikeEq.22as,
G(o ,u )=E logQ(o |u )−logC−logP(o |s )+ (23)
t t Q(ot+1|u<t+1) t+1 <t+1 t+1 t+1
 Risk Ambiguity 
 
| {z }| {z }
EFEofactionattimet
| E {z [G(o ,u })]. (24)
Q(ut+1|ot+1)Q(ot+1|u≤t) t+1 t+1
EFEoffutureactions
| {z }
Notethatthepriorpreferencedistributionintheequationaboveisoverobservationso,C=P(o). RewritingEq.23in
asimilarfashiontothepreviouslydiscussedclassicalactiveinferenceweobtain
G(o ,u )=D [Q(o |u )||C]+E H[P(o |s )]+ (25)
t t KL t+1 <t+1 Q(st+1|u<t+1) t+1 t+1
Risk Expectedambiguity
| {z } | {z }
EFEofactionattimet
| E {z [G(o ,u })]. (26)
Q(ut+1|ot+1)Q(ot+1|u≤t) t+1 t+1
EFEoffutureactions
| {z }
Thefirsttwotermscanbeinterpretedthesamewayaswe didforEq.23intheprevioussection. However,thethird
terminEq.25givesrisetoarecursivetree-searchalgorithm,accumulatingfreeenergiesofthefuture(asdeepaswe
evaluateforwardintime). SuchanevaluationispictoriallyrepresentedinFig.1(A).
While Bellman optimal DaCostaetal. [2021], one unavoidable limitation of the sophisticated inference planning
algorithmis that it faces a worse curse of dimensionalityfor even relatively small planninghorizons. For example,
to evaluate the goodness of an action within a period of fifteen time-steps into the future, and with eight available
actions and a hundredhidden states, requires an exorbitant(100∗8)15 (≈ 3.5∗1043) calculations, in comparison
to 100∗(8)15(≈ 3.5∗1015) for classical active inference. A simple solution proposed in [Fristonetal., 2021] is
to eliminate tree search branches by setting a threshold value to predictive probabilities such as Q(u |o ) in
t+1 t+1
7
Onefficientcomputationinactiveinference APREPRINT
Figure1:GraphicstocompareandcontrastthedifferencesbetweenthesophisticatedinferenceandDPEFE(Dynamic
programming in expected free energy) algorithm planning schemes. A: Sophisticated inference algorithm uses an
extensivetreesearch,goingforwardintime,toaccumulatefreeenergyofthefuturepaths. So,anagent’spreference
for observations, when matched with future predictions, will inform an optimal state-action trajectory, as shown in
thetreesearch. Light-purplestatesrepresentthepreferredobservationsatthatgiventimestep,andlight-blueactions
are the optimalactionsinferredthroughthe tree search. As notedin Fristonetal. [2021], an agentcansignificantly
reducethetreesearchcomplexitybyterminatingthesearchwhentheactionprobabilityfallsbelowacertainthreshold.
However,thisapproximationdoesnotguaranteeoptimalpolicyastheagentmightmisspreferredobservationsdeeper
inthetreesearch.B:IntheDPEFEalgorithm,anagentstartsplanningbackwardsfromafixedplanninghorizon.Here,
theEFEoffuturestatesinformsEFEofstate-actionpairsonestepbackwardintime. Hence,theplanningcomplexity
oftreesearchisavoided,butthepreferenceforfuturestatespropagatestoinfluencedecisionsatprevioustimesteps.
Sincetheagentneedstoevaluateonlyatable(ofEFE)ateveryplanningstep,thisplanningalgorithmislinearintime,
numberofstates,andnumberofactions.
Eq.25. So,forexample,whenQ(u |o )<1/16duringplanning,thealgorithmterminatesthesearchoverfuture
t+1 t+1
branches.Thisrestrictionsignificantlyreducesthecomputationaltime,andasetofensuing(meaningful)simulations
waspresentedin[Fristonetal.,2021].
Anotherlimitationisthatinallactiveorsophisticatedinferenceagentstofacilitatedesirablebehaviour,apriorprefer-
enceneedstobedefinedbythemodellerorlearnedbytheagentsSajidetal.[2021b,2022c]informingtheagentthat
somestatesarepreferabletoothers,asdemonstratedinFig.2(B)foragridproblemgiveninFig.2(A).Aninformed
priorpreferenceenablestheagenttosolvethisnavigationtaskbyonlyplanningfourormoretimestepsahead. Itcan
takeactionandmovetowardsa‘morepreferredstate’ifnotthefinalgoalstate. However,withoutsuchinformation,
theagentis‘blind’(cf. Fig.2(C))andcanonlyfindtheoptimalmovewhenplanningthewholeeight-steptrajectory
forthegivengrid.
We first noticedthis limitation whencomparingdifferentactiveinferenceschemesto variouswell-knownreinforce-
ment learning algorithms in [Pauletal., 2021] in a fully observable setting (i.e., MDPs). In the next section, we
demonstrate how to scale the sophisticated inference scheme using dynamic programming for the general case of
POMDP-basedgenerativemodels.
8
Onefficientcomputationinactiveinference APREPRINT
Figure 2: Informedand uninformedpriorpreferences: A: A navigationproblem, B: A strictly defined, sparse prior
preferencewhichhasinformationonlyaboutthefinalgoalstate,C:Informedpriorpreferencenecessaryfor‘pruning
oftreesearch’insophisticatedinference(lightcolourstatesaremorepreferred)
5 Dynamicprogramming forevaluatingexpected free energy
The Bellman optimality principle states that the sub-policy of an optimal policy (for a given problem) must itself
will be an optimal policy for the corresponding sub-problem [SuttonandBarto, 2018]. Dynamic programming is
an optimisation technique that naturally follows the Bellman optimality principle; rather than attempting to solve a
problemasawhole,dynamicprogrammingattemptstosolvesub-partsoftheproblemandintegratethesub-solutions
intoasolutiontotheoriginalproblem.Thisapproachmakesdynamicprogrammingscalefavourably,aswesolveone
sub-problematatimetointegratelater. Themorewebreakdownthelargeproblemintocorrespondingsub-problems,
themorecomputationallytractableisthesolution.
Inspired by this principle, let us consider a spatial navigation problem that the agent needs to solve in our setting.
Theoptimalsolutiontothisnavigationproblemisasequenceofindividualsteps. Ourpriorpreferenceforthe‘goal
state’isfortheendofourtimehorizonofplanning. So,theagentmaystartplanningfromthelasttimestep(astep
sub-problem)andgobackwardstosolvetheproblem. Thisapproachisalsocalled planningbybackwardinduction
Zhouetal.[2019].
So,foraplanninghorizonofT (i.e.,theagentaimstoreachgoalstateattimeT),theEFEofthe(last)actionforthe
T −1thtimestepinaPOMDPsettingiswrittenas:
G(u ,o )=D [Q(o |u ,s )||C]. (27)
T−1 T−1 KL T T−1 T−1
Theterm,G(u |o )istheexpectedfreeenergyassociatedwithanyactionu ,givenwearein(hidden)state
T−1 T−1 T−1
s . This estimate measureshow much we believe observationsat time T will align with our prior preferenceC.
T−1
Notethat,forsimplicity,weignoredthe‘expectedambiguity’termintheequationabove,i.e. theuncertaintyofstate-
observationmapping(orlikelihood),cf. Eq.25. Thisdoesnotaffectoursubsequentderivations;wecanalwaysadd
itasanadditionalterm. Thefollowingderivationprovidedtechnicaldetailsofdynamicprogrammingwhilefocusing
onlyonthe‘risk’terminG.
ToestimateQ(o |u ,s ),wemakeuseofthepredictionaboutstatesQ(s )thatcanoccurattimeT:
T T−1 T−1 T
Q(s |u ,s )=B ·Q(s ), (28)
T T−1 T−1 uT−1 T−1
andgiventhepredictionQ(s ),wewrite
T
Q(o |u ,s )=A·Q(s |u ,s ) (29)
T T−1 T−1 T T−1 T−1
=A· BT−1·Q(s ) . (30)
u T−1
(cid:0) (cid:1)
Next,usingEq.28,thecorrespondingactiondistribution(foractionselection)iscalculatedattimeT,
Q(u |o )=σ(−G(u |o )), (31)
T−1 T−1 T−1 T−1
9
Onefficientcomputationinactiveinference APREPRINT
wherewerecursivelycalculatetheexpectedfreeenergyforactionsandthecorrespondingaction-distributionsfortime
stepsT −2,T −3,...,t=1backwardsintime,8
G(u |o )=D [Q(o |u ,s )||C]+E [G(u |o )]. (32)
t t KL t+1 t t Q(ot+1,ut+1|ot,ut) t+1 t+1
EFEofactionattimet EFEofnextactionatt+1
| {z } | {z }
Intheequationabove,thesecondtermcondensesinformationaboutallfutureobservationsratherthandoingaforward
treesearchintime. ToinformG(u |o ),weconsiderallpossibleobservation-actionpairsthatcanoccurintimet+1
t t
andusethepreviouslyevaluatedG(u |o ). InEq.32,weevaluateQ(o ,u |o ,u )using,
t+1 t+1 t+1 t+1 t t
Q(s ,u |s ,u )=Q(s |s ,u )·Q(u |s ). (33)
t+1 t+1 t t t+1 t t t+1 t+1
B Actiondistribution
| {z } | {z }
WethenmapthedistributionQ(s ,u |s ,u )totheobservationspaceandevaluateQ(o ,u |o ,u )usingthe
t+1 t+1 t t t+1 t+1 t t
likelihoodmappingA.InEq.33,weassumethatactionsintimeareindependentofeachother,i.e.u isindependentof
t
u . Eventhoughactionsareassumedtobeexplicitlyindependentintime,theinformation(andhencedesirability)
t+1
aboutactionsarealsoinformedbackwardsintimefromtherecursiveevaluationofexpectedfreeenergy.
WhileevaluatingtheEFE,G,backwardsintime,weusedtheactiondistributioninEq.31. Thisactiondistributioncan
bedirectlyusedforactionselection.Givenanobservationoattimet,u maybesampled9from,
t
u ∼Q(u |o =o). (34)
t t t
In the nextsection, we summarisethe aboveformulationas a novelactive inferencealgorithmusefulformodelling
intelligentbehaviourinsequentialPOMDPsettings.
5.1 AlgorithmicformulationofDPEFE
Here,weformaliseagenericalgorithmthatcanbeemployedforasequentialPOMDPproblem. Themainalgorithm
(see Alg.1) works sequentially in time and brings together three differentaspects of the agent’s behaviour, namely,
perception(inference),planning,andlearning.
Forplanning,thatis,toevaluatetheexpectedfreeenergy(G)foractions(givenstates)intime,weemploytheplanning
algorithm(See. Alg.2)asasubroutinetoAlg.1. Inthemostgeneralcase,thealgorithmisinitialisedwith’flat’priors
forthe likelihoodfunction(A) and transitiondynamics(B). Thealgorithmalso allows usto equipthe agentwith a
more informedprioraboutA and B. LearningC in the DPEFE algorithm is setting C as a one-hotvector with the
encounteredgoalstate. Thistechniqueacceleratestheparameters’learningprocessduringtrialsandimprovesagent
performance.Wecanalsomakeavailablethe‘true’dynamicsoftheenvironmenttotheagentwheneverpresent.With
‘true’dynamicsavailableattheagent’sdisposal,theagentcaninferhiddenstatesandplanaccurately.Thenextsection
discussesadifferentapproachtoamelioratingthecurseofdimensionalityinsophisticatedinference.Later,wediscuss
a potential learning rule for the prior preference distribution C inspired by a seminar work in the control theoretic
literature.
6 Learning priorpreferences
Intheprevioussection,weintroducedapracticalalgorithmsolutionthatspeedsupplanninginsophisticatedinference.
The second innovationon offer is to enable learning of preferencesC such that smaller planning horizons become
sufficientforouragenttotakeoptimalactions,asdiscussedinFig.2. Aseminalworkfromtheliteratureoncontrol
theoryproposesusinga‘desirability’function,scoringhowdesirableeachstateis, tocomputeoptimalactionsfora
particularclassofMDPsand,importantly,showingthattheplanningcomplexityofcomputingthoseactionsislinear
intime[Todorov,2006]. WhentheunderlyingMDPmodeloftheenvironmentisunavailableandtheagentneedsto
take actionsbasedsolely on a stream of samplesof states andrewards(i.e., s ,r ,s ), an onlinealgorithmcalled
t t t+1
Z-learning,inspiredfromthetheoreticaldevelopmentsin[Todorov,2006],wasproposedtosolvethisproblem.Given
8FortimesotherthanT−1,thefirstterminEq.32doesnotcontributetosolvingtheparticularinstanceifConlyaccommodates
preferencetoa(time-independent) goal-state. However, foratemporallyinformedC,i.e. withaseparatepreferenceforreward
maximisationateachtimestep,thistermwillmeaningfullyinfluenceactionselection.
9Precisionofaction-sectionmaybecontrolledbyintroducingapositiveconstantinsidethesoftmaxfunctionσ(.)inEq.31.The
highertheconstant,thehigherthechanceofselectingtheactionwithlessEFE.
10
Onefficientcomputationinactiveinference APREPRINT
Algorithm1ActiveinferenceinasequentialPOMDP
C←o ⊲priorpreference(Known/Learnedasone-hotvector(sparse)whenencounteringofgoal-state
goal
D←s ⊲Known/Learnedattimet=1
start
T ←PlanninghorizonoftheDPEFEagent
a←a +ǫ ⊲Thepriorisusuallyanuninformed’flat’distribution.
prior
A←Dir(a)
b ←b +ǫ∀u ⊲ǫisanegligiblepositivevaluetoensurenumericalstability
u prior
B ←Dir(b )∀u
u u
T ⊲ThresholdforepisodelengthsetinEnvironment
max
whileTruedo ⊲Loopforever
fortfrom1toT do
max
Inference:
ift=1then
P(s )←D(known) ⊲Prioratt=1
1
Observeo =o fromEnvironment
1 start
Evaluate,Q(s ) ⊲Inference,RefEq.8
t=1
else
EvaluateP(s ) ⊲Ref. Eq.10
t
EvaluateQ(s ) ⊲Inference,RefEq.8
t
endif
Planningandactionselection
EvaluateG(u |o ) ∀tǫ1,..,T −1, oǫO ⊲Planning,Ref. Algorithm.2
t t
EvaluateQ(u ) ∀t,o ⊲Actiondistribution,Ref. Eq.31
t
Sampleu ∼Q(u ). ⊲Sampleaction,RefEq.34
t t
Observeo ←From-Environmentbytakingactionu
t+1 t
Learning
ift>1then
b ←b +Q(u )·(Q(s )⊗Q(s )) ⊲RefEqn.11
u u t−1 t t−1
endif
a←a+o ⊗Q(s ) ⊲RefEqn.13
t t
Endoftrial
ift=T then
max
Environmentisreseti.es ←s
True start
t←1
endif
ifGoal-achievedthen
C←One-hot(o )
goal
Environmentisreseti.es ←s
True start
t←1
endif
endfor
A=Dir(a) ⊲Updatinglikelihood
B =Dir(b ) ⊲Updatebeliefsabouttransitiondynamics
u u
endwhile ⊲Endofexperiment
anoptimaldesirabilityfunctionz(s),theoptimalcontrol,orpolicy,isanalyticallycomputable.Thecalculationofz(s)
doesnotrelyonknowledgeoftheunderlyingMDPbutinstead,onthefollowingonlinelearningrule:
zˆ(s )←(1−η )zˆ(s )+η exp(r )zˆ(s ), (35)
t t t t t t+1
where,ηisalearningratethatiscontinuouslyoptimised—seebelow. Thesetwotermsformaweightedaveragethat
updatestheestimateofzˆ(s ),withη controllingthebalancebetweentheoldestimateandthenewinformation.
t t
11
Onefficientcomputationinactiveinference APREPRINT
Algorithm2Planningbackwardsintime
A←PassedfromAlg.1
B←PassedfromAlgorithm.1
C←PassedfromAlgorithm.1
T ←PassedfromAlgorithm.1
Planning
fort=T −1tot=1do
ift=T −1then
EvaluateQ(o |u ,s ) ⊲Ref. Eq.29
T T−1 T−1
EvaluateG(u |o ) ⊲Ref. Eq.27
T−1 T−1
else
EvaluateQ(o |u ,o ) ⊲Ref. Eqn.29
t t−1 t−1
EvaluateG(u |o ) ⊲Ref. Eq.32
t t
endif
endfor
Inspired by these developments, we write a learning rule for updating C which can be useful for the sophisticated
inferenceagent. Giventhesamples(o ,r ,o ),anagentmaylearntheparameterconlineusingaruleanalogousto
t t t+1
Eq.35,
c(o )←(1−η )c(o )+η exp(r )c(o ). (36)
t t t t t t+1
In the aboveequation, c(o ) represents the desirability of an observationo at time t. The value of c(o ) is updated
t t
dependingontherewardreceivedandthedesirabilityoftheobservationreceivedatthenexttimestepc(o ).
t+1
Thelearningrateηisatime-dependentparameterinZ-learning,asgivenintheequationbelow.eisahyperparameter
weoptimisethatinfluenceshowfast/slowηgetsupdatedovertimeTodorov[2009]:
e
η = . (37)
t
e+t
Ifη ishigh,thealgorithmputsmoreweightonthenewinformation. Ifη islow,thealgorithmputsmoreweighton
t t
thecurrentestimate. UsingtheupdateruleinEq.36withthelearningrateevolvingasin(37),thevalueofcevolves
overtimeandmaybeusedtoupdateConline,ensuringthatCisacategoricaldistributionoverobservationsusingthe
softmaxfunction:
C=σ(c). (38)
WeusethestandardgridworldenvironmentsasshowninFig.3fortheevaluationoftheperformanceofvariousagents
(moredetails in the nextsections). Fig.6, is a visualisation thatrepresentsthe learnedpriorpreference(forthe grid
showninFig. 3(A))usefulforthesophisticatedinferenceagent.Withaninformedpriorpreferencelikethis,theagent
needstoplanonlyonetimestepaheadtonavigatethegridsuccessfully. ItshouldbenotedthatintheDPEFEsetting,
wefixthepriorpreferenceCeitherbeforeatrialorlearnitwhenweencounterthegoalasaone-hotvector.Wearenot
learninganinformedpriorpreferencefortheDPEFEagentinthesimulationspresentedinthepaper. Themethodfor
learningpriorpreferencediscussedinthissectionholdsforanyagent,butinourpaper,DPEFEisnotusingthisfeature
to demonstrateits abilityto plandeeper. When we aid an active inferencealgorithmwith the learningrulefor C, a
planninghorizonofT =1sufficestotakedesirableactions(i.e. withnodeeptreesearchlikeinSIorpolicyspace(Π)
asinCAIF).Withconsideringonlythenexttimestep, (i.e. onlytheconsequenceofimmediatelyavailableactions),
planninginallactiveinferenceagents(CAIF,SI,andDPEFE)arealgorithmicallyequivalent. Intherestofthepaper,
wecallthisagentwithplanninghorizonT =1,whichisaidedwiththelearningruleofC asactiveinferenceAIF(T
=1)agent. Inoursimulations,wecomparetheperformanceofthesetwoapproaches(i.e.,deepplanningwithsparse
C andshort-termplanningwith learningC). An animationthatvisualisesthe learningpriorpreferencedistribution
forthegridover50episodesinFig.2canbefoundinthislink. Inthefollowingsection,wediscussandcomparethe
computationalcomplexityofplanningbetweenexistingandnewlyintroducedschemes.
7 Computational complexity
In this section, we compare the computational complexity in evaluating the expected free energy term, used for
planninganddecisionmaking, with two otheractiveinferenceapproaches: classical activeinferenceDaCostaetal.
[2020],Sajidetal.[2021a],andsophisticatedinferenceFristonetal.[2021].
12
Onefficientcomputationinactiveinference APREPRINT
Inclassicalactiveinference(DaCostaetal.[2020],Sajidetal.[2021a]),theexpectedfreeenergyforanMDP(i.e.,a
fullyobservablecase)isgivenby,
G(π|s )=D [Q(s |π)||P(s )]. (39)
t−1 KL t t
Here,P(s )representsanagent’spriorpreferenceandisequivalenttoCinanMDPsetting.Inthispaper,Cisdirectly
t
definedin termsof the hiddenstates. To avoid confusion,we alwaysuse the notationC in this paperregardingthe
observationso.
Similarly,forsophisticatedinference[Fristonetal.,2021],wehave,
G(u )=D [Q(s |u )||P(s )]+E [G(u )]. (40)
t KL t+1 <t+1 t+1 Q(ut+1) t+1
Intheaboveequation,werestricttherecursiveevaluationofthesecondterm,forwardintime,tilla‘planninghorizon
(T)’asmentionedinFristonetal.[2021].T necessaryfor’full-depthplanning’i.eplanningtotheendoftheepisodeis
oftenrequiredforsparselydefinedpriorpreferences.Thisisrequiredsincetheagentwouldnotbeabletodifferentiate
thedesirabilityofactionsuntilreachingthelaststepoftheepisodethroughatreesearch.
In classical active inference, to evaluate Eq.39, the computational complexity is proportional to: O[card(S) ×
card(U)T]. Forsophisticatedinference,toevaluateEq.40, thecomplexityscalesproportionallyto: O[(card(S)×
card(U))T].ThedimensionsofthequantitiesinvolvedarespecifiedinTab.1. AndrecallthatbothEq.39andEq.40
ignorethe‘ambiguity’termforsimplicity.
Sophisticatedinference Classicalactiveinference
Term Dimension Term Dimension
s card(S)(cardinalityofS) π card(U)T
τ+1
Q(u ) card(U) s card(S)
t+1 t
Q(s |u ) card(S) Q(s |π) card(S)×card(U)T
t+1 <t+1 t
P(s ) card(S) P(s ) card(S)
τ+1 t
G(u ) card(S) G(π|t) card(U)T
t+1
O[(card(S)×card(U))T] O[card(S)×card(U)T]
Table1:ComputationalcomplexityinevaluatingEFE.
ForevaluatingEFEusingdynamicprogramming,theexpectedfreeenergyforanMDPcanbededucedfromEq.32as,
G(u |s )=D [Q(s |s ,u )||P(s )]+E [G(u |s )]. (41)
t t KL t+1 t t t+1 Q(st+1|st,ut) t+1 t+1
Sinceweonlyevaluateontime-stepaheadinEq.41,evenwhenevaluatingbackwardsintime,thecomplexityscales
as: O[card(S)×card(U)×T].
8 Simulations results
8.1 Setup
WeperformsimulationsinthestandardgridworldenvironmentinFig.3toevaluatetheperformanceofourproposed
algorithms. Theagentisbornin a randomstartstate atthebeginningofeveryepisodeandcantake oneofthefour
available actions (North, South, East, West) at every time step to advance towards the goal state until the episode
terminateseitherbyatime-out(10000,20000,and40000stepsforthegridsinFig.3respectively)orbyreachingthe
goalstate. Forcompleteness,wecomparetheperformanceofthefollowingalgorithmsinthegridworld:
• Q-learning:abenchmarkmodel-freeRLalgorithmWatkinsandDayan[1992]
• Dyna-Q:abenchmarkmodel-basedRLalgorithmimprovinguponQ-learningPengandWilliams[1993]
• DPEFEalgorithmwithstrictlydefined(sparse)C(SeeSec.5.1)
• ActiveinferencealgorithmaidedwithlearningruleforC(SeeSec.6)andplanninghorizonofT =1i.ewith
nodeeptreesearchlikeinSI,orpolicyspace(Π)asinCAIF.Withconsideringonlythenexttimestep,(i.e.
onlytheconsequenceofimmediatelyavailableactions)planninginallactiveinferenceagents(CAIF,SI,and
DPEFE) are algorithmically equivalent. In the rest of the paper, we call this agent with planning horizon
T =1aidedwiththelearningruleofC asactiveinferenceAIF(T=1)agent.
13
Onefficientcomputationinactiveinference APREPRINT
Figure3: A: A standardgrid worldof 100states with 50 validstates. B: A grid of400states with 204validstates.
C: A grid of 900 states with 497 valid states. These three grids are used for evaluatingthe performanceof various
schemes.
Figure 4: The summary of agents’ performancein the two grids. A: Deterministic grid (100 states), B: Stochastic
versionofthegridinA(100states,partiallyobservable,stochastictransitions(POMDP)).
WeperformsimulationsindeterministicandstochasticgridvariationsshowninFig.3. Thedeterministicvariationis
a fullyobservablegridwithnonoise. So, anagentfullyobservesthepresentstate—i.e., anMDPsetting. Also, the
outcomesofactionsarenon-probabilisticwithnonoise—i.e.,adeterministicMDPsetting. Inthestochasticvariation,
wemaketheenvironmentmorechallengingtonavigatebyadding25%noiseinthetransitionsand25%noiseinthe
observed state. In this case, the agent faces uncertaintyat every time step aboutthe underlyingstate (i.e., partially
observable)andthenextpossiblestate(i.e.,stochastictransitions)—i.e.,astochasticPOMDPsetting.
8.2 Summaryofresults
The agents’performancein thisnavigationproblemis summarisedin Fig.4andFig.5. Performanceis quantifiedin
terms of how quickly an agentlearnsto solve the grid task, i.e. the total score. The agentreceivesa reward of ten
pointswhenthegoalstateisreachedandasmallnegativerewardforeverysteptaken.Thetotalscorehencerepresents
how fast the agent navigated to the goal state for a given episode. The grid has a fixed goal state throughout the
episodesinFig.4(A,B)andFig.5(A).ForsimulationsinFig.5(B),thegoalstateisshiftedtoanotherrandomstate
14
Onefficientcomputationinactiveinference APREPRINT
Figure5: Thesummaryofagents’performanceinthetwogrids. A:Stochasticgrid(400states,partiallyobservable,
stochastictransitions(POMDP)),B:StochasticgridinAwithgoal-staterandomizedaftereverytenepisodes.
every10episodes.Thissetuphelpstoevaluatetheadaptabilityofagentsinthefaceofchangesintheenvironment.It
isclearthatduringtheinitialepisodes,theagentstakelongertoreachthegoalstatebutlearntonavigatequickerasthe
episodesunfold.StandardRLalgorithms(i.e.,Dyna-QandQ-Learning)areusedheretobenchmarktheperformance
ofactiveinferenceagents,astheyareefficientstate-of-the-artalgorithmstosolvethissortoftask.
Inoursimulations,theDPEFEalgorithmperformsatparwiththeDyna-QalgorithmwithaplanningdepthofT =80
10 (See (Fig.4 (A,B), Fig.5(A)).The DPEFE agentperformsevenbetterwhen we randomisedgoal-statesevery10
episode(Fig.5(B)).IncontrasttoonlinelearningalgorithmslikeDyna-Q,activeinferenceagentscantakeadvantage
ofthere-definableexplicitpriorpreferencedistributionC. FortheAIF(T=1)agent,weobservethattheperformance
improvesovertimebutisnotasgoodastheDPEFEagent. ThisisbecausetheAIF(T=1)agentplansforonlyone
stepaheadinourtrialsbydesign. WecouldalsoobservethattheQ-Learningagentperformsworsethantherandom
agentandrecoversslowerthantheAIF(T=1)agentwhenfacedwithuncertaintyinthegoalstate. Itisapromising
direction to optimise the learning of prior preference C in the AIF(T = 1) agent, ensuring accuracy in the face of
uncertainty.Allsimulationswereperformedfor100trialswithdifferentrandomseedstoensurethereproducibilityof
results.
Besidesthis,weobservealongertimetoachievethegoalstateforbothactiveinferenceagents(evenlongerthanthe
‘Randomagent’)intheinitialepisodes. Thisisacharacteristicfeatureofactiveinferenceagents,astheirexploratory
behaviourdominatesduringtheinitialtrials. Thegoal-directedbehaviourdominatesonlyaftertheagentsufficiently
minimisesuncertaintyinthemodelparametersTschantzetal.[2020].
8.3 OptimisinglearningrateparameterforAIF(T=1)agent
ThelearningruleproposedforsophisticatedinferenceinEq.37requiresa(manually)optimisedvalueofeforevery
environment that influences the learning rate η. Todorov [2009] inspires this learning rule, where the value of η
t
determines how fast the parameter c convergesfor a given trial. The structure of learned c is crucial for the active
inferenceagent,asCdetermineshowmeaningfultheplanningisfortheagent. InFig.B.1, weplottheperformance
of the AIF(T = 1) agent as a function of e for the grids in Fig. 3. A promising direction for future research is to
improvethelearningrulebasedonη andfine-tunethemethodforlearningC. TheobservationinFig.B.1isthatthe
10aplanninghorizonmorethananyoptimalpathinthisgrid. Sincethestartstateisrandomized,optimalpathscanhavemany
lengthsinthegrid.AplanningdepthofT =80ensuresthattheagentplansenoughnottomissthelengththatneedstobecovered
inanysetting.
15
Onefficientcomputationinactiveinference APREPRINT
Figure6:A:ThesparselydefinedpreferencedistributionusedbyDPEFEagentinsimulations,B:Thelearnedprefer-
encedistributionbyAIF(T=1)agentover50episodes.Lightercoloursimplyahigherpreferenceforthecorresponding
states.
Figure7: Comparingcomputationalcomplexityofmethodsdiscussedinthispaper. A:Orderofcomputationalcom-
plexity(logscale)vsTimehorizonofplanning(T).Herecard(S)=100,card(U)=4.,B:Orderofcomputational
complexity(linearscale)vscard(S). Here,card(U) = 4andT = 2(T=1forAIF(T=1)agent). Weobservethat
exceptforDPEFEandAIF(T=1)methods,computationalcomplexitybecomesintractableevenforT assmallT =2
andcard(S)=5.
performanceoftheAIF(T=1)agentisnotheavilydependentonthevalueofe.Weuseddifferentvaluesofe>10000
inAIF(T=1)agentsinallsettingsinthispaper.
8.4 Anemphasisoncomputationalcomplexity
Tounderstandwhytheclassicalactiveinference(CAIF)andSImethodscannotsolvethesegridenvironmentswiththe
traditionalplanningmethod,weprovideanexemplarsettinginTab. 2. ConsiderthesmallgridasshowninFig.3with
card(S)=100,card(U)=4,T=30.Tab.2summarisesthecomputationalcomplexityofsimulatingvariousactive
16
Onefficientcomputationinactiveinference APREPRINT
inferenceagentsforthissmallgridworldproblem. Thecomputationalcomplexityexceedspracticalimplementations
evenwithaT=2planninghorizon.WecanobservethisvisuallyinFig.7.
Method Orderdimension(O) Approx.Oforspecificcase
CAIF(T=30) O[card(S)×card(U)T] 1018
SI(T=30) O[(card(S)×card(U))T] 1068
SI(T=2) O[card(S)×card(U)2]. 106
DPEFE(T=30)∗ O[card(S)×card(U)×T]. 103
AIF(T=1)∗ O[card(S)×card(U)]. 0.2∗103
Table2:ComputationalcomplexityforevaluatingEFEwithcard(S)=100,card(U)=4,T =30.CAIF:classical
active inference, SI: sophisticated inference with full tree search, DPEFE: dynamic programming active inference
agent,AIF(T=1):Activeinferenceagentplanningonetime-stepaheadandlearningthepriorpreference,*proposed
inthispaper. Withouttreesearch,thesophisticatedinference(SI)agentisalgorithmicallyequivalenttotheclassical
active inference agent (CAIF). In the rest of the paper, we call this agent with planning horizon T = 1 as active
inferenceAIF(T=1)agent.
However,wenotethattheproposedsolutionoffirstlearningthepriorpreferences(SeeSec.6)usingtheZ-learningrule
enablestheactiveinference(AIF)agenttolearnandsolvetheunknownenvironmentbyavoidingthecomputational
complexityofadeeptreesearch. Itshouldalsobenotedthatneitheroftheactiveinferencealgorithms(DPEFEand
AIF (T=1)) was equipped with meaningful priors about the (generative) model parameters (B, C, and D). Agents
startblindlywith‘uninformed’modelpriorsandevolvebyintegratingallaspectsofbehaviour: perception,planning,
decision-making,andlearning.Yet,thefactthat,likeDyna-Q,theystartwithamodeloftheworldmeansthattheyare
muchlessagnosticthanthemodel-freealternativeofferedbyQ-learning. Thefollowingsectiondiscussesthemerits
andlimitationsoftheproposedsolutionstooptimisedecision-makinginactiveinference.
9 Discussion
In this work, we explored the usefulness of active inference as an algorithm to model intelligent behaviour and its
application to a benchmark control problem, the stochastic grid world task. We identified the limitations of some
ofthemostcommonformulationsofactiveinferenceFristonetal.[2021],whichdonotscalewellforplanningand
decision-makingtasksinhigh-dimensionalsettings. We proposedtwocomputationalsolutionstooptimiseplanning:
harnessingthemachineryofferedbydynamicprogrammingandtheBellmanoptimalityprincipleandharnessingthe
Z-learningalgorithmtolearninformedpreferences.
First, our proposed planning algorithm evaluates expected free energy backwards in time, exploiting Bellman’s op-
timality principle,consideringonlythe immediatefutureasin the dynamicprogrammingalgorithm. We presentan
algorithmforgeneralsequentialPOMDPproblemsthatcombinesperception,actionselectionandlearningunderthe
singlecostfunctionofvariationalfreeenergy. Additionally,thepriorpreference,i.e.,thegoalstateaboutthecontrol
task,wasstrictlydefined(i.e.,uninformed)andsuppliedtotheagent,unlikewell-informedpriorpreferencesasseen
in earlier formulations. Secondly, we exploredthe utility of equippingagents so as to learn their prior preferences.
Weobservedthatlearningthepriorpreferenceenablestheagenttosolvethetaskwhileavoidingthecomputationally
(oftenprohibitively)expensivetree search. We usedstate-of-the-artmodel-basedreinforcementlearningalgorithms,
suchasDyna-Q,tobenchmarktheperformanceofactiveinferenceagents.Lastly,thereisfurtherpotentialtooptimise
computationaltimebyexploitingapproximationparametersinvolvedinplanninganddecision-making. Forexample,
thesoftmaxfunctionsusedwhileplanninganddecision-makingdeterminetheprecisionofoutputdistributions.There
is also scope to optimise furtherthe SI agentproposedin this paper by learning the prior preference. Based on the
Z-learningmethod,thelearningruleforpriorpreferenceparametersshallbeoptimisedandfine-tunedforactiveinfer-
enceapplicationsin futurework. Since the Z-learningmethodis fine-tunedfora particularclassof MDPproblems
Todorov[2006], we leave a detailed comparisonofthe two approachesto futurework. We concludethatthe above
resultsadvanceactiveinferenceasa promisingsuite ofmethodsformodellingintelligentbehaviourandforsolving
stochasticcontrolproblems.
10 Acknowledgments
AP acknowledges research sponsorship from IITB-Monash Research Academy, Mumbai and the Department of
Biotechnology, Government of India. AR is funded by the Australian Research Council (Refs: DE170100128 &
DP200100757)andAustralianNationalHealthandMedicalResearchCouncilInvestigatorGrant(Ref:1194910).AR
17
Onefficientcomputationinactiveinference APREPRINT
isaCIFARAzrieliGlobalScholarintheBrain,Mind&ConsciousnessProgram. AR,NS,andLDareaffiliatedwith
TheWellcomeCentreforHumanNeuroimaging,supportedbycorefundingfromWellcome[203147/Z/16/Z].NSis
fundedbytheMedicalResearchCouncil(MR/S502522/1)andthe2021-2022MicrosoftPhDFellowship. LDissup-
portedbytheFondsNationaldelaRecherche,Luxembourg(Projectcode: 13568875). Thispublicationisbasedon
workpartiallysupportedbytheEPSRCCentreforDoctoralTraininginMathematicsofRandomSystems: Analysis,
ModellingandSimulation(EP/S023925/1).
11 Software note
Allthecodeforagents,optimisationandgridenvironmentsusedarecustomwritteninPython3.9.15andisavailable
inthisprojectrepository:https://github.com/aswinpaul/dpefe_2023.
References
RichardS.SuttonandAndrewG.Barto. ReinforcementLearning: AnIntroduction. TheMITPress, secondedition,
2018. URLhttp://incompleteideas.net/book/the-book-2nd.html.
Emanuel Todorov. Linearly-solvable markov decision problems. In B. Schölkopf, J. Platt, and T. Hoff-
man, editors, Advances in Neural Information Processing Systems, volume 19. MIT Press, 2006. URL
https://proceedings.neurips.cc/paper/2006/file/d806ca13ca3449af72a1ea5aedbed26a-Paper.pdf.
Emanuel Todorov. Efficient computation of optimal actions. Proceedings of the National
Academy of Sciences, 106(28):11478–11483, 2009. doi:10.1073/pnas.0710743106. URL
https://www.pnas.org/doi/abs/10.1073/pnas.0710743106.
DrewFudenbergandJeanTirole. Gametheory. MITpress,1991.
KaihongLu, GuangqiLi, and LongWang. Onlinedistributed algorithmsforseeking generalizednash equilibriain
dynamicenvironments. IEEETransactionsonAutomaticControl,66(5):2289–2296,2020.
Dilip Mookherjee. Optimalincentiveschemeswith manyagents. The ReviewofEconomicStudies, 51(3):433–446,
1984. ISSN00346527,1467937X. URLhttp://www.jstor.org/stable/2297432.
J. Von NeumannandO.Morgenstern. TheoryofGamesandEconomicBehavior. Theoryof GamesandEconomic
Behavior.PrincetonUniversityPress,Princeton,NJ,US,1944.
JeremyBentham. AnIntroductiontothePrinciplesofMoralsandLegislation. HistoryofEconomicThoughtBooks,
1781.
JohnStuartMill. Utilitarianism. Longmans,GreenandCompany,1870. ISBN978-1-4992-5302-3.
P Ivan Pavlov (1927). Conditioned reflexes: An investigation of the physiological activity of the cerebral cortex.
AnnalsofNeurosciences,17(3):136–141,July2010. ISSN0972-7531. doi:10.5214/ans.0972-7531.1017309.
ChristopherJCHWatkinsandPeterDayan. Q-learning. Machinelearning,8:279–292,1992.
JingPengandRonaldJWilliams. Efficientlearningandplanningwithinthedynaframework. Adaptivebehavior,1
(4):437–454,1993.
Ben Goertzel. Artificial general intelligence: concept, state of the art, and future prospects. Journal of Artificial
GeneralIntelligence,5(1):1,2014.
Samuel J Gershman. What have we learned about artificial intelligence from studying the brain?
https://gershmanlab.com/pubs/NeuroAI_critique.pdf,2023. Accessed: 2023-05-29.
Richard Feynman. Statistical Mechanics: A Set Of Lectures. Westview Press, Boulder, Colo, 1st edition edition,
March1998. ISBN978-0-201-36076-9.
PeterDayan,GeoffreyE.Hinton,RadfordM.Neal,andRichardS.Zemel. TheHelmholtzMachine. NeuralCompu-
tation,7(5):889–904,September1995. ISSN0899-7667,1530-888X. doi:10.1162/neco.1995.7.5.889.
Karl Friston. The free-energy principle: A unified brain theory? Nature Reviews Neuroscience, 11(2):127–138,
February2010. ISSN1471-003X,1471-0048. doi:10.1038/nrn2787.
Karl Friston, Lancelot Da Costa, Noor Sajid, Conor Heins, Kai Ueltzhöffer, Grigorios A. Pavliotis, and Thomas
Parr. The free energy principle made simpler but not too simple. arXiv:2201.06387 [cond-mat, physics:nlin,
physics:physics,q-bio],January2022.
LancelotDaCosta,NoorSajid,ThomasParr,KarlFriston,andRyanSmith. RewardMaximizationThroughDiscrete
ActiveInference. NeuralComputation,35(5):807–852,April2023. ISSN0899-7667. doi:10.1162/neco_a_01574.
18
Onefficientcomputationinactiveinference APREPRINT
Lancelot Da Costa, Thomas Parr, Noor Sajid, Sebastijan Veselic, Victorita Neacsu, and Karl
Friston. Active inference on discrete state-spaces: A synthesis. Journal of Mathemati-
cal Psychology, 99:102447, 2020. ISSN 0022-2496. doi:10.1016/j.jmp.2020.102447. URL
https://www.sciencedirect.com/science/article/pii/S0022249620300857.
CorradoPezzato,CarlosHernándezCorbato,StefanBonhof,andMartijnWisse. Activeinferenceandbehaviortrees
for reactive action planning and execution in robotics. IEEE Transactions on Robotics, 39(2):1050–1069, 2023.
doi:10.1109/TRO.2022.3226144.
Guillermo Oliver, Pablo Lanillos, and Gordon Cheng. An empirical study of active inference on a hu-
manoid robot. IEEE Transactions on Cognitive and Developmental Systems, 14(2):462–471, jun 2022.
doi:10.1109/tcds.2021.3049907. URLhttps://doi.org/10.1109.2021.3049907.
George Deane, Mark Miller, and Sam Wilkinson. Losing ourselves: Active inference, depersonalization, and
meditation. Frontiers in Psychology, 11, 2020. ISSN 1664-1078. doi:10.3389/fpsyg.2020.539726. URL
https://www.frontiersin.org/articles/10.3389/fpsyg.2020.539726.
SergioRubin. Futureclimates: Markovblanketsandactiveinferenceinthebiosphere. JournalofTheRoyalSociety
Interface,17:20200503,112020. doi:10.1098/rsif.2020.0503.
ZafeiriosFountas, Noor Sajid, Pedro A. M. Mediano, and KarlFriston. Deep active inferenceagentsusing Monte-
Carlomethods. arXiv:2006.04176[cs,q-bio,stat],June2020.
Takazumi Matsumoto, Wataru Ohata, Fabien CY Benureau, and Jun Tani. Goal-directed planning and goal under-
standingbyextendedactiveinference: Evaluationthroughsimulatedandphysicalrobotexperiments. Entropy,24
(4):469,2022.
KarlJ.Friston,JeanDaunizeau,andStefanJ.Kiebel. Reinforcementlearningoractiveinference? PLOSONE,4(7):
1–13,072009.doi:10.1371/journal.pone.0006421. URLhttps://doi.org/10.1371/journal.pone.0006421.
KarlFriston. A freeenergyprincipleforbiologicalsystems. Entropy(Basel,Switzerland),14:2100–2121,112012.
doi:10.3390/e14112100.
Noor Sajid, Philip J. Ball, Thomas Parr, and Karl J. Friston. Active inference: Demystified and compared.
Neural Computation, 33(3):674–712, January 2021a. ISSN 0899-7667. doi:10.1162/neco_a_01357. URL
https://doi.org/10.1162/neco_a_01357.
PietroMazzaglia,TimVerbelen,OzanÇatal,andBartDhoedt. Thefreeenergyprincipleforperceptionandaction:A
deeplearningperspective. Entropy,24(2):301,2022.
BerenMillidge, AlexanderTschantz,AnilK.Seth, andChristopherL.Buckley. On therelationshipbetweenactive
inferenceandcontrolasinference. InTimVerbelen,PabloLanillos,ChristopherL.Buckley,andCedricDeBoom,
editors, Active Inference, pages3–11, Cham, 2020.SpringerInternationalPublishing. ISBN 978-3-030-64919-7.
doi:https://link.springer.com/chapter/10.1007/978-3-030-64919-7_1.
Mahault Albarracin, Inês Hipólito, Safae Essafi Tremblay, Jason G. Fox, Gabriel René, Karl Friston, and Maxwell
J. D. Ramstead. Designing explainableartificial intelligence with active inference: A frameworkfor transparent
introspectionanddecision-making,2023.
Karl Friston, Lancelot Da Costa, Danijar Hafner, Casper Hesp, and Thomas Parr. Sophisticated inference.
Neural Computation, 33(3):713–763, February 2021. ISSN 0899-7667. doi:10.1162/neco_a_01351. URL
https://doi.org/10.1162/neco_a_01351.
Raphael Kaplan and Karl J. Friston. Planning and navigation as active inference. Biological
Cybernetics, 112(4):323–343, 2018. ISSN 1432-0770. doi:10.1007/s00422-018-0753-2. URL
https://doi.org/10.1007/s00422-018-0753-2.
Franz Kuchling, Karl Friston, Georgi Georgiev, and Michael Levin. Morphogenesis as bayesian infer-
ence: A variational approach to pattern formation and control in complex biological systems. Physics of
Life Reviews, 33:88–108, 2020. ISSN 1571-0645. doi:https://doi.org/10.1016/j.plrev.2019.06.001. URL
https://www.sciencedirect.com/science/article/pii/S1571064519300909.
Alexander Tschantz, Anil K. Seth, and Christopher L. Buckley. Learning action-oriented models through ac-
tive inference. PLOS Computational Biology, 16(4):1–30, 04 2020. doi:10.1371/journal.pcbi.1007805. URL
https://doi.org/10.1371/journal.pcbi.1007805.
ThomasParr and Karl J. Friston. The discrete and continuousbrain: From decisions to movement-andback again.
Neural computation, 30(29894658):2319–2347,September 2018. ISSN 0899-7667. doi:10.1162/neco_a_01102.
URLhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC6115199/.
19
Onefficientcomputationinactiveinference APREPRINT
William S. Lovejoy. A survey of algorithmic methods for partially observed markov decision processes. An-
nals of Operations Research, 28(1):47–65, 1991. ISSN 1572-9338. doi:10.1007/BF02055574. URL
https://doi.org/10.1007/BF02055574.
Guy Shani, Joelle Pineau, and Robert Kaplow. A survey of point-based pomdp solvers. Autonomous Agents
and Multi-Agent Systems, 27(1):1–51, 2013. ISSN 1573-7454. doi:10.1007/s10458-012-9200-2. URL
https://doi.org/10.1007/s10458-012-9200-2.
Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. Planning and
acting in partially observable stochastic domains. Artificial Intelligence, 101(1):99–
134, 1998. ISSN 0004-3702. doi:https://doi.org/10.1016/S0004-3702(98)00023-X. URL
https://www.sciencedirect.com/science/article/pii/S000437029800023X.
JelleBruineberg,ErikRietveld,ThomasParr,LeendertvanMaanen,andKarlJFriston. Free-energyminimizationin
jointagent-environmentsystems: A nicheconstructionperspective. Journaloftheoreticalbiology, 455:161–178,
2018.
Noor Sajid, Panagiotis Tigas, Alexey Zakharov, Zafeirios Fountas, and Karl Friston. Exploration and preference
satisfactiontrade-offinreward-freelearning. InICML2021WorkshoponUnsupervisedReinforcementLearning,
2021b.
KarlFriston. Afreeenergyprincipleforaparticularphysics,2019.
David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational Inference: A Review for Statisticians. Jour-
nal of the American Statistical Association, 112(518):859–877, April 2017. ISSN 0162-1459, 1537-274X.
doi:10.1080/01621459.2017.1285773.
Noor Sajid, Francesco Faccio, Lancelot Da Costa, Thomas Parr, Jürgen Schmidhuber, and Karl Friston. Bayesian
brainsandtherényidivergence. NeuralComputation,34(4):829–855,2022a.
Nicholas Rhinehart, Jenny Wang, Glen Berseth, John Co-Reyes, Danijar Hafner, Chelsea Finn, and Sergey Levine.
Informationispower:intrinsiccontrolviainformationcapture.AdvancesinNeuralInformationProcessingSystems,
34:1074745–10758,2021.
Glen Berseth, Daniel Geng, Coline Devin, Chelsea Finn, Dinesh Jayaraman, and Sergey Levine. Smirl: Surprise
minimizingrlindynamicenvironments. arXivpreprintarXiv:1912.05510,2019.
Bert de Vries and Karl J. Friston. A factor graph description of deep temporal active inference. Fron-
tiers in Computational Neuroscience, 11:95, 2017. ISSN 1662-5188. doi:10.3389/fncom.2017.00095. URL
https://www.frontiersin.org/article/10.3389/fncom.2017.00095.
Karl J. Friston, Thomas Parr, and Bert de Vries. The graphical brain: Belief propagation and ac-
tive inference. Network Neuroscience, 1(4):381–414, 2017. doi:10.1162/NETN_a_00018. URL
https://doi.org/10.1162/NETN_a_00018.
Noor Sajid, Lancelot Da Costa, Thomas Parr, and Karl Friston. Active inference, bayesian optimal design, and
expectedutility. TheDriveforKnowledge:TheScienceofHumanInformationSeeking,page124,2022b.
PhilippSchwartenbeck,JohannesPassecker,TobiasUHauser,ThomasHBFitzGerald,MartinKronbichler,andKarlJ
Friston. Computationalmechanismsofcuriosityandgoal-directedexploration. Elife,8:e41703,2019.
Thomas Parr and Karl J. Friston. Generalised free energy and active inference. Biological Cy-
bernetics, 113(5):495–513, 2019. ISSN 1432-0770. doi:10.1007/s00422-019-00805-w. URL
https://doi.org/10.1007/s00422-019-00805-w.
Anthony Triche, Anthony S. Maida, and Ashok Kumar. Exploration in neo-hebbian reinforcement learning:
Computational approaches to the exploration–exploitation balance with bio-inspired neural networks. Neu-
ral Networks, 151:16–33, 2022. ISSN 0893-6080. doi:https://doi.org/10.1016/j.neunet.2022.03.021. URL
https://www.sciencedirect.com/science/article/pii/S0893608022000995.
ThéophileChampion,LancelotDaCosta,HowardBowman,andMarekGrzes´.BranchingTimeActiveInference:The
theoryanditsgenerality. arXiv:2111.11107[cs],November2021a.
ThéophileChampion,HowardBowman, andMarekGrzes´. BranchingTime ActiveInference: Empiricalstudyand
complexityclassanalysis. arXiv:2111.11276[cs],November2021b.
O. Çatal, T. Verbelen, J. Nauta, C. D. Boom, and B. Dhoedt. Learning perception and planning with deep active
inference. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP),pages3952–3956,2020. doi:10.1109/ICASSP40776.2020.9054364.
20
Onefficientcomputationinactiveinference APREPRINT
LancelotDa Costa, ThomasParr,Biswa Sengupta,andKarlFriston. Neuraldynamicsunderactiveinference: Plau-
sibilityandefficiencyofinformationprocessing. Entropy,23(4),2021. ISSN1099-4300. doi:10.3390/e23040454.
URLhttps://www.mdpi.com/1099-4300/23/4/454.
NoorSajid,PanagiotisTigas,ZafeiriosFountas,QinghaiGuo,AlexeyZakharov,andLancelotDaCosta. Modelling
non-reinforcedpreferencesusingselectiveattention. arXivpreprintarXiv:2207.13699,2022c.
AswinPaul,NoorSajid,ManojGopalkrishnan,andAdeelRazi. Activeinferenceforstochasticcontrol. InMachine
LearningandPrinciplesandPracticeofKnowledgeDiscoveryinDatabases,pages669–680,Cham,2021.Springer
InternationalPublishing. ISBN978-3-030-93736-2.doi:https://doi.org/10.1007/978-3-030-93736-2_47.
DiZhou,MinSheng,JieLuo,RunziLiu,JiandongLi,andZhuHan. Collaborativedataschedulingwithjointforward
andbackwardinductioninsmallsatellitenetworks.IEEEtransactionsoncommunications,67(5):3443–3456,2019.
21
Onefficientcomputationinactiveinference APREPRINT
A Derivationofoptimal state-belief
Wewanttodifferentiatethefollowingw.r.t. Q(s):
F = Q(s)[logQ(s)−logP(o|s)−logP(s)]. (A.1)
Xs
First,notethatthederivativeofthelogarithmfunctionis 1. Second,observethatthederivativeofQ(s)withrespect
x
toQ(s)is1. Withthesetwopiecesinmind,wecandifferentiateF:
Let’sdefinef(s)=logQ(s)−logP(o|s)−logP(s),then:
dF df dQ(s)
= [ ·Q(s)+f(s)· ] (A.2)
dQ(s) dQ(s) dQ(s)
Xs
Thederivativeoff(s)withrespecttoQ(s)canbecomputedas:
df 1 1
= −0−0= (A.3)
dQ(s) Q(s) Q(s)
Thisleadsto:
dF 1
= [Q(s)· +f(s)] (A.4)
dQ(s) (cid:18)Q(s)(cid:19)
Xs
= 1+logQ(s)−logP(o|s)−logP(s) (A.5)
Xs
So,thederivativeofF withrespecttoQ(s)is:
dF
= 1+logQ(s)−logP(o|s)−logP(s) (A.6)
dQ(s)
Xs
ThegoalistominimizethefreeenergyF withrespecttothedistributionQ(s). Tofindtheminimum,wecansetthe
derivativeofF withrespecttoQ(s)tozero.Fromthepreviousderivation,weknowthat:
dF
= 1+logQ(s)−logP(o|s)−logP(s) (A.7)
dQ(s)
Xs
Settingthisequaltozerogives:
1+logQ(s)−logP(o|s)−logP(s)=0 (A.8)
logQ(s)=logP(o|s)+logP(s)−1 (A.9)
However, note that the log function is typically normalized such that the sum of the probabilities in Q(s) equals 1
(sinceQ(s)isaprobabilitydistribution),sowecansafelyignorethe−1term:
logQ(s)=logP(o|s)+logP(s) (A.10)
TheoptimaldistributionQ∗(s)thatminimizesthefreeenergyF isthus:
logQ∗(s)=logP(o|s)+logP(s). (A.11)
22
Onefficientcomputationinactiveinference APREPRINT
B Optimisinglearning parameter forAIF (T=1)agent
FigureB.1:AsamplegraphofmanualoptimisationforeforZ-learningintheAIF(T=1)algorithm.A:Forastochastic
gridwith100states,B:Forastochasticgridwith400states. Weobservethattheagent’sperformanceisnotheavily
dependentonthevalueofethatcontrolsthelearningparameterη .
t
23

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "On efficient computation in active inference"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
