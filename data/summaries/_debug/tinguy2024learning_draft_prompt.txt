=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Learning Dynamic Cognitive Map with Autonomous Navigation
Citation Key: tinguy2024learning
Authors: Daria de Tinguy, Tim Verbelen, Bart Dhoedt

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2024

Abstract: Inspired by animal navigation strategies, we introduce a novel computational model to navigate
andmapaspacerootedinbiologicallyinspiredprinciples.Animalsexhibitextraordinarynavigation
prowess,harnessingmemory,imagination,andstrategicdecision-makingtotraversecomplexand
aliased environments adeptly. Our model aims to replicate these capabilities by incorporating a
dynamically expanding cognitive map over predicted poses within an Active Inference framework,
enhancingouragent’sgenerativemodelplasti...

Key Terms: navigation, novel, autonomous, environments, dynamically, daria, dynamic, cognitive, expanding, learning

=== FULL PAPER TEXT ===

Learning Dynamic Cognitive Map with
Autonomous Navigation
Daria de Tinguy1,∗, Tim Verbelen2 and Bart Dhoedt1
1Ghent University, Ghent, Belgium
2Verses, Los Angeles, California, USA
Correspondence*:
Daria de Tinguy
daria.detinguy at ugent.be
ABSTRACT
Inspired by animal navigation strategies, we introduce a novel computational model to navigate
andmapaspacerootedinbiologicallyinspiredprinciples.Animalsexhibitextraordinarynavigation
prowess,harnessingmemory,imagination,andstrategicdecision-makingtotraversecomplexand
aliased environments adeptly. Our model aims to replicate these capabilities by incorporating a
dynamically expanding cognitive map over predicted poses within an Active Inference framework,
enhancingouragent’sgenerativemodelplasticitytonoveltyandenvironmentalchanges. Through
structure learning and active inference navigation, our model demonstrates efficient exploration
and exploitation, dynamically expanding its model capacity in response to anticipated novel
un-visited locations and updating the map given new evidence contradicting previous beliefs.
Comparative analyses in mini-grid environments with the Clone-Structured Cognitive Graph
model (CSCG), which shares similar objectives, highlight our model’s ability to rapidly learn
environmental structures within a single episode, with minimal navigation overlap. Our model
achieves this without prior knowledge of observation and world dimensions, underscoring its
robustness and efficacy in navigating intricate environments.
Keywords:autonomousnavigation;activeinference;cognitivemap;structurelearning;dynamicmapping;knowledgelearning
1 INTRODUCTION
Humans effortlessly discern their position in space, plan their next move, and rapidly grasp the layout
of their surroundings (1, 2) when faced with ambiguous sensory input (3). Replicating these abilities
in autonomous artificial agents is a significant challenge, requiring robust sensory systems, efficient
memorymanagement,andsophisticateddecision-makingalgorithms.Unlikehumans,artificialagentslack
inherentcognitiveabilitiesandadaptivelearningmechanisms,particularlywhenconfrontedwithaliased
observations,wheresensoryinputsareambiguousormisleading(4).
To replicate human navigational abilities, an agent must capture the dynamic spatial layout of the
environment, localise itself and predict the consequences of its actions. Most attempts to achieve this
combine those fundamental elements in SLAM algorithms (Simultaneous Localisation and Mapping),
oftenbasedonEuclidianmaps(5,6).However,thesemethodsrequiresubstantialmemoryastheworld
expands. Other strategies involve deep learning models, which depend on large datasets and struggle to
adapttounexpectedeventsnotencounteredduringtraining(7).Amoreefficientalternativeliesincognitive
graphsormapsandlearningamentalrepresentationoftheworldfrompartialobservations(8,9),creating
a symbolic structure of the environment (10, 11). Cognitive graphs, by definition, represent a ”mental
understandingofanenvironment”derivedfromcontextualcueslikespatialrelationships(12).Alongside
1
4202
voN
31
]OR.sc[
1v74480.1142:viXra
de Tinguy et al. Learning Dynamic Cognitive Map with Autonomous Navigation
this structure, the ability to imagine the outcomes of actions enables more reliable navigation decisions
basedonpreferences(13,14).
Our approach integrates those biological capabilities into a unified model. Using visual observations
andproprioception(15),weconstructacognitivemapthroughagenerativemodel,enablingnavigation
with an Active Inference (AIF) framework. This model links states by incorporating observations and
positions throughtransitions, as illustrated inFigure 1b), showing the processedobservation of the agent
andc)presentingtheresultingcognitivegraph.UsingBayesianinference,themodelpredictsfuturestates
Figure1. a)Fromafull3by3roomsmini-gridenvironment(16,17)tob)roomsobservationlayoutas
perceivedbytheagentandthepathithastakenbetweenrooms-composedofalinefromblacktowhite-,
c)showstheagentfinalinternaltopologicalgraph(cognitivegraph)linkingallthelocationsbetweenthem.
and positions, growing its cognitive map by forming prior beliefs about un-visited locations. As new
observationsaremade,theagentupdatesitsinternalmodel,dynamicallyrefiningitsrepresentationofthe
environment(11).Thiscontinualadjustmentallowstheagenttoeffectivelynavigatecomplexenvironments
byanticipatingandlearningfromunchartedareas(18).Ourinternalpositioningsystemdrawsinspiration
fromtheneuralpositioningsystemfoundinrodentsandprimates,aidinginself-localisationandproviding
anintrinsicmetricformeasuringdistanceandrelativedirectionbetweenlocations(15,3,19).
To achieve goal-directed navigation and exploration, we employ AIF to model the agent’s intrinsic
behaviourinabiologicallyplausibleway.Unlikemethodsrelyingonpre-trainingforspecificenvironments,
ourapproachintroducesa navigationanddynamiccognitivemapgrowingbased ontheFreeEnergy(FE)
principle. This map is inspired by mechanisms observed in animals, such as border cells for obstacle
detection (20) and the visual cortex for visual perception. It continuously expands by predicting new
observationsandadaptingdynamicallytochangingenvironmentalstructures.
This work aims to develop an autonomous agent that can determine where it is, decide where to
navigate,andlearnthestructureofcomplex,unknownenvironmentswithoutpriortraining,mimickingthe
adaptability and spatial awareness observed in biological organisms. Traditional exploration approaches
and deep learning models struggle in dynamic settings, requiring extensive memory and pre-collected
datasetstopredictfuturesettings,ortheyfacedifficultiesinadaptingtountrainedsituations.Thechallenge
istodesignamodelthatallowsagentstoautonomouslybuild,update,andexpandaninternalmapbased
on current sensory data and past beliefs, efficiently managing ambiguous observations (such as aliased
states)andrespondingflexiblytounexpectedenvironmentalchanges.
Ourcontributiontothisproblemencompassesseveralkeyaspects:
2
de Tinguy et al. Learning Dynamic Cognitive Map with Autonomous Navigation
• Proposinganoveldynamiccognitivemappingapproachthatallowsagentstopredictandextendtheir
internalmapoverimaginedtrajectories,enablinganticipatorynavigationandrapidadaptationtonew
environments.
• Developinganavigationmodelthatoperateswithoutpre-trainingorpriorexposure,allowingtheagent
tosuccessfullyexploreandmakedecisionsinunfamiliarenvironments.
• ProposingaflexiblenavigationbehaviourfullyexplicitbyrelyingupontheAIFframework.
• Outperforming in environmental learning and decision-making efficiency the Clone-Structured
CognitiveGraph(CSCG)model(21),aprominentmodelforcognitivemaprepresentation(21).
• Showcasing robust adaptability, where the model responds seamlessly to dynamic environmental
changes, replicating rat maze-like scenarios, thus emphasising its practical application in flexible,
real-worldnavigationtasks.
• Incorporatingbiologicallyinspiredprocesseslikebordercellsandvisualcortexperception,ouragent’s
navigationstrategyistheoreticallygroundedandscalabletomorerealisticsettings.
2 RELATED WORK
Asagentsnavigatetheirsurroundings,theyconnectobservationstoconstructaninternalmaporgraphof
theenvironment.Thisintuitivedecision-makingprocessispropelledbyincentivessuchasfood,safety,or
exploration,rapidlyguidingagentstowardtheirobjective(2).
Motionplanning
Navigationtasksareoftencategorisedintotwoprimaryscenariosbasedontheagent’sfamiliaritywith
theenvironmentanditsobjectives.Inscenarioswheretheenvironmentispartiallyknownandtheagentis
already aware of its destination, the primary focus is achieving efficient retrieval and executing actions
reliably. Thisentailsleveragingexisting knowledgeabout theenvironment’sstructureand landmarks to
navigate swiftly and accurately towards the predetermined goal (7). Typically, this task centers around
solvingthemotionplanningproblem:”HowtomovefrompointAtoB?”Methodssuchasthosein(22,23)
propose enhanced versions of Model Predictive Path Integral (MPPI) navigation, which calculates the
optimal sequence of control actions by simulating multiple future trajectories and selecting the one
minimising a cost function. They allow real-time adaptation to obstacles in dynamic environments by
continuously updating their navigation based on the most recent state and cost estimates of the control
trajectory.However,thesemethodsoftenrelyonpreciselocalisationwithinalocalmap,requiresubstantial
computationalcapacity,andmaybevulnerabletosensorfailures.
Goalidentification
Our current model is not focusing specifically on solving motion planning, but on determining where
theagentisandwhereitshouldgobasedonavailableinformation.Unliketraditionalmodels,itdoesnot
require precise or absoluteknowledge of its position; instead, it relies on its internally inferredbelief about
itslocationandconsidersobstaclesrelativetothispositionratherthantoaglobalmap.Asaresult,sensor
failuresarelesscritical, providedthe agentcanadapttosituationswitheither nosensoryinputormultiple
inputs. In this work, the agent is expected to receive visual observations and detect obstacles through a
systemanalogoustoLiDAR.Failureofthesesensors,however,wouldhaltnavigation.
When the agent must determine both its location and intended direction in addition to planning its
movement, the task becomes significantly more complex, especially in unknown environments. The
agent must engage in map building and employ a form of reasoning to effectively operate and navigate
3
de Tinguy et al. Learning Dynamic Cognitive Map with Autonomous Navigation
throughunfamiliarsurroundings(24,25).Thisinvolvesdynamicallyconstructingacognitivemapofthe
environment,integratingsensoryinformation,andadaptingbehaviourbasedonlearnedspatialrelationships
and environmental cues (26, 27). Works relying on Gaussian processes such as (28) unify navigation,
mapping, and exploration, reducing redundancy and improving exploration efficiency, however, it is
weak to featureless environments, over-reliant on a correct position estimation and are weak to sudden
displacements. Our research addresses the challenges posed by determining where it should go and
how it should do it by proposing an agent capable of seeking information gain and reaching desired
observationsinbothfamiliarandunfamiliarenvironments.Ourapproachensuresrobustperformanceeven
inscenariosinvolvingpotentialdisruptions,suchaskidnapping,repeatedobservationsorenvironmental
modifications,withoutrequiringanyenvironment-specificpre-training.Throughthisframework,weseek
to advance the capabilities of fully autonomous agents in navigating and solving tasks in diverse and
dynamicenvironments.
Spatialrepresentation
Spatialrepresentationplaysasignificantroleinrobotnavigationandboastsarichhistory,offeringdiverse
approacheswiththeirownsetofadvantagesandchallenges(6).WhilemanySLAMsystemsrelyonmetric
mapstonavigate,whichprovideprecisespatialinformation(4,5),thereisagrowinginterestintopological
mapping(29,28)duetoitsbiologicalplausibilityandlowercomputationalmemoryrequirements.Cognitive
mapsarementalrepresentationsofspatialknowledgethatexplainhowagentsnavigateandapprehendtheir
environment(21).Acognitivemapencompassesthelayoutofphysicalspaces(9),landmarks,distances
betweenlocations,andtherelationshipsamongdifferentelementswithintheenvironment(30,10).Models
like the Clone-Structured Cognitive Graph (CSCG) (31) and Transformer representations (32) create
cognitivemaps(usuallyrepresentedastopologicalgraphs)usingpartialobservations,offeringreusable,
and flexible representations of the environment. However, these models often entail significant training
time, typicallyinvolving fixedpolicies orrandom motionsandrequire astatically defined cognitive map
dimension.
In biological systems, such as animals, the hippocampus plays a crucial role in managing episodic
memory, spatial reasoning, and rapid learning (33) while structured knowledge about the environment
is gradually acquired by the neocortex (3, 34). This enables remarkable adaptability and efficiency in
navigation,withanimalsoftenrequiringminimalinstancestolearnandnavigatecomplexenvironments
(1). They leverage cognitive mapping strategies to adapt to changes, swiftly grasp the layout of their
surroundings,andefficientlyreturntopreviouslyvisitedplaces.
Drawing inspiration from these natural mechanisms, the compact cognitive map (35) proposes an
extendableinternal map basedon movementinformation (howfar theagent is fromany pastlocation) and
currentvisualrecognition.Oursystemsharesthisgoalofrememberingsignificantobservations,forminga
spatialrepresentation,andresolvingambiguitythroughcontextualcues(36).Furthermore,weextendits
adaptabilitybyproactivelyforecastingtheextensionoftheinternalcognitivemapbeforetheexperience
ofnewspatialinformation.Thisanticipatorycapabilityempowersthesystemtopredictpotentialaction
outcomes in unobserved areas, enhancing its ability to navigate in unknown territory. This proactive
approachalignsmorecloselywiththeefficientdecision-makingprocessesobservedinbiologicalagents,
allowingforrapidadaptationandeffectivenavigationindiverseenvironments.
ActiveInference
Animalsnavigatetheirenvironmentsadeptlybycombiningsensoryinputswithproprioception(3).This
enablesthemtoavoidbeingmisledbyrepeatedevidence,aphenomenonknownasaliasing.Suchnavigation
4
de Tinguy et al. Learning Dynamic Cognitive Map with Autonomous Navigation
strategiescan beelucidated throughActiveInference (13). ActiveInference appliedtonavigation, entails
the continual refinement of internal models based on sensory feedback, facilitating adaptive and effective
decision-making within the environment. Serving as a normative framework, AIF elucidates cognitive
processingandbraindynamicsinbiologicalorganisms(37,38).Itpositsthatbothactionandperception
aimto minimisean agent’sFree Energy, acting asan upperlimit tosurprise. Central toactiveinference
are generativemodels, encapsulating causal relationshipsamong observable outcomes, agent actions, and
hidden environmental states. These environmental states remain ’hidden’ as they are shielded from the
agent’sinternalstatesbyaMarkovblanket(8).Leveragingpartialobservations,theagentconstructsits
own beliefs regarding hidden states, enabling action selection and subsequent observation to refine its
beliefsrelyingonthePartiallyObservableMarkovDecisionModel(POMDP)(39).
At the heart of decision-making and adaptive behaviour lies a delicate balance between exploitation
and exploration (40). Exploitation involves selecting the most valuable option based on existing beliefs
abouttheworld,whileexplorationentailschoosingoptionstolearnandunderstandtheenvironment(11).
Recentbehaviouralstudiesindicatethathumansengageinacombinationofbothrandomandgoal-directed
exploration strategies (14). In our model, policies are chosen stochastically using the concept of FE.
Regardlessofthestrategyemployed,theagentattemptstominimisesurprisebyformulatingpoliciesthat
increase the likelihood of encountering preferred states, whether it is a specific observation (e.g. food)
orthecuriositytocomprehendtheenvironment’sstructure(40).Thisapproachfacilitatesactivelearning
by swiftly diminishing uncertainty regarding model parameters and enhancing knowledge acquisition
aboutunknown contingencies. Italsoregulatesmodelparameterupdates inresponsetonewevidencein
achangingenvironment.Typicallythisdecision-makingreliesonpriorsoverthecurrentworldstructure
and pastobserved outcomes topredict the nextpolicies, usually leaning onstatic structures. Past research
suchasBayesianmodelreductiontechniques(41),orselectionmechanisms(42),aimtoexpandmodelsto
accommodateemergingpatternsofobservation,relyingonpastandcurrentobservations.Weproposeto
goonestepfurtherbyextendingourcognitivemapoverpredictedoutcomes,allowingtheagenttoreason
overfutureun-visitedstates.
All those mechanisms lead an agent to efficiently explore or forage autonomously in any structured
environment,evenifitchanges.Itislearningasitgoesinafew-shotorone-shotlearningaswouldmicein
alabyrinth(2)relyingonpredictionsandobservations.
3 METHOD
Themethodsectiondescribesourapproachtodynamiccognitivemappingandnavigationinunfamiliar
environments. Figure2 provides astep-by-step overview ofour model’s process, visualisedas aflowchart.
Thisincludesobservingtheenvironment, updatinginternalmatriceswithnewinformation, inferringthe
currentstate,predictingpossiblemotions,andselectingthenextactionbasedonupdatedbeliefs.
To demonstrate the agent’s navigation abilities, we tested it in a series of mini-grid environments,
as illustrated in figure 1, where each map comprises rooms with a consistent floor colour connected by
randomlypositionedcorridorsseparatedbycloseddoors,allthemaplayoutsusedinthisworkarepresented
inAppendix1andwillbefurtherdetailedinsection4.
Our agent starts exploring without prior knowledge of the environment’s dimensions or potential
observations. It starts by inferring the initial state and pose from the initial observation and expands
its model based on anticipating the outcomes of moving in the four cardinal directions. It accounts for
potential unexplored adjacent areas before formulating decision-making policies, based on the model’s
5
de Tinguy et al. Learning Dynamic Cognitive Map with Autonomous Navigation
Figure2. Thisblockdiagramoutlinestheagent’sdecision-makingandmappingprocess.Theagentbegins
by perceiving an observation, which updates the A matrix—representing the observation model—by
o
adjustingitsdimensionstoincorporatenewsensorydata.Theagenttheninfersitscurrentstatebasedonthis
updatedmodel.Next,itpredictstheoutcomesofpossiblemotionsinalldirections,enablingittoupdatethe
modelinrelevantdirections.Thetransitionmodels,B ,B ,A andA ,areupdatedwithanticipatednew
s p p o
transitions(tonewstateorsamestateifobstacle),whereB representsprobabilistictransitionsbetween
p
positions dueto motion,B captures statetransition basedon imaginedspatial structure, andA encoding
s p
theprobabilityofapositiongivenastate.Usingthisinformation,theagentdecidesonitsnextactionand
executesitbymoving,repeatingtheprocess.
existing beliefs. In Figure 1a), the agent’s traversal of an environment is depicted, with each room
encoded as a state by the model and the visual observation being the colour of the room’s floor. Motion
betweenroomsthroughadoorsuggestsatransitionbetweenstates,whilewallsarerecognisedasobstacles
through RGB observations. The presented inference mechanism is expected to operate at the highest
levelofabstractionwithinahierarchicalframeworksuchas(17),wherelowerlayershandlemotion,the
observation processanddetect thepresenceofobstacles, akinto theoutputexpected from,respectively,
ourmotorcortex,visualcortexandbordercellsmechanism(20).
Figure1b)illustratesouragentnavigationthroughprocessedobservations,inthecaseofourmini-grid
environments,thelowerlevelsofthehierarchicalmodelsummarisetheroominformationtoasinglecolour
(i.e., the floor colour) and obstacle detection (i.e., walls or doors) fed to the highest level of abstraction
(i.e.,presentedmodel).Thepresentedmodelgeneratesaninternaltopologicalmapgivenmotionsbetween
roomssuchasthefinalmapshowninFigure1c).Eachstatecontainsinformationabouttheroom‘sfloor
colouranditsinferredposition.
3.1 State Inference
Beforeeachstep,theagentengagesinstateinferencebyintegratingthelatestobservationandinternal
positioning,followingthePOMDPgraphdepictedinFigure3,wherethecurrentstates andpositionp are
t t
inferredbasedonthepreviousstates ,positionp andactiona leadingtothecurrentobservation
t−1 t−1 t−1
6
de Tinguy et al. Learning Dynamic Cognitive Map with Autonomous Navigation
o .ThegenerativemodelcapturingthisprocessisdescribedbyEquation(1),wherethejointprobability
t
distribution over time sequences of states, observations, and actions is formulated. Tildes (˜) denote
sequencesovertime.
τ
(cid:89)
P(o˜,s˜,p˜,a˜) = P(o |s )P(s )P(p ) P(o |s )P(s ,p |s ,p ,a ) (1)
0 0 0 0 t t t t t−1 t−1 t−1
t=1
The inference process operates within an AIF framework, where sensory inputs and prior beliefs are
integratedtoinfertheagent’scurrentstate.
Duetotheposteriordistributionoverastatebecomingintractableinlargestatespaces,weusevariational
inferenceinstead.ThisapproachintroducesanapproximateposteriordenotedasQ(s˜,p˜|o˜,a˜)andpresented
inEquation(2)(43).
τ
(cid:89)
Q(s˜,p˜|o˜,a˜) = Q(s ,p |o ) Q(s ,p |s ,p ,a ,o ) (2)
0 0 0 t t t−1 t−1 t−1 t
t=1
TheinferenceschemedefinedinEquation(2),heavilyreliesonpriorsandobservationstolocalisethe
agentwithinitsenvironment.
Therefore,weassociateobservationso ,stemmingfromvisualinformation,withourinferredposition
t
p , generated by the agent’s proprioception. This improves state inference in an aliased environment,
t
wheretheagentissimultaneouslyinferringitsstateandbuildingitscognitivemap.Intheabsenceofprior
information, the internal positioning p is initialised at the start of exploration as an origin (i.e., a tuple
(0,0)).Itisthenupdatedwhentheagenttransitionsbetweenrooms(e.g.,bypassingthroughadoor).
When theagent is certain aboutits current state, it updatespast beliefs aboutobservations andtransitions
in responseto environmental changes. This ensuresrapid re-alignment betweenthe agent’smodel and the
evolvingenvironment, withinoneorafew iterations, dependingontheagent’sconfidenceinitsoutdated
beliefs.Thespecificlearningmechanismofourmodelwillbeexplainedinsection3.4.
In scenarios where the agent is unexpectedly relocated to an unfamiliar position, as tested in some of
ourexperiments,itscertaintyaboutitslocationdecreases,sinceitscurrentstateandpreviousactionsno
longer match its present observations. This drop in confidence occurs as the probability of being in a
particularstate—consideringbothpositionandobservation—fallsbelowapredefinedcertaintythreshold
(see Appendix 2.1 for exact values). When this threshold is crossed, the agent suspends reliance on
its position information to infer state, focusing instead exclusively on observations to regain sufficient
confidence over its state. The position and model updates are paused during this re-estimation of its
locationuntiltheagentachievesadequateconfidenceinitsstate.Thisapproach,guidedbyFreeEnergy
minimisation, prioritises gatheringinformation about itswhereabouts, fosteringinformed decision-making
inuncertainconditionsandencouragingaresilientcomprehensionofitsnewstateandposition.
3.2 Free Energy
Typically,agentsare assumed tominimisetheirvariational freeenergydenotedF, whichcanserveasa
metric to quantify the discrepancy between the joint distribution P and the approximate posterior Q as
presented in Equation (3) (38). This equation shows how adequate is the model to explain the past and
currentobservations.
7
de Tinguy et al. Learning Dynamic Cognitive Map with Autonomous Navigation
Figure3. FactorgraphofthePOMDPinourgenerativemodel,showingtransitionsfromthepasttothe
present(uptotime-stept)andextendingintothefuture(time-stept+1).Pastobservationsaremarkedin
blue,indicatingtheyareknown.Inthefuturesteps,actionsfollowapolicyπ influencingthenewstates
andposition inorangeand newpredictions ingrey. Thepositionat timet, p , isdeterminedby thepolicy
t
andthepriorposition p ,whilethecurrentstate s isinferredfromtheobservationo ,thepositionp ,
t−1 t t t
and the previous state s . Transitions between states are ruled by the B matrices, which define how
t−1
priorconditionscontributetothecurrentoneconsideringtakenactions.Amatricesrepresentconditional
probabilitiesofthequantitiestheyconnect.
F = E [log[Q(s˜,p˜|a˜,o˜)]−log[P(s˜,p˜,a˜,o˜)]]
Q(s˜,p˜|a˜,o˜)
= D [Q(s˜,p˜|a˜,o˜))||P(s˜,p˜|a˜,o˜)]−log[P(o˜)]
KL
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
(3)
posteriorapproximation logevidence
= D [Q(s˜,p˜|a˜,o˜))||P(s˜,p˜|a˜)]−E [log[P(o˜|s˜)]
KL Q(s˜,p˜|a˜,o˜)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
complexity accuracy
Activeinference agents aim to minimisetheir free energy by engagingin three main processes: learning,
perceiving,andplanning.Learninginvolvesoptimisingthemodelparameters,perceivingentailsestimating
themostlikelystate, andplanninginvolvesselectingthepolicyoraction sequence thatleadstothelowest
expectedfreeenergy.Essentially,thismeansthattheprocessimplicatesformingbeliefsabouthiddenstates
offeringapreciseandconciseexplanationofobservedoutcomeswhileminimisingcomplexity(37).
Whileplanning,weminimisetheExpectedFreeEnergy(EFE)instead,denotedG,indicatingtheagent’s
anticipatedfreeenergyafterimplementingapolicyπ.UnlikethevariationalfreeenergyF,whichfocuses
on current and past observations, the expected free energy incorporates future expected observations
generatedbytheselectedpolicy.
TocalculatethisexpectedfreeenergyG(π)overeachstepτ ofapolicywesumtheexpectedfreeenergy
ofeachtime-step.
(cid:88)
G(π) = G(π,τ) (4)
τ
8
de Tinguy et al. Learning Dynamic Cognitive Map with Autonomous Navigation
G(π,τ) =E [log(Q(s ,p |π)−log(Q(s ,p |o ,π))]
Q(oτ,sτ,pτ|π) τ τ τ τ τ
(cid:124) (cid:123)(cid:122) (cid:125)
informationgainterm
(5)
−E [log(P(o ))]
Q(oτ,sτ,pτ|π)) τ
(cid:124) (cid:123)(cid:122) (cid:125)
utilityterm
Theexpectedinformationgainquantifiestheanticipatedshiftintheagent’sbeliefoverthestatefromthe
priorQ(s |π)totheposteriorQ(s |o ,π)whenpursuingaparticularpolicy.Ontheotherhand,theutility
τ τ τ
termassessestheexpectedlogprobabilityofobservingthepreferredoutcomeunderthechosenpolicy.This
valueintuitivelymeasuresthelikelihoodthatthepolicywillguidetheagenttowarditspriorpreferences.
Priorpreferencesareembeddedwithintheagent’smodelasanobjectiveortargetstatetheagentshould
worktoward.Usuallygivenasapreferredstateorobservation.FreeEnergyindirectlyencouragesoutcomes
that align with its preferences or target states. This approach makes the utility term less about ”reward”
inthetraditionalsenseofReinforcementLearningandmoreaboutachievingcoherencewiththeagent’s
built-inpreferences,balancingthiswithexploration(informationgain).Theagentthereforechoosesthe
optimalpolicytofollowamongpossiblepoliciesbyapplying:
P(π) = σ(−γG(π)) (6)
Where σ, the softmaxfunction istempered witha temperatureparameter γ convertingthe expected free
energy of policies into a categorical distribution over policies. Playing with the temperature parameter
alters thestochasticity ofthe navigation, the agentbeing more orless likely tochoose the optimalpolicy
ratherthananyotherone.DetailsaboutthedefinitionofpoliciescanbefoundinAppendix2.2.
3.3 Active Inference and Cognitive Map Navigation
Whennavigatingwithinanenvironment,theagentusesAIFtocontinuouslyrefineitsknowledgeofthe
world by updating itsmodel parameters, more specifically, transition probabilities (how likely it is to move
fromonestatetoanother)andlikelihoods(howlikelyitistoobservecertainfeaturesgivenastate).These
quantitiesareupdated,consideringtheactionstakenbytheagentandtheobservationsgatheredresulting
from them. These updateshelp the agentto reduce the gapbetween its predicted andactual observations,
effectivelyfine-tuningitsinternalmodeltomatchtherealenvironmentbetter.Thisapproachallowsthe
agent to anticipate future states better and select actions that will minimise future EFE, thus optimising
itsnavigationstrategy.Typically, thisprocessassumesthattheagenthassomepriorknowledgeaboutthe
environment, such as its dimensions or the types of observations it might encounter (44, 13). This prior
knowledge allows the agent to form expectations over observations or states, even though it might not
knowwhichobservationscorrespondtowhichlocations.Forinstance,theagentmightexpecttoencounter
walls,doors,orspecificfloorcolours,butitdoesnotknowwhereexactlythesewillbe.
Identifyingamodelarchitecturewiththerightlevelofcomplexitytocaptureaccuratelytheenvironment
is problematic. Therefore, we take the approach of letting the model expand dynamically and explore
as needed. Model expansion, however, requires a trigger indicating the need for an increase in model
complexity.
Somehaveexpandedtheirmodeluponreceivingnewpatternsofobservations(17,42).Whenconsidering
the case of exploring room structured mazes it implies the creation of new states only when the agent
physicallytransitionstoanewlocation/room.
9
de Tinguy et al. Learning Dynamic Cognitive Map with Autonomous Navigation
In an environment where multiple actions could have potentially created new states, only generating
states uponactual observationmeans theagent will forgetthat previouslyvisited roomscould haveled to
otherunexploredspaces.Consequently,thegenerativemodelcannotleveragethisknowledgetopredict
accurately thepresence or absenceof adjacent roomsfrom past visitedrooms since thesepotential states
havenotbeenincludedinthemodel.
Therefore,whentheagentevaluatespoliciesusingEFE,itfailstoaccuratelypredictactionconsequences
if it has not directly experienced those transitions. This short-sighted exploration strategy neglects the
potentialconnectionsbetweenvisitedandun-visitedlocations,leadingtosub-optimalexploration.
To illustrate this, consider our agent expanding its internal map only after directly observing the new
rooms. Initially, the agent observes its current room and knows that doors may lead to other rooms, but
it has no prior knowledge of what lies behind these doors. This uncertainty results in a high predicted
informationgainintheEFE.Whentheagentfinallycrossesadoorandencountersanewobservation,it
generatesanewstatecorrespondingtothisobservation.However,thedoorsitdidnotexploreareforgotten,
as only really observed outcomes are considered to update the model. Thus, the agent will not consider
theseun-visited areasas offeringsignificantlygreater potentialforinformation gainin itsfutureplanning.
Thisleadstoaslowerexplorationprocess,wheretheagenttakeslongertovisitallroomsbecauseitfailsto
anticipatetheexistenceofun-visiteddistantlocationsandignoreshowtoreachthem.Themodeloverlooks
explorationopportunitiesthatcouldhelpitselectthemosteffectiveexplorationstrategyoverall.
To address this, our model learns to expand its internal map based on all available opportunities and
grows based on predicted observations or states rather than solely on actual observations. This way, the
agent retains awareness of all potential actions, including those it has not yet executed, allowing it to
systematicallyexploretheenvironmentbyconsideringbothvisitedandun-visitedlocations.Thisstrategy
enhancestheconnectivitybetweendifferentareasoftheenvironment(enablingtheagenttopredictthat
different doors are likely to lead to the same room) and enables the agent to explore more efficiently,
reducingthetimerequiredtomapitssurroundingsfully.
Figure 4. Example of an internal map layout based on initial dimensions, expansion strategy, and a given
pathinamaze-o)pathinorange-.withobservedstates/roomsobservationsbeingtheroomfloor.a)Astatic
map expects 9 rooms/states but lacks connectivity and observation details. A dynamic map can expand
indefinitely with b) new observations or c) predictions. a) Requires knowing the environment’s size in
advance,whileb)doesnotforeseenewroomsinun-visitedpastareas.c)Considersthepossibilityofevery
doorleadingtoun-visitedrooms.
Figure4presentsthethreeapproaches:
a)whenweknowtheenvironmentdimension(staticcognitivemapdimension),
10
de Tinguy et al. Learning Dynamic Cognitive Map with Autonomous Navigation
b)whentheinternalmapgrowsuponreceivingnewobservations,
c)whentheinternalmapgrowsuponpredictingnewstatesyettobeexplored.
thefirstplano)presentsanagent’smotioninaroommaze,thearrowinorangeillustratesthepaththe
agentfollowsfromthegreentothepurpleroom.a)requiresapriorovertheenvironment,thusnotbeing
adaptedtofullyunknownmazes,b)isnotconsideringtheblueroominitsmodeleventhoughitknewit
existedwhenintheredroom(S ),butsinceitchosetomovetowardthepurpleroom,thisknowledgeis
1
lost.c)Considersallthemotionsitcouldhavetakentochangerooms.Thus,itremembersthattheblue
roomexistsandthatithasneverbeenthere,improvingdrasticallyexplorationasitcanplantomovethere
fromanywhereinthemaze.
3.4 Cognitive Map Extension
In this section, we present how the model learns the structure of the environment by expanding the
cognitivemap based on predictions, before obtaining actual observations, and updating beliefs upongiven
newevidence.
Each time the agent moves, the model infers its current state and pose by integrating its motion and
observations, subsequently updatingthe model parameters. In line with theBayesian reduction model (41),
iftheobservationlikelihood A failstoencompassthecurrentobservation, thisindicatesthenoveltyofthe
o
observation.Asaresult,thedimensionalityoftheobservationlikelihoodisexpandedtoincorporatethe
new observation, and the model parameters θ are updated accordingly to reflect this extended observation
likelihood.
Theoptimisationofbeliefsregardingthegenerativemodelparametersθ involvesminimisingtheFree
Energy F , whileaccounting forpriorbeliefs anduncertaintiesrelatedto bothparametersand policies, as
θ
outlinedin(38):
θ =(A ,A ,B )
o p s
(7)
F =E [F(π,θ)]+D [Q(θ)||P(θ)]+D [Q(π)||P(π)]
θ Q(π,θ) KL KL
This optimisation in Equation 7 balances the expected accuracy of the model’s predictions and the
necessityofmaintainingcoherencewithpriorbeliefs.Thisensuresthattheagent’slearnedrepresentations
and actions are both accurate and consistent with existing knowledge. The parameter set θ includes the
following Markov matrices:the statetransition B = P(s |s ,a ), theobservationlikelihood A =
s t t−1 t−1 o
P(o |s )andthepositionlikelihoodA = P(p |s ).ThepositiontransitionmatrixB = P(p |p ,a )
t t p t t p t t−1 t−1
is not updated through Active Inference; instead, it is a deterministic metric in which the size grows as
the agent explores, guided by the policy π. This indicates that the transition between two positions is
determinedbythegivenactionandthepreviousposition.
Incontrast,Equation7indicatesthatthestatetransitionmodelB isupdatedbasedonthetransitionsthe
s
agentexperiencesduringexploration.Furtherdetailsregardingthelearningratesandmatrixinitialisation
canbefoundinAppendix2.1.
Once the current model is fully updated, policy outcomes are predicted using Expected Free Energy
(EFE) in the four cardinal directions from the agent’s current state. Walls and doors are identified from
RGB observations and incorporated to determine the feasible directions for room transitions, influencing
the transition probabilities. Equation 8 presents the EFE of a policy π with collision observation c.
The probability P(c) is binary, meaning there either is or isn’t a collision, with probabilities of 1 or 0,
11
de Tinguy et al. Learning Dynamic Cognitive Map with Autonomous Navigation
respectively.Thelearningtermoftheequationsshowshowmuchwelearnaboutthepositionlikelihood
considering an obstacle or not while the inference term evaluates the state s and position p given
t+1 t+1
the collision observation c . It reflects how well the model infers the state and position based on that
t+1
observation.
G(π) = E [logQ(s ,p ,A |π)−logQ(s ,p ,A |c ,π)−logP(c )]
Qπ t+1 t+1 p t+1 t+1 p t+1 t+1
= −E [logQ(A |s ,p ,c ,π)−logQ(A |s ,p ,π)]
Qπ p t+1 t+1 t+1 p t+1 t+1
(cid:124) (cid:123)(cid:122) (cid:125)
expectedinformationgain(learning)
−E [logQ(s ,p |c ,π)−logQ(s ,p |π)] (8)
Qπ t+1 t+1 t+1 t+1 t+1
(cid:124) (cid:123)(cid:122) (cid:125)
expectedinformationgain(inference)
− E [logP(c )]
Qπ t+1
(cid:124) (cid:123)(cid:122) (cid:125)
expectedriskofcollision
The transition model B tracks possible transitions between positions. It determines the next position
p
byincrementingthepreviousonebasedontheagent’smotion.Whencalculatingtransitionprobabilities
between positions,detected obstaclesare takeninto account. Ifan obstacleis detectedin a given direction,
the transition probability to that position is set to zero, and the predicted new position is disregarded.
Conversely, ifnoobstaclesaredetectedand the agentpredictsanewposition p thatdoesnotmatchany
t+1
previouslyknownpositions(i.e.,thepositionhasnotbeenimaginedbefore),thesizeofB isexpanded
p
toincludethisnewlydiscoveredposition.Thisapproachenablesthemodeltoadaptandremembernew
positionsastheyareencountered.Consideringanyposep,equation9representstheExpectedFreeEnergy
(EFE) of the prior over the position likelihood parameters, summed across all policies. The sigmoid
function, σ, transforms the negative EFE into a probability value (42). This equation evaluates how
effectivelythecurrentpositionlikelihoodA explainstherelationshipbetweenthestates,theposep,and
p
thecollisionobservationc.
P(A ) = σ(−G)
p
G(A ) = E [logP(p,s|A )−logP(p,s|c,A )−logP(c)]
p Q p p
Ap
= −E [logP(p,s|c,A )−logP(s|p,A )−logP(p|A )]
Q p p p
Ap (9)
(cid:124) (cid:123)(cid:122) (cid:125)
expectedinformationgain
−E [logP(c)]
Q
Ap
(cid:124) (cid:123)(cid:122) (cid:125)
expectedvalue
Ifthepredictednextpositionp doesnotcorrespondtoanyexistingstateinthemodel(considering
t+1
thestateprobabilityconditionedontheposition)themodelrecognisesthispositionascorrespondingtoa
new,unexploredstate.Thusthemodelintroducesanewstateinitsrepresentationtoaccountforthisnew
place, expanding all probability matrices to accommodate the additional state dimension. This ensures
that the state dimensions remain consistent across all matrices. The newly introduced state is assigned
a high probability in the position likelihood matrix A , such that P(s |p ) = 1, ensuring the model
p t+1 t+1
accuratelyreflectsthisnewpositioninitsstaterepresentation.
12
de Tinguy et al. Learning Dynamic Cognitive Map with Autonomous Navigation
Theobservationlikelihoodmatrix A assignsuniformprobabilitiesacross itsdistributionforun-visited
o
states,reflectingtheagent’slimitedknowledgeabouttheunexploredstates.Essentially,theagentcannot
predictwhichobservationtoexpectatanewlypredictedlocationduetothelackofpriorexperiencethere.
The model first evaluates the likelihood of reaching the predicted position given a specific motion to
calculatethetransitionprobabilitiesbetweenacurrentandpredictedstatebasedonposition.Ifanobstacle
isdetectedinthedirectionofthisexpectedposition,thetransitionprobabilityisnull,meaningtheaction
resultsintheagentstayinginthesamestate.Conversely,ifnoobstacleisdetected,thetransitionlikelihood
betweenthecurrentandpredictedstatesismaximised.Thisstatetransitionisdeterminedbytheposition
transitionandthecorrespondingprobabilityP(s |p ),ensuringthattheposition-basedtransitioniscorrectly
t t
mappedtothestatetransitionwithinthemodel.Inbothscenarios,theDirichletdistributionofthetransition
matrixB isupdatedsimilarlytohowitiswithexperiencedtransitions,butwithalowerlearningrate.This
s
lowerlearningrateaccountsforthegreateruncertaintyassociatedwithpredictedtransitionscomparedto
thosealreadyexperienced.DetailsaboutthelearningratescanbefoundinAppendix2.1.
TheseunexploredstatesarehighlyattractiveintheEFEframeworkbecausevisitingthemofferssignificant
informationgain.
Figure5. Fromaphysicalmotiontothesubsequentcognitivegraphupdate.Thefirstredroomisinitialised
withcurrentobservationandpredictedmotionsinthefourdirections. GoingDownandRightholdsnew
unknownstates.Bygoingrighttowardtheblueroom,thestateisupdatedwithanewblueobservationand
predictedmotionsinalldirections,increasingtheconfidenceinthered-blueroomtransitionanddefiningit
asbi-directional.
The schematic depicted in Figure 5 illustrates a straightforward process of updating the state from
the physical world to the cognitive graph. Unknown states are weakly defined as existing but lacking
any observations, resulting in weaker transition probabilities compared to discovered states through
experimentaltransitionsholdingobservations.
4 RESULTS
Our experiments are designed to assess the efficacy of our model in constructing cognitive graphs by
linkingvisitedlocationswithanticipatedones,therebyenablingefficientexplorationandgoalachievement.
WeconductedcomparativeanalyseswithCSCGtoassessseveralfunctionalities.Theseincludelearning
spatialmapsunderaliasedobservationsanddecision-makingstrategiesaimedatinternalobjectives,suchas
explorationorreaching specific observations. Thosetestsweredone withandwithoutprior environmental
information (i.e. is the model familiar with the environment layout). Finally, we validated our agent’s
self-localisationcapabilityfollowingakidnappingandre-localisationatarandomplaceandhowitre-plans
afterobservingobstaclesonitspath.
13
de Tinguy et al. Learning Dynamic Cognitive Map with Autonomous Navigation
The Clone-Structured Cognitive Graph (CSCG) is a graph-based cognitive architecture where nodes
representbeliefsandedgesaretransitionsbetweenthesestates.Clonesofthesenodesallowtheagentto
maintainmultiplehypothesesaboutitsenvironment,enablingparallelexplorationandevaluationofdifferent
action sequences and their outcomes. This structure supports dynamic and adaptive decision-making in
aliasedenvironments(21).MoreabouttheCSCGcanbefoundinAppendix2.3.
The environments comprise interconnected rooms, ranging from configurations with 7 to 23 rooms.
ThesearrangementsvaryfromT-shapedlayoutsto3by3,4by4roomgrids,donuts-shapedlayoutsand
a reconstruction resembling a Tolman’s maze, all with or without aliased room colours. Environments
containing several rooms with the same floor colour are considered aliased and named such in our
experiments.
We consider both models (CSCG and ours) as part of a hierarchical framework (17). Information is
processedatthelowerlevels,generatingforeachroom:
• asinglecolourperroomasobservation
• spatialboundaryinformation(i.e.expectedriskofcollision)indicatingobstaclesorpossiblewaysout
(doors)(20)
The agents can move in the four cardinal directions or remain stationary in the current room. Moving
towardsadoorleadstoenteringanewroomwhilemovingtowardsawalldoesnotaltertheobservation.
Detailedobservationlayoutsofthe8environmentsusedcanbefoundinAppendix1,withanexampleof
the3x3roomslayoutandobservationsillustratedinFigure1.
4.1 Dynamic Extension
Bymovingthroughspace,gatheringobservations,andupdatingtheirinternalbeliefs,bothourmodeland
CSCGexhibittheabilitytolearntheunderlyingspatialmapoftheenvironment,akintothenavigational
capabilitiesobservedinanimals(9).However,whileaCSCGhasastaticcognitivemapandlearnsthrough
random exploration, our model starts with a smaller state dimension and makes informed decisions at
each step based on its internal beliefs and preferences. This mechanism enhances the relevance of each
movementandacceleratesthelearningprocess,mirroringtheefficientnavigationstrategiesseeninanimals
(14).
Table1. Theaveragenumberofstepsrequiredtofullylearntheenvironmentsforeachmodel.
CSCG
models\ CSCG CSCG
Oracle ours CSCG poseob
environments poseob randompolicy
randompolicy
3x3 11 14.4±2 231.9±59 221.3±35 214.6±44 205.7±22
3x3 alias 11 18.1±2 376.3±35 201.5±20 338.8±30 210.4±22
4x4 15 34.0±4 375.7±36 405.5±78 419.6±98 380.5±53
4x4 alias 15 48.9±9 509.5±107 387.5±67 554.1±70 391.3±66
T maze 9 12.5±1 284.5±69 297.7±108 261.7±96 232.5±73
T maze alias 9.5 14.3±4 592.5±28 282.7±124 602.0±69 341.0±75
donuts 13 15.6±2 519.6±127 450.0±87 416.3±105 493.1±132
Bothmodelsweremadetolearntheconnectivitybetweenrooms.Theagentsspawnedatrandomstart
locationsandweretaskedwithseekinginformationgainfromtheenvironment-inotherwords,theywere
expectedto learnthe layoutof theenvironment. Ourmodel is ledby informationgain onlyas nopreferred
14
de Tinguy et al. Learning Dynamic Cognitive Map with Autonomous Navigation
Figure 6. A path -from black to white- our agent takes to fully learn the environment, most rooms are
accessedtwicetolearnthetransitionsbetweenconnectedroomsfully.
observationsaregiventothemodel,theutilitytermissilentduringpureexploration.Theresultsaveraged
over20testspermodelandenvironment,aresummarisedinTable1andFigure7.Ourmodeldemonstrates
efficient exploration in all those environments. Exploration is considered complete when the internal belief
regarding transitions between places aligns with the ground-truth transition matrix of the environment,
with a minimum confidence of 60% over all correct transitions. This level of confidence was deemed
enoughtoguaranteeagoodunderstandingofthemazestructureforallmodels.Toensureafaircomparison,
we provided CSCG with either colour observations alone (referred to as ’CSCG’ in our results) as in
our proposed model, or colour-position pairs observations (termed ’CSCG pose ob’ in our results). We
consideredprovidingground-truthpositionandcolourasanobservationduringexplorationwithoutprior
Figure 7. The Figure displays the average number of steps required to discover all the rooms on a
logarithmic scale, with the oracle serving as the benchmark for the minimum steps needed to visit all
roomsonce.Aliasedrooms,markedbyidenticalobservationsacrossdifferentlocations,presentachallenge
by potentially misleading the agent about its current position. Nonetheless, our agent has a meaningful
pathway,discoveringallroomsatleast30%fasterthanCSCG.
15
de Tinguy et al. Learning Dynamic Cognitive Map with Autonomous Navigation
as equivalent to ourmodel’s proprioceptive ability. Additionally, the CSCG can navigaterandomly as in
theoriginalpaper(21)orbyconsideringalocalisationbeliefsimilartoourmodel,coupledwiththeCSCG
Viterbi navigation module as was realised in (45). Additional information about the CSCG architecture
andtrainingprocesscanbefoundinappendix2.3.Twomajordifferenceshighlightthecapabilitiesofour
model compared to the CSCG model: our model can expand its internal map dynamically, adapting to
the environmentwithout requiring apre-defined dimensionsize, and itcan reason overunexplored areas,
whereastheCSCGmodelneedstoencounteranewplacetointegratethatstateintoitsinternalmap.
Table1showstheimpactofthisdifferenceindesign.Wecancomparethenumberofstepsrequiredby
eachagenttoexplorediverseenvironments,withtheoracle,anA-staralgorithm,definingtheminimum
number of steps needed to visit all rooms once. In the T-shaped maze, the oracle’s number of steps is
averagedconsideringtheirstartingposes.Ourmodeldemonstratessignificantlyfasterlearningofallmap
structuresthantheCSCGalgorithm.AllCSCGmodelsrequireasubstantialnumberofstepstoexplorethe
environment, regardless of whether the observations are ambiguous or not. Neither random exploration nor
Viterbi-basednavigationsignificantlyimprovesitsperformance.Thedecision-makingprocessdoesnot
prioritisepoliciesguidingtheagenttowardunexploredareas,limitingitsoveralleffectiveness.Figure6
illustrates a typical path undertaken by our agent within a 4x4 observation environment. Despite a few
instances of overlapping paths, our agent demonstrates a discerning approach, making informed decisions
ratherthanresortingtorandomexploration.Thisstrategicdecision-makingresultsinanaveragemaximum
numberof stepsofourmodel beingabouttwentytimes smallerthanthose oftheCSCG.To delvedeeper
into the exploration process, we examine the first discovery of all rooms, as depicted in Figure 7 on
a logarithmic scale. Remarkably, our model’s performance closely resembles the oracle’s, suggesting
a tendency to prioritise novelty exploration over confirming connections between rooms. Emphasising
imagined beliefsleads tofaster convergenceof locationsconnectivity, however, italso carriesthe risk of
forming false connections not substantiated by observation. To mitigate this risk, we opted to maintain
stateconnectionsmadeoutofimaginedbeliefsweakerthanthoseformedthroughobservation.
Figure7a. Figure7b.
Figure7. AveragenumberofstepsneededbyeachmodeltoreachthegoalcomparedtoOracle’sideal
steps.(a)Averagestepsoverallaliasedenvironments.(b)Averagestepsoverallnon-aliasedenvironments.
Knowing the environment allows the models to reach the goal more efficiently and closer to the ideal
numberofsteps.
16
de Tinguy et al. Learning Dynamic Cognitive Map with Autonomous Navigation
4.2 Autonomous Localisation
While exploring, our agent prioritised information gain, in the following experiments we give a preferred
observation(afloorcolour)tostimulateexploitativebehaviourinstead.Theagentcannavigateguidedbya
preferenceregardlessofwhethertheenvironmentisfamiliarornotwiththedesiretoreachthatobjective.
Agoalisconsideredreachediftheagentdecidestostayatthedesiredlocation,thusdiscardingrandom
stumblinguponitasasuccessfulattempt.Figure7showstheaveragenumberofstepseachmodelrequires
toreachthegoal,consideringbothaliasedandnon-aliasedenvironments.Theresultsareaveragedover
environmenttype(aliasedmazeornot),eachmodelhad20runspergoaldistanceandenvironment.Without
prior knowledge about the environment, our model systematically explores the map until it encounters
thegoal, resultinginahighernumberofstepsthan theoracle,whichhasprecise knowledgeofthegoal
location.However,whenwerelocateouragentafterexploration,keepingthemapinmemory(i.e.ourswt
prior),theresultsaligncloselywiththeoracleinnon-aliasedenvironments(Figure7b)andapproachoracle
performanceinaliasedenvironments(Figure7a),wheretheagentmayneedafewstepstolocaliseitself
whilesearchingforthegoal.AdemonstrationofourlocalisationprocessispresentedinFigure8.CSCG,
ontheotherhand,exclusivelyoperateswithpriorknowledgeoftheenvironmentandemploysaViterbi
algorithm for navigation (46). Despite having prior information similar to our agent, CSCG’s performance
isnotconsistentlysuperiorwhenprovidedonlywithcolourobservations.Asouragent,CSCGalsorequires
self-localisationbasedonmultipleobservationsinsuchscenarios.Overallouragentdemonstratesexcellent
efficiencyinreachingandrecognisingthegoal.Evenwithoutprioroverthemap,theagentdoesnotreturn
toalreadyexploredareasunlessnecessary,demonstratingabiologicallyplausiblepathplanning.
TheimpactofaliasedobservationsoverourmodelisdemonstratedinFigures8.Theleftpanelofeach
figure presents the agent (represented by an X) provided with a prior distribution over the map (agent’s
state value reported at the upper left of each tile) and the policies’ expected free energy with a darker
greycoloursignifyinghigherpreferencetoproceedtowardthatpath.Therightpaneldepictstheagent’s
confidence in its location (i.e. state) given collected observations. The agent starts at the bottom of the
T-maze (Figure 8a), observing only the present colour, which is observable in three different locations
(hereinstates0,1and7). Theagentcan’ttellfromjustthissingleobservationwhereitis yet.Thisresults
in divided confidence between those three states when inferring its potential localisation. In the right
panel,wecanseethattheagentconsidersgoingforwardasthebestoption.Notably,theimaginedTturns
are incorrect due to the agent’s confidence in its localisation being mainly split between states 0 and 1,
while the agent is, in reality, at state 7. Moving to Figure 8b, representing step 1, the agent adjusts its
internalbeliefsregardinglocalisationbasedonthecolourofthepreviousroomandthecurrentobservation.
Thiscorrectioninbeliefsleadstoarefinementintheagent’sperceivedlocalisation,ascanbeseeninthe
associatedbarplot.InFigure8c,correspondingtostep2,theagentexhibitsasignificantlyhigherlevelof
certaintyregardingitswhereaboutsinlocation1.Itconfidentlydeterminesthatthegoalislocatedeitherto
theleftorrightbutrulesoutthepossibilityofitbeingbehind.Finally,Figure8dportraysthefourthstep,
wheretheagent demonstratesfullconfidencein itslocalisationandsuccessfullyidentifies theobjective
withdark greyshading onthe goals.This highlevel ofconfidenceindicates theagent’sstrong beliefin its
internalrepresentations.Therefore,thenextstepswillcorrectlyleadtheagenteitherrightorlefttoward
thepreferredobservation.
17
de Tinguy et al. Learning Dynamic Cognitive Map with Autonomous Navigation
Figure8a. Figure8b.
Figure8c. Figure8d.
Figure 8. Our agent imagined trajectories (left panel) alongside their corresponding localisation beliefs
(depicted as red bars) at each step. Each number within the aliased T-maze signifies the internal state
attributedbytheagentduringexploration.Theagent’spositionismarkedbyanX,whilethegoalisared
observationattheendoftheaisles.TheterminationpointsoftheTaislesandtheshadingofthegreypath
reflecttheagent’sconfidenceinitsbeliefs.
4.3 Dynamic Remapping
Canthe modelcorrectlyupdate itsinternalcognitivemapgivennewevidencecontradicting itsprior? To
verifythatweconducttwoexperiments.OneintheDonutenvironment,wherethereareonlytwopaths
toreachagoal(alongandashorterpath),bygoingleftorrightandasecondonereproducingTolman’s
secondmazeexperimentwithobstacleslocatedatseveralpositions.
In our Donut experiment, the agent has previously learned the environment from a randomly selected
startingpoint withoutanyobstacle. Then, theagent waskidnappedto acornerof themapwith thenewly
implementedobjectiveofobservingalightlypinkcolour(highlightedbyaredsquareinFigure9).Our
model is given a floor colour as objective and exploitation and exploration have equivalent weights in
thenavigation.Pathplanningstepsshouldbefollowedfollowingthesequencefrom1)to10)depictedin
Figure9.Atfirst,theagentsuccessfullyplansitspathtoaccommodatethenewobjectivebymovingthrough
theupperpath,asdepictedinthefirstframe1)ofFigure9.Thedarkgreyshadingindicatesthehighlevelof
confidenceinthispath,thedarkertheshade,thehigherthecertitudethisimaginedpolicyleadstoadesired
outcome.Wedisruptedtheshortestpathbyaddinganobstacle,ashighlightedbyawhitesquareinFigure9.
Thisobstructioninvalidatedtheagent’soriginalroute,leadingtoanotabledecreaseintheprobabilityof
reaching the goal along the intended path (as evident in frame 2) of Figure 9). Consequently, the agent
initiatesaremappingprocesspromptly,updatingitsbeliefsregardinggraphconnectivity.Thesubsequent
frames,fromframes3)to10)inFigure9,depicttheagent’sincreasingconfidenceasitnavigatescloserto
theobjectiveusingthelongestpath.
18
de Tinguy et al. Learning Dynamic Cognitive Map with Autonomous Navigation
Inthisscenariowherearoomalongtheimaginedpathwasunexpectedlyclosedoff,theagentdemonstrates
its capability to dynamically adapt its navigation strategy in response to changes in the environment,
effectivelyleveragingnewobservationstoreviseitspathandachieveitsobjective.Failuretoimaginean
alternative path would have resulted in the agent stubbornly trying to pass a closed door. This flexible
connectivityexistsdespiteusingagenerativemodel,whichisknowntohaveahardtimerevisingstrong
beliefs.Thisisduetotwothings,firstly,whenexperimentingwithablockage,ouragentupdatesitsinternal
model with negative parameter learning (see appendix 2.1). Secondly, the inherent growth of our agent,
as it grows in its state or observation dimension, the probabilities get more distributed, so transitions to
paststatesthathavenotbeenexperimentedforalongtimenaturallyweakenovertime.Reproducingthe
mechanismofanimal-likesynapticplasticity(47).
Figure9. Ouragent imaginedpath toward theobjective (squared red)and re-planningwhen realisingthe
desiredpathisblocked(closedroomsquaredwhite).
Our second test to assert the robustness of our model to dynamic environments entails replicating the
Tolman maze experiment outlined in (48). This maze configuration, illustrated in Figure 10a, involves
placing starving rats at a starting point (marked by a red triangle) with food positioned at the opposite
end of the map. Three routes are available to reach the food, with Route 1 being the shortest and Route
3 the longest. However, these routes can be blocked at points A and B, where the blockages affect the
accessibilityofRoutes1and2.Inourcase,theagentcan’tobserveobstaclesfromadistance,ithastobe
in contact with them from an adjacent room to learn about it. Each coloured grid in Figure 10b represents
anindependent closedroom andthe whitebox representsanobstacle disposedat positionsA andB. Thus,
we rely on the ”insight” (49) of our agent and its ability to update its beliefs rather than its perceptual
capabilitiestoreachthegoal.
This ”insight” of our agent is based on two key parameters: its planning ability, determining how far
ahead it can imagine, and cognitive map plasticity, dictating how well it can adjust past beliefs to new
evidence.Weallowedtheagenttopredictactionconsequencesupto14stepsahead,thusfromanyobstacle,
theagentcouldimaginereachingthegoalanyway.
Themodelisprovidedwitharedvisualobservationasitspreferredpriorthroughouttheentireexperiment
with a weight on EFE utility term of 2 (which results in the agent favouring reaching the objective over
exploring).The10 agentsconsistentlystartatthebottomofthemaze asdepictedinfigure10aredtriangle.
The agents followed threeseries of 12 runs in the Tolman maze with no obstacle (figure 10b.o), then with
anobstacleatpositionA(figure10b.A)andfinallyatpositionB(figure10b.B).
19
de Tinguy et al. Learning Dynamic Cognitive Map with Autonomous Navigation
Figure10a. Figure10b.
Figure 10. (a) Floorplan with obstacle locationinspired by Tolman and Honzik’s secondmaze (48), (b)
showstheactualobservationmapwithoutandwiththeblockageonpositionA)andB)
Initially,theagenthasnopriorknowledgeoftheenvironmentandobservations.Every20steps,theagent
is”kidnapped”andrepositionedatthestartingpointwithoutbeinginformedofthisrelocation.Theagent
mustinferandcorrectitsbeliefaboutitslocationbasedonsubsequentobservations.Figure11showsthe
pathchoicefrequencyoverallagentsgivenanobstaclecomparedto MartinetandArleo(50)resultson
100animatsreproducingTolman’sinsights.
Duringthefirst12runs,themazecontainsnoobstacles,andthefastestroutetothegoalispath1.Our
agenttypically exploresallavailablepaths atleastonce, followingAIF frameworkas understandingthe
environmenthelpsminimisefreeenergy.Althoughtheagentalternatesbetweenthethreepathstobalance
betweenexplorationandexploitativebehaviour, path1isgenerallypreferred(almost50%ofthetime)as
wecanseeinthepathcounttable2,recordingalltheattemptstoreachthegoalafterexperimentingwith
theblockageinpath1,ifany.Itcloselyalignswithwhatwewouldexpectfromahungryratasshownin
figure11firstcolumn,itresemblesTolman’sexpectationsofratbehaviour.Ouragenttendstoalternate
between taking the quickest path and taking another alternative path, with a clear regularity. Extending
the number of runs would show path 1 being taken 50% of the time and paths 2 and 3 25% of the time
each. The reason why it isn’t exactly 50% of the time is because the agent takes a few runs to explore
theenvironmentinsteadofalwaysreachingthegoal.Thisalternatepathselectionislinkedtothebalance
chosenbetweenexploitation(thepreferenceofbeingatthegoal)andexploration(thepreferenceoflearning
the environment). When a pathis consideredbetter understoodthan theothers, the agentcounterbalances
byre-exploringthoseotherpaths.Let’srememberthattheagentcanpredictreachingthegoalfromthose
pathsaswell,evenifittakesmoresteps.
An obstacle is introduced at position A for the next series of 12 runs. The agent continues to navigate
using the same memory, which has accumulated experience from the previous runs in an obstacle-free
environment. Path 2 becomes the quickest route. After encountering the obstacle a few times, the agent
updates its internal map and shifts its preference to path 2, though it still periodically checks paths 1
and 3, and reaffirms that path 1 remains blocked. Trials consider all the tentative to reach the goal after
experimentingwiththeblockageinpath1.ContrarytoMartinetandArleo(50)results,ouragentsshow
a more divided comportment with path 2 being preferred but not completely neglecting path 3. Finally,
theobstacleismovedtopositionB,andtheexperimentcontinuesfor12moreruns.Initially,theagents
often attemptto traversepath 2, butafter furtherupdating its modelto accountfor thenew blockage, they
predominantlyswitchtopath3,whileperiodicallycheckingpath2again.
20
de Tinguy et al. Learning Dynamic Cognitive Map with Autonomous Navigation
Figure11. OurresultscomparedtoL.-E.Martinet&al’s(51).Inourstudy,theagent’sflowpathstowards
the objective (top of the map) are shown, with re-planning occurring when the desired path is blocked.
Thevaryingcolourgradientofthelinesindicatesthefrequencyofselectionforeachpathoverallagents.
Panels A and B illustrate obstacles at points A and B, respectively. The second row is adapted from
L.-E.Martinet&al’swork(51).Theoccupancygridmapsdemonstratethelearningofmazetopologyby
simulatedanimals,initiallywithoutobstacles,showingasignificantpreferenceforRoute1.Whenablock
is introduced at point A, the animals predominantly choose Route 2. With an obstacle placed at point B,
theanimalsmainlyoptforRoute3.
These results demonstrate our agent’s robustness in handling kidnapping scenarios and its ability to
update its internal cognitive map when new evidence contradicts previously well-verified beliefs. The
modelexhibitsaclearandadaptablebehaviour,akintoratsnavigatingamaze.Byadjustingtheagent’s
predictionhorizon,theweightingoftheutilitytermintheExpectedFreeEnergy,andthestochasticityin
path selection (testing values defined in appendix 2.1), we can control the navigation behaviour of our
agent, favouringexploration or exploitation. Thesevariations inbehaviour are easilyinterpretable through
theAIFframeworkourmodeluses,avoidingtheopacityoftenassociatedwithblack-boxmodels.
Table2. Pathcountoveralltenagentswhentheyreachthegoalconsideringtheobstacleposition.Trials,
wheretheobjectiveisnotreachedunder20steps(explorationphases),arenotcounted.
Path1 Path2 Path3
Noobstacle 48 27 28
ObstacleA 0 70 49
ObstacleB 0 36 48
5 DISCUSSION
This study proposes a novel high-level abstraction model grounded in biologically inspired principles,
aiming to replicate key aspects of animal navigation strategies (3, 52). Integrating a dynamic cognitive
21
de Tinguy et al. Learning Dynamic Cognitive Map with Autonomous Navigation
graph alongside internal positioning within an Active Inference Framework is central to our approach.
This novel combination enables our model to dynamically expand its cognitive map upon prediction
while navigating any ambiguous mini-grid environment with or without prior, mirroring the adaptive
learning and efficient exploration abilities observed in animals (1, 2). Comparative experiments with
the Clone-Structured Graph (CSCG) model (21) underscore the superiority of our approach in learning
environmentstructureswithminimaldataandwithoutpreparatoryknowledgeofspecificobservationand
state-space dimensions.Moreover, we demonstrated themodel’s abilityto adapt itscognitive mapto new
evidenceandreachpreferredobservationsinvarioussituations.Thepresentedmodelcanrepresentlarge
or complex environments like warehouses or houses, without significant memory or storage demands
for the cognitive map itself due to its matrix-based structure. However, as the agent attempts to make
long-termpredictions—projectingfurtherstepsintothefuture—computationalrequirementsforprocessing
powerandmemoryincrease.Inthiswork,weimplementupto13-steppredictionswithoutissueduetothe
efficiencyofourpolicydesign;achievingpredictionsbeyond10stepsisnotableforthistypeofgenerative
model. Looking ahead, it could be interesting to investigate the impact of a perfect memory on future
policies and exploration efficiency in addition to measuring the impact of belief certitude on the graph
dynamicadaptation.Furthermore,splittingthedatabyunsupervisedclustering(53),orbyusingthemodel’s
predictionerrortochunktheobservationsintoseparatelocations(54)wouldofferacloserapproximationto
animalbehaviourandallowopenspaceexploration.Thiscouldextendthismodelcapacityintoreal-world
scenarios, suchas StreetLearn(55)based onGoogle mapobservationsor simulatedrealisticenvironments
suchasHabitat(56).Therealworldpresentsadditionalchallenges,suchassegmentinglargeopenspaces
into manageableinformation chunksas realisedby Hwanget al.(57), efficientlyprocessing visual data or
other sensory inputs(with a method such asKerblet al. (58)), and distinguishing new observations from
known ones based on memory. Moreover, navigating around local static and dynamic obstacles effectively
is essential, Williams et al. (59) propose a possible solution to move between locations while avoiding
obstacles. Eachof these challenges can be addressed individually and integrate the mentioned solutions to
enhance our model’s performance without altering the core navigation strategy. Finally developing this
high-levelabstractionmodelwithahierarchicalmodel(17)wouldallowthemodeltoreasonoverdifferent
levels of abstraction temporally and spatially extending its navigation to local in-room navigation and
long-termplanning.
FUNDING
ThisresearchreceivedfundingfromtheFlemishGovernmentunderthe“Onder-zoeksprogrammaArtificie¨le
Intelligentie(AI)Vlaanderen”programme.
DATA AVAILABILITY STATEMENT
Thecodegeneratedforthisstudycanbefoundinthehigh-levelnavplanning:https://github.com/
my-name-is-D/high_level_nav_planning/tree/main.
REFERENCES
1.TyukinIY, GorbanAN, Alkhudaydi MH,Zhou Q. Demystification offew-shot and one-shotlearning.
CoRRabs/2104.12174(2021).
2.RosenbergM,ZhangT,PeronaP,MeisterM. Miceinalabyrinthshowrapidlearning,suddeninsight,
andefficientexploration. eLife10(2021)e66175. doi:10.7554/eLife.66175.
22
de Tinguy et al. Learning Dynamic Cognitive Map with Autonomous Navigation
3.Zhao M. Human spatial representation: What we cannot learn from thestudies of rodent navigation.
JournalofNeurophysiology120(2018). doi:10.1152/jn.00781.2017.
4.LajoieP,HuS,BeltrameG,CarloneL. ModelingperceptualaliasinginSLAMviadiscrete-continuous
graphicalmodels. CoRRabs/1810.11692(2018).
5.CamposC,ElviraR,Rodr´ıguezJJG,MontielJMM,Tardo´sJD. ORB-SLAM3:anaccurateopen-source
libraryforvisual,visual-inertialandmulti-mapSLAM. CoRRabs/2007.11898(2020).
6.Placed JA, Strader J, Carrillo H, Atanasov N, Indelman V, Carlone L, et al. A survey on active
simultaneous localization and mapping: State of the art and new frontiers. IEEE Transactions on
Robotics(2023)1–20. doi:10.1109/TRO.2023.3248510.
7.LevineS,ShahD. Learningroboticnavigationfromexperience:principles,methodsandrecentresults.
Philosophical Transactions of theRoyal Society B: Biological Sciences 378(2022). doi:10.1098/rstb.
2021.0447.
8.HaD,SchmidhuberJ. Worldmodels. CoRRabs/1803.10122(2018).
9.Peer M, Brunec IK, Newcombe NS, Epstein RA. Structuring knowledge with cognitive maps and
cognitive graphs. Trends in Cognitive Sciences 25 (2021) 37–54. doi:https://doi.org/10.1016/j.tics.
2020.10.004.
10.EpsteinR,PataiEZ,JulianJ,SpiersH. Thecognitivemapinhumans:Spatialnavigationandbeyond.
NatureNeuroscience20(2017)1504–1513. doi:10.1038/nn.4656.
11.FristonK,MoranRJ,NagaiY,TaniguchiT,GomiH,TenenbaumJ. Worldmodellearningandinference.
NeuralNetworks144(2021)573–590. doi:https://doi.org/10.1016/j.neunet.2021.09.011.
12.[Dataset] American-Psychological-Association. Cognitive map. https://dictionary.apa.
org/cognitive-map(2024). Accessed:2024-06-25.
13.KaplanR,FristonK. Planningandnavigationasactiveinference. bioRxiv(2017). doi:10.1101/230599.
14.Gershman S. Deconstructing the human algorithms for exploration. Cognition 173 (2017) 34–42.
doi:10.1016/j.cognition.2017.12.014.
15.BushD,BarryC,MansonD,BurgessN. Usinggridcellsfornavigation. Neuron87(2015)507–520.
16.[Dataset]Chevalier-BoisvertM,WillemsL,PalS. Minimalisticgridworldenvironmentforopenaigym.
https://github.com/maximecb/gym-minigrid(2018).
17.deTinguyD,VandeMaeleT,VerbelenT,DhoedtB. Spatialandtemporalhierarchyforautonomous
navigation using active inference in minigrid environment. Entropy 26 (2024) 83. doi:10.3390/
e26010083.
18.Whittington JCR, McCaffary D, Bakermans JJW, Behrens TEJ. How to build a cognitive map. Nature
Neuroscience25(2022)1257–1272. doi:10.1038/s41593-022-01153-y. Epub2022Sep26.
19.EdvardsenV,BicanskiA,BurgessN. Navigatingwithgridandplacecellsinclutteredenvironments.
Hippocampus30(2019). doi:10.1002/hipo.23147.
20.SolstadT,BoccaraCN,KropffE,MoserMB,MoserEI. Representationofgeometricbordersinthe
entorhinalcortex. Science322(2008)1865–1868. doi:10.1126/science.1166466.
21.George D, Rikhye R, Gothoskar N, Guntupalli JS, Dedieu A, La´zaro-Gredilla M. Clone-structured
graph representations enable flexible learning and vicarious evaluation of cognitive maps. Nature
Communications12(2021). doi:10.1038/s41467-021-22559-5.
22.Mohamed IS, Yin K, Liu L. Autonomous navigation of agvs in unknown cluttered environments:
Log-mppi control strategy. IEEE Robotics and Automation Letters 7 (2022) 10240–10247. doi:10.
1109/LRA.2022.3192772.
23.MohamedIS,XuJ,SukhatmeGS,LiuL. Towardsefficientmppitrajectorygenerationwithunscented
guidance:U-mppicontrolstrategy(2023).
23
de Tinguy et al. Learning Dynamic Cognitive Map with Autonomous Navigation
24.ChaplotDS, GandhiD,Gupta S,GuptaA, SalakhutdinovR. Learning toexplore usingactive neural
slam. InternationalConferenceonLearningRepresentations(ICLR)(2020).
25.[Dataset]RitterS,FaulknerR,SartranL,SantoroA,BotvinickM,RaposoD. Rapidtask-solvingin
novelenvironments(2021).
26.MirowskiP,PascanuR,ViolaF,SoyerH,BallardAJ,BaninoA,etal. Learningtonavigateincomplex
environments. CoRRabs/1611.03673(2016).
27.Gupta S, Davidson J, Levine S, Sukthankar R, Malik J. Cognitive mapping and planning for visual
navigation. CoRRabs/1702.03920(2017).
28.Ali M, Jardali H, Roy N, Liu L. Autonomous navigation, mapping and exploration with gaussian
processes. ProceedingsoftheRobotics:ScienceandSystems(RSS)(Daegu,RepublicofKorea)(2023).
29.Chaplot DS, Salakhutdinov R, Gupta A, Gupta S. Neural topological SLAM for visual navigation.
CoRRabs/2005.12256(2020).
30.FooP,WarrenW,DuchonA,TarrM. Dohumansintegrateroutesintoacognitivemap?map-versus
landmark-basednavigationofnovelshortcuts. Journalofexperimentalpsychology. Learning,memory,
andcognition31(2005)195–215. doi:10.1037/0278-7393.31.2.195.
31.[Dataset] Guntupalli JS, Raju R, Kushagra S, Wendelken C, Sawyer D, Deshpande I, et al. Graph
schemasasabstractionsfortransferlearning,inference,andplanning(2023). doi:10.48550/arXiv.2302.
07350.
32.Dedieu A, Lehrach W, Zhou G, George D, La´zaro-Gredilla M. Learning cognitive maps from
transformerrepresentationsforefficientplanninginpartiallyobservedenvironments(2024).
33.StachenfeldKL, BotvinickMM, GershmanSJ. The hippocampusas apredictivemap. bioRxiv(2016).
doi:10.1101/097170.
34.[Dataset]Raju RV, GuntupalliJS,Zhou G,La´zaro-GredillaM, GeorgeD. Spaceisa latentsequence:
Structuredsequencelearningasaunifiedtheoryofrepresentationinthehippocampus(2022).
35.ZengT,SiB. Abrain-inspiredcompactcognitivemappingsystem. CoRRabs/1910.03913(2019).
36.TomovMS,YagatiS,KumarA,YangW,GershmanSJ. Discoveryofhierarchicalrepresentationsfor
efficientplanning. bioRxiv(2018). doi:10.1101/499418.
37.Friston K. Life as we know it. Journal of the Royal Society, Interface / the Royal Society 10 (2013)
20130475. doi:10.1098/rsif.2013.0475.
38.ParrT,PezzuloG,FristonK. ActiveInference:TheFreeEnergyPrincipleinMind,Brain,andBehavior
(TheMITPress)(2022). doi:10.7551/mitpress/12441.001.0001.
39.Friston K, FitzGerald T, Rigoli F, Schwartenbeck P, Doherty JO, Pezzulo G. Active inference and
learning. Neuroscience & Biobehavioral Reviews 68 (2016) 862–879. doi:https://doi.org/10.1016/j.
neubiorev.2016.06.022.
40.SchwartenbeckP,PasseckerJ,HauserTU,FitzGeraldTH,KronbichlerM,FristonKJ. Computational
mechanisms of curiosity and goal-directed exploration. eLife 8 (2019) e41703. doi:10.7554/eLife.
41703.
41.FristonK,ParrT,ZeidmanP. Bayesianmodelreduction(2019).
42.FristonKJ,CostaLD,TschantzA,KieferA,SalvatoriT,NeacsuV,etal. Supervisedstructurelearning
(2023).
43.Smith R, Friston KJ, Whyte CJ. A step-by-step tutorial on active inference and its application to
empiricaldata. JournalofMathematicalPsychology107(2022)102632. doi:https://doi.org/10.1016/j.
jmp.2021.102632.
44.Neacsu V, Mirza MB, Adams RA, Friston KJ. Structure learning enhances concept formation in
syntheticactiveinferenceagents. PLOSONE 17(2022)1–34. doi:10.1371/journal.pone.0277199.
24
de Tinguy et al. Learning Dynamic Cognitive Map with Autonomous Navigation
45.deMaeleTV,DhoedtB,VerbelenT,PezzuloG. Integratingcognitivemaplearningandactiveinference
forplanninginambiguousenvironments(2023).
46.KanungoT. UMDHMM:Hiddenmarkovmodeltoolkit. KornaiA,editor,ExtendedFiniteStateModels
ofLanguage(Cambridge,UK:CambridgeUniversityPress)(1999).
47.Eichenbaum H. The hippocampus as a cognitive map ... of social space. Neuron 87 (2015) 9–11.
doi:https://doi.org/10.1016/j.neuron.2015.06.013.
48.TolmanECHC. ”insight”inrats. Univ.Calif.Publ.Psychol.4(1930)215–232.
49.Aru J, Dru¨ke M, Pikama¨e J, Larkum ME. Mental navigation and the neural mechanisms of insight.
TrendsinNeurosciences46(2023)100–109. doi:https://doi.org/10.1016/j.tins.2022.11.002.
50.Martinet LE, Arleo A. A cortical column model for multiscale spatial planning. Proceedings of the
11th International Conference on Simulation of Adaptive Behavior: From Animals to Animats (Berlin,
Heidelberg:Springer-Verlag)(2010),SAB’10,347–358.
51.Martinet LE, Passot JB, Fouque B, Meyer JA, Arleo A. Map-based spatial navigation: A cortical
column model for action planning. Spatial Cognition VI. Learning, Reasoning, and Talking about
Space.SpatialCognition2008(Berlin,Heidelberg:Springer-Verlag)(2008),vol.5248,39–55. doi:10.
1007/978-3-540-87601-4 6.
52.BalaguerJ,SpiersH,HassabisD,SummerfieldC. Neuralmechanismsofhierarchicalplanningina
virtualsubwaynetwork. Neuron90(2016)893–903. doi:10.1016/j.neuron.2016.03.037.
53.Asano Y, Rupprecht C, Vedaldi A. Self-labelling via simultaneous clustering and representation
learning. InternationalConferenceonLearningRepresentations(2020).
54.VerbelenT,deTinguyD,MazzagliaP,CatalO,SafronA. Chunkingspaceandtimewithinformation
geometry. Proceedings of the Thirty-Sixth Conference on Neural Information Processing Systems
(NeurIPS2022),Information-TheoreticPrinciplesinCognitiveSystemsWorkshop(2022),6.
55.MirowskiP,Banki-HorvathA,AndersonK,TeplyashinD,HermannKM,MalinowskiM,etal. The
streetlearnenvironmentanddataset. CoRRabs/1903.01292(2019).
56.[Dataset]SavvaM,KadianA,MaksymetsO,ZhaoY,WijmansE,JainB,etal. Habitat:Aplatformfor
embodiedairesearch(2019).
57.Hwang J, Hong ZW, Chen E, Boopathy A, Agrawal P, Fiete I. Grid cell-inspired fragmentation and
recallforefficientmapbuilding. TMLR(2024). FeaturedCertification.
58.Kerbl B, Kopanas G, Leimku¨hler T, Drettakis G. 3d gaussian splatting for real-time radiance field
rendering(2023).
59.WilliamsG,DrewsP,GoldfainB,RehgJM,TheodorouEA. Information-theoreticmodelpredictive
control: Theory and applications to autonomous driving. Trans. Rob. 34 (2018) 1603–1622. doi:10.
1109/TRO.2018.2865891.
60.HeinsC,MillidgeB,DemekasD,KleinB,FristonK,CouzinID,etal. pymdp:Apythonlibraryfor
activeinferenceindiscretestatespaces. JournalofOpenSourceSoftware7(2022)4098. doi:10.21105/
joss.04098.
APPENDIX
1 ENVIRONMENTS
Theagent’sperceptualcapabilitiesarelimitedtoperceivingthecurrentroom’scolourandobstacleposition.
ThecolourobservationsreceivedbythemodelaredetailedinFigure12,theyareassociatedwithnumerical
valuestoaidvisualisation. Negative valuescorrespondtoun-observablepositions,indicating thatmoving
25
de Tinguy et al. Learning Dynamic Cognitive Map with Autonomous Navigation
inthatdirectionleadstoanobstacle.Ground-truthpositionsarealignedwiththeaxesoftheenvironments,
whereeachposeisrepresentedasa(row,column)pair-(y,x)-withineachenvironment.
Figure 12. Environments observationswiththeir namesas usedinthis paper. Thenegative observations
areun-observablebytheagent.
2 MODELS COMPLEMENTARY INFORMATION
Bothmodelsknowinadvancetheactionstheycantake,definedinourmini-gridenvironmentsasbeingup,
down,left,rightandstay-donotmove-.Toeaselearningweinformbothagentsthatstayingmeansnot
changingstate.
2.1 Our Model Parameters
Ourmodelleveragesthepymdpframework(60),aPythonlibrarydesignedforactiveinferenceindiscrete
statespaces.Initially,themodelstartswitha2x2dimensionforbothstateandobservationmatricesdueto
Python’smatrixrequirements.However,onlythefirstdimensioncontainsmeaningfulinformationatthe
start(indicatingtheinitialpositionandcurrentobservation).Astheagentexplores,themodeldynamically
expandsitsstateandobservationdimensions,incorporatingnewinformationfrombothitspredictionsand
actualobservations.
Theagentcontainsassub-models:
• theMarkovmatricesA (observationlikelihood)P(o |s ),B (statetransition)P(s |s ,a )and
o t t s t t−1 t−1
A (positionlikelihood)P(p |s ).
p t t
• thelistofsuccessivepredictedposesastuplesB (positiontransition)P(p |p ,a )
p t t−1 t−1
Thetransitionprobabilityupdate,asdepictedinEquation10,incorporatesvariouslearningratesbasedon
differentscenarios.Specifically,anexperimentedtransitionemploysalearningrateof10,whilethereverse
transitionisassignedarateof 7.Anexperimentedimpossibletransition(statecan’tbereachedbecauseof
anobstacle)isgiventhesamevaluesbutanegativelearningrate.Incontrast,apredictedtransitionadopts
alearningrateof5intheforwarddirectionand3inthereversedirection.Thisdifferenceinlearningrateis
usedtoadjustthecertaintywehaveaboutthechosenpolicyπ.Table3recapitulatesthosevalues.
26
de Tinguy et al. Learning Dynamic Cognitive Map with Autonomous Navigation
Table3. Transitionlearningratedependingonthesituation
Transitions Possible Impossible Imagined
Forward 7 -7 5
Reverse 5 -5 3
A higher learning rate reflects more confidence in the policy, while a lower learning rate indicates
uncertainty.Byweightingthelearningrate,themodelensuresthatthetransitionprobabilityisconsidered
initsdecision-makingindynamicenvironments.
B = Q(s |s ,π)Q(s )∗B ∗learning rate (10)
π t t−1 t−1 π
Thelikelihoodprobabilityupdateissimilarwithalearningratefixedto1.
Toassesstheagent’sconfidenceinitscurrentpose,weevaluatethecertaintyassociatedwithitsinferred
state by considering both observation and pose. If this confidence falls below 70%, the position is
disregarded for state inference until the agent can achieve a clearer understanding of its whereabouts.
This ensures that the model relies only on well-supported information when making decisions, thereby
improvingtheaccuracyandreliabilityofitsstateestimations.
The expected free energy is calculated for each future time-step as defined in equation 5 the agent
considersandisthenaggregatedtoinferthemostlikelysequenceofactionstoreachapreferredstate.This
beliefinpoliciesisachievedthrough:
P(π) = σ(−γG(π)) (11)
Whereσ,thesoftmaxfunctionistemperedwithatemperatureparameterγ convertingtheexpectedfree
energy of policies into a categorical distribution over policies. We set γ to 16 in our model to add more
weighttofavourpoliciesminimisingEFE.
2.2 Our Model Policies Generation
We have tried diverse policy creation to reduce the computational load on imagining all possible
transitions. Policy algorithms are linked to our models’ versioning as well. We have 3 different ways to
createpolicies,fromthemostcomputationallyexpensivetothelightest.
Considering the look-ahead as being the number of steps the agent can project itself over. The
first policy creation is simply implied to consider all possible combinations for our 5 actions
(Up/Down/Left/Right/Stay). This results in an exponential number of combinations: n policies =
5lookahead. Our second policy generation method creates paths covering all possible motions in the
look-aheadrangewhilemakingsureapathdoesn’treturnonitstrack.The’STAY’actionisaddedafter
everynewactionasanindependent path,suchthat theagentcanimaginestoppingatanytime. Thisresults
intheapproximatedparabolicequation:
n policies=˜86.3∗lookahead2 −392.0∗lookahead+377.0 (12)
27
de Tinguy et al. Learning Dynamic Cognitive Map with Autonomous Navigation
Finally,thelighterpolicygenerationproducessimpleL-shapedpathscoveringthelook-aheadrange(17),
addingthe’STAY’actionateachnewactionaddedinapathasanewindependentpolicy.Thecomplexity
ofthissolutionispolynomial:
n policies = 4∗lookahead2 +1 (13)
WithouttheSTAYaction,itwouldbecomelinear,allowingforaverylargelook-aheadwithoutworrying
aboutcomputationcost. However,themodel accuracyonimaginedconsequencesoveralongprediction
rangeisalsotoconsiderandhasnotbeenanalysedinthisstudy.
WhicheverpolicygenerationstrategyisusedallthegeneratedpoliciesfollowtheAIFprocessofselection
describedin(13).
In this work, we use 16 as a constant for the temperature γ. This means that we are highly likely to
choosetheoptimalpolicyratherthananyotherone.
2.3 CSCG Model Parameters
The CSCG model is imagined as a static matrix, it is therefore given the information of how many
observations it is going to encounter at initialisation. The number of clones has been set up to 10 so it
could work in all of our environments without issue. This means that the CSCG model can’t cope with
environmentslargerthanwhatitsmodeldimensioncanincorporate.
While training itsinternal beliefs given the sequence of actionsand observations we setthe pseudocount
to0.05andlettheagenttrainfor200iterations,thenfortheViterbioptimisationofthestateswesetthe
pseudocountto0.0001andletitrunfor100iterations.Thepseudocountisasmallconstantensuringthat
any transition under any action has a non-zero probability. It also improves convergence, however, too
smallduringtraininganditgiveswrongstateestimations.Allvalueshavebeencarefullyselectedtogive
the best result possible in the smallest amount of recursion. More details relative to the model can be
foundin(21).Inthisstudy,were-traintheCSCGmodelforeach5consecutivestepsoftheexploration,
consideringallthepastobservationsandactionsateachre-train.
2.4 Computational Costs
Weaveragedthecomputationtimeover5runsforbothourmodelandtheCSCGmodeltocomplete100
explorationsteps.Theresultsindicatethatourmodelissignificantlyfaster,underscoringitsrelevancefor
real-worldapplications.TheslowerperformanceoftheCSCGmodelcanbeattributedtoitsneedtofully
retrainevery5steps,whereasourmodelcontinuouslyupdatesitsinternalmapwithpredictedandobserved
changesateachstep.Inthesetests,ourmodeloperatedwithalook-aheadpredictionrangeof6steps.
Table4. ComputationaltimeinsecondsofoursandCSCGmodelwhileforcedtoexploreduring100steps
inanenvironment.Ourmodelimaginesthepoliciesover6consecutivesteps.
model Ours CSCG
Execution
14.95 48.35
time(s)
28

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Learning Dynamic Cognitive Map with Autonomous Navigation"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
