=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Development of Compositionality and Generalization through Interactive Learning of Language and Action of Robots
Citation Key: vijayaraghavan2024development
Authors: Prasanna Vijayaraghavan, Jeffrey Frederic Queisser, Sergio Verduzco Flores

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2024

Abstract: OneSentenceSummary: Generalizationinlearningverb-nouncompositionsimproves
significantlywithincreasedtrainingtaskvariations.
Humans excel at applying learned behavior to unlearned situations. A crucial
componentofthisgeneralizationbehaviorisourabilitytocompose/decompose
awholeintoreusableparts,anattributeknownascompositionality. Oneofthe
fundamental questions in robotics concerns this characteristic. ”How can lin-
guistic compositionality be developed concomitantly with sensorimotor skills
throug...

Key Terms: significantly, action, generalization, compositions, language, sensorimotor, development, robots, unlearned, interactive

=== FULL PAPER TEXT ===

Development of Compositionality and
Generalization through Interactive Learning
of Language and Action of Robots
Prasanna Vijayaraghavan,1 Jeffrey Frederic Queißer,1
Sergio Verduzco Flores,1 Jun Tani1∗
1OkinawaInstituteofScienceandTechnology,
Okinawa,Japan
∗Towhomcorrespondenceshouldbeaddressed;E-mail: jun.tani@oist.jp.
Abstract
OneSentenceSummary: Generalizationinlearningverb-nouncompositionsimproves
significantlywithincreasedtrainingtaskvariations.
Humans excel at applying learned behavior to unlearned situations. A crucial
componentofthisgeneralizationbehaviorisourabilitytocompose/decompose
awholeintoreusableparts,anattributeknownascompositionality. Oneofthe
fundamental questions in robotics concerns this characteristic. ”How can lin-
guistic compositionality be developed concomitantly with sensorimotor skills
through associative learning, particularly when individuals only learn partial
linguistic compositions and their corresponding sensorimotor patterns?” To
address this question, we propose a brain-inspired neural network model that
integratesvision,proprioception,andlanguageintoaframeworkofpredictive
coding and active inference, based on the free-energy principle. The effective-
ness and capabilities of this model were assessed through various simulation
1
4202
luJ
32
]IA.sc[
2v59991.3042:viXra
experimentsconductedwitharobotarm. Ourresultsshowthatgeneralization
in learning to unlearned verb-noun compositions, is significantly enhanced
when training variations of task composition are increased. We attribute this
to self-organized compositional structures in linguistic latent state space be-
ing influenced significantly by sensorimotor learning. Ablation studies show
that visual attention and working memory are essential to accurately gener-
ate visuo-motor sequences to achieve linguistically represented goals. These
insights advance our understanding of mechanisms underlying development
of compositionality through interactions of linguistic and sensorimotor expe-
rience.
Introduction
The problem of generalizing learned behavior to unlearned situations is easy for humans, but
incrediblychallengingforcognitiverobots. Compositionality(1–4)isamajorlinguisticcompe-
tencythatisessentialforgeneralizationofcognitivebehavior. Lakeandcolleagues(5)consider
compositionality as one of the three fundamental competencies necessary to build machines
that can learn to think like humans. Although interpretations of compositionality vary, Hupkes
et.al.(6)defineitbyidentifyingitsessentialcomponents. Amongthese,systematicity,theabil-
ity to recombine known parts and rules for use in a novel context, is a central component of
compositionality, on which we focus in the current study. Recent deep learning models seem
to trivialize this problem, but in reality, they offer little insight into how language develops in
humans. Although it can be argued that using large language models (LLMs) for end-to-end
learningshowsthattheycanunderstandthemeaningsofwordsbylearningfromalargecorpus
collectedintherealworld(7–11),theycannotaccessanysensorimotorpatternsassociatedwith
words and sentences. Our objective is to understand how the aforementioned systematicity as-
2
pectofcompositionalityinlanguageandbehaviorcanco-developthroughtheirinteractions,by
buildinganintegrativeneuralnetworkmodelforconductingroboticsimulationexperiments.
We use a developmental robotic approach in conjunction with the free-energy principle
(FEP)(12)toaddressthisproblem. Modelingembodiedlanguagewithdevelopmentalrobotics
isconsistentwiththeconstructivistview,orusage-basedtheoryoflanguageacquisition(13,14).
Embodiment is considered a necessary precondition for developing higher thoughts (15). Ac-
cording to Piaget (16) infants develop body-rationality representation through sensorimotor
interactions with the environment, accompanied by goal-directed actions. Neuroscience re-
searchers (17–19) have found that modulation of motor system activity occurs while listening
to sentences expressing actions, suggesting that humans infer actions from language and vice-
versa. Developmental robotics (20) also addresses the symbol grounding (21) problem, which
seeks to understand how symbols commonly used in linguistic expressions are associated with
meaning in the real world. Cognitive competencies such as visual attention and visual working
memory (VWM) are crucial in development of embodied language (14,22–25). Using de-
velopmental robotics, several studies have investigated the association of language and visuo-
proprioceptivebehaviorwithhierarchicalmulti-modalrecurrentneuralnetworks(26–31).
In parallel with the developmental robotic approach, recent advances in cognitive neuro-
science have underscored the significance of theoretical frameworks such as the free-energy
principle(FEP)inmodelingcognitivebrainmechanisms. AccordingtotheFEP,perceptionand
action are modeled in the framework of predictive coding (PC) (32,33) and active inference
(AIF) (34–36), respectively. PC is a theory of perception that provides a unifying framework
for neuronal mechanisms of top-down prediction and perceptual learning of sensory informa-
tion. AIF is a process for inferring actions that minimize the error between preferred sensation
and predicted actional outcomes. Some neural network models (35–39) have been success-
fully incorporated into goal-directed planning schemes based on active inference to show that
3
artificial agents can generate adequate goal-directed behaviors based on learning in the habitu-
ated range of the world. Some of these models (35,36) generated action plans by optimizing
thepolicy,andothers(37–39)optimizelow-dimensionallatentvariablesbyminimizingthefu-
ture expected free energy. Teleology, a philosophical concept that explains phenomena based
on their ultimate goals or purposes rather than merely their causes or origins, aligns closely
with principles of goal-directed behavior observed in these models. In the context of human
behavior, teleology offers a framework for interpreting actions as inherently goal-directed and
purpose-driven(40,41),providingaphilosophicalunderpinningthatcomplementsmechanistic
insightsofferedbyFEP-basedapproaches.
Inspired by these ideas we propose a neural network model (Figure 1A), to study co-
development of linguistic compositionality paralleling sensorimotor experience. It consists of
RNN-basedgenerativenetworksthathandlepredictionofvision,proprioception,andlanguage.
These modalities are integrated by the associative network. Our model utilizes an executive
layer mechanized by predictive coding inspired variational RNN (PV-RNN) (42) to integrate
language with visuo-proprioceptive sequences. PV-RNN, a neural network consistent with the
FEP, contains probabilistic latent variables that allow it to learn probabilistic structure hidden
inthedata. Somestudies(43,44)havefoundthatabstractrepresentationssuchasvectorrepre-
sentationsreconcilecompositionalgeneralizationwithdistributedneuralcodes.
We introduce a parametric bias (PB) (45,46) vector as the language latent variable. PB
is a low dimensional latent state vector used in recurrent neural network models. In training
of multiple temporal patterns, PB vector space is self-organized such that each temporal pat-
tern is encoded by a specific point in the PB vector space. Sugita and Tani (26) showed that
word sequences and corresponding behavioral temporal patterns can be bound using the PB
facilitatingthedevelopmentoflinguisticcompositionality. Inthecurrentmodel,theassociative
PV-RNN is constrained by the PB vector, which influences learning of associations between
4
vision,proprioception,andlinguistics.
This model learns to generate visuo-proprioceptive sequences with appropriate linguistic
predictionsbyminimizingevidencefreeenergy. Thetrainedmodelgeneratesappropriatevisuo-
proprioceptivesequencestoachievelinguisticallyrepresentedgoalsviagoal-directedplanning,
by means of active inference. Figure 1(B) and (C) show the visuo-motor sequences predicted
by the model compared with observed ground truth. We use a teleology-inspired approach to
goal-directed planning proposed by Matsumoto et.al (38). The underlying concept is that goal
expectationisgeneratedateverytimestepinsteadofexpectingthegoalatadistalstep.
Through simulation experiments we reach the following conclusions: First, generalization
in learning improves significantly as the number of variations in task compositions increase.
Second, the compositional structure that emerges in the linguistic latent state representation is
significantly influenced by sensorimotor learning. Specifically, we observed that the linguistic
latentrepresentationofactionalconceptsdevelopsbypreservingsimilarityamongcorrespond-
ingsensorimotorpatterns. Last,byperformingablationstudieswefoundthatthemodel’sability
toaccuratelygeneratevisuo-proprioceptivesequencesissignificantlyimpactedbythepresence
ofvisualattentionandworkingmemorymodules.
Results
In the current study we introduce vision based object manipulation tasks with a robotic arm
(Figure S1). The tasks include grasping, moving (in four different directions; left, right, front,
and back) and stacking. These tasks were performed on 5-cm cubic blocks of five colors (red,
green, blue, purple and yellow). Tasks are linguistically represented by sentences like ”grasp
X .”, ”move X left .”, ”move X right .”, ”move X front .”, ”move X back .” and ”put X on Y .”;
”X” indicates the color of the object being manipulated (any of the five colors) and ”Y” is the
color of the object at the base (we use green, blue or yellow) in the stacking task. Examples
5
Figure 1: (A) Model Architecture: Each modality generates visual, proprioceptive or linguis-
tic predictions. Visual (conv-LSTM) and Proprioceptive (LSTM) modalities are integrated by
the Associative PV-RNN and Linguistic LSTM is bound to Associative PV-RNN via Paramet-
ric Bias (PB). Visual predictions are enhanced by two visual working memory (VWM-1 amd
VWM-2) and attention mechanism, for which the parameters are generated by the Propriocep-
tiveLSTM.Foragivenlinguisticgoal”putgreenonblue”,(B)top: predictedvisualsequence,
bottom: observedgroundtruth;(C)top: jointangletrajectorypredictedbythemodelcompared
withthegroundtruth,bottom: motorpredictionerror.
6
of visuo-motor sequences of different types of tasks are shown in Figure S2. In total, there are
fortypossiblecombinations(5nouns,8verbs). Themodelwastrainedwithdatacollectedfrom
the physical robot. However, all evaluations were performed based on the ability of the model
togeneratementallysimulatedtrajectoriesofvisuo-proprioceptivesequences.
We perform two experimental evaluations with the above setup. First, we evaluate model
performanceforgeneralizationtounlearnedobjectpositionsandunlearnedlinguisticcomposi-
tions. We further emphasize this by comparing model performance among different degrees of
sparsityintrainingdata. Wealsoevaluatethemodel’sabilitytounderstandvisuo-proprioceptive
behavioral sequences by inferring the appropriate linguistic description. Second, we perform
anablationexperimentinordertostudytheimpactofvisualattentionandworkingmemoryon
themodel’sgeneralizationcapability.
Experiment I
In this experiment we evaluated the ability of the model to generalize to unlearned object po-
sitions and unlearned language compositions. The dataset was divided into four groups, each
with a different number of combinations. Group A contained 40 combinations (5 nouns and
8 verbs). Group B comprised 30 combinations (5 nouns and 6 verbs). Group C included 15
combinations (5 nouns and 3 verbs) and Group D contained 9 combinations (3 nouns and 3
verbs). Inordertoevaluatemodelperformanceingeneralizationinlearning,wefurtherdivided
each group with different ratios of training. Details of different training ratios in each group
are described in Table 1. Note that in Group D, since the total number of combinations is nine,
we used the ratios 77%, 66% and 33% instead of 80%, 60% and 40%. Details of individual
compositionsinthefourgroupsareillustratedinFiguresS3,S4andS5.
7
Table1: Trainingratios
Groups 1 2 3
GroupA(5x8) 32/40(80%) 24/40(60%) 16/40(40%)
GroupB(5x6) 24/30(80%) 18/30(60%) 12/30(40%)
GroupC(5x3) 12/15(80%) 9/15(60%) 6/15(40%)
GroupD(3x3) 7/9(77%) 6/9(66%) 3/9(33%)
InferenceofVisuo-ProprioceptiveSequencestoAchieveLinguisticallySpecifiedGoals
The model was trained with visuo-proprioceptive sequences that start at random initial config-
urations of objects in the work space. As previously mentioned, the trained model performed
goal-directedplanningusingactiveinferencefornovelobjectpositionsandunlearnedcomposi-
tionsoflinguisticallyrepresentedgoals(U-C),aswellasfornovelobjectpositionsandlearned
linguistically represented goals (U-P). The error between the visuo-proprioceptive plan gener-
ated by the network and the ground truth of visuo-proprioceptive trajectory was measured to
evaluatethemodel’sgeneralizationperformance.
Anexampleofsuccessfulgenerationofagoal-directedactionplantoachievealinguistically
represented goal, when trained with Group A1, is shown in Figure 2. A visuo-proprioceptive
error of 0.0113 was observed in this successful example. This figure shows mental simulation
ofthegeneratedmotorplanandtheexpectedvisualtrajectoryassociatedwiththelinguistically
specified goal (”put green on blue .”). Figure 2(G) compares ground truth joint-angle trajec-
tories of the test sequence with inferred trajectories from the planning process. Trajectories
0-4 (red, blue, green, yellow, purple) represent joint angles of all 5 active rotary joints of the
robot arm and joint. Number 5 (brown) refers to linear actuators of the robot gripper. The
visual stream shows every 5th time step of the generated sequence of the model. The current
focal area, in terms of size and position of the attention transformation is indicated by a red
square. Parameterization of the attention transformer is generated as an additional output of
the multi-layer proprioceptive LSTM, as previously mentioned. Attention (red box in Figure
8
time steps
selgna
tnioj
dezilamroN
t = 0 t = 50
A
B
C
D
E
F
G
H
Figure 2: Goal-directed planning using active inference: the model generated the above visuo-
proprioceptive sequence for the linguistically specified goal ”put green on blue .”. (A) masked
representaiton of VWM-2; (B) VWM-1; (C) model prediction of attended visual stream; (D)
final simulated prediction of the visual stream, the red box indicates coordinates for attention
predicted by the proprioceptive LSTM; (E) the ground truth target for comparison; (F) differ-
encebetweenthepredictedvisualstreamandthegroundtruthtarget;(G)normalizedjointangle
trajectorypredictedbythemodelcomparedwiththecorrespondinggroundtruth;and(H)mean
differencebetweenthepredictedjointanglesandthegroundtruth.
9
2(D)) is directed toward the object to be manipulated and the gripper when the gripper starts to
approach the object. Contents of VWM-2 and VWM-1 are illustrated in Figure 2(A) and (B),
respectively. The shape and color of the manipulated object are represented in VWM-2. Note
that when the object is being moved it disappears from VWM-1 and appears in VWM-2. This
information flow between VWM-1 and VWM-2 emerged in the visual network through train-
ing,whichisessentialforgeneralizationtonovelsituations,baseduponourpreviouswork(39).
We leverage this emergent mechanism in the current model to facilitate grounding of language
to visuo-proprioceptive behavior. These observations show that a mental image of a contin-
uous visuo-proprioceptive pattern can be generated using goal-directed planning to achieve a
linguisticallyspecifiedgoal.
It is evident that the model is adept at associating linguistically represented goals with cor-
responding visuo-proprioceptive sequences. Details of the model’s performance for general-
izing to unlearned object positions (U-P) with learned linguistically represented goals and to
unlearnedcompositions(U-C)oflinguisticallyrepresentedgoalsareprovidedinTableS1. De-
spite fewer variations of linguistic composition in training, the model still maintained a low
error ≤ 0.0405 (Group D3), for U-P. Figure 3 shows the comparison of generalization perfor-
mancebetweenU-PandU-Cfordifferentgroupswiththehighesttrainingratio. AlthoughU-P
performance does not change significantly, depending on the composition scale, U-C perfor-
mance improves as the variation of task composition in the training increases from Group D1
toGroupA1.
Figure 4 illustrates the difference in U-C performance between groups when trained with
different training ratios, as mentioned above. In the majority of failed cases, the model con-
fused colors of the object or misinterpreted the action to be performed on the object. This
indicatedthatfailureingeneralizationoccurredattheabstractlevel,sincelow-levelpredictions
still generated sequences corresponding to a different action or performed the specified action
10
0.07
0.06
0.05
0.04
0.03
0.02
0.01
0.00
GroupA(5x8) GroupB(5x6) GroupC(5x3) GroupD(3x3)
Groups
rorrE
egarevA
U-P
U-C
Figure 3: Comparison of average visuo-proprioceptive error, for inference of visuo-
proprioceptive plans, between unlearned object positions (U-P) and unlearned compositions
(U-C) among groups with different number of compositions with the highest training ratio of
80%.
11
0.12
0.10
0.08
0.06
0.04
0.02
0.00
GroupA(5x8) GroupB(5x6) GroupC(5x3) GroupD(3x3)
Groups
rorrE
egarevA
80%
60%
40%
Figure 4: Comparison of average visuo-proprioceptive error, between groups with different
training ratios for inference of visuo-proprioceptive plans to achieve unlearned compositions
(U-C)oflinguisticallyrepresentedgoals.
on a different object. Examples of visuo-proprioceptive sequence generated by the model in
cases of successful and failed generalizations are shown in Figures S6 and S7, respectively. As
just noted that even when the model fails to generalize, it still generates visuo-proprioceptive
sequencesthatdonotresultinlargeerrors. Thislikelyexplainswhy,despitepoorperformance
insomeinstances,theaverageerrorremainsrelativelylow.
Wequalitativelyanalyzedalatentspace,theparametricbiasPBspace,oflinguistic-LSTM
with kernel principal component analysis (KPCA) (47), using linear kernels. Figure 5 shows a
scatterplot of mean KPCA values of each cluster in the PB space of the model for all groups
12
Figure5: ScatterplotofmeanKernelPCAvaluesoflatentstatePBvectorsforallgroupswith
thehighesttrainingratio. (A):GroupA1(5x8,80%)(B):GroupB1(5x6,80%)(C):GroupC1
(5x3, 80%) (D): Group D1 (3x3, 77%). U-C refers to unlearned compositions that were used
for testing. Colors of markers indicate the color of the object being manipulated. The variance
explainedbythetwocomponentsofKPCAforallgroupswasgreaterthan90%
13
with the highest training ratio. Figure S9 shows the data distribution corresponding to mean
values shown here. The topology of hidden states corresponding to verb phrases (”grasp”,
”moveleft”,etc)seemstohaveacommonstructure,whenvisualizedseparatelyforeachobject
noun. For example, in Figure 5(A), representations of actions related to the stacking task are
alignedontheleftsideandfollowtheorderof”putXongreen”,”putXonyellow”and”putX
on blue” (X refers to the object noun that corresponds to the object color in our experiments),
fromtoptobottom. SimilarlyinFigure5(A)therepresentationofactionsrelatedto”move”are
clusteredontherightsideandfollowtheorder”moveXright”,”moveXback”,”moveXfront”
and”moveXleft”,fromtoptobottom. Also,therepresentationofactionsrelatedto”grasp”is
always between moving and stacking actions. Two things can be said from these observations.
First, PB vectors corresponding to similar action categories become similar and second, this
structure is largely similar for different colors, which implies compositionality between verbs
andnouns.
Thisstructureismoreconsistentamongdifferentobjectcolorswhenthevarietyoftaskcom-
positionsinlearningishigh,asseeninFigures5(A)and(B).Incontrast,inFigures5(C)and(D)
topologies of hidden states are inconsistent among different object colors. We find that struc-
turalrelationshipsbetweenlearnedcompositionsareextrapolatedbythemodeltogeneralizeto
unlearned compositions of nouns and verbs, illustrated by positions of unlearned compositions
(U-C)(Figure5). Themodel’sabilitytoextrapolatethisstructuretounlearnedcompositionsim-
proves as the number of variations in task compositions increases. We also performed Welch’s
unequal variances T-test (48) to compare the generalization performance (for U-C) between
groups, results of which are shown in Tables S3 and S4. The model showed robust generaliza-
tionperformancewhentrainedwithonly60%oftrainingdata,forgroupswithagreatervariety
of task compositions (Group A and B). Groups trained with 40% sparsity performed poorly,
irrespectiveofthevarietyoftaskcompositionsintraining. Weobservedasignificantdifference
14
in model performance between subsequent groups, where groups with more variations in task
compositionsalwaysoutperformedgroupswithfewerelementsforcompositions.
LanguageInferencefromObservedVisuo-ProprioceptiveSequences
The model’s ability to infer linguistically represented goals from visuo-proprioceptive behav-
iors was evaluated, with success defined by accurate inference of unlearned word sequences.
Generalization performance for unlearned object positions (U-P) and linguistic compositions
(U-C) is shown in Table S2, with Welch’s T-test results in Tables S5 and S6. Comparisons
across different training ratios are illustrated in Figures S8, demonstrating that increased train-
ing composition size and ratios improve generalization. This is analogous to results obtained
forinferenceofvisuo-proprioceptiveplansequences(Figure4). Althoughthemodelminimizes
the error between generated visuo-proprioceptive sequences and the ground truth, success is
measured by accuracy of inferred linguistic goals (Figure S8 and Table S2). Generalization
performance in inferring appropriate linguistic goals is poorer than expected. This is possibly
causedbytherelativelynoisyvisualimagesequencethatwasusedforinference(Notethatthis
goal inference, as well as training of the network, was performed by sampling real visual data
operatedbyaphysicalrobot.)
Experiment II: Ablation Study
In order to assess the impact of visual attention and working memory on the current model’s
performance, we conducted ablation studies to evaluate the model’s ability to generate men-
talsimulationofvisuo-proprioceptivesequenceswhenprovidedwithlinguisticallyrepresented
goals for Group A1 (5x8, 80%). We ablated the visual attention and visual working memories
(VWM-1 and VWM-2) (Table 2) and trained the model from scratch before evaluation. Ab-
lation of either working memory or the attention module significantly affected the ability of
15
the model to generate successful visual predictions, which resulted in poor generalization per-
formance (Table 2). The model showed significantly degraded generalization performance for
both unlearned object positions (U-P) and unlearned compositions (U-C). The proprioceptive
predictioncapabilityofthemodelwasalsoreducedsignificantly,butlessthanvisualprediction
capability,duetoablationofthevisualnetwork. Thisisreflectedintheproprioceptiveaccuracy
(Table2).
Table2: AblationStudy: PredictionError
VisualError(µ±SD)% ProprioceptiveError(µ±SD)%
Condition U-P U-C U-P U-C
VWM-1&2withattention 0.0196±0.0016 0.0249±0.0038 0.0110±0.0014 0.0117±0.0020
onlyVWM-1withattention 0.0407±0.0041 0.0543±0.0011 0.0136±0.0008 0.0158±0.0014
onlyVWM-2withattention 0.0391±0.0031 0.0506±0.0074 0.0149±0.0002 0.0155±0.0015
VWM-1&2withnoattention 0.0480±0.0050 0.0657±0.0046 0.0154±0.0007 0.0169±0.0075
noVWM-1&2andnoattention 0.0734±0.0052 0.0960±0.0068 0.0198±0.0004 0.0238±0.0037
The model performed comparatively better when there was at least one visual working
memory with attention, compared with having no attention module. These results highlight
thesignificanceoftheinteractionbetweenvisualattentionandworkingmemoryforgenerating
accurate visuo-proprioceptive predictions. Further implications of these results are discussed
below.
Discussion
This study investigated how generalization in compositionality can be achieved through the
process of associative learning between action and language, even with limited amount of the
experience, even with limited training data. We hypothesized that increasing task composition
variation used in learning would improve generalization. This hypothesis was evaluated by
conductingasetofsimulatedroboticexperimentsusingtheframeworkofactiveinference(35).
More specifically, we studied how robots can learn to generate goal-directed action plans by
16
adequately inferring visuo-proprioceptive sequences to achieve linguistically specified goals.
We also examined how such a robot can infer linguistically represented goals from observa-
tion of provided visuo-proprioceptive sequences. For this purpose, we built upon our previous
work (39) by adding a language processing LSTM and a PV-RNN in the associative layer, and
byusingateleologicalapproachforgoal-directedplanning(38). Theproposedmodelutilizeda
complexvisualnetworkwithsub-modules,includingvisualattentionandvisualworkingmem-
ory modules (VWMs), to facilitate grounding of language to visuo-proprioceptive behavioral
sequences.
Ouranalysisofsimulationexperimentresultstoinferactionplansforlinguisticallyspecified
goalsledtothefollowingfindings. First,generalizationperformanceinlearningunlearnedcom-
positions improves with increased vocabulary, as well as the training ratio. This is supported
by an analysis of the PB-space which showed more consistent relational structures among
different concepts combining actions and object nouns. These emerge for cases with more
variations of task composition in learning. Notably, we find representations in the PB-space
self-organized based on the similarity of actions. This validates our basic hypothesis. Second,
performance for position generalization does not depend on the size of composition used in
learning. Thisresultcanbeunderstoodbyconsideringthatpositiongeneralizationcompetency
should be developed in the lower level of the network model, which does not interact directly
withlinguisticcompositionalprocessing.
Althoughseveralstudieshaveinvestigatedgroundingoflanguagewithvisuo-proprioceptive
behaviorusingrecurrentneuralnetworkmodels(26–30,49),fewhaveaddressedthemechanism
underlying compositionality. To the best of our knowledge, there have been no models that ex-
aminedthecompositionalnatureoflanguagegroundedinvisuo-proprioceptivebehavior,using
the active inference framework. The current study explored a possible underlying mechanism
for linguistic compositionality, co-developed with sensory-motor skills to manipulate objects,
17
using active inference. Extended studies should investigate how this mechanism can be devel-
opedgraduallythroughincrementallearning,ashumanchildrendo.
By conducting ablation studies, we find that generalization performance in learning is sig-
nificantly reduced when either visual working memory or visual attention is deleted from the
model. This can be explained by considering that in the current model, visual image is per-
ceived based on structures rather than as simple pixel patterns, by means of visual attention
andworkingmemorymechanisms. Actually,previouswork(39)usingasimilarvisualnetwork
showed that coupled mechanisms of visual attention and working memory enable a manipu-
lated object to be segmented from the background. Therefore, it is highly likely that marriage
ofstructuralvisualinformationprocessingandcompositionallinguisticinformationprocessing
enhances generalization in learning in the current task. Analysis of the model’s performance
ininferringlinguisticallyrepresentedgoalsfromobservationofvisuo-proprioceptivesequences
showed results analogous to those obtained for inferring visuo-proprioceptive plan sequences
fromprovidedlinguisticgoals.
According to Hupkes et al. (6) compositionality of a neural network model should satisfy
the following characteristics: systematicity - it should systematically combine known parts and
rules ; productivity - it should extend its predictions beyond the lengths observed during train-
ing; substitutivity - its predictions should be robust to synonymous substitutions; localism -
the degree to which the meaning of compositional expression depends on its immediate, local
structures vs global structures; and overgenralization - it should not favor particular rules or
exceptions during training, i.e., it should not overgeneralize. We see evidence that our model
demonstrates at least a rudimentary level of systematicity through its ability to generate appro-
priate visuo-proprioceptive sequence plans for unlearned combinations of actions and nouns,
effectivelygeneralizing tonovel scenarios. Thiscapability confirmsthat themodel cansynthe-
sizenew,meaningfulsequencesfrompreviouslylearnedpartsandrules,meetingakeycriterion
18
forcompositionality. Althoughtestingforeveryaspectofcompositionalityisbeyondthescope
ofthecurrentstudy,itisapromisingavenueforfutureresearch.
We showed that generalization performance in learning unseen compositions increases as
the size of the vocabulary and variations in task composition increase, evidenced by the best
performanceseeninthelargestgroup(5x8composition). Thisresultoffersaminimalpotential
solution to the poverty of stimulus problem (50). If the dimensions of composition increase
to include not only verbs and object nouns, but also various modifiers such as adverbs and
adjectives, and each dimension consists of hundreds of elements, as we experience in daily
life, covering all possible combinations across all dimensions would result in a combinatorial
explosion. Faced with this problem, our expectation is that the required amount of experience
forgeneralizationinlearningisnotproportionaltotheproductofthenumberofelementsacross
all dimensions, but rather is proportional to their summation, provided that the elemental size
of each dimension is relatively large. Future studies should evaluate this possibility, which
aims beyond generalization shown by typical neural network models, by conducting the same
experiments under drastically scaled settings. The current study was necessary in order to
examine basic mechanisms accounting for how generalization can be achieved in language-
behaviorcompositionalitywithrigorousanalysis,beforescalingupthesystem. Additionally,it
aimedtoinvestigatehowthemodel’scapabilityimproveswithincrementsinsizesofindividual
elementsineachdimension.
Previous work with PV-RNN-based models (38,51) utilized the ”online error regression”
scheme, where prediction error serves as an input feature to retroactively correct the history of
predictions(postdiction)whilesimultaneouslypredictingfuturebehavior. Thisapproach,while
effective, is computationally intensive and relies on low-dimensional input features for feasi-
bility. This limitation is especially significant given that our model evaluations were not con-
ductedonphysicalrobots. Eventhoughtheproposedmodelshowscompetitiveperformancein
19
generating appropriate visuo-proprioceptive trajectories, compared to ground truth trajectories,
executing the generated trajectories with a real robot may not yield the desired level of perfor-
mance. Inferring object positions accurately from 64x64 RGB images in the video can result
in large errors in the real world. A single-pixel error from noise in the 64x64 RGB image can
result in a position error of several centimeters, which will significantly affect the behavioral
performance of the robot, especially when the robot attempts to grasp objects. If a 256x256
RGB image can be used, the position error could be reduced to <1 cm. This scheme, how-
ever, prohibits real time computation (It takes several minutes to generate a single visuo-motor
plantrajectory),sinceexpensiveback-propagationthroughtime(BPTT)computationshouldbe
conductedthroughtheconvLSTM.Futurestudiesshouldinvestigatemoreefficientsolutionsto
speed-up this part of the computation, e,g., developing a C++ compiler for the whole system,
instead of using the current Python-based program to achieve real-time operation of physical
robots using the proposed model. Upon solving these issues the model will have the potential
to be scaled up to more complex tasks with rich linguistic descriptions for cognitive robots to
interact with the real world. We are actively working on this problem to make future iterations
ofthemodelmorecomputationallyefficient.
Models like CLIP (Contrastive Language–Image Pre-training) (52) and CLIP-Guided Gen-
erative Latent Space Search (CLIP-GLaSS) (53) learn to associate images and textual descrip-
tions in a joint embedding space. They employ a contrastive learning objective to align em-
beddings of images and their corresponding textual descriptions. While our model may share
some features with these approaches, the PB-vector is not a shared embedding for behavior
and language. Instead it acts as a bottleneck to constrain both visuo-proprioceptive sequences
andwordsequencessuchthattheysharesimilarstructures. Ourrationalefornotusingashared
embedding space for visuo-proprioceptive behavior and language is to maintain flexibility in
learning behavioral patterns that can achieve the same linguistic goals. For example, the robot
20
may learn multiple trajectories to achieve the linguistically represented goal of ”put red on
green.”,dependingonobjectpositions.
A major limitation of the current study is the absence of communication in a societal con-
text. Taskswereexecutedbyasingleroboticarm,lackingactiveengagementwithotheragents,
which is essential for language development in humans (22). The model operates in a small
workspace with a limited vocabulary. One possible way to scale the model is to increase the
number of compositional elements (adverbs, adjectives, conjunctions, etc) to form longer sen-
tences, as discussed previously. Moreover, extending the model to incorporate multiple agents,
suchasinRT-2(7)),eachwiththeirownmodels,couldfacilitateexaminationofcommunication
dynamicswithinasocietalcontext.
Recent advances in LLMs have shown incredible performance with robots working in real-
world environments (54–57). These models, equipped with sophisticated language processing
capabilities,haveenabledrobotstocomprehendandgeneratehuman-likelanguage,facilitating
seamless interaction with users and enhancing their overall functionality. However, it’s impor-
tant to note a significant difference in the way language is acquired by these models compared
to humans. While humans develop language skills through interaction with their environment,
as we have tried to emulate in our model by intermingling linguistic cues with physical ex-
periences and sensory inputs, the language capabilities of LLMs are predominantly acquired
throughpassiveexposuretovastlinguisticdatasets. TheextenttowhichLLMscantrulyunder-
standlanguageinahuman-likemanner(58–61)isanintriguingquestion. Asroboticscontinues
toadvance,bridgingthegapbetweenlanguageunderstandinginmachinesandhumansremains
a key research challenge. Efforts to incorporate embodied interaction and sensorimotor experi-
ences into language learning processes for robots hold promise to enhance the naturalness and
robustnessoftheirlinguisticabilitiesinreal-worldscenarios.
21
Materials and Methods
We propose a hierarchically organized generative model to handle multiple modalities includ-
ing vision, proprioception and language. The model is trained end-to-end through supervised
learning,utilizingtrainingexamplescontainingvisuo-proprioceptivesequencespairedwithcor-
respondinglinguisticexpressions. Themodellearnstogeneratevisuo-proprioceptivesequences
corresponding to associated linguistic expressions, by minimizing evidence free energy. The
trained model is used to generate goal-directed plans in which the goal is represented by lin-
guistic expression. Goal-directed visuo-proprioceptive sequences are generated by minimizing
expected free energy, by means of active inference. Notably, this model inherits the vision
module, with multiple visual working memory modules and visual attention (39). In order to
integrate language into this model we employ the parametric bias (PB) scheme proposed by
SugitaandTani(26). Forallexperiments,themodelwastrainedandevaluatedwithfiverandom
seeds,asdescribedintheImplementationDetailssectionoftheSupplementaryMaterials.
Model
TheoverallarchitectureofthecurrentmodelisillustratedinFigure1(A).Themodelconsistsof
RNN-basedgenerativenetworksthathandlepredictionofvision,proprioceptionandlanguage.
These modalities are integrated by the associative network. There are three layers of stacked
LSTM and convLSTM for the proprioception and vision networks, respectively. The language
networkisimplementedasasingle-layerLSTMwithaparametricbias(PB)vector(Figure6).
The associative network is a single-layer PV-RNN that connects to the top layer of the visual
and proprioceptive networks. Language is bound to the associative network through a binding
loss between the linguistic PB vectors and P(cid:103)B that is generated by the associative network
t
(Figure6).
Eachlayerinthevisuo-proprioceptivepathwayreceivescontextualinformationfromneigh-
22
boringlayers. Top-downconnectivityprovidessignalfromthesubsequenthigher-levellayeror
from the associative layer of the model. Those layers propagate the prediction or belief of
the network down to the sensorimotor level. A deconvolution operation is applied in the vi-
sual pathway, with dimensionality increasing from top to bottom. Visual and proprioceptive
LSTM cells on the same layer of the model are connected via lateral connections. As in top-
down processing, a deconvolution operation is applied to expand the low-dimensional space
of proprioceptive representations to match the dimensions of the feature space of convLSTMs.
Bottom-upconnectivitysendsneuralactivationfromthelowerlayerofthemodelorthecurrent
sensory input, i.e. vision or proprioception, into the subsequent higher-level layer. Visual input
isprocessedbyanattentionmodule,andaconvolutionoperationisappliedtoreducethedimen-
sion of projections to the next higher layer. Visual processing incorporates visual attention and
saving/reading of visual images, using visual working memory (VWM). These are performed
autonomouslyaspartoftheinferenceprocess.
Theproprioceptivenetworkpredictssequencesofjointangles(m )andmultiplelowdimen-
(cid:101)t
sionalcontrolsignals(αatt andαM2). Thesecontrolsignalsactasparametersofvisualattention
t t
and visual image transformation (39,62) in order to modulate information flow in the visual
system. Visual attention performs dynamic adjustment of the pixel density in different regions
of the image generated by the visual network. This allows the model to focus on and predict
the visual appearance of manipulated objects in greater detail, while static parts of the gener-
ated images can be retrieved from the VWM-1. Furthermore, an additional parametric control
of pixel-wise transformation (62) of images stored in VWM-2 allows the model to imitate dy-
namic changes of object images during its manipulation. This parametric control is performed
by the output of proprioceptive LSTM (αM2). This transformation is limited to affine image
t
transformationsconsideringthenatureofthetaskusedinourexperiments.
The visual network predicts pixel images of the currently attended region (vatt) and a set
t
23
of masks that are used to mix the predicted image with the contents of each VWM (see Figure
1 (A)). The final visual image is predicted through the interaction of this convLSTM predic-
tion with parameterized visual image operations, including attention, inverse attention, fusion,
and transformation. Attention is performed by application of the current attention filter, pa-
rameters of which are predicted by the proprioceptive LSTM on the plain visual image. In-
verse attention is simply an inverse transformation of attended image. The fusion operations
(denotedbysymbol ) fuses two sources of visual streams with a pixel-wise mixing ratio,
using outputs and corresponding masks generated from the multi-layer convLSTM. Fusion op-
erationsareutilizedtocomposethefinalprediction,aswellastoupdatetheVWMs.
Generation of top-down signals from the associative PV-RNN is based on the adaptive pa-
rameterA(representingtheapproximateposteriorprobabilitydistributionintermsofthemean
andstandarddeviationateachtimestep),whichisoptimizedtogetherwithnetworkparameters
during training, to minimize the evidence free energy. The language network predicts a short
sequence of words (S(cid:101)) at each time step of the visuo-proprioceptive sequence. Each word (
(cid:101)
s
i
)
is represented by a one hot vector, and the corpus is limited to 20 possible words. Language
predictiondependsonparametersofthelinguistic-LSTMandtheparametricbias(PB)vector,
whichisconstrainedbythepredictedP(cid:103)B fromtheassociativePV-RNNthroughabindingloss
t
(equation30)(Figure6).
During learning, updating adaptive variables and connectivity weights of the network is
performed to minimize the reconstruction error between prediction and observation. To this
end, back-propagation of the error is performed inversely through the aforementioned top-
down and bottom-up pathways to update values of adaptive latent variables A and PB. In
summary, the model represents a variational Bayes generative model in which learned multi-
modal spatio-temporal sequence patterns, including proprioception, vision and language, can
bereconstructedbyadequatelyinferringthecorrespondingprobabilisticlatentvariablesAand
24
Figure6: GraphicalrepresentationoftheassociativePV-RNNandlinguisticLSTM
25
PBs. Thisispossiblebecauseallfabricatedfunctions,suchasvisualattention,maskoperation,
andaffinetransformation,aredesignedasdifferentiablefunctions.
The lowest layers of vision, proprioception and language modules receive corresponding
sensory inputs, pixel based images v , the softmax representation of the current joint angle
t
configuration m , and a series of one-hot vectors as linguistic input s . The computation in the
t i
inputlayersofthenetworkcanbedefinedas:
vnet = ATT(v ,αatt), (1)
l=0,t t t
mnet = SoftMax(m ), (2)
l=0,t t
snet = s , (3)
l=0,i i
with visual attention transformation ATT(v ,αatt) parameterized by αatt applied to the vi-
t t t
sual input. We use the suffix i to denote individual word steps of the language. It is important
to note that while vision and proprioception are synchronized and share the same number of
time steps, language is expressed as a sentence in which each word is represented by a one-hot
vector. Therefore linguistic prediction, limited to five word steps, is predicted by the model at
eachstepofthevisuo-proprioceptivesequence(Figure6).
The proprioceptive prediction mnet as well as the low-dimensional parameterizations αatt
t t
and αM2, which modulate attention and the affine transformation of VWM-2, are generated
t
fromthehiddenstatesoftheproprioceptivepathwayas:
mnet = FFN(mnet ), (4)
t l=1,t
αatt = FFN(mnet ), (5)
t l=1,t
αM2 = FFN(mnet ), (6)
t l=1,t
with FFN denoting a fully connected feed-forward network. Note that the FFNs do not share
thesameconnectivityweights.
26
Neural activation in the visual pathway (stacked convLSTM) for layer l = 1 to l = L at
timesteptisdefinedas:
(cid:40) (cid:0) (cid:1)
ConvLSTM vnet ,mnet ,anet , ifl = L
vnet = l−1,t l,t−1 t−1 (7)
l,t ConvLSTM (cid:0) vnet ,mnet ,vnet (cid:1) , otherwise.
l−1,t l,t−1 l+1,t−1
Neuralactivationintheproprioceptivepathway(stackedLSTM)isdefinedas:
(cid:40) (cid:0) (cid:1)
LSTM mnet ,vnet ,anet , ifl = L
mnet = l−1,t l,t−1 t−1 (8)
l,t LSTM (cid:0) mnet ,vnet ,mnet (cid:1) , otherwise.
l−1,t l,t−1 l+1,t−1
ThemodelusesonlyoneLSTMlayerwithPBforthelanguagenetwork. Itsneuralactiva-
tionisdefinedas:
snet = LSTM(snet ,PB) (9)
i i−1
As previously mentioned, the visual and proprioceptive pathways are connected by lateral
connections in each layer of convLSTM and LSTM blocks, respectively. Additionally, the
model includes an associative PV-RNN for a combined representation of both pathways in the
highest layer. PV-RNN is comprised of deterministic d and stochastic z latent variables. The
PV-RNN generates predictions from a prior distribution p and infers an approximate posterior
distribution q by means of prediction error minimization on the generated sensory output x.
Thepriorgenerativemodelp isfactorizedasshowninfollowingequation.
θ
p (x ,d ,z |d ) =
θ 1:T 1:T 1:T 0
(cid:89) T (10)
p (x |d )p (d |d ,z )p (z |d ).
θx t t θ
d
t t−1 t θz t t−1
t=1
Thepriordistributionp (z |d )isaGaussiananditdependsond ,exceptattheinitial
θz t t−1 t−1
time step t = 1 which is fixed as a unit Gaussian with zero mean. Each sample of the prior
27
distributionzp iscomputedasshowninthefollowingequation.
t
(cid:40)
0, ift = 1
µp =
t
tanh(W d ), otherwise
d,zµp t−1
(cid:40)
1, ift = 1 (11)
σp =
t
exp(W d ), otherwise
d,zσp t−1
zp = µp +σp ∗ϵ.
t t t
where ϵ is a random noise sample such that ϵ ∼ N(0,I). W is the connectivity weight
matrix. Weomitthebiasterminallequationsforthesakeofbrevity.
Since computing the true posterior distribution is intractable, the model infers an approxi-
mateposterior(q )attimestept,zq computedasshowninequation12. Aµ ,Aσ areadaptive
φ t 1:T 1:T
variables, inferred through back-propagation by minimizing the prediction error and the com-
plexityterm,asdetailedbelow. Theseareusedtocomputethemeanandstandarddeviationfor
theapproximateposteriorateachstepinasequence.
µq = tanh(Aµ),
t t
σq = exp(Aσ), (12)
t t
zq = µq +σq ∗ϵ.
t t t
We use a multiple timescale recurrent neural network (MTRNN) (63), adapted for a single
layer, as the RNN for the associative PV-RNN layer. The deterministic latent variable of the
associativelayeranet iscomputedasfollows,
t
(cid:18) (cid:19) (cid:18)
1 1
d = 1− d + W anet +W zq
t τ t−1 τ a,a t−1 z,a t
(13)
(cid:19)
+W vnet +W mnet ,
v,a l=L,t−1 m,a l=L,t−1
anet = tanh(d ). (14)
t t
where τ is the time constant that determines the rate at which the network integrates infor-
mation over time. The associative layer also predicts a parametric bias (P(cid:103)B ) vector at each
t
28
time step, which is bound to the PB vector of the language module through a binding loss
(equation30). TheP(cid:103)B vectoriscomputedas:
t
P(cid:103)B = tanh(W d ) (15)
t d,pb t
W istheconnectivityweightmatrixbetweendandPB.
d,pb
Our previous study (39) showed that visual image transformations by attention and inverse
attention are among the most important elements for successful development of visual work-
ing memory function during end-to-end learning. As mentioned previously, visual attention
is performed by an attention transformation parameterized by scaling and coordinates of a fo-
cal position. These parameters are generated by the proprioceptive multi-layer LSTM, which
receivestop-downsignalsfromtheassociativePV-RNNinthehigherlevel. Thismeansthatop-
timal parameters for visual attention during training and goal-directed planning are determined
bytheinferenceofoptimallatent-statevaluesA .
1:T
The visual attention and visual working memory systems are applied to the output of the
convLSTMblock,whichincludepredictionoftheattendedvisualimagevnetandasetofmasks,
t
computedas:
vnet = tanh(Deconv(vnet )), (16)
t l=1,t
(cid:20) (cid:21)
gM1
t = ATT−1(Sig(Deconv(vnet )),αatt) (17)
gpred l=1,t t
t
(cid:20) (cid:21)
gM2
t = Sig(Deconv(vnet )), (18)
gnet l=1,t
t
with a sigmoidal activation function Sig. Note that the Deconv operations do not share
the same connectivity weights. The masks gM1 and gM2 modulate the pixel-wise update of the
t t
VWM-1andVWM-2,respectively. Furthermore,themasksgpred andgnet decidehowmuchthe
t t
finalvisualpredictiondependsontheVWMsorvnet (seefigure1(A)).
t
29
Detailsofnetwork-wiseoperationsforvisualworkingmemories,VWM-1andVWM-2,are
describedbythefollowingequations:
vwmM1 = (1−gM1)⊙vwmM1
t+1 t t
(19)
+gM1 ⊙ATT−1(vatt,αatt).
t t t
Equation19describeshowcontentsofVWM-1(vwmM1 ),canbeupdated,wheregM1 denotes
t+1 t
a pixel-wise mask and ATT−1 performs inverse attention transformation, parameterized by
αatt (equation 5), on the predicted attended visual image vatt. The element-wise multiplication
t t
operatordenotedbysymbol⊙fusesthevisualstreamandmask.
vwmM2 = gM2 ⊙TRAN(vwmM2,αM2)
t+1 t t t
(20)
+(1−gM2)⊙vatt.
t t
Equation 20 describes how VWM-2, vwmM2, can be updated. The variable gM2 denotes
t
a pixel-wise mask that defines the fusion of transformed contents TRAN(vwmM2,αM2) of
t t
VWM-2 with vatt, the predicted image in the attended feature space from the previous step.
t
This ensures that the contents of vwmM2 are influenced only by the visual prediction in the
attended region and by the transformed vwmM2. Prediction vatt of attended visual images is
t
performed by a fusion of the predictions made by the convLSTM, gnet and the contents of
t
VWM-2,definedby:
vatt = gnet ⊙vnet +(1−gnet)⊙TRAN(vwmM2,αM2). (21)
t t t t t t
The visual output v is computed by fusion of the contents of VWM-1, vwmM1, with the
(cid:101)t t
predictedattendedimagevatt. InverseattentiontransformationATT−1 isappliedtovatt tomake
t t
itpossibletofusewiththecontentsVWM-1,definedas:
30
v = gpred ⊙ATT−1(vatt,αatt)+(1−gpred)⊙vwmM1. (22)
(cid:101)t t t t t t
The final proprioceptive prediction is generated by a decoding of the softmax encoded pre-
dictionsoftheLSTMtogetthetrajectory,m ,ofjointangleconfigurations:
(cid:101)t
m = SoftMax−1(mnet). (23)
(cid:101)t t
The final linguistic prediction, s , is computed through a fully connected output layer fol-
(cid:101)i
lowedbyasoftmaxactivationfunctiontogetaone-hotvectorrepresentationforeachword,
s = SoftMax(FFN(snet)) (24)
(cid:101)i i
The sentence predicted at every time step of the behavior is the same and is defined as
S(cid:101) = (
(cid:101)
s
1
,
(cid:101)
s
2
,
(cid:101)
s
3
,
(cid:101)
s
4
,
(cid:101)
s
5
). Examples of sentences describing actions performed by the robot are
”putredongreen.”,”graspred.”,”moveredleft.”,etc,whereeachwordisrepresentedbyone
hot vector. Note that the sentences are of different lengths; therefore, in order for all sentences
to be the same length, the remaining steps are masked with zero vectors to get a maximum of
five word steps. The final character, s of every sentence is always the vector corresponding to
(cid:101)5
”.”,indicatingtheendofthesentence.
Learning by Minimizing Free Energy
The model learns to generate visuo-proprioceptive sequences by minimizing the evidence free
energy F (12). Free energy is comprised of an accuracy term and a complexity term, as
explained in the Materials and Methods of the Supplementary Materials (subsection entitled
”Training by minimizing Free Energy”). In the current model, during learning the free energy
is minimized by updating model parameters that include network weights, biases, the paramet-
ricbiasPB,andtheadaptivelatentvariableA . Theaccuracytermcanbedefinedasthesum
t:T
31
ofreconstructionerrorsfromeachmodality. Thesearedefinedas:
T T
(cid:88) (cid:88)
L = L = catt ⊙(v −v )2, (25)
v v,t t (cid:98)t (cid:101)t
t=1 t=1
T T
(cid:88) (cid:88)
L = L = D (SoftMax(m )||m ), (26)
m m,t KL (cid:98)t (cid:101)net,t
t=1 t=1
5
(cid:88)
L = (s −s )2 (27)
S (cid:98)i (cid:101)i
i=1
L is the visual error, L is the proprioceptive error and L is the language error. catt
v m S t
is error term that balances the contribution of the focal and peripheral regions of the visual
error. This impedes over-representation of back-propagated gradients of the focal area. De-
tails of calculation of catt are described in section S1 of Supplementary Materials. Lengths of
t
visuo-proprioceptive sequences are denoted by T. The proprioceptive target for time step t is
a softmax encoding of joint angles m . Visual targets at each time step are observed images
(cid:98)t
v and language targets are a sequence of words s . Note that language has a maximum of five
(cid:98)t (cid:98)i
wordsthatdescribeeachtask.
Additionally, we introduce a binding loss between P(cid:103)B predicted and generated by PV-
t
RNNandPBinferredbylinguisticLSTMshownas:
T
(cid:88)
L = k∗ (P(cid:103)B −PB)2 (28)
pb t
t=1
k is the binding coefficient that influences the strength of binding between language and be-
havior. The associative network must predict a constant parametric bias at all time steps in
order to minimize this loss. This imposes a constraint on the associative network to generate
visuo-proprioceptivepredictionsthatareboundtolanguage. Thisallowsflexibilityinrepresen-
tation,sincethesamelinguisticexpressioncanbeassociatedwiththetaskperformedatrandom
positionsofobjectsintheworkspace.
Thelossoptimizedbythemodelduringtrainingisthendefinedas:
32
T
(cid:88)
L = L +L +L +L +w D (q (z |X)||p (z )) (29)
v m s pb KL φ t θ t
t=1
X indicates the sensory observation in all modalities. The complexity term is the KL di-
vergence between the approximate posterior and prior distributions. Note the formal similarity
between equation 31 and equation 25 (and 32) where the accuracy corresponds to the various
componentsoftrainingloss,L. Inthecurrentmodel,freeenergyismodifiedbyinclusionofthe
meta-prior w that weights the complexity term. w is a hyperparameter that affects the degree
ofregularizationinPV-RNN,determinedbytheexperimenter.
ThemodelupdatestheadaptivelatentvariableA (usedtocomputeq asdescribedabove)
1:T φ
withnetworkweightsandbiasesinordertominimizethefreeenergy,whichconsistsofthepre-
dictionerrorbetweenthepredictedsensationandtheobservedgroundtruthandthecomplexity
term, which is the KL-divergence between the prior (p ) and the approximate posterior (q ).
θ φ
The prior is also learned through minimization of free energy. The learned prior is utilized to
generate sensory predictions during the inference phase. Algorithm 1, in the Supplementary
Materials,describesthelearningprocessforasingletrainingsequence.
Goal-directed planning using Active Inference
Activeinferenceproposesthatactiongenerationisawaytominimizeerrorbetweenthedesired
goal state and the predicted goal state by acting appropriately on the environment (see Supple-
mentary Materials under the subsection ”Linguistically Specified Goal-Directed Planning by
Active Inference”). A standard interpretation of goal-directed planning is to assume that the
goal is presented at the distal step of a behavioral sequence. In the current model, we use a
teleological approach to goal-directed planning proposed by Matsumoto et.al (38). The central
idea is that goal expectation is generated at every time step instead of just at the distal step. In
33
ourmodel,theexpectedfreeenergyorgoallossisdefinedas:
T
(cid:88)
Lg = Lg +Lg +w D (q (z )||p (z )), (30)
S pb KL φ t θ t
t=1
Lg isthereconstructionerrorbetweenpredictedlanguageandgoallanguage,Lg isthebinding
S PB
loss between the corresponding parametric bias PB vectors. The KL divergence (complexity
term) between the approximate posterior and prior distributions is weighted by the meta-prior
w. The length of the visuo-proprioceptive sequence is denoted as T. Back-propagation of
error is performed for inferring adaptive latent variables (A) in the associative PV-RNN and
the parametric bias (PB) in the linguistic LSTM (while the connectivity weights of the whole
network are fixed) by which, plans, in terms of visuo-proprioceptive sequences are generated.
Algorithm 2, in the Supplementary Materials, describe both types of inference processes. The
modelperformanceisevaluatedbasedonitsabilitytosuccessfullygenerateactionplans(visuo-
proprioceptivesequences)whengivenaspecificlinguisticallyrepresentedgoal.
Language Inference from Observed Visuo-Proprioceptive Sequences
Themodelcanalsogeneratelinguisticpredictionsthatcorrespondtoobservedvisuo-proprioceptive
sequences, by optimizing approxiamte posterior (q ) and parametric bias (while the connectiv-
φ
ity weights of the whole network are fixed) in order to minimize the free energy of the visuo-
proprioceptive sequences (Figure S11). The free energy that is minimized for inference of
linguisticallyrepresentedgoalisdefinedas:
T
(cid:88)
Lg = Lg +Lg +Lg +w D (q (z )||p (z )), (31)
v m pb KL φ t θ t
t=1
Lg and Lg is the error between observed ground truth and the vision and proprioceptive pre-
v m
dictions, respectively. Algorithm 3, in the Supplementary Materials, describe both types of
inferenceprocesses.
34
References
1. N.Chomsky,SyntacticStructures(MoutonandCo.,TheHague,1957).
2. G.Evans,TheVarietiesofReference(OxfordUniversityPress,1982).
3. F. Gottlob, Collected Papers on Mathematics, Logic, and Philosophy (Wiley-Blackwell,
1991).
4. T. Janssen, Frege, contextuality and compositionality., Journal of Logic, Language and
Information10,115-136(2001).
5. B. M. Lake, T. D. Ullman, J. B. Tenenbaum, S. J. Gershman, Building machines that learn
andthinklikepeople,BehavioralandBrainSciences40,e253(2017).
6. D. Hupkes, V. Dankers, M. Mul, E. Bruni, Compositionality decomposed: How do neural
networksgeneralise?,JournalofArtificialIntelligenceResearch67,757-795(2020).
7. C. Lynch, et al., Interactive language: Talking to robots in real time, IEEE Robotics and
AutomationLetterspp.1–8(2023).
8. S.Nolfi,Ontheunexpectedabilitiesoflargelanguagemodels,arXiv:2308.09720(2023).
9. M. Abdou, et al., Can language models encode perceptual structure without grounding? a
casestudyincolor,arXiv:2109.06129(2021).
10. S. Yousefi, L. Betthauser, H. Hasanbeig, R. Millie`re, I. Momennejad, Decoding in-context
learning: Neuroscience-inspired analysis of representations in large language models,
arXiv:2310.00313(2024).
11. E. Pavelick, Symbols and grounding in large language models, Phil. Trans. R. Soc. A
(2023).
35
12. K. J. Friston, The free-energy principle: a unified brain theory?, Nature Reviews Neuro-
science11,127-38(2010).
13. T. Cameron-Faulkner, E. Lieven, M. Tomasello, A construction based analysis of child
directedspeech,CognitiveScience27,843–873(2003).
14. M. Tomasello, The usage-based theory of language acquistion (In Edith L. Bavin (Ed.),
TheCambridgehandbookofchildlanguage(pp.69-87),CambridgeUniv.Press,2009).
15. L. Smith, M. Gasser., The development of embodied cognition: six lessons from babies,
ArtifLife.Winter-Spring;11(1-2):13-29(2005).
16. J.Piaget,Thelanguageandthoughtofthechild (Meridian,NewYork,1955).
17. G.Buccino,etal.,Listeningtoaction-relatedsentencesmodulatestheactivityofthemotor
system: A combined tms and behavioral study, Cognitive Brain Research 24, 355-363
(2005).
18. F. R. Dreyer, F. Pulvermu¨ller, Abstract semantics in the motor system? – an event-related
fmri study on passive reading of semantic word categories carrying abstract emotional and
mentalmeaning,Cortex 100,52-70(2018).
19. F. Pulvermu¨ller, L. Fadiga, Active perception: sensorimotor circuits as a cortical basis for
language,NatureReviewsNeuroscience11(5),351-360(2010).
20. P. Oudeyer, G. Kachergis, W. Schueller, Computational and robotic models of early lan-
guagedevelopment: Areview,CoRRabs/1903.10246(2019).
21. S.Harnad,Thesymbolgroundingproblem,PhysicaD:NonlinearPhenomena42,335–346
(1990).
36
22. M. Tomasello, The social bases of language acquisition, Social development 1, 67–87
(2006).
23. M. Tomasello, First verbs: A case study of early grammatical development (Cambridge
UniversityPress,1992).
24. L.B.Smith,S.Jayaraman,E.Clerkin,C.Yu,TheDevelopingInfantCreatesaCurriculum
forStatisticalLearning,TrendsinCognitiveSciences22,325–336(2018).
25. L. Raggioli, A. Cangelosi, Embodied attention in word-object mapping: A developmen-
tal cognitive robotics model, 2022 IEEE International Conference on Development and
Learning(ICDL)pp.156–163(2022).
26. Y. Sugita, J. Tani, Learning Semantic Combinatoriality from the Interaction between Lin-
guisticandBehavioralProcesses,AdaptiveBehavior 13,33–52(2005).
27. A. Cangelosi, et al., Integration of action and language knowledge: A roadmap for de-
velopmentalrobotics,IEEETransactionsonAutonomousMentalDevelopment 2,167–195
(2010).
28. A. Cangelosi, F. Stramandinoli, A review of abstract concept learning in embodied agents
androbots,Phil.Trans.R.Soc.B(2018).
29. S.Heinrich,S.Wermter,Interactivenaturallanguageacquisitioninamulti-modalrecurrent
neuralarchitecture,ConnectionScience30,99–133(2018).
30. A. Akakzia, C. Colas, P. Y. Oudeyer, M. Chetouani, O. Sigaud, Decstr: Learning goal-
directed abstract behaviors using pre-verbal spatial predicates in intrinsically motivated
agents,Proc.ofICLR(2021).
37
31. T. Yamada, H. Matsunaga, T. Ogata, Paired recurrent autoencoders for bidirectional trans-
lation between robot actions and linguistic descriptions, IEEE Robotics and Automation
Letters3,3441–3448(2018).
32. R. P. Rao, D. H. Ballard, Predictive coding in the visual cortex: a functional interpretation
ofsomeextra-classicalreceptive-fieldeffects,Natureneuroscience2,79(1999).
33. K. J. Friston, S. Kiebel, Predictive coding under the free-energy principle, Philosophical
transactionsoftheRoyalSocietyB:Biologicalsciences364,1211–1221(2009).
34. K. J. Friston, J. Daunizeau, S. J. Kiebel, Reinforcement learning or active inference?, PloS
one4,e6421(2009).
35. K. J. Friston, J. Daunizeau, J. Kilner, S. J. Kiebel, Action and behavior: a free-energy
formulation,Biologicalcybernetics102,227–260(2010).
36. H. Brown, K. J. Friston, S. Bestmann, Active inference, attention, and motor preparation,
Frontiersinpsychology2,218(2011).
37. T. Matsumoto, J. Tani, Goal-Directed Planning for Habituated Agents by Active Inference
UsingaVariationalRecurrentNeuralNetwork.,Emtropy(2020).
38. T. Matsumoto, W. Ohata, F. Benureau, J. Tani, Goal-Directed Planning and Goal Under-
standing by Extended Active Inference: Evaluation through Simulated and Physical Robot
Experiments,Entropy(2022,24,469).
39. J. Queißer, M. Jung, T. Matsumoto, J. Tani, Emergence of content-agnostic information
processing by a robot using active inference, visual attention, working memory, and plan-
ning.,NeuralComput.(2021Aug19;33(9):2353-2407).
38
40. S.R.Sehon,Goal-directedactionandteleologicalexplanation,CausationandExplanation
pp.155–170(2007).
41. G. Csibra, S. B´ıro´, O. Koo´s, G. Gergely, One-year-old infants use teleological representa-
tionsofactionsproductively,CognitiveScience27,111–133(2003).
42. A. Ahmadi, J. Tani, A novel predictive-coding-inspired variational rnn model for online
predictionandrecognition,Neuralcomputation31,2025–2074(2019).
43. S. Bernardi, et al., The geometry of abstraction in the hippocampus and prefrontal cortex,
Cell 183,954-967.e21(2020).
44. T. Ito, et al., Compositional generalization through abstract representations in human and
artificialneuralnetworks,AdvancesinNeuralInformationProcessingSystems35,32225–
32239(2022).
45. J.Tani,M.Ito,Self-organizationofbehavioralprimitivesasmultipleattractordynamics: A
robot experiment, IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems
andHumans33,481–488(2003).
46. J. Tani, M. Ito, Y. Sugita, Self-organization of distributedly represented multiple behavior
schemata in a mirror system: Reviews of robot experiments using RNNPB, Neural Net-
works17,1273-1289(2004).
47. B. Scho¨lkopf, A. Smola, K.-R. Mu¨ller, Nonlinear Component Analysis as a Kernel Eigen-
valueProblem,NeuralComputation10,1299-1319(1998).
48. B. L. Welch, The generalization of ’student’s’ problem when several different population
variancesareinvolved,Biometrika34,28-35(1947).
39
49. T.Taniguchi,Collectivepredictivecodinghypothesis: Symbolemergenceasdecentralized
bayesianinference,PsyArXiv(2023).
50. N. Chomsky, Poverty of stimulus: Unfinished business, Studies in Chinese linguistics 33,
3–16(2012).
51. W.Ohata,J.Tani,Investigationofthesenseofagencyinsocialcognition,basedonframe-
works of predictive coding and active inference: A simulation study on multimodal imita-
tiveinteraction,FrontiersinNeurorobotics14,61(2020).
52. A. Radford, et al., Learning transferable visual models from natural language supervision,
arXiv:2103.00020(2021).
53. F. A. Galatolo, M. G. C. A. Cimino, G. Vaglini, Generating images from caption and vice
versa via clip-guided generative latent space search, International Conference on Image
ProcessingandVisionEngineering(2021).
54. J.-B.Alayrac,etal.,Flamingo: avisuallanguagemodelforfew-shotlearning,Advancesin
NeuralInformationProcessingSystems35,23716–23736(2022).
55. M. Ahn, et al., Do as i can, not as i say: Grounding language in robotic affordances,
arXiv:2204.01691(2022).
56. D. Driess, et al., Palm-e: An embodied multimodal language model, arXiv:2303.03378
(2023).
57. A. Brohan, et al., Rt-2: Vision-language-action models transfer web knowledge to robotic
control,arXiv:2307.15818(2023).
58. D. J. Chalmers, The conscious mind: In search of a fundamental theory (Oxford Paper-
backs,1997).
40
59. G. Marcus, E. Davis, Insights for ai from the human mind, Commun. ACM 64, 38–41
(2020).
60. G.Pezzulo,T. Parr,P. Cisek,A. Clark,K. J.Friston,Generating meaning: activeinference
andthescopeandlimitsofpassiveai.,TrendsCognSci.28(2),97-112(2024Feb).
61. T.Yoshida,A.Masumori,T.Ikegami,Fromtexttomotion: Groundinggpt-4inahumanoid
robot”alter3”,arXivpreprint,arXiv:2312.06571(2023).
62. M. Jaderberg, K. Simonyan, A. Zisserman, k. kavukcuoglu, Spatial transformer networks,
AdvancesinNeuralInformationProcessingSystems28(2015).
63. F. Shibata Alnajjar, Y. Yamashita, J. Tani, The hierarchical and functional connectivity
of higher-order cognitive mechanisms: neurorobotic model to investigate the stability and
flexibilityofworkingmemory,FrontiersinNeurorobotics7(2013).
64. C. Bishop, Pattern Recognition and Machine Learning (Information Science and Statistics
(Berlin,Heidelberg: Springer-Verlag,2006).
65. K. Diederik P., B. Jimmy, Adam: A method for stochastic optimization, arXiv:1412.6980
(2017).
66. R. Pascanu, T. Mikolov, Y. Bengio, On the difficulty of training recurrent neural networks,
Proceedings of the 30th International Conference on Machine Learning 28, 1310–1318
(2013).
41
Acknowledgments
ThisworkwassupportedbyfundingfromOkinawaInstituteofScienceandTechnology(OIST)
Graduate University. JT was partially supported by the Japan Society for the Promotion of
Science (JSPS) KAKENHI, Transformative Research Area (A): unified theory of prediction
and action [24H02175]. The authors are grateful for the help and support provided by the lab
members in the Cognitive Neurorobotics Research Unit and the Scientific Computing section
of Research Support Division at OIST. We thank Takazumi Matsumoto for providing the base
codefordatacollectionandStevenAirdforeditingthemanuscript.
42
Supplementary Materials
MaterialsandMethods
Supplementarytext
Figs. S1toS11
TablesS1toS6
AlgorithmsforLearningandInference
Materials and Methods
Thecodeusedforexperimentsisavailableat: https://github.com/oist-cnru/FEP-
based-model-of-Embodied-Language.git. Access to the data will also be made
availablethroughtherepository.
TrainingDataAcquisition
We collected RGB video data from a fixed camera and joint angle trajectories from the robotic
arm (Torobo Arm; Tokyo Robotics Inc.), shown in Figure S1, with 7 degrees of freedom. Ro-
tation of the end effector is not used in these experiments, so it is omitted. The actuator is
programmed in the joint-space position control in order to perform different object manipula-
tiontasksonobjectsplacedatrandompositionsintheworkspace. Trajectoriesgeneratedfrom
the programmed controller are recorded to train the model. Object positions are sampled from
aworkspaceof10x10and8x8gridfortrainingandtestingdata,respectively. Trajectorieswere
recorded at 20 Hz and down-sampled to reduce computational costs. There was approximately
a 10% variation in the sequence lengths of the collected data. Video data were down sampled
to sequence of 64x64 RGB data. After pre-processing, the resulting visuo-proprioceptive se-
quenceswere50±3timestepsforalltrajectories. Therecordeddatasetcontains400sequences
43
(10sequencesforeachnoun-verbcombination),andwitheachsequencegeneratedfromdiffer-
ent initial object positions. Volumes of training and testing data differ based on the number of
compositional elements in each group. These are separated into four groups; Group A (5x8) is
a permutation of 5 nouns and 8 verbs for a total of 40 combinations, Group B (5x6) is a per-
mutationof5nounsand5verbsforatotalof30combinations,GroupC(5x3)isapermutation
of 5 nouns and 3 verbs for a total of 15 combinations, and Group D (3x3) is a permutation of
3 nouns and 3 verbs for a total of 9 combinations. These combinations were trained with three
degreesofsparsitytoevaluatetherobustnessofthemodel.
ImplementationDetails
The associative network, which integrates the vision, proprioception, and language modules,
contains a single PV-RNN layer with 256 neurons and 20 stochastic units zp. The posterior
units zq have the same dimensions as zp. The language network consists of a single layer of
LSTM with 40 neurons and a 10 dimensional parametric bias. The proprioceptive and visual
pathways of the model are based on three layers of multi-layer LSTM cells and convLSTM
cells, respectively. The multi-layer convLSTM contain 16, 32, and 64 feature maps from the
lowest to the highest layer. To project features to the next higher layer, a convolutional kernel
of size 5x5, stride 2x2, and padding size of 2x2 were used. A deconvolutional kernel size of
6x6,stride2x2,andpaddingof2x2wereusedtoprojectfeaturestotheconsequentlowerlayer.
For lateral connections from hidden states of the proprioceptive pathway to the visual pathway
and the projection from the associative LSTM, convolutional kernel sizes are selected in such
way as to match the feature dimensionality of the corresponding layer of the visual pathway.
Forward transformation of the attention transformer down-scales the resolution by a factor of
0.625 (64x64 to 40x40 pixels), to fit the size of prerecorded datasets. The multi-layer LSTM
oftheproprioceptivepathwaycontains256,128,and64neuronsfromthelowesttothehighest
44
layer. The proprioceptive pathway for prediction of proprioception and parameterization of
the attention modules was performed by a multilayer perceptron (MLP) with one hidden layer
of 256 neurons. Layer normalization and rectified linear unit (ReLU) activation functions are
utilized. The model was trained and evaluated with five random seeds (0, 5, 10, 15 and 20) for
allexperimentalevaluations.
TrainingbyminimizingFreeEnergy
Trainingofthemodelwasdonebyminimizingfreeenergy. Foragenerativemodelp (X),free
θ
energyisdefinedas:
F = −E [lnp (X|z)]+D [q (z|X)||p(z)], (32)
qφ(z) θ KL φ
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
a)Accuracy b)Complexity
with latent variable z, observations X and model parameters φ and θ. The accuracy term
(a)istheexpectedlikelihoodofobservedsensationsX,givenz. Thiscanalsobeinterpretedas
thereconstructionerrorbetweenpredictedsensationandgroundtruth. Thecomplexityterm(b)
facilitatesregularizationofthemodelbyminimizationoftheKullback-Leibler(KL)divergence
between the approximate posterior q (z|X) and prior p(z). The evidence free energy F can be
φ
minimizedwithrespecttotheposteriordistributionq (z)as:
φ
q (z) = argminF (33)
φ
qφ(z)
Negative free energy is also known as an evidence lower bound in machine learning literature
(64). Therefore, minimizing free energy is equivalent to maximizing the evidence or marginal
likelihood for a generative model. In the current model, during learning the free energy is
minimized by updating model parameters that include network weights, biases, the parametric
biasPB,andtheadaptivelatentvariableA .
t:T
45
Training of the model was done by minimizing the loss function in equation 31, performed
usingtheADAMoptimizer(65). OptimizationofparametricbiasPB,adaptivelatentvariables
A, weights, and biases was performed for over 5000 epochs, until convergence of learning.
The learning rate was set to 5×10−4 and the hyper-parameter w was set to 0.05. The binding
coefficientkforthebindinglossbetweenpredictedP(cid:103)BandlanguageparametricbiasPBisset
to 100. To prevent instability during training, i.e., exploding gradient problem, we performed
gradient clipping (66), which re-scales gradients based on the ℓ -norm in case the norm of the
2
gradientsexceeds0.2. Themeanandstandarddeviationoftheprioratfirsttimestepwassetto
0and1,respectively.
LinguisticallySpecifiedGoal-DirectedPlanningbyActiveInference
The trained model is evaluated via active inference. More specifically, a sequence of action is
inferredbyminimizingtheexpectedfreeenergyG forthefutureconsideringpossibleeffectsof
actionsaappliedtotheenvironment. TheexpectedfreeenergyG isdefinedas:
G = −E [lnp (X(a)|z)]+D [q (z)||p(z)], (34)
qφ(z) θ KL φ
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
a)Accuracyinfuture b)Complexity
X is the preferred sensation given as a function of action a. The model performs goal-
directed planning using active inference (Figure S10), by minimizing the expected free energy
of a goal state represented by language, e.g. ”put red on green .”, that is associated with the
corresponding visuo-proprioceptive sequences. The expected free energy G can be minimized
withrespecttotheposteriordistributionq (z)as:
φ
q (z) = argminG (35)
φ
qφ(z)
Thetrainedmodelwasevaluatedforitsabilitytosuccessfullygenerategoal-directedvisuo-
proprioceptiveplanstoachievelinguisticallyspecifiedgoalsbymeansofactiveinference. Plan-
46
ning of action for previously unseen object position (U-P) as well as unlearned language com-
position (U-C) was done by minimizing the expected free energy defined in equation 30. In
order to update the posterior latent variables (A) and parametric bias (PB) at each epoch, the
visuo-proprioceptivesequencewasgenerated5timesbyrepeatedstochasticsamplingofA,the
mean and variance of which were inferred. The values of A and linguistic PB that result in
the smallest planning error (Lg) after 50 iterations of inference was selected as the final result
of the planning process for the given goal. The visuo-proprioceptive sequences generated from
thefinallatentstatesobtainedwereconsideredthefinalplangeneratedbythemodel.
LanguageInferencefromObservedVisuo-ProprioceptiveSequences
The model’s ability to understand given visuo-proprioceptive behaviors by inferring corre-
spondinglinguisticallyrepresentedgoalswasevaluated. Inferenceoflinguisticallyrepresented
goals was done by minimizing the free energy (See equation 31). In the case of inferring lin-
guisticallyrepresentedgoals,themodelgeneratedthelinguisticdescription5timesbyrepeated
samplingofposteriorlatentvariables(A). Themodelupdatestheposteriorlatentvariables(A)
and parametric bias (PB) at each iteration of the inference process. Values of A and linguistic
PB that result in accurate prediction of linguistic goals after 50 iterations of inference were
selectedasthefinalresultofthisinferenceprocessforagivenvisuo-proprioceptivesequence.
S1: Implementation of Focused Vision Loss
The vision loss catt performs a scaling of the error signal with respect to the distance from the
t
centerofthefocusedregion. Ourpreviousstudy(39)showedthatthisbalancesthecontribution
offocusedandunfocusedregionsinthevisualprediction. catt iscomputedas:
t
catt = α
(cid:18)(cid:20)
α
t
a
,
t
3
t
(cid:21)
−
(cid:20)
i/
N in
(cid:21)(cid:19)2
, (36)
t,ij s αatt j/
N
t,4 in
47
for image resolution (N ,N ) in order to scale the visual error signal relative to the distance
in in
fromthecenterofthefocalregion.
S2: Variation of Task Compositions in Training
Weusedifferentnumbersofcombinationstostudythemodel’sgeneralizationcapability. These
are divided into four groups. Group A is a combination of 5 nouns and 8 verbs. Figure S3(A)
represents group A1, which used 32/40 (80%) combinations to train the model. For example
thecombinationofthenoun”Green”andtheverb”moveleft”(readsas”movegreenleft.”)is
usedasatestsequence;therefore,itwasnotlearnedbythemodelduringtraining.
Figures S3(B) & (C) show group A2 (60%) & A3 (40%), respectively. Figures S4 (A),
(B) and (C) show the compositions used in group B1 (80%), B2 (60%) and B3 (40%); figures
S5(A), (B) and (C) show the compositions used in group C1 (80%), C2 (60%) and C3 (40%),
and figures S5(D), (E) and (F) show the compositions used in group D1 (7/9, 77%), D2 (6/9,
66%)andD3(3/9,33%).
S3: Results
Hereweshowcasetwosequencesgeneratedbythemodel. FigureS6showsthevisuo-proprioceptive
sequence successfully generated by the model to achieve the linguistically specified goal ”put
red on green .”. Figure S7 shows the visuo-proprioceptive sequence generated by the model,
whereitfailedtoachievethelinguisticallyspecifiedgoal”putblueonyellow.”.
Statisticalanalysisofmodelperformance
WeperformedWelch’sunequalvariancesT-test(48)tocomparethegeneralizationperformance
between groups. Welch’s T-test was chosen due to the small sample size for training and eval-
uation. We used a sample size of 5 (number of random seeds used for sampling) and assumed
unequalvarianceforperformingtheanalyses. TheresultsshowthatgroupA1(5x8,80%),with
48
the highest number of combinations, performs significantly better than other groups. Table S3
and S4 show a pairwise comparison of the different groups in inferring visuo-proprioceptive
trajectories to achieve linguistically represented goals, with training ratios of 80% and 60% ,
respectively.
We also compared the generalization performance for the task of inferring linguistically
representedgoalsfromobservedvisuo-proprioceptivesequences. Eventhoughtheoverallgen-
eralizationperformancewaspoor,weobservedanincreaseinperformancewhenthenumberof
compositionalelementsintrainingwasincreased. TableS5andS6showapairwisecomparison
ofthedifferentgroupswith80%and60%trainingratios,respectively.
S4: Graphical Description of the Inference Process
Figure S10 illustrates the inference of latent variables for inferring visuo-motor sequences that
achieve given linguistically represented goals. Figure S11 illustrates the inference of latent
variables for inferring linguistically represented goals from observed visuo-proprioceptive se-
quences.
49
Figure S1: Experimental setup: Torobo arm (Tokyo Robotic Inc.) with 7 degrees of freedom
manipulating5cmcubicblocksofdifferentcolorsintheworkspace.
50
FigureS2: Samplevisualandcorrespondingnormalizedproprioceptivetrajectoriesofeachtype
oftaskusedintheexperiment. ToptoBottom: ”graspyellow”,”moveblueleft”,”movepurple
right”, ”move green front”, ”move red back”, ”put yellow on blue”, ”put red on green”, ”put
blueonyellow”.
51
FigureS3: DatacompositioninGroupA(5x8). (A)GroupA1(5x8,80%),(B)GroupA2(5x8,
60%),and(C)GroupA3(5x8,40%)
52
FigureS4: DatacompositioninGroupB(5x6). (A)GroupB1(5x6,80%),(B)GroupB2(5x6,
60%),and(C)GroupB3(5x6,40%)
53
Figure S5: Data composition in Group C and D (5x3 and 3x3). (A) Group C1 (5x3, 80%), (B)
Group C2 (5x3, 60%), (C) Group C3 (5x3, 40%), (D) Group D1 (3x3, 77%), (E) Group D2
(3x3,66%),and(F)GroupD3(3x3,33%)
54
Figure S6: Visuo-Proprioceptive sequence generated by the model to achieve the linguistically
representedgoalof”putredongreen.”. (A):imaginedvisualtrajectorygeneratedbythemodel;
(B): the ground truth target; (C): difference between predicted and visual sequence and ground
truth, (D): normalized joint angle trajectory predicted by the model compared with the cor-
responding ground truth; (E): the mean difference between the predicted joint angles and the
groundtruth.
55
Figure S7: Visuo-Proprioceptive sequence generated by the model to achieve the linguistically
represented goal of ”put blue on yellow.”. (A): imagined visual trajectory generated by the
model; (B): the ground truth target; (C): difference between predicted and visual sequence and
ground truth, (D): normalized joint angle trajectory predicted by the model compared with the
correspondinggroundtruth;(E):themeandifferencebetweenthepredictedjointanglesandthe
groundtruth.
56
90
80
70
60
50
40
30
GroupA(5x8) GroupB(5x6) GroupC(5x3) GroupD(3x3)
Groups
)%(
etar
sseccuS
80%
60%
40%
FigureS8: Comparisonofgeneralizationperformance,forinferenceoflinguisticgoals,forun-
learned compositions (U-C) among groups with different number of compositions and training
ratios.
57
Figure S9: Scatterplot of Kernel PCA of the latent state PB vector for all groups with highest
trainingratio. (A):GroupA1(5x8,80%)(B):GroupB1(5x6,80%)(C):GroupC1(5x3,80%)
(D): Group D1 (3x3, 77%). U-C refers to unlearned compositions that were used for testing.
Colors of markers indicate colors of objects being manipulated. The variance explained by the
twocomponentsofKPCAforallgroupswasgreaterthan90%
58
Figure S10: Graphical description of goal-directed planning using active inference to infer
visuo-proprioceptivesequencesgivenlinguisticallyrepresentedgoal. Parametersinredtextare
updatedinordertominimizethepredictionerror
59
Figure S11: Graphical description of inferring linguistically represented goal from observed
visuo-proprioceptive sequences. Parameters in red text are updated in order to minimize the
predictionerror
60
TableS1: Accuracy: InferenceofVisuo-ProprioceptiveSequences
Visuo-proprioceptiveError(µ±SD)%
U-P U-C
GroupA1(5x8,80%) 0.0306±0.0016 0.0361±0.0049
GroupA2(5x8,60%) 0.0331±0.0017 0.0500±0.0021
GroupA3(5x8,40%) 0.0342±0.0018 0.0710±0.0080
GroupB1(5x6,80%) 0.0328±0.0015 0.0494±0.0040
GroupB2(5x6,60%) 0.0334±0.0017 0.0597±0.0065
GroupB3(5x6,40%) 0.0346±0.0019 0.0890±0.0054
GroupC1(5x3,80%) 0.0346±0.0018 0.0615±0.0031
GroupC2(5x3,60%) 0.0351±0.0021 0.0791±0.0059
GroupC3(5x3,40%) 0.0395±0.0010 0.1071±0.0057
GroupD1(3x3,77%) 0.0341±0.0018 0.0655±0.0053
GroupD2(3x3,66%) 0.0344±0.0019 0.0896±0.0054
GroupD3(3x3,33%) 0.0405±0.0021 0.1226±0.0093
Algorithm1Training
Initializeparameters:
θ ,Ad,PBd;∀ ∈ D
net d train
fore ← 1toN do
epochs
fort ← 1toTdo
sampleposterior:
zq ← µq +ϵ∗σq
t t t
generation:
(v
(cid:101)t
,m
(cid:101)t
,S(cid:101),P(cid:103)B
t
) ←−
Fwd(v
(cid:101)t−1
,m
(cid:101)t−1
,S(cid:101),PBd,zq
t
,θ
net
)
end
computeloss:
d
l ← L(v
(cid:101)
,m
(cid:101)
,S(cid:101),P(cid:103)B ,v,m,S,PBd,)
gradientdescent:
(θ ,Ad,PB) ← Adam(∂ l,∂ l,∂ l);
net θ A PB
end
61
TableS2: Accuracy: InferenceofLinguisticallyRepresentedGoals
Successrate(µ±SD)%
U-P U-C
GroupA1(5x8,80%) 72.50±0.71 71.00±1.74
GroupA2(5x8,60%) 69.16±0.69 67.87±1.51
GroupA3(5x8,40%) 70.00±1.42 51.66±0.88
GroupB1(5x6,80%) 70.00±0.74 61.66±2.78
GroupB2(5x6,60%) 70.55±0.99 60.16±1.32
GroupB3(5x6,40%) 68.33±0.91 47.88±1.11
GroupC1(5x3,80%) 71.66±1.39 62.00±3.06
GroupC2(5x3,60%) 71.11±0.99 59.00±2.38
GroupC3(5x3,40%) 68.33±2.78 46.22±1.49
GroupD1(3x3,77%) 71.42±2.02 59.00±1.67
GroupD2(3x3,66%) 71.66±1.82 56.00±1.52
GroupD3(3x3,33%) 70.00±2.98 41.00±2.92
TableS3: Welch’sT-test: InferenceofVisuo-ProprioceptiveSequences,TrainingRatio: 80%
Group1 Group2 T-statistic P-value
A1 B1 -7.43 9.03x10−5
A1 C1 -15.48 1.56x10−6
A1 D1 -14.48 5.60x10−7
B1 C1 -8.45 4.19x10−5
B1 D1 -8.57 4.09x10−5
C1 D1 -2.30 5.78x10−2
TableS4: Welch’sT-test: InferenceofVisuo-ProprioceptiveSequences,TrainingRatio: 60%
Group1 Group2 T-statistic P-value
A2 B2 -5.02 4.44x10−3
A2 C2 -16.42 1.50x10−5
A2 D2 -24.16 2.00x10−6
B2 C2 -7.81 5.40x10−5
B2 D2 -12.50 2.00x10−6
C2 D2 -4.64 1.69x10−3
62
TableS5: Welch’sT-test: InferenceofLinguisticallyRepresentedGoal,TrainingRatio: 80%
Group1 Group2 T-statistic P-value
A1 B1 10.06 2.6x10−5
A1 C1 9.03 7.4x10−5
A1 D1 17.59 1.1x10−7
B1 C1 -0.29 7.7x10−1
B1 D1 2.89 2.4x10−2
C1 D1 3.04 2.1x10−2
TableS6: Welch’sT-test: InferenceofLinguisticallyRepresentedGoal,TrainingRatio: 60%
Group1 Group2 T-statistic P-value
A2 B2 13.59 9.73x10−7
A2 C2 11.12 1.34x10−5
A2 D2 17.59 1.13x10−7
B2 C2 1.50 1.80x10−1
B2 D2 7.30 9.20x10−5
C2 D2 3.75 7.50x10−3
Algorithm2Inferenceofvisuo-motorsequence
Initializeparameters:
PBg,Ag;∀ ∈ D
g test
fore ← 1toN do
iterations
fort ← 1toTdo
sampleposterior:
zq ← µq +ϵ×σq
t t t
generation:
(v
(cid:101)t
,m
(cid:101)t
,P(cid:103)B
t
,S(cid:101)) ←−
Fwd(v
(cid:101)t−1
,m
(cid:101)t−1
,S(cid:101),PBg,zq
t
,θ
net
)
end
losscalculation:
g
lg ← Lg(S(cid:101)g,P(cid:103)B ,Sg,PBg)
gradientdescent:
(Ag,PBg) ← Adam(∂ lg,∂ lg);
A PB
end
Choosebestsolutionfromalliterations
63
Algorithm3Inferenceoflinguisticgoals
Initializeparameters:
PBg,Ag;∀ ∈ D
g test
fore ← 1toN do
iterations
fort ← 1toTdo
sampleposterior:
zq ← µq +ϵ×σq
t t t
generation:
(v
(cid:101)t
,m
(cid:101)t
,P(cid:103)B
t
,S(cid:101)) ←−
Fwd(v
(cid:101)t−1
,m
(cid:101)t−1
,S(cid:101),PBg,zq
t
,θ
net
)
end
losscalculation:
g
lg ← Lg(v
(cid:101)
,m
(cid:101)
,P(cid:103)B ,v,m,PBg)
gradientdescent:
(Ag,PBg) ← Adam(∂ lg,∂ lg);
A PB
end
Choosebestsolutionfromalliterations
64

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Development of Compositionality and Generalization through Interactive Learning of Language and Action of Robots"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
