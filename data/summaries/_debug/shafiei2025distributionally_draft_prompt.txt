=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Distributionally Robust Free Energy Principle for Decision-Making
Citation Key: shafiei2025distributionally
Authors: Allahkaram Shafiei, Hozefa Jesawada, Karl Friston

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Key Terms: distributionally, principle, decision, energy, despite, agents, robust, natural, training, performance

=== FULL PAPER TEXT ===

Distributionally Robust Free Energy Principle for Decision-Making
Allahkaram Shafiei 1,∗ Hozefa Jesawada 2,∗ Karl Friston 3 Giovanni Russo 4 B
November 25, 2025
Abstract. Despite their groundbreaking performance, autonomous agents can misbehave when
training and environmental conditions become inconsistent, with minor mismatches leading to undesir-
ablebehaviorsorevencatastrophicfailures. Robustnesstowardsthesetraining-environmentambiguities
is a core requirement for intelligent agents and its fulfillment is a long-standing challenge towards their
real-world deployments. Here, we introduce a Distributionally Robust Free Energy model (DR-FREE)
thatinstillsthiscorepropertybydesign. Combiningarobustextensionofthefreeenergyprinciplewith
a resolution engine, DR-FREE wires robustness into the agent decision-making mechanisms. Across
benchmark experiments, DR-FREE enables the agents to complete the task even when, in contrast,
state-of-the-art models fail. This milestone may inspire both deployments in multi-agent settings and,
at a perhaps deeper level, the quest for an explanation of how natural agents – with little or no training
– survive in capricious environments.
Introduction
A popular approach to designing autonomous agents is to feed them with data, using Reinforcement
Learning (RL) and simulators to train a policy (Fig. 1a). Deep RL agents designed on this paradigm have
demonstrated remarkable abilities, including outracing human champions in Gran Turismo,1 playing Atari
games,2 controlling plasmas3 and achieving champion-level performance in drone races.4 However, despite
their groundbreaking performance, state-of-the-art agents cannot yet compete with natural intelligence
in terms of policy robustness: natural agents have, perhaps through evolution, acquired decision-making
abilities so that they can function in challenging environments despite little or no training.6–8 In contrast,
for artificial agents, even when they have access to a high fidelity simulator, the learned policies can be
brittletomismatches,orambiguities,betweenthemodelavailableduringlearningandtherealenvironment
(Fig. 1b). For example, drone-champions and Atari-playing agents assume consistent environmental
conditions from training, and if this assumption fails, because, e.g., the environment illumination or color
of the objects changes, or the drone has a malfunctioning – so that its dynamics becomes different from
the one available during training – learned policies can fail. More generally, model ambiguities – even if
minor – can lead to non-robust behaviors and failures in open-world environments.9 Achieving robustness
towardsthesetraining/environmentambiguitiesisalong-standingchallenge10–12 forthedesignofintelligent
machines6,13,14 that can operate in the real world.
1 CzechTechnicalUniversity,Prague,CzechRepublic. 2 NewYorkUniversityAbuDhabi,AbuDhabiEmirate. 3 Wellcome
Centre for Human Neuroimaging, Institute of Neurology, University College London, United Kingdom. 4 Department of
Information and Electrical Engineering and Applied Mathematics, University of Salerno, Italy.
∗ These authors contributed equally. B e-mail: giovarusso@unisa.it
1
5202
voN
42
]IA.sc[
3v32231.3052:viXra
Here we present DR-FREE, a free energy15,16 computational model that addresses this challenge: DR-
FREE instills this core property of intelligence directly into the agent decision-making mechanisms. This
is achieved by grounding DR-FREE in the minimization of the free energy, a unifying account across
information theory, machine learning,17–23 neuroscience, computational and cognitive sciences.24–30 The
principle postulates that adaptive behaviors in natural and artificial agents arise from the minimization
of variational free energy (Fig. 1c). DR-FREE consists of two components. The first component is an
extension of the free energy principle: the distributionally robust (DR) free energy (FREE) principle,
which fundamentally reformulates how free energy minimizing agents handle ambiguity. While classic
free energy models (Fig. 1c) obtain a policy by minimizing the free energy based on a model of the
environment available to the agent, under our robust principle, free energy is instead minimized across
all possible environments within an ambiguity set around a trained model. The set is defined in terms of
statistical complexity around the trained model. This means the actions of the agent are sampled from a
policythatminimizesthemaximumfreeenergyacrossambiguities. Therobustprincipleyieldstheproblem
statementforpolicycomputation. Thisisadistributionallyrobustproblemhavingafreeenergyfunctional
as objective and ambiguity constraints formalized in terms of statistical complexity. The problem has not
only a nonlinear cost functional with nonlinear constraints but also probability densities over decision
variables that equip agents with explicit estimates of uncertainty and confidence. The product of this
framework is a policy that minimizes free energy and is robust across model ambiguities. The second key
component of DR-FREE – its resolution engine – is the method to compute this policy. In contrast to
conventional approaches for policy computation based on free energy models, our method shows that the
policy can be conveniently found by first maximizing the free energy across model ambiguities – furnishing
a cost under ambiguity – and then minimizing the free energy in the policy space (Fig. 1d). Put simply,
policies are selected under the best worst-case scenario, where the worst cases accommodate ambiguity.
Our robust free energy principle yields – when there is no ambiguity – a problem statement for policy
computation naturally arising across learning22,31,32 – in the context of maximum diffusion (MaxDiff) and
maximum entropy (MaxEnt) – and control.46 This means that DR-FREE can yield policies that not only
inherit all the desirable properties of these approaches but ensures them across an ambiguity set, which is
explicitly defined in the formulation. In MaxEnt – and MaxDiff – robustness depends on the entropy of
the optimal policy, with explicit bounds on the ambiguity set over which the policy is robust available in
discrete settings.32 To compute a policy that robustly maximizes a reward, MaxEnt needs to be used with
a different, pessimistic, reward32 – this is not required in DR-FREE. These desirable features of our free
energy computational model are enabled by its resolution engine. This is – to the best of our knowledge
– the only available method tackling the full distributionally robust, nonlinear and infinite-dimensional
policy computation problem arising from our robust principle; see Results and Sec. S2 in Supplementary
Information for details. In Supplementary Information we also highlight a connection with the Markov
Decision Processes (MDPs) formalism. DR-FREE yields a policy with a well-defined structure: this is a
soft-max with its exponent depending on ambiguity. This structure elucidates the crucial role of ambiguity
on optimal decisions, i.e., how it modulates the probability of selecting a given action.
DR-FREE not only returns the policy arising from our free energy model, but also establishes its per-
formance limits. In doing so, DR-FREE harbors two implications. First, DR-FREE policy is interpretable
and supports (Bayesian) belief updating. The second implication is that it is impossible for an agent faced
with ambiguity to outperform an ambiguity-free agent. As ambiguity vanishes, DR-FREE recovers the
policy of an agent that has perfect knowledge of its environment, and no agent can obtain better per-
formance. Vice-versa, as ambiguity increases, DR-FREE shows that the policy down-weights the model
2
available to the agent over ambiguity.
We evaluate DR-FREE on an experimental testbed involving real rovers, which are given the task
of reaching a desired destination while avoiding obstacles. The trained model available to DR-FREE is
learned from biased experimental data, which does not adequately capture the real environment and
introduces ambiguity. In the experiments – even despite the ambiguity arising from having learned a
model from biased data – DR-FREE successfully enables the rovers to complete their task, even in settings
where both a state-of-the-art free energy minimizing agent and other methods struggle, unable to complete
the task. The experiments results – confirmed by evaluating DR-FREE in a popular higher dimensional
simulated environment – suggest that, to operate in open environments, agents require built-in robustness
mechanisms and these are crucial to compensate for poor training. DR-FREE, providing a mechanism
that defines robustness in the problem formulation, delivers this capability.
Our free energy computational model, DR-FREE, reveals how free energy minimizing agents can com-
puteoptimalactionsthatarerobustoveranambiguitysetdefinedintheproblemformulation. Itestablishes
a normative framework to both empower the design of artificial agents built upon free energy models with
robust decision-making abilities, and to understand natural behaviors beyond current free energy explana-
tions.35–39 Despite its success, there is no theory currently explaining if and how these free energy agents
can compute actions in ambiguous settings. DR-FREE provides these explanations.
Results
DR-FREE. DR-FREE comprises a distributionally robust free energy principle and the accompanying
resolution engine – the principle (Fig. 2a) is the problem statement for policy computation; the resolution
engine is the method for policy computation. The principle establishes a sequential policy optimization
framework, where randomized policies arise from the minimization of the maximum free energy over ambi-
guity. The resolution engine finds the solution in the space of policies. This is done by computing – via the
maximum free energy over all possible environments in the ambiguity set – a cost associated to ambiguity.
Then, the ensuing maximum free energy is minimized in policy space (Fig. 1d).
In Fig. 1a, random variables X k−1 and U k are state at time k−1 and action at k (see Methods and Sec.
S3ofSupplementaryInformation). Theagentinfersthestateofastochasticenvironmentp k( x k ∣ x k−1 ,u k)
and the action is sampled from π k( u k ∣ x k−1) . In the decision horizon (e.g., from 1 to N) the agent-
environmentinteractionsarecapturedbyp 0∶N ,definedasp 0( x 0) ∏N k=1 p k( x k ∣ x k−1 ,u k) π k( u k ∣ x k−1) ,where
p 0( x 0) isaninitialprior. Thetrainedmodelavailabletotheagent, p¯ k( x k ∣ x k−1 ,u k) , doesnot(necessarily)
match its environment and the goal of DR-FREE is to compute a policy that, while optimal, is robust
against the training/environment ambiguities. For example, if – as in our experiments – the robot model
of Fig. 1a is learned from corrupted/biased data, then it differs from the real robot and this gives rise to
ambiguities. These ambiguities are captured via the ambiguity set of Fig. 1b. For a given state/action
pair, this is the set of all possible environments with statistical complexity within η k( x k−1 ,u k) from the
trained model. For this reason, the ambiguity set B η( p¯ k( x k ∣ x k−1 ,u k)) is captured via the Kullback-
Leibler (KL) divergence: the ambiguity set is the set of all possible models, say p k( x k ∣ x k−1 ,u k) , such that
D KL( p k( x k ∣ x k−1 ,u k)∣∣ p¯ k( x k ∣ x k−1 ,u k)) ≤η k( x k−1 ,u k) . TheradiusofambiguityavailabletoDR-FREE,
η k( x k−1 ,u k) , is positive and bounded. For a state/action pair, a small radius indicates low ambiguity,
meaning that the agent is confident in its trained model. Vice-versa, high values indicate larger ambiguity
and hence low confidence in the model. See Methods for the detailed definitions.
DR-FREE computes the optimal policy via a distributionally robust generalization of the free en-
3
Figure 1: Comparison between free energy and robust free energy for policy computation. a.
A robotic agent navigating a stochastic environment to reach a destination while avoiding obstacles. At a
given time-step, k−1, the agent determines an action U from a policy using a model of the environment
k
(e.g., available at training via a simulator possibly updated via real world data) and observations/beliefs
(grouped in the state X k−1 ). The environment and model can change over time. Capital letters are
random variables, lower-case letters are realizations. b. The trained model and the agent environment
differ. This mismatch is a training/environment (model) ambiguity: for a state/action pair, the ambiguity
set is the set of all possible environments that have statistical complexity from the trained model of at
most η k( x k−1 ,u k) . We use the wording trained model in a very broad sense. A trained model is any
model available to the agent offline: for example, this could be a model obtained from a simulator or, for
natural agents, this could be hardwired into evolutionary processes or even determined by prior beliefs.
c. A free energy minimizing agent in an environment matching its own model. The agent determines an
⋆
action by sampling from the policy π k( u k ∣ x k−1) . Given the model, the policy is obtained by minimizing
the variational free energy: the sum of a statistical complexity (with respect to a generative model, q 0∶N )
x u
and expected loss (state/action costs, c ( k )( x k) and c k ( )( u k) ) terms. d. DR-FREE extends the free energy
principle to account for model ambiguities. According to DR-FREE, the maximum free energy across all
environments – in an ambiguity set – is minimized to identify a robust policy. This amounts to variational
policy optimization under the epistemic uncertainty engendered by ambiguous environment.
ergy principle that accounts for ambiguities. This first component of DR-FREE, providing the problem
statement for policy computation, generalizes the conventional free energy principle to ensure policies re-
main robust even when the environment deviates from the trained model. Our principle is formulated as
⋆
follows: over the decision horizon, the optimal policy sequence { π k( u k ∣ x k−1)}1∶N is obtained by mini-
mizing over the policies the maximum free energy across all possible environments in the ambiguity set
expected under the policy in question. The expected free energy combines two terms: (i) the statistical
complexity, D KL( p 0∶N ∣∣ q 0∶N) , of the agent-environment behavior from q 0∶N ; (ii) the expected cumulative
loss E p0∶N [ ∑N k=1( c k ( x )( X k) +c ( k u )( U k))] . The principle is formalized in Fig. 2a (see Methods for details)
as a distributionally robust policy optimization problem in the probability space. DR-FREE computes the
4
Figure 2: DR-FREE a. Summarizing the distributionally robust free energy principle – the problem
statement for policy computation. Our generalization of active inference yields an optimization framework
where policies emerge by minimizing the maximum free energy over all possible environments in the
ambiguity set, which formalizes the constraints in the problem formulation. b. The resolution engine to
find the policy. Given the current state, the engine uses the generative model and the loss to find the
maximum free energy D KL( p k( x k ∣ x k−1 ,u k)∣∣ q k( x k ∣ x k−1 ,u k)) +E pk x k x k−1,u k [ c¯ k( X k)] across all the
environments in the ambiguity set. This yields the cost of ambiguity η k( (x k ∣ −1 ,u k) )+c˜ ( x k−1 ,u k) that builds
up the expected loss for the subsequent minimization problem. In this second problem, the variational free
⋆
energy is minimized in the space of polices providing: (i) π k( u k ∣ x k−1) , the DR-FREE policy from which
actions are sampled. Elements that guarantee robustness in green – these terms depend on ambiguity; (ii)
thesmallestfreeenergythattheagentcanachieve, i.e., thecost-to-goc¯ x fedbacktothemaximization
k( k)
problem at the next time-step. For reactive actions, where N =1, the cost-to-go equals the state cost given
by the agent loss. c. Using the generative model and the state-cost, DR-FREE first computes the cost of
ambiguity, whichisnon-negative. This, togetherwiththeactioncostisthenusedtoobtaintheexponential
kernel in the policy, i.e. exp ( −c ( k u )( u k) −η k( x k−1 ,u k) −c˜ ( x k−1 ,u k)) . After multiplication of the kernel
⋆
with q k( u k ∣ x k−1) and normalization, this returns π k( u k ∣ x k−1) .
5
policy via bi-level optimization, without requiring the environment p k( x k ∣ x k−1 ,u k) and without perform-
ing stochastic sampling for the ambiguity set. The ambiguity set defines the problem constraints – these
are nonlinear in the decision variable – and the min-max objective is the free energy – also nonlinear in the
decision variables (see Sec. S2 in the Supplementary Information for connections with other frameworks).
As also shown in Fig. 2a, the complexity term in the min-max objective regularizes the optimal policy
to prevent environment-agent interactions that are overly complex with respect to q 0∶N . This is specified
as q 0( x 0) ∏N k=1 q k( x k ∣ x k−1 ,u k) q k( u k ∣ x k−1) , with q 0( x 0) being a prior. Hence, the first term in the min-
max objective biases the optimal solution of our robust principle – DR-FREE policy – towards q 0∶N . The
second term in the objective minimizes the worst case expected loss across ambiguity. We refer to q 0∶N as
generative model, although – for the application of our principle in Fig 2a – this is not necessarily required
to be a time-series model. For example, in some of our navigation experiments, q 0∶N only encodes the
agent goal destination – in this case the complexity term in Fig. 2a encodes an error from the goal. In
contrast, in a second set of experiments – where we relate DR-FREE to a state-of-the-art approach from
the literature22,31 – q 0∶N encodes a time-series model. Our formulation also allows for the generative model
q k( x k ∣ x k−1 ,u k) – provided to DR-FREE – to be different from the true environment p k( x k ∣ x k−1 ,u k) –
not available to the agent – and the trained model p¯ k( x k ∣ x k−1 ,u k) , see also Fig. 1a-b. This feature can
be useful in applications where, by construction, the model available to the agent and the generative model
differ.25 Both reactive and planned behaviors can be seen through the lenses of our formulation: a width
ofthedecisionhorizonN greaterthan1meansthatthepolicyattime-stepk iscomputedaspartofaplan.
If N = 1, the agent action is reactive (or greedy). These actions frequently emerge as reflexive responses
in biologically inspired motor control.40,41 In brief, this generalization of active inference can be regarded
as robust Bayesian model averaging to accommodate epistemic uncertainty about the environment, where
the prior over models is supplied by the KL divergence between each model and the trained model. When
there is no ambiguity, our robust principle in Fig. 2a connects with free energy minimization in active
inference based upon expected free energy42 – which itself generalizes schemes such as KL control and
control as inference.39,43–45 See Methods and Sec. S2 of Supplementary Information.
The policy optimization problem in Fig. 2a is infinite-dimensional as both minimization and maxi-
mization are in the space of probability densities. This enables a Bayes-optimal handling of uncertainty
and ambiguity that characterizes control and planning as (active) inference. DR-FREE resolution engine –
the method to compute the policy – not only finds the policy but also, perhaps counterintuitively, returns
a solution with a well-defined and explicit functional form. The analytical results behind the resolution
engine are in Sec. S3 and Sec. S6 of the Supplementary Information. In summary, the analytical results
showthat, ateachk, theoptimalpolicycanbefoundviaabi-leveloptimizationapproach, firstmaximizing
free energy across the ambiguity constraint and then minimizing over the policies. While the maximization
problem is still infinite dimensional, its optimal value – yielding a cost of ambiguity – can be obtained
by solving a scalar optimization problem. This scalar optimization problem is convex and has a global
minimum. Therefore, once the cost of ambiguity is obtained, the resulting free energy can be minimized in
the policy space and the optimal policy is unique. These theoretical findings are summarized in Fig. 2b.
Specifically, the policy at time-step k is a soft-max (Fig. 2b) obtained by equipping the generative model
u
q k( u k ∣ x k−1) with an exponential kernel. The kernel contains two costs: the action cost c ( k )( u k) and the
cost of ambiguity, η k( x k−1 ,u k) +c˜ ( x k−1 ,u k) . Intuitively, given the cost-to-go c¯ k( x k) , the latter cost is the
maximum free energy across all possible environments in the ambiguity set. The infinite dimensional free
energy maximization step, which can be reduced to scalar convex optimization, yields a cost of ambiguity
that is always bounded and non-negative (Fig. 2c and Methods). This implies that an agent always in-
6
curs a positive cost for ambiguity and the higher this cost is for a given state/action pair, the lower the
probability of sampling that action is. The result is that DR-FREE policy balances between the cost of
ambiguity and the agent beliefs encoded in the generative model (Fig. 2c).
DR-FREE succeeds when ambiguity-unaware free energy minimizing agents fail. To evaluate
DR-FREEwespecificallyconsideredanexperimentwheresimplicitywasadeliberatefeature,ensuringthat
theeffectsofmodelambiguityondecision-makingcouldbeidentified,benchmarkedagainsttheliterature,46
andmeasuredquantitatively. Theexperimentationplatform(Fig. 3a)istheRobotarium,47 providingboth
hardware and a high-fidelity simulator. The task is robot navigation: a rover needs to reach a goal desti-
nation while avoiding obstacles (Fig. 3b). In this set-up we demonstrate that an ambiguity-unaware free
energy minimizing agent – even if it makes optimal actions – does not reliably complete the task, while
DR-FREE succeeds. The ambiguity-unaware agent from the literature46 computes the optimal policy by
solving a relaxation of the problem in Fig. 2a without ambiguity. This agent solves a policy computation
problem – relevant across learning and control56 – having DR-FREE objective but without constraints.
We performed several experiments: in each experiment, DR-FREE, used to compute reactive actions, only
had access to a trained model p¯ k( x k ∣ x k−1 ,u k) and did not know p k( x k ∣ x k−1 ,u k) . We trained off-line a
Gaussian Process model, learned in stages. At each stage, data were obtained by applying randomly sam-
pled actions to the robot and a bias was purposely added to the robot positions (see Experiments settings
in Methods for training details and Sec. S5 in Supplementary Information for the data) thus introducing
ambiguity. The corrupted data from each stage were then used to learn a trained model via Gaussian
Processes. Fig. 3c shows the performance of DR-FREE at each stage of the training, compared to the per-
formance of a free energy minimizing agent that makes optimal decisions but is ambiguity unaware. In the
first set of experiments, when equipped with DR-FREE, the robot is always able to successfully complete
the task (top panels in Fig. 3c): in all the experiments, the robot was able to reach the goal while avoiding
the obstacles. In contrast, in the second set of experiments, when the robot computes reactive actions by
minimizing the free energy – without using DR-FREE – it fails the task, crashing in the obstacles, except
in trivial cases where the shortest path is obstacle-free (see Fig. 3c, bottom; details in Methods). The
conclusion is confirmed when this ambiguity-unaware agent is equipped with planning capabilities. As
shown in Supplementary Fig. 3 for different widths of the planning horizon, the ambiguity-unaware agent
still only fulfills the task when the shortest path is obstacle-free, confirming the findings shown in Fig. 3c,
bottom. The experiments provide two key highlights. First, ambiguity alone can have a catastrophic im-
pact on the agent and its surroundings. Second, DR-FREE enables agents to succeed in their task despite
the very same ambiguity. This conclusion is also supported by experiments where DR-FREE is deployed
on the Robotarium hardware. As shown in Fig. 3d, DR-FREE in fact enabled the robot provided by the
Robotarium to navigate to the destination, effectively completing the task despite model ambiguity. The
computation time measured on the Robotarium hardware experiments was of approximately 0.22 seconds
(Methods for details). See Data Availability for a recording; code also provided (see Code Availability).
Supplementary Fig. 4 presents results from a complementary set of experiments in the same domain but
featuring different goal positions and obstacle configurations. The experiments confirm that, despite ambi-
guity, DR-FREE consistently enables the robot to complete the task across all tested environments (code
also available).
DR-FREE elucidates the mechanistic role of ambiguity on optimal decision making. DR-
FREE policy (Fig. 2b) assigns lower probabilities to states and actions associated with higher ambiguity.
7
Figure3: DR-FREEevaluation. a. Unicyclerobotsof11cm×8.5cm×7.5cm(width,length,height)that
need to achieve the goal destination, x , avoiding obstacles. The work area is 3m×2m, the robot position
d
is the state, and actions are vertical/horizontal speeds; q k( x k ∣ x k−1 ,u k) is a Gaussian centered in x d and
q k( u k ∣ x k−1) is uniform. See Methods for the settings. b. The non-convex state cost for the navigation
task. See Methods for the expression. c. Comparison between DR-FREE and a free-energy minimizing
agent that makes optimal decisions but is unaware of the ambiguity. DR-FREE enables the robot to
successfully complete the task at each training stage. The ambiguity-unaware agent fails, except when
the shortest path is obstacle-free. Training details are in Methods. d. Screenshots from the Robotarium
platform recording of one experiment. DR-FREE allows the robot (starting top-right) to complete the
task (trained model from stage 3 used). e. How DR-FREE policy changes as a function of ambiguity.
By increasing the radius of ambiguity by 50%, DR-FREE policy (left) becomes a policy dominated by
ambiguity (right). As a result, actions with low ambiguity are assigned higher probability. Screenshot of
the robot policy when this is in position 0.2,0.9 , i.e., near the middle obstacle. The ambiguity increase
[ ]
deterministicallydrivestherobotbottom-left(notethehigherprobability)regardlessofthepresenceofthe
obstacle. f. Belief update. Speeds/positions from the top-right experiments in panel c) are used together
with F =16 state/action features, φ i( x k−1 ,u k) =E p¯k x k x k−1,u k [ ϕ i( X k)] in Supplementary Fig. 1b. Once
the optimal weights, w ⋆ , are obtained, the reconstru ( ct ∣ ed cost ) is −E ∑16 w ⋆ ϕ X . Since
i p¯k x k x k−1,u k [ i=1 i i( k)]
this lives in a 4-dimensional space, we show −∑16 w ⋆ ϕ x , which can ( be ∣ conve ) niently plotted.
i=1 i i( k)
8
Insimplerterms,anagentthatfollowsDR-FREEpolicyismorelikelytoselectactionsandstatesassociated
with lower ambiguity. DR-FREE yields a characterization of the agent behavior in regimes of small and
large ambiguity. Intuitively, as ambiguity increases, DR-FREE yields a policy dominated by the agent’s
generative model and radius of ambiguity. In essence, as ambiguity increases, DR-FREE implies that the
agent grounds decisions on priors and ambiguity, reflecting its lack of confidence. Conversely, when the
agent is confident about its trained model, DR-FREE returns the policy of a free energy minimizing agent
making optimal decisions in a well-understood, ambiguity-free, environment.
Characterizing optimal decisions in the regime of large ambiguity amounts at studying DR-FREE pol-
icy (Fig. 2b) as η k( x k−1 ,u k) increases. More precisely, this means studying what happens when η min =
u
minη k( x k−1 ,u k) increases. Since c˜ ( x k−1 ,u k) and c k ( )( u k) are non-negative, and since c˜ ( x k−1 ,u k) does
not depend on η k( x k−1 ,u k) for sufficiently large η min , this means that, when η k( x k−1 ,u k) is large enough,
exp ( −η k( x k−1 ,u k) −c˜ ( x k−1 ,u k) −c ( k u )( u k)) ≈ exp ( −η k( x k−1 ,u k)) in DR-FREE policy (derivations in
⋆
Supplementary Information). Therefore, as ambiguity increases, π k( u k ∣ x k−1) only depends on the gen-
erative model and the ambiguity radius. Essentially, DR-FREE shows that, when an agent is very unsure
about its environment, optimal decisions are dominated by the generative model and the radius of ambi-
guity. In our experiments, an implication of this is that with larger η k( x k−1 ,u k) the robot may sacrifice
physical risk (obstacle avoidance) in favor of epistemic risk (avoiding model mismatch). Hence, an inter-
esting but not usual outcome is that risk-averse is not equal to obstacle-avoidance especially when model
uncertainty is not evenly distributed. This behavior is clearly evidenced in our experiments. As shown
in Fig. 3e, as ambiguity increases the agent’s policy, becoming dominated by ambiguity, deterministically
directstherobottowardsthegoalposition–associatedtothelowestambiguity–disregardingthepresence
of obstacles. At the time-step captured in the figure, the robot was to the right of the middle obstacle.
Consequently, while DR-FREE policy with the original ambiguity radius would assign higher probabilities
to speeds that drive the robot towards the bottom of the work-area, when ambiguity increases the robot
is instead directed bottom-left and this engenders a behavior that makes the robot crash in the obstacle.
Conversely, to characterize the DR-FREE policy in the regimes of low ambiguity, we need to study
how the policy changes as η k( x k−1 ,u k) shrinks. As η k( x k−1 ,u k) →0, the ambiguity constraint is relaxed
and c˜ ( x k−1 ,u k) simply becomes D KL( p¯ k( x k ∣ x k−1 ,u k)∣∣ q k( x k ∣ x k−1 ,u k)) +E p¯k x k x k−1,u k [ c¯ k( X k)] . See
( ∣ )
Supplementary Information for the derivations. This yields the optimal policy from the literature.46 The
minimum free energy attained by this policy when there is no ambiguity is always smaller than the free
energy achieved by an agent affected by ambiguity. Hence, DR-FREE shows that ambiguity cannot be
exploited by a free energy minimizing agent to obtain a better cost. See Methods for further discussion.
Supplementary Fig. 5 show experiments results for different ambiguity radii. Experiments confirm that,
due to the presence of ambiguity, when the radius is set to zero the agent – now ambiguity unaware – does
not always complete the task. Additionally, we conduct a second set of experiments in which the trained
model p¯ k( x k ∣ x k−1 ,u k) coincides with p k( x k ∣ x k−1 ,u k) . This scenario remains stochastic but no longer
features ambiguity. The results (Supplementary Fig. 6) show that – unlike in the previous experiments –
DR-FREEenablestherobottocompletethetaskevenwhentheambiguityradiusissetto0. Thisoutcome
is consistent with our analysis: when there is no ambiguity, the KL divergence between p k( x k ∣ x k−1 ,u k)
and p¯ k( x k ∣ x k−1 ,u k) is identically zero and DR-FREE therefore succeeds even for a zero ambiguity radius.
We provide the code to replicate the results (see Code Availability).
DR-FREE supports Bayesian belief updating. DR-FREE policy associates higher probabilities to
actions for which the combined action and ambiguity cost c k ( u )( u k) +η k( x k−1 ,u k) +c˜ ( x k−1 ,u k) is higher.
9
This means that the reason why an action is observed can be understood by estimating this combined cost:
DR-FREE supports a systematic framework to achieve this. Given a sequence of observed states/actions,
(xˆ k−1 , uˆ k ) and the generative policy q k( u k ∣ x k−1) , the combined cost can be estimated by minimizing
the negative log-likelihood. The resulting optimization problem is convex if a widely adopted (see Meth-
ods) linear parametrization of the cost in terms of known (and arbitrary) features is available. Given F
state/action features and G action features, this parametrization is ∑F i=1 w i φ i( x k−1 ,u k) +∑G i=1 v i γ i( u k) .
Reconstructing the cost then amounts at finding the optimal weights that minimize the negative log-
likelihood, with likelihood function being ∏M k=1 p ⋆ k( uˆ k ∣ xˆ k−1 ;v,w ) . Here, M is the number of observed
state/inputs pairs and p ⋆ k( uˆ k ∣ xˆ k−1 ;v,w ) is, following the literature,46 the DR-FREE policy itself but
with −c ( k u )( u k) −η k( x k−1 ,u k) −c˜ ( x k−1 ,u k) replaced by the linear parametrization (v and w are the stacks
of the parametrization weights). We unpack the resulting optimization problem in the Methods. This
convenient implication of DR-FREE policy allows one to reconstruct the cost driving the actions of the
rovers using DR-FREE in our experiments and Fig. 3f shows the outcome of this process. The similarity
with Fig. 3b is striking and to further assess the effectiveness of the reconstructed cost we carried out a
number of additional experiments. In these experiments, the robots are equipped with DR-FREE policy
but, crucially, −c ( k u )( u k) −η k( x k−1 ,u k) −c˜ ( x k−1 ,u k) is replaced with the reconstructed cost. The outcome
from these experiments confirms the effectiveness of the results: as shown in Supplementary Fig. 1d,
the robots are again able to fulfill their goal, despite ambiguity. Additionally, we also benchmarked our
reconstruction result with other state-of-the-art approaches. Specifically, we use an algorithm from the
literature that, building on maximum entropy,48 is most related to our approach.49 This algorithm makes
use of Monte Carlo sampling and soft-value iteration. When using this algorithm – after benchmarking it
on simpler problems – we observed that it would not converge to a reasonable estimate of the robot cost
(see Supplementary Fig. 1e for the reconstructed cost using this approach and the Methods for details).
Since our proposed approach leads to a convex optimization problem, our cost reconstruction results could
be implemented via off-the-shelf software tools. The code for the implementation is provided (see Code
Availability).
Relaxing ambiguity yields maximum diffusion. Maximum diffusion (MaxDiff) is a policy compu-
tation framework that generalizes maximum entropy (MaxEnt) and inherits its robustness properties. It
outperforms other state-of-the-art methods across popular benchmarks.22,31 We show that the distribu-
tionally robust free energy principle (Fig. 2a) can recover, with a proper choice of q 0∶N , the MaxDiff
objective when ambiguity is relaxed. This explicitly connects DR-FREE to MaxDiff and – through it – to
a broader literature on robust decision-making (Sec. S2 of Supplementary Information). In MaxEnt – and
MaxDiff – robustness guarantees stem from the entropy of the optimal policy,32 with explicit a-posteriori
bounds on the ambiguity set over which the policy guarantees robustness available for discrete settings and
featuring a constant radius of ambiguity [32, Lemma 4.3]. To compute policies that robustly maximize a
reward, MaxEnt must be used with an auxiliary, pessimistic, reward.32 In contrast, by tackling the prob-
lem in Fig. 2a, DR-FREE defines robustness guarantees directly in the problem formulation, explicitly via
the ambiguity set. As a result, DR-FREE policy is guaranteed to be robust across this ambiguity set. As
detailed in Sec. S2 of Supplementary Information, DR-FREE is, to our knowledge, the full min-max prob-
lem in Fig. 2a – featuring at the same time a free energy objective and distributionally robust constraints
– remains a challenge for many methods.11,22,31,32 This is not just a theoretical achievement uniquely
positioning DR-FREE in the literature – we explore its implications by revisiting our robot navigation
task: we equip DR-FREE with a generative model that recovers MaxDiff objective and compare their
10
performance. The experiments reveal that DR-FREE succeeds in settings where MaxDiff fails. This is
because DR-FREE not only retains the desirable properties of MaxDiff, but also guarantees them in the
worst case over the ambiguity set.
In MaxDiff, given some initial state x , policy computation is framed as minimizing in the policy space
0
D KL( p 0∶N ∣∣ p max( x 0∶N ,u 1∶N)) . This is the KL divergence between (using the time-indexing and notation
in Fig. 2a for consistency) p 0∶N and p max( x 0∶N ,u 1∶N) = Z 1 ∏N k=1 p max( x k ∣ x k−1) exp ( r ( x k ,u k)) . In this last
expression, r x ,u is the state/action reward when the agent transitions in state x under action u , Z
( k k) k k
is the normalizer and p max( x k ∣ x k−1) is the maximum entropy sample path probability.22 On the other
hand, the distributionally robust free energy principle (Fig. 2a) is equivalent to
min max D KL( p 0∶N ∣∣ q˜ 0∶N) (1)
{
πk
(
u k
∣
x k−1 )}1∶N pk
(
x k
∣
x k−1,u k
)
∈B η
(
p¯k
(
x k
∣
x k−1,u k
))
in the sense that the optimal solution of (1) is the same as the optimal solution of the problem in Fig. 2a
(see Methods). In the above expression, q˜ 0∶N = Z 1q 0∶N exp ( −∑N k=1( c ( k x )( x k) +c ( k u )( u k))) and Z is again a
normalizer. When there is no ambiguity, the optimization problem in (1) is relaxed and it becomes
min D KL( p 0∶N ∣∣ q˜ 0∶N) (2)
{
πk
(
u k
∣
x k−1 )}1∶N
Given an initial state x , as we unpack in the Methods, this problem has the same optimal solution as
0
the MaxDiff objective when the reward is −c ( k x )( x k) −c ( k u )( u k) and in the DR-FREE generative model –
which we recall is defined as q 0( x 0) ∏N k=1 q k( x k ∣ x k−1 ,u k) q k( u k ∣ x k−1) – we set q k( x k ∣ x k−1 ,u k) to be the
maximum entropy sample path probability and q k( u k ∣ x k−1) to be uniform.
The above derivations show that relaxing ambiguity in DR-FREE yields, with a properly defined q 0∶N ,
the MaxDiff objective – provided that the rewards are the same. This means that, in this setting, DR-
FREE guarantees the desirable MaxDiff properties in the worst case, over B η( p¯ k( x k ∣ x k−1 ,u k)) . To eval-
uate the implications of this finding, we revisit the robot navigation task. We equip DR-FREE with q 0∶N
defined as described above, in accordance with MaxDiff, and compare DR-FREE with MaxDiff itself. In
the experiments, the cost is again the one of Fig. 3b and again the agents have access to p¯ k( x k ∣ x k−1 ,u k) .
Initial conditions are the same as in Fig. 3c. Given this setting, Fig. 4a summarizes the success rates of
MaxDiff – i.e., the number of times the agent successfully reaches the goal – for different values of key
hyperparameters:22 samples and horizon, used to compute the maximum entropy sample path probability.
The figure shows a sweetspot where 100% success rate is achieved, yielding two key insights. First, increas-
ingsamplesincreasesthesuccessrate. Second–andrathercounter-intuitively–planninghorizonsthatare
too long yield a decrease in the success rate and this might be an effect of planning under a wrong model.
Worst performance are obtained with horizon set to 2 and Fig. 4b confirms that the success rates remain
consistent when an additional temperature-like22 hyperparameter is changed. Given this analysis, to com-
pare DR-FREE with MaxDiff, we equip DR-FREE with the generative model computed in accordance
with MaxDiff (with horizon set to 2 and samples to 50). In this setting, MaxDiff successfully completes
the task when the shortest path between the robot initial position and the goal is obstacle free (Fig. 4c).
In contrast, in this very same setting, the experiments show that DR-FREE – computing reactive actions
– allows the robot to consistently complete its task (Fig. 4d). This desirable behavior is confirmed when
samples is decreased to 10 (Fig. 4e). In summary, the experiments confirm that DR-FREE succeeds in
settings where MaxDiff fails. Experiments details are reported in Methods (Experiments settings) and
Supplementary Information (Sec. S6 and Tab. S-1). See also code availability.
11
Finally, we evaluate DR-FREE in the MuJoCo33 Ant environment (Fig. 5a). The goal is for the
quadrupedagenttomoveforwardalongthex-axiswhilemaintaininganuprightposture. Eachepisodelasts
1000 steps, unless the Ant becomes unhealthy – a terminal condition defined in the standard environment
that indicates failure. We compare DR-FREE with all previously considered methods, as well as with
model-predictive path integral control34 (NN-MPPI). Across all experiments, the agents have access to
the trained model p¯ k( x k ∣ x k−1 ,u k) and not to p k( x k ∣ x k−1 ,u k) . The trained model was obtained using
the same neural network architecture as in the original MaxDiff paper,22 which also included benchmarks
with NN-MPPI. The cost provided to the agents is the same across all the experiments and corresponds
to the negative reward defined by the standard environment. Fig. 5b shows the experimental results for
this setting. The experiments yield two main observations. First, DR-FREE outperforms all comparison
methods on average, and even the highest error bars (standard deviations from the mean) of the other
methods do not surpass DR-FREE average return. Second, in some of the trials, the other methods
would terminate prematurely the episode due to the Ant becoming unhealthy. In contrast, across all DR-
FREE experiments the Ant is always kept healthy and therefore episodes do not terminate prematurely.
See Experiments Settings in Methods and Supplementary Information for details; code also provided.
Discussion
Robustness is a core requirement for intelligent agents that need to operate in the real world. Rather
than leaving its fulfillment to – quoting the literature5 – an emergent and potentially brittle property
from training, DR-FREE ensures this core requirement by design, building on the minimization of the free
energy and installing sequential policy optimization into a rigorous (variational or Bayesian) framework.
DR-FREE provides not only a free energy principle that accounts for environmental ambiguity, but also
the resolution engine to address the resulting sequential policy optimization framework. This milestone
is important because addresses a challenge for intelligent machines operating in open-worlds. In doing
so, DR-FREE elucidates the mechanistic role of ambiguity on optimal decisions and its policy supports
(Bayesian) belief-based updates. DR-FREE establishes what are the limits of performance in the face of
ambiguity, showing that, at a very fundamental level, it is impossible for an agent affected by ambiguity to
outperform an ambiguity-free free energy minimizing agent. These analytic results are confirmed by our
experiments.
In the navigation experiments, we compared the behaviors of an ambiguity-unaware free energy min-
imizing agent46 with the behavior of an agent equipped with DR-FREE. All the experiments show that
DR-FREE is essential for the robot to successfully complete the task amid ambiguity, and this is confirmed
when we consider additional benchmarks and different environments. DR-FREE enables to reconstruct
the cost functions that underwrote superior performance over related methods. Our experimental setting
is exemplary not only for intelligent machines, underscoring the severe consequences of ambiguity, but also
for natural intelligence. For example, through evolutionary adaptation, bacteria can navigate unknown
environments and this crucial ability for survival is achieved with little or no training. DR-FREE suggests
thatthis may bepossible ifbacteria followa decision-makingstrategy that, while simple, foresees arobust-
ness promoting step. Run-and-tumble motions50,51 might be an astute way to achieve this: interpreted
through DR-FREE, tumbles might be driven by free energy maximization, needed to quantify across the
environment a cost of ambiguity, and runs would be sampled from a free-energy minimizing policy that
considers this cost.
DR-FREEoffersamodelforrobustdecisionmakingviafreeenergyminimization,withrobustnessguar-
12
Figure 4: DR-FREE and MaxDiff. a. MaxDiff success rates for different values of the sampling size
and planning horizon. Experiments highlight a sweetspot in the hyperparameters with 100% success rate.
Worst rates are obtained for low horizons, where the success rate is between 25% and approximately 40%.
All experiments are performed with the temperature-like hyperparameter α set to 0.1. Data for each cells
obtained from 12 experiments corresponding to the initial conditions in Fig. 3c. b. Success rates for
different values of α and samples when horizon is set to 2. Success rates are consistent with the previous
panel – for the best combination of parameters, MaxDiff agent completes the task half of the times. See
Supplementary Fig. 7 for a complementary set of MaxDiff experiments. c. Robot trajectories using the
MaxDiff policy when the horizon is equal to 2 and samples is set to 50. MaxDiff fulfills the task when the
shortest path is obstacle-free. d. DR-FREE allows the robot to complete the task when it is equipped
with a generative model from MaxDiff computed using the same set of hyperparameters from the previous
panel. e. This desirable behavior is confirmed even when samples is decreased to 10. See Methods and
Supplementary Information for details.
antees defined in the problem formulation – it also opens a number of interdisciplinary research questions.
First, our results suggest that a promising research direction originating from this work is to integrate DR-
13
Figure 5: Ant experiments. a. ScreenshotfromtheMuJoCoenvironment(Antv-3). Thestatespaceis
29-dimensionalandtheactionspaceis8-dimensional. b. Performancecomparison. Chartsshowmeansand
bars standard deviations from the means across 30 experiments. In some episodes, the ambiguity unaware,
MaxDiff and NN-MPPI agents terminate prematurely due to the Ant becoming unhealthy; rewards were
set to zero from that point to the end of the episode. The Ant becomes unhealthy in 6% of the episodes
for the ambiguity unaware agent, 20% for MaxDiff and 23% for NN-MPPI. In contrast, the Ant remains
healthy in all DR-FREE experiments. As in previous experiments, DR-FREE is used to compute reactive
actions. The ambiguity-unaware policy46 corresponds to DR-FREE with the ambiguity radius set to zero.
FREE with perception and learning, coupling training with policy computation. This framework would
embed distributional constraints in the formulation of the policy computation problem, as in DR-FREE,
while retaining perception and learning mechanisms inspired by, for example, MaxDiff and/or evidence
free energy minimization. The framework would motivate analytical studies to quantify the benefits of
integrated learning over an offline pipeline. Along these lines, analytical studies should be developed to
extend our framework so that it can explicitly account for ambiguities in the agent cost/reward. Second,
DR-FREE takes as input the ambiguity radius and this motivates the derivation of a radius estimation
mechanism within our model. Through our analytic results we know that reducing ambiguity improves
performance; hence, integrating in our framework a method to learn ambiguity would be a promising step
towards agents that are not only robust, but also antifragile.52 Finally, our experiments prompt a broader
question: what makes for a good generative model/planning horizon in the presence of ambiguity? The
answer remains elusive – DR-FREE guarantees robustness against ambiguity and experiments suggest
that it compensates for poor planning/models; however, with e.g., more task-oriented model/planning,
ambiguity-unaware agents could succeed. This yields a follow-up question. In challenging environments,
is a specialized model better than a multi-purpose one for survival?
If, quoting the popular aphorism, all models are wrong, but some are useful, then relaxing the require-
ments on training, DR-FREE makes more models useful. This is achieved by departing from views that
14
emphasize the role, and the importance, of training: in DR-FREE the emphasis is instead on rigorously
installing robustness into decision-making mechanisms. With its robust free energy minimization principle
and resolution engine, DR-FREE suggests that, following this path, intelligent machines can recover ro-
bust policies from largely imperfect, or even poor, models. We hope that this work may inspire both the
deploymentofourfreeenergymodelinmulti-agentsettings(withheterogeneousagentssuchasdrones, au-
tonomousvesselsandhumans)acrossabroadrangeofapplicationdomainsand,combiningDR-FREEwith
Deep RL, lead to learning schemes that – learning ambiguity – succeed when classic methods fail. At a
perhapsdeeperlevel –asambiguityisakeytheme(seealsoSec. S7inSupplementaryInformation)across,
e.g., psychology, economics and neuroscience53–55 – we hope that this work may provide the foundation for
a biologically plausible neural explanation of how natural agents – with little or no training – can operate
robustly in challenging environments.
Methods
The agent has access to: (i) the generative model, q0∶N ; (ii) the loss, specified via state/action costs
c(x) ∶X →R, c(u) ∶U →R, with X and U being the state and action spaces (see Sec. S1 and Sec. S3 of the
k k
Supplementary Information for notation and details); (iii) the trained model p¯k x k x k−1,u k .
( ∣ )
Distributionally Robust Free Energy Principle. Model ambiguities are specified via the ambiguity set around
p¯k x k x k−1,u k , i.e., B η p¯k x k x k−1,u k . This is the set of all models with statistical complexity of at most
( ∣ ) ( ( ∣ ))
ηk x k−1,u k from p¯k x k x k−1,u k . Formally, B η p¯k x k x k−1,u k is defined as the set
( ) ( ∣ ) ( ( ∣ ))
pk x k x k−1,u k ∈D∶DKL pk x k x k−1,u k p¯k x k x k−1,u k ≤ηk x k−1,u k ,supppk x k x k−1,u k ⊆suppqk x k x k−1,u k .
{ ( ∣ ) ( ( ∣ )∣∣ ( ∣ )) ( ) ( ∣ ) ( ∣ )}
The symbol D stands for the space of densities and supp for the support. The ambiguity set captures all
the models that have statistical complexity of at most ηk x k−1,u k from the trained model and that have a
( )
support included in the generative model. This second property explicitly built in the ambiguity set makes
the optimization meaningful as violation of the property would make the optimal free energy infinite. The
radius ηk x k−1,u k is positive and bounded. As summarized in Fig. 2a, the principle in the main text yields
( )
the following sequential policy optimization framework:
ExpectedLoss
Complexity
³¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹N¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹·¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹µ
{ π k ⋆ ( u k ∣ x k−1 )}1∶N ∈ {πk(u a k r ∣ g x m k− i 1 n )}1∶N {pk(x k∣x m k− a 1 x ,u k)}1∶N ³D¹¹¹¹¹¹ K ¹¹¹¹¹¹¹¹¹¹¹¹ L ¹¹¹¹¹¹¹¹¹¹¹¹¹ ( ¹¹¹¹¹¹¹¹p¹¹¹¹¹¹¹¹¹ 0 ¹¹¹¹¹· ∶N ¹¹¹¹¹¹¹¹ ∣ ¹¹¹¹ ∣ ¹¹¹¹¹¹¹¹q¹¹¹¹¹¹¹¹ 0 ¹¹¹¹¹¹¹¹ ∶ ¹¹¹ N ¹¹¹¹¹¹¹¹¹¹¹¹¹µ ) +E p0∶N ⎡ ⎢ ⎢ ⎢
⎣
k ∑ =1 c( k x) ( X k ) +c( k u) ( U k ) ⎤ ⎥ ⎥ ⎥
⎦
s.t.pk x k x k−1,u k ∈B η p¯k x k x k−1,u k , ∀k=1,...,N.
( ∣ ) ( ( ∣ ))
´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹A¹¹¹¹¹¹¹¹¹¹¹¹m¹¹¹¹¹¹¹¹¹¹¹¹b¹¹¹¸igu¹¹¹¹¹¹¹¹i¹¹¹¹¹t¹¹¹¹¹¹y¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶
This is an extension of the free energy principle28 accounting for policy robustness against model ambigu-
ities. We are not aware of any other free energy account that considers this setting and the corresponding
infinite-dimensional optimization framework cannot be solved with excellent methods. When the ambi-
guity constraint is removed and the loss is the negative log-likelihood, our formulation reduces to the
expected free energy minimization in active inference. In this special case, the expected complexity (i.e.,
ambiguity cost) becomes risk; namely, the KL divergence between inferred and preferred (i.e., trained)
outcomes. The expected free energy can be expressed as risk plus ambiguity; however, the ambiguity
in the expected free energy pertains to the ambiguity of likelihood mappings in the generative model
15
(i.e., conditional entropy), not ambiguity about the generative model considered in our free energy model.
In both robust and conventional active inference, the complexity term establishes a close relationship be-
tweenoptimalcontrolandJaynes’maximumcaliber(a.k.a., pathentropy)orminimumentropyproduction
principle.57,58 It is useful to note that, offering a generalization to free energy minimization in active in-
ference, our robust formulation yields as special cases other popular computational models such as KL
control,59control as inference,43 and the Linear Quadratic Gaussian Regulator. Additionally, when the
loss is the negative log-likelihood, the negative of the variational free energy in the cost functional is the
evidence lower bound60 a key concept in machine learning and inverse reinforcement learning.48 With its
resolutionengine, DR-FREEshowsthatinthisverybroadset-uptheoptimalpolicycanstillbecomputed.
Drawing the connection between MaxDiff and DR-FREE. We start with showing that the robust free energy
principle formulation in Fig. 2a has the same optimal solution as (1). We have the following identity:
DKL ( p0∶N ∣∣ q0∶N ) +E p0∶N ⎡ ⎢ ⎢ ⎢ ⎣ k ∑ N =1 c( k x) ( X k ) +c( k u) ( U k ) ⎤ ⎥ ⎥ ⎥ ⎦ =DKL ( p0∶N ∣∣ q˜0∶N ) −lnE q0∶N ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ exp⎛ ⎝ − k ∑ N =1 ( c( k x) ( X k ) +c( k u) ( U k )) ⎞ ⎠ ⎤ ⎥ ⎥ ⎥ ⎥ ⎦
The left hand-side is the objective of Fig. 2a. In the right-hand side, q˜0∶N is given in the main text and
E q0∶N[ exp ( − ∑ N k=1( c( k x) ( X k ) +c( k u) ( U k )))] is the normalizing constant denoted by Z in the main text. The last
term in the right-hand side does not depend on the decision variables and this yields that the min-max
problem in Fig. 2a has the same optimal solution as (1). Next, we show why – with the choice of q0∶N
described in the main text – DR-FREE yields the MaxDiff objective when there is no ambiguity. In this
case, the ambiguity constraint is relaxed and DR-FREE min-max problem becomes
{πk(u k m ∣x i k n −1)}1∶N DKL⎛ ⎝ p0∶N ∣∣ Z 1 q0∶Nexp⎛ ⎝ − k ∑ N =1 ( c( k x) ( x k ) +c( k u) ( u k )) ⎞ ⎠ ⎞ ⎠ (3)
Thisistheproblemgivenin(2)butwithq˜0∶N explicitlyincludedintheobjectivefunctional. Toestablishthe
connection between MaxDiff and DR-FREE we recall that the MaxDiff objective consists in minimizing in
the policy space the KL divergence between p0∶N and pmax x0∶N,u1∶N , which can be conveniently written as
( )
Z 1 ∏ N k=1( pmax ( x k ∣ x k−1 )) exp ( ∑ N k=1 r ( x k,u k )) . Now, this minimization problem has the same optimal solution of
1 N N
min DKL⎛p0∶N ∏ pmax x k x k−1 p¯ uk x k−1 exp⎛∑r x k,u k ⎞⎞ (4)
{πk(u k∣x k−1)}1∶N
⎝
∣∣ Z k=1 ( ( ∣ ) ( ∣ )) ⎝k=1 ( )
⎠⎠
when p¯ uk x k−1 is uniform. The equivalence can be shown by noticing that this reformulation has been
( ∣ )
obtained by adding and subtracting the constant quantity ln∏ N k=1 p¯
(
uk
∣
x k−1
)
to the MaxDiff cost functional.
The similarity between (4) and (3) is striking. In particular, the two problems are the same when: (i) x0 is
given, (ii) in q0∶N – which we recall is defined as p
(
x0
)
∏ N k=1 qk
(
x k
∣
x k−1,u k
)
qk
(
u k
∣
x k−1
)
– we have qk
(
x k
∣
x k−1,u k
)
settopmax ( x k ∣ x k−1 ) andqk ( u k ∣ x k−1 ) uniform; (iii)r ( x k,u k ) =−c( k x) ( x k ) −c( k u) ( u k ) . Thisestablishestheconnection
between DR-FREE and MaxDiff objective from the Results.
Resolutionengine. Boththevariationalfreeenergyandtheambiguityconstraintarenonlinearintheinfinite-
dimensional decision variables and this poses a number of challenges that are addressed with our resolution
engine. The resolution engine allows to tackle the sequential policy optimization framework arising from
our robust free energy principle. We detail here the resolution engine and refer to Supplementary Infor-
mation for the formal treatment. Our starting point is the robust free energy principle formulated via
the above sequential policy optimization framework. This can be solved via a backward recursion where,
16
starting from k=N, at each k the following optimization problem needs to be solved:
π m k (u ∣k i ) n −1 DKL ( π k (u ∣k ) −1∣∣ q k (u ∣k ) −1) +E π k (u ∣k ) −1[ c( k u) ( U k )] + p m ( k x ∣k a ) − x 1 E π k (u ∣k ) −1[ DKL ( p( k x ∣k ) −1∣∣ q k (x ∣k ) −1) +E p( k x ∣k ) −1[ c¯k ( X k )]]
s.t. p( k x ∣k ) −1 ∈B η ( p¯k ( x k ∣ x k−1,u k )) .
In the above expression, for compactness we used the shorthand notations π(u) , q(u) , p(x) and q(x) for
k∣k−1 k∣k−1 k∣k−1 k∣k−1
πk u k x k−1 , qk u k x k−1 , pk x k x k−1,u k and qk x k x k−1,u k , respectively. The term c¯k x k is the cost-to-go.
( ∣ ) ( ∣ ) ( ∣ ) ( ∣ ) ( )
This is given by c¯k ( x k ) =c( k x) ( x k ) +cˆk+1 ( x k ) , where cˆk+1 ( x k ) is the smallest free energy that can be achieved
by the agent at k+1. That is, cˆk+1 x k is the optimal solution of the above optimization problem evaluated
( )
at k+1. When k = N, cˆN+1 x N is initialized at 0. This means that, for reactive actions, e.g., reflexes,
( )
c¯k ( x k ) =c( k x) ( x k ) . The above reformulation is convenient because it reveals that, at each k, π k ⋆ ( u k ∣ x k−1 ) can
be computed via a bi-level optimization approach, consisting in first maximizing over pk x k x k−1,u k , ob-
( ∣ )
taining the maximum expected variational free energy across all possible environments in the ambiguity
set, to finally minimize over the policies. Crucially, this means that to make optimal decisions, the agent
does not need to know the environment that maximizes the free energy but rather it only needs to know
what the actual maximum free energy is. In turn, this can be found by first tackling the problem in green
in Fig. 2b, i.e., finding the cost of ambiguity, and then taking the expectation E ⋅ . While the
πk(u k∣x k−1)[]
problem in green in Fig. 2b is infinite-dimensional, DR-FREE finds the cost of uncertainty by solving
a convex and scalar optimization problem. This is possible because the optimal value of the problem in
green in Fig. 2b equals ηk x k−1,u k +minα≥0V˜ α x k−1,u k . In this expression, α is a scalar decision variable and
( ) ( )
V˜ α x k−1,u k , detailed in the Supplementary Information, is a scalar function of α, convex for all α≥0. The
( )
global non-negative minimum of V˜ α x k−1,u k is c˜ x k−1,u k . In summary, the free energy maximization step
( ) ( )
can be conveniently solved with off-the-shelf software tools. In DR-FREE, the cost of ambiguity promotes
robustness and contributes to the expected loss for the subsequent minimization problem in Fig. 2b. The
optimalsolutionofthisclassofproblemshasanexplicitexpression(π k ⋆ ( u k ∣ x k−1 ) inFig. 2b)andtheoptimal
value is cˆk x k−1 used at the next step in the recursion. Derivations in Supplementary Information.
( )
Why is it always better to be ambiguity-aware. As ambiguity vanishes, c˜ ( x k−1,u k ) becomes DKL ( p¯( k x ∣k ) −1∣∣ q k (x ∣k ) −1) +
E p¯(
k
x
∣k
) −1[ c¯k ( X k )] . Thus, the DR-FREE policy becomes
π k ⋆ ( u k ∣ x k−1 ) = ∫ qk ( u q k k ( ∣ u ∣k x ) − k 1 − e 1 x ) p ex ( − p D ( − K D L K ( p L ¯k ( ( p¯ x k k ( x ∣ k x k ∣ − x 1 k , − u 1 k , ) u k ∣∣ ) qk ∣∣ ( q x k k ( x ∣ k x k ∣ − x 1 k , − u 1 k , ) u ) k − )) E − pk E ( p x k k ( ∣x x k k − ∣x 1 k ,u −1 k , ) u [ k c¯ ) k [ ( c¯ X k ( k X )] k − ) c ] ( k − u c ) ( k ( u u ) k ( ) u ) k )) du k .
This is the optimal policy of an ambiguity-free agent46 (with pk x k x k−1,u k = p¯k x k x k−1,u k ). Given the
( ∣ ) ( ∣ )
current state x k−1 , the optimal cost is
−ln ∫U qk ( u k ∣ x k−1 ) exp ( −DKL ( p¯k ( x k ∣ x k−1,u k )∣∣ qk ( x k ∣ x k−1,u k )) −E p¯k(x k∣x k−1,u k)[ c¯k ( x k )] −c( k u) ( u k )) du k.
This is smaller than the cost achieved by the agent affected by ambiguity. In fact, when there is ambiguity,
the DR-FREE policy achieves the optimal cost −ln ∫ qk ( u k ∣ x k−1 ) exp ( −ηk ( x k−1,u k ) −c˜ ( x k−1,u k ) −c( k u) ( u k )) du k and
DKL ( p¯( k x ∣k ) −1∣∣ q k (x ∣k ) −1) +E p¯
k
(x
∣k
) −1[ c( k x) ( X k )] <ηk ( x k−1,u k ) +c˜ ( x k−1,u k ) . See Sec. S4 in the Supplementary Information for
the formal details.
WhyDR-FREESupportsBayesianbeliefupdating. Theapproachadoptsawidelyusedparametrization46,48,61,62
17
of the cost in terms of F state-action features, φi x k−1,u k , and G action features, γi u k . No assumptions
( ) ( )
are made on the features, which can be e.g., nonlinear. With this parametrization, given M observed
actions/state pairs, the likelihood function – inspired by the literature46,62 – is
k ∏ M =1∫ qk qk ( u ( u k k = ∣ uˆ x k k− ∣ 1 x k = − xˆ 1 k = −1 xˆ ) k e − x 1 p ) e ( x ∑ p F i= ( 1 ∑ w F i= i 1 φ w i ( iφ xˆk i − ( 1 xˆ , k u − k 1, ) uˆ + k ∑ ) + G i=1 ∑ v G i i = γ 1 i v ( i u γi k ( ) u ) ˆ d k u )) k ,
where w and v are the stacks of the weights wi and vi , respectively. The negative log-likelihood46,62 is then
given by
M F G
−L w,v =− ∑⎛lnqk u k =uˆk x k−1 =xˆk−1 + ∑wiφi xˆk−1,uˆk + ∑viγi uˆk
( ) k=1⎝ ( ∣ ) i=1 ( ) i=1 ( )
F G
−ln ∫ qk ( u k ∣ x k−1 =xˆk−1 ) exp⎛ ⎝i ∑ =1 wiφi ( xˆk−1,u k ) + i ∑ =1 viγi ( u k ) ⎞ ⎠ du k⎞ ⎠ .
The cost reconstruction in the main paper is then obtained by finding the weights that are optimal for
the problem minw,v −L w,v , after dropping the first term from the cost because it does not depend on the
( )
weights. Convexity of the problem follows because46,62 the cost functional is a conical combination of con-
vex functions. See Supplementary Information.
Experimentssettings. DR-FREE was turned into Algorithm 1 shown in the Supplementary Information and
engineered to be deployed on the agents (see Code Availability). In the robot experiments pk x k x k−1,u k is
( ∣ )
N ( x k−1 +u kdt,Σ ) with Σ= ⎡ ⎢ ⎢ ⎢ ⎢ ⎢0 0 . . 0 0 0 0 0 1 2 0 0 . . 0 0 0 0 0 1 2⎤ ⎥ ⎥ ⎥ ⎥ ⎥ , and where x k = [ px,k,py,k ] T is the position of the robot at time-step k,
⎢ ⎥
u k = vx,k,vy,k T is the inpu⎣t velocity ve⎦ctor, and dt=0.033s is the Robotarium time-step. In these experiments,
[ ]
the state space is −1.5,1.5 × −1,1 m, matching the work area, and the action space is −0.5,0.5 × −0.5,0.5
[ ] [ ] [ ] [ ]
m/s. In accordance with the maximum allowed speed in the platform, the inputs to the robot were
automatically clipped by the Robotarium when the speed was higher than 0.2 m/s. DR-FREE does not
have access to pk x k x k−1,u k . Its trained model, p¯k x k x k−1,u k , is learned via Gaussian Processes (GPs)
( ∣ ) ( ∣ )
with covariance function being an exponential kernel. The trained model was learned in stages and the
model learned in a given stage would also use the data from the previous stages. The data for the
training had a bias: the input u was sampled from a uniform policy and the next observed robot position
k
was corrupted by adding a quantity proportional to the current position (0.1x
k−1
). See Supplementary
Information for the details and the data used for training. This means that the models learned at each
stage of the training data were necessarily wrong: the parameters of the trained model at each stage
of the training are in Supplementary Fig. 1a. For the generative model, qk x k x k−1,u k =N x d,Σx , with
( ∣ ) ( )
Σx = 0.0001I2 , and qk u k x k−1 being the uniform distribution. Also, the ambiguity radius, ηk x k−1,u k =
( ∣ ) ( )
DKL qk x k x k−1,u k p¯k x k x k−1,u k and clipped at 100, is higher the farther the robot is from the goal
( ( ∣ )∣∣ ( ∣ ))
position (we recall that the agent has access to both qk x k x k−1,u k and p¯k x k x k−1,u k ). This captures
( ∣ ) ( ∣ )
higher agent confidence as it gets closer to its goal destination (see Supplementary Fig. 2 , also reporting
the number of steps recorded across the experiments). The state cost in Fig. 3b, adapted from the
literature,46 is c
(
x k
)
= 50
(
x k −x d
)
2+20∑ n i=1 gi
(
x k
)
+5b
(
x k
)
, where: (i) x d is the goal destination and thus this
term promotes goal-reaching; (ii) n = 6, gi are Gaussians, N oi,Σo , with Σo = 0.025I2 (I2 is the identity
( )
matrix of dimension 2). The oi ’s are in Supplementary Fig. 1c) and capture the presence of the obstacles.
Hence, the second term penalizes proximity to obstacles; (iii) b x k is the boundary penalty term given
( )
by b ( x k ) ∶= ∑ 2 j=1( exp ( −0.5 ( px,k σ −bxj ) 2 ) +exp ( −0.5 ( py,k σ −byj ) 2 ))/ σ√2π, with bxj , byj representing the jth component
of the boundary coordinates (bx = −1.5,1.5 , by = −1,1 ) and σ =0.02. In summary, the cost embeds obstacle
[ ] [ ]
18
avoidance in the formulation. The optimal policy of the ambiguity-unaware free energy minimizing agent
is available in the literature.46 This is again exponential but, contrary to DR-FREE policy in Fig. 3b, the
exponent is now −DKL ( p¯k ( x k ∣ x k−1,u k )∣∣ qk ( x k ∣ x k−1,u k )) −E p¯k(x k∣x k−1,u k)[ c( k x) ( x k )] . Across experiments, the first
term dominated the second, which accounted for obstacles (see Supplementary Information for details).
As a result, the policy consistently directs the robot along the shortest path to the goal, disregarding the
presence of the obstacles. This explains the behavior observed in Fig. 3c. The computation time reported
in the main text were obtained from DR-FREE deployment on the Robotarium hardware – measurements
obtained using the time() function in Python and averaging across the experiment.
The benchmark for our belief updating is with respect to the Infinite Horizon Maximum Causal En-
tropy Inverse RL algorithm with Monte-Carlo policy evaluation and soft-value iteration (not required in
DR-FREE belief update) from the literature.49 In order to use this algorithm, we discretized the state
space in a 50×50 grid (this step is not required within our results) and we needed to redefine the action
space as −1.0,1.0 × −1.0,1.0 . This was then discretized into a 5×5 grid. The corresponding reconstructed
[ ] [ ]
cost in Supplementary Fig. 1e was obtained with the same dataset and features used to obtain Fig. 3f.
After multiple trials and trying different settings, we were not able to obtain a reconstructed cost that
was better than the one in Supplementary Fig. 1e. The settings used to obtain Supplementary Fig. 1e
are: (i) initial learning rate of 1, with an exponential decay function to update the learning rate after
each iteration; (ii) discount factor for soft-value iteration, 0.9; (iii) initial feature weights randomly selected
from a uniform distribution with support −100,100 ; (iv) gradient descent stopping threshold, 0.01. The
[ ]
code to replicate all the results is provided (see Code Availability). For the comparison with MaxDiff22
we used the code available on the paper repository. In the experiments reported in Fig. 4, consistently
with all the other experiments, MaxDiff had access to p¯k x k x k−1,u k and cost/reward from Fig. 3b. In the
( ∣ )
experiments of the Supplementary Fig. 7b, MaxDiff learns both the reward and the model. In this case,
the data used by MaxDiff is corrupted by the same bias as the data used by DR-FREE (100000 data points
are used for learning, consistently with the original MaxDiff repository). Across all MaxDiff experiments,
again consistently with the original code, the additional MaxDiff hyperparameters are γ =0.95, λ=0.5 and
learning rate 0.0005 (this last parameter is used only in the experiments where MaxDiff does not have access
to the reward/model). No changes to the original MaxDiff code were made to obtain the results. The
available MaxDiff implementation natively allows users to configure if the framework uses reward/model
supplied by the environment. These details, together with a table summarizing all the parameter settings
used in the experiments, are given in the Supplementary Information (see at the end of Sec. S5 and Tab.
S-1). For the experiments of Fig. 5, the Ant becomes unhealthy when either the value of any of the states
is no longer finite or the height of the torso is too low/high (we refer to the environment documentation
at https://gymnasium.farama.org/environments/mujoco/ant/ for the standard definition of the unhealthy condition).
The cost is the negative reward provided by the environment and the trained model p¯k x k x k−1,u k is
( ∣ )
learned from 10000 data points collected through random policy rollouts, using the same neural network
as in the Ant experiments in the MaxDiff paper.22 The same network – providing as output mean and
variance – is used across all the experiments. For the DR-FREE generative model, qk u k x k−1 is uniform
( ∣ )
and qk x k x k−1,u k is set as an anisotropic Gaussian having the same mean provided by the neural network
( ∣ )
but with a different variance. The ambiguity radius captures higher agent confidence as this is moving in
the right direction. See Supplementary Information for details. MaxDiff and NN-MPPI implementations
are from the MaxDiff repository.
Schematic figure generation. The schematic figures were generated in Apple Keynote (v. 13.2). Panels were
19
assembled using the same software.
Data Availability
All (other) data needed to evaluate the conclusions in the paper and to replicate the experiments are
available in the paper, Supplementary Information and accompanying code (see the Assets folder in Code
Availability). A recording from the robot experiments, together with the figures of this paper, is available
at the folder Assets of our repository.63
Code Availability
Pseudocode for DR-FREE is provided in the Supplementary Information. The full code for DR-FREE to
replicate all the experiments is provided at our repository.63 The folder Experiments contains our DR-
FREE implementation for the Robotarium experiments. The folder also contains: (i) the code for the
ambiguity-unaware free energy minimizing agent; (ii) the data shown in Fig. SI-8, together with the GP
models and the code to train the models; (iii) the code to replicate the results in Fig. 3f and Fig. 3e. The
folder also contains the code for the experiments in Supplementary Fig. 4 and provides the instructions to
replicate the experiments of Supplementary Fig. 5 and Supplementary Fig. 6. The folder Belief Update
Benchmarkcontainsthecodetoreplicateourbenchmarksforthebeliefupdatingresults. ThefolderAssets
contains all the figures of this paper, the data from the experiments used to generate these figures, and the
moviefromwhichthescreen-shotsofFig. 3dweretaken. ThefolderMaxDiffBenchmarkcontainsthecode
to replicate our MaxDiff benchmark experiments. We build upon the original code-base from the MaxDiff
paper,22 integrating it in the Robotarium Python environment. The sub-folder Ant Benchmark contains
the code for the Ant experiments. MaxDiff and NN-MPPI implementations are from the literature.22
As highlighted in the Discussion, extending our analytical results to consider ambiguity inherently in
the reward is an open theoretical research direction, interesting per se. Nevertheless, we now discuss how
DR-FREE can be adapted to this setting. To achieve this, quoting form the literature,32 one could define
a modified problem formulation where the reward is appended to the observations available to the agent.
In this setting, the reward becomes – quoting the literature32 – the last coordinate of the observation, so
that it can be embedded into model ambiguity.
References
[1] Peter R. Wurman, Samuel Barrett, Kenta Kawamoto, James MacGlashan, Kaushik Subramanian,
Thomas J. Walsh, Roberto Capobianco, Alisa Devlic, Franziska Eckert, Florian Fuchs, Leilani Gilpin,
Piyush Khandelwal, Varun Kompella, HaoChih Lin, Patrick MacAlpine, Declan Oller, Takuma Seno,
CraigSherstan,MichaelD.Thomure,HoumehrAghabozorgi,LeonBarrett,RoryDouglas,DionWhite-
head, Peter Du¨rr, Peter Stone, Michael Spranger, and Hiroaki Kitano. Outracing champion Gran
Turismo drivers with deep reinforcement learning. Nature, 602(7896):223–228, February 2022.
[2] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare,
AlexGraves,MartinRiedmiller,AndreasK.Fidjeland,GeorgOstrovski,StigPetersen,CharlesBeattie,
Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and
20
Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533,
February 2015.
[3] Jonas Degrave, Federico Felici, Jonas Buchli, Michael Neunert, Brendan Tracey, Francesco Carpanese,
Timo Ewalds, Roland Hafner, Abbas Abdolmaleki, Diego de las Casas, Craig Donner, Leslie Fritz,
Cristian Galperti, Andrea Huber, James Keeling, Maria Tsimpoukelli, Jackie Kay, Antoine Merle,
Jean-Marc Moret, Seb Noury, Federico Pesamosca, David Pfau, Olivier Sauter, Cristian Sommariva,
Stefano Coda, Basil Duval, Ambrogio Fasoli, Pushmeet Kohli, Koray Kavukcuoglu, Demis Hassabis,
and Martin Riedmiller. Magnetic control of Tokamak plasmas through deep reinforcement learning.
Nature, 602(7897):414–419, February 2022.
[4] Elia Kaufmann, Leonard Bauersfeld, Antonio Loquercio, Matthias Mu¨ller, Vladlen Koltun, and Davide
Scaramuzza. Champion-leveldroneracingusingdeepreinforcementlearning. Nature,620(7976):982–987,
August 2023.
[5] Katherine M. Collins, Ilia Sucholutsky, Umang Bhatt, Kartik Chandra, Lionel Wong, Mina Lee, Cede-
gao E. Zhang, Tan Zhi-Xuan, Mark Ho, Vikash Mansinghka, Adrian Weller, Joshua B. Tenenbaum,
and Thomas L. Griffiths. Building machines that learn and think with people. Nature Human Behaviour,
8(10):1851–1863, October 2024.
[6] Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J. Gershman. Building
machines that learn and think like people. Behavioral and Brain Sciences, 40:e253, November 2016.
[7] Vitaly Vanchurin, Yuri I. Wolf, Mikhail I. Katsnelson, and Eugene V. Koonin. Toward a theory of
evolution as multilevel learning. Proceedings of the National Academy of Sciences, 119(6), February 2022.
[8] H´ector Mar´ın Manrique, Karl John Friston, and Michael John Walker. ‘snakes and ladders’ in paleoan-
thropology: From cognitive surprise to skillfulness a million years ago. Physics of Life Reviews, 49:40–70,
July 2024.
[9] Mayank Kejriwal, Eric Kildebeck, Robert Steininger, and Abhinav Shrivastava. Challenges, evaluation
and opportunities for open-world learning. Nature Machine Intelligence, 6(6):580–588, June 2024.
[10] Robert D. McAllister and Peyman M. Esfahani. Distributionally robust model predictive control:
Closed-loop guarantees and scalable algorithms. IEEE Transactions on Automatic Control, 70(5):2963–2978,
May 2025.
[11] Janosch Moos, Kay Hansel, Hany Abdulsamad, Svenja Stark, Debora Clever, and Jan Peters. Robust
reinforcement learning: A review of foundations and recent advances. Machine Learning and Knowledge
Extraction, 4(1):276–315, March 2022.
[12] BaharTaskesen,DanIancu,C¸a˘gılKoc¸yig˘it,andDanielKuhn. Distributionallyrobustlinearquadratic
control. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advancesin
Neural Information Processing Systems, 36, 18613–18632. Curran Associates, Inc., 2023.
[13] Luc Rocher, Arnaud J. Tournier, and Yves-Alexandre de Montjoye. Adversarial competition and
collusion in algorithmic markets. Nature Machine Intelligence, 5(5):497–504, May 2023.
21
[14] Maxwell T. West, Shu-Lok Tsang, Jia S. Low, Charles D. Hill, Christopher Leckie, Lloyd C. L. Hol-
lenberg, Sarah M. Erfani, and Muhammad Usman. Towards quantum enhanced adversarial robustness
in machine learning. Nature Machine Intelligence, 5(6):581–589, May 2023.
[15] Geoffrey E. Hinton and Richard S. Zemel. Autoencoders, minimum description length and Helmholtz
free energy. In J. Cowan, G. Tesauro, and J. Alspector, editors, Advances in Neural Information Processing
Systems, volume 6. Morgan-Kaufmann, 1993.
[16] Geoffrey E. Hinton, Peter Dayan, Radford M. Neal, and Richard S.Zemel. The Helmholtz machine.
Neural Computation, 7:889–904, September 1995.
[17] Sharu T . Jose and Osvaldo Simeone. Free energy minimization: A unified framework for modeling,
inference, learning, and optimization [lecture notes]. IEEESignalProcessingMagazine, 38(2):120–125, March
2021.
[18] Mohamed Hibat-Allah, Estelle M. Inack, Roeland Wiersema, Roger G. Melko, and Juan Carrasquilla.
Variational neural annealing. Nature Machine Intelligence, 3(11):952–961, October 2021.
[19] Thomas Parr, Giovanni Pezzulo, and Karl J. Friston. ActiveInference: TheFreeEnergyPrincipleinMind,Brain,
and Behavior. The MIT Press, March 2022.
[20] Prateek Jaiswal, Harsha Honnappa, and Vinayak A. Rao. On the statistical consistency of risk-
sensitiveBayesiandecision-making. InProceedingsofthe37thInternationalConferenceonNeuralInformationProcessing
Systems, NIPS ’23, Red Hook, NY, USA, 2024. Curran Associates Inc.
[21] Terence D. Sanger. Risk-aware control. Neural Computation, 26(12):2669–2691, December 2014.
[22] Thomas A. Berrueta, Allison Pinosky, and Todd D. Murphey. Maximum diffusion reinforcement
learning. Nature Machine Intelligence, 6(5):504–514, May 2024.
[23] Pietro Mazzaglia, Tim Verbelen, and Bart Dhoedt. Contrastive active inference. In M. Ranzato,
A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information
Processing Systems, volume 34, pages 13870–13882. Curran Associates, Inc., 2021.
[24] Karl Friston. The free-energy principle: a rough guide to the brain? Trends in Cognitive Sciences,
13(7):293–301, July 2009.
[25] ConorHeins,BerenMillidge,LancelotDaCosta,RichardP.Mann,KarlJ.Friston,andIainD.Couzin.
Collective behavior from surprise minimization. Proceedings of the National Academy of Sciences, 121(17), April
2024.
[26] Tony J. Prescott and Stuart P. Wilson. Understanding brain functional architecture through robotics.
Science Robotics, 8(78), May 2023.
[27] Jakob Hohwy. The Predictive Mind. Oxford University Press, November 2013.
[28] Karl Friston, Lancelot D. Costa, Noor Sajid, Conor Heins, Kai Ueltzh¨offer, Grigorios A. Pavliotis,
and Thomas Parr. The free energy principle made simpler but not too simple. PhysicsReports, 1024:1–29,
June 2023.
22
[29] Sebastian Gottwald and Daniel A. Braun. The two kinds of free energy and the Bayesian revolution.
PLOS Computational Biology, 16(12):e1008420, December 2020.
[30] Abraham Imohiosen, Joe Watson, and Jan Peters. Active Inference or Control as Inference? A Unifying View,
page 12–19. Springer International Publishing, September 2020.
[31] Thomas Berrueta. Robot Thermodynamics. Ph.d. dissertation, Nortwestern University, December 2024.
Available online at https://www.proquest.com/openview/faffd739b9b7a1becbd5e99b0fbd83fe/.
[32] Benjamin Eysenbach and Sergey Levine. Maximum entropy RL (provably) solves some robust RL
problems. In International Conference on Learning Representations, 2022.
[33] Emanuel Todorov, Tom Erez and Yuval Tassa. MuJoCo: A physics engine for model-based control.
In Proceedings of 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026–5033, 2012
[34] Grady Williams, Nolan Wagener, Brian Goldfain, Paul Drews, James Rehg, Byron Boots, Evangelos
Theodorou. Information theoretic MPC for model-based reinforcement learning. In Proceedings of 2017
IEEE International Conference on Robotics and Automation, pp. 1714–1721, 2017
[35] Karl Friston, Francesco Rigoli, Dimitri Ognibene, Christoph Mathys, Thomas Fitzgerald, and Gio-
vanni Pezzulo. Active inference and epistemic value. Cognitive Neuroscience, 6(4):187–214, March 2015.
[36] Thomas Parr and Karl J. Friston. Uncertainty, epistemics and active inference. Journal of The Royal
Society Interface, 14(136):20170376, November 2017.
[37] Yuki Konaka and Honda Naoki. Decoding reward–curiosity conflict in decision-making from irrational
behaviors. Nature Computational Science, 3(5):418–432, May 2023.
[38] Koosha Khalvati, Seongmin A. Park, Saghar Mirbagheri, Remi Philippe, Mariateresa Sestito, Jean-
Claude Dreher, and Rajesh P. N. Rao. Modeling other minds: Bayesian inference explains human
choices in group decision-making. Science Advances, 5(11), November 2019.
[39] Antonella Maselli, Pablo Lanillos, and Giovanni Pezzulo. Active inference unifies intentional and
conflict-resolution imperatives of motor control. PLOS Computational Biology, 18(6):e1010095, June 2022.
[40] Julian F .V . Vincent, Olga A . Bogatyreva, Nikolaj R . Bogatyrev, Adrian Bowyer, and Anja K.
Pahl. Biomimetics: its practice and theory. Journal of the Royal Society Interface, 3(9):471–482, April 2006.
[41] Giovanni Pezzulo, Francesco Rigoli, and Karl Friston. Active inference, homeostatic regulation and
adaptive behavioural control. Progress in neurobiology, 134:17–35, November 2015.
[42] Thomas Parr and Karl J. Friston. Generalised free energy and active inference. Biological Cybernetics,
113(5–6):495–513, September 2019.
[43] Bart V.D. Broek, Wim Wiegerinck, and Hilbert J. Kappen. Risk sensitive path integral control. In
Conference on Uncertainty in Artificial Intelligence, 2010.
[44] Hagai Attias. Planning by probabilistic inference. In Christopher M. Bishop and Brendan J. Frey,
editors, Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics, volume R4 of Proceedings
of Machine Learning Research, 9–16. Reissued by PMLR on 01 April 2021.
23
[45] Matthew Botvinick and Marc Toussaint. Planning as inference. Trends in Cognitive Sciences,
16(10):485–488, October 2012.
[46] EmilandGarrabe,HozefaJesawada,CarmenDelVecchio,andGiovanniRusso. Onconvexdata-driven
inverse optimal control for nonlinear, non-stationary and stochastic systems. Automatica, 173:112015,
March 2025.
[47] Sean Wilson, Paul Glotfelter, Li Wang, Siddharth Mayya, Gennaro Notomista, Mark Mote, and Mag-
nus Egerstedt. The Robotarium: Globally Impactful Opportunities, Challenges, and Lessons Learned
in Remote-Access, Distributed Control of Multirobot Systems. IEEEControlSystemsMagazine, 40(1):26–44,
February 2020.
[48] Brian D. Ziebart, Andrew Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum Entropy Inverse
Reinforcement Learning. In Proceedingsofthe23rdNationalConferenceonArtificialIntelligence-Volume3, AAAI’08,
page 1433–1438. AAAI Press, 2008.
[49] Zhengyuan Zhou, Michael Bloem, and Nicholas Bambos. Infinite Time Horizon Maximum Causal En-
tropy Inverse Reinforcement Learning. IEEE Transactions on Automatic Control, 63(9):2787–2802, September
2018.
[50] Zahra Alirezaeizanjani, Robert Grossmann, Veronika Pfeifer, Marius Hintsche, and Carsten Beta.
Chemotaxis strategies of bacteria with multiple run modes. Science Advances, 6(22), May 2020.
[51] Alexander Tschantz, Anil K. Seth, and Christopher L. Buckley. Learning action-oriented models
through active inference. PLOS Computational Biology, 16(4):e1007805, April 2020.
[52] Nassim N. Taleb. Antifragile: Things that gain from disorder. Random House, November 2012.
[53] Andrew Barto, Marco Mirolli, and Gianluca Baldassarre. Novelty or surprise? Frontiers in Psychology,
4:907, December 2013.
[54] Ming Hsu, Meghana Bhatt, Ralph Adolphs, Daniel Tranel, and Colin F. Camerer. Neural systems
respondingtodegreesofuncertaintyinhumandecision-making. Science, 310:1680–1683, December2005.
[55] Paul J. Zak. Neuroeconomics. PhilosophicalTransactionsoftheRoyalSocietyB:BiologicalSciences, 359:1737–1748,
November 2004.
[56] E´miland Garrab´e and Giovanni Russo. Probabilistic design of optimal sequential decision-making
algorithms in learning and control. Annual Reviews in Control, 54:81–102, November 2022.
[57] Edwin T . Jaynes. The minimum entropy production principle. Annual Review of Physical Chemistry,
31(1):579–601, October 1980.
[58] Hermann Haken and Juval Portugali. Relationships. Bayes, Friston, Jaynes and Synergetics 2nd Foundation, 85 –
104. Springer International Publishing, February 2021.
[59] Emanuel Todorov. Efficient computation of optimal actions. Proceedings of the National Academy of Sciences,
106(28):11478–11483, April 2009.
[60] Kevin P. Murphy. Probabilistic Machine Learning: Advanced Topics. MIT Press, 2023.
24
[61] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.
[62] Krishnamurthy Dvijotham and Emanuel Todorov. Inverse optimal control with Linearly-Solvable
MDPs. In 27th International Conference on Machine Learning, 335–342, 2010.
[63] Hozefa Jesawada, DR-FREE repository, Zenodo, https://doi.org/10.5281/zenodo.17638771, 2025
Acknowledgments. AH and HJ did this work while at University of Salerno. HJ and GR supported by
the European Union-Next Generation EU Mission 4 Component 1 CUP E53D23014640001. KF sup-
ported by funding from the Wellcome Trust (Ref: 226793/Z/22/Z). AS supported by MOST - Sustainable
Mobility National Research Center and received funding from the European Union Next-GenerationEU
(PIANO NAZIONALE DI RIPRESA E RESILIENZA (PNRR)–MISSIONE 4 COMPONENTE 2, IN-
VESTIMENTO 1.4–D.D. 1033 17/06/2022) under Grant CN00000023. This document reflects only the
authors’ views and opinions. We acknowledge the use of ChatGPT for assistance in improving the wording
and grammar of this document. GR and HJ wish to thank Prof. Del Vecchio (Sannio University) who
allowed HJ to perform early preliminary experiments before him joining University of Salerno. GR thanks
Prof. Francesco Bullo (University College Santa Barbara, USA), Prof. Michael Richardson (Macquarie
University, Australia) and Prof. Mirco Musolesi (University College London, UK) for the insightful dis-
cussions and comments on an early version of this paper.
Author contributions. KF and GR conceptualized, designed and formulated the research. AS and GR con-
ceptualized, designed and formulated resolution engine concepts. AS developed all the proofs with inputs
from GR. AS, GR and HJ revised the proofs. GR and HJ designed the experiments. HJ, with inputs from
GR and AS, implemented DR-FREE, performed the experiments and obtained the corresponding figures
and data. DR-FREE code was revised by AS. All authors contributed to the interpretation of the results.
GR wrote the manuscript with inputs from all the authors. All authors contributed to and edited the
manuscript.
Competing interests. The authors declare no competing interests.
25
Supplementary Information: Distributionally Robust Free
Energy Principle for Decision-Making
Allahkaram Shafiei 1,∗ Hozefa Jesawada 2,∗ Karl Friston 3 Giovanni Russo 4 B
Supplementary Information
We provide supplementary figures and the formal details for the statements and results in the main text.
After providing some background (Sec. S1) we relate DR-FREE with other frameworks (Sec. S2) and
give the formal statements behind the resolution engine in Fig. 2b (Sec. S3). In Sec. S4 we show why
DR-FREE shows that it is always better to be ambiguity aware. After reporting the supplementary details
of the experiments (Sec. S5) we provide the proofs of all the statements (Sec. S6). Finally, in Sec. S7, we
unpack some possible psychological and translational implications of DR-FREE.
1 CzechTechnicalUniversity,Prague,CzechRepublic. 2 NewYorkUniversityAbuDhabi,AbuDhabiEmirate. 3 Wellcome
Centre for Human Neuroimaging, Institute of Neurology, University College London, United Kingdom. 4 Department of
Information and Electrical Engineering and Applied Mathematics, University of Salerno, Italy.
∗ These authors contributed equally. B e-mail: giovarusso@unisa.it
1
Supplementary Figures
Supplementary Fig. 1. a. The kernel function for the Gaussian Process learning (from Methods) of
the trained model together with the parameters learned at each stage of learning. b. The features for
cost reconstruction used to obtain Figure 3f. c. Centers of the Gaussians promoting obstacles avoidance
in Fig. 3 experiments (the parameter o in the agent cost given in Methods). d. Recordings from the
i
Robotarium simulator of experiments where the robots are equipped with DR-FREE policy but using the
reconstructed cost. The initial positions are different from the ones used in the experiments in the main
paper. e. Cost reconstructed with related method from the literature.1
Supplementary Fig. 2. In all the experiments from Fig. 3c, p k x k x k−1 ,u k always belongs
to the ambiguity set B η p¯ k x k x k−1 ,u k . The plots confirm this by showing that η k x k−1 ,u k –
see Experiments settings in Methods for the radius used in the experim(ents∣ – is alw)ays bigger than
D KL p k x k x k−1 ,u k p¯ k ( x( k x∣ k−1 ,u k )a)cross all the experiments. Column a. is from( the exp)er-
iments in Fig. 3c top-left. Column b. is from the experiments in Fig. 3c top-middle. Col-
umn(c. ( is∣from th)e∣∣exp(erim∣ents in )F)ig. 3c top-right. In all plots, η k x k−1 ,u k is in red and
D KL p k x k x k−1 ,u k p¯ k x k x k−1 ,u k in green. Time series stop when the episode ends (x-axes show
the number of steps for each experiment). ( )
( ( ∣ )∣∣ ( ∣ ))
Supplementary Fig. 3. Behavior of the ambiguity-unaware agent when equipped with planning. Panel
a. shows the behavior of the agent when N =3. Panels b–d. show the behavior of the agent when N =5,
N =10 and N =50, respectively. In all panels the initial positions for the robot are the same as in Fig. 3c.
Supplementary Fig. 4. DR-FREE experiments for the robot navigation task in different environments.
a. Experimental results for different obstacles and goal configurations. Across all the experiments, DR-
FREE enables the robot to successfully complete the task, consistently with our main results. b. The
corresponding costs (scale on the right). The cost function is the same to the one described in Methods,
with updated coordinates for obstacles and goal positions to match the different experimental set-ups.
Supplementary Fig. 5. Experiments results for different ambiguity levels. a. With respect to the
settings used in Fig. 3c, here the ambiguity radii are: (i) decreased by 50% (left); (ii) decreased by 90%
(middle); set to 0 (right). In this last case, p k x k x k−1 ,u k never belongs to the ambiguity constraint.
Consequently, consistently with our analytical results, the agent can fail in the presence of ambiguity. b.
Following the format of Supplementary Fig. 2,(each∣ column)shows – for each of the experiments – when
p k x k x k−1 ,u k belongstotheambiguitysetB η p¯ k x k x k−1 ,u k . ColorcodingisasinExpectedData2.
This panel (middle column) reveals that DR-FREE can still complete the task even when p k x k x k−1 ,u k
doe(s n∣ot always)lie within the ambiguity set. c(. D( R-∣FREE po)l)icy for different ambiguity levels. The
heatmaps – computed for the same ambiguity levels as in the top panels – show how the( rob∣ot policy)
changes when this is in position −0.5,−0.5 . When the radius is set to 0, the ambiguity constraint is
relaxed and DR-FREE policy coincides with the optimal policy for this relaxed problem. The process is
representative for all robot traject[ory points.] See Sec. S5 in Supplementary Information.
Supplementary Fig. 6. Experiments results when there is no ambiguity. a. The trained model
p¯ k x k x k−1 ,u k coincides with p k x k x k−1 ,u k . Experiments use the same ambiguity radii as in Supple-
mentary Fig. 5. This time, since the scenario does not feature ambiguity, DR-FREE enables the robot to
ful(fill t∣he task in)all cases, even whe(n th∣e ambigu)ity radius is set to 0 (right). b. As in Supplementary Fig.
5 and consistently with our analysis, when the radius is set to 0, the ambiguity constraint is relaxed and
DR-FREE policy again coincides with the optimal policy for this relaxed problem. As in Supplementary
Fig. 5, the heatmap corresponds to the robot policy when this is in position −0.5,−0.5 . See also Fig.
SI-9.
[ ]
Supplementary Fig. 7. Additional MaxDiff experiments. Panel a. shows the experiments with MaxDiff
having access to both reward and model. The top row shows the experiments with hyperparameters taken
from the sweetspot in Fig. 4a where MaxDiff achieves 100% success rate – horizon set to 20 and samples
to 50. The agent is in fact able to always complete the task. However, near the goal position, in some of
the experiments, we observe an erratic behavior. Upon experimenting with the environment, we believe
that this behavior might be due to planning under a model affected by ambiguity. This phenomenon is
more apparent in the bottom row. Here, the horizon set to 50 and samples to 100. In panel b. MaxDiff
does not have access to the reward/model and the policy is learned – these experiments are developed to
check consistency of success rates. The learning rate is set to 0.0005 and 100000 data points are used to
learn the policy, consistently withthe MaxDiffcode forthe point massexample. Data arecorrupted by the
same bias used in DR-FREE experiments. The top row shows the behavior of the agent using the learned
policy when horizon is 2 and samples 50 (as in Fig. 4c). The robot behavior is consistent with the one in
Fig. 4c. The bottom row has horizon set to 20 and samples to 50. The learned policy achieves results that
are consistent with the top row (panel a). In all panels, initial positions are the same as the ones in the
main text. See Methods (at the end of Experiments settings) and Supplementary Information (Sec. S5 –
MaxDiff settings) for the details.
S1 Background
Sets and operators are in calligraphic characters and vectors in bold. A random variable is denoted by V
and its realization is v. The symbol ∶= denotes a definition. We consider continuous (discrete) random
variables and we denote the probability density function (pdf) of V by p v (for discrete variables, p v
is probability mass function, pmf). The convex subset of pdfs (pmfs) is denoted by D. The expectation
( ) ( )
of a function h ⋅ of a continuous V is denoted E h V ∶= h v p v dv, where the integral is over
p ∫v
the (compact) support of p v , which we denote by suppp; whenever it is clear from the context, we
() [ ( )] ( ) ( )
omit the subscript in the sum (for discrete variables, the integral is replaced with the sum). The joint
( )
pdf/pmf of V and V is denoted by p v ,v ; the conditional pdf/pmf of V with respect to (w.r.t.) V
1 2 1 2 1 2
is p v v . Given p v and q v , we say that p v is absolutely continuous with respect to (w.r.t.) q v
1 2
( )
if su
(
ppp
∣
⊆s
)
uppq. We
(
de
)
note th
(
is
)
by writing p≪
(
q.
)
Countable sets are denoted by w k k1 ∶kn , where w
(
k i
)
s
the generic set element, k (k ) is the index of the first (last) element and k ∶k is the set of consecutive
1 n 1 n
{ }
integers between (including) k and k . Finally, we denote by L1 V the space integrable functions on V
1 n
and we use the shorthand notation a.s. for almost surely.
( )
The Kullback-Leibler Divergence. We recall the definition for the Kullback-Leibler (KL) divergence2
Definition S1.1. The KL divergence of p v w.r.t. q v with p≪q is:
( ) ( ) p v
DKL p q ∶=
∫ v
p v ln
q v
dv.
( )
( ∣∣ ) ( ) ( )
Intuitively, the KL divergence is a measure of the proximit(y )of the pair of pdfs. This is bounded only
if p≪q [3, Chapter 8]. Also, p,q ↦D p q is a jointly convex function and hence p↦D p q ,
KL KL
q ↦D p q are convex. We also recall the following chain rule for the KL divergence:
KL
( ) ( ∣∣ ) ( ∣∣ )
Lemma(S1∣.∣1.)Let V and Z be two random variables and let p v,z and q v,z be two joint pdfs. Then:
DKL p v,z q v,z =DKL p v q v +E p( v ( ) DK ) L p z ( v ) q z v .
( ( )∣∣ ( )) ( ( )∣∣ ( )) [ ( ( ∣ )∣∣ ( ∣ ))]
Finally, the following result is useful to characterize the feasibility domain of our control problem. In
the statement, adapted from [4, Proposition 2.1], M V is the subset of probability measures on V ⊆Rk
Lemma S1.2. For any µ∈M V , V ⊆Rk and any M( )<∞, the set
( ) ν ∈M V ∶ DKL ν µ ≤M ,
{ ( ) ( ∣∣ ) }
is compact with respect to weak convergence of probability measures.
S2 Relating DR-FREE with other frameworks
We recall that DR-FREE computes the policy via the distributionally robust generalization of the free
energy principle in Fig. 2a. This formulation – offering the problem statement for policy computation
– explicitly embeds ambiguity constraints to account for (model) ambiguity. The formulation yields a
distributionally robust sequential policy optimization problem featuring the free energy as objective and
ambiguity constraints formalized via the KL divergence. Both the objective and constraints are nonlinear
SI-9
in the decision variables ofthe problem. The constraintsexplicitly definean ambiguity set withradius that
can depend on both state and action. In the main text we showed that the policy optimization problem
in Fig. 2a tackled by DR-FREE has the same optimal solution as
min max D KL p 0∶N q˜ 0∶N (SI-1)
{πk( u k∣ x k−1)}1∶N pk( x k∣ x k−1,u k) ∈B η(p¯ k( x k∣ x k−1,u k))
( ∣∣ )
We then showed that – with a proper choice of q˜ 0∶N – the MaxDiff objective is a relaxation of the above
problem when there is no ambiguity. Starting from this finding – and building on the links highlighted in
the main text and in the Methods – we now connect DR-FREE with other related robust decision-making
frameworks, placing it in the context of the broad literature on maximum entropy, variational inference,
control-as-inference, distributionally robust learning and optimization.
As highlighted in the main text, MaxDiff can generalize5,6 maximum entropy7,8 (MaxEnt) inheriting
thedesirablepropertiesofMaxEntpolicies. Aremarkablepropertyispolicyrobustnesstoperturbationsin
thedynamics.9 Formally,MaxEntpoliciesmaximizealowerboundofthedistributionallyrobustobjective9
N
max min E r˜ X ,U (SI-2)
{πk( u k∣ x k−1)}1∶N pk( x k∣ x k−1,u k) ∈P˜
p0∶N
k
∑
=1
k k
[ ( )]
In (SI-2), r˜is a pessimistic reward,9 this means that to compute policies that robustly maximize a reward
MaxtEnt must be used with a different (pessimistic) reward function. Also, in (SI-2) the ambiguity set
P˜ is not arbitrary; rather, its radius9 is constant – it directly depends upon the entropy of the optimal
policy, which is the solution of MaxEnt itself and explicit bounds are available only in discrete settings.
The bounds come from the fact that, to obtain robustness estimates, the authors9 do not solve the full
problem in (SI-2) but rather obtain a lower bound for the inner minimization. DR-FREE solves the
problem in Fig. 2a – and hence in (SI-1). In doing so, DR-FREE defines robustness against ambiguity
in the problem formulation, via the ambiguity set. This means not only that DR-FREE policy is robust
across this ambiguity set, but also that DR-FREE does not need to be applied on a pessimistic policy. The
key enabler to achieve these desirable properties is the ability to solve the robust free energy formulation
in Fig. 2a. As we detail next, we are not aware of any other approach to solve this problem in the broad
literature on robust decision-making. Namely, our suvey highlights that, in this broader landscape, there
is no method that can solve the full min−max problem in Fig. 2a. We take as a starting point MaxEnt
– which we discussed in the main text. While distributionally robust versions of imitation RL based on
this principle have been proposed, results are obtained10 by assuming that – besides finite state spaces –
the underlying optimization features a cost functional and constraints that are both linear in the decision
variable of the inner optimization problem. This does not hold for the functional and constraints of our
robust free energy principle. More broadly, as also surveyed in the literature11–13,16–18,20 the linearity
assumption on the objective function in one of the decision variables is crucial for other state-of-the-art
policy computation methods across learning and control. This includes highly influential distributionally
robust frameworks such as distributionally robust Q-learning.21 In this work, while ambiguity constraints
are formalized via KL divergence – as in DR-FREE – the optimization objective benefits from being
linear in the decision variable of the inner problem. As a result, distributionally robust Q-learning cannot
compute a policy that solves the full problem in Fig. 2a. KL divergence is also widely used as a cost
functionalinthecontextofvariationalinference(andcontrol-as-inference). However,alsointhisliterature,
when distributionally robust frameworks are proposed, in line with the distributionally robust Q-learning
setting, the KL divergence (and the entropy) is included in the problem in a way that guarantees linearity
SI-10
of the cost with respect to the inner decision variable.14 Finally, we also position DR-FREE – and its
contributions – in the context of the rich literature on distributionally robust optimization (DRO). Since
Scarf’spioneeringwork,15 DRO(andBayesianDRO)hasemergedasacentraltopicinrobustoptimization
– gaining increasing attention in the context of machine learning.22–26,29 DRO concepts have developed
significantly over the years and we refer to surveys27,28 for an overview of this vast literature. In these
works – and surveys – the optimization problem benefits from the cost being linear in at least one decision
variable – thus ruling out having free energy as objective. More recently,24 nonlinear DRO problems are
considered and a Frank-Wolfe algorithm has been introduced to tackle a class of problems featuring cost
functions that have a continuous Lipschitz G-derivative. This assumption does not hold for the free energy
objective of the distributionally robust free energy principle in Fig. 2a.
Insummary,DR-FREEallowstosolvedistributionallyrobust,min−max,policyoptimizationproblems
having free energy as cost functional and, at the same time, distributionally robust constraints defined
via the KL divergence. This contribution – interesting per se – does not just bring broad theoretical
implications. As discussed in the main text, it is this key advancement that enables DR-FREE to retain
the desirable theoretical properties of state-of-the-art – provably robust – methods such as MaxEnt and
MaxDiff, guaranteeing these properties over the ambiguity set defined in the problem formulation.
S2.1 DR-FREE and the MDP formalism
Following the literature on distributionally robust Markov Decision Processes,21,30 we complement the
above survey by relating the sequential policy computation problem tackled in DR-FREE (Fig. 2a) and
the MDP formalism. MDPs can be typically defined according to a 6-tuple30 T,γ,X,U,p˜,r , where: (i)
T is the decision horizon; (ii) γ ∈ 0,1 is the discount factor; (iii) X and U are the state/action spaces;
( )
(iv) p˜ x k x k−1 ,u k is the probability of reaching x k from x k−1 after taking action u k ; (v) r is the expected
( ]
reward. These elements are directly mapped onto DR-FREE policy computation problem. Namely, in
( ∣ )
DR-FREE: (i) T is the decision horizon (N in Fig. 2a); (ii) γ = 1; (iii) X and U are again the state and
actionspaces(seealsoatthebeginningofSec. S3); (iv)p˜ x k x k−1 ,u k isp k x k x k−1 ,u k ; (v)thereward
is the negative of the cost. Following standard distributionally robust MDP frameworks,21 an adversarial
( ∣ ) ( ∣ )
picks – within an ambiguity set – the worst-case transition model that minimizes the cumulative reward.
In DR-FREE it maximizes the cumulative cost in the ambiguity set. That is:
N
{pk( x k∣ x m k− a 1 x ,u k)}1∶N D KL p 0∶N q 0∶N +E p0∶N k ∑ =1 c( k x) X k +c( k u) U k
s.t. p k x k x k− ( 1 ,u k ∣∣∈B η )p¯ k x k x [ k−1 ,u k (, ∀)k =1,.(..,N)].
This is the inner maximization probl(em ∣in Fig. 2a) with(am(bigu∣ity set de)fi)ned in Methods.
S3 Resolution Engine Details
We now describe the formal details of the engine in Fig. 2b (proofs in Sec. S6). The state-space is X⊆Kn
and the action space is U⊆Kp. The spaces can be both continuous and discrete (we let K be either R or
Z). Also, we make the following two assumptions:
A1 p k x k x k−1 ,u k andq k x k x k−1 ,u k areboundedandsuppp¯ k x k x k−1 ,u k ⊆suppq k x k x k−1 ,u k ;
( ∣ ) ( ∣ ) ( ∣ ) ( ∣ )
SI-11
A2 thestatecostisnon-negativeupperboundedinX, theactioncostisnon-negativelowerboundedinU.
Our starting point is the optimization problem from the Methods, also reported here for convenience:
min D π(u) q(u) +E c(u) U + maxE D p(x) q(x) +E c¯ X
π(u) ∈D KL k∣k−1 k∣k−1 π k (u ∣k ) −1 k k p(x) π k (u ∣k ) −1 KL k∣k−1 k∣k−1 p( k x ∣k ) −1 k k
k∣k−1 k∣k−1
( ∣∣ ) [ ( )] [ ( ∣∣ ) [ ( )]]
s.t. p( k x ∣k ) −1 ∈B η p¯ k x k x k−1 ,u k .
(SI-3)
( ( ∣ ))
We first show how (SI-3) is tackled in DR-FREE. Then we show that this problem is the right one that
should be solved to find the optimal policy in accordance with the robust free energy principle.
Tackling SI-3. At each k, according to (SI-3) the optimal policy can be found by first finding the optimal
valueoftheinnermaximizationproblemandthenminimizingoverthepolicies. DR-FREEreliesonfinding
the optimal value of the inner problem via a convenient scalar reformulation. To obtain the reformulation,
we first show that for the inner problem in (SI-3) the expectation and maximization can be swapped.
Intuitively, this is possible because the feasibility domain is well-behaved (convex and compact, see Sec.
S6.2) and the decision variable of the optimization problem does not depend on the pdf over which the
expectation is taken. This intuition is formalized with the following result:
Lemma S3.1. The optimal solution, p( k x ∣k ) − ,⋆ 1 ∶= p ⋆ k x k x k−1 ,u k , for the inner maximization problem in
(SI-3) exists and it holds that:
( ∣ )
E π
k
(u
∣k
)
−1
DKL p( k x ∣k ) − ,⋆ 1 q k (x ∣k ) −1 +E p
k
(x
∣k
)
−
,⋆
1
c¯ k X k
=
p( k x ∣k ) −1
[
∈B η(p¯
m
k(
( a
x
x
k∣ x k−1,
∣
u
∣
k))
E
π k (
)
u ∣k ) −1
DKL [ p(
k
x
∣
(
k
)
−1
)] q ]
k
(x
∣k
)
−1
+E
p( k x ∣k ) −1
c¯
k
X
k
.
[ ( ∣∣ ) [ ( )]]
Essentially, the above result establishes that the maximization step can be performed by first solving
the problem in green in Fig. 2b and then taking the expectation. Still, this is an infinite dimensional
optimization problem that we seek to conveniently recast as a scalar optimization. To do so, we leverage
a change of variables technique29 using the likelihood ratio (or Radon-Nikodym derivative)
p(x)
r k∣k−1 ∶=
p¯
k
(x
∣k
)
−1 . (SI-4)
k∣k−1
With this change of variables, and exploiting the fact that the frontier of the feasibility domain is the set
of all p( k x ∣k ) −1 ∈D such that D KL p k (x ∣k ) −1 p¯ k (x ∣k ) −1 =η k x k−1 ,u k , we obtain the following:
Lemma S3.2. Consider the pr(oblem i∣∣n green)in Fi(g. SI-3. )Then:
p( k x ∣k ) −1 ∈B η(p¯ m k( a x x k∣ x k−1,u k)) DKL p k (x ∣k ) −1 q k (x ∣k ) −1 +E p( k x ∣k ) −1 c¯ k X k =η k x k−1 ,u k −m α≥ a 0 x r k m ∣k− i 1 n ∈R L r k∣k−1 ,α ,
( ∣∣ ) [ ( )] ( ) ( )
(SI-5)
SI-12
where L r k∣k−1 ,α is given by
( ) q(x)
L r k∣k−1 ,α =E p¯( k x ∣k ) −1 ⎡ ⎢ ⎢ r k∣k−1 ln p¯ k ( k x ∣ ∣ k k ) − − 1 1 −r k∣k−1 c¯ k X k +α r k∣k−1 lnr k∣k−1 ⎤ ⎥ ⎥ −αη k x k−1 ,u k . (SI-6)
( ) ⎢ ( ) ( )⎥ ( )
⎢ ⎥
Theresultestablishest ⎢ ⎣hatthecostofambiguityisη k x k−1 ,u k −max α≥0 ⎥ ⎦min r k∣k−1 ∈R L r k∣k−1 ,α . Next,
we need to find the optimal value of −max α≥0 min r k∣k−1 ∈ ( R L r k∣k− ) 1 ,α . This is c˜ x k−1 ,u k ( in Fig. ) 2b. To
this aim, using (SI-6) we have:
( ) ( )
−m
α≥
a
0
x
r k
m
∣k−
i
1
n
∈R
L r k∣k−1 ,α =
( ) q(x) (SI-7)
−m α≥ a 0 x −η k x k−1 ,u k α+ r k m ∣k− i 1 n ∈R E p¯( k x ∣k ) −1 ⎡ ⎢ ⎢ r k∣k−1 ln p¯ k ( k x ∣ ∣ k k ) − − 1 1 −r k∣k−1 c¯ k X k +αr k∣k−1 lnr k∣k−1⎤ ⎥ ⎥ .
( ) ⎢ ( ) ⎥
⎢ ⎥
⎢ ⎥
Then, we define ⎣ ⎦
q(x)
W α x k−1 ,u k ∶= r k m ∣k− i 1 n ∈R E p¯( k x ∣k ) −1 ⎡ ⎢ ⎢ r k∣k−1 ln p¯ k ( k x ∣ ∣ k k ) − − 1 1 −r k∣k−1 c¯ k X k +αr k∣k−1 lnr k∣k−1⎤ ⎥ ⎥ , (SI-8)
( ) ⎢ ( ) ⎥
⎢ ⎥
⎢ ⎥
so that the problem in (SI-7) can be writte⎣n as ⎦
min η k x k−1 ,u k α−W α x k−1 ,u k . (SI-9)
α≥0
( ) ( )
The c˜ x k−1 ,u k in Fig. 2b is the optimal value of the above problem. The following theorem states that
c˜ x k−1 ,u k can be found by solving a scalar and convex optimization problem:
( )
T(heorem)S3.1. For each x k−1 and u k , the optimal value of the problem in (SI-9) is finite and given by:
c˜ x k−1 ,u k ∶=minV˜ α x k−1 ,u k , (SI-10)
α≥0
( ) ( )
where
1
p¯(x) expc¯ x α
V˜ α x k−1 ,u k = ⎧ ⎪⎪⎪⎪⎪⎪⎪⎪ αlnE p¯( k x ∣k ) −1 ⎡ ⎢ ⎢
⎢ ⎢
⎛
⎜
k∣k−1
q k (x ∣k ) −1
k
(
k
)⎞
⎟
⎤ ⎥ ⎥
⎥ ⎥
+αη k
(
x k−1 ,u k
)
, α>0 (SI-11)
with
( ) ⎨ ⎪⎪⎪⎪⎪⎪⎪⎪ M
(
x k−1 ,u k ⎢ ⎢
⎣
)
⎝, ⎠ ⎥ ⎥
⎦
α=0,
⎩
p¯(x)
expc¯ x
M x k−1 ,u k ∶= limsup ln
k∣k−1 k k
≥0. (SI-12)
x k ∈suppp¯( k x ∣k ) −1 ⎛ q k (x ∣k ) −1 ( )⎞
( ) ⎜ ⎟
⎝ ⎠
Theaboveresultsshowsthat, inordertodetermineitsoptimaldecisionamidenvironmentalambiguity,
the agent does not need to know what is the worst case p k x k x k−1 ,u k . Rather, the agent only needs to
know what is the cost of ambiguity, i.e., c˜ x k−1 ,u k and η k x k−1 ,u k . In our experiments (see Results)
( ∣ )
we computed c˜ x k−1 ,u k , and hence built the cost of ambiguity in Fig. 2b, by solving the optimization
( ) ( )
problem in (SI-10). This problem is the one included in the algorithm deployed on the agents (reported in
( )
Section S5). Moreover, we did not have to build any specialized software to solve this problem because it is
SI-13
convex and admits a global minimum. We can claim this because of the next result, which also establishes
that c˜ x k−1 ,u k is bounded. To introduce the result we define explicitly
( ) 1
p¯(x) α
V α ( x k−1 ,u k ) =αlnE p¯( k x ∣k ) −1 ⎡ ⎢ ⎢ ⎢ ⎢ ⎛ ⎜ q k k (x ∣ ∣ k k ) − − 1 1 ⎞ ⎟ exp ( c¯ k ( α x k ) ) ⎤ ⎥ ⎥ ⎥ ⎥ +αη k ( x k−1 ,u k ) . (SI-13)
⎢⎝ ⎠ ⎥
⎢ ⎥
Theorem S3.2. For each x k−1 and u k , the⎣following statements hold⎦.
1. lim V α x k−1 ,u k =+∞. Also, V α x k−1 ,u k >0 for every α>0;
α→∞
2. V α x k−1 (,u k is:)(i) linear and equ(al to lnc¯+)η k x k−1 ,u k α, if there exists some constant, say c¯, such
that p¯( k x ∣k ) −1 expc¯ k( x k) =c¯; (ii) strictly convex on 0,+∞ otherwise;
( q(x)) ( )
k∣k−1
( )
3. limV α x k−1 ,u k =M x k−1 ,u k , where M x k−1 ,u k is defined by (SI-12);
α→0
4. V˜ α x k− ( 1 ,u k ha ) s a glo ( bal minim ) um on 0, (+∞ . )
Insum( mary,th)eaboveresultsshowthatth[eoptim) alvaluefortheinnermaximizationproblemin(SI-3)
is E π(u) η k X k−1 ,U k +c˜ X k−1 ,U k and c˜ x k−1 ,u k can be effectively computed via convex optimiza-
k∣k−1
tion. This yields the problem in Fig. 2b where the variational free energy is minimized across the policies.
[ ( ) ( )] ( )
The next step is to show that (SI-3) is the right problem to solve and, if so: (i) give the optimal solution
of the minimization problem; (ii) establish how c¯ x is built. This is done next.
k k
( )
The optimal policy. The next result shows why, at each k, problem in (SI-3) needs to be solved, gives
the optimal policy and determines what is the lowest free energy that the agent can achieve.
Corollary S3.1. The distributionally robust free energy principle yields the optimal policy
π(u),⋆
, with
k∣k−1
{ }
π k (u ∣k ) − ,⋆ 1 =π k ⋆ u k x k−1 =π k (u ∣k ) − ,⋆ 1 = ∫ q k q (u ∣ k ( k ) u ∣ − k ) − 1 1 ex ex p p ( − − η η k k x( x k− k− 1 , 1 u ,u k k ) − − c˜ c˜ x( x k− k− 1 , 1 u ,u k k ) − − c( k c u ( k ) u) u( u k k )) du k , (SI-14)
( ∣ )
( ( ) ( ) ( ))
where
p¯(x)
exp c¯ x
c˜ x k−1 ,u k =min limsup ln
k∣k−1 k k
,
⎧ ⎪⎪⎪ x
k
∈suppp¯(
k
x
∣k
)
−1
⎛ q
k
(x
∣k
) (−1 ( ))⎞
( )
m α> i 0 n η k
⎨ ⎪⎪⎪
⎩ ( x k−1 ,u k ) α+α
⎜
⎝ lnE p¯( k x ∣k ) −1 ⎡ ⎢ ⎢ ⎢ ⎢ ⎛ ⎜ p q ¯ k ( k ( x x ∣ ∣ k k ) ) − − 1 1 ⎞ ⎟
⎟
⎠α
1
exp ( c¯ k ( α x k ) ) ⎤ ⎥ ⎥ ⎥ ⎥ ⎫ ⎪⎪⎪⎪ ⎬ ;
(SI-15)
and ⎢ ⎢ ⎝ ⎠ ⎥ ⎥ ⎪⎪⎪⎪
⎣ ⎦⎭
c¯ k x k =c( k x) x k +cˆ k+1 x k , (SI-16)
with ( ) ( ) ( )
cˆ N+1 x N =0,
(SI-17)
cˆ k x(k−1) =−ln ∫ q k (u ∣k ) −1 exp −η k x k−1 ,u k −c˜ x k−1 ,u k −c( k u) u k du k , k ∈1∶N.
( ) ( ( ) ( ) ( ))
SI-14
Moreover, the optimal cost, i.e., the smallest free energy that can be achieved, is bounded and given by
∑ E p( x k−1) cˆ k X k−1 . (SI-18)
k∈1∶N
[ ( )]
S4 Why It Is Better to Be Ambiguity-Free
We give the formal details explaining why an agent interacting with an ambiguous environment cannot
outperform an agent that has no environmental ambiguity. First, we characterize what happens to the
problem in the left hand-side of (SI-25) as the ambiguity set shrinks. Then, we discuss what this implies
for the optimal policy. We recall that the radius of ambiguity is bounded and therefore there is some
η <+∞ such that η k x k−1 ,u k ≤η. Also, the optimal value of the problem in (SI-25) is c˜ x k−1 ,u k , which
is given in Theorem S3.1, and we use the notation c˜ η x k−1 ,u k to stress the dependency of c˜ x k−1 ,u k on
( ) ( )
η. We also discuss why the optimal policy down-weights the model over the ambiguity radius (see main
( ) ( )
text) when ambiguity increases.
Theorem S4.1. Let the set-up of item 2(ii) in Theorem S3.2 hold. Then:
η li → m 0 c˜ η x k−1 ,u k =DKL p¯( k x ∣k ) −1 q k (x ∣k ) −1 +E p¯( k x ∣k ) −1 c¯ k X k . (SI-19)
( ) ( ∣∣ ) [ ( )]
We characterize what happens to c˜ x k−1 ,u k as ambiguity vanishes. Next, we establish that an
ambiguity-aware agent making optimal decisions necessarily outperforms an agent affected by ambigu-
( )
ity. In essence, DR-FREE forbids that ambiguity is exploited to achieve better performance.
Lemma S4.1. The cost of an agent affected by ambiguity cannot be better than the optimal cost of an
ambiguity-free agent. That is,
−ln
∫ U
q
k
(u
∣k
)
−1
exp −DKL p¯(
k
x
∣k
)
−1
q
k
(x
∣k
)
−1
−E
p¯( k x ∣k ) −1
c¯
k
x
k
−c(
k
u) u
k
du
k
(SI-20)
<−ln ∫ U q k (u ∣k ) −1 ex(p −η k (x k−1 ,u∣ k ∣ −c˜ x) k−1 ,u k −[c( k u() u)] k du k ( )) (SI-21)
( ( ) ( ) ( ))
We recall from the main text that characterizing the policy when ambiguity increases amounts at
studyingwhathappenswhenη min =minη k x k−1 ,u k increases. Notethat,fromTheoremS3.1,c˜ x k−1 ,u k
is independent on η k x k−1 ,u k when η min is sufficiently large. In fact, from the proof of Theorem S3.2(i)
( ) ( )
the left hand side in the first line of (SI-11) is greater than αη k x k−1 ,u k . Therefore, for η min sufficiently
( )
large, the exponents in DR-FREE policy become −η k x k−1 ,u k , thus yielding the results observed in the
( )
main text.
( )
S5 Supplementary Details For The Experiments
The hardware used for the in-silico experiment results was a laptop with an 12th Gen Intel Core i5-12500H
2.50 GHz processor and 18 GB of RAM. A comparative table of the time taken, at each step, to compute
the control action for each of the policy computation methods from the main text is reported at the end
of the Supplementary Information (Tab. S-3).
DR-FREE deployment. The pseudocode implementing the resolution engine deployed on the agents
is given in Algorithm 1. We first discuss the setting for the robot experiments. The Inputs line specifies
SI-15
what information, given the current position/state, DR-FREE has available at each k. The algorithm out-
⋆
puts (line 4) the policy π k u k x k−1 , which we recall being a soft-max. The exponential in the soft-max
contains the radius of ambiguity, η k x k−1 ,u k , and c˜ x k−1 ,u k . This latter term is computed in lines
( ∣ )
1−3, which directly follow from the statement of Theorem S3.1. In the experiments, the input space
( ) ( )
is discretized in a 5×5 grid. In our experimental deployment, η k x k−1 ,u k is defined as described in
the Methods (Experiments settings); as highlighted in the Discussion and in the Methods, in the current
( )
implementation, the radius of ambiguity is provided to DR-FREE rather than learned. As also outlined
in the Methods, the η k x k−1 ,u k used in the experiments is set to capture higher agent confidence (i.e.,
lower ambiguity) as the agent gets closer to its goal destination. Supplementary Fig. 2 shows that, ef-
( )
fectively, p k x k x k−1 ,u k is always inside the ambiguity set. All other inputs to the algorithm defined
within the Methods. In Fig. 3, the generative model q k x k x k−1 ,u k for the experiments – a Gaussian
( ∣ )
centered in the goal position – is goal-encoding. Instead, for the DR-FREE experiments reported in Fig. 4,
( ∣ )
q k x k x k−1 ,u k is set to p max x k x k−1 from the MaxDiff framework as described in Results. See Code
Availability. In the accompanying code, we used scipy solver to tackle the optimization and we verified
( ∣ ) ( ∣ )
numerically that it would achieve the global minimum. In line 1 of Algorithm 1, the max was computed
by obtaining samples from p¯ k x k x k−1 ,u k and evaluating the expression on the right-hand side for each
sample. The largest value was then selected as maximum.
( ∣ )
Algorithm 1: Pseudocode for DR-FREE resolution engine deployed on the robots
Result: Robot policy
Input: q k x k x k−1 ,u k , q k u k x k−1 , p¯ k x k x k−1 ,u k , η k x k−1 ,u k , c( k x) x k
⋆
Output: π (k u ∣ k x k−1 ) ( ∣ ) ( ∣ ) ( ) ( )
1 M x k−1 ,u k ( ← x k ∣ ∈suppp ) ¯ m k( a x x k∣ x k−1,u k) ln p¯ k( x k∣ q x k k ( − x 1 k ,u ∣ x k k ) − e 1 x , p u k c( k ) x) ( x k) ;
1
2 V˜ α
(
x k−1 ,u k
)
← ⎧ ⎪⎪⎪⎪⎪⎪⎪⎪ αlnE p¯ k( x k∣ x k−1,u k)⎡ ⎢ ⎢
⎢ ⎢
⎛
⎜
(
p¯ k ( x k ∣ x k−1 q , k ( u x ∣k k ) −)1 expc( k
x)
) ( x k )⎞
⎟
α
⎤ ⎥ ⎥
⎥ ⎥
+αη k
(
x k−1 ,u k
)
, α>0
3 c˜ x
(
k−1 ,u k ←
)
m
α≥
⎨ ⎪⎪⎪⎪⎪⎪⎪⎪
⎩
i
0
n
M
V˜ α
(
x
x
k
k
−
−
1
1
,
,
u
u
k
k
)
,
;
⎢ ⎢
⎣
⎝ ⎠ ⎥ ⎥
⎦
α=0;
4 π k( ⋆ u k x k)−1 ← ∫ q q k k ( ( u( u k k ∣ x ∣ x k k − − 1 1 ) ) e e x x p) p ( ( − − η η k k ( ( x x k k − − 1 1 ,u ,u k k ) ) − − c˜ c ( ˜( x x k k − − 1 1 ,u ,u k k ) ) ) ) du k ;
Robot trajectories used to obtain the trained model at each phase of training are shown in Fig. SI-8.
( ∣ )
The trajectories were obtained by sampling actions from a uniform distribution and then injecting in the
position a bias equal to 0.1x k−1 . See Code Availability for the data used for training.
Ambiguity-unaware agent deployment. The optimal policy from the literature19 is given by
exp −D KL p¯ k x k x k−1 ,u k q k x k x k−1 ,u k −E p¯ k( x k∣ x k−1,u k) c( k x) x k
.
∑u k ex(p −D K(L p¯(k x∣k x k−1 ,u)k∣∣ q (k x∣k x k−1 ,u))k −E p¯ k( x k∣ x k−1,u[ k) c( k x ( ) x)]k)
( ( ( ∣ )∣∣ ( ∣ )) [ ( )])
In the experiments, the divergence was considerably higher than the expected cost, which accounted
for the presence of obstacles. As a result, in the policy, the exponent was approximately equal to
exp −D KL p¯ k x k x k−1 ,u k q k x k x k−1 ,u k . Since in the experiments q k x k x k−1 ,u k is a Gaus-
sian centered in the goal destination, this policy would direct the robot along the shortest path towards
( ( ( ∣ )∣∣ ( ∣ ))) ( ∣ )
the goal without taking into account the presence of the obstacles and thus explaining the behavior ob-
SI-16
Figure SI-8: Position data collected for training at each training stage. The corresponding trained model
is detailed in the Methods.
served in the main text. In the accompanying code, the policy is computed with a numerical method that
prevents underflow due to large divergences.
Belief update. The optimization problem solved to obtain the belief update results is
M F F G
min ∑ − ∑ w i f i xˆ k−1 ,uˆ k +ln ∑ q k u k x k−1 =xˆ k−1 exp ∑ w i f i xˆ k−1 ,u k + ∑ v i g i u k .
v,w
k=1
⎛
i=1 u
k
i=1 i=1
⎞
( ) ( ∣ ) ( ( ) ( ))
This problem⎝– derived by dropping the first term from the negative log-likelihood in the Metho⎠ds – is
convex. Data points used to reconstruct the cost and code available (see Code Availability). The code
builds on the one from.19
Increasing ambiguity. Fig. 3e gives a screenshot of the policy when the ambiguity radius is increased,
without clipping, as detailed in the main text. When exp −η k x k−1 ,u k leads to underflows, we replace
0with1e-10. Thefigurewasobtainedbydiscretizingtheinputspaceina50×50grid. SeeCodeAvailability.
( ( ))
Decreasing ambiguity. As in Fig. 3e, Supplementary Fig. 5c and Supplementary Fig. 6b were obtained
by discretizing the input space in a 50×50 grid. As described in the main paper, when the ambiguity
radius is zero (the ambiguity constraint is relaxed) DR-FREE policy can be conveniently computed by
simply setting c˜ x k−1 ,u k as in (SI-19). Fig. SI-9 confirms that, as the radius decreases, DR-FREE policy
approaches the optimal policy for the relaxed problem also known in the literature.19
( )
MaxDiff settings. We use the code provided in the original MaxDiff paper,5 interfacing it with the
Robotarium. We created a wrapper around the Robotarium environment so that this could be compatible
with the MaxDiff available code structure. In the original code, the MaxDiff authors provide a flag to let
MaxDiff use the real environment (reward and model) rather than learning it from data. In the MaxDiff
experiments of the main text and Supplementary Fig. 7 (panel a) MaxDiff has access to the reward and
model and this is achieved by properly setting the flag. For the experiments in the Supplementary Fig. 7
(panel b) the flag value is changed so that MaxDiff learns reward and model. See Code Availability for the
code to replicate the results. Tab. S-1 contains the MaxDiff hyperparameters used to obtain the results
described in the paper.
SI-17
Figure SI-9: Mismatch between DR-FREE policy at different ambiguity levels and the optimal policy
from the literature.19 The mismatch is quantified using the KL divergence between the two policies. Blue
curve corresponds to Supplementary Fig. 5 and red curve to Supplementary Fig. 6, respectively. The
x-axis shows the scaling factor for the ambiguity radius and the y-axis the KL divergence. Values are for
radii decreased by 50%, 70%, 90%, 99% and for a radius of 0. The dashed lines connecting the points are
interpolations. When the radius is zero, the KL divergence is also zero – the two policies are the same.
Ant experiments settings. The task consists in moving the Ant forward, in the x-coordinate direction.
ThecovariancematrixforthegenerativemodelinDR-FREEisprovidedinourrepository. FortheMaxDiff
and NN-MPPI implementations, we use the code from the literature.5 In MaxDiff and NN-MPPI – both
having access to the environment reward – the horizon is set to 2 in accordance with the other MaxDiff
experiments from the main text. The other hyperparameters, summarized in Tab. S-4, are taken as in
the Ant experiments from the literature.5 The table also reports the architecture of the neural network
providing mean and variance for p¯ k x k x k−1 ,u k . This is the same architecture used in the MaxDiff
paper5 for the Ant experiments. The ambiguity radius, capturing higher confidence as the Ant moves
( ∣ )
forward, is the KL divergence between a Gaussian centered at a torso x-velocity of 1 m/s and a Gaussian
whose mean and variance are obtained by the neural network for the torso x-velocity. Fig. SI-10 shows
that p k x k x k−1 ,u k is always inside the ambiguity set. See repository for details.
( ∣ )
S6 Proving The Statements
We now give the detailed proofs for the formal statements in the Supplementary Information. Proofs are
presentedforcontinuousvariables; thediscretecasefollowsanalogousarguments. First, weintroducesome
instrumental results.
S6.1 Instrumental Results
The following result, Bauer’s Maximum Principle, is adapted from [31, Corollary A.4.3]. The result is
instrumental to tackle concave programs that arise within the proof of our results when maximizing free
energy across all possible environments in the ambiguity set
SI-18
Figure SI-10: p k x k x k−1 ,u k belongs to the ambiguity set B η p¯ k x k x k−1 ,u k . As in Supple-
mentary Fig. 2, the plot confirms this by showing that η k x k−1 ,u k – in red – is always bigger than
D KL p k x k x k−1 ,u( k ∣p¯ k x k x) k−1 ,u k – in green. The figure, obta(ine(d fro∣m one ex)p)eriment, is repre-
sentative for all the Ant experiments. ( )
( ( ∣ )∣∣ ( ∣ ))
Lemma S6.1. Let: (i) K be a nonempty compact set in a Hausdorff locally convex topological vector space
X; (ii) ∂K be the frontier of K; (iii) f ∶ X → R be an upper-semi-continuous convex function. Then,
maxf =maxf.
K ∂K
We also leverage the following Lagrange Duality result [32, Section 8.6, Theorem 1]
Theorem S6.1. Let: (i) X be a linear vector space; (ii) Ω be a convex subset of X; (iii) f be a real-valued
convex functional on Ω; (iv) G be a real convex mapping of X. Assume that:
1. there exists some x˜∈Ω such that G x˜ <η;
2. µ ∶= minf x , s.t G x ≤η,x∈Ω( i)s finite, i.e., µ <∞.
0 0
Then: { ( ) ( ) }
min f x =maxmin f x +α G x −η .
G(x) ≤η α≥0 x∈Ω
x∈Ω
( ) [ ( ) ( ( ) )]
S6.2 Finding The DR-FREE Policy
After characterizing the set B η p¯ k x k x k−1 ,u k , we give a proof for Lemma S3.1, which shows that in
(SI-3) the expectation and maximization can be swapped.
( ( ∣ ))
Lemma S6.2. The set B η p¯ k x k x k−1 ,u k in (SI-3) is convex and compact in L1 X equipped with the
usual norm.
( ( ∣ )) ( )
SI-19
Proof. We first show convexity. Pick any p˜ 1 ,p˜ 2 ∈ B η p¯ k x k x k−1 ,u k and a constant, say t ∈ 0,1 .
Then, by convexity of the KL divergence we have:
( ( ∣ )) [ ]
D KL tp˜ 1 + 1−t p˜ 2 p¯ k (x ∣k ) −1 ≤tD KL p˜ 1 p¯( k x ∣k ) −1 + 1−t D KL p˜ 2 p¯( k x ∣k ) −1 ≤η k x k−1 ,u k .
( ( ) ∣∣ ) ( ∣∣ ) ( ) ( ∣∣ ) ( )
This, together with the fact that supp tp˜ 1 + 1−t p˜ 2 ⊆q k (x ∣k ) −1 shows that B η p¯ k x k x k−1 ,u k is convex.
Now, we prove that the set is also compact and in what follows we let η < +∞ be a positive number
( ( ) ) ( ( ∣ ))
such that η k x k−1 ,u k ≤ η, ∀x k−1 ,u k . To this aim, we leverage Lemma S1.2. By Radon-Nikodym The-
orem [33, Theorem 6.10, page 121] it follows that for each probability measure ν ∈ M X , there exists
( )
a unique pdf, say p , such that ν E = p dλ for all measurable set E (λ is the Lebesgue measure on
ν ∫E ν
( )
X). In turn, by means of Lemma S1.2, we have that the set B˜ = ν ∈ M X ∶ D ν µ ≤ η is
η KL
( )
compact. Let p¯(x) be the pdf corresponding to µ and consider the functional Φ ∶ M X →L1 X , with
k∣k−1
{ ( ) ( ∣∣ ) }
Φ ν =px,v , (L1 X is equipped with the usual norm). We now prove that Φ is continuous. To this aim,
k∣k−1
( ) ( )
pick the measure ν ∈ M X given by ν → ν, i.e., for all E, one has that px,vn dλ → px,v dλ and
( ) ( )
n ∫E k∣k−1 ∫E k∣k−1
hence Φ ν →Φ ν in the usual L1 norm. This shows that Φ is in fact continuous. Moreover, continuous
n
( )
functions map compact sets into compact sets and, additionally, Φ B˜ η = B η p¯ k x k x k−1 ,u k . This
( ) ( )
yields the conclusion.
( ) ( ( ∣ ))
Next, we prove Lemma S3.1, where we establish that in (SI-3) the expectation and maximization can
be swapped.
Proof of Lemma S3.1. First, note that (as shown in Lemma S6.2) the set B η p¯ k x k x k−1 ,u k is
convex and compact. This, together with the continuity and convexity of the KL divergence in the decision
( ( ∣ ))
variable implies that
p(x),⋆
exists. By definition, we have
k∣k−1
max D p(x) q(x) +E c¯ X
p( k x ∣k ) −1 ∈B η(p¯ k( x k∣ x k−1,u k))
KL k∣k−1 k∣k−1 p(
k
x
∣k
)
−1
k k
=D KL p( k x ∣k ) − ,⋆ 1 q k (x ∣k ) −1 +E p (
k
(x
∣k
)
−
,⋆
1
c¯ k ∣∣ X k ) , [ ( )]
( ∣∣ ) [ ( )]
and hence
E max D p(x) q(x) +E c¯ X
π k (u ∣k ) −1 ⎡ ⎢ ⎢ p( k x ∣k ) −1 ∈B η(p¯ k( x k∣ x k−1,u k)) KL k∣k−1 k∣k−1 p( k x ∣k ) −1 k k ⎤ ⎥ ⎥
⎢ ( ∣∣ ) [ ( )]⎥
=E π
k
(u
∣k
) ⎢ ⎢ ⎣
−1
D KL p k (x ∣k ) − ,⋆ 1 q k (x ∣k ) −1 +E p
k
(x
∣k
)
−
,⋆
1
c¯ k X k ⎥ ⎥ ⎦ (SI-22)
≤ [ max ( ∣∣ E ) D p([x) ( q()x])] +E c¯ X .
p( k x ∣k ) −1 ∈B η(p¯ k( x k∣ x k−1,u k)) π k (u ∣k ) −1 KL k∣k−1 k∣k−1 p( k x ∣k ) −1 k k
[ ( ∣∣ ) [ ( )]]
Additionally, for each feasible
p(x)
,
k∣k−1
D p(x) q(x) +E c¯ X ≤ max D p(x) q(x) +E c¯ X .
KL k∣k−1 k∣k−1 p(
k
x
∣k
)
−1
k k
p( k x ∣k ) −1 ∈B η(p¯ k( x k∣ x k−1,u k))
KL k∣k−1 k∣k−1 p(
k
x
∣k
)
−1
k k
( ∣∣ ) [ ( )] ( ∣∣ ) [ ( )]
SI-20
That is, for each feasible
p(x)
,
k∣k−1
E D p(x) q(x) +E c¯ X
π
k
(u
∣k
)
−1
KL k∣k−1 k∣k−1 p(
k
x
∣k
)
−1
k k
[ ( ∣∣ ) [ ( )]]
≤E max D p(x) q(x) +E c¯ X .
π k (u ∣k ) −1 ⎡ ⎢ ⎢ p( k x ∣k ) −1 ∈B η(p¯ k( x k∣ x k−1,u k)) KL k∣k−1 k∣k−1 p( k x ∣k ) −1 k k ⎤ ⎥ ⎥
⎢ ( ∣∣ ) [ ( )]⎥
⎢ ⎥
⎢ ⎥
Hence we may continue th⎣e chain of inequalities in (SI-22) with ⎦
max E D p(x) q(x) +E c¯ X
p( k x ∣k ) −1 ∈B η(p¯ k( x k∣ x k−1,u k)) π k (u ∣k ) −1 KL k∣k−1 k∣k−1 p( k x ∣k ) −1 k k
[ ( ∣∣ ) [ ( )]] (SI-23)
≤E max D p(x) q(x) +E c¯ X ,
π k (u ∣k ) −1 ⎡ ⎢ ⎢ p( k x ∣k ) −1 ∈B η(p¯ k( x k∣ x k−1,u k)) KL k∣k−1 k∣k−1 p( k x ∣k ) −1 k k ⎤ ⎥ ⎥
⎢ ( ∣∣ ) [ ( )]⎥
⎢ ⎥
⎢ ⎥
Combining (SI-22) and (S⎣I-23) yields the desired conclusion. ⎦
Next, we obtain the scalar reformulation enabling the computation of the cost of ambiguity in Fig. 2b.
Before proving this result, consider the likelihood ratio in (SI-4). Note that, a.s., for every feasible
p(x)
,
k∣k−1
we have r k∣k−1 ≥0 and E p¯(x) r k∣k−1 =1. This motivates the following:
k∣k−1
[ ]
Definition S6.1. The set of likelihood ratios generated by p( k x ∣k ) −1 is R ∶= r k∣k−1 ∶ E p¯(
k
x
∣k
)
−1
r k∣k−1 =
1, r k∣k−1 ≥0, a.s. . { [ ]
We can now p}rove Lemma S3.2.
Proof of Lemma S3.2. The maximization problem in green in Fig. 2b can be recast as:
p¯(x)
r k m ∣k− a 1 x ∈R E p¯( k x ∣k ) −1 ⎡ ⎢ ⎢ r k∣k−1 ⎛ lnr k∣k−1 +ln q k k (x ∣ ∣ k k ) − − 1 1 +c¯ k X k ⎞ ⎤ ⎥ ⎥ (SI-24)
⎢ ⎜ ( )⎟⎥
s.t.E p¯(
k
x
∣k
)
−1
⎢ ⎢
⎣
r k∣k−1 l⎝nr k∣k−1 ≤η k x k−1 ,u k . ⎠ ⎥ ⎥
⎦
[ ] ( )
Also, from Lemma S6.1, by setting:
p¯(x)
• f r k∣k−1 ∶=E p¯( k x ∣k ) −1 r k∣k−1 lnr k∣k−1 +r k∣k−1 ln q k k (x ∣ ∣ k k ) − − 1 1 +r k∣k−1 c¯ k X k ;
( ) [ ( )]
• K ∶= r k∣k−1 ∶ r k∣k−1 ∈ R, E p¯(x) r k∣k−1 lnr k∣k−1 ≤ η k x k−1 ,u k , which is compact and convex.
k∣k−1
Compactness follows from Lemma S6.2, while convexity in the decision variable follows from the fact
{ [ ] ( )}
that E p¯(x) r k∣k−1 lnr k∣k−1 is convex w.r.t. r k∣k−1 ;
k∣k−1
• X ∶=L1 X , [ which is a top ] ological vector space;
( )
SI-21
we have that the optimal value of the problem in (SI-24) is given by:
p¯(x)
⎧ r k m ∣k− a 1 x ∈R E p¯( k x ∣k ) −1 r k∣k−1 lnr k∣k−1 +r k∣k−1 ln q k k (x ∣ ∣ k k ) − − 1 1 +r k∣k−1 c¯ k X k
⎪⎪⎪⎪⎪
s.t E p¯(x) r k∣k [−1 lnr k∣k−1 =η k x k−1 ,u k ( )]
⎨ k∣k−1
⎪⎪⎪⎪⎪
⎩ =η k x k−1 ,u
[
k + ⎧ ⎪⎪⎪⎪⎪ r
s
k
.
m
t
∣k−
E
a 1 x ∈
p¯
R
(x)
E
]
p¯( k x ∣
r
k ) −
k
1
∣k
(
[−
r
1
k
l
∣
n
k−
r
1
k
l
∣k
n
−
)p
1
q
¯
k
(
k (
x
x ∣ ∣ k k
)
)
=
− − 1 1
η
+
k
r
x
k∣
k
k
−
−
1
1
,
c¯
u
k
k(
X
.
k
)]
,
( ) ⎨ k∣k−1
which, by assumptions A1–A2, has a
⎪⎪⎪⎪⎪
⎩bounded op
[
timal value. A
]
lso, w
(
e have:
)
p¯(x) q(x)
⎧ r k m ∣k− a 1 x ∈R E p¯( k x ∣k ) −1 r k∣k−1 ln q k k (x ∣ ∣ k k ) − − 1 1 +r k∣k−1 c¯ k X k =− ⎧ r k m ∣k− i 1 n ∈R E p¯( k x ∣k ) −1 r k∣k−1 ln p¯ k ( k x ∣ ∣ k k ) − − 1 1 −r k∣k−1 c¯ k X k .
⎪⎪⎪⎪⎪
s.t E p¯(x) r k∣k [−1 lnr k∣k−1 =η k x k−1 ,u k( )]
⎪⎪⎪⎪⎪
s.t E p¯(x) r k∣k [−1 lnr k∣k−1 =η k x k−1 ,u k( )]
⎨ k∣k−1 ⎨ k∣k−1
⎪⎪⎪⎪⎪
[ ] ( )
⎪⎪⎪⎪⎪
[ ] ( ) (SI-25)
⎩ ⎩
The cost in the right-hand side of (SI-25) is linear in the decision variable; thus, its optimal value is the
same as
q(x)
r k m ∣k− i 1 n ∈R E p¯( k x ∣k ) −1 r k∣k−1 ln p¯ k ( k x ∣ ∣ k k ) − − 1 1 −r k∣k−1 c¯ k X k
s.t E p¯(x) r k∣k [−1 lnr k∣k−1 ≤η k x k−1 ,u k( . )]
k∣k−1
Thus, we can leverage Theorem S6.1 to[ obtain a ref]ormul(ation of )the functional convex optimization
problem in the right-hand side of (SI-25) having r k∣k−1 as decision variable. To this aim, in Theorem S6.1
set: (i)X ∶=L1 X ; (ii)Ω∶=K,withKdefinedabove; (iii)G r k∣k−1 =E p¯(x) r k∣k−1 lnr k∣k−1 −η k x k−1 ,u k ,
k∣k−1
and f r k∣k−1 =(E p¯ ) ( k x ∣k ) −1 r k∣k−1 ln p q ¯ k ( k ( x x ∣ ∣ k k ) ) − − 1 1 −r k∣k−1 c¯ k X k . Furth ( er, not ) e that, wi [ th these defini ] tions ( : (i) Ω is ) a
convex ( subse ) t of the li [ near vector space X; (ii) ( f i ) s ] real-valued and convex w.r.t. r k∣k−1 on Ω; (iii) G is a
real convex mapping of r k∣k−1 . The hypotheses of Theorem S6.1 are all satisfied and this implies that
p( k x ∣k ) −1 ∈B η(p¯ m k( a x x k∣ x k−1,u k)) D KL p k (x ∣k ) −1 q k (x ∣k ) −1 +E p( k x ∣k ) −1 c¯ k X k =η k x k−1 ,u k −m α≥ a 0 x r k m ∣k− i 1 n ∈R L r k∣k−1 ,α ,
( ∣∣ ) [ ( )] ( ) ( )
with L r k∣k−1 ,α given in (SI-6). This yields the desired conclusion.
( )
Next, we give the proof for Theorem S3.1, which yields the scalar, convex, optimization problem to
compute c˜ x k−1 ,u k .
( )
Proof of Theorem S3.1. We discuss separately two cases for α>0 and α=0.
Case (i): α > 0. The problem in (SI-8) is a convex functional optimization problem with cost and
constraints that are differentiable in the decision variable. We find the optimal solution by studying the
SI-22
variations of the Lagrangian associated to the problem (SI-8):
q(x)
L˜ r k∣k−1 ,λ =E p¯( k x ∣k ) −1 ⎡ ⎢ ⎢ r k∣k−1 ln p¯ k k (x ∣ ∣ k k ) − − 1 1 −r k∣k−1 c¯ k X k +α r k∣k−1 lnr k∣k−1 ⎤ ⎥ ⎥ +λ E p¯( k x ∣k ) −1 r k∣k−1 −1
( ) ⎢ ( ) ( )⎥ ( [ ] )
=E p¯( k x ∣k ) −1 ⎡ ⎢ ⎢ ⎢ ⎢ ⎣r k∣k−1 ln p q ¯ k k ( ( x x ∣ ∣ k k ) ) − − 1 1 −r k∣k−1 c¯ k X k +α r k∣k−1 lnr k∣k−1 ⎥ ⎥ ⎦+λr k∣k−1⎤ ⎥ ⎥ −λ.
⎢ ( ) ( ) ⎥
⎢ ⎥
⎢ ⎥
In particular, by studying⎣the variations of the Lagrangian, we find a stationary solu⎦tion of the problem,
which, since this is convex, is a global minimum. First, we consider the variation of the Lagrangian with
respect to r k∣k−1 . This is given by:
q(x)
δ r L˜ r k∣k−1 ,λ =E p¯( k x ∣k ) −1 ⎡ ⎢ ⎢ ln p¯ k ( k x ∣ ∣ k k ) − − 1 1 −c¯ k X k +α 1+lnr k∣k−1 +λ ⎤ ⎥ ⎥ . (SI-26)
( ) ⎢ ( ) ( ) ⎥
⎢ ⎥
⎢ ⎥
Therefore, any candidate optimal solution,⎣say r˜ k∣k−1 , must satisfy: ⎦
q(x)
ln
p¯
k
(x
∣k
)
−1 −c¯ k X k +α 1+lnr˜ k∣k−1 +λ=0. (SI-27)
k∣k−1
( ) ( )
That is,
1
r˜ k∣k−1 =exp −λ
α
−α
⎛
p
q
¯
k
( k
(
x
x ∣
∣
k
k )
) −
−
1
1
⎞
α exp c¯ k
( α
x k
)
. (SI-28)
( )⎜ ⎟ ( )
Now, the variation of the Lagrangian w.r.t. λ is giv ⎝ en by: ⎠E p¯(x) r k∣k−1 −1=0. This condition must hold
k∣k−1
foreveryfeasiblesolutionand, hence, inparticularforr˜ k∣k−1 given [ in(SI- ] 28). Byimposingthisstationarity
condition on r˜ k∣k−1 we obtain:
−λ−α 1
exp = (SI-29)
α 1
E p¯( k x ∣k ) −1 α exp c¯ k( x k)
( ) p¯( k x ∣k ) −1⎡ ⎢ q k (x ∣k ) −1 α ⎤ ⎥
⎢ ⎢( ) ( )⎥ ⎥
⎢ ⎥
Hence, from (SI-29) and (SI-28) we get: ⎣ ⎦
1
p¯( k x ∣k ) −1 α exp c¯ k( x k)
r ∗ = q k (x ∣k ) −1 α , (SI-30)
k∣k−1
E ( p¯( k x) ∣k ) −1 α
1
e ( xp c¯ k( ) x k)
p¯( k x ∣k ) −1⎡ ⎢ q k (x ∣k ) −1 α ⎤ ⎥
⎢ ⎢( ) ( )⎥ ⎥
whichiswelldefined∀α>0. Notethat, byassum ⎢ ⎣ptionsA1–A2, r
k
∗
∣k−1
is ⎥ ⎦bounded. Thus, oneimplicationis
that
suppp(x) ⊂suppp¯(x)
. In turn, again from assumption A1 this implies that
suppp(x) ⊂suppq(x)
k∣k−1 k∣k−1 k∣k−1 k∣k−1
Next, we show that the optimal cost of the problem in (SI-8) is finite. To this aim, we note that the
SI-23
∗
optimal cost of the problem equals the value of the Lagrangian evaluated at r . In turn, we have:
k∣k−1
q(x)
L˜ r ∗ ,λ =E r ∗ ln k∣k−1 −r ∗ c¯ X +α r ∗ lnr ∗ +λ E r ∗ −1
k∣k−1 p¯( k x ∣k ) −1 ⎡ ⎢ ⎢ k∣k−1 p¯ k (x ∣k ) −1 k∣k−1 k k k∣k−1 k∣k−1 ⎤ ⎥ ⎥ p¯( k x ∣k ) −1 k∣k−1
( ) ⎢ ( ) ( )⎥ ( [ ] )
⎢ 1 ⎥
=E p¯( k x ∣k ) −1 ⎡ ⎢ ⎢ ⎢ ⎢ ⎢E ( p q ¯ k ( k ( x x ∣ ∣ k k ) ) − − p¯ 1 1 ( k ⎢ ⎣ x) ∣k ) − α 1 ex α 1 p e ( x c¯ p k( α x c¯ k k ) ( ) x k) ⎛ ⎜ ⎜ ⎜ ln ⎛ ⎜ ⎜ ⎜ p q ¯ k ( k ( x x ∣ ∣ k k ) ) − − 1 1E ( p q ¯ k ( k ( x x ∣ ∣ k k p ) ) ¯ − − ( k 1 1 x ∣k ) ) −1 ex α p 1 ( e ⎥ ⎦ c¯ x k p ( x c¯ k k ) ( x ) k) α ⎞ ⎟ ⎟ ⎟ −c¯ k X k ⎞ ⎟ ⎟ ⎟ ⎤ ⎥ ⎥ ⎥ ⎥ ⎥
⎢ ⎢ ⎢ p¯( k x ∣k ) −1⎡ ⎢ q k (x ∣k ) −1 α ⎤ ⎥ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ p¯( k x ∣k ) −1⎡ ⎢ q k (x ∣k ) −1 α ⎤ ⎥ ⎟ ⎟ ⎟ ( )⎟ ⎟ ⎟ ⎥ ⎥ ⎥
⎢ ⎢ ⎢ ⎢( 1 ) ( )⎥ ⎥ ⎜ ⎜ ⎢ ⎢( ) ( )⎥ ⎥ ⎟ ⎟⎥ ⎥
⎢ ⎣ p¯ ⎢ ⎣ ( k x ∣k ) −1 α exp c¯ k( x k) ⎥ ⎦ ⎝ ⎝ ⎢ ⎣ ⎥ ⎦ ⎠ ⎠⎥ ⎦
=E p¯( k x ∣k ) −1 ⎡ ⎢ ⎢ ⎢ ⎢ ⎢E ( q k (x ∣k ) − p¯ 1 ( k x) ∣k ) −1 α 1 e ( xp α c¯ k( ) x k) ⎛ ⎜ ⎜ ⎜ ln ⎛ ⎜ ⎜ ⎜E p¯ e ( k x x ∣k ) p −1( c¯ k α 1 ( e x x k p ))c¯ k( x k) α ⎞ ⎟ ⎟ ⎟ −c¯ k X k ⎞ ⎟ ⎟ ⎟ ⎤ ⎥ ⎥ ⎥ ⎥ ⎥
⎢ ⎢ ⎢ p¯( k x ∣k ) −1⎡ ⎢ q k (x ∣k ) −1 α ⎤ ⎥ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ p¯( k x ∣k ) −1⎡ ⎢ q k (x ∣k ) −1 α ⎤ ⎥ ⎟ ⎟ ⎟ ( )⎟ ⎟ ⎟ ⎥ ⎥ ⎥
⎢ ⎢ ⎢ ⎢( )1 ( )⎥ ⎥ ⎜ ⎜ ⎢ ⎢( ) ( )⎥ ⎥ ⎟ ⎟⎥ ⎥
⎢ ⎢p¯(x) α ⎥⎝ ⎝ ⎢ ⎥ ⎠ ⎠⎥
=−αlnE ⎣ ⎣k∣k−1 exp c¯ k x k ⎦ , ⎣ ⎦ ⎦
p¯( k x ∣k ) −1 ⎡ ⎢ ⎢
⎢ ⎢
⎛
⎜
q k (x ∣k ) −1 ⎞
⎟ (
( α )
)
⎤ ⎥ ⎥
⎥ ⎥
⎢⎝ ⎠ ⎥
⎢ ⎥
whichisfinitebya⎣ssumptionsA1–A2. Byus⎦ingthislastexpressioninto(SI-9)yieldsthedesiredconclusion.
Case (ii): α = 0. We note that M x k−1 ,u k is bounded (assumptions A1–A2) and, when α = 0,
η k x k−1 ,u k α−W α x k−1 ,u k becomes
( )
( ) ( p¯( )x) p¯(x)
r k m ∣k− a 1 x ∈R E p¯( k x ∣k ) −1 ⎡ ⎢ ⎢ r k∣k−1 ln q k k (x ∣ ∣ k k ) − − 1 1+r k∣k−1 c¯ k X k ⎤ ⎥ ⎥ = r k m ∣k− a 1 x ∈R E p¯( k x ∣k ) −1 ⎡ ⎢ ⎢ r k∣k−1 ⎛ ln q k k (x ∣ ∣ k k ) − − 1 1+c¯ k X k ⎞ ⎤ ⎥ ⎥ , (SI-31)
⎢ ( )⎥ ⎢ ⎜ ( )⎟⎥
⎢ ⎥ ⎢ ⎥
⎢ ⎥ ⎢ ⎝ ⎠⎥
which we want to p⎣rove being equal to M x k−1 ,u⎦k . To this aim,⎣note that, by definition of l⎦imit superior,
there exists a sequence
x(n) ∈suppp¯(x)
such that (using the extended notation)
k k∣k−1 ( )
ln p q ¯ k k (x x ( k ( k n n ) ) ∣x x k k − − 1 1 , , u u k k ) exp c¯ k x( k n) →M x k−1 ,u k , and p k x( k n) x k−1 ,u k →p, as n→∞, (SI-32)
( ( )) ( ) ( ∣ )
( ∣ )
Therefore, for the cost in (SI-31), we have that, in the feasibility domain of the problem:
p¯(x) p¯(x)
exp c¯ x
E p¯( k x ∣k ) −1 ⎡ ⎢ ⎢ r k∣k−1 ⎛ ln q k k (x ∣ ∣ k k ) − − 1 1+c¯ k x k ⎞ ⎤ ⎥ ⎥ ≤E p¯( k x ∣k ) −1 ⎡ ⎢ ⎢ r k∣k−1 limsupln ⎡ ⎢ ⎢ ⎛ k∣k−1 q k (x ∣k ) (−1 k ( k ))⎞ ⎤ ⎥ ⎥ ⎤ ⎥ ⎥ =M x k−1 ,u k .
⎢ ⎜ ( )⎟⎥ ⎢ ⎢⎜ ⎟⎥⎥ ( )
⎢ ⎥ ⎢ ⎢ ⎥⎥
⎢ ⎝ ⎠⎥ ⎢ ⎢⎝ ⎠⎥⎥
Next, we⎣show that this is indeed th⎦e optimal⎣value of the prob⎣lem. Indeed, by contra⎦d⎦iction, assume that
there exists some M˜ such that
p¯(x)
E p¯( k x ∣k ) −1 ⎡ ⎢ ⎢ r k∣k−1 ⎛ ln q k k (x ∣ ∣ k k ) − − 1 1+c¯ k x k ⎞ ⎤ ⎥ ⎥ <M˜ <M x k−1 ,u k .
⎢ ⎜ ( )⎟⎥ ( )
⎢ ⎥
⎢ ⎝ ⎠⎥
Then,bytakingthelimitsuperio⎣roftheaboveexpressionw⎦ewouldhaveM x k−1 ,u k ≤M˜ <M x k−1 ,u k ,
( ) ( )
SI-24
which is a contradiction. Finally, we prove that M x k−1 ,u k is non-negative. Indeed, by contradiction,
assume that M x k−1 ,u k <0. In turn, this means that there exists some β <0 such that
( )
( ) p¯(x)
expc¯ x
limsupln
k∣k−1 k k
<β <0,
⎛ q k (x ∣k ) −1 ( )⎞
⎜ ⎟
⎝ ⎠
so that, in particular
p¯(x)
expc¯ x
ln
k∣k−1 k k
<β.
q(x)
k∣k−1 ( )
In turn, this implies that
p¯(x)
expc¯ x
<q(x)
expβ. By taking integration we get
k∣k−1 k k k∣k−1
( )
1= p¯(x) dx ≤ p¯(x) expc¯ x dx ≤expβ,
∫ k∣k−1 k ∫ k∣k−1 k k k
( )
where the first inequality follows from the fact that the cost is non-negative (following from assumption
A2). The above chain of inequalities yields a contraction as we assumed that β <0.
Finally, we prove Corollary S3.1, which ultimately states that the policy in Fig. 2b is in fact optimal
and explains how c¯ x is built. Since Corollary S3.1 tackles the minimization problem in the policy
k k
space – a problem consistent with prior literature,19 the proof leverages its key arguments.
( )
Proof of Corollary S3.1. Consider the cost in the robust free energy principle (main text). By the
chain rule for the KL divergence (Lemma S1.1) we can recast the robust free energy principle as the sum
of the following sub-problems, see, e.g.,:19
N−1
{π k (u ∣k
m
) −1
i
}
n
1∶N−1{p( k x ∣k
m
) −1
a
}
x
1∶N−1
D KL p 0∶N−1 q 0∶N−1 +
k
∑
=1
E p( x k−1) E p k∣k−1 c( k x) X k +c( k u) U k
(SI-33)
( ∣∣ ) [ [ ( ) ( )]]
s.t. π k (u ∣k ) −1 ∈D, p k (x ∣k ) −1 ∈B η p¯ k x k x k−1 ,u k , ∀k ∈1∶N −1,
( ( ∣ ))
and
π m (u) in p m (x) ax E p( x N−1) D KL p N x N ,u N x N−1 q N x N ,u N x N−1 +E pN( x N,u N∣ x N−1) c¯ N X N +c( N u) U N
N∣N−1 N∣N−1
[ ( ( ∣ )∣∣ ( ∣ )) [ ( ) ( )]]
s.t. π N (u ∣ ) N−1 ∈D, p N (x ∣ ) N−1 ∈B η p¯ N x N x N−1 ,u N ,
(SI-34)
( ( ∣ ))
where c¯ x = c(x) x . Now, following similar arguments to the ones used in Lemma S3.1, we have
N N N N
that the problem in (SI-34) can be solved by solving
( ) ( )
π m (u) in pN( x N m ∣ x a N x −1,u N) D KL p N x N ,u N x N−1 q N x N ,u N x N−1 +E pN( x N,u N∣ x N−1) c¯ N X N +c( N u) U N
N∣N−1
( ( ∣ )∣∣ ( ∣ )) [ ( ) ( )]
s.t. π N (u ∣ ) N−1 ∈D, p N (x ∣ ) N−1 ∈B η p¯ N x N x N−1 ,u N ,
(SI-35)
( ( ∣ ))
and then by taking the expectation over p x N−1 . To tackle the problem in (SI-35) we note that the first
term in the cost can be written as
( )
E πN( u N∣ x N−1) D KL p N x N x N−1 ,u N q N x N x N−1 ,u N +D KL π N u N x N−1 q N u N x N−1 ,
[ ( ( ∣ )∣∣ ( ∣ ))] ( ( ∣ )∣∣ ( ∣ ))
SI-25
and also
E c¯ X +c(u) U =
p N∣N−1 N N N N
E πN( u N∣ [x N−1 ( ) E p) N( x N∣ x N ( −1,u N ) ) ] c¯ N X N +E πN( u N∣ x N−1) c( N u) U N .
[ [ ( )]] [ ( )]
That is, the cost of the problem in (SI-35) can be written as:
E D p(x) q(x) +E c¯ X +D π(u) q(u) +E c(u) U .
π
k
(u
∣k
)
−1
KL N∣N−1 N∣N−1 p(
N
x
∣
)
N−1
N N KL N∣N−1 N∣N−1 π
N
(u
∣
)
N−1
N N
(SI-36)
[ ( ∣∣ ) [ ( )]] ( ∣∣ ) [ ( )]
This yields a problem of the form of (SI-3) and hence, by the above results:
• the optimal solution of the problem in (SI-36) is
π(u),⋆ = q N (u ∣ ) N−1 exp −η N x N−1 ,u N −c˜ x N−1 ,u N −c( N u) u N , (SI-37)
N∣N−1 ∫ q N (u ∣ ) N−1 exp − ( η N x(N−1 ,u N ) −c˜ x(N−1 ,u N ) −c N (u) u(N )) du N
( ( ) ( ) ( ))
with
p¯(x)
exp c¯ x
c˜ x N−1 ,u N =min limsup ln
N∣N−1 N N
,
⎧ ⎪⎪⎪ x
N
∈suppp¯(
N
x
∣
)
N−1
⎛ q
N
(x
∣
)
N
(−1 ( ))⎞
( ) ⎨ ⎜ ⎟
m α> i 0 n η N
⎪⎪⎪
⎩ ( x N−1 ,u N ) α+ ⎝ αlnE p¯( N x ∣ ) N−1 ⎡ ⎢ ⎢ ⎢ ⎢ ⎛ ⎜ p¯( k x ∣k ) −1 e q x k ( p x ∣ ⎠ k ) − c¯ 1 N ( x N )⎞ ⎟ α
1
⎤ ⎥ ⎥ ⎥ ⎥ ⎫ ⎪⎪⎪⎪ ⎬ ;
⎢
⎢
⎝ ⎠ ⎥
⎥
⎪⎪⎪⎪
• the corresponding optimal cost is ⎣ ⎦⎭
cˆ N x N−1 =−ln ∫ q N (u ∣ ) N−1 exp −η k x k−1 ,u k −c˜ x N−1 ,u N −c( N u) u N du N ,
( ) ( ( ) ( ) ( ))
and therefore the cost for the problem in (SI-34) is
E p( x N−1) cˆ N X N−1 , (SI-38)
[ ( )]
which is also bounded.
The policy and cost in (SI-37) and (SI-38) are, respectively, the optimal solution and cost given in the
statement of the result at k = N. Therefore, by using (SI-38) and (SI-33) we have that the robust free
energy principle can be written as
N−1
{π k (u ∣k
m
) −1
i
}
n
1∶N−1{p( k x ∣k
m
) −1
a
}
x
1∶N−1
D KL p 0∶N−1 q 0∶N−1 +
k
∑
=1
E p( x k−1) E p k∣k−1 c( k x) X k +c( k u) U k
( ∣∣ ) [ [ ( ) ( )]] (SI-39)
+E p( x N−1) cˆ N X N−1
s.t. π k (u ∣k ) −1 ∈D, p k (x ∣k ) −1 ∈B η p¯ k[ x k( x k−1) , ] u k , ∀k ∈1∶N −1.
( ( ∣ ))
On the other hand, we have that
E p( x N−1) cˆ N X N−1 =E p( x N−2) E p N−1∣N−2 cˆ N X N−1 ,
[ ( )] [ [ ( )]]
SI-26
so that (using again the chain rule for the KL divergence) the cost in (SI-39) can be written as:
N−2
D KL p 0∶N−2 q 0∶N−2 + ∑ E p( x k−1) E p k∣k−1 c( k x) X k +c k (u) U k
k=1
+E p( ( x N−2) D ∣∣ KL p N ) −1∣N−2 q N−1∣N− [ 2 +E p [ N−1∣N− ( 2 c¯ N ) −1 X N ( −1 )+]] c( N u − ) 1 U N−1 ,
[ ( ∣∣ ) [ ( ) ( )]]
with
c¯ N−1 x N−1 ∶=c( N x − ) 1 x N−1 +cˆ N x N−1 . (SI-40)
The problem at k = N −1 can again b(e solv)ed indep(enden)tly on(the ot)hers and c¯ N−1 x N−1 is bounded.
Hence, by iterating the above steps one obtains that:
( )
• the optimal solution at k =N −1 is given by
π(u),⋆ = q N (u − ) 1∣N−2 exp −η N−1 x N−2 ,u N−1 −c˜ x N−2 ,u N−1 −c( N u − ) 1 u N−1 , (SI-41)
N−1∣N−2 ∫ q N (u − ) 1∣N−2 exp − ( η N−1 x(N−2 ,u N−1 − ) c˜ x(N−2 ,u N−1 − ) c( N u − ) 1 u(N−1 ) d )u N−1
( ( ) ( ) ( ))
with
c˜ x N−2 ,u N−1 =min limsup ln p¯( N x − ) 1∣N−2 exp c¯ N−1 x N−1 ,
⎧ ⎪⎪⎪ x N−1 ∈suppp¯( N x − ) 1∣N−2 ⎛ q N (x − ) 1 ( ∣N−2 ( ))⎞
( ) ⎨ ⎜ ⎟
m α> i 0 n η N
⎪⎪⎪
⎩ −1
(
x N−2 ,u N−1
)
α+ ⎝ αlnE p¯( N x − ) 1∣N−2 ⎡ ⎢ ⎢
⎢ ⎢
⎛
⎜
p¯( k x ∣k ) −1 exp q k ( c¯ x ∣k N ) − ⎠ − 1 1 ( x N−1 )⎞
⎟
α
1
⎤ ⎥ ⎥
⎥ ⎥
⎫ ⎪⎪⎪⎪
⎬
.
This is the optimal policy given in the statement at k =N −1
⎢
⎢ ⎣
⎝ ⎠ ⎥
⎥ ⎦
⎪⎪⎪⎪
⎭
• the optimal cost for the problem at k =N −1 is bounded and given by
E p( x N−2) cˆ N−1 X N−2 . (SI-42)
[ ( )]
The desired conclusions are then drawn by induction after noticing that, at each k, the problem in the
robust free energy formulation can always be broken down into two sub-problems, with the problem at the
last time-step given by:
π
m
(u
i
)
n
p
m
(x
a
)
xE p( x k−1) D KL p k∣k−1 q k∣k−1 +E p k∣k−1 c¯ k X k +c( k u) U k
k∣k−1 k∣k−1 (SI-43)
[ ( ∣∣ ) [ ( ) ( )]]
s.t. π k (u ∣k ) −1 ∈D, p( k x ∣k ) −1 ∈B η p¯ k x k x k−1 ,u k ,
( ( ∣ ))
with c¯ x bounded and given, at each k, by the recursion in (SI-16) and (SI-17).
k k
( )
S6.3 Computing The Cost Of Ambiguity
Theorem S3.2 establishes several properties useful to compute c˜ x k−1 ,u k .
( )
SI-27
Proof. To prove part (1) it suffices to show that
1
p¯(x) α
lnE k∣k−1 exp c¯ k x k ≥0.
p¯( k x ∣k ) −1 ⎡ ⎢ ⎢
⎢ ⎢
⎛
⎜
q k (x ∣k ) −1 ⎞
⎟ (
( α )
)
⎤ ⎥ ⎥
⎥ ⎥
⎢⎝ ⎠ ⎥
⎢ ⎥
The proof is then by contradiction. Indeed⎣, if we assumed that ⎦
1
p¯(x) α
lnE k∣k−1 exp c¯ k x k <0,
p¯( k x ∣k ) −1 ⎡ ⎢ ⎢
⎢ ⎢
⎛
⎜
q k (x ∣k ) −1 ⎞
⎟ (
( α )
)
⎤ ⎥ ⎥
⎥ ⎥
⎢⎝ ⎠ ⎥
⎢ ⎥
this would imply ⎣ ⎦
1
p¯(x) α
k∣k−1 exp c¯ k x k p¯(x) −p¯(x) dx <0.
∫ X ⎡ ⎢ ⎢
⎢ ⎢
⎛
⎜
q k (x ∣k ) −1 ⎞
⎟ (
( α )
)
k∣k−1 k∣k−1⎤ ⎥ ⎥
⎥ ⎥
k
⎢⎝ ⎠ ⎥
⎢ ⎥
In turn, this would mean that⎣ ⎦
1
p¯(x) α
k∣k−1 exp c¯ k x k p¯(x) −p¯(x) <0, a.e in X
⎡ ⎢ ⎢
⎢ ⎢
⎛
⎜
q k (x ∣k ) −1 ⎞
⎟ (
( α )
)
k∣k−1 k∣k−1⎤ ⎥ ⎥
⎥ ⎥
⎢⎝ ⎠ ⎥
⎢ ⎥
and consequently, ⎣ ⎦
1
p¯(x) expc¯ x α
k∣k−1 k k
<1 a.e in X (SI-44)
⎛ q k (x ∣k ) −1 ( )⎞
⎜ ⎟
⎝ ⎠
which by taking logarithm implies that
p¯(x)
ln
k∣k−1
<0 a.e in X.
q(x)
k∣k−1
Finally, by multiplying both sides of the above inequality by
p¯(x)
and taking the integral we would have:
k∣k−1
D p¯(x) q(x) ≤0,
KL k∣k−1 k∣k−1
( ∣∣ )
so that
p¯(x) =q(x)
. However, this contradicts (SI-44) as by assumption A2 the state cost is positive.
k∣k−1 k∣k−1
Next we prove part (2) and we start with the case (ii). Note that, for a given function f, we have that
if f α is strictly convex for α > 0, then αf 1 is also strictly convex in the same domain. In fact, pick
α
( ) ( )
SI-28
some α,β >0 and t∈ 0,1 . If the function is strictly convex, we have
[ ] 1 tα 1 1−t β 1
tα+ 1−t β f = tα+ 1−t β f . + .
tα+ 1−t β tα+ 1−t β α tα+ 1−t β β
( )
( ( ) ) ( ) ( ( ) )[ ( tα 1 1−t β )1]
( ) < tα+ 1−t β ( f) + ( ) f
tα+ 1−t β α tα+ 1−t β β
( )
=( tαf (1 + ) 1−)[ t βf ( 1 ,) ( ) ( ) ( )]
α β
( ) ( ) ( )
which shows the desired statement. Let
α
p¯(x)
expc¯ x
y α ∶= k∣k−1 k k p¯(x) dx
∫ ⎛ q k (x ∣k ) −1 ( )⎞ k∣k−1 k
( ) ⎜ ⎟
α ⎝ ⎠
and f α ∶=ln ∫ p¯( k x ∣k ) −1 q e ( x x p ) c¯ k( x k) p¯( k x ∣k ) −1 dx k +η k x k−1 ,u k =lny α +η k x k−1 ,u k . Then, V α x k−1 ,u k =
k∣k−1
αf 1 (an)dwepr(ovetheresultb)yshowingthatf(α isstri)ctlycon(ve)x. No(w,thefu)nctionf isd(ifferentiab)le
α
and its second derivative w.r.t. α is:
( ) ( )
d2 y ′′ α y α − y ′ α 2
f α = ,
dα y2 α
( ) ( ) ( ( ))
( )
where we the shorthand notation y ′ α and y ′′ α to den(ot)e the first and second derivative of y w.r.t. α.
Now, a direct calculation yields:
( ) ( )
α 2
p¯(x)
expc¯ x
p¯(x)
expc¯ x
y ′′ α y α − y ′ α 2 = k∣k−1 k k ln k∣k−1 k k p¯(x) dx
∫ ⎛ q k (x ∣k ) −1 ( )⎞ ⎛ q k (x ∣k ) −1 ( )⎞ k∣k−1 k
( ) ( ) ( ( )) α ⎜ ⎟ ⎜ ⎟
⋅ p¯( k x ∣k ) −1 expc¯ k x k p¯ ⎝ (x) dx ⎠ ⎝ ⎠ (SI-45)
∫ ⎛ q k (x ∣k ) −1 ( )⎞ k∣k−1 k
⎜ ⎟
α 2
⎝
p¯(x)
expc¯ x⎠
p¯(x)
expc¯ x
− k∣k−1 k k ln k∣k−1 k k p¯(x) dx .
⎛ ∫ ⎛ q k (x ∣k ) −1 ( )⎞ ⎛ q k (x ∣k ) −1 ( )⎞ k∣k−1 k ⎞
⎜ ⎜ ⎟ ⎜ ⎟ ⎟
⎝ ⎝ ⎠ ⎝ ⎠ ⎠
Moreover, note that:
α 2
p¯(x)
expc¯ x
p¯(x)
expc¯ x
k∣k−1 k k
ln
k∣k−1 k k p¯(x)
dx
⎛ ∫ ⎛ q k (x ∣k ) −1 ( )⎞ ⎛ q k (x ∣k ) −1 ( )⎞ k∣k−1 k ⎞
⎜ ⎜ ⎟ ⎜ ⎟ ⎟
α α 2
⎝ ⎝ p¯(x) expc¯ x⎠ ⎝2 p¯(x) expc¯ x⎠ ⎠ p¯(x) expc¯ x 2
= ⎛
⎜ ⎜
∫ ⎛
⎜
k∣k−1 q k (x ∣k ) −1 k ( k )⎞
⎟
⎛
⎜
ln k∣k−1 q k (x ∣k ) −1 k ( k )⎞
⎟(
p¯( k x ∣k ) −1
)
1 2. ⎛
⎜
k∣k−1 q k (x ∣k ) −1 k ( k )⎞
⎟ (
p¯( k x ∣k ) −1
)
2 1 dx k ⎞
⎟ ⎟
α 2 α 2
< ⎝ ⎝ p¯( k x ∣k ) −1 expc¯ k x k ⎠2 ⎝ ln p¯ k (x ∣k ) −1 expc¯ k x k ⎠ p¯(x) 1 2 ⎝ dx . p¯( k x ∣k ) −1 ⎠ expc¯ k x k 2 p¯ ⎠ (x) 2 1 dx
∫ ⎛
⎜ ⎜
⎛
⎜
q k (x ∣k ) −1 ( )⎞
⎟
⎛
⎜
q k (x ∣k ) −1 ( )⎞
⎟(
k∣k−1
)
⎞
⎟ ⎟
k ∫ ⎛
⎜ ⎜
⎛
⎜
q k (x ∣k ) −1 ( )⎞
⎟ (
k∣k−1
)
⎞
⎟ ⎟
k
α 2 α
= ⎝⎝p¯( k x ∣k ) −1 expc¯ k x k ⎠ l ⎝ n p¯ k (x ∣k ) −1 expc¯ k x k ⎠ p¯(x) dx ⎠ p¯( k x ∣k ) −1 e⎝x⎝pc¯ k x k p¯(x) d ⎠ x , ⎠
∫ ⎛ q k (x ∣k ) −1 ( )⎞ ⎛ q k (x ∣k ) −1 ( )⎞ k∣k−1 k∫ ⎛ q k (x ∣k ) −1 ( )⎞ k∣k−1 k
⎜ ⎟ ⎜ ⎟ ⎜ ⎟
⎝ ⎠ ⎝ ⎠ ⎝ ⎠
SI-29
where we used Cauchy inequality (which is strict in this case). By combining the above expression with
(SI-45) we get that y ′′ α y α − y ′ α 2 > 0 and hence d d α 2 2 f α > 0. That is, V α x k−1 ,u k is strictly
convex. To prove case ((i))it(suffi) ce(s t(o k)n)ow that, when ln p¯( k x ∣k ) −1 ( exp ) c¯ k( x k) =c¯, Cauchy i(nequality)holds with
q(x)
k∣k−1
equality.
We now prove part (3) and start with noticing that limsup α→0 V α x k−1 ,u k ≤ M x k−1 ,u k . In fact,
by definition of M x k−1 ,u k we get that
( ) ( )
V ( α x k−1 ,u ) k ≤ln ∫ exp M x k α −1 ,u k p¯( k x ∣k ) −1 dx k α +αη k x k−1 ,u k ,
( )
( ) ( ( ) ) ( )
which means that limsup α→0 V α x k−1 ,u k ≤ M x k−1 ,u k . Next, we show that liminf α→0 V α x k−1 ,u k ≥
M x k−1 ,u k , and from here we will draw the desired conclusions. To this aim, pick any ε > 0 and define
( ) ( ) ( )
the set
( ) p¯(x)
expc¯ x
X ε ∶= x k ∈X ∶
k∣k−1 k k
≥exp M x k−1 ,u k −ϵ ,
⎧ ⎪⎪⎪ q k (x ∣k ) −1 ( ) ⎫ ⎪⎪⎪
⎨ ( ( ) ) ⎬
which is non-empty by defini⎪⎪⎪tion of M x k−1 ,u k . Then, note that ⎪⎪⎪
⎩ ⎭
( ) 1
p¯(x) α
V α ( x k−1 ,u k ) =αlnE p¯( k x ∣k ) − p 1 ¯( ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢x ⎛ ⎜ ⎝ ) q k k ( e x ∣ ∣ k k x ) − − p 1 1 c¯ ⎞ ⎟ ⎠ x exp ( α 1 c¯ k ( α x k ) ) ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ +η α k ( x k−1 ,u k ) α
≥ln ⎛
⎜ ⎜
∫ X ε⎛
⎜
k⎣ ∣k−1 q k (x ∣k ) −1 k ( k )⎞
⎟
p¯( k x ∣k ) −1 dx ⎦ k
α
⎞
⎟ ⎟
+αη k
(
x k−1 ,u k
)
≥ M⎝ x k− ⎝ 1 ,u k −ϵ +ln ∫ X ⎠p¯( k x ∣k ) −1 dx k ⎠+αη k x k−1 ,u k ,
ε
( ( ) ) ( ) ( )
where to obtain the first inequality we used the fact that
1 1
p¯(x) expc¯ x α p¯(x) expc¯ x α
k∣k−1 k k p¯(x) dx ≥ k∣k−1 k k p¯(x) dx .
∫ X ⎛ q k (x ∣k ) −1 ( )⎞ k∣k−1 k ∫ X ε⎛ q k (x ∣k ) −1 ( )⎞ k∣k−1 k
⎜ ⎟ ⎜ ⎟
⎝ ⎠ ⎝ ⎠
The above chain of inequalities shows that
liminfV α x k−1 ,u k ≥M x k−1 ,u k −ϵ.
α→0
( ) ( )
Hence, we have that:
M x k−1 ,u k −ϵ≤liminfV α x k−1 ,u k ≤limsupV α x k−1 ,u k ≤M x k−1 ,u k .
α→0 α→0
( ) ( ) ( ) ( )
Moreover,sinceϵisarbitrary,bytakingthelimitε→0wegetliminf α→0 V α x k−1 ,u k =limsup α→0 V α x k−1 ,u k =
M x k−1 ,u k . In turn, this means that lim α→0 V α x k−1 ,u k =M x k−1 ,u k .
( ) ( )
The proof of part (4) is by contradiction. Define the following set
( ) ( ) ( )
S = α≥0; V˜ α x k−1 ,u k ≤M x k−1 ,u k .
{ ( ) ( )}
SI-30
We note that S is: (i) non-empty (by definition of M x k−1 ,u k ); (ii) closed, due to (right) continuity
of the function; (iii) bounded (indeed, if S was unbounded, then it would have been possible to find
( )
an unbounded sequence, α n → ∞, such that α n ∈ S and therefore V˜ α x k−1 ,u k ≤ M x k−1 ,u k and this
contradicts part(1) ofthe statement). Now, sinceV˜ α x k−1 ,u k iscontinuous at 0(by part(3)), thenit has
( ) ( )
a minimum over the compact and bounded set S, i.e., there exists α ⋆ ∈S such that min α∈S V˜ α x k−1 ,u k =
( )
V˜ α⋆ x k−1 ,u k . We want to show that this is indeed a global minimum over the set 0,∞ . To prove
( )
this, assume by contradiction that there exists some β > 0 that does not belong to S and such that
( ) [ )
V˜ β x k−1 ,u k <V˜ α⋆ x k−1 ,u k . In turn, this would imply that V˜ β x k−1 ,u k <V˜ α⋆ x k−1 ,u k ≤M x k−1 ,u k .
That is, β ∈S. However, this is a contradiction because β was assumed to not belong to S.
( ) ( ) ( ) ( ) ( )
S6.4 Determining The Role Of Ambiguity
Theorem S4.1 establishes how c˜ x k−1 ,u k changes when the radius of ambiguity shrinks. This result is
instrumentaltodetermineifanagentaffectedbyambiguitycanoutperformanideal, ambiguity-free, agent.
( )
Proof of Theorem S4.1. We use the formulation given in the problem in (SI-25) and let
θ
p¯(x)
expc¯ X
ϕ θ ∶=lnE
k∣k−1 k k
.
p¯( k x ∣k ) −1 ⎡ ⎢ ⎢ ⎛ q k (x ∣k ) −1 ( )⎞ ⎤ ⎥ ⎥
( ) ⎢⎜ ⎟ ⎥
⎢ ⎥
⎢⎝ ⎠ ⎥
In what follows we use the shorthand notation ⎣ϕ(k) θ to denote the ⎦k-th order derivative of ϕ θ . By
definition of the optimal solution of the problem in (SI-25), we have the following chain of identities:
( ) ( )
1 1
p¯( k x ∣k ) −1 expc¯ k( X k) α ⋆ p¯( k x ∣k ) −1 expc¯ k( X k) α ⋆
η =E p¯( k x ∣k ) −1 ⎡ ⎢ ⎢ ⎢ ⎢ ⎢E ( p¯( k q x ∣ k k ( ) x ∣ − k ) 1 − e 1 xpc¯ k( X ) k) α 1 ⋆ ln E ( p¯( k q x ∣ k k ( ) x ∣ − k ) 1 − e 1 xpc¯ k( X ) k) α 1 ⋆ ⎤ ⎥ ⎥ ⎥ ⎥ ⎥
⎢ ⎢ ⎢ p¯( k x ∣k ) −1⎡ ⎢ q k (x ∣k ) −1 ⎤ ⎥ p¯( k x ∣k ) −1⎡ ⎢ q k (x ∣k ) −1 ⎤ ⎥ ⎥ ⎥ ⎥
E ⎢ ⎢ ⎢ ⎣ p¯( k x ∣k ) −1 e ⎢ ⎢ ⎢ ⎣ x ( pc¯ k( X k) α 1 ⋆ ln ) p¯( k x ∣k ) − ⎥ ⎥ ⎥ ⎦1 expc¯ k( X k) ⎢ ⎢ ⎢ ⎣ ( ) ⎥ ⎥ ⎥ ⎦ ⎥ ⎥ ⎥ ⎦ 1
= p¯( k x ∣k ) −1⎡ ⎢ ⎢ ⎢ ⎢ ⎣ ( α⋆E p¯( k q x ∣k k ( ) x − ∣k ) 1 −1 ⎡ ⎢ ⎢ ⎢( p¯( k x ∣k ) − ) 1 q e k ( x x ∣k p ) − c¯ 1 k ( ( X k) ) α q 1 k ( ⋆ x ∣k ⎤ ⎥ ⎥ ⎥ ) −1 ) ⎤ ⎥ ⎥ ⎥ ⎥ ⎦ −lnE p¯( k x ∣k ) −1 ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ ⎛ ⎜ ⎝ p¯( k x ∣k ) −1 e q x k (x ∣ p k ) − c¯ 1 k ( X k )⎞ ⎟ ⎠ α ⋆ ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ .
Hence, by letting θ = 1 , this ⎢ ⎣yields ⎥ ⎦
α⋆
η =θϕ(1) θ −ϕ θ , (SI-46)
( ) ( )
SI-31
Also, for the optimal value of the problem in (SI-25) we have
1
c˜ x k−1 ,u k =E p¯( k x ∣k ) −1 ⎡ ⎢ ⎢ ⎢ ⎢ ⎢E (
p¯( k x ∣k ) −
p¯
1
( k q x
e
∣ k k (
x
) x ∣ − k
p
) 1 −
c¯
e 1
k
x
(
p
X
c¯
k
k
)
( X ) k
α
)
⋆
α 1 ⋆ ln ⎛ p¯( k x ∣k ) −1 e q x k (x ∣ p k ) − c¯ 1 k ( X k )⎞ ⎤ ⎥ ⎥ ⎥ ⎥ ⎥
( ) ⎢ ⎢ ⎢ p¯( k x ∣k ) −1⎡ ⎢ q k (x ∣k ) −1 ⎤ ⎥ ⎜ ⎝ ⎟ ⎠ ⎥ ⎥ ⎥
E ⎢ ⎢ ⎢ ⎣ p¯( k x ∣k ) −1 e ⎢ ⎢ ⎢ ⎣ x ( pc¯ k( X k) α 1 ⋆ ln ) p¯( k x ∣k ) − ⎥ ⎥ ⎥ ⎦1 expc¯ k( X k) ⎥ ⎥ ⎥ ⎦
= p¯( k x ∣k ) −1⎡ ⎢ ⎢ ⎢ ⎢ ⎣ ( E q k (x ∣k ) −1 p¯( k x ∣k ) −1 ) expc¯ k( X ( k) α 1 ⋆ q k (x ∣k ) −1 ) ⎤ ⎥ ⎥ ⎥ ⎥ ⎦ =ϕ(1) θ .
p¯( k x ∣k ) −1⎡ ⎢ q k (x ∣k ) −1 ⎤ ⎥ ( )
⎢ ⎢( ) ⎥ ⎥
⎢ ⎥ ⋆
Next, we prove the result by: (i) showing that⎣the equation in (SI-4⎦6) has a solution, say θ ; (ii) obtaining
an expression for c˜ x k−1 ,u k by computing ϕ(1) θ ⋆ and expressing it as a function of η.
( ) ( )
Finding the root of (SI-46). First, we show that a solution exists and is unique. To this sim, since ϕ is
continuous and differentiable, we Taylor expand the right-hand side in (SI-46) around θ =0. This yields:
∞ ∞ ∞ ∞
θϕ(1) θ −ϕ θ = ∑ 1 ϕ(m+1) 0 θm+1− ∑ 1 ϕ(m) 0 θm = ∑ 1 ν m+1 θm+1− ∑ 1 ν m θm
m! m! m! m!
m=0 m=0 m=0 m=0
∞ ∞
( ) ( ) 1 ( )1 (1) 1 1
= − ν θm = ν θm = ν θ2+ ν θ3+O θ4 ,
∑ m−1 ! m! m ∑ m m−2 ! m 2 2 3 3
m=1 m=2
[ ] ( )
where we used the shorthand ( notat ) ion ν
m
to denote ϕ(m) ( 0 . M ) oreover, following similar arguments as
those used to show (SI-45), we have that ν
2
= ϕ(2) 0 > 0. Thus, for small enough η, there exists a root
( )
⋆
for (SI-46) and, since ϕ is strictly convex by assumption, the root θ is unique. Moreover, the function
( )
θϕ(1) θ −ϕ θ is strictly increasing ( d θϕ(1) θ −ϕ θ =θϕ(2) θ >0 for θ >0) and hence we can invert
dθ
(SI-46). In particular, we have:
( ) ( ) [ ( ) ( )] ( )
η = 1 ν θ ⋆2+ 1 ν θ ⋆3+ 1 ν θ ⋆4+O θ ⋆5 = 1 θ ⋆2 ν 1+ 2ν 3 θ ⋆+ ν 4 θ ⋆2+O θ ⋆3 ,
2 3 4 2
2 3 8 2 3ν ν
2 2
( ) [ ( )]
so that
−1
θ ⋆ = 2η 1+ 2ν 3 θ ⋆+ ν 4 θ ⋆2+O θ ⋆3 2 = 2η 1− ν 3 θ ⋆+O θ ⋆2 , (SI-47)
√ν 3ν ν √ν 3ν
2 2 2 2 2
[ ( ( ))] [ ( )]
where we used the binomial series expansion for a negative fractional power. Here, we used the identity
−1 1
1+ a x+a x2+O x3 2 =1− a x+O x2 .
1 2 1
2
( ( ⋆ ( ))) ( )
Next, disregarding the higher order terms in θ we have
θ ∗ ≈
2η
, (SI-48)
√ν
2
SI-32
which, together with (SI-47), yields
θ ⋆ = 2η 1− ν 3 θ ⋆+O θ ⋆2 = 2 η2 1 − 2ν 3 η+O η 3 2 . (SI-49)
√ν 3ν √ν 3ν2
2 2 2 2
[ ( )] ( )
We can now compute c˜ x k−1 ,u k .
Computing c˜ x k−1 ,u k ( . By us ) ing the above expression, we have, by letting A x ∶=ln p¯( k x ∣k ) −1 q e ( x x p ) c¯ k( x k) :
k∣k−1
( ) ( )
∞
c˜ x k−1 ,u k =ϕ(1) θ ∗ = ∑ ν k+1 θ ∗k =ν 1 +ν 2 θ ∗+ ν 3 θ ∗2+O θ ∗3
k! 2
k=0
( ) ( ) ( )
=ν 1 +ν 2 √ν 2 η 1 2 − 3 2 ν ν 3 2 η+O η2 3 + ν 2 3 ν 2 η+O η 3 2 +O η 3 2
2 2 2
⎛ ⎞
=ν 1 + 2⎝ ν 2 η 1 2 + ν 3 η+O η 3 2 ( ) ⎠ ( ( )) ( )
3ν
2
√
=E A x +O η2 1 , ( )
p¯(x)
k∣k−1
[ ( )] ( )
where we used the definition of ν . The above expression gives the desired conclusion, as
1
η li → m 0 c˜ x k−1 ,u k =E p¯( k x ∣k ) −1 A x =D KL p¯( k x ∣k ) −1 q k (x ∣k ) −1 +E p¯( k x ∣k ) −1 c¯ k X k .
( ) [ ( )] ( ∣∣ ) [ ( )]
We can now give the proof of Lemma S4.1, which establishes that, for a free energy minimizing agent,
ambiguity cannot be exploited to achieve better performance.
Proof of Lemma S4.1. By breaking down (SI-20), we need to show that
D KL p¯ k (x ∣k ) −1 q k (x ∣k ) −1 +E p¯(
k
x
∣k
)
−1
c¯ k X k <η k x k−1 ,u k +c˜ x k−1 ,u k .
( ∣∣ ) [ ( )] ( ) ( )
That is, by exploiting the definition of the KL divergence
p¯(x) expc(x)
x
E p¯( k x ∣k ) −1 ⎡ ⎢ ⎢ ln k∣k−1 q k (x ∣k ) − k 1 ( k ) ⎤ ⎥ ⎥ <η k x k−1 ,u k +c˜ x k−1 ,u k . (SI-50)
⎢ ⎥ ( ) ( )
⎢ ⎥
Assumingonthecontrarythat ⎢ ⎣theaboveequationdo ⎥ ⎦esnothold, wethenhaveη k x k−1 ,u k +c˜ x k−1 ,u k ≤
ln
p¯(
k
x
∣k
)
−1
expc(
k
x)
(
x
k) p¯(x) dx , this leads to
∫X q(x) k∣k−1 k ( ) ( )
k∣k−1
p¯(x) expc(x)
x
c˜ x k−1 ,u k < ∫ X ln k∣k−1 q(x) k k p¯( k x ∣k ) −1 dx k . (SI-51)
k∣k−1 ( )
( )
Next, we will analyze two cases,
SI-33
Case (1): if c˜ x k−1 ,u k =M x k−1 ,u k , by definition of M x k−1 ,u k , we get
( ) p¯(x) ( expc(x) )
x
( )
c˜ x k−1 ,u k < ∫ X ln k∣k−1 q(x) k k p¯( k x ∣k ) −1 dx k ≤ ∫ X M x k−1 ,u k p¯( k x ∣k ) −1 dx k =M x k−1 ,u k
k∣k−1 ( )
( ) ( ) ( )
which is a contradiction.
Case (2): in this case, we have that there exists some α ⋆ >0 such that
1
p¯(x) expc¯ x α ⋆
c˜
(
x k−1 ,u k
)
=η k
(
x k−1 ,u k
)
α ⋆+α ⋆ lnE p¯( k x ∣k ) −1 ⎡ ⎢ ⎢
⎢ ⎢
⎛
⎜
k∣k−1 q k (x ∣k ) −1 k ( k )⎞
⎟
⎤ ⎥ ⎥
⎥ ⎥
.
⎢⎝ ⎠ ⎥
⎢ ⎥
Then, employing Jensen inequality, it follows that: ⎣ ⎦
p¯(x)
expc¯ x
c˜ x k−1 ,u k ≥η k x k−1 ,u k α ⋆+E p¯( k x ∣k ) −1 ⎡ ⎢ ⎢ ln ⎛ k∣k−1 q k (x ∣k ) −1 k ( k )⎞ ⎤ ⎥ ⎥ .
( ) ( ) ⎢ ⎜ ⎟⎥
⎢ ⎥
This however contradicts (SI-51) since α ⋆ >0 and η k x k−1 , ⎢ ⎣u k ⎝>0. ⎠⎥ ⎦
( )
S7 Concluding Interdisciplinary Remarks And Potential Implications
across Psychology, Economics and Neuroscience
Ambiguityisakeythemeinbothpsychologyandeconomics.45–47 Thelargeliteratureinthisareaspeaksto
akeydialectic: ontheonehand,psychologicalstudiessuggestthatpeopleprefercertaintyoveruncertainty.
Ontheotherhand, theopportunitytoresolveuncertaintyunderwritescognitiveflexibilityinformationand
novelty-seeking behaviour.35,37,40,41,48,52,53 In active inference, this is often described in terms of intrinsic
motivation or epistemic affordance.37,44,50 The paradigms used to study ambiguity generally leverage the
exploration-exploitation dilemma:36,38,39,48,50 e.g., using the two-step maze task or multiarmed bandit
problems. An empirical evaluation of active inference formulations is available in the literature.43 In
behavioural economics, ambiguity refers to uncertain outcomes with uncertain probabilities in contrast
with risk that refers to uncertain outcomes with certain probabilities. Similarly, ambiguity in this work
refers to uncertainty about the environment (i.e., world) model – as opposed to the risk entailed by
uncertainty about outcomes under a given model.
The neural correlates of ambiguity and risk – as assessed using EEG and multiarmed bandit tasks54
– appear to segregate; with activity in the frontal, central and parietal regions reflecting ambiguity; with
activity in frontal and central brain regions reflecting risk. In economic decision making using fMRI,
ambiguouscueselicitactivityinthefrontalandparietalcortexduringoutcomeanticipation.34 Theauthors
suggest that these regions subserve a general function of contextual analysis that reflects situational or
ambiguity awareness.
In this broad context, DR-FREE can potentially offer a formal model of ambiguity awareness that
can be leveraged using computational phenotyping.42,49 In principle, it is possible to estimate ambiguity
awareness – e.g., in terms of the ambiguity radius – by estimating the radius that best explains a given
subject’s responses. For example, computational phenotyping of this sort has been used to characterise
psychiatric cohorts in terms of greater decision uncertainty during approach-avoidance conflict.51
SI-34
Hyperparameter Figure 4.c SF 4.a top SF 4.a bottom SF 4.b top SF 4.b bottom
Model layer N/A N/A N/A 128×128 128×128
Discount factor (γ) 0.95 0.95 0.95 0.95 0.95
Learning rate N/A N/A N/A 0.0005 0.0005
Batch size N/A N/A N/A 128 128
α 0.1 0.1 0.1 0.1 0.1
λ 0.5 0.5 0.5 0.5 0.5
Horizon 2 20 50 2 20
Samples 50 50 100 50 50
Table S-1: MaxDiff Hyperparameters across experiments. SF stands for Supplementary Figure. Model layer refers to the architecture of the neural
network model used to approximate dynamics and reward. The notation x×y means that there are two layers – the width of the first layer is of x
neurons and the width of the second layer is of y neurons. Discount (γ in the text) tunes the importance of future rewards in planning. Learning rate
is used in gradient-based optimization for updating the dynamics/reward model parameters. Batch size determines the number of training data points
used per update step in dynamics/reward model training. The parameter α in the text is the temperature-like parameter in MaxDiff’s objective.
The parameter λ is used to scale the softmax-like weighting of trajectory costs. It controls how sharply or smoothly the algorithm differentiates between
high-cost and low-cost trajectories during action selection. Horizon sets the planning horizon for policy roll-outs. Samples indicates the number of
trajectory samples used for expectation estimation during policy roll-outs. We refer to the original MaxDiff code for a more detailed explanation of each
of the hyperparameters. The values used for the experiments of this paper are set in accordance with the code from the Maxdiff repository.
Hyperparameter Value
Discount factor 0.99
Stopping criterion 10
−5
Maximum iteration 2000
Gradient stopping criterion 0.001
Learning rate 1
1+k
Table S-2: Hyperparameter settings for the belief update benchmark. Discount factor determines the trade-off between immediate and long-term
returns in soft-value iteration. Stopping Criterion specifies the convergence threshold for soft-value iteration. Maximum iteration is the maximum
number of iterations allowed in the soft-value iteration optimisation loop. Gradient stopping criterion is the stopping criterion for gradient descent
optimisation. Learning rate of the gradient descent decays linearly with rate from 1 (k is the learning step). All parameters are defined in our
repository.
SI-35
Method Average Computation time (sec)
DR-FREE 0.22 (763 steps)
Ambiguity Unaware 0.04 (556 steps)
MaxDiff 0.02 (637 steps)
Table S-3: average time required to output an action at each k for all the policy computation methods considered in the paper. The measurements
were obtained as described in the Methods (Experiments settings section). The recordings were obtained from the Robotarium hardware experiments
– in parentheses the number of steps for each experiment (Ambiguity Unaware has a lower number of steps because the robot encountered an obstacle
before reaching the destination). In DR-FREE and ambiguity unaware agent q 0∶N is the one from the first section in the Results – also, the number of
samples used in line 2 of Algorithm 1 was set to 50, consistently with the MaxDiff settings. In fact, for the MaxDiff agent, the planning horizon was set
to 20, and the number of sampled trajectories was also set to 50. This was a configuration where MaxDiff would consistently complete the task.
Hyperparameter MaxDiff NN-MPPI
Model layer 512×512×512 512×512×512
Discount factor (γ) 0.95 0.95
α 5 N/A
λ 0.5 0.5
Samples 1000 1000
Table S-4: Relevant MaxDiff and NN-MPPI hyperparameters for the Ant experiments. Definitions are as in Tab. S-1 and parameters are as in the
literature.5 The network, used in all our Ant experiments, is from the original MaxDiff repository: it outputs mean and variance of p¯ k x k x k−1 ,u k .
( ∣ )
SI-36
Supplementary References
[1] Zhengyuan Zhou, Michael Bloem, and Nicholas Bambos. Infinite Time Horizon Maximum Causal
Entropy Inverse Reinforcement Learning. IEEE Transactions on Automatic Control, 63(9):2787–2802,
September 2018.
[2] S.KullbackandR.Leibler. Oninformationandsufficiency. AnnalsofMathematicalStatistics,22:79–87,
1951.
[3] Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in Telecommu-
nications and Signal Processing). Wiley-Interscience, USA, 2006.
[4] Francis J . Pinski, Gideon Simpson, Andrew M . Stuart, and Hendrik Weber. Kullback–Leibler ap-
proximation for probability measures on infinite dimensional spaces. SIAM Journal on Mathematical
Analysis, 47(6):4091–4122, November 2015.
[5] Thomas A. Berrueta, Allison Pinosky, and Todd D. Murphey. Maximum diffusion reinforcement learn-
ing. Nature Machine Intelligence, 6(5):504–514, May 2024.
[6] ThomasBerrueta. Robot Thermodynamics. Ph.d.dissertation, NortwesternUniversity, December2024.
Available online at https://www.proquest.com/openview/faffd739b9b7a1becbd5e99b0fbd83fe/.
[7] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep
energy-based policies. In Proceedings of the 34th International Conference on Machine Learning -
Volume 70, ICML’17, 1352–1361, 2017.
[8] Brian D. Ziebart, J. Andrew Bagnell, and Anind K. Dey. Modeling interaction via the principle
of maximum causal entropy. In Proceedings of the 27th International Conference on International
Conference on Machine Learning, ICML’10, 1255–1262, Madison, WI, USA, 2010. Omnipress.
[9] Benjamin Eysenbach and Sergey Levine. Maximum entropy RL (provably) solves some robust RL
problems. In International Conference on Learning Representations, 2022.
[10] Mohammad A. Bashiri, Brian Ziebart, and Xinhua Zhang. Distributionally robust imitation learning.
In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances
in Neural Information Processing Systems, 34, 24404–24417. Curran Associates, Inc., 2021.
[11] Hyuk Park, Duo Zhou, Grani A. Hanasusanto, and Takashi Tanaka. Distributionally robust path
integral control. In 2024 American Control Conference (ACC), pages 1164–1171, 2024.
[12] Charalambos D. Charalambous and Farzad Rezaei. Stochastic uncertain systems subject to relative
entropyconstraints: Inducednormsandmonotonicitypropertiesofminimaxgames. IEEETransactions
on Automatic Control, 52(4):647–663, April 2007.
[13] Bart P. G. Van Parys, Daniel Kuhn, Paul J. Goulart, and Manfred Morari. Distributionally robust
control of constrained stochastic systems. IEEE Transactions on Automatic Control, 61(2):430–442,
February 2016.
[14] BohanWu,BennettZhu,andDavidMBlei. Distributionallyrobusstposteriorsampling–avariational
Bayes approach. In Frontiers of Probabilistic Inference workshop at ICLR 2025, 2025.
SI-37
[15] Herbert E. Scarf. A Min-Max Solution of an Inventory Problem. RAND Corporation, Santa Monica,
CA, 1957.
[16] Shengbo Wang, Nian Si, Jose Blanchet, and Zhengyuan Zhou. On the foundation of distributionally
robust reinforcement learning, 2024.
[17] Robert D. McAllister and Peyman M. Esfahani. Distributionally robust model predictive control:
Closed-loopguaranteesandscalablealgorithms. IEEE Transactions on Automatic Control, 70(5):2963–
2978, May 2025.
[18] Janosch Moos, Kay Hansel, Hany Abdulsamad, Svenja Stark, Debora Clever, and Jan Peters. Robust
reinforcementlearning: Areviewoffoundationsandrecentadvances. Machine Learning and Knowledge
Extraction, 4(1):276–315, March 2022.
[19] EmilandGarrabe,HozefaJesawada,CarmenDelVecchio,andGiovanniRusso. Onconvexdata-driven
inverse optimal control for nonlinear, non-stationary and stochastic systems. Automatica, 173:112015,
March 2025.
[20] BaharTaskesen,DanIancu,C¸a˘gılKoc¸yig˘it,andDanielKuhn. Distributionallyrobustlinearquadratic
control. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances
in Neural Information Processing Systems, 36, 18613–18632. Curran Associates, Inc., 2023.
[21] Zijian Liu, Qinxun Bai, Jose Blanchet, Perry Dong, Wei Xu, Zhengqing Zhou, and Zhengyuan Zhou.
Distributionally robust Q-learning. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepes-
vari,GangNiu,andSivanSabato,editors,Proceedings of the 39th International Conference on Machine
Learning, volume 162 of Proceedings of Machine Learning Research, 13623–13643, July 2022.
[22] Johannes Kirschner, Ilija Bogunovic, Stefanie Jegelka, and Andreas Krause. Distributionally robust
bayesian optimization. In Silvia Chiappa and Roberto Calandra, editors, Proceedings of the Twenty
Third International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of
Machine Learning Research, 2174–2184, August 2020.
[23] Alexander Shapiro, Enlu Zhou, and Yifan Lin. Bayesian distributionally robust optimization. SIAM
Journal on Optimization, 33(2):1279–1304, June 2023.
[24] Mohammed Rayyan Sheriff and Peyman Mohajerin Esfahani. Nonlinear distributionally robust opti-
mization. Mathematical Programming, December 2024.
[25] Bart PG Van Parys, Peyman M. Esfahani, and Daniel Kuhn. From data to decisions: Distributionally
robust optimization is optimal. Management Science, 67(6):3387–3402, November 2020.
[26] John C. Duchi, Peter W. Glynn, and Hongseok Namkoong. Statistics of robust optimization: A
generalized empirical likelihood approach. Mathematics of Operations Research, 46(3):946–969, August
2021.
[27] Fengming Lin, Xiaolei Fang, and Zheming Gao. Distributionally robust optimization: A review on
theory and applications. Numerical Algebra, Control & Optimization, 12(1):159, March 2022.
[28] Hamed Rahimian and Sanjay Mehrotra. Frameworks and results in distributionally robust optimiza-
tion. Open Journal of Mathematical Optimization, 3:1–85, July 2022.
SI-38
[29] Zhaolin Hu and L J. Hong. Kullback-Leibler divergence constrained distributionally robust optimiza-
tion. Available at Optimization Online, 1(2):9, November 2013.
[30] Huan Xu and Shie Mannor. Distributionally Robust Markov Decision Processes. In J. Lafferty,
C. Williams, J. Shawe-Taylor, R. Zemel and A. Culotta, editors, Advances in Neural Information
Processing Systems, 23. Curran Associates, Inc., 2010.
[31] Constantin P. Niculescu and Lars-Erik Persson. Convex Functions and Their Applications. Springer
New York, 2006.
[32] David G. Luenberger. Optimization by Vector Space Methods. John Wiley & Sons, Inc., USA, 1st
edition, 1997.
[33] Walter Rudin. Real and complex analysis. McGraw-Hill International Editions, 1987
[34] Dominik R. Bach, Ben Seymour, and Raymond J. Dolan. Neural activity associated with the passive
prediction of ambiguity and risk for aversive events. Journal of Neuroscience, 29:1648–1656, February
2009.
[35] Daniel E. Berlyne. Novelty and curiosity as determinants of explanatory behaviour. British Journal
of Psychology-General Section, 41:68–80, May 1950.
[36] Jonathan D. Cohen, Samuel M. McClure, and Angela J. Yu. Should I stay or should I go? How the
human brain manages the trade-off between exploitation and exploration. Philosophical Transactions
of the Royal Society B: Biological Sciences, 362:933–942, March 2007.
[37] Andrew W. Corcoran, Giovanni Pezzulo, and Jakob Hohwy. From allostatic agents to counterfactual
cognisers: active inference, biological regulation, and the origins of cognition. Biology & Philosophy,
35:32, April 2020.
[38] Nathaniel D. Daw, John P. O’Doherty, Peter Dayan, Ben Seymour, and Raymond J. Dolan. Cortical
substrates for exploratory decisions in humans. Nature, 441:876–879, June 2006.
[39] Shin Ishii, Wako Yoshida, and Junichiro Yoshimoto. Control of exploitation-exploration meta-
parameter in reinforcement learning. Neural Networks, 15:665–687, July 2002.
[40] Julian Kiverstein, Marius Miller, and Erik Rietveld. The feeling of grip: novelty, error dynamics, and
the predictive brain. Synthese, 196, October 2017.
[41] Romy M. Krebs, Bjo¨rn H. Schott, Henrik Schutze, and Emrah Duzel. The novelty exploration bonus
and its attentional modulation. Neuropsychologia, 47:2272–2281, September 2009.
[42] Dragutin Markovi´c, Annalina M.F. Reiter, and Stefan J. Kiebel. Revealing human sensitivity to a
latent temporal structure of changes. Frontiers in Behavioral Neuroscience, 16:962494, October 2022.
[43] Dragutin Markovi´c, Hrvoje Stoji´c, Sven Schwo¨bel, and Stefan J. Kiebel. An empirical evaluation of
active inference in multi-armed bandits. Neural Networks, 144:229–246, December 2021.
[44] Pierre-YvesOudeyerandFr´ed´ericKaplan. Whatisintrinsicmotivation? Atypologyofcomputational
approaches. Frontiers in Neurorobotics, 1:6, November 2007.
SI-39
[45] Joel M. Pearson, Kristina K. Watson, and Michael L. Platt. Decision making: the neuroethological
turn. Neuron, 82:950–965, June 2014.
[46] Ming Hsu, Meghana Bhatt, Ralph Adolphs, Daniel Tranel, and Colin F. Camerer. Neural systems
responding to degrees of uncertainty in human decision-making. Science, 310:1680–1683, December
2005.
[47] Paul J. Zak. Neuroeconomics. Philosophical Transactions of the Royal Society B: Biological Sciences,
359:1737–1748, November 2004.
[48] Philipp Schwartenbeck, Thomas Fitzgerald, Raymond J. Dolan, and Karl Friston. Exploration, nov-
elty, surprise, and free energy minimization. Frontiers in Psychology, 4:710, October 2013.
[49] Philipp Schwartenbeck and Karl Friston. Computational phenotyping in psychiatry: A worked exam-
ple. eNeuro, 3:0049-0016.2016, August 2016.
[50] Philipp Schwartenbeck, Johannes Passecker, Tobias U. Hauser, Thomas H. FitzGerald, Martin Kro-
nbichler, and Karl J. Friston. Computational mechanisms of curiosity and goal-directed exploration.
eLife, 8:e41703, May 2019.
[51] Ryan Smith, Natasa Kirlic, Jennifer L. Stewart, Joni Touthang, Robert Kuplicki, Sahib S. Khalsa,
Justin Feinstein, Martin P. Paulus, and Robin L. Aupperle. Greater decision uncertainty character-
izes a transdiagnostic patient sample during approach-avoidance conflict: a computational modelling
approach. Journal of Psychiatry & Neuroscience, 46:E74–E87, January 2021.
[52] Bianca C. Wittmann, Nathaniel D. Daw, Ben Seymour, and Raymond J. Dolan. Striatal activity
underlies novelty-based choice in humans. Neuron, 58:967–973, June 2008. Philosophical Transactions
of the Royal Society B: Biological Sciences, 359:1737–1748, 2004.
[53] AndrewBarto,MarcoMirolli,andGianlucaBaldassarre. Noveltyorsurprise? FrontiersinPsychology,
4:907, December 2013.
[54] ShiyuZhang,YuchenTian,QiangLiu,andHaoWu. Theneuralcorrelatesofnoveltyandvariabilityin
humandecision-makingunderanactiveinferenceframework. eLifeSciencesPublications, Ltd,February
2025.
SI-40

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Distributionally Robust Free Energy Principle for Decision-Making"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
