=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Successor Representation Active Inference
Citation Key: millidge2022successor
Authors: Beren Millidge, Christopher L Buckley

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2022

Abstract: Recent work has uncovered close links between between classical reinforcement learning al-
gorithms,Bayesianfiltering,andActiveInferencewhichletsusunderstandvaluefunctionsinterms
ofBayesianposteriors. Analternative,butlessexplored,model-freeRLalgorithmisthesuccessor
representation,whichexpressesthevaluefunctionintermsofasuccessormatrixofexpectedfuture
state occupancies. In this paper, we derive the probabilistic interpretation of the successor repres-
entation in terms of Bayesian filtering and ...

Key Terms: successor, representation, california, losangeles, versesresearchlab, inference, millidge, active, buckley, friston

=== FULL PAPER TEXT ===

SUCCESSOR REPRESENTATION ACTIVE INFERENCE
BerenMillidge∗ ChristopherLBuckley
MRCBrainNetworksDynamicsUnit, SussexAIGroup,DepartmentofInformatics,
UniversityofOxford,UK, UniversityofSussex,UK
VersesResearchLab, VersesResearchLab,
LosAngeles,California,USA LosAngeles,California,USA
beren@millidge.name C.L.Buckley@sussex.ac.uk
ABSTRACT
Recent work has uncovered close links between between classical reinforcement learning al-
gorithms,Bayesianfiltering,andActiveInferencewhichletsusunderstandvaluefunctionsinterms
ofBayesianposteriors. Analternative,butlessexplored,model-freeRLalgorithmisthesuccessor
representation,whichexpressesthevaluefunctionintermsofasuccessormatrixofexpectedfuture
state occupancies. In this paper, we derive the probabilistic interpretation of the successor repres-
entation in terms of Bayesian filtering and thus design a novel active inference agent architecture
utilizingsuccessorrepresentationsinsteadofmodel-basedplanning. Wedemonstratethatactivein-
ference successor representations have significant advantages over current active inference agents
intermsofplanninghorizonandcomputationalcost. Moreover,wedemonstratehowthesuccessor
representation agent can generalize to changing reward functions such as variants of the expected
freeenergy.
1 Introduction
Active Inference (AIF) is an unifying theory of action selection in theoretical neuroscience K. Friston et al. (2015);
K. Friston, Samothrakis and Montague (2012); K. J. Friston, Daunizeau and Kiebel (2009). It suggests that action
selection, like perception, is fundamentally a problem of inference and that agents select actions by maximizing
evidenceunderabiasedgenerativemodelDaCosta,Parretal.(2020);K.Friston,FitzGerald,Rigoli,Schwartenbeck
andPezzulo(2017a). ActiveinferenceoperatesundertheaegisoftheBayesianbrainhypothesisDoya,Ishii,Pouget
andRao(2007);KnillandPouget(2004)andfreeenergyprinciplesAguilera,Millidge,TschantzandBuckley(2021);
Buckley, Kim, McGregor and Seth (2017); K. Friston (2019); K. Friston and Ao (2012); K. Friston, Kilner and
Harrison(2006);Millidge,SethandBuckley(2021)andpossessesseveralneurobiologicalprocesstheoriesK.Friston
etal.(2017a);Parr,Markovic,KiebelandFriston(2019).
∗Correspondingauthor.
2202
luJ
02
]IA.sc[
1v79890.7022:viXra
Recent work Millidge, Tschantz, Seth and Buckley (2020) has uncovered close links between active inference and
the framework of control as inference, which shows how many classical reinforcement learning algorithms can be
understood as performing Bayesian inference to infer optimal actions Attias (2003); Levine (2018); Rawlik (2013);
Toussaint(2009). Theseworks,aswellastherelateddualitybetweencontrolandinferenceinlinearlysolvableMDPs
Todorov (2008, 2009) has allowed us to understand classical objects in reinforcement learning such as Q-functions
and value functions in terms of Bayesian filtering posteriors. Similarly, close connections between active inference
andreinforcementlearningmethodshavealsobeendemonstratedMillidge(2019a,2019b);Tschantz,Millidge,Seth
and Buckley (2020b). It has been shown that deep active inference agents can be derived that can perform actor-
criticalgorithmsMillidge(2019b)aswellasmodel-basedreinforcementlearningFountas,Sajid,MedianoandFriston
(2020); Tschantz, Millidge, Seth and Buckley (2020a); Tschantz et al. (2020b), while the fundamental difference
betweenthemhasbeenfoundtoberelatedtotheencodingofvalueintothegenerativemodelMillidge(2021);Millidge
etal.(2020). Moreover,ithasbecomeobviousthatactiveinferencecanbetreatedinamodel-free(Bellman-equation)
paradigmwithsimplyadistinctrewardfunction(theexpectedfreeenergy)DaCosta, Sajid, Parr, FristonandSmith
(2020);Millidge(2019b).However,whilemuchofthisworkhasfocusedonunderstandingvaluefunctionsandmodel-
basedRL,anotherfundamentalobjectinmodel-freereinforcementlearningisthesuccessorrepresentation,whichhas
receivedmuchlessattentionoverall. Thesuccessorrepresentation(SR)Dayan(1993)providesanalternativewayto
estimate value functions. Instead of estimating the value function with the Bellman backup, a successor matrix of
long-term discounted state transitions is estimated instead and then dynamically combined with the reward function
to yield the value function for a fixed policy. Compared to estimating the value function directly, the SR requires
morememorytostorethesuccessormatrixbutgrantstheabilitytodynamicallyrecomputethevaluefunctionasthe
rewardfunctionchangesaswellasprovidingacompressedformofa‘cognitivemap’oftheenvironmentwhichcanbe
directlyusedforexplorationandoptiondiscoveryMachado,BellemareandBowling(2020);Machadoetal.(2017);
Momennejad(2020). moreover,fromaneuroscientificperspective,theSRhasbeencloselylinkedtorepresentations
in the hippocampus Momennejad et al. (2017); Stachenfeld, Botvinick and Gershman (2017) which are concerned
with representing abstract (usually spatial) relations Behrens et al. (2018); Whittington, McCaffary, Bakermans and
Behrens(2022);Whittingtonetal.(2020).
Inthiswork, applyingtheprobabilisticinterpretationoftheSR,weshowcasehowtheSRcanbedirectlyintegrated
withstandardmethodsinactiveinference,resultinginthesuccessor-representationactiveinference(SR-AIF)agent.
We show how SR-AIF has significant computational complexity benefits over standard AIF and that, moreover, the
explicitgenerativemodelinAIFenablestheSRtobecomputedinstantlywithoutrequringsubstantialexperiencein
theenvironment. Additionally,weshowhowSRmethodscanflexiblyrepresentthevaluefunctionoftheEFEandcan
beusedtodynamicallytrade-offexplorationandexploitationatrun-time.
2 ActiveInference
Discrete-state-space active inference possesses a large literature and several thorough tutorials Da Costa, Parr et al.
(2020);K.Friston,FitzGerald,Rigoli,SchwartenbeckandPezzulo(2017b);Smith,FristonandWhyte(2022),sowe
onlyprovidetheessentialshere. AIFconsidersagentsactinginPOMDPswithobservationso, statesx, andactions
u. Theagentoptimizesoverpoliciesπ = [u ,u ,...]whicharesimplysequencesofactions. Theagentistypically
1 2
2
assumedtobeequippedwithagenerativemodeloftheenvironmentp(o ,x )whichdescribeshowobservations
1,T 1:T
andstatesarerelatedovertime.Thisgenerativemodelcanbefactorizedintotwocorecomponents:alikelihoodmodel
p(o |x ) which states how observations are generated from states and is represented by a likelihood matrix denoted
t t
A,andatransitionmodelp(x |x ,u )whichstateshowstateschangedependingonthepreviousstateandaction
t t−1 t−1
and is represented by a transition matrix denoted B(u). The rewards or goals of the agents are encoded as strong
priorsinthegenerativemodelandareencodedina‘goalvector’denotedC. SinceAIFconsidersagentsembedded
in a POMDP it has to solve both state inference and action selection problems. State inference is performed using
variationalinferencewithacategoricalvariationaldistributionq(x )whichisobtainedbyminimizingthevariational
t
freeenergyforaspecifictimestept,
q∗(x )=argmin F =argmin E [logq(x |o )−logp(o ,x |x ,u )] (1)
t t q(xt|ot) t t t t t−1 t−1
q q
AIFusesauniqueobjectivefunctioncalledtheExpectedFreeEnergy(EFE)whichcombinesutilityorrewardmax-
imization with an information gain term which promotes exploration. AIF agents naturally perform both reward-
seekingandinformation-seekingbehaviourK.Fristonetal.(2015);Millidge,TschantzandBuckley(2021);Millidge,
Tschantz,SethandBuckley(2021);ParrandFriston(2019). TheEFEisdefinedas,
G (o ,x )=E [logq(x )−logp˜(o ,x |x ,u )]
t t t q(ot,xt) t t t t−1 t−1
=E [logp˜(o )]−E (cid:2) KL[q(x |o )||q(x )] (cid:3) (2)
q(ot,xt) t q(ot) t t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ExpectedUtility ExpectedInformationGain
Wherep˜isa‘biased’generativemodelwhichcontainsthegoalpriorvectorC.Ascanbeseen,theEFEcanbedecom-
posedintoareward-seekingandexploratorycomponentwhichunderliestheflexibleuncertainty-reducingbehaviour
ofAIFagents. Toselectactions,AIFsamplesfromtheprioroverpoliciesq(π)whichisdefinedasthesoftmaxover
the path integral of the EFE into the future for each timestep. Typically, future policies are evaluated up to a time
horizonT. Thispathintegralcanbeexpressedas,
T
(cid:88)
q(π)=σ( Gπ(o ,x )) (3)
t t t
t
where σ(x) = e−x is the softmax function. Evaluating this path integral exactly for each policy is typically
(cid:80) e−x
x
computationallyextremelyexpensiveandhasexponentialcomplexityduetotheexponentiallybranchingnumberof
possible futures to be evaluated. This causes AIF agents to run slowly in practice and has encouraged research into
alternative‘deep’activeinferenceagentswhichestimatethispathintegralinothermoreefficient(butonlyapproxim-
ate)waysK.Friston,DaCosta,Hafner,HespandParr(2021)Here,wepresentanovelapproachbasedonsuccessor
representations.
3 SuccessorRepresentation
The Successor Representation Dayan (1993) provides an alternative way to compute the value function of a state.
Instead of directly learning the value function (or Q function), for instance by temporal difference (TD) learning,
the successor representation learns the successor matrix, which is the discounted long term sum of expected state
occupancies,fromwhichthevaluefunctioncanbedynamicallycomputedbysimplymultiplingthesuccessormatrix
withtherewardfunction. ThisallowstheSRtoinstantlyadaptbehaviourtochangingrewardfunctionsonlinewithout
explicitmodel-basedplanning.
3
Thevaluefunctioncanbedefinedastheexpectedlongtermsumofrewards,
Vπ(x)=r(x)+γBπVπ(x)
=r(x)+γBπ[r(x)+γ2Bπ[r(x)+γ3Bπ[···]]] (4)
WhereweassumeafixedpolicyπforthevaluefunctionandtransitionmatrixBandwhereγisascalardiscountrate.
Due to this fixed policy assumption, the max operator in the Bellman equation disappears, so the Bellman equation
becomeslinear. Wecanrearrange,
Vπ(x)=r(x)+γBπ[r(x)+γ2Bπ[r(x)+γ3Bπ[···]]]
=(I+γBπ+γ2BπBπ+···)r(x)=Mπr(x) (5)
WhereMπ isthesuccessormatrixandcanbethoughtofasencodingthelong-runprobabilitythatstatextransitions
tostatex(cid:48).
4 SuccessorRepresentationasInference
In a special class of MDPs known as linearly solvable MDPs there is a general duality between control and infer-
ence Todorov (2008, 2009) such that control can be cast as a Bayesian filtering problem where the value function
correspondstotheposterior. Toseethis,considertheoptimalBellmanequation,
V∗(x )=argmax (cid:2) r(x )+c(u)+E [γV∗(x )] (cid:3) (6)
t t p(xt+1|xt,u) t+1
u
wherewehaveaddedanadditionalcontrolcostc(u). ThefundamentalchallengeisthenonlinearityoftheBellman
equationduetotheargmaxoperation. Todorov(2009)noticedthatifthedynamicsareconsideredtobecompletely
controllable and set by the action p(x |x ,u) = u(·|x ), while the original dynamics instead take the form of a
t+1 t t
‘dynamicsprior’inthecontrolcostwhichissettoKL[u(·|x )||p(x |x ,u)]whichpenalizesdivergencefromthe
t t+1 t
priordynamics,thentheargmaxisanalyticallysolvable. Bydefiningthe‘desirabilityfunction’z(x) = e−V∗(x) and
exponentiating,wecanobtainalinearequationinz,
z(x)=e−r(x)E [γz(x )] (7)
p(xt+1|xt) t+1
whichcanbesolvedeasily. Crucially,however,thisequationtakesthesameformastheBayesianfilteringrecursion
p(x |o ) ∝ p(o |x )E p(x |o )whenwemaketheidentificationofthe‘desirability’z(x )withthepos-
t t t t p(xt|xt−1 t−1 t−1 t
teriorp(x
t
|o
t
)andtheexponentiatedrewarde−r(xt)withthelikelihoodp(o
t
|x
t
). Interestingly,thissamerelationship
between exponentiated reward and probability is also used heuristically in the control as inference literature Levine
(2018). Anadditionalsubtlepointisthatcontrolisaboutthefutureinsteadofthepastsothesumtelescopesforward
insteadofbackwardsintime. ByfactoringEquation6asinEquation5,itisstraightforwardtoobservethat,
T T T
(cid:88) (cid:16)(cid:89)(cid:88) (cid:17) (cid:88)
M = γτ p(x |x ) = γτp(x |x ) (8)
i i−1 τ t
τ=t i=t xi τ=t
Ineffect,wecanthinkofM asrepresentingthediscountedsumoftheprobabilitiesofallthepossibletimestoreach
statexoverthetimehorizon. Asimilarandnovelresultcanbederivedforthecaseofgeneral(notlinearlysolvable)
MDPs but with a fixed policy except here we derive an upper bound on the SR instead of an equality. We begin by
4
takingthelogofthebackwardsBayesianfilteringposteriorandthenrepeatedlyapplyingJensen’sinequalitytoobtain,
logp(x |o )=logp(o |x )+logE [logp(x |o )]
t t t t p(xt+1|xt) t+1 t+1
≤logp(o |x )+E
(cid:104)(cid:2)
logp(o |x )+E [log...]
(cid:3)(cid:105)
(9)
t t p(xt+1|xt) t t p(xt+2|xt+1)
WhichhasthesamerecursivestructureasthelinearBellmanEquationforafixedpolicy(Equation4)solongaswe
maintaintheequivalencebetweenthevaluefunctionandthelogposteriorandtherewardandtheloglikelihoodand
implicitlysetthediscountfactorγto1. ThetechniqueinEquation5canthenbeappliedtogivethesameprobabilistic
interpretationoftheSRasEquation8. Insum, wehaveshownhowoptimalcontrolcanbeassociatedwithfiltering
andBayesianposteriorsexactlyinthecaseoflinearMDPsandtheBayesianposteriorasanupperboundinthecase
ofafixedpolicy. TheseresultsprovideasoundprobabilisticandBayesianinterpretationoftheSR,whichhashiterto
beenmissingintheliterature,andletusdesignaprincipledactiveinferenceagentbasedupontheSR.
5 SuccessorRepresentationActiveInference
UsingtheprobabilisticinterpretationoftheSRandtheequationsofdiscretestate-spaceAIF,wecanconstructanAIF
agentwhichutilizestheSRtocomputevaluefunctionsofactionsinsteadofmodel-basedplanning. Thatis,thepolicy
posteriorpathintegralq(π) = σ(G) = σ(
(cid:80)T
G )canbeconsideredasavaluefunctionanddynamicallycomputed
t t
usingtheSR.ThefactthatindiscreteAIFthegenerativemodeltransitionmatrixB(u)isgivenallowsustodispense
with learning the successor matrix from experience. However, to apply the SR, we need to choose which policy π
we wish to compute the value function under. This choice is important since the default policy must assign enough
probabilitymasstoallpartsofthestate-spacetobeabletoprovideanaccuratevalueestimatethere. Heuristically,we
set the default policy to be uniform over the action space p(u) = 1 where A is the cardinality of the action space.
A
Thisletsusdefinethedefaulttransitionmatrix,
B˜ =E [B(u)]= 1 (cid:88) B[:,:,u ] (10)
p(u) A i
i
GivenB˜,wecananalyticallycalculatetheSRusingtheinfiniteseriesresult,
Mπ =(I+γB˜+γ2B˜2···)=(I−γB˜)−1 (11)
This means that as long as the generative model is known, the EFE value function q(π) can be computed exactly
without any interaction with the environment by first computing Mπ as in Equation 11 and then multiplying by the
reward function which is the EFE G = MπGπ(x). From this EFE value function actions can be sampled from the
posterioroveractionsas,
u∼q(π)=σ(G) (12)
A slight complication is that while the SR is defined for MDPs, AIF typically assumes a POMDP structure with
observationsothatdonotfullyspecifythehiddenstatebutarerelatedthroughthelikelihoodmatrixA. Weaddress
thisbycomputingobservationvaluefunctionsastheexpectedstateposteriorunderthestateposteriordistribution,
Vπ(o)=E [Vπ(x)]=qMπGπ (13)
q(x|o)
whereq = [q ,q ···]isthecategoricalvariationalposterior. TheSR-AIFalgorithmcanthusbesummarizedasfol-
1 2
lows: wearegivenagenerativemodelcontainingtheAandBmatricesandasetofdesiredstatesC. Atinitialization,
5
the agent computes the successor matrix Mπ using the default policy with Equation 10. For each action in a given
state,SR-AIFcomputestheEFEvaluefunctionGπ forthatactionandthenactionsaresampledfromthepolicypos-
teriorwhichisthesoftmaxovertheEFEaction-valuefunctions. InaPOMDPenvironmentexactlythesameprocess
takesplaceexceptinsteadofaction-statewehaveaction-observationvaluefunctionswhicharecomputedasEquation
13.
5.1 ComputationalComplexity
IntheorythecomputationalcomplexityofSR-AIFissuperiorthanstandardAIFasstandard-AIFusesmodel-based
planning which evaluates the EFE value function G by exhaustively computing all possible future trajectories for
different policies. This has a cost that grows exponentially in the time horizon due to the branching of possible
futures. If we denote the number of actions A, the dimension of the state-space X and the time-horizon T, we can
approximately say that the computational complexity of standard AIF is of order O(XT2 ·AT) since the number
of possible trajectories is approximately AT where evaluating each step of a trajectory costs of order X and we
mustrepeatthisforeachtimestep. Thisisexponentialinthetime-horizonandrendersAIFunsuitableforlongterm
planning. Several heuristic methods have been proposed to handle this, usually by pruning obviously unsuccessful
policies K. Friston et al. (2021). However, this does not remove the exponential complexity but only reduces it by
a constant factor. In practice, this exponential explosion is also handled by simply reducing the time-horizon or the
policyspacetobesearched,whichrenderstheevaluationofG approximateandmakestheAIFagentsmyopictolong
termrewardcontingencies.
By contrast, SR-AIF analytically computes an approximation to the EFE value function G directly from the known
transitiondynamicsbyEquation11. Thismeansthatnoexhaustivefuturesimulationisrequiredforeachactionbut
instead only a one-time cost is incurred at initialization. The main cost is the matrix inverse of approximately X3.
Then an action must be selected which costs of order A. This means the total complexity of SR-AIF is of order
O(X3 + AT). SR-AIF thus reduces the computational complexity of AIF from exponential to cubic and hence,
in theory, allows discrete-state-space active inference to be applied to substantially larger problems than previously
possible.
6 Experiments
WeempiricallydemonstratethesuperiorcomputationalcomplexityandultimateperformanceofSR-AIFasthestate-
spaceandtime-horizongrowsonaseriesofgrid-worldenvironments. Theseprovideasimpletest-bedenvironment
for evaluating computational complexity in practice without the confounding factors introduced by a more complex
environment.TheagentisinitializedrandomlyinanN×N gridandmustreacharewardlocatedinthebottomcorner
ofthegrid. Onaverage,asthegridsizeincreases,boththestate-spacesizeandtheplanninghorizonrequiredtofind
thisrewardincrease.
We implemented the AIF agent using the pymdp library for discrete state-space AIF Heins et al. (2022). We found
thatforlargergrid-sizesthematrixinverseusedtocomputethesuccessormatrixoftenbecamenumericallyunstable.
Heuristically,wecounteredthisbyincreasingthe‘discountrate’γ inEquation11tobegreaterthan1(weused5for
largergrid-sizes). OtherwiseγforSR-AIFandstandardAIFwassetto0.99. Intuitivelythiscanbeseenasweighting
6
Figure1: TopRow: A:Schematicofthegrid-worldtask. TheAIFagentisinitializedinarandomsquareandmust
makeittothebottomcornertoobtainreward.B:ThetotalrewardobtainedonaverageforSR-AIFandAIFagents.Due
toalimitedplanninghorizon,theAIFagentcannotsolvelargergridworldsandhenceincurslargenegativerewards.
C: Computational cost (measured in compute time per episode) for SR-AIF. For small gridworlds, SR-AIF is more
expensive since the matrix inversion cost dominates while for larger gridworlds the cost of standard AIF increases
exponentially. BottomRow: VisualizationofthedefaultpolicymatrixB˜,SuccessormatrixM,andestimatedvalue
functionVπ fora3×3gridworld.
future states more than the present and had the numerical effect of increasing the differences in the value function
betweenfarawaystateswhichwouldotherwisecollapsetoanegligiblevalue. This,however,isaheuristicdeviceand
removestheprobabilisticinterpretationofthediscountrateasaprobabilityofagentsurvivalasinLevine(2018). For
theactiveinferenceagent,tokeepcomputationtimesmanagable,weusedanplanninghorizonof7andpolicylength
of7.
This task had no epistemic contingencies but only involved reward maximization. A key aspect of active inference
thoughisitsnativehandlingofuncertaintythroughtheEFEobjective.Here,wedemonstratethattheSRrepresentation
canadapttouncertaintyanddynamicallychangethebalanceofexplorationandexploitation. Todemonstratethis,we
introduceanuncertaintyandexplorationcomponentintothegridworldtaskbysettingsomesquaresofthegridtobe
‘unknowable’ such that if the agent is on these squares, it is equally likely to receive an observation from any other
squareinthesamerowofthegrid. ThisisdonebysettingcolumnsoftheAmatrixtouniformdistributionsforeach
‘unknowable’squareinthegrid.WeshowthatifequippedwiththeEFEobjectivefunction,SR-AIFisabletoinstantly
recomputethevaluefunctionbasedonthisinformationintheAmatrixwithouthavingtochangethesuccessormatrix
M. Moreover,duetothepropertythatthevaluefunctioncanberecomputedforeachrewardfunction,thisallowsa
dynamicweightingoftheutilityandinformationgaincomponentsoftheEFEtotakeplaceatruntime.
7
Figure 2: Effect of introducing observation uncertainty into the model. A: the A matrix with two ‘unknowable’
squaresresultinginauniformdistributionintwocolumns. B:thecorrespondingentropyofthestate-spacewiththe
two‘unknowable’squareshavinghighentropy. CandD:ThevaluefunctioncomputedusingtheEFEwhichresponds
positively to regions of high uncertainty since there is the potential for information gain compared to the standard
rewardfunction. SR-AIFisabletocorrectlycombinebothutilityandepistemicdrivesatruntime.
7 Discussion
Inthispaper,wehavederivedaprobabilisticinterpretationoftheSRandrelatedittocontrolasinferenceandlinear
RL. We then constructed an SR-AIF algorithm which exhibits superior significant performance and computational
complexitybenefitstostandardAIFduetoitsamortizationofpolicyselectionusingasuccessormatrixwhichcanbe
computedanalyticallyatinitialization.
ItisimportanttonotethatwhiletheSR-AIFhassubstantiallybettercomputationalcomplexity,thiscomesatthecost
ofanecessaryapproximation. Thesuccessormatrixiscomputedonlyforafixeddefaultpolicyπ andthechoiceof
thispolicycanhavesignificanteffectsupontheestimatedvaluefunctionandhenceuponbehaviour. Thechoiceofthe
default policy is thus important to performance and was here chosen entirely on heuristic grounds. Principled ways
ofestimatingorbootstrappingbetterdefaultpolicieswouldbeimportantforimprovingtheperformanceofSR-AIFin
practice. ThisisespeciallyimportantduetothereflexivityofRLenvironmentswherebythedefaultexplorationpolicy
determineswhatdatawillbesampledfromtheenvironmentwhichisthenusedtofurthertrainthemodelandrefine
theSR.Alternatively,theMDPitselfcouldberegularizedsothatitbecomeslinearasinTodorov(2009)suchthatthe
optimal policy can be solved for directly. This approach has been applied in a neuroscience context Piray and Daw
(2021)buttheextensiontoactiveinferenceremainstobeinvestigated.
AnotherpointofextensionisthatherewehaveconsideredAIFandSR-AIFinthecontextofasinglesensorymodality
and a single-factor generative model. However, many tasks modelled by AIF use multiple factors and modalities to
expressmorecomplexrelationshipsandcontingencies. TheextensionofSR-AIFtomultiplemodalitiesandfactorsis
straightforwardalgebraically,buthassubtleimplementationdetailsandislefttofuturework.
8
8 CodeAvailability
Codetoreproduceallexperimentsandfigurescanbefoundat:
https://github.com/BerenMillidge/Active Inference Successor Representations.
9 Acknowledgements
Beren Millidge is supported by the BBSRC grant BB/S006338/1 and by Verses Research. CLB is supported by
BBRSCgrantnumberBB/P022197/1andbyJointResearchwiththeNationalInstitutesofNaturalSciences(NINS),
Japan,programNo. 01112005.
References
Aguilera, M., Millidge, B., Tschantz, A.&Buckley, C.L. (2021). Howparticularisthephysicsofthefreeenergy
principle? PhysicsofLifeReviews.
Attias,H. (2003). Planningbyprobabilisticinference. InAistats.
Behrens, T. E., Muller, T. H., Whittington, J. C., Mark, S., Baram, A. B., Stachenfeld, K. L. & Kurth-Nelson, Z.
(2018). Whatisacognitivemap? organizingknowledgeforflexiblebehavior. Neuron,100(2),490–509.
Buckley,C.L.,Kim,C.S.,McGregor,S.&Seth,A.K. (2017). Thefreeenergyprincipleforactionandperception:
Amathematicalreview. JournalofMathematicalPsychology,81,55–79.
DaCosta,L.,Parr,T.,Sajid,N.,Veselic,S.,Neacsu,V.&Friston,K.(2020).Activeinferenceondiscretestate-spaces:
asynthesis. arXivpreprintarXiv:2001.07203.
DaCosta,L.,Sajid,N.,Parr,T.,Friston,K.&Smith,R. (2020). Therelationshipbetweendynamicprogrammingand
activeinference: Thediscrete,finite-horizoncase. arXivpreprintarXiv:2009.08111.
Dayan, P. (1993). Improvinggeneralizationfortemporaldifferencelearning: Thesuccessorrepresentation. Neural
Computation,5(4),613–624.
Doya,K.,Ishii,S.,Pouget,A.&Rao,R.P. (2007). Bayesianbrain: Probabilisticapproachestoneuralcoding. MIT
press.
Fountas, Z., Sajid, N., Mediano, P.&Friston, K. (2020). Deepactiveinferenceagentsusingmonte-carlomethods.
Advancesinneuralinformationprocessingsystems,33,11662–11675.
Friston,K. (2019). Afreeenergyprincipleforaparticularphysics. arXivpreprintarXiv:1906.10184.
Friston,K.&Ao,P.(2012).Freeenergy,value,andattractors.Computationalandmathematicalmethodsinmedicine,
2012.
Friston,K.,DaCosta,L.,Hafner,D.,Hesp,C.&Parr,T. (2021). Sophisticatedinference. NeuralComputation,33(3),
713–763.
Friston,K.,FitzGerald,T.,Rigoli,F.,Schwartenbeck,P.&Pezzulo,G. (2017a). Activeinference: aprocesstheory.
Neuralcomputation,29(1),1–49.
Friston,K.,FitzGerald,T.,Rigoli,F.,Schwartenbeck,P.&Pezzulo,G. (2017b). Activeinference: aprocesstheory.
Neuralcomputation,29(1),1–49.
Friston, K., Kilner, J. & Harrison, L. (2006). A free energy principle for the brain. Journal of Physiology-Paris,
100(1-3),70–87.
9
Friston,K.,Rigoli,F.,Ognibene,D.,Mathys,C.,Fitzgerald,T.&Pezzulo,G. (2015). Activeinferenceandepistemic
value. Cognitiveneuroscience,6(4),187–214.
Friston, K., Samothrakis, S. & Montague, R. (2012). Active inference and agency: optimal control without cost
functions. Biologicalcybernetics,106(8-9),523–541.
Friston,K.J.,Daunizeau,J.&Kiebel,S.J. (2009). Reinforcementlearningoractiveinference? PloSone,4(7).
Heins, C., Millidge, B., Demekas, D., Klein, B., Friston, K., Couzin, I. & Tschantz, A. (2022). pymdp: A python
libraryforactiveinferenceindiscretestatespaces. arXivpreprintarXiv:2201.03904.
Knill, D. C. & Pouget, A. (2004). The bayesian brain: the role of uncertainty in neural coding and computation.
TRENDSinNeurosciences,27(12),712–719.
Levine,S. (2018). Reinforcementlearningandcontrolasprobabilisticinference: Tutorialandreview. arXivpreprint
arXiv:1805.00909.
Machado,M.C.,Bellemare,M.G.&Bowling,M.(2020).Count-basedexplorationwiththesuccessorrepresentation.
InProceedingsoftheaaaiconferenceonartificialintelligence(Vol.34,pp.5125–5133).
Machado, M. C., Rosenbaum, C., Guo, X., Liu, M., Tesauro, G. & Campbell, M. (2017). Eigenoption discovery
throughthedeepsuccessorrepresentation. arXivpreprintarXiv:1710.11089.
Millidge,B. (2019a). Combiningactiveinferenceandhierarchicalpredictivecoding: Atutorialintroductionandcase
study.
Millidge,B. (2019b). Deepactiveinferenceasvariationalpolicygradients. arXivpreprintarXiv:1907.03876.
Millidge,B. (2021). Applicationsofthefreeenergyprincipletomachinelearningandneuroscience. arXivpreprint
arXiv:2107.00140.
Millidge, B., Seth, A. & Buckley, C. L. (2021). A mathematical walkthrough and discussion of the free energy
principle. arXivpreprintarXiv:2108.13343.
Millidge,B.,Tschantz,A.&Buckley,C.L. (2021). Whencetheexpectedfreeenergy? NeuralComputation,33(2),
447–482.
Millidge,B.,Tschantz,A.,Seth,A.&Buckley,C. (2021). Understandingtheoriginofinformation-seekingexplora-
tioninprobabilisticobjectivesforcontrol. arXivpreprintarXiv:2103.06859.
Millidge, B., Tschantz, A., Seth, A. K. & Buckley, C. L. (2020). On the relationship between active inference and
controlasinference. arXivpreprintarXiv:2006.12964.
Momennejad,I. (2020). Learningstructures: predictiverepresentations,replay,andgeneralization. CurrentOpinion
inBehavioralSciences,32,155–166.
Momennejad,I.,Russek,E.M.,Cheong,J.H.,Botvinick,M.M.,Daw,N.D.&Gershman,S.J.(2017).Thesuccessor
representationinhumanreinforcementlearning. Naturehumanbehaviour,1(9),680–692.
Parr, T. & Friston, K. J. (2019). Generalised free energy and active inference. Biological cybernetics, 113(5-6),
495–513.
Parr,T.,Markovic,D.,Kiebel,S.J.&Friston,K.J. (2019). Neuronalmessagepassingusingmean-field,bethe,and
marginalapproximations. Scientificreports,9(1),1–18.
Piray,P.&Daw,N.D. (2021). Linearreinforcementlearninginplanning,gridfields,andcognitivecontrol. Nature
Communications,12(1),1–20.
Rawlik,K.C. (2013). Onprobabilisticinferenceapproachestostochasticoptimalcontrol.
10
Smith, R., Friston, K. J. & Whyte, C. J. (2022). A step-by-step tutorial on active inference and its application to
empiricaldata. Journalofmathematicalpsychology,107,102632.
Stachenfeld, K. L., Botvinick, M. M. & Gershman, S. J. (2017). The hippocampus as a predictive map. Nature
neuroscience,20(11),1643–1653.
Todorov,E.(2008).Generaldualitybetweenoptimalcontrolandestimation.In200847thieeeconferenceondecision
andcontrol(pp.4286–4292).
Todorov, E. (2009). Efficient computation of optimal actions. Proceedings of the national academy of sciences,
106(28),11478–11483.
Toussaint,M. (2009). Probabilisticinferenceasamodelofplannedbehavior. KI,23(3),23–29.
Tschantz, A., Millidge, B., Seth, A. K. & Buckley, C. L. (2020a). Control as hybrid inference. arXiv preprint
arXiv:2007.05838.
Tschantz,A.,Millidge,B.,Seth,A.K.&Buckley,C.L. (2020b). Reinforcementlearningthroughactiveinference.
arXivpreprintarXiv:2002.12636.
Whittington,J.C.,McCaffary,D.,Bakermans,J.J.&Behrens,T.E. (2022). Howtobuildacognitivemap: insights
frommodelsofthehippocampalformation. arXivpreprintarXiv:2202.01682.
Whittington, J.C., Muller,T.H., Mark, S.,Chen, G., Barry,C., Burgess,N.&Behrens, T.E. (2020). Thetolman-
eichenbaummachine: unifyingspaceandrelationalmemorythroughgeneralizationinthehippocampalforma-
tion. Cell,183(5),1249–1263.
11

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Successor Representation Active Inference"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
