=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Exploring Action-Centric Representations Through the Lens of Rate-Distortion Theory
Citation Key: varona2024exploring
Authors: Miguel de Llanza Varona, Christopher L. Buckley, Beren Millidge

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2024

Key Terms: lens, representations, action, rate, sussex, centric, theory, efficient, exploring, data

=== FULL PAPER TEXT ===

Exploring Action-Centric Representations
Through the Lens of Rate-Distortion Theory
Miguel De Llanza Varona1,2, Christopher Buckley1,2, Beren Millidge3
1 School of Engineering and Informatics, University of Sussex, Brighton, UK
2 VERSES Research Lab, Los Angeles, California, USA
M.De-Llanza-Varona@sussex.ac.uk, C.L.Buckley@sussex.ac.uk
3 MRC Brain Networks Dynamics Unit, University of Oxford, Oxford, UK
beren@millidge.name
Abstract. Organisms have to keep track of the information in the en-
vironment that is relevant for adaptive behaviour. Transmitting infor-
mation in an economical and efficient way becomes crucial for limited-
resourced agents living in high-dimensional environments. The efficient
coding hypothesis claims that organisms seek to maximize the informa-
tion about the sensory input in an efficient manner. Under Bayesian
inference, this means that the role of the brain is to efficiently allocate
resourcesinordertomakepredictionsaboutthehiddenstatesthatcause
sensorydata.However,neitherofthoseframeworksaccountsforhowthat
information is exploited downstream, leaving aside the action-oriented
role of the perceptual system. Rate-distortion theory, which defines op-
timallossycompressionunderconstraints,hasgainedattentionasafor-
malframeworktoexploregoal-orientedefficientcoding.Inthiswork,we
explore action-centric representations in the context of rate-distortion
theory. We also provide a mathematical definition of abstractions and
wearguethat,asasummaryoftherelevantdetails,theycanbeusedto
fixthecontentofaction-centricrepresentations.Wemodelaction-centric
representations usingVAEsandwefindthatsuchrepresentations i)are
efficient lossy compressions of the data; ii) capture the task-dependent
invariances necessary to achieve successful behaviour; and iii) are not
in service of reconstructing the data. Thus, we conclude that full re-
construction of the data is rarely needed to achieve optimal behaviour,
consistent with a teleological approach to perception.
Keywords: Rate-distortiontheory·Action-centricrepresentations·Ef-
ficient coding · Bayesian Inference
1 Introduction
Embodied agents have to focus on the relevant information from their environ-
ment to achieve adaptive behaviour. Their resource-limited cognition and the
high-complexity structure inherent to the environment force them to economize
the transmission of information. Thus, the goal of the perceptual system is to
generate representations that are useful for successful behaviour while at the
same being encoded in the most efficient manner.
4202
peS
31
]IA.sc[
1v29880.9042:viXra
2 Miguel De Llanza Varona, Christopher Buckley, Beren Millidge
A well-known hypothesis in theoretical neuroscience called the efficient cod-
inghypothesisproposesthattheneuralcodinginthebrainisoptimizedtomax-
imize sensory information under metabolic and capacity constraints [3,13,23].
In particular, this hypothesis suggests that neurons are tuned to the statistical
propertiesoftheenvironment,whichallowsthemtoefficientlyallocatesignaling
resourcestogeneratecompressedlow-dimensionalrepresentationsoftheenviron-
ment. In this theoretical framework, it is commonly assumed that the function
of neurons is to maximize their capacity to account for all the variability in
the sensory input. In information theory terms, this means that the brain seeks
to maximize the mutual information between stimuli and neurons’ response to
reduce as much as possible the uncertainty about the environment, which is
defined by its entropy. While this hypothesis answers the question about in-
formation processing under biological constraints, it leaves aside the utilitarian
aspect of perception [11,14,15,18,20].
Cognition can’t be fully understood without its ecological context, as agents
are coupled with their environment forming a perception-action feedback loop
[22].Inthissense,thefunctionalroleofperceptualprocessinghastobeinservice
of achieving behavioural objectives, and to do that, perceptual representations
must efficiently encode the relevant information needed by the motor system to
guidefutureactions.Thus,akeycomponentoftheperceptualsystemistosum-
marize relevant sensory information to generate action-centric representations.
The teleological essence of the perceptual system imposes a normativity on
representations:aperceptualrepresentationisaccurateifitcapturestherelevant
information needed downstream and discards the irrelevant details. Thus, we
need an extra ingredient to account for the goodness of representations under
constraints. This is where the rate-distortion theory comes into play [19]. This
subfield of information theory defines the optimal trade-off between channel
capacity and expected communication error. When error-free communication is
not necessary to guide behaviour, the optimal encoding is a lossy compression
of the input.
Interestingly,rate-distortiontheorycanbeseenasawaytoperformBayesian
inference under constraints. Under a Bayesian approach to cognition, the brain
performs inference to compute an optimal posterior distribution over hidden
environmental states given sensory data [8]. As computing the true posterior
is usually intractable, the brain approximates the true posterior by optimizing
the variational free energy [4,9,10]. The main conceptual contribution of rate-
distortion theory is to define the “goodness” of that approximation, as comput-
ing the true posterior is not always necessary to act optimally. In the context of
active inference, it has been shown that action-oriented models learn parsimo-
nious representations of the environment by capturing relevant information for
behaviour[21].Inthesamespirit,weinvestigatetheinformation-theoreticprop-
ertiesofaction-centricrepresentationsandtheirrelationtotheformaldefinition
of abstractions we propose.
In this work, we explore action-centric representations under the lens of
rate-distortion theory to account for the teleological aspect of perception. To
Action-Centric Representations and Rate-Distortion Theory 3
do that, we provide a mathematical definition of abstraction that allows us to
specify the task-relevant information that should carry an action-centric rep-
resentation. Given the tight connection between Bayesian Inference and rate-
distortion theory, we use a Variational Autoencoder (VAE) framework to model
action-centric representations as optimal lossy compression. Our results show
that action-centric representations are optimal lossy compressions of the data;
can be successfully used in downstream tasks; and crucially, they achieve that
without being in service of reconstructing the data.
2 Efficient coding and rate-distortion theory
2.1 Efficient coding
The efficient coding hypothesis states that neurons are optimized to maximize
the information they carry about sensory states. In doing so, neurons have to
generate minimal redundancy codes to economically use limited resources. In
particular, neurons seek to maximize the ratio between information about sen-
sory inputs, defined by the mutual information I(X;Z) between sensory data
X and neural responses Z, and the channel capacity C: I(X;Z). The maximum
C
mutual information is upper bounded by the channel capacity
C ≥I(X;Z) (1)
so the best efficient coding satisfies
I(X;Z)=C (2)
where neuronal encoding exploits the whole bandwidth of the channel.
2.2 Rate-distortion theory as goal-oriented efficient coding
Under the classical conception of efficient coding, the exploitation of informa-
tiondownstreamisignored.Whennotallsensoryinformationisneededtoguide
behaviour, error-free communication is not expected. This is precisely what is
addressed by the rate-distortion theory, which provides the theoretical founda-
tions for optimal lossy data compression. Formally, the rate-distortion function
defines an optimal lossy compression Z of some data X as the minimization of
their mutual information I(X;Z) given some expected distortion D associated
with reconstructing X from its lossy compression Z. It is defined as [6]
R(D)= min I(X;Z) (3)
q(z|x):Dq(x,z)≤D
where q is the optimal distribution of z given x that satisfies the expected dis-
tortion constraint and the rate R is an upper bound on the mutual information:
R≥I(X;Z) (4)
4 Miguel De Llanza Varona, Christopher Buckley, Beren Millidge
TheexpecteddistortionDisdefinedbysomearbitrarylossfunction(e.g.,mean-
squared error) that quantifies the faithfulness of information transmission (i.e.,
how well can the data be recovered from its optimal lossy compression). Lossy
compression sacrifices the capacity to represent all the information in the input
in service of transmitting information that allows adaptive behaviour. Having
a faithfulness criterion allows the brain to efficiently represent the world by
allocating just the necessary amount of resources required to navigate the en-
vironment (Figure 1). Thus, rate-distortion adds a teleological perspective to
efficient coding that shifts the focus from efficient information maximization to
efficient transmission of action-oriented information.
Inthelossyregimeoftherate-distortion(i.e.,allpointssuchthatD >0),the
obtained representations can be understood as abstractions of the data, as their
functionistosummarizetherelevantpropertiesofthedataneededdownstream.
In the next section, we provide a mathematical definition of abstractions based
ontheintuitionthatareentitiesthatconveythenecessaryinformationtoanswer
a set of queries about the data. The mathematical formulation of abstractions
is crucial to determine the content of action-centric representations.
Fig.1: Rate-distortion function for a discrete random variable with four uni-
formly distributed states. Assuming that behavioral objectives are achievable
even when half of the information generated at the source is missing; that is,
when the expected distortion D does not exceed 0.5 (x-axis), an optimal agent
with bounded rationality can rely on a lossy compression scheme and transmit
information at a rate R of 0.20 bits (y-axis).
3 Abstractions and action-centric representations
3.1 Mathematical formalization of abstractions
Anabstractionisthereductionofcomplexitybydiscardingcertainfeatureswhile
preserving others. As a relational concept, an abstraction involves two compo-
nents:itsobject(whatisbeingabstracted)anditscontent(whattheabstraction
Action-Centric Representations and Rate-Distortion Theory 5
is about). The content of the abstraction is a summary of the relevant proper-
ties of its object and the relevancy is fixed, ultimately, by the agent’s needs.
Thus,abstractionsareintrinsicallyteleological entities;thatis,theirmeaningor
content is fixed by their function or purpose, which is to transmit information
about the properties of interest for an agent.
Following [16], we address the content of abstractions as the information
necessary to answer a set of queries about the data. A query captures what the
agent wants to know about the data (i.e., what is relevant). Formally, given a
setofqueriesaboutthedataQ={Q ,Q ...Q },eachqueryisamappingfrom
1 2 3
the data distribution to a probability distribution over a subset of elements of
the data Q : X → p(q|x). A good abstraction is one that fulfills its purpose;
namely, one that keeps track of those properties that make it possible to answer
a particular query. Thus, the ‘goodness’ of an abstraction z for a given query
can be defined as:
L (x,z)=D[Q(x)||Q(r(z))] (5)
Q
where Q(x) is the query distribution over the true system or data, Q(r(z)) is
the query distribution over a lossy reconstruction of the data r(z) produced by
the abstraction z, and D is an arbitrary divergence function. Without loss of
generality, the ‘goodness’ of an abstraction given a set of queries can be defined
as the weighted loss over all the queries given the abstraction:
(cid:88)
L(x,z)= p(Q )L (x,z) (6)
i Qi
Qi∈Q
Ideally, the mutual information between the abstractions and the query
should be the same as the information transmitted between the data and the
query:
I(X;Q)=I(Q;Z) (7)
The intuition is that a good abstraction Z of the data should reduce the uncer-
tainty of the data X in the same way as the query Q does.
3.2 Abstractions as sufficient and non-superfluous representations
Following [7], an abstract representation Z that captures the relevant details
of the data X to answer a query Q should be sufficient (I(X;Q|Z) = 0) and
non-superfluous (I(X;Z|Q)=0):
I(X;Z|Q)=I(X;Z)−I(X;Q;Z) (8)
(cid:124) (cid:123)(cid:122) (cid:125)
Superfluous
=I(X;Z)−I(X;Q)+I(X;Q|Z) (9)
(cid:124) (cid:123)(cid:122) (cid:125)
Sufficient
=I(X;Z)−I(X;Q) (10)
6 Miguel De Llanza Varona, Christopher Buckley, Beren Millidge
therefore
(cid:0) (cid:1) (cid:0) (cid:1)
I(X;Z|Q)=0 ∧ I(X;Q|Z)=0 ⇐⇒ I(X;Z)=I(X;Q) (11)
From an information theory perspective, a good abstraction Z only carries the
relevant information in the data; that is, the information necessary to answer a
query.Notethatthisisacontinuumwhereatoneextremetheoptimalcompres-
sion captures all the information in the data when the query contains the same
information as the data H(Q)=H(X) (an ideal scenario in the efficient coding
hypothesis, where the goal is to maximize mutual information):
I(X;Q)=H(X)−H(X|Q)=H(X)−H(X|X)=H(X) (12)
therefore
I(X;Z)=I(X;Q) (13)
I(X;Z)=H(X) (14)
whichcorrespondstothelosslesscompressionregimeoftherate-distortionfunc-
tion.Onthecontrary,whenthecommunicationchannelisclosed,thenwerecover
the other extreme of the rate-distortion curve, where the mutual information is
zero. This is the case when knowing the query does not reduce the uncertainty
of the data:
I(X;Q)=H(X)−H(X|Q)=H(X)−H(X)=0 (15)
therefore
I(X;Z)=I(X;Q) (16)
I(X;Z)=0 (17)
Anyotherstageinbetweenisacasewherethequerycarriespartialinformation
about the data. Importantly, these information-theoretic entities are implicitly
optimizedinrate-distortiontheory.Ontheonehand,sufficientinformationisre-
latedtopredictabilityand,therefore,tocommunicationfidelity,whichissatisfied
when the expected distortion allows for answering the query (i.e., successful be-
haviour). On the other hand, non-superfluous information is related to the min-
imization of the mutual information up to a point in which only query-relevant
information is encoded in the abstraction. Thus, optimal lossy representations,
whosefunctionistoencodetherelevantinvariancesandsymmetriesinthedata,
lie in the rate-distortion curve.
4 Variational Free Energy and Rate-distortion theory
Ascomputingtherate-distortionfunctionisintractableinhigh-dimensionalsys-
tems [6], variational inference can be used as a proxy of the amount of informa-
tion transmitted through a communication channel. In variational inference, a
Action-Centric Representations and Rate-Distortion Theory 7
quantity called variational free energy sets an upper bound on the sensory sur-
prisal (i.e., the entropy of sensory states), and by minimizing it is reduced the
uncertainty about the sensory data allowing for predictability of future states
and adaptive behaviour. One common variational free energy decomposition is
the ELBO, which involves two terms, accuracy and complexity, and is formally
defined as [17]
(cid:90) q(z|x)
F = q(z|x)ln (18)
p(x,z)
(cid:90) q(z|x) (cid:90)
F = q(z|x)ln − q(z|x)lnp(x|z) (19)
p(z)
F =D [q(z|x)||p(z)]− E [lnp(x|z)] (20)
KL
(cid:124) (cid:123)(cid:122) (cid:125) z∼q
Complexity (cid:124) (cid:123)(cid:122) (cid:125)
Accuracy
Tomodellossyrepresentationsofthedata,weuseVAEsduetoitscloserela-
tiontovariationalinferenceandrate-distortiontheory.VAEsisanunsupervised
learning framework that captures the underlying data distribution by using i)
an encoder that learns a latent representation of the data; and ii) a decoder
that generates data-like samples from the latent representation. The objective
function optimized by VAEs is the ELBO, where the complexity term can be
seen as a regularizer applied to the latent space, and the accuracy term as the
faithfulness of the decoder’s reconstruction.
As has been recently shown [1,12], the ELBO is implicitly optimizing the
rate-distortion function. On the one hand, the expected complexity is an upper
bound on the mutual information I(X;Z) (see Appendix C for full derivation):
(cid:2) (cid:3)
E D [q(z|x)||p(z)] ≥I(X;Z) (21)
KL
p(x)
just as the rate R is in rate-distortion theory. On the other hand, the expected
distortioncanbemeasuredusinganylossfunctionthatcaptureshowfaithfulthe
reconstructionofthedecoderresemblestheinputdata(e.g.,hammingdistance).
Inthiscase,thenegativelog-likelihoodusedinVAEscanbeusedasadistortion
measure between the input and its reconstruction, so D can be defined as:
(cid:2) (cid:3)
D =− E logp (x|z) (22)
θ
z∼qϕ(z|x)
Thus,variationalinferencecanbeunderstoodthroughthelensofrate-distortion
is characterized as
(cid:2) (cid:3) (cid:2) (cid:3)
F =− E logp (x|z) +D q (z|x)||p(z) (23)
θ KL ϕ
z∼qϕ(z|x) (cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125) Rate
Distortion
8 Miguel De Llanza Varona, Christopher Buckley, Beren Millidge
5 Methods
5.1 Model
Inspired by the utilitarian perspective on the efficient coding hypothesis and
the mathematical foundations of abstractions, we present a modified VAEs to
model action-centric representations (Figure 2). The main novelty of the VAEs
presented here lies in the accuracy term of the free energy (Eq. (20)). Contrary
to vanilla VAEs, where the goal is to learn latent representations of the data to
reconstruct it as faithfully as possible, here we are interested in learning action-
centric representations that convey sufficient and non-superfluous information
about a query. In this model, full reconstruction of the data is not expected.
The final form of the objective function for our action-centric VAEs is:
(cid:2) (cid:3)
F =−D[Q(x)||Q(r(z))]+βD q (z|x)||p(z) (24)
KL ϕ
where β is the gradient of the rate with respect to the distortion ∂R = β and
∂D
here it’s used to target specific regimes of the rate-distortion plane [5]. The
accuracyismodifiedtoaccountforthegoodnessoftheabstraction.Thetraining
f
(a)Algorithmicdetailsoftheaction-centric (b) Architecture of the action-
VAEs centric VAEs
Fig.2: Action-centric VAEs. Left: algorithmic level of the action-centric VAEs.
Right: a schematic overview of the architecture. The novel component is in the
accuracy term of the ELBO. Instead of measuring the faithfulness of the recon-
struction,itismeasuredhowgoodisthereconstructionforaspecifictask,which
in this case is an image classifier.
pipeline is as follows. First, we define a query to be the discrimination of the
tenclassesoftheFASHION-MNISTdataset.Wefirsttrainedaclassifier,usinga
CNN,onthetaskspecifiedbythequery(i.e.,multiclassclassification).Oncethe
discriminatoristrainedwetrainedboththevanillaVAEsandouraction-centric
VAEs.Importantly,bothVAEshavethesamechannelcapacity,astheysharethe
Action-Centric Representations and Rate-Distortion Theory 9
same architecture, so the maximum achievable rate in both models is the same.
ThecrucialdifferenceisthatourVAEsisnottrainedtofullyreconstructthedata
but to generate reconstructions that can be well-classified by the discriminator.
By doing this divergence measure, we can evaluate the goodness of the abstract
representations for the given query. To compute the rate-distortion function, we
trained several VAEs using different β to study the rate-distortion trade-off in
different regimes and the potential differences between vanilla VAEs and our
model.
Using this model we can investigate whether the latent space can efficiently
encodejusttherelevantinvariancesandsymmetriesrequiredforthedownstream
task without the need to generate faithful reconstructions of the data. If that is
the case, full reconstruction no longer becomes a necessary condition for goal-
oriented representations. In the next section, we present the main results and
their connection to the theoretical framework presented previously.
5.2 Results
(a)Accuracyasafunction (b) Fully reconstruction (c) Action-centric objec-
of the number of steps. objective. tive.
Fig.3:Rate-distortioncurvemadeupofdifferentVAEs.Notethatasadistortion
measure here we use the accuracy. Left figure: vanilla VAEs that try to maxi-
mize the mutual information given the channel constraints. Right figure: lossy
compression VAEs whose function is to maximize utility downstream given the
channel constraints.
The results regarding the transmission of information in the two different
VAEs are shown in Figure 3. In (Figure 3a) it can be seen how action-oriented
VAEs converges faster to an encoding-decoding scheme that is useful for the
downstream task (measured by the accuracy), compared to the VAEs. This in-
dicates that action-centric representations might require less exposure to data,
whichmakesthemmoreefficientintermsofexploitingtheavailableinformation.
Figure 3b and Figure 3c show the rate-distortion curve for both types of
VAEs. It is clear how action-oriented representations require significantly less
information from the data to achieve better results in the downstream task. In
10 Miguel De Llanza Varona, Christopher Buckley, Beren Millidge
particular,itcanbeseenthattransmittingatarateofaround10bitstheaction-
centric VAEs reaches almost 85% of accuracy, compared to the 67% achieved
by the vanilla VAEs at approximately the same rate. This suggests that lossy
compression leads to efficient codings and, importantly, to better behaviour.
The results so far indicate that the main function of representations might
not be to fully reconstruct the data, but not capture the relevant invariances in
thedataexploitedbyoptimalbehaviour.Weexplicitlyshowthisbyinvestigating
the reconstructions obtained by action-oriented representations. Figure 4 shows
asampleofthereconstructionsobtainedbythevanillaandaction-centricVAEs,
respectively.WhilethevanillaVAEsgeneratesrelativelyfaithfulreconstructions
of the data, the action-centric VAEs generates meaningless and uninterpretable
images. Interestingly, these action-oriented reconstructions are classified with
approximately 85% of accuracy, which suggests that the underlying structure of
these reconstructions is preserving some important invariances and symmetries
ofthedata.Onthecontrary,thefullreconstructionmightcarryirrelevantinfor-
mationthatisnon-taskspecific,whichcouldexplainwhytheyaremoredifficult
to classify.
Fig.4: Data reconstruction by VAEs and action-centric VAEs.
6 Discussion
Agents need to navigate complex environments with limited biological informa-
tion processing. Under this circumstance, an optimal perceptual system has to
efficiently allocate cognitive resources to transmit the relevant sensory informa-
tion to achieve successful behaviour. Thus, the goal of perception is not to gen-
erate faithful reconstructions of the sensory input, but abstract representations
that are useful downstream.
A common approach to representations in Artificial Intelligence and Neuro-
scienceisthattheyshouldbeinserviceoffullyreconstructingthedata.However,
suchrepresentationswillcarryirrelevantinformationfordownstreamtasksthat
only depend on the exploitation of specific invariances and symmetries of the
data.
Action-Centric Representations and Rate-Distortion Theory 11
In this work, we explore useful efficient coding within the framework of rate-
distortion to explore optimal information processing for task-dependent con-
texts. We have provided a formal definition of abstractions that can be used
to learn action-centric representations whose main function is to capture the
task-dependant invariances in the data. Such lossy compressions of the data lie
near optimal points of the rate-distortion curve. Crucially, we show that action-
centric representations i) are efficient lossy compressions of the data; ii) capture
thetask-dependentinvariancesnecessarytoachieveadaptivebehaviour;andiii)
are not in service of reconstructing the data. This could shed some light on how
organisms are not optimized to reconstruct their environment; instead, their
representational system is tuned to convey action-relevant information.
Interestingly,ourworkresonateswithrecentresearchonmultimodallearning
such as the joint embedding predictive architecture and multiview systems [2,
7]. The main objective of these models is to obtain representations that are
useful downstream but from which it’s not possible to reconstruct the data.
These representations learn the relevant invariances by maximizing only the
informationsharedacrossdifferentviewsormodalitiesofthedata.Wearguethat
action-centric representations operate in a similar way, as shared information
across views is an implicit way to define a query (see Appendix D).
An interesting line of research is to explore faithful reconstruction in the
context offine-grained queriessuch aspixel predictability.Wehypothesize that,
as the number of pixel-specific queries approaches the pixel space of the image,
the abstract representation might allow for faithful reconstruction of the data.
Although that could be to the detriment of worse performance on downstream
tasks.
Inconclusion,thisworksetsapromisinglineofresearchinthefieldofrepre-
sentational theory by understanding representations not as faithful reconstruc-
tions of the data but as action-driven entities.
References
1. Alemi, A., Poole, B., Fischer, I., Dillon, J., Saurous, R.A., Murphy, K.: Fixing a
broken elbo. In International conference on machine learning pp. 159–168 (2018)
2. Bardes, A., Ponce, J., LeCun, Y.: Vicreg: Variance-invariance-covariance regular-
ization for self-supervised learning. arXiv preprint arXiv:2105.04906. (2021)
3. Barlow, H.B.: Possible principles underlying the transformation of sensory mes-
sages. Sensory communication 1(01), 217–233 (1961)
4. Buckley, C.L., Kim, C.S., McGregor, S., Seth, A.K.: The free energy principle for
action and perception: A mathematical review. Journal of Mathematical Psychol-
ogy 81, 55–79 (2017)
5. Burgess, C.P., Higgins, I., Pal, A., Matthey, L., Watters, N., Desjardins, G., Ler-
chner, A.: Understanding disentangling in β-vae. arXiv preprint arXiv:1804.03599
(2018)
6. Cover, T., Thomas, J.: Elements of Information Theory. New York: Wiley. (2006)
7. Federici,M.,Dutta,A.,Forr´e,P.,Kushman,N.,Akata,Z.:Learningrobustrepre-
sentationsviamulti-viewinformationbottleneck.arXivpreprintarXiv:2002.07017
(2020)
12 Miguel De Llanza Varona, Christopher Buckley, Beren Millidge
8. Friston, K.: The free-energy principle: a rough guide to the brain?.. Trends in
cognitive sciences 13(07), 293–301 (2009)
9. Friston,K.:Thefree-energyprinciple:aunifiedbraintheory?Naturereviewsneu-
roscience 11(2), 127–138 (2010)
10. Friston, K.: A free energy principle for biological systems. Entropy 14(11), 2100–
2121 (2012)
11. Genewein, T., Leibfried, F., Grau-Moya, J., Braun, D.A.: Bounded rationality,
abstraction,andhierarchicaldecision-making:Aninformation-theoreticoptimality
principle. Frontiers in Robotics and AI 2, 27 (2015)
12. Hoffman,M.D.,Johnson,M.J.:Elbosurgery:yetanotherwaytocarveupthevari-
ationalevidencelowerbound.InWorkshopinAdvancesinApproximateBayesian
Inference, NIPS 1(2) (2016)
13. Laughlin,S.:Asimplecodingprocedureenhancesaneuron’sinformationcapacity.
Zeitschrift fu¨r Naturforschung c 36(9-10), 910–912 (1981)
14. Lieder, F., Griffiths, T.L.: Resource rational analysis: Understanding human cog-
nitionastheoptimaluseoflimitedcomputationalresources.BehavioralandBrain
Sciences 47 (2020)
15. Manookin, M.B., Rieke, F.: Two sides of the same coin: Efficient and predictive
neural coding. Annual Review of Vision Science (9) (2023)
16. Millidge, B.: Towards a mathematical theory of abstraction. arXiv preprint
arXiv:2106.01826. (2021)
17. Millidge, B., Seth, A., Buckley, C.L.: Predictive coding: a theoretical and experi-
mental review. arXiv preprint arXiv:2107.12979. (2021)
18. Park, I.M., Pillow, J.W.: Bayesian efficient coding. BioRxiv 178418 (2017)
19. Shannon,C.E.:Codingtheoremsforadiscretesourcewithafidelitycriterion.IRE
Nat. Conv. 4(142-163) (1959)
20. Sims, C.R.: Rate–distortion theory and human perception. Cognition 152(46),
181–198 (2016)
21. Tschantz,A.,Seth,A.K.,Buckley,C.L.:Learningaction-orientedmodelsthrough
active inference. PLoS computational biology 16(4) (2020)
22. de Wit, M.M., de Vries, S., van der Kamp, J., Withagen, R.: Affordances and
neuroscience:Stepstowardsasuccessfulmarriage.NeuroscienceandBiobehavioral
Reviews 80, 622–629 (2017)
23. Zhou, D., et al.: Efficient coding in the economics of human brain connectomics.
Network Neuroscience 6(1), 234–274 (2022)
Action-Centric Representations and Rate-Distortion Theory 13
A Model details
The classifier used to implement the query is a deep convolutional network
(CNN)withthreeconvolutionallayers.Thenumberoffiltersforthefirstlayeris
16,anditisdoubledineachlayer.Thekernelsizeis3inalllayers,andpadding
is set to 1, also in all layers. Stride is 1 in the first two layers, and 2 in the third
one.Inaddition,batchnormalizationisappliedineachlayer;16forthefirstone,
and doubled in each layer. The activation function in each layer is ReLU, and
max pooling is applied in the first two layers, both with a kernel size of 2, and
stride of 2 in the first and 1 in the second. Between the first two fully connected
layersitisusedadropoutof0.2.Thenumberofneuronsforthefullyconnected
layers is 512, 128, and 10. We use the Adam optimizer with a learning rate of
0.001. We trained the classifier for 15 epochs with a batch size of 64.
RegardingtheVAEs,theencoderisaCNNof4layerswiththesameparam-
eters as the CNN. Every VAEs trained has 8 latent dimensions and are trained
for20epochsusingabatchsizeof64.InthecaseofthevanillaVAEs,theβ used
to draw the rate-distortion curve are 100, 40, 20, 10, 5, 1, 0.5, 0.1, and 0.01. For
the custom VAEs, the β values are 6e-2, 3e-2, 1e-2, 6e-3, 3e-3, 1e-3, 5e-4, 1e-4,
1e-5, 1e-6.
B Latent space of VAEs
PCAtoexploreandshowthelatentspaceofthevanillaandaction-centricVAEs
that achieve a good performance downstream:
(a) Vanilla VAEs (β =0.5) (b) Action-centric VAEs (β =1e−4)
Fig.5: Latent spaces of the two types of VAEs.
As can be seen, our VAEs achieves a compact meaningful encoding of the
data,withanapparentbetterseparabilityamongclassesthanthevanillaVAEs.
14 Miguel De Llanza Varona, Christopher Buckley, Beren Millidge
C ELBO and RDT
One way to derive the upper bound on mutual information from the complexity
term of the ELBO is:
(cid:2) (cid:3) (cid:2) (cid:90) q(z|x) (cid:3)
E D [q(z|x)||p(z)] = E q(z|x)ln dxdz (25)
KL p(z)
p(x) p(x)
(cid:2) (cid:90) q(z|x)q(z) (cid:3)
= E q(z|x)ln dxdz (26)
p(z)q(z)
p(x)
(cid:2) (cid:90) q(z|x) (cid:90) q(z) (cid:3)
= E q(z|x)ln dxdz+ q(z|x)ln dxdz
q(z) p(z)
p(x)
(27)
(cid:90) q(z|x) (cid:90) q(z)
= q(z|x)q(x)ln dxdz+ q(z|x)q(x)ln dxdz
q(z) p(z)
(28)
(cid:90) q(x,z) (cid:90) q(z)
= q(x,z)ln dxdz+ q(z)ln dz (29)
q(x)q(z) p(z)
=I(X;Z)+D [q(z)||p(z)] (30)
KL
≥I(X;Z) (31)
Another way to derive this upper bound is by splitting the expected com-
plexity into conditional entropy and entropy terms:
(cid:2) (cid:3) (cid:2) (cid:90) q(z|x) (cid:3)
E D [q(z|x)||p(z)] = E q(z|x)ln dxdz (32)
KL p(z)
p(x) p(x)
(cid:90) (cid:90)
= q(z|x)q(x)lnq(z|x)dxdz− q(z|x)q(x)lnp(z)dxdz
(33)
(cid:90) (cid:90)
= q(x,z)lnq(z|x)dxdz− q(z)lnp(z)dz (34)
Inthelastequation,wecanseethatthefirsttermisthenegativeconditional
entropy−H(Z|X)whichisoneofthetwotermsinwhichthemutualinformation
is decomposed: I(Z;X) = H(Z)−H(Z|X). To get the entropy H(Z) we need
to replace p(z) by an approximate distribution q(z). By Jensen’s inequality, we
know that D [q(z)||p(z)]≥0, therefore, we know that:
KL
(cid:90) (cid:90)
q(z)lnq(z)− q(z)lnp(z)≥0 (35)
(cid:90) (cid:90)
q(z)lnq(z)≥ q(z)lnp(z) (36)
Replacing that term in the previous expression (34) we get:
Action-Centric Representations and Rate-Distortion Theory 15
(cid:90) (cid:90) (cid:90) (cid:90)
q(x,z)lnq(z|x)dxdz− q(z)lnp(z)dz ≥ q(x,z)lnq(z|x)dxdz− q(z)lnq(z)dz
(37)
(cid:90) (cid:90) (cid:90) q(x,z)
q(x,z)lnq(z|x)dxdz− q(z,x)lnq(z)dxdz ≥ q(x,z)ln dxdz =I(X;Z)
q(x)q(z)
(38)
D Multiview architecures and queries
Givena query Q(X)overX inamultiview scenarioitcanbeunderstoodasthe
subset of information contained in the intersection of X and t(X) such that:
Q(X)∈X∩t(X) (39)
asthetransformationtonlypreservesthosesymmetriesrelevantforthequery
(i.e., relevant to solve a set of tasks that only depend on those invariances).
Therefore, the relevant query in a multiview scenario can be defined as:
Q(X)=p(X,t(X)) (40)
MutualinformationbetweenX andZ andbetweenQ(X)andZ is(assuming
that X, X’ and Z form a dag where Z only depends on X):
(cid:90) p(x,z)
I(X;Z)= p(x,z)ln dxdz (41)
p(x)p(z)
(cid:90) p(q,z)
I(Q(X);Z)= p(q,z)ln dqdz (42)
p(q)p(z)
(cid:90) p(x,x′,z)
= p(x,x′,z)ln dxdx′dz (43)
p(x,x′)p(z)
(cid:90) p(x′)p(x|x′)p(z|x)
= p(x,x′,z)ln dxdx′dz (44)
p(x′)p(x|x′)p(z)
(cid:90) p(z|x)
= p(x,x′,z)ln dxdx′dz (45)
p(z)
(cid:90) p(x,z)
= p(x,x′)p(z|x)ln dxdx′dz (46)
p(x)p(z)
(cid:90) p(x,z) p(x,z)
= p(x) ln dxdz (47)
p(x) p(x)p(z)
(cid:90) p(x,z)
= p(x,z)ln dxdz (48)
p(x)p(z)
=I(X;Z)=I(X;X′) (49)
16 Miguel De Llanza Varona, Christopher Buckley, Beren Millidge
ThemutualinformationbetweenthelatentZ andoneoftheviewsX isequal
to the mutual information between the query distribution Q(X) and the latent
Z. As the mutual information between an optimal lossy representation and its
corresponding view is equal to the mutual information between views, then, the
information conveyed by the query is the one shared by the views. This shows
that the multiview architecture is essentially a query-oriented system where the
transformations applied to the data keep specific invariances with respect to a
set of implicit queries of interest.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Exploring Action-Centric Representations Through the Lens of Rate-Distortion Theory"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
