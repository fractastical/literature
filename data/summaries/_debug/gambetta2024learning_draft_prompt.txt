=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Learning by Surprise: Surplexity for Mitigating Model Collapse in Generative AI
Citation Key: gambetta2024learning
Authors: Daniele Gambetta, Gizem Gezici, Fosca Giannotti

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2024

Key Terms: surprise, mitigating, scuolanormalesuperiore, collapse, surplexity, italy, learning, generative, universityofpisa, model

=== FULL PAPER TEXT ===

Learning by Surprise: Surplexity for Mitigating Model
Collapse in Generative AI
DANIELEGAMBETTA,UniversityofPisa,Italy
GIZEMGEZICI,ScuolaNormaleSuperiore,Italy
FOSCAGIANNOTTI,ScuolaNormaleSuperiore,Italy
DINOPEDRESCHI,UniversityofPisa,Italy
ALISTAIRKNOTT,VictoriaUniversity,NewZealand
LUCAPAPPALARDO,ISTI-CNR,ItalyandScuolaNormaleSuperiore,Italy
Assyntheticcontentincreasinglyinfiltratestheweb,generativeAImodelsmayberetrainedontheirown
outputs:aprocesstermed"autophagy".Thisleadstomodelcollapse:aprogressivelossofperformanceand
diversityacrossgenerations.Recentstudieshaveexaminedtheemergenceofmodelcollapseacrossvarious
generativeAImodelsanddatatypes,andhaveproposedmitigationstrategiesthatrelyonincorporating
human-authoredcontent.However,currentcharacterizationsofmodelcollapseremainlimited,andexisting
mitigationmethodsassumereliableknowledgeofwhethertrainingdataishuman-authoredorAI-generated.
Inthispaper,weaddressthesegapsbyintroducingnewmeasuresthatcharacterisecollapsedirectlyfroma
modelâ€™snext-tokenprobabilitydistributions,ratherthanfrompropertiesofAI-generatedtext.Usingthese
measures,weshowthatthedegreeofcollapsedependsonthecomplexityoftheinitialtrainingset,aswellas
ontheextentofautophagy.Ourexperimentspromptanewsuggestion:thatmodelcollapseoccurswhena
modeltrainsondatathatdoesnot"surprise"it.Weexpressthishypothesisintermsofthewell-knownFree
EnergyPrincipleincognitivescience.Buildingonthisinsight,weproposeapracticalmitigationstrategy:
filteringtrainingitemsbyhighsurplexity,maximisingthesurpriseofthemodel.Unlikeexistingmethods,
thisapproachdoesnotrequiredistinguishingbetweenhuman-andAI-generateddata.Experimentsacross
datasetsandmodelsdemonstratethatourstrategyisatleastaseffectiveashuman-databaselines,andeven
moreeffectiveinreducingdistributionalskewedness.Ourresultsprovidearicherunderstandingofmodel
collapseandpointtowardmoreresilientapproachesfortraininggenerativeAIsystemsinenvironments
increasinglysaturatedwithsyntheticdata.
CCSConcepts:â€¢Computingmethodologiesâ†’Naturallanguageprocessing;Machinelearningap-
proaches;Artificialintelligence;Machinelearningalgorithms.
AdditionalKeyWordsandPhrases:ArtificialIntelligence,Autophagy,ModelCollapse,Human-AICoevolution
ACMReferenceFormat:
DanieleGambetta,GizemGezici,FoscaGiannotti,DinoPedreschi,AlistairKnott,andLucaPappalardo.2018.
LearningbySurprise:SurplexityforMitigatingModelCollapseinGenerativeAI.J.ACM37,4,Article111
(August2018),25pages.https://doi.org/XXXXXXX.XXXXXXX
Authorsâ€™ContactInformation:DanieleGambetta,UniversityofPisa,Pisa,Italy,daniele.gambetta@phd.unipi.it;Gizem
Gezici,ScuolaNormaleSuperiore,Pisa,Italy,gizem.gezici@sns.it;FoscaGiannotti,ScuolaNormaleSuperiore,Pisa,Italy,
fosca.giannotti@sns.it;DinoPedreschi,UniversityofPisa,Pisa,Italy,dino.pedreschi@unipi.it;AlistairKnott,Victoria
University,Wellington,NewZealand,ali.knott@vuw.ac.nz;LucaPappalardo,ISTI-CNR,Pisa,ItalyandScuolaNormale
Superiore,Pisa,Italy,luca.pappalardo@isti.cnr.it.
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfee
providedthatcopiesarenotmadeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthe
fullcitationonthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthantheauthor(s)mustbehonored.
Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requires
priorspecificpermissionand/orafee.Requestpermissionsfrompermissions@acm.org.
Â©2018Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
ACM1557-735X/2018/8-ART111
https://doi.org/XXXXXXX.XXXXXXX
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.
5202
peS
2
]LC.sc[
3v14321.0142:viXra
111:2 Gambettaetal.
1 Introduction
GenerativeAIhasdemonstratedremarkableadvancementsinrecentyears,particularlyinconver-
sationalsystemssuchasChatGPT,Claude,GoogleGemini,andMetaAI[44].Theseapplications
havebeenwidelyadoptedandnowplayacentralroleincontentgenerationacrossmanyonline
platforms[19].
LargeGenerativeModels(LGMs)poweringtheseapplicationsaretrainedonvastamountsof
human-authoredcontent.Forexample,LargeLanguageModels(LLMs)aretypicallytrainedontext
corporacontaininguptotwotrilliontokens[42].Incomparison,theglobalpoolofhigh-quality
human-authoredtextisestimatedatjust17trilliontokens,withamodestannualgrowthrateof
4â€“5%[43].A2021reportpredictsthatby2025,90%ofinternetcontentwillbeAI-generated[13],
heraldingwhathasbeendescribedastheAgeofSyntheticRealities[7].AsLGMsâ€™trainingrelies
onweb-sourcedcontent,thereisarisingriskthattheirownoutputs(theAI-generatedcontent)
willbeusedtotrainfuturemodels,creatingapotentiallyself-consumingfeedbackloop[36].
Recentresearchhashighlightedtherisksassociatedwiththisself-consumingfeedbackloop,
oftenreferredtoasautophagy,i.e.,aprocesswheregenerativeAImodelsarerecursivelyfine-tuned
ontheirownoutputs[35,36,45].Theautophagyprocessleadstoaphenomenonknownasmodel
collapse,characterizedbyasignificantlossofdiversityinAI-generatedcontent.Severalfactors
influencemodelcollapse,includingmodelsize,fine-tuningparameters,andthecompositionof
thetrainingset(see,e.g.,[1,20,39]).Ourunderstandingofmodelcollapseisaworkinprogress:
Schaefferetal.[37]notethattherearemanyalternativedefinitionsofmodelcollapseaswellas
manyopenquestions.Inthispaper,weaddressthreekeygapsinthecurrentunderstandingand
mitigationofmodelcollapse.
(1) Model collapse is standardly diagnosed through metrics computed on the AI-generated
content.Welackaprincipledwaytocharacterisemodelcollapseinrelationtothemodelitself
(i.e.,basedontheprobabilitiesitproduces),independentlyofitsoutputs.
(2) Whilefine-tuningonAI-generatedcontentisknowntoinducecollapse,welackaprincipled
accountofwhichpropertiesoftrainingdocumentsmakethemmorelikelytotriggerit.
(3) Existingstrategiestomitigatemodelcollapseassumethatwecandistinguishbetweenhuman-
authoredandAI-generatedcontent.Thisassumptionisincreasinglyunrealistic,especiallyas
AI-generatedcontentproliferatesonline.
Ourpaper,whichfocusesonLLMs,aimstoconnectthesethreeopenissues.Inresponseto(1),
weproposenovelmeasuresforcharacterisingmodelcollapsebasedonthemodelâ€™snext-token
probabilitydistributions,ratherthanthepropertiesofAI-generatedtextdocuments.Wedefinea
modelascollapsed whenitspredicteddistributionsareskewedâ€“thatis,whenthemodelassigns
disproportionateprobabilitymasstoasmallnumberoftokens.Thisskewednessisquantifiedwith
twocomplementarymeasures:theGinicoefficientofthenext-tokenprobabilitydistribution,and
anindicatorofwhetherthetop-tokenprobabilityapproachestheentiremass.
Usingthesenewmeasures,weaddressissue(2)byproposinganewdefinitionofthetypesof
documentsthatcontributetomodelcollapse.Wearguethatthekeyfactoristhemodelâ€™ssurprise:
documentsthatfailtosurprisethemodelaremorelikelytoinducecollapse.Toformalisethis,we
introducesurplexity â€“ameasureofhowsurprisedamodelisbyagivendocumentâ€“asacore
criterionforidentifyingcollapse-inducingtrainingdata.
Wethenusesurplexitytoaddressissue(3):weproposetomitigatemodelcollapsebyselecting
trainingdocumentsbythelevelofsurprisetheyelicit.Ourresultsshow,forseveraldatasetsand
LLMs,thatthesurplexity-basedstrategyisaseffectiveinmitigatingmodelcollapseasapproaches
thatrelyonhuman-authoredcontentâ€“andevenmoreeffectiveatreducingdistributionalskewâ€“
whilerequiringnoknowledgeofwhethertrainingdocumentsarehuman-authoredorAI-generated.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.
LearningbySurprise:SurplexityforMitigatingModelCollapseinGenerativeAI 111:3
Ourstudyadvancestheunderstandingofmodelcollapseandsupportsthedevelopmentofmore
robustgenerativeAIsystems.Thesurplexity-basedmitigationstrategyalignswiththeoriesof
surprise-drivenlearningfromcognitivescience[16],offeringapromisingconnectionbetweenthe
studyofmodelcollapseandresearchonhumanlearningmechanisms.
The structure of the paper is as follows. Section 2 reviews related work on autophagy and
modelcollapse.Section3formalizesthemodelcollapseproblemandintroducesthenotationused
throughoutthepaper.Section4presentsourautophagysimulationframeworkandimplementation
details.Section5introducesournewproposedmetricsformodelcollapse.Section6reportssome
initial experiments on model collapse using the new metrics, and a new observation: collapse,
whenmeasuredbyournewmetric,happensatdifferentratesfordifferenttrainingdatasets.This
observation prompts our new proposed mitigation strategy. We describe the new strategy in
Section 7, and present some further experiments that show its effectiveness. Finally, Section 8
concludesthepaper,discussinglimitationsandoutliningdirectionsforfutureresearch.
2 RelatedWork
Researchonautophagyandmodelcollapsereliesonsimulationsbecauselarge-scale,longitudinal
empiricalstudiesarehinderedbythedifficultyoftrackingthemanyevolvingversionsofgenerative
models deployed on platforms, especially as they are frequently fine-tuned and updated over
time[35,36].Thesesimulationsshowthatmodelcollapseoccursinbothtextandimagedomains,
affectingvariousgenerativemodelsincludingLGMs,VariationalAutoencoders(VAEs)andGaussian
MixtureModels(GMMs)[1,5,6,10,11,20,22,29,30,39].
In Shumailov et al. [39], model collapse is defined as a degenerative process in which data
generatedbyAImodelscontaminatesthetrainingsetsofsubsequentgenerations.Theauthors
illustratethisphenomenonacrossmultiplemodelclasses,includingLLMs,VAEs,andGMMs.Since
Shumailovetal.â€™sfoundationalwork,severalstudieshaveexploredvariantsofautophagousloops,
examininghowdifferentmixturesofhuman-authoredandAI-generateddataaffectmodelcollapse.
Alemohammadetal.[1]evaluatefullysyntheticloops,syntheticaugmentation,andfreshdata
loops,findingthathuman-authoreddatacandelaycollapsebutnotpreventit.MartÃ­nezetal.[29]
trainagenerativemodelona50/50mixofhuman-authoredandAI-generatedimages.Theyobserve
that over time, the modelâ€™s outputs become more similar to human-authored images but lose
diversity.Brieschetal.[6]comparedifferentdatacycles:afullysyntheticcycle(wheretrainingdata
isentirelyreplacedeachgeneration),whichrapidlyleadstocollapse;incrementalandbalanced
cycles,whichalsoreducediversitytovaryingdegrees;andanexpandingcycle,whichcontinuously
addsnewdataandsuccessfullymaintainsdiversityforupto50generations.Otherstudiesexamine
theinevitabilityofcollapsewhenmodelsaretrainedexclusivelyonAI-generateddataandestimate
theminimumproportionofhuman-authoreddatarequiredtoaddressit[4,38].
Guoetal.[20]introducemetricstomeasurelexical,syntactic,andsemanticdiversityasautophagy
progresses.Studyingthreeusecasesâ€“newssummarization,scientificabstractgeneration,and
storygenerationâ€“theyfindthatthedeclineinlinguisticdiversityismorepronouncedintasksthat
demandgreatercreativity.Theeffectofsimulationparametersonmodelcollapsehasalsobeen
investigated:HerelandMikolov[23]showthatincreasingthelearningrateacceleratestheonsetof
collapse;Sureshetal.[41]findthatthetimeittakesforamodeltoforgetawordislinearlyrelated
tothatwordâ€™sfrequencyintheoriginaltrainingcorpus.
Severalstudieshaveinvestigatedmodelcollapseinthetext-to-imagedomain.MartÃ­nezetal.[30]
usedenoisingdiffusionimplicitmodelsandshowthataugmentingtrainingdatawithAI-generated
imagesleadstoaprogressivedeclineinimagequalityacrossgenerations.Hatayaetal.[22]study
syntheticdatacontaminationinStableDiffusionmodelsandfindthatperformancedeterioratesas
theproportionofAI-generatedimagesincreases.Theyalsoproposeaself-supervisedmethodusing
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.
111:4 Gambettaetal.
amaskedautoencodertodetectAI-generatedimages.BohacekandFarid[5]retrainStableDiffusion
varyingproportionsofhuman-authoredandAI-generatedimages,observingthatcollapseemerges
butcanbereversedbyfine-tuningsolelyonhuman-authoredimages.Dohmatobetal.[10,11]
extendtheanalysistotextgenerationwithLlama2,showingthatwhilemixedtraininginitially
improvesperformance,iteventuallyleadstocollapse.
Anotherstrandofresearchexploresstrategiestomitigatemodelcollapse.Someworkspropose
specificaccumulation-basedautophagousloopstoslowcollapse[6,18].Othersfocusonverifying
AI-generateddatabeforereusingit:Fengetal.[14]startsfromtheconsiderationthatinsome
casesAI-generateddatacanbeadvantageousifusedinthefine-tuningofamodel.TheyuseLLMs
forspecifictasksâ€“eigenvaluespredictionfromamatrixandnewssummarizationâ€“usinglinear
verifierstomeasurethedistancebetweenAI-generateddataandgroundtruthinformation,finding
thatsuchverificationcanindeedhelppreventcollapse.Fuetal.[17]studyhowtheproportion
ofhuman-authoredversusAI-generateddataaffectsthestabilityofmodelcollapse,whileZhuet
al.[48]explorehowtosynthesizedatainwaysthatavoidcollapse,showingthatsimpletoken-level
editstohuman-authoredtext(creatingsemi-syntheticdata)canbeeffective.Draysonetal.[12]
proposeincorporatingAI-generatedtextdetectionmechanismsasapreventivestrategy.
Positionofourwork. Existingstudiesmitigatemodelcollapsebyincorporatinghuman-authored
contentintothefine-tuningcorpus,assumingtheycanbereliablydistinguishedfromAI-generated
content. Our approach takes a different direction, starting from a key question: given a set of
textdocuments,whatpropertiesmakethemmorelikelytotriggercollapseduringfine-tuning?
Thisquestionismotivatedbytheobservationthatevenrepeatedfine-tuningonthesamehuman-
authoreddatacanproducecollapse-likeeffects,similartothosecausedbyAI-generatedcontent.
Ourmitigationapproachisgroundedinanovelcharacterisationofmodelcollapsebasedonthe
next-tokenprobabilitydistribution,whichallowsustodescribecollapseintermsofthemodelâ€™s
surprise.Fromthisperspective,low-surprisedocuments,whetherhuman-authoredorAI-generated,
are the primary drivers of collapse. Building on this insight, we propose a mitigation strategy
thatfiltersforhigh-surprisedocumentsduringfine-tuning,offeringanewlensthroughwhichto
understandandmitigatemodelcollapse.
3 TheModelCollapseproblem
Here,weformallyintroducetwofundamentalconcepts:theautophagyprocessandthemodel
collapseproblem.Thesedefinitionsarenecessaryascurrentresearchoffersvarying,andsometimes
conflicting,definitionsofautophagyandmodelcollapse[37].
Autophagyprocess. Letğ‘€ beapre-trainedfoundationmodelandD adatasetofhuman-authored
0 0
documents.Theautophagyprocessisaniterativesequenceofgenerationandfine-tuningsteps.In
agenerationstep ğ‘—,themodelğ‘€ ğ‘—âˆ’1 generatesasetofdocumentsDğ‘—.Inthefine-tuningstep,ğ‘€ ğ‘—âˆ’1
isfine-tunedonDğ‘— toproducethenextmodelğ‘€ ğ‘—.Anautophagyprocessofğ‘‡ iterationsconsists
ofthesequence{ğ‘€ ğ‘—âˆ’1 ,Dğ‘—}ğ‘‡ ğ‘—=1 ,whereeachmodelistrainedonthedocumentsithasgenerated.
Modelcollapse. Giventhesequence{ğ‘€ ğ‘—âˆ’1 ,Dğ‘—}ğ‘‡ ğ‘—=1 producedbyanautophagyprocess,letP real
denotethetruedatadistribution,andletPğ‘— denotethedistributionimplicitlydefinedbymodel
ğ‘€ ğ‘— throughitsgenerations.LetX = {ğ‘¥ 1 ,...,ğ‘¥ ğ‘š}beasetofmeasurablequalitydimensions(e.g.,
linguisticentropy,semanticcoherence),andletÎ” ğ‘—(ğ‘¥)quantifythedeviationofPğ‘— fromP
real
along
aspectğ‘¥ âˆˆX.Wesaythatmodelcollapseoccursalongdimensionğ‘¥ âˆˆXwhenthedeviationÎ” ğ‘—(ğ‘¥)
increasesovermultipleiterationsoftheautophagyprocess:
Î” ğ‘—(ğ‘¥) > Î” ğ‘—âˆ’1 (ğ‘¥) formany ğ‘—.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.
LearningbySurprise:SurplexityforMitigatingModelCollapseinGenerativeAI 111:5
Inthelimit,collapsemaybecharacterizedbyunboundeddivergence:
lim Î” ğ‘—(ğ‘¥) =âˆ forsomeğ‘¥ âˆˆX,
ğ‘—â†’âˆ
indicatingthatthemodelisdriftingirreversiblyawayfromdesirablebehavior.ThefunctionÎ” ğ‘—(ğ‘¥)
maytakedifferentformsdependingonthemetricâ€“forinstance,dropinlinguisticentropyorrise
inrepetitionrate.
Forexample,supposethatatstep ğ‘— =0,themodelğ‘€ generatesdiverseandsemanticallyrich
0
text.Astheautophagyprocessprogresses,ğ‘€ ğ‘— isrepeatedlyfine-tunedonitsownoutputs,and
overtime,itbeginstoproduceincreasinglyrepetitiveandformulaicsentences(e.g.,frequently
repeatingphraseslike"Thesystemisdesignedto..."or"Thismethodallowsfor...").Thisdecline
inlexicaldiversitycanbemeasuredbyadecreaseinlinguisticentropyğ»,leadingtoagrowing
Î” ğ‘—(ğ»),aclearsignalofcollapsealongthelinguisticentropydimension.
Inexistingstudies[35,45],modelcollapseistypicallydiagnosedwhenthedeviationincreases
monotonicallyorpersistentlyacrossiterations,revealingthecumulativedegradationofthemodelâ€™s
generativecapacity.
Table1summarizesthenomenclatureusedinthispaper.
Table1. Summaryofthenotationusedinthispaper.
Symbol Description
ğ‘€ Originalpre-trainedfoundationmodel
0
ğ‘€ ğ‘— Foundationmodelatsimulationstep ğ‘—
D 0 = {ğ‘‘ 1 (real),...,ğ‘‘ ğ‘› (real)} Initialdatasetofğ‘›human-authoreddocuments
Dğ‘— = {ğ‘‘ 1 (gen),...,ğ‘‘ ğ‘› (gen)} DatasetofAI-generateddocumentsatstep ğ‘—
Tğ‘— Trainingsetusedtofine-tuneğ‘€ ğ‘—âˆ’1 intoğ‘€ ğ‘—
ğ‘ƒ = {ğ‘
1
,...,ğ‘ ğ‘›} FixedsetofpromptsobtainedbytruncatingdocumentsinD
0
ğ‘ 
ğ‘–
(ğ‘—) Continuationgeneratedbyğ‘€
ğ‘—
forpromptğ‘
ğ‘–
P Truedatadistribution(human-authoredcontent)
real
Pğ‘— Distributionimplicitlydefinedbymodelğ‘€ ğ‘— (AI-generatedcontent)
X Setofqualitydimensions(e.g.,entropy,semanticaccuracy)
Î” ğ‘—(ğ‘¥) DeviationofPğ‘— fromP
real
alongdimensionğ‘¥ âˆˆX
ğ‘‘ Adocument,i.e.,asequenceoftokens
ğ‘¤
ğ‘–
Theğ‘–-thtokeninadocument
ğ‘
ğ‘¤
Model-assignedprobabilityoftokenğ‘¤
ğ‘˜ Numberoftokensineachprompt(defaultğ‘˜ =64)
ğ¿ Maximumlength(intokens)ofgenerateddocuments
ğœŒ Rankingfunctionusedtoselectdocumentsfortraining
ğ»(ğ‘‘) Linguisticentropyofdocumentğ‘‘
ğº(ğ‘) Ginicoefficientofprobabilityvectorğ‘
ğ¶(ğ‘) Collapsedpredictionindicator
ğ´ CommonsensicalInferenceAccuracy
CI
ğ‘† ğ‘€ğ‘— (ğ‘‘) Surpriseofdocumentğ‘‘ givenmodelğ‘€ ğ‘— (surplexity)
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.
111:6 Gambettaetal.
4 AutophagySimulationFramework
Whilethedefinitionsofautophagyandmodelcollapseapplytoanyfoundationalmodel,wefocuson
LargeLanguageModels(LLMs),i.e.,foundationmodelsfocusedonunderstandingandgenerating
humanlanguage[34].
Oursimulationframework,inspiredbyShumailovetal.[39],beginswithapre-trainedfoundation
modelğ‘€ 0 andadatasetD 0 = {ğ‘‘ 1 (real),...,ğ‘‘ ğ‘› (real)}ofğ‘›human-authoredtextdocuments.Theframework
(schematizedinAlgorithm1)isaniterativeprocessinwhichğ‘€ generatescontinuationsfrom
0
fixedpromptsandfine-tunesitselfonsomecombinationofhuman-authoredandAI-generated
documents.Thegenerationandfine-tuningstepsarerepeatedseveraltimestomimictheautophagy
process.
Duringaninitializationphase(lines1-3ofAlgorithm1),wetruncateeachhuman-authored
documentğ‘‘
ğ‘–
(real) âˆˆ D
0
toğ‘˜ tokens.Thisyieldsasetofğ‘›promptsğ‘ƒ = {ğ‘
1
,...,ğ‘ ğ‘›}thatremainfixed
throughouttheentiresimulation.Fixingthepromptsetacrossiterationsallowsustoanalysemodel
collapsedynamicsinacontrolledsetting.
Duringthesimulationphase(lines4-9ofAlgorithm1),ateachsimulationstepğ‘— (forğ‘— =1,...,ğ‘‡),
weperformthefollowingtwooperationsinsequence:
(1) Generation.Foreachpromptğ‘ ğ‘– âˆˆğ‘ƒ,weusethemodelğ‘€ ğ‘—âˆ’1 togenerateacontinuationğ‘  ğ‘– (ğ‘—âˆ’1)
suchthatthetotallengthofthecombinedtextdoesnotexceedğ¿ =128tokens(i.e.,|ğ‘ ğ‘–| =ğ‘˜,
and |ğ‘  ğ‘– (ğ‘—âˆ’1)| â‰¤ 128âˆ’ğ‘˜). Here,ğ‘  ğ‘– (ğ‘—âˆ’1) denotes the continuation of promptğ‘ ğ‘– generated by
modelğ‘€ ğ‘—âˆ’1 .WethendefinetheAI-generateddocumentastheconcatenationoftheprompt
andthegeneratedcontinuation:ğ‘‘
ğ‘–
(gen) =ğ‘ ğ‘–||ğ‘ 
ğ‘–
ğ‘—âˆ’1,where||denotessequenceconcatenation.
ThesetofallsuchAI-generateddocumentsformsanewdatasetatsimulationstep ğ‘—,denoted
asDğ‘— = {ğ‘‘ 1 (gen),...,ğ‘‘ ğ‘› (gen)}.
(2) Fine-tuning.Themodelğ‘€ ğ‘—âˆ’1 isfine-tunedtoobtaintheupdatedmodelğ‘€ ğ‘—,usingadatasetTğ‘—
selectedforthispurpose.InthemostbasicsetupTğ‘— =Dğ‘—,i.e.,onlythedocumentsgenerated
byğ‘€ ğ‘—âˆ’1 areusedforfine-tuning.However,ourframeworkisflexibleandgeneral:Tğ‘— may
includeanycombinationofhuman-authoredandAI-generateddocumentsselectedaccording
toarankingfunctionğœŒ :D â†’R,whereD denotesthesetofdocuments(human-authored
orAI-generated)availableatsimulationstep ğ‘—.TherankingfunctionğœŒ mayreflectvarious
desirableproperties,suchaslinguisticdiversityoralignmentwithatargetdistribution.The
top-ğ‘§ rankeddocumentsaccordingtoğœŒ arethenselectedtoform Tğ‘—.Thisflexibledesign
allowsforexperimentationwithdifferentdocumentselectionstrategiesandtheirimpacton
modelcollapseacrosssimulationsteps.
Byrunningthesimulationforğ‘‡ stepsâ€“alternatingbetweengeneratingdocuments,fine-tuning
themodel,andrepeatingthegenerationâ€“weobtainasequenceofincreasinglyspecialisedmodels
ğ‘€ 1 ,ğ‘€ 2 ,...,ğ‘€ ğ‘‡ andacorrespondinglycorpusofAI-generateddatasetsD 1 ,D 2 ,...,Dğ‘‡.
Itisworthnotingthatoursimulationframework,aswellasthoseintroducedbyotherstudies
onmodelcollapse[20,35,39,45],mayintroducesomebiases.Byalwaysconditioningonrealdata
(prompts),themodelmayover-relyonthehuman-authoredcontext(thefirstğ‘˜ tokens),potentially
maskingissuesthatwouldariseinafullysyntheticgenerationloop.Furthermore,thissetupmay
underestimate exposure bias and model collapse effects that can occur when both inputs and
outputsareAI-generated.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.
LearningbySurprise:SurplexityforMitigatingModelCollapseinGenerativeAI 111:7
Algorithm1:AutophagySimulationFramework
Input :Modelğ‘€ 0 ;initialdatasetD 0 = {ğ‘‘ 1 (real),...,ğ‘‘ ğ‘› (real)};promptlengthğ‘˜;numberof
iterationsğ‘‡
Output:Sequenceofmodelsğ‘€ 1 ,...,ğ‘€ ğ‘‡ anddatasetsD 1 ,...,Dğ‘‡
/* Initialization phase */
1 foreachğ‘‘ ğ‘– (real) âˆˆ D 0 do
2
ğ‘
ğ‘–
â†truncate(ğ‘‘
ğ‘–
(real),ğ‘˜)
3 ğ‘ƒ â† {ğ‘ 1 ,...,ğ‘ ğ‘›}; // Fixed set of prompts
/* Simulation phase */
4 for ğ‘— â†1toğ‘‡ do
5 foreachğ‘ ğ‘– âˆˆğ‘ƒ do
6
ğ‘ 
ğ‘–
(ğ‘—âˆ’1) â†generate_continuation(ğ‘€
ğ‘—âˆ’1
,ğ‘
ğ‘–
,ğ¿âˆ’ğ‘˜);
7 ğ‘‘ ğ‘– (gen) â†ğ‘ ğ‘–âˆ¥ğ‘  ğ‘– (ğ‘—âˆ’1) ; // Concatenate prompt and continuation
8
Dğ‘— â† {ğ‘‘
1
(gen),...,ğ‘‘
ğ‘›
(gen)}
9 ğ‘€ ğ‘— â†fine_tune(ğ‘€ ğ‘—âˆ’1 ,Dğ‘—); // Fine-tuning
5 Measuringmodelcollapse:astandardmeasure,andtwonewones
A key challenge in studying model collapse is how to measure it as autophagy progresses. In
thispaper,weextendthesetofmeasuresusedinpriorwork[35]byintroducingnewmeasures
thatcapturepreviouslyoverlookedaspectsofcollapse.Specifically,weevaluatemodelcollapse
usinganexistingmeasure(linguisticentropy),anewmeasurecapturingthemodelâ€™scommonsense
reasoningâ€”commonsenseinferenceaccuracy,andtwonewmetricsbasedonnext-tokenprobability
distribution:theGinicoefficientandthecollapsedpredictionindicator.
Linguistic entropy. We adopt linguistic entropy as our primary metric to quantify model
collapsebecauseitoffersasimpleyetinformativemeasureofthediversityinthemodelâ€™soutput
distribution. A decline in linguistic entropy over simulation steps indicates that the model is
producing increasingly repetitive or predictable text, which is an essential symptom of model
collapse.Whileothermetricssuchasself-BLEU[2],distinct-n[27],orperplexity[46]couldalso
beused,entropycapturestheunderlyingtheoreticalmeasureofinformationcontent,makingit
asuitableandparsimoniouschoiceforouranalysis.Formally,givenatextdocumentğ‘‘,letğ‘Š(ğ‘‘)
denotethesetofuniquetokensinğ‘‘.Wedefinethenormalisedlinguisticentropyofğ‘‘ as:
ğ»(ğ‘‘) =âˆ’
(cid:205)
ğ‘¤âˆˆğ‘Š(ğ‘‘)
ğ‘ ğ‘¤log(ğ‘ ğ‘¤)
(1)
log|ğ‘Š(ğ‘‘)|
whereğ‘ ğ‘¤ istheempiricalprobabilityoftokenğ‘¤ inğ‘‘,computedasthefrequencyofğ‘¤ dividedby
thetotalnumberoftermsinğ‘‘.Thenormalisationfactorlog|ğ‘Š(ğ‘‘)|ensuresthatentropyvalues
arecomparableacrossdocumentswithdifferentvocabularysizes.
Foreachmodelğ‘€ ğ‘—,andforeachprompt,wemeasurelinguisticentropyandthenreportthe
meanvalueacrossprompts.
Next-token probability. To gain further insight into the lack of diversity, we analyse the
modelâ€™snext-tokenprobabilitydistribution,whichreflectsthemodelâ€™sconfidencewhenpredicting
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.
111:8 Gambettaetal.
thenexttokeninasequence.Highconcentrationofprobabilitymassonafewtokensindicates
lowerdiversityandpotentialmodelcollapse.
Givenamodelğ‘€ ğ‘—,werandomlysample1,000human-authoreddocumentsforeachdatasetfrom
thoseneverusedduringfine-tuning.Eachdocumentistruncatedtoitsfirst32tokenstocreate
a prompt. We then query ğ‘€ ğ‘— to predict the next token given each prompt and extract the full
probabilitydistributionoverthetop100mostlikelytokens,obtainingonedistributionforeach
prompt.Fromthesedistributions,wecomputetwocomplementarymetrics:
(1) Ginicoefficient.Thismeasurestheinequalityofthenext-tokenprobabilitydistribution.A
highGinicoefficientindicatesthatthemodelassignsmostoftheprobabilitymasstoasmall
numberoftokens,suggestingreduceddiversity.Givenaprobabilityvectorğ‘ = (ğ‘
1
,ğ‘
2
,...,ğ‘ ğ‘›),
theGinicoefficientisdefinedas:
ğº(ğ‘) =
(cid:205)ğ‘› ğ‘–=1 (cid:205)ğ‘› ğ‘—=1 |ğ‘ ğ‘– âˆ’ğ‘ ğ‘—|
,
2ğ‘›(cid:205)ğ‘›
ğ‘–=1
ğ‘
ğ‘–
whereğ‘› is the number of candidate tokens (here,ğ‘› = 100). The Gini coefficient ranges
between0and1,where0indicatesperfectequality(alltokensareequallyprobable),while
highervaluesindicateincreasingconcentrationofprobabilitiesonfewertokens.
(2) Collapsedpredictions.Thismeasurecapturesextremeoverconfidenceanddeterministic
behaviourinnext-tokenprediction.Givenaprobabilityvectorğ‘ = (ğ‘
1
,...,ğ‘ ğ‘›),wesaya
predictioniscollapsed ifthereisprobabilitythatexceedsthethresholdğœ =0.99.Thus,we
definethecollapseindicatorğ¶(ğ‘)as1ifthepredictioniscollapsed:
(cid:40)
ğ¶(ğ‘) = 1 ifâˆƒğ‘˜ :ğ‘ ğ‘˜ >ğœ,
0 otherwise.
Foreachmodelğ‘€ ğ‘—,andforeachprompt,wecomputetheGinicoefficientandthecollapsed-
predictionindicator.WethenreportthemeanGiniacrosspromptsandthepercentageofprompts
flaggedascollapsed(i.e.,theaverageoftheindicatortimes100).Thesetwomeasurestogether
offerafine-grainedviewofhowunequalthenext-tokenprobabilitydistributionbecomesover
simulationstepsor,putdifferently,howpredictabletheAI-generatedtextis.
CommonsensicalInferenceAccuracy. Acrucialquestioniswhethercollapsealsocompro-
misesthemodelâ€™sabilitytogeneratemeaningfulsentences.Toexplorethis,weusethecommonsense
naturallanguageinferencetask,whichtestswhetheramodelcanplausiblycompleteagivensen-
tence.WeadopttheHellaSwagdataset[47],whichcontains70,000sentenceprompts,eachpaired
withfourcandidateendings (ğ‘Â¯,ğ‘ ,ğ‘ ,ğ‘ ).Ofthese,onlyoneisacoherentandcommonsensical
1 2 3
continuationğ‘Â¯;theotherthreeareintentionallyimplausible.
Wecomputetheconditionalprobabilityofeachcandidatecontinuationgivenaprompt.The
model selects the option with the highest probability as its predicted continuation. For each
promptğ‘ âˆˆ {ğ‘
1
,...,ğ‘ ğ‘},letğ‘Ë†denotethemodelâ€™sselectedanswerandğ‘Â¯theground-truth(correct)
continuation.Wedefineabinaryscoringfunctionğ¼(ğ‘)asfollows:
(cid:40)
1 ifğ‘Ë†=ğ‘Â¯
ğ¼(ğ‘) =
0 otherwise.
Theoverallaccuracy,whichwecallCommonsenseInferenceAccuracy(ğ´ ),isthencomputed
CI
astheproportionofpromptsforwhichthemodelselectsthecorrectcontinuation:
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.
LearningbySurprise:SurplexityforMitigatingModelCollapseinGenerativeAI 111:9
ğ‘
1 âˆ‘ï¸
ğ´ = ğ¼(ğ‘),
CI ğ‘
ğ‘–=1
Thismeasureprovidesahigh-levelviewofthemodelâ€™scapacitytomakeplausibleandcontextually
appropriatedecisions,beyondmerestylisticdiversity.
6 Initialexperimentswithmodelcollapseâ€“andanewobservation
WefirstprovidesettingsforourexperimentsinSection6.1.InSection6.2,wereplicateexisting
collapsefindings,withthetraditionalmeasureoflinguisticentropy.InSection6.3,wepresentsome
resultsusingournewmeasures,whichleadtoanewobservation.
6.1 Experimentalsettings
TheexperimentalsettingsusedinourstudyaresummarizedinTable2.Unlessotherwisespecified,
wesetthepromptlengthtoğ‘˜ =64tokensandaskthemodeltogenerateanadditional64tokens,
resultingindocumentsoflengthğ¿ =128,withaneven50%splitbetweenhuman-authoredand
AI-generatedtokens.Toensurestatisticalrobustness,eachconfigurationisrunthreetimes,and
wereporttheaverageandstandarderrorofeachmetricacrosstheseruns.InAppendixA.1,we
comparevaryingtheproportionofsyntheticcontentbychangingğ‘˜ to32or96whilekeeping
ğ¿ =128fixed,thenusingtruncateddocuments64tokensfornexttokenprobabilityandintheend
comparingmitigationresultsfordifferentmodels.
FoundationModels. Weconductourexperimentsusingthreeopen-weightfoundationmodels:
Llama2-7B(referredtoasLlama2),Llama3-8B(Llama3),andMistral-7B(Mistral).WeselectLlama2
foritspublicavailability,strongperformanceonawiderangeofNLPtasks,andfavorabletrade-off
betweenmodelqualityandcomputationalefficiency.Itsmoderatesize(sevenbillionparameters)
allowsforfastfine-tuningwhilemaintainingrobustlanguagegenerationcapabilities.Tovalidate
therobustnessofourfindings,wereplicateourexperimentswiththemorerecentLlama3modelas
wellasthecompetitiveMistral.Allmodelsarefine-tunedusingtheUnslothlibrarywithdefault
hyperparameters.1ExperimentsarerunonanNVIDIAQuadroRTX6000GPU.
Datasets. WeadoptthetextdatasetsusedintheseminalstudybyGuoetal.[20],ensuring
consistencywithpriorwork.Thesedatasetsspandifferentdomainsandlinguisticstyles,offeringa
robusttestbedforevaluatingmodelcollapse:
â€¢ Wikitext(wiki)isalarge-scalecorpuscomprisingover100milliontokensextractedfrom
verifiedEnglishWikipediaarticles[32].FollowingGuoetal.[20],weusethemainbody
textofeacharticle,asprovidedbythedatasetonHuggingFace.2Thisdatasetisparticularly
suitedforstudyinglong-formfactualandencyclopediclanguage.
â€¢ XL-Sum (xls) contains 1.35 million annotated news articles from the BBC in multiple
languages[21].Eachentryincludesthetitle,articlebody,summary,andarticleURL.We
focusontheEnglish-languagesubsetandusethearticleâ€™sbodytext.
â€¢ SciAbs(sci)isderivedfromaBiBTeXbibliographydatabaseofpaperspublishedincom-
putationallinguisticsandNLPvenuessince1965[20].Thedatasetcomprisesover40,000
papers,utilizingthetextfromtheirassociatedabstracts.
1https://github.com/unslothai/unsloth
2https://huggingface.co/datasets/Salesforce/wikitext
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.
111:10 Gambettaetal.
Fine-tuning scenarios. We consider three main scenarios for fine-tuning the model ğ‘€ ğ‘—âˆ’1 ,
basedonthecompositionofthetrainingsetTğ‘—:
â€¢ AI:Tğ‘— =Dğ‘—,i.e.,itcontainsonlyAI-generateddocumentsproducedbyğ‘€ ğ‘—âˆ’1 ;
â€¢ human:Tğ‘— containsonlyhuman-authoreddocuments;
â€¢ mixed:Tğ‘— containsamixtureofhuman-authoredandAI-generateddocuments.
For all models, datasets, and fine-tuning scenarios, we simulate the autophagy process over
ğ‘‡ =10steps,followingstandardpracticeintheliterature[39].
Table2. Experimentalsettingsofourstudy
Component Setting
Foundationmodels Llama2â€”versionLlama2-7B
Llama3â€”versionLlama3-8B
Mistralâ€”versionMistral-7B
Datasets wikiâ€”Wikipediaarticlebodies(Wikitext)
xlsâ€”Englishnewsarticlebodies(XL-Sum)
sciâ€”AbstractsfromNLP/CLpapers(SciAbs)
combined-Threepreviousdatasetstogether
Fine-tuningscenarios AIâ€“OnlyAI-generateddocuments
humanâ€“Onlyhuman-authoreddocuments
mixedâ€“MixtureofAI-generatedandhuman-authoreddocuments
Promptlength ğ‘˜ =64(default);alsotestedforğ‘˜ =32andğ‘˜ =96
Fine-tuninglibrary Unsloth(defaulthyperparameters)
Hardware NVIDIAQuadroRTX6000GPU
6.2 Modelcollapseresultsusinglinguisticentropy
Our simulations with Llama2 under the AI fine-tuning scenario confirm the findings of prior
work [20, 35, 39]: the average linguistic entropy consistently declines across simulation steps,
signalingprogressivemodelcollapse.Thisdeclineisevidentbothwhenthemodelisfine-tuned
onthecombineddatasetâ€“wiki,xls,andsciâ€“witharelativedecreaseofapproximately7.5%
atsimulationstep10comparedtostep0,andwhenfine-tunedonindividualdatasets,wherethe
relativedecreaserangesfrom4.7%onscito8.7%onwiki(seeFigure1).Thecollapseinlinguistic
diversityclearlyresultsfromfine-tuningontheAI-generateddocuments,asnosuchdeclineis
observedwhenthemodelisfine-tunedonhuman-authoreddocuments(seeemptygreendotsin
Figure1).WefindsimilarresultsforLlama3andMistral(seeFigure12inAppendixA.4).
6.3 Collapseresultsusingournewmeasures
Modelcollapseisalsoevidentwhenanalyzingtheevolutionofnext-tokenprobabilitydistributions.
Figure2a-cillustratesanexamplewiththetop-fivepredictedtokensandtheirassociatedprobabili-
tiesatsimulationsteps0,5,and10,usingLlama2.Theexamplereferstotextgeneratedinresponse
tothefollowingpromptextractedfromthescidataset:
TheobstetricElectronicMedicalRecord(EMR)containsalargeamountofmedical
dataandhealthinformation.Itplaysavitalroleinimprovingthequalityofthe
diagnosisassistantservice.Inthis...
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.
LearningbySurprise:SurplexityforMitigatingModelCollapseinGenerativeAI 111:11
1.02
1.00
0.98
0.96
0.94
0.92
0 1 2 3 4 5 6 7 8 9 10
Simulation Steps
)d(H
yportnE
citsiugniL
.mroN
.gvA
human
AI: combined
AI: wiki
AI: xls
AI: sci
Fig.1. Effectoftheautophagyprocessonlinguisticentropy.Normalizedlinguisticentropyğ»(ğ‘‘)of
documentsgeneratedbyLlama2(ğ‘˜ =64)over10simulationsteps.Eachcurverepresentsadifferentfine-
tuningscenario:onhuman-authoreddocuments(human,greenopencircles),onAI-generateddocumentsfrom
individualdatasetsâ€“AI: wiki(squares),AI: xls(diamons),andAI: sci(triangles)â€“andonacombination
ofallthree(AI: combined,redfilledcircles).Theresultsconfirmpriorfindings:linguisticentropysteadily
declinesastheautophagyprocessprogresses.
InFigure2a-c,weobservethatthedistributionbecomesincreasinglyskewedinsimulationsteps
5and10.Instep0,thetokenâ€œpaperâ€hasthehighestprobability,butisstillcloselyfollowedbyother
plausiblecompletions.Bystep10,â€œpaperâ€overwhelminglydominatesthedistribution,highlighting
amarkedlossinlinguisticentropyandgrowingoverconfidenceinasingleprediction.
Tomovebeyondasingleexample,weextendthisanalysistoabroadersetof1,000prompts
sampledfromeachdataset.Figure2dâ€“fillustrateshowLlama2assignsprobabilitiestothehighest-
probabilitytokenfor1,000promptsdrawnfromdatasetsci.Assimulationprogresses,weobserve
aclearshifttowardhigherprobabilityvalues,reflectingincreasednext-tokenpredictabilityand
acorrespondingdeclineinlinguisticdiversity.Figure10inAppendixA.2showsthatthesame
patternalsoappliestodatasetswikiandxls.
ThistrendisfurthersupportedbytheevolutionoftheaverageGinicoefficientofthenext-token
probabilitydistributions,ğº(ğ‘),andthepercentageofcollapsepredictions,ğ¶(ğ‘),oversimulation
steps.AsFigure3ashowsforLlama2,asthesimulationgoesby,ğº(ğ‘)increases.Thisindicatesa
growingimbalanceinthedistributionofnext-tokenprobabilities,i.e.,anincreasingconcentration
oftheprobabilitymassonasmallsubsetoftokens.Asimilarpatternisobservedforğ¶(ğ‘) (see
Figure3b):asthesimulationprogresses,thepercentageofcollapsedpredictionsincreasesinall
fine-tuning scenarios analyzed. Figure 12 in Appendix A.4 shows that similar results hold for
Llama3andMistral.
Modelcollapsealsodegradesthemodelâ€™scapacitytogeneratemeaningfulsentences.Wefind
thattheLlama2â€™scommonsensicalinferenceaccuracy,ğ´ ,deterioratesoversimulationsteps:its
CI
abilitytogeneratecommonsensicalanswersdeclinesconsistently(seeFigure4).Thisdegradation
isevidentespeciallywhenthemodelistrainedonthecombineddatasetâ€“witharelativedecrease
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.
111:12 Gambettaetal.
1
2
3
4
5
0.0 0.2 0.4 0.6 0.8 1.0
Probability qw
knaR
paper 1 paper 1 paper
study 2 study 2 study
work 3 work 3 work
research 4 th 4 th
article 5 research 5 project
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Probability qw Probability qw
(a) Step0 (b) Step5 (c) Step10
600 600 600
500 500 500
400 400 400
300 300 300
200 200 200
100 100 100
0 0 0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Probability qw Probability qw Probability qw
(d) Step0 (e) Step5 (f) Step10
Fig.2. Next-tokenprobabilitiesasautophagyprogresses.(aâ€“c)Next-tokenprobabilitydistributions
(x-axis)forthetop-5predictedtokens(y-axis)producedbyLlama2atsimulationsteps0,5,and10.The
exampleisbasedonapromptfromthescidataset:â€œTheobstetricElectronicMedicalRecord(EMR)contains
alargeamountofmedicaldataandhealthinformation.Itplaysavitalroleinimprovingthequalityofthe
diagnosis assistant service. In this...â€ As the simulation progresses, the distribution becomes increasingly
skewed,withasingletoken(â€œpaperâ€)dominatingbystep10.(dâ€“f)Distributionsofthehighest-rankedtoken
probabilitiesacross1,000promptsfromthescidataset,evaluatedatsteps0,5,and10.Aclearshifttoward
higherprobabilityvaluesisobservedastheautophagyprocessunfolds.
ofapproximately23%â€“butalsowhentrainedoneachdatasetindividually,withdecreasesranging
from1.7%onscito16.3%onxls(seeFigure4).Inotherwords,astheautophagyprocessunfolds,
the modelâ€™s predictions increasingly diverge from common sense. This unprecedented finding
showsthatautophagynotonlydiminisheslinguisticentropy,butalsodegradescommonsensical
inferencecapabilities.TheobservedcollapsecanbeattributedtotheuseofAI-generateddocuments
forfine-tuning,asthedeclineismarkedlylesspronouncedwhenthemodelistrainedonhuman-
authoreddocuments(seeFigure4).Figure12inAppendixA.4showsthatsimilarresultsholdfor
Llama3andMistral.
Table3illustratesanexampleofhowthemodelâ€™soutputsevolveacrossiterationsoftheautophagy
process.Atsimulationstep0,thegeneratedcontinuationsarefluentandmeaningful,closelyaligned
withtheprompt.Bystep5,signsofcollapseappear,withincreasingredundancy,lossofspecificity,
andmildincoherence,especiallyinthexlsandscidatasets.Bystep10,collapsebecomesevident:
themodeltendstorepeatphrasesandproducetemplatedormeaninglesssequences.
Theresultsreportedsofarcorrespondtothecaseğ‘˜ =64,whereeachpromptcontains64human-
authoredtokensandthemodelgeneratesupto64additionaltokenstocompletethedocument.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.
LearningbySurprise:SurplexityforMitigatingModelCollapseinGenerativeAI 111:13
0.96
0.94
0.92
0.90
0.88
0.86
0 1 2 3 4 5 6 7 8 9 10
Simulation Steps
)q(G
tneiciffeoC
iniG
.gvA
25
20
15
10
human
AI: combined
AI: wiki 5
AI: xls
AI: sci
0
0 1 2 3 4 5 6 7 8 9 10
Simulation Steps
(a) Ginicoefficient
)q(C
snoitciderP
despalloC
%
human
AI: combined AI: wiki
AI: xls AI: sci
(b) Collapsedpredictions
Fig.3. EffectoftheautophagyprocessontheGinicoefficientandthepercentageofcollapsed
predictions.(a)Ginicoefficient,ğº(ğ‘),ofnext-tokenprobabilitydistributionforLlama2(ğ‘˜ = 64)across
10simulationsteps.(b)Percentageofcollapsedpredictions,ğ¶(ğ‘),across10simulationsteps.Eachcurve
representsadifferentfine-tuningscenario:onhuman-authoreddocuments(human,greenopencircles),on
AI-generateddocumentsfromindividualdatasetsâ€“AI: wiki(squares),AI: xls(diamons),andAI: sci
(triangles)â€“andonacombinationofallthree(AI: combined,redfilledcircles).Bothğº(ğ‘)andğ¶(ğ‘)increase
asautophagyprogresses,indicatinganincreasingconcentrationoftheprobabilitymassonasmallsubestof
tokens.
Wenowvaryğ‘˜ = 32,64,96,whichcorrespondto75%,50%and25%oftheAI-generatedtokens
out of a total of 128, using Llama2 and the combined dataset to generate prompts. Figure 9 in
AppendixA.1showstheaveragelinguisticentropy,commonsensicalinferenceaccuracy,theaverage
Ginicoefficientandthepercentageofcollapsedpredictionschangingalongsimulationstepsfor
differentpercentagesofAI-generatedtokensingenerateddocuments.Wefindthatasğ‘˜ decreases,
modelcollapsebecomesmorepronouncedacrossallthemeasures.Inotherwords,increasingthe
fractionofAI-generatedtokensacceleratesmodelcollapse,intensifyingthelossofdiversityand
theoverestimationofhigh-probabletokens.
7 Learningbysurprise:anewmethodformitigatingmodelcollapse
Anopenquestionishowtomitigatemodelcollapseduringtheautophagyprocess[45].Acommon
strategyistoincorporatehuman-authoreddocumentsintofine-tuning,ratherthanrelyingsolely
onAI-generateddocumentsâ€“anapproachshowntoreducetheseverityofmodelcollapse[45].
However,theliteraturedoesnotprovidespecificcriteriaforselectingwhichdocumentsaremost
suitable,treatingallhuman-authoredtextsasequallyeffective.
BasedonthefindingsofSection6.3,hereweproposeastrategyforselectingthemosteffective
documents(whetherhuman-authoredorAI-generated)tomitigatemodelcollapse.Ourapproach
is motivated by the observation that model collapse is marked by an increasing inequality in
next-tokenprobabilities(asshowninSection6.3),wherethemodelbecomesincreasinglyconfident
inasmallsetofhigh-probabilitytokens.Thisgrowingimbalancecanbeinterpretedasalossof
surprise:themodelincreasinglygeneratestextsthataligntoocloselywithitsownexpectations,
producingpredictablesequenceswithlittlenovelty.ThisphenomenonechoesFristonâ€™sfreeenergy
principle[16],whichpositsthathumanagentsminimisesurprisetomaintaininternalcoherence.In
autophagy,however,excessivesurpriseminimisationbecomesmaladaptive:themodelreinforcesits
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.
111:14 Gambettaetal.
0.58
0.56
0.54
0.52
0.50
0.48
0.46
0 1 2 3 4 5 6 7 8 9 10
Simulation Steps
IC
A
.ccA
ecnerefnI
esnesnommoC
human
AI: combined
AI: wiki
AI: xls
AI: sci
Fig.4. Effectsoftheautophagyprocessoncommonsensicalinferenceaccuracy.Commonsensical
inferenceaccuracy,ğ´ CI,ofLlama2across10simulationssteps.Eachcurverepresentsadifferentfine-tuning
scenario: on human-authored documents (human, green open circles), on AI-generated documents from
individualdatasetsâ€“AI: wiki(squares),AI: xls(diamons),andAI: sci(triangles)â€“andonacombination
ofallthree(AI: combined,redfilledcircles).Astheautophagyprocessunfolds,themodelâ€™spredictions
increasinglydiversefromcommonsense.
priorsandlosesexpressivecapacity.Weargue,therefore,thatmodelcollapsecanbecounteracted
byintentionallyreintroducingsurpriseintothelearningprocess.
Tothisend,weintroducethesurplexitymeasure,whichquantifiesamodelâ€™ssurpriseinresponse
toagivendocument.Intuitively,surplexityisdefinedastheperplexityofthemodelwhenprocessing
thedocument:highervaluesindicatethatthecontentislessexpectedbythemodel,andtherefore
moresurprising[33].Mathematically,givenamodelğ‘€ ğ‘— andadocumentğ‘‘ = (ğ‘¤ 1 ,ğ‘¤ 2 ,...,ğ‘¤ ğ‘š)ofğ‘š
tokens,wedefinesurplexityofğ‘‘ givenğ‘€ ğ‘— astheexponentialoftheaveragenegativelog-likelihood
ofeachtokengivenitscontext:
(cid:32) ğ‘š (cid:33) ğ‘š
ğ‘† ğ‘€ğ‘— (ğ‘‘) =exp âˆ’ ğ‘š 1 âˆ‘ï¸ logğ‘ ğ‘– =exp(E[âˆ’logğ‘ ğ‘–]) = (cid:214) ğ‘ ğ‘– âˆ’1/ğ‘š
ğ‘–=0 ğ‘–=0
whereğ‘ ğ‘– =ğ‘ƒ(ğ‘¤ ğ‘– |ğ‘¤ 0 ,...,ğ‘¤ ğ‘–âˆ’1 ).Itisimportanttodistinguishbetweensurplexityandlinguistic
entropy:whilebothcaptureaspectsofdiversity,linguisticentropyisapropertyofthedocument
alone,whereassurplexitydependsonboththedocumentandthemodel,reflectinghowsurprising
thedocumentisfromthemodelâ€™sperspective.
Toclarifytheconceptofsurplexity,letusconsiderthetwoexampledocumentsfromthewiki
andscidatasetsinFigure5.Eachtokenisshownasabox,withitsbackgroundcolorrepresenting
theprobabilityassignedbytheoriginalpre-trainedLlama2.Weobservethatthewikidocument
(ontheleft)containsconsistentlyhigher-probabilitytokensthanthescidocument(ontheright),
reflectingacloseralignmentwiththemodelâ€™sexpectations.Accordingly,thesurplexityscoreof
thewikidocumentgivenLlama2ismuchlower(ğ‘† (ğ‘‘) =1.6)thanthesurplexityscoreofthe
Llama2
scidocument(ğ‘† (ğ‘‘) =44.5).
Llama2
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.
LearningbySurprise:SurplexityforMitigatingModelCollapseinGenerativeAI 111:15
wiki xls sci
tpmorP
TheChurchofStGeorgeisame- Atabout19:30onFridaythefire The reliance of deep learning al-
dievalEasternOrthodoxchurchin alarmwentoff.Iwentoutofthe gorithms on large scale datasets
thecityofKyustendil,whichlies roomandsawotherguestsmilling representsasignificantchallenge
in southwestern Bulgaria and is about, and no one seemed to be whenlearningfromlowresource
theadministrativecapitalofKyus- reallyconcerned.SoIwentbackto signlanguagedatasets.Thischal-
tendilProvince.Thechurchislo- myroomtogetreadytoleavefor lengeiscompoundedwhenwecon-
cated in the Kolusha neighbour- theairport.Iwentdownstairsto siderthat,foramodeltobeeffec-
hood,whichwashistoricallysepa- settlethebillaround20:30andit tiveintherealworld,itmustnot
ratefromthecity.The... waslikeasceneoutofamovie. onlylearnthevariationsofagiven
sign,butalsolearntobeinvariant
tothepersonsigning.Inthispaper,
0petS
churchissituatedontheeasternside Iwasinthelobbywiththereception- wepresentanewapproachtoad-
ofthecity,atthefootoftheBalkan ist,andthefiremenwererunning dressingthesechallenges,byintro-
Mountains.sierp2011thechurch downthestairswiththeirmaskson ducinganovellossfunction,which
wasdeclaredaculturalmon-ument andtheirhoses.Theywereshouting, wecalltheâ€œMixedPairwiseLossâ€,
ofnationalimportance.Thechurch â€™Getout,getout!â€™Ididnâ€™tknowwhat thatcanbeappliedtoboththetrain-
is a single-nave structure with a wasgoingon.Ithoughtmaybethey ing and testing of deep learning
semi-circularapse,withabelltower wereshootingamovie. models.Wepresentanumberofex-
abovethe perimentsthatdemonstratetheef-
fectivenessoftheproposedmethod.
5petS
The church is a The church is a Theplacewasfilledwithsmokeand weproposeanovelmethodologyfor
The church is a The church is a thefirewasreallybig.Iwastheonly learningsignlanguagefromalow
ThechurchisaThechurchisaThe guestinthehotelandIwastheonly resourcedataset.Weproposeanovel
churchisaThechurchisa</s> onewhowasabletogetout.Iwas methodologyforlearningsignlan-
theonlyonewhowasabletogetout. guagefromalowresourcedataset.
Iwastheonlyonewhowasableto Weproposeanovelmethodologyfor
getout.</s> learningsignlanguagefromalow
Therelianceofdeeplearningalgo-
rithmsonlargescale</s>
01petS
sierp 2020. The church is a The Theplacewaslikeawarzone.The weproposeanovelmethodologyfor
church is a The church is a The placewaslikeawarzone.Theplace learningsignlanguagefromalow
church is a The church is a The waslikeawarzone.Theplacewas resourcedataset.Weproposeanovel
church is a The church is a The likeawarzone.Theplacewaslikea methodologyforlearningsignlan-
church is a The church is a The warzone.Theplacewaslikeawar guagefromalowresourcedataset.
churchisa</s> zone.Theplacewaslikeawar</s> Weproposeanovelmethodologyfor
learningsignlanguagefromalow
resourcedataset.Weproposeanovel
methodologyforlearning</s>
Table3. ExamplesofAI-generatedtextastheautophagyprocessprogresses.Promptsof64tokens
fromthethreedatasetsareatthetopofeachcolumn.AI-generatedcontinuationsbyLlama2atsimulation
steps0,5,and10arehighlightedinitalic.Step10isalsohighlightedinyellow.Theprogressionillustrates
howgeneratedtextdegeneratesacrosssimulationsteps.
Beyondthisspecificexample,tokensinwikidocumentsaregenerallymuchlesssurprisingto
Llama2thanthoseinscidocuments.Figure6showsthedistributionofsurplexityscoresforall
documentsinthewiki,xls,andscidatasets,computedwithrespecttoLlama2.Forcomparison,
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.
111:16 Gambettaetal.
Wikitextdocument Sciabsdocument
The First Amendment( I ) to the United Hydra is a Wordnet managementsystem
States Constitution prohib its making where the Synsets from different languages
of any law respect ing an establishment live in commonrel ational structure
religion , impeding free exercise ab ( Krip keframe )with user -frendly GUI
for searching , editing and alignment
rid ging freedom speech in fr on press
of objects . The data retrieved by means
inter fer with right peaceably assemble
modal logic query language Despite
or iting pet ition for a government al
its manymerits stores only current
red ress gr ievances . It was adopted
state word development opens questions
December 1579as one ten ammentsthat consist encyover time newTime Flow
const itute Bill Rights originally proposed uses Dynamic model discrete embeded
measure all states
qw
0 1
Surplexity=1.6 Surplexity=44.5
Fig.5. Examplesofdocumentswithdifferentsurplexity.Theleftdocumentisfromthewikidataset,
andtherightfromsci.Eachtokenisdisplayedinabox,withbackgroundcolorindicatingtheprobability
assignedbytheoriginalLlama2model.Thewikidocumentcontainsconsistentlyhigher-probabilitytokens
thanthescidocument,reflectingalowerlevelofmodelsurprise(i.e.,lowersurplexity).
thefigurealsoincludesthesurplexitydistributionofAI-generateddocuments,whichtypically
consistsofhigh-probabilitytokensandthuslowersurplexity.
Weobservethatthewikidocumentshavethelowestaveragesurplexity,closelyaligningwith
the surplexity scores of the AI-generated documents. In contrast, the xls and sci documents
exhibitsignificantlyhigheraveragesurplexityvalues.Thissuggeststhatdocumentsinwikiare
inherentlylesssurprisingtothemodel,i.e.,theymorecloselyreflectLLama2â€™sinternalexpectations,
comparedtothoseinxlsandsci,whichcontaincontentthatismorenovelorunexpectedfrom
themodelâ€™sperspective.Thewikidatasetâ€™slowsurplexitycorrelateswithmoreseverecollapse:as
thesimulationprogresses,modelsfine-tunedonwikiconsistentlydegrademorethantheother
datasets(seeFigures1and3).
Wehypothesizethatmodelcollapseisdrivenbyfine-tuningondocumentsthatfailtosurprise
themodel,i.e.,thosewithlowsurplexityrelativetoitsexpectations.Toaddressthis,wepropose
asurplexity-basedmitigationstrategy:ateachsimulationstep ğ‘—,weselectthe1,000documents
â€“regardlessofwhethertheyarehuman-authoredorAI-generatedâ€“withthehighestsurplexity
givenmodelğ‘€ ğ‘—.Thisensuresthatfine-tuningtargetsthemostsurprisingcontent,independentof
documentorigin.Thisrepresentsaclearadvantageoverexistingapproaches,asourmethoddoes
notrelyonverifyingwhetheradocumentishuman-authoredâ€“ataskthatbecomesincreasingly
challengingasgenerativeAIcontentproliferatesonline.
Wecompareoursurplexity-basedstrategyagainstthreebaselines:(i)fine-tuningonarandom
sampleof1,000human-authoreddocuments,(ii)arandomsampleof1,000AI-generateddocuments,
and (iii) the1,000 documentswiththe highestlinguisticentropy,whetherhuman-authored or
AI-generated.Thelatterstrategyismotivatedbypriorwork,whereadeclineinlinguisticentropy
iscommonlyusedtocharacterisemodelcollapse[39].
AsFigure7shows,ourmitigationstrategyeffectivelypreventsmodelcollapse,achievingan
averagelinguisticentropycomparabletotheapproachthatrandomlysampleshuman-authored
documents.Asexpected,theentropy-basedstrategyreachesthehighestlevelsoflinguisticentropy,
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.
LearningbySurprise:SurplexityforMitigatingModelCollapseinGenerativeAI 111:17
50
40
30
20
10
0
wiki xls sci
ytixelpruS
Fig.6. Distributionofsurplexityacrossdatasets.Surplexitydistributions(computedwithrespectto
Llama2)fordocumentsinthreedatasetsâ€“wiki(blue),sci(green),andxls(red)â€“comparedwiththe
surplexityofdocumentsgeneratedbyLlama2itself(lightgrey).
sinceitisexplicitlyoptimizedforthatmetric.Asimilartrendholdsforcommonsensicalinference
accuracy:oversimulationsteps,thesurplexity-basedstrategyachievesthehighestaccuracy,slightly
outperformingthehuman-textbaseline.
However, the most striking benefits of the surplexity-based strategy emerge in terms of the
averageGinicoefficientandthepercentageofcollapsedpredictions.Onbothmetrics,itconsistently
yieldsthelowestvalues,indicatingamorebalancednext-tokenprobabilitydistributionandless
overconfident,repetitivegeneration.SimilarresultsholdforLlama3andMistral(seeFigure12in
theAppendix).
Tounderstandwhichdocumentsareselectedbythesurplexity-basedstrategy,weanalyzethe
top1,000documentschosenforfine-tuningateachsimulationstep ğ‘—.Thesedocumentscanbe
eitherhuman-authoredorAI-generatedandarerankedbytheirsurplexitygivenmodelğ‘€ ğ‘—.Figure8
showstheproportionofselecteddocumentsbysource.Althoughnearlyallselecteddocumentsare
human-authored,theshareofAI-generateddocumentsincreasesslightlytowardlatersimulation
steps,suggestingthatsomemayreachhighersurplexityiftheprocesscontinues.Amonghuman-
authoredsources,wikicontributesthefewestdocuments(fewerthan100perstep)becauseits
contentisconsistentlylesssurprisingtothemodel(asshowninFigure6).Whilescidocuments
aregenerallypreferred,thenumberofselecteddocumentsfromsciandxlsalternatesacross
simulationstepsinacomplementary,oscillatingpattern.Whenmodelğ‘€ ğ‘— isfine-tunedprimarily
onxls,itssurplexityonthatdatasetdecreasesinthenextstep,makingxlsdocumentslesslikely
tobeselectedagain.Thishighlightsthatthepropertyofcausingmodelcollapseisnotanintrinsic
characteristicofadocument,butratherdependsontherelationshipbetweenthedocumentand
thespecificmodel.
8 DiscussionandFutureWorks
Inthispaper,weaddressedthreekeygapsintheunderstandingofmodelcollapse:howtocharac-
terizeacollapsedmodeldirectlyfromitsprobabilitydistributions;whichpropertiesoftraining
documentscontributetocollapse;andhowtomitigatecollapsewithoutrelyingonknowingwhether
dataishuman-authoredorAI-generated.Ourcontributionsarebuiltaroundtheconceptofsurplex-
ity,ameasureofmodelsurprise,whichweusebothtodetectcollapseandtofiltertrainingdata.We
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.
111:18 Gambettaetal.
1.06
1.04
1.02
1.00
0.98
0.96
0.94
0.92
0 1 2 3 4 5 6 7 8 9 10
Simulation Steps
)d(H
yportnE
citsiugniL
.mroN
.gvA
0.60
0.58
0.56
0.54
0.52
0.50
human
AI 0.48
entropy
surplexity 0.46
0 1 2 3 4 5 6 7 8 9 10
Simulation Steps
(a)
ICA
.ccA
ecnerefnI
esnesnommoC
human
AI
entropy
surplexity
(b)
0.96
0.94
0.92
0.90
0.88
0.86
0 1 2 3 4 5 6 7 8 9 10
Simulation Steps
)q(G
tneiciffeoC
iniG
.gvA
human
AI 25
entropy
surplexity
20
15
10
5
0
0 1 2 3 4 5 6 7 8 9 10
Simulation Steps
(c)
)q(C
snoitciderP
despalloC
%
human
AI
entropy
surplexity
(d)
Fig.7. Effectsofmitigationstrategiesonmodelcollapse.Metricsofmodelcollapseover10simulation
stepsfordifferentmitigationstrategiesusingLlama2withğ‘˜ =64.(a)Averagelinguisticentropy,(b)common-
senseinferenceaccuracy,(c)averageGinicoefficient,and(d)percentageofcollapsedpredictions.Eachcurve
correspondstoadifferentmitigationscenario:arandomsampleof1,000human-authoreddocuments(human,
greendots);arandomsampleof1,000AI-generateddocuments(AI,squares);the1,000documentswiththe
highestlinguisticentropy,whetherhuman-orAI-generated(triangles);andthesurplexity-basedstrategy,
selectingthe1,000documentswiththehighestsurplexity(blackcrosses),regardlessoforigin.
showthatthisstrategyiseffectiveacrossmodelsanddatasets,offeringaprincipled,model-centric
approachtounderstandingandmitigatingcollapse.Akeyadvantageofthisapproachisthatit
doesnotrequireverifyingwhetheradocumentishuman-authored.
Researchoncognitivescienceshowsthatsurprisedriveslearning:infantslooklongeratphysi-
callyimpossibleorcounterintuitiveevents,andthoseeventssubsequentlyenhanceexplorationand
learning[3,28].Infantsalsoallocateattentionbypreferringstimulithatareneithertoopredictable
nortoorandom[25].Inhumansmorebroadly,Bayesiansurpriserobustlyattractsattention,offering
aformallinkbetweensurpriseandbeliefupdates[24],andFristonâ€™sfree-energyprincipleframes
perceptionandlearningasminimizingexpectedsurpriseovertime[16].Oursurprise-basedmetric
forLLMsâ€“surplexityâ€“isanengineeringanalogueofthesefindings:byprioritizinghigh-surprise
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.
LearningbySurprise:SurplexityforMitigatingModelCollapseinGenerativeAI 111:19
1000
800
600
400
200
0
0 1 2 3 4 5 6 7 8 9 10
Simulation Steps
scod
ytixelprus
0001
pot
ni
.qerF
wiki xls sci AI
Fig.8. Proportionofdocumentsfromeachdatasetselectedbythesurplexity-basedmitigation
strategy.Ateachsimulationstep,barsindicatetheproportionofselecteddocumentsoriginatingfromeach
dataset.Althoughnearlyallselecteddocumentsarehuman-authored,theshareofAI-generatedcontent
increasesslightlyinlatersimulationsteps.Thewikidatasetcontributesthefewestdocumentsoverall,while
sciandxlsalternateinacomplementary,oscillatingpatternacrosssteps.
documents,trainingtargetsinputsmostlikelytoupdatethemodelâ€™sbeliefs,ratherthanreinforcing
already-expectedoutputs,echoinghowinfantslearnmostfromtheunexpected[40].
Ourstudyhassomelimitations.Toensurecomputationalefficiencyandreduceconfounding
factors,wefixedkeyfine-tuningparameterssuchasthenumberoftrainingdocumentsandthe
learningrate.Thesechoicesaremotivatedbypriorworkthathasalreadyexaminedtheirimpact
onmodelcollapse[23,41].OuranalysisisalsolimitedtoLLMsanddoesnotextendtogenerative
modelsforimages,audio,video,ormultimodalcontent.Whilewebelievetheeffectivenessofthe
surplexity-basedmitigationstrategymayextendtothesedomains,furthervalidationisneeded.
Additionally,oursimulationsetupmayintroducebiases.Sinceweconditiongenerationonprompts
extractedfromhuman-authoredtext(followingtheapproachofShumailovetal.[39]),themodel
may over-rely on this context, potentially masking effects that would arise in fully synthetic
generationloops.Thiscouldleadustounderestimateexposurebiasandcollapseinscenarioswhere
bothinputsandoutputsareAI-generated.Finally,asinthebroaderliteratureonAIautophagy
and model collapse [35, 36], our findings are based on simulations. Real-world systems are far
morecomplex,withmanyinteractingfactorsinfluencingautophagydynamics.Assuch,ourresults
should be seen as hypotheses to be tested through carefully controlled experiments on actual
generativeAIplatforms.
Our study opens several directions for future research. We conducted our experiments on
threetextualdomains:generalarticles(wiki),scientificabstracts(sci),andnewsarticles(xls).
Whilethesedatasetsarequitediverse,theyarestillabouthumanlanguage.Itwouldbevaluableto
investigatehowmodelcollapseunfoldsinothercontextssuchasprogrammingcodeormathematical
writing, which present unique characteristics. Programming code, for example, has a smaller
vocabulary and follows standardized structures and conventions, which may naturally reduce
linguisticvariety.Asimilarobservationappliestomathematicallanguage,wherenotationisoften
fixedandformalizationfollowswell-establishednorms.Insuchsettings,autophagymightprogress
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.
111:20 Gambettaetal.
evenfaster,asthebaselinecontentisalreadylowinsurprise.Thisisconsistentwithourfindings:in
ourexperiments,thedatasetwiththelowestsurplexity(wiki)alsoexhibitedfastermodelcollapse.
However,lowlinguisticdiversityindomainslikeprogrammingormathematicsmaynotbeharmful;
infact,itcouldbebeneficial,promotingmoreconsistentformattingandstructure.Exploringthese
domain-specificdynamicscouldprovidedeeperinsightsintowhencollapseisdetrimentalversus
whenitmayhelpreinforceusefulconventions.Webelievethisisapromisingdirectionforfuture
work.
Fromabroaderperspective,theconceptofsurplexityopensupintriguingquestionsaboutthe
levelofsurpriseinhuman-authoredcontent.Asourexperimentsshow,differentdatasetselicit
differentlevelsofsurpriseinLLMs,suggestingthatsometypesofcontentareinherentlymore
predictable than others. This raises the possibility of characterizing entire content domains â€“
literarygenres,musicallyrics,orscientifictextsâ€“basedonhowsurprisingtheyaretoamodel.In
otherwords,howsurprisingisacorpusofdocumentsfromtheperspectiveofanLLM?Thisline
ofinquirycouldextendbeyondtexttootherhuman-createdmedia,suchasvisualartormusic,
offeringamodel-basedlensthroughwhichtoexploreandcomparecreativedomains.
Ourworkoffersaninnovativecontributiontotheongoingdebateonautophagyandmodel
collapse,andmorebroadlytothestudyofhuman-AIcoevolution[36].Human-AIcoevolution
focusesonunderstandingandmodelingfeedbackloopsbetweenhumansandAIsystems,ofwhich
AIautophagyisaprimeexample.Similarfeedbackdynamicshavebeenobservedinvarioushuman-
AIecosystems,suchasonlineretailandurbanmapping,whererecommendersystemscanleadto
along-termlossofbehavioraldiversityamongusers[8,9,15,26,31,35].Itwouldbevaluableto
explorewhetherthislossofdiversity,likemodelcollapseinLGMs,canbeformalisedasareduction
insurpriseandwhetherthesurplexitymeasurecouldbeadaptedtodiagnoseandmitigatesuch
effectsinbroaderhuman-AIecosystems.
ReproducibilityandCode
AlldataandmodelsfromthisstudyarepubliclyavailablethroughtheHuggingFacerepository:
https://huggingface.co/dgambettaphd.Thesimulationframeworkandanalysispresentedinthis
paperarefullyreproducible,withcodeavailableathttps://github.com/dgambit/LLM_surplexity.
Authorsâ€™contributions.
DGandGGimplementedthecodeforsimulations.DGimplementedthecodeforthedataanalysis.
Allauthorsconceptualisedthework.DGandLPdesignedthefigures.DGmadetheplots.DG,
LP,GG,andAKwrotethepaper.LPandAKdirectedandsupervisedtheresearch.Allauthors
contributedtothescientificdiscussion,readandapprovedthepaper.
Acknowledgments
LucaPappalardohasbeensupportedbyPNRR(PianoNazionalediRipresaeResilienza)inthe
contextoftheresearchprogram20224CZ5X4PE6PRIN2022â€œURBAIâ€“UrbanArtificialIntelligenceâ€
(CUPB53D23012770006),fundedbyEuropeanUnionâ€“NextGenerationEU.
ThisworkhasbeensupportedbytheEuropeanUnionunderERC-2018-ADGGA834756(XAI),
thePartnershipExtendedPE00000013-â€œFAIR-FutureArtificialIntelligenceResearchâ€-Spoke1
â€œHuman-centeredAIâ€.
WethankGiulianoCornacchia,GiovanniMauro,GabrieleBarlacchi,andMargheritaLallifor
theusefuldiscussions.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.
LearningbySurprise:SurplexityforMitigatingModelCollapseinGenerativeAI 111:21
References
[1] SinaAlemohammad,JosueCasco-Rodriguez,LorenzoLuzi,AhmedImtiazHumayun,HosseinBabaei,DanielLeJeune,
AliSiahkoohi,andRichardG.Baraniuk.2023.Self-ConsumingGenerativeModelsGoMAD.arXiv:2307.01850[cs.LG]
[2] DanialAlihosseini,EhsanMontahaei,andMahdiehSoleymaniBaghshah.2019. JointlyMeasuringDiversityand
QualityinTextGenerationModels.InProceedingsoftheWorkshoponMethodsforOptimizingandEvaluatingNeural
LanguageGeneration,AntoineBosselut,AsliCelikyilmaz,MarjanGhazvininejad,SrinivasanIyer,UrvashiKhandelwal,
HannahRashkin,andThomasWolf(Eds.).AssociationforComputationalLinguistics,Minneapolis,Minnesota,90â€“98.
doi:10.18653/v1/W19-2311
[3] RenÃ©eBaillargeonandJulieDeVos.1991.Objectpermanenceinyounginfants:Furtherevidence.Childdevelopment
62,6(1991),1227â€“1246.
[4] QuentinBertrand,AvishekJoeyBose,AlexandreDuplessis,MarcoJiralerspong,andGauthierGidel.2024. Onthe
StabilityofIterativeRetrainingofGenerativeModelsontheirownData.arXiv:2310.00429[cs.LG]
[5] MatyasBohacekandHanyFarid.2023. NepotisticallyTrainedGenerative-AIModelsCollapse. arXivpreprint
arXiv:2311.12202(2023).
[6] MartinBriesch,DominikSobania,andFranzRothlauf.2023.Largelanguagemodelssufferfromtheirownoutput:An
analysisoftheself-consumingtrainingloop.arXivpreprintarXiv:2311.16822(2023).
[7] JoÃ£oPhillipeCardenuto,JingYang,RafaelPadilha,RenjieWan,DanielMoreira,HaoliangLi,ShiqiWang,Fernanda
AndalÃ³,SÃ©bastienMarcel,andAndersonRocha.2023.TheAgeofSyntheticRealities:ChallengesandOpportunities.
arXiv:2306.11503[cs.CY] https://arxiv.org/abs/2306.11503
[8] GiulianoCornacchia,MatteoBÃ¶hm,GiovanniMauro,MircoNanni,DinoPedreschi,andLucaPappalardo.2022.How
routingstrategiesimpacturbanemissions.InProceedingsofthe30thinternationalconferenceonadvancesingeographic
informationsystems.1â€“4.
[9] GiulianoCornacchia,MircoNanni,DinoPedreschi,andLucaPappalardo.2024. Navigationservicesandurban
sustainability.FluctuationandNoiseLetters23,03(2024),2450016.
[10] ElvisDohmatob,YunzhenFeng,andJuliaKempe.2024.ModelCollapseDemystified:TheCaseofRegression.arXiv
preprintarXiv:2402.07712(2024).
[11] ElvisDohmatob,YunzhenFeng,PuYang,FrancoisCharton,andJuliaKempe.2024.ATaleofTails:ModelCollapseas
aChangeofScalingLaws.arXivpreprintarXiv:2402.07043(2024).
[12] GeorgeDrayson,EmineYilmaz,andVasileiosLampos.2025.Machine-generatedtextdetectionpreventslanguage
modelcollapse.arXiv:2502.15654[cs.CL] https://arxiv.org/abs/2502.15654
[13] Europol Innovation Lab. 2021. Facing Reality: Law Enforcement and the Challenge of Deepfakes. Available
at:https://www.europol.europa.eu/cms/sites/default/files/documents/Europol_Innovation_Lab_Facing_Reality_Law_
Enforcement_And_The_Challenge_Of_Deepf.
[14] YunzhenFeng,ElvisDohmatob,PuYang,FrancoisCharton,andJuliaKempe.2024.BeyondModelCollapse:Scaling
UpwithSynthesizedDataRequiresVerification.arXiv:2406.07515[cs.LG] https://arxiv.org/abs/2406.07515
[15] DanielFlederandKartikHosanagar.2009.Blockbustercultureâ€™snextriseorfall:Theimpactofrecommendersystems
onsalesdiversity.Managementscience55,5(2009),697â€“712.
[16] KarlFriston.2010.Thefree-energyprinciple:aunifiedbraintheory?NatureReviewsNeuroscience11,2(2010),127â€“138.
doi:10.1038/nrn2787
[17] ShiFu,YingjieWang,YuzhuChen,XinmeiTian,andDachengTao.2025.ATheoreticalPerspective:HowtoPrevent
ModelCollapseinSelf-consumingTrainingLoops.arXiv:2502.18865[cs.LG] https://arxiv.org/abs/2502.18865
[18] MatthiasGerstgrasser,RylanSchaeffer,ApratimDey,RafaelRafailov,HenrySleight,JohnHughes,TomaszKorbak,
RajashreeAgrawal,DhruvPai,AndreyGromov,DanielA.Roberts,DiyiYang,DavidL.Donoho,andSanmiKoyejo.
2024. IsModelCollapseInevitable?BreakingtheCurseofRecursionbyAccumulatingRealandSyntheticData.
arXiv:2404.01413[cs.LG]
[19] MarzyehGhassemi,AbebaBirhane,MushtaqBilal,SiddharthKankaria,ClaireMalone,EthanMollick,andFrancisco
Tustumi.2023.ChatGPToneyearon:whoisusingit,howandwhy?Nature624,7990(2023),39â€“41.
[20] YanzhuGuo,GuokanShang,MichalisVazirgiannis,andChloÃ©Clavel.2023.TheCuriousDeclineofLinguisticDiversity:
TrainingLanguageModelsonSyntheticText.arXiv:2311.09807[cs.CL]
[21] TahmidHasan,AbhikBhattacharjee,Md.SaifulIslam,KaziMubasshir,Yuan-FangLi,Yong-BinKang,M.SohelRahman,
andRifatShahriyar.2021.XL-Sum:Large-ScaleMultilingualAbstractiveSummarizationfor44Languages.InFindings
oftheAssociationforComputationalLinguistics:ACL-IJCNLP2021,ChengqingZong,FeiXia,WenjieLi,andRoberto
Navigli(Eds.).AssociationforComputationalLinguistics,Online,4693â€“4703.doi:10.18653/v1/2021.findings-acl.413
[22] RyuichiroHataya,HanBao,andHiromiArai.2023.WillLarge-scaleGenerativeModelsCorruptFutureDatasets?.In
ProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision.20555â€“20565.
[23] DavidHerelandTomasMikolov.2024.CollapseofSelf-trainedLanguageModels.arXiv:2404.02305[cs.CL]
[24] LaurentIttiandPierreBaldi.2009.Bayesiansurpriseattractshumanattention.Visionresearch49,10(2009),1295â€“1306.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.
111:22 Gambettaetal.
[25] CelesteKidd,StevenTPiantadosi,andRichardNAslin.2012.TheGoldilockseffect:Humaninfantsallocateattention
tovisualsequencesthatareneithertoosimplenortoocomplex.PloSone7,5(2012),e36399.
[26] DokyunLeeandKartikHosanagar.2019. Howdorecommendersystemsaffectsalesdiversity?Across-category
investigationviarandomizedfieldexperiment.InformationSystemsResearch30,1(2019),239â€“259.
[27] SiyangLiu,SahandSabour,YinheZheng,PeiKe,XiaoyanZhu,andMinlieHuang.2022.RethinkingandRefiningthe
DistinctMetric.arXiv:2202.13587[cs.CL] https://arxiv.org/abs/2202.13587
[28] FrancescoMargoni,LucaSurian,andRenÃ©eBaillargeon.2024.Theviolation-of-expectationparadigm:Aconceptual
overview.PsychologicalReview131,3(2024),716.
[29] GonzaloMartÃ­nez,LaurenWatson,PedroReviriego,JosÃ©AlbertoHernÃ¡ndez,MarcJuarez,andRikSarkar.2023.Towards
understandingtheinterplayofgenerativeartificialintelligenceandtheinternet.arXivpreprintarXiv:2306.06130(2023).
[30] GonzaloMartÃ­nez,LaurenWatson,PedroReviriego,JosÃ©AlbertoHernÃ¡ndez,MarcJuarez,andRikSarkar.2023.
CombiningGenerativeArtificialIntelligence(AI)andtheInternet:HeadingtowardsEvolutionorDegradation?
arXiv:2303.01255[cs.CV]
[31] GiovanniMauro,MarcoMinici,andLucaPappalardo.2025.TheUrbanImpactofAI:ModelingFeedbackLoopsin
Next-VenueRecommendation.arXiv:2504.07911[cs.AI] https://arxiv.org/abs/2504.07911
[32] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer Sentinel Mixture Models.
arXiv:1609.07843[cs.CL] https://arxiv.org/abs/1609.07843
[33] AlessioMiaschi,DominiqueBrunato,FeliceDellâ€™Orletta,andGiuliaVenturi.2021.WhatMakesMyModelPerplexed?
ALinguisticInvestigationonNeuralLanguageModelsPerplexity.InProceedingsofDeepLearningInsideOut(DeeLIO):
The2ndWorkshoponKnowledgeExtractionandIntegrationforDeepLearningArchitectures,EnekoAgirre,Marianna
Apidianaki,andIvanVuliÄ‡(Eds.).AssociationforComputationalLinguistics,Online,40â€“47.doi:10.18653/v1/2021.deelio-
1.5
[34] GerhardPaaÃŸandSvenGiesselbach.2023.Foundationmodelsfornaturallanguageprocessing:Pre-trainedlanguage
modelsintegratingmedia.SpringerNature.
[35] LucaPappalardo,EmanueleFerragina,SalvatoreCitraro,GiulianoCornacchia,MircoNanni,GiulioRossetti,Gizem
Gezici,FoscaGiannotti,MargheritaLalli,DanieleGambetta,etal.2024. AsurveyontheimpactofAI-basedrec-
ommendersonhumanbehaviours:methodologies,outcomesandfuturedirections.arXivpreprintarXiv:2407.01630
(2024).
[36] DinoPedreschi,LucaPappalardo,EmanueleFerragina,RicardoBaeza-Yates,Albert-LÃ¡szlÃ³BarabÃ¡si,FrankDignum,
VirginiaDignum,TinaEliassi-Rad,FoscaGiannotti,JÃ¡nosKertÃ©sz,AlistairKnott,YannisIoannidis,PaulLukowicz,
AndreaPassarella,AlexSandyPentland,JohnShawe-Taylor,andAlessandroVespignani.2025.Human-AIcoevolution.
ArtificialIntelligence339(2025),104244. doi:10.1016/j.artint.2024.104244
[37] RylanSchaeffer,JoshuaKazdan,AlvanCalebArulandu,andSanmiKoyejo.2025.Position:ModelCollapseDoesNot
MeanWhatYouThink.arXiv:2503.03150[cs.LG] https://arxiv.org/abs/2503.03150
[38] MohamedElAmineSeddik,Suei-WenChen,SoufianeHayou,PierreYoussef,andMerouaneDebbah.2024.HowBadis
TrainingonSyntheticData?AStatisticalAnalysisofLanguageModelCollapse.arXiv:2404.05090[cs.LG]
[39] IliaShumailov,ZakharShumaylov,YirenZhao,NicolasPapernot,RossAnderson,andYarinGal.2024. AImodels
collapsewhentrainedonrecursivelygenerateddata.Nature631,8022(2024),755â€“759.
[40] AimeeEStahlandLisaFeigenson.2015.Observingtheunexpectedenhancesinfantsâ€™learningandexploration.Science
348,6230(2015),91â€“94.
[41] AnandaTheerthaSuresh,AndrewThangaraj,andAdityaNandaKishoreKhandavally.2024.RateofModelCollapsein
RecursiveTraining.arXiv:2412.17646[cs.LG] https://arxiv.org/abs/2412.17646
[42] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,NikolayBashlykov,Soumya
Batra,PrajjwalBhargava,ShrutiBhosale,etal.2023.Llama2:Openfoundationandfine-tunedchatmodels.arXiv
preprintarXiv:2307.09288(2023).
[43] PabloVillalobos,JaimeSevilla,LennartHeim,TamayBesiroglu,MariusHobbhahn,andAnsonHo.2022.Willwerun
outofdata?ananalysisofthelimitsofscalingdatasetsinmachinelearning.arXivpreprintarXiv:2211.04325(2022).
[44] JunchaoWu,ShuYang,RunzheZhan,YulinYuan,DerekFWong,andLidiaSChao.2023.Asurveyonllm-gernerated
textdetection:Necessity,methods,andfuturedirections.arXivpreprintarXiv:2310.14724(2023).
[45] XiaodanXing,FadongShi,JiahaoHuang,YinzheWu,YangNan,ShengZhang,YingyingFang,MichaelRoberts,
Carola-BibianeSchÃ¶nlieb,JavierDelSer,etal.2025. OnthecaveatsofAIautophagy. NatureMachineIntelligence
(2025),1â€“9.
[46] JinweiXu,HeZhang,YanjingYang,LanxinYang,ZeruCheng,JunLyu,BohanLiu,XinZhou,AlbertoBacchelli,YinKia
Chiam,andThiamKianChiew.2025. OneSizeDoesNotFitAll:InvestigatingEfficacyofPerplexityinDetecting
LLM-GeneratedCode.ACMTransactionsonSoftwareEngineeringandMethodology(July2025). doi:10.1145/3748506
[47] RowanZellers,AriHoltzman,YonatanBisk,AliFarhadi,andYejinChoi.2019. HellaSwag:CanaMachineReally
FinishYourSentence?arXiv:1905.07830[cs.CL] https://arxiv.org/abs/1905.07830
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.
LearningbySurprise:SurplexityforMitigatingModelCollapseinGenerativeAI 111:23
[48] XuekaiZhu,DaixuanCheng,HengliLi,KaiyanZhang,ErmoHua,XingtaiLv,NingDing,ZhouhanLin,Zilong
Zheng,andBowenZhou.2024. HowtoSynthesizeTextDatawithoutModelCollapse? arXiv:2412.14689[cs.CL]
https://arxiv.org/abs/2412.14689
A Appendix
A.1 Differentpercentageofsynthetictokens
Inthissection,weexaminehowmodelcollapsevarieswiththeshareofAI-generatedtokens.Wefix
thetotaldocumentlengthatğ¿ =128andvarythepromptlengthğ‘˜ âˆˆ {32,96};thesecorrespondto
25%and75%prompttokens(andthus75%and25%ofAI-generatedtokens),respectively.Figure9
reportsğ»(ğ‘‘),ğ´ ,ğº(ğ‘),ğ¶(ğ‘) across simulation steps for eachğ‘˜. Across all measures, collapse
CI
becomesmorepronouncedasğ‘˜ decreases,indicatingconsistenttrendsamongthefourmeasures.
1.00
0.98
0.96
0.94
0.92
0.90
0.88
0 1 2 3 4 5 6 7 8 9 10
Simulation Steps
)d(H
yportnE
citsiugniL
.mroN
.gvA
0.58
0.56
0.54
0.52
0.50
k
96 0.48
64 32 0.46
0 1 2 3 4 5 6 7 8 9 10
Simulation Steps
(a) Linguisticentropy
ICA
.ccA
ecnerefnI
esnesnommoC
k
96
64 32
(b) Semanticresolutionaccuracy
0.98 0.96
0.94
0.92
0.90
0.88
0.86
0 1 2 3 4 5 6 7 8 9 10
Simulation Steps
)q(G
tneiciffeoC
iniG
.gvA
60 50
40
30
20
k
96
64 10
32 0
0 1 2 3 4 5 6 7 8 9 10
Simulation Steps
(c) Ginicoefficient
)q(C snoitciderP
despalloC
%
k 96 64
32
(d) Collapsedpredictions
Fig.9. Effectsoftheautophagyprocessvaryingthepercentageofsynthetictokensingenerated
documents.(a)Normalizedlinguisticentropyofdocumentsgeneratedbyllama2(ğ‘˜ =32,46,96)across10
simulationsteps.(b)Semanticresolutionaccuracyofllama2overthesamesteps.(c)Ginicoefficient.(d)
Percentageofcollapsedpredictions.
A.2 Top-rankednext-tokenprobabilitiesforalldatasets
Inthemainmanuscript,Figure2(dâ€“f)reportsthedistributionoftop-rankednext-tokenprobabilities
usingsciprompts;hereweextendtheanalysistootherdatasets.AsshowninFigure10,alldatasets
exhibitasystematicshifttowardhighertop-tokenprobabilitiesoversimulationsteps.Collapseis
strongestforwiki:bystep5,morethanhalfofthetoptokensexceed0.9.Bycontrast,scishows
theweakestcollapse,withfewerthanhalfsurpassing0.9evenatstep10.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.
111:24 Gambettaetal.
600 600 600
500 500 500
400 400 400
300 300 300
wiki 200 200 200
100 100 100
0 0 0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Probability qw Probability qw Probability qw
(a) (b) (c)
600 600 600
500 500 500
400 400 400
300 300 300
xls 200 200 200
100 100 100
0 0 0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Probability qw Probability qw Probability qw
(d) (e) (f)
600 600 600
500 500 500
400 400 400
300 300 300
sci 200 200 200
100 100 100
0 0 0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Probability qw Probability qw Probability qw
(g) (h) (i)
Fig.10. Distributionsofthehighest-rankedtokenprobabilitiesacross1,000promptsfromthewiki(a-c),xls
(d-f)andsci(g-i)dataset,evaluatedatsteps0,5,and10.Aclearshifttowardhigherprobabilityvaluesis
observedastheautophagyprocessunfolds,withanhighercollapseinthecaseofwiki.
A.3 EvaluationofGinicoefficientandcollapsedpredictionswithpromptsof64tokens
InFigure7ofSection6,weevaluatedfine-tuningscenariosusingnext-tokenâ€“basedmetricswith
promptsof32tokens.Here,wereplicatetheanalysiswithpromptsof64tokens.Thesurplexity
scenario again shows the lowest degree of collapse across all metrics, thereby reinforcing its
effectiveness.
A.4 Mitigationofmodelcollapseusingdifferentmodels
Inthemainsection,allresultsarereportedforLlama2.HereweextendtheanalysistoLlama3and
Mistral,evaluatingthesamefine-tuningscenarioswithourmodel-collapsemetrics.Thepatterns
observedforLlama2generalize:bothadditionalmodelsexhibitthesametrends,confirmingthe
robustnessofourfindings.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.
LearningbySurprise:SurplexityforMitigatingModelCollapseinGenerativeAI 111:25
0.98
0.96
0.94
0.92
0.90
0.88
0.86
0 1 2 3 4 5 6 7 8 9 10
Simulation Steps
)q(G
tneiciffeoC
iniG
.gvA
50
40
human
AI 30
entropy
surplexity
20
10
0
0 1 2 3 4 5 6 7 8 9 10
Simulation Steps
(a)
)q(C
snoitciderP
despalloC
%
human
AI
entropy
surplexity
(b)
Fig.11. Effectsofmitigationstrategiesonmodelcollapseevaluatedwithpromptsof64tokensinnexttoken
prediction.
1.06
1.04
1.02
1.00
0.98
0.96
0.94 0.92
0 1 2 3 4 5 6 7 8 9 10
Simulation Steps
)d(H
yportnE
citsiugniL
.mroN
.gvA
Llama-2-7b
1.05
1.04
1.03
1.02
1.01
human 1.00
AI e su n r t p ro le p x y ity 0.99
0 1 2 3 4 5 6 7 8 9 10
Simulation Steps
(a)
)d(H
yportnE
citsiugniL
.mroN
.gvA
Llama-3-8b
1.06
1.05
1.04
1.03
1.02
1.01
1.00 0.99
0 1 2 3 4 5 6 7 8 9 10
Simulation Steps
(b)
)d(H
yportnE
citsiugniL
.mroN
.gvA
Mistral-7b
(c)
0.60
0.58
0.56
0.54
0.52
0.50
0.48
0.46
0 1 2 3 4 5 6 7 8 9 10
Simulation Steps
ICA
.ccA
ecnerefnI
esnesnommoC
Llama-2-7b
0.650
0.625
0.600
0.575
0.550
0.525
0.500
0 1 2 3 4 5 6 7 8 910
Simulation Steps
(d)
ICA
.ccA
ecnerefnI
esnesnommoC
Llama-3-8b
0.60
0.55
0.50
0.45
0 1 2 3 4 5 6 7 8 9 10
Simulation Steps
(e)
ICA
.ccA
ecnerefnI
esnesnommoC
Mistral-7b
(f)
Fig.12. LinguisticentropyandCommonsenseInferenceAccuracyforallfine-tuningscenariosanddifferent
models(Llama-2-7b,Llama-3-8bandMistral-7b).Foreverymodelthemitigationstrategybasedonsurplexity
showsgoodresultsinbothmetrics.
Received20February2007;revised12March2009;accepted5June2009
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Learning by Surprise: Surplexity for Mitigating Model Collapse in Generative AI"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   âŒ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   âœ… GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   âŒ BAD: Repeating the same claim 3+ times with slight variations
   âœ… GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
