=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Scalable predictive processing framework for multitask caregiving robots
Citation Key: idei2025scalable
Authors: Hayato Idei, Tamon Miyake, Tetsuya Ogata

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Key Terms: task, processing, robots, scalable, wiping, caregiving, japan, hierarchical, repositioning, framework

=== FULL PAPER TEXT ===

Title:
Scalable predictive processing framework for multitask caregiving robots
Authors:
Hayato Idei,1* Tamon Miyake,2 Tetsuya Ogata,3 Yuichi Yamashita1*
Affiliations:
1Department of Information Medicine, National Institute of Neuroscience, National Center
of Neurology and Psychiatry; Tokyo, Japan.
2Future Robotics Organization, Waseda University; Tokyo, Japan
3Department of Intermedia Art and Science, Waseda University; Tokyo, Japan.
*Corresponding author. Email: idei@ncnp.go.jp or yamay@ncnp.go.jp
Abstract:
The rapid aging of societies is intensifying demand for autonomous care robots; however, most
existing systems are task-specific and rely on handcrafted preprocessing, limiting their ability to
generalize across diverse scenarios. A prevailing theory in cognitive neuroscience proposes that the
human brain operates through hierarchical predictive processing, which underlies flexible cognition
and behavior by integrating multimodal sensory signals. Inspired by this principle, we introduce a
hierarchical multimodal recurrent neural network grounded in predictive processing under the free-
energy principle, capable of directly integrating over 30,000-dimensional visuo-proprioceptive
inputs without dimensionality reduction. The model was able to learn two representative caregiving
tasks, rigid-body repositioning and flexible-towel wiping, without task-specific feature engineering.
We demonstrate three key properties: (i) self-organization of hierarchical latent dynamics that
regulate task transitions, capture variability in uncertainty, and infer occluded states; (ii) robustness
to degraded vision through visuo-proprioceptive integration; and (iii) asymmetric interference in
multitask learning, where the more variable wiping task had little influence on repositioning,
whereas learning the repositioning task led to a modest reduction in wiping performance, while the
model maintained overall robustness. Although the evaluation was limited to simulation, these
results establish predictive processing as a universal and scalable computational principle, pointing
toward robust, flexible, and autonomous caregiving robots while offering theoretical insight into
the human brainâ€™s ability to achieve flexible adaptation in uncertain real-world environments.
Page 1 of 22
Main Text:
INTRODUCTION
As societies worldwide age rapidly, the growing demand for long-term care is exacerbated
by an increasingly severe shortage of professional caregivers (1â€“3). Physically demanding
tasks such as patient repositioning or body cleaning are not only labor intensive but also a
leading cause of musculoskeletal disorders, particularly lower-back pain, among caregivers
(4, 5). To address these challenges, various assistive robotic technologies have been
developed (6â€“12), ranging from transfer devices and exoskeletons to humanoid systems
designed for lifting or repositioning patients. However, most existing systems are either
intended to support human operators or are specialized for a single, narrowly defined task,
thus limiting their utility across the diverse and unpredictable scenarios encountered in real
care settings. Therefore, there is an urgent need for next-generation care robots capable of
acting autonomously and flexibly across multiple caregiving tasks. However, this remains
a formidable challenge as such tasks involve direct interaction with humans and are
characterized by substantial uncertainty and variability. Moreover, caregiving behaviors,
such as supporting body parts, adjusting posture, or wiping body surfaces, differ
fundamentally in their motor patterns, interaction objects, and sources of uncertainty. The
development of a unified computational framework that enables a single robot to learn and
generalize across these heterogeneous caregiving tasks represents an open and pressing
frontier in robotics.
By contrast, humans can flexibly perform a wide range of caregiving tasks with a single
brain, even under uncertain and dynamic conditions. Such cognitive flexibility is thought to
arise from the brainâ€™s remarkable capacity to efficiently process the continuous influx of
massive, multimodal sensory signals and to contextualize them for adaptive behavior. An
influential theoretical framework in neuroscience is predictive processing, a computational
framework under the free-energy principle that explains cognition as the minimization of
prediction errors between sensory inputs and internally generated predictions (13â€“18).
Rather than passively receiving inputs, the brain is viewed as an active inference system
that continuously anticipates sensory signals and updates its internal models in response to
prediction errors. This perspective has increasingly inspired research in artificial
intelligence and robotics, where predictive processingâ€“based neural architectures are being
explored as a means of endowing robots with adaptive, brain-like learning capabilities
across diverse tasks (19â€“28).
Despite this promise, a fundamental challenge remains: extending predictive processing to
the direct integration of high-dimensional, multimodal sensory data, which is a prerequisite
for robust multitask learning in real-world robotics. Current robotic approaches often rely
on preprocessing, dimensionality reduction, or additional external processing modules, such
as specialized transformations or attention-based mechanisms, to extract features from each
modality (e.g., vision or proprioception), typically in a task-specific manner. Although
effective in constrained or well-defined settings, such strategies inherently limit
generalization because they depend on modality- and task-specific assumptions. This
limitation becomes particularly problematic in multitask caregiving scenarios, where tasks
differ widely in motor patterns, sensory targets, and sources of uncertainty. However, a
unified computational framework that can directly integrate raw, high-dimensional, and
uncertain sensory signals across modalities is lacking. Such a framework would not only
enable robust and flexible learning across heterogeneous caregiving tasks but also validate
Page 2 of 22
predictive processing as a general computational principle for intelligent behavior under
real-world uncertainty.
Here, we introduce a scalable predictive-codingâ€“inspired variational recurrent neural
network (PV-RNN) designed to integrate high-dimensional, multimodal sensory
information. PV-RNN is a generative model that infers probabilistic and dynamic latent
states under the free-energy principle, thereby capturing hidden stochastic structures in
temporal data and remaining robust even in continuous and uncertain time series arising
from humanâ€“robot interactions (29â€“31). In our earlier work, we extended PV-RNN into a
hierarchical multimodal architecture and showed that it could self-organize dynamic
modulations of hierarchical uncertainty estimation, while accounting for complex neural
phenomena such as sensory attenuation and allostasis, as reported in neuroscience (32â€“33).
Building on this foundation, we refined the framework to enhance predictive
performance/scalability by introducing additional feedforward processing, enabling the
model to better handle high-dimensional sensory signals with rapid, jagged temporal
variations, while still operating solely under the principle of prediction error minimization
(Fig. 1A and 1B). In contrast to conventional methods that depend on handcrafted features
or dimensionality reduction, the proposed scalable PV-RNN learns end-to-end through
variational free-energy minimization, which directly integrate raw, high-dimensional
sensory signals across modalities. This capability enables the model to flexibly self-organize
latent representations suited to diverse modalities and tasks, thereby offering a unified
framework for multitask learning in robotics. Beyond its engineering contributions,
evaluating the scalable PV-RNN on caregiving tasks also provides scientific value, enabling
us to test whether prediction error minimization, as a single computational principle, can
support the dynamic processing of high-dimensional multimodal information and yield
robust adaptive behavior under uncertainty, as observed in humans.
RESULTS
Task description
This study investigated robot learning in caregiving tasks through high-dimensional visuo-
proprioceptive integration. We employed the Dry-AIREC humanoid robot developed by
Tokyo Robotics Inc., Tokyo, Japan (Fig. 1C). Dry-AIREC is equipped with multiple sensing
modalities, including proprioceptive and visual sensors, providing a suitable platform for
studying embodied interaction. The head was fitted with binocular RGB cameras, and each
arm had seven degrees of freedom (DOFs) with torque sensors at every joint. For input to
the scalable PV-RNN, we used 28-dimensional proprioceptive signals (14 joint angles and
torques) and 32,256-dimensional visual signals (binocular RGB images at 16Ã—16, 32Ã—32,
and 64Ã—64 resolutions).
The robot was positioned in front of a bed on which a mannequin (height: 1800 mm;
shoulder width: 470 mm; leg length: 885 mm; arm length: 750 mm; weight: 8 kg) served as
the care recipient. We focused on two representative caregiving tasks, repositioning and
wiping, that are central to maintaining the health and quality of life of individuals with
limited mobility (Fig. 1D). The repositioning task (supine-to-sitting transfer) comprised
three subtasks: (i) reaching the left hand to the mannequinâ€™s neck (reaching), (ii) placing the
left hand under the upper back while the right hand supported the legs (holding), and (iii)
Page 3 of 22
lifting the upper body (lifting). The wiping task also comprised three subtasks: (i) reaching
for a towel placed on the mannequin (reaching), (ii) wiping the body surface (wiping), and
(iii) completing the wiping motion (releasing).
The learning and test data were obtained through teleoperation using a motion-capture
system under three bed heights (590, 650, and 710 mm). We recorded visuo-proprioceptive
sequences for 18 learning trials (9 per task, 3 per height) and 6 test trials (3 per task, 1 per
height).
The repositioning task required dynamic manipulation of a rigid but heavy body with
shifting load distribution, while adapting to variations in bed height, initial posture, and
motion timing. By contrast, the wiping task required manipulation of a flexible towel across
curved and uneven surfaces, while also accommodating variations in bed height, towel
position, initial posture, and motion pattern. Both tasks required accurate target reaching,
adaptation to object properties, and robustness to diverse spatiotemporal uncertainties.
Together, they represent qualitatively distinct sources of variability and rigid-body
dynamics under load versus flexible-object surface interaction. The central question
addressed in this study was whether a single computational model can simultaneously learn
and generalize across heterogeneous caregiving tasks.
Predictive processing of the neural network
The scalable PV-RNN is grounded in the free-energy principle; rather than directly
consuming sensory inputs, it learns and infers by minimizing variational free energy across
time steps (32).
>
1
ğ¹ = (ğ’™ âˆ’ğ’™) )# +ğ‘Š1ğ· 3ğ‘(ğ’› (ğ’) |ğ’† )||ğ‘(ğ’› (ğ’) |ğ’… (ğ’) ): (1)
! 2+,,,ğ’•-,,,ğ’•. +.,/,,,,ğ’•,,,ğ’•,:ğ‘»-,,,,ğ’•,,,ğ’•,5,ğŸ.
?@A
Prediction error Influence of prior beliefs
ğ’’,(ğ’)
Here, ğ’™ denotes the observed sensory inputs, ğ’™) the predicted sensory inputs, ğ’› the
ğ’• ğ’• ğ’•
ğ’‘,(ğ’)
posterior belief (latent states inferred from observations), ğ’› the prior belief (latent states
ğ’•
(ğ’)
predicted from past experience), ğ’† the back-propagated error signal, and ğ’… the states of
ğ’•:ğ‘» ğ’•
recurrent units (time step ğ‘¡, last step ğ‘‡, network level ğ‘™). Variational free energy has two
components, the first quantifies sensory prediction error, while the second is the Kullbackâ€“
Leibler divergence (KLD) between posterior and prior, capturing how past experience
constrains posterior inference. The balancing hyperparameter ğ‘Š was set to 0.005, following
our prior work on multimodal PV-RNNs (32).
During learning, both the time-invariant synaptic weights ğ and the sequence-specific
ğ’’,(ğ’)
posterior beliefs ğ’› are updated to minimize the cumulative free energy across entire
ğŸ:ğ‘»
learning sequences. Importantly, posterior beliefs are updated independently for each
sequence, enabling the simultaneous learning of multiple tasks.
During testing, the synaptic weights remain fixed, and the model performs online inference
ğ’’,(ğ’)
by updating ğ’› within a sliding short-time window (H = 30) at each time step. This
ğ’•5ğ‘¯GğŸ:ğ’•
procedure locally minimizes the variational free energy, thereby implementing an adaptive
inference process that dynamically adjusts the latent states to incoming sensory information.
Page 4 of 22
Thus, the proposed scalable PV-RNN functions entirely through predictive processing
based on variational free-energy minimization, without requiring handcrafted feature
extraction, dimensionality reduction, or any additional external processing. For evaluation,
we trained five scalable PV-RNNs with different random initializations to reconstruct visuo-
proprioceptive sequences from 18 learning datasets. The trained networks were then tested
in simulation-based generalization experiments using six test datasets, with five inference
trials per sequence, yielding 30 test trials per trained network. All test evaluations were
conducted in a simulation rather than on a physical robot.
Experiment 1: Multitask generalization through hierarchical probabilistic inference
Figure 2A illustrates an example of successful online inference for a test visuo-
proprioceptive sequence in the repositioning task. Figure 2B presents results of an ablation
analysis quantifying how latent states in each network module contributed to visual
predictions. Contribution strength is visualized in grayscale, with brighter (whiter) regions
indicating image areas more strongly influenced by the ablated module (see Materials and
Methods). Figure 2B enables comparison between the grayscale visualization of visual
contributions from each module and the actual image, where contours are highlighted. These
results revealed self-organized hierarchical task representations. The exteroceptive module
maintained continuous visual representations, particularly highlighting regions
corresponding to the robotâ€™s left arm and the mannequinâ€™s face. The multimodal associative
module exhibited strong latent dynamics when the visuo-proprioceptive relationship was
most prominent, for instance, as the robotâ€™s hands approached the mannequin (steps 0â€“75)
or during the lifting phase (steps 200â€“300). In contrast, the executive module showed
marked changes in latent states and enhanced visual representations specifically at subtask
boundaries, for example, at the onset of reaching (step ~0), at the initiation of left- and right-
hand movements during the holding phase (steps ~75 and ~125), and at the beginning of the
lifting phase (step ~200). These patterns suggest that the executive module functioned as a
controller for motion switching, selectively signaling transitions between distinct subtask
segments.
Figure 3 shows the corresponding results of the wiping task. Overall, the three modules
displayed similar hierarchical specializations as in the repositioning task. The exteroceptive
module maintained continuous visual representations, particularly highlighting regions
corresponding to the robotâ€™s left arm and the mannequinâ€™s body. The multimodal associative
module captured dynamic visuo-proprioceptive integration, particularly during the reaching,
wiping, and releasing phases (steps 75â€“225). The executive module again exhibited state
changes at subtask transitions, strongly detecting the shifts from initial posture to reaching
and from wiping to releasing, while the transition from reaching to wiping appeared
smoother. Notably, in predicted images (Fig. 3B), the mannequinâ€™s body contours occluded
by the robot arms were inferred, demonstrating the modelâ€™s capacity to complete uncertain
hidden states. Collectively, these findings indicate that scalable PV-RNN developed
hierarchical processing, with each module assuming distinct roles in dynamically allocating
task-dependent attention and controlling subtask transitions.
Finally, Figure 4 compares task-specific uncertainty dynamics estimated by the network.
For the mean level of uncertainty (Fig. 4A), prior sigma values were first averaged across
all time steps and latent states within each trial, and then further averaged across 75 test runs
for each task (five trials for each of the three test sequences and five independently trained
networks). For variability in uncertainty (Fig. 4B), we quantified temporal fluctuations of
Page 5 of 22
prior sigma by computing, at each time step, the absolute difference from its value at the
previous step, averaging these differences across all time steps and latent states within each
trial, and then averaging them across the 75 test runs. Using these aggregated measures, we
found that the mean level of estimated uncertainty was similar between repositioning and
wiping, whereas variability in uncertainty was significantly greater for the wiping task. This
suggests that the model flexibly adapted to differences in volatility across tasks, enabling
generalization despite qualitatively distinct sources of uncertainty.
Experiment 2: Robustness under uncertainty through multimodal integration
To evaluate whether our model maintains robustness under uncertain sensory conditions,
we tested its performance when visual inputs were degraded in resolution. Figure 5A
presents predicted 16Ã—16 and 64Ã—64 images when the model updated its latent states using
only low-resolution (16Ã—16) visual inputs in combination with proprioceptive inputs. For
comparison across trials, results are aligned at the same time step, revealing variability in
motion timing and patterns among sequences. Remarkably, the model was able to generate
high-resolution (64Ã—64) visual predictions even though these signals were not provided
during inference, accurately capturing task-dependent differences in timing and motion
variability.
Figure 5B quantifies the effect of reduced visual resolution on the prediction error for full-
resolution images. Values were first averaged across all time steps and sensory dimensions
within each trial, and then further averaged across 150 trials (five trials for each of six test
sequences and five networks). In both the presence and absence of proprioceptive input,
performance decreased with lower visual resolution; however, this decline was significantly
mitigated when proprioceptive input was available (red vs. blue bars in Fig. 5B).
These findings highlight the importance of multimodal predictive processing in ensuring
robustness under both partial and uncertain sensory conditions. Once trained, the model can
compensate for missing or degraded visual inputs by leveraging proprioceptive information,
thereby sustaining accurate predictions. Moreover, the ability to operate with reduced-
dimensional sensory inputs after training indicates the potential of our framework to achieve
computational efficiency without sacrificing robustness.
Experiment 3: Interference between tasks in learning
To investigate the influence of multitask learning on generalization, we examined the
performance while varying the balance of learning data between the repositioning and
wiping tasks. Figure 6A shows the prediction errors on test sequences of the repositioning
task when the number of repositioning training sequences was fixed at nine, while the
number of wiping training sequences was varied (0, 3, 6, and 9). Prediction error values
were first averaged across all time steps and sensory dimensions, including both
exteroceptive and proprioceptive domains within each trial, and then further averaged across
75 trials (five trials for each of the three test sequences and five networks). Figure 6A
demonstrates that increasing the amount of wiping data did not significantly influence the
generalization performance in repositioning. This suggests that learning the wiping task did
not interfere with the acquisition of task-specific representations required for repositioning.
Page 6 of 22
In contrast, Figure 6B shows that increasing the amount of repositioning training data led
to a statistically significant rise in the mean prediction error for the wiping task; however,
the effect was modest and not disruptive to overall generalization performance. These
results indicate that the scalable PV-RNN maintained robust performance without a
marked drop even when the number of learned tasks increased, while still suggesting an
asymmetric pattern of task interference.
DISCUSSION
This study introduced a brain-inspired framework for integrating high-dimensional,
multimodal sensory information through a unified computational principle of prediction
error minimization, implemented in a multimodal PV-RNN. By operating directly on raw
sensory signals without task-specific preprocessing or external modules, the framework
enabled flexible generalization across qualitatively distinct caregiving tasks. Using
repositioning and wiping as representative cases, we demonstrated three key properties, (i)
self-organization of hierarchical latent dynamics that regulated task-dependent allocation of
attention, captured variability in uncertainty, and enabled the inference of occluded states;
(ii) robustness under uncertain sensory conditions through visuo-proprioceptive integration;
and (iii) asymmetric interference in multitask learning, where the more variable wiping task
exerted minimal influence on repositioning, whereas learning the repositioning task resulted
in a statistically significant but modest degradation in wiping performance, yet the model
maintained overall robustness. Taken together, these results highlight the potential of
predictive processing as a foundation for multitasking robotic intelligence in caregiving.
Beyond the engineering significance, these findings provide a broader discussion on the
computational principles of cognition (34â€“37). Predictive processing under the free-energy
principle has been proposed as a unifying framework for perception and action. However,
its capacity to process real-world, high-dimensional multimodal inputs has remained
underexplored. Our results address this gap by demonstrating the computational validity of
the principle in robotics, rather than strict biological fidelity. The model reproduced
properties often observed in humans: hierarchical task representation (38â€“43), inference of
occluded states, and reliance on proprioception to maintain robust object manipulation when
visual information is ambiguous, such as in dimly lit environments. Moreover, the
asymmetric interference between repositioning and wiping suggests that tasks with higher
variability in uncertainty (such as wiping) may exert weaker influence on the learning of
other tasks, whereas tasks with lower variability (such as repositioning) tend to have
stronger effects. This asymmetry may be partly explained by differences in how variability
in uncertainty shapes internal representations. Tasks with greater variability in uncertainty
(such as wiping) may foster the formation of more flexible and generalized latent
representations, enabling robust adaptation across contexts. In contrast, tasks with lower
variability (such as repositioning) may lead to more specialized and rigid representations,
which can interfere with the learning of other tasks. This perspective provides insight into
human developmental learning processes, suggesting that exploratory behaviors such as
early motor babbling are effective for acquiring diverse skills without interfering with the
learning of other behaviors (44â€“46). While speculative, these parallels suggest that our
approach provides a promising computational tool to link neural principles with adaptive
behavior in complex environments.
From an applied robotics perspective, the proposed framework offers practical advantages
in three respects, robustness, as it can generalize across heterogeneous tasks without
Page 7 of 22
handcrafted preprocessing; efficiency, as it can later operate with reduced sensory resolution
to lower computational costs; and scalability, as the same principle can be extended to
additional tasks and modalities. These features underscore predictive processing as a
versatile and principled pathway toward autonomous care robots capable of addressing
diverse real-world demands.
However, this study has several limitations. First, it was restricted to simulation-based
evaluations using teleoperated data; real-time control experiments on physical robots were
not conducted. Second, although effective, the online inference process is computationally
demanding, running at approximately 3 Hz in our experimental computer environment when
processing proprioceptive signals together with binocular 16Ã—16 RGB inputs. Thus, it
requires more efficient implementations for real-time operation. Third, the mannequin used
as the care recipient was mechanically simplified (lighter and more stable than humans),
reducing ecological validity. Addressing these limitations will require advances in real-time
optimization, more realistic humanâ€“robot interaction scenarios, and scaling to broader
caregiving contexts.
Despite these constraints, our findings demonstrate that a brain-inspired computational
framework can endow robots with robustness and versatility reminiscent of human
cognition. Progress toward real-time implementations and scaling to richer task sets holds
promise for the development of next-generation care robots capable of supporting humans
in diverse and uncertain environments. Such robots would not only alleviate caregiver
burden but also shed light on the computational principles that enable biological intelligence
to thrive in complex worlds.
MATERIALS AND METHODS
Experimental design
The aim of this study was to develop and evaluate a brain-inspired framework for robust
multitask learning in caregiving contexts. Specifically, we tested whether a single
computational principle, prediction error minimization, can support the integration of high-
dimensional, multimodal sensory information and enable generalization across qualitatively
different tasks. To this end, we employed a scalable PV-RNN, a hierarchical multimodal
generative model designed to infer probabilistic latent states under the free-energy principle.
Using this framework, we investigated whether a robot could simultaneously learn and
generalize two distinct caregiving tasks, repositioning and wiping, that differ in their motor
patterns, interaction objects, and sources of uncertainty. Importantly, the present study did
not involve real-time physical execution, instead, all evaluations were conducted in
simulation using visuo-proprioceptive data sequences recorded through teleoperation.
Robot setup and data acquisition
Learning and test data were obtained by teleoperating a dual-arm humanoid robot with a
motion-capture system. During task execution, proprioceptive signals (joint angles and
torques from both arms) and exteroceptive signals (binocular RGB images) were recorded
as time-series data. Each signal was normalized to the range â€“0.9 to 0.9 using its maximum
and minimum values, consistent with the activation functions of the neural network. The
two caregiving tasks were (i) repositioning, in which the robot reached toward and lifted the
upper back of a mannequin, requiring the manipulation of a rigid body with varying load
Page 8 of 22
distribution; and (ii) wiping, in which the robot reached for and manipulated a towel to clean
the mannequinâ€™s curved surface, requiring control of a flexible object under variable contact
conditions. Both tasks were performed under variability in initial mannequin posture, bed
height, and action timing, thereby introducing spatiotemporal uncertainty into the visuo-
proprioceptive data.
In total, 24 visuo-proprioceptive sequences were collected, 18 sequences (9 per task, 3 per
bed height) for learning and 6 sequences (3 per task, 1 per bed height) for testing. To
evaluate robustness across different initializations, we trained five scalable PV-RNNs with
independently randomized synaptic weights. For testing, each trained model performed
online inference across all test sequences with five independent trials, yielding 30 test trials
per model.
Prediction generation of the scalable PV-RNN
The scalable PV-RNN was organized into three hierarchical levels, consistent with our
previous work. Level 1 encoded low-level exteroceptive and proprioceptive representations,
Level 2 integrated these modalities into mid-level sensorimotor representations, and Level
3 controlled higher-level patterns of visuo-proprioceptive integration. To enhance the
processing of high-dimensional sensory inputs, an additional feedforward neural network
was inserted between Level 1 and the sensory prediction outputs, enabling more flexible
handling of complex signals.
Prediction generation was performed in a top-down manner through the network hierarchy.
(I) (I)
The internal state â„ and output ğ‘‘ of the ğ‘–th deterministic recurrent units of the ğ‘ th target
!,H !,H
sequence at time step ğ‘¡ is computed as
1 1
â„ (I) = D1 ğœ” ğ‘‘ (I) + 1 ğœ” ğ‘§ (I) + 1 ğœ” ğ‘‘ (I) + ğ‘ H+I1âˆ’ Jâ„ (I) (ğ‘– âˆˆ ğ¼ ,ğ¼ ,ğ¼ ,ğ¼ )
!,H ğœ HJ !5A,J HJ !,J HJ !,J H ğœ !5A,H Ed Pd Ad Cd
JâˆˆLSd JâˆˆLSz JâˆˆLHd
(2)
ğ‘‘ (I) = tanhRâ„ (I) S (ğ‘– âˆˆ ğ¼ ,ğ¼ ,ğ¼ ğ¼ ) (3)
!,H !,H Ed Pd Ad Cd
Here, ğ¼ , ğ¼ , ğ¼ , and ğ¼ denote index sets of deterministic recurrent states in the
Ed Pd Ad Cd
exteroceptive, proprioceptive, multimodal associative, and executive modules, respectively.
Similarly, ğ¼ , ğ¼ , ğ¼ , and ğ¼ denote index sets of probabilistic latent states. ğ¼ , ğ¼ , and ğ¼
Ez Pz Az Cz Sd Sz Hd
denote recurrent states of the same module, latent states of the same module, and recurrent
states of the higher-level module, respectively. ğœ” is the synaptic weight from the ğ‘—th to the
HJ
(I)
ğ‘–th neuron; ğ‘§ is the output of the ğ‘—th latent (posterior) state at time step ğ‘¡; ğœ is the time
!,J
constant of the recurrent unit; and ğ‘ is the bias of the ğ‘–th recurrent unit. A smaller ğœ yields
H
faster-changing dynamics, while a larger ğœ produces slower dynamics. The initial internal
(I) (I)
states of the recurrent units â„ (ğ‘– âˆˆ ğ¼ ,ğ¼ ,ğ¼ ,ğ¼ ) were set to 0 (thus, ğ‘‘ = 0).
S,H Ed Pd Ad Cd S,H
The latent variable ğ’› follows a multivariate Gaussian distribution with diagonal covariance,
(I) (I)
implying independence between ğ‘§ and ğ‘§ for ğ‘–,ğ‘— âˆˆ ğ¼ ,ğ¼ ,ğ¼ ,ğ¼ âˆ§ğ‘– â‰  ğ‘—. The prior
!,H !,J Ez Pz Az Cz
distribution was parameterized from previous recurrent states in the same module:
Page 9 of 22
(I) (I) (I) (I) (I),T (I),T
ğ‘Rğ‘§ S = ğ‘Rğ‘§ |ğ‘‘ S = ğ’©Rğ‘§ ;ğœ‡ ,ğœ S (4)
!,H !,H !5A,J !,H !,H !,H
ğœ‡ (I),T = tanhD1ğœ” ğ‘‘ (I) H (5)
!,H HJ !5A,J
J
ğœ (I),T = expD1ğœ” ğ‘‘ (I) H (6)
!,H HJ !5A,J
J
where (ğ‘– âˆˆ ğ¼ âˆ§ğ‘— âˆˆ ğ¼ )âˆ¨(ğ‘– âˆˆ ğ¼ âˆ§ğ‘— âˆˆ ğ¼ )âˆ¨(ğ‘– âˆˆ ğ¼ âˆ§ğ‘— âˆˆ ğ¼ )âˆ¨(ğ‘– âˆˆ ğ¼ âˆ§ğ‘— âˆˆ ğ¼ ).
Ez Ed Pz Pd Az Ad Cz Cd
The posterior distribution was calculated as
ğ‘Rğ‘§ (I) |ğ’† (ğ’”) S = ğ’©Rğ‘§ (I) ;ğœ‡ (I),V ,ğœ (I),V S (ğ‘– âˆˆ ğ¼ ,ğ¼ ,ğ¼ ,ğ¼ ) (7)
!,H ğ’•:ğ‘»(ğ’”) !,H !,H !,H Ez Pz Az Cz
ğœ‡
(I),V
= tanhRğ‘
(I),W
S (8)
!,H !,H
ğœ (I),V = expRğ‘ (I),X S (9)
!,H !,H
ğ‘§ (I) = ğœ‡ (I),V +ğœ (I),V Ã—ğœ–, ğœ–~ğ’©(0,1) (10)
!,H !,H !,H
(ğ’”)
Here, ğ’‚ is the adaptive internal state of units parameterizing the posterior distributions.
ğ’•
(ğ’”)
These adaptive variables ğ’‚ were initialized from the corresponding prior distributions
ğ’•
states before the training or inference process.
Two fully connected feedforward neural networks (FNN2L) for exteroceptive signals and
one FNN1L for proprioceptive signals mapped recurrent states to predictions:
ğ’‡ (ğ’”) = ğ¹ğ‘ğ‘2ğ¿Rğ’… (ğ’”) S (11)
ğ’•,ğ‘°Ef ğ’•,ğ‘°Ed
ğ’‡ (ğ’”) = ğ¹ğ‘ğ‘1ğ¿Rğ’… (ğ’”) S (12)
ğ’•,ğ‘°Pf ğ’•,ğ‘°Pd
â§
tanhD1 ğœ” ğ‘“ (I) H (ğ‘– âˆˆ ğ¼ )
âª âª HJ !,J Eo
ğ‘¥p (I) = JâˆˆLEf (13)
!,H
â¨
âªtanhD1 ğœ” ğ‘“ (I) H (ğ‘– âˆˆ ğ¼ )
âª HJ !,J Po
â© JâˆˆLPf
Here, ğ¼ and ğ¼ denote index sets of units in the lower layer of the FNN2L and FNN1L,
Ef Pf
respectively. ğ¼ and ğ¼ denote the output units for exteroceptive and proprioceptive
Eo Po
predictions, respectively.
Cost function of the scalable PV-RNN
(I)
Variational free-energy ğ¹ in the PV-RNN is formulated as:
!
Page 10 of 22
>
(I) (ğ’”) (ğ’”) (ğ’),(ğ’”) (ğ’”) (ğ’),(ğ’”) (ğ’),(ğ’”)
ğ¹ = âˆ’logğ‘Rğ’™ yğ’… S+ğ‘Š1Rğ· 3ğ‘(ğ’› |ğ’† )||ğ‘(ğ’› |ğ’… ):S (14)
! +,,,,-ğ’•,,,ğ’•,. +,.,/,,,,ğ’•,,,,,ğ’•:,ğ‘»-,,,ğ’•,,,,,ğ’•,5,ğŸ,,.
?@A
Accuracy Complexity
The first term (negative accuracy term) corresponds to the negative log-likelihood.
Assuming that each sensory state follows a Gaussian distribution with unit variance, this
(ğ’”)
reduces to the squared prediction error between the observed sensations ğ’™ and predicted
ğ’•
)ğ’™
(ğ’”)
sensations (constant term omitted):
ğ’•
1 #
âˆ’Accuracy = 1 Rğ‘¥ (I) âˆ’ğ‘¥p (I) S (15)
2 !,H !,H
HâˆˆLEoâˆ¨LPo
In practice, the accuracy term was normalized by the dimensionality of each sensory
modality.
The second term (complexity) is the KLD between posterior and prior latent distributions,
quantifying the influence of prior beliefs. Assuming both distributions are multivariate
Gaussians with diagonal covariance, the KLD can be expressed analytically as
# #
ğœ (I),T Rğœ‡ (I),T âˆ’ğœ‡ (I),V S +Rğœ (I),V S 1
!,H !,H !,H !,H
Complexity = 1Dlog + âˆ’ H (16)
ğœ (I),V (I),T # 2
2Rğœ S
H !,H
!,H
Here, ğ‘– âˆˆ ğ¼ ,ğ¼ ,ğ¼ ,ğ¼ . This complexity term was also normalized by the latent
Ez Pz Az Cz
dimensionality of each module.
(ğ’”)
During learning, both synaptic weights ğ and adaptive variables ğ’‚ were updated to
ğŸ:ğ‘»(ğ’”)
minimize cumulative free energy:
d(,)
ğ¹ = 11ğ¹ (I) (17)
?_â€˜abHbc !
IâˆˆL !@A
-
During online inference, synaptic weights were fixed, and only adaptive variables were
updated. Free energy was minimized within a sliding short-time window ğ»:
!
ğ¹
Hbe_a_bf_
= 1 ğ¹
!.
(18)
!.@!5gGA
Adaptive variables within the time window were iteratively updated based on this local free
energy, with the window advancing at each time step.
Crucially, this formulation enables the scalable PV-RNN to operate solely through
variational free-energy minimization, without relying on handcrafted feature extraction,
dimensionality reduction, or auxiliary processing modules external to the free-energy
principle.
Hyper-parameter setting of the scalable PV-RNN
For simplicity, the dimensions of recurrent variables in the exteroceptive, proprioceptive,
Page 11 of 22
multimodal associative, and executive modules were set to ğ‘ = ğ‘ = ğ‘ = ğ‘ = 30.
hi ji ki li
Latent variable dimensions were set to ğ‘ = 40 and ğ‘ = ğ‘ = ğ‘ = 30. These values
hm jm km lm
were determined in a preliminary study that searched for the minimal configuration enabling
successful reconstruction across all trained networks. Specifically, we first set identical
dimensions for recurrent and latent variables (10, 20, or 30) and then increased the number
of latent variables in the exteroceptive module to 40, which was necessary for adequate
reconstruction. The fully connected layer in the proprioceptive pathway comprised 40 units,
while the exteroceptive pathway included two fully connected layers with 40 and 50 units,
respectively.
To incorporate a temporal hierarchy, we implemented multiple timescales across modules.
In the lower perceptual modules, time constants were set as ğœ = ğœ = 2 or 4 (half of the
ji hi
recurrent units each). In the multimodal associative module, ğœ = 4 or 8, and in the
ki
executive module ğœ = 8 or 16. Thus, higher-level modules exhibited slower neural
li
dynamics than lower-level ones, realizing a hierarchical temporal structure.
Synaptic weights were initialized with the Xavier method (47). Biases of deterministic
recurrent variables were initialized and fixed to random values drawn from a Gaussian
distribution ğ’©(0,10), following a previous study (48). During learning, synaptic weights
and adaptive (posterior) variables were updated for ğ‘–ğ‘¡ğ‘Ÿ = 100,000 iterations using
?_â€˜abHbc
the rectified Adam optimizer (49) with learning rate ğ‘™ğ‘Ÿ = 0.001, ğ›½ = 0.9, ğ›½ =
?_â€˜abHbc A #
0.999.
In online inference, only adaptive variables were updated. At each sensorimotor time step
ğ‘¡, they were updated ğ‘–ğ‘¡ğ‘Ÿ = 50 times for a short time window of length ğ» = 30 (or
Hbe_a_bf_
ğ» = ğ‘¡ ğ‘–ğ‘“ ğ‘¡ < 30), with learning rate ğ‘™ğ‘Ÿ = 1.0. Parameter settings were selected by
Hbe_a_bf_
systematically searching combinations of ğ‘™ğ‘Ÿ = {1.0}Ã—ğ‘–ğ‘¡ğ‘Ÿ =
Hbe_a_bf_ Hbe_a_bf_
{20,30,40,50}Ã—ğ» = {10,20,30,40,50} , choosing the minimal configuration that
achieved successful reconstruction of the test data.
Ablation study
To analyze how latent states at different hierarchical levels contributed to visual predictions,
we conducted an ablation study during the online inference tests. For each test sequence,
predicted images were generated under two conditions: (i) the standard condition, with all
latent states intact, and (ii) the ablation condition, with latent states of a specific module
removed. Predictions were averaged across 25 trials (five test trials for each of the five
independently trained networks). At each time step, the pixel-wise difference between the
two conditions was computed. The averaged differences were visualized in grayscale, where
brighter regions indicated visual areas most influenced by the ablated module. Figures 2B
and 3B present the representative results from the right camera at 64Ã—64 resolution,
revealing distinct module-specific contributions to hierarchical task processing.
Statistical analysis
Paired t-tests were used to compare the mean and temporal changes in prior sigma (Fig. 4).
To analyze the effects of proprioceptive input on visual prediction errors under different
image-resolution conditions, a two-way repeated-measures aligned rank transform analysis
of variance (ART-ANOVA) with Holm-corrected post hoc tests was performed (Fig. 5). For
examining prediction errors under different data-balance conditions, one-way ART-
Page 12 of 22
ANOVAs followed by Holm-corrected multiple comparisons were conducted (Fig. 6). All
statistical tests were two-tailed, and the significance level was set at ğ‘ < 0.05. Because this
study involved an unprecedented computational simulation, it was difficult to estimate
effect sizes in advance; therefore, no statistical methods were used to pre-determine the
sample size. Given the high reproducibility of the simulation, a minimum sample size of
five networks was adopted. Even with this small sample, the analyses revealed clear
statistical differences, suggesting that larger samples would not have substantially altered
the main findings. It should be noted that, due to the limited sample size, these statistical
tests were performed primarily as supportive quantitative analyses to complement the
descriptive and comparative findings, rather than to draw definitive inferential conclusions.
All data analyses were conducted using R software (version 4.5.1).
References
1. A. Cieza, K. Causey, K. Kamenov, S. W. Hanson, S. Chatterji, T. Vos, Global estimates
of the need for rehabilitation based on the Global Burden of Disease study 2019: a
systematic analysis for the Global Burden of Disease Study 2019. The Lancet 396, 2006â€“
2017 (2020).
2. R. Martinez-Lacoba, I. Pardo-Garcia, F. Escribano-Sotos, Aging, dependence, and long-
term care: A systematic review of employment creation. INQUIRY 58,
00469580211062426 (2021).
3. C. H. Jones, M. Dolsten, Healthcare on the brink: navigating the challenges of an aging
society in the United States. npj Aging 10, 22 (2024).
4. N. Wiggermann, J. Zhou, N. McGann, Effect of repositioning aids and patient weight on
biomechanical stresses when repositioning patients in bed. Hum. Factors 63, 565â€“577
(2021).
5. A. Gilchrist, A. PokornÃ¡, Prevalence of musculoskeletal low back pain among registered
nurses: Results of an online survey. J. Clin. Nurs. 30, 1675â€“1683 (2021).
6. N. Maalouf, A. Sidaoui, I. H. Elhajj, D. Asmar, Robotics in nursing: A scoping review. J.
Nurs. Scholarsh. 50, 590â€“600 (2018).
7. A. Brinkmann, C. F. BÃ¶hlen, C. Kowalski, S. Lau, O. Meyer, R. Diekmann, A. Hein,
Providing physical relief for nurses by collaborative robotics. Sci. Rep. 12, 8644 (2022).
8. F. Zhang, Y. Demiris, Learning garment manipulation policies toward robot-assisted
dressing. Sci. Robot. 7, eabm6010 (2022).
9. C. Kowalski, A. Brinkmann, C. F. BÃ¶hlen, P. Hinrichs, A. Hein, A rule-based robotic
assistance system providing physical relief for nurses during repositioning tasks at the care
bed. Int. J. Intell. Robot. Appl. 7, 1â€“12 (2023).
10. Y. Wang, Z. Sun, Z. Erickson, D. Held, â€œOne policy to dress them all: Learning to dress
people with diverse poses and garmentsâ€ in Robotics: Science and Systems XIX (Robotics:
Science and Systems Foundation, 2023;
http://www.roboticsproceedings.org/rss19/p008.pdf).
11. G. Cheng, Y. Huang, X. Zhang, H. Chen, J. Ota, An overview of transfer nursing robot:
Classification, key technology, and trend. Robot. Auton. Syst. 174, 104653 (2024).
12. T. Miyake, N. Saito, T. Ogata, Y. Wang, S. Sugano, Dual-arm Motion Generation for
Repositioning Care based on Deep Predictive Learning with Somatosensory Attention
Mechanism. arXiv arXiv:2407.13376 [Preprint] (2025).
https://doi.org/10.48550/arXiv.2407.13376.
Page 13 of 22
13. K. Friston, The free-energy principle: a unified brain theory? Nat. Rev. Neurosci. 11, 127â€“
138 (2010).
14. A. M. Bastos, W. M. Usrey, R. A. Adams, G. R. Mangun, P. Fries, K. J. Friston, Canonical
microcircuits for predictive coding. Neuron 76, 695â€“711 (2012).
15. A. Clark, Whatever next? Predictive brains, situated agents, and the future of cognitive
science. Behav. Brain Sci. 36, 181â€“204 (2013).
16. A. K. Seth, Interoceptive inference, emotion, and the embodied self. Trends Cogn. Sci. 17,
565â€“573 (2013).
17. J. Hohwy, New directions in predictive processing. Mind Lang. 35, 209â€“223 (2020).
18. T. Isomura, K. Kotani, Y. Jimbo, K. J. Friston, Experimental validation of the free-energy
principle with in vitro neural networks. Nat. Comm. 14, 4547 (2023).
19. P. -C. Yang, K. Sasaki, K. Suzuki, K. Kase, S. Sugano, T. Ogata, Repeatable folding task
by humanoid robot worker using deep learning. IEEE Robot. Autom. Lett. 2, 397â€“403
(2017).
20. H. Idei, S. Murata, Y. Chen, Y. Yamashita, J. Tani, T. Ogata, A neurorobotics simulation
of autistic behavior induced by unusual sensory precision. Comput. Psychiatr. 2, 164
(2018).
21. W. Lotter, G. Kreiman, D. Cox, A neural network trained for prediction mimics diverse
features of biological neurons and perception. Nat. Mach. Intell. 2, 210â€“219 (2020).
22. H. Idei, S. Murata, Y. Yamashita, T. Ogata, Paradoxical sensory reactivity induced by
functional disconnection in a robot model of neurodevelopmental disorder. Neural Netw.
138, 150â€“163 (2021).
23. N. Saito, T. Ogata, S. Funabashi, H. Mori, S. Sugano, How to select and use tools? : Active
perception of target objects using multimodal deep learning. IEEE Robot. Autom. Lett. 6,
2517â€“2524 (2021).
24. H. Ito, K. Yamamoto, H. Mori, T. Ogata, Efficient multitask learning with an embodied
predictive model for door opening and entry with whole-body control. Sci. Robot. 7,
eaax8177 (2022).
25. H. Idei, Y. Yamashita, Elucidating multifinal and equifinal pathways to developmental
disorders by constructing real-world neurorobotic models. Neural Netw. 169, 57â€“74
(2024).
26. T. Van de Maele, T. Verbelen, P. Mazzaglia, S. Ferraro, B. Dhoedt, Object-centric scene
representations using active inference. Neural Comput. 36, 677â€“704 (2024).
27. J. Gornet, M. Thomson, Automated construction of cognitive maps with visual predictive
coding. Nat. Mach. Intell. 6, 820â€“833 (2024).
28. P. Vijayaraghavan, J. F. QueiÃŸer, S. V. Flores, J. Tani, Development of compositionality
through interactive learning of language and action of robots. Sci. Robot. 10, eadp0751
(2025).
29. A. Ahmadi, J. Tani, A novel predictive-coding-inspired variational RNN model for online
prediction and recognition. Neural Comput. 31, 2025â€“2074 (2019).
30. W. Ohata, J. Tani, Characterizing the sense of agency in humanâ€“robot interaction based
on the free energy principle. npj Complexity 2, 12 (2025).
31. H. Sawada, W. Ohata, J. Tani, Human-robot kinaesthetic interactions based on the free-
energy principle. IEEE Trans. Syst. Man Cybern.: Syst. 55, 366â€“379 (2025).
32. H. Idei, W. Ohata, Y. Yamashita, T. Ogata, J. Tani, Emergence of sensory attenuation
based upon the free-energy principle. Sci. Rep. 12, 14542 (2022).
Page 14 of 22
33. H. Idei, J. Tani, T. Ogata, Y. Yamashita, Future shapes present: autonomous goal-directed
and sensory-focused mode switching in a Bayesian allostatic network model. npj
Complexity 2, 23 (2025).
34. M. Asada, K. Hosoda, Y. Kuniyoshi, H. Ishiguro, T. Inui, Y. Yoshikawa, M. Ogino, C.
Yoshida, Cognitive developmental robotics: A survey. IEEE Trans. Auton. Ment. Dev. 1,
12â€“34 (2009).
35. A. Cangelosi, G. Metta, G. Sagerer, S. Nolfi, C. Nehaniv, K. Fischer, J. Tani, T. Belpaeme, G.
Sandini, F. Nori, L. Fadiga, B. Wrede, K. Rohlfing, E. Tuci, K. Dautenhahn, J. Saunders, A.
Zeschel, Integration of Action and Language Knowledge: A Roadmap for Developmental
Robotics. IEEE Trans. Auton. Ment. Dev. 2, 167â€“195 (2010).
36. T. J. Prescott, S. P. Wilson, Understanding brain functional architecture through robotics.
Sci. Robot. 8, eadg6014 (2023).
37. T. J. Prescott, K. Vogeley, A. Wykowska, Understanding the sense of self through robotics.
Sci. Robot. 9, eadn2733 (2024).
38. E. Koechlin, C. Ody, F. Kouneiher, The architecture of cognitive control in the human
prefrontal cortex. Science 302, 1181â€“1185 (2003).
39. E. Koechlin, T. Jubault, Brocaâ€™s area and the hierarchical organization of human behavior.
Neuron 50, 963â€“974 (2006).
40. D. Badre, M. Dâ€™Esposito, Functional magnetic resonance imaging evidence for a
hierarchical organization of the prefrontal cortex. J. Cogn. Neurosci. 19, 2082â€“2099
(2007).
41. J. J. F. Ribas-Fernandes, A. Solway, C. Diuk, J. T. McGuire, A. G. Barto, Y. Niv, M. M.
Botvinick, A neural signature of hierarchical reinforcement learning. Neuron 71, 370â€“379
(2011).
42. T. Wen, J. Duncan, D. J. Mitchell, Hierarchical representation of multistep tasks in
multiple-demand and default mode networks. J. Neurosci. 40, 7724 (2020).
43. D. Cellier, I. T. Petersen, K. Hwang, Dynamics of hierarchical task representations. J.
Neurosci. 42, 7276 (2022).
44. N. Dominici, Y. P. Ivanenko, G. Cappellini, A. dâ€™Avella, V. MondÃ¬, M. Cicchese, A.
Fabiano, T. Silei, A. Di Paolo, C. Giannini, R. E. Poppele, F. Lacquaniti, Locomotor
primitives in newborn babies and their development. Science 334, 997â€“999 (2011).
45. K. Libertus, A. S. Joh, A. W. Needham, Motor training at 3 months affects object
exploration 12 months later. Dev. Sci. 19, 1058â€“1066 (2016).
46. K. E. Adolph, J. M. Franchak, The development of motor behavior. WIREs Cogn. Sci. 8, e1430
(2017).
47. Xavier Glorot, Yoshua Bengio, â€œUnderstanding the difficulty of training deep feedforward
neural networksâ€ in Proc. Thirteenth Int. Conf. Artificial Intelligence and Statistics, Yee Whye
Teh, Mike Titterington, Eds. (PMLR, 2010;
https://proceedings.mlr.press/v9/glorot10a.html)vol. 9, pp. 249â€“256.
48. H. Idei, S. Murata, Y. Yamashita, T. Ogata, homogeneous intrinsic neuronal excitability induces
overfitting to sensory noise: A robot model of neurodevelopmental disorder. Front. Psychiatry
11, 762 (2020).
49. L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, J. Han, On the variance of the adaptive learning
rate and beyond. arXiv arXiv:1908.03265 [Preprint] (2021). http://arxiv.org/abs/1908.03265.
Page 15 of 22
Acknowledgments:
Funding:
Japan Science and Technology Agency ACT-X No. JPMJAX24C2 (HI)
Japan Science and Technology Agency Moonshot R&D No. JPMJMS2031 (YY,
TO)
Japan Science and Technology Agency Core Research for Evolutional Science and
Technology No. JPMJCR21P4 (YY)
AMED Multidisciplinary Frontier Brain and Neuroscience Discoveries
(Brain/MINDS 2.0) No. JP24wm0625407 (YY)
Author contributions:
Conceptualization: HI, YY
Methodology: HI
Investigation: HI, TM
Visualization: HI, YY
Funding acquisition: HI, TO, YY
Project administration: HI, YY
Supervision: TO, YY
Writing â€“ original draft: HI
Writing â€“ review & editing: HI, TM, TO, YY
Competing interests: Authors declare that they have no competing interests.
Page 16 of 22
Figures:
A B
Level 3 Executive control
Level 2 Multimodal association
Level 1 Proprioception Exteroception
Fully-connected Fully-connected
layer(cid:186)1 layer(cid:186)2
Proprioceptive Exteroceptive
28 dim. prediction !"! Prediction !"! 32256 dim.
PE PE
Dual arm angles Binocular RGB
28 dim. and torques !! visual images !! 32256 dim.
64(cid:186)64
32(cid:186)32
16(cid:186)16
C D
Repositioning task Wiping task
Initial pose Reaching Holding Initial pose Reaching Reaching
Holding Lifting Lifting Wiping Wiping Releasing
Fig. 1. Computational framework and task setting. (A) Architecture of the scalable PV-
RNN. During learning, sequential posteriors of latent states across all modules and
time-invariant synaptic weights are updated by minimizing variational free-energy
accumulated over all training sequences. PE: prediction error; KLD: Kullbackâ€“
Leibler divergence. (B) Temporal processing and online inference in the PV-RNN.
During inference, posteriors in all modules are updated through minimization of
variational free energy within a short time window (ğ»), while synaptic weights
remain fixed. For clarity, the distributed structure of the proprioceptive and
exteroceptive modules is omitted. (C) The AIREC humanoid robot. (D) Two
caregiving tasks used in this study: repositioning and wiping, both learned by the
scalable PV-RNN.
Page 17 of 22
A B
t=25 t=50 t=75 t=100 t=125 t=150 t=175 t=200 t=225 t=250 t=275 t=300
Exe
!
"
Mul
!
"
Ext
!
"
Pro
! Predicted image (64(cid:186)64, right camera)
"
Actual image (64(cid:186)64, right camera)
Joint
angle
Torque
Time step
Reaching Holding Lifting
Fig. 2. Hierarchical probabilistic inference during the repositioning task. (A)
Dynamics of probabilistic latent states in each module, along with predicted and
actual proprioceptive inputs. For the mean ğœ‡ and sigma ğœ (standard deviation) of
latent states, posteriors and priors are shown as solid and dashed lines,
respectively. For joint angles and torques, actual and predicted values are indicated
by solid and dashed lines, respectively. Different colors represent distinct latent
states, joint angles, or torques. For clarity of presentation, the time series of latent
states are plotted for 15 selected states. Exe: Executive module. Mul: Multimodal
associative module. Ext: Exteroceptive module. Pro: Proprioceptive module. (B)
Visual representations in each module, together with predicted and actual visual
inputs. For reference, an additional image emphasizing the contours of the actual
visual inputs is shown beneath them. Contributions of latent states were analyzed
through an ablation study. Brighter regions indicate greater influence of the
corresponding moduleâ€™s latent states on generating visual predictions. Results are
shown for 64Ã—64 images from the right camera.
Page 18 of 22
A B
t=25 t=50 t=75 t=100 t=125 t=150 t=175 t=200 t=225
Exe
!
"
Mul
!
"
Ext
!
"
Pro
!
Predicted image (64(cid:186)64, right camera)
"
Actual image (64(cid:186)64, right camera)
Joint
angle
Torque
Time step
Initial posture Reaching Wiping Releasing
Fig. 3. Hierarchical probabilistic inference during the wiping task. (A) Dynamics of
probabilistic latent states in each module, along with predicted and actual
proprioceptive inputs. For the mean ğœ‡ and sigma ğœ (standard deviation) of latent
states, posteriors and priors are shown as solid and dashed lines, respectively. For
joint angles and torques, actual and predicted values are indicated by solid and
dashed lines, respectively. Different colors represent distinct latent states, joint
angles, or torques. For clarity of presentation, the time series of latent states are
plotted for 15 selected states. Exe: Executive module. Mul: Multimodal associative
module. Ext: Exteroceptive module. Pro: Proprioceptive module. (B) Visual
representations in each module, together with predicted and actual visual inputs.
For reference, an additional image emphasizing the contours of the actual visual
inputs is shown beneath them. Contributions of latent states were analyzed through
an ablation study. Brighter regions indicate greater influence of the corresponding
moduleâ€™s latent states on generating visual predictions. Results are shown for
64Ã—64 images from the right camera.
Page 19 of 22
Network module
Fig. 4. Estimated uncertainty and its temporal variability in repositioning and wiping
tasks. (A) Mean levels of prior sigma estimated in each module. A paired t-test
indicated that the difference in mean prior sigma across all modules between the
repositioning and wiping tasks was not significant (ğ‘¡(4) = 1.67,ğ‘ = 0.17). (B)
Temporal changes in prior sigma, reflecting variability in uncertainty across time.
A paired t-test showed that the change in prior sigma averaged across all modules
was significantly smaller in the repositioning task than in the wiping task (ğ‘¡(4) =
âˆ’8.15,ğ‘ = 0.0012). For both (A) and (B), values were first averaged across all
time steps and latent states within each trial, and then averaged across 5 trials, 3
test sequences, and 5 independently trained networks for each task. Error bars
represent standard errors across the 5 networks. Exe: Executive module. Mul:
Multimodal associative module. Ext: Exteroceptive module. Pro: Proprioceptive
module. All: All modules. Error bars represent standard errors across the 5
networks.
Page 20 of 22
amgis
roirp
fo
leveL
A
Repositioning
Wiping
Network module
amgis
roirp
ni
egnahC
B
A Repos59 Repos65 Repos71 Wip59 Wip65 Wip71
ï¼ˆt=300ï¼‰ ï¼ˆt=300ï¼‰ ï¼ˆt=300ï¼‰ ï¼ˆt=150ï¼‰ ï¼ˆt=150ï¼‰ ï¼ˆt=150ï¼‰
16(cid:186)16 image
ï¼ˆobservationï¼‰ B
16(cid:186)16 image
prediction
64(cid:186)64 image
ï¼ˆnot providedï¼‰
Resolution of provided images
64(cid:186)64 image
prediction
Fig. 5. Robustness under limited visual inputs. (A) Examples of observed and predicted
images at different resolutions. From top to bottom: observed 16Ã—16 image,
predicted 16Ã—16 image, non-provided 64Ã—64 image, and predicted 64Ã—64 image.
Results are shown for three bed-height conditions (590 mm, 650 mm, and 710
mm) in both tasks. (B) Effects of proprioceptive input on visual prediction errors
under different image-resolution conditions. Prediction errors were computed
across all image resolutions and averaged over 5 trials, 6 test sequences, and 5
independently trained networks, separately for conditions with and without
proprioceptive prediction error (PE). Error bars represent standard errors across the
5 networks. A two-way nonparametric repeated-measures ANOVA using the
aligned rank transform (ART-ANOVA) revealed significant main effects of image
resolution (ğ¹(2,20) = 105.57,ğ‘ < 0.001) and proprioceptive condition (with vs.
without proprioceptive PE) (ğ¹(1,20) = 46.09,ğ‘ < 0.001), as well as a significant
interaction between them (ğ¹(2,20) = 13.57,ğ‘ < 0.001). Post hoc pairwise
comparisons with Holm correction confirmed that prediction errors were
significantly lower in the presence of proprioceptive input than in its absence (ğ‘ <
0.001). A significant simple effect of proprioceptive condition was also observed
when only 16Ã—16 images were provided (ğ‘ = 0.034).
Page 21 of 22
rorre
noitciderp
lausiV
With proprioceptive PE
Without proprioceptive PE
A
Balance of learning data
Fig. 6. Interference between tasks in learning. (A) Prediction errors for the
repositioning task under different data-balance conditions. The number of learning
sequences for repositioning was fixed at 9, while the number of wiping sequences
was varied. A one-way aligned rank transform ANOVA (ART-ANOVA) reported
no significant differences in prediction error among data conditions (ğ¹(3,16) =
0.62,ğ‘ = 0.61). (B) Prediction errors for the wiping task under different data-
balance conditions. The number of learning sequences for wiping was fixed at 9,
while the number of repositioning sequences was varied. A one-way ART-
ANOVA indicated a significant effect of data condition on prediction error
(ğ¹(3,16) = 19.16,ğ‘ < 0.001). Post hoc multiple comparisons using the Holm
correction showed that the prediction error for the wiping task was significantly
lower when the number of repositioning sequences was 0 than when it was 6 (ğ‘ <
0.001) or 9 (ğ‘ < 0.001). In addition, prediction error was significantly lower
when the number of repositioning sequences was 3 than when it was 6 (ğ‘ =
0.0033) or 9 (ğ‘ = 0.0033). For both (A) and (B), values are averaged across 5
trials, 3 test sequences, and 5 independently trained networks for each learning
condition. Error bars indicate standard errors across the 5 networks.
Page 22 of 22
rof
rorre
noitciderP
ksat
gninoitisoper
Balance of learning data
rof
rorre
noitciderP
ksat
gnipiw
B

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Scalable predictive processing framework for multitask caregiving robots"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   âŒ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   âœ… GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   âŒ BAD: Repeating the same claim 3+ times with slight variations
   âœ… GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
