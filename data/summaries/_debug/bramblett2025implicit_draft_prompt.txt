=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Implicit Coordination using Active Epistemic Inference for Multi-Robot Systems
Citation Key: bramblett2025implicit
Authors: Lauren Bramblett, Jonathan Reasoner, Nicola Bezzo

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Key Terms: systems, system, implicit, communication, approach, belief, significant, reasoning, robots, robot

=== FULL PAPER TEXT ===

Implicit Coordination using Active Epistemic Inference
for Multi-Robot Systems
Lauren Bramblett, Jonathan Reasoner, and Nicola Bezzo
Abstract—A Multi-robot system (MRS) provides significant
advantages for intricate tasks such as environmental moni-
toring, underwater inspections, and space missions. However,
addressingpotentialcommunicationfailuresorthelackofcom-
municationinfrastructureinthesefieldsremainsachallenge.A
significant portion of MRS research presumes that the system
can maintain communication with proximity constraints, but
this approach does not solve situations where communication
is either non-existent, unreliable, or poses a security risk.
Some approaches tackle this issue using predictions about
other robots while not communicating, but these methods
generally only permit agents to utilize first-order reasoning,
which involves reasoning based purely on their own obser-
vations. In contrast, to deal with this problem, our proposed
framework utilizes Theory of Mind (ToM), employing higher-
order reasoning by shifting a robot’s perspective to reason
Fig. 1. Pictorial representation of the problem presented in the paper.
about a belief of others observations. Our approach has two
The robots are unable to explicitly communicate their beliefs and must
main phases: i) an efficient runtime plan adaptation using
convey their intentions through sensorimotor communication. In the left
activeinferencetosignalintentionsandreasonaboutarobot’s frame, the red and blue robot are unable to converge to a correct belief
own belief and the beliefs of others in the system, and ii) a state using only first-order reasoning. In the right frame, the red and blue
hierarchicalepistemicplanningframeworktoiterativelyreason robotclearlydisplaytheirintentionsbyusinghigher-orderreasoningabout
aboutthecurrentMRSmissionstate.Theproposedframework theirobservations.
outperformsgreedyandfirst-orderreasoningapproachesandis
thus making exaggerated movements to communicate their validatedusingsimulationsandexperimentswithheterogeneous
robotic systems. intentions.Instandardmulti-robotmissions,thereareusually
no strategies in place for communication breakdowns, or
Note—Videos are provided in the supplementary material and
also at https://www.bezzorobotics.com/lb-smcs24. Code the strategies that do exist rely solely on each robot’s first-
will be available on the same website upon publication. orderunderstandingoftheenvironmentandsystem.Planning
actions socially requires a robot to infer the intentions
I. Introduction
and beliefs of other agents, empathizing to predict what
Multi-robot systems (MRS) have the potential to trans- other agents want and know about each other. The capacity
form current robotics applications, performing tasks more to reason about the perspective of another agent is the
effectively and efficiently than a single robot. Central to foundation of theory of mind (ToM) which enables the “I
MRS research is the idea that robots work together to know that you know that I know” paradigm without the
accomplishcommongoals.Theneedforcooperativeteaming need for explicit communication among actors. Formally,
is evident in numerous applications, such as search and as described in [1], there are different orders of reasoning
rescue missions, firefighting, and underwater exploration. possible: zero-order is a belief about oneself, first-order is
Effective collaboration requires robots to communicate and a belief about others, second-order is a belief about what
synchronize their actions, but difficulties often occur when othersbelieveaboutoneself,andthird-orderisabeliefabout
communication is restricted, disrupted, or presents a secu- what others believe about each other. By using a second
rity concern. Especially for a heterogeneous MRS where and higher order reasoning architecture, we can increase
different robots have different operating or sensing capa- the operational effectiveness of multi-robot systems during
bilities. Humans have an inherent ability to “see things disconnected operations, allowing robots to reason about
from another’s perspective” by understanding and sharing the capability of other robots and plan according to local
the beliefs of others without communicating. Imagine a observationsanddistributedbeliefs.Epistemicplanningisan
parent attempting to convey a message to a child solely approach that integrates each agent’s belief into a planning
through their movements. The parent might understand the paradigm, enabling them to plan based not only on their
child’s short attention span and limited observational skills, perceptions but also on what they believe other agents know
or intend to do [2].
Lauren Bramblett, Jonathan Reasoner, and Nicola Bezzo are with the Theory of mind and epistemic planning can enable robots
Departments of Systems and Information Engineering and Electrical and
to reason about the probable knowledge and intentions of
Computer Engineering, University of Virginia, Charlottesville, VA 22904,
USA.Email:{qbr5kx, vqh7rx, nb6be}@virginia.edu others. Our previous work shows that in environments with
5202
beF
1
]OR.sc[
2v70930.1052:viXra
limited communication and uncertain operating conditions, the framework for epistemic planning, active inference for
epistemicplanningcanenableanMRStoachieveexploration decision-making and task allocation in Section IV. Simula-
and task allocation missions [3], [4]. In conjunction, active tions and experiments validating our method are presented
inference can be used to compute belief-action pairs. Active in Sections V and VI, respectively. Finally, conclusions and
inference operates on the principle that all entities strive to future work are discussed in Section VII.
minimize variational free energy. This results in straightfor-
II. RelatedWork
ward update rules for actions, perceptions, policy choices,
learning processes, and the representation of uncertainty, TheoryofMind(ToM)andepistemicplanningareclosely
which can extend to multi-agent processes [5]. related concepts in artificial intelligence and cognitive sci-
In this work, we focus on the following question: How ence [7]. ToM refers to the ability of an agent to attribute
can we ensure cooperative and efficient behavior for multi- mental states, such as beliefs, desires, and intentions to
robottaskswhenrobotscannotexplicitlycommunicate?Our others, enabling it to predict and interpret their actions [8]
proposed solution has two main components: 1) an efficient while epistemic planning is a type of automated planning in
onlineplanningmechanismthatleveragesactiveinferenceto artificial intelligence that deals with knowledge and beliefs
signaltootherstheirownknowledgeandintentionsbasedon ofagents[2].IntegratingToMintoepistemicplanningallows
the current epistemic state and probable goals to accomplish agentstoanticipateandrespondtotheknowledgeandbeliefs
the mission, and 2) heirarchical epistemic planning that of other agents, leading to more effective coordination and
leverages our recent research [6] which allows the robots decision-making in multi-agent systems. The authors in [9]
to reason about the system goals and adapt its belief about and [10] show that nested beliefs and reasoning in multi-
the system at runtime. agent planning can better equip agents to work in teams and
ConsidertheexampleinFig.1,wherethreerobotscannot showthatthisintegrationiscrucialforapplicationsrequiring
communicate and the mission is defined by two tasks, one sophisticated interaction and collaboration among multiple
requiring two robots and the other requiring only one robot. intelligent agents. This paper characterizes ToM for a multi-
Duringtheoperation,eachrobotmaintainsabeliefaboutthe robot system similar to [11] in that we employ epistemic
system defined by the likelihood that any robot is moving planning as a logical mechanism to account for the system’s
toward one of the two tasks in the environment. In the left knowledgeandbeliefs.Epistemicplanningcanadopttheper-
frame, each robot uses only its first-order observations, rea- spectives of other robots within the system, reasoning about
soning about each robot based only on its own observations. their knowledge and uncertainties, thereby preventing first-
In the right frame, each robot uses higher-order reasoning order reasoning deadlock. Previous multi-agent planners,
to not only infer other robots’ goals based on its own such as in [12]–[15] typically maintain separate knowledge
observationsbutalsobyempathizingwithhowotherrobots’ bases for each agent in the scenario. However, these static
beliefs would change based on its own actions and their first-order representations lack the expressiveness required
subsequent observations. We note that by using only first- for more complex scenarios involving nested perspective-
order reasoning, the red and blue robots cannot determine taking[16]andwhentheenvironmentorthesystemchanges
the goals of the other, and they converge to an incorrect over time.
belief.Usinghigher-orderreasoning,theredandbluerobots Dynamic Epistemic Logic (DEL) extends the concepts of
bothaccountfortheobservationoftheotherindecidingtheir epistemic planning and ToM by providing a formal logical
next actions and clearly indicate their intent and beliefs. In frameworktoreasonaboutchangesinknowledgeandbeliefs
this way, the MRS is able to converge to a correct belief overtime[17].Whileepistemicplanningfocusesondevising
state using only local observations. In this work, we utilize plans that consider the current epistemic states of agents,
up to the third level of reasoning to allow robots to abstract DEL specifically addresses how these states evolve through
the perceptions of other agents and more accurately derive actions and observations [18]. DEL, as a result, is a more
the beliefs of the system. flexible representation of the dynamics of knowledge and
Thecontributionofthisworkistwo-fold:i)ahigher-order belief, enabling adaptive planning in scenarios where agents
active inference framework for multi-robot task allocation must continuously update their understanding of the world
withoutcommunication,andii)anepistemicplanningframe- and each other’s mental states [19]. In multi-robot systems,
work for belief updates and iterative task allocation. To the DEL allows each robot in the MRS to reason and plan
bestofourknowledge,thisisthefirstpapercombiningepis- using its beliefs of other robots’ beliefs of the system
temiclogicandactiveinferencewithruntimetaskallocation while disconnected, updating its beliefs and policy if new
adaptationsandnocommunication.Weshowthatourhigher- actions or events are observed, and routing to communicate
order reasoning method outperforms traditional greedy task when necessary [17]. DEL has recently been integrated
allocation and first-order active inference algorithms to allo- into robotics applications. The method presented in [20]
cate tasks in an environment without communication. recreates the Sally-Anne psychological test for human-robot
Therestofthepaperisorganizedasfollows:inSectionII, interactions. Typical DEL-based multi-agent research uses
we provide an overview of current research in multi-robot epistemic planning for game theory-based policies [21].
task allocation, epistemic planning, and active inference. In Active inference provides a probabilistic model for agents
Section III, we formally define the problem followed by to make decisions and update beliefs by minimizing uncer-
tainty and surprise [22]. Using Bayesian inference, agents where the function h maps a robot i’s position to its
i
predict sensory inputs and select actions aligned with their observation y(t) given noise ζ.
i
goals, incorporating their own and others’ knowledge and In addition, we let the set G ⊆G represent the subset of
i
beliefs. This inherently involves Theory of Mind and DEL, alltasksinGthatrobotibelievesisassignedtotheMRS.All
as agents model and anticipate others’ mental states for possible combinations of these assignments are represented
effective interaction [23]. Integrating active inference with bythepowersetP(G)andvalidconfigurationsforthemulti-
ToM and epistemic planning enables sophisticated planning robot mission are denoted as G⊆P(G).
and decision-making, allowing agents to refine their un- For ease of discussion, in this work, we assume that all
derstanding and actions for optimal outcomes in complex, robots know the location of all tasks G present and the
multi-agent environments such as in [24]. This connection sensor configuration ω of each robot i in the system. To
i
is essential for developing intelligent systems capable of methodically allocate tasks to the multi-robot system, we
adaptive and cooperative behavior in uncertain and dynamic first assign a subset of all tasks to the multi-robot system
settings. Previous work on active inference in [25] utilized for time τ to decrease the complexity of assigning a distinct
Policy Belief Learning to improve conveyance of intent set of tasks to each robot. We represent this problem as a
through action, paired with a reward system, which incen- bi-level resource optimization problem [31]:
tivized actions that improve the overall understanding of the Problem 1: Upper-Level – Epistemic System Tasking:
operational environment. Additionally, [26] evaluated how DesignanepistemicstrategyforanMRStoallocateasubset
activeinferencecouldbeperformedinrobotteamswhichcan oftasksfromVtothesystematanygiventimet,accounting
typically communicate and how the lack of communication for uncertainty in local observations and considering that
can be exploited to better decrease uncertainty in an agent’s robots are unable to communicate.
beliefs of the world. Previous work was also performed in Problem 2: Lower-Level – Intent Signaling for Subtask
both leader-follower and leaderless models in [5] of which Assignment: Given the subset of tasks to complete from
the similar approach to their leaderless work was used the upper-level optimization, formulate a policy for effective
in our approach. Several works have incorporated realistic intent signaling for each robot and efficient task completion.
applications, including the authors in [27] who use active The mathematical formulation of Problem 1 and 2 can be
inference and behavior trees were shown to improve the expressed as:
robustness of plans for a mobile manipulator. Additionally,
(cid:88)|G| (cid:88)|R| (cid:88)G
authors in [28] showed that perception, path generation, min c x + c′ z (3)
τ τ iτ iτ
localization,andmappingnaturallyemergefromusingactive x,z τ=1 i=1 τ=1
inference and minimizing free energy. (cid:88)|G|
Recently, a new perspective on the theory of mind (ToM) subject to x ≤ K (4)
τ
called the Bayesian mind has attracted attention, being sug- τ=1
gested as a feedforward model for decision-making [29]. In x ∈{0,1} ∀τ∈{1,...,|G|}, (5)
τ
thiswork,weextendtheconceptoftheBayesianmindusing (cid:88)|R| (cid:88)|G|
active inference as in [30] and [5], and incorporate dynamic z ∈argmin b z′ (6)
iτ iτ iτ
epistemic tasking with higher-order reasoning. We demon- z′ iτ i=1 τ=1
strate that while first-order reasoning can yield good results, The variable c represents the cost associated with selecting
i
higher-order reasoning provides more robust outcomes even task τ represented by the binary variable x . c′ represents
τ iτ
in the absence of communication and presence of uncertain the cost associated with assigning robot i to task τ where
sensor measurements. the assignment for robot i at time τ is denoted by the binary
III. ProblemFormulation variablez iτ .b iτ representsthecostinthelower-levelproblem
for assigning robot i to task τ. K is a constant representing
ConsideraMRSofnrobotsinthesetR.Weletx denote
i
the maximum number of tasks that can be selected from the
the state variable of the robot i that evolves according to
setG.Theupper-levelobjectivefunctionminimizesthetotal
general dynamics at time t such that:
costofselectingtasksandassigningrobots,whilethelower-
x˙ i (t)=f i (x i (t),u i (t),ν i ) (1) levelproblemensurestheoptimalassignmentofrobotstothe
whereu
i
(t)∈Rdu andthevariableν
i
∈Rdν denotethecontrol selectedsubsetoftasks.Theconstraintsensurethateachtask
is assigned to exactly one robot if selected from the set G
input and zero-mean Gaussian process uncertainty, respec-
and that each robot is assigned to at most one task.
tively. The function f represents the stochastic dynamics of
i
robotigivenacontrolinputandprocessuncertainty.Wealso IV. Approach
assume that all robots are equipped with sensors that allow
Our proposed framework is designed for a task allocation
them to ascertain certain measurements from other robots in
problem (TAP) in which robots are unable to communicate
the system. A robot’s continuous observation y(t) at time t
i explicit information, but must instead signal their intent and
depends on its own position x(t) and sensor configuration
i infer other robot’s intent in the system. When solving the
ω (e.g., camera, lidar) such that:
i TAP, robots must update their beliefs about the state of the
y(t)=h(x(t),ω)+ζ. (2) system and also empathize with what others might believe.
i i i i
Torealizethesechanges,eachrobotobservestheobservable thefunctionsinΨ.¬ϕandϕ∧ϕarepropositionsthatcanbe
statesofeachrobotandreasonsaboutthesysteminahierar- negatedandformlogicalconjunctions,whereϕ∈ APand AP
chicalmanner.Initially,eachrobotevaluatesthesubtasksthat is a finite set of atomic propositions. We denote the set of
needtobeallocatedbythesystemattimet,usingepistemic possible worlds by W, where each world w ∈ W represents
reasoning to converge a common belief about the allocation a distinct state of the system where each robot is assigned
of tasks within the multi-robot system. Subsequently, each to a subset of tasks. Kϕ and Bϕ are interpreted as “robot i
i i
robot examines the evidence related to the movements of knows ϕ” and “robot i believes ϕ”, respectively. Practically,
all robots in the system and ultimately signals its own we consider ϕ to be the generic assignment of a robot to a
intent,employingactiveinferencetoreducethesystem’sfree task.
energy. The diagram in Fig. 2 illustrates this decentralized We represent the global epistemic state as s =
framework, where robots first gather observations about the (W,(R) ,L,w) where L : W → AP assigns a label to each
i i∈R
MRS.Theseobservationsarethenfilteredbasedonprevious world defined by its true propositions and the accessibility
measurements before generating and assessing allocations relation R represents the uncertainty of robot i at run-time.
i
for the MRS to execute at time t. Upon generating these An accessibility relation R for robot i defines which worlds
i
solutions, the resulting perspective of robot i is denoted as are indistinguishable to i; that is, R(w,v) holds if robot i
i
s andrepresentsthepossibleassignmentsofrobotstotasks. cannot distinguish between worlds w and v. A robot may
i
not be able to distinguish worlds if the evidence associated
with both worlds is equivalent or similar according to the
uncertainty associated with our observations from (2).
Sequences of relations are used to represent higher-order
knowledge. For example, the statement “robot i knows that
robot j knows ϕ” is true in s if and only if s|= KK ϕ. This
i j
condition is satisfied when ϕ is true in all worlds accessible
from w through the composite relation of R and R . The
i j
perspective of robot i on the system state is notated as s.
i
Dynamicepistemiclogicisexpandedfromepistemiclogic
Fig.2. Diagramoftheproposedapproach through action models [20]. These models affect a robot’s
perception of an event and influence its set of reachable
In the next sections, we will initially concentrate on the
worlds, R. A robot may plan to reduce the run-time uncer-
epistemic framing of the problem and how a robot can use i
taintybytakingactions.Anactionatransformsaworldwto
higher-order reasoning to update its beliefs. We then show
aworldw′ suchthatifw|=[a]ϕ,thenϕholdsintheresultant
how active inference can be used to model how to measure
world w′. Robots generate plans π that are sequences of
the results of deeper reasoning to enhance the accuracy i
actions π =⟨a ,a ,...,a ⟩ leading from an initial state s0
of non-communicative robots. We will also explore scal- i i1 i2 ik i
toagoalstate s∗ ∈Γ.HereΓ representsthesetofepistemic
able runtime tools for employing this belief-based inference i i i
goal states for the robot which are defined as the possible
methodbeforediscussingtheonlineepistemicdistributionof
goal configurations the system aims to achieve.
subtaskstoenablemoreeffectivereasoningduringoperation.
A. Epistemic Structures for Multi-Robot Reasoning π =⟨a ,a ,...,a ⟩ (7)
i i1 i2 ip
In this section, we frame the problem using dynamic
such that
epistemic logic (DEL) and epistemic planning to enable
multi-robot goal selection in environments where direct s0 i −− a →i1 s i1 −− a →i2 ···−− a →ip s∗ i ∈Γ i . (8)
communication between robots is not feasible. To overcome
Coordinationamongrobotsisachievedthroughcontinuous
thechallengesofcoordinationinthiscommunication-limited
observation and nested belief updates:
environment, we use dynamic epistemic logic (DEL) and
epistemic planningwithin eachagent inour system.Our ap- B ←y (9)
i i
proach leverages DEL to model the knowledge, beliefs, and
intentionsofrobots,integratingthiswithplanningalgorithms which represents what robot i believes about robot j’s goal
for goal selection and sequence optimization. To limit the assignments,notingthatB i representsroboti’snestedbeliefs
possible combinations of execution policies for each robot about the system such that B i ϕ |= B i ...B r ϕ where the
to infer about the MRS, we use epistemic logic and allow subscript denotes the nested belief of the rth robot from the
the robots to reason about the system state. For this work, perspectiveofroboti.Theplanningprocessincorporatesthe
theepistemiclanguage,L(Ψ,AP,R)isobtainedasfollowsin robots’ knowledge and beliefs:
Backus-Naur form [32]:
π =Plan(s0,Γ,B) (10)
i i i i
ϕ(cid:70) H(η) | ϕ∧ϕ | ¬ϕ | Kϕ | Bϕ
i i
This allows robots to infer each other’s goals:
where i ∈ R. H ∈ Ψ with Ψ being a set of functions that
describe the system state. η generally denotes arguments for ∀i, j∈R, B[y ](Γ ) (11)
i j j
If robot i observes that robot j is moving towards goal The generative model of any ith robot in our approach
g , it updates its beliefs to B(Γ = g ) where if is mathematically defined similar to the formalism first
m i j m
B(j is moving towards g ) then B(Γ = g ). Robots up- introducedby[5];however,weaugmentthismodelwithcon-
i m i j m
date their beliefs based on actions, a, and observations, y, tinuous states, observations, and actions represented in [35].
i
using DEL: We let the dynamics model for the generative model be
represented by (1), influenced by control inputs u(t) and
B ←a or B ←y (12) i
i i i
processnoise.Next,weformulatetheobservationlikelihood
for a robot i’s observations y(t) at time t as:
This framework guarantees both soundness and complete- i
ness. The inference rules accurately represent the environ- P(y(t)|x(t),ω)∼N(y(t);h(x(t),ω),Σ) (13)
i i i i i i i i
ment’sstate,and withenoughobservations,thegoals canbe
inferred correctly. Combining DEL with epistemic planning where h maps the robot i’s position x(t) to its observation
i i
offersaneffectivestrategyformanagingmulti-robotsystems y(t)andΣ denotesthecovariancematrixthatrepresentsthe
i i
without the need for direct communication. However, refin- effect of noise on the observation.
ing these beliefs and developing a logical policy requires a The two main components of active inference are belief
probabilistic method that can manage complex reasoning. In updates and active selection. In our application, we note
the subsequent section, we merge the epistemic framework that each robot maintains a belief over the possible goal
into the active inference model, utilizing nested beliefs and configurations for the multi-robot system. Depending on
data gathering to signal a robot’s intentions and infer the the application, these goal configurations should represent
goals of other robots. possible states that will accomplish a pre-defined mission.
Each robot maintains this belief about the possible goal
B. Active Inference for Decision Making configurations that the system is performing at time t. We
represent this posterior belief as:
Active inference robots perform perception and action
planning by minimizing variational free energy. To mini- Q(G|y(t),ω) (14)
i i
mize free energy, these robots utilize a generative model
that depicts the joint probability of the stochastic variables where G is a subset of possible goal configurations that
responsible for their perceptions [33]. Fig. 3 shows our would accomplish the task allocation problem in (6). We
generative model for this framework where a robot receives use Bayes’ rule to update the prior belief P(G) based on the
observationsy i ∈O, ∀i∈R.Observationsarethenprocessed likelihood P(y i (t)|G,ω i ) derived from the observations and
through generalized Bayesian filtering [34], leading to an sensor configurations of robot i. This is modeled as follows:
update in the beliefs of the system. Each robot can then
Q(G|y(t),ω)∝ P(y(t)|G,ω)P(G). (15)
utilizetheserevisedbeliefstoforecastthesystem’sbehavior. i i i i
This process results in a robot i creating a set of policies for These posterior updates are then used in the active selection
all robots, but only able to control its own policy. However, component; however, we can only approximate the posterior
these actionsaffect theenvironment and,in turn,the state of given that we do not have direct access to the true system
thesystemasperceivedbyotherrobots.Thiscyclecontinues, state. Therefore, we approximate the posterior q(G) as fol-
enabling the robots to infer the intentions of other robots lows:
and to use their own control policies to influence the beliefs q(G)∝L(G|y(t),Ω)P(G) (16)
i i i
of others. Active inference is distinct from perception or
learning because it involves an active process driven by the where robot i’s approximate posterior q(G) is proportional
goalofproducingobservationsthatareminimallysurprising. to the product of the likelihood function L(y(t),Ω,G) and
i i
prior P(G). The likelihood function is discussed further in
the following section, but first we define how we use the
belief update for the free energy calculation.
By employing active inference, a robot executes a
perception-policy loop through the application of the afore-
mentionedmatricestohiddenstatesandobservations.Inour
scenario, perception involves estimating which of the valid
goal configurations the system is achieving. At the start of
any mission, the MRS might have access to a prior over
goal configurations providing each robot with an initial state
estimate, which is then refined by subsequent observations.
Foranticipatedfuturestates,therobotdeducesthecurrent
hiddengoalconfigurationGtakingintoaccounttheexpected
Fig.3. Overviewofgenerativemodelandprocessusedinourmulti-robot transitions defined by the control u and general dynamics
application. We assume that the state is hidden and the robot is only able t
in (1). Active inference utilizes an approximate posterior for
toobserveusingtheirownon-boardsensingcapability(e.g.,depthsensors,
cameras). hidden states and control policies. As demonstrated by the
authors in [36], the distribution is most accurately approx- inwhichrobotsrelyontheirsharedprobabilisticunderstand-
imated by minimizing the variational free energy (VFE), ing of the mission to achieve complex objectives.
which is defined at time t as: In the subsequent section, we discuss the particular like-
lihood function from (16) used to revise a robot’s belief
F(x i (t),u i (t),G,y i (t),ω i )= regarding the hidden states of the system. By employing
H[q(G)]+D [q(G)∥ P(x(t)|y(t),ω)] (17) this function, robots can methodically gather evidence and
KL i i i
deduce the actual state of the system, or, in this work, the
where H is the model uncertainty computed using Shan- correct goal configuration that assigns a goal to each robot.
non entropy and D denotes the Kullback-Leibler (KL)
KL
C. Higher-Order Evidence-Based Reasoning
divergence. This can be further generalized to expected free
energy for a policy π: As previously mentioned in (16), given a set of valid
i
goal configurations G, a likelihood function is an important
E [F]=E [H[q(G)]]+ function to update a robot’s approximate posterior belief
πi πi
E [D [q(G)∥ P(x(t)|y(t),ω)]] (18) about the hidden states Q(G). A factor in interpreting likeli-
πi KL i i i
hood based on observations is salience. Salience describes
Theexpectedfreeenergy(EFE)isametricthatintegratesthe how prominent or emotionally striking something is. In
entropyofthevariationaldistributionQ(G)withtheexpected neuroscience,salienceisanattentionalmechanismthathelps
log-likelihood of the generative model. By minimizing F, organisms learn and survive by allowing them to focus on
robotsadjusttheirbeliefstobetterapproximatethetruepos- the most relevant sensory data. In our application, salience
terior distribution, balancing model complexity with align- is the evidence that a robot is aligned with a goal g j ∈ G.
ment to observed data. The EFE comprises two components The set G is different in that G is the set of all goals in an
that assess the quality of the policy. The first component is environment, but G is the valid goal configurations for the
the expected Kullbeck-Leibler divergence, which promotes multi-robot mission such that G⊆P(G) from G.
low-riskpoliciesbyminimizingthediscrepancybetweenthe We note that previous salience functions used in active
approximate posterior and the desired outcome. The second inferenceandroboticsliteraturesuchasin[5],[37],typically
component is the expected entropy of the posterior over only use up to first-order reasoning to define their evidence
hiddenstates,representingtheepistemicaspectofthequality and subsequent posterior belief. We begin our formulation
score and encouraging policies that reduce uncertainty in generally with the salience value defined as:
future outcomes. (cid:32) (cid:33)
1
In this paper, we conceptualize the beliefs over hidden e( i k) ←υ i (k)(y i ,Ω,G)=exp − η h( i k)(y i ,Ω,G) (19)
statesQ(G)foramulti-robotsystem(MRS)asaprobabilistic
framework that represents the likelihood of various goal where the array e(k) ∈ R|G| is the mapping of observations
i
configurations g˜ ∈G being the true state of the system. This to evidence for goals in G from the perspective of robot
approachallowsustoencodeadiscretearrayofprobabilities i and given the sensor configurations of all robots in the
that correspond to the different ways in which the robots systemΩ.Thesuperscriptkdenotesthelevelofreasoningat
mightbearrangedtoachievetheirrespectiveobjectives,par- whichtherobotisevaluatingitsobservations.Sincearobot’s
ticularly in scenarios where direct communication between observations are independent of other robot’s observations,
robots is not feasible. wecanaggregatetheevidenceassociatedwitheachroboti’s
To illustrate, consider the scenario depicted in Fig. 1. In perspective of other robots. For example, evidence can be
this example, there are two distinct goals: one that requires evaluated as metrics such as distance to a goal or relative
the collaboration of two robots and the other that can be ac- angles to a goal as shown in Fig. 4(a) and Fig. 4(b), re-
complishedbyasinglerobot.Thesetofvalidconfigurations spectively. Generally, the following function maps a positive
in this scenario is given by G = [(0,1,1),(1,0,1),(1,1,0)]. evidence value to each goal configuration in G such that:
Each tuple in G represents a possible state of the system,
h(k)(y,Ω,G)=
(cid:88)(cid:104)
h(k)(y,Ω,g )
(cid:105)|G|
(20)
where the elements of the tuple indicate the allocation of i i ij,a i j j=1
robots to each goal. For example, the configuration (0,1,1)
a∈R
k
implies that one robot is assigned to the first goal, while whereRk denotesthesubsetofrobotsthatareconsideredfor
i
the other two are assigned to the second goal. Thus, the kth-orderreasoningfromtheperspectiveofroboti,thevalue
belief distribution Q(G) captures the probability that any of h (ω,Ω,g ) is positive for a goal g ∈G, and robot r that
ij,r i j j
theseconfigurationsreflectsthetruestateofthesystem.This indicates if a robot is aligned with a goal g . The notation
j
probabilistic representation serves as a critical mechanism |G| denotes the number of elements in the set G.
for decision-making, guiding robots in selecting actions that One can observe that in a heterogeneous system, a robot
alignwiththemostprobableconfigurations.Indoingso,the maynotalwaysbeabletoconsiderotherrobots’perspectives
systemcandynamicallyadapttouncertaintiesandvariations if the perspective is not measurable. As such, evidence
in the environment, optimizing the overall mission outcome h(k)(y,Ω,G) = 0 when a robot i is unable to compute the
ij,r i
despite the absence of explicit communication between evidence from the perspective of robot r (ω ≻ ω). For
r i
robots. This method enables a form of implicit coordination example, consider Fig. 4, a vehicle that might only be able
to measure angles cannot abstract the depth information that eachindependentlikelihoodhasvariance(σ2).Thecombined
i
other vehicles are able to observe, but angles are able to be variance in the joint likelihood can be lower due to the
abstractedfromdepthinformation.Thisinformationisrepre- aggregation of information. In addition, joint likelihood can
sentedinthesetRk ∈R and,asaresult,nonewinformation better manage the bias-variance tradeoff. While independent
i
is mapped by robot i from robot r’s perspective and is not likelihoods may lead to higher variance due to lack of
abletoinformthevariationaldistributionfornon-measurable dependency modeling, a joint likelihood balances the bias
perspectives for second- and third-order reasoning. introduced by modeling dependencies and the variance re-
ductionduetojointestimation.Thejointlikelihoodprovides
a more accurate expectation for each step, leading to more
stable estimates.
Lemma 1: In a multi-robot system, incorporating higher-
order reasoning (second- and third-order) reduces the vari-
ance of the robots’ belief distributions compared to first-
order reasoning by leveraging joint likelihoods that account
for dependencies among robots, assuming that the errors in
the observations from other robots are bounded and have a
mean of zero.
(a) ZeroandFirstOrder (b) SecondandThirdOrder Proof: We aim to prove this result by direct proof.
Specifically, we will show that higher-order reasoning leads
Fig.4. Pictorialdepictionofobservationmappingtoevidenceanddepth
ofreasoning. to a reduction in the variance of the belief distributions over
goal configurations compared to first-order reasoning, under
Higher-order reasoning can be iteratively aggregated to theassumptionthat theerrorsintheobservations fromother
form a comprehensive joint probability distribution. To cal- robots are bounded and have a mean of zero.
culate the likelihood of a robot being aligned with any Consideramulti-robotsystemwhereeachrobotievaluates
particular goal in G, we let the probability distribution for the evidence e(1) for each goal g ∈ G based on its own
i
kth-order reasoning be represented as: observations, leading to a first-order probability distribution:
 
P i (G)=σ  (cid:88) e( i k)  (21) P(1)(g)= exp(e( i 1)(g)) . (25)
k i (cid:80) exp(e(1)(g′))
g′∈G i
where e(k) represents the evidence gathered using (19) and
i This first-order distribution, P(1)(g), has a variance denoted
σ is representative of the softmax function. The belief over i
by σ2. However, this distribution does not incorporate the
the goal configurations specified by G can be inferred using i
dependencies or the information that might be inferred from
a joint probability distribution of the result from (21). We
the perspectives of other robots.
formulate the joint probability distribution we first initialize
Assumethattheobservationsmadebyarobotiaresubject
as:
PJ(g)=1 ∀g∈Gn (22) to an error term ζ from (2) and is proportional to the error
i of the salience measure for each pair of robots and goals,
whereGnrepresentsallthepossiblecombinationsofnrobots ϵ (g), which is bounded and has a mean of zero:
j
assigned to |G| tasks. Then we calculate the joint probability
distribution for all possible goal configurations as ϵ i (g)∼N(0,σ2 ϵ ) and |ϵ i (g)|≤ϵ max for all g∈G. (26)
(cid:89)n
Whenhigher-orderreasoningisintroduced,robotiadjusts
PJ(g˜)= PJ(g ,...,g )= P(g) (23)
i i 1 n i i its belief by considering the evidence from other robots,
i=1
which includes their respective error terms. For instance, in
where P i (g i ) ∈ P i (G) and g˜ ∈ G. Additionally, the goals second-orderreasoning,theprobabilitydistributionbecomes:
g ,...,g represents the allocation of a goal in the set G to
e c o a o 1 f c n v h fi a g l r i u o d r b a n c o t o i t o . n n fi T s g h u i e n ra r G t e i s o a u n n l s t d t i o w n e f ( o 2 e r 3 x m ) tra g a c iv d t e i a s s n tr d t i h b n e u o ti p r o m r n o a : b l a iz b e ili t t h y e f s o u r bs a e ll t P( i 2)(g)= (cid:80) g′∈ e G x e p x (cid:16) p e (cid:16) ( i 1 e ) ( i ( 1 g )( ) g + ′) (cid:80) + j (cid:80) (cid:44)i E j(cid:44) [ i P E ( j [ 1 P )( ( j g 1) ) ( + g′) ϵ j + (g ϵ ) j ] ( (cid:17) g′)] (cid:17).
(27)
PJ(g˜) Because the errors ϵ (g) are bounded and have a mean
L i (G|y i (t),Ω)= (cid:80) i PJ(g˜′) ∀g˜ ∈G (24) of zero, their contributi j on to the variance of P(2)(g) will
g˜′∈G i i
average out as more robots’ beliefs are considered. This
where the likelihood L (G|y(t),Ω) can be used to update
i i process reduces the overall variance in the joint probability
the prior from (16).
distribution, which can be expressed as:
Thejointlikelihoodassociatedwithhigher-orderreasoning
(cid:89)
can increase the robustness of the overall system because of PJ(g˜)= P(g). (28)
i i
theintegrationofinformationacrossmultiplelayers.Suppose
g∈g˜
To quantify the reduction in variance, we examine the planningforreducingthegoalconfigurationspossibleineach
Kullback-Leibler (KL) divergence between the robots’ joint iteration.
beliefs and their expected beliefs:
D. Epistemic Allocation for Dimensionality Reduction
(cid:16) (cid:17)
D KL P i J(g˜)∥E[P i J(g˜)] . (29) Despite the benefits of salience in managing information
overload, the challenge of dimensionality remains. To ad-
Since the errors in observations are bounded and mean-
dress this, we use epistemic allocation, which leverages the
zero, the KL divergence decreases as the robots’ beliefs
principles of epistemic logic to reduce the set of possible
become more aligned, indicating a convergence towards a
configurations in G, thereby enhancing the system’s scal-
common understanding. This reduced divergence leads to a
ability and robustness. Epistemic planning involves robots
decrease in the variance of the belief distribution:
makingdecisionsbasedontheirknowledgeandbeliefs,with
 
σ2
joint
=Var
 (cid:89)
P
i
(g)

<σ2
first-order
. (30)
t
s
h
h
e
are
a
d
im
en
o
v
f
iro
re
n
d
m
u
e
c
n
in
t.
g
In
u
t
n
h
c
i
e
s
r
c
ta
o
i
n
n
t
t
e
y
xt
a
,
n
th
d
e
a
fu
ch
n
i
c
e
t
v
io
in
n
g
fo
g
r
o
c
a
h
l
o
s
o
i
s
n
ing
a
g∈g˜
subset goals plays a crucial role. Let B(g ) represent robot
i j
Finally, the reduced variance is reflected in the decreased i’s belief about goal g . The belief update mechanism uses a
j
entropy of the joint belief distribution: softmax function over the evidence e between robot i and
ij
(cid:88) goal j, formalized as:
H(PJ(g˜))=− PJ(g˜)logPJ(g˜), (31)
i i i
 
where H(PJ(g˜)) ≤ H(P(1)(g˜
g˜
)
∈
)
G
. Lower entropy indicates that
B
i
(g
j
)=σ
 (cid:88)
e(
i
k
j
)

(32)
i i k
therobots’beliefsaremorecertainandlessdispersed,which
Thisupdaterepresentstheprobabilitythatrobotibelieves
corresponds to reduced variance.
goal g is achievable, given the evidence. With higher-order
In summary, by incorporating higher-order reasoning and j
reasoning, robots can collectively maximize the diversity of
considering bounded, mean-zero errors in observations from
evidence. This involves each robot considering not just their
other robots, the system reduces the KL divergence and
perspective but the perspectives of all robots to select goals
variance in the belief distribution, leading to more stable,
that maximize the collective knowledge. Formally, the set of
accuratepredictions.Thisprovesthathigher-orderreasoning
chosen goals G can be described as:
is beneficial for reducing uncertainty and improving the c
overall performance of the multi-robot system.
G ={g |argmaxB(g ),∀i} (33)
Higher-order reasoning models, even when based on first- c j j i j
order measurements, reduce the overall variance of param-
This selection ensures that the chosen goals maximize the
eter estimates by capturing dependencies and interactions
diversity and coverage of evidence across all robots, thus
betweendifferentlayersofreasoning.Thisleadstoenhanced
enhancing the collective knowledge and reducing overall
robustness,asthemodelcanprovidemorestableandreliable
uncertainty.
estimates in the presence of noise and uncertainties.
We motivate these formulations by again considering the V. Simulations
exampleshowninFig.4.Consideramulti-robotsystemcon- This section showcases the outcomes obtained through
sisting of two ground vehicles equipped with depth sensors Python simulations of our method executed by multi-robot
and one aerial vehicle equipped with a monocular camera. team. We compare zero-order reasoning [38], a first-order
The robots are attempting to allocate tasks without commu- reasoning baseline derived from [5], [30], and our method
nication and ground robot R 1 is assessing evidence between employing higher-order reasoning to reach a valid goal con-
a goal g 1 and other two robots R 2 ,R 3 . In Fig. 4(a), we figuration.Thissectioncomparestheselevelsofreasoningin
show the observations and subsequent evidence calculation three different scenarios: rendezvous, where robots converge
for R 1 ’s sensor configuration which allows R 1 to calculate to a common goal; task allocation, where robots converge
thedistancetothegoalforbothitself(zero-orderreasoning) to separate goals; and multi-task allocation, where robots
and other robots (first-order reasoning). In Fig. 4(b), since individually complete sequential goals. Each scenario and
R 1 is equipped with a depth sensor, it is also able to abstract its respective comparison are discussed in the following
the sensor measurements of R 3 , allowing R 1 to estimate the subsections.
change in R ’s belief.
3
A. Rendezvous
Wenotethatthroughminimizingtheexpectedfreeenergy
and using the likelihood function to update the posterior we In the first scenario comparison, robots are tasked with
will maximize the probability of converging to a common convergingtoasinglegoalamongstseveralpossiblechoices.
goalconfigurationwithoutanycommunication.However,the Random configurations of goal locations, robot sensor con-
Bayesianmethodinherentlysuffersfromthecurseofdimen- figurations, and starting positions of robots are generated
sionality, since the number of possible joint configurations for 50 trials per each combination of robots and goals. The
grows as the number of robots and/or the number of goals numberofrobotsandgoalsvariesbetweentwoandfive.The
grows. Thus, in the next section, we introduce epistemic size of the environment for each test is set at 30m×30m and
the maximum number of iterations or time steps per simula-
tion is set to 150 iterations. The maximum velocity for each
robotis1m/s,andthemulti-robotsystemhasconvergedifall
robotsreachasinglegoalwithin150iterationsandarewithin
1.5m of the position of the goal. The observation error is
normally distributed as N(0,0.5) for distance measurements
and N(0,0.1) for angular measurements. The multi-robot
system is randomly spawned with one of two different types
ofsensorconfigurations.Onesensorconfigurationisarange
sensor (ω ) which can observe distance measurements to
1
other robots, while the other configuration (ω ) can measure
2
relativeanglestotheobservingrobot’sposition.Weconsider
Fig. 6. Comparison of using zero- and first-order versus higher-order
that ω ≻ ω since the robots are capable of abstracting reasoningforarendezvousmission.
1 2
angle measurements from distance measurements. We show
asampleresultinFig.5comparingfirst-orderreasoningand
of robots. The environment size for each trial is set at
higher-order reasoning in a sample environment where two
30m×30m and the maximum number of iterations is set
robots with two different sensor configurations are trying to
to 100 iterations. The maximum velocity for each robot
converge to a single goal. The red UAV can observe angles
is 1m/s and the multi-robot system has converged if all
(ω ) while the blue UGV can measure distances (ω ).
2 1 robots reach a separate goal within 150 iterations and are
within 1.5m of the position of the goal. Observation error is
normally distributed as N(0,0.5) for distance measurements
and N(0,0.1) for angular measurements. The multi-robot
system is randomly spawned with one of two different types
of sensor configurations as in previous sections. We show a
sample result in Fig. 7 comparing first-order reasoning and
higher-order reasoning in a sample environment where two
robots with two different sensor configurations are trying to
converge to a single goal.
(a) First-orderreasoning (b) Higher-orderreasoning
Fig.5. Samplecomparisonofwhereinfirst-orderreasoningtheredUAV
doesnotconsidertheblueUGV’sperceptionofitsmovements
As shown in Fig. 5(a), the red UAV does not consider
the blue UGV’s perception of its movements to gain more
certainty about its observations before the robots end up
converging to a goal farther away. In contrast, higher-order
reasoningallowedtheredUAVtomakesmallmovementsto (a) First-orderreasoning (b) Higher-orderreasoning
gain more certainty about its observations before converging
Fig. 7. Illustration of our simulations showing that first-order reasoning
totheclosergoal.Theresultsshowninthisexampleexplain
failstoallowtheblueandgreenUAVstoconveytheirintentionsorreason
how first-order reasoning is more prone to fail when the from the other robot’s perspective, resulting in both robots converging on
objective is to rendezvous at a goal. The results of all trials the same task. Higher-order reasoning allows the UAVs to interpret and
signalclearintentions,successfullycompletingdifferenttasks.
aredepictedinFig.6whichshowthathigher-orderreasoning
results in a higher success rate and that an increase in
As illustrated in Fig. 7(a), the blue and green UAVs
complexity does not result in a significant decrease in suc-
are incapable of resolving their belief discrepancies. In
cess,whiletheperformanceofzeroandfirst-orderreasoning
contrast, Fig. 7(b) demonstrates that through higher-order
continues to decrease exponentially as the number of robots
reasoning, the UAVs can convey and understand intentions,
increases.
allowing them to resolve task assignment conflicts without
communication.TheresultsofalltrialsaredepictedinFig.8
B. Task Allocation
whichshowthathigher-orderreasoningimprovesthesuccess
In the second set of trials, we show a comparison be-
rate and that an increase in complexity does not result in a
tweenthe samezero- andfirst-order reasoningbaselines and
significant decrease in success similar to the results from
higher-order reasoning when converging to separate goals.
Fig. 6.
Goal locations, robot sensor configurations, and initial robot
C. Multi-Robot Multi-Task Allocation
positions are randomly generated for 50 trials per number
of robots, which range from two to five robots. In these Lastly, we show our method’s performance for a multi-
comparisons, the number of tasks is equal to the number robot multi-task assignment problem where robots must
(a) First-orderreasoning (b) Higher-orderreasoning
Fig. 8. Comparison of using zero- and first-order versus higher-order Fig.10. Illustrationofoursimulationsformulti-robotmulti-taskscenarios.
reasoningforataskallocationmission. In (a), the robots are unable to discern each others’ intentions and causes
redundant task completion. In (b), the robots are able to identify distinct
taskstoaccomplishanddecipherotherrobots’intentions.
decide without explicit communication or a centralized al-
gorithm to accomplish tasks in the environment. Target
capture system for localization. Vehicles start at various
locations,robotsensorconfigurations,initialrobotpositions,
positions in the environment. The experiments were carried
and task locations are randomly generated for 30 trials outina4m×5.5mspace.Theresultsofasampleexperiment
per number of robots ranging between 2 and 8 robots,
withtwopotentialequidistantrendezvouslocationsandthree
as well as number of tasks ranging between 20 and 45
ground vehicles are shown in Fig. 11.
tasks. In total, the data for each level of reasoning and the
number of robots is aggregated for 150 trials per category.
In Fig. 9, we show that utilizing higher-order reasoning
allowsrobotstodecreaseredundancywhencompletingtasks
in the environment and accomplish all tasks more quickly
than just zero- or first-order reasoning. We observe that
while the performance difference is minimal for teams of
two robots, significant improvements are evident as the
team size increases, especially for groups with 4 or more
robots. The following example in Fig. 10 illustrates that
employing lower-order reasoning prevents robots from un-
derstanding the intentions of others, leading to duplicated
tasks and extending the time required to accomplish tasks in
the environment. In contrast, higher-order reasoning enables
Fig. 11. Experiment where three robots rendezvous at one of two
robots to communicate their intentions, thereby reducing
equidistantlocationswithoutexplicitcommunication.
redundancy and increasing system efficiency.
As shown in the figure, each robot initially is uncertain
about which goal the system should converge to. After a
small number of measurements, the robots makes their in-
tentions explicit by minimizing free energy and maximizing
evidence that they are moving toward the green rendezvous
point. The robots accomplish this by taking exaggerated
paths toward the green rendezvous point. This is also the
case depicted in Fig. 12 where robots move toward distinct
goals and signal their intentions using exaggerated paths.
We show similarly that we can perform multiple tasks
per robot with heterogeneous sensing capabilities. Fig. 13
shows several snapshots of the results of this sample virtual
experiment using the RotorS Firefly and Clearpath Jackal
Fig. 9. Comparison of using zero- and first-order versus higher-order
reasoningforataskallocationmission. models in Gazebo and RViz where an aerial vehicle can
only observe the angles of other robots from tasks, while
the ground robots can observe the depth. Similarly to the
VI. Experiments
simulations, we assume that the ground robots can abstract
Our approach was also validated through several labora- the angle measurements of the aerial vehicle. Fig. 13(a)
tory experiments with a multi-robot team. The team consists shows the starting location of all the robots and the tasks.
of several Husarion ROSbot 2.0s that used a Vicon motion Fig. 13(b) shows that the robots initially move to signify
approach enables robots to cooperate and achieve common
goals even when explicit communication is not possible.
Our findings show that higher-order reasoning, extending
up to the third level, significantly enhances the ability of
MRS to converge to correct belief states and complete tasks
efficiently. The hierarchical epistemic planning combined
with active inference for runtime plan adaptation provides
a robust solution to mitigate the challenges of limited
communication in heterogeneous robot teams. Future work
will focus on optimizing epistemic planning techniques and
exploring the integration of even higher levels of reasoning.
Additionally,weaimtoextendourframeworktomorecom-
plex scenarios and larger robot teams, such as sensors with
Fig. 12. A four robot experiment where each robot needs to accomplish
limited field-of-views and cluttered environments, further
adistincttaskwithoutexplicitcommunication.
enhancing the decision-making capabilities of multi-agent
systems.
which tasks they are going to complete while Fig. 13(c)
shows the majority of tasks accomplished without explicit VIII. Acknowledgements
communication. In Fig. 13(d) all tasks have been accom-
This work is based on research sponsored by Northrop
plished and the robots return to their initial location. In
Grumman through the University Basic Research Program.
this way, the robots cooperatively complete all tasks in the
environment, accounting for the heterogeneity of the robots References
in the system and without explicit communication.
[1] A.Valle,D.Massaro,I.Castelli,andA.Marchetti,“Theoryofmind
development in adolescence and early adulthood: The growing com-
plexityofrecursivethinkingability,”Europe’sjournalofpsychology,
vol.11,no.1,p.112,2015.
[2] T. Bolander and M. B. Andersen, “Epistemic planning for single-
and multi-agent systems,” Journal of Applied Non-Classical Logics,
vol.21,no.1,pp.9–34,2011.
[3] L.Bramblett,S.Gao,andN.Bezzo,“Epistemicpredictionandplan-
ningwithimplicitcoordinationformulti-robotteamsincommunica-
tionrestrictedenvironments,”in2023IEEEInternationalConference
onRoboticsandAutomation(ICRA),2023,pp.5744–5750.
[4] L.BramblettandN.Bezzo,“Epistemicplanningformulti-robotsys-
temsincommunication-restrictedenvironments,”FrontiersinRobotics
andAI,vol.10,p.1149439,2023.
(a) (b) [5] D. Maisto, F. Donnarumma, and G. Pezzulo, “Interactive inference:
amulti-agentmodelofcooperativejointactions,”IEEETransactions
onSystems,Man,andCybernetics:Systems,2023.
[6] L. Bramblett, B. Miloradovic, P. Sherman, A. V. Papadopoulos,
and N. Bezzo, “Robust online epistemic replanning of multi-robot
missions,”arXivpreprintarXiv:2403.00641,2024.
[7] M.K.Ho,R.Saxe,andF.Cushman,“Planningwiththeoryofmind,”
TrendsinCognitiveSciences,vol.26,no.11,pp.959–971,2022.
[8] J. R. Anderson, D. Bothell, M. D. Byrne, S. Douglass, C. Lebiere,
andY.Qin,“Anintegratedtheoryofthemind.”Psychologicalreview,
vol.111,no.4,p.1036,2004.
[9] C. Muise, V. Belle, P. Felli, S. McIlraith, T. Miller, A. R. Pearce,
andL.Sonenberg,“Efficientmulti-agentepistemicplanning:Teaching
planners about nested belief,” Artificial Intelligence, vol. 302, p.
103605,2022.
(c) (d)
[10] Y.ZhangandB.Williams,“Adaptationandcommunicationinhuman-
robotteamingtohandlediscrepanciesinagents’beliefsaboutplans,”
Fig. 13. RViz and Gazebo snapshots of virtual experiment with het-
inProceedingsoftheInternationalConferenceonAutomatedPlanning
erogeneous vehicles. As shown, the robots use higher-order reasoning to
accomplishmultipletasks,evenwithdifferentsensorconfigurations.
andScheduling,vol.33,no.1,2023,pp.462–471.
[11] T. Engesser, T. Bolander, R. Mattmu¨ller, and B. Nebel, “Coopera-
tive epistemic multi-agent planning for implicit coordination,” arXiv
VII. Conclusion preprintarXiv:1703.02196,2017.
[12] K. Talamadupula, G. Briggs, T. Chakraborti, M. Scheutz, and
Inthiswork,wedemonstratedtheeffectivenessofutilizing S. Kambhampati, “Coordination in human-robot teams using mental
modeling and plan recognition,” in 2014 IEEE/RSJ International
higher-order reasoning for multi-robot systems (MRS) oper-
ConferenceonIntelligentRobotsandSystems. IEEE,2014,pp.2957–
atingundercommunicationconstraints.Byintegratingtheory 2962.
ofmind(ToM)andepistemicplanning,ourproposedframe- [13] G. Buisan and R. Alami, “A human-aware task planner explicitly
reasoning about human and robot decision, action and reaction,”
work allows robots to infer the knowledge and intentions of in Companion of the 2021 ACM/IEEE International Conference on
othersbasedontheirobservationsandlastknownstates.This Human-RobotInteraction,2021,pp.544–548.
[14] J. Hwang, J. Kim, A. Ahmadi, M. Choi, and J. Tani, “Dealing with SymposiuminRobotandHumanInteractiveCommunication. IEEE,
large-scalespatio-temporalpatternsinimitativeinteractionbetweena 2010,pp.138–143.
robotandahumanbyusingthepredictivecodingframework,”IEEE [38] G.Gutin,A.Yeo,andA.Zverovich,“Travelingsalesmanshouldnot
Transactions on Systems, Man, and Cybernetics: Systems, vol. 50, begreedy:dominationanalysisofgreedy-typeheuristicsforthetsp,”
no.5,pp.1918–1931,2018. DiscreteAppliedMathematics,vol.117,no.1-3,pp.81–86,2002.
[15] Y.Liu,X.Xie,J.Sun,andD.Yang,“Event-triggeredprivacypreser-
vationconsensuscontrolandcontainmentcontrolfornonlinearmass:
Anoutputmaskapproach,”IEEETransactionsonSystems,Man,and
Cybernetics:Systems,2024.
[16] S. Lemaignan and P. Dillenbourg, “Mutual modelling in robotics:
Inspirations for the next steps,” in Proceedings of the Tenth Annual
ACM/IEEE International Conference on Human-Robot Interaction,
2015,pp.303–310.
[17] H.VanDitmarsch,W.vanDerHoek,andB.Kooi,Dynamicepistemic
logic. SpringerScience&BusinessMedia,2007,vol.337.
[18] J. Van Benthem, “Games in dynamic-epistemic logic,” Bulletin of
EconomicResearch,vol.53,no.4,pp.219–248,2001.
[19] I.A.CiardelliandF.Roelofsen,“Inquisitivedynamicepistemiclogic,”
Synthese,vol.192,no.6,pp.1643–1687,2015.
[20] T. Bolander, L. Dissing, and N. Herrmann, “Del-based epistemic
planningforhuman-robotcollaboration:Theoryandimplementation,”
in Proceedings of the International Conference on Principles of
Knowledge Representation and Reasoning, vol. 18, no. 1, 2021, pp.
120–129.
[21] B.Maubert,S.Pinchinat,F.Schwarzentruber,andS.Stranieri,“Con-
current games in dynamic epistemic logic,” in Proceedings of the
Twenty-NinthInternationalJointConferenceonArtificialIntelligence,
2021,pp.1877–1883.
[22] K. Friston, T. FitzGerald, F. Rigoli, P. Schwartenbeck, G. Pezzulo
etal.,“Activeinferenceandlearning,”Neuroscience&Biobehavioral
Reviews,vol.68,pp.862–879,2016.
[23] G.Pezzulo,F.Rigoli,andK.J.Friston,“Hierarchicalactiveinference:
atheoryofmotivatedcontrol,”Trendsincognitivesciences,vol.22,
no.4,pp.294–306,2018.
[24] M.Albarracin,D.Demekas,M.J.Ramstead,andC.Heins,“Epistemic
communitiesunderactiveinference,”Entropy,vol.24,no.4,p.476,
2022.
[25] Z.Tian,S.Zou,I.Davies,T.Warr,L.Wu,H.B.Ammar,andJ.Wang,
“Learning to communicate implicitly by actions,” in Proceedings of
theAAAIConferenceonArtificialIntelligence,vol.34,no.05,2020,
pp.7261–7268.
[26] M. A. Schack, J. G. Rogers, and N. T. Dantam, “The sound of
silence: Exploiting information from the lack of communication,”
IEEERoboticsandAutomationLetters,2024.
[27] C.Pezzato,C.H.Corbato,S.Bonhof,andM.Wisse,“Activeinference
and behavior trees for reactive action planning and execution in
robotics,” IEEE Transactions on Robotics, vol. 39, no. 2, pp. 1050–
1069,2023.
[28] O. C¸atal, T. Verbelen, T. Van de Maele, B. Dhoedt, and A. Safron,
“Robotnavigationashierarchicalactiveinference,”NeuralNetworks,
vol.142,pp.192–204,2021.
[29] G. Pezzulo and P. Cisek, “Navigating the affordance landscape:
feedbackcontrolasaprocessmodelofbehaviorandcognition,”Trends
incognitivesciences,vol.20,no.6,pp.414–424,2016.
[30] M.PriorelliandI.P.Stoianov,“Flexibleintentions:Anactiveinference
theory,”FrontiersinComputationalNeuroscience,vol.17,p.1128694,
2023.
[31] P.-Q.Huang,Q.Zhang,andY.Wang,“Bileveloptimizationviacol-
laborationsamonglower-leveloptimizationtasks,”IEEETransactions
onEvolutionaryComputation,2023.
[32] D.E.Knuth,“Backusnormalformvs.backusnaurform,”Communi-
cationsoftheACM,vol.7,no.12,pp.735–736,1964.
[33] K.Friston,T.FitzGerald,F.Rigoli,P.Schwartenbeck,andG.Pezzulo,
“Active inference: a process theory,” Neural computation, vol. 29,
no.1,pp.1–49,2017.
[34] K. Friston, K. Stephan, B. Li, and J. Daunizeau, “Generalised filter-
ing.”MathematicalProblemsinEngineering,vol.2010,2010.
[35] T.Parr,K.Friston,andG.Pezzulo,“Generativemodelsforsequential
dynamics in active inference,” Cognitive Neurodynamics, pp. 1–14,
2023.
[36] R. Smith, K. J. Friston, and C. J. Whyte, “A step-by-step tutorial
on active inference and its application to empirical data,” Journal of
mathematicalpsychology,vol.107,p.102632,2022.
[37] P. Lison, C. Ehrler, and G.-J. M. Kruijff, “Belief modelling for
situationawarenessinhuman-robotinteraction,”in19thInternational

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Implicit Coordination using Active Epistemic Inference for Multi-Robot Systems"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
