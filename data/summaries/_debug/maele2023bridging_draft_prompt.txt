=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Bridging Cognitive Maps: a Hierarchical Active Inference Model of Spatial Alternation Tasks and the Hippocampal-Prefrontal Circuit
Citation Key: maele2023bridging
Authors: Toon Van de Maele, Bart Dhoedt, Tim Verbelen

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2023

Abstract: Cognitive problem-solving benefits from cognitive maps aiding navigation and planning. Previous
studiesrevealedthatcognitivemapsforphysicalspacenavigationinvolvehippocampal(HC)allocentric
codes,whilecognitivemapsforabstracttaskspaceengagemedialprefrontalcortex(mPFC)task-specific
codes. Solving challenging cognitive tasks requires integrating these two types of maps. This is exem-
plified by spatial alternation tasks in multi-corridor settings, where animals like rodents are rewarded
uponexecutin...

Key Terms: task, bridging, prefrontal, circuit, tasks, information, cognitive, hierarchical, alternation, hippocampal

=== FULL PAPER TEXT ===

Bridging Cognitive Maps: a Hierarchical Active Inference Model of
Spatial Alternation Tasks and the Hippocampal-Prefrontal Circuit
Toon Van de Maele
IDLab, Department of Information Technology, Ghent University - imec, Ghent, Belgium
Bart Dhoedt
IDLab, Department of Information Technology, Ghent University - imec, Ghent, Belgium
Tim Verbelen∗
VERSES Research Lab, Los Angeles, USA
Giovanni Pezzulo∗,†
Institute of Cognitive Sciences and Technologies, National Research Council, Rome, Italy
July 30, 2024
Abstract
Cognitive problem-solving benefits from cognitive maps aiding navigation and planning. Previous
studiesrevealedthatcognitivemapsforphysicalspacenavigationinvolvehippocampal(HC)allocentric
codes,whilecognitivemapsforabstracttaskspaceengagemedialprefrontalcortex(mPFC)task-specific
codes. Solving challenging cognitive tasks requires integrating these two types of maps. This is exem-
plified by spatial alternation tasks in multi-corridor settings, where animals like rodents are rewarded
uponexecutinganalternationpatterninmazecorridors. ExistingstudiesdemonstratedtheHC–mPFC
circuit’s engagement in spatial alternation tasks and that its disruption impairs task performance. Yet,
a comprehensive theory explaining how this circuit integrates task-related and spatial information is
lacking. We advance a novel hierarchical active inference model clarifying how the HC – mPFC circuit
enables the resolution of spatial alternation tasks, by merging physical and task-space cognitive maps.
Through a series of simulations, we demonstrate that the model’s dual layers acquire effective cognitive
mapsfornavigationwithinphysical(HCmap)andtask(mPFCmap)spaces,usingabiologically-inspired
approach: a clone-structured cognitive graph. The model solves spatial alternation tasks through recip-
rocal interactions between the two layers. Importantly, disrupting inter-layer communication impairs
difficult decisions, consistent with empirical findings. The same model showcases the ability to switch
between multiple alternation rules. However, inhibiting message transmission between the two layers
results in perseverative behavior, consistent with empirical findings. In summary, our model provides
a mechanistic account of how the HC – mPFC circuit supports spatial alternation tasks and how its
disruption impairs task performance.
Keywords: cognitivemap,hierarchicalactiveinference,spatialalternationtasks,hippocampus,medial
prefrontal cortex, disruption
*Theseauthorscontributedequallytothiswork
†Correspondingauthor: giovanni.pezzulo@istc.cnr.it
Preprint
1
4202
luJ
72
]CN.oib-q[
3v36411.8032:viXra
1 Introduction
To solve cognitive problems, such as finding the shortest route to a goal destination in a busy city, we can
useso-calledcognitive maps ofthesituationthataffordsflexibleplanning. Theconceptofthecognitivemap
has been initially popularized by Tolman, especially in the context of spatial navigation (Tolman, 1948).
An extensive body of literature has investigated the neuronal underpinnings of cognitive maps for spatial
navigation in humans, rodents, and other animals. This research has established the crucial importance
of structures of the medial temporal lobe and especially of the hippocampal formation in the creation of
codes for allocentric space, such as place cells (in the hippocampus) (O’Keefe and Nadel, 1979; O’Keefe and
Dostrovsky,1971)andgridcells(intheentorhinalcortex)(Haftingetal.,2005). Anemergingperspectiveto
understandthesefindingsisthatcognitivemappinginthehippocampalformationmightbedescribedatthe
computationallevelasasequencelearningproblemandthatplacecells,gridcellsandotherspecializedcodes
might emerge from such sequence learning (Whittington et al., 2020, 2022; George et al., 2021; Raju et al.,
2022a; Stachenfeld et al., 2017; Stoianov et al., 2022; Chen et al., 2022; Levenstein et al., 2024; Recanatesi
et al., 2021). Furthermore, it has been suggested that the same hippocampal codes that support spatial
navigationmightalsosupportnavigationin“cognitive”domains(Bellmundetal.,2018;Epsteinetal.,2017;
Buzs´aki and Moser, 2013).
In parallel, the concept of a cognitive map can apply to the abstract state spaces that describe the
sequential stages of a task at hand (for example, buying a gift in one’s preferred shop and then bringing it
to a friend’s house) as opposed to the more fine-grained spatial codes found in the hippocampal formation.
Recent studies reported that prefrontal structures, such as the orbitofrontal cortex, might host such coarse
task codes, which might permit representing (for example) the current and the next stages of the task (Niv,
2019; Schuck et al., 2016) as well as future navigational goals (Basu et al., 2021), as opposed to a physical
location.
Crucially, during goal-directed spatial navigation (and other cognitive problems), it is often necessary to
combine cognitive maps of task space (to find a sequence of coarse-grained actions to progress in task space,
such as ensuring the correct sequence of destinations to buy and deliver a gift) and physical space (to find
a path that reaches the next intended destination) (Ito et al., 2015; Pezzulo et al., 2014b; Verschure et al.,
2014). However, we still lack a comprehensive theory that explains the ways cognitive maps of physical and
task space interact when executing cognitive tasks.
Ausefulstartingpointtounderstandtheinteractionoftask-relatedandspatialcodesisrodentmemory-
guided,spatialalternationtasks. AprominentexampleisthespatialalternationtaskintheW-mazestudied
in(Jadhavetal.,2012). AsvisualizedinFigure1,theW-mazecomprisesthreecorridors. Tocollectrewards,
theanimalhastovisitthecorridorsinthecorrectorder,accordingtoan“alternationrule”(e.g. left,center,
right, center, left, etc) that is initially unknown and has to be learned by trial and error.
A series of experiments showed that solving the spatial alternation task engages coordinated patterns
of neural activity in the hippocampus (HC), which putatively encodes the spatial aspects of the task, and
the prefrontal cortex (mPFC), which putatively learns the alternation rule and uses it to guide behav-
ior (Benchenane et al., 2011; Shin and Jadhav, 2016; Colgin, 2011; Siapas et al., 2005; Khodagholy et al.,
2017). Forexample,theHC-mPFCinteractionisselectivelyenhancedduringepochsrequiringspatialwork-
ing memory (Jones and Wilson, 2005). Furthermore, the coordination of neural activity in both structures
spans multiple timescales, from theta sequences during navigation to reactivation (replay) activity during
inter-trial periods prior to navigation – and is considered crucial for the information exchange between the
two areas during the task (Tang et al., 2021).
Notably, disrupting awake hippocampal reactivations (sharp wave ripples, SWR) at decision points im-
pairedtheanimal’sperformance,butonlyfortheaspectsofthealternationtaskthatrequirespatialworking
memory (Jadhav et al., 2012), see Figure 1. Specifically, disrupting awake SWRs impaired outbound de-
cisions (from the central corridor to the left or right corridor), which require memory for immediate past
outer arm location. Rather, the same SWR disruption did not impair inbound decisions (from the left or
right corridor to the central corridor) that do not require memory of the past corridor; nor did it impair
self-localization(Jadhavetal., 2012). Thestudy concludesthatimpairingSWRsprevents thehippocampus
from providing information about past locations (and/or future options) to the prefrontal cortex, which is
2
(a)
Figure1: Spatial alternation tasks (LCRC).ThisfigureexemplifiestheW-mazeusedin (Jadhavetal.,
2012) to study spatial alternation and the role of the hippocampal - prefrontal dynamics. The W-maze is
characterized by three separate corridors. The animal can acquire the reward at the end of each corridor,
however, to receive the reward it must do so in the correct order (e.g. Left, Center, Right, Center, Left,
etc). The spatial alternation task implies that when the animal is in the center position, it has to make a
(difficult) outbound decision: it has to go either to the left or the right, depending on where it comes from.
This outbound decision, therefore, requires a spatial memory component. In contrast, when the animal is
in one of the side corridors (left or right), the only correct action is to move to the center. This inbound
decision is considered simpler since it does not require a memory component. Trajectory order is indicated
by the shade where lighter is earlier.
responsibletolearnthealternationruleandtouseittoguidebehavior. Inotherwords,theSWRdisruption
impairs communication between the hippocampus and the prefrontal cortex, hence preventing the latter
from correctly inferring the current stage in task space.
The importance of hippocampal-prefrontal communication for memory guided decisions is confirmed by
anotherstudyinwhichanimalshadtolearnandsubsequentlyswitchbetweenthreespatialalternationrules
in the same three-arm maze (Den Bakker et al., 2022). This study showed that disrupting neural activity
in the mPFC directly following hippocampal sharp-wave ripples (but not after a random delay) impairs the
animal’s ability to switch between learned rules.
Here,weadvanceanovelcomputationaltheorythatcaststheinteractionsbetweenthehippocampus(HC)
and the medial prefrontal cortex (mPFC) during spatial alternation tasks, in terms of hierarchical active
inference(Friston,2010;Fristonetal.,2017a;Bogacz,2017;Buckleyetal.,2017;Parretal.,2022;Smithetal.,
2022; Isomura et al., 2023). Our theory has two main tenets. First, the HC and the mPFC learn cognitive
mapsfornavigationinphysicalandtaskspace,respectively,usingthesamestatisticalcomputationsmodeled
asPartiallyObservableMarkovDecisionProcesses,POMDP(Kaelblingetal.,1998)),butbasedondifferent
inputs (see Section 4). Second, the neural circuit formed by the HC and the mPFC realizes a hierarchical
active inference architecture, which can solve spatial alternation tasks. Hierarchical active inference rests
on reciprocal, bottom-up, and top-down message passing. In the bottom-up pathway, the lower hierarchical
level (encoding the HC map) infers the current location based on sensory information and communicates
it to the higher level (encoding the mPFC map). In turn, the higher level infers the next goal location
(in the mPFC map) and sets it as a goal for spatial navigation (in the HC map), in a top-down manner.
Impairmentsofthemessagepassingpreventthearchitecturefromcorrectlysolvingspatialalternationtasks.
The separation of timescales automatically emerges from the structure of hierarchical active inference. The
lower level passes the bottom-up message when a free-energy threshold is met, indicating it is confident it
has reached its goal.
Weexemplifythenewtheorybypresentingtwosimulationsofrodentspatialalternationtasks,withintact
and impaired HC- mPFC interactions (Jadhav et al., 2012; Den Bakker et al., 2022). Our first simulation,
presentedinSection2.2,bringsthreemainconclusions. First,themodelisabletolearnthespatialstructure
ofthemaze(HCmap)andspatialalternationrules(mPFC)bynavigatingintheenvironment. Second,when
the HC - mPFC circuit is configured as a hierarchical active inference system, it effectively solves spatial
alternationtasks,throughbottom-upandtop-downmessagepassing. Third,disruptingtheinteractionfrom
the HC to the mPFC breaks the spatial memory and impairs the animal’s ability to make outbound but
3
not inbound decisions, as observed empirically (Jadhav et al., 2012). Our second simulation, presented in
Section 2.2.1, shows two additional features of the model. First, in a task in which three alternative spatial
alternation rules are in play, the HC - mPFC circuit permits inferring the current rule. Second, selectively
inhibiting the mPFC impairs this ability and provokes distinct behavioral patterns, as observed empirically
in the rodent study reported in (Den Bakker et al., 2022).
We provide the code for the simulations at https://github.com/toonvdm/bridging-cognitive-maps.
2 Results
2.1 The W-maze setup and the hierarchical active inference (HAI) agent
In this work, we consider the spatial alternation task in the W-maze of (Jadhav et al., 2012), see Figure 1.
In this task, an animal (or an artificial agent) can acquire rewards at the end of each corridor, providing
that the corridors are visited in the correct order (e.g. left, center, right, center, left).
In this section, we present a hierarchical active inference (HAI) agent (Parr et al., 2022) for solving the
spatial alternation task. Central to this framework is the fact that agents are endowed with a generative
model,describinghowobservedoutcomesaregeneratedfromahidden,unobservedstate,whichisinfluenced
by the agent’s actions. The agent infers these hidden states by minimizing the free energy functional with
respecttoitsbeliefoverthestateandactstominimizeitsexpectedfreeenergyovertime(Parretal.,2022).
For more details on these mechanisms, the reader is referred to Section 4.2.
Inoursimulations, theHAIagentisendowedwithahierarchicalgenerativemodeloftwolayers, whichis
schematically illustrated in Figure 2a (see Section 4.2 for a formal introduction). In this generative model,
the lowest layer takes up the functional role of the hippocampus, namely encoding the spatial structure of
themaze(O’KeefeandNadel,1979). Incontrast,thehighestlayertakesthefunctionalroleoftheprefrontal
cortex, namely encoding the structure of task space (Schuck et al., 2016). The agent learns both maps
using the same sequence learning algorithm: the clone structured cognitive graphs (CSCG (George et al.,
2021)), see Section 4 for details. The latent states learned by the CSCG at the two levels are schematically
illustrated within the two boxes of Figure 2a.
Crucially, the HAI agent performs inference (of where it is in physical space, i.e., its pose, and in task
space) and planning (of the next goal in physical and task space) through message-passing between the two
hierarchicallevels. AsshowninFigure2a,level1observesthestructuralaspectsoftheenvironmentthrough
its bottom-up observations, i.e. the three-by-three grid around the agent as shown in Figure 2b and selects
local movements in physical space such as “turn left”, “turn right” or “move forward” to reach any goal
location (which is fed by the highest level, see below).
Once level 1 has navigated towards its local goal, it passes a bottom-up message containing the current
state of the agent to level 2. In turn, level 2 processes this message and infers the next stage of the task to
achieve a reward (as it is endowed with a prior preference to achieve reward at each step, see Section 4.2).
Actions at level 2 are abstract and are matched to the bottom-up received observable states from level 1.
Since level 1 states encode the agent’s pose, these actions essentially mean “move to target location”. We
specifically encode only actions corresponding to target locations at the end of a corridor (i.e. observation
1 in Figure 2b). The selected action is sent to level 1 in a top-down manner and corresponds to the goal
location for level 1. Once level 1 reaches the goal location, this process repeats.
Finally, the right part of Figure 2a illustrates the fact that the lowest level of the model is responsible
for the action-perception loop with the environment – here, an implementation of the W-maze in the Min-
igrid (Chevalier-Boisvert et al., 2018) simulator. Observations in the Minigrid (Chevalier-Boisvert et al.,
2018) are acquired as a square around the agent. In this work, we chose a range of three, yielding three-by-
three observations which are then mapped to a unique one-hot index. For each step, the agent also observes
a binary value indicating the reward presence. The actions the agent can perform are “turn left”, “turn
right” and “move forward”, which are also represented as a one-hot index. See Section 4.2 for more details
on the model implementation
4
(a)
(b)
Figure 2: Illustration of the hierarchical active inference (HAI) agent and the W-maze setup
(a) The hierarchical generative model of the HAI agent. The figure shows that the two layers of the model
include cognitive maps of physical space (level 1) and task space (level 2). These cognitive maps essentially
represent transitions between learned locations and stages of the task, respectively. Both maps are learned
usingaprobabilisticsequencelearningalgorithm: theclonestructuredcognitivegraph(CSCG(Georgeetal.,
2021)). Within each block, a subset of states from the CSCG is visualized, namely, the active states when
pursuing the alternation rule (LCRC). For a visualization of the full set of states from the CSCG, we refer
to Figure 6. The dynamics between the physical (level 1) and task space (level 2) CSCG in the hierarchical
model are regulated by bottom-up message passing (that supports inference) and top-down message passing
(that supports planning). Bottom-up inference messages from the physical space model are passed to the
task space model, while top-down actions drive planning from the task-space model to the physical space
model. An essential aspect of these interactions is that the two levels operate on a different temporal scale.
Transitions at the physical level, through the agent’s movement, occur at every time step. Rather, the task
level only transitions when the intermediate goal (of reaching a certain position in the maze) is achieved.
This abstraction allows for hierarchical planning. Finally, the interaction with the environment using an
action-perception loop with the world, the W-maze, is shown on the right. (b) The 20 distinct observations
theagentcanencounterintheW-mazecreatedintheMinigridenvironment(Chevalier-Boisvertetal.,2018).
5
2.2 Experiment 1: Solving the spatial alternation task
We replicate the task and experiment from a study on rodents (Jadhav et al., 2012) where the animal is
taught this specific alternation task. In this study, disruption of the hippocampal sharp-wave ripple (SWR)
reduces the performance on outbound trajectories, but not on inbound trajectories.
We first allow the HAI agent to learn the two cognitive maps for solving the spatial alternation task,
usingtwoCSCGs(see(Georgeetal.,2021)andSection4fordetailsonthelearningprocedure). Thelearned
maps for the W-maze are shown within the two boxes of Figure 2a. For ease of visualization, within each
box, only a subset of states from the CSCG is shown, namely, the states that are active when correctly
pursuingthealternationrule(thecompletelearnedmapsareshowninFigure6). Wedenotethesequenceof
this alternation rule as LCRC (for Left, Center, Right, Center). For a visualization of the full set of states
from the CSCG, please see Figure 6.
Thehippocampalmaponthelowestlevelorganizesthe20observationsthattheagentencountersduring
navigation (illustrated in Figure 2b) into a coherent graph, which nicely reflects the 3 corridors of the W-
maze. In this map, the circles reflect the states learned by the CSCG, while the transitions reflect the
agent’s spatial actions (e.g., “turn left”, “turn right”, or “move forward”). The nodes are color-coded (and
numbered) according to the observations that they encode. Note that while the agent encounters the same
observationsmultipletimesduringnavigation(i.e.,theobservationsarealiased),theCSCGcorrectlyreflects
the fact that they are part of different behavioral sequences. For example, observation 1 (that corresponds
to the location in which a reward can be collected) appears three times, at the apex of each of the latent
sequences that represent each of the three corridors.
Theprefrontalmaponthehighestlevelcorrespondstoamuchsmallergraphthatencodesthetask-specific
sequence of corridors that the agent has to visit to secure rewards. In this graph, the nodes correspond to
(color-coded)corridorsinwhichrewardshavebeencollectedandtheedgescorrespondtohigher-orderactions
(“go to corridor”). Note that there are two nodes of the same color: both correspond to the central corridor
but in different phases in task space (i.e., after the left and the right corridors, respectively). This indicates
that the agent has successfully learned a low-dimensional representation of (or a finite state machine for)
the task rule LCRC.
We then investigate whether the HAI agent equipped with the learned cognitive maps for physical and
taskspaceisabletosolvespatialalternationtasks. Forthis,weendowtheHAIagentwithapriorpreference
for maximizing the reward and place it in a random pose in the W-maze. We then let the agent run for
300 steps, and record whether it selected the correct or the incorrect corridor to find the reward. We repeat
this experiment for 20 trials, to collect statistics. Figure 3a shows that the HAI agent has near-perfect per-
formance during the easiest, inbound decisions and very high (above 80%) performance during the harder,
outbound decisions, closely approximating the empirical results reported in Jadhav et al. (2012). Further-
more, as expected, it significantly outperforms a Random agent that selects the next corridor randomly
(t-test p=0). Figure 3b shows that after a few time steps, once the HAI agent has inferred where it is (in
both physical and task space), it consistently follows the rule and is able to collect rewards at every step.
We performed two control experiments, which reveal that the HAI model is robust to a wide choice of
parameters (Appendix D.2) and to greater levels of uncertainty (Appendix D.3). Our control experiments
also illustrate that in scenarios characterized by greater uncertainty, the expected free energy functional
used by the HAI model to select policies is more advantageous compared to the sole objective of reward
maximization that is common in economic and reinforcement learning settings. The expected free energy
functional effectively balances two components: a pragmatic imperative to maximize reward (e.g., to reach
the next subgoal to secure reward) and an epistemic imperative to gain information (e.g., to reduce uncer-
tainty about the current pose, by going to places where unambiguous observations could be found, e.g., the
T-junction),seeSection4.2. BycomparingHAIagentswithandwithouttheepistemicimperative,wefound
that the pragmatic imperative to secure rewards is sufficient to address a W-maze with low uncertainty, but
addingtheepistemicimperativetoreduceuncertaintyaboutone’shiddenstate(here,thepose)significantly
increases performance in a W-maze with greater uncertainty (Appendix D.3). This is because an agent en-
dowedwithepistemicuncertaintycanexplicitlyplantoself-localize(e.g.,byvisitingunambiguouslocations,
suchasthebottomofcorridors),whichisespeciallyadvantageouswithgreateruncertainty(Parretal.,2022;
6
(a) (b)
(c)
Figure 3: Experiment 1: Solving the spatial alternation task with intact and disrupted spatial
memory. (a) The success rate in the spatial alternation task for the HAI agent without disruption, the
HAI agent with disruption (HAI disrupted), and the Random agent, separated for both in- and outbound
trajectories. The success rate is computed over 20 trials per agent, with 300 steps per trial, only recording
the presence of reward at the end of the selected corridors. (b) The reward over time for the HAI agent and
a disrupted HAI agent. The shaded areas in the plot indicate outbound trajectories. (c) The success rate
for the HAI agent with (HAI disrupted) and without (HAI) disruption in the alternative light environment.
The success rate is computed over 20 trials per agent, with 300 steps per trial, only recording the presence
of reward at the end of the selected corridors. The agent is disrupted by disabling the connection from
the transition model of the POMDP, separated for in- and outbound trajectories. We observe that the
performance on inbound trajectories does not change, while for outbound trajectories this drops to random
chance.
7
Schwartenbeck et al., 2019).
2.2.1 Disrupting spatial memory impairs outbound decisions
Having established that the HAI agent correctly solves the spatial alternation task, we next ask whether
an impairment of spatial memory disrupts its performance in outbound decisions, as shown experimentally
by Jadhav et al. (Jadhav et al., 2012). In this study, the experimenters disrupted hippocampal SWR at
decisionpointsandobservedthatthedisruptionpreventedaccesstospatialmemory–renderingtherodents
unable to make correct outbound decisions – but left hippocampal spatial codes intact. The experimenters,
therefore, concluded that the disruption was caused by a failure of communication or updating at the level
of the PFC.
In analogy with this procedure, we realized a variant of the HAI agent (HAI disrupt) in which we
implement the SWR disruption by preventing the belief updates (through the transition model) at the
second level of the hierarchical model (Figure 7 as explained in Section 4.2). By doing this, we remove the
agent’s spatial memory about task space. However, the agent is still able to receive the bottom-up message
from level 1 and thus knows where it currently is in physical space – in keeping with the finding that spatial
codes are intact after SWR disruptions (Jadhav et al., 2012).
WhenwecomparetheHAIagentwithandwithoutdisruptioninthespatialalternationtask, weobserve
thesamebehaviorastheexperimentalstudy: theHAIdisruptedagentcorrectlyaddressesinbounddecisions
but makes outbound decisions at a chance level, see Figure 3a. The simulation shows that the disruption
only impairs the outbound trajectories (please see Appendix D.1 for additional simulations exploring how
thechoiceofthegammaparameterinfluencestheresults). Amoredetailedexampleofthispatternofresults
can be appreciated in Figure 3b, which shows that the HAI disrupted agent tends to miss rewards, but only
during outbound decisions.
2.2.2 Control experiment: the disruption impairs outbound decisions even when task vari-
ables are observed
Finally, we performed a control simulation to identify more clearly why the disruption impairs the ability
to make the outbound decisions. It is worth noting that in the higher-level (prefrontal) map of the HAI
model, the task variables (color-coded nodes in Figure 2) are hidden (i.e., they do not have corresponding
observations),unlikethespatialstatesinthehippocampalmapthathavecorrespondingobservations. Hence,
the agent’s estimate of the hidden state at the level of task space depends entirely on spatial memory, as
encoded in the model’s transition function – which we disrupt.
To assess whether the partial observability of task space is key to explain the effects of the disruption,
we considered an alternative (control) environment where the task variable is observable, however, the
component required for spatial memory is unobserved. We consider the following variant of the W-maze
environment: themazeisaugmentedwithalightsignal. Dependingonwheretheagentjustcollectedreward,
a different color is shown. We implement this by adding an additional observation modality containing a
categorical variable, indicating the previous rewarding corridor.
WerepeatedthesamedisruptionexperimentasbeforeandfoundthatthedisruptedHAIagentmaintains
a high success rate for inbound decisions, but makes outbound decisions at chance level (Figure 3c). This
control experiment therefore indicates that disruption of spatial memory impairs the ability to make the
outbound decisions, even when task variables are observable.
2.3 Experiment 2: learning multiple rules and flexible switching between them
In the next experiment, we investigate whether the HAI agent is able to learn multiple alternation rules as
proposed in (Den Bakker et al., 2022). We consider three spatial alternation rules, where for each rule, a
different corridor serves as the alternation point (LCLR, LCRC, and RCRL). Note that for simplicity, we
use the same structure and notation as in the previous experiment with the W-maze, despite the study of
8
(b)
(c)
(a) (d)
Figure4: Experiment 2: Multiple spatial alternation rules (LCLR, LCRC, and RCRL).(a)Rep-
resentation of the states used in the CSCG for the three distinct rules. States again map to the conjunction
of the visited corridor and the presence of reward. Colors in the graph indicate the rule for which the state
is used (red for rule 1, green for rule 2, and blue for rule 3). The gray nodes represent other states, either
withoutrewardorintransitionbetweenmultiplerules(b)Disruptionexperimentforthemodelthatlearned
three rules, evaluated on a single rule. The success rate is computed over 20 trials per agent, with 300 steps
per trial. (c) Average reward for the agents trained on 3 rules, 1 rules and random selected. The case where
the rule is constant (left), and the rule switches every 300 steps to a randomly selected next rule (right)
is considered. This experiment was repeated over 20 trials of 900 steps. (d) Performance during a single
representative trial of 1000 steps, during which the rule switches randomly, every 150 steps. The top plot
shows the rule that is currently in play. The center plot shows the belief over rule, which is computed by
first extracting states belonging to a single rule, and then measuring the likelihood of being in one of these
states. The bottom plot shows the rewards collected over time.
multiple alternation rules of (Den Bakker et al., 2022) has been conducted on a three-arm radial maze, not
a W-maze.
In this experiment, the HAI agent is endowed with a cognitive map of physical space which is the same
as Experiment 1 (since the maze is always the same). However, it has to learn a new cognitive map of task
space, which now comprises the dynamics of the three rules. For this, we let the agent explore the maze for
8000 steps, with alternating task rules after blocks of 1000 steps (in task space, i.e. visits to corridors). To
aid the learning process, we guide the sequence and select the correct action 75% of the time, and random
otherwise.
The learned cognitive map of task space is visualized in Figure 4a. For ease of visualization, we extract
the states that are active when the agent has correctly inferred the rule, after a warmup period (15 steps)
9
in each block. Similar to Section 2.2, the states of the task space correspond to the distinct colors, however
sincethedifferentruleshavedistincttransitiondynamics,thesearemappedtouniquestates. Figure4auses
differentcolorstovisualizethestatesofthethreedifferentrules(redforrule1, greenforrule2, andbluefor
rule 3). The smaller dark gray states are states used for transitioning between rules, or encode the corridors
where no reward is found. We observe that for each of the individual rules, four states are active. This is an
indication of correct learning since four is the optimal number required for each alternation problem.
Then, we evaluate whether the HAI agent is able to infer which rule is currently in place – in order
to continue collecting rewards when the rule switches. For this, we test the HAI agent (endowed with a
preference to maximize reward) in a task in which the rule switches every 300 steps.
Figure 4b shows the success rate of various agents for a fixed rule over 20 trials. From the success rate
of the HAI agent trained on the three rules (HAI 3 rules), we can conclude that the agent properly encodes
the rule. When we apply the same disruption as in Experiment 1 (Section 2.2), the success rate for inbound
trajectories is now also affected (HAI 3 rules disrupt). This drop in performance is caused by the fact that
when considering multiple rules, there is also ambiguity over which rule is currently in play, for which the
spatial memory component is crucial.
Figure 4c compares the performance of various agents in two scenarios: in the former, there is a single
rule, which remains fixed throughout the experiment (left plot), whereas in the latter, there are three rules,
which switch randomly every 300 steps, over 20 trials. The left plot shows that both the Active Inference
agents trained on three rules (HAI 3 rules) and on one rule (HAI 1 rule) achieve a very high success rate in
thefirstscenarioandbothoutperformarandomagent(random). Thisresultindicatesthatlearningmultiple
rules does not decrease performance when a single rule is in place. The right plot shows that the Active
Inference agent trained on multiple rules (HAI 3 rules) achieves a high success rate in the second scenario,
too,indicatingitcorrectlylearnedalltherulesandsuccessfullyalternatesthem. TheActiveInferenceagent
trained on a single rule (HAI 1 rule) achieves a lower success rate, as expected, but still it outperforms a
random agent. This result shows that learning only a subset of the task rules could lead to a relatively
good performance in the W-maze, but still it is possible to discriminate between agents that learn the task
completely, partially or select randomly from their behavior.
Figure 4d illustrates the behavior of the agent during a single, representative trial. As shown in the
figure, shortly after the rule switches (top panel), the HAI agent is able to correctly update its belief about
the current rule in its plan (center panel) and secure rewards (bottom panel). The inference of the rule
currently in play follows standard Bayesian approach: when the rule changes, the agent receives unexpected
observations (since expected rewards are not delivered) and correctly makes a transition to the subspace of
the task rule map that encodes the most likely rule. In the center panel of Figure 4d, we visualize the belief
over each rule as the probability of being in one of the four states used in a particular rule (the rules are
color-coded as Figure 4a) in or in another phase if the state does not occur in either of the rules (grey line).
When the agent has a high probability of a rule, it no longer misses the reward. In some cases, it misses the
reward because there is a potential overlap between the two rules. For example, around step 200 it could
be either of the rules, as it only picks the right one once, and the wrong one after – which could happen
for each corridor in each of the rules. Note that in this architecture, the belief about the currently active
rule is implicit in the model, but it could be made explicit by adding a hierarchical layer that maintains a
probability distribution over the map or the task one is currently in, as in (Stoianov et al., 2022).
2.3.1 Disrupting spatial memory yields specific patterns
Another important finding of the study of Den Bakker et al. (2022) is that disrupting optogenetically the
medial prefrontal cortex (mPFC) immediately after a hippocampal SWR is detected significantly impairs
performance, by increasing the occurrence of three distinct maladaptive patterns: (i) rotated alternation,
wheretheagentfollowsadifferentrule,(ii)backandforth,wheretheagentalternatesbetweentwocorridors,
and (iii) circling behavior, where the agent iterates over all corridors in a cyclic pattern (see Figure 10 for a
graphicalillustrationofthesepatterns). Theauthorsreportedaproportionof35%alternation,20%rotated
alternation, 15 % back and forth, and 10 % circling.
Thebehavioralpatternsthataregeneratedfromthesametypeofdisruptionasconductedinexperiment
10
Table 1: Behavioral patterns during disruption: Classified observed behavioral patterns according
to Den Bakker et al. (2022) in different scenarios for agents trained on the three rules indicated by (3) and
a single rule indicated by (1).
HAI HAI HAI HAI Random
control (1) disrupt (1) control (3) disrupt (3)
Alternation 86.33% 49.07% 76.00% 16.80% 3.47%
Rotated alternation 0.00% 0.80% 1.07 % 13.55% 8.24%
Back and forth 2.68% 38.40% 2.40 % 38.48% 8.86%
Circling 1.61% 1.60% 8.80 % 17.34% 5.86%
Other 9.38% 10.13% 11.73% 13.82% 73.75%
1-wherethetransitionmodelofthePOMDPislesioned-areinvestigated. Wematchthepatternsobserved
in the biological counterpart of this experiment (Den Bakker et al., 2022), and evaluate the trajectories
generated by the agent trained on the three rules, for an agent with and without the spatial memory
disruption from Section 2.2, as well as a random agent. The results are reported in Table 1. This shows
that, whenlesioned, ourmodelreproducesallfourbehavioralpatternsseeninlesionedanimals(DenBakker
et al., 2022) (alternation, rotated alternation, back and forth, and circling) although in slightly different
proportions compared to the empirical study (please see Appendix D.1 for a simulation how the choice of
γ affects these proportions). We observe that for the disrupted agent, a large part of the behaviors can be
classifiedasback-and-forthbehavior. Thespecificproportionsofcategorizationareafunctionoftheagents’
habitpolicyE.Thishabitactsasapriorovertheselectedaction(oflevel2). Whennotemporalinformation
is considered through the lesion, this prior will dominate, and the agent will act according to the statistics
of the training data. This can be observed by looking at HAI disrupt (1), where the agent behavior falls
back to either the alternation or the back-and-forth case. Finally, we also note that the patterns generated
by the random agent are generally classified as the other category, indicating that the behavior generated
by our disruption is not random.
Furthermore, we performed the same experiment for an agent trained on a single rule. We observe that
both rotated alternation and circling behavior drops to near 0% as shown in the table indicated by HAI
control (1) and HAI disrupt (1). This is due to the agent not considering other rules, when falling back to
its habit policy. To the best of our knowledge, the pattern of errors in animals trained with a single rule
have not been systematically studied and hence our results could be considered as a novel prediction of the
HAI model.
2.3.2 Disrupting communication between the two levels yields perseveration behavior
TheprevioussimulationsillustratethatdisruptionsoftheHAImodelpermitreproducingempiricalfindings
of rodent studies (Den Bakker et al., 2022; Jadhav et al., 2012). However, our model permits realizing
synthetic experiments – for example, other types of disruptions – that have not been studied empirically so
far, but which could be interpreted as novel predictions of the model.
Here, we investigate the potential effects of a disruption of hippocampal - prefrontal communication
that completely impairs the communication between the two levels of the HAI model. Specifically, in
this simulation, we prevent level 1 from sending any bottom-up information to level 2. This is a severe
impairment, since hierarchical inference rests upon the reciprocal messages passing between the two levels –
andlevel2onlyreceivesobservationsfromlevel1, notfromtheenvironment. Figure5showsthedifferences
between the physical trajectory of the intact HAI agent (left) and the disrupted HAI agent (right) in which
communication between the two levels is impaired. The shade of the trajectory indicates the order of visits
(lighter is earlier). The figure shows that while the intact HAI agent follows the correct rule, the disrupted
HAI agent perseveres in choosing the center corridor.
Interestingly, the agent does check at the unambiguous T-junction to self-localize and ensure that it is
stillinthecorrectcorridor(whichisafeatureoftheepistemicdynamicsofactiveinference, seeSection4.2),
11
Figure 5: Disruption experiment for rule switching: when communication between the level 1 and
level 2 model is suppressed, perseveration behavior is observed. Trajectory order is indicated by the shade
where lighter is earlier, noise was added to the trajectory to visualize overlapping visits.
butneverselectsthenextgoalinthetaskrule. Thisisbecause,withoutthebottom-upmessagefromlevel1,
level2isunabletocorrectlyrecognizethatonephaseofthetaskhasbeenachievedandhenceneverupdates
the top-down plan. This result illustrates the possibility to use the model to generate novel predictions – in
thiscase,abouthowdisruptingcommunicationbetweenthetwolevelsproducesperseveration–whichcould
be subsequently tested empirically.
3 Discussion
Here,weadvancedanovelcomputationaltheoryofhippocampal(HC)-prefrontal(PFC)interactionsduring
cognitive tasks that require navigating in both physical and task space – such as spatial alternation tasks.
Empirical studies of spatial alternation have assessed that they depend on the animal’s spatial memory,
whichisatleastinpartmaintainedbyHC-PFCcommunication(JonesandWilson,2005;Spiers,2008;Shin
andJadhav,2016;PataiandSpiers,2021;Tangetal.,2021;SimonsandSpiers,2003)andthatdisruptionof
this communication can impair difficult (outbound) decisions (Jadhav et al., 2012) and the ability to switch
between multiple rules (Den Bakker et al., 2022).
Our computational model is based on – and unites – two established theories. The former is a theory of
cognitive map formation, based on a statistical sequence learning algorithm: the clone-structured cognitive
graph (CSCG) (George et al., 2021). Previous studies assessed that CSCG are computationally effective
and have biological validity since they are able to successfully reproduce a number of empirical observations
aboutcognitivemapformationinthehippocampus,includingforexampletheemergenceofplacecellcoding
and splitter cells (Raju et al., 2022b; Sun et al., 2023) and orthogonalized state representations (Sun et al.,
2023) (see also (Whittington et al., 2020, 2022) for alternative proposals about the computational principles
that underlie spatial map formation). Here we extended this body of work by showing that CSCGs can
be used in a hierarchical scheme to learn not just cognitive maps for physical space (putatively linked to
hippocampal computations), but also more abstract cognitive maps for task space (putatively linked to
prefrontal computations). Furthermore, we used learned CSCGs as components of a hierarchical generative
model for active inference (Parr et al., 2022). Active inference is a normative framework to model sentient
behaviorintermsoffreeenergyminimizationand(approximateBayesian)inferenceoveragenerativemodel,
which is gaining popularity in cognitive neuroscience. We have shown that by combining two learned CSCG
maps (for physical and task space), it is possible to design an effective hierarchical active inference (HAI)
agentabletosolvespatialalternationtasks. Interestingly,thisschemeaffords(hierarchical)planningbyonly
using local computations – that is, top-down and bottom-up message-passing between the two hierarchical
levels.
Furthermore, by simulating the interruption of HC - PFC communication in our model, we were able to
correctly reproduce impairments of difficult (outbound) decisions (Jadhav et al., 2012) (Experiment 1) and
of correct rule switching, unveiling the same kind of maladaptive behavior – rotated alternation, back and
forth, and circling behavior (Experiment 2) – observed empirically in rodents (Den Bakker et al., 2022).
Ourmodel suggeststhat theselectiveimpairmentof outbound decisionsprovokedbyhippocampalSWR
disruptions(Jadhavetal.,2012)isduetothefactthattheSWRsconveymessagestohigherstructures, like
12
the PFC, which are used to update a belief about the current stage of the task (specifically, this message is
key to propagate the belief about task state at level 2 over time). This interpretation is in keeping with the
ideaof(Jadhavetal.,2012)thattheimpairmentisatthelevelofspatialmemory, notofhippocampalplace
coding, but to our knowledge, ours is the first work that provides a mechanistic model of this theoretical
proposal. Furthermore, our model suggests that the maladaptive behavior found in (Den Bakker et al.,
2022) could be due to the impossibility for the higher, prefrontal component (level 2) to correctly update
itsbelief, basedonbottom-upmessagepassingfromthehippocampalcomponent(level1). Thisperspective
is coherent with the finding of (Den Bakker et al., 2022) that the only disruption of mPFC that prevents
spatial rule switching is one that directly follows hippocampal SWRs – hence highlighting the importance
of coherent HC - PFC reactivations to solve spatial alternation tasks. Finally, our simulations suggest that
other,moresevereinterruptionsofHC-PFCcommunication(Section2.3.2)couldproduceaspecificpattern
of maladaptive behavior – namely, perseverative behavior – that differs from those observed in the above
disruption experiments. This is a novel prediction that remains to be tested empirically.
BesideshelpingunderstandHC-PFCinteractions, oursimulations(AppendixD.3)suggestthatlooking
at the animal’s epistemic behavior (e.g., the selection of actions to self-localize and reduce uncertainty
about the current pose) during uncertain tasks could be important to elucidate its strategy; in particular,
whether it aims to maximize reward or also to minimize its uncertainty – as predicted by active inference
(Parr et al., 2022; Schwartenbeck et al., 2019; Rens et al., 2023). Future studies might assess whether
epistemicimperativesareimportantdriversofbehaviorduringspatialalternationorsimilartasks,especially
inconditionsofhighuncertainty,suchaswhentheanimalisrandomlyplacedinamaze(asinoursimulations)
or when spatial contingencies or task rules change unexpectedly.
It could, in principle, be possible to solve the navigation tasks studied in this article using a non-
hierarchical generative model with a single “map” (and a single CSCG) that encompasses both spatial
and task-related components. However, the hierarchical structure of the HAI generative model used in
this study better reflects the implicit division of labor between HC and PFC circuits, which is well estab-
lished empirically in rodent studies. For example, inactivating prefrontal structures during navigation tasks
tends to disrupt rule-related contingencies and deliberative behavior, while sparing spatial representation
Den Bakker et al. (2022); Schmidt et al. (2019). Therefore, our model reflects the widespread perspective
that goal-directed navigation depends on the coordinated interplay between (inferential) processes at two
levels, which could be associated with HC and PFC structures (Shin and Jadhav, 2016; Tang et al., 2021;
Patai and Spiers, 2021; Ito et al., 2015). The division of labor between HC and PFC is also central to
other prominent accounts, such as the Complementary Learning Systems framework, which highlights that
faster learning in HC facilitates slower learning in neocortex – with the latter integrating across episodes
to extract semantic structure (McClelland et al., 1995). Furthermore, from a computational perspective,
the hierarchical structure of the HAI model affords a useful factorization: learning a novel rule in a known
maze, as we did in Experiment 2, only requires re-train the higher-level CSCG, while leaving the lower-level
CSCG unchanged. This might be more problematic when using a single CSCG that combines spatial and
task-related information.
The current study has several limitations that will need to be addressed in future studies. First, for
efficiency reasons, we learned the CSCGs offline (before embedding them into the HAI), using a simplified
procedure: weusedpredefinedtrajectoriesthatexhaustivelycoveredtheW-mazeasinputsforthecognitive
map of physical space and 75% correct trajectories as inputs for the cognitive map of task space. In the
future, it would be interesting to train CSCG online, similar to (Lazaro-Gredilla et al., 2023), by guiding
the exploration through active inference dynamics (Friston et al., 2017a; Schwartenbeck et al., 2019; Parr
et al., 2022). A second research avenue is to relax the separation of the timescales between the two levels,
by selecting their inputs (e.g., level 1 takes all sensory observations as inputs, whereas level 2 only considers
observations that could be rewarding – and in particular, observation 1 in Figure 2b). In the future, it
would be interesting to explore methods to learn hierarchical models with multiple timescales (Yamashita
and Tani, 2008; Hinton et al., 2006) and effective state spaces for navigation and for task rules in self-
supervised (and/or reward-guided) ways, as shown in prior work (Stoianov et al., 2022, 2018, 2016; Niv,
2019). Thismightalsohelpunderstandthereciprocalinfluencesbetweencognitivemaplearningatdifferent
13
levels and in different (e.g., prefrontal versus hippocampal) brain structures. A third challenge is to avoid
having the agent learn from scratch each new maze or rule. Recent work in transfer learning shows that it
is possible to reuse existing cognitive maps or latent task representations to learn novel and similar tasks
much faster (Stoianov et al., 2022; Guntupalli et al., 2023; Stoianov et al., 2016; Swaminathan et al., 2023).
Extending our architecture with transfer learning abilities would be important to provide more accurate
models of how animals learn cognitive maps, especially given the strong evidence for the reuse of existing
neural sequences and cognitive maps in the hippocampus (Liu et al., 2019a; Farzanfar et al., 2023).
Another limitation of the model presented in this study is that in the CSCG of the task layer, we made
explicit the fact that the relevant task states are the ends of corridors, because the animals can (only)
acquire rewards in such states. This design choice reflects the animals’ knowledge, given that in the spatial
alternation task (Jadhav et al., 2012), the three arms had reward food wells at their endpoints. However,
future work should consider generative models that learn autonomously what the relevant task states are,
eveniftheyarenotexplicitlycuedasinJadhavetal.(2012). Thereisanestablishedliteratureshowingthat
latenttaskstatesrelevantforcognitivecontrolcanbelearnedautonomouslyusingBayesian(nonparametric)
methods(CollinsandFrank,2013;Stoianovetal.,2016,2018;GershmanandBlei,2012)andthattaskrules
can be learned using deep reinforcement learning (Wang et al., 2018). However, it remains to be assessed in
future research how to integrate these methods in the HAI model.
Future studies might consider how to adapt the HAI agent to robot navigation and planning. There is
increasing interest in applying active inference to robotic domains, given the appeal of its unified approach
tocontrol,stateestimation,andworldmodellearning(Lanillosetal.,2021;DaCostaetal.,2022;Taniguchi
et al., 2023). Hierarchical active inference is especially appealing, since it facilitates planning over long time
horizons. Indeed, the computational burden required to calculate the expected free energy of long policies
is high, but it can be substantially reduced by splitting work between (higher-level) policies that select (a
sequenceof)subgoal(s)alongwith(lower-level)policiesthatselectthespecificactionstoreacheachsubgoal
(Donnarumma et al., 2016). For example, one study built a hierarchical model for robot navigation, using
multiple layers of recurrent state space models (C¸atal et al., 2021). Another study realized a hierarchical
model for next-frame video prediction, using a subjective timescale for the predictions (Zakharov et al.,
2021). In another study (Van de Maele et al., 2023), a single layer CSCG was embedded within the active
inferenceframeworkandwasabletosupportnavigationindifferentmazes. Aninterestingdirectionoffuture
work could be adding a (learned) hierarchical layer that maps the high-dimensional observations space to
a discrete state space and stack the proposed hierarchical active inference on top with multiple layers of
CSCGs. This would potentially afford abstract reasoning and planning for complex tasks, directly from
sensory observations such as pixels.
Finally, future studies might consider in more detail the functional role and content of hippocampal
reactivations and replay in the hippocampus and other brain structures, such as the prefrontal cortex and
theventralstriatum(WilsonandMcNaughton,1994;Foster,2017;PfeifferandFoster,2013;Liuetal.,2019b;
Lansinketal.,2009;Peyracheetal.,2009;WittkuhnandSchuck,2021;Liuetal.,2018;Buzs´aki,2015,2019;
Gupta et al., 2010; Nour et al., 2021; Pezzulo et al., 2014a, 2017). Previous studies suggested that replay
might play different roles, which range from memory functions to planning, compositional computation and
the optimization of the brain’s generative models (Stoianov et al., 2022; Shin et al., 2017; Mattar and Daw,
2018; Pezzulo et al., 2019, 2021; Wittkuhn et al., 2021; Kurth-Nelson et al., 2023). However, these works
have mostly considered hippocampal replay, not coordinated replay in the hippocampus and the prefrontal
cortex (and other brain areas). It might be interesting to combine the insights of these studies with the
hierarchical architecture of the HAI agent, to test whether (for example) planning and model optimization
benefitfromthecombinedreplayofmultiplebrainstructures,asopposedtolocalreplayinthehippocampus.
4 Methods
Inthispaper,wedevelopahierarchicalplanningagentbycombiningtwocomponents: weuseclonestructured
cognitive graphs (CSCG) to learn “cognitive maps” of physical and task space; and hierarchical active
inference to form hierarchical plans. In the next sections, we explain the two key components of our
14
(a) (b)
Figure 6: Learned cognitive graphs. The arrows indicate possible transitions from one state to the next.
Thenodeinthecenterwhereallnodespointto,istheaddedandabsorbing“dummy”state. (a)Thelearned
CSCG for the physical space model (level 1). The color in the graph represents distinct observations, only
thestatesthatareactivewhenpursuingthespatialalternationrule(LCRC)arecolored,theotherstatesare
marked in gray. (b) The learned CSCG for the task space model (level 2). The color in the graph indicates
the states that were active when pursuing the spatial alternation rule (LCLR). While only four states are
necessarytoencodetherule,itcanbeobservedthatmanymorestatesareaddedbylearningtheotherpaths
that might have been visited during training.
approach: CSCG (Section 4.1) and hierarchical active inference (Sec. 4.2). Finally, we explain how we
combined these (Section 4.3).
4.1 Clone-structured cognitive graphs (CSCG) for learning cognitive maps of
physical and task space
Clone-structured cognitive graphs (CSCGs) are a probabilistic model for representing sequences of data,
e.g. a sequence of action-observation pairs (Gothoskar et al., 2019; George et al., 2021). They are a special
case of Hidden Markov Models (HMM), where each observation maps to a subset of the hidden states: the
so-called clones of this observation. While these states have the same observation likelihood, they differ in
the implied dynamics encoded in their transition model. Through the sequence of action-observation pairs,
specific clones will have a higher likelihood and can therefore disambiguate the aliased observations.
The CSCGs are learned using the expectation-maximization (EM) algorithm for HMMs (the Baum-
Welch algorithm) which maximizes the ELBO as described in George et al. (2021). During the E-step, the
posteriors over states are estimated through smoothing, i.e. the forward-backward algorithm. The M-step
then selects the optimal parameters for the transition model, given this sequence of visited states. For the
update equations, the reader is referred to George et al. (2021).
We consider two cognitive maps, that represent the W-maze task at two distinct levels. The first level
considers the structure of the environment (i.e. where can the agent walk and where are the walls), and the
second level encodes the task rule (i.e. in which order the corridors yields the reward). We learn these two
maps in a sequential fashion, using two distinct CSCGs.
We first collect a sequence of data for learning the spatial structure by exploring the maze. We designed
theexplorationsequencetoexhaustivelycovertheW-mazesuchthatapathfromeachposetoeachpossible
15
otherposeispresent. ThissequencewasusedtolearntheparametersofaCSCGforthefirst(physical)level,
using the expectation-maximization mechanism described in (George et al., 2021). We initialize the level 1
CSCG with 20 clone states per observation. After learning, the model is pruned using a Viterbi-decoding
algorithmasdescribedin(Georgeetal.,2021)andmappedtothePOMDPofthefirstlevelofthehierarchical
generative model (as will be explained in Section 4.3) for engaging in active inference. The learned graph is
visualized in Figure 6a. The nodes in the graph correspond to the learned nodes and their colors represent
the observations encountered in the states, during the correct execution of the spatial alternation task. The
graynodesrepresentotherstatesrequiredfortransitioningbetweenstatesortrajectoriesoutsidethecorrect
spatial alternation. Note that the graph shown in Figure 2a only shows the colored nodes, but not the gray
nodes.
We use the learned cognitive map of physical space to learn the cognitive map of task space. We first
extract the states that are distinctly representing the end of each corridor (i.e. matching with observation 1
inFigure 2b)anduse themasthe actionsforthe tasklevel. Whenan actionisselected, this meansthatthe
agentismovingtowardoneofthesestates,andthustheendofoneofthecorridors. Theobservationsatthis
level are the presence of reward (“1” if the agent is following the rule, “0” otherwise), and the reached level
1 state (physical level). To aid the learning process, we guide the agent to follow the rule 75% of the time.
We now learn a CSCG with 10 clones per observation using the same expectation-maximization scheme for
the single rule scheme, and 32 clones for the scenario with three rules. After learning, the model is pruned
using a Viterbi algorithm as described in (George et al., 2021) and mapped to the POMDP of the second
level of the hierarchical generative model to engage in active inference (see Section 4.3). For a robustness
analysis on the model capacity with respect to amount of rules, rule length and amount of clones per state,
the reader is referred to the appendix. We visualize the learned cognitive graph in Figure 6b, where the
states used when pursuing the rule are colored in red. It can be observed that the spatial alternation rule is
encoded in this graph, using four states, as this is the optimal amount required for the spatial alternation
rule (two states for the center, one for the left, and one for the right corridor). Note that the graph shown
in Figure 2a only shows the colored nodes, but not the gray nodes.
4.2 Hierarchical active inference
Active inference is a normative framework to describe cognitive processing and brain dynamics in living
organisms (Parr et al., 2022). It assumes that action and perception both minimize a common functional:
theminimizationofvariationalfreeenergy(whichisanupperboundtotheorganisms’surprise). Centralto
active inference is the idea that living organisms are equipped with generative models, capturing the causal
relations between observable outcomes, the agent’s actions, and hidden states.
It is important to note the difference between the agent’s generative model and the generative process.
The former represents the internal generative model that the agent uses to attribute consequences to its ac-
tions,whilethelatterrepresentsthetrueprocessfromwhichoutcomesaregeneratedintheworld. Crucially,
the agent is unable to observe the hidden state of the generative process, as they are separated through a
Markov blanket given the observation and action hidden variables. However, the agent is able to perform
actions and observe the generated outcomes (Parr et al., 2022; Pezzulo et al., 2018, 2015).
4.2.1 The hierarchical generative model
We endow the active inference agent with the hierarchical generative model depicted in Figure 7. In this
section, we provide a high-level overview of the generative model, whereas in Section 4.3 we discuss imple-
mentation details.
The hierarchical generative model illustrated in Figure 7 is split into two distinct levels. The highest
hierarchical level (level 2) reasons at a more abstract level, e.g. about which corridor to visit, while the
lowest level (level 1) considers the step-by-step navigation in the environment. Each of these levels can
be interpreted as an individual partially observable Markov decision process (POMDPs) (Kaelbling et al.,
1998), operating at different timescales: faster for level 1 and slower for level 2. However, crucially, the two
levels interact reciprocally by exchanging messages, as described below.
16
Figure7: The Hierarchical Generative Model: Thisgenerativemodelconsistsoftwohierarchicallevels,
where the top level operates at a slower timescale than the lower level. The policy at the highest level sets
the actions for the top level, which determines temporal transitions between states at level 2. The level
2 state then generates the presence of reward and the level 1 state. At level 1, the states generate the
observations and the policy generates the actions, depending upon the selected level 2 action. Finally, when
a policy is pursued, a message is passed to the upper level, transitioning the upper level to the next state.
This highlights the different timescales, whereas level 1 operates in the timescale of the agent’s movement,
the upper level operates at the timescale of the reached subgoals (visited corridors). Orange circles denote
observed variables, while white circles denote unobserved variables.
The hierarchical model supports the hierarchical selection of abstract policies that move the agent from
onegoaltotheother, followingthetaskrule(s)(atlevel2)andfromonespatiallocationtotheother, based
on the spatial map (at level 1). Conceptually, this hierarchical selection starts when the agent’s preferred
observation is set to find the reward (technically, in active inference, this is done by assuming a strong prior
for the reward observation) and it involves both levels.
The most fundamental goal of Level 2 is to select a policy π2 that moves the agent through task space,
by steering abstract actions a2 that follow the rule learned by the agent (or in the case of multiple rules,
t
as in Experiment 2, the rule currently inferred to be in place). These actions indicate the location that the
agent will then try to reach in the spatial map (level 1). In this way, the level 2-action conditions the level
1 policy π1. The level 2 state s2 is a latent representation of the corridor in which the agent is currently
t
located in, and where it comes from (i.e. where the agent is in rule space). This abstraction is enforced by
the choice of considering the reachable states to be matched to observation 1 (Figure 2b), reflecting the fact
thattheagentknowsthepotentialrewardlocations(which, intheanimalexperiments, arebaitedwithfood
wells). The agent only considers reward on level 2 and thus generates the presence of reward directly from
the level 2 state.
At level 1, the policy π1 is conditioned by level 2, by setting the preferred state as reaching one of the
17
spatiallocationsencodedinthecognitivegraph. Thepolicyπ1 generatesthelow-levelactionsthatnavigate
the agent from one pose (position and orientation, encoded in hidden state s1) to the next. Then, the
t
agent receives observations o from the environment, which reflect its current pose in the maze and which
t
permitupdatingthehiddenstateestimate. Notethatreachingthepreferredspatiallocationrequiresmaking
multipletransitionsatlevel1,butlevel2onlymakesatransitionbetweenstateswhenthepreferenceatlevel
1isreached(orequivalentlywhenthelevel2actionis“performed”). Thiscreatesa separationoftimescales
because multiple transitions at level 1 are nested between two subsequent actions at level 2.
4.2.2 Active inference
As shown in Figure 7, the generative model considered in this paper is a hierarchically stacked (two levels)
POMDP. In order to reduce complexity, we first discuss active inference for a single layer POMDP, as
depicted in Figure 8. At time step t, we have observation o , action a generated from policy π, and state
t t
s . The generative model is factorized as:
t
(cid:89)
P(˜s,˜a,˜o)=P(s ) P(o |s )P(s |s ,a )P(a ), (1)
0 t t t t−1 t−1 t−1
t=1
where the tilde represents a sequence over time. As computing the posterior distribution over the state
is intractable for large state spaces, we resort to variational inference and introduce the approximate poste-
rior Q(˜s|˜o,˜a), the variational free energy measures the discrepancy between the joint distribution and the
approximate posterior:
F =E [logQ(˜s|˜o,˜a)−logP(˜s,˜a,˜o)] (2)
Q(˜s|˜o,˜a)
Active inference agents minimize this quantity through learning (by optimizing the model parameters),
perception (by estimating the most likely state), and planning (by selecting the policy or action sequence
that results in the lowest expected free energy).
The expected free energy G is a quantity that is only used during planning and represents the agent’s
variational free energy expected after pursuing a policy π. It is distinct from the variational free energy,
because it requires considering future observations generated from the selected policy, as opposed to the
current (and past) observations.
Active inference realizes goal-directed behavior by selecting policies that minimize expected free energy
–and thatare expected toyield observationsthat arecloser topreferredobservations(or priorpreferences).
This is done by setting the approximate posterior of the policy proportional to this quantity:
Q(π)=σ(γG(π)+logE), (3)
where σ is the softmax function, γ is a temperature variable, and E is a prior distribution over actions,
or habit. Actions are sampled according to this posterior distribution, where low temperatures yield more
deterministic behavior. At a given timestep τ, G is computed for a given policy according to the following
quantity as formalized in (Heins et al., 2022):
G(π,τ)=−E (cid:2) D [Q(s |o ,π)||Q(s |π)] (cid:3) −E (cid:2) logP(o) (cid:3) (4)
Q(oτ|π) KL τ τ τ Q(oτ|π)
+E [D (Q(s |o )||P(s |o ,π))] (5)
Q(oτ|π) KL τ τ τ τ
(cid:124) (cid:123)(cid:122) (cid:125)
ExpectedApproximationError≥0
≥−E (cid:2) D [Q(s |o ,π)||Q(s |π)] (cid:3) −E (cid:2) logP(o) (cid:3) (6)
Q(oτ|π) KL τ τ τ Q(oτ|π)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Epistemicvalue PragmaticValue
This quantity is decomposed into an epistemic value and a pragmatic value (Friston et al., 2017a). When
evaluating G, we compute the upper bound shown in Equation (6). We thus make the assumption that
the expected approximation error (Equation (5)) is zero (Heins et al., 2022). The epistemic value computes
the expected information gain (Bayesian surprise) between the prior Q(s |π) and posterior Q(s |o ,π).
τ τ τ
18
Intuitively, this quantity represents how much the agent expects the belief over the state to shift when
pursuing this policy. The pragmatic value then measures the expected log probability of observing the
preferred observation under the selected policy, intuitively computing how likely it is that this policy will
drivetheagenttoitspriorpreferences. ThefullexpectedfreeenergyforafinitetimehorizonT iscomputed
as
(cid:80)T
G(π,τ).
τ=1
4.2.3 Hierarchical active inference
Here, we discuss how the active inference scheme introduced above is extended to realize hierarchical per-
ception and planning, through bottom-up and top-down message passing between the two levels of the
hierarchical generative model shown in Figure 7.
During perception, first, the low-level state s1 is inferred given observations o and level 1 actions a1
t t
using the inference mechanism implemented in PyMDP (Heins et al., 2022). When the free energy of the
lowest level reaches a pre-specified threshold, a message containing the most likely level 1 state s1 the agent
t
isinispassedtothelevelabove(level2). Thisthresholdischosentobereachedwhenthelevel1-preference
is reached, and uncertainty is below a fixed level. When level 2 gets this bottom-up message, it queries the
environment for the presence of reward and observes r . This conjunction of observations, together with
τ
high-level action a2 is used to infer the belief over the level 2 state Q(s2), in the same way as done for level
1.
During planning, i.e. generating a sequence of actions that (should) drive the agent toward its goal,
policies are generated in a top-down manner, in the sense that goals set at level 2 determine the prior
preference (and then in turn the policy) at level 1. First, the level 2 policy is selected by sampling from
the approximate posterior over π2 proportional to its expected free energy. The action selected at this level
then conditions the policy at level 1 π1, for which in turn the approximate posterior Q(π1) is computed.
The low-level action a1 is then sampled according to this distribution, similar to the hierarchical generative
t
model described by Friston et al. (Friston et al., 2017b).
As depicted in Figure 7, the policy of the lower level sets a limited amount of steps. In particular, when
the preference of the lower level is reached, a message is passed to the upper level, transitioning it into the
next state. In this way, inferring the policy at the lower level only considers one action of the upper level at
a time.
The posterior over the level 2 policy Q(π2) is first inferred, as it has no dependencies from above. To do
this, the expected free energy is computed, for which we look one time step ahead in our implementation
as this could predict the next corridor in which the agent encounters reward. We consider Equation 7, for
which the observations are now a conjunction of reward r and level 1-states s1:
τ
G2(π2,τ)≥−E (cid:2) D [Q(s2|r ,s1,π2)||Q(s2|π2)] (cid:3) −E (cid:2) logP(r ,s1 ) (cid:3) , (7)
Q(rτ,s1
t=τ
|π2) KL τ τ t τ Q(rτ,s1
t=τ
|π2) τ t=τ
Note that the level 1 state is synchronized with the level 2 state, and only the state observed at this
synchronized time is considered, hence we denoted the time index for the level 1 state by t=τ. In practice,
weusedatemperature(γ)valueof0.5forthislevel. ForthehabitpolicyE,weuseacategoricaldistribution
over action, that is conditioned on the current state of the agent. The action a2 is then sampled according
τ
to Q(π2).
The agent, however, is not able to act upon the physical world yet with this level 2 action. The agent
now has to infer the posterior distribution over the policy at level 1 Q(π1). Because we now want to move
towardsaspecificstate, i.e. adisambiguatedobservation, wesetthepreferenceinstatespaceforcomputing
the expected free energy at level 1. In practice, Equation 6, for level 1 then boils down to:
G1(π1,t)≥−E (cid:2) D [Q(s1|o ,π1)||Q(s1|π1)] (cid:3) −E (cid:2) logP(s1) (cid:3) , (8)
Q(ot|π1) KL t t t Q(s1
t
|π1) t
where the first term yields the expected information gain after pursuing policy π1, and the latter the
expectedutility. Inotherwords,howclosetheexpectedstateisfromthepreferredstate,setbytheactionof
thelevelabove. Thisquantityisthenusedtoapproximatetheposterioroverthepolicyatlevel1fromwhich
19
Figure 8: Factor graph of a POMDP: The conditional dependencies in Hidden Markov Models are
parameterized by a set of matrices. The A matrix parameterizes the likelihood model, i.e. how states s
t
map to observations o . The B matrix parameterizes the transition model, i.e. how state s changes at
t t+1
each timestep, dependent on policy π. The C vector denotes the preferred observation or state, and directly
influences the G factor which conditions π. The policy π depends on the habit E. Orange circles denote
that the variable is observed.
action a1 is sampled. At this level, we set a temperature (γ) value of 0.5, and a uniform habit distribution.
t
Crucially, in order to evaluate G1 at planning depth 1, we set the prior preference (C matrix) as such that
the preference for each state is proportional to the distance to the goal, i.e. intuitively this means that the
agent can follow a breadcrumb trail towards the goal, given that it properly inferred the current state.
4.3 Casting CSCGs as partially observable Markov decision processes
CSCGs can be used directly to plan (George et al., 2021). However, in this work, we use the two learned
CSCGs to create a hierarchical generative model for active inference – and then use hierarchical active
inference to solve the spatial alternation tasks.
UsingthetwoCSCGstocreateahierarchicalgenerativemodelforactiveinferencerequiresmappingthem
intotwopartiallyobservableMarkovdecisionprocesses(POMDPs)(Kaelblingetal.,1998),themathematical
framework used in discrete time active inference.
Practically,aPOMDPisdescribedbyasetoffourarraysasshowninFigure8. Wedescribethesearrays
using the symbol notation typically adopted in discrete time active inference (Parr et al., 2022). The A
matrix encompasses the likelihood model P(o|s), or how states are mapped to observations. The B tensor
entailsthetransitionmodelP(s |s ,a ),orhowstateschangeovertime,conditionedontheselectedaction.
t+1 t t
TheCvectorsetstheprioroverfutureobservationsorstates,dependingontheimplementation. Thisvector
is used within active inference to embed the preference or goal state/observation into the agent. Finally, the
D vector describes the initial belief over the state P(s).
As mentioned earlier, clone-structured cognitive graphs are a special case of hidden Markov models and
can be therefore easily mapped to POMDPs (which extend HMMs, too). First, we consider the mapping of
the likelihood. This A matrix represents the mapping of observation to state, specifically A = P(o |s ).
i,j i j
ThematrixcanbeconstructedthroughthedefinitionoftheCSCG,adeterministicmappingofclonestateto
its observation is set: P(o |s )=1∀s ∈C(o ) and P(o |s )=0∀s ∈/ C(o ), where C(o ) denotes all clone
i j j i i j j i i
states for observation o . When constructing the model we only consider states for which the marginalized
i
probability P(s ) surpasses a threshold of 0.0001. Finally, we add an additional “dummy” state to the
j
additional “dummy” observation, to which unlikely actions are mapped (see below).
To define the transition model, or B tensor, we use the learned parameters of the model. Through the
learning process described in Section 4.1, the CSCG has learned the transition probability P(s |s ,a ).
t+1 t t
This is parameterized as a tensor for which B =P(s |s ,a ). We construct this tensor with the learned
i,j,k i j k
20
probabilities, for the same states that are considered in the likelihood matrix. Note that for ease of imple-
mentation, the active inference routines that we used require that any action could be executed from any
state. However, in our task, some actions are not available or highly unlikely in some states (e.g., turning
actions in a corridor). To handle this, we created a “dummy state” in the POMDP, which corresponds to
observation 20 in Figure 2b to which we map the “highly unlikely” actions and to which we assign a large
negativevalue(seebelow)toensurethatitisneverselectedduringplanning. Inpractice,wesetatransition
probability of 10−12 from every state to the dummy state (after which we re-normalize the tensor to sum
to one for each action and state). We also set a self-transition of 1 for the dummy state, therefore ensuring
that it is “absorbing”.
We endow the active inference agent with prior preferences to secure rewards in the spatial alternation
tasks in the C matrix. For ease of implementation, we set these prior preferences manually, rather than ex-
tractingthemfromthelearnedparametersoftheCSCG.Forlevel2,wesetapreferenceovertheconjunction
of observations where the reward is 1 to encourage the agent to follow the learned rule. As described above,
whenthelevel2actionisselected, itsetstheCmatrixoflevel1toalargepreferenceforthepreferredstate
and to a value proportional to the distance (in the learned state space of level 1) from said given state to
the preferred state (except for the dummy state, for which the C vector has a fixed value. This is set to the
lowest value, making the dummy state the least preferred state).
TheprioroverthestateisparameterizedbytheDmatrix. Weparameterizethisasauniformdistribution
over all the states, except for the dummy state. This reflects the fact that in our simulations, the active
inference agent is placed in the W-maze with a random pose and has to self-localize.
Finally, thehabit, ortheprioroveractionisparameterizedbytheEtensor. Forlevel1, weparameterize
thisasauniformdistribution. Forlevel2,wemodelthishabitasaDirichletdistribution,thatisconditioned
on the current state of the agent, and is proportional to actions that were rewarding during learning.
Using these tensors, inference at each single level can be implemented as a Bayes filter, iteratively
computing the posterior over state at each timestep using the following update:
qs =σ(A·o +B ·qs ), (9)
t t πt−1 t−1
where qs denotes the parameters of the categorical distribution over state, i.e. Q(s |o ,π)=Cat(qs1), o
t τ τ τ t
represents the observation as a one-hot vector, σ denotes the softmax function, and the B-tensor is sliced
by the policy π (Friston et al., 2017a). In the first timestep, qs is initialized as the prior matrix D.
t−1 0
The expected free energy G for a considered policy π can then be evaluated using these tensors for each
t
future timestep τ. As we specify the preference over state for the first level, and over observation for the
second level, the distinction is made explicitly. For the first level, this boils down to (Parr et al., 2022):
G1 =diag(A1⊤·lnA1)·qs1 −qo1 ·lnqo1 +qs1 ·lnC1, (10)
τ τ τ τ τ
where the superscript 1 denotes the first level, qs1 denotes the parameters of the categorical distribution
τ
over state, i.e. Q(s1|o1,π1) = Cat(qs1), which is computed as qs1 = B ·qs1. The parameters of the
τ τ t τ τ 1,π2 t
t
categoricalovertheobservationisdenotedbyqo1,i.e. Q(o1)=Cat(qo1),andiscomputedasqo1 =A1·qs1.
τ τ τ τ τ
The same equation can be used for the expected free energy at the second level, except now the preference
is specified as an expected observation:
G2 =diag(A2⊤·lnA2)·qs2 −qo2 ·lnqo2 +qo2 ·lnC2, (11)
τ τ τ τ τ
where the superscript 2 denotes the second level. Q(s2|o2,π1) = Cat(qs2), which is computed as qs2 =
τ τ t τ τ
B ·qs2. Theparametersofthecategoricalovertheobservationatlevel2isdenotedbyqo2,i.e. Q(o2)=
2,π2 t τ τ
t
Cat(qo2), and is computed as qo2 = A2 ·qs2. The full expected free energy G can then be acquired by
τ τ τ
summing over time horizon T: G =
(cid:80)T
G , and the posterior over the policies is achieved through
τ=t+1 τ
Q(π)=σ(γ·G+lnE).
21
Acknowledgements
ThisresearchreceivedfundingfromtheEuropeanUnion’sHorizon2020FrameworkProgrammeforResearch
andInnovationundertheSpecificSpecificGrantAgreementsNo. 945539(HumanBrainProjectSGA3)and
No. 952215 (TAILOR); the European Research Council under the Grant Agreement No. 820213 (ThinkA-
head), the Italian National Recovery and Resilience Plan (NRRP), M4C2, funded by the European Union
– NextGenerationEU (Project IR0000011, CUP B51E22000150006, “EBRAINS-Italy”; Project PE0000013,
CUP B53C22003630006, ”FAIR”; Project PE0000006, CUP J33C22002970002 “MNESYS”), and the PRIN
PNRR P20224FESY. The GEFORCE Quadro RTX6000 and Titan GPU cards used for this research were
donatedbytheNVIDIACorporation. Thefundershadnoroleinstudydesign, datacollectionandanalysis,
decision to publish, or preparation of the manuscript.
References
Basu, R., Gebauer, R., Herfurth, T., Kolb, S., Golipour, Z., Tchumatchenko, T., and Ito, H. T. (2021). The
orbitofrontal cortex maps future navigational goals. Nature, 599(7885):449–452.
Bellmund, J.L., G¨ardenfors, P., Moser, E.I., andDoeller, C.F.(2018). Navigatingcognition: Spatialcodes
for human thinking. Science, 362(6415):eaat6766.
Benchenane,K.,Tiesinga,P.H.,andBattaglia,F.P.(2011). Oscillationsintheprefrontalcortex: agateway
to memory and attention. Current Opinion in Neurobiology, 21(3):475–485.
Bogacz, R. (2017). A tutorial on the free-energy framework for modelling perception and learning. Journal
of Mathematical Psychology, 76:198–211.
Buckley, C. L., Kim, C. S., McGregor, S., and Seth, A. K. (2017). The free energy principle for action and
perception: A mathematical review. Journal of Mathematical Psychology, 81:55–79.
Buzs´aki, G. (2015). Hippocampal sharp wave-ripple: A cognitive biomarker for episodic memory and plan-
ning. Hippocampus, 25(10):1073–1188.
Buzs´aki, G. (2019). The brain from inside out. Oxford University Press.
Buzs´aki, G. and Moser, E. I. (2013). Memory, navigation and theta rhythm in the hippocampal-entorhinal
system. Nature neuroscience, 16(2):130–138.
Chen,Y.,Zhang,H.,Cameron,M.,andSejnowski,T.(2022). Predictivesequencelearninginthehippocam-
pal formation. bioRxiv, pages 2022–05.
Chevalier-Boisvert,M.,Willems,L.,andPal,S.(2018). Minimalisticgridworldenvironmentforgymnasium.
Colgin, L. L. (2011). Oscillations and hippocampal–prefrontal synchrony. Current opinion in neurobiology,
21(3):467–474.
Collins,A.G.andFrank,M.J.(2013). Cognitivecontroloverlearning: creating,clustering,andgeneralizing
task-set structure. Psychological review, 120(1):190.
Da Costa, L., Lanillos, P., Sajid, N., Friston, K., and Khan, S. (2022). How Active Inference Could Help
Revolutionise Robotics. Entropy, 24(3):361.
DenBakker,H.,VanDijck,M.,Sun,J.-J.,andKloosterman,F.(2022). Sharp-waverippleassociatedactivity
in the medial prefrontal cortex supports spatial rule switching. preprint, Neuroscience.
Donnarumma, F., Maisto, D., and Pezzulo, G. (2016). Problem solving as probabilistic inference with
subgoaling: explaining human successes and pitfalls in the tower of hanoi. PLoS computational biology,
12(4):e1004864.
22
Epstein, R. A., Patai, E. Z., Julian, J. B., and Spiers, H. J. (2017). The cognitive map in humans: spatial
navigation and beyond. Nature neuroscience, 20(11):1504–1513.
Farzanfar, D., Spiers, H. J., Moscovitch, M., and Rosenbaum, R. S. (2023). From cognitive maps to spatial
schemas. Nature Reviews Neuroscience, 24(2):63–79.
Foster, D. J. (2017). Replay comes of age. Annual review of neuroscience, 40:581–602.
Friston, K. (2010). The free-energy principle: a unified brain theory? Nature Reviews Neuroscience,
11(2):127–138.
Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., and Pezzulo, G. (2017a). Active Inference: A
Process Theory. Neural Computation, 29(1):1–49.
Friston, K. J., Rosch, R., Parr, T., Price, C., and Bowman, H. (2017b). Deep temporal models and active
inference. Neuroscience & Biobehavioral Reviews, 77:388–402.
George, D., Rikhye, R. V., Gothoskar, N., Guntupalli, J. S., Dedieu, A., and L´azaro-Gredilla, M. (2021).
Clone-structuredgraphrepresentationsenableflexiblelearningandvicariousevaluationofcognitivemaps.
Nature Communications, 12(1):2392.
Gershman, S. J. and Blei, D. M. (2012). A tutorial on bayesian nonparametric models. Journal of Mathe-
matical Psychology, 56(1):1–12.
Gothoskar,N.,Guntupalli,J.S.,Rikhye,R.V.,L´azaro-Gredilla,M.,andGeorge,D.(2019). Differentclones
fordifferentcontexts: Hippocampalcognitivemapsashigher-ordergraphsofaclonedhmm. bioRxiv,page
745950.
Guntupalli, J. S., Raju, R. V., Kushagra, S., Wendelken, C., Sawyer, D., Deshpande, I., Zhou, G., L´azaro-
Gredilla, M., and George, D. (2023). Graph schemas as abstractions for transfer learning, inference, and
planning. arXiv preprint arXiv:2302.07350.
Gupta, A. S., Van Der Meer, M. A., Touretzky, D. S., and Redish, A. D. (2010). Hippocampal replay is not
a simple function of experience. Neuron, 65(5):695–705.
Hafting, T., Fyhn, M., Molden, S., Moser, M.-B., and Moser, E. I. (2005). Microstructure of a spatial map
in the entorhinal cortex. Nature, 436(7052):801–806.
Heins, C., Millidge, B., Demekas, D., Klein, B., Friston, K., Couzin, I., and Tschantz, A. (2022). pymdp: A
Python library for active inference in discrete state spaces. arXiv:2201.03904.
Hinton, G. E., Osindero, S., and Teh, Y.-W. (2006). A fast learning algorithm for deep belief nets. Neural
computation, 18(7):1527–1554.
Isomura, T., Kotani, K., Jimbo, Y., and Friston, K. J. (2023). Experimental validation of the free-energy
principle with in vitro neural networks. Nature Communications, 14(1):4547.
Ito, H. T., Zhang, S.-J., Witter, M. P., Moser, E. I., and Moser, M.-B. (2015). A prefrontal–thalamo–
hippocampal circuit for goal-directed spatial navigation. Nature, 522(7554):50–55.
Jadhav, S. P., Kemere, C., German, P. W., and Frank, L. M. (2012). Awake Hippocampal Sharp-Wave
Ripples Support Spatial Memory. Science, 336(6087):1454–1458.
Jones, M. W. and Wilson, M. A. (2005). Theta Rhythms Coordinate Hippocampal–Prefrontal Interactions
in a Spatial Memory Task. PLoS Biology, 3(12):e402.
Kaelbling, L. P., Littman, M. L., and Cassandra, A. R. (1998). Planning and acting in partially observable
stochastic domains. Artificial intelligence, 101(1-2):99–134.
23
Khodagholy, D., Gelinas, J. N., and Buzs´aki, G. (2017). Learning-enhanced coupling between ripple oscilla-
tions in association cortices and hippocampus. Science, 358(6361):369–372.
Kurth-Nelson, Z., Behrens, T., Wayne, G., Miller, K., Luettgau, L., Dolan, R., Liu, Y., and Schwartenbeck,
P. (2023). Replay and compositional computation. Neuron, 111(4):454–469.
Lanillos,P.,Meo,C.,Pezzato,C.,Meera,A.A.,Baioumy,M.,Ohata,W.,Tschantz,A.,Millidge,B.,Wisse,
M., Buckley, C. L., and Tani, J. (2021). Active Inference in Robotics and Artificial Agents: Survey and
Challenges. arXiv:2112.01871.
Lansink, C. S., Goltstein, P. M., Lankelma, J. V., McNaughton, B. L., and Pennartz, C. M. (2009). Hip-
pocampus leads ventral striatum in replay of place-reward information. PLoS biology, 7(8):e1000173.
Lazaro-Gredilla, M., Deshpande, I., Swaminathan, S., Dave, M., and George, D. (2023). Fast exploration
and learning of latent graphs with aliased observations. Publisher: arXiv Version Number: 3.
Levenstein, D., Efremov, A., Eyono, R. H., Peyrache, A., and Richards, B. A. (2024). Sequential predictive
learning is a unifying theory for hippocampal representation and replay. bioRxiv, pages 2024–04.
Liu,K.,Sibille,J.,andDragoi,G.(2018). Generativepredictivecodesbymultiplexedhippocampalneuronal
tuplets. Neuron, 99(6):1329–1341.
Liu, K., Sibille, J., and Dragoi, G. (2019a). Preconfigured patterns are the primary driver of offline multi-
neuronal sequence replay. Hippocampus, 29(3):275–283.
Liu, Y., Dolan, R. J., Kurth-Nelson, Z., and Behrens, T. E. (2019b). Human Replay Spontaneously Reorga-
nizes Experience. Cell, 178(3):640–652.e14.
Mattar,M.G.andDaw,N.D.(2018). Prioritizedmemoryaccessexplainsplanningandhippocampalreplay.
Nature neuroscience, 21(11):1609–1617.
McClelland, J. L., McNaughton, B. L., and O’Reilly, R. C. (1995). Why there are complementary learning
systemsinthehippocampusandneocortex: insightsfromthesuccessesandfailuresofconnectionistmodels
of learning and memory. Psychological review, 102(3):419.
Niv, Y. (2019). Learning task-state representations. Nature neuroscience, 22(10):1544–1553.
Nour, M. M., Liu, Y., Arumuham, A., Kurth-Nelson, Z., and Dolan, R. J. (2021). Impaired neural replay of
inferred relationships in schizophrenia. Cell, 184(16):4315–4328.
O’Keefe, J. and Dostrovsky, J. (1971). The hippocampus as a spatial map. preliminary evidence from unit
activity in the freely-moving rat. Brain Research Volume, 34:171–175.
O’Keefe,J.andNadel,L.(1979).Pr´ecisofO’Keefe&Nadel’sThehippocampusasacognitivemap.Behavioral
and Brain Sciences, 2(4):487–494.
Parr, T., Pezzulo, G., and Friston, K. J. (2022). Active Inference: The Free Energy Principle in Mind,
Brain, and Behavior. The MIT Press.
Patai, E. Z. and Spiers, H. J. (2021). The Versatile Wayfinder: Prefrontal Contributions to Spatial Naviga-
tion. Trends in Cognitive Sciences, 25(6):520–533.
Peyrache, A., Khamassi, M., Benchenane, K., Wiener, S. I., and Battaglia, F. P. (2009). Replay of rule-
learningrelatedneuralpatternsintheprefrontalcortexduringsleep. Nature neuroscience,12(7):919–926.
Pezzulo, G., Donnarumma, F., Maisto, D., and Stoianov, I. (2019). Planning at decision time and in the
background during spatial navigation. Current opinion in behavioral sciences, 29:69–76.
24
Pezzulo, G., Kemere, C., and Van Der Meer, M. A. (2017). Internally generated hippocampal sequences
as a vantage point to probe future-oriented cognition. Annals of the New York Academy of Sciences,
1396(1):144–165.
Pezzulo, G., Rigoli, F., and Friston, K. (2015). Active inference, homeostatic regulation and adaptive
behavioural control. Progress in neurobiology, 134:17–35.
Pezzulo,G.,Rigoli,F.,andFriston,K.J.(2018). Hierarchicalactiveinference: atheoryofmotivatedcontrol.
Trends in cognitive sciences, 22(4):294–306.
Pezzulo, G., Van der Meer, M. A., Lansink, C. S., and Pennartz, C. M. (2014a). Internally generated
sequences in learning and executing goal-directed behavior. Trends in cognitive sciences, 18(12):647–657.
Pezzulo, G., Verschure, P. F., Balkenius, C., and Pennartz, C. M. (2014b). The principles of goal-directed
decision-making: from neural mechanisms to computation and robotics.
Pezzulo, G., Zorzi, M., and Corbetta, M. (2021). The secret life of predictive brains: what’s spontaneous
activity for? Trends in Cognitive Sciences, 25(9):730–743.
Pfeiffer,B.E.andFoster,D.J.(2013). Hippocampalplace-cellsequencesdepictfuturepathstoremembered
goals. Nature, 497(7447):74–79.
Raju, R. V., Guntupalli, J. S., Zhou, G., L´azaro-Gredilla, M., and George, D. (2022a). Space is a latent
sequence: Structured sequence learning as a unified theory of representation in the hippocampus. arXiv
preprint arXiv:2212.01508.
Raju, R. V., Guntupalli, J. S., Zhou, G., L´azaro-Gredilla, M., and George, D. (2022b). Space is a latent se-
quence: Structuredsequencelearningasaunifiedtheoryofrepresentationinthehippocampus. Publisher:
arXiv Version Number: 1.
Recanatesi, S., Farrell, M., Lajoie, G., Deneve, S., Rigotti, M., and Shea-Brown, E. (2021). Predictive
learning as a network mechanism for extracting low-dimensional latent space representations. Nature
communications, 12(1):1417.
Rens,N.,Lancia,G.L.,Eluchans,M.,Schwartenbeck,P.,Cunnington,R.,andPezzulo,G.(2023). Evidence
for entropy maximisation in human free choice behaviour. Cognition, 232:105328.
Schmidt, B., Duin, A. A., and Redish, A. D. (2019). Disrupting the medial prefrontal cortex alters hip-
pocampal sequences during deliberative decision making. Journal of neurophysiology, 121(6):1981–2000.
Schuck, N., Cai, M., Wilson, R., and Niv, Y. (2016). Human Orbitofrontal Cortex Represents a Cognitive
Map of State Space. Neuron, 91(6):1402–1412.
Schwartenbeck, P., Passecker, J., Hauser, T. U., FitzGerald, T. H., Kronbichler, M., and Friston, K. J.
(2019). Computational mechanisms of curiosity and goal-directed exploration. eLife, 8:e41703.
Shin, H., Lee, J. K., Kim, J., and Kim, J. (2017). Continual Learning with Deep Generative Replay.
arXiv:1705.08690 [cs].
Shin, J. D. and Jadhav, S. P. (2016). Multiple modes of hippocampal–prefrontal interactions in memory-
guided behavior. Current Opinion in Neurobiology, 40:161–169.
Siapas, A. G., Lubenov, E. V., and Wilson, M. A. (2005). Prefrontal phase locking to hippocampal theta
oscillations. Neuron, 46(1):141–151.
Simons,J.S.andSpiers,H.J.(2003). Prefrontalandmedialtemporallobeinteractionsinlong-termmemory.
Nature reviews neuroscience, 4(8):637–648.
25
Smith, R., Friston, K. J., and Whyte, C. J. (2022). A step-by-step tutorial on active inference and its
application to empirical data. Journal of Mathematical Psychology, 107:102632.
Spiers, H. J. (2008). Keeping the goal in mind: Prefrontal contributions to spatial navigation. Neuropsy-
chologia, 46(7):2106–2108.
Stachenfeld, K. L., Botvinick, M. M., and Gershman, S. J. (2017). The hippocampus as a predictive map.
Nature neuroscience, 20(11):1643–1653.
Stoianov, I., Genovesio, A., and Pezzulo, G. (2016). Prefrontal goal codes emerge as latent states in proba-
bilistic value learning. Journal of Cognitive Neuroscience, 28(1):140–157.
Stoianov, I., Maisto, D., and Pezzulo, G. (2022). The hippocampal formation as a hierarchical generative
model supporting generative replay and continual learning. Progress in Neurobiology, 217:102329.
Stoianov, I. P., Pennartz, C. M., Lansink, C. S., and Pezzulo, G. (2018). Model-based spatial navigation
in the hippocampus-ventral striatum circuit: A computational analysis. PLoS computational biology,
14(9):e1006316.
Sun, W., Winnubst, J., Natrajan, M., Lai, C., Kajikawa, K., Michaelos, M., Gattoni, R., Fitzgerald, J. E.,
andSpruston,N.(2023). Learningproducesahippocampalcognitivemapintheformofanorthogonalized
state machine. preprint, Neuroscience.
Swaminathan, S., Dedieu, A., Raju, R. V., Shanahan, M., Lazaro-Gredilla, M., and George, D. (2023).
Schema-learning and rebinding as mechanisms of in-context learning and emergence. Publisher: arXiv
Version Number: 1.
Tang,W.,Shin,J.D.,andJadhav,S.P.(2021). Multipletime-scalesofdecision-makinginthehippocampus
and prefrontal cortex. eLife, 10:e66227.
Taniguchi,T.,Murata,S.,Suzuki,M.,Ognibene,D.,Lanillos,P.,Ugur,E.,Jamone,L.,Nakamura,T.,Ciria,
A.,Lara,B.,andPezzulo,G.(2023). Worldmodelsandpredictivecodingforcognitiveanddevelopmental
robotics: frontiers and challenges. Advanced Robotics, 37(13):780–806.
Tolman, E. C. (1948). Cognitive maps in rats and men. Psychological review, 55(4):189.
VandeMaele, T., Dhoedt, B., Verbelen, T., andPezzulo, G.(2023). Integratingcognitivemaplearningand
active inference for planning in ambiguous environments. Publisher: arXiv Version Number: 1.
Verschure, P. F., Pennartz, C. M., and Pezzulo, G. (2014). The why, what, where, when and how of goal-
directed choice: neuronal and computational principles. Philosophical Transactions of the Royal Society
B: Biological Sciences, 369(1655):20130483.
Wang, J. X., Kurth-Nelson, Z., Kumaran, D., Tirumala, D., Soyer, H., Leibo, J. Z., Hassabis, D., and
Botvinick, M. (2018). Prefrontal cortex as a meta-reinforcement learning system. Nature neuroscience,
21(6):860–868.
Whittington, J. C., Muller, T. H., Mark, S., Chen, G., Barry, C., Burgess, N., and Behrens, T. E. (2020).
TheTolman-EichenbaumMachine: UnifyingSpaceandRelationalMemorythroughGeneralizationinthe
Hippocampal Formation. Cell, 183(5):1249–1263.e23.
Whittington, J. C. R., McCaffary, D., Bakermans, J. J. W., and Behrens, T. E. J. (2022). How to build a
cognitive map. Nature Neuroscience, 25(10):1257–1272.
Wilson, M. A. and McNaughton, B. L. (1994). Reactivation of Hippocampal Ensemble Memories During
Sleep. Science, 265(5172):676–679.
26
Wittkuhn, L., Chien, S., Hall-McMaster, S., and Schuck, N. W. (2021). Replay in minds and machines.
Neuroscience & Biobehavioral Reviews, 129:367–388.
Wittkuhn, L. and Schuck, N. W. (2021). Dynamics of fmri patterns reflect sub-second activation sequences
and reveal replay in human visual cortex. Nature communications, 12(1):1795.
Yamashita,Y.andTani,J.(2008). Emergenceoffunctionalhierarchyinamultipletimescaleneuralnetwork
model: a humanoid robot experiment. PLoS computational biology, 4(11):e1000220.
Zakharov, A., Guo, Q., and Fountas, Z. (2021). Variational Predictive Routing with Nested Subjective
Timescales. Publisher: arXiv Version Number: 2.
C¸atal, O., Verbelen, T., Van De Maele, T., Dhoedt, B., and Safron, A. (2021). Robot navigation as
hierarchical active inference. Neural Networks, 142:192–204.
27
Supplementary materials: Bridging Cognitive Maps: a Hierarchi-
cal Active Inference Model of Spatial Alternation Tasks and the
Hippocampal-Prefrontal Circuit
A Learning the spatial level using a Hidden Markov Model
We investigate the benefit of using the CSCG’s for learning the distinct layers of the hierarchy over using
standard Hidden Markov Models on a navigation task. Specifically, we learn the spatial level using both
approaches on the same dataset: a trajectory of the agent that contains a path from each distinct pose
(position and orientation) to another. We consider the following variants, where the model is used by an
active inference agent implemented in pymdp (Heins et al., 2022): (i) a CSCG agent, as used in the main
text, (ii) a HMM agent trained using the Baum-Welch algorithm (on the same data), and (iii) a HMM fine
tuned by the Viterbi algorithm (on the same data).
The performance of each agent is measured as the success rate for reaching each corridor end from each
considered starting pose in the maze within 30 steps. Since the observations are ambiguous, and the goals
are specified in state space, the corresponding state is first extracted using the following mechanism: an
agent is started in an unambiguous position (the T-junction) and the path to a corridor end is replayed,
while the states are inferred. The resulting state is the state for which the agent has encoded this corridor
end. From the results shown in Table 2, it is clear that only the CSCG is able to properly navigate the
ambiguous W-maze.
We determine the cause for the low performance of the HMM agents to be the inability to disambiguate
the distinct corridor ends. As shown in Figure 9, this fails even when starting from unambiguous positions,
e.g. the bottom row of the maze has some unique identifiers such as the T-junction and the 2 corners. In
contrast, the CSCG encodes each corridor end in a distinct state.
Table 2: Success ratio for spatial navigation: The performance for reaching the three corridor ends as
a goal location using the different models (CSCG, HMM, HMM+Viterbi) starting from each starting pose:
16 positions × 4 directions × 3 goals = 192 trajectories.
success ratio % success
CSCG Agent 192/192 100.0 %
HMM Agent 23/192 11.9%
HMM + Viterbi Agent 32/192 16.7%
B Behavioral patterns observed in the W-maze
ThedifferentbehavioralpatternsfromDenBakkeretal.(2022)aredepictedinFigure10,specificallyforthe
W-maze considered in this paper. There are four distinct patterns: (i) Alternation: the pattern of following
a rule thet is in place. (ii) Rotated alternation: the pattern of following a rule that is currently not in place.
(iii)Backandforth: iteratingovertwocorridors,and(iv)Circling: visitingeachcorridorinacyclicpattern.
In both Den Bakker et al. (2022) and our study, these are measured over sub-sequences of four consecutive
corridor visits.
C Model specification
The hyperparameters of the model are depicted in Table 3. The threshold to determine when a message
shouldbepassedtothelevelaboveisF1 ,andisrelatedtothemax-valueC usedintheconstruction
threshold max
ofthepreferenceorCmatrix. Thepolicylengthparametersdeterminehowmanystepsareconsideredwhen
evaluating the expected free energy.
28
(a) (b)
(c)
Figure 9: State disambiguation for the W-maze: This figure displays the replayed trajectory from
unambiguouslocationsinthebottomrowofthemazetowardeachofthecorridorends. Thetrajectoriesare
colored by the final inferred state, i.e. the state that encodes the end of the corridor.
(a)
Figure10: BehavioralpatternsfortheW-maze: Visualizationofmultiplebehavioralpatternsmeasured
over 4 visits of corridors. The dotted line indicates the trajectory, and the numbers in the corridor ends
indicate the order of the visit. (i) alternation: following the rule, (ii) rotated alternation: following another
rule that is currently not in play, (iii) back-and-forth: iterating between two corridors, and (iv) circling:
cyclic evaluation of the different corridors.
29
Table3: Thehyperparametersofthehierarchicalactiveinferencemodel. Thesuperscriptindicatesthelevel
for which this parameter is used (e.g. γ1 is the temperature for level 1.)
Parameter Value
γ1 0.5
γ2 0.5
F1 -14.8
threshold
C1 15
max
C2 1
max
policy length1 5
policy length2 2
n clones1 20
n clones2(1 rule) 10
n clones2(3 rules) 32
(a) (b)
Figure 11: Sensitivity analysis for γ. (a) The mean reward acquired for different values of γ in both the
spatialandtasklevel. Weconsideredvaluesfrom0.5to20instepsof0.5. (b)Successrateforahierarchical
active inference (HAI), a disrupted HAI agent, and a random agent trained on a single rule. The values are
computed over 20 trials, of 300 steps.
D Control analyses for robustness
D.1 Sampling temperature γ
We performed a sensitivity analysis on the sampling temperature (γ parameter) for both levels, as shown
in Figure 11a. We observed that the specific value of γ does not have a large influence on performance, as
assessed by mean reward collected.
We also conducted the same simulation as in Section 2.2, but with a sampling temperature of 16. Fig-
ure 11b reports the mean success rate of visiting consecutive corridors while following the rule for an agent
with and without disruption. This figure indicates that, without disruption, the agent is able to follow the
rule and is near-perfect in collecting reward for both in- and outbound trajectories. When the disruption
is applied, (i.e. the transition model of the highest level is impaired), the agent can still solve the inbound
scenario, but drops to a chance-level performance for the outbound scenario. The higher value of γ ensures
that the agent leverages it’s generative model more (i.e. the distribution over the next action has lower
entropy) and is closer to greedily selecting the lowest expected free energy. This ensures that the agent
will not select the central corridor to visit, as the generative model can predict that this will not yield any
reward.
Wenotethatthechoiceoftemperature γ hasalargeimpactonthegeneratedbehavior(Table4). When
30
Table 4: Behavioral patterns during disruption: Classified observed behavioral patterns according
to Den Bakker et al. (2022) in different scenarios for agents trained on the three rules indicated by (3) and
a single rule indicated by (1). Agent’s policies are sampled using a sampling temperature γ of 16.
HAI HAI HAI HAI Random
control (1) disrupt (1) control (3) disrupt (3)
Alternation 80.48% 61.80% 87.06% 0.00 % 3.47%
Rotated alternation 0.00% 0.00 % 0.00% 0.00 % 8.24%
Back and forth 2.41% 26.26% 4.31% 83.77% 8.86%
Circling 0.00% 0.80 % 4.85% 0.00 % 5.86%
Other 17.11% 11.14% 3.77% 16.23% 73.75%
using a large value of γ, the agent resorts to greedy behavior (with respect to the expected free energy),
while lower values allow the agent to sample according to the distribution over actions. We observe that
with a temperature of 16, most of the generated behavior can be categorized in the back and forth category
(83.77%).
D.2 Control analysis: model capacity
Weperformedarobustnessanalysisofthemodelcapacity(amountofparameters,lengthoftherules,andthe
amount of rules) by evaluating the average collected reward. The experiment considers 5 trials of 900 steps
per trial, where the rule switches every 150 steps. The models are trained using the same hyperparameters
from the main text, i.e. the amount of clones from Table 3, and a sequence of 8000 corridor visits. In the
case of multiple rules as in Experiment 2 (LCLR, LCRC, and RCRL), the rule switches every 1000 visits.
In Figure 12a, the average collected reward is visualized in function of the amount of clone states, after
a warmup period of 50 steps to infer the rule. We observe that the performance remains constant when we
vary theamountof clones. Weobservethatfora singleclone, themodeldoesnothavethecapacity tolearn
the rule for both models. We observe that the model is able to learn (an average collected reward of over
80%) the single rule starting at 4 states, and three rules from 8 clone states.
Next, we evaluated the impact of rule length. In the main text, all rules had a fixed length of four. We
now lengthen this rule by adding more corridors (e.g. for the single rule case this becomes: 4: LCRC, 5:
LCRCR,6: LCRCRC,and7: LCRCRCL).Weobserveadropinperformancewhentherulesbecomelonger,
indicating that the model does not have enough capacity to learn the rules. Note that in rule 5, the (R)ight
corridor can be followed by a (L)eft or (C)enter corridor, while in the other rules this is always (C)enter,
which could cause this change.
Finally, we evaluated the performance of our agent when multiple rules of length four are considered.
For this experiment, we now consider a sequence of 1050 steps, where the rule switches every 150 steps. We
consider an agent with 32 clones and vary the amount of learned rules between one and eight. We observe
that the average collected reward remains similar.
D.3 Control analysis: noisy scenario
We conducted a robustness study that evaluates how robust the active inference model is in case of a noisy
environment. Consider the same W-maze where a single rule is in place, however, now whenever the agent
enters a corridor, there is a 1/3 chance that the agent’s prior beliefs (of both the navigation and prefrontal
model) are reset to a uniform distribution. This means that, after this reset, the agent needs to infer what
rule it is currently in, and where it currently is.
We compared two agents: one that uses the expected free energy functional described in the main text
(Active Inference agent) and another that only uses the utility and thus acts as a greedy agent (Utility
agent). Figure 13a shows that the average success rate (i.e. the ratio of corridor visits that yield reward) of
thetwoagentstrainedonthesamerule,over300trials,issignificantlygreaterfortheActiveInferenceagent
31
(a)
(b)
(c)
Figure 12: Capacity analysis of the hierarchical CSCG. Each point is computed over 5 trials of length
900 steps, where the rule switches every 150 steps if multiple rules are in place. (a) The average collected
reward in function of the number of clones for both the one and three rule case. (b) The average collected
reward in function of rule length. (c) The average collected reward in function of the amount of learned
rules.
32
compared to the Utility agent (p-value 0.0147). This is because while the Utility agent only seeks reward
by reaching the end of corridors, the Active Inference agent seeks unambiguous locations to self-localize.
Figure 13b shows that the Active Inference agent also needs fewer steps to reach rewards, with a mean of
23.48 steps for the Active Inference agent and 26.3 steps for the Utility agent, but this difference does not
reachsignificance. Thisisprobablybecausethiscountalsoincludesthestepsrequiredtoreachunambiguous
locations, which are greater for the Active Inference than for the Utility agent.
(a) (b)
Figure 13: Control Experiment: Robustness of the agent in a noisy scenario where the agent
state is reset with a 33.3% probability upon entering a corridor. (a) The success rate of an active
inference and a purely utility agent in visiting the corridors in the W-maze according to a fixed rule, in
the noisy environment. The active inference agent has higher accuracy in the choice of the correct corridor
accordingtotherule(t-testp=0.0147). Thevaluesarecomputedover20trialsof300steps. (b)Theaverage
amount of steps between visiting a rewarding corridor. There is no significant difference between the visit
times for both agents (t-test p=0.348) The values are computed over 20 trials of 300 steps.
33

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Bridging Cognitive Maps: a Hierarchical Active Inference Model of Spatial Alternation Tasks and the Hippocampal-Prefrontal Circuit"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
