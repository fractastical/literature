=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: An Active Inference Model of Covert and Overt Visual Attention
Citation Key: mišić2025active
Authors: Tin Mišić, Karlo Koledić, Fabio Bonsignorio

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Key Terms: task, sensory, cueing, overt, visual, inference, precisions, active, model, covert

=== FULL PAPER TEXT ===

5202
yaM
6
]VC.sc[
1v65830.5052:viXra
An Active Inference Model of Covert and Overt
Visual Attention
Tin Misˇic´, Karlo Koledic´, Fabio Bonsignorio, Ivan Petrovic´, and Ivan Markovic´1
Abstract—The ability to selectively attend to relevant stimuli
whilefilteringoutdistractionsisessentialforagentsthatprocess
complex,high-dimensionalsensoryinput.Thispaperintroducesa
modelofcovertandovertvisualattentionthroughtheframework
of active inference, utilizing dynamic optimization of sensory
precisions to minimize free-energy. The model determines visual
sensory precisions based on both current environmental beliefs
and sensory input, influencing attentional allocation in both
covert and overt modalities. To test the effectiveness of the
model, we analyze its behavior in the Posner cueing task and a
simple target focus task using two-dimensional(2D) visual data.
Reactiontimesaremeasuredtoinvestigatetheinterplaybetween
exogenousandendogenousattention,aswellasvalidandinvalid
cueing.Theresultsshowthatexogenousandvalidcuesgenerally
lead to faster reaction times compared to endogenous and
invalidcues.Furthermore,themodelexhibitsbehaviorsimilarto
inhibitionofreturn,wherepreviouslyattendedlocationsbecome
suppressed after a specific cue-target onset asynchrony interval.
Lastly,weinvestigatedifferentaspectsofovertattentionandshow
Fig.1:Atthecoreoftheproposedmodelarethebeliefsabout
that involuntary, reflexive saccades occur faster than intentional
ones, but at the expense of adaptability. the causes of sensory inputs. These beliefs and action signals
Index Terms—active inference, visual attention, Posner cueing are updated through attractor goals and error updates to min-
task imize free-energy. The dedicated bottom-up attention module
regulates attention through dynamic sensory precisions.
I. INTRODUCTION
Attentionasacognitiveprocessallowsagentstoselectively
focus on specific stimuli while ignoring others. This ability activesaccadesinvisualsearch[9]–[12],theintegrationofvi-
helps humans avoid sensory overload, and as robots acquire sualattentionandbottom-upactionfromrawtwo-dimensional
more complex sensory channels it could help decrease the visual data within the active inference framework remains
computational load required to perform in daily tasks, such as unexplored.Thisisparticularlyimportantinrobotics,asvision
objecttrackingandvisualsearch,aswellassocialinteractions is a fundamental sensory modality, with images serving as a
[1]–[3]. Attention is often separated into top-down, or goal- primary source of perceptual input for decision-making and
driven attention, and bottom-up or stimulus-driven attention, interaction with the environment.
with some theories including hysteresis as a third component Visual attention and its models are most often tested using
[4]. Top-down attention bilaterally activates dorsal posterior the Posner cueing task, i.e., the Posner paradigm. The Posner
parietal and frontal regions of the brain, while bottom-up cueing task is an experimental paradigm used to study covert
attention activates the right-lateralized ventral system, with visualattention[17],[18].Participantsareaskedtofixateona
the dorsal frontoparietal system combining the two into a central point while a cue directs attention to a location where
“salience map” during visual search [5], [6]. Furthermore, a target may appear. The cue can either be endogenous –
visualattentionisseparatedintoovertandcovertattention[7], meaningthatattentionisvoluntarilyguidedbasedonsymbolic
[8], with overt attention involving saccadic eye movements to cues (e.g., an arrow pointing left or right), or exogenous –
theattentionaltarget,andcovertattentionreferringtoattention meaning that attention is automatically drawn by a sudden,
shifts to the target while the eyes remain fixated elsewhere. peripheral stimulus (e.g. a bright flash or a flickering box).
Multiple approaches exist to model attention, more numerous Endogenous cueing is considered to be top-down because it
being those that are based on Bayesian inference [9]–[16]. requires cognitive processing and active interpretation of the
While previous studies have modeled visual attention and cue, while exogenous cueing is considered to be bottom-up
because it does not require conscious interpretation. Reaction
This research has been supported by the H2020 project AIFORS under timesandaccuracyaremeasuredtoassesshowcuesinfluence
GrantAgreementNo952275 attentional shifts.
1University of Zagreb Faculty of Electrical Engineering and Computing,
Croatia;Correspondence:tin.misic@fer.hr,ivan.markovic@fer.hr Through the original Posner paradigm [17], [18] and its
variations,valuableinsightshavebeengainedaboutattentional gence and the surprise [9], [29], [30]:
processes. Covert attentional shifts to a target area occur prior
F(z,s)=−L(q)
toanyeyemovement[18],[19],andvalidcuesproducefaster
(1)
responses than invalid cues [17], [18]. Exogenous cues were =D KL [q(z)||p(z|s)]−lnp(s),
shown to produce faster reaction times than endogenous cues
where z and s represent latent system states and sensory ob-
[20], [21], showing that bottom-up attention is faster because
servations, respectively, while the KL-divergence is computed
it requires no conscious processing. The question of weather
between the posterior p(z|s) and the approximate variational
attentionalselectionisobject-basedorlocation-basedhasalso
densityq(z).Giventhat,thesurpriseisdefinedasthenegative
been thoroughly researched, and the consensus is that both
log-probability of an outcome −lnp(s). If the variational
types are not mutually exclusive, but are dependent on the
density q(z) is assumed to factor into Gaussian probability
current task [22]–[24]. Research supporting location-based
density functions (pdfs) [9], [29], [32]:
attention has shown that the distance from the focus point
playsaroleinreactiontime,withreactiontimesincreasingas q(z)= (cid:89) q(z )= (cid:89) N(µ ,Π−1), (2)
i i i
target eccentricity increased [25]–[27].
i i
Inthispaperweproposeamodelofvisualattention,shown
the free-energy then becomes dependent only on the most
in Fig. 1, viewed through the lens of active inference [28]
probable hypotheses, beliefs µ , and precision matrices Π
– a computational approach derived from the free-energy i i
of the latent system states z [9], [32]:
principle (FEP). According to the FEP, systems adapt and
act in a way that minimizes their free-energy [29]. Free-
F(µ,s)=−lnp(s,µ)+C
energy is a concept borrowed from physics, statistics, and (3)
=−lnp(s|µ)−lnp(µ)+C.
informationtheorythatlimitsthesurpriseonasampleofdata
given a generative model. This principle helps to explain how
Furthermore, sensory observations s and beliefs µ are
biological systems resist the natural tendency to disorder, and
definedinthecontextofhierarchicaldynamicmodels[9],[29],
their action, perception, and learning processes [30]. In the
[30], [32]:
FEP, attention is theoretically achieved by optimizing sensory
s˜=g˜(µ˜)+w
s
precisions, their parameters, and mutual precision weighing (4)
Dµ˜ =f˜(µ˜)+w .
[9]–[14],[31].Biasedcompetitionandendogenous/exogenous µ
attention have been studied in this context, and the precision
Here, µ˜ indicates generalized coordinates of beliefs with
optimization produces behaviors similar to human attention multiple temporal orders, µ˜ = {µ,µ′,µ′′,···}, which allow
[9], [15].
for a richer approximation of the environment dynamics, D
The contribution of this paper is an active inference model stands for the differential shift operator Dµ˜ = {µ′,µ′′,···}
of overt and covert visual attention by investigating precision in the generalized equation of system dynamics f˜(µ˜), while
optimization for visual data and how it generates endoge- g˜(µ˜) is the sensor model that maps current beliefs to sensory
nous/exogenous attention and action control. The proposed observations. The amplitudes of random fluctuations w and
s
modelincludesbothtop-downandbottom-upvisualattention, w are state dependent and are defined as Gaussian pdfs with
µ
aswellascovertandovertshiftsinattention.Theseproperties covariances Σ and Σ , respectively [9], [32]:
s µ
aredemonstratedthroughthePosnercueingtaskandasimple
targetfocustaskonvisual2Ddata.Avariationalauto-encoder w s ∼N(µ i ,Σ s (z,s,γ)) (5)
(VAE) was used for the visual generative model, and model w ∼N(µ ,Σ (z,s,γ)).
µ i µ
training and experiments were done in the Gazebo simulator
in the Robot Operating System (ROS). The precision matrices Π i are the inverses of these covari-
ances, Π := Π (z,s,γ) = Σ (z,s,γ)−1, with precision
The paper is organized as follows. In Sec. II we give i i i
parameters γ that control the amplitudes [9], [15]. The pre-
an overview of the theoretical background and elaborate the
cisions are dynamic and depend on the current states and
proposed approach that is based on free-energy minimization
sensoryinput.Itisthroughoptimizationofprecisionsandtheir
with 2D precision optimization and overt saccades through
parameters that attention is achieved [9]–[14], [31].
active inference. Section III shows the results of the Posner
cueing tasks and active attention trials. Section IV provides
B. Perceptual and Active Inference
the discussion of the results while Sec. V concludes the paper
and provides directions for future work. Perception, action, and learning can all be optimized
through the minimization of free-energy. In this paper we
only consider perception and action, and leave the learning
II. PROPOSEDMETHOD
processes of attention for future work. Action and beliefs are
A. Free-energy Minimization optimized through gradient descent [28]–[30], [32]:
Free-energyisdefinedasthenegativeevidencelowerbound µ˜˙ −Dµ˜ =−∂ F(µ˜,s˜)
µ˜
(6)
(ELBO), or as the sum of the Kullback-Leibler (KL) diver- a˙ =−∂ F(µ˜,s˜).
a
(cid:104) (cid:105)
The likelihood and prior in (3) also become generalized • 1 2 Tr Π˜− s 1∂ ∂ Π˜ µ˜ s − 1 2 e˜T s ∂ ∂ Π˜ µ˜ se˜ s : free-energy gradients
and can be partitioned within and across temporal orders d, fromthesensoryprecisions,servesasbottom-upattention
respectively [32]: • 2 1Tr (cid:104) Π˜− µ 1∂ ∂ Π˜ µ˜ µ (cid:105) − 1 2 e˜T µ ∂ ∂ Π˜ µ˜ µe˜ µ : free-energy gradients
p(s˜|µ˜)= (cid:89) p(s[d]|µ[d]) fromthesystemdynamicsprecisions,servesastop-down
attention.
d (7)
p(µ˜)= (cid:89) p(µ[d+1]|µ[d]). 2) Action update: Action is also updated through the min-
imization of free-energy [28]–[30], [32]:
d
These partitions are also assumed to take the following Gaus- a=argminF(µ,s), (12)
a
sian pdf form:
with the action update taking the following form:
p(s[d]|µ[d])= |Π s [d]|1 2 exp (cid:18) − 1 e[d]T Π [d]e[d] (cid:19) ∂s˜T
(cid:113) (2π)L 2 s s s a˙ =−∂ a F(µ,s)=− ∂a Π˜ s e˜ s
(8) (cid:34) (cid:35) (13)
p(µ[d+1]|µ[d])= (cid:113) |Π ( µ 2π [d] ) | M 1 2 exp (cid:18) − 1 2 e[ µ d]T Π µ [d]e[ µ d] (cid:19) , + 2 1 Tr Π˜− s 1 ∂ ∂ Π˜ s˜ s ∂ ∂ s a ˜ − 1 2 e˜T s ∂ ∂ Π˜ s˜ se˜ s ∂ ∂ s a ˜ ,
with bottom-up attention components in relation to sensory
where L and M are the respective dimensions of sensory input, analogous to those in relation to belief in (11). These
observations s and internal beliefs µ. Therein, e[ s d] and e µ [d] control signals act as reflexive saccades [33], [34]. The gra-
represents sensory and system dynamics prediction errors: dient ∂s˜ is an inverse mapping from sensory data to actions,
∂a
which is usually considered a ”hard problem” [35].
e[d] =s[d]−g[d](µ[d])=s[d]−p[d]
s (9) The implementations of all gradients in terms of belief,
e µ [d] =µ[d+1]−f[d](µ[d]), action and sensory input are elaborated in Appendix A.
where p[d] = g[d](µ[d]) are sensory predictions generated by III. RESULTS
the generative sensor model. Note that in our case the system
A. Implementation of the proposed model
dynamics model is defined through flexible intentions h(k)
The graphical representation of the developed model1 can
[32], where for each intention k ∈(0,K−1):
be seen in Fig. 1. The current belief µ is passed as input
f(k)(µ)=l·E(k)+w(k) =l·(h(k)−µ)+w(k), (10) to exteroceptive, proprioceptive, and interoceptive generative
i µ µ
models. The predictions p of these models are compared to
with l being the gain of intention errors E(k). The imple- the actual sensory input s and the prediction errors e are
i s
mentation of the generative sensor models g[d] is presented in used to drive action, as well as to update the current beliefs.
subsection III-A. The generative models for proprioceptive (camera pitch and
1) Belief update: With state- and sensory-dependent preci- yaw) and interoceptive (symbolic cue signals) sensory input
sions, the belief update takes the following form: are trivial identity matrices, while the generative model for
the exteroceptive visual sensory input is the decoder of a
µ˜˙ =Dµ˜+
∂g˜T
Π˜ e˜ +
∂f˜T
Π˜ e˜ −DTΠ˜ e˜ disentangled variational auto-encoder (VAE). The VAE has
∂µ˜ s s ∂µ˜ µ µ µ µ been trained to disentangle the position of the target in the
1 (cid:34) ∂Π˜ (cid:35) 1 ∂Π˜ image, as well as the target’s presence in the image. This
+ 2 Tr Π˜− s 1 ∂µ˜ s − 2 e˜T s ∂µ˜ se˜ s (11) disentanglementsimplifiestheconversionfromintrinsicimage
coordinates to extrinsic camera orientation angles. The VAE
(cid:34) (cid:35)
1 ∂Π˜ 1 ∂Π˜ architecture, training and latent space encoding are elaborated
+ Tr Π˜−1 µ − e˜T µe˜ ,
2 µ ∂µ˜ 2 µ ∂µ˜ µ in Appendix B.
The belief state is composed of the following components:
with Tr being the trace of a matrix. The terms that comprise • Symboliccuebelief–interoceptiveendogenouscueswill
the belief update equation are: present the cue position on the image, and this belief
• ∂ ∂ µ g˜ ˜ T Π˜ s e˜ s : likelihood error computed at the sensory should mirror that from the sensory input
level, representing the free-energy gradient of the like- • Camera orientation belief – proprioceptive belief over
lihood relative to the belief µ˜[d] in (9) the extrinsic pitch and yaw angles of the camera viewing
• r ∂ ∂ e µ f ˜ p ˜T re Π s ˜ e µ n e t ˜ i µ ng :b th a e ck fr w e a e r - d en e e r r r g o y rf g r r o a m die th n e t n re e l x a t ti t v e e m t p o o t r h a e lo b r e d li e e r f , • t V h i e su e a n l vi b ro e n li m ef en - t an encoding of the exteroceptive visual
µ˜[d+1] in (9) input, disentangled to encode the target position and
• −DTΠ˜ µ e˜ µ : forward error coming from the previous presence so they are easily interpreted
temporal order, representing the free-energy gradient rel-
1The implemented model is available at: https://github.com/TinMisic/
ative to the belief µ˜[d] in (9) AIF---visual-attention/tree/ICDL
• Covert attention belief - belief over the amplitude and
center of a radial basis function (RBF) used to calculate
the visual sensory precisions.
ThesensorydataandbeliefshapesareelaboratedinAppendix
C.Thebeliefsareupdatedthroughbottom-uppredictionerror
gradients, as well as through top-down attractors α generated
from the current beliefs, according to the flexible intentions
theory proposed in [32]. These goal-directed intentions en-
courage action through the proprioceptive camera orientation,
(a)Visualsensoryprecisionmatrix (b)Sensoryprecisionfree-energygra-
aswellascovertattentionthroughtheshiftsoftheRBFcenter dient,µ˙u=−0.839
and amplitude.
Fig. 2: The center of the RBF is (-0.25, 0.0), while the error
Sensory precision Π for the visual input is dynamic and
s appears at (-0.75, 0.0). The u-component of the RBF center
calculated based on the current overt attention belief and
is pushed toward the error with the update µ˙ =−0.839.
u
sensory input. We assume that there is no correlation between
individual pixels, so Π is defined as:
s
 
π (µ,s) 0 ··· 0
1
 0 π 2 (µ,s) ··· 0 
Π s = 

. .
.
. .
.
... . .
.
 

, (14)
0 0 ··· π (µ,s)
L L×L
where L = 32×32(×3) is the dimensionality of the visual
data.Wefurtherassumethattheindividualprecisionfunctions
π (µ,s)aredeterminedbyRBFsbasedonthecovertattention
i
center and the presence of a target-specific property, in our
case the color red: Fig. 3: Trial sequence of events. The model is first initialized
for 10 steps, then a cue appears for 50 simulation steps. The
π (µ,s)=π(x,y,µ,s)= cue is then removed for a variable interval, known as cue-
i
µ (cid:18) (cid:18) (x−µ )2+(y−µ )2 (cid:19) (cid:19) target onset asynchrony (CTOA). After that the target appears
amp ln − u v +1 +c until it is detected by the model or 1000 steps have passed.
2 b2
1 (cid:18) (cid:18) (x−r (s))2+(y−r (s))2 (cid:19) (cid:19)
+ ln − u v +1 +c ,
2 b2
both endogenous and exogenous cueing in valid and invalid
(15)
settings. The endogenous cue is given through the symbolic
where [µ ,µ ,µ ] are covert attention beliefs,
amp u v cue signal which has to be processed into an intention that
[r (s),r (s)] is the centroid of the biggest red object.
u v moves both the center of covert attention and the belief over
The parameters of the precision function, b=2.6 and c=1,
thesphere’sposition.Theexogenouscueisabriefappearance
are empirically chosen to ensure that the RBF values span
ofthetargetobject,whichmovesthecenterofcovertattention
from 0 to 1 across the image area. The shape of the RBF was
through the bottom-up free-energy gradient from the sensory
chosen so that the belief update pushes the covert attention
precision,andthebeliefoverthesphere’spositionthroughthe
toward the area of the image with the highest error, while
likelihooderrorfromtheVAE.Avalidcuesettingiswhenthe
a Gaussian RBF would push it away from the error. The
target appears at the same position as the cue, and an invalid
sensory precision matrix generated by this RBF and the
cue setting when the target appears at a position opposite of
resulting free-energy gradient caused by a prediction error
the cue with respect to the central focus point.
can be seen in Fig. 2. The precision nevertheless decreases
For each of the four task variations, N = 200 trials were
the further a point is from the focus center, mimicking human
conducted.Foreachtrial,thepositionofthetargetisrandomly
foveation [26], [27].
generated with varying distance from the focus point. Fig. 3
B. Simulating the Posner Cueing Task shows the sequence of events in a single trial. As this cueing
ThePosnercueingtaskisusedtodemonstratetheproposed task is meant to test covert attention, overt attention through
model’s exogenous and endogenous covert attention. The action signals was disabled.
model’s sensory inputs are its current camera orientation, a The reaction time in simulation steps as a function of
symbolic cue signal and visual data of an empty scene in distancefromthefocuspointisshowninFig.4,foreachofthe
which a red sphere might appear as a target. Note that the fourtaskvariations.Sincetheinternalbeliefsaboutthecovert
endogenous cue is given through the interoceptive sensory attention and the sphere position are easily interpretable, we
channel, not as an arrow in the visual channel as illustrated caneasilyseetheshiftsofcovertattentionandsphereposition
in Fig.3. We performed four variations of the cueing task, for belief for the valid task variations in Fig. 5.
300
200
100
0
0 2 4 6 8 10
Target distance from focus point (px)
)spets(
emiT
noitcaeR
Endogenous-Valid
Endogenous-Invalid
Exogenous-Valid
Exogenous-Invalid
Fig.4:Reactiontimesandtheiraveragesasafunctionoftarget
distance from focus point (CTOA = 100 for each trial)
6
4
2
0
0 50 100 150 200 250
Simulation Time (steps)
)xp(
retnec
morf
ecnatsiD
200
180
160
140
120
100
0 100 200 300 400 500 600
Cue-Target Onset Asynchrony (steps)
Endogenous
Exogenous
Fig. 5: Covert attention center (dashed lines) and sphere
position beliefs (solid lines) during valid trials, for both
endogenous and exogenous cues. The horizontal line is the
true target distance from center, and the vertical lines indicate
trial events as in Fig. 3: the cue appears at step 10, disappears
at step 60, target appears at step 160.
To examine the effect that the CTOA interval plays in
reaction time, the previous trial variations were performed for various CTOA lengths. The average reaction times are shown
in Fig. 6.
C. Action Signals from Bottom-up Attention
Since action can be determined from free-energy optimiza-
tion, overt attention in the form of eye saccades or camera
orientation changes can be as well implemented. Here we ex-
amined focus reach times for two action-update contributions:
• Top-down proprioceptive action signals: − ∂ ∂ a s˜T Π˜ s e˜ s –
these are determined from the prediction error of the
proprioceptive channel, between the proprioceptive input
)spets(
emiT
noitcaeR
Endogenous-Valid
Endogenous-Invalid
Exogenous-Valid
Exogenous-Invalid
Fig. 6: Average trial reaction time as a function of CTOA.
Results are shown for endogenous-valid, endogenous-invalid,
exogenous-valid, exogenous-invalid task variations.
400
300
200
100
0 2 4 6 8 10 12 14 16
Target distance from focus point (px)
)spets(
emiT
hcaeR
Bottom-up Action
Top-down action
Fig. 7: Reach times and their averages for different initial
target distances.
and current proprioceptive beliefs (which are attracted to
higher intentions)
• Bottom-up visual precision action signals:
(cid:104) (cid:105) 1 2 Tr Π˜− s 1∂ ∂ Π˜ s˜ s ∂ ∂ a s˜ − 1 2 e˜T s ∂ ∂ Π˜ s˜ se˜ s∂ ∂ a s˜ – these are
determined through the bottom-up derivative of the
precision matrix. Since the action update is dependent
only on the sensory input, only the second half of (15)
contributes to the action update.
The trials start with a 10-step initialization interval, after
which the target appears at a random position in the agent’s
field of view. The trial is finished when the agent successfully
focuses the target at the center of its field of view. The reach
times as a function of the initial target distance can be seen
in Fig. 7.
IV. DISCUSSION compared to bottom-up orienting. Although bottom-up overt
orienting is faster, it can only effectively orient to one point
Ourproposedmodelwastestedonexogenous,endogenous,
in the visual area, while top-down overt orienting can handle
validandinvalidvariationsofthePosnerparadigm,aswellas
multiple objects through multiple flexible intentions (at the
on a simple target reach task. It captures the effects of both
cost of speed). We leave multiple-object overt attention for
endogenous and exogenous attention, as well as the impact of
future work.
cuevalidity,alongwithovertattentionbehaviorsininvoluntary
actions, all of which have been observed in location-based
V. CONCLUSION
modelsandhumanexperimentaldata.FromtheresultsinFig.
4 we can conclude the following: In this paper, we have proposed an active inference model
• Onaverage,validcuesproducefasterreactiontimesthan of covert and overt visual attention. The proposed model
invalid ones [17], [18]. This can be explained by the successfully demonstrates known attentional phenomena and
location-based encoding of the target and the location- mechanismsinthecontextofthePosnercueingtaskandasim-
based covert focus in the visual image. This produces pleactiveorientingtask.Itshowsthatvalidcuesproducefaster
a spotlight effect suggested in location-based models reactiontimesthaninvalidcues,andthatexogenouscuespro-
of attention [22]–[24]. An invalid cue causes a greater duce faster reaction times than endogenous cues. The model
shift of the ”spotlight” upon target onset, thus increasing also successfully demonstrates location-based attention, with
reaction time. reaction times increasing with target eccentricity. Although
• Bottom-up exogenous cues produce faster reaction times not modeled, the developed model exhibits behavior similar
than top-down endogenous cues [20], [21]. Bottom-up to inhibition of return, with previously cued areas becoming
exogenous cues by error gradients through the VAE de- suppressedafteracertaincue-targetonsetasynchronyinterval.
coderarefasterandrequirenointerpretationinhigherin- Futureworkwillinvestigatethisemergenceofinhibitionof
tentional areas, unlike top-down endogenous cues which return, as well as extend the model with multiple possible
require intentional interpretation of symbolic cues to targets/intentions to further test object-based and location-
update target belief. based effects. Overt saccades will also be examined further,
• reaction times for every trial variation increase as target with a focus on varying attraction to different objects. We
eccentricity increases [25]–[27]. This is a result of the plan to further develop and test this framework as a model of
location-based object encoding, as well as the shape of perception, learning, and action in autonomous robots.
the RBF used for the precision matrix.
REFERENCES
RegardingtheshiftsincovertattentiondemonstratedinFig.
5, covert attentional focus is much faster to update than the [1] P.Lanillos,J.F.Ferreira,andJ.Dias,“Designinganartificialattention
beliefoverthetarget’slocation,inthecaseofbothendogenous system for social robots,” in 2015 IEEE/RSJ International Conference
onIntelligentRobotsandSystems(IROS),IEEE,Sept.2015.
and exogenous cues. This mirrors the findings that covert
[2] P. Lanillos, E. Dean-Leon, and G. Cheng, “Multisensory object dis-
attentional shifts occur quickly, before conscious perception covery via self-detection and artificial attention,” in 2016 Joint IEEE
of target [18], [19] or active overt shifts in attention [33], InternationalConferenceonDevelopmentandLearningandEpigenetic
Robotics(ICDL-EpiRob),IEEE,Sept.2016.
[34].
[3] P. Lanillos, J. F. Ferreira, and J. Dias, “Multisensory 3d saliency for
Fig. 6 illustrates the effect of different CTOA intervals on artificialattentionsystems,”092015.
reaction times, with invalid cues leading to faster reaction [4] S.Shomstein,X.Zhang,andD.Dubbelde,“Attentionandplatypuses,”
WileyInterdiscip.Rev.Cogn.Sci.,vol.14,p.e1600,Jan.2023.
times than valid ones in the exogenous variation after longer
[5] M.CorbettaandG.L.Shulman,“Controlofgoal-directedandstimulus-
CTOAintervals(∼350steps),andintheendogenousvariation drivenattentioninthebrain,”Nat.Rev.Neurosci.,vol.3,pp.201–215,
at a slightly later stage. Although not explicitly modeled, this Mar.2002.
[6] P. Mengotti, A.-S. Ka¨sbauer, G. R. Fink, and S. Vossel, “Lateraliza-
behavior is similar to an attentional mechanism of inhibition
tion,functionalspecialization,anddysfunctionofattentionalnetworks,”
ofreturn(IOR)[24],[25],whereapreviouslycuedvisualarea Cortex,vol.132,pp.206–222,Nov.2020.
becomes attentionally supressed after longer CTOA intervals [7] L.V.Kulke,J.Atkinson,andO.Braddick,“Neuraldifferencesbetween
in exogenous cues. Since this was not explicitly modeled, this covertandovertattentionstudiedusingEEGwithsimultaneousremote
eyetracking,”Front.Hum.Neurosci.,vol.10,p.592,Nov.2016.
model behavior will be examined in future work.
[8] C. D. Blair and J. Ristic, “Attention combines similarly in covert and
Overt visual attention in the form of camera orientation overtconditions,”Vision(Basel),vol.3,p.16,Apr.2019.
actionsignalswasexaminedinasimpletargetreachtask.The [9] H.FeldmanandK.J.Friston,“Attention,uncertainty,andfree-energy,”
Front.Hum.Neurosci.,vol.4,2010.
resultsinFig.7showthatbottom-upovertorientingisoverall
[10] T. Parr, D. A. Benrimoh, P. Vincent, and K. J. Friston, “Precision and
faster than top-down intentional orienting, which is explained falseperceptualinference,”Front.Integr.Neurosci.,vol.12,p.39,Sept.
by the sensitivity of the precision to red objects (or any 2018.
[11] T.ParrandK.J.Friston,“Uncertainty,epistemicsandactiveinference,”
predeterminedvisualobjectofinterest,likefaces[34]).Thisis
J.R.Soc.Interface,vol.14,p.20170376,Nov.2017.
similarlyreflectedinhowreactiontimechangeswithdistance. [12] M. B. Mirza, R. A. Adams, K. Friston, and T. Parr, “Introducing a
Bothformsoforientingexhibitanincreasingtrendinreaction bayesian model of selective attention based on active inference,” Sci.
Rep.,vol.9,p.13915,Sept.2019.
timeasdistanceincreases;however,top-downorientingshows
[13] T.ParrandK.J.Friston,“Attentionorsalience?,”Curr.Opin.Psychol.,
a steeper rise, indicating a greater sensitivity to distance vol.29,pp.1–5,Oct.2019.
[14] D. Parvizi-Wayne, “How preferences enslave attention: calling into the visual generative model is the gradient of the VAE
questiontheendogenous/exogenousdichotomyfromanactiveinference decoder computed by backpropagation.
[15] p M e . rs W pe . c S ti p ve ra ,” tli P n h g e , n “ o P m re e d n i o c l t . iv C e o c g o n d . i S n c g i. a , s Se a p m t. o 2 d 0 e 2 l 4 o . f biased competition • ∂ ∂ Π˜ µ˜ s : Since the sensory precision matrix is assumed to
invisualattention,”VisionRes.,vol.48,pp.1391–1408,June2008. bediagonal,thisgreatlysimplifiescalculationofthegra-
[16] L.IttiandP.Baldi,“Bayesiansurpriseattractshumanattention,”Vision dients ∂πi for each pixel i from the individual precision
Res.,vol.49,pp.1295–1306,June2009. ∂µ
[17] M. Posner, M. Nissen, and W. Ogden, “Attended and unattended pro- functions π i (µ,s). The sensory precision gradient ∂ ∂ Π˜ µ˜ s
cessingmodes:Theroleofsetforspatiallocation,”ModesofPerceiving is a tensor of shape L×L×M.
[18]
a
M
n
.
d
I
P
.
r
P
o
o
ce
sn
ss
e
i
r
n
,
g
“O
In
r
f
i
o
e
r
n
m
ti
a
n
t
g
ion
o
,
f
v
a
o
t
l
t
.
en
1
t
3
io
7
n
,
,”
01
Q
1
.
97
J.
8.
Exp. Psychol., vol. 32, •
∂Π˜
µ : The optimization of system dynamics precisions
∂µ˜
pp.3–25,Feb.1980. Π˜ is left for future work, and they are assumed to be
[19] M.S.Peterson,A.F.Kramer,andD.E.Irwin,“Covertshiftsofattention µ
precede involuntary eye movements,” Percept. Psychophys., vol. 66, constant. Their gradients are therefore zero.
pp.398–405,Apr.2004. • ∂Π˜ s :Thegradientiscalculatedinawaysimilarto ∂Π˜ s,
[20] J. Jonides, “Voluntary versus automatic control over the mind’s eye’s ∂s˜ ∂µ˜
with the gradient being a tensor of shape L×L×L.
movement,”inAttentionandPerformanceIX,pp.187–203,1981.
[21] M.ChealandD.R.Lyon,“Centralandperipheralprecuingofforced- • ∂s˜ : The inverse mapping from sensory data to actions
∂a
choicediscrimination,”Q.J.Exp.Psychol.A,vol.43,pp.859–880,Nov. is generally considered a “hard problem” [35]. However,
1991.
it is fairly simple in our case: the centroid of the color
[22] S. P. Vecera and M. J. Farah, “Does visual attention select objects or
locations?,”J.Exp.Psychol.Gen.,vol.123,no.2,pp.146–160,1994. red is converted into pitch and yaw angles (assuming we
[23] R. Egly, J. Driver, and R. D. Rafal, “Shifting visual attention between know the intrinsic parameters of the camera model).
objects and locations: Evidence from normal and parietal lesion sub-
jects,”J.Exp.Psychol.Gen.,vol.123,no.2,pp.161–177,1994. B. Variational Autoencoder
[24] I. Reppa, W. C. Schmidt, and E. C. Leek, “Successes and failures
in producing attentional object-based cueing effects,” Atten. Percept. The encoder consists of a convolutional layer 3 × 3 (in
Psychophys.,vol.74,pp.43–69,Jan.2012. channels:3,outchannels:32),followedbyfourresidualdown-
[25] R.M.Klein,“Inhibitionofreturn,”TrendsCogn.Sci.,vol.4,pp.138–
sampling blocks (32→64, 64→128, 128→256, 256→512). A
147,Apr.2000.
[26] S. Pinker and C. J. Downing, Attention and Performance XI: Mecha- fullyconnectedlayermapsthe512-dimensionalfeaturevector
nismsofattentionandvisualsearch. Hillsdale,NJ:Erlbaum,1985. to 64, followed by another producing a 2 × 8-dimensional
[27] M.Carrasco,D.L.Evert,I.Chang,andS.M.Katz,“Theeccentricity
latent space output. The decoder mirrors this structure, with
effect:targeteccentricityaffectsperformanceonconjunctionsearches,”
Percept.Psychophys.,vol.57,pp.1241–1261,Nov.1995. a fully connected layer expanding 8 to 64, reshaped into a
[28] T.Parr,G.Pezzulo,andK.J.Friston,Activeinference. TheMITPress, 512×H/16×W/16 feature map, followed by four residual
2022.
upsampling blocks (512→256, 256→128, 128→64, 64→32)
[29] K.Friston,J.Kilner,andL.Harrison,“Afreeenergyprincipleforthe
brain,”J.Physiol.Paris,vol.100,pp.70–87,July2006. andafinal3×3convolutionallayer(32output).TheVAEwas
[30] K. Friston, “The free-energy principle: a unified brain theory?,” Nat. implemented and trained in pytorch on 240,000 32×32×3
Rev.Neurosci.,vol.11,pp.127–138,Feb.2010.
imagesrandomlygeneratedintheGazebosimulator.Thelatent
[31] R. Kanai, Y. Komura, S. Shipp, and K. Friston, “Cerebral hierarchies:
predictiveprocessing,precisionandthepulvinar,”Philos.Trans.R.Soc. space was disentangled with manual encodings of sphere’s
Lond.BBiol.Sci.,vol.370,p.20140169,May2015. image coordinates for each of the training images.
[32] M.PriorelliandI.P.Stoianov,“Flexibleintentions:Anactiveinference
theory,”Front.Comput.Neurosci.,vol.17,p.1128694,Mar.2023.
C. Sensory Data and Belief Shape
[33] R. Walker, D. G. Walker, M. Husain, and C. Kennard, “Control of
voluntary and reflexive saccades,” Exp. Brain Res., vol. 130, pp. 540– The three different kinds of sensory input are as follows:
544,Feb.2000.
[34] L. Kauffmann, C. Peyrin, A. Chauvin, L. Entzmann, C. Breuil, and • Proprioceptive: pitch and yaw angles of the camera’s
N. Guyader, “Face perception influences the programming of eye orientation in the simulator, expressed in radians.
movements,”Sci.Rep.,vol.9,p.560,Jan.2019. • Visual: a 32 × 32 × 3 RGB image captured by the
[35] K. J. Friston, J. Daunizeau, J. Kilner, and S. J. Kiebel, “Action and
simulated camera model.
behavior:afree-energyformulation,”Biol.Cybern.,vol.102,pp.227–
260,Mar.2010. • Symbolic cue: a floating-point array with two elements,
containingtheimagecoordinatesthatcuewherethetarget
APPENDIX
may appear.
A. Implementation of gradients
The belief is a concatenation of the following elements:
The gradients with respect to beliefs, action and sensory
• Symboliccuebelief:twoelementsthatmirrorthesensory
data given in (11) and (13) depend on the different imple-
input for the symbolic cue
mentations of system dynamics, generative models, sensory
• Proprioceptive belief: two elements that mirror the sen-
precisions and the type of sensory data:
sory input for the pitch and yaw angles
• ∂ ∂ µ f ˜ ˜ : The gradient of the system dynamics function • Visual encoding belief: the visual encoding used by the
defined in (10) w.r.t. the belief µ is fairly simple, seeing decoder to generate visual predictions. The first three
as it is defined as an affine transformation of the belief. elementsencodethesphere’spositionandpresence,while
• ∂g˜ : The gradients of the generative models w.r.t. the the rest are free latent variables
∂µ˜
belief for the proprioceptive and interoceptive models • Covert focus belief: represents the center and amplitude
are simple identity matrices. However, the gradient of of the RBF used in the calculation of the sensory preci-
sion.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "An Active Inference Model of Covert and Overt Visual Attention"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
