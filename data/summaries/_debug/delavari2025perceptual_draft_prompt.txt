=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Perceptual Motor Learning with Active Inference Framework for Robust Lateral Control
Citation Key: delavari2025perceptual
Authors: Elahe Delavari, John Moore, Junho Hong

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Key Terms: lateral, robust, control, skills, awareness, learning, perceptual, inference, framework, active

=== FULL PAPER TEXT ===

Perceptual Motor Learning with Active Inference Framework for
Robust Lateral Control
Elahe Delavari1, John Moore2, Junho Hong3, and Jaerock Kwon4
.elbisseccaebregnolonyamnoisrevsihthcihwretfa,ecitontuohtiwderrefsnartebyamthgirypoC.noitacilbupelbissoprofEEEIehtotdettimbusneebsahkrowsihT
is crucial: while fundamental movement skills (e.g., hop-
Abstract—This paper presents a novel Perceptual Motor ping, jumping, running, and balance) lay the groundwork,
Learning (PML) framework integrated with Active Inference perceptual motor development connects these skills with
(AIF) to enhance lateral control in Highly Automated Vehicles
sensoryprocessing,enablingchildrentodevelopbodyaware-
(HAVs). PML, inspired by human motor learning, emphasizes
ness, spatial awareness, directional awareness, and temporal
the seamless integration of perception and action, enabling
efficientdecision-makingindynamicenvironments.Traditional awareness [8]. To realize a PML, we propose to use AIF,
autonomous driving approaches—including modular pipelines, whichhasemergedasapromisingalternativetoconventional
imitation learning, and reinforcement learning—struggle with control theories, grounded in theories of human brain func-
adaptability, generalization, and computational efficiency. In
tion that posit the continuous prediction of sensory inputs
contrast, PML with AIF leverages a generative model to
andtheminimizationofdiscrepanciesbetweenexpectedand minimize prediction error (“surprise”) and actively shape
vehicle control based on learned perceptual-motor represen- actual observations [9]. By unifying perception and action
tations. Our approach unifies deep learning with active in- withinasinglegenerativeframework,activeinferenceallows
ference principles, allowing HAVs to perform lane-keeping an autonomous agent to actively anticipate environmental
maneuverswithminimaldataandwithoutextensiveretraining
changes rather than merely reacting to them. Nevertheless,
across different environments. Extensive experiments in the
our work extends this paradigm by incorporating principles
CARLA simulator demonstrate that PML with AIF enhances
adaptability without increasing computational overhead while from perceptual motor development—a concept tradition-
achieving performance comparable to conventional methods. ally rooted in developmental psychology. Fig. 1 shows an
These findings highlight the potential of PML-driven active overview of our method.
inference as a robust alternative for real-world autonomous
driving applications.
Environment Agent
I. INTRODUCTION Perceived
Observation Environment
The rapid evolution toward fully Highly Automated Ve-
World
hicles (HAVs) demands decision-making frameworks that Sensing
not only emulate human cognition but also robustly gen-
eralize across diverse and unforeseen environments without Action
extensive retraining. Traditional approaches in autonomous Learning
Algorithm
driving—including Modular Pipelines (MP) [1], Imitation
Policy
Learning (IL) [2][3], and Reinforcement Learning (RL)
[4]—have paved the way for significant advances. However,
Fig. 1. Overview of Perceptual Motor Learning (PML) with Active
eachofthesemethodsfacesinherentlimitations.MPs,while
Inference Frameworks (AIF). The agent senses the environment through
modularandinterpretable,sufferfromerrorpropagationand its sensors and makes observations. These observations shape the agent’s
inflexibility; IL is capable of mimicking expert behavior yet beliefsandperceptionoftheworld.Basedonitsperception,theagenttakes
actionsthat,inturn,affecttherealenvironment.(IconmadebyFreepikfrom
often fails when confronted with novel scenarios; and RL,
www.flaticon.com)
despite its ability to learn through interaction, is heavily
reliant on finely tuned reward-shaping, which is a process
In our framework for autonomous driving, we draw a
that is both labor-intensive and highly task-specific.
parallel to this human developmental process. Our model
To address the aforementioned limitations, we propose
leverages a deep learning architecture to fuse perceptual
Perceptual Motor Learning (PML) [5][6] with Active Infer-
andmotorinformation,constructinganinternalworldmodel
ence Framework (AIF) [7]. PML refers to the process by
that captures the causal dynamics of the environment. This
whichsensoryinputs(perception)andmotoroutputs(action)
model enables the system to predict the consequences of
become integrated to enable effective interaction with the
its actions and adapt to new road conditions without retrain-
environment. In early human development, this integration
ing—addressingthecoreissueofgeneralizationthatplagues
traditionalILandRLmethods.Ratherthanblindlyimitating
*This work was supported in part by the Ford/University of Michigan
expertbehaviororrelyingonexternallydefinedrewards,our
Research Alliance Program (10.13039/100007270) and in part by the
NationalScienceFoundation(NSF)underGrantMRI2214830. approach infers how the world works to support robust and
1E. Delavari, 3J. Hong, and 4J. Kwon are with the University of flexible decision-making.
Michigan-Dearborn. {elahed, jhwr, jrkwon}@umich.edu
To evaluate the effectiveness of our approach, we applied
Correspondingauthor:J.Kwon.
2J.MooreiswithFordMotorCompany.jmoor422@ford.com ittothelateralcontrolofself-drivingvehiclesinasimulated
5202
raM
5
]OR.sc[
2v67610.3052:viXra
urban environment. The model was trained on data from that are then used by a control algorithm to maneuver the
a simulated large urban area with multiple-lane highways vehicle. CAL aims to combine the advantages of MP and
and tested in distinctively different settings: a town with IL by using a Deep Neural Network (DNN) to learn low-
mountainous roads and a small town characterized by multi- dimensional, yet informative, affordances from high-level
ple bridges. This diverse testing environment highlights the directional inputs. The approach handles complex urban
model’s ability to generalize across varied road structures. drivingscenarios,includingnavigatingintersections,obeying
In summary, the key contributions of our work are: traffic lights, and following speed limits. However, choosing
• Toourbestknowledge,thisstudyisthefirstintegration the best set of affordances is challenging. Furthermore, the
of PML with AIF in HAVs. By leveraging an internal effectiveness of this method relies heavily on precise affor-
world model to learn the causal relationships between dance predictions. Any inaccuracies or errors in predicting
actions and environmental changes, our system adapts affordancesmayresultinlessthanoptimalorevenhazardous
to new scenarios without requiring retraining, ensuring driving decisions.
robust performance across diverse environments. Use of RL has shown promising results in training au-
• PML with AIF allows greater resilience and adaptabil- tonomousagentsthroughtrialanderror[4][16][17].However,
ity in novel driving conditions, whereas traditional IL it also has its own set of limitations. RL methods typi-
methods suffer from long tail problems. cally necessitate extensive sample data to develop effective
• PML with AIF streamlines training, making it both policies, which can be especially challenging in real-world
computationally efficient and easily scalable across dif- settings where data acquisition is costly and time-intensive.
ferentdrivingtasksbyeliminatingtheneedforcomplex Achieving a balance between exploration (testing new ac-
reward engineering. tions) and exploitation (leveraging known actions) presents
a substantial hurdle in RL. Designing appropriate reward
II. RELATEDWORK
functions is critical for RL but can be difficult and problem-
There are different methods for controlling an HAV in specific. Poorly designed rewards can lead to unintended
an environment. One of the most popular methods is MP behaviorsorsub-optimalperformance.Ensuringthestability
in autonomous driving systems, which generally consists and convergence of RL algorithms is challenging, especially
of dedicated modules for perception, prediction, planning, in complex and dynamic environments. Instabilities during
and control. While this approach offers clear advantages in training can lead to unreliable and unpredictable agent be-
terms of interpretability and modularity, it also has several havior [10][11].
limitations. Errors in upstream modules (e.g., perception) Liang et al.[17] introduce Controllable Imitative Rein-
can propagate downstream, leading to compounding errors forcement Learning (CIRL), which combines IL and RL
in prediction, planning, and control. MPs can be inflexible, to address the challenges of autonomous urban driving
making it difficult to adapt the system to new tasks or in complex, dynamic environments. CIRL utilizes a Deep
environments without significant re-engineering. This rigid- Deterministic Policy Gradient (DDPG) approach to guide
ity can limit the scalability and versatility of the system. explorationinaconstrainedactionspaceinformedbyhuman
Joint optimization across modules is challenging due to the demonstrations. This method allows the driving agent to
different nature of tasks each module performs [10][11]. learnadaptivepoliciestailoredtodifferentcontrolcommands
Another method that is vastly used is Imitation Learn- (e.g., follow, turn left, turn right, go straight). Despite using
ing (IL) which has been a popular method for developing IL to guide initial exploration, CIRL still faces challenges
autonomous driving systems due to its ability to mimic associated with exploration in RL. The agent may struggle
expert behavior from demonstrations and a lot of work to discover optimal policies in highly dynamic and unpre-
has been done in this domain [2][3][10][12]. Traditional IL dictable environments without effective exploration strate-
methods,however,oftenstrugglewithgeneralizingtounseen gies.
states not covered by the expert’s demonstrations. When the Active inference has been gaining interest in HAV re-
agent encounters scenarios outside the training data, it may search. Friston et al. [18] proposed it as a process theory
fail to perform adequately, leading to safety and reliability of the brain that minimizes prediction error (surprise) by
issues [10][13]. Additionally, in complex scenarios such as updatingitsgenerativemodelorinferringactionsthatreduce
urbandriving,high-leveldecisions(e.g.choosingtoturnleft uncertainty. While applied to simple simulated agents, its
or right at an intersection) cannot be easily inferred from use in realistic scenarios has been limited. Catal et al. [19]
perceptual input alone, leading to ambiguity and oscillations integrated deep learning with active inference for real-world
intheagent’sbehavior.Toovercomethis,Codevillaetal.[10] robot navigation, using high-dimensional sensory data to
introduced Conditional Imitation Learning (CIL), where the construct generative models without predefined state spaces.
driving policy is conditioned on high-level commands. Their work demonstrated successful navigation in a ware-
Direct perception methods are also the other category housesetting,thoughinacontrolledenvironment.deTinguy
of methods that are used [14]. Sauer et al. [15] propose et al. [20] introduced a scalable hierarchical model for
Conditional Affordance Learning (CAL), a direct perception autonomous navigation, combining curiosity-driven explo-
approach for autonomous urban driving. This method maps ration with goal-directed planning using visual and motion
video inputs to intermediate representations (affordances) perception. Their multi-layered approach (context, place,
motion) facilitated efficient navigation with fewer steps than states from sensory observations. By minimizing VFE, the
other models, though validation was limited to simulations agent continuously updates its beliefs, ensuring its internal
in a mini-grid environment. Nozari et al. [13] combined model remains consistent with sensory input. In our case,
active inference with IL to overcome IL’s limitations. Their input images serve as observations, while the states remain
frameworkemploysaDynamicBayesianNetwork(DBN)to hidden. Consequently, we model the environment as a Par-
encodeexpertdemonstrationsandincludesanactivelearning tiallyObservableMarkovDecisionProcess(POMDP),where
phase for policy refinement. This approach dynamically bal- the agent must infer hidden states from sensory data. The
ances exploration and exploitation, enhancing adaptability. key variables and functions in our framework are defined as
However, DBNs impose high computational overhead, mak- follows:
ing them impractical for resource-constrained autonomous • Variables:
driving platforms. – Hidden state: s
t
∈RdS
Tothebestofourknowledge,mostpriorstudiesonaction- – Action: a
t
∈RdA
based future scene prediction focus on standard driving – Observation: o
t
∈RdO
scenarios [21][22] or simplified environments like Atari – Data: D={o,a,o }T
games [23][24], where predicted images closely resemble t t t+1 t=0
• Functions:
the input. Even in these cases, models often struggle to gen-
– Observation model: oˆ = f (s)
erate high-quality predictions, leading to blurriness. Unlike t o t
– Forward transition model: oˆ = f (o,a)
previous studies that focus on predicting future scenes in t+1 s t t
– Inverse transition model: aˆ = f−1(o,o )
simple or highly constrained environments, we apply this t s t t+1
– Distance model: d = f (o,oˆ)
ideawithinaPMLwithAIF,integratinganencoder-decoder t d t t
– Policy: π :s →a
DNN architecture [25] as the forward transition model. Our t t
approach explicitly models the effect of steering actions on TheVFEservesasanupperboundonsurpriseandisused
futureobservations.Byleveragingagenerativemodelwithin to infer hidden states:
AIF, we improve scene prediction, leading to more reliable
(cid:104) (cid:105)
action selection and decision-making under uncertainty. F =E q(st+1) −lnp(o t+1 |s t+1 )
(1)
(cid:0) (cid:1)
+D q(s )∥p(s |o) ,
III. METHOD KL t+1 t+1 t
OurapproachintegratesPMLwithAIFtodevelopacom- where p(o t+1 |s t+1 ) is the likelihood of the next observa-
putationally efficient and explainable autonomous driving tion given the inferred hidden state, p(s t+1 |o t ) is the prior
system. PML allows the agent to learn from sensory experi- belief about the next state given past observations, and
(cid:0) (cid:1)
ences, dynamically linking perception with motor execution, D KL q(s t+1 )∥ p(s t+1 |o t ) measures the divergence between
while active inference ensures that decisions minimize pre- the approximate posterior and the prior belief, ensuring the
dictionerrorsbasedontheFreeEnergyPrinciple(FEP)[26]. update remains close to the true distribution.
In this work, decision-making is guided by the vehicle’s To implement PML with AIF for autonomous driving,
currentstate,sensoryobservationsfromasinglefront-facing we employ a two-stage process consisting of offline task-
camera, an internal world model, and predictions of how the agnostic training and online action selection. In the offline
environmentwillchangebasedonpotentialactions.Notably, phase, we pretrain a forward transition model f s [27] to
our model operates solely on visual perception, without learn transition dynamics, enabling the agent to predict how
relying on privileged information such as LiDAR, GPS, or future observations evolve under different actions. Once
HD maps. This design choice ensures that the model learns trained, this model remains fixed and does not require
andgeneralizesfromvisionalone,makingitmoreadaptable further updates. In the online task-specific phase, the agent
and comparable to human driving capabilities. predictsfuturestatesforallpossibleactionsateachtimestep
and selects the action that minimizes the EFE to optimize
A. Perceptual Motor Learning with Active Inference
decision-making under uncertainty.
In autonomous systems, PML enables agents to build In the context of autonomous driving, AIF involves the
internal representations of the environment and adapt their vehicle continuously predicting its sensory inputs, which
actions dynamically based on sensory feedback. By lever- consist of visual observations of the road, and minimizing
aging PML principles, an autonomous vehicle can enhance prediction error through either perception, by updating its
its control strategies through learned associations between internal model, or action, by making steering adjustments.
environmental perception and motor execution. This forms Tomodelperception,Weemployaforwardtransitionmodel
the foundation for our approach, where we integrate PML to predict future observations given the current state and
within an AIF to optimize decision-making under uncer- action. Meanwhile, action selection is performed through
tainty. According to FEP, agents minimize Free Energy (FE) sensory-motor simulation, which effectively serves as an
[26], a measure of surprise or prediction error. However, inverse transition model f−1 by inferring the action that
s
sinceFEisintractabletocomputedirectly,anapproximation would lead to a desired future state. In this process, the
called Variational Free Energy (VFE) [7] is used. VFE agentevaluatescovertactionsbeforeexecution,selectingthe
enablesBayesianinference,allowinganagenttoinferhidden one that minimizes the EFE. Given that our task focuses
onlane-keeping,weincorporatepreference-based[28]action prediction errors. To find an action that minimizes G, the
selection, encouraging actions that maintain lane alignment. agentgeneratescovertactionstoimaginefuturestatescaused
by the actions. To align with goal-directed behavior, we
B. Perception Model
compare predicted observations oˆ with a preferred future
t+1
For perception, a forward transition model f s is trained to sensory state o pref using Structural Similarity Index Measure
containthenecessaryinformationfortheagenttounderstand (SSIM) [31]. The SSIM index is between -1 and 1, where 1
the environment in the context of the driving task. This f s is perfect similarity, and -1 is maximum dissimilarity. Thus,
is used to generate future images influenced by the steering our distance model (f ) can be defined as 1−SSIM. The
d
angle. Training process of the forward transition model f s action can be selected by (3). Fig 3 illustrates the action
is depicted in Fig. 2. For learning a forward transition selection process.
model f , this paper uses a method inspired by action-based
s
representation learning [29]. argmin(1−SSIM(oˆ ,o )). (3)
t+1 pref
As human beings, it is simple to predict how the environ- i
ment will change based on the change of steering action, Thepreferredfuturesensorystateisrepresentedasprefer-
as humans are exceptionally good at generating missing ence.Forinstance,inalane-keepingscenario,itcorresponds
information. However, it is not straightforward for a feed- to a straight road within the same lane. Fig. 4 illustrates
forwardDNNtopredictthedetailsofascenewhenprovided the predicted semantic segmented images alongside their
only one frame and the corresponding action—especially ground truth, highlighting the SSIM difference. Road only
when the future scene is significantly different from the grayscaled version of the semantic segmented image used
current scene. The primary challenge lies in the network’s to simplify the problem and reduce computational overhead
difficultyinlearningastrongcorrelationbetweenthecurrent while maintaining the core driving information.
and future scene, even with the help of action. In this paper, we define steering as the action, while
WeusedaU-NetwithanXception-stylearchitecture[25], speed is kept constant to eliminate extra complexity. The
incorporatingtask-specificmodificationstobettercapturethe future scene is predicted based on the current scene and the
actionandobservationdependency.U-Net[30]iswell-suited selected action. The forward transition model f s provides
for tasks with limited training data, as it efficiently extracts prior knowledge of how different steering actions influence
hierarchical features using convolutional layers in both the the future scene for the HAV. To guide decision-making, the
encoding and decoding paths. Its skip connections help model also requires an expectation of the future—referred
preserve spatial information from early layers, enhancing to as preference. Using the current state and the transition
feature retention. These properties make it an ideal choice model f s , the HAV selects an action by comparing predicted
for the forward transition model. future scenes with the expected preference. The action asso-
ciated with the highest similarity to this expected outcome
C. Action Selection Model is chosen.
Minimizing VFE ensures that the agent’s internal beliefs
IV. EXPERIMENTALSETUP
alignwithobservations,allowingittoinferstatesaccurately.
While VFE is crucial for state estimation, it does not dictate A. DNN Architecture and Training
which actions the agent should take. For decision-making, 1) PML: We used a U-Net Xception-style model in
theagentminimizesExpectedFreeEnergy(EFE)[7],which Keras 3 [32]. The input image was encoded with the
predicts the impact of different actions on future states. To encodingpartoftheU-Netandthentheactionwasaddedby
make goal-directed decisions, the agent minimizes the EFE, concatenating the action through Dense layers. For this task,
which considers both epistemic (information-seeking) and the upsampling layers are changed to the conv2d transpose
pragmatic (goal-aligned) components: layer as they can make the reconstructed image less blurry.
Inaddition,asonlytheroadpartoftheimageisconsidered,
(cid:104) grayscale images are used. So, one channel in both the
G=E D (q(s |a)∥p (s ))
q(ot+1,st+1|at) KL t+1 t pref t+1 input layer and output layer is used. The input and output
(2)
(cid:105)
−H(q(o |s ,a)) , images have the same size of 160×160×1. The RELU
t+1 t+1 t
activation function is used for encoding and decoding layers
where D (q(s |a)∥p (s )) encourages the selection andtheELUactivationfunctionisusedforthepartwherethe
KL t+1 t pref t+1
of actions that lead to desired states, and H(q(o |s ,a)) actionwasadded.Fortheoutputlayer,thesigmoidactivation
t+1 t+1 t
represents uncertainty reduction, guiding exploration. function was used. A learning rate of 1e-4 and batch size of
The agent balances goal-directed behavior with learning 128 were used to train this model.
from its environment by seeking smaller G. In our work, WetestedthetrainedmodelintheCARLA(CarLearning
we focused on minimizing the first part of EFE with action to Act) [33] simulator using a covert action approach to
selection,astheexplorationisnotbeneficialforourcasedue find an optimal steering command. The model predicted the
to offline training of the forward transition model f . This future image for twenty different steering values between -1
s
approach allows the vehicle to adapt its driving behavior and1,withastepsizeof0.1,andthepredictedimageswere
in real-time, making steering adjustments that minimize comparedwithapreferenceforthetask.Thecomparisonwas
Environment s* s* s*
t t+1 t+2
(Hidden State)
Agent
f f f f f
o d o d o
o ô f ô o ô f ô o ô …
t t s t+1 t+1 t+1 s t+2 t+2 t+2
Action enc dec enc dec enc dec
Generator
…
f a a
a t t+1
Perceived
s s s
Environment t t+1 t+2
Fig.2. Task-agnosticofflineperceptionmodeltraining.Ahiddenstates
t
∗ isperceivedthroughtheobservationmodel fo.Anobservationot isencoded
andkeptinsidetheagentasaperceivedstate.Whenanactionat isappliedtotheenvironmentbytheagent,thecausalchangesintheenvironmentshould
beobservedasoˆt+1.Theforwardtransitionmodel fs isbeingtrainedusingthedistancemodel fd withot+1 andoˆt+1.Theencoderanddecodercanbe
theinsideof fs inpractice.
TABLEI
aiaaaa oc
t t t t t t DNNARCHITECTUREUSEDFORTHEBCAGENT
ôi LayerType Description Kernel Size Activation
t+1
o f f /Units
t s d
Input InputImage (160,160,3) -
Conv2D+BN 32filters,stride2 5x5 ReLU
Fig.3. Actionselection.Theforwardtransitionmodel fs generatescausal
observationsbasedoncovertactions.ocispreferenceforatask.Thedistance Conv2D+BN 64filters,stride2 3x3 ReLU
t
model fd calculates the dissimilarity between the observations and the
Conv2D+BN 128filters,stride2 3x3 ReLU
preferenceandfindswhichactionwillgeneratethemostsimilarobservations
Dropout 20%rate - -
tothepreference.
Conv2D+BN 256filters,stride2 3x3 ReLU
Flatten Flatten - -
Dense Fullyconnected 512 ReLU
Dropout 20%rate - -
Dense Fullyconnected 128 ReLU
Dropout 20%rate - -
Fig. 4. Predicted and the ground truth image for semantic segmented Dense Fullyconnected 64 ReLU
imageswiththeSSIMdifferencebetweenthem. Dense Fullyconnected 1 -
B. Simulator and Dataset
conducted using SSIM, selecting the action corresponding
CARLA is a high-fidelity simulator that provides a re-
to the predicted image with the highest SSIM score. This
alistic environment for developing, training, and validating
process ensured that for each input image, the optimal
autonomous driving technologies in urban settings. CARLA
steering action was chosen, enabling the car to drive in the
has multiple versions, each supporting different towns and
simulator. The fourth future image prediction at a constant
features. We used CARLA 0.9.14 to test and validate the
speed was used for driving in the experiments.
proposed PML with AIF in terms of online driving perfor-
2) Imitation Learning: Imitation learning encompasses mance. Then, we also used CARLA 0.8.4 to compare other
various approaches, including Behavioral Cloning (BC) and previouslypublishedmethodsintheoriginalCARLAbench-
Inverse Reinforcement Learning (IRL). To validate the PML mark (referred to as CoRL2017) because the CoRL2017
agent’s adaptability to new scenarios, we used BC for com- benchmark is only compatible with the 0.8.4 version.
parison, training a model to replicate expert demonstrations We trained our PML agent in Town06, which has long,
throughsupervisedlearningandevaluatingtheironlinedriv- many-lanehighwayswithmanyhighwayentrancesandexits.
ing performance. The DNN architecture of the BC agent is To conduct fair comparisons with a BC method, the BC
shown in Table I. A learning rate of 10−3 and batch size of agent was also trained in the same town. Then, these two
64 were used to train this DNN. agents were tested in Town01 (a small town with a river
and several bridges) and Town04 (a small town embedded quentlyoccurringsteeringvalues.Althoughthisapproachled
in the mountains with a loop highway). To assess their tobetterdrivingbehavior,theagentstillexhibiteddifficulties
robustness, we conducted tests across four different lanes in turns and struggled to recover when veering off the track.
within each town. The test tracks used in these towns were To address this, we collected an additional 32,000 sam-
selected randomly and are illustrated in Fig. 5. We selected ples, specifically demonstrating recovery maneuvers from
three different driving scenarios in each town: straight, one- different lanes. After applying the normalization process,
turn, and two-turn. They are labeled as [A], [B], and [C], the final dataset consisted of 47,313 images paired with
respectively. steering angles, ensuring a well-distributed representation of
1) PML: For a task-agnostic exploration of the environ- different driving scenarios. All models are implemented on
ment, 36,000 images in Town06, which enables rigorous the TensorFlow framework. The lambda Workstation having
testing across various lane configurations. To expand the two NVIDIA RTX 6000 was used for training the images
dataset, images were augmented by flipping them along of semantic segmentation and RGB images, and NVIDIA
with their corresponding steering angles, resulting in a total GeForce RTX 4060 was used for training grayscale images
of 72,000 image-steering pairs. A deliberate zigzag driving and doing the tests in the CARLA simulator.
pattern was used during data collection to ensure a diverse
and evenly distributed dataset covering the full range of V. RESULTS
steering angles from -1 (full left) to 1 (full right). This
A. Performance Comparison
approach ensures the model learns the causal relationship
PML proved to be a robust and adaptable agent for
between steering inputs and visual observations, improving
autonomous driving, as evidenced by the comprehensive
its ability to generalize across different driving conditions.
Images used as preferences for the PML agent are pre- analysis presented in Table II. In the table, the deviation
is the average value of the Euclidean distance between the
sented in Fig. 6. Since the road structure varies between
towns, different preferences were used to adapt the model vehicle’s positions and the corresponding waypoints, and
the success rate shows, across different tracks, how many
accordingly. For benchmark experiments in Town01 and
Town02,asinglepreferenceimage(thefirstimageinFig. 6) tracks were successfully completed. For each track, four
different experiments were conducted, and the success rate
wasusedforbothtowns,astheirroadstructureswerenearly
was computed based on the success of these experiments.
identical.
The deviations should be considered bearing in mind that
To evaluate the effectiveness of our approach, we con-
the whole lane in Town01 has a width of 4 meters, and
ducted two sets of experiments. First, we compared our
Town04 and Town06 have a width of 3.5 meters. Table II
PML-based method against a BC agent across different
presents the performance metrics of PML and BC agents
tracks in Town01, Town04, and Town06. Second, we tested
across various tasks in different urban environments. The
ourmethodusingtheCoRL2017benchmark,whichiswidely
performance is evaluated using two key metrics: average
used for evaluating autonomous driving policies. The stan-
deviation and success rate. The results indicate that the
dard CoRL2017 evaluation considers only RGB images, but
since our method relies on road-only segmented images, we PML agent consistently outperforms the BC agent across
various tasks and environments, achieving higher success
added a semantic segmentation camera instead of RGB to
rates and lower deviations in most cases. In Town01, PML
extract relevant information.
2) Imitation Learning: The BC agent was trained on demonstrated perfect performance with a 100% success rate
in all tasks, whereas BC struggled. In Town04, PML main-
a dataset collected from Town06, which is the same en-
tained strong performance across tasks, with BC showing
vironment where the PML agent was trained and tested
comparable results only in the Straight and Two Turns task.
across all towns. Training BC agents in Town06 was par-
In Town06, both agents performed similarly in terms of
ticularly challenging due to the presence of multiple lanes,
success rate, but PML exhibited lower deviation, indicating
requiring a diverse dataset covering different lane positions.
more precise trajectory tracking. Overall, PML proved to
Additionally, the vehicle often veered too close to the curb
be a more robust and adaptive approach, particularly in
when navigating turns. To ensure a fair comparison, the BC
challenging scenarios.
agent was trained exclusively for lateral control. To capture
high-quality driving behavior, we collected 61,000 samples,
B. CoRL2017 Benchmark
ensuring an approximately even distribution across different
lanes in the simulator. Despite applying data augmenta- We also compared our results with various methods,
tion techniques such as brightness adjustments and image including Modular Pipeline (MP), Imitation Learning (IL),
flipping, the results remained unsatisfactory. To improve and Reinforcement Learning (RL), as implemented in the
performance, we explicitly included flipped versions of the CARLA simulator [33]. Additionally, we included bench-
images in the dataset and applied a normalization strategy. mark results from CIRL [17], CAL [15], and LBC [11], as
The collected data was normalized to balance the dataset shown in Table III. For these studies, Town01 was used for
acrossdifferentsteeringangles.Specifically,weensuredthat training, while Town02 served as the testing environment.
eachsteeringbincontainedacomparablenumberofimages, Note that both towns were not seen to our PML agent in the
preventing the model from being biased toward more fre- training stage.
BB CC
CC
AA
AA
AA
BB
CC BB
Fig.5. TracksusedfortestinginTown01,Town04,andTown06fromlefttoright.[A]showsthestraight,[B]showstheone-turn,and[C]showsthe
two-turntrack.
Our PML withAIF method outperforms ILand RL in the
straight driving task in Town01, though it is slightly outper-
formed by the other approaches (See Table III). However,
it is important to note that, unlike other methods, we did
Fig. 6. The examples of the preferences. The first is for Town01 and not use any training data from Town01, which gives our
Town02. The center two are for lanes 1 and 2 in Town04. The right two approach a strong generalization advantage. In Town02, our
arelanes3and4inTown06. agentdemonstratesdrivingperformancecomparabletoCAL
and MP and surpasses RL in the straight task.
TABLEII
Although CIRL and LBC achieve better results, our
PERFORMANCECOMPARISON
method offers a significantly simpler training pipeline. LBC
Town Agent Task Avg.Dev.(↓) Success(%↑) employsatwo-stagetrainingprocess,makingitmorecompu-
Straight 0.4080 100.00 tationallydemandingandresource-intensive.Similarly,CIRL
PML One-Turn 0.5400 100.00 integratesadual-stagelearningprocessthataddscomplexity
TwoTurns 0.4860 100.00
in both implementation and hyperparameter tuning. The
Town01 Overall 0.4780 100.00
need to balance IL and RL in CIRL introduces additional
Straight 0.9780 0.00 challenges, such as overfitting to demonstration data or
One-Turn 1.2170 0.00
BC inadequate generalization from RL. Overall, our PML with
TwoTurns 3.4180 25.00
AIF-based approach delivers competitive results, highlight-
Overall 1.8710 8.33
ing its potential as an alternative to more complex learning
Straight 0.8120 100.00
One-Turn 0.9440 100.00 frameworks.
PML
TwoTurns 0.8230 100.00
Town04 Overall 0.8597 100.00 VI. CONCLUSION
Straight 0.6880 100.00
In this paper, we introduced a novel PML-based method
One-Turn 1.7140 0.00
BC TwoTurns 0.8750 100.00 for controlling an autonomous vehicle, leveraging the FEP
to dynamically adapt to different road types, including two-
Overall 1.0923 66.67
Straight 0.7010 75.00 lane and multi-lane environments. Our method significantly
One-Turn 0.5730 100.00 enhancestheadaptabilityandperformanceofHAVs,achiev-
PML
TwoTurns 0.6460 100.00
ing comparable results to more complex baseline methods
Town06 Overall 0.6400 91.67 in the CARLA benchmark. Our results demonstrate that the
Straight 1.1630 75.00 PML agent, despite its simplicity, performs robustly across
One-Turn 1.7840 100.00
BC variousurbanenvironmentsanddrivingtasks.Itconsistently
TwoTurns 1.4790 100.00
outperforms BC in terms of lower average deviations and
Overall 1.4753 91.67
higher success rates in simulated environments.
Future work will aim to enhance the capabilities of
TABLEIII our PML-based agent by incorporating longitudinal control
SUCCESSRATERESULTINCORL2017BENCHMARK. (throttleandbrake)andaddressingmorecomplextaskssuch
as lane changing, handling dynamic objects in the driving
MP IL RL CAL CIRL LBC PML
scene, and adhering to traffic rules. Additionally, we aim to
[33] [33] [33] [15] [17] [11] (Ours)
validate our approach using real-world data and explore its
Town01-Train 98 95 89 100 98 100 96(Test)
application to actual autonomous driving scenarios. While
Town02-Test 92 97 74 93 100 100 92(Test)
this study focused on the lane-keeping task, our approach
can be extended to more complex maneuvers, such as lane- [21] Y.-H. Kwon and M.-G. Park, “Predicting Future Frames Using Ret-
changing. rospectiveCycleGAN,”inProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,2019,pp.1811–1820.
[22] D.Zhu,H.Chern,H.Yao,M.S.Nosrati,P.Yadmellat,andY.Zhang,
REFERENCES
“Practical Issues of Action-Conditioned Next Image Prediction,” in
2018 21st International Conference on Intelligent Transportation
[1] C. Badue, R. Guidolini, R. V. Carneiro, P. Azevedo, V. B. Cardoso,
Systems(ITSC),Nov.2018,pp.3150–3155.
A. Forechi, L. Jesus, R. Berriel, T. M. Paixa˜o, F. Mutz, L. de Paula
[23] J.Oh,X.Guo,H.Lee,R.L.Lewis,andS.Singh,“Action-Conditional
Veronese,T.Oliveira-Santos,andA.F.DeSouza,“Self-drivingcars:
VideoPredictionusingDeepNetworksinAtariGames,”inAdvances
Asurvey,”ExpertSystemswithApplications,vol.165,p.113816,Mar.
inNeuralInformationProcessingSystems,vol.28. CurranAssociates,
2021.
Inc.,2015.
[2] D. A. Pomerleau, “ALVINN: An Autonomous Land Vehicle in a
[24] E. Wang, A. Kosson, and T. Mu, “Deep action conditional neural
Neural Network,” in Advances in Neural Information Processing
networkforframepredictioninAtarigames,”Technicalreport,2017.
Systems,vol.1. Morgan-Kaufmann,1988.
[25] F. Chollet, “Xception: Deep Learning With Depthwise Separable
[3] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp,
Convolutions,”inProceedingsoftheIEEEConferenceonComputer
P. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang, X. Zhang,
VisionandPatternRecognition,2017,pp.1251–1258.
J.Zhao,andK.Zieba,“EndtoEndLearningforSelf-DrivingCars,”
[26] K.Friston,“Thefree-energyprinciple:Aunifiedbraintheory?”Nature
Apr.2016.
ReviewsNeuroscience,vol.11,no.2,pp.127–138,Feb.2010.
[4] A.Kendall,J.Hawke,D.Janz,P.Mazur,D.Reda,J.-M.Allen,V.-D.
[27] D.M.Wolpert,Z.Ghahramani,andM.I.Jordan,“AnInternalModel
Lam,A.Bewley,andA.Shah,“LearningtoDriveinaDay,”in2019
forSensorimotorIntegration,”Science,vol.269,no.5232,pp.1880–
International Conference on Robotics and Automation (ICRA), May
1882,Sep.1995.
2019,pp.8248–8254.
[28] P.Mazzaglia,T.Verbelen,andB.Dhoedt,“ContrastiveActiveInfer-
[5] J. E. Clark, “On the Problem of Motor Skill Development,” Journal
ence,”inAdvancesinNeuralInformationProcessingSystems,vol.34.
ofPhysicalEducation,Recreation&Dance,May2007.
CurranAssociates,Inc.,2021,pp.13870–13882.
[6] “Perceptual and Motor Development Domain - Child Develop-
[29] Y.Xiao,F.Codevilla,C.Pal,andA.Lopez,“Action-basedRepresen-
ment (CA Dept of Education),” https://www.cde.ca.gov/sp/cd/re/
tationLearningforAutonomousDriving,”inProceedingsofthe2020
itf09percmotdev.asp.
ConferenceonRobotLearning. PMLR,Oct.2021,pp.232–246.
[7] T. Parr, G. Pezzulo, and K. J. Friston, Active Inference: The Free
[30] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional
Energy Principle in Mind, Brain, and Behavior. MIT Press, Mar.
Networks for Biomedical Image Segmentation,” in Medical Image
2022.
Computing and Computer-Assisted Intervention – MICCAI 2015,
[8] D. F. Stodden, J. D. Goodway, S. J. Langendorfer, M. A. Roberton,
N.Navab,J.Hornegger,W.M.Wells,andA.F.Frangi,Eds. Cham:
M. E. Rudisill, C. Garcia, and L. E. Garcia, “A Developmental
SpringerInternationalPublishing,2015,pp.234–241.
Perspective on the Role of Motor Skill Competence in Physical
[31] Z. Wang, A. Bovik, H. Sheikh, and E. Simoncelli, “Image quality
Activity:AnEmergentRelationship,”Quest,vol.60,no.2,pp.290–
assessment:Fromerrorvisibilitytostructuralsimilarity,”IEEETrans-
306,May2008.
actionsonImageProcessing,vol.13,no.4,pp.600–612,Apr.2004.
[9] K.J.FristonandK.E.Stephan,“Free-energyandthebrain,”Synthese,
[32] “Keras documentation: Image segmentation with a U-Net-like
vol.159,no.3,pp.417–458,Dec.2007.
architecture,” https://keras.io/examples/vision/oxfordpetsimage
[10] “End-to-End Driving Via Conditional Imitation Learning | IEEE
segmentation/.
Conference Publication | IEEE Xplore,” https://ieeexplore.ieee.org/
[33] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun,
abstract/document/8460487.
“CARLA:AnOpenUrbanDrivingSimulator,”inProceedingsofthe
[11] D. Chen, B. Zhou, V. Koltun, and P. Kra¨henbu¨hl, “Learning by
1st Annual Conference on Robot Learning. PMLR, Oct. 2017, pp.
Cheating,” in Proceedings of the Conference on Robot Learning.
1–16.
PMLR,May2020,pp.66–75.
[12] H.Xu,Y.Gao,F.Yu,andT.Darrell,“End-To-EndLearningofDriving
ModelsFromLarge-ScaleVideoDatasets,”inProceedingsoftheIEEE
Conference on Computer Vision and Pattern Recognition, 2017, pp.
2174–2182.
[13] S.Nozari,A.Krayani,P.Marin-Plaza,L.Marcenaro,D.M.Go´mez,
andC.Regazzoni,“ActiveInferenceIntegratedWithImitationLearn-
ing for Autonomous Driving,” IEEE Access, vol. 10, pp. 49738–
49756,2022.
[14] C.Chen,A.Seff,A.Kornhauser,andJ.Xiao,“Deepdriving:Learning
affordance for direct perception in autonomous driving,” in Pro-
ceedings of the IEEE International Conference on Computer Vision
(ICCV),December2015.
[15] A.Sauer,N.Savinov,andA.Geiger,“ConditionalAffordanceLearn-
ing for Driving in Urban Environments,” in Proceedings of The 2nd
ConferenceonRobotLearning. PMLR,Oct.2018,pp.237–252.
[16] M. Toromanoff, E. Wirbel, and F. Moutarde, “End-to-End Model-
Free Reinforcement Learning for Urban Driving Using Implicit Af-
fordances,”inProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,2020,pp.7153–7162.
[17] X. Liang, T. Wang, L. Yang, and E. Xing, “CIRL: Controllable
Imitative Reinforcement Learning for Vision-based Self-driving,” in
ProceedingsoftheEuropeanConferenceonComputerVision(ECCV),
2018,pp.584–599.
[18] K. Friston, P. Schwartenbeck, T. Fitzgerald, M. Moutoussis,
T.Behrens,andR.J.Dolan,“Theanatomyofchoice:Activeinference
andagency,”FrontiersinHumanNeuroscience,vol.7,Sep.2013.
[19] O. C¸atal, S. Wauthier, T. Verbelen, C. De Boom, and B. Dhoedt,
“Deep Active Inference for Autonomous Robot Navigation,” Mar.
2020,comment:workshoppaperatBAICSatICLR2020.
[20] D.deTinguy,T.VandeMaele,T.Verbelen,andB.Dhoedt,“Spatial
and Temporal Hierarchy for Autonomous Navigation Using Active
Inference in Minigrid Environment,” Entropy, vol. 26, no. 1, p. 83,
Jan.2024.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Perceptual Motor Learning with Active Inference Framework for Robust Lateral Control"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
