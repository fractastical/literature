=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Reframing the Expected Free Energy: Four Formulations and a Unification
Citation Key: champion2024reframing
Authors: Théophile Champion, Howard Bowman, Dimitrije Marković

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2024

Abstract: Active inference is a leading theory of perception, learning and decision making, which can be applied
to neuroscience, robotics, psychology, and machine learning. Active inference is based on the expected
free energy, which is mostly justified by the intuitive plausibility of its formulations, e.g., the risk
plus ambiguity and information gain / pragmatic value formulations. This paper seek to formalize
the problem of deriving these formulations from a single root expected free energy definitio...

Key Terms: united, unification, energy, four, kingdom, university, expected, formulations, computing, kent

=== FULL PAPER TEXT ===

4202
beF
22
]IA.sc[
1v06441.2042:viXra
Reframing the Expected Free Energy:
Four Formulations and a Unification.
Th´eophile Champion tmac3@kent.ac.uk
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Howard Bowman H.Bowman@kent.ac.uk
University of Birmingham, School of Psychology and School of Computer Science,
Birmingham B15 2TT, United Kingdom
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
University College London, Wellcome Centre for Human Neuroimaging (honorary)
London WC1N 3AR, United Kingdom
Dimitrije Markovi´c dimitrije.markovic@tu-dresden.de
Technische Universit¨at Dresden, Department of Psychology
Dresden 01069, Germany
Marek Grze´s m.grzes@kent.ac.uk
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Editor: TO BE FILLED
Abstract
Active inference is a leading theory of perception, learning and decision making, which can be applied
to neuroscience, robotics, psychology, and machine learning. Active inference is based on the expected
free energy, which is mostly justified by the intuitive plausibility of its formulations, e.g., the risk
plus ambiguity and information gain / pragmatic value formulations. This paper seek to formalize
the problem of deriving these formulations from a single root expected free energy definition, i.e., the
unification problem. Then, we study two settings, each one having its own root expected free energy
definition. In the first setting, no justification for the expected free energy has been proposed to date,
but all the formulations can be recovered from it. However, in this setting, the agent cannot have
arbitrary prior preferences over observations. Indeed, only a limited class of prior preferences over
observations is compatible with the likelihood mapping of the generative model. In the second setting,
ajustificationoftherootexpectedfreeenergydefinitionisknown,butthissettingonlyaccounts fortwo
formulations, i.e., the risk over states plus ambiguity and entropy plus expected energy formulations.
Keywords: Active Inference, Expected Free Energy, Unification Problem
1. Introduction
Active inference (Friston et al., 2016; Itti and Baldi, 2009; Schwartenbeck et al., 2018; FitzGerald et al.,
2015; Fountas et al., 2020; Sancaktar et al., 2020; C¸atal et al., 2020; Cullen et al., 2018; Millidge, 2019)
is a framework for decision-making under uncertainty, in which the agent is equipped with a (gener-
ative) model that encodes the environment dynamics, and a variational posterior approximating the
true posterior over latent variables. The variational posterior is computed by minimizing a function
called the variational free energy (VFE), also known as the negative evidence lower bound in machine
learning (Fox and Roberts, 2012; ?). While the variational posterior defines the most likely state of the
environment, it does not indicate which action should be selected. Instead, the agent aims to reach a
set of preferred states or observations, by minimizing the expected free energy (EFE).
1
While the variational free energy has a clear root definition that all other formulations are derived
from, the literature does not clearly identify such a root definition for the expected free energy, leaving
the question of antecedence amongst its many formulations as moot.
The EFE is a function defining the cost of performing a particular policy as a trade-off between
exploration and exploitation, e.g., the goal is to maximize pragmatic value (reward), while also maxi-
mizing information gain. The pragmatic value relies on the prior preferences of the agent, which specify
the preferred states or observations, and provides the agent with its goal-directed behaviour.
In the active inference literature, the prior preferences over observation are usually denoted P(o;C),
where C is a vector of parameters, i.e., there is no distribution over C. Thus, the prior preferences over
observation can also be written P(o). Importantly, the prior preferences are denoted using the letter
P, suggesting that these preferences are part of the generative model. However, P(o) in the generative
model refers to the marginal likelihood. Therefore, the symbol P(o) has a dual meaning, i.e., it refers to
both the prior preferences and the marginal likelihood. This dual meaning may not seem problematic
at first, but introduces hidden inconsistencies. For example, in most cases, the prior preferences will
be constant, i.e., not updating during a run. However, in general, the prior over future state P(s) will
change as time passes and the agent gathers new observations, thus the marginal likelihood will change,
suggesting an inconsistency resulting from the conflation of the two interpretations. More formally,
when using Bayes theorem:
P(o|s)P(s)
P(s|o) = ,
P(o)
the denominator corresponds to the joint distribution P(o,s) marginalized over the states s. However,
since the prior preferences are also denoted P(o), the denominator could be interpreted as being the
prior preferences, but in most cases: P(o) 6= P(o;C). Importantly, the dual meaning problem can also
emerge if the joint distribution P(o,s) is marginalized using the sum-rule of probability or split using
the product rule, i.e.,
P(o) = P(o,s) or P(o,s) = P(s|o)P(o).
s
X
To solve the dual meaning problem, the prior preferences are sometime considered as being part of a
target distribution. However, in this paper, we show that this assumption restricts the class of valid
prior preferences, and leads to a definition of the expected free energy that is not currently justified. In
the following sections, we explore two possible interpretations of Parr et al. (2022), and explain their
limitation. Appendix B and C provide a description of the properties used throughout this paper.
2. Generative model
In active inference, the agent is equipped with a (generative) model of its environment that spans all
time steps until the present time t. This model is composed of (a) hidden states s representing
0:t
states of the environment that the agent does not directly observe, (b) observations o , which represent
0:t
measurements made by the agent, and (c) actions a that the agent performed in the environment.
0:t−1
For the sake of conciseness, s , o , and a will be denoted s, o, and a, repectively. Moreover, in this
0:t 0:t 0:t−1
paper, we assume that observations depend on states, and each state depends on the state and action at
the previous time step. Formally, this setting is called a Partially Observable Markov Decision Process
(POMDP), and the model definition is as follows:
t t
P(o,s|a) =∆ P(s ) P(o |s ) P(s |s ,a ).
0 τ τ τ τ−1 τ−1
τ=0 τ=1
Y Y
3. Variational distribution
The generative model described in the previous section encodes prior beliefs about the environment
dynamics. However, when making measurements of key quantities, e.g., o, the agent needs to compute
2
posterior beliefs about the states, e.g., P(s|o,a). These posterior beliefs encode the new beliefs of
the agent when taking into consideration the new observations. Unfortunately, computing the true
posterior can either be analytically intractable or simply too computationally expensive. Therefore, the
true posterior is generally approximated by a variational distribution Q(s|a):
Q(s|a) ≈ P(s|o,a) ∝ P(o,s|a)
variational posterior true posterior generative model
In active inference, the variational posterior 1) factorises over time steps, i.e., a temporal mean-field
| {z } | {z } | {z }
approximation, but 2) all states still depend on the policy a. These two assumptions lead to the
following definition of the variational distribution:
t
Q(s|a) =∆ Q(s |a)
τ
τ=0
Y
4. Variational inference and the variational free energy
To sum up, the agent is provided with a generative model P(o,s|a) and a variational distribution
Q(s|a). Givensomemeasurements o, thevariationaldistributionneeds toapproximatethetrueposterior
P(s|o,a). This can be formally expressed as minimising the Kullback-Leibler divergence between the
approximate and true posteriors:
Q∗(s|a) = argminD [Q(s|a)||P(s|o,a)]
KL
Q(s|a)
Minimising this KL-divergence and minimising the variational free energy (VFE) is equivalent (see
proof below). Intuitively, the VFE trades-off accuracy, i.e., how well are the observations predicted, and
complexity, i.e., how far the posterior is from the prior. More formally, the VFE is defined as follows:
F Q(s|a),a,o = D [Q(s|a)||P(s|a)]−E [lnP(o|s)]
KL Q(s|a)
h i complexity accuracy
| {z } | {z }
The derivation of the variational free energy proceeds as follows:
Q∗(s|a) = argminD [Q(s|a)||P(s|o,a)]
KL
Q(s|a)
P(o|s)P(s|a)
= argmin E lnQ(s|a)−ln
Q(s|a)
P(o|a)
Q(s|a) " #
P = argmin E [lnQ(s|a)−lnP(o|s)P(s|a)]+lnP(o|a)
Q(s|a)
Q(s|a)
constant
= argminD [Q(s|a)||P(s|a)]−E [lnP(o| | s)] {z }
KL Q(s|a)
Q(s|a)
complexity accuracy
| {z } | {z }
= argmin F Q(s|a),a,o ,
Q(s|a)
h i
variationalfreeenergy
| {z }
3
where the d-separation criterion (Koller and Friedman, 2009) (which simplifies P(o|s,a) to P(o|s)
in the second line, given the independence of o and a given s) and Bayes theorem have been used,
P
along with the linearity of expectation and the log-property.
In this paper, we focus on planning, therefore we do not explain how the optimal variational
distribution Q∗(s|a) is computed. The interested reader is referred to (Champion et al., 2022b;
C Parr et al., 2019; Friston et al., 2017; Winn et al., 2005) for an approximate scheme based on vari-
ationalmessagepassing, and(Champion et al.,2022c;Kschischang et al.,2001)foranexactscheme
based on the sum-product algorithm.
5. Planning and the expected free energy
After performing inference, the agent has at its disposal posterior beliefs encoded by the optimal varia-
tional distribution Q∗(s|a). At this point, the agent needs to choose the next action to perform in the
environment. In active inference, the cost of a policy is given by the expected free energy (EFE), thus
the goal of planning is to identify the policy with the smallest EFE.
Unfortunately, given a time horizon of planning h, the number of policies is |A|h−t, where A is the
set of all actions available to the agent, and |A| is the cardinality of this set. As the number of policies
grows exponentially with the time horizon, computing the EFE of all policies requires exponential time.
Therefore, it is important to search the space of policies efficiently. For example, one can use Monte-
Carlo treesearch (Champion et al., 2022a,d), which maintainsa balancebetween exploiting policieswith
low EFE and exploring rarely visited action sequences. Another solution would be to use sophisticated
inference (Friston et al., 2021), which implements a tree search over actions and outcomes in the future,
by using a recursive form of expected free energy.
In this paper, we postulate that the expected free energy is based on two distributions: the fore-
cast F(o,s|a) and target T(o,s|a) distributions, where the future observations, states, and actions are
denoted o = o , s = s and a = a , respectively. The forecast distribution predicts the fu-
t+1:h t+1:h t:h−1
ture according to the agent’s best beliefs about the current states of the environment, and its generative
model. In contrast, the target distribution encodes the states and observations the agent wants to reach.
A general formulation of the target is given here, allowing it to change with policies. Although, in most
cases, it will not change.
Q Why is the forecast distribution introduced?
In the active inference literature, it is frequent to see Bayes theorem being used between factors of
the generative model P(o,s|a) and the variational distribution Q(s|a), or even to see factors from
the generative model P(x|y) being replaced by their variational counterpart Q(x|y). However,
Bayes theorem is a corollary of the product rule of probability, i.e.,
P(x|y)P(y)
A P(x,y) = P(x|y)P(y) = P(y|x)P(x) ⇔ P(y|x) = .
P(x)
Thus, technically, Bayes theorem cannot be used between factors of two different distributions. To
be really explicit, equality between P(y|x) and the right-hand-side would not hold if, for example,
P(y) were replaced by any distribution different to P(y), and similarly for P(x|y)P(y) and P(x).
Importantly, it is straightforward to see that the generative model and the variational distribution
4
are two different distributions. The easiest way to see this, is to realise that these two distributions
do not even share the same domain, i.e., the generative model is a distribution over states s and
observations o, while the variational distribution is a distribution over states s only. One might
consider observations to be implicitly present, but in this respect, they are a ground term, with a
particular value, rather than random variables with a distribution over possible values. Another
intuitive argument is that the generative model encodes the prior beliefs of the agent, then upon
A receiving newdata, theagent computes its (approximate) posterior beliefs. Iftheprior was equal to
the posterior, there will be no point in performing inference in the first place. Since the generative
model and the variational distribution are two different distributions, one should be careful when
replacing factors from one distribution by factors from the other. It is for this reason that the
forecast distribution is introduced. Put simply, the forecast distribution provides a bridge between
factors of the variational posterior and those of the generative model.
5.1 The unification problem
In this section, we formalise the problem of deriving the four EFE formulations, which can be found
in the literature (see below), from the EFE definition, i.e., the unification problem. Specifically, the
unification problem is a 4-tuple P = hF,T,G ,Ci, where F is the forecast distribution, T is the target
rt
distribution, G is the definition of the expected free energy, and C = {C ,C ,C ,C } is a set
rt RSA ROA IGPV 3E
containing the four formulations of the expected free energy. Solving P consists of finding a definition
of F, T, and G such that it is possible to derive C from G for all C ∈ C.
rt X rt X
We now define the four formulations of the expected free energy, which are based upon the definitions
in Parr et al. (2022). The formulation for the risk over states and ambiguity is as follows:
C (a) =∆ D [F(s|a)||T(s|a)]+ E H[F(o|s)] .
RSA KL F(s|a)
riskoverstates am(cid:2)biguity (cid:3)
Importantly, the risk over states is th|e KL-div{erzgence bet}wee|n the pr{ezdictive p}osterior over states F(s|a)
and the prior preferences over states T(s|a), and the ambiguity is the expected entropy of the likelihood
mapping according to the generative model. The risk over states pushes the predictive posterior towards
the prior preferences, while the ambiguity encourages the agent to visit states producing a low entropy
distribution over observations, i.e., if we arrive at a state, we know which observation(s) to expect. The
formulation for the risk over observations and ambiguity is as follows:
C (a) =∆ D [F(o|a)||T(o|a)]+ E H[F(o|s)] .
ROA KL F(s|a)
riskoverobservations am(cid:2)biguity (cid:3)
The ambiguity term is identical, an|d the risk{zover obse}rva|tions is {azKL-div}ergence, which pushes the
predictive posterior over observations F(o|a) to be as close as possible to the prior preferences over
observations T(o|a). The formulation for the information gain and pragmatic value is as follows:
C (a) =∆ −E D F(s|o,a)||F(s|a) −E [lnT(o|a)].
IGPV F(o|a) KL F(o|a)
(cid:2) info(cid:2)rmationgain (cid:3)(cid:3) pragmaticvalue
Importantly, the information g|ain is a KL-div{ezrgence that reli}es o|nly on{fzactors}from the forecast dis-
tribution. This prevents degenerate behaviours were the agent stops exploring its environment, i.e.,
information loss (Champion et al., 2023). In addition, the pragmatic value is based on the preferred
observations T(o|a), which provides the agent with its goal directed behaviour. Finally, the expected
energy and entropy formulation is as follows:
C (a) =∆ −H[F(s|a)]−E [lnT(o,s|a)]
3E F(o,s|a)
entropy expected energy
| {z } | {z }
5
Theentropytermensuresthatagoodpolicyisonewhichkeeps ouroptionsopenbyallowingustoreacha
wide varietyofstates, asimpliedbyJaynes’ theoryofmaximum entropy(Jaynes,1957a,b). Additionally,
as shown in the proof below, the expected energy encourages the agent to reach its preferred states,
while also pushing the agent to select states for which the associated distribution over observations has
low entropy, i.e., given a state, we know which observation(s) to expect.
Starting with the negative expected energy, one can use the product rule, the log-property and the
linearity of expectation to get:
−E [lnT(o,s|a)] = −E [lnT(o|s,a)]−E [lnT(s|a)].
F(o,s|a) F(o,s|a) F(s|a)
expected energy
Then, assuming th|at the f{ozrecast di}stribution is a partially observable Markov decision process,
and that the likelihood of the forecast and target distributions are the same, we obtain:
−E [lnT(o,s|a)] = −E [lnF(o|s,a)]−E [lnT(s|a)]
F(o,s|a) F(o,s|a) F(s|a)
expected energy
P
= −E [lnF(o|s)]−E [lnT(s|a)]
| {z } F(o,s|a) F(s|a)
where, F(o|s,a) = F(o|s), due to d-separation, observing that o and a are conditionally indepen-
dent given s. Lastly, using this same property again after applying the product rule and then using
the definition of entropy gives the final result:
−E [lnT(o,s|a)] = E [H[F(o|s)]]−E [lnT(s|a)],
F(o,s|a) F(s|a) F(s|a)
expected energy ambiguity pragmaticvalue
This equation shows|that ma{xzimising }expe|cted en{ezrgy mea}ns|selectin{gz states}that minimise the
ambiguity of the likelihood while maximising the pragmatic value of states.
5.2 Forecast distribution
As previously mentioned, the forecast distribution predicts the future according to the agent’s best
beliefs about the current states of the environment, and its generative model. More formally, the
forecast distribution factorizes as follows:
h h
F(o,s|a) =∆ F(s |a ) F(o |s ) F(s |s ,a ).
t+1 t τ τ τ τ−1 τ−1
τ=t+1 τ=t+2
Y Y
Additionally, wemakethreeassumptionstodefinethefactorsoftheforecastdistribution. Theseassump-
tions define the forecast distribution in terms of factors from the generative model and the variational
distribution. But importantly, these definitions are made explicit here, leaving no uncertainty about
these relationships amongst probability distributions. First, we assume that the likelihood in the future
F(o |s ) is the same as in the past P(o |s ), i.e., the likelihood of the forecast distribution is the same
τ τ τ τ
as the likelihood of the generative model. Second, the temporal transition in the future F(s |s ,a )
τ τ−1 τ−1
is the same as in the past P(s |s ,a ), i.e., the temporal transition of the forecast distribution is the
τ τ−1 τ−1
same as the temporal transition of the generative model. Third, the agent’s best prior over the current
state F(s ) is given by the optimal variational posterior Q∗(s |a). More formally:
t t
F(o |s ) = P(o |s )
τ τ τ τ
F(s |s ,a ) = P(s |s ,a )
τ τ−1 τ−1 τ τ−1 τ−1
F(s ) = Q∗(s |a)
t t
6
Using the above assumptions, we obtain F(s |a ) by taking the expectation of the temporal transition
t+1 t
w.r.t. the prior over states at time t. For generality, we define this as an integral, but this could be
specified to a summation in the discrete case:
F(s |a ) = F(s |s ,a )F(s ) ds = P(s |s ,a )Q∗(s |a) ds
t+1 t t+1 t t t t t+1 t t t t
Zst Zst
5.3 Target distribution
The second distribution of interest is the target distribution, which encodes the states and observations
that the agent wants to reach. In the following section, we define the target distribution as follows:
h
T(o,s|a) =∆ T(o |s )T(s |a),
τ τ τ
τ=t+1
Y
where T(o |s ) = P(o |s ) and T(s |a) = Cat(s ;C ). Note, C is the matrix known as the C matrix
τ τ τ τ τ τ s s
in the active inference literature, where the prior preferences are defined w.r.t. states. Also, the target’s
likelihood is equal to P(o |s ), which means that given any state, the agent wants to reach the obser-
τ τ
vations that will arise naturally according to the likelihood of the generative model, i.e., P(o |s ). If
τ τ
the target changes with policy a, this would have to be built into the C matrix, but in most cases, the
s
target distribution would be fixed.
5.4 Solving the unification problem
With the forecast and target distributions laid out, we now focus on the unification problem. We
will explore whether any of the EFE formulations could serve as a root definition from which all other
formulations couldbederived. First, we definetherootexpected freeenergy astherisk over observations
plus ambiguity:
G (a) =∆ D [F(o|a)||T(o|a)]+ E H[F(o|s)] = C (a). (1)
rt KL F(s|a) ROA
riskoverobservations am(cid:2)biguity (cid:3)
| {z } | {z }
5.4.1 The information gain / pragmatic value formulation
Inthissection, we demonstratethattheinformationgain/pragmaticvalueformulationcanberecovered
from the C (a)-as-root expected free energy definition. The derivation relies on the following equality:
ROA
F(s|a) F(o|a)
= , (2)
F(s|o,a) F(o|s)
whichholdsbecausetheforecastdistributionisapartiallyobservableMarkovdecisionprocess.
We start by re-arranging Bayes theorem as follows:
F(o|s,a)F(s|a) F(s|a) F(o|a)
F(s|o,a) = ⇔ = .
F(o|a) F(s|o,a) F(o|s,a)
Then, in a partially observable Markov decision process, o ⊥⊥ a | s, (i.e., the Markov property
P
ensures that observation sequences are conditionally independent of policies, if the sequence of
states is known), thus F(o|s,a) = F(o|s) and:
F(s|a) F(o|a)
= .
F(s|o,a) F(o|s)
7
Importantly, by starting with the definition of C (a) and using (2), one can show that:
IGPV
G (a) = −E [D [F(s|o,a)||F(s|a)]]−E [lnT(o|a)] = C (a). (3)
rt F(o|a) KL F(o|a) IGPV
informationgain pragmaticvalue
| {z } | {z }
Starting with the definition of C (a):
IGPV
C (a) = −E [D [F(s|o,a)||F(s|a)]]−E [lnT(o|a)],
IGPV F(o|a) KL F(o|a)
informationgain pragmaticvalue
and using the KL-divergence|definition, and{zthat F(o,s|a) =} F|(s|o,a){Fz(o|a), b}y the product rule,
we obtain:
C (a) = −E [lnF(s|o,a)−lnF(s|a)]−E [lnT(o|a)]
IGPV F(o,s|a) F(o|a)
Then, by using the log-properties and (2) to replace
F(s|o,a)
by
F(o|s),
we get:
P F(s|a) F(o|a)
C (a) = −E [lnF(o|s)−lnF(o|a)]−E [lnT(o|a)]
IGPV F(o,s|a) F(o|a)
Next, the linearity of expectation can be applied to re-arrange the expression as follows:
C (a) = −E [lnF(o|s)]+ E [lnF(o|a)−lnT(o|a)]
IGPV F(o,s|a) F(o|a)
Lastly, recognizing the entropy and KL-divergence definitions leads to the final results:
C (a) = D [F(o|a)||T(o|a)]+ E H[F(o|s)] = G (a).
IGPV KL F(s|a) rt
riskoverobservations am(cid:2)biguity (cid:3)
| {z } | {z }
5.4.2 The risk over states vs ambiguity formulation
Inthissection, wedemonstratethattheriskoverstatesplusambiguityisanupperboundoftheexpected
free energy. Restarting from the EFE definition, one can show that:
G (a) ≤ D [F(o,s|a)||T(o,s|a)]+ E H[F(o|s)] .
rt KL F(s|a)
(cid:2) (cid:3)
We follow the proof provided in Appendix B of Parr et al. (2022), but using our notation and going
from the end of the proof to the beginning. Restarting from the EFE definition:
G (a) = D [F(o|a)||T(o|a)]+ E H[F(o|s)] ,
rt KL F(s|a)
riskoverobservations am(cid:2)biguity (cid:3)
P
we obtain an upper bound on t | he EFE b { y z adding t } he f|ollowing{zbound, }which is the expectation of
a KL-divergence and cannot be negative:
G (a) ≤ D [F(o|a)||T(o|a)]+ E H[F(o|s)] + E D [F(s|o,a)||T(s|o,a)] .
rt KL F(s|a) F(o|a) KL
riskoverobservations (cid:2) (cid:3) (cid:2) bound (cid:3)
| {z } | {z }
8
Next, using the linearity of expectation and the log-property, the bound can be merged to the risk
over observations:
P
G (a) ≤ D [F(o,s|a)||T(o,s|a)]+ E H[F(o|s)] . (4)
rt KL F(s|a)
(cid:2) (cid:3)
Additionally, if one assumes that T(o|s) = F(o|s), then restarting from Equation 4, one can show that
the risk over states plus ambiguity is an upper bound of the expected free energy, i.e.,
G (a) ≤ D [F(s|a)||T(s|a)]+ E H[F(o|s)] = C (a). (5)
rt KL F(s|a) RSA
(cid:2) (cid:3)
Once again, we keep following the proof presented in Appendix B of Parr et al. (2022) backward.
Let us restart from Equation 4:
G (a) ≤ D [F(o,s|a)||T(o,s|a)]+ E H[F(o|s)] .
rt KL F(s|a)
(cid:2) (cid:3)
Then, using the definition of the KL-divergence, the linearity of expectation and the log-property,
we can split the KL-divergence as follows:
G (a) ≤ D [F(s|a)||T(s|a)]+ E H[F(o|s)]
rt KL F(s|a)
+ E [lnF(o|s)]−E [(cid:2)lnT(o|s)].(cid:3)
F(o,s|a) F(o,s|a)
P
Next, using our assumption that T(o|s) = F(o|s), we can get:
G (a) ≤ D [F(s|a)||T(s|a)]+ E H[F(o|s)]
rt KL F(s|a)
+ E [lnF(o|s)]−E [(cid:2)lnF(o|s)],(cid:3)
F(o,s|a) F(o,s|a)
=0
which simplifies to: | {z }
G (a) ≤ D [F(s|a)||T(s|a)]+ E H[F(o|s)] = C (a).
rt KL F(s|a) RSA
riskoverstates am(cid:2)biguity (cid:3)
| {z } | {z }
Importantly, since the risk over states plus ambiguity is an upper bound of the EFE, minimising the
upper bound will also minimise the EFE.
5.4.3 Expected energy vs entropy formulation
Finally, restarting from the risk over states plus ambiguity in Equation 5, one can demonstrate that:
G (a) ≤ C (a) = −H[F(s|a)]−E [lnT(o,s|a)] = C (a)
rt RSA F(o,s|a) 3E
entropy expected energy
Thus, the expected energy vs entropy fo|rmu{laztion} ca|n be reco{vzered in t}his setup.
Restarting from Equation 5:
P
G (a) ≤ C (a) = D [F(s|a)||T(s|a)]+ E H[F(o|s)] .
rt RSA KL F(s|a)
(cid:2) (cid:3)
9
and using the KL-divergence and entropy definitions, we obtain:
G (a) ≤ C (a) = E lnF(s|a)−lnT(s|a) −E lnF(o|s) .
rt RSA F(s|a) F(o,s|a)
(cid:2) (cid:3) (cid:2) (cid:3)
Then, using our assumption that F(o|s) = T(o|s), as well as the linearity of expectation and the
log-property, we get:
G (a) ≤ C (a) = E lnF(s|a)−lnT(s|a) −E lnT(o|s)
P rt RSA F(s|a) F(o,s|a)
= E (cid:2)[lnF(s|a)]−E [l(cid:3)nT(o,s|a)].(cid:2) (cid:3)
F(s|a) F(o,s|a)
Finally, recognizing the entropy definition, we obtain the desired result:
G (a) ≤ C (a) = −H[F(s|a)]−E [lnT(o,s|a)] = C (a).
rt RSA F(o,s|a) 3E
entropy expected energy
| {z } | {z }
6. Limitations
In the previous section, we defined the expected free energy as the risk over observations plus ambiguity,
i.e., C (a), and showed that it is equal to the information gain and pragmatic value formulation,
ROA
i.e., C (a). Then, following the proof in Appendix B of Parr et al. (2022), the risk over states plus
IGPV
ambiguity C (a) was shown to be an upper bound of C (a). Finally, the entropy plus expected
RSA ROA
energy formulation C (a) was derived from C (a). In summary:
3E RSA
G (a) =∆ C (a) = C (a) ≤ C (a) = C (a). (6)
rt ROA IGPV RSA 3E
Importantly, the proofs leading to equation (6) rely on the assumption that the likelihood of the forecast
and target distributions are equal, i.e., F(o|s) = T(o|s). In the following subsections, we study the
limitations of the formalism presented in Section 5.
6.1 Prior preferences over observations
In this section, we study the assumptions made in Section 5.4 and their consequences. For simplicity,
we only consider the case where the time horizon h is equal to t + 1, and where o , s , as well
t+1 t+1
as a are discrete random variables. In this case, o = o = o , s = s = s , and similarly
t t+1:h t+1 t+1:h t+1
a = a = a . Note, in Section 5.4, the EFE is defined as the risk over observations plus ambiguity,
t:h−1 t
i.e.,
G (a ) =∆ D [F(o |a )||T(o |a )]+ E H[F(o |s )] = C (a ).
rt t KL t+1 t t+1 t F(st+1|at) t+1 t+1 ROA t
riskoverobservations (cid:2)ambiguity (cid:3)
| {z } | {z }
6.1.1 The assumptions seemingly lead to an equation with no valid solution
In this section, we show that the assumptions made in Section 5, seemingly lead to an equation with no
valid solution. First, note that in the active inference literature, the prior preferences over observations
are defined as follows: T(o |a ) = Cat(o ;C ). Additionally, recall that the proof in Section 5.4.2
t+1 t t+1 o
relies on the assumption that T(o |s ) = F(o |s ), where the likelihood of the forecast distri-
t+1 t+1 t+1 t+1
bution is also the likelihood of the generative model, i.e., T(o |s ) = F(o |s ) = P(o |s ).
t+1 t+1 t+1 t+1 t+1 t+1
In the active inference literature, the likelihood of the generative model is defined as: P(o |s ) =
t+1 t+1
Cat(o |s ;A). To sum up, we have T(o |a ) = Cat(o ;C ) and T(o |s ) = Cat(o |s ;A).
t+1 t+1 t+1 t t+1 o t+1 t+1 t+1 t+1
Using the sum and product rules of probability, we have:
T(o |a ) = T(o |s ,a )T(s |a ) ⇔ C = AC , (7)
t+1 t t+1 t+1 t t+1 t o s
X
st+1
10
where without loss of generality, we let T(s |a ) = Cat(s ;C ). Importantly, the above assumes
t+1 t t+1 s
that:
T(o |s ,a ) = T(o |s ),
t+1 t+1 t t+1 t+1
which holds because in the target distribution, the observations are independent of the policy given the
states, i.e., using the d-separation criteria one can show that: o ⊥⊥ a | s . Re-starting from (7),
t+1 t t+1
one can solve for C and get:
s
C = AC ⇔ C = A−1C .
o s s o
However, the above equation may not have any valid solution. For example, if:
0.6 0.4 0.8
A = and C = , (8)
0.4 0.6 o 0.2
(cid:20) (cid:21) (cid:20) (cid:21)
then one can show that:
3 −2 2
A−1 = and C = A−1C = ,
−2 3 s o −1
(cid:20) (cid:21) (cid:20) (cid:21)
which is not a valid solution as C are the parameters of a categorical distribution. Thus, the elements
s
of C should add up to one, and be between zero and one. Importantly, finding a renormalization C¯
s s
of C is impossible. Indeed, if A is invertible, then the inverse is unique. Thus, C is the only matrix
s s
that satisfies: C = A−1C .
s o
6.1.2 The class of valid prior preferences over observations
The problem described in the previous section occurs because we are defining two distributions over the
random variable o , and these distributions are not compatible with each other. This indicates that in
t+1
the setting of Section 5, one cannot defined the prior preferences over observations arbitrarily. Instead,
given the likelihood mapping T(o |s ) = Cat(o |s ;A), one can only define prior preferences over
t+1 t+1 t+1 t+1
states, i.e., T(s |a ) = Cat(s ;C ), Then, the prior preferences over observations can be computed
t+1 t t+1 s
as follows:
T(o |a ) = T(o |s ,a )T(s |a ) ⇔ C = AC . (9)
t+1 t t+1 t+1 t t+1 t o s
X
st+1
While this tells us how the prior over observations can be computed, it does not characterise the class
of all valid prior preferences over observations. To characterise this class, we need to remember that
n×n matrices can be understood as linear transformations of the n-dimensional Euclidean space. To
understand this, let’s take the following 2×2 matrix as an example:
| |
B = ~ b ~ b ,
 1 2 
| |
 
and let:
1 0
~i = , and: ~j = ,
0 1
(cid:20) (cid:21) (cid:20) (cid:21)
be the two standard basis vectors. One can see that: ~ b = B~i and ~ b = B~j. In other words, the
1 2
matrix B transforms the vector ~i into the vector ~ b , and the vector ~j into the vector ~ b . More generally,
1 2
the matrix B transforms each vector ~x into a vector ~y = B~x. Geometrically, this can be understood
11
as mapping each point of the Euclidean space ~x, to another point ~y in the space. For example, the
transformation corresponding to the following matrix:
0.5 0.5
B = ,
0.5 −0.5
(cid:20) (cid:21)
is illustrated in Figure 1. Importantly, since the transformation is linear, the lines of the grid in Figure
1 remain parallel and equally spaced. Thus, knowing where ~i and ~j land under the transformation B is
enough to know where all the other points on the grid land, i.e.,
~i
and
~j
defines one parallelogram and all
the others (parallelograms) are obtained by copy and pasting this parallelogram along the transformed
axes. Going back to the following equation: C = AC , we can now better understand which prior
o s
preferences over observations C are compatible with the likelihood mapping A.
o
~j
~a
2
~i
~a
1
Figure 1: This figure illustrates how matrices can be seen as linear transformations of the Euclidean
space. The example taken is a matrix that rotates the Euclidean space by 45 degree clockwise
and scales each axis by a factor of 0.5.
Note that C is a linear transformation of C , where the transformation is defined by the elements of the
o s
matrix A. Moreover, C are the parameters of a categorical distribution, which means that its elements
s
are positive and sum up to one. Geometrically, this means that C is in the 1-dimensional simplex of
s
the Euclidean space. Additionally, since A defines the probability of each observation given each state,
all the elements of A are positive and the columns of A sum up to one. In other words, the columns of
A correspond to points on the 1-dimensional simplex of the Euclidean space.
Figure 2 illustrates the linear transformation corresponding to the A matrix of the previous section, c.f.,
Equation 8. Recall that the columns of A (i.e., ~a and ~a ) correspond to points on the 1-dimensional
1 2
simplex (represented in blue on the left of Figure 2). Importantly, the standard basis vector
~i
and
~j
are
mapped by A to ~a and ~a , respectively. While this is happening, the gray grid (Figure 2 left) will be
1 2
squeezed into the red grid (Figure 2 right), and the 1-dimensional simplex represented by a long blue
segment will be squeezed into a shorter blue segment. This short blue segment is the class of valid prior
preferences over observations C .
o
Indeed, the matrix A−1 performs the inverse linear transformation, i.e., A−1 transforms the red grid
into the gray grid. Therefore, if a point is on the short blue segment in the right-hand-side of Figure
2, it will be mapped back to the original 1-dimensional simplex (the long blue segment). However, if a
point starts on the blue dotted line (outside the short blue segment), it will be mapped outside of the
original 1-dimensional simplex.
12
To conclude, the class of valid prior preferences over observations is the class of all vectors C that
o
can be obtained as a linear combination of the columns of A, where the weights of the linear combina-
tion are positive numbers between zero and one that sum up to one, i.e., all the vectors that satisfies the
equation C = AC . Geometrically, the class of valid prior preferences over observations corresponds
o s
to the short blue segment obtained by applying A to the 1-dimensional simplex.
✗
✗
✓ ✓
Figure 2: This figure illustrates the linear transformation corresponding to the A matrix in Equation
8. The gray grid represents a set of points in the Euclidean space on which the linear trans-
formation corresponding to the A matrix will be applied. The red grid represents where the
gray grid lands when applying this linear transformation. The long (blue) segment on the
left-hand-side of the figure corresponds to the 1-dimensional simplex in which the prior pref-
erences over states C lives. The short (blue) segment on the right-hand-side of the figure
s
corresponds to where the 1-dimensional simplex lands when applying the linear transforma-
tion, i.e., the class of valid prior preferences over observations C . The green point (✓ on
o
the right-hand-side) corresponds to a point that lives on the projected simplex (i.e., the short
blue segment). This point will be projected back to the original 1-dimensional simplex (i.e.,
the long blue segment) by A−1. The brown point (✗ on the right-hand-side) corresponds to
a point that lives outside the short blue segment. This point will be projected outside of the
long blue segment by A−1.
6.2 Justification of the expected free energy
In this section, we discuss the expected free energy justification presented in Appendix B of Parr et al.
(2022). The appendix derives C (a) from the assumption that an agent aims to reach its prior
RSA
preferences, i.e., F(s ) = T(s |a ) for some time step τ in the future. Importantly, this derivation
τ τ τ−1
provides a justification for C (a), i.e., an upper bound of G (a) as defined in Section 5, but not for
RSA rt
G (a) itself. Thus, the expected free energy of Section 5, i.e., C (a), remains justified only by its
rt ROA
intuitive definition. To resolve this issue, we could try to define the expected free energy as the risk over
states plus ambiguity:
C (a) = C (a) ≤ C (a) = C (a) =∆ G (a).
ROA IGPV RSA 3E rt
In this case, we have a justification for the expected free energy, i.e., C (a), but C (a) and C (a)
RSA ROA IGPV
are now lower bounds of G (a). As minimizing a lower bound of the expected free energy does not imply
rt
minimizing the expected free energy, C (a) and C (a) cannot be recovered in this setting. The
ROA IGPV
results of the derivations presented in this paper are summarized in Table 1. Out of two potential
definitions for the expected free energy, one is justified but can only recover two EFE formulations, and
the other is not currently justified but can recover the four formulations.
13
d
e
G fi
G
P
S
A
O
A
E s
ti
CI C R C R C3 j u
risk over states vs ambiguity as definition ✗ ✓ ✗ ✓ ✓
risk over observations vs ambiguity as definition ✓ ✓ ✓ ✓ ✗
Table 1: This table summarises the results of the derivations of the expected free energy formulations,
and whether a justification of the EFE has been provided in the literature. The first row
corresponds to the definition of Parr et al. (2022) where the expected free energy is defined
as the risk over states vs ambiguity formulation, while the second row corresponds to another
interpretation of Parr et al. (2022) where the expected free energy is defined as the risk over
observations vs ambiguity formulation. Cells containing ✗ mean that the formulation cannot
be recovered (or no justification is known), while cells containing ✓ correspond to the case
where the formulation can be recovered (or a justification is known).
7. Conclusion
This paper aimed to formalize the expected free energy definition, as well as the problem of deriving
its four formulations, i.e., the unification problem. When the expected free energy is defined as the
risk over observations plus ambiguity, all formulations can be recovered, and can therefore be used in
practice. However, an important contribution of this paper was to show that some prior preferences over
observations are incompatible with the likelihood mapping. Thus, we are left with a dilemma, either
the modeller has to carefully pick the prior preferences of the agent to avoid any conflict, or we have to
let go of the theoretical connection between the four formulations.
Another issue is the absence of a justification for the risk over observations plus ambiguity formula-
tion. While there exists a justification for the risk over states plus ambiguity formulation, justifying a
lower bound is not enough to justify the expected free energy. Therefore, future research should focus
on finding a derivation of the risk over observations plus ambiguity from first principles. Importantly,
while the risk over states plus ambiguity is justified, this definition of the expected free energy does not
allow us to recover the four formulations. Thus, it does not constitute a valid solution to the unification
problem.
Note, we only studied two possible definitions of the expected free energy. An alternative set of
proofs and/or another factorization of the forecast and target definitions may allow us to recover all four
decompositions, while also removing the conflict between prior preferences and likelihood. However,
testing all possible factorizations and proofs is outside the scope of this paper.
Finally, this paper provides a solid foundation for future research, especially in the context of deep
active inference. Indeed, this paper clarifies the expected free energy definition, but unfortunately, it
does explain how to compute it using deep neural networks. Thus, additional research is required to
implement and empirically evaluate the proposed expected free energy definition.
References
Ozan C¸atal, Tim Verbelen, Johannes Nauta, Cedric De Boom, and Bart Dhoedt. Learn-
ing perception and planning with deep active inference. In 2020 IEEE International Con-
ference on Acoustics, Speech and Signal Processing, ICASSP 2020, Barcelona, Spain, May
4-8, 2020, pages 3952–3956. IEEE, 2020. doi: 10.1109/ICASSP40776.2020.9054364. URL
https://doi.org/10.1109/ICASSP40776.2020.9054364.
Th´eophile Champion, Howard Bowman, and Marek Grze´s. Branching time active in-
ference: Empirical study and complexity class analysis. Neural Networks, 152:450–
14
466, 2022a. ISSN 0893-6080. doi: https://doi.org/10.1016/j.neunet.2022.05.010. URL
https://www.sciencedirect.com/science/article/pii/S0893608022001824.
Th´eophile Champion, Lancelot Da Costa, Howard Bowman, and Marek Grze´s. Branch-
ing time active inference: The theory and its generality. Neural Networks, 151:295–
316, 2022b. ISSN 0893-6080. doi: https://doi.org/10.1016/j.neunet.2022.03.036. URL
https://www.sciencedirect.com/science/article/pii/S0893608022001149.
Th´eophile Champion, Marek Grze´s, and Howard Bowman. Multi-modal and multi-factor branching
time active inference, 2022c. URL https://arxiv.org/abs/2206.12503.
Th´eophile Champion, Marek Grze´s, and Howard Bowman. Branching Time Active Inference with
Bayesian Filtering. Neural Computation, 34(10):2132–2144, 09 2022d. ISSN 0899-7667. doi: 10.1162/
neco a 01529. URL https://doi.org/10.1162/neco_a_01529.
Th´eophile Champion, Marek Grze´s, Lisa Bonheme, and Howard Bowman. Deconstructing deep active
inference, 2023. URL https://arxiv.org/abs/2303.01618.
Maell Cullen, Ben Davey, Karl J. Friston, and Rosalyn J. Moran. Active inference
in openai gym: A paradigm for computational investigations into psychiatric ill-
ness. Biological Psychiatry: Cognitive Neuroscience and Neuroimaging, 3(9):809 –
818, 2018. ISSN 2451-9022. doi: https://doi.org/10.1016/j.bpsc.2018.06.010. URL
http://www.sciencedirect.com/science/article/pii/S2451902218301617. Computational
Methods and Modeling in Psychiatry.
Thomas H. B. FitzGerald, Raymond J. Dolan, and Karl Friston. Dopamine, reward learning, and active
inference. Frontiers in Computational Neuroscience, 9:136, 2015. ISSN 1662-5188. doi: 10.3389/
fncom.2015.00136. URL https://www.frontiersin.org/article/10.3389/fncom.2015.00136.
Zafeirios Fountas, Noor Sajid, Pedro A. M. Mediano, and Karl J. Friston. Deep ac-
tive inference agents using Monte-Carlo methods. In Hugo Larochelle, Marc’Aurelio
Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances
in Neural Information Processing Systems 33: Annual Conference on Neural Informa-
tion Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL
https://proceedings.neurips.cc/paper/2020/hash/865dfbde8a344b44095495f3591f7407-Abstract.htm
Charles W. Fox and Stephen J. Roberts. A tutorial on variational Bayesian inference. Artificial In-
telligence Review, 38(2):85–95, Aug 2012. ISSN 1573-7462. doi: 10.1007/s10462-011-9236-8. URL
https://doi.org/10.1007/s10462-011-9236-8.
Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, John O Doherty, and Gio-
vanni Pezzulo. Active inference and learning. Neuroscience & Biobehavioral Reviews, 68:862 – 879,
2016. ISSN 0149-7634. doi: https://doi.org/10.1016/j.neubiorev.2016.06.022.
Karl Friston, Lancelot Da Costa, Danijar Hafner, Casper Hesp, and Thomas Parr. Sophisticated In-
ference. Neural Computation, 33(3):713–763, 03 2021. ISSN 0899-7667. doi: 10.1162/neco a 01351.
URL https://doi.org/10.1162/neco_a_01351.
Karl J. Friston, Thomas Parr, and Bert de Vries. The graphical brain: Belief propagation and active
inference. Network Neuroscience, 1(4):381–414, 12 2017. ISSN 2472-1751. doi: 10.1162/NETN a
00018. URL https://doi.org/10.1162/NETN_a_00018.
Laurent Itti and Pierre Baldi. Bayesian surprise attracts human attention. Vision Research, 49
(10):1295 – 1306, 2009. ISSN 0042-6989. doi: https://doi.org/10.1016/j.visres.2008.09.007. URL
http://www.sciencedirect.com/science/article/pii/S0042698908004380. Visual Attention:
Psychophysics, electrophysiology and neuroimaging.
15
Edwin T Jaynes. Information theory and statistical mechanics. Physical review, 106(4):620, 1957a.
Edwin T Jaynes. Information theory and statistical mechanics. ii. Physical review, 108(2):171, 1957b.
Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques. MIT press,
2009.
Frank R Kschischang, Brendan J Frey, and H-A Loeliger. Factor graphs and the sum-product algorithm.
IEEE Transactions on information theory, 47(2):498–519, 2001.
Beren Millidge. Combining active inference and hierarchical predictive coding: A tutorial introduction
and case study., 2019. URL https://doi.org/10.31234/osf.io/kf6wc.
Thomas Parr, Dimitrije Markovic, Stefan J Kiebel, and Karl J Friston. Neuronal message passing using
mean-field, bethe, and marginal approximations. Scientific reports, 9(1):1889, 2019.
Thomas Parr, Giovanni Pezzulo, and Karl J Friston. Active inference: the free energy principle in mind,
brain, and behavior. MIT Press, 2022.
Cansu Sancaktar, Marcel A. J. van Gerven, and Pablo Lanillos. End-to-end pixel-based deep ac-
tive inference for body perception and action. In Joint IEEE 10th International Conference on
Development and Learning and Epigenetic Robotics, ICDL-EpiRob 2020, Valparaiso, Chile, Octo-
ber 26-30, 2020, pages 1–8. IEEE, 2020. doi: 10.1109/ICDL-EpiRob48136.2020.9278105. URL
https://doi.org/10.1109/ICDL-EpiRob48136.2020.9278105.
Philipp Schwartenbeck, Johannes Passecker, Tobias U Hauser, Thomas H B FitzGer-
ald, Martin Kronbichler, and Karl Friston. Computational mechanisms of curios-
ity and goal-directed exploration. bioRxiv, 2018. doi: 10.1101/411272. URL
https://www.biorxiv.org/content/early/2018/09/07/411272.
John Winn, Christopher M Bishop, and Tommi Jaakkola. Variational message passing. Journal of
Machine Learning Research, 6(4), 2005.
Appendix A: comments, questions, answers, and proofs
In this paper, we used the following blocks:
C A comment.
Q A question.
A An answer.
P A proof.
Appendix B: important properties
In this paper, we made extensive use of four basic properties. The first is the sum-rule of probability.
Given two sets of random variables X and Y , the sum-rule of probability states that:
P(Y) = P(X,Y) dX. (10)
X
Z
16
The sum-rule can then be used to sum out random variables from a joint distribution. The second
property is called the product-rule of probability, and can be used to split a joint distribution into
conditional distributions. Given two sets of random variables X and Y , the product-rule of probability
states that:
P(X,Y) = P(X|Y)P(Y). (11)
Next, if a and b are two real numbers, then a relevant property of logarithm is the following:
ln(a×b) = ln(a)+ln(b). (12)
Put simply, this allows us to turn the logarithm of a product into a sum of logarithms, and we will
refer to the above equation as the “log-property”. Finally, the last property is called the linearity of
expectation. Given a random variable X, and two real numbers a and b, the linearity of expectation
states that:
E [aX +b] = aE [X]+b, (13)
where the expectation is w.r.t. the marginal distribution over X, i.e., P(X).
Appendix C: d-separation criterion
Given a Bayesian network, the d-separation criterion provides a bridge between the graph topology and
theindependence assumptionsholdingwithintheBayesiannetwork. LetG = (V,E)beadirectedgraph
rt
corresponding to a Bayesian network, where V and E are the graph’s vertices and edges, respectively.
A trail is a sequence of vertices (V ,V ,...,V ) such that there is an edge V → V or V → V for all
1 2 k i i+1 i+1 i
i ∈ {1,...,k −1}. Intuitively, trails connect two variables V and V ; conceptually if a trail is blocked,
1 k
then V does not provide any new information about V through this trail. The notion of blocked trail
1 k
is based on colliders:
Definition 1 (Collider) Within a trail (V ,V ,...,V ), a collider is a vertex V s.t. V → V ← V .
1 2 k j j−1 j j+1
Definition 2 (Blocked trail) Given a set of vertices S ⊆ V, and two vertices V ,V ∈ V, we say that
1 2
a trail between V and V is blocked by S if at least one node of the trail is a collider not in S and with
1 2
no descendants in S, or at least one vertex of the trail that is not a collider is in S.
Importantly, two vertices in the graph can be connected through multiple trails; if all trails between
these two vertices are blocked, then we say that these vertices are d-separated. This can be generalized
to sets of vertices as shown below:
Definition 3 (D-separated) Given a set of vertices S ⊆ V, and two vertices V ,V ∈ V, we say that
1 2
V and V are d-separated by S if all trails between V and V are blocked.
1 2 1 2
Given three sets of vertices V ,V ,S ⊆ V. We say that V and V are d-separated by S, if each
1 2 1 2
C
node in V is d-separated from all nodes in V given the nodes in S.
1 2
Finally, the d-separation theorem states that: if two sets of vertices are d-separated in the graph, then
the associated random variables are conditionally independent, i.e.,
Theorem 4 (d-separation and independence) Given three sets V ,V ,S ⊆ V. If V and V are
1 2 1 2
d-separated by S, then V and V are conditionally independent given S, i.e., V ⊥⊥ V | S.
1 2 1 2
In practice, the d-separation theorem is used on the generative model’s graph. The goal is to know
whether aconditional assumptionholds, i.e., whether V ⊥⊥ V | S holds. Ifitdoes, then: P(V |S,V ) =
1 2 1 2
P(V |S).
1
17

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Reframing the Expected Free Energy: Four Formulations and a Unification"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
