=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Value of Information and Reward Specification in Active Inference and POMDPs
Citation Key: wei2024value
Authors: Ran Wei

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2024

Abstract: Expectedfreeenergy(EFE)isacentralquantityinactiveinferencewhichhasrecentlygainedpop-
ularity due to its intuitivedecompositionof the expectedvalue of controlinto a pragmaticand an
epistemic component. While numerous conjectures have been made to justify EFE as a decision
making objective function, the most widely accepted is still its intuitiveness and resemblance to
variationalfree energyin approximateBayesian inference. In this work, we take a bottom up ap-
proachandask: takingEFEasgiven,what’...

Key Terms: energy, agent, information, cognitive, specification, inference, pomdps, active, value, free

=== FULL PAPER TEXT ===

4202
guA
31
]IA.sc[
1v24560.8042:viXra
VALUE OF INFORMATION AND REWARD SPECIFICATION IN
ACTIVE INFERENCE AND POMDPS
RanWei
VERSESResearchLab
ran.wei@verses.ai
ABSTRACT
Expectedfreeenergy(EFE)isacentralquantityinactiveinferencewhichhasrecentlygainedpop-
ularity due to its intuitivedecompositionof the expectedvalue of controlinto a pragmaticand an
epistemic component. While numerous conjectures have been made to justify EFE as a decision
making objective function, the most widely accepted is still its intuitiveness and resemblance to
variationalfree energyin approximateBayesian inference. In this work, we take a bottom up ap-
proachandask: takingEFEasgiven,what’stheresultingagent’soptimalitygapcomparedwitha
reward-drivenreinforcementlearning(RL)agent,whichiswellunderstood?BycastingEFEunder
aparticularclassofbeliefMDPandusinganalysistoolsfromRLtheory,weshowthatEFEapproxi-
matestheBayesoptimalRLpolicyviainformationvalue.Wediscusstheimplicationsforobjective
specificationofactiveinferenceagents.
1 Introduction
Active inference (Parretal., 2022) is an agent modeling framework derived from the free energy principle, which
roughly states that all cognitive behavior of an agent can be described as minimizing free energy, an information
theoretic measure of the "fit" between the environment and the agent’s internal model thereof (Friston, 2010). In
recent years, active inference has seen increased popularity in various fields including but not limited to cognitive
and neuralscience, machinelearning, and robotics(Smithetal., 2021; Mazzagliaetal., 2022; Lanillosetal., 2021).
Onecommonapplicationofactiveinferenceacrossthesefieldsisinmodelingdecisionmakingbehavior,oftentaking
placeinpartiallyobservableMarkovdecisionprocesses(POMDP).Thisoffersactiveinferenceascomplementary,a
potentialalternativeto,orapossiblegeneralizationofoptimalcontrolandreinforcementlearning(RL).
The central difference between active inference and RL is that instead of choosing actions that maximize expected
reward or utility, active inference agents are mandated to minimize expected free energy (EFE), which in its most
commonformiswrittenas(DaCostaetal.,2020):
EFE(a)= E Q(o|a) [logP˜(o)] E Q(o|a) [KL[Q(so,a) Q(sa)]] (1)
− − | || |
Pragmaticvalue Epistemicvalue
Here,aisasequenceofactionstobee | valuated { , z Q(sa) } and | Q(oa)areth { e z agent’spredict } ionoffuturestatessand
| |
observationso, Q(so,a) is the futureupdatedbeliefsaboutstates givenfutureobservations,P˜(o) is a distribution
encodingtheagent’s | preferredobservationsandKLdenotesKullback-Leibler(KL)divergence,ameasureofdistance
betweentwodistributions.
OnecanobtainanintuitiveunderstandingoftheEFEobjectivebyanalyzingthetwotermsseparately.Thefirsttermis
thenegativeexpectedloglikelihoodofpredictedfutureobservationsunderthepreferenceortargetdistribution,which
isequivalenttothecrossentropybetweenthepredictedandpreferredobservationdistributions.Minimizingthisterm
encouragestheagenttotakeactionsthatleadtopreferredobservations.Itisthususuallyreferredtoasthe"pragmatic
value"or"expectedvalue". The secondtermis theexpectedKL divergencebetweenthepredictedfuturestatesand
updatedbeliefsaboutfuturestatesgivenfutureobservations,whichquantifiesthebeliefupdateamount. Thistermis
usuallyreferredtoas"epistemicvalue"or"expectedinformationgain"becauseitencouragestheagenttotakeactions
thatleadtoahigheramountofbeliefupdate–animplicitresolutionofuncertainty.
ValueofInformationandRewardSpecificationinActiveInferenceandPOMDPs
TheintuitiveadditionofpragmaticandepistemicvalueshasbeentakenasoneofthemajorappealsofEFE.Insome
sense, itputsbothvaluesunderthe"samecurrency"whenevaluatingthetotalvalueofactions(Fristonetal.,2015).
Thisperspectivehasmotivatedpriorworktointerpretepistemicvalueasthe"valueofinformation"(DaCostaetal.,
2020), a term which has a similar connotation in economics (Howard, 1966). Indeed, experimental evaluations of
activeinferenceagentshaveshownthattheepistemicvalueterminEFEcontributestostructuredexploratorybehavior,
resolving uncertainty before attempting to obtain reward, often leading to higher coverage of the state space and
enhancedtaskperformance(Millidge,2020;Tschantzetal.,2020;Engströmetal.,2024). Suchabehaviorprimitive
isespeciallyimportantinchallengingpartiallyobservabletaskenvironments.
Itappears,atafirstglance,thatRLandoptimalcontrolmisstheepistemicvalueterm. However,itiswidelyknown
that the Bayes optimal policy in POMDPs already trades off exploration and exploitation (Royetal., 2005). This
makesintuitivesensebecauseresolvinguncertaintyoftenleadstomoredownstreamrewards,essentiallyby"opening
up"opportunities. Specifically,the Bayesoptimalpolicyleveragesthe equivalencebetweenPOMDPs anda special
classofMDPsdefinedontherewardandtransitionofbeliefscalledbeliefMDPstocharacterizetheexpectedvalue
(i.e.,cumulativereward)followinganactiongiventhecurrentbelief,fromwhichanoptimalpolicycanbeconstructed
asamappingfrombeliefstoactions(Kaelblingetal.,1998). Thesepolicies,asdemonstratedbyBayesadaptiveRL
andmeta RL, also exhibitstructuredexploratorybehavior(Zintgrafetal., 2019; Duanetal., 2016). Itthusbegsthe
question:
What is the relationship between the Bayes optimal RL policy and the active inference policy based on optimizing
EFE?
Themaincontributionofthispaperisprovidingoneanswertotheabovequestion:
EFEapproximatestheBayesoptimalRLpolicyviaepistemicvalue.
WeachievethisbyfirstestablishingtheequivalencebetweentheEFEobjectiveandadifferentclassofbeliefMDPs,
whichallowsustodefineEFE-optimalpoliciesratherthanactionsequences(i.e.,plans)toformdirectcomparisons
with RL policies. We then examinethe source of epistemic behaviorin POMDPs usinga definition of the value of
informationfor POMDPs based on Howard’s informationvalue theory(1966). In brief, the value of informationis
thedifferenceintheexpectedvaluesbetweentheBayesoptimalpolicyandanother"naive"policywhichplansasifit
wouldnotbeabletoupdatebeliefsbasedonobservationsinthefuture.Whencastingthelatterpolicyalsousingbelief
MDPs,weobservethatitusesthesamebelieftransitiondynamicsastheEFEpolicybutitusesthesamebeliefreward
astheBayesoptimalpolicy. OurkeyresultisaregretboundshowingthattheEFEobjectiveclosestheperformance
gap between the naive policy and the Bayes optimal policy by augmenting the reward function of the former with
epistemicvalue.Wediscusstheimplicationsofourresultsforspecifyingactiveinferenceagentsinpractice.
OurworkiscomplementarytopriorworkexaminingtherelationshipbetweenactiveinferenceandRL(Millidgeetal.,
2020;Watsonetal.,2020;DaCostaetal.,2023)andtheeffectofepistemicvalueonagentbehavior(Schwöbeletal.,
2018; Koudahletal., 2021). However,instead of trying to derive the EFE objectivefrom first principles, we take a
"bottomup"approachandanalyzeitagainstthewell-knownBayesoptimalpolicy. Toourknowledge,thisisthefirst
regretboundofactiveinferenceagentsinrewardseekingtasks.
2 Background
Inthissection,weintroducenotationsforMarkovdecisionprocess,partiallyobservableMarkovdecisionprocess,and
thebeliefMDPviewofPOMDPs. WethenintroduceactiveinferenceandtheEFEobjective.
2.1 MarkovDecisionProcess
A discretetimeinfinite-horizondiscountedMarkovdecisionprocess(MDP;SuttonandBarto,2018)isdefinedbya
tuple M = ( , ,P,R,µ,γ), where is a set of states, a set of actions, P : ∆( ) a state transition
probabilitydi S stri A bution(alsocalledtran S sitiondynamics),R A : Rareward S fu × nc A tion → ,µ:∆ S ( )theinitialstate
S×A→ S
distribution,andγ (0,1)a discountfactor. Inthiswork, we considerplanningas opposedtolearning, wherethe
∈
MDPtupleM isknowntotheagentratherthanhavingtobeestimatedfromsamplesobtainedbyinteractingwiththe
environmentdefinedbyM. Weuseπ : ∆( )todenoteatime-homogeneousMarkovianpolicywhichmapsa
S → A
statetoadistributionoveractions.RollingoutapolicyintheenvironmentforafinitenumberoftimestepsT induces
asequenceofstatesandactionsτ =(s ,a )(alsoknownasatrajectory)whichisdistributedaccordingto:
0:T 0:T
T
P(τ)= P(s s ,a )π(a s ), (2)
t t−1 t−1 t t
| |
t=0
Y
2
ValueofInformationandRewardSpecificationinActiveInferenceandPOMDPs
where P(s s ,a ) = µ(s ). We use ρπ(s,a) = E[ ∞ γtPr(s = s,a = a)] to denote the state-action
o | −1 −1 0 P t=0 t t
occupancymeasureofpolicyπinenvironmentwithdynamicsP,wheretheexpectationistakenw.r.t. theinteraction
process(2)forT . WedenotethenormalizedoccupaPncymeasure,alsocalledthemarginalstatedistributionor
statemarginal,as → dπ( ∞ s,a)=(1 γ)ρπ(s,a).
P − P
SolvingaMDPreferstofindingapolicyπ whichmaximizestheexpectedcumulativediscountedrewardintheenvi-
ronmentJ(π)definedas:
∞
J(π)=E γtR(s ,a ) . (3)
t t
" #
t=0
X
The process of finding an optimalpolicy is sometimes referred to as reinforcementlearning and it is a well-known
resultthatthereexistsatleastonetime-homogeneousMarkovianpolicywhichisoptimalw.r.t. (3)(SuttonandBarto,
2018). Thissignificantlysimplifiesouranalysislatercomparedtofinitehorizonun-discountedMDPsforwhichthe
optimalpolicyistime-dependent.Thequantity 1 hasasimilarnotiontoplanninghorizon,becauseitrepresentsthe
1−γ
timestepatwhichdiscountingiseffectivelyzero. Theoptimalpolicyπ∗ ischaracterizedbytheBellmanoptimality
equation:
Q(s,a)=R(s,a)+γE s′∼P(·|s,a) [V(s′)], V(s)=maxQ(s,a), (4)
a
from which it can be obtained by taking the action which maximizes the action value function Q for each state as
π∗(as) = δ(a argmax Q(s,a˜)), whereδ(a b)isthediracdeltadistributionwhichhasprobability1ifa = b
a˜
| − −
andprobability0elsewhere. TheadvantagefunctionA(s,a) = Q(s,a) V(s) 0quantifiesthesuboptimalityof
− ≤
an action. We will omit the notation in most cases. When needed, we denote the value and advantage functions
associatedwithpolicyπandM ∗ DPM asQπ ,Vπ,Aπ .
M M M
2.2 PartiallyObservableMarkovDecisionProcess
A discrete time infinite-horizondiscountedpartially observableMDP (POMDP; Kaelblingetal., 1998) is character-
ized by a tuple M = ( , , ,P,R,µ,γ), where the newly introducedsymbol is a set of observations, and the
S A O O
new transitiondynamicsP consistsofthe state transitionprobabilitydistributionP(s s ,a ) andan observation
t+1 t t
|
emissiondistributionP(o s ). InaPOMDPenvironment,theagentonlyhasaccesstoobservationsemittedfromthe
t t
|
environmentstatebutnotthestateitself. ItisthusgenerallynotsufficienttoconsiderMarkovianpoliciesbutpolicies
thatdependonthehistoryofobservation-actionsequences,i.e.,π(a h )whereh =(o ,a ).
t t t 0:t 0:t−1
|
Itisawell-knownresultthattheBayesianbeliefdistributionb = P(s h )isasufficientstatisticfortheinteraction
t t t
|
history(Kaelblingetal.,1998). ThehistorydependentvaluefunctionsandpolicyinPOMDPcanthusbewrittenin
termsofbeliefs:
Q(b,a)= b(s)R(s,a)+γ P(o′ b,a)V(b′(o′,a,b)), V(b)=maxQ(b,a), (5)
| a
s o′
X X
whereP(o′ b,a)= P(o′ s′)P(s′ s,a)b(s)andb′(o′,a,b)denotesthebeliefupdatefunctionfrompriorb(s)to
|
s,s′
| |
theposterior:
P
P(o′ s′) P(s′ s,a)b(s)
b′(o′,a,b):=b′(s′ o′,a,b)= | s | . (6)
| P(o′ s′) P(s′ s,a)b(s)
s′ | P s |
TheoptimalpolicyderivedfromtheabovevaluefunctionPsissometimesPreferredtoastheBayesoptimalpolicy(Duff,
2002).
Thebeliefvaluefunctionsin(5)implyaspecialclassofMDPsknownasbeliefMDPs(Kaelblingetal.,1998)where
therewardanddynamicsaredefinedonthebeliefstateas:
R(b,a)= b(s)R(s,a), P(b′ b,a)=P(o′ b,a)δ(b′ ˜b′(o′,a,b)). (7)
| | −
s
X
Thestochasticityinthebeliefdynamicsisentirelyduetothestochasticityofthenextobservation;thebeliefupdating
processitselfisdeterministic.
Inthiswork,wegeneralizethenotionofbeliefMDPtorefertoanyMDPdefinedonthespaceofbeliefs. However,
notallbeliefMDPscouldyieldtheoptimalpoliciesforsomePOMDPs.
3
ValueofInformationandRewardSpecificationinActiveInferenceandPOMDPs
2.3 ActiveInference
Active inference is an application of the variational principle to perception and action, where intractable Bayesian
beliefupdates(i.e.,(6))areapproximatedbyvariationalinference(DaCostaetal.,2020). Ateverytimestept,vari-
ational inference searches for an approximate posterior Q(s ) which maximizes the evidence lower bound of data
t
marginalloglikelihood,orequivalentlyminimizesthevariationalfreeenergy :
F
(Q)=E [logQ(s ) logP(o ,s )], (8)
F
Q(st) t
−
t t
where P(o ,s ) = P(o s )P(s ). In the context of POMDPs, the prior is given by P(s ) =
t t t t t t
|
P(s s ,a )Q(s ). It is well-known that the optimal variational approximation under appropriately
st−1 t | t−1 t−1 t−1
chosen family of posteriordistributionsequalsto the exactposterior in (6) (Bleietal., 2017). We will thus assume
P
appropriatechoicesofvariationalfamilyandomitsuboptimalbeliefupdatinginsubsequentanalyses.
Centraltothecurrentdiscussionisthepolicyselectionobjectivefunctionsusedinactiveinference,whichisitsmain
differencefromclassicPOMDPs. Inparticular,activeinferenceintroducesanobjectivefunctioncalledexpectedfree
energy(EFE)which,givenaninitialbeliefQ (s )andafinitesequenceofactionsa ,isdefinedas(Fristonetal.,
0 0 0:T−1
2017):
EFE(a ,Q )=E [logQ(s a ) logP˜(o ,s )], (9)
0:T−1 0 Q(o1:T,s1:T|a0:T−1) 1:T
|
0:T−1
−
1:T 1:T
whereQ(s a )isdefinedastheproductofthemarginalstatedistributionsalongtheactionsequence(weshow
1:T 0:T−1
|
howthiscanbeapproximatelyobtainedasaresultofvariationalinferenceanddiscusstheimplicationofdefiningthis
insteadasthejointdistributionintheappendix,whichalsocontainsallderivationsandproofs):
T
Q(s a )= Q(s Q ,a ),
1:T 1:T−1 t t−1 t−1
| |
t=1 (10)
Y
Q(s Q ,a ):= P(s s ,a )Q(s Q ,a ),
t t−1 t−1 t t−1 t−1 t−1 t−2 t−2
| | |
s Xt−1
and Q(o ,s a ) = T P(o s )Q(s Q ,a ). These distributions represent the agent’s posterior
1:T 1:T | 1:T−1 t=1 t | t t | t−1 t−1
predictive beliefs about states and observations in the future. Notice (9) is different from (1), but it is used here
becauseitismoregeneral(ChaQmpionetal.,2024).
The distribution P˜(o ,s ) is interpreted as a "preference" distribution under which preferred observations and
1:T 1:T
stateshavehigherprobabilities.WhiletherearemultiplewaystospecifyP˜intheliterature,wewillfocusonthemost
popularspecification:
T
P˜(o ,s )= P˜(o )P˜(s o ), (11)
0:T 0:T t t t
|
t=0
Y
whereP˜(s o )isanarbitrarydistribution. ThisspecificationallowsustofactorizeEFEovertimeandconstructthe
t t
|
followingapproximation:
T
EFE(a ,Q ) E [logP˜(o )] E [KL[Q(s o ,a ) Q(s a )]], (12)
0:T−1 0
≈ −
Q(ot|a0:T−1) t
−
Q(ot|a0:T−1) t
|
t 0:T−1
||
t
|
0:T−1
t=1
X Pragmaticvalue Epistemicvalue
where KL denotes Kullback-L|eiblier dive{rzgence, Q(o} a| ) = P(o s ){Qz(s Q ,a ) is the p}osterior
t | 0:T−1 st t | t t | t−1 t−1
predictiveoverobservations,andQ(s o ,a ) P(o s )Q(s Q ,a )isthefutureposteriorgivenposterior
t t 0:T−1 t t t t−1 t−1
| ∝ | | P
predictiveoffuturestatesaspriorandfutureobservations.WediscussthisapproximationandoptimalchoiceofP˜(so)
|
furtherintheappendix.
As preempted in the introduction, in (12), the first term "pragmatic value" scores the quality of predicted observa-
tionsunderthepreferreddistribution. Thesecondterm"epistemicvalue"measuresthedistancebetweenfutureprior
Q(s a )andposteriorbeliefsQ(s o ,a ),whichcorrespondstotheamountofexpected"informationgain"
t 0:T−1 t t 0:T−1
| |
from futureobservations. The epistemic value term is an especially salient differencebetween active inferenceand
classicPOMDPs.
4
ValueofInformationandRewardSpecificationinActiveInferenceandPOMDPs
3 Unifying ActiveInference and RLUnder Belief MDPs
The use of EFE vs. reward and the search for action sequences (i.e., plans) vs. policies are the main contentions
between active inference and RL. In this section, we show that active inference can be equally represented using
rewardandpolicyinaspecialclassofbeliefMDPs. ThekeyistoshowthattheEFEobjectivecanbecharacterized
usingarecursiveequationakintotheBellmanequation.Thiscanbeachievedimmediatelybyexpressingthepredictive
distributionateachstepusingthepredictivedistributionatthepreviousstep:
EFE(a ,Q )
0:T−1 0
T
E [logP˜(o )] E [KL[Q(s o ,a ) Q(s a )]]
≈ −
Q(ot|a0:T−1) t
−
Q(ot|a0:T−1) t
|
t 0:T−1
||
t
|
0:T−1
t=1
X (13)
T−1
= E [logP˜(o )] E [KL[Q(s o ,Q ,a ) Q(s Q ,a )]]
−
Q(ot+1|Qt,at) t+1
−
Q(ot+1|Qt,at) t+1
|
t+1 t t
||
t+1
|
t t
t=0
X
= E [logP˜(o )] E [KL[Q(s o ,Q ,a ) Q(s Q ,a )]]+EFE(a ,Q ).
−
Q(o1|Q0,a0) 1
−
Q(o1|Q0,a0) 1
|
1 0 0
||
1
|
0 0 1:T−1 1
TherecursiveequationimpliesatransitiondynamicsoverthestatemarginalQ whichonlydependsontheprevious
t
statemarginalQ ,i.e.,thetransitionisMarkovian.Theper-timestepEFEonlydependsonthecurrentstatemarginal.
t−1
UsingtheequivalencebetweentheoptimalQ andb ,wecanwritetherewardandtransitiondynamicsofthebelief
t t
MDPimpliedbyEFEasfollows:
REFE(b,a)=E P(o′|b,a) [logP˜(o′)]+E P(o′|b,a) [KL[b(s′ o′,b,a) b(s′ b,a)]] (14a)
| || |
:=R˜(b,a)+IG(b,a), (14b)
Popen(b′ b,a)=δ(b′ b′(a,b)), whereb′(a,b):=b′(s′ b,a)= P(s′ s,a)b(s). (14c)
| − | |
s
X
ByconstructingtheabovebeliefMDP,thesearchforoptimalactionsequencescanbeequallyrepresentedasthesearch
foroptimalbelief-actionpolicies.
Proposition 3.1. (Active inference policy) The EFE achieved by the optimal action sequence can be equivalently
achievedbyatime-indexedbelief-actionpolicyπ(a b ).
t t
|
Proof. TheproofisduetotheabovebeliefMDPcharacterization.Analternativeproofisgivenintheappendix.
These identitiesenableusto defineinfinite-horizondiscountedbeliefMDPs usingthe EFE rewardanddynamicsin
(14) and restrict our search to time-homogeneousMarkovian belief-action policies. A similar result was presented
recentlybyMalekzadehandPlataniotis(2022). However,ratherthanfocusingonpolicyoptimizationalgorithms,our
goalhereistoclarifythebeliefMDPimpliedbyEFE.
However,noticeafewdifferencesbetweentheEFEbeliefMDPandtheBayesoptimalbeliefMDP.First, thebelief
dynamicsin(14c)doesnotcontainobservationo;ratheritisthemarginalpredictionofthenextstategiventheprevious
belief. Suchabeliefdynamicshasbeenreferredtoasopen-loopintheliterature(Flaspohleretal.,2020)inthesense
thatit doesnottake into accountthe possibility of updatingbeliefsbased on futureobservations,akin to open-loop
controls.IncontrasttothePOMDPbeliefdynamicsin(7),theopen-loopbeliefdynamicsisdeterministicgivena.
Second,theEFErewardfunctioncontainsaninformationgaintermwhichcorrespondstoepistemicvalue. Thefirst
termpragmaticvalueisdefinedastheexpectedloglikelihoodofthenextobservation. Thisdoesnotintroducemuch
differencefromthePOMDPrewardfunctionbecausewecandefinetheactiveinferencepreferencedistributionasa
BoltzmanndistributionparameterizedbyarewardfunctionP˜(o) exp(R˜(o))andassumethatR˜(o)self-normalizes
∝
sothatthepartitionfunctionequals1. Theresultingrewardcanstillbewrittenasalinearcombinationofstate-action
reward:
R˜(b,a)=E
P(o′|b,a)
[logP˜(o′)]
= b(s) P(s′ s,a) P(o′ s′)R˜(o′)
| | (15)
s s′ o′
X X X
= b(s)R˜(s,a).
s
X
5
ValueofInformationandRewardSpecificationinActiveInferenceandPOMDPs
Thelinearityandthusconvexityofthestate-actionrewardisanimportantpropertyofPOMDPs,becauseitimpliesthe
optimalvaluefunctionisalsoconvexinthebeliefs,whichmeansthatlowerentropyormorecertainbeliefsgenerally
correspondtohighervalues(Kaelblingetal.,1998).Theadditionofinformationgain,however,makestheEFEreward
no longerconvex. In this case, the agentmay be drivento collectmoreinformationand "distracted"fromaccruing
taskrewards.
Proposition3.2. TheEFErewardfunctionasdefinedin(14b)isconcaveinthebelief.
Proofsketch. Informationgaincanberearrangedasfollows:
IG(b,a)=H[P(o′ b,a)] E P(s′|b,a) [H[P(o′ s′)]] (16)
| − |
whereHdenotesShannonentropyandthesecondtermislinearinthebelief. Sinceentropyisconcave,thecombined
rewardfunctionisalsoconcave.
Insummary,theEFEobjectiveandtheclassicPOMDPcanbeunderstoodastwodifferentbeliefMDPswithdifferent
rewardfunctionsanddifferentdynamics.
4 Analyzing PoliciesinMDPs
ThebeliefMDPcharacterizationsofboththeEFEpolicyandtheBayesoptimalpolicyenableustouseMDPanalysis
toolsforPOMDPs. Themainanalysistoolsweuseinthispaperarerecentextensionsoftheperformancedifference
lemma(KakadeandLangford,2002)andsimulationlemma(KearnsandSingh,2002)whicharewell-knownresults
in RL theory that quantify the performancedifferencebetween differentpolicies or the same policy in differenten-
vironments. To compare active inference with RL, we are interested in the setting where two policies are optimal
w.r.t. bothdifferentrewardsanddifferentdynamics,however,the evaluationrewardanddynamicsare equivalentto
only one of the policies (here referredto as the expertpolicy). The following lemma, which extendslemma 4.1 in
(Vemulaetal.,2023)tothesettingofdifferentrewards,givestheperformancegap(alsoknownastheregret)between
thetwopolicies:
Lemma 4.1. (Performance difference in mismatched MDPs) Let π and π′ be two policies which are optimal w.r.t.
twoMDPsM andM′. ThetwoMDPssharethesameinitialstatedistributionanddiscountfactorbuthavedifferent
rewardsR,R′ anddynamicsP,P′. Denote∆R(s,a) = R′(s,a) R(s,a). Theperformancedifferencebetweenπ
andπ′ whenbothareevaluatedinM isgivenby: −
J (π) J (π′)
M M
−
= (1 1 γ) E (s,a)∼dπ P Aπ M ′ ′(s,a)
− h i
Policyadvantageunderexpertdistribution
+| (1 1 γ) E (s,a { ) z ∼dπ P ′ ∆R(s,a})+γ E s′∼P′(·|s,a) [V M π′ ′(s′)] − E s′′∼P(·|s,a) [V M π′ ′(s′′)] (17)
− h (cid:16) (cid:17)i
Reward-modeladvantageunderowndistribution
+| (1 1 γ) E (s,a)∼dπ P − ∆R(s,a)+γ E s′′∼ { P z (·|s,a) [V M π′ ′(s′′)] − E s′∼P′(·|s,a) [V M π′ ′(s′)]} .
− h (cid:16) (cid:17)i
Reward-modeldisadvantageunderexpertdistribution
Lemma4.1deco | mposestheperformancegapinMDPM b { e z tweenpolicyπ (theexpert)andπ′ intothre } eterms. The
first term is the advantage value of π′ under the expert’s state-action marginaldistribution. The second term is the
differenceinrewardbetweenMDPM′ andM andthedifferenceinthevalueVπ′ ofπ′ inM′ duetothedifference
M′
indynamicsexpectedunderthestate-actionmarginaldistributionofπ′. Thistermquantifiesthe"advantage"ofbeing
evaluatedinoneMDPvsanother.Thelasttermistheoppositeofreward-modeladvantage,i.e.,disadvantage,expected
undertheexpertpolicyπ’sstate-actionmarginaldistribution.
One can obtain an intuitive understanding of (17) by attempting to minimize the performance gap via optimizing
R′,P′,givenwerequireπ′tobetheoptimalpolicyw.r.t. someR′,P′. First,itholdsthatwhenR′,P′arerespectively
equaltoR,P,therewardandmodeladvantagesarezeros,andthepolicyadvantageiszeroasaresult.Thismeansone
canreadpolicy,reward,andmodeladvantageasameasureoferrorfromtheexpertMDPandpolicy.Whensucherror
isnonzero,R′,P′areoptimizedtoincreasereward-modeladvantageundertheexpertdistributionanddecreasereward-
modeladvantageunderthepolicy’sowndistribution. Thisencouragesπ′ tochooseactionsthatleadtostate-actions
6
ValueofInformationandRewardSpecificationinActiveInferenceandPOMDPs
achievedbytheexpertpolicy,eventuallymatchingexpertdistributionandthusexpertperformance.Thispropertyhas
beenusedtolearnvalue-awaredynamicsmodelstorobustlyimitateexpertbehaviorinofflineinversereinforcement
learning(Weietal.,2023).
Using(17), we canobtainanupperboundontheperformancegapintermsofthe policyadvantageandrewardand
modeldifference:
a ǫ L P c e t ′ m io = m n E m a ( a 4 s r . , 2 g a) . i ∼ n F a d o l π P r d [K e th n L e si [ s t P e y t ( r t · i a | n s t g i , o a c ) o d | π P n | P ′ s ( i s d ′ , ( a e · ) r | s e , d a i ) C n ]] . l , e T a m h n m e d a p R e 4 m ′ r . f 1 a o , x r l m e = t a ǫ n m π c ′ e a = x g s a E , p a ( | i s R s ,a b ′ ) ( ∼ o s u d , π P n a d [ ) | e | A . d π M L a ′ e s ′ t : (s th , e a) tw | ], o ǫ p R o ′ l = icie E s (s h , a a) v ∼ e d b π P o [ u | ∆ nd R e ( d s s , t a a ) t | e ] - ,
dπ P (s,a) ≤
1 C+1 (C+1)γR′
J M (π) − J M (π′) ≤ 1 γ ǫ π′ + 1 γ ǫ R′ + (1 γ)2 max√2ǫ P′ (18)
− − −
Lemma4.2showsthatthe performancegapis linear(w.r.t. planninghorizon)in the expectedpolicyadvantageand
rewarddifferenceandquadraticinthemodeldifference.Thus,modeldifferenceisamaincontributortoperformance
differenceifithasasimilarmagnitudetopolicyandrewarddifferences. However,itshouldbenotedthatthisbound
canbeoverlyconservative(sometimesknownastheworst-casebound;Rossetal.,2011)sinceitdoesn’tconsiderthe
possibilityofrewardadvantagebeingcancelledoutbymodeladvantage.
5 ValueofInformationinPOMDPs
GiventheprimarydifferencebetweenactiveinferenceandRListhedefinitionofepistemicvalueandopen-loopbelief
dynamics,weaskwhetheritcouldbeseenasanapproximationtotheBayesoptimalpolicy,specificallytheepistemic
aspectthereof?Tothisend,wefirstanalyzethe"valueofinformation"intheBayesoptimalpolicy.Wethenshowthat
epistemicvalueclosesthegaptotheBayesoptimalpolicybymakingupforthelossofinformationvalue.
5.1 ValueofInformationinBayesOptimalRLPolicy
It’scolloquiallyacceptedthattheBayesoptimalpolicycharacterizedbythevaluefunctionsin(5)optimallytradesoff
explorationandexploitation.However,it’snotimmediatelyobviouswhatisbeingtradedoff,thecomparisonismade
againstwhichalternativeactionorpolicy,andhowlargeistheperformancegap. Inthispaper,weadopttheviewthat
what’sbeingtradedoffisthevalueofinformation,whichwetrytoquantifyinanactionorpolicy.In(Howard,1966),
thevalueofinformationforasinglestepdecisionmakingproblemisdefinedastherewardadecisionmakeriswilling
to give away if they could have their uncertainty resolved (e.g., by a clairvoyant). Formally, the expected value of
perfectinformation(EVPI)isdefinedasthedifferencebetweentheexpectedvaluegivenperfectinformation(EVPI)
|
andtheexpectedvaluewithoutperfectinformation(EV).
InthePOMDPsetting,theagentcannotingeneralobtainperfectinformationaboutthehiddenstate,butanobservation
that is usually correlated with the state. It turns outthat this correspondsto an extension of Howard’sdefinition in
the single step decision making setting called the value of imperfect information (RaiffaandSchlaifer, 2000). For
consistencyinnotation,wewilllabelitastheexpectedvalueofperfectobservation(EVPO)anddefineitas:
EVPO =EV PO EV ,
| −
EV =max b(s)R(s,a),
a (19)
s
X
EV PO = P(os)b(s)maxR(b(so),a).
| | a |
o s
XX
Similar to EVPI, EVPO is non-negativebecause an optimal decision maker cannot gain information and do worse
(Howard,1966).
Extendingthisdefinitionforthemulti-stagesequentialdecisionmakingsetting,wehavethefollowingcorollaryofEV
andEVPOforPOMDPs:
|
EV : Qopen(b,a)= b(s)R(s,a)+γVopen(b′(a,b)), (20a)
s
X
EV PO: Q(b,a)= b(s)R(s,a)+γ P(o′ b,a)V(b′(o,a,b)). (20b)
| |
s o′
X X
7
ValueofInformationandRewardSpecificationinActiveInferenceandPOMDPs
The definition is the same as that of Flaspohleretal. (2020), except here we introduce additional motivation and
justificationbasedontheframeworkofHoward(1966)andRaiffaandSchlaifer(2000). ItisclearthatEVPOisthe
|
sameastheBayesoptimalvaluefunction.Interestingly,EVusestheopen-loopbeliefdynamicsthatwesawearlierin
EFEbutitusesthesamerewardasBayesoptimalpolicy. WethuslabelitsvaluefunctionsasQopen andVopen. The
followingpropositionshowsthatEVPOinthePOMDPsettingisalsonon-negative:
Proposition 5.1. Let Qopen(b,a),Vopen(b) and Q(b,a),V(b) denote the open and closed-loop value functions as
definedin(20),itholdsthat:
Q(b,a) Qopen(b,a)andV(b) Vopen(b)forallb ∆( )anda . (21)
≥ ≥ ∈ S ∈A
Intuitively,theclosed-loopBayesoptimalpolicyisbetterbecauseitcantakeactionsthatleadtofutureobservations
whichuponupdateleadtolowerentropybeliefs. Giventheclosed-loopvaluefunctionisconvexinthebeliefs,lower
entropybeliefsgenerallyhavehighervalue.Theseactionsarereferredtoasepistemicactions.
However,simplycomparingopenandclosed-loopvaluefunctionsdoesn’tgiveusanadequatemeasureofthevalue
ofinformationsinceinmostrealisticsettings,agentsareallowedtoobservetheenvironmentandupdatetheirbeliefs
despiteusingpotentiallysuboptimalopen-looppolicies. We thusconsiderthissetting bydeployingbothpoliciesin
a POMDP for which the closed-looppolicy is optimal, and the only differencebetween the two policies is that the
open-looppolicywillchooseactionsaccordingto (20a) asif itwouldnotbeabletoobservetheenvironmentin the
future. Fromlemma4.1weknowthattheprimarycontributortotheperformancegapbetweenthetwopoliciesisthe
differenceintheirtransitiondynamicsandtheresultingmodeladvantage.Thefollowingpropositioncharacterizesthe
advantageoftheclosed-loopdynamics:
Proposition5.2. LetR =max R(s,a). Theclosed-loopmodeladvantageisboundedasfollows:
max s,a
| |
R
0
≤
E P(b′|b,a) [Vopen(b′)]
−
E Popen(b′′|b,a) [Vopen(b′′)]
≤ 1
ma
γ
x 2IG(b,a). (22)
−
p
Itshowsthattheadvantageofclosed-loopdynamicsisprimarilyduetoinformationgainwhichscaleslinearlyw.r.t.
theplanninghorizon.
5.2 MainResult: EFEApproximatesBayesOptimalRLPolicy
The main insight of this work is that EFE closes the optimality gap between open and closed-looppolicies by aug-
mentingtherewardoftheopen-looppolicywiththeepistemicvalueterm. Giventhepragmaticvalueislinearinthe
belief(15),wewilluseitasthesharedrewardbetweenactiveinferenceandRLagents,i.e.,R(s,a)=R˜(s,a).
Proposition5.2showsthattheadvantageofclosed-loopbelieftransitionisproportionaltotheinformationgainpro-
videdbythe nextobservation. While theagentcannotchangeeither belieftransitiondistributions,it canchangeits
reward to alter the reward-modeladvantage and the marginal distribution under which it is evaluated. An obvious
choicefortherewardadvantageistosetittotheinformationgaininordertocancelwiththeinformationdisadvantage
ofopen-loopbelief dynamics. To ensurethe agentdoesnotgetdistractedby gaininginformationandstill focuson
taskrelevantbehavior,wemakethefollowingassumptiononpreferencedistributionspecification:
Assumption 5.3. (Preference specification)The preference distribution or reward is specified such thatthe gain in
pragmaticvalueafterreceivinganewobservationishigherthanthelossinepistemicvalueinexpectationunderthe
Bayesoptimalpolicyπinclosed-loopbeliefdynamicsP:
E
(b,a)∼dπ
(b(so) b(s))R(s,a) E (b,a)∼dπ[IG(b(s),a) IG(b(so),a)]. (23)
P " | − #≥ P − |
s
X
Thisassumptionalsoensuresthattheadvantageofclosed-loopbeliefdynamicsundertheEFEvaluefunctionisnon-
negative. Inpractice,sincetheBayesoptimalpolicybehaviorcanbedifficulttoknowapriori,wecanapproximate
theabovebysettingarewardfunctionsuchthattherewarddifferenceissufficientlyhigh. Intheappendix,weprove
thattheadvantageupperboundgiventhisassumptionisthesameasthatevaluatedundertheopen-loopbeliefMDPin
proposition5.2. Tofacilitatethecomparisonbetweenopen-loopandEFEpolicy,weintroducetwomoreassumptions:
Assumption5.4. (Policybehavior)Wemakethefollowingassumptionsonthebehavioroftheevaluatedpolicies:
1. The absolute advantage of the EFE policy πEFE expected under the Bayes optimal policy’s marginal
distribution is no worse than that of the open-loop policy πopen: ǫ π˜ = E (b,a)∼dπ P [ | Aπ P open (b,a) | ] ≥
E (b,a)∼dπ P [ | Aπ P EFE (b,a) | ].
8
ValueofInformationandRewardSpecificationinActiveInferenceandPOMDPs
2. Forboththeopen-looppolicyπopen andEFEpolicyπEFE, italwaysholdsthatIG(b,a) 2foranyb,a
≥
sampledfromeithertheirownortheexpertpolicy’smarginaldistribution.
Notethatbothassumptionsareconservativebuttheywillenableustofocusthecomparisonofbothpoliciesontheir
informationseekingbehavior.Assumption1isreasonablebecauseweexpecttheEFEpolicytobemoresimilartothe
expertthan the open-looppolicy given the informationgain reward encouragesinformationseeking behavior. This
enablesustoremovepolicyadvantagefromthecomparison.Assumption2ispartlynumericallymotivatedbecauseit
allowsustofurtherupperboundtheclosed-loopmodeladvantageinproposition5.2via√2KL KLsothattheIG
≤
rewardbonusinEFEcanbedirectlycomparedwithclosed-loopmodeladvantageandsubtractedfromit. Inpractice,
many POMDP environmentsare much more benign in that partial observability, and thus the value of information,
decreasesto zero in a smallnumberof time steps (Liuetal., 2022). In that case, the differencebetweenopen-loop,
EFE,andBayesoptimalpoliciesbecomeverysmall. Thus,thesettingweconsiderisharderormorepessimistic.
Thefollowingtheorem,whichisthe mainresult, givestheperformancegapofbothpoliciescomparedtotheBayes
optimalpolicy:
Theorem 5.5. Let all policies be deployed in POMDP M and all are allowed to update their beliefs accordingto
b′(o′,a,b). Let ǫ
IG
= E (b,a)∼dπ[IG(b,a)] denotesthe expectedinformationgainunderthe Bayesoptimalpolicy’s
P
belief-action marginal distribution and let the belief-action marginal induced by both open-loop and EFE policies
have bounded density ratio with the Bayes optimal policy
dπ
P
˜(b,a)
C. Under assumptions 5.3 and 5.4, the
dπ P (b,a) ∞ ≤
performancegapoftheopen-loopandEFEpoliciesfromthe(cid:13)optimal(cid:13)policyareboundedas:
(cid:13) (cid:13)
(cid:13) (cid:13)
1 (C+1)γR
J (π) J (πopen) ǫ + maxǫ ,
M − M ≤ 1 γ π˜ (1 γ)2 IG
− − (24)
1 (C+1)γR C+1
J (π) J (πEFE) ǫ + maxǫ ǫ .
M − M ≤ 1 γ π˜ (1 γ)2 IG − 1 γ IG
− − −
Theorem5.5showsthattheperformancegapofbothpoliciesarelinear(w.r.t.planninghorizon)inthepolicyadvantage
and quadratic in the information gain. However, the EFE policy improves over the open-loop policy with a linear
increase in information gain. As mentioned before, these bounds are conservative estimates since the information
seekingproprietyoftheEFEpolicycouldfurtherreducepolicydisadvantageandtheIGbonuscouldfurtherreduce
closed-loopmodeladvantage.
6 Discussions
OurresultshighlightthenuancedrelationshipbetweenactiveinferenceandtheclassicapproachtoPOMDPs. Inthis
section,weprovideafewcomplementaryperspectivesonrelatedPOMDPapproximationandextensionsfromtheRL
literatureanddiscussobjectivespecificationinactiveinferenceinformedbyourresults.
6.1 POMDPApproximationandExtensions
InthePOMDPplanningliterature,thereisasuiteofapproximationtechniquestoovercometheintractabilityofexact
beliefupdateandvaluefunctionrepresentation. ThesimplestonesarethemaximumlikelihoodheuristicandQMDP
heuristicwhichfirstcomputetheunderlyingMDPvaluefunctionandthenobtainthebeliefvaluefunctionusingeither
themostlikelystateunderthecurrentbelieforabelief-weightedaverage(Littmanetal.,1995).Theseapproximations
leveragethefactthatMDPvaluefunctions(indiscretespace)areeasytocompute,buttheycanbeoverlyoptimistic
sincetheyimplicitlyassumethestateinthenexttimestepwillbefullyobserved(Hauskrecht,2000). Asaresult,the
agentdoesnottakeinformationgatheringactions.
To address this shortcoming, there is a special set of heuristics dedicated to inducinginformationgatheringactions
(Royetal.,2005). Theseinformationgatheringheuristicstypicallyoperateina"dual-mode"fashionwhereexploita-
tionandexplorationarearbitratedbysomecriterion. Forexample,inCassandraetal.(1996),theexploitationmode
choosesactionsbasedontheunderlyingMDPwhereasthe explorationmodechoosesactionsto minimizebeliefen-
tropy in the next time step. These two modes are arbitrated by the entropy of the current belief. Complementary
to dual-mode execution, Flaspohleretal. (2020) propose to interleave open-loop with closed-loop belief dynamics
when the value of informationis low to speed up value function computation. In doingso, these methodsalleviate
theexpensivebeliefupdatingoperationduringplanning. WhileEFEresemblestheseheuristicsandthusamenableto
efficiencygain,itintroducesaninformationgainterminthereward,whichcouldbeanexpensiveoperationinitself
(Belghazietal.,2018).
9
ValueofInformationandRewardSpecificationinActiveInferenceandPOMDPs
Recently, there is a family of methods called information directed sampling (IDS) which also introduces informa-
tion objectives primarily to improve Thompson sampling-based algorithms in the context of multi-arm bandits and
BayesianRL(RussoandVanRoy,2018;Luetal.,2023;HaoandLattimore,2022;Chakrabortyetal.,2023). These
problemscanbeseenassubsetsofPOMDPswheretheonlyhiddenstateistheunknownenvironmentmodelparame-
ters(Doshi-VelezandKonidaris,2016). Similartoourwork,theiranalysesarealsobasedoncharacterizationofthe
relationshipbetweeninformationgainandregret,butinsteadviaa quantitycalled"informationratio". Furthermore,
weconsiderplanningwithopen-loopbeliefdynamicsratherthanThompsonsamplingandourfocusisonanalyzing
EFE.
Beyondinformationgatheringheuristics,thereisafamilyofPOMDPextensionscalledactivesensingorρ-POMDP
(Arayaetal., 2010), where the reward functionis directly defined on beliefs. These POMDPs are typically used to
modelsettings where the reductionin belief entropyis the primary goal, such as in the explorationof an area, and
a goal-relatedrewardcanbe optionallyadded. Withoutlossofgenerality,we candefinethisrewardasthe one-step
aheadbeliefentropy:
RAS(b,a)= E
P(o′|b,a)
[H[b′(s′ o′,b,a)]]
− |
1
E P(o′|b,a) [E b′(s′|o′,b,a) [logb′(s′ o′,b,a) log ]] (25)
∝ | −
|S|
=E
P(o′|b,a)
[KL[b′(s′ o′,b,a) ˜b(s′)]].
| ||
where˜b(s′)= 1 isauniformpriorbelief.Thisshowsthattheactivesensingobjectivecanbewrittenasaspecialtype
|S|
ofinformationgainthatisevaluatedagainstauniformpriorbelief,thusresemblingtheEFEobjective. Anattractive
property of this objective is that it is convex in the belief, and thus is the value function, which makes the agent
potentiallyless distractedbyinformationgainwhentask rewardsare introduced. A furtherdifferenceisthatituses
closed-loopbeliefdynamicswhichenablesbetteroptimizationoftheinformationobjective.
6.2 ObjectiveSpecificationinActiveInference
Theobjectivefunctionsin activeinferencehavebeensubjectto variousinterpretationssinceitsinceptioninthelate
2000’sand have only slowed down relatively recently (GottwaldandBraun, 2020). The EFE objective, which first
appearedintheliteratureasearlyas2015in(Fristonetal.,2015),wasinitiallymotivatedbyanintuitiveargumentthat
"freeenergyminimizingagentsshouldchooseactionstominimize(expected)freeenergy". However,farfrombeing
heuristic,theEFEobjectiveisrootedinthefreeenergyprinciplewhichadoptsaphysicsandinformationgeometric
perspective,ratherthanadecisiontheoreticperspective,onagentbehavior(Fristonetal.,2023b,a;Barpetal.,2022),
inwhichcaseopen-loopbeliefdynamicsisthenaturaloutcome.Itshouldbementioned,however,thattheinformation
geometricderivationof EFE relieson a "preciseagent" assumptiononthe environmentin whichfutureactionsand
observationsareassumedtohavematchingentropy(Barpetal.,2022;DaCostaetal.,2024). Itremainsopenwhether
thisassumptionissatisfiedinrealenvironments.
Recently,Fristonetal.(2021)introduceda"sophisticated"versionofEFEasanimprovedplanningobjectiveforactive
inferenceagents,whereinsteadofevaluatingEFEbasedonfuturestatemarginals,EFEisevaluatedbasedonfuture
posteriorbeliefsQ(s o ,a ). ThismeansthatthebeliefMDPunderlyingthesophisticatedEFEusestheclosed-
t t 0:t−1
|
loopbeliefdynamicsratherthantheopen-loopbeliefdynamicsinthevanillaEFE,however,theinformationgainterm
isstillusedintherewardfunction. ThismeansthatwecannolongerviewsophisticatedEFEasanapproximationto
theBayesoptimalpolicy. Rather,thecombinationofpragmaticvalueandclosed-loopbeliefdynamicsrendersparts
of sophisticatedEFE exactlyequalto the Bayesoptimalbelief MDP, untilthe equivalenceis "broken"again by the
additionalinformationgainterm. Doesthismeantheagentmaybemotivatedtoacquiretoomuchinformationwhile
compromisingtask performance? A simple manipulation shows that if we define the preference distribution as the
exponentiatedreward multipliedby a negativetemperatureparameterλ P˜(o) exp(λR˜(o)), then the EFE reward
∝
becomesproportionaltoaweightedcombinationofrewardandinformationgain:
R˜(s,a) P(s′ s,a) P(o′ s′)λR˜(o′)
∝ | |
s′ o′
X X
=λR˜(s,a), (26)
1
REFE(b,a) b(s)R˜(s,a)+ IG(b,a),
∝ λ
s
X
wherechoosingahighλ correspondstopurelyoptimizingreward. However,thisdoesmeanthatwhenλisnot
→∞
sufficientlyhigh,inwhichcasetheobjectivehighlyresemblesactivesensing,theagentmaybedistracted.Butwhether
10
ValueofInformationandRewardSpecificationinActiveInferenceandPOMDPs
this will be the case dependson the actual environment. Thus, similar to assumption5.3, achieving Bayes optimal
behaviorrequiressettingthepreferenceinsuchawaythatthecumulativerewardoutweighscumulativeinformation.
AnotherperspectiveontheEFEobjectiveisthattheagentperformsdistributionmatchingasopposedtorewardmax-
imization (DaCostaetal., 2023) where the agentadditionally seeks outdiverse states or observations. This can be
seenfromarearrangementofthepragmatic-epistemicdecompositionoftheEFEobjective:
REFE(b,a)=E P(o′|b,a) [logP˜(o′)]+E P(o′|b,a) [KL[b′(s′ o′,b,a) b′(s′ b,a)]]
| || |
= KL[P(o′ b,a) P˜(o′)] E P(s′|b,a) [H[P(o′ s′)]] . (27)
− | || − |
Risk Ambiguity
Thisisthewell-knownrisk-ambiguity|decompo{szitionofE}FE(|Sajidetal{.,z2021),wh}erethefirstterm"risk"measures
theKLdivergenceofthepredictedobservationdistributionfromthepreferredobservationdistributionandthesecond
term"ambiguity"measurestheentropyofobservationsexpectedunderpredictedfuturestates.
In the MDP setting, with closed-loop belief updating, the objective reduces to the following due to no ambiguity,
whichispreciselythewell-knowndistributionmatchingobjective(Hafneretal.,2020):
REFE(s,a)= KL[P(s′ s,a) P˜(s′)], (28)
− | ||
Thisobjectivehasbeenshowntoenhanceexplorationandtest-timeadaptationinanRLsetting(Leeetal.,2019).
Again,asshownin(DaCostaetal.,2023),distributionmatchingandrewardmaximizationcanbeinterpolatedusing
atemperatureparameteronthestatepreferenceP˜(s) exp(λR˜(s)):
∝
REFE(s,a)=E P(s′|s,a) [logP˜(s′)]+H[P(s′ s,a)]
|
1 (29)
∝
E P(s′|s,a) [R˜(s′)]+
λ
H[P(s′
|
s,a)].
Inthissetting,thetemperatureparameterλrepresentsthealloweddispersionaroundtheoptimalbehavior(orpathof
leastaction)specifiedbythefirstexpectedrewardtermin(29). Alternatively,itcanbeinterpretedasthetightnessof
the(soft)constrainttoabidebyoptimalbehavior,followingtheconstrainedmaximumentropyviewofthefreeenergy
principle(Fristonetal.,2023b).
Puttingtogethertheseperspectives, itappearsthatthe notionof"Bayes optimal"in the spiritofactive inference(in
closed-loop),aswellasextensionsofPOMDPs,maynotberestrictedtotheusualsenseofBayesiandecisiontheory
(i.e.,maximizingutility;Howard1966;RaiffaandSchlaifer2000;Berger2013);itmayalsoapplytothatofBayesian
optimaldesign(i.e., maximizinginformationgain; Lindley1956; MacKay1992)andprincipleof maximumcaliber
(i.e.,maximizingcoverage;Jaynes1980).
7 Conclusion
Inthispaper,westudythetheoreticalconnectionbetweenactiveinferenceandreinforcementlearningandshowthat
the epistemic value in the EFE objective of active inference can be seen as an approximationto the Bayes optimal
RL policy in POMDPs, achieving a linear improvement in regret compared to a naive policy which doesn’t take
intoaccountthevalueofinformation. Theresultsalso suggestthat, fromtheperspectiveofRL, thespecificationof
EFEneedstobalancerewardwithinformationgainintheenvironment,viaanappropriatetemperatureparameter(λ).
Conversely,fromtheperspectiveofactiveinference,anEFEminimizingagentwillpursueaBayesoptimalRLpolicy,
underasuitabletemperatureparameter. Thisconclusionmighthavebeenanticipatedbyonereadingofthecomplete
classtheorem(Wald,1947;Brown,1981);namely,foranypairofrewardfunctionandchoices,thereexistssomeprior
beliefsthatrenderthechoicesBayesoptimal,inadecisiontheoreticsense(Berger,2013).
Acknowledgement
TheauthorwouldliketothankAlexKiefer,AxelConstant,DavidHyland,KarlFriston,LanceDaCosta,PeterWaade,
RyanSingh,SanjeevNamjoshi,andShoheiWakayamaforhelpfulfeedback.
References
A. Agarwal, N. Jiang, S. M. Kakade, andW. Sun. Reinforcementlearning: Theoryand algorithms. CS Dept., UW
Seattle,Seattle,WA,USA,Tech.Rep,32:96,2019.
11
ValueofInformationandRewardSpecificationinActiveInferenceandPOMDPs
M.Araya,O.Buffet,V.Thomas,andF.Charpillet. Apomdpextensionwithbelief-dependentrewards. Advancesin
neuralinformationprocessingsystems,23,2010.
A. Barp, L. Da Costa, G. França, K. Friston, M. Girolami, M. I. Jordan, and G. A. Pavliotis. Geometric methods
for sampling, optimization, inference, and adaptive agents. In Handbook of Statistics, volume 46, pages 21–78.
Elsevier,2022.
M.I.Belghazi,A.Baratin,S.Rajeshwar,S.Ozair,Y.Bengio,A.Courville,andD.Hjelm. Mutualinformationneural
estimation. InInternationalconferenceonmachinelearning,pages531–540.PMLR,2018.
J.O.Berger. StatisticaldecisiontheoryandBayesiananalysis. SpringerScience&BusinessMedia,2013.
D. M. Blei, A. Kucukelbir, and J. D. McAuliffe. Variational inference: A review for statisticians. Journal of the
AmericanstatisticalAssociation,112(518):859–877,2017.
L. D.Brown. A completeclasstheoremforstatistical problemswith finite samplespaces. TheAnnalsofStatistics,
pages1289–1300,1981.
A.R.Cassandra, L.P.Kaelbling,andJ.A.Kurien. Actingunderuncertainty: Discretebayesianmodelsformobile-
robotnavigation.InProceedingsofIEEE/RSJInternationalConferenceonIntelligentRobotsandSystems.IROS’96,
volume2,pages963–972.IEEE,1996.
S. Chakraborty,A. S. Bedi, A. Koppel, M. Wang, F. Huang,andD. Manocha. Steering: Stein informationdirected
explorationformodel-basedreinforcementlearning. arXivpreprintarXiv:2301.12038,2023.
T.Champion,H.Bowman,D.Markovic´,andM.Grzes´. Reframingtheexpectedfreeenergy:Fourformulationsanda
unification. arXivpreprintarXiv:2402.14460,2024.
L. Da Costa, T. Parr, N. Sajid, S. Veselic, V. Neacsu, and K. Friston. Active inference on discrete state-spaces: A
synthesis. JournalofMathematicalPsychology,99:102447,2020.
L. Da Costa, N. Sajid, T. Parr, K. Friston, and R. Smith. Reward maximization through discrete active inference.
NeuralComputation,35(5):807–852,2023.
L.DaCosta,S.Tenka,D.Zhao,andN.Sajid.Activeinferenceasamodelofagency.arXivpreprintarXiv:2401.12917,
2024.
F.Doshi-VelezandG.Konidaris.Hiddenparametermarkovdecisionprocesses:Asemiparametricregressionapproach
fordiscoveringlatenttaskparametrizations.InIJCAI:proceedingsoftheconference,volume2016,page1432.NIH
PublicAccess,2016.
Y.Duan,J.Schulman,X.Chen,P.L.Bartlett,I.Sutskever,andP.Abbeel. rl2: Fastreinforcementlearningviaslow
reinforcementlearning. arXivpreprintarXiv:1611.02779,2016.
M.O.Duff. OptimalLearning:ComputationalproceduresforBayes-adaptiveMarkovdecisionprocesses. University
ofMassachusettsAmherst,2002.
J. Engström, R. Wei, A. D. McDonald, A. Garcia, M. O’Kelly, and L. Johnson. Resolving uncertainty on the fly:
modelingadaptivedrivingbehaviorasactiveinference. Frontiersinneurorobotics,18:1341750,2024.
G.Flaspohler,N.A.Roy,andJ.W.FisherIII. Belief-dependentmacro-actiondiscoveryinpomdpsusingthevalueof
information. AdvancesinNeuralInformationProcessingSystems,33:11108–11118,2020.
K.Friston. Thefree-energyprinciple:aunifiedbraintheory? Naturereviewsneuroscience,11(2):127–138,2010.
K. Friston, F. Rigoli, D.Ognibene,C. Mathys, T.Fitzgerald,andG. Pezzulo. Activeinferenceandepistemicvalue.
Cognitiveneuroscience,6(4):187–214,2015.
K. Friston, T. FitzGerald, F. Rigoli, P. Schwartenbeck,and G. Pezzulo. Active inference: a processtheory. Neural
computation,29(1):1–49,2017.
K.Friston,L.DaCosta,D.Hafner,C.Hesp,andT.Parr.Sophisticatedinference.NeuralComputation,33(3):713–763,
2021.
K. Friston, L. Da Costa, N. Sajid, C. Heins, K. Ueltzhöffer, G. A. Pavliotis, and T. Parr. The free energyprinciple
madesimplerbutnottoosimple. PhysicsReports,1024:1–29,2023a.
K. Friston, L. Da Costa, D. A. Sakthivadivel, C. Heins, G. A. Pavliotis, M. Ramstead, and T. Parr. Path integrals,
particularkinds,andstrangethings. PhysicsofLifeReviews,2023b.
S.GottwaldandD.A.Braun. Thetwokindsoffreeenergyandthebayesianrevolution. PLoScomputationalbiology,
16(12):e1008420,2020.
D.Hafner,P.A.Ortega,J.Ba, T.Parr,K.Friston,andN.Heess. Actionandperceptionasdivergenceminimization.
arXivpreprintarXiv:2009.01791,2020.
12
ValueofInformationandRewardSpecificationinActiveInferenceandPOMDPs
B. HaoandT.Lattimore. Regretboundsforinformation-directedreinforcementlearning. Advancesinneuralinfor-
mationprocessingsystems,35:28575–28587,2022.
M.Hauskrecht. Value-functionapproximationsforpartiallyobservablemarkovdecisionprocesses. Journalofartifi-
cialintelligenceresearch,13:33–94,2000.
R.A.Howard. Informationvaluetheory. IEEETransactionsonsystemsscienceandcybernetics,2(1):22–26,1966.
E. T. Jaynes. The minimum entropy production principle. Annual Review of Physical Chemistry, 31(1):579–601,
1980.
L.P.Kaelbling,M.L.Littman,andA.R.Cassandra. Planningandactinginpartiallyobservablestochasticdomains.
Artificialintelligence,101(1-2):99–134,1998.
S. Kakade and J. Langford. Approximately optimal approximate reinforcement learning. In Proceedings of the
NineteenthInternationalConferenceonMachineLearning,pages267–274,2002.
M. Kearnsand S. Singh. Near-optimalreinforcementlearning in polynomialtime. Machinelearning, 49:209–232,
2002.
M.T.Koudahl,W.M.Kouw,andB.deVries. Onepistemicsinexpectedfreeenergyforlineargaussianstatespace
models. Entropy,23(12):1565,2021.
P.Lanillos,C.Meo,C.Pezzato,A.A.Meera,M.Baioumy,W.Ohata,A.Tschantz,B.Millidge,M.Wisse,C.L.Buck-
ley,etal.Activeinferenceinroboticsandartificialagents:Surveyandchallenges.arXivpreprintarXiv:2112.01871,
2021.
L.Lee,B.Eysenbach,E.Parisotto,E.Xing,S.Levine,andR.Salakhutdinov. Efficientexplorationviastatemarginal
matching. arXivpreprintarXiv:1906.05274,2019.
D.V.Lindley. Onameasureoftheinformationprovidedbyanexperiment. TheAnnalsofMathematicalStatistics,27
(4):986–1005,1956.
M.L.Littman,A.R.Cassandra,andL.P.Kaelbling. Learningpoliciesforpartiallyobservableenvironments:Scaling
up. InMachineLearningProceedings1995,pages362–370.Elsevier,1995.
Q. Liu, A. Chung, C. Szepesvári, and C. Jin. When is partially observable reinforcementlearning not scary? In
ConferenceonLearningTheory,pages5175–5220.PMLR,2022.
X. Lu, B. Van Roy, V. Dwaracherla, M. Ibrahimi, I. Osband, Z. Wen, et al. Reinforcement learning, bit by bit.
FoundationsandTrends®inMachineLearning,16(6):733–865,2023.
D. J. MacKay. Information-basedobjective functionsfor active data selection. Neural computation, 4(4):590–604,
1992.
P.MalekzadehandK.N.Plataniotis. Activeinferenceandreinforcementlearning:Aunifiedinfer-enceoncontinuous
stateandactionspacesunderpartiallyobserv-ability. arXivpreprintarXiv:2212.07946,2022.
P. Mazzaglia, T. Verbelen, O. Catal, and B. Dhoedt. The free energy principle for perception and action: A deep
learningperspective. Entropy,24(2):301,2022.
B.Millidge. Deepactiveinferenceasvariationalpolicygradients. JournalofMathematicalPsychology,96:102348,
2020.
B. Millidge, A. Tschantz, A. K. Seth, and C. L. Buckley. On the relationship betweenactive inferenceand control
asinference. InActiveInference: First InternationalWorkshop, IWAI2020,Co-locatedwith ECML/PKDD 2020,
Ghent,Belgium,September14,2020,Proceedings1,pages3–11.Springer,2020.
T.Parr,G.Pezzulo,andK.J.Friston. Activeinference: thefreeenergyprincipleinmind,brain,andbehavior. MIT
Press,2022.
H.RaiffaandR.Schlaifer. Appliedstatisticaldecisiontheory,volume78. JohnWiley&Sons,2000.
S. Ross, G. Gordon,andD.Bagnell. Areductionofimitationlearningandstructuredpredictiontono-regretonline
learning. In Proceedingsof the fourteenth internationalconferenceon artificialintelligence andstatistics, pages
627–635.JMLRWorkshopandConferenceProceedings,2011.
N. Roy, G. Gordon, and S. Thrun. Finding approximate pomdp solutions through belief compression. Journal of
artificialintelligenceresearch,23:1–40,2005.
D. Russo and B. Van Roy. Learning to optimize via information-directed sampling. Operations Research, 66(1):
230–252,2018.
N.Sajid,P.J.Ball, T.Parr,andK.J.Friston. Activeinference: demystifiedandcompared. Neuralcomputation,33
(3):674–712,2021.
13
ValueofInformationandRewardSpecificationinActiveInferenceandPOMDPs
S.Schwöbel,S.Kiebel,andD.Markovic´. Activeinference,beliefpropagation,andthebetheapproximation. Neural
computation,30(9):2530–2567,2018.
R.Smith,P.Badcock,andK.J.Friston. Recentadvancesintheapplicationofpredictivecodingandactiveinference
modelswithinclinicalneuroscience. PsychiatryandClinicalNeurosciences,75(1):3–13,2021.
R.S.SuttonandA.G.Barto. Reinforcementlearning:Anintroduction. MITpress,2018.
J.TomczakandM.Welling. Vaewithavampprior. InInternationalconferenceonartificialintelligenceandstatistics,
pages1214–1223.PMLR,2018.
A.Tschantz,M.Baltieri,A.K.Seth,andC.L.Buckley. Scalingactiveinference. In2020internationaljointconfer-
enceonneuralnetworks(ijcnn),pages1–8.IEEE,2020.
A. Vemula, Y. Song, A. Singh, D.Bagnell, andS. Choudhury. The virtuesoflazinessin model-basedrl: A unified
objectiveandalgorithms. InInternationalConferenceonMachineLearning,pages34978–35005.PMLR,2023.
A.Wald.Anessentiallycompleteclassofadmissibledecisionfunctions.TheAnnalsofMathematicalStatistics,pages
549–555,1947.
J. Watson, A. Imohiosen, and J. Peters. Active inference or control as inference? a unifying view. arXiv preprint
arXiv:2010.00262,2020.
R.Wei,S.Zeng,C.Li,A.Garcia,A.D.McDonald,andM.Hong.Abayesianapproachtorobustinversereinforcement
learning. InConferenceonRobotLearning,pages2304–2322.PMLR,2023.
J.Winn,C.M.Bishop,andT.Jaakkola. Variationalmessagepassing. JournalofMachineLearningResearch,6(4),
2005.
L.Zintgraf,K.Shiarlis,M.Igl,S.Schulze,Y.Gal,K.Hofmann,andS.Whiteson. Varibad: Averygoodmethodfor
bayes-adaptivedeeprlviameta-learning. arXivpreprintarXiv:1910.08348,2019.
14
ValueofInformationandRewardSpecificationinActiveInferenceandPOMDPs
A Appendix
A.1 ProofsforSection2.3
DerivationofQ(s a ) in(10) fromvariationalinference We aim toobtaina predictivedistributionover
1:T 0:T−1
|
future states s given an action sequence a using variationalinference. Typically, active inference assumes
1:T 0:T−1
a mean-field factorization of the variational distribution Q(s a ) = T Q(s a ). Since there is no
1:T | 0:T−1 t=1 t | 0:T−1
observationandthusnolikelihoodterm,thevariationalfreeenergy canbewrittenas:
F Q
(Q)=E [logQ(s a ) logP(s a )]
F
Q(s1:T|a0:T−1) 1:T
|
0:T−1
−
1:T
|
0:T−1
T
=E (logQ(s a ) logP(s s ,a ))
Q(s1:T|a0:T−1)
"
t
|
0:T−1
−
t
|
t−1 t−1
# (30)
t=1
X
T
= E [logQ(s a ) logP(s s ,a )].
Q(st−1:t|a0:T−1) t
|
0:T−1
−
t
|
t−1 t−1
t=1
X
From(Winnetal.,2005),weknowtheoptimalvariationaldistributionhastheform:
Q(s a ) exp(E [logP(s s ,a )])
t
|
0:T−1
∝
Q(st−1|a0:T−1) t
|
t−1 t−1
exp(logE [P(s s ,a )])
≈
Q(st−1|a0:T−1) t
|
t−1 t−1
(31)
= P(s s ,a )Q(s a )
t t−1 t−1 t−1 0:T−1
| |
s Xt−1
:=Q(s Q ,a ).
t t−1 t−1
|
which recovers the definition in (10). The approximation in the second line is due to Jensen’s inequality and does
not significantly affect our results, because we know from the variational inference literature that the optimal vari-
ational distribution must be equal to that of exact inference, which is given by the last line. This also matches the
implementationinPymdp1,whichisoneofthemainsoftwarerepositoriesforactiveinference.
Active inference and QMDP It is crucial to have a precise definition of the distributions Q(s a ) and
0:T 0:T−1
|
Q(o ,s a ). Inthemaintext,wehavespecifiedtheseastheproductofmarginaldistributionsoverstatesand
0:T 0:T 0:T−1
|
observations.Here,webrieflystudytheconsequencesofdefiningtheseasthejointdistributions:
T
Q(s a )=b(s ) P(s s ,a ),
0:T 0:T−1 0 t t−1 t−1
| |
t Y =1 (32)
T
Q(o ,s a )=b(s )P(o s ) P(s s ,a )P(o s ).
0:T 0:T 0:T−1 0 0 0 t t−1 t−1 t t
| | | |
t=1
Y
WestartbyfactorizingthefullEFEobjectivein(9)as:
EFE(a )
0:T−1
=E [logQ(s a ) logP˜(o ,s )]
Q(o1:T,s1:T|a0:T−1) 1:T
|
0:T−1
−
1:T 1:T
T
=E logP(s s ,a ) logP˜(o ,s )
Q(o1:T,s1:T|a0:T−1)
"
t
|
t−1 t−1
−
t t
#
X t=1(cid:16) (cid:17)
=E logP(s s ,a ) logP˜(o ,s )
b(s0)P(s1|s0,a0)P(o1|s1) " 1 | 0 0 − 1 1 (33)
T
+E logP(s s ,a ) logP˜(o ,s )
Q(o2:T,s2:T|s0:1,a1:T−1)
"
t
|
t−1 t−1
−
t t
##
X t=2(cid:16) (cid:17)
=E logP(s s ,a ) logP˜(o ,s )+EFE(a )
b(s0)P(s1|s0,a0)P(o1|s1) 1
|
0 0
−
1 1 1:T−1
h i
=E E [logP(s s ,a ) logP˜(o ,s )]+E [EFE(a )] .
b(s0) P(s1|s0,a0)P(o1|s1) 1
|
0 0
−
1 1 P(s1|s0,a0) 1:T−1
h i
1https://github.com/infer-actively/pymdp
15
ValueofInformationandRewardSpecificationinActiveInferenceandPOMDPs
Thisallowsustowritedownarecursiveequation:
Q(s ,a )=E [logP(s s ,a ) logP˜(o ,s )]+E [V(s )],
t t P(st+1|st,at)P(ot+1|st+1) t+1
|
t t
−
t+1 t+1 P(st+1|st,at) t+1
R(st,at) (34)
V(s t )=m| axQ(s t ,a t ), {z }
a
and
EFE(a )=E [Q(s ,a )]. (35)
0:T−1 b(s0) 0 0
Thiscorrespondstowhat’sknownastheQMDPapproximationinthePOMDPliterature(Littmanetal.,1995),which
isknowntooverestimatethevalueofabeliefbyplanningundertheimplicitassumptionthatfuturestatesareobserved
(Hauskrecht,2000).
EFEboundandchoiceofpreference DespitebeingthemostpopularchoiceofEFE,thepragmatic-epistemicvalue
decomposition(12)isactuallyaboundonthefullEFEdefinedin(9). Toshowthis,let’sconsiderasingletimestep
since both formulations can be decomposed across time steps. Recall that the pragmatic-epistemic decomposition
assumesthefollowingfactorizationofP˜(o,s)=P˜(o)P˜(so). ThefullEFEcanbewrittenas:
|
EFE (a )=E [logQ(s a ) logP˜(o ,s )]
t 0:T−1 Q(ot,st|a0:T−1) t
|
0:T−1
−
t t
= E [logP˜(o )] E [logP˜(s o )]+E [Q(s a )]
−
Q(ot|a0:T−1) t
−
Q(ot,st|a0:T−1) t
|
t Q(st|a0:T−1) t
|
0:T−1
= E [logP˜(o )]+E [Q(s o ,a )] E [logP˜(s o )]
−
Q(ot|a0:T−1) t Q(ot,st|a0:T−1) t
|
t 0:T−1
−
Q(ot,st|a0:T−1) t
|
t
+E [Q(s a )] E [Q(s o ,a )] (36)
Q(st|a0:T−1) t
|
0:T−1
−
Q(ot,st|a0:T−1) t
|
t 0:T−1
= E [logP˜(o )]+E KL[Q(s o ,a ) P˜(s o )]
−
Q(ot|a0:T−1) t Q(ot|a0:T−1) t
|
t 0:T−1
||
t
|
t
E [KL[Q(s o ,a ) Q(s a )]]
−
Q(ot|a0:T−1) t
|
t 0:T−1
||
t
|
0:T−1
E [logP˜(o )] E [KL[Q(s o ,a ) Q(s a )]].
≥−
Q(ot|a0:T−1) t
−
Q(ot|a0:T−1) t
|
t 0:T−1
||
t
|
0:T−1
Thus,tokeeptheboundtight,wecouldsetP˜(so)as:
|
P˜∗(so)=arg min E KL[Q(s o ,a ) P˜(s o )]
| P˜(s|o)
Q(ot|a0:T−1) t
|
t 0:T−1
||
t
|
t
arg min E KL[P˜(s o ) Q(s o ,a )] (37)
≈ P˜(s|o)
Q(ot|a0:T−1) t
|
t
||
t
|
t 0:T−1
exp E [logQ(s o ,a )] ,
∝
Q(ot|a0:T−1) t
|
t 0:T−1
wherethe approximationin the secondlineassumesthe forwardandreverseKL divergenceshavesimilar solutions.
(cid:0) (cid:1)
Theresultonthelastlineissometimesreferredtoastheaggregateposterior(TomczakandWelling,2018). However,
sincetheaggregateposteriordependsontheactionsequenceevaluated,thetightestboundisachievedbyanaggregate
posteriorthatupdatesduringeachEFEoptimizationsteptoensurethatthefinalaggregateposteriorisevaluatedunder
theoptimalactionsequence.
Proposition A.1. (Active inference policy; restate of proposition 3.1) The EFE achieved by the optimal action se-
quencecanbeequivalentlyachievedbyatime-indexedbelief-actionpolicyπ(a Q ).
t t
|
Proof. While the proof in the main text is given by characterizing the EFE objective as a belief MDP, we give an
alternativeproofherebasedonBellmanoptimalityforthefullEFEobjectivein(9)startingwiththebasecase:
EFE(a ,Q )=E [logQ(s Q ,a ) logP˜(o ,s )]. (38)
T−1 T−1 Q(oT,sT|QT−1,aT−1) T
|
T−1 T−1
−
T T
Itiseasytoseethat
minEFE(a ,Q )= max π(a Q )EFE(a ,Q ), (39)
T−1 T−1 T−1 T−1 T−1 T−1
aT−1 πT−1 |
a XT−1
wheretheoptimalpolicyisπ∗ (a Q )=δ(a argmin EFE(a˜ ,Q )).
T−1 T−1 | T−1 T−1 − a˜T−1 T−1 T−1
Applyingtheidentityrecursively,wehave:
minE [EFE(a ,Q )]=minE
πt
π(at|Qt) t t
πt
π(at|Qt)
(cid:26) (40)
E Q(ot+1,st+1|Qt,at) [logQ(s t+1
|
Q t ,a t )
−
logP˜(o t+1 ,s t+1 )]+E π∗(at|Qt) [EFE(a t+1 ,Q t+1 )] .
(cid:27)
Theoptimalpolicyateachstepcanbeobtainedbyπ(a Q )=δ(a argmin EFE(a˜ ,Q )).
t
|
t t
−
a˜t t t
16
ValueofInformationandRewardSpecificationinActiveInferenceandPOMDPs
PropositionA.2. (Restateofproposition3.2)TheEFErewardfunctionasdefinedin(14b)isconcaveinthebelief.
Proof. RecalltheEFErewardisdefinedas:
R(b,a)=E P(o′|b,a) [logP˜(o′)]+E P(o′|b,a) [KL[b′(s′ o′,b,a) b′(s′ b,a)]]. (41)
| || |
From(15)weknowthefirsttermislinearinthebeliefb.
Thesecondtermcanbewrittenas:
E P(o′|b,a) [KL[b′(s′ o′,b,a) b′(s′ b,a)]]
| || |
=E P(o′,s′|b,a) [logb′(s′ o′,b,a) logb′(s′ b,a)]
| − |
=E P(o′,s′|b,a) [logb′(s′ b,a)+logP(o′ s′) logP(o′ b,a) logb′(s′ b,a)]
| | − | − |
=E P(o′,s′|b,a) [logP(o′ s′) logP(o′ b,a)] (42)
| − |
=H[P(o′ b,a)] E P(s′|b,a) [H[P(o′ s′)]]
| − |
= P(o′ b,a)logP(o′ b,a) b(s) P(s′ s,a)H[P(o′ s′)].
− | | − | |
o′ s s′
X X X
Thesecondtermaboveisalinearfunctionofthebelief.
Applyingthedefinitionofconvexitytothenegativeofthefirstterm:
P(o′ λb+(1 λ)b′,a)logP(o′ λb+(1 λ)b′,a)
| − | −
o′
X
= P(o′ s,a)[λb(s)+(1 λ)b′(s)]log (λb(s)P(o′ s,a)+(1 λ)b′(s)P(o′ s,a)))
X o′ X s | − " X s | − | # (43)
λP(o′ b,a)+(1 λ)P(o′ b′,a)
= [λP(o′ b,a)+(1 λ)P(o′ b,a)]log | − |
| − | λ+(1 λ)
o′ −
X
λP(o′ b,a)logP(o′ b,a)+ (1 λ)P(o′ b′,a)logP(o′ b′,a),
≤ | | − | |
o′ o′
X X
wherethelastlineusesthelogsuminequalityandshowstheequationisconvex. Thus,thefirsttermisconcaveand
theEFErewardisconcaveinthebelief.
A.2 ProofsforSection4
LemmaA.3. (PerformancedifferenceinmismatchedMDPs;restateoflemma4.1)Letπandπ′betwopolicieswhich
areoptimalw.r.t. twoMDPsM andM′. ThetwoMDPssharethesameinitialstatedistributionanddiscountfactor
but have different rewards R,R′ and dynamics P,P′. Denote ∆R(s,a) = R′(s,a) R(s,a). The performance
differencebetweenπandπ′ whenbothareevaluatedinM isgivenby: −
J (π) J (π′)
M M
−
= (1 1 γ) E (s,a)∼dπ P Aπ M ′ ′(s,a)
− h i
Advantageunderexpertdistribution
+| (1 1 γ) E (s,a { ) z ∼dπ P ′ ∆R(s,a})+γ E s′∼P′(·|s,a) [V M π′ ′(s′)] − E s′′∼P(·|s,a) [V M π′ ′(s′′)] (44)
− h (cid:16) (cid:17)i
Reward-modeladvantageunderowndistribution
+| (1 1 γ) E (s,a)∼dπ P − ∆R(s,a)+γ E s′′∼ { P z (·|s,a) [V M π′ ′(s′′)] − E s′∼P′(·|s,a) [V M π′ ′(s′)]} .
− h (cid:16) (cid:17)i
Reward-modeldisadvantageunderexpertdistribution
| {z }
Proof. Following(Vemulaetal.,2023),weexpandtheperformancedifferenceas:
J (π) J (π′)=E [Vπ(s ) Vπ′ (s )]
M − M µ(s0) M 0 − M 0 (45)
=E
µ(s0)
[V
M
π(s
0
)
−
V
M
π′ ′(s
0
)]+E
µ(s0)
[V
M
π′ ′(s
0
)
−
V
M
π′ (s
0
)].
17
ValueofInformationandRewardSpecificationinActiveInferenceandPOMDPs
Thesecondtermcanbeexpandedas:
E
µ(s0)
[V
M
π′ ′(s
0
)
−
V
M
π′ (s
0
)]
=E s0∼µ(·),a0∼π′(·|s0) [R′(s 0 ,a 0 )+γE s1∼P′(·|s0,a0) [V M π′ ′(s 1 )] − R(s 0 ,a 0 ) − γE s1∼P(·|s0,a0) [V M π′ (s 1 )]]
=E s0∼µ(·),a0∼π′(·|s0) [∆R(s 0 ,a 0 )+γE s1∼P′(·|s0,a0) [V M π′ ′(s 1 )] − γE s1∼P(·|s0,a0) [V M π′ ′(s 1 )]
+γE
s1∼P(·|s0,a0)
[V
M
π′ ′(s
1
)]
−
γE
s1∼P(·|s0,a0)
[V
M
π′ (s
1
)]] (46)
=E s0∼µ(·),a0∼π′(·|s0) [∆R(s 0 ,a 0 )+γE s1∼P′(·|s0,a0) [V M π′ ′(s 1 )] − γE s1∼P(·|s0,a0) [V M π′ ′(s 1 )]]
+γE s0∼µ(·),a0∼π′(·|s0),s1∼P(·|s0,a0) [V M π′ ′(s 1 ) − V M π′ (s 1 )],
terma
where∆R(s,a)=R′(s,a) R(s,a). | {z }
−
Expandingterma,wearriveatasimilarstructuretotheabove:
terma=E a1∼π′(·|s1) [R′(s 1 ,a 1 )+γE s2∼P′(·|s1,a1) [V M π′ ′(s 2 )] − R(s 1 ,a 1 ) − γE s2∼P(·|s1,a1) [V M π′ (s 2 )]]
=E a1∼π′(·|s1) [∆R(s 1 ,a 1 )+γE s2∼P′(·|s1,a1) [V M π′ ′(s 2 )] − γE s2∼P(·|s1,a1) [V M π′ ′(s 2 )]] (47)
+γE a1∼π′(·|s1),s2∼P(·|s1,a1) [V M π′ ′(s 2 ) − V M π′ (s 2 )] .
terma’
| {z }
Wecanthusunrollthelasttermiterativelyandobtain:
E
µ(s0)
[V
M
π′ ′(s
0
)
−
V
M
π′ (s
0
)]
∞
=E " γt ∆R(s t ,a t )+γE s′∼P′(·|st,at) [V M π′ ′(s′)] − γE s′′∼P(·|st,at) [V M π′ ′(s′′)] # (48)
X t=0 (cid:16) (cid:17)
= (1 1 γ) E (s,a)∼dπ P ′ ∆R(s,a)+γ E s′∼P′(·|s,a) [V M π′ ′(s′)] − E s′′∼P(·|s,a) [V M π′ ′(s′′)] ,
− h (cid:16) (cid:17)i
wheretheexpectationinthesecondlineistakenw.r.t. thestochasticprocessinducedbyπ′,P.
Wenowexpandthefirsttermintheperformancedifference:
E
s0∼µ(·)
[V
M
π(s
0
)
−
V
M
π′ ′(s
0
)]
= E
s0∼µ(·)
[V
M
π(s
0
)
−
E
a0∼π(·|s0)
[Qπ
M
′ ′(s
0
,a
0
)]] + E
s0∼µ(·)
[E
a0∼π(·|s0)
[Qπ
M
′ ′(s
0
,a
0
)]
−
V
M
π′ ′(s
0
)]
(cid:18) (cid:19) (cid:18) (cid:19)
= E
s0∼µ(·),a0∼π(·|s0)
[Qπ
M
′ ′(s
0
,a
0
)]
−
V
M
π′ ′(s
0
)]
(cid:18) (cid:19)
+E [R(s ,a )+γE [Vπ(s )]]
s0∼µ(·),a0∼π(·|s0) 0 0 s1∼P(·|s0,a0) M 1
− E s0∼µ(·),a0∼π(·|s0) [R′(s 0 ,a 0 )+γE s1∼P′(·|s0,a0) [V M π′ ′(s 1 )]]
=E
s0∼µ(·),a0∼π(·|s0)
[Aπ
M
′ ′(s
0
,a
0
)]
(49)
+E
s0∼µ(·),a0∼π(·|s0) −
∆R(s
0
,a
0
)+γE
s1∼P(·|s0,a0)
[V
M
π(s
1
)]
−
γE
s1∼P(·|s0,a0)
[V
M
π′ ′(s
1
)]
(cid:20)
+γE s1∼P(·|s0,a0) [V M π′ ′(s 1 )] − γE s1∼P′(·|s0,a0) [V M π′ ′(s 1 )]
(cid:21)
=E
s0∼µ(·),a0∼π(·|s0)
[Aπ
M
′ ′(s
0
,a
0
)]
+E s0∼µ(·),a0∼π(·|s0) [ − ∆R(s 0 ,a 0 )+γE s1∼P(·|s0,a0) [V M π′ ′(s 1 )] − γE s1∼P′(·|s0,a0) [V M π′ ′(s 1 )]]
+γE
s0∼µ(·),a0∼π(·|s0),s1∼P(·|s0,a0)
[V
M
π(s
1
)
−
V
M
π′ ′(s
1
)] .
termb
| {z }
18
ValueofInformationandRewardSpecificationinActiveInferenceandPOMDPs
Applythesameunrollingmethodtotermb,wehave:
E
s0∼µ(·)
[V
M
π(s
0
)
−
V
M
π′ ′(s
0
)]
∞
=E γtAπ
M
′ ′(s
t
,a
t
)
" #
t=0
X
∞
+E " γt − ∆R(s t ,a t )+γE s′′∼P(·|st,at) [V M π′ ′(s′′)] − γE s′∼P′(·|st,at) [V M π′ ′(s′)] # (50)
X t=0 (cid:16) (cid:17)
= (1 1 γ) E (s,a)∼dπ P Aπ M ′ ′(s,a)
− h i
+ (1 1 γ) E (s,a)∼dπ P − ∆R(s,a)+γ E s′′∼P(·|s,a) [V M π′ ′(s′′)] − E s′∼P′(·|s,a) [V M π′ ′(s′)] ,
− h (cid:16) (cid:17)i
wheretheexpectationsinthefirstequalityisagaintakenw.r.t. thestochasticprocessinducedbyπ,P.
Puttingtogether,wehave:
J (π) J (π′)
M M
−
= (1 1 γ) E (s,a)∼dπ P Aπ M ′ ′(s,a)
− h i
+ (1 1 γ) E (s,a)∼dπ P ′ ∆R(s,a)+γ E s′∼P′(·|s,a) [V M π′ ′(s′)] − E s′′∼P(·|s,a) [V M π′ ′(s′′)] (51)
− h (cid:16) (cid:17)i
+ (1 1 γ) E (s,a)∼dπ P − ∆R(s,a)+γ E s′′∼P(·|s,a) [V M π′ ′(s′′)] − E s′∼P′(·|s,a) [V M π′ ′(s′)] .
− h (cid:16) (cid:17)i
PropositionA.4. (Modeladvantagebound)LetVπ(s)bethevaluefunctionofpolicyπ indynamicsP withreward
P
R(s,a)andletR =max R(s,a). TheabsolutevalueoftheadvantageofdynamicsP overP′isboundedby:
max s,a
| |
R
E P(s′|s,a) [V P π(s′)] − E P′(s′′|s,a) [V P π(s′′)] ≤ 1 ma γ x 2KL[P( ·| s,a) || P′( ·| s,a)]. (52)
−
(cid:12) (cid:12) p
(cid:12) (cid:12)
Proof. TheproofisthesameaslemmaB.2. in(Weietal.,2023).
E P(s′|s,a) [V P π(s′)] − E P′(s′′|s,a) [V P π(s′′)]
(cid:12) (cid:12)
(cid:12)= Vπ(s′)(P(s′ s,a) P′(s′ s,a)) (cid:12)
(cid:12)
(cid:12)X
s′ P | − | (cid:12)
(cid:12)
(cid:12) (cid:12)
(1)(cid:12) (cid:12) Vπ(s′) P(s′ s,a) P′(s′ s,a) (cid:12) (cid:12) (53)
≤ | P || | − | |
s′
X
(2)
Vπ() P( s,a) P′( s,a)
≤ k P · k ∞ k ·| − ·| k 1
(3)
Vπ() 2KL[P( s,a) P′( s,a)],
≤ k P · k ∞ ·| || ·|
where(1)usesJensen’sinequalitysincetheinnerspumisaconvexcombination,(2)usesHolder’sinequalityand(3)
usesPinsker’sinequality.Thecoefficient Vπ() E [ ∞ γtmax R(s,a)]= Rmax.
k P · k ∞ ≤ π,P t=0 s,a | | (1−γ)
Puttingtogether,wehave: P
R
E P(s′|s,a) [V P π(s′)] − E P′(s′′|s,a) [V P π(s′′)] ≤ 1 ma γ x 2KL[P( ·| s,a) || P′( ·| s,a)]. (54)
−
(cid:12) (cid:12) p
(cid:12) (cid:12)
L ǫ R e ′ m = ma E ( A s, . a 5 ) . ∼d (R π P e [ | s ∆ ta R te (s o , f a l ) e | m ], m ǫ P a ′ 4 = .2) E F ( o s, r a) t ∼ he dπ P s [ e K tt L in [ g P( c · o | s n , s a id ) e || r P ed ′( · i | n s, l a em )] m ], a an 4 d .1 R , m ′ le a t x ǫ π = ′ = ma E x s (s ,a ,a | ) R ∼ ′ d ( π P s [ , | a A ) π M | . ′ ′ ( L s e , t a t ) h | e ],
19
ValueofInformationandRewardSpecificationinActiveInferenceandPOMDPs
′
twopolicieshaveboundedstate-actionmarginaldensityratio dπ P (s,a) C. Theperformancegapisboundedas:
dπ P (s,a) ≤
1 C+1 (C+1)γR′
J M (π) − J M (π′) ≤ 1 γ ǫ π′ + 1 γ ǫ R′ + (1 γ)2 max√2ǫ P′. (55)
− − −
Proof. Theabsolutevalueoftheperformancegapcanbewrittenas:
J (π) J (π′)
M M
| − |
≤ (1
1
γ)
E
(s,a)∼dπ P |
Aπ
M
′ ′(s,a)
|
− h i
1 1
+ (1 γ) E (s,a)∼dπ P [ − ∆R(s,a)] + (1 γ) E (s,a)∼dπ P ′ [∆R(s,a)] (56)
+ (1 − γ γ) (cid:12) (cid:12) (cid:12) E (s,a)∼dπ P E s′′∼P(·|s,a) (cid:12) (cid:12) (cid:12)[V M π′ ′(s − ′′)] − (cid:12) (cid:12) (cid:12) E s′∼P′(·|s,a) [V M π′ ′(s′) (cid:12) (cid:12) (cid:12)]
+ (1 − γ γ) (cid:12) (cid:12) (cid:12) E (s,a)∼dπ P ′ h E s′∼P′(·|s,a) [V M π′ ′(s′)] − E s′′∼P(·|s,a) [V M π′ ′(s′′)] i(cid:12) (cid:12) (cid:12)
− (cid:12) h i(cid:12)
duetoJensen’sinequality. (cid:12) (cid:12)
(cid:12) (cid:12)
Expandingthethirdtermrewardadvantageontherighthandside:
dπ′
(s,a)
E (s,a)∼dπ P ′ [∆R(s,a)] = (cid:12) E (s,a)∼dπ P " d P π P (s,a) ∆R(s,a) #(cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
dπ′
(s,a)
(cid:12)
(cid:12)
≤
(cid:12)E
(s,a)∼dπ P "(cid:12)d
P
π P (s,a)
∆R(s,a)
(cid:12)#
(cid:12)
(57)
(cid:12) (cid:12)
dπ′
(s,a)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
≤(cid:13) d P π
P
(s,a)(cid:13) (cid:12) E (s,a)∼dπ P [ | ∆R(s(cid:12),a) | ]
(cid:13) (cid:13)∞
=C(cid:13)
(cid:13)
ǫ R′. (cid:13)
(cid:13)
(cid:13) (cid:13)
ForthesecondtermwedropC fromtheaboveduetonodistributionmismatch.
ApplyingpropositionA.4tothelastterm:
E (s,a)∼dπ P ′ E s′∼P′(·|s,a) [V M π′ ′(s′)] − E s′′∼P(·|s,a) [V M π′ ′(s′′)]
(cid:12) (cid:12) (cid:12)= (cid:12) E (s,a)∼d h π P " d d π P π P ′ ( ( s s , , a a ) ) E s′∼P′(·|s,a) [V M π′ ′(s′)] − E s′′∼P(·| i s (cid:12) (cid:12) (cid:12) ,a) [V M π′ ′(s′′)] #(cid:12)
(cid:12) (cid:16) (cid:17) (cid:12)
≤ (cid:12) (cid:12) (cid:12)E (s,a)∼dπ P "(cid:12) d d π P π P ′ ( ( s s , , a a ) ) E s′∼P′(·|s,a) [V M π′ ′(s′)] − E s′′∼P(·|s,a) [V M π′ ′(s′′)] (cid:12)# (cid:12) (cid:12) (cid:12)
(cid:12) (cid:16) (cid:17)(cid:12)
≤(cid:13) d d π P π P ′ ( ( s s , , a a ) ) (cid:13) (cid:12) (cid:12) (cid:12) k V M π′ ′( · ) k ∞ E (s,a)∼dπ P [ k P( ·| s,a) − P′( ·| s,a) k 1 ] (cid:12) (cid:12) (cid:12) (58)
(cid:13) (cid:13)∞
≤(cid:13) (cid:13) (cid:13) (cid:13) d d π P π P ′ ′ ( ( s s , , a a ) ) (cid:13) (cid:13) (cid:13) (cid:13) k V M π′ ′( · ) k ∞ 2E (s,a)∼dπ P [KL[P( ·| s,a) || P′( ·| s,a)]]
(cid:13) (cid:13)∞ q
(cid:13)CR′ (cid:13)
= (cid:13) (cid:13) max√2 (cid:13) (cid:13)ǫ P′.
1 γ
−
AgainforthefourthtermwedropC fromtheaboveduetonodistributionmismatch.
PuttingtogetherandapplythefactthatJ (π) J (π′),wehave:
M M
≥
1 C+1 (C+1)γR′
J M (π) − J M (π′) ≤ 1 γ ǫ π′ + 1 γ ǫ R′ + (1 γ)2 max√2ǫ P′. (59)
− − −
20
ValueofInformationandRewardSpecificationinActiveInferenceandPOMDPs
A.3 ProofsforSection5.1
A.3.1 HelpfulIdentities
PropositionA.6. (Open-loopvaluefunctionconvexity)Theopen-loopvaluefunctionasdefinedin(20a)ispiece-wise
linearandconvexinthebeliefs.
Proof. Recallthedefinitionoftheopen-loopvaluefunctionis:
Qopen(b,a)= b(s)R(s,a)+γVopen(b′(a,b)). (60)
s
X
Furthermore,itisavalidbeliefMDPgiventhedeterministictransitionofthebeliefstatedefinedin(14c).
Although this is an infinite horizon value function, due to the contraction mapping property of Bellman equation
(Agarwaletal.,2019),itcanbeapproximatedarbitrarilycloseusingafinitenumberofK iterationsstartingfromthe
base case Qopen(b,a) = b(s)R(s,a). It is clear the base case value functionVopen(b) = max Qopen(b,a˜) is
k=0 s k=0 a˜ k=0
piecewiselinearandconvexinb.
P
Foriterationk 1,..., ,wehave:
∈{ ∞}
Qopen(b,a)= b(s)R(s,a)+γmaxQopen(b′(a,b),a′). (61)
k+1 a′ k
s
X
Thebeliefupdateb′(a,b)= P(s′ s,a)b(s)islinearandconvexinb,makingthesecondtermpiecewiselinearand
s |
convex.Thefirsttermisalsolinearandconvex.Thecombinationisthuspiecewiselinearandconvex.
P
Proposition A.7. (EVPO non-negativity) Let the expected value of perfect observation for a single stage decision
makingproblemwithrewardR(s,a),priorbeliefb(s)andmarginalobservationdistributionP(o)= P(os)b(s)
s |
bedefinedas:
P
EVPO =EV PO EV ,
| −
EV =max b(s)R(s,a),
a (62)
s
X
EV PO = P(o)max b(so)R(s,a).
| a |
p s
X X
ItholdsthatEVPO 0.
≥
Proof. Wewishtoshow:
P(o)max b(so)R(s,a) max b(s)R(s,a′). (63)
a | ≥ a′
o s s
X X X
Letusedefinea∗(o)=argmax b(so)R(s,a),anda∗ =argmax b(s)R(s,a)sothatwecanwritetheLHS
as P(o) b(so)R(s,a∗(o a ))an s dth | eRHSas b(s)R(s,a∗). a s
o s | P s P
Bydefinition,wehave:
P P P
b(so)R(s,a∗(o)) b(so)R(s,a∗), (64)
| ≥ |
s s
X X
sincea∗(o)istheoptimalactiontakingintoconsiderationofo.
ApplyingexpectationoverP(o)totheaboveinequality,wehave:
P(o) b(so)R(s,a∗(o)) P(o) b(so)R(s,a∗)
| ≥ |
o s o s
X X X X (65)
= b(s)R(s,a∗),
s
X
whichcompletestheproof.
21
ValueofInformationandRewardSpecificationinActiveInferenceandPOMDPs
PropositionA.8. (EVPOupperbound)LetR = max R(s,a). Theexpectedvalueofperfectobservationas
max s,a
| |
definedin(62)isupperboundedasfollows:
EVPO R 2E [KL[b(so) b(s)]]. (66)
max P(o)
≤ | ||
q
Proof. RecallthedefinitionofEVPOis:
EVPO=E [V(b(so))] V(b(s))
P(o)
| −
=E max b(so)R(s,a(o)) max b(s)R(s,a)
P(o)
"a(o) | #− a
s s
X X
(67)
E b(so)R(s,a∗(o)) b(s)R(s,a∗(o))
P(o)
≤ " | #−
s s
X X
=E R(s,a∗(o))(b(so) b(s)) ,
P(o)
" | − #
s
X
wherewe haveused a∗(o) = argmax b(so)R(s,a(o)) andthe inequalityis dueto a∗(o) beingsuboptimal
a(o) s |
forthesecondterm.
P
TakingtheabsolutevalueoftheaboveEVPObound,wehave:
EVPO = E R(s,a∗(o))(b(so) b(s))
P(o)
| | (cid:12) " | − #(cid:12)
(cid:12) X s (cid:12)
(cid:12) (cid:12)
(1)(cid:12) (cid:12)
(cid:12)E R(s,a∗(o))(b(so) b(s)) (cid:12)
P(o)
≤ "(cid:12) | − (cid:12)#
(cid:12)X s (cid:12)
(cid:12) (cid:12) (68)
(2) (cid:12) (cid:12)
E (cid:12) R(s,a∗(o)) b(so) b(s)(cid:12)
P(o)
≤ " | || | − |#
s
X
(3)
R(, ) E [ b(so) b(s) ]
∞ P(o) 1
≤ k · · k k | − k
(4)
R 2E [KL[b(so) b(s)]]
max P(o)
≤ | ||
where(1)and(2)areduetoJensen’sinequality,(3q)isduetoHolder’sinequality,and(4)isduetoPinsker’sinequality.
A.3.2 MainResultsofSection5.1
Proposition A.9. (EVPO-POMDP non-negativity; restate of proposition 5.1) Let Qopen(b,a),Vopen(b) and
Q(b,a),V(b)denotetheopenandclosed-loopvaluefunctionsasdefinedin(20),itholdsthat:
Q(b,a) Qopen(b,a)andV(b) Vopen(b)forallb ∆( )anda . (69)
≥ ≥ ∈ S ∈A
Proof. Recalltheopenandclosed-loopvaluefunctionsaredefinedas:
Qopen(b,a)= b(s)R(s,a)+γVopen(b′(a,b)), Vopen(b)=maxQopen(b,a),
a
s
X (70)
Q(b,a)= b(s)R(s,a)+γ P(o′ b,a)V(b′(o′,a,b)), V(b)=maxQ(b,a).
| a
s o′
X X
Althoughthese areinfinite horizonvaluefunctions,againdueto theircontractionmappingproperty(Agarwaletal.,
2019), they can be approximatedarbitrarily close using a finite number of K iterations starting from the base case
Q (b,a)= b(s)R(s,a).
k=0 s
Startingwithk =1,wehave:
P
Qopen(b,a)= b(s)R(s,a)+γVopen(b′(a,b)), Vopen(b)=max b(s)R(s,a),
1 0 0 a
s s
X X (71)
Q (b,a)= b(s)R(s,a)+γ P(o′ b,a)V (b′(o′,a,b)), V (b)=max b(s)R(s,a).
1 0 0
| a
s o′ s
X X X
22
ValueofInformationandRewardSpecificationinActiveInferenceandPOMDPs
Takingthedifferencebetweenthetwovaluefunctionsandmultiplyby 1,wehave:
γ
1
[Q (b,a) Qopen(b,a)]
γ 1 − 1
= P(o′ b,a)V (b′(o′,a,b)) Vopen(b′(a,b))
| 0 − 0
o′ (72)
X
= P(o′ b,a)max b′(s′ o′,a,b)R(s′,aclose) max b′(s′ a,b)R(s′,aopen)
| aclose | −aopen |
o′ s s
X X X
=EVPO 0,
≥
wherethe secondto last lineequalsEVPO inpropositionA.7underpriorbeliefb′(s′ b,a)forall b ∆(s),a .
| ∈ ∈ A
Thusitmustbenon-negative.
Applyingtheabovetothevaluefunctionsatk =1,wehave:
V (b) Vopen(b)= maxQ (b,aclose) maxQopen(b,aopen)
1 − 1 aclose 1 −aopen 1
Q (b,aopen∗) Qopen(b,aopen∗) (73)
≥ 1 − 1
0,
≥
wherewehavedefinedaopen∗ =argmax aopen Qo 1 pen(b,aopen).
Nowconsiderk =2,where
Qopen(b,a)= b(s)R(s,a)+γVopen(b′(a,b)), Vopen(b)=maxQopen(s,a),
2 1 1 1
a
s
X (74)
Q (b,a)= b(s)R(s,a)+γ P(o′ b,a)V (b′(o′,a,b)), V (b)=maxQ (s,a).
1 1 1 1
| a
s o′
X X
Takingthedifferencebetweenthetwovaluefunctionsagain,wehave:
1
[Q (b,a) Qopen(b,a)]
γ 2 − 2
= P(o′ b,a)V (b′(o,a,b)) Vopen(b′(a,b))
| 1 − 1
o′
X
(75)
= P(o′ b,a) max b′(so′,a,b)R(s,a ′close)+ P(o′′ b′,a ′close)V (b′′(o′′,a ′close,b′))
0
o′ | a′close( s | o′′ | )
X X X
max b′(sa,b)R(s,a ′open)+Vopen(b′′(a ′open,b′)) .
−a′open( | 0 )
s
X
23
ValueofInformationandRewardSpecificationinActiveInferenceandPOMDPs
Let a ′open∗ = argmax a′open
s
b′(s
|
a,b)R(s,a ′open)+V
0
open(b′′(a ′open,b′)) and a ′close∗ =
argmax a′close s b′(s | o′,a,b)R(s,a ′nclPose)+ o′′ P(o′′ | b′,a ′close)V 0 (b′′(o′′,a ′close,b′)) o,wehave:
n o
P P
P(o′ b,a) max b′(so′,a,b)R(s,a ′close)+ P(o′′ b′,a ′close)V (b′′(o′′,a ′close,b′))
0
o′ | a′close( s | o′′ | )
X X X
max b′(sa,b)R(s,a ′open)+Vopen(b′′(a ′open,b′))
−a′open( | 0 )
s
X
P(o′ b,a) max b′(so′,a,b)R(s,a ′close)+ P(o′′ b′,a ′open∗)V (b′′(o′′,a ′open∗,b′))
0
≥ o′ | a′close( s | o′′ | )
X X X
b′(sa,b)R(s,a ′open∗)+Vopen(b′′(a ′open∗,b′))
−( | 0 ) (76)
s
X
= P(o′ b,a) max b′(so′,a,b)R(s,a ′close) b′(sa,b)R(s,a ′open∗)
o′ | (a′close s | − s | )
X X X
EVPO≥0
+ | P(o′ b,a) P(o′′ b′,a ′open∗)V {z (b′′(o′′,a ′open∗,b′)) Vopen(b′′(a ′open } ∗,b′))
o′ | ( o′′ | 0 − 0 )
X X
≥0dueto(72)
0.
| {z }
≥
Applyingtheabovetok 1,..., recursively,wehave:
∈{ ∞}
Q(b,a) Qopen(b,a)andV(b) Vopen(b). (77)
≥ ≥
Proposition A.10. (Closed-loop model advantage upper bound) Let R = max R(s,a). The closed-loop
max s,a
| |
modeladvantageisupperboundedasfollows:
R
E P(b′|b,a) [Vopen(b′)]
−
E Popen(b′′|b,a) [Vopen(b′′)]
≤ 1
ma
γ
x 2IG(b,a). (78)
−
p
Proof. Recalltheclosed-loopmodeladvantageisdefinedas:
E P(b′|b,a) [V(b′)] E Popen(b′′|b,a) [V(b′′)]=E P(o′|b,a) [V(b′(s′ o′,b,a))] V(b′(s′)) (79)
− | −
Tosimplifynotation,wewilldroptheconditioningonb,aintheexpectation. Thisalsoenablesustoremovethe"′"
notation.
Wewilluseasimilarmethodasbeforewhereweleveragethecontractionmappingpropertyofthevaluefunctionand
startfromthebasecase. Itisclearforthebasecasek =0whereV(b)=max b(s)R(s,a),themodeladvantage
a s
isEVPOandthustheupperboundfrompropositionA.8applies. Tosimplifynotation,let’sdenotetheupperbound
asC(b)sinceb(so)canbecalculatedfromb(s) P
|
24
ValueofInformationandRewardSpecificationinActiveInferenceandPOMDPs
Wenowconsiderk =1:
E [V (b(so))] V (b(s))
P(o) 1 1
| −
=E max b(so)R(s,aclose)+γV (b′(aclose,b(so)))
P(o) 0
"aclose | | #
s
X
max b(s)R(s,aopen)+γV (b′(aopen,b(s)))
0
−"aopen #
s
X
E b(so)R(s,aclose∗)+γV (b′(aclose∗,b(so)))
P(o) 0
≤ " | | #
X s (80)
b(s)R(s,aclose∗)+γV (b′(aclose∗,b(s)))
0
−" #
s
X
=E b(so)R(s,aclose∗) b(s)R(s,aclose∗)
P(o)
" | − #
s s
X X
terma
+γE V (b′(aclose∗,b(so))) V (b′(aclose∗,b(s))) .
| P(o) 0 {z 0 }
| −
(cid:2) termb (cid:3)
| {z }
TermaisthesameastheoneinEVPO,thustheupperboundC(b)appliesagain.Intermb,recalltheopen-loopbelief
updatesaredefinedas:
b′(a,b(so))= P(s′ s,a)b(so):=b′(s′ o),
| | | |
s
X (81)
b′(a,b(s))= P(s′ s,a)b(s):=b′(s′).
|
s
X
Duetotheconvexityofthevaluefunctions,wehavetermb 0. Furthermore,termbcorrespondstoEVPOforstage
0withmodifiedbeliefupdatesasdefinedabove.ThusC(b′) ≥ appliesagain.
Combiningboth,wehave:
E [V (b(so))] V (b(s))
P(o) 1 1
| −
R max 2E P(o) [KL[b(so) b(s)]]+γR max 2E P(o) [KL[b′(s′ o,aclose∗) b′(s′)]] (82)
≤ | || | ||
q q
R 2E [KL[b(so) b(s)]]+γR 2E [KL[b(so) b(s)]],
max P(o) max P(o)
≤ | || | ||
q q
wherethesecondinequalityisduetodataprocessinginequality.
Applyingtheabovetok 2,..., recursively,wehave:
∈{ ∞}
∞
E P(o′|b,a) [V(b′(s′ o′))] V(b′(s′)) R max γt 2E P(o′|b,a) [KL[b′(s′ o′) b′(s′)]]
| − ≤ | ||
X t=0 q (83)
R
=
1
ma
γ
x 2E P(o′|b,a) [KL[b′(s′
|
o′)
||
b′(s′)]].
− q
A.4 ProofsforSection5.2
PropositionA.11. (EFEEVPOupperbound)LetR = max R(s,a). Theexpectedvalueofperfectobserva-
max s,a
| |
tionasdefinedin(62)isupperboundedasfollows:
EVPOEFE R˜ 2E [KL[b(so) b(s)]]. (84)
max P(o)
≤ | ||
q
25
ValueofInformationandRewardSpecificationinActiveInferenceandPOMDPs
Proof. Recalltheone-stepEFEbeliefrewardis:
R(b,a)= b(s)R(s,a)+IG(b,a), (85)
s
X
wheretherewardisdefinedasR(s,a):=R˜(s,a)in(14b)andIG(b,a)istheinformationgain.
WecanthuswriteEVPOas:
EVPO
=E max b(so)R(s,a(o))+IG(b(so),a(o)) max b(s)R(s,a)+IG(b(s),a)
P(o)
"a(o) | | #− a " #
s s
X X
E b(so)R(s,a∗(o))+IG(b(so),a∗(o)) b(s)R(s,a∗(o))+IG(b(s),a∗(o))
P(o)
≤ " s | | #−" s # (86)
X X
=E R(s,a∗(o))(b(so) b(s)) +E [IG(b(so),a∗(o)) IG(b(s),a∗(o))]
P(o) P(o)
" | − # | −
s
X ≤0
| {z }
E R(s,a∗(o))(b(so) b(s)) ,
P(o)
≤ " | − #
s
X
wherewehaveuseda∗(o) = argmax b(so)R(s,a(o))andthelastinequalityisduetoIGbeingaconcave
a(o) s |
function of beliefs. The remaining term is the same as the one in proposition A.8. Thus, applying the result from
propositionA.8wecompletetheproof. P
PropositionA.12. (EFEclosed-loopmodeladvantageupperbound)LetR =max R(s,a). Theclosed-loop
max s,a
| |
modeladvantageundertheEFEvaluefunctionisupperboundedasfollows:
R
E P(b′|b,a) [VEFE(b′)]
−
E Popen(b′′|b,a) [VEFE(b′′)]
≤ 1
ma
γ
x 2IG(b,a) (87)
−
p
Proof. SimilartotheprooftopropositionA.10,westartwiththebasecasewhichiscoveredbypropositionA.11. To
simplifynotation,wedroptheEFEsuperscriptwiththeunderstandingthatVEFE isthevaluefunctionundertheEFE
beliefMDP.
26
ValueofInformationandRewardSpecificationinActiveInferenceandPOMDPs
Startingwithk =1,wehave:
E [V (b(so))] V (b(s))
P(o) 1 1
| −
=E max b(so)R(s,aclose)+IG(b(so),aclose)+γV (b′(aclose,b(so)))
P(o) 0
"aclose | | | #
s
X
max b(s)R(s,aopen)+IG(b(s),aopen)+γV (b′(aopen,b(s)))
0
−"aopen #
s
X
E b(so)R(s,aclose∗)+IG(b(so),aclose∗)+γV (b′(aclose∗,b(so)))
P(o) 0
≤ " | | | #
s
X
b(s)R(s,aclose∗)+IG(b(s),aclose∗)+γV (b′(aclose∗,b(s)))
0
−" #
s
X
(88)
=E b(so)R(s,aclose∗) b(s)R(s,aclose∗)
P(o)
" | − #
s s
X X
+E [IG(b(so),aclose∗) IG(b(s),aclose∗)]
P(o)
| −
≤0
+γ| E P(o) V 0 (b′(aclose∗,{bz(so))) V 0 (b′(aclose } ∗,b(s)))
| −
(cid:2) (cid:3)
E b(so)R(s,aclose∗) b(s)R(s,aclose∗)
P(o)
≤ " | − #
s s
X X
terma
+γE V (b′(aclose∗,b(so))) V (b′(aclose∗,b(s)))
| P(o) 0 {z 0 }
| −
(cid:2) termb (cid:3)
Wearriveatthesamef|ormaspropositionA.10.{Wzhilewecannotguarantee}termb>0,thesameupperboundholds.
The next remark ensures the expected closed-loop model advantage under the EFE reward is non-negative, which
providesthemotivationforassumption5.3.
Finally,applyingtheaboverecursivelytok 2,..., ,wecompletetheproof.
∈{ ∞}
RemarkA.13. (Motivationforassumption5.3)ToensuretheEFEmodeladvantageexpectedundertheBayesoptimal
policyπisnon-negative,weneedtosettherewardsuchthat:
E
(b,a)∼dπ
(b(so) b(s))R(s,a) E (b,a)∼dπ[IG(b(s),a) IG(b(so),a)], (89)
P " | − #≥ P − |
s
X
wheredπ isthemarginaldistributioninducedbytheBayesoptimalpolicyintheclosed-loopbeliefdynamics.
P
TheoremA.14. (Open-loopandEFEpolicyperformancegaps; restateoftheorem5.5)Letallpoliciesbedeployed
in POMDP M and all are allowed to update their beliefs according to b′(o′,a,b). Let ǫ
IG
= E (b,a)∼dπ[IG(b,a)]
P
denotestheexpectedinformationgainundertheBayesoptimalpolicy’sbelief-actionmarginaldistributionandletthe
belief-actionmarginalinducedbybothopen-loopandEFEpolicieshaveboundeddensityratiowiththeBayesoptimal
policy
dπ
P
˜(b,a)
C. Under assumptions5.3 and 5.4, the performance gap of the open-loopand EFE policies
dπ P (b,a) ∞ ≤
fromth(cid:13)eoptima(cid:13)lpolicyareboundedas:
(cid:13) (cid:13)
(cid:13) (cid:13)
1 (C+1)γR
J (π) J (πopen) ǫ + maxǫ ,
M − M ≤ 1 γ π˜ (1 γ)2 IG
− − (90)
1 (C+1)γR C+1
J (π) J (πEFE) ǫ + maxǫ ǫ .
M − M ≤ 1 γ π˜ (1 γ)2 IG − 1 γ IG
− − −
27
ValueofInformationandRewardSpecificationinActiveInferenceandPOMDPs
Proof. LetusstartbyboundingtheabsolutevalueoftheEFEpolicy’sperformancegap:
J (π) J (πEFE)
M M
| − |
≤ 1
1
γ
E (b,a)∼dπ
P
[Aπ M EF E E FE(b,a)]
(cid:12) − (cid:12)
(cid:12) 1 (cid:12) (91)
+ (cid:12) (cid:12) 1 γ E (b,a)∼dπ P − IG(b,a)+ (cid:12) (cid:12) γ E b′′∼P(·|b,a) [V M πE E F F E E (b′′)] − E b′∼Popen(·|b,a) [V M πE E F F E E (b′)]
+ (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 − − 1 γ E (b,a)∼dπ P E h FE h IG(b,a)+ (cid:16) γ (cid:16) E b′′∼Popen(·|b,a) [V M πE E F F E E (b′′)] − E b′∼P(·|b,a) [V M πE E F F E E (b′ (cid:17) )] i (cid:17) (cid:12) (cid:12) (cid:12) (cid:12) i (cid:12) (cid:12) .
Examinin(cid:12)gthesecondterm,wehave: (cid:12)
(cid:12) (cid:12)
1 1 γ E (b,a)∼dπ P − IG(b,a)+γ E b′′∼P(·|b,a) [V M πE E F F E E (b′′)] − E b′∼Popen(·|b,a) [V M πE E F F E E (b′)]
(cid:12) (cid:12) − 1 h (cid:16) γR (cid:17)i (cid:12) (cid:12)
(cid:12) (cid:12)
≤ 1 γ
E (b,a)∼dπ
P −
IG(b,a)+
1
ma
γ
x 2IG(b,a) (cid:12) (cid:12)
(cid:12) − (cid:20) − (cid:21)(cid:12)
(cid:12) 1 γR p (cid:12)
≤
(cid:12) (cid:12)
1 γ
E (b,a)∼dπ
P −
IG(b,a)+
1
ma
γ
xIG(b,a) (cid:12) (cid:12) (92)
(cid:12) − (cid:20) − (cid:21)(cid:12)
(cid:12)γR +γ 1 (cid:12)
=(cid:12) (cid:12) m (1 ax γ)2 − E (b,a)∼dπ P [IG(b,a)] (cid:12) (cid:12)
− (cid:12) (cid:12)
=
γR m
(1
ax +
γ)
γ
2
−
1 E(cid:12)
(cid:12) (b,a)∼dπ
P
[IG(b,a)].
(cid:12)
(cid:12)
−
Pluggingintotheperformancegap,wehave:
J (π) J (πEFE)
M M
| − |
≤ 1
1
γ
E (b,a)∼dπ
P
[Aπ M EF E E FE(b,a)]
(cid:12) − (cid:12)
(cid:12) (cid:12) γR +γ 1 (cid:12) (cid:12) γR +γ 1 dπEFE (b,a)
+(cid:12) m (1 ax − γ)2 − E (b,a)∼dπ P [IG((cid:12)b,a)]+ m (1 ax − γ)2 − E (b,a)∼dπ P "(cid:12) (cid:12) P dπ P (b,a) IG(b,a) (cid:12) (cid:12) #
≤ 1 1 γ E (b,a)∼dπ P [Aπ M EF E E FE(b,a)] (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (93)
(cid:12) − (cid:12)
(cid:12) (cid:12) γR +γ 1 (cid:12) (cid:12) γR +γ 1 dπEFE (b,a)
+(cid:12) m (1 ax − γ)2 − E (b,a)∼dπ P [IG((cid:12)b,a)]+ m (1 ax − γ)2 − (cid:13) (cid:13) P dπ P (b,a) (cid:13) (cid:13)∞ E (b,a)∼dπ P [ | IG(b,a) | ]
1 (C+1)(γR +γ 1) (cid:13) (cid:13)
max (cid:13) (cid:13)
= 1 γ ǫ πEFE + (1 γ)2 − ǫ IG (cid:13) (cid:13)
− −
1 (C+1)γR C+1
≤ 1 γ ǫ πopen + (1 γ)2 maxǫ IG − 1 γ ǫ IG .
− − −
Fortheopen-looppolicywhichdoesnothavetheIGterminthereward,itiseasytoseethattheperformancegapis:
1 (C+1)γR
| J M (π) − J M (πopen) |≤ 1 γ ǫ πopen + (1 γ)2 maxǫ IG . (94)
− −
A.5 ProofsforSection6.2
Equivalence between state marginal matching (Leeetal., 2019) and closed-loop EFE in MDP The marginal
statedistributionattimesteptfollowingpolicyπisdefinedas:
dπ(s )=b (s b ,π)
t t t t | t−1
= P(s s ,a )π(a s )b (s ), (95)
t t−1 t−1 t−1 t−1 t−1 t−1
| |
s Xt−1a Xt−1
28
ValueofInformationandRewardSpecificationinActiveInferenceandPOMDPs
wherethestatemarginalatonetimestepdependsonthestatemarginalattheprevioustimestep.
Wecandefinethetime-averagedstatemarginalmatchingproblemasstatemarginalmatchingatalltimesteps:
T
1
argminKL[dπ(s) P˜(s)]=argmin KL[dπ(s ) P˜(s )]
π || π T t t || t
X
t=0
(96)
T
=argmin E [logb (s b ,π) logP˜(s )].
π
bt(st|bt−1,π) t t
|
t−1
−
t
t=0
X
Sinceweconsiderclosed-looppolicieswheretheagentcanobservetheenvironmentstateratherthancomputingthe
nextstatemarginalfromanimprecisecurrentstatemarginal,itfollowsthat:
b (s b ,π)=P(s s ,a ). (97)
t t t−1 t t−1 t−1
| |
Thustheobjectiveisequivalentto:
T
minE logP(s s ,a ) logP˜(s ) . (98)
π
P(s0:T,a0:T−1)
"
t
|
t−1 t−1
−
t
#
X t=0(cid:16) (cid:17)
Thisisthesameobjectivein(Leeetal.,2019)andalso(DaCostaetal.,2023).
29

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Value of Information and Reward Specification in Active Inference and POMDPs"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
