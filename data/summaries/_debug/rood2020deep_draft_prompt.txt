=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: A deep active inference model of the rubber-hand illusion
Citation Key: rood2020deep
Authors: Thomas Rood, Marcel van Gerven, Pablo Lanillos

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2020

Key Terms: rubber, environment, body, deep, illusion, inference, active, brain, model, hand

=== FULL PAPER TEXT ===

A deep active inference model of the
rubber-hand illusion
Thomas Rood, Marcel van Gerven, and Pablo Lanillos
Department of Artificial Intelligence
Donders Insitute for Brain, Cognition and Behaviour
Montessorilaan 3, 6525 HR Nijmegen, the Netherlands
p.lanillos@donders.ru.nl
Abstract. Understanding how perception and action deal with senso-
rimotor conflicts, such as the rubber-hand illusion (RHI), is essential
to understand how the body adapts to uncertain situations. Recent re-
sultsinhumanshaveshownthattheRHInotonlyproducesachangein
the perceived arm location, but also causes involuntary forces. Here, we
describe a deep active inference agent in a virtual environment, which
we subjected to the RHI, that is able to account for these results. We
show that our model, which deals with visual high-dimensional inputs,
producessimilarperceptualandforcepatternstothosefoundinhumans.
Keywords: Active inference · Rubber-hand illusion · Free-energy opti-
mization · Deep learning.
1 Introduction
The complex mechanisms underlying perception and action that allow seamless
interaction with the environment are largely occluded from our consciousness.
To interact with the environment in a meaningful way, the brain must integrate
noisysensoryinformationfrommultiplemodalitiesintoacoherentworldmodel,
from which to generate and continuously update an appropriate action [13]. Es-
pecially, how the brain-body deals with sensorimotor conflicts [8,16], e.g., con-
flicting information from different senses, is an essential question for both cog-
nitive science and artificial intelligence. Adaptation to unobserved events and
changes in the body and the environment during interaction is a key character-
istic of body intelligence that machines still fail at.
The rubber-hand illusion (RHI) [2] is a well-known experimental paradigm
from cognitive science that allows the investigation of body perception under
conflicting information in a controlled setup. During the experiment, human
participants cannot see their own hand but rather perceive an artificial hand
placedinadifferentlocation(e.g.15cmfromtheircurrenthand).Afteraminute
of visuo-tactile stimulation [10], the perceived location of the real hand drifts
towards the location of the artificial arm and suddenly the new hand becomes
part of their own.
0202
ceD
22
]IA.sc[
2v80470.8002:viXra
2 T. Rood et al.
We can find some RHI modelling attempts in the literature; see [12] for an
overview until 2015. In [18], a Bayesian causal inference model was proposed to
estimatetheperceivedhandpositionafterstimulation.In[8]amodelinspiredby
thefree-energyprinciple[5]wasusedtosyntheticallytesttheRHIinarobot.The
perceptual drift (mislocalization of the hand) was compared to that of humans
observations.
Recent experiments have shown that humans also generate meaningful force
patterns towards the artificial hand during the RHI [1,16], adding the action
dimension to this paradigm. We hypothesise that the strong interdependence
between perception and action can be accounted for by mechanisms underlying
active inference [7].
In this work, we propose a deep active inference model of the RHI, based
on [14,17,19], where an artificial agent directly operates in a 3D virtual reality
(VR) environment1. Our model 1) is able to produce similar perceptual and
activepatternstohumanobservationsduringtheRHIand2)providesascalable
approach for further research on body perception and active inference, as it
dealswithhigh-dimensionalinputssuchasvisualimagesoriginatedfromthe3D
environment.
2 Deep active inference model
Weformalisebodyperceptionandactionasaninferenceproblem[11,3,7,17].The
unobserved body state is inferred from the senses (observations) while taking
into account its state prior information. To this end, the agent makes use of two
sensory modalities. The visual input s is described by a pixel matrix (image)
v
and the proprioceptive information s represents the angle of every joint of the
p
arm – See Fig. 1a.
Computationofthebodystateisperformedbyoptimizingthethevariational
free-energybound[7,17].Underthemean-fieldandLaplaceapproximationsand
definingµasthebrainvariablesthatencodethevariationaldensitythatapprox-
imates the body state distribution and defining a as the action exerted by the
agent, perception and action are driven by the following system of differential
equations (see [6,4,19] for a derivation):
µ˙ =−∂ F =−∂ eTΣ−1e −∂ eTΣ−1e −∂ eTΣ−1e (1)
µ µ p p p µ v v v µ f µ f
a˙ =−∂ F =−∂ eTΣ−1e (2)
a a p p p
e =s −g (µ) (3)
p p p
e =s −g (µ) (4)
v v v
e =−f(µ) (5)
f
Note that this model is a specific instance of the full active inference model [5]
tailored to the RHI experiment. We wrote the variational free-energy bound in
1 Code will be publicly available at https://github.com/thomasroodnl/
active-inference-rhi
A deep active inference model of the rubber-hand illusion 3
µ 2 FC 10 1 24 FC 81 2 92 1 R 288 C 1 T 28 1 161 C 2 1 816 C 6 T 4 2 32 C 64 2 32 C 4 T 8 3 64 C 48 3 64 C 3 T 21 4 28 C 32 4 128 CT 125 5 6
(b) Convolutional decoder
2
FCµ
1 I 256 C 1 V 62M1 56P1 C 3 V 2 2 12M8P2 C 6 V 4 3 64MP3 C 1 V 28 4 32MP4 C 1 V 28 5 16MP5R 8192 FC 40 1 96 FC 10 2 24 FC 51 3 2 FCl 2 og(σ) z 2
(c) VAE (encoder only)
(a) Deep Active inference
Fig.1: Deep active inference model for the virtual rubber-hand illusion. (a) The
brain variables µ that represent the body state are inferred through propriocep-
tive e and visual e prediction errors and their own dynamics f(µ). During the
p v
VRimmersion,theagentonlyseestheVRarm.Theensuingactionisdrivenby
proprioceptive prediction errors. The generative visual process is approximated
bymeansofadeepneuralnetworkthatencodesthesensoryinputintothebody
state through a bottleneck. (b,c) Visual generative architectures tested.
terms of the prediction error e and for clarity, we split it into three terms that
correspond to the visual, proprioceptive and dynamical component of the body
state. The variances Σ ,Σ ,Σ encode the reliability of the visual, propriocep-
v p µ
tiveanddynamicsinformation,respectively,thatisusedtoinferthebodystate.
The dynamics of the prediction errors are governed by different generative pro-
cesses. Here, g (µ) is the generative process of the visual information (i.e. the
v
predictorofthevisualinputgiventhebrainstatevariables),g (µ)isthepropri-
p
oceptive generative process and f(µ) denotes internal state dynamics (i.e. how
the brain variables evolve in time)2.
Due to the static characteristics of the passive RHI experiment we can sim-
plify the model. First, the generative dynamics model does not affect body up-
datebecausetheexperimentalsetupdoesnotallowforbodymovement.Second,
we fully describe the body state by the joint angles. This means that the s and
p
the body state match. Thus, g(µ) = µ plus noise and the inverse mapping
∂ g (µ) becomes an all-ones vector. Relaxing these two assumptions is out of
µ p
the scope of this paper. We can finally write the differential equations with the
2 NotethatinEquation(5),thepredictionerrorwithrespecttotheinternaldynamics
e f =µ(cid:48)−f(µ) was simplified to e f =−f(µ) under the assumption that µ(cid:48) =0. In
other words, we assume no dynamics on the internal variables.
4 T. Rood et al.
generative models as follows:
µ˙ =Σ−1(s −g (µ))+∂ g (µ)TγΣ−1(s −g (µ)) (6)
p p p µ v v v v
a˙ =−∆ Σ−1(s −g (µ)) (7)
t p p p
where γ has been included in the visual term to modulate the level of causality
regarding whether the visual information has been produced by our body in the
RHI – see Sec. 2.2. Equation 7 is only valid if the action is the velocity of the
joint. Thus, the sensor change given the action corresponds to the time interval
between each iteration ∂ s=∆ .
a t
We scale up the model to high-dimensional inputs such as images by ap-
proximating the visual generative model g (µ) and the partial derivative of the
v
errorwithrespecttothebrainvariables∂ e bymeansofdeepneuralnetworks,
µ v
inspired by [19].
2.1 Generative model learning
We learn the forward and inverse generative process of the sensory input by
exploiting the representational capacity of deep neural networks. Although in
this work we only address the visual input, this method can be extended to any
other modality. To learn the the visual forward model g (µ) we compare two
v
different deep learning architectures, that is, a convolutional decoder (Fig. 1b)
and a variational autencoder (VAE, Fig. 1c).
Theconvolutionaldecoderwasdesignedinsimilarfashiontothearchitecture
used in [19]. After training the relation between the visual input and the body
state, the visual prediction can be computed through the forward pass of the
network and its inverse ∂g(µ)/∂µ by means of the backward pass. The VAE
was designed using the same decoding structure as the convolutional decoder
to allow a fair performance comparison. This means that these models mainly
differed in the way they were trained. In the VAE approach we train using the
full architecture and we just use the decoder to compute the predictions in the
model.
2.2 Modelling visuo-tactile stimulation synchrony
To synthetically replicate the RHI we need to model both synchronous and
asynchronous visuo-tactile stimulation conditions. We define the timepoints at
which a visual stimulation event and the corresponding tactile stimulation take
place,denotedt andt respectively.InspiredbytheBayesiancausalmodel[18],
v t
we distinguish between two causal explanations of the observed data. That is,
C =c signifies that the observed (virtual) hand produced both the visual and
1
the tactile events whereas C =c signifies that the observed hand produced the
2
visualeventandourrealhandproducedthetactileevent(visualandtactileinput
come from two different sources). The causal impact of the visual information
on the body state is represented by
γ =p(c 1 |t v ,t t)= p(t v ,t t |c 1 p ) ( p t v (c , 1 t t ) | + c 1 p ) ( p t v (c , 1 t t ) |c 2)p(c 2) (8)
A deep active inference model of the rubber-hand illusion 5
where p(t ,t | c ) is defined as a zero-mean Gaussian distribution over the
v t 1
difference between the timepoints (p(t −t |c )) and p(t ,t |c ) is defined as
v t 1 v t 2
a uniform distribution since under c , no relation between t and t is assumed.
2 v t
This yields the update rule
(cid:40) p(tv,tt|c1)γt if visuo-tactile event
γ t+1 = γ p(t · v, e t x t| p γt ( ) − ·γt (t + − p m (t a v x ,t ( t t | v c2 ,t ) t ( ) 1 ) − 2 γ · t r ) ), otherwise (9)
t ∆−1 decay
t
Note that γ is updated only in case of visuo-tactile events. Otherwise, an expo-
nential decay is applied.
Visuo-tactile Camera
stimulation
Virtual arm:
Left
Real arm:
Center
Fig.2:VirtualenvironmentandexperimentalsetupmodelledintheUnityengine.
3 Experimental setup
We modelled the RHI in a virtual environment created in Unity, as depicted
in Fig. 2. This environment was build to closely match the experimental setup
used in the human study described in [16]. This experiment exposed human
participants to a virtual arm located to the left and right of their real arm, and
applied visuo-tactile stimulation by showing a virtual ball touching the hand
and applying a corresponding vibration to the hand. Here, the agent’s control
consisted of two degrees of freedom: shoulder adduction/abduction and elbow
flexion/extension. The environment provided proprioceptive information on the
shoulder and elbow joint angles to the agent. Visual sensory input to the model
originated from a camera located between the left and the right eye position,
producing 256×256 pixel grayscale images. Finally, the ML-Agents toolkit was
used to interface between the Unity environment and the agent in Python [9].
The agent arm was placed in a forward resting position such that the hand was
located 30 cm to the left of the body midline (center position). Three virtual
arm location conditions were evaluated: Left, Center and Right. The Center
condition matched the information given by proprioceptive input. Visuo-tactile
6 T. Rood et al.
stimulation was applied by generating a visual event at a regular interval of two
seconds, followed by a tactile event after a random delay sampled in the range
[0, 0.1) for synchronous stimulation and in the range [0, 1) for asynchronous
stimulation. The initial γ value was set to 0.01 and we ran N =5 trials each for
30 s (1500 iterations).
1.0
0.5
0.0
0.5
1.0
L C R
)mc(
tfird
rotceffe
dne
lautpecreP
1.0 1.0
VAE ConvDecoder
0.5 0.5
0.0 0.0
0.5 0.5
1.0 1.0
L C R L C R
(a) Perception all (b) Percept. Sync. (c) Percept. Async.
0.4
0.2
0.0
0.2
0.4
L C R
)2sm(
noitarelecca
detpmettA
0.4 0.4
VAE ConvDecoder
0.2 0.2
0.0 0.0
0.2 0.2
0.4 0.4
L C R L C R
(d) Action all (e) Action Sync. (f) Action Async.
(g) Human recorded forces
 9 $ (  6 K R X O G H U  & R Q Y ' H F R G H U  6 K R X O G H U  9 $ (  ( O E R Z  & R Q Y ' H F R G H U  ( O E R Z
(h) Jacobian ∂ µ g(µ) learnt for both visual models.
Fig.3:Modelresults.(a,b,c)Meanperceptualend-effectordrift(incm).(d,e,f)
Mean horizontal end-effector acceleration. (g) Mean forces exerted by human
participants in a virtual rubber-hand experiment (from [16]). (h) Visual repre-
sentation of the Jacobian learnt for the visual models.
A deep active inference model of the rubber-hand illusion 7
4 Results
We observed similar patterns in the drift of the perceived end-effector location
(Fig. 3a) and the end-effector action (Fig. 3). These agree with the behavioural
data obtained in human experiments (Fig. 3g). For the left and right condition,
weobservedforcesinthedirectionofthevirtualhandduringsynchronousstim-
ulation (Fig. 3e). However, non-meaningful forces were produced using the con-
volutionaldecoderfortherightcondition.Forthecentercondition,bothmodels
produced near-zero average forces. Lastly, asynchronous stimulation produced,
with both models, attenuated forces (Fig. 3f). The learnt visual representation
differed between the VAE and the Convolutional decoder approaches (Fig. 3h).
The VAE obtained smoother and more bounded visual Jacobian values, likely
due to its probabilistic latent space.
5 Conclusion
In this work, we described a deep active inference model to study body per-
ception and action during sensorimotor conflicts, such as the RHI. The model,
operatingasanartificialagentinavirtualenvironment,wasabletoproducesim-
ilar perceptual and active patterns to those found in humans. Further research
will address how this model can be employed to investigate the construction of
the sensorimotor self [15].
References
1. Asai, T.: Illusory body-ownership entails automatic compensative movement: for
the unified representation between body and action. Experimental brain research
233(3), 777–785 (2015)
2. Botvinick, M., Cohen, J.: Rubber hands ‘feel’ touch that eyes see. Nature
391(6669), 756–756 (Feb 1998). https://doi.org/10.1038/35784, https://doi.
org/10.1038/35784
3. Botvinick, M., Toussaint, M.: Planning as inference. Trends in cognitive sciences
16(10), 485–488 (2012)
4. Buckley, C.L., Kim, C.S., McGregor, S., Seth, A.K.: The free energy principle for
action and perception: A mathematical review. Journal of Mathematical Psychol-
ogy 81, 55–79 (2017)
5. Friston, K.: The free-energy principle: a unified brain theory? Nature Reviews
Neuroscience11(2),127–138(Feb2010).https://doi.org/10.1038/nrn2787,https:
//doi.org/10.1038/nrn2787
6. Friston,K.,Mattout,J.,Trujillo-Barreto,N.,Ashburner,J.,Penny,W.:Variational
free energy and the laplace approximation. Neuroimage 34(1), 220–234 (2007)
7. Friston, K.J., Daunizeau, J., Kilner, J., Kiebel, S.J.: Action and behav-
ior: a free-energy formulation. Biological Cybernetics 102(3), 227–260 (Mar
2010). https://doi.org/10.1007/s00422-010-0364-z, https://doi.org/10.1007/
s00422-010-0364-z
8 T. Rood et al.
8. Hinz, N.A., Lanillos, P., Mueller, H., Cheng, G.: Drifting perceptual patterns
suggest prediction errors fusion rather than hypothesis selection: replicating the
rubber-hand illusion on a robot. arXiv preprint arXiv:1806.06809 (2018)
9. Juliani, A., Berges, V.P., Vckay, E., Gao, Y., Henry, H., Mattar, M., Lange, D.:
Unity: A general platform for intelligent agents (2018)
10. Kalckert, A., Ehrsson, H.H.: The onset time of the ownership sensation
in the moving rubber hand illusion. Frontiers in Psychology 8, 344
(2017). https://doi.org/10.3389/fpsyg.2017.00344, https://www.frontiersin.
org/article/10.3389/fpsyg.2017.00344
11. Kappen,H.J.,G´omez,V.,Opper,M.:Optimalcontrolasagraphicalmodelinfer-
ence problem. Machine learning 87(2), 159–182 (2012)
12. Kilteni,K.,Maselli,A.,Kording,K.P.,Slater,M.:Overmyfakebody:bodyowner-
shipillusionsforstudyingthemultisensorybasisofown-bodyperception.Frontiers
in human neuroscience 9, 141 (2015)
13. K¨ording, K.P., Wolpert, D.M.: Bayesian integration in sensorimotor learning. Na-
ture427(6971),244–247(Jan2004).https://doi.org/10.1038/nature02169,https:
//doi.org/10.1038/nature02169
14. Lanillos,P.,Cheng,G.:Adaptiverobotbodylearningandestimationthroughpre-
dictivecoding.In:2018IEEE/RSJInternationalConferenceonIntelligentRobots
and Systems (IROS). pp. 4083–4090. IEEE (2018)
15. Lanillos, P., Dean-Leon, E., Cheng, G.: Enactive self: a study of engineering per-
spectives to obtain the sensorimotor self through enaction. In: Developmental
Learning and Epigenetic Robotics, Joint IEEE Int. Conf. on (2017)
16. Lanillos, P., Franklin, S., Franklin, D.W.: The predictive brain in ac-
tion: Involuntary actions reduce body prediction errors. bioRxiv (2020).
https://doi.org/10.1101/2020.07.08.191304, https://www.biorxiv.org/content/
early/2020/07/08/2020.07.08.191304
17. Oliver, G., Lanillos, P., Cheng, G.: Active inference body perception and action
for humanoid robots. arXiv preprint arXiv:1906.03022 (2019)
18. Samad, M., Chung, A.J., Shams, L.: Perception of body ownership is
driven by bayesian sensory inference. PloS one 10(2), e0117178–e0117178 (Feb
2015).https://doi.org/10.1371/journal.pone.0117178,https://pubmed.ncbi.nlm.
nih.gov/25658822
19. Sancaktar, C., van Gerven, M., Lanillos, P.: End-to-end pixel-based deep active
inference for body perception and action. arXiv preprint arXiv:2001.05847 (2020)

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "A deep active inference model of the rubber-hand illusion"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
