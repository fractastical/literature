=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Reactive Environments for Active Inference Agents with RxEnvironments.jl
Citation Key: nuijten2024reactive
Authors: Wouter W. L. Nuijten, Bert de Vries

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2024

Key Terms: systems, bert, agents, environments, reactive, complex, inference, active, agent, eindhoven

=== FULL PAPER TEXT ===

Reactive Environments for Active Inference
Agents with RxEnvironments.jl
Wouter W.L. Nuijten1[0009−0007−0689−9300] and Bert de
Vries1,2[0000−0003−0839−174X] {w.w.l.nuijten,
bert.de.vries}@tue.nl
1 Eindhoven University of Technology, 5612 AP Eindhoven, the Netherlands
2 GN Hearing, 5612 AB Eindhoven, The Netherlands
Abstract. Active Inference is a framework that emphasizes the inter-
action between agents and their environment. While the framework has
seen significant advancements in the development of agents, the envi-
ronmentalmodelsareoftenborrowedfromreinforcementlearningprob-
lems, which may not fully capture the complexity of multi-agent inter-
actions or allow complex, conditional communication. This paper intro-
ducesReactiveEnvironments,acomprehensiveparadigmthatfacilitates
complexmulti-agentcommunication.Inthisparadigm,bothagentsand
environmentsaredefinedasentitiesencapsulatedbyboundarieswithin-
terfaces.Thissetupfacilitatesarobustframeworkforcommunicationin
nonequilibrium-Steady-State systems, allowing for complex interactions
andinformationexchange.WepresentaJuliapackageRxEnvironments.
jl, which is a specific implementation of Reactive Environments, where
we utilize a Reactive Programming style for efficient implementation.
The flexibility of this paradigm is demonstrated through its application
to several complex, multi-agent environments. These case studies high-
light the potential of Reactive Environments in modeling sophisticated
systems of interacting agents.
Keywords: Active Inference · Agent-Environment Interaction · Reac-
tive Environments · Reactive Programming
1 Introduction
The Free Energy Principle (FEP) [7] distinguishes itself from other theories of
self-organization by taking an interaction-centric perspective. Active inference
(AIF) is an implication of the Free Energy Principle that extends the FEP to
control and decision-making in self-organizing natural systems.
Inthisframework,agentspossessaninternalgenerativemodelforpredicting
observations from an unknown external process. The model updates its internal
(perceptive) and active (control) states to minimize prediction errors. This uni-
fyingprinciplehasprofoundimplicationsforunderstandinghowagentsperceive
and act within complex environments.
4202
peS
71
]YS.ssee[
1v78011.9042:viXra
2 W. W. L. Nuijten & B. de Vries
AIFpositsthatagentsactivelyseektominimizetheirfreeenergy(ameasure
related to surprise or prediction error) by updating their beliefs about the envi-
ronment and selecting actions that align with these beliefs [9]. This formulation
bridges the gap between theoretical principles and practical implementations of
FEP in agent-environment interactions.
To simulate a synthetic AIF agent, researchers need the ability to control
interactions between agents and their environment in practical scenarios. For
example, a significant theory from AIF is that the human brain learns from
the proprioceptive feedback it receives from muscles [1]. Since proprioceptive
and exteroceptive sensory channels do not necessarily run at the same time-
frequency rates, researchers need fine-grained control over the communication
protocolbetweentheagentandtheenvironment.Inothersettings,onecouldbe
interested in having multiple agents share the same world, allowing communica-
tion between agents [10]. Current solutions from the reinforcement learning or
control theory community, such as Gymnasium [23] do not give end-users these
controlsoverdetailsoftheenvironment,insteadfocusingonimplementingasin-
gleagent-environmentinteractionthroughatransitionfunction.Theimperative
programming style used in these frameworks limits the communication between
agentsandenvironmentswithapredefinedtimestep,observationfrequency,and
action frequency.
This paper introduces Reactive Environments, which adopt a reactive pro-
grammingapproachtoenvironmentdesign.Incontrasttotheirimperativecoun-
terparts, Reactive Environments are not limited by strict communication con-
straints and natively allow multi-sensor, multimodal interaction between agent
andenvironment.Wewilldiscusshowareactiveprogrammingstrategyaddresses
the flaws of current frameworks and introduce RxEnvironments.jl, a specific im-
plementation of Reactive Environments in the Julia language [4]. We will show
how implementing complex real-world environments with fine-grained control
overanagent’sobservationsisstreamlinedinRxEnvironments.jl.Themainfea-
tures of RxEnvironments.jl are:
– Detailedcontroloverobservations.Differentsensorychannelscanexecuteat
differentfrequenciesorcanbetriggeredonlywhenspecificactionsaretaken,
allowing for complex interactions.
– Nativesupportformulti-agentenvironments:multipleinstancesofthesame
agenttypecanbespawnedinthesameenvironmentwithoutadditionalcode.
– Reactivity: By employing a reactive programming style, we ensure that en-
vironments will emit observations when prompted, and will idle when no
computation is necessary.
– Supportformulti-entitycomplexenvironmentswheretheagent-environment
framework does not suffice.
WithRxEnvironments,wehopetocontributetostandardizingthecreationand
simulation of Active Inference agents, allowing researchers to share their envi-
ronments and potentially creating standardized benchmarks in the future.
The main contributions of this paper are as follows:
Reactive Environments for Active Inference Agents with RxEnvironments.jl 3
– We define the Reactive Environment concept in Section 3.2.
– In Section 3.3, we introduce RxEnvironments as a package to create envi-
ronments for Active Inference agents.
– In Section 4, we demonstrate how to create complex environments with
unique needs.
2 Related Work
In reinforcement learning, the creation and sharing of control environments has
mainly been standardized with the introduction of Gymnasium [23]. Users can
use the step function in Gym to define a transition function, and Gym will han-
dle the environmental simulation. A similar alternative in Python, based on the
MuJoCophysicsengine,isDeepmindControlSuite[21].Theequivalentalterna-
tiveintheJuliaprogramminglanguagewouldbeReinforcementLearning.jl[22].
These packages export high-level interfaces to the environments they describe,
alleviating the user’s burden of timekeeping. Although these packages are de-
signed explicitly for reinforcement learning, which involves computing a reward
metricateverystate,theycanalsobeusedforActiveInferenceastheydescribe
general control environments [24,17]. Although the realization of environments
isalsopartofpopularpackagessuchasPyMDP[11]andtheSPM-DEMtoolbox
[6], these packages have their primary focus on agent creation. As a result, we
do not present RxEnvironments as a substitute but rather as a comprehensive
framework that agent-centric packages can use.
In general, we observe that there is no standardized way of defining environ-
mentsforActiveInferenceagents.WhilesomeimplementationsuseGymnasium
[24,17,20], others use specialized toolboxes for implementing Active Inference
agents for their environment simulation [5,10]. With RxEnvironments, we aim
to unite all use cases for environments in Active Inference in a robust and com-
prehensive package.
Reactive Programming (RP) has wide applications in various domains and
issimilartotheActorModel[13].RPdoesnotassumeanythingaboutthe data
generation process, allowing computation both on static datasets and real-time
asynchronous sensor observations. In Reactive Programming, it is necessary to
define how the system should react to changes in data or events rather than
explicitly programming sequences of steps. This approach is similar to an in
situcontrolsystemthatcangatherdatathroughitssensorsasynchronouslyand
respond accordingly to incoming stimuli.
3 Methods
3.1 A Model for Interaction in Active Inference
Self-organizing agents maintain their existence by creating a boundary that
separates their internal states from the external states of their environment
[12,25,18,15]. An agent can only affect its environment through its actuators,
4 W. W. L. Nuijten & B. de Vries
while its internal states are influenced only by stimuli received through its sen-
sors. Therefore, an agent has a set of actuator and sensor interfaces that it uses
to communicate with its environment. We will refer to this collection of all ac-
tuators and sensors of an agent as its boundary. A schematic of this interaction
model can be seen in Figure 1.
Actuator Actuator
Action Action
Agent Environment Agent
Observation Observation
Sensor Sensor
Boundary Boundary
Fig.1.GeneralcommunicationprotocolinanActiveInferenceenvironmentcontaining
two agents. The terms "Actuator" and "Sensor" are used from the agents’ points of
view.Weseethatbothagentshaveaboundarywithactuatorsandsensorswithwhich
they interact with the environment.
Here, the duality between the agent and environment is notable; the actions
emitted by the agent are perceived as observations for the environment, and
viceversa.This dualseparation ofinternal andexternal stateshas promptedan
overarching term for agents and environments, which we call an Entity. Note
thatEntitiesareseparatefromFristonian"things"[8]inthesensethatanEntity
can be any "thing" or an environment, making it a superclass to things. We use
the following definition of an Entity:
Definition 1. Entity. An Entity is a structure with a set of actuators and sen-
sors called a boundary that allows it to communicate with other Entities.
An Entity can, but is not obliged to, have an internal state that the sensor in-
terfacesinitsboundarycaninfluence.Entitieswithconnectedpairsofactuators
and sensors are mutually "subscribed." We use this term because any action
emitted by an agent prompts a change in the internal state of an environment.
Generally, an emitted action by an Entity prompts activity (either a response
or an update of an internal state) from the subscribed entities. In Figure 1, we
see three entities, two agents that are both subscribed to the same environment
Entity, with the boundaries of both agents expanded.
The notion of a Markov Blanket is prevalent in Active Inference literature
[18,14], and it denotes the statistical partition between a system’s internal and
externalstates.Itisworthnotingthattheconceptofaboundary,asformulated
inthissection,coincideswiththenotionofaMarkovBlanketusedinActiveIn-
ference. Therefore, modeling communication as an interaction between different
Reactive Environments for Active Inference Agents with RxEnvironments.jl 5
Entitiesthroughtheirboundariesisanadequateimplementationofthecommu-
nication of a probabilistic model with a Markov Blanket and its environment.
3.2 Reactive Environments
CommunicationbetweenEntitiesflowsthroughtheirrespectiveboundaries.Any
Entity can send data through its actuator interface to its subscribers at any
point,promptingactivityinsubscribedentities.Buildinguponthediscussionin
Section2regardingtheReactiveProgrammingparadigm,weextendtheconcept
to environments in agent-based systems. Entities should process sensory data
seamlessly and respond to subscribers. Just as programmers define how systems
respond to impulses, we aim to define how entities react to impulses exerted by
the entities to which they are subscribed. To this extent, we define a Reactive
Environment:
Definition 2. Reactive Environments. A Reactive Environment is a pair E =
(A,S) where A is a setof Entities and S is a mutableset of subscriptions, where
every s ∈ S is a pair (A ,A ), A ,A ∈ A. Each Entity responds reactively to
1 2 2 2
sensory impulses received from any of its subscribers through its sensors and is
able to emit data to any subscriber through its actuators.
ForEntitiesinaReactiveEnvironment,weprovideasetofdesideratathatenable
thedesignofcomplexcommunicationnetworkswithintheReactiveEnvironment
framework.
– Entitiesshouldbeabletoupdatetheirinternalstateinresponsetoreceived
impulses. The update should be based either on the impulse emitter or the
type of observation. For example, an agent should update its internal state
differently when receiving an audio signal versus a video signal.
– An Entity should be able to determine whether or not to transmit any re-
ceived impulse to its subscribers. For example, suppose an agent sends a
proprioceptive signal, and the Entity representing the environment receives
it. In that case, the environment can match it with an observation, but it
does not have to transmit it to its other subscribers.
– Atanygiventime,anEntityshouldbeabletosendasignaltoitssubscribers,
such as a video camera emitting a 60Hz signal.
– An Entity should be able to send different signals to different subscribers
when emitting. For example, an environment Entity can send different ob-
servations to different agent entities based on their relative position in the
environment.
In Figure 2, we see a flow chart of the logic we want every Entity to go through
whenever they get an observation. This logic can also be triggered regularly
to mimic a sensor continuously providing data at a fixed rate. An algorithm
implementingthelogicinthisflowchartallowsforallthebehaviorslistedabove.
6 W. W. L. Nuijten & B. de Vries
Check how much time has passed
since last update, and update inter- update!
nal state to current time
Incorporate new observation of type
receive!
T into internal state
For every subscriber
Check whether we should emit new
actions to subscriber when obtaining emits
observation of type T
No Yes
Do nothing Determine what
what to send
to send
Sendtosubscriber send!
Fig.2. Internal Entity logic is applied when an observation is received. On the left,
we outline the steps an Entity should follow when processing an observation. On the
right,wespecifytheRxEnvironmentsfunctionsthatuserscancreatetocustomizethis
behavior.
3.3 RxEnvironments.jl: a Particular Implementation of Reactive
Environments
Inthissection,weintroduceRxEnvironments.jl3,apackageintheopen-source
Julia[4]languagethatimplementsthecommunicationprotocolandthedesider-
ata described in Section 3.2. In RxEnvironments, we take an Entity-centric
standpoint and implement all communication logic on the Entity level. This
means that entities representing agents and environments have no constraints
on their communication, allowing agent-agent subscriptions or multi-agent en-
vironmentsnatively.ThereactiveprogrammingfeaturesofRxEnvironmentsare
based on the Rocket.jl Reactive Programming library [2]. In Figure 2 we see the
flow chart that describes the logic entities go through when processing observa-
tions, on the right we see the corresponding functions in RxEnvironments. In
this section, we will go through these functions in more detail.
3 https://github.com/biaslab/RxEnvironments.jl
Reactive Environments for Active Inference Agents with RxEnvironments.jl 7
Revisingthetransitionfunction Inpopularenvironmentdesignframeworks,
such as Gymnasium [23], the transition function for an environment takes the
action emitted by an agent and the previous state. It produces a new state
and an observation for the agent. In our Entity-centric approach, this modeling
assumption is restrictive for various reasons:
– Inascenariowhereentitiescanhavemultiplesubscribersandbesubscribed
to multiple entities, it is not desirable to trigger the transition function for
every received stimulus. To illustrate, consider a multi-agent environment
where all actions emitted by each agent should be gathered before updat-
ing and advancing the environment time. This ensures that the transition
function is not invoked too frequently.
– The transition function cannot have a constant time interval. It should be
a function of the elapsed time since the last update to account for stimuli
being received at any time.
For these reasons, we split the transition update function into two different
distinct parts:
1. Thefunctionreceive!updatestheinternalstateofthereceivingEntityby
incorporatingthestimulusreceived,giventheemittingandreceivingentities
and the stimulus as inputs.
2. The function update! takes an Entity and the time elapsed since the last
update of this Entity and updates the internal state of the Entity to reflect
that additional time has elapsed.
By splitting these two functions, we can natively support multi-agent environ-
ments,andwecansimulatethebehavioroftheinternalstateofanEntityinthe
absence of stimuli. A concrete example would be that multiple agents could still
observe a ball bouncing in an environment without explicitly abstaining from
action at every time step since the dynamics of the bouncing ball is described
in the update! function, which will continue to be called even in the absence of
stimuli.
Control over emission logic In Section 3.2, we have emphasized the im-
portance of controlling the emission logic when an Entity receives a stimulus.
Therefore, we expose two functions that govern emission logic for entities in
RxEnvironments:
1. The emits function operates by taking in the receiving Entity, the stimulus
data received by the Entity, and any subscribers of the Entity. The function
thenreturnsavaluedeterminingwhethertheEntityshouldemitthestimulus
to a particular subscriber. This function is called for all subscribers.
2. The function what_to_send takes the receiving Entity, the stimulus data
that the Entity receives, and any subscriber of the Entity. This function is
called when emits returns true and determines which stimulus will be sent
to that subscriber.
8 W. W. L. Nuijten & B. de Vries
Inthisway,designersofenvironmentshavefullcontroloverwhenandhowtheir
entities should emit. In the next section, we will demonstrate the versatility of
this framework by designing several complex environments.
Internal triggers for emission Of course, not all emissions in Entity interac-
tions are triggered by external impulses. Sensors, for example, usually provide
dataatasteadyfrequency.Therefore,weexposeaninterfacetoemitobservations
to all subscribers at regular intervals. By creating entities for different sensors
and attaching them to an overarching Entity, we can create complex systems
that combine data from different sensors at different observation frequencies.
Replicating classical reinforcement learning environments In classical
reinforcement learning environments, the environment is often seen as a passive
recipient of actions from the agent, responding to actions with observations in
the next time step. A different way to view environments is to consider them
as reactive environments. In this case, the Entity representing the environment
waitsuntilallsubscribedentitieshaveemittedbeforecallingthestatetransition
function at a predetermined time interval. This approach transforms classical
reinforcement learning environments into specific instances of reactive environ-
ments, making environmental simulation more flexible and generalizable.
4 Case Studies
Inthissection,wewillimplementseveralincreasinglycomplexenvironments.In
every environment, we employ a specific strategy from Reactive Environments
that cannot be replicated in popular environment creation packages. 4
4.1 Mountain Car
In this section, we implement a classic environment in reinforcement learning,
the Mountain Car environment [24], with a slight difference: whenever an agent
emits an action (e.g., setting the throttle of the engine), we match this with an
observation from the environment that contains the engine force applied. For
example, if the agent decides to apply 300% of its engine force, the environment
will reply that only 100% is applied. Presenting observations in this way aligns
with [1]. To realize this, we design our environment to trigger different imple-
mentations of what_to_send based on the input stimulus: Whenever a throttle
action is received, we return the throttle action that is applied to the environ-
ment,andwelettheenvironmentemitonaregularfrequencyof2Hztoreplicate
a sensor that measures the position and velocity of the mountain car. In Figure
3, we see a schematic overview of the interactions between the agent and the
environment. We see that we have two distinct types of observations from the
4 The implementation of the environments discussed in this section can be found at
https://github.com/wouterwln/RxEnvironments-Examples
Reactive Environments for Active Inference Agents with RxEnvironments.jl 9
Sensorinformation
Proprioceptivefeedback
Agentaction
Agent
Environment
t
0s 1s 2s 3s
Fig.3. Overview of interactions in the Mountain Car environment over time. The
environmentemitssensorinformationataregularinterval(2Hzinthisexample),and
whenever the agent emits an action, the environment instantaneously responds with
proprioceptive feedback to the agent.
agent, sensory and proprioceptive feedback, and the logic for obtaining both is
different.
4.2 Football Environment
Next, we describe the implementation of a simulation environment of a football
match.Footballisthemostpopularsportintheworldandinvolvestwoteamsof
11 players who handle a ball with their feet to score goals. A football game is a
complexmulti-agentgamewheretheentityrepresentingtheenvironmenthasto
handleinputsfromall22agentstoletthegamerunsmoothly.Additionally,since
we are in a noncooperative game between two teams, players can emit signals
(shout) to their teammates, so we also have an agent-to-agent communication
channel. Since all players have their position and orientation on the pitch, their
field of vision and the observations they receive are also different for each agent.
WemodelthisenvironmentwithasingleEntityrepresentingthestateofthe
worldand22Entitiesrepresentingtheindividualplayers.Theworldcontainsthe
ball and the references to all 22 player bodies, so collisions and on-ball actions
canberesolved.AllplayerEntitiesaresubscribedtotheworldEntitybutarenot
subscribedtoeachother.Wedonotexplicitlymodelagent-to-agentinteractions
because this would unnecessarily complicate the subscription graph. Instead, a
playercanchoosetoemitasignaltoallotherplayers,whichtheworldEntitywill
forward to all other players. In a sense, the world Entity represents the "global"
state of the system that keeps track of all physical interactions. At the same
time, all player Entities contain their local states and receive observations from
theglobalstate.InFigure4,wehavevisualizedanexampleofthisenvironment.
This example shows that we can, with the code used to define a single player,
createa22-playerenvironment.InthisYouTubevideo,weshowthatwecansend
commandstoallindividualplayersasynchronously.Here,wesendthecommand
to run in a random direction to a random player every 0.1 seconds.
10 W. W. L. Nuijten & B. de Vries
Fig.4. Plot of the setup of our football environment, showing the pitch and the 22
players. The ball is positioned on the center spot. An animation of this environment
where we send random run commands to players can be found here.
Due to the intricate dynamics of the football game and its non-trivial set
of rules, we have decided only to model running and on-ball actions. We aim
to demonstrate the multi-agent nature of Reactive Environments rather than
create a comprehensive football environment.
4.3 Hearing Aid Environment
Hearing aids often feature advanced acoustic noise reduction algorithms. In re-
cent years, we have seen the rise of active inference-based agents that parame-
terize hearing aid noise reduction algorithms [19]. Since a hearing aid has very
limited computing power and battery capacity, sometimes part of the agent’s
neededcomputationsmustbeperformedonaseparatewearabledevice,e.g.,the
patient’s phone. This configuration leads to a unique multi-entity system where
thehearingaidiscontinuallycommunicatingwiththreedifferententities:(1)the
outside world, which emits acoustic signals; (2) the user (hearing aid patient),
who receives the hearing aid output signal (and can potentially emit feedback
to the hearing aid about the perceived performance of the hearing aid); and
(3) with the intelligent agent at the user’s phone. Figure 5 shows a schematic
overview.
Thus, we obtain a complex entity interaction, where the hearing aid can
obtain stimuli from all 3 subscribed entities and should process the data ac-
cordingly: an acoustic signal from the outside world should be processed and
emitted to both the user and the agent; a new proposal for parameter settings,
i.e., the agent’s actions, should be incorporated into the signal processing algo-
Reactive Environments for Active Inference Agents with RxEnvironments.jl 11
Hearingaid
Environment
User
Audio Transformedaudio
Parameters Audio
Appraisals
AIF agent
Fig.5. Schematic of the subscriptions in the hearing aid environment.
rithm; and user appraisals should also be forwarded to the agent that will use
these appraisals to update its future parameter proposals.
Inshort,wehaveacomplexmulti-entityinteractionwhereeveryentityshould
handle different stimuli in different ways. We have to control which signal to
emit and when to emit the signal. Note that the hearing aid should not send
a signal to the user when receiving a new set of parameters but only when the
soundfromtheoutsideenvironmentisregistered.InaReactiveEnvironment,all
interactions are well-defined, and we can observe the signal the user hears while
also designing an agent that would take the place of the agent in our setting.
5 Discussion
While Reactive Environments generalize environments for learning agents, the
design of agents that can interact with a Reactive Environment should still
be investigated. This is because, in our framework, there are fewer constraints
on the communication between an agent and its surroundings. Traditionally,
our agent receives 1 observation per predetermined timestep and can process
this observation accordingly. In a Reactive Environment, agents can receive (or
not receive) data at any point in time, necessitating an internal clock for the
agents themselves. Although relieving this constraint on communication allows
for interesting experiments (we can disable a sensor for a particular agent to
simulate the sensor breaking down and investigate how our agent handles this
loss of data), the authors are not aware of any implementation of agents that
are built for this level of flexibility. An interesting avenue is Reactive Message
PassingonaFactorGraph[16,3],whichemploysthesamereactiveprogramming
strategy to Bayesian inference that we have taken in this paper to environment
design.
12 W. W. L. Nuijten & B. de Vries
6 Conclusions
In this paper, we presented the concept of a Reactive Environment and a par-
ticularimplementation,RxEnvironments.jl.Weshowedthatenvironmentsde-
finedintheclassicalreinforcementlearningliteraturecanbewrittenasparticular
casesofReactiveEnvironments,andweshowedthatwecanmodelmorecomplex
interactions within this paradigm. In particular, we showed that with Reactive
Environmentsweareabletomodelthecomplexcommunicationsbetweenagents
and environments necessary to realize Active Inference simulations. In our case
studies, we showed that our framework can be used to define a multitude of
different environments, demonstrating the expressive power of the framework.
Furthermore, we have presented RxEnvironments.jl, a particular implemen-
tation of Reactive Environments. Extensions of this work might investigate the
classes of agents that handle the communication protocol employed by Reactive
Agents to simulate how agents would operate in the field.
Acknowledgements
This publication is part of the project "ROBUST: Trustworthy AI-based Sys-
tems for Sustainable Growth" with project number KICH3.LTP.20.006, which
is (partly) financed by the Dutch Research Council (NWO), GN Hearing, and
the Dutch Ministry of Economic Affairs and Climate Policy (EZK) under the
program LTP KIC 2020-2023.
The authors thank Thijs van de Laar, Magnus Koudahl, and Tim Nisslbeck
for their insightful discussions during the project’s execution.
References
1. Adams,R.A.,Shipp,S.,Friston,K.J.:Predictionsnotcommands:activeinference
inthemotorsystem.BrainStructure&Function218(3),611–643(2013).https:
//doi.org/10.1007/s00429-012-0475-5,https://www.ncbi.nlm.nih.
gov/pmc/articles/PMC3637647/
2. Bagaev, D.: Rocket.jl: Reactive extensions library for Julia (2020), https://
github.com/ReactiveBayes/Rocket.jl
3. Bagaev, D., de Vries, B.: Reactive Message Passing for Scalable Bayesian Infer-
ence (Dec 2021). https://doi.org/10.48550/arXiv.2112.13251, http:
//arxiv.org/abs/2112.13251, arXiv:2112.13251 [cs]
4. Bezanson,J.,Edelman,A.,Karpinski,S.,Shah,V.B.:Julia:AFreshApproachto
NumericalComputing(Jul2015).https://doi.org/10.48550/arXiv.1411.
1607, http://arxiv.org/abs/1411.1607, arXiv:1411.1607 [cs]
5. Esaki, K., Matsumura, T., Minusa, S., Shao, Y., Yoshimura, C., Mizuno, H.:
Dynamical Perception-Action Loop Formation with Developmental Embodiment
for Hierarchical Active Inference. In: Buckley, C.L., Cialfi, D., Lanillos, P., Ram-
stead,M.,Sajid,N.,Shimazaki,H.,Verbelen,T.,Wisse,M.(eds.)ActiveInference.
pp. 14–28. Springer Nature Switzerland, Cham (2024). https://doi.org/10.
1007/978-3-031-47958-8_2
Reactive Environments for Active Inference Agents with RxEnvironments.jl 13
6. Friston, K.J., Trujillo-Barreto, N., Daunizeau, J.: DEM: a variational treatment
ofdynamicsystems.NeuroImage41(3),849–885(Jul2008).https://doi.org/
10.1016/j.neuroimage.2008.02.054
7. Friston,K.:Thefree-energyprinciple:aunifiedbraintheory?NatureReviewsNeu-
roscience 11(2), 127–138 (Feb 2010). https://doi.org/10.1038/nrn2787,
https://www.nature.com/articles/nrn2787,number:2Publisher:Nature
Publishing Group
8. Friston,K.:Afreeenergyprincipleforaparticularphysics(Jun2019).https://
doi.org/10.48550/arXiv.1906.10184, http://arxiv.org/abs/1906.
10184, arXiv:1906.10184 [q-bio]
9. Friston, K.J., Daunizeau, J., Kilner, J., Kiebel, S.J.: Action and behavior: a
free-energy formulation. Biological Cybernetics 102(3), 227–260 (Mar 2010).
https://doi.org/10.1007/s00422-010-0364-z,https://doi.org/10.
1007/s00422-010-0364-z
10. Friston,K.J.,Parr,T.,Heins,C.,Constant,A.,Friedman,D.,Isomura,T.,Fields,
C., Verbelen, T., Ramstead, M., Clippinger, J., Frith, C.D.: Federated infer-
ence and belief sharing. Neuroscience & Biobehavioral Reviews 156, 105500 (Jan
2024). https://doi.org/10.1016/j.neubiorev.2023.105500, https://
www.sciencedirect.com/science/article/pii/S0149763423004694
11. Heins,C.,Millidge,B.,Demekas,D.,Klein,B.,Friston,K.,Couzin,I.D.,Tschantz,
A.:pymdp:APythonlibraryforactiveinferenceindiscretestatespaces.Journalof
OpenSourceSoftware7(73), 4098(May2022).https://doi.org/10.21105/
joss.04098, https://joss.theoj.org/papers/10.21105/joss.04098
12. Hesp, C., Ramstead, M., Constant, A., Badcock, P., Kirchhoff, M., Friston, K.: A
Multi-scaleViewoftheEmergentComplexityofLife:AFree-EnergyProposal.In:
Georgiev, G.Y., Smart, J.M., Flores Martinez, C.L., Price, M.E. (eds.) Evolution,
Development and Complexity. pp. 195–227. Springer Proceedings in Complexity,
Springer International Publishing, Cham (2019). https://doi.org/10.1007/
978-3-030-00075-2_7
13. Hewitt,C.,Bishop,P.,Steiger,R.:Session8formalismsforartificialintelligencea
universalmodularactorformalismforartificialintelligence.In:Advancepapersof
theconference.vol.3,p.235.StanfordResearchInstituteMenloPark,CA(1973)
14. Kaufmann, R., Gupta, P., Taylor, J.: An active inference model of collective
intelligence. Entropy 23(7), 830 (Jun 2021). https://doi.org/10.3390/
e23070830,http://arxiv.org/abs/2104.01066,arXiv:2104.01066[cs,eess]
15. Kirchhoff, M., Parr, T., Palacios, E., Friston, K., Kiverstein, J.: The
Markov blankets of life: autonomy, active inference and the free en-
ergy principle. Journal of The Royal Society Interface 15(138), 20170792
(Jan 2018). https://doi.org/10.1098/rsif.2017.0792, https:
//royalsocietypublishing.org/doi/10.1098/rsif.2017.0792, pub-
lisher: Royal Society
16. Loeliger, H.A., Dauwels, J., Hu, J., Korl, S., Ping, L., Kschischang, F.R.:
The Factor Graph Approach to Model-Based Signal Processing. Proceedings of
the IEEE 95(6), 1295–1322 (Jun 2007). https://doi.org/10.1109/JPROC.
2007.896497
17. Van de Maele, T., Dhoedt, B., Verbelen, T., Pezzulo, G.: Integrating cognitive
map learning and active inference for planning in ambiguous environments (Aug
2023). https://doi.org/10.48550/arXiv.2308.08307, http://arxiv.
org/abs/2308.08307, arXiv:2308.08307 [cs]
14 W. W. L. Nuijten & B. de Vries
18. Palacios, E.R., Razi, A., Parr, T., Kirchhoff, M., Friston, K.: On Markov blan-
ketsandhierarchicalself-organisation.JournalofTheoreticalBiology486,110089
(Feb 2020). https://doi.org/10.1016/j.jtbi.2019.110089, https://
www.sciencedirect.com/science/article/pii/S0022519319304588
19. Podusenko,A.,vanErp,B.,Koudahl,M.,deVries,B.:AIDA:AnActiveInference-
BasedDesignAgentforAudioProcessingAlgorithms.FrontiersinSignalProcess-
ing2(Mar2022).https://doi.org/10.3389/frsip.2022.842477,https:
//www.frontiersin.org/articles/10.3389/frsip.2022.842477, pub-
lisher: Frontiers
20. Safa, A., Verbelen, T., Keuninckx,L.,Ocket, I., Bourdoux, A., Catthoor, F., Gie-
len, G., Cauwenberghs, G.: Active Inference in Hebbian Learning Networks (Jun
2023). https://doi.org/10.48550/arXiv.2306.05053, http://arxiv.
org/abs/2306.05053, arXiv:2306.05053 [cs]
21. Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D.d.L., Budden, D.,
Abdolmaleki, A., Merel, J., Lefrancq, A., Lillicrap, T., Riedmiller, M.: DeepMind
Control Suite (Jan 2018). https://doi.org/10.48550/arXiv.1801.00690,
http://arxiv.org/abs/1801.00690, arXiv:1801.00690 [cs]
22. Tian, J.: ReinforcementLearning. jl: A reinforcement learning package
for the julia programming language (2020), https://github.com/
JuliaReinforcementLearning/ReinforcementLearning.jl
23. Towers,M.,Terry,J.K.,Kwiatkowski,A.,Balis,J.U.,Cola,G.,Deleu,T.,Goulão,
M., Kallinteris, A., KG, A., Krimmel, M., Perez-Vicente, R., Pierré, A., Schul-
hoff, S., Tai, J.J., Tan, A.J.S., Younis, O.G.: Gymnasium (Feb 2024). https:
//doi.org/10.5281/zenodo.10655021, https://zenodo.org/records/
10655021
24. Ueltzhöffer, K.: Deep Active Inference. Biological Cybernetics 112(6), 547–
573 (Dec 2018). https://doi.org/10.1007/s00422-018-0785-7, http:
//arxiv.org/abs/1709.02341, arXiv:1709.02341 [q-bio]
25. Varela, F.G., Maturana, H.R., Uribe, R.: Autopoiesis: The organization of
living systems, its characterization and a model. Biosystems 5(4), 187–196
(May 1974). https://doi.org/10.1016/0303-2647(74)90031-8, https:
//www.sciencedirect.com/science/article/pii/0303264774900318
Reactive Environments for Active Inference Agents with RxEnvironments.jl 15
A Appendix: Creating a simple environment
In this code example, we will demonstrate the creation of a simple environment
in RxEnvironments, demonstrating that the additional boilerplate code needed
to write an imperative environment as a Reactive Environment is minimal. We
will implement the Bayesian Thermostat example, which is also showcased in
the RxEnvironments documentation.
A.1 Defining the environment
The Bayesian Thermostat environment is a very simple environment that moni-
tors the temperature in a room. The temperature can fluctuate between a mini-
malandamaximaltemperature,andanagentcaninfluencethistemperatureby
adding or subtracting heat from the room. Furthermore, the environment cools
down over time.
A.2 Environment boilerplate
Inthissectionwewillwriteallboilerplatecodenecessarytoruntheenvironment.
We start by defining the structures needed to store the temperature and envi-
ronment properties and expose helper functions that change this temperature,
namely the add_temperature! function.
(cid:7) (cid:4)
using Distributions
# Empty agent, could contain states as well
struct ThermostatAgent end
mutable struct BayesianThermostat{T}
temperature::T
min_temp::T
max_temp::T
end
# Helper functions
temperature(env::BayesianThermostat) = env.temperature
min_temp(env::BayesianThermostat) = env.min_temp
max_temp(env::BayesianThermostat) = env.max_temp
noise(env::BayesianThermostat) = Normal(0.0, 0.1)
set_temperature!(env::BayesianThermostat, temp::Real) = env.
temperature = temp
function add_temperature!(env::BayesianThermostat, diff::
Real)
env.temperature += diff
if temperature(env) < min_temp(env)
set_temperature!(env, min_temp(env))
elseif temperature(env) > max_temp(env)
set_temperature!(env, max_temp(env))
end
(cid:6)end (cid:5)
16 W. W. L. Nuijten & B. de Vries
A.3 RxEnvironments specific code
Inthissection,wewillimplementtheRxEnvironments-specificcode.Wehavea
verysimpleinteractionscheme:Whentheagentemitsanaction,wewantthisto
be incorporated into the environment state, but we only want the environment
to emit observations on a fixed frequency, and not present an observation when-
ever the agent chooses to change the environment. Therefore, we implement the
receive!, update!, emits and what_to_send functions for the environment:
(cid:7) (cid:4)
# When the environment receives an action from the agent, we
shouldn't emit back to the agent
RxEnvironments.emits(::BayesianThermostat, ::ThermostatAgent
, ::Real) = false
# In any other case, we should emit (This line is obsolete
since this is the default behavior, but we include it
for clarity)
RxEnvironments.emits(::BayesianThermostat, ::ThermostatAgent
, any) = true
# When the environment receives an action from the agent, we
add the value of the action to the environment
temperature.
RxEnvironments.receive!(recipient::BayesianThermostat,
emitter::ThermostatAgent, action::Real) =
add_temperature!(recipient, action)
# The environment sends a noisy temperature observation to
the agent.
RxEnvironments.what_to_send(recipient::ThermostatAgent,
emitter::BayesianThermostat) = temperature(emitter) +
rand(noise(emitter))
# The environment cools down over time.
RxEnvironments.update!(env::BayesianThermostat, elapsed_time
(cid:6) )= add_temperature!(env, -0.1 * elapsed_time) (cid:5)
A.4 Invoking the environment
We now have all the code necessary to kickstart our environment:
(cid:7) (cid:4)
environment = RxEnvironment(BayesianThermostat(0.0, -10.0,
10.0); emit_every_ms = 1000)
(cid:6)agent = add!(environment, ThermostatAgent()) (cid:5)
Now, your environment will be running, and agent will receive a noisy observa-
tion from the environment every second.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Reactive Environments for Active Inference Agents with RxEnvironments.jl"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
