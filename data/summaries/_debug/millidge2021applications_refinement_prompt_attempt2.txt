Fix these issues in your summary:
- CRITICAL: The current summary has severe repetition issues. You MUST eliminate all repeated sentences, phrases, and paragraphs. Each idea should be expressed only once. If you find yourself repeating content, remove the duplicates entirely. Focus on variety and uniqueness in your wording.
- Severe repetition detected: Same phrase appears 7 times (severe repetition)

Current summary:
### OverviewThis paper, “Applications of the Free Energy Principle to Machine Learning and Neuroscience,” investigates the potential of the Free Energy Principle (FEP) to address complex problems in both fields. The authors argue that systems, including the brain, can be understood as performing variational Bayesian inference to minimize a variational free energy, effectively self-organizing and maintaining a stable state. The central thesis is that the FEP provides a unifying framework for understanding perception, action, and learning.### MethodologyThe authors state: "The free energy principle (FEP) (Friston,2019a; Friston & Ao,2012a; Friston, Kilner, & Harrison,2006; Parr, DaCosta, & Friston,2020) is an emerging theory in theoretical neuroscience which aims to tackle extremely deep and fundamental question–can one characterise necessary behaviour of any system that maintains a statistical separation from its environment (Bruineberg, Dolega, Dewhurst, & Baltieri,2020; Friston,2019a; Parr, Da Costa, & Friston,2020)?" They note that it argues any such system can be seen as performing an elemental kind of Bayesian inference where the dynamics of the internal states of such a system can be interpreted as minimizing a variational free energy functional (Beal,2003)1, and thus performing approximate (variational) Bayesian inference(Friston,2019a). The authors further explain that the FEP isthus effectively aformalization and generalizationoftheAshbyangoodregulatorprinciple(Conant

Paper text:
Applications of the Free Energy Principle to
Machine Learning and Neuroscience
Beren Millidge
NI VE
U R
S
E I
H T
T Y
O H
F G
R
E
D I N B U
Doctor of Philosophy
Institute for Adaptive and Neural Computation
School of Informatics
University of Edinburgh
2021
1202
nuJ
03
]IA.sc[
1v04100.7012:viXra
Abstract
In this thesis, we explore and apply methods inspired by the free energy principle to
two important areasin machinelearningand neuroscience. Thefree energyprinciple
is a general mathematical theory of the necessary information-theoretic behaviours
of systems which maintain a separation from their environment. A core postulate of
the theory is that complex systems can be seen as performing variational Bayesian
inferenceandminimizinganinformation-theoreticquantitycalledthevariationalfree
energy. The freeenergy principleoriginatedin, andhas beenextremely influentialin
theoreticalneuroscience,havingspawnedanumberofneurophysiologicallyrealistic
processtheories,andmaintainingcloselinkswithBayesianBrainviewpoints.
Thethesisissplitintothreemainpartswhereweapplymethodsandinsightsfromthe
freeenergyprincipletounderstandquestionsfirstinperception,thenaction,andfinally
learning. Specifically, in the first section, we focus onthe theory of predictive coding,
a neurobiologically plausible process theory derived from the free energy principle
under certain assumptions, which argues that the primary function of the brain is to
minimizepredictionerrors. Wefocusonscalinguppredictivecodingarchitecturesand
simulate large-scale predictive coding networks for perception on machine learning
benchmarks;weinvestigatepredictivecoding’srelationshiptootherclassicalfiltering
algorithms,andwedemonstratethatmanybiologicallyimplausibleaspectsofcurrent
modelsofpredictivecodingcanberelaxedwithoutundulyharmingtheperformance
ofpredictivecodingmodelswhichallowsforapotentiallymoreliteraltranslationof
predictivecodingtheoryintocorticalmicrocircuits.
Inthesecondpartofthethesis,wefocusontheapplicationofmethodsderivingfromthe
freeenergyprincipletoaction. Westudytheextensionofmethodsof‘activeinference’,
aneurobiologicallygroundedaccountofactionthroughvariationalmessagepassing,
to utilize deep artificial neural networks, allowing these methods to ‘scale up’ to be
competitivewithstateoftheartdeepreinforcementlearningmethods. Additionally,we
i
showthattheseactiveinferenceinspiredmethodscanbringconceptualclarityandnovel
perspectivesto deepreinforcement learning. We showhow active inferencereveals the
importanceof deepgenerative models andmodel-based planningfor adaptive action,
as wellas information-seekingexploration whicharises undera unifiedmathematical
frameworkfromactiveinference. Finally,weprovideaunifiedmathematicallyprinci-
pledframeworkforunderstandingandderivingmanyinformation-seekingexploration
objectives through the lens of a dichotomy between ‘evidence’ and ‘divergence’ ob-
jectives. We show that this distinction is crucial for understanding and relating the
many exploratory objectives in both the reinforcementlearning, active inference, and
cognitivesciencecommunitiesandthatthisprovidesageneralmathematicalframework
forspecifyingtheobjectivesunderlyingintelligent,adaptivebehaviour.
Finally,wefocusonapplicationsofthefreeenergyprincipletoquestionsoflearning
where we attempt to understand how credit assignment can take place in the brain.
First,wedemonstratethat,undercertainconditions,thepredictivecodingalgorithmcan
closelyapproximatethebackpropagationoferroralgorithmalongarbitrarycomputation
graphs,whichunderliesthetrainingofessentiallyallcontemporarymachinelearning
architectures,thusindicating apotentialpathto the directimplementationofmachine
learningalgorithmsinneural circuitry. Finally,we explore otheralgorithmsforbiologi-
callyplausiblecreditassignmentinthebrain,andpresentActivationRelaxation,anovel
algorithm which can approximatebackprop usingonly locallearning rules whichare
substantiallysimplerthanthosenecessaryforpredictivecoding. Weadditionallyshow
thatthesomerelaxationsthatapplytopredictivecoding,alsoworkfortheactivation
relaxationalgorithm,thusproducinganextremelyelegantandeffectivealgorithmfor
localapproximationstobackpropinthebrain.
In sum, we believe we have demonstrated the theoretical utility of the free energy
principle,bydemonstratinghowmethodsinspiredbyitcaninterfaceproductivelywith
otherfields,specificallyneuroscienceandmachinelearning,todevelopandimprove
ii
existing methods, as well as inspire novel advances, in all three areas of perception,
action,andlearning. Moreover,throughoutthisthesis,wedemonstrateimplicitly,the
theoretical benefit brought about by the FEPs unified treatment of these seemingly
disparateprocesses,undertherubricoffreeenergyminimization.
iii
Acknowledgements
Iwouldlike tothankmysupervisor,Richard Shillcock, forallhis helpandadviceover
theyears;forhisgivingmefreedomtoworkonthetopicsinthisthesiseventhoughthey
did notalign withhis plannedresearch trajectory, and forhis perseverance inhandling
myendlessdrafts. Secondly,ahugethankstoChristopherLBuckley,AlecTschantz,
andAnil Sethforhosting mefor ayearat theSacklerCentre andtheEvolutionary and
AdaptiveSystemsGroup(EASY)attheUniversityofSussex. Youhavealltaughtme
somuchaboutthecraftofresearch,andwithoutyourmentorshipandguidance,Iwould
notbehalftheresearcherIamtoday. IwouldalsoliketothankConorHeinsformany
stimulatingconversationsaroundthefreeenergyprincipleandrelatedtopics. Finally,
andaboveall, Iwouldlike tothankmywife, MycahBanks,for herunendingloveand
supportthroughoutthisentireprocess;heraidwithproofreadingandfigurepreparation
for many papers, and her sacrifice inputting up with ahusband who always hassome
moreresearchtodo. Withoutyou,noneofthiswouldbepossible. Thankyou,Mycah.
iv
Declaration
Ideclarethatthisthesiswascomposedbymyself,thattheworkcontainedhereinismy
ownexceptwhereexplicitlystatedotherwiseinthetext,andthatthisworkhasnotbeen
submittedforanyotherdegreeorprofessionalqualificationexceptasspecified.
(BerenMillidge)
v
Table of Contents
1 Introduction 1
1.1 ThesisOverview . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2 StatementofContributions . . . . . . . . . . . . . . . . . . . . . . . 7
1.2.1 IncludedinThesis . . . . . . . . . . . . . . . . . . . . . . . 8
1.2.2 NotIncludedintheThesis . . . . . . . . . . . . . . . . . . . 13
2 TheFreeEnergyPrinciple 16
2.1 HistoryandLogicalStructure . . . . . . . . . . . . . . . . . . . . . . 21
2.2 Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.3 MarkovBlankets . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
2.4 VariationalInference . . . . . . . . . . . . . . . . . . . . . . . . . . 34
2.5 IntrinsicandExtrinsicinformationgeometries . . . . . . . . . . . . . 37
2.6 Self-OrganizationandVariationalInference . . . . . . . . . . . . . . 40
2.7 TheExpectedFreeEnergyandActiveInference . . . . . . . . . . . . 47
2.8 PhilosophicalStatusoftheFEP . . . . . . . . . . . . . . . . . . . . . 50
2.9 DiscussionofAssumptionsrequiredfortheFEP . . . . . . . . . . . . 57
2.9.1 AssumptionsontheFormoftheLangevinDynamics . . . . . 60
2.9.2 TheMarkovBlanketCondition . . . . . . . . . . . . . . . . 61
2.9.3 AssumptionsofthefreeenergyLemma . . . . . . . . . . . . 63
2.10 ActiveInference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
2.10.1 DiscreteState-spacemodelsandPerception . . . . . . . . . . 71
vi
2.10.2 ActionSelectionandtheExpectedFreeEnergy . . . . . . . . 72
2.11 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
3 PredictiveCoding 76
3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
3.2 PredictiveCoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
3.3 Hierarchicalpredictivecoding . . . . . . . . . . . . . . . . . . . . . 83
3.3.1 DynamicalPredictivecoding . . . . . . . . . . . . . . . . . . 90
3.4 PredictiveCodingandKalmanFiltering . . . . . . . . . . . . . . . . 101
3.4.1 TheKalmanFilter . . . . . . . . . . . . . . . . . . . . . . . 105
3.4.2 PredictiveCodingasKalmanFiltering . . . . . . . . . . . . . 107
3.4.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
3.4.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
3.5 RelaxedPredictiveCoding . . . . . . . . . . . . . . . . . . . . . . . 122
3.5.1 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
3.5.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
3.5.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
3.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
4 ScalingActiveInference 144
4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
4.1.1 ReinforcementLearning . . . . . . . . . . . . . . . . . . . . 145
4.1.2 DeepReinforcementLearning . . . . . . . . . . . . . . . . . 154
4.1.3 Model-freevsModel-based . . . . . . . . . . . . . . . . . . 156
4.1.4 ExplorationandExploitation . . . . . . . . . . . . . . . . . . 159
4.1.5 ControlasInference . . . . . . . . . . . . . . . . . . . . . . 163
4.2 DeepActiveInference . . . . . . . . . . . . . . . . . . . . . . . . . 168
4.2.1 Model-Free: ActiveInferenceasVariationalPolicyGradients 171
4.2.2 Model-based: ReinforcementLearningthroughActiveInference189
vii
4.2.3 RelatedWork . . . . . . . . . . . . . . . . . . . . . . . . . . 199
4.2.4 IterativeandAmortisedInference . . . . . . . . . . . . . . . 201
4.2.5 ControlasHybridInference . . . . . . . . . . . . . . . . . . 206
4.3 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
5 TheMathematicalOriginsofExploration 218
5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
5.2 OriginsoftheExpectedFreeEnergy . . . . . . . . . . . . . . . . . . 221
5.2.1 ControlasInferenceandActiveInference . . . . . . . . . . . 228
5.3 EvidenceandDivergenceObjectives . . . . . . . . . . . . . . . . . . 234
5.3.1 ControlasInference . . . . . . . . . . . . . . . . . . . . . . 241
5.3.2 KLControl . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
5.3.3 ActiveInference . . . . . . . . . . . . . . . . . . . . . . . . 243
5.3.4 ActionandPerceptionasDivergenceMinimization . . . . . . 245
5.4 Towards a General Theory of Mean-Field Variational Objectives for
Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
5.4.1 EncodingValue . . . . . . . . . . . . . . . . . . . . . . . . . 250
5.4.2 GeneralGraphicalModels . . . . . . . . . . . . . . . . . . . 261
5.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
6 CreditAssignmentintheBrain 268
6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268
6.1.1 BackpropagationintheBrain . . . . . . . . . . . . . . . . . 270
6.2 PredictiveCodingApproximatesBackpropAlongArbitraryComputa-
tionGraphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
6.2.1 MethodsandResults . . . . . . . . . . . . . . . . . . . . . . 286
6.2.2 RNNandLSTM . . . . . . . . . . . . . . . . . . . . . . . . 292
6.3 InterimDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . 301
6.4 ActivationRelaxation . . . . . . . . . . . . . . . . . . . . . . . . . . 305
viii
6.4.1 MethodandResults . . . . . . . . . . . . . . . . . . . . . . 309
6.4.2 LooseningConstraints . . . . . . . . . . . . . . . . . . . . . 311
6.4.3 InterimDiscussion . . . . . . . . . . . . . . . . . . . . . . . 318
6.5 Three-FactorLearningRulesandaDirectImplementation . . . . . . 322
6.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326
6.7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330
7 Discussion 333
7.1 Question1: ScalingActiveInference . . . . . . . . . . . . . . . . . . 337
7.2 Question2: TheMathematicalOriginsofExploration . . . . . . . . . 345
7.3 Question3: CreditAssignmentintheBrain . . . . . . . . . . . . . . 352
7.4 ClosingThoughts . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363
A DerivationofKalmanFilteringEquationsfromBayes’Rule 367
B AppendixB:EquationsoftheLSTMcell 370
C AppendixC:PredictiveCodingUndertheLaplaceApproximation 372
References 375
ix
List of Figures
2.1 ThelogicalflowoftheargumentoftheFEPfromtheinitialformulation
tothecrucialapproximateBayesianinferencelemma. Webeginwitha
settingofrandomLangevinstochasticdynamicalsystems,whichpos-
sessanon-equilibrium-steadystate. ByapplyingtheAodecomposition,
we can understand the dynamics in terms of a gradient descent upon
thesurprisal. UpontheadditionofaMarkovBlanketpartition,wecan
expresssubsets intermsoftheirownmarginalflows viathemarginal
flow lemma. If we then identify the internal states as parametrizing
avariationaldistributionovertheexternalstates,we caninterpretthe
marginalflowonthesurprisalasaflowonthevariationalfreeenergy,
undertheLaplaceapproximation. . . . . . . . . . . . . . . . . . . . 20
x
2.2 TheintuitionbehindtheMarkovBlanketpartition. Thebrain(orbacil-
lus) consists of internal states µ which are separated from the outside
world(externalstatesηbytheblanketstatesb,whichcanthemselves
bepartitionedintosensorystatess,representingthesensoryepithelia,
andwhich aredirectlyinfluenced byexternalstates, andactivestatesa
representingtheorganismseffectorsandwhicharedirectlyinfluenced
by internal states, and act on external states. We see that perception
concernstheminimizationoffree energyoftheinternalstates,while
action concerns the minimization of the expected free energy of the
activestates. FigureoriginallyappearedinFriston(2019a) . . . . . . 32
3.1 MNISTdigits inthetraining setrecreatedbythe network. Toprowthe
actualdigits,bottomrow,thepredictivereconstructions. . . . . . . . 86
3.2 UnseenMNISTdigitsinthetestsetrecreatedbythenetwork. Toprow
theactualdigits,bottomrow,thepredictivereconstructions. . . . . . . 87
3.3 Images of hallucinated digits "dreamt" by the network. These were
generated by sampling the latent space around the representations of
someexemplardigitsinthelatentspace,andthenlettingthepredictive
codingnetworkgenerateitspredictionfromthechosenlatentstate. . . 87
3.4 APCAclusteringplotofthevaluesoftestMNISTdigitsinthelatent
space. Eventhoughthe20dimensionallatentspacehasbeenreduced
down to two, clusters are still visible. For instance, all the 1s are
clustered in the top left corner. We thus see that predictive coding
appears to be a powerful and fully unsupervised learning algorithm,
capable of separating out distinct digits in the latent space, despite
not being trained with any label information at all – and purely on
reconstruction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
xi
3.5 TestsetCIFARdigitsreconstructedbythenetwork. Thefirstofthetwo
linesistheimageandthesecondisthereconstruction. Thenetworkis
extremelygoodatreconstructingCIFARimages. . . . . . . . . . . . 89
3.6 The CIFAR model interpolating between a horse and a cat. Read the
imageslefttorighttoptobottom-liketext. Theinterpolationisdone
bysteppinginthelatentspacefromtherepresentationofthefirstimage
inthedirectionoftheseconduntilitisreached. . . . . . . . . . . . . 90
3.7 Predictionerrorsandpredictionforsimpletoydynamicalmodels. The
task of the dynamical predictive coding model is to learn to predict
a sinewave using only the first two dynamical orders – so including
position, velocity, and acceleration. The model starts from randomly
initialized parameters. We see that the model very quickly learns to
matchthe incomingsinewaveobservations withonlyminimal errorat
thebeginning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
3.8 Dynamical models tested on more challenging sawtooth and square
waveinputs. Themodelwasrandomlyinitializedandonlymodelledthe
firsttwodynamicalorders(soposition,velocity,acceleration). Apart
fromabriefinitialperiodofuncertainty,themodelrapidlylearnedto
predictthesemorechallengingwaveshapes. . . . . . . . . . . . . . . 99
3.9 The training graphsof thefull constructmodel. It cansuccessfully pre-
dictthefirstthreetemporalderivativesofasinewave,andalsominimise
prediction error up to multiple hierarchical layers. The full construct
modelwasrandomlyinitialized,andlearntbothparametersandinferred
statespurely online– thusachievinga‘double deconvolution’(Friston,
Trujillo-Barreto,&Daunizeau,2008). . . . . . . . . . . . . . . . . . 100
xii
3.10 The true dynamics, control input, and observations generated by a
random C matrix. These are the source of truth that the predictive
coding Kalman filter tries to approximate. The observations differ
substantiallyfromthetruedynamicsduetotherandomCmatrix,which
makesthe inference problemfaced bythe predictive coding filtermuch
more challenging,since itmust de-scramblethe observations toinfer
thetruedynamics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
3.11 Tracking performance of our gradient filter compared to the true val-
uesandtheanalyticalKalmanFilter.Weshowthetrackingover2000
timesteps. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
3.12 Here, we zoom in on 100 timestep period to demonstrate tracking
performanceinminiatureandtheeffectoffewgradientupdates. Inthis
case,weplottedtheestimatesafteronly5steps. Eventwostepsoften
suffice(withalargelearningrate)toprovideveryaccurateestimates . 115
3.14 FilteringperformanceforadaptivelylearningboththeAandBmatrices
inconcert(secondrow). Thefilteringbehaviouroftheoftherandomly
initialized filters without adaptive learning is also shown. Importantly,
withlearningtheestimatedposition,velocity,andaccelerationstrack
theirtruevaluespreciselywhilethe estimateswithoutlearningandjust
randomlyinitializedAorAandBmatricesrapidlydivergefromthetruth.118
3.15 Very poor tracking behaviour with a learnt C matrix. This is despite
thefactthattheBayesianlossfunctionrapidlydecreasestoaminimum.
This shows that the filter can find a prediction-error minimizing "so-
lution"whichalmostarbitrarilydepartsfromrealityiftheC-matrixis
randomized. PanelDshowsthelosscomputedbythenetworkwhich
rapidly declines, even while the predictions rapidly diverge from the
truth(Panelsa,b,c) . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
xiii
3.16 The weight transportproblem and our solution. On the leftis the stan-
dard predictive coding network architecture. Our diagram represents
thepredictionerrorsεofonelayerreceivingpredictionsandtransmitted
prediction errors to the value neurons of the layer above. Prediction
errors are transmitted upwards using the same weight matrix θT as
thepredictionsaretransmitteddownwards. Ontheright,oursolution
eschews this biological implausibility by proposing a separate set of
backwards weight θ˜ (in red), which are learned separately using an
additionalHebbianlearningrule. . . . . . . . . . . . . . . . . . . . . 129
3.17 Testaccuracyofpredictivecodingnetworkswithbothlearntbackwards
weights, and the ideal weight transposes with both relu and tanh ac-
tivation functions on the MNIST and FashionMNIST datasets. Both
networksobtainalmostidenticallearningcurves,thussuggestingthat
learntbackwardsweightsallowforasolutiontotheweight-transport
problem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
3.18 Testaccuracy ofpredictivecoding networkswithand withoutthenon-
linear derivative term, using relu and tanh activation functions on the
MNIST and FashionMNIST datasets. We find that on the MNIST
datasetperformanceissimilar,whileontheFashionMNISTdatasetand
thetanhactivationfunction,thelackofthenonlinearderivativeappears
toslightlyhurtperformance. . . . . . . . . . . . . . . . . . . . . . . 132
3.19 The error-connectivity problem and our solution. On the left, the
biologically implausible one-to-one connectivity between value and
error nodes required by the standard predictive coding theory. On
the right, our solution to replace these one to one connections by a
fullyconnectedconnectivitymatrixψ. BylearningψwithaHebbian
learning rule we are able to achieve comparable performance to the
one-to-oneconnectionswithafullydispersedconnectivitymatrix. . . 133
xiv
3.20 Testaccuracyofpredictivecodingnetworkswithandwithoutlearnable
error connections for both relu and tanh activation functions on the
MNISTandFashionMNISTdatasets. Weseethat,interestingly,using
learnterrorweightsdecreasedperformanceonlywiththetanhbutnot
therelunonlinearity,andthenonlyslightlyintheFashionMNISTcase. 135
3.21 Schematic representations of the architecture across two layers of a.)
the standard predictive coding architecture and b.) The fully relaxed
architecture. Importantly,thisarchitecturehasfullconnectivitybetween
allnodesandalsonon-symmetricforwardandbackwardsconnectivity
inallcases. Ineffect,thisarchitectureonlymaintainsabipartitegraph
betweenerrorandvalueneurons,butnootherclearstructure . . . . . 136
3.22 Test accuracy standard and fully relaxed predictive coding networks
(the combined algorithm), for both relu and tanh activation functions
on the MNIST and FashionMNIST datasets. We see that, interest-
ingly,performance is degraded in all cases and that the relu networks
areespeciallyaffected–withcatastrophicdeclinesinperformanceto
become almost random. The reasons for this are currently unknown
andwillbeinvestigatedinfuturework. . . . . . . . . . . . . . . . . . 137
4.1 Graphicalmodelforcontrolasinference,withoptimalityvariablesΩ.
Otherthantheoptimalityvariables,thegraphicalmodeltakestheform
ofaMarkovDecisionProcesswithactionsaandstatess. Thestateof
aspecifictimestepdependsontheactionandstateofthelasttime-step.
By writing out an explicit graphical model like this, we can apply a
whole field’s worth of inference algorithms on graphical models like
thistosolvecontrolproblems. . . . . . . . . . . . . . . . . . . . . . 163
xv
4.2 ComparisonofthemeanrewardobtainedbytheActiveInferenceagent
comparedtotworeinforcementlearningbaselinealgorithms–Actor-
Critic and Q learning on the CartPole environment. We demonstrate
the learning curves over 2000 episodes, averaged over 5 different seeds.
500 is the maximum possible reward. We see that while the vanilla
actor critic agent initially learns faster, over a long time horizon, the
activeinferenceagentoutperformsit–andbothperformbetterthanthe
vanillaQlearningagent. . . . . . . . . . . . . . . . . . . . . . . . . 181
4.3 ComparisonofActiveInference with standardreinforcementlearning
algorithms on the Acrobot environment. Here we see the learning
curves plotted over five seeds over 20000 episodes. The maximum
possible reward in this environment was 0, so no agents are optimal.
Weseeagainthatactiveinferenceoutperformstheothertwomethods
consistently. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
4.4 ComparisonofActiveInferencewithreinforcementlearningalgorithms
on the Lunar-Lander environment. Learning curves presented over
15000 episodes, averaged over 5 seeds. Here the vanilla policy gra-
dient algorithm strongly outperforms the others, for unclear reasons,
although active inference is still comparable with the other standard
reinforcementlearningalgorithms. Ascoreof200isoptimal. . . . . . 183
4.5 WecomparethefullActiveInferenceagent(entropyregularization+
transitionmodel) withan ActiveInference agentwithout thetransition
model, and without both the entropy term and the transition model).
Weseethatwhileremovingthetransitionmodelappearstohavelittle
effect, removing the entropy regularisation term substantially impairs
performance. Thismaybeduetotheentropytermaidinginstavingoff
policycollapse. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
xvi
4.6 ComparisonoftherewardsobtainedbythefullyablatedActiveInfer-
enceagentwithstandardreinforcement-learningbaselinesofQ-learning
and Actor-Critic on the CartPole environment. Learning curves are
averaged over 5 seeds. We see that despite being fully ablated, the
activeinferenceagentcontinoustoperformcomparablywithstandard
reinforcementlearningagents. . . . . . . . . . . . . . . . . . . . . . 185
4.7 (A) Mountain Car: Average return after each episode on the sparse-
rewardMountainCartask. Ouralgorithmachievesoptimalperformance
inasingletrial. (B)CupCatch: Averagereturnaftereachepisodeon
thesparse-rewardCupCatchtask. Here,resultsamongstalgorithmsare
similar,withallagentsreachingasymptoticperformanceinaround20
episodes. (C&D)HalfCheetah: Averagereturnaftereachepisodeon
thewell-shapedHalfCheetahenvironment,fortherunningandflipping
tasks, respectively. Wecompare ourresults tothe average performance
ofSACafter100episodeslearning, demonstratingouralgorithmcan
perform successfully in environments which do not require directed
exploration. Each line is the mean of 5 seeds and filled regions show
+/-standarddeviation. . . . . . . . . . . . . . . . . . . . . . . . . . . 196
4.8 (A & B) Mountain Car state space coverage: We plot the points in
state-spacevisitedbytwoagents-onethatminimizesthefreeenergyof
theexpectedfuture(FEEF)andonethatmaximisesreward. Theplots
arefrom20episodesandshowthattheFEEFagentsearchesalmostthe
entirety of state space, while the reward agent is confined to a region
thatcanbereachedwithrandomactions. (C)AntMazeCoverage: We
plotthepercentageofthemazecoveredafter35episodes,comparing
the FEEF agent to an agent acting randomly. These results are the
averageof4seeds. . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
xvii
4.9 OverviewofclassicRLandcontrolalgorithmsinourscheme. Standard
model-freeRLcorrespondstoamortisedpolicies,planningalgorithms
areiterativeplanning,andcontroltheoryinfersiterativepolicies. The
amortisedplansquadrantisempty,perhapssuggestingroomfornovel
algorithms. The position of the algorithms within the quadrant is not
meaningful. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
4.10 (a - c): Amortised predictions of q (a|s;θ) are shown in red, where •
φ
denote the expected states, shaded areas denote the predicted actions
varianceateachstep,andtheexpectedtrajectoryrecoveredbyiterative
inferenceisshowninblue. At theonsetoflearning(a),theamortised
predictions are highly uncertain, and thus have little influence on the
final approximate posterior. As the amortised model f (·) learns (b),
φ
thecertaintyoftheamortisedpredictionsincrease,suchthatthefinal
posteriorremainsclosertotheinitialamortisedguess. Atconvergence,
(c), the iterative phase of inference has negligible influence on the
final distribution, suggesting convergence to a model-free algorithm.
(d) Here, we compare our algorithm to its constituent components –
thesoft-actorcritic(SAC)andanMPCalgorithmbasedonthecross-
entropymethod(CEM).Theseresultsdemonstratethatthehybridmodel
significantlyoutperformsbothofthesemethods. . . . . . . . . . . . . 212
xviii
5.1 Numerical illustration of optimizing a multimodal desired distribu-
tion with an Evidence objective (Panel A) vs a Divergence Objective
(panel B). The desire distribution consistedof the sum of two univari-
ate Gaussian distributions, with means of 1 and 4 and variances of
1 and 0.4 respectively. We then optimized an expected future distri-
bution, which also consisted of two Gaussians with free means and
variancesusingbothanEvidenceandaDivergenceobjective. Ascan
be seen, optimizing the Evidence Objective results in the agent fit-
ting the predicted future density entirely to an extremely sharp peak
around the mode of the desired distribution. Conversely, optimizing
a divergence objective leads to a precise match of the predicted and
desireddistributions(panelBshowsthe twodistributions almostpre-
cisely on top of one another). As a technical note, to be able to see
both the evidence and deisre distributions on the same scale, for the
evidenceobjectivethepredicteddistributionisnormalizedbutthede-
sired distribution is not. Code for these simulations can be found at:
https://github.com/BerenMillidge/origins_information_seeking_exploration.237
6.1 Top: Backpropagation on a chain. Backprop proceeds backwards
sequentially and explicitly computes the gradient at each step on the
chain. Bottom: Predictivecodingonachain. Predictions,andprediction
errorsareupdatedinparallelusingonlylocalinformation. Importantly,
whiletheoriginalcomputationgraph(blacklines)mustbeaDAG,the
augmentedpredictivecodinggraphiscyclic,duetothebackwards(red)
predictionerrorconnections. . . . . . . . . . . . . . . . . . . . . . . 283
√
6.2 Top: Thecomputationgraphofthenonlineartestfunctionv =tan( θv )+
L 0
sin(v2). Bottom: graphsofthelogmeandivergencefromthetruegra-
0
dient and the divergence for different learning rates. Convergence to
theexactgradientsisexponentialandrobusttohighlearningrates. . . 289
xix
6.3 Meandivergencebetweenthetruenumericalandpredictivecodingbackprops
overthe course oftraining. In general, the divergenceappeared tofollowa
largely random walk pattern, and was generally neglible. Importantly, the
divergencedidnotgrowovertimethroughouttraining,implyingthaterrors
fromslightlyincorrectgradientsdidnotappeartocompound. . . . . . . . 293
6.4 TrainingandtestaccuraciesoftheCNNnetworkontheSVHNandCIFAR
datasetsusingthecross-entropyloss. Ascanbeseenperformanceremainsvery
close to backprop, thus demonstrating that our predictive coding algorithm
canbeusedwithdifferentlossfunctions,notjustmean-squared-error. . . . 294
6.5 Test accuracyplotsfor thePredictiveCodingand BackpropRNNand
LSTMon theirrespectivetasks,averagedover5 seeds. Performance is
againindistinguishablefrombackprop. . . . . . . . . . . . . . . . . . 297
6.6 Training losses for the predictive coding and backprop RNN. As ex-
pected,theyareeffectivelyidentical. . . . . . . . . . . . . . . . . . . 298
6.7 Computation graph and backprop learning rules for a single LSTM
cell. InputstotheLSTMcellarethecurrentinputx andtheprevious
t
embedding h . These are then passed through three gates – an input,
t
forget,andoutputgate,beforetheoutputofthewholeLSTMcellcan
becomputed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
6.8 TheLSTMcellcomputationgraphaugmentedwitherrorunits,evincing
the connectivity scheme of the predictive co

Rewrite the summary fixing the issues. Use exact title: Applications of the Free Energy Principle to Machine Learning and Neuroscience
PROFESSIONAL TONE: Begin directly with content - NO conversational openings.
NO REPETITION. Each sentence must be unique. Vary attribution phrases.
Extract quotes VERBATIM from paper text - do NOT modify or "correct" them.