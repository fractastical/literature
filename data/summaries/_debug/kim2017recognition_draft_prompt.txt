=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Recognition Dynamics in the Brain under the Free Energy Principle
Citation Key: kim2017recognition
Authors: Chang Sub Kim

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2017

Key Terms: principle, energy, action, free, least, dynamics, organisms, recognition, brain, under

=== FULL PAPER TEXT ===

8102
naJ
02
]CN.oib-q[
3v81190.0171:viXra
Recognition Dynamics in the Brain under the Free
Energy Principle
Chang Sub Kim
Department of Physics, Chonnam National University, Gwangju 61186,Republic of
Korea
E-mail: cskim@jnu.ac.kr
Abstract. Weformulatethecomputationalprocessesofperceptionintheframework
of the principle of least action by postulating the theoretical action as a time integral
of the free energy in the brain sciences. The free energy principle is accordingly
rephrased as that for autopoietic grounds all viable organisms attempt to minimize
thesensoryuncertaintyabouttheunpredictableenvironmentoveratemporalhorizon.
By varying the informational action, we derive the brain’s recognition dynamics
(RD) which conducts Bayesian filtering of the external causes from noisy sensory
inputs. Consequently, we effectively cast the gradient-descent scheme of minimizing
thefreeenergyintoHamiltonianmechanicsbyaddressingonlypositionsandmomenta
of the organisms’ representations of the causal environment. To manifest the
utility of our theory, we show how the RD may be implemented in a neuronally
based biophysical model at a single-cell level and subsequently in a coarse-grained,
hierarchicalarchitectureofthebrain. WealsopresentformalsolutionstotheRDfora
modelbraininlinearregimeandanalyzethe perceptualtrajectoriesaroundattractors
in neural state space.
Keywords: recognition dynamics, Bayesian filtering, perception, free energy principle,
sensory uncertainty, informational action, principle of least action
Recognition Dynamics in the Brain under the Free Energy Principle 2
1. Introduction
The quest for a universal principle that may explain the cognitive and behavioral
operation of the brain is of great scientific interest at present. The apparent difficulty in
addressing the quest is the gap between the information processing and the biophysics
that governs neurophysiology in the brain. However, it is evident that the matter,
which is the ground-stuff upon which the brain functions emerge, comprises neurons
obeying the laws guided by physics principles. Thus, any biological principles that
attempt toexplainthebrain’slarge-scaleworkings must copewithouraccepted physical
reality [1]. It appears that on the current approaches still prevails the classical, effective
epistemology of regarding perceptions as constructing hypotheses which may hit upon
truth by producing symbolic structures matching physical reality [2, 3, 4].
One influential candidate at present that seeks for such a rubric in neuroscience
is the free energy principle (FEP) [5, 6, 7]. For a technical appraisal of the FEP, we
refer to [8] where the theoretical assumptions and the mathematical structure involved
in the FEP are reviewed in great detail. We have noticed [9] suggesting ‘variational
neuroethology’ which explains, integrating the FEP with evolutionary systems theory,
how living systems appear to resist the second law of thermodynamics. To state
compactly, the FEP offers that all viable organisms perceive and act on the external
world by instantiating a probabilistic causal model embodied in their brain in a way to
ensure their adaptive fitness or autopoiesis [10]. The biological mechanism that endows
the organism’s brain with the operation is theoretically framed into an information-
theoretic measure, which we call ‘informational free energy (IFE)’. According to the
FEP, a living system tries to minimize the sensory surprisal when it faces an external
cause that perturbs its spontaneous equilibrium within its physiological boundary by
pursuing perceptive as well as active inferences. However, the brain does not preside
over instreaming sensory distribution; accordingly, the brain cannot directly minimize
thesensory surprisal but, instead, minimizes itsupper bound, theIFE.The probabilistic
rationaleoftheFEParguesthatthebrain’srepresentationsoftheuncertainenvironment
are the sufficient statistics, e.g., means or variances, of a probability density encoded in
the brain. The variational parameters are supposed to be encoded as physical variables
in the brain. The brain statistically infers the external causes of sensory input by
Bayesian filtering, using its internal top-down model about predicting, or generating,
the sensory data. Filtering is a probabilistic approach to determining the external states
fromnoisymeasurements ofsensorydata[11]. Thereisgrowingexperimental supportfor
thebrain’smaintaining internal modelsoftheenvironment topredict sensory inputsand
to prepare actions, see for instance [12]. The computational operation of the abductive
inference is subserved by the brain variables and the resulting perceptual mechanics is
termed as the ‘recognition dynamics (RD)’.
Although the suggestion of the FEP has been promising to account for the brain’s
inferring mechanism of and acting upon sensory causes, we find certain theoretical
subtleties in the conventional formulation:
Recognition Dynamics in the Brain under the Free Energy Principle 3
First, the FEP minimizes the IFE at each point in time for successive sensory
inputs [13]. However, precisely the objective function to be minimized is the
continuously accumulated IFE over a finite time. The minimization must be
;
performedconcerningtrajectoriesoveratemporalhorizonacrosswhichanorganism
encounters with atypical events to its natural habitat and biology.
Second, the FEP employs the gradient-descent method in practically executing
minimization of the IFE [15], which is widely used in machine learning theory to
solve engineering optimization problems efficiently. The engaged scheme allows
formulation to find heuristically optimal solutions in the FE landscape, but it not
derived from a scientific principle.
Third, the FEP introduces the notion of the ‘generalized coordinates’ of an infinite
number of the so-called ‘generalized motions’ to account for the dynamical nature
of the environment [16]. The ensuing theoretical construct is a generalization of the
standard Newtonian mechanics.§ With the hired theory, however, it is obscure to
decide the number of independent dynamical variables for a complete description.
In practice, typically dynamic truncation is made at a finite embedding order by
assuming that the precision of random fluctuations on higher orders of motion
disappears very quickly.
Fourth, the FEP introduces the hydrodynamics-like concepts of the ‘path of a
mode (motion of expectation)’ and the ‘mode of a path (expected motion)’ by
distinguishing the dynamic update from the temporal update of a time-dependent
state [17]. Because the distinction is essential to ensure an equilibrium solution
to the RD in employing the dynamical generative models, further theoretical
exploration seems worthwhile.
Fifth, the FEP considers the states of the environment ‘hidden’ because what the
brain faces is only a probabilistic sensory mapping. Subsequently, a distinction
is made between the hidden-state representations, responsible for intra-level
dynamics, and causal-state representations, responsible for inter-level dynamics, in
the hierarchical brain [18]. Such a distinction must emerge as neuronal dynamics in
thebrainondifferent timescales. Accordingly, abiophysicallygroundedformulation
that supports the top-down idea is required.
In this paper, we present a mechanical formulation of the RD in the brain in the
; According to the FEP, the updating or learning of the generative model takes places in the brain
on a longer time scale than that associated with perceptual inference. To derive the RD of the slow
variables for synaptic efficacy and gain, the time-integral of the IFE is taken as an objective function;
however, again the gradient descent method is executed in a pointwise way in time [14].
§ The mechanical state of a particle is specified only by position and velocity in the Newtonian
mechanics, and no physicalobservables are assigned to the dynamical orders beyond the second-order.
Insome literature[19], the conceptof‘jerk’isassignedto the third-ordertime-derivativeofpositionas
aphysicalreality. Fromthemathematicalperspective,suchageneralizationisnotforbidden. However,
not only higher-orders are difficult to measure, but more seriously it raises the question of what the
corresponding cause to jerk as the force to acceleration. And, the same impasse in all next orders.
Recognition Dynamics in the Brain under the Free Energy Principle 4
framework of Hamilton’s principle of least action [20]. Motivated by the aforementioned
theoretical observations, we try to resolve some of the technical complexities in the
FEP framework. To be specific, the goal is to recast the gradient-descent strategy of
minimizing the IFE into the mathematical framework that appeals to the normative
physics rules. We do this by hypothesizing the IFE as a Lagrangian of the brain
which enters the theoretical action, being the fundamental objective function to be
minimized in continuous time under the principle of least action. Consequently, we
reformulate the RD regarding only the canonical, physical realities to eschew the
generalized coordinates of infinitely recursive time-derivatives of the continuous states
of the organism’s environment and brain. In the canonical description, the dynamical
state of a system is specified only by positions and their first-order derivatives.
In this work, supported by the present day evidence [22, 23], we admit the bi-
directional facet in informational flow in the brain. The environment begets sensory
dataatthebrain-environment interfacesuch assensory receptorsorinteroceptorswithin
an organism. The incited electro-opto-chemical interaction in sensory neurons must
transduce forward in the anatomical structure of the brain. Whereas complying with
the idea of perception as constructing hypotheses, there must be backward pathway as
well in information processing in the functional hierarchy of the brain. To understand
how such bidirectional functional architecture is emergent from the electrophysiology
of biophysics and anatomical organization of the brain is a forefront research interest
(see, for instance, [24] and references therein). We shall consider a simple model that
effectively incorporates the functional hierarchy while focusing our attention on the
brain’s perceptual mechanics of inferring of the external world, given sensory data. The
problem of learning of the environment via updating the internal model of the world
and of active inference of changing sensory input via action on the external world, see
for instance [21], is deferred for an upcoming paper.
Here, we shall outline howinthis work we cast Bayesian filtering intheFEP using a
variationalprincipleofleastactionandhowwearticulatetheminimizationofthesensory
uncertainty in terms of an associated Lagrangian and Hamiltonian. Furthermore, given
a particular form for the differential equations, afforded by computational neuroscience,
one can see relatively easily how neuronal dynamics could implement the Bayesian
filtering: (i) According to the FEP, the brain represents the environmental features
statistically efficiently, using the sufficient statistics µ. We assume that µ stands for
the basic computational unit of the neural attributes of perception in the brain. Such
a constituent is considered a ‘perceptual particle’ which may be a single neuron or
physically coarse-grained multiple neurons forming a small particle; (ii) We postulate
thattheLaplace-encoded IFEintheFEP, denotedasF (Sec. 2.1), serves asaneffective,
informational Lagrangian(IL) ofthe brain, written asL. Accordingly, theinformational
action (IA), which we denote by S, is defined to be time-integral of the approximate
IFE (Sec. 3.1); (iii) Then, conforming to the Hamilton principle of least action, the
equations of motion of the perceptual particles are derived mathematically by varying
the IA with respect to both µ and µ9 . The resulting Lagrange equations constitute
Recognition Dynamics in the Brain under the Free Energy Principle 5
the perceptual mechanics, i.e., the RD of the brain’s inferring of the external causes
of the sensory stimuli (Sec. 3.1); (iv) In turn, we obtain the brain’s informational
Hamiltonian H from the Lagrangian via a Legendre transformation. Consequently,
we derive a set of coupled, first-order differential equations for µ and its conjugate p ,
µ
which are equivalent to the perceptual mechanics derived from the Lagrange formalism.
The resulting perceptual mechanics is our derived RD in the brain. Accordingly, the
brain performs the RD in the state space spanned by the position µ and momentum
p variables of the constituting neural particles. (Sec. 3.2); (v) We adopt the Hodgkin-
µ
Huxley (H-H) neurons as biophysical neural correlates which form the basic perceptual
units in the brain. We first derive the RD of sensory perception at a single-neuron level
where the membrane potential, ionic transport, and gating are the relevant physical
attributes. Subsequently, we scale up the cellular formulation to furnish a functional
hierarchical-architecture ofthebrain. Onthis coarse-grainedscale, theperceptual states
are the averaged properties of many interacting neurons. We simplify the hierarchical
picture with two averaged, activation and connection variables, mediating the intra- and
inter-level dynamics, respectively. According to our formulation of the hierarchical RD
in the brain, as sensory perturbation comes in at the lowest level, i.e., sensory interface,
the brain carries out the RD in its functional network and finds an optimal trajectory
which minimizes the IA.
To summarize, we have adopted the IFE as an informational Lagrangian of
the brain and subsequently employed the principle of least action to construct the
Hamiltonian mechanics of cognition. In doing so, only positions and momenta of the
neural particles have been addressed as dynamical variables; positions and momenta
are the metaphorical terms for the perceptual states and the brain’s prediction errors,
respectively. We do not distinguish the causal and hidden states, both of which must
emerge as biophysical neuronal activities on different timescales. The resulting RD
is statistically deterministic, arising from unpredictable motions of the environmental
states and noisy sensory mapping. Furthermore, the derived RD describes not only the
temporal development of the brain variables but also the prediction errors. The rate
of the prediction errors is not incorporated in the conventional formulation of the FEP.
The successful solutions of the RD are stable equilibrium trajectories in the neural state
space, specifying the tightest upper bound of the sensory uncertainty, conforming to the
rephrased FEP. Our formulation allows solutions in an analytical form in linear regimes
near fixed points, expanded in terms of the eigenvectors of the Jacobian and, thus,
provides with tractability of real-time analysis. We hope that our theory will motivate
further investigations of some model brains with numerical simulations and also of the
active inference and learning problems.
Thispaperisorganizedasfollows. WefirstrecapitulatetheFEPinSec.2tosupport
our motivation for casting the gradient descent scheme into the standard mechanical
formulation. In the followed Sec. 3 we present the RD reformulated in the Lagrangian
and Hamiltonian formalisms. Then, in Sec. 4 biophysical implementations of our theory
at the cellular level and in the scaled-up hierarchical brain are formulated, where
Recognition Dynamics in the Brain under the Free Energy Principle 6
nonlinear as well as linear dynamical analyses are carried out. Finally, a discussion
is provided in Sec. 5.
2. The free energy principle
To unveil our motivation for this paper, we shall compactly digest here the IFE and
the FEP, that are currently exercised in the brain sciences. The RD is an organism’s
organization of executing minimization of the IFE in the brain under the FEP. In
practice, there are various ways of IFE-minimizing schemes, for instance, variational
message passing and belief propagation, which do not lend themselves to treatment
regarding generalized coordinates of motion. Our treatment in this paper is more
relevant to Bayesian filtering and predictive coding schemes that have become a popular
metaphor for message passing in the brain. Filtering is the problem of determining the
state of a system from noisy measurements [11]. For a detailed technical appraisal of
the FEP, we refer to [8] from which we borrow the mathematical notations.
2.1. The informational free energy
A living organism occupies a finite space and time in the unbounded, changing world
while interacting with the rest of the world, comprising its environment. The states of
the environment are denoted as ϑ collectively, which are ‘hidden’ from the organism’s
perspective. The signals from the environment are registered biophysically at the
organism’s sensory interface as sensory data ϕ.
The organism’s brain faces uncertainty when it tries to predict the sensory inputs,
theamountofwhichisquantifiedasthesensory uncertainty H. Thesensoryuncertainty
isdefined tobeanaverage oftheself-information, lnp ϕ , over theprobability density
´ p q
p ϕ encoded at the interface,
p q
H dϕ lnp ϕ p ϕ . (1)
” t´ p qu p q
ż
The self-information, which is also termed as the sensory ‘surprise’ or ‘surprisal’
in information theory, quantifies the survival tendency of living organisms in the
unpredictable environment. Assuming that the sensory density describes an ergodic
ensemble of sensory streaming , one may convert the sensory uncertainty into a time-
}
average as
1 T
dϕ lnp ϕ p ϕ dt lnp ϕ t ,
t´ p qu p q “ T t´ p p qqu
0
ż ż
where T is the temporal window over which an environmental event takes places, i.e., a
temporal horizon. Here, onemay manipulatethe right-handside(RHS) ofthe preceding
} ThisergodicityassumptionisanessentialingredientoftheFEP,whichhypothesizesthattheensemble
average of the surprisal is equal to the time-average of that, regarding the surprisal as a statistical,
dynamical quantity.
Recognition Dynamics in the Brain under the Free Energy Principle 7
equation by adding a Kullback-Leibler divergence to the integrand to get
q ϑ q ϑ
lnp ϕ dϑq ϑ ln p q dϑq ϑ ln p q .
´ p q` p q p ϑ ϕ Ñ p q p ϑ,ϕ
ż p | q ż p q
The outcome brings about the mathematical definition of the IFE,
q ϑ
F q ϑ ,p ϑ,ϕ dϑq ϑ ln p q , (2)
r p q p qs ” p q p ϑ,ϕ
ż p q
which is expressed as a functional of the two probability densities, q ϑ and p ϑ,ϕ ;
p q p q
whereq ϑ andp ϑ,ϕ aretermedtherecognitiondensity (R-density)andthegenerative
p q p q
density (G-density), respectively. The R-density is the organism’s probabilistic
representation of the external world, which the organism’s brain uses in approximately
inferring the causes ϑ of inputs ϕ. The G-density, a joint probability between ϑ and ϕ,
underlies the top-down model about how the sensory data are biophysically generated
by interaction between the brain and the environment. By construction, the surprisal is
smaller than the IFE by the added positive-amount, accordingly the sensory uncertainty
is bounded from above in accordance with
dt lnp ϕ dtF q ϑ ,p ϑ,ϕ . (3)
r´ p qs ď r p q p qs
ż ż
Note that the sensory uncertainty on the left-hand side (LHS) of Eq. (3) specifies the
accumulated surprisal over a temporal horizon involved in an environmental event.
Equation (3) constitutes a mathematical statement of the FEP: “All viable
organisms try to avoid being placed in an atypical situation in their environmental
habitatsforexistence byminimizing thesensoryuncertainty. However, organismsdonot
possess direct control over the sensory distribution p ϕ ; accordingly they, instead, are
p q
to minimize the upper bound of Eq. (3), dtF as a proxy for the sensory uncertainty.”
The brain conducts the minimization probabilistically by updating the R-density to
ş
approximate the posterior density p ϑ ϕ , namely carrying out Bayesian inference of
p | q
the causes ϑ of the sensory data ϕ. In the conventional application of the FEP the
following approximate inequality is usually exercised [25, 26],
lnp ϕ F. (4)
´ p q ď
However, note that the inequality, Eq. (4) is not equivalent to Eq. (3), in general. It is
only a point approximation piecewise in time.
Here, a difficulty arises because the functional shape of the R-density is not given
a priori. It may be fixed after knowing all orders of its moments of the external states,
which is not possible. To circumvent the difficulty, usually one invokes variational Bayes
by assuming a Gaussian fixed-form for the R-density, the ‘Laplace approximation’,
1
2
q ϑ exp ϑ µ 2ζ N ϑ;µ,ζ , (5)
p q “ ?2πζ ´p ´ q {p q ” p q
(
which is fully characterized simply by its means µ and variances ζ, namely first and
second order sufficient statistics, respectively. Then, by substituting Eq. (5) into Eq. (2)
Recognition Dynamics in the Brain under the Free Energy Principle 8
and after some technical approximations, see [8] for the details, one can convert the IFE
functional F into
F q ϑ ,p ϑ,ϕ lnp µ,ϕ F µ,ϕ . (6)
r p q p qs Ñ ´ p q ” p q
At the end of manipulation the outcome becomes a function of only the means µ, given
the sensory input ϕ; where the dependence of variances has been removed by their
optimal values. The resulting IFE function F in Eq. (6) is termed the ‘Laplace-encoded’
IFE in which the parameters µ, specifying the organism’s belief or expectation of the
environmental states, are the organism’s probabilistic representation of the external
world. In turn, it is argued that the variational parameters µ are encoded in the brain
as biophysical variables.
To proceed with minimization of the IFE in the filtering scheme, a model for noisy-
data measurement and also the equations of motion of the states must be supplied. The
FEP assumes a formal homology between the external dynamics and the organism’s
top-down belief: The former describes, according to physics laws, the equations of
motionof environmental states and the sensory-data registering process. And, the latter
prescribes the internal dynamics of the representations of the environmental states and
the generative model of the sensory data in the organism’s brain [13]. Following this
idea, we hypothesize that the registered data ϕ are predicted by the organism according
to a linear or nonlinear morphism,
ϕ g µ z, (7)
“ p q`
where g µ is a map from µ onto ϕ and z is the involved random fluctuation. Also, the
p q
brain’s representations µ of the causes are assumed to obey the stochastic equation of
motion,
dµ
f µ w (8)
dt “ p q`
where f µ is a linear or nonlinear function of the organism’s expectation of
p q
environmental dynamics and w is the associated random fluctuation.
Assuming mutually uncorrelated Gaussian fluctuations, w and z, of the organism’s
beliefs, one may furnish the models for the likelihood p ϕ µ and the empirical prior
p | q
p µ , which jointly enter the Laplace-encoded IFE in Eq. (6) in the factorized form,
p q
p ϕ,µ p ϕ µ p µ . (9)
p q “ p | q p q
Using the notation introduced in Eq. (5), they are given explicitly as
p ϕ µ N ϕ g µ ;0,σ , (10)
z
p | q “ p ´ p q q
p µ N µ9 f µ ;0,σ , (11)
w
p q “ p ´ p q q
where we have set µ9 dµ dt, and the normal densities are assumed to possess zero
“ {
means with variances σ and σ , respectively. When the fluctuations are statistically
z w
stationary, the variances are handled as constant.; however nonstationarity can also be
taken into account by assuming an explicit time-dependence in the variances. Finally,
Recognition Dynamics in the Brain under the Free Energy Principle 9
by substituting Eqs. (10) and (11) into Eq. (6), one can convert the Laplace-encoded
IFE , up to a constant, into
1 1 1
F µ,ϕ
σ´1
ε
2 σ´1
ε
2
ln σ σ , (12)
p q “ 2 z z ` 2 w w ` 2 p z w q
where the new variables have been defined as
ε ϕ g µ and ε µ9 f µ .
z w
” ´ p q ” ´ p q
The auxiliary variable ε specifies the discrepancy between the sensory data ϕ and the
z
brain’s prediction g µ . Similarly, ε specifies the discrepancy between the change of
w
p q
the environmental representations µ9 and the organism’s belief f µ .
p q
It is straightforward to extend the formulation to the multiple, correlated noisy
inputs. However, for simplicity, we shall continually work in the single-variable picture
and will promote it to the general situation later.
2.2. Gradient descent scheme of the RD
With the Laplace-encoded IFE as an instrumental tool, the organism’s brain
searches for the tightest bound for the surprisal, conforming to Eq. (4), by varying
its internal states µ. The critical question is what machinery the brain hires for the
minimization procedure. Typically the gradient descent method in machine learning
theory is employed in the conventional approach.
To give an idea of the gradient-descent scheme, here we set up a simple gradient-
descent equation, in the usual manner, by regarding the IFE function F as an objective
function as
µ9 κ∇ F. (13)
µ
“ ´
In the above µ9 implies a temporal or sequential update of the brain variable µ and ∇ is
µ
thegradientoperatorwithrespect toµ, andκisthelearning ratethatcontrolsthespeed
of optimization. In steady state, defined by
µ9
0, the solution
µp0q
to the relaxation
”
equation, Eq. (13) must satisfy ∇ F 0. Subsequently, it may be interpreted that
µ
“
such a solution corresponds to an equilibrium (or fixed) point of the IFE function F,
specifying a local minimum in the IFE landscape.
Byinspection, however, wefindthatthegradient-descent construct inthepreceding
way bears an ambiguity when it is applied to dynamic causal models such as Eq. (8).
Thisisbecauseimposingthecondition,
µ9
0ontheLHSofEq.(13), doesnotguarantee
”
a desired equilibrium point in the state space spanned by µ. The reason is that µ9 also
appears on the RHS of Eq. (13) via F: The gradient operation on the RHS of Eq. (13)
can be performed explicitly for given F, Eq. (12) to give
g f
µˆ ∇ F σ´1 ϕ g B σ´1 µ9 f B .
¨ µ “ ´ z p ´ q µ ´ w p ´ q µ
B B
This subtlety does not appear in the conventional theory which incorporates the
nonstationarity, i.e., the aspect of continually changing external states, into formulation
using the mathematical construct of unbounded, higher-order motion of the generalized
Recognition Dynamics in the Brain under the Free Energy Principle 10
coordinates.¶ It is an attempt to allow a more precise specification of a system’s
dynamical state. The generalized coordinates are defined to be a row vector in the
state space spanned by all orders of time-derivatives of bare state µ,
µ˜ µ,µ1,µ2, µ r0s ,µ r1s ,µ r2s , (14)
“ p ¨¨¨q ” p ¨¨¨q
where vector components are defined, with understanding µ µ, as
r0s
”
µ µ1 Dµ .
rn`1s “ rns ” rns
Note that the notationDµ µ1 has been introduced to denote the dynamical update
rns ” rns
ofthe component µ , which isin contrast to thenotationµ9 forthe sequential update.
rns rns
Also, two components of a vector at different dynamical orders in the generalized
coordinates are mutually independent variables. Similarly, the sensory-data ϕ˜ are
expressed in the generalized coordinates as a row vector,
ϕ˜ ϕ,ϕ1,ϕ2, ϕ r0s ,ϕ r1s ,ϕ r2s , . (15)
“ p ¨¨¨q ” p ¨¨¨q
Each component in the vectors, µ˜ and ϕ˜, is to be considered as a dynamically-
independent variable. Also, assuming that the random fluctuations, z and w, are
analytic, they have been written in the generalized coordinates as z˜and w˜, respectively.
Then, the generalization of Eqs. (7) and (8) follows after some technical approximations
as (for details, see [8])
ϕ˜ g˜ z˜, (16)
“ `
Dµ˜ f˜ w˜; (17)
“ `
where Dµ˜ µ1,µ2,µ3, . For reference, we explicitly speel out n, Eqs. (16) and (17)
“ p ¨¨¨q
at dynamical orders as
g
ϕ B µ z ,
rns
“ µ
rns
`
rns
B
f
Dµ B µ w .
rns
“ µ
rns
`
rns
B
Note that different dynamical orders of the noises z˜ and w˜ may be considered
to be statistically correlated in general. Then, the Laplace-encoded IFE can be
mathematically constructed from multivariate correlated Gaussian noises with zero
means and covariance matrices Σ and Σ ,
w z
F µ˜,ϕ˜ 1 µ˜ 9 f˜ Σ´1 µ˜ 9 f˜ T 1 ln Σ
p q “ 2t ´ u w t ´ u ` 2 | w |
1 1
ϕ˜ g˜ Σ´1 ϕ˜ g˜ T ln Σ , (18)
` 2t ´ u z t ´ u ` 2 | z |
¶ The terminologyof the generalizedcoordinatesin generalizedfiltering is dissimilarfrom its common
usage in physics. In classical mechanics, the generalized coordinates refer the independent coordinate
variables which are required to completely specify the configuration of a system with a holonomic
constraint,notincludingtheirtemporalderivatives. Thenumberofgeneralizedcoordinatesdetermines
the degree of freedom in the system [20]. Accordingly, the term, ‘generalized states’ seems better suit
generalized filtering.
Recognition Dynamics in the Brain under the Free Energy Principle 11
where µ˜ 9 f˜ T is the transpose of row vector µ˜ 9 f˜ .; Σ and Σ´1 are the determinant
t ´ u t ´ u | w | w
and the inverse of the covariance matrix Σ , respectively, etc. In many practical
w
exercises, however, usually, the conditional independence among different dynamical
orders is imposed. Consequently, the noise distribution at each dynamical order is
assumed to be an uncorrelated Gaussian density about zero means. This simplification
corresponds to the Wiener process or Markovian approximation [11]. Here, we recall
that the generalized states µ˜ are the means of the brain’s probabilistic model of the
dynamical world, the R-density Eq. (5) after rewritten in the generalized coordinates.
Note Eq. (18) is a direct generalization of Eq. (12).
Furnished with the extra theoretical constructs, the IFE becomes a function of
the generalized coordinates µ˜, given sensory data ϕ˜, F F µ˜,ϕ˜ . Accordingly, the
“ p q
gradient-descent scheme must be extended to incorporate the generalized motions in its
formulation. This is done by the theoretical prescription that the dynamical update Dµ˜
9
is distinctive from the sequential update µ˜. Consequently, one recasts Eq. (13) into the
form,
9
µ˜ Dµ˜ κ∇ µ˜ F µ˜,ϕ˜ . (19)
´ “ ´ p q
Withtherevised gradient-descent equation, theconventional FEPclaims thattheIFEis
minimizedbyreachingadesiredfixedpointµ˜˚ µ˜ t inthegeneralizedstatespace
” p Ñ 8q
spanned by µ˜. This corresponds to the situation when the two rates of the generalized
brain variables, µ˜ 9 and Dµ˜ become coincident, namely µ9 Dµ at every dynamic
rns rns
“
order n. To support the idea it is argued that the purpose of Eq. (19) is to place the
gradient descent in the frame of reference that moves with the mean µ˜ , see [15]. The
entire minimization procedure is compactly expressed in the literature as
µ˜˚ argminF µ˜,ϕ˜ m ,
“ µ˜ p | q
wherewehaveinsertedminF toindicateexplicitly thattheminimizationisconditioned
on the generative model of an organism.
In brief, the brain performs the RD of perceptual inference by biophysically
implementing Eq. (19) in the gray matter. The steady-state solution µ˜˚ specifies
minimum value of the IFE, say F min F µ˜˚,ϕ˜ , giving the tightest bound of the
“ p q
surprisal [see Eq. (4)] associated with a given sensory experience ϕ˜. Despite its frequent
employment in practicing the FEP, we have disclosed some subtleties involved in the
conventional formulation of the gradient descent scheme, which has motivated our
reformulation.
3. The informational action principle
The RD condensed in Sec. 2.2 is based on the mathematical statement Eq. (4) of
the FEP, which is a point approximation of Eq. (3). Here we reformulate the RD by
complying with the full mathematical statement of FEP given in Eq. (3). Accordingly,
weneedaformalismthatallowsminimizationofthetime-integral oftheIFE,notateach
Recognition Dynamics in the Brain under the Free Energy Principle 12
point in time. Wehave come to assimilate that the theoretical ‘action’ inthe principle of
least action neatly serves the goal [20]. This formalism allows us to eschew introduction
of the generalized coordinates of a dynamical state comprising an infinite number of
time-derivatives of the brain state µ. Consequently, not required is the distinctive
classification of time-derivative of the parametric update
(µ9
) and the dynamical update
(Dµ) of the state variable. In what follows, we shall consistently use the dot symbol to
denote time-derivative of a dynamical variable.
3.1. Lagrangian formalism
To formulate the RD from the principle of least action, the ‘Lagrangian’ of the
system must be supplied. We define the informational Lagrangian (IL) of the brain,
denoted by L, as the Laplace-encoded IFE function,
L µ,µ9 ;ϕ F µ,µ9 ;ϕ ,
p q ” p q
where we have placed the semicolon in L to indicate that µ and µ9 are the two brain’s
dynamical variables, given sensory input ϕ. The sensory inputs are stochastic and
time-dependent, in general; ϕ ϕ t , reflecting the changing external states, of which
“ p q
generative processes are to be supplied by physics laws. The proposed IL is not a
physical quantity but an information-theoretic object. When we take Eq. (12) as an
explicit expression for F, the IL is written up as
1 1
L µ,µ9 ;ϕ σ´1 µ9 f µ 2 σ´1 ϕ g µ 2 . (20)
p q “ 2 w p ´ p qq ` 2 z p ´ p qq
1
Note that we have dropped out the term, ln σ σ in writing Eq. (20) by assuming
2 z w
p q
it as a constant, which then does not affect the dynamics of µ and µ9 . This assumption
of statistical nonstationarity may be lifted by introducing time-dependence in the
variances,
σ σ t and σ σ t .
w w z z
“ p q “ p q
Still, however, the dropped-out term does not affect the dynamics because a term that
can be expressed as a total time-derivative in the Lagrangian will not do anything [20].
Next, we postulate that the perceptual dynamics of the neural particles conforms
to the principle of least action [20]. Accordingly, we suppose that the brain’s perceptual
operation corresponds to searching for an optimal dynamical path that minimizes the
informational action (IA), denoted by S,
tf
S dt L µ,µ9 ;ϕ ; (21)
” p q
żti
where t t T is the temporal horizon over which a living organism encounters an
f i
´ ”
environmental event. When functional derivative of S is carried out with respect to µ
and
µ9
, it gives
L tf tf d L L
δS B δµ dt B B δµ.
“ µ ´ dt µ9 ´ µ
„B ti żti ˆ B B ˙
Recognition Dynamics in the Brain under the Free Energy Principle 13
By imposing δS 0 under the condition that initial and final states are fixed,
”
δµ t 0 δµ t ,
i f
p q “ “ p q
we derive the Lagrangian equation as
d L L
B B 0. (22)
dt µ9 ´ µ “
B B
Using the specified Lagrangian, Eq. (20), in Eq. (22), we obtain a Newtonian equation
of motion for the brain variable µ,
σ w ´1 v9 Λ¯ 1 Λ¯ 2 , (23)
“ `
where we have defined the kinematic velocity to be
v µ9
”
and the additional notations on the RHS as
f g
Λ ¯ 1 ” σ w ´1 fB µ and Λ ¯ 2 ” ´ σ z ´1 p ϕ ´ g q B µ . (24)
B B
Equation (23) entails the RD of the brain in the Lagrangian formulation. We interpret
that the inverse of the variance σ´1 plays, as a metaphor, a role of inertial mass of
w
the neural particles. Accordingly, the LHS of Eq. (23) represents an inertial force, i.e.
the product of ‘inertial mass’ and ‘acceleration’,
µ:
. Note that the inverse of variance
is interpreted as precision in the Friston formulation [8], which gives a measure for the
accuracy of the brain’s expectation or prediction of sensory data. So, the precision is the
‘informational mass’ of the neural particle metaphorically. Also, the terms Λ¯ , i 1,2,
i
“
on the RHS are interpreted as the ‘forces’ that drive the internal Λ¯ 1 as well as sensory
Λ¯ 2 excitations in the brain. The acceleration can be evaluated from µ: Λ¯ i σ w ´1 when
“ {
the net force is known.
ř
While the organism’s brain integrates the RD for incoming sensory input, an
optimaltrajectoryµ˚ t iscontinuously achievedintheneural-configurationspace. And,
p q
the steady-state condition in the long-time limit t is given by
Ñ 8
µ9˚ v˚ const, (25)
“ “
wherethenetforcevanishes. Notethatequation(25)defininganattractor,µ µ˚ ,
eq
“ p8q
is more general than the simple guess, µ9˚ 0. The optimal trajectory µ˚ t minimizes
“ p q
the IA, which, in turn, provides the organism with the tightest estimate of the sensory
uncertainty, see equation (3).
3.2. Hamiltonian formalism
The mechanical formulation can be made more modish in terms of Hamiltonian
language which admits position and momentum as independent brain variables, instead
of position and velocity in the Lagrangian formulation. The positions and the momenta
span phase space of a physical system, which defines the neural state space of the
organism’s brain.
Recognition Dynamics in the Brain under the Free Energy Principle 14
The ‘canonical’ momentum p, which is conjugate to the position µ, is defined via
Lagrangian L as [20]
L
p B σ´1 µ9 f , (26)
” µ9 “ w p ´ q
B
which evidently differs from the ‘kinematic’ momentum
σ´1v σ´1µ9
. Then,
w “ w
the informational ‘Hamiltonian’ H may be constructed from the Lagrangian using
Legendre’s transformation [20],
L
H µ,p;ϕ B µ9 L µ,µ9 ;ϕ . (27)
p q “
µ9
´ p q
B
ÿ
The first term on the RHS of Eq. (27) can be further manipulated to give
L
B
µ9 σ´1 µ92 σ´1 µ9f.
µ9 “ w ´ w
B
ÿ
By plugging the outcome and also the Lagrangian L given in Eq. (20) into Eq. (27), we
obtain the Hamiltonian as a function of µ and p as desired, given ϕ,
H µ,p;ϕ T p V µ,p;ϕ , (28)
p q “ p q` p q
where Eq. (26) has been used to replace µ9 with p. The first term on the RHS of Eq. (28)
is the ‘kinetic energy’ which depends only on momentum,
p2
T p . (29)
p q “ 2σ´1
w
Also, the second term on the RHS of Eq. (28) is the ‘potential energy’ which depends
on both position and momentum,
V µ,p;ϕ V µ;ϕ pf µ , (30)
p q “ p q` p q
where we have defined the momentum-independent term separately as V,
1
V µ;ϕ
σ´1
ϕ g
2
. (31)
p q “ ´2 z p ´ q
We remark that the sensory stimuli ϕ enter the Hamiltonian only through the potential-
energy part V which becomes ‘conservative’ when ϕ is static. Here, we shall assume
that the variances associated with the noisy data are constant. For time-varying
sensory inputs, in general, the Hamiltonian is nonautonomous. In Fig. 1 we depict
the conservative potential energy, using three-term approximations for the generative
function,
2
g µ b b µ b µ .
1 2 2
p q « ` `
For convenience, we have assumed a fixed sensory input, ϕ 15, and set parameters as
“
b 1 ,b 2 ,b 3 0,1,0.01 . We have observed numerically that the static sensory signal ϕ
p q “ p q
changes the distance between two unstable fixed points, but do not affect the location
of the stable equilibrium point. Also, the depth of the stable equilibrium valley gets
deeper as the magnitude of ϕ increases.
Recognition Dynamics in the Brain under the Free Energy Principle 15
V(µ; )
µ
-100 -50 50
-5
-10
-15
-20
-25
-30
Figure1. Thepotentialenergy,giveninEq.(31),inarbitraryunits;wherethedashed
andsolidcurves are for the varianceσz “100 and30,respectively. Bothcases exhibit
astableminimuminthe centralwellandtwounstablemaximaonthe sidehills,which
contribute to determining the FE landscape.
Next, we take the total derivative of the Hamiltonian given in Eq. (27) with respect
to µ and µ9 to get
dH µ,p;ϕ d pµ9 dL µ,µ9 ;ϕ
p q “ p q´ p q
ÿ L L
µ9dp pdµ9 B dµ B dµ9
“ ` ´ µ ` µ9
ˆB B ˙
p9 dµ µ9dp.
µ
“ ´ `
By comparing the last expression with the formal expansion,
H H
dH B dµ B dp,
“ µ ` p
B B
we identify the Hamilton equations of motion for independent variables µ and p of a
neural particle,
H
µ9
B (32)
“ p
B
H
p9 B . (33)
“ ´ µ
B
For given H in Eq. (28), we spell out the RHS of Eq. (32) to get
1
µ9 p f (34)
“ σ´1 `
w
which is identical to Eq. (26). Similarly, the second equation, Eq. (33) is spelled out
V f
p9 B B p. (35)
“ ´ µ ´ µ
B B
The first term on the RHS of Eq. (35) specifies the conservative force,
V g
B
σ´1
ϕ g B .
´ µ Ñ ´ z p ´ q µ
B B
Recognition Dynamics in the Brain under the Free Energy Principle 16
Whereas, the second term on the RHS of Eq. (35) specifies the dissipative force, where
f µ plays the role of damping coefficient.
B {B
The derived set of coupled equations for the variables µ and p furnish the RD of the
brain in phase space spanned by µ and p, which involve only first order time-derivatives.
When time-derivative is taken once more for both sides of Eq. (34) with followed
substitution of Eq. (35) for
p9
, the outcome becomes identical to the Lagrangianequation
of motion, Eq. (23). This observation confirms that two mechanical formulations, one
from the Lagrangian and the other from the Hamiltonian, are in fact equivalent.
In the Hamiltonian formulation, the brain’s fulfilling of the RD is equivalent to
finding an optimal trajectory µ˚ t ,p˚ t in phase space. For a static sensory input,
p p q p qq
thedynamics governed by Eqs. (34) and(35) is autonomous, andforthe time-dependent
sensory input it becomes non-autonomous. The RD can be integrated by providing
appropriate models for the generative functions f and g. The attractor µ˚ ,p˚
p p8q p8qq
would be a focus or center in phase space, which can be calculated by simultaneously
imposing the conditions on LHSs of Eqs. (34) and (35) ,
µ9˚ 0 and p9˚ 0. (36)
“ “
One can readily confirm that these fixed-point conditions match with the Newtonian
equilibrium condition, Λ¯ 0 in the Lagrangian formulation, see section 3.1. The
i i “
situation corresponds to the brain’s resting state at a local minimum on the energy
ř
landscape defined by the Hamiltonian function.
3.3. Multivariate formulation
Having established the Hamiltonian dynamics for a single brain variable µ, we now
extend our formulation to the general case of the multivariate brain. We denote µ as
t u
a row vector of N brain states as done in Sec. 2.1,
µ µ ,µ , ,µ ,
1 2 N
t u “ p ¨¨¨ q
that respond to the multiple of sensory inputs in a general way,
ϕ ϕ ,ϕ , ,ϕ .
1 2 N
t u “ p ¨¨¨ q
For simplicity, we neglect the statistical correlation of the fluctuations associated with
environmental variables and also with sensory inputs. Then, within the independent-
particle approximation of uncorrelated brain variables, the Laplace-encoded IFE
Eq. (18) furnishes the multivariate Lagrangian,
N
1
L µ , µ9 ; ϕ σ´1 µ9 f µ 2 σ´1 ϕ g µ 2 ,(37)
pt u t u t uq “ 2 wαp α ´ α pt uqq ` zα p α ´ α pt uqq
α“1
ÿ “ ‰
where we have dropped out the terms which contain only the variances, σ and σ ,
zα wα
assuming that the noises are statistically nonstationary. One may extend Eq. (37) to
interacting neural nodes in terms of covariance matrix formulation [8], which is not
Recognition Dynamics in the Brain under the Free Energy Principle 17
our concern here, either. Subsequently, the conjugate momentum to the generalized
coordinate µ is determined by an explicit evaluation of
α
L
p B σ´1 µ9 f . (38)
α
“
µ9
“
wα
p
α
´
α
q
α
B
Note the momentum p gives a measure of the discrepancy, weighted by the inverse
α
variance σ , between the change of the probabilistic representation of the environment
wα
µ9 and the organism’s belief of it f . The weighting factor σ´1 is called the precision
α α wα
in the FEP. In turn, the Hamiltonian of the multivariate brain can be constructed from
Eq. (28) as
H µ , p ; ϕ T µ , p ; ϕ V µ , p ; ϕ (39)
pt u t u t uq “ pt u t u t uq` pt u t u t uq
where first term on the RHS is the kinetic energy,
p 2
α
T p ; ϕ (40)
pt u t uq ” 2σ´1
α wα
ÿ
and the potential energy V is identified as
1
V µ , p ; ϕ
σ´1
ϕ g
2
p f . (41)
pt u t u t uq ” ´2 zα p α ´ α q ` α α
α „ 
ÿ
Then, it is straightforward to derive the RD of the variables µ and p , given sensory
α α
data ϕ , as
α
H 1
µ9 B p f , (42)
α “ p “ σ´1 α ` α
B α wα
and for their conjugate momenta,
H g f
p9 B B α p B α p . (43)
α ϕα α
“ ´ µ “ ´ µ ´ µ
α α α
B B B
In writing equation (43), for notational convenience we have introduced an auxiliary
quantity p ,
ϕα
p
σ´1
ϕ g .
ϕα zα α α
” p ´ q
Equations (42) and (43) are a coupled set of equations for the computational units,
µ and p , describing the brain states and their conjugate momenta, respectively, given
α α
the sensory discrepancy p between the observed data ϕ and their predictions g µ .
ϕα α α α
p q
With some working models for f and g , they shape the RD in the brain’s multi-
α α
dimensional phase space in the Hamiltonian prescription.
In Fig. 2 we present a schematic illustration of the perceptual circuitry implied by
the RDat a neural node. The classification ofexcitatory andinhibitory activation of the
computational units is not absolute because the overall sign depends on the generative
functionf andmapg , whicharenotspecified. Itisadmissibletoassumethatthebrain
α α
is, at the outset, in a resting state. As the sensory inputs ϕ come in, the organism’s
α
brain performs the RD online, by integrating Eqs. (42) and (43), to attain an optimal
trajectory in neural phase space,
µ µ˚ t and p p˚ t ,
α “ αp q α “ αp q
Recognition Dynamics in the Brain under the Free Energy Principle 18
μ p
f(μ) α α ∂ f(μ)
μ
∂ g(μ)
g(μ) μ
φ p
α φα
Figure 2. Theperceptualcircuitryatneuralnodeαinwhichsensorydataϕα stream;
where it is depicted that the computational units µα and pα are positively activated
by arrows and negatively by lines ended with filled dots. The conjugate momenta pα,
defined in Eq. (38), to the brain variables µα mimic the precision-weightedprediction
errors in the language of predictive coding [27].
which minimize the IA, see Eq. (21). The entire minimization procedure may be stated
abstractly as
µ˚,p˚ arg min S µ ,p ;ϕ m , (44)
p α αq “ µα,pα p α α | q
where S is the IA and m has been inserted to indicate explicitly that minimization is
conditioned on the organism as a model of the world.
Note that in our revised RD is involved not only the organism’s prediction of the
environmental change via its representation µ but also the dynamics of its prediction-
α
error p .
α
4. Biophysical implementation
Weknowthattheanatomyandentirefunctionsofanorganism’sbraindevelopfrom
single cells. In order to provide empirical Bayes in the FEP with a solid biophysical
basis, we must start with known biophysical substrates and then introduce probabilities
to describe a neuron, neurons, and a network. Until now, however, most work has
taken the reverse direction: Theory prescribes first a conjectural model and then tries
to allocate possible neural correlates. At present, our knowledge remains limited on how
biophysical mechanisms of neurons implicate predictions and model aspects about the
environment, while a ‘neurocentric’ approach to the inference problem seems suggestive
to bridge the gap [28, 29].
Here, we regard coarse-grained Hodgkin-Huxley (H-H) neurons asthe generic, basic
building-blocks of encoding and transmitting a perceptual message in the brain. The
famous H-H model continues to be used to this day in computational neuroscience
studies of neuronal dynamics [30, 31]. In extracellular electrical recordings, the local
field potential and multi-unit activity result in as combined signals from a population
Recognition Dynamics in the Brain under the Free Energy Principle 19
of neurons [32]. Such averaged neuronal variables must subserve the perceptual states
and conduct the cognitive computation in the brain. We shall call them ‘small’ neural
particles and envisage that a small neural particle enacts a node that collectively forms
thewholeneural network onalargescale. Beforeproceeding, weshallmentionthatthere
are many biophysical efforts to describe such averaged neuronal properties; for instance,
the neural mass models and neural field theories are a few examples [33, 34, 35, 36, 37].
Also, we note the bottom-upeffort of trying to understand the large-scale brain function
at the cortical microcircuit level based on the averaged, spikes and synaptic inputs over
a coarse-grained time interval [38, 39].
4.1. Single cell description
We first present how our formulation may be implemented at a single-cell level by
hypothesizing that each neuron reflects the fundamentals of the perceptual computation
of the whole system. A typical neuron receives current information about its
surroundings from the sensory periphery via glutamate, which excites or inhibits the
membrane potential V with regulating the gating variables γ and ionic concentrations
l
n ; where l is the ion channel index. We assume that V, n , γ specify the neural
l l l
p t u t uq
states of a neuron as a neural observer in the neural configurational space [29]. We
encapsulate the neural states as components in a multi-dimensional row vector,
µ V, n , γ µ ,µ ,µ , .
l l 1 2 3
t u “ p t u t uq “ p ¨¨¨q
The H-H equation for excitation of the membrane voltage V in a spatially
homogeneous cell is given by
dV
C γ G E V I t (45)
l l l ex
dt “ p ´ q` p q
l
ÿ
where C is the membrane capacitance, G is the maximal conductance of ion channel l,
l
γ is the probability factor associated with opening or closing channel l which in general
l
a product of activation and inactivation gating variables, and I is the external driving
ex
current. For simplicity, contributions from leakage current as well as synaptic input are
assumed to be included in the external currents. The reverse potential E of l-th ion
l
channel is given, allowing its time-dependence via nonequilibrium ion-concentrations, in
general, as
k T n t
B li
E t ln p q, (46)
l
p q “ q n t
l lo
p q
where k is the Boltzmann constant, T is the metabolic temperature of an organism,
B
q is the ionic charge of channel l, and n t and n t are the instantaneous ion
l li lo
p q p q
concentrations inside and outside the membrane, respectively. In the steady state
without external current, I 0, V tends to the resting (Nernst) potential V t
ex
“ p Ñ 8q
with retaining ionic concentrations in electro-chemical equilibrium. The gating variable
Recognition Dynamics in the Brain under the Free Energy Principle 20
γ of ion channels is assumed to obey the kinetics, a different model of that may be
l
preferable,
dγ 1
l
γ γ η , (47)
l leq l
dt “ ´τ p ´ q`
l
where the relaxation time τ and steady-state gating variable γ depend on the
l leq
membrane potential, in general,
τ τ V and γ γ V ,
l l leq leq
“ p q “ p q
and η is the noise involved in the process.
l
For ionic concentration dynamics, we suppose that ion concentrations n
l
t u
vary slowly compared to the membrane potential and gating-channel kinetics, and
consequently treat them statically in our work. This restriction can be lifted when
a more detailed description is required for ion concentration dynamics. Accordingly, the
reverse potentials E are also treated statically in below.
l
Then, the state equations for the multivariate neural vaiable µ neatly map onto
t u
the standard form suggested in the FEP,
dµ
α
f V, γ , n w t , (48)
α l l α
dt “ p t u t uq` p q
where α runs 1,2, with implying µ 1 V, µ 2 γ 1, and µ 3 γ 2, etc. The driving
¨¨¨ “ “ “
functions f , that are specified to be
α
1 1
f V, γ ; n γ G E V I , (49)
V l l l l l ex
p t u t uq “ C p ´ q` C
l
ÿ
1
f V, γ ; n γ γ . (50)
γlp
t
l
u t
l
uq “ ´ τ p
l
´
leq
q
l
The terms w in Eq. (48) describe the noisy synaptic and/or leakage current w flowing
α V
into the neural cell, not the deterministic contribution I which is included in f ,
ex V
and the noise w η associated with the activation and inactivation of ion channels,
γl
“
l
respectively. For both noises, we assume the Gaussian distributions N µ9 f ;0,σ
α α wα
p ´ q
with variances σ about zero means.
wα
Regarding neuronal response to the sensory stimulus ϕ , we adopt the usual
α
generative map in the FEP [see Eq. (7)] as
ϕ g V, γ , n z , (51)
α α l l α
“ p t u t uq`
where g is the generative map that is unknown but must be supplied for practical
α
application and z characterizes the stochastic nature of the sensory reading which
α
we assume the normal distribution N ϕ g ;0,σ . With the present model, we
α α zα
p ´ q
admit that the neural observer responds to the sensory data instantly by means of the
neuronal states. Currently, we do not possess a firm ground on biophysical processes of
the sensory prediction.
As a working example, here we consider a H-H neuron which allows fast relaxation,
i.e. τ 1, of gating variables to their steady-states, γ t γ γ V . In this
l l l leq
! p q Ñ p8q “ p q
case our neural particle is fully characterized by a single dynamical variable of V. Note
Recognition Dynamics in the Brain under the Free Energy Principle 21
the time-dependence of the gating variables occurs only implicitly through the long-
time membrane voltages in Eq. (49). Then, the RD of our neural particle is fulfilled
in a two-dimensional state space spanned by µ V,p µ,p , prescribed by the
V
t u “ p q ” p q
Hamiltonian function, equation (28),
p2
1
H µ,p
σ´1
ϕ g
2
pf.
p q “ 2σ´1 ´ 2 z p ´ q `
w
While the ‘dissipative’ function f is explicitly given in the H-H model as
1
f µ γ µ G E µ I C, (52)
leq l l ex
p q “ C p q p ´ q` {
l
ÿ
the ‘conservative’ function g must be additionally supplied. Also, one needs to
make the voltage-dependence of γ available in practice. Note the Hamiltonian is
leq
nonautonomous, in general, because it explicitly depends on time through both the
sensory input ϕ t and the driving current I in f, and also through σ t , σ t when
ex w z
p q p q p q
the noisy data are statistically nonstationary.
Figure 3. Hamiltonian function Hpµ,pq in arbitrary units for the chosen set of
parameters given in the main text; where the black curve on the energy landscape
is the trajectory which is calculated by solving the Hamilton equations of motion for
aninitialconditionatpµ,pq“p2.5,´5.0q. [Forinterpretationofthereferencestocolor
in this figure, the reader is referred to the web version of this article.]
In Fig. 3 we present the energy landscape described by the Hamiltonian function,
assuming static sensory data, constant driving currents, and statistical stationarity.
Since our knowledge is limited to the functional form of g µ and f µ , we have taken
p q p q
the algebraic approximations [40],
2
g µ a a µ a µ ,
0 1 2
p q « ` `
Recognition Dynamics in the Brain under the Free Energy Principle 22
2 3
f µ b b µ b µ b µ .
0 1 2 3
p q « ` ` `
For numerical purposes, we have specified a 0 ,a 1 ,a 2 0,1,1 and b 0 ,b 1 ,b 2 ,b 3
p q “ p q p q “
0,0.1,1,1 , and also assumed a fixed sensory input with equal masses (precisions ) on
p q
the brain’s internal model and belief of sensory prediction as
ϕ 1.0 and
σ´1
0.1
σ´1
.
“ w “ “ z
The Hamilton equations of motion, Eqs. (42) and (43), bring about the nonlinear
RD as
µ9 Λ1 µ,p;t , (53)
“ p q
p9 Λ2 µ,p;t , (54)
“ p q
where the ‘force’ functions Λ1 and Λ2 are specified as
1
Λ1
“
f
p
µ
q` σ´1
p, (55)
w
g f
Λ2
“ ´
σ
z
´1
p
ϕ
´
g
q
B
µ ´
B
µ
p. (56)
B B
We have chosen an initial state and solved the equations of motion, for the same
parameters used in Fig. 3, to obtain a trajectory in phase space. The outcome is drawn
on the energy surface in Fig. 3. According to the present model, the neural observer
performs the RD, given the sensory input ϕ and, consequently, obtains the optimal
trajectory µ˚,p˚ conforming to Eq. (44). In the long-time limit the brain will reach a
p q
fixed (equilibrium) point µ˚ ,p˚ in the state space, that is specified by intersections
p eq eqq
of two isoclines,
Λ µ,p; 0, i 1,2.
i
p 8q “ “
We have determined the fixed points numerically. It turns out that there exist three
real solutions for the specified system parameters, 1.23,0.05 , 0.50, 0.01 , and
p´ q p´ ´ q
0.07, 0.04 , which are depicted as the blue dots in Fig. 4. By further analysis, we
p ´ q
have found that only the middle point is a stable equilibrium solution and the other
two specify saddle points. Figure 4 shows a flow of trajectories, obtained from arbitrary
initial points on the red-colored circle of radius µ2 p2 1.6, in phase space.
` “
To gain an insight into how the system approaches to a steady state, we inspect
the optimal trajectories near an equilibrium point,
µ˚ µ˚ δµ˚ and p˚ p˚ δp˚.
eq eq
« ` « `
We expand Eqs. (53) and (54) to the linear order in the deviations δµ˚ and δp˚ and,
after rearrangement, obtain the normal form,
d δµ˚ R R δµ˚
11 12
0. (57)
dt ˜ δp˚ ¸ ` ˜ R 21 R 22 ¸˜ δp˚ ¸ “
Recognition Dynamics in the Brain under the Free Energy Principle 23
p
2
1
µ
-3 -1.5 1.5 3
-1
-2
Figure 4. Optimal trajectories in phase space which are obtained by integrating the
RD, equations (53) and (53), fromthe initial conditions arbitrarilychosenon the red-
colored circle; where the blue dots are the equilibrium points among which only the
middle dot at p´0.50,´0.01q is a stable fixed point, and other two points are saddle
points. The stable fixed point turns out to be a center, which we have confirmed by
linear stability analysis and also numerically. [For interpretation of the references to
color in this figure, the reader is referred to the web version of this article.]
In Eq. (57) the elements of the relaxation (Jacobian) matrix R are specified to be
f 1
R 11 B , R 12
“ ´ µ “ ´σ´1
„B eq w
g 2 2g 2f
R 21 “ σ z
´1
´ B µ `p ϕ ´ g q B µ2 ´ B µ2 p ,
« ff
ˆB ˙ B B eq
f
R 22 B ;
“ µ
„B eq
where the partial derivatives are to be evaluated at the equilibrium points. Here, for
notational convention we denote the column vector as
δµ˚
δψ .
” δp˚
˜ ¸
Then, the formal solution to Eq. (57) is written as
δψ t
e´Rtδψ
0 .
p q “ p q
One may expand the initial state ψ 0 in terms of the eigenvectors of R as
p q
δψ 0 c φ ,
α α
p q “
ÿ
Recognition Dynamics in the Brain under the Free Energy Principle 24
where the eigenvalues λ and eigenvectors φ are determined by the secular equation,
α α
Rφ λ φ .
α α α
“
Consequently, the solutions to the linear RD at a single node level is completed as
2
δψ t c e´λαtφ , (58)
α α
p q “
α“1
ÿ
where the expansion coefficients c are fixed by the initial condition.
α
In the linear regime, a geometrical interpretation of the equilibrium solutions is
possible by inspecting the eigenvalues of the Jacobian matrix R. Considering that the
matrixRisnotsymmetric, weanticipatethattheeigenvalues arenotreal. Furthermore,
because the trace of the relaxation matrix equals zero, the sum of the two eigenvalues
must be zero. Thus, when the determinant of R is positive, the two eigenvalues λ 1 and
λ 2 would be pure imaginary with opposite sign. Consequently, in the present particular
model, the resulting equilibrium point is likely to be a center. We have confirmed
numerically that the eigenvalues of the Jacobian corresponding to the stable equilibrium
point in figure 4 meet the condition for a center.
4.2. The hierarchical neural network
Here, we suppose that there are a finite number of levels inthe perceptual hierarchy
of the whole system and that for simplicity each level is characterized efficiently as a
single neural node. Further, we assume that the neural node at hierarchical level i is
described by the coarse-grained, activation and connection variables, denoted as Vpiq
and Spiq, respectively. The activation variable describes action potential at a node, and
the connection variable describes inter-level synaptic input and output variables. Both
variables are derived from a population of neurons and thus vary on a coarse-grained
space and time scale. The technical details of how one may derive such a coarse-graining
description are not our scope, for a reference see [37]. They form the coordinates in
brain’s configurational space,
µpiq Vpiq,Spiq ,
“ p q
where the superscript runs i 1,2, ,M, with M denoting the highest level.
“ ¨¨¨
We assume that the activation variables Vpiq obey the effective dynamics with noise
wpiq within each hierarchical level i,
dVpiq
fpiq Vpiq,Spiq wpiq, (59)
dt “ p q`
which is a direct generalization of Eq. (48) with incorporating the hierarchical
dependence via Spiq. For inter-level dynamics, we propose that the connection variables
are updated by one-level higher connetion as well as activation variables, subjected to
the stochastic equations,
dSpiq
gpi`1q Vpi`1q,Spi`1q zpiq, (60)
dt “ p q`
Recognition Dynamics in the Brain under the Free Energy Principle 25
where zpiq represents the noise associated with the process. The brain’s top-down
prediction functions fpiq and gpiq must be supplied in practical implementation. Note
there is only spontaneous fluctuation at the top cortical level, i M, accordingly
“
gpM`1q
0. (61)
“
Also, we constrain that the sensory data ϕ enter the interface (or boundary between)
of the brain and the environment specified as the lowest hierarchicl level, i 1.
“
Subsequently we assume that the brain’s prediction of the sensory inputs is performed
by way of an instantaneous mapping,
Sp0q gp1q Vp1q,Sp1q zp0q,
(62)
“ p q`
where, for notational convenience, we have set
Sp0q
ϕ t .
” p q
We remark that the hierarchical equations Eq. (60) we propose is dissimilar to the
conventional formulation which assumes the static model in the entire hierarchy like the
one Eq. (62) at the sensory interface, see [8]. We treat here the connection variables
dynamically not statically to treat lateral and hierarchical dynamics symmetrically.
The rates of the activation and connection variables may be subjected to different time-
scales, thatcanbeincorporated, forinstance, byintroducingdistinctive relaxation-times
in their generative functions. It turns out that our equations suit the formalism of the
Hamilton action principle neatly.
Having specified our hierarchical model, we write the informational Lagrangian for
the constructed neural network by generalizing Eq. (37) with a single sensory input for
now, as
M M
9 9 1 2 1 2
L V,V;S,S;ϕ mpiq εpiq mpiq εpiq , (63)
p q “ 2 w w ` 2 z z
i“1 i“0
ÿ ` ˘ ÿ ` ˘
where mpiq and mpiq are the inertia masses, associated with the Gaussian noises, wpiq
w z
and zpiq, respectively, defined to be
mpiq 1 σpiq and mpiq 1 σpiq. (64)
w ” { w z ” { z
The auxiliary variables in the Lagrangian are defined to be (i 1)
ě
9
εpiq Vpiq fpiq Vpiq,Spiq , (65)
w
” ´
εpiq S 9 piq gpi`1q Vpi`1q,Spi`1q . (66)
z ” ´ ` ˘
We interpret that
εpiq
specifies`the discrepa˘ncy between the change in the present
w
lateral state and the brain’s on-level prediction, which may be considered as the lateral
prediction-error. On the other hand,
εpiq
measures the prediction error between the
z
change in the present hierarchical state and its prediction from one higher level via the
generative map g, which may be viewed as the hierarchical prediction-error. Note
εp0q
z
in the second term on the RHS of Eq. (63) is defined separately as
εp0q Sp0q gp1q Vp1q,Sp1q
,
z ” ´
` ˘
Recognition Dynamics in the Brain under the Free Energy Principle 26
which specifies an error estimation in sensory prediction at the lowest hierarchical level.
The generalized momenta, conjugate to Vpiq and Spiq, are readily calculated for
i 1, respectively, as
ě
L
ppiq B mpiqεpiq, (67)
V ” V 9 piq “ w w
B
L
ppiq B mpiqεpiq, (68)
S ” S 9 piq “ z z
B
Note that the inverse variances
mpiq
,
mpiq
have been termed the informational masses,
w z
see discussion below equation (24). The role of the inertial masses is to modulate the
discrepancy between the change of the perceptual states and their prediction. Thus, in
our theory, momentum
ppiq
is a measure of lateral prediction-error modulated by inertial
V
mass
mpiq
, and momentum
ppiq
is a measure of hierarchical prediction-error modulated
w S
by inertial mass
mpiq
. And, the heavier the mass is, the bigger the precision becomes.
z
Given the Lagrangian Eq. (63), we can formulate the informational Hamiltonian
by performing a Legendre transformation,
H V 9 piqppiq S 9 piqppiq L.
“ V ` S ´
i
ÿ´ ¯
After some manipulation, we obtain the outcome as
M
H V,p ;S,p ;ϕ T piq Vpiq , (69)
V S
p q “ `
i“1
ÿ` ˘
where the infromational kinetic energy is defined to be (i 1)
ě
1 2 1 2
Tpiq p ,p ppiq ppiq , (70)
p V S q “ 2mpiq V ` 2mpiq S
w z
´ ¯ ´ ¯
and the potential energy to be (i 2)
ě
Vpiq V,p ;S,p ;ϕ ppiqfpiq ppiqgpi`1q. (71)
p V S q ” V ` S
Note the potential energy at the lowest level is specified separately to be
1 2
Vp1q pp1qfp1q pp0q
; (72)
“ V ´ 2mp0q S
z
´ ¯
where, for notational convention, we have written the weighted prediction-error
associated with the sensory measurement as
pp0q mp0qεp0q mp0q
ϕ
gp1q
,
S ” z z “ z p ´ q
which, unlike ppiq for i 1, is not a canonical momentum. Consequently, the multi-level
S ě
Hamiltonian Eq. (69) has been prescribed via the perceptual states in the hierarchical
chain, i 1,2, ,M, denoted as a four dimensional column vector ψpiq at each level,
“ ¨¨¨
ψpiq Vpiq,ppiq,Spiq,ppiq T ψpiq,ψpiq,ψpiq,ψpiq T ,
“ p V S q ” p 1 2 3 4 q
where T means the transverse operation.
Next, it is straightforward to generate the Hamiltonian equations of motion for the
brain’s perceptual states ψpiq. The results are the coupled differential equations for the
Recognition Dynamics in the Brain under the Free Energy Principle 27
four computational components at each level (i 1), which are, in turn, hierarchically-
ě
connected among adjacent levels:
V 9 piq B H 1 ppiq fpiq, (73)
“ ppiq “ mpiq V `
B V w
S 9 piq B H 1 ppiq gpi`1q, (74)
“ ppiq “ mpiq S `
B S z
H fpiq gpiq
p9piq
B B
ppiq
B
ppi´1q
; (75)
V “ ´ Vpiq “ ´ Vpiq V ´ Vpiq S
B B B
H fpiq gpiq
p9piq
B B
ppiq
B
ppi´1q.
(76)
S “ ´ Spiq “ ´ Spiq V ´ Spiq S
B B B
According to the derived RD, the sensory inputs ϕ enter the brain-environment
interface at the level j 1, which are instantly predicted by the organism’s lowest-
level generative model g “ p1q Vp1q,Sp1q . Subsequently, the resulting prediction-error pp0q
acts as a source to updat
p
e the pre
q
diction-errors,
pp1q
and
pp1q
. The change of o
S
n-
V S
level perceptual states
Vp1q
and
Sp1q
are predicted by the generative models
fp1q
and
gp2q
with additional modulations from the perceptual momenta
pp1q
and
pp1q
, being the
V S
lateral and hierarchical prediction-errors, respectively. At higher levels i 2, the intra-
ě
level dynamics of the activation state Vpiq is updated, through Eq. (73), by the on-
level generative function fpiq and prediction error ppiq , while the change of the current
V
hierarchical stateSpiq isdetermined, throughEq. (74), by theinter-level prediction gpi`1q
andtheon-level predictionerror
ppiq
. Theorganism’stop-downmessage flowismediated
S
by the connection state Spiq via Eq. (74) as Spi`1q,Vpi`1q Spiq. And, Eqs. (75), and
p q Ñ
(76) govern the coupled, bottom-up propagation of the prediction errors, mediated by
ppiq
,
ppiq ppi`1q,ppi`1q
. In Fig. 5 we draw a diagram that schematically illustrates
S S Ñ p S V q
the perceptual architecture of the hierarchical network, at lowest two levels, implied by
Eqs. (73)-(76).
Here, we emphasize that the dynamics of precision-weighted prediction errors,
encapsulated in canonical momenta in which mass takes over the role of precision, are
takenintoaccountinourHamiltonianformulationonanequalfootingwiththedynamics
of prediction of the state variables. This aspect is also in contrast to the conventional
minimization algorithm which entails differential equations only for the update of the
brain states without carrying parallel ones for the prediction errors. Consequently, the
message passing in our model shows different features in the details compared with the
neural circuitry from the conventional RD [41]. However, the general message flow, in
terms of the computational units, of feedforward, feedback, and lateral connections hold
the same in the hierarchical brain network. We recognize an attempt to incorporate the
brain’s computation of prediction errors in the FEP can be found in a recent tutorial
model [42].
Here, for mathematical compactness, we rewrite the filtering algorithm, Eqs. (73)-
(76), as
dψpiq
α Λpiq ψpiq , (77)
dt “ α pt α uq
Recognition Dynamics in the Brain under the Free Energy Principle 28
S(2) V(2) p(2) p(2)
S V
S(1) V(1) p(1) p(1)
S V
φ p(0)
S
Figure 5. A schematic of the neural circuitry which conducts the RD Eqs. (73)-
(76) in the hierarchical network of the brain; where the computational units
pSpiq,Vpiq,ppiq,ppiqq,
i“1,2,¨¨¨,M, are connected by arrows for excitatory (positive)
S V
inputs and by lines ended with filled dots for inhibitory (negative) inputs. Note that
the prediction error
pp0q
of incoming sensory data ϕ, at the lowest level, induces an
S
inhibitory change in the perceptual momenta
ppp1q,pp1qq.
Subsequently, the prediction
S V
error propagates up in the hierarchy,
pp1q
Ñ
ppp2q,pp2qq,
etc. On the other hand, the
S S V
top-down message passing is mediated by means of the connection states Spiq. For
instance, the connection state Sp1q is top-down predicted by both units pSp2q,Vp2qq
from one higher-level.
where the hierarchical index i runs from 1 to M, α runs from 1 to 4, and the force
function Λ
piq
is the corresponding RHS to each vector component
ψpiq
at cortical level
α α
i. The obtained hierarchical equations are the highlight of our theory, prescribing the
RD of the brain’s sensory inference under the FEP framework.
To apply our formulation to an empirical brain, one needs to supply the generating
function of lateral dynamics fpiq and the hierarchical connecting function gpiq, that
piq
enter the force functions Λ in the perceptual mechanics, Eq. (77). For the generating
α
function we once again use the H-H model Eq. (49) to write
fpiq Vpiq,Spiq γ G˜ E Vpiq G˜ Spiq E Vpiq , (78)
leq l l S S
p q “ ´ ` ´
l
ÿ ` ˘ ` ˘
where G˜ are the channel conductances normalized by the capacitance C. And, the
l
second term on the RHS accounts for other deterministic driving sources such as leakage
and/or lateral synaptic currents with G˜ being the normalized synaptic conductance.
S
The hierarchical connection function, for which we have limited biophysical knowledge,
shall be taken in a simple form here as
gpiq Vpiq,Spiq Γ Vpiq Spiq, (79)
p q “ p q
Recognition Dynamics in the Brain under the Free Energy Principle 29
where the function Γ specify the voltage-dependent synaptic plasticity from hierarchical
level ito level i 1. Inaddition, asinthe single-nodecase, onemust supply approximate
´
models for voltage-dependence of the gating variables γ and the connection strength
leq
Γ. For instance, one may take the quadratic approximations [40],
γ Vpiq b b Vpiq b VpiqVpiq,
leq l0 l1 l2
p q « ` `
Γ Vpiq a 0 a 1 Vpiq a 2 VpiqVpiq.
p q « ` `
Having laid down the lateral and hierarchical generative models, the organism’s
brain can now perform the RD given a streaming of noisy inputs. While conducting the
filtering, an optimal trajectory is obtained in multi-dimensional phase space,
ψ˚piq ψ˚piq t ,
α “ α p q
which, in the end, tends to a fixed point, ψpiq ψ˚piq t . The necessary
α,eq α
“ p Ñ 8q
equilibrium condition to Eq. (77) is specified by
Λpiq ψpiq 0. (80)
α pt α uq “
Although the full time-dependent solutions must be invoked numerically, one may
inspect the perceptual trajectories near a fixed point by linear analysis. To this end, we
consider a small deviation of αth component of the perceptual state vector ψ˚piq at the
α
cortical level i, δψpiq , from the fixed point ψpiq ,
α α,eq
ψ˚piq ψpiq δψpiq.
α « α0 ` α
Then, we expand Eq. (77) about the fixed point to linear order in the small deviation,
and after some manipulation we get the hierarchical equations for
δψpiq
,
α
dδψpiq 4 4 M
α Rpiqδψpiq
Φ
pijqδψpjq
; (81)
dt ` αβ β “ αβ β
β“1 β“1j‰i
ÿ ÿ ÿ
where the αβ component of the 4 4 Jacobian matrix at cortical level i is specified by
ˆ
piq
Λ
Rpiq B α ,
αβ “
«
ψpiq
ff
B β eq
and the inter-level connection between level i and level j in the hierarchical pathway is
specified by
piq
Λ
pijq α
Φ B ;
αβ “
«
ψpjq
ff
B β eq
where the subscript eq indicates that the matrix elements are to be evaluated at the
equilibrium points. To cast the inhomogeneous term into a more suggestive form we
further inspect it in detail within the models specified: We observe first that the matrix
elements Φ pijq do not vanish only for α 3 because only the force function Λ piq possesses
αβ “ 3
ψpjq for j i as variables via gpi`1q [see Eq. (74)]. Second, because gpi`1q depends solely
β ‰
on the hierarchical level-index i 1, only matrix elements with the hierarchical index
`
j i 1 survives. Combining these two observations, the source term on the RHS of
“ `
Recognition Dynamics in the Brain under the Free Energy Principle 30
Eq. (81) is converted into a vector at level i 1 with only a single nonvanishing α 3
` “
component,
4 M
Φ
pijqδψpjq δζpi`1q
αβ β ” α
β“1j‰i
ÿ ÿ
which to be complete we spell out explicitly as
gpi`1q gpi`1q
δζ α
pi`1q
“ δ α3 $ « B ψ 1 pi`1q ff δψ 1
pi`1q
` « B ψ 3 pi`1q ff δψ 3
pi`1q
, , (82)
& B eq B eq .
where δ α3 is the Kronecker
%
delta.
-
Finally, we shall present a formal solution to the linearized perceptual mechanics
Eq. (81), that can be obtained by a direct integration with respect to time. The result
takes the form
t
δψpiq t e´Rpiqtδψpiq 0 dt1e´Rpiqpt´t1qδζpi`1q t1 . (83)
p q “ p q` p q
0
ż
We next solve the eigenvalue problem at each hierarchical level, which is defined to be
Rpiqφpiq λpiqφpiq, (84)
α α α
“
where λpiq and φpiq are the eigenvalues and corresponding eigenvectors at level i,
α α
respectively. Then, we expand the initial state δψpiq 0 in terms of the complete
p q
eigenvectors:
δψpiq 0 apiqφpiq. (85)
p q “ α α
Similarly, we may expandÿthe inhomogeneous vector δζpi`1q as
δζpi`1q t1 bpi`1q t1 φpiq, (86)
p q “ α p q α
where note the expansion
ÿ
coefficients
bpi`1q
are time-dependent. By substituting the
α
expansions Eqs. (85) and (86) into Eq. (83) we obtain the desired formal solution near
equilibrium points,
4 4 t
δψpiq t a e´λα piqtφpiq φpiq dt1e´λp α iqpt´t1qbpi`1q t1 . (87)
p q “ α α ` α α p q
α“1 α“1 ż 0
ÿ ÿ
The geometrical approach to a fixed point is again determined by the eigenvalues
λpiq
;
α
however, the details are driven by the time-dependent generative sources
bpi`1q
t from
α
p q
one-level higher in the hierarchy.
To sum, responding to sensory streaming ϕ Sp0q, at the lowest hierarchical level
“
(i 1), the brain in an initial resting state performs the hierarchical RD by integrating
“
Eq. (77) to infer the external causes. The ensuing brain’s computation corresponds
to minimizing the IA, which is an upper bound of the sensory uncertainty, whose
mathematical statement, equation (3), is repeated compactly as
H p ϕ S F;ϕ ,
r p qs ď r s
Recognition Dynamics in the Brain under the Free Energy Principle 31
where the sensory uncertainty H was defined in Eq. (1) and the IA on the RSH
is expressed here in terms of the hierarchical states as S F;ϕ dtF ψpiq ;ϕ .
α
r s “ pt u q
Conforming to the FEP, the minimum value of IA specifies the tightest bound of the
ş
sensory uncertainty over a relevant biological time-scale, which preserves the organism’s
current model of the environment.
5. Discussion
We have recast the FEP following the principles of mechanics, which articulates
that all living organisms are evolutionally self-organized to tend to minimize the sensory
uncertainty about uninhabitable environmental encounters. The sensory uncertainty is
an average of the surprisal over the sensory density registered on the brain-environment
interface, the sensory surprisal being the self-information of the sensory probability
density. The FEP suggests that the organisms implement the minimization by calling
forth the IFE in the brain. The time-integral of the IFE gives an estimate of the
upper bound of the sensory uncertainty. We have enunciated that the minimization of
the IFE must continually take place over a finite temporal horizon of an organism’s
unfolding environmental event. Our scheme is a generalization of the conventional
theory which approximates minimization of the IFE at each point in time when it
performs the gradient descent. The sensory uncertainty is an information-theoretical
Shannon entropy [43]; however, in this work, we have circumvented the term, ‘sensory
entropy’ to call the sensory uncertainty. The reason is that ‘minimization of the
sensory entropy’ is reminiscent of Erwin Schro¨dinger’s word, ‘negative entropy’ which
carries a disputable connotation in implying how the living organism avoids decay. He
subsequently suggested FE instead as a more appropriate notion in the context [1].
Conforming to the second law of thermodynamics, the organism’s adaptive fitness of
minimizing the sensory uncertainty must contribute to increasing the total entropy of
the brain and its environment.
We have adopted the Laplace-encoded IFE as an informational Lagrangian in
implementing the FEP under the principle of least action. And, by subscribing to
the standard Newtonian dynamics, we have considered the IFE a function of position
andvelocity asthe metaphorsfor theorganism’s brainvariable andtheir first-order time
derivative, respectively, in the continuous-time picture. The brain variable maps onto
the first-order sufficient statistics of the recognition density launched in the organism’s
brain to perform the RD, Bayesian filtering of the noisy sensory data. In the following
Hamiltonian formulation, the RD prescribes momentum, conjugate to a position, as a
mechanical measure of prediction error weighted by mass, the precision. The theoretical
construct of generalized coordinates introduced in the prevailing theory to specify the
extended states of higher-orders of motion has been eschewed in our formulation. The
features of changing world enter our theory via the sensory inputs in continuous time,
and the statistical nonstationarity of the noise is embedded in time-dependence of the
variances of the Gaussian fluctuations in the organism’s belief of the changing state
Recognition Dynamics in the Brain under the Free Energy Principle 32
and sensory generation. The temporal correlations of the dynamical states may be
incorporated as time-dependent covariances, but not explored in this work. Also, in
our theory all the parameters in the RD are specified in the Hamiltonian; thus, there
require no extra parameters like learning rates in the gradient descent scheme to control
the speed of convergence to a steady state. According to our formulation, the brain’s
Helmholtzian perception corresponds to finding anoptimal trajectory in the hierarchical
functional network by minimizing the IA. When the braincompletes the RD by reaching
a desired fixed-point or a limit cycle, it remains resting, i.e., being spontaneous, until
another sensory stimulus will come in.
We have applied our formalism to a biophysically grounded model for hierarchical
neural dynamics by suggesting that the large-scale architecture of the brain be an
emergent coarse-graineddescription oftheinteracting singleneurons. Wehave admitted
the asymmetric top-down rationale of sensory inference in our formalism, which is an
essential facet of the FEP: The sensory inputs at the interface, the lowest hierarchical
level, were assumed to be instantaneously mapped onto the organism’s belief, encoded
in the brain variables, about environmental causes with associated noise. Differently,
however, from the instant model at the lowest level, we have generalized that the
inter-level filtering in the brain’s functional hierarchy obeys the stochastic dynamics,
supplied with the organism’s dynamical generative model of environmental states. The
resulting RD is deterministic and hierarchical, which notably incorporates dynamics
of both predictions and prediction errors of the perceptual states on an equal footing.
Consequently, the details of the ensuing neural circuitry from our formulation differs
fromtheonesupportedbythegradient-descent schemewhichgeneratesonlydynamicsof
the causal and hidden states, not their prediction errors. However, the general structure
of message passing, namely descending predictions and ascending prediction-errors in
the hierarchical network, shows the same. Also, our obtained RD tenably underpins the
causality: For a specified set of the perceptual positions and corresponding momenta,
at the outset, responding to sensory inputs that may be slow or fast time-dependent,
the RD can be integrated online. The arbitrariness involved in deciding the number of
generalized coordinates for a complete description and also the ambiguity in specifying
unknowable initial conditions can be averted.
In short, it is still a long way to understanding how the Bayesian FEP in
neurosciences may be made congruous with the biophysical reality of the brain. It
is far from clear how the organism embodies the generative model of the environment
in the physical brain. Our theory only delivers a hybrid of the biologically plausible
information-theoretic framework of the FEP and the mechanical formulation of the
RD under the principle of least action. To borrow what Hopfield puts in words, “it
lies somewhere between a model of neurobiology and a metaphor for how the brain
computes” [44]. We hope that our effort will guide a step forward to unraveling the
challenging problem.
Recognition Dynamics in the Brain under the Free Energy Principle 33
References
[1] Schr¨odinger,E.(1967).WhatisLife? MindandMatter.Cambridge: CambridgeUniversityPress.
[2] von Helmholtz, H. (1962). Trietise on physiological optics. Vol. III. Mineola: Dover Publication,
Inc.
[3] Gregory, R. L. (1980). Perceptions as hypoheses. Philosophical Transactions of the Royal Society
of London B. 290: 181-197.
[4] Dayan, P., Hinton, G. E., Neal, R. M., & Zemel, R. S. (1995). The Helmholtz machine. Neural
Computation. 7: 889–904.
[5] Friston, K. (2009). The free-energy principle: a rough guide to the brain? Trends in Cognitive
Science. 13: 293–301.
[6] Friston, K. (2010). The free-energy principle: a unified brain theory? Nature Reviews.
Neuroscience. 11: 127-138.
[7] Friston, K. (2013). Life as we know it. Journal of Royal Society Interface. 10: 20130475.
[8] Buckley, C. L., Kim, C. S., McGregor, S., & Seth, A. K. (2017). The free energy principle for
actionandperception: Amathematicalreview,JournalofMathematicalPsychology.81: 55-79.
http://dx.doi.org/10.1016/j.jmp.2017.09.004.
[9] Ramstead, M. J. D., Badcock, P. B., & Friston, K. J. (2017).
https://doi.org/10.1016/j.plrev.2017.09.001.
[10] Maturana, H., & Varela, F. (1980). Autopoiesis and Cognition: The Realization of the Living.
Boston: D. Reidel.
[11] Jazwinski, A. H. (1970). Stochastic Process and Filtering Theory. New York: Academic Press.
[12] Berkes, P., Orban, G., Lengyel, M., & Fiser, J. (2011). Spontaneous cortical activity reveals
hallmakrs of an optimal internal model of the environment. Science. 331: 83-87.
[13] Friston,K.J.,Daunizeau,J.,Kilner,J.,&Kiebel,S.J.(2010).Actionandbehavior: afreeenergy
formulation. Biological Cybernetics. 102(3): 227-260.
[14] Friston, K. J., & Stephan, K. E. (2007). Free-energy and the brain. Synthese. 159: 417-458.
[15] Friston. K., Stephan, K., Li, B., & Daunnizeau, J. (2010). Generalized Filtering. Mathematical
Problems in Engineering. 261670.
[16] Friston, K. J. (2008). Variational Filtering. NeuroImage. 41: 747-766.
[17] Friston,K.(2008).Hierarchicalmodelsinthebrain.PLoSComputationalBiology.4(11): e1000211.
[18] Friston, K. (2006). A free energy principle for the brain. Journal of Physiology-Paris.100: 70-87.
[19] Sprott,J.C.(1997).Somesimplechaoticjerkfuncitons.AmericalJournalofPhysics.65: 537-543.
[20] Landau, L. P. (1998) Classical Mechanics. (2nd ed.). New York: Springer-Verlag.
[21] Friston, K. J., Daunizeau, J., & Kiebel, S. J. (2009). Reinforcement learning or active inference?.
PLoS One. 4(7): e6421.
[22] Markov, N. T., Vezoli, J., Chameau, P., Falchier, A., Quilodran, R., Huissoud, C., Lamy, C.,
Misery, P., Giroud, P., Ullman, S., Barone, P., Dehay, C., Knoblauch, K., & Kennedy, H.
(2014). Anatomy of hierarchy: Feedforward and feedback pathways in Macaque visual cortx.
Journal of Comparative Neurology. 522:225-259.
[23] Michalareas, G., Vezoli, J., van Pelt, S., Schoffelen, J.-M. & Kennedy, H. (2016). Alpha-beta
andgammarhythmssubservefeedbackandfeedforwardinflunecesamonghumanvisualcortical
areas. Neuron. 89:384-397.
[24] Markov,N. T., & Kennedy, H. (2013). Current Opinion in Neurobiology.23:187-194.
[25] Friston, K. & Kiebel, S. (2009). Cortical circuits for perceptual inference. Neural Networks. 22:
1093-1104.
[26] Friston,K.,Adams, R.A., Perrinet,L.,& Breakspear,M. (2012).Frontiersin Psychology.3: 151.
[27] Rao, R. P. N. & Ballard, D. H. (1999). Predictive coding in the visual cortex: a functional
interpretation of some extra-classicalreceptive-field effects. Natue neuroscience 2(1): 79-87.
[28] Fiorillo, C. D. (2008). Towards a general theory of neural computation based on prediction by
single neurons. PLoS One. 3(10).
Recognition Dynamics in the Brain under the Free Energy Principle 34
[29] Fiorillo, C. D., Kim, J. K., & Hong, S. Z. (2014). The meaning of spikes from the neuron’s
point of view: predivtive homeostatis generates the appearance of randomness. Frontiers in
Computational Neuroscience. 8:49.
[30] Hodgkin, A., & Huxley, A. (1952). A quantitative description of membrane current and its
application to conduction and excitation in nerve. Journla of Physiology,. 117:500-544.
[31] Hille,B.(2001).IonChannelsofExcitableMembranes.(3rded.).Sunderland: SinauerAssociates,
Inc.
[32] Einevoll,G.T.,Kayser,C.,Logothetis,N.K.,&Panzeri,S.(2013).NatureReviews.Neuroscience.
14:770.
[33] Jansen, B. H., Zouridakis, G., & Brandt, E. (1993). A neurophysiologically-based mathematical
model of flash visual potentials. BiologicalCybernetics. 68: 275-283.
[34] Jirsa, V. K., & Haken, H. (1996). Field theory of electromagnetic brain activity. Physical Review
Letter. 77: 960-963.
[35] Robinson, P. A., Rennie, C. J., & Wright, J. J. (1997). Propagation and stability of waves of
electrical activity in the cerebral cortex. Physical Review E. 56: 826-840.
[36] David, O., Friston, K. J. (2003). A neural mass model for MEG/EEG: coupling and neuronal
dynamics. Neuroimage. 20: 1743-1755.
[37] Deco, G., Jirsa, V. K., Robinson, P. A., Breakspear, M., & Friston, K. (2008). The dyamic brain:
From spiking neurons to neural masses and cortical fields. PLoS Compt. Biol. 4: e1000092.
[38] Potjans, T. C., & Diesmann, M. (2014). The Cell-type specific cortical microcircuit: Relating
structure and activity in a full-scale spiking network model. Cerebral Cortex. 24(3):785-806.
[39] Steyn-Ross, M. L., & Steyn-Ross, D. A. (2016). From individual spiking neurons to population
behavior: Systematic elimination of short-wavelength spatial models. Physical Review E. 93:
022402.
[40] Wilson,H.R.(1999).Simplifieddynamicsofhumanandmammaianneocorticalneurons.J.Theor.
Biol. 200: 375-388.
[41] Bastos, A. M., Usrey, W. M., Adams, R. A., Mangun, G. R., Fries P.,& Friston, K. J. (2012).
Canonical microcircuits for predictive coding. Neuron 76: 695-711.
[42] Bogacz,R.(2017).Atutorialonthe free-energyframeworkformodellingperceptionandlearning.
Journal of mathematical psychology,76 (B): 198–211.
[43] Shannon, C. E.(1948).A mathematicaltheory ofcommunication. BellSystem Technical Journal.
27: 379-423,623-656.
[44] Hopfield, J. J. (1999). Brain, neural network, and computation. Review of modeern physics. 71:
S431-S437.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Recognition Dynamics in the Brain under the Free Energy Principle"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
