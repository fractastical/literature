=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: On the Variational Costs of Changing Our Minds
Citation Key: hyland2025variational
Authors: David Hyland, Mahault Albarracin

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Key Terms: minds, david, belief, oxford, variational, mahault, beliefs, information, verses, changing

=== FULL PAPER TEXT ===

On the Variational Costs of Changing Our Minds
David Hyland1 and Mahault Albarracin2
1 University of Oxford, Oxford, United Kingdom
2 VERSES AI Research Lab, Los Angeles, CA 90016, USA
david.hyland@cs.ox.ac.uk
mahault.albarracin@verses.ai
Abstract. The human mind is capable of extraordinary achievements,
yet it often appears to work against itself. It actively defends its cher-
ished beliefs even in the face of contradictory evidence, conveniently in-
terprets information to conform to desired narratives, and selectively
searches for or avoids information to suit its various purposes. Despite
thesebehavioursdeviatingfromcommonnormativestandardsforbelief
updating, we argue that such ‘biases’ are not inherently cognitive flaws,
but rather an adaptive response to the significant pragmatic and cogni-
tive costs associated with revising one’s beliefs. This paper introduces a
formalframeworkthataimstomodeltheinfluenceofthesecostsonour
belief updating mechanisms.
Wetreatbeliefupdatingasamotivatedvariationaldecision,whereagents
weightheperceived‘utility’ofabeliefagainsttheinformationalcostre-
quired to adopt a new belief state, quantified by the Kullback-Leibler
divergence from the prior to the variational posterior. We perform com-
putationalexperimentstodemonstratethatsimpleinstantiationsofthis
resource-rational model can be used to qualitatively emulate common-
placehumanbehaviours,includingconfirmationbiasandattitudepolar-
isation.Indoingso,wesuggestthatthisframeworkmakesstepstoward
a more holistic account of the motivated Bayesian mechanics of belief
change and provides practical insights for predicting, compensating for,
and correcting deviations from desired belief updating processes.
Keywords: Belief Change · Motivated Reasoning · Active Inference ·
Cognitive Effort
“[H]uman reason is both biased and lazy. Biased because it overwhelmingly finds
justifications and arguments that support the reasoner’s point of view, lazy be-
cause reason makes little effort to assess the quality of the justifications and
arguments it produces.” (Mercier and Sperber, 2017, p. 9) [35].
1 Introduction
Humanity faces an increasingly paradoxical epistemic problem. Never before
have people been able to obtain so much information so quickly, yet at the
same time, many societies have become increasingly polarised and paralysed
5202
peS
22
]IA.sc[
1v75971.9052:viXra
2 D. Hyland and M. Albarracin
by conflicting narratives. A feature that is common to several manifestations
of this predicament, including public health crises and conspiracy theorising, is
the presence of actors who tenaciously defend beliefs long after the balance of
evidence has shifted. What, exactly, makes changing our minds so hard?
A natural approach to answering this question may begin by supposing a nor-
mative standard or benchmark against which to compare the actual processes
ofhumanbeliefchange.Theprimarynormativemodelforrationalbeliefupdat-
ing is Bayesian reasoning. According to this model, probabilistic beliefs should
be adjusted proportionally to the strength of evidence according to Bayes’ rule.
However, the persistent discrepancy between the Bayesian standard and actual
human belief updating raises questions about whether our epistemic processes
are inherently irrational or if something is missing from the traditional pic-
ture [34].
The Bayesian paradigm largely remains silent on why humans fall short of its
ideals, primarily due to its assumptions that belief-revision is cost-free and that
the driver of epistemic processes should be probabilistic coherency [26]. In prac-
tice, revising one’s beliefs incurs metabolic costs, cognitive effort, and crucially,
pragmaticrisksandopportunities.Ascientistretractingacherishedhypothesis,
a politician breaking ranks with their party, or a public figure admitting error
each pay tangible costs that a cost-free Bayesian calculus does not account for.
Withoutaprincipledwaytomodeltheeffectofsuchcostsonbeliefrevision,ap-
parent“irrationalities” includingconfirmationbias[36],motivatedreasoning[29],
attitudepolarisation[33],andbeliefpersistence[15]seemlikefundamentalflaws
in human cognition.
We argue that such apparent deviations from Bayesian norms are adaptive re-
sponses of agents operating under motivational considerations and real resource
constraints.Inparticular,weformalisecognitivebelief-changecostsusingtheKL
divergence to quantify informational distances between belief states, represent-
ing the ‘informational work’ required for belief state transitions. Our approach
also integrates social and pragmatic factors. Beliefs are influenced by identity,
social status, and interpersonal dynamics; fears of ostracism or admitting errors
can increase resistance to change. Our hope is that through such modelling, we
can take steps toward developing general frameworks that explain not just iso-
lated sources of non-Bayesian belief updating, but also the inherent trade-offs
between competing considerations associated with changing our minds.
1.1 Contributions and Paper Structure
Ourprimarycontributionistheproposalofavariationalcostfunctionalforbelief
revision that models the influence of pragmatic affordances and cognitive costs
on human belief updating. Secondly, we present results from simplified com-
putational experiments demonstrating how varying conservatism and likelihood
weightingparametersqualitativelyexhibitphenomenasuchasconfirmationbias,
On the Variational Costs of Changing Our Minds 3
evidence search asymmetries, and attitude polarisation. Finally, we discuss the
implications of our model and promising future directions.
2 Related Literature
“There is considerable evidence that people are more likely to arrive at conclu-
sions that they want to arrive at, but their ability to do so is constrained by
theirabilitytoconstructseeminglyreasonablejustificationsfortheseconclusions.
(Kunda, 1990) [29].
2.1 Decision-Theoretic Models of Belief Updating
Drawingonframeworksofdecisionmaking,humanbeliefrevisionisincreasingly
being treated as a value-based decision [28,47,51]. On this view, beliefs are
updated not purely based on their accuracy, but are associated with a utility.
Theutilityofholdingabeliefisderivedfromtheoutcomesitleadsto,whichcan
be internal (emotional comfort, positive feelings) or external (acceptance within
a community, job opportunities) [51]. This is supported by several arguments
highlighting the centrality of affect in decision-making and belief-updating [13,
27,50].Certainbeliefsmaygiverisetoutilityinproportiontohowwelltheytrack
orpredictreality,inwhichcase,thereisanincentivefortheagenttoseektruthful
beliefs. Other beliefs may demand that the agent confabulates an elaborate yet
tenuousnarrativethatcoincidentallysupportstheirdesiredconclusion.Inother
words, a belief’s usefulness can be orthogonal to its truthfulness.
2.2 Cognitive Costs
Inadditiontothepragmaticincentivesthatshapebeliefupdating,thereareun-
avoidablecoststhatanyagentmustpaytochangetheirbeliefs.Thesecostshave
beenstudiedfromtheperspectiveofbounded/resource/computationalrational-
ity,wherethepresenceofsomeformofcostassociatedwithcognitionisexplicitly
modelled in an agent’s decision-making [21,30,31,39–41,54,64]. Belief updating
can be understood as a thermodynamic process involving transitions between
mental states, where each transition incurs unavoidable dissipative costs [17].
These costs arise from fundamental physical principles governing information
processinginbiologicalsystemsatthelevelofneuralcomputationandmetabolic
energy expenditure [62,63].
The transition between belief states involves both work-like and heat-like com-
ponents.Thework-likecomponentcorrespondstothedirectedshiftofthebelief
state,whileheatdissipationoccursintheformofentropyproductionduringthe
transitions between belief states in finite amounts of time [3]. The total dissi-
pation produced by belief updating can be quantified as the difference between
the reversible work theoretically possible and the actual work captured during
the transition process. This represents the unavoidable cost of finite-time belief
changes [43].
4 D. Hyland and M. Albarracin
This thermodynamic perspective helps to explain why, other considerations be-
ing equal, rapid belief changes tend to be more costly and inefficient compared
togradualupdates.Thesystemmustbalancethespeedofbeliefrevisionagainst
theincreaseddissipativecostsofrapidchange.Thisintuitioncanbemademore
precise using concepts from finite-time thermodynamics [3]. The total entropy
productioninasequenceofstep-equilibrationsisboundedby∆Su ≥ L2,where
2K
L is the thermodynamic length of the belief change pathway and K is the num-
berofintermediateequilibrationsteps[48].Thus,increasingthenumberofsteps
decreasesthelowerboundontotalentropyproduction,permittingmoreefficient
pathways of belief change. The brain appears to possess several remarkable fea-
turesthataidinminimisingthesecosts.Forinstance,theefficientcodinghypoth-
esissuggeststhatneuralrepresentationsofsensoryinformationarestructuredto
minimise the number of neuronal spikes required to transmit a given signal [4].
Understanding these fundamental thermodynamic constraints provides insight
into why belief change can be so difficult even in the presence of contradictory
evidence.Thebrainmustcarefullybalancetheenergeticandinformationalcosts
ofupdatingagainstthepotentialbenefits.Itisunclearpreciselyhowsignificantly
the thermodynamic costs of belief change contribute to this effect, and it would
be worthwhile empirically investigating how such considerations can contribute
to and explain belief inertia.
2.3 Social Costs
Humanbeliefsservenotonlyasinternalmodelsoftheworldbutalsoassocialsig-
nals and commitments. In active inference and variational learning frameworks,
agents update beliefs to minimise surprise or prediction error, yet these updates
occur in a social context where beliefs fulfil both epistemic (truth-seeking) and
social-coordinationfunctions[2,6,9,35,57,58,61].Believing(ordisbelieving)cer-
tain propositions can grant individuals emotional comfort or group acceptance,
independent of the belief’s accuracy [51]. This dual role means that an agent’s
posterior after observing new evidence is not determined by epistemic consid-
erations alone, but also by the expected social and personal utilities associated
with holding particular beliefs [1,22,29,51]. Consequently, standard Bayesian
updates, which are focused purely on data and prior likelihood, are often tem-
pered by an additional motive: to align with valued identities and norms that
confer utility on the belief state [9,22,35,61]. This insight echoes the idea that
all thinking is “wishful” thinking to some extent, with motivational imperatives
modulating inferential processes [28]. The free energy minimised during belief
updatingthusimplicitlyincludesnotjustaccuracy-related(surprisal)termsbut
also pragmatic terms capturing the work required to overcome cognitive inertia
and social repercussions [2,8].
Changing one’s mind can threaten group affiliations and invite real or perceived
socialsanctions(e.g.,lossofstatus,trust,ormembership)[6].Beliefsoftenfunc-
tion as markers of group identity, so revising a key belief may signal disloyalty
On the Variational Costs of Changing Our Minds 5
or value misalignment, incurring social costs like ostracism or ridicule. Antici-
pation of such costs creates a strong deterrent to belief revision, especially for
identity-linked beliefs maintained by tight-knit communities and normative ex-
pectations [2,35]. Indeed, social norms enforce a kind of epistemic conformity:
individualsinternalizetheexpectationthatthey“ought” toholdcertainbeliefsto
remain in good standing [6,22]. From a decision-analytic perspective, the utility
ofabeliefthereforeincludesnotonlyitstruth-trackingbenefitsbutalsoitssocial
payoff. A false or unfounded belief might persist if it brings social acceptance or
emotional relief, whereas a truthful belief might be resisted if it carries stigma
or existential dread. Accordingly, belief change in social contexts resembles a
form of motivated reasoning: agents are inclined to arrive at the conclusions
they want (or need) to reach, as long as they can justify them to themselves
and others [29]. Here, “wants” are not arbitrary whims but structured by social
identityandnormativepressures—whatonewantstobelieveisoftenwhatone’s
group wants one to believe. An agent will unconsciously search for justifications
toretainbeliefsthatservevaluedsocialgoals(e.g.solidarity,consistency,pride)
and discount evidence that threatens those goals. The free energy landscape is
warped by social potential energy. Certain directions of belief change appear
steep (costly) due to the interpersonal consequences associated with them.
Empirical research supports these principles. For instance, people consistently
overestimate the severity of the social sanctions they will face for changing a
politically charged belief, leading to excessive self-censorship and public confor-
mity [55]. In one set of their studies, U.S. partisans expected far more backlash
from their in-group if they voiced a dissenting opinion than what actually ma-
terialised, with an average overestimation effect size of d≈0.87. These inflated
expectations of ostracism or punishment (sometimes stemming from an egocen-
tric bias in social perspective-taking) make belief revision seem riskier than it
truly is. Accordingly, individuals often stick to publicly defending their prior
attitudes,evenwhenprivatelygrapplingwithcontraryevidence.Socialpsychol-
ogists refer to this pattern as identity-protective cognition, wherein reasoning
processes bend to protect the agent from the social identity costs of admitting
error. The effect can become self-reinforcing. If everyone fears speaking up or
changing their mind, the apparent unanimity of belief within the group remains
unchallenged, further raising the perceived cost of dissent. Yet research also
shows that these perceived social costs are malleable. Prompting individuals to
reflect on their past loyalty and contributions to the group can reassure them
that a change of mind will not irrevocably brand them as “disloyal,” thereby
significantly reducing their concern about sanction and encouraging more open
expression of revised beliefs.
Beliefsaremulti-functionalcognitivetoolsthatbalanceaccuracy,utility,andin-
ertia. They must at once represent the world (epistemic accuracy), support our
emotionalneedsandmoralvalues,andcoordinatewithoursocialmilieu(utility),
allwhileminimisingdrasticrevisionsthatincurcognitiveandsocial“work” (iner-
tia). This perspective prepares us to interpret classic phenomena—confirmation
6 D. Hyland and M. Albarracin
bias,selectiveexposuretoinformation,andattitudepolarisation,notasinexpli-
cable failures of rationality, but as strategic trade-offs given the agent’s objec-
tives.Anagentfacinghighcostsforbeliefchangewillrationallyexhibitakindof
conservatism. Agents will favour information that confirms existing beliefs and
avoids provoking costly updates. Indeed, a confirmation bias in information-
seeking can be seen as an adaptive strategy to preserve high-utility beliefs by
selectively attending to congruent evidence and filtering out challenges. Experi-
mentalstudiesofselectiveexposuredocumentthatpeoplespendmoretimewith
news and arguments that align with their preexisting attitudes than with those
that contradict them, even when source credibility is controlled [60]. By skim-
ming “friendly” evidence, individuals reduce the likelihood of encountering data
thatwoulddemandpainfulsocialreadjustmentsorinternalvalueconflicts.Simi-
larly,communitiesmaybecomepolarisedwheneachside’sbeliefscarrytheirown
social rewards—members of opposing groups double down on group-consistent
narratives, bolstering internal cohesion at the expense of cross-group accuracy.
Over time, this self-reinforcing selection and interpretation of evidence drives
groupattitudesfurtherapart,aseachgrouplivesinabubblewheremaintaining
their version of reality is pragmatically advantageous [2]. The following sections
will explore how confirmation bias in evidence appraisal, asymmetrical informa-
tion search, and polarisation dynamics emerge naturally once we acknowledge
that changing one’s mind is not “free.” It incurs variational costs, paid in both
cognitive effort and social capital, which a resource-rational mind navigates by
carefully weighing when belief change is truly worth the price.
2.4 Confirmation Bias
Confirmation bias manifests through selective attention mechanisms, as shown
in recent experimental work [46,56]. Westerwick et al. demonstrated that when
selectingpoliticalinformationonline,participantsspentmoretimewithcontent
matchingtheirexistingviews,regardlessofsourcequality[60].Thisbiasemerged
from participants’ choices rather than the content itself. Building on this, [56]
revealed that making a categorical choice selectively enhanced sensitivity to
subsequent evidence consistent with that choice, similar to attentional cueing
effects. [46] proposed a neural mechanism for this bias, suggesting that choices
direct feature-based attention to amplify processing of choice-consistent sensory
evidencewhilesuppressinginconsistentinformation.Together,thesefindingsin-
dicatethatconfirmationbiasoperatesthroughearlyattentionalselectionrather
than solely in later-stage decision processes.
2.5 Motivated Reasoning
Confirmationbiasisatypeofmotivatedreasoning,aprocesswhereinformation
processing is biased toward achieving desired outcomes rather than accuracy
alone [29]. Motivations can be accuracy-driven, encouraging unbiased reason-
ing, or directional, prompting strategies that reinforce existing beliefs, identity,
or preferred conclusions. However, motivated reasoning remains constrained by
On the Variational Costs of Changing Our Minds 7
plausibility; people select cognitive processes, such as memory retrieval and in-
terpretation,thatjustifyfavouredconclusionsratherthaninventingimplausible
beliefs [14,24,44].
Individuals revise beliefs asymmetrically, giving more weight to confirmatory
or emotionally favourable evidence than to equally informative negative evi-
dence [32]. Motivated reasoners also selectively trust or avoid sources based on
alignment with their views, effectively assigning lower reliability to disconfirm-
ing information. This acts like a biased Bayesian filter, reducing the impact of
contradictory evidence on belief updating [45].
Consequently, shared evidence can polarise rather than unify groups with op-
posing and even similar priors. When interpreting balanced evidence through a
biased lens, individuals’ initial beliefs often become more extreme, exacerbating
attitude polarisation [2,5].
2.6 Biased Reasoning
Two recent frameworks have been proposed to explain some of the biases that
occur in reasoning: coherence-based reasoning (CBR) [53] and belief-consistent
information-processing (BCIP) [37]. Coherence-based reasoning posits that in-
dividuals strive to maintain a consistent and interconnected set of beliefs, min-
imisingcognitivedissonance.Morespecifically,inCBR,aconstraint-satisfaction
networksettlesintoanattractorbybidirectionallyreshapingbothbeliefsandin-
coming information to maximise overall coherence. Crucially, strongly activated
priors are harder to dislodge, so they often anchor the attractor state. Simi-
larly, belief-consistent information processing describes the tendency to favour
information that aligns with pre-existing beliefs, a process that is less cogni-
tivelydemandingthanevaluatingandintegratingcontradictoryevidence.BCIP
is a special case of CBR’s coherence construction under conditions of dominant
priors [38].
These two frameworks can be reconciled with our account through the lens
of cognitive economy. Our variational cost framework formalises this principle
by suggesting that altering one’s beliefs incurs a cognitive cost, quantified by
the KL divergence, which measures the informational distance between prior
and posterior beliefs. Moreover, the weight of other firmly held beliefs can be
explained by the presence of costs associated with revising more strongly held
beliefs.Forexample,theexpectedcostassociatedwithmodifyingafundamental
beliefsuchas“Imakecorrectassessmentsoftheworld” wouldbeincreasedlevels
ofdoubtaboutthereliabilityofone’sassessments,potentiallyleadingtogreater
general levels of uncertainty and the accompanying negative affect that often
arises.
In this sense, both coherence-based reasoning and belief-consistent information
processing can be viewed as cognitive strategies that minimise the costs that we
describe.Bymaintainingcoherenceandselectivelyprocessinginformation,indi-
viduals reduce the “informational work” required to update their mental models
8 D. Hyland and M. Albarracin
of the world, thereby avoiding the significant pragmatic and cognitive expen-
ditures associated with belief revision. In essence, these frameworks highlight
different facets of the same underlying drive to manage cognitive resources ef-
ficiently, where the perceived utility of a belief is weighed against the inherent
costs of mental reorganisation.
3 A Motivated Variational Belief Change Model
As a starting point, we take inspiration from variational inference [7,59], which
underpins the mathematical formalism of the Free-Energy Principle and Active
Inference [18,19]. In the standard Bayesian paradigm, the goal is to infer a
posterior belief p(s | o) about the state of the world s given an observation
o. According to Bayes’ rule, finding this posterior requires one to compute the
model evidence or marginal likelihood p(o), which is an intractable problem in
general.Variationalinferenceaimstoreformulatethisproblembyrecastingitas
anoptimisation problem overavariationalfamilyQofprobabilitydistributions.
The objective function of this optimisation problem is the negative evidence
lower bound (ELBO) or variational free energy (VFE), and is given by
F[q(s),o]=−E [logp(o|s)] + D [q(s) || p(s)] (1)
q(s) KL
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Accuracy Complexity
=−E [logp(s,o)] − H[q(s)]. (2)
q(s)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Energy Entropy
The decomposition in Equation 1 highlights a key tension between the expected
loglikelihoodoftheobservation(accuracy)andtheKLdivergencefromtheprior
to the variational posterior (complexity). In particular, this complexity acts as
a regulariser on the agent’s posterior beliefs, penalising models that differ more
from the prior.
Core to the description of any agent is a description of its boundary, also com-
monly known as its Markov blanket. The Markov blanket of an object describes
the interface via which it is coupled to its environment. According to the Free
EnergyPrinciple(FEP),theinternalpathsofsystemspossessingaMarkovblan-
ket can be viewed as probabilistic beliefs about external paths, and the internal
and active paths of the system appear to minimise its VFE [20]. When moving
to descriptions of agents, however, the system’s internal states embody not only
apredictivemodeloftheworld,butalsopreferences overpossibleconfigurations
of the agent. This is where active inference (AIF) comes into the picture.
AIF extends the FEP to recognise the role of the actions that agentic systems
canperformtoinfluencetheenvironmentand,vicariously,theirobservations[10,
12,42]. Cast here in the variational perspective, the objective function that is
positedtodrivedecision-makinginAIFistheexpected free energy (EFE),which
is a functional of a policy (sequence of actions) π, and is given by
On the Variational Costs of Changing Our Minds 9
Fig.1: A depiction of the causal influences of different components of the agent-
environmentpaironeachother.Externalpathssrepresentstatesoftheworldexternal
to a system or agent under consideration. This system possesses a Markov blanket,
which separates internal from active paths and can itself be divided into sensory and
activepaths.Wefurtherassumethatinternalpathsconsistoftwodistinctcomponents:
beliefsandpreferences,whichmutuallyinfluenceeachother,areinfluencedbysensory
paths, and in turn influence active paths.
G(π)=−E [D [q(s|o,π) || q(s|π)]]−E [logp˜(o)], (3)
q(s,o|π) KL q(o|π)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Epistemicvalue Pragmaticvalue
wherep˜(o)isaprobabilitydistributionrepresentingtheagent’spreferencesover
their own observations, commonly known as a prior preference [10]. Here, we
propose to extend this picture by considering the implications of assuming that
agents have preferences about their own beliefs, and develop a mathematical
framework for describing the ensuing implications for belief updating. In other
words, we suggest that the C matrix, which is used in the active inference lit-
erature to parameterise the preference prior [10,23,42], can be extended to be
definedovertheagent’sownbeliefsaswell,ratherthanonlyobservations/states.
For the purposes of this study, we focus on the mechanisms that drive belief
change in agents. In particular, we are interested in the mapping from sensory
states to belief states. We investigate the consequences of assuming that this
mappingiscomprisedoftwokeycomponents.Thefirstcomponentisapreference
satisfaction component, represented here by an "expected utility" term, which
can be related to prior preferences through a softmax transformation [11]. The
second component is a direct cost for belief updating, which is quantified by the
KL divergence from the agent’s prior beliefs to their posterior beliefs.
We will further assume that agents’ preferences are grounded only in particular
paths,andnotdirectlyonexternalpaths,whichisinconcordancewithanaffect-
driven view on motivation [49,52]. Under these assumptions, an agent’s prefer-
10 D. Hyland and M. Albarracin
Fig.2: Schematic depicting the components involved in our proposed model of belief
updating. Prior beliefs and observations act as inputs to a process that optimises a
balance between a belief utility functional and a complexity term, measuring the KL
divergence from prior to the ultimate posterior beliefs.
ences can be described mathematically by a utility functional U : Q×O →R
defined over observations and beliefs, but not external states.
Given this, we model an agent’s belief updating processes as a variational opti-
misation process that maximises the following functional of beliefs and observa-
tions:
F[q(s),o] = U[q(s),o] −λ D [q(s) || p(s)], (4)
KL
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
beliefutility complexity
where λ ≥ 0 is a parameter determining the relative strength of the cost of
beliefupdatingtothebeliefutilityterm.Thebeliefutilitytermmayormaynot
depend on the accuracy of the model, and depending on its form, can give rise
to different belief updating behaviours. Under this model, taking λ→0 induces
a belief update that is purely driven by belief utility, which is akin to assuming
that the agent is able to instantaneously and effortlessly convince themselves of
whatever they wish to believe. On the other hand, taking λ→∞ increases the
cost of updating to the point that the agent is no longer able to change their
mind, under any circumstances.
Importantly, one particular form for the belief utility that we investigate is a
linear combination of what we term an affective utility and a weighted expected
log-likelihood or accuracy term, which takes the following form:
U[q(s),o]= U[q(s),o] +α E [logp(o|s)], (5)
q(s)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
affectiveutility accuracy
where α ≥ 0 is a likelihood weighting parameter, which determines the extent
to which the agent’s final belief distribution explains the data it has observed.
A higher value of α can be interpreted as a stronger desire to arrive at be-
liefs that explain the observed data well. Moreover, for constant affective utility
functions and α = λ = 1, we recover the VFE as a special case of “accuracy-
On the Variational Costs of Changing Our Minds 11
Fig.3:Plotsdepictinghowagentswithdifferentconservatismparametersλandlikeli-
hoodweightparametersαrespondtoevidencethatconfirmsorcontradictstheirbelief
preferences to varying degrees. Left: Final belief of the probability q(s=0) of state 0
occurring as the evidence strength (in the form of a likelihood p(o|s=0)) varies from
0 to 1 for different values of λ. Right: Final belief q(s=0) as evidence strength varies
for different values of α.
motivated” belief updating. In the following section, we study the predictions
made by adopting the belief utility functional given in Equation 5.
4 Experiments and Results
To study the implications of our proposed model on how motivated agents up-
datetheirbeliefs,weconductedaseriesofminimalexperimentsusingcategorical
distributions3.Inallsimulations,weconsiderhowasinglepieceofevidencepre-
sentedintheformofalikelihooddistributionmaybeselectedandsubsequently
influence the belief updating process. In particular, we demonstrate that un-
der our model, several key features of human belief updating are qualitatively
recovered. Moreover, our model can serve as a framework to generate testable
predictions and simulations of human behaviour in various scenarios.
4.1 How do different agents react to differing degrees of good vs
bad news?
Inourfirstsetofexperiments,weaimtounderstandandillustratetheeffectsof
varyingthestrengthofevidence,theconservatismparameterλandthelikelihood
weightparameterαonbeliefupdating.Inthisscenario,anagentbeginswithan
initialpriorovertheoutcomesofaBernoullirandomvariable(i.e.,abiasedcoin
flip) specified by p(s = 0) = 0.3, and receives evidence of varying strengths in
the form of a likelihood p(o|s). For Bernoulli random variables, we will assume
that the hidden state s may take the values 0 or 1. The agent updates their
3 Thecodeforgeneratingtheexperimentalresultscanbefoundathttps://github.
com/dkhyland/motivated-variational-belief-updating
12 D. Hyland and M. Albarracin
Fig.4:PlotsoftheobjectivevalueforScenarios1and2withdifferentcombinationsof
evidence. In both scenarios, we fix α=2.0 and use a linear affective utility functional
withU[q(s),o]=q(s=0).Left:Scenario1,whereEvidenceAhashighutilitybuthigh
KL from the prior and Evidence B has low utility but low KL from the prior. Right:
Scenario 2, where Evidence A has high utility and low KL, whereas Evidence B has
low utility and high KL.
beliefs to minimise the objective in Equation 4 under the belief utility given in
Equation 5.
In Figure 3, we plot the final belief in state 0 as we vary the evidence strength
p(o | s = 0) between 0 and 1 along the x axis and the values of λ (left) and
α (right) as a spectrum for λ ∈ [1,10] and α ∈ [1,10], along with the Bayesian
update. From the left plot, we observe that higher values of λ lead to updates
that are closer to the prior, whereas lower values of λ lead to updates that are
more sensitive to the affective utility. From the right plot, the opposite effect
is observed – higher likelihood weights lead to more sensitivity to the evidence,
and lower likelihood weights increase sensitivity to the affective utility.
4.2 How do the relative strengths of belief utility and conservatism
affect the selection of evidence?
In this study, we demonstrate the presence of a form of confirmation bias in our
model,andseektounderstandhowdifferentcomponentsofthemodelaffectthe
selection of evidence in our motivated agent. In particular, recall that a crucial
tenet within active inference is that agents are active sense-makers, selecting
evidencetoresolveuncertaintyinbothspecificandnon-specificmannersinorder
to develop a better model of the world and achieve their ultimate objectives. In
this experiment, we extend this notion to include the motivated selection of
evidence to either confirm or disconfirm an agent’s preferences.
We consider what happens when an agent with a linear affective utility who
prefers to believe that p(s = 0) = 1 is presented with two pieces of evidence.
We studied two different scenarios for what these pieces of evidence may be. In
Scenario1,thefirstpieceofevidence(EvidenceA)is‘confirmatory’,inthesense
that it provides evidence for the agent’s desired belief, but is further (induces
On the Variational Costs of Changing Our Minds 13
updates with a larger KL divergence) from the agent’s prior compared to the
second piece of evidence (Evidence B). Evidence B is ‘contradictory’, in the
sense that it is evidence against the agent’s desired belief but is closer to the
agent’s prior. In Scenario 2, Evidence A has both a higher affective utility and
induces updates with a lower KL from the prior to the posterior. In Figure 4,
the objective value is plotted as we sweep across values of λ ∈ [0.1,100]. In
Scenario 1, we observe a threshold at which the agent switches from selecting
confirmatoryevidencetoselectingcontradictoryevidence,whereasthisdoesnot
occur in Scenario 2. Intuitively, this is because for low values of λ, the utility
term dominates belief updating, but for higher values of λ, the cognitive cost
term dominates. In contrast, when both the utility component is higher and
cognitive costs are lower for one piece of evidence over the other, there is never
a reason for the agent to choose to observe disconfirmatory evidence.
4.3 How do belief conservatism and likelihood weighting affect
attitude polarisation?
Fig.5: Plots of attitude polarisation effects between two agents who begin with the
same priorbeliefs and observe the sameevidence, but have different affective utilities.
Agent 1 linearly prefers to believe that q(s=0)=1, whereas Agent 2 linearly prefers
tobelievethatq(s=0)=0.Left:Finalbeliefsaswevaryλfrom0to10.Right:Final
beliefs as we vary α from 0 to 10.
In our final experiment, we simulated a basic attitude polarisation scenario,
where two agents, Agents 1 and 2, began with the same prior belief about the
probabilityq(s=0),andobservedthesameevidenceintheformofalikelihood.
Bothagentswereendowedwithlinearaffectiveutilityfunctions,butAgent1had
a preference for believing that q(s = 0) = 1 and Agent 2 had a preference for
believingthatq(s=0)=0.Plottingthefinalbeliefsafterupdatingaccordingto
our model as we varied λ and α independently in Figure 5, we observe that for
low values of both parameters, the two agents’ final beliefs differed significantly,
demonstratingabasicformofattitudepolarisation.However,asweincreasethe
14 D. Hyland and M. Albarracin
two parameters, the agents’ final beliefs converged toward similar (though not
necessarily Bayes rational) beliefs.
5 Discussion
Though much work needs to be done in empirically validating instantiations of
the framework, our findings lend plausibility to the idea that realistic belief up-
datingissubjecttosignificantinertiaandbias,drivenlargelybytheconstraints
imposed on human agents by both internal cognitive limitations and external
structures.
Realistic belief revision is rarely drastic, especially in the presence of cognitive
costs for updating. From a cognitive standpoint, rapid updates necessitate more
complex neural rewiring and a higher cognitive load, which can overwhelm lim-
itedcognitiveresources.Agentswouldtendtoavoidsuchcostlyleaps.Incremen-
tal steps across belief space reduce immediate costs but may also cumulatively
result in lower total energetic and informational expenditure.
Underthisview,wehypothesisethatgradualtransitionsaretypicallymoresus-
tainable and preferable overall. Such considerations could explain why individu-
als are naturally inclined to resist abrupt changes in their belief systems despite
potentially strong contradictory evidence, reinforcing conservative patterns of
information integration.
Strategies for effective belief updating Our basic model suggests several
strategic insights for promoting more effective belief updating. Given the high
cost of large leaps in belief space, strategies should prioritise incrementalism.
This involves structuring information exposure in manageable segments that
progressivelyleadindividualstowardsdesiredbeliefs,therebyreducingtheener-
getic,cognitive,andsocialresistancetodramaticchanges.Socialnetworksshould
be leveraged strategically: encouraging cross-cutting social ties and diversity in
informationalenvironmentscanreducetheperceivedsocialrisksassociatedwith
belief change.
5.1 Future Directions
In considering future avenues for research based on our current findings, several
promising directions are worth exploring.
Completing the Action-Perception Loop So far, our model has focused
on the processes involved in the updating of beliefs, taking into account sensory
evidence.Ourpreliminarydataselectioninvestigationtakesthisastepfurtherby
demonstratinghowdecisionsaboutwhatdatatoobservecaninfluencedecision-
making. However, further work is required to fully integrate motivation into the
perception-action loop.
On the Variational Costs of Changing Our Minds 15
Addition of temporal considerations So far, our model has not explicitly
incorporated the temporal aspect of belief updating, which we believe to be sig-
nificant in modelling the various costs that must be taken into consideration.
Indeed, several works have posited a central role of rates of change in free en-
ergy/prediction errors as crucial to understanding affect [16,25]. Extending the
model to account for the role of time would allow a more detailed analysis of
how belief trajectories could be optimised, rather than single updates.
Extensions to group dynamics Further work could more explicitly incor-
porate group dynamics, particularly focusing on how social networks influence
belief inertia and revision costs. Future work could explore the degree to which
group identity and perceived social costs shape belief stability, potentially repli-
cating frameworks similar to those presented by [2] on epistemic communities.
By examining how belief updates propagate through structured networks and
assessing how identity-protective reasoning reinforces certain belief states, we
can quantify the inertia inherent within closely knit communities. Moreover,
evaluating the relative weight of belief confidence levels and their susceptibility
to drift could provide deeper insights into the dynamics of belief evolution in
socialcontexts.Suchextensionsmayalsoclarifyhownetworkedbeliefsreinforce
each other, creating feedback loops that stabilise misinformation.
Further empirical validation Empirical validation remains essential for con-
firming and refining our theoretical propositions. Future empirical work will
rigorously test model predictions using controlled laboratory experiments, field
studies, and simulation analyses. For instance, quantifiable predictions derived
from our framework—such as the relationship between KL divergence, belief
revision speed, and associated cognitive or social costs—could be tested experi-
mentally by monitoring physiological or neural responses during belief updating
tasks. Longitudinal field studies examining belief trajectories within real-world
social groups could also provide valuable validation, providing insights into how
incremental versus rapid belief changes correlate with tangible social and cogni-
tive outcomes.
Acknowledgments. TheauthorswouldliketothankLancelotDaCostaandTomáš
Gavenčiak for helpful discussions and feedback.
References
1. Albarracin, M., Bouchard-Joly, G., Sheikhbahaee, Z., Miller, M.,
Pitliya, R. J., and Poirier, P. Feeling our place in the world: An active
inference account of self-esteem. Neuroscience of Consciousness 2024, 1 (2024),
niae007.
2. Albarracin, M., Demekas, D., Ramstead, M. J., and Heins, C. Epistemic
communities under active inference. Entropy 24, 4 (2022), 476.
3. Andresen,B.Currenttrendsinfinite-timethermodynamics.AngewandteChemie
International Edition 50, 12 (2011), 2690–2704.
16 D. Hyland and M. Albarracin
4. Barlow, H. B., et al. Possible principles underlying the transformation of
sensory messages. Sensory communication 1, 01 (1961), 217–233.
5. Bartels, L. M. Beyond the running tally: Partisan bias in political perceptions.
Political Behavior 24, 2 (2002), 117–150.
6. Bicchieri, C., and Mercier, H. Norms and Beliefs: How Change Occurs. Ox-
ford University Press, Oxford, 2014.
7. Blei, D. M., Kucukelbir, A., and McAuliffe, J. D. Variational inference:
Areviewforstatisticians. JournaloftheAmericanstatisticalAssociation112,518
(2017), 859–877.
8. Bouizegarene, N., Ramstead, M. J. D., Constant, A., Friston, K. J.,
and Kirmayer, L. J. Narrative as active inference: an integrative account of
cognitive and social functions in adaptation. Frontiers in Psychology 15 (2024),
1345480.
9. Constant,A.,Ramstead,M.J.D.,Veissière,S.P.L.,andFriston,K.J.
Regimesofexpectations:Anactiveinferencemodelofsocialconformityandhuman
decision making. Frontiers in Psychology 10 (2019), 679.
10. Da Costa, L., Parr, T., Sajid, N., Veselic, S., Neacsu, V., and Friston,
K. Activeinferenceondiscretestate-spaces:Asynthesis. JournalofMathematical
Psychology 99 (2020), 102447.
11. Da Costa, L., Sajid, N., Parr, T., Friston, K., and Smith, R. Reward
maximizationthroughdiscreteactiveinference. Neural Computation 35,5(2023),
807–852.
12. DaCosta,L.,Tenka,S.,Zhao,D.,andSajid,N. Activeinferenceasamodel
of agency. arXiv preprint arXiv:2401.12917 (2024).
13. Deane, G., Mago, J., Fotopoulou, A., Sacchet, M., Carhart-Harris,
R.,andSandved-Smith,L. Thecomputationalunconscious:Adaptivenarrative
control, psychopathology, and subjective well-being, Jan 2024.
14. Ditto, P. H., Pizarro, D. A., and Tannenbaum, D. Motivated moral rea-
soning. Psychology of learning and motivation 50 (2009), 307–338.
15. Ecker, U., Lewandowsky, S., Cook, J., Schmid, P., Fazio, L., Brashier,
N., Kendeou, P., Vraga, E., and Amazeen, M. The psychological drivers of
misinformation belief and its resistance to correction. Nature Reviews Psychology
1 (01 2022), 13–29.
16. Fernandez Velasco, P., and Loev, S. Affective experience in the predictive
mind:areviewandnewintegrativeaccount.Synthese198,11(2021),10847–10882.
17. Fields, C., Goldstein, A., and Sandved-Smith, L. Making the thermody-
namic cost of active inference explicit. Entropy 26, 8 (2024), 622.
18. Friston, K. The free-energy principle: a unified brain theory? Nature reviews
neuroscience 11, 2 (2010), 127–138.
19. Friston, K., Da Costa, L., Sajid, N., Heins, C., Ueltzhöffer, K., Pavli-
otis, G. A., and Parr, T. The free energy principle made simpler but not too
simple. Physics Reports 1024 (2023), 1–29.
20. Friston, K., Da Costa, L., Sakthivadivel, D. A., Heins, C., Pavliotis,
G. A., Ramstead, M., and Parr, T. Path integrals, particular kinds, and
strange things. Physics of Life Reviews (2023).
21. Gershman, S. J., Horvitz, E. J., and Tenenbaum, J. B. Computational
rationality:Aconvergingparadigmforintelligenceinbrains,minds,andmachines.
Science 349, 6245 (2015), 273–278.
22. Guénin-Carlut, A., and Albarracin, M. On embedded normativity: An ac-
tive inference account of agency beyond flesh. In Active Inference, vol. 1630 of
On the Variational Costs of Changing Our Minds 17
Communications in Computer and Information Science. Springer Nature, Cham,
Switzerland, 2024, pp. 91–105.
23. Heins, C., Millidge, B., Demekas, D., Klein, B., Friston, K., Couzin, I.,
and Tschantz, A. pymdp:Apythonlibraryforactiveinferenceindiscretestate
spaces. arXiv preprint arXiv:2201.03904 (2022).
24. Jain, S. P., and Maheswaran, D. Motivated reasoning: A depth-of-processing
perspective. Journal of Consumer Research 26, 4 (2000), 358–371.
25. Joffily,M.,andCoricelli,G.Emotionalvalenceandthefree-energyprinciple.
PLoS computational biology 9, 6 (2013), e1003094.
26. Jones, M., and Love, B. C. Pinning down the theoretical commitments of
bayesian cognitive models. Behavioral and Brain Sciences 34, 4 (2011), 215–231.
27. Kiverstein, J., Miller, M., and Rietveld, E. Desire and motivation in pre-
dictive processing: An ecological-enactive perspective. Review of Philosophy and
Psychology (2024), 1–21.
28. Kruglanski, A. W., Jasko, K., and Friston, K. All thinking is ‘wishful’
thinking. Trends in Cognitive Sciences 24, 6 (2020), 413–424.
29. Kunda, Z. Thecaseformotivatedreasoning. Psychologicalbulletin108,3(1990),
480.
30. Lewis, R. L., Howes, A., and Singh, S. Computational rationality: Linking
mechanism and behavior through bounded utility maximization. Topics in cogni-
tive science 6, 2 (2014), 279–311.
31. Lieder, F., and Griffiths, T. L. Resource-rational analysis: Understanding
humancognitionastheoptimaluseoflimitedcomputationalresources. Behavioral
and brain sciences 43 (2020), e1.
32. Little, A. T. How to distinguish motivated reasoning from bayesian updating.
Political Behavior (2025), 1–25.
33. Lord, C., Ross, L., and Lepper, M. Biased assimilation and attitude polar-
ization:Theeffectsofpriortheoriesonsubsequentlyconsideredevidence. Journal
of Personality and Social Psychology 37 (11 1979), 2098–2109.
34. Mandelbaum,E.Troubleswithbayesianism:Anintroductiontothepsychological
immune system. Mind & Language 34, 2 (2019), 141–157.
35. Mercier,H.,andSperber,D.Theenigmaofreason.HarvardUniversityPress,
2017.
36. Nickerson, R. S. Confirmation bias: A ubiquitous phenomenon in many guises.
Review of General Psychology 2, 2 (1998), 175–220.
37. Oeberst, A., and Imhoff, R. Toward parsimony in bias research: A proposed
common framework of belief-consistent information processing for a set of biases.
Perspectives on Psychological Science 18, 6 (2023), 1464–1487.
38. Oeberst, A., Mischkowski, D., and Imhoff, R. Belief-consistentinformation
processingorcoherence-basedreasoning:Integratingtwoparsimoniousframeworks
for biases. European Journal of Social Psychology (2025).
39. Ortega, P. A., and Braun, D. A. Thermodynamics as a theory of decision-
making with information-processing costs. Proceedings of the Royal Society A:
Mathematical, Physical and Engineering Sciences 469, 2153 (2013), 20120683.
40. Ortega, P. A., Braun, D. A., Dyer, J., Kim, K.-E., and Tishby,
N. Information-theoretic bounded rationality. arXiv preprint arXiv:1512.06789
(2015).
41. Parr, T., Holmes, E., Friston, K. J., and Pezzulo, G. Cognitiveeffortand
active inference. Neuropsychologia 184 (2023), 108562.
42. Parr, T., Pezzulo, G., and Friston, K. J. Active inference: the free energy
principle in mind, brain, and behavior. MIT Press, 2022.
18 D. Hyland and M. Albarracin
43. Parrondo, J. M., Horowitz, J. M., and Sagawa, T. Thermodynamics of
information. Nature physics 11, 2 (2015), 131–139.
44. Patterson, R., Operskalski, J. T., and Barbey, A. K. Motivated explana-
tion. Frontiers in human neuroscience 9 (2015), 559.
45. Pilgrim, C., Sanborn, A., Malthouse, E., and Hills, T. T. Confirmation
biasemergesfromanapproximationtobayesianreasoning. Cognition 245 (2024),
105693.
46. Prat-Ortega, G., and de la Rocha, J. Selectiveattention:Aplausiblemech-
anismunderlyingconfirmationbias. Current Biology 28,19(2018),R1151–R1154.
47. Priniski, J. H., Solanki, P., and Horne, Z. A bayesian decision-theoretic
framework for studying motivated reasoning, Oct 2022.
48. Salamon, P., Andresen, B., Nulton, J., Roach, T. N., and Rohwer, F.
Morestagesdecreasedissipationinirreversiblestepprocesses.Entropy25,3(2023),
539.
49. Sennesh, E., and Ramstead, M. An affective-taxis hypothesis for alignment
and interpretability. arXiv preprint arXiv:2505.17024 (2025).
50. Sharot, T., and Garrett, N. Forming beliefs: Why valence matters. Trends
in Cognitive Sciences 20, 1 (2016), 25–33.
51. Sharot, T., Rollwage, M., Sunstein, C. R., and Fleming, S. M. Whyand
when beliefs change. Perspectives on Psychological Science 18, 1 (2023), 142–151.
52. Shenhav, A. The affective gradient hypothesis: An affect-centered account of
motivated behavior. Trends in Cognitive Sciences (2024).
53. Simon, D., and Read, S. J. Toward a general framework of biased reasoning:
Coherence-based reasoning. Perspectives on Psychological Science 20, 3 (2025),
421–459.
54. Simon, H. A. Theoriesofboundedrationality. DecisionandOrganization (1964),
161–176.
55. Spelman, T., Elnakouri, A., Kteily, N., and Finkel, E. J. Overestimating
thesocialcostsofpoliticalbeliefchange.JournalofExperimentalSocialPsychology
105 (2023), 104115.
56. Talluri, B. C., Urai, A. E., Tsetsos, K., Usher, M., and Donner, T. H.
Confirmation bias through selective overweighting of choice-consistent evidence.
Current Biology 28, 19 (2018), 3128–3135.
57. Vasil, J., Badcock, P. B., Constant, A., Friston, K. J., and Ramstead,
M.J.D. Aworlduntoitself:Humancommunicationasactiveinference. Frontiers
in Psychology 11 (2020), 417.
58. Veissière, S. P. L., Constant, A., Ramstead, M. J. D., Friston, K. J.,
and Kirmayer, L. J. Thinking through other minds: A variational approach to
cognition and culture. Behavioral and Brain Sciences 43 (2020), e90.
59. Wainwright, M. J., Jordan, M. I., et al. Graphical models, exponential
families,andvariationalinference.FoundationsandTrends®inMachineLearning
1, 1–2 (2008), 1–305.
60. Westerwick, A., Kleinman, S. B., and Knobloch-Westerwick, S. Con-
firmation bias in online searches: Impacts of selective exposure before an election
on political attitude strength and shifts. Journal of Communication 67, 4 (2017),
660–684.
61. Williams, D. Socially adaptive belief. Philosophical Studies 178, 3 (2021), 785–
804.
62. Wolpert, D. H. The free energy requirements of biological organisms; implica-
tions for evolution. Entropy 18, 4 (2016), 138.
On the Variational Costs of Changing Our Minds 19
63. Wolpert, D. H., Korbel, J., Lynn, C. W., Tasnim, F., Grochow, J. A.,
Kardeş,G.,Aimone,J.B.,Balasubramanian,V.,DeGiuli,E.,Doty,D.,
et al. Is stochastic thermodynamics the key to understanding the energy costs
of computation? Proceedings of the National Academy of Sciences 121, 45 (2024),
e2321112121.
64. Zhu, J.-Q., Sanborn, A., Chater, N., and Griffiths, T. Computation-
limited bayesian updating. In Proceedings of the Annual Meeting of the Cognitive
Science Society (2023), vol. 45.
20 D. Hyland and M. Albarracin
A Analytical Solution for Optimal Belief Updates in the
Linear Affective Utility Case
Inthisappendix,wederivetheclosed–formoptimalposteriorthatminimisesthe
variational objective introduced in 3. Throughout, let S be a finite set of latent
states s ∈ S, q(s) the candidate posterior, and p(s) the fixed prior. Observed
data are denoted by o with likelihood p(o | s). The objective functional to be
minimised is
F[q(s),o] := U[q(s),o] +α E (cid:2) logp(o|s) (cid:3) −λ D [q(s) || p(s)]. (6)
q KL
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
affectiveutility accuracy complexity
Here U[q(s),o] represents the affective utility of a belief state q(s) and obser-
vation o, and α,λ modulate respectively the weight assigned to the epistemic
evidence and the inertia (or cost) of deviating from the prior.
For the case of linear affective utilities, we have
(cid:88)
U[q(s),o] = c q(s), (7)
s
s∈S
with coefficients c ∈R capturing the valence of believing state s.
s
(cid:80)
We maximise (6) under the normalisation constraint q(s) = 1. Introducing
s
a Lagrange multiplier η ∈R gives the augmented Lagrangian
(cid:88) (cid:16) (cid:88) (cid:17)
L(q,o,η)= c q(s)+αE [logp(o|s)]−λD [q(s) || p(s)]+η 1− q(s) .
s q KL
s∈S s
(8)
Stationarity with respect to each q(s) yields
∂L (cid:104) q(s)(cid:105)
0= =c + α logp(o|s) − λ 1+log − η. (9)
∂q(s) s p(s)
Solving (9) for q(s) and exponentiating, we obtain
(cid:20) 1(cid:16) (cid:17) (cid:21)
q(s)=p(s)exp c +α logp(o|s)−η −1 (10)
λ s
∝p(s)exp
(cid:104) 1(cid:0)
c +α logp(o|s)
(cid:1)(cid:105)
. (11)
λ s
Normalising with the partition function
Z(o) := (cid:88) p(s′) exp (cid:104) 1(cid:0) c +α logp(o|s′) (cid:1)(cid:105) , (12)
λ s′
s′∈S
we arrive at the optimal variational posterior
p(s)exp
(cid:2) λ−1(cid:0)
c +α logp(o|s)
(cid:1)(cid:3)
q⋆(s)= s . (13)
Z(o)
On the Variational Costs of Changing Our Minds 21
B Additional Figures
Fig.6: Heatmaps depicting the variational objective and final belief landscapes for
different (λ,α) pairs. The upper two panels depict the objective and belief landscapes
(leftandright,respectively)forevidenceintheformofalikelihoodwherep(o|s=0)=
0.3, and the bottom two represent the same but for evidence p(o|s = 0) = 0.7. For
disconfirmatory evidence (top),
22 D. Hyland and M. Albarracin
Fig.7: Contoured heatmaps depicting various quantities as we vary both the conser-
vatismparameterλandthelikelihoodweightparameterαinScenario1ofwoursecond
experiment (Section 4.2). Top left: heatmap of the objective landscape under confir-
matory evidence, i.e., evidence that aligns with the agent’s desired belief. Top right:
heatmap of the objective landscape under contradictory evidence, i.e., evidence that
contradictstheagent’sdesiredbelief.Bottomleft:heatmapofthefinalbeliefq(s=0)
after evidence selection. The white line depicts the boundary at which the agent se-
lectsEvidenceA(leftofboundary)overEvidenceB(rightofboundary).Bottomright:
heatmap showing the difference in the objectives for confirmatory and contradictory
evidence.TheblacklineagaindepictstheboundarybetweenchoosingEvidenceAover
Evidence B.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "On the Variational Costs of Changing Our Minds"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
