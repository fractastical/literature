=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Active Inference-Based Optimization of Discriminative Neural Network Classifiers
Citation Key: fallah2023active
Authors: Faezeh Fallah

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2023

Abstract: Commonlyusedobjectivefunctions(losses)forasupervisedoptimizationofdiscriminativeneural
networkclassifierswereeitherdistribution-basedormetric-based. Thedistribution-basedlosseswere
mostlybasedonthecrossentropyandfittedthenetworkmodeltothedistributionofthetraining
samples. Thiscouldcompromisethegeneralization(predictiveperformanceonunseensamples)or
causeclassificationbiasestowardsthedominantclassesofanimbalancedclass-sampledistribution.
Themetric-basedlossescouldmakethenetworkmodelindependentofan...

Key Terms: optimization, discriminative, groundtruth, classifiers, neural, themetric, inference, active, network, basedlosses

=== FULL PAPER TEXT ===

Active Inference-Based Optimization of Discriminative Neural
Network Classifiers
FaezehFallah
InstituteofSignalProcessingandSystemTheory
UniversityofStuttgart,Pfaffenwaldring47
70569Stuttgart,Germany
faezeh.fallah@iss.uni-stuttgart.de
Abstract
Commonlyusedobjectivefunctions(losses)forasupervisedoptimizationofdiscriminativeneural
networkclassifierswereeitherdistribution-basedormetric-based. Thedistribution-basedlosseswere
mostlybasedonthecrossentropyandfittedthenetworkmodeltothedistributionofthetraining
samples. Thiscouldcompromisethegeneralization(predictiveperformanceonunseensamples)or
causeclassificationbiasestowardsthedominantclassesofanimbalancedclass-sampledistribution.
Themetric-basedlossescouldmakethenetworkmodelindependentofanydistributionandthus
improveitsgeneralization. However,themetricsinvolvedinthemwerebinaryclassificationmetrics.
Thisimpliedtodecomposeamulticlassclassificationintoaseriesofone-vs-allclassificationsand
thenformtheoveralllossfromanaverageoftheone-vs-alllosses. Thisaveragingcouldnaturally
leadtoabiastowardsthedominantclasses. Moreover,themetric-basedlossescouldsufferfrom
discrepancieswhenaclasswasabsentinboththereference(groundtruth)labelsandthepredicted
labels. Totackletheseissues,recentworkshaveusedacombinationofthedistribution-basedand
metric-basedlosses. Inthispaper,weformulatedtheoptimizationofadiscriminativeneuralnetwork
classifierwithintheframeworkofactiveinferenceandshowedthatthecrossentropy-basedlosses
wereindeedthevariationalfreeenergyofaretrospectiveactiveinference. Then,weproposedanovel
optimizationprocesswhichnotonlytackledtheunbalancednessoftheclass-sampledistributionof
thetrainingsamplesbutalsoprovidedamechanismtotackleerrorsinthereference(groundtruth)
labelsofthetrainingsamples. Thiswasachievedbyproposinganovelalgorithmtofindcandidate
classificationlabelsofthetrainingsamplesduringthenetworkoptimizationandanovelobjective
functionfortheoptimizations. Thealgorithmcouldfindthecandidatelabelsofthetrainingsamples
fromtheirpriorprobabilitiesandthecurrentlyestimatedposteriorsonthenetwork. Theproposed
objectivefunctionincorporatedthesecandidatelabelsalongwiththeoriginalreferencelabelsandthe
priorsofthetrainingsampleswhilestillbeingdistribution-based. Theproposedalgorithmwasthe
resultofcastingthegeneralizedKellycriterionforoptimalbettingintoamulticlassclassification
problem. Tothisend,weshowedthattheobjectivefunctionofthegeneralizedKellycriterionwas
atightupperboundoftheexpectedcomplexityoftheexpectedfreeenergyofaprospectiveactive
inference. Thisinturnallowedustoderiveourproposedobjectivefunctionfromsuchanexpected
freeenergy. Theincorporationofthepriorsintotheoptimizationnotonlyhelpedtotackleerrorsin
thereferencelabelsbutalsoallowedtoreduceclassificationbiasestowardsthedominantclassesby
focusingtheattentionoftheneuralnetworkonimportantbutminorityforegroundclasses.
Preprint.Underreview.
3202
nuJ
4
]GL.sc[
1v74420.6032:viXra
1 BackgroundandMotivation
1.1 ActiveInference
Bayesian inference enabled perception, learning, and decision making in a passive or active perceptual task. This
perceptioncouldbeoveracategorical(multinomial)distributionofindependentandmutuallyexclusivestates. This
distributionassignedoneprobabilitytoeachstateofeachobservationwiththesumoftheseprobabilitiesforeach
observationbeingone. Thatis,eachobservationcouldonlybeinonestateatatime. Inanactiveperception,anagent
activelyengagedwithitsenvironmenttogatherinformation,seekpreferredobservations,avoidunpreferredobservations,
andtakeactionswhichcouldreduceuncertaintyandmaximizereward. Ifthestates,observations,andpolicies(actions)
couldbediscretized,thenthetaskscouldbeformulatedovercategoricaldistributionsofthestates,observations,and
policies. Theseformedadiscretestate-spacemodelinwhichthetimecouldbediscreteaswell. Anactiveperception
ruled by the Bayesian inference was called an active inference. The Bayesian inference inferred joint/posterior
distributionofagenerative/discriminativemodelbyusingtheBayes’theorem. Fortheclassification/segmentationtasks
addressedinthisdissertation,adiscriminativemodelwassufficient. Thus,werestrictedtheuseoftheactiveinference
toadiscriminativemodelandonlyinvolvedtheposteriorsinourformulations[Smith2022].
AccordingtotheBayes’theorem,foreachobservation(o),state(s),andpolicy(π),theposteriorp(s|o,π)couldbe
deducedfromthelikelihoodp(o|s,π)as
p(o|s,π)·p(s|π)
p(s|o,π)= (1)
p(o|π)
(cid:80)
withp(o|π)= p(o|s,π)beingthemodelevidenceorthemarginallikelihood. Thisway,theBayesianinference
s|π
enabledperception,learning,anddecisionmakingbymodelinversion,i.e. deductionoftheposteriorp(s|o,π)fromthe
likelihoodp(o|s,π). Thisresultedinamaximumaposterioriestimation. Inasimplerapproach,amaximumlikelihood
estimation might be followed. However, the maximum likelihood estimation was prone to overfitting because the
likelihoodsonlyencodedthealeatoricuncertaintyofthemodelcausedbynoise(disturbances)initsprocess. The
epistemic(cognitive)uncertaintyofthemodelwasreflectedbythestates’priors{p(s|π)} andthemodelevidence
s
p(o|π)includedintheposteriors. Thecomputationofthemodelevidenceimpliedtosumthelikelihoodsofevery
observationoverallpossiblestates. Formostofthecategoricaldistributionsthiscomputationwasintractable. Also,
byincreasingthenumberofthestatesthenumberofthesummationtermsincreasedexponentially. Forcontinuous
distributionsthissummationmostlyturnedintoanonconvexintegrationofnoclosed-form(analytical)solution. To
enableacomputationallytractableactiveinference,theBayes’theoremgotapproximatedbyminimizing
• variationalfreeenergy(VFE)1forperceptionandlearning
• expectedfreeenergy(EFE)foroptimaldecisionmaking,planning,andactionselection.
Eachoftheaforementionedobjectivefunctionsdependedonthepolicies(actions). Accordingly,theminimization
ofeachofthemprovidedanestimateoftheposteriorsconditionedonthepolicies. However,theVFEresultedfrom
acourseofpoliciesbasedontheobservationsinthepastandpresentbuttheEFEresultedfromacourseofpolicies
basedontheobservationsinthefuture. Thus,theVFEandtheEFErespectivelyenabledretrospectiveandprospective
policyevaluations. Thisdifferencematteredinthecaseswhereoptimalpoliciesforthepastorpresentwerenotthe
optimalpoliciesforthefutureorviceversa. Toderivetheaforementionedobjectives,negativelogarithmofbothsides
oftheBayes’formulawastakenand−ln (cid:0) p(o|π) (cid:1) wasintroducedtobetheself-informationorsurprisal2ofthemodel
evidencep(o|π). Then,theVFEgotdefinedtobetheupperboundofthisquantity. Thisway,byminimizingtheVFE,
1ThetermfreeenergystemmedfromconnectionsbetweentheBayesianinferenceandtheBayesianmechanicsrulingfreeenergy
inparticular(quantum)physicselaboratedbyneuroscientists[Friston2019].
2Useofthenaturallogarithmresultedininformationbeingmeasuredinnats.Incontrast,useofthelog resultedininformation
2
beingmeasuredinbits.
2
thesurprisalordeviationbetweenobservationsandpredictionsofthemodelgotminimizedortheamountofevidence
anobservationcouldprovideforthemodelgotmaximized,i.e. themodelevidencegotmaximized.
Asdetailedin[Smith2022],theobjectivefunctionoftheVFEwasgivenby
(cid:104) (cid:105) (cid:104) (cid:0) (cid:1)(cid:105)
L =KL p(s|π)||q(s|π) −E ln q(o|s)
VFE p(s|π)
(cid:104) (cid:0) (cid:1) (cid:0) (cid:1)(cid:105) (cid:104) (cid:0) (cid:1)(cid:105)
=E ln p(s|π) −ln q(s|π) −E ln q(o|s)
p(s|π) p(s|π)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
complexity accuracy
= (cid:88) p(s|π)·ln (cid:0) p(s|π) (cid:1) − (cid:88) p(s|π)·ln (cid:0) q(s|π) (cid:1) − (cid:88) p(s|π)·ln (cid:0) q(o|s) (cid:1) (2)
s|π s|π s|π
(cid:88) (cid:0) (cid:1) (cid:88) (cid:0) (cid:1)
= p(s|π)·ln p(s|π) + −p(s|π)·ln q(o|π)
s|π s|π
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
−entropy crossentropy
withq(·)beingthedistributionapproximatingthetruedistributionp(·),KL[p(·)||q(·)]beingtheKullback-Leibler(KL)
divergence(dissimilarity)betweenp(·)andq(·),andE [·]beingtheexpectationwithrespecttop(s|π). TheKL
p(s|π)
divergencewasderivedfromtheAkaikeinformationcriterion(AIC)measuringthegoodnessofamodelintermsof
itsunderfitting(estimationbiasonseensamples)andoverfitting(predictivevarianceonunseensamples). TheAIC
measuredtheamountofinformationloss(relativeentropy)resultedfromrepresentingamodelwithanothermodel.
Here,thecrossentropywasnotadistancemetricbecausethecrossentropyoftwoidenticaldistributionsequaledtheir
entropy. However,aftersubtractingtheentropyfromthecrossentropy,theKLdivergencebecomeadistancemetric.
Thatis, theKLdivergenceoftwoidenticaldistributionswaszero[Kullback1951,McMillan1956]. Thisway, the
minimizationofL amountedtofindingthedistributionq(·)whichbestfittedp(·). Thebestfitwastheminimizerof
VFE
thecomplexity(overfitting)andthemaximizeroftheaccuracy. TheminimizationofL wasindependentofp(s|π).
VFE
ThusbyaddingtheentropytermtoL ,anobjectivefunctioncalledthecrossentropylosswasobtainedas
VFE
(cid:88) (cid:0) (cid:1)
L =− p(s|π)·ln q(o|π) . (3)
CE
s|π
Ifq(·)wasGaussian,thenthecrossentropylossbecomeasumofsquarederrors.
TheminimizationoftheEFEselectedoptimalpolicies(actions)bysolvingtheexplore-exploitdilemma[Friston2019].
Thatis,wheninformationaboutthestateswerenotenough,itemphasizedonexploration(maximizationofinformation
gainorminimizationofuncertainty). Whentheinformationwasenough,itemphasizedonexploitation(maximization
ofrewardorminimizationofexpectedcomplexity). Thechoiceoftheexploratoryortheexploitativeoptimization
dependedonthecurrentuncertaintyandthefuture(expected)reward. Thisway,theminimizationoftheEFEsoughtthe
policieswhichcouldleadtofutureobservationsoptimizingthetrade-offbetweenthemaximizationoftheinformation
gain and the maximization of the reward. These self-evidencing observations were called to be preferred. The
incidenceprobabilityofapreferredobservationowasdenotedbyp(o). Asdetailedin[Smith2022],theobjective
functionoftheEFEwasgivenby
(cid:104) (cid:105) (cid:104) (cid:2) (cid:3)(cid:105)
L =KL p(o)||q(o|π) +E H q(o|π) (4)
EFE p(s|π)
(cid:104) (cid:0) (cid:1) (cid:0) (cid:1)(cid:105) (cid:104) (cid:2) (cid:3)(cid:105)
=E ln p(o) −ln q(o|π) +E H q(o|π)
p(o) p(s|π)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
expectedcomplexity uncertainty
(cid:88) (cid:104) (cid:0) (cid:1) (cid:0) (cid:1)(cid:105) (cid:88) (cid:88) (cid:0) (cid:1)
= p(o)· ln p(o) −ln q(o|π) + −p(s|π)· q(o|π)·ln q(o|π)
o s|π o|π
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
expectedcomplexity uncertainty
3
(cid:2) (cid:3) (cid:80) (cid:0) (cid:1)
with H q(o|π) = − q(o|π)·ln q(o|π) being the entropy of q(o|π). This way, active inference provided a
o|π
unifiedmathematicalframeworktomodelinterdependentaspectsofperception,learning,anddecisionmaking. This
framework could build highly flexible and generalizable generative models which could explain neuro-cognitive
behavioralprocessesaswellaspartiallyobservableMarkovdecisionprocesses[Friston2019,Smith2022].
1.2 OptimizationofDiscriminativeNeuralNetworkClassifiers
A neural network was composed of several perceptrons (nodes) in multiple layers. The layers included an input
layer, some hidden layers, and an output layer. A perceptron contained a nonlinear function called an activation
andwasconnectedtootherperceptronsinneighboringlayersviasomeweightsandabias. Theseweights,biases,
and the nonlinear activations formed main parameters of the neural network. Besides, the neural network had
somehyperparametersdefiningitsarchitectureanditsoptimizationprocess. Neuralnetworkshavedemonstrated
promisingresultsinawiderangeofapplications. Thiswasduetotheuniversalapproximationtheoremstatingthata
feed-forwardnetworkwithahiddenlayercontainingafinitenumberofneurons(perceptrons)couldapproximateany
continuousfunctiononacompactsubsetofRdifandonlyiftheusedactivations(perceptrons’nonlinearities)were
nonpolynomial. Thenumberoftheparametersofsuchanapproximatingmodeldefineditscapacitytorepresentand
topredictpatterns. Forafullyconnectedneuralnetwork,thisnumberwasO(n ·n2 )wheren wasthe
layer width layer
numberoflayers(depthofthenetwork)andn2 wasthenumberofperceptronsperlayer(widthofthenetwork).
width
Thus,anincreaseinthewidthincreasedthenumberoftheparametersfasterthananincreaseinthenumberoflayers.
Anincreaseinthenumberofparametersincreasedthechanceofoverfitting. Moreover,awideshallownetworkcould
fittothepatternsintheseen(training)samplesbutcouldnotpredictthepatternsinunseen(validationortest)samples.
Toenhancethegeneralization(predictiveperformanceonunseensamples),theneuralnetworkshouldcontainmore
layers(becomedeeper)[Dean2012,Ruder2016,Goodfellow2016].
Inafullyconnectedneuralnetwork,everyperceptronwasconnectedtoalltheperceptronsinitsneighboringlayers.
Thisnetworklackedthecapabilityofcapturingregional(intra-layer)neighborhoodpatternsandthusneededhandcrafted
featurestoaccomplishitstask. Tohaveanend-to-endneuralnetwork,directlyapplicabletotheinputsampleswithout
anypreprocessingorexplicitfeatureextraction,thefeaturesshouldbeextractedbythenetworkitself. Thisimpliedto
captureregional(intra-layer)neighborhoodpatternsthroughlimitedreceptivefields. Thereceptivefieldofaperceptron
definedthesizeandtheshapeoftheregionattheinputofthenetworkaffectingtheoutputoftheperceptron. The
receptivefieldwasdeterminedbythekernelandthedeptoftheperceptronintheneuralnetwork. Thedeeperthe
perceptroninthenetworkwasthelargeritsreceptivefieldbecome.
Theapplicationofaperceptron’skerneltoitsinputsreturnedanumberoffeaturemaps. Byincreasingthereceptive
fieldoftheperceptron,thenumberandtheabstractionlevelofitsfeaturemapsgotincreasedbutthesizeofeachmap
gotdecreased. Accordingly,byusingdifferentkernelsandlocatingtheperceptronsatdifferentdepthsofthenetwork,
featuresofdifferentresolutionsandabstractionlevelscouldbeobtained. Besidescapturingsubtlefeaturesandpatterns,
akernel-basednetworkenabledweightsharingbyapplyingthesamekernelcoefficientstovariousregionsinspace.
Thisresultedinasignificantlylowernumberofparametersthanafullyconnectednetworkandthusreducedthechance
ofoverfittingandimprovedthegeneralization(predictiveperformanceonunseensamples). Inaddition,itreducedthe
numberofsamplesneededtotrain(optimize)thenetwork. Aneasy-to-implementkernelforestimatingacategorical
distributioninaclassificationproblemoracontinuousdistributioninaregressiontaskwasconvolutional1. Thistype
ofkernelformedaconvolutionalneuralnetwork(CNN)whichcouldbeend-to-endanddeepaswell.
As shown in Figure 1, a neural network could be plain or Bayesian. In the plain network, each parameter, i.e.
eachweight,bias,oractivation,hadasinglevalue. IntheBayesiannetwork,eachparameterhadavectorofvalues
representingitsdistributionanduncertainty. TheBayesiannetworkwasformedfromanensembleofplainnetworks.
Thatis,multipleplainnetworksgotbuiltandthentheBayesiannetwork’sparametersgotderivedfromaweighted
1Inpractice,manymachinelearninglibrariesavoidedthesignflipactioninvolvedintheconvolutionandthussimplyimplemented
acrosscorrelationbetweentheinputsandthekernelsofeachlayer.
4
Figure1:Aneuralnetworkwithplain(single-valued)weightsandbiases(a),plainactivations(b),Bayesian(distributed)weights
andbiases(c),andBayesianactivations(d).
averageoftheplainnetworks’parameterswiththeweightofeachnetworkbeingtheposteriorsestimatedbyitfor
thetrainingsamples. Accordingly, whateverderivedorconcludedfortheplainnetworkscouldbeextendedtothe
Bayesiannetworks. Inthefollowing,wesimplyreferredtotheplainneuralnetworkastheneuralnetwork. Sucha
networkdemandedanobjectivefunctionandaprocesstooptimizeitsparametersaswellasaregularizationtomitigate
overfitting. Acommonlyusedobjectivefunctionforsuchanetworkwasthecrossentropylossintroducedin(10). The
commonlyusedoptimizationprocesseswerebasedonthegradient(firstderivative)descentoftheobjectivefunction
[Kingma2015]. Theregularizationwasmostlydonebypenalizinglargeperceptrons’weightsordroppingperceptrons
oflowconfidentweightsinamethodcalledDropout[Gal2015,Jospin2022].
The gradient descent optimization relied on the fact that the opposite direction of the gradient (first derivative) of
the scalar field of the objective function pointed to the minimum of the function. Accordingly, in each iteration
i∈{1,··· ,n }ofthisoptimization,amovementinthedirectionofthenegativegradientoftheobjectivefunction
it
atthecurrentpointupdatedthenetwork’sparameters. Thisoptimizationhadalinearcomplexitywithregardtothe
numberofnetwork’sparameters. Thegradientateachiterationwastheaveragegradientofthetrainingsamplespassed
throughthenetwork’slayers. Thesamplescouldbepassedone-by-oneorallatonce. Theformerledtoastochastic
andthelatterledtoabatch-basedoptimization. Acompletepassthroughallthetrainingsampleswascalledanepoch
[Dean2012,Ruder2016,Goodfellow2016].
Theaveragingofthegradientsofthebatch’ssamplesresultedinasmoothvariationofthecostversustheiterations.
Inaddition,thebatch-basedoptimizationallowedtoapplyvectorizedandparallelizedoperations. However,itwas
restrictedtoconvexorrelativelysmootherrormanifoldsandcouldonlyfindlocalminima. Moreover,feedingalarge
batch of samples become memory intensive. The stochastic gradient descent optimization updated the network’s
parametersbypassingonesamplethroughthenetworkineachiteration. Thiscouldavoidmemoryissues,couldaddress
nonconvexoptimizations,andcouldevenfindglobalminima. However,duetoamorefrequentupdateofthenetwork’s
parametersitresultedinfluctuatingcostversustheiterations. Dependingonthesamples’gradientsthefluctuations
mightneverreachaminimumbutratherdancearoundit. Moreover,thestochasticoptimizationcouldnotbenefitfrom
thevectorizedortheparallelizedoperations.
Anintermediatebetweenthestochasticandthebatch-basedoptimizationwasamini-batch-basedoptimization. In
thisapproach,thetrainingsamplesgotdividedinton disjointbatches,i.e. T = ∪nbatchT . Then,ineach
batch train b=1 b
iterationi∈{1,··· ,n },thesamplesofonebatchgotpassedthroughthenetworkandtheaveragegradientofthese
it
samplesupdatedthenetwork’sparameters. Thesizeorthenumberofthebatcheswasahyperparameter. Thisway,by
adaptingthesizeorthenumberofthebatches,themini-batch-basedoptimizationcouldutilizethevectorizedandthe
parallelizableoperationstospeedupitscomputationswhilefittingthefluctuationsofthecostversustheiterationsto
thenonconvexityoftheaddressedproblem. Accordingly,ifn wasthenumberofepochs,thenthenetworkwas
epoch
optimizedbyn =(|T |/|T |)×n iterations. Ineachepoch,thebatchesandthesamplesofeachbatchgot
it train b epoch
randomlyshuffledtoavoidoverfittingtosomeofthesamples.
5
Withα ∈(0,1)beingthelearningrate(stepsize),η(i)beingthevectorofthemainparametersoftheneuralnetwork
lr
intheiterationi∈{1,··· ,n },and∇ (L)beingthegradientofagenericobjectivefunctionLwithregardtothese
it η(i)
parameters,wehad
η(i) =η(i−1)−α ·δ(i). (5)
lr
Inthegradientdescentoptimization,δ(i) =∇ (L). Thisresultedinaslowconvergenceandsensitivitytoabrupt
η(i−1)
variationsofthegradientduetonoiseandperturbations. Tospeeduptheconvergence,topropeloutoflocalminima,
andtosmoothoutthegradientvariations,inthemethodofmomentum,δ(i)gotdefinedtobeanexponentiallyweighted
movingaverage(firstmoment)ofthecurrentandpastgradients. Theaveragingweightwasadecayratecalledfirst
moment rate β ∈ [0,1). It emphasized the importance of recent gradients to the older ones. For β = 0, the
fm fm
momentumboileddowntothegradientdescent. Forβ = 1andα ≈ 0itresultedinendlessfluctuationsofthe
fm lr
costversustheiterationslikethemovementsofaballinafrictionlessbowl. Twomajorbottlenecksofthegradient
descentandthemomentumwerethepossibilityofbeingtrappedintosaddlepoints(i.e. pointsofzerogradientsin
alldirections)andaslowupdateinthedirectionsofsparsefeaturesofweakgradients. Totacklethese,theadaptive
gradientalgorithm(AdaGrad)definedδ(i)tobetheinstant(current)gradientdivided(normalized)bythesquareroot
ofthesumofthesquaredgradients. Thisscalingallowedtoavoidsaddlepointsandadaptedthegradientandthusthe
optimizationrateineachdirectiontoitshistoryofupdates. Thatis,themoreafeature(direction)wasupdatedinthe
pastthelessitwouldbeupdatedinthefuture.
Despiteoftheseimproves,theAdaGradwasslowsincethesumofthesquaredgradientsonlygrewbutnevershrank.
Thisgrowthalsoresultedinarapiddecayofδ(i) andthusapoorperformanceindealingwithnonconvexobjective
functionsanddensefeatures(directionsofstronggradients). Therootmeansquarepropagation(RMSprop)fixed
these issues by replacing the sum of the squared gradients with an exponentially weighted moving average of the
squaredgradients. Thiswascalledsecondmomentofthegradient. Theaveragingweightwasadecayratecalledthe
secondmomentrateβ ∈[0,1). Itemphasizedtheimportanceofrecentgradientstotheolderones. Moreover,inthe
sm
formationofδ(i),thedivision(normalization)oftheinstantgradientbythesecondmomentbalancedthestepsize.
Morespecifically,itdecreasedthestepsizeforlargegradientstopreventtheirexplosionandincreasedthestepsizefor
smallgradientstopreventtheirvanishing. Theexplodingandthevanishinggradientswerecommonissuesofdeep
neuralnetworks.
Theadaptivemomentestimation(Adam)combinedthemomentum(firstmoment)withtheRMSprop(secondmoment)
totakeadvantagesofboth. Thiswasdonebydefiningtheδ(i)tobethefirstmomentdivided(normalized)bythesecond
moment. Thisway,theAdamgottheconvergencespeedfromthemomentumandtheabilitytoadaptthegradientsin
differentdirectionsfromtheRMSprop[Kingma2015]. Morespecifically,
(cid:112)
δ(i) =mˆ(i)⊘ (cid:0)◦ vˆ(i)⊕10−8(cid:1) g(i) =∇ (L)
η(i−1)
biasedfirstmoment: m(i) =β ⊙m(i−1)⊕(1−β )⊙g(i)
fm fm
bias-correctedfirstmoment: mˆ(i) =m(i)⊘(1−βi ) (6)
fm
biasedsecondmoment: v(i) =β ⊙v(i−1)⊕(1−β )⊙g(i)⊙g(i)
sm sm
bias-correctedsecondmoment: vˆ(i) =v(i)⊘(1−βi ).
sm
Alltheaforementionedtechniquesreliedonthegradient(firstderivative)ofthescalarfieldoftheobjectivefunctionof
theneuralnetwork. ThesecondderivativeofthisscalarfieldwasrepresentedbyaHessianmatrix. Commonlyused
optimizationtechniquesbasedontheHessianmatrixweretheNewtonandthequasi-Newtonmethod,theconjugate
gradientmethod,andtheLevenberg-Marquardtalgorithm[Dean2012,Ruder2016]. Acommonwaytooptimizea
network’sparametersbyanyoneofthederivative-basedtechniqueswasabackpropagation. Thismethoddemandedthe
objectivefunctiontobeexpressedintermsofthenetwork’soutputs(goodnessofthemodel)andtobedifferentiable
withrespecttotheoutputsofeverylayer. Incaseofusingthegradientoftheobjectivefunctionwithrespecttothe
6
network’sparameters,thisgradientgotexpressedasaproductofthelayerwiseerrors. Then,thebackpropagationtook
thefollowingsteps:
• initializedthenetwork’sparameterswithrandomnumbers.
• passedabatchthroughallthelayersandcomputedtheoutputsofeverylayer.
• computedtheerroratthelastlayerbycomparingthepredictionswiththereferences.
• propagatedtheerrorfromthelastlayertothefirstlayertofindtheerrorofeachlayer.
• expressedthegradientoftheobjectivefunctionasaproductofthelayerwiseerrors.
• updatedthenetwork’sparametersaccordingto(5).
1.3 CommonlyUsedObjectiveFunctions
Foraprobabilisticestimate,theoutputsoftheneuralnetworkgotconvertedtoprobabilities(posteriors)byusinga
softmax(normalizedexponential)function. Thisfunctionconvertedavectortoanothervectorwhoseelementssummed
uptooneandeachelementoftheoutputhadamonotonicrelationshipwithanelementoftheinput. Inourcase,the
inputvectorwasthenetwork’soutputsforeachsampleandhadalengthofn =|L|. Thisway,theoutputofthe
clas
softmaxfunctioncouldbeinterpretedasacategoricalprobabilitydistributionofamultinomialclassificationovern
clas
mutuallyexclusiveclasses. Thatis,everysamplecouldonlyhaveonereferenceclassificationlabel. Aspecialcase
ofthesoftmaxfunctionwasthesigmoidfunction. Thisfunctionassumedthattheclasseswereindependentbutnot
mutuallyexclusive. Thus,everysamplecouldhavemultiplereferencelabels. Thesigmoidfunctioncastamultinomial
classificationintoaseriesofbinary(one-vs-all)classifications. Accordingly,itsoutputsdidnotnecessarilysumupto
one. Forasamplev ∈ T ⊆ T ,thenetwork’soutputsattheith iterationoftheoptimizationformedavector
b,j b train
z(i) =[z(i) ] . Then,theposteriorspˆ(i) =[pˆ(i) ] producedbyapplyingthesoftmaxfunctiontotheseoutputs
b,j b,j,c c∈L b,j b,j,c c∈L
were
pˆ(i) = exp (cid:0) z b ( , i j ) ,c (cid:1) ∈(0,1) with (cid:88) pˆ(i) =1. (7)
b,j,c (cid:80) exp (cid:0) z(i) (cid:1) b,j,c
k∈L b,j,k c∈L
Accordingly, if the training samples T ⊆ T were used to optimize the network’s parameters in the iteration
b train
i∈{1,··· ,n },thenL =[l ] =[l ] =[l ] wasthe|T |×n matrixofvectorizedreferencelabelsof
it b b,j j b,c c b,j,c j,c b clas
thesesamples,Z(i) =[z(i)] =[z(i) ] wasthe|T |×n matrixofthenetwork’soutputsforthesesamples,and
b b,j j b,j,c j,c b clas
Pˆ(i) =[pˆ(i)] =[pˆ(i) ] wasthe|T |×n matrixoftheirclassificationposteriorsestimatedbythenetwork.
b b,j j b,j,c j,c b clas
Ifthereference(groundtruth)labelsofthetrainingsamplesT wereprovidedatthetimeofoptimization(training),
train
thenforeachsamplev ∈T ⊆T thevectorl wasaone-hot-encodingofitsreferencelabell ∈Landwas
b,j b train b,j b,j
givenby
(cid:40)
1 ifc=l =referencelabelofv ∈T
b,j b,j b
l =[l ] with l = . (8)
b,j b,j,c c∈L b,j,c
0 otherwise
If the reference (ground truth) labels of the training samples T were not provided at the time of optimization
train
(training),thenforeachsamplev ∈T ⊆T thevectorl was
b,j b train b,j
1
l b,j =[l b,j,c ] c∈L = n ⊙1 nclas=|L| . (9)
clas
Foradiscriminativeneuralnetworkclassifieractingon|L|=n classes,acommonwaytoevaluatetheestimated
clas
posteriorsagainstthereferencelabelswastousethecrossentropylossintroducedin(3). Inthisapplication,thepolicies
πincorporatedin(3)representedthenetwork’sparameters. Eachstateswasaclassc∈Landeachobservationowasa
samplev ∈T ⊆T . Accordingly,p(s|π)=p(s)wastheoccurrenceprobabilityofaclass(state)swhichcould
b,j b train
berepresentedbythevectorizedreferencelabelsofthesamples(observations). Also,q(o|π)wastheclassification
7
posteriorestimatedbythenetwork’sparametersπ forthereferenceclassificationlabelofasample(observation)o.
Withthese,thecrossentropylossofthediscriminativeneuralnetworkclassifierbecome
L (Pˆ(i),L )= −1 (cid:88) (cid:88) l ·ln (cid:0) pˆ(i) (cid:1) . (10)
CE b b |L|·|T | b,j,c b,j,c
b j∈T bc∈L
Iftheposteriorsweregeneratedbythesoftmaxfunction,thenthislosswascalledasoftmaxcrossentropyloss. As
detailedin(2),thecrossentropylossresultedfromtheminimizationoftheVFEthroughminimizingtheKLdivergence
(dissimilarity)betweenthereferencedistributionp(·)andtheestimateddistributionq(·). Inacategoricalclassification,
thereferencedistributionp(·)wasthehistogramoftheclass-sampledistributionofthetrainingsamples. Theestimated
distributionq(·)wasaknownfunctionparametrizedwiththenetwork’sparameters. Thisway,thecrossentropyloss
andtheobjectivefunctionsoftheactiveinferencecomparedthedistributionsandthusweredistribution-based. Ifthe
class-sampledistributionofthetrainingsampleswasimbalanced,thenithadmaximaatthedominantclasses. These
maximaformedminimaofthecrossentropyloss. Thus,anyminimizerofthecrossentropylosscouldbetrappedinto
thoseminimaandcouldthusreturnclassificationsbiasedtowardsthedominantclassesofthetrainingsamples.
To reduce the impacts of the dominant classes on the optimization of a neural network, the cross entropy loss got
weightedand/ormodulated. Theresultinglossesincluded
(1) weightedcrossentropylosswhichweightedthecontributionofeachclassc∈Lbytheinverseofitsfrequency
w ∈(0,1)inthebatchT ⊆T and(optionally)weightedthecontributionofeachsamplev ∈T ⊆T
b,c b train b,j b train
byitsdistanced ∈ R totheborderofthenearestclassanditsdistanced ∈ R totheborderofthe
b,j,1 ≥0 b,j,2 ≥0
secondnearestclassthroughtheweightw ∈(0,1)[Ronneberger2015,Badrinarayanan2016]
b,j
L (Pˆ(i),L )= −1 (cid:88) (cid:88) w ·l ·ln (cid:0) pˆ(i) (cid:1) (11)
WCE b b |L|·|T | b,j,c b,j,c b,j,c
b j∈T bc∈L
(cid:80) |T | (cid:16) (d +d )2(cid:17)
w =w +w = k∈L b,k +w ·exp − b,j,1 b,j,2 (12)
b,j,c b,c b,j |T |+10−8 mo 2·σ2
b,c mo
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
wb,c∈(0,1) wb,j∈(0,1)
(cid:16) (cid:17)
withw =10,σ =5,and|T |=card {l =1} . Thedistancestotheclassificationborderscouldbe
mo mo b,c b,j,c
computedbyapplyingmorphologicaloperatorstothesamplesintheclassificationdomain,e.g. thespatialdomain
inanimagesegmentationtask.
(2) focal(modulatedcrossentropy)losswhichweightedthecontributionofeachclassbythedifficultyofclassifying
itssampleswiththedifficultiesbeinghighlightedwithamodulationfactorγ ∈ R . Thatis,thehigherthe
mod +
γ ∈ R was, the more the easy samples got downweighted to emphasize the role of the difficult samples
mod +
[Lin2018]
L (Pˆ(i),L )= −1 (cid:88) (cid:88)(cid:0) 1−pˆ(i) (cid:1)γmod ·l ·ln (cid:0) pˆ(i) (cid:1) . (13)
FL b b |L|·|T | b,j,c b,j,c b,j,c
b j∈T bc∈L
(3) weightedfocallosswhichadditionallyweightedthecontributionofeachclassc∈Lbytheinverseofitsfrequency
w ∈(0,1)inthebatchT ⊆T [Lin2018]
b,c b train
L (Pˆ(i),L )= −1 (cid:88) (cid:88) w · (cid:0) 1−pˆ(i) (cid:1)γmod ·l ·ln (cid:0) pˆ(i) (cid:1) . (14)
WFL b b |L|·|T | b,c b,j,c b,j,c b,j,c
b j∈T bc∈L
Theweightedcrossentropyandtheweightedfocallosshighlightedtheroleoftheminorityclassesovertheroleof
the majority classes by including the weight w ∈ (0,1) in their terms. This way, the more a class had training
b,c
samples,thelessitsclassificationerrorscontributedtotheoverallloss. Inaso-calledclass-balancedcrossentropyloss
[Cui2019],eachweightw ∈(0,1)gotdefinedbasedontheeffectivenumbern ∈(0,1)ofthetrainingsamplesof
b,c b,c
8
theclassc∈Linthefeaturespaceas
w =
(cid:20)
1−
n
b,c
−1 (cid:21)
/
(cid:20)
1−
(cid:16)n
b,c
−1(cid:17)|T b,c| (cid:21)
. (15)
b,c n n
b,c b,c
Thismethodassumedthateachsampleinthefeaturespacecoveredasubspaceandtheoverallsamples’subspaces
ofeachclassformeditsprototypicalsubspace. Then,thevolumeofthisprototypedefinedtheeffectivenumberof
theclass. However,inmostoftheapplications,thefeaturespacewashardlyaccessible. Inaneuralnetwork,itwas
alsovariableacrossthenetwork’slayers. Moreover,thecomputationofthesubspacecoveragesinthefeaturespace
wasexpensiveanddependingonthedimensionalityandthegeometryofthespace. Accordingly,in[Cui2019],each
numbern ∈(0,1)gothandledasahyperparameter.
b,c
Theaforementionedweightingandmodulationschemescouldreducetheimpactsofthedominantclassesoftheseen
(training)samplesonthenetwork’soptimization. However,theywerestillbasedonthecrossentropylossandthus
fittedthenetwork’smodeltotheseendistribution. Thiscouldcompromisethenetwork’sgeneralization(predictive
performanceonunseensamples)whenthedistributionoftheunseen(validationortest)samplesdifferedfromthe
distributionoftheseen(training)samples. Anobjectiveevaluationofaclassifieronunseensamplescouldbedone
throughseveralmetrics. Amongthesemetrics,theDicecoefficient(DICE)anditsequivalenttheJaccardindex(JI)
provided perceptual clues, scale invariance, and counts of false positive and false negative mispredictions. The JI
wasalsocalledtheintersectionoverunion(IoU)andtheDICEwastheF-β scorewithβ = 1. Thesemetricscould
becomputedwithalowcomplexity. Thisenabledtheirintegrationintoaniterativeoptimizationofneuralnetwork
classifiersintheformofmetric-basedlosses. Then,theoptimumnetwork’sparameterswerethemaximizersofthe
DICE[Milletari2016]ortheminimizersoftheJaccarddistance(JD)=1−JI=1−IoU[Bertels2019].
TheDICE=F-1scoreandtheJD=1−JI=1−IoUdirectlycomparedthebinarymasksofthepredictedandthereference
labels of the training samples without considering their distribution. This made the network’s model independent
ofanydistributionandcouldthustacklethedifferencesoftheseenandunseendistributions. However,thebinary
maskscomparedbythesemetricsgotformedfromdiscrete-valuedlabels. Thishinderedtointegratethosemetrics
intoacontinuousoptimizerwithbackpropagation. Morespecifically,thepredictedlabelsweretheresultsofapplying
an arg max operation to the classification posteriors pˆ(i) = [pˆ(i) ] estimated by the network. This operation
b,j b,j,c c∈L
was nonlinear, irreversible, and indifferentiable. Thus, to integrate the metrics into a continuous optimizer with
backpropagation,thenetwork’soutputsz(i) =[z(i) ] shouldbestoredineachiterationi∈{1,··· ,n }andfor
b,j b,j,c c∈L it
each sample v ∈ T ⊆ T . These storages got retrieved during the backpropagation and thus increased the
b,j b train
memoryfootprintofthenetworkandhinderedtooptimizealargenetworkwithalargenumberofsamplesperbatch
[Bertels2019].
Tointegratetheaforementionedmetricsintoacontinuousoptimizationframework,theyshouldbereplacedbytheir
continuousrelaxed(real-valued)surrogates. FortheDICE,thissurrogatecomparedthevectorizedreferencelabels
L = [l ] = [l ] = [l ] against the classification posteriors Pˆ(i) = [pˆ(i)] = [pˆ(i) ] estimated by the
b b,j j b,c c b,j,c j,c b b,j j b,j,c j,c
networkas
L (Pˆ(i),L )= 2 (cid:88) (cid:80) j∈T b l b,j,c ·pˆ b (i ,j ) ,c . (16)
DICE b b |L| c∈L (cid:80) j∈T b (cid:2) l b 2 ,j,c +pˆ( b i ,j )2 ,c (cid:3)
TheaboveDICElosswasreversibleanddifferentiableandcouldthusbeintegratedintoagradientdescentoptimization
withbackpropagation[Milletari2016]. However,itsnonconvexityhindereditswideuseinmanyapplications. Other
metricssuchasthemeansymmetricsurfacedistanceandtheHausdorffdistancewerealsononconvexbesidesbeingtoo
complexforaniterativeoptimizationprocess[Jadon2020]. Inaddition,eachdiscrete-valuedmetricwasasetfunction
mappingfromasetofmispredictionstoasetofrealnumbers. However,amongthem,onlythesetfunctionoftheJD
wassubmodular. ThisallowedtofindaconvexclosureoftheJDinapolynomialtime. Thisconvexclosurewasa
convexcontinuousrelaxed(real-valued)surrogatetakingnonnegativereal-valuedmispredictionsasinputs. Another
9
metricofthesepropertieswastheHammingdistance. TheconvexclosureoftheJDgotderivedaccordingtothesmooth
convexLovászextensionofsubmodularsetfunctions[Berman2018,Bertels2019]. TheJDwasdefinedas
|V ∪V |\|V ∩V | |V \V |+|V \V |
prd ref prd ref = prd ref ref prd .
Jaccarddistance(JD)=1−JI= |V ∪V | |V ∪V | (17)
prd ref prd ref
Based on this definition, the set function of the JD for the batch T ⊆ T and the class c ∈ L in the iteration
b train
i∈{1,··· ,n }was
it
(cid:16) (cid:17)
nnz M(i)
JD: M(
b
i
,c
) ∈{0,1}|T b| (cid:55)−→
nnz (cid:16) {l =1}∪
b,
{
c
ˆl(i) =1} (cid:17)
∈R (18a)
b,j,c b,j,c
(cid:40) 1 ifc=argmax {pˆ(i) }
with ˆl(i) = k b,j,k forming ˆl(i) =[ˆl(i) ] (18b)
b,j,c
0 otherwise
b,j b,j,c c∈L
and M(
b
i
,c
) = (cid:104)(cid:8) l
b,j,c
=1,ˆl
b
(i
,j
)
,c
̸=1 (cid:9) ∪ (cid:8) l
b,j,c
̸=1,ˆl
b
(i
,j
)
,c
=1 (cid:9)(cid:105) ∈{0,1}|T b| (18c)
beingthesetofmispredictionsdefinedoverthediscretehypercube{0,1}|T b|. Also,nnz(M(i))wasthenumberof
b,c
nonzeroelementsofthebinarysetM(i). ToformtheconvexcontinuoussurrogateoftheJD,firstM(i) ∈{0,1}|T b|
b,c b,c
shouldbereplacedbyanonnegativereal-valuedmispredictionvectorm(i) =[m(i) ] ∈R|T b|. Then,thesurrogate
b,c b,j,c j ≥0
shouldbefoundinR|T b|. ThissearchwasNP-hardunlesstheJDwassubmodular. AccordingtoProposition11in
≥0
[Yu2020],thesetfunctionJD:{0,1}|T b| (cid:55)−→Rwassubmodular. Thatis,
∀M
1
,M
2
∈{0,1}|T b| : JD(M
1
)+JD(M
2
)≥JD(M
1
∪M
2
)+JD(M
1
∩M
2
). (19)
Under this condition, the convex closure of JD : {0,1}|T b| (cid:55)−→ R in R|T b| was tight and continuous and could be
≥0
computedinapolynomialtime. ThisconvexclosurewascalledtheLovászextensionandwasgivenin[Lovász1983,
Fujishige1991]as
(cid:20) (cid:21)
JD: m(i) ∈R|T b| (cid:55)−→ 1 (cid:88) m(i) ·g (cid:0) m(i)(cid:1) ∈R
b,c ≥0 |T | b,j,c j b,c
b j∈T
b
(20)
with g
(cid:0) m(i)(cid:1)
=JD
(cid:0)
{u ,··· ,u }
(cid:1)
−JD
(cid:0)
{u ,··· ,u }
(cid:1)
j b,c 1 j 1 j−1
beingthejthelementofthegradientg (cid:0) m( b i ,c )(cid:1) and{u 1 ,··· ,u |T b| }denotingapermutationoftheelementsofm( b i ,c ) =
[m(i) ] indescendingorder,i.e. [m(i)] ≥ ··· ≥ [m(i)] . Thus,theJD (cid:0) m(i)(cid:1) wasaweightedaverageofthe
b,j,c j b,c u1 b,c u|Tb| b,c
elementsofthemispredictionvectorm(i) ∈R|T b|withtheweightsbeingtheelementsofthefirstderivative(gradient)
b,c ≥0
of JD withrespectto m(i) ∈ R|T b|. This way, theLovászextension JD interpolatedJD in R|T b| \{0,1}|T b| while
b,c ≥0 ≥0
havingthesamevaluesasJDon{0,1}|T b|[Bach2013,Berman2018].
Forabinaryclassification,themispredictionvectorm(i) =[m(i) ] ∈R|T b|wasgivenbym(i) =max (cid:2) (1−z(i) ·
b,c b,j,c j ≥0 b,j,c b,j,c
l ), 0 (cid:3) withz(i) =[z(i) ] beingthenetwork’soutputs(beforethesoftmaxfunction)attheithiterationforthe
b,j,c b,j b,j,c c∈L
samplev ∈T ⊆T . ThismispredictionvectorresultedinaconvexpiecewiselinearsurrogatecalledtheLovász
b,j b train
hingeloss[Yu2020].
Foramulticlassclassification,themispredictionvectorm(i) =[m(i) ] ∈R|T b|wasformedfromtheclassification
b,c b,j,c j ≥0
posteriorspˆ(i) =[pˆ(i) ] producedbythesoftmaxfunctionin(7). Thismispredictionvectorresultedinaconvex
b,j b,j,c c∈L
continuoussurrogatewithregardtothebatchT ⊆T andtheclassc∈Lintheiterationi∈{1,··· ,n }. Thus,
b train it
fortheclassificationovern =|L|classes,theoveralllosswasanaverageoftheseclass-specificsurrogates. This
clas
10
Figure2:Downsampling(left)andupsampling(right)intheV-net.
overalllosswascalledtheLovász-Softmaxlossandwasgivenin[Berman2018]as
L (Pˆ(i),L )= 1 (cid:88) (cid:88) m(i) ·g (cid:0) m(i)(cid:1)
LS b b |L|·|T | b,j,c j b,c
b c∈Lj∈T
b
(cid:40) 1−pˆ(i) ifc=l (21)
with m(i) =[m(i) ] ∈R|T b| and m(i) = b,j,c b,j,c ∈(0,1).
b,c b,j,c j ≥0 b,j,c pˆ(i) otherwise
b,j,c
ThecomputationoftheLovászextensionJDin(20)impliedtosorttheelementsofm(i) = [m(i) ] ∈ R|T b| and
b,c b,j,c j ≥0
to call the JD with the permutation order. The sort had a complexity of O (cid:0) |T | · log(|T |) (cid:1) and the call had a
b b
complexityofO(|T |). However,bykeepingatrackofthecumulativenumberoffalsepositiveandfalsenegative
b
mispredictions,thecomplexityofthecallcouldbeamortizedtoO(1). Thatis,ineachiteration,insteadofcomputing
thegradientfromscratchonlythegradientgotupdated. Inthiscase,theoverallcomplexityofcomputing(20)become
O (cid:0) |T | · log(|T |) (cid:1) . The procedure of computing the gradient of the Lovász-Softmax loss in (21) was given by
b b
Algorithm1in[Berman2018].
TheconvexityandthedifferentiabilityoftheLovász-Softmaxlossin(21)allowedtouseitasanobjectivefunctionfor
optimizingadiscriminativeneuralnetworkclassifierbyagradientdescentoptimizerwithbackpropagation. Also,the
operationsinvolvedinitscomputationweredifferentiableandimplementableongraphicsprocessingunits(GPUs).
1.4 BaselineArchitecture
Each convolutional layer of a neural network could extract features of a certain resolution while being capable of
downsamplingorreducingthespatialresolutionbyusinganappropriatestride. Theseallowedtolearnhierarchical
(multiresolution)featuresbycascadingmultipleconvolutionallayers. Theoppositeofaconvolutionallayerwasa
transposedconvolutionaloradeconvolutionallayerofsimilarfeaturelearningcapabilitybutaninherentupsamplingor
increaseofthespatialresolution. Byfollowingtheconvolutionallayerswiththedeconvolutionallayersanencoder-
decoderarchitecturewasobtained. Theencoderwasadownsampler,acompressor,oracontractorperforminganalysis.
Thedecoderwasanupsampler,adecompressor,oranexpanderperformingsynthesis. Eachencoder/decoderwas
composedofmultiplestages. Eachstageprocessedfeaturesofacertainresolutionthroughoneormoreconvolution-
al/deconvolutionallayersandthendownsampled/upsampleditsnewlycomputedfeaturestothenextresolution. To
avoidlossofinformationduetothedownsampling,ineachencoderstage,thenumberofthenewlycomputedfeatures
gotmultipliedbythedownsamplingrate. Conversely,ineachdecoderstage,thenumberofthenewlycomputedfeatures
gotdividedbytheupsamplingrate.
Awidelyusedneuralnetworkofsuchanencoder-decoderarchitecturewastheU-net. Astheinputspassedthroughits
encoderstages,theprogressivelyexpandingreceptivefieldsofitsconvolutionallayersincreasedtheabstractionandthe
contextofitsextractedfeatures. Thus,attheendoftheencoderorbottomoftheU,featuresofminimumresolutionbut
maximumabstractionandcontextwereobtained. Thespatialresolutionofthesefeaturesgotreconstructedbypassing
11
Table1:ThereceptivefieldsandthesizesofthefeaturemapsatdifferentstagesoftheV-net.
ReceptiveField
Stage SizeofFeatureMaps
Encoder Decoder
1 5×5×5 551×551×551 128×352×256
2 22×22×22 546×546×546 64×176×128
3 72×72×72 528×528×528 32×88×64
4 172×172×172 476×476×476 16×44×32
5 372×372×372 372×372×372 8×22×16
themthroughthedeconvolutionallayersofthedecoderstagesandcombiningthemwithoriginalhigherresolution
features. Theoriginalfeaturesweredirectlyobtainedfromthecorrespondingencoderstagethroughaskipconnection.
Thatis, featuresextractedbyeachencoderstagegotforwardedtothecorrespondingdecoderstagetocompensate
informationlossduetothedownsampling.Thisfeatureforwardingcouldenhancethedelineationofboundariesbetween
differentclassesandspeduptheconvergenceoftheoptimization. Attheendofthedecoder,theresultingfeaturemaps
hadaresolutionandsizeliketheinputofthenetwork. Aweightedaverageofthesefeaturemapscombinedtheminto
thedesirednumberofclasses. Thiswasdonebypassingthemthroughaconvolutionallayerof1×1×1kernelsize,0
padding,andstrideof1ineachdimension. Asgivenby(7),theresultingnetwork’soutputsgotthenpassedthrougha
softmaxfunctiontoproducetheestimatedclassificationposteriorsforthesamples[Ronneberger2015].
ThedownsamplingandtheupsamplingoftheU-netmadeitahierarchicalarchitecturecapableofcapturing,analyzing,
andsynthesizingfeaturesatdifferentspatialresolutions. Thisway,theU-netcouldautomaticallyextractlocaland
contextualpatterns. Thelocalpatternsgotcapturedbytheshallowerlayersandthecontextualpatternsbythedeeper
layersofalargerreceptivefield.Attheend,thedecodersynthesized(gatheredandassembled)thelocal(highresolution)
andthecontextual(lowresolution)featuresintothefinalclassification. Theseenabledalocalizationaswellasan
accurateclassificationinanydomainofanysizeandthusmadetheU-netabreakthroughforend-to-endoptimizations.
Moreover,makingalltheoperationsoftheU-net3Dallowedtoapplyitto3Dvolumetricdomains. The3DU-net
gotenhancedbymakingitsencoderstagesresidual. Thatis,theinputofeachencoderstagegotaddedtoitsoutput.
Thiscouldmitigatevanishinggradientsandspeeduptheconvergenceoftheoptimization[He2016a]. Inaddition,
the3DU-netcouldlearn3Dvolumetricstructuresoutofsparselyannotated2Dslices. Thisallowedtouseitina
semi-automatedannotationprocessaswellasafullyautomated3Ddetection[Çiçek2016,Rakhlin2018].
Inthe3DU-net,eachdownsampling/upsamplinghadafactorof2andwasdonethroughamax-pooling/unpooligover
a2×2×2kernelwithastrideof2ineachdimension. Also,eachconvolutionallayerapplied0padding. Thus,the
validpartofeachfeaturemapattheoutputofeachconvolutionallayerhadasmallersizethanitsinputfeaturemap.
Inaddition,the3DU-netlearnedtheresidualfunctionsonlyinitsencoderstages. Inaso-calledV-net,the3DU-net
becomefullyconvolutionalbyapplyingeachdownsampling/upsamplingthroughaconvolutional/deconvolutional
layerofakernelsizeof2×2×2,a0padding,andastrideof2ineachdimension. Toavoidlossofinformation,each
downsamplingdoubledthenumberoffeaturemaps. Conversely,eachupsamplinghalvedthenumberoffeaturemaps.
Figure2showsthedownsamplingandtheupsamplingintheV-net.
Incontrasttothemax-pooling/unpooligoperations,theconvolution/deconvolution-baseddownsampling/upsampling
wasreversibleanddifferentiable. Theseallowedtobackpropagateeachdownsampling/upsamplingwithoutneedingto
storeitsinputspersampleanditeration. Thisway,thememoryfootprintoftheV-netbecomemuchlessthanthe3D
U-netwhiletheanalysisandcomprehensionofitsinternalprocessgotsimplified. Moreover,eachconvolutionofthe
V-netappliedanappropriatepaddingtomakethefeaturemapsatitsoutputofthesamesizeasitsinput. Furthermore,
theV-netlearnedtheresidualfunctionsnotonlyintheencoderstagesbutalsointhedecoderstages. Thisfurther
boosteditsperformanceandspedupitsoptimization[Milletari2016]. Thisway,the3DU-netortheV-netgotwidely
usedinmanyapplications[Rakhlin2018,Li2022]. Accordingly,weresortedtoanend-to-endoptimizationofthe3D
fullyconvolutionalandresidualV-netforourimplementationsandevaluations. Forthis,wetailoredthenumberand
12
Figure3:DifferentnormalizationtechniquesappliedtoafeaturemapofsizeN×D×H×W×CwithNdenotingthenumberof
batches,Cdenotingthenumberofchannels,andD×H×Wdenotingthespatialdimensions.Ineachcase,thebluevoxels
gotnormalizedbythesamemeanandvarianceaggregatedacrossthem.
thesizesofthefeaturemapsandthekernelsoftheconvolutional/deconvolutionallayerstoourvolumetricfat-water
images. Also,throughthenetwork,weprocessedthedatainanN×D×H×W×CformatwithN=|T |beingthenumber
b
ofthevolumetricfat-waterimagesineachbatch,Cbeingthenumberofthefeaturemaps,Dbeingthedepth,Hbeing
theheight,andWbeingthewidthofeachfeaturemap. Wetrained(optimized)theV-netbyusingamini-batch-based
gradientdescentoptimizerwithbackpropagationandasufficientlylargeinputvolumetocaptureasmuchcontextual
informationaspossible. DuetothememorylimitationsoftheusedGPU,wecouldonlyinclude2volumetricfat-water
imagesineachbatch. Moreover,eachvolumetricfat-waterimagehad2channelscontainingitsvoxelwisefatandwater
intensities. Accordingly,attheinputofthenetwork,N×D×H×W×C=2×128×352×256×2.
Eachencoder/decoderstageoftheV-netextractedandlearnedfeaturesofacertainspatialresolutionbyusingone
tothree3D(volumetric)convolutional/deconvolutionallayers. Inourcase,eachoftheselayershadakernelsizeof
5×5×5,apaddingof2,andastrideof1ineachdimension. Also,regardingthesizeofourimagesandthesizes
oftheaddressedobjects(tissues)inoursegmentations,wefound5stages(resolutionlevels)tobesufficientforour
hierarchicalfeaturelearning. Table1showsthereceptivefieldsandthesizesofthefeaturemapsatdifferentstages. As
canbeseen,theinnermost(deepest)stageofthenetworkcouldalreadycapturetheentirecontextoftheinputvolume.
Thisallowedtoperceivethewholeanatomyofinterestandensuredaccesstoenoughcontextualinformationforreliably
classifyingeachvoxelattheoutputoftheneuralnetworkclassifier.
Besidestheconvolutional/deconvolutionallayers,eachresidualencoder/decoderstagenormalizeditsfeaturemaps
andappliednonlinearitiestothem. LiketheoriginalV-net,weusedaparametricrectifiedlinearunit(PReLU)witha
parametera ∈R foreachnonlinearactivation. Theparametera ∈R controlledtheoutputsfornegative
prelu ≥0 prelu ≥0
inputsandthuswascalledthecoefficientofleakage. Itgotoptimizedalongwiththemainparameters(weightsand
biases)ofthenetwork. Thenormalizationofthefeaturemapsdecoupledthelengthsofthenetwork’sgradientsfrom
theirdirections. Thiscouldacceleratetheconvergenceoftheoptimizationsandthusallowedhigherlearningrates. It
couldalsostabilizetheoptimizationsbymitigatingtheinternalcovariateshift1,enhancingtherobustnessagainstthe
initializations,andsmoothingtheobjectivefunction. Moreover,itcouldpenalizelargenetwork’sweightsandthereby
reducetheoverfittingorimprovethegeneralization.
WemodifiedtheV-netbychangingthetypeofthenormalizationfrombatchnormalization[Ioffe2015]toinstance
(contrast)normalization[Ulyanov2016]. Thecommonlyusedbatchnormalizationwasbasedonmini-batchstatistics.
Thatis,duringthetraining,themeanandthevarianceofeachfeaturemapofeachbatchgotlearnedacrossallthe
dimensions(D,H,W)andalltheNmembersofthebatchtonormalize(removebiasandscaleof)thecorresponding
featuremapintheevaluationphase. Theinstancenormalizationtookasimilarapproach. However,itcomputedthe
meanandthevarianceofeachfeaturemapofeachbatchonlyacrossthedimensions(D,H,W).Incaseofhavingasmall
1changesofstochasticdistributionsoftheinputsofeachlayerofthenetworkduetothechangesoftheparametersoftheprevious
layers
13
Figure4:Aresidualencoder/decoderstagecomprising2convolutional/deconvolutionallayers(a)withthenew(b)andtheoriginal
order(c)oftheoperations.
batchsize,likeourcase,theexponentialmovingaveragesofthemeanandthevarianceofeachfeaturemapofeach
batchhadstrongfluctuationsacrossthetrainingiterations. Thiswasduetothepoorstatisticalpowerofthesmallbatch
andtherebymadethebatchnormalizationineffective. Inthiscase,theinstancenormalizationwasmoreeffectiveand
consistent[Ulyanov2016]. Othervarietiesofthenormalizationwerethelayerandthegroupnormalization[Wu2020].
Figure3showstheirdifferencestothebatchandtheinstancenormalization.
WealsomodifiedtheV-netbychangingtheorderofoperationsineachresidualencoder/decoderstage. Insteadof
the convention of applying the normalization between the convolution/deconvolution and the nonlinear activation,
as suggested in [He2016b], we applied a full preactivation normalization and removed after-addition activation.
Figure4comparesthenewandtheoriginalordersoftheoperationsofaresidualencoder/decoderstagecomprising2
convolutional/deconvolutionallayers. Theadvantageoftheneworderwasthatitmadetheoverallnonlinearfunctionof
eachstagearealidentitymapping. Thisenabledadirectandcleanpropagationofsignalsfromonestagetoanother
stageinbothforwardandbackwarddirections. Otherkindsofskipconnectionswhichinvolvedasortofscaling(likethe
Dropout),gating,orconvolution/deconvolutiononthesignalpathcouldhamperacleanpropagationoftheinformation
andthusleadtooptimizationproblems. Moreover,thenewordercouldimprovethegeneralizationofthenetwork’s
modelbyreducingitsoverfitting. Thatis,itincreasedtheerroronseen(training)samplesbutreducedtheerroron
unseen(validationortest)samples. Furthermore,intheoriginalorder,additionoftheshortcuttothenormalizedsignal
madetheoverallsignalattheinputofthelastnonlinearactivationunnormalized. However,intheneworder,thesignal
attheinputofeachnonlinearactivationwasnormalized. Figure5showsthedescribedV-netarchitecture.
Tomitigateoverfittingandtheimbalancedclass-sampledistributionofthetrainingsamples,attentionmechanisms
got proposed. These methods aimed to focus the attention of the network’s parameters on important (foreground)
minorityclasses. Thisattentioncouldreducethetrainingsamplestoaneffectivesubsetofalowerunbalancednessthan
theoriginalset. Itcouldalsovanishtheredundantorirrelevantnetwork’sparametersbysuppressingfeatureactivations
inirrelevantregionsoftheclassificationdomain. Theseinturnreducedtheoverfittingandspeduptheconvergenceof
thenetwork’soptimization. Theattentioncouldbestimulatedbyincorporatingpriorsintotheoptimizationprocess
and/ormodifyingthenetwork’sarchitecture. Neitherthecrossentropy-basednorthemetric-basedlosses,definedin
subsection1.3,couldaccommodatethepriorsofthesamples. Consequently,theattentionmechanismswererestricted
toarchitecturalmodifications.
Trainable (optimizable) attention mechanisms were categorized as hard or soft. The hard attention mechanisms
iterativelycroppedaregionofinterestthroughaMonteCarlosamplingoptimizedbyareinforcementlearning. These
14
Figure5:Schematicofthe3DfullyconvolutionalandresidualV-netwiththeencoderandthedecoderstagesonitsleftandright
side,respectively.
sampling-basedupdateswereindifferentiableandthushardtooptimize. Thesoftattentionmechanismsinvolveda
differentiablemodelcomposedofreal-valuedparameters. Thus,theycouldbeoptimizedthroughagradientdescent
optimizerwithbackpropagation. Theoutputofthesoftattentionmodelforeachfeaturemapwasaprobabilisticmap
15
calledattentionmap. Inanadditiveoramultiplicativeattentionmechanismthismapgotcomputedbyaddingor
multiplyingthefilteredfeaturemap(s)byafilteredgatingmap,respectively. Iftheattentionmapwascommutedby
aconvolutionalneuralnetwork(CNN),theneachfilterwasaconvolutionallayer. Theattentionmechanismturned
intoaself-attentionifthegatingmapswereproducedinternally. Theelementwisemultiplicationoradditionofeach
attentionmapwithitscorrespondingfeaturemaphighlightedsalientfeaturesfortheclassification. Thisenabledan
attention-basedfeaturepoolingorpruning.Ifthegatingmapsbroughtcontextualinformation,thenthefeaturepooling
waswithregardtothecontextualdependenciesofthefeatures. Besidesmitigatingtheoverfittingandtheimbalanced
class-sampledistributionofthetrainingsamples,theattention-basedfeaturepoolingcouldenhancethesensitivity,
thepredictionaccuracy,andtherobustnessoftheneuralnetworkclassifier. Acommonlyusedarchitectureforsoft
attentionwasaregionproposingfeed-forwardCNN.Abottleneckofthisapproachwasitsexcessiveandredundantuse
ofthemodel’sparametersandfeatures. Thiscouldincreasetheoveralloptimizationoverheadandtheoverfittingbefore
theconvergenceoftheoptimizationcouldrealizeanyattentionforapossiblereductionofthenetwork’sparameters
[Oktay2018].
Asmentionedearlier,theU-netandtheV-netwerecapableofextracting(analyzing)andreconstructing(synthesizing)
multiresolution(multiscale)features. Thiswasdonebyextractingcoarserfeaturesthroughdownsamplingthefeature
mapsacrosstheencoderstagesandthenreconstructingfiner(higherresolution)featuresacrossthedecoderstages. To
thisend,thereceptivefieldatthecoarsestresolutionwastobelargeenoughtocaptureallthecontextualinformation
highlighting the overall category and location of the foreground classes. After the localization, the finer (higher
resolution)featuresdelineatedboundariesbetweendifferentclassesmoreprecisely. Thesealtogetherallowedtocapture
largeshapeandsizevariationsintheclassificationdomainandthusimprovedtheclassificationaccuracy.
Thereconstructionofthefiner(higherresolution)featuresineachdecoderstagewaswiththehelpofthefeatures
extractedbythecorrespondingencoderstageatthesamespatialresolution. Thisfeatureforwardingreducedredundant
andrepeatedcomputationofthefeaturesandthusenhancedefficiencyintheusageofthecomputationalpowerand
memory. Theplainskipconnectionofthefeatureforwardingpathcouldbereplacedbyanattentiongaterealizingan
attention-basedfeaturepooling.Thispoolingvanishedredundantfeaturesrightbeforetheconcatenationoftheoriginal
featureswiththereconstructedfeatures. Thisway,itcouldsuppressirrelevantregionsintheclassificationdomainby
vanishingredundantnetwork’sperceptrons.Thisinturnreducedtheoverfittingofthenetworkandtheunbalancednessof
thesamples’distributionseenatthetimeofitstraining(optimization).Furthermore,thecomputationaloverheadofsuch
anattentiongatewasmuchlowerthantheregionproposingCNN.Thisandthereductionofthenetwork’sparameters
couldreducethecomputationalcomplexityoftheoptimizationsandspeeduptheirconvergence[Oktay2018].
Apromisingself-attentionmechanismforintegrationintoeachfeatureforwardingpathoftheU-netortheV-netwasa
grid-basedgatingmodule. Inthisapproach,eachgatingmapwasnotfixedacrosstheelementsofitscorresponding
featuremapsforwhichtheattentionmapsweretobecomputed. Instead,itwasafeaturemapofalower(coarser)
resolutionalreadygeneratedbythenetworkitself. Thisway,theresultingattentionmapsweregrid-based(i.e. variable
acrosstheelementsofthefeaturemaps)andcouldthushighlightsalientfeatureswithrespecttolocalpatterns. The
gatingbasedonthefeaturemapsofalower(coarser)resolutionallowedtoconsiderabiggercontextinthefeature
poolingandtherebydisambiguatedirrelevantandnoisyfeatures. Moreover,thegrid-basedgatingmoduleeliminated
theneedtoanexternalexplicitregionproposingCNNbyimplicitlyproposingsoft(probabilistic)mapofthetarget
structuresonthefly. Thisattentionmechanismcouldbetrainedfromscratchtofocusonthetargetstructuresofvarying
shapesandsizeswithoutadditionalsupervision. Itsfilters(lineartransformations)downweightedthegradientsfrom
irrelevantregionsandcouldthusbeimplementedthroughconvolutionallayersfilteringthenetwork’sactivationsin
bothforwardandbackwardpasses[Oktay2018].
In [Oktay2018, Zuo2021, Li2022], to reduce the number of the parameters and the computational complexity of
the attention gates, each filter was a convolutional layer of 0 padding and 1×1×1 kernel size, i.e. without any
spatialsupport. Todownsampletheinputfeaturemapsofeachattentiongatetotheresolutionofitsgatingmaps,the
convolutionalfiltersofthefeaturemapshadastrideof2ineachdimension. Moreover,eachattentiongatehandleda
16
Figure6:Schematicoftheoriginal(upperrow)andtheproposed(lowerrow)grid-basedgatingmodulewithFdenotingthetensor
oftheinputfeaturemaps,Gdenotingthetensorofthegatingmaps,Adenotingthetensoroftheattentionmaps,F′
denotingthetensoroftheoutputfeaturemaps,andeachblueboxdepictingaconvolutionallayer.
binaryclassificationandthuscomputedacommonattentionmapforallthefeaturemapsatitsinput. Tothisend,the
downsamplingconvolutionalfiltersofthefeaturemapslinearlytransformedthemtoanintermediatenumberoffeature
mapsdenotedbyC’.Also,theconvolutionalfiltersofthegatingmapslinearlytransformedthemtoC’intermediate
maps. Theintermediatefeature/gatingmapsweretobemoresemanticallydiscriminativethantheoriginalfeature/gating
mapsinlocalizingthetargetstructures. Thus,thenumberC’wasaresolution-specifichyperparameterandneededtobe
optimizedforeachattentiongateseparately. Then,accordingtoanadditiveattentionmechanism,theintermediate
downsampledfeaturemapsgotaddedtotheintermediategatingmapsandthenpassedthroughanonlinearrectified
linearunit(ReLU),a1×1×1convolutionallayerof0paddingandastrideof1,andanonlinearSigmoidlayerto
formtheattentionmapforalltheinputfeaturemaps. Thisattentionmaphadalowerresolutionthantheinputfeature
mapsandthuswasupsampledbyagrid-basedtrilinearinterpolationtothesameresolutionastheinputfeaturemaps. In
comparisontoamultiplicativeattention,theadditiveattentionwasmorecomputationallydemandingbutmoreeffective
inenhancingtheclassificationaccuracy.
To handle a multiclass classification over n = |L| classes, we modified the aforementioned gating module by
clas
replacing the nonlinear Sigmoid function with a nonlinear Softmax function. Also, after the ReLU operation, the
1×1×1convolutionallayerdidnotmaptheoutputsoftheReLUtoonechannelrathertothenumberoffeature
mapsattheinputofthegatingmodule. Thatis,insteadofcomputingonecommonattentionmapforalltheinput
featuremaps,wecomputedanattentionmapforeachfeaturemapseparatelyandindependentlyfromotherfeature
maps. Furthermore,tosimplifythenetwork’soptimizationweeliminatedtheresolution-specifichyperparameterC’
definingthenumberoftheintermediatefeature/gatingmaps. Tothisend,the1×1×1convolutionallayerdirectly
appliedtotheinputfeaturemapstransferredthemtothenumberofchannelsalreadyexistingintheinputgatingmaps.
Thisinturneliminatedthe1×1×1convolutionallayerdirectlyappliedtotheinputgatingmapsandthusfurther
simplifiedthearchitectureofthegatingmodule. Figure6comparestheoriginalgatingmodulewithourproposedone
andFigure7showstheV-netarchitecturewithsuchagatingmoduleineachofitsfeatureforwardingpaths.
Toreducetheoverfittingofthebaselinearchitecturestotheseen(training)samplesandtherebyimprovethegeneraliza-
tion(predictiveperformanceonunseensamples),weappliedDropouttoeveryperceptron(node)ofthesearchitectures.
ThistechniquehadacommonrootwithaBayesianneuralnetworkwhich,asdescribedinsubsection1.2,wasanensem-
bleofplainneuralnetworks. Inthetraining(optimization)phase,theDropoutdroppedsomeoftheperceptrons(nodes)
ofthenetworkbyvanishingtheirincomingandoutgoingweights. Thekeep(retention)probabilityofeachperceptron
(node)wastheoccurrenceprobabilityofaBernoullidistributedrandomvariable. Thisprobabilitywashandledlikea
17
Figure7:Schematicofthe3DfullyconvolutionalandresidualV-netwiththegrid-basedgatingmoduleineachofitsfeature
forwardingpaths.
tunablehyperparameterindicatingtheconfidence(inverseofthevariance)ofthenode’sestimations. Weconsidereda
commonretentionprobabilityforalltheperceptrons(nodes)ofeachencoder/decoderstageofthebaselinearchitectures.
Forthesth encoder/decoderstage,thisprobabilitywasdenotedbyp ∈ [0,1]. Inthetestphase,alltheperceptrons
s
(nodes)ofthenetworkwerekept.However,theoutgoingweightsofeachnodegotmultipliedbyitsretentionprobability
18
optimizedduringthehyperparameteroptimization. TheDropoutwasshowntobesuperiortootherregularization
techniquessuchastheweightdecaywhichpenalizedtheweightsoflargel norms. Thissuperioritycomeatthecostof
2
ahighernumberofiterationsforconvergenceoftheoptimizations[Srivastava2014,Gal2015,Jospin2022].
2 OutlineofContributions
All the metric-based losses introduced in subsection 1.3 were independent of the class-sample distribution of the
trainingsamplesandcouldthusenhancethegeneralization(predictiveperformanceonunseensamples)ofaneural
networktrained(optimized)withthem. However,themetricsinvolvedinthoselosseswerebinaryclassificationmetrics.
Thisimpliedtodecomposeamulticlassclassificationintoaseriesofone-vs-allclassificationsandthenformitsoverall
lossfromanaverageoftheone-vs-alllosses. ThiswasobservableinthedefinitionoftheDICElossin(16)andthe
Lovász-Softmaxlossin(21).
Theaveragingacrosstheclassescouldnaturallyleadtoabiastowardsthedominantclasses,i.e.classesofmoresamples.
Thisbiascouldnotbemitigatedbyaweightingmechanismsuchastheonesincorporatedinthedistribution-basedlosses
introducedinpage8andpage8. Thereasonwasthatsuchaweightingcoulddiminishthefalsepositivemispredictions
ondominantclassesandcouldthusmisleadtheoptimization. Moreover,ifaclasswasabsentinboththereference
labelsandthepredictedlabels,thenDICE=JI=1andJD=0.
Allthedistribution-basedlossesintroducedinsubsection1.3werebasedonthecrossentropyandhadacommonroot
withthevariationalfreeenergy(VFE)ofaretrospectiveactiveinference. Theselossesfittedthenetwork’smodeltothe
class-sampledistributionofthetrainingsamplesandcouldthuscompromisethenetwork’sgeneralizationwhenthe
distributionofunseen(validationortest)samplesdifferedfromthedistributionoftheseen(training)samples. However,
asdescribedinpage8andpage8,theselossescouldreducetheclassificationbiasestowardsthedominantclasses
byweightingeachclass’stermwithregardtoitsnumberofsamplesorimportance. Inspiteofthiscapability,there
existednooptimalweightingwhichcouldbeincorporatedintothecrossentropy-basedlossestomakethemequivalent
toanyofthemetric-basedlosses. Thus,tobenefitfromtheadvantagesofthecrossentropy-basedandthemetric-based
losseswhilemitigatingtheirdrawbacks,acombinationofthemwasused. Alternatively,toreducetheoverfittingand
thustoimprovethegeneralizationofthecrossentropy-basedlosses,additionalco-trainingwithaugmentedtraining
samples got conducted. Also, to reduce the classification biases towards the dominant classes, the false positive
mispredictionsofthenetworktrainedwiththemetric-basedlossesgotpost-correctedbyusingmorphologicaloperations
[Isensee2018,Bertels2019,Jadon2020,Chen2022].
Despiteofsomeimproves,alltheaforementionedschemesimposedextraoverheadstothetrainingorpredictionsofthe
neuralnetworks. Inaddition,theaugmentationofthetrainingsamplesobtainedfromimageswasmostlydoneonthe
flybyapplyinggamma(luminance)modifications,mirroring,randomscaling,randomrotation,andrandomelastic
deformation1totheoriginalimages. Thesetechniquescouldnotbeeasilyappliedtomedicalimageswherepathological
alterationsshouldbedifferentiatedfromtheaugmentations. Moreover,noneoftheaforementionedschemescould
completelymitigatetheoverfittingofalargenetworktoalimitednumberofthetrainingsamplesortheclassification
biasestowardsthedominantclasses. Furthermore,noneofthedescribedlossescouldincorporatepriorsorhandle
errorsoruncertaintiesinthereferencelabelsofthetrainingsamples[Lo2021]. Errorsinthereferencelabelsof
thetrainingsamplescouldarisefromhumanerrorsinthemanualannotationsofthetrainingsamplesandimagesorthe
errorsinducedbynoiseandartifacts. Uncertaintiesandambiguitiesinthereferencelabelsofthetrainingsamplescould
stemfromsimilarfeaturesandtexturesofdifferentclasses. Thesesimilaritiesnotonlyconfusedthemanualannotators
butalsotheneuralnetworkrelyingonthosefeaturesandtexturesforlearningboundariesbetweendifferentclasses.
Tomitigatetheaforementionedbottlenecks,weproposed
1TheelasticdeformationswereobtainedfromaB-splineinterpolationoveragridofcontrolpointsonadensedeformationfield.
19
(1) anovelalgorithm,basedonthegeneralized(multinomial)Kellycriterionforoptimalbetting,torecomputethe
referencelabelsofthetrainingsamplesbyusingtheirpriorsandthecurrentlyestimatedclassificationposteriorson
thenetwork;
(2) anovelobjectivefunction,basedontheexpectedfreeenergy(EFE)ofaprospectiveactiveinference,withthe
capabilityof
• incorporating prior probabilities of the training samples to focus the attention of the neural network on
importantbutminorityforegroundclassesandtherebyreshapetheeffectivelyseendistributionforareduction
oftheclass-sampleunbalancedness,theoverfitting,andtheclassificationbiasestowardsthedominantclasses;
• representingtheprecisionandrecallmetricsbyitstermstoenhancetherobustnessofthenetwork’soptimiza-
tionagainsttheclass-sampleunbalancedness;
(3) aprocesstointegratetheproposedalgorithmandtheproposedobjectivefunctionintoamini-batch-basedgradient
descentoptimizerwithbackpropagation.
TheproposedalgorithmforrecomputingthereferencelabelswaslistedinAlgorithm1. Thisalgorithmcalculateda
setofcandidatelabelsforeachtrainingsamplefromitspriorandcurrentlyestimatedposteriorprobabilitiesonthe
network. Thisalgorithmresultedfromourreformulationofthegeneralized(multinomial)Kellycriterionforoptimal
bettingonmultiplehorsesinahorserace. ThisreformulationcastthegeneralizedKellycriterionintoamulticlass
classificationproblembyinterpretingeachtrainingsampleasabettor,eachclassasahorse,andeachiterationofthe
network’soptimizationasahorserace. Then,theclassificationpriorofthetrainingsamplewithregardtoeachclass
becomethewinprobabilityofthecorrespondinghorse. Theclassificationposteriorcurrentlyestimatedbythenetwork
forthetrainingsamplewithregardtothesameclassbecomethebeliefprobabilityofthecorrespondinghorse. The
proposedsetsofcandidatelabelsgotthenpluggedintotheproposedobjectivefunctiontoformthecurrentlossforan
update(optimization)ofthenetwork’sparametersinthecurrentiteration. Thus,insteadofareferencelabel,asetof
candidatelabelsgotconsideredforeachtrainingsampleineachiteration.
Thisconsiderationallowedtomitigatetheaforementioneduncertaintiesandambiguitiesinthelabelsgeneratedfrom
manualannotationsinthepresenceofnoise,artifacts,andsimilarfeaturesortexturesofdifferentclasses. Inother
words,thesetsofcandidatelabelscouldhandlepossibleoverlapsbetweendifferentclassesandthusenhancedthe
reliabilityandtheflexibilityoftheneuralnetwork’soptimization. Morespecifically,thesesetscouldhelpagradient
descentoptimizertoescapefromlocaloptimumscausedbytheoriginalreferencelabels. Moreover,ifthereference
labelsofsometrainingsamplesweremissing,thentheircandidatelabelscouldstillbecomputedfromtheirpriorsand
posteriors. Thissemi-supervisedoptimizationwasofparticularimportanceintheapplicationswherethemanual
annotationsofthereferencelabelswerecostlyandcumbersome.
OurproposedAlgorithm1forfindingthecandidatelabelsaimedtominimizetheobjectivefunctionofthegeneralized
Kellycriterion. Thisminimizedfunctionwasgivenby(36)andwasindeedtheexpectedcomplexitytermoftheEFEof
aprospectiveactiveinference. Thatis,theobjectivefunctionofthegeneralizedKellycriterionwasatightupperbound
oftheexpectedcomplexityoftheEFE.TheEFEwasgivenby(4)andwascomposedofanexpectedcomplexityterm
plusanuncertaintyterm. Asdescribedinsubsection1.1,theminimizationoftheexpectedcomplexitywasequivalent
tothemaximizationofthereward. TherewardmaximizationwasalsoagoaloftheKellycriterionandcouldthusbe
partiallyfulfilledbyfindingthecandidatelabelsthroughtheproposedAlgorithm1.
Morespecifically, fromtheprior(win)andtheposterior(belief)probabilitiesofeachtrainingsample(bettor), the
generalizedKellycriterioncomputedoptimalallocationfractionsofthebettor’sassetforbettingonthecandidate
classes(horses)1. Theseallocationfractionsmaximizedthegeometricaverageofthegrowthrateofthebettor’sassetor
thereward. Tofurthermaximizethereward,theexpectedcomplexityoftheEFEshouldbeminimizedfurther. This
wasdoablebyhavingenoughinformationormaximizingtheinformationgain,i.e. minimizingtheuncertaintyofthe
1Theallocationfractionsfornoncandidateclasses(horses)werezero.
20
EFE.Accordingly,tooptimizeadiscriminativeneuralnetworkclassifier,weproposedanovelobjectivefunctionbased
ontheEFEofaprospectiveactiveinference. Thisfunctionwasgivenby(39)andwasreversibleanddifferentiablewith
respecttotheoutputsofeverylayeroftheneuralnetwork. Thus,asdescribedinsubsection1.2,itcouldbeminimized
byagradientdescentoptimizerwithbackpropagation.
Asexplainedinsubsection1.3,allthecrossentropy-basedlossesweredistribution-basedandstemmedfromtheVFE
givenby(2)foraretrospectiveactiveinference. TheVFEwascomplexityminusaccuracy. Thecomplexityreflected
theoverfittingoftheneuralnetwork’smodeltothedistributionofseen(training)samplesandthusthevarianceofthe
predictionsonunseen(validationortest)samples. Theaccuracywasinverselyproportionaltothebias(difference)of
thepredictionsfromtheirtruevalues. Thus,theminimizationoftheVFEimpliedtominimizethecomplexityorthe
overfittingwhilemaximizingtheclassificationaccuracybyminimizingtheclassificationbias. Thisway,theVFEand
thecrossentropy-basedlossesaddressedthebias-variancetradeoffoftheclassificationproblemswithoutconsidering
theunbalancednessoftheclass-sampledistributionoftheseensamples.
Incontrast,theEFEgivenby(4)foraprospectiveactiveinferenceandthusourproposedobjectivefunctionin(39)
addressed the unbalancedness of the class-sample distribution of the seen (training) samples by representing the
precision and recall metrics in their terms. The precision and the recall metrics were independent of the correct
classification of unimportant majority samples (designated by true negatives) and instead focused on the correct
classificationofimportantminoritysamples(designatedbytruepositives). Thismadethemlesssensitivethantheother
metricstotheimbalancedclass-sampledistributionsandtheclassificationbiasestowardsthedominantclasses.
As mentioned earlier, the minimization of the EFE or our proposed objective function implied to minimize the
expectedcomplexityandtheuncertainty. Theminimizationoftheexpectedcomplexityimpliedtomaximizethe
rewardandtherewardwasequivalenttotherecall(completenessordiversity). Theminimizationoftheuncertainty
implied to maximize the information gain or the precision (exactness or confidence). This way, the EFE and our
proposedobjectivefunctionaimedtomaximizetheprecisionandtherecallmetrics. Thisallowedthemtohandlean
imbalancedclass-sampledistributionwhilestillbeingdistribution-based[Feldman2010,Tatbul2018,Smith2022].
Moreover,ourproposedobjectivefunctioncouldincorporatethepriorprobabilitiesofthetrainingsamplesdirectlyand
indirectly. Theindirectincorporationwasthroughusingthecandidateclassificationlabelscomputedfromthepriors
andtheposteriorsofthetrainingsamplesbytheproposedAlgorithm1. Thisincorporationresultedinagroupingofthe
termsoftheproposedobjectivefunctionwithregardstothecandidateandnoncandidatelabels. Morespecifically,the
priorsortheposteriorsofthenoncandidatelabelsgotsummedtogethertoformacollectivepriororposteriorforthe
noncandidateclasses. Thisway,thenoncandidateclassesformedacollectiveclasstogetherandtheneuralnetworkgot
enforcedtofindtheboundarybetweeneachcandidateclassandthecollectiveclassofthenoncandidates. Incomparison
tocomputingtheboundariesbetweeneachpairoftheclasses,thisgroupingreducedtheeffectivenumberoftheclasses
andtheboundariesneededtobecomputed. Thisinturnreducedthenetwork’scomplexityanditsoverfittingtotheseen
(training)distributionandcouldthusenhanceitsgeneralization(predictiveperformanceonunseensamples).
Thedirectincorporationofthepriorprobabilitiesofthetrainingsamplesintotheobjectivefunctionofthenetwork’s
optimization could focus the attention of the neural network on important but minority foreground classes. This
couldreshapethedistributioneffectivelyseenbythenetworkduringitsoptimizationandcouldtherebyreducethe
class-sampleunbalancedness,theoverfitting,andtheclassificationbiasestowardsthedominantclasses[Maier2019].
Similareffectscouldresultfromthearchitecture-basedattentionmechanismsdescribedinsubsection1.4. Thatis,ifno
priorprobabilitieswereprovided,thenstrongerposteriorsresultedfromanarchitecture-basedattentionmechanism
shouldhelp. Inthebaselinearchitecturedescribedinsubsection1.4,anattentiongatecouldbeincorporatedintoeach
featureforwardingpathbetweenanencoderstageanditscorrespondingdecoderstage. Withoutsuchagate,thefeature
forwardingpathwasaplainskipconnection.
Ourproposedalgorithmforfindingthecandidatelabelsandourproposedobjectivefunctionforoptimizingadiscrimi-
nativeneuralnetworkclassifiergotintegratedintoamini-batch-basedgradientdescentoptimizerwithbackpropagation
21
byusingtheprocessproposedinsection4. Thisprocessgotevaluatedagainstasimilarprocessincorporatingarepre-
sentativeofthecrossentropy-basedlossesorarepresentativeofthemetric-basedlossesintroducedinsubsection1.3.
Therepresentativeofthecrossentropy-basedlosseswastheweightedfocalloss. Thislosscomprisedofamodulating
factorandaweightingmechanismtoalleviateclassificationbiasestowardsthedominantclassesofthetrainingsamples.
Therepresentativeofthemetric-basedlosseswastheLovász-Softmaxloss. Besidesbeingsmoothanddifferentiable,to
thebestofourknowledge,thislosswastheonlyconvexlossamongthemetric-basedlosses.
Accordingly,theevaluatedlosseswere
(1) theproposedobjectivefunctiongivenby(39)
(2) theweightedfocallossgivenby(14)
(3) theLovász-Softmaxlossgivenby(21).
Theseevaluationswereonanend-to-endoptimizationofthebaselinearchitecturedescribedinsubsection1.4. Foreach
case,thebaselinearchitecturewasonceusedwithoutattentiongatesasdepictedinFigure5andonceusedwiththe
attentiongatesasdepictedinFigure7. Also,for(2)and(3)eachtrainingsamplewasaccompaniedbyitsreference
(groundtruth)labeltofulfillthesupervisednatureoftheseobjectivefunctions. However, ourproposedalgorithm
forfindingthecandidatelabelsandourproposedobjectivefunctiongotevaluatedaccordingtoafullysupervised,a
semi-supervised,andanunsupervisedapproach. Theseresultedinthetrainingsamplesbeing
(1) accompaniedbytheirreferencelabelsandtheirpriors→fullysupervised
(2) onlyaccompaniedbytheirreferencelabels→semi-supervised
(3) onlyaccompaniedbytheirpriors→semi-supervised
(4) accompaniedbyneithertheirreferencelabelsnortheirpriors→unsupervised.
Theunsupervisedcaseonlyreliedontheposteriorsestimatedbytheneuralnetworkduringitsoptimizationandcould
thusbeconsideredasaself-supervisedcaseaswell.
Forthecaseswiththepriors,thepriorprobabilitiesofthetrainingsamplescouldbecomputedbyamultiatlasregistration.
Ifnopriorprobabilitieswereprovidedatthetimeofoptimization(training),thenuniformpriorsgotassumed. Ifthe
reference(groundtruth)labelsofthetrainingsamplesT wereprovidedatthetimeofoptimization(training),then
train
foreachsamplev ∈T ⊆T thevectorizedreferencelabell wastheone-hot-encodingofitsreferencelabel
b,j b train b,j
l ∈Landwasgivenby(8). IfthereferencelabelsofthetrainingsamplesT werenotprovidedatthetimeof
b,j train
optimization,thenforeachsamplev ∈T ⊆T thevectorl wasuniformandgivenby(9).
b,j b train b,j
For each evaluation case, the main parameters and the hyperparameters of the baseline architecture got trained
(optimized)toautomaticallysegmentn =|L|=8classesofvertebralbodies(VBs),intervertebraldisks(IVDs),
clas
psoasmajor(PM)andquadratuslumborum(QL)muscles,epicardialadiposetissues(EpAT),pericardialadiposetissues
(PeAT),cardiacperivascularadiposetissues(PvAT),andbackgroundoneachvolumetricfat-waterimage. Tothisend,
thevolumetricfat-waterimagesgotdividedintoatrainingandatestset. ThetrainingsetformedthesamplessetT
train
andgotusedtooptimizethemainparametersandthehyperparametersofthebaselinearchitecturebyeachmethod. The
testsetformedthesamplessetT andgotusedtoevaluatetheclassificationperformanceofthebaselinearchitecture
test
afterbeingfullyoptimizedbyeachmethod. Thetrainingsetwascomposedofsamplesaccompaniedbytheirreference
labelsandpriors. Thetestsetwascomposedofsamplesaccompaniedbytheirreferencelabels. Thereferencelabels
ofthetestsampleswerenotfedtotheneuralnetwork. Theywererathercomparedagainstthecorrespondinglabels
predictedbythenetworktoevaluatetheclassificationperformanceofthenetwork. Thepredictedlabelofeachsample
wastheindexofitsmaximumclassificationposteriorestimatedbythenetwork.
Finally, our proposed optimization process was based on the generalized Kelly criterion for optimal betting and a
prospectiveactiveinference. Itaddressedoptimizationofdiscriminativeneuralnetworkclassifierswithafeed-forward
22
architecture. Activeinference-basedoptimizationscouldfosterbuildinghighlyflexibleandgeneralizablegenerative
modelswithandwithoutmemory. Anexampleofamodelwiththememorywastheonewhichcouldexplainapartially
observableMarkovdecisionprocess. Thismodelcouldbeimplementedbyarecurrentoralongshort-termmemory
network [Friston2019, Smith2022, Carr2022]. Accordingly, our proposed optimization process could be easily
extendedtogenerativeorrecurrentneuralnetworkssuchasthenetworksin[Alom2019,Zuo2021,Mubashar2022].
3 ApplicationoftheKellyCriteriontoClassification
Thegeneralized(multinomial)Kellycriterionproposedoptimalallocationfractionsofabettor’sassetinbettingon
multiplehorsesinahorserace. Eachhorsehadawinandabeliefprobability. Thewinprobabilitywasthechanceof
thehorsetowintherace. Thebeliefprobabilitywasthecollectivebeliefofotherbettorsaboutthechanceofthehorse
towintherace. Thus,foraspecificbettor,anoptimumbettingstrategywastoinvestasmuchaspossibleonahorseof
maximumwinprobabilityandminimumbeliefprobability(minimumnumberofotherbettorsinvestingonit). Thiswas
basedontheassumptionthatallthebettorsfollowedthesamestrategyandthegainofahorsewingotdividedbetween
allthebettorswhohaveinvestedonit. Therefore,thelesserthebeliefprobabilitywas,thehigherthepaidgaintothe
investingbettorwouldbe[Kelly1956,Smoczynski2010].
Tooptimizeadiscriminativeneuralnetworkclassifierinamulticlassclassificationovern =|L|classesbyusingthe
clas
generalizedKellycriterion,weassumed
• everytrainingsamplev ∈T ⊆T tobeabettor
b,j b train
• everyclassc∈Ltobeahorse
• everyiterationi∈{1,··· ,n }oftheoptimizationtobearoundofhorseracewithitsgamblingcompetitionsamong
it
thebettors(trainingsamples)
• thewinprobabilityofeachhorse(class)c∈Lforeachbettor(trainingsample)v ∈T ⊆T tobetheprior
b,j b train
probabilitya ∈(0,1)estimatedbyanotherclassifier1
b,j,c
• the belief probability of each class c ∈ L for each sample v ∈ T ⊆ T to be the classification posterior
b,j b train
pˆ(i) ∈(0,1)estimatedbythenetworkinthecurrentiterationi.
b,j,c
Itshouldbenotedthatinthebetting,thewinprobabilitiesofthehorsesweresharedacrossthebettors,but,inthe
classification,eachsamplehaditsownwinprobabilityforeachclass. Moreover,theinterpretationoftheestimated
posteriorsofthenetworkasthebeliefprobabilitiesmightlookcounterintuitivebecauseeachsample(bettor)hadno
othersamples(bettors)tocompetewith. Thustheoverallbeliefaboutaclass(horse)couldnotbecollectedfromother
samples(bettors). Moreover,itwasmoretemptingtoselectaclass(investonahorse)ofmaximumbeliefprobabilityas
thisprobabilitycouldbeanindicatorofthechanceoftheclass(horse)towin. Ourdefinitionofthewinprobabilityand
ourcounterintuitivedefinitionofthebeliefprobabilitycouldbeexplainedunderanattentionmechanism.
Ononehand,theselectionoftheclasses(horses)ofmaximumwinprobabilityencouragedthenetworktofocuson
classesofconfident(high)priorprobabilities. Inanimagesegmentationtaskconductedinaspatialdomain,thisimplied
tofocusonimportant(relevant)regionshighlightedbyhighpriorprobabilitiesintheimage. Ontheotherhand,the
selectionoftheclasses(horses)ofminimumbeliefprobabilityencouragedthenetworktofocusoninconfident(low)
posteriorsandthustoimproveitsclassificationbytacklingdifficultexamples.
Ineachiteration(race)i,foreachtrainingsample(bettor)v ∈T ⊆T ,theKellycriterionproposedallocation
b,j b train
fractionsgˆ(i) = (cid:2) gˆ(i) ∈[0,1] (cid:3) ofitsassetforbettingonn =|L|classes(horses). Ifintheiteration(race)ithe
b,j b,j,c c∈L clas
class(horse)c ∈ Lwon,thentheassetofv ∈ T ⊆ T wouldbemultipliedby (cid:104) 1− (cid:80) gˆ(i) + gˆ b (i ,j ) ,c (cid:105)−1 .
b,j b train k∈L b,j,k pˆ(i)
b,j,c
Weassumedthattheoutcomesoftheiterations(horseraces)wereindependentidenticallydistributed(i.i.d.) random
1Ifnopriorprobabilitieswereprovidedthenuniformpriorsgotassumed.
23
variables. Thus,afteriiterations,thegeometricaverageofthegrowthrateoftheassetofv ∈ T ⊆ T with
b,j b train
n(i) ∈[0,i]numberofwinsforeachclassc∈Lbecome
c
η(i) = (cid:89) (cid:20) 1− (cid:88) gˆ(i) + gˆ b (i ,j ) ,c (cid:21)−n( c i)/i i= (cid:88) n(i). (22)
b,j b,j,k pˆ(i) c
c∈L k∈L b,j,c c∈L
Bytakingtheln(·)ofbothsidesof(22),oneobtained
ln(η(i))= (cid:88)−n( c i) ·ln (cid:20) 1− (cid:88) gˆ(i) + gˆ b (i ,j ) ,c (cid:21)
b,j i b,j,k pˆ(i)
c∈L k∈L b,j,c
(23)
lim n( c i) =a =⇒ lim ln(η(i))= (cid:88) −a ·ln (cid:20) 1− (cid:88) gˆ(i) + gˆ b (i ,j ) ,c (cid:21) .
i→∞ i b,j,c i→∞ b,j b,j,c b,j,k pˆ(i)
c∈L k∈L b,j,c
Iftheallocationfractionsg(i) = (cid:2) g(i) ∈[0,1] (cid:3) proposedbytheKellycriterionforeachsample(bettor)v ∈
b,j b,j,c c∈L b,j
T ⊆T wereasymptoticallyoptimumoveralongrun(i→∞),thentheymaximizedthegeometricaveragein
b train
(22). Duetothemonotonicincreaseoftheln(·)function,themaximizationof(22)wasequivalenttothemaximization
of(23). Thisway,theasymptoticallyoptimumallocationfractionswerethemaximizersoftheaveragedlogarithmsof
(cid:104) (cid:105)
thegrowthratein(23). Thatis,g(i) =argmax ln(η(i)) or
b,j gˆ(i) b,j
b,j
g(i) =argmin (cid:104) −ln(η(i)) (cid:105) =argmin (cid:34) (cid:88) a ·ln (cid:20) 1− (cid:88) gˆ(i) + gˆ b (i ,j ) ,c (cid:21)(cid:35) . (24)
b,j
gˆ b (i ,j )
b,j
gˆ b (i ,j ) c∈L
b,j,c
k∈L
b,j,k pˆ(
b
i
,j
)
,c
Asdetailedin[Smoczynski2010],gˆ(i) = (cid:2) gˆ(i) (cid:3) ∈[0,1]nclas=|L|formedaconvexset
b,j b,j,c c∈L
G(i) = (cid:40) gˆ(i) ∈[0,1]nclas=|L| (cid:12) (cid:12) (cid:12) (cid:20) 1− (cid:88) gˆ(i) + gˆ b (i ,j ) ,c (cid:21) >0 (cid:41) ⊆[0,1]nclas=|L| (25)
b,j b,j (cid:12) b,j,k pˆ(i)
k∈L b,j,c
which was an intersection of half spaces. Each half space was a side of a hyperplane. In addition, in the above
optimization, (cid:2) 1− (cid:80) gˆ(i) (cid:3) ∈[0,1] =⇒ (cid:80) gˆ(i) ∈[0,1]. Thatis,itwasallowedtobackahorsetowinbut
k∈L b,j,k k∈L b,j,k
nottolayahorsetolose. Thisconditionconstrainedeverygˆ(i) ∈G(i) toastricterconvexsetgivenby
b,j b,j
G′(i) = (cid:110) gˆ(i) ∈G(i) (cid:12) (cid:12) (cid:88) gˆ(i) ≤1 and ∀c∈L:gˆ(i) ≥0 (cid:111) ⊆G(i). (26)
b,j b,j b,j (cid:12) b,j,k c,j b,j
k∈L
Thedefinitionofln(η(i))in(23)showedthatitwasafinitelinearcombinationofstrictlyconcavelogarithmswiththe
b,j
coefficientsbeingthepriorsa = (cid:2) a ∈(0,1) (cid:3) . Thisway,theln(η(i))becomedifferentiable,strictlyconcave
b,j b,j,c c∈L b,j
downwards,andofauniquemaximumontheboundaryofeveryboundedsubsetofG(i). Accordingly,tofindthe
b,j
maximizersofln(η(i))ortheoptimumallocationfractionsg(i) = (cid:2) g(i) ∈[0,1] (cid:3) ,itwasenoughtoonlyexplorethe
b,j b,j b,j,c c∈L
boundariesofG′(i) ⊆G(i) [Smoczynski2010]. Thisexploration(maximization)couldbedonebyusingthemethodof
b,j b,j
LagrangemultipliersandtheKarush-Kuhn-Tucker(KKT)theory[Boyd2004]. Thatis,insteadofmaximizingln(η(i)),
b,j
wemaximized
γ(i) =ln(η(i))+ (cid:104)(cid:88) λ(i) ·gˆ(i) (cid:105) +λ(i) · (cid:104) 1− (cid:88) gˆ(i) (cid:105) (27)
b,j b,j b,j,k b,j,k b,j,0 b,j,k
k∈L k∈L
with (cid:8) λ(i) ∈R (cid:9)|L| beingtheLagrangemultipliers.
b,j,k ≥0 k=0
24
TheKKTtheorystatedthateveryconstrainedmaximizerofln(η(i))wasanunconstrainedmaximizerofγ(i). The
b,j b,j
unconstrained maximization of γ(i) was done through vanishing its gradient (derivatives) with respect to gˆ(i) =
b,j b,j
(cid:2) gˆ(i) ∈[0,1] (cid:3) . Thatis,
b,j,c c∈L
∂γ(i) −a +a /pˆ(i)
b,j = b,j,c b,j,c b,j,c +λ(i) −λ(i) =0. (28)
∂gˆ(i) 1− (cid:80) gˆ(i) +gˆ(i) /pˆ(i) b,j,c b,j,0
b,j,c k∈L b,j,k b,j,c b,j,c
ThisresultedinthefollowingKKToptimalityconstraints:
if λ(i) ·gˆ(i) =0 =⇒ λ(i) =0 if gˆ(i) >0
b,j,c b,j,c b,j,c b,j,c
if λ(i) · (cid:104) 1− (cid:88) gˆ(i) (cid:105) =0 =⇒ λ(i) =0 if (cid:88) gˆ(i) <1. (29)
b,j,0 b,j,k b,j,0 b,j,k
k∈L k∈L
Theallocationfractionsgˆ(i) = (cid:2) gˆ(i) ∈[0,1] (cid:3) andtheLagrangemultipliers (cid:8) λ(i) ∈R (cid:9)|L| shouldfulfill(29)
b,j b,j,c c∈L b,j,k ≥0 k=0
ontheconvexsetG′(i) ⊆ G(i). Accordingto[Smoczynski2010],themaximumofln(η(i))under (cid:80) gˆ(i) = 1
b,j b,j b,j k∈L b,j,k
waslessthanitsmaximumunder (cid:80) gˆ(i) <1. Thus,in(26),wereplaced (cid:80) gˆ(i) ≤1with (cid:80) gˆ(i) <1
k∈L b,j,k k∈L b,j,k k∈L b,j,k
andobtainedλ(i) =0from(29). Foreachsample(bettor)v ∈T ⊆T ,theclasses(horses)whoseallocation
b,j,0 b,j b train
fractionswerenonzeroweredeemedtobecandidateandformedthesetL(i) with
b,j
∀c∈L(i) ⊆L: gˆ(i) >0 and λ(i) =0
b,j b,j,c b,j,c
(30)
∀c∈L−L(i) : gˆ(i) =0 and λ(i) ≥0.
b,j b,j,c b,j,c
Then,solving(28)undertheaboveconditionsgave
(cid:80)
a
∀c∈L(i) ⊆L: g(i) =a −pˆ(i) ·
k∈L−L(
b
i
,
)
j
b,j,k
(31)
b,j b,j,c b,j,c b,j,c (cid:80) pˆ(i)
k∈L−L(i) b,j,k
b,j
(cid:80) k∈L−L(
b
i
,
)
j
ab,j,k
(cid:80)
=⇒ s(i) =1− (cid:88) g(i) =1− (cid:88) g(i) =1 (cid:122) − (cid:88) (cid:125)(cid:124) a (cid:123) + k∈L−L( b i , ) j a b,j,k · (cid:88) pˆ(i)
b,j b,j,c b,j,c b,j,c (cid:80) pˆ(i) b,j,c
c∈L c∈L(
b
i
,
)
j
c∈L(
b
i
,
)
j
k∈L−L(
b
i
,
)
j
b,j,k c∈L(
b
i
,
)
j
(cid:88) (cid:20) (cid:80) c∈L(i) pˆ( b i ,j ) ,c (cid:21) (cid:80) k∈L−L(i) a b,j,k
= a · 1+ b,j = b,j (32)
b,j,k (cid:80) pˆ(i) (cid:80) pˆ(i)
k∈L−L(
b
i
,
)
j
k∈L−L(
b
i
,
)
j
b,j,k k∈L−L(
b
i
,
)
j
b,j,k
(cid:80) (cid:80)
a a
∀c ∈ L(i) ⊆ L : s (i) + g b (i ,j ) ,c = k∈L−L( b i , ) j b,j,k + a b,j,c − k∈L−L( b i , ) j b,j,k = a b,j,c (33)
b,j b,j pˆ(i) (cid:80) pˆ(i) pˆ(i) (cid:80) pˆ(i) pˆ(i)
b,j,c k∈L−L(i) b,j,k b,j,c k∈L−L(i) b,j,k b,j,c
b,j b,j
(cid:80)
a
∀c∈L(i) ⊆L and ∀l∈L−L(i) :
a
b,j,l ≤s(i) =
k∈L−L(
b
i
,
)
j
b,j,k
<
a
b,j,c. (34)
b,j b,j pˆ(i) b,j (cid:80) pˆ(i) pˆ(i)
b,j,l k∈L−L(i) b,j,k b,j,c
b,j
4 ProposedObjectiveandProcessofOptimization
Byusingourclassification-basedformulationoftheKellycriterioninsection3weproposedanobjectivefunction
andaprocessforoptimizingdiscriminativeneuralnetworkclassifiers. Tobegeneric, weformulatedtheobjective
andtheprocessinsuchawaythattheycouldaccommodateafullysupervised,asemi-supervised,oranunsupervised
25
Algorithm1:DeterminationofthesetofcandidateclassificationlabelsL(i) ⊆L
b,j
Input:Posteriorpˆ(i) =[pˆ(i) ] andpriora = (cid:2) a ∈(0,1) (cid:3) probabilities
b,j b,j,c c∈L b,j b,j,c c∈L
Output:ThesetofcandidatelabelsL(i) ⊆Lforthesamplev ∈T ⊆T
b,j b,j b train
Initialization:L(i) ←∅, s(i) ←1
b,j b,j
(cid:110) (cid:111)
• Calculate q(i) =a /pˆ(i) andsortitinadescendingorder.
b,j,c b,j,c b,j,c c∈L
• AssignthesortedsettoQ(i) =
(cid:110)
q(i)
(cid:111)nclas=|L|
withkbeingthesortedindexandϕ :k→cbeingamapfrom
b,j b,j,k sort
k=1
thesortedindexktotheoriginalindexc.
while Q(i) ̸=∅ do
b,j
q←Q(i)(1):Takethefirst(maximum)elementofQ(i) andassignittoq.
b,j b,j
ifq>s(i) then
b,j
Insertϕ (1)intoL(i).
sort b,j
Q(i) ←Q(i) −Q(i)(1).
b,j b,j b,j
Restartkwith1andupdatethemapϕ :k→c.
sort
Updates(i) using(32).
b,j
else
Breakthewhileloop.
end
IfL(i) =∅theninsertthereferenceclassificationlabell ∈LintoL(i).
b,j b,j b,j
ReturnL(i).
b,j
optimization. In the fully supervised optimization, both the reference (ground truth) labels and the prior (win)
probabilities of the training samples were provided at the time of optimization (training). In the semi-supervised
optimization,eitherthereferencelabelsortheprior(win)probabilitiesofthetrainingsampleswerenotprovidedat
thetimeofoptimization(training). Intheunsupervisedoptimization,neitherthereferencelabelsnortheprior(win)
probabilitiesofthetrainingsampleswereprovidedatthetimeofoptimization(training). Ifnopriorprobabilitieswere
providedatthetimeofoptimization(training),thenuniformpriorsgotassumed. Ifthereference(groundtruth)labelsof
thetrainingsamplesT wereprovidedatthetimeofoptimization(training),thenforeachsamplev ∈T ⊆T
train b,j b train
thevectorizedreferencelabell wasaone-hot-encodingofitsreference(groundtruth)labell ∈Landwasgiven
b,j b,j
by(8). Ifthereference(groundtruth)labelsofthetrainingsamplesT werenotprovidedatthetimeofoptimization
train
(training),thenforeachsamplev ∈T ⊆T thevectorl wasuniformandgivenby(9).
b,j b train b,j
Wedenotedthevectorizedreference labels, thefixedprior(win)probabilities, andtheestimatedposterior(belief)
probabilities of the samples in the batch T ⊆ T with the |T |×n matrices of L = [l ] = [l ] ,
b train b clas b b,j j b,j,c j,c
A =[a ] =[a ] ,andPˆ(i) =[pˆ(i)] =[pˆ(i) ] ,respectively. Also,theallocationfractionsestimatedbythe
b b,j j b,j,c j,c b b,j j b,j,c j,c
Kellycriterionforthesesamplesformeda|T |×n matrixdenotedbyGˆ(i) =[gˆ(i)] =[gˆ(i) ] .
b clas b b,j j b,j,c j,c
Ineachiterationi ∈ {1,··· ,n }ofoptimizingadiscriminativeneuralnetworkclassifier,wefirstfoundthesetof
it
candidate classification labels L(i) ⊆ L for each sample (bettor) v ∈ T ⊆ T . To this end, we proposed
b,j b,j b train
Algorithm 1 by using (31), (32), (33), and (34). Through this algorithm, the set of candidate labels L(i) ⊆ L got
b,j
computed from the estimated posterior (belief) probabilities pˆ(i) = (cid:2) pˆ(i) ∈(0,1) (cid:3) and the fixed prior (win)
b,j b,j,c c∈L
probabilitiesa = (cid:2) a ∈(0,1) (cid:3) ofthesample(bettor)v ∈T ⊆T .
b,j b,j,c c∈L b,j b train
ThesetL(i) ⊆ Lcouldcontainmultipleclasslabelsorbeempty. Anemptysetimpliedthatthecurrentposterior
b,j
(belief)andthefixedprior(win)probabilitiesfoundnoclasslabel,eventhereferencelabell ∈ L,tobereliable
b,j
enoughfortheoptimizationoftheneuralnetworkclassifier. Thiscouldresultinnofurtherupdateoftheposterior
26
Table2:Equivalenceofthenotationsusedintheobjectivefunctionsoftheactiveinference(leftcolumn)andtheneuralnetwork
optimization(rightcolumn).
p(s|π) l : cth entryofthevectorizedreferencelabelofthesamplev ∈ T ⊆ T
b,j,c b,j b train
p(o) a :prior(win)probabilityofthesamplev ∈ T ⊆ T
b,j,c b,j b train
q(o|π) pˆ (i) :estimatedposterior(belief)probabilityofthesamplev ∈ T ⊆ T
b,j,c b,j b train
(cid:80) a :collectivepriorofnoncandidateclassesofthesamplev ∈ T ⊆ T
k∈L−L(i) b,j,k b,j b train
b,j
(cid:80) pˆ (i) :collectiveposteriorofnoncandidateclassesofthesamplev ∈ T ⊆ T
k∈L−L(i) b,j,k b,j b train
b,j
(belief)probabilitiesinthefollowingiterations. Toavoidthisstandstill,attheendoftheAlgorithm1,ifL(i) =∅,then
b,j
thereferencelabell ∈Lofthesample(bettor)v ∈T ⊆T gotinsertedintoit.
b,j b,j b train
Byextending(24)toallthesamplesinthebatchT ⊆T ,oneobtained
b train
G(i) =argmin 1 (cid:88) (cid:88) a ·ln (cid:20) 1− (cid:88) gˆ(i) + gˆ b (i ,j ) ,c (cid:21) . (35)
b
Gˆ( b i)
|L|·|T
b
|
j∈T bc∈L
b,j,c
k∈L
b,j,k pˆ(
b
i
,j
)
,c
(cid:124) (cid:123)(cid:122) (cid:125)
LKelly(Gˆ(
b
i))
However,theoptimumallocationfractionsG(i) =[g(i)] =[g(i) ] hadaclosedformsolutiongivenby(31). This
b b,j j b,j,c j,c
solutionresultedin(32)and(33)andallowedtoexpress
min L (Gˆ(i))=L (G(i))= 1 (cid:88) (cid:88) a ·ln (cid:20) s(i) + g b (i ,j ) ,c (cid:21) (36)
Gˆ( b i) Kelly b Kelly b |L|·|T b | j∈T bc∈L b,j,c b,j pˆ b (i ,j ) ,c
 (cid:34) (cid:35)   (cid:80) a 
= |L|· 1 |T
b
|
j
(cid:88)
∈T b
 
c∈
(cid:88)
L( b i , ) j
a b,j,c ·ln a pˆ(
b
b i
,
,
j
) j
,
,
c
c + 
k∈L
(cid:88)
−L( b i , ) j
a b,j,k   ·ln (cid:80) k
k
∈
∈
L
L
−
−
L
L
( b
( b
i
i
,
,
) j
) j
pˆ(
b
b i
,
,
j
) j
,
,
k
k    .
Asgivenby(10),thecrossentropylossforoptimizingdiscriminativeneuralnetworkclassifierswasthevariationalfree
energy(VFE)ofaretrospectiveactiveinference. Thatis,
L (Pˆ(i),L )= −1 (cid:88) (cid:88) l ·ln (cid:0) pˆ(i) (cid:1) ≡ − (cid:88) p(s|π)·ln (cid:0) q(o|π) (cid:1) . (37)
CE b b |L|·|T | b,j,c b,j,c
b j∈T bc∈L s|π
Also,theexpectedfreeenergy(EFE)ofaprospectiveactiveinferencewasgivenin(4)as
(cid:88) (cid:104) (cid:0) (cid:1) (cid:0) (cid:1) (cid:105) (cid:88) (cid:88) (cid:0) (cid:1)
L EFE = p(o)· ln p(o) −ln q(o|π) + −p(s|π)· q(o|π)·ln q(o|π) . (38)
o s|π o|π
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125)
expected complexity
uncertainty
Our proposed Algorithm 1 for finding the candidate labels L(i) aimed to minimize the objective function of the
b,j
generalizedKellycriterion. Thisminimizedfunctionwasgivenby(36). Acomparisonof(38)and(36)withregardto
(37)revealedthattheminimizedobjectiveoftheKellycriterionwastheexpectedcomplexitytermoftheEFEofa
prospectiveactiveinference. Thatis,theobjectivefunctionofthegeneralizedKellycriterionwasatightupperbound
oftheexpectedcomplexityoftheEFE.ThisequivalencegotsummarizedinTable2andimpliedthatthepreferred
observationsdenotedbyowererealizedthroughdividingLintocandidateL(i) andnoncandidateclassesL−L(i)
b,j b,j
andthenhandlingthenoncandidateclassesaltogetherasoneclass. Tothisend,in(36),theprior(win)probabilitiesof
thenoncandidateclassesgotsummedtogethertoformtheircollectiveprior(win)probability. Similarly,theestimated
27
Figure8:Sagittalslicesofthefeaturemapsatthespatialregionsenclosingthevertebralbodiesandtheintervertebraldisksatthe
outputsofdifferentencoder/decoderstagesofthebaselinearchitecturedepictedinFigure5afterbeingoptimizedbythe
proposedobjectivefunctionanditsassociatedoptimizationprocess.
posterior(belief)probabilitiesofthenoncandidateclassesgotsummedtogethertoformtheircollectiveposterior(belief)
probability.
TheEFEin(38)wascomposedofanexpectedcomplexitytermplusanuncertaintyterm. Asdescribedinsubsec-
tion1.1,theminimizationoftheexpectedcomplexitywasequivalenttothemaximizationofthereward. Thereward
28
maximizationwasalsoagoaloftheKellycriterionandcouldthusbepartiallyfulfilledbyfindingthecandidatelabels
throughtheproposedAlgorithm1. Tofurthermaximizethereward,theexpectedcomplexityshouldbeminimized
further. This was doable by having enough information or maximizing the information gain, i.e. minimizing the
uncertainty.Accordingly,tooptimizeadiscriminativeneuralnetworkclassifier,weproposedanovelobjectivefunction
basedontheEFEofaprospectiveactiveinference. Theproposedfunctionwasgivenby
L (Pˆ(i),A ,L )= −1 (cid:88) (cid:88) l ·pˆ(i) ·ln (cid:2) pˆ(i) (cid:3) + (39)
EFE b b b |L|·|T | b,j,c b,j,c b,j,c
b j∈T bc∈L
(cid:124) (cid:123)(cid:122) (cid:125)
uncertainty
+ 1 (cid:88)
(cid:34)
(cid:88) a ·ln
(cid:20)
a b,j,c
(cid:21)
+
(cid:20)
(cid:88) a
(cid:21)
·ln
(cid:20) (cid:80)
k∈L−L( b i , ) j
a b,j,k(cid:21)(cid:35)
.
|L|·|T
b
|
j∈T b c∈L( b i , ) j
b,j,c pˆ(
b
i
,j
)
,c k∈L−L( b i , ) j
b,j,k (cid:80)
k∈L−L( b i , ) j
pˆ(
b
i
,j
)
,k
(cid:124) (cid:123)(cid:122) (cid:125)
expectedcomplexity
ThisfunctionwasreversibleanddifferentiablewithrespecttotheposteriorsPˆ(i). Asgivenby(7),theseposteriorswere
b
generatedbyapplyingtheSoftmaxfunctiontothenetwork’soutputsZ(i) =[z(i)] =[z(i) ] . Thus,theproposed
b b,j j b,j,c j,c
functionwasalsodifferentiablewithrespecttotheZ(i)andtheoutputsofeverylayer. Asdescribedinsubsection1.2,
b
theseallowedtominimizeitbyagradientdescentoptimizerwithbackpropagation.
We preceded the minimization of (39) with a partial minimization of its expected complexity term by finding the
candidateclassificationlabelsL(i) ofeachsample(bettor)v ∈T ⊆T throughtheAlgorithm1proposedbased
b,j b,j b train
ontheKellycriterion.
Accordingly,ineachiterationi∈{1,··· ,n }ofourproposedoptimizationprocess,everysamplev ∈T ⊆T
it j b train
gotpassedthroughthenetworktoestimateitsclassificationposteriorsPˆ(i) = (cid:2) pˆ(i) ∈(0,1) (cid:3) =[pˆ(i) ] . Fromthese
b b,j j b,j,c j,c
posteriorsandthefixedpriorsa = (cid:2) a ∈(0,1) (cid:3) ofthesample,itscandidateclassificationlabelsL(i) ⊆Lgot
b,j b,j,c c∈L b,j
computedbyusingtheproposedAlgorithm1. Then,thelossatthelastnetwork’slayergotobtainedbyinputtingthe
posteriors,thepriors,andthecandidatelabelsofthesamplesintotheproposedfunctionin(39). Bypropagatingthis
lossfromthelastlayertothefirstlayer,thelossofeverylayergotobtained. Then,thegradient(firstderivative)ofeach
layer’slossgotcalculatedwithrespecttoitsoutputs. Theproductoftheselayerwisegradientsgotusedbythegradient
descentoptimizertoupdatethenetwork’sparameters.
Inanimagesegmentationtask,eachsamplev ∈T ⊆T wasanimagepatchprocessedbyanetwork’slayer. In
b,j b train
ourbaselinearchitecturedescribedinsubsection1.4,eachnetwork’slayerprocessedsamples(patches)ofacertain
spatialresolution. Themultiresolutionhierarchyofthenetworkwastheresultofdownsamplingandupsamplingeach
volumetricfat-waterimagethroughconvolutionalanddeconvolutionallayers,respectively. Forsakeofsimplicity,we
omittedtheresolutionspecifyingindicesfromthesamples’notations.
Figure 8 shows sagittal slices of the feature maps at the spatial regions enclosing the vertebral bodies and the
intervertebraldisksattheoutputsofdifferentencoder/decoderstagesofthebaselinearchitecturedepictedinFigure5
afterbeingoptimizedbytheproposedobjectivefunctionanditsassociatedoptimizationprocess.
5 Network’sParametersandTheirOptimization
Ourproposedalgorithmforfindingthecandidatelabelsandourproposedobjectivefunctionforoptimizingadiscrimi-
nativeneuralnetworkclassifiergotintegratedintoamini-batch-basedgradientdescentoptimizerwithbackpropagation
byusingtheprocessproposedinsection4. Thisprocessgotevaluatedagainstasimilarprocessincorporatingarepre-
sentativeofthecrossentropy-basedlossesorarepresentativeofthemetric-basedlossesintroducedinsubsection1.3.
Therepresentativeofthecrossentropy-basedlosseswastheweightedfocalloss. Thislosscomprisedofamodulating
factorandaweightingmechanismtoalleviateclassificationbiasestowardsthedominantclassesofthetrainingsamples.
29
Therepresentativeofthemetric-basedlosseswastheLovász-Softmaxloss. Besidesbeingsmoothanddifferentiable,to
thebestofourknowledge,thislosswastheonlyconvexlossamongthemetric-basedlosses.
Accordingly,theevaluatedlosseswere
(1) theproposedobjectivefunction(Po)givenby(39)
(2) theweightedfocalloss(Fo)givenby(14)
(3) theLovász-Softmaxloss(Lo)givenby(21).
Theseevaluationswereonanend-to-endoptimizationofthebaselinearchitecturedescribedinsubsection1.4. Foreach
case,thebaselinearchitecturewasonceusedwithoutattentiongates(Na)asdepictedinFigure5andonceusedwiththe
attentiongates(At)asdepictedinFigure7. Also,for(2)and(3)eachtrainingsamplewasaccompaniedbyitsreference
(groundtruth)labeltofulfillthesupervisednatureoftheseobjectivefunctions. However, ourproposedalgorithm
forfindingthecandidatelabelsandourproposedobjectivefunctiongotevaluatedaccordingtoafullysupervised,a
semi-supervised,andanunsupervisedapproach. Theseresultedinthetrainingsamplesbeing
(1) accompaniedbytheirreferencelabelsandtheirpriors(GrPr)→fullysupervised
(2) onlyaccompaniedbytheirreferencelabels(GrNp)→semi-supervised
(3) onlyaccompaniedbytheirpriors(NgPr)→semi-supervised
(4) accompaniedbyneithertheirreferencelabelsnortheirpriors(NgNp)→unsupervised.
Forthecaseswiththepriors,thepriorprobabilitiesofthetrainingsamplescouldbecomputedbyamultiatlasregistration.
Ifnopriorprobabilitieswereprovidedatthetimeofoptimization(training),thenuniformpriorsgotassumed. Ifthe
reference(groundtruth)labelsofthetrainingsamplesT wereprovidedatthetimeofoptimization(training),then
train
foreachsamplev ∈T ⊆T thevectorizedreferencelabell wastheone-hot-encodingofitsreferencelabel
b,j b train b,j
l ∈Landwasgivenby(8). IfthereferencelabelsofthetrainingsamplesT werenotprovidedatthetimeof
b,j train
optimization,thenforeachsamplev ∈T ⊆T thevectorl wasuniformandgivenby(9).
b,j b train b,j
For each evaluation case, the main parameters and the hyperparameters of the baseline architecture got trained
(optimized)toautomaticallysegmentn =|L|=8classesofvertebralbodies(VBs),intervertebraldisks(IVDs),
clas
psoasmajor(PM)andquadratuslumborum(QL)muscles,epicardialadiposetissues(EpAT),pericardialadiposetissues
(PeAT),cardiacperivascularadiposetissues(PvAT),andbackgroundoneachvolumetricfat-waterimage. Tothisend,
thevolumetricfat-waterimagesgotdividedintoatrainingandatestset. ThetrainingsetformedthesamplessetT
train
andgotusedtooptimizethemainparametersandthehyperparametersofthebaselinearchitecturebyeachmethod. The
testsetformedthesamplessetT andgotusedtoevaluatetheclassificationperformanceofthebaselinearchitecture
test
afterbeingfullyoptimizedbyeachmethod. Thetrainingsetwascomposedofsamplesaccompaniedbytheirreference
labelsandpriors. Thetestsetwascomposedofsamplesaccompaniedbytheirreferencelabels. Thereferencelabels
ofthetestsampleswerenotfedtotheneuralnetwork. Theywererathercomparedagainstthecorrespondinglabels
predictedbythenetworktoevaluatetheclassificationperformanceofthenetwork. Thepredictedlabelofeachsample
wastheindexofitsmaximumclassificationposteriorestimatedbythenetwork.
The main parameters of the baseline architecture included the weights and the biases of the convolutional and
deconvolutional layers, the leakage coefficient a ∈ R of every nonlinear PReLU activation, and the means
prelu ≥0
andvariancesofthe(instance)normalizersintroducedinpage14. Priortotheoptimizationofthemainparameters,
they should be initialized. This initialization was extremely important for the weights of the convolutional and
deconvolutionallayersofaresidualnetworkofseverallayersandthusdifferentpathsofsignalpropagation. Without
aproperweightinitialization,somepartsofthenetworkmighthaveexcessiveactivationsandthusproducestronger
gradientswhilesomeotherpartsmightproduceweakergradientsandthusgetoptimizedless. Toavoidthis,arandom
initialization of the weights with the aim of breaking symmetries and making each feature map of a unit variance
30
was suggested. For this, the weights were drawn from a certain distribution. In networks with nonlinear Sigmoid
orhyperbolictangentactivationsaswellaslinearactivations,theproperinitializationsoftheweightsofeverylayer
(cid:112) (cid:112)
wererandomnumbersdrawnfromauniformdistributionintherange[− 6/(n +n ), 6/(n +n )]with
in out in out
n being the number of incoming network connections (fan-in) and n being the number of outgoing network
in out
connections(fan-out)ofthelayer. ThistypeofinitializationwascalledaGlorotoraXavier initializationandwas
showntobeimproperfornetworksinvolvingnonlinearrectifiedlinearunits,includingthePReLU,astheiractivations
[Glorot2010]. Forthesenetworks, likeourbaselinearchitecture, theproperinitializationsoftheweightsofevery
convolutional/deconvolutional layer were random numbers drawn from a Gaussian distribution with a mean of 0
(cid:112)
and a standard deviation of 2/n [He2015, Ronneberger2015]. For a convolutional layer of a kernel size of
in
5×5×5,16inputfeaturemaps,and32outputfeaturemaps,thenumberofincomingnetworkconnections(fan-in)
was5×5×5×16=2000andthenumberofoutgoingnetworkconnections(fan-out)was32. Thebiasesofevery
convolutional/deconvolutionallayerwereinitializedto0. TheleakagecoefficientofeverynonlinearPReLUactivation
got initialized to 0.15 to allow a small leakage of negative inputs. The means and the variances of the (instance)
normalizersgotinitializedto0and1respectively.
Thehyperparametersofthebaselinearchitectureandtheirdiscretizedvalueswere
• numberofconvolutional/deconvolutionallayersn ∈{1,2,··· ,5}ofthesthencoder/decoderstageoftheV-netof
s
thebaselinearchitecture
• Dropout’sretentionprobabilityp ∈{0.1,0.2,··· ,0.9}oftheperceptrons(nodes)ofthesthencoder/decoderstage
s
oftheV-netofthebaselinearchitecture.
Tooptimizethemainparametersandthehyperparametersofthebaselinearchitecturebyeachmethod,arandomsearch
overthediscretizedhyperparametervaluesanda5-foldcrossvalidationwereconducted. Tothisend, thetraining
setgotdividedinto5subsets. Then,foreachmethod,ineachoptimizationtrial,asetofhyperparametervaluesgot
randomly selected. With these hyperparameter values, 5 times training and validation got performed according to
the5-foldcrossvalidation. Ineachfold,themainparametersofthebaselinearchitecturegotoptimizedon4subsets
byusingamini-batch-basedgradientdescentoptimizerwithbackpropagation. Thegradientdescentoptimizerwas
theAdamoptimizerdescribedinsubsection1.2. Theresultingnetworkmodelgotthenevaluatedontheremaining
(validation)subsetbycalculatingtheprecisionandtherecallmetricsforeachofthen −1=8−1=7foreground
clas
classesagainsttherestoftheclasses. Thisway,fortheselectedhyperparametervalues,attheendofthe5-foldcross
validation,5networkmodelsand7precisionand7recallvaluespernetworkmodelwereobtained. Foreachmodel,
the 7 precision and the 7 recall values got averaged. Then, for the selected hyperparameter values, the model of
maximumaveragedprecisionandrecallwasthebestperformingmodel. Theoptimizationtrialscontinuedbyrandomly
selectinganothersetofhyperparametervaluesuntilthebestperformingmodelresultedfromthecurrenthyperparameter
valuescouldnotexceedtheaveragedprecisionandrecallvaluesofanyofthebestmodelsinthelast50trials. The
precisionandrecallmetricswereselectedduetotheirrobustnessagainsttheimbalancedclass-sampledistributions.
Moreover, theaforementionedcrossvalidationaimedtoreducetheimpactsoftherandomizedinitializationofthe
main parameters on the resulting network models. The 5 folds were selected with regard to the maximum size of
thebaselinearchitectureandthesufficiencyofthenumberoftrainingandvalidationsamplesfortheoptimization
andevaluationineachfold,respectively. Theaboveprocesswasdonebyusingthetoolsprovidedinthedistributed
asynchronous hyperparameter optimization (Hyperopt) library in Python [Bergstra2015]. For the hyperparameter
selection,inadditiontotherandomization,thislibraryprovidedatreeofParzenestimators(TPE)anditsadaptive
variant. TheTPEwasmoreappropriateforbeliefneuralnetworksofundirectedgraphtopologythanthefeed-forward
networkslikeourbaselinearchitecture[Bergstra2011,Bergstra2012].
TheevaluatedobjectivefunctionsandtheAdam-basedgradientdescentoptimizerinvolvedfollowingfixedparameters:
• N=|T |=2: Asexplainedinpage13,duetothememorylimitationsoftheusedGPU,only2volumetricfat-water
b
imageswereincludedineachmini-batch.
31
Figure9:Convergencepatternsofdifferentevaluationcaseswitheachcaseoptimizingitsmainparameterswiththebestperforming
hyperparameters.
32
Table3:Optimizedhyperparametersandtheoveralltime[hours]ofoptimizingthemainparametersandthehyperparametersfor
eachevaluationcase.
EvaluationCase
PoNaGrPr PoNaGrNp PoNaNgPr PoNaNgNp PoAtGrPr PoAtGrNp
sretemaraprepyH
n 2 2 2 3 1 1
1
n 2 3 2 4 2 2
2
n 3 3 4 4 2 2 3
n 3 4 4 5 3 3
4
n 3 4 5 5 3 4
5
p 0.9 0.8 0.8 0.7 0.9 0.9
1
p 0.8 0.7 0.7 0.7 0.7 0.8
2
p 0.7 0.7 0.7 0.7 0.7 0.7
3
p 0.7 0.7 0.7 0.6 0.6 0.7
4
p 0.6 0.6 0.6 0.6 0.5 0.6
5
time 83 90 95 107 82 85
EvaluationCase
PoAtNgPr PoAtNgNp FoNaGrNp FoAtGrNp LoNaGrNp LoAtGrNp
sretemaraprepyH
n 1 2 2 2 1 1
1
n 2 3 3 2 2 2
2
n 3 4 4 3 3 2 3
n 3 4 4 3 4 3
4
n 4 5 4 4 4 3
5
p 0.9 0.9 0.9 0.9 0.9 0.9
1
p 0.8 0.8 0.8 0.8 0.8 0.8
2
p 0.7 0.8 0.8 0.7 0.7 0.7
3
p 0.6 0.7 0.7 0.6 0.7 0.6
4
p 0.5 0.6 0.6 0.5 0.5 0.5
5
time 88 97 84 82 79 77
• γ =2: Modulatingfactorofthefocallossgivenby(13).
mod
• α =0.001: Learningrate(stepsize)ofthegradientdescentoptimizerdefinedin(5). Thislearningratedidnotneed
lr
tobeadaptedmanuallyastheAdamoptimizerautomaticallychangedtheeffectivelearningratebytheratioofthe
exponentialmovingaverageofthefirstmomenttotheexponentialmovingaverageofthesecondmoment.
• β =0.90: Decayrateoftheestimatedfirstmoments.
fm
• β =0.99: Decayrateoftheestimatedsecondmoments.
sm
• m(0) =0: Initialfirstmoments.
• v(0) =0: Initialsecondmoments.
Thenumberofiterationsn ∈{10,··· ,15000}wasdeterminedaccordingtoanearlystoppingcriterion. Thatis,when
it
theexponentialmovingaverageofthevalidationerror(loss)wasnotimprovedwithinthelast100iterations,thenthe
optimizationgotstopped.
Figure9showsconvergencepatternsofdifferentevaluationcaseswitheachcaseoptimizingitsmainparameterswith
thebestperforminghyperparameters.
Theaforementionedoptimizationswereconductedon4NVIDIATITANX® GPUsof12GBmemoryeachandby
usingamemoryefficientcuDNN3implementationoftheconvolutional/deconvolutionallayersandtheTensorFlowTM
libraryofversion2.3[Abadi2016].
Table 3 shows the optimized hyperparameters and the overall time of optimizing the main parameters and the hy-
perparametersforeachevaluationcase. Aftertheoptimizations,anautomaticsegmentationofthen =8classes
clas
onanunseenvolumetricfat-waterimagetookaround3secondsforeachevaluationcaseontheGPUsusedforthe
optimizations.
33
References
[Abadi2016] MartínAbadi,AshishAgarwal,PaulBarham,EugeneBrevdo,ZhifengChen,CraigCitro,GregS.Corrado,
AndyDavis,JeffreyDean,MatthieuDevin,SanjayGhemawat,IanGoodfellow,AndrewHarp,Geoffrey
Irving,MichaelIsard,YangqingJia,RafalJozefowicz,LukaszKaiser,ManjunathKudlur,JoshLevenberg,
DanMane,RajatMonga,SherryMoore,DerekMurray,ChrisOlah,MikeSchuster,JonathonShlens,
BenoitSteiner,IlyaSutskever,KunalTalwar,PaulTucker,VincentVanhoucke,VijayVasudevan,Fernanda
Viegas,OriolVinyals,PeteWarden,MartinWattenberg,MartinWicke,YuanYuandXiaoqiangZheng,
TensorFlow:Large-scalemachinelearningonheterogeneousdistributedsystems.arXive-prints,March
2016,https://www.tensorflow.org/.
[Alom2019] MdZahangirAlom,ChrisYakopcic,MahmudulHasan,TarekM.TahaandVijayanK.Asari,Recurrent
residualU-Netformedicalimagesegmentation.JMedImaging,Volume6,2019.
[Bach2013] FrancisBach,Learningwithsubmodularfunctions: Aconvexoptimizationperspective.FoundTrends
MachLearn,Volume6,Pages145–373,2013.
[Badrinarayanan2016] V.Badrinarayanan,A.KendallandR.Cipolla,SegNet:Adeepconvolutionalencoder-decoderarchitecture
forimagesegmentation.ArXive-prints,2016.
[Bergstra2011] JamesBergstra,RémiBardenet,YoshuaBengioandBalázsKégl,Algorithmsforhyperparameterop-
timization.InProceedingsofAdvancesinNeuralInformationProcessingSystems,Pages2546–2554,
2011.
[Bergstra2012] JamesBergstraandYoshuaBengio,Randomsearchforhyperparameteroptimization.JMachLearnRes,
Volume13,Pages281–305,feb2012.
[Bergstra2015] JamesBergstra,BrentKomer,ChrisEliasmith,DanYaminsandDavidDCox,Hyperopt:aPythonlibrary
formodelselectionandhyperparameteroptimization.ComputSciDiscov,Volume8,Page014008,2015,
https://github.com/hyperopt/hyperopt.
[Berman2018] MaximBerman,AmalRannenTrikiandMatthewB.Blaschko,TheLovasz-Softmaxloss: Atractable
surrogatefortheoptimizationoftheintersection-over-unionmeasureinneuralnetworks.InProceedings
oftheIEEEConferenceonComputerVisionandPatternRecognition,Pages4413–4421,2018.
[Bertels2019] JeroenBertels,TomEelbode,MaximBerman,DirkVandermeulen,FrederikMaes,RafBisschopsand
MatthewB.Blaschko,OptimizingtheDicescoreandJaccardindexformedicalimagesegmentation:
Theoryandpractice.InProceedingsoftheInternationalConferenceonMedicalImageComputingand
Computer-AssistedIntervention,Pages92–100,2019.
[Boyd2004] S.P.BoydandL.Vandenberghe,ConvexOptimization.CambridgeUniversityPress,2004.
[Carr2022] StevenCarr,NilsJansenandUfukTopcu,Task-awareverifiableRNN-basedpoliciesforpartiallyobserv-
ableMarkovdecisionprocesses.JArtifIntellRes,Volume72,Pages819–847,jan2022.
[Chen2022] Tzu-HsuanChenandTianSheuanChang,RangeSeg:Range-Awarerealtimesegmentationof3DLiDAR
pointclouds.IEEETransIntellVeh,Volume7,Pages93–101,2022.
[Çiçek2016] ÖzgünÇiçek,AhmedAbdulkadir,SoerenS.Lienkamp,ThomasBroxandOlafRonneberger,3DU-Net:
Learningdensevolumetricsegmentationfromsparseannotation.InProceedingsoftheInternational
ConferenceonMedicalImageComputingandComputer-AssistedIntervention,Pages424–432,2016.
[Cui2019] YinCui,MenglinJia,Tsung-YiLin,YangSongandSergeBelongie,Class-balancedlossbasedoneffective
numberofsamples.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,
Pages9260–9269,2019.
[Dean2012] JeffreyDean,GregCorrado,RajatMonga,KaiChen,MatthieuDevin,MarkMao,MarcRanzato,Andrew
Senior,PaulTucker,KeYang,QuocV.LeandAndrewY.Ng,Largescaledistributeddeepnetworks.In
ProceedingsofAdvancesinNeuralInformationProcessingSystems,Pages1223–1231,2012.
[Feldman2010] HarrietFeldmanandKarlFriston,Attention,uncertainty,andfree-energy.FrontHumNeurosci,Volume4,
2010.
[Friston2019] KarlFriston,Afreeenergyprincipleforaparticularphysics.ArXive-prints,2019.
[Fujishige1991] S.Fujishige,Submodularfunctionsandoptimization.ISSN,ElsevierScience,1991.
[Gal2015] YarinGalandZoubinGhahramani,DropoutasaBayesianapproximation:Representingmodeluncertainty
indeeplearning.ArXive-prints,2015.
[Glorot2010] XavierGlorotandYoshuaBengio, Understandingthedifficultyoftrainingdeepfeed-forwardneural
networks.InProceedingsoftheInternationalConferenceonArtificialIntelligenceandStatistics,Volume9,
Pages249–256,2010.
[Goodfellow2016] IanGoodfellow,YoshuaBengioandAaronCourville,DeepLearning.MITPress,2016.
34
[He2015] Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun, Delving deep into rectifiers: Surpassing
human-levelperformanceonImageNetclassification.InProceedingsoftheIEEEInternationalConference
onComputerVision,Pages1026–1034,2015.
[He2016a] KaimingHe,XiangyuZhang,ShaoqingRenandJianSun,Deepresiduallearningforimagerecognition.
InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,Pages770–778,
2016.
[He2016b] KaimingHe,XiangyuZhang,ShaoqingRenandJianSun,Identitymappingsindeepresidualnetworks.In
ProceedingsoftheEuropeanConferenceonComputerVision,Pages630–645,2016.
[Ioffe2015] SergeyIoffeandChristianSzegedy,Batchnormalization:Acceleratingdeepnetworktrainingbyreducing
internalcovariateshift.InProceedingsoftheInternationalConferenceonMachineLearning,Volume37,
Pages448–456,2015.
[Isensee2018] FabianIsensee,PhilippKickingereder,WolfgangWick,MartinBendszusandKlausH.Maier-Hein,No
New-Net.InProceedingsoftheInternationalWorkshoponBainlesion:Glioma,MultipleSclerosis,Stroke
andTraumaticBrainInjuries,Pages234–244,2018.
[Jadon2020] ShrutiJadon,Asurveyoflossfunctionsforsemanticsegmentation.InProceedingsoftheIEEEConference
onComputationalIntelligenceinBioinformaticsandComputationalBiology,Pages1–7,2020.
[Jospin2022] LaurentValentinJospin,WrayL.Buntine,FaridBoussaïd,HamidLagaandMohammedBennamoun,
Hands-onBayesianneuralnetworks-Atutorialfordeeplearningusers.ArXive-prints,2022.
[Kelly1956] J.L.Kelly,Anewinterpretationofinformationrate.BellSystTechJ,Volume35,Pages917–926,1956.
[Kingma2015] Diederik P. Kingma and Jimmy Ba, Adam: A method for stochastic optimization. In Proceedings of
InternationalConferenceonLearningRepresentations,EditedbyYoshuaBengioandYannLeCun,2015.
[Kullback1951] S.KullbackandR.A.Leibler,Oninformationandsufficiency.AnnMathStatist,Pages79–86,1951.
[Li2022] SijiaLi,FurkatSultonov,QingshanYe,YongBai,Jun-HyunPark,ChilsigYang,MinseokSong,Sungwoo
KooandJae-MoKang,TA-Unet:Integratingtripletattentionmodulefordrivableroadregionsegmentation.
Sensors,Volume22,2022.
[Lin2018] T.Lin,P.Goyal,R.Girshick,K.HeandP.Dollar,Focallossfordenseobjectdetection.IEEETrans
PatternAnalMachIntell,2018.
[Lo2021] JustinLo,JillianCardinell,AlejoCostanzoandDafnaSussman,Medicalaugmentation(Med-Aug)for
optimaldataaugmentationinmedicaldeeplearningnetworks.Sensors,Volume21,2021.
[Lovász1983] L.Lovász,Submodularfunctionsandconvexity.InMathematicalProgrammingTheStateoftheArt,Pages
235–257,SpringerBerlinHeidelberg,1983.
[Maier2019] Andreas K. Maier, Christopher Syben, Bernhard Stimpel, Tobias Würfl, Mathis Hoffmann, Frank
Schebesch,WeilinFu,LeonidMill,LasseKlingandSilkeH.Christiansen,Learningwithknownoperators
reducesmaximumtrainingerrorbounds.NatMachIntell,Volume1,Pages373–380,2019.
[McMillan1956] B.McMillan,Twoinequalitiesimpliedbyuniquedecipherability.IEEETransInfTheory,Pages115–116,
1956.
[Milletari2016] F.Milletari,N.NavabandS.Ahmadi,V-Net:FullyConvolutionalNeuralNetworksforVolumetricMedical
ImageSegmentation.InInternationalConferenceon3DVision,Pages565–571,2016.
[Mubashar2022] Mehreen Mubashar, Hazrat Ali, Christer Grönlund and Shoaib Azmat, R2U++: A multiscale recur-
rentresidualU-Netwithdenseskipconnectionsformedicalimagesegmentation.NeuralComputAppl,
Volume34,Pages17723–17739,2022.
[Oktay2018] OzanOktay,JoSchlemper,LoicLeFolgoc,MatthewLee,MattiasHeinrich,KazunariMisawa,Kensaku
Mori,StevenMcDonagh,NilsYHammerla,BernhardKainz,BenGlockerandDanielRueckert,Attention
U-Net:Learningwheretolookforthepancreas.ArXive-prints,2018.
[Rakhlin2018] AlexanderRakhlin,AlexDavydowandSergeyNikolenko,Landcoverclassificationfromsatelliteimagery
withU-NetandLovász-Softmaxloss.InProceedingsoftheIEEEConferenceonComputerVisionand
PatternRecognitionWorkshops,Pages257–2574,2018.
[Ronneberger2015] OlafRonneberger,PhilippFischerandThomasBrox,U-Net: Convolutionalnetworksforbiomedical
imagesegmentation.InProceedingsoftheInternationalConferenceonMedicalImageComputingand
Computer-AssistedIntervention,Pages234–241,2015.
[Ruder2016] SebastianRuder,Anoverviewofgradientdescentoptimizationalgorithms.ArXive-prints,2016.
[Smith2022] RyanSmith,KarlJFristonandChristopherJWhyte,Astep-by-steptutorialonactiveinferenceandits
applicationtoempiricaldata.JMathPsychol,Volume107,2022.
[Smoczynski2010] PeterSmoczynskiandDaveTomkins,Anexplicitsolutiontotheproblemofoptimizingtheallocationsofa
Bettor’swealthwhenwageringonhorseraces.MathSci,Volume35,Pages10–17,2010.
35
[Srivastava2014] NitishSrivastava,GeoffreyHinton,AlexKrizhevsky,IlyaSutskeverandRuslanSalakhutdinov,Dropout:
Asimplewaytopreventneuralnetworksfromoverfitting.JMachLearnRes,Volume15,Pages1929–1958,
2014.
[Tatbul2018] NesimeTatbul,TaeJunLee,StanZdonik,MejbahAlamandJustinGottschlich,Precisionandrecallfor
timeseries.InProceedingsofAdvancesinNeuralInformationProcessingSystems,Volume31,2018.
[Ulyanov2016] DmitryUlyanov,AndreaVedaldiandVictorLempitsky,Instancenormalization:Themissingingredient
forfaststylization.ArXive-prints,2016.
[Wu2020] YuxinWuandKaimingHe,Groupnormalization.IntJComputVision,Volume128,Pages742–755,2020.
[Yu2020] JiaqianYuandMatthewB.Blaschko,TheLovaszhinge:Anovelconvexsurrogateforsubmodularlosses.
IEEETransPatternAnalMachIntell,Volume42,Pages735–748,2020.
[Zuo2021] QiangZuo,SongyuChenandZhifangWang,R2AU-Net:Attentionrecurrentresidualconvolutionalneural
networkformultimodalmedicalimagesegmentation.SecurCommunNetw,Pages1–10,2021.
36

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Active Inference-Based Optimization of Discriminative Neural Network Classifiers"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
