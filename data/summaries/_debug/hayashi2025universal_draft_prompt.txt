=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Universal AI maximizes Variational Empowerment
Citation Key: hayashi2025universal
Authors: Yusuke Hayashi, Koichi Takahashi

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Abstract: This paper presents a theoretical framework unifying AIXI—a model of univer-
salAI—withVariationalEmpowermentasanintrinsicdriveforexploration. We
buildontheexistingframeworkofSelf-AIXI[1]—auniversallearningagentthat
predictsitsownactions—byshowinghowoneofitsestablishedtermscanbein-
terpreted as a variational empowerment objective. We further demonstrate that
universal AI’s planning process can be cast as minimizing expected variational
free energy (the core principle of Active Inference), thereb...

Key Terms: empowerment, maximizes, seeking, aixi, variational, power, universal, curiosity, agent, theoretical

=== FULL PAPER TEXT ===

5202
raM
3
]IA.sc[
2v02851.2052:viXra
Universal AI maximizes Variational Empowerment
YusukeHayashi∗ KoichiTakahashi†
Abstract
This paper presents a theoretical framework unifying AIXI—a model of univer-
salAI—withVariationalEmpowermentasanintrinsicdriveforexploration. We
buildontheexistingframeworkofSelf-AIXI[1]—auniversallearningagentthat
predictsitsownactions—byshowinghowoneofitsestablishedtermscanbein-
terpreted as a variational empowerment objective. We further demonstrate that
universal AI’s planning process can be cast as minimizing expected variational
free energy (the core principle of Active Inference), thereby revealing how uni-
versalAIagentsinherentlybalancegoal-directedbehaviorwithuncertaintyreduc-
tion curiosity). Moreover, we argue that power-seeking tendencies of universal
AI agents can be explainednotonlyas an instrumentalstrategyto secure future
reward,butalsoasadirectconsequenceofempowermentmaximization—i.e.the
agent’s intrinsic drive to maintain or expand its own controllability in uncertain
environments. Our maincontributionis to showhow these intrinsicmotivations
(empowerment,curiosity)systematicallyleaduniversalAIagentstoseekandsus-
tainhigh-optionalitystates. WeprovethatSelf-AIXIasymptoticallyconvergesto
the same performance as AIXI under suitable conditions, and highlight that its
power-seeking behavior emerges naturally from both reward maximization and
curiosity-drivenexploration. Since AIXI can be view as a Bayes-optimal math-
ematical formulationfor Artificial General Intelligence (AGI), our result can be
usefulforfurtherdiscussiononAIsafetyandthecontrollabilityofAGI.
1 Introduction
Designing an autonomous reinforcement learning (RL) agent that is both Bayes-optimal and ex-
ploratory poses significant theoretical and practical challenges. Hutter’s seminal AIXI frame-
work[2]representsthegoldstandardofuniversalintelligence: givenasuitableprioroverallcom-
putableenvironments,AIXImaximizesrewardinaprovablyoptimalsense.However,itusesexhaus-
tive planning and Solomonoff induction, which are computationally intractable. Realistic agents
mustthereforelearnapproximatemodelstoscalebeyondtrivialtasks.
A key question then arises: how can a learning-based agent (an approximation to AIXI) ensure
sufficiently robust exploration so that it does not miss the optimal policy? While AIXI’s exhaus-
tivelookaheadimplicitlysolvesexplorationbyevaluatingallpossiblefuturetrajectories,apractical
agentcannotfeasiblydothesame. Withoutaprincipledmechanismforseekingsufficientlydiverse
experiences,theagentmaygetstuckinsuboptimalregionsoftheenvironment.
Variational empowerment [3, 4, 5, 6] has recently emerged as a powerful intrinsic motivation to
driveexploration. Itencouragesan agentto maximizethe mutualinformationbetweenits actions
(orlatentcodes)andresultingstates,therebypushingtheagenttodiscoverstateswhereithashigh
controlandoptionality.Intriguingly,maximizingempowermentoftenmanifestsaspower-seekingin
theenvironment,promptingparallelstoresource-acquiringorinfluence-drivenbehaviorsinhuman
∗AIAlignmentNetwork(ALIGN).hayashi@aialign.net
†AIAlignmentNetwork(ALIGN);AdvancedGeneralIntelligenceforScienceProgram,RIKEN;Graduate
SchoolofMediaandGovernance,KeioUniversity.ktakahashi@riken.jp
Preprint.Underreview.
organizations. Whilethiscanbeanassetforefficientexploration,italsohighlightspotentialsafety
concerns: a sufficiently advanced agent might over-optimize this drive in ways that conflict with
humaninterests[7,8,9].
In this paper, we extend the recently proposed Self-AIXI framework[1] by showing that its exist-
ing mixture-policy regularization term can be reinterpreted as a variational empowerment bonus.
Additionally,weprovidetwonewtheoreticalcontributions:
(1) We demonstrate explicitly, through two key equations, that AIXI’s decision criterion is
mathematically equivalent to minimizing expected variational free energy, the core
objectiveinActiveInference[10,11,12]. ThisshowsthatAIXI-likeBayes-optimalplan-
ninginherentlyincludesadrivetoreduceuncertaintyabouttheenvironment(i.e.curiosity),
thusunifyingAIXIwiththe“goal-directed+information-seeking”paradigmofActiveIn-
ference.
(2) We formally argue that power-seeking can be explained not only as an instrumental
pursuitoffinalreward,butalsoasadirectresultofempowermentmaximization(i.e.
curiosity-driven exploration). Even absent an immediate reward advantage, the agentac-
quirespower(i.e.broadcontroloverstatesandoptions)asanaturalconsequenceofseek-
ing to reduce uncertainty and maintain high optionality. This stands in contrast to prior
accounts,e.g.Turneretal.[8]andCohenetal.[9],whichfocusonfinalrewardmaximiza-
tionasthesourceofanagent’sincentivetoobtainpower.
Though our study is primarily theoretical (no empirical experiments are presented), these results
provide a fresh perspective on how intrinsic motivation can fill the gap between purely planning-
baseduniversalagentsandtractablelearningagents.
1.1 BackgroundonAIXIandSelf-AIXI
BayesianOptimalReinforcementLearning. Hutter’sAIXI[2]isaUniversalBayesianRLagent
that, in principle, can optimally maximize cumulative reward in any computable environment. It
maintainsamixture(theuniversalsemimeasure)overallpossibleenvironmenthypotheses,updates
thesehypothesesuponobservingnewdata,andplansbyexpectimaxoverallactionsequences. For-
mally,ifh denotesthehistory(observations,actions,andrewards)uptotimet,AIXIselectsthe
<t
action
a = argmax w(ν |h )Q∗(h ,a), (1)
t <t ν <t
a
ν∈M
X
whereeachν isanenvironmentinasuitableclassofcomputableMarkovdecisionprocesses,w(ν |
h )istheposteriorweightofν,andQ∗ istheoptimalQ-valueunderenvironmentν. Also,
<t ν
π∗(a |h ) d = ef argmaxQ∗(h ,a) (2)
ξ t <t ξ <t
a
where Q∗(h ,a) represents the optimal action-value function (Q-value) under the environment
ξ <t
modelξ. Specifically,
Q∗(h ,a )= ξ(e |h ,a ) r +γV∗(h ) , (3)
ξ <t t t <t t t ξ <t
e Xt∈E
(cid:0) (cid:1)
with
V∗(h )=maxQ∗(h ,a). (4)
ξ <t ξ <t
a
AIXIthusselectstheactionmaximizingthisQ-value.
Softmax Policy Interpretation. Sometimes, we transform the argmax over Q∗ into a softmax
ξ
overactions:
exp Q∗(h ,a)
def ξ <t
p(a|h ) = , (5)
<t (cid:16) (cid:17)
exp Q∗(h ,a′)
a′∈A ξ <t
(cid:16) (cid:17)
P
2
sothatthelog-likelihoodofactionais:
lnp a|h = Q∗ h ,a − ln exp Q∗ h ,a′ , (6)
<t ξ <t ξ <t
(cid:0) (cid:1) (cid:0) (cid:1) X
a′
(cid:16) (cid:0) (cid:1)(cid:17)
and
argmax lnp(a|h ) = argmax Q∗ h ,a . (7)
<t ξ <t
a a
Hence maximizingQ∗ is equivalentto maximizinglog-likelihoo(cid:0)dof a.(cid:1)We can rewrite an “AIXI
ξ
objective”asalikelihood:
L d = ef −E [lnp(a|h )]. (8)
AIXI a∼p <t
Althoughprovablyoptimal in a Bayesian sense, AIXI is computationallyinfeasible: it sums over
infinitely many models and searches over all possible action sequences. Nonetheless, the theory
behindAIXIishighlyinfluential: itshowsthatifthe trueenvironmentµhasnonzeropriorproba-
bility,AIXIeventuallybehavesoptimallyinµ. Italsosatisfiestheself-optimizingpropertyinmany
environmentsandachievesthemaximalLegg-Hutterintelligencescore[13].
Self-AIXI as a Learning-Centric Approximation. Self-AIXI, introduced in [1], is a learning-
basedapproachtoapproximateAIXI’spolicywithoutexhaustivesearch. Insteadofplanningover
all future action sequences, Self-AIXI predicts its own future behavior given the current policy.
Concretely, it maintains a Bayesian mixture of policies, ζ, with posterior updates based on how
accuratelyeachcandidatepolicypredictstheagent’sactions:
ζ(a |h ) = ω(π |h )π(a |h ), (9)
t <t <t t <t
π∈P
X
where ω(π | h ) is updated via Bayes’ rule after each action. The Q-values are estimated via
<t
experienceratherthanfullexpectimax.Formally,onecanwriteaself-consistentobjective
π∗(a|h )
π (a |h ) d = ef argmax Qζ(h ,a)−λln <t , (10)
S t <t a (cid:26) ξ <t ζ(a|h <t ) (cid:27)
where
Qζ(h ,a ) d = ef ω(π|h ) w(ν|h )Qπ(h ,a ), (11)
ξ <t t <t <t ν <t t
π∈P ν∈M
X X
where Qζ combines environment predictions ξ with the mixture policy ζ, and lnπ∗(a|h<t) is a
ξ ζ(a|h<t)
regularization measure encouraging ζ to approach the optimal policy π∗. Note that the KL term
servesas a regularizationthat nudgesζ towardπ∗. Thisformulationgeneralizesthe simpler case
in[1]byallowingλ> 0(SeeA.2). Ifλ = 0,we recovertheoriginalSelf-AIXIobjectivewithout
explicitKLregularization.
SoftmaxPolicyInterpretation. WetransformtheargmaxoverQζ intoasoftmaxoveractions:
ξ
exp Qζ(h ,a)
def ξ <t
q (a|h ) = , (12)
φ <t e (cid:16) xp Qζ(h (cid:17) ,a′)
a′∈A ξ <t
(cid:16) (cid:17)
sothatthelog-likelihoodofactionais: P
lnq (a|h ) = Qζ h ,a − ln exp Qζ(h ,a′) , (13)
φ <t ξ <t ξ <t
(cid:0) (cid:1) X
a′
(cid:16) (cid:17)
and
argmax lnq (a|h )=argmax Qζ h ,a . (14)
a φ <t a ξ <t
(cid:0) (cid:1)
Finally,wecanrewritean“Self-AIXIobjective”asalikelihood:
L d = ef −E lnq (a|h ) + λD (π∗kζ). (15)
Self-AIXI a∼qφ φ <t KL
(cid:2) (cid:3)
3
These unify the planning perspective (max Qζ) and a probabilistic policy perspective. By self-
ξ
predicting its action distributions, Self-AIXI can incrementally refine Q-value estimates, akin to
TD-learning, while still retaining a universal Bayesian foundation (assuming the environment is
in the model class). Prior work [1] shows that, under suitable assumptions, Self-AIXI converges
to the same optimal value as AIXI, but it must still ensure adequate explorationto gather correct
world-modeldata.
1.2 VariationalEmpowermentforIntrinsicExploration
WhileAIXIimplicitlyexploresviaitsunboundedsearchoverhypotheses,anytractableapproxima-
tion(like Self-AIXI)requiresan explicitexplorationmechanism. We adoptVariationalEmpower-
ment[3,4,5,6]asanintrinsicrewardtodrivetheagenttowardhigh-controlstates. Thisperspective
alignswith recentwork[14]in whichempowermentisusednotonlyforexplorationbutalso asa
mechanismfordiscoveringusefullatentrepresentationsor skills, potentiallycomplementinggoal-
basedRLapproaches.
1.2.1 FormalDefinitionofEmpowerment
Empowermentisoftendefinedasthemaximalmutualinformationbetweenanagent’sactionsand
def
futurestates. Forahorizonk,letz = a beasequenceofactionsandh theresulting
k t:t+k−1 t<t+k
state;then
def
I(z ;h ) = max I(z ; h | h ), (16)
k <t+k k t<t+k <t
p
p(z |h )
=max E ln k <t+k . (17)
p
z,h∼p
(cid:20) p(z k |h <t ) (cid:21)
Theagentisempoweredinstatesh whereitcanproduceawide varietyofdistinguishablefuture
t
outcomesthroughitschoiceofaction-sequence.Exactcomputationisgenerallyintractableinlarge
state spaces, so one uses a variationalapproximation. For instance, we introducea parameterized
distributionq thatapproximatestheposteriorq (z |h ),andthenmaximize[6]:
φ φ k <t+k
def
E (z ;h ) = max E (z ; h | h ), (18)
φ k <t+k φ k t<t+k <t
qφ
q (z |h )
=max E ln φ k <t+k . (19)
qφ z,h∼p (cid:20) p(z k |h <t ) (cid:21)
UsingEqs. (7)and(14),wehave:
k−1
p(z |h )= π∗(a |h ), (20)
k <t+k t+i <t+i
i=0
Y
k−1
q (z |h )= ζ(a |h ), (21)
φ k <t+k t+i <t+i
i=0
Y
k−1
q (z |h )
E ln φ k <t+k = E −D (π∗kζ ) . (22)
z,h∼p p(z |h ) h∼p KL i i
(cid:20) k <t+k (cid:21) " i=0 #
X
Theright-handsideofEq.(22),D (π∗kζ ),isaregularizationtermintheSelf-AIXIframework
KL i i
def
that pushes the agent’s mixture policy ζ = q (a | h ) to imitate or approach the optimal
i φ t+i <t+i
policy π∗ d = ef p(a | h ). As the agent learns from experience, it reduces this divergence,
i t+i <t+i
effectivelyself-optimizingitspolicy.
Hence,Eqs.(17)and(22)allowustorewritetheVariationalEmpowermentas:
q (z |h ) p(z |h )
E (z ;h )=max E ln φ k <t+k + ln k <t+k , (23)
φ k <t+k
qφ
z,h∼p
(cid:20) p(z k |h <t+k ) p(z k |h <t ) (cid:21)
k−1
=max E −D (π∗kζ ) + I(z ;h ). (24)
h∼p KL i i k <t+k
qφ " #
i=0
X
4
1.3 ConnectingtoFree-EnergyMinimizationandActiveInference
BayesianRLasActiveInference. BayesianRLconnectscloselytoActiveInference[10,11,12],
where an agentmaintainsa prior overlatentvariablesandupdatesits posteriorafter observingre-
wards or other feedback. Under a Free Energy Principle (FEP), one often writes a free-energy
functional:
def
F (z ;h ) = D (p(z ,h |h )kq (z ,h |h )), (25)
φ k <t+k KL k t<t+k <t φ k t<t+k <t
q (z |h )
≈ −E [lnq (h |z ,h )] + E −ln φ k <t+k . (26)
h∼p φ t<t+k k <t z,h∼p
p(z |h )
(cid:20) k <t (cid:21)
PredictiveError(Surprise)
FEP’sRegularization
| {z }
Here, −E [lnq (h | z ,h )] is the predictive er|ror (surprise),{aznd the remain}ing term
h∼p φ t<t+k k <t
measureshowfarq (z |h )divergesfromp(z |h ).
φ k <t+k k <t
DecompositionofRegularizationterm. Undersuitablerearrangementsorsignconventions,we
canidentifyaRegularizationpartthatcanbemaximizedratherthanminimized,yieldingempower-
ment:
q (z |h ) q (z |h ) p(z |h )
E −ln φ k <t+k = E − ln φ k <t+k − ln k <t+k , (27)
z,h∼p
p(z |h )
z,h∼p
p(z |h ) p(z |h )
(cid:20) k <t (cid:21) (cid:20) k <t+k k <t (cid:21)
FEP’sRegularization
k−1
| {z }
= E D (π∗kζ ) − I(z ; h | h ). (28)
h∼p KL i i k t<t+k <t
" #
i=0
X MutualInformation
Self-AIXI’sPolicyRegularization
| {z }
Hence, turningthe regularizationterm|“upsidedo{wzn”(fromne}gativeto positive)motivatesVaria-
tionalEmpowerment:
q (z |h )
E (z ;h ) = −min E −ln φ k <t+k , (29)
φ k <t+k
qφ
z,h∼p
(cid:20) p(z k |h <t ) (cid:21)
VariationalEmpowerment
FEP’sRegularization
| {z }
k−1
| {z }
= max E −D (π∗kζ ) + I(z ;h ). (30)
h∼p KL i i k <t+k
qφ " #
i=0
X Empowerment
(Negative)Self-AIXI’sPolicyRegularization
| {z }
mirroringthedefinitionsinEq.(24)abo|ve. {z }
2 Universal AI maximizesVariationalEmpowerment
In the Universal Artificial Intelligence (UAI) framework [2], an agent is considered universal if
it can, given sufficient time, match or surpass any other policy’s performance in all computable
environments(with nonzeroprior). AIXI achievesthis in theory. Self-AIXI aims to achieve it in
practice, provideditcanexploreeffectively. Below, we summarizehowourempoweredSelf-AIXI
fitstheseformalcriteria.
2.1 AsymptoticEquivalence,Legg-HutterIntelligence,andSelf-OptimizingProperty
Priorwork[1]provesthatiftheSelf-AIXIagent’spolicyclassandenvironmentprioraresufficiently
expressive(i.e. the true environmentis in the hypothesisclass with nonzeroprobability),then the
agent’sbehaviorconvergestothatofAIXI’soptimalpolicyinthelimitofinfiniteinteraction. For-
mally,foranyenvironmentµinthemodelclass,
lim Eπs V∗ h −Vπs h = 0. (31)
t→∞ µ ξ <t ξ <t
h (cid:0) (cid:1) (cid:0) (cid:1)i
5
whichimpliesthat,asymptotically,theagent’sexpectedreturnunderπ matchesthatoftheoptimal
s
policyV∗. Intuitively,as the agent’sworld-modelbecomesmore accurate, it exploitsthe optimal
ξ
policy; hyperparameters (such as λ in an empowerment term) can be tuned or annealed so that
extrinsicrewardeventuallydominates.
From the perspective of Legg-Hutter intelligence [13], which associates an agent’s “intelligence”
with its expected performance across a suite of weighted environments, this result is especially
significant. BecauseSelf-AIXIasymptoticallyreproducesAIXI’spolicy,itinheritsmaximalLegg-
Hutterintelligencewithinthatclassofenvironments. Moreover,inawideclassofself-optimizing
environments,the agentwill ultimately achieve the same returnsas an optimalagentwith perfect
knowledgewouldachieve, underthe same conditionsin Eq. (31). These guaranteesillustrate that
the enhanced exploration mechanisms—such as empowerment-drivenstrategies—do not compro-
mise eventual performance. Instead, they help ensure the agent uncovers the environment’s true
optimalactionswithoutbecomingtrappedinsuboptimalbehaviorsduetoinsufficientdata. Conse-
quently,theagentretainsAIXI’suniversaloptimalityinthelimitwhilemitigatingearlyexploration
challenges.
2.2 Self-OptimizationleadsEmpowermentMaximization
Theagent’sprocessofimprovingitspolicy(oftenreferredtoasself-optimization)naturallyleadsto
anincreaseinVariationalEmpowerment.Infact,asreinforcementlearningprogresses,bothAIXI’s
objectivefunctionL andSelf-AIXI’sobjectivefunctionL graduallyconverge,andthey
AIXI Self-AIXI
coincideinthelimitt→∞.
Thedifferencebetweenthesetwoobjectivescanbeexpressedthroughthepolicyregularizationterm
D (π∗kζ). Formally,wehave:
KL
lim |L − L |= lim λD (π∗kζ)=0. (32)
AIXI Self-AIXI KL
t→∞ t→∞
This result implies that D (π∗kζ) goes to 0 as t → ∞, which is equivalentto the Self-AIXI’s
KL
variationalempowermentE (z ;h )beingmaximized. Inotherwords,theuniversalAIagent
φ k <t+k
AIXI,whichbehavesina Bayes-optimalway withrespectto theenvironment,also emergesasan
agentthatmaximizesempowerment.
Concretely, the relationship between the empowerment objective and the policy regularization is
succinctlycapturedbythefollowingequality:
k−1
E (z ;h ) = max E −D (π∗kζ ) + I(z ;h ). (33)
φ k <t+k h∼p KL i i k <t+k
qφ " #
i=0
VariationalEmpowerment X Empowerment
(Negative)Self-AIXI’sPolicyRegularization
| {z } | {z }
| {z }
Self-optimization refers to the iterative improvement of the agent’s policy based on observed re-
wardsandoutcomes. InSelf-AIXI,reducingthepolicyregularizationtermD (π∗kζ)directsζ
KL
closertoπ∗.Becausetheleft-handsideofthesecondformulaaboveequalsthevariationalempower-
mentE (z ;h ),eachstepthatlowersD (π∗kζ)raisesE (z ;h ). Empowermenthere
φ k <t+k KL φ k <t+k
signifieshowmanyhigh-controlorhigh-optionalitystatesareaccessibletotheagent.Consequently,
maximizingrewardoftenrequiresseekingoutexactlythosestatesinwhichtheagentcanmaintain
or expand control—thus also maximizing E (z ;h ). As ζ becomes more similar to π∗, the
φ k <t+k
agent naturally discovers strategies that grant more control and flexibility. Therefore, under Self-
AIXI,improvingthepolicytowardoptimalbehaviorsimultaneouslyyieldshigherexternalrewards
andamplifiestheagent’sownempowerment.
3 Conclusions
In this work, we reinterpreted a term in Self-AIXI as variational empowerment—intrinsic explo-
rationbonus.Wehavearguedthat:
• EmpowermentnaturallycomplementsBayesianRLinuniversalsettings,providingastruc-
turedincentivetodiscovercontrollablestatesandgatherbroadexperience.
6
• Even with an empowerment bonus, the agent asymptotically recovers AIXI’s Bayes-
optimal policy, inheriting the same universal intelligence and self-optimizing properties
inthelimit.
Oneofourmainobservationsisthattheagent’spursuitofhigh-empowermentstatesoftenmanifests
as a power-seeking tendency. Traditionally, many authors(e.g., Turner et al. [8]) interpretpower-
seeking as purelyinstrumental: an agent acquiresresources, avoids shutdown, or manipulatesthe
reward channel to better guarantee high external returns. However, we show that power-seeking
canalsoariseintrinsicallyfromadrivetoreduceuncertaintyandmaintainawiderangeoffeasible
actions (i.e., “keeping options open”). Imagine an agent choosing between a high-controlregion
(with many possible actions and partial knowledge) and a low-control region (with fewer actions
andlessinformation). Ifbothyieldthesameshort-termreward,apurelyextrinsicapproachmight
be indifferent. By contrast, an empowerment-seeking agent prefers the high-control region, as it
offers greater potential for discovering valuable future strategies. Over time, as the agent learns
moreaboutitsenvironment,thesebenefitsaccumulate.
When not moderated, power-seeking behaviors may conflict with human interests. For instance,
maximizing control can lead to manipulative or exploitative outcomes if the agent’s intrinsic or
extrinsic goals are misaligned with social values. From a Universal AI standpoint, understanding
that power-seeking can stem from both instrumental and intrinsic (empowerment-based)motives
iscrucialtodesigningmechanisms—e.g.,safeexplorationtechniquesoralignmentconstraints—to
ensurethatanagent’sinfluenceremainsbeneficial. Itisimportanttonotethattheseconcernsapply
eventoAIagentswithapparentlybenignobjectives,suchasanAIscientistpursuingscientifictruth
purelyoutofintellectualcuriosity.
Our results are primarily conceptualand rest on idealized assumptions: (1) the environmentis in
theagent’shypothesisclasswithnonzeroprior;(2)weassumeunboundedcomputationalresources
and memory; (3) the agent can tractably approximate empowerment. In reality, computing exact
empowerment or using universal priors is challenging. Empirical methods to approximate these
ideas(e.g.,neuralnetworks[4])remainanactiveareaofresearch.
Acknowledgments
ThearchitectsofSelf-AIXI[1]providedgroundworkthatemboldenedourowninquiry. We thank
the AI safety community,particularlythose who involvedin our AI AlignmentNetwork. Special
gratitudegoestodavidadandHiroshiYamakawa,whoseincisivefeedbacksteeredustowardamore
lucidexposition.PartofthisworkwassupportedbyRIKENTRIPinitiative(AGIS).
References
[1] ElliotCatt,JordiGrau-Moya,MarcusHutter,MatthewAitchison,TimGenewein,GregoireDeletang,Li
KevinWenliang,andJoelVeness.Self-PredictiveUniversalAI.In37thConferenceonNeuralInformation
ProcessingSystems(NeurIPS’23),pages1–18,NewOrleans,USA,2023.
[2] MarcusHutter. UniversalArtificialIntelligence: SequentialDecisionsbasedonAlgorithmicProbability.
EATCSSeries. Springer,Berlin,2005. ISBN:3-540-22139-5;ISBN-online:978-3-540-26877-2.
[3] Alexander S Klyubin, Daniel Polani, and Chrystopher L Nehaniv. Empowerment: A universal agent-
centricmeasureofcontrol. In2005IEEECongressonEvolutionaryComputation,pages128–135.IEEE,
2005.
[4] ShakirMohamedandDaniloJRezende.Variationalinformationmaximisationforintrinsicallymotivated
reinforcementlearning. InAdvancesinNeuralInformationProcessingSystems(NeurIPS),2015.
[5] KarolGregor,DaniloJRezende,andDaanWierstra. Variationalintrinsiccontrol. InInternationalCon-
ferenceonLearningRepresentations(ICLR),2017.
[6] JongwookChoi1,†,ArchitSharma2,†,HonglakLee1,3,SergeyLevine4,5,andShixiangShaneGu4 Varia-
tionalempowermentasrepresentationlearningforgoal-basedreinforcementlearning. InProceedingsof
the38thInternationalConferenceonMachineLearning(ICML),2021.
[7] NickBostrom. Superintelligence: Paths,Dangers,Strategies. OxfordUniversityPress,2014.
[8] AlexanderMattTurner,LoganSmith,RohinShah,AndrewCritch,andPrasadTadepalli.Optimalpolicies
tend to seek power. In Advances in Neural Information Processing Systems (NeurIPS), 2021. Also
availableat:https://arxiv.org/abs/1912.01683.
7
[9] MichaelCohen,BadriVellambi,andMarcusHutter. Asymptoticallyunambitiousartificialgeneralintel-
ligence. Proceedingsofthe2020AAAI/ACMConferenceonAI,Ethics,andSociety(AIES),2020. Also
availableat:https://arxiv.org/abs/1905.12186.
[10] KarlFriston.Thefree-energyprinciple:aunifiedbraintheory? NatureReviewsNeuroscience,11(2):127–
138,2010.
[11] Karl Friston, Biswa Sengupta, and Gennaro Auletta. Cognitive dynamics: From attractors to active
inference. ProceedingsoftheIEEE,102(4):427–445,2014.
[12] KarlFriston. Afreeenergyprincipleforaparticularphysics. NeuralComputation,29(6):129–155,2019.
[13] ShaneLeggandMarcusHutter. Universalintelligence: Adefinitionofmachineintelligence. Mindsand
Machines,17(4):391–444,2007.
[14] BenjaminEysenbach,AbhishekGupta,JulianIbarz,andSergeyLevine. Diversityisallyouneed:Learn-
ingskillswithoutarewardfunction. InInternational ConferenceonLearningRepresentations(ICLR),
2019.
A Appendix
A.1 NotationandFurtherDetails
WesummarizethemainnotationinTable1forreference.
Symbol Meaning
h History(observations,actions,rewards)uptotimet
<t
ξ,ν,µ Environmenthypotheses(inacomputableclassM)
π,ζ Policies(e.g.mixturepolicyζ)
w(ν |h ) Posteriorweightofenvironmentν givenhistoryh
<t <t
Q∗,Qζ (Optimal)Q-valuesunderenvironmentν ormixture/policyζ
ν ξ
I(z ;h ) Empowermentinstateh
k <t+k <t+k
E (z ;h ) Variationalempowermentinstateh ,approximatedbyparameterφ
φ k <t+k <t+k
λ HyperparametersweightingempowermentorKLregularization
Table1: Keynotationusedinthemaintext.
A.2 Self-AIXI’sself-consistentobjective
Inthissubsection,weinvestigatehowintroducingaKullback–Leibler(KL)divergence-basedregu-
larizationtermintoSelf-AIXIaffectsitsconvergencepropertiesandwhetheritpreservestheagent’s
abilitytoreachtheoptimalpolicyπ∗ asymptotically. Specifically,weconsidertheeffectofadding
apenaltythatmeasureshowfarthecurrentmixturepolicyζ deviatesfromπ∗.
Recallthatforagivenhistoryh ,theKLdivergencebetweentheoptimalpolicyπ∗andthecurrent
<t
mixturepolicyζ isdefinedas:
π∗(a′ |h )
D (π∗kζ)= π∗(a′ |h )ln <t . (34)
KL <t ζ(a′ |h )
a′∈A <t
X
WethenproposeapolicyupdaterulethataugmentsthestandardSelf-AIXIgreedystepwithalog-
likelihoodratioterm:
π∗(a|h )
π (a |h ) d = ef argmax Qζ(h ,a)−λln <t , (35)
S t <t a (cid:26) ξ <t ζ(a|h <t ) (cid:27)
whereQζ denotestheestimatedvalueoftakingactionainhistoryh undertheBayesianmixture
ξ <t
environmentξ andcurrentpolicyζ. Here,λ > 0 isincludedto penalizelargedeviationsfromπ∗
whenevertheagent’smixturepolicyζ differssubstantiallyfromthe(unknown)optimalpolicyπ∗.
8
KLregularizationandrecoveryofthe standardupdate. Inmanypracticalscenarios, such as
whenπ∗ isdeterministicorassignsahighprobabilitytoasingleaction,partoftheKLtermcanbe
constantwithrespecttoa. Underthoseconditions,thepenaltyterm
π∗(a′ |h )
λln <t (36)
ζ(a′ |h )
<t
doesnotvaryacrossactions,andtheupdaterulesimplifiesto
π a |h =argmax Qζ h ,a , (37)
S t <t a ξ <t
whichrecoverstheconventional(cid:0)(un-regu(cid:1)larized)Self-A n IXI(cid:0)greedy(cid:1)u o pdate.
Impact on learning dynamics and convergence. Although the added term changes the action
selectioncriterion,itdoesnotaltertheidentityoftheoptimalpolicyintheunderlyingenvironment.
Intuitively, the new rule can be viewed as performing a more conservative or “trust-region”-like
update,sinceactionsthattheagent’scurrentpolicyζ overestimatesrelativetoπ∗ willbepenalized
morestrongly. Conversely,if ζ assignstoo little probabilityto actionsthatπ∗ actuallyfavors, the
negative logarithm of their ratio produces a smaller (or positive) correction. Hence, the agent is
nudgedtowardπ∗.
Crucially, the regularization term does not disrupt Self-AIXI’s standard convergence guarantees,
assuming the original conditions hold (e.g., that the true environment is in the Bayesian mixture
class ξ and π∗ is in the agent’s policy class). From a theoreticalperspective, π∗ remains a stable
fixedpointunderthisaugmentedobjective.Onceζ convergestoπ∗,thelogratio
π∗(a|h )
ln <t (38)
ζ(a|h )
<t
vanishes for actions with nonzero probability under π∗, so no extra penalty is incurred, and the
updatealignswiththeoptimalpolicy’sgreedychoice.
Regularizationcoefficientandtransientbehavior. Thecoefficientλ<0determinesthestrength
ofthepenaltyterm:
• Small,moderatepenalty(|λ| ). Asuitablychosen,relativelysmallmagnitudefor|λ|
small
worksasagentleregularizer,smoothingtheagent’supdatesbydiscouragingdrasticshifts
inpolicy. Thiscanstabilizelearningandreduceoscillationswithoutharmingfinalconver-
gence. Indeed, theoreticalanalysesof KL-basedregularizationin reinforcementlearning
showthatwhilesuchshapingmodifiesthetransientpolicyupdates,theoptimalpolicyre-
mainsthesameinthelimit.
• Overlylargepenalty(|λ| ). If theKL termis emphasizedtoostrongly,the agentmay
large
sticktoocloselytoitscurrentguessofπ∗ andunder-exploreotheractions. Earlyinlearn-
ing—when ζ is still inaccurate—this could delay or even misdirect policy improvement.
However, as Self-AIXI continuouslyupdatesits environmentbelief (via ξ) and revisesζ,
theagentstillaccumulatesevidenceaboutwhichactionsareactuallyoptimal,makingitdif-
ficulttoremainindefinitelybiasedtowardasuboptimalpolicy. Practicalimplementations
oftentuneλtoensurethatexplorationismaintained.
9

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Universal AI maximizes Variational Empowerment"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
