=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Active Inference through Incentive Design in Markov Decision Processes
Citation Key: wei2025active
Authors: Xinyi Wei, Chongyang Shi, Shuo Han

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Key Terms: decision, markov, types, problem, processes, design, leader, incentive, follower, inference

=== FULL PAPER TEXT ===

Active Inference through Incentive Design in Markov Decision Processes
XinyiWei∗1, ChongyangShi∗1, ShuoHan2, AhmedH.Hemida3, CharlesA.Kamhoua3, Jie
Fu1
1UniversityofFlorida,Gainesville,FL
2UniversityofIllinois,Chicago. 3DEVCOMArmyResearchLaboratory
{weixinyi,c.shi,fujie}@ufl.edu,hanshuo@uic.edu{ahmed.h.hemida.ctr,
charles.a.kamhoua.civ}@army.mil
Abstract accountingfortheactiveinteractionsofmultipleusersorfol-
lowers. It is applied in various domains such as economic
Wepresentamethodforactiveinferencewithpar- market[Myerson,1981;Williams,2011;EasleyandGhosh,
tial observations in stochastic systems through in- 2015],smartcity[Meietal.,2017;Kazhamiakinetal.,2015],
centive design, also known as the leader-follower and smart grid [Braithwait et al., 2006; Alquthami et al.,
game. Consideraleaderagentwhoaimstoinfera 2021].
follower agent’s type given a finite set of possible In the classical problem formulation, the leader and fol-
types. Different types of followers differ in either lowers have respective payoff/reward functions and aim to
thedynamicalmodel,therewardfunction,orboth. optimize their own rewards. We focus on a specific class
We assume the leader can partially observe a fol- of incentive design where the leader is to infer the follow-
lower’s behavior in the stochastic system modeled ers’type(intentions,rewards,ordynamicmodel)bypartially
asaMarkovdecisionprocess,inwhichthefollower observing the follower’s active interactions with the system.
takesanoptimalpolicytomaximizeatotalreward. We refer to this as the problem of active inference through
To improve inference accuracy and efficiency, the incentivedesign.
leader can offer side payments (incentives) to the In particular, consider a finite hypothesis set of possible
followers such that different types of them, under followers, where each follower’s planning problem is mod-
the incentive design, can exhibit diverging behav- eledasaMarkovdecisionprocess(MDP)withtheobjective
iors that facilitate the leader’s inference task. We of maximizing total discounted rewards. Followers’ MDPs
show the problem of active inference through in- may differ in their transition dynamics, discount factors, or
centivedesigncanbeformulatedasaspecialclass rewardfunctions. Ateachtimestep, theleaderobservesthe
of leader-follower games, where the leader’s ob- activity of one follower, selected from the hypothesis set,
jective is to balance the information gain and cost throughimperfectandnoisyobservations. Theleader’sgoal
of incentive design. The information gain is mea- istoidentifywhichfolloweriscurrentlyinteractingwiththe
sured by the entropy of the estimated follower’s system. To improve inference accuracy within a finite time
type given partial observations. Furthermore, we horizon,theleaderstrategicallydesignsanincentivepolicyto
demonstratethatthisproblemcanbesolvedbyre- offersidepayments(additionalrewards)withintheenviron-
ducingasingle-leveloptimizationthroughsoftmax ment,influencingthefollowers’bestresponses. Theseincen-
temporal consistency between followers’ policies tives,however,incuradirectcosttotheleader’sownpayoff.
and value functions. This reduction allows us to With only partial, noisy observations of the follower’s state
develop an efficient gradient-based algorithm. We sequence, the leader calibrates the incentive policy to mini-
utilize observable operators in the hidden Markov mize uncertainty about the follower’s type, while balancing
model(HMM)tocomputethenecessarygradients thetrade-offbetweeninferenceaccuracyandthecostofpro-
and demonstrate the effectiveness of our approach vidingincentives.
throughexperimentsinstochasticgridworldenvi-
Relatedwork. Incentivedesignhasbeenstudiedindiffer-
ronments.
ent contexts, particularly in federated learning for IoT en-
vironments and mobile ad hoc networks (MANETs). Zhan
et al. [Zhan et al., 2020] used deep reinforcement learn-
1 Introduction
ing for optimal pricing strategies in federated learning sys-
Incentivedesign[BoltonandDewatripont,2005], isalsore- tems, while Li et al. [Li and Shen, 2011] applied game the-
ferredtoastheprincipal-agentorleader-followergame. Ho ory to study cooperation incentives in MANETs. In ad-
et al. [1981] address applications where a planner or leader dition, Ratliff and Fiez [2020] addresses a principal-agent
aimstooptimizesystemperformancewhileanticipatingand problem with multiple agents of unknown types. The prin-
cipal optimizes an objective function which depends on the
∗Equalcontribution. datafromstrategicdecisionmakers(agents). Thefollowers’
5202
beF
01
]YS.ssee[
1v56070.2052:viXra
decision-makingprocessesarecategorizedintoNashequilib- problem into a single-level optimization, enabling the
rium strategies and myopic update rules. They develop an solutionwithgradientdescentmethods.
algorithm that the principal can employ to learn the agents’
• Weconsiderthattheleaderhasonlypartialandnoisyob-
decision-making processes while simultaneously designing
servationsofthefollowers’trajectories,andwedevelop
incentives to change their response to one that is more de-
efficient methods for computing the gradient terms.
sirable. Adaptiveincentivedesignin[Maetal.,2024]intro-
These methods leverage observable operators [Jaeger,
ducesgradient-ascentalgorithmstocomputetheleader’sop-
2000] within the framework of HMM, enabling more
timalincentivestrategy,despitethelackofknowledgeabout
accurate and computationally efficient gradient compu-
the follower’s reward function. While adaptive incentive
tation.
design focuses on dynamically adapting the leader’s policy
based on the follower’s response, our work examines a dif- • Finally, we demonstrate the accuracy and effectiveness
ferentproblem:incentivedesignthatsupportsinferencetasks of our proposed methods through experimental valida-
underpartialobservability.Specifically,weassumetheleader tion. In this research, specifically, we minimize uncer-
hasapriordistributionoverpotentialfollowertypesandaims taintywhenalltypesoffollowersprovidetheirbestre-
to reduce uncertainty in the posterior distribution of these sponses to given side payments, using gradient descent
typesbasedonlimitedobservations. Toachievethis,wepro- methods.
poseaninformation-theoreticobjectivefortheleader, rather
than defining the leader’s value in terms of cumulative dis- 2 PreliminariesandProblemformulation
countedrewards. Sinceinformation-theoreticmeasurescan-
Notation. ThesetofrealnumbersisdenotedbyR. Random
notbedirectlyexpressedascumulativerewards,wedevelop
variables will be denoted by capital letters and their realiza-
novelsolutionsforthisclassofincentivedesignproblems.
tionsbylowercaseletters(X andx). Asequenceofrandom
Information-theoreticmetricsarealso widelyusedinvar-
variablesandtheirrealizationswithlengthT aredenotedas
ious active inference problems. Egorov et al. [2016] and
X and x . The notation x refers to the i-th compo-
Araya et al. [2010] formulate a partially observable Markov 0:T 0:T i
nent of a vector x ∈ Rn or the i-th element of a sequence
decisionprocess(POMDP)witharewardfunctiondependent
x ,x ,...,whichwillbeclarifiedbythecontext. Givenafi-
ontheagent’sinformationstateorbelief. Forinstance,intar- 0 1
nitesetS,letD(S)bethesetofallprobabilitydistributions
get surveillance, a patrolling team is rewarded for reducing
overS. ThesetST denotesthesetofsequenceswithlength
uncertainty in its belief about an intruder’s position or state.
T composed of elements from S, and S∗ denotes the set of
Similarly, inintentinference, ShenandHow[2019]utilized
allfinitesequencesgeneratedfromS.
the negative entropy of the belief over an opponent’s intent
This active inference problem involves a leader-follower
as a reward, maximizing the total reward to enhance active
framework where a leader can offer side payments to sup-
inference. Also, Shi et al. [2024] use Shannon conditional
plementthefollowers’originalrewards,andthefollowersal-
entropyasaninformationleakagemeasuretosolvetheactive
ways take the best responses. The leader can partially ob-
inferenceprobleminwhichanagentcanactivelyquerysen-
serve the followers’ behavior and aims to minimize uncer-
sorstoinferanunknownvariableinahiddenMarkovmodel.
tainty about their types, balancing against the cost of side
Toourknowledge,information-theoreticobjectiveshavenot
payments.
been considered for the leader-follower games. Existing so-
lutionstoactiveinferenceareformotionplanningofasingle Thefollower’smodel. Asingle-agentMDPisdefinedas:
agentorateamofagents,andcannotbeextendedtosolvethe
incentivedesignproblemsintheleader-followergame. M =(S,A,P,µ,γ,R¯),
Ourproblemformulationandmethodsarealsoapplicable
whereS isasetofstates,Aisasetofactions,P: S×A→
forintentinference,whichisakeyresearchtopicofAIalign- D(S)isaprobabilistictransitionfunctionsuchthatP(s′|s,a)
ment. Inferring humans’ intentions or capabilities can en- is the probability of reaching state s′ given action a being
surethatAIsystemstailorthedecisionsforthespecificuser
takenatstates,µ∈D(S)isaninitialstatedistribution. The
group and avoid unintended or harmful behaviors [Leike et
original reward function for the agent without any side pay-
al.,2018;SoaresandFallenstein,2014]. mentisR¯: S×A→RwhereR¯(s,a)istherewardreceived
Ourcontribution. Ourresearchadvancesactiveinference bythefollowerfortakingactionainstates.
throughthefollowingkeycontributions:
Different types of followers. We consider the interaction
• Weintroduceanovelincentivedesignframeworkforac- betweenaleaderandafinitesetT offollowerswithdifferent
tive inference, employing conditional entropy to quan- types. For each type i ∈ T, follower i’s planning problem
tify the leader’s uncertainty about the follower’s type is modeled by an MDP M = (S,A,P ,µ ,γ ,R¯ ). Dif-
i i i i i
fromtheleader’spartialobservations. ferent types have different initial state distributions, reward
functions,discountingfactors,ortransitiondynamics. With-
• We show the problem of incentive design can be
outlossofgenerality,weassumethefollowershavethesame
formulated as a bi-level optimization problem, using
stateandactionspaces.
an augmented-state hidden Markov model constructed
from the set of policies of different potential follower Incentivesassidepayments. Theleadercanallocateside
types. Wheneachfollowerfollowsanoptimalentropy- paymentstotheenvironment. Asidepaymentallocationisa
regularizedpolicy,itispossibletotransformthebi-level function: x: S ×A → R ,hereafterreferredtoastheside
+
payment. Specifically, x(s,a) is the additional non-negative Definition1. Givenacollectionofpolicies,referredtoasa
reward that the leader offers to follower i when follower i policyprofile,π =[π ] offollowers,andtheleader’sob-
i i∈T
takesactionainstates,foreachi ∈ T. Letxbeavectorof servationfunctionsforfollowers{E ,i ∈ T}. Thefollowing
i
dimensionS ×Arepresentingsidepaymentsforeachstate- HMMcanbeconstructed:
actionpair,withitsdomaindenotedasX.
M(π)=⟨S×T,P,O,E,µ ⟩
0
Duetotheindependentdynamicsandindependentreward
functions with side payments, for each follower i ∈ T, • S ×T is the augmented state space. Each state (s,i)
given a side payment x, follower i’s modified reward func- includesastateinthefollower’sMDPandatypeofthe
tionR (x)isdefinedasfollows: Forall(s,a)∈S×A, follower.
i
• P :S×T →∆(S×T)isdefinedby
R (s,a;x)=R¯ (s,a)+x(s,a). (1) π
i i
P ((s′,j)|(s,i))
π
Foreachi∈T,followeri’splanningproblemwithsidepay-
mentxisthusmodeledasthefollowingMDP: = (cid:26)(cid:80) a∈A P i (s′|s,a)π i (s,a) ifi=j
0 otherwise.
M (x)=(S,A,P ,µ ,γ ,R (x)).
i i i i i
Inotherwords,atthestate(s,i),followeriwilltakean
Given the MDP M (x) and a Markov policy π : S → action by following his policy π (s), and the type does
i i
D(A), let V (µ ,R (x),π) be the value function evaluated notchange.
i i i
forpolicyπ.
• µ ∈ ∆(S ×T)istheinitialstatedistribution. Forall
0
Leader’s partial observation Given a follower’s MDP (s,i)∈S×T,
M i (x) and a set O of observations, the leader’s observation µ 0 (s,i)=µ i (s)P(T=i).
function is defined by E : S → D(O). That is, the leader
i
whereT isarandomvariablerepresentingtheestimated
only partially observes the state of the follower. Note that
typeofthefollower. P(T)isthepriordistributionover
itis easytoextend thisobservationfunction toallowpartial
possibletypes.
observations of both states and actions, by augmenting the
follower’sstateswithactionstaken. • Oisafinitesetofobservations.
Aninformalproblemstatementisgivenbelow. • E :S×T →D(O)istheobservationfunction,defined
Problem1. Considerthattheleader’sobjectiveistoestimate byE(o|(s,i)) = E i (o|s)istheprobabilityofobserving
thetypeofafollowerbasedonafiniteobservationsequence. owhenagentiatthestates.
Theleadercanincentivizedivergingbehaviorsfromdifferent LetO denotearandomvariablerepresentingtheobserva-
t
followers by providing side payments, which come at a cost tionattimet, andleto beaspecificrealizationofthisran-
t
to the leader. How should the leader design a side payment domvariable. WedenotetheposteriorestimateofthetypeT
strategy to balance the estimation-utility trade-off? That is, givenanobservationo asP (T|O =o ).
0:T π 0:T 0:T
how can the leader maximize the estimation accuracy while Next, we define the planning objective—Shannon condi-
minimizingthetotalcostofsidepayments? tionalentropy. Information-theoreticmetricsarewidelyused
asplanningobjectives,withentropymeasuresbeingparticu-
Remark 1. This problem formulation is motivated by AI-
larlycommonforquantifyinginformationleakage.Theseen-
alignmentproblems,wheretheAIagentistotailorsomede-
tropymeasureshavebeenextensivelystudiedinchannelde-
cisionrecommendationstotheuserbasedontheinferredin-
signproblems,whichcanbeinterpretedasone-stepdecision-
tention or capability of the user. For example, ride-sharing
makingproblems[KhouzaniandMalacaria,2017].
platformscontinuouslymonitordriverbehaviorstoestimate
driverpreferencesandcapabilities.Bydesigningstrategicin- Definition2. LetY := O . Theconditional Shannonen-
0:T
centivessuchassurgepricingandcompletionbonuses,these tropyoftheagent’stypegiventheobservationsisdefinedby,
platformscanimprovetheaccuracyandefficiencyofthein- (cid:88)
H(T |Y,M(π))= P (y)H(T|Y =y,M(π))
ference, and thus provide more personalized services, such π
asrouterecommendationsandrider-drivermatching. Other y∈Y
(2)
(cid:88)(cid:88)
applications are related to AI-based security and intrusion =− P (i,y)logP (i|y),
π π
detection. By strategically allocating sensors and decoys,
i∈T y∈Y
a security system can manipulate users’ perceptual reward
wherey isasampleobservationsequence, andY isasetof
functionstoelicitdivergentbehaviorsbetweenattackersand
allfiniteobservationsequencesoflengthT.
normalusers.
Given different policy profiles π and π′, the entropies
H(T |O ,π)andH(T |O ,π′)canbedifferent.
3 Theincentivedesignproblemformulation 0:T 0:T
Fortheleader’sgoalofinferringthefollower’stype,apol-
Wefirststudyhowtomeasuretheleader’suncertaintyabout icyprofilemakestheleader’staskeasierifithasalowercon-
thecurrentfollower’stypeafterafinitesequenceofobserva- ditionalentropy. Basedonthisrelation,weformulatethefol-
tions,whenthefollowers’policiesareknownandtheleader lowing incentive design problem where the objective of the
observes a follower’s behavior through imperfect observa- leaderistoallocatethesidepaymentsxtoimprovetheaccu-
tions. racyandefficiencyofinferringthetypeofthefollower.
Problem 2 (Incentive design for intention inference). As- 3.2 Incentivedesignwithgradientdescent
suming the followers always provide the best responses, the
Forconvenience,definethesoftmaxpolicyπ as
leader’s incentive design for intention inference problem is θ
thefollowingbi-leveloptimizationproblem: exp(θ /τ)
mi x n ∈ im X ize H(T|O 0:T ,M(π⋆(x)))+h(x) π θ (s,a)= (cid:80) a′∈A ex s p , ( a θ s,a′ /τ) , θ ∈R|S×A|. (7)
subjectto π i ⋆(x)∈argmaxV i (µ i ,R i (x),π),∀i∈T. Notation. Given this parametrization, we will replace the
π∈Π notation of policy π (or policy profile π) with policy pa-
(3) rameter θ (or policy parameter profile θ ≜ [θ ] ). Thus,
i i∈T
t
w
h
h
e
e
b
re
es
π
t
⋆
r
(
e
x
sp
)
o
=
nse
[
s
π i ⋆
o
(
f
x
fo
)]
l
i
l
∈
o
T
we
i
r
s
s,
a
a
p
n
o
d
li
h
cy
:
p
X
ro
→
file
R
cons
i
i
s
st
a
ing
sid
o
e
f M(π θ ) ≜ M(θ) and P πθ ≜ P θ . Similar to the nota-
+ tion of θ, We denote Q⋆(R(x)) ≜ [Q⋆(R (x))] , and
paymentcostfunction,whichisdifferentiable. i i∈T
R(x)≜[R (x)] .
i i∈T
3.1 Reductiontosingle-leveloptimization Becausetheentropy-regularizedoptimalpolicyπ⋆(x)isan
i
We show how to leverage entropy-regularized MDP implicitfunctionofx,weconsiderusingthegradient-descent
[Nachum et al., 2017] to reduce the bi-level optimization methodtofindastationarypointfortheobjectivefunctionin
problemequation(3)toasingle-leveloptimizationproblem. equation(6).
Thefollowingreviewstheentropy-regularizedMDPsolution Let’sdefineJ 1 (θ) = H(T|Y;M(θ))(recallY = O 0:T ),
for a single agent, which is the same for all followers. We notethatθshouldbeQ⋆(R(x))intheequation(6). Let
omittheindexofafollowerforclarity.
J(x):=J (Q⋆(R(x)))+h(x).
1
Entropy-regularized optimal value/policy. The optimal
value function V⋆ of the entropy-regularized MDP with re- Followingthechainrule,thederivativeofJ withrespecttox
specttotherewardfunctionRsatisfiesthefollowingentropy- isgivenby
regularizedBellmanequation[Nachumetal.,2017]:
DJ(x)=DJ (Q⋆(R(x)))·DQ⋆(R(x))·DR(x)+Dh(x).
(cid:88) 1
V⋆(s,R)=τlog exp{(R(s,a) (8)
a∈A Because h is given, Dh(x) can be computed analytically.
+γE V⋆(s′,R))/τ}, ∀s∈S. (4) Similarly, DR(x) can be computed analytically given the
s′∼P(·|s,a)
functionR (x),foreachi∈T.
Notethat, asτ approaches0, equation(4)recoversthestan- i
dard optimal Bellman equation. Let Q⋆(R): S ×A → R In the following, we show how to compute
DJ (Q⋆(R(x)))andDQ⋆(R(x))respectively.
be the optimal state-action value function (also called Q- 1
function)oftheentropy-regularizedMDPunderrewardR: ComputingDJ (Q⋆(R(x))). Letθ = Q⋆(R(x)). Com-
1
Q⋆(s,a,R)=R(s,a)+E V⋆(s′,R). puting DJ (θ) involves computing the gradient of the con-
s′∼P(·|s,a) 2 1
ditional entropy H(T|Y;M(θ)) w.r.t. the parameter θ of
Forafixedtemperatureparameterτ,theoptimalpolicyofthe
policyprofile,i.e. DJ (θ)=∇ H(T|Y;θ)⊺.
entropy-regularizedMDPisuniquelydefinedby 1 θ
Byusingatrickthat∇ P (y)=P (y)∇ logP (y)and
exp(Q⋆(s,a,R)/τ) θ θ θ θ θ
π⋆(s,a)= . (5) thepropertyofconditionalprobability,wehave
(cid:80) exp(Q⋆(s,a′)/τ)
a′∈A
Then, the optimal policy of the entropy-regularized MDP ∇ θ H(T|Y;θ)
canbewrittensuccinctlyasπ ,whereQ⋆(R)isviewed (cid:88) (cid:88)(cid:104)
Q⋆(R) =− ∇ P (i,y)log P (i|y)
asavectorinR|S×A|. θ θ 2 θ
In the following, we use V⋆(R ), Q⋆(R ), π⋆, π⋆ (resp.
y∈OT i∈T
i i i (cid:105)
V⋆(R (x)), Q⋆(R (x)), π⋆(x), π⋆(x))foroptimalentropy- +P (i,y)∇ log P (i|y)
i i i θ θ 2 θ
regularizedvaluefunction,Q-valuefunction,policy,andpol-
(cid:88) (cid:88)(cid:104)
icyprofilewithrespecttofolloweri’srewardR (resp. mod- =− P (y)∇ P (i|y)log P (i|y)+
i θ θ θ 2 θ
ifiedrewardR (x)withsidepaymentx).
i y∈OT i∈T
Duetotherelationbetweentheoptimalpolicyandtheop-
∇ P (i|y)(cid:105)
timal state-action value function of the entropy-regularized P (i|y)∇ P (y)log P (i|y)+P (y) θ θ .
θ θ θ 2 θ θ ln2
MDP given by equation (5), the lower-level problem in the
(9)
bileveloptimizationprobleminequation(3)hasauniqueso-
GivenapriordistributionP(T),wehave
lution π . We can reduce the problem 2 to the fol-
Q⋆(Ri(x))
lowingsingle-leveloptimizationproblem. (cid:88)
P (y)= P(T =i)P (y|T =i),
θ θ
minimizeH(T|O ,M(π⋆(x))+h(x). (6) i∈T
0:T
x∈X (cid:80)
whereπ⋆(x)=[π Q⋆(Ri(x)) ] i∈T isthepolicyprofileandeach w ab h i e li r t e y P of θ ( g y e | n T er = ati i n ) g = the o s b ∈ s S e P rv θ at ( i y o | n s, s i e ) q µ u 0 e ( n s c , e i) y is g th iv e e p n ro th b e -
π istheentropy-regularizedoptimalpolicy:
Q⋆(Ri(x)) follower i following a policy parameterized by θ —the i-th
i
π Q⋆(Ri(x)) =argm
π
a
i
xV(µ i ,R i (x),π i ),∀i∈T. componentofpolicyprofileparametersθ.
Wecancalculatethegradient∇ P (T =i|y)as ∇ P (y|T = i)andP (y|T = i)byreplacingthesingle-
θ θ θi θ θ
agentHMMwiththefolloweri’sHMMM (θ )andtheob-
i i
∇ P (T =i|y)
θ θ servationwithE ,andthencompute∇ P (y)andP (y).
i θi θi θi
P (y|T =i) Itisobservedthat∇ P (y|T = i) = 0becausetheob-
=P(T =i)∇ θ θj θ
θ P (y) servation process of follower i is not influenced by the fol-
θ
lower j’s policy, when i ̸= j. With these computation, we
(cid:104)∇ P (y|T =i) P (y|T =i) (cid:105)
=P(T =i) θ P θ θ (y) − θ P2 θ (y) ∇ θ P θ (y) , obt I a t i i n s ∇ no θ t P ed θ ( t T hat = , t i h |y ou ). gh O is a finite set of observations,
(10) it is combinatorial and may be too large to enumerate. To
(cid:80)
where∇ θ P θ (y)= i∈T P(T =i)∇ θ P θ (y|T =i). mitigatethisissue,wecanemploysampleapproximationsto
To complete the gradient computation, we only need to estimate∇ H(T|Y;θ): GivenK sequencesofobservations
θ
determine ∇ θ P θ (y|T = i) for each type of follower. We {y 1 ,...,y K },wecanapproximateH(T|Y;θ)by
can utilize the observable operators [Jaeger, 2000] to com-
putethisgradientforthepartiallyobservablesysteminduced 1 (cid:88) K (cid:88)
byfolloweri’spolicy. H(T|Y;θ)≈− K P θ (i|y k )logP θ (i|y k ), (13)
k=1i∈T
Observable operators for single-agent HMM. Consider
a single-agentMDP M = ⟨S,A,P,µ 0 ,R⟩, aparameterized andapproximate∇ θ H(T|Y;θ)by
Markovpolicyπ inducesahiddenMarkovmodel
θ ∇ H(T|Y;θ)
θ
M(θ)=⟨S,A,P θ ,µ 0 ,O,E⟩ 1 (cid:88) K (cid:88) (cid:2)
≈− logP (i|y )∇ P (i|y )
where S = {1,...,N}, O = {1,...,M}, P θ (s′|s) = K θ k θ θ k
(cid:80) P(s′|s,a)π (a|s) and E : S → D(O) is an obser- k=1i∈{0,1}
a∈A θ
vationfunctionfortheleader. +P (i|y )logP (i|y )∇ logP (y )+ ∇ θ P θ (i|y k )(cid:3) .
Inthissingle-agentHMM,lettherandomvariableofstate, θ k θ k θ θ k log2
observation,andcontrolaction,attimepointtbedenotedas (14)
X t ,O t ,A t ,respectively. LetTθ ∈RN×N bethetransposed ComputingDQ⋆(R(x)). ThederivativeDQ⋆(R(x))isa
statetransitionmatrixinthesingle-agentHMMM with
θ block diagonal matrix. Each block along the main diagonal
Tθ =P (X =i|X =j). correspondstoDQ⋆(R i (x))foreachfolloweri∈T.
i,j θ t+1 t Thefollowingproposition(proveninMaetal.[2024])al-
Let O ∈ RM×N be the observation probability matrix lowsustocomputeDQ⋆(R i (x))giveneachfollower’sMDP
withO o,j =E(o|j)foreacho∈Oandj ∈S. M i withrewardR i (x).
Definition3. Giventhesingle-agentHMMM , foranyob- Proposition 2. Consider an infinite-horizon MDP M =
servation o, the observable operator Aθ is a m θ atrix of size (S,A,P,s 0 ,γ,R)withdiscounting.LetQ⋆(R): S×A→R
o be the optimal state-action value function of the entropy-
N ×N withitsij-thentrydefinedas
regularized MDP under the reward function R. For any
Aθ[i,j]=Tθ O , (s,a),(s˜,a˜)∈S×A,itholdsthat
o i,j o,j
whichistheprobabilityoftransitioningfromstatejtostatei
∂Q⋆
s,a =1 (s,a)+γE (cid:88) π (s′,a′)
∂Q⋆
s′,a′ ,
andatthestatej,anobservationoisemitted.Inmatrixform, ∂R (s˜,a˜) s′∼P(·|s,a) Q⋆(R) ∂R
s˜,a˜ s˜,a˜
a′∈A
Aθ =Tθdiag(O ,...,O ). (15)
o o,1 o,N
where
(cid:26)
Proposition1. Giventhesingle-agentHMMM ,theproba- 1 if(s,a)=(s˜,a˜),
θ 1 (s,a)= (16)
bilityofobservingyis (s˜,a˜) 0 otherwise.
P (y)=1⊤Aθ ...Aθ µ . (11) Foranygiven(s′,a′)∈S×A,equation(15)isintheform
θ n ot o0 0
ofthestandardBellmanequationforstate-actionvaluefunc-
where 1 is a vector of size N. An the derivative of P (y)
N θ tion. Thisimpliesthatthepartialderivativecanbecomputed
withrespecttoθis using any method for solving the Bellman equation [Ma et
al.,2024].
t Lastly, usingthegradientcomputationmethodsdescribed
(cid:88)
∇ P (y)= 1⊤Aθ ...∇ Aθ ...Aθ µ . (12) above,wecancomputethetotalgradientDJ(x)andemploy
θ θ N ot θ oi o0 0
agradient-descentalgorithmtofindastationarypointofthe
i=0
objectivefunction.
Proof. Theproofisbasedonthepropertyofobservableop-
Remark 2. Though the method places no restriction on the
erators [Jaeger, 2000] and the derivative of a parameterized
side payment x—the leader can modify the follower’s re-
tensorproduct.
ward at any state-action pair. In practice, the side pay-
Based on the above analysis for a general single-agent ment decision variables x may be constrained to be within
HMM M(θ) and observation function E, we can obtain a set X of feasible allocation. In addition, let λ be the
Figure1:Firerescuetaskingridworldenvironment. Figure2:Behaviorcomparisontaskingridworldenvironment.
number of decision variables in X, the time complexity of If the robot is outside the sensor’s range, the observer only
calculatingDQ⋆(R(x))·DR(x)inthesingle-agentcaseis receivesanullobservation.
O(λ|S|2|A|2+|S|3|A|2). Thus,theleadercanselectasub- Inthisenvironment,theleadercanonlyallocatesidepay-
setofstate-actionpairsforsidepaymentallocationtoreduce ments in the cell (5,0). When setting the side payment, we
thenumberλandsubsequentlythecomputations. can assign a positive value to only one action at the target
state, i.e., x((5,0),a) > 0 for a single action a ∈ A, while
4 Experiments settingx((5,0),a)=0forallotheractions. Sincethetargets
are sink states, the agent will always choose the action with
Example1(Firerescuetask). Wedemonstratetheeffective-
thepositiverewardwhencomputingtheoptimalpolicy. This
ness of the proposed policy-based algorithm in a fire rescue
approach will reduce the effective dimension of side pay-
task (Figure 1) 1. The scenario involves two types of rescue
ment. The cost function is defined as h(x) = β∥x∥ where
robots (followers) that start from the same initial position, 1
β = 0.05, represents the cost of side payments in this spe-
(0,0), and navigate toward specific goal locations (the tar-
cificcell. Inoptimization,wesetthetimehorizontoT =12
getstoberescued)markedbyflags. Therobotscanmovein
anduseK =2000sampledtrajectoriesforapproximationof
fourcompassdirections: north,south,east,andwest. How-
gradients.
ever, their movement is stochastic. When a robot moves in
Figure 3a illustrates that as the algorithm converges. The
a chosen direction, there is a probability α that it will also
objectivefunctionJ(x)convergesto0.299. Theconditional
shift in one of the two adjacent directions. For example, if
entropyH(T|Y,M(π⋆(x ))approaches0.153. Andtheside
a robot moves east, there is a probability α that it will also t
payment reaches0.291. Thetrend ofobjective functionand
movenorthandanotherprobabilityαthatitwillmovesouth.
entropyaresimilarbecausewesettheweightβ =0.05.Then
Ifarobotencountersanobstacleorboundary, itremainsin
thecostfunctionh(x)hasasmallerimpactontheobjective
place.
function. Since the conditional entropy is close to 0—the
Type 1 robots are equipped with superior hardware com-
minimal value of entropy—the observations provide signif-
pared to Type 2 robots, resulting in lower movement uncer-
icantinformationabouttherobot’stype.
tainty (α = 0.05 for Type 1 vs. α = 0.1 for Type 2). Ad-
The conditional entropy is inversely proportional to the
ditionally, Type 1 robots have better fireproof armor. Thus,
side payment, a behavior driven by the environmental con-
if a Type 1 robot enters a fire cell, only a small negative re-
figuration. Whentheinitialsidepaymentis1, bothtypesof
ward of −0.1 is incurred. In contrast, Type 2 robots, which
robotsmovetowardtheflagatcell(5,5)duetothepresence
aremorevulnerabletofire,sufferamuchlargernegativere-
offiresaroundcell(5,0). Hence, theyareindistinguishable
ward of −20. Both robot types receive a positive reward of
and the entropy is high as 0.793. However, as the side pay-
0.1uponreachinggoal(flag)states,whicharedesignatedas
mentincreases,theType1robot,whichhasalowfirepenalty,
sinkstates. Sincethegoalstatesaresinkstates, robotscon-
chooses to move to (5,0), while the Type 2 robot, facing a
tinuetoaccumulatediscountedrewardsafterreachingthem.
much higher fire penalty, still moves to (5,5). This diver-
Thediscountfactorissettoγ =0.9foralliwhencomputing
i genceinmovementmakesiteasiertodistinguishbetweenthe
theoptimalpolicyforeachrobottype.
two robot types, thereby reducing entropy. Clearly, increas-
Forperception,theenvironmentisequippedwithfoursen-
ingthesidepaymenttobehigherthan0.291willimprovethe
sors {1,2,3,4}, each with a distinct range. If a robot is
inference accuracy, but at too much cost for the leader. Our
within sensor i’s range, the observer receives observation i,
solution achieves a balance between the inference accuracy
fori ∈ {1,2,3,4}witha90%probabilityandanullobser-
andsidepaymentcost.
vation (”n”) with a 10% probability due to false negatives.
Example2(Behaviorcomparisontask). Weusetheexample
1Thecodeisavailableonhttps://drive.google.com/drive/folders/ showninFigure2tofurtherillustratetheapplicationofour
1a33YT3gDoJxXNxU36dHfi3ae40E BIc ?usp=sharing algorithminactiveinference. Thisenvironmentissimilarto
(a)Theresultofthefirerescue. (b)Theresultofthebehaviorcomparison. (c)Inferenceduringtheoptimization.
Figure3:Theresultsofexperiments.
Example1,butwithadditionalobstacles.
nifi T c h a e n r t e ly a d re iff a er ls e o nt tw be o ha ty v p io e r s s. of T r h o e bo d ts yn ( a fo m ll i o c w n e o r i s s ) e w is ith se s t ig to - Pˆ θ (T =i|Y)≈ M 1 (cid:88) M P θ (T =i|Y =y k ). (17)
α = 0.05 for Type 1 and α = 0.25 for Type 2, meaning k=1
theType2robotmovesmorerandomlythantheType1robot. The estimator Pˆ (T = i|Y) represents the probability that
Both robots receive a positive reward of 0.1 upon reaching θ
therobotisoftypei. Iftheposterioriscloseto1,theleader
theirtarget(flags).Additionally,theyincuracontinuousneg-
can confidently infer that the true type is i. In this exper-
ative reward of −0.01 in other cells, encouraging them to
iment, we fix the true robot type to be Type 2. When the
movetowardtheirtargetsmorequickly. Thetargetsaresink
side payment is 1, the posteriors Pˆ (T = 1|Y) = 0.447
statesaswell,whichmeansthattherobotscancontinuously θ
collectrewardshere. and Pˆ θ (T = 2|Y) = 0.553 are close to 0.5, making it dif-
ficult to determine which type is the true type. As the side
paymentincreasesandconvergestotheoptimalvalueof0.5,
Inthisenvironment,theleadercanonlyallocatesidepay-
the posterior Pˆ (T = 1|Y) decreases to about 0.15, while
mentinthecell(5,5). Allothersettingsremainthesameas θ
inExample1. Pˆ θ (T = 2|Y) increases to about 0.85. At this point, the
leadercanconfidentlyinferthatthetruetypeisType2.
Figure 3b shows that the conditional entropy eventually These experimental results from the stochastic grid world
converges to 0.390, while the initial conditional entropy is examplesvalidatetheaccuracyandeffectivenessofourpro-
0.945. The side payment converges to 5. As the side pay- posedmethods. Thesetwocasestudiesdemonstratedtheuse
ment increases, the robots’ behavior changes from indistin- of incentive design to distinguish users with different capa-
guishable to distinguishable given partial observations. It is bilities/dynamicsand/ordifferentrewardfunctions.
observed that under an initial side payment x((5,5),a) = 1
for a certain action a ∈ A, the cell (5,5) is not attractive to 5 Conclusion
either robot, as both can obtain sufficient rewards from cell
(5,0). However,asthesidepaymentincreases,thecell(5,5) Inthispaper,weintroducedaclassofactiveinferenceprob-
becomes more appealing. In this scenario, the Type 1 robot lemsthroughincentivedesignanddevelopedabi-levelopti-
can ensure reaching (5,5) with a high probability, whereas mization method to solve a locally optimal solution for the
theType2robotcannotbecauseitismorelikelytoruninto leader’sincentivepolicy. Ourapproachusesconditionalen-
obstacleswithitshigherstochasticity. Asaresult,increasing tropytomeasureuncertaintyaboutfollowers’typesfromthe
the reward at (5,5) makes it easier for the leader to distin- leader’spartialobservationsandconsideranincentivedesign
guishbetweenthetwotypesofrobots. with an objective to balance information gain and incentive
cost.
Thefollowinggraph(Figure3c)showstheresultsofinfer- Future work could explore several directions: 1) Integrat-
enceduringtheoptimizationprocess. Wedefinetheestima- ing active inference with adaptive incentive design for per-
tor of posterior distribution Pˆ θ (T = i|Y) ≜ E Y [P θ (T = sonalized systems; 2) Extending the proposed method from
(cid:80)
i|Y)] = y P θ (y)P(T = i|Y = y) given different side model-basedtomodel-freesetting,wheretheleaderonlyhas
payments x, which are sampled during the optimization. access to collected data from different followers in the past
Given y ,k = 1,...,M of sampled observation sequences, interactions.Thisextensionwillbecrucialformanypractical
k
theestimatorcanbeapproximatedby applications.
References Haibo Mei, Stefan Poslad, and Shuang Du. A game-theory
basedincentiveframeworkforanintelligenttrafficsystem
Thamer Alquthami, Ahmad H Milyani, Muhammad Awais,
as part of a smart city initiative. Sensors, 17(12):2874,
andMuhammadBRasheed. Anincentivebaseddynamic
2017.
pricing in smart grid: a customer’s perspective. Sustain-
ability,13(11):6066,2021. RogerBMyerson. Optimalauctiondesign. Mathematicsof
operationsresearch,6(1):58–73,1981.
Mauricio Araya, Olivier Buffet, Vincent Thomas, and
Franc¸cois Charpillet. A POMDP extension with belief- Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale
dependentrewards. InJ.Lafferty, C.Williams, J.Shawe- Schuurmans. Bridging the gap between value and policy
Taylor, R. Zemel, and A. Culotta, editors, Advances in based reinforcement learning. Advances in neural infor-
Neural Information Processing Systems, volume 23. Cur- mationprocessingsystems,30,2017.
ranAssociates,Inc.,2010.
LillianJRatliffandTannerFiez. Adaptiveincentivedesign.
Patrick Bolton and Mathias Dewatripont. Contract Theory. IEEE Transactions on Automatic Control, 66(8):3871–
MITPressBooks,1,2005. Publisher: TheMITPress. 3878,2020.
StevenBraithwait,DanielGHansen,andLaurenceDKirsch. Macheng Shen and Jonathan P. How. Active perception in
Incentives and rate designs for efficiency and demand re- adversarial scenarios using maximum entropy deep rein-
sponse. Lawrence Berkeley National Laboratory. LBNL- forcement learning. In 2019 International Conference on
60132,2006. RoboticsandAutomation(ICRA),page3384–3390.IEEE
Press,2019.
DavidEasleyandArpitaGhosh. Behavioralmechanismde-
sign: Optimal crowdsourcing contracts and prospect the- ChongyangShi,ShuoHan,MichaelDorothy,andJieFu.Ac-
ory. In Proceedings of the Sixteenth ACM Conference on tiveperceptionwithinitial-stateuncertainty: Apolicygra-
EconomicsandComputation,pages679–696,2015. dientmethod.IEEEControlSystemsLetters,8:3147–3152,
2024.
Maxim Egorov, Mykel J Kochenderfer, and Jaak J Uud-
mae. Target surveillance in adversarial environments us- Nate Soares and Benja Fallenstein. Aligning superintelli-
ingPOMDPs. InProceedingsoftheThirtiethAAAICon- gencewithhumaninterests: Atechnicalresearchagenda.
ferenceonArtificialIntelligence,pages2473–2479.AAAI Machine Intelligence Research Institute (MIRI) technical
Press,2016. report,8,2014.
Yu-Chi Ho, P Luh, and Ramal Muralidharan. Information Noah Williams. Persistent private information. Economet-
structure,stackelberggames,andincentivecontrollability.
rica,79(4):1233–1275,2011.
IEEETransactionsonAutomaticControl, 26(2):454–460, Yufeng Zhan, Peng Li, Zhihao Qu, Deze Zeng, and Song
1981. Guo. A learning-based incentive mechanism for feder-
atedlearning.IEEEInternetofThingsJournal,7(7):6360–
Herbert Jaeger. Observable Operator Models for Discrete
6368,2020.
StochasticTimeSeries. NeuralComputation,12(6):1371–
1398,062000.
Raman Kazhamiakin, Annapaola Marconi, Mirko Perillo,
Marco Pistore, Giuseppe Valetto, Luca Piras, Francesco
Avesani, and Nicola Perri. Using gamification to incen-
tivizesustainableurbanmobility. In2015IEEEfirstinter-
nationalsmartcitiesconference(ISC2),pages1–6.IEEE,
2015.
MHR.KhouzaniandPasqualeMalacaria. Leakage-minimal
design: Universality, limitations, and applications. In
2017 IEEE 30th Computer Security Foundations Sympo-
sium(CSF),pages305–317,2017.
JanLeike,DavidKrueger,TomEveritt,MiljanMartic,Vishal
Maini, and Shane Legg. Scalable agent alignment via
reward modeling: a research direction. arXiv preprint
arXiv:1811.07871,2018.
Ze Li and Haiying Shen. Game-theoretic analysis of co-
operation incentive strategies in mobile ad hoc networks.
IEEE Transactions on mobile computing, 11(8):1287–
1303,2011.
HaoxiangMa,ShuoHan,AhmedHemida,CharlesKamhoua,
andJieFu. AdaptiveincentivedesignforMarkovdecision
processeswithunknownrewards. 2024. OpenReview.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Active Inference through Incentive Design in Markov Decision Processes"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
