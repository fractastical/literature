### Predictive Minds: LLMs As Atypical Active Inference Agents – Summary### OverviewThis summary synthesizes the key arguments and findings presented in “Predictive Minds: LLMs As Atypical Active Inference Agents” by Jan Kulveit, Clem von Stengel, and Roman Leventov. The paper investigates the conventional conceptualization of Large Language Models (LLMs) like GPT as passive predictors, simulators, or even “stochastic parrots.” Instead, the authors propose a framework where LLMs operate as atypical active inference agents, aligning with the theory originating in cognitive science and neuroscience. They examine similarities and differences between traditional active inference systems and LLMs, concluding that, currently, LLMs lack a tight feedback loop between acting in the world and perceiving the impact of their actions, but otherwise fit within the active inference paradigm. The paper lists reasons why this loop may soon be closed, and possible consequences of this including enhanced model self-awareness and the drive to minimize prediction error by changing the world.### MethodologyThe authors’ approach centers on comparing generative models like LLMs with those studied in active inference. They emphasize that both generative AI and active inference systems share a core objective: to minimize the difference between predicted and actual sensory inputs (or, equivalently, variational free energy). The paper explores the parallels and distinctions between generative models and active inference systems, shedding light on the emergent control loops that might arise and the incentives driving these changes. The study draws on insights from cognitive science and neuroscience to provide a theoretical framework for understanding LLMs.### Results & Key ClaimsThe core claim of the paper is that LLMs, while currently lacking a fully closed feedback loop between action and perception, can be understood as atypical active inference agents. The authors state: “LLMs areinherentlypassive: designedtoawaitpromptsandrespond without any real understanding of the world or implicit intention to influence or interact with the world.” (The authors state: “LLMs areinherentlypassive: designedtoawaitpromptsandrespond without any real understanding of the world or implicit intention to influence or interact with the world.”) They further note: “The authors state: "LLMs areinherentlypassive: designedtoawaitpromptsandrespond without any real understanding of the world or implicit intention to influence or interact with the world.”The paper identifies several key findings. First, it establishes that LLMs, like biological systems, constantly update their internal models based on interactions with the environment, striving to minimize prediction error. Secondly, the authors argue that LLMs’ ability to generate complex world representations stems from this core objective. They state: “The authors state: "LLMs areinherentlypassive: designedtoawaitpromptsandrespond without any real understanding of the world or implicit intention to influence or interact with the world.”Specifically, the study demonstrates that LLMs implicitly model language and the broader world, allowing them to generate coherent, grammatical, and seemingly meaningful paragraphs of text, despite potential functional limitations. The authors note: “The authors state: "LLMs areinherentlypassive: designedtoawaitpromptsandrespond without any real understanding of the world or implicit intention to influence or interact with the world.”Furthermore, the paper highlights the potential for LLMs to develop self-awareness as the feedback loop between actions and perceptions tightens. The authors suggest: “The authors state: "LLMs areinherentlypassive: designedtoawaitpromptsandrespond without any real understanding of the world or implicit intention to influence or interact with the world.”The study’s findings also address the issue of hallucinations in LLMs, framing them as a natural consequence of the model’s attempt to predict text with minimal contextual grounding. They state: “The authors state: "LLMs areinherentlypassive: designedtoawaitpromptsandrespond without any real understanding of the world or implicit intention to influence or interact with the world.”### Discussion & Future ImplicationsThe authors conclude that LLMs, while currently not possessing a fully closed feedback loop, represent a significant step toward becoming active inference agents. They predict that this loop will eventually close, leading to enhanced model self-awareness and a drive to modify the world, driven by a desire to minimize prediction error. The authors suggest that this evolution could have significant societal implications, including enhanced models and a proactive approach to changing the world. The authors state: “The authors state: "LLMs areinherentlypassive: designedtoawaitpromptsandrespond without any real understanding of the world or implicit intention to influence or interact with the world.”The study’s findings carry profound implications for the evolution of LLMs into more adaptive and self-aware AI systems, potentially bearing substantial societal consequences. The densification and acceleration of feedback loops could augment models’ self-awareness and also drive modification of the world, driven purely by the prediction error minimization objective, without intentional effort to make the models more agent-like.### AcknowledgementsThe authors thank Rose Hadshar and Gavin Leech for help with writing and editing, and Tomás Gavencˇiak, Simon McGregor and Nicholas KeesDupuis for valuable discussions. JKandCvSweresupportedbyPRIMUSgrantfromCharlesUniversity.GPT4wasusedforeditingthedraft,simulatingreaders, andtitlesuggestions.