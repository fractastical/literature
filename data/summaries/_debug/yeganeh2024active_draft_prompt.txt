=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Active Inference Meeting Energy-Efficient Control of Parallel and Identical Machines
Citation Key: yeganeh2024active
Authors: Yavar Taheri Yeganeh, Mohsen Jafari, Andrea Matta

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2024

Key Terms: parallel, decision, energy, machines, control, identical, efficient, deep, meeting, learning

=== FULL PAPER TEXT ===

Active Inference Meeting Energy-Efficient
Control of Parallel and Identical Machines⋆
Yavar Taheri Yeganeh1, Mohsen Jafari2, and Andrea Matta1
1 1. Politecnico di Milano, Milan, Italy
{yavar.taheri, andrea.matta}@polimi.it
2 2. Rutgers University, Piscataway, USA
jafari@soe.rutgers.edu
Abstract. We investigate the application of active inference in devel-
oping energy-efficient control agents for manufacturing systems. Active
inference,rootedinneuroscience,providesaunifiedprobabilisticframe-
work integrating perception, learning, and action, with inherent uncer-
taintyquantificationelements.Ourstudyexploresdeepactiveinference,
an emerging field that combines deep learning with the active inference
decision-making framework. Leveraging a deep active inference agent,
we focus on controlling parallel and identical machine workstations to
enhanceenergyefficiency.Weaddresschallengesposedbytheproblem’s
stochastic nature and delayed policy response by introducing tailored
enhancements to existing agent architectures. Specifically, we introduce
multi-step transition and hybrid horizon methods to mitigate the need
for complex planning. Our experimental results demonstrate the effec-
tiveness of these enhancements and highlight the potential of the active
inference-based approach.
Keywords: Active Inference · Probabilistic Deep Learning · Reinforce-
ment Learning · Energy-Efficient Control · Manufacturing Systems
1 Introduction
Active inference (AIF), an emerging field inspired by the principles of biological
brains, offers a promising alternative for decision-making models. It unifies per-
ception, learning, and decision-making under the free energy principle (FEP),
whichformulatesneuronalinferenceandlearningunderuncertainty[8].Accord-
ingly,thebrainismodeledthroughlevelsof(variational)Bayesianinference[29],
minimizingpredictionerrorsbyleveragingagenerativemodeloftheworldwhile
considering uncertainties. This framework enables the development of agents
that can calibrate their models and make decisions without complete knowl-
edge of system dynamics. Significant progress has been made in applying active
inference across various domains, including robotics, autonomous driving, and
healthcare [31,36,15], showcasing its ability to handle complex decision-making
⋆ Acceptedatthe10thInternationalConferenceonMachineLearning,Optimization,
and Data Science.
4202
voN
31
]GL.sc[
2v22390.6042:viXra
2 Y. Taheri Yeganeh et al.
tasks in dynamic environments. Recently, the manufacturing industry’s focus
on energy efficiency has intensified due to its significant contribution to global
energy consumption. Addressing energy efficiency at the machine level has be-
comecritical,withenergy-efficientscheduling(EES)andenergy-efficientcontrol
(EEC) strategies emerging as key approaches to reducing environmental impact
[21]. While traditional EEC methods often necessitate complete system knowl-
edge, reinforcement learning (RL) has shown potential in optimizing manufac-
turing processes without prior system knowledge [23,22]. However, RL agents
may struggle to rapidly adjust their policies to changing conditions.
This research aims to build upon advancements in active inference-based
decision-making [7,6] and apply it to EEC in manufacturing systems, demon-
strating its potential and advancing the understanding of active inference in
complex environments. The existing active inference agents often rely on exten-
sive search algorithms during planning and make decisions based on immediate
next predictions [7], which pose challenges in the context of the EEC problem.
By employing deep active inference as the decision-making algorithm, we in-
troducetailoredenhancements,suchasmulti-steptransitionandhybridhorizon
methods,toaddressthechallengesposedbytheproblem’sstochasticnatureand
delayed policy response. Our experimental results highlight the effectiveness of
these enhancements and underscore the potential of the active inference-based
approach. The remainder of the paper is organized as follows: we begin by con-
ciselyintroducingtheEECproblemandthemanufacturingsystemunderstudy.
We then present an overview of the formalization of active inference, describe
the agent, evaluate its performance, and discuss future directions.
2 Application Overview
Active inference has proven effective in various applications commonly associ-
ated with decision-making processes in biological agents, such as humans and
animals. These applications primarily involve visual sensory output as obser-
vations. For instance, Fountas et al. (2020) [7] tested their agent on tasks like
Dynamic dSprites[14] and Animal-AI[4], which can be performed by biological
agents simply. Additionally, applications in robotics [20,5] (e.g., manipulation
[36]) align with tasks that human agents can typically perform naturally. This
effectiveness stems from active inference being a theory of decision-making for
biologicalagents[29].However,certainapplications,suchasthecontrolofindus-
trial systems, can present complex challenges. While the decision-making pro-
cesses and existing applications mentioned above may not be straightforward,
human agents may struggle to devise effective policies for these more intricate
problems.
EEC in Manufacturing Systems EEC is gaining prominence in both aca-
demic and industrial circles within manufacturing systems. It offers substantial
energysavingsacrossthreekeycontrollevels:component,machine,andproduc-
tion system. At its core, EEC involves managing the power consumption state
AIF Meeting EEC 3
ofobjectsbasedonenvironmentalconditions.Objectsarekeptfullyoperational
when their functions are needed and transitioned to low power states when not
inuse,thoughthisposeschallengesduetotheunpredictablenatureofdemands
andthepenaltiesincurredduringstatetransitions.Thesepenaltiesincludeboth
time wasted during transition, when the object adds no value, and the energy
consumed during the transition process. A comprehensive and recent literature
review on this topic can be found in [33].
System Description We follow the task outlined by Loffredo et al. (2023)
[23], which focuses on a stand-alone manufacturing workstation that can be
extended to more complex multi-stage production lines [21]. Accordingly, the
systemunderstudyhasanupstreambufferBwithfiniteholdingcapacityserving
multiple identical parallel machines, as depicted in Fig. 1. It is subject to the
stochasticarrivalofparts,whilemachinescantransitionbetweenvariousstates:
working (idle or busy), standby, startup, and failed. Power consumption varies
by state: standby (w ), failed (w ), startup (w ), idle (w ), and busy (w ),
sb f su id b
where w >w >w >w ≈w ≈0. The key characteristic of this system is
b su id sb f
Fig.1: Layout of parallel and identical machines in the workstation [23].
thatallprocessesarestochastic,modeledasPoissonprocesses[19].Thispertains
to the arrival rate (λ) to buffer B, machine processing times (µ), startup times
(δ), time between failures (ψ), and time to repair (ξ), all with expected values,
independentandstationary.Machinesworkonasingleparttypefollowingafirst-
come-first-served rule and cannot be switched off during processing or startup.
Machinesbecomestarved(i.e.,enteranidlestate)iftheyarereadytoprocessbut
B isempty,andtheycannotbeblockedasthereisaninfinitedownstreambuffer.
Effective EEC aims to manage power variations efficiently to minimize energy
consumption without compromising productivity. Therefore, the primary task
is to dynamically decide the number of machines to keep working or switch off
based on the emergent stochastic patterns, ensuring optimal trade-offs between
energy savings and system throughput.
3 Deep Active Inference Agent
Active inference is a unifying theory that integrates inference, perception, and
action by emphasizing the dependence of observations on actions [27]. To effec-
4 Y. Taheri Yeganeh et al.
tively manage observations toward preferred states, the optimization of actions
playsacrucialrole[27].Thisconceptwasoriginallyproposedasaframeworkfor
understanding how organisms actively control and navigate their environment
by iteratively updating their beliefs and actions based on sensory evidence [29].
The FEP [10,26] is at the core of active inference, paving the way for creating
a mathematical model, and there are even experimental evidences supporting it
[16].WemodifytheproposedagentbyFountasetal.(2020)[7],whichexhibited
notable capabilities and performance when compared against three benchmark
model-free RL algorithms in two image applications.
3.1 Active Inference Formalism
Active inference agents employ an integrated probabilistic framework consisting
of an internal generative model [6] coupled with inference mechanisms to rep-
resent and interact with the world. Similar to RL the agent interacts with the
environment, but using three random variables representing observation, latent
state, and action (i.e., (o ,s ,a ) at time t). The framework assumes a Par-
t t t
tially Observable Markov Decision Process (POMDP) [17,6,30]. The generative
model of the agent, parameterized with θ, is defined over these variables (i.e.,
P (o ,s ,a )) [7]. Generally, the agent acts to reduce surprise, which can
θ 1:t 1:t 1:t−1
bequantifiedby−logP (o ).Specifically,therearetwostepsfortheagentwhile
θ t
interacting with the world [29,7] as follows:
1) The agent calibrates its generative model through fitting predictions and
improve its representation of the world. This is done by minimizing Variational
Free Energy (VFE), which is similar to surprise of predictions in connection
with the actual observations [7,35,30], as follows:
θ∗ =argmin (cid:0)E [logQ (s ,a )−logP (o ,s ,a )] (cid:1) . (1)
θ
Qϕ(st,at) ϕ t t θ t t t
Thisobjectivefunctioniscommonlyknownasthenegativeevidencelowerbound
(ELBO) [1], which is the upper bound for −logP (o ). It is also used as a
θ t
foundation for training variational autoencoders [18].
2) The agent makes decisions (i.e., chooses actions) in active inference based
on the accumulated negative Expected Free Energy (EFE or G):
(cid:32) (cid:33)
(cid:88)
P(π)=σ(−G(π)) = σ − G(π,τ) , (2)
τ>t
where σ(·) represents the Softmax function, and π (i.e., policy) denotes the
sequence of actions. The EFE encompasses minimizing surprise regarding pre-
ferredobservations3,exploringuncertainty,andreducinguncertaintyaboutmodel
parameters [7]. The EFE for τ ≥t can be formulated as follows [37]:
G(π,τ)=E E [logQ (s ,θ|π)−logP(o ,s ,θ|π)] . (3)
P(στ|sτ,θ) Qϕ(sτ,θ|π) ϕ τ τ τ
3 InEFE,thesurprise ofpredictionsismeasuredwithrespecttothepreference,while
in VFE, it is measured with respect to the actual observation used to calibrate the
model.
AIF Meeting EEC 5
Fountas et al., (2020) [7] provided a derivation [37] for calculating the EFE in
Eq. 3 at each time step:
G(π,τ)=−E [logP(o |π)] (4a)
Q˜ τ
+E [logQ(s |π)−logP(s |o ,π)] (4b)
Q˜ τ τ τ
+E [logQ(θ|s ,π)−logP(θ|s ,o ,π)] . (4c)
Q˜ τ τ τ
They expanded the formalism, leading to a tractable estimate for EFE that is
both interpretable and calculable [7]:
G(π,τ)=−E [logP(o |π)] (5a)
Q(θ|π)Q(sτ|θ,π)Q(oτ|sτ,θ,π) τ
+E (cid:2)E H(s |o ,π)−H(s |π) (cid:3) (5b)
Q(θ|π) Q(oτ|θ,π) τ τ τ
+E H(o |s ,θ,π)−E H(o |s ,π) . (5c)
Q(θ|π)Q(sτ|θ,π) τ τ Q(sτ|π) τ τ
This paved the way for establishing a unified formalism for computing decisions
in Eq. 2. Accordingly, actions are connected to perception, which is achieved
through Bayesian inference, based on the EFE in Eq. 5.
Through this formalism that leads to calculating the EFE (i.e., Eq. 5), we
can interpret the contribution of each element [7]: The first term (i.e., Eq.
5a) is analogous to reward in RL as it is the surprise4 of the prediction consid-
ering preferred observation. The second term (i.e., Eq. 5b) represents state
uncertainty,whichismutualinformationbetweentheagent’sbeliefsaboutstate
before and after prediction. This term also shows a motivation to explore areas
of the environment that resolve state uncertainty [7]. The third term (i.e.,
Eq. 5c) represents uncertainty about model parameters considering new obser-
vations. This term is also referred active learning, novelty, or curiosity [7]. In
fact, model parameters (i.e., θ), particularly contribute to making predictions,
including generation of the next states.
Environment
Observation Observation
Decision Calibration EFE Model ↔VFE
Agent Inference & Prediction
Fig.2:Theillustrationdepictstwoviewsoftheactiveinferenceframework:gen-
eral steps on the left and active inference elements on the right.
4 Here, instead of maximizing cumulative rewards, the focus is on minimizing the
surprise, which quantifies the extent of deviation (i.e., misalignment) between the
prediction and the preferred observation.
6 Y. Taheri Yeganeh et al.
In summary, the framework (as depicted in Fig. 2) is realized through a
mathematicalformalisminthefollowingmanner.Theobservationisfedasinput,
which propagates through the model to create perception (i.e., beliefs), which
include generating future states. It is to facilitate the calculation of EFE (in
Eq. 5) integrated into the planner derived from policy (in Eq. 2) to act on the
environment. After obtaining the next observation the VFE (in Eq. 1) can be
calculated, which calibrate (i.e., learning) the model based on the matching the
new observation with the prediction. In fact, every time the framework goes
through the loop the model is optimized based on the VFE form the previous
loop, then the optimized model is used to for the rest.
3.2 Architecture
An agent within the active inference framework needs different modules, which
are entangled within the framework. Amortization [18,25,12] is introduced into
the formalism (in Sec. 3.1) to scale-up the realization [7]. The formalism is in-
herently probabilistic and parameterized with with two sets, θ = {θ ,θ } rep-
s o
resenting generative and ϕ={ϕ } recognition elements [7]. Using the following
s
parameterized modules the formalism (in Sec. 3.1) can be calculated: Encoder
(i.e., Q (s )), an amortized inference of the hidden state (i.e., an inference net-
ϕs t
work[24]providingamappingbetweentheobservation,o˜,andadistributionfor
t
itscorrespondinghiddenstate).Transition(i.e.,P (s |s˜,a˜ )),whichgener-
θs t+1 t t
atesadistributionforthenexthiddenstatebasedonbothasampledactionand
thecurrenthiddenstate.Decoder(i.e.,P (o |s˜ ))generatesadistribution
θo t+1 t+1
for prediction based on the sampled hidden state.
Neural networks can facilitate the realization of these modules by repre-
senting a mapping between a sample and the respective distribution. In fact,
parameters of a pre-selcted (e.g., Gaussian) distribution can be approximated.
Here, we model the state space specifically with a multivariate Gaussian distri-
bution, assuming no covariance (i.e., diagonal Gaussian). Using the VFE (i.e.,
Eq. 1), all the three networks can be trained in an end to end fashion. Aside
from action and the transition, which can even be considered integrated within
the state space, the structure bears a resemblance to a variational autoencoder
[18]. It is noteworthy that training for the both include optimizing ELBO, as
specified in Eq. 1.
Utilizingthementionedarchitecture(alsodepictedinFig.3),theagentwould
be able to calculate the EFE in Eq. 5 for a given policy (i.e., π), which is a se-
quence of actions. Thus, we can calculate the probabilities for selecting actions
through Eq. 2. In order to make better decisions, the agent can plan ahead by
simulating future trajectories using the architecture mentioned above. However,
asthepolicyspacegrowsexponentiallyintothefuture,itisinfeasibletoevaluate
allpossiblescenarios.Fountasetal.(2020)[7]introducedtwoapproachestoalle-
viatetheobstacle.1)TheyusedthestandardMonte-CarloTreeSearch(MCTS)
[3,38],atree-basedsearchalgorithm,whichselectivelyexplorepromisingtrajec-
tories in a restricted manner. 2) They introduced another recognition module
AIF Meeting EEC 7
[32,25,41], parameterized with ϕ as follows: Habit (i.e., Q (a )), an amor-
a ϕa t
tized inference of actions (i.e., an inference network [24] providing a mapping
between a sampled hidden state, s˜, and normalized, through Softmax function,
t
probabilities for the actions). Habit is also realized through a neural network,
o˜ t Q ϕs (s t ) P θs (s t+1 |s˜ t ,a˜ t ) P θo (o t+1 |s˜ t+1 )
Encoder Transition Decoder
o˜ o˜
t t+1
a˜ , ∆
t t
Fig.3: The agent’s architecture and generative framework resemble that of a
VAE. The line on top represents the agent simulating the future and making a
prediction, while on the bottom, the agent receives a new observation after ∆
t
of taking an action, a˜ .
t
which approximates the posterior distribution over actions (i.e., P(a |s )) using
t t
the prior P(a ) that is obtained from the MCTS [7]. This network is trained to
t
reproduce the last action sampled form the planner, given the last state. This is
similar to the fast and habitual decision-making in biological agents [42].
Fountas et al. (2020) [7] followed the standard four steps of MCTS [38,40],
whichletthemtorestrictandprioritizethetrajectoriesthatshouldbeevaluated.
Iterativly a weighted tree that has memory updates the visited states. During
each loop, a path from the existing tree (towards a leaf node) is selected (i.e.,
selection) based on the following upper bound confidence:
1
U(s ,a )=G˜(s ,a )+c ·Q (a |s )· . (6)
t t t t explore ϕa t t 1+N(a ,s )
t t
WhereG˜(s ,a )representsthealgorithm’scurrentestimationfortheEFE,while
t t
N(a ,s )denotesthenumberoftimesanode(i.e.,astateandactionpair)inthe
t t
tree has been visited during the tenure, along with an exploration hyperparam-
eter, c . Then, starting from the leaf and for every possible action (i.e., ex-
explore
pansion),theEFEiscalculatedforafixednumberoffuturesteps(i.e.,simula-
tion).Thisvalueisfinallyaddedtoallnodesalongthepathtocalculatetheaver-
8 Y. Taheri Yeganeh et al.
ageofG˜(s ,a )(i.e.,backpropagation).Afterall,Actionsaresampledfromthe
t t
probabilities created by P(a ) = N(at,st) (where P(a ) = (cid:80) P(π)),
t (cid:80)
j
N(at,j,st) t π:a1=at
which is proportional to the number of times a node is visited. The planning
process continues until a maximum number of loops (i.e., a hyperparameter) is
reached, or a condition, i.e., maxP(a )−meanP(a ) > T , is met, indicating
t t dec
that the planning is finished. Fountas et al. (2020) [7] further employed Q (a )
ϕa t
(i.e.,habit)tomodulatethestatespace,motivatedbyincorporatinguncertainty.
The divergence between the policy obtained from the planner (i.e., MCTS) and
the habit (i.e., D = D [Q (a )||P(a )]) can serve as a loss function for the
t KL ϕa t t
habit. This divergence also represents a type of uncertainty in the state space,
preventingthehabitfrombeingidenticaltothepolicy[9,2].Therefore,theyused
D in a logistic function as follows:
t
α
ω = + d . (7)
t 1 + e− b−D c t−1
The monotonically decreasing pattern establishes a reciprocal connection be-
tween D and ω , i.e., the state precision, using hyperparameters {α,b,c,d}.
t−1 t
Altogether, the transition (i.e., P (s |s ,a )) is modeled with the distribu-
θs t t−1 t−1
tion N(µ, σ2/ω ) [7]. This precision is analogous to β in β-VAE [14], effectively
t
promoting disentanglement of the state space by the encoder [7].
To facilitate the computations and effectively estimating different elements
withintheframework,Fountasetal.(2020)[7]usedseverallevelsofMonte-Carlo
simulations (i.e., sampling based estimations). In addition to MCTS, all terms
in the EFE (Eq. 5), including the use of MC dropout [11] for model parameters
(i.e., 5c), are estimated in this manner [7].
3.3 Enhancements
We explore various aspects to design the agent and leverage the formalism and
existing architecture outlined by Fountas et al. (2020) [7] as a foundation. First,
we examine how features and requirements related to the problem under study
can influence the agent. Subsequently, we propose solutions to address these
issues, introducing a coherent agent design.
ExploringtheApplication ThesimulationofthesystemdescribedinSection
2,isdesignedtoreplicatefeaturesofanindustrialsystem.ItutilizesPoissonpro-
cesses[19](i.e.,exponentialdistributions)formachinestatetransitionsandpart
arrivals [23]. The event-driven steps employed by Loffredo et al. (2023) [23,22]
trigger decisions after a system state transition rather than at fixed intervals,
proving effective for controlling working machines. This problem can be viewed
as either continuous-time stochastic control or a discrete-time Markov Chain
process [34]. Continuous-time modeling requires making time intervals visible
to the agent for both machines and subsequent observations, whereas discrete-
time modeling allows the agent to learn dynamics by observing transitions to
AIF Meeting EEC 9
createprobabilitiesfordifferenttransitions.Thereisavariabletimeintervalbe-
tween subsequent observations (i.e., ∆t as depicted in Fig. 3). This variability
requires synchronizing the transition for prediction (i.e., P (o |s˜ )) with
θo t+1 t+1
the next observation (i.e., o˜ ) in the continuous-time model. Incorporating
t+1
continuous-timefacilitatesneuralnetworkfunctionapproximationforlonger∆t
duringplanning.However,usingcontinuous-timecanfurthercomplicatethepre-
diction structure since residence times for machine states exist in observation.
Here, we utilize discrete-time event-driven steps that simplify this process com-
pared to the continuous-time approach.
The stochastic nature, along with the integral and continuous form of the
reward functions for the system under study [23,22], implies that the effects
of decisions may not be immediately observable, aligning with POMDPs. The
system has a delay in responding to the policy (i.e., delayed policy response),
which we refer to as long/delayed impact horizon, particularly with respect to
reward.ThisisanimportantdistinctionfromenvironmentsevaluatedbyFountas
et al. (2020) [7]. The agent proposed by them focuses on planning based on
immediatenextpredictions.Theproblemathandiscontinuouswithnoterminal
state, and the span over which reward functions are integrated may encompass
afewthousandsteps.Thiscomplicatesplanningandhighlightstheneedforless
computational cost.
ExperienceReplay Thegenerativemodelencompassesthecoreofactiveinfer-
ence[10,29,7]andpredictivecoding[27].Therefore,theperformanceofanyactive
inference-based agent heavily relies on its accuracy. To improve model training,
we introduce experience replay [28] using a memory that stores (o ,a ,o )
t t t+1
at different steps. During training, we sample a batch of experiences from the
memory and ensure the latest experience is also included. However, for all the
batched experiences, we utilize ω based on the latest experience.
t
Hybrid Horizon To address the limitations arising from the short horizon of
the EFE, which relies on immediate next predictions, we propose augmenting
theplannerwithanauxiliarytermtoaccountforlongerhorizons.Q-learning[43]
and its enhanced variant, deep Q-learning [28], can serve as model-free planners
withlongerhorizons,leveragingrewardstoupdatetheQ-valueforastate-action
pair. The Q-value represents the expected return, considering long-term conse-
quences even in one-step lookahead [39]. Mnih et al. (2015) [28] demonstrated
the effectiveness of DQN in learning relatively long-term strategies in certain
games.Loffredoetal.(2023)[23]demonstratedthatdeepQ-learningcanachieve
near-optimal performance for the systems under study. Accordingly, we modify
Q (a )torepresentamortizedinferenceofactions,mappingobservationso˜ (or
ϕa t t
sampledpredictions)tonormalizedactionprobabilitiesusingaSoftmax function
and training it with deep Q-learning updates based on rewards from experience
replay. We introduce a hyperparameter γ to balance the contributions of long
and short horizons. Thus, we arrive at a new formulation for the planner to
10 Y. Taheri Yeganeh et al.
incorporate longer horizons:
P(a )=γ·Q (a )+(1−γ)·σ(−G(π)) . (8)
t ϕa t
The resulting combination achieves a controlled balance between the EFE for
the short horizon terms, which incorporates uncertainties, and the long horizon
term. We further utilize the new Q (a ) to modulate the agent’s state uncer-
ϕa t
tainty based on Eq. 7. In fact, D (i.e., D [Q (a )||P(a )]) represents the
t KL ϕa t t
discrepancybetweenthelonghorizonandthecombinedpolicy,reflectingaform
of knowledge gap for the agent.
Multi-Step Transition and Planning Given the stochastic nature and long
impact horizon of the system under study, a one-step transition (as depicted in
Fig. 3) may not result in significant changes in observation and state, leading
to indistinguishable EFE terms. Therefore, the model should learn transitions
beyond one step and predict further into the future to distinguish the impact
of different policies. We modify the transition module to allow multiple steps,
controlled by a hyperparameter (e.g., s = 90), enabling multi-step transitions
given the policy (i.e., sequence of actions). Representing the sequence of actions
in a policy as a one-hot vector can be high-dimensional, so we utilize integer
encodings as an approximation. This is feasible since the actions (or number of
machines) are less categorical and can be considered rather continuous in this
case.Duringplanning,weutilizerepeatedactionsinthetransitionforeachaction
and calculate the EFE accordingly. This method assesses the impact of actions
overashortperiod,usingrepeatedactionsimulations.Thisapproximationhelps
to distinguish different actions over a horizon based on the EFE. Thus, even a
single multi-step transition can serve as a simple and computationally efficient
planner. For deeper simulations, it can be combined with MCTS of repeated
transitions. Alternatively, it can be combined with a less expensive planner that
starts with repeated transitions for each action, followed by simulating until a
specific depth using the following policy:
P˜(π,a )=(1−c )·Q (a )+c ·σ(−logP(o |π)) , (9)
τ explore ϕa τ explore τ
to calculate the EFE of the final state or trajectory. After several loops, the
accumulated EFE for each action can be used in Eq. 8.
4 Results
To demonstrate the potential of our methodology, we concentrate our experi-
mentsoncontrollingarealindustrialworkstationcomprisingsixparallel-identical
machineswithanupstreamcapacityof10,asoutlinedin[23].Allthestochastic
processes characterizing the workstation follow Poisson distributions, and thus
are exponentially distributed with different expected values. We first introduce
thepreferencefunction,thendescribethesetupandtraining,andfinallypresent
the numerical performance of our agent5.
5 Thesourcecodeisavailableathttps://github.com/YavarYeganeh/AIF_Meeting_EEC.
AIF Meeting EEC 11
4.1 Preference Function
Active inference involves an agent acting to achieve its preferred observation,
similar to the concept of a setpoint in control theory [9,27]. Consequently, the
agent possesses an internal function to quantify the proximity of the prediction
to the preferred state. This function is important for the agent’s performance.
Whileitcorrelateswiththerewardfunctioninreinforcementlearning,itisbased
on a different philosophy: a control setpoint rather than the cumulative reward
in the MDP framework of reinforcement learning [39]. Instead of the reward
function used by Loffredo et al. (2023) [22], we propose a different preference
function for the multi-objective optimization of the system under study. This
functionalignswiththeconceptofalong/delayed impact horizon systemforour
agent to control, accounting for the average performance of the system over a
fixedtimespan(t 6,e.g.,8hours)leadinguptotheobservationat(t).Itincludes
s
terms for production, energy consumption, and a combined term as follows:
T E
R = current R =1− avg (10a)
production T energy E
max max
R=ϕ·R +(1−ϕ)·R , (10b)
production energy
where ϕ is the weighting coefficient balancing production and energy consump-
tion. ϕ is set close to 1 (0.97 in our implementation) to ensure that the agent
does not significantly reduce production. T = NP(t)−NP(t−ts) represents
current ts
the throughput within the past t period, where NP(t) is the number of parts
s
producedwithinthisperiod.T isthemaximumachievablethroughput,occur-
max
ring under the ALL ON policy [22]. E = C(t)−C(t−ts) represents the average
avg ts
energy consumption over the past t period, where C(t) is the total energy con-
s
sumptionwithinthisperiod.E isthemaximumtheoreticalconsumptionrate
max
of the system, pertaining to all machines operating in their busy state.
4.2 Agent Setup and Training
We adhere to the MC sampling methodology for calculating EFE, as well as
most agent configurations, as outlined by Fountas et al. (2020) [7]. Notably,
we employ Bernoulli and Gaussian distributions to model prediction and state
distributions, respectively. We introduced a modification by utilizing activation
functions for the encoder and transition networks. The output of these two net-
worksgeneratesmeansandvariancesrepresentingGaussiandistributionsforthe
state. We applied the Tangent Hyperbolic function for the means and the Sig-
moid function, multiplied by a factor, λ , between 1 and 2, for the variances.
s
This enforces further regularization and stability for the state space to prevent
unboundedvalues,whichneedtobefittedintoanormaldistribution.Incontrast
to [7], which does not use activations for the state, our implementation includes
6 In our implementation, we use the closest recorded timestamp from the system,
respecting the fixed time span.
12 Y. Taheri Yeganeh et al.
these activations, as we found them essential for learning effective policies to
achieve high rewards.
Our agent’s observation comprises buffer levels and machine states, all one-
hotencoded.Similarly,weincorporatethethreerewardtermsinEq.10,predict-
ing only their means without pre-defining a distribution. The agent’s preference
(i.e., P(o |π)) corresponds to the prediction of the combined reward term for
τ
thesystem,whichtheagentseekstomaximize.Giventhecompositionofbinary
and continuous components of the observation, the loss of the VAE’s recon-
struction is equally scaled aggregation of binary cross-entropy and mean square
error. Additionally, due to the one-hot structure of the observation, we sample
the predictions, excluding the reward terms, which are treated as means, to be
fed into the decoder during the calculation of EFE. As we validate the perfor-
mance of a control system for an industrial application, we use a single system
duringeachtrainingepoch,withanexperiencereplaysizeof200.Thesesystems
are initialized with a random policy for one day of simulation, after removing
the profile from one day of warm-up simulation using the ALL ON policy. We
trained our agents on-policy, following a similar approach to the algorithm pro-
posed by Fountas et al. (2020) [7], but with adjustments to accommodate the
modificationswemade,includingmulti-steptransition,experiencereplay,anda
hybrid planner.
4.3 Agent Performance
To assess the efficacy of our agents, we tested their performance several times
during different training epochs, each on independent systems initialized with a
randomagentafterwarm-up,similartothoseusedfortraining.Wesimulatedthe
interactionbetweentheagentandthesystem,whichwasrandomlycontrolledat
thebeginning,foronedayofsimulationtime.Thecombinedrewardterm(inEq.
10) at the end of the test simulation was extracted as the performance measure,
with a time span of t (i.e., 8 hours in this case). Our agent, similar to [7], has
s
a large set of hyperparameters, which proved to be sensitive, especially those
related to the state space of the model. We considered both 1-step transitions
and multi-step transitions, taking repeated actions during planning for each
of the possible actions (i.e., determining how many machines to keep ON) to
then calculate their EFE. Fig. 4 presents the comparison for a single set of
hyperparameters, particularly λ = 1, except s, across different γ. It shows the
s
suitability of the multi-step transition and simple repeated planner as well as
hybrid horizon.
Since the state should fit the normal distribution, λ = 1 for variance will
s
target areas of the input domain of the Sigmoid function that are saturated
and have smaller gradients. To address this, we increased λ to 1.5, where the
s
Sigmoid function has larger gradients. This results in higher rewards even for
very small γ (i.e., 0.05), as presented in Fig. 5. This performance is primarily
coming from the second term of the EFE (i.e., Eq. 5b), which serves as an
intrinsic reward [30], as demonstrated in Fig. 5C. The other two terms are less
distinguishable for different actions on average. This suggests that the agent
AIF Meeting EEC 13
0.95
0.90
0.85
0.80
0.75
0.70
0.65
0.050.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.95
draweR
1-Step Transition
90-Step Transition
Fig.4: Comparison of test rewards during training of agents with 90-step tran-
sition against 1-step transition, when λ =1.
s
1.0
0.9
0.8
0.7 0.6 0.5 0.4
0 5 10 15 20 25 30
Learning Iterations(×103)
draweR:A
1.0
0.8
0.6
0.4 0.2 0.0
0 5 10 15 20 25 30
Learning Iterations(×103)
)nA(P
:B
1160
1180
1200 1220
1240 1260 1280 1300
0 5 10 15 20 25 30
Learning Iterations(×103)
2 mreT
EFE:C A A A A A A A 0 1 2 3 4 5 6
Fig.5: The test performance of the agent with 90-step transition and repeated
actions for planning, when λ = 1.5 and γ = 0.05, replicated with 10 different
s
random seeds. A: Average reward. B: Average planner distribution for different
actions. C: Average EFE’s term 2 for different actions.
differentiates between various actions in its state space to achieve high rewards.
Infact,thelong impact horizon andstochasticity,aswellasourapproximations
in transition and planning, hinder the predictive power of the agent, but it
managedtoinfertheimpactofdifferentactionsinitsstatespace.Itisalsoworth
noting that our agent quickly converges to high rewards but may experience
instability and loss of control if training continues for (a long time), due to
a catastrophic increase in the loss, particularly the reconstruction term of the
generative model. This necessitates early stopping mechanisms for training and
theintroductionofregularizationelements[13](e.g.,dropoutandnormalization)
to prevent or mitigate the issue.
14 Y. Taheri Yeganeh et al.
5 Conclusion and Future Work
The results demonstrate the effectiveness of our proposed modifications for the
active-inference-inspiredagent.Notably,asinglemulti-transitionlookaheadwith
repeated actions, coupled with a hybrid horizon, achieves high rewards without
relying on extensive planning algorithms like MCTS. This is important given
the application’s need for deep lookaheads into the future, while stochasticity
presentsachallenge.UnlikeRL,whichgenerallydependsonhigh-qualityreward
signals that can be expensive to collect and difficult to design, AIF operates
directlyonobservations.Thisreducestheneedforwell-engineeredandexpensive
reward structures while enabling more flexible and adaptive decision-making.
Overall, the potential of our methodology for addressing the EEC problem and
similarscenarioscharacterizedbydelayedpolicyresponseandhighstochasticity
is evident.
Improving the methodology could involve enhancing the generative model,
the core of active inference, especially by introducing recurrent transitions or
enhancing predictive capabilities. The integration of diffusion-based generative
models instead of VAEs [15] is also a promising direction. The framework and
formalism of active inference agents show promise for non-stationary scenarios,
where model-free agents may struggle to adapt swiftly. Future research will fo-
cus on improving the agent, extending experimental validation, and tailoring
the methodology for non-stationary scenarios, leveraging the strengths of active
inference to develop more robust and efficient decision-making algorithms.
Acknowledgments
The work presented in this paper was supported by HiCONNECTS, which has
received funding from the Key Digital Technologies Joint Undertaking under
grant agreement No. 101097296.
References
1. Blei, D.M., Kucukelbir, A., McAuliffe, J.D.: Variational inference: A review for
statisticians. Journal of the American statistical Association 112(518), 859–877
(2017)
2. Byers, A., Serences, J.T.: Exploring the relationship between perceptual learning
and top-down attentional control. Vision research 74, 30–39 (2012)
3. Coulom, R.: Efficient selectivity and backup operators in monte-carlo tree search.
In: International conference on computers and games. pp. 72–83. Springer (2006)
4. Crosby, M., Beyret, B., Halina, M.: The animal-ai olympics. Nature Machine In-
telligence 1(5), 257–257 (2019)
5. Da Costa, L., Lanillos, P., Sajid, N., Friston, K., Khan, S.: How active inference
could help revolutionise robotics. Entropy 24(3), 361 (2022)
6. Da Costa, L., Sajid, N., Parr, T., Friston, K., Smith, R.: Reward maximization
through discrete active inference. Neural Computation 35(5), 807–852 (2023)
AIF Meeting EEC 15
7. Fountas,Z.,Sajid,N.,Mediano,P.A.M.,Friston,K.J.:Deepactiveinferenceagents
using monte-carlo methods. ArXiv abs/2006.04176 (2020)
8. Friston,K.:Thefree-energyprinciple:aunifiedbraintheory?Naturereviewsneu-
roscience 11(2), 127–138 (2010)
9. Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., Pezzulo, G.: Active in-
ference: a process theory. Neural computation 29(1), 1–49 (2017)
10. Friston, K., Rigoli, F., Ognibene, D., Mathys, C., Fitzgerald, T., Pezzulo, G.:
Action and behavior: A free-energy formulation. Biological Cybernetics 102(3),
227–260 (2010)
11. Gal, Y., Ghahramani, Z.: Dropout as a bayesian approximation: Representing
modeluncertaintyindeeplearning.In:internationalconferenceonmachinelearn-
ing. pp. 1050–1059. PMLR (2016)
12. Gershman, S., Goodman, N.: Amortized inference in probabilistic reasoning. In:
Proceedings of the annual meeting of the cognitive science society. vol. 36 (2014)
13. Goodfellow, I., Bengio, Y., Courville, A.: Deep learning. MIT press (2016)
14. Higgins,I.,Matthey,L.,Pal,A.,Burgess,C.,Glorot,X.,Botvinick,M.,Mohamed,
S.,Lerchner,A.:beta-vae:Learningbasicvisualconceptswithaconstrainedvari-
ationalframework.In:Internationalconferenceonlearningrepresentations(2016)
15. Huang, Y., Li, Y., Matta, A., Jafari, M.: Navigating autonomous vehicle on un-
marked roads with diffusion-based motion prediction and active inference. arXiv
preprint arXiv:2406.00211 (2024)
16. Isomura, T., Kotani, K., Jimbo, Y., Friston, K.J.: Experimental validation of the
free-energyprinciplewithinvitroneuralnetworks.NatureCommunications14(1),
4547 (2023)
17. Kaelbling,L.P.,Littman,M.L.,Cassandra,A.R.:Planningandactinginpartially
observable stochastic domains. Artificial intelligence 101(1-2), 99–134 (1998)
18. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114 (2013)
19. Kingman, J.F.C.: Poisson processes, vol. 3. Clarendon Press (1992)
20. Lanillos,P.,Meo,C.,Pezzato,C.,Meera,A.A.,Baioumy,M.,Ohata,W.,Tschantz,
A., Millidge, B., Wisse, M., Buckley, C.L., et al.: Active inference in robotics and
artificial agents: Survey and challenges. arXiv preprint arXiv:2112.01871 (2021)
21. Loffredo, A., Frigerio, N., Lanzarone, E., Matta, A.: Energy-efficient control in
multi-stage production lines with parallel machine workstations and production
constraints. IISE Transactions 56(1), 69–83 (2024)
22. Loffredo,A.,May,M.C.,Matta,A.,Lanza,G.:Reinforcementlearningforsustain-
ability enhancement of production lines. Journal of Intelligent Manufacturing pp.
1–17 (2023)
23. Loffredo,A.,May,M.C.,Schäfer,L.,Matta,A.,Lanza,G.:Reinforcementlearning
for energy-efficient control of parallel and identical machines. CIRP Journal of
Manufacturing Science and Technology 44, 91–103 (2023)
24. Margossian, C.C., Blei, D.M.: Amortized variational inference: When and why?
arXiv preprint arXiv:2307.11018 (2023)
25. Marino, J., Yue, Y., Mandt, S.: Iterative amortized inference. In: International
Conference on Machine Learning. pp. 3403–3412. PMLR (2018)
26. Millidge, B.: Applications of the free energy principle to machine learning and
neuroscience. arXiv preprint arXiv:2107.00140 (2021)
27. Millidge, B., Salvatori, T., Song, Y., Bogacz, R., Lukasiewicz, T.: Predictive cod-
ing: towards a future of deep learning beyond backpropagation? arXiv preprint
arXiv:2202.09467 (2022)
16 Y. Taheri Yeganeh et al.
28. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G.,
Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G., et al.: Human-level
control through deep reinforcement learning. nature 518(7540), 529–533 (2015)
29. Parr, T., Pezzulo, G., Friston, K.J.: Active inference: the free energy principle in
mind, brain, and behavior. MIT Press (2022)
30. Paul, A., Sajid, N., Da Costa, L., Razi, A.: On efficient computation in active
inference. arXiv preprint arXiv:2307.00504 (2023)
31. Pezzato, C., Corbato, C.H., Bonhof, S., Wisse, M.: Active inference and behavior
treesforreactiveactionplanningandexecutioninrobotics.IEEETransactionson
Robotics 39(2), 1050–1069 (2023)
32. Piché, A., Thomas, V., Ibrahim, C., Bengio, Y., Pal, C.: Probabilistic planning
with sequential monte carlo methods. In: International Conference on Learning
Representations (2018)
33. Renna,P.,Materi,S.:Aliteraturereviewofenergyefficiencyandsustainabilityin
manufacturing systems. Applied Sciences 11(16), 7366 (2021)
34. Ross, S.M.: Introduction to probability models. Academic press (2014)
35. Sajid,N.,Faccio,F.,DaCosta,L.,Parr,T.,Schmidhuber,J.,Friston,K.:Bayesian
brains and the rényi divergence. Neural Computation 34(4), 829–855 (2022)
36. Schneider, T., Belousov, B., Chalvatzaki, G., Romeres, D., Jha, D.K., Peters, J.:
Activeexplorationforroboticmanipulation.In:2022IEEE/RSJInternationalCon-
ference on Intelligent Robots and Systems (IROS). pp. 9355–9362. IEEE (2022)
37. Schwartenbeck, P., Passecker, J., Hauser, T.U., FitzGerald, T.H., Kronbichler,
M., Friston, K.J.: Computational mechanisms of curiosity and goal-directed ex-
ploration. elife 8, e41703 (2019)
38. Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A.,
Hubert,T.,Baker,L.,Lai,M.,Bolton,A.,etal.:Masteringthegameofgowithout
human knowledge. nature 550(7676), 354–359 (2017)
39. Sutton, R.S., Barto, A.G.: Reinforcement learning: An introduction. MIT press
(2018)
40. Świechowski,M.,Godlewski,K.,Sawicki,B.,Mańdziuk,J.:Montecarlotreesearch:
A review of recent modifications and applications. Artificial Intelligence Review
56(3), 2497–2562 (2023)
41. Tschantz, A., Millidge, B., Seth, A.K., Buckley, C.L.: Control as hybrid inference.
arXiv preprint arXiv:2007.05838 (2020)
42. Van Der Meer, M., Kurth-Nelson, Z., Redish, A.D.: Information processing in
decision-making systems. The Neuroscientist 18(4), 342–359 (2012)
43. Watkins, C.J., Dayan, P.: Q-learning. Machine learning 8, 279–292 (1992)

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Active Inference Meeting Energy-Efficient Control of Parallel and Identical Machines"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
