=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Simulating Biological Intelligence: Active Inference with Experiment-Informed Generative Model
Citation Key: paul2025simulating
Authors: Aswin Paul, Moein Khajehnejad, Forough Habibollahi

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Abstract: Withrecentandrapidadvancementsinartificialintelligence(AI),understandingthefoundationof
purposefulbehaviourinautonomousagentsiscrucialfordevelopingsafeandefficientsystems.While
artificialneuralnetworkshavedominatedthepathtoAI,recentstudiesareexploringthepotential
ofbiologicallybasedsystems,suchasnetworksoflivingbiologicalneuronalnetworks. Alongwith
promisesofhighpoweranddataefficiency,thesesystemsmayalsoinformmoreexplainableand
biologically plausible models. In this work, we propose a framework ...

Key Terms: experiment, biological, simulating, australia, models, generative, inference, active, model, informed

=== FULL PAPER TEXT ===

SIMULATING BIOLOGICAL INTELLIGENCE: ACTIVE INFERENCE
WITH EXPERIMENT-INFORMED GENERATIVE MODEL
APREPRINT
AswinPaul1,2,MoeinKhajehnejad1,ForoughHabibollahi3,BrettJ.Kagan3andAdeelRazi1,4
1TurnerInstituteforBrainandMentalHealth,SchoolofPsychologicalSciences,MonashUniversity,Clayton3800,Australia
2VERSES,LosAngeles,California,USA
3CorticalLabsPtyLtd,Melbourne3056,Australia
4CIFARAzrieliGlobalScholarsProgram,CIFAR,Toronto,Canada
August12,2025
ABSTRACT
Withrecentandrapidadvancementsinartificialintelligence(AI),understandingthefoundationof
purposefulbehaviourinautonomousagentsiscrucialfordevelopingsafeandefficientsystems.While
artificialneuralnetworkshavedominatedthepathtoAI,recentstudiesareexploringthepotential
ofbiologicallybasedsystems,suchasnetworksoflivingbiologicalneuronalnetworks. Alongwith
promisesofhighpoweranddataefficiency,thesesystemsmayalsoinformmoreexplainableand
biologically plausible models. In this work, we propose a framework rooted in active inference,
ageneraltheoryofbehaviour,tomodeldecision-makinginembodiedagents. Usingexperiment-
informed generative models, we simulate decision-making processes in a simulated game-play
environment,mirroringexperimentalsetupsthatusebiologicalneurons. Ourresultsdemonstrate
learningintheseagents,providinginsightsintotheroleofmemory-basedlearningandpredictive
planninginintelligentdecision-making. ThisworkcontributestothegrowingfieldofexplainableAI
byofferingabiologicallygroundedandscalableapproachtounderstandingpurposefulbehaviourin
agents.
Keywords GenerativeModels¬∑ActiveInference¬∑CounterfactualLearning¬∑BiologicallyInspiredAI¬∑Explainable
ArtificialIntelligence
1 Introduction
The rapid advancement of artificial intelligence (AI) underscores a critical need for a more robust foundation for
purposefuldecision-makinginautonomousagents,essentialfordevelopingsafeandefficientsystems. Whileartificial
neuralnetworks(ANNs)havepavedmanypathsinAI,theirlimitationsinemulatingthenuanced,adaptivebehaviours
observed in biological systems are becoming increasinglyapparent. This recognition hasspurred exploration into
biologicalparadigms,wherelivingneuronalnetworks(BNNs)mayofferprofoundinsightsandefficienciescurrently
beyondthereachofconventionalANNs.
Recent experimental work, such as the DishBrain system Kagan et al. [2022], exemplifies this potential. In these
experiments,corticalneuronsculturedonsiliconchipsandintegratedintoaclosed-loopsimulatedgameenvironment
demonstratedtheabilitytomodifytheirbehaviourinrealtime. Thisfinding,whereevenminimalbiologicalsubstrates
exhibitadaptiveintelligencethroughsensoryfeedbackKaganetal.[2022],hashelpedestablishsyntheticbiological
intelligence(SBI),aparadigmleveraginglivingneuralsystemsascomputationalsubstratesKaganetal.[2023]. Such
biologicallygroundedapproaches,alongsidenovelexplorationsinnaturalcomputationlikeDNA-basedmolecular
learningsystemsEvansetal.[2024],areexpandingthehorizonsofAIbyemphasisingbiologicalplausibility.
Despitethesesignificantexperimentalstrides,acrucialgappersists: thelackoftheoreticalframeworkscapableof
comprehensivelymodellingandexplainingtheemergentintelligenceobservedinthesebiologicalsystems. Existing
AImodels,particularlythosebasedondeeplearning,oftenfallshortincapturingtheadaptive,memory-driven,and
5202
guA
9
]IA.sc[
1v08960.8052:viXra
ActiveInferenceforBiologicallyPlausibleDecision-MakingModels APREPRINT
embodiednatureinherentinbiologicaldecision-making. Toaddressthis, ourworkintroducesagenerativemodel
inspiredbytheDishBrainexperiments(seeSection2.2),leveragingactiveinferenceasatheoreticalframeworkfor
modellingbehaviour. Thisapproachallowsustosimulatedecision-makingprocesses,focusingonhowagentslearnand
engageinpredictiveplanningwithinasimulatedgame-playenvironmentmirroringbiologicalexperiments(asdetailed
inSection3.1).
Oursimulationsdemonstratehowtheseagentslearn,offeringinsightsintotheinterplayofmemory-basedlearning
and predictive planning (Section 3.3). Furthermore, we systematically compare various decision-making schemes
(Section3.2)anddiscussthebroaderimplicationsofourfindingsforneuroscience,AI,andembodiedintelligencein
Section4. Ultimately,thisstudyaimstoadvancethedevelopmentofexplainable,biologicallyinspiredAIbyintegrating
biologically plausible mechanisms with robust computational modelling. Our results highlight the pivotal role of
memoryindecision-making,offeringacontrasttoconventionalplanning-centricAI,andcontributetothecreationof
moreinterpretableandadaptiveintelligentsystems.
2 Methods
2.1 GenerativemodelsbasedonPOMDPs
Agenerativemodelisascaled-downversionofthenaturalworld,butnottoosimpleFristonetal.[2023a]. Thepurpose
is to enable the agent to have prior expectations (about incoming observations) and simultaneously predict future
observationstocontroltheminitsfavour. So,activeinference,atitscore,isamodel-basedframework. However,it
doesnotimposeaparticularstructureonsuchmodels. Severalmodelsareviablefortheseproblems,includingPartially
ObservableMarkovDecisionProcesses(POMDPs)Kaelblingetal.[1998],Lovejoy[1991]andGaussianmixture
modelsPernkopfandBouchaffra[2005],dependingonthespecificrequirementsofthetask. Generativemodelsare
gainingpopularityinmachinelearningforvariousreasons,liketheemergenceofChatGPTKocon¬¥ etal.[2023]. Their
popularityisalsoemerginginfieldsincludingweathermodellingJinetal.[2023]andproteinengineeringIngraham
etal.[2023]. Inthispaper,westicktothePOMDParchitecture,assumingtheagentencodesadiscreteworldmodel
DaCostaetal.[2020](SeeSec.2.1).
POMDPsofferauniversalstructuretomodeldiscretestate-spaceenvironmentswhereparameterscanbeexpressedas
tractablecategoricaldistributions[Kaelblingetal.,1998]. APOMDPcanbeformallydefinedasatupleoffinitesets
(S,O,U,B,A):
‚ó¶ s‚ààS :S isasetofhiddenstates(s)causingobservationso.
‚ó¶ o ‚àà O :O isasetofobservations, whereo = s, inthefullyobservablesetting. Inapartiallyobservable
setting,o=f(s).
‚ó¶ u‚ààU :U isasetofactions(u).
‚ó¶ B:encodestheone-steptransitiondynamics,P(s |s ,u )i.e.,theprobabilityoftransitioningtostates
t t‚àí1 t‚àí1 t
attimetgiventhatactionu istakeninstates attimet‚àí1.
t‚àí1 t‚àí1
‚ó¶ A:encodesthelikelihoodmapping,P(o |s ),forthepartiallyobservablesetting.
œÑ œÑ
‚ó¶ D:Encodestheprioroftheagentaboutthehiddenstatefactors.
‚ó¶ E:Encodestheprioroftheagentaboutactionsu.
In a POMDP, the hidden states (s) generate observations (o) through the likelihood mapping (A) in the form of a
categoricaldistribution,P(o |s )=Cat(A√ós ).
œÑ œÑ œÑ
BisacollectionofsquarematricesB ,whereB representstransitiondynamicsP(s |s ,u =u): (B)determines
u u t t‚àí1 t‚àí1
thedynamicsofsgiventheagent‚ÄôsactionuasP(s |s ,u )=Cat(B √ós ). In[A√ós ]and[B √ós ],
t t‚àí1 t‚àí1 ut‚àí1 t‚àí1 œÑ uœÑ œÑ
s isrepresentedasaone-hotvectorthatismultipliedthroughregularmatrixmultiplication1.
œÑ
TheMarkovianityofPOMDPsmeansthatstatetransitionsareindependentofhistory(i.e. states onlydependsupon
t
thestate-actionpair(s ,u )andnots , u etc.). Thegenerativemodelcanbesummarisedasfollows,
t‚àí1 t‚àí1 t‚àí2 t‚àí2
t t
(cid:89) (cid:89)
P(o ,s ,u )=P(A)P(B)P(D)P(E) P(o |s ,A) P(s |s ,u ,B). (1)
1:t 1:t 1:t œÑ œÑ œÑ œÑ‚àí1 œÑ‚àí1
œÑ=1 œÑ=2
1One-hotisagroupofbitsamongwhichthelegalcombinationsofvaluesareonlythosewithasinglehigh(1)bitandallthe
otherslow(0).Here,thebit(1)isallocatedtothestates=s
œÑ
2
ActiveInferenceforBiologicallyPlausibleDecision-MakingModels APREPRINT
So,fromtheagent‚Äôsperspective,whenencounteringastreamofobservationsintime,suchas(o ,o ,o ,...,o ),asa
1 2 3 t
consequenceofperformingaseriesofactions(u ,u ,u ,...,u ),thegenerativemodelquantitativelycouplesand
1 2 3 t‚àí1
quantifiesthecausalrelationshipfromactiontoobservationthroughsomeassumedhiddenstatesoftheenvironment.
Thesearecalled‚Äòhidden‚Äôstatesbecausetheagentcannotdirectlyobservethehiddenstatessintheenvironment. The
agent maintains beliefs about s, using the observations o. In the experiment (Fig. A.1), the observations directly
representthestateparameters,suchastheballpositionwithoutnoise. Hence,theproblemismorecontrol-orientedthan
aperceptionproblem. So,throughoutthepaper,weassumeA:thatencodesthelikelihoodmapping(P(o |s ))isan
œÑ œÑ
identitydistribution.
Basedonthisrepresentation,anagentcannowattempttooptimiseitsactionstokeepreceivingpreferredobservations.
Sofar,theintroducedgenerativemodelhasnoconceptof‚Äòpreference‚Äôand‚Äògoal‚Äô. Inthenextfewsections,wefocuson
decision-makingschemesinactiveinference,addressingvariationsofalgorithmsusedtomodel‚Äògoal-directed‚Äô(i.e.
purposeful)behaviour. Inthenextsection,welayouttheexperiment-informedgenerativemodelwedesignedandused
throughoutthepaper.
2.2 Experimentinformedgenerativemodel
A. Structure: E. Parameters of
interest:
‚óè State space
(38 * 8 * 8 =
‚óè B
2432 states)
(Transition
‚óè Action space
dynamics in
‚óè (Up, Down, and
POMDP)
Stay)
y
‚óè Observations (Fully
‚óè CL (Counterfactual
observable states.)
x state-action
mapping)
B. X-coordinate of ball C. Y-coordinate of D. Y-coordinate of
(ball-x) ball paddle ‚óè ùö™
(ball-y) (paddle-y) (Risk, CL method)
(Assumed since ball-x is (Assumed since ball-y
‚óè C
communicated to communicated through (Assumed to be part of
‚ÄòDishBrain‚Äô through 38 8 stimulus electrodes the generative model as (Prior preference
distinct frequencies (4-41 which are evenly control is exerted. distribution,
Hz) denoting evenly spaced along the y-axis Modality is similar to learned for DPEFE
spaced positions, i.e generating 8 possible ball‚Äôs y-coordinate, i.e and AIF)
38 states.) place-coded positions.) 8 states.)
Figure 1: A: Layouts the basic structure of the generative model with dimensions of difference modalities. The
correspondingdimensions(38,8,and8)arejustifiedinB,C,andD.B,C,D:Theobservationmodalitiesinvolved
are‚Äòball-x‚Äô,‚Äòball-y‚Äô,and‚Äòpaddle-y‚Äô. Thedimensionsofeachmodalityaredeterminedaccordingtotheexperiment
specifications. E:Theparametersofinterestinthegenerativemodelusedineachdecision-makingalgorithmmentioned
inthispaper.
State-outcome modalities The information about the Pong game states was communicated to the population of
neuronsviatheDishBrainsystemusingbiologicallysafevoltageandfrequencylevelsintheexperiment. Weassume
threestate-observationmodalitiescorrespondingtotheinputsignalsinthegenerativemodel: ‚Äô ball-x‚Äô,‚Äòball-y‚Äô,and
‚Äòpaddle-y‚Äô. Theyrepresenttheball‚Äôsxandycoordinatesandtheycoordinateofthemiddleofthepaddle,respectively.
Intheexperiment,thexcoordinateoftheballrepresentsitsdistancefromthepaddlealongthex-axis. Theexperiment
communicatesitthrougharate-codedcomponentof38distinctfrequenciesrangingfrom4‚àí41Hertz.Theycoordinate
oftheballistransmittedthroughoneofthe8evenlydistributedstimulationelectrodesonthey-axisofHD-MEA(See
Fig. 1B,CandD).Usingthisinformation,wecansetupthestructureofthegenerativemodel.
3
ActiveInferenceforBiologicallyPlausibleDecision-MakingModels APREPRINT
Structure Weassumethestatespaceofthegenerativemodelhas2432states(SeeFig. 1,A)incorrespondencewith
thedimensionsmentionedabove. 38distinctstatesrepresenting‚Äòball-x‚Äô,and8differentstatesforeachofthe‚Äòball-y‚Äô
and‚Äòpaddle-y‚Äô,respectively. Theactionspacehasthreeelements,namely‚ÄòUp‚Äô,‚ÄòDown‚Äô,and‚ÄòStay‚Äô,thesameasinthe
simulatedgameenvironmentofDishBrain. Finally,theobservationsareassumedtobefullyobservablestates,asthere
isnonoiseincommunicatingthestatestothechip.
Parametersofinterest Inthiswork,weareparticularlyinterestedinsomeparametersofthePOMDPcriticalto
decision-makingschemes. Forthefullstructureofthegenerativemodel,refertoSec.2.1inMethods. Theparametersof
interestare:
‚Ä¢ B: Thetransitiondynamicsrepresenttheknowledgeaboutstatetransitionsinthesystem. AninformedB
helpspredictthefuturestatesinthehopeofcontrollingthem.
‚Ä¢ CL: ThecounterfactuallearningmethodIsomuraetal.[2023],Pauletal.[2024a](CFL)whichdependson
thestate-actionmappingCLfordecision-making.
‚Ä¢ Œì: TheRisktermincounterfactuallearningrepresentstheagent‚Äôsconfidence. Adecrease/increaseinŒìcan
indicateachangeinstrategy,influencingthelearningofCLmappingusedindecision-making.
‚Ä¢ C: Thepriorpreferencedistributionwhichdrivescontrolintheclassicalactiveinferenceagents.
Welimitedourselvestothediscrete-time, discrete-statePOMDParchitecturecaptivatedbytheirexplicitstructure.
Severalcandidatesexist,suchasGaussianmixturemodelsFristonetal.[2018]forcontinuousproblemsandANNs
Millidgeetal.[2020]forrepresentingthedynamicsforhighdimensionalproblems. Aninterestingobservationmade
whileexperimentingwithactiveinferencemodelsistheinfluenceoftinychangestothestructureanddimensionsofthe
modelinperformance. Thefreedomofmodelstructurecomeswiththecostoffine-tuningdetailsbutopensupendless
groundforexploration. Algorithmsforself-learningthestructureofgenerativemodelsareapromisingdirectionto
pursueFristonetal.[2023b].
2.3 AIF:Classicalformulationofdecision-makinginactiveinference
Activeinferenceisafirst-principleapproachformodellingbehaviourandrevolvesaroundacentralpostulate: every
‚Äòagent‚Äôensuressurvivalbyminimisingobservations(o)thatarehighin‚Äòentropy‚ÄôFriston[2010]. Thisdefinitioncallsfor
amethodtoevaluatetheentropyofanincomingobservation(o). Theentropyofobservation,alsoknownas‚Äòsurprise‚Äô
intheactiveinferenceliteratureFristonetal.[2023a],isdefinedas:
S(o)=‚àílog(P(o)). (2)
RecentexperimentalworksKaganetal.[2022],Strongetal.[2024],Isomuraetal.[2023]usetheconceptof‚Äòsurprise‚Äô
intheactiveinferenceliteraturetodesignfeedbacksignalsforembodiedagentsthatdemonstrateimprovedlearning
withtime. InEq. 2,P(o)representstheprobabilityofagivenobservationo. Itshouldbenotedthatanagentisignorant
oftheactuallikelihood(P (o))ofanyobservation,asitisnotinfullcontroloftheenvironment. Theagentisnot
true
directlyinformedofhowprobableanincomingobservationis. Hence,toestimateP(o),theagentmustinternally
encodeanduseamodeltolearnandpredicttheprobabilityofincomingobservations. Thisinternalmodeliscalledthe
generativemodelFristonetal.[2012],DaCostaetal.[2020].
Traditionally,planninganddecision-makingbyactiveinferenceagentsrevolvearoundminimisingthevariationalfree
energyoftheobservationsoneexpectsinthefuture. Toimplementthis,wedefineapolicyspacecomprisingsequences
ofactionsintime. Thepolicyspaceinclassicalactiveinference[Sajidetal.,2021]isdefinedasacollectionofpolicies
œÄ :
n
Œ†={œÄ ,œÄ ,...,œÄ }, (3)
1 2 N
whicharethemselvessequencesofactionsindexedintime;thatis,œÄ =(u ,u ,...,u ),whereu isoneoftheavailable
1 2 T t
actionsinU,andT istheagent‚Äôsplanninghorizon. N isthetotalnumberofuniquepoliciesdefinedbypermutationsof
availableactionsuoveratimehorizonofplanningT.
To enable goal-directed behaviour, we must quantify the agent‚Äôs preference for sample observations o. The prior
preferenceforobservationsisusuallydefinedasacategoricaldistributionoverobservations,
C=Cat(o). (4)
So,ifthevaluecorrespondingtoanobservationinCisthehighest,itisthemostpreferredobservationfortheagent.
Giventhesetwoadditionalparameters(Œ†andC),wecandefineanewquantitycalledtheexpectedfreeenergy(EFE)
ofapolicyœÄsimilartothedefinitionin[Sajidetal.,2021,ParrandFriston,2019]as,
4
ActiveInferenceforBiologicallyPlausibleDecision-MakingModels APREPRINT
T
G(œÄ)= (cid:88) D (cid:2) Q(o |œÄt)||C(cid:3) +E [H[P(o |s )]]. (5)
KL t Q(st|st‚àí1,œÄt‚àí1) t t
t=1(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Risk Expectedambiguity
InEq.(5)above,œÄt isthet-thelementinœÄ,i.e. theactioncorrespondingtotimetforpolicyœÄ. Theterm,Q(o |œÄt)
t
representsthemostlikelyobservationcausedbythepolicyœÄattimet. D standsfortheKL-divergence,which,when
KL
minimised,forcesthedistributionQ(o |œÄt)closertowardsC. Thistermisalsocalledthe"Risk,"whichmeasuresthe
t
riskofanactionthatleadstooutcomesthatdivergefromtheagent‚Äôspreferredstates. Thus,actionsareselectedbased
ontheirexpectedKLdivergence‚Äîessentiallychoosingactionsthatminimizethisdivergence,therebyminimizing
"risk. Thisrepresentsthegoal-directedbehaviouroftheagent. TheKL-divergencebetweentwodistributions,P andQ,
isdefinedas:
(cid:88) P(i)
D (P||Q)= P(i)log , (6)
KL Q(i)
i
andP =QifandonlyifD (P||Q)=0.
KL
InthesecondtermofEq.5,H[P(o |s )]standsforthe(Shannon)entropyofP(o |s )definedas,
t t t t
(cid:88)
H(P(o))=‚àí P(o)logP(o). (7)
oœµO
Thesecondtermisalsocalledthe‚ÄòExpectedambiguity‚Äôterm. WhentheexpectedentropyofP(o |s )w.r.tthebelief
t t
Q(s |s ,œÄt‚àí1)islower,theagentismoreconfidentofthestate-observationmapping(i.e.,A)initsgenerativemodel.
t t‚àí1
Itmayalreadybeevidentthattheaboveformulationhasonefundamentallimitation: inthestochasticcontrolproblems
commonlyencounteredinpractice,thesizeofpossibleactionspaces,U,andthetimehorizonsofplanning,T,make
thepolicyspacetoolargetobecomputationallytractable. Forexample,witheightavailableactionsinU andatime
horizonofplanningT = 15,thetotalnumberof(definable)policiesthatneedtobeconsideredare(3.5‚àó1013)35
trillion. Evenforthisrelativelysmall-scaleexample,thispolicyspaceisnotcomputationallytractabletosimulateagent
behaviour(unlessadditionaldecisiontreesearchmethods[Fountasetal.,2020,Championetal.,2021a,b]orpolicy
amortisationFountasetal.[2020],√áataletal.[2020]areconsideredorbyeliminatingimplausiblepolicytrajectories
usingOccam‚Äôsprinciple). Inthiswork,weonlyconsiderone-stepplanningastheproblemdimensionsareintractable
forplanningwiththisapproach. Werefertotheagentusingthismethodforplanningas‚ÄòAIF-1‚Äôabbreviating‚ÄòActive
inferenceagentwithone-stepplanning‚Äô.
WenowdescribeanimprovedschemethatusesdynamicprogrammingprinciplesBellman[1966]toredefinepolicy
spaceandplanningaltogether.
2.4 DPEFE:Dynamicprogramminginexpectedfreeenergy
InarecentworkPauletal.[2021],anefficientplanningalgorithmwasproposedusingdynamicprogrammingprinciples.
ThealgorithmicformulationgeneralisedforaPOMDPsettingcanbefoundinPauletal.[2024b]. Wesummarisethis
methodbelow:
ForaplanninghorizonofT (i.e.,theagentaimstoreachgoalstateattimeT),theEFEofthe(last)actionforthe
T ‚àí1thtimestepcanbewrittenas:
G(u ,o )=D [Q(o |u ,s )||C]. (8)
T‚àí1 T‚àí1 KL T T‚àí1 T‚àí1
Theterm,G(u |o )istheexpectedfreeenergyassociatedwithanyactionu ,givenwearein(hidden)state
T‚àí1 T‚àí1 T‚àí1
s . ThisestimatemeasureshowmuchwebelievethattheobservationsattimeT willalignwithourpriorpreference
T‚àí1
C.
ToestimateQ(o |u ,s ),wemakeuseofthepredictionaboutstatesthatcanoccurattimeT,Q(s ):
T T‚àí1 T‚àí1 T
Q(s |u ,s )=B ¬∑Q(s ), (9)
T T‚àí1 T‚àí1 uT‚àí1 T‚àí1
andgiventhepredictionQ(s ),weget
T
Q(o |u ,s )=A¬∑Q(s |u ,s ) (10)
T T‚àí1 T‚àí1 T T‚àí1 T‚àí1
=A¬∑ (cid:0)BT‚àí1¬∑Q(s ) (cid:1) . (11)
u T‚àí1
5
ActiveInferenceforBiologicallyPlausibleDecision-MakingModels APREPRINT
Next,usingEq.9,thecorrespondingactiondistribution(foractionselection)iscalculatedattimeT,
Q(u |o )=œÉ(‚àíG(u |o )). (12)
T‚àí1 T‚àí1 T‚àí1 T‚àí1
Here,werecursivelycalculatetheexpectedfreeenergyforactionsandthecorrespondingaction-distributionsfortime
stepsT ‚àí2,T ‚àí3,...,t=1backwardsintime,2
G(u |o )=D [Q(o |u ,s )||C]+E [G(u |o )]. (13)
t t KL t+1 t t Q(ot+1,ut+1|ot,ut) t+1 t+1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
EFEofactionattimet EFEofnextactionatt+1
Intheequationabove,thesecondtermcondensesinformationaboutallfutureobservationsratherthandoingaforward
treesearchintime. ToinformG(u |o ),weconsiderallpossibleobservation-actionpairsthatcanoccurintimet+1
t t
andusethepreviouslyevaluatedG(u |o ). InEq.13,weevaluateQ(o ,u |o ,u )using,
t+1 t+1 t+1 t+1 t t
Q(s ,u |s ,u )=Q(s |s ,u )¬∑Q(u |s ). (14)
t+1 t+1 t t t+1 t t t+1 t+1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
B Actiondistribution
WethenmapthedistributionQ(s ,u |s ,u )totheobservationspaceandevaluateQ(o ,u |o ,u )usingthe
t+1 t+1 t t t+1 t+1 t t
likelihoodmappingA. InEq.14,weassumethatactionsintimeareindependentofeachother,i.e. u isindependentof
t
u . Eventhoughactionsareassumedtobeexplicitlyindependentintime,theinformation(andhencedesirability)
t+1
aboutactionsisalsoinformedbackwardsfromtherecursiveevaluationofexpectedfreeenergy.
WhileevaluatingtheEFE,G,backwardsintime,weusedtheactiondistributioninEq.12. Thisactiondistributioncan
bedirectlyusedforactionselection. Givenanobservationoattimet,u maybesampled3from,
t
u ‚àºQ(u |o =o). (15)
t t t
WerefertoagentsusingtheDPEFEmethodfordecisionmakingas‚ÄòDP-T‚Äô,abbreviating‚ÄòActiveinferenceagentusing
dynamicprogrammingwithplanninghorizonT.
2.5 CFL:Counterfactuallearning
A different approach to decision-making does not rely on planning but on counterfactual learning (CFL). In the
counterfactuallearningmethod,theagentlearnsastate-actionmappingCLinsteadofevaluatingEFEdirectly4. This
state-actionmappingislearnedusinga‚ÄôRisk‚ÄôparameterŒì(t)usingtheupdateequationasgiveninIsomuraandFriston
[2020]as:
CL‚ÜêCL+t‚ü®(1‚àí2Œì(t))‚ü®u ‚äós ‚ü©‚ü©. (16)
t t‚àí1
Here,‚ü®¬∑‚ü©referstotheaverageovertime,and‚äóistheKronecker-productoperator. Giventhestate-actionmappingCL,
agentsamplesactionsfromthedistribution,
P(u|s ) =œÉ(ln CL¬∑s ). (17)
t‚àí1 CL t‚àí1
Thefreeparameterinourmodelisthenumberofpastinstances(ofstate-actionpairs)theagentstoresinmemoryuse
ineverytime-steptolearnCLinEq.16. Inthispaper,‚ÄòCFL-T‚Äôrepresentstheactiveinferenceagentwithamemory
horizonofT.
ThefunctionalformofŒì(t)usedinthesimulationsofthisworkis:
Œì(t) =0.55 (18)
prior
AvalueofŒìgreaterthan0.55correspondsto‚Äúhigherrisk‚ÄùintheCFLmethod. Aninitialvaluegreaterthan0.5is
necessarytoenablelearningIsomuraandFriston[2020].
2FortimesotherthanT ‚àí1,thefirstterminEq.13doesnotcontributetosolvingtheparticularinstanceifConlyaccommodates
preferencetoa(time-independent)goal-state. However,foratemporallyinformedC,i.e. withaseparatepreferenceforreward
maximisationateachtimestep,thistermwillmeaningfullyinfluenceactionselection.
3Precisionofaction-sectionmaybecontrolledbyintroducingapositiveconstantinsidethesoftmaxfunctionœÉ(.)inEq.12.The
highertheconstant,thehigherthechanceofselectingtheactionwithlessEFE.
4Fortheexactformofthegenerativemodelandfreeenergy,refertoIsomuraandFriston[2020]
6
ActiveInferenceforBiologicallyPlausibleDecision-MakingModels APREPRINT
ForupdatingŒì,weusetheequation,
1
Œì(t)‚ÜêŒì(t)‚àí . (19)
T ‚àít
goal
Here,T iswhentheagentreceivesapositiveenvironmentalreward. So,thesoonertheagentreachesthe‚Äògoal-state‚Äô,
goal
thequickertheŒì(t),i.e.,riskconvergestozero. Alltheupdaterulesthispaperdefinescanbederivedfromthepostulate
thattheagenttriestominimisethe(variational)freeenergyw.r.tthegenerativemodelIsomuraetal.[2023],Pauletal.
[2024b]. ItwasobservedinPauletal.[2024a]andKhajehnejadetal.[2024]thatthisvariationoftheCFLmethodis
wellsuitedforenvironmentsthatrequirespontaneousdecision-makingandisdata-efficient(EnvironmentsliketheCart
Pole(OpenAIGym)Bartoetal.[1983],gameofPongKhajehnejadetal.[2024]).
Thealgorithmsimplementedinthispaperfordecision-making(DPEFEPauletal.[2024b]andCFLPauletal.[2024b])
arelow-costcomputationalmethodsagainstclassicalactiveinferencealgorithms. Theimprovementincomputational
complexityofthesemodelsenabledsimulationsinthestatespaceconsistingofmorethantwothousandstatesandhas
thepotentialtoscaletomoredemandingenvironments.
2.6 Explainability: Normalisedtotalentropy
Toexaminesomeparameters,wedefinethetotalentropy(TE)ofaparameteras,
(cid:88) (cid:88)(cid:88)
TE(Z)= H(X)=‚àí p(x)logp(x). (20)
XœµZ XœµZxœµX
InEq.20,Z isaparameterliketheCLmappinginthecounterfactuallearningmethod,andH,theShannonentropy.
X is a probability distribution collection comprising the parameter Z. For example, the CL mapping is made up
ofprobabilitydistributionsP(u|s )‚àÄsœµS, representinghowprobableanactionuis, givenastatesattimet. For
t
visualisingtheevolutionoftotalentropywithtime,wescalethevectorstoringtotalentropiesineachtimesteptovalues
betweenzeroandone,i.e. normalise5it. Normalisationisadvantageoustousherebecauseitemphasisestheunderlying
trendsratherthantheabsolutevalues. Scalingalsoenablescomparisonbetweengroups. Wecallthismeasurethe
normalisedtotalentropy(NTE).
3 Results
3.1 Counterfactuallearningmethodwithmemorydemonstratessignificantimprovementinlearning
Firstly, we simulate active inference agents using the counterfactual learning algorithm Isomura et al. [2023] for
decision-making. Weuseamemory-augmentedversionofthealgorithmdevelopedinPauletal.[2024a],inwhichthe
agentusesthelastT timestepsforlearningtheCLmapping,referredtoas‚ÄòCFL-T‚Äôinthepaper. Formoredetails,
refertoSec.2.5inMethods.
Tostandardisethesimulationresultsagainsttheexperimentalresults,wetime-stampeverytrial,matchingatotaltrial
lengthoftwentyminutesasintheDishBrainexperiments. Inthebiologicalexperiments,anaverageof69.04¬±7.95
gameepisodescomprisedatrialof20minutesKaganetal.[2022],Khajehnejadetal.[2024]. Inoursimulations,
weconducttrialswith70episodeseachandaddtimelabelstomatchatotaltriallengthof20minutes. Toensure
reproducibilityandminimisetheimpactofhyperparameterselection,weconduct100trialswithdifferentrandomseeds.
Giventheassignedtimestamps,wecancomparetheperformanceofinvitroagentstoourinsilicoagents.
FromFig.2,weobserveimprovementintheperformanceof‚ÄòCFL-T‚Äôagentswithhighermemoryhorizon(T)across
allgamemetrics. Consideringonlylowermemoryhorizons(toprow),theCFL-4agentoutperformsthe‚Äôbiological
agents‚Äô(HCCandMCC)acrossalltheperformancematrices. Thememoryhorizonof4andaboveessentiallyimplies
ahigherabilitytousetheinformationfromthepasttoimproveperformance. However,amemoryhorizonof4may
notbebiologicallyplausible,giventhatthepresenceoflong-termmemoryintheDishBrainsystemhasnotyetbeen
demonstrated. Notably,CFLagentswiththelowestmemoryhorizons(1-2)performatparwiththeMCCandHCC
groups,andtheseobservationsmotivateustostudytheroleofmemoryinsystemsliketheDishBrainandinspirefuture
experimentsdependentonmemory.
InFig.2(bottomrow),weobserveanimprovementinperformancewithahighermemoryhorizon. Longmemory
horizons,forexample,16and32,arebiologicallyimplausibleforsystemslikeDishBrain. However,theseresultsstress
therelevanceoffurtherstudyingtheroleofmemoryinsuchembodiedsystems. InSec. 3.2,westudytheperformance
5TonormaliseavectorV,weusethesklearnmachinetoolkit,specifically:sklearn.preprocessing.normalize(V,norm=‚Äômax‚Äô).
7
ActiveInferenceforBiologicallyPlausibleDecision-MakingModels APREPRINT
A B C
D E F
Figure 2: Memory horizon influences performance in CFL agents compared with MCC and HCC groups.
Changesinagentperformanceareassessedbycomparingthefirst5minutes(green)tothelast15minutes(orange)of
thetrials. TheaveragenumberofA,D:hitsperrally,B,E:percentageofaces(ballsmissedaftertheinitialserve),
andC,F:percentageoflongrallies(‚â•3consecutivehits)areplottedovera20-minutereal-timeequivalentforCFL
agentsandbiologicalMCCandHCCcultures. Toprow(A,B,C):CFLagentswithshortermemoryhorizonsexhibit
a linear improvement in performance across all game metrics, with CFL-4 outperforming MCC and HCC groups.
However,memoryhorizonsof4orhighermaynotbebiologicallyplausible,aslong-termmemoryhasnotyetbeen
demonstrated in DishBrain. Bottom row (D, E, F): Agents with longer memory horizons (e.g., 16 and 32) show
continued performance gains, although such horizons are implausible for DishBrain-like systems. These findings
highlighttheroleofmemoryindecision-makingunderactiveinferenceandsuggestavenuesforfurtherexperimental
investigationsinembodiedbiologicalsystems.
ofdifferentdecision-makingalgorithmsinactiveinferenceandinterprettheirperformance. Weuselinearregressionto
furtherexploreagents‚Äôperformanceincontinuoustime(SeeFig.B.2). Next,weperformacomparisonacrossother
popularactiveinferencealgorithms.
3.2 Comparisonacrossotherpopularactiveinferencealgorithms
In this section, we compare the classical decision-making scheme in active inference (AIF-1) Sajid et al. [2022],
DaCostaetal.[2020]anddynamicprogramminginexpectedfreeenergy(DPEFE)Pauletal.[2024b],Fristonetal.
[2023c]. Formoredetails,refertoSec.2.3andSec.2.4inMethods.
Fig. 3summarisesthesimulationresultswiththeabove-mentionedagents. Forthesefigures,weselectagentswith
performancecomparabletothe‚ÄòMCC‚Äôand‚ÄòHCC‚Äôgroups. FromFig. 3weobservethefollowing:
‚Ä¢ FromFig. 3(toprow),weobservethatall‚ÄòDP-T‚Äôagentsimprovesimilarlytothe‚ÄòHCC‚Äôgroup. Ahigher
planninghorizondoesnotseemtohelpsignificantlywithperformance. Also,overaplanninghorizonof10,
theperformancedeclines. Thiseffectcanbeattributedtoover-planning.
8
ActiveInferenceforBiologicallyPlausibleDecision-MakingModels APREPRINT
A B C
D E F
Figure 3: Performance of in-silico active inference agents compared to in-vitro MCC and HCC groups in a
simulatedPongenvironment. Thegameperformanceisevaluatedacrossthreekeymetrics: A,D:Averagerallylength
(higherisbetter),B,E:Percentageoflongrallies(higherisbetter),andC,F:Percentageofaces(lowerisbetter). Top
row(A,B,C):Activeinferenceagentswithdifferentplanninghorizons(‚ÄòAIF-1‚Äôand‚ÄòDP-T‚Äô)showimprovements
comparabletotheHCCgroup. However,aplanninghorizonbeyond10leadstoadeclineinperformance,likelydueto
over-planning. Additionally,DPEFEagentsperformsimilarlyacrossthetask,suggestingthatincreasedplanningdepth
doesnotsignificantlyenhanceperformanceinPong. Bottomrow(D,E,F):The‚ÄòCFL-3‚Äôagentoutperformsallother
groupsacrossallgamemetrics,reinforcingtheideathatmemory-baseddecision-makingismoreeffectiveforin-game
environmentslikePongthanplanning-basedapproaches. Theseresultshighlightthetrade-offsbetweenmemoryand
planninginactiveinference-basedcontrol.
‚Ä¢ Similarly,fromFig.3(toprow),allDPEFEagentsperformsimilarlyintheponggametask. Thissuggests
thatahigherplanninghorizonisnothelpfulintheponggameenvironment. Thisobservationsuggeststhat
memory-baseddecision-makingismoreusefulforin-gameenvironmentslikePongthanplanningalgorithms
likeDPEFE.
‚Ä¢ From From Fig.3 (bottom row), ‚ÄòCFL-3‚Äô group outperforms other groups in all measured game metrics
(‚ÄòAveragerallylength‚Äô,‚Äò%ofLongRallies‚Äô,and‚Äò%ofAces‚Äô).
‚Ä¢ FromFig.3(bottomrow),weobservethatthe‚ÄòAIF-1‚Äôagentdemonstratesarelativeimprovementcomparable
withthecontrol‚ÄòCTL‚Äôgroup. Evidently,withone-stepplanning,theagentseemstomakenodifferencein
performanceovertime.
SeeFig. B.1foradifferentrepresentationofthesamedata. InFig. B.1(C),wecomparetherelativeimprovementof
CFLagentswithdifferentmemoryhorizons. Weseethat‚ÄòCFL-4‚Äôoutperformseveryothergroupregardingrelative
improvement.
Inthenextsection,weexaminetheevolutionofmodelparametersanddrawinsightsfromthem. Weleveragethemodel
parameterstounderstandthebasisofperformanceonadeeperlevel.
9
ActiveInferenceforBiologicallyPlausibleDecision-MakingModels APREPRINT
3.3 Modelparametersrevealexplainableinsightsintotheperformanceoftheactiveinferenceagents
A B C
D E F
Figure4: EvolutionofkeyparametersinCFL,DP-5,andAIF-1agents. A:Evolutionoftheaverageriskterm
(Œì )inCFLagents(logscale). Asignificantdropinriskisobservedonlyforthe‚ÄòCFL-4‚Äôagent, aligningwithits
t
superiorperformance. B,C:Evolutionofthenormalisedtotalentropy(NTE)ofthetransitiondynamicsforB:DP-5
andC:AIF-1agents. Thedecreasingentropyinbothcasesindicatesthatagentsareprogressivelylearningaboutthe
environment. D,E,F:EvolutionoftheNTEforkeymodelparameters: theCLvector(D),andpriorpreferences(E,
F)inDP-5andAIF-1agents,respectively. D:TheCLvector‚Äôsentropydecreasessignificantlyforhighermemory
horizons,highlightinggoal-directedlearninginCFLagents. E,F:InbothDP-5andAIF-1agents,theentropyofprior
preferencedistributionsincreasesovertime,suggestingthatnospecificballorpaddlepositionisinherentlypreferred,
reinforcingthattheprimaryobjectiveisdefendingtheballratherthanfavouringparticulargamestates. Thesefindings
underscoredifferencesinhowmemory-based(CFL)andplanning-based(DP-5,AIF-1)agentslearnandadaptinthe
gameenvironment.
Sincealltheparametersinourmodelsarenon-approximate,wecanexaminehowtheyevolvetodrawinsightsintothe
performanceandlearningoftheagents. Weexaminetheparametersintimeframessimilartothebiologicalexperiment.
InFig. 4(A),welookattheaverageŒì (Risk)inCFLagentsonalogscale. Weseeasignificantdropintherisk
t
parameterovertime,onlyforthe‚ÄòCFL-4‚Äôagent. Thisobservationqualitativelymatchesthesignificantincreasein
performanceofthe‚ÄòCFL-4‚ÄôagentasobservedinFig. 2.
Tostudytheevolutionofparametersthatencodeprobabilitydistributions(suchasB,CL,C),weproposeameasure
calledthe‚ÄôNormalisedtotalentropy(NTE)‚Äôinthispaper. Formoredetails,seeSec.2.6. NTErepresentshowinformed
aprobabilitydistributionis;hence,alowerNTEcorrespondstoamoreinformeddistributionrelativetoanuninformed
priorwiththehighestentropy. InFig. 4(D),weseethattheNTEoftheCLvectordecreaseswithtimeandhasthe
highestdecreaseforhighermemoryhorizons. Thisisevidencefortheemergenceofgoal-directedlearningintheCFL
agentsandalsoresultsinimprovedperformanceovertime,whichisobservedinFig.2. Wealsoobservethesignificant
decreaseintheNTEforthe‚ÄòCFL-4‚ÄôagentinFig.4(D)matchingthehigherperformanceof‚ÄòCFL-4‚ÄôinFig.2.
10
ActiveInferenceforBiologicallyPlausibleDecision-MakingModels APREPRINT
Additionally,thedynamicprogrammingagent‚Äôsmodelparametersrevealthemethod‚Äôslimitationinthisenvironment.
InFig. 4(secondcolumn),weexaminetheparametersoftheDP-5agent. Thereisnoparticularreasonforchoosing
thisgroupoverotherDP-Tgroups. InFig. 4(B),welookatthenormalisedtotalentropyofthetransitiondynamicsB.
Theentropycorrespondingtoallstate-observationmodalitiesdecreases,indicatingthattheagentislearningaboutthe
environment. Adecreasedentropyrepresentstheprobabilitydistributionsbecomingmoreinformedthanauniform
priorwithmaximumentropy. Similarly,inFig. 4(E),weseethattheentropyofthepriorpreferencedistributionC
increasesforallstate-observationmodalities. Thisobservationiscounter-intuitiveaswewouldexpectlearningofsome
sort,butapossibleexplanationisthatlearningapriorpreferenceisnotpossibleinthisgamesincenoparticularballor
paddlepositionispreferred. Onlydefendingtheballinthegamegivespositiverewards,andnoparticularpositionof
theballandpaddleisadvantageous. Weinterpretitastheagentisnotlearningaparticularpreferenceaboutanyofthe
observations,asdefendingtheballisthegoal,andaparticularpositionoftheballandpaddleneedstobefavoured.
SimilartotheDP-5agent,inFig.4(lastcolumn),weexaminethemodelparametersofthe‚ÄòAIF-1‚Äôagentandobservea
similartrend. FromFig.4(C),weseethattheentropyofthetransitiondynamicsofallmodalitiesdecreases,which
providesevidencethattheagentislearningmoreabouttheenvironment.Interestingly,theentropyofthepriorpreference
distributionincreases,asseeninFig.4(B).
Weleavethedetailedanalysisofparametersandtestingdifferentstructuresofthegenerativemodelstofuturework.
Theinsightsfromthisworkwillenableanalysisofsimilarfutureexperiments,openingaparadigmofprobingintothe
basisofintelligence.
4 Discussion
Wesuccessfullydemonstratetheefficacyoftheactiveinferenceframeworkinmodellingpurposefuldecision-making
within a synthetic biological intelligence context, marking a significant step in understanding such systems. Our
simulationsrevealthatagentsemployingmemory-basedlearningandpredictiveplanningcanefficientlynavigateand
adaptwithindynamicenvironments. Theactiveinferenceapproachprovidesabiologicallyplausibleandinherently
interpretable model for how systems, akin to biological neuronal networks, can achieve continuous learning and
real-timeresponsiveness. Byunifyingperception,action,andlearningunderasingleprobabilisticcalculus,active
inferenceemergesasacompellingalternativetotraditionalmachinelearningparadigms,particularlyfordeveloping
biologicallyinspiredAI.
Theadvantagesofthisapproachareunderscoredwhenconsideringtheremarkablesampleefficiencyofbiological
systems, a trait many artificial systems, including deep reinforcement learning (RL) agents, struggle to emulate
Khajehnejadetal.[2024]. Ouradoptionofactiveinferenceaimstocapturenotonlythisefficiencybutalsothecrucial
elementsofbiologicalplausibilityandinterpretability,aspectswhereopaque,extensively-tunedRLmodelsoftenfall
shortFriston[2010,2012]. Thismakesactiveinferenceparticularlywell-suitedformodellingsystemslikeDishBrain,
wheretheinterplayofexternalstimuliandemergentbehaviournecessitatesaprobabilisticframeworkadeptatmanaging
uncertaintyandfacilitatingstructuredlearning.
Theanalysisofkeymodelparameters,suchastheriskterm(Œì)andthenormalizedtotalentropy(NTE)oftheCL
vector,offersexplainableinsightsintotheagents‚Äôlearningdynamics. Forinstance,theobservedreductioninriskand
NTEinhigh-performingagentssignifiesincreasingconfidenceandrefinementintheirdecision-makingstrategiesover
timePauletal.[2024a],supportingtheemergenceofgoal-directedbehaviour. Thesefindingsarepivotalforadvancing
safeandtransparentAIsystemsAlbarracinetal.[2023],astheyallowustotracehowuncertaintyisminimisedthrough
adaptiveexploration,therebybridgingexperimentalneurosciencewiththeoreticalmodelsofintelligence.
Acriticalconsiderationforfutureworkisthebiologicalinstantiationofmemorymechanisms. Whileoursimulations
showenhancedperformanceinCFLagentswithextendedmemory,currentexperimentalevidencefromsystemslike
DishBrainindicatesrapidnetworkreorganisationandpopulation-widedynamicsratherthanexplicitmemoryrecall
Kagan et al. [2022], Habibollahi et al. [2023]. This highlights an important avenue for investigation: determining
whethersophisticatedmemory,asmodelled,isessentialforbiologicallearning,orifrapidadaptiverestructuringalone
underpinstheobservedbehaviours.
Futureresearchshouldthereforefocusondissectingtherolesofdifferentmemorytypesandadaptivedynamicsinboth
syntheticandbiologicalagents. Learningmoreaboutthesedistinctionswillbeinstrumentalinrefiningourmodelsand
couldpavethewayforinnovativehybridsystemsthatsynergisetheadaptivestrengthsofbiologicalintelligencewith
theexplanatorypowerofcomputationalframeworkslikeactiveinference. Ultimately,thisworkcontributestoamore
integratedunderstandingofpurposefulbehaviour,pushingforwardthedevelopmentofAIthatisnotonlyintelligent
butalsotransparentandgroundedinbiologicalprinciples.
11
ActiveInferenceforBiologicallyPlausibleDecision-MakingModels APREPRINT
5 Acknowledgments
AP currentlyworksat VERSES Inc., California, USA and acknowledges PhD fellowship fromthe Department of
Biotechnology,GovernmentofIndiareceivedfrom2019to2023. ARisfundedbytheAustralianResearchCouncil
(Refs: DE170100128&DP200100757)andtheAustralianNationalHealthandMedicalResearchCouncilInvestigator
Grant(Ref: 1194910). ARisaCIFARAzrieliGlobalScholarintheBrain,Mind&ConsciousnessProgram. AR,NS,
andLDareaffiliatedwithTheWellcomeCentreforHumanNeuroimaging,supportedbycorefundingfromWellcome
[203147/Z/16/Z].B.J.K.&F.H.areemployeesofandholdoptionsorsharesinCorticalLabsPteLtd.
6 Softwarenote
Allthecodeforactiveinferenceagents,pong-gameenvironment,andvisualisationusedarecustomwritteninPython
3.9.15andisavailableinthisprojectrepository: https://github.com/aswinpaul/pong_ai_2023.
References
BrettJ.Kagan,AndyC.Kitchen,NhiT.Tran,ForoughHabibollahi,MoeinKhajehnejad,BradynJ.Parker,Anjali
Bhat,BenRollo,AdeelRazi,andKarlJ.Friston. Invitroneuronslearnandexhibitsentiencewhenembodiedina
simulatedgame-world. Neuron,110:3952‚Äì3969.e8,Dec2022.
Brett J Kagan, Christopher Gyngell, Tamra Lysaght, Victor M Cole, Tsutomu Sawai, and Julian Savulescu. The
technology,opportunitiesandchallengesofsyntheticbiologicalintelligence. Biotechnologyadvances,page108233,
2023.
ConstantineGlenEvans,JacksonO‚ÄôBrien,ErikWinfree,andArvindMurugan. Patternrecognitioninthenucleation
kineticsofnon-equilibriumself-assembly. Nature,625(7995):500‚Äì507,January2024.
Karl Friston, Lancelot Da Costa, Noor Sajid, Conor Heins, Kai Ueltzh√∂ffer, Grigorios A. Pavliotis, and Thomas
Parr. Thefreeenergyprinciplemadesimplerbutnottoosimple. PhysicsReports,1024:1‚Äì29,2023a. ISSN0370-
1573. doi:https://doi.org/10.1016/j.physrep.2023.07.001. URLhttps://www.sciencedirect.com/science/
article/pii/S037015732300203X. Thefreeenergyprinciplemadesimplerbutnottoosimple.
LesliePackKaelbling,MichaelL.Littman,andAnthonyR.Cassandra. Planningandactinginpartiallyobservable
stochasticdomains.ArtificialIntelligence,101(1):99‚Äì134,1998.ISSN0004-3702.doi:https://doi.org/10.1016/S0004-
3702(98)00023-X. URLhttps://www.sciencedirect.com/science/article/pii/S000437029800023X.
WilliamS.Lovejoy. Asurveyofalgorithmicmethodsforpartiallyobservedmarkovdecisionprocesses. Annalsof
OperationsResearch,28(1):47‚Äì65,1991. ISSN1572-9338. doi:10.1007/BF02055574. URLhttps://doi.org/
10.1007/BF02055574.
F.PernkopfandD.Bouchaffra. Genetic-basedemalgorithmforlearninggaussianmixturemodels. IEEETransactions
onPatternAnalysisandMachineIntelligence,27(8):1344‚Äì1348,2005. doi:10.1109/TPAMI.2005.162.
JanKocon¬¥,IgorCichecki,OliwierKaszyca,MateuszKochanek,DominikaSzyd≈Ço,JoannaBaran,JulitaBielaniewicz,
Marcin Gruza, Arkadiusz Janz, Kamil Kanclerz, Anna Kocon¬¥, Bart≈Çomiej Koptyra, Wiktoria Mieleszczenko-
Kowszewicz, Piotr Mi≈Çkowski, Marcin Oleksy, Maciej Piasecki, ≈Åukasz Radlin¬¥ski, Konrad Wojtasik, Stanis≈Çaw
Woz¬¥niak,andPrzemys≈ÇawKazienko. Chatgpt: Jackofalltrades,masterofnone. InformationFusion,99:101861,
2023. ISSN1566-2535. doi:https://doi.org/10.1016/j.inffus.2023.101861. URLhttps://www.sciencedirect.
com/science/article/pii/S156625352300177X.
Kyo-HoonJin,Kyung-SuKang,Baek-KyunShin,June-HyoungKwon,Soo-JinJang,Young-BinKim,andHan-Guk
Ryu. Developmentofrobustdetectorusingtheweatherdeepgenerativemodelforoutdoormonitoringsystem. Expert
SystemswithApplications,234:120984,2023. ISSN0957-4174. doi:https://doi.org/10.1016/j.eswa.2023.120984.
URLhttps://www.sciencedirect.com/science/article/pii/S0957417423014860.
JohnB.Ingraham,MaxBaranov,ZakCostello,KarlW.Barber,WujieWang,AhmedIsmail,VincentFrappier,DanaM.
Lord,ChristopherNg-Thow-Hing,ErikR.VanVlack,ShanTie,VincentXue,SarahC.Cowles,AlanLeung,Jo√£oV.
Rodrigues,ClaudioL.Morales-Perez,AlexM.Ayoub,RobinGreen,KatherinePuentes,FrankOplinger,NishantV.
Panwar,FritzObermeyer,AdamR.Root,AndrewL.Beam,FrankJ.Poelwijk,andGevorgGrigoryan. Illuminating
protein space with a programmable generative model. Nature, 623(7989):1070‚Äì1078, 2023. ISSN 1476-4687.
doi:10.1038/s41586-023-06728-8. URLhttps://doi.org/10.1038/s41586-023-06728-8.
Lancelot Da Costa, Thomas Parr, Noor Sajid, Sebastijan Veselic, Victorita Neacsu, and Karl Friston. Active in-
ference on discrete state-spaces: A synthesis. Journal of Mathematical Psychology, 99:102447, 2020. ISSN
12
ActiveInferenceforBiologicallyPlausibleDecision-MakingModels APREPRINT
0022-2496. doi:10.1016/j.jmp.2020.102447. URL https://www.sciencedirect.com/science/article/
pii/S0022249620300857.
TakuyaIsomura,KiyoshiKotani,YasuhikoJimbo,andKarlJ.Friston. Experimentalvalidationofthefree-energyprinci-
plewithinvitroneuralnetworks. NatureCommunications,14(1):4547,2023. ISSN2041-1723. doi:10.1038/s41467-
023-40141-z. URLhttps://doi.org/10.1038/s41467-023-40141-z.
AswinPaul,TakuyaIsomura,andAdeelRazi. Onpredictiveplanningandcounterfactuallearninginactiveinference.
Entropy,26(6),2024a. ISSN1099-4300. URLhttps://www.mdpi.com/1099-4300/26/6/484.
KarlJFriston,RichardRosch,ThomasParr,CathyPrice,andHowardBowman. Deeptemporalmodelsandactive
inference. Neuroscience&BiobehavioralReviews,90:486‚Äì501,2018.
BerenMillidge,AlexanderTschantz,AnilK.Seth,andChristopherL.Buckley. Ontherelationshipbetweenactive
inferenceandcontrolasinference. InTimVerbelen,PabloLanillos,ChristopherL.Buckley,andCedricDeBoom,
editors,ActiveInference,pages3‚Äì11,Cham,2020.SpringerInternationalPublishing. ISBN978-3-030-64919-7.
doi:https://link.springer.com/chapter/10.1007/978-3-030-64919-7_1.
KarlJFriston,LancelotDaCosta,AlexanderTschantz,AlexKiefer,TommasoSalvatori,VictoritaNeacsu,Magnus
Koudahl,ConorHeins,NoorSajid,DimitrijeMarkovic,etal. Supervisedstructurelearning,2023b.
KarlFriston. Thefree-energyprinciple: aunifiedbraintheory? NatureReviewsNeuroscience,11(2):127‚Äì138,2010.
ISSN1471-0048. doi:10.1038/nrn2787. URLhttps://doi.org/10.1038/nrn2787.
VincentStrong,WilliamHolderbaum,andYoshikatsuHayashi. Electro-activepolymerhydrogelsexhibitemergent
memorywhenembodiedinasimulatedgameenvironment. CellReportsPhysicalScience,2024/08/282024. ISSN
2666-3864. doi:10.1016/j.xcrp.2024.102151. URLhttps://doi.org/10.1016/j.xcrp.2024.102151.
KarlFriston,SpyridonSamothrakis,andReadMontague. Activeinferenceandagency: optimalcontrolwithoutcost
functions. BiologicalCybernetics,106(8):523‚Äì541,2012. ISSN1432-0770. doi:10.1007/s00422-012-0512-8. URL
https://doi.org/10.1007/s00422-012-0512-8.
NoorSajid,PhilipJ.Ball,ThomasParr,andKarlJ.Friston. Activeinference: Demystifiedandcompared. Neural
Computation,33(3):674‚Äì712,January2021. ISSN0899-7667. doi:10.1162/neco_a_01357. URLhttps://doi.
org/10.1162/neco_a_01357.
Thomas Parr and Karl J. Friston. Generalised free energy and active inference. Biological Cybernetics, 113
(5):495‚Äì513, 2019. ISSN 1432-0770. doi:10.1007/s00422-019-00805-w. URL https://doi.org/10.1007/
s00422-019-00805-w.
ZafeiriosFountas,NoorSajid,PedroA.M.Mediano,andKarlFriston. DeepactiveinferenceagentsusingMonte-Carlo
methods. arXiv:2006.04176[cs,q-bio,stat],June2020.
Th√©ophileChampion,LancelotDaCosta,HowardBowman,andMarekGrzes¬¥. BranchingTimeActiveInference: The
theoryanditsgenerality. arXiv:2111.11107[cs],November2021a.
Th√©ophileChampion,HowardBowman,andMarekGrzes¬¥. BranchingTimeActiveInference: Empiricalstudyand
complexityclassanalysis. arXiv:2111.11276[cs],November2021b.
O. √áatal, T. Verbelen, J. Nauta, C. D. Boom, and B. Dhoedt. Learning perception and planning with deep active
inference. InICASSP2020-2020IEEEInternationalConferenceonAcoustics, SpeechandSignalProcessing
(ICASSP),pages3952‚Äì3956,2020. doi:10.1109/ICASSP40776.2020.9054364.
RichardBellman. Dynamicprogramming. Science,153(3731):34‚Äì37,1966. doi:10.1126/science.153.3731.34. URL
https://www.science.org/doi/abs/10.1126/science.153.3731.34.
AswinPaul,NoorSajid,ManojGopalkrishnan,andAdeelRazi. Activeinferenceforstochasticcontrol. InMachine
LearningandPrinciplesandPracticeofKnowledgeDiscoveryinDatabases,pages669‚Äì680,Cham,2021.Springer
InternationalPublishing. ISBN978-3-030-93736-2. doi:https://doi.org/10.1007/978-3-030-93736-2_47.
AswinPaul,NoorSajid,LancelotDaCosta,andAdeelRazi. Onefficientcomputationinactiveinference. Expert
Systems with Applications, 253:124315, 2024b. ISSN 0957-4174. URL https://www.sciencedirect.com/
science/article/pii/S0957417424011813.
Takuya Isomura and Karl Friston. Reverse-Engineering Neural Networks to Characterize Their Cost Functions.
NeuralComputation, 32(11):2085‚Äì2121,112020. ISSN0899-7667. doi:10.1162/neco_a_01315. URLhttps:
//doi.org/10.1162/neco_a_01315.
MoeinKhajehnejad,ForoughHabibollahi,AswinPaul,AdeelRazi,andBrettJ.Kagan. Biologicalneuronscompete
withdeepreinforcementlearninginsampleefficiencyinasimulatedgameworld,2024. URLhttps://arxiv.org/
abs/2405.16946.
13
ActiveInferenceforBiologicallyPlausibleDecision-MakingModels APREPRINT
AndrewG.Barto,RichardS.Sutton,andCharlesW.Anderson. Neuronlikeadaptiveelementsthatcansolvedifficult
learning control problems. IEEE Transactions on Systems, Man, and Cybernetics, SMC-13(5):834‚Äì846, 1983.
doi:10.1109/TSMC.1983.6313077.
NoorSajid,LancelotDaCosta,ThomasParr,andKarlFriston. Activeinference,bayesianoptimaldesign,andexpected
utility. TheDriveforKnowledge: TheScienceofHumanInformationSeeking,page124,2022.
Karl J. Friston, Tommaso Salvatori, Takuya Isomura, Alexander Tschantz, Alex Kiefer, Tim Verbelen, Magnus T.
Koudahl,AswinPaul,ThomasParr,AdeelRazi,BrettJ.Kagan,ChristopherL.Buckley,andMaxwellJamesD.
Ramstead. Active inference and intentional behaviour. ArXiv, abs/2312.07547, 2023c. URL https://api.
semanticscholar.org/CorpusID:266191299.
KarlFriston. Afreeenergyprincipleforbiologicalsystems. Entropy(Basel,Switzerland),14:2100‚Äì2121,112012.
doi:10.3390/e14112100.
MahaultAlbarracin,In√™sHip√≥lito,SafaeEssafiTremblay,JasonG.Fox,GabrielRen√©,KarlFriston,andMaxwell
J.D.Ramstead. Designingexplainableartificialintelligencewithactiveinference: Aframeworkfortransparent
introspectionanddecision-making,2023. URLhttps://arxiv.org/abs/2306.04025.
ForoughHabibollahi,BrettJKagan,AnthonyNBurkitt,andChrisFrench. Criticaldynamicsariseduringstructured
informationpresentationwithinembodiedinvitroneuronalnetworks. NatureCommunications,14(1):5287,2023.
KarlJ.Friston,JeanDaunizeau,andStefanJ.Kiebel. Reinforcementlearningoractiveinference? PLOSONE,4(7):
1‚Äì13,072009. doi:10.1371/journal.pone.0006421. URLhttps://doi.org/10.1371/journal.pone.0006421.
EmreONeftciandBrunoBAverbeck. Reinforcementlearninginartificialandbiologicalsystems. NatureMachine
Intelligence,1(3):133‚Äì143,2019.
14
ActiveInferenceforBiologicallyPlausibleDecision-MakingModels APREPRINT
A AnoverviewoftheDishBrainexperimentandresults
A B
CTL / IS / RST: Baseline groups
MCC / HCC: Mice/Human cortical culture.
FigureA.1: A:Ahigh-levelschematicofthe‚ÄòDishBrain‚Äôexperiment. Liveneuronpopulationsareculturedonasilicon
chipandembodiedinasimulatedgameenvironmentof‚ÄòPong‚Äô,wheretheyreceiveinformationaboutgamestatesand
feedbacksignalstocontrolthepaddlethroughelectrophysiologicalstimulationandrecording. B:Relativeimprovement
ofgroupsinoneofthegamemetrics(averagerallylength). Weobservethatboththemousecorticalculture(‚ÄòMCC‚Äô)
andhumancorticalculture(‚ÄòHCC‚Äô)groupsdemonstrateimprovementinperformancewithtime. Theperformance
iscomparedacrosstwoblocksoftimeintheexperiment: thefirstfiveminutesandthenextfifteenminutesofthe
twenty-minutetrial. The‚ÄòHCC‚Äôgroupdemonstratedhigherrelativeperformanceimprovementthanothergroups. The
baselinegroupsarecontrol(‚ÄòCTL‚Äô),in-silico(‚ÄòIS‚Äô),andrest(‚ÄòRST‚Äô).FiguresadaptedfromKaganetal.[2022].
The‚ÄòDishBrain‚Äôsystem(Fig. A.1(A))containsactiveneuronpopulationscoupledtoasimulatedgameenvironmentof
‚ÄòPong‚Äô. Inthissetup,theneuronscontrolpaddlemovementswiththegoalofinterceptingtheball. Withnoopponent
player, theballbouncesoffthebackwall, returningtowardsthepaddleaftereachsuccessfulhit. Whentheballis
missed, thegamerestartswiththeballatarandomlocation. Thegameperformanceisevaluatedacrossthreekey
metrics: averagerallylength(longerralliesindicatingbetterperformance),percentageoflongrallies(theproportion
ofrallieslastingmorethanthreehits,higherthebetter),andpercentageofaces(instanceswheretheballismissed
immediately,withfeweracesbeingbetter). Theball‚Äôsxandy-axislocationisencodedtotheneuronsviaelectrical
stimulation,combiningplace-codedandrate-codedsignals. Throughgame-outcomedependentfeedback,learning
andimprovementintheperformanceof‚ÄòMCC‚Äôand‚ÄòHCC‚Äô6isreportedasmeasurableimprovements,particularlyin
averagerallylength,asdemonstratedinFig. A.1(B).
DishBrainisanidealplatformforstudyingtheemergenceofpurposefulbehaviour,asitdemonstratestheabilityto
learnandadapttoitsenvironmentdenovowithoutanypriorexposure. Totheoreticallyanalysethisphenomenon,we
turntoactiveinference,afirstprincipleapproachtomodellingbehaviourthatemergedinneuroscienceandwasinitially
proposedasaunifiedtheoryofbrainfunctionFriston[2010,2012]. Activeinferencepositsthatbehaviourisdrivenby
anagent‚Äôsbeliefsaboutitsenvironmentratherthanbeingaseparateprocess,asseeninframeworkslikereinforcement
learningFristonetal.[2009],NeftciandAverbeck[2019]. Thisapproachallowsustomodelthedynamicsoflearning
anddecision-makinginabiologicallyplausiblemanner,offeringadeeperunderstandingofhowsystemslikeDishBrain
exhibitadaptivebehaviour.
6‚ÄòMCC‚Äôand‚ÄòHCC‚Äô:Mouseandhumancorticalcells
15
ActiveInferenceforBiologicallyPlausibleDecision-MakingModels APREPRINT
B Additionalfigures
A B C
Figure B.1: Relative improvement in performance (Average rally length) of in-silico active inference agents
comparedtoin-vitrogroupsofinterestinthesimulatedgameenvironmentofPong. A:Performanceofthe‚ÄòAIF-1‚Äô
agentcomparedtoothergroupsacrosskeygamemetrics. TheAIF-1agentdemonstratesarelativeimprovementover
time,butitsplanning-basedapproachdoesnotyieldsignificantadvantagesinthePongenvironment,suggestingthat
memory-basedstrategiesmaybemorebeneficial. B:Performanceof‚ÄòDP-T‚Äôagentsagainstsignificantgroups. While
DP-Tagentsexhibitinitiallearning,performancegainsplateauwithincreasedplanninghorizons,andover-planning
(beyondahorizonof10)resultsindiminishedeffectiveness,likelyduetoover-planning. C:Performanceof‚ÄòCFL-T‚Äô
agentscomparedtosignificantgroups. CFLagentswithhighermemoryhorizonsconsistentlyoutperformothergroups
acrossallmetrics,reinforcingtheroleofmemoryinadaptivedecision-making. Thesefindingshighlightthetrade-offs
betweenplanning-basedandmemory-drivendecision-makingapproaches,withmemory-basedstrategiesprovingmore
effectiveinreal-timeinteractivetaskslikePong.
16
ActiveInferenceforBiologicallyPlausibleDecision-MakingModels APREPRINT
A B C
D E F
FigureB.2: Continuous-timeanalysisofmemoryhorizoneffectsonCFLagentperformance. Changesinagent
performancearetrackedovera20-minutereal-timeequivalentusinglinearregression. TheaveragenumberofA,D:
hitsperrally,B,E:percentageofaces(ballsmissedaftertheinitialserve),andC,F:percentageoflongrallies(‚â•3
consecutivehits)areanalyzedforCFLagentsandbiologicalMCCandHCCcultures. Toprow(A,B,C):CFLagents
withshortermemoryhorizonsexhibitasteady,linearimprovementacrossallgamemetrics,withCFL-4surpassing
MCCandHCCgroups. However,memoryhorizonsof4orgreatermaynotbebiologicallyplausible,asDishBrain‚Äôs
long-termmemorycapabilitiesremaintobestudied. Bottomrow(D,E,F):Agentswithlongermemoryhorizons(e.g.,
16and32)continuetoshowperformancegains,thoughsuchhorizonsareimplausibleforDishBrain-likesystems.These
resultsreinforcetheroleofmemoryindecision-makingunderactiveinferenceandprovideadynamic,time-continuous
perspectiveonlearninginembodiedsystems.
17
ActiveInferenceforBiologicallyPlausibleDecision-MakingModels APREPRINT
A B C
D E F
FigureB.3: Boxplotcomparisonofin-silicoactiveinferenceagentsandin-vitroMCCandHCCgroupsina
simulatedPongenvironment. Gameperformanceissummarizedusingboxplotsacrossthreekeymetrics: A,D:
Averagerallylength(higherisbetter),B,E:Percentageoflongrallies(higherisbetter),andC,F:Percentageofaces
(lowerisbetter). Toprow(A,B,C):Activeinferenceagentswithvaryingplanninghorizons(‚ÄòAIF-1‚Äôand‚ÄòDP-T‚Äô)
exhibitperformanceimprovementscomparabletotheHCCgroup. However,planninghorizonsexceeding10lead
toadeclineinperformance,likelyduetoover-planning. Additionally,DPEFEagentsshowconsistentperformance
acrossallmetrics,suggestingthatdeeperplanningdoesnotyieldsignificantbenefitsinthistask. Bottomrow(D,E,
F):The‚ÄòCFL-3‚Äôagentachievesthehighestperformanceacrossallmetrics,furtheremphasizingthatmemory-based
decision-makingismoreeffectivethanplanning-basedapproachesindynamicenvironmentslikePong. Boxplots
provideaclearervisualizationofvariabilityandconsistencyacrosstrials,reinforcingtheobservedtrade-offsbetween
memoryandplanninginactiveinference-basedcontrol.
18

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Simulating Biological Intelligence: Active Inference with Experiment-Informed Generative Model"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ‚ùå BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ‚úÖ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ‚ùå BAD: Repeating the same claim 3+ times with slight variations
   ‚úÖ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
