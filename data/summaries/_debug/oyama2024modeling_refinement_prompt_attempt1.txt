=== IMPORTANT: ISOLATE THIS PAPER ===
You are revising a summary for ONLY the paper below. Do NOT reference or use content from any other papers.
Paper Title: Modeling Autonomous Shifts Between Focus State and Mind-Wandering Using a Predictive-Coding-Inspired Variational RNN Model
Citation Key: oyama2024modeling
REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Issues to fix:
1. CRITICAL: The current summary has severe repetition issues. You MUST eliminate all repeated sentences, phrases, and paragraphs. Each idea should be expressed only once. If you find yourself repeating content, remove the duplicates entirely. Focus on variety and uniqueness in your wording.
2. Severe repetition detected: Same sentence appears 3 times (severe repetition)

Current draft (first 2000 chars):
=== EXTRACTED QUOTES ===1."Thecurrentstudyinvestigatespossibleneuralmechanismsunderlingautonomousshiftsbetweenfocusstateandmind-wanderingbyconductingmodel simulationexperiments. On this purpose, we modeled perception processes of continuous sensory sequences using our previous proposed variationalRNNmodelwhichwasdevelopedbasedonthefreeenergyprinciple. Thecurrentstudyextendedthismodelbyintroducinganadaptationmechanismofameta-levelparameter,referredtoasthemeta-priorw,whichregulatesthecomplexityterminthefreeenergy...."2."Mind-wanderingtendstooccurmorefrequentlyduringtasksthatareeithertooeasyortoodifficult. Whentasksarelessdemanding, suchasimplyattendingtobreathing, instancesofmind-wanderingincrease."3."Aninterestingaspectisthatthetransitionfromthefocusedstate(FS)tothemind-wanderingstate(MW)oftenhappenswithoutconsciousawareness,whereastheshiftfromMWbacktoFSinvolvesrecognizingthemind-wanderingepisodeconsciously[6]."4."Variousstudieshaveinvestigatedthepsychologicalandsystematicmechanismsunderlyingtheseshifts. Forexample,[7]arguedthatthetransitionfromFStoMWisgradual,asevidencedbyincreasingresponsetimesduring focusedtasks."5."Incontrast,[8]suggestedthattheshiftisabrupt,triggeredbysuddeninternalorexternalstimuli."6."Theauthorsstate:“Thecurrentstudyinvestigatespossibleneuralmechanismsunderlingautonomousshiftsbetweenfocusstateandmind-wanderingbyconductingmodel simulationexperiments. Thecurrentstudyextendedthismodelbyintroducinganadaptationmechanismofameta-levelparameter,referredtoasthemeta-priorw,whichregulatesthecomplexityterminthefreeenergy....”"7."Theauthorsnote:“Thecurrentstudyextendedthismodelbyintroducinganadaptationmechanismofameta-levelparameter,referredtoasthemeta-priorw,whichregulatesthecomplexityterminthefreeenergy.”"8."Theauthorsstate:“Mind-wanderingtendstooccurmorefrequentlyduringtasksthatareeithertooeasyortoodifficult. Whentasksarelessdemanding, suchasimplyattendingtobreathing, instancesofmind-wanderingincrease.”"9."Theauthorsstate:“Aninterestingaspectisthatthetr...

Key terms: model, autonomous, state, variational, inspired, okinawainstituteofscienceandtechnologygraduateuniversity, modeling, okinawa

=== FULL PAPER TEXT ===
MODELING AUTONOMOUS SHIFTS BETWEEN FOCUS STATE AND
MIND-WANDERING USING A PREDICTIVE-CODING-INSPIRED
VARIATIONAL RNN MODEL
HenriqueOyama
CognitiveNeuroroboticsResearchUnit
OkinawaInstituteofScienceandTechnologyGraduateUniversity
Okinawa
henrique.oyama@oist.jp
JunTani
CognitiveNeuroroboticsResearchUnit
OkinawaInstituteofScienceandTechnologyGraduateUniversity
Okinawa
jun.tani@oist.jp
ABSTRACT
Thecurrentstudyinvestigatespossibleneuralmechanismsunderlingautonomousshiftsbetween
focus state and mind-wandering by conducting model simulation experiments. On this purpose,
we modeled perception processes of continuous sensory sequences using our previous proposed
variationalRNNmodelwhichwasdevelopedbasedonthefreeenergyprinciple. Thecurrentstudy
extendedthismodelbyintroducinganadaptationmechanismofameta-levelparameter,referredtoas
themeta-priorw,whichregulatesthecomplexityterminthefreeenergy. Oursimulationexperiments
demonstratedthatautonomousshiftsbetweenfocusedperceptionandmind-wanderingtakeplace
whenwswitchesbetweenlowandhighvaluesassociatedwithdecreaseandincreaseoftheaverage
reconstructionerroroverthepastwindow. Inparticular,highwprioritizedtop-downpredictions
whilelowwemphasizedbottom-upsensations. Thispaperexploreshowourexperimentresultsalign
withexistingstudiesandhighlightstheirpotentialforfutureresearch.
Keywords mind-wandering·predictivecoding·freeenergyprinciple·variationalRNN
1 Introduction
Duringmindfulnesspractice,suchasfocusingonsensationslikebreathing,ourattentionsometimesspontaneously
deviatestomentalimageryorthoughtsaboutthepastandfuture,aphenomenonknownasmind-wandering[1,2,3].
Thisshiftfromafocusedstatetomind-wanderingcanoccurnotonlyduringmeditationbutalsoineverydayactivities,
suchasdriving,listeningtomusic,ortastingfood.
Mind-wanderingtendstooccurmorefrequentlyduringtasksthatareeithertooeasyortoodifficult. Whentasksare
lessdemanding, suchassimplyattendingtobreathing, instancesofmind-wanderingincrease. Conversely, during
morechallengingtasks,likereadingcomplexmaterial,ourmindsaremorepronetowanderbecausemaintainingfocus
becomesdifficultoverextendedperiods[4,5].
Aninterestingaspectisthatthetransitionfromthefocusedstate(FS)tothemind-wanderingstate(MW)oftenhappens
withoutconsciousawareness,whereastheshiftfromMWbacktoFSinvolvesrecognizingthemind-wanderingepisode
consciously[6]. Variousstudieshaveinvestigatedthepsychologicalandsystematicmechanismsunderlyingtheseshifts.
Forexample,[7]arguedthatthetransitionfromFStoMWisgradual,asevidencedbyincreasingresponsetimesduring
focusedtasks. Incontrast,[8]suggestedthattheshiftisabrupt,triggeredbysuddeninternalorexternalstimuli.
4202
ceD
02
]CN.oib-q[
1v02651.2142:viXra
[9]proposedamodelwherementalstatesalternatebetweenFSandMW,withMWepisodesendingwhenindividuals
consciouslyrecognizetheirmind-wanderingandreturntothetask. This“two-stagemodel”assumesthattheprobability
ofbeinginFSishigheratthebeginningofanepisodeanddecreasesovertime. However,contrarytothisprediction,
[10]foundthattheprobabilityofFSdoesnotdeclinewithinanFS-MWepisodeinasubjectstudyintroducingaprobein
arandomtimingduringtheepisode. Toaddressthisdiscrepancy,theauthorsproposedthe“multiplesub-eventmodel”,
which hypothesizes that unconscious alternations between FS and MW occur multiple times before an individual
becomesawareofbeinginMW.Theirsimulationstudysuggestedthatasthenumberofsub-sequencesincreases,the
declineintheprobabilityofFSbecomeslesspronounced.
Although the above mentioned studies clarified some phenomena in the shift from FS to MW from psychological
observation,theyhavenotprovidedsufficientaccountsfortheunderlyingneuronalmechanisms. Recently,somestudies
[11,12]suggestedsystemlevelneurosciencemodelsincorporatedwiththeconceptofthefreeenergyprinciple(FEP)
[13]. Here,FEPisbrieflyexplainedforbetterunderstandingofthereaders. TheFEPisaneurosciencetheorythat
has attracted large attention. The FEP posits that humans and animals execute various functions such as learning,
perception,andactiongenerationtomaximizetheirchancesofsurvivalbyminimizingsurprisestheyencounterduring
interactionwiththeenvironment. AccordingtotheFEP,thesefunctionsareachievedbyoptimizinggenerativemodels
forpredictingthesensation,wherebyacommonstatisticalquantitycalledfreeenergyisminimized. TheFEPsupports
twoframeworks,oneispredictivecodingandtheotherisactiveinference. Predictivecodingprovidesaformalism
accountingforhowagentsperceivesensations. Itsuggeststhatthebrainpredictssensoryobservationsinthetop-down
pathway,whileatthesametimeupdatingposteriorbeliefsaboutthosesensationsinthebottom-uppathwaywhenever
errors arise between predictions and observations [14, 13, 15]. By updating posterior beliefs in the direction of
minimizingerrors,perceptualinferencefortheobservedsensationcanbeachieved. Ontheotherhand,activeinference
(AIF)providesatheoryforactiongenerationbyassumingthatthebrainisembodieddeeplyandembeddedinthe
environment,suchthatactingonitchangesfuturesensoryobservation. Then,AIFconsidersthatactionsshouldbe
selectedsuchthattheerrorbetweenthedesiredandpredictedsensationscanbeminimized[16,17].
[11]posturestheunderlyingmechanismofshiftfromFStoMWusingactiveinferenceof“mentalaction”intermsof
attentionchanges. Theproposedmodelassumeshierarchicalprobabilisticgenerativemodelwhereinthehiddenmeta-
awarenessstatesinthehigherlevelaccountfor"howawareamIofwheremyattentionsis?",thehiddenmentalstates
inthemiddleleveldealingwithfocusofattentionaccountfor"whatamIpayingattentionto?",andthesensorimotor
hiddenstatesinthelowestleveldofor"whatamIperceivingortryingtodo?"accordingtotheauthors. Thestatesat
eachlevelconditiontheonesinthenextlowerlevelbycontrollingtheirprecisionsorbeliefs. Agent’sperceptualand
attentionalstatesareinferredateachtimestepbymeansofactiveinferenceinminimizingtheexpectedfreeenergy.
Theresultsofsimulationexperimentsshowthatwhenthemeta-awarenessstateismanuallyshiftedfromhightolow,
distractedorMWstateisdevelopedmorefrequently. Underthiscondition,redirectionbacktoFSbyconsciouslybeing
awareofthecurrentMWstatetendstotakemoretimebecauseoflessprecisionintheattentiontowarddistractedstate.
[12]investigatedmind-wanderingmechanismbyconductingamodelsimulationstudyonallostasisusinghierarchically
organized variation recurrent neural network, so-called the PV-RNN [18]. Dynamic behavior of PV-RNN can be
characterizedbyameta-levelparameter,referredtoasmeta-priorw,thatregulatesthecomplexitytermagainstthe
accuracyterminfreeenergywhichisminimizedintheinferenceoftheposteriorprobabilitydistributionofthelatent
variables. Itwasshownthathighsettingofmeta-priorw enhancesgenerationofthetop-downimagerywhilelow
settingofitenhancesthebottom-upsensoryperception[19,20,21]. Analogoustothis,[12]showedthatlowsettingof
wgeneratesstrongersensorybottom-upwhichleadstoFSwhereinlesschangeinmovementaswellasneuralactivity
areobserved. Ontheotherhand,highsettingofwgeneratesweakattentiontosensationandstrongertop-downwhich
leadstoMWwhereinmoremovementaswellasneuralactivity.
The aforementioned FEP-based studies provide valuable insights into macroscopic neural mechanisms, such as
redirectingattentiontofocusedstatesbyinferringone’sattentionalstate,orgeneratingmind-wanderingbybalancing
top-downandbottom-upinformationflows. However,thesestudiesdonotprovidesystematicexplanationsforhow
theshiftsbetweenFSandMWcouldbeautonomouslygenerated,sincetheshiftfromFStoMWin[11]iscausedby
manualchangeofthemeta-awarestatefromhightolowandtheonein[12]doesthisbyresettingmeta-priorwfrom
lowtohighvalue.
Inthisregard,thecurrentstudyspeculatesthatautonomoustransitionbetweenFSandMWcouldbegeneratedby
introducinganadaptationmechanismofmeta-priorwtoPV-RNNinwhichwismodulatedwithresponsetosome
macroscopicvariablessuchasanaveragepredictionerror. Inourstudy,PV-RNNlearnstopredictatargetsequenceof
continuouslychangingsensorypatternswhichisgeneratedbymeansofpredeterminedprobabilistictransitionsamong
asetofcyclicpatterns. Inthetestphaseafterthetraining,givenoneofthepre-trainedcyclicpatternsasthetarget
inputs,thePV-RNNpredictsencounteringsensoryinputsbysimultaneouslyinferringtheapproximatedposteriorofthe
latentstateateachtimestepbyminimizingthefreeenergywhileadaptingw. Analogoustostudies[19,20,21,12],
2
whenwmodulatestoalowervaluebyreflectingsurgeoftheaveragereconstructionerror,theinferenceprocessmay
improvebyplacinggreateremphasisonbottom-upsensations. ThissituationmaycorrespondtoFS.Ontheother
hand,whenwmodulatestoahighervaluebyrespondingtodeclineoftheaveragereconstructionerror,thePV-RNN
maygeneratetop-downimagerybyfollowingthelearnedprobabilistictransitionsofpatternswhileignoringthetarget
sensoryinputs. ThismaycorrespondtoMW.OursimulationstudywithPV-RNNundervariousparametersettingswill
evaluatethishypothesis. Thefollowingsectionintroducestheproposedmodel,followedbyadetaileddescriptionofthe
simulationexperimentsetup,thepresentationoftheresults,andadiscussionthatincludesproposalsforextensionsto
futureresearchwork.
2 MaterialsandMethods
2.1 Overview
Thisstudyinvestigatesautonomousshiftsbetweenthefocusedstate(FS)andmind-wandering(MW)duringaperception
taskusingsequentialsensoryinputpatterns. Thepredictivecodingframeworkisemployedtomodelthisperception
process. Predictivecodingassumesagenerativemodelthatpredictssensorysequencesbylearningboththelatentstate
transitionfunctionandthelikelihoodmappingfromlatentstatestosensoryobservations. Additionally,thisgenerative
modelinfersthecurrentlatentstatethroughcontinuoussensorysequenceobservations.
Bothlearningandinferenceprocessesareachievedbyminimizingpredictionerroror,morespecifically,freeenergy. We
hypothesizethatFSisenhancedbystrengtheningbottom-upinference,whileMWbecomesmorelikelybyemphasizing
top-downsensorypatterngeneration. ItisalsohypothesizedthatshiftsbetweenFSandMWtakeplaceautonomously
incorporatingwithadaptationofmeta-levelstateswithresponsetoparticularsystemvariables. Totestthis,wepropose
anextendedversionofavariationalrecurrentneuralnetworkmodel, referredtoasthePredictiveCodingInspired
VariationalRNN(PV-RNN)[18]. DetailsoftheoriginalPV-RNNanditsextensionsareprovidedinthefollowing
sections.
2.2 PredictiveCodingInspiredVariationalRNNModel(PV-RNN)
ThePV-RNNisbasedonthefreeenergyprinciple[13],wherelearningandinferenceareachievedbyminimizingfree
energy(Equation1)inaccordancewithBayes’theorem:
F =D [q (z|X)∥p (z)]−E [logp (X|z)]
(cid:124)
KL ϕ
(cid:123)(cid:122)
θ
(cid:125) (cid:124)
qϕ(z|X)
(cid:123)(cid:122)
θ
(cid:125) (1)
complexity accuracy
Here,p (X)isthemarginallikelihoodofthesensoryobservationX,giventhegenerativemodelp parameterizedbyθ.
θ θ
Thelatentvariableszandinferencemodelq ,parameterizedbyϕ,allowforposteriorinferencethroughminimization
ϕ
offreeenergy. Freeenergyconsistsoftwoterms: thecomplexityterm(ameasureofdivergencebetweenpriorand
posteriordistributions)andtheaccuracyterm(log-likelihoodofsensoryobservations)[22]. PV-RNNservesasbotha
generativemodelandaninferencemodel. Thegenerativemodelpredictsfuturesensoryinputsviatop-downprocesses,
whiletheinferencemodelestimatestheapproximateposteriorfromobservedsensorysequencesthroughfreeenergy
minimizationasbottom-upprocesses.
ThefollowingsubsectionsdescribethePV-RNNimplementationandtheuseofthemeta-priorw.
2.2.1 ModelImplementation
ThefreeenergyF˜forPV-RNNpredictingatimeseriesofT stepsisgivenby:
T
F =w (cid:88) E (cid:2) D [q (z |d ,X )∥p (z |d )] (cid:3)
qϕ(z1:t−1|dt−1,Xt−1:T) KL ϕ t t−1 t:T θ t t−1
t=1
(cid:124) (cid:123)(cid:122) (cid:125)
complexity
(2)
T
(cid:88)
− E [logp (X |d )]
qϕ(z1:t−1|dt−1,Xt:T) θ t t
t=1
(cid:124) (cid:123)(cid:122) (cid:125)
accuracy
3
PV-RNNintroducestwotypesoflatentvariables: probabilisticlatentvariables(z)governedbyGaussiandistributions,
anddeterministiclatentvariables(d). TheirrelationshipsareshowninFigure1. Inequation2,ameta-levelparameter,
namedmeta-priorw,isintroducedtobalancethecomplexityandaccuracytermsduringthisprocess. Thisregulation
is particularly important when the limited amount of training data prevents reliable estimation of latent variable
distributions. Also,dynamicbehaviorofPV-RNNislargelyaffectedbysettingofthemeta-prior. Itwasshownthathigh
settingofmeta-priorwenhancesgenerationofthetop-downimagerywhilelowsettingofitenhancesthebottom-up
sensoryperception[19,20,21,12].
Figure1: Ahierarchicaltwo-layerPV-RNNarchitecture. Solidbluelinesrepresentthegenerativeprocess,whiledotted
redlinesindicatetheinferenceprocess. Theshadedareashowsaninferencewindowoflength3.
Next,theforwardcomputationofeachvariableusedinPVRNNisdescribed. Ateachtimestept,theinternalstatesof
thel-thlayer(hl)arerecursivelycomputed:
t
(cid:18) 1 (cid:19) 1 (cid:16) (cid:17)
hl = 1− hl + Wll d˜l +Wll zl +Wll+1d˜l+1 +Wll−1d˜l−1 +bl
t τl t−1 τl dd t−1 zd t dd t−1 dd t−1 h (3)
d˜l =tanh(hl)
t t
ThePV-RNNstructuresupportshierarchicalinformationprocessingusingtimeconstantsτl,enablingthedifferentiation
oftemporaldynamicsacrosslayers[23,24].
4
Thegenerativemodelcomputespriordistributions(zp)asGaussianvariablesparameterizedbymean(µp)andstandard
t t
deviation(σp):
t
µp =tanh(Wll d˜ +bp)
t dµ t−1 µ
σp =exp(Wll d˜ +bp) (4)
t dσ t−1 σ
zp =µp+σp∗ϵ withϵ ∼N(0,I)
t t t t t
bp andbp arebiastermsforµpandσp,respectively. ϵrepresentsanoisesampledfromastandardnormaldistribution
µ σ t t
forusageofthereparameterizationtrick[25]. Analogoustothecomputationofthepriordistribution,theinference
modelq approximatestheposteriorzq asaGaussiandistributionwithmeanµq andstandarddeviationσq.
ϕ t t t
µq =tanh(Wll d˜ +Aµ+bq)
t dµ t−1 t µ
σq =exp(Wll d˜ +Aσ+bq) (5)
t dσ t−1 t σ
zq =µq+σq∗ϵ withϵ ∼N(0,I)
t t t t t
where bq and bq are bias terms for computing µq and σq, respectively. Aµ and Aσ are adaptive variables to be
µ σ t t t t
optimizedforinferringtheposteriordistributionwhichisparameterizedbyµq andσq.
t t
Intuitively,therandomvariablezpcanberegardedasatime-dependentprior/top-downexpectationabouttheencounter-
ingsensation. TheadaptivevectorA(i.e.,zq)canberegardedastheapproximateposteriordistributionthatmayor
maynotbeclosetothepriordistribution,dependingonthesettingofmeta-prior. zpandzq areusedbythegenerative
andinferencemodel,respectivelytocomputethelatentvariabled.
2.2.2 LearningandInference
ThefreeenergyF ofPV-RNNcanbecomputedasfollowsbyadaptingtheoriginalequation2. GivenaPV-RNNwith
Llayers,predictingaT timeseriessensoryinputs,F canbewrittenas
T (cid:34) L (cid:35) T
F = (cid:88) (cid:88) w˜lD [q (zl|dl ,X )∥p (zl|dl )] − (cid:88) ∥X −X¯ ∥2 (6)
KL ϕ t t−1 t:T θ t t−1 t t 2
t=1 l=1 t=1
wherew˜l iswspecifictolthlayer,andX¯ denotesthepredictionoutputofthePV-RNN.Inequation6,weapproximate
theexpectationwithrespecttotheapproximateposteriorbyiterativesampling. Also,theaccuracytermisreplacedby
thesquarederror,whichcanberegardedaspecialcaseofcomputationoflog-likelihoodwhereineachdimensionofX
andX¯ isindependentandfollowsaGaussiandistributionwithstandarddeviation1. SincetheKullback-Leibler(KL)
divergencebetweentwoone-dimensionalGaussiandistributionstakesasimpleexpression,equation6isreducedto
(cid:88) T (cid:34) (cid:88) L (cid:88) R z l (cid:35) (cid:88) T
F = w˜l δ(l,r,t) − ∥X −X¯ ∥2 (7)
t t 2
t=1 l=1 r=1 t=1
where
σp,l,r (µq,l,r−µp,l,r)2+(σq,l,r)2 1
δ(l,r,t)=log t + t t t − (8)
σq,l,r 2(σp,l,r)2 2
t t
µp,l,r representsrthelementofµl oftheprior,andthesamenotationisappliedtoµq,l,r,σp,l,r,andσq,l,r. Rl denotes
t t t t t z
the dimension of zl. Given that the complexity term is summed over all the dimension of z, which is arbitrary to
t
thenetworkdesign,andtheaccuracytermistoallthedatadimension,whichvariesamongdata,thefreeenergyis
normalizedwithrespecttothedimensionofzandthedatadimension. Therefore,introducingsuchnormalization,the
freeenergyofPV-RNNinthestudyiscomputedby
F = (cid:88) T (cid:34) (cid:88) L wl δ(l,r,t) (cid:35) − 1 (cid:34) (cid:88) T ∥X −X¯ ∥2 (cid:35)
t=1 l R z l R X t=1 t t 2 (9)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
complexity accuracy
whereR isthedatadimension,Rl isthenumberofzvariablesineachlayer,andwl =Rlw˜l.
X z z
Byminimizingequation9,theposteriorinferenceisperformedduringnetworklearningandduringtheperception
task. Figure1showsaschematicillustrationoftheposteriorinferenceprocessofatwo-layerPV-RNNmodelusedin
thecurrentsimulationwithanoptimizationwindowofthreetimesteps. Ateverysensorystep,anadaptivevariable
5
Ainthewindowisoptimizedthroughmultipleepochsofstochasticgradientdescent. Inthenetworklearningphase,
weightsandbiasparametersθandϕofthegenerativeandinferencemodels,includinganadaptivevariableAforthe
approximateposteriorzq arejointlyoptimized. Intheperceptiontaskphase,networkparametersθandϕarefixed,and
freeenergyisminimizedateachtimestepwithinadedicatedinferencewindowbyoptimizingonlyAparameterizing
theapproximateposterior.
2.2.3 AdaptationofMeta-Prior
The meta-prior w is dynamically adapted based on the average prediction error (er ) over a fixed length time
sum
windowinthepast. Whentheerrordecreasesbelowapredefinedthreshold(Thr ),wtransitionstoahighvalue(wH),
L
prioritizingtop-downgeneration,whichleadstogeneratingMW.Thiscanbeintuitivelyunderstoodfromanalogythat
continuingeasyorpredictabletaskstendstoinitiateMW[5,4]. Conversely,whentheaveragepredictionerrorexceeds
anupperthreshold(Thr ),wtransitionstoalowvalue(wL),enhancingbottom-upinference. Theimplementation
H
strategy for autonomous meta-prior switching between FS and MW is described in Algorithm 1. Specifically, the
probabilisticshiftingbetweenthetwomodesisgivenbyequations10-11,whereTempisthetemperature,atunable
parameterthatcanreflecthowstochasticordeterministicthesystemis(seeSection3.2). Itishighlyspeculatedthatthis
dynamicadaptationshouldenableautonomoustransitionsbetweenFSandMW,aswillbevalidatedinthesimulation
experimentsdetailedinsubsequentsections.
Algorithm1AutonomousMeta-PriorSwitchingBetweenFocusState(FS)andMind-Wandering(MW)
1: Initializemeta-priorw(eitherwLorwH)
2: ifw==wLthen
3: ComputetransitionprobabilityfromFStoMW:
(cid:18) (cid:19)
−(er −Thr )
P(FS →MW)=sigmoid sum L (10)
Temp
4: Generaterandomnumberr ∼G(0,1)
5: ifr <P(FS →MW)then
6: Setmeta-priortow←wH
7: endif
8: elseifw==wH then
9: ComputetransitionprobabilityfromMWtoFS:
(cid:18) (cid:19)
er −Thr
P(MW →FS)=sigmoid sum H (11)
Temp
10: Generaterandomnumberr ∼G(0,1)
11: ifr <P(MW →FS)then
12: Setmeta-priortow←wL
13: endif
14: endif
3 ExperimentsandResults
3.1 ModelTraining
First,wetrainedaPV-RNNwith2-dimensionalsensorysequencedata. Thetrainingdatacomprised80sequences,each
containing2160timesteps. Forpreparingthosetrajectories,wedesigned2different2-dimensionalcyclicpatterns,
onewithperiodicityof40timestepsandtheotherwithperiodicityof27timesteps. Eachtrajectorywasmadeof
probabilisticswitchingamongthese2cyclicpatternswhereinafteronecycleofaparticularpatternthesamepattern
repeatswithaprobabilityof60%andthepatterntransitstotheotherpatternwithaprobabilityof40%equally. Noise
hasbeenaddedtoindividualpointsatrandomlyspacedintervals. Theintervalsbetweennoisepointsaredetermined
bydrawingfromanormaldistribution(meanof1,standarddeviationof10),providingavariabletimestepsize. At
eachnoiseinterval,Gaussiannoise(meanof0,standarddeviationof0.001)isaddedtothecurrentdatapoint,slightly
perturbingitscoordinatestosimulatenaturalfluctuationswithoutdisruptingthecyclicstructure. Apartofthetraining
trajectoryisshowninFigure2.
6
Figure 2: Training trajectory over 400 time steps (top plot) and its representation in X −Y space (bottom plot).
Target−X andTarget−Y correspondtothefirstandseconddimensionsofthetrainingtrajectory,respectively.
ThenetworkparametersusedfortrainingPV-RNNarelistedinTable1. #d,#z,τ,wtr indicatesthenumberofd
neurons,numberofzneurons,timeconstant,andmeta-priorduringthetrainingphase,respectively.
Table1: PV-RNNtrainingparameters.
#d #z τ wtr
Layer1 30 2 1 0.001
Layer2 15 1 5 0.01
ThePV-RNNwastrainedover130,000epochsminimizingfreeenergyinEquation9usingtheAdamoptimizer[26]
andback-propagationthroughtime(BPTT)[27]withlearningrate0.001tooptimizeallnetworkparametersofθandϕ
ofthegenerativeandinferencemodel,andtheadaptivevariableAcorrespondingtoeachtrainingtrajectory.
Thetrainednetworkwasevaluatedonthebasisofhowwellprobabilistictransitionsinthetrainingdatawerereflectedin
thePV-RNNgenerativeprocess,theso-calledpriorgenerationofthePV-RNN,whichisconductedwithoutperforming
theinferenceofthelatentvariableswithsensoryobservation.Inpriorgeneration,thepriordistributionzpwasinitialized
1
withaunitGaussian(Equation4)andthenlatentstateswererecursivelycomputedtogeneratenetworkoutputsequences.
Figure3showsanexampleofthepriorgenerationoutputsover1200timesteps. Wecanseethatthepatternsshift
fromonetoanother, wherethetwopatternsusedfortrainingappearrandomly. Inaddition, usingacategorizerto
discriminatebetweenthetwopatterns,differentpriorgenerationoutputsover50,000timestepshaveshownaprobability
of38%-43%ofswitchingtoadifferentpatternandaprobabilityof57%-62%ofstayinginthesamepattern,whichare
closetothetrainingdataset.
7
Figure 3: Prior generation over 1200 time steps under trained model with meta-prior wtr from Table 1 (top plot),
selectedactivitiesofthedneuronsinthebottomlayerofthePV-RNN(middleplot),andarepresentationinX−Y
space(bottomplot).Output−XandOutput−Y correspondtothefirstandseconddimensionsofthepriorgeneration
outputtrajectory,respectively.
3.2 Testingofperceptiontask
ThetrainedPV-RNNwastestedbyperformingtheperceptiontask. Inthetest,theinferenceprocesswasperformed
withintheinferencewindow,whileoneofthetrainedpatternswasusedasthetargetsensorysequencefortheinference
ofthelatentvariables. Thelengthoftheinferencewindowwassetto400timesteps. Theadaptationofmeta-prior,w,
duringinferencewiththemonitoringoftheaveragepredictionerrorover300timesteps1 wascarriedoutusingthe
parameterslistedinTable2.
Table2: PV-RNNtestingparameters.
wL wH Temp Thr Thr
L H
Layer1 0.001 100 0.01 0.1 0.5
Layer2 0.01 1000
ThemechanisticbehaviorwhenwadaptedtolowandhighvaluesareshowninFigures4and5,respectively. Theplots
showtheoutputtrajectory,thetargetsensorysequence,theaveragepredictionerror,andKLdivergenceatthePV-RNN
1FortheimplementationstrategydescribedinAlgorithm1,thelengthoftheinferencewindowandtimewindowforcomputing
theaveragereconstructionerrorcanbeconsidereddesigndecisionsanddonotneedtobethesamelength.Thischoicemaydepend
onhowthehighandlowthresholdsaredefined,whichimpacttheprobabilityoftransitionfromFStoMW(andvice-versa)and,
thus,theexpectedprobabilisticbehaviorofthesystem.Forinstance,anaveragereconstructionerrorcomputedoverasmalltime
windowmaynotreachormaybetoofarfromadesiredthreshold.Inthisscenario,theprobabilityofstayinginthecurrentstate(FS
orMW)wouldremainlargeovertheentiresimulationtime.
8
bottomlayerforeachcase. ItcanbeseeninFigure4thatwhenwadaptedtowL,apatternusedforthetargetsensory
sequenceisgeneratedwellduringinferencewhiletheaveragepredictionerrorremainslow(below0.03)overtheentire
inferencewindow. ThisindicatesthatadaptationofwtowL enabledtheoutputtoaccuratelyreconstructthetarget
sensorysequence. ThisperiodisanalogoustoasituationofFS.
Ontheotherhand,Figure5demonstratesaperiodwhenwadaptedtowH. Inthisperiod,theinferencetrajectoryis
generatedsimilarlytothepriorgenerationshownin(Figure3). Inparticular,wecanobserveinFigure5thatafterafew
cyclesofonepattern,theinferencetrajectorygeneratestheotherpattern,returnstothepreviouspattern(whichisout
ofphasefromthetargetsensorysequenceduetothedifferentperiodicitybetweenthetwocyclicpatterns),andthen
switchesagaintotheotherpattern. Asaresult,theaveragereconstructionerrorincreasesoncetheinferencetrajectory
startstodeviatefromthetargetsensorysequence. ThisobservationisanalogoustoasituationofMW.
Figure4: Fromtoptobottom: inferenceoutputtrajectorywithmeta-priorwLfromTable2,selectedactivitiesofthe
dneuronsinthebottomlayerofthePV-RNN,averagereconstructionserrorovertheinferencewindowattimestep
542,andKLdivergenceatthePV-RNNbottomlayer. Inference−X andInference−Y correspondtothefirstand
seconddimensionsoftheinferenceoutputtrajectory,respectively.
9
Figure5: Fromtoptobottom: inferenceoutputtrajectorywithmeta-priorwH fromTable2,selectedactivitiesofthe
dneuronsinthebottomlayerofthePV-RNN,averagereconstructionserrorovertheinferencewindowattimestep
283,andKLdivergenceatthePV-RNNbottomlayer. Inference−X andInference−Y correspondtothefirstand
seconddimensionsoftheinferenceoutputtrajectory,respectively.
SelecteddactivitiesofthePV-RNNbottomlayerduringpriorgenerationaftertraining,aswellasduringinferencewith
adaptationofwtolowandhighvalues,areshowninFigures3-5. Inbothcasesoftheinference,thecorrespondence
betweendactivitypatternsandtheoutputpatternsisanalogoustothatobservedduringpriorgenerationaftertraining.
Specifically,thedactivitiesfollowasinglepatternwhenwadaptedtothelowvalue,whilethedactivitiesalternate
betweentwopatterns,closelyreflectingthedynamicsofthedactivitiesseeninthepriorgenerationwhenwadaptedto
thehighvalue.
Figure6showstheoverallbehaviorofautonomousshiftsbetweentwodistinctperiodsobtainedintheexperiments.
Theplotsshowtheaveragereconstructionerrorduringinferenceandmeta-priorvaluesofthePV-RNNbottomlayer
overtime. ItcanbeobservedthatwhenPV-RNNisunderwH,theaveragepredictionerrorincreasesasclosetothe
highthresholdvalue,whichmakestheprobabilityofswitchingfromwH towLlargeraccordingtoequation11. Then,
10
wisswitchedtothelowvalue(wL). Afterthisshift,theaveragepredictionerrorcontinuestodeclineuntilitbecomes
closetothelowthresholdvalue,whichincreasestheprobabilityofswitchingfromwLtowH. wisthenswitchedback
tothehighvalue. TheformercasecorrespondstotheshiftfromMWtoFSandthelattercasecorrespondstotheshift
fromFStoMW.
Finally, we investigated the effect of changing temperature values on the characteristics of the shifts between FS
and MW. For this purpose, we counted the number of transitions occurred from FS to MW during 1000 steps in
theperceptiontest. TheresultsareshowninFigure7. ItcanbeseenthatthetransitionfrequencyfromFStoMW
increaseswhenthetemperatureincreases. Inparticular,forlargertemperaturevalues,thetransitionsfromFStoMW
becomemorefrequent(i.e.,thesystembecomesmorerandom)sincetheprobabilityofswitchingfromFStoMW
becomescloserto50%duetotheargumentinsidethesigmoidfunctionbeingclosertozeroinequation10. Incontrast,
whenthetemperatureissmaller,thetransitionsfromFStoMWbecomelessfrequent(i.e.,thesystembecomesmore
deterministic),whichprimarilyhappenwhentheaveragereconstructionerrorreachesthelowthreshold. Forthecase
studyinFigure6,0.01waschosentobethetemperaturewithameanof1.23transitionsfromFStoMWper1000time
steps.
Figure6: Reconstructionerroroverinferencewindowcomputedateachtimestep(topplot)andadaptivemeta-prior
value(w)ofthePV-RNNbottomlayerovertime(bottomplot).
11
Figure7: TransitionfrequencyfromFStoMWper1000timestepsunderdifferenttemperaturevalues. Themeanand
standarddeviationaredisplayedforthreeintermediatecaseswhentemperatureis0.01,0.10,and0.50.
4 Discussion
Thisstudyexploredtheneuralmechanismsunderlyingautonomousshiftsbetweenthefocusedstate(FS)andmind-
wandering(MW)throughsimulationexperimentsusinganewlyproposedmodelbasedonthefreeenergyprinciple.
Theproposedmodel,anextensionofPV-RNN,introducesanadaptationmechanismforameta-levelparameter,the
meta-priorw,whichismodulatedbasedontheaveragereconstructionerroroverafixed-sizepastwindow. Specifically,
wprobabilisticallyswitchestoahighvaluewhentheaveragereconstructionerrordecreasesclosetoaminimalthreshold
andtoalowvaluewhentheaveragereconstructionerrorincreasesnearamaximalthreshold.
Inthesimulationexperiments,thePV-RNNwasfirsttrainedtogenerateprobabilistictransitionsbetweentwodistinct
cyclicpatterns. Intheperceptiontaskphase,latentvariableswithintheinferencewindowwereinferredtominimizethe
reconstructionerrorforgiventargetsensorysequencewhileadaptingw. Oneofthetrainedcyclicpatternswasusedas
thetarget.
Whenwshiftedtoalowvalue,strongerbottom-upsensoryperceptiondominated,regeneratingtheobservedsensory
sequenceintheoutputswithminimalreconstructionerrorwhileallowinglargerKullback-Leiblerdivergencebetween
thepriorandtheapproximateposterior. Thisleadstoafocusedstate. Conversely,whenwshiftedtoahighvalue,the
approximatedposteriorisattractedtowardthepriorbystrongermeanofminimizingtheKullback-Leiblerdivergence
betweenthepriorandtheapproximatedposterior. Thisallowedstrongertop-downprocessingwhilelessattendingto
sensation,generatingrelativelylargereconstructionerrorintheinferencewindow. Thisresultsinastateresembling
mind-wandering.
One limitation of the current study is that the proposed model does not account for the phenomenon of becoming
consciouslyawareofMW,whichenablesredirectionofattentionbacktoFS.[11]hypothesizethatinferringa"true
meta-state" by asking, "How aware am I of where my attention is?" could trigger self-awareness of MW. While
thedynamicallychangingmeta-priorinthecurrentmodelmodulatesthebalancebetweentop-downandbottom-up
informationflow,leadingtoshiftsbetweenFSandMW,itmaycorrespondtothemeta-stateproposedin[11]. However,
thecurrentmodellacksamechanismforexplicitlyinferringsuchameta-state,makingitunabletoaccountforself-
12
awarenessofit. Futurestudiesshouldaddressthislimitationbyextendingthemodeltoincludeaninferencemechanism
forameta-state.
Howdothecurrentresultsrelatetoeitherthetwo-stagemodel[9]orthemultiplesub-eventmodel[10]described
previously?Thetwo-stagemodelsuggeststhattheprobabilityofremaininginFSdecreasesovertimeduringanFS-MW
episode,whichconcludeswithconsciousawarenessofMW.Incontrast,themultiplesub-eventmodelpositsalesser
decreaseinthisprobability,speculatingthatmultipleunconsciousshiftsbetweenFSandMWoccurbeforeMWis
consciouslynoticed. Sincethecurrentmodeldoesnotaccountforself-awarenessofMW,asdiscussedearlier,itis
challengingtodirectlyalignitsresultswitheitherofthesemodels.
Finally, numerous studies have indicated that MW during the resting state is intricately linked to the functional
organizationanddynamicsofbrainnetworks,particularlythedefaultnetwork(DN),centralexecutivenetwork(CEN),
andsaliencenetwork(SN)[28,29,30].Thecurrentstudydoesnotmodelinteractionsbetweensuchdistinctnetworks.
Extendingthemodeltoincorporatedynamicinteractionsamongthesenetworkswouldprovideatighterconnectionto
establishedneuroscientificfindingsonresting-statephenomenaandofferdeeperinsightsintomind-wandering.
References
[1] JonathanSmallwoodandJonathanWSchooler. Thescienceofmindwandering: Empiricallynavigatingthe
streamofconsciousness. Annualreviewofpsychology,66(1):487–518,2015.
[2] KalinaChristoff,ZacharyCIrving,KieranCRFox,RNathanSpreng,andJessicaRAndrews-Hanna. Mind-
wanderingasspontaneousthought: adynamicframework. Naturereviewsneuroscience,17(11):718–731,2016.
[3] PaulSeli,EvanFRisko,DanielSmilek,andDanielLSchacter. Mind-wanderingwithandwithoutintention.
Trendsincognitivesciences,20(8):605–617,2016.
[4] PaulSeli,MahikoKonishi,EvanFRisko,andDanielSmilek. Theroleoftaskdifficultyintheoreticalaccountsof
mindwandering. ConsciousnessandCognition,65:255–262,2018.
[5] CatarinaIPeral-Fuster,RhiannonSHerold,OliverJAlder,OmarElkelani,SaraIRibeiro-Ali,EleanorMDeane,
AlexanderPLMartindale,ZiqiaoQi,CarinaEIWestling,andHarryJWitchel. Intentionalmindwanderingis
objectivelylinkedtoloweffortandtaskswithhighpredictability. InProceedingsoftheEuropeanConferenceon
CognitiveErgonomics2023,pages1–8,2023.
[6] JonathanSmallwoodandJessicaAndrews-Hanna.Notallmindsthatwanderarelost:theimportanceofabalanced
perspectiveonthemind-wanderingstate. Frontiersinpsychology,4:441,2013.
[7] Rodrigo A Henríquez, Ana B Chica, Pablo Billeke, and Paolo Bartolomeo. Fluctuating minds: spontaneous
psychophysicalvariabilityduringmind-wandering. PLoSOne,11(2):e0147174,2016.
[8] DavidRVagoandFadelZeidan. Thebrainonsilent: mindwandering,mindfulawareness,andstatesofmental
tranquility. AnnalsoftheNewYorkAcademyofSciences,1373(1):96–113,2016.
[9] MatthewJVoss,MeeraZukosky,andRanxiaoFrancesWang. Anewapproachtodifferentiatestatesofmind
wandering: Effectsofworkingmemorycapacity. Cognition,179:202–212,2018.
[10] MeeraZukoskyandRanxiaoFrancesWang. Spontaneousstatealternationsinthetimecourseofmindwandering.
Cognition,212:104689,2021.
[11] LarsSandved-Smith, CasperHesp, JérémieMattout, KarlFriston, AntoineLutz, andMaxwellJDRamstead.
Towardsacomputationalphenomenologyofmentalaction: modellingmeta-awarenessandattentionalcontrol
withdeepparametricactiveinference. Neuroscienceofconsciousness,2021(1):niab018,2021.
[12] HayatoIdei,KeisukeSuzuki,andYuichiYamashita.Awarenessofbeing:Acomputationalneurophenomenological
modelofmindfulness,mind-wandering,andmeta-attentionalcontrol. 2024.
[13] Karl Friston. A theory of cortical responses. Philosophical transactions of the Royal Society B: Biological
sciences,360(1456):815–836,2005.
[14] RajeshPNRaoandDanaHBallard. Predictivecodinginthevisualcortex: afunctionalinterpretationofsome
extra-classicalreceptive-fieldeffects. Natureneuroscience,2(1):79,1999.
[15] AndyClark. Surfinguncertainty: Prediction,action,andtheembodiedmind. OxfordUniversityPress,2015.
[16] KarlJFriston,JeanDaunizeau,JamesKilner,andStefanJKiebel. Actionandbehavior:afree-energyformulation.
Biologicalcybernetics,102(3):227–260,2010.
[17] KarlFriston,JérémieMattout,andJamesKilner.Actionunderstandingandactiveinference.Biologicalcybernetics,
104:137–160,2011.
13
[18] AhmadrezaAhmadiandJunTani. Anovelpredictive-coding-inspiredvariationalrnnmodelforonlineprediction
andrecognition. Neuralcomputation,31(11):2025–2074,2019.
[19] WataruOhataandJunTani. Investigationofthesenseofagencyinsocialcognition,basedonframeworksof
predictive coding and active inference: A simulation study on multimodal imitative interaction. Frontiers in
Neurorobotics,14:61,2020.
[20] Hendry F. Chame, Ahmadreza Ahmadi, and Jun Tani. A hybrid human-neurorobotics approach to primary
intersubjectivityviaactiveinference. FrontiersinPsychology,11:3207,2020.
[21] NadineWirkuttis,WataruOhata,andJunTani. Turn-takingmechanismsinimitativeinteraction: Roboticsocial
interactionbasedonthefreeenergyprinciple. Entropy,25(2):263,2023.
[22] KarlFriston. Thefree-energyprinciple: aunifiedbraintheory? NatureReviewsNeuroscience,11(2):127–38,
2010.
[23] YuichiYamashitaandJunTani. Emergenceoffunctionalhierarchyinamultipletimescaleneuralnetworkmodel:
ahumanoidrobotexperiment. PLoScomputationalbiology,4(11),2008.
[24] GuidoSchillaci,AlejandraCiria,andBrunoLara.Trackingemotions:Intrinsicmotivationgroundedonmulti-level
predictionerrordynamics. 10thJointIEEEICDL-EPIROB,pages1–8,2020.
[25] DiederikPKingmaandMaxWelling. Auto-encodingvariationalbayes. arXiv:1312.6114,2014.
[26] DiederikPKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. arXivpreprintarXiv:1412.6980,
2014.
[27] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations by error
propagation. Technicalreport,CaliforniaUnivSanDiegoLaJollaInstforCognitiveScience,1985.
[28] MaliaFMason,MichaelINorton,JohnDVanHorn,DanielMWegner,ScottTGrafton,andCNeilMacrae.
Wanderingminds: thedefaultnetworkandstimulus-independentthought. science,315(5810):393–395,2007.
[29] ChristineAGodwin,MichaelAHunter,MatthewABezdek,GregoryLieberman,SethElkin-Frankston,VictoriaL
Romero,KatieWitkiewitz,VincentPClark,andEricHSchumacher. Functionalconnectivitywithinandbetween
intrinsicbrainnetworkscorrelateswithtraitmindwandering. Neuropsychologia,103:140–153,2017.
[30] EkaterinaDenkova,JasonSNomi,LucinaQUddin,andAmishiPJha. Dynamicbrainnetworkconfigurations
duringrestandanattentiontaskwithfrequentoccurrenceofmindwandering.Humanbrainmapping,40(15):4564–
4576,2019.
14

=== REVISE TO ===
PROFESSIONAL TONE: Begin directly with content - NO conversational openings like 'Okay, here's...'

1. Fix all issues above
2. Title: "Modeling Autonomous Shifts Between Focus State and Mind-Wandering Using a Predictive-Coding-Inspired Variational RNN Model"
3. Include 10-15 quotes from paper text
   - Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
   - Use consistent quote formatting: 'The authors state: "quote"' or vary attribution phrases
   - Vary attribution phrases to avoid repetition
   - CRITICAL: Only extract quotes that actually appear in the paper text
4. ELIMINATE ALL REPETITION - each sentence must be unique
   - Check before each sentence: 'Have I already said this?' If yes, write something new
   - Vary attribution phrases - do NOT repeat 'The authors state' multiple times
5. Extract methodology, results with numbers, key quotes
6. 1000-1500 words, structured with ### headers

Generate COMPLETE revised summary.