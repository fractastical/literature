=== IMPORTANT: ISOLATE THIS PAPER ===You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.Do NOT mix information from different papers. Only use information from THIS specific paper.Paper Title: Active Inference AI Systems for Scientific DiscoveryCitation Key: duraisamy2025activeAuthors: Karthik DuraisamyREMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.Year:2025Abstract: The rapid evolution of artificial intelligence has led to expectations of transformative impact on science, yet current systems remain fundamentally limited in enabling genuine scientific discovery. This perspective contends that progress turns on closing three mutually reinforcing gaps in abstraction, reasoning and empirical grounding. Central to addressing these gaps is recognizing complementary cognitive modes: thinking as slow, iterative hypothesis generation—exploring counterfactual...Key Terms: systems, established, scientific, genuine, principles, reasoning, spaces, gaps, inference, michigan=== FULL PAPER TEXT ===Active Inference AI Systems for Scientific DiscoveryKarthik DuraisamyMichigan Institute for Computational Discovery & Engineering,University of Michigan, Ann Arbor.AbstractThe rapid evolution of artificial intelligence has led to expectations of transformative impact on science, yet current systems remain fundamentally limited in enabling genuine scientific discovery. This perspective contends that progress turns on closing three mutually reinforcing gaps in abstraction, reasoning and empirical grounding. Central to addressing these gaps is recognizing complementary cognitive modes: thinking as slow, iterative hypothesis generation—exploring counterfactual spaces where physical laws can be temporarily violated to discover new patterns—and reasoning as fast, deterministic validation, traversing established knowledge graphs to test consistency with known principles. Abstractions in this loop should be manipulable models that enable counterfactual prediction, causal attribution, and refinement. Design principles—rather than a monolithic recipe—are proposed for systems that reason in imaginary spaces and learn from the world: causal, multimodal models for internal simulation; persistent, uncertainty-aware scientific memory that distinguishes hypotheses from established claims; formal verification pathways coupled to computations and experiments. It is also argued that the inherent ambiguity in feedback from simulations and experiments, and underlying uncertainties make human judgment indispensable, not as a temporary scaffold but as a permanent architectural component. Evaluations must assess the system’s ability to identify novel phenomena, propose falsifiable hypotheses, and efficiently guide experimental programs toward genuine discoveries.Overthepastdecade,theevolutionofAIfoundationmodelresearchhasfollowedaclearsequenceofdiscretejumpsincapability. TheadventoftheTransformer[62]markedaphasedominatedbyarchitecturalinnovations,whichwasrapidlysucceededbyscalingdemonstrationssuchasGPT-2[55]. Thematurationoflarge-languagemodelpre-trainingthengavewaytotheusabilityturn:chat-orientedmodelsfinetunedforalignmentandsafetythatenableddirecthumaninteraction[47]. Thecurrentfrontierischaracterizedbyreasoning-emulation systems that incorporate tool use, scratch-pad planning, or program-synthesis objectives[45]. A fifth, still-incipient phase points toward autonomous agents that can decompose tasks, invoke external software or laboratories, and learn from the resulting feedback. Scientific applications of AI have echoed each of these transitions at a compressed cadence. As examples, SchNet translated architectural advances to quantum chemistry[59]; AlphaFold leveraged domain knowledge infused scaling to solve protein-fold prediction[30]; ChemBERTa[14]andFourCastNet[48]adaptedlanguageandvisioninnovationstomolecularandclimate domains; and AlphaGeometry applied reasoning-centric objectives to symbolic mathematics[61].Collectively, recent works [24,7,9] chart a shift from single, specialized pre-trained model to workflow orchestration, suggesting that future breakthroughs may hinge on integrating heterogeneous, domain-aware agents capable of planning experiments, steering simulations, and iteratively refining hypotheses across scales.Thishighlightsadeeperchallengeforscientificdiscovery,whichmustreasonacrossstackedlayersofabstraction: the emergence of unexpected phenomena at higher scales, just as local atmospheric equations do not directly predict large-scale El Niño patterns. To address this challenge, it may be required to deliberately architect systems with built-in mechanisms for hierarchical inference, equipping them with specialized components that can navigate between reductionist details and emergent phenomena. A compelling counter-argument posits that such abstract reasoning is not a feature to be explicitly engineered, but an emergent property that will arise from sufficient scale and diverse data. Proponents of this view might point to tools such as AlphaGeometry [61], where complex, formal reasoning appears to emerge from a foundation model trained on vast synthetic data. However, we contend that while scaling can master any pattern present in the training distribution—even highly complex ones—it is fundamentally limited to learning correlational structures.Scientific discovery, in contrast, hinges on understanding interventional and counterfactual logic: what happens when the system is deliberately perturbed? This knowledge cannot be passively observed in static data; it must be actively acquired through interaction with the world or a reliable causal model thereof. The ‘reality gap’ thus remains a significant barrier that pure scaling may not cross.It is also pertinent to examine the nature of present-day scientific discovery before speculating the role of AI. Modern science has moved beyond the romanticized vision of solitary geniuses grappling with nature’s mysteries. It may be difficult to generalize or even define the nature of discovery, but it is safe to assume that many of today’s discoveries emerge from vast collaborations parsing petabytes of data from instruments such as the Large Hadron Collider or from distributed sensor networks or large-scale computations and most importantly, refining hypotheses in concurrence with experiments and simulations. In fields such as high-energy physics, the bottleneck has shifted toward complexity management, whereas in data-constrained arenas such as fusion-plasma diagnostics, insight scarcity remains dominant; any general framework must therefore account for both regimes. Even if one possesses the raw data to answer profound questions, we often lack the cognitive architecture to navigate the combinatorial explosion of hypotheses, interactions, and emergent phenomena. This creates an opportunity for AI systems—to excel precisely where human cognition fails, in maintaining consistency across very high-dimensional parameter spaces, identifying and reasoning about subtle patterns in noisy data. At this juncture, it has to be emphasized that generating novel hypotheses might be the easy part [25]: the challenge is in rapidly assessing the impact of a hypothesis or action in an imaginary space. Thus AI systems have to be equipped with rich world models that can rapidly explore vast hypothesis spaces, and integrated with efficient computations and experiments to provide valuable feedback.Against this backdrop, this perspective piece is organized around three interlocking hurdles (i) the abstraction gap, which separates low-level statistical regularities from the mechanistic concepts on which scientists actually reason; (ii) the reasoning gap, which limits today’s models to correlation-driven pattern completion rather than causal, counterfactual inference; and (iii) the reality gap, which isolates computation from the empirical feedback loops that ultimately arbitrate truth. The central thesis is that scientific discovery demands a reimagining of AI architectures. Future AI systems must integrate active inference principles, maintaining persistent scientific memory while engaging in closed-loop interaction with both simulated and physical worlds. The following section examines each gap in detail before presenting an integrated architecture that addresses these challenges holistically.1 Fundamental Gaps in AI ModelsThepathfrom currentAIcapabilities to genuine scientific discovery is obstructed by interconnected barriers that reflect deep architectural limitations rather than scaling or engineering challenges. These gaps are not independent failures but symptoms of a unified problem: current AI systems lack the cognitive architecture necessary for scientific thinking. Understanding these gaps requires recognizing that they form a mutually reinforcing system of constraints: without rich abstractions there is little substrate for reasoning, and without tight coupling to reality even the most elegant abstractions may drift toward irrelevance.1.1 The Abstraction GapWhile early models largely manipulated tokens and pixels, recent advances in concept-bottleneck networks[35], symmetry-equivariant graph models[60], and neuro-symbolic hybrids[40] show preliminary evidence that contemporary AI can already represent and reason over higher-order scientific concepts and principles. Yet a physicist reasons in conservation laws and symmetry breaking, whereas language models still operate on surface statistics. Closing this abstraction gap requires addressing several intertwined weaknesses.Inset1: Thinking and reasoningUnderstanding how AI systems can enable scientific discovery requires examining the cognitive processes that these systems attempt to emulate. In this context, a critical distinction emerges between thinking and reasoning: Thinking can be operationalized as an iterative, exploratory process—searching for partial solutions in the form of patterns without guaranteed convergence. It is the slow, generative phase where new connections form and novel patterns emerge from a number of possibilities. Reasoning, by contrast, represents the fast, deterministic traversal of established knowledge structures—building the most expressive path through a graph of already-discovered patterns. This dichotomy [29] may explain why current AI systems excel at certain tasks while failing at others. Large language models can reason impressively when the requisite patterns exist in their training data—they rapidly traverse their learned knowledge graphs to construct seemingly intelligent responses. Yet they struggle with genuine thinking: the patient, iterative discovery of patterns that do not yet exist in their representational space. Scientific discovery demands both capabilities in careful balance.Thinking generates hypotheses by discovering new patterns through mental simulation and exploration; reasoning then rapidly tests these patterns against existing knowledge and empirical constraints. The purpose of thinking therefore is not to solve problems directly but to expand the pattern vocabulary available for subsequent reasoning. Each thinking cycle potentially adds new nodes and edges to the knowledge graph, creating shortcuts and abstractions that make previously intractable reasoning paths suddenly accessible. This is perhaps why breakthrough discoveries often seem obvious in retrospect—the thinking