=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: A Review of Latent Representation Models in Neuroimaging
Citation Key: vázquezgarcía2024review
Authors: C. Vázquez-García, F. J. Martínez-Murcia, F. Segovia Román

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2024

Abstract: Neuroimaging data, particularly from techniques like MRI or PET, offer rich but complex in-
formation about brain structure and activity. To manage this complexity, latent representation
models –such as Autoencoders, Generative Adversarial Networks (GANs), and Latent Diffusion
Models (LDMs)– are increasingly applied. These models are designed to reduce high-dimensional
neuroimaging data to lower-dimensional latent spaces, where key patterns and variations related to
brain function can be identif...

Key Terms: representation, review, latent, structure, spaces, models, data, dimensional, like, neuroimaging

=== FULL PAPER TEXT ===

Review: Latent representation models in neuroimaging
C. V´azquez-Garc´ıa1, F. J. Martinez-Murcia1, F. Segovia Roma´n1, Juan M. Go´rriz1
December 2024
1 Department of Signal Theory, Telematics and Communications, University of Granada, Spain
Abstract
Neuroimaging data, particularly from techniques like MRI or PET, offer rich but complex in-
formation about brain structure and activity. To manage this complexity, latent representation
models –such as Autoencoders, Generative Adversarial Networks (GANs), and Latent Diffusion
Models (LDMs)– are increasingly applied. These models are designed to reduce high-dimensional
neuroimaging data to lower-dimensional latent spaces, where key patterns and variations related to
brain function can be identified. By modelling these latent spaces, researchers hope to gain insights
into the biology and function of the brain, including how its structure changes with age or disease,
orhowitencodessensoryinformation,predictsandadaptstonewinputs. Thisreviewdiscusseshow
these models are used for clinical applications, like disease diagnosis and progression monitoring,
butalsoforexploringfundamentalbrainmechanismssuchasactiveinferenceandpredictivecoding.
These approaches provide a powerful tool for both understanding and simulating the brain’s com-
plex computational tasks, potentially advancing our knowledge of cognition, perception, and neural
disorders.
1 Introduction
Neurodegenerative diseases (NDDs), such as Alzheimer (AD) or Parkinson (PD) are amongst the most
prevalent in the world, affecting over 29.8 million and 6.2 million people respectively. In the last years,
the prevalence of these diseases has escalated drastically, with more than a twofold increase from 1990
to 2015. This increase has led to a decline in quality of life as the average life expectancy gets larger.
In addition, there is a large spectrum of neurological disorders that affect millions of people all over
the world across all ages, such as Schizophrenia (SZ), ADHD, Autism or Brain tumors, hindering their
daily lives. These diseases do not only have a severe effect on the healthcare system, but they also have
an impact on the economy. Therefore, the need for effective clinical therapies is becoming increasingly
urgent.
Currently, most therapeutic approaches prioritize improving patients’ quality of life and alleviating
symptoms, rather than directly addressing the underlying causes of the disease. Although these treat-
ments may offer temporary relief for certain individuals, their efficacy is often case-dependent and tends
to diminish over time. Moreover, they are unable to halt or reverse disease progression, as clinical trials
generally begin many years after the neurochemical processes driving the condition have already been
set in motion.
ResearchinneurosciencehasshownthatneuropathologicalprocessesofNDDssuchasADorPDbegin
years, even decades, before the first symptoms appear. This presymptomatic stage of the diseases opens
a window of opportunity to implement preventive therapies and treatments. In this context, Disease-
ModifyingTherapies(DMTs)haveemergedasabasisfordevelopingnewtherapeutictechniques. These
therapies aim to modify the pathological processes underlying neurological diseases by targeting the
mechanisms that drive their progression . Without such a comprehension, the identification of the right
mechanisms to target may be misinterpreted. For instance, most pharmacological trials for AD have
been developed to decrease the levels of Amyloid-beta (Aβ) aggregates and plaques by using drugs.
Nevertheless,thesetrialshavenotbeensuccessfulinretrievingthephenotypesofthedisease,compelling
many researchers and funding bodies to shift the focus to another potential causes. On the other hand,
non-degenerative diseases, such as neurological disorders, even though they might not be as dramatic as
NDDs, they also hinder severely the lives of the patients in most cases. Alike NDDs, the mechanisms
underlyingthesedisordersarestillnotwellunderstood. Moreover,thevastheterogeneityofthespectrum
of these disorders greatly difficult the progress in research.
1
4202
ceD
42
]VC.sc[
1v44891.2142:viXra
Beyond clinical practice, there is a growing interest in understanding the fundamental mechanisms
of brain function. Research has revealed that the brain operates as a higly dynamic system capable of
encoding,predicting,andadaptingtoenvironmentalinputs. Theseprocesses,includingsensoryencoding,
predictivecoding,andactiveinference,reflectthebrain’sabilitytogenerateinternalrepresentationsthat
optimizeperceptionandbehavior. However,unravelingthesemechanismsremainsachallengeduetothe
high-dimensional nature of neuroimaging data, which captures both brain structure and function but is
difficult to interpret effectively.
In this context, research has focused on designing techniques that allow to interpret and capture
relevant insights from these non-invasive neuroimaging techniques. In the last years a variety of models
andtechniqueshavebeendevelopedtoanalyzeclinicaldata,amongthemlatentegenerativemodelshave
emerged as powerful tools for addressing these challenges. This review aims to provide a comprehensive
overview of how latent generative models are applied in neuroimaging. We explore their role in clinical
tasks, suchasdiseasediagnosis, progressionmonitoring, andharmonizationofmulti-sitedata, aswellas
their contributions to fundamental neuroscience, including the study of brain networks, cognitive pro-
cesses,andsensoryrepresentations. Bybridgingclinicalandtheoreticalresearch,thesemodelsoffernew
opportunitiestoadvanceoutunderstandingofbrainfunctionanddysfunction,ultimatelypavingtheway
for improved diagnosis, therapies, and insights into the brain’s remarkable computational capabilities.
2 An opportunity: latent representations
Medical images are commonplace in hospitals and laboratories worldwide. In neuroimaging, these im-
ages are extensively used for their non-invasive nature and the valuable insights they provide into the
structure and functionality of the brain. In the context of neurodegenerative diseases (NDDs) and neu-
rological disorders, these neuroimaging techniques play a critical role in monitoring disease progression
andassistingclinicaldecisions. Despitetheirutility,neuroimagingdatacanbehighlycomplex,requiring
both technological expertise and an understanding of the diseases being studied. Moreover, the high
dimensionality of these data, when combined with other biomarkers, presents significant challenges for
modeling and interpretation.
One promising approach to tackling these challenges comes from the manifold hypothesis, which
suggests that real high-dimensional data tend to lie on or near a lower-dimensional manifold. This
means that even though the data may have a large number of dimensions, much of it is constrained to
a more compact, lower-dimensional structure. This hypothesis, first explored in mathematics and later
applied to various fields like psychology and neuroscience, proposes that the high-dimensional data we
collect, including neuroimaging data, can be better understood by uncovering this underlying manifold
structure.
Figure 1: Simplified scheme of the latent representation. High-dimensional neuroimaging scans are
processed through a model capable of computing the probability distribution of a lower-dimensional
manifold that captures relevant patterns in our dataset, whether the model is implicit or explicit.
2
The manifold hypothesis has become particularly influential in the analysis of neural data. In psy-
chology, for instance, it has been suggested that the brain encodes information in a lower-dimensional
space, which allows it to process complex sensory input efficiently [13, 43, 32]. One usual problem of
high-dimensional data, is the curse of dimensionality [20], which suggests that when the number of fea-
tures (e.g., voxels in an image) exceeds the number of samples, traditional methods may fail to provide
reliable models. For example, neuroimaging volumes of dimensions 90×110×90 can result in hun-
dreds of thousands of parameters, but we typically have only a few thousand samples, making our data
space very sparse. Thus, uncovering the latent manifolds that are believed to exist in high-dimensional
neuroimaging data becomes essential for understanding the brain’s complexities.
To explore the idea of the manifold hypothesis in data science, various mathematical and compu-
tational methods have been proposed. Techniques such as Principal Component Analysis (PCA) and
Multi-DimensionalScaling(MDS)havelongbeenusedtoreducethedimensionalityofdataandidentify
its latent structure. However, these techniques rely on linear assumptions and may fail to capture com-
plex, non-linear relationships within the data. For instance, PCA and MDS use Euclidean distances to
define proximity between data points, but this can be misleading in the presence of non-linear manifold
structures [66].
Inthiscontext,LocallyLinearEmbedding(LLE)[59]emergedasamethodthatattemptstocapture
non-linear structures by modeling data based on local relationships between neighboring points. While
elegant, these techniques are sensitive to noise and may perform poorly if the data does not satisfy the
assumptions necessary for their successful application.
The rise of Deep Neural Networks (DNNs) has brought new methods for modeling complex latent
manifolds. Unlike linear methods, DNNs can capture non-linear relationships within data, making them
ideal for high-dimensional neuroimaging data. Models like Autoencoders (AEs), Generative Adversarial
Networks(GANs),andLatentDiffusionModels(LDMs)areparticularlywell-suitedforthistask,asthey
learn to represent data in lower-dimensional latent spaces through non-linear transformations. These
approaches have opened up new avenues for understanding the underlying patterns in neuroimaging
data, contributing not only to clinical diagnosis but also to our understanding of the brain’s encoding
processes.
3 Latent Generative Models
Deep learning models have proven highly effective for deriving latent representations of complex, high-
dimensional, non-linear data in neuroimaging, as highlighted in the previous section. This is typically
achievedthroughtheuseoflatentgenerativemodels,whicharedesignedtolearncompactrepresentations
of input data while simultaneously generating new samples that adhere to the true underlying data
distribution. These models excel at capturing the essential features and variations within the data,
enabling a robust characterization of its distribution. In this section, we delve into the foundational
principlesunderlyingthemostwidelyemployedlatentgenerativemodels,settingthestageforsubsequent
sections where their applications in neuroimaging will be examined in detail.
3.1 The (Variational) Autoencoder
An Autoencoder (AE) is a neural network that learns to encode input data into a lower-dimensional
latent space and then reconstruct it back to its original form. It is commonly used for dimensionality
reduction, feature extraction, and denoising. The AE consists of two main components: an encoder,
which compresses the input data into the latent representation, and a decoder, which reconstructs the
data from this compressed representation.
While AEs are effective at capturing key features of the data, their latent representation is learned
directly from the data without additional constraints. This means that the structure of the latent space
may not be well-organized or interpretable, and it might not effectively capture the variability within
the dataset. For example, in the MNIST dataset of handwritten digits, each digit has unique variations
depending on the writer’s style (e.g., different stroke thicknesses or curvatures for the digit ’2’). An AE
might learn a representation for each sample that captures its unique details, but these representations
might not generalize well to unseen variations.
Similarly, when applied to datasets like magnetic resonance images (MRI) of healthy subjects and
subjects with neurodegenerative diseases (NDDs), an AE might capture general brain patterns but fail
to account for inter-subject variability. For instance, two patients with Alzheimer’s disease (AD) might
exhibit different patterns of brain degeneration, one might show severe hippocampal atrophy, while
3
Figure2: SimplifiedschemeoftheVAE.Thisexplicitmodelcapturestheprioridistributionthatencodes
high-dimensional data into the latent manifold, allowing to sample and access the latent variables of the
model.
another might have more widespread degeneration. An AE may not organize the latent space in a
way that reflects these distinct pathological variations, limiting its ability to model and generalize the
variability in the data.
This problem was tackled by Kingma in its work [39], where he introduced the Variational Autoen-
coder (VAE), which is a modification of the usual AE, based on Bayesian Inference. This generative
model, instead of learning a fixed representation, VAEs learn a probabilistic distribution of the latent
representations. Inthiswork,Kingmaproposedawaytolearntheintractableposteriordistributionthat
encode the real data into the latent representation. The idea is that neuroimages of a certain disease
X are produced by an underlying pathological process Z that we do not know. Mathematically, this
defines a joint probabilistic model known as generative model p(x,z) = p(x)p(z|x), where p(x) is the
marginallikelihoodoftheneuroimagingdata,andtheposteriordistributionp(z|x)modelshowthisdata
is encoded into the underlying latent representation. In practice, these probabilities are intractable due
to the complexity, and so a variational inference (VI) model q (z|x) is introduced to substitute the real
ϕ
posterior distribution, which is parameterized by a deep neural network known as the encoder. Using
this VI approach we can obtain a lower bound of the marginal loglikelihood of data p(x):
logp(x)≥E [logp(x|z)]−D (q (z|x)||p(z)). (1)
ϕ KL ϕ
The first term can be interpreted as a reconstruction error, which tries to minimize the structural
differencebetweenrealandsyntheticdata,whilethesecondtermD istheKullback-Leiblerdivergence,
KL
which measures the difference between the VI posterior distribution and the prior distribution of the
latent variables p(z). This ELBO loss is a competition between precise reconstruction of images and
regularization of the latent space, i.e., capturing relevant variability of the data. If the KL divergence
is very low the reconstruction will be very accurate but the variability of the latent space will be very
small, which is undesirable.
The main advantage of the VAE-based model is that the information codified into the latent repre-
sentation contains precious information about the real neuroimages, since they are trained to mimic the
true codifying distribution, however, the reconstructions are usually very blurry compared to other DL
models. This is to be expected because we can see in the ELBO equation (1) that reconstruction detail
is sacrificed for the sake of a meaningful latent representation.
3.2 Latent Diffusion Models
LatentDiffusionModels(LDMs)arelatentgenerativemodelsthatfocusontransformingtheinputdata
into noise through a diffusion process and learn how to revert that process to generate high quality
samples. The main idea of this process is to map the complex data into a latent representation, where
4
thediffusionprocessismoreefficientandeasytohandle,inordertolearnhowtogeneratenewsynthetic
data in this latent representation. The LDM is divided in three main stages: i) encoding of the input
data into a latent representation, which is usually performed with an encoding model, such as a AE, ii)
forward noisy diffusion, and iii) reverse process. During the forward diffusion, random gaussian noise
is added to the input data. This is performed via a Markov process, meaning that each diffusion step
dependsonlyontheimmediatepreviousstepandnotinthehistoryoftheprocess. Ateachstep,asmall
amount of noise is added to the entire latent representation, such that the process is described by:
(cid:112)
q(z |z )=N(z ; 1−β z ,β I). (2)
t t−1 t t t−1 t
This distribution describes how the diffusion step z is produced by the previous step z . This can be
t √ t−1
translated as, given z , z is distributed as a normal distribution with mean 1−β z and variance
√t−1 t t t−1
β I, where the term 1−β controls how much of z is conserved and β controls how much noise is
t t t−1 t
added. The entire diffusion process is simply the product of each diffusion step.
Once the data has turned into noise, the objective of the LDM is to train a generative model that
learns how to reverse this process and retrieve the original representation z from the final noisy step
0
z . To do that, a UNet or transformer is commonly trained to estimate the noise ϵ (z ,t). To do so, the
t θ t
goal is to minimize the difference between estimated noise and real noise added to the input:
L(θ)=E (cid:2) ||ϵ (z ,t)−ϵ(z )||2(cid:3) , (3)
z0,t θ t t
where ϵ(z ) is the real noise at step t and ϵ (z ,t) is the prediction of the model. Once the denoising
t θ t
process is finished we can reconstruct the synthetized neuroimage by using the decoder of the model we
used to encode the images into the latent representation as xˆ = Decoder(zˆ). As we will see this model
is commonly used in neuroimaging to produce high quality images from latent representations.
Figure3: SimplifiedschemeoftheLDM.Thislatentgenerativemodeltakesalatentspaceandgeneratea
forward diffusion, introducing noise into this representations. Then, an inverse process learns to denoise
the latent space to generate a reconstructed latent space using a denoising U-Net.
As we have discussed before, VAEs tend to reconstruct blurry images due to their nature, in order
to obtain meaninful representations. If we combine a VAE model with a LDM we can exploit the
rich representations learnt by the VAE and still generate high quality images. Notice that, unlike the
VAE, LDMs do not learn how to capture relevant underlying information about neurological processes
hidden in neuroimages or how to model their distributions. Instead, they need a previous step to obtain
representations and then they make use of that information to generate new data. In that sense, LDMs
can be thought as a tool to handle the rich information of the latent space, more than a method to
extract them.
3.3 Generative Adversarial Networks
GANs were introduced by Goodfellow in 2014 [28] as DNN based on two counterparts: a generator G
and a discriminator D. The GAN can be thought as a competition between a generative model that is
5
learning how to generate synthetic data that resembles the input, and discriminator that is trying to
guess whether the received image is real o generated. The generator G takes a noise vector z from a
latent space and maps it into a data space of neuroimages, while the discriminator takes a sample x
which can be real or generated, and computes the probability of x being real. Hence, it is formalised a
a minmax problem, where the loss function is:
minmaxV(D,G)=E [logD(x)]+E [log(1−D(G(z)))]. (4)
G D
x∼pdata(x) z∼pz(x)
The goal is to learn the generator distribution p over the data x. We can think of the model as
g
a child who is trying to learn how to draw. The generator is the child and his task is to make the
drawings look real. The discriminator is a teacher who is trying to guess whether the drawing is real
or not (generated). The generator is trying to fool the discriminator and the discriminator is trying to
improve in order not to be fooled. If we take a look at the loss function (4) what we see is that the
generator wants to minimize V(D,G) and the discriminator is trying to maximize it. The first term of
that equation assures that the discriminator classifies the real samples as real, while the second term
assures that the generator produces better images.
Figure 4: Simplified scheme of the GAN model. A latent space is fed to a generator that produces
fake reconstructed images. Then, a discriminator learns to distinguish between real and fake images,
updating the weights of the model according to the loss function
Now, we have to take into account that, even though GAN models can learn a relationship between
latent representations and real data, this latent space is not structured or explicitly defined. While
VAEs learn an explicit latent representation that we can interpret and exploit for different analyses,
GANs learn these relationships implicitly. Specifically, GANs optimize their loss function through the
adversarial “game” between the generator and the discriminator, without approximating a probabilistic
distribution explicitly, as VAEs do. However, the representation of GANs is implicit and inaccessible for
analysis,makingitclosertoimplicitmodelslikeLatentDiffusionModels(LDMs),usefulforhigh-quality
generation or synthesis.
4 State-of-the-art
Inthissectionwewillpresentthemostrelevantlinesofresearchinobtainment,exploitation,andanalysis
of latent representations through latent generative models in neuroimaging. Here, we will explore how
researches have been using the bayesian inference framework as a mean for understanding the internal
cognitive processes and neural connections in the brain. Additionally, we will also explore how these
latent representations are used to create models that aim to aid typical clinical practice problems such
as the inter-center noise due to the different use of scans and methodologies of each institution, as well
as the translation of low resolution neuroimages into higher resolutions, like 7T MRI. Moreover, we will
see how these latent representations are able to capture longitudinal patterns of the brain, allowing us
6
to model brain aging or classify different neurodegenerative diseases or disorders, such as Alzheimer or
Schizophrenia. Furthermore,wewilldiscusshowfunctionalbrainnetworkscanbeextractedusinglatent
spaces, providing us with relevant insights of how these brain networks work for subjects with certain
disorders, such as ADHD. Next, we will see how these latent representations are useful for integrating
differenttypesofdata,anopenprobleminmedicinethatgrantsuswithgreatinsightofdiseases. Finally,
wewillalsoseehowwecanbothsynthesizeimagesinordertoaugmentourdatasetsorevenreconstruct
visual information through fMRI maps, as well as other works based on latent representations.
4.1 The Brain as an inference machine
As we have discussed, generative models are bioinspired by the encoding and decoding mechanisms of
the brain. However, these models are not just powerful tools to analyze or generate data, but they also
give us an opportunity to understand the most abstract and fundamental processes of the brain.
Traditionally,itwasthoughtthatthebrainreceivesstimulifromitssurroundingsandprocessesthem
to form perceptions. However, we now know from psychology that perception is not a passive process.
Instead, the brain actively generates inferences about the environment based on prior knowledge and
expectations. This active inference means that the brain not only perceives but also predicts what is
going to happen next, using previous knowledge and past experiences to predict future outcomes. This
actually resembles the usual Bayesian Inference that we are familiar with, where we could consider that
the brain is producing constant beliefs about the world and updating those beliefs according to the
sensory information following the Bayes’ rule:
p(x|z)p(z)
p(z|x)= , (5)
p(x)
where p(z) is the belief and p(x|z) is the likelihood that the observation x fits the belief z. The brain
is, without a doubt, a physical object, and thus there must exist a mathematical formalism to describe
it. While this formalism may be highly complex, Bayesian inference, though not the ultimate answer,
undoubtedly brings us a step closer to understanding how neural encoding, perception, and cognition
operate. In this context, several researchers have leveraged the powerful Bayesian framework to gain
deeper insights into how inference and representations are generated within the brain.
For instance, authors in [24] discuss the relationship between generative models, brain function and
neuroimaging. They focus on how functional specialization of the brain depends on the integration
betweenpopulationofneurons. Theyusegenerativemodelstoexplainthepaperoffeedbackconnections
among neurons allowing cortical regions to reconfigure dynamically depending on the context. The
authorsexploretheideathatforwardandbackwardconnectionsindeeplearningmodelsareverysimilar
to the bottom-up and top-down connections of neurons. Bottom-up connections refer to the flow of
information from lower levels (sensory areas that receive stimuli) to higher levels (areas that process
more complex information). This represents the data-driven pathway, where stimuli are progressively
transformed into perceptions. In top-down the information flows from higher to lower areas, in which
case, higher areas of the brain generate predictions about what they expect to receive and send these
predictions to lower areas. These predictions act as a guide or context for a better interpretation of the
received sensory information. If the prediction does not coincide with the input information the neuron
population would produce an error that error is used to adjust the predictions in the higher areas until
these predictions coincide with the perceived reality. The error disappears once there is a consensus
between prediction and reality.
The main idea of the authors is that processing within the brain does not only take into account
informationcomingfromstimuli(bottom-up)butalsofromthepredictionscreatedbytheactiveinference
of the brain. Moreover, this means that what we perceive is not only a product of the stimuli but it
is also influenced by the constant context and prediction generated by the brain based on the previous
experiences and environment. The authors in this paper argue that the brain can be conceptualized as
a system that is constantly generating predictions about the external world, adjusting these predictions
according to the feedback provided by the external stimuli. These generative models allow the brain to
construct an internal representation of reality that helps to optimize its prediction ability and reaction
to stimuli.
Here, they also propose that this theory can be experimentally validated using neuroimaging tech-
niques. First,theyconsiderataskconsistingofsaying”yes”whenthesubjectsseearecognisableobject,
and ”yes” when they see an unrecognisable non-object. The idea is to see how recognisable objects are
perceivedinthebrainbyspecificareasrelatedtovisualobjectrecognition,whileabstract,unrecognisable
7
objects activate different areas. Using this comparison they can isolate which brain areas are actually
related to object recognition. Now, they consider a factorial design, in which an additional component
is added to the recognition task. Imagine the same object vs non-object recognition task but, instead of
saying ”yes” the subjects have to name the object (if it is a recognisable object) or name the colour of
the non-object. In this task, there is an additional name retrieval component. This component modifies
the brain response that is observed in the previous ”yes” task, meaning that a new cognitive process
is taking place (name retrieval, in this case). This task allows us to see how different areas interact
between them. The main idea behind this experiment is that the context of the additional component
influences the processing of object recognition. The name retrieval component adds a complexity layer
ontopofthevisualrecognitiontask. Whenthesubjecthastonametheobject,thebrainpredictswhich
informationshouldbeavailable,whichmodulatestheperceptionoftheobject. Thebrainisnotpassively
reacting to the bottom-up information, but it is also expecting a name and creating its own predictions.
The brain does not simply react passively to bottom-up information; it also anticipates and generates
its own predictions. Furthermore, the literature has demonstrated that top-down information flows play
a significant role in this process. For example, as [21, 22] mention, mention, the perceptual encoding of
”moving quickly” is known to occur in the visual area V5, while the visual area V2 is responsible for
processingbasicshapesandborders. IthasbeenobservedthatpredictionsgeneratedinV5aresentback
toV2,influencingtheprocessingofvisualinformationtowhatisexpectedtoreceive. Moreover, authors
in[35]havefound,inmotorpreparation,thatthebrainisabletofindoptimalpreparationsubspacesthat
are closer to motor response. In addition, they discuss that this optimal anticipatory control of future
movements requires a form of internal feedback which is implemented through a thalamic-cortical loop.
This aligns to the idea that there is a forward and backward propagation and the idea of an internal
prediction.
Moreover, the authors discuss in a review on the impact of computational neuroscience on neu-
roimaging [23] how latent generative models are essential for understanding latent representations in
neuroimaging. They note that these models provide access to latent computational variables, which
represent mental states inferred during tasks. This is compared to the brain’s ability to infer the mental
states of others, a process extensively studied in psychology known as the Theory of Mind [54]. The
authors claim that, by using generative models, the beliefs of a subject over another individual during a
gamecanbeestimated,whichistranslatedtoneuralcorrelationsinfMRI.Thisapproachallowsmapping
of computational variables to neural representations. To exemplify this generative brain idea they use
the paradigm of the Theory of Mind, by using the cooperative game experiment. In this experiment,
the participants play a cooperative known as ”stag hunt”. In this game, the subjects have to either
hunt smaller prey on their own (rabbits) or cooperate with other players to hunt larger prey (stag).
When we have two or more players, there is a joint state space of all players, representing their mental
representations, that is, each representation can not be understood individually but they are affected by
the presence of other players. This gives raise to a difficult problem known as infinite regression. If I
wanttoevaluateyourstate, Ineedtoknowhowyouevaluatemystate, forwhichyouneedtoknowhow
I evaluate your state evaluating my state, and so on. This problem can be addressed by assuming that
thisinferenceisnotinfinitebutthereissomelimitorupperboundreferredasthelevelofsophistication.
In the stag game experiment the subjects were told that they were playing with other participants, but
insteadtheywereplayingagainstacomputerthatcouldsimulatedifferentlevelsofsophistication(levels
of inference over the other players’ mental states). The computer could act as a player that had a lot
of understanding of the game (high sophistication) or very little understanding (low sophistication). In
this experiment the level of sophistication is a latent variable. In each round of the game the subjects
had to make decisions based on their beliefs about the opponents. These decisions were used to update
thesophisticationoftheopponent,whichhighlyresemblesBayes’rule. Thesubjectformsaprioridistri-
bution over the levels of sophistication of its opponent and, through observation during the game, they
update such beliefs, creating a posterior distribution over the sophistication. Once the estimations have
beenobtained,theycanbeusedasafunctionofthestimulusinafMRIanalysistoidentifyneuralcorre-
lations associated to the sophistication representation and the uncertainty of the subject. In this paper
the authors show experimental validation showing how the computed posterior distributions match the
levels of sophistication simulated by the the computed as the game advances. They also show evidence
that suggest functional specialization for inference on the sophistication [74].
On the other hand, the author in [26] proposes an alternative approach of this brain inference
paradigm based on implicit density models. In this work they try to explain how the usual Bayes’
rule, although very powerful, can not explain certain problems like why do certain illusions feel so real.
They mention the Ramachandran and Hirstein (1997) experience about wallpaper in a bathroom [58].
8
When gazing at a wallpaper in a bathroom we subjectively experience the wallpaper in our periphery
with high fidelity even though we perceive it with low fidelity. However, the wallpaper behind us is
not experienced. We infer with high confidence that there is wallpaper behind us even though we have
no experience of seeing it. Similarly, we know that there is one blind stop in our eye due to the optic
nerve, where light is not processed. However, we are not aware of this effect because the brain is able
to fill in the missing information by inferring. In this paper, the author argues that this problem can be
issued by interpreting Bayes’ rule from a different perspective. Instead of thinking of an explicit latent
representation, where we can directly access the latent variables, they use an implicit model, where a
”generator” draws samples from a generative model which are then fed, along with real sensory data, to
a ”discriminator” that tries to tell apart which samples are real and which are fake. As we discussed in
the latent generative model section, this describes a GAN model, as opposed to an explicit model (e.g.,
VAE). They mention that, in this context, if the visual system plays the role of the generator and our
perception plays the role of the discriminator, then the illusions can be understood as either the visual
system reporting things that are not there (or failing to report things that actually are there), or the
perceptionfailingtodiscriminatebetweenrealexperiencesandfalseones. Astheauthorindicate,adys-
functional generator (visual system) will produce abnormal content, while a dysfunctional discriminator
(our perceptual experience) will report fake content as real.
The author then try to use this generative approach to explain the previous wallpaper experience.
The generator produces a realistic simulation that the discriminator has to accept as either real or fake.
Inthiscontext, perceptualexperiencehingesonwhetherthediscriminatordeemsit”real.”Forinstance,
peripheral vision or the blind spot might be accepted as real, even though the brain does not attempt
to reconstruct objects outside our visual field. While the generator cannot simulate what lies behind
us with accuracy, it can create plausible reconstructions of the blind spot or peripheral vision through
interpolation. The inference of something like wallpaper behind us is shaped by prior experiences rather
than by direct visual reconstruction, distinguishing it from other scenarios.
Additionally, the author discusses how the GAN model could be applied to the brain and how the
different components would correlate with the neural components. Neuroimaging studies have proven
that the median anterior prefrontal cortex is implicated in discriminating whether a visual object has
previously been seen or if it is a product of imagination [37], suggesting that it helps to control the
fidelity of perceived experiences. Moreover, certain disorders like schizophrenia have shown reduced
activation of this area during reality monitoring tasks, which may explain why some people experience
hallucinations or are not able to tell apart between real and imagined experiences. On the other hand,
thegeneratormodelisverysimilartothefeedbackmechanismsofthebraincortex,aswehavepreviously
discussed in other works [24]. The proposed adversarial model suggests that the brain might implement
generative models through probabilistic population code, meaning that probabilistic representations of
perception are distributed through a population of neurons, where activity of these populations reflect
the probability of different perceptual beliefs.
Finally, the author explain the process of delusion formation using the adversarial brain paradigm.
Inthiscontext, thereisatwo-factor[14]failure, inwhichbothgeneratoranddiscriminatormalfunction.
The abnormal content from the defective generator (which is usually easily reported as fake by the
discriminator) can be accepted because the discriminator is also defective, giving rise to delusions.
4.2 Image-to-Image translation & Harmonization
One open problem in the field of neuroscience analysis is that every research center, institution or
collaboration, aiming to gather subject data in many modalities (ranging from neuroimaging scans such
asMRI-T1,MRI-T2,PET,CT,etc. tootherclinicaldatasuchasAβ amyloid,tauorevengenetics),use
their own measures and techniques to register such information. This means that each project chooses
which biomarkers are going to be measured and the specifics of the scans used to obtain images. Even
amongst a certain project there are usually differences in the scans used. This produces a tremendous
problem in the data analysis field, since it hinders the possibility of validation. Any model that aims
to produce insights about a certain disease must be applicable to any database in order to validate its
results. However, in the field of neuroimaging, there are certain patterns that differentiate images that
are taken using different methodologies, which is translated as inter-center noise. For example, if we try
to train a VAE using images from two different databases, this inter-center noise will severely affect the
coding of our latent representation, which will be noisy too, as a consequence. In order to address this
issue,researchershaveproposeddifferentmethodsandtechniquesusinggenerativemodelstoisolatethis
inter-center noise and eliminate it from the images in order to obtain a harmonized dataset from two or
9
more datasets. These models exploit the fact the the noise is codified in the latent space to isolate it.
The. authors in [44] discuss the possibility of domain image translation as a solution for multi-modal
image co-registration, a process crucial to align images obtained from different modalities. This work
suggeststhattranslationtechniquescanhelpthetransformationofimagesfromdifferentmodalitiesina
a common representation space, where latent representations could be used to capture relevant features
of images. However, these techniques do not only work for modality translation (e.g. from PET to CT),
but also for any kind of domain translation (e.g. inter-site translation).
For instance, the authors in [73] use a simple GAN model based on a generator and a discriminator.
Here,latentrepresentationsobtainedthroughthegeneratorarecomparedtorealimagesfromthetarget
modalityusingadiscriminator. ThenetworkistrainedusingadversariallossandL1lossthatguarantees
coherence between the original modality and the target modality by using low-level features along with
high-level features. To validate this model they evaluated the performance of translation from different
modalities, T1-T2, T2-T1, T1-T2flair, T2-T2flair, PD-T2 (proton density), T2-PD. Using the MAE,
PSNR,MI(mutualinformation)andSSIMperformancemetrics,theyfindthatthefullmodel(GAN+L1)
outperformsthetraditionalmethodssuchasRandomForest(RF)andCA-GA(contextualGAN)inmost
metricsandmodalities. Qualitatively,theyseethatthecombinationofadversarialandL1lossesproduces
clearimages,lessblurrythantheothermethods. Furthermore,theauthorsemploytheirmodeltoassess
the cross-modality registration task, utilizing real T2 images and generated T1 images. They create two
sets of subjects: one set, consisting of fixed subjects with T2 images and generated T1 images, serves as
areference,whilethesecondset,comprisingmovingsubjects,isusedtoaligntheirT2imageswiththose
of the reference set and their real T1 images with the generated T1 images. Each registration produces
deformationfields,whicharesubsequentlycombinedtogenerateafuseddeformationfieldthatintegrates
informationfrombothprocesses. TheregistrationperformanceisevaluatedusingtheDicecoefficientand
the Distance Between Corresponding Landmarks (DBCL) in brain structures. The results demonstrate
that the inclusion of the generated modalities contributes to an improvement in the Dice score and a
reductioninthedistance,indicatingamoreaccurateregistrationcomparedtousingonlyrealmodalities.
Finally, they used the model for the segmentation task in which they found that the generated modality
adds additional information to improve the segmentation, compared to using a single modality. They
evaluated a tumor segmentation task in the Brats2015 dataset, obtaining relevant performance metrics.
Authors in [1] proposed a similar but sophisticated model named MedGAN. Instead of simply using
adversarialandL1losses, thismodelcombinesadversarialandnon-adversarial(contentandstyle)losses
to translate neuroimages from one domain to another, specifically PET-CT translation. The model
consists of a CasNet generator which processes the input (PET) images to translate them to CT images
through several coding-decoding U-blocks. These blocks extract high-level features of the images. In
order to assure that the texture and structural features of the transformed input image coincide to the
target domain features they use style-content losses (non-adversarial loss). The content loss makes sure
thattheanatomicalcontentoftheimageispreserved,whilethestylelosstranslatesthevisualfeaturesof
theimagefromorigintotargetdomain. ThecontentlossobtainstherepresentationoftherealCTimage
used to train and the generated CT to compare them. Finally, a discriminator (adversarial loss) trains
themodeltotellapartrealfromgeneratedinput. Inordertovalidatetheirmodeltheyperformablation
studiesandcomparetherealandgenerateddatausingseveralperformancemetricssuchasSSIM,PSNR,
MSE,UQI(universalqualityindex)andLPIPS(learnedperceptualimagepatchsimilarity). Theyobtain
the best values of all metrics using the MedGAN architecture with every component (style loss, content
loss,perceptualloss,andadversarialloss). Theyalsocomparetheseresultswithotherstate-of-artmodels
such as pix2pic, ID-CGAN, Fila-sGAN and find that MedGAN outperforms them in most cases. This
work reveals the relevance of both the anatomical and style latent representations of the images. These
lower-dimensional manifolds are able to capture biological and visual properties of the data, allowing
both a better performance and interpretability.
Authors in [38] use a LDM to translate 3D MRI across different modalities. Here, the latent repre-
sentations are used to capture the relevant characteristics of a certain modality and convert them to a
similar latent to the target domain. This conversion process is performed through a MS-SPADE (Multi-
ple Switchblade Spatially Adaptative Normalization), which adapts the latents of the origin domain to
the style of the target domain latents, aiding the style transfer. This block applies a normalization to
the latents that adjusts µ and σ of the features depending on the target modality. Each modality has
its own set of normalization parameters. Additionally, these transformations are adaptative and spatial.
Instead of an uniform transformation of the latents, the block adjusts different areas according to what
is needed to match the target style. The block applies modulation functions to each pixel of the origin
latent that locally adjusts it so that it adopts the style characteristics of the target. This modulation is
10
unique to each modality. The idea of this architecture is that the modules of the MS-SPADE block can
be changed according to the target modality that we want to transform to, allowing transformation to
severaldifferentmodalitiesusingonesinglemodel,insteadofusingonemodelforeachtranslation. Com-
parison to other state-of-art techniques such as pix2pix or CycleGAN show a better metric performance
in T1-T2 and T2-FLAIR translation.
Other works aim to use image-to-image translation in order to improve the quality and resolution of
MR images. Authors in [69] claim to demonstrate that parameterization of conditional GANs in terms
of spatial-intensity transformations (SIT) improves image fidelity and robustness to artifacts in medical
image-to-image translation. Their SIT model can be seen as a way to manipulate latent representations
throughspatialandintensitytransformations. Theinputimagesarecodifiedintoalatentrepresentation
using a generator. The spatial deformations applied to this latent code allows the model to perform
morphological changes in the image, which is understood as a more precise representation of anatomical
variations, meanwhile the intensity transformations allow the model to adjust the features of intensity.
By separating spatial and intensity transformations, the SIT model allows for a clear interpretation of
how variations in latent space are translated into anatomical changes and intensity of generated images.
This model achieves a significative improvement of quality of images, reducing artifacts and improving
anatomicalfidelity. Theyevaluatethemodelusingalongitudinalpredictionofneurodegenerativepatients
task, where the model was able to predict trajectories of brain aging. Moreover, the model showed
robustness in multi-center clinical data. The SIT framework provide a decomposition of anatomical and
texturalchangesinthebrainthroughlatentrepresentationsthatpromisetobeusefulinaclinicalsetting.
Authors in [81] propose a model to perform harmonization between sites, named CALAMITY (Con-
trast Anatomy Learning and Analysis for MR Intensity Translation and Integration). This methods
tackles contrast variability across multi-site MR images. The model is based on the information bottle-
neck(IB)theoryanditemploysacodificationnetworktolearnadisentangledlatentspacethatseparates
anatomical and contrast information. The model is composed of an encoder (one for anatomy E and
β
another for contrast E , a decoder, and a D discriminator, which distinguishes anatomical features
θ β
according to the site, which helps to keep a global and uniform representation of the anatomy for all
sites. Themainideaisthattheanatomicallatentrepresentationβ shouldonlycontainanatomicalinfor-
mation, while the θ representation only contains contrast information specific of each site. To force θ to
only learn contrast information, they train using two images of the same subject at a certain site but of
differentmodalities. Theonlyusefulinformationisthecommoncontrast,becausebothimagesrepresent
the same anatomy, and so they expect θ to learn contrast information. In addition, if the CALAMITY
model is trained on sites A and B, it can be fine-tuned to be applied in site C by using a small subset of
its images, which allows for practical generalization. They found that their model outperforms baseline
(no harmonization) and other harmonization techniques in the literature. However, the authors point
thatalimitationofthemodelisthatthemodelassumesthatT1-wandT2-wofthesamesubjectcapture
the same anatomy, which is not always true.
Building on this approach, authors in [8] propose ImUnity, a VAE-GAN-based harmonization model
that introduces several novel elements. Unlike CALAMITy, ImUnity takes two 2D images of the same
subject from different locations of the brain and feed them as input. The first image is used to represent
theanatomicalstructure,whilethesecondoneprovidescontrastinformation. Thelatentrepresentations
of each, obtained with a VAE, are concatenated to a single latent space, while the discriminator tries to
differentiatebetweenharmonizedandrealimagesdemandingtheVAEtoproducebetterrepresentations.
Ontheotherhand,theconfusionmoduleisconnectedtothelatentspaceandtriestopredicttheoriginal
site of the image. A confusion loss is used to unlearn domain information, forcing the encoder to learn
a representation that is invariant across domains. The biological preservation module is not mandatory
andisusedtointroduceadditionalfeaturessuchassexorpresenceofdiseases. Next,inordertocreatea
3D model, they use a 2.5D approach, where the model is trained in the three perpendicular axis and the
results are combined along each axis. To validate the model, they perform three different experiments.
In the first experiment they evaluate the harmonization task using traveling subjects from OASIS and
SRPBS datasets. They compare the harmonized images and the real ones and use different performance
metrics such as SSIM and histograms, showing that ImUnity significantly improves the SSIM value
compared to a CycleGAN and CALAMITY [81]. During the second experiment they classify the site of
the MRI, where they found that the results of the harmonized data was superior to the non-harmonized
ones, diminishing the capacity fo classify the origin of the site, suggesting the elimination of noise.
Finally, for the third experiment they evaluated whether the model is able to improve classification
of ASD (Autism Spectrum Disorder). They observed that in most cases, the AUC (Area Under the
Curve)improvedwhenperformingcross-validationinseveralsites,from2differentsitesto11,evaluation
11
classification before and after harmonization. They found that in every combination of sites the AUC
improved after harmonization.
Theauthorsin[71]furtheradvanceharmonizationbyaddressingmachine-inducednoise. Inthiswork,
theyclaimthatMRIsuffersfromahugemachine-inducednoise. Theydiscussthatmosterrorsandnoise
inMRIdonotcomefrombiologicalproblemsbutfromdifferencesinscannersacrossdifferentinstitutions
and studies. To address this issue, they propose a model to harmonize MR images and eliminate the
inter-siteartifacts. Thegoalistoreducethenon-biologicalvariationsbyusingastyletranslation. Their
DLEST(DisentangledLatentEnergy-BasedStyleTranslation)consistsofthreemodules: 1)site-invariant
imagegeneration(SIG),site-specificstyletranslation(SST)and3)site-specificMRIsynthesis. Thefirst
module uses an AE to codify the MRI into a latent representation, such that the anatomical content
can be reconstructed with specific information from the site. This module aims to keep anatomical
information. Then, SST applies a energy-based model to the latent space in order to modify its style
from the origin site to the style of the target site. The last module generates synthetic MRI data with
the desired style, useful for augmentation techniques. The main idea of the energy-based model is to
learn the distribution of the target site and change the latent representations from the origin site to the
targetdistribution. Todothat,themodelassignsvaluesofenergytodifferentconfigurationsofthelatent
variables using P
θ
(z
y
) ∝ e−Eθ(zy), where E
θ
(z
y
) is the energy function that evaluates how compatible
is a latent representation to the style target distribution. The goal is that the configurations closer to
thetargetdistributionhavelowervaluesofenergy. OptimizationisperformedusingStochasticGradient
LangevinDynamics,whichsimulatesarandomwalktolowenergyregions. Tovalidatetheirresultsthey
use the public databases OpenBHB abd SRPBS. They compare the DLEST model to several models:
non-learning techniques (Histogram matching HM and Spectrum Swapping), GAN models (CycleGAN)
andImUnity[8]. Theyperformfourdifferentevaluationtaskstocomparetheperformanceoftheirmodel
to the rest. The first task consists of histogram comparison and visualization of features, where the
DLEST model shows a better alignment of histograms, specially in the white matter, the gray matter
and CSF intensity pikes. In the second task they evaluate if the model is able to eliminate enough
information related to the site by using a classifier. A clasfying network is trained to distinguish images
fromdifferentsites. TheDLESTachievesabalancedaccuracyof0.336,significantlylowercomparedthe
other methods, which implies that the model is able to erase most of the irrelevant information. Next,
a segmentation task shows that the model obtains consistent and robust results during segmentation,
outperforming traditional methods. Finally, they evaluate whether the model can generate synthetic
MRI data that is able to mach the style of a certain site. DLEST obtains high values of SSIM and
Pearson correlation coefficient, similar to the ones obtained using pairs of real images from the site,
meaning that the model captures the site variability correctly.
Using the same a anatomical/contrast latent space framework, authors in [16] propose a harmoniza-
tion model that leverages the inversion of the MR signal equation. This signal equation describes how
MR images are generated from the interaction of the tissues and the magnetic field. Inverting this equa-
tion implies decomposition of the MR image in its fundamental parts [34]. Authors in this work use this
inversion to create a disentangled latent space composed of two components: an anatomical latent space
β and a contrast representation θ. To do so, they use a encoder-decoder model based on U-Net, where
the decoder takes the concatenation of β and θ to produce synthetic images. To encode the anatomical
representation they use one-hot encoding H ×W ×C , with H and W being the height and width
β
of the image; and the one-hot encoding being along a channel C . This encoding allows restriction of
β
information that can be passed through this latent space, since a flexible β may pass extra information
about the contrast. On the other hand, contrast is encoded into a vector C . To train this model they
θ
network is fed with a pair of T1-w and T2-w images, which contain the same anatomy, and so their β
representationsmustbeequal,whilethecontrastmapswillbedifferent,duetothedecompositionofthe
MRsignalequation. Inordertovalidatetheirmodeltheytraineditusing90subjectsfromthreedifferent
sites. Using SSIM and PSNR they computed the performance of the model by comparing harmonized
images with real scans from traveling subjects. Using these subjects, they compared the scanned image
in site A and its harmonized counterpart with the scan in site Be, finding that the harmonized images
have a dramatic increase in the SSIM compared to the real image.
In [56], the authors propose a model for translating 3T MRI images to 7T MRI images using ad-
versarial learning. They highlight that a common issue with supervised methods is their reliance on a
large number of paired 3T-7T images, which are difficult to obtain. To address this, they introduce a
wavelet-based semi-supervised adversarial network that leverages the wavelet transform, which decom-
poses images into various frequency bands and spatial locations. This approach enables the analysis of
theimagestructureatmultiplelevels,capturingbothfine-graineddetailsandglobalfeatures. TheGAN
12
operates in this frequency domain, incorporating both spatial and frequency information, allowing the
model to synthesize details at various frequency levels while preserving the high-resolution anatomical
features typical of 7T images. The total loss function includes an adversarial loss that matches the
distribution of synthetic images with that of the target images, a pair-wise loss that uses 3T-7T pairs to
measure pixel-wise differences between the generated and real 7T images, a cycle consistency loss that
ensures the transformation from 3T to 7T and back to 3T results in a matching image, and a wavelet
loss that enforces the preservation of structural details.
To validate their model, the authors use 15 pairs of 3T-7T images and assess performance using the
PSNRandSSIMmetrics. Theycomparetheirresultstothoseoffourother7Timagesynthesismethods:
MCCA [3], RF [2], CAAF [50], and DDCR [77]. Their model outperforms the others in both metrics.
Additionally, theyconduct an ablationstudy to evaluate the contribution ofeachmodule to themodel’s
performance. Theydemonstratethatomittingthepair-wiselosssignificantlyreducesperformance,while
removing the cycle consistency loss leads to a decrease in PSNR values. Finally, they conclude that the
wavelet loss is essential for capturing fine anatomical details.
4.3 Visual reconstruction using fMRI
Image reconstruction is a topic that has gathered significant attention in recent years. In computer
vision we receive an image (e.g. a natural image) and try to either reconstruct that image (for image
synthesis) by learning its distribution, or identify different elements within the image (for segmentation
or parcelling tasks). In neuroimaging, researchers haven been trying to unravel the neural encoding
of visual information by characterizing how this information X is codified by the brain into latent
representations Z. In general, the idea is to find the distribution p(X|Z) that leads from visual input
to neural representations that are later used to generate cognitive processes and responses. To do so,
researchers use the information from fMRI images to see how different parts of the brain react to visual
input. These activation maps contain relevant information of the latent representations, which can be
analysed to understand the encoding process.
For instance, authors in [63] use a GAN approach model to reconstruct natural images from fMRI.
Here, they use a codifying model based on a feature-weighted receptive field (fwRF) model, described
in previous work [62]. This model predicts brain activity in response to visual stimuli, generating an
activity vector V from an image X, allowing prediction of fMRI responses to new images. An AE is
applied to reduce dimensionality of V, generating a latent code C that captures the relevant features of
the brain activity. A conditional GAN is trained using the latent representation. The GAN is trained
to generate images that are consistent to such code C.
They found that the model accurately predicts brain activity in response to natural images, and
recovers well-known patterns of the brain in visual processing. They also found that reconstructed
images were not easily recognizable by eye, but relevant features such as dominant lines were preserved
by the model.
Authorsin[61]proposeamethodtoreconstructnaturalimagesfrombrainactivitymeasuredthrough
fMRI scans using a GAN-based approach. During the experiments, subjects are presented with natural
images from a dataset while their brain activity is recorded. A GAN is first trained on the dataset of
natural images to learn a latent space capable of generating realistic reconstructions. Simultaneously, a
predictivemodelistrainedtomaptherecordedfMRIsignalstotheGAN’slatentspace. Toachievethis,
fMRI data are used as input to the predictive model, while the corresponding latent space coordinates
of the GAN serve as the target labels.
This setup allows the predictive model to learn the relationship between brain responses and the
latent space coordinates of the GAN. Once trained, the system is tested by presenting subjects with
novel images not included in the original dataset. Their fMRI responses are processed by the predictive
model to generate latent space coordinates, which are then fed to the GAN. The GAN subsequently
reconstructs an approximation of the presented image based on the derived latent space representation.
The main idea is that the fMRI responses are linearly related to the latent representations. To validate
their model, authors use three different datasets: BRAINS, that contains hand-written numbers, vim-1
with gray-scale natural images, and Generic Object Decoding, with gray-scale images of objects. They
found that general structured of the images was reconstructed by the model but the quality depended
on the dataset. For BRAINS dataset the reconstructed images were recognised by the subjects in 54%
of the cases, while in vim-1 and Generic Object Decoding was a 66.4% and 66.2% respectively.
Authorsin[31]proposeaverysimilarmodelbutwithaVAEapproachtoreconstructnaturalimages
fromfMRIresponsesfromsubjectspassivelywatchingnaturalvideos. TheVAEisusedtoextractlatent
13
representations of every video frame, while a regression model is trained to link the fMRI response to
the latent space, like in the previous work. The training of the regression model is simple. Since during
training we know which video is the subject watching, we can link the fMRI obtained with the latent
space of the video generated by the VAE. Once the model is trained we can show the subjects new
videos, obtain their fMRI, use the regression model to estimate the latent space, and decode it to obtain
thereconstructedvideo. UsingfMRItoreconstructthevideos, theyfoundthatthereconstructionswere
blurrybutwereabletocapturerelevantfeaturessuchasposition,approximateshapeoftheobjects,and
contrast. To further validate their model they used the inverse process. Using the latent representation
of a video, they used the regression model to predict the cortical activity. They found that the early
visual areas (V1, V2 and V3) showed a higher prediction performance compared to the higher visual
areas. However, the authors claim that the model demonstrate a modest level of success.
To address this issue, the authors in [65] propose using a latent diffusion model (LDM) approach
to reconstruct natural images from brain activity. This method leverages not only visual information
but also semantic representations to enhance the fidelity of the reconstructions. Using the LDM, they
achieve high-resolution images with substantial semantic fidelity. The process begins with obtaining
a latent representation z using an autoencoder (AE) trained on natural images, while a linear model
learns to map the fMRI data to the latent representations. The AE decoder then reconstructs an
initial image X , which possesses approximate visual fidelity but lacks fine semantic details. Next, the
z
authors introduce a textual representation c linked to the fMRI responses from high visual areas. This
representationencodessemanticinformation(incontrasttothestructuraldetailstypicallyfoundinlower
visualareas). Themodelsubsequentlycombinesz andcasinputsinadiffusionprocessbasedonaU-Net
architecture. Finally, the AE decoder generates the reconstructed image X , which incorporates both
zc
the visual information from lower areas and the semantic fidelity contributed by high visual areas. To
validate their model, the authors conducted an ablation analysis using three configurations: a model
with only z, one with only c, and a model integrating both z and c. As expected, the model using
only z provided a good general visual structure but lacked semantic fidelity. Conversely, using only
c resulted in images with rich semantic content but poor visual precision. When both z and c were
used, the model successfully captured both visual and semantic content. For better interpretability, the
authors examined the correspondence between the representations and brain activation patterns using
L2-regularized linear regression. They found that z representations correlated with patterns in early
visual areas (associated with borders and textures), while c representations were linked to late visual
areas (associated with semantic information). The combined zc representations exhibited activation
patternsbalancedacrossbothearlyandlateareas. Additionally,toenhanceinterpretability,theauthors
analyzed the progression of zc representations throughout the diffusion process. They observed that
visual information is reconstructed during the early stages, while high-order semantic details emerge in
the later stages.
As the presence of semantic information has proven to be key in reconstructing natural image from
fMRI, the authors in [29] further improve this by proposing Cortex2Image, a framework that aims to
reconstructnaturalimagesfromfMRIdatawithhighsemanticfidelityanddetailedfeatures. Themodel
comprises two main components: Cortex2Semantic and Cortex2Detail, integrated with a pre-trained
IC-GAN generator. Cortex2Semantic employs a spherical convolutional network to map fMRI brain
responses to semantic image features, leveraging spherical kernels to account for the spatial topology of
the cortical surface. The fMRI data is represented as multichannel data on a standardized icosahedral
mesh of the brain, enabling feature extraction related to semantic categories using residual blocks and
dense layers.
Cortex2Detail complements this by mapping brain responses to fine-grained image details, such as
color,size,andorientation,usingavariationalapproachtomodeltheinherentuncertaintyinfMRIdata.
The combination of these components with IC-GAN facilitates the generation of high-fidelity images by
integrating semantic and detailed features. Importantly, training Cortex2Detail alongside the IC-GAN
generator in an end-to-end manner significantly reduces computational overhead compared to previous
optimization-heavy methods.
To validate the model, the authors compared Cortex2Image with RidgeSemantic (linear ridge re-
gression), RidgeImage (linear regression that predicts both semantic and fine-grained details) and Cor-
tex2Semantic. They used both high-level (latent space distance of SWAV and EfficientNet-B1) and
low-level (SSIM and pixel-wise correlation) metrics. They found that the model achieves high semantic
reconstruction, such as zebras on grass or a train on tracks. However, fine-grained details are harder
to achieves, but the model is able to reconstruct some of them, like the black strips of the zebras, the
orientationofaplane. Theyalsofoundthatmostfailereswereduetothepresenceofmultipleobjectsor
14
categorieswithinanimage, orduetoobjectsbeingpartiallyoccluded. Also, theyperformedanablation
study comparing the full model to Cortex2Semantic, finding that the later model is not able to capture
fine details like orientation or spatial distribution of the objects.
The authors in [51] introduce a model that outperforms the two previous works [65] and [29], which
consists of a VDVAE and LDM approach named Brain-Diffuser to improve both semantic and visual
reconstruction of fMRI reconstruction of natural images. VDVAE is a hierarchical VAE model that
introduces several dependent latent variable layers. This VDVAE is used to extract low-level features
from the images, which serve as an initial guess. Next, as usual, they use a regression model to link the
fMRI responses to the latent space. In the second stage they use a latent diffusion process conditioned
onpredictedmultimodal(textandimages)featuresusingCLIP-VandCLIP-T[57]whichlinkthevisual
and textual representations into a common space. Finally, the reconstructed image contains both visual
and semantic information. In general, the authors found that the reconstructed images capture most
of the layout and semantics of the ground truth. However, like the previous work, the Brain-Diffuser
approach fails to reconstruct under certain situations such as occlusion or objects. They compare their
results to [65], [29], using SSIM, AlexNet(2), PixCorr, InceptionV3, CLIP, EffNet-B and SWAV as
performance metrics. Authors claim that theirs is the best performing model by a decent margin for all
the quantitative metrics. Next, an ablation study showed that VDVAE have a better performance but
only for the low-level metrics while it obtained the worst results for the high-level metrics. On the other
hand, the Brain-Diffuser model without the VDVAE is not able to capture visual structure, suggesting
that the inicial guess of the VDVAE is necessary but not sufficient. Another ablation study showed
that CLIP-T aids to achieve both low-level and high-level performance. The high-level improvement of
CLIP-T is expected since text is used to add semantic fidelity, however, the low-level improvement can
be explained due to the layout of the image (such as number of objects and orientation, which can be
addedtothetext). Next,ablationofCLIP-Vdoesnotreducelow-levelinformationasmuchasexpected.
Finally, to improve interpretability of the model, the authors perform a ROI analysis to see the relation
between brain region and the components of the model. The found that V1-V4 visual regions carries
more information from the VDVAE features, while specific regions (words, faces, places, etc.) carries
more information from the features of CLIP.
Authors in [30] propose a model with high semantic and visual information, outperforming all the
previousstate-of-the-artworksthatwehavepreviouslydiscussed. TheMindLDMmodelisbasedonLDM
andVAE,usedforreconstructingnaturalimagesfromfMRIusingbothvisualandsemanticinformation.
The model has 4 stages: pretraining of fMRI feature extration, alignment of fMRI and text in CLIP
space,generationofdepthmapsusingVDVAE,andLDM.First,latentrepresentationsofthefMRIdata
is obtained using a MAE encoder. Next, using CLIP common feature space, multi-tag text is aligned to
fMRIembeddings. Inordertoobtainbettervisualreconstructions, aVDVAEistrainedtoobtaindepth
maps from the natural images and, using a regressor, depth maps can be obtained from the fMRI input
images, which are used to guide the generation of the images. Finally, a diffusion process generates the
image conditioned on the fMRI latent, the text and the depth map.
The authors validate the model using both low-level and high-level performance metrics. For the
low-level metrics, they evaluate the similarity of basic features between reconstructed image and ground
truth, using SSIM, PixCorr, AlexNet(2) and AlexNet(5), which compare features such as borders or
texture. For the high-level evaluation they use InceptionV3, CLIP, EffNet-B and SWAV. They compare
theirmodelto[29],[65],[51]. QuantitativeresultsshowthatMindLDMoutperformstheothermodelsin
key metrics, showinghigher valuesin high-levelmetricsandgoodresults inlow-levelmetrics, suggesting
high precision in both structural and semantic similarity. Furthermore, the authors also perform an
ablation study by removing elements such as the fMRI/text alignment or the use of depth maps to
guidereconstruction. Theyfoundthatthemulti-tagapproachwasmoreeffectivethannaturallanguage.
Also, the study showed that the fMRI/text alignment is crucial to the model, leading to large semantic
deviations if not implemented. On the other hand, depth maps proved to be necessary to achieve
structural reconstruction. This ablation study indicates that every part of the model is necessary to
reconstruct the images.
4.4 Brain ageing analysis
The brain undergoes a complex and multifaceted process shaped by changes in both its structure and
cognitive function throughout an individual’s lifespan. This process is further influenced by factors such
as sex, age, and external elements like social interactions, making the modeling of brain changes over
time a challenging task. Despite these challenges, such modeling is essential for distinguishing between
15
normal brain evolution and the onset of neuropathologies. For example, analyzing aging patterns can
help determine whether an older individual is experiencing typical age-related cognitive decline or the
early stages of a neurodegenerative disease. Similarly, it can provide insights into the developmental
stages of the neonatal brain, enabling the identification of potential issues in early brain development.
Authors in [72] use a VGG-like network consisting of convolutional layers with small kernels to
synthesize 2D MRI images of subjects at older age t, given a ground truth MRI t . They represent
0
age as ordinal vectors, ensuring that difference between vectors is correlated to difference between ages.
The model consists of a generator based on a transformer, which generates the samples at t from an
input at t , and a discriminator that is trained to distinguish the ground truth at t and the generated
0
image at the same age to compare the performance of the model. The generator takes the image x as
t0
input encodes it to obtain a latent representation, concatenates with the age difference vector v , apply
d
the transformer, and outputs the image xˆ. This image is fed into the discriminator along with v and
t d
evaluates whether the image looks real and consistent with the target age. The total loss comprises the
adversarial loss, L , and an identity preservation loss, L . The latter ensures that the identity of
GAN ID
subjectsremainsintactthroughouttheagingprocessbyassessingwhetherthedifferencebetweenxˆ and
t
x is within acceptable bounds for a given age difference d.
t0
Due to the insufficient availability of longitudinal data, the authors train a predictor, denoted as
f , to estimate the age of the subjects in the dataset. The performance of this predictor is evaluated
pred
by computing the difference between the predicted and actual ages. The results indicate that brain
changes are minimal during the early stages of aging. However, after the age of 52, significant brain
changes occur, capturing features that align with existing literature on brain aging.
In addition, the authors conduct a longitudinal analysis using a small subset of the ADNI dataset.
They report a mean absolute error (MAE) of 0.08 for their model, which outperforms a conditional
GAN-based approach with an MAE of 0.21, and a CycleGAN approach with an MAE of 0.20. These
results suggest that the images generated by the proposed model are realistic and effectively preserve
the identity of the subjects.
Authors in [12] use a VAE approach using PET scans and age data of the subjects. In this work,
theyhypothesizethatlatentfeaturesofthePETscansdonotchangewithtime. ToVAEtaketheinput
PET at time t and obtain its latent representation, which is then concatenated to the future age t and
0
decoded to obtain the reconstructed image xˆ . To validate the model they compared the follow-up PET
t
of the subject to the reconstructed image using delta maps. They found that the average predicted
changesofthebrainwerecorrelatedtotherealchangesofthefollowup. Thedeltamapswerepositively
correlated with the delta maps of the real images. This seems to indicate that the model was able to
capturechangepatternsofbrainmetabolismthataligntotheobservedrealchanges. Moreover,thestudy
showed a good capability to capture subject variability. Some subjects showed a global diminishing of
metabolismsuggestingnormalageingorcognitivedeterioration,whileothersubjectsshowedanincrease
of metabolism in specific areas such as the frontotemporal cortex, which may indicate a compensation
of the metabolism. In general, the latent representation were able to provide information about how the
brain metabolism changes over time.
In addition, the authors developed a model that added APOE4 state information along with the age
to the latent representation in order to see how it affects the development. They found that adding
APOE4 produced a lower metabolism in the hippocampus and amygdala. From the age of 60 onwards
they found that metabolic differences were less pronounced, suggesting that changes related to APOE4
aremoresignificativeintheearlystages. TheyalsofoundusingdeltamapsthatAPOE4relatedchances
showed a rapid decrease in metabolism in the occipital areas.
Authors in [11] propose a longitudinal model for generating synthetic images that simulate patient
trajectories, filling in the missing data between patient visits, extracting features of MRI images using a
VAE model. The trajectory of a patient is modeled through a linear equation
d
(cid:88)
l (t)=eηi(t−τ )·e + λk·e , (6)
i i 1 i k
k=2
where η is a velocity parameter that quantifies the progression of the individual, τ is the delay, and λ
i i i
arespatialparametersthatdeterminetheshapeofthetrajectoryintheeuclideanspace. Themodeluses
a variational approach where the latent variables represent the parameters of the trajectory equation,
learningthedistributionofsuchvariables. Theyassumethatthoseparametersareaindependentforeach
subject. If we image modeling of an AD subject, η would indicate velocity of the progression, τ the
i i
onset of the degeneration, and λ how the changes affect the space (e.g, size of the lesion). The authors
i
16
assume that the latent space is factorized as q (z |x )=q (η |x )q (τ |x )q (λ |x ). Once the trajectory
ϕ i i ϕ i i ϕ i i ϕ i i
l (t ) has been evaluated, the decoder of the VAE transform the features into a reconstructed image.
i j
TheauthorsvalidatethemodelusingMSEandSSIMperformancemetrics. UsingtheADNIdataset,
they found that the trajectories showed an increase of the ventricles over time, which is an indicator of
the progression of the disease. The authors claim that although the trajectories seem to be realistic, the
model requires more validation by expert physicians.
Authors in [9] use a VAE coupled with a regression model to identify age-related patterns. The
regression model uses input data to predict the age. The loss term includes the usual ELBO loss of the
VAE and an extra term for the regression that measures how precise is the model predicting age, by
usingtheKLdivergencebetweenthecomputedagedistributionq(c|x)andtherealagedistributionp(c).
In the latent prior they include a term p(z|c) to assure that the latent representations z are influenced
by the age c.
Theauthorstraintwodifferentmodels,oneforfemalesubjectsandasecondmodelformalesubjects.
In order to interpretate the latent space they use SAGE (Shapley Additive Global importance) fro each
dimension, that quantifies the predictive power of each independent feature of the latent space and
computethecontributionofeachregiontotheprediction. Also,theyusedOLS(OrdinatyLeastSquares)
regression to estimate the relation between each risk factor and each latent variable. They found that
the VAE+regression model outperforms VAE+MLP model. Using SAGE, they where able to identify
thecharacteristicregionsvulnerabletodementia,whichwherethenlinkedtoriskfactorsandtrajectories
relatedtoage,clinicalandcognitivevariables. UsingtheOLS,theyfounddifferencesbetweenthefemale
andthemalemodel,wheremaleshadmorecorrelationsindicatinggoodhealth,whereasthefemalemodel
showed less correlations but an increase of the risk factors.
In addition, they found that both their VAE-regression model and the VAE-MLP overestimated age
of young individuals and underestimated age of old subjects. Moreover, the were able to identify four
different key dimensions in the female model, some of them related to dementia (frontal and parietal
lobes, and hippocampus). In the male model they also found four dimensions, some related to dementia
(temporal (entorhinal) and parietal lobes, hippocampus and frontal lobe. In conclusion, the model is
abletoidentifypossiblebrain-relatedtrajectoriesandassociatethemwithdementia-relatedriskfactors.
Authors in [5] use an Implicit Neural Representation (INR) approach to model and represent con-
tinuous functions like 3D images instead of representing data as pixels, to model the development of
neonatal brain during the third semester of pregnancy. They model images as functions that depend on
aspacedomainΩ(pixelcoordinates),anormalizedtimeT andalatentspacethatrepresentstheidentity
of the subject Λ, which define a function f (x,t,z). During training, the intensity value of the image is
θ
computed using f and minimized such that the MSE between predicted intensity Iˆ and I are similar.
θ x x
The idea is that the latent vectors are the same for all the images of a single subject (SSL, Subject
Specific Latent), like the previous subject, aiding the model to capture its identity, while time is used
to model the development and ageing. In order to tackle the problem of limited longitudinal images for
some subjects the authors use a Stochastic Global Latent Augmentation (SGLA) where a global latent
isusedduringtrainingforthesesubjects. Thisallowsthenetworktolearntopredictimagesatdifferent
ages. During training, optimization allows the disentanglement of identity and age. The global latent is
shared among every subject with only one image and in every iteration there is a probability that the
model will use the global latent instead of the specific subject latent. With this training approach the
model is forced to learn how the features of the subjects can be generalized from the age t, allowing to
simulate how the images would be during the different stages of the development even when there is no
enough data.
Tovalidatethemodeltheycomparetoadenoisingdiffusionmodelwithgradientguidance(DDM+GG).
For each subject in the dHCP (T2 MRI) dataset they consider two scans taken at different temporal
points. UsingthePSNR,SSIMandMAEmetricstheyfoundthattheirmodeloutperformedthediffusion
model. Theyalsoperformedanablationstudycomparingtheperformanceofthemodelwithandwithout
every combination of SSL and SGLA, finding that the best performing model was the one that included
both methods. Using measures of the head circumference (HC) they found that the needed both SSL
and SGLA to find correlation between HC of the predicted image and the ground truth. Qualitatively
they found that the reconstructed images matched the size correctly but had some difficulties capturing
details such as the exact shape of the folds, due to the bottleneck of the latent space.
17
4.5 Disease classification
Generativelatentmodels,particularlyVariationalAutoencoders(VAEs)andtheirvariants,havebecome
pivotalinthefieldofneuroimagingfordiseaseclassification. Thesemodelsareuniquelysuitedtoextract
structured, low-dimensional latent representations from high-dimensional neuroimaging data, capturing
complex patterns while preserving critical information about disease-related variations.
The latent representations obtained from generative models offer several advantages: they provide
a compact and interpretable feature space for classification tasks, enable the exploration of disease
progressiontrajectories,andallowtheintegrationofuncertaintyintopredictions. Thissectionhighlights
recent advancements in applying generative latent models to neuroimaging for classifying neurological
and psychiatric diseases, focusing on the extraction and utilization of latent spaces to enhance both
predictive performance and interpretability. By leveraging these latent spaces, researchers not only
improve classification metrics but also gain insights into the neurobiological underpinnings of disorders,
paving the way for more personalized and precise diagnostics.
Authors in [4] propose a VAE+MLP model to predict the status (Normal or Alzheimer) of a patient
6 months after a visit, using MRI. The VAE extract relevant features of the MRI which they use to see
whichbrainareascontributethemosttothemodelandquantifythedistributionofpossibletrajectories
ofthesubject. Topredictthestatuslabeltheyuseagenerativeclassifierthatmodelsthejointdistribution
of features and labels p(X,Y).
ThelatentrepresentationsoftheVAEarefedtotheMLPtopredictthestatus,usingacross-entropy
lossalongwiththeusualELBOoftheVAE.UsingtheVAEapproachtheauthorsobtainanaccuracyof
74.40±0.01 and a F1-score of 0.66. Additionally, they perform a risk analysis. For each MRI in the test
set, they take 100 samples from the latent space and predict the future disease status. They find that
59% of the samples are not at risk, while the rest show varying degrees of risk. In contrast, the other
models used for comparison (CNN and CNN-AE) predict that only 2.79% and 9.16% of the samples
are not at risk, suggesting that the VAE approach provides a more robust prediction of risk compared
to the other models. Qualitatively, using relevance maps, the authors found that the model focuses on
anatomical specific areas for the prediction, such as the cerebellum, the neocortex and the brainstem,
which are associated with the progression of Alzheimer’s disease. They claim that their study predicts
future progression, while other studies focus on correlating changes with current symptoms. However,
they note that there are limitations in terms of which specific features the model is using within this
regions, suggesting that further research is needed to understand the model’s behaviour.
On the other hand, authors in [78] use a multi-modal fusion of different data modalities, such as
MRI or PET, using ADNI scans, to classify dementia patients. They aim to create a model that is
both useful in clinical setting and interpretable. To perform the multi-modal learning the authors use a
Negative Matrix Factorization (NMF) which is a technique that decomposes a data matrix into simpler
components, with the property of each matrix being non-negative. If we have a matrix X of dimensions
d×n where n is the number of samples and d the number of features, the method targets to decompose
X into two matrices B, containing the fundamental features and H that indicates how to reconstruct
the data. The idea is that the product B·H resembles X. In the multi-modal case we have B(v) and
H(v), one pair of matrices for each independent modality. However, using the model
V
(cid:88)
min ||X(v)−B(v)H(v)||2, (7)
v=1
there would be no relation between different modalities. In order to address this issue, the authors use a
common H matrix for all modalities. In order to obtain deep latent representations, they use the deep
NMF model, which consists on decomposing each matrix into lower representations:
X(v) ≈B(v)...B(v)H . (8)
1 L L
Using these latent representations the authors classify Alzheimer patients in the ADNI dataset into
three categories: Normal Control (CN), Mild Cognitive Impairment (MCI) and AD, utilizing MRI and
PET scans. They compare their model to other state-of-the-art classification models, using accuracy,
sensitivity,specifityandF-scoreasperformancemetrics. Thefoundthattheproposedmodeloutperforms
othermethodssuchasSVM,MKL,shallowNMF,etc. inallmetricvaluesonallclassificationtaks. They
also found that all methods using multi-modality data outperforms all models using a single modality,
demonstrating its robustness. In addition, they perform a pMCI vs sMCI classification task, showing
that the model shows the highest accuracy values compared to other state-of-the-art methods.
18
The authors of [48] propose a model designed to improve the diagnosis of psychiatric disorders,
particularly schizophrenia and bipolar disorder, by addressing challenges such as overfitting and the
extraction of irrelevant patterns common in classification tasks. The proposed model employs a deep
generative model (DGM) that calculates the posterior probability of a subject’s state based on their
fMRIdata. Toextractfeaturesfromtheimages, themodelincorporatesamodifiedVAEthatintegrates
the diagnosis label (control or disease) directly into its structure. This integration enables the model
to distinguish relevant patterns associated with the diagnosis from irrelevant variations, such as noise
or frame-wise variability. The inclusion of the label as an independent variable explicitly conditions the
modeltoassociatemeaningfulfeatureswiththelabelwhiledisentanglingandignoringirrelevantfactors.
The authors compare their model against several baselines, including functional connectivity (FC)-
based methods, autoencoders (AE), multilayer perceptrons (MLP), Gaussian Mixture Models (GMM),
anddynamicalmodelsbasedonMarkovchainsandLSTM.Theproposedmodelsignificantlyoutperforms
thebaselinesinmultipleperformancemetricsforschizophrenia, achievingabalancedaccuracyof85.3%,
along with superior Matthews correlation coefficient and F1-score. Additionally, it identifies key brain
regions, such as the thalamus, which is strongly associated with schizophrenia. For bipolar disorder, the
balanced accuracy reaches 81.5%, with other metrics comparable to the baseline models. In this case,
the regions most influential for classification are the cerebellum and several frontal regions, consistent
with findings in the literature.
Authorsin[6]dividepsychiatricspectrumdisordersintoneurophysiologicallydefinedsubgroupsusing
a generative embedding approach. They hypothesize that, within a Dynamic Causal Modelling (DCM)
framework,patientswithschizophreniacanbeclassifiedintosubgroupscorrespondingtodistinctclinical
subtypes, particularly based on the severity of negative symptoms assessed by the PANSS-NS scale.
First, they extract temporal series from the visual, parietal, and prefrontal regions during a working
memorytask. TheDCMframeworkisemployedtoinfereffectiveneuralconnectivityacrosstheseregions,
modelingthebrainasadynamicalsystemwhereneuronalpopulationsinteractviasynapticconnections.
This generates a feature space that encodes effective connectivity parameters. Using Bayesian inference,
the posterior distribution of these parameters is estimated for each subject, and these estimates are
then converted into feature vectors representing the strength of specific connections. Each subject is
embeddedina12-dimensionalspacethatencapsulatesthearchitectureoftheirbrainnetworkundertask
conditions.
Using a support vector machine (SVM) classifier, they distinguish controls from patients with a
balanced accuracy of 78%. Subsequently, applying Gaussian mixture models (GMM) for unsupervised
clustering, they identify distinct subgroups. Restricting the analysis to patients reveals three neuro-
physiologically defined clusters with unique connectivity patterns. Critically, these clusters map onto
differences in clinical symptomatology, specifically in the severity of negative symptoms, thus validating
the relevance of their approach.
The authors in [18] propose a Spatiotemporal Attention Autoencoder (STAAE) for ADHD classi-
fication by capturing long-distance dependencies of global features using fMRI data. The attention
mechanism addresses the limitations of RNNs and CNNs when handling high-dimensional fMRI data.
This mechanism facilitates the capture of global dependencies through the use of query, key, and value
matrices[67]. TheSTAAEemploysanencoder-decoderframework,wheretheencodermapssequencesof
symbolicfMRIrepresentationsintointermediaterepresentations. Theserepresentationsarethenutilized
to estimate matrix coefficients via Lasso regression, constructing spatial maps that link latent features
to brain regions associated with Rest State Networks (RSNs). By leveraging these RSNs, the authors
create a brain atlas that combines the ROIs of all RSNs. Using this functional connectome derived from
the RSNs, they classify ADHD subjects.
VisualinspectionoftheseRSNsconfirmstheirinterpretabilityandalignmentwiththeRSNsreported
in ADHD literature. To benchmark their method against other baseline approaches, the authors use the
overlap score, demonstrating that the STAAE accurately identifies intrinsic RSNs. Furthermore, they
comparetheclassificationaccuracyoftheSTAAEwiththatofSDL,RAE,SVM,andICA,showingthat
the STAAE outperforms these methods across all four datasets. However, the authors acknowledge that
the effects of hyperparameters remain insufficiently explored, highlighting the need for further research
in this area.
4.6 Functional brain networks
Functional brain networks (FBNs) play a critical role in understanding neural connectivity and activity.
Several recent studies have leveraged deep learning techniques to uncover these networks from fMRI
19
data. For instance, the work in [55] introduces a deep variational autoencoder (DVAE) to derive latent
representations from 4D fMRI data signals (3D volumes + temporal axis) associated with Functional
Brain Networks (FBNs). Using the latent space, they apply Lasso regression between the original fMRI
data (converted to a 2D matrix by concatenating time and voxel dimensions) and the latent variables.
This generates a coefficient matrix that is mapped back to the 3D brain space. Each row of the matrix
is transformed into a 3D image representing an FBN, effectively acting as a filter to identify the voxels
where each latent variable is most relevant for functional activity. This approach links latent variables
to specific FBNs.
Temporal series are then extracted from regions of interest (ROIs) defined by these FBNs, and Pear-
son’s correlation coefficients between ROIs are computed to construct functional connectivity matrices
for each subject. These matrices serve as input for supervised classifiers, including linear and radial
SVMs, Random Forest, and a custom-designed deep neural network (DNN).
To validate the DVAE model, the authors compare it with Sparse Dictionary Learning (SDL) and
a standard autoencoder (AE). The results show that DVAE outperforms AE in deriving meaningful
FBNs, particularly in small datasets, as it avoids overfitting and captures more generalized features.
Quantitative results demonstrate robustness across datasets and superior performance in three out of
the five centers analyzed in the ADHD-200 dataset.
ThestudyhighlightsthehierarchicalorganizationofFBNsderivedfromdifferentDVAElayers. Shal-
low layers capture simple connectivity patterns, while deeper layers reveal complex, global interactions.
This hierarchy is validated through the Inheritance Similarity Rate (ISR), which measures how FBNs
from shallower layers overlap with and contribute to those in deeper layers. Results indicate that multi-
ple partially overlapping FBNs in shallow layers combine to form more complete FBNs in deeper layers,
aligning with findings in the literature.
Finally,theDVAEpipelinedemonstratescompetitiveperformanceforADHDclassification,achieving
higher accuracy in some centers compared to other fMRI-based methods and offering robust results
despitevariabilityindatasetsizeandscanningparameters. TheauthorsemphasizetheDVAE’scapacity
to extract hierarchical and biologically meaningful features, making it a promising tool for fMRI data
analysis.
The authors in [76] propose the Deep Multimodal Brain Network (DMBN) model, which integrates
structural and functional brain networks, representing them as graphs. The structural graph acts as
a scaffold, imposing constraints on functional activity, while functional activity gradually influences
the structural anatomy over time [7]. The DMBN model employs an encoder-decoder architecture to
translatestructuralnetworksintofunctionalrepresentations. Initially,noderepresentationsareextracted
using a convolutional kernel on the graphs, capturing connectivity patterns among neighboring nodes
in the structural network, which are then used to learn the functional network representation. To
enhancethemodel’sabilitytodynamicallyadaptconnectionweights,anattentionmechanismisapplied,
enabling the identification of nuanced relationships between nodes. The model captures the non-linear
andindirectrelationshipbetweenstructuralandfunctionalconnectivity,learninganeffectivetranslation
from one modality to the other. The decoder reconstructs the functional network from the structural
node representation, capturing both direct connections and complex, non-linear interactions.
The authors benchmarked the DMBN model against five state-of-the-art methods, including three
machine learning models (tBNE [10], MK-SVM [19], mCCA+ICA [64]) and two deep learning models
(BrainNetCNN [36], Brain-Cheby [40]). DMBN outperformed all the baselines in a gender prediction
task,achievinganaccuracyof81.9%anda10%improvementintheF1-score. Usingsaliencymaps,they
identified the top 10 brain regions most relevant to the prediction task. These regions included cortical
areas such as the orbital gyrus, precentral gyrus, and insular gyrus, as well as subcortical areas like the
basal ganglia, which are critical for cognitive regulation, motor and emotional control, and likely exhibit
gender-related differences. Moverover, an ablation analysis shows that the use of multimodal networks
is essential to obtain a representative latent space, along with the attention mechanism.
Furthermore, the authors applied DMBN to a disease classification task using the PPMI database
for Parkinson’s Disease. The model demonstrated superior performance compared to baseline models,
achieving a 5−9% improvement in accuracy. Additionally, DMBN identified 10 key regions associated
with Parkinson’s biomarkers, such as the bilateral hippocampus and basal ganglia, which are well-
documented in neuroimaging studies of Parkinson’s Disease.
The study by Dong et al. [17] employ a 3D residual autoencoder (ResAE) to model deep represen-
tations of fMRI data. Their approach simplifies functional brain network (FBN) estimation by using a
deepautoencoder(AE)tolearnlatentrepresentationsoffunctionalactivity. Theselatentrepresentations
are subsequently used for FBN estimation via lasso regression, enabling the construction of a functional
20
activity network matrix. Each row of the matrix maps to the 3D brain space, generating an anatomical
spatialmap. TheauthorscomparethegroundtruthHemodynamicResponseFunction(HRF),measured
as the response to stimuli during tasks, with the temporal features generated by the ResAE, finding a
high degree of precision in the match.
To further validate their approach, the authors compare the performance of their deep ResAE with
a shallower ResAE, demonstrating that the deep ResAE extracts features that align more closely with
the ground truth than those obtained by the shallower model.
Moreover, the temporal features extracted by the deep ResAE are mapped back to the MNI space
to analyze the corresponding spatial maps. The study reveals that each hidden layer learns functional
networksofvaryingcomplexity,dependingonthedepthofthelayer. Theauthorsfindthattheresulting
FBNs are interpretable and consistent with findings in the existing literature. Using overlap rates, they
showthatthedeepResAEsignificantlyoutperformstheshallowermodelincapturingmeaningfulFBNs.
Finally, their model successfully identifies resting-state networks (RSNs) even during task perfor-
mance. Notably, the model captures principal RSNs, including the visual, default mode, sensorimotor,
auditory, executive control, and frontoparietal networks. This demonstrates that the model can dynam-
ically identify both task-related FBNs and active RSNs.
4.7 Multimodality integration
Generativelatentmodelshaveproventobehighlyeffectiveinintegratingmultimodalneuroimagingdata
by projecting diverse data sources into shared lower-dimensional latent spaces. These models enable the
extraction of unified representations that capture complementary aspects of the underlying data while
maintaining interpretability and scalability.
Forinstance,theVariationalAutoencoder(VAE)proposedin[25]integratesfunctionalandstructural
neuroimaging data into a shared latent space using a single encoder-decoder scheme. This approach
facilitatestheextractionofcross-modalpatterns, allowingforinterpolationandinterpretabilitybetween
modalities. By employing these shared latent representations, the model achieved superior performance
in schizophrenia classification tasks compared to traditional fusion methods.
To evaluate the effectiveness of the representations, the authors conducted a schizophrenia classifi-
cation task. The model achieved a receiver operating characteristic area under the curve (ROC-AUC)
of 0.8609, outperforming both eatly and late fusion PCA methods. The VAE’s ability to reconstruct
differences in the latent space was also validated through visualization of group differences between
schizophreniapatientsandhealthycontrols. Keyregions,suchasthethalamusandcerebellum,emerged
as important, consistent with prior findings.
Moreover,themodeldemonstratedscalabilitywithincreasinglatentdimensions,retainingmeaningful
information without overfitting, even in small datasets. Importantly, the latent space showed clusters
corresponding to specific modalities, such as intrinsic functional networks (ICNs) and structural MRI
(sMRI)supportingitsrobustness. Themodel’sgenerativecapabilityofferspotentialapplicationsindata
augmentation and further exploraation modality-specific interactions.
On the other hand, following the same multimodality approach, authors in [47] propose a joint VAE
model that learns a shared latent representation of both 123I-ioflupane SPECT images and clinical data
scoresofParkinson’sDisease(PD).Unlikepreviouswork, theyusetwoVAEs, oneforneuroimagingand
anotherforclinicaldata,withsharedlatentspacescross-relatedbyanadditionallosstermthatminimizes
differences between shared latents while ensuring disentanglement of non-shared latents. Moreover, the
authors employ Maximum Mean Discrepancy (MDD) instead of the usual Kullback-Leibler divergence
to maximize mutual information between input data and latent space.
Using this shared latent representation, the model achieved an R2 of up to 0.86 in same-modality
tasks and 0.441 in cross-modality tasks for predicting motor symptomatology and clinical features such
asUPDRS.Theframeworkdemonstratesthefeasibilityofbridgingneuroimagingandclinicalmodalities,
identifying latent features predictive of motor symptoms and cognitive deficits. Notably, their analy-
sis revealed that neuroimaging-based latent features are predominantly specific to motor impairments,
reflectingdopaminergicdeficitscharacteristicofPD.Thestudyunderscorestheimportanceofdatastan-
dardization and DSSIM for enhancing latent representations, making a significant contribution to the
understandingofmultimodalneurodegenerativepatterns. Futuredirectionsincludeearly-stagedetection
and extending the framework to other neuroimaging modalities.
In [27], the authors propose a model to integrate imaging and genetic data into a lower-dimensional
manifoldguidedbyclinicaldiagnosis. Eachsubjectintheframeworkhasthreeinputs: anfMRIactivation
map, genetic information, and a binary diagnosis (control or affected). The model employs two feature
21
matrices,AandB,representingimagingandgeneticdata,respectively,alongwithasharedlatentvector
z , unique for each subject but common across both modalities. This setup models the imaging data as
m
f ≈ ATz and the genetic data as g ≈ BTz . The matrices A and B represent the anatomical and
m m m m
geneticbasissharedacrosssubjects,whilef andg projectthespecificfeaturesofeachsubjectintothe
m m
latent space. Using the shared latent representation z , the authors predict the probability of a subject
m
being affected by the condition via logistic regression, σ(zTc) where c is a regression coefficient vector.
m
The sigmoid function ensures probabilities near one for high zTc values. To validate their approach, the
m
authors compare four models: RT (random forest), CCA+RT, Img (only the imaging subspace), and
Img+gen(imagingandgeneticdata). Usingsensitivity,specificity,andaccuracyasmetrics,theImg+gen
model outperforms others. They identify key regions from the matrix A, including the dorsolateral
prefrontal cortex, implicated in executive function deficits characteristic of schizophrenia, as well as
the hippocampus and parahippocampus, regions commonly affected by the disorder. Additionally, the
model highlights five single nucleotide polymorphisms (SNPs) most relevant to the latent space. The
authorsperformagenome-wideassociationstudy(GWAS),confirmingthattheseSNPsarepartofgenes
linked to schizophrenia. This demonstrates the model’s ability to uncover clinically relevant biomarkers
integrating imaging and genetic data.
In[33], theauthorsproposeamoredirectapproachtointegratingimagingandgeneticdatabylever-
aginganimplicitgenerativemodelbasedonLatentDiffusionModels(LDM).Theirframeworkgenerates
MRI data conditioned on labels and genotypes. First, an autoencoder extracts latent representations
from MRI slices. These representations are then fed into a diffusion process conditioned on both la-
bels and genotypes. Labels are concatenated with the latent space, while genotypes are incorporated
via a transformer. The use of cross-attention enables the model to focus selectively on relevant genetic
features, guiding the diffusion process to create meaningful new representations.
The model was validated on the ADNI dataset and compared against five frameworks: CGAN,
StackGAN, AttGAN, a label-conditioned LDM, and several variants of the proposed LDM (G2I, G2I-1,
and G2I-1+Att). Evaluation metrics included PSNR, FID, SSIM, and MS-SSIM. Results showed that
thefullmodel(G2I-1+Att)outperformedallcompetitors,generatingimagescloselyresemblingrealdata
and accurately capturing key markers of Alzheimer’s progression, such as ventricular enlargement and
cortical atrophy. Notably, only LDM-based models consistently produced complete and realistic MRI
scans, whereas GAN-based models struggled with generating fine details. While AttGAN achieved the
highest PSNR score, its outputs appeared blurry, highlighting that PSNR alone may not fully capture
visual quality.
An ablation study further demonstrated that incorporating genetic data via cross-attention signif-
icantly improved image quality compared to the baseline, underscoring the importance of this design
choice.
4.8 Image synthesis
The generation of synthetic medical images has emerged as a vital tool for addressing challenges such
as data scarcity, privacy concerns, and the need for controlled experimentation. Among the most pop-
ular frameworks for image synthesis are Generative Adversarial Networks (GANs) and Latent Diffusion
Models (LDMs), which have proven particularly effective in medical imaging and neuroimaging applica-
tions. These models are widely used to enhance image realism, diversity, and structural fidelity, playing
a critical role in generating data for training algorithms and simulating complex anatomical variations.
Zhanget. al[75]presenttheSketching-renderingunconditionalGenerativeAdversarialNetwork(Skr-
GAN), an innovative approach that incorporates a sketch-based structure to guide the image generation
process. By decomposing the generation into a sketch module (G ) for structural outlines and a color
s
renderingmodule(G ),themodelpreservesessentialanatomicalfeatures,suchasedgesandboundaries,
p
derived through Sobel edge detection. This framework demonstrates the adaptability of GANs in gener-
ating synthetic images that retain critical structural characteristics, highlighting their utility in medical
imaging.
They use MS-SSIM, Sliced Wasserstein Distance (SWD) and Frechet Inception Distance (FID) as
performance metrics, where the SWD measures the distance necessary to transform a probability distri-
bution into another one. On the other hand, FID compares the statistical features (such as µ or σ) of
both real and generated images.
ExpandingonthelimitationsoftraditionalVAEsandGANs, authorsin[53],introducealatentdiffu-
sion model tailored for generating high-resolution 3D brain MRI scans. LDM combines the efficiency of
autoencoder-basedlatentrepresentationswiththeprobabilisticmodelingcapabilitiesofdiffusionmodels.
22
Theauthorstrainedthemodelon31,740T1-weightedMRIscansfromtheUKBiobank,compressingin-
putdatatoalatentspacebeforeapplyingthediffusionprocess. Thisapproachenabledstablegeneration
of high-quality images while managing computational demands.
ModelperformancewasevaluatedusingmetricssuchasFrechetInceptionDistance(FID),MS-SSIM,
and 4-G-R-SSIM, where lower scores denote better diversity and realism. The LDM outperformed
baseline methods like LSGAN [46] and VAE-GAN [41], producing sharper and more realistic images.
Additionally,leveragingahybridconditionalapproachwithcross-attentionmechanisms,themodelcould
generateimagesconditionedonattributeslikeage,sex,ventricularvolume,andnormalizedbrainvolume.
Ventricular volumes, assessed with SynthSeg, showed a Pearson correlation of 0.972 between generated
and conditioned values, demonstrating strong alignment. For age prediction, a 3D convolutional neural
network trained on the generated images achieved a correlation of r =0.692 with input values.
The authors also tested the model’s ability to extrapolate conditioning variables beyond training
ranges, producing plausible outputs, such as abnormally large or absent ventricles. To foster repro-
ducibility and research, they released a dataset of 100,000 synthetic brain MRI scans via Academic
Torrents [52], Figshare, and the HDRUK Gateway. This work highlights the potential of diffusion mod-
els using latent representations in overcoming data scarcity in medical imaging while preserving privacy.
4.9 Other works
Additionally to the ongoing lines of investigation mentioned in this review, some other authors have
conducted various studies to further enhance the processing, analysis, and understanding of neuroimag-
ing. These works vary in topics and include studies about generation of 3D MR images from 2D slices,
increasing of resolution of MRI, interpretability of latent representations, the use of genetic information
along with imaging, or even the use of EEG signals in the context of latent representations.
For instance, authors in [68] use a 2D VAE to learn representations of 2D brain MRI, and a Gamma
model that captures relationships between slices to generate 3D MRI. By estimating the mean and
covarianceinthelatentspaceofthe2Dmodelinthedirectionoftheslicetheycanobtainthedistribution
alongtheslicedirectionandgeneratetheremainingslices. Theyalsouseavalidationmethodforvolumes
thatquantifieshowgoodthesegmentationsarecomparedtorealsegmentationsofthebrain. Tovalidate
the model the compare their 2D VAE with a 3D VAE and other 3D models (3D WGAN GP, 3D VAE
GAN, 3D α-GAN and 3D α-WGAN), finding that their method is able to succesfully sample consistent
brainvolumes. Eventhoughboththeirmodelandthe3Dmodelproduceblurryimages,thatisexpected
due to the nature of the VAE model. Additionally, they compute the MMD (distance to the original
distribution) and the MS-SSIM, showing that their model produces volumes that are both diverse and
close to the real distribution of the data. Since these metrics have no anatomical meaning, they also use
the Realistic Atlas Score (RAS) metric, finding that the proposed model outperforms state-of-the-art
approaches.
On the other hand, the authors in [79] employ a latent diffusion approach to generate 3D brain MRI
from 2D slices. They argue that generating volumes from 2D MRI often leads to inconsistencies due to
the inherent complexity of the data. To address this, they propose an efficient, high-resolution method
for generating MRI volumes from 2D images. Their approach utilizes an encoder to generate two latent
representations: z from the source modality and z from the target modality. These latent spaces are
c
concatenatedandthenfedintotheU-Netofthediffusionprocess. Toextendthemodelfromslice-based
to volume-based generation, they incorporate volumetric layers and fine-tune the U-Net. These layers
enable the model to learn latent features that account for volumetric coherence across slices, and are
implemented using 1D convolutions.
TheauthorsevaluatetheirmodelusingSSIM,MAE,andPSNRmetrics,comparingitagainstbaseline
methods such as Pix2Pix, Palette [60], Pix2Pix 3D, and CycleGAN 3D. Their results show that their
model outperforms all others in every metric, across both the S2M and RIRE [70] datasets. Qualitative
analysis further demonstrates that the proposed model produces synthetic images with higher detail
and fewer inconsistencies compared to the baseline models. Additionally, an ablation study reveals that
fine-tuning the volumetric layers helps reduce volumetric artifacts.
Authorsin[45]proposeamodeltoenhancetheinterpretabilityofdeeplearningmodelsappliedtoMRI
classification. The key idea is to organize the latent space according to clinically meaningful variables,
specifically the Neuropsychological Z-global score (NPZ), which measures cognitive impairment. This
ensures that differences in the latent space reflect changes in the NPZ score. To achieve this, the
latent space is disentangled along a linear direction, τ, that captures variations in NPZ. The model is
trained on 3D MRIs and their corresponding NPZ scores σ, optimizing both a classification loss and a
23
disentanglement loss. The disentanglement loss is defined as:
(cid:88)
L = (||∆τ(i,j)||2−|∆σ(i,j)|), (9)
r
(i,j)∈D
where||∆τ(i,j)||representsthedifferencebetweentheprojectionsoflatentrepresentationsalongτ, and
|∆σ(i,j)| is the difference in NPZ scores for the corresponding MRI pairs. This setup ensures a linear
relationship between changes in NPZ and the latent space.
Applied to a diverse dataset of 519 cross-sectional MRIs spanning multiple conditions (MCI, HAND,
controls,HIVwithoutHAND),theproposedmodelachievedabalancedaccuracyof46.13%,significantly
outperforming the baseline model (39.9%, p = 0.009). Notably, the disentangled latent space was not
only interpretable but also demonstrated robustness across demographic factors such as age, sex, and
education, which are often confounding variables in neuroimaging studies.
Saliency maps derived from the model highlighted key brain regions driving classification, such as
the superior temporal gyrus and caudate nucleus for MCI, and the cerebellar white matter for HAND.
Thesefindingsalignwithknownpatternsofneurodegenerationreportedintheliterature,reinforcingthe
clinical and neuroscientific relevance of the model.
In [42], the authors propose a Variational Autoencoder (VAE) framework to derive latent represen-
tations from multichannel EEG data, aimed at recognizing emotional states in subjects. The VAE is
chosenforitsabilitytomodeltheunderlyingprobabilitydistributionsofthedata,makingitparticularly
effective for capturing the intrinsic, emotion-related factors from EEG signals. Recognizing the dynamic
andtime-dependentnatureofemotionsandbrainactivity,theauthorsintegrateLongShort-TermMem-
ory(LSTM)networkstoprocesstemporalsequencesoftheextractedlatentfactorsandidentifypatterns
linked to emotional states.
The framework’s performance is evaluated on the DEAP and SEED datasets, using the F1-score as
a metric. Results show that the combination of VAE and LSTM significantly outperforms traditional
approaches such as ICA and Autoencoder (AE), both in terms of reconstruction and emotion classifica-
tion. Notably, the VAE demonstrates superior performance with fewer latent dimensions, indicating its
efficacy in extracting meaningful latent features from EEG data. This work establishes a robust method
for emotion recognition and highlights the potential of generative models in decoding latent cognitive
processes.[REVISAR]
In[80],theauthorsemphasizetheneedforanassumption-freemodelcapableofdetectinganomaliesin
brainimages. Theycritiquecurrentreconstruction-basedapproaches,whichrequiretailoringthenetwork
architecture to the specific image modality and type of anomaly being evaluated. This dependency not
onlylimitsgeneralizationbutalsocontradictsthegoalofcreatingflexibleandbroadlyapplicablemodels
for medical imaging.
Toaddressthis,theyproposeenhancingthetraditionalreconstructiontermwiththeKullback-Leibler
(KL) divergence, aiming to improve the robustness and accuracy of anomaly localization. They demon-
strate experimentally that relying solely on reconstruction error is suboptimal, as it can lead to limited
performance depending on the latent space configuration. On the HCP dataset, they show that models
combining reconstruction and KL divergence generally outperform reconstruction-only approaches in
anomaly detection, particularly in cases with diverse hyperparameter settings. Even in the few cases
where reconstruction error performs better, such performance is achieved under restrictive conditions,
suchasusingasmalllatentspace. Additionally,ontheBraTS2017dataset,theircombinedmethodcon-
sistently outperforms reconstruction-only methods, further validating its efficacy across datasets with
different characteristics.
5 Discussion
The use of latent representation models in neuroimaging has demonstrated significant potential fro
addressing key challenges in the field, such as the high dimensionality of data, inter-subject variability,
and the integration of multimodal datasets. Throughout this review, we have explored the theoretical
foundations of latent spaces, their alignment with the manifold hypothesis, and their application across
a spectrum of neuroimaging tasks.
Generative models, particularly VAEs, GANs, and LDMs, have proven to be essential tools for
uncovering meaningful latent structures in neuroimaging data. VAEs offer probabilistic insights that
capturevariabilityinpathologicalprocesses,whileLDMsexcelingeneratinghigh-qualitysyntheticdata.
GANs, on the other hand, have been instrumental in applications requiring detailed image synthesis
24
and harmonization. Despite these advances, we should also highlight the limitations inherent to each
model, including trade-offs between reconstruction quality and latent space interpretability, as well as
the challenges posed by data heterogeneity.
Fromapracticalstandpoint,theabilityoflatentgenerativemodelstoharmonizedatasets,reconstruct
brain activity, and model disease progression represents a critical step forward advancing personalized
medicine and enhancing diagnostic accuracy. For instance, harmonization techniques reduce inter-set
variability, enabling robust multi-center studies, while brain reconstruction and ageing models provide
new avenues for studying disease progression and neurodevelopment. Studies have shown that LDM
and GAN based models stand out among latent models in synthesizing high quality images that encode
relevant semantic information [29, 65], or harmonization tasks [69, 56] . On the other hand, explicit
VAE-based models have proven to outperform most models in capturing patterns of brain progression
or neurodegeneration [12, 4, 48, 47].
Additionally, the role of latent generative models in advancing our understanding of brain function
and active inference underscores their potential to bridge computational neuroscience and clinical appli-
cations. Inference based model has proven to be a hot topic in the understanding of cognitive processes
and unraveling the arise of consciousness [24] . Moreover, these models also seem to be a promising
research gateway for understanding neural coding and decoding, as we have seen that some studies
demonstrate the similarity between latent generative models and visual encoding or delusions formation
in the brain [21, 22, 26].
However, there remain important open questions about the interpretability of latent spaces, the
integration of multimodal data, and the scalability of these approaches in clinical settings. For instance,
3T to 7T MRI translation faces the problem of the disentangling latent space, in which the capture of
a real anatomical representation and its interpretablity is still an open issue. Moreover, most studies
in the literature focus on using the latent representations of neuroimaging as a tool for synthesizing or
reconstructingimagesorvolumes, whileverylittleworksmakeuseoftheinformationofthelatentspace
to perform statistical analysis. This issue becomes clear as most of the studies use GAN or LDM based-
models, which are implicit frameworks. Although these models take into account the learned features
of the representations, we have no access to the latent variables, which greatly hinders the analysis and
interpretability of the models. Even among the models that use explicit models like VAE, only a few of
them, such as [47, 25], perform statistical methods to the latent variables.
Future research should focus not only on improving these preexisting models but also exploing the
exploitabilityoftheserepresentationsasameanforobtainingrelevantinsightsthatcanberelatedtobrain
processes or biomarkers that characterize neurodegenerative diseases. Furthermore, latent generative
modelsbasedonbayesianinferencehaveproventoexcelinfurtheringourunderstandingaboutunderlying
brain processes, their limitations brings to light the need to pursue new inference frameworks, like
empirical bayesian inference [49].
Topic Architecture Database Ref
Brain as DNN [24]
an inference BI [23]
machine
GAN [26]
Image-to-Image BraTs2015, Iseg2017, CGAN [73]
translation MRBrain13, ADNI,
& Harmonization RIRE
Private GAN+non-adversarial losses [1]
BraTS2021, IXI MS-SPADE [38]
ADNI, MRI-GENIE SIT-cGAN [69]
OpenBHB, SRPBS DLEST [71]
OASIS, SRPBS VAE-GAN [8]
IXI, OASIS3, BLSA cVAE+IB loss [81]
IXI, 2 Private U-Net [16]
Private GAN + non-adversarial losses [56]
Visual BRAINS, Generic Object GAN+linear regression [61]
reconstruction Decoding
using fMRI
25
ILSVRC2012, Private VAE+linear regression [31]
NSD AE+LDM [65]
NSD spherical [29]
CNN+variational+IC-GAN
NSD VDVAE+regression+LDM [51]
vim-1 fwRF+GAN [63]
HCP, NSD MAE+CLIP+VDVAE+LDM [30]
Cam-CAN, ADNI VGG+adversarial & identity [72]
Brain ageing analysis
losses
ADNI VAE [12]
ADNI VAE+temporal linear model [11]
UK Biobank VAE+ age regression [9]
dHCP INR [5]
ADNI VAE+MLP [4]
Disease classification
ADNI Negative Matrix Factorization [78]
OpenfMRI CVAE+DGM [48]
Public [15] BI+SVM+GMM [6]
ADHD-200 STAAE+lasso regression [18]
Functional brain
ADHD-200 DVAE+lasso regression [55]
networks
WU-Minn HPC Convolutional ker- [76]
nel+attention
HCP Q3 3D ResAE+lasso regression [17]
Multimodality
FBIRN, B-SNIP, COBRE VAE [25]
integration
PPMI joint VAE [47]
Private Matrix latent representation [27]
ADNI-1, ADNIGO/2 AE+cLDM [33]
Brain MRI GAN [75]
Image synthesis
UK Biobank AE+LDM [53]
HCP VAE+Gamma model [68]
Other works
SWI-to-MRA, RIRE LDM+1D convolutions [79]
UCSF CNN+disentanglement loss [45]
DEAP, SEED VAE+LSTM [42]
HCP, BraTS2017 VAE [80]
Table 1: Overview of the studies.
References
[1] KarimArmanious,ChenmingJiang,MarcFischer,ThomasKu¨stner,TobiasHepp,KonstantinNiko-
laou, Sergios Gatidis, and Bin Yang. Medgan: Medical image translation using gans. Computerized
medical imaging and graphics, 79:101684, 2020.
[2] Khosro Bahrami, Feng Shi, Islem Rekik, Yaozong Gao, and Dinggang Shen. 7t-guided super-
resolution of 3t mri. Medical physics, 44(5):1661–1677, 2017.
26
[3] Khosro Bahrami, Feng Shi, Xiaopeng Zong, Hae Won Shin, Hongyu An, and Dinggang Shen. Re-
constructionof7t-likeimagesfrom3tmri. IEEE transactions on medical imaging,35(9):2085–2097,
2016.
[4] Sumana Basu, Konrad Wagstyl, Azar Zandifar, Louis Collins, Adriana Romero, and Doina Precup.
Earlypredictionofalzheimer’sdiseaseprogressionusingvariationalautoencoders. InMedical Image
Computing and Computer Assisted Intervention–MICCAI 2019: 22nd International Conference,
Shenzhen, China, October 13–17, 2019, Proceedings, Part IV 22, pages 205–213. Springer, 2019.
[5] FlorentinBieder,PaulFriedrich,H´el`eneCorbaz,AliciaDurrer,JuliaWolleb,andPhilippeC.Cattin.
Modeling the neonatal brain development using implicit neural representations. In International
Workshop on PRedictive Intelligence In MEdicine, pages 1–11. Springer, 2024.
[6] Kay H Brodersen, Lorenz Deserno, Florian Schlagenhauf, Zhihao Lin, Will D Penny, Joachim M
Buhmann,andKlaasEStephan.Dissectingpsychiatricspectrumdisordersbygenerativeembedding.
NeuroImage: Clinical, 4:98–111, 2014.
[7] Ed Bullmore and Olaf Sporns. The economy of brain network organization. Nature reviews neuro-
science, 13(5):336–349, 2012.
[8] Stenzel Cackowski, Emmanuel L Barbier, Michel Dojat, and Thomas Christen. Imunity: a general-
izablevae-gansolutionformulticentermrimageharmonization. MedicalImageAnalysis,88:102799,
2023.
[9] Berta Calm Salvans, Irene Cumplido Mayoral, Juan Domingo Gispert, and Veronica Vilaplana.
Identifying brain ageing trajectories using variational autoencoders with regression model in neu-
roimagingdatastratifiedbysexandvalidatedagainstdementia-relatedriskfactors. InInternational
Workshop on PRedictive Intelligence In MEdicine, pages 149–160. Springer, 2024.
[10] Bokai Cao, Lifang He, Xiaokai Wei, Mengqi Xing, Philip S Yu, Heide Klumpp, and Alex D Leow.
t-bne: Tensor-based brain network embedding. In proceedings of the 2017 SIAM international
conference on data mining, pages 189–197. SIAM, 2017.
[11] Cl´ementChadebec,EviMCHuijben,JosienPWPluim,St´ephanieAllassonni`ere,andMaureenAJM
van Eijnatten. An image feature mapping model for continuous longitudinal data completion and
generation of synthetic patient trajectories. In MICCAI Workshop on Deep Generative Models,
pages 55–64. Springer, 2022.
[12] Hongyoon Choi, Hyejin Kang, Dong Soo Lee, and Alzheimer’s Disease Neuroimaging Initiative.
Predicting aging of brain metabolic topography using variational autoencoder. Frontiers in aging
neuroscience, 10:212, 2018.
[13] SueYeon Chung and Larry F Abbott. Neural population geometry: An approach for understanding
biological and artificial neural networks. Current opinion in neurobiology, 70:137–144, 2021.
[14] MaxColtheart,PeterMenzies,andJohnSutton.Abductiveinferenceanddelusionalbelief.Cognitive
neuropsychiatry, 15(1-3):261–287, 2010.
[15] Lorenz Deserno, Philipp Sterzer, Torsten Wu¨stenberg, Andreas Heinz, and Florian Schlagenhauf.
Reduced prefrontal-parietal effective connectivity and working memory deficits in schizophrenia.
Journal of Neuroscience, 32(1):12–20, 2012.
[16] Blake E Dewey, Lianrui Zuo, Aaron Carass, Yufan He, Yihao Liu, Ellen M Mowry, Scott Newsome,
Jiwon Oh, Peter A Calabresi, and Jerry L Prince. A disentangled latent space for cross-site mri
harmonization. In International conference on medical image computing and computer-assisted
intervention, pages 720–729. Springer, 2020.
[17] Qinglin Dong, Ning Qiang, Jinglei Lv, Xiang Li, Tianming Liu, and Quanzheng Li. Discover-
ing functional brain networks with 3d residual autoencoder (resae). In Medical Image Computing
and Computer Assisted Intervention–MICCAI 2020: 23rd International Conference, Lima, Peru,
October 4–8, 2020, Proceedings, Part VII 23, pages 498–507. Springer, 2020.
27
[18] Qinglin Dong, Ning Qiang, Jinglei Lv, Xiang Li, Tianming Liu, and Quanzheng Li. Spatiotemporal
attention autoencoder (staae) for adhd classification. In Medical Image Computing and Computer
Assisted Intervention–MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8,
2020, Proceedings, Part VII 23, pages 508–517. Springer, 2020.
[19] Martin Dyrba, Michel Grothe, Thomas Kirste, and Stefan J Teipel. Multimodal analysis of func-
tional and structural disconnection in a lzheimer’s disease using multiple kernel svm. Human brain
mapping, 36(6):2118–2131, 2015.
[20] Jerome H Friedman. On bias, variance, 0/1—loss, and the curse-of-dimensionality. Data mining
and knowledge discovery, 1:55–77, 1997.
[21] Karl Friston. Learning and inference in the brain. Neural Networks, 16(9):1325–1352, 2003.
[22] Karl Friston. A theory of cortical responses. Philosophical transactions of the Royal Society B:
Biological sciences, 360(1456):815–836, 2005.
[23] Karl J Friston and Raymond J Dolan. Computational and dynamic models in neuroimaging. Neu-
roimage, 52(3):752–765, 2010.
[24] Karl J Friston and Cathy J Price. Generative models, brain function and neuroimaging. Scandina-
vian Journal of Psychology, 42(3):167–177, 2001.
[25] Eloy Geenjaar, Noah Lewis, Zening Fu, Rohan Venkatdas, Sergey Plis, and Vince Calhoun. Fusing
multimodal neuroimaging data with a variational autoencoder. In 2021 43rd Annual International
Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), pages 3630–3633.
IEEE, 2021.
[26] Samuel J Gershman. The generative adversarial brain. Frontiers in Artificial Intelligence, 2:18,
2019.
[27] Sayan Ghosal, Qiang Chen, Aaron L Goldman, William Ulrich, Karen F Berman, Daniel R Wein-
berger, Venkata S Mattay, and Archana Venkataraman. Bridging imaging, genetics, and diagnosis
in a coupled low-dimensional framework. In Medical Image Computing and Computer Assisted
Intervention–MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13–17,
2019, Proceedings, Part IV 22, pages 647–655. Springer, 2019.
[28] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information
processing systems, 27, 2014.
[29] ZijinGu, KeithJamison, AmyKuceyeski, andMertSabuncu. Decodingnaturalimagestimulifrom
fmri data with a surface-based convolutional network. arXiv preprint arXiv:2212.02409, 2022.
[30] Junhao Guo, Chanlin Yi, Fali Li, Peng Xu, and Yin Tian. Mindldm: Reconstruct visual stimuli
from fmri using latent diffusion model. In 2024 IEEE International Conference on Computational
Intelligence and Virtual Environments for Measurement Systems and Applications (CIVEMSA),
pages 1–6. IEEE, 2024.
[31] Kuan Han, Haiguang Wen, Junxing Shi, Kun-Han Lu, Yizhen Zhang, Di Fu, and Zhongming Liu.
Variational autoencoder: An unsupervised model for encoding and decoding fmri activity in visual
cortex. NeuroImage, 198:125–136, 2019.
[32] MehrdadJazayeriandSrdjanOstojic. Interpretingneuralcomputationsbyexaminingintrinsicand
embedding dimensionality of neural activity. Current opinion in neurobiology, 70:113–120, 2021.
[33] SooyeonJeon,YujeeSong,andWonHwaKim. Gene-to-image: Decodingbrainimagesfromgenetics
vialatentdiffusionmodels.InInternationalWorkshoponPRedictiveIntelligenceInMEdicine,pages
48–60. Springer, 2024.
[34] Amod Jog, Aaron Carass, Snehashis Roy, Dzung L Pham, and Jerry L Prince. Mr image synthesis
by contrast learning on neighborhood ensembles. Medical image analysis, 24(1):63–76, 2015.
[35] Ta-Chu Kao, Mahdieh S Sadabadi, and Guillaume Hennequin. Optimal anticipatory control as a
theory of motor preparation: A thalamo-cortical circuit model. Neuron, 109(9):1567–1581, 2021.
28
[36] Jeremy Kawahara, Colin J Brown, Steven P Miller, Brian G Booth, Vann Chau, Ruth E Grunau,
Jill G Zwicker, and Ghassan Hamarneh. Brainnetcnn: Convolutional neural networks for brain
networks; towards predicting neurodevelopment. NeuroImage, 146:1038–1049, 2017.
[37] Elizabeth A Kensinger and Daniel L Schacter. Neural processes underlying memory attribution on
a reality-monitoring task. Cerebral Cortex, 16(8):1126–1133, 2006.
[38] Jonghun Kim and Hyunjin Park. Adaptive latent diffusion model for 3d medical image to image
translation: Multi-modal magnetic resonance imaging study. In Proceedings of the IEEE/CVF
Winter Conference on Applications of Computer Vision, pages 7604–7613, 2024.
[39] Diederik P Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
[40] Sofia Ira Ktena, Sarah Parisot, Enzo Ferrante, Martin Rajchl, Matthew Lee, Ben Glocker, and
Daniel Rueckert. Metric learning with spectral graph convolutions on brain connectivity networks.
NeuroImage, 169:431–442, 2018.
[41] Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, and Ole Winther. Autoen-
coding beyond pixels using a learned similarity metric. In International conference on machine
learning, pages 1558–1566. PMLR, 2016.
[42] XiangLi,ZhigangZhao,DaweiSong,YazhouZhang,JingshanPan,LuWu,JidongHuo,Chunyang
Niu, and Di Wang. Latent factor decoding of multi-channel eeg for emotion recognition through
autoencoder-like neural networks. Frontiers in neuroscience, 14:87, 2020.
[43] GraceLindsay. UncoveringHiddenDimensionsinBrainSignals. MITPress,Cambridge,MA,2022.
[44] Jiahao Lu, Johan O¨fverstedt, Joakim Lindblad, and Nataˇsa Sladoje. Is image-to-image translation
the panacea for multimodal image registration? a comparative study. Plos one, 17(11):e0276196,
2022.
[45] JocastaManasseh-Lewis,FelipeGodoy,WeiPeng,RobertPaul,EhsanAdeli,andKilianPohl. Neu-
rocognitivelatentspaceregularizationformulti-labeldiagnosisfrommri. InInternationalWorkshop
on PRedictive Intelligence In MEdicine, pages 185–195. Springer, 2024.
[46] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley.
Least squares generative adversarial networks. In Proceedings of the IEEE international conference
on computer vision, pages 2794–2802, 2017.
[47] Francisco J Martinez-Murcia, Juan Eloy Arco, Carmen Jimenez-Mesa, Fermin Segovia, Ignacio A
Illan, Javier Ramirez, and Juan Manuel Gorriz. Bridging imaging and clinical scores in parkinson’s
progression via multimodal self-supervised deep learning. International Journal of Neural Systems,
pages 2450043–2450043, 2024.
[48] Takashi Matsubara, Tetsuo Tashiro, and Kuniaki Uehara. Deep neural generative model of func-
tionalmriimagesforpsychiatricdisorderdiagnosis. IEEE Transactions on Biomedical Engineering,
66(10):2768–2779, 2019.
[49] Carl N Morris. Parametric empirical bayes inference: theory and applications. Journal of the
American statistical Association, 78(381):47–55, 1983.
[50] Nassir Navab, Joachim Hornegger, William M Wells, and Alejandro Frangi. Medical Image Com-
putingandComputer-AssistedIntervention–MICCAI2015: 18thInternationalConference,Munich,
Germany, October 5-9, 2015, Proceedings, Part III, volume 9351. Springer, 2015.
[51] FurkanOzcelikandRufinVanRullen.Naturalscenereconstructionfromfmrisignalsusinggenerative
latent diffusion. Scientific Reports, 13(1):15666, 2023.
[52] WalterH.L.Pinaya,Petru-DanielTudosiu,JessicaDafflon,PedroF.DaCosta,VirginiaFernandez,
Parashkev Nachev, S´ebastien Ourselin, and M. Jorge Cardoso. Synthetic dataset of 100,000 brain
mri scans, 2022. Available at Academic Torrents.
[53] Walter HL Pinaya, Petru-Daniel Tudosiu, Jessica Dafflon, Pedro F Da Costa, Virginia Fernandez,
ParashkevNachev,SebastienOurselin,andMJorgeCardoso. Brainimaginggenerationwithlatent
diffusionmodels. InMICCAI Workshop on Deep Generative Models,pages117–126.Springer,2022.
29
[54] David Premack and Guy Woodruff. Does the chimpanzee have a theory of mind? Behavioral and
brain sciences, 1(4):515–526, 1978.
[55] Ning Qiang, Qinglin Dong, Fangfei Ge, Hongtao Liang, Bao Ge, Shu Zhang, Yifei Sun, Jie Gao,
and Tianming Liu. Deep variational autoencoder for mapping functional brain networks. IEEE
Transactions on Cognitive and Developmental Systems, 13(4):841–852, 2020.
[56] Liangqiong Qu, Shuai Wang, Pew-Thian Yap, and Dinggang Shen. Wavelet-based semi-supervised
adversarial learning for synthesizing realistic 7t from 3t mri. In Medical Image Computing and
Computer Assisted Intervention–MICCAI 2019: 22nd International Conference, Shenzhen, China,
October 13–17, 2019, Proceedings, Part IV 22, pages 786–794. Springer, 2019.
[57] Alec Radford, Jongwei Kim, C. Hallacy, Aditya Ramesh, Gabriel Goh, Shibani Agarwal, Gauri
Sastry, Amanda Askell, Paul Mishkin, Jack Clark, and et al. Learning transferable visual models
from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.
[58] Vilayanur S Ramachandran and William Hirstein. Three laws of qualia: What neurology tells us
about the biological functions of consciousness. Journal of consciousness studies, 4(5-6):429–457,
1997.
[59] Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embed-
ding. science, 290(5500):2323–2326, 2000.
[60] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David
Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH
2022 conference proceedings, pages 1–10, 2022.
[61] Katja Seeliger, Umut Gu¨¸clu¨, Luca Ambrogioni, Yagmur Gu¨¸clu¨tu¨rk, and Marcel AJ Van Gerven.
Generativeadversarialnetworksforreconstructingnaturalimagesfrombrainactivity. NeuroImage,
181:775–785, 2018.
[62] Ghislain St-Yves and Thomas Naselaris. The feature-weighted receptive field: an interpretable
encoding model for complex feature spaces. NeuroImage, 180:188–202, 2018.
[63] Ghislain St-Yves and Thomas Naselaris. Generative adversarial networks conditioned on brain
activity reconstruct seen images. In 2018 IEEE international conference on systems, man, and
cybernetics (SMC), pages 1054–1061. IEEE, 2018.
[64] Jing Sui, Godfrey Pearlson, Arvind Caprihan, Tu¨lay Adali, Kent A Kiehl, Jingyu Liu, Jeremy
Yamamoto,andVinceDCalhoun. Discriminatingschizophreniaandbipolardisorderbyfusingfmri
and dti in a multimodal cca+ joint ica model. Neuroimage, 57(3):839–855, 2011.
[65] Yu Takagi and Shinji Nishimoto. High-resolution image reconstruction with latent diffusion models
from human brain activity. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 14453–14463, 2023.
[66] Joshua B Tenenbaum, Vin de Silva, and John C Langford. A global geometric framework for
nonlinear dimensionality reduction. science, 290(5500):2319–2323, 2000.
[67] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
L(cid:32) ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information
Processing Systems (NeurIPS), pages 5998–6008, 2017.
[68] AnnaVolokitin,ErtuncErdil,NeeravKarani,KeremCanTezcan,XiaoranChen,LucVanGool,and
EnderKonukoglu. Modellingthedistributionof3dbrainmriusinga2dslicevae. InMedical Image
Computing and Computer Assisted Intervention–MICCAI 2020: 23rd International Conference,
Lima, Peru, October 4–8, 2020, Proceedings, Part VII 23, pages 657–666. Springer, 2020.
[69] Clinton J Wang, Natalia S Rost, and Polina Golland. Spatial-intensity transforms for medical
image-to-image translation. IEEE transactions on medical imaging, 42(11):3362–3373, 2023.
[70] Jay West, J Michael Fitzpatrick, Matthew Y Wang, Benoit M Dawant, Calvin R Maurer Jr,
Robert M Kessler, Robert J Maciunas, Christian Barillot, Didier Lemoine, Andre Collignon, et al.
Comparisonandevaluationofretrospectiveintermodalitybrainimageregistrationtechniques. Jour-
nal of computer assisted tomography, 21(4):554–568, 1997.
30
[71] Mengqi Wu, Lintao Zhang, Pew-Thian Yap, Hongtu Zhu, and Mingxia Liu. Disentangled latent
energy-based style translation: An image-level structural mri harmonization framework. arXiv
preprint arXiv:2402.06875, 2024.
[72] Tian Xia, Agisilaos Chartsias, Sotirios A Tsaftaris, and Alzheimer’s Disease Neuroimaging Ini-
tiative. Consistent brain ageing synthesis. In Medical Image Computing and Computer Assisted
Intervention–MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13–17,
2019, Proceedings, Part IV 22, pages 750–758. Springer, 2019.
[73] Qianye Yang, Nannan Li, Zixu Zhao, Xingyu Fan, Eric I-Chao Chang, and Yan Xu. Mri cross-
modality image-to-image translation. Scientific reports, 10(1):3753, 2020.
[74] Wako Yoshida, Ben Seymour, Karl J Friston, and Raymond J Dolan. Neural mechanisms of belief
inference during cooperative games. Journal of Neuroscience, 30(32):10744–10751, 2010.
[75] Tianyang Zhang, Huazhu Fu, Yitian Zhao, Jun Cheng, Mengjie Guo, Zaiwang Gu, Bing Yang,
Yuting Xiao, Shenghua Gao, and Jiang Liu. Skrgan: Sketching-rendering unconditional generative
adversarial networks for medical image synthesis. In Medical Image Computing and Computer
Assisted Intervention–MICCAI 2019: 22nd International Conference, Shenzhen, China, October
13–17, 2019, Proceedings, Part IV 22, pages 777–785. Springer, 2019.
[76] Wen Zhang, Liang Zhan, Paul Thompson, and Yalin Wang. Deep representation learning for multi-
modal brain networks. In Medical Image Computing and Computer Assisted Intervention–MICCAI
2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part VII 23,
pages 613–624. Springer, 2020.
[77] Yongqin Zhang, Jie-Zhi Cheng, Lei Xiang, Pew-Thian Yap, and Dinggang Shen. Dual-domain
cascaded regression for synthesizing 7t from 3t mri. In Medical Image Computing and Computer
Assisted Intervention–MICCAI 2018: 21st International Conference, Granada, Spain, September
16-20, 2018, Proceedings, Part I, pages 410–417. Springer, 2018.
[78] Tao Zhou, Mingxia Liu, Huazhu Fu, Jun Wang, Jianbing Shen, Ling Shao, and Dinggang Shen.
Deepmulti-modallatentrepresentationlearningforautomateddementiadiagnosis. InInternational
conferenceonmedicalimagecomputingandcomputer-assistedintervention,pages629–638.Springer,
2019.
[79] Lingting Zhu, Zeyue Xue, Zhenchao Jin, Xian Liu, Jingzhen He, Ziwei Liu, and Lequan Yu. Make-
a-volume: Leveraging latent diffusion models for cross-modality 3d brain mri synthesis. In In-
ternational Conference on Medical Image Computing and Computer-Assisted Intervention, pages
592–601. Springer, 2023.
[80] David Zimmerer, Fabian Isensee, Jens Petersen, Simon Kohl, and Klaus Maier-Hein. Unsupervised
anomaly localization using variational auto-encoders. In Medical Image Computing and Computer
Assisted Intervention–MICCAI 2019: 22nd International Conference, Shenzhen, China, October
13–17, 2019, Proceedings, Part IV 22, pages 289–297. Springer, 2019.
[81] Lianrui Zuo, Blake E Dewey, Yihao Liu, Yufan He, Scott D Newsome, Ellen M Mowry, Susan M
Resnick,JerryLPrince,andAaronCarass.Unsupervisedmrharmonizationbylearningdisentangled
representations using information bottleneck theory. NeuroImage, 243:118569, 2021.
31

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "A Review of Latent Representation Models in Neuroimaging"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
