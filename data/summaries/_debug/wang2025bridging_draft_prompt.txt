=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Bridging Probabilistic Inference and Behavior Trees: An Interactive Framework for Adaptive Multi-Robot Cooperation
Citation Key: wang2025bridging
Authors: Chaoran Wang, Jingyuan Sun, Yanhui Zhang

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Abstract: ThispaperproposesanInteractiveInferenceBehaviorTree(IIBT)frameworkthatintegratesbe-
haviortrees(BTs)withactiveinferenceunderthefreeenergyprinciplefordistributedmulti-robot
decision-making. TheproposedIIBTnodeextendsconventionalBTswithprobabilisticreason-
ing, enablingonlinejointplanningandexecutionacrossmultiplerobots. Itremainsfullycom-
patible with standard BT architectures, allowing seamless integration into existing multi-robot
controlsystems. Withinthisframework,multi-robotcooperationisform...

Key Terms: robotcooperation, task, abstract, trees, behavior, aninteractiveframeworkforadap, robot, interactive, bridgingprobabilisticinferenceandbehaviortrees, bridging

=== FULL PAPER TEXT ===

Graphical Abstract
BridgingProbabilisticInferenceandBehaviorTrees:AnInteractiveFrameworkforAdap-
tiveMulti-RobotCooperation
ChaoranWang,JingyuanSun,YanhuiZhang,ChangjuWu
Interactive Inference
communication Root
communication
communication R
R
o
o
o
o
t
t
Condition Task
Condition Task
Condition Task
Robot1
Observations
Continuous-Time
Minimize Free Energy
Environment
Planning
5202
ceD
4
]OR.sc[
1v40440.2152:viXra
Highlights
BridgingProbabilisticInferenceandBehaviorTrees:AnInteractiveFrameworkforAdap-
tiveMulti-RobotCooperation
ChaoranWang,JingyuanSun,YanhuiZhang,ChangjuWu
• AnovelInteractiveInferenceBehaviorTree(IIBT)frameworkisproposed.
• IntegratesActiveInferencewithBehaviorTreesformulti-robotdecision-making.
• Enablesdistributedandadaptivecooperationunderuncertaintyandpartialobservability.
• Introducesjointpreferencematricesforinter-robotreasoningandpolicyalignment.
• Validatedthroughsimulationandreal-worldmulti-robottasks,showingrobustnessgains.
Bridging Probabilistic Inference and Behavior Trees: An
Interactive Framework for Adaptive Multi-Robot Cooperation
ChaoranWanga,1,JingyuanSunb,YanhuiZhanga,ChangjuWua
aSchoolofAeronauticandAstronautics,ZhejiangUniversity,No.866,YuhangtangRoad,Xihu
District,Hangzhou,310027,Zhejiang,China
bShanghaiHuaweiTechnologiesCo.,Ltd,Shanghai,201799,China
Abstract
ThispaperproposesanInteractiveInferenceBehaviorTree(IIBT)frameworkthatintegratesbe-
haviortrees(BTs)withactiveinferenceunderthefreeenergyprinciplefordistributedmulti-robot
decision-making. TheproposedIIBTnodeextendsconventionalBTswithprobabilisticreason-
ing, enablingonlinejointplanningandexecutionacrossmultiplerobots. Itremainsfullycom-
patible with standard BT architectures, allowing seamless integration into existing multi-robot
controlsystems. Withinthisframework,multi-robotcooperationisformulatedasafree-energy
minimizationprocess,whereeachrobotdynamicallyupdatesitspreferencematrixbasedonper-
ceptual inputsand peer intentions, therebyachieving adaptive coordination inpartially observ-
able and dynamic environments. The proposed approach is validated through both simulation
and real-world experiments, including a multi-robot maze navigation and a collaborative ma-
nipulationtask,comparedagainsttraditionalBTs(https://youtu.be/KX_oT3IDTf4). Experimental
results demonstrate that the IIBT framework reduces BT node complexity by over 70%, while
maintainingrobust,interpretable,andadaptivecooperativebehaviorunderenvironmentaluncer-
tainty.
Keywords:
Interactiveinference,behaviortree,multirobot,jointaction.
1. Introduction
Cooperativedecision-makingamongmultiplerobotsoperatingindynamicanduncertainen-
vironmentsremainsafundamentalchallengeinautonomoussystems[1,2]. Astaskcomplexity
grows—ranging from industrial assembly and warehouse logistics to search-and-rescue opera-
tions—robotteamsmustnotonlycoordinateactionsandshareinformationbutalsoadapttheir
behaviorstochangingenvironmentalconditionsinrealtime[3,4]. Traditionalcentralizedplan-
ning approaches provide global coordination capabilities but often suffer from high computa-
tional cost and limited scalability [5, 6, 7, 8]. Conversely, fully distributed or reactive control
architecturesofferrapidresponsesbutfrequentlyfailtomaintaincoherentteam-levelstrategies
underuncertainty.
∗Correspondingauthor
Emailaddresses:chaoran_w@zju.edu.cn(ChaoranWang),sunjingyuan1@huawei.com(JingyuanSun)
Interactive Inference
communication
communication
communication
Roo
R R
t
o o o o t t
Condition Task
Condition Task
Condition Task
Robot1
Observations
Continuous-Time
Environment Minimize Free Energy
Planning
Figure1: Overviewoftheproposedframework. Multiplerobotsperforminteractiveinferencetojointlyminimizefree
energy,dynamicallyupdatetheirpolicies,andcoordinateactionsinashared,continuouslyevolvingenvironment.
To balance structured decision-making with real-time adaptability, BTs have emerged as a
widelyadoptedcontrolparadigminbothroboticsandgameAI.BTsofferamodular,hierarchical,
and interpretable framework that enhances code reusability, debugging efficiency, and system
transparency [9, 10]. Their execution semantics—based on ticking nodes that return Success,
Failure, or Running statuses—allow robots to reactively adapt to changing conditions without
requiring a complete redesign of the control logic [11, 12]. However, once a BT structure is
constructed, it remains inherently deterministic [13]. This limitation makes it challenging to
apply BTs in scenarios characterized by partial observability, dynamic task dependencies, and
evolving cooperation requirements [14, 15]. As a result, BT-based systems often rely on static
decision paths, which can degrade performance when environmental or task-related conditions
deviatefromdesign-timeassumptions.
Meanwhile,interactiveinference,groundedinthefreeenergyprinciple,providesaprobabilis-
ticfoundationforperception,prediction,anddecision-making[16,17].Byminimizingexpected
freeenergy,agentscaniterativelyinferhiddenstatesandselectactionpoliciesthatbalanceex-
ploratoryinformationgatheringwithgoal-directedbehavior[18].Despiteitssuccessincognitive
modeling and single-robot active perception, the application of interactive inference to multi-
robot systems remains limited. More importantly, existing inference frameworks are typically
monolithicorcentralized,makingthemdifficulttoembedintomodular,node-baseddecisionar-
chitectureslikeBTs. Consequently,afundamentalgappersistsbetweentheinterpretabilityand
modularityofBTsandtheprobabilisticadaptabilityofinference-basedmethods.
Tobridgethisgap,thispaperproposesanInteractiveInferenceBehaviorTree(IIBT)frame-
work that embeds free-energy–based probabilistic reasoning directly into BT execution nodes.
The proposed framework preserves the modularity and interpretability of BTs while enabling
eachnodetoperformadaptiveinferencebasedoncontextualobservations. Throughthisintegra-
tion, multiplerobotscanjointlyinferandupdatetheirpoliciesonline, dynamicallyadaptingto
environmentalvariationsandtheactionsofotheragentsduringtaskexecution.
Themaincontributionsofthisworkaresummarizedasfollows:
• IntegrationofprobabilisticinferenceintoBTs:WeproposeanovelIIBTnodethatseam-
lesslyintegratesfree-energy–basedinferenceintoBTexecutionsemantics,enablingonline
adaptationwithoutalteringtheBTstructure.
2
• Distributed cooperative policy selection: Each node performs local inference based on
expected free energy, supporting scalable, decentralized, and coherent decision-making
amongmultiplerobots.
• Comprehensive experimental validation: The proposed approach is evaluated through
both simulation and real-world experiments, demonstrating significant improvements in
task success rate, decision efficiency, and BT complexity compared to conventional ap-
proaches.
Theremainderofthispaperisorganizedasfollows. Section2reviewsrelatedworkoninter-
activeinferenceandBT-basedplanning. Section3formulatesthecooperativedecision-making
problemandintroducesthetheoreticalfoundation. Section4presentstheproposedIIBTframe-
work in detail. Implementation and case studies are described in Section 5, while Section 6
reportsexperimentalresults. Finally,Section7discussesthefindingsandconcludesthepaper.
2. RelatedWork
2.1. InteractiveInferenceinRoboticSystems
Interactive inference, grounded in the free energy principle, has emerged as a powerful
paradigm for unified perception, prediction, and decision-making under uncertainty [4, 3]. By
formulatingcontrolasaprocessofminimizingexpectedfreeenergy, agentscaniterativelyup-
datetheirbeliefsabouthiddenstatesandselectpoliciesthatbalanceepistemicexplorationwith
pragmaticgoal-directedactions[1,18].
Initial research primarily focused on cognitive modeling and single-robot active percep-
tion[16,17]. Morerecentstudieshaveextendedtheseideastomulti-robotcontexts, including
distributed control [5], federated inference and belief sharing [19], and collective state estima-
tioninpartiallyobservableenvironments[20].Additionally,recentworkshaveexploredimplicit
coordinationmechanismswhererobotscoordinatewithoutexplicitcommunicationbyinferring
thelatentintentionsofteammates[21].
Whiletheseapproachesdemonstratetheversatilityofactiveinferenceinrobotics,mostrelyon
centralizedgenerativemodelsorglobalstatesynchronization,whichlimitscalabilityinrealistic
multi-robotdeployments. Furthermore, thesemethodstypicallylackstructuredrepresentations
for hierarchical task decomposition, which constrains their integration into modular decision-
makingarchitectures.
2.2. BehaviorTreesforRoboticPlanning
BTs have become a prominent alternative to classical decision architectures such as finite
state machines, offering a modular, hierarchical, and interpretable control framework [22]. By
decomposing complex behaviors into control nodes (e.g., Sequence, Selector) and execution
nodes(e.g.,Condition,Action),BTsallowdeveloperstobuildscalabledecisionpoliciesthatare
reusableandeasilydebuggable[14,11].
BTshavebeenwidelyappliedinroboticnavigation[15],manipulation[13],multi-robotcoor-
dination[8],andhuman-robotcollaboration[12]. Toenhanceadaptability,researchershavein-
tegratedBTswithmachinelearning[23],symbolicplanning[24],andprobabilisticmodels[25],
aswellasstudiedBTperformancemetricsanddesignevaluationmethodologies[26].
Despitetheseadvances,conventionalBTsremainlargelydeterministicandstaticoncedefined.
Mostextensionsfocusonofflinelearningorexternalprobabilisticreasoninglayers, ratherthan
3
incorporatingprobabilisticinferencedirectly intoBT executionsemantics. Asa result, current
approaches still struggle to handle dynamic task priorities, partial observability, and emergent
multi-robotinteractionsinaunifiedframework.
2.3. ResearchGapandMotivation
Insummary,twocomplementaryresearchlineshaveemerged: inference-basedmethodsoffer
probabilisticreasoningandadaptabilitybutlackmodularstructure,whileBT-basedapproaches
provideinterpretabilityandcomposabilitybutcannotreasonprobabilisticallyoradaptonline. A
fewattemptshavecombinedactiveinferenceandBTsforreactivesingle-agentcontrol[18],but
theseeffortsstopshortofembeddingfree-energy–basedreasoningwithinBTexecutionnodes.
To the best of our knowledge, no prior work has integrated interactive inference directly
intotheexecutionsemanticsofBTstoenabledistributed,adaptive,andcooperativemulti-robot
decision-making.Thisgapmotivatesthepresentwork,whichaimstounifythesecomplementary
strengthsthroughtheproposedInteractiveInferenceBehaviorTree(IIBT)framework.
3. Preliminary
Notation
Tofacilitatethefollowingderivations, wesummarizethemainsymbolsandtheirdefinitions
inTable1.
Table1:Notationsummaryusedinthepreliminarysection.
Symbol Description
R Thei-throbotinateamofN robots
i
Oi & Observationreceivedbyrobotiattimeτ
τ
si HiddenstateofrobotR
i
si Stateofrobotiattimeτ
τ
Πi ={πi,...,πi } Policysetavailabletoroboti
1 K
πi Thek-thpolicyforroboti
k
πi Theoptimalpolicyforroboti
∗
πi Thepolicyforrobotiattimeτ
τ
P Generativedistribution
Q Variationalposteriordistribution
Ai ObservationlikelihoodmatrixP(Oi|si)
τ τ
Bi StatetransitionmatrixP(si|si ,πi)
π τ τ−1 k
Ci Outcomepreferenceprior
Di InitialstatepriorP(si)
1
E Priorpreferenceoverpolicies
F(πi) Variationalfreeenergyunderpolicyπi
k k
G(πi) Expectedfreeenergyunderpolicyπi
k k
γ Precisionparameterbalancingplanningandinference
σ(·) Softmaxfunction
4
-th Robot
β
γ
π
(π)
-th Robot
π
Figure2:ThefigureillustratestheinteractiveinferenceprocessbetweenrobotsR iandR jusingagenerativemodel.
3.1. GenerativeModelforMulti-RobotInference
WeconsiderateamofNcooperativerobotsR={R ,...,R }operatinginadynamicenviron-
1 N
ment.EachrobotR maintainsaninternalmodeloftheenvironmentthroughagenerativeprocess
i
that relates hidden states, actions (policies), and observations over time. This model serves as
thefoundationforinferenceandplanning.
The joint probability of the observation sequence Oi , latent states si , and policy πi for
1:T 1:T k
roboticanbeexpressedas:
(cid:16) (cid:17) (cid:16) (cid:17)(cid:89)T (cid:16) (cid:17)(cid:89)T (cid:16) (cid:17)
P Oi ,s¯i |πi =P si P Oi|si P si|si ,πi
1:T 1:T 1 τ τ τ τ−1
τ=1 τ=2
(1)
(cid:89)T (cid:89)T
= si ·Di Oi ·Aisi si ·Bisi ,
1 τ τ τ τ−1
τ=1 τ=1
Wherethetermsareinterpretedasfollows:
• P(si): Priorovertheinitialhiddenstate,encodedbyDi.
1
• P(πi): Prioroverpolicies,influencedbyEandCi.
k
• P(Oi|si): Likelihoodofobservationgiventhestate,parameterizedbyAi.
τ τ
• P(si|si ,πi): Transitionmodelunderpolicyπi,parameterizedbyBi.
τ τ−1 k k π
This generative structure forms the basis for both state estimation and action planning, cap-
turing the causal relationships between actions, latent states, and observations in multi-robot
collaboration.
5
3.2. VariationalInferenceandEvidenceLowerBound
Inprobabilisticroboticsanddecision-making,computingtheexactposteriordistributionover
hiddenstatesandstrategies,P(si ,πi|Oi ),isintractableduetotheexponentialgrowthofthe
1:T k 1:T
state space and the nonlinear observation models involved [27, 28]. Variational inference pro-
videsatractableapproximationbyintroducingasurrogatedistributionQ(si ,πi)andminimiz-
1:T k
ingitsKullback-Leibler(KL)divergencefromthetrueposterior:
(cid:104) (cid:105)
KL Q(si ,πi)∥P(si ,πi|Oi ) . (2)
1:T k 1:T k 1:T
This objective is equivalent to maximizing the Evidence Lower Bound (ELBO), a standard
formulationinBayesianinference[29,27]:
(cid:104) (cid:105) (cid:104) (cid:105)
L=E lnP(Oi ,si ,πi) −E lnQ(si ,πi) . (3)
Q 1:T 1:T k Q 1:T k
The negative ELBO is referred to as the variational free energy [30, 31], which can be ex-
pressedas:
(cid:104) (cid:105)
F(πi)=KL Q(si ,πi)∥P(si ,πi|Oi ) −lnP(Oi ). (4)
k 1:T k 1:T k 1:T 1:T
SincethemarginallikelihoodlnP(Oi )isconstantwithrespecttotheoptimizationobjective,
1:T
minimizingF isequivalenttominimizingtheKLdivergence. Expandingthetermsyields:
(cid:104) (cid:105) (cid:104) (cid:105)
F(πi)=KL Q(si|πi)∥P(si|si ,πi) −E lnP(Oi|si) . (5)
k (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)τ(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)k(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)τ(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)τ(cid:32)(cid:32)−(cid:32)(cid:32)(cid:32)1(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)k(cid:32)(cid:32) (cid:32) Q (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)τ(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)τ(cid:32)(cid:32)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Complexity Accuracy
Here,thecomplexitytermpenalizesdivergencebetweentheposteriorandthepriortransition
model,constrainingtheinternalmodel’sdeviationfromknowndynamics. Theaccuracyterm,
incontrast,rewardsbeliefsthatbetterexplaintheobservedsensorydata.
Byminimizingthevariationalfreeenergy,therobotcontinuouslyalignsitsinternalgenerative
modelwithexternalobservations,enablingrobuststateestimationandsituationalawareness[32,
33]. Thismechanismprovidesthefoundationforinteractivedecision-makingandcollaborative
policyselectioninmulti-agentsystems.
3.3. ExpectedFreeEnergyandPolicySelection
WhilethevariationalfreeenergyF governsperceptionbyinferringlatentstatesfromcurrent
observations, decision-making in uncertain environments requires reasoning about future out-
comes. This is achieved by minimizing the expected free energy (EFE) G for each candidate
policyπi [32]:
k
(cid:104) (cid:105) (cid:104) (cid:105)
G(πi)=KL Q(Oi|πi)∥P(Oi) − E H(Oi|si) (6)
k (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)τ(cid:32)(cid:32) k(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)τ(cid:32)(cid:32) (cid:32) Q (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)τ(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)τ(cid:32)(cid:32)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Extrinsicvalue(goalalignment) Intrinsicvalue(informationgain)
Theexpectedfreeenergycanbeinterpretedasthesumoftwocomplementaryterms:
- Extrinsic value: The first term is the Kullback-Leibler divergence between predicted out-
comesandpriorpreferencesCi,whichencouragespoliciesthatleadtooutcomesconsistentwith
taskgoalsanddesiredstates. -Intrinsicvalue: Thesecondtermrepresentsexpectedinforma-
tiongainabouthiddenstates. Maximizingthistermpromotesepistemicexplorationbyselecting
policiesthatreduceuncertainty,improvingfuturestateestimation.
6
Together,thesetwocomponentsbalancegoal-directedexploitationanduncertainty-reducing
exploration[34,25],apropertyparticularlyimportantformulti-robotcoordinationwherefuture
contingenciescannotbeexhaustivelyenumerated.
Theposteriorprobabilityofexecutingaspecificpolicyisthendefinedbyasoftmaxdistribu-
tionthatintegratesbothvariationalandexpectedfreeenergyterms:
(cid:16) (cid:17)
p(πi)=σ lnE −F(πi)−γG(πi) (7)
k k k k
WhereE denotesaprioroverpolicies(oftenuniform),F encodesthecurrent-stateevidence,
k
andGcapturestheexpectedfutureutilityofapolicy.Thehyperparameterγregulatestherelative
precision of decision-making: a larger γ emphasizes epistemic actions (information-seeking),
whileasmallerγbiasesdecisionstowardexploitingknownrewards.
This formulation ensures that each robot selects strategies that jointly minimize epistemic
uncertainty and maximize task-relevant outcomes, enabling coherent and adaptive multi-robot
coordinationevenunderpartialobservabilityandenvironmentaluncertainty.
4. Methodology
4.1. ApproachOverviewandSystemArchitecture
Toenabledistributed, adaptive, andcooperativedecision-makinginmulti-robotsystems, we
proposeanInteractiveInferenceBehaviorTree(IIBT)frameworkthattightlyintegratesproba-
bilistic inference with the modular decision-making structure of BTs. Conventional BT-based
methods typically rely on pre-defined control logic, which limits their adaptability and robust-
nessunderuncertainty. Incontrast,theproposedIIBTarchitectureembedsinferencecapabilities
directlywithinBTexecutionnodes,allowingrobotstoupdatetheirbeliefsonline,dynamically
adjustdecisionpriorities,andcoordinatewithteammatesinpartiallyobservableenvironments.
Environment
Planning
Share Intention
n
Robot n
n Robot 2
Behavior Tree status? ? 2
T R r o a b n o s t f 1 er Logic Beh a v i o r T P r o e l e icy Cshtaotouses? ? 1
2 Upd
T
a
r
t
a
e
n
b
s
e
f
l
e
ie
r
f
Logic （BehI na v t ir o e ra P Tctr o ie l ve ic e y I n C fssh ett r oaa e ott nuus c ess?e） ?
1 UUpp U dd T p aa d r tte a e a n t bb e s ee f ll b e ii e ee r l ff i L e o f gic Coll （ abo I （ （ n r a t t I I r inn e v a tt e rr PP c eTe t oo aa i a P lv l ccs ii e t c t c k o i i yy l I vvM i n c ee CC y f s o B I e I hh t dnn r TC aoo e e ff too l h e n e s uss rrot cese ee ao e nn ts ） u c ecee s ）） Intention
Collabo（ratIinvter eTaacstki vMe oIdneflerence）
Policy Choose
Update belief
CoClloalblaobrao（triavtIein vTtera eTsakac sMtki vMoed oeIldneflerence）
Collaborative Task Model
Collaborative Task Model
Figure3: WorkflowofinteractivenodesintheBT.EachrobotcollectsitslocalobservationsOi,abstractstheminto
τ
logicalvariablesLi, andupdatesitsbelief si. TheBTemitsapreferencematrixCi totheinferencemodule, which
τ
queriestaskmodelsforAi,Bi,Di,incorporatesotherrobots’intentions,andreturnsstate/policyinformationbacktothe
π
BTforexecution.
EachrobotR maintainsaninternalgenerativemodel
i
Mi ={Ai,Bi,Ci,Di},
π
7
as defined in Table 1, which captures the probabilistic relationships between sensory observa-
tions, latent states, action dynamics, and task preferences. Based on this model, the inference
layer estimates latent state beliefs si, predicts possible outcomes under candidate policies πi,
τ k
andevaluatestheirexpectedfreeenergy. TheresultingposteriorbeliefsarethenfedintotheBT
layer, which orchestrates task execution through a structured hierarchy of condition and action
nodes.
This integration transforms the BT from a static execution tree into a dynamic, belief-aware
controlarchitecture. Itinheritstheinterpretability,modularity,andmaintainabilityofBTswhile
gainingtheadaptivity,robustness,andcoordinationcapabilitiesassociatedwithprobabilisticin-
ference. Thishybriddesignisparticularlyadvantageousinmulti-robotscenarioswhereagents
mustmakedecisionsbasedonpartialobservations,dynamicallychangingobjectives,anduncer-
tainintentionscommunicatedbypeers.
4.2. InteractiveInferenceBTNode(IIBT)Design
TheIIBTnodeservesasthecoreinterfacethatbridgesthereactiveexecutionflowofabehavior
tree with the probabilistic reasoning of the inference module. At each tick, the node not only
decides which action policy to execute but also updates its belief and task preference in light
of new observations and teammates’ inferred intentions. Algorithm 1 outlines the complete
reasoningandexecutioncycleofthenodeforrobotR.
i
8
Algorithm1:StrategySelectionProcesswithinanIIBTNodeforRobotR
i
1 Tick(ℑ task)
2 ℑ task.r←running
3
RetrieveMi={Ai,Bi
π
,Ci,Di}fromtaskmodel
4
RetrievepreferencematrixCifromℑ
task
5 AcquirecurrentobservationOi τ fromenvironment
6
Updatebeliefstatesi
τ
andlogicvariablesLi
7
ConstructcandidatepolicysetΠi
8 fork∈{1,...,N}\ido
9 ReceiveR kpolicyintentionπk
10
Πi←Πi∪{πk}
11 end
12
πi
τ
←InteractiveInfer(si
τ
,Mi,Πi)
13 ifπi τ ≡πi stop then
14 ℑ task.r←success
15 returnℑ task.r
16 else
17
whileLi(cid:60)πi.precdo
18
Li= fL(πi.prec)
19
Ci←Ci+Li
20
ReconstructΠi
task
21 fork∈{1,...,N}\ido
22 ReceiveR kpolicyintentionπk
23
Πi←Πi∪{πk}
24 end
25 πi τ ←InteractiveInfer(si τ ,Mi,Πi,Oi τ )
26 ifTimeoutthen
27 ℑ task.r← failure
28 returnℑ task.r
29 end
30 end
31
ifcountConnectedRobot=N−1then
32 Execute(πi τ )
33
Ci←Ci−Li
34 else
35 Execute(πi wait )
36 end
37 ifπi τ ∈πi stop .precthen
38
πi
τ
←πi
stop
39 end
40 ℑ task.r←running
41 returnℑ task.r
42 end
Role in the architecture.. As shown in Fig. 3, the IIBT node is the execution-time interface
that closes the loop between probabilistic inference and the BT tick-cycle. At every tick, it (i)
readsthecurrentbeliefandobservation,(ii)updatesthepreferencematrixusingsymboliclogical
evidence, (iii) incorporates peer intentions, (iv) runs interactive inference over the candidate
policies, and (v) executes or defers actions depending on team connectivity and preconditions.
ThefulltickroutineisgiveninAlg.1.
As shown in Fig. 3, the IIBT node closes the perception–inference–action loop during each
BTtick.Atruntime,it(i)collectslocalobservationsandcurrentbeliefs,(ii)transformssymbolic
9
logicintoquantitativepreferenceupdates, (iii)exchangespolicyintentionswithpeers, and(iv)
performs interactive inference to select the most plausible cooperative policy (Alg. 1). This
makeseachnodeanautonomousreasoningunitthatalignsitslocalexecutionpolicywithboth
environmentalfeedbackandteam-levelbeliefconsistency.
Inputs and maintained state.. For robot R, the node retrieves its generative model Mi =
i
{Ai,Bi,Ci,Di} (line 3), obtains sensory observation Oi (line 5), and updates both the filtered
π τ
beliefsi anditssymbolicabstractionLi(line6). ThecandidatepolicypoolΠi(line7)iscontin-
τ
uouslyexpandedwithpeerintentionsπk fromotherrobots(lines8–11),creatingabelief-aware
andcontext-sensitiveactionspace.
ModelSemantics.. Eachrobotmaintainsalatentstatevector
si =[si ,si ,si ,si ]⊤,
τ loc hold place free
representingtherobot’sbeliefinreachingalocation,grasping,placing,orbeingidle. Theinfer-
enceprocessoperatesonthejointstate
sjoint =[s1,s2,...,sN,sresult]⊤,
τ τ τ τ τ
corresponding respectively to the robots’ beliefs and the global task outcome. Each state di-
mension is normalized as a probability distribution representing belief strength (e.g., si =
hold
[0.9,0.1]⊤indicates90%confidenceofholdingtheobject).
Here,sresultisagloballatentvariablesummarizingthecooperativetaskoutcome(e.g.,overall
τ
success, failure, or pending status). This concatenated representation allows the inference pro-
cesstocapturecross-robotdependenciessuchastemporalordering,spatialcoupling,andshared
resourceconstraints. Inpractice,eachIIBTnodemaintainsandupdatesitsownmarginalbelief
si at time τ, but exchanges summarized information about other agents’ inferred states during
τ
coordination,therebyrealizingadistributedyetcoherentjointinferenceprocessacrosstheteam.
Thisformulationcapturesinter-robotdependencieswithinaunifiedgenerativeprocessEq.(1).
Observationrepresentationandjointobservationmatrix.. ForasystemconsistingofN cooper-
ativerobots,theoverallobservationatdiscretetimestepτisrepresentedas
Ojoint =[O1,O2,...,ON,Oresult]⊤,
τ τ τ τ τ
where Ojoint denotes the joint observation vector of the multi-robot system. It concatenates all
τ
individualrobots’localobservationsandatask-leveloutcomevector,formingthesensoryinter-
facebetweenthephysicalenvironmentandtheinferenceprocess. EachelementofOi isabinary
τ
or probabilistic indicator that reflects whether a certain physical or symbolic event is currently
observed.
ForeachrobotR (i=1,...,N),thelocalobservationvector
i
Oi =[oi,oi,...,oi ]⊤
τ 1 2 mi
encodes the status of its task-relevant latent variables. Each component oi (j = 1,...,m) is a
j i
binaryobservationassociatedwiththe j-thlatentstatesi:
j

oi =
 1, ifthestatesi
j
isachievedattimeτ,
j 0, otherwise.
10
Hence,Oi providesadirectlogicalmappingfromtherobot’sperceptionspacetoitslatentbelief
τ
state si. Inpractice,theseelementscanbecomputedfromsensorfeedback,symboliccondition
τ
checks,orcommunicationmessages.Forinstance,ifrobotR successfullygraspsanobject,then
1
o1 =1whileotherentriesremainzero.
hold
In addition to local observations, the system maintains a task-level observation vector Oresult
τ
thatsummarizestheglobalcooperativeoutcomeatthecurrenttimestep:
Oresult =[oresult , oresult , oresult]⊤.
τ success failure null
EachentryinOresult isabinaryindicatorspecifyingwhethertheglobaltaskhasreachedacorre-
τ
spondingterminalorintermediatestate:

oresult =
 1, iftheoveralltaskisinstatuskattimeτ,
k 0, otherwise.
This vector serves as a shared global signal that allows all robots to condition their inference
on collective task progress, enabling synchronization and cooperative adaptation among team
members.
By vertically stacking all local and global observations, the joint observation matrix can be
expressedas
 o1 o1 o1 o1 ⊤
Oj τ oint =

o
l
2
l . .
.
o
o
c
c
o
h
2
h . .
.
o
o
l
l
d
d
o
p
2
p
l
l . .
.
a
a
c
c
e
e
o
f
2
f . .
.
r
r
e
e
e
e

,
oresult oresult oresult –
success failure null
where“–”indicatesanullornon-applicableentry.Eachrowcorrespondstooneagent(including
theglobalresultlayer),whileeachcolumndenotesasemanticdimensionofthetaskspace,such
as localization, grasping, placement, or idle status. This joint structure allows the inference
moduletointegrateheterogeneoussensoryandsymbolicinformationacrossagents,providinga
unifiedobservationbasisfordistributedbeliefupdatingwithintheIIBTframework.
Observationandtransitionmodelmatrices.. Consequently,thelikelihoodmatrixAiandtransi-
tionmatrixBi areconstructedinblock-diagonalformtorepresentbothindividualrobotdynam-
π
icsandinter-robotdependencies:
Ai 0 0 ··· 0 
Ai =

0
0
. . .
r1
A
0
. . .
i
r2
0
0
. . .
A
·
.
·
.
i
r
·
.
N
0
0
. . .

,
0 0 0 ··· Ai
result
B11 B12 ··· B1N
Bi π =

B
. . .
21 B
. . .
22 ·
.
·
.
·
.
B
. . .
2N

.
BN1 BN2 ··· BNN
11
Here,eachblockAi andBij correspondstorobotR ’slocalobservationandtransitionmodel,
rj j
respectively. The diagonal terms Bii encode the self-dynamics of each robot, while the off-
diagonaltermsBij (i (cid:44) j)representthecouplingeffectsbetweenrobots,suchasphysicalinter-
ference,taskdependencies,orcoordinationconstraints. Thebottom-righttermAi describes
result
the observation likelihood of the global task outcome, associated with the global observation
vectorOresultdefinedearlier.
τ
This block-structured formulation enables the inference module to reason about multi-agent
dependenciesprobabilistically,whileeachIIBTnodestillperformslocalupdatesthroughdecen-
tralizedmessagepassing.
Local likelihood model.. For each robot R, the observation (likelihood) matrix Ai defines the
i
conditionalprobabilityofreceivingaparticularobservationgiventhecurrentlatentstate:
P(Oi |si)=Aisi,
τ τ τ
whereAi ∈Rmi ×ni mapsthen
i
-dimensionallatentstatespacetothem
i
-dimensionalobservation
space. EachcolumnofAi specifiesthelikelihooddistributionoverobservableoutcomeswhen
thesystemisinaparticularhiddenstate.
Physically, Ai captures the reliability of the perception or sensing process. For instance, a
diagonal entry of 0.9 indicates that the vision or gripper sensor correctly reflects the true state
90%ofthetime, whiletheremaining0.1modelsobservationnoisecausedbyocclusion, light-
ing change, or sensor failure. When the task involves symbolic communication (e.g., “object
placed”), the same formulation applies, treating message acknowledgment as an observation
channel.
Localtransitionmodel.. Thestate-transitionmatrixBi encodeshoweachrobot’sinternalbelief
π
evolvesovertimegivenitsexecutedpolicyπi. Formally,
P(si |si,πi)=Bi si,
τ+1 τ πi τ
whereBi ∈Rni ×ni istheaction-specifictransitionmatrixassociatedwithpolicyπi.Eachcolumn
πi
ofBi representstheprobabilitydistributionofthenextstategiventhecurrentstateandaction.
πi
Differentactionscorrespondtodifferenttransitionmatrices:
B , B , B , B ,
move pick place idle
where, for example, B increases the probability of si = 1 as navigation proceeds, B
move loc pick
increasestheprobabilityof si = 1afterasuccessfulgrasp,B increasestheprobabilityof
hold place
si = 1onceplacementisachieved,andB ≈ I maintainsthecurrentbeliefstatewhenno
place idle 4
actionisexecuted.
These transition probabilities are empirically estimated from execution logs, using suc-
cess/failure ratios or temporal statistics, and are normalized column-wise to ensure valid prob-
ability distributions. By combining Ai and Bi , each robot maintains a physically grounded
πi
generative model that connects sensory uncertainty, action dynamics, and latent belief updates
withintheInteractiveInferenceBehaviorTreeframework.
12
Priorstatedistribution.. ThepriormatrixDi definestheinitialbeliefoverthelatenttaskstates
ofrobotR
i
beforetaskexecutionbegins. Formally,Di ∈Rni isacolumnvectorrepresentingthe
initialprobabilitydistributionofthelatentstatesi :
τ=0
P(si )=Di,
τ=0
wheren denotesthenumberoftask-relevanthiddenstatesmaintainedbyrobotR.Eachelement
i i
di ofDi =[di,di,...,di ]⊤specifiesthepriorprobabilityofthe j-thlatentstatebeingtrueatthe
j 1 2 ni
initialtimestep. Thisinitializationencodestheassumptionthattherobothasnotyetreacheda
targetlocation,graspedanobject,orcompletedaplacementattimeτ=0.
From a probabilistic perspective, Di serves as the starting point of the generative process,
definingthepriorbeliefP(si)inthejointdistributionEq.(1),whereP(si)correspondsexactlyto
0 0
Di. ThisensuresthatallsubsequentinferencesandbeliefupdateswithintheIIBTframeworkre-
maingroundedinaconsistentprobabilisticprior,promotingstableinitializationandreproducible
behavioracrossdifferentrobotagents.
ThepreferencematrixCi definestheextrinsicdesirabilityofpotentialobservationoutcomes,
guidingeachrobot’spolicyselectiontowardgoal-consistentbehaviors. Inthemulti-robotsetup,
Ciismodeledjointlyratherthanindependentlyperagenttoensurethatallpreferencesarealigned
with both individual objectives and collective task constraints. Formally, the joint preference
matrixattimestepτisexpressedas
Ci =[Ci , Ci , ..., Ci , Ci ]⊤,
r1 r2 rN result
whereeachblockCi
rj
∈Rmj encodesthepreferencedistributionofrobotR
j
overitslocalobser-
vationspaceOj,andCi representstheteam-levelpreferenceoverglobaloutcomes(e.g.,task
τ result
successorfailure).
Eachpreferencevectorisdefinedas
Ci =[ci, ci, ..., ci ]⊤,
rj 1 2 mj
whereci denotesthedesirabilityofobservingoutcomeoj underthecurrenttaskobjective. High
k k
preferencevaluesbiastheinferenceprocesstowardpoliciesexpectedtoproducethoseoutcomes,
asreflectedintheexpectedfreeenergycomputationoftheInteractiveInferencestep.
The preference matrix Ci specifies desirable observation outcomes derived from task goals,
e.g.,highweightonl duringgraspingphasesandonl duringassembly.Duringexecution,
hold place
Ci is adaptively updated through the logic-to-preference mapping f , which injects symbolic
L
conditions (e.g., preconditions or unmet goals) into the preference vector to bias the expected
freeenergy.
Overall, thecombination{Ai,Bi,Ci,Di}definesacompactyetphysically groundedgenera-
π
tivemodelthatlinkssensoryuncertainty,actiondynamics,andgoalpreference. Thesematrices
can be tuned directly from empirical robot data or analytically set based on system reliability,
providing a clear interface between probabilistic inference and the robot’s physical control do-
main.
Tick-cycleflow.. Atthebeginningofeachtickthenodesetsitsreturnstatustorunning(line2)
and constructs a local candidate policy pool Πi (line 7). It then collects the most recent policy
intentionsbroadcastbyotherrobots(lines8–11)andaugmentsΠi accordingly,yieldingapeer-
aware candidate set. Given (si,Mi,Πi), the node calls InteractiveInfer (line 12) to obtain the
τ
13
currentpolicyπi viathefree-energybasedposterior(cf. Sec.3.3). Iftheselectedpolicyisthe
τ
terminalpolicyπi (line13),thenodereturnssuccess(lines14–15).
stop
Algorithm2:InteractiveInfer(lite): One-stepActive-InferenceScoringforR
i
Input:Beliefsi,modelMi={Ai,Bi,Ci,Di},candidatesΠi,currentobservationOi,policypriorE,precision
τ π τ
γ
Output:Selectedpolicyπi andposteriorQ(πi)
τ
1 P(Oi τ )←Softmax(Ci) // preference-induced outcome prior
2
foreachπ∈Πido
3 b′←Bi(·|π)si τ // next-belief (one-step rollout)
4 q(Oi τ )←Aib′ // predicted outcome marginal
5 Eext ←DKL (cid:0) q(Oi τ )∥P(Oi τ ) (cid:1)
6 Eint ← (cid:80) s b′(s)H (cid:0) Ai(:,s) (cid:1)
7 G(π)←Eext −Eint
8 F(π)←− (cid:80) s si τ (s)logAi(cid:0) Oi τ ,s (cid:1)
9
Score[π]←lnE(π)−F(π)−γG(π)
10 end
11 Q(πi)←Softmax(cid:0) Score[·] (cid:1) // log-sum-exp stabilized
12 πi τ ←argmax π∈Πi Q(πi=π)
13
returnπi
τ
,Q(πi)
Logical-to-preferenceshaping.. Inourmulti-robotsetup,thepreferencematrixCi isalsomod-
eledjointlyratherthanindependentlyperagent. Specifically,Ci isstructuredasacolumn-wise
concatenationCi =[Ci ,Ci ,...,Ci ]⊤,whereeachblockencodestheextrinsicpreferencesof
r1 r2 result
one robot. C1 and C2 represent, respectively, the desired observation likelihoods for robots R
1
andR . Eachelementci inCi = [ci,ci,...,ci ]⊤ correspondstothedesirabilityofobserving
2 k rj 0 1 M
outcomeounderrobotR’scurrenttaskobjective.
i
Ifpreconditionsforπi arenotsatisfied(Li (cid:60) πi.prec, line17), thenodecomputesthemini-
τ τ
mallogicalevidencerequiredtosatisfythepreconditionusingthemapping f (·)(line18),and
L
appliesitasanadditiveupdateonCi(line19):
Li = f (πi.prec), Ci ←Ci+Li.
L τ
Intuitively, this raises the extrinsic preference for outcomes that make the precondition true,
thereby biasing the EFE toward prerequisite-achieving actions. The node then reconstructs the
task-specificpolicypoolΠi (line20),refreshespeerintentions(lines21–24),andre-runsIn-
task
teractiveInfer(line25). Ifatimeoutoccurs(lines26–29),thenodefailsfast,returningfailure.
Executionandsynchronization.. Whenthecommunicationlayerreportsthatallteammatesare
connected (countConnectedRobot = N −1, line 31), the node executes the selected policy πi
τ
(line 32). Immediately after dispatch, it rolls back the temporary preference boost associated
withthejust-satisfiedlogicalincrement(line33),i.e.,
Ci ←Ci−Li,
restoring the baseline preferences to avoid long-term drift. If full connectivity is not met, the
node executes a wait policy πi (line 35), preserving safety and coordination while messages
wait
converge.
Terminationguard.. Ifthecurrentlyselectedpolicybecomesamemberofthestopprecondition
set (line 37), the node promotes it to πi (line 38). Otherwise, it continues in running state
stop
(line40)andreturnscontroltotheparentBTcomposite(line41).
14
Discussionandinterface. TheIIBTnodeexposestwolight-weightinterfacestotherestofthe
BT: (i) a logic-to-preference adapter f that transforms symbolic BT conditions into additive
L
updatesonCi,and(ii)theInteractiveInfercallthatconverts(si,Mi,Πi)intoasoftmaxposterior
τ
overpolicies,usingthevariationalfreeenergyF (perceptionterm)andexpectedfreeenergyG
(prospectionterm)definedinSecs.3.3. ThisdesignpreservesBTinterpretabilityandreactivity
whileendowingeachnodewithuncertainty-aware,preference-drivenadaptation.
5. ImplementationofInteractiveInferenceNodes
5.1. RobotsInteractiveInference: ASimpleExample
This section presents a case study on interactive inference in a multirobot system, showing
howrobotsplanbehaviorsunderunknownobjectiveswhileminimizingfreeenergy.
p p p p p p
0 1 3 4 5 6
p p p p p p p
7 8 9 10 11 12 13
p p p p p p p
14 15 16 17 18 19 20
p p p p p
21 22 23 25 27
goal goal
1 2
p p p p p p
29 30 31 32 33 34
p p p p p p p
35 36 37 38 39 40 41
p p p p p p p
42 43 44 45 46 47 48
Figure4:Tworobots,R 1andR 2,operateina7x7grid,withcellpositionslabeledas{p0,p1,...,p48 }.Theenvironment
featurestwogoals,goal1andgoal2,andeachrobotcreatespathstobothgoals.
Weexaminetworobots,R andR ,inanenvironmentshowninFig.4withtwogoals(goal
1 2 1
and goal ). The robots cannot identify their goals but can see each other’s positions on a grid
2
mapwithlocations{p ,p ,...,p }.
0 1 48
The strategy set Π1 includes the strategies for robot R to achieve two goals. The paths
1
to goal are {p ,p ,p ,p ,p } and {p ,p ,p ,p ,p ,p ,p }. The paths to goal
1 28 29 30 31 24 28 35 36 37 38 31 24 2
are {p ,p ,p ,p ,p ,p ,p } and {p ,p ,p ,p ,p ,p ,p ,p ,p ,p ,p }.
28 29 30 31 32 33 26 28 35 36 37 38 39 40 41 34 27 26
The strategy set for robot R (Π2) includes paths to goal : {p ,p ,p ,p ,p ,p ,p }
2 1 2 1 8 15 22 23 24
and {p ,p ,p ,p ,p }; and paths to goal : {p ,p ,p ,p ,p ,p ,p } and
2 3 10 17 24 2 2 3 4 11 18 25 26
{p ,p ,p ,p ,p ,p ,p }.
2 3 4 5 12 19 26
ThetaskrequiresrobotsR andR toreachdifferentgoalssimultaneously. Thecombination
1 2
ofstrategyselectionsisdefinedasΠ1×Π2 ={(π1,π2)|π1 ∈Π1,π2 ∈Π2}.
For robot R , the hidden states are s1 = {s1,s2}, where s1 = [p ,p ,...,p ]⊤ and s2 =
1 τ τ τ 0 1 48 τ
[p ,p ,...,p ]⊤.
0 1 48
Wedefinetheobservationsetsas
O1 ={Or1, Or2, Oresult},
τ τ τ τ
15
Root Root
? IsArrived
(goal1, goal2)
IsArrived Exist(obstacle) MoveTo(goal2)
(goal1 or goal2)
d 1,goal1 <d 1,goal2 MoveTo(goal1)
（a） （b）
Figure5: Fig.(a)showsthetraditionalmethodforrobotR 1 toselectthenearestgoalwhileconsideringotherrobots’
states. Fig.(b)illustratestheinteractiveinferencenode,whichselectsastrategytominimizefreeenergy. Bothfigures
demonstratethesamefunctionality.
whereOr1isrobotR ’spositionobservation,Or2containsobservationsfromrobotR ,andOresult
τ 1 τ 2 τ
indicates whether both robots reached their goals simultaneously. The likelihood matrix is de-
finedas
A1 ={A1 , A1 , A1 }.
r1 r2 result
The matrix A1 illustrates the relationship between robot R ’s observable position and its
r1 1
hidden state, with a probability of accurately determining its position at 0.9952. Formally,
A1 {p,p,:} = 0.9952 for i ∈ [0,48], indicating that robot R correctly determines its own
r1 i i 1
position with probability 0.9952. The matrix A1 {p,p,:} = 0.904 for i ∈ [0,48] reflects the
r2 i i
accuracy of estimating the other robot’s position. Let A1 denote the joint inference result.
result
Success occurs when robots R and R achieve different goals simultaneously, represented by
1 2
A1 (1,p ,p ) = 1, orviceversa. Iftherobotsfailtoachievetheirgoalssimultaneouslyor
result 24 26
selectthesamegoal,itresultsinataskfailure,indicatedbyA1 (2,p ,p )=1,orviceversa.
result 24 26
Ifneitherrobotreachesagoal,thetaskresultisnull,denotedasA1 (0,:,:).
result
The transition matrix B1 describes how hidden states evolve over time τ based on control
actionsa τ ∈ U. Asequenceofcontrolactionsisrepresentedasπi = {a τ=1 ,a τ=2 ,...,a τ=n },with
πi ∈Πi,whereΠi includesallstrategiesforrobotR. ThematrixB1 ={B1 , B1 }consistsofthe
i r1 r2
statetransitionmatricesforrobotR androbotR . SetBi{p ,p ,a } = 1.0,wherea isthe
1 2 next cur τ τ
robot’sactionattimeτ, p isitscurrentposition,and p isthepositionafteractiona .
cur next τ
Table2:SettingStrategyPrioritiesinExperiments
Planning Precondition Postcondition
πi Exist(obstacle) li =−(max(Ci)+1)
obs obs ri
πi !Exist(obstacle) li =max(Ci)+1
points And!IsArrived(points) add ri
!Exist(obstacle)
πi And!IsArrived(goal) li =max(Ci)+1
goal goal ri
AndIsArrived(points)
πi IsArrived(goal) -
stop
The preference matrix Ci, aligned with the observation matrix Oi, indicates preferences for
τ
goallocationsregardingtaskoutcomes,asdefinedin
C1 ={C1 , C1 , C1 },
r1 r2 result
whereC1 {p ,p } = 1,andC1 {success} = 1. ThesearesetbasedonthePref-Weightsin
{r1,r2} 24 26 result
Table5.1.
16
WederivetheprobabilitydistributionsofstrategiesinΠ1×Π2basedonrobotconfigurations.
The robots select and execute the strategies with the highest probabilities, resulting in the fol-
lowingdistributions:
P =[0.151×10−7,0.566×10−6,0.995×10−7,0.210×10−6],
g1−g1
P =[0.405×10−7,0.357×10−7,0.081,0.072],
g1−g2
P =[0.746×10−7,0.847,0.682×10−7,0.573×10−6],
g2−g1
P =[0.133×10−7,0.117×10−7,0.408×10−7,0.360×10−7].
g2−g2
Here,P representstheprobabilitydistributionofrobotR movingtowardgoal whilerobot
gi −gj 1 i
R moves toward goal . Robot R will follow the path {p ,p ,p ,p ,p ,p ,p }, while
2 j 1 28 29 30 31 32 33 26
robotR willfollowthepath{p ,p ,p ,p ,p ,p ,p }.
2 2 1 8 15 22 23 24
5.2. InteractiveInferenceNodesforConflictHandling
Wedemonstratehowrobotsresolvetaskconflictsinadynamicenvironment,aimingtomeet
theexpectedpostconditionmatrixπ1 .postcforachievingthestrategyπ1 (seeTable5.1).
goal stop
a' a' 6 a 5 b' b 5
5 6 b
4
a
4
b'
a' 4 Obstacle 5
b
a 3
a' 3 b'
3 4 b'
3
a b
2 Interactive Inference 2
a
1
b
Robot1 Policy Robot2 Policy 1
Figure6: RobotR 1, locatedatpositiona2, detects (cid:110) anobstacleatpos (cid:111) itiona3. Consequently, robotR 1 abandonsits
original path {a1,a2,a3,a4,a5 } and modifies it to a2,a′
3
,a′
4
,a′
5
,a′
6
,a5 . To ensure that it reaches the destination b5
simultane
(cid:110)
ouslywithrobotR 1,
(cid:111)
robotR 2,positionedatb2,engagesininteractiveinferencewithrobotR 1andacquiresa
newpath b2,b′
3
,b′
4
,b′
5
,b′
6
,b5 .
Whilepursuingtheirgoals,therobotsevaluateexecutionconditionsinrealtimeattheinter-
active inference nodes (Algorithm 1, Line 17). When executing π1 , robots must ensure no
goal
obstacles block the current path, a precondition π1 .prec = {E1 ,E1 ,E1 }. In the current
goal r1 r2 result
strategy, E1 {p ,p ,p ,p ,p ,p } = 0 and E1 {p ,p ,p ,p ,p ,p } = 0. If a tempo-
r1 29 30 31 32 33 26 r2 1 8 15 22 23 24
raryobstacleisaddedat p onrobotR ’spath,theobservationlogicalquantityforrobotR is
30 1 1
L1 = {l1 ,l1 ,l1 },wherel1 {p } = l1 = −1. Thus,L1 (cid:60) π1 .prec. Theenvironmentallogi-
r1 r2 result r1 30 obs goal
calvariableisaddedtothepreferencematrixC1,resultinginC1 {p } = −1,C1 {p ,p } = 1,
r1 30 r1 24 26
andC1 {p ,p }=1.
r2 24 26
At this stage, robot R generates several obstacle avoidance strategies, denoted as
1
Π1 . These strategies are transformed into the path sets {p ,p ,p ,p ,p ,p } and
obs 29 22 15 16 17 24
{p ,p ,p ,p ,p ,p ,p ,p }.
29 36 37 38 31 32 33 26
Consequently, the strategy set for robot R becomes Π1 = Π1 ∪π1 . Meanwhile, robot
1 obs goal
R , located at p , generates a new strategy set Π2 , which includes {p ,p ,p ,p ,p ,p }
2 8 goal 1 8 9 10 17 24
17
and {p ,p ,p ,p ,p ,p ,p ,p }. Robot R incorporates its current strategy into Π2 =
1 8 9 10 11 18 19 26 2
Π2 ∪π2 .Aftercombiningthetwostrategysetsandexecutinginteractiveinference,theresult-
goal goal
ingstrategydistributionisP(Π1×Π2)=[0.784×10−6,0.166×10−6,0.521×10−7,0.999,0.156×
10−6,0.134×10−7,0.898×10−13,0.256×10−7,0.554×10−14].Basedontheprincipleofmin-
imizing free energy, robot R will move along the path {p ,p ,p ,p ,p ,p ,p }, while
1 29 36 37 38 31 32 33
robotR willchoosethepath{p ,p ,p ,p ,p ,p }.Duetotheintroductionofobstacles,both
2 1 8 9 10 17 24
robotsabandontheiroriginalstrategiesinfavorofnewonesthatminimizefreeenergy.
6. Experiments
6.1. CooperativeNavigationwithoutPredefinedGoals
Charger
Station
Obstacle
Goal
Robot
Figure7:Thefactoryscenefeaturesthreerobots,movableobstacles,andchargingstations,withtherobotscollaborating
toreachdesignatedgoals.
We previously demonstrated how interactive inference nodes manage tasks and conflicts. In
this subsection, we use these nodes to create a complex BT for mobile robots performing col-
laborativetasksindynamicenvironments. Inoursimulation1,threerobotsautonomouslyselect
goalsandadapttochanges,coordinatingtheirmovementsthroughaBTcontrolledbyinteractive
nodes. ThetasksettingsfortherobotgroupareshowninTableI.Fig.8showstheindependent
BTcontrolstrategiesforeachrobot. TraditionalBTsrequire21nodes,whileourapproachuses
only5,achievinga76.2%reductionindesigncomplexity.
Robots R , R , and R start at positions p , p , and p , respectively. The goal , goal ,
1 2 3 2 28 45 1 2
andgoal arelocatedat p , p ,and p ,whiletheirchargingstationsareat p , p ,and p .
3 24 26 32 13 27 41
In subsequent experiments, strategy sets Π1, Π2, and Π3 employ the A* algorithm to design
pathsforeachgoal,incorporatinga = waitintotheactionstrategiesforeachrobot. Toreduce
τ
computationalload,weutilizepairwiseinteractions—suchasbetweenR andR ,andbetween
1 2
R andR —asillustratedinFig.9.
1 3
Forinstance,R interactswithbothR andR . RobotR generatesstrategiesforgoal ,goal ,
1 2 3 1 1 2
orgoal (Algorithm1,Line7). RobotsR andR thenapplytheinteractiveinferenceprocedure
3 1 2
(Algorithm1,Line10)usingthedefinedlikelihoodmatrixA1andtransitionmatrixB1.RobotR
1
selectsthestrategywiththehighestposteriorprobability.Thesameinferenceprocessisrepeated
betweenrobotsR andR . InthepreferencematrixC1, wesetC1 {p ,p ,p } = 1and
1 3 {r1,r2,r3} 24 26 32
C1 {success}=1.
result
1https://youtu.be/KX_oT3IDTf4
18
?
MoveTo(Charger)
?
Battery Enough? IsArrived
(goal1, goal2)
R3-R2 IsArrived
(goal1 or goal2)
Joint Arrival? Min(Sum(Policy1,Policy2)) ?
MoveTo(G2)
?
R3-R1
Policy1:R1 →G1 MoveTo(G1)
0.0 0.2 0E.4xist O0b.s6tacle?0.8 Pol 1C ic .o y 0n (O st b r s u t c a t c le) Exist WayPoints? Polic C y o (W ns a tr y u P c o t i nts)
Probability
Figure8: Theblueareashowsthereplacementoftraditionalcontrolframeworkswithinteractiveinferencenodes,em-
phasizingtheircompressioneffectsrelativetotraditionalBTnodes.
spuorG
3Strategies Distribution( 3)
1.000
0.857×106
0.802×106
0.723×106
0.568×106
0.201×1026
0.184×1026
0.161×1019
0.148×1019 0.119×1019
0.533
0.472×106
0.441×106
0.467
0.327×106
0.110×1026
0.101×1026
0.100×1019 0.920×1020
0.162×1020
0.0 0.2 0.4 0.6 0.8 1.0
Probability
2R-3R
1R-3R
3Strategies Distribution( 3)
0 0 . . 8 8 0 57 2 × × 1 1 0 0 6 6 0.999
0.723×106
0.568×106 0.201×1026
0.184×1026
0.161×1019
0.148×1019
0.119×1019
0.533
0.472×106
0.441×106
0.467
0.327×106 0.110×1026
0.101×1026
0.100×1019
0.920×1020
0.162×1020
0.0 0.2 0.4 0.6 0.8 1.0
Probability
3R-2R
1R-2R
2Strategies Distribution( 2)
0.0 0.2 0.4 0.6 0.8 1.0
Probability
3R-1R
2R-1R
1Strategies Distribution( 1)
0.139×108 0.617×101
0.523×109 0.606×101 0.117×109 0.220
0.169×1014 0.924×101
0.679 0.770×107 0.306 0.385×107
0.104×101 0.316
0.395×102 0.177
0.884×103 0.595×101
0.998×108 0.125×101
0.155×108 0.586×106
0.102×108 0.576×106
0.919×1014 0.446
0.204×108 0.406×106
0.553 0.432×1014 0.427 0.153×1013
0.117×101 0.554
0.775×102 0.567×106
0.156×107 0.565×106
0.303×108 0.124×106
Figure9:TheprobabilitydistributionsforrobotsR 1,R 2,andR 3intheleft,middle,andrightfigures,respectively.
Thesixthstrategyhasthehighestposteriorprobability: P(π1)=0.553,P(π2)=0.679. Robot
6 6
R follows the path {p ,p ,p ,p ,p ,p ,p } and communicates this policy intention to
1 28 21 22 23 24 25 26
robot R . Robot R then infers its strategy with probabilities of 0.554 and 0.316, selecting
2 2
{p ,p ,p ,p ,p ,p ,p } to relay to robot R . Robot R performs inference based on the
2 9 16 16 23 23 24 3 3
intentions of robots R and R , selecting {p ,p ,p ,p ,p ,p ,p }. This demonstrates
1 2 45 38 31 31 31 31 32
distributedinteractiveinferenceamongtherobots.
ThissectiondetailsthetaskexecutionprocessforrobotR . Whilemovingtoposition p ,it
1 21
encountersanobstacleatp .TherobotupdatesitsenvironmentalobservationOr1ateachcycle,
23 τ
generatinglogicalvariables
L1 ={l1 ,l1 ,l1 ,l1 }.
r1 r2 r3 result
Robot R assigns a preference value of −1 to the obstacle at p , resulting in a conflict be-
1 23
tweentheobstacle-avoidanceandcurrentstrategies,asL1{l1 } (cid:60) !Exist(obstacle)(Algorithm1,
r1
Line 17). Robot R adds L1{l1 } to matrix C1 (Algorithm 1, Line 18) and guides robots R ,
1 r1 1
R , and R to develop new strategies (Algorithm 1, Line 19). The preference matrix updates
2 3
to C1 = C1 +l1 . Robot R then performs inference (Algorithm 1, Line 21), yielding maxi-
r1 r1 r1 1
mum strategy probabilities of 0.166 and 0.252, leading to the path {p ,p ,p ,p ,p ,p }.
21 14 15 16 17 24
Robot R selects {p ,p ,p ,p ,p ,p }, and robot R chooses {p ,p ,p ,p ,p ,p }.
2 9 10 17 24 31 32 3 38 38 31 24 25 26
19
1Strategies Distribution( 1) 2Strategies Distribution( 2) 3Strategies Distribution( 3)
0.139×108 0.617×101
0 0 . . 1 5 1 2 7 3 × × 1 1 0 0 9 9 0.606 0 × .2 1 2 0 0 1 0 0 . . 8 8 0 57 2 × × 1 1 0 0 6 6 0.999
0.169×1014 0.924×101 0.723×106
0.679 0.770×107 0.568×106
0.306 0.385×107 0.201×1026
0.104×101 0.316 0.184×1026
0.395×102 0.177 0.161×1019
0.884×103 0.595×101 0.148×1019
0.998×108 0.125×101 0.119×1019
0.155×108 0.586×106 0.533
0.102×108 0.576×106 0.472×106
0.919×1014 0.446 0.441×106
0.204×108 0.406×106 0.467
0.553 0.432×1014 0.327×106
0.427 0.153×1013 0.110×1026
0.117×101 0.554 0.101×1026
0.775×102 0.567×106 0.100×1019
0.156×107 0.565×106 0.920×1020
0.303×108 0.124×106 0.162×1020
0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0
Probability Probability Probability
Anobstacleat p causesrobotR toswitchfromgoal togoal ,promptingrobotsR andR to
23 1 2 1 2 3
modifytheirgoalsandstrategies.
1Strategies Distribution( 1) 3Strategies Distribution( 3)
0.388×1013
0.142×1012
0.134×1012
0.121×1012
0.853×106 0.999
0.162×106
0.139×1012
0.128×1012
0.105×1012
0.164×106
0.142×1012
0.133×1012
0.121×1012
0.850×106 0.999
0.342×1013 0.138×1012 0.128×1012 0.105×1012
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Probability Probability
3R-1R
2R-1R
3R-2R
1R-2R
2R-3R
1R-3R
2Strategies Distribution( 2)
0.132 0.669
0.130 0.331
0.119×105 0.207×105
0.240 0.165×104
0.117 0.983×105 0.705×106 0.431×105
0.504×105 0.100×106
0.252 0.498×107
0.128 0.473×1012
0.119×105 0.206×1012
0.127 0.170
0.125 0.167
0.107 0.165
0.768×101 0.170
10 0 .S . 4 5 t4r 4 a7 8 t× × e1g 1 0i 0 es1 1 Distribution( 1) 2Stra0 0t. .e1 1g6 6i3 6es Distribution( 2) 3Strategies Distribution( 3)
0 0 0 0 . . . . 1 1 5 1 0 6 1 2 3 0 . 9 7 3 9 0 6 . × × × × 1 . 8 0 1 0 9 1 1 1 1. 2 1 7 × 0 0 0 0 3 6 1 6 1 9 9 8 0 4 1 0 0 0 0 . . . . 2 2 2 2 0 0 3 4 5 5 0 . . 6 6 7 9 1 6 .9 1 0 × × × × 2 7 6 1 1 1 1 0 4 × × 0 0 0 0 . × 2 1 1 2 1 0 0 7 7 7 7 0 0 1 1 1 0 0 0 . . . 7 8 8 2 0 57 3 2 × × × 1 1 1 0 0 0 6 6 6 0.999
0.306 0.679 0.0 0 0 . .7 307 8.025 × × 1 1 0 00. 7 74 0.6 0.8 1.0 0 0 . . 2 5 0 6 1 8 × × 1 1 0 0 2 6 6
0.104×101 Pro0b.3a1b6ility 0.184×1026
0.395×102 0.177 0.161×1019
0.884×103 0.595×101 0.148×1019
Figure10:Afterintrod
0
0 u
.
.
1
9 c
5
9
5
8 in
×
× g
1
1
0
0 o
8
8 bstaclesintotheplan
0
0
.
n .
5
1 e
8
2
6
d 5
×
× p
1
1
0
a 0 t
6
h 1 ofrobotR 1,thedis 0 t .1 r 1 ib 9× u 1 t 0 io 1 n 9 ofp
0
l
.
a
53
n
3
ningstrategiesforrobots
R 1,R 2,andR 3.
0
0
.
.
9
1
1
0
9
2
×
×
1
1
0
0
1
8
4
0.576×106
0.446 0
0
.
.
4
4
4
7
1
2
×
×
1
1
0
0
6
6
0.204×108 0.406×106 0.467
0.553 0.432×1014 0.327×106
WhenrobotR movesto0.4p27 ,anewob0s.1t5a3×c1l0e1a3tp blocksrob0.o11t0×R10,26whiletheoriginalobstacle
10.117×101 14 301.554 0.101×10326
remains. Thepref 0 0 0e . . . 3 1 7r 0 5 7e 3 6 5 × × ×n 1 1 1c 0 0 0e 8 7 2 matrixforrobo 0 0 0 . . .t 1 5 5 2 6 6R 4 5 7 × × × 31 1 1 0 0 0u 6 6 6 pdatestoC1 r3 = 0 0 0C . . . 1 9 1 2 6 01 r 0 2 0 3 × × × 1 1 1+ 0 0 0 2 2 1 l0 0 9 r 1 3 . Theupdatedstrategies
are {p ,p ,p ,p ,p ,p ,p } for robot R , {p ,p ,p ,p ,p ,p ,p } for robot R ,
14 15 01.60 01.27 0.244 0.625 0.8261.00.0 0.2 0.410.6 106.8 11.700.0180.2180.4250.6 250.8 312.0 2
and{p ,p ,p ,p ,Pproba,bpility,p }forrobotPRroba.bility Probability
38 39 32 25 18 17 24 3
0.0 0.2 0.4 0.6 0.8 1.0
Probability
2R-3R
1R-3R
3Strategies Distribution( 3)
0.329×1013
0.137×1012
0.130×1012
0.119×1012
0.972×1013 0.186×1013
0.160×1019
0.148×1019
0.879×106 0.999
0.159×106
0.142×1012
0.134×1012
0.144×106
0.105×1012 0.465×1020
0.165×1019
0.154×1019
0.102×105 0.999
0.0 0.2 0.4 0.6 0.8 1.0
Probability
3R-1R
2R-1R
1Strategies Distribution( 1)
0.153×106
0.720×107
0.123×106
0.812×107
0.575 0.425
0.588×107
0.305×107
0.865×108
0.240×1012
0.103×101
0.661×107
0.602×102
0.447×102
0.527 0.435
0.745×102
0.557×102
0.338×102
0.128×107
0.0 0.2 0.4 0.6 0.8 1.0
Probability
3R-2R
1R-2R
2Strategies Distribution( 2)
0.509
0.354
0.813×101
0.565×101
0.116×106 0.568×105
0.377×105
0.223×105
0.113×105
0.718×106
0.448
0.405×106
0.165×1013
0.597×1013
0.577×1013 0.552
0.568×106
0.555×106
0.108×106
0.404×106
Figure11:AfterintroducingobstaclesintotheplannedpathofrobotR 3,thedistributionofplanningstrategiesforrobots
R 1,R 2,andR 3.
Duringtaskexecution,weintroducetemporarywaypointsforrobotsR andR (seeTableI).
1 3
These waypoints take precedence over πi . When robot R moves to p , robots R and R
goal 1 17 2 3
will move to p and p , respectively. Robots R and R add waypoints p and p , with
25 39 1 2 4 35
l1 (p ) = 2andl1 (p ) = 2. ThepreferencematricesareupdatedasC1 = C1 +l1 andC1 =
r1 4 r3 35 r1 r1 r1 r3
C1 +l1 . Tominimizefreeenergyandreachtheirgoalssimultaneously,robotR selectsthepath
r3 r3 1
{p ,p ,p ,p ,p ,p ,p ,p ,p },robotR chooses{p ,p ,p ,p ,p ,p ,p ,p ,p },
17 10 3 4 4 3 10 17 24 2 18 25 25 25 25 25 25 25 32
androbotR adopts{p ,p ,p ,p ,p ,p ,p ,p ,p }.
3 25 32 39 39 32 25 25 25 26
DistributedBTsallowinteractiveBTnodestocooperatewithtraditionalBTnodes,ensuring
robot coordination and autonomous decision-making. Each robot has a Battery Enough? con-
ditionnodeandaMoveToChargeractionnode,enablingthemtoautonomouslydetachfromthe
swarm and reach a charging station when battery levels are low. All strategies must meet the
20
0.0 0.2 0.4 0.6 0.8 1.0
Probability
3R-1R
2R-1R
1Strategies Distribution( 1)
0.0 0.2 0.4 0.6 0.8 1.0
Probability
3R-2R
1R-2R
2Strategies Distribution( 2)
0.0 0.2 0.4 0.6 0.8 1.0
Probability
2R-3R
1R-3R
3Strategies Distribution( 3)
0.141×1068 0.462×106 0.261×1082
0.133×1062 0.454×106 0.255×1082
0.322 0.548×106 0.261×1082
0.101×1055 0.497 0.250×1082
0.635×1050 0.459×106 0.261×1082
0.323 0.438×106 0.249×1082
0.139×1068 0.245 0.582×107
0.889×1063 0.468×106 0.501
0.355 0.444×106 0.499
0.922×1063 0.258 0.111×1082
0.119×1055 0.368×106 0.267×1082
0.101×1042 0.539 0.267×1082
0.355 0.351×106 0.235×1082
0.838×1049 0.345×106 0.239×1082
0.715×1036 0.461 0.268×1082 0.313 0.311×106 0.260×1082
0.131×1055 0.299×106 0.579×107
0.115×1042 0.263×1020 0.479
0.332 0.690×1020 0.521
0.394×108 0.637×1020 0.110×1082
Figure12: AfteraddingwaypointsforrobotsR 1andR 3,thedistributionofplanningstrategiesforrobotsR 1,R 2,and
R 3.
Battery Enough precondition. When a robot runs the interactive node MoveToGoal, it returns
a running status each cycle. If the battery is low, the MoveToGoal node returns a failure sta-
tus, causing disconnection. When the Battery Enough? node shows a failure status, the robot
activatestheMoveToCharger node. Forinstance, robotR movesautonomouslyfrom position
1
p to Charging Station 1 (p ), disconnecting from robots R and R . Robots R and R wait
4 13 2 3 2 3
for R to reconnect after recharging. Once charged, R returns to its position and resumes its
1 1
strategy. If robot R detects a low battery at p , it goes to Charging Station 3 (p ) while R
3 25 41 1
and R wait. Upon reunion, all three reach their destination together, adjusting their strategies
2
toπ{1,2,3} = π . Thisexperimentshowsthatrobotscaneffectivelyusemulti-robotinteractive
stop
nodes with traditional BT nodes, maintaining coordinated movement and autonomous disen-
gagementduringgrouptasks.
Table3:QUANTITATIVECOMPARISONOFBEHAVIORTREECOMPLEXITY
Traditional IIBT-Node Absolute Relative
Component BT Approach Reduction Reduction
SequenceNodes 5 1 4 80%
FallbackNodes 4 1 3 75%
ConditionNodes 7 1 6 85%
ActionNodes 5 2 3 60%
TotalNodes 21 5 16 76.2%
TreeDepth 8 2 6 75%
DesignComplexity High Low - -
AsillustratedinFig.8,theproposedIIBTarchitecturereplaceslargeportionsoftraditionalBT
control logic (shown in blue) with compact interactive inference nodes. These nodes integrate
actionselection,preconditionchecking,andinter-robotcoordinationwithinasingleprobabilis-
tic framework, significantly simplifying the tree topology. While a conventional BT relies on
multiple layers of Sequence, Fallback, and Condition nodes to handle task transitions and re-
coverybehaviors,theIIBTnodeinternallyperformsthesefunctionsthroughbeliefupdatesand
free-energyminimization.
Toquantitativelyevaluatethisstructuralcompression,Table3comparesthenodecomposition
andtreedepthofthetraditionalBTagainsttheproposedIIBTapproach. Theresultsconfirmthat
embedding inference capabilities into BT nodes yields a 76.2% reduction in node count and a
75%decreaseintreedepth,effectivelytransformingadeep,rule-basedhierarchyintoacompact,
21
adaptivedecisionstructure.
6.2. MultirobotCooperativeObjectPlacementReal-WorldExperiment
Building upon the navigation experiments presented in the previous subsection, this study
extends the proposed Interactive Inference Behavior Tree (IIBT) framework from multi-robot
motioncoordinationtoareal-worldcollaborativeobjectplacementtask. Thegoalofthisexper-
iment is to quantitatively evaluate the generality and robustness of the IIBT-Node architecture
underphysicalconditionswithsensorynoiseandactuationuncertainty. Specifically,theexper-
iment aims to verify (1) the practical deployability of the BT-based control framework on real
robotplatforms,and(2)therobustnessoftheinteractiveinferencemechanismwhenfacedwith
perceptualandcontroldisturbances.
Withinthissetup, therobotagentsarecapableofsharingreal-timetaskstatesandexecuting
coordinatedmovements. Theactionspaceforeachrobotisdefinedas
U ={moveTo(goal), pick(obj), place(obj), idle}.
Toencompassthefullrangeofpotentialcooperativebehaviors,ajointstrategysetΠ{1,2} iscon-
structedas
Π{1,2} ={π ,π ,...,π },
0 1 15
whereeachjointpolicyπ isatupleπ =(a ,a ),witha ∈U. Accordingly,thecompleteset
i i R 1 R 2 R j
ofjointpolicycombinationsisenumeratedas:
π0 =(moveTo(goal),moveTo(goal)), π1 =(moveTo(goal),pick(obj)),
π2 =(moveTo(goal),place(obj)), π3 =(moveTo(goal),idle),
π4 =(pick(obj),moveTo(goal)), π5 =(pick(obj),pick(obj)),
π6 =(pick(obj),place(obj)), π7 =(pick(obj),idle),
π8 =(place(obj),moveTo(goal)), π9 =(place(obj),pick(obj)),
π10 =(place(obj),place(obj)), π11 =(place(obj),idle),
π12 =(idle,moveTo(goal)), π13 =(idle,pick(obj)),
π14 =(idle,place(obj)), π15 =(idle,idle).
Thisjointstrategyspacerepresentsallpossiblecombinationsofcooperativeactionsbetween
robots R and R , forming the foundation for subsequent inference-based policy selection.
1 2
Within the proposed framework, the update of each robot’s task preference matrix Ci is mod-
ulatedbyitscorrespondinglogicalvariableset
Li ={li ,li ,li ,li }.
loc hold place free
ThelogicalvariablesLiserveassymbolicrepresentationsofdiscretetaskstatesandactaslogical
priorsthatshapethepreferenceupdateinCiaccordingtotherobot’sperceivedprogressandtask
requirements.Thedefinitionsofthehiddenstatevectorsiandthecorrespondinglogicalvariables
LiusedinthisexperimentarelistedinTable4.
Table4:DefinitionsofRobotHiddenStatesandLogicalVariables(li∈Li)
·
HiddenState LogicalVariable SemanticDescription
si li Posteriorbeliefofreachinggoallocation
loc loc
si li Posteriorbeliefofgraspingtheobject
hold hold
si li Posteriorbeliefofobjectplacement
place place
si li Posteriorbeliefofbeingidleortask-free
free free
22
Thepreconditionsandpostconditionsgoverningtheexecutionofeachroboticactionarede-
fined in Table 5. Each action corresponds to a logical transition that determines whether the
associatedconditioncanbeexecutedwithintheIIBT-Node. Uponactioncompletion,thepost-
conditionsupdatethelogicalvariablesetLi andindirectlymodifythetaskpreferencematrixCi
throughadditiveadjustmentstotherelevantentries.
Table5:Actionspecificationswithpreconditionsandpostconditions
Action Preconditions Postconditions
moveTo(loc) !IsReached(loc) li =max(Ci)+1
loc
IsReached(loc)
pick(obj) li =max(Ci)+1
!IsHolding hold
IsReached(loc)
place(obj) IsHolding(obj) li =max(Ci)+1
place
!IsPlaced(obj,loc)
As shown, each postcondition reflects the logical progression of the task: for instance, exe-
cuting moveTo(loc) increases the preference for the IsReached(loc) state (li ), while pick(obj)
loc
andplace(obj)incrementthecorresponding si and si beliefs. Theidleaction,incontrast,
hold place
maintainsthesi belief,indicatingnoactivetaskengagement.
free
ToenableprobabilisticreasoningwithintheIIBT-Node,thissectionformalizestheprobabilis-
ticmatricesthatconstitutethecoreoftheinferenceprocess. AlikelihoodmatrixAi isdefined
foreachrobotR,modelingtheconditionalrelationshipbetweenthehiddenstate si andtheob-
i
servationmatrixOi asP(Oi | si). Thismatrixcaptureshowsensoryevidenceupdatestherobot’s
τ τ
beliefstateunderperceptualuncertainty.
 
0.9 0.025 0.025 0.025
Ai =

0
0
.
.
0
0
2
2
5
5 0
0
.0
.9
25
0.
0
0
.9
25 0
0
.
.0
02
2
5
5

. (8)
0.025 0.025 0.025 0.9
EachrowofAi correspondstoaspecifichiddenstate si andrepresentstheconditionalprob-
k
ability distribution P(Oi | si) over possible observations. For example, the diagonal entry of
τ k
0.9inthefirstrowindicatesa90%probabilityofcorrectlyperceivingtheintendedfeature(e.g.,
targetlocation)whenthesystemisinthatstate. Thelowoff-diagonalprobabilities(0.025)rep-
resent potential sensor noise or ambiguous observations. The observation Oi is thus generated
τ
accordingtothismapping,providingthebasisforBayesianbeliefupdatesduringinference.
Subsequently, asetofstatetransitionmatricesBi isdefinedtocharacterizetheprobabilistic
π
dynamics of si under the execution of each action policy π. Each element Bi(s′|s) denotes the
π
probabilityoftransitioningfromstate sto s′ givenπ. TogetherwithAi,thesetransitionmodels
completetheprobabilisticgenerativestructureoftheIIBT-Node.
Having defined the probabilistic model and action semantics of the IIBT-Node, this section
introduces the specific joint task designed for real-world multi-robot evaluation. As illustrated
in Fig. 13, the experimental environment comprises three key locations: Goal A, Goal B, and
theRendezvousPointC.Thissetupextendsthepreviousnavigationexperimentintoacoopera-
tivemanipulationdomain,enablingevaluationoftheproposedframeworkundermorecomplex
physicalinteractionsandgoaldependencies.
23
Two quadruped robots, R and R , are deployed to execute a collaborative object placement
1 2
mission. Each robot is equipped with a front-mounted manipulator and onboard cameras for
local perception, allowing them to share task-relevant state information in real time. Robot R
1
is assigned to navigate toward Goal A, grasp a bottle, and transport it to Rendezvous Point C.
Robot R is tasked with navigating to Goal B, picking up a plate, and likewise delivering it to
2
the same rendezvous point. The shared objective is for R to precisely place the bottle onto
1
theplateheldbyR atLocationC,therebycompletingthecooperativemanipulationtask. This
2
configurationallowsbothagentstoinferandupdatetheiractionpoliciesfromthejointpolicyset
Π{1,2} ={π ,π ,...,π }basedontheirjointobservationsO{1,2}andpreferencematricesC{1,2}.
0 1 15 τ
Bottle
Plate
Robot 1
A
Robot 2
B
C
Figure13:Experimentalscenarioillustratingcollaborativeobjectgraspingandplacementbytwoquadrupedrobots.
In this cooperative manipulation task, temporal dependencies between actions emerge natu-
rally. Acriticalinter-robotdependencyexists:thesuccessfulexecutionoftheplaceactionbyR
1
iscontingentuponR havingalreadypositionedtheplateatLocationC.IfR reachesLocation
2 1
CbeforeR completesitsplacementaction,itmustenterawaitingstate(s1 )untilitdetectsthat
2 free
theplateisinplace. Thisdependencyrequireseachrobottoreasonnotonlyaboutitsownlatent
state si andselectedactionai,butalsotoinferandadapttotheevolvingstrategyofitspartner
τ τ
inrealtime,therebyencapsulatingthecorechallengeofmulti-agentinteractiveinference.
To evaluate this mechanism, a baseline was implemented using a traditional BT design ap-
proach. Fig. 14 presents the executable BT structures for R and R , both incorporating the
1 2
proposed interactive inference nodes (IIBT-nodes). For robot R , the IIBT-nodes include IsH-
1
olding(Bottle)andIsPlaced(Bottle,Plate),complementedbyconventionalBTnodessuchasthe
conditionIsReached(GoalA)andtheactionmoveTo(GoalC).ForrobotR ,theIIBT-nodesin-
2
cludeIsHolding(Plate)andIsMovingWith(Bottle,Plate),togetherwithstandardnodessuchasthe
conditionIsReached(GoalC)andtheactionmoveTo(GoalC).
Fig.15furtherillustratestheBTsthatarefunctionallyequivalenttotheaforementionedIIBT-
nodes, demonstrating the structural expansion required to achieve the same logical expressive-
24
IsHolding(Bottle) ? IsPlacedAt(Bottle,plate) IsHolding(Plate) MoveTo(Goal C)
IsReached(Goal C) MPool Cviceo yTn (os O(tGr b u sot caa tl c Cle))
Figure14:Interactiveinferencebehaviortreesfor(left)robotR 1and(right)robotR 2.
nessinaconventionalBTframework. Specifically,theequivalentBTfortheIsHolding(Bottle)
node of R is shown in Fig. 15(a), while the one corresponding to IsPlaced(Bottle,Plate) is
1
depicted in Fig. 15(b). Similarly, for R , the equivalent BTs for the IsHolding(Plate) and Is-
2
MovingWith(Bottle,Plate)nodesarepresentedinFig.16(a)andFig.16(b),respectively. Dueto
distincttaskobjectives,thearchitecturesofthetworobots’BTsdiffer,highlightingthemodular
adaptabilityoftheproposedIIBTframework.
Toquantifytheefficacyofourapproach,Table6providesanodecountcomparisonbetween
the interactive inference BT (Fig. 14(a)) and its functionally equivalent traditional counterpart
(Fig.15(a)). Theconventionalimplementationnecessitates4Sequencenodes,9Fallbacknodes,
9 Condition nodes, and 11 Action nodes, totaling 33 nodes. In contrast, our approach reduces
theserequirementsto1Sequencenode,1Fallbacknode,1Conditionnode,and3Actionnodes,
totaling merely 6 nodes—achieving a compression rate of 81.8%. Furthermore, the structural
depthiscompressedfrom7layersto3layers,substantiallyalleviatingdesigncomplexity.
Similarly, Table 7 presents comparisons for robot R ’s complete decision-making control
1
model. The traditional BT requires 10 total nodes, compressed to just 6 nodes using our
method—a70%reduction—withdepthreducedfrom5layersto2layers.
? IsHolding(Bottle) IsPlacedAt(Bottle,plate) ?
IsHolding(Bottle) IsPlacedAt(Bottle,plate)
IsReached(R2,Goal B) ? Pick(Bottle) ( R Is 2 R ,G ea o c a h l e C d ) I ( s R H 2 o ,P ld la in te g ) ? Place(Bottle,Plate)
IsReached(Bottle) MoveTo(Goal C) IsHolding(Bottle) Pick(Bottle)
（a） （b）
Figure15:StructuralcompressioncomparisonofBTnodesforRobotR 1.
? IsHolding(Plate)
IsHolding(Plate)
IsReached(R1,Goal A) ? Pick(Plate)
IsReached(Goal B) MoveTo(Goal B)
Figure16:StructuralcompressioncomparisonofBTnodesforRobotR 2.
At the initial time step (τ = 0), both robots R and R are in the idle state. Their joint
1 2
observationmatrixO{1,2},composedofindividualobservationvectorsO1 andO2 andatask
τ=0 τ=0 τ=0
resultobservationOresult,isdefinedas:
τ=0
25
Table6:QUANTITATIVECOMPARISONOFR 1BEHAVIORTREECOMPLEXITY
Traditional Ours Absolute Relative
Component BT BT Reduction Reduction
SequenceNodes 4 1 3 75%
FallbackNodes 9 1 8 88.9%
ConditionNodes 9 1 8 88.9%
ActionNodes 11 3 8 73%
TotalNodes 33 6 27 81.8%
TreeDepth 7 3 4 57.1%
DesignComplexity High Low - -
Table7:QUANTITATIVECOMPARISONOFR 2BEHAVIORTREECOMPLEXITY
Traditional Ours Absolute Relative
Component BT BT Reduction Reduction
SequenceNodes 2 1 1 50%
FallbackNodes 2 0 2 100%
ConditionNodes 3 0 3 100%
ActionNodes 3 2 1 33.3%
TotalNodes 10 3 7 70%
TreeDepth 5 2 3 60%
DesignComplexity High Low - -
 
O{
τ
1
=
,2
0
} =
 0
0
0
0
0
0
0
0
1 −
1
1

, (9)
whereeachrowcorrespondstoonerobot’sobservationvectorOi = [oi ,oi ,oi ,oi ].
τ=0 loc hold place free
Avalueof“1”inthelastcolumnindicatesthatbothrobotsarecurrentlyinthe si state. This
free
observationservesastheinitialconditionforbeliefinferencewithintheIIBT-node,fromwhich
eachagentbeginsreasoningaboutitssubsequentactionsbasedonC{1,2}andΠ{1,2}.
At this initial stage, the value “1” in O{1,2} indicates that both robots are in the si state,
τ=0 free
signifying that the task execution has not yet begun. Under this condition, the IIBT-Node of
R directs its reasoning focus toward the interactive node IsHolding(Bottle), while R remains
1 2
inactive to prevent potential conflicts arising from parallel policy execution. To formalize this
coordinationlogic,ajointextrinsicpreferencematrixC{1,2}isdefinedasfollows:
 
C1 =
 0
0
1
1
0
0
0
0
0 −
0
1

. (10)
Each row in C1 corresponds to one robot’s preference distribution over the logical state set
{l ,l ,l ,l }. The first row represents R ’s extrinsic preference, encouraging transition
loc hold place free 1
towards1 ,whilethesecondrowexpressesR ’spreferencetoremain“free.”Duringinference,
hold 2
R will thus select the pick(obj) action only when o2 = 1, thereby establishing a logically
1 free
consistentdivisionoflaborbetweenthetworobots.
26
However,thecurrentobservationalstateO{1,2} (cid:44) {li ,li }(Algorithm1,line17),indicating
τ loc free
that the precondition li for the pick(obj) action is not yet satisfied. Consequently, Ci must be
loc
adaptivelyadjustedthroughlogicalevidenceLitoalignthedesiredgoalstatewithenvironmental
constraints. Thisadaptiveadjustmentmechanismformsthefoundationofinteractiveinference
withintheIIBTframework.
Tosatisfytheunsatisfiedpreconditionli forpick(obj),theoriginalCiisaugmentedbyasup-
loc
plementarylogicalconstraintmatrixLi,introducingthenecessaryconditionforspatialreacha-
bility:
 
Li =
 2
0
0
0
0
0
0
0
0 −
0
0

. (11)
The entry “2” in the first row explicitly encodes a higher-priority constraint on the
moveTo(goal) action. This adjustment ensures that, given the current observation, the agent
prioritizes navigating to the goal location before attempting to grasp the object. Consequently,
theupdatedcomprehensivepreferencematrixbecomes:
 
Ci =Ci+Li =
 2
0
1
1
0
0
0
0
0 −
0
1

.
This update demonstrates that Ci evolves dynamically during task execution, incorporating
contextual information and logical constraints derived from Oi and Li. Following Expected
τ
Free Energy (EFE) inference over Π{1,2} = {π ,π ,...,π }, the IIBT-node determines the MAP
0 1 15
jointactionasπ = (moveTo(goal),idle), withP(π ) = 0.255(Fig.17). ThisindicatesthatR
3 3 1
navigateswhileR remainsidle,achievingconflict-freecoordination.
2
Figure17:ProbabilitydistributionoverjointstrategiesafterEFEinference.
UponcompletionofmoveTo(goal),thetemporarylogicalconstraintLiisremoved:
 
Ci =Ci−Li =
 0
0
1
1
0
0
0
0
0 −
0
1

.
27
EFEinferenceisthenrepeated,yieldingπ = (pick(obj),idle)withP(π ) = 0.205(Fig.18).
7 7
ThisdrivesR toexecutepick(obj),demonstratingdynamicre-evaluationandadaptivecoopera-
1
tion.
Figure18:UpdatedprobabilitydistributionoverjointstrategiesaftercompletionofthemoveTo(goal)action.
Next, robotR enterstheinteractivenodeIsHolding(Plate). Itconstructsitslocalconstraint
2
matrixL2,encodingbothintrinsicpreconditionsandinter-agentrequirements:
 
L2 =
 0
0
0
0
2
0
0
0
0 −
0
0

. (12)
The first row imposes the dependency that R must be free before R executes pick(plate),
1 2
while the second encodes R ’s intrinsic preference for holding. EFE inference over Π{1,2} pro-
2
ducesdistributionsshowninFig.19andFig.20.
Figure19:ProbabilitydistributionoverjointstrategiesforR 2aftercompletingmoveTo(goal).
The selected joint strategies are π = (idle,moveTo(GoalB)) and π = (idle,pick(plate)),
12 13
with P(π ) = 0.255 and P(π ) = 0.205. Afterward, both robots execute moveTo(Goal C).
12 13
28
Figure20:UpdatedprobabilitydistributionoverjointstrategiesforR 2afterpick(plate).
Uponarrival,R activatesIsPlaced(Bottle,Plate),triggeringanotherinferenceiterationtoevalu-
1
ateplacementreadiness(Fig.21).
Figure21:Finalprobabilitydistributionoverjointstrategiesduringthecollaborativeobjectplacementphase.
ThefinalpreferencematrixC2isupdatedas:
 
C2 =
 0
0
1
1
1
0
0
0
0 −
0
0

.
Here, the first row denotes R ’s elevated preference for place(obj), while the second main-
1
tains R ’s preference for s2 , ensuring stability during placement. Through iterative inference
2 free
andpreferenceadjustment, theIIBT-Nodeeffectivelycoordinatesinter-robotdependenciesand
convergestowardsuccessfultaskcompletion.
29
7. Conclusion
This work presented the Interactive Inference Behavior Tree (IIBT-Node), a unified control
nodethatintegratesactiveinferencewiththemodulararchitectureofBehaviorTrees(BTs)for
decentralizedmulti-robotcooperation. Byembeddingadynamicpreferencematrixwithineach
node,theproposedframeworkenablesrobotstoinfer,adapt,andcoordinatetheiractionsunder
uncertainty while preserving the interpretability and modularity of BTs. Extensive validation
was conducted through both simulation and physical experiments using quadruped robots. In
thesimulatedmulti-robotnavigationtasks,theIIBT-NodereducedtheBTstructuralcomplexity
by76.2%, whileinreal-worldcollaborativemanipulationexperiments, anequivalentreduction
ranging from 70% to 81.8% was achieved. These results confirm that the proposed approach
generalizes effectively across different action spaces, maintaining consistent reasoning perfor-
mance and robust coordination under partial observability. Overall, the IIBT-Node provides a
scalable and interpretable mechanism for multi-robot systems to achieve autonomous coopera-
tion and conflict-free decision-making. Future work will focus on extending the framework to
heterogeneousrobotteamsandexploringreal-timelearningofpreferencematricesinlarge-scale
environments.
References
[1] P.Lanillos,C.Meo,C.Pezzato,A.A.Meera,M.Baioumy,W.Ohata,A.Tschantz,B.Mil-
lidge, M. Wisse, C. L. Buckley et al., “Active inference in robotics and artificial agents:
Surveyandchallenges,”arXivpreprintarXiv:2112.01871,2021.
[2] M. Priorelli, I. P. Stoianov, and G. Pezzulo, “Embodied decisions as active inference,”
bioRxiv,pp.2024–05,2024.
[3] G. Pezzulo, T. Parr, and K. Friston, “Active inference as a theory of sentient behavior,”
BiologicalPsychology,p.108741,2024.
[4] K. J. Friston, T. Parr, C. Heins, A. Constant, D. Friedman, T. Isomura, C. Fields, T. Ver-
belen,M.Ramstead,J.Clippingeretal.,“Federatedinferenceandbeliefsharing,”Neuro-
science&BiobehavioralReviews,p.105500,2023.
[5] D.Maisto,F.Donnarumma,andG.Pezzulo,“Interactiveinference:amulti-agentmodelof
cooperativejointactions,”IEEETransactionsonSystems,Man,andCybernetics: Systems,
2023.
[6] N. Wirkuttis and J. Tani, “Leading or following? dyadic robot imitative interaction using
theactiveinferenceframework,”IEEERoboticsandAutomationLetters,vol.6,no.3,pp.
6024–6031,2021.
[7] D. A. Friedman, A. Tschantz, M. J. Ramstead, K. Friston, and A. Constant, “Active in-
ferants: An active inference framework for ant colony behavior,” Frontiers in behavioral
neuroscience,vol.15,p.647732,2021.
[8] A.ClodicandR.Alami,“Whatisittoimplementahuman-robotjointaction?” Robotics,
AI,andhumanity: Science,ethics,andpolicy,pp.229–238,2021.
30
[9] M.Ghallab,D.Nau,andP.Traverso,“Theactor’sviewofautomatedplanningandacting:
Apositionpaper,”ArtificialIntelligence,vol.208,pp.1–17,2014.
[10] M.Nixon,R.B.Havekost,L.O.Jundt,M.G.Ott,A.Webb,D.Stevenson,M.Lucas,and
K.J.Beoughter,“Processcontrolsystemusingacontrolstrategyimplementedinalayered
hierarchyofcontrolmodules,”Jan.191999,uSPatent5,862,052.
[11] S. S. O. Venkata, R. Parasuraman, and R. Pidaparti, “Kt-bt: A framework for knowledge
transfer through behavior trees in multirobot systems,” IEEE Transactions on Robotics,
2023.
[12] R. Hull, D. Moratuwage, E. Scheide, R. Fitch, and G. Best, “Communicating intent as
behaviour trees for decentralised multi-robot coordination,” in 2024 IEEE International
ConferenceonRoboticsandAutomation(ICRA). IEEE,2024,pp.7215–7221.
[13] S.Gugliermo,E.Schaffernicht,C.Koniaris,andF.Pecora,“Learningbehaviortreesfrom
planningexpertsusingdecisiontreeandlogicfactorization,”IEEERoboticsandAutoma-
tionLetters,vol.8,no.6,pp.3534–3541,2023.
[14] M.Colledanchise,D.Almeida,andP.Ögren,“Towardsblendedreactiveplanningandact-
ing using behavior trees,” in 2019 international conference on robotics and automation
(ICRA). IEEE,2019,pp.8839–8845.
[15] Y. Luo, H. Bai, D. Hsu, and W. S. Lee, “Importance sampling for online planning under
uncertainty,”TheInternationalJournalofRoboticsResearch,vol.38,no.2-3,pp.162–181,
2019.
[16] J.-H. Kim and P. Vadakkepat, “Multi-agent systems: a survey from the robot-soccer per-
spective,”IntelligentAutomation&SoftComputing,vol.6,no.1,pp.3–17,2000.
[17] J. Li, C. Hua, H. Ma, J. Park, V. Dax, and M. J. Kochenderfer, “Multi-agent dynamic
relationalreasoningforsocialrobotnavigation,”arXivpreprintarXiv:2401.12275,2024.
[18] C.Pezzato,C.H.Corbato,S.Bonhof,andM.Wisse,“Activeinferenceandbehaviortrees
for reactive action planning and execution in robotics,” IEEE Transactions on Robotics,
vol.39,no.2,pp.1050–1069,2023.
[19] K. J. Friston, T. Parr, C. Heins, A. Constant, D. Friedman, T. Isomura, C. Fields, T. Ver-
belen,M.Ramstead,J.Clippingeretal.,“Federatedinferenceandbeliefsharing,”Neuro-
science&BiobehavioralReviews,vol.156,p.105500,2024.
[20] S.Wakayama,A.Candela,P.Hayne,andN.Ahmed,“Activeinferenceincontextualmulti-
armed bandits for autonomous robotic exploration,” arXiv preprint arXiv:2408.04119,
2024.
[21] L. Bramblett, J. Reasoner, and N. Bezzo, “Implicit coordination using active epistemic
inference,”arXive-prints,pp.arXiv–2501,2025.
[22] M. Colledanchise and L. Natale, “On the implementation of behavior trees in robotics,”
IEEERoboticsandAutomationLetters,vol.6,no.3,pp.5929–5936,2021.
31
[23] X.Li,Y.Li,J.Zhang,X.Xu,andD.Liu,“Embeddingmulti-agentreinforcementlearning
intobehaviortreeswithunexpectedinterruptions,”Complex&IntelligentSystems,vol.10,
no.3,pp.3273–3282,2024.
[24] R. Liu, G. Wan, M. Jiang, H. Chen, and P. Zeng, “Autonomous robot task execution in
flexible manufacturing: Integrating pddl and behavior trees in ariac 2023,” Biomimetics,
vol.9,no.10,p.612,2024.
[25] E.Scheide,G.Best,andG.A.Hollinger,“Synthesizingcompactbehaviortreesforproba-
bilisticroboticsdomains,”AutonomousRobots,vol.49,no.1,p.3,2025.
[26] S.Gugliermo,D.C.Dominguez,M.Iannotta,T.Stoyanov,andE.Schaffernicht,“Evaluat-
ingbehaviortrees,”RoboticsandAutonomousSystems,vol.178,p.104714,2024.
[27] D.M.Blei,A.Kucukelbir,andJ.D.McAuliffe,“Variationalinference:Areviewforstatis-
ticians,” Journal of the American statistical Association, vol. 112, no. 518, pp. 859–877,
2017.
[28] M.I.Jordan,Z.Ghahramani,T.S.Jaakkola,andL.K.Saul,“Anintroductiontovariational
methodsforgraphicalmodels,”Machinelearning,vol.37,no.2,pp.183–233,1999.
[29] D.P.KingmaandM.Welling,“Auto-encodingvariationalbayes,”inInternationalConfer-
enceonLearningRepresentations(ICLR),2014.
[30] K. Friston, “The free-energy principle: a unified brain theory?” Nature reviews neuro-
science,vol.11,no.2,pp.127–138,2010.
[31] K.Friston,T.FitzGerald,F.Rigoli,P.Schwartenbeck,G.Pezzuloetal.,“Activeinference
andlearning,”Neuroscience&BiobehavioralReviews,vol.68,pp.862–879,2016.
[32] T.ParrandK.J.Friston,“Generalisedfreeenergyandactiveinference,”Biologicalcyber-
netics,vol.113,no.5,pp.495–513,2019.
[33] G.Pezzulo,F.Rigoli,andK.Friston,“Activeinference,homeostaticregulationandadap-
tivebehaviouralcontrol,”Progressinneurobiology,vol.134,pp.17–35,2015.
[34] P.Schwartenbeck,J.Passecker,T.U.Hauser,T.H.FitzGerald,M.Kronbichler,andK.J.
Friston, “Computational mechanisms of curiosity and goal-directed exploration,” elife,
vol.8,p.e41703,2019.
32

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Bridging Probabilistic Inference and Behavior Trees: An Interactive Framework for Adaptive Multi-Robot Cooperation"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
