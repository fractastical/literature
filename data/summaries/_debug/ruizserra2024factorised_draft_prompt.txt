=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Factorised Active Inference for Strategic Multi-Agent Interactions
Citation Key: ruizserra2024factorised
Authors: Jaime Ruiz-Serra, Patrick Sweeney, Michael S. HarrÃ©

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2024

Key Terms: factorised, andmulti, agents, theuniversityofsydney, centreforcomplexsystems, australia, agentsystems, sydney, individual, inference

=== FULL PAPER TEXT ===

Factorised Active Inference for Strategic Multi-Agent Interactions
JaimeRuiz-Serra PatrickSweeney MichaelS.HarrÃ©
CentreforComplexSystems, CentreforComplexSystems, CentreforComplexSystems,
TheUniversityofSydney TheUniversityofSydney TheUniversityofSydney
Sydney,Australia Sydney,Australia Sydney,Australia
jaime.ruizserra@sydney.edu.au pswe2854@uni.sydney.edu.au michael.harre@sydney.edu.au
Abstract advancingfieldsasdiverseasneuroscience,economics,andmulti-
Understanding how individual agents make strategic decisions agentsystems.
withincollectivesisimportantforadvancingfieldsasdiverseaseco- Individual-levelpreferencesincentivisethebehaviourthatshapes
nomics,neuroscience,andmulti-agentsystems.Twocomplemen- collectiveoutcomes.Whilethesepreferencesmaynotalwayscon-
taryapproachescanbeintegratedtothisend.TheActiveInference flict,tensionsbetweencooperationandnon-cooperationleadto
framework(AIF)describeshowagentsemployagenerativemodel emergenthigher-levelstructures.Gametheorymodelsincentivised
toadapttheirbeliefsaboutandbehaviourwithintheirenvironment. socialinteractionswithpotentiallycompetingobjectives,wherea
Gametheoryformalisesstrategicinteractionsbetweenagentswith utilityfunctionmapsbehaviourtotherealnumbers.ANashequilib-
potentiallycompetingobjectives.Tobridgethegapbetweenthe riumrepresentsthepointwhereagents,independentlymaximising
two,weproposeafactorisationofthegenerativemodelwhereby theirutility,havenoincentivetochangetheirstrategy.
eachagentmaintainsexplicit,individual-levelbeliefsaboutthe Bridgingthegapbetweenidealisedgame-theoreticmodelsand
internalstatesofotheragents,andusesthemforstrategicplanning the often messy realities of agents interacting in complex envi-
inajoint context.Weapplyour model toiteratedgeneral-sum ronmentspresentsapersistentchallenge.Traditionalgametheory
gameswithtwoandthreeplayers,andstudytheensembleeffects oftenfalterswhenagentsdeviatefromperfectrationality[10].This
ofgametransitions,wheretheagentsâ€™preferences(gamepayoffs) challengebecomesparticularlysalientinthefaceofstrategicuncer-
changeovertime.Thisnon-stationarity,beyondthatcausedby
tainty,whereagentsgrapplewithuncertaintyabouttheactionsand
reciprocaladaptation,reflectsamorenaturalisticenvironmentin
intentionsofothers,andequilibriumselection,wheremultiplepo-
whichagentsneedtoadapttochangingsocialcontexts.Finally, tentialequilibriaexistwithoutclearmechanismsforconvergence.
wepresentadynamicalanalysisofkeyAIFquantities:thevaria- Shohametal.[88]drewattentiontoakeyissuewithequilibrium
tionalfreeenergy(VFE)andtheexpectedfreeenergy(EFE)from selection:â€œItseemstousthatsometimesthereisarushtoinvesti-
numericalsimulationdata.Theensemble-levelEFEallowsusto gatetheconvergenceproperties,motivatedbythewishtoanchor
characterisethebasinsofattractionofgameswithmultipleNash thecentralnotionofgametheoryinsomeprocess,attheexpense
Equilibria under different conditions, and we find that it is not ofmotivatingthatprocessrigorouslyâ€.
necessarilyminimisedattheaggregatelevel.ByintegratingAIF
TheActiveInferenceframework(AIF),aprocesstheoryrooted
andgametheory,wecangaindeeperinsightsintohowintelligent inneuroscience,canofferacompellingperspectiveonthesechal-
collectivesemerge,learn,andoptimisetheiractionsindynamic lenges.AIFprovidesanempiricallyinformedaccountofperception,
environments,bothcooperativeandnon-cooperative. action,andlearningunderuncertaintythathasrapidlymatured
inrecentyears[24],butlacksaclearframeworkformulti-agent
Keywords strategicinteractions[17,25].Gametheoryoftenassumesperfect
rationalityandcompleteinformation,whichAIFrelaxes.Employ-
freeenergyprinciple;gametheory;theoryofmind
inggametheoryasamodelofincentiviseddecision-making,and
ACMReferenceFormat: AIFasthecognitiveprocessunderlyingindividualdecisionsallows
JaimeRuiz-Serra,PatrickSweeney,andMichaelS.HarrÃ©.2025. Factorised forexperimentswithdynamicagentpreferenceswhileproviding
ActiveInferenceforStrategicMulti-AgentInteractions.InProc.ofthe24th
accesstohowtheirbeliefsandprecisionchangeinresponseto
InternationalConferenceonAutonomousAgentsandMultiagentSystems(AA-
others. Our results shed light on the mutual influence between
MAS2025),Detroit,Michigan,USA,May19â€“23,2025,IFAAMAS,10pages.
individualcognitionandstructuraldynamicsatthecollectivelevel.
WebeginbyreviewingrecentworkontheintersectionofAIF
1 Introduction andBayesianagents,gametheory,andmulti-agentsystems(Â§2).
Collectiveintelligence,theemergentabilityofgroupstosolveprob- Integratingthetwoendsofthespectrum,weproposeafactorisa-
lemsmoreeffectivelythanindividuals,isaphenomenonobserved tionofthegenerativemodelwherebyanagentmaintainsexplicit,
acrossbiological,social,andartificialsystems.Understandingthe individual-levelbeliefsabouttheinternalstatesofotheragentsand
mechanisms that drive this collective behaviour is essential for usesthemforstrategicplanninginajointcontext(Â§3).
Weapplyourmodeltoiteratedgeneral-sumgameswithtwoand
ThisworkislicensedunderaCreativeCommonsAttributionInter-
threeplayersandstudytheensembleeffectsofgametransitions,
national4.0License. wheretheagentsâ€™preferences(gamepayoffs)andtheirassociated
Proc.ofthe24thInternationalConferenceonAutonomousAgentsandMultiagentSystems equilibriachangeovertime(Â§4).Wepresentadynamicalanalysis
(AAMAS2025),Y.Vorobeychik,S.Das,A.NowÃ©(eds.),May19â€“23,2025,Detroit,Michigan, oftwokeyAIFquantities:thevariationalfreeenergy(VFE)(Â§4.1)
USA.Â©2025InternationalFoundationforAutonomousAgentsandMultiagentSystems andtheexpectedfreeenergy(EFE)(Â§4.2)fromnumericalsimulation
(www.ifaamas.org).
5202
yaM
02
]AM.sc[
2v26370.1142:viXra
data.Theensemble-levelEFEallowsustocharacterisethebasins andtrust(theinteractionhistorycaninfluencefuturedecisions,
ofattractionofgameswithmultipleNashEquilibria(suchasthe whereagentsmaychoosetotrustorpunishbasedonprevious
StagHunt)underdifferentconditions,andwefindthatitisnot behaviour)[4].
necessarilyminimisedattheaggregatelevel[47].
2 Background
2.1 Iteratednormal-formgames
2.2 Bayesianlearningingames
Iteratednormal-formgames(INFG)[41]provideastructuredframe-
BayesianlearninginstrategicgamesbuildsonSavageâ€™sfounda-
worktostudystrategicinteractionsbetweenagents,allowingfor tionalworkinBayesiandecisiontheory,whichoriginallyaddressed
theanalysisofdecision-makingprocessesoverrepeatedencounters. gamesagainstnature[86].Inmulti-agentsettings,outcomesde-
INFGextendthebasicframeworkofnormal-formgames,where pendonthestrategicinterplayofagents,whereeachagentâ€™sactions
players(agents)simultaneouslychoosestrategies(actions),and influenceandrespondtothoseofothers.Thisinterdependencecre-
theirpayoffsdependonthecombinationofallchosenstrategies. atesadynamic,non-stationaryenvironment,requiringagentsto
Inaniteratedsetting,thisprocessrepeatsovermultiplerounds, adaptcontinuallyasstrategiescoevolve[3,45,61].
allowingagentstoobserveoutcomesandpotentiallyadapttheir Thesimulationliteraturefocusesonhowagentscanlearnequi-
strategiesovertime.INFGaredefinedbyasetofagents,asetof libriathroughrepeatedinteractions.Thechoiceofpriorssignifi-
allowableactionsforeachagent,U ğ‘–,agamepayofffunction,g,
cantlyinfluenceswhichequilibriaagentsachieve[16,27,68].Un-
mappingeveryjointoutcome(actionsofallplayersinvolved)to dercertainconditions,rationallearningmayconvergeasymptot-
arealnumber(payoffvalueforagivenoutcome)â€”thusencoding icallytoaNashequilibriumifagentsâ€™priorscontainaâ€˜grainof
(oftenasamatrix)theincentivesorpreferencesoftheagentsâ€”, truthâ€™[51,52,66].Afundamentalmodelinthisareaisfictitious
andpossiblythetotalnumberofrounds(timesteps)thegameis play,whereagentsestimateopponentsâ€™strategiesbyaveraging
playedfor.Weusethetermâ€˜egoâ€™torefertoanyarbitraryagent pastactionsandchoosingtheirbestresponse[9,82].Thisframe-
fromwhoseperspectivewearedescribingthegame,andâ€˜alterâ€™to workcanbeinterpretedassequentialBayesianinference,where
refertoanyotheragentparticipatingintheinteraction. eachagentassumesopponentsfollowanunknown,independent,
Thesimplestgamesare(symmetric)two-player,two-action(2Ã—2)
andstationarystrategy[95].Extensionsoffictitiousplayintroduce
games1.Hereactionsareğ‘¢ âˆˆU ={0,1}â‰¡{c,d}(â€˜cooperateâ€™and
stochasticactionselection[28,30,63],exponentialforgetting[29],
â€˜defectâ€™)foreachagent.Egoâ€™spayoffsforeachofthefourpossible non-stationarystrategies[87],andvariationalinference[80].
outcomesinthesegamesarecommonlyreferredtoasreward(ğ‘…)
InAI,themulti-agentsystemsliteratureexploresBayesianmeth-
whenbothagentscooperate,temptation(ğ‘‡)whenegodefectsand
odsforcoordinationandlearning,thoughtheyhavenotyetgained
altercooperates,sucker(ğ‘†)whenegocooperatesbutalterdefects,
thesametractionasinsingle-agentreinforcementlearning[32].
andpenalty(ğ‘ƒ)whenbothdefect.Fromegoâ€™spointofview,her
Acommonapproachistoadaptsingle-agentalgorithms,suchas
payoffsare: theBayes-AdaptiveMarkovDecisionProcess[19]anditsexten-
(cid:20)ğ‘… ğ‘†(cid:21)
g= (1) sions[12,39,81,83].Thesealgorithms,however,oftenstrugglein
ğ‘‡ ğ‘ƒ
multi-agentenvironmentsduetothenon-stationarityintroduced
Canonicalgamescanbedeterminedbytherelativeorderingof by agentsâ€™ coevolving strategies, making it challenging for any
thesepayoffvalues[42],forexamplethewell-knownPrisonerâ€™s
singleagenttoconvergetoanoptimalpolicy.
Dilemma (PD) hasğ‘‡ > ğ‘… > ğ‘ƒ > ğ‘†, the Chicken game (Ch) has Toaddressthesechallenges,type-basedreasoninghasemerged
ğ‘‡ > ğ‘… > ğ‘† > ğ‘ƒ,andtheStagHunt (SH)hasğ‘… >ğ‘‡ > ğ‘ƒ > ğ‘†.By
asaprominentapproachforexplicitlymodellingotheragents.In
settingeachasanintegerbetween1and4asin[17],wehave: thisapproach,agentsmodelothersâ€™behavioursasmappingsfrom
(cid:20) 3 1 (cid:21) (cid:20) 2 3 (cid:21) (cid:20) 4 1 (cid:21) interactionhistoriestoactionprobabilities[1,44],allowingthemto
PD= 4 2 , Ch= 4 1 , SH= 3 2 , (2) anticipateandrespondtoawiderangeofstrategies.Thismethod
addresses the heterogeneity of multi-agent systems by classify-
fromwhichwecanobtainthepayofffunction,e.g.
ingagentsaccordingtotheirlearningcapabilitiesandinformation
g Ch (d,c)=ğ‘‡ Ch=4. structures[61].Bystartingwithaprioroverthesetypes,agentssys-
tematicallyupdatetheirbeliefsbasedonobservedactions,refining
Inthesingle-shotversionofanormal-formgame,agentstypi-
theirpredictionsandstrategiesastheygathermoreinformation
callymaximisetheirpayoffforthatroundalone.However,initer-
abouttheiropponentsâ€™behaviors[2,11,46,90,92].
atedgames,agentsmustconsiderlong-termoutcomes[31].This
Recursivereasoningbuildsontype-basedmethodsbyincorpo-
opensupnewpossibilitiesforstrategicbehaviour,includinglearn-
ratingnotonlyanagentâ€™sbeliefsaboutothersbutalsotheirbeliefs
ing(agentscanlearnfrompreviousinteractionsadjustingtheir
aboutthebeliefsofothers,formingahierarchicalstructure.This
strategytoimprovefutureoutcomes),reciprocity(agentsmight
approachunderliesmodelsliketheInteractivePartiallyObserv-
cooperateiftheybelieveotherswillreciprocateinfuturerounds,
ableMarkovDecisionProcess[34â€“36],whereagentsmaintainand
balancingshort-termlossesforlong-termgains),andreputation
updatebeliefsaboutothersâ€™beliefsandstrategiesacrossmultiple
1Two-actionsettings(e.g.,cooperate/defect)arestandardingametheoryduetotheir levels.Bymodellingnestedbeliefs,agentsbetteranticipateothersâ€™
simplicity,analyticaltractability,andclarityofincentives.Thesesettingsfocuson
actions,layingthefoundationforTheoryofMindmodelsthatsup-
fundamentaldynamicsandarewidelyapplicabletoreal-worldscenariosabstracted
intobinarydecisions. portmoresophisticatedandadaptiveinteractions[6,18,67,76,92].
Anotherlineofresearchextendsgraphicalmodelstomulti-agent inclassicalandquantumcontexts.Thiscomplexityhelpsexplain
contexts,leveragingconditionalindependenciesforefficientrepre- whyreal-worldsystemsoftenfailtoconvergetostableoutcomes.
sentationandinference.Multi-agentinfluencediagramsandgraph- Groundingstrategicdecision-makinginAIFprovidesamore
icalgamescapturedependenciesamongagents,enablingefficient realisticmodelforsocialinteractions,movingbeyondtheassump-
computationbyfocusingonlocalinteractions[54,56,57].Simi- tionsofperfectrationalityandcompleteinformationintraditional
larly,action-graphgamesandexpectedutilitynetworksoptimise gametheory,withafoundationinneuroscience.
inferencebystructuringinteractionsaroundsharedactionswithin
agentsubsets,whichisparticularlyadvantageousinsparselycou- 3 Modeldescription
pledgames[49,50,58]. InINFG,ğ‘ agentsinteractbyselectinganactioneachtimestep
Finally,theintersectionofBayesianlearningandboundedratio-
withthegoalofmaximisingtheirpayoffsasdeterminedbythe
nalityframesdecision-makingasconstrainedoptimisationunder gamepayofffunction,g.Theagentsobservetheactionstakenby
uncertainty.ProductDistributionTheoryappliestheMaximumEn-
eachoftheagents(includingthemselves)intheprecedingstepin
tropyprinciple[48]toderiveequilibriawhereagentsbalanceutility
ordertodecidehowtoactinthecurrentstep.
maximisationwithcomputationalcosts[91].GrÃ¼nwaldandDawid Inourmodel2,theseobservationsareperceivedviadifferent
demonstratethatmaximisingentropyandminimisingworst-case modalities,ğ‘š,oneforeachagent.Forexample,forğ‘ =3agents
expected loss are dual problems, structured as zero-sum games ğ‘šâˆˆ{ğ‘–,ğ‘—,ğ‘˜},|U|=2actions,andtakinganegocentricperspective,
betweenadecision-makerandnature[38].ThermodynamicDeci- eachobservationiso=(ğ‘œ
ğ‘–
,ğ‘œ
ğ‘—
,ğ‘œ
ğ‘˜
) âˆˆ{c,d}ğ‘,withthefirstmodal-
sionTheoryexpandstheseprinciples,integratingutility(energy) itybeingegoâ€™sactionğ‘œ ğ‘–,ğ‘¡ = ğ‘¢ ğ‘–,(ğ‘¡âˆ’1),andsubsequentmodalities
andinformation-processingcosts(entropy)withinavariational ğ‘œ
ğ‘—,ğ‘¡
=ğ‘¢
ğ‘—,(ğ‘¡âˆ’1)
eachpertainingtoanalter(ğ‘—,ğ‘˜).Furtherdetailsare
framework where agents minimise free energy. This approach
providedinÂ§3.2.1.Inthefollowing,weomittimeandagentindices
naturallyextendstovariationalBayesianinference,enablingef-
whereimpliedbycontext.
ficient,approximateposteriorupdatesunderboundedrationality
Toacteffectivelyinresponsetohercounterparts,egomusttake
constraints[74].Thisframeworkgeneralisestorisk-sensitivecon-
intoaccounteachofheropponentsâ€™propensityforplayingeach
trol[20,53,60,69,70]andadversarialcontexts[71,72],illustrating actionatagiventime(e.g.forğ‘— toplayâ€˜cooperateâ€™,orğ‘(ğ‘¢
ğ‘—
=c)).
itsversatilityacrossdiversedecision-makingscenarios. Thispropensityisdrivenbytheopponentâ€™sâ€˜internalworldâ€™,ğœ“ ğ‘—,
whichisnotobservabletoego[62].
2.3 GametheoryandActiveInference Anappropriatewaytomodelğœ“ ğ‘— isasahiddenstateusinga
Partially-ObservableMarkovDecisionProcess(POMDP),where,each
ThissectionoutlineshowAIFandgametheoryhavebeencombined timestep,agentsinferthecurrenthiddenstateğ‘  âˆˆSbasedonan
tomodelstrategicdecision-makinginsocialinteractions.Yoshida observationğ‘œ âˆˆO,andcaninfluenceitthroughtheiractionsğ‘¢ âˆˆU
etal.[93]examinehowindividualsinfertheintentionsofothers
tomaximisetheirpayoff.AIFdistinguishesbetweentheexternal
inthespatialStagHuntgame,highlightingthatpeopleengagein (ontological)generativeprocess,whichrepresentstheactualdynam-
recursivethinkingaboutothersâ€™beliefs.Thisapproachconnects icsoftheenvironment,andtheinternal(epistemic)generativemodel
strategic thinking in game theory with Bayesian inference and
ofeachagent,whichencodesitsbeliefsaboutthosedynamics,and
boundedrationalityincognitivepsychology,providinginsights
assuchisagoodmatchforthePOMDPformalism[15,89].
intohowpeoplemakedecisionsinuncertainsocialenvironments.
Moutoussisetal.[65]developaformalmodelofinterpersonal 3.1 GenerativeModel
inferencebasedonactiveinferenceprinciples,whereagentsinfer
Anagentâ€™sgenerativemodelconsistsofajointdistributionover
theirpartnerâ€™slikelytype(cooperativeordefecting)byobserving
hiddenstates,observations,policies(actionsequences),andmodel
pastactionsandupdatingtheirbeliefs.Thisdemonstrateshowthe
parameters.Theshorttimescaledynamicsareencodedin
computationalmodelsusedtodescribeindividualdecision-making
canbeextendedtothecomplexitiesofsocialinteraction,where ğ‘¡
(cid:214)
understandingothersâ€™mentalstatesiscrucial. ğ‘(ğ‘  0:ğ‘¡ ,ğ‘œ 0:ğ‘¡ |ğ‘¢ 0:ğ‘¡âˆ’1 )=ğ‘(ğ‘  0 ) ğ‘(ğ‘œ ğœ |ğ‘  ğœ )ğ‘(ğ‘  ğœ |ğ‘  ğœâˆ’1 ,ğ‘¢ ğœâˆ’1 ),
Demekasetal.[17]buildonthisbyshowinghowAIFagentscan ğœ=1
learneffectivestrategiesintheiteratedPrisonerâ€™sDilemmabycon- includingtheagentâ€™spriorbeliefsabouttheinitialstateoftheworld
tinuouslyupdatingbeliefsabouttransitionprobabilitiesbetween ğ‘(ğ‘  0 )encodedinD;thetransitionmodelğ‘(ğ‘  ğ‘¡+1 |ğ‘  ğ‘¡ ,ğ‘¢ ğ‘¡ ),encodedin
gamestates.Theirgenerativemodeltrackshowlearningratesinflu- B;andtheobservationlikelihoodğ‘(ğ‘œ|ğ‘ ),encodedinA.Fordiscrete
encestrategydevelopment,offeringanalyticalclaritythroughbelief POMDPs,thedistributionscanbeobtainedfromtheirencodingasa
updates.Hylandetal.[47]introducetheâ€˜Free-EnergyEquilibriaâ€™ categoricaldistribution,e.g.ğ‘(ğ‘œ|ğ‘ )=Cat(A)whereAisa|O|Ã—|S|
framework,extendingtheEFEtostrategiccontextsbyconditioning matrix,Bisa|S|Ã—|S|Ã—|U|tensor,andDavectorinthesimplex
predictionsonthejointpoliciesofagents.Thisframeworkmerges Î” S.
Nashequilibriawithboundedrationality,proposingthatcoopera- Inourapplicationtostrategicinteractions,egoinfersthehidden
tioncouldemergeasagentsalignactionsthroughjointfree-energy stateofeachagent[6,84]inacorrespondingfactor,ğ‘› âˆˆ {ğ‘–,ğ‘—,ğ‘˜},
minimization. ofthegenerativemodel.Egomustmakevastsimplificationsin
FieldsandGlazebrook[21]furtherexplorehowphysicalinter- modellingeachalterâ€™strueandcomplexinternalworldğœ“ ğ‘—â€”which
actionscanbeframedasgamesusingtheFreeEnergyPrinciple,
drawingattentiontotheundecidabilityofachievingNashequilibria 2CodeavailableatGitHub/RuizSerra/factorised-MA-AIF
encompassesallthecomponentsinalterâ€™sgenerativemodel,en- modelBğ‘¢,i.e.
codinghisbeliefs(A,B,D,ğ‘;seeÂ§3.2),preferences(C;seeÂ§3.3),and
âˆ‘ï¸
constraints(suchashabitsE,orlevelofrationalityğ›½ 1;seeÂ§3.4)[33], ğ‘(ğ‘  ğ‘¡ )â‰ˆğ‘(ğ‘  ğ‘¡ |ğ‘¢ ğ‘¡âˆ’1 )= ğ‘(ğ‘  ğ‘¡ |ğ‘  ğ‘¡âˆ’1 ,ğ‘¢ ğ‘¡âˆ’1 )ğ‘(ğ‘  ğ‘¡âˆ’1 ) (5)
all depicted in Figure 1. Our approach is to model this internal
ğ‘ ğ‘¡âˆ’1
world(externaltoego)asacategoricaldistributionoverdifferent 3.2.1 Factorisedmodel GenerativemodelsinpreviousAIF-adjacent
types,withparameterss,placingtheopponenttypeonthesimplex applicationstogametheory[17,22,65]assumethestatespaceis
Î” S.Inthesimplestsuchmodel,whichweadopthere,thesetypes jointacrossallagentsinthegame(i.e.{cc,cd,...dd}fortheirtwo
maybeâ€˜cooperatorâ€™orâ€˜defectorâ€™,andtheopponentcouldbeany- agents).Isamean-fieldfactorisationofthevariationalposterior,
wherebetweenthetwo(i.e.,wemodeltheopponentâ€™spropensity ğ‘(ğ‘ 
ğ‘–
)ğ‘(ğ‘ 
ğ‘—
)ğ‘(ğ‘ 
ğ‘˜
),adequateinthegame-theoreticcontext,orshould
forplayingeachaction)3. ajointdistribution,ğ‘(ğ‘ 
ğ‘–
,ğ‘ 
ğ‘—
,ğ‘ 
ğ‘˜
),beassumed[80]?Thatis,canthe
Egofurtherassumesthatheropponentsplaytheactionsthey hiddenstatesbeconsideredindependentofeachother?Recallthat,
meantoplay,rulingoutthepossibilityofâ€˜tremblinghandâ€™imper- whenouragentsperforminference,theyapproximatetheposte-
fections.Thissimplifiesourmodelsuchthatthereisnoâ€˜ambiguityâ€™ riorğ‘(ğ‘ |ğ‘œ)â‰ˆğ‘(ğ‘ ).Inthecaseofrepeatednormal-formgames,the
intheenvironment,sothelikelihoodmodelforeachfactorAğ‘›isan hiddenstateğ‘  ğ‘— is(theparameterisationof)theposteriordistribu-
identitymatrix,orequivalentlythattheobservationlikelihoodisa tionoveractions(policy)ğ‘(ğ‘¢
ğ‘—
)ofopponentğ‘—,andtheobservation
Kroneckerdeltadistribution,ğ‘(ğ‘œ ğ‘š |ğ‘  ğ‘› )=ğ›¿ ğ‘œğ‘š,ğ‘ ğ‘› ,âˆ€ğ‘š=ğ‘›âˆˆ{ğ‘–,ğ‘—,ğ‘˜}. ğ‘œ ğ‘— istheactionğ‘¢ ğ‘— takenbythisopponent.Soinferringadistri-
butionoverasingleopponentâ€™shiddenstateinvolvesfindinga
3.2 Variationalinference ğ‘(ğ‘ 
ğ‘—
)â‰ˆğ‘(ğ‘ 
ğ‘—
|ğ‘œ
ğ‘–
,ğ‘œ
ğ‘—
,ğ‘œ
ğ‘˜
).
Everytimestep,havingobservedtheactionsofeachagent,ğ‘œ ğ‘› =ğ‘¢ ğ‘›, TheMarkovBlanket (MB)ofanodeorsetofnodes J isde-
egohastoinfertheunderlyingğ‘ 
ğ‘›
â‰¡ğ‘(ğ‘¢
ğ‘›
),foreachfactor.This finedasthesetofJâ€™sparents,Jâ€™schildren,andanyotherpar-
entailsupdatingherposteriorbeliefsğ‘ ğ‘– (ğ‘  ğ‘— |ğ‘œ ğ‘— )abouteachhidden entsofJâ€™schildren[78].Underthisdefinition,ifweletJ bethe
statefactorthroughBayesianinversion.Thetrueposteriormaybe nodesâ€˜insideâ€™agent ğ‘—,includingğ‘  ğ‘— = ğ‘(ğ‘¢ ğ‘— ),theMBconsistsof
intractable,soitisapproximatedbyavariationalposteriorğ‘ ğ‘– (ğ‘  ğ‘— ). {ğ‘œ ğ‘– ,ğ‘œ ğ‘— ,ğ‘œ ğ‘˜ ,ğ‘¢ ğ‘— },whichâ€˜shieldâ€™theinternalstatesof ğ‘— fromtheex-
ThisisachievedbyminimisingtheVariationalFreeEnergy(VFE), ternalworld.InINFG,takingdynamicsintoaccount,thisMBsetis
which is expressed as (having omitted the agentğ‘– and factor ğ‘— actually{ğ‘¢ ğ‘–(ğ‘¡âˆ’1) ,ğ‘¢ ğ‘—(ğ‘¡âˆ’1) ,ğ‘¢ ğ‘˜(ğ‘¡âˆ’1) ,ğ‘¢ ğ‘—ğ‘¡ }.Thus,wecansaythat
indice ğ¹ s [ğ‘ fo ,ğ‘œ r ] br = ev E (cid:124) i (cid:32) ğ‘ t (cid:32)(cid:32) y (cid:32) ( (cid:32)(cid:32) ğ‘  (cid:32) ) (cid:32)(cid:32) : ) (cid:32)(cid:32)(cid:32) [ (cid:32)(cid:32)(cid:32) âˆ’ (cid:32)(cid:32)(cid:32)(cid:32) (cid:123) lo (cid:122) g (cid:32)(cid:32)(cid:32) ğ‘ (cid:32)(cid:32)(cid:32)(cid:32) ( (cid:32)(cid:32)(cid:32) ğ‘œ (cid:32)(cid:32)(cid:32) , (cid:32)(cid:32) ğ‘  (cid:32)(cid:32)(cid:32) ) (cid:32) (cid:125) ]âˆ’ğ» (cid:124) (cid:32)(cid:32) [ (cid:32) (cid:123) ğ‘ (cid:122) (ğ‘  (cid:32)(cid:32) ) (cid:32) (cid:125) ] (3a) i.e., ğ‘  ğ‘— is c ğ‘  o ğ‘— nd âŠ¥ it { io ğ‘  ğ‘– n , a ğ‘  l ğ‘˜ ly } (cid:12) (cid:12) in {ğ‘¢ d ğ‘– e ( p ğ‘¡ e âˆ’ n 1 d ) , e ğ‘¢ nt ğ‘—(ğ‘¡ o âˆ’ f 1 ğ‘  ) ğ‘– ,ğ‘¢ a ğ‘˜ n ( d ğ‘¡âˆ’ ğ‘  1 ğ‘˜ ) ,ğ‘¢ (a ğ‘— n ğ‘¡ d }, any other
energy entropy nodeoutsideğ‘—)given{ğ‘¢ ğ‘–(ğ‘¡âˆ’1) ,ğ‘¢ ğ‘—(ğ‘¡âˆ’1) ,ğ‘¢ ğ‘˜(ğ‘¡âˆ’1) ,ğ‘¢ ğ‘—ğ‘¡ }.Furthermore,
=Dkl (cid:2)ğ‘(ğ‘ ) (cid:13) (cid:13) ğ‘(ğ‘ |ğ‘œ) (cid:3) âˆ’logğ‘(ğ‘œ) â‰¥âˆ’logğ‘(ğ‘œ), (3b) attimeğ‘¡ whenğ‘–infersğ‘  ğ‘—,theactionğ‘¢ ğ‘—ğ‘¡ hasnothappenedyet,so
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) thatnodedoesnotexistintheDynamicBayesianNetwork.There-
divergence evidence evidence fore,ğ‘(ğ‘  ğ‘— ) â‰ˆ ğ‘(ğ‘  ğ‘— |ğ‘œ ğ‘– ,ğ‘œ ğ‘— ,ğ‘œ ğ‘˜ ), i.e. given the MBs of agents, their
withğ‘(ğ‘ )theprioroverhiddenstates,andğ‘(ğ‘ )thevariationalpos- internalstatesareconditionallyindependent.
terior,ortheagentâ€™sbeliefsabouthiddenstates.Wemodelbeliefs
Accordingly,inourmodel,eachagentğ‘–infersthehiddenstatefor
viaaDirichlet(ğœ½)distributionwithvariationalparametersğœ½.We
everyfactorğ‘›âˆˆ{ğ‘–,ğ‘—,ğ‘˜}individually,andthusretainsacollection
approximatetheVFEusingthefollowingMonteCarlosampling ofparametersğœ½ âˆˆR + ğ‘Ã—|U| ,withğ‘ thenumberoffactors(agents
procedure:letğ‘ âˆ¼Dirichlet(ğœ½)betheğ‘™-thsamplefromtheDirich- beingtracked)and|U|thenumberofactions.
ğ‘™
letdistribution.Withğ¿samples{ğ‘ }ğ¿ ,wecanwriteanunbiased
ğ‘™ ğ‘™=1
estimateoftheVFE4as 3.3 Preferencesandplanning
ğ¹[ğ‘,ğ‘œ] =âˆ’E ğ‘(ğ‘ ) (cid:2) logğ‘(ğ‘œ|ğ‘ )+logğ‘(ğ‘ )âˆ’logğ‘(ğ‘ ) (cid:3) (4a) AIFextendsBayesianlearningbyintegratingactionanddecision-
making,allowingagentstoactivelyreduceuncertaintyandachieve
ğ¿
â‰ˆğ¹Ë†(cid:2) {ğ‘ }ğ¿ ,ğ‘œ(cid:3) = 1 âˆ‘ï¸ ğ¹[ğ‘ ,ğ‘œ] (4b) goal-directedbehaviourindynamicenvironments.Agentsplanand
ğ‘™ ğ‘™=1 ğ¿ ğ‘™ selectactionsthatbothgatherinformationandsatisfytheirpref-
ğ‘™=1 erencesoverobservationsğ‘âˆ—(ğ‘œ),encodedinC(suchthatğ‘âˆ—(ğ‘œ)=
andoptimisethevariationalparametersğœ½ throughstochasticgradi- Cat(C)indiscretesettings).Thisrequirescounterfactualthinking:
entdescentonğ¹Ë†.Havingfoundğœ½âˆ—uponcompletingtheprocedure,
â€˜whatwouldIbelikelytoobserveifIweretodoğ‘¢ ğ‘–?â€™.Thegenerative
wecanrecovertheinferredposteriorğ‘(ğ‘ )astheexpectedvalueof
modelemployedininferencecanbeusedforplanningbypredicting
Dirichlet(ğœ½âˆ—),whichservesastheoptimalpointestimateundera
futurestates(viathetransitionmodelB ğ‘¢Ë† )andobservations(viathe
quadraticlossfunction[8]. likelihoodA)givencounterfactualactionsğ‘¢Ë†ğ‘– (distinguishedfrom
Sincevariationalinferenceoccurseverytimestep,thepriorğ‘(ğ‘ ) actualactionsğ‘¢ ğ‘–).Inthefollowing,wedenotepredictivevariables
isobtainedfromthepreviouslyinferredstateandthetransition withabar,e.g.ğ‘œÂ¯ğ‘— iswhatğ‘–mightobserveğ‘— doinginthenexttime
3Thesupportofthisdistributionneednotbelimitedtothenumberofactions.For step.
example,anagentcouldconsiderfourpossibletypesforpolicylength2,ormore Agentsachievethedesiredexploration-exploitationtrade-off
abstracttypessuchasâ€˜TitforTatâ€™[4]. byselectingactionsthatminimiseanExpectedFreeEnergy(EFE),
4ThechosenformoftheVFEforoptimisationisderivedfrom(3a)byreplacing
comprisingsalienceandpragmaticvalueterms.Inwhatfollows,we
ğ‘(ğ‘œ,ğ‘ )=ğ‘(ğ‘œ|ğ‘ )ğ‘(ğ‘ )andsubsumingalltermsintoasingleexpectationoperatorin
(4a). describethesetermsinmoredetailandadaptthemtoINFG.
exploratorybehaviour.Thistermcapturestheuncertaintyabout
thenextobservationintheeventthatğ‘¢Ë†ğ‘–istheactiontaken.Actions
whoseoutcomeswearemostuncertainaboutarepreferred,aswe
standtogainthemostinformationfromthem.Salienceisadditive
overfactors:
ğœ[ğ‘¢Ë†ğ‘– ] = âˆ‘ï¸ ğ»(cid:0)ğ‘(ğ‘œÂ¯ğ‘š |ğ‘¢Ë†ğ‘– )(cid:1) (8)
ğ‘š
3.3.2 Pragmaticvalue Bychoosingactionsthatmaximiseprag-
maticvalue(ğœŒ),agentsactivelypursuetheirpreferences,closingthe
gapbetweenpredictedandpreferredobservations.Innormal-form
games,preferencesaredeterminedbythegamepayoffsg,andwe
canconvertdirectlyfromonetotheotherwith
ğ‘âˆ—(ğ‘œ ğ‘– ,ğ‘œ ğ‘— ,ğ‘œ ğ‘˜ )=ğœ(cid:0)g(ğ‘œ ğ‘– ,ğ‘œ ğ‘— ,ğ‘œ ğ‘˜ )(cid:1) (9)
âˆ€(ğ‘œ
ğ‘–
,ğ‘œ
ğ‘—
,ğ‘œ
ğ‘˜
) âˆˆUğ‘,whereğœisthesoftmaxfunction5[17,73].
Thisimpliesajointinteractioncontext,sincethepreferencesfor
eachobservationmodalitycomefromajointdistributionandare
generallynotindependent.Thishighlightsthecoreprincipleof
gametheory,namelytheinterdependenceofagentsâ€™preferences
andactions.Herethepreferencesforeachobservationmodalityare
derivedfromajointdistribution,meaningthatagentsâ€™outcomes
areinterconnected,andtheirstrategiescannotbeconsideredin
isolation.Theessenceofgametheoryliesinanalyzinghowthese
dependenciesshapedecision-makingandtheresultingequilibria
instrategicinteractions.Agentstracktheâ€˜mentalstatesâ€™ofother
agentsindividually(inğ‘(ğ‘ 
ğ‘—
)),buttheymustlearnhowtheyinter-
playinagivenjointcontextdefinedbyg(e.g.whentwoagentsplay
aPrisonerâ€™sDilemma,orwhenthreeagentsplayChicken).Unlikein
classicalgametheory,however,ouragentsdonotknowthepayoff
function(preferences)oftheiropponents.
Thepredictedobservations,asaposteriorpredictivedistribu-
tionğ‘(ğ‘œÂ¯|ğ‘¢Ë†) for each modality, need to be merged into a single
ğ‘(ğ‘œÂ¯ğ‘– ,ğ‘œÂ¯ğ‘— ,ğ‘œÂ¯
ğ‘˜
|ğ‘¢Ë†ğ‘– )tobeabletocomparetothepreferences.Thecon-
ditionalindependencebetweentheinternalstatesoftheagents,
determinedbytheirrespectiveMBs(Â§3.2.1),allowsustodefinethis
jointposteriorastheproduct[91]
(cid:214)
ğ‘(ğ‘œÂ¯ğ‘– ,ğ‘œÂ¯ğ‘— ,ğ‘œÂ¯
ğ‘˜
|ğ‘¢Ë†ğ‘– )= ğ‘(ğ‘œÂ¯ğ‘š |ğ‘¢Ë†ğ‘– ) (10)
ğ‘šâˆˆ{ğ‘–,ğ‘—,ğ‘˜}
whereeachfactorâ€™sposteriorpredictiveobservationisobtained
ğœ“
F
ğ‘–
i
.
g
T
ur
h
e
e
1
a
:
g
T
e
h
n
e
to
p
b
e
s
r
e
c
r
e
v
p
e
t
s
io
t
n
h
-
e
a
a
ct
c
i
t
o
io
n
n
l
s
o
o
o
f
p;
a
e
ll
g
a
o
g
â€™s
en
â€˜in
ts
t
,
e
f
r
r
n
o
a
m
lw
w
o
h
r
i
l
c
d
h
â€™, fromthe(factorâ€™s)likelihoodmodelAğ‘— as6
sheupdatesherbeliefsğ‘(ğ‘ )tominimiseVFE.Thesebeliefs ğ‘(ğ‘œÂ¯ğ‘— |ğ‘¢Ë†ğ‘– )=E ğ‘(ğ‘ Â¯ğ‘— |ğ‘¢Ë†) [ğ‘(ğ‘œÂ¯ğ‘— |ğ‘ Â¯ğ‘— )]. (11)
areusedtoplanhernextactionbyminimisingEFE.
Thepragmaticvalueisthusdefinedasthenegativecross-entropy
3.3.1 Salience Otherwiseknownasâ€˜epistemicvalueâ€™,salience(ğœ) betweentheposteriorpredictiveobservationandthepreference
distributions,whichagentsaimtomaximise(cf.log-lossminimisa-
capturestheinformationgainabouthiddenstatesâ€”orhowanac-
tion):
tionisanticipatedtochangeoneâ€™sbeliefsâ€”withgreaterchangesin
beliefsholdinghigherepistemicvalue[77].Itcanbedecomposed ğœŒ[ğ‘¢Ë†ğ‘– ] =E ğ‘(ğ‘œÂ¯ğ‘–,ğ‘œÂ¯ğ‘—,ğ‘œÂ¯ğ‘˜ |ğ‘¢Ë†ğ‘– ) (cid:2) logğ‘âˆ—(ğ‘œÂ¯ğ‘– ,ğ‘œÂ¯ğ‘— ,ğ‘œÂ¯ ğ‘˜ ) (cid:3). (12)
intoadifferencebetweentwoentropicterms: Furthermore,fortheegoâ€™sownfactor,ğ‘œÂ¯ğ‘– =ğ‘¢Ë†ğ‘–isguaranteedwith
ğœ[ğ‘¢Ë†] =E ğ‘(ğ‘œÂ¯|ğ‘¢Ë†) (cid:2) Dkl (cid:2)ğ‘(ğ‘ Â¯|ğ‘¢Ë†,ğ‘œÂ¯) (cid:12) (cid:12) (cid:12) (cid:12) ğ‘(ğ‘ Â¯|ğ‘¢Ë†) (cid:3)(cid:3) (6) fullcertaintyinthecounterfactualwhereğ‘¢Ë†ğ‘– isplayed(i.e.whereğ‘¢ ğ‘–
=ğ»(cid:0)ğ‘(ğ‘œÂ¯|ğ‘¢Ë†)(cid:1)âˆ’E
ğ‘(ğ‘ Â¯|ğ‘¢Ë†)
(cid:2)ğ»(cid:0)ğ‘(ğ‘œÂ¯|ğ‘ Â¯)(cid:1)(cid:3)
(7) 5Thesoftmaxoperationhereisnotstrictlynecessary;wecouldjustaswellgowith
SinceourINFGenvironmentisunambiguous(cf.Â§3.1),thesec- ğ‘âˆ—(ğ‘œğ‘–,ğ‘œğ‘—,ğ‘œğ‘˜ )=exp(cid:0)g(ğ‘œğ‘–,ğ‘œğ‘—,ğ‘œğ‘˜ )(cid:1) underanenergyfunctioninterpretation.How-
ever,byensuringğ‘âˆ—isaprobabilitydistribution,weensuretheEFEvaluesareinthe
ondterm(ambiguity)iszero.Thereis,however,informationto
positiverange(i.e.inNats)[37,59].
begainedstillfromactingtomaximisethefirstterm,leadingto 6ğ‘(ğ‘œÂ¯ğ‘— |ğ‘¢Ë†ğ‘– )=(cid:205)
ğ‘ Â¯ğ‘—
ğ‘(ğ‘œÂ¯ğ‘—,ğ‘ Â¯ğ‘— |ğ‘¢Ë†ğ‘– )=(cid:205)
ğ‘ Â¯ğ‘—
ğ‘(ğ‘œÂ¯ğ‘— |ğ‘ Â¯ğ‘— )ğ‘(ğ‘ Â¯ğ‘— |ğ‘¢Ë†ğ‘– )=E ğ‘(ğ‘ Â¯ğ‘—|ğ‘¢Ë†)[ğ‘(ğ‘œğ‘— |ğ‘ Â¯ğ‘— )]
wouldinactualitybeğ‘¢Ë†ğ‘–).Accordingly,wecansetğ‘(ğ‘œÂ¯ğ‘– |ğ‘¢Ë†ğ‘– )=ğ›¿ ğ‘œÂ¯ğ‘–,ğ‘¢Ë†ğ‘– , 3.5.1 Novelty Withlearningpresent,theagentscanconsiderhow
theKroneckerdeltadistribution7.Thismakesthepragmaticvalue theiractionsarelikelytoinfluencetheirgenerativemodel.Inthis
case,anagentpredictshowthetransitionmodelmightchangeif
ğœŒ[ğ‘¢Ë†] â‰¡E ğ‘(ğ‘œÂ¯ğ‘— )ğ‘(ğ‘œÂ¯ğ‘˜ ) [logğ‘âˆ—(ğ‘œÂ¯ğ‘– ,ğ‘œÂ¯ğ‘— ,ğ‘œÂ¯ ğ‘˜ )|ğ‘œÂ¯ğ‘– =ğ‘¢Ë†ğ‘– ], (13) theyweretoplayactionğ‘¢Ë†ğ‘– (foreachfactorğ‘›):
i i . n e t . e , r e p q r u e i t v a a ti l o en n t th to at th lo e g g ğ‘ a âˆ— m is e- ( t p h r e o o p r o e r t t ic io e n x a p l e t c o t ) ed th u e t u il t it il y i , ty un fu d n e c r ti t o h n e â€”B ğ‘¢Ë†ğ‘–,ğ‘› =B ğ‘¢Ë†ğ‘–,ğ‘› +ğ›¼ ğ‘™ (sÂ¯ğ‘› âŠ—sğ‘› ) (19)
NoveltyisanadditionaltermintheEFE,constitutingadditional
(i.e.,thegamepayoffs,aswedidinEq.9).Finally,wehavetheEFE,
epistemicvalue:
ğº[ğ‘¢Ë†ğ‘– ] =âˆ’ğœŒ[ğ‘¢Ë†ğ‘– ]âˆ’ğœ[ğ‘¢Ë†ğ‘– ], (14) ğœ‚[ğ‘¢Ë†ğ‘– ] = âˆ‘ï¸ Dkl (cid:2)â€”B ğ‘¢Ë†ğ‘–,ğ‘› (cid:12) (cid:12) (cid:12) (cid:12)B ğ‘¢Ë†ğ‘–,ğ‘› (cid:3) (20)
whichtheagentsminimisethroughtheiractions,effectivelymax- ğ‘›
imisingsalience(ğœ)andpragmaticvalue(ğœŒ).Forthezero-ambiguity
summedoverfactors.IncludingnoveltyintheEFE,wehave
caseunderconsideration,theEFEreducesto(inshorthand)
ğº[ğ‘¢Ë†ğ‘– ] =âˆ’E ğ‘âˆ’ğ‘– [logğ‘âˆ—|ğ‘¢Ë†ğ‘– ]âˆ’ âˆ‘ï¸ ğ»(cid:0)ğ‘ ğ‘š (cid:1), (15)
3.5.2 Bayesianm
ğº
od
[ğ‘¢
e
Ë†
l
ğ‘– ]
re
=
du
âˆ’
ct
ğœŒ
io
[ğ‘¢
n
Ë†ğ‘– ]
A
âˆ’
B
ğœ
a
[
y
ğ‘¢Ë†
e
ğ‘–
s
]
ia
âˆ’
n
ğœ‚
m
[ğ‘¢
o
Ë†
d
ğ‘– ]
elreductionpro
(2
c
1
e
)
-
ğ‘š
dure[15]appliedtoB(foreachfactorandaction)atlearningtime
highlightingtherelationshipwithpreviousinformation-theoretic helpsreduceoverfitting.Aâ€˜reducedâ€™modelB~ ğ‘¢,ğ‘› = ğœ(ğ›¼ ğ‘Ÿ âˆ’1Bğ‘¢,ğ‘› )
treatmentsofboundedrationalityingametheory[75,91]. is proposed (with reduction rateğ›¼ ğ‘Ÿ = 1.25) and their evidence
differenceiscomputedas
3.4 Actionselection
T of he ea s c e h lec p t o io ss n ib o l f e a a c c ti t o io n n s ğº is [ d ğ‘¢Ë† r ğ‘– i ] ve (o n r b G yt i h n e v p e r c e t c o i r sio n n o - t m at o io d n u ) l , a a te n d d E t F h E e logğ‘Ëœ(ğ‘œ ğ‘› )âˆ’logğ‘(ğ‘œ ğ‘› )=logE B ğ‘¢ â€² ,ğ‘›
(cid:20)B
B
~
ğ‘¢ ğ‘¢ , , ğ‘› ğ‘›
(cid:21)
. (22)
agentâ€™shabitsE(uniform): Ifthedifferenceispositive(respectivelynegative),theevidence
forthereduced(resp.original)modelisgreater,soit(resp.the
ğ‘¢ ğ‘– âˆ¼ğ‘(ğ‘¢Ë†ğ‘– )=ğœ(cid:0) logEâˆ’ğ›¾G (cid:1), (16) posteriormodel)isselectedastheupdatedmodel.
whereğ›¾ isaprecisionparameterupdatedeachtimestep,
4 Resultsanddiscussion
ğ›½
ğ›¾ = ğ›½
0
âˆ’ 1 âŸ¨GâŸ© , (17) Game transitions increase the non-stationarity of the environ-
ment beyond that caused by reciprocal adaptation, resulting in
withfixedhyperparameters(ğ›½ 0 ,ğ›½ 1 ).ğ›½ 0(shape)representsabase- rolereversals,strategicuncertainty,andeventualequilibriumse-
linelevelofuncertaintyornoise,andğ›½ 1(rate)reflectshowstrongly lection. We use a time-based linear interpolation of the payoff
theagentâ€™sprecision(orconfidence)isinfluencedbyenvironmental matricesoftwogamesgandgâ€²totransitionbetweenthem.Given
feedback.Intuitively,ğ›½ 0controlsthethresholdatwhichtheagent atimeoftransitionğ‘¡ xandatransitiondurationğ‘‡ x,thepreferences
startsdoubtingitsactionselections,whileğ›½ 1modulatestherate ğ‘âˆ— ofeachagentareupdatedeachtimestepwithintheinterval
o an f d pr o e b c s i e s r io v n ed u o p u d t a c t o i m ng es b . a H se ig d h o e n rv t a h l e ue d s iff o e f r ğ›½ e 1 n c c o e r b re e s t p w o e n e d n t e o x g p r e e c a t t e e d r w (ğ‘¡ i x t âˆ’ h ğ‘‡ 2 x) â‰¤ğ‘¡ â‰¤ (ğ‘¡ x +ğ‘‡ 2 x),viamixingparameterğ‘™ = (cid:0)ğ‘¡âˆ’(ğ‘¡ x âˆ’ğ‘‡ 2 x)(cid:1)/ğ‘‡ x,
sensitivitytodiscrepancies,makingtheagentmoreâ€˜rationalâ€™and ğ‘âˆ— =ğœ (cid:16) (1âˆ’ğ‘™)g + ğ‘™gâ€² (cid:17) . (23)
noise-averseinrefiningitsactionpolicy[23,26].Externalfeedback
isaccountedforinâŸ¨GâŸ©=E ğ‘(ğ‘¢Ë†ğ‘– ) (cid:2)ğº[ğ‘¢Ë†ğ‘– ] (cid:3) ,theexpectedEFEunder 4.1 VFEandstrategicuncertainty
thecurrentactionprobabilitiesforthisagent.
Toillustratethedynamicsoftransitioningbetweengames,werun
3.5 Learning atwo-playerChgamefor500iterations,followedbyaSHgamefor
another500,withpayoffsasperEq.2.Thetransitionoccursover
LearninginAIFentailsupdatingmodelparameters,andoccursata
slowerratethaninference.Inourmodel,agentsupdatethetransi- ğ‘‡ x =10iterations.Theoutcomeisshownastimeseriesplotsfor
t g i e o r n s m am o p d l e e l d B un (i i n f i o t r ia m ll l y y u fr n o i m fo 1 rm 8 ) â‰¤ e ğ‘‡ ve ğ¿ ry â‰¤ ğ‘‡ 3 ğ¿ 0 s e t a e c p h s, ti w m h e e l r e e a ğ‘‡ rn ğ¿ in is g a o n cc in u t r e s - . v u a n r d T io e h r u e s t V h q e F u E c a , u n ğ¹ r t r i [ t e ğ‘ i n e , s ğ‘œ t ] b f , o e q r li u e e a a fs n ch t ğ‘ ifi ( o ğ‘  e f ) s . th A h e o s w t s w h s o o u w a rp g n r e i i n s n t in s F g i i n g a . F n 2 ig o (fi u b r r s e s e t r 2 v r . o a w tio )â€” n a ğ‘œ n i d s
Thisrandomoffsetensuresagentsdonotlearninlockstep,which
furtherhighlightedbythestylisedboundsinFig.3â€”,theensem-
mightcauseartifactsinthedynamics.Theparametersareupdated
basedonpasttransitions,â„ ğ‘¡:ğ‘¡+ğ‘‡ğ¿ = (sğ‘¡ ,ğ‘¢ ğ‘–,ğ‘¡ ,sğ‘¡+1 ,ğ‘¢ ğ‘–,ğ‘¡+1 ,...,sğ‘¡+ğ‘‡ğ¿ ), b at le ğ‘¡ i â‰ˆ si 2 n 5 i 0 ti , a w lly he i n n , a fo m llo ix w e i d ng eq a u m ili o b d ri e u l m pa . r T a h m e et s e y r m u m pd et a r t y e, i ğ‘– s â€™s b p ro o k li e c n y
via
B ğ‘¢ â€² ,ğ‘› =Bğ‘¢,ğ‘› +
ğ‘¡+
âˆ‘ï¸
ğ‘‡ğ¿ âˆ’1
ğ›¼ ğ‘™ ğ›¿ ğ‘¢,ğ‘¢ğ‘–,ğœ (sğ‘›,ğœ+1 âŠ—sğ‘›,ğœ ) (18)
m
to
o
w
v
a
e
r
s
d
t
s
o
c
w
o
a
o
r
p
d
e
s
r
d
a
e
ti
f
o
ec
n
ti
(
o
in
n
c
,
e
a
n
n
t
d
iv
ğ‘—
is
a
e
p
d
pr
b
o
y
pr
th
ia
e
te
g
ly
am
re
e
sp
p
o
a
n
y
d
o
s
ff
b
s)
y
.T
m
h
o
e
vi
e
n
n
g
-
ğœ=ğ‘¡
sembleremainsintheselecteddcequilibriumfortheremainderof
foreachfactorğ‘›,withlearningrateğ›¼ = 1,andwhere âŠ— isthe theChgame.
ğ‘™
outerproductandsğ‘›,ğ‘¡ sufficientstatisticsforğ‘(ğ‘  ğ‘› )attimeğ‘¡.We Iftheagentsâ€™ğ›½ 1valuesweremuchhigher,theVFEwouldfollow
referthereaderto[15]forfurtherdetails. the green stylised curve in Fig. 3 much more closely. However,
the chosen ğ›½ 1 = 15 value causes some â€˜suboptimalâ€™ actions to
7or,equivalently,1(ğ‘¢Ë†ğ‘– ),theone-hotencodingoftheactionunderconsideration besampledfromğ‘(ğ‘¢Ë†)occasionally,sosomeobservationsdeviate
cd dd
cd
dd
mixed cc/dd cd/dc
cc
dc
dc cc
mixed equilibrium status role strategic equilibrium status
equilibrium selection quo reversal confusion selection quo
Figure 2: Dynamics of a game transition with two agents
(ğ›½ 1=15).500stepsofChfollowedby500stepsofSHwitha10-
steptransition.IntheEFEplots,bluerepresentsâ€˜cooperateâ€™,
andpinkrepresentsâ€˜defectâ€™.Inthepolicyheatmapplots,a
lightercolourindicatesahigherprobability.
fromtheâ€˜statusquoâ€™(Fig.3,green)inone(orange)orboth(red)
observationmodalities,showingdifferentlevelsofsurprise(VFE)
each8.
Afterthegametransition,thereisarolereversal:thecooperating
agent becomes a defector, and vice versa. This is explained by
theEFE,ğº[ğ‘¢Ë†](Fig.2,secondrow),whereğ‘–â€™s(negative)pragmatic
valueofcooperating(âˆ’ğœŒ[c],blue)dropsdramatically,surpassing
the pragmatic value of defecting (âˆ’ğœŒ[d], pink). This is because
the preferencesğ‘âˆ— have now changed, and because ğ‘— has been
cooperatinguptothatpointâ€”i.e.,ğ‘–believesğ‘—isacooperator,thathe
canbetrustedduetohisreputation,makingitmoreappealingforğ‘–
tocooperateaswell.However,theobverseisalsoatplayforğ‘—,which
transitionstodefecting.Wenotethatthisrolereversalhappens
regardlessofthetransitionduration,ğ‘‡ x.Withthetransition,there
isasuddenincreaseintheepistemicvalue(salienceğœ andnovelty
ğœ‚)ofactions,leadingtoexploratorybehaviour.
8TheVFEisadditiveoverthefactorsofthegenerativemodel
EFV
noitisnart
emag
Figure3:StylisedboundsonthedynamicsoftheVFE.
7.5
5.0
2.5
0.0
ğ”Š
SH2 SHg
Ch SH Ch SHg
7.5
5.0
2.5
0.0
ğ”Š
SHr SHp
Ch SHr Ch SHr
7.5
5.0
2.5
0.0
0 200 400
ğ‘¡
600 800 1000
ğ”Š
SHgSHr ğ‘¡=1000
Ch SHg SHr
SH2 SHg SHr SHp SHgSHr
Figure4:Theensemble-levelexpectedEFE,ğ”Š,highlights(the
relativesizeofthebasinofattractionof)theequilibriaof
agame(ğ›½ 1 = 30).Thebottom-rightplotshowsthekernel
densityestimate(Gaussiankernel,0.08bandwidth)ofthe
PDFoffinalvaluesundereachcondition.
Astheagentspersistwiththeirnewstrategies,thepragmatic
valueofeachactionchangestoreflectthem.Theabsolutedifference
(cid:12) (cid:12) ğœŒ ğ‘— [c] âˆ’ğœŒ ğ‘— [d] (cid:12) (cid:12)diminishes,increasingtheentropyofthepolicy,
ğ‘(ğ‘¢Ë†ğ‘— ),uptoapointofmaximalâ€˜strategicconfusionâ€™,wheretheVFE
isapproximatelythesameforanypossibleobservation(everything
isjustassurprisingasanythingelse).Thisconfusionisresolvedby
equilibriumselection,withasmallspikeinnoveltyfollowedbya
decreaseinsalienceasthetwoagentsâ€™policiesconvergetotheir
finalvalues.
Inthisparticulartrial,theensemblearrivesatapayoff-dominant
equilibriumwithpredominantlyccstrategies(modulatedbythe
rationalityoftheagents).Butthismaynotalwaysbethecase,as
weshownext.
4.2 EFEandequilibriumselection
Theensemble-levelexpectedEFE,ğ”Š=(cid:205) âŸ¨GâŸ©(ğ‘–),closelyrelatedto
ğ‘–
thejointobjectiverecentlyproposedin[47],providesacompact
measureofthestateoftheensembleovertime.Byrunningseveral
trialsofanINFGwecanusethisstatistictocharacterisethedifferent
equilibriaofthegame,aswellas(anapproximationof)therelative
sizeoftheirbasinofattraction.
InFigure4,weshowvaluesofğ”Šunderdifferentexperimental viapenalisingcertainbehaviour(cf.mechanismdesign),withmild
conditions.Thedatawereobtainedbyrunning50trialsofacho- success.Ontheotherhand,includinganinterimtransitionthrough
sen(sequenceof)INFGforeachcondition,whichintheplotsare SHg generatestrustandthusâ€˜bootstrapsâ€™cooperativebehaviour,
shownsuperimposed(foragivencondition).Datapointsthatare stewardingthecollective[5]towardsthePDEwithoutneedingto
superimposedontheplotsappeardarker,highlightingtherelative resorttopenalties.
numberoftrialsthattakesimilarvalues.ThechosenINFGareagain
ChfollowedbySH,withatwo-playercondition(SH2;asinFig.2),
5 Conclusion
andthefollowingthree-playerconditionsusingthreevariantsof
theSHgame: WehaveproposedafactorisationofthegenerativemodelofAIF
â€¢ aâ€˜greenâ€™variant(SHg)whereonlytwoagentsarerequired agentsthatbringstheframeworkincloseralignmentwithgame
inordertosuccessfullyhuntastag, theory, particularly for multi-agent interactions. In this factori-
â€¢ aâ€˜redâ€™variant(SHr)whereallthreeagentsarerequired,and sation,eachagenthasexplicit,individual-levelbeliefsaboutthe
â€¢ aâ€˜penaltyâ€™variant(SHp),whereallthreeagentsarerequired internalstatesofothers,andusesthemtoplanstrategicallyina
andthetemptationtodefectislowered(bysettingğ‘‡ =ğ‘ƒ). joint(game-theoretic)context.Thisallowsegotoflexiblytrack
theinternalstatesofothersoutsidethejointinteractioncontext,
Theirrespectivepayoffmatricesare:
andincorporatethatinformationasrequiredbytheinteraction.
(cid:20)(cid:20)ğ‘… ğ‘…(cid:21) (cid:20)ğ‘‡ ğ‘ƒ(cid:21)(cid:21) (cid:20)(cid:20)ğ‘… ğ‘†(cid:21) (cid:20)ğ‘‡ ğ‘ƒ(cid:21)(cid:21)
Thiswouldbeparticularlyusefulwhentheagentsinvolvedina
SHg=
ğ‘… ğ‘†
,
ğ‘ƒ ğ‘ƒ
; SHr=
ğ‘† ğ‘†
,
ğ‘ƒ ğ‘ƒ
;
giveninteractionchangeoriftheagentparticipatesinmultiple
interactionsatagiventime(cf.networkgames).
(cid:20)(cid:20)ğ‘… ğ‘†(cid:21) (cid:20)ğ‘ƒ ğ‘ƒ(cid:21)(cid:21)
SHp=
ğ‘† ğ‘†
,
ğ‘ƒ ğ‘ƒ
Wehaveappliedourproposedmodeltotwo-andthree-agent
INFGandincludedtransitionsbetweengamesthattheagentshave
such that e.g. g SHg (c,d,c) = ğ‘…, but g SHr (c,d,c) = ğ‘†, withğ‘… > toadaptto.WehaveshownhowtheVFEandEFEcanbeusedto
ğ‘‡ > ğ‘ƒ > ğ‘† assignedtheintegersfrom1to4.Thepayoffmatrix analysethedynamicsofensemblesofagentsinteractingstrategi-
for the three-player Ch has the same form as SHr, except with cally[7]â€”inparticular,tohighlighttheequilibriaofgamesandthe
ğ‘‡ >ğ‘… >ğ‘† >ğ‘ƒ(asperÂ§2.1).Afifthandfinalexperimentalcondition relativesizeoftheirattractorsâ€”,andprovidedanexamplewhere
isincluded,withanadditionaltransitionfromSHgtoSHrpost-Ch. thisisusedtomotivateaninterventionleadingtheensembletoa
Thefivetime-seriesplots(oneforeachcondition)inFig.4show betteroutcomebasedontrust,ratherthanpunishment.Theuse
superimposedseriesfor50repeats.Thebottom-rightplotshowsa of these measures may help in the conceptualization of groups
side-by-sidecomparisonofthevaluesofğ”Šatğ‘¡ =1000undereach ofagentsascollectiveagentswithself-organizingdynamicsand
condition. operationalclosure[47,55,79].
WefirstlookattheChperiod,wheretheensemblestartsata AIF and game theory applied to multi-agent systems offer a
mixed equilibrium (ğ‘¡ = 0) and reaches a pure equilibrium (ğ‘¡ â‰¤ richtheoreticalandexperimentallandscapeforexploringadaptive
500).Inthetwo-agentcase,theensembletendstowardoneofcd behavioursinintelligentagentinteractions.Thisintersectionof
ordcfairlyquickly,althoughinoneofthetrialsitstaysinthe cognitivescienceandartificialintelligencenotonlyprovidesin-
mixedequilibriumthroughtotheend.Therearenounderlying sightsintoindividualdecision-makingbutalsopavesthewayfor
configurationchangesforChbetweenthethree-agentcases,soany understandingthecollectivedynamicsthatshapesocialbehaviour
differencesarecausedbystochasticity.InthefinalChequilibrium, incomplexenvironments.
invariably,oneoftheagentsdefectsandtherestcooperate(up Egoâ€™sbeliefsaboutherownpolicyğ‘(ğ‘¢Ë†ğ‘– )reflectaformofintro-
tosymmetry).Sinceğ”Šisadditive,itdecreasesforthetwo-player spection:inferringoneâ€™sinternalmentalstatesbyobservingoneâ€™s
condition, as only one of two agents has to choose the â€˜worseâ€™ actions[43,85].Thisiscontrastedwithinteroception:thepercep-
action,comparedtotheincreaseunderthethree-playerconditions, tionofinternalbodilysensations,whichwouldprovidedirectaccess
wheretwo-thirdsoftheensembleselecttheâ€˜worseâ€™action.Thisis tointernalstateslikeğ‘(ğ‘¢Ë†ğ‘– ).Byendowingegowithinteroceptiveac-
aninstancewheretheNashEquilibriumisnotsociallyoptimal. cesstoğ‘(ğ‘¢Ë†ğ‘– ),onecouldbypasstheneedforfurtherinferenceabout
ThefinalChequilibriumsetsthepriorconditionsforthesub- herinternalstateinfuturestepsofthemodel.Futureworkshall
sequentSHgame,amountingtoaâ€˜pre-equilibriumâ€™[94,p.3].The explorehowlearningtheobservationmodelcouldcapturethera-
SHgamehasarisk-dominantequilibrium(RDE)withahigherğ”Š tionalityofanopponent;thepotentialformorecomplextransition
(i.e.worseoverall),andapayoff-dominantequilibrium(PDE)with modelsconditionedontheactionsofallagents,orformodelling
alowerğ”Š(i.e.betteroverall).IntheSH2 andSHp conditions,we otherhiddenvariablessuchasopponentpreferences[14,84];orthe
seeabifurcationoccurwhereaportionofthetrialsendsineach effectsofdifferentEFEformulations[13,40,64]ongameoutcomes.
equilibrium,withthemajorityofSH2(resp.SHp)endinginthePDE
(resp.RDE).Thisshowstherelativesizeoftheirbasinsofattraction
(notingthatSH2mayneedbeyondğ‘¡ =1000toconvergefurther). Acknowledgments
AlltheSHgtrialsendinthePDE;allofSHr,intheRDE.Thisis JRSissupportedbyanAustralianGovernmentResearchTraining
somewhatparadoxical:whentwoarerequiredtocooperate(SHg),all Program(RTP)Scholarship.WeacknowledgetheGadipeopleof
threecooperate;conversely,whenthreearerequiredtocooperate theEoranationasthetraditionalcustodiansofthelandonwhich
(SHr),nonecooperates.Interestingly,theSHpvariantcouldbeseen TheUniversityofSydneynowstands,andthattheirsovereignty
asanattempttodirecttheensembletowardsabetterequilibrium wasneverceded.
References
//doi.org/10.1016/j.neubiorev.2023.105500
[26] KarlJFriston,FrancescoRigoli,DimitriOgnibene,ChristophMathys,Thomas
[1] StefanoVAlbrecht,JacobWCrandall,andSubramanianRamamoorthy.2016.
Beliefandtruthinhypothesisedbehaviours. ArtificialIntelligence235(2016), F C i o t g z n g i e t r iv a e ld N , e a u n r d os G ci i e o n v c a e n 6 n , i 4 P (O ez c z t. u 2 l 0 o 1 . 5 2 ) 0 , 1 1 5 8 . 7â€“ A 2 c 1 t 4 iv . e h I t n tp fe s r :/ e / n d c o e i.o a r n g d /1 E 0 p .1 i 0 st 8 e 0 m /1 i 7 c 5 V 88 a 9 lu 28 e. .
63â€“94.
2015.1020053
[2] StefanoVAlbrechtandSubramanianRamamoorthy.2015. Agame-theoretic [27] DrewFudenbergandDavidKLevine.1993.Self-confirmingequilibrium.Econo-
m
sy
o
s
d
te
e
m
la
s
n
.
d
ar
b
X
e
i
s
v
t-
p
r
r
e
e
s
p
p
r
o
i
n
n
s
t
e
ar
le
X
a
i
r
v
n
:1
in
50
g
6
m
.0
e
1
t
1
h
7
o
0
d
(2
fo
01
r
5
a
)
d
.
hoccoordinationinmultiagent metrica:JournaloftheEconometricSociety(1993),523â€“545.
[28] DrewFudenbergandDavidKLevine.1995.Consistencyandcautiousfictitious
[3] S ag te e f n a t n s o :A V c A o l m br p e r c e h h t e a n n s d iv P e e s t u er rv S e t y on a e n . d 20 o 1 p 8 e . n A p u r t o o b n l o em mo s. u A s r a t g ifi en ci t a s l m In o t d el e l l i l g in en g c o e t 2 h 5 e 8 r [29] p D l r a e y w .J F o u u d rn e a n l b o e f rg Ec a o n n d om D i a c v D id yn K a . m L i e c v s i a n n e d .1 C 9 o 9 n 8 t . ro T l h 1 e 9, th 5- e 7 or ( y 19 o 9 f 5 l ) e , a 1 r 0 n 6 i 5 n â€“ g 1 i 0 n 89 g . ames.
(2018),66â€“95.
Vol.2.MITPress.
[4] RobertAxelrodandWilliamDHamilton.1981.Theevolutionofcooperation.
Science211,4489(1981),1390â€“1396. [30] D
Ga
re
m
w
es
F
a
u
n
d
d
en
E
b
c
e
o
r
n
g
om
an
ic
d
B
D
e
a
h
v
a
i
v
d
io
K
r
L
29
e
,
v
1
in
-2
e.
(
1
19
9
9
9
9
9
)
.
,
C
10
o
4
n
â€“
d
1
it
3
i
0
o
.
naluniversalconsistency.
[5] Joseph B Bak-Coleman, Mark Alfano, Wolfram Barfuss, Carl T Bergstrom, [31] DrewFudenbergandJeanTirole.1991.Repeatedgames.InGameTheory.MIT
MiguelACenteno,IainDCouzin,JonathanFDonges,MirtaGalesic,AndrewS
Press,Cambridge,Massachusetts,150â€“192.
Gersick,JenniferJacquet,AlbertBKao,RachelEMoran,PawelRomanczuk,
[32] MohammadGhavamzadeh,ShieMannor,JoellePineau,AvivTamar,etal.2015.
D S Sc t a e ie n w n i a c e e r l d s I s 1 R h 1 i u 8 p , b 2 o e 7 f n G ( s J t u l e o l i b y n a 2 , l 0 K C 2 a o 1 i l ) a l , e e J c 2 t T 0 iv o 2 e 5 m 7 B b 6 e a 4 h 1 k a 1 , v 8 J i a . o y h r. t J P tp r V o s a : c / n e / e d B d o i a i n . v o g e r s l g , o / f a 1 t n 0 h . d 1 e 0 E N 7 l a 3 k t / e i p o n U n a a s W l . A 2 e 0 c b 2 a e 5 d r 7 e . 6 m 2 4 0 y 1 2 1 o 1 8 f . B Le a a y r e n s i i n a g n 8 re , i 5 n - f 6 or (2 c 0 em 15 e ) n ,3 t 5 le 9 a â€“ r 4 n 8 i 3 n . g:asurvey.FoundationsandTrendsÂ®inMachine
[33] HerbertGintis.2006. TheFoundationsofBehavior:TheBeliefs,Preferences,
[6] C M h o r d is el B in ak g e j r o , i R n e t b b e e c l c ie a f S -d ax es e i , r a e n a d t J t o ri s b h u u t a io T n e . n I e n n P b r a o u c m ee . d 2 i 0 n 1 g 1 s . o B f a t y h e e si A a n n n t u h a e l or M y e o e f ti m ng in o d f : andConstraintsModel. BiologicalTheory1,2(June2006),123â€“127. https:
theCognitiveScienceSociety,Vol.33. //doi.org/10.1162/biot.2006.1.2.123
[34] PiotrJGmytrasiewiczandPrashantDoshi.2004.InteractivePOMDPs:Proper-
[7] W of o M lf u ra lt m i-A B g a e r n fu t s L s e . a 2 r 0 n 2 i 2 n . g. D N y e n u a r m al ic C a o l m Sy p s u t t e i m ng s a a n s d a A L p e p v l e ic l a o t f io C ns og 3 n 4, it 3 iv ( e Fe A b n . a 2 l 0 y 2 s 2 i ) s , t P i r e o s ce a e n d d in p g r s e o li f m th in e a T r h y ir r d es I u n l t t e s r . n I a n ti I o n n t a er l n J a o t in io t n C a o l n C f o er n e f n e c re e n o c n e A on ut A on u o t m on o o u m s o A u g s e A nt g s e a n n ts d :
[8]
1
Jo
6
s
5
Ã©
3â€“
M
16
B
7
e
1
r
.
na
h
r
t
d
tp
o
s
a
:/
n
/d
d
o
A
i.o
d
r
r
g
ia
/
n
10
F
.1
M
00
S
7
m
/s0
it
0
h
5
.
2
2
1
0
-
0
0
9
2
.
1-
B
0
a
6
y
1
e
1
s
7
ia
-0
ntheory.Vol.405. John
MultiagentSystems,Vol.3.1374â€“1375.
[35] PiotrJGmytrasiewiczandPrashantDoshi.2005. Aframeworkforsequential
[9] W Ge i o le r y ge & W So B n r s o . wn.1951.IterativeSolutionofGamesbyFictitiousPlay.InActivity planninginmulti-agentsettings. JournalofArtificialIntelligenceResearch24
AnalysisofProductionandAllocation,TjallingC.Koopmans(Ed.).JohnWiley& (2005),49â€“79.
[36] PiotrJGmytrasiewiczandEdmundHDurfee.2000. Rationalcoordinationin
[10]
S
C
o
o
n
li
s
n
,
F
N
C
ew
am
Y
e
o
r
r
e
k
r.
,
2
3
0
7
1
4
1
â€“
.
3
B
7
e
6
h
.
avioralgametheory:Experimentsinstrategicinteraction.
multi-agentenvironments.AutonomousAgentsandMulti-AgentSystems3(2000),
319â€“350.
PrincetonUniversityPress.
[37] SebastianGottwaldandDanielABraun.2020.TheTwoKindsofFreeEnergy
[11] D ba a s v e i d d le C a a r r n m in e g l i a n n m d u S l h ti a -a u g l e M nt a s r y k s o t v e i m tc s h :E .1 x 9 p 9 lo 9 r . at E io x n pl s o t r r a at t e io g n ies s . tr A a u te to g n ie o s m f o o u r s m Ag o e d n e t l s - andtheBayesianRevolution. PLOSComputationalBiology16,12(Dec.2020),
andMulti-agentsystems2(1999),141â€“172. e1008420. https://doi.org/10.1371/journal.pcbi.1008420
[38] PeterDGrÃ¼nwaldandAPhilipDawid.2004.Gametheory,maximumentropy,
[12] G
re
e
in
o
f
r
o
g
r
i
c
o
e
s
m
C
e
h
n
a
t
l
l
k
e
i
a
a
r
d
n
a
in
k
g
is
:A
an
B
d
ay
C
e
r
s
a
i
i
a
g
n
B
ap
o
p
u
r
t
o
il
a
ie
c
r
h
.
.
2
In
00
P
3
r
.
oc
C
ee
o
d
o
i
r
n
d
g
i
s
n
o
a
f
ti
t
o
h
n
eS
i
e
n
co
m
n
u
d
l
I
t
n
ia
te
g
r
e
n
n
a
t
-
minimumdiscrepancyandrobustBayesiandecisiontheory.theAnnalsofStatistics
tionalJointConferenceonAutonomousAgentsandMultiagentSystems.709â€“716. 32,4(2004),1367â€“1433.
[39] ArthurGuez,DavidSilver,andPeterDayan.2012.EfficientBayes-adaptiverein-
[13] ThÃ©ophileChampion,HowardBowman,DimitrijeMarkoviÄ‡,andMarekGrzeÅ›. forcementlearningusingsample-basedsearch.AdvancesinNeuralInformation
2024.ReframingtheExpectedFreeEnergy:FourFormulationsandaUnification. ProcessingSystems25(2012).
arXiv:2402.14460[cs]
[40] DanijarHafner,PedroAOrtega,JimmyBa,ThomasParr,KarlJFriston,and
[14] AlexJChanandMSchaar.2021. ScalableBayesianInverseReinforcement
Learning.InICLR. NicolasHeess.2022.ActionandPerceptionasDivergenceMinimization. https:
//doi.org/10.48550/arXiv.2009.01791arXiv:2009.01791[cs,math,stat]
[15] LancelotDaCosta,ThomasParr,NoorSajid,SebastijanVeselic,VictoritaNeacsu, [41] JamesHannan.1957.ApproximationtoBayesRiskinRepeatedPlay.Contributions
a
Jo
n
u
d
r
K
na
a
l
rl
o
J
f
F
M
ri
a
s
t
t
h
o
e
n
m
.2
a
0
ti
2
c
0
a
.
l
A
Ps
c
y
ti
c
v
h
e
o
I
lo
n
g
fe
y
re
9
n
9
c
(
e
D
o
e
n
c.
D
2
i
0
s
2
c
0
re
),
te
10
S
2
t
4
a
4
te
7
-
.
Sp
h
a
t
c
t
e
p
s
s
:
:/
A
/d
S
o
y
i.
n
o
t
r
h
g
e
/
s
1
i
0
s
.
. totheTheoryofGames3(1957),97â€“139.
[42] MichaelSHarrÃ©.2018.Multi-AgentEconomicsandtheEmergenceofCritical
1016/j.jmp.2020.102447
Markets. arXiv:1809.01332[nlin,q-fin]
[16] EddieDekel,DrewFudenberg,andDavidK.Levine.2004. Learningtoplay
Bayesiangames.GamesandEconomicBehavior46,2(2004),282â€“303. [43] M
M
i
i
c
n
h
d
a
â€™?
el
G
S
a
H
m
a
e
r
s
rÃ©
1
.
3
2
,
0
3
2
(
2
Ju
.
n
W
e
h
2
a
0
t
2
C
2)
a
,
n
46
G
.
am
ht
e
tp
T
s
h
:/
e
/
o
d
r
o
y
i.o
T
r
e
g
ll
/1
U
0
s
.3
a
3
b
9
o
0
u
/g
t
1
a
3
n
03
A
0
I
0
â€˜
4
T
6
heoryof
[17] DaphneDemekas,ConorHeins,andBrennanKlein.2024.AnAnalyticalModel
ofActiveInferenceintheIteratedPrisonerâ€™sDilemma.InActiveInference,Christo- [44] J
p
o
la
h
y
n
e
C
rs,
H
I
a
â€“
r
I
s
II
an
P
y
a
i
r
.
t
1
I
9
.
6
T
7
h
.
e
G
b
a
a
m
si
e
c
s
m
w
o
it
d
h
e
i
l.
nc
M
o
a
m
n
p
a
l
g
e
e
t
m
ei
e
n
n
f
t
o
s
r
c
m
ie
a
n
t
c
io
e
n
14
p
,
la
3
y
(
e
1
d
96
b
7
y
),
â€œB
15
a
9
y
â€“
es
1
i
8
a
2
n
.
â€
pherL.Buckley,DanielaCialfi,PabloLanillos,MaxwellRamstead,NoorSajid,
[45] PabloHernandez-Leal,MichaelKaisers,TimBaarslag,andEnriqueMunoz
HideakiShimazaki,TimVerbelen,andMartijnWisse(Eds.).SpringerNature
DeCote.2017.Asurveyoflearninginmultiagentenvironments:Dealingwith
Switzerland,Cham,145â€“172. https://doi.org/10.1007/978-3-031-47958-8_10 non-stationarity.arXivpreprintarXiv:1707.09183(2017).
[18] PrashantDoshi,PiotrGmytrasiewicz,andEdmundDurfee.2020. Recursively
modelingotheragentsfordecisionmaking:Aresearchperspective. Artificial [46] TrongNghiaHoangandKianHsiangLow.2013.Ageneralframeworkforinter-
[19] I M nt i e c l h li a g e e l n O ce â€™G 2 o 79 rd ( o 2 n 02 D 0) u , ff 1 . 0 2 3 0 2 0 0 2 2 . . OptimalLearning:Computationalproceduresfor a m c o ti d n e g la B n a d ye m s- o o d p e t l im pr a io ll r y .a w r i X th iv s p e r l e f- p i r n i t n e t r a e r s X te i d v:1 a 3 g 0 e 4 n .2 ts 02 u 4 si ( n 2 g 01 a 3 r ) b . itraryparametric
Bayes-adaptiveMarkovdecisionprocesses.UniversityofMassachusettsAmherst. [47] DavidHyland,TomÃ¡Å¡GavenÄiak,LancelotDaCosta,ConorHeins,VojtechKo-
varik,JulianGutierrez,MichaelJ.Wooldridge,andJanKulveit.2024.Free-Energy
[20] MatthewFellows,AnujMahajan,TimGJRudner,andShimonWhiteson.2019.
V in IR N E e L u : ra A l v In a f r o ia rm tio a n ti a o l n in P f r e o r c e e n ss c i e n f g ra S m ys e t w em o s rk 32 fo ( r 2 r 0 e 1 i 9 n ) f . orcementlearning.Advances [48] E I E n q d u w IC i i l M n ib L T ria 2 J : a 0 T y 2 o 4 n w e W s a . o r 2 r d 0 k a 0 sh 3 T . o h p P e r o o o n r b y a M b o o i f l d i I t e n y l t s e th o ra f e c o H t r i u y o m : n T s a h B n e e F l t o e w g e e d ic e b n o a f c B k s o c f i u o e n n r d c A e e . I d A C ly a l - i m R gn a b t m r i i o e d n n g a t e . l U A n ge iv n e t r s - .
[21] ChrisFieldsandJamesF.Glazebrook.2024.NashEquilibriaandUndecidability
inGenericPhysicalInteractionsâ€”AFreeEnergyPerspective.Games15,5(Oct. sityPress.
[49] AlbertJiangandKevinLeyton-Brown.2010. Bayesianaction-graphgames.
2024),30. https://doi.org/10.3390/g15050030 AdvancesinNeuralInformationProcessingSystems23(2010).
[22] IsmaelT.Freire,X.Arsiwalla,J.PuigbÃ²,andP.Verschure.2019.ModelingTheory
ofMindinMulti-AgentGamesUsingAdaptiveFeedbackControl.ArXiv(2019). [50] A
ga
lb
m
e
e
r
s
t
.
X
G
in
am
Ji
e
a
s
n
a
g
n
,K
d
e
E
v
c
i
o
n
n
L
om
ey
i
t
c
o
B
n
e
-
h
B
a
ro
v
w
ior
n,
7
a
1
n
,
d
1
N
(2
a
0
v
1
i
1
n
),
A
1
R
41
B
â€“
h
1
a
7
t
3
.
.
2011.Action-graph
[23] K G Bi a i o o r b l v e a J h n a F n v r i i i o s P t r o e a z n l z R , u e T l v o h i , e o w e m t s a a 6 s l 8 . F 2 ( i 2 0 t 0 z 1 G 1 6 6 . e ) r , A a 8 c l 6 d t 2 i , v â€“ F e 8 r 7 a in 9 n . f c e e r s e c n o ce R a ig n o d li l , e P a h rn il i i n p g p . S N ch eu w r a os r c te ie n n b c e e c & k, [51] J E a c m on e o s m S ic Jo B r e d h a a n v . io 1 r 99 3 1 , . 1( B 1 a 9 y 9 e 1 s ), ia 6 n 0â€“ le 8 a 1 r . ninginnormalformgames. Gamesand
[52] EhudKalaiandEhudLehrer.1993.RationallearningleadstoNashequilibrium.
[24] KarlJFriston,ConorHeins,TimVerbelen,LancelotDaCosta,TommasoSalvatori, Econometrica:JournaloftheEconometricSociety(1993),1019â€“1045.
DimitrijeMarkovic,AlexanderTschantz,MagnusKoudahl,ChristopherBuckley,
[53] HilbertJKappen,VicenÃ§GÃ³mez,andManfredOpper.2012.Optimalcontrolasa
andThomasParr.2024. FromPixelstoPlanning:Scale-FreeActiveInference. graphicalmodelinferenceproblem.Machinelearning87(2012),159â€“182.
https://doi.org/10.48550/arXiv.2407.20292arXiv:2407.20292[cs,q-bio]
[54] MichaelKearns,MichaelLLittman,andSatinderSingh.2013.Graphicalmodels
[25] KarlJFriston,ThomasParr,ConorHeins,AxelConstant,DanielFriedman, forgametheory.arXivpreprintarXiv:1301.2281(2013).
TakuyaIsomura,ChrisFields,TimVerbelen,MaxwellRamstead,JohnClip-
[55] JulianKiverstein,MichaelDKirchhoff,andTomFroese.2022.TheProblemof
p in in g. ger N , e a u n r d os C ci h e r n i c s e to & ph B er io D be . h F a r v it i h or . a 2 l 02 R 4 e . vie F w ed s e 1 r 5 at 6 ed (Ja In n f . e 2 r 0 en 24 c ) e , a 1 n 0 d 55 B 0 e 0 l . ief h S t h tp ar s - : M rob e o a t n ic in s g 1 : 6 T ( h Ju e n F e re 2 e 02 E 2 n ). erg h y ttp P s r : i / n /d ci o p i. l o e r a g n /1 d 0 A .3 r 3 t 8 ifi 9/ c f i n a b l o A t g .2 e 0 n 2 c 2 y .8 . 4 F 4 r 7 o 7 n 3 tiersinNeuro-
[56] DaphneKollerandBrianMilch.2001.Structuredmodelsformulti-agentinterac- [77] ThomasParr,GiovanniPezzulo,andKarlJFriston.2022.ActiveInference:The
tions.InProceedingsofthe8thConferenceonTheoreticalAspectsofRationality FreeEnergyPrincipleinMind,Brain,andBehavior. TheMITPress. https:
andKnowledge.233â€“248. //doi.org/10.7551/mitpress/12441.001.0001
[57] DaphneKollerandBrianMilch.2003.Multi-agentinfluencediagramsforrepre- [78] JudeaPearl.2014. ProbabilisticReasoninginIntelligentSystems:Networksof
sentingandsolvinggames.GamesandEconomicBehavior45,1(2003),181â€“221. PlausibleInference.Elsevier.
[58] PierfrancescoLaMuraandYoavShoham.2013.Expectedutilitynetworks.arXiv [79] MaxwellJDRamstead,MichaelDKirchhoff,AxelConstant,andKarlJFriston.
preprintarXiv:1301.6714(2013). 2021.MultiscaleIntegration:BeyondInternalismandExternalism.Synthese198,
[59] YannLeCun,SumitChopra,RaiaHadsell,MarcAurelioRanzato,andFuJie 1(Jan.2021),41â€“70. https://doi.org/10.1007/s11229-019-02115-x
Huang.2006.ATutorialonEnergy-BasedLearning.InPredictingStructuredData, [80] IeadRezek,DavidSLeslie,StevenReece,StephenJRoberts,AlexRogers,Ra-
G.Bakir,T.Hofman,B.Scholkopt,A.Smola,andB.Taskar(Eds.).MITPress. jdeepKDash,andNicholasRJennings.2008.OnSimilaritiesbetweenInference
[60] SergeyLevine.2018.Reinforcementlearningandcontrolasprobabilisticinfer- inGameTheoryandMachineLearning.JournalofArtificialIntelligenceResearch
ence:Tutorialandreview.arXivpreprintarXiv:1805.00909(2018). 33(Oct.2008),259â€“283. https://doi.org/10.1613/jair.2523
[61] TaoLi,YuhanZhao,andQuanyanZhu.2022.Theroleofinformationstructures [81] MarcRigter,BrunoLacerda,andNickHawes.2021.Risk-averseBayes-adaptive
ingame-theoreticmulti-agentlearning. AnnualReviewsinControl53(2022), reinforcementlearning.AdvancesinNeuralInformationProcessingSystems34
296â€“314. (2021),1142â€“1154.
[62] MarlizeLombardandPeterGÃ¤rdenfors.2023. Causalcognitionandtheory [82] JuliaRobinson.1951.Aniterativemethodofsolvingagame.AnnalsofMathe-
ofmindinevolutionarycognitivearchaeology. BiologicalTheory18,4(2023), matics54,2(1951),296â€“301.
234â€“252. [83] StephaneRoss,BrahimChaib-draa,andJoellePineau.2007. Bayes-adaptive
[63] RichardDMcKelveyandThomasRPalfrey.1995.Quantalresponseequilibria POMDPs.AdvancesinNeuralInformationProcessingSystems20(2007).
fornormalformgames.GamesandEconomicBehavior10,1(1995),6â€“38. [84] JaimeRuiz-SerraandMichaelSHarrÃ©.2023.InverseReinforcementLearningas
[64] BerenMillidge,AlexanderTschantz,andChristopherL.Buckley.2021.Whence theAlgorithmicBasisforTheoryofMind:CurrentMethodsandOpenProblems.
theExpectedFreeEnergy?NeuralComputation33,2(Feb.2021),447â€“482. https: Algorithms16,2(Feb.2023),68. https://doi.org/10.3390/a16020068
//doi.org/10.1162/neco_a_01354 [85] LarsSandved-Smith,CasperHesp,JÃ©rÃ©mieMattout,KarlJFriston,AntoineLutz,
[65] MichaelMoutoussis,NelsonTrujillo-Barreto,WaelEl-Deredy,RaymondDolan, andMaxwellJDRamstead.2021.TowardsaComputationalPhenomenologyof
andKarlJFriston.2014.AFormalModelofInterpersonalInference.Frontiersin MentalAction:ModellingMeta-AwarenessandAttentionalControlwithDeep
HumanNeuroscience8(2014). ParametricActiveInference.NeuroscienceofConsciousness2021,1(Jan.2021),
[66] JohnHNachbar.2005. Beliefsinrepeatedgames. Econometrica73,2(2005), niab018. https://doi.org/10.1093/nc/niab018
459â€“480. [86] LeonardJ.Savage.1954.Thefoundationsofstatistics.Wiley,NewYork.
[67] BrendaNg,KofiBoakye,CarolMeyers,andAndrewWang.2012.Bayes-adaptive [87] JeffSShammaandGÃ¼rdalArslan.2005.Dynamicfictitiousplay,dynamicgradient
interactivePOMDPs.InProceedingsoftheAAAIConferenceonArtificialIntelli- play,anddistributedconvergencetoNashequilibria.IEEETrans.Automat.Control
gence,Vol.26.1408â€“1414. 50,3(2005),312â€“327.
[68] YawNyarko.1994. Bayesianlearningleadstocorrelatedequilibriainnormal [88] YoavShoham,RobPowers,andTrondGrenager.2007.IfMulti-AgentLearning
formgames.EconomicTheory4(1994),821â€“841. IstheAnswer,WhatIstheQuestion? ArtificialIntelligence171,7(May2007),
[69] BrendanOâ€™Donoghue.2021.VariationalBayesianreinforcementlearningwith 365â€“377. https://doi.org/10.1016/j.artint.2006.02.006
regretbounds. AdvancesinNeuralInformationProcessingSystems34(2021), [89] RyanSmith,KarlJFriston,andChristopherJWhyte.2022. AStep-by-Step
28208â€“28221.
TutorialonActiveInferenceandItsApplicationtoEmpiricalData.Journalof
[70] BrendanOâ€™Donoghue,IanOsband,andCatalinIonescu.2020.Makingsenseofre- MathematicalPsychology107(April2022),102632. https://doi.org/10.1016/j.jmp.
inforcementlearningandprobabilisticinference.arXivpreprintarXiv:2001.00805 2021.102632
(2020). [90] FinneganSouthey,MichaelPBowling,BryceLarson,CarmeloPiccione,Neil
[71] DanielAOrtegaandPedroABraun.2011. Information,utilityandbounded Burch,DarseBillings,andChrisRayner.2012.Bayesâ€™bluff:Opponentmodelling
rationality.InArtificialGeneralIntelligence:4thInternationalConference,AGI inpoker.arXivpreprintarXiv:1207.1411(2012).
2011,MountainView,CA,USA,August3-6,2011.Proceedings4.Springer,269â€“274. [91] DavidHWolpert.2006.InformationTheoryâ€”TheBridgeConnectingBounded
[72] PedroOrtegaandDanielLee.2014.Anadversarialinterpretationofinformation-
RationalGameTheoryandStatisticalPhysics.InComplexEngineeredSystems:
theoreticboundedrationality.InProceedingsoftheAAAIConferenceonArtificial ScienceMeetsTechnology,DanBraha,AliA.Minai,andYaneerBar-Yam(Eds.).
Intelligence,Vol.28. Springer,Berlin,Heidelberg,262â€“290. https://doi.org/10.1007/3-540-32834-3_12
[73] PedroAOrtegaandDanielABraun.2009.AConversionbetweenUtilityand [92] SarahAWu,RoseEWang,JamesAEvans,JoshuaBTenenbaum,DavidC
Information. https://doi.org/10.48550/arXiv.0911.5106arXiv:0911.5106[cs,math] Parkes,andMaxKleiman-Weiner.2021.Toomanycooks:Bayesianinferencefor
[74] PedroAOrtegaandDanielABraun.2013. Thermodynamicsasatheoryof
coordinatingmulti-agentcollaboration.TopicsinCognitiveScience13,2(2021),
decision-makingwithinformation-processingcosts.ProceedingsoftheRoyalSoci- 414â€“432.
etyA:Mathematical,PhysicalandEngineeringSciences469,2153(2013),20120683. [93] WakoYoshida,RayJDolan,andKarlJFriston.2008. GameTheoryofMind.
[75] PedroAOrtega,DanielABraun,JustinDyer,Kee-EungKim,andNaftaliTishby. PLOSComputationalBiology4,12(Dec.2008),e1000254. https://doi.org/10.1371/
2015.Information-TheoreticBoundedRationality. arXiv:1512.06789[cs,math, journal.pcbi.1000254
stat] [94] HPeytonYoung.2004.TheInteractiveLearningProblem.InStrategicLearning
[76] AlessandroPanellaandPiotrGmytrasiewicz.2017.InteractivePOMDPswith andItsLimits,HPeytonYoung(Ed.).OxfordUniversityPress,0. https://doi.org/
finite-statemodelsofotheragents.AutonomousAgentsandMulti-AgentSystems 10.1093/acprof:oso/9780199269181.003.0001
31(2017),861â€“904. [95] HPeytonYoung.2004.Strategiclearninganditslimits.OxfordUniversityPress.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Factorised Active Inference for Strategic Multi-Agent Interactions"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   âŒ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   âœ… GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   âŒ BAD: Repeating the same claim 3+ times with slight variations
   âœ… GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
