### OverviewThis paper investigates intrinsic rewards for exploration without harm from observational noise, based on the Free Energy Principle. The authors propose hidden state curiosity, a mechanism that rewards agents for resolving uncertainty about their environment. They train six types of agents – a baseline, entropy-driven, prediction error curious, hidden state curious, and hybrid agents – to navigate mazes, evaluating the effectiveness of these approaches. The core of their work is to demonstrate that hidden state curiosity can be more resilient to observational noise than prediction error curiosity, aligning with the principles of active inference.### MethodologyThe authors employ a recurrent neural network (VRNN) architecture for their forward model, allowing for temporal dependencies and handling partially observable Markov Decision Processes (POMDPs). The forward model predicts observations based on the agent’s current state and action, and the authors train this model to minimize free energy. The model consists of a recurrent GRU layer, followed by linear layers for both the mean and standard deviation of the predicted observations. The model is trained using the Adam optimizer with a learning rate of0.01. The agents are trained using a batch size of32, and the training is performed for500 epochs. The agents are evaluated by measuring their ability to find the goal in a biased T-maze, double T-maze, and triple T-maze. The authors also implement a delayed actor training scheme, where the actor is trained every two steps, and the critics are trained every step.### ResultsThe authors found that entropy and curiosity improve agent exploration, especially when both are implemented together, as implied by the Free Energy Principle. Specifically, agents trained with hidden state curiosity demonstrated resilience against curiosity traps, which hinder agents with prediction error curiosity. The authors observed that agents trained with prediction error curiosity were attracted to the randomly colored walls in the mazes, showcasing the negative impact of observational noise. The results were quantified by measuring the percentage of agents that successfully found the goal in each maze after500 epochs. The authors found that agents trained with hidden state curiosity had a success rate of85% in the T-maze,78% in the double T-maze, and72% in the triple T-maze. In contrast, agents trained with prediction error curiosity had success rates of60%,55%, and50%, respectively. The authors also found that the agents trained with hidden state curiosity were able to learn the maze layouts more quickly than the other agents.### DiscussionThe authors argue that their findings support the notion that agents can learn to explore their environments by actively seeking to resolve uncertainty about their surroundings. They suggest that this approach is more robust to observational noise than traditional methods that rely on predicting observations based on a fixed model. The authors propose that hidden state curiosity is a promising approach for developing intelligent agents that can operate in complex and uncertain environments. They also suggest that this approach could be applied to a wide range of applications, such as robotics, autonomous driving, and medical diagnosis. The authors conclude that their work provides a valuable contribution to the field of reinforcement learning and active inference.### Further ResearchThe authors suggest several avenues for future research, including investigating the use of hidden state curiosity in more complex environments, exploring the use of different forward model architectures, and developing methods for automatically tuning the hyperparameters of the forward model. They also suggest that further research is needed to understand the relationship between hidden state curiosity and other forms of intrinsic motivation.