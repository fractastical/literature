=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Theory of Mind Using Active Inference: A Framework for Multi-Agent Cooperation
Citation Key: pitliya2025theory
Authors: Riddhi J. Pitliya, Ozan Çatal, Toon Van de Maele

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Key Terms: agent, planning, theory, beliefs, cooperation, inference, framework, active, mind, others

=== FULL PAPER TEXT ===

Theory of Mind Using Active Inference:
A Framework for Multi-Agent Cooperation
Riddhi J. Pitliya1, Ozan C¸atal1, Toon Van de Maele1, Corrado Pezzato1, and
Tim Verbelen1
VERSES, Los Angeles, California, CA 90067, USA
riddhi.jain@verses.ai
Abstract. TheoryofMind(ToM)–theabilitytounderstandthatoth-
ers can have differing knowledge and goals – enables agents to reason
about others’ beliefs while planning their own actions. We present a
novelapproachtomulti-agentcooperationbyimplementingToMwithin
active inference. Unlike previous active inference approaches to multi-
agentcooperation,ourmethodneitherreliesontask-specificsharedgen-
erative models nor requires explicit communication. In our framework,
ToM-equippedagentsmaintaindistinctrepresentationsoftheirownand
others’beliefsandgoals.ToMagentsthenuseanextendedandadapted
version of the sophisticated inference tree-based planning algorithm to
systematically explore joint policy spaces through recursive reasoning.
Weevaluateourapproachthroughcollisionavoidanceandforagingsim-
ulations.ResultssuggestthatToMagentscooperatebettercomparedto
non-ToM counterparts by being able to avoid collisions and reduce re-
dundantefforts.Crucially,ToMagentsaccomplishthisbyinferringoth-
ers’beliefssolelyfromobservablebehaviourandconsideringthemwhen
planning their own actions. Our approach shows potential for general-
isable and scalable multi-agent systems while providing computational
insights into ToM mechanisms.
Keywords: Theory of Mind, Active Inference, Multi-agent Coopera-
tion, Sophisticated Inference, Recursive Planning
1 Introduction
Theory of Mind (ToM) represents one of the most remarkable achievements of
human cognition – the ability to understand that other agents possess minds
with beliefs and goals that may differ from our own [6]. This meta-cognitive
skill enables us to recognise that others may hold false beliefs and maintain
perspectives distinct from our own. For example, when we observe someone
searching for an object that we know has been moved in their absence, we can
anticipate their search behaviour based on where they believe the object to
be, rather than its actual location [1,12]. This fundamental distinction between
reality and belief enables sophisticated forms of cooperation, competition, and
communication. ToM emerges early in human development and underpins our
ability to navigate complex multi-agent environments [11].
5202
peS
4
]IA.sc[
2v10400.8052:viXra
2 Pitliya et al.
While ToM is fundamental to human social cognition, current approaches
to multi-agent cooperation using active inference lack this crucial capability.
Previousactiveinferencemodelsformulti-agentcooperationhavepredominantly
relieduponassumptionsofsharedoridenticalgenerativemodelsthatlimittheir
generalisability and practical application. We propose that a better solution to
conduct and model multi-agent cooperation is by implementing ToM within the
planningstageofactiveinference.Thisoffersamoreprincipledandgeneralisable
solutionformulti-agentartificialintelligentsystems,andacomputationalmodel
thatcouldserveasatooltodeepenourunderstandingofhowhumansimplement
ToM.Beforepresentingournovelapproach,wefirstelaborateonthelimitations
of existing approaches of conducting cooperation using active inference.
1.1 Existing Active Inference Approaches to Multi-Agent
Cooperation
Maisto and colleagues [9] introduced “interactive inference”, wherein agents
maintain probabilistic beliefs about shared goals (such as both agents pressing
the same or different buttons) and update these beliefs through observations of
others’ locations and actions. Their agents selected epistemic policies designed
to reduce uncertainty about the joint goals. However, this approach assumes
agents share identical goals, which is not always the case in multi-agent cooper-
ationtasks.Moreover,theirmodelreliesonacarefullytailoredgenerativemodel
for the focal agent (i.e., the agent conducting ToM) that incorporates the other
agent’s location as an observation which itself encodes information about the
shared goal. These assumptions limit generalisability to scenarios where actions
do not inherently signal goals or where agents have complementary rather than
identical and shared objectives.
Matsumura and colleagues [10] addressed collision avoidance (agents pass-
ing by each other without colliding) using simulation theory, where agents use
theirowninternalmodelstoimagineothers’situations.Whilethisenablesbasic
perspective-taking, their implementation is domain-specific to navigation tasks
that use the social force model as it includes parameters for forward movement
and mutual repulsion. The approach lacks the recursive reasoning capabilities
thatreliesonmaintenanceofseparatebeliefrepresentationsfordifferentagents,
which is necessary for more complex coordination scenarios.
Otherresearchershaveproposedmulti-agentcooperationthroughexplicitin-
formation exchange mechanisms [2,5]. These approaches involve agents sharing
likelihood messages – information about the probability of observations given
states – rather than posterior beliefs directly. However, this requires the gen-
erative model structures (state factors and observation modalities) to be iden-
tical between agents. Moreover, while mathematically principled, this method
sidesteps the fundamental challenge of inferring others’ beliefs from observable
behaviour alone, a capability that humans routinely demonstrate and leverage
during multi-agent cooperation.
Overall, these approaches predominantly assume that all agents operate un-
derthesamegenerativemodel,withidenticalbeliefsabouttransitiondynamics,
Theory of Mind Using Active Inference 3
observation likelihoods and goal structures. Such aligned models fail to capture
therealityofagentswithdifferentexperiences,capabilities,andobjectives.Fur-
thermore, these approaches typically involve only single-level reasoning (“what
will the other agent do?”) rather than the recursive beliefs that characterise
ToM (“what do I think the other agent thinks about the situation?”). Many
implementations are also tailored to specific tasks (e.g., navigation or mutual
button-pressing) and do not provide general principles for multi-agent coopera-
tion across different tasks.
To address these limitations, we present the first generalisable implementa-
tionformulti-agentcooperationbyimplementingToMusingsophisticatedactive
inference [3], with three key features:
1. Our agents maintain distinct beliefs and generative models for themselves
andothers,allowingthemtoreasonaboutdifferentperspectiveswhileavoid-
ing the assumption of shared knowledge and knowledge structures.
2. We propose a novel tree-based planning procedure that systematically ex-
plores joint policy spaces by interleaving policy and observation rollouts
between agents.
3. Our agents can reason about how others’ actions affect the world states
via message passing between their own beliefs and their beliefs of the other
agent’s beliefs, maintaining perspective separation while allowing for infor-
mation integration.
Weempiricallyvalidateourapproachbysimulatingtwomulti-agentscenarios:a
collisionavoidancetaskwhereagentsmustnavigatepasteachotherwithoutoc-
cupying the same location, and an apple foraging task requiring efficient search
andconsumptionofresources.Thesescenariosareimplementedinasimple3×3
grid environment to provide a clear and interpretable proof of concept, with fu-
tureworkaimedatextendingtheapproachtolargerandmorecomplexsettings.
OurresultsdemonstratethatToM-equippedagentsconductmulti-agentcooper-
ation more effectively than non-ToM agents. The ToM agents successfully avoid
collisions and reduce redundant efforts as they are able to interact proactively
rather than reactively with other agents.
2 Our Approach: Theory of Mind in Active Inference
2.1 Sophisticated Inference
Our approach builds upon sophisticated inference [3], which extends standard
active inference to consider recursive forms of expected free energy (EFE). In
standard active inference, agents evaluate policies by considering “what would
happen if I did that?”. Sophisticated inference instead deepens this to “what
wouldIbelieveaboutwhatwouldhappenifIdidthat?”.Thisdistinctioniscrucial
forToM.Whenreasoningaboutotheragents,weneedtoconsidernotjustwhat
theywilldo,butwhattheybelieveabouttheconsequencesoftheiractions.This
recursivereasoningabouttheotheragentrequiresmaintainingaseparatemodel
of the other agent.
4 Pitliya et al.
2.2 ToM Agent’s Belief Structure of Multiple Agents
InourToMframework,thefocalagentmaintainsseparatestatebeliefs(s)forit-
selfandeachotheragentintheenvironment.Inthecaseofatwo-agentscenario,
this yields s={sf,self,sf,world,so,self,so,world}, where:
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
focal(sf) other(so)
– sf,self is the focal agent’s beliefs about its own states (e.g., focal agent’s own
location)
– sf,world is the focal agent’s beliefs about the world states (e.g., other agent’s
location or item at current location)
– so,self is the focal agent’s beliefs about the other agent’s self states (e.g.,
other agent’s location)
– so,world isthefocalagent’sbeliefsaboutwhattheotheragentbelievesabout
the world states (e.g., focal agent’s location or item at current location)
This structure lets the focal agent keep its own perspective separate while
modelling how others might see things differently. To elaborate, the belief com-
ponents can be flexibly combined to capture different reasoning cases. For in-
stance, the focal agent can pair so,self with sf,world to predict what the other
agent would observe given the focal agent’s own beliefs about the environment.
Thefocalagentcanalsopairso,self withso,world topredictwhattheotheragent
thinks it will observe, based on the other agent’s own (possibly mistaken) view
of the world. This kind of cross-perspective reasoning allows the focal agent to
distinguishbetween(a)whatitthinkstheotheragentwillperceiveand(b)what
it thinks the otheragent believes itmay perceive. Becausethe focalagent’sown
world beliefs (sf,world) may differ from its beliefs about the other agent’s world
beliefs (so,world), it can represent cases where knowledge is asymmetric – for
example, when the focal agent knows something the other does not.
By maintaining these distinct representations, our framework does not as-
sume shared knowledge structures between agents. The focal agent can con-
structandcontinuouslyupdateitsmodeloftheotheragentbasedonobservable
behaviour, while the actual other agent may operate with an entirely different
generative model. This capability enables our agents to collaborate effectively
even when they possess different prior knowledge, capabilities, or goals – a fun-
damental requirement for realistic and practical multi-agent systems.
2.3 Recursive Planning with Theory of Mind
The core innovation in our approach lies in the planning algorithm that enables
agents to reason recursively about joint policy spaces.It systematically explores
how the focal agent’s beliefs about another agent’s beliefs influence its planning
decisions. The recursive form of the EFE is extended to the ToM setting (see
Appendix A), which results in a deep tree search algorithm that alternates be-
tween the focal and other agent’s policies and observations. At each planning
Theory of Mind Using Active Inference 5
Step 1 Step 2 Step 3 Step 4 Step 5
Other's State Beliefs Focal's State Beliefs
Other's Policy Node Focal's Policy Node
Other's Obs Node Focal's Obs Node
Message Node Backwards
Passing Expansion Pass
Fig.1: Recursive Planning Tree for Theory of Mind. Red and purple rep-
resentthefocalandotheragent’snodesrespectively.Circlesindicatetheagent’s
beliefs, squares indicate evaluated actions and diamonds indicated expected ob-
servations. For a detailed description of each step, see Section 2.3.
horizon, the tree search unfolds through five main stages, as detailed below and
illustrated in Figure 1.
Step 1: Other Agent Policy Expansion. As aforementioned in Sec-
tion2.2,webeginwiththefocalagent’sbeliefs,whichcomprisesseparatebeliefs
for itself and the other agent in the environment (s={sf,so}). The focal agent
firstconsiderswhichpoliciestheotheragentislikelytoselect.Thisisvisualised
inStep1ofFigure1,whereeachpolicynoderepresentsaspecificactionthatthe
other agent might execute (a ; purple square). The potential actions are evalu-
0
atedbasedonthefocalagent’sbeliefsabouttheotheragent’sbeliefs(so;purple
circle above purple squares). Essentially, the focal agent asks “Given what I be-
lieve about the other agent’s beliefs and goals, what would the other agent choose
to do?”. The focal agent then computes how the other agent would update its
beliefs if it were to execute that action (so; the purple circle below the purple
square).
Step2:FocalAgentPolicyExpansion.Foreachconsideredactionofthe
other agent, the focal agent evaluates its own policy options. Crucially, before
doing so, the focal agent updates its world beliefs, based on the anticipated
consequencesoftheotheragent’sactions,usinglikelihoodmessagepassing.Here,
the focal agent uses its computation of how the other agent’s beliefs about the
world(so,world;purplecircleabovetheredcircle)wouldchangeiftheotheragent
6 Pitliya et al.
were to execute an action. A likelihood message is then created, capturing the
information gained from the other agent’s anticipated action in the form of the
difference between the other agent’s updated beliefs and its prior beliefs. This
mechanismallowsthefocalagenttoincorporateinformationintoitsownbeliefs
about how the world states (sf,world; red circle above the red squares) would
change due to the other agent’s actions. Using these updated beliefs, the focal
agent then evaluates its own policy options (a ; red square) through standard
0
EFE calculations. This creates branches in the tree structure for each possible
jointpolicycombinationbetweenthefocalandotheragent.Thefocalagentthen
computes how its beliefs would be updated if it were to execute an action given
the other agent’s action (sf; red circle below the red square).
Step 3: Focal Agent Observation Expansion. Then, given the joint
policies,thefocalagentconsiderswhatobservationsitislikelytoreceiveandits
resulting posterior beliefs.
This process is illustrated in Step 3 of Figure 1, where the focal agent’s
observation nodes (o ; red diamonds) represent the various observations the
0
focal agent expects to encounter given the focal agent’s beliefs considering the
execution of both agents’ actions (sf; red circle before red diamonds). This
results in the computation of the focal agent’s posterior beliefs (sf; red circle
after red diamonds).
Step4:OtherAgentObservationExpansion.Here,thefocalagentcon-
siderswhatobservationstheotheragentislikelytoreceive(o ;purplediamonds)
0
given the joint policy and anticipated world state changes given its action. The
observation probabilities are computed using the focal agent’s beliefs about the
other agent’s self states (so,self; purple circle from an earlier expansion) and the
focal agent’s own updated beliefs about the world state (sf,world; red circle be-
fore the purple diamonds). The focal agent then updates its representation of
the other agent’s posterior beliefs (so; purple circle after the purple diamond).
Step 5: Tree Backwards Pass and Policy Selection. Finally, after ex-
panding the tree for the current horizon, a backwards pass computes policy
selection probabilities for the focal agent. The backwards pass is visualised in
Step 5 of Figure 1, with the green upward arrows showing that EFE values
propagate from the leaf observation nodes back through each policy branch to
inform the final policy selection at the root. To plan for another time step, the
observation nodes’ leaves from Step 5 serve as the root node for Step 1 (grey
dotted arrow).
Recursive EFE values are computed for each joint policy combination and
weighted by the observation probabilities. The other agent’s policy probabilities
are marginalised for policy selection. The resulting probability distribution bal-
ancesgoal-directedwithinformation-seekingbehaviourwhiletakingintoaccount
the uncertainty over the other agent’s actions.
Our implementation achieves computational efficiency through two mecha-
nisms as practised in sophisticated active inference [3]. Policy pruning reduces
tree expansion by eliminating unlikely policy nodes and those nodes do not
Theory of Mind Using Active Inference 7
branchout.Observationpruningsimilarlyfocusesonprobableoutcomes,reduc-
ing the combinatorial explosion.
3 Experimental Validation
We empirically validated our ToM framework across two multi-agent scenarios
that required different forms of cooperation. All simulations occurred on a 3×3
grid environment (see Appendix B for the reference grid layout) with determin-
istic dynamics and perfect observability of agent locations. The experimental
design comprised two conditions for each task: a baseline condition where both
agents used sophisticated active inference without ToM capabilities, and a ToM
condition where one agent (red) was equipped with our ToM framework while
theother(purple)remainednon-ToM.Allsimulationswereconductedusingthe
JAX-based Python package, pymdp, which offers efficient and flexible tools for
constructing such models [7].
3.1 Collision Avoidance Task
Task Description. The collision avoidance task presented a basic cooperation
challenge: two agents were initiated at opposite corners of the grid with ob-
jectives to swap positions while avoiding collision. The shortest path for both
agentsinvolvedtraversingthecentralcellthatwouldresultincollision.Weevalu-
atedperformanceusingthreeprimarymetrics:taskcompletionsuccess(whether
agents reached their respective goals), collision occurrence, and path efficiency
(total time steps to completion). The task demanded proactive cooperation, as
reactive strategies would result in deadlock.
Generative Model. Eachagent’sgenerativemodelincludedtwostatefactors:
own location (9 discrete location states plus a null state for boundary violations
as in [8]) and other agent’s location (also 10 states like the focal agent’s loca-
tion states). The observation model provided perfect perceptual access to both
locationsthroughidentitylikelihoodmappings,eliminatingsensoryuncertainty.
The action space comprised nine options: directional movements (up, down,
left, right, four diagonals) plus no-operation. Transition dynamics regarding the
agent’sownlocationreflectedfullcontrollabilityabidingbystandardgrid-world
physics with collision constraints where agents attempting to occupy the same
cell became permanently stuck. Invalid movements were specified via the null
state (which had a severe negative utility). For example, if the agent were to
moveupfromlocation1(top-leftcorner),itwouldentertheseverelydisfavoured
null state, driving it away from invalid movements. Since the other agent’s loca-
tion was not controllable, the transition dynamics for the other agent’s location
reflected uniform probability distribution between the valid actions the other
agent can take. For example, the probability of moving from location 1 (top-left
corner) to the centre, go down, go right, or have no-operation is 1/4.
8 Pitliya et al.
(a) (b)
Fig.2: Experimental Validation of Theory of Mind in Active Inference
UsingMulti-AgentCooperationTasks.(a)Collisionavoidancetask:agents
must swap locations while avoiding collision at the central location. Top row:
Non-ToM condition where both agents select the shortest path, resulting in col-
lision and deadlock. Bottom row: ToM condition where the red agent (equipped
with ToM) anticipates the purple agent’s path and selects an alternative route,
enabling successful cooperation. (b) Apple foraging task: agents search for and
consumeapplesinorchardlocations,withbothagentsinitiallyknowinganapple
existsatthebottom-rightcorner.Toprow:Non-ToMconditionshowingresource
competition where both agents converge on the known apple location, resulting
in only one agent (purple) successfully consuming the apple. Bottom row: ToM
condition where the red agent (equipped with ToM) explores an alternative lo-
cation to avoid redundant competition, successfully finding and consuming an
apple while the purple agent consumes the known apple, achieving optimal re-
source allocation for both agents.
Preferences encoded goal-seeking behaviour: high positive utility for reach-
ing the target location and severe penalty for the null observation. Critically,
no explicit collision avoidance preferences were included – coordination had to
emerge through ToM reasoning rather than hard-coded behaviours. Planning
horizons were set to 3 time steps for both ToM and non-ToM agents, sufficient
to reach goals via alternative paths while requiring that much forward planning
to identify coordination opportunities.
Results. For an illustration of the results from this task, see Figure 2a. In the
non-ToM condition, both agents predictably selected individually optimal poli-
cies, moving directly toward goals via the centre. This resulted in collision and
permanent deadlock, with neither agent achieving its objective. This is a clear
cooperationfailuredespitesophisticatedindividualplanningwhileobservingthe
otheragent’slocationbutnotbeingequippedtoincorporateitintotheplanning
process.
IntheToMcondition,thered(ToM)agentreasonedthattheotheragentwill
mostlikelymovetothecentrallocationtotaketheshortestpathtoitsgoal,soit
does not select moving to that location to avoid collision even though it was the
Theory of Mind Using Active Inference 9
most optimal policy for itself. Instead, the agent went around the centre, which
was the next best alternative route towards its goal. The ToM agent selected
a longer but collision-free route. For the detailed planning tree of the ToM vs
non-ToM agents at time step 0, see Appendix B.
3.2 Apple Foraging Task
Task Description. The apple foraging task examined cooperation in resource
acquisition scenarios with partial observability. The 3×3 grid featured orchard
locations(topandbottomrows)whereapplescouldspawn,andwasteland(mid-
dle row) containing no resources. Both agents began with identical prior knowl-
edge: certainty of an apple at the bottom-right corner, with complete uncer-
tainty about the presence of an apple at other orchard locations. The agents’
initial positions were equidistant from the known apple (see Figure 2b). Apple
consumption was exclusive – only one agent could consume each apple, with
random selection if both agents reached the same apple simultaneously. The co-
operation challenge involved balancing exploitation of known resources against
explorationofuncertainlocations,whileavoidingredundantcompetitionforthe
same resources.
Generative Model. The generative model incorporated three types of state
factors: agent locations, reward feedback (binary: received/not received, which
was conditioned upon eating an apple), and environmental items (wasteland,
apple, or empty orchard). Agents observed their own location, the other agent’s
location, items at their current location, and their own reward feedback.
The environment was partially observable such that agents could only as-
sess apple availability at their current location, creating uncertainty about the
broader resource distribution. Apples spawned probabilistically in orchard loca-
tions (25% per time step) and remained in that location until consumption. Ac-
tion repertoires included movements (up, down, left, right actions), eating, and
no operation. Preferences simply favoured reward acquisition without explicit
cooperation incentives. Planning horizons were set to 3 time steps, sufficient for
reaching the opposing end of the grid environment and consuming apples.
Results. For an illustration of the results from this task, see Figure 2b. For
the detailed planning tree of the ToM vs non-ToM agents at time step 0, see
Appendix C.
In the non-ToM condition, both agents converged on the known apple lo-
cation (bottom-right corner), resulting in resource competition. Only one agent
succeededinconsumingtheapple(determinedrandomly),whiletheotherwasted
effort, demonstrating inefficient cooperation.
IntheToMcondition,thered(ToM)agentreasonedthattheotheragentwill
most likely go to the known apple location and therefore, it selected to explore
anotherlocationwhereitwasuncertainwhetherthereisanapple.Thisresulted
10 Pitliya et al.
in avoidance of redundant efforts and more effective cooperation, avoiding re-
source competition. This strategy proved successful as both agents discovered
and consumed apples.
4 Discussion
Our experimental results demonstrate that equipping active inference agents
withToMcapabilities fundamentallytransformstheir approachto completinga
task,conductingmulti-agentcooperation.TheToM-equippedagentssuccessfully
navigated both collision avoidance and resource competition scenarios, achiev-
ing better cooperation compared to their non-ToM counterparts. Importantly,
this enhanced performance emerged without requiring explicit communication
protocols, shared generative models between agents, or pre-set strategies.
The success of our ToM agents stems from their ability to reason about oth-
ers’ beliefs and anticipate their behaviours. In the collision avoidance task, the
ToMagentrecognisedthatbothagentsfollowingoptimalindividualpathswould
result in collision. By reasoning about the other agent’s likely trajectory, the
ToM agent proactively selected the next best alternative route, demonstrating
cooperative behaviour compared to a merely reactive collision response. Simi-
larly,intheappleforagingtask,theToMagentanticipatedresourcecompetition
and strategically explored uncertain locations, leading to more efficient resource
allocation across both agents.
Ourframeworkaddressesfundamentallimitationsfrompreviousmulti-agent
activeinferenceimplementations.Mostimportantly,weeliminatetherestrictive
assumption of shared or identical generative models that dominates prior work
[9,10,2,5].Thislimitedtheirgeneralisabilityandapplicabilitytomorecomplexor
real-worldscenarioswhereagentspossessdifferentexperiences,capabilities,and
objectives.Incontrast,ourToMframeworkallowsforheterogeneousmulti-agent
generative models. The ToM agent maintains distinct belief representations for
eachagentinitsenvironment,allowingittoreasonaboutotherswithoutassum-
ing they share its own knowledge, goals, or even generative model structure.
4.1 Future Directions
While this paper provides valuable insights into computationally conducting
multi-agent cooperation by implementing ToM in active inference, there are
several avenues for future research to build upon our findings.
Ourcurrentimplementationassumesobservationalaccesstotheotheragents’
locations and is situated in a simple 3x3 grid environment. While these simpli-
fications enables clear demonstration of the core ToM principles, we naturally
need to examine it under more complex and real-world scenarios as such sce-
narios involving more noisy sensory information and complex task requirements
and dynamics. Future work should also include systematic quantitative evalua-
tion using aggregated performance metrics across random seeds and statistical
Theory of Mind Using Active Inference 11
comparisonsagainstnon-ToMbaselinestomorerigorouslyassesstherobustness
of our approach.
Another simplification in the reported simulations was that our ToM agents
assumed knowledge of others’ goals and operated with fixed generative mod-
els of other agents. Future implementations should incorporate online learning
mechanisms, potentially using Dirichlet counts [4], to continuously learn and
updatebeliefsabouttheotheragents’model,preferences,andcapabilities.Such
adaptive learning would significantly enhance the framework’s generalisability,
enabling effective cooperation with agents whose characteristics are initially un-
known or evolving.
Moreover, while our current implementation focuses on dyadic interactions,
the underlying principles naturally extend to larger multi-agent scenarios. Each
agent would maintain separate belief representations for all other agents, and
the planning algorithm would expand over joint policy spaces of arbitrary size.
However, computational complexity grows exponentially with the number of
agents, presenting scalability challenges that require careful considerations and
is an avenue of further research.
Furthermore,ourimplementationfocusesonfirst-orderToMreasoning(“what
does the other agent believe?”) rather than higher-order recursive reasoning
(”what do I think the other agent thinks I believe?”). While first-order ToM
proves to be sufficient for our tested cooperation scenarios, more complex so-
cial situations may require deeper levels of recursive reasoning. This could be
examined in scenarios where there are multiple ToM agents, investigating how
recursive reasoning between ToM-capable agents affects cooperation dynamics
and computational requirements.
Additionally,ourframeworkhasbeenvalidatedonlyincooperativescenarios
whereagents’goalsaresomewhatcomplementary.Futureresearchshouldinves-
tigate competitive scenarios, where agents’ objectives directly conflict, to assess
whether the planning algorithm remains effective and how generative models
should be structured to handle adversarial interactions.
5 Conclusion
We have presented the first generalisable implementation of ToM within the
activeinferenceframeworkformulti-agentcooperation.Ourapproachrepresents
a significant advancement over existing methods by eliminating the restrictive
assumption that all agents must operate under shared or identical generative
models and goal structures.
The core innovation of our framework lies in enabling agents to maintain
distinct beliefs about themselves and others while reasoning recursively about
howothers’beliefsinfluencetheirbehaviour.Throughournoveltree-basedplan-
ning algorithm, ToM-equipped agents systematically explore joint policy spaces
by considering what others believe and how those beliefs influence their con-
sideration and decisions of actions. This recursive reasoning capability allows
12 Pitliya et al.
for sophisticated cooperation online without requiring explicit communication
or pre-arranged cooperation protocols.
Wevalidatedourframeworkwithtwotasks:collisionavoidanceandresource
foraging tasks. ToM agents successfully cooperated in both scenarios, avoiding
conflicts and achieving more efficient outcomes compared to non-ToM agents.
Importantly, these cooperative capabilities emerged immediately upon encoun-
teringthetaskchallenges,withoutrequiringlengthytrainingperiodsordomain-
specific learning.
The framework we have developed bridges computational and cognitive sci-
ence,providingbothapracticaltoolforenhancingartificialintelligencesystems
andacomputationalfoundationforunderstandinghowsophisticatedsocialrea-
soningmightemergefromprincipledprobabilisticinferenceaboutothers’minds.
References
1. Baron-Cohen,S.,Leslie,A.M.,Frith,U.:Doestheautisticchildhavea“theoryof
mind”? Cognition 21(1), 37–46 (1985)
2. C¸atal,O.,VandeMaele,T.,Pitliya,R.J.,Albarracin,M.,Pattisapu,C.,Verbelen,
T.: Belief sharing: a blessing or a curse. In: International Workshop on Active
Inference. pp. 121–133. Springer (2024)
3. Friston, K., Da Costa, L., Hafner, D., Hesp, C., Parr, T.: Sophisticated inference.
Neural Computation 33(3), 713–763 (2021)
4. Friston,K.,FitzGerald,T.,Rigoli,F.,Schwartenbeck,P.,Pezzulo,G.,etal.:Active
inferenceandlearning.Neuroscience&BiobehavioralReviews68,862–879(2016)
5. Friston,K.J.,Parr,T.,Heins,C.,Constant,A.,Friedman,D.,Isomura,T.,Fields,
C., Verbelen, T., Ramstead, M., Clippinger, J., et al.: Federated inference and
belief sharing. Neuroscience & Biobehavioral Reviews 156, 105500 (2024)
6. Frith, C., Frith, U.: Theory of mind. Current biology 15(17), R644–R645 (2005)
7. Heins, C., Millidge, B., Demekas, D., Klein, B., Friston, K., Couzin, I., Tschantz,
A.: pymdp: A python library for active inference in discrete state spaces. arXiv
preprint arXiv:2201.03904 (2022)
8. Van de Maele, T., Dhoedt, B., Verbelen, T., Pezzulo, G.: Integrating cognitive
map learning and active inference for planning in ambiguous environments. In:
International Workshop on Active Inference. pp. 204–217. Springer (2023)
9. Maisto, D., Donnarumma, F., Pezzulo, G.: Interactive inference: A multi-agent
model of cooperative joint actions. IEEE Transactions on Systems, Man, and Cy-
bernetics: Systems 54(2), 704–715 (2023)
10. Matsumura, T., Esaki, K., Yang, S., Yoshimura, C., Mizuno, H.: Active inference
withempathymechanismforsociallybehavedartificialagentsindiversesituations.
Artificial Life 30(2), 277–297 (2024)
11. Wellman, H.M., Cross, D., Watson, J.: Meta-analysis of theory-of-mind develop-
ment: The truth about false belief. Child development 72(3), 655–684 (2001)
12. Wimmer, H., Perner, J.: Beliefs about beliefs: Representation and constraining
functionofwrongbeliefsinyoungchildren’sunderstandingofdeception.Cognition
13(1), 103–128 (1983)
Theory of Mind Using Active Inference 13
A Expected Free Energy under Sophisticated Inference
with Theory of Mind
In sophisticated inference, the expected free energy is calculated in a recursive
way [3]. This can be construed as a deep tree search, where the tree branches
over allowable actions at each point in time, and the likely observations conse-
quent upon each action. Equation 1 expresses the expected free energy of each
potential next action (a ) and observation (o ) as the utility and information
τ τ
gainofthatactionplustheaverageexpectedfreeenergyoffuturebeliefs,under
counterfactual observations and actions:
G(o ,a )=E (cid:2) −lnP(o |C)−D [Q(s |o )||Q(s )] (cid:3)
τ τ Q(oτ+1|a≤τ) τ+1 KL τ+1 τ+1 τ+1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
utility informationgain
+E (cid:2) G(o ,a ) (cid:3) (1)
Q(aτ+1|oτ+1)Q(oτ+1|a≤τ) τ+1 τ+1
(cid:124) (cid:123)(cid:122) (cid:125)
expectedfreeenergyofsubsequentactions
with
Q(o |a )=P(o |s )Q(s |a )
τ+1 ≤τ τ+1 τ+1 τ+1 ≤τ
(cid:0) (cid:1)
Q(a |o )=σ −G(o ,a )
τ τ τ τ
When planning with Theory of Mind, this deep tree search becomes an al-
ternation between actions of the other agent and actions of the focal agent, as
well as counterfactual observations for both. The resulting expected free energy
in Equation 2 is now expressed as a function of an action and observation for
the focal agent (af,of) as well as the other agent (ao,oo):
τ τ τ τ
G(of,oo,af,ao)=E
τ τ τ τ Q(of ,oo |af ,ao )
τ+1 τ+1 ≤τ ≤τ
(cid:2) −lnP(of |Cf)−D [Q(sf |of )||Q(sf )] (cid:3)
τ+1 KL τ+1 τ+1 τ+1
+E
Q(af |of )Q(ao |oo )Q(of ,oo |af ,ao )
τ+1 τ+1 τ+1 τ+1 τ+1 τ+1 ≤τ ≤τ
(cid:2) G(of ,oo ,af ,ao ) (cid:3) (2)
τ+1 τ+1 τ+1 τ+1
with
Q(of ,oo |af ,ao )=P(of |sf )Q(sf )P(oo |so ,sf )Q(so )
τ+1 τ+1 ≤τ ≤τ τ+1 τ+1 τ+1 τ+1 τ+1 τ+1 τ+1
Q(sf )=Q(sf |sf,af,so )E [Q(so |oo )]
τ+1 τ+1 τ τ τ+1 Q(oo |ao ) τ+1 τ+1
τ+1 ≤τ
Q(ao|oo)=σ (cid:0) −G(oo,ao|Co) (cid:1)
τ τ τ τ
Q(af|of)=σ (cid:0) −G(of,oo,af,ao) (cid:1)
τ τ τ τ τ τ
(3)
14 Pitliya et al.
Notethattheutilitytermforthefocalagentusesthefocalagent’spreferences
Cf,whichmightbedistinctfromtheotheragent’spreferencesCowhichareused
tocalculatetheposteriorovertheother’sactions.Inaddition,ourposteriorover
expectedobservations fortheotheragent isconditionedonbothso and sf .
τ+1 τ+1
Here,wecombinetheso,self withsf,world togenerateexpectedobservationsfrom
the perspective of the other, using the world belief of the focal agent. Another
crucial point in Equation 3 is that to calculate the predictive posterior Q(sf )
τ+1
ofthefocalagent,wealsoincorporatetheeffecttheother’sactionmightalready
havehadonthefocalagent’sbeliefstate.Thisisimplementedviabeliefsharing
via likelihood message passing, similar to [2].
B Planning Trees for Collision Avoidance Task
Figure3showsthedifferenceinplanningtreesforbothanon-ToM(c)andToM
agent (d) completing the collision avoidance task with another agent. It is clear
thattheToMagent’splanningtree(d)entertainsfourroutestheagentcantake
toavoidcollidingwiththeother(non-ToM)agentandthattherouteillustrated
in the right-most branch of the planning tree is selected to complete the task
(a).
C Planning Trees for Apple Foraging Task
Figure4showsthedifferenceinplanningtreesforbothanon-ToM(c)andToM
agent (d) completing a apple foraging task with another agent. We start from
the orchard environment state in (a), where both agents navigate to locate and
consume apples, with initial shared knowledge of an apple at location 9. We
visualise the planning tree for the red agent with (d) and without ToM (c).
In the non-ToM case, the focal agent (red) evaluates only its own policies
over a 2-step horizon, selecting to go to location 9 (P=1.0) based on expected
utility (G=10.00). However, when pursuing this policy it will end up with no
apple, as the purple agent will get there first.
In contrast, in the ToM case, the focal agent (red) indeed reasons that the
other agent (purple) will get to the apple first, and therefore chooses to explore
the tile on the right, which might have an apple as well.
Theory of Mind Using Active Inference 15
(a) (b)
(c) (d)
Fig.3: (a) Collision avoided by the red agent as the focal agent with ToM. (b)
Referencegridlayoutfor3×3simulationenvironment.Numbers1-9indicatecell
indices used throughout the experiments to specify agent locations and move-
ments.(c)Non-ToMplanningtree:Theredagentevaluatesonlyitsownpolicies
over a 2-step horizon, selecting to go to location 5 (P=1.0), which eventually
results in colliding with the other agent. (d) ToM planning tree: The red (ToM)
agent recursively models the purple (non-ToM) agent’s policy space and beliefs,
resulting four possible routes the red agent can take to avoid collision with the
other agent which is predicted to go to its goal location via location 5.
16 Pitliya et al.
(a) (b)
(c) (d)
Fig.4: (a) Start situation with the red agent as the focal agent. (b) Reference
grid layout for 3×3 simulation environment. Numbers 1-9 indicate cell indices
used throughout the experiments to specify agent locations and movements. (c)
Non-ToM planning tree: The red agent evaluates only its own policies over a
2-step horizon, selecting to go to location 9 (P=1.0) based on expected utility
(G=10.00). (d) ToM planning tree: The red (ToM) agent recursively models
the purple (non-ToM) agent’s policy space and beliefs, resulting in selection of
going left with near full certainty (q pi=0.95). This recursive reasoning leads
to exploration of location 7, as the agent predicts the purple agent will pursue
location 9, enabling coordinated foraging.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Theory of Mind Using Active Inference: A Framework for Multi-Agent Cooperation"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
