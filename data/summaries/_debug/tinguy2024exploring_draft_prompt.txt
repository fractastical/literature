=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Exploring and Learning Structure: Active Inference Approach in Navigational Agents
Citation Key: tinguy2024exploring
Authors: Daria de Tinguy, Tim Verbelen, Bart Dhoedt

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2024

Key Terms: exhibit, navigation, approach, agents, structure, mapping, memory, verses, exploring, learning

=== FULL PAPER TEXT ===

Exploring and Learning Structure:
Active Inference Approach in Navigational Agents
Daria de Tinguy1[0000−0003−1112−049X], Tim Verbelen2, and bart Dhoedt1
1 Ghent University, Ghent, Belgium first_name.family_name@ugent.be
2 Verses AI tim.verbelen@verses.ai
Abstract. Drawing inspiration from animal navigation strategies, we
introduce a novel computational model for navigation and mapping,
rooted in biologically inspired principles. Animals exhibit remarkable
navigationabilitiesbyefficientlyusingmemory,imagination,andstrate-
gicdecision-makingtonavigatecomplexandaliasedenvironments.Build-
ing on these insights, we integrate traditional cognitive mapping ap-
proacheswithanActiveInferenceFramework(AIF)tolearnanenviron-
ment structure in a few steps. Through the incorporation of topological
mapping for long-term memory and AIF for navigation planning and
structure learning, our model can dynamically apprehend environmen-
talstructuresandexpanditsinternalmapwithpredictedbeliefsduring
exploration.ComparativeexperimentswiththeClone-StructuredGraph
(CSCG)modelhighlightourmodel’sabilitytorapidlylearnenvironmen-
talstructuresinasingleepisode,withminimalnavigationoverlap.thisis
achieved without prior knowledge of the dimensions of the environment
or the type of observations, showcasing its robustness and effectiveness
in navigating ambiguous environments.
Keywords: exploration·activeinference·topologicalgraph·structure
learning.
1 Introduction
A functional navigation system must seamlessly fulfil three key functions: self-
localisation, mapping, and path planning. This requires both a sensing compo-
nent for spatial perception and a storage capability to extend these perceptions
temporally and spatially [33]. Animals exhibit a remarkable capacity for rapidly
learning the structure of their environment, often in just one or a few visits,
relying on memory, imagination, and strategic decision-making [32,26].
The hippocampus and neocortex play crucial roles in episodic memory, spa-
tialrepresentation,andrelationalinference.Mammalsrelyonmentalrepresenta-
tionsofspatialstructures,traditionallyviewedaseithercognitivemapsorcogni-
tivegraphs,conceptualisingenvironmentalspaceasanetworkofnodes[33,25,7,1].
Recent research suggests an integrated approach combining these concepts is
more effective [23].
4202
peS
2
]IA.sc[
2v28950.8042:viXra
2 D. de Tinguy & al
Our approach adopts this viewpoint, proposing a topological map incor-
porating internal motion (Euclidean parameters) to delineate spatial experi-
ences. The neural positioning system, found in rodents and primates, supports
self-localisation and provides a metric for distance and direction between lo-
cations [33]. This system includes place cells, heading direction cells [15], grid
cells [2], speed cells [14], and border cells [30], working together to enable rapid
learning, disambiguation of aliases, and a comprehensive understanding of spa-
tial navigation [5].
Building on these concepts, we introduce a novel model that dynamically
learnsenvironmentalstructureandexpandsitscognitivemap.Integratingvisual
informationandproprioception(inferredbodymotion),ourmodelconstructslo-
cations and connections within its cognitive map. Starting with uncertainty, the
model envisions action outcomes, expanding its map by incorporating hypothe-
ses into its generative model, analogous to Bayesian model reduction [9] that
grows its model upon receiving new observation, we extend ours upon predicted
beliefs. This process allows our model to efficiently navigate and comprehend
environmental structures with minimal steps, using an active inference naviga-
tion scheme [17]. Compared to the Clone-Structured Graph (CSCG) [24], our
model rapidly learns environmental layouts more efficiently. An exploration run
is shown in Figure 1, displaying from left to right the full environment, the ex-
tracted observation and exploration path and the resulting internal map of the
agent.
Fig.1.Fromthemini-gridenvironment[3,31]withdifferentroomsannotatedbycolour
to the path our agent took -from black to white- to form a successful exploration
correctly linking all the rooms up to the agent’s internal topological graph with the
state associated to each room.
2 Related work
Whilenavigatingtheirenvironment,animalsoftenencounterambiguoussensory
inputs due to aliasing, resulting in repetitive observations, such as encountering
Exploring and Learning Structure 3
two overly similar corridors. They must rapidly disambiguate the structure of
their environment to navigate successfully.
Models like the Clone-Structured Graph (CSCG) [12] or Transformers rep-
resentations [4] have been proposed to form cognitive maps that disambiguate
aliased environments through partial observations. However, these models re-
quiresubstantialtrainingtimeusingrandomorhard-codedpolicies.Incontrast,
animals adapt their actions based on subtle cues and incentives, learning to
navigate with minimal instances [32].
Animals exhibit decision-making abilities powered by imagination (estimat-
ingactions’consequences)andaholisticunderstandingoftheenvironment,nat-
urally imagining un-visited areas and guiding their next steps [2]. This intuitive
decision-makingprocess,consideringincentiveslikefoodorsafety,rapidlydirects
them toward their objectives [26].
Integrating observations with proprioception [33] helps animals circumvent
aliasing,usingaprocesssimilartoactiveinferenceforjudgement[17].Activein-
ference involves continuously updating internal models based on sensory inputs,
enabling adaptive and efficient decision-making. This normative framework ex-
plainscognitiveprocessingandbraindynamicsbypositingthatactionsandper-
ceptions aim to minimise free energy, encapsulating causal relationships among
observable outcomes, actions, and hidden states [22,8].
Atthecoreofadaptivebehaviouristhebalancebetweenexploitation(select-
ingthemostvaluableoptionbasedonexistingbeliefs)andexploration(choosing
options that facilitate learning) [27]. Recent behavioural evidence suggests hu-
mansmixrandomandgoal-directedexploration[11].Ourmodeladoptsthisbal-
ance through free energy minimisation, choosing stochastic policies to enhance
environmentalunderstanding.Thisenablesactivelearning,rapidlyreducingun-
certaintyovermodelparameters(i.e.reducinguncertaintyoverourbeliefs)[27].
By balancing curiosity and goal-directed behaviour through free energy, our
systemguidesanagentmeaningfullyandlearnsinabiologicallyplausibleway[22].
It achieves few-shot or one-shot learning, similar to mice in a labyrinth [26]. By
projecting the consequences of actions into its internal map, the agent extends
its imagination beyond known territories, improving its navigation ability to
explore environments of any dimension.
3 Method
Inourstudy,ouragentinitiatesexplorationoftheenvironmentwithoutanyprior
knowledge regarding the observations and dimensions of the map it is about to
navigate. Subsequently, we will clarify how, at each step, the agent engages in
inferring the current state, a process that integrates both the notion of obser-
vation and proprioception (position perception given a motion). This inference
task involves updating past beliefs based on the latest observation and motion,
following the principles of a Partially Observable Markov Model (POMDP).
Henceforth, the agent strategically envisions sequences of actions to explore,
termed policies, while concurrently expanding its internal map to accommodate
4 D. de Tinguy & al
potentialunexploredareaswithuncertainpriors.Althoughtheagentmayknow
the relative positions of these areas, it does not foresee observations. This iter-
ative and multi-step process serves as the cornerstone for the agent’s adaptive
learning and navigation strategies within the environment.
3.1 Inference and spatial abstraction
InthecontextofActiveInference(AIF),theprocessofinferringtheagent’scur-
rentstateinvolvesintegratingsensoryinputsandpriorbeliefswithinaPartially
ObservableMarkovDecisionProcess(POMDP).Weconsiderthatourinference
mechanismoperatesatthehighestlevelofabstractionwithinahierarchicalspa-
tial framework [31], where lower layers handle observation transformation and
the concept of blocked paths, akin to how visual observations are processed
in the visual cortex and motion limitations are perceived by border cells [30].
Figure 1 illustrates the agent navigating through an environment, where doors
signify transitions to different states while walls correspond to obstacles. We
give our agent the notion that doors lead to another location, while walls lead
to the same observation and the pose stays static. At the centre of this Figure,
weseeapathtakenbytheagentdepictedalongwiththeobservationsperceived
by our model, demonstrating how observations are simplified and generalised at
the highest abstraction level into a single colour per room (floor colour). The
internaltopologicalmapgeneratedbytheagentbasedonitsexplorationpathis
presentedinthefinalframeofFigure1.TheunderlyingPOMDPmodelguiding
this inference process is depicted in Figure 2, where the current state s (defin-
t
ing a room) and position p (the location of that room) are inferred based on
t
the previous state s , p and action a leading to the current observa-
t−1 t−1 t−1
tion o (the colour of that room). The generative model capturing this process
t
is described by Equation 1, where the joint probability distribution over time
sequences of states, observations, and actions is formulated. Tildes are used to
denote sequences over time.
τ
(cid:89)
P(o˜,s˜,p˜,a˜)=P(o |s )P(s )P(p )P(a ) P(o |s )P(s ,p |s ,p ,a )
0 0 0 0 0 t t t t t−1 t−1 t−1
t=1
(1)
Due to the posterior distribution over a state becoming intractable in large
state spaces, we use variational inference instead. This approach introduces an
approximateposteriordenotedasQ(s˜,p˜|o˜,a˜)andispresentedinequation2[28].
τ
(cid:89)
Q(s˜,p˜|o˜,a˜)=Q(s ,p |o ) Q(s ,p |s ,p ,a ,o ) (2)
0 0 0 t t t−1 t−1 t−1 t
t=1
The classical inference scheme heavily relies on past and current experiences
to localise the agent within its environment, using observation alone, the agent
would be weak to aliased observations at different locations. By combining ob-
servation with the agent’s proprioception the model is much more robust in dif-
ferentiating ambiguous environments. The internal positioning p is initialised
0
Exploring and Learning Structure 5
Fig.2. factor graph POMDP of our generative model transitioning from past and
present(uptotime-stept)tofuture(time-stept+1).Theposep isinferredfromthe
t
previous pose p and the action from policy π, while the state s determined by the
t−1 t
corresponding observation o and influenced by the previous state s , pose p and
t t−1 t
actiona .Pastactionsandobservationsareassumedobservable,indicatedbyablue
t−1
colour. In the future, the actions are defined by a policy π influencing the new states
and position in orange and new predictions in grey.
at the start of exploration in the absence of prior information, and is updated
as the agent transitions between rooms (e.i., by passing through a door), thus
as long as the agent is confident in its current state. the POMDP factor graph
showing the association between poses, states and observations is illustrated in
Figure 2
Iftheagentweretobekidnappedandre-localisedelsewhere,theobservation
oandinferredpositionpwouldnotmatchexpectationsandtheconfidenceinthe
statewoulddecrease.Iftheconfidenceinthestategoesbelowagiventhreshold,
the agent stops updating its internal model given new information and focuses
on re-gaining confidence over its state/location.
However, inferring the position p has much more to offer than localisation
robustness, it is key to extending the internal map over unexplored areas yet to
be integrated into the model through parameter learning.
3.2 Parameter Learning
Learning within the Active Inference framework encompasses the adaptation of
beliefs concerning model parameters, such as transition probabilities P(s |s )
t t−1
(e.g. how rooms are connected) and likelihood probabilities P(o |s ) (e.g. what
t t
a room looks like). These parameters reflect the structural connectivity of the
environment and the expected sensory outcomes given particular states.
Generative models in Active Inference rely on prior beliefs regarding param-
eter distributions, with updates driven by the active inference framework [22].
6 D. de Tinguy & al
Unlike traditional discrete-time POMDP, where either transitions or likelihoods
probabilities are fixed and updating parameters implies reasoning over a fixed
spatial dimension [21,13,19], our model learns the probabilities of all its Markov
matrices and extends their dimensions dynamically. The state transitions B =
s
P(s |s ,a ) and the observation A = P(o |s ), position likelihoodA =
t t−1 t−1 o t t p
P(p |s ) probabilities are optimised over transitions. The position transition
t t
B =P(p |p ,a ),however,isnotaMarkovmatrixandentailsanincremen-
p t t−1 t−1
tal process based on consecutive motions (experimented or predicted), without
any parameters to be learned by belief optimisation.
The optimisation of beliefs of the generative model parameters θ occur after
state inference and involves minimising the free energy F while considering
θ
prior beliefs and uncertainties associated with both parameters and policies, as
defined in [22]:
θ =(A ,A ,B )
o p s
(3)
F =E [F(π,θ)]+D [Q(θ)||P(θ)]+D [Q(π)||P(π)]
θ Q(π,θ) KL KL
With P and Q being respectively the joint distribution and the approximate
posteriorofthemodel.Themodelupdatesitsparametersbasedonobserveddata
and transitions, expanding the observation dimension of A upon encountering
newinformationascanbeseenin[29].Atinitialisation,highcertaintyisassigned
tothelikelihoodprobabilitiesintegratingthefirstobservation.Afterrealisingthe
parameter update based on priors, the model edits its internal map dimensions
andparametersbasedonpredictedtransitions,expandingallparametersintheir
statedimensions,thusimprovingexplorationinunexploredenvironmentsofany
unknown size.
3.3 Incorporating spatial dynamics in model parameters
Toextendtheinternalmap(ourstatespace),weproposeanovelapproachwhere
theagentpredictsone-steppolicyoutcomesinalldirectionsconsideringdetected
obstacles.B canexpanditspositiondimensiongivenamotionandA considers
p p
the probability of being at a given state given the position. When we predict a
new position given no obstacle in a direction, B expands. If the expected mo-
p
tion leads to an un-visited location, s does not exist in the model. The state
t+1
is undefinedwhile the position p is certain,therefore allMarkovmatrices are
t+1
expected to grow in their state dimension to match this new prediction. This
process enables the dynamic expansion of the dimensions of both the observa-
tion and position likelihoods (A , A ) and state transition (B ) to consider the
o p s
novel state s in a process equivalent to [9]. Subsequently, the state transition
t+1
probability (B ) and position likelihood (A ) can be updated through the same
s p
equation 4, here shown with a transition matrix.
B =Q(s |s ,π)Q(s )∗B ∗learning_rate (4)
π t+1 t t π
Withs beinganewstateiftherearenoobstaclesdetectedandanewposition
t+1
is predicted or the same state s otherwise. The learning rate is set higher for
t
Exploring and Learning Structure 7
experimented transitions than imagined transitions such that we form new con-
nections weaker toward expected places compared to visited places as we would
expect from animal synaptic learning [6]. A has grown in its state dimension,
o
however, it lacks information regarding the specific observation o expected
t+1
in that location, resulting in a uniform distribution for that state. Such areas
exhibithighuncertaintyintheirobservationlikelihoodmodel.Usingthoseprior,
the agent can leverage the Active Inference scheme to determine where to di-
rect itself to maximise its objective (e.g. forming a comprehensive map of the
environment). While previous models such as [9,29] adjust their internal model
growth to accommodate new patterns of observations, we extend the concept to
predict,un-visited,areasandgeneratenewstatesholdingnoobservation.Those
unknown states are therefore highly attractive when seeking information gain
and largely improve exploration strategy.
3.4 Policy Selection in Active Inference
Policy selection plays a crucial role in exploring those expected states generated
by the model. The AIF guides the agent’s decision-making process based on the
minimisation of expected surprise and uncertainty. Policy selection, informed
by the AIF, determines the agent’s actions and map extension in response to
sensory inputs and internal beliefs.
Typically, agents are assumed to desire to minimise their variational free
energy(F),whichcanserveasametrictoquantifythediscrepancybetweenthe
jointdistributionP andtheapproximateposteriorQaspresentedinEquation5.
F =E [log[Q(s˜,p˜|a˜,o˜)]−log[P(s˜,p˜,a˜,o˜)]
Q(s˜,p˜|a˜,o˜)
=D [Q(s˜,p˜|a˜,o˜))||P(s˜,p˜|a˜,o˜)]− log[P(o˜)]
KL
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
posteriorapproximation logevidence (5)
=D [Q(s˜,p˜|a˜,o˜))||P(s˜,p˜,a˜)]−E [log[P(o˜|s˜)]
KL Q(s˜,p˜|a˜,o˜)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
complexity accuracy
Activeinferenceagentsaimtominimisetheirfreeenergybyengaginginthree
main processes: learning, perception, and planning. Learning involves optimis-
ing the model parameters, perception entails estimating the most likely state,
and planning involves selecting the policy or action sequence that leads to the
lowest expected free energy. Essentially, this means that the process involves
forming beliefs about hidden states that offer a precise and concise explanation
of observed outcomes while minimising complexity.
While planning, however, we use the expected free energy (G), indicating
the agent’s anticipated variational free energy following the implementation of
a policy π. Unlike the variational free energy, which focuses on current and past
observations,theexpectedfreeenergyincorporatesfutureexpectedobservations
generated by the selected policy.
8 D. de Tinguy & al
G(π,τ)=E [log(Q(s |π)−log(Q(s |o ,π))]
Q(oτ,sτ|π) τ τ τ
(cid:124) (cid:123)(cid:122) (cid:125)
informationgainterm
(6)
−E [log(P(o ))]
Q(oτ,sτ|π)) τ
(cid:124) (cid:123)(cid:122) (cid:125)
utilityterm
The expected information gain quantifies the anticipated shift in the agent’s
belief over the state from the prior Q(s |π) to the posterior Q(s |o ,π) when
τ τ τ
pursuing a particular policy. On the other hand, the utility term assesses the
expected log probability of observing the preferred outcome under the chosen
policy. This value intuitively measures the likelihood that the policy will guide
the agent toward its prior preferences. In this study, we give no prior preference
to the agent, as it does not know the environment (unknown observations and
map size).
To calculate this expected free energy G(π) over each step τ of a policy we
sum the expected free energy of each time-step.
(cid:88)
G(π)= G(π,τ) (7)
τ
To consider the best policy, we recall that active inference achieves goal-
directed behaviour by selecting policies minimising this expected free energy,
thereby aiming to produce observations closer to preferred outcomes or prior
preferences. This is achieved by setting the approximate posterior over policies
as in Equation 8 [19]:
P(π)=σ(−γG(π)) (8)
Where σ, the softmax function is tempered with a temperature parameter γ,
given as a hyper-parameter, converting the expected free energy of policies into
a categorical distribution over policies. Actions are then sampled based on this
posterior distribution, with lower temperatures resulting in more deterministic
behaviour.
By navigating without a clear preference, we desire the highest information
gain, effectively pushing the agent toward states it anticipates but doesn’t know
what to expect from.
4 Results
We explore experimental scenarios where an agent navigates within a grid en-
vironment with cardinal motions and still motion. The agents have no direct
access to a map of the environment and visual observations are considered to
undergo hierarchical processing, transforming them from a vector to a single
descriptor corresponding to one colour per room. They receive localised sensory
inputs, correspondingto the current room they arein. Sensory inputs which are
possibly repeated at different locations (aliased observations). Given a series of
Exploring and Learning Structure 9
discretised egocentric observations and actions, the agent must deduce the la-
tent topology of its environment to assess various navigation options. Learning
this latent graph from aliased observations presents a challenge for most artifi-
cial agents [18]. We contrast our model with CSCG [10], a specialised variant
ofHiddenMarkovModels(HMM).CSCGemploysaprobabilisticapproach,us-
ingsequencesofaction-observationpairswithoutassumingEuclideangeometry.
Each observation corresponds to a subset of hidden states known as clones. Al-
though these states share the same observation likelihood, they differ in their
implieddynamicsencodedinthetransitionmodel.Byanalysingthesequenceof
action-observationpairs,specificcloneswithhigherlikelihoodscandisambiguate
the aliased observations. Initially, CSCG gathers a dataset through maze explo-
ration to learn the spatial structure [10].
To make the two models more similar for a fair comparison, we include our
model’s current state estimation mechanism in the CSCG approach [19]. More-
over, we decided to see how CSCG would behave if we included the position as
anobservation.Thiseffectivelyremovesaliasingandisbelievedequivalenttothe
proprioception of our model when starting without prior, we call that specific
case "CSCG pose ob". We compared the performance of our model (receiving
only observation as input) to the CSCG receiving only visual observations or
visual observation-position pairs with or without random exploration policies.
TheCSCGinternalpathestimatorisbasedontheViterbimethod[16,10]andis
updatedevery5stepswiththesequenceofpairsgoingfromthefirstobservation
to the current time-step.
Our environments are composed of several rooms connected in diverse ways
(fullyconnected3by3and4by4roomsenvironments,T-shaped,donuts-shaped
mazeswithandwithoutaliasedfloorcolours).Allmodelsreceivetheroomfloor
colourasobservation.Anexampleofa3by3roomsenvironmentandextracted
observations per room are presented in Figure 1 first and second panel. The
agents can move in the four cardinal directions or choose to stay at the present
location.
In our exploration runs, across all environments, agents are initially placed
at random starting positions and tasked with learning the environment’s topol-
ogy. Results represent the mean over a minimum of ten successful runs in each
environment. Notably, our agent always achieves successful exploration, while
CSCG occasionally fails due to insufficient steps allotted to learn the topology.
TheOraclemodel,analogoustoanA-starpathplanning,demonstratestheideal
scenario where the agent seizes the full topology of the environment by visiting
each position only once. In the case of the T-maze, results are averaged across
all runs considering the starting positions of the models.
OurexplorationresultscanbeseeninFigure3a.Explorationisdeemedcom-
plete when the internal belief over the transition between observations aligns
with the ground-truth transition matrix with a minimum certainty of 60% over-
all correct transitions, the threshold was set arbitrarily based on the resulting
successfultransitionrepresentationastheonethatcanbeseenina3by3obser-
vations map depicted in figure 4. The figure shows how well-defined are possible
10 D. de Tinguy & al
(a) average steps to explore the environments
(b) average steps to discover the environments
Fig.3. The average steps are depicted on a logarithmic scale. Remarkably, our agent
achievesalltasksinsignificantlyfewerstepscomparedtotheCSCGmodel.Theoracle
setsthebenchmark,representingtheminimumstepsnecessarytovisitallroomsonce.
Additionally, an aliased room signifies the recurrence of identical observations across
variouslocations,posingachallengeasitcouldmisleadtheagentregardingitscurrent
position.
transitions compared to impossible transitions (due to walls). We also see that
givinguniqueobservations(visualobservation-positionpair)informationreduces
theCSCGtrainingtimeofabout100stepsinaliasedsquaredenvironmentsand
200inT-shapedmazes,mostprobablyduetoitsstructure,theagentstaysstuck
inanaisle.HoweverusingrandompolicyortheViterbialgorithmfornavigation
does not improve exploration, because the agent can not extrapolate on un-
seen observations, thus finally leading to almost random action selection. This
demonstrates the benefit of map extension over un-visited areas.
Exploring and Learning Structure 11
Fig.4. Example of a successful transition representation between positions in a 3x3
gridmap.Eachstateintheplotispairedwithitscorrespondingground-truthposefor
clarity(pose,state).Theintensityofcolourinthefigureindicatesthelevelofcertainty
the agent has about the transition.
Ifwecomparethenumberofstepsrequiredtolearnthestructureoftheenvi-
ronmentandthenumberofstepstheagenttakestovisitallunknownpositions,
wecandeduceafewthings.Firstly,nothavinganimaginationoverpossibletra-
jectoriesdisadvantagestheCSCG,asitrepeatedlyvisitsknownrooms,randomly
or not, instead of being attracted to novelty as ours is. Ours has prior over non-
visitedstates,renderingunknownroomshighlyuncertain,andthusattractiveto
diminish the agent’s internal model’s parameters uncertainty. Secondly, we see
ouragentexploringallroomswithstepscloselymatchingtheoracle,thisimplies
thatitcouldhavethepotentialtolearntransitionsfaster,inaone-shotlearning
if we were to increase confidence in imagined beliefs. However, this could also
consolidate misbeliefs about transitions, in those experiments we let the agent
confirm its priors instead of over-trusting them by setting the learning rate of
the model low on predicted transitions. The given exploration seems to follow
biological evidence on mouse behaviour in a maze [26].
WegiveaqualitativeexampleoftheagentbehaviourintheT-mazeFigure5.
Figure 5a shows the full path taken by a line varying from black to yellow, the
agent starts at the bottom of the T-maze. Figure 5b shows the imagined trajec-
tories of the agent (represented by X in the figure) at various steps to read from
left to right and top to bottom. Imagined trajectories are associated with their
expected free energy, the darker, the more desirable the path to the agent. The
agent is purely driven by information gain in those experiments. Our model has
low interest in paths leading into the current room walls and is highly attracted
to unexplored areas. Upon reaching the end of the right aisle (1st image, 2nd
row of Fig 5b), the unexplored aisle is notably more attractive than the previ-
ously visited one, highlighting the agent’s preference for uncertain observations
overconfirmingexistingbeliefs.Whilereturningtothestartingpoint,theagent
shows interest in paths going through walls. This is because these transitions
become more intriguing as the agent has gained a better understanding of the
12 D. de Tinguy & al
environment’sconnectivity.Aconsolidationofitsbeliefcanberealisedthrougha
newobservationofthewalls.Thoseobservationsconfirmthattheagentexhibits
a coherent and effective exploration behaviour akin to how we would explore an
environment, first discovering all areas before delving into specific details.
(a) (b)
Fig.5. Exploration of a T-maze starting at the base of the T. a) depicts the full path
asalinetransitioningfromblacktowhite.b)showcases,fromtoptobottomcolumns
one to two, the agent -represented as an X- imagined optimal policies. Darker colours
indicate higher expected free energy.
5 Discussion
Thisstudyproposesanovelhigh-levelabstractionmodelinformedbybiologically
plausible principles mimicking key points of animal navigation strategies [33,1].
ByintegratingadynamiccognitivegraphwithinternalpositioningandanActive
InferenceFramework,ourmodelsuccessfullyexplorestheenvironmentandlearns
itsstructureinafewsteps,asexpectedfromanimals[32,26],facilitatingadaptive
learning and efficient exploration. Moreover, allowing the internal map to grow
with expected beliefs not only creates a map adapted to any environment di-
mension,shapeorobservationsbutalsoenhancesexplorationbycreatinghighly
uncertain states where the whereabouts are predictable but the corresponding
observationsaren’t.ComparativeexperimentswiththeClone-StructuredGraph
(CSCG) model [10] underscore the effectiveness of our approach in learning en-
vironmentstructureswithminimaldataandwithoutpriorknowledgeofspecific
observation dimensions. This is mainly due to our agent’s capacity to imagine
actions’ consequences and integrate them into its beliefs. Moving forward, it
would be interesting to increase the prediction range of new states to integrate
intothemodelanddeterminetheimpactonnavigation.Moreover,studyingthe
impact of a perfect memory on future policies and exploration efficiency, as well
as seeing how the agent fares when trying to reach a defined objective it has
prior upon in a familiar or novel environment would enhance the research. Fi-
nallydeployingthismodelinreal-worldscenariossuchasStreetLearn[20],based
on Google map observations, would approach further this mechanism to animal
behaviour and provide more conclusive evidence.
Exploring and Learning Structure 13
Acknowledgement
ThisresearchreceivedfundingfromtheFlemishGovernmentunderthe“Onder-
zoeksprogramma Artificiële Intelligentie (AI) Vlaanderen” programme.
References
1. Balaguer, J., Spiers, H., Hassabis, D., Summerfield, C.: Neural mechanisms of
hierarchicalplanninginavirtualsubwaynetwork.Neuron90,893–903(052016).
https://doi.org/10.1016/j.neuron.2016.03.037
2. Bush,D.,Barry,C.,Manson,D.,Burgess,N.:Usinggridcellsfornavigation.Neu-
ron87,507–520(2015),https://api.semanticscholar.org/CorpusID:7275119
3. Chevalier-Boisvert, M., Willems, L., Pal, S.: Minimalistic gridworld environment
for openai gym. https://github.com/maximecb/gym-minigrid (2018)
4. Dedieu, A., Lehrach, W., Zhou, G., George, D., Lázaro-Gredilla, M.: Learning
cognitive maps from transformer representations for efficient planning in partially
observed environments (2024)
5. Edvardsen, V., Bicanski, A., Burgess, N.: Navigating with grid and place cells in
cluttered environments. Hippocampus 30 (08 2019). https://doi.org/10.1002/
hipo.23147
6. Eichenbaum, H.: The hippocampus as a cognitive map ... of social space.
Neuron 87(1), 9–11 (2015). https://doi.org/https://doi.org/10.1016/j.
neuron.2015.06.013, https://www.sciencedirect.com/science/article/pii/
S0896627315005267
7. Epstein, R., Patai, E.Z., Julian, J., Spiers, H.: The cognitive map in humans:
Spatial navigation and beyond. Nature Neuroscience 20, 1504–1513 (10 2017).
https://doi.org/10.1038/nn.4656
8. Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., Doherty, J.O.,
Pezzulo, G.: Active inference and learning. Neuroscience & Biobehavioral Re-
views 68, 862–879 (2016). https://doi.org/https://doi.org/10.1016/j.
neubiorev.2016.06.022, https://www.sciencedirect.com/science/article/
pii/S0149763416301336
9. Friston, K., Parr, T., Zeidman, P.: Bayesian model reduction (2019)
10. George, D., Rikhye, R., Gothoskar, N., Guntupalli, J.S., Dedieu, A., Lázaro-
Gredilla, M.: Clone-structured graph representations enable flexible learning and
vicarious evaluation of cognitive maps. Nature Communications 12 (04 2021).
https://doi.org/10.1038/s41467-021-22559-5
11. Gershman, S.: Deconstructing the human algorithms for exploration. Cognition
173, 34–42 (12 2017). https://doi.org/10.1016/j.cognition.2017.12.014
12. Guntupalli, J.S., Raju, R., Kushagra, S., Wendelken, C., Sawyer, D., Deshpande,
I., Zhou, G., Lázaro-Gredilla, M., George, D.: Graph schemas as abstractions for
transferlearning,inference,andplanning(022023).https://doi.org/10.48550/
arXiv.2302.07350
13. Heins, R.C., Mirza, M.B., Parr, T., Friston, K., Kagan, I., Pooresmaeili, A.: Deep
activeinferenceandsceneconstruction.FrontiersinArtificialIntelligence3(2020).
https://doi.org/10.3389/frai.2020.509354, https://www.frontiersin.org/
articles/10.3389/frai.2020.509354
14. Hinman, J., Brandon, M., Climer, J., Chapman, W., Hasselmo, M.: Multiple run-
ning speed signals in medial entorhinal cortex. Neuron 91 (07 2016). https:
//doi.org/10.1016/j.neuron.2016.06.027
14 D. de Tinguy & al
15. Jacobs, J., Kahana, M., Ekstrom, A., Mollison, M., Fried, I.: A sense of direction
in human entorhinal cortex. Proceedings of the National Academy of Sciences of
theUnitedStatesofAmerica107,6487–92(032010).https://doi.org/10.1073/
pnas.0911213107
16. Jelinek, F.: Continuous speech recognition by statistical methods. Proceedings of
the IEEE 64(4), 532–556 (1976). https://doi.org/10.1109/PROC.1976.10159
17. Kaplan, R., Friston, K.: Planning and navigation as active inference (12 2017).
https://doi.org/10.1101/230599
18. Lajoie,P.,Hu,S.,Beltrame,G.,Carlone,L.:ModelingperceptualaliasinginSLAM
via discrete-continuous graphical models. CoRR abs/1810.11692 (2018), http:
//arxiv.org/abs/1810.11692
19. de Maele, T.V., Dhoedt, B., Verbelen, T., Pezzulo, G.: Bridging cognitive
maps: a hierarchical active inference model of spatial alternation tasks and the
hippocampal-prefrontal circuit (2023)
20. Mirowski, P., Banki-Horvath, A., Anderson, K., Teplyashin, D., Hermann, K.M.,
Malinowski, M., Grimes, M.K., Simonyan, K., Kavukcuoglu, K., Zisserman, A.,
Hadsell, R.: The streetlearn environment and dataset. CoRR abs/1903.01292
(2019), http://arxiv.org/abs/1903.01292
21. Neacsu,V.,Mirza,M.B.,Adams,R.A.,Friston,K.J.:Structurelearningenhances
concept formation in synthetic active inference agents. PLOS ONE 17(11), 1–34
(112022).https://doi.org/10.1371/journal.pone.0277199,https://doi.org/
10.1371/journal.pone.0277199
22. Parr, T., Pezzulo, G., Friston, K.: Active Inference: The Free Energy Principle
in Mind, Brain, and Behavior (03 2022). https://doi.org/10.7551/mitpress/
12441.001.0001
23. Peer, M., Brunec, I.K., Newcombe, N.S., Epstein, R.A.: Structuring knowledge
withcognitivemapsandcognitivegraphs.TrendsinCognitiveSciences25(1),37–
54 (2021). https://doi.org/https://doi.org/10.1016/j.tics.2020.10.004,
https://www.sciencedirect.com/science/article/pii/S1364661320302503
24. Peer, M., Brunec, I.K., Newcombe, N.S., Epstein, R.A.: Structuring knowledge
withcognitivemapsandcognitivegraphs.TrendsinCognitiveSciences25(1),37–
54 (2021). https://doi.org/https://doi.org/10.1016/j.tics.2020.10.004,
https://www.sciencedirect.com/science/article/pii/S1364661320302503
25. Raju,R.V.,Guntupalli,J.S.,Zhou,G.,Lázaro-Gredilla,M.,George,D.:Spaceisa
latentsequence:Structuredsequencelearningasaunifiedtheoryofrepresentation
in the hippocampus (2022)
26. Rosenberg, M., Zhang, T., Perona, P., Meister, M.: Mice in a labyrinth show
rapid learning, sudden insight, and efficient exploration. eLife 10, e66175
(jul 2021). https://doi.org/10.7554/eLife.66175, https://doi.org/10.7554/
eLife.66175
27. Schwartenbeck, P., Passecker, J., Hauser, T.U., FitzGerald, T.H., Kronbichler,
M., Friston, K.J.: Computational mechanisms of curiosity and goal-directed ex-
ploration. eLife 8, e41703 (may 2019). https://doi.org/10.7554/eLife.41703,
https://doi.org/10.7554/eLife.41703
28. Smith, R., Friston, K.J., Whyte, C.J.: A step-by-step tutorial on ac-
tive inference and its application to empirical data. Journal of Mathemat-
ical Psychology 107, 102632 (2022). https://doi.org/https://doi.org/10.
1016/j.jmp.2021.102632, https://www.sciencedirect.com/science/article/
pii/S0022249621000973
Exploring and Learning Structure 15
29. Smith,R.,Schwartenbeck,P.,Parr,T.,Friston,K.J.:Anactiveinferenceapproach
to modeling structure learning: Concept learning as an example case. Frontiers in
Computational Neuroscience 14 (2020). https://doi.org/10.3389/fncom.2020.
00041, https://www.frontiersin.org/articles/10.3389/fncom.2020.00041
30. Solstad, T., Boccara, C.N., Kropff, E., Moser, M.B., Moser, E.I.: Repre-
sentation of geometric borders in the entorhinal cortex. Science 322(5909),
1865–1868 (2008). https://doi.org/10.1126/science.1166466, https://www.
science.org/doi/abs/10.1126/science.1166466
31. de Tinguy, D., Van de Maele, T., Verbelen, T., Dhoedt, B.: Spatial and tempo-
ral hierarchy for autonomous navigation using active inference in minigrid envi-
ronment. Entropy 26(1), 83 (Jan 2024). https://doi.org/10.3390/e26010083,
http://dx.doi.org/10.3390/e26010083
32. Tyukin, I.Y., Gorban, A.N., Alkhudaydi, M.H., Zhou, Q.: Demystification of few-
shotandone-shotlearning.CoRRabs/2104.12174(2021),https://arxiv.org/
abs/2104.12174
33. Zhao,M.:Humanspatialrepresentation:Whatwecannotlearnfromthestudiesof
rodent navigation. Journal of Neurophysiology 120 (08 2018). https://doi.org/
10.1152/jn.00781.2017

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Exploring and Learning Structure: Active Inference Approach in Navigational Agents"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
