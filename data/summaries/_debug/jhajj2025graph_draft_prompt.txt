=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Graph Distance as Surprise: Free Energy Minimization in Knowledge Graph Reasoning
Citation Key: jhajj2025graph
Authors: Gaganpreet Jhajj, Fuhua Lin

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Abstract: Inthiswork,weproposethatreasoninginknowledgegraph(KG)networkscanbeguidedbysurpriseminimization.
Entitiesthatarecloseingraphdistancewillhavelowersurprisethanthosefartherapart. Thisconnectsthe
FreeEnergyPrinciple(FEP)[1]fromneurosciencetoKGsystems,wheretheKGservesastheagentâ€™sgenerative
model.Weformalizesurpriseusingtheshortest-pathdistanceindirectedgraphsandprovideaframeworkfor
KG-basedagents.Graphdistanceappearsingraphneuralnetworksasmessagepassingdepthandinmodel-based
reinforcementlearningasworl...

Key Terms: graph, surprise, energy, knowledge, reasoning, distance, entities, minimization, generative, high

=== FULL PAPER TEXT ===

Graph Distance as Surprise: Free Energy Minimization in
Knowledge Graph Reasoning
GaganpreetJhajj1,*, FuhuaLin1
1SchoolofComputingandInformationSystems,AthabascaUniversity,Canada
Abstract
Inthiswork,weproposethatreasoninginknowledgegraph(KG)networkscanbeguidedbysurpriseminimization.
Entitiesthatarecloseingraphdistancewillhavelowersurprisethanthosefartherapart. Thisconnectsthe
FreeEnergyPrinciple(FEP)[1]fromneurosciencetoKGsystems,wheretheKGservesastheagentâ€™sgenerative
model.Weformalizesurpriseusingtheshortest-pathdistanceindirectedgraphsandprovideaframeworkfor
KG-basedagents.Graphdistanceappearsingraphneuralnetworksasmessagepassingdepthandinmodel-based
reinforcementlearningasworldmodeltrajectories.Thiswork-in-progressstudyexploreswhetherdistance-based
surprisecanextendrecentworkshowingthatsyntaxminimizessurpriseandfreeenergyviatreestructures[2].
Keywords
KnowledgeGraphs,GraphNeuralNetworks,ActiveInference,SemanticGrounding,Agents
1. Introduction
TheFreeEnergyPrinciple(FEP)suggeststhatbiologicalsystemsminimizesurprisebymaintaining
accurate world models [1, 3, 4]. Recently, Murphy et al. [2] demonstrated that syntactic operations
minimizesurprisethroughshallowtreestructures. Theyquantifiedsurpriseviatreedepth(geometric
complexity)andKolmogorovcomplexity(algorithmiccomplexity),approximatedthroughLempel-Ziv
compression[5,6].
InFEP,agentsminimizevariationalfreeenergyğ¹ = âˆ’logğ‘ƒ(ğ‘œ,ğ‘ )âˆ’ğ»[ğ‘„(ğ‘ )],whereğ‘œrepresents
observations, ğ‘  hidden states, ğ‘ƒ the generative model, and ğ‘„ the agentâ€™s beliefs [1]. The first term,
âˆ’logğ‘ƒ(ğ‘œ,ğ‘ ), quantifies surprise: entities with high probability under the generative model (high
ğ‘ƒ(ğ‘œ,ğ‘ ))yieldlowsurprise(lowâˆ’logğ‘ƒ(ğ‘œ,ğ‘ )). Forsyntactictrees,Murphyetal. [2]usedtreedepthto
proxythisprobability;weextendthisprincipletogeneralgraphsusingshortest-pathdistance.
Inactiveinference,minimizingfreeenergydrivesbothperception(updatingbeliefsğ‘„(ğ‘ ))andaction
(selecting policies that reduce uncertainty) [3]. We apply this principle to KG reasoning: entities at
shorter graph distances have a higher probability under the agentâ€™s graph-based generative model.
Thecentralquestionweaddressis: givenaKGservingasanagentâ€™sgenerativemodel,whichentity
groundings are plausible for a query in context? We propose one principled approach: plausibility
inverselycorrelateswithgraphdistance.
Knowledge graphs (KGs) are increasingly integrated with modern AI agents, with the ability to
improve reasoning, memory, and planning [7, 8, 9, 10, 11, 12, 13, 14, 15, 16]. Unlike syntactic tree
structures,KGsaredirectedgraphsthatcancontaincyclesandmultiplepathsbetweennodes(entities).
In this preliminary work, we propose that surprise in KG reasoning corresponds to graph distance,
wheretheKGservesastheagentâ€™sgenerativemodel. Entitiesthatrequireshorterpathsfromcontext
areunsurprising,whereasdistantordisconnectedentitiesaremoresurprising. Thisisunlikesurprise-
driven exploration in RL [17, 18], where agents maximize surprise to explore, FEP agents minimize
surprisebymaintainingaccurategenerativemodels. OurworkconnectstheFEPtopracticalKGsystems
throughshortest-pathdistance,providingtheoreticalfoundationsforgraphneuralnetworks[19,20,21]
andmodel-basedreinforcementlearning[22,23].
NORAâ€™25:1stWorkshoponKnowledgeGraphs&AgenticSystemsInterplayco-locatedwithNeurIPS,Dec.1,2025,MexicoCity,
Mexico
$gjhajj1@learn.athabascau.ca(G.Jhajj);oscarl@athabascau.ca(F.Lin)
(cid:26)0000-0001-5817-0297(G.Jhajj);0000-0002-5876-093X(F.Lin)
Â©2025Copyrightforthispaperbyitsauthors.UsepermittedunderCreativeCommonsLicenseAttribution4.0International(CCBY4.0).
2. From Syntax to Semantics
Murphy et al. [2] quantified syntactic surprise via tree depth. We extend this to arbitrary directed
graphswithcycles. GivenaKGğ’¢ = (â„°,â„›,ğ’¯)withentitiesâ„°,relationsâ„›,andtriplesğ’¯ âŠ† â„° Ã—â„›Ã—â„°,
geometricsurpriseis:
â§
â¨minğ‘‘
ğ’¢
(ğ‘,ğ‘’) ifpathexists
ğ‘†geo(ğ‘’ | ğ¶) = ğ‘âˆˆğ¶ (1)
â©ğ›¼ otherwise
whereğ‘‘ (ğ‘,ğ‘’)istheshortestdirectedpathlengthfromcontextğ‘ âˆˆ ğ¶ toentityğ‘’(computedviaBFS,
ğ’¢
AppendixB),andğ›¼isahyperparameterpenalizingdisconnection. Inourworkedexample,wesetğ›¼ = 5;
ingeneral,ğ›¼shouldexceedthegraphâ€™sdiameter(longestshortest-pathdistance)toensuredisconnected
entitiesalwayshavehighersurprisethananyconnectedentity. Combinedwithalgorithmiccomplexity
[2]:
ğ¹(ğ‘’ | ğ¶) = ğ‘†geo(ğ‘’ | ğ¶)+ğœ†ğ¾(ğœ‹
ğ¶â†’ğ‘’
) (2)
whereğ¾(ğœ‹ )isKolmogorovcomplexityoftherelationpath,approximatedviaLempel-Zivcompres-
ğ¶â†’ğ‘’
sion,andğœ†weightsthecomponents. Fortrees,thisrecoversMurphyâ€™streedepth;forgeneralgraphs,it
handlescyclesnaturally.
ConnectiontoFEP:UnderFEP,agentsminimizeğ¹ = âˆ’logğ‘ƒ(ğ‘œ,ğ‘ )âˆ’ğ»[ğ‘„(ğ‘ )][1]. Interpreting
theKGastheagentâ€™sgenerativemodel,wepositâˆ’logğ‘ƒ(ğ‘’ | ğ¶) âˆ ğ‘‘ (ğ¶,ğ‘’): shorterdistancesindicate
ğ’¢
higherprobability.Thusğ‘†geo implementsthesurpriseterm,whileğ¾(ğœ‹)approximatesğ»[ğ‘„(ğ‘ )].Figure1
illustratesthiswithapoliticalKGexample(detailedcalculationsinAppendixA).
A. Murphy et al.: Syntax (Trees) F = GraphDepth + ?K B. Our Work: Semantics (KGs)
Prime
Trudeau holdsPosition depth = 2
Minister
A hasLeader
Canada successor
Tree Depth (context)
B C hasLeader
Prime
Harper holdsPosition
Minister depth = 2
D E F G
No Path
Biden depth = ?
Figure1:Extendingsurprisefromtreestoknowledgegraphs.FollowingstandardKGdesign(e.g.,Wikidata),
wemodelâ€œPrimeMinisterâ€asapositionnode.Givencontextâ€œCanadaâ€,leaders(Trudeau,Harper)areatdistance
1,thepositionnodeatdistance2,whiledisconnectedentities(Biden)havedistanceâˆ.Thesuccessorrelation
demonstratescyclehandling.
3. Theoretical Justification
Threeprinciplesjustifytheshortest-pathdistance: (1)Propergeneralization: Fortrees,itrecovers
Murphyâ€™streedepth. (2)Least-action: Shortestpathsminimizecumulativecost,aligningwithactive
inferencewhereagentsminimizeexpectedfreeenergy[3]. (3)Computationalgrounding: InGNNs,
ğ‘˜ message-passingiterationsaggregateğ‘˜-hopneighborhoods[19,21];minimizingiterationsminimizes
distanceandsurprise. Cyclesposenoissue: FEPaccommodatescircularcausality[24],andBFShandles
cyclesviavisitedsets(AppendixB).
4. Implications and Future Work
Thiswork-in-progressconnectsFEPfromneurosciencetoKGreasoninginAIsystems. Thepresented
frameworkofferspracticalimplications: (1)Entitygrounding: LLM-KGsystemscouldrankcandidate
entitygroundingsbycomputingğ‘†geo viaBFSfromdiscoursecontextentities,preferringgroundings
withlowerfreeenergy[10,9];(2)KGembeddings: embeddingmethodscouldpreservedistance-based
surprisestructure[25];(3)GNNarchitecture: depthcouldbeselectedtobalancecomputationalcost
againstthesurprisehorizonneededforatask.
FutureworkincludesempiricalvalidationonbenchmarkKGdatasets(FB15k-237[26],YAGO[27]),
comparisonwithhumansemanticsimilarityjudgments,integrationwithexistingKGreasoningsystems
[10,28,9],andextensiontotemporalKGs.
Thisworkrepresentsanearly-stageexplorationofapplyingFEPtoknowledgegraphreasoning.While
weproposedtheshortest-pathdistanceasaprincipledformalizationofsurprise,otherformulations
maybemoreelegantorpractical.
Weaimtopresentthiscontributionasaninitialresearchdirectionratherthanadefinitivesolution.
Wealsoencouragethecommunitytodevelopcomplementaryorimprovedapproachestoconnecting
FEPprincipleswithgraph-basedreasoning.
Acknowledgments
We acknowledge the support of the Natural Sciences and Engineering Research Council of Canada
(NSERC),AlbertaInnovates,AlbertaAdvancedEducation,andAthabascaUniversity,Canada. Wewould
alsoliketothankthereviewersfortheirsuggestionsonhowtoimprovethiswork.
Declaration on Generative AI
Duringthepreparationofthiswork,theauthor(s)usedGrammarlyandClaude(Anthropic)forGrammar
andspellingchecks.
References
[1] K.Friston, Thefree-energyprinciple: aunifiedbraintheory?, NatureReviewsNeuroscience11
(2010)127â€“138.URL:https://www.nature.com/articles/nrn2787.doi: .
10.1038/nrn2787
[2] E.Murphy,E.Holmes,K.Friston, Naturallanguagesyntaxcomplieswiththefree-energyprinciple,
Synthese203(2024)154.URL:https://link.springer.com/10.1007/s11229-024-04566-3.doi:
10.1007/
.
s11229-024-04566-3
[3] K. Friston, T. FitzGerald, F. Rigoli, P. Schwartenbeck, G. Pezzulo, Active Inference: A Process
Theory, NeuralComputation29(2017)1â€“49.URL:https://direct.mit.edu/neco/article/29/1/1-49/
8207.doi: .
10.1162/NECO_a_00912
[4] T. Parr, G. Pezzulo, K. J. Friston, Active Inference: The Free Energy Principle in Mind,
Brain, and Behavior, The MIT Press, 2022. URL: https://direct.mit.edu/books/book/5299/
Active-InferenceThe-Free-Energy-Principle-in-Mind. doi:
10.7551/mitpress/12441.001.
.
0001
[5] M. Li, P. VitÃ¡nyi, An Introduction to Kolmogorov Complexity and Its Applications, Texts in
ComputerScience,SpringerNewYork,NewYork,NY,2008.URL:http://link.springer.com/10.1007/
978-0-387-49820-1.doi: .
10.1007/978-0-387-49820-1
[6] J. Ziv, A. Lempel, A universal algorithm for sequential data compression, IEEE Transactions
onInformationTheory23(1977)337â€“343.URL:https://ieeexplore.ieee.org/document/1055714/.
doi: .
10.1109/TIT.1977.1055714
[7] L.Chen,P.Tong,Z.Jin,Y.Sun,J.Ye,H.Xiong, Plan-on-graph: self-correctingadaptiveplanningof
largelanguagemodelonknowledgegraphs, in: Proceedingsofthe38thInternationalConference
onNeuralInformationProcessingSystems,NIPSâ€™24,CurranAssociatesInc.,RedHook,NY,USA,
2025.Event-place: Vancouver,BC,Canada.
[8] Y.Cui,Z.Sun,W.Hu, Aprompt-basedknowledgegraphfoundationmodelforuniversalin-context
reasoning, in: Proceedingsofthe38thInternationalConferenceonNeuralInformationProcessing
Systems,NIPSâ€™24,CurranAssociatesInc.,RedHook,NY,USA,2025.Event-place: Vancouver,BC,
Canada.
[9] X. He, Y. Tian, Y. Sun, N. V. Chawla, T. Laurent, Y. LeCun, X. Bresson, B. Hooi, G-Retriever:
Retrieval-AugmentedGenerationforTextualGraphUnderstandingandQuestionAnswering, in:
The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL:
https://openreview.net/forum?id=MPJ3oXtTZl.
[10] G. Jhajj, X. Zhang, J. R. Gustafson, F. Lin, M. P.-C. Lin, Educational Knowledge Graph Cre-
ation and Augmentation via LLMs, in: A. Sifaleras, F. Lin (Eds.), Generative Intelligence
and Intelligent Tutoring Systems, Springer Nature Switzerland, Cham, 2024, pp. 292â€“304.
doi: .
10.1007/978-3-031-63031-6_25
[11] G. Jhajj, J. R. D. Gustafson, R. Morland, C. E. Gutierrez, M. P.-C. Lin, M. A. A. Dewan, F. Lin,
NeuromorphicKnowledgeRepresentation: SNN-BasedRelationalInferenceandExplainabilityin
KnowledgeGraphs, in: S.Graf,A.Markos(Eds.),GenerativeSystemsandIntelligentTutoring
Systems,volume15724,SpringerNatureSwitzerland,Cham,2026,pp.159â€“165.URL:https://link.
springer.com/10.1007/978-3-031-98284-2_13.doi: .
10.1007/978-3-031-98284-2_13
[12] G.Jhajj,Y.Nomura, JackandthebeansTALK:Towardsquestionansweringinplantbiology, in:
EighthWideningNLPWorkshop(WiNLP2024)PhaseII,2024.URL:https://openreview.net/forum?
id=0DlJEPHHKe.
[13] R.D.Morland,F.Lin, Anadaptableclient-serverarchitectureforgeneratingeducationalcontent
usinglargelanguagemodels, BulletinoftheTechnicalCommitteeonLearningTechnology(ISSN:
2306-0212)25(2025)42â€“49.
[14] J.R.D.Gustafson,G.Jhajj,X.Zhang,F.O.Lin, Enhancingproject-basedlearningwithagenai
toolbasedonretrieval: Augmentedgenerationandknowledgegraphs, in: AIApplicationsand
StrategiesinTeacherEducation,IGIGlobal,2025,pp.161â€“194.
[15] G.Jhajj,F.Lin, AugmentingjapaneselanguageacquisitionviaLLMsandASR, in: IEEESmart
WorldCongress2025(IEEESWCâ€™25),Calgary,Canada,2025,p.3.84.
[16] M.R.Kabir,F.Lin, Anllm-poweredadaptivepracticingsystem., in: LLM@AIED,2023,pp.43â€“52.
[17] D. Pathak, P. Agrawal, A. A. Efros, T. Darrell, Curiosity-driven exploration by self-supervised
prediction, in: Proceedingsofthe34thInternationalConferenceonMachineLearning-Volume
70,ICMLâ€™17,JMLR.org,2017,p.2778â€“2787.
[18] T.Rakotoaritina,G.Jhajj,C.Reinke,K.Doya, Information-theoreticformulationandcombination
ofintrinsicrewards: Novelty,surpriseandempowerment, in: SeventhInternationalWorkshop
onIntrinsicallyMotivatedOpen-endedLearning,2025.URL:https://openreview.net/forum?id=
WN7ofwXNvv.
[19] T.N.Kipf,M.Welling, Semi-SupervisedClassificationwithGraphConvolutionalNetworks, in:
InternationalConferenceonLearningRepresentations,2017.URL:https://openreview.net/forum?
id=SJU4ayYgl.
[20] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. Van Den Berg, I. Titov, M. Welling, Modeling Re-
lational Data with Graph Convolutional Networks, in: A. Gangemi, R. Navigli, M.-E. Vidal,
P. Hitzler, R. Troncy, L. Hollink, A. Tordai, M. Alam (Eds.), The Semantic Web, volume 10843,
SpringerInternationalPublishing,Cham,2018,pp.593â€“607.URL:https://link.springer.com/10.
1007/978-3-319-93417-4_38.doi: .
10.1007/978-3-319-93417-4_38
[21] P.W.Battaglia,J.B.Hamrick,V.Bapst,A.Sanchez-Gonzalez,V.Zambaldi,M.Malinowski,A.Tac-
chetti, D. Raposo, A. Santoro, R. Faulkner, C. Gulcehre, F. Song, A. Ballard, J. Gilmer, G. Dahl,
A.Vaswani,K.Allen,C.Nash,V.Langston,C.Dyer,N.Heess,D.Wierstra,P.Kohli,M.Botvinick,
O.Vinyals,Y.Li,R.Pascanu,Relationalinductivebiases,deeplearning,andgraphnetworks,2018.
URL:http://arxiv.org/abs/1806.01261.doi: ,arXiv:1806.01261.
10.48550/arXiv.1806.01261
[22] R.S.Sutton,A.G.Barto,ReinforcementLearning: AnIntroduction,ABradfordBook,Cambridge,
MA,USA,2018.
[23] B.Millidge,A.Tschantz,A.K.Seth,C.L.Buckley, OntheRelationshipBetweenActiveInference
andControlasInference, in: T.Verbelen, P.Lanillos, C.L.Buckley, C.DeBoom(Eds.), Active
Inference, volume 1326, Springer International Publishing, Cham, 2020, pp. 3â€“11. URL: https:
//link.springer.com/10.1007/978-3-030-64919-7_1.doi: .
10.1007/978-3-030-64919-7_1
[24] K. Friston, M. Levin, B. Sengupta, G. Pezzulo, Knowing oneâ€™s place: a free-energy approach
to pattern regulation, Journal of The Royal Society Interface 12 (2015) 20141383. URL: https:
//royalsocietypublishing.org/doi/10.1098/rsif.2014.1383.doi: .
10.1098/rsif.2014.1383
[25] A.Bordes,N.Usunier,A.Garcia-DurÃ¡n,J.Weston,O.Yakhnenko, Translatingembeddingsfor
modelingmulti-relationaldata, in: Proceedingsofthe27thInternationalConferenceonNeural
InformationProcessingSystems-Volume2,NIPSâ€™13,CurranAssociatesInc.,RedHook,NY,USA,
2013,pp.2787â€“2795.Event-place: LakeTahoe,Nevada.
[26] M.Schlichtkrull,T.N.Kipf,P.Bloem,R.v.d.Berg,I.Titov,M.Welling, Modelingrelationaldata
withgraphconvolutionalnetworks, in: Europeansemanticwebconference,Springer,2018,pp.
593â€“607.
[27] F.M.Suchanek,G.Kasneci,G.Weikum, Yago: acoreofsemanticknowledge, in: Proceedingsof
the16thInternationalConferenceonWorldWideWeb,WWWâ€™07,AssociationforComputing
Machinery,NewYork,NY,USA,2007,p.697â€“706.URL:https://doi.org/10.1145/1242572.1242667.
doi: .
10.1145/1242572.1242667
[28] Y. Deng, C. Ye, Z. Huang, M. D. Ma, Y. Kou, W. Wang, GraphVis: Boosting LLMs with Visual
KnowledgeGraphIntegration, in: TheThirty-eighthAnnualConferenceonNeuralInformation
ProcessingSystems,2024.URL:https://openreview.net/forum?id=haVPmN8UGi.
A. Worked Example: Free Energy Calculations
WedemonstratefreeenergycalculationsusingtheCanadianPrimeMinisterknowledgegraphfrom
Figure1.
A.1. ScenarioandKnowledgeGraph
Consider query â€œWho is the Prime Minister?â€ with context ğ¶ = {Canada}. The knowledge graph
contains:
Entities: â„° = {Canada,Trudeau,Harper,PrimeMinister,Biden}
Relations: (Canada,
hasLeader
, Trudeau), (Canada,
hasLeader
, Harper), (Trudeau,
,PrimeMinister),(Harper, ,PrimeMinister),(Trudeau, ,
holdsPosition holdsPosition successor
Harper),(Harper, ,Trudeau)
predecessor
Thesuccessor/predecessorrelationsformacycle: Trudeauâ†”Harper. Importantly, Bidenhasno
directedpathfromCanada(separatesubgraph).
A.2. ComputingGeometricSurprise
UsingBFSfromCanada,wecomputeshortestdirectedpaths:
â€¢ ğ‘‘(Canada,Trudeau) = 1(directvia hasLeader )
â€¢ ğ‘‘(Canada,Harper) = 1(directvia hasLeader )
â€¢ ğ‘‘(Canada,PrimeMinister) = 2(via hasLeader then holdsPosition )
â€¢ ğ‘‘(Canada,Biden) = âˆ(nopath)
Therefore: ğ‘†geo(Trudeau) = ğ‘†geo(Harper) = 1,ğ‘†geo(PrimeMinister) = 2,andğ‘†geo(Biden) = ğ›¼ =
5.
ThecyclebetweenTrudeauandHarperdoesnotaffectdistances: BFSselectstheshortestpath(direct
edge)andhandlescyclesviavisitedset(AppendixB).
A.3. ComputingAlgorithmicComplexity
Foreachgrounding,weestimateKolmogorovcomplexityviarelationpathpatterns:
Trudeau&Harper: Pathsğœ‹ = [hasLeader]usefrequentrelations,yieldinghighcompression(low
ğ¾(ğœ‹)).
PrimeMinisternode: Pathğœ‹ = [hasLeader,holdsPosition]usesstandardrole-modelingpat-
terns,alsoyieldinglowğ¾(ğœ‹).
Biden: NopathfromCanada. Thegroundingrequiresirregularcross-countryreasoningnotrepre-
sentedinthegraph(highğ¾(ğœ‹)).
A.4. FreeEnergyResults
Combiningcomponentswithğœ† = 1:
Entity ğ‘†geo ğ¾(ğœ‹) ğ¹
Trudeau 1 Low âˆ¼1.3
Harper 1 Low âˆ¼1.3
Biden 5 High âˆ¼5.5
Interpretation: Realgroundings(Trudeau,Harper)exhibitlowfreeenergy: (1)shortdistance(1
hop), (2) regular relation patterns. The impossible grounding (Biden) exhibits high free energy: (1)
disconnection(nopath),(2)irregularpattern. TheframeworkcorrectlyidentifiesbothTrudeauand
Harperasplausible(bothwereCanadianPMs)whilerejectingBiden(USpresident).
We focus on entity groundings (Trudeau, Harper, Biden) rather than the position node itself, as
queriesaboutleadershiptypicallyseekindividualsratherthanabstractroles. ThePrimeMinisternode,
atdistance2,wouldhaveintermediatesurprise(ğ‘†geo = 2,ğ¹ â‰ˆ 2.3),butisnotadirectanswertoâ€œWho
isthePrimeMinister?â€ Thisdemonstrateshowourframeworknaturallydistinguishesbetweenentities
atdifferentlevelsofabstractioninreifiedKGschemas.
This demonstrates three key properties: (1) cycles handled naturally, (2) multiple valid answers
coexistwithequalsurprise,(3)disconnectedentitiescorrectlypenalized.
B. Mathematical Details
B.1. Breadth-FirstSearchAlgorithm
Givendirectedgraphğ’¢ = (â„°,â„›,ğ’¯)andcontextğ¶ âŠ† â„°,wecomputeğ‘†geo(ğ‘’ | ğ¶)viaBFS:
Algorithm1ComputeGeometricSurprise
Require: Knowledgegraphğ’¢,contextğ¶,targetentityğ‘’
Ensure: Geometricsurpriseğ‘†geo(ğ‘’ | ğ¶)
1: Initialize: ğ‘‘(ğ‘) â† 0forallğ‘ âˆˆ ğ¶;ğ‘‘(ğ‘£) â† âˆforğ‘£ âˆˆ/ ğ¶
2: ğ‘„ â† ğ¶ (queue),ğ‘‰ â† ğ¶ (visitedset)
3: whileğ‘„ Ì¸= âˆ…do
4: ğ‘¢ â†dequeuefromğ‘„
5: foreachoutgoingedge(ğ‘¢,ğ‘Ÿ,ğ‘£) âˆˆ ğ’¯ do
6: if ğ‘£ âˆˆ/ ğ‘‰ then
7: ğ‘‘(ğ‘£) â† ğ‘‘(ğ‘¢)+1
8: ğ‘‰ â† ğ‘‰ âˆª{ğ‘£},enqueueğ‘£ toğ‘„
9: endif
10: endfor
11: endwhile
12: return ğ‘‘(ğ‘’)ifğ‘‘(ğ‘’) < âˆ,elseğ›¼
Properties: (1)Correctness: BFSfindsshortestpathsinğ‘‚(|â„°|+|ğ’¯|)time. (2)Cyclehandling: Visited
setğ‘‰ preventsre-visitingnodes,ensuringtermination. (3)Directionality:Onlyoutgoingedgesfollowed,
respectingdirection.
B.2. KolmogorovComplexityApproximation
Weapproximateğ¾(ğœ‹ )viaLempel-Zivcompression: (1)Extractrelationsequenceğœ‹ = [ğ‘Ÿ ,...,ğ‘Ÿ ]
ğ¶â†’ğ‘’ 1 ğ‘˜
fromshortestpath. (2)Encodeasstring(e.g.,â€œpm|successorâ€). (3)CompresswithLZ77. (4)Compute
ratioğ¾(ğœ‹) = compressed/original.
Interpretation: Regularpatterns(frequentrelations,shortsequences)achievehighcompression
(lowğ¾). Irregularpatterns(rarerelations,longsequences)achievelowcompression(highğ¾). This
approximates Kolmogorov complexity, which is uncomputable [5]. Murphy et al. [2] use the same
approximationforsyntacticpatterns.
B.3. ConnectiontoActiveInference
Inactiveinference,agentsminimizeexpectedfreeenergyğº(ğœ‹)[3,4]:
ğº(ğœ‹) = ğ· [ğ‘„(ğ‘œ|ğœ‹)â€–ğ‘ƒ(ğ‘œ)]+E [ğ»[ğ‘ƒ(ğ‘ |ğ‘œ)]] (3)
ğ¾ğ¿ ğ‘„(ğ‘œ|ğœ‹)
âŸ â âŸ â
Pragmatic Epistemic
balancingpragmaticvalue(exploitation)andepistemicvalue(exploration).
Pragmaticvalue: Entitiesatshorterdistancesaremorelikely: ğ‘ƒ(observeğ‘’ | ğ¶)increasesasğ‘†geo
decreases,makinglow-distanceentitiespreferredforgoal-directedactions.
Epistemicvalue: Entitiesatlongerdistancesprovidehigherinformationgain: observingdistant
entitiesreducesuncertaintyaboutunexploredgraphregions,makinghigh-distanceentitiespreferred
forexploration.
Ourğ‘†geo implementspragmaticvalue: lowsurpriseentitiespreferredforexploitation. Extensions
couldweightdistanceinverselyforepistemicvalue,valuinghigh-surpriseentitiesforexploration.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Graph Distance as Surprise: Free Energy Minimization in Knowledge Graph Reasoning"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   âŒ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   âœ… GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   âŒ BAD: Repeating the same claim 3+ times with slight variations
   âœ… GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
