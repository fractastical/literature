=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Integrating large language models and active inference to understand eye movements in reading and dyslexia
Citation Key: donnarumma2023integrating
Authors: Francesco Donnarumma, Mirco Frosolone, Giovanni Pezzulo

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2023

Abstract: We present a novel computational model employing hierarchical active inference to simulate read-
ing and eye movements. The model characterizes linguistic processing as inference over a hierarchical
generativemodel,facilitatingpredictionsandinferencesatvariouslevelsofgranularity,fromsyllablesto
sentences. Ourapproachcombinesthestrengthsoflargelanguagemodelsforrealistictextualpredictions
and active inference for guiding eye movements to informative textual information, enabling the testing
ofpred...

Key Terms: dyslexia, movements, large, language, processing, models, reading, hierarchical, understand, inference

=== FULL PAPER TEXT ===

Integrating large language models and active inference to
understand eye movements in reading and dyslexia
Francesco Donnarumma1,†, Mirco Frosolone1,†, and Giovanni Pezzulo1,*
1Institute of Cognitive Sciences and Technologies, National Research Council, Rome, Italy
†Shared first authorship
*Corresponding author: giovanni.pezzulo@istc.cnr.it
August 11, 2025
Abstract
We present a novel computational model employing hierarchical active inference to simulate read-
ing and eye movements. The model characterizes linguistic processing as inference over a hierarchical
generativemodel,facilitatingpredictionsandinferencesatvariouslevelsofgranularity,fromsyllablesto
sentences. Ourapproachcombinesthestrengthsoflargelanguagemodelsforrealistictextualpredictions
and active inference for guiding eye movements to informative textual information, enabling the testing
ofpredictions. Themodelexhibitsproficiencyinreadingbothknownandunknownwordsandsentences,
adhering to the distinction between lexical and nonlexical routes in dual route theories of reading. Our
modelthereforeprovidesanovelapproachtounderstandthecognitiveprocessesunderlyingreadingand
eye movements, within a predictive processing framework. Furthermore, our model can potentially aid
in understanding how maladaptive predictive processing can produce reading deficits associated with
dyslexia. As a proof of concept, we show that attenuating the contribution of priors during the reading
process leads to incorrect inferences and a more fragmented reading style, characterized by a greater
number of shorter saccades, aligning with empirical findings regarding eye movements in dyslexic indi-
viduals. In summary, our model represents a significant advancement in comprehending the cognitive
processesinvolvedinreadingandeyemovements,withpotentialimplicationsforunderstandingdyslexia
in terms of maladaptive inference.
Keywords: Hierarchical active inference, Predictive coding, Large Language Models, Reading,
Dyslexia
1 Introduction
Processing natural language – encompassing understanding, reading, and producing linguistic content –
represents a fundamental ability of our species. Extensive research in psychology, neuroscience, linguis-
tics, and machine learning has explored the intricate ways we process natural language. Neuroscientific
studieshaverevealedthatnaturallanguageprocessingisinherentlyhierarchical,involvingmultiplebrain
regions and the integration of various sensory inputs [8, 7, 60]. This hierarchical processing spans from
1
5202
guA
8
]CN.oib-q[
3v14940.8032:viXra
individual letters, phonemes, and words to complete sentence comprehension, with a hierarchy of brain
areas actively maintaining these elements in working memory across multiple time scales [32].
Recentstudies have increasinglysupportedthe idea ofhierarchical predictive coding asaformal the-
ory describing perception as an inferential process, involving reciprocal exchanges between predictions
andpredictionerrorsacrossbrainhierarchies[69,21]. Inparticular,variouscomputationalneuroimaging
studies employing large language models (LLMs) during linguistic tasks have provided compelling evi-
dence for both the predictive nature of language processing and the prediction hierarchies proposed by
predictive coding [29, 77, 1, 90, 10, 9, 34]. A recent computational neuroimaging study investigated hu-
manelectrocorticographic(ECoG)responsestonarrativesusingLLMstrainedtopredictthenextword.
Remarkably, the study revealed that like the LLMs, the brain engages in next-word prediction before
wordonset,computespredictionerrorsignals,andutilizeslatentrepresentationsofwords(embeddings)
contextualized based on the sequence of prior words [29].
Converging evidence emerges from three recent fMRI studies utilizing deep learning models during
linguistictasks. Thesestudiesnotonlyconfirmthepredictivenatureoflanguageprocessingbutalsolend
supporttothepredictionhierarchiesproposedbypredictivecoding. ThefirstfMRIstudy[77]traineda
LLMtopredictthenextwordatmultipletimescales,identifyingeventboundariesashighsurprise(also
explored in [1, 90, 10]). Analyzing human functional magnetic resonance data during story listening,
the study revealed an event-based hierarchy of surprise signals evolving along temporoparietal regions,
with surprise signals gating bottom-up and top-down connectivity across neighboring time scales. Fur-
thermore,anotherfMRIstudyprovidedcompellingevidenceofthebrain’sabilitytopredictahierarchy
of representations spanning multiple timescales in the future [9]. Enhancing LLMs with the capability
to predict beyond the next word increased their fit with human data. The study also highlighted the
hierarchical organization of brain predictions, ranging from temporal cortices predicting shorter-range
representations (e.g., the next word) to frontoparietal cortices predicting higher-level, longer-range, and
more contextual representations.
Notably, a separate fMRI study reported that evoked brain responses to words are influenced by
linguistic predictions and a metric of unexpectedness, closely aligning with the hierarchical predictive
processingschemes,wherelower-levelpredictionsareinformedbyhigher-levelpredictions[34]. Addition-
ally,thestudydemonstratedthatthesehierarchicalpredictionscanbewell-alignedwithstandardlevels
of analysis in psycholinguistics, including meaning, grammar, words, and speech sounds, reinforcing the
validityofthestandarddecomposition. Takentogether,thesestudies,alongsideothers[43,88,42,89,19],
provide compelling support for the significance of prediction and hierarchical predictive coding in lan-
guageprocessing. Moreover,theyunderscorethegrowingrelevanceofLLMsincomprehendinglinguistic
processing [29].
Despitetheseadvancements,theabovestudieshaveprimarilyfocusedonLLMsthatpassivelyreceive
sensory information, rather than actively searching for it. However, linguistic tasks, such as reading
written text and listening to speech, are inherently active processes [18, 51, 27]. For example, during
reading, eye movements (saccades) actively guide attention to relevant parts of the text, rather than
processing every piece of text linearly. This active reading process suggests that saccades play a crucial
role in hypothesis testing, selecting informative parts of the text to test predictions [18, 51, 14, 24].
Several reading models incorporating eye movements, such as E-Z Reader [72], SWIFT [17, 79, 65],
U¨ber-Reader[71,87],Glenmore[73,74],SEAM[66],therationalmodelofeyemovements[2]andOB1-
Reader[81]significantlyadvancedourunderstandingoftheroleofreadingdynamics. Thesemodelshave
successfully replicated numerous empirical findings, highlighting how word-level attributes like length,
frequency,andpredictabilityimpactreadingdynamics. However,thesemodelsdonotexploitthegener-
2
ativecapabilitiesofrecentlargelanguagemodelsanddonotfullyalignwiththeaforementionedevidence
of hierarchical predictive processing supporting reading and eye movements.
Inthispaper,weproposeanovelcomputationalmodelthatunifieshierarchicalpredictiveprocessing
and hypothesis testing during reading by integrating the LLM BERT (Bidirectional Encoder Represen-
tations from Transformers) [13] with active inference [23, 55]: a theory of perception and action based
on Bayesian principles, whereby agents minimize expected surprise (or variational free energy) through
iterativebeliefupdatingandactionselection. Ourmodelviewsreadingasanactive(Bayesian)inference
problem, employing a hierarchical generative model to represent causal relationships between textual
elements at different levels (letters, syllables, words, and sentences). By generating predictions at each
level and testing them through saccades, our model actively simulates reading.
The model incorporates three significant insights. Firstly, it conceptualizes linguistic processing as
inference, employing a hierarchical generative model that allows for predicting and inferring at different
timescales, such as syllables, words, and sentences. By integrating the LLM BERT [13] at the highest
hierarchicallevel,ourmodelcanhandlerealisticreadingtaskseffectively,processingsentencesofarbitrary
length. Additionally,thankstoitshierarchicalstructure,ourmodelcanreadbothknownwordsword-by-
wordandunknownwordssyllable-by-syllable,akintolexicalandnonlexicalroutesindualroutetheories
of reading [11].
Secondly, the model utilizes its generative capabilities not only for language recognition and pre-
diction, similar to LLMs, but also for simulating eye movements and saccades. In our model, reading
involves an active, hypothesis testing process: the model generates saccades to the most informative
parts of the text to validate its predictions and disambiguate among competing hypotheses about the
content being read [18, 1, 14, 15, 24, 51].
Thirdly, our model can be potentially used to gain insights on reading deficits, such as dyslexia.
Dyslexia, a common reading disorder affecting 5-10% of the population, is associated not only with
atypical brain activation patterns during language processing [60] but also with atypical eye movement
patterns, such as increased numbers of forward saccades and decreased saccade lengths compared to
controlgroups[92,12,45,20,36,70]. Crucially,ithasbeensuggestedthatdyslexiacouldbecharacterized
as a disorder of inference: a computational difficulty in effectively combining prior information, such
as implicit memory of previous words, with noisy observations like the currently perceived word [39].
Specifically,theproposalisthatthereadingdifficultiesmightarisebecausepriorinformationisassigned
excessivelylowweighting(orprecision)relativetoitsinternalnoise. Thelowprecision-weightingincreases
the time required to form a coherent understanding of text (or to perform auditory discrimination, as
in [39]), because novel observations consistently surprise dyslexics. By incorporating this proposal in
our model, we provide a proof of concept that it can qualitatively reproduce eye movement patterns
associated with dyslexia.
Inthefollowingsections,wedemonstratethemodel’scapabilitiesthroughsimulationsofwordreading
(Simulation 1), sentencereading(Simulation 2), readingunknownwordsandsentences(Simulation 3),
and reading with prior information about the topic (Simulation 4).
2 Methods
In this section, we provide a summary description of the model, a brief introduction to active inference
[55], and a detailed discussion of the hierarchical active inference reading model used in this paper.
3
2.1 Brief explanation of the model
We present a novel hierarchical active inference model for reading and eye movements during reading
[23]. Consistent with evidence of hierarchical language processing in the brain [34], our model consists
ofthreelevelsrepresentingsyllables,words,andsentences,asillustratedinFigure1(a-b). Ateachlevel,
the inference process involves integrating three types of messages: bottom-up messages from nodes at
the level below (conveying observations), top-down messages from nodes at the level above (convey-
ing predictions), and lateral messages from nodes representing the previous timestep at the same level
(providing memory). For instance, the inference about the current syllable is informed by bottom-up
observationsaboutthecurrentlyobservedletter,top-downpredictionsfromthecurrentlyinferredword,
andlateralinformationabouttheprevioussyllable. Similarly,theinferenceaboutthecurrentwordrelies
on bottom-up observations (i.e., the currently estimated syllable), top-down predictions from the level
above (i.e., at the sentence level), and lateral information about the previous word.
Toensurethatourmodeliscapableofreadingrealistictext,weincorporatethelargelanguagemodel
BERT [13] to generate prior probability distributions over the current word when reading single words,
and over the next words based on previously inferred words when reading sentences. BERT was cho-
sen because it offers computationally efficient access to explicit probability distributions over upcoming
words and sentence continuations. Given an initial context—a sequence of already read words—and a
predictive horizon of H steps, BERT produces K candidate sentence completions. These candidates
are used to construct nested multinomial distributions over sentences, their constituent words, and the
syllablescomprisingthosewords. Thesedistributionsthenserveaspriorsforthethreelevelsofthehier-
archical active inference model, thereby initializing the inference process to estimate the most plausible
continuation of the sentence. Inference proceeds over successive time steps as new words are observed.
Afterafixednumberofinferencesteps,BERTisqueriedagainwiththeupdatedcontext—includingthe
newly read words—and the process is repeated. This iterative procedure enables the model to read and
infer sentences of varying lengths.
Lastly,ourmodelhasthecapabilitytodeterminewheretomake(forwardorbackward)saccadesand
select the next letter observation [14]. The process of selecting where to direct the next saccade (i.e.,
which position in the syllable) is guided by the expected information gain of lexical elements across all
three levels of the model. Specifically, saccades are directed to the syllable position where the resulting
letterobservationisexpectedtoreduceuncertaintyaboutthecurrentsyllable(Level1),word(Level2),
and sentence (Level 3); see Section 2 for further details.
Figure1(c)visuallyrepresentsthesaccadesgeneratedbyourmodelwhilereadinganexamplesentence:
Active inference model of eye movements. The model demonstrates uncertainty about the first word
(active) and therefore reads it using two saccades to the first (a) and fourth (i) letters. It follows the
same process for the subsequent two words (inference and model). However, at this stage, it skips the
next two words, as it perceives the most informative word to be the one following (movements). This
mechanismshowcasesthemodel’sabilitytodynamicallyselectthemostrelevantinformationduringthe
reading process.
Figure 1(d) illustrates the probabilities of correctly recognized syllables (blue), words (red), and
sentences (yellow) during the reading process. Over time, these probabilities approach one, indicating
successful recognition. Additionally, the figure demonstrates that the levels operate at different time
scales,withlowerlevelsprocessinginformationmorequicklythanhigherlevels. Thistemporalseparation
arises because each level sends messages to the level above only after accumulating sufficient confidence
(asshownbytheverticalbarsinthefigure),aprocessthattypicallyrequiresmultipleroundsofinference
(b)
(a)
(c)
(d)
Figure 1: The hierarchical active inference model of reading and eye movements. (a) The hierarchical
model comprises three layers for syllables (Level 1), words (Level 2), and sentences (Level 3), represented
using the formalism of Partially Observable Markov Decision Processes (POMDPs). Empty nodes indicate
hidden variables, filled nodes represent observations, and edges depict probabilistic relations between the
nodes, as detailed in B. (b) Model parameters. (c) The sequence of saccades generated by the model while
readinganexamplesentence. Yellowdotsindicatethepositionsofthesaccadesinthetext,andcoloredlines
trace the sequence of saccades from red to yellow. (d) Evolution of the probabilities of correctly recognized
syllables(bluelines),words(orangelines),andsentences(yellowlines)overtime,whilereadingthesentence
of Panel c. The blue circles at the bottom level indicate saccades. Once the first level confidently infers a
syllable(orwhenthemaximumlevelofiterationsisreached),itsendsabottom-upsignaltothesecondlevel
(indicated by the orange vertical bar) to aid in word inference. Similarly, the second level sends a signal to
the third level (indicated by the yellow vertical bar) upon confidently inferring a word. Please refer to the
main text for further explanation.
[25]. Note that the time it takes to arrive at a high probability of syllable can be interpreted as saccade
duration – implying that the longer the time required to resolve uncertainty about the fixated syllable,
the longer the saccade duration.
2.2 Brief introduction to Active Inference
ActiveInferenceisaframeworkthatmodelsanagent’saction-perceptionloopbyminimizingvariational
free energy [55]. The key objective is to minimize free energy, and to achieve this, the agent possesses
a generative model (like the one illustrated in Figure 1), which captures the joint probability of the
stochasticvariables(hiddenstatesandobservations)usingtheformalismofprobabilisticgraphicalmodels
[3]. The agent’s perception involves making inferences about the hidden states based on the observed
sensory inputs, while action involves selecting actions that can change the hidden states and optimize
the model’s predictions. The generative model for active inference is defined as follows:
P(o ,s ,u ,γ|Θ)=
0:K 0:K 1:K
K
(cid:89)
P(γ|Θ)P(π|γ,Θ)P(s |Θ) P(o |s ,Θ)P(s |s ,π ,Θ) (1)
0 t t t+1 t t
t=0
where
• P(o |s ,Θ)=A,
t t
• P(s |s ,π ,Θ)=B(u =π ),
t+1 t t t t
• P(π |γ,Θ)=σ (cid:0) lnE−γ·GΘ(π ) (cid:1) ,
t t
• P(γ,Θ)∼Γ(α,β), and
• P(s |Θ)=D.
0
The set Θ={A,B,C,D,E,α,β} parametrizes the generative model:
• The(likelihood)matrixAencodestherelationsbetweentheobservationsOandthehiddencauses
of observations S.
• The (transition) matrix B defines how hidden states evolve over time t, as a function of a control
state(action)u ;notethatasequenceofcontrolstatesu ,u ,...,u ,... definesanaction policy (or
t 1 2 t
policy for short) π .
t
• The matrix C encodes an a-priori probability distribution over observations P(o ) and typically
τ
encodes the agent’s preferences; this prior distribution parameter is needed for the resolution of
Equation (5), with P(o )=C.
τ
• The matrix D is the prior belief about the initial hidden state, before receiving any observation.
• E encodes a prior over the policies (reflecting habitual components of action selection).
• γ ∈ R is a precision that regulates action selection and is sampled from a Γ distribution, with
parameters α and β.
Inactiveinference,theprocessofperceptioninvolvesestimatinghiddenstatesbasedonobservations
and previous hidden states. At the start of the simulation, the model has access to an initial state
estimates throughDandreceivesanobservationo thathelpsrefinetheestimatebyusingthelikelihood
0 0
matrix A. Subsequently, for each time step t=1,...,K, the model infers its current hidden state s by
t
consideringthetransitionsdeterminedbythecontrolstateu ,asspecifiedinB. Activeinferenceemploys
t
an approximate posterior over (past, present, and future) hidden states and parameters (s ,u ,γ).
0:K 1:K
This approach utilizes a factorized form of variational inference, which corresponds to a framework
developedinphysicsknownasmeanfieldtheory[54]. Usingameanfieldapproximation,namelyassuming
that all variables are independent, the factorized approximated posterior can be expressed as:
K
(cid:89)
Q(s ,u ,γ)=Q(π)Q(γ) Q(s |π ) (2)
0:K 1:K t t
t=0
where the sufficient statistics are encoded by the expectations µ = (˜sπ,π,γ), with ˜sπ = ˜sπ,...,˜sπ.
0 K
Following a variational approach, the approximate posterior Q(s |π ) over hidden states under a given
t t
policyπ ateachtimesteptisoptimizedbyminimizingthevariationalfreeenergy(F)(see[55])defined
t
as:
F =D [Q(s |π )∥P(s |π )]−E [lnP(o |s )] (3)
t KL t t t t Q(st|πt) t t
Thefirstterm,D [Q(s |π )∥P(s |π )],istheKullback-Leibler(KL)divergencebetweentheapproxi-
KL t t t t
mateposteriorandtheprioroverhiddenstates. Thistermquantifiesthecomplexity oftheposterior—i.e.,
how much information (in bits or nats) is required to move from prior beliefs P(s |π ) to the posterior
t t
Q(s |π ). Minimizing this term encourages the posterior belief to have low complexity, i.e., to remain
t t
close to prior expectations.
The second term, −E [lnP(o |s )], corresponds to the expected negative log-likelihood (or
Q(st|πt) t t
inaccuracy) of the observation o given the inferred hidden states. Minimizing this term encourages the
t
posterior beliefs to be predictive of the actual sensory observations, thus promoting accuracy. In sum,
minimizing the variational free energy trades off complexity against accuracy.
The approximate posterior Q(s |π ) is optimized when the sufficient statistics are:
t t
sπ ≈σ(lnA·o +ln(B(π )·sπ )) (4a)
t t t−1 t−1
π=σ(lnE−γ·G(π )) (4b)
t
α
γ = (4c)
β−G(π)
Actionselectionintheactiveinferenceframeworkinvolveschoosingapolicy,representedbyasequence
of control states u ,u ,...,u , that is expected to minimize free energy most effectively in the future.
1 2 t
The policy distribution π is defined by the Softmax function σ(·). In this equation, a crucial rule is
played by the expected free energy (EFE) of the policies, denoted by G. The EFE incorporates goal-
directed components of action selection and can be interpreted as the expected variational free energy
under future outcomes. Additionally, in this equation the precision term γ plays a role in encoding the
confidence of beliefs concerning G.
The EFE G(π ) of each policy π is defined as:
t t
K
(cid:88)
G(π )= D [Q(o |π)∥P(o )]+E [H[P(o |s )]] (5)
t KL τ τ Q˜ τ τ
τ=t+1
where D [·∥·] and H[·] are, respectively, the Kullback-Leibler divergence and the Shannon entropy,
KL
Q(o ,s |π) ≜ P(o ,s )Q(s |π) is the predicted posterior distribution, Q(o |π) = (cid:80) Q(o ,s |π)
τ τ τ τ τ τ sτ τ τ
is the predicted outcome, P(o ) is a categorical distribution representing the preferred outcome and
τ
encoded by C, and P(o |s ) is the likelihood of the generative model encoded by the matrix A.
τ τ
The EFE in (5) can be used as a quality score for the policies and has two terms:
• Expected Cost. Thefirsttermof (5)istheKullback-Leiblerdivergencebetweenthe(approximate)
posterior and prior over the outcomes and it constitutes the pragmatic (or utility-maximizing)
component of the quality score. This term favours the policies that entail low risk and minimise
the difference between predicted (Q(o |π)) and preferred (P(o )≡C) future outcomes.
τ τ
• ExpectedAmbiguity. Thesecondtermof (5)istheexpectedentropyundertheposterioroverhidden
states and it represents the epistemic (or uncertainty-minimizing) component of the quality score.
This term favours policies that lead to states that maximize information gain and diminish the
uncertainty of future outcomes H[P(o |s )].
τ τ
AfterscoringallthepoliciesusingtheEFE,actionselectionisperformedbydrawingovertheaction
posteriorexpectationsderivedfromthesufficientstatisticπcomputedvia(4b). Then,theselectedaction
isexecuted,themodelreceivesanovelobservationandtheperception-actioncyclestartsagain. Werefer
the reader to [55] for a complete derivation of the equations presented in this section.
2.3 Hierarchical Active Inference model for reading
The active inference model employed in this study is structured hierarchically, consisting of three levels
denoted by i ∈ 1,2,3 in our simulations. At each level, the model encodes hidden variables associated
with different aspects of textual content: syllables at Level 1, words at Level 2, and sentences at Level
3. This hierarchical organization allows the model to effectively capture and process information at
multiplelinguisticscales,enablingtheinferenceandgenerationofsyllables,words,andsentencesduring
the reading process. For a visual representation of the model’s architecture, refer to Figure 1(a-b).
Thehiddenstatesofeachlevelareobtainedbythetensorialproduct: S(i) =S(i)⊗S(i)⊗S(i) among
1 2 3
three factors:
• the Content S(i), i.e., the textual content proper: Syllable, Word or Sentence,
1
• the Location S(i), i.e. the position of a portion of the textual content (Location of the Letter in
2
thesyllable,LocationoftheSyllableintheword,andLocationoftheWordintheSentence),thus
corresponding to a S(i−1)-type content of the next lower level if i>1 or to location of a Letter in
1
a Syllable if i=1.
• the Topic S(i), i.e. the context to which each Content variable is associated. Please note that we
3
only use this factor in Simulation 4.
The Content is the crucial element of the chain: it is the text that is obtained from the recursive
concatenation of the elements from the level below. A Syllable is a concatenation of N(1) Letters c(0)
(whicharenothiddenstatesbutobservations,belongingtoafiniteAlphabet S(0),seebelow). AWord isa
1
concatenationofN(2) syllablesc(1) belongingtothesetofSyllables S(1) andaSentence isconcatenation
1
ofN(3) Words c(2) belongingtothesetS(2). Ingeneral,thei-thcontentc(i) ∈S(i) isaconcatenationof
1 1
N(i) sub-contents c(i−1) ∈S(i−1) such that
n 1
c(i) =c(i−1),...c(i−1)
1 N(i)
The observations O(i) =O(i)⊗O(i)⊗O(i) consists of the tensorial product among:
1 2 3
• the observation O(i) of the content of the level below S(i−1). Note that only Level 1 of the model
1 1
receives an actual textual observation: namely, a letter c(0) belonging to the Alphabet S(0). The
1
other two levels receive as observations the content at the level below: the observations at levels 2
and 3 are the inferred syllables and words, respectively;
• the observation O(i) corresponding to the location S(i) of the S(i−1) content;
2 2 1
• the feedback response r∈O(i), (“correct“ or “wrong”) that reports whether the currently inferred
3
content C(i) has been correctly classified (i.e., assigned to the correct topic) or not. We only use
this observation at Level 3 (i.e., O3) of Simulation 4, in order to compare models with or without
3
prior information about the topic of the sentence.
The likelihood mapping p(O(i)|S(i)) between hidden states S(i) and observations O(i) is specified
through the tensor A(i), defined as the tensorial product A(i) =A(i)⊗A(i)⊗A(i) where
1 2 3
• A(i) is a 4-order tensor mapping the hidden states S(i) (3-order tensors) to the observation O(i)
1 1
corresponding to the content S(i−1) if i > 1 or to the observation of a letter if i = 1; In our
1
simulations, this probability is typically an identity matrix: the probability of observing content
c(i) ∈ S(i) if the reader agent is in the corresponding location l(i) ∈ S(i) is set to 1. Note that
1 2
adding noise to the likelihood function would allow modeling the effects of perceptual deficits or
poor familiarity with words.
• A(i) is a 4-order tensor mapping the hidden states S(i) to observation O(i) corresponding to the
2 2
locationsS(i);Alsointhiscaseweassumenonoise,sotheprobabilityofobservingagivenlocation
2
when the reader agent eye point to that location is set to 1.
• A(i) is a 4-order tensor mapping the hidden states S(i) to the feedback response in O(i). In our
3 3
simulationweadoptabinaryprobabilityinoursimulation: thelikelihoodissetto1ifthecontent
belongs to the correct topic and to 0 otherwise. Please note that in this paper this factor is used
only in Simulation 4.
Themappingp(s(i)|s(i) ,π )betweenhiddenstatesgiventhecontrolstateuisspecifiedbythetensor
t−1 t
B(i), defined as the tensorial product B(i) =B(i)⊗B(i)⊗B(i) where
1 2 3
• B(i) maps the transition between content states at successive time steps. Note that in our simula-
1
tions,weusetransitionmodelswithoutnoise(forthecontrolmodel)andwithnoiseaddedatoneor
morehierarchicallevels(forthedyslexicmodels). Forthis,weformalizethemappingp(c(i)|c(i) ,π )
t t−1 t
by assigning 1−δ to the correct target transition and assigning the value δ/(M(i)−1) across the
otherM(i)−1states,whereM(i)isthetotalnumberofContentstatesS(i). Thismodulatingfactor
1
δ(i) ∈[0,1], is set to 0 in the control model and to 0.15 in the dyslexic models.
• B(i)mapsthetransitionsbetweenlocations. Weequipthemodelwiththeprobabilityp(l(i)|l(i) ,π )
2 t t−1 t
to jump from one location l(i) ∈ S(i) to another, without noise (i.e., the act of jumping to the
2
selectedlocationisexecutedwithnoerrors). Whileinprincipleonecanjumptoanylocation,here
werestrictthejumppossibilities, bysettingpriorsoverlocationsD(i) andpoliciesE(i), seebelow.
• B(i) maps the probability p(tp(i)|tp(i) ,π ) to jump from one topic tp(i) ∈ S(i)to another. We
3 t t−1 t 3
consider this transition to be noiseless. This action is only relevant in Simulation 4, in which we
compare the model with or without prior information about the topic. Please note that we only
use this factor in Simulation 4.
The tensor C(i) =C(i)⊗C(i)⊗C(i) encodes the priors over the observations that encode preferred
1 2 3
outcomes:
• C(i),priorpreferencesontheoutcomesO(i),(e.g. forLevel3encodingsentences,prioronexpected
1 1
words), which is flat in our simulations;
• C(i),priorpreferencesontheoutcomesO(i) (e.g. forLevel3encodingsentences,prioronexpected
2 2
word locations), which is flat in our simulations;
• C(i), prior preferences on the feedback response O(i) (e.g. for Level 3 encoding sentences, a higher
3 3
prior for having classified the sentence according to the “correct” topic than the “wrong” topic.
Please note that we only use this factor in Simulation 4.
The tensor D(i) =D(i)⊗D(i)⊗D(i) encodes the priors over the hidden states:
1 2 3
• D(i), priors over the Content at the corresponding level. It corresponds to the initial distribution
1
of the content variable (e.g. at Level 3, it is the prior over the initial sentence distribution). For
simplicity,hereweuseflatpriors. However,itispossibletousethisfeaturetomodelthefactthat
syllabes, words and sentences have different frequencies.
• D(i), priors on the Locations. For simplicity, here we assign a very high prior to the first element
2
of the content, following the assumption that people start reading (for example) a word from the
first syllable. During reading, after each loop the D(3) is initialized to the next word not yet read.
1
• D(i), priors on the Topics. We only use this feature in Simulation 4, in which we show that
3
having prior information about the topic of the sentence can speed up its reading. In all the other
simulations, we set flat priors over Topics.
The matrix E(i) encodes the priors over the policies.
2.4 Saccade selection
Saccade selection arises from a competition among policies spanning all hierarchical levels, determining
the location of the next word in the sentence (Level 3), syllable in the word (Level 2), and letter in the
syllable(Level1). Thenumberofpoliciesateachlevelvariesdependingonthesimulation. Forexample,
whenreading8-wordsentencesinSimulation2,themodeluses8policiestojumpamongwords,4policies
to jump among syllables and 5 policies to jump among letters.
Policy selection considers the Expected Free Energy (EFE) described in (5). In our simulations, we
employflatpriorpreferencesonContentandLocation(denotedasC(i)andC(i)),respectively),resulting
1 2
in policy selection that solely weighs the information gain associated with the following word, syllable,
or letter. However, in Simulation 4, policy selection also depends on the greater preference (C(i)) for
3
receivinga“correct”thana“wrong”feedbackresponse(weremindthatthisfactorisabsentintheother
simulations).
Furthermore, policy selection considers the prior distribution over policies (denoted as E(i)). Our
simulations adopt flat priors for Level 1 (E(1)) and Level 2 (E(2)), signifying no inherent constraints
on transitioning between letters within a syllable or between syllables within a word. To discourage
overlylongsaccades,afinitemovingwindowforfavoredtransitionsamongwordlocationsissetforLevel
3 (E(3)). This is achieved through a Poisson distribution Pois(λ), where λ equals 6 (as displayed in
Figure 2, depicted by red bars). Additionally, the probability of reading the same word after having
already recognized it is set to zero, resulting in the normalization of the distribution (note however
that it the distribution allows backtracking to a previous word during reading). This culminates in the
distribution depicted by the black bars in Figure 2. The chosen distribution enables the creation of a
finitewindowtodetermineprobabilitiesrelatedtothecontentunderconsideration. Iteratingthiswindow
at each step enables the processing of sentences of arbitrary length.
Furthermore,Figure3presentsaschematicillustrationofthetextualelementsinferredbythemodel
while reading the sentence “Active inference model of eye movements” as depicted in Figure 1. The
diagram distinguishes between two factors of the generative model – Content and Location – illustrated
bytheleftandrighttrees,respectively. Theleft(content)treeinFigure3demonstratesthatthemodel
0.15
0.1
0.05
0
-5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10
locations
)
(p
E distribution
Figure 2: Priors on the policies that make transitions between word locations at Level 3 of the generative
model. WefirstsetthepriorasaPoissondistributionwithλ=6(redbars),centeredonthecurrentlocation
(i.e., the value of the maximum of the distribution is in 0). We next set the probability of reading the same
word two consecutive times to zero and re-normalize the distribution, leading the distribution shown by the
black bars.
Figure 3: Graphical explanation of the model’s reading process for the example sentence “Active inference
model of eye movements” introduced in Figure 1. The figure illustrates the distinction between two factors
of the generative model: content (left tree) and location (right tree). Refer to the main text for a detailed
explanation.
infersthefirstword“active”atLevel2byexaminingtheletter“a”. This,inturn,leadstotheinference
of the syllable “ac” at Level 1, followed by the letters “t” and “e”, enabling the inference of the syllable
“tive” at Level 2. The right (location) tree in Figure 3 shows the respective locations of these three
letters“a”,“t”,and“e”withintheirsyllablesatLevel1,andthepositionsof“ac”and“tive”withinthe
word at Level 2. This process continues for the subsequent words (excluding “of” and “eye”, which are
skipped during reading, as seen in Figure 3), until the entire sentence is recognized.
3 Results
3.1 Simulation 1: Reading single words
In this task, the objective is to read and recognize 100 words consisting of either 4 or 8 letters, selected
from a pool of 726 words ranging from 1 to 8 letters (see Tab. S.14 for the list of words used in this
simulation, allofwhicharepartofthedictionaryofBERT).Syllablesforeachwordaregeneratedfrom
1
0.98
0.96
0.94
4-letter 8-letter
Number of letters in the word
drow
tcerroc
eht
fo
ytilibaborP
1
0.98
0.96
0.94
0.92
Control Model 0.9
Dyslexic Model
0.88
4-word 8-word
Number of words in the sentence
(a)
ecnetnes
tcerroc
eht
fo
ytilibaborP
Control Model
Dyslexic Model
(b)
Figure 4: Results of Simulations 1 and 2: Probabilities assigned to the correct words. (a) Simulation 1:
Probabilities assigned by the Control model (CM) and the Dyslexic model (DM) while reading 4-letter and
8-letter words. (b) Simulation 2: Probabilities assigned by the two models while reading 4-word and 8-word
sentences. In this and the subsequent boxplots, the horizontal line represents the mean µ. The edges of the
rectanglerepresentµ±σ,whereσ isthestandarddeviationofthemean. Theareawithintheboxplotabove
themeanrepresentstheintervalbetweenµ+σ,whilethatbelowtheredlinerepresentstheintervalbetween
µ−σ. The vertical black lines limits extend within the range of µ±3σ.
wordsusingthemethodsdescribedin[44]. Foreaseofevaluation,eachwordisassignedthesamea-priori
probability of 1/726 in the model.
The model’s performance is assessed using four metrics: word recognition accuracy, the probability
assignedtothecorrectword,thenumberofsaccades(forward,backward,ortotal),andtheiramplitude
(i.e., the number of locations between letters). It is important to note that for this specific simulation,
only levels 1 and 2 of the model are utilized, while Level 3 is not included.
To better evaluate the model, we compare two versions: (i) a Control model (CM) with no noise in
its transition functions, allowing it to adequately consider the prior context during reading, and (ii) a
Dyslexic model (DM),aimingtocapturequalitativelykeyaspectsofthisdisorder. Previousresearchhas
demonstrated that individuals with dyslexia exhibit a reading style characterized by more fragmented
and laborious patterns of eye movement, with a higher number of shorter saccades when reading words,
and these difficulties are more pronounced with longer words [92, 46, 20]. To provide a proof of concept
that our DM model can replicate this pattern of results, here we follow the proposal that dyslexics
struggletointegratepriorlinguisticcontext[39]–andintroducenoiseinthetransitionfunctionsofboth
the first (syllable) level and the second (word) levels. For this, we set δ(1) = δ(2) = 0.15 in the DM
model, see Methods section.
TheCM demonstratesperfectaccuracy(100%),whereastheDM exhibitsslightlyloweraccuracyfor
both 4-letter (99%) and 8-letter words (97%). Additionally, the CM assigns a 100% probability to the
correct words, whereas the DM assigns a significantly lower probability to the correct word, especially
for longer words. Please refer to Figure 4(a) and Table S.1 for detailed results.
Moreover,theDM exhibitsasignificantlyhighernumberoftotalsaccades(Figure5(a))andbackward
saccades (i.e., saccades to any of the preceding letters, Figure 5(b)) while reading both 4- and 8-letter
wordscomparedtotheCM.DetailedstatisticalcomparisonsareavailableinTab. S.2andS.3. Todelve
(a) (b)
Figure 5: Simulation 1 results: Total number of saccades (a) and number of backward saccades (b) while
reading4-letterand8-letterwords. ThefigurecomparestheControl Model (CM) withnonoise,theDyslexic
Model (DM) with noise at all hierarchical levels, and two versions of the DM, with noise only at the level
of syllables (DM - Noise on Level 1) or words (DM - Noise on Level 2). See Table S.2 and Table S.3 for
detailed results and statistical comparisons.Horizontal bars indicate statistically significant differences from
the Control Model (CM). Significance levels are indicated as follows: * p<0.05, ** p<0.01, *** p<0.001.
deeperintotheimpactofnoiseoneyemovementdynamicsduringwordreading,wecreatedtwovariants
of the DM, one with noise in the transition functions at the syllable level (DM - Noise on Level 1) and
another with noise at the word level (DM - Noise on Level 2). This analysis reveals that noise at the
first (syllable) level has a more significant effect than noise at the second (word) level.
Inaddition,theDM exhibitssignificantlyshorterforwardsaccades(Figure6(a))butlongerbackward
saccades (Figure 6(b)) while reading both 4- and 8-letter words in comparison to the CM. Detailed
statistical comparisons are available in Table S.4 and in Table S.5.
In summary, our model successfully reproduces a wide range of empirical findings regarding how
dyslexic individuals read single words, encompassing accuracy, reaction times, and eye movements. The
lower probability assigned to words by the DM compared to the CM, as depicted in Figure 4(a), closely
mirrors the results reported by [92, Fig. 1(B)]. The accuracy results (Table S.1) are also consistent with
the finding that dyslexic individuals fail to correctly read a small percentage of single words. Moreover,
oursimulationsrevealthatdyslexicindividualsreadsignificantlyslowerthancontrols[92,52,Fig.1(A)].
This slower reading arises from the DM making more saccades than the CM, as illustrated in Figure 5,
with reading time being proportional to the number of saccades.
Furthermore, the results displayed in Figure 6(a) closely match the finding that dyslexic individuals
2.5
2
1.5
1
0.5
4-letter 8-letter
Number of letters in the word
]#[
sedaccas
drawrof
fo
edutilpmA
1.8
Control Model
Dyslexic Model
1.6
***
1.4
1.2
1
**
0.8
0.6
0.4
4-letter 8-letter
Number of letters in the word
(a)
]#[
sedaccas
drawkcab
fo
edutilpmA
Control Model
Dyslexic Model
***
***
(b)
Figure 6: Simulation 1 results: Amplitude of forward (a) and backward (b) saccades while reading 4-letter
and 8-letter words. The figure compares two models: the Control Model (CM) with no noise, and the
Dyslexic Model (DM) with noise at both hierarchical levels. See Table S.4 and Table S.5 for detailed results
and statistical comparisons.
make shorter forward saccades compared to controls, particularly evident for longer words [46, Fig. 4].
Lastly, the results presented in Figure 6(b) closely resemble the finding that dyslexic individuals make
significantly more and larger backward saccades than controls during reading [59, Fig. 5]. Collectively,
these outcomes demonstrate that our model effectively captures possible aspects of reading skill dif-
ferences and/or development, providing valuable insights into the underlying mechanisms of reading
disorders like dyslexia.
Significantly, our model offers a mechanistic understanding for all these observed findings, including
the fragmented reading style of dyslexics. The DM’s shorter forward saccades stem from its poor con-
textual memory, making it less capable of predicting the next word efficiently. Consequently, it requires
more saccades and time to read a text. Additionally, the increased need for backward saccades arises
because the DM occasionally has to backtrack in the text to retrieve lost context. Moreover, our model
readily explains why dyslexic reading impairments are more pronounced for longer words: noise in the
transition function(s) accumulates over time, making reading longer words progressively more challeng-
ing. This comprehensive mechanistic explanation underscores the potential usefulness of our model in
shedding light on the underlying cognitive processes contributing to dyslexia and its impact on reading
behavior.
3.2 Simulation 2: Reading Sentences
This task involves reading and recognizing 100 sentences composed of 4 or 8 words (all of which are
included in the dictionary of BERT [13]), with each word consisting of 1 to 4 syllables. The sentences
with4wordshaveanaveragelengthof25.96±1.96letterswithintherangeof[21,31]. Similarly,sentences
with8wordshaveanaveragelengthof50.24±1.64letters,intheintervalof[47,55](seeTableS.13). To
makethetextchallenging,wecarefullydesignedsentenceswithasubstantialwordoverlap(TableS.15).
For this task, we utilize the comprehensive model depicted in Figure 1, encompassing three hierarchical
levels. Atthebeginningofthesimulation,weuseBERTtogeneratepriorprobabilitydistributionsover
upcoming sentences, comprising the next 4 or 8 words. These distributions are continuously updated
during reading as new syllables and words are inferred through informative saccades.
Previous studies have shown that proficient readers can scan lines of text using only a few saccades,
while dyslexic individuals exhibit an increased number of shorter saccades [36, 70, 12] and fewer word
skipping occurrences [6, 40, 33] compared to controls. Additionally, their performance decrease and
increased number of saccades are influenced by the number of words in the text (see [12], Fig. 2). To
verifywhetherourmodelaccuratelyreplicatesthesefindings,wecompareaControlmodel(CM)withno
noiseandaDyslexicmodel(DM)withnoiseintroducedinthetransitionfunctionsofallthreehierarchical
levels (setting the parameter δ(1) =δ(2) =δ(3) =0.15, see Methods section).
Consistentwiththeempiricalfindings,theDM assignssignificantlylowerprobabilitiestothecorrect
sentencescomparedtotheCM,andthiseffectismorepronouncedforlongersentences(Figure4(b)and
Table S.6). Additionally,theDM exhibitsasignificantlyhighernumberoftotalsaccades(Figure7(a-c))
andbackwardsaccades(Figure7(b-d))thantheCM duringsentencereading. Whencomparingdifferent
variants of the DM with noise introduced at various levels, we find that noise at the level of sentences
has the most significant impact on impairing the reading performance.
Moreover,theDM exhibitssignificantlyshorterforwardsaccades(Figure8(a))andlongerbackward
saccades (Figure 8(b)) when reading both 4-word and 8-word sentences, in comparison to the CM.
Detailed statistical comparisons can be found in Table S.9 and in Table S.10.
Collectively, these results build upon those observed for single words (Simulation 1) and successfully
(a) (b)
Figure 7: Simulation 2 results: Total number of saccades (a) and number of backward saccades (b) while
reading 4-word and 8-word sentences. The figure compares the Control Model (CM) with no noise, the
Dyslexic Model (DM) with noise at both hierarchical levels, and three alternative versions of the Dyslexic
Model, where we added noise only at the level of syllables (DM - Noise on Level 1), words (DM - Noise
on Level 2), or sentences (DM - Noise on Level 3). See Table S.7 and Table S.8 for detailed results and
statistical comparisons.
13
12
11
10
9
8
7
6
5
4
4-word 8-word
Number of words in the sentence
]#[
sedaccas
drawrof
fo
edutilpmA
Control Model
Dyslexic Model
*** 25
20
***
15
10
5
0
4-word 8-word
Number of words in the sentence
(a)
]#[
sedaccas
drawkcab
fo
edutilpmA
Control Model
Dyslexic Model
***
***
(b)
Figure 8: Results of Simulation 2: amplitude of forward (a) and backward (b) saccades when reading
4-word and 8-word sentences. The figure compares the Control Model (CM) with no noise and the Dyslexic
Model (DM) with noise at all the hierarchical levels. See Table S.9 and Table S.10 for detailed results and
statistical comparisons.
replicate empirical findings on dyslexic individuals’ reading of sentences. The trends illustrated in Fig-
ure7(a)closelyresembletheoutcomesreported[12,Fig.3(A)],whichdemonstratesthatdyslexicsmake
more saccades compared to controls while reading single lines of text (equivalent to reading 8-word sen-
tencesinFigure7(a)). Moreover,thenumberofsaccadesexhibitedbytheDMinoursimulationsaligns
well with the numerical outcomes from the same study. Similarly, the outcomes in Figure 8(a) closely
resemble the findings depicted in [12, Fig. 3(B)], revealing that dyslexics make shorter forward saccades
than controls. In this context, the amplitudes of both dyslexics and the DM model closely correspond.
However,thelargeramplitudeofbackwardsaccadesobservedinourDM (Figure8b)isincontrastwith
the shorter amplitude of regressions observed in dyslexic readers in [12, Fig. 3d].
Insummary,ourresultssuccessfullyreplicatetheobservationthatdyslexicindividualstendtoproduce
a higher number of shorter saccades while reading sentences, and that their reading performance is
influenced by the number of words in the sentence (see [12, Fig. 2]). The underlying causes of these
impairments are similar to those identified in the single-word reading task (Simulation 1), but their
effects are amplified in sentence reading, where comprehension depends on integrating information over
longer timescales. This also helps explain why noise at the sentence level significantly disrupts task
performance(Figure7). Whilethesequalitativeresultsarepromising,furtherworkisneededtoachieve
acloser—andmorequantitative—alignmentbetweenthemodelandempiricalfindingsondyslexia,aswell
astocalibratemodelparametersappropriately. Forinstance,ourDMgeneratesarelativelyhighnumber
ofregressive(backward)saccades. Thisoccursbecausethemodeloftenlosescontextualinformationand
mustbacktracktoearlierwordsinthesentencetoreestablishit. However,thispatternisnotcommonly
observed in dyslexic readers ([12, Fig. 3d]), and the reasons for this discrepancy remain to be explored.
3.3 Simulation 3. Reading novel words and sentences
Up to this point, our simulations involved reading known words and sentences, already present in the
BERTvocabulary. Hence,themodelcouldconfidentlyassignprobabilitiesatthesecondandthirdlayers.
Nevertheless,ourmodelcanalsoreadunknown wordsandsentences,despiteitsinabilitytoassignthem
a prior probability. When confronted with unknown words or sentences, the model reads syllable-by-
syllable (or word-by-word) and simultaneously acknowledges uncertainty at the second (or third) level.
This mode aligns with the nonlexical (or sublexical) route in dual-route theories of reading [11].
We conducted a simulation consisting of 100 trials, each involving the reading and recognition of
a four-word sentence, designed for comparison with the corresponding tests in Simulation 2. In each
trial, one of the four words was unknown—specifically, it was removed from the model’s vocabulary.
This resulted in a set of 100 unique unknown words, each ranging from 1 to 4 syllables in length (see
TableS.16forthecompletelist). Thesentenceshadanaveragelengthof25.46±1.92letters,witharange
of[22,32]. Atwo-samplet-testcomparingsentencelengthsbetweenthissetandthesetwithonlyknown
wordsusedinSimulation2revealednosignificantdifference(p≈0.27),confirmingthatthetwosetswere
well matched in terms of both word and sentence length. Trials were terminated upon either successful
completion of the sentence or failure of the model to recognize a word. The simulation results indicate
that the total number of saccades increased substantially when reading sentences containing unknown
words (mean ≈12.04) compared to Simulation 2 (mean ≈4.36); see Table 1 for statistical comparisons.
Furthermore, while sentence recognition at Level 3 in Simulation 2 was nearly perfect (p(C3|O)≈1), it
dropped to approximately 0.49 (on average) in the presence of unknown words, indicating a failure to
correctly recognize the sentence.
Table 1: Simulation 3. Comparison of two conditions: reading 100 sentences from Simulation 2 (4 words –
all known) versus 100 sentences in which one word was unknown, i.e., removed from the model’s vocabulary
(4 words – 1 unknown). The table summarizes the comparison between these two conditions, reporting
the mean, standard error, and p-value of t-tests for three key measures: sentence length, total number of
saccades during reading, and the probability of sentence recognition.
Sentence Length Total number of Saccades Probability
4 words – Known 25.96±1.96 4.36±0.11 1.00 (SE <10−15)
4 words – 1 Unknown 25.46±1.92 12.04±0.44 0.49±0.04
p-value p≈0.27 p<0.001 p<0.001
Toillustratethemodel’sbehavior,wedescribearepresentativeexampleinvolvingthereadingofthe
final four words of the sentence This paper is also framed in an offbeat manner, comparing two versions
of the model: one in which all the words exist in the model’s vocabulary (Figure 9(a)) and another in
which the word offbeat was removed from the vocabulary (Figure 9(b)). In the version with all known
words,themodelbehavessimilarlytotheprevioussimulations,accuratelyrecognizingallthewordsand
the sentence. This is evident in Figure 9(a), where probabilities at Levels 2 and 3 quickly converge to
one after a few iterations.
Conversely, the version without knowledge of the word offbeat repeatedly makes saccades to the
syllablesofthisword(atLevel1)untilitreachesthemaximumnumberofiterationssetforthissimulation.
It correctly recognizes each syllable and could therefore potentially read the word aloud “non-lexically”
(andpotentiallyaddittoitsvocabulary)[11]. However,itdoesnotrecognizetheword(atLevel2)orthe
sentence(atLevel3). AsshowninFigure9(b),boththeprobabilityoftheunknownwordatLevel2and
the probability of the sentence at Level 3 remain below 0.5, indicating a failure of accurate recognition.
Afterfailingtorecognizetheunknownword(offbeat),theLevel2inferencesettlesonthemostprobable
alternative in the model’s vocabulary (e.g., upbeat). Unlike the simulation described earlier, here the
inference process continues even after an unrecognized word. As a result, the model is able to correctly
identify the final word in the sentence (manner).
The example in Fig. 9 illustrates the distinction between reading a known versus a novel word and
the termination conditions for inference at each hierarchical level. The inference can terminate in two
ways: first, whenthemodelreachesathresholdofconfidenceabouttheto-be-recognizedsyllable, word,
orsentence(i.e.,theExpectedAmbiguityoverhiddenstatesofEquation5fallsbelowafixedthreshold,
χ(i) =1/8). This happens when the model successfully infers a known syllable, word, or sentence.
Second, the inference can terminate when it reaches a maximum number of iterations K(i) at each
max
i-th level. This sets the maximum number of times Level 1 can jump between letters to recognize a
syllable, the maximum number of times Level 2 can jump between syllables to recognize a word, or the
maximumnumberoftimesLevel3canjumpbetweenwordstorecognizeasentence. Inoursimulations,
for illustrative purposes, we set K(i) to very high values (K(3) =7 and K(2) =K(1) =6) but these
max max max max
values can be fine tuned to fit individual participants or participant groups.
AsshowninFigure9,Level1successfullyinfersallthesyllableswithin3iterationsandneverreaches
K(1) . However, Level 2 fails to recognize the unknown word offbeat: it continues jumping between
max
the syllables off and beat until it reaches the maximum number of allowed iterations (6 iterations,
indicated in the grey panel). When the Level 2 inference of the current word is halted, a low confidence
message is reported to the level above while inference may still proceed with the next words (that are
successfullyrecognized). Similarly,Level3failstoinfertheunknownsentenceafterthemaximumnumber
ofiterations(7iterations),despitecorrectlyrecognizingmostofthewords. Attheendofthesimulation,
themodelidentifiesthatthecauseoffailureisatLevel2,asLevel1correctlyrecognizedallthesyllables.
Consequently, the model reads the unknown word syllable-by-syllable, i.e. using the nonlexical route in
dual route theories of reading [11].
3.4 Simulation 4: Knowing the topic improves sentence recognition
Inthissection,weshowcaseanothercapabilityofthemodelthathasnotbeendiscussedyet. Themodel
can be enriched with priors regarding the linguistic topic at each hierarchical level, such as the topic
of the sentence at the third level. This prior knowledge makes certain sentences a-priori more (or less)
likely, thus expediting the recognition process if the context aligns correctly.
To illustrate this, we conduct a task involving the recognition of 100 sentences, each containing 9
words,generatedbyBERT.Thesesentencescompriseanaverageof69.55±5.02letters,rangingbetween
61 and 82 characters(see Table S.13). We allocate one-thirdof thesentencesto each ofthe threetopics
of the European Research Council (ERC): Physical Sciences and Engineering (PE), Life Sciences (LS),
andSocialSciencesandHumanities(SH);seeTableS.17forafullspecificationofthesentencesandtheir
1
0.5
0
0 0.5 1 1.5 2
)O|3C(p
Sentence
1
0.5
0
0 0.5 1 1.5 2
)O|2C(p
Word
1
0.5
0
0 0.5 1 1.5 2
time [s]
)O|1C(p
Syllable
(a)
(b)
Figure 9: Simulation 3 results: Reading known vs. unknown words. Evolution of the probabilities of
correctly recognized syllables (blue lines), words (orange lines), and sentences (yellow lines) over time, while
readingthefinalfourwordsofthesentenceThis paper is also framed in an offbeat manner, intwocases: (a)
whenallthewordsareknownand(b)whenthewordoffbeat isunknown(i.e.,removedfromthevocabulary).
The blue circles at the bottom level indicate saccades. The figure format is consistent with Figure 1(d).
12
11.5
11
10.5
10
9.5
9
8.5
8
]#[
sedaccas
fo
rebmun
latoT
2
**
1.5
1
0.5
Flat Informative
Priors Priors
(a)
]#[
sedaccas
drawkcab
fo
rebmuN
Flat Informative
Priors Priors
(b)
Figure 10: Simulation 4 results: How prior information affects the reading process. The figure displays
the total saccades (a) and backward saccades (b) of two model versions: one with a flat prior about the
topic of the text (flat prior) and another with certain knowledge of the topic (informative prior). The
comparisonhighlightstheimpactofpriorinformationonthenumberandnatureofsaccadesduringreading.
See Table S.11 and Table S.12 for detailed results and statistical comparisons.
respective topics. Please note that in this simulation, we only use topics at the third level.
We compare the performance of two versions of the model: one with a flat prior (33.3%) regarding
thesentencebeingread,andanotherwithaninformativeprior thatassignsaprobabilityof100%tothe
correct topic. As depicted in Figure 10, the model with an informative prior requires significantly fewer
totalsaccadesandbackwardsaccadesthanthemodelwithaflatprior(seeTableS.11andTableS.12for
statistical comparisons). This advantage arises because knowing the topic allows the model to predict
certainwordswithhighconfidence,enablingittoskipunnecessarysaccadesduringthereadingprocess.
Intriguingly, despitethemodelwithaninformative prior skippingseveralwords, itdoesnotnecessitate
more backward (corrective) saccades compared to the model with a flat prior. This indicates that the
prior information promotes a better speed-accuracy trade-off.
4 Discussion
In this paper, we presented a novel hierarchical active inference model of reading, which combines the
robustgenerativeabilitiesoflargelanguagemodels[13,67]withtheinferentialandinformation-gathering
capacities of active inference [23, 55].
Large language models, like BERT [13] and GPT (Generative Pre-trained Transformer) [67], have
demonstrated remarkable performance in various natural language processing tasks, including question
answering, language translation, text classification, and sentence prediction. These models can learn es-
sentiallinguisticstructureswithoutexternalsupervision,makingthemvaluabletoolstoexplorelanguage
processing in the brain [49, 29]. However, they lack the active strategies employed by human readers,
such as the ability to make saccades selectively on the most informative parts of the text [18, 51].
To capture active reading, we integrate BERT [13] into a hierarchical active inference framework
[23]. Active inference is a theoretical framework that describes how the brain employs probabilistic
inferenceoveragenerativemodeltoeffectivelysamplesensoryinputs,minimizingvariationalfreeenergy
or prediction errors – and that has been applied too many tasks, such as perceptual processing, goal-
directed navigation, robot control, and social interaction [37, 48, 53, 86, 61, 55]. For modeling reading,
weutilizeahierarchicalgenerativemodel,aligningwithevidencesuggestingthatlanguageprocessingin
the brain operates through hierarchically organized predictions and prediction errors [77, 9, 34].
By placing BERT at the top of the hierarchy, we ensure accurate next-word predictions and the
model’s capability to tackle real-world reading tasks. Beyond next-word prediction, our hierarchical
model facilitates both inferring the content being read and predicting future elements across different
linguisticlevels,suchassyllables,words,andsentences. Oursimulationresultsdemonstratethemodel’s
perfectaccuracyininferringwords(Simulation1)andsentences(Simulation2)duringreading. Addition-
ally,thishierarchicalstructureenablesthemodeltoreadnovelwordsnotpresentinBERT’svocabulary
(Simulation 3), assembling them syllable-by-syllable, representing the nonlexical (or sublexical) route
in dual route theories of reading [11]. Moreover, the model can incorporate prior knowledge, like the
topic of the sentence, to expedite the reading process (Simulation 4). Significantly, the model offers in-
terpretability through its alignment of distinct levels with established linguistic constituents (sentences,
words,syllables),whichcouldcorrespondtodistributedneuralpopulationsorcellassemblieswithinthe
brain[34,64]. Whiledelvingintotheneuralmechanismsthatunderliethisproposedframeworkextends
beyond the confines of this article, future studies could encompass the simulation of neuronal dynamics
during the inference process, in terms of variational free energy minimization [25, 38].
Using active inference, our model characterizes reading as an active, hypothesis testing process,
allowingustosimulateeyemovementsduringreading. Thecoreideaisthatsaccadesaredirectedtothe
mostinformativepartsofthetext,enablingthemodeltotestitspredictionsandreduceuncertaintyabout
thetextbeingread[18,51,14,24]. Remarkably,thisepistemicobjective,aimedatuncertaintyreduction,
emerges naturally from active inference, as explained in Section 2. In summary, our model offers a
comprehensiveaccountofprediction-basedwrittentextprocessing,involvinghierarchicalinferenceofthe
content being read, prediction of upcoming textual elements, and active testing of predictions through
saccades, which, in turn, inform further inference.
Interestingly, our model also enables the simulation of abnormal eye movement patterns observed in
reading disorders such as dyslexia [47, 35, 60]. While the simulations presented here serve as a proof of
concept,theyqualitativelyreplicateempiricalfindingsshowingthatindividualswithdyslexia,compared
toneurotypicalreaders,exhibitfragmentedtextprocessing. Thisischaracterizedbyanincreasednumber
of shorter saccades during the reading of both single words (Simulation 1) and sentences (Simulation 2)
[92,12,36,70]. Ourmodelattributesthesereadingdeficitstoaspecificdisorderofpredictiveprocessing:
namely, an attenuation of the influence of prior information during inference [39]. By introducing noise
into the model’s transition functions, we impair its working memory of prior inference steps, thereby
forcing it to repeatedly gather evidence. This mechanism, which reflects the cumulative impact of noise
on inference, accounts for why perceptual difficulties in dyslexia become increasingly pronounced with
longer words and more complex sentence structures [92, 12].
However,itiscrucialtonotethatoursimulationsonlyaimtoprovideaproofofconceptthatdisorders
of reading such as dyslexia can be aligned with hierarchical predictive processing theories. Dyslexia is
highly heterogeneous, and its underlying causes are still heavily debated [56, 93, 68, 82, 83, 91]. It is
unlikelythattheattenuationofpriors[39]exploredinthisarticlewouldprovideacomprehensiveaccount
ofallaspectsofdyslexia. However,ourmodeliseasilyextendabletoincorporateother(non-alternative)
mechanisms that could contribute to reading impairments. For instance, various proposals point to
disordersoflow-levelinformationsamplingandattentionshifting[31],oravisualattentionspandisorder
characterizedbyareductioninthenumberoflettersthatcanbeprocessedinparallel[59]. Additionally,
errors in eye movements (overshooting and undershooting) can be incorporated, by introducing noise in
the(transition)functionmappingeyemovementstosubsequentspatialpositions. Theseproposals(and
others) can be readily integrated into the model by modifying the likelihood function that maps letters
to syllables, as described in Section 2. Furthermore, while this study used predefined levels of noise in
the transition models, these parameters can be fit to human data. Such extensions may pave the way
for more comprehensive investigations of dyslexia within the predictive processing framework.
A limitation of this study is that for simplicity, it uses a relatively simple LLM: BERT [13]. We
selected BERT because it is computationally efficient and provides easy access to explicit probability
distributions over the next words and sentences. Future work could adopt more advanced LLM models,
toprovidemorerobustlinguisticpredictions. Furthermore,wemadesomesimplifyingassumptions. For
example,weassumedthatsyllables,wordsandsentencesarerepresentedassequenceoflettersandthat
readingusesanabsoluteletterpositioncoding. Futureworkcouldinvestigatealternativeassumptions,see
forexample[80]foradiscussionoftheabsoluteversusrelativepositioncodinginreading. Furthermore,
future work could replace the the priors over policies that make transitions between word locations
illustrated in Figure 2 with a more realistic distribution derived from empirical data. Future studies
could also explore more systematically the role of the different model parameters and hyperparameters
(e.g., word and sentence length, amount of noise) and address more complex linguistic datasets.
Additionally,futureworkcouldcomparesidebysidethemodelpresentedherewithestablishedmodels
in the field, such as connectionist personalized models, which have been very successful in explaining
deficits observed in dyslexia [56, 91], and models that simulate eye movements, such as E-Z Reader
[72],SWIFT[17,79,65],U¨ber-Reader[71,87],Glenmore[73,74],SEAM[66],therationalmodelofeye
movements[2],andOB1-Reader[81]. Byfittingspecificparametersofthepresentedmodel(e.g.,different
levelsofnoiseinthetransitionand/orlikelihoodfunctions),itwouldbepossibletodeveloppersonalized
models (or computational phenotypes [78]), potentially shedding light on the mechanisms underlying
reading difficulties and informing the development of effective interventions to improve reading skills.
The model presented here could be expanded to capture hierarchically higher and more abstract
aspects of language processing, such as those related to shared narratives, frames and scripts, which
characterize our cultural niches [50, 76]. When opportunely placed within hierarchical architectures,
these higher-level constructs could produce a cascade of predictions that contextualize and guide lower-
level linguistic predictions and eye movements [5, 26, 57]. The presence of these additional constructs
createsacomplexinterplaybetweenpriorpredictionsatdifferenthierarchicallevels–forexample,those
induced by scripts, statistical regularities of sentences and word frequencies – and between prediction
errors arising at different levels, which remain to be investigated in future studies.
In principle, the model presented here could also be extended to investigate developmental aspects
of language and reading acquisition. Using a pretrained LLM such as BERT can be interpreted as
mimicking the capabilities of adult readers. To simulate developmental trajectories, one could impose
constraints on BERT’s vocabulary or its ability to generate accurate sentence completions. Limiting
the vocabulary would force the model to rely more heavily on syllable-by-syllable reading, engaging a
non-lexical processing route. Reducing prediction accuracy would result in less confident priors, leading
themodeltoskipfewerwords,commitmorereadingerrors,andengageinmorefixationsandbackward
saccades. Whetheramodelwiththeseconstraintscansuccessfullyreplicatethepatternsobservedduring
reading development remains to be evaluated in future studies.
Anotherimportantdirectionforfutureresearchistheintegrationofmorerealisticperceptualmodules.
This would allow the model to capture the complexity of the feature extraction process (e.g., [84]), the
parallelprocessingofmultipleletters(e.g.,[30]),andotherperceptualfactorsknowntoinfluencedyslexia,
such as the beneficial effects of increased letter spacing (e.g., [93]). A possible approach to realize this
integration is to employ neural networks as front-end perceptual modules that provide probabilistic
outputs (e.g., categorical distributions over letters or syllables), which can then be incorporated as
likelihoods within a Bayesian model [28, 75, 62].
Finally, it is worth noting that the hierarchical control architecture proposed here can be extended
to a wide range of cognitive tasks beyond reading. Many such tasks can be naturally decomposed
in hierarchical control terms by considering the separation of timescales between slower, higher-level
perceptualandactionprocessesandfaster,lower-levelones[4,58,41,22]. Forinstance,spatialnavigation
often involves high-level planning over goals or subgoals and low-level motor actions required to reach
them [86, 16, 85, 14]. Similarly, action observation and understanding can be structured hierarchically
bydistinguishingbetweendistalgoals,proximalintentions,andimmediatemotoracts[63]. Futurework
could adapt the hierarchical framework introduced here to explore these and other cognitive domains,
shedding light on how flexible, multi-timescale processing supports complex behavior.
Acknowledgments
This research received funding from the European Research Council under the Grant Agreement No.
820213 (ThinkAhead), the Italian National Recovery and Resilience Plan (NRRP), M4C2, funded by
the European Union – NextGenerationEU (Project IR0000011, CUP B51E22000150006, “EBRAINS-
Italy”; Project PE0000013, “FAIR”; Project PE0000006, “MNESYS”), and the Ministry of University
andResearch,PRINPNRRP20224FESY,PRIN20229Z7M8NandPRIN2020529PCP.TheGEFORCE
QuadroRTX6000andTitanGPUcardsusedforthisresearchweredonatedbytheNVIDIACorporation.
We used a Generative AI model to correct typographical errors and edit language for clarity.
References
[1] Christopher Baldassano, Janice Chen, Asieh Zadbood, Jonathan W Pillow, Uri Hasson, and Ken-
nethANorman. Discoveringeventstructureincontinuousnarrativeperceptionandmemory. Neu-
ron, 95(3):709–721, 2017.
[2] KlintonBicknellandRogerLevy. Arationalmodelofeyemovementcontrolinreading. InProceed-
ings of the 48th annual meeting of the Association for Computational Linguistics,pages1168–1178,
2010.
[3] ChristopherMBishopandNasserMNasrabadi.Patternrecognitionandmachinelearning.Springer,
2006.
[4] Matthew M Botvinick, Yael Niv, and Andew G Barto. Hierarchically organized behavior and its
neural foundations: A reinforcement learning perspective. cognition, 113(3):262–280, 2009.
[5] Nabil Bouizegarene, Maxwell JD Ramstead, Axel Constant, Karl J Friston, and Laurence J Kir-
mayer. Narrative as active inference: an integrative account of cognitive and social functions in
adaptation. Frontiers in Psychology, 15:1345480, 2024.
[6] Maria Pia Bucci, Dominique Br´emond-Gignac, and Zo¨ı Kapoula. Poor binocular coordination of
saccadesindyslexicchildren. Graefe’sarchiveforclinicalandexperimentalophthalmology,246:417–
428, 2008.
[7] ReeseButterfussandPanayiotaKendeou. Theroleofexecutivefunctionsinreadingcomprehension.
Educational Psychology Review, 30:801–826, 2018.
[8] Cheryl M Capek, Daphne Bavelier, David Corina, Aaron J Newman, Peter Jezzard, and Helen J
Neville. Thecorticalorganizationofaudio-visualsentencecomprehension: anfmristudyat4tesla.
Cognitive brain research, 20(2):111–119, 2004.
[9] Charlotte Caucheteux, Alexandre Gramfort, and Jean-R´emi King. Evidence of a predictive coding
hierarchy in the human brain listening to speech. Nature human behaviour, 7(3):430–441, 2023.
[10] Claire HC Chang, Samuel A Nastase, and Uri Hasson. Information flow across the cortical
timescalehierarchyduringnarrativeconstruction. ProceedingsoftheNationalAcademyofSciences,
119(51):e2209307119, 2022.
[11] Max Coltheart. Modeling reading: The dual-route approach. The science of reading: A handbook,
1:6–23, 2005.
[12] Maria De Luca, Enrico Di Pace, Anna Judica, Donatella Spinelli, and Pierluigi Zoccolotti. Eye
movement patterns in linguistic and non-linguistic tasks in developmental surface dyslexia. Neu-
ropsychologia, 37(12):1407–1420, 1999.
[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. NAACL HLT 2019 - 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies - Proceedings of the Conference, 1:4171 – 4186, 2019. Cited by: 22093.
[14] Francesco Donnarumma, Marcello Costantini, Ettore Ambrosini, Karl Friston, and Giovanni Pez-
zulo. Action perception as hypothesis testing. Cortex, 89:45–60, 2017.
[15] Francesco Donnarumma, Haris Dindo, Pierpaolo Iodice, and Giovanni Pezzulo. You cannot speak
andlistenatthesametime: Aprobabilisticmodelofturn-taking.Biologicalcybernetics,111(2):165–
183, 2017.
[16] FrancescoDonnarumma,DomenicoMaisto,andGiovanniPezzulo. Problemsolvingasprobabilistic
inference with subgoaling: explaining human successes and pitfalls in the tower of hanoi. PLoS
computational biology, 12(4):e1004864, 2016.
[17] RalfEngbert,AntjeNuthmann,EikeMRichter,andReinholdKliegl. Swift: adynamicalmodelof
saccade generation during reading. Psychological review, 112(4):777, 2005.
[18] Marcello Ferro, Dimitri Ognibene, Giovanni Pezzulo, and Vito Pirrelli. Reading as active sensing:
a computational model of gaze planning during word recognition. Frontiers in Neurorobotics, 4:6,
2010.
[19] StefanLFrank,LeunJOtten,GiuliaGalli,andGabriellaVigliocco.Theerpresponsetotheamount
of information conveyed by words in sentences. Brain and language, 140:1–11, 2015.
[20] Leon Franzen, Zoey Stark, and Aaron P Johnson. Individuals with dyslexia use a different visual
sampling strategy to read text. Scientific reports, 11(1):6449, 2021.
[21] Karl Friston. A theory of cortical responses. Philosophical transactions of the Royal Society B:
Biological sciences, 360(1456):815–836, 2005.
[22] Karl Friston. Hierarchical models in the brain. PLoS computational biology, 4(11):e1000211, 2008.
[23] Karl Friston. The free-energy principle: a unified brain theory? Nature reviews neuroscience,
11(2):127–138, 2010.
[24] KarlFriston,RickAAdams,LaurentPerrinet,andMichaelBreakspear. Perceptionsashypotheses:
saccades as experiments. Frontiers in psychology, 3:151, 2012.
[25] Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, and Giovanni Pezzulo.
Active inference: a process theory. Neural computation, 29(1):1–49, 2017.
[26] KarlJFriston,MaxwellJDRamstead,AlexBKiefer,AlexanderTschantz,ChristopherLBuckley,
MahaultAlbarracin,RiddhiJPitliya,ConorHeins,BrennanKlein,BerenMillidge,etal. Designing
ecosystems of intelligence from first principles. Collective Intelligence, 3(1):26339137231222481,
2024.
[27] Karl J Friston, Noor Sajid, David Ricardo Quiroga-Martinez, Thomas Parr, Cathy J Price, and
Emma Holmes. Active listening. Hearing research, 399:107998, 2021.
[28] DileepGeorge,WolfgangLehrach,KenKansky,MiguelLa´zaro-Gredilla,ChristopherLaan,Bhaskara
Marthi, Xinghua Lou, Zhaoshi Meng, Yi Liu, Huayan Wang, et al. A generative vision model that
trainswithhighdataefficiencyandbreakstext-basedcaptchas. Science,358(6368):eaag2612,2017.
[29] Ariel Goldstein, Zaid Zada, Eliav Buchnik, Mariano Schain, Amy Price, Bobbi Aubrey, Samuel A
Nastase, Amir Feder, Dotan Emanuel, Alon Cohen, et al. Shared computational principles for
language processing in humans and deep language models. Nature neuroscience, 25(3):369–380,
2022.
[30] T Hannagan, A Agrawal, L Cohen, and S Dehaene. Emergence of a compositional neural code for
writtenwords: Recyclingofaconvolutionalneuralnetworkforreading. Proceedings of the National
Academy of Sciences, 118(46):e2104779118, 2021.
[31] Riitta Hari, Hanna Renvall, and Topi Tanskanen. Left minineglect in dyslexic adults. Brain,
124(7):1373–1380, 2001.
[32] Uri Hasson, Janice Chen, and Christopher J Honey. Hierarchical process memory: memory as an
integral component of information processing. Trends in cognitive sciences, 19(6):304–313, 2015.
[33] Stefan Hawelka, Benjamin Gagl, and Heinz Wimmer. A dual-route perspective on eye movements
of dyslexic readers. Cognition, 115(3):367–379, 2010.
[34] Micha Heilbron, Kristijan Armeni, Jan-Mathijs Schoffelen, Peter Hagoort, and Floris P De Lange.
A hierarchy of linguistic predictions during natural language comprehension. Proceedings of the
National Academy of Sciences, 119(32):e2201968119, 2022.
[35] TzipiHorowitz-Kraus,NicoleCicchino,MeravAmiel,ScottKHolland,andZviaBreznitz. Reading
improvement in english-and hebrew-speaking children with reading difficulties after reading accel-
eration training. Annals of dyslexia, 64:183–201, 2014.
[36] FlorianHutzlerandHeinzWimmer. Eyemovementsofdyslexicchildrenwhenreadinginaregular
orthography. Brain and language, 89(1):235–242, 2004.
[37] Jungsik Hwang, Jinhyung Kim, Ahmadreza Ahmadi, Minkyu Choi, and Jun Tani. Dealing with
large-scalespatio-temporalpatternsinimitativeinteractionbetweenarobotandahumanbyusing
the predictive coding framework. IEEE Transactions on Systems, Man, and Cybernetics: Systems,
50(5):1918–1931, 2018.
[38] TakuyaIsomura,KiyoshiKotani,YasuhikoJimbo,andKarlFriston. Experimentalvalidationofthe
free-energy principle with in vitro neural networks. bioRxiv, pages 2022–10, 2022.
[39] Sagi Jaffe-Dax, Ofri Raviv, Nori Jacoby, Yonatan Loewenstein, and Merav Ahissar. A computa-
tional model of implicit memory captures dyslexics’ perceptual deficits. Journal of Neuroscience,
35(35):12116–12126, 2015.
[40] StephanieJaintaandZo¨ıKapoula. Dyslexicchildrenareconfrontedwithunstablebinocularfixation
while reading. PloS one, 6(4):e18694, 2011.
[41] Etienne Koechlin and Christopher Summerfield. An information theoretical approach to prefrontal
executive function. Trends in cognitive sciences, 11(6):229–235, 2007.
[42] Miika Koskinen, Mikko Kurimo, Joachim Gross, Aapo Hyva¨rinen, and Riitta Hari. Brain activity
reflectsthepredictabilityofwordsequencesinlistenedcontinuousspeech. Neuroimage,219:116936,
2020.
[43] Marta Kutas and Steven A Hillyard. Brain potentials during reading reflect word expectancy and
semantic association. Nature, 307(5947):161–163, 1984.
[44] Franklin Mark Liang. Word Hy-phen-a-tion by Com-put-er. PhD thesis, Citeseer, 1983.
[45] G Reid Lyon, Sally E Shaywitz, and Bennett A Shaywitz. A definition of dyslexia. Annals of
dyslexia, 53:1–14, 2003.
[46] ManfredMacKeben,SusanneTrauzettel-Klosinski,JensReinhard,UteDu¨rrw¨achter,MartinAdler,
and Gunther Klosinski. Eye movement control during single-word reading in dyslexics. Journal of
Vision, 4(5):4–4, 2004.
[47] Jos´eMMaisog, Erin REinbinder, DLynnFlowers, PeterETurkeltaub, andGuinevereF Eden. A
meta-analysis of functional neuroimaging studies of dyslexia. Annals of the new York Academy of
Sciences, 1145(1):237–259, 2008.
[48] Domenico Maisto, Francesco Donnarumma, and Giovanni Pezzulo. Interactive inference: a multi-
agent model of cooperative joint actions. IEEE Transactions on Systems, Man, and Cybernetics:
Systems, 2023.
[49] Christopher D Manning, Kevin Clark, John Hewitt, Urvashi Khandelwal, and Omer Levy. Emer-
gent linguistic structure in artificial neural networks trained by self-supervision. Proceedings of the
National Academy of Sciences, 117(48):30046–30054, 2020.
[50] Marvin Minsky. Society of mind. Simon and Schuster, 1986.
[51] Dennis Norris. The bayesian reader: explaining word recognition as an optimal bayesian decision
process. Psychological review, 113(2):327, 2006.
[52] Beth A O’Brien, J Stephen Mansfield, and Gordon E Legge. The effect of print size on reading
speed in dyslexia. Journal of Research in Reading, 28(3):332–349, 2005.
[53] Guillermo Oliver, Pablo Lanillos, and Gordon Cheng. An empirical study of active inference on a
humanoidrobot. IEEETransactionsonCognitiveandDevelopmentalSystems,14(2):462–471,2021.
[54] Giorgio Parisi. Statistical field theory. Frontiers in physics. Addison-Wesley, Redwood City, CA,
1988.
[55] Thomas Parr, Giovanni Pezzulo, and Karl J Friston. Active inference: the free energy principle in
mind, brain, and behavior. MIT Press, 2022.
[56] Conrad Perry, Marco Zorzi, and Johannes C Ziegler. Understanding dyslexia through personalized
large-scale computational models. Psychological science, 30(3):386–395, 2019.
[57] Giovanni Pezzulo, Gu¨nther Knoblich, Domenico Maisto, Francesco Donnarumma, Elisabeth
Pacherie, and Uri Hasson. A predictive processing framework for joint action and communication.
PsyArXiv Preprints, 2025.
[58] Giovanni Pezzulo, Francesco Rigoli, and Karl J Friston. Hierarchical active inference: a theory of
motivated control. Trends in cognitive sciences, 22(4):294–306, 2018.
[59] Chlo´ePrado,MatthieuDubois,andSylvianeValdois.Theeyemovementsofdyslexicchildrenduring
reading and visual search: impact of the visual attention span. Vision research, 47(19):2521–2530,
2007.
[60] Cathy J Price. A review and synthesis of the first 20 years of pet and fmri studies of heard speech,
spoken language and reading. Neuroimage, 62(2):816–847, 2012.
[61] MatteoPriorelli,FedericoMaggiore,AntonellaMaselli,FrancescoDonnarumma,DomenicoMaisto,
Francesco Mannella, Ivilin Peev Stoianov, and Giovanni Pezzulo. Modeling motor control in
continuous-time active inference: a survey. IEEE Transactions on Cognitive and Developmental
Systems, 2023.
[62] MatteoPriorelliandIvilinPeevStoianov. Flexibleintentions: Anactiveinferencetheory. Frontiers
in Computational Neuroscience, 17:1128694, 2023.
[63] Riccardo Proietti, Giovanni Pezzulo, and Alessia Tessari. An active inference model of hierarchical
action understanding, learning and imitation. Physics of Life Reviews, 46:92–118, 2023.
[64] FriedemannPulvermu¨ller. Wordsinthebrain’slanguage. Behavioralandbrainsciences,22(2):253–
279, 1999.
[65] Maximilian M Rabe, Johan Chandra, Andr´e Kru¨gel, Stefan A Seelig, Shravan Vasishth, and Ralf
Engbert. Abayesianapproachtodynamicalmodelingofeye-movementcontrolinreadingofnormal,
mirrored, and scrambled texts. Psychological Review, 128(5):803, 2021.
[66] MaximilianMRabe,DarioPaape,DanielaMertzen,ShravanVasishth,andRalfEngbert.Seam: An
integrated activation-coupled model of sentence processing and eye movements in reading. Journal
of Memory and Language, 135:104496, 2024.
[67] AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal. Language
models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[68] Franck Ramus. Developmental dyslexia: specific phonological deficit or general sensorimotor dys-
function? Current opinion in neurobiology, 13(2):212–218, 2003.
[69] Rajesh PN Rao and Dana H Ballard. Predictive coding in the visual cortex: a functional interpre-
tation of some extra-classical receptive-field effects. Nature neuroscience, 2(1):79–87, 1999.
[70] Keith Rayner. Eye movements, perceptual span, and reading disability. Annals of Dyslexia, pages
163–173, 1983.
[71] Erik D Reichle. Computational models of reading: A handbook. Oxford University Press, 2021.
[72] Erik D Reichle, Keith Rayner, and Alexander Pollatsek. Toward a model of eye movement control
in reading. Psychological review, 105(1):125, 1998.
[73] RonanGReillyandRRadach. Glenmore: Aninteractiveactivationmodelofeyemovementcontrol
in reading. In Proceedings of the 9th International Conference on Neural Information Processing,
2002. ICONIP’02., volume 3, pages 1194–1200. IEEE, 2002.
[74] Ronan G Reilly and Ralph Radach. Some empirical tests of an interactive activation model of eye
movement control in reading. Cognitive Systems Research, 7(1):34–55, 2006.
[75] Cansu Sancaktar, Marcel AJ Van Gerven, and Pablo Lanillos. End-to-end pixel-based deep active
inference for body perception and action. In 2020 Joint IEEE 10th International Conference on
Development and Learning and Epigenetic Robotics (ICDL-EpiRob), pages 1–8. IEEE, 2020.
[76] Roger C Schank and Robert P Abelson. Scripts, plans, goals, and understanding: An inquiry into
human knowledge structures. Psychology press, 2013.
[77] Lea-Maria Schmitt, Julia Erb, Sarah Tune, Anna U Rysop, Gesa Hartwigsen, and Jonas Obleser.
Predicting speech from a cortical hierarchy of event-based time scales. Science Advances,
7(49):eabi6070, 2021.
[78] Philipp Schwartenbeck and Karl Friston. Computational phenotyping in psychiatry: a worked
example. eneuro, 3(4), 2016.
[79] Joshua D Seelig, Tyler Jay, Alison R Preston, Gabriele Fey, and Robert L Goldstone. Bayesian
surprise predicts human event segmentation in story listening. bioRxiv, 2020.
[80] Joshua Snell, Jonathan Grainger, and Martijn Meeter. Relative letter-position coding revisited.
Psychonomic Bulletin & Review, 29(3):995–1002, 2022.
[81] Joshua Snell, Sam van Leipsig, Jonathan Grainger, and Martijn Meeter. Ob1-reader: A model of
word recognition and eye movements in text reading. Psychological review, 125(6):969, 2018.
[82] JohnSteinandVincentWalsh. Toseebutnottoread;themagnocellulartheoryofdyslexia. Trends
in neurosciences, 20(4):147–152, 1997.
[83] Paula Tallal. Improving language and literacy is a matter of time. Nature Reviews Neuroscience,
5(9):721–728, 2004.
[84] Alberto Testolin, Ivilin Stoianov, and Marco Zorzi. Letter perception emerges from unsupervised
deeplearningandrecyclingofnaturalimagefeatures. Naturehumanbehaviour,1(9):657–664,2017.
[85] MomchilSTomov,SamyuktaYagati,AgniKumar,WanqianYang,andSamuelJGershman.Discov-
eryofhierarchicalrepresentationsforefficientplanning.PLoScomputationalbiology,16(4):e1007594,
2020.
[86] Toon Van de Maele, Bart Dhoedt, Tim Verbelen, and Giovanni Pezzulo. Bridging cognitive maps:
A hierarchical active inference model of spatial alternation tasks and the hippocampal-prefrontal
circuit. arXiv preprint arXiv:2308.11463, 2023.
[87] Aaron Veldre, Lili Yu, Sally Andrews, and Erik D Reichle. Towards a complete model of reading:
Simulatinglexicaldecision,wordnaming, andsentencereadingwithu¨ber-reader. InProceedings of
the 42nd annual conference of the cognitive science society. Cognitive Science Society, 2020.
[88] Hugo Weissbart, Katerina D Kandylaki, and Tobias Reichenbach. Cortical tracking of surprisal
during continuous speech comprehension. Journal of cognitive neuroscience, 32(1):155–166, 2020.
[89] Roel M Willems, Stefan L Frank, Annabel D Nijhof, Peter Hagoort, and Antal Van den Bosch.
Prediction during natural language comprehension. Cerebral Cortex, 26(6):2506–2516, 2016.
[90] Asieh Zadbood, Janice Chen, Yuan Chang Leong, Kenneth A Norman, and Uri Hasson. How we
transmit memories to other brains: constructing shared neural representations via communication.
Cerebral cortex, 27(10):4988–5000, 2017.
[91] Johannes C Ziegler, Conrad Perry, and Marco Zorzi. Learning to read and dyslexia: From theory
to intervention through personalized computational models. Current Directions in Psychological
Science, 29(3):293–300, 2020.
[92] PierluigiZoccolotti,MariaDeLuca,EnricoDiPace,FilippoGasperini,AnnaJudica,andDonatella
Spinelli. Word length effect in early reading and in developmental dyslexia. Brain and language,
93(3):369–373, 2005.
[93] Marco Zorzi, Chiara Barbiero, Andrea Facoetti, Isabella Lonciari, Marco Carrozzi, Marcella Mon-
tico, Laura Bravar, Florence George, Catherine Pech-Georgel, and Johannes C Ziegler. Extra-
largeletterspacingimprovesreadingindyslexia. Proceedings of the National Academy of Sciences,
109(28):11455–11459, 2012.
Supplementary materials
Integrating Large Language Models
and Active Inference
to understand eye movements
in Reading and Dyslexia
Francesco Donnarumma
Mirco Frosolone
Giovanni Pezzulo
Institute of Cognitive Sciences and Technologies
National Research Council
Via Gian Domenico Romagnosi, 18A
00196 Rome Italy
Corresponding author: Giovanni Pezzulo E-mail: giovanni.pezzulo@istc.cnr.it
1
Supplementary info
In this document, we provide the tables of statistics from the simulations presented in the main article
(foundinSupplementaryTablessection)andtheDatasetsofsentencesandwordsusedinthesimulation
(found in Datasets section).
Supplementary tables
Belowarethesupplementarytablespresentingstatisticsrelatedtothesimulationsdiscussedinthemain
manuscript.
Supplementary tables for Simulation 1
Table S.1 displays the accuracy and probability assigned to the correct words. Table S.2 shows the
T-tests on total saccades and Table S.3 the backward saccades. Table S.4 and Table S.5 presents the
T-testsfortheamplitudeofforwardandbackwardsaccadesrespectively. Thep-valueslowerto0.05are
indicated in bold.
Table S.1: Simulation 1: Reading words of 4 or 8 letters. The table presents the accuracy and the
probabilityassignedtothecorrectwordsby4differentmodels: theControl model (CM),theDyslexic model
(DM) variant with noise at Level 1 only, the Dyslexic model (DM) variant with noise at Level 2 only, and
the Dyslexic model (DM) with noise at both levels. See Figure 4 and the main text for details.
4-letterword 8-letterword
Accuracy Probability Accuracy Probability
ControlModel 100 1.00±1.03·10−14 100 1.00±7.85·10−15
DM-NoiseonLevel1 100 1.00±3.32·10−04 100 0.99±3.82·10−04
DM-NoiseonLevel2 99 0.99±3.73·10−15 97 0.97±1.38·10−02
DyslexicModel 99 0.99±3.32·10−04 97 0.97±1.15·10−02
Table S.2: Simulation 1. T-tests for simulation of 4-letter and 8-letter word reading. The table displays
the results of T-tests conducted on the total number of saccades for different models with respect to the
Control model (CM): the Dyslexic model (DM) variant with noise at Level 1 (syllable) only, the Dyslexic
model (DM) variant with noise at Level 2 (word) only, and the Dyslexic model (DM) with noise at both
levels. See Figure 5 and the main text for details.
Total number of saccades
4 - letter word 8 - letter word
DM - Noise Level 1 F[1,198] = 17.6,p ≈ 4.1·10−05 F[1,198] = 12.7,p ≈ 4.5·10−04
DM - Noise Level 2 F[1,196] = 4.4,p ≈ 3.6·10−02 F[1,190] = 5.2,p ≈ 2.4·10−02
Dyslexic Model F[1,196] = 16.7,p ≈ 6.4·10−05 F[1,192] = 10.7,p ≈ 1.3·10−03
Supplementary tables for Simulation 2
Table S.6 display the accuracy and probability assigned to the correct words. Table S.7 and Table S.8
shows the T-tests on total saccades and back saccades. Table S.9 presents the T-tests for amplitude of
forward and Table S.10 the T-test for amplitude of backward saccades respectively. The p-values lower
to 0.05 are indicated in bold.
2
Table S.3: Simulation 1. T-tests for simulation of 4-letter and 8-letter word reading. The table displays
the results of T-tests conducted on the number of backward saccades for different models with respect to
theControl model (CM):theDyslexic model (DM)variantwithnoiseatLevel1(syllable)only, theDyslexic
model (DM) variant with noise at Level 2 (word) only, and the Dyslexic model (DM) with noise at both
levels. See Figure 5 and the main text for details.
Number of backward saccades
4 - letter word 8 - letter word
DM - Noise Level 1 F[1,198] = 21.5,p ≈ 6.5·10−06 F[1,198] = 17.5,p ≈ 4.4·10−05
DM - Noise Level 2 F[1,198] = 4.8,p ≈ 3.0·10−02 F[1,190] = 5.7,p ≈ 1.8·10−02
Dyslexic Model F[1,196] = 21.7,p ≈ 5.8·10−06 F[1,192] = 13.3,p ≈ 3.4·10−04
Table S.4: Simulation 1. T-tests for simulation of 4-letter and 8-letter word reading. The table displays
the results of T-tests conducted on the amplitude of forward saccades for different models with respect to
theControl model (CM):theDyslexic model (DM)variantwithnoiseatLevel1(syllable)only, theDyslexic
model (DM) variant with noise at Level 2 (word) only, and the Dyslexic model (DM) with noise at both
levels. See Figure 6 and the main text for details.
Amplitude of forward saccades
4 - letter word 8 - letter word
Dyslexic Model F[1,844] = 9.4,p ≈ 2.2·10−03 F[1,678] = 46.5,p ≈ 2.0·10−11
Table S.5: Simulation1. T-testsforsimulationof4-letterand8-letterwordreading. Thetabledisplaysthe
results of T-tests conducted on the amplitude of + backward saccades for different models with respect to
theControl model (CM):theDyslexic model (DM)variantwithnoiseatLevel1(syllable)only, theDyslexic
model (DM) variant with noise at Level 2 (word) only, and the Dyslexic model (DM) with noise at both
levels. See Figure 6 and the main text for details.
Amplitude of backward saccades
4 - letter word 8 - letter word
Dyslexic Model F[1,321] = 133.5,p ≈ 4.6·10−26 F[1,312] = 24.5,p ≈ 1.2·10−06
Table S.6: Simulation 2: Reading sentences of 4 or 8 words. The table presents the accuracy and the
probabilityassignedtothecorrectwordsby4differentmodels: theControl model (CM),theDyslexic model
(DM) variant with noise at Level 1 only, the Dyslexic model (DM) variant with noise at Level 2 only, the
Dyslexic model (DM) variant with noise at Level 3 only, and the Dyslexic model (DM) with noise at all
levels. See Figure 4 and the main text for details.
4 - word sentence 8 - word sentence
Accuracy Probability Accuracy Probability
Control Model 100 1.00±3.66·10−16 100 1.00±3.21·10−12
DM - Noise on Level 1 100 1.00±3.64·10−06 100 1.00±5.69·10−08
DM - Noise on Level 2 100 1.00±3.40·10−07 100 1.00±3.21·10−12
DM - Noise on Level 3 97 0.97±4.25·10−03 93 0.93±1.04·10−02
Dyslexic Model 96 0.96±4.41·10−03 93 0.93±1.04·10−02
3
Table S.7: Simulation 2. T-tests for simulation of 4-word and 8-word sentence reading. The table displays
the results of T-tests conducted on the total number of saccades for different models with respect to the
Control model (CM): the Dyslexic model (DM) variant with noise at Level 1 (syllable) only, the Dyslexic
model (DM) variant with noise at Level 2 (word) only, the Dyslexic model (DM) variant with noise at Level
3 (sentence) only, and the Dyslexic model (DM) with noise at all levels. See Figure 7 and the main text for
details.
Total number of saccades
4 - word sentence 8 - word sentence
DM - Noise Level 1 F[1,198] = 0.02, p ≈ 0.90 F[1,198] = 0.01, p ≈ 0.92
DM - Noise Level 2 F[1,198] = 0.97, p ≈ 0.45 F[1,198] = 0.45, p ≈ 0.58
DM - Noise Level 3 F[1,198] = 66.8,p ≈ 3.5·10−14 F[1,196] = 105.9,p ≈ 3.9·10−20
Dyslexic Model F[1,198] = 75.5,p ≈ 1.4·10−15 F[1,196] = 113.6,p ≈ 3.2·10−21
Table S.8: Simulation 2. T-tests for simulation of 4-word and 8-word sentence reading. The table displays
the results of T-tests conducted on the number of backward saccades for different models with respect to
theControl model (CM):theDyslexic model (DM)variantwithnoiseatLevel1(syllable)only, theDyslexic
model (DM) variant with noise at Level 2 (word) only, the Dyslexic model (DM) variant with noise at Level
3 (sentence) only, and the Dyslexic model (DM) with noise at all levels. See Figure 7 and the main text for
details.
Number of backward saccades
4 - word sentence 8 - word sentence
DM - Noise Level 1 F[1,198] = 0.45, p ≈ 0.52 F[1,198] = 0.82, p ≈ 0.46
DM - Noise Level 2 F[1,198] = 0.01, p ≈ 0.90 F[1,198] = 1.86, p ≈ 0.17
DM - Noise Level 3 F[1,198] = 44.7,p ≈ 2.3·10−10 F[1,196] = 119.4,p ≈ 5.1·10−22
Dyslexic Model F[1,198] = 53.2,p ≈ 7.1·10−12 F[1,196] = 117.7,p ≈ 8.8·10−22
Table S.9: Simulation 2. T-tests for simulation of 4-word and 8-word sentence reading. The table displays
the results of T-tests conducted on the amplitude of forward saccades for different models with respect to
theControl model (CM):theDyslexic model (DM)variantwithnoiseatLevel1(syllable)only, theDyslexic
model (DM) variant with noise at Level 2 (word) only, the Dyslexic model (DM) variant with noise at Level
3 (sentence) only, and the Dyslexic model (DM) with noise at all levels. See Figure 8 and the main text for
details.
Amplitude of forward saccades
4 - word sentence 8 - word sentence
Dyslexic Model F[1,878] = 18.0,p ≈ 2.5·10−05 F[1,1238] = 10.9,p ≈ 9.8·10−04
Simulation 4
Table S.11 and Table S.12 presents T-tests on total number of saccades and the number backward
saccades when the model is augmented with the topic of the sentence at Level 3 of the hierarchy. The
p-values lower to 0.05 are indicated in bold.
4
Table S.10: Simulation2. T-testsforsimulationof4-wordand8-wordsentencereading. Thetabledisplays
the results of T-tests conducted on the amplitude of backward saccades for different models with respect to
theControl model (CM):theDyslexic model (DM)variantwithnoiseatLevel1(syllable)only, theDyslexic
model (DM) variant with noise at Level 2 (word) only, the Dyslexic model (DM) variant with noise at Level
3 (sentence) only, and the Dyslexic model (DM) with noise at all levels. See Figure 8 and the main text for
details.
Amplitude of backward saccades
4 - word sentence 8 - word sentence
Dyslexic Model F[1,318] = 14.7,p ≈ 1.5·10−04 F[1,350] = 42.5,p ≈ 2.5·10−10
Table S.11: Simulation4. T-testsforsimulationofa9-wordsentencereading,belongingtodifferenttopics.
ThetabledisplaystheresultsofT-testsconductedonthetotalnumberofsaccadesfordifferentmodelswith
respect to the Control model (CM): the Dyslexic model (DM) variant with noise at Level 1 (syllable) only,
the Dyslexic model (DM) variant with noise at Level 2 (word) only, the Dyslexic model (DM) variant with
noise at Level 3 (sentence) only, and the Dyslexic model (DM) with noise at all levels. See Figure 10 and
the main text for details.
Total number of saccades
Flat Priors VS Informative Priors F[1,198] = 4.1,p ≈ 4.1·10−03
Table S.12: Simulation4. T-testsforsimulationofa9-wordsentencereading,belongingtodifferenttopics.
The table displays the results of T-tests conducted on the number of backward saccades for different models
with respect to the Control model (CM): the Dyslexic model (DM) variant with noise at Level 1 (syllable)
only, the Dyslexic model (DM) variant with noise at Level 2 (word) only, the Dyslexic model (DM) variant
with noise at Level 3 (sentence) only, and the Dyslexic model (DM) with noise at all levels. See Figure 10
and the main text for details.
Number of backward saccades
Flat Priors VS Informative Priors F[1,198] = 2.6, p ≈ 0.11
Datasets
Inthissection,wepresentthedatasetsutilizedinoursimulations. Informationregardingcontentlength
inthedatasetscanbefoundinTableS.13. Furthermore,TableS.14displaysthedictionaryofwordsused
inSimulation1, whileTableS.15providesinsightintothesentencesusedinSimulation2. Additionally,
Simulation 4’s sentences and corresponding labels are showcased in Table S.17.
Table S.13: This table reports the mean number of characters, the standard deviation, minimum and
maximum number of characters that compose the relative set of sentences used in the simulations.
Standard
Mean Min Max
Deviation
Simulation 2 (4 words) 25.96 1.96 21 31
Simulation 2 (8 words) 50.24 1.64 47 55
Simulation 3 (4 words - 1 Unknown) 25.46 1.92 22 32
Simulation 4 69.55 5.02 61 82
5
Table S.14: DictionaryusedinSimulation1(wordsextractedfromBERTDictionary)
1 - letter 2- letter 3- letter 4- letter 5- letter 6- letter 7- letter 8- letter
A AC AMC ARES AGREE ACTING AIRPORT ACQUIRED
B AL ARM ARTS ANZAC ADVENT ALCOHOL ACTUALLY
C AM ATA ASKS ARDEN AIRMEN ANGRILY ADVANCED
D AR AUS BEDS ARIEL ANGLIA ARCADIA ALPHABET
E AW BAO BORE AZURE ARMAND ASSURED AMERICAN
F AX BEG BRET BARED AUSTEN AVENUES APPLETON
G BF BEN BUSY BEGUN BECKER BALANCE ARGUMENT
H BI BIT BUTT BIRDS BEIRUT BENGALS ARRANGED
I BK BOP CALE BLAND BERMAN BERWICK ATLANTIC
J BO BUY CARL BUICK BIGGER BOUNCED BALANCED
K BR CAB COAL BULGE BISHOP BURUNDI BASILICA
L BT CAD COLA BUTCH BUMPER CAREERS BENEDICT
M CA CAI COOL CELLO BURDEN CEILING BOTSWANA
N CB CEO CUBS CLEAR CANTOR CHANCEL BOULDERS
O CH COE DIRT CLUBS CAUSAL CLICKED BREACHED
P CL CPU DOIN CODES CEREAL CLIENTS BREAKING
Q CM DEX DOVE COSTS CESARE COMMITS CADILLAC
R CN DIG DRIP CRIED CESSNA CROOKED CAMPBELL
S CO DIP DRUM CURLY CHICKS DERRICK CARRYING
T CP EKI ELLE DANNY CLOCKS DESCENT CATALINA
U CU EPA EMIR DATED COINED DESPAIR COLISEUM
V ED ETA EMMA DEBUT DECKER DEVISED COLUMBIA
W EH FIG FANS DISCS DISMAY EDUARDO CONNECTS
X EL FLU FEES DOUBT DRINKS EMBASSY CONVINCE
Y EM FUN FIJI DUCKS EXEMPT EMERSON CUMMINGS
Z EU HAN FINS DUMMY FACADE EMPLOYS CURRENCY
FI HBO FLOP ECOLE FINLEY ENEMIES CYCLONES
FL HEM FLUX ELIZA GENTRY ESCAPES CYLINDER
FT HIT FRAN ESSEN HAILEY EXHAUST DARKENED
FU HOP GIFT FEMME HANNAH FAINTLY DECEASED
FX HUE GIGS FISTS HASSAN FLEDGED DECIDING
GE IBN GLOW FLOOR HATRED FORSTER DISABLED
GI ICE GRAM FLOYD HELMUT FREEWAY DISASTER
GO ICH GREY GOUGH HOOVER FRONTED DONNELLY
GS ICT HAAS GROWS HUTTON GALILEO EMERITUS
GT ILE HANK GRUNT HYBRID GEOLOGY EMPLOYER
GU ION HILL GUARD INCOME GLIDING EPILOGUE
HA IRA HISS HELLO INPUTS HAMBURG EREBIDAE
HC KIM HUEY HIRES INTERN HANDLED EVACUATE
HM LAL INDY HORDE JOSEPH HIGHEST EVOLVING
HO LAY JACE JOINT JUAREZ HONNEUR EXAMINER
HU LEG JAIN JOYCE KHYBER HORNETS FABULOUS
IK LES JEAN LATER LANDON HOWEVER FIGHTERS
IN LOU JING LEANS LENGTH HOWLING FRANKLIN
IO LTD KRIS LEAVE LENNON HUNCHED FRICTION
IR MED LAMA LOOSE LIKELY INHALED GESTURES
IX MPH LANG LOVER LILITH IRANIAN GROUPING
JA MPS LETO LUCHA LITTER JEALOUS HOMICIDE
JD MSC LIAR LYDIA MADRAS JUSTINE IMPERIAL
KE MUD MACK MAHAL MANUEL KNIGHTS INCURRED
KG NBA MARS MERGE MARGIN LABELED INITIATE
KN NHL MASK MINSK MARKED LASTING INJURIES
KO NHS MEAT MISSY MCLEAN LEBANON INSANITY
KS NIK MIKA MIXED MOLINA MANNING JUDICIAL
LC NOS MOJO MOMMA MUSEUM MCBRIDE LABRADOR
LI NOV MOOD NAILS MYSELF MORALLY MAGAZINE
6
LP NPR MUCH NORMA NEWELL MUSCLED MARATHON
MG NYC NEAT OLDER ONIONS NATALIE MEANINGS
MI OFF NINO OUTTA OPENLY NEURONS MOHAMMAD
ML OUT NOSE PAPUA OPPOSE NEVILLE MONSIEUR
MR PAT NRHP PATIO PANZER NIKOLAI MOROCCAN
NA PAU OBOE PIANO PAVING NILSSON MOUNTING
NH PBA OWLS PILES PAYTON NOMINAL MUSHROOM
NI PBS PASS PLUTO PETALS OFFENSE OLYMPIAD
NO PCS PERU PROSE PILOTS ONSTAGE PACKAGES
NS PEI PORN QAEDA PLANES PATCHES PATERSON
NT PEW RAAF RALLY POLITE PEPTIDE PHILLIPS
NZ PHI RAMP REEFS POTION PERFECT POPULACE
OG PHP RAMS REVUE PRAGUE PEUGEOT POUNDING
OH POD RUDD ROACH PRICED PIONEER PREACHER
OL PUN SANK ROUEN PULSES PLANNED PRESTIGE
OP ROE SAXE RUSSO PUPILS PLAYFUL QUARTERS
OS ROM SITU SCOTT RASHID PORTICO RAILROAD
OZ RUM SIZE SHORE REPORT PROTEST RECEIVES
PH RYE SLID SINGH RIPLEY RAPIDLY REJOINED
PO SAO SODA SITED ROBERT READERS SCENARIO
PR SEA SOON SLAMS RUNNER RECITAL SEMINOLE
RC SHI STAY SPRAY SAILOR REDWOOD SHOOTOUT
RR SHU STYX SQUAT SAXONY REISSUE SHOWERED
RY SPP SUFI STADE SCARES ROOSTER SINGULAR
SE SSR SWAM STAFF SCREAM RUINING SOMERSET
SF SUE TEAM STALL SEIZED SECURED SOUTHEND
SM TAI THRU STATE SESAME SILENCE SPECIALS
SQ TEA TODD STERN SEVERN SMASHED SPEEDING
SR TEX TOLD STORM SIGNAL SPELLED STRANDED
TC TIS TOPS STUMP SPEAKS STATION STRIPPED
TU TNA TRIP TOUGH SPINES STEWART SYMBOLIC
TV UFO TUNA TRANS STALLS STRANGE TARGETED
TX VAR UGLY TREAT STARTS STRIKES TAXATION
UP VII USER TREES SURFER SWELLED THROTTLE
UR WAT VEGA VAULT TORINO TIGHTLY TIMELINE
VA WAY VIDA VIJAY TREVOR TOPICAL TOURISTS
VE WOW VISA VISAS TURTLE TURTLES UNCOMMON
VP XII WEEP VOMIT VENDOR ULYSSES VALENTIN
VU XVI WHOA WEARS VERBAL UNITING VIGOROUS
WC YES WIFE WHORE WANTED UNNAMED VILLAINS
XI YET WINS WRAPS WASHED VACANCY WARRIORS
YA YOO WITS WRIST WIRING VISIBLE WEREWOLF
YE ZEE YUKI YEMEN WORTHY WALKING WORRYING
YO ZEV ZOOM YOUTH XAVIER WARTIME ZIMBABWE
7
Table S.15: DictionaryusedinSimulation2
FAITH CONQUERS ALL OBSTA- FAITHCONQUERSALLOBSTACLESANDTRASCENDSALLADVERSITY
CLES
FAITHCONQUERSEACHHURDLE FAITHCONQUERSALLOBSTACLESANDTRASCENDSALLMISFORTUNE
FAITH CONQUERS EACH SET- FAITHCONQUERSEACHHURDLEBUTTRASCENDSALLMISFORTUNE
BACK
FAITH CONQUERS EVERY CHAL- FAITHCONQUERSEACHSETBACKBUTSURMOUNTSALLOBSTACLES
LENGES
FAITH CONQUERS EVERY OB- FAITHCONQUERSEVERYOBSTACLESANDPREVAILSOVERADVERSITY
STACLES
FAITHDEFEATSALLOBSTACLES FAITHCONQUERSEVERYOBSTACLESANDSURMOUNTSALLOBSTACLES
FAITH OVERCOMES ALL HIN- FAITHDEFEATSALLOBSTACLESANDTRASCENDSALLADVERSITY
DRANCES
FAITH OVERCOMES ALL OBSTA- FAITHDEFEATSALLOBSTACLESANDTRASCENDSALLMISFORTUNE
CLES
FAITH OVERPOWERS ALL LIMI- FAITHOVERCOMESALLHINDRANCESANDTRASCENDSOVERMISFORTUNE
TATIONS
FAITH PREVAILS AGAINST BAR- FAITHOVERCOMESALLOBSTACLESANDTRASCENDSALLADVERSITY
RIERS
FAITH PREVAILS AGAINST OB- FAITHOVERCOMESALLOBSTACLESANDTRASCENDSALLMISFORTUNE
STACLES
FAITH PREVAILS OVER EVERY FAITHOVERPOWERSALLLIMITATIONSBUTTRASCENDSALLMISFORTUNE
ADVERSITY
FAITH PREVAILS OVER OBSTA- FAITHPREVAILSAGAINSTBARRIERSANDTRASCENDSOVERMISFORTUNE
CLES
FAITH SURMOUNTS ALL OBSTA- FAITHPREVAILSAGAINSTOBSTACLESANDTRASCENDSALLMISFORTUNE
CLES
FAITH SURPASSES ALL OBSTA- FAITHPREVAILSEVERYADVERSITYBUTSURMOUNTSALLOBSTACLES
CLES
FAITH SURPASSES ALL OPPOSI- FAITHPREVAILSOVERADVERSITYBUTSURMOUNTSALLOBSTACLES
TION
FAITH TRIUMPHS AGAINST OB- FAITHPREVAILSOVEROBSTACLESANDTRASCENDSALLMISFORTUNE
STACLES
FAITH TRIUMPHS OVER CHAL- FAITHSURMOUNTSALLOBSTACLESANDSURMOUNTSALLMISFORTUNE
LENGES
FAITH TRIUMPHS OVER OBSTA- FAITHSURPASSESALLOBSTACLESANDTRASCENDSALLADVERSITY
CLES
FAITH VANQUISHES ALL OBSTA- FAITHSURPASSESALLOBSTACLESANDTRASCENDSALLMISFORTUNE
CLES
FAITH VANQUISHES EVERY DIF- FAITHSURPASSESALLOBSTACLESBUTSURMOUNTSALLADVERSITY
FICULTY
HOPE CONQUERS ALL LIMITA- FAITHSURPASSESALLOPPOSITIONBUTSURMOUNTSALLMISFORTUNE
TIONS
HOPE CONQUERS ALL OBSTA- FAITHTRIUMPHSAGAINSTOBSTACLESANDTRASCENDSALLMISFORTUNE
CLES
HOPE CONQUERS EACH CHAL- FAITHTRIUMPHSOVERCHALLENGESANDTRASCENDSOVERMISFORTUNE
LENGES
HOPE CONQUERS EACH HIN- FAITHTRIUMPHSOVEROBSTACLESANDTRASCENDSALLADVERSITY
DRANCES
HOPECONQUERSEACHHURDLE FAITHTRIUMPHSOVEROBSTACLESANDTRASCENDSALLMISFORTUNE
HOPE CONQUERS EACH INHIBI- FAITHVANQUISHESALLOBSTACLESANDTRASCENDSALLMISFORTUNE
TION
HOPE CONQUERS EACH LIMITA- FAITHVANQUISHESEVERYDIFFICULTYBUTTRASCENDSALLMISFORTUNE
TIONS
HOPE CONQUERS EACH SET- HOPECONQUERSALLOBSTACLESANDTRASCENDSALLADVERSITY
BACK
HOPE CONQUERS EVERY AD- HOPECONQUERSALLOBSTACLESANDTRASCENDSALLMISFORTUNE
VERSITY
8
HOPECONQUERSEVERYBARRI- HOPECONQUERSEACHSETBACKBUTSURMOUNTSALLOBSTACLES
ERS
HOPE CONQUERS EVERY CHAL- HOPEDEFEATSALLOBSTACLESANDTRASCENDSALLADVERSITY
LENGES
HOPE CONQUERS EVERY DIFFI- HOPEDEFEATSALLOBSTACLESANDTRASCENDSALLMISFORTUNE
CULTY
HOPE CONQUERS EVERY HUR- HOPEOVERCOMESALLOBSTACLESANDTRASCENDSALLADVERSITY
DLE
HOPECONQUERSEVERYOBSTA- HOPEOVERCOMESALLOBSTACLESANDTRASCENDSALLMISFORTUNE
CLES
HOPE CONQUERS EVERY RE- HOPEPREVAILSOVEROBSTACLESANDTRASCENDSALLADVERSITY
STRAINT
HOPEDEFEATSALLOPPOSITION HOPEPREVAILSOVEROBSTACLESANDTRASCENDSALLMISFORTUNE
HOPE OVERCOMES ALL CHAL- HOPESURPASSESALLOBSTACLESANDTRASCENDSALLADVERSITY
LENGES
HOPE OVERCOMES EACH HOPESURPASSESALLOBSTACLESANDTRASCENDSALLMISFORTUNE
BOUNDARY
HOPE OVERCOMES EACH HIN- HOPETRIUMPHSOVEROBSTACLESANDTRASCENDSALLADVERSITY
DRANCES
HOPE PREVAILS AGAINST AD- HOPETRIUMPHSOVEROBSTACLESANDTRASCENDSALLMISFORTUNE
VERSITY
HOPE PREVAILS AGAINST BAR- LOVECONQUERSALLADVERSITYANDTRASCENDSALLADVERSITY
RIERS
HOPE PREVAILS AGAINST IM- LOVECONQUERSALLADVERSITYANDTRASCENDSALLMISFORTUNE
PEDIMENT
HOPE PREVAILS OVER OBSTA- LOVECONQUERSALLADVERSITYBUTTRASCENDSALLMISFORTUNE
CLES
HOPE SURMOUNTS ALL DIFFI- LOVECONQUERSALLADVERSITYBUTTRASCENDSEVERYMISFORTUNE
CULTIES
HOPE SURPASSES ALL OBSTA- LOVECONQUERSALLBOUNDARIESANDTRASCENDSALLADVERSITY
CLES
HOPE SURPASSES EVERY CON- LOVECONQUERSALLBOUNDARIESANDTRASCENDSALLMISFORTUNE
FINEMENT
HOPESURPASSESEVERYOBSTA- LOVECONQUERSALLBOUNDARIESBUTTRASCENDSALLMISFORTUNE
CLES
HOPE TRANSCENDS EACH RE- LOVECONQUERSALLBOUNDARIESBUTTRASCENDSEVERYMISFORTUNE
STRICTION
HOPE TRIUMPHS AGAINST OB- LOVECONQUERSALLDIFFERENCESANDTRASCENDSALLADVERSITY
STACLES
HOPE TRIUMPHS OVER CHAL- LOVECONQUERSALLDIFFERENCESANDTRASCENDSALLMISFORTUNE
LENGES
HOPE VANQUISHES ALL HIN- LOVECONQUERSALLDIFFERENCESBUTTRASCENDSALLMISFORTUNE
DRANCES
LOVE OVERCOMES EACH HIN- LOVECONQUERSALLDIFFERENCESBUTTRASCENDSEVERYMISFORTUNE
DRANCES
LOVE CONQUERS ALL ADVER- LOVECONQUERSALLOBSTACLESANDTRASCENDSALLADVERSITY
SITY
LOVE CONQUERS ALL BOUND- LOVECONQUERSALLOBSTACLESANDTRASCENDSALLMISFORTUNE
ARIES
LOVE CONQUERS ALL DIFFER- LOVECONQUERSALLOBSTACLESBUTTRASCENDSALLADVERSITY
ENCES
LOVE CONQUERS ALL OBSTA- LOVECONQUERSALLOBSTACLESBUTTRASCENDSEVERYADVERSITY
CLES
LOVE CONQUERS ANY OBSTA- LOVECONQUERSEVERYBARRIERSANDPREVAILSOVERADVERSITY
CLES
LOVE CONQUERS EACH OBSTA- LOVECONQUERSEVERYOBSTACLESBUTTRASCENDSALLMISFORTUNE
CLES
LOVECONQUERSEVERYBARRI- LOVEDEFEATSALLOBSTACLESANDPREVAILSOVERADVERSITY
ERS
9
LOVE CONQUERS EVERY CHAL- LOVEOVERCOMESALLOBSTACLESANDCONQUERSALLADVERSITY
LENGES
LOVE CONQUERS EVERY CON- LOVEOVERCOMESALLOBSTACLESANDCONQUERSALLMISFORTUNE
FINEMENT
LOVE CONQUERS EVERY DIFFI- LOVEOVERCOMESALLOBSTACLESANDCONQUERSEVERYOBSTACLES
CULTY
LOVE CONQUERS EVERY HIN- LOVEOVERCOMESALLOBSTACLESANDTRASCENDSALLADVERSITY
DRANCES
LOVE CONQUERS EVERY HUR- LOVEOVERCOMESALLOBSTACLESANDTRASCENDSALLMISFORTUNE
DLE
LOVECONQUERSEVERYLIMITA- LOVEOVERCOMESALLOBSTACLESBUTCONQUERSALLADVERSITY
TIONS
LOVECONQUERSEVERYOBSTA- LOVEOVERCOMESALLOBSTACLESBUTCONQUERSEVERYADVERSITY
CLES
LOVE CONQUERS EVERY SET- LOVEOVERCOMESALLOBSTACLESBUTTRASCENDSALLADVERSITY
BACK
LOVEDEFEATSALLOBSTACLES LOVEOVERCOMESALLOBSTACLESBUTTRASCENDSEACHADVERSITY
LOVEDEFEATSALLOPPOSITION LOVEOVERCOMESALLOBSTACLESBUTTRASCENDSEVERYADVERSITY
LOVE OVERCOMES ALL OBSTA- LOVEPREVAILSAGAINSTCHALLENGESANDPREVAILSOVERADVERSITY
CLES
LOVE PREVAILS AGAINST CHAL- LOVEPREVAILSAGAINSTOBSTACLESANDOVERCOMESOVERMISFORTUNE
LENGES
LOVE PREVAILS AGAINST OB- LOVEPREVAILSAGAINSTOBSTACLESANDPREVAILSOVERADVERSITY
STACLES
LOVE PREVAILS OVER OBSTA- LOVEPREVAILSOVEROBSTACLESANDTRASCENDSALLMISFORTUNE
CLES
LOVE SURMOUNTS ALL OBSTA- LOVESURMOUNTSALLOBSTACLESBUTTRASCENDSALLADVERSITY
CLES
LOVE SURPASSES ALL OBSTA- LOVESURMOUNTSALLOBSTACLESBUTTRASCENDSALLMISFORTUNE
CLES
LOVE TRIUMPHS AGAINST OB- LOVESURPASSESALLOBSTACLESANDPREVAILSOVERADVERSITY
STACLES
LOVE TRIUMPHS OVER OBSTA- LOVETRIUMPHSAGAINSTOBSTACLESANDDEFEATSALLADVERSITY
CLES
LOVE VANQUISHES ALL OBSTA- LOVETRIUMPHSAGAINSTOBSTACLESANDDEFEATSALLOBSTACLES
CLES
UNITY BREAKS THROUGH ALL LOVETRIUMPHSAGAINSTOBSTACLESANDDEFEATSEACHADVERSITY
BARRIERS
UNITY CONQUERS AGAINST LOVETRIUMPHSAGAINSTOBSTACLESANDDEFEATSOVERADVERSITY
ODDS
UNITY CONQUERS ALL OBSTA- LOVETRIUMPHSAGAINSTOBSTACLESANDDEFEATSOVEROBSTACLES
CLES
UNITY CONQUERS DESPITE LOVETRIUMPHSAGAINSTOBSTACLESBUTDEFEATSALLADVERSITY
ODDS
UNITYDEFEATSAGAINSTODDS LOVETRIUMPHSAGAINSTOBSTACLESBUTDEFEATSALLOBSTACLES
UNITY DEFEATS ALL OPPOSI- LOVETRIUMPHSAGAINSTOBSTACLESBUTDEFEATSEACHADVERSITY
TION
UNITY OVERCOMES AGAINST LOVETRIUMPHSAGAINSTOBSTACLESBUTDEFEATSOVERADVERSITY
ODDS
UNITY OVERCOMES ALL CHAL- LOVETRIUMPHSAGAINSTOBSTACLESBUTDEFEATSOVEROBSTACLES
LENGES
UNITYOVERPOWERSALLRESIS- LOVETRIUMPHSOVEROBSTACLESANDPREVAILSOVERADVERSITY
TANCE
UNITYPREVAILSAGAINSTODDS UNITYCONQUERSALLADVERSITYANDTRASCENDSALLMISFORTUNE
UNITYPREVAILSAMIDSTODDS UNITYCONQUERSALLADVERSITYANDTRASCENDSEACHMISFORTUNE
UNITYPREVAILSDESPITEODDS UNITYCONQUERSALLBOUNDARIESANDTRASCENDSALLADVERSITY
UNITYPREVAILSOVERODDS UNITYCONQUERSALLBOUNDARIESANDTRASCENDSALLMISFORTUNE
UNITYRISESABOVEALLOBSTA- UNITYCONQUERSALLBOUNDARIESANDTRASCENDSEACHADVERSITY
CLES
10
UNITY SUBDUES ALL DIFFICUL- UNITYCONQUERSALLBOUNDARIESANDTRASCENDSEACHMISFORTUNE
TIES
UNITY SUCCEEDS AGAINST UNITYCONQUERSALLDIFFERENCESANDTRASCENDSALLMISFORTUNE
ODDS
UNITY SURMOUNTS ALL OBSTA- UNITYCONQUERSALLOBSTACLESANDTRASCENDSALLMISFORTUNE
CLES
UNITY SURPASSES AGAINST UNITYOVERCOMESALLOBSTACLESANDCONQUERSALLADVERSITY
ODDS
UNITY TRIUMPHS AGAINST UNITYOVERCOMESALLOBSTACLESANDCONQUERSALLMISFORTUNE
ODDS
UNITY TRIUMPHS OVER BARRI- UNITYOVERCOMESALLOBSTACLESANDTRASCENDSALLADVERSITY
ERS
UNITY VANQUISHES ALL HIN- UNITYOVERCOMESALLOBSTACLESANDTRASCENDSALLMISFORTUNE
DRANCES
11
Table S.16: DictionaryusedinSimulation3(newunknownwordsnotincludedinpreviousdictionary)
NEWWORDS
ANOTHER COURAGE HANDLE MOMENT SILENT
BANNER CULTURE HEALTH MOTHER SILVER
BEFORE CUSTOM HEAVEN MOTION SIMPLE
BENEATH DANGER HONEST NATION SPIRIT
BETWEEN DEEPER HUNGER NATURE STRENGTH
BLESSED DESERT INSIDE NUMBER STRONG
BORDER DIFFERENT INSIGHT OFFBEAT THOUGHT
BOTTLE ECHOES ISLAND OPTION THROUGH
BREEZE EMPIRE JOURNEY PATIENT TRAVEL
BRIDGE FAMILY JUSTICE PEACE TUNNEL
BUTTER FAMOUS KINDNESS PEOPLE VALUES
BUTTON FATHER KINGDOM POSSIBLE VISION
CANDLE FELLOW LADDER REASON VOICES
CENTER FOREST LANTERN REMOTE WARMTH
CHANGE FORGIVE LEGEND RHYTHM WINDOW
CHAPTER FREEDOM LIGHTS SECRET WINTER
CHOICE FROZEN LITTLE SERVICE WITHOUT
CIRCLE FUTURE MARKET SHADOW WONDER
CLOSED GENTLE MEMORY SHELTER WONDER
CORNER GROWTH MIRACLE SHELVE WORLD
12
Table S.17: DictionaryusedinSimulation4
Sentences ERC Topic
DEDICATEDTOADVANCINGACADEMICKNOWLEDGEANDADDRESSINGTECH- LifeSciences
NOLOGICALCHALLENGES
DEDICATED TO ADVANCING APPLIED KNOWLEDGE AND ADDRESSING TECH- LifeSciences
NOLOGICALCHALLENGES
DEDICATED TO ADVANCING BASIC KNOWLEDGE AND ADDRESSING HEALTH Social Sciences and Hu-
CHALLENGES manities
DEDICATEDTOADVANCINGBIOMEDICALKNOWLEDGEANDADDRESSINGSO- Physical Sciences and En-
CIETALCHALLENGES gineering
DEDICATED TO ADVANCING COMPUTATIONAL CHANGE AND ADDRESSING LifeSciences
TECHNOLOGICALCHALLENGES
DEDICATED TO ADVANCING COMPUTATIONAL DIVERSITY AND ADDRESSING LifeSciences
TECHNOLOGICALCHALLENGES
DEDICATED TO ADVANCING COMPUTATIONAL IDENTITY AND ADDRESSING LifeSciences
TECHNOLOGICALCHALLENGES
DEDICATEDTOADVANCINGCOMPUTATIONALKNOWLEDGEANDADDRESSING LifeSciences
TECHNOLOGICALOUTCOMES
DEDICATED TO ADVANCING COMPUTATIONAL KNOWLEDGE AND MANAGING LifeSciences
TECHNOLOGICALCHALLENGES
DEDICATED TO ADVANCING COMPUTATIONAL KNOWLEDGE AND OVERCOM- LifeSciences
INGTECHNOLOGICALCHALLENGES
DEDICATED TO ADVANCING COMPUTATIONAL KNOWLEDGE AND SOLVING LifeSciences
TECHNOLOGICALCHALLENGES
DEDICATED TO ADVANCING COMPUTATIONAL KNOWLEDGE AND UNDER- LifeSciences
STANDINGTECHNOLOGICALCHALLENGES
DEDICATED TO ADVANCING COMPUTATIONAL KNOWLEDGE BY ADDRESSING LifeSciences
TECHNOLOGICALCHALLENGES
DEDICATEDTOADVANCINGCOMPUTATIONALKNOWLEDGEFORADDRESSING LifeSciences
TECHNOLOGICALCHALLENGES
DEDICATED TO ADVANCING COMPUTATIONAL KNOWLEDGE THROUGH AD- LifeSciences
DRESSINGTECHNOLOGICALCHALLENGES
DEDICATED TO ADVANCING COMPUTATIONAL KNOWLEDGE WHILST AD- LifeSciences
DRESSINGTECHNOLOGICALCHALLENGES
DEDICATED TO ADVANCING COMPUTATIONAL LITERACY AND ADDRESSING LifeSciences
TECHNOLOGICALCHALLENGES
DEDICATED TO ADVANCING COMPUTATIONAL STUDIES AND ADDRESSING LifeSciences
TECHNOLOGICALCHALLENGES
DEDICATED TO ADVANCING CULTURAL AWARENESS AND ADDRESSING SOCI- Physical Sciences and En-
ETALCHALLENGES gineering
DEDICATEDTOADVANCINGCULTURALCHANGEANDADDRESSINGSOCIETAL Physical Sciences and En-
CHALLENGES gineering
DEDICATEDTOADVANCINGCULTURALCONSCIOUSNESSANDADDRESSINGSO- Physical Sciences and En-
CIETALCHALLENGES gineering
DEDICATEDTOADVANCINGCULTURALKNOWLEDGEANDADDRESSINGSOCI- Physical Sciences and En-
ETALCHALLENGES gineering
DEDICATEDTOADVANCINGCULTURALKNOWLEDGEANDADDRESSINGSOCI- Physical Sciences and En-
ETALOUTCOMES gineering
DEDICATEDTOADVANCINGCULTURALKNOWLEDGEANDADDRESSINGSOCI- Physical Sciences and En-
ETALPROBLEMS gineering
DEDICATEDTOADVANCINGCULTURALKNOWLEDGEANDADDRESSINGSOCI- Physical Sciences and En-
ETALQUESTIONS gineering
DEDICATEDTOADVANCINGCULTURALKNOWLEDGEANDCONFRONTINGSO- Physical Sciences and En-
CIETALCHALLENGES gineering
DEDICATED TO ADVANCING CULTURAL KNOWLEDGE AND MANAGING SOCI- Physical Sciences and En-
ETALCHALLENGES gineering
DEDICATED TO ADVANCING CULTURAL KNOWLEDGE AND OVERCOMING SO- Physical Sciences and En-
CIETALCHALLENGES gineering
13
DEDICATED TO ADVANCING CULTURAL KNOWLEDGE AND RESOLVING SOCI- Physical Sciences and En-
ETALCHALLENGES gineering
DEDICATED TO ADVANCING CULTURAL KNOWLEDGE IN ADDRESSING SOCI- Physical Sciences and En-
ETALCHALLENGES gineering
DEDICATED TO ADVANCING CULTURAL KNOWLEDGE OR ADDRESSING SOCI- Physical Sciences and En-
ETALCHALLENGES gineering
DEDICATEDTOADVANCINGCULTURALKNOWLEDGETHROUGHADDRESSING Physical Sciences and En-
SOCIETALCHALLENGES gineering
DEDICATED TO ADVANCING CULTURAL KNOWLEDGE WHILST ADDRESSING Physical Sciences and En-
SOCIETALCHALLENGES gineering
DEDICATEDTOADVANCINGCULTURALSTUDIESANDADDRESSINGSOCIETAL Physical Sciences and En-
CHALLENGES gineering
DEDICATED TO ADVANCING CULTURAL VALUES AND ADDRESSING SOCIETAL Physical Sciences and En-
CHALLENGES gineering
DEDICATED TO ADVANCING CURRENT KNOWLEDGE AND ADDRESSING Social Sciences and Hu-
HEALTHCHALLENGES manities
DEDICATED TO ADVANCING CURRENT KNOWLEDGE AND ADDRESSING SOCI- Physical Sciences and En-
ETALCHALLENGES gineering
DEDICATEDTOADVANCINGMEDICALAWARENESSANDADDRESSINGHEALTH Social Sciences and Hu-
CHALLENGES manities
DEDICATED TO ADVANCING MEDICAL CHANGE AND ADDRESSING HEALTH Social Sciences and Hu-
CHALLENGES manities
DEDICATED TO ADVANCING MEDICAL KNOWLEDGE AND ADDRESSING Social Sciences and Hu-
HEALTHCHALLENGES manities
DEDICATED TO ADVANCING MEDICAL KNOWLEDGE AND ADDRESSING Social Sciences and Hu-
HEALTHOUTCOMES manities
DEDICATED TO ADVANCING MEDICAL KNOWLEDGE AND ADDRESSING Social Sciences and Hu-
HEALTHQUESTIONS manities
DEDICATED TO ADVANCING MEDICAL KNOWLEDGE AND CHALLENGING Social Sciences and Hu-
HEALTHCHALLENGES manities
DEDICATED TO ADVANCING MEDICAL KNOWLEDGE AND CONFRONTING Social Sciences and Hu-
HEALTHCHALLENGES manities
DEDICATED TO ADVANCING MEDICAL KNOWLEDGE AND MEETING HEALTH Social Sciences and Hu-
CHALLENGES manities
DEDICATEDTOADVANCINGMEDICALKNOWLEDGEANDRESOLVINGHEALTH Social Sciences and Hu-
CHALLENGES manities
DEDICATED TO ADVANCING MEDICAL KNOWLEDGE AND SOLVING HEALTH Social Sciences and Hu-
CHALLENGES manities
DEDICATED TO ADVANCING MEDICAL KNOWLEDGE AND UNDERSTANDING Social Sciences and Hu-
HEALTHCHALLENGES manities
DEDICATEDTOADVANCINGMEDICALKNOWLEDGEBYADDRESSINGHEALTH Social Sciences and Hu-
CHALLENGES manities
DEDICATED TO ADVANCING MEDICAL KNOWLEDGE IN ADDRESSING HEALTH Social Sciences and Hu-
CHALLENGES manities
DEDICATEDTOADVANCINGMEDICALKNOWLEDGEORADDRESSINGHEALTH Social Sciences and Hu-
CHALLENGES manities
DEDICATED TO ADVANCING MEDICAL KNOWLEDGE THROUGH ADDRESSING Social Sciences and Hu-
HEALTHCHALLENGES manities
DEDICATED TO ADVANCING MEDICAL KNOWLEDGE WHILST ADDRESSING Social Sciences and Hu-
HEALTHCHALLENGES manities
DEDICATED TO ADVANCING MEDICAL VALUES AND ADDRESSING HEALTH Social Sciences and Hu-
CHALLENGES manities
DEDICATED TO ADVANCING NEW KNOWLEDGE AND ADDRESSING HEALTH Social Sciences and Hu-
CHALLENGES manities
DEDICATED TO ADVANCING NEW KNOWLEDGE AND ADDRESSING SOCIETAL Physical Sciences and En-
CHALLENGES gineering
DEDICATEDTOADVANCINGSCIENTIFICADVANCESANDADDRESSINGHEALTH Social Sciences and Hu-
CHALLENGES manities
14
DEDICATED TO ADVANCING SCIENTIFIC ADVANCES AND ADDRESSING TECH- LifeSciences
NOLOGICALCHALLENGES
DEDICATED TO ADVANCING SCIENTIFIC CONSENSUS AND ADDRESSING SOCI- Physical Sciences and En-
ETALCHALLENGES gineering
DEDICATEDTOADVANCINGSCIENTIFICCONSENSUSANDADDRESSINGTECH- LifeSciences
NOLOGICALCHALLENGES
DEDICATED TO ADVANCING SCIENTIFIC DISCIPLINES AND ADDRESSING Social Sciences and Hu-
HEALTHCHALLENGES manities
DEDICATEDTOADVANCINGSCIENTIFICDISCIPLINESANDADDRESSINGTECH- LifeSciences
NOLOGICALCHALLENGES
DEDICATED TO ADVANCING SCIENTIFIC INQUIRY AND ADDRESSING HEALTH Social Sciences and Hu-
CHALLENGES manities
DEDICATEDTOADVANCINGSCIENTIFICINQUIRYANDADDRESSINGSOCIETAL Physical Sciences and En-
CHALLENGES gineering
DEDICATEDTOADVANCINGSCIENTIFICINQUIRYANDADDRESSINGTECHNO- LifeSciences
LOGICALCHALLENGES
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING Social Sciences and Hu-
HEALTHCHALLENGES manities
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING Social Sciences and Hu-
HEALTHGOALS manities
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING Social Sciences and Hu-
HEALTHISSUES manities
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING Social Sciences and Hu-
HEALTHOBJECTIVES manities
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING Social Sciences and Hu-
HEALTHPROBLEMS manities
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING SO- Physical Sciences and En-
CIETALCONCERNS gineering
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING SO- Physical Sciences and En-
CIETALOBJECTIVES gineering
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING SO- Physical Sciences and En-
CIETALPROBLEMS gineering
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING SO- Physical Sciences and En-
CIETALQUESTIONS gineering
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING LifeSciences
TECHNOLOGICALBOUNDARIES
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING LifeSciences
TECHNOLOGICALCHALLENGES
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING LifeSciences
TECHNOLOGICALCONCERNS
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING LifeSciences
TECHNOLOGICALGOALS
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING LifeSciences
TECHNOLOGICALISSUES
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING LifeSciences
TECHNOLOGICALOBJECTIVES
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING LifeSciences
TECHNOLOGICALPROBLEMS
DEDICATEDTOADVANCINGSCIENTIFICKNOWLEDGEANDCHALLENGINGSO- Physical Sciences and En-
CIETALCHALLENGES gineering
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND CHALLENGING LifeSciences
TECHNOLOGICALCHALLENGES
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND CONFRONTING Social Sciences and Hu-
HEALTHCHALLENGES manities
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND CONFRONTING LifeSciences
TECHNOLOGICALCHALLENGES
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND FACING HEALTH Social Sciences and Hu-
CHALLENGES manities
15
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND FACING TECHNO- LifeSciences
LOGICALCHALLENGES
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND MANAGING Social Sciences and Hu-
HEALTHCHALLENGES manities
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND MANAGING SOCI- Physical Sciences and En-
ETALCHALLENGES gineering
DEDICATEDTOADVANCINGSCIENTIFICKNOWLEDGEANDMANAGINGTECH- LifeSciences
NOLOGICALCHALLENGES
DEDICATEDTOADVANCINGSCIENTIFICKNOWLEDGEANDMEETINGHEALTH Social Sciences and Hu-
CHALLENGES manities
DEDICATEDTOADVANCINGSCIENTIFICKNOWLEDGEANDOVERCOMINGSO- Physical Sciences and En-
CIETALCHALLENGES gineering
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND OVERCOMING LifeSciences
TECHNOLOGICALCHALLENGES
DEDICATEDTOADVANCINGSCIENTIFICKNOWLEDGEANDREDUCINGHEALTH Social Sciences and Hu-
CHALLENGES manities
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND REDUCING TECH- LifeSciences
NOLOGICALCHALLENGES
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND SOLVING HEALTH Social Sciences and Hu-
CHALLENGES manities
DEDICATEDTOADVANCINGSCIENTIFICKNOWLEDGEANDSOLVINGSOCIETAL Physical Sciences and En-
CHALLENGES gineering
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND UNDERSTANDING LifeSciences
TECHNOLOGICALCHALLENGES
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE BY ADDRESSING SOCI- Physical Sciences and En-
ETALCHALLENGES gineering
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE FOR ADDRESSING Social Sciences and Hu-
HEALTHCHALLENGES manities
16

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Integrating large language models and active inference to understand eye movements in reading and dyslexia"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
