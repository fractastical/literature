=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Self-Evidencing Through Hierarchical Gradient Decomposition: A Dissipative System That Maintains Non-Equilibrium Steady-State by Minimizing Variational Free Energy
Citation Key: mcculloch2025selfevidencing
Authors: Michael James McCulloch

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Key Terms: decomposition, minimizing, maintains, equilibrium, state, variational, dissipative, systems, principle, steady

=== FULL PAPER TEXT ===

Self-Evidencing Through Hierarchical Gradient Decomposition:
A Dissipative System That Maintains Non-Equilibrium
Steady-State by Minimizing Variational Free Energy
Michael James McCulloch
michael.james.mcculloch@gmail.com
Code available at: https://doi.org/10.5281/zenodo.17363831
Abstract 1 Introduction
Life exists far from thermodynamic equilibrium.
From single cells to brains, biological systems main-
The Free Energy Principle (FEP) states that self-
tain their structural integrity and functional or-
organizingsystemsmustminimizevariationalfreeen-
ganization by continuously dissipating energy and
ergy to persist (Friston, 2010, 2019), but the path
entropy into their environment (Prigogine, 1977;
from principle to implementable algorithm has re-
Schr¨odinger, 1944). What separates living systems
mainedunclear. Wepresentaconstructiveproofthat
from inert matter is this capacity to resist the slide
the FEP can be realized through exact local credit
toward maximum entropy, which enables memory,
assignment. The system decomposes gradient com-
adaptation, and intelligence.
putation hierarchically: spatial credit via feedback
alignment, temporal credit via eligibility traces, and
The Physical Principle of Self-Organization.
structuralcreditviaaTrophicFieldMap(TFM)that
Prigogine’s theory of dissipative structures (Pri-
estimates expected gradient magnitude for each con-
gogine, 1977; Nicolis and Prigogine, 1977) provides
nection block. We prove these mechanisms are ex-
the thermodynamic foundation: open systems can
act at their respective levels and validate the central
spontaneously organize into ordered states when
claim empirically: the TFM achieves 0.9693 Pear-
driven by external energy flows. These structures
son correlation with oracle gradients. This exactness
are fundamentally dynamic; their order arises from
produces emergent capabilities including 98.6% re-
steady flux patterns that persist only through con-
tention after task interference, autonomous recovery
tinuous energy dissipation. The brain is an archety-
from 75% structural damage, self-organized critical-
pal example: a self-organizing dissipative structure
ity (spectral radius ρ ≈ 1.0), and sample-efficient
whose 20 watts of power consumption (Sengupta
reinforcement learning on continuous control tasks
et al., 2013) maintains both metabolic function and
without replay buffers. The architecture unifies Pri-
thepossibilityofcognitionitself(Fristonetal.,2006;
gogine’sdissipativestructures(Prigogine,1977),Fris-
Sengupta et al., 2013).
ton’s free energy minimization (Friston, 2010), and
Hopfield’s attractor dynamics (Hopfield, 1982; Amit
et al., 1985a,b), demonstrating that exact hierarchi- The Free Energy Principle as a Theory of
cal inference over network topology can be imple- Self-Organization. Friston’s Free Energy Princi-
mented with local, biologically plausible rules. ple (FEP) (Friston, 2010, 2019; Friston et al., 2023)
1
5202
tcO
02
]EN.sc[
1v61971.0152:viXra
provides a formal account of how such dissipative et al., 1986; Werbos, 1990).
systems maintain their non-equilibrium steady-state 3. Structural credit assignment: Which connec-
(NESS). Any system that can be distinguished from tions should exist at all? This is the search prob-
its environment (that possesses a Markov blanket lemofnetworkarchitectureoptimization(Mocanu
separating internal from external states) must act to etal.,2018;Elskenetal.,2019;Whiteetal.,2023).
minimize variational free energy, an upper bound on
surprise(negativelogmodelevidence). Systemsthat Classical solutions to these problems require non-
failtodothisexperienceescalatingsurprise,losetheir localinformationandviolatethephysicalconstraints
structural integrity, and dissolve back into thermal ofbiologicalsystems. Backpropagationrequiressym-
equilibrium. Free energy minimization constitutes a metric feedback connections. BPTT requires storing
physical necessity for any system that persists over complete gradient trajectories (Werbos, 1990). Ar-
time as a distinguishable entity (Friston, 2019). chitecturesearchrequiresglobalfitnesssignalsorex-
haustiveenumeration. Noneofthesemechanismsare
A system minimizing free energy implicitly per-
consistentwiththelocal,online,andcontinualnature
forms Bayesian inference on the causes of its sen-
of biological learning.
sory inputs (Dayan et al., 1995; Knill and Pouget,
2004). The internal states of such a system can be
interpretedasencodingposteriorbeliefsaboutexter- OurContribution: AConstructiveProof. We
nal states, with learning corresponding to updates of present a neural architecture that solves all three
a generative model (Rao and Ballard, 1999; Bastos credit assignment problems locally and exactly, pro-
et al., 2012). When extended to include action selec- viding a constructive proof that the FEP can be im-
tion(activeinference),theFEPpredictsthatsystems plementedasascalablealgorithm. Thesystemoper-
shouldbothinferthecausesoftheirobservationsand ates as a dissipative structure maintaining its NESS
activelysampletheworldtomakeitmorepredictable through three nested inference loops:
(Friston, 2009; Friston and Ao, 2012).
1. A feedback alignment pathway learns to
The Gap: From Principle to Implementation. project output errors into neuron-level credit sig-
While the FEP provides an elegant theoretical ac- nals, converging to exact spatial gradients in the
count of biological self-organization, the path from relevant error subspace (solving the weight trans-
universal principle to functional algorithm has re- port problem) (Moskovitz et al., 2019).
mained unclear. How does a physical system com- 2. Eligibility traces (Sutton, 1984, 1988) imple-
posed of local components implement this global im- ment optimal exponential filtering of past activ-
perative? The challenge is one of credit assign- ity, providing exact temporal credit under learn-
ment: given an outcome (e.g., a prediction error), ingtimescaleseparation(solvingthestorageprob-
which parameters are responsible and how should lem).
they change? 3. A Trophic Field Map (TFM) integrates spa-
This problem decomposes into three nested sub- tial and temporal credit signals to compute the
problems operating on different timescales: exactexpectedgradientmagnitudeforeachpoten-
tialconnectionblock,providingstructurallyexact
1. Spatial credit assignment: Given an output credit that guides network growth and pruning
error, which neurons are responsible? This is the (solving the search problem).
weighttransportproblemofbackpropagation(Lil-
licrap et al., 2016; Nøkland, 2016). The system’s hierarchical organization mirrors the
2. Temporal credit assignment: Which past ac- nested timescales of biological plasticity: fast state
tivity states, potentially seconds ago, caused the dynamics (τ = 20ms), intermediate eligibility
fast
current outcome? This is the storage problem traces (τ = 200ms), slow homeostatic adaptation
elig
of Backpropagation Through Time (Rumelhart (τ = 1000s), and glacial structural consolidation
act
2
(TFM EMA α ≈ 10−6). This temporal hierarchy Roadmap. Section2developsthetheoreticalfoun-
supports rapid within-task learning while preserving dation, connecting the FEP to dissipative structures
long-termstructuralmemory(thetopologicalscaffold andderivingthethree-levelcreditassignmenthierar-
that defines the system’s compositional capacity). chy. Section 3 presents the architecture and learning
rules. Section 4 provides empirical validation of ex-
Empirical Validation. We validate the central actness claims. Section 5 examines continual learn-
theoretical claims with quantitative evidence: ing capabilities. Section 6 analyzes the theoretical
properties that produce these behaviors. Section 7
• Structuralexactness: TheTFMachieves0.9693 discusses implementation, limitations, and future di-
Pearson correlation with oracle gradients, with rections.
residual error attributable to finite-sample noise.
• Continual learning: 98.6% task retention after
interference, showing that the system allocates or- 2 Theoretical Foundation:
thogonal topological resources to distinct tasks.
Self-Organization Through
• Compositionaltransfer: 69.8%positivetransfer
between tasks, showing structural reuse of compu- Free Energy Minimization
tational motifs.
• Self-organized criticality: The network au- 2.1 Dissipative Structures and Non-
tonomously maintains operation at the edge of Equilibrium Steady-State
chaos (spectral radius ρ ≈ 1.0), maximizing com-
putational capacity. A system exists as a distinguishable entity only if
• Antifragility: After 75% structural ablation, the it maintains a Markov blanket (a statistical bound-
system autonomously recovers to within 4.7× of ary separating internal from external states) (Pearl,
baseline error, demonstrating structural memory 1988; Friston, 2019). For open systems exchanging
in the TFM. energy with their environment, persistence requires
continuousworktopreventequilibration. Thisisthe
essence of a dissipative structure (Prigogine, 1977):
Theoretical Significance. Exact local credit as-
an organized pattern that maintains its form because
signment(andbyextension,thefullFEP)canbeim-
of continuous energy dissipation.
plementedinascalableneuralarchitecture. Thesys-
temperformsexacthierarchicalinferenceonagener-
ative model, where structural plasticity is itself part Thermodynamic Foundations. At thermody-
of the inference process. The TFM computes the ex- namic equilibrium, all macroscopic flows cease and
act expected gradient, making structural learning a entropy is maximized. Any deviation from equilib-
form of model selection under the principle of min- rium(anystructure,gradient,ororganization)repre-
imum description length (Hinton and Zemel, 1993; sentslowentropyandwilldecayunlessactivelymain-
Wallace and Dowe, 1999). tained. The second law of thermodynamics guaran-
Byframingneurallearningastheself-organization tees this: isolated systems evolve toward maximum
of a dissipative system minimizing free energy, we entropy. However, open systems can maintain low-
move beyond viewing brains as computers executing entropystatesbyexportingentropytotheirenviron-
algorithms to understanding them as physical sys- mentatarateexceedinginternalentropyproduction
tems instantiating a universal principle. The work (Schr¨odinger, 1944; Nicolis and Prigogine, 1977).
connects Prigogine’s thermodynamics, Friston’s in- Biologicalsystemsarearchetypaldissipativestruc-
formation geometry (Dayan et al., 1995), and Hop- tures (Chirumbolo and Vella, 2024). A bacterium
field’s attractor networks, showing how these for- swimming up a glucose gradient, a neuron maintain-
malisms compose into a unified account of biological ing its resting potential, and a brain processing sen-
intelligence. sory information all exist in non-equilibrium steady-
3
states (NESS) sustained by continuous energy dissi- system that minimizes F implicitly minimizes sur-
pation. The metabolic cost provides the mechanism prise while performing approximate Bayesian infer-
bywhichstructurepersists. Stoptheenergyflowand ence (Friston et al., 2006; Buckley et al., 2017).
the structure dissolves.
The Learning Problem as NESS Maintenance. Self-Evidencing: The Imperative of Existence.
For a neural system, maintaining NESS means more TheFEPstatesthatanysystemwithaMarkovblan-
than metabolic homeostasis; it requires maintaining ket will appear to minimize variational free energy
a predictive model of the world. A network with over time (Friston, 2019). This follows from tautol-
a poor generative model experiences high surprise: ogy: systemsthatfailtominimizefreeenergyexperi-
its predictions systematically fail, its internal states ence escalating surprise, lose their statistical bound-
become uncorrelated with external causes, and the ary, and cease to exist as individuated entities. The
system loses the ability to distinguish self from en- systemsweobservearepreciselythosethatsucceeded
vironment. The Markov blanket degrades. Surprise at this minimization (Hohwy, 2016).
constitutes an existential threat (Friston, 2010; Fris- For systems with dynamics x˙ = f(x,o), free en-
ton et al., 2023). ergy minimization can be shown to arise from the
Learning, from this perspective, is the process by flow’s solenoidal (conservative) and irrotational (dis-
which a dissipative system adapts its structure to sipative) components:
minimize expected surprise, thereby maintaining its
NESS. The loss function emerges from the physics
µ˙ = −Γ∇ F + Ω∇ Q (4)
of persistence. Systems that learn are systems that µ µ
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
survive. gradientflow solenoidalflow
where Γ and Ω are positive definite, and Q is a flow
2.2 The Free Energy Principle:
potential (Friston et al., 2023). The first term per-
Bayesian Mechanics of Self-
forms gradient descent on free energy (implementing
Organization
inference), while the second term encodes conserva-
tive dynamics (implementing predictions of change).
Variational Free Energy as an Upper Bound
on Surprise. Letsdenoteexternal(hidden)states
and o denote observations at the Markov blanket.
The surprisal of an observation is: From Passive to Active Inference. When ex-
ternal states depend on actions a, the system can
S(o)=−lnp(o) (1) minimize expected free energy over future trajecto-
ries (Friston, 2009; Friston and Ao, 2012):
For a system with internal states µ encoding an
approximateposteriorq(s|µ),thevariationalfreeen-
(cid:34) (cid:35)
ergy is: (cid:88)
G(π)=E lnq(s |µ)−lnp(o ,s )
q(oτ,sτ|π) τ τ τ
F =E [−lnp(o,s)]+E [lnq(s|µ)] (2) τ
q q (5)
for policies π. Minimizing G drives the system
This can be decomposed as:
to both reduce uncertainty (epistemic foraging) and
F =D [q(s|µ)∥p(s|o)]+(−lnp(o)) (3) alignobservationswithpreferences(goal-directedbe-
KL
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) havior)(Fristonetal.,2015;ParrandFriston,2020).
accuracy surprisal
Our current work focuses on the perceptual compo-
Since the KL divergence is non-negative, F ≥ nent (passive inference), though the framework ex-
−lnp(o). Free energy upper bounds surprise. A tends naturally to action selection.
4
2.3 The Hierarchical Credit Assign- This problem is addressed by three-factor learning
ment Problem rules (Fr´emaux and Gerstner, 2016; Gerstner et al.,
2018) and eligibility traces (Sutton, 1988; Gupta
Consider a recurrent network with state x(t) ∈ RN,
et al., 2023).
recurrent weights W, and observations o(t) gener-
Connection to FEP: Temporal credit assigns re-
ated from a target y(t). The network minimizes a
sponsibilityforoutcomestothehistoryofcausesthat
loss L(o,y), which we interpret as an approximation
generatedthem. Inferringthegenerativeprocess(the
tovariationalfreeenergy. Creditassignmentrequires
dynamicalmodel)fromobservations requiresexactly
computing:
this. An optimal solution should weight past states
by their causal influence, which decays exponentially
∂L
=
∂L
·
∂x
j ·
(cid:88)∂(Wx)
j
(t)
(6) in recurrent systems.
∂W ∂x ∂(Wx) ∂W
ij j j ij
t
(cid:124)(cid:123)(cid:122)(cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
spatial Jacobian temporal
2.3.3 Structural Credit: Inferring Model
This decomposes into three nested problems, each Structure
correspondingtoadifferentaspectofinferenceunder
Which connections should exist in the generative
the FEP:
model? For block-sparse networks with B blocks,
this requires deciding which of O(B2) potential con-
2.3.1 Spatial Credit: Inferring Responsibil- nection blocks W(ij) should be allocated. This is a
ity
formofNeuralArchitectureSearch(NAS)(Liuetal.,
2019; Real et al., 2019).
Given an error δ = o−y at the output, which in-
ternal states are responsible? True backpropagation Connection to FEP: Structural credit is model se-
computes: lection. UndertheFEP,theoptimalmodelstructure
∂L is the one that minimizes free energy while paying
=RTδ (7)
∂x a complexity cost for additional parameters (Hin-
ton and Zemel, 1993; Friston et al., 2016). This
where R is the readout matrix. This requires a
is Bayesian Occam’s razor: simpler models are pre-
backward pathway that mirrors the forward path-
ferredunlessadditionalcomplexityisjustifiedbyim-
way’s weights (the weight transport problem (Lilli-
proved evidence. The structural learning problem is
crap et al., 2016)).
thus inference over network topologies.
Connection to FEP: The spatial gradient is the
prediction error ϵ that drives internal state up-
dates toward configurations that minimize free en-
2.4 Hierarchical Decomposition:
ergy. Computing this error is equivalent to inferring
Three Levels of Exact Inference
which internal states failed to accurately predict ob-
servations.
We now show that the three credit assignment prob-
lems can be solved exactly using only local informa-
2.3.2 Temporal Credit: Inferring Causality tion, provided we separate their timescales:
Which past states x(t − τ) caused the current er-
ror? BPTT solves this by backpropagating gradients Level 1: Spatial Inference via Feedback Align-
throughtime,requiringstorageofthecompletestate ment. Problem: Map output error δ ∈ Rdout to
trajectory: neuron-levelcreditϵ∈RN withoutaccessingforward
weights.
T
∂L (cid:88) ∂L ∂x(t) Solution: Maintain a separate feedback projec-
= (8)
∂W
ij t=1
∂x(t)∂W
ij
tion W
fb
∈ RN×dout that adapts to minimize align-
5
ment error with the target projection RTδ: temporal gradient in expectation. The use of a diag-
onalJacobianapproximationcapturestheexactfirst-
ϵ=W fb δ (9) order temporal dynamics.
∆W ∝−(W δ−RTδ)δT (10)
fb fb FEP Interpretation: Eligibility traces perform
inference on the temporal structure of the generative
Theorem 1 (Spatial Exactness). Under con-
model. They encode a belief distribution over when
tinuous learning, the feedback projection converges
relevant causes occurred, with the exponential decay
suchthatthecomponentofϵparalleltoδ equalsthe
implementing optimal Bayesian filtering for systems
true backpropagated gradient (Lillicrap et al., 2016;
with exponentially decaying influence.
Nøkland, 2016; Moskovitz et al., 2019).
Proof. The learning rule performs gradient descent Level3: StructuralInferenceviaTrophicField
on ∥W δ − RTδ∥2. At equilibrium, E[(W δ − Map. Problem: Estimatewhichpotentialconnec-
fb fb
RTδ)δT] = 0, implying W aligns with RT in the tionsminimizefreeenergywithoutexhaustivesearch.
fb
subspace spanned by error signals. Components or- Solution: Compute a Trophic Field Map that in-
thogonaltothissubspacedonotaffectlearning,mak- tegrates spatial and temporal credit to estimate ex-
ing spatial credit exact where it matters. pected gradient magnitude:
FEPInterpretation: Thefeedbackpathwayper- T t+1 =(1−α)T t +α (cid:12) (cid:12)tr¯c t ¯ϵT gated,t (cid:12) (cid:12) (12)
formsinferenceontheinversegenerativemodel. The
where tr¯c ∈ RB and ¯ϵ ∈ RB are block-averaged
internalstatesµ≡ϵencodebeliefsaboutwhichhid- gated
eligibility and Jacobian-gated error signals.
den causes (neurons) generated the prediction error,
Theorem3(StructuralExactness). TheTFM
converging to the true posterior.
computes the exact expected block-level gradient
magnitude:
Level 2: Temporal Inference via Eligibility
(cid:12) (cid:12)
Traces. Problem: Link current postsynaptic er- (cid:12) (cid:12) (cid:18) (cid:19)
r s o to r ri ϵ n j g (t) hi t s o to p ry a . st presynaptic activity x i (t′) without T ij ∝E  (cid:12) (cid:12) (cid:12) (cid:12)k∈ (cid:88) i,l∈j ∂ ∂ W L kl (cid:12) (cid:12) (cid:12) (cid:12) +O √ 1 T (13)
Solution: Maintain slow-decaying eligibility
Proof. The synapse-level gradient is:
traces that implement optimal exponential filtering
(Sutton, 1988): ∂L
= ϵ ·(1−x2)· trc (14)
∂W kl (cid:124)(cid:123)(cid:122) l (cid:125) (cid:124) (cid:123)(cid:122) l (cid:125) (cid:124)(cid:123)(cid:122) k (cid:125)
trc i (t+1)=α elig trc i (t)+(1−α fast )x i (t) (11) spatial Jacobian temporal
From Theorem 1, ϵ provides exact spatial credit.
where α = exp(−∆t/τ ). Plasticity uses ∆W ∝ l
k k ij FromTheorem2,trc providesexacttemporalcredit
ϵ (t)trc (t), aformofthree-factorrule(Fr´emauxand k
j i in expectation. The Jacobian term (1−x2) is neces-
Gerstner, 2016). l
sary for exactness (it’s the derivative of tanh). The
Theorem 2 (Temporal Exactness). For
TFM computes the EMA of block-averaged outer
ητ ≪1, eligibility-based updates compute the ex-
elig products of these terms:
act expected temporal gradient under the stationary
distribution. (cid:12) (cid:12)
(cid:12) (cid:12)
Proof. The trace implements a kernel K(τ) ∝ T ij =E α (cid:12) (cid:12) (cid:12)ℓ 1 2 (cid:88) ϵ l (1−x2 l )trc k (cid:12) (cid:12) (cid:12)  (15)
(cid:12) k∈i,l∈j (cid:12)
exp(−τ/τ ) that optimally weights past states by
elig
their causal influence in recurrent networks (Sutton, By linearity of expectation, this equals the ex-
1988). For learning timescales slow relative to eligi- pected magnitude of the total block gradient, with
√
bility decay, the expected update matches the true finite-sample error O(1/ T).
6
FEP Interpretation: The TFM performs struc- recordofwhichconnectionshavehistoricallyreduced
turalinference,estimatingthemodelevidencefordif- surprise.
ferent connection configurations (Hinton and Zemel, When the system encounters a new task, it allo-
1993). Blocks with high T are those where connec- catestopologicalresources(connectionblocks)where
ij
tionswouldmostreducefreeenergy. Structuralplas- the TFM predicts they will minimize free energy.
ticity guided by the TFM performs Bayesian model When an old task recurs, the TFM’s memory guides
reduction (Friston et al., 2016), pruning connections rapid reconstruction of the relevant structure. This
withlowevidenceandgrowingconnectionswithhigh reconstructs the generative model itself, guided by a
evidence. persistent record of what has worked before.
The system exhibits a form of meta-learning: it
2.5 Hierarchical Integration and Self- learns how to allocate its learning resources to min-
imize long-term surprise. The FEP predicts exactly
Evidencing
this: systems should adapt their structure to reduce
The three levels compose into a unified free energy expectedfuturefreeenergy(Fristonetal.,2015;Sajid
minimization process: et al., 2021).
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12)(cid:88) 3(cid:12) Arc√hitecture: A Self-
T =E (cid:12) trc · ϵ (1−x2) (cid:12)+O(1/ T)
ij α(cid:12) k l l (cid:12)
(cid:124)(cid:123)(cid:122)(cid:125) (cid:12) (cid:124)(cid:123)(cid:122)(cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:12) Organizing Dissipative Sys-
structuralinference (cid:12)k,l temporalinference spatialinference(cid:12)
(16) tem
Each level solves a distinct inference problem:
3.1 Block-Sparse Recurrent Dynam-
• Spatial: Which hidden causes (neurons) explain
ics
the prediction error?
• Temporal: When did these causes occur?
The network consists of N neurons partitioned into
• Structural: Whichcausalpathways(connections)
B blocks of size ℓ. The state evolves according to:
should exist in the model?
dx
The nested timescales ensure separation of con- τ =−x+tanh(Wx+W u+b)+ξ(t) (17)
fastdt in
cerns. Fast spatial inference responds to immediate
errors. Intermediate temporal inference integrates where W is block-sparse with constrained connec-
over behavioral timescales. Slow structural infer- tions per row, W is the input projection, b are
in
ence consolidates long-term regularities into topol- biases, and ξ(t) is Gaussian noise.
ogy. This hierarchy mirrors the multi-timescale na-
ture of biological plasticity (Fusi et al., 2005; Benna Blocks as Local Attractor Basins. Within
andFusi,2016) andimplements theFEPat multiple each block, connections are dense (except self-
levels of organization. connections). This creates a local Hopfield-like en-
ergy function (Hopfield, 1982; Amit et al., 1985b)
Self-Evidencing Through Structural Adapta- wherepatternscanbestored. Thesparseinter-block
tion. The system maintains its NESS by continu- connections then couple these local attractors into a
ouslyadaptingitsstructuretominimizeexpectedfree compositional state space (Smolensky, 1990; Plate,
energy. Unlike static architectures that implement a 1995).
fixed generative model, this system performs infer- This architecture instantiates a ”Hopfield network
enceover generativemodels,selectingtopologiesthat of Hopfield networks” (Krotov and Hopfield, 2016,
bestexplainitsexperience. TheTFMisthememory 2020): each block maintains local attractor dynam-
of this structural inference process, a slowly evolving ics, while the TFM learns which inter-block connec-
7
tionscreateusefulcompositions. Thisprovidesexpo- 3.4 Synaptic Plasticity: Error-Gated
nentialcompositionalcapacity: patternsinvolvingK Three-Factor Learning
blocks scale as (cid:0)B(cid:1) (cℓ)K, where c is the capacity per
K
block. Recurrent weights update via:
∆W ∝tanh(ϵ )·(η trc trc +η x (x −x W ))−η W
ij j h i j o i j i ij d ij
3.2 Multi-Timescale Auxiliary Vari-
(23)
ables
This is a three-factor rule (Fr´emaux and Gerstner,
2016; Gerstner et al., 2018): presynaptic eligibility
Eligibility Traces (Temporal Credit).
trc ,postsynapticerrorϵ ,andtheircorrelation. The
i j
error signal tanh(ϵ ) acts as a gain control, gating
dtrc j
τ =−trc+(1−α )x, τ =10τ (18) plasticitywhenprecision(inverseuncertainty)ishigh
elig dt fast elig fast
(Friston et al., 2012; Bogacz, 2017).
Activity Traces (Homeostatic Regulation).
RoleofErrorModulation. Withouttheϵ term,
j
da therulereducestoHebbian-Ojalearning, whichcap-
τ =−a+|x|, τ =5000τ (19)
actdt act elig tures correlations indiscriminately. The error gate is
necessary: it provides the gradient on free energy,
The activity trace provides a slow-changing record directing plasticity toward parameter configurations
of neuron usage, supporting homeostatic plasticity that reduce surprise. Ablation studies (Section 5.4)
that prevents runaway dynamics (Turrigiano, 1999; confirmthatremovingerrormodulationcausescatas-
Zenke et al., 2017). trophicforgetting;thesystemlosestheabilitytoform
task-specific attractor landscapes and collapses to a
single, task-averaged representation.
3.3 Error Feedback and Spatial Credit
Assignment
NLMS Normalization: Adaptive Inference.
A linear readout yˆ =Rx generates predictions. The Allplasticitysignalsarenormalizedbyactivitymag-
error δ =yˆ−y is fed back via: nitude: ∝1/∥x∥2. This implements inverse-variance
weighting: when activity is low (weak signal), plas-
ϵ=W fb δ (20) ticity is amplified; when activity is high, plasticity
is suppressed to prevent runaway growth (Haykin,
The readout adapts via Normalized Least Mean 2001). This is necessary for online learning in non-
Squares (NLMS) (Haykin, 2001): stationary environments where signal power varies
over time (Section 4.4).
δxT
∆R=−η (21)
R∥x∥2+ϵ
small
3.5 Trophic Field Map: Structural
The feedback pathway adapts slowly to align with Credit and Model Selection
the true gradient’s projection:
The TFM is computed via exponential moving aver-
∆W ∝−(W δ−RTδ)δT (22) age of block-averaged gradient estimates:
fb fb
with η ≪η ≪η , ensuring timescale separation. T =(1−α)T +α|tr¯c ¯ϵT | (24)
fb R w t+1 t t gated,t
8
where: tanh nonlinearity, not merely correlation magnitude.
1 (cid:88)
tr¯c = trc (t) (25)
i ℓ k TFM as Structural Memory. With α ≈ 10−6,
k∈blocki the TFM time constant is ∼ 106 steps (effectively
1 (cid:88)
¯ϵ = ϵ (t)(1−x (t)2) (26) permanent on task timescales). This slow integra-
gated,j ℓ l l
tioncreatesapersistentmemoryofwhichconnection
l∈blockj
blocks have historically been valuable for reducing
The Jacobian term (1−x2) is required; it ensures free energy. When catastrophic damage occurs (Sec-
the TFM estimates the true gradient through the tion 5.6), this memory guides reconstruction.
3.6 Continuous Plasticity Algorithm
The system’s continuous adaptation is governed by a unified set of online update rules applied at each
internal timestep ∆t. These rules, executed in parallel, define the evolution of the recurrent weights (W),
homeostatic biases (b), readout weights (R), and a trophic support map (T) that guides structural changes.
The complete learning algorithm is specified by the following system of equations:
∆W =tanh(E )(η ·trc trc +η ·x (x −x W ))− η W
ij j h i j o i j i ij d ij
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
GatedHebbian-OjaPlasticity WeightDecay
(cid:28) (cid:29)
1
∆b =η (p∗−a )
j b j ∥x∥2+ϵ
batch
(cid:124) (cid:123)(cid:122) (cid:125)
HomeostaticRegulation
(cid:28) (cid:29) (27)
x
∆R =−η (ypred−ytarget) j
kj out k k ∥x∥2+ϵ
batch
(cid:124) (cid:123)(cid:122) (cid:125)
NLMSReadoutUpdate
T mn (t+1)=(1−α)T mn (t)+α (cid:12) (cid:12)t¯rc m ·E¯ g ⊤ ated,n (cid:12) (cid:12)
(cid:124) (cid:123)(cid:122) (cid:125)
TrophicDynamics(EMA)
where x is the neural activation vector, trc is the eligibility trace, a is the homeostatic trace, and p∗ is
the activity setpoint. The term E = error ·(1−x2) represents the post-synaptic variational signal gated
j j j
by the local Jacobian, where error is the local error for neuron j. The trophic map update operates on
j
block-averaged fields: t¯rc is the average eligibility trace in block m, and E¯ is the average gated
m gated,n
variational signal in block n. For stability, all weight updates (∆W,∆R) and the resulting weights (W′,R′)
are projected to a maximum L2 norm.
3.7 Structural Plasticity: Re-
source Competition and Self-
Organization
Connection blocks compete for limited resources
based on a viability metric:
viability =∥W(ij)∥ ×(1+T ) (28)
ij F ij
9
This combines current strength (synapse norm) Self-Organized Criticality. Figure 10 shows the
with potential utility (trophic support). A dynamic system autonomously maintains operation at the
survival threshold θ adapts to network density edgeofchaos(spectralradiusρ≈1.0). Thisemerges
survival
and error magnitude: as a property of the structural plasticity mechanism.
Systemsatcriticalityexhibitmaximalcomputational
θ =percentile ({viability }) (29) capacity, longest memory, and optimal information
survival p ij
transmission(Langton,1990;BeggsandPlenz,2003;
wherethepercentilepincreaseswithresourcescarcity Shew et al., 2009). The TFM-driven pruning and
and error. growth naturally drive the network to this critical
point, implementing a form of self-organized critical-
ity (Bak et al., 1987) through gradient-based struc-
Pruning: Existing blocks with viability < θ
survival tural learning.
are removed.
4 Empirical Validation of Ex-
Growth: Newblocksaregrowninlocationsofhigh
trophic support. The process implements a rela- actness
tive competition: potential connection locations are
weighted by their normalized trophic value, and the We now validate the three central claims: that spa-
most promising candidates are selected stochasti- tial, temporal, and structural credit assignment are
cally. A new connection’s viability is estimated as exact, not approximate.
θ × (T /max(T)), ensuring new connections
survival ij
must compete on equal footing with existing ones.
4.1 Structural Exactness: TFM Cor-
This ecological competition implements Bayesian
relation with Oracle Gradients
model reduction (Friston et al., 2016): connections
with insufficient evidence for their existence are
Protocol. We froze plasticity and analyzed inter-
pruned, while new connections are added where the
nal credit signals over 100 timesteps. At each step,
TFM predicts they will reduce free energy. The sys-
we computed:
tem self-organizes toward topologies that maximize
model evidence. 1. H [i,j]: Localheuristicfromblock-averagedel-
post
igibility and Jacobian-gated error
2. G [i,j]: Oracle gradient via exact backpropa-
MappingtoReinforcementLearning. Forcon- post
gation through recurrent weights
trol tasks such as Lunar Lander, the learning sig-
nals are adapted from the reinforcement learning
Bothwereaveragedovertimeandcorrelatedacross
framework (Sutton and Barto, 1998). The error block pairs.
signal driving the system is derived from the Re-
ward Prediction Error (RPE), or TD-error: RPE =
t
Results. Pearson correlation: 0.9693. Spearman
r +γV(x )−V(x ). Thefeedbackpathway(W )
t t+1 t fb
correlation: 0.9330 (Figure 1).
is trained to map this scalar RPE to a target costate
defined by the value function’s weights: E =
target
RPE ·R⊤. The policy readout itself is updated us- Interpretation. This near-perfect correlation val-
t V
ing a separate advantage signal, typically calculated idates Theorem 3. The TFM computes the exact ex-
via Generalized Advantage Estimation (GAE). This pectedgradientmagnitude. Thesmallresidual(0.031
demonstrates how the general-purpose credit assign- Pearson error) is consistent with finite-sample noise:
√
ment machinery is specialized for the sparse and de- O(1/ T) ≈ O(1/10) = 0.1 is the expected noise
layed reward signals characteristic of RL. level. No systematic bias is observed.
10
Figure 2: Spatial credit alignment during long-term
learning. Thecosinesimilaritybetweenlearnedfeed-
back ϵ and true gradient projection ϵ∗ (blue, left)
graduallyconvergesto1.0over50,000steps,showing
eventual exact spatial credit assignment. Prediction
MSE (red, right) drops to baseline early in training
while alignment is still poor (< 0.4), showing that
learning proceeds with approximate gradients before
the system self-corrects toward exactness. This vali-
Figure 1: Structural credit exactness: TFM vs. ora-
dates that the feedback pathway performs inference
clegradient. Scatterplotcomparingthelocaltrophic
on the inverse generative model.
heuristicH againsttrueblock-levelgradientmag-
post
nitude G computed via backpropagation. Pear-
post
son: 0.969,Spearman: 0.933. Thenear-perfectcorre-
lation empirically validates Theorem 3, showing that Results. Cosine similarity gradually converges to-
hierarchical gradient decomposition provides struc- ward 1.0 over long-term training (Figure 2). In con-
turally exact credit assignment. The small residual trast, prediction MSE drops to baseline within the
is attributable to finite-sample noise inherent in on- first few thousand steps, long before alignment is
line, stochastic learning. complete.
This is the paper’s central empirical claim: local
credit assignment for structural learning can be ex-
act. Network topology is directly inferred from local
Interpretation. This validates Theorem 1 and re-
gradient signals.
veals a property of note: effective learning precedes
exact credit assignment. Early in training, the feed-
4.2 Spatial Exactness: Feedback back signal is misaligned (cosine <0.4), yet the net-
Alignment Quality work rapidly reduces error. Approximate gradients
suffice to guide the system into the correct attractor
Protocol. We trained a 256-neuron network (8 basin, after which the feedback pathway self-corrects
blocks×32neurons,batch32)onMackey-Glasspre- toward exactness.
diction for 50,000 steps. At each post-washout step,
we computed: For biological learning: brains may not require ex-
act backpropagation from the outset. Approximate
1. Learned feedback signal: ϵ=W δ credit signals can bootstrap learning, and the credit
fb
2. Analytic target: ϵ∗ =RTδ assignment mechanism itself improves through expe-
3. Cosine similarity: cos(ϵ,ϵ∗) rience.
11
4.3 Temporal Exactness: Eligibility
Trace Predictiveness
Protocol. We ran exact forward-mode e-prop gra-
dientcomputationona1024-neuronnetworkover24
timesteps. We compared three gradient estimates:
1. Exact: Forward-modeeligibilitywithfullJacobian
propagation
2. Diagonal: (dL/dx)·(1−x2)⊗EMA(x) (our im-
plementation)
3. EMA-only: (dL/dx)⊗EMA(x) (no Jacobian)
Wemeasuredcorrelationandrankingmetrics(AU-
ROC, Precision@10%) for identifying top-gradient
connections.
Results. Diagonal approximation vs. exact: Pear-
Figure3: Temporalcreditexactness: eligibilitytraces
son 0.840, Spearman 0.828, AUROC 0.911, Preci-
vs. forward-mode e-prop. Scatter plot compar-
sion@10% 0.569 (Figure 3).
ing the diagonal factorized approximation (eligibil-
ity traces with Jacobian correction) against exact
Interpretation. This validates Theorem 2. The forward-mode gradients with full Jacobian propaga-
eligibility traces with Jacobian correction are highly tion. Pearson: 0.840, Spearman: 0.828, AUROC:
predictive of true temporal credit. The strong AU- 0.911. The strong correlation validates Theorem 2,
ROC (0.911) shows good ranking of connections by showingthateligibilitytracesimplementoptimalex-
importance. Theimperfectcorrelation(0.84)reflects ponential filtering for temporal credit assignment in
that our implementation uses a diagonal Jacobian recurrent networks.
approximation, which discards off-diagonal coupling
terms. This approximation captures the dominant
Interpretation. Thehighcosinesimilarity(0.968)
temporal credit structure and is fully local.
indicates approximate directional alignment with e-
prop, but the weak Pearson correlation (0.195) and
modest ranking metrics reveal a fundamental differ-
4.4 Weight Update Alignment: The
ence in connection prioritization.
Role of NLMS Adaptation
This divergence arises from NLMS normalization
(Haykin, 2001): all plasticity signals are scaled by
Protocol. Wecomparedtheactualweightchanges
1/∥x∥2. This implements inverse-variance weighting;
∆W produced by our plasticity rules against exact
timestepswithlowactivityreceiveamplifiedupdates,
forward-mode e-prop gradients for a 512-neuron net-
while high-activity timesteps are suppressed. This
work over 20 timesteps. We measured block-wise
is fundamentally different from e-prop’s magnitude-
Frobenius norm correlations and ranking metrics.
preserving gradient accumulation.
Ablation studies (Section 4.5) show this normal-
Results. Cosine similarity 0.968, Pearson correla- ization is functionally necessary. Removing it causes
tion 0.195, AUROC 0.636, Precision@10% 0.125 complete learning failure (MSE remains at initializa-
(Figures 4, 5). tion baseline). NLMS is a classical adaptive filtering
12
algorithmprovenoptimalforonlinelearningwithun-
known or time-varying signal power (Haykin, 2001).
The weak e-prop correlation is the signature of this
adaptive mechanism, not an approximation error.
The system thus trades gradient fidelity for three
properties:
1. Stability: Adaptive learning rates prevent diver-
gence in online settings where static rates fail
2. Biological plausibility: Local magnitude-free
rules avoid global gradient computations
3. Online robustness: Learning proceeds with
highly variable activity distributions
Combinedwithexactspatial(Section4.2), tempo-
ral (Section 4.3), and structural (Section 4.1) credit
assignment, hierarchical gradient decomposition can
use adaptive filtering for stable continual learning
without sacrificing biological plausibility.
Figure 4: Weight update alignment with e-prop.
Scatter plot comparing block-wise weight changes
4.5 Ablation Study: Necessity of
∥∆W(ij)∥ from local plasticity rules against exact
NLMS Normalization F
e-propgradients. Cosine: 0.968,Pearson: 0.195. The
Tovalidatethatactivitynormalizationisfunctionally moderate cosine indicates approximate directional
necessary, we performed systematic ablations on a alignment, while weak Pearson reveals fundamental
256-neuronnetworktrainedonMackey-Glassfor100 differences from NLMS inverse-variance weighting.
steps. This normalization (∝1/∥x∥2) is functionally neces-
sary; ablation shows removing it eliminates learning.
Theweake-propcorrelationreflectsadaptivefiltering
Conditions.
principles required for stable online learning, trading
1. Original: Both architectural scaling and NLMS gradientfidelityforbiologicalplausibilityandrobust-
normalization ness.
2. No NLMS: Architectural scaling only, removed
inverse state norms
• Neither: MSE remained at 1.0 (0% improve-
3. No Architecture Scaling: NLMS only, set di-
ment). Complete failure.
visors to 1.0
4. Neither: Pure e-prop-style gradient accumula-
tion Interpretation. NLMS normalization is required
for learning to occur at all. The two normalization
schemes work synergistically: architectural scaling
Results.
preventsper-blocknormexplosion,whileNLMSpro-
• Original: MSE1.0→0.12(88%errorreduction). vides adaptive rate scaling. Removing either causes
Learning succeeded. collapse.
• No NLMS: MSE remained at 1.0 (0% improve- This validates that the weak e-prop correlation re-
ment). Complete failure. flectsnecessaryadaptivefilteringratherthanapprox-
• No Architecture Scaling: MSE1.0→0.95(5% imation error. The system implements a principled
reduction). Severe impairment. algorithm for online learning in non-stationary en-
13
5.1 Experimental Setup
Wesubjectedthesystemtomultiplecontinuallearn-
ing challenges involving time-series prediction with
shifting dynamics (changing sine frequencies, switch-
ing to square waves, random walks). Networks
ranged from 128 to 327,680 neurons.
5.2 Task Retention After Distribution
Shift
Protocol. Train on Task A until convergence,
switch to unrelated Task B for extended training,
then test zero-shot recall and one-step relearning on
Task A.
Figure 5: Precision-recall curve for connection pri-
oritization. Average Precision: 0.151. The modest
ranking performance reflects NLMS inverse-variance Results.
weighting rather than direct gradient accumulation.
Byadaptivelyscalinglearningratesbasedoninstan- • Zero-shot recall: Performance degraded by 8745%
taneous activity (∝ 1/∥x∥2), the system prioritizes (catastrophic forgetting)
connections differently than standard gradient de- • After one learning step on Task A: Performance
scent. Ablation confirms this normalization is func- restored to within 1.4% of original baseline
tionally necessary for learning, showing that the sys- • Retention score: 98.6%
tem implements adaptive filtering principles proven
optimalforonlinelearningwithvariablesignalpower.
Interpretation. This supports an attractor basin
model of memory implemented through structural
preservation. Learning Task B shifts the network’s
vironments, where activity distributions vary unpre- state dynamics into a new attractor basin (causing
dictably over time. zero-shot failure), but the topological scaffold defin-
ing Task A’s attractor is preserved in the connection
structure.
AsingleerrorsignalfromTaskAprovidessufficient
gradienttorapidlyguidethesystem’sstatebackinto
5 Continual Learning: Emer-
the correct basin. The topology encodes the attrac-
gence of Compositional tor structure, while fast synaptic dynamics handle
basin selection. The TFM’s slow timescale preserves
Memory
this topological memory even during extended Task
B training.
Wenowshowthatexactstructuralcreditassignment Thesystemallocatesdistincttopologicalresources
producespowerfulcontinuallearningcapabilitiesthat (connection blocks) to different tasks, preventing in-
emerge from the system’s self-organizing dynamics. terference at the structural level while allowing flex-
Thisaddressestheproblemofcatastrophicforgetting ible reuse of neurons across tasks. This is consistent
(McCloskey and Cohen, 1989; French, 1999; Kirk- with complementary learning systems theory (Mc-
patrick et al., 2017). Clelland et al., 1995).
14
5.3 Positive Transfer Between Tasks the TFM automatically segregates structure when
tasks drive conflicting credit signals.
Protocol. CompareinitialTaskBperformancefor:
(1) naive network, (2) network pre-trained on Task
5.5 Relearning Acceleration
A.
Protocol. After forgetting Task A (via Task B
Results. Pre-trained network showed 69.8% im-
training), measure time to re-converge for: (1) ex-
provement in initial Task B performance.
perienced network, (2) naive network.
Interpretation. The network reuses computa-
Results. Experienced network relearned 1.04×
tionalmotifs(topologicalsubstructures)learneddur-
faster.
ing Task A that are also relevant for Task B. The
TFMidentifiesandreinforcesthesesharedstructures,
Interpretation. The preserved topology provides
supporting compositional transfer. The structural
astructuralpriorthatscaffoldsrapidre-optimization
memory forms a library of reusable computational
of synaptic weights. The modest speedup (4%) sug-
primitives.
gests that for these tasks, weight convergence is the
This is analogous to hierarchical Bayesian infer-
primary bottleneck once good structure is found.
ence, where lower-level structure (e.g., edge detec-
Thisconfirmsthatstructuralmemorysupportsmore
tors) is shared across tasks while higher-level struc-
efficientrelearningthanstartingfromscratch,consis-
turespecializes. Theblock-sparsetopologynaturally
tent with theories of memory consolidation (McClel-
implements this hierarchy: shared blocks form the
land et al., 1995; Benna and Fusi, 2016).
backbone while task-specific blocks provide special-
ization, a form of hierarchical knowledge reuse (Mc-
Clelland et al., 1995). 5.6 Antifragility: Recovery from
Catastrophic Damage
5.4 Rapid Task Switching Without
Protocol. After convergence, ablate 75% of con-
Interference
nection blocks randomly. Allow system to au-
Protocol. Alternatebetweentwodistincttasksev- tonomously recover without retraining signal.
ery 200 steps for 10 switches.
Results. Networkautonomouslyrecoverederrorto
Results. Performance on both tasks remained sta- within 4.7× of pre-damage baseline.
ble with 0.0% degradation across switches.
Interpretation. The TFM, operating on a very
Interpretation. The TFM maintains separate slow timescale, retains a memory of which connec-
credit landscapes for each task. In addition, credit tions were significant even after their physical re-
assignment is surgical, it does not repurpose weights moval. Thishistoricalcreditmapguidestheregrowth
that have naught to do witht the task. When tasks of connections that matter, supporting self-repair.
have conflicting requirements, structural plasticity The system recovers from damage and uses the
canallocatedistinctconnectionblocks,preventingin- perturbation to test and refine its structural mem-
terference at the structural level while fast dynamics ory (Taleb, 2012). Connections that were marginally
rapidly switch between attractor basins. usefulmaynotberebuilt,resultinginasparser,more
The network can maintain multiple task represen- efficient topology post-recovery.
tationssimultaneouslybyallocatingorthogonaltopo- This is reminiscent of biological recovery from le-
logicalresources. Thesystemdoesnotneedtoexplic- sions,whereneuralcircuitsreorganizetorestorefunc-
itly detect task boundaries or maintain task labels; tion (Nudo, 2006; Xerri, 2012). The TFM provides a
15
plausible mechanism: a persistent memory of func-
tional connectivity that guides autonomous recon-
struction.
5.7 Sample-Efficient Reinforcement
Learning Without Replay
To validate that the architecture extends beyond
supervised prediction to control tasks with delayed
credit assignment, we tested the system on the Lu-
narLandercontinuouscontrolbenchmark. Theagent
must learn a policy mapping 8-dimensional state
observations to 4 discrete thrust actions, receiving
sparse reward only upon successful landing. The
agentwasconfiguredtousesingle-stepTD(0)returns
(Sutton and Barto, 1998) and learned directly from
its online experience trajectory without using expe-
rience replay or hypothetical planning rollouts. The Figure 6: Lunar Lander learning curve with on-
network consisted of 1024 neurons organized into 32 line architecture search. Top: Episode rewards
blocks of 32 neurons each, with a potential connec- (blue, translucent) show high variance characteristic
tion space of 322 =1024 inter-block connections. of stochastic control, with moving average (orange)
demonstrating rapid learning from −318 to positive
reward by episode 119. First successful landing (re-
Results. The system achieved successful landings
ward >200 threshold) at episode 35 (+238 reward).
(reward > 200) within 35 episodes, achieving +238
Bottom: 100-episode moving average clearly shows
reward. Over 427 total episodes, the system com-
progression to sustained positive reward (+46.4 av-
pleted 92 successful landings (21.5% success rate).
erage for episodes 300+, range +32 to +60). Green
The 100-episode moving average improved from ini-
shading indicates positive reward region. The sys-
tial −318 to sustained positive reward (+32 to +60)
tem achieved 92 successful landings over 427 total
by episode 298, demonstrating robust policy conver-
episodes (21.5% success rate), demonstrating robust
gence (Figure 6). This demonstrates real-time struc-
policyconvergencewithTD(0)learningandnoexpe-
tural credit assignment under the challenging condi-
rience replay.
tions of delayed rewards, stochastic dynamics, and
non-stationary value landscapes characteristic of on-
linepolicylearning. TheTFMsuccessfullynavigated
a structural search space of O(103) potential connec-
tions,convergingtoasparsesolutionandmaintaining
efficiency matches modern deep RL methods while
stable topology throughout training.
using only local plasticity rules and no replay sug-
geststheTFMcapturesfundamentalstructureinthe
Interpretation. The TFM provides exact struc- credit assignment problem that replay-based meth-
tural credit even when rewards are separated from ods approximate through brute-force memorization.
actions by dozens of timesteps. The eligibility traces WhilethisexperimentusesstandardRL(Suttonand
bridge the temporal gap (linking past actions to cur- Barto, 1998) rather than active inference proper,
rentrewards),whiletheTFMintegratesthesesignals it validates that the hierarchical credit assignment
to identify which connection blocks support value mechanism scales to control problems with delayed,
predictionandpolicyselection. Thefactthatsample sparse rewards.
16
Figure 7: Distributional shift tolerance. The net-
workencountersfivedistinctdistributionshifts: slow
sine (period 50), fast sine (period 10), square wave,
slowsineagain,andrandomwalk. TheEWMAerror
adaptswithineachphasewhilemaintaininglowerror
throughout. Thesystemexhibitsnocatastrophicfor- Figure 8: Concept drift adaptation. At timestep
getting, demonstrating continual learning supported 1500(redline),thetargetdynamicalregimeabruptly
by structural segregation of task-specific topological shifts (tau parameter 17 → 30). The EWMA MSE
resources guided by exact credit assignment via the showsrapidadaptationtothenewregimewithoutre-
TFM. training. Predictionscloselytracktargetsaroundthe
drift point, showing antifragile response to sudden
distributionalchanges. Thesystemtreatssurpriseas
6 Theoretical Analysis: Why
evidence for model revision rather than catastrophic
Exact Credit Prevents For- failure.
getting
Intermediate Timescale: Eligibility Trace Fil-
tering. The eligibility traces implement temporal
6.1 Multi-Timescale Defense Against
credit assignment with an exponential kernel. This
Interference
meansonlyrecentactivitypatternsinfluenceplastic-
ity. When switching from Task A to Task B, Task
CatastrophicforgettingoccurswhenupdatesforTask
A activity patterns decay from the eligibility traces
B destructively interfere with parameters necessary
within a few time constants (∼ 200ms), preventing
for Task A (McCloskey and Cohen, 1989; French,
them from being incorrectly credited for Task B er-
1999). Our system mitigates this through a multi-
rors.
timescale defense:
Slow Timescale: Structural Preservation.
Fast Timescale: Error-Gated Plasticity. The TFM integrates gradient signals over hundreds
Synaptic updates are modulated by task-specific er- of thousands of timesteps (α ≈ 10−6). This creates
ror signals via the tanh(ϵ ) term. When performing a persistent structural memory that is quasi-static
j
well on Task A, error is low, and plasticity is sup- relative to task timescales. The topological scaffold
pressed,protectingTaskAparametersduringTaskB defining Task A’s attractor is preserved even during
learning. This implements precision-weighted learn- extended Task B training.
ing (Friston et al., 2012): updates are scaled by con- Thistemporalhierarchyimplementsanaturalform
fidence, preventing low-confidence signals from cor- of memory consolidation: rapid learning occurs in
rupting high-confidence knowledge. synapticweights,slowconsolidationmovestohomeo-
17
where c≈0.15 is the capacity per block. For B =64
blocks of size ℓ = 32, patterns with K = 4 active
blocks give:
(cid:18) (cid:19)
64
capacity∼ (0.15×32)4 ≈7.6×108 patterns
4
(31)
This exponential scaling in the number of active
blocks provides vastly greater capacity than mono-
lithic networks of the same size (N = 2048 → 307
patterns), consistent with modern analyses of as-
sociative memory capacity (Krotov and Hopfield,
2020).
TFM Makes Compositional Search Tractable.
Figure 9: Recovery from structural perturbation. At Without the TFM, finding useful compositions re-
timestep 2000 (red line), a large perturbation is ap- quires searching O(B2) potential connections. The
plied. The EWMA MSE shows rapid recovery, with TFM provides a local gradient on this search space,
error returning to baseline within hundreds of steps. making it tractable. The system performs gradient-
Predictions diverge briefly but quickly re-align with based structure search, improving substantially over
targets. This antifragile behavior shows how exact evolutionary or random methods.
structural credit assignment (TFM) supports self-
repair by maintaining a persistent memory of func- 6.3 Attractor Networks of Attractor
tional connectivity that guides autonomous recon- Networks
struction after damage.
Each block, with dense internal connectivity, forms
a local Hopfield network capable of storing patterns
staticbiases, andstructuraltopologyremainsstable, (Hopfield, 1982). The sparse inter-block connections
actingaslong-termmemory(Fusietal.,2005;Benna thencoupletheselocalenergylandscapesintoacom-
and Fusi, 2016). positional state space.
Hierarchical Energy Function. The total en-
6.2 Block Structure and Composi-
ergy can be decomposed:
tional Capacity
B
(cid:88) (cid:88)
E(x)= E (x )+ E (x ,x ,W(ij))
A monolithic network of N neurons has memory ca- local i coupling i j
pacity proportional to N (Hopfield: ∼ 0.15N (Hop- i=1 i̸=j
(32)
field, 1982; Amit et al., 1985b)). A block-structured
LocalenergyE correspondstointra-blockpat-
network can represent patterns both within blocks local
tern completion. Coupling energy E corre-
and through combinations of active blocks. coupling
sponds to inter-block consistency constraints. The
TFM learns which coupling terms minimize total en-
Compositional Capacity Bound. For patterns ergy (equivalently, free energy).
involving K blocks:
Task-Specific Attractors via Orthogonal
(cid:18) B (cid:19) Structure. Different tasks require different inter-
capacity∼ (cℓ)K (30)
K block coupling patterns. By allocating distinct
18
connection blocks to different tasks, the system
creates orthogonal attractor landscapes in the com-
positional space. Task A activates blocks {1,3,5,7}
withspecificcouplings,whileTaskBactivatesblocks
{2,4,6,8} with different couplings. The attractors
do not interfere because they occupy orthogonal
subspaces of the full state space.
This explains the 98.6% retention result: Task B
learning does not destroy Task A attractors because
they are structurally segregated. A single Task A er- Figure 10: Self-organized criticality. The network’s
rorsignalprovidesastrongenoughgradienttoguide spectral radius ρ(J ) hovers near 1.0 throughout
t
the network’s state back into the Task A attractor training, indicating autonomous maintenance at the
basin. edge of chaos. This critical regime balances stabil-
ity (sub-critical, ρ < 1) with rich dynamics (super-
critical, ρ > 1), maximizing computational capac-
6.4 Self-Organized Criticality
ity. The structural plasticity mechanism naturally
Figure 10 shows the system maintains operation at self-organizes to this regime without explicit tuning,
the edge of chaos (spectral radius ρ ≈ 1.0). This is anemergentpropertyofTFM-guidedgradient-based
an emergent property. structure search, implementing self-organized criti-
cality through local interactions.
Why Criticality Emerges. The TFM-driven
structural plasticity balances two opposing forces:
• Power-lawavalanches: Observedincorticalnet-
works(BeggsandPlenz,2003;PlenzandThiagara-
1. Growth pressure: High-gradient connections
jan, 2007)
are added, increasing connectivity and pushing ρ
higher
The system’s autonomous convergence to this
2. Pruning pressure: Low-viability connections
regime shows that gradient-based structural learning
are removed, decreasing connectivity and pushing
implements a universal computational principle.
ρ lower
Thesystemsettleswheretheseforcesbalance,pre-
6.5 Topological Persistence: Memory
ciselyatthecriticalpointwhereρ≈1. Thisisaform
in Structure
of self-organized criticality (Bak et al., 1987): local
interactions (TFM-guided pruning/growth) produce Thesystemexhibitsmemoryacrossthreenestedlev-
aglobalproperty(criticality)withoutexplicittuning. els:
Why Criticality Matters. Systems at criticality Level 1: Synaptic Weights (Fast, τ ∼ 103
exhibit:
steps). Rapid learning of task-specific patterns
within the current structural scaffold. Vulnerable to
• Maximal computational capacity: Ability to
interference but quickly adaptable.
perform complex transformations (Langton, 1990;
Bertschinger and Natschl¨ager, 2004)
• Longest memory: Information persists for max- Level 2: Homeostatic Biases (Intermediate,
imal duration (Beggs and Plenz, 2003) τ ∼106 steps). Slow consolidation of activity pat-
• Optimalinformationtransmission: Balanceof terns into biases. Provides stability against rapid
integration and differentiation (Shew et al., 2009) fluctuations while allowing long-term adaptation.
19
Level 3: Network Topology (Glacial, τ ∼ 109
steps). The TFM integrates over hundreds of mil-
lionsoftimesteps,creatinganearlypermanentmem-
ory of which structures were historically valuable.
This topological memory is what supports 98.6% re-
tentionafterinterferenceand4.7×recoveryafter75%
damage.
Thishierarchymirrorsbiologicalmemorysystems,
where:
• Short-term memory: fast synaptic dynamics Figure 11: Short-horizon memory capacity. The R2
• Long-term memory: slow synaptic consolidation scoreforpredictingdelayedinputsignalsdecayswith
• System-level memory: structural connectivity pat- delay. At delay 1, the network achieves near-perfect
terns (McClelland et al., 1995; Squire, 2004) recall(R2 ≈1.0). Capacityextendsto6-8stepswith
R2 > 0.4, showing substantial short-term memory
from recurrent dynamics and eligibility traces. This
Topological Memory as Model Evidence. validates the temporal credit mechanism also serves
From the FEP perspective, the TFM is a record of as a working memory store, integrating information
model evidence: which connection blocks have his- across behaviorally relevant timescales.
torically reduced free energy. Structural plasticity
guided by the TFM performs Bayesian model selec- 7 Implementation and Perfor-
tion over network topologies, with the TFM acting
mance
asaslow-movingpriorthatbiasessearchtowardpre-
viously successful structures.
7.1 Triton Kernels for Block-Sparse
Operations
6.6 Short-Term Memory Capacity
We implemented custom Triton kernels for GPU-
native block-sparse operations. A fused kernel per-
Figure 11 shows the network maintains substantial
forms:
short-term memory, with R2 >0.4 for predicting de-
layed signals up to 6-8 timesteps in the past. 1. Block-sparse matrix multiplication (using CSR
format)
2. ExponentialEulerintegrationofallstatevariables
Mechanism. Memory capacity arises from two 3. Deterministic, chunking-invariant noise injection
sources:
Thisminimizesmemorybandwidthbykeepingthe
entireupdateloopon-chip,achievinghigharithmetic
1. Recurrent dynamics: Echo state property of
intensity. Separate kernels handle weight updates
the reservoir (Jaeger, 2001; Maass et al., 2002)
and TFM calculations.
2. Eligibility traces: Explicit memory of past ac-
tivity with τ =200ms
elig 7.2 Computational Complexity
The eligibility traces extend memory beyond what Forward pass and plasticity: O(T ·B·C·ℓ2), where
recurrent dynamics alone provide. This validates C is average connections per block row.
that the temporal credit mechanism also serves as TFM update: O(B2) (cheap, computed once per
a working memory store. batch, not per timestep).
20
Thisismoreefficientthandenseoperations,which computed exactly from local gradient signals, reduc-
would be O(T ·(Bℓ)2). For typical parameters (C ≪ ing architecture search to gradient descent.
B),thisprovides∼B/C speedup,supportingscaling
to 327,680 neurons on a single GPU. Connecting Prigogine, Friston, and Hopfield.
Theworkunifiesthreemajortheoreticalframeworks:
7.3 Performance Benchmarks
• Prigogine: Dissipative structures maintaining
Figures12and13showthroughputandlatencyscal- NESS through energy dissipation
ing. • Friston: Freeenergyminimizationastheprinciple
governing self-organization
• Hopfield: Attractor networks as memory mecha-
Key Results:
nisms
• 16K neurons: 24,000+ items/s throughput (batch
These represent different perspectives on the same
64)
phenomenon. The network is simultaneously a dis-
• 327K neurons: 4,700+ items/s throughput (batch
sipative structure (thermodynamics), a free energy
64)
minimizer (information theory), and a compositional
• Minimum latency: ∼11ms (batch 8, 4 substeps)
attractor network (dynamical systems).
• Maximum throughput: batch 64, 1 substep
Theseresultsshowthatthearchitecturescaleseffi- 8.2 Empirical Contributions
ciently,withthroughputremaininghighevenfornet-
worksapproachinghalfamillionneurons. Theblock- Quantitative Validation of Exactness. The
sparse design supports practical training of much 0.9693 TFM-oracle correlation provides strong em-
larger networks than would be feasible with dense pirical support for Theorem 3. This confirms that
implementations. the exactness holds under realistic conditions with
finite sampling, noise, and limited precision.
8 Discussion: From Principle
Continual Learning. The98.6%retention,69.8%
to Practice transfer, and autonomous recovery results show that
exact credit assignment produces continual learning
8.1 Theoretical Contributions capabilitiesqualitativelydifferentfromstandardneu-
ral networks. The system does not need explicit task
A Constructive Proof of the FEP. The Free boundaries, replay buffers, or parameter protection;
EnergyPrinciplecanbeimplementedasascalableal- continual learning emerges from the physics of self-
gorithm,notjustatheoreticalframework. Thethree- organization.
level hierarchical decomposition provides a construc-
tive path from physical principle (maintaining NESS
Self-Organized Criticality. The autonomous
through free energy minimization) to computational
convergence to the edge of chaos validates that
mechanism (exact local credit assignment).
gradient-based structural learning implements a uni-
versal computational principle, suggesting a connec-
Exact Structural Credit Assignment. The tion between the FEP and theories of computation
TFM is the paper’s primary theoretical contribu- at criticality.
tion. Previous work on structural plasticity has re-
liedonheuristics(activity-basedpruning(Hanetal.,
8.3 Limitations
2015)), global fitness signals (evolutionary methods
(Mocanuetal.,2018)), ormeta-learningovertopolo- Static Benchmark Performance. On standard
gies (Elsken et al., 2019). Structural credit can be benchmarks like Mackey-Glass, the model achieves
21
NRMSE 0.1215, respectable for an online, adaptive 8.4 Broader Implications
system but not state-of-the-art compared to static
Neuroscience. The work suggests that biological
models optimized for single-task performance. This
learningmaybemoreexactthanpreviouslythought.
is expected: the system trades peak performance
If feedback alignment converges (as we show), the
for continual learning capability. Exploring whether
braindoesnotneedsymmetricfeedback; itcanlearn
TFM-guided architecture search can improve static
to provide exact gradients asymptotically. This re-
benchmarks remains for future work.
solves the weight transport problem without requir-
ing implausible biological mechanisms.
Block-Level Granularity. Structuralcreditisas-
signed at the block level (ℓ = 32 neurons), not Machine Learning. The TFM provides a practi-
the synapse level. While this appears sufficient for cal method for differentiable architecture search that
the tasks tested, it precludes finer-grained topologi- scales to large networks. Unlike NAS methods that
cal adaptations. Investigating whether synapse-level trainthousandsofcandidatearchitectures(Zophand
TFM signals can be computed efficiently is an area Le,2017;Realetal.,2019), TFM-guidedgrowthand
for future research. pruning perform gradient-based search online during
training.
Convergence Theory. While we establish exact- Artificial Life. The system’s ability to maintain
ness at equilibrium, formal analysis of convergence itselfatcriticality,recoverfromcatastrophicdamage,
ratesforthecoupledsynapticandstructuraldynam- and allocate resources to minimize surprise suggests
ics remains open. This is challenging because the ithascrossedathresholdfromsimulatingintelligence
dynamics operate on vastly different timescales, cre- to instantiating the physical principles that underlie
ating a singular perturbation problem (Bertschinger it,withimplicationsforunderstandingthetransition
and Natschl¨ager, 2004). from non-living to living systems (Kauffman, 1993).
Extension to Active Inference. The current 9 Conclusion
work focuses on passive inference (prediction). The
naturalextensionistoactiveinference,whereactions We have presented a neural architecture that instan-
are selected to minimize expected future free energy tiates the Free Energy Principle through hierarchi-
(Friston and Ao, 2012; Parr and Friston, 2020). The cal gradient decomposition. The system maintains
TFM framework extends naturally: expected gradi- its non-equilibrium steady-state by minimizing vari-
entsoveractionsequencesguidestructuralallocation ational free energy across three nested levels: spatial
for policy learning. Our codebase contains a fully creditviafeedbackalignment,temporalcreditviael-
implemented ‘ActiveInferenceAgent‘ that shows this igibility traces, and structural credit via the Trophic
extension. Field Map.
Ourcentralempiricalclaimisthatstructuralcredit
assignment can be exact, not approximate. The
Biological Plausibility. While the three-factor 0.9693 TFM-oracle correlation validates this, show-
learning rule and eligibility traces are biologically ing that local signals can precisely estimate which
plausible (Fr´emaux and Gerstner, 2016; Gerstner connections minimize surprise. This exact struc-
et al., 2018), some aspects remain abstract (e.g., tural inference produces stable, compositional at-
block-level averaging for the TFM). Future work tractor landscapes that support continual learning:
should investigate whether finer-grained local mech- 98.6% task retention, 69.8% positive transfer, and
anisms can approximate the TFM computation. autonomous recovery from 75% structural ablation.
22
The work connects the physics of self-organization Physical Review A, 32(2):1007–1018, 1985a. doi:
(Prigogine’s dissipative structures), the information 10.1103/PhysRevA.32.1007.
geometry of inference (Friston’s Free Energy Princi-
ple), and the computational mechanisms of memory Daniel J Amit, Hanoch Gutfreund, and Haim Som-
(Hopfield’s attractor networks). By showing these polinsky. Storing infinite numbers of patterns in
frameworkscomposeintoaunifiedaccountofbiolog- a spin-glass model of neural networks. Physi-
ical intelligence, we demonstrate that the FEP pro- cal Review Letters, 55(14):1530–1533, 1985b. doi:
vides a constructive algorithm that can be scaled to 10.1103/PhysRevLett.55.1530.
large networks.
Per Bak, Chao Tang, and Kurt Wiesenfeld. Self-
The system performs exact hierarchical inference
organized criticality: An explanation of the 1/f
on a generative model where structure is itself part
noise. Physical Review Letters, 59(4):381–384,
of the inference process. The TFM is a quantity de-
1987. doi: 10.1103/PhysRevLett.59.381.
rived from first principles: the expected gradient on
freeenergy. StructuralplasticityguidedbytheTFM Andre M Bastos, W Martin Usrey, Rick A Adams,
implements Bayesian model reduction, pruning con- George R Mangun, Pascal Fries, and Karl J Fris-
nections with insufficient evidence and growing con- ton. Canonical microcircuits for predictive cod-
nectionswherethegradientpredictstheywillreduce ing. Neuron, 76(4):695–711, 2012. doi: 10.1016/j.
surprise. neuron.2012.10.038.
Exact local credit assignment (and by extension,
the full Free Energy Principle) can be implemented John M Beggs and Dietmar Plenz. Neuronal
in a scalable, biologically plausible architecture. The avalanches in neocortical circuits. Journal of Neu-
brain’smechanismsforlearningmaybelessofanap- roscience,23(35):11167–11177,2003.doi: 10.1523/
proximation and more of an exact, elegant solution JNEUROSCI.23-35-11167.2003.
to the problem of maintaining a self-organizing dissi-
Marcus K Benna and Stefano Fusi. Computational
pative structure that persists by minimizing its own
principles of synaptic memory consolidation. Na-
surprise.
ture Neuroscience, 19(12):1697–1706, 2016. doi:
The framework extends naturally to active infer-
10.1038/nn.4401.
ence, where the TFM guides policy structure. The
accompanying codebase includes an Active Inference Nils Bertschinger and Thomas Natschl¨ager. Real-
agent implementation demonstrating this extension. timecomputationattheedgeofchaosinrecurrent
Open questions include formal analysis of conver- neuralnetworks. Neural Computation,16(7):1413–
gence dynamics and finer-grained mechanisms for 1436, 2004. doi: 10.1162/089976604323057443.
synapse-level structural credit.
Byframingneurallearningastheself-organization Rafal Bogacz. A tutorial on the free-energy frame-
of a dissipative system minimizing free energy, we work for modelling perception and learning. Jour-
move beyond viewing brains as computers executing nal of Mathematical Psychology, 76:198–211, 2017.
algorithms to understanding them as physical sys- doi: 10.1016/j.jmp.2015.11.003.
tems instantiating a universal principle. Intelligence
Christopher L Buckley, Chang Sub Kim, Simon Mc-
is a state of matter.
Gregor, and Anil K Seth. The free energy prin-
ciple for action and perception: A mathematical
review. Journal of Mathematical Psychology, 81:
References
55–79, 2017. doi: 10.1016/j.jmp.2017.09.004.
Daniel J Amit, Hanoch Gutfreund, and Haim Som- Salvatore Chirumbolo and Antonio Vella. The un-
polinsky. Spin-glass models of neural networks. derlying dynamics of life and its evolution: A
23
prigogine-inspired informational dissipative sys- Karl Friston, Francesco Rigoli, Dimitri Ognibene,
tem, 2024. URL https://arxiv.org/abs/2412. Christoph Mathys, Thomas Fitzgerald, and Gio-
02459. Submitted December 3, 2024; Last revised vanni Pezzulo. Active inference and epistemic
February 19, 2025. value. Cognitive Neuroscience,6(4):187–214,2015.
doi: 10.1080/17588928.2015.1020053.
Peter Dayan, Geoffrey E Hinton, Radford M Neal,
and Richard S Zemel. The helmholtz machine. Karl Friston, Thomas FitzGerald, Francesco Rigoli,
Neural Computation, 7(5):889–904, 1995. doi: Philipp Schwartenbeck, John O’Doherty, and Gio-
10.1162/neco.1995.7.5.889. vanniPezzulo. Activeinferenceandlearning. Neu-
roscience & Biobehavioral Reviews, 68:862–879,
Thomas Elsken, Jan Hendrik Metzen, and Frank 2016. doi: 10.1016/j.neubiorev.2016.06.022.
Hutter. Neural architecture search: A survey.
Karl Friston, Lancelot Da Costa, Noor Sajid, Conor
Journal of Machine Learning Research, 20(55):1–
Heins, Kai Ueltzh¨offer, Grigorios A Pavliotis, and
21, 2019.
ThomasParr. Thefreeenergyprinciplemadesim-
Nicolas Fr´emaux and Wulfram Gerstner. Neuromod- pler but not too simple. Physics Reports, 1024:
ulated spike-timing-dependent plasticity, and the- 1–29, 2023. doi: 10.1016/j.physrep.2023.07.001.
oryofthree-factorlearningrules. FrontiersinNeu-
Karl J. Friston, Tamara Shiner, Thomas FitzGer-
ral Circuits, 9:85, 2016. doi: 10.3389/fncir.2015.
ald,JosephM.Galea,RickAdams,HarrietBrown,
00085. Published online January 19, 2016.
Raymond J. Dolan, Rosalyn Moran, Klaas Enno
Stephan, and Sven Bestmann. Dopamine, affor-
Robert M French. Catastrophic forgetting in con-
dance and active inference. PLOS Computational
nectionist networks. Trends in Cognitive Sciences,
Biology, 8(1):1–20, 01 2012. doi: 10.1371/journal.
3(4):128–135, 1999. doi: 10.1016/S1364-6613(99)
pcbi.1002327. URL https://doi.org/10.1371/
01294-2.
journal.pcbi.1002327.
Karl Friston. The free-energy principle: a rough
Stefano Fusi, Patrick J Drew, and Larry F Ab-
guide to the brain? Trends in Cognitive Sciences,
bott. Cascade models of synaptically stored mem-
13(7):293–301, 2009. doi: 10.1016/j.tics.2009.04.
ories. Neuron, 45(4):599–611, 2005. doi: 10.1016/
005.
j.neuron.2005.02.001.
Karl Friston. The free-energy principle: A unified WulframGerstner,MarcoLehmann,VasilikiLiakoni,
brain theory? Nature Reviews Neuroscience, 11 Dane Corneil, and Johanni Brea. Eligibility traces
(2):127–138, 2010. doi: 10.1038/nrn2787. and plasticity on behavioral time scales: experi-
mental support of neohebbian three-factor learn-
Karl Friston. A free energy principle for a particu-
ingrules. FrontiersinNeuralCircuits,12:53,2018.
lar physics, 2019. URL https://arxiv.org/abs/
doi: 10.3389/fncir.2018.00053.
1906.10184.
Dhawal Gupta, Scott M. Jordan, Shreyas Chaud-
KarlFristonandPingAo. Freeenergy,value,andat- hari, Bo Liu, Philip S. Thomas, and Bruno Castro
tractors. Computational and Mathematical Meth- da Silva. From past to future: Rethinking eligibil-
ods in Medicine, 2012:937860, 2012. doi: 10.1155/ ity traces, 2023. URL https://arxiv.org/abs/
2012/937860. 2312.12972.
Karl Friston, James Kilner, and Lee Harrison. A SongHan,JeffPool,JohnTran,andWilliamJ.Dally.
free energy principle for the brain. Journal of Learningbothweightsandconnectionsforefficient
Physiology-Paris, 100(1-3):70–87, 2006. doi: 10. neuralnetworks,2015. URLhttps://arxiv.org/
1016/j.jphysparis.2006.10.001. abs/1506.02626.
24
Simon Haykin. Kalman Filtering and Neural Net- Dmitry Krotov and John J Hopfield. Dense asso-
works. Wiley-Interscience, New York, NY, 2001. ciative memory for pattern recognition. In Ad-
ISBN 978-0471369981. doi: 10.1002/0471221546. vances in Neural Information Processing Systems,
volume 29, pages 1172–1180, 2016.
Geoffrey E Hinton and Richard S Zemel. Autoen-
Dmitry Krotov and John J Hopfield. Large asso-
coders,minimumdescriptionlengthandhelmholtz
ciative memory problem in neurobiology and ma-
free energy. In Advances in Neural Information
chine learning. In Proceedings of the 37th Inter-
Processing Systems,volume6,pages3–10.Morgan
national Conference on Machine Learning, pages
Kaufmann, 1993.
5577–5587. PMLR, 2020.
Jakob Hohwy. The self-evidencing brain. Nouˆs, 50
Christopher G Langton. Computation at the edge of
(2):259–285, 2016. doi: 10.1111/nous.12062.
chaos: Phase transitions and emergent computa-
tion. Physica D: Nonlinear Phenomena, 42(1-3):
John J Hopfield. Neural networks and physical sys-
12–37, 1990. doi: 10.1016/0167-2789(90)90064-V.
tems with emergent collective computational abil-
ities. Proceedings of the National Academy of Sci- Timothy P Lillicrap, Daniel Cownden, Douglas B
ences, 79(8):2554–2558, 1982. doi: 10.1073/pnas. Tweed, and Colin J Akerman. Random synap-
79.8.2554. tic feedback weights support error backpropaga-
tion for deep learning. Nature Communications,
Herbert Jaeger. The ”echo state” approach to 7:13276, 2016. doi: 10.1038/ncomms13276.
analysing and training recurrent neural networks
– with an erratum note. Technical Report GMD Hanxiao Liu, Karen Simonyan, and Yiming Yang.
Report 148, German National Research Center for DARTS: Differentiable architecture search. In
Information Technology, Fraunhofer Institute for International Conference on Learning Represen-
Autonomous Intelligent Systems, 2001. Revised tations, 2019. URL https://openreview.net/
version published January 26, 2010. forum?id=S1eYHoC5FX.
Wolfgang Maass, Thomas Natschl¨ager, and Henry
Stuart A Kauffman. The Origins of Order: Self-
Markram. Real-time computing without stable
Organization and Selection in Evolution. Oxford
states: A new framework for neural computa-
University Press, Oxford, 1993. ISBN 978-0-19-
tion based on perturbations. Neural Compu-
505811-6.
tation, 14(11):2531–2560, 2002. doi: 10.1162/
089976602760407955.
James Kirkpatrick, Razvan Pascanu, Neil Rabi-
nowitz, Joel Veness, Guillaume Desjardins, An-
James L McClelland, Bruce L McNaughton, and
drei A Rusu, Kieran Milan, John Quan, Tiago
Randall C O’Reilly. Why there are complemen-
Ramalho, Agnieszka Grabska-Barwinska, Demis
tarylearningsystemsinthehippocampusandneo-
Hassabis, Claudia Clopath, Dharshan Kumaran,
cortex: insights from the successes and failures
and Raia Hadsell. Overcoming catastrophic for-
of connectionist models of learning and memory.
getting in neural networks. Proceedings of the
Psychological Review, 102(3):419–457, 1995. doi:
National Academy of Sciences, 114(13):3521–3526,
10.1037/0033-295X.102.3.419.
2017. doi: 10.1073/pnas.1611835114.
Michael McCloskey and Neal J Cohen. Catastrophic
David C Knill and Alexandre Pouget. The bayesian interference in connectionist networks: The se-
brain: the role of uncertainty in neural coding quentiallearningproblem. InPsychology of Learn-
andcomputation.TrendsinNeurosciences,27(12): ing and Motivation, volume24, pages109–165.El-
712–719, 2004. doi: 10.1016/j.tins.2004.10.007. sevier, 1989. doi: 10.1016/S0079-7421(08)60536-8.
25
Decebal Constantin Mocanu, Elena Mocanu, Pe- Rajesh PN Rao and Dana H Ballard. Predictive
ter Stone, Phuong H Nguyen, Madeleine Gibescu, coding in the visual cortex: a functional inter-
and Antonio Liotta. Scalable training of arti- pretation of some extra-classical receptive-field ef-
ficial neural networks with adaptive sparse con- fects. Nature Neuroscience, 2(1):79–87, 1999. doi:
nectivity inspired by network science. Nature 10.1038/4580.
Communications, 9:2383, 2018. doi: 10.1038/
Esteban Real, Alok Aggarwal, Yanping Huang, and
s41467-018-04316-3.
Quoc V Le. Regularized evolution for image clas-
Theodore H. Moskovitz, Ashok Litwin-Kumar, and sifier architecture search. Proceedings of the AAAI
L. F. Abbott. Feedback alignment in deep con- Conference on Artificial Intelligence, 33(01):4780–
volutional networks, 2019. URL https://arxiv. 4789, 2019. doi: 10.1609/aaai.v33i01.33014780.
org/abs/1812.06488.
DavidERumelhart,GeoffreyEHinton,andRonaldJ
Williams. Learning representations by back-
Gregoire Nicolis and Ilya Prigogine. Self-
propagating errors. Nature, 323(6088):533–536,
Organization in Nonequilibrium Systems: From
1986. doi: 10.1038/323533a0.
Dissipative Structures to Order through Fluctua-
tions. Wiley-Interscience, New York, 1977. ISBN
Noor Sajid, Philip J Ball, Thomas Parr, and Karl J
978-0471024019.
Friston. Active inference: Demystified and com-
pared. Neural Computation, 33(3):674–712, 2021.
Arild Nøkland. Direct feedback alignment provides
doi: 10.1162/neco a 01357.
learning in deep neural networks. In Advances
in Neural Information Processing Systems, vol- Erwin Schr¨odinger. What is Life? The Physical
ume 29, pages 1037–1045, 2016. Aspect of the Living Cell. Cambridge University
Press, Cambridge, 1944.
RandolphJNudo.Plasticity.NeuroRx,3(4):420–427,
2006. doi: 10.1016/j.nurx.2006.07.006. Biswa Sengupta, Martin B Stemmler, and Karl J
Friston. Information and efficiency in the nervous
Thomas Parr and Karl J Friston. Attention or system—a synthesis. PLoS Computational Biol-
salience? Current Opinion in Psychology, 29:1– ogy, 9(7):e1003157, 2013. doi: 10.1371/journal.
5, 2020. doi: 10.1016/j.copsyc.2018.10.006. pcbi.1003157.
Judea Pearl. Probabilistic Reasoning in Intelligent Woodrow L Shew, Hongdian Yang, Thomas Peter-
Systems: Networks of Plausible Inference. Mor- mann, Rajarshi Roy, and Dietmar Plenz. Neu-
gan Kaufmann, San Mateo, CA, 1988. ISBN 978- ronal avalanches imply maximum dynamic range
1558604790. in cortical networks at criticality. Journal of Neu-
roscience,29(49):15595–15600,2009.doi: 10.1523/
Tony A Plate. Holographic reduced representations.
JNEUROSCI.3864-09.2009.
IEEE Transactions on Neural Networks, 6(3):623–
641, 1995. doi: 10.1109/72.377968. Paul Smolensky. Tensor product variable binding
and the representation of symbolic structures in
DietmarPlenzandTaraCThiagarajan.Theorganiz- connectionist systems. Artificial Intelligence, 46
ing principles of neuronal avalanches: cell assem- (1-2):159–216, 1990. doi: 10.1016/0004-3702(90)
blies in the cortex? Trends in Neurosciences, 30 90007-M.
(3):101–110, 2007. doi: 10.1016/j.tins.2007.01.003.
LarryRSquire. Memorysystemsofthebrain: abrief
Ilya Prigogine. Time, structure, and fluctuations. history and current perspective. Neurobiology of
Science, 201(4358):777–785, 1977. doi: 10.1126/ Learning and Memory, 82(3):171–177, 2004. doi:
science.201.4358.777. 10.1016/j.nlm.2004.06.005.
26
Richard S Sutton. Learning to predict by the meth- Barret Zoph and Quoc V Le. Neural architecture
ods of temporal differences. Machine Learning, 3 search with reinforcement learning. In 5th Inter-
(1):9–44, 1988. doi: 10.1007/BF00115009. national Conference on Learning Representations
(ICLR), 2017.
Richard S Sutton and Andrew G Barto. Reinforce-
mentLearning: AnIntroduction. MITPress,Cam-
bridge, MA, 1998. ISBN 0-262-19398-1.
Richard Stuart Sutton. Temporal credit assign-
ment in reinforcement learning. PhD thesis, 1984.
AAI8410337.
Nassim Nicholas Taleb. Antifragile: Things That
Gain from Disorder. Random House, New York,
2012. ISBN 978-1-4000-6782-4.
Gina G Turrigiano. Homeostatic plasticity in neu-
ronal networks: the more things change, the more
they stay the same. Trends in Neurosciences, 22
(5):221–227, 1999. doi: 10.1016/S0166-2236(98)
01341-1.
Chris S Wallace and David L Dowe. Minimum mes-
sagelengthandkolmogorovcomplexity. The Com-
puter Journal, 42(4):270–283, 1999. doi: 10.1093/
comjnl/42.4.270.
Paul J Werbos. Backpropagation through time:
What it does and how to do it. Proceedings of
the IEEE, 78(10):1550–1560, 1990. doi: 10.1109/
5.58337.
Colin White, Mahmoud Safari, Rhea Sukthanker,
Binxin Ru, Thomas Elsken, Arber Zela, De-
badeepta Dey, and Frank Hutter. Neural archi-
tecture search: Insights from 1000 papers, 2023.
URL https://arxiv.org/abs/2301.08727.
Christian Xerri. Plasticity of cortical maps: multi-
ple triggers for adaptive reorganization following
brain damage and spinal cord injury. The Neu-
roscientist, 18(2):133–148, 2012. doi: 10.1177/
1073858410397894.
Friedemann Zenke, Ben Poole, and Surya Ganguli.
Continual learning through synaptic intelligence.
InProceedingsofthe34thInternationalConference
onMachineLearning,volume70,pages3987–3995.
PMLR, 2017.
27
Figure 12: Performance benchmarks for block-sparse networks. Top row: throughput and latency vs. net-
worksizeforvariousbatch/substepconfigurations. Throughputpeaksatbatch64with1substep,achieving
4712 items/s for 327,680 neurons. Bottom heatmaps show the tradeoff between batch size and evolution
substeps for the largest network. Smaller batches with more substeps minimize latency (∼11ms), while
larger batches maximize throughput (∼4700 items/s).
28
Figure 13: 3D visualization of throughput and la-
tency layer cakes. Left: throughput decreases with
more substeps and smaller batches. Right: latency
increases with larger batches and more substeps.
Each layer represents a different network size, show-
ing consistent scaling behavior across architectures
from 16K to 327K neurons.
29

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Self-Evidencing Through Hierarchical Gradient Decomposition: A Dissipative System That Maintains Non-Equilibrium Steady-State by Minimizing Variational Free Energy"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
