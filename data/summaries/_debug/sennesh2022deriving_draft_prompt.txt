=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Deriving time-averaged active inference from control principles
Citation Key: sennesh2022deriving
Authors: Eli Sennesh, Jordan Theriault, Jan-Willem van de Meent

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2022

Key Terms: deriving, surprise, average, principles, jordan, university, control, averaged, northeastern, inference

=== FULL PAPER TEXT ===

Deriving time-averaged active inference from
control principles
Eli Sennesh1[0000−0001−7014−8471], Jordan Theriault1[0000−0002−4680−0172],
Jan-Willem van de Meent2[0000−0001−9465−5398], Lisa Feldman
Barrett1[0000−0003−4478−2051], and Karen Quigley1[0000−0001−8844−990X]
1 Northeastern University, Boston MA 02115, USA
{sennesh.e,jordan theriault,l.barrett,k.quigley}@northeastern.edu
2 University of Amsterdam, 1090 GH Amsterdam, the Netherlands
j.w.vandemeent@uva.nl
Abstract. Active inference offers a principled account of behavior as
minimizing average sensory surprise over time. Applications of active
inference to control problems have heretofore tended to focus on finite-
horizonordiscounted-surpriseproblems,despitederivingfromtheinfinite-
horizon, average-surprise imperative of the free-energy principle. Here
we derive an infinite-horizon, average-surprise formulation of active in-
ference from optimal control principles. Our formulation returns to the
rootsofactiveinferenceinneuroanatomyandneurophysiology,formally
reconnecting active inference to optimal feedback control. Our formula-
tionprovidesaunifiedobjectivefunctionalforsensorimotorcontroland
allows for reference states to vary over time.
Keywords: Hierarchical control · path-integral control · Infinite-time
average-cost.
1 Introduction
Adaptive action requires the integration and close coordination of sensory with
motorsignalsinthenervoussystem.Activeinference[17]providesoneofthefew
available unifying theories of sensorimotor control; it says that the nervous sys-
temencodesbothsensoryandmotorsignalsasafferentpredictionsandreafferent
predictionerrors.Sensorypredictionsinduceerrorsthatcanonlybequashedby
updating the predictions, while motoric predictions induce errors that can be
quashed by simply moving the body to conform to the predicted trajectory [1].
Thefreeenergyprinciple,followingthelogicofactiveinference,saysthatorgan-
ismsmaintaintheirself-organizationasawholeovertimebyavoidingsurprising
interactions between their internal and external environments [16]. This entails
maintainingbodilystateswithinhomeostaticranges[41]byissuingsensory,pro-
prioceptive, and interoceptive predictions that minimize errors under a “prior
preference” [11] or “non-equilibrium steady-state” [19] density. Such a density
must be stationary throughout time.
2202
guA
22
]YS.ssee[
1v10601.8022:viXra
2 E. Sennesh et al.
Early “non-equilibrium steady-state” formulations of active inference pro-
vided probability densities over full trajectories of movement and interaction
[19,20]. In regulatory terms, this corresponds to covariation of bodily states un-
der a “just enough, just in time” [54] mode of regulation that physiologists have
labelled homeostasis [8] with time-varying set points, rheostasis [36], and re-
cently allostasis [54,10,48,60]. A control theorist would call these trajectories or
set-points a reference trajectory or “reference signal” that a controller tries to
track. However, many more recent formulations of active inference use state-
space models with fixed “prior preferences” that correspond to homeostatic set-
points or ranges [11]. They also typically employ either finite time horizons or
exponential discounting of expected free energy, unlike the original formulation
of active inference in terms of average surprise over time. A control theorist
would refer to these as reference states rather than reference trajectories.
This paper will rederive active inference as minimization of path-entropy
over an infinite time horizon. The paper’s formulation will derive from the first
principlesofinfinite-horizon,average-costoptimalcontrol;willallowpreferences
to vary according to their own generative model, and will unify motor active
inference[1](mAI)withdecisionactiveinference[52](dAI).Thiswillalsounify
the computational principles behind motor active inference - the “equilibrium
point”[14,29]or“referenceconfiguration”[15]hypotheses-withthehigher-level
study of sensorimotor behavior as optimal feedback control. Finally, the paper’s
formalism will provide a unified free energy functional for perception, motor
action, and decision making over time.
Section 2 will explain this paper’s notation and lay out an example gener-
ative model supporting the necessary features for the intended formulation of
active inference. Section 3 will summarize belief updating in generative models,
give a recognition model to match the generative model, describe the free en-
ergy principle for perceptual inference, and finish by describing active inference.
Section 4 will then extend active inference to the setting of an explicit reference
model prescribing behavior and give the control criterion corresponding to ac-
tive inference under the free energy principle. Section 5 will derive the resulting
free energy bounds whose optimization will yield a Bellman-optimal feedback
controllerbasedonthegenerativeandrecognitionmodels.Section6willdiscuss
related work; consider implementation issues for infinite-horizon, average-cost
active inference; and conclude. Appendix A will provide derivations for equa-
tions that would otherwise have broken the flow of the paper.
2 Preliminaries and notation
Thispaperwillexplainitsformulationofactiveinferenceintermsofthediscrete-
time graphical model in Figure 1. Like many generative models used to lay out
active inference [27,42], this model employs a hierarchy of temporal scales. We
number these timescales from the shortest to the longest, while numbering ran-
domvariableswithdiscretetimestepst∈1...T fromlefttoright.Forsimplicity,
we also restrict our graphical model to only three levels of hierarchy: observable
variables,fastlatentvariables,andslowrandomvariables.Followingthoserules,
Deriving time-averaged active inference from control principles 3
t=1,…,∞
elacsemiT
gnisaercnI
Passage of Time
s(2)
s(2) t+1 s(2)
t t+2
a(2) s(1) s(1)
t t t+1
a t (1) o t
a
t
Fig.1. A hierarchical generative model we use as an example in this paper. Two
variables (s(1),s(2)) denote unobserved latent states, and each a(k+1) parameterizes a
t t t
reference model for s(k). o represents observed sensory outcomes, and a represents
t t t
the feedback control actions generated by motor reflex-arcs.
observationso andfeedbackmotoractionsa are1-Markov;they“tick”atevery
t t
time-step. The fast latent variables s(1) and a(1) also change at every time-step.
t t
At the next level up, slow latent variables s(2) and a(2) are 2-Markov; they
t t
“tick” every second time-step t+2. We assume arbitrary state spaces for all
random variables, latent and observed, without any discrete or linear-Gaussian
assumptions about their conditional densities. Some evidence suggests [24] that
the brain may in fact represent time by learning a combination of frequencies in
the Laplace domain [51], and so the use of only three levels in the model should
not be taken to describe anything biological.
We write the combined latent states
s(1:2) =(s(1),s(2))
t t t
and the “actions” or reference states
a(0:2) =(a ,a(1),a(2)).
t t t t
We can therefore write the complete state at a time-step t as
s =(o ,s(1:2),a(0:2)).
t t t t
We will denote probability densities over actions as policies π and probability
densitiesinthegenerativemodelasp (witharbitraryparametersθ).Thelowest
θ
level of conditional probability densities then consists of
p (a ,o |a(1),s(1))=π(a |o ,a(1))p (o |a(1),s(1)),
θ t t t t t t t θ t t t
4 E. Sennesh et al.
the fast latent state level consists of
p (a(1),s(1) |s(1) ,a(2),s(2),a )=π(a(1) |s(1),a(2))p (s(1) |s(1) ,s(2),a ),
θ t t t−1 t t t−1 t t t θ t t−1 t t−1
and the slow latent state level consists of
p (a(2),s(2) |s(2) ,a )=π(a(2) |s(2))p (s(2) |s(2) ,a ).
θ t t t−1 t−1 t t θ t t−1 t−1
We write the complete state of the generative model p at a time-step t with its
θ
associated conditional densities as
p (s |s )=p (a ,o |a(1),s(1))p (a(1),s(1) |s(1) ,a(2),s(2),a )
θ t t−1 θ t t t t θ t t t−1 t t t−1
p (a(2),s(2) |s(2) ,a ), (1)
θ t t t−1 t−1
and the joint density over time (conditioned on a fixed initial state s ) as
0
T
(cid:89)
p (s |s )= p (s |s ). (2)
θ 1:T 0 θ t t−1
t=1
Themodeltreatsoutcomeso asobserved,a asafeedback-drivenmotoraction,
t t
and other variables as latent. Inspired by the referent configuration account of
motor control [15,30], the model treats a(1:2) as parameterizing “prior prefer-
t
ences” or referent configurations
R(s )=R(o |a(1))R(s(1) |a(2)). (3)
t t t t t
a modelsthefeedbackcontrolactionofmotorreflexes.a(1) parameterizesaref-
t t
erence state for o . a(2) parameterizes a reference model for s(1). Since reference
t t t
trajectories direct action, we consider their distributions to be policies
π(a ,a(1:2) |o ,s(1:2))=π(a |a(1),o )π(a(1) |s(1),a(2))π(a(2) |s(2)). (4)
t t t t t t t t t t t t
s(2), as the highest level latent state, has no reference model. In neuroscience,
t
it might correspond to predictive modeling at the highest level of the neuraxis
or cortical hierarchy [3,31,44]. In an engineering setting, it might contain both
environment and task state [37,46,56] or reward machine [25,7] state.
The likelihood p (o | a(1),s(1)) does not specify the reference model; it in-
θ t t t
stead provides the statistical grounding for both the latent states and the refer-
encemodelparameters.Themodelheredoesnotassumethatreferencedensities
at all levels are prespecified or learned, but instead leaves that issue open.
Wethendesignateascostfunctionsthesurprisalsovercompletestates(under
the reference model) and over observations (under the generative model)
J(s )=−logR(s ), (5)
t t
L(s )=−logp (o |a(1),s(1)). (6)
t θ t t t
Deriving time-averaged active inference from control principles 5
p Probability density for the generative model
θ
q Probability density for the recognition model
φ
R Probability density for the reference model
π Policy density over actions and references
t Discrete time-step index
o Observations
t
s(1:2) Unobserved model states
t
a(1:2) Parameters to a reference model R
t
a Control actions
t
s A complete model state for time t
t
Table 1. Random variable names used in this paper
Equation 5 equals the negative of the reward function used in distribution-
conditioned reinforcement learning [38] and can represent any control objective.
This paper will condition behavioral trajectories upon an initial state s as
0
context. This initial state corresponds to the beginning of a behavioral episode.
The following states from time 1 until time T, sampled from a generative model
with parameters θ, are then written as sampled from the joint density
s ∼p (s |s ).
1:T θ 1:T 0
This section has described a generative model and a decision objective under
which to formulate active inference. Table 1 summarizes the notation the rest
of the paper will use. The next section will lay out belief updating for the gen-
erative model, a recognition model to represent updated beliefs, and the free
energyprincipleforperceptualinference.Latersectionswillshowhowtoextend
free-energy minimization to approximate a feedforward planner (in the gener-
ative model) and feedback controller (in the recognition model) that minimize
surprisal under the reference model.
3 Surprise minimization and the free energy principle
Section 2 gave a generative model and a way of writing arbitrary preferences as
probability densities. However, the formalism constructed so far would induce a
merelyfeedforwardmodel-basedplanner,onewhichcouldnotcorrectupcoming
movements in light of observations. Bayes’ rule specifies how to update proba-
bilistic beliefs about unobserved variables in light of observations:
p (o ,s(1:2),a(1:2) |s )
p (s(1:2),a(1:2) |o ,s )= θ 1:t 1:t 1:t 0 . (7)
θ 1:t 1:t 1:t 0 p (o |s )
θ 1:t 0
ThedenominatorofEquation7iscalledthemarginallikelihood,anditsnegative
logarithm is the surprise under the generative model
h(o )=−logp (o |s ).
1:t θ 1:t 0
6 E. Sennesh et al.
Friston’s free energy principle [21] posits that a system, organism, or agent in a
changing environment preserves its structure against the randomness of its en-
vironment by embodying a generative model of its environment and minimizing
that model’s long-term average surprise
1
H(o )= lim −logp (o |s ). (8)
t T→∞T θ 1:T 0
In most generative models, neither the denominator of Equation 7 nor the sur-
prise of Equation 8 are analytically tractable, and Bayesian inference requires
approximation. Active inference in particular approximates optimal belief up-
dating by substituting a tractable recognition model q (with parameters φ) for
φ
the posterior distribution
s(1:2),a(1:2) ∼q (s(1:2),a(1:2) |o ,a ,s ),
1:T 1:T φ 1:T 1:T 1:T 1:T 0
T
q (s(1:2),a(1:2) |o ,a ,s )= (cid:89) q (s(1:2),a(1:2) |o ,a ,s ,s ).
φ 1:T 1:T 1:T 1:T 0 φ t t t t t+1 t−1
t=1
This recognition model is conditioned on both the previous time-step t−1 and
the next time-step t+1, and can therefore perform retroactive belief updates.
To improve the recognition model’s approximation to the posterior distri-
bution, active inference entails evaluating and minimizing the variational free
energy (Equation 9, derivation in Proposition 1 in Appendix A)
(cid:104) (cid:105)
F (t)=E −logp (o |a(1),s(1)) +
θ,φ qφ θ t t t
(cid:16) (cid:17)
D q (s(1:2),a(1:2) |o ,s ,s )(cid:107)p (s(1:2),a(1:2) |s ) . (9)
KL φ t t t t+1 t−1 θ t t t−1
The free energy serves as a tractable upper bound to the surprise
H(o )≤F (t).
t θ,φ
Intuitively, given an observation at each time-step t, minimizing the free energy
amounts to updating the beliefs of the recognition model q to approximate the
φ
posterior distribution of the generative model p . A model-based controller can
θ
then use those updated beliefs to revise or plan actions into the future. Active
inference has therefore often been formulated as using action to minimize this
free energy bound. Such a move then prompts the question of how to encode a
desirable reference trajectory into the generative model or another term of the
free energy bound [18]. The next section will define notions of surprise and free
energy that encode fit to an explicitly specified reference trajectory.
4 Active inference with an explicit reference
Minimizing free energy fits a model-based controller’s generative and recogni-
tion models to ongoing trajectories of observations. However, for the updated
Deriving time-averaged active inference from control principles 7
beliefs to determine action, the controller must use them to evaluate the fit to
the reference trajectory (Equation 5) and emit motor actions. Fortunately, Thi-
jssen [58] gave an interpretation of probabilistic updating in terms of control:
the recognition model q acts as a state-feedback controller, for which the varia-
φ
tionalfreeenergybecomesarunningcontrolcost.Thissectionwillshowhowto
evaluate fit to the reference trajectory under the recognition model, and specify
the functional it must optimize to serve as a feedback controller.
The generative model in Section 2 and recognition model in Section 3 use
discrete time-steps and explicitly specify the “pathwise” reference model sepa-
rately from the generative and recognition models. The surprise to minimize is
therefore the long run average of the cross-entropy
T
1 (cid:88)
H(q ,R)= lim E [−logR(s )]. (10)
φ T→∞T st∼qφ t
t=1
Equation10givesthelong-termaveragesurpriseofusingthereferencemodelto
approximate the posterior beliefs of the recognition model. Replacing the refer-
encemodelwiththeforwardgenerativemodelwouldthenamounttominimizing
thelong-termaveragesurprise(entropy);thisgeneralizationtreatsthereference
model as specifying a trajectory for the feedback controller to track.
Standardpropertiesoffreeenergyfunctionalsimplythatadesirableobjective
functionalwouldupperboundthesumofreferencesurpriseandsensorysurprise
H(R(s ))+H(o )≤J(t). (11)
t t
Such a free energy functional would balance the reference model’s surprise (the
firstterm) withthegenerativemodel’ssurprise (thesecondterm). Infactit can
be formed simply by adding Equation 10 to Equation 9
J (t)=H(q ,R)+F (t) (12)
θ,φ φ θ,φ
=E [J(s )]+F (t), (13)
st∼qφ t θ,φ
and expanding the term for Equation 9 will yield a long-form expression
J (t)=E [J(s )]+E [L(s )]+
θ,φ qφ t qφ t
(cid:16) (cid:17)
D q (s(1:2),a(1:2) |o ,s ,s )(cid:107)p (s(1:2),a(1:2) |s ) . (14)
KL φ t t t t+1 t−1 θ t t t−1
Equation 14 gives an objective functional in terms of
– The reference surprisal under the recognition model,
– The observation surprisal under the recognition model, and
– The deviation of the recognition model from the generative model.
Neuroscientists [12,50] and ecologists [53] have found evidence that animals
optimizeaglobal capture rate J¯inmanydecisions:rewardsminuscosts,divided
by time. Active inference modelers typically ground the construct of “reward”
8 E. Sennesh et al.
in reduction of surprise [35], and so a broad field of evidence comes together
to support the time-averaging functional form implied by Bayesian mechanics
in both their “steady-state density” and “pathwise” formulations [45]. The next
section will therefore apply the principles of stochastic optimal feedback con-
trol for the partially observed setting and average-cost criterion, and solve the
resulting control problem to formulate active inference.
5 Deriving time-averaged active inference from optimal
control
The average-cost criterion for optimality entails minimizing the indefinite sur-
prise rate with respect to the generative model p (s |s )
θ 1:T 0
J˜(s )= lim E (cid:2) J¯ (s ) (cid:3) . (15)
0
T→∞
pθ(s1:T|s0) θ,φ 1:T
This minimization requires estimating Equation 15 for each behavioral episode
in context, a “global surprise rate” in terms of J (t)
θ,φ
T
J¯ (s )= 1 (cid:88) J (t). (16)
θ,φ 1:T T θ,φ
t=1
Plugging Equation 14 into Inequality 11 shows that minimizing Equation 16
will, by proxy, minimize the reference and sensory surprise in the context of a
sampled state trajectory s . This estimation does not require a prespecified
0:T
episode length T, and can be performed under the generative model
J¯ (s )=E (cid:2) J¯ (s ) (cid:3) . (17)
θ,φ 0 s1:T∼pθ(s1:T|s0) θ,φ 1:T
Having estimates of Equation 17 will enable minimizing the mean-centered sur-
prise at each time-step
h(t;s )=J (t)−J¯(s ). (18)
0 θ,φ 0
The differential Bellman equation [59] defines optimal behavior as recursively
minimizing the mean-centered surprise at each time-step, or surprise-to-go
(cid:104) (cid:105)
H˜∗(t;s )=h(t;s )+minE H˜∗(t+1;s ) . (19)
0 0
at
st+1∼pθ(·|st) 0
The minimization over actions in Equation 19 assumes a fixed action space and
feedforward planning, which may result in very high-dimensional recursive opti-
mization problems. These assumptions also prove empirically, as well as compu-
tationally,problematic.Organismsarenotbornknowingalltheiraffordances[9];
theylearnthem[40].Noise[13,32],uncertainty[23],andvariability[47]areubiq-
uitousinmotorcontrol,andsomovementmustbestabilizedbyonlinefeedback.
Deriving time-averaged active inference from control principles 9
Stochasticoptimalfeedback controlthereforerequiresanoptimalityprinciple
thatallowsforintegratingobservationsbetweenactionsteps.Ratherthanrecur-
sivelyoptimizeindividualactions,Equation20belowthereforeinsteadconsiders
optimality of the feedback-stabilized transition density
(cid:104) (cid:105)
H˜∗(t;s )=h(t;s )+minE H˜∗(t+1;s ) . (20)
0 0
qφ
st+1∼qφ(·|st) 0
Equation 20 defines an optimal controller as one that achieves optimal state
transitions; individual actions act only as parameters to the optimal transition
density. These optimal state transitions take the form of a generative model
for agency, in which the generative model p (s | s ) produces feasible state
θ t+1 t
transitions and the Bellman optimality criterion “weighs” them according to
their surprise-to-go
(cid:16) (cid:17)
exp −H˜∗(t+1;s ) p (s |s )
0 θ t+1 t
q∗(s |s )= . (21)
t+1 t (cid:104) (cid:16) (cid:17)(cid:105)
E exp −H˜∗(t+1;s )
st+1∼pθ(·|st) 0
The denominator of Equation 21 would typically correspond to the marginal
probability of an observation. Here it consists of the present state’s expected
surprise-to-go weight under the generative model. Potential future states that
leadtohighsurpriseunderthereferencemodelwillhavehighsurprise-to-goand
thereforelowweightunderEquation21.Presentstatesthatleadtostatesclosely
fitting the reference trajectory will have low surprise-to-go, resulting in a high
denominator that spreads weight around among possible future states.
The availability of a closed-form density for the optimal transition density
will help simplify the differential Bellman equation itself. Proposition 3 (in Ap-
pendix A) shows that by substituting Equation 21 into Equation 20 we can
obtain a path-integral expression for the optimal differential surprise-to-go with
both the feedforward controller p
θ
(cid:34) (cid:32) T (cid:33)(cid:35)
H˜∗(s )=−logE exp (cid:88) (J(s )+L(s ))−J¯(s ) , (22)
0 pθ(s1:T|s0) t t 0
t=1
and the feedback controller q
φ
(cid:34) (cid:32) T (cid:33)(cid:35)
H˜∗(s )=−logE exp (cid:88) J (t)−J¯(s ) . (23)
0 qφ(s1:T|s0) θ,φ 0
t=1
These equations employ “smooth” minimization rather than “hard” recursive
minimization, and so they support feedforward planning, feedback-driven up-
dating, and sensitivity of behavior to risk [57,39]. Jensen’s inequality will then
yield a tractable upper bound on the optimal differential surprise-to-go under
the feedback controller q
φ
(cid:34) T (cid:35)
H˜∗(s )≤−E (cid:88) h(t;s ) =F˜∗ . (24)
0 qφ(s1:T|s0) 0 θ,φ
t=1
10 E. Sennesh et al.
Minimizing this differential free energy F˜∗ minimizes both the sensory sur-
θ,φ
priseandtheoptimalsurprise-to-gofunctionbyproxy.Thiskindofinformation-
theoretic upper bound on a surprisal term is precisely what predictive coding
process theories [4,6] posit that the brain can optimize by updating θ and φ.
6 Discussion
Related work Our formulation follows in a tradition of unifying active inference
with optimal control approaches. Our hierarchical graphical model follows most
closely from the one featured by Friston [22] and Pezzulo [42] for hierarchical
active inference in decision making and motor control. In contrast to theirs, our
model includes only a single observation at the lowest hierarchical level rather
than one observed variable per level.
We also draw inspiration from information-theoretic control schemes not la-
belled by their authors as “active inference”. Piray and Daw [43] considered a
path-integral control approach to planning and reinforcement learning, which
they related to grid cells in the entorhinal cortex. Mitchell et al [34] modeled
motorlearningasminimizationofafreeenergyfunctional.Nasrianyetal’swork
on distribution-conditioned reinforcement learning gave us our scheme for pa-
rameterizing reference distributions [38], and Sennesh et al [49] applied such an
objective to active inference modeling of interoception and allostatic regulation.
Implementations Weemployedtheinfinite-horizon,average-surprisecriterionto
fitwiththeapparenttime-averagingofdopaminesignalsinthebrain[12,50],but
algorithmsforthiscontrolcriterionremainanactiveresearchareawithnostan-
dardapproach.Arecentsurvey[28]showedthatmostsoftwareimplementations
of active inference models still involve either finite horizons or exponential dis-
counting criteria.Those whichdo supportinfinitehorizons andnonlinear model
families mostly take algorithmic inspiration from reinforcement learning (RL).
In that domain, Tadepalli and Ok [55] published the first model-based RL
algorithm for our criterion in 1998, while Baxter and Bartlett [5] gave a biased
policy gradient estimator. It took another decade for Alexander and Brown [2]
to give a recursive decomposition for average-cost temporal-difference learn-
ing. Zhang and Ross [61] have only recently published the first adaptation of
“deep” reinforcement learning algorithms (based on function approximation) to
theaverage-cost criterion,which remainsmodel free.Jafarnia-Jahromiet al[26]
recently gave the first algorithm for infinite-horizon, average-cost partially ob-
servable problems with a known observation density and unknown dynamics.
Conclusion Thisconcludesthederivationofaninfinite-horizon,average-surprise
formulation of active inference. Since our formulation contextualizes behavioral
episodes, it only requires planning and adjusting behavior in context (e.g. from
timesteps1toT),despiteoptimizinga“global”(indefinite)surpriserate(Equa-
tion 15). We suggest that this formulation of active inference can advance a
probabilistic approach to model-based, hierarchical feedback control [40,33].
Deriving time-averaged active inference from control principles 11
A Detailed derivations
This appendix provides detailed derivations for equations used elsewhere, par-
ticularly where doing so would have distracted from the flow of the paper.
Proposition 1 (Variational free energy as divergence from an unnor-
malized joint distribution). The variational free energy (Equation 9) is de-
fined as the Kullback-Leibler divergence of the recognition model q from the
φ
unnormalized joint distribution of the generative model p
θ
(cid:16) (cid:17)
F (t)=D q (s(1:2),a(1:2) |o ,s ,s )(cid:107)p (s |s ) ,
θ,φ KL φ t t t t+1 t−1 θ t t−1
and therefore equals a sum of the cross entropy between the recognition model
and the sensory likelihood and the exclusive KL divergence from the recognition
model to the generative model over the latent variables
(cid:104) (cid:105)
F (t)=E −logp (o |a(1),s(1)) +
θ,φ qφ θ t t t
(cid:16) (cid:17)
D q (s(1:2),a(1:2) |o ,s ,s )(cid:107)p (s(1:2),a(1:2) |s ) .
KL φ t t t t+1 t−1 θ t t t−1
Proof. Takingadivergencebetweenthe(normalized)recognitionmodelandthe
(unnormalized) joint generative model will yield
(cid:16) (cid:17)
F (t)=D q (s(1:2),a(1:2) |o ,s ,s )(cid:107)p (s |s )
θ,φ KL φ t t t t+1 t−1 θ t t−1
(cid:34) (cid:35)
p (s |s )
=E −log θ t t−1
qφ(s( t 1:2),a( t 1:2)|ot,st+1,st−1) q (s(1:2),a(1:2) |o ,s ,s )
φ t t t t+1 t−1
(cid:34) (cid:35)
p (o |a(1),s(1))p (s(1:2),a(1:2) |s )
=E −log θ t t t θ t t t−1
qφ(s( t 1:2),a( t 1:2)|ot,st+1,st−1) q (s(1:2),a(1:2) |o ,s ,s )
φ t t t t+1 t−1
(cid:34) (cid:35)
(cid:104) (cid:105) p (s(1:2),a(1:2) |s )
=E −logp (o |a(1),s(1)) −E log θ t t t−1 ,
qφ θ t t t qφ q (s(1:2),a(1:2) |o ,s ,s )
φ t t t t+1 t−1
as required.
Proposition 2 (KL divergence of the optimal feedback controller from
the feedforward controller). The exclusive Kullback-Leibler divergence of the
optimal feedback controller q∗ from the feedforward generative model p is
θ
(cid:104) (cid:105)
D (q∗(s |s )(cid:107)p (s |s ))=−E H˜∗(t+1;s ) −
KL t+1 t θ t+1 t q∗(st+1|st) 0
(cid:104) (cid:16) (cid:17)(cid:105)
logE exp −H˜∗(t+1;s ) . (25)
pθ(st+1|st) 0
Proof. We begin by writing out the definition of a KL divergence
(cid:20) (cid:21)
p (s |s )
D (q∗(s |s )(cid:107)p (s |s ))=E −log θ t+1 t .
KL t+1 t θ t+1 t q∗(st+1|st) q∗(s |s )
t+1 t
12 E. Sennesh et al.
Thedefinitionofq∗intermsofp (Equation21)allowstheinnerratioofdensities
θ
to simplify to
p (s |s )
θ t+1 t =p (s |s )(q∗(s |s ))−1
q∗(s |s ) θ t+1 t t+1 t
t+1 t
 (cid:104) (cid:16) (cid:17)(cid:105)
E exp −H˜∗(t+1;s )
=(cid:40)p θ (cid:40)(s t (cid:40) +1 (cid:40) | (cid:40) s (cid:40) t ) e p x θ p (s (cid:16) t+ − 1| H s ˜ t ∗ ) (t+1;s ) (cid:17) (cid:40)p (cid:40)(s (cid:40)(cid:40) | 0 (cid:40) s (cid:40) ) 
0 θ t+1 t
(cid:104) (cid:16) (cid:17)(cid:105)
E exp −H˜∗(t+1;s )
p
θ
(s
t+1
|s
t
)
=
pθ(st+1|st) 0
.
(cid:16) (cid:17)
q∗(s t+1 |s t ) exp −H˜∗(t+1;s )
0
This simplified ratio therefore has the logarithm
p (s |s ) (cid:104) (cid:16) (cid:17)(cid:105)
log θ t+1 t =logE exp −H˜∗(t+1;s ) +H˜∗(t+1;s )
q∗(s |s ) pθ(st+1|st) 0 0
t+1 t
and the divergence becomes
D (q∗(s |s )(cid:107)p (s |s ))=
KL t+1 t θ t+1 t
(cid:104) (cid:105) (cid:104) (cid:16) (cid:17)(cid:105)
−E H˜∗(t+1;s ) −logE exp −H˜∗(t+1;s ) .
q∗(st+1|st) 0 pθ(st+1|st) 0
Proposition 3 (Path-integral expression for the optimal differential
surprise-to-go). The optimal differential surprise-to-go function defined by the
Bellman equation (Equation 20)
(cid:104) (cid:105)
H˜∗(t;s )=h(t;s )+minE H˜∗(t+1;s )
0 0
qφ
st+1∼qφ(·|st) 0
can be simplified by substituting in q∗ to obtain a path-integral expression
(cid:34) (cid:32) T (cid:33)(cid:35)
H˜∗(s )=−logE exp (cid:88) (J(s )+L(s ))−J¯(s ) ,
0 pθ(s1:T|s0) t t 0
t=1
(cid:34) (cid:32) T (cid:33)(cid:35)
=−logE exp (cid:88) J (t)−J¯(s ) .
qφ(s1:T|s0) θ,φ 0
t=1
Proof. Substituting Equation 21 into Equation 20 yields
(cid:104) (cid:105)
H˜∗(t;s )=J¯(s )−J (t)+E H˜∗(t+1;s ) , (26)
0 0 θ,φ q∗(st+1|st) 0
(cid:104) (cid:105)
whose recursive term is E H˜∗(t+1;s ) . The divergence term in J
q∗(st+1|st) 0
(Equation 14) will cancel this term. By Proposition 2 the divergence equals
D (q∗(s |s )(cid:107)p (s |s ))=
KL t+1 t θ t+1 t
(cid:104) (cid:105) (cid:104) (cid:16) (cid:17)(cid:105)
−E H˜∗(t+1;s ) −logE exp −H˜∗(t+1;s ) .
q∗(st+1|st) 0 pθ(st+1|st) 0
Deriving time-averaged active inference from control principles 13
Substituting Equation 25 into Equation 14 will yield
(cid:104) (cid:105) (cid:104) (cid:16) (cid:17)(cid:105)
−J (t)=E H˜∗(t+1;s ) +logE exp −H˜∗(t+1;s )
θ,φ q∗(st+1|st) 0 pθ(st+1|st) 0
+E [−J(s )]+E [−L(s )],
qφ t qφ t
whose first term will cancel the recursive optimization when substituted into
Equation 26. The result will be a “smoothly minimizing” expression for the
optimal differential surprise-to-go
H˜∗(t;s )=J¯(s )−(J(s )+L(s ))
0 0 t t
(cid:104) (cid:16) (cid:17)(cid:105)
−logE exp −H˜∗(t+1;s ) ,
pθ(st+1|st) 0
and after unfolding of the recursive expectation, a path-integral expression for
the optimal differential surprise-to-go
(cid:34) (cid:32) T (cid:33)(cid:35)
H˜∗(s )=−logE exp (cid:88) (J(s )+L(s ))−J¯(s ) .
0 pθ(s1:T|s0) t t 0
t=1
Sampling a trajectory of states from a feedback controller q instead of the
φ
feedforward planner p will then result in a nonzero divergence term
θ
(cid:34) (cid:32) T (cid:33)(cid:35)
H˜∗(s )=−logE exp (cid:88) J (t)−J¯(s ) .
0 qφ(s1:T|s0) θ,φ 0
t=1
References
1. Adams,R.A.,Shipp,S.,Friston,K.J.:Predictionsnotcommands:activeinference
in the motor system. Brain Structure and Function 218(3), 611–643 (2013)
2. Alexander, W.H., Brown, J.W.: Hyperbolically discounted tempo-
ral difference learning. Neural Computation 22(6), 1511–1527 (2010).
https://doi.org/10.1162/neco.2010.08-09-1080
3. Barrett, L.F., Simmons, W.K.: Interoceptive predictions in the brain. Nature
Reviews Neuroscience 16(7), 419–429 (2015). https://doi.org/10.1038/nrn3950,
http://dx.doi.org/10.1038/nrn3950%5Cnhttp://dx.doi.org/10.1038/nrn3950%
5Cnpapers3://publication/doi/10.1038/nrn3950
4. Bastos, A.M., Usrey, W.M., Adams, R.A., Mangun, G.R., Fries, P., Friston, K.J.:
Canonical microcircuits for predictive coding. Neuron 76(4), 695–711 (2012)
5. Baxter, J., Bartlett, P.L.: Infinite-horizon policy-gradient estimation.
Journal of Artificial Intelligence Research 15, 319–350 (Nov 2001).
https://doi.org/10.1613/jair.806
6. Bogacz,R.:Atutorialonthefree-energyframeworkformodellingperceptionand
learning. Journal of mathematical psychology 76, 198–211 (2017)
7. Camacho, A., Icarte, R.T., Klassen, T.Q., Valenzano, R., McIlraith, S.A.: LTL
and beyond: Formal languages for reward function specification in reinforcement
learning. In: IJCAI International Joint Conference on Artificial Intelligence. vol.
2019-Augus, pp. 6065–6073 (2019). https://doi.org/10.24963/ijcai.2019/840
14 E. Sennesh et al.
8. Carpenter,R.:Homeostasis:apleaforaunifiedapproach.Advancesinphysiology
education 28(4), 180–187 (2004)
9. Cisek, P., Kalaska, J.F.: Neural mechanisms for interacting with a world
full of action choices. Annual Review of Neuroscience 33, 269–298 (2010).
https://doi.org/10.1146/annurev.neuro.051508.135409
10. Corcoran, A.W., Hohwy, J.: Allostasis, interoception, and the free energy princi-
ple: Feeling our way forward. In: The Interoceptive Mind: From homeostasis to
awareness, pp. 272–292. Oxford University Press (2019)
11. Da Costa, L., Parr, T., Sajid, N., Veselic, S., Neacsu, V., Friston, K.: Active in-
ference on discrete state-spaces: a synthesis. Journal of Mathematical Psychology
99, 102447 (2020)
12. Daw, N.D., Touretzky, D.S.: Behavioral considerations suggest an average reward
td model of the dopamine system. Neurocomputing 32, 679–684 (2000)
13. Faisal, A.A., Selen, L.P., Wolpert, D.M.: Noise in the nervous system. Nature
reviews neuroscience 9(4), 292–303 (2008)
14. Feldman, A.G.: Once more on the equilibrium-point hypothesis (λ model)
for motor control. Journal of Motor Behavior 18(1), 17–54 (1986).
https://doi.org/10.1080/00222895.1986.10735369
15. Feldman, A.G.: Referent control of action and perception (2015).
https://doi.org/10.1007/978-1-4939-2736-4
16. Friston,K.:Thefree-energyprinciple:aunifiedbraintheory?Naturereviewsneu-
roscience 11(2), 127–138 (2010)
17. Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., Pezzulo, G.: Active in-
ference: a process theory. Neural computation 29(1), 1–49 (2017)
18. Friston, K., Samothrakis, S., Montague, R.: Active inference and agency: opti-
malcontrolwithoutcostfunctions.BiologicalCybernetics106(8–9),523–541(Oct
2012). https://doi.org/10.1007/s00422-012-0512-8
19. Friston,K.,Stephan,K.,Li,B.,Daunizeau,J.:Generalisedfiltering.Mathematical
Problems in Engineering 2010 (2010)
20. Friston, K.J., Daunizeau, J., Kiebel, S.J.: Reinforcement learning or active infer-
ence? PloS one 4(7), e6421 (2009)
21. Friston, K.J., Daunizeau, J., Kilner, J., Kiebel, S.J.: Action and behav-
ior: A free-energy formulation. Biological Cybernetics 102(3), 227–260 (2010).
https://doi.org/10.1007/s00422-010-0364-z
22. Friston, K.J., Rosch, R., Parr, T., Price, C., Bowman, H.: Deep temporal models
and active inference. Neuroscience and Biobehavioral Reviews 77(April), 388–402
(2017).https://doi.org/10.1016/j.neubiorev.2017.04.009,citationKey:Friston2017
23. Gallivan, J.P., Chapman, C.S., Wolpert, D.M., Flanagan, J.R.: Decision-making
in sensorimotor control. Nature Reviews Neuroscience 19(9), 519–534 (2018)
24. Howard, M.W.: Formal models of memory based on temporally-varying represen-
tations.In:Thenewhandbookofmathematicalpsychology,Volume3.Cambridge
University Press (2022)
25. Icarte,R.T.,Klassen,T.Q.,Valenzano,R.,McIlraith,S.A.:Usingrewardmachines
for high-level task specification and decomposition in reinforcement learning. In:
35thInternationalConferenceonMachineLearning,ICML2018.vol.5,pp.3347–
3358 (2018)
26. Jafarnia-Jahromi,M.,Jain,R.,Nayyar,A.:Onlinelearningforunknownpartially
observablemdps.In:Proceedingsofthe25thInternationalConferenceonArtificial
Intelligence and Statistics (AISTATS). vol. 151, p. 21. Proceedings of Machine
Learning Research, Valencia, Spain (2022)
Deriving time-averaged active inference from control principles 15
27. Kiebel, S.J., Daunizeau, J., Friston, K.J.: A hierarchy of time-scales
and the brain. PLOS Computational Biology 4(11), 1–12 (11 2008).
https://doi.org/10.1371/journal.pcbi.1000209, https://doi.org/10.1371/journal.
pcbi.1000209
28. Lanillos,P.,Meo,C.,Pezzato,C.,Meera,A.A.,Baioumy,M.,Ohata,W.,Tschantz,
A., Millidge, B., Wisse, M., Buckley, C.L., Tani, J.: Active inference in robotics
and artificial agents: Survey and challenges (arXiv:2112.01871) (Dec 2021), http:
//arxiv.org/abs/2112.01871, arXiv:2112.01871 [cs]
29. Latash,M.L.:Motorsynergiesandtheequilibrium-pointhypothesis.MotorControl
14(3), 294–322 (2010). https://doi.org/10.1123/mcj.14.3.294
30. Latash,M.L.:PhysicsofBiologicalActionandPerception.AcademicPress(2019).
https://doi.org/10.1016/C2018-0-04663-0
31. Livneh, Y., Sugden, A.U., Madara, J.C., Essner, R.A., Flores, V.I., Sugden,
L.A., Resch, J.M., Lowell, B.B., Andermann, M.L.: Estimation of Current and
Future Physiological States in Insular Cortex. Neuron 105(6), 1094–1111.e10
(2020). https://doi.org/10.1016/j.neuron.2019.12.027, https://doi.org/10.1016/j.
neuron.2019.12.027
32. Manohar,S.G.,Chong,T.T.J.,Apps,M.A.,Batla,A.,Stamelou,M.,Jarman,P.R.,
Bhatia, K.P., Husain, M.: Reward pays the cost of noise reduction in motor and
cognitive control. Current Biology 25(13), 1707–1716 (2015)
33. Merel, J., Botvinick, M., Wayne, G.: Hierarchical motor control in
mammals and machines. Nature Communications 10(1), 1–12 (2019).
https://doi.org/10.1038/s41467-019-13239-6, http://dx.doi.org/10.1038/
s41467-019-13239-6
34. Mitchell,B.A.,Lauharatanahirun,N.,Garcia,J.O.,Wymbs,N.,Grafton,S.,Vet-
tel, J.M., Petzold, L.R.: A minimum free energy model of motor learning. Neural
computation 31(10), 1945–1963 (2019)
35. Morville, T., Friston, K., Burdakov, D., Siebner, H.R., Hulme, O.J.: The homeo-
static logic of reward. bioRxiv p. 242974 (2018)
36. Mrosovsky, N.: Rheostasis: the physiology of change. Oxford University Press
(1990)
37. Nasiriany, S., Lin, S., Levine, S.: Planning with Goal-Conditioned Policies. In:
Advances in Neural Information Processing Systems. No. NeurIPS (2019)
38. Nasiriany, S., Pong, V.H., Nair, A., Khazatsky, A., Berseth, G., Levine, S.: DisCo
RL: Distribution-Conditioned Reinforcement Learning for General-Purpose Poli-
cies. In: IEEE International Conference on Robotics and Automation (2021),
http://arxiv.org/abs/2104.11707
39. Pan, Y., Theodorou, E.A.: Nonparametric infinite horizon Kullback-Leibler
stochastic control. IEEE SSCI 2014 - 2014 IEEE Symposium Series on Com-
putational Intelligence - ADPRL 2014: 2014 IEEE Symposium on Adaptive
Dynamic Programming and Reinforcement Learning, Proceedings 2(2) (2014).
https://doi.org/10.1109/ADPRL.2014.7010616
40. Pezzulo,G.,Cisek,P.:NavigatingtheAffordanceLandscape:FeedbackControlas
a Process Model of Behavior and Cognition. Trends in Cognitive Sciences 20(6),
414–424 (2016). https://doi.org/10.1016/j.tics.2016.03.013, http://dx.doi.org/10.
1016/j.tics.2016.03.013
41. Pezzulo, G., Rigoli, F., Friston, K.: Active inference, homeostatic regulation and
adaptive behavioural control. Progress in neurobiology 134, 17–35 (2015)
42. Pezzulo, G., Rigoli, F., Friston, K.J.: Hierarchical active inference: A the-
ory of motivated control. Trends in Cognitive Sciences 22(4), 294–306 (2018).
https://doi.org/10.1016/j.tics.2018.01.009
16 E. Sennesh et al.
43. Piray, P., Daw, N.D.: Linear reinforcement learning in planning, grid fields, and
cognitive control. Nature communications 12(1), 1–20 (2021)
44. Quigley, K.S., Kanoski, S., Grill, W.M., Barrett, L.F., Tsakiris, M.: Functions
of Interoception: From Energy Regulation to Experience of the Self. Trends
in Neurosciences 44(1), 29–38 (2021). https://doi.org/10.1016/j.tins.2020.09.008,
https://doi.org/10.1016/j.tins.2020.09.008
45. Ramstead, M.J., Sakthivadivel, D.A., Heins, C., Koudahl, M., Millidge, B.,
Da Costa, L., Klein, B., Friston, K.J.: On bayesian mechanics: A physics of and
by beliefs. arXiv preprint arXiv:2205.11543 (2022)
46. Ringstrom, T.J., Hasanbeig, M., Abate, A.: Jump operator planning:
Goal-conditioned policy ensembles and zero-shot transfer. arXiv preprint
arXiv:2007.02527 (2020)
47. Scholz, J.P., Scho¨ner, G.: The uncontrolled manifold concept: identifying control
variablesforafunctionaltask.Experimentalbrainresearch126(3),289–306(1999)
48. Schulkin, J., Sterling, P.: Allostasis: a brain-centered, predictive mode of physio-
logical regulation. Trends in neurosciences 42(10), 740–752 (2019)
49. Sennesh,E.,Theriault,J.,Brooks,D.,vandeMeent,J.W.,Barrett,L.F.,Quigley,
K.S.: Interoception as modeling, allostasis as control. Biological Psychology 167,
108242 (2021)
50. Shadmehr, R., Ahmed, A.A.: Vigor: Neuroeconomics of movement control. MIT
Press (2020)
51. Shankar, K.H., Howard, M.W.: A scale-invariant internal representation of time.
Neural Computation 24(1), 134–193 (2012)
52. Smith, R., Ramstead, M.J., Kiefer, A.: Active inference models do not contra-
dictfolkpsychology.Synthese200(2)(2022).https://doi.org/10.1007/s11229-022-
03480-w, https://doi.org/10.1007/s11229-022-03480-w
53. Stephens,D.W.,Krebs,J.R.:Foragingtheory.In:Foragingtheory.Princetonuni-
versity press (2019)
54. Sterling, P.: Allostasis: a model of predictive regulation. Physiology & behavior
106(1), 5–15 (2012)
55. Tadepalli, P., Ok, D.K.: Model-based average reward reinforcement learning.
Artificial Intelligence 100(1–2), 177–224 (1998). https://doi.org/10.1016/s0004-
3702(98)00002-2
56. Tang, Y., Kucukelbir, A.: Hindsight Expectation Maximization for Goal-
conditioned Reinforcement Learning. In: Proceedings of the 24th International
Conference on Artificial Intelligence and Statistics (AISTATS). vol. 130 (2021),
http://arxiv.org/abs/2006.07549
57. Theodorou, E.: Relative entropy and free energy dualities: Connections to path
integral and kl control. 2012 IEEE 51st IEEE Conference p. 1466–1473 (2012)
58. Thijssen, S., Kappen, H.J.: Path integral control and state-dependent feedback.
Physical Review E - Statistical, Nonlinear, and Soft Matter Physics 91(3), 1–7
(2015). https://doi.org/10.1103/PhysRevE.91.032104
59. Todorov,E.:Efficientcomputationofoptimalactions.ProceedingsoftheNational
AcademyofSciencesoftheUnitedStatesofAmerica106(28),11478–11483(2009).
https://doi.org/10.1073/pnas.0710743106
60. Tschantz, A., Barca, L., Maisto, D., Buckley, C.L., Seth, A.K., Pezzulo,
G.: Simulating homeostatic, allostatic and goal-directed forms of intero-
ceptive control using active inference. Biological Psychology 169, 108266
(2022).https://doi.org/https://doi.org/10.1016/j.biopsycho.2022.108266,https://
www.sciencedirect.com/science/article/pii/S0301051122000084
Deriving time-averaged active inference from control principles 17
61. Zhang, Y., Ross, K.W.: On-policy deep reinforcement learning for the average-
rewardcriterion.In:Proceedingsofthe38thInternationalConferenceonMachine
Learning. p. 11 (2021)

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Deriving time-averaged active inference from control principles"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
