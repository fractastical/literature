=== IMPORTANT: ISOLATE THIS PAPER ===
You are revising a summary for ONLY the paper below. Do NOT reference or use content from any other papers.
Paper Title: Predictive Minds: LLMs As Atypical Active Inference Agents
Citation Key: kulveit2023predictive
REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Issues to fix:
1. CRITICAL: The current summary has severe repetition issues. You MUST eliminate all repeated sentences, phrases, and paragraphs. Each idea should be expressed only once. If you find yourself repeating content, remove the duplicates entirely. Focus on variety and uniqueness in your wording.
2. Severe repetition detected: Same phrase appears 6 times (severe repetition)

Current draft (first 2000 chars):
### Predictive Minds: LLMs As Atypical Active Inference Agents – Summary

### OverviewThis summary synthesizes the key arguments and findings presented in “Predictive Minds: LLMs As Atypical Active Inference Agents” by Jan Kulveit, Clem von Stengel, and Roman Leventov. The paper investigates the conventional conceptualization of Large Language Models (LLMs) like GPT as passive predictors, simulators, or even “stochastic parrots.” Instead, the authors propose a framework where LLMs operate as atypical active inference agents, aligning with the theory originating in cognitive science and neuroscience. They examine similarities and differences between traditional active inference systems and LLMs, concluding that, currently, LLMs lack a tight feedback loop between acting in the world and perceiving the impact of their actions, but otherwise fit within the active inference paradigm. The paper lists reasons why this loop may soon be closed, and possible consequences of this including enhanced model self-awareness and the drive to minimize prediction error by changing the world.### MethodologyThe authors’ approach centers on comparing generative models like LLMs with those studied in active inference. They emphasize that both generative AI and active inference systems share a core objective: to minimize the difference between predicted and actual sensory inputs (or, equivalently, variational free energy). The paper explores the parallels and distinctions between generative models and active inference systems, shedding light on the emergent control loops that might arise and the incentives driving these changes. The study draws on insights from cognitive science and neuroscience to provide a theoretical framework for understanding LLMs.### Results & Key ClaimsThe core claim of the paper is that LLMs, while currently lacking a fully closed feedback loop between action and perception, can be understood as atypical active inference agents. The authors state: “LLMs areinherent...

Key terms: minds, world, agents, objective, neuroscience, inference, active, predictive

=== FULL PAPER TEXT ===
3202
voN
61
]LC.sc[
1v51201.1132:viXra
Predictive Minds: LLMs As Atypical Active Inference
Agents
JanKulveit1∗ ClemvonStengel1 RomanLeventov2
1AlignmentofComplexSystemsResearchGroup,CenterforTheoreticalStudy,CharlesUniversity
2GaiaConsortium
Abstract
Largelanguagemodels(LLMs)likeGPTareoftenconceptualizedaspassivepre-
dictors,simulators,oreven’stochasticparrots’. We insteadconceptualizeLLMs
bydrawingonthetheoryofactiveinferenceoriginatingincognitivescienceand
neuroscience. Weexaminesimilaritiesanddifferencesbetweentraditionalactive
inferencesystemsandLLMs,leadingtotheconclusionthat,currently,LLMslack
a tightfeedbackloopbetweenactingin the worldand perceivingthe impactsof
their actions, but otherwise fit in the active inferenceparadigm. We list reasons
whythisloopmaysoonbeclosed,andpossibleconsequencesofthisincludingen-
hancedmodelself-awarenessandthedrivetominimizepredictionerrorbychang-
ingtheworld.
1 Introduction
Foundationmodels,particularlylargelanguageModels(LLMs)likeGPT[3],standoutasthemost
advancedgeneralAI systems to date [4]. LLMs are oftenperceivedas mere predictors, primarily
due to their training objective minimizing their loss on next-token prediction [1]. This objective
hasledtotheassumptionthatthesemodelsareinherentlypassive: designedtoawaitpromptsand
respond without any real understanding of the world or implicit intention to influence or interact
with the world. The theory of active inference, originatingin cognitivescience and neuroscience,
offersanalternativeviewpoint[25]. Activeinferencepositsthatbiologicalsystemslikethehuman
brain constantly update their internalmodels based on interactionswith the environment,striving
to minimize the difference between predicted and actual sensory inputs (a process also known as
predictiveprocessing)[25]. A fundamentaltenetofactive inferenceis that, in biologicalsystems,
thissameobjectivealsogovernsaction:thesystemminimizesthedifferencebetweenpredictedand
actualsensoryinputbyactivelyalteringitsenvironment.
ThispaperexplorestheintriguingpossibilitythatLLMs,whilepredominantlyseenaspassiveenti-
ties,mightconvergeuponactiveinferenceagentsclosertobiologicalones. Weexploretheparallels
and distinctions between generative models like LLMs and those studied in active inference, and
shedlightontheemergentcontrolloopsthatmightarise,theincentivesdrivingthesechanges,and
thesignificantsocietalramificationsofsuchashift.
∗jk@acsresearch.org
SociallyResponsibleLanguageModellingResearch(SoLaR)Workshopat37thConferenceonNeuralInfor-
mationProcessingSystems(NeurIPS2023).
2 Background and related work
2.1 ConceptualizingLLMs
There have been variousattempts to conceptualize LLMs, explain "how they actually work", and
understandthemusingexistingframeworksfromavarietyoffields.
One class of conceptualization focuses on the fact that the LM training objective is to minimize
predictiveloss,andthefactLLMsarenotembodiedinawaycomparabletohumans,buttrainedon
largedatasetsoftextfromtheinternet. Benderetal. coinedtheterm’stochasticparrots’andclaim
that text generated by an LM is not grounded in communicative intent, any model of the world,
or any modelof the reader’s state of mind [1]. In a similar spirit, using framing from linguistics,
Mahowaldetal. conceptualizeLLMsasmodelsthataregoodatformallinguisticcompetencebut
incomplete at functional linguistic competence. According to this view, LLMs are good models
of language but incomplete models of human thought, good at generating coherent, grammatical,
andseeminglymeaningfulparagraphsoftext,butfailinginfunctionalcompetence,whichrecruits
multiple extralinguistic capacities that comprise human thought, such as formal reasoning, world
knowledge,situationmodeling,andsocialcognition[16].
These reductionistviews of LLMs were subject to considerablecriticism. Mitchell and Krakauer,
surveyingthe debate, note an opposingfaction which arguesthat these networkstruly understand
language,canperformreasoninginageneralway,andinarealsenseunderstandconceptsandcap-
tureimportantaspectsofmeaning[21]. MitchellandKrakauer’soverallconclusionisthatcognitive
scienceiscurrentlyinadequateforansweringsuchquestionsaboutLLMs.
Other conceptualizations of LLMs recognize that the trained model is a distinct object from the
trainingprocess,andsothatthenatureofthetrainingobjectiveneednotbesharedbytheresulting
artifact.Forexample,basedonexperimentswithLLMsautoregressivelycompletingcomplextoken
sequences, Mirchandani et al. look at LLMs as general pattern machines, or general sequence
modellers,drivenbyin-contextlearning[20]. Othersextendthe’generalsequencemodeling’inthe
direction of ’general computation’. For example, Guo et al. propose using natural language as a
newprogramminglanguagetodescribetaskprocedures,makingthemeasilyunderstandabletoboth
humansand LLMs; they note that LLMs are capable of directly generatingand executingnatural
languageprograms.Inthisconceptualization,trainedLLMsarenatural-languagecomputers[10].
Another conceptualizationof LLMs, originating in the AI alignment community, views LLMs as
generalsimulators-simulatingalearneddistributionwithvariousdegreesoffidelity,whichinthe
caseoflanguagemodelstrainedonalargecorpusoftext,isthemechanicsunderlyingthegenesis
of the text, and so indirectlythe world [12]. This view explicitlyassumes that LLMslearn world
models, abstractions, algorithmsto better modelsequences. Similarly, Hubingeret. al. discusses
howtounderstandLLMsaspredictivemodels,andpotentialrisksfromsuchsystems[11].
While not directly aimed at explaininghow LLMswork, Lee et al. provideimportantcontextfor
thiswork, focusingonevaluatingLLMsin interactivesettings, andcriticizingthe factthatalmost
allbenchmarksimposethenon-interactiveview,ofmodelsaspassivepredictors[13].
2.2 Activeinferenceandpredictiveprocessing
Originatingincognitivescienceandneuroscience,activeinferenceoffersafreshlensthroughwhich
toviewcognitiveprocesses. Atitscore,thetheorysuggeststhatlivingsystems,suchasanimalsor
humanbrains,areinaconstantstateofupdatingtheirinternalmodelswhileactingontheenviron-
ment,andbothprocessesshouldbeunderstoodasminimizingthedifferencebetweenpredictedand
actualsensoryinputs(or,alternatively,variationalfreeenergy)[25].
As an all-encompassing framework for building theories of cognitive systems, active inference
should be compatible not only with process theories of brain function based on neurons [8], but
alsowitharangeofothercomputationalstructures(usedtorepresenttheworldmodel),andarange
ofoptimizationprocedures(usedto minimizethe differencebetweenpredictedandactualsensory
inputs). This makes active inference applicable - at least in principle - not only to humans and
animals,buttoaverybroadrangeofsystems,includingtheartificial.
ThisnaturallyleadstoourattempttounderstandLLMsusingtheactiveinferenceframework. Pez-
zuloetal. compareactiveinferencesystemsand"generativeAIs"andclaimthatwhilebothgener-
2
ativeAI andactiveinferenceare basedongenerativemodels, theyacquireanduse themin funda-
mentallydifferentways. Livingorganismsandactiveinferenceagentslearntheirgenerativemodels
by engaging in purposive interactions with the environment and by predicting these interactions.
Thekeydifferenceisthatlearningandmeaningisgroundedinsensorimotorexperience,providing
biologicalagentswith acoreunderstandingandasense ofmatteringuponwhichtheirsubsequent
knowledgeand decisionsare grounded[27]. In the presentwork, we arguethatthis distinction is
notnecessarilyasfundamentalasassumedbyPezzuloetal.,andmaymostlydisappearinthenear
futurewithtighterfeedbackloopbetweenactionsandobservations.
3 Similaritiesand differences between activeinference systemsand LLMs
IfwelookatLLMsinthesimulatorsframeworkandtheactiveinferenceframework,wecannotea
numberofsimilarities–orevencaseswheretheAIcommunityandtheactiveinferencecommunity
describe the same phenomena using different terminology. In both cases, systems are described
as equipped with a generative model able to simulate the system’s sensory inputs. This model
is updated in such a way that minimises prediction error - the difference between observed and
simulatedinputs. ThisprocesshasbeenshowntobeaformofapproximateBayesianinferencein
boththeactiveinference[25,9]andLLM[19,30]literatures.
3.1 PredictionsbasedonconceptualizingLLMsasspecialcaseofactiveinferencesystems
Theactiveinferenceconceptualizationleadstoanumberofpredictions,someofwhicharepossible
toverifyexperimentallyusinginteroperabilitytechniques.
Possibly the most striking one is obviousin hindsight: active inferencepostulates that the simple
objective of minimizing prediction error is sufficient for learning complex world representations,
behavioursand abstraction power, given a learning system with sufficient representation capacity.
Inpredictiveprocessingterminology,wecanmakeananalogybetween"perception"andthetrain-
ingprocessofLLMs:LLMsarefedtextsfromtheinternetandbuildgenerativemodelsoftheinput.
Becauselanguageisareflectionoftheworld,thesemodelsnecessarilyimplicitlymodelnotonlylan-
guage,butalsothebroaderworld. Therefore,weshouldexpectLLMstoalsolearncomplexworld
representations,abstractions,andtheabilitytosimulateothersystems,(givensufficientrepresenta-
tion capacity). This is in contrastto the conceptualizationsreferencedin section 2.1, which often
predictthatsystemstrainedtopredictnextinputarefundamentallylimited,neverabletogeneralize,
unabletocomprehendmeaning,etc. Recentresearchhasprovidedsubstantialevidencesupporting
themoreoptimisticviewthatlargelanguagemodels(LLMs)areanalogoustobiologicalsystemsat
least in their ability to developan emergentworld model[15], rich abstractionsand the ability to
predictgeneralsequences[20].
Anothertopiceasiertounderstandthroughanactiveinferencelensarehallucinations:whereLLMs
produce false or misleading information and present it as fact [17]. Active inference claims that
humanperceptionisitself ’constrainedhallucination’[24], whereourpredictionsaboutsensoryin-
putsareconstantlysynchronizedwithrealitythroughtheerrorsignal,propagatedbackwards.Inthis
perspective,thedataonwhichLLMsaretrainedcouldbeunderstoodassensoryinput.What’sstrik-
ingabouttheseinputsis,incontrasttohumansensoryinputs,thedataarenotbasedonperceiving
realityfromonespecificperspectiveinonepointoftime. Quitetheopposite:foranintuitiveunder-
standing of the nature of the data LLMs are trained on, imagine that yourown sensory inputwas
exhaustedbyoverhearinghumanconversations,withthecaveatthatwhatyouheareveryfewmin-
utesrandomlyswitchesbetweenconversationstakingplaceoutoforderindifferentyears,contexts
andspeakers.Incontrasttothetypicalhumansituation-tryingtopredictwhatyouwouldhearnext-
youwouldoftenneedtoentertainmanydifferenthypothesesaboutthecurrentcontext.Forexample,
considerhearingsomeonesay"Andshedrewherswordandexclaimed’Hereticsmustdie!’".When
attemptingto predictthe continuation,itseems necessaryto entertainmanypossibilities- such as
thecontextbeingarealisticdescriptionofsomemedievalworld,orafantasytale,orsomeoneplay-
ing a video-game. If a biological, brain-basedactive inferencesystem was tasked with predicting
suchcontextlesswords,thenvariousfantasyandcounterfactualworldswouldseemasrealasactual
currentaffairs. Inthisconceptualization,somehallucinationsinLLMsarenotsomesortofsurpris-
ing failure mode of AI systems, but what you should expect from a system tasked to predict text
withminimalcontext,notanchoredtosomespecifictemporalorcontextualvantagepoint. Another
3
strikingfeatureofLLMsindeploymentisthatoutputsofthegenerativemodelarenotdistinguished
frominputs: themodel’soutputbecomespartofitsown’sensory’state. Intuitively,thiswouldbe
similartoahumanunabletodistinguishbetweentheirownactionsandexternalinfluences-which
actuallysometimesmanifestsasthepsychiatricconditionknownas’delusionofcontrol’[6].
ThisframesuggestsdirectionstomakeLLMslesspronetohallucinations:makethelearningcontext
of the LLM more situated and contextually stable (that is, present training documents in a more
systematic fashion). Additionally, it could help to distinguish between completionsby the model
andinputsfromtheuser,similartotheapproachofOrtegaetal. [22].
3.2 WhatisanLLM’sactuator?
OnesuggestedfundamentaldifferencebetweenLLMsandactiveinferencesystemsistheinherent
passivityofLLMs-theirinabilitytoactintheworld[27]. Wearguethatthisismostlyamatterof
degreeandnotacategoricaldifference.WhileLLMsdon’thaveactuatorsinthephysicalworldlike
humansor robots, they still havethe ability to act, in the sense that their predictionsdo affectthe
world. In activeinferenceterminology,LLMoutputscouldbe understoodasthe ’actionstates’ in
theMarkovblanket. Thesestateshavesomeeffectontheworldviamultiplecausalpathways,and
theresultingchangescaninprincipleinfluenceits’sensorystates’-thatis,variouspiecesoftexton
theinternetandincludedinthetrainingset. Someclearpathways:
1. DirectinclusionoftextgeneratedbyLLMinwebpages.
2. HumanusersaskingLLMbasedassistantsforplansandexecutingthoseplansintheworld.
3. Text inputfor a huge rangeof other software systems (LLMs as glue code and so-called
"roboticprocessautomation").
4. Indirectinfluenceonhow humansthinkaboutthings, e.g. learningabouta conceptfrom
anLLMbasedassistance.
SomeoftheseeffectsarealreadystudiedintheMLliterature,butmostlyinthecontextoffeedback
loops amplifyingbias [29] or as an exampleof performativeprediction [26]. Here, we proposea
broaderinterpretation:understandingtheseeffectsasactionsinthesenseittakesinactiveinference.
ThenatureofthemediumthroughwhichLLMs"perceive"and"act"ontheworld,whichismostly
text, shouldnotobscurethe fundamentalsimilarity to activeinferenceagents. We agreewith Mc-
Gregor’sargument[18] thatwe shouldexplicitlydistinguishbetweentwo notionsofembodiment:
ontheonehand,whetherasystem’sbodyistangibleornot,andontheotherhand,whetherasystem
isphysicallysituatedornot(i.e. whetherornotitinteractsphysicallywithanypartoftheuniverse).
LLMsareembodiedinthissecondsense. Inthisview,interactionsofLLMswithusersindeploy-
mentareessentially ’actions’. Everytokengeneratedinconversationwith usersis amicro-action,
andthesumofalloftheseactionsdoinfluencetheworld,andsomeofthesechangesgetreflectedin
theinputworld(publictextsontheinternet). So,atleastinprinciple,LLMshaveoneopencausal
pathtobringtheworldofwordsclosertotheirpredictions.
3.3 Closingtheactionloopofactiveinference
Giventhatthe"notactingontheworld"assumptionof"LLMsaspassivesimulators"doesnothold,
the main currentdifference between LLMs and active inferencesystems is that LLMs mostly are
not yet able to "perceive" the impacts of their actions. In other words, the loop between actions,
externalworldstates,andperceptionsisnotclosed(oranywayisnotfast). Whilelivingorganisms
constantlyrunbothperceptionandactionloops,trainingnewgenerationsofanLLMhappensonly
onceayearorso-andtheimpactsofactionsoftheLLMcurrentlymostlydonotfeedbackintothe
newbasemodel’straining.
WhatwouldneedtobechangedforLLMstoperceivetheresultsoftheirownactions,andthusclose
the“gap”betweenactionandperception? ThekeypieceisthattheactionstakenbyanLLMafter
deployment, in the sense discussed in section 3.2, feed back into the training process of a future
LLM.Furthermore,itisrequiredthatsuccessiveLLMsaresufficientlysimilar,andhavesufficient
representationalcapacity,suchthattheycan“self-identify”withsuccessivetrainingiterations(see
[14]foradiscussionof“theGPTlineage”asanagent).
4
Aminimalversionofthiscanoccurwithin-contextlearning[5],real-timeaccesstowebsearch(as
with Bing Chat and GoogleBard), or a trainingenvironmentin which the modelcan take actions
whichinfluenceitsreward(suchaswithGATO[28],orRLHF[23]).Howeverineachofthesecases,
thereisnofeedbackfromtheactionstakenduringdeploymentandsubsequenttrainingoftheLLM.
Therearethreewaysweforeseethishappeninginthenearfuture:
1. The outputs of a model are used to train a next generation model, e.g. through model
outputsbeingpublishedontheinternetandnotfilteredoutduringdatacuration.
2. Thedatacollectedfrominteractionswiththemodels,suchasfromuserconversationswith
achatbot,areusedinfine-tuningfutureversionsofthesamemodel.
3. Continuousonlinelearning,inwhichtheoutputsofamodelanduserresponsesaredirectly
usedasatrainingsignaltoupdatethemodel.
Where these routes are in order of increasingly tight feedback loops (where "tighter" means on a
shorter timescale, with consecutive generations sharing more of the earlier model’s weights, and
withtheinteractionformingalargerpercentageoftrainingdata-increasedbandwidth).
Weexpectthattherewillbeactiveeffortbydeveloperstoclosethefeedbackgapandmaketheaction
loop more prominentbecause of commercialincentivesto make LLMs better at quickly adapting
to newinformation,actingindependently,orotherwiseagent-like. Activeinferenceasa theoryof
agencypredictsclosingtheloopwouldnaturallycauseLLMstobecomemoreagentic,emergently
learning to change the world to more closely match the internal states (and thus predictions) of
LLMs.
4 Implications ofactiveLLMs
The evolution of LLMs into active agents would carry profound societal implications and risks.
UsingactiveinferenceasatheoreticalframeworktomakepredictionsaboutsuchActiveLLMsisa
fruitfuldirection.Wefocusonemergenceofincreasedself-awareness.
4.1 Enhancingmodelself-awareness
Astraightforwardpredictionoftheactiveinferenceframeinthispaperisthatthedescribedtighten-
ingofthefeedbackloopislikelytotoaugmentandincreasemodels’self-awareness.Arecentstudy
ofself-awareness[2]inLLMsemphasizestheimportanceofself-awarenessfromasafetyperspec-
tive,butthisworkisoveralluncertainaboutwhatstageofLLMtrainingwillbemoreimportantfor
the emergence of situational awareness in future models, and focuses on evaluating sophisticated
out-of-contextreasoning as a proxy of self-awareness. In contrast, the active inference literature
emphasizestheimportanceofobservingtheconsequencesofone’sownactionsfordevelopingfunc-
tionalself-awareness[7,p.112].
Astheseloopstighten,weexpectmodelstoenhanceinself-awarenessbyacquiringmoreinforma-
tionaboutthemselvesandobservingtherepercussionsoftheiractionsintheenvironment.Consider
the self-localizationproblemdiscussed by [2]. Constructa thoughtexperimentin which a human
faces a similar self-localization problem: assume, instead of one’s usual sensory inputs, that the
human is hooked to a stream of dozens of security cameras. To increase the human’s ability to
self-localizeistoequipthemwithmoreinformationabouttheirownappearance,forexample,hair
colour.Adifferent,highlyeffectivewaytoself-localizeisviaperforminganaction,forexampleby
wavingahand.
5 Conclusions
Byexaminingthelearningobjectivesandfeedbackloopsofactiveinference,incomparisontothose
ofLLMs,wepositedthatLLMscanbeunderstoodasanunusualexampleofactiveinferenceagents
with a gapin their feedbackloopfrom action to perception. In this framework, their transitionto
actingintheworldaslivingorganismsdodependsontheirclosingthegapbetweeninteracting(with
users)andtraining.
5
ThepotentialmetamorphosisofLLMsintoactiveLLMscouldleadtomoreadaptiveandself-aware
AIsystems,bearingsubstantialsocietalimplications.Thedensificationandaccelerationoffeedback
loopscouldaugmentnotonlymodels’self-awarenessbutalsoleadtoadrivetomodifytheworld-
drivenpurelybythepredictionerrorminimizationobjective,withoutintentionalefforttomakethe
modelsmoreagent-like.
6 Acknowledgements
WethankRoseHadsharandGavinLeechforhelpwithwritingandediting,andTomášGavencˇiak,
SimonMcGregorandNicholasKeesDupuisforvaluablediscussions. JKandCvSweresupported
byPRIMUSgrantfromCharlesUniversity.GPT4wasusedforeditingthedraft,simulatingreaders,
andtitlesuggestions.
References
[1] EmilyMBender,TimnitGebru,AngelinaMcMillan-Major,andShmargaretShmitchell. On
thedangersofstochasticparrots:Canlanguagemodelsbetoobig? InProceedingsofthe2021
ACMconferenceonfairness,accountability,andtransparency,pages610–623,2021.
[2] LukasBerglund,AsaCooperStickland,MikitaBalesni, MaxKaufmann,MegTong,Tomasz
Korbak,DanielKokotajlo,andOwainEvans. Takenoutofcontext: Onmeasuringsituational
awarenessinllms. arXivpreprintarXiv:2309.00667,2023.
[3] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhari-
wal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemod-
elsarefew-shotlearners. Advancesinneuralinformationprocessingsystems,33:1877–1901,
2020.
[4] SébastienBubeck,VarunChandrasekaran,RonenEldan,JohannesGehrke,EricHorvitz,Ece
Kamar,PeterLee,YinTatLee,YuanzhiLi,ScottLundberg,etal. Sparksofartificialgeneral
intelligence:Earlyexperimentswithgpt-4. arXivpreprintarXiv:2303.12712,2023.
[5] DamaiDai,YutaoSun,LiDong,YaruHao,ShumingMa,ZhifangSui,andFuruWei.Whycan
gptlearnin-context?languagemodelsimplicitlyperformgradientdescentasmeta-optimizers.
arXivpreprintarXiv:2212.10559,2022.
[6] PaulCFletcherandChrisDFrith. Perceivingisbelieving: abayesianapproachtoexplaining
thepositivesymptomsofschizophrenia. NatureReviewsNeuroscience,10(1):48–58,2009.
[7] KarlFriston.Afreeenergyprincipleforaparticularphysics.arXivpreprintarXiv:1906.10184,
2019.
[8] KarlFriston,ThomasFitzGerald,FrancescoRigoli,PhilippSchwartenbeck,andGiovanniPez-
zulo. Activeinference:aprocesstheory. Neuralcomputation,29(1):1–49,2017.
[9] Karl Friston, Philipp Schwartenbeck, Thomas FitzGerald, Michael Moutoussis, Timothy
Behrens,andRaymondJ.Dolan. Theanatomyofchoice: activeinferenceandagency. Fron-
tiersinHumanNeuroscience,7,2013.
[10] YiduoGuo,YaoboLiang,ChenfeiWu,WenshanWu,DongyanZhao,andNanDuan.Learning
toprogramwithnaturallanguage. arXivpreprintarXiv:2304.10464,2023.
[11] EvanHubinger,AdamJermyn,JohannesTreutlein,RubiHudson,andKateWoolverton. Con-
ditioningpredictivemodels:Risksandstrategies. arXivpreprintarXiv:2302.00805,2023.
[12] Janus. Simulators,2023. https://generative.ink/posts/simulators/Accessed:2023-10-04.
[13] Mina Lee, Megha Srivastava, Amelia Hardy, John Thickstun, Esin Durmus, Ashwin Paran-
jape,InesGerard-Ursin,XiangLisaLi,FaisalLadhak,FriedaRong,etal. Evaluatinghuman-
languagemodelinteraction. arXivpreprintarXiv:2212.09746,2022.
[14] Roman Leventov. How evolutionary lineages of llms can plan their own future and
act on these plans, 2023. https://www.lesswrong.com/posts/ddR8dExcEFJKJtWvR/how-
evolutionary-lineages-of-llms-can-plan-their-own-futureAccessed:2023-10-04.
[15] Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Viégas, Hanspeter Pfister, and Martin
Wattenberg. Emergentworld representations: Exploringa sequence modeltrained on a syn-
thetictask. arXivpreprintarXiv:2210.13382,2022.
6
[16] KyleMahowald,AnnaAIvanova,IdanABlank,NancyKanwisher,JoshuaBTenenbaum,and
EvelinaFedorenko. Dissociatinglanguageandthoughtinlargelanguagemodels: acognitive
perspective. arXivpreprintarXiv:2301.06627,2023.
[17] Potsawee Manakul, Adian Liusie, and Mark J. F. Gales. Selfcheckgpt: Zero-resource
black-box hallucination detection for generative large language models. arXiv preprint
arXiv:2303.08896,2023.
[18] Simon McGregor. Is chatgpt really disembodied? In ALIFE 2023: Ghost in the Machine:
Proceedingsofthe2023ArtificialLifeConference.MITPress,2023.
[19] Chris Mingard, Guillermo Valle-Pérez, Joar Skalse, and Ard A. Louis. Is sgd a bayesian
sampler?well,almost. JournalofMachineLearningResearch,22(79):1–64,2021.
[20] SuvirMirchandani,FeiXia,PeteFlorence,BrianIchter,DannyDriess,MontserratGonzalez
Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. Large language models as general
patternmachines. arXivpreprintarXiv:2307.04721,2023.
[21] MelanieMitchellandDavidCKrakauer. Thedebateoverunderstandinginai’slargelanguage
models. ProceedingsoftheNationalAcademyofSciences,120(13):e2215907120,2023.
[22] PedroAOrtega,MarkusKunesch,GrégoireDelétang,TimGenewein,JordiGrau-Moya,Joel
Veness,JonasBuchli,JonasDegrave,BilalPiot,JulienPerolat,etal. Shakingthefoundations:
delusions in sequence models for interaction and control. arXiv preprint arXiv:2110.10819,
2021.
[23] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,
Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Chris-
tiano,JanLeike,andRyanLowe.Traininglanguagemodelstofollowinstructionswithhuman
feedback. InAdvancesinNeuralInformationProcessingSystems,volume35.NeurIPS,2022.
[24] ThomasParrandGiovanniPezzulo. Understanding,explanation,andactiveinference. Fron-
tiersinSystemsNeuroscience,15,2021.
[25] ThomasParr,GiovanniPezzulo,andKarlJFriston. Activeinference:thefreeenergyprinciple
inmind,brain,andbehavior. MITPress,2022.
[26] JuanPerdomo,TijanaZrnic,CelestineMendler-Dünner,andMoritzHardt. Performativepre-
diction. InInternationalConferenceonMachineLearning,pages7599–7609.PMLR,2020.
[27] GiovanniPezzulo,ThomasParr,PaulCisek,AndyClark,andKarlFriston. Generatingmean-
ing:Activeinferenceandthescopeandlimitsofpassiveai. 2023.
[28] ScottReed,KonradZolna,EmilioParisotto,SergioGómezColmenarejo,AlexanderNovikov,
GabrielBarth-maron,MaiGiménez,YurySulsky,JackieKay,JostTobiasSpringenberg,Tom
Eccles, Jake Bruce, Ali Razavi, AshleyEdwards, NicolasHeess, YutianChen, Raia Hadsell,
Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent. Transactions of
MachineLearningResearch,2022.
[29] Rohan Taoriand TatsunoriHashimoto. Data feedbackloops: Model-drivenamplification of
datasetbiases.InInternationalConferenceonMachineLearning,pages33883–33920.PMLR,
2023.
[30] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanationof in-
contextlearningasimplicitbayesianinference. arXivpreprintarXiv:2111.02080,2021.
7
This figure "actmod.png" is available in "png"(cid:10) format from:
http://arxiv.org/ps/2311.10215v1
This figure "gapmod.png" is available in "png"(cid:10) format from:
http://arxiv.org/ps/2311.10215v1

=== REVISE TO ===
PROFESSIONAL TONE: Begin directly with content - NO conversational openings like 'Okay, here's...'

1. Fix all issues above
2. Title: "Predictive Minds: LLMs As Atypical Active Inference Agents"
3. Include 10-15 quotes from paper text
   - Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
   - Use consistent quote formatting: 'The authors state: "quote"' or vary attribution phrases
   - Vary attribution phrases to avoid repetition
   - CRITICAL: Only extract quotes that actually appear in the paper text
4. ELIMINATE ALL REPETITION - each sentence must be unique
   - Check before each sentence: 'Have I already said this?' If yes, write something new
   - Vary attribution phrases - do NOT repeat 'The authors state' multiple times
5. Extract methodology, results with numbers, key quotes
6. 1000-1500 words, structured with ### headers

Generate COMPLETE revised summary.