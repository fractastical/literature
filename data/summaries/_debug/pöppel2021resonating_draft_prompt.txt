=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Resonating Minds -- Emergent Collaboration Through Hierarchical Active Inference
Citation Key: pöppel2021resonating
Authors: Jan Pöppel, Sebastian Kahl, Stefan Kopp

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2021

Abstract: Introduction Working together on complex collaborative tasks requires agents to coordinate their
actions. Doingthisexplicitlyorcompletelypriortotheactualinteractionisnotalwayspossiblenor
sufficient. Agents also need to continuously understand the current actions of others and quickly
adapt their own behavior appropriately. Here we investigate how efficient, automatic coordination
processesatthelevelofmentalstates(intentions, goals), whichwecallbeliefresonance, canlead
tocollaborativesituatedprob...

Key Terms: minds, bielefelduniversity, techfak, resonating, agents, bielefeld, emergent, actions, collaborative, hierarchical

=== FULL PAPER TEXT ===

RESONATING MINDS – EMERGENT COLLABORATION THROUGH
HIERARCHICAL ACTIVE INFERENCE
THISVERSIONOFTHEARTICLEHASBEENACCEPTEDFORPUBLICATIONINCOGNITIVECOMPUTATION,AFTER
PEERREVIEWBUTISNOTTHEVERSIONOFRECORDANDDOESNOTREFLECTPOST-ACCEPTANCE
IMPROVEMENTS,ORANYCORRECTIONS. THEVERSIONOFRECORDISAVAILABLEONLINEAT:
http://dx.doi.org/10.1007/s12559-021-09960-4
JanPo¨ppel SebastianKahl
SocialCognitiveSystems SocialCognitiveSystems
BielefeldUniversity BielefeldUniversity
jpoeppel@techfak.uni-bielefeld.de skahl@techfak.uni-bielefeld.de
StefanKopp
SocialCognitiveSystems
BielefeldUniversity
skopp@techfak.uni-bielefeld.de
December3,2021
ABSTRACT
Introduction Working together on complex collaborative tasks requires agents to coordinate their
actions. Doingthisexplicitlyorcompletelypriortotheactualinteractionisnotalwayspossiblenor
sufficient. Agents also need to continuously understand the current actions of others and quickly
adapt their own behavior appropriately. Here we investigate how efficient, automatic coordination
processesatthelevelofmentalstates(intentions, goals), whichwecallbeliefresonance, canlead
tocollaborativesituatedproblem-solving.
Method We present a model of hierarchical active inference for collaborative agents (HAICA). It
combines efficient Bayesian Theory of Mind processes with a perception-action system based on
predictiveprocessingandactiveinference. Beliefresonanceisrealizedbylettingtheinferredmen-
talstatesofoneagentinfluenceanotheragent’spredictivebeliefsaboutitsowngoalsandintentions.
Thisway,theinferredmentalstatesinfluencetheagent’sowntaskbehaviorwithoutexplicitcollab-
orativereasoning.
Results We implement and evaluate this model in the Overcooked domain, in which two agents
with varying degrees of belief resonance team up to fulfill meal orders. Our results demonstrate
thatagentsbasedonHAICAachieveateamperformancecomparabletorecentstateoftheartap-
proaches, while incurring much lower computational costs. We also show that belief resonance is
especiallybeneficialinsettingsweretheagentshaveasymmetricknowledgeabouttheenvironment.
ConclusionTheresultsindicatethatbeliefresonanceandactiveinferenceallowforquickandeffi-
cientagentcoordination,andthuscanserveasabuildingblockforcollaborativecognitiveagents.
Keywords Multi-AgentCollaboration,TheoryofMind,PredictiveProcessing,CognitiveModel
1 Introduction
Collaboration is a fundamental human skill. Many situations we are confronted with can only be mastered when
working together with others on a common goal and in a coordinated effort. For example, when preparing a meal
togetherinakitchen,agentsneedtocoordinatetodeterminesub-tasksforeachagentorforjointlyworkingonasub-
task when necessary. Often this requires explicit coordination prior to task execution, e.g., through jointly planning
1202
ceD
2
]AM.sc[
1v01210.2112:viXra
ResonatingMinds–EmergentCollaborationThroughHierarchicalActiveInference APREPRINT
andnegotiatingsub-tasks. However,suchapriorcoordinationisoftennotsufficientnorevenpossible,becausemost
taskstakeplaceinunpredictableenvironmentsorwithagentsthatmaybehaveinunexpectedways. Likewise,agents
maynotalwaysbeabletocoordinateexplicitly,e.g.,bycommunicating,duringtheinteraction. Consequently,agents
needtocoordinatetheiractionson-the-flyinordertocollaboratesuccessfully.
Wehumanshaveevolvedrichabilitiesforcoordinatingourcollaborativeactionson-the-fly. Insocialinteraction,we
managetoworktogetherevenwithoutpriorexperienceorfixedsocialroles[47,34]. Thisispossiblebecause,despite
alackofdirectaccesstootheragents’minds,fromayoungageweareabletoperceivetheactionsofotheragentsand
formhypothesesabouttheircurrentintentionalstate(actionintentions,plans,orgoals)orepistemic-attentionalstate
(beliefs,assumptions,perceptions)[22]. TheunderlyingabilityiscalledmentalizingorTheoryofMind (ToM)[38].
Ataperceptuallevel,weareabletorecognizeandpredictanotheragent’sactionsandmotorintentions[50]bywayof
resonanceprocessesthatinvolveourownmotorsysteminourperceptionofothers[43].
Obviously,successfulcoordinationofcollaborationdoesnotonlyrequirerecognizingwhatotheragentsarecurrently
doing,whichintentionstheymightactupon,andwhatthismayimplyfortheentirejointtask. Collaborativeagents
also need to adapt their own actions accordingly in order to ensure coordinated behavior. While this may require
explicitre-planningandevenre-negotiatingsub-tasksinsomecases(e.g. “Soyouaregettingtheplate? ThenIwill
get the cutlery.”), it may also happen implicitly and continuously, e.g., in a situated joint action such as carrying a
tabletogether. Howthesetwoprocesses(explicitandimplicitcoordination)interactandplayouttogetherisalargely
open question. The former is able to handle complex collaboration problems with rich representations of task and
partner knowledge, but the cognitive inference and decision processes are known to be uncertain, costly and often
even intractable [8]; the latter is efficient and robust in dynamic environments and assumed to rest on sensorimotor
processes,butitseemsrestrictedtocoordinativepatternsofsinglemotoractions. Finally,agentsmayevenhappento
collaboratewithoutanydeliberatecoordination,e.g.,whentheenvironmentrestrictsindividualactionsandinaway
thatbringsabouttheircoordination.
In this paper we ask if, and to what extent, successful collaboration between artificial agents on complex situated
tasks can emerge even from minimal, distributed on-the-fly coordination processes. Traditionally, collaboration in
artificialagentshasbeenapproachedasplanningproblemthatmustbesolvedeitherinacentralizedordecentralized
fashion[15].Enablingcomplexcollaborativetaskswithreal-timeon-the-flycoordinationbetweenautonomousagents,
however, is still a challenge and usually requires approximations or heuristic processes. One reason for this is that
bothgeneratingaccurateactionpredictionsforotheragentsusingafull-blownToMandoptimalplanningincomplex,
dynamicenvironmentsaretoocostlytobeperformedonline[5,2]. Simplificationsarethuscommonplace. Itisfor
this reason that, e.g., most artificial agents perform best in simulations in which the other agents are very similar to
themselves(asfoundin[11]).
We propose an inter-agent coordination mechanism that rests on and extends the active inference framework for
prediction-based perception, action, and decision-making [14, 1, 19]. This framework is based on embodied cog-
nitiveprocessingprinciples. Weadoptittomodelhowagentsactinginasituatedtaskenvironmentareinfluencedby
whatotheragentsaredoingandintendtodo, whileonlyimplicitlyreasoningabouttheagents’on-the-flycoordina-
tion. Similartorelatedwork,wedevelopandtestthismodelinasimulatedcollaborativetaskdomaininspiredbythe
Overcooked[21] computer game. In this domain, multiple agents need to collaborate in preparing as many ordered
mealsaspossibleinagivenkitchenenvironmentandwithinafixedamountoftime.
Withtheproposedmodel,weaimtomaketwocontributions. First,wepresentahierarchicalpredictiveprocessing-
based model for perception and action. This model is able to generate responsive, yet goal-directed behavior in an
efficientandrobustway. Crucially, itsolvestheplanningproblemwithoutanyexplicitreasoningaboutorrepresen-
tation of the task structure (e.g., how exactly a specific meal needs to be prepared). This is achieved by integrating
bottom-up information, e.g., about current affordances for actions in the environment, with top-down information
abouttheagent’sgoals. Theintegrationyieldstheagent’scurrentintentionsandinfluencesitsactionsandperception.
Second, weproposeanimplicitcoordinationstrategy, calledbeliefresonance, thatrestsonintegratingbeliefsabout
anotheragent’sintentionsorgoals,withbeliefsaboutone’sownactionsandintentions. Thedegreetowhichanother
agent’s intention influences one’s own beliefs is controlled by a so-called susceptibility parameter (SP). Using this
parameter, different roles (e.g., leader or follower) can be realized by controlling how susceptible agents are to the
mental states of another agent. In order to infer another’s mental state efficiently, we integrate the active inference
frameworkwithalight-weightmentalizingapproachbasedonamodelofBayesianTheoryofMind(BToM)[3].
Framed within this model, we want to investigate to what extent the combination of these basic, neuro-cognitively
inspired [18, 30] mechanisms facilitates the emergence of efficient, real-time collaborative behavior. Clearly, this
approachdoesnotaccountforplanningoptimalactionsordividinglaborintocomplementarysub-tasks,whichrequires
complexrepresentationsandplanningprocesses. Instead,weseektoexplorewhatkindofcollaborativebehaviorcan
emergethatisnotintherepertoireofindividualagents. Tobemoreexact, agentsarenotexplicitlyreasoningabout
2
ResonatingMinds–EmergentCollaborationThroughHierarchicalActiveInference APREPRINT
their collaborative task but are coupled through situated perception and action in a shared environment, along with
a minimal social-cognitive ability for inferring and adopting others’ intentions. The resulting implicit coordination
processesimposelowercomputationalrequirementsandarethusapplicableforreal-timeinteraction.
In the remainder of this paper we first discuss related work and theoretical backgrounds. We will then present a
hierarchical active inference for collaborative agents (HAICA) model for agents engaged in situated collaborative
tasks.Asafoundation,wewillinitiallyfocusondescribingthecomputationalmodelforbeliefupdatesacrossdifferent
layers,andthenturntotheimplementationofbeliefresonanceinthemodel. Afterwards,wepresentthecollaborative
cooking task domain and describe how the model is applied to it. This also includes an explanation of an efficient
ToMapproachthatweproposetouseinHAICA.Furthermore,wepresentsimulationstudieswithtwoagentsacting
indifferentenvironmentsintheOvercookeddomain. Wereportresultsfromevaluatingthemodelwhilevaryingthe
SPvaluesforeachofthetwoagentsinordertoevaluatetheeffectsofvaryingdegreesofbeliefresonanceonimplicit
coordination. Wealsoinvestigatetheeffectivenessofbeliefresonanceinasymmetricscenarioswhereoneagenthas
more task knowledge than the other. Finally, we compare our results with current state of the art models both from
reinforcementlearning(RL)aswellasprobabilisticBayesianmodelsbasedoninverseplanning.
2 RelatedWorkandBackground
Developing models and systems for multi-agent collaboration has received increasing attention in fields such as au-
tonomoussystems,human-robotinteraction,multi-agentsystems,aswellasPsychologyandCognitiveScience.Work
inComputerScienceandA.I.hasfocusedmostlyonformalizingthecoordinationproblemandfinding“optimal”so-
lutionsthroughmulti-agentplanning[48]. Usually,agentsareassumedtocoordinateinadvance,eitherexplicitlyor
implicitlybysharingcommoncommunicationprotocolsand/orplanningalgorithms. Thestrongestformofsuchprior
coordinationwouldinvolveacentralizedcontrolinchargeoffindinganoptimalplanforthewholeteam[16].
Others have looked at multi-agent scenarios that do not allow for a full prior coordination or centralized control,
butinsteadrequireagentstoadapttoeachotherindividuallyanddynamically, andevenestablishaformof“ad-hoc
teamwork”[6,45,12].Thisproblemofon-the-flycoordinationhasrecentlybecomeprominentincollaborativerobots
andservicerobots[35]andhasbeenapproachedmostlyforattentionandmotor-levelcoordinationofphysicaltasks
between humans and robots, e.g., when handing over objects or jointly carrying them [35]. The underlying models
ofon-the-flycoordinationemphasizetheroleoftheagents’co-presenceinasharedenvironmentandtheircoupling
throughperception-actionorforce-feedbackloops.
Onarelatednote,researchinPsychologyhasexploredhowhumanscoordinate(bothspatiallyandtemporally)during
joint action by means of perceiving, simulating and predicting others [50, 36]. This work has provided substantial
evidence that people rely on different mechanisms to coordinate their joint actions. These mechanisms do not only
involveexplicitplanningbutalsolower-levelsensorimotorcoordinationthatyieldsanemergenceofcoordinatedbe-
havior[31].
A number of computational approaches to model key abilities for collaborating with others have been proposed.
Examplesincludejointattention[24],goalrecognition[49,32],onlineplanning[52],orcollaborativediscourse[42].
Most importantly for our work, significant advancements have been made in developing computational models of
interpreting an action of another agent in terms of the intentions, beliefs, or emotional states that may have caused
saidaction. Thisso-calledmind-reading,orTheoryofMind(ToM),requirestheobservertohaveagoodmodelofthe
actingagent. Commonlyoneassumesanaiverationalagentthatselectsactionswhichmaximizeasubjectiveutility
[5,25]. EarlierworkdefinedsuchmodelsintermsoftheBDIframework[9],whilelaterworkshiftedtoprobabilistic
models[39]orBayesianmodelsbasedoninverseplanning[5,4,26]. InthecontextofToM,theseinverseplanning
modelshavealsobeentermedBayesianTheoryofMind(BToM)[3]. Suchmodelshavebeenabletofitempiricaldata
andthushavegreatconceptualandtheoreticalmerits,astheyoffercomputationalaccountsofhowhumansreasonand
actinthesocialdomain.However,thestandardBToMframeworkrequiresacompletegenerativemodelforallactions,
whichiscostlytoacquire,formulate,andcompute[8]. FullBToMisthustoocomplexforon-the-flycoordinationin
real-timeinteraction(seeSect.5forruntimemeasurements)andcanonlybeappliedinapproximatedschemes[37].
Recently,workhasalsostartedtoexploretheuseofdeeplearningtorecognizedifferentkindsofsyntheticagentsand
topredicttheirfuturebehavior[40].
Only a few approaches have started to look at how artificial agents can be endowed with capabilities such as ToM
to enableor improvecontinuous multi-agentcollaboration. Clearly, implementations arealways highlyspecific and
attunedtotheparticularscenariotheyaimtomodel. Comparabilityofapproachesthusisamajorconcern. Onemulti-
agent collaboration scenario that has been adopted lately in various studies is from the Overcooked computer game
[21],inwhichmultipleagentsneedtoworktogethertoprepareorderedmealsinaspecifickitchenlayoutinalimited
3
ResonatingMinds–EmergentCollaborationThroughHierarchicalActiveInference APREPRINT
amount of time. In this task, agents need to coordinate on-the-fly on high-level sub-tasks in order to improve team
performance.
The Overcooked domain has been previously tackled using Deep Reinforcement Learning to train a collaborative
policyfromminimaldomainknowledge[11,44]. ABToMapproachhasalsobeenusedinasimplifiedversionofthe
task[53]. Inthelatterapproach, Wuetal. proposeBayesianDelegation, aBayesianmodelthatenables(joint)task
inference using knowledge about sub-tasks with pre- and post-conditions to determine partial orderings of different
recipes. Each“agentsimulatesafictitiouscentralizedplannerthatcontrolstheactionsofallagentsworkingtogether
onthesamesub-task”([53,p. 422])inordertodeterminethebestactiontotakefortheagent. Thatis,theapproach
istosearchforthemostsuitablecollaborativeactioninordertocompletethetaskinasfewstepsaspossible.
In the present work, we ask how successful collaboration can emerge based on cognitively simpler means, without
explicitly representing and solving an inherently complex multi-agent decision or planning problem. We use the
Overcooked domain in the setting presented in [11] and additionally evaluate our model in the version discussed in
[53](Sect.5). Ourapproachisrelatedtotheonepresentedin[53]aswealsoemployaprobabilisticBToMmodelto
identifyandadapttoanotheragent’sunderlyinggoalson-the-fly. Wealsoprovidedomainknowledgeintheformof
possible(action)intentions,suchasfetchingacertainitemorpreparinganingredient,andtheirlikelihoodsgivenan
observationoftheenvironment. However, ourapproachisdifferentaswedonotfeedToMhypothesesintoexplicit
actionplanningwithpre-andpost-conditionsofdifferentintentions. Furthermore,ourmodeldoesnotrequireexplicit
specificationofhowdifferentintentionsneedtobeorderedtocompletearecipe. Rather,wewanttoinvestigatehow
mentalizingcanbeintegratedwithaprediction-basedperception-actionloopinanindividualagent,andwhetherthis
canbringaboutmulti-agentcoordinationinreal-timewhensuchagentscometointeractinasharedenvironment.
Webaseourapproachonalargebodyofrecentworkonpredictiveprocessingandactiveinference. Theunderlying
assumptionisthatactionrecognitionrestsonprinciplesofpredictiveprocessing[14]wherepredictionsaboutsensory
stimuliarecontinuouslyformedandevaluatedagainstincomingsensoryinput.Anagent’ssystemicgoalistominimize
so-calledfreeenergy,i.e.,itsuncertaintyintermsofitssurpriseanddivergencewithrespecttoitspredictions[41,17].
For action control, predictive processing views action as a form of active inference by which the environment is
affected to reduce uncertainty about predictions that stem from beliefs about the world [1]. The generative process
isinvertedtopredictnextactionsand, thus, attenuatepredictionerrors[19]. Thismechanismhasbeendiscussedas
a form of so-called affordance competition, a possible mechanism for action selection [13] based on possible goals
achievablethroughthataction.
Acommonapproachtocopewiththecomplexityofsequentialdecision-makingistoemployhierarchicalmodelsthat
makeuseofadomain’sinherentstructure[23].However,relativelylittleworkhastriedtoapplythisapproachtointer-
agentcollaboration. Fristonetal.[18]proposedtotackletheproblemofinferringtheintentionbehindcommunicative
behavior by coupling two predictive processing-based models. Brandi et al. [10] proposed a predictive processing-
based model to study interaction in terms of the participant’s social agency, i.e., their ability to predict a partner’s
contributions to an interaction. This approach, however, has not been applied to collaboration in real-time situated
problem-solving.
The hierarchical model presented here builds on the previously proposed Hierarchical Predictive Belief Update
(HPBU) [28]. HPBU was modeled after the assumed cortical micro-circuitry for predictive coding [7] and is based
onprinciplesofactiveinference. ItcombinesempiricalBayesianupdateswithinalayerandlineardynamicupdates
(informofaKalmanfilter)betweenlayersofitsprocessinghierarchy. HPBUwasdevelopedasagenerativemodel
thatlearns,predictsandproduceshandwrittendigitsinanunsupervisedwayusinghierarchicalabstractionsspanning
from low-level movements to motor schemas corresponding to digits. This was used as a sensorimotor basis for a
model of self-other distinction [28] and was later extended to the social domain with additional levels that form a
simplified mentalizing system to perform belief coordination in multi-agent scenarios [27]. In the present work we
employ HPBU in more complex collaboration tasks and extend it with a mechanism for on-the-fly coordination as
describednext.
3 HierarchicalActiveInferenceforCollaboratingAgents(HAICA)
Inthissectionwepresentamodelforhierarchicalactiveinferenceforcollaborativeagents(HAICA)asonebuilding
block for collaborative agents that are supposed to coordinate efficiently and on-the-fly with others co-situated in a
jointtask. WewillfirstintroducethegeneralprinciplesunderlyingHAICA,beforedescribinghowitisimplemented
andappliedtotheOvercookeddomaininSect.4.2.
The overall structure of the model is shown in Fig. 1(a), instantiated for the green agent. Every agent is equipped
withahierarchicalmodelforbehaviorperceptionandgenerationbasedonpredictiveprocessing. Themodelgoverns
4
ResonatingMinds–EmergentCollaborationThroughHierarchicalActiveInference APREPRINT
a) b)
orders
HAICA
goal
Goal Layer
inferred goal
other’s
evidence intention
mental
prediction
state
intention
Intention Layer
other’s action observation t
action
ACIAH
intention
prediction
other’s intention
ACIAH
current intention
observation
intention
prediction
other’s current
intention intention
observation
Figure 1: (a) Overview of the proposed model of hierarchical active inference for collaborative agents (HAICA) in
the Overcooked domain, depicted for the green agent. Two predictive layers encode the agent’s beliefs about its
own intentions and goals. They are updated based on top-down (predictions) and bottom-up (evidence) information
originatinginternally(intheagent)orexternally(fromtheenvironment). Anadditionalcomponent(formentalizing
or ToM; blue box) is included to infer the other agent’s intentions or goals from its behavior. This information is
integratedasadditionalbeliefsinwhatwecallbeliefresonanceattherespectivelevels(seeSect.3.2). (b)Illustration
of the integration process at the intention layer of the green agent. Initially, the agent is not certain about its own
intention. GivenanorderforTomatosoupthegoallayerpredictscorrespondingintentionsforthatorder,e.g.,picking
up a tomato or plate. At the same time, a belief about the other (blue) agent’s intention is being inferred (getting a
plate). Belief resonance would tilt the green agent towards the other’s intention, i.e., getting a plate with increasing
certainty. However, bottom-up evidence about the environment validates possible intentions resulting in the agent
choosingtopickupatomatosinceitspathtowardstheplatesisblocked.
theagent’sbehaviorthroughgenerativeprocessesworkingatdifferentlevelsofabstractionofthetaskrepresentation.
Inthispaper,weemploytwolayersthataregearedtowardstheOvercookeddomain,namelyagoalandanintention
layer. Thegoallayerdeterminesanagent’sowndesirewithrespecttowhichordertoworkon(e.g.,atomatosoup),
whiletheintentionlayerrepresentshigh-levelactionsthatcanbetakeninthekitchen(e.g.,pickingupaspecificitem).
Importantly, the layers represent the agent’s beliefs about its own current goals and intentions, respectively. These
beliefsaresubjecttocontinuous,prediction-basedupdateprocessesthattakeintoaccountbothtop-down(predictions)
and bottom-up (evidence) information. These information sources can be seen as proposed beliefs, e.g., concerning
the agent’s current intention, which are integrated to determine the new state of the respective layer (as described
formally in Sect. 3.1). Crucially, this integration process is identical at each layer. The only differences are their
domainandthesourceoftheincominginformation. AsshowninFig.1(a),forthisdomainthegoallayerisconcerned
with the perceived orders (top-down predictions) as well as the currently available intentions (bottom-up evidence).
Theagent’scurrentgoal,inturn,proposessuitableintentionstop-downtotheintentionlayer,whichintegratesthese
proposedintentionswithcurrentbeliefsaboutperceivedactionaffordancesintheenvironment(bottom-up).Naturally,
theseactionaffordances(asinpots,dishesandvegetablesinakitchen)arealwaysalsoinfluencedbytheotheragent’s
actions. Atanypointintime,theagentselectsanactioninaccordancewithitsmostlikelyintention.
The model’s hierarchy (green solid boxes in Fig. 1(a)) is complemented with a mentalizing component (blue box)
toenrichtheagent’sperception-actionloopwithadditionalinformationabouttheotheragentthatcannotbereadily
observed. For the integration of these components we propose a mechanism called belief resonance, which allows
the belief update of the agent’s internal state to be affected by the inferred beliefs about another agent’s intentions
and goals. The idea is that – similar to assumed resonance processes in the human sensorimotor system – agents
can in that way align at the level of intentions and goals, which may implicitly lead to better coordinated actions.
NotethatHAICAdoesnotmakeanyassumptionsabouthowthementalizingcomponentworks,itonlyrequiresthe
5
ResonatingMinds–EmergentCollaborationThroughHierarchicalActiveInference APREPRINT
top-down
P
(
L |L
)
t j j+
prediction
K P
j+
( L )
p t j
old state new state
P L
Kj P
(
L
)
t−1 ( j ) t t j
evidence
K P
j−
L
e t ( j )
Layer j
P
(
L |L
)
t j j−
bottom-up
Figure2: Realizationofthebeliefupdatewithinapredictivelayer. EachcirclerepresentsaKalmanfilterintegration
withtherespectiveKalmangains. Top-downP (L |L )andbottom-upP (L |L )likelihoodsarefirstintegrated
t j j+ t j j−
withtheempiricalpriorP (L ):=P (s ) ∀s ∈L toproducethepredictionPj+(L )andevidencePj−(L ).
t−1 j t−1 j j j t j t j
K and K are fixed meta-parameters but Kj is computed by based on the previously computes predictions and
p e t
evidence. ThefinalposteriorP (L )iscomputedbyintegratingthetop-downpredictionwiththebottom-upevidence.
t j
eventuallyinferredbeliefsabouttheotheragent’smentalstates. Fig.1(b)illustratesthecompleteintegrationprocess
for the intention layer. Furthermore, HAICA does not specify explicitly when or in what order intentions are to
be considered. Goal directed and coordinated behavior instead emerges solely from the hierarchical belief update.
Top-downinformation, hereintheformofintentionpredictionsoriginatingfromtheagent’scurrentgoals, specifies
theintentionsthatarerequiredtoachievethatgoal,e.g.,aspecificmealorder,withoutimposinganorderingofthese
steps.Nextintentionsareselectedonlybyvalidatingwhatshouldbedoneingeneralagainstwhatactionsarecurrently
affordedbytheenvironment(givenintheformofbottom-upevidence).
Next,wedescribethecomputationalmechanismforthebeliefupdateprocessateachlayerinthehierarchicalmodel
(Sect. 3.1), and then present how belief resonance is formalized in this model (Sect. 3.2). The domain-dependent
implementationofmentalizingisexplainedinSect.4.2.2afterwehaveintroducedtheOvercookeddomainindetail.
3.1 Integratingtop-downandbottom-upinformation
EachlayerL representsacertainabstractmentalstaterelatedtothetaskdomain,e.g.,thegoallayerrepresentsthe
j
differentgoalsordesirestheagentmayhave. Alayerj definesaprobabilitydistributionP (L ):=P (s ) ∀s ∈L
t j t j j j
across its domain L to capture the degrees of belief the agent has regarding its own state. These distributions are
j
updatedeverytimettheagentperceivesnewobservations.Morespecifically,asshowninFig.2,eachlayerisupdated
throughprobabilisticinformationfromthelayerabove(L ),intheformoftop-downpredictiveinfluences,aswell
j+
asfromthelayerbelow(L ), asbottom-upevidence. Thisinformationtakestheformofassumptionstheadjacent
j−
layersholdaboutwhatlayerj’sstateshouldlooklikefromtheircurrentperspective. Inthecaseofouterlayersinthe
hierarchy, the layers receive this kind of information directly from the environment. Both top-down and bottom-up
informationarefirstintegratedwiththeprior(P (L )),i.e.,thelayer’spreviousbeliefstate.
t−1 j
The integrationis performedusing aKalman filter, resulting inseparate top-downpredictions about thelayer’s new
statefromthenexthigherlayerL asPj+(L )andbottom-upevidencefromthenextlowerlayerL asPj−(L ):
j+ t j j− t j
6
ResonatingMinds–EmergentCollaborationThroughHierarchicalActiveInference APREPRINT
Pj+(L ):=Pj+(s )=P (s )+K [P (s |L )−P (s )] ∀s ∈L (1)
t j t j t−1 j p t j j+ t−1 j j j
Pj−(L ):=Pj−(s )=P (s )+K [P (s |L )−P (s )] ∀s ∈L (2)
t j t j t−1 j e t j j− t−1 j j j
Here,K andK aretheKalmangainsforthepredictionandevidence,respectively. Thesearemeta-parametersthat
p e
can increase the importance of specific information if desired. Usually, they can both be set to 0.5, thus weighing
incoming and prior information equally. P (L |L ) and P (L |L ) are the top-down predictions and bottom-up
t j j+ t j j−
evidence,respectively. Wecomputethesedistributionsbytreatingtheouterlayersassoftevidence:
(cid:88)
P (L |L ):=P (s |L )= P(s |sj+)P (sj+) ∀s ∈L (3)
t j j+ t j j+ j t j j
sj+∈Lj+
P(s |sj+) is the likelihood function for a specific state s in L , given a specific state sj+ in L . P (L |L ) is
j j j j+ t j j−
computedanalogouslybutwithP (sj−)asweupdatelayersindecreasingorder.
t−1
Generally,aKalmanfilterisalineardynamicupdatethatprovidestheoptimalstateestimateofanunderlying(noisy)
signal[29]. Sinceinourcasetheinformationsources(alayer’stop-downprediction,bottom-upevidenceandothers’
inferredstate)areinherentlynoisy,wechosetheKalmanfiltertoestimatethetruestatebasedonthesenoisyproposals.
Kalmanfiltershavepreviouslybeenproposedasamodelforoptimalstateestimationinsensorimotorintegration[51],
forend-stoppingandotherextra-classicalreceptivefieldeffects[41],andtheyhavebeenusedasaforwardmodelfor
internalstateestimationinoptimalfeedbackcontrol[46].
Atthispointwehavetwodifferentdistributionsconcerningthelayer’sstate,theevidencePj−(L )andtheprediction
t j
Pj+(L ).Thefinalbeliefupdatemechanismisbasedonpreviouswork[28]andcombinesempiricalBayesianupdates
t j
withanadditionalKalmanfilterthatusesalayer-specificKalmangainKj. Kj iscomputedbasedona“freeenergy”
t t
Fj anda“precision”πj,i.e.,theinversevarianceofthepredictionerror(PE )). TheintegratedposteriorPj isthen
t t t t
approximatedasfollows:
P (L ):=P (s )=P+(s )+Kj(Pj−(s )−Pj+(s )) ∀s ∈L (4)
t j t j t j t t j t j j j
Fj
Kj = t (5)
t Fj +πj
t t
1
πj =ln (6)
t var(PE)
PE (L )=Pj−(s )−Pj+(s ) ∀s ∈L (7)
t j t j t j j j
Fj =H(Pj+(L ))+D (Pj+(L )||Pj−(L )) (8)
t t j KL t j t j
Eq. 8 consists of the entropy H(Pj+(L )) of the prediction distribution and the cross-entropy
t j
D (Pj+(L )||Pj−(L )) between the prediction and the evidence. This way, the integrated posterior P (L )
KL t j t j t j
results from the precision-weighted bottom-up and top-down information. The resulting posterior is then used to
computethelikelihoodsforthenexthigher(asevidence)andnextlower(asprediction)levelstoinformtheirupdates.
Thisintegrationofinformationissensitivetotheuncertaintyateverylevel,astheusedKalmangaindependsonthe
precisionoftheincominginformation.
During interaction with the environment, any information inferred at a layer has to be integrated with higher-layer
predictionssothattheresultingbehaviorgenerationdoesnotdiscountimportantsensoryevidenceordisregardhelpful
predictionsfrommoreabstractreasoningorrichercontext. Tothatend,bothbottom-upandtop-downinformationis
weightedandintegratedinawaythatissensitivetotheuncertaintyintheavailableinformation. Generally, making
correct predictions about hidden causes under uncertainty is tricky and the influence of prediction errors on prior
predictionshastobebalancedcarefully. Thisbalancingactshoulddependontheuncertaintyitselfandisaccounted
forbyprecisionweightingofthepriorpredictions[20].
3.2 BeliefResonance
In order for an agent to make use of the inferred mental state of another agent Q (L ) := Q (s ) ∀s ∈ L , this
t j t j j j
third source of information is integrated through a mechanism of belief resonance. The core idea is to make the
7
ResonatingMinds–EmergentCollaborationThroughHierarchicalActiveInference APREPRINT
top-down inferred mental state
P t(L j |L j+ ) Q t ( L j )
prediction
K
p
P
t
j+ ( L
j
) KSP
old state
P L
P L t ′( j ) new state
( )
t−1 j Kj P ( L )
t t j
evidence
K Pj− L
e t ( j )
Layer j
P t(L j |L j− )
bottom-up
Figure3: ExtensionofthebeliefupdatemodelinFig.2withbeliefresonance. Thetop-downpredictionPj+(L )is
t j
first updated using the inferred mental state Q (L ) := Q (s ) ∀s ∈ L using a Kalman filter with susceptibility
t j t j j j
parameterKSP asgain,beforeupdatingandvalidatingthiswiththeagent’sownbottom-upevidencePj−(L )forthe
t j
finalposteriorP (L ).
t j
agent’s self-beliefs about its own intentions and goals (which are affecting its actions by way of active inference)
affected by its current beliefs about the intentions and goals of the interaction partner. That is, we assume that the
agent has inferred a distribution Q (L ) about the mental state of another agent. While HAICA does not make any
t j
assumptionshereabouthowthismentalstatehasbeeninferred,wedonotethaton-the-flycoordinationimposesreal-
timerequirements.Classicalinverse-planningorotherfullBToMadaptationscanquicklybecomecomputationallytoo
demandingwithouttheuseofstrongpriors[5]. Sincewearefocusingonefficientmodelsforon-the-flycoordination
here,weusea“satisficing”(i.e.satisfyingandsufficing)approach[37]withaBToMmodeladaptedtotheOvercooked
domain. ThisapproachwillbepresentedindetailinSect.4.2.2.
InordertointegratetheinferreddistributionQ (L ),throughwhatwecallherebeliefresonance,weexpandthebelief
t j
updateschemeinEq.4tomodelaninfluenceofinferredothers’beliefsonone’sownaction-inducingbeliefsateach
layer. ThisresultsintheupdatedmodelshowninFig.3anddefinedinEq.9below.
Followingthetenetofdevelopingaminimalmodel,wechosetotreattheinferredbeliefsQ (L )asanothersourceof
t j
informationinadditiontobottom-uportop-downinformation. Thatis,theintegrationschemedoesnotdifferentiate,
at the level of internal beliefs, between own actions and others’ actions. This is similar to the resonance processes
assumed for motor action and perception in the human sensorimotor system [43], but now applied to the level of
abstract beliefs about action intentions and goals. We conjecture that this may be sufficient to implicitly lead to
inter-agentcoordination.
Crucially,beliefresonanceiscontingentonthe(un-)certaintyinherentintherespectivebeliefs. Further,theadoption
of the inferred (ToM) beliefs is modulated by a susceptibility parameter (SP) that we realize using another Kalman
gainKSP. Thisgainfactorcontrolstheinfluenceoftheinferredbeliefsontheagent’sposteriorbeliefdistributionat
each layer. While we set a fixed SP for the agents in this paper, in general, we do not assume this gain factor to be
fixedoridenticalforeveryinteractionpartnerorsituation. Rather,itcanbeusedtomodelroles,personalitytraits,or
maybeupdatedcontinuouslythroughouttheinteractionbasedonanevaluationoftheinteractionpartner.
Mathematically,beliefresonanceconsistsofatwo-stepbeliefupdatethatfirstintegratestheagent’sinferredevidence
Q (L )forlayerjwiththeagent’spredictionsPj+(L )intoanintermediateposteriorP(cid:48)(L ).P(cid:48)(L )isthenupdated
t j t j t j t j
and validated with the agent’s own evidence Pj−(L ) for the final posterior P (L ). This is an extension of the
t j t j
dynamicbeliefupdatepreviouslyproposedinEq.4:
8
ResonatingMinds–EmergentCollaborationThroughHierarchicalActiveInference APREPRINT
P(cid:48)(L ):=P(cid:48)(s )=Pj+(s )+KSP(Q (s )−Pj+(s )) ∀s ∈L (9)
t j j t j t j t j j j
P (L ):=P (s )=P(cid:48)(s )+Kj(P(cid:48)(s )−Pj−(s )) ∀s ∈L (10)
t j t j t j t t j t j j j
Note that Eq. 9 only takes into account the top-down information for layer j in the form of Pj+(L ) as well as the
t j
informationinferredfortheotheragentatthesamelayerQ (L ). BesidesusingthespecialKalmangainKSP, itis
t j
functionallyequivalenttothegeneralbeliefupdateruleappliedtotheevidencefrominferredbeliefs.Beliefresonance
can thus be understood as updating the agent’s internal beliefs from both internal and (social) external sources of
evidence.
In this way, the SP influences how strongly an agent is susceptible to the inferred mental states of the other agent.
HighervalueswillcauseastrongerresonanceasQ (L )willhaveastrongerinfluenceonP(cid:48)(L ).Thiscouldresultin
t j t j
ashiftintheagent’sownbeliefs,atleastiftheinferredbeliefsarefairlycertain,i.e.,fewbeliefshavehighprobabilities.
In contrast, low SP values will reduce the impact of the inferred mental states. Note, that Eq. 9 will simplify to
P(cid:48) =Pj+(L )ifSP=0,resultinginthesamemodelpresentedinFig.2orEq.4. Atthesametime,theagent’sown
t t j
bottom-upinformationistakenintoaccounttovalidatetheresonatingbeliefsforanygivensituation. Below,wewill
brieflyshowtheeffectofintegratingthebeliefabouttheotheragentafterthebottom-upinformation,todemonstrate
theimportanceofthisvalidation.
4 ApplyingHAICAtotheOvercookeddomain
InordertoevaluatetheproposedHAICAmodel,wehaveappliedittocollaborationtasksintheOvercookeddomain.
In this section, we introduce this Overcooked domain in detail and describe how the layers of the model with their
respectivelikelihoodfunctionsarerealizedinthisparticulardomain.Afterwards,wepresentanadaptedBToMframe-
work for efficient mentalizing as required for belief resonance. Finally, we describe how specific intentions of the
agentsareturnedintoactualbehavior.
4.1 TheOvercookeddomain
ThedomainisinspiredbythevideogameOvercooked[21]. Inthisgame,multipleagentsworktogetherindifferent
kitchen layouts in a 2D grid-world, to prepare as many ordered meals as possible in a limited amount of time. In
general,ateamisrewardedforeveryorderitcompletes,withaneworderreplacingthefulfilledone. Differentorders
maygiveadifferentamountofpointsdependingonthecomplexityoftheorder.
Within the environment, agents can move in the four cardinal directions to the next square (if it is free) and turn
towardsthatdirection. Furthermore,agentscanpickupobjectslikeingredients(onions,tomatoesorlettuce)orplates
byinteractingwiththeseobjects.Heldobjectscanbedroppedontoafreecounterinasimilarmanner.Theenvironment
mayfurthercontainspecialobjects,suchasacuttingboardoracookingpotthatanagentcaninteractwithinorderto
“prepare”ingredientsforthedifferentorders.
Forthiswork,weimplementedtheOvercookedenvironmentsimilarlytoCarrolletal. [11]: Inthisimplementation,
agentsneedtoexplicitlyperforman“interact”actioninordertopickupordropanitemandinteractwiththecooking
potordeliverytile. Agentsneedtopreparedifferentkindsofsoups(OnionorTomatosoup)usingacookingpot. The
environmentalwaysprovidestwoordersfortheagentstoworkon.Eachsoupiscookedbyputtingthreeingredientsof
therespectivetype(onionortomato)intothecookingpot. Theingredientsandplatescanbetakenfrom“dispensers”
whichprovideanunlimitedamountoftheseobjects. Onceapotcontainsalltheingredientsforarecipeitwillstart
cooking. Eachdifferentflavorofsouprequiresadifferentbutfixedamountoftimestepstocook. Thecookedsoup
canthenbetakenoutbyanagentbyinteractingwiththepotwhileholdingaplate.
Sincethesouprequirestimetocook,agentsshouldusethattimetoworkonanotherorderinthemeantimetoincrease
efficiency. Thisinteractionfavorscoordinationonahigherlevel,asagentsshouldstartpreparingthenextorderwhen
theyrealizethatallingredientsforthefirstorderarealreadybeingpreparedbyanotheragent.
We consider the five different kitchen layouts shown in Fig. 4: asymmetric, spacey, crowded, ring and forced, from
lefttoright. MostoftheselayoutswerealsoappliedinthestudybyCarrolletal. [11]andarewell-suitedtoevaluate
collaboration as they pose different demands: The first layout (asymmetric) has the agents physically separated but
sharingtwopotsinthemiddle. Asthereisnointerference,hardlyanycoordinationisrequiredandeitheragentcould
solvethetaskbyitself. Thesecondlayout(spacey)issimilarinthattheagentscouldsolvetheproblemindividually.
However,avalidcollaborationstrategymayinvolvesplittinguptheordersamongtheagents. Theremainingkitchen
9
ResonatingMinds–EmergentCollaborationThroughHierarchicalActiveInference APREPRINT
Figure4: FivedifferentkitchenlayoutsintheOvercookeddomainthatincreasinglyrequiremulti-agentcoordination
(fromlefttoright): asymmetric,spacey,cramped,ringandforced. Visualassetsadaptedfrom[11].
Table1: IntentionsofagentsintheOvercookeddomain. Parameterabbreviationsstandfor(O)nion,(T)omato,(D)ish
and(S)oup.
Intention Parameters Meaning
get-item O,T,D,S Pickuptheclosestitem
drop-item O,T,D,S Dropitemontheclosestfreecounter
interact-with-pot Pot Interactwiththeconcretepot
deliver-soup T,O Deliversouptoservingtile
hand-over O,T,D,S Dropitemontheclosestfreecounteraccessiblebybothagents
wait - Donothing
layouts require increasing amounts of coordination in order for the agents to be able to solve the task. Agents need
to avoid blocking the paths of the other agent in the cramped layout and even more so in the ring layout. Finally,
theforcedlayoutrequiresthetwoagentstocollaborateastheingredientsandthepotsarephysicallyseparated. Each
agentonlyhasaccesstoeithertheingredientsorthepots.
4.2 ImplementationofHAICAinOvercooked
AsshowninFig.1anddescribedabove,werealizetwolayersoftheHAICAmodelintheOvercookeddomain. The
goallayerrepresentstheagent’sdesireatagiventimeandholdsadistributionacrossthedifferentordersanagentmay
workon(OnionorTomatosoup). Thislayerreceivesitstop-downinfluencedirectlyfromtheorders,withalikelihood
that represents the order distribution, e.g., 50/50 if both Onion and Tomato soups are ordered. The second layer
representstheintentions,i.e.,highlevel-actionsanagentmaycarryout. Allpossibleintentionsandtheirparameters
arelistedinTable1.
Theintentionscanbeseenastheagent’sactionablecapabilitieswithintheenvironment. Inordertobeabletosolve
the task at all, an agent needs to be able to pick up and drop items and interact with the varying objects in the
environment. The “hand-over” intention may appear like hard-coding coordination. However, this intention only
providestheagentwiththecapabilitytohandanobjectover. Asmentionedabove,itisnotexplicitlyspecifiedwhen
or how this intention is to be used in order to aid collaboration. Instead, HAICA will choose the intention when it
becomesmostlikelyaccordingtothetop-downpredictionsbasedontheagent’sgoalandthebottom-upaffordances
perceivedintheenvironment.
Each of these higher-level intentions requires a sequence of actions in the environment. How these intentions are
turnedintolow-levelactionsisexplainedinSect.4.2.3.
4.2.1 Modelingdomain-specificintentionlikelihoods
For each of the intentions listed in Table 1, we defined functions to compute their likelihood given the current state
oftheenvironment. Toshowtherobustnessofthemodelandtoensurereal-timecapability,thelikelihoodfunctions
used here are primarily affordance checks. Specifically, most of these functions return either 0 or 1 which are then
normalizedoverallintentionsfortheactuallikelihood. Afunctionreturns0incaseanintentionisnotapplicablein
thecurrentstate,e.g.,anagentcannotpick-upanotheritemifitisalreadyholdingsomethinginitshands. Theonly
exceptionhereisthelikelihoodforwaitwhichissimplyafixedvalueof1/|intention|(≈ 0.08withtheinstantiated
intentionsoutlinedabove)beforenormalization,toindicatethattheagentcannotcurrentlydoanythinguseful.
Morefine-grainedcontrolaswellasbetterperformancecanbeachievedbyoptimizingtheselikelihoodseitherthrough
manualfine-tuningorlearningfromdata.Forexample,whiletheagentcansolvethetaskwiththecurrentimplementa-
10
ResonatingMinds–EmergentCollaborationThroughHierarchicalActiveInference APREPRINT
tionofinteract-with-pot,itwouldbeadvantageoustotakethedistancetothepotsaswellasthenumberofingredients
alreadyinthemintoaccount. Thatway,theagentcouldfavorapotthatitalreadystartedtofillbeforestartingtofilla
secondpot.
For the evaluation reported below, we modeled this heuristic along with some slack, i.e., non-zero probability for
deliveringasoupthatwasnotorderedanddroppinganitemthatiscurrentlyusablewithapotortheservingtile. The
“hand-over” likelihood is reduced if the other agent is currently holding something or if the agent could handle the
heldingredientitself. Thelikelihoodfunctionsin-betweenthetwolayersforboth,top-downfromgoaltointentionas
wellasbottom-upfromintentiontogoal,aresimilarlynormalizedaffordancechecks. Forinstance,thelikelihoodfor
get-item(Onion)giventhegoalTomatosoupwouldbe0sinceTomatosoupdoesnotrequireanonion.
Overall,thelikelihoodsimplicitlyencodetheagent’srecipeknowledge. Theinter-layerlikelihoodsdefinewhichsteps
are required for a certain meal in general, e.g., picking up tomatoes or plates, interacting with a pot and delivering
thesoup, butnottheirordering. Theideaisthatagoal-directedorderingofintentionsemergessincethebottom-up
likelihoodsinvalidateintentionsthatarenot(yet)applicable. Intheexampleillustratedin1(b)theintentionofpicking
upaplateisinvalidatedduetotheenvironmentalconstraints. Bychoosingthesedeterministicinter-layerlikelihoods
weassumethateachagentknowsthestepsrequiredforthedifferentorders.
4.2.2 Efficientinferenceofotheragents’mentalstates
TheproposedmentalizingapproachisbasedontheBToMframework,usingagenerativemodeltodefinehowdifferent
mentalstatesinfluenceanagent’sbehavior.Here,weconsiderthementalstatescorrespondingtothetwocentrallayers
thatthehierarchicalmodelassumes,namelypossiblegoals(ororders)(G)andintentions(orabstractactions)(I)an
agentmayhave. Thisresultsinthefollowinggenerativemodel:
(cid:88)
P(a,o)= P(a|i,g,o)·P(o|i,g)·P(i|g)·P(g) (11)
i∈I,g∈G
whereP(a|i,g,o)definesthelikelihoodofanactionagivenacertainintentioni,agoalg andcurrentstateobserva-
tionso.P(o|i,g)modelsthelikelihoodforspecificstateobservationsandP(i|g)modelsthelikelihoodofanintention
giventhespecificgoal.
ThismodelcanbeinvertedusingBayes’rule:
(cid:88)
P(i|a,o)∝ P(a|i,g,o)·P(o|i,g)·P(i|g)·P(g) (12)
g∈G
(cid:88)
P(g|a,o)∝ P(a|i,g,o)·P(o|i,g)·P(i|g)·P(g) (13)
i∈I
Generally, inferring the mental states G and I from this generative model would require us to evaluate all possible
actions. Further,thelikelihoodfunctionP(a|i,g,o)itselfmaybedifficulttocomputedependingonthescenario. As
anexample, Wuetal. [53]followthecommonapproachtosettheactionlikelihoodproportionaltothesoft-maxof
the expected future reward Q∗(s,a), modulated by a “rationality” parameter. This requires the agents to compute
estimations of the expected rewards, which in itself can be computationally very demanding as can be seen in our
runtimecomparisonbelow.
Sincecollaborationoftenrequirestimelyreactionsandusingacompletegenerativemodelquicklybecomestoocostly
[37], we opted for an approximate but “satisficing” solution. This is achieved by approximating the likelihood
P(a|i,g,o) = α if a = a∗, with a∗ being the action the agent would take in the situation o if it had intention i
and goal g. We can compute a∗, e.g., by utilizing the agent’s own action selection capabilities. While the classical
soft-maxapproachmodelsanagent’sactionlikelihoodstocorrelatewith“rational”behavior(usuallyregulatedbythe
“rationality” parameter), our simplification assumes the other agent should behave similar to oneself. We use α to
modelhowsimilarweareassumingtheotheragenttobe. Lowvaluesofαwillleadtomorerandomactions,different
fromwhattheagentwoulddoitself.
A general problem with probabilistic integration over time is that of disappearing probabilities. Integrating several
verysmalllikelihoods,e.g.,foraparticularintention,willresultinprobabilitiesapproachingthenumericalzerodueto
roundingerrors. Counter-evidenceintheformofstronglikelihoodsforthesameintentionwillthenhardlyinfluence
the posterior at that point. This results in a system that would react very slowly to changes in the behavior of the
11
ResonatingMinds–EmergentCollaborationThroughHierarchicalActiveInference APREPRINT
otheragent. SincewerequirequickadaptationtotheotheragentintheOvercookedscenario, weemployasoftmax
normalizationafterupdatingtheinferredbeliefsinconjunctionwithaddingafixednoiseµpriortonormalization:
P(cid:48)(i|a,o)∝exp[β·(P(i|a,o)+µ)] (14)
P(cid:48)(g|a,o)∝exp[β·(P(g|a,o)+µ)] (15)
whereβ controlsthestrengthofthemaximization. Smallvaluesforβ willresultinpushingthedistributiontowards
uniformity, while large values will reinforce the largest outcomes of the distribution. µ is used to ensure that all
outcomeshaverelevantprobabilitymassessothatthesystemcanreactquicklytochanginglikelihoods.Theseupdated
posteriordistributionsconcerningtheintentionsandgoalsarethenusedasQ (L )dependingonthelayerinthebelief
t j
resonancedescriptioninSect.3.2. Iflayerj istheintentionlayer,P(cid:48)(i|a,o)isusedastheinferredbelief,whilewe
useP(cid:48)(g|a,o)inthegoallayer.
TheusedapproximationofP(a|i,g,o)canbereplacedwithahandcraftedorlearnedmodeloftheproperlikelihoodif
required. However,asshownbelow,eveninitssimpleformitprovestobesufficientlyreliableandthusdemonstrates
howsimpleToMinferencescanbeachievedwithoutafullgenerativemodel.Further,sincethisapproachonlyrequires
discreteactionpredictions,itcanbecombinedwithpredictiveprocessingmodels,suchastheHAICAmodelemployed
here,aswellasmoretraditionalplanners.
4.2.3 Processingcycleandturningintentionsintoactions
EverytimeanagentreceivesnewobservationsfromtheenvironmentitfirstusestheToMmoduletoupdateitsbeliefs
regarding the other agent’s mental state, according to Eq. 14 and 15. The agent then computes the top-down and
bottom-upbeliefsatboththegoalandintentionlayer, beforeintegratingallinformationusingEq.9and10toform
thenewposteriors. Thegoallayerreceivesthecurrentordersastop-downinfluence,whiletheintentionlayerreceives
thedifferentlikelihoodsbasedontheagent’scurrentobservations. Theagent’slastposteriorisusedasprior, unless
it received a reward in which case its mental state is reset in preparation for the next task. Then, the maximum a
posteriori (MAP) intention is picked as the agent’s new active intention, i.e., the intention it is actually going to act
upon. Weregardthishierarchicallyinformedandprecision-weightedselectionprocessofanactiveintentionasaform
ofactiveinference.
Settingthenextactiveintentionmustbefollowedbyplanningthelow-levelactionstofulfillit. Wuetal. [53]used
boundedreal-timedynamicprogramming(BRTDP)tofindanoptimalpolicyforaselectedsub-task. Weoptedfora
simplerapproachinfavorofgreatercomputationalefficiency. Generally,intentionsrelatetointeractionswithcertain
objectsintheenvironment.Thismeansthatinordertofulfillanintention,theagentusuallyneedstomovetothetarget
object, which could be an ingredient or a preparation object, e.g., the cooking pot. Given that the intention tells us
whatobjectwewanttointeractwith,wecanemployabest-firstsearchalgorithmlikeA*tofindappropriatelow-level
actions(Left,Right,Up,Down,InteractorWait)foraselectedintention. Notethat,whileverysimpleandefficient,
this approach to action planning does not take the other agent’s likely future actions into account. As a result, one
agentmayblockaccesstocertainobjectsthatanotheragentmayneed.
Due to the dynamic nature of the task, an agent may need to change its intention before achieving it. One example
wouldbethatanotheragenthasalreadyfulfilledacertainstepintherecipeorbecausetheotheragentblocksaccess
to certain resources. Since the agent is updating its mental state every time it receives new observations, the model
is able to cope with these challenges inherently and immediately. Still, one important addition we made is that of
“punishing aborted intentions”. Due to the dynamic nature of the model, when two agents each equipped with this
model collaborate, they may end up switching back and forth between two alternating intentions, each of which
invalidatingthecurrentintentionoftheotheragent. Insteadofemployinganexplicitmemoryintheformofastate-
intentionhistory,weoptedtoonlycheckiftheagent’scurrentintentionchangedwithoutachievingit. Iftheprevious
intentionwasaborted,wepunishitbyreducingitspriorprobabilityforthenextroundofbeliefupdates.
5 Results
TostudyifandhowtheproposedHAICAmodelcanenableagentstocollaborativelysolvejointtasks,weranaseries
ofsimulationsinordertoinvestigatethefollowingaspects. Firstly,weanalyzeiftheHAICAmodelisabletosolve
thetasksatall,asitdoesnotinvolveexplicittask-planningorcoordination. Secondly,weexploreifandwhenbelief
resonance is beneficial for collaboration between two agents. To that end, each agent is equipped with the belief
resonancemodelbutwithvaryingSPvalues. Weexpecttofindlayoutswithhigherdemandsforcollaboration,such
as ring or forced, to be solved best by teams with agents that are susceptible to one another’s mental state. As an
12
ResonatingMinds–EmergentCollaborationThroughHierarchicalActiveInference APREPRINT
Table2: Metaparametersusedfortheevaluation
Parameter Value Explanation
α 0.9 ActionlikelihoodofpredictedactioninToMmodule
β 2 SoftmaxparameterforToMmodule
µ 0.1 RandomnoiseforToMmodule
K ,K 0.5 Kalmangainsforbottom-upandtop-downbeliefintegration
e p
additional condition, we evaluate teams in which either both agents know the orders or only the first agent (hence
requiringthesecondagenttoinferwhichmealsneedtobepreparedfromthefirstagent’sbehavior). Suchascenario
representsaleader-followerdynamic. Thirdly,weevaluatetheimportanceofvalidatingresonatedbeliefsviabottom-
upevidence.Forthat,wetestanalternativeordertointegratethedifferentinformationsources,i.e.,weswapPj−(L )
t j
withQ (L )aswellasK withKSP inEqs.9and10. Thiscorrespondstotheagentintegratingitsowninformation
t j t
sources first (top-down and bottom-up), before integrating the inferred belief about the other. Finally, we directly
comparetheHAICAmodelwithtwostateoftheartsolutionspresentedin[11]and[53],respectively. Althoughthe
proposedmodelisfocusedmoreonefficienton-the-flycollaboration,thiscomparisonwillallowustoputourresults
intoalargerperspectivewithregardtotherelationbetweenoptimalityandefficiency.
All results reported in the following were obtained by averaging team rewards achieved in simulations with over
20 episodes of 400 time steps each. New orders (Onion or Tomato soup) were generated at random, rewards were
obtained after having prepared a meal and corresponded to the respective cooking times: 20 points for Onion soup
and 15 for Tomato soup. Agents always started an episode at the same positions (see Fig. 4), the SP values were
assignedatrandomtothetwoagentstoavoidinfluencesoftheirstartingpoints. Table2liststhevaluesforallused
metaparameters.
Fig. 5 shows the average rewards achieved in different kitchen layouts by teams in which a) both agents know the
currentordervs.onlyoneagentknowsit,andb)theagentsdonotperformbeliefresonance(SP=0)vs.withtheoptimal
combinationsofSPvalues. Todeterminetheseoptimal, i.e., bestperforming, SPvalues, wetestedallcombinations
between0and1in0.1increments. Asabaseline,aswellasforthefirstevaluatedaspect,weaddtherewardachieved
byasingleagentusingtheproposedmodel. Thissingleagentequippedwiththeproposedmodelwasabletosolvethe
cookingtasksinallbuttheforced layoutwithfairlyconsistentperformance. Theforced layoutrequirestwoagents,
oneineachoftheseparatedpartsofthekitchen,tobeabletoprepareanymealsatall.
5.1 Effectofbeliefresonancewhenbothagentsknowtheorders
Withoutanybeliefresonance(SP=0),theagentsachievescoresrangingfrom123.75(±4.75)inthecramped layout
to268.5(±3.55)intheasymmetriclayout(Fig.5). Exceptforthecrampedlayout,twoagentsperformedbetterthan
a single one, indicating that the hierarchical model alone already supports some form of uncoordinated collabora-
tion. ConsideringtheoptimalcombinationsofSPvalues,otherthan0forboth,theteamperformanceimprovesonly
marginallyintheasymmetricandspaceyconditions. asymmetricimprovesto270.5(±3.89)withSPvaluesof0and
0.2forthetwoagentsandspaceyimprovesto136.75(±3.93)withSPvaluesof0.1forbothagents.
Fig.6visualizestheagents’performancedependingontheirSPvaluecombinationsinaheatmap. Forthis, wefirst
computedtheaveragerewardforeachcombinationofSPvaluesandlayoutacrossthe20episodes. Wethencomputed
the mean score for each combination of SP values. While all combinations were able to complete some orders,
the agents performed best with rather low SP values for both agents. Considering the layouts individually, we find
thattheoptimalcombinationsforalllayoutshadoneagentnot(SP=0)oronlyminimally(SP=0.1)performingbelief
resonance.ThebestSPvaluewas0.2forthesecondagentinasymmetric,0.3incramped,0.4inringand0.1inspacey.
Especiallyintheasymmetriccondition,however,otherSPvaluesforthesecondagentachievesimilarperformance.
5.2 Effectofbeliefresonancewhenonlyoneagentknowstheorders
In this condition, one agent (agent 2) was made “order blind” by always receiving a uniform distribution over soup
orders. The average team performance declines when one agent does not know the orders and the agents do not
performbeliefresonance(SP=0)ascanbeseeninFig.5. However,whenconsideringtheoptimalSPcombinations,
theteamperformancerecoversinallbuttheforced layout, oftengettingclosetotheperformancewhenbothagents
areinformedoftheorders.
13
ResonatingMinds–EmergentCollaborationThroughHierarchicalActiveInference APREPRINT
Both know order (SP=0)
One knows order (SP=0)
250 Both know order (Optimal SP)
One knows order (Optimal SP)
Single Agent
200
150
100
50
0
Figure5: Averagerewardinthedifferentlayoutsforasingleagentandfortwoagentswithdifferentknowledge(both
oronlyoneknowingtheorders)anddifferentdegreesofbeliefresonance(SP =0orsettotheoptimalvalue). Error
barsindicatethestandarderror.
AsshowninFig.7,theoptimalSPcombinationsstillinvolvefairlylowvaluesforagent1thatknowstheorders(SP
of0or0.1). The“orderblind”agent,however,performsbestwithSPvaluesbetween0.2(forspacey)to0.9for(ring).
Finally,similartothecasewhenbothagentsknowtheorders,performancedeterioratessubstantiallywhenbothagents
useSPvaluesbeyond0.6.
5.3 Effectofbeliefintegrationorder
When we integrate the inferred other-beliefs after updating the agent’s beliefs using the top-down and bottom-up
information (instead of in-between), the average team performance is a lot more dependent on very low SP values,
asshowninFig.8. Acrossalllayouts, agentsonlyperformreasonablywellwithSPvaluesof0, i.e., withoutbelief
resonance. Very low values (0.1 or 0.2) can still yield results comparable to the original integration order presented
above. However, team performance quickly breaks down for higher SP values. This is especially the case for the
layouts ring, forced and cramped, where SP values beyond 0.5 for any agent result in the agents mostly failing to
completeanyordersatall.
5.4 Comparisonwithstateoftheart: ReinforcementLearning
InordertoputtheresultsshowninFig.5intoperspective, wecomparethemwiththeresultsreportedbyCarrollat
al.[11]forthebestRLmodelinsimilarkitchenlayouts(asymmetric,ring,forced,andcramped).Onlyinthecramped
layout are the ingredients, the pot and the serving area arranged slightly differently. It is noteworthy, however, that
Carrolletal. usedasimplertaskbylimitingthepossibleorderstoonlyonetype(Onionsoup). Thus,forcomparison,
wealsorestrictedthekitchenlayoutsinFig.4toonlycontainonions.
14
ResonatingMinds–EmergentCollaborationThroughHierarchicalActiveInference APREPRINT
SP agent 2
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
1
tnega
PS
0 25 50 75 100 125 150
Average score
Figure6: MeanrewardfordifferentSPvaluecombinationsaveragedacrossthedifferentkitchenlayoutswhenboth
agentsknowtheorders(averagedacrossN=20runsforeachlayout).
Fig.9showsthescoresobtainedwithtwovariantsoftheHAICAmodel,alongsidethescoresreportedin[11]forRL
agents trained via population based training (PBT). We also added Carroll et al.’s results achieved with an optimal
policydeterminedviacoupledplanning(CP),inonlytwoofthelayoutsinwhichcomputingtheoptimalpolicywas
feasible.
Theresultsrepresenttheaveragerewardachievedbytheagentsafter400steps,wheretheOnionsouprequires20time
stepstocookandyields20pointsupondelivery. NotethatCarrolletal.[11]averagedacross5randomseeds,while
the results for the model proposed here were averaged across 20 runs. Note also that we only included the results
ofagentscollaboratingwithcopies ofthemselvesastheperformancein[11]generally deteriorated formixed-agent
setups.
Generally, the performance of the proposed HAICA model is comparable to and only marginally worse than those
achievedthroughdeeplearning[11]inthelayoutscramped,ringandforced. Intheasymmetriccondition,theHAICA
modelsachievesignificantlybetterperformance,mostlikelyduetothefactthattheagentsareabletomakeuseofboth
pots instead of only one the deep RL agents tend to use. Yet, the comparison with the CP agents also demonstrates
thatHAICAdoesnotreachascorethatisachievablewithanoptimalpolicy.
5.5 Comparisonwithstateoftheart: BayesianDelegation
The mentalizing approach used in the HAICA model with belief resonance is more similar to probabilistic BToM
approachesthantoRL.Forthisreason,wealsocomparedtheHAICAmodeltoanexplicitplanningapproachusing
BayesianDelegation(BD)[53]. Insteadofre-implementingtheenvironmentaswedidwiththesoupscenarioabove,
weusedthecodeprovidedbyWuetal. [53]andimplementedtheHAICAagentfortheirslightlydifferentrealization
of the Overcooked domain. In this domain, agents are supposed to prepare three different kinds of salads (lettuce,
15
ResonatingMinds–EmergentCollaborationThroughHierarchicalActiveInference APREPRINT
SP agent 2
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
1
tnega
PS
0 25 50 75 100 125
Average score
Figure7: MeanrewardfordifferentSPvaluecombinationsaveragedacrossthedifferentkitchenlayoutswhenonly
agent1knowsthecurrentorders(averagedacrossN=20runsforeachlayout).
tomato and mixed). The preparation of a salad involves cutting the ingredients (lettuces and tomatoes) at a knife
station and happens instantly upon interaction. A major difference is that each object (ingredient or plate) is only
availableatthequantityrequiredtocompletetheorder,unlikethedispensersconsideredabove. Wuetal. considered
three possible tasks: prepare and deliver one Tomato salad, two salads, one of each type (Tomato+Lettuce) or one
Mixed salad. Theyfurtherevaluatedtheirmodelinthreedifferentkitchenlayouts(Full-Divider, Partial-Dividerand
Open-Divider)resultingin9differentscenarios.
Implementing the HAICA agents in this environment is largely identical to the implementation presented above. It
basicallyinvolveschangingthegoaldomaintothethreesaladrecipes(lettuce,tomatoandmixed)andrenamingthe
intentionsandtheirobjectsaccordingtothenewingredients(lettuceandtomato)andinteractionobject(cuttingboard).
Whilethelikelihoodfunctionsneededtobeadaptedtothedifferentenvironmentimplementationanddynamics,their
functionalityremainedfundamentallythesameaswell.
Someprecautionsareneededinthisscenario,though. Sinceeachingredientisonlyavailableonceandtheyneedto
becombined,agentscangetstuckinbothpickinguponepartoftherecipe(e.g.,oneagentpickingupthecutlettuce
andtheotheraplate). Theymayendupcyclingthroughplacinganitemdownfortheotheragent,andpickingitor
theotheritembackup. Thisisnotaprobleminthesoupscenariowhichincludeddispenserswithunlimitedamounts
ofdifferentitems. Atthesametime,eachreciperequiresmultipleofthesameingredients. Inordertoreducechances
for such cycles, we punished interacting with an item directly after placing it down by reducing the corresponding
intention’spriorprobability.
TocomparetheHAICAmodelwiththeBDmodel,weusemetricssimilartothoseusedbyWuetal.[53]. Insteadof
determiningateamscoreafterafixedamountoftime,theymeasuredthenumberoftimestepsrequiredtocomplete
asingletask. Theyfurtherevaluatedtheiragentswithrespecttothenumberofcompletedsub-tasks(e.g.,cuttingthe
tomatoesfortheTomatosalad)afteramaximumof100timesteps. SincetheproposedHAICAmodelisagnosticto
16
ResonatingMinds–EmergentCollaborationThroughHierarchicalActiveInference APREPRINT
SP agent 2
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
1
tnega
PS
0 25 50 75 100 125 150
Average score
Figure 8: Mean reward for different SP value combinations averaged across the different kitchen layouts when the
inferred belief was integrated after the top-down and bottom-up information (averaged across N=20 runs for each
layout).
Table3: PairsofSPvaluesthatachievedthebestsuccessrateinthedifferentscenarios. Iftwocombinationsledtothe
samesuccessrate,thelowernumberoftimestepsdecided.
KitchenLayout Tomato Tomato+Lettuce Mixed
Full-Divider 0.3&1.0 0.8&1.0 0.0&0.3
Open-Divider 0.0&0.7 0.2&1.0 0.3&0.4
Partial-Divider 0.4&0.4 0.3&0.4 0.1&0.8
sub-tasks we cannot compare it on that metric. Instead, we decided to focus on the percentage of overall successes
within100timesteps. Anepisodewasconsideredasuccessiftheagentsmanagedtocompletethetaskentirely,e.g.,
onlydeliveringtheLettucesaladbutnottheTomatosaladwouldnotcountasasuccessintheTomato+Lettucetask.
Additionally,wemeasuretheactualclocktimeeachepisoderequires. Thisisimportant,sinceourfocusisnottobe
competitive with explicit planning in terms of optimal action selection, but rather to achieve satisficing results that
enablereal-timeinteractions.
Fig.10presentsthesuccesspercentagesforeachofthe9differentscenariosachievedbyagentteamswithtwovariants
oftheHAICAmodelandtheBDmodel. ThevaluesfortheBDmodelwereobtainedbyrunningthereplicationscript
provided by Wu et al. [53] without any changes except for an additional time-clock measurement. We also re-used
theirscripttoevaluatetheHAICAmodelbysimplyreplacingtheirBDagentmodelwiththerespectiveHAICAmodel.
Thatway,weevaluatedallcombinationsofSPparametersbetween0and1in0.1incrementsfortheHAICAagents.
WepresentheretheresultsforpairswithSP=0aswellastheoptimalSPvalues,i.e.,thosethatmaximizetheagent’s
success rate, which are listed in Table 3. The results show that all agent teams always succeed in preparing just the
17
ResonatingMinds–EmergentCollaborationThroughHierarchicalActiveInference APREPRINT
400 HAICA (SP=0)
HAICA (optimal SPs)
PBT
350
CP
300
250
200
150
100
50
0
y
m m
etric
cr a m
p
e
d
ri
n
g
f
orc
e
d
as
Figure9: Comparisonofaveragerewardsachievedbydifferentagentmodelsinvariouskitchenlayouts(withslight
differencesinthecrampedlayout). PBTrepresentsthedeepRLagentstrainedusingpopulationbasedtraining,while
CPrepresentstheoptimalagentsusingcoupledplanningpresentedin[11]. ThePBTandCPresultsaremeanscores
over5trails,whiletheHAICAscoreswereaveragedover20. Errorbarsrepresentthestandarderror.
Tomato salad, except those using the HAICA model without belief resonance (SP=0). When preparing two dishes
(the Tomato+Lettuce task), the HAICA models always perform worse than BD. On the other hand, BD appears to
strugglemostwiththeMixedsaladinthePartial-DividerandOpen-Dividerkitchenlayouts. Inthistask,theHAICA
modelwithoptimalSPparametersperformsbetterthanBD.ComparingtheseoptimalSPvaluestothesoupdomain
discussedbefore,weseetheyarenotablylargerinthisdomain. ThisisespeciallytruefortheTomato+Lettucetaskin
twoofthethreekitchenlayouts.
Table4listsfurtherresultsusingcomparablemetrics. Valuesareaggregatedacrossall9scenariosusedin[53],where
eachscenariowasre-run20timeswithdifferentrandomseeds. Thepercentageofsuccessesareaveragedoverthese
20seedsaswellasthedifferentscenarios. Theotherstatisticsareaveragedacrossallepisodes. BDreliesonQ-value
estimationscomputedusingBRTDP[33]. TouseBRTDP,oneconfiguresthemaximumnumberofstepstosimulate
alongwiththemaximumnumberofsimulationstoperformwhenestimatingtheQ-values. InordertocompareBD
with different computational budgets, we also tried running BD with less optimal but computationally cheaper Q-
valueestimations. Tothatend,wedecreasedtheseparametervaluesfrom75/100(default)to10/2andeven1/1(for
#steps and #simulations, respectively). All models were run on the same machine using an Intel(R) Xeon(R) CPU
E5-1620@3.50GHz.
Theseresultsshowthattheapproachpresentedhereissignificantlyworsecomparedtotheoptimalplanningachieved
byBDwithdefaultparameterswhenitcomestothenumberoftimesteps(oractions)thatarerequiredtosolvethe
task. Asanepisodeisdeemedafailureafter100timesteps,thepercentageofsuccessesforthisdomainisalsoworse,
especiallyinthecasewithSP = 0forbothagents,i.e.,withoutbeliefresonance. However,ifweaggregatethedata
18
ResonatingMinds–EmergentCollaborationThroughHierarchicalActiveInference APREPRINT
1.00 1.00 1.00
0.75 0.75 0.75
0.50 0.50 0.50
0.25 0.25 0.25
0.00 0.00 0.00
1.00 1.00 1.00
0.75 0.75 0.75
0.50 0.50 0.50
0.25 0.25 0.25
0.00 0.00 0.00
1.00 1.00 1.00
0.75 0.75 0.75
0.50 0.50 0.50
0.25 0.25 0.25
0.00 0.00 0.00
rediviD-lluF
rediviD-nepO
rediviD-laitraP
Tomato Tomato+Lettuce Mixed
HAICA (SP=0) HAICA (opt. SPs) BD
Figure10: Percentageofsuccessfultaskcompletion(outof20trials)achievedbydifferentagentmodelpairsinthree
differentkitchenlayoutsandthreedifferenttasksusedbyWuetal.[53].BDrepresentstheBayesianDelegationmodel
proposedthere,theHAICAmodelsdifferwithrespecttothedegreeofbeliefresonanceofeachagent(noresonance
forSP=0,ortheoptimaldegreeandSPvalueforthisscenario).
Table4:Averagenumberofsteps,percentageofsuccesses,averagetimeperepisode(inseconds)andaveragetimeper
step(inseconds)withtheirstandarderrorfortwovariantsofHAICAwithdifferentSPvaluesandtheBayesianDele-
gation(BD)modelproposedin[53]usingdifferentparametersfortheQ-valueapproximationwithBRTDP(maximum
numberofstepspersimulation/maximumnumberofsimulations). Thedefaultvaluesare75/100.
HAICAwithSP=0doesnotuseanybeliefresonance,whileoptimal(opt.) aggregatesthebestSPcombinationsfor
eachlayout. (Averagedacross20seedsandthe9scenarios.)
Model TimeSteps Success Run-time TimeperStep
HAICA(SP=0) 71.41±1.78 0.76±0.03 1.54±0.06 0.02±0.00
HAICA(opt. SPs) 60.79±1.69 0.89±0.02 1.32±0.05 0.02±0.00
BD(default) 36.52±1.51 0.92±0.02 2111.55±133.85 55.87±3.33
BD(10/2) 51.02±1.85 0.86±0.03 67.14±2.95 1.29±0.02
BD(1/1) 77.12±1.93 0.53±0.04 21.40±0.77 0.27±0.01
19
ResonatingMinds–EmergentCollaborationThroughHierarchicalActiveInference APREPRINT
for the best SP combinations for each of the different layouts, the HAICA model performs almost as well as BD in
termsofsuccesses.
Thedifferencesinperformancecanbeputintorelationwiththedifferencesinrun-time.Whiletheagentpairequipped
withHAICAmodelscompletesanepisodein1.37-1.55secondsonaverage,agentsusingtheBDmodelwithoptimal
planningrequiremorethan35minutestocompleteanepisodeonaverage.Naturally,thistranslatesdirectlytothetime
theagentsrequiretodecideoneachaction. Whileeachstep,i.e.,theactionselectionofbothagentsandtheupdateof
theenvironment,tookonly0.02secondswiththeHAICAmodels,ittookcloseto56secondsonaveragewiththeBD
models. Ifoneneglectsthetimetoupdatetheenvironment(whichisidenticalregardlessofthechosenagentmodel),
eachBDagenttakesaround28secondstodecideuponanactiononaverage,whileeachHAICAagentonlyrequires
around 0.01 seconds. BD’s real-time capability can be greatly improved by limiting the computational resources of
theQ-valueapproximation. Areductionfrom75/100ofsimulatedstepsandrepeatedsimulationsto10/2providesan
impressive speedup, resulting inaround 1.29 seconds per action or even 0.27seconds per action by only simulating
1 step only once. However, the worse Q-value approximation results in a lower number of successes and a related
increaseinthenumberofstepsrequiredtocompleteeachscenario. Notably,evenwithminimalQ-valueestimation,
BDrequiressubstantiallylongerthanHAICA.
6 Discussion
Our evaluation studies reveal several interesting insights about the proposed HAICA model. First, the results show
thattheagentsareabletocollaborativelysolvethetasksofpreparingmealsinthedifferentenvironmentsinthe“soup”
domain,albeitwithvaryingsuccess. TheHAICAmodelsperformedbestintheassymetriclayout. However,theraw
scorescannotbecompareddirectlyacrosskitchenlayouts. Oneimportantfactorforthedifferencesbetweenkitchen
layoutsisthedistancebetweeningredientsandthepots. Thenumberofstepsthatagentshadtoperformtomovean
ingredientintoapotistheshortestintheasymmetriclayout. Thislayoutalsohasthebenefitthatagentscannotblock
eachother’spathinthiscondition.
Notably,andcontrarytoourexpectation,theagentswerealsoabletosolvelayoutsthatnecessitateacertainamount
of collaboration such as ring or forced. They did so even without belief resonance (SP=0). This can be explained
bythechosenintentionsandtheirlikelihoodfunctions. Thehand-over intentionwillbecomeactivewhentheagent
is not able to do anything else that may help it complete the meal, regardless of what the other agent may currently
be doing. That way, the agent on the l.h.s. in the forced layout (see rightmost sub-figure in Fig. 4) will pass over
potentiallyrequireditems. Likewise,theagentonther.h.s. maythenbeforcedtoworkonanorderitdidnotinitially
intendto,simplybecauseitonlyhasaccesstotheingredientsfortheotherorder. Itisworthnotingthatthisdoesnot
implyexplicitcoordination.Theagentssimplyrealizefromtheirbottom-upaffordancesthatalltheycandotoachieve
their goal is to use the hand-over action. When considering the optimal combinations of SP values, no significant
improvements were achieved, with only minor improvements in the asymmetric and spacey layouts. The best SP
combinations usually include very low SP values. This is most likely due to “collaboration” being inherent in the
intentionssuchthattheagentsstartcollaboratingsimplybydoingwhattheyareabletodogiventhenatureofthetask
andtheirjointsituatednessinthesharedenvironment.
Belief resonance does have a very positive effect in scenarios with unbalanced information. When only one agent
knows the current order, belief resonance is sufficient for the other agent to take up a follower role in the collab-
oration. Consequently, team performance suffered substantially without belief resonance (SP=0), while the agents’
performancealmostrecoveredtothelevelofthefirstconditionwhentheagentsusedoptimalSPvaluesformostofthe
kitchenlayouts. Theoptimalcombinationsusuallycontainedone“leader”withalowSP(≤ 0.1)andone“follower”
withvaluesgoingupto0.9,i.e.becomingverysusceptibletotheinferredintentionsandgoalsoftheother.Onlyinthe
forced layoutthereisnoimprovement. Wesuspectthatthisstemsfromthefactthattheagentontheright-handside
usuallydoesnotgetmuchofachoiceaboutitsnextactionsregardlessofitknowingtheordersornot. Theagenton
theleft-handsidewouldjusttakerandomguessesifitdoesnotperceivetheordersandisunabletoinferusefulinfor-
mationfromtheotheragent.Overall,theseresultsindicatethatbeliefresonancecanbeausefulbuildingblockforfast
and adaptive collaboration through on-the-fly coordination, especially in situations that benefit from leader-follower
dynamicsandthatprovideagentswithsharedactionaffordances.
WhenlookingattheSPvaluesinmoredetail,wefindthatinthesoupdomainlargeSPvaluesforbothagentsusually
lead to a reduced team performance, in the worst case with agents being unable to complete a single order. This
is understandable because agents with very high SP values overwrite their own goals and intentions at every step
withthoseinferredabouttheotheragent. Whilethismayseemlikeanargumentagainstbeliefresonance,wewould
interpret it as a need for adaptive collaboration and complementary roles in addition to it. A fixed SP value can be
detrimentalasitmayleadtosituationswheretheagentsaretoosusceptibletotheinferredbeliefs, resultinginboth
20
ResonatingMinds–EmergentCollaborationThroughHierarchicalActiveInference APREPRINT
agentstryingto“followthelead”oftheotheragentwithnooneleading. Indeed,theresultsshowthatonecompetent
agent leading (and being more “stubborn”) is usually better. Further, apart from using lower SP values, one could
also avoid this problem by providing agents with memory and having them stick to chosen intentions unless those
arecompletedorbecomeimpossible. FutureworkshouldlookatdynamicallyadaptingtheSPparameterinorderto
naturallytakeupleaderorfollowerrolesdependingontheotheragent.
Regarding the order in which information is integrated during belief resonance, we find that performance is a lot
worseforallbutonlysmallSPvalueswhentheinferredother-beliefsareintegratedintoself-beliefslast. Thiscanbe
explainedwhenconsideringthemeaningbehindthedifferentsourcesofinformation. Bottom-upinformationreflects
theagent’scurrentaffordances. Bysuppressingoroverwritingitsownbeliefswiththeinferredbeliefs(dependingon
theSPvalue), anagentmayendupchoosingactiveintentionsthatarenotpossibleforitself. Whenusingtheorder
proposedinEq.9and10,however,theinferredbeliefisfirstcombinedwiththetop-downinformation,beforebeing
“validated”againstbottom-upinformationaboutcurrentactionaffordancesoftheenvironment.
AlthoughHAICAisnotdesignedtosearchforthebestpossiblecollaborativeactions, weweresurprisedbyitsper-
formance compared to other state of the art approaches. The model achieves a comparable performance as state of
theartdeepRLapproaches[11]trainedandtestedonsimplerversionsoffourofthelayoutsconsideredhere. Inthe
asymmetriclayout,HAICAagentsevenoutperformedtheRLagents,whichtendtoonlyuseoneoftheavailablepots
and wait for the soup to be fully cooked, instead of preparing ingredients in the second pot in the meanwhile. The
proposedHAICAmodelinsteadtriestoworkonpresentordersaslongasthebottom-upaffordancesallowappropriate
actions. These agents thus make use of the second pot while a soup is cooking in the first. The three other kitchen
layoutseitherhadonlyasinglepottobeginwith(cramped),orthepositionofthesecondpotwasnotaccessible,at
leastwhenthefirstonewasinuse(ringandforced). Itisalsoworthnotingthat,duetothesimplificationofhaving
onlyoneorder[11],HAICAcouldonlyplayoutitsmentalizingandbeliefresonanceattheintentionlayer.
ConsideringthecomparisonwithBayesianDelegation(BD)[53]inthedifferentsaladsetting,wefindthattheHAICA
modelselectslessoptimalactions. ThisisnotsurprisingasBDagentsperformexplicitplanningandtrytofindthe
optimalplansgiventheirsub-taskassignments. Thisissimilartotheoptimalbehaviorachievedbycoupledplanning
in[11]. Theminimalisticactionselectionstrategyemployedheredoesnottaketheotheragent’sfutureactionsinto
accountwhenplanningownactions. Theenvironment’saffordancesmaycauseeachagenttopickuponeoftheitems
thatneedtobecombined. Whiletheagentsrealizethattheyneedtohandtheiritemovertotheotheragent,sincethey
cannotcompletetherecipebythemselves,theydonotconsidertheotheragentpotentiallydoingthesamething. Since
there is no explicit coordination, both agents tend to get “stuck” placing their item down and picking it back up or
takingtheitemtheotheronejustputdown. Thisleadstomanyunnecessaryactionsandagentsonlyhaveachanceto
getoffthiscycleduetotheproposed“punishment”oftheserepetitiveintentions. Inordertodetectandactivelyavoid
suchscenarios,moreexplicitplanningorfine-tuningwouldberequired.
Interestingly,thesaladscenarioappearstoallowforSPvaluesthataresubstantiallyhigherthaninthesoupdomain.
Whiletheoptimalrolesstillincludedone“leader”agentwitharelativelylowSPvalueinmostscenarios,both“leader”
and “follower” agents had higher SP values overall. This is probably due to the fact that the agents only need to
complete one dish. Therefore, there is no ambiguity with respect to the agent’s goal in this domain. This would
alreadyreducetheeffectbeliefresonancecanhaveontheagent’sbehavior. Anotherreasoncouldbethateachobject
is available only as often as it is required. This reduces the effect of belief resonance even further as the inferred
intentionoftheotherwilloftendirectlybeinvalidatedbythebottom-upinformation,sinceitisnolongerpossiblefor
theotheragenttoperformthesameintention. Yet,whiletheSPvaluedidnotappeartohavemuchofaneffectonthe
agent’ssuccessrateinboththeTomatoandTomato+Lettucetasks,agentswithlowerSPvaluesperformedbetterwhen
workingontheMixed saladtasks. Thesefindingsprovideadditionalevidencefortheneedforartificialagentstobe
abletosociallyadapttheirroleandcollaborativebehaviordependingonthesituationaswellastheotheragent. Inthe
HAICAmodel,thisadaptationcanbeoperationalizeddirectlybymeansofadjustingthedegreeofbeliefresonance,
i.e.,theSPparameterdynamically.
Finally,wehavearguedthatreal-timecollaborationhingesontheabilityofagentstocoordinatewithothersefficiently
andon-the-fly.Inthisregard,HAICAbearsasignificantadvantageoverotherapproachessuchastherecentBDmodel.
WhileBDagentscanselectactionstopreparethedishesinonlyaminimalnumberofsteps,thehighcomputational
costsofdecisionmakingonaverageleadtoalmosthalfaminuteoftimeneededtodecideonwhichactiontoperform.
Asaresult, theagentsrequiredmuchlongertodeliverthesaladthantheproposedHAICAmodel, whichselectsits
action in a fraction of a second. BD’s efficiency can be greatly improved at the cost of the accuracy of the Q-value
estimation and, in turn, team performance. However, the time differences between BD and HAICA are not solely
a product of BD’s Q-value estimation. Even with only minimal estimation, BD still requires more than 10 times
longerthanHAICAwhileperformingsignificantlyworseatthatpoint.ThisdifferencelikelystemsfromBD’sexplicit
planningapproachthattriestofindthebestpossibleactionsforallagents.
21
ResonatingMinds–EmergentCollaborationThroughHierarchicalActiveInference APREPRINT
Whileefficiencyisnotaconcerninthesyntheticscenariosdiscussedabove,otherreal-worldscenariosordomainsdo
impose time constraints on the agents. For example, a real-world kitchen domain would have dynamics that do not
waitfortheagents’actions,suchasamealburningifcookedtoolong. Anotheraspectwherereal-timecapabilityis
crucialisforhuman-agentinteraction. Notonlywouldahumancollaboratorgetfrustratedwithanagentthatreacts
too slowly, but the human would likely perform multiple actions in the time the agent still considered how to best
respondtothefirstactionitobserved. Thismayresultintheagentplanningwithaninvalidworldstateandmaythus
leadtoseriouscollaborationproblems.
7 Conclusion
Inthispaperwehavestartedtoexploreif“satisficingcollaboration”canemergebetweenagentsthatproducehighly
situated, prediction-based behavior while being affected by what they assume other agents intend to do. We have
proposedamodelforhierarchicalactiveinferenceforcollaborativeagents(HAICA),whichintegratesToM-inferred
mentalstatesofanotheragentintoaprobabilisticandpredictive-processingbasedgoalandintentionformation. Akin
to how sensorimotor processes start to resonate with observed behavior in the human or animal brain, we call this
mechanismbeliefresonance. Crucially,HAICAdoesnotrequireanyspecificmeansofseparatingthementalperspec-
tivesofthetwoagents, orforexplicitlyplanningandcoordinatingtheiractivities. Itthusprovidesaratherminimal
“cognitive infrastructure” for collaborative agents and is to be seen as one building block that needs to be comple-
mented with more powerful cognitive processes for inference and planning. Our goal was to explore what kinds of
collaborativetaskscanbetackledwithsuchaminimalaccountandhowefficientlyandrobustlythiswouldwork.
Simulations with fully implemented versions of HAICA in different versions of the Overcooked domain show that
twoagentsequippedwiththeproposedmodelwereabletopreparemealstogetherinaresource-efficientmanner. We
sawcoordinationbetweenagentsemergewhiletheydynamicallyadaptedtoeachother,withoutengaginginexplicit
coordinationorglobalplanning. Thatis,thecoordinationwasmediatedonlythrough(1)theirco-situatedperceiving
andactinginasharedworld,and(2)theirabilitytounderstandandresonatewitheachother’sactionintermsofthe
intentionsandgoalsthatdriveit.Theimpactofbeliefresonance,fromthepointofviewofthebelief-resonatingagent,
is thereby controlled by a susceptibility parameter (SP) that implies different emerging roles of the agent (leader or
follower). Thisleadstoacoordinationeffectthatismostnotableinsituationswheretheagentshavedifferentaccess
totask-relatedinformation(here,knowingthecurrentorders).
Wewanttoreiteratethatwedonotseetheproposedmodelasageneralsolutiontothecollaborationproblem.Although
our results are comparable to the state of the art in deep reinforcement learning models [11], they are, of course,
worse than those achieved when considering optimal (joint) policies [53]. However, determining such a policy is
infeasible for independent agents and is computationally very demanding even in simple 2D domains as considered
here. Enablingcollaborationwithoutdedicatedpriorcoordinationthusrequiresagentstoadapttoeachotherquickly,
continuously and effectively. Such an adaptation may involve explicitly planned coordination or no coordination
at all depending on the situation. Real-world collaborative systems (e.g., collaborative robots) will likely need to
adapttheirowncoordinationapproachesdependingonthesituationandtheotheragent. Thatis,agentsmayneedto
switchbetweendeliberatingabout(planning)themostsuitableactionsandreactingmore“intuitively”totimecritical
changes. The results presented here provide valuable insights into possible building blocks for this based on which
situatedsystemswithmoresophisticatedcollaborationskillscanbebuilt.
Different improvements are possible to reduce or prevent current shortcomings of the presented model, such as not
taking the other agent’s likely next actions into account when deciding on low-level actions, or not holding on to
selectedintentions. Suchimprovementsmayinvolvemoreexplicitreasoningandplanning. Onecould, e.g., usethe
explicitBRTDPplannerusedin[53]insteadofourbest-firstsearchrealization. Iftheresultingfutureexpectedreward
estimatesarealsousedtocomputetheactionlikelihoodfortheToMmodule,onewouldcreateaHAICAmodelwith
explicitplanningverysimilartoBD.Wewouldexpectsuchamodeltobemorecompetitivewithrespecttotheopti-
malityoftheproducedbehavior. Oneshould,however,alwaystakethecomputationalburdensofmoresophisticated
methodsintoaccount,soasnottoimpedetheoverallsystem’sreal-timecapabilities. Especiallyifweconsiderreal-
world applications and/or human-agent interactions, the real-time capabilities of the developed collaborative agents
becomes crucial. Furthermore, we have only tested the HAICA model in scenarios with two agents. Future work
shouldlookintohowtoapplyHAICAtointeractionsinvolvingmultipleagents. Onewouldlikelyneedtoimplement
attentionmechanismsthatselecttheagenttoresonatewith. Anotherimportantaspectforfutureworkistheneedto
dynamicallydetermineandadaptanagent’sroleincomplexcollaborationscenarios. Dynamicroleadaptationmayin
partbeachievablewithaminimalmodel,e.g.,byallowingbeliefresonancetobevariablysusceptible. Inthissense,
themodelpresentedhereisafirststeptowardselucidatinghowcollaborativebehaviorcanemergefromthedynamic
interplayofprediction-basedperceptionandaction,minimalsocialcognition,andsituationalaffordances.
22
ResonatingMinds–EmergentCollaborationThroughHierarchicalActiveInference APREPRINT
References
[1] Adams, R.A., Shipp, S., Friston, K.J.: Predictionsnotcommands: activeinferenceinthemotorsystem. Brain
StructureandFunction218(3),611–643(2012). DOI10.1007/s00429-012-0475-5
[2] Albrecht, S.V., Ramamoorthy, S.: Ad hoc coordination in multiagent systems with applications to human-
machine interaction. In: Proceedings of the 2013 international conference on Autonomous agents and multi-
agentsystems,pp.1415–1416(2013)
[3] Baker, C., Saxe, R., Tenenbaum, J.: Bayesian theory of mind: Modeling joint belief-desire attribution. In:
Proceedingsoftheannualmeetingofthecognitivesciencesociety,vol.33(2011)
[4] Baker,C.L.,Jara-Ettinger,J.,Saxe,R.,Tenenbaum,J.B.: Rationalquantitativeattributionofbeliefs,desiresand
perceptsinhumanmentalizing. NatureHumanBehaviour1(4),0064(2017). DOI10.1038/s41562-017-0064
[5] Baker,C.L.,Saxe,R.,Tenenbaum,J.B.: Actionunderstandingasinverseplanning. Cognition113(3),329–349
(2009). DOIhttps://doi.org/10.1016/j.cognition.2009.07.005
[6] Barrett, S., Rosenfeld, A., Kraus, S., Stone, P.: Making friends on the fly: Cooperating with new teammates.
ArtificialIntelligence242,132–171(2017). DOIhttps://doi.org/10.1016/j.artint.2016.10.005
[7] Bastos, A., Usrey, W., Adams, R., Mangun, G., Fries, P., Friston, K.: Canonical microcircuits for predictive
coding. Neuron76(4),695–711(2012). DOIhttps://doi.org/10.1016/j.neuron.2012.10.038
[8] Blokpoel,M.,Kwisthout,J.,vanderWeide,T.P.,Wareham,T.,vanRooij,I.: Acomputational-levelexplanation
ofthespeedofgoalinference. JournalofMathematicalPsychology57(3),117–133(2013). DOIhttps://doi.org/
10.1016/j.jmp.2013.05.006
[9] Bosse,T.,Memon,Z.A.,Treur,J.: Arecursivebdiagentmodelfortheoryofmindanditsapplications. Applied
ArtificialIntelligence25(1),1–44(2011). DOI10.1080/08839514.2010.529259
[10] Brandi,M.L.,Kaifel,D.,Bolis,D.,Schilbach,L.:Theinteractiveself–areviewonsimulatingsocialinteractions
tounderstandthemechanismsofsocialagency. i-com18(1),17–31(2019). DOIdoi:10.1515/icom-2018-0018
[11] Carroll, M., Shah, R., Ho, M.K., Griffiths, T., Seshia, S., Abbeel, P., Dragan, A.: On the util-
ity of learning about humans for human-ai coordination. In: H. Wallach, H. Larochelle, A. Beygelz-
imer, F. d’ Alche´-Buc, E. Fox, R. Garnett (eds.) Advances in Neural Information Processing Systems,
vol. 32. Curran Associates, Inc. (2019). URL https://proceedings.neurips.cc/paper/2019/file/
f5b1b89d98b7286673128a5fb112cb9a-Paper.pdf
[12] Chen, S., Andrejczuk, E., A. Irissappane, A., Zhang, J.: Atsis: Achieving the ad hoc teamwork by sub-task
inferenceandselection. In: ProceedingsoftheTwenty-EighthInternationalJointConferenceonArtificialIntel-
ligence,IJCAI-19,pp.172–179.InternationalJointConferencesonArtificialIntelligenceOrganization(2019).
DOI10.24963/ijcai.2019/25
[13] Cisek,P.: Corticalmechanismsofactionselection: theaffordancecompetitionhypothesis. PhilosophicalTrans-
actionsoftheRoyalSocietyB:BiologicalSciences362(1485),1585–1599(2007). DOI10.1098/rstb.2007.2054
[14] Clark,A.: Whatevernext? Predictivebrains,situatedagents,andthefutureofcognitivescience. Behavioraland
BrainSciences36(3),181–204(2013). DOI10.1017/S0140525X12000477
[15] DeWeerdt,M.,Clement,B.: Introductiontoplanninginmultiagentsystems. MultiagentandGridSystems5(4),
345–355(2009)
[16] Dimarogonas, D.V., Johansson, K.H.: Event-triggered control for multi-agent systems. In: Proceedings of the
48hIEEEConferenceonDecisionandControl(CDC)heldjointlywith200928thChineseControlConference,
pp.7131–7136.IEEE(2009). DOI10.1109/CDC.2009.5399776
[17] Friston,K.: Afreeenergyprincipleforbiologicalsystems. Entropy14(11),2100–2121(2012). DOI10.3390/
e14112100
[18] Friston,K.,Frith,C.: Aduetforone. ConsciousnessandCognition36,390–405(2015). DOIhttps://doi.org/10.
1016/j.concog.2014.12.003
[19] Friston,K.J.,Daunizeau,J.,Kilner,J.,Kiebel,S.J.: Actionandbehavior: Afree-energyformulation. Biological
Cybernetics102(3),227–260(2010). DOI10.1007/s00422-010-0364-z
[20] Friston, K.J., Kiebel, S.: Predictive coding under the free-energy principle. Philosophical Transactions of the
RoyalSocietyB:BiologicalSciences364(1521),1211–1221(2009). DOI10.1098/rstb.2008.0300
[21] Games,G.T.: Overcooked(2016)
23
ResonatingMinds–EmergentCollaborationThroughHierarchicalActiveInference APREPRINT
[22] Gergely,G.,Csibra,G.:Teleologicalreasoningininfancy:thenaivetheoryofrationalaction.TrendsinCognitive
Sciences7(7),287–292(2003). DOIhttps://doi.org/10.1016/S1364-6613(03)00128-1
[23] Herbort,O.,Butz,M.V.,Hoffmann,J.:TowardstheAdvantagesofHierarchicalAnticipatoryBehavioralControl.
In: ProceedingsoftheKogWis05,pp.77–82.Basel(2005)
[24] Huang, C.M., Thomaz, A.L.: Effects of responding to, initiating and ensuring joint attention in human-robot
interaction. In: 2011Ro-Man,pp.65–71.IEEE(2011). DOI10.1109/ROMAN.2011.6005230
[25] Jara-Ettinger,J.,Schulz,L.E.,Tenenbaum,J.B.: Thenaiveutilitycalculusasaunified,quantitativeframework
for action understanding. Cognitive Psychology 123, 101334 (2020). DOI https://doi.org/10.1016/j.cogpsych.
2020.101334
[26] Jern, A., Lucas, C.G., Kemp, C.: People learn other people’s preferences through inverse decision-making.
Cognition168,46–64(2017). DOIhttps://doi.org/10.1016/j.cognition.2017.06.017
[27] Kahl,S.:SocialMotorics-apredictiveprocessingmodelforefficientembodiedcommunication.Diss.,Bielefeld
University,Bielefeld,Germany(2020). DOI10/ghbm58
[28] Kahl,S.,Kopp,S.: Apredictiveprocessingmodelofperceptionandactionforself-otherdistinction. Frontiers
inPsychology9,2421(2018). DOI10.3389/fpsyg.2018.02421
[29] Kalman,R.E.,Bucy,R.S.: NewResultsinLinearFilteringandPredictionTheory. JournalofBasicEngineering
83(1),95–108(1961). DOI10.1115/1.3658902
[30] Kilner, J.M., Friston, K.J., Frith, C.D.: Predictive coding: an account of the mirror neuron system. Cognitive
Processing8(3),159–166(2007). DOI10.1007/s10339-007-0170-2
[31] Knoblich,G.,Butterfill,S.,Sebanz,N.: Psychologicalresearchonjointaction: Theoryanddata. Psychologyof
LearningandMotivation54,59–101(2011). DOIhttps://doi.org/10.1016/B978-0-12-385527-5.00003-6
[32] Masters,P.,Sardina,S.: Cost-basedgoalrecognitionforpath-planning. In: Proceedingsofthe16thConference
onAutonomousAgentsandMultiAgentSystems,pp.750–758(2017)
[33] McMahan,H.B.,Likhachev,M.,Gordon,G.J.: Boundedreal-timedynamicprogramming: Rtdpwithmonotone
upper bounds and performance guarantees. In: Proceedings of the 22nd International Conference on Machine
Learning,ICML’05,pp.569—-576.AssociationforComputingMachinery,NewYork,NY,USA(2005). DOI
10.1145/1102351.1102423
[34] Misyak, J.B., Melkonyan, T., Zeitoun, H., Chater, N.: Unwritten rules: virtual bargaining underpins social
interaction,culture,andsociety. TrendsinCognitiveSciences18(10),512–519(2014). DOIhttps://doi.org/10.
1016/j.tics.2014.05.010
[35] Mutlu,B.,Terrell,A.,Huang,C.M.: Coordinationmechanismsinhuman-robotcollaboration. In: Proceedings
of the Workshop on Collaborative Manipulation, 8th ACM/IEEE International Conference on Human-Robot
Interaction,pp.1–6.Citeseer(2013)
[36] Nikolaidis, S., Hsu, D., Srinivasa, S.: Human-robot mutual adaptation in collaborative tasks: Models and
experiments. The International Journal of Robotics Research 36(5-7), 618–634 (2017). DOI 10.1177/
0278364917690593
[37] Po¨ppel,J.,Kopp,S.: Satisficingmodelsofbayesiantheoryofmindforexplainingbehaviorofdifferentlyuncer-
tainagents. In: Proceedingsofthe17thinternationalconferenceonautonomousagentsandmultiagentsystems,
pp.470–478(2018)
[38] Premack,D.,Woodruff,G.: Doesthechimpanzeehaveatheoryofmind? BehavioralandBrainSciences1(4),
515–526(1978)
[39] Pynadath,D.V.,Si,M.,Marsella,S.C.: Modelingtheoryofmindandcognitiveappraisalwithdecision-theoretic
agents. Socialemotionsinnatureandartifact: emotionsinhumanandhuman-computerinteractionpp.70–87
(2011)
[40] Rabinowitz, N., Perbet, F., Song, F., Zhang, C., Eslami, S.M.A., Botvinick, M.: Machine theory of mind. In:
J. Dy, A. Krause (eds.) Proceedings of the 35th International Conference on Machine Learning, Proceedings
of Machine Learning Research, vol. 80, pp. 4218–4227. PMLR (2018). URL https://proceedings.mlr.
press/v80/rabinowitz18a.html
[41] Rao, R.P., Ballard, D.H.: Predictive coding in the visual cortex: A functional interpretation of some extra-
classicalreceptive-fieldeffects. NatureNeuroscience2(1),79–87(1999). DOI10.1038/4580
[42] Rich, C., Sidner, C.L.: Collaborative discourse, engagement and always-on relational agents. In: 2010 AAAI
FallSymposiumSeries.Citeseer(2010)
24
ResonatingMinds–EmergentCollaborationThroughHierarchicalActiveInference APREPRINT
[43] Schu¨tz-Bosbach,S.,Prinz,W.: Perceptualresonance: action-inducedmodulationofperception. TrendsinCog-
nitiveSciences11(8),349–355(2007). DOIhttps://doi.org/10.1016/j.tics.2007.06.005
[44] Song, Y., Wang, J., Lukasiewicz, T., Xu, Z., Xu, M.: Diversity-driven extensible hierarchical reinforcement
learning. Proceedings of the AAAI Conference on Artificial Intelligence 33(01), 4992–4999 (2019). DOI
10.1609/aaai.v33i01.33014992
[45] Stone,P.,Kaminka,G.A.,Kraus,S.,Rosenschein,J.S.: Adhocautonomousagentteams: Collaborationwithout
pre-coordination. In: Twenty-FourthAAAIConferenceonArtificialIntelligence(2010)
[46] Todorov, E., Jordan, M.I.: Optimal feedback control as a theory of motor coordination. Nature Neuroscience
5(11),1226–1235(2002). DOI10.1038/nn963
[47] Tomasello,M.,Carpenter,M.,Call,J.,Behne,T.,Moll,H.:Understandingandsharingintentions:Theoriginsof
culturalcognition. BehavioralandBrainSciences28(5),675—-691(2005). DOI10.1017/S0140525X05000129
[48] Torren˜o,A.,Onaindia,E.,Komenda,A.,Sˇtolba,M.: Cooperativemulti-agentplanning: Asurvey. ACMCom-
putingSurveys(CSUR)50(6),1–32(2017). DOI10.1145/3128584
[49] Vered,M.,Kaminka,G.A.,Biham,S.: Onlinegoalrecognitionthroughmirroring: Humansandagents. In: The
FourthAnnualConferenceonAdvancesinCognitiveSystems(2016)
[50] Vesper, C., Abramova, E., Bu¨tepage, J., Ciardo, F., Crossey, B., Effenberg, A., Hristova, D., Karlinsky, A.,
McEllin, L., Nijssen, S.R.R., Schmitz, L., Wahn, B.: Joint action: Mental representations, shared information
and general mechanisms for coordinating with others. Frontiers in Psychology 7, 2039 (2017). DOI 10.3389/
fpsyg.2016.02039
[51] Wolpert, D.M., Ghahramani, Z., Jordan, M.I.: An internal model for sensorimotor integration. Science
269(5232),1880–1882(1995). DOI10.1126/science.7569931
[52] Wu,F.,Zilberstein,S.,Chen,X.: Onlineplanningformulti-agentsystemswithboundedcommunication. Artifi-
cialIntelligence175(2),487–511(2011). DOIhttps://doi.org/10.1016/j.artint.2010.09.008
[53] Wu, S.A., Wang, R.E., Evans, J.A., Tenenbaum, J.B., Parkes, D.C., Kleiman-Weiner, M.: Too many cooks:
Bayesian inference for coordinating multi-agent collaboration. Topics in Cognitive Science 13(2), 414–432
(2021). DOIhttps://doi.org/10.1111/tops.12525
25

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Resonating Minds -- Emergent Collaboration Through Hierarchical Active Inference"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
