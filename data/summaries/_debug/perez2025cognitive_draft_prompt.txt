=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Cognitive Effort in the Two-Step Task: An Active Inference Drift-Diffusion Model Approach
Citation Key: perez2025cognitive
Authors: Alvaro Garrido Perez, Viktor Lemoine, Amrapali Pednekar

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Key Terms: task, approach, effort, university, drift, information, cognitive, step, inference, active

=== FULL PAPER TEXT ===

Cognitive Effort in the Two-Step Task: An Active
Inference Drift-Diffusion Model Approach
Álvaro Garrido-Pérez1[0009−0003−5481−8166], Viktor Lemoine1, Amrapali
Pednekar1[0009−0005−6194−3955], Yara Khaluf2[0000−0002−5590−9321], and Pieter
Simoens1[0000−0002−9569−9373]
1 IDLab, Department of Information Technology, Ghent University - imec, Belgium
alvaro.garridoperez@ugent.be
2 Wageningen University & Research, Wageningen, The Netherlands
Abstract. High-leveltheoriesrootedintheBayesianBrainHypothesis
often frame cognitive effort as the cost of resolving the conflict between
habits and optimal policies. In parallel, evidence accumulator models
(EAMs)provideamechanisticaccountofhoweffortarisesfromcompeti-
tionbetweenthesubjectivevaluesofavailableoptions.AlthoughEAMs
have been combined with frameworks like Reinforcement Learning to
bridgethegapbetweenhigh-leveltheoriesandprocess-levelmechanisms,
relatively less attention has been paid to their implications for a unified
notionofcognitiveeffort.Here,wecombineActiveInference(AIF)with
the Drift-Diffusion Model (DDM) to investigate whether the resulting
AIF-DDMcansimultaneouslycapturetheeffortarisingfrombothhabit
violation and value discriminability. To our knowledge, this is the first
time AIF has been combined with an EAM. We tested the AIF-DDM
on a behavioral dataset from the two-step task and compared its pre-
dictions to an information-theoretic definition of cognitive effort based
onAIF.Themodel’spredictionssuccessfullyaccountedforsecond-stage
reaction times but failed to capture the dynamics of the first stage. We
argue the latter discrepancy likely stems from the experimental design
ratherthanafundamentalflawinthemodel’sassumptionsaboutcogni-
tiveeffort.Accordingly,weproposeseveralmodificationsofthetwo-step
tasktobettermeasureandisolatecognitiveeffort.Finally,wefoundthat
integrating the DDM significantly improved parameter recovery, which
could help future studies to obtain more reliable parameter estimates.
Keywords: Active inference · Drift-diffusion model · Cognitive effort
1 Introduction
Building upon the Bayesian Brain Hypothesis (BBH), numerous studies in the
past decade have tried to formalize cognitive effort using information-theoretic
principles[25].AccordingtotheBBH,humansmaintainaninternalworldmodel
encoded in prior beliefs, which they continuously update as they interact with
the environment. Within this context, cognitive effort arises from the conflict
5202
peS
21
]CN.oib-q[
2v53440.8052:viXra
2 A. Garrido-Pérez et al.
between a pre-existing belief about how to act (a habit) and an updated belief
about the optimal policy [10].
In a decision-making task, cognitive effort may also arise from the competi-
tion between the subjective values of the available choices. When these values
are closer together—that is, when value discriminability is low—reaction times
(RTs)tendtoincrease(e.g.,[7,3]).AlthoughRTsarenotadirectmeasureofcog-
nitiveeffort,theyareoftenusedasaproxy(e.g.,[24]),basedontheassumption
that slower responses reflect more information processing.
From an information-theoretic perspective, the effect of value discriminabil-
ity on RTs can be understood as the additional cognitive effort required to re-
solve increased choice uncertainty [13,8,6]. Yet, information theory provides no
explicit account of the underlying deliberation process, which complicates the
task of linking its predictions to specific neural signatures. In contrast, evidence
accumulatormodels(EAMs)offeramechanisticexplanationforhowvaluecom-
petition shapes decision speed, which may be empirically tested (e.g., [18]).
A major limitation of using EAMs to study cognitive effort is that they are
agnostic to how beliefs are formed. This limitation is often addressed by com-
bining EAMs with Reinforcement Learning (RL) [11]. However, in recent years,
Active Inference (AIF) has emerged as a powerful alternative to RL, offering a
first-principles perspective on perception, learning, and decision-making [5].
Here, we investigate whether integrating a Drift Diffusion Model (DDM),
a prominent class of EAM, with AIF can simultaneously capture the influence
of both value discriminability and habit violation on cognitive effort. To our
knowledge, this is the first attempt to combine AIF with an EAM to model
humanbehaviour.WeevaluatetheintegratedAIF-DDMusingthetwo-steptask,
a version of the multi-armed bandit in which participants must plan two steps
ahead [2]. Furthermore, we compare its predictions with a recently proposed
definition of cognitive effort in AIF [10].
Our work builds directly on a recent study that developed an AIF model of
the two-step task [4]. The study demonstrated that AIF outperformed a Hybrid
Reinforcement Learning (HRL) model (which combines model-free and model-
based strategies [16]) in two out of four datasets, while achieving comparable
performance in the remaining two. Moreover, the authors provided compelling
evidence for directed exploration—a key differentiator between AIF and HRL.
Despite these achievements, the study could not determine which specific AIF
learningmechanismsparticipantswereusing.Givenpriorevidencethatintegrat-
ingaDDMintoanHRLmodelimprovedthereliabilityofmodel-basedestimates
[16], we tested whether the combined AIF–DDM could resolve this ambiguity.
2 Methods
2.1 Participants and behavioural task
In this section, we will briefly describe the two-step task and the behavioural
dataset that we used to fit the computational models. For more details on the
Cognitive Effort in the Two-Step Task 3
experimental procedure, we encourage reading the paper that made the dataset
publicly available [17].
As the name suggests, each trial of the two-step task consists of two stages
(Fig. 1). In the first stage, participants choose between two actions. Each action
leadstooneofthetwosecond-stagestatesthroughaprobabilistictransitionthat
is either common (p = 0.7) or rare (p = 0.3). Importantly, the two first-stage
actions have opposite most-likely transitions. These transition probabilities re-
main constant throughout the task. After transitioning to a second-stage state,
participantsmakeafinalchoicebetweentwoactions.Eachofthesesecond-stage
actionsresultsinamonetaryrewardornoreward,dependingonitscurrentout-
come probability.Incontrasttothefixedtransitions,theseoutcomeprobabilities
fluctuate independently over time, following Gaussian random walks.
Fig.1: Abstract representation of the two-step task. At the first stage (top), a
choice leads to one of two second-stage states (bottom). Transitions are either
common (p=0.7, thick arrows) or rare (p=0.3). The two initial actions have
opposing common transitions. A second-stage choice may result in a monetary
reward with a probability that fluctuates over time.
We analysed data from the "Magic Carpet" dataset, which was made avail-
ableby[17]andoriginallycomprised24participants.Inthisexperiment,partici-
pantshada2-seconddeadlinetorespondateachstage.Everytimethisdeadline
wassurpassed,thetrialwaslabelledasamissed trialandtheparticipantmoved
on to the next stage or trial. Before conducting any analysis, we identified and
removedthesetrials.WealsoconsideredinvalidtrialsthosewithatleastoneRT
smallerthan100ms(toexcludeanticipatoryresponsesthataretoofasttoreflect
genuine deliberation [9]). Finally, any participant with more than 10% invalid
trials was removed from the dataset. One participant was excluded under this
criterion, with 44.1% of their trials being either missed or too fast. Therefore,
the original sample size was reduced to n=23. In total, 6.53% of the data was
excludedfromtheanalysis(includingallthedatafromtheremovedparticipant).
4 A. Garrido-Pérez et al.
2.2 An active inference drift-diffusion model of the two-step task
Inthissection,weintroducetheAIFmodeldevelopedby[4]anddiscusshowwe
integrate it with a DDM. Further explanations of the equations are provided in
Appendix A. To establish a comparative benchmark, we implemented an HRL-
DDM.Sincethelattermodelissecondarytoourmainanalysis,itsmathematical
formulation is only detailed in Appendix B.
In what follows, we will use the same notation as in [4]. For a given trial t,
thefirst-stagestateandchosenactionsaredenotedbys anda ,respectively.
1,t 1,t
Likewise, the second-stage state and chosen action are denoted by s and a ,
2,t 2,t
respectively.Notethatthefirst-stagestatecanonlybes ,butthesecond-stage
A
state may be s or s , depending on the transition (see Fig. 1). Outcomes
B C
o ∈ {0,1} represent the observed reward (or absence of it) after choosing a
t
second-stage action.
According to the AIF model developed by [4], an agent performing the two-
steptaskisequippedwithagenerativemodel(i.e.,asetofbeliefsabouthowthe
task environment evolves given its actions). In the two-step task, agents must
learn the outcome probabilities of each of the four second-stage actions. We will
refer to the set of these four probabilities as θ, and the belief distribution over
θ, at a given trial t, π (θ). In addition, the agent must learn the four transition
t
probabilitiesp(s |s ,a ).However,aspointedoutby[4],thesetransitionsare
2,t 1,t 1,t
accurately learned in a few trials and therefore, action-selection is only sensitive
to information regarding outcome probabilities.
Thus, an agent performing the two-step task must optimally balance getting
as many rewards as possible (exploitation) and learning the four outcome prob-
abilities (exploration). According to AIF, this balance is achieved by selecting
actions that minimize a quantity known as Expected Free Energy (EFE), which
in this case is given by the following equation:
G (a)=−E [lnp(o |C)]−E [D (π (θ)|o ,a∥π (θ))] (1)
t p(ot;πt(θ)|a) t p(ot;πt(θ)|a) KL t t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Realisingpreferences Modelparameterexploration
(Exploitation) (ActiveLearning)
WhereG (a)istheEFEofagivenactionatagiventrial,p(o |C)isthedistri-
t t
butionoverpriorpreferredobservations(Eq.5,AppendixA),whichdependson
a free parameter λ. Note that in the two-step task, the preferred observation is
togettherewardafterasecond-stagechoice.D istheKullback-Leiblerdiver-
KL
gence between prior beliefs about second-stage outcome probabilities π (θ) and
t
posteriorbeliefsafterselectinganactionandobservingitsoutcome.p(o ;π (θ)|a)
t t
isadistributionequaltop(o |θ,a)π (θ).Notethat,unliketraditionalEFEformu-
t t
lations, there is no hidden state exploration term because in the real experiment
participants can always see the state in which they are [15].
Eq. 1 can be used to calculate EFEs for second-stage actions. However, for
first-stage actions, the level of optimality depends upon the EFEs of the final
states and the transition probabilities. Therefore, for the first-stage actions, the
EFE equation is given by:
Cognitive Effort in the Two-Step Task 5
(cid:88) (cid:88)
G(a )=p(s |s ,a ) G(a ) + p(s |s ,a ) G(a ) (2)
j B A j 2 C A j 2
a2∈AB a2∈AC
Where A and A are the sets of available actions in the second-stage states
B C
s and s respectively, and G(a ) are the second-stage EFEs given by Eq. 1.
B C 2
Previous studies have reported a tendency for participants to repeat initial-
stage actions, regardless of the outcome history [4]. Within AIF, this behaviour
is formalised through a habit term E. In the model of [4], E is parameterised
by κ, which modulates the extent of an agent’s reliance on habits (see Eq. 6,
Appendix A).
At the first stage, an agent will select an action that minimizes the EFE
corrected by the habitual bias Gnet(s ,a )=G (s ,a )+E(a ). At the second
t 1 1 t 1 1 1
stage, no habitual bias is assumed, and therefore Gnet(s ,a )=G (s ,a ).
t 2 2 t 2 2
For the original AIF model [4], the choice probabilities of an agent at trial
t, stage p = {1,2} and state s, will be given by the softmax distribution of the
corresponding net EFEs parametrized by an inverse temperature parameter, γ
p
(one for each stage). However, for the AIF-DDM the choice will be determined
by a drift-diffusion process, with a non-decision time t , boundary separation
nd
a and a drift rate v , that depends on the difference in Gnet of the two
bs p,s,t
available actions such that:
v =vmod[Gnet(s,a)−Gnet(s,a′)], a,a′ ∈A (3)
p,s,t p t t s
Where A is the set of available actions at state s, and vmod is a free pa-
s p
rameter that regulates the sensitivity to Gnet differences (which can also be
interpreted as the agent’s information-processing speed). Following the original
AIF model [4], we fit a separate vmod parameter for each stage, an approach
p
analogous to the use of stage-specific inverse temperatures.
In the classical DDM, a starting-point bias parameter z is often included.
However,inthetwo-steptask,thesymbolsrepresentingthedifferentactionswere
randomly displayed either on the left or the right of the screen [17]. Therefore,
we set z =0.5 (effectively cancelling the bias in the drift-diffusion process).
After completing every trial, the agent updates its beliefs about the second-
stage outcome probabilities. In [4], the updating rules for the prior distribution
over outcome probabilities (Eqs. 8 and 9 in Appendix A), depend on four free
parameters: the learning rate l, the prior volatility, v , which modulates the
PS
influence of surprise on the agent’s beliefs, volatility of sampled actions v ,
SD
whichmodulateshowbeliefsoverchosenactionsdecay,orareforgotten,andthe
volatility of unsampled actions v , similar to v but for unchosen actions.
UD SD
The study by [3] tested four AIF variants distinguished by their learning
rules: a No Unsampled-Decay model (AIF , v = 0); a No Sampled-Decay
NUD UD
model (AIF , v = 0), a No Predictive Surprise model (AIF , v = 0)
NSD SD NPS PS
and a model including all the learning mechanisms (AIF ). Although solid
FULL
evidence was found favouring AIF over HRL, the best-fitting AIF variant could
not be determined [4]. Consequently, we fitted all four models in our analysis.
6 A. Garrido-Pérez et al.
2.3 Cognitive effort and active inference
A recent formalisation of cognitive effort (ξ) based on AIF defines it as the
KL divergence between context-sensitive beliefs about how to act P (π) and
G
context-insensitive prior beliefs, or habits P (π) [10]:
E
ξ ≜D [P (π)||P (π)]=E [lnP (π)]− E [lnP (π)] (4)
KL G E PG G PG E
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Effort Contextsensitive Contextinsensitive
P (π)=Cat(σ(−E)) P (π)=Cat(σ(−G−E))
E G
Where G and E are vectors comprising the context-sensitive EFEs and the
context-insensitive priors (or habits) of the available actions, respectively.
From Eq. 4, two key predictions can be derived. First, high effort will be
experienced when there is an incongruence between G and E. Second, when
the elements of G are of similar magnitude to one another, cognitive effort is
minimal regardless of E.
Forthefirststageofthetwo-steptask,theAIF-DDMalignswiththefirstpre-
dictionofEq.4.Forachoicebetweenactionsa anda ,withG=[G ,G ],and
A B A B
E=[E ,E ], RTs should increase as the magnitude of the drift rate decreases,
A B
whichisproportionalto|∆Gnet|=|G +E −G −E |.Therefore,higherRTs
A A B B
may occur when, for example |G | ≫ |G |, |E | ≪ |E | and |G | ≈ |E |. In
A B A B A B
other words, when there is an incongruence between G and E.
However, our model contradicts the second prediction of Eq. 4, because for
a constant E, RTs should increase as |G | gets closer to |G |. Interestingly,
A B
for the second stage, Eq. 4 predicts minimal effort if we assume no habits (i.e.
E = E = 0), in contrast to AIF-DDM, where effort can still be high if the
A B
context-sensitive EFEs are closely matched.
2.4 Model fitting and comparison procedures
WefollowedthesameMaximumLikelihoodEstimate(MLE)procedureasin[4],
tofitthemodels’freeparameterstothebehaviouraldata3.Foreachparticipant
andmodel,wefoundtheparametersetwiththehighestlikelihoodusingScipy’s
’L-BFGS-B’ algorithm [19]. This step was repeated 35 times for each partici-
pant, with different (uniformly) randomized initializations for all parameters.
After completing all the runs, we selected the parameter set with the maximum
likelihood and used its value for model comparison.
For the pure AIF and HRL models, we computed the likelihoods using the
choice probability distributions of each model. For AIF-DDM and HRL-DDM
however, the likelihoods were given by the Wiener’s First-Passage Time Distri-
bution (WFPT) provided by the HDDM Python package [22]. Further details
can be found in Appendix C.
3 Code available at https://github.com/decide-ugent/aif-ddm
Cognitive Effort in the Two-Step Task 7
For model comparison, we relied on the Statistical Parametric Mapping
(SPM)[21]andVariationalBayesianAnalysis(VBA)[1]toolboxes,usingMAT-
LAB R2022b. We performed a group-level random-effect Bayesian model selec-
tion (BMS) procedure, which requires the log-model evidence (LME) of each
modelforeachparticipant.WeusedtwoLMEapproximations:theBayesianIn-
formation Criterion (BIC) and the Akaike’s Information Criterion (AIC) scores
[12](seeAppendixC).UsingtheBMSprocedure,weestimatedfourmodelcom-
parisonmetrics:theExpectedPosteriorProbability(EPP);theEstimatedModel
Frequency (EMF), or the proportion of participants best fit by the model; the
Exceedance Probability (EP), which quantifies the likelihood that the model is
more frequent than competing models across participants; and the Protected
Exceedance Probability (PEP), similar to EP but accounting for the possibility
that apparent differences in model frequencies arise due to chance [14].
The SPM toolbox allows comparing the performance of model families (i.e.,
models that share a common feature). As in [4], we use this feature to com-
pare the overall AIF performance (considering its four variants) to HRL. Since
PEPs are unavailable for family-level inference, we only calculate EPs for these
analyses.
2.5 Model and parameter recovery procedures
To perform the parameter recovery analysis, we simulated a dataset for each
model using parameter values sampled from uniform distributions with ranges
equal to those used to fit the real dataset. Next, we fitted the synthetic dataset
andobtainednew(recovered)parameters.Finally,wecheckediftheoriginaland
recoveredparameterswerecorrelatedbycalculatingPearson’scorrelations.This
process was repeated 23 times (equal to the number of participants in the real
experiment) for each model.
For the model recovery analysis, each model was first used to simulate a full
experimentaldatasetwiththesameparametersetsasintheparameterrecovery
analysis. We then fitted all models to each simulated dataset and calculated
the proportion of times each model was identified as best-fitting. Results were
summarized in a confusion matrix (Fig. 2).
3 Results
3.1 Model recovery analysis
ModelrecoveryanalysisindicatesthatAIF-DDM isfrequentlymisclassified
FULL
aseitherAIF-DDM orAIF-DDM (Fig.2).Thepoorrecoverylikelystems
NSD NSD
from the full model encompassing all update rules present in the other two
models [4]. The rest of the models had a better but modest recovery (except
HRL-DDM, which had a perfect recovery). Thus, combining AIF with a DDM
could not help substantially to differentiate between different learning variants
as we hypothesised. Overall, both AIC and BIC scores show similar results.
Although the AIC score seems slightly more reliable since it could achieve a
better recovery for both AIF-DDM and AIF-DDM .
FULL NUD
8 A. Garrido-Pérez et al.
Fig.2:Modelrecoveryresults.Eachcellcontainsthefractionofsimulatedexper-
iments from a given true model, for which a corresponding model was classified
as the best-fitting, according to the BIC score (left) or the AIC score (right).
3.2 Model comparison results
TheBMSanalysisrevealeddifferentresultsdependingonthescoreused.Accord-
ing to the BIC score, AIF-DDM and AIF-DDM dominated moderately
NSD NPS
across participants with Expected Posterior Probabilities, EPP = [0.39,0.31],
Protected Exceedance Probabilities, PEP = [0.63,0.29] and Estimated Model
Frequencies, EMF = [0.44,0.35], respectively (see Fig. 5, first row). However,
according to the AIC scores AIF-DDM was the best-fitting model with
FULL
EPP=0.57,PEP=0.97andEMF=0.7(seeFig.5,secondrow).Thedisagree-
ment between AIC and BIC scores was also reported for the model comparison
between pure HRL and AIF models [4], and likely stems from the fact that BIC
penalizes complexity more than AIC.
Even though the two BMS procedures produced conflicting results, we se-
lected AIF-DDM as the best-fitting model, and focus exclusively on its
FULL
results in the subsequent sections. This decision was based on the model re-
covery analysis, which showed that the full model is frequently misclassified as
either AIF-DDM or AIF-DDM . Nonetheless, we acknowledge that the
NSD NPS
BIC-based comparison favoured AIF-DDM and AIF-DDM , so we repli-
NSD NPS
cated our analysis for these models and show their results in Appendix D. We
excluded AIF-DDM from further analysis as it performed poorly in both
NUD
BMS procedures. Finally, the model family selection analysis revealed similar
resultstothosereportedinthestudythatcomparedpureAIFandHRL[4].We
found that the AIF-DDM family outperformed the HRL-DDM model according
to both BIC (EP = 1, EMF = 0.9, EPP = 0.88) and AIC (EP = 1, EMF =
0.92, EPP = 0.9) scores.
Cognitive Effort in the Two-Step Task 9
3.3 Parameter recovery analysis
IntegratingaDDMwithAIFsubstantiallyimprovedtherecoveryoftheparame-
terssharedbetweenAIF-DDMandpureAIFmodels(seeAppendixE,Table1).
ForthepureAIFmodels,parameterrecoverywasfarfromperfect.Forexample,
for AIF , only v had a Pearson’s correlation greater than .6 between its
FULL DU
ground-truth and recovered values. In contrast, for the AIF-DDM models, v ,
DU
v , v , a and t showed excellent recovery (r > .90, p < .001), l and κ
DS PS bs nd
were well recovered (r > .84, p < .001) and p and λ had a moderate recovery
r
(r >.67, p<.001). The drift rate parameters vmod and vmod had moderate-to-
1 2
poor recovery, depending on the specific model. Nonetheless, their recovery was
stillsuperiortothatoftheinversetemperatureparameters(γ andγ )fromthe
1 2
pure AIF model.
3.4 Expected free energy discriminability affects second stage, but
not first stage reaction times
To evaluate the models’ goodness-of-fit and predictive accuracy, we compared
model-simulated behaviour on the two-step task to the observed behaviour of
participants.Thiswasdoneusingadecile-binnedanalysisbasedontheabsolute
differenceinthenetEFEbetweenthetwoactions,|∆Gnet|.ForeachAIF-DDM
model (except AIF-DDM ), we first computed the trial-by-trial |∆Gnet| for
NUD
eachparticipantforbothstages,basedontheirbest-fitmodelparameters,choice
history, and the experienced sequence of rewards and transitions. From the re-
sulting distribution of |∆Gnet| values, for each participant and stage, we iden-
tified the decile boundaries (10th-90th percentiles). We then binned each trial’s
observedRTandchoiceintooneoftendecilebinsaccordingtoits|∆Gnet|value.
For example, a trial with a |∆Gnet| value below the 10th percentile was placed
in the first bin (0th-10th). Within each bin, we calculated two metrics for each
participant: (1) the mean RT, and (2) the probability of choosing the action
with the lower net EFE, P(choose Gnet). These participant-level metrics were
min
thenaveragedacrossparticipantsforeachdecilebin.Finally,foreachtrialstage,
we generated 100 simulations to estimate the model’s predicted RT and choice
probability.Thesesimulatedmetricswerethenbinnedandaveragedinthesame
way as the observed data.
The simulation analysis results were qualitatively similar across all models,
therefore we only display the predictions of the AIF-DDM model as an
FULL
example in Fig. 3. Equivalent plots for the remaining models are presented in
theAppendixF.Inbothstages,participantsweremorelikelytoselecttheaction
with the minimum net EFE as the value of |∆Gnet| increased. This trend was
well-captured by all the AIF-DDM models (see Appendix F).
The AIF-DDM models also predict that the mean RT should decrease as a
functionofEFEdiscriminability(i.e.,as|∆Gnet|increases)forbothstages.This
effect can be observed for the second stage; however, it is almost negligible for
the first one (Fig. 3, bottom left).
10 A. Garrido-Pérez et al.
Fig.3: Comparison of observed (blue) and model-predicted (orange) behaviour.
Lines show the mean choice probability (top) and mean reaction time (bot-
tom)acrossparticipants,binnedbynetExpectedFreeEnergy(|∆Gnet|)deciles.
Shaded areas are ±1 standard error of the mean.
Multiple explanations may account for the mismatch in first-stage RTs. The
firstconcernsthe2-seconddeadline.AsdescribedinSection2.1,allmissedtrials
were excluded from the analysis, effectively truncating the observed RT distri-
bution. In contrast, the AIF-DDM models do not impose this time restriction,
resulting in higher predicted RT means (see Appendix G for further details).
However, this effect alone is unlikely to fully explain the misfit, since the second
stage had the same deadline, yet a far less pronounced mismatch.
A second possible explanation stems from the well-documented tendency for
participants to repeat their previous first-stage choice [4]. In both AIF(-DDM)
and HRL(-DDM), a habit term is included only in the first-stage equations. It
is therefore plausible that, for most first-stage choices, the habit strength of the
previously selected option is sufficient to produce fast responses—even in trials
falling into the lowest |∆Gnet| deciles. In contrast, with the lack of a habit term
in the second-stage equations, |∆Gnet| values may be smaller or more variable.
This effect was particularly notable for the participant with the largest fitted κ
value (where κ regulates the tendency to repeat the previous first-stage choice).
However, it was not observed for the rest of the participants (see Fig. 4).
A third potential explanation is that a substantial portion of the cognitive
effort in the initial stage may arise from mental simulation or partial explo-
ration of the decision tree, a process not accounted for by the assumption that
RTs depend solely on |∆Gnet|. Nevertheless, the limited complexity of the task
(involving only four possible branches) constrains the extent of such planning.
Cognitive Effort in the Two-Step Task 11
Fig.4: Observed reaction time (RT) vs. |∆Gnet| for the AIF-DDM model,
FULL
separated by task stage. Each circle represents a single trial. Trials from the
’sticky’ participant, identified by the highest fitted κ value, are shown in red.
Trials from all other participants are shown in blue. For the sticky participant,
notice the cluster of first-stage trials with high |∆Gnet| values compared to the
second-stage trials that are more concentrated at low |∆Gnet| values.
3.5 The lack of net EFE discriminability effect on first-stage
reaction times might be explained by the experimental design
Afinalpotentialexplanationforthelackof|∆Gnet|effectonfirst-stageRTsisthe
experimental design. In the trial sequence of the two-step task, the elapsed time
fromthemomentparticipantsmakethesecond-stagechoiceoftheprevioustrial
until they are presented with the first-stage options of the next trial is variable
and long (3.2-3.8s). This time interval could allow participants to engage in
"pre-thinking" about their upcoming first-stage choice before the options are
presented. As a result, the recorded first-stage RT may only capture the final
execution of an already made decision, rather than the full deliberation process.
In contrast, the interval between the first- and second-stage choices is fixed and
shorter(2s),leavinglessroomforpre-thinkingandmakingthesecond-stageRT
a more reliable measure of cognitive effort.
The difference in inter-choice intervals could also explain why mean first-
stageRTsareshorterthanmeansecond-stageRTs(seeFig.3),whichmayseem
counterintuitive since, according to AIF, participants in the first stage should
engage in planning, a more costly cognitive process than the one-shot second-
stage decision.
4 Discussion
In this study, we introduced a novel AIF-DDM and evaluated its predictions in
thetwo-steptask.Wediscussedthedifferencebetweenthemodelpredictionsand
arecentlydevelopedinformation-theoreticformalismofcognitiveeffortbasedon
AIF (Eq. 4, [10]).
Our model predicts that both the need to overcome a pre-existing habit and
the difficulty of discriminating between options with similar subjective values
(EFEs) increase cognitive effort during the first stage of the two-step task. We
12 A. Garrido-Pérez et al.
arguethatforthefirststage,botheffectscanberegardedasasinglecompetition
process between the ’habit-corrected’ or net EFEs of the available options. For
the second stage, habits are assumed to play no role, and therefore, the model
predicts that cognitive effort only depends on the discriminability between pure
context-sensitive EFEs.
We found empirical evidence that EFE discriminability affects second-stage
RTs of the two-step task as predicted by the AIF-DDM model – an effect that
is not captured by the information-theoretic formalism. However, we found that
the discriminability between the habit-corrected EFEs had a negligible effect
on the first-stage RTs, contrary to the AIF-DDM predictions. We argue that
the latter observation could be a result of the long inter-trial intervals that
may allow participants to "pre-think" their choice before being presented with
the options. Thus, first-stage RTs may only reflect motor execution, not the
preceding cognitive effort.
Lastly,wefoundthatintegratingaDDMwithAIFdidnothelptodistinguish
between different learning mechanisms proposed by [4]. However, the DDM in-
tegrationimprovedparameterrecoverycomparedtopureAIF.Thelatterresult
was expected, since the same effect has been reported for HRL in the two-step
task [16]. Nonetheless, the result might be relevant for computational psychia-
try studies interested in finding the link between certain AIF parameters and
specific pathologies.
Ourstudyhasmultiplelimitations.Firstly,wefocusedoncomparingthepre-
dictions of AIF-DDM with a single information-theoretic definition of cognitive
effort. Our intention, however, is not to disprove this particular formalism, but
to illustrate that a more complete theory of cognitive effort should include both
theeffectofsubjectivevaluecompetitionandtheviolationofpriorexpectations.
Secondly, we focused on studying how cognitive effort may result from the com-
petitionbetweentheEFEsandpriorbeliefsofthedifferentavailableoptions.We
did not, however, address the costs associated with learning or with generating
EFEs, which are likely to differ between the two stages. Finally, cognitive effort
is a complex phenomenon that may depend upon multiple factors, such as the
objectivecognitivedemandofataskortheparticipant’sinformation-processing
speed.SomeofthesefactorsmaybecapturedbydifferentAIF-DDMparameters
(e.g., a and κ for impulsiveness and motivation, respectively); however, this
bs
analysis was beyond the scope of this paper.
A future study could evaluate whether the pre-thinking effect is responsi-
ble for the AIF-DDM misfit for the first-stage RTs by reducing the inter-trial
intervals. This could be achieved by, for example, reducing the amount of time
thattherewardsaredisplayed.Moreover,theproblemofrulingoutmissedtrials
could be mitigated by increasing the available time to make a choice, using a
likelihoodfunctionthatcantakeintoaccounttheprobabilityofmissingadead-
line, or introducing collapsing thresholds in the DDM [20]. Implementing these
changescouldmoveusclosertoabetterunderstandingofcognitiveeffortinthe
two-step task.
Cognitive Effort in the Two-Step Task 13
A Further details on Active Inference in the two-step
task
In[4],thedistributionoverpriorpreferredobservationsisgivenbythefollowing
equation:
1
P(o |C)= eotλe−(1−ot)λ (5)
t Z(λ)
Where λ is a free parameter that regulates how much an agent prefers to
realize prior preferences (in this case, getting a reward) over gaining new infor-
mation. Thus, λ regulates the exploration-exploitation trade-off.
AsmentionedinSection2.2,agentsareassumedtotendtorepeatfirst-stage
actions independent of the observed outcomes. This behaviour is modelled in
AIF by introducing a habit term. In [4], this quantity is described by:
1
E(a
j
)=
Z(κ)
eδat−1,aj κe−(1−δat−1,aj )κ (6)
Where κ is a precision parameter regulating the agent’s reliance on habits.
After every trial, the AIF agent updates its beliefs about the second-stage
outcome probabilities. Since these probabilities are mutually independent, the
overalldistributionequalstheproductoffourbetadistributions,eachdescribing
the believed outcome probability of its respective action and parameterized by
its corresponding α and β.
2 2
(cid:89) (cid:89)
π (θ)= Be(α ,β ) (7)
t s,a,t s,a,t
s=1a=1
According to the learning rule implemented by [4], after observing the out-
come of a chosen action, its corresponding α and β parameters are updated
following these rules:
α =(1−χ )α +δ o l+(1−ν )α +ν α (8)
s,a,t t s,a,t−1 at,a t SD s,a,t−1 SD 0
β =(1−χ )β +δ (1−o )l+(1−ν )β +ν β (9)
s,a,t t s,a,t−1 at,a t SD s,a,t−1 SD 0
χ = mPS (10a) m= ν PS (10b)
t 1+mPS 1−ν PS
Where χ ∈[0,1] is the surprise-modulated adaptation rate for trial t which
t
dependsonthepredictivesurprisePS =−lnp(o ;π (θ)),andthepriorvolatility
t t
parameter ν ∈ [0,1] which modulates the influence of surprise on the agent’s
PS
beliefs. In Eqs. 8 and 9, l is the learning rate parameter, ν is the volatility
SD
parameter of chosen actions and ν modulates how over time α and β decay
SD
towards their prior values α and β , respectively.
0 0
14 A. Garrido-Pérez et al.
Evidence from multi-armed bandit tasks suggests a bias in prior outcome
probabilities. To model this effect, a free parameter, p , is fitted to each partic-
r
ipant, which determines the initial (or uninformed) α and β prior values, such
that α =2(1−p ) and β =2p [4].
0 r 0 r
The decay effect of the chosen actions captured by ν , may also affect
SD
unchosen actions, and therefore, for these, the updating rules reduce to:
α =(1−ν )α +ν α (11)
s,a,t UD s,a,t−1 UD 0
β =(1−ν )β +ν β (12)
s,a,t UD s,a,t−1 UD 0
Where ν is the volatility of unchosen actions.
UD
Thesurpriseupdatemechanismforchosenactionsandthedecaymechanisms
for chosen and unchosen actions may not all be simultaneously operating [4].
Therefore, four variants of the AIF models can be implemented, each with a
different combination of update rules as pointed out in Section 2.2.
In addition to the outcome probabilities, agents must also learn the transi-
tionprobabilities.Aspointedby[4],beforestartingtheexperiment,participants
completed an instruction phase as well as 50 practice trials and therefore, they
were aware that the transition probabilities of initial-stage actions were mir-
rored and could either be p(s |s ,a ) = p(s |s ,a ) = 0.7 or p(s |s ,a ) =
B A A C A B B A A
p(s |s ,a ) = 0.3 (with p(s |s ,a ) = 1−p(s |s ,a ) and p(s |s ,a ) =
C A B B A A C A A B A B
1−p(s |s ,a ),respectively).In[4],thistransitionlearningismodelledbyhav-
C A B
ingagentscounttransitions,andoneachtrial,choosingthemostlikelytransition
structure based on the observed frequencies.
B Hybrid reinforcement learning drift-diffusion model
According to the HRL-DDM model, an agent performing the two-step task in
states,andtrialt,willselectanactionathatmaximizestheexpecteddiscounted
future reward (or state-action value), denoted by Q (s,a). Traditionally, state-
t
action values can be computed using either a model-free QMF or a model-based
t
strategy QMB.
t
In the two-step task, after completing each trial, model-free state-action val-
uesareupdatedfollowingtheSARSA(λ)algorithm.Forthechosensecond-stage
action a and the final state (either s or s , depending on the transition), its
2 B c
corresponding state-action value is updated as follows:
QMF(s ,a )=QMF(s ,a )+α δ (13)
t+1 2 2 t 2 2 2 2,t
Where α ∈ [0,1] is the second-stage learning rate parameter and δ , is
2 2,t
thesecond-stagepredictionerror,definedasthedifferencebetweentheobserved
outcome and its state-action value, such that δ =o −QMF(s ,a )
2,t t t 2 2
Since there is no outcome after choosing a first-stage action, and its opti-
mality ultimately depends on the outcome observed in the second stage, the
updating rule for the model-free state action value of the first-stage action will
Cognitive Effort in the Two-Step Task 15
depend on the second-stage prediction error δ and will be given by the follow-
2,t
ing equation:
QMF(s ,a )=QMF(s ,a )+α [δ +λδ ] (14)
t+1 A 1 t A 1 1 2,t 1,t
Where α ∈ [0,1] is the first-stage learning rate parameter, δ , is the first-
1 1,t
stage prediction error, defined as δ = QMF(s ,a )−QMF(s ,a ), and λ ∈
1,t t 2 2 t A 1
[0,1] is the eligibility parameter, which determines how much the second-stage
prediction error influences the first-stage state-action value.
Forthemodel-basedstrategy,agentsareassumedtohaveagenerativemodel
ofthetask,whichinthiscasemeansthattheyknow(orlearn)thestatetransition
probabilities and use these to calculate state-action values. For the first-stage
actions, the model-based values depend on the second-stage model-free values,
such that:
(cid:88)
QMB(s ,a )= P(s |s ,a ) max QMF(s ,a ) (15)
t A 1 2 A 1 t 2 2
s2∈{sB,sC}
a2∈As2
Inthefirststage,theagentwillfollowacombinationofmodel-freeandmodel-
basedstrategies,suchthatthetotalnetexpectedrewardofagivenactiona ,at
j
the first-stage state, s is given by a linear combination of the trial’s model-free
A
QMF and model-based QMB expected rewards:
t t
Qnet(s ,a )=wQMB(s ,a )+(1−w)QMF(s ,a )+ρrep (a ) (16)
t A j t A j t A j t j
Wherew ∈[0,1]isaweightparameterthatregulatestherelativeinfluenceof
model-based and model-free strategies in the decision-making. The last term in
the equation is added to model the tendency observed in participants to repeat
the first-stage choice of the previous trial. The level of response stickiness is
regulated by the parameter ρ, which multiplies the function rep(a) – equal to 1
if the first-stage action is the same as the one chosen in the previous trial and 0
otherwise.
Atthesecondstage,thestate-actionvaluesarecomputedusingonlyamodel-
free strategy, and it is assumed that there is no tendency to repeat the previous
action. Therefore, for the second stage, the net state-action value reduces to
Qnet(s ,a )=QMF(s ,a ).
t 2 2 t 2 2
Once state-action values are computed, the HRL-DDM agent, will pick an
action a at trial t, state s and stage p={1,2} through a drift-diffusion process
with a drift rate given by the following equation:
v =vmod[Qnet(s,a)−Qnet(s,a′)], a,a′ ∈A (17)
p,s,t p t t s
Where A is the set of available actions at state s.
s
16 A. Garrido-Pérez et al.
C Further details on the model fitting and comparison
procedures
Forthetwo-steptask,theMLEprocedureconsistsoffindingtheparticipant’sset
of parameter values of the model m, θˆMLE, that maximizes the likelihood of the
m
first-stageandsecond-stagebehaviouraldata(d1 andd2 ,respectively),given
1:T 1:T
the parameters set, p(d1 ,d2 |θ ,m), or equivalently, log-likelihood, LL =
1:T 1:T m
log p(d1 ,d2 |θ ,m). This quantity can be expressed as the sum of the indi-
1:T 1:T m
vidual trial LLs [23], such that:
T
(cid:88)
LL= logp(d1,d2 |d1 ,d2 ,θ ,m) (18)
t t 1:t−1 1:t−1 m
t=1
To use a standard minimization procedure, instead of maximizing the LL,
we minimize the negative of it (NLL). Moreover, we can express the LL of each
trial’s data as the product of the first-stage and second-stage data, giving the
following expression for the NLL:
T
(cid:88)
NLL=− logp(d1 |d1 ,d2 ,θ ,m)+logp(d2 |d1 ,d2 ,θ ,m)
t 1:t−1 1:t−1 m t 1:t 1:t−1 m
t=1
(19)
Wherep(d1 |d1 ,d2 ,θ ,m)andp(d2 |d1 ,d2 ,θ ,m)aretheprob-
t 1:t−1 1:t−1 m t 1:t 1:t−1 m
abilities of each choice (or choice and RT) of the first and second stages, respec-
tively, given the parameters of the model and the information available up to
that moment.
For AIF and HRL models, dp, corresponds to the choice made at stage p
t
and trial t, so Eq. 19 can be evaluated using the corresponding choice proba-
bility distributions of each model. For AIF-DDM and HRL-DDM, however, dp,
t
corresponds to the choice and RT tuple of stage p and trial t. Hence, to com-
pute the likelihood of each trial and phase, we use the WFPT distribution as
implemented by the HDDM Python package. The WFPT distribution takes as
input the parameters a , t , as well as the drift rates, which for HRL-DDM
bs nd
and AIF-DDM are calculated using Eqs. 17 and 3, respectively.
To perform the BMS procedure for model comparison, we require an ap-
proximation for the LME of each model for each participant. Instead of calcu-
lating this quantity, we use two approximations; one based on the BIC score
(LME ≈ −BIC/2) and another based on the AIC score (LME ≈ −AIC/2)
[12],wheretheBICandAICscoresarecomputedusingthefollowingequations:
BIC:=kln(n)−2LˆL (20)
AIC:=2k−2LˆL (21)
Where k is the number of free parameters in the model, n is the number of
trials, and LˆL is the maximum LL value, for a given participant and model.
Cognitive Effort in the Two-Step Task 17
D Model comparison plots
Fig.5: Bayesian model selection results comparing the five candidate models.
Theanalysiswasperformedseparatelyusinglog-modelevidenceapproximations
based on BIC (top row) and AIC (bottom row) scores. Each column represents
a different metric for model comparison: (left) Expected Posterior Probabilities
(EPP), (middle) Protected Exceedance Probabilities (PEP), and (right) Esti-
mated Model Frequencies (EMF). In the EMF plots, the red horizontal line
indicatesthechancelevel(0.2),anderrorbarsrepresentthestandarddeviation.
TheAIC-basedanalysisconsistentlyfavourstheAIF-DDM model,whilethe
FULL
BIC-basedresultsaremoredistributedacrossAIF-DDM ,andAIF-DDM .
NSD NPS
18 A. Garrido-Pérez et al.
E Parameter recovery results
Table 1: Parameter recovery results for the four AIF variants. The Pearson’s
correlation andp-values between original andrecovered parametersfor the pure
AIFmodelsaredenotedbyr andp,respectively.FortheAIF-DDMmodels,the
Pearson’s correlations and p-values are denoted by r and p , respectively.
ddm ddm
(a) AIF(-DDM) (b) AIF(-DDM)
FULL NSD
Parameter r p r p Parameter r p r p
DDM DDM DDM DDM
l .41 .054 .89 <.001 l .62 .002 .96 <.001
v .91 <.001 >.99 <.001 v .90 <.001 .97 <.001
DU DU
v .42 .043 .99 <.001 v .76 <.001 .98 <.001
DS PS
v .52 .012 .98 <.001 λ .49 .017 .68 <.001
PS
λ .51 .014 .71 <.001 κ .68 <.001 .85 <.001
κ .53 .009 .97 <.001 p .83 <.001 .97 <.001
r
p .28 .190 .93 <.001 γ .10 .644
r 1
γ .17 .428 γ .31 .143
1 2
γ .38 .073 vmod .68 <.001
2 1
vmod .46 .027 vmod .59 .003
1 2
vmod .73 <.001 a .99 <.001
2 bs
a .98 <.001 t >.99 <.001
bs nd
t >.99 <.001
nd
(c) AIF(-DDM) (d) AIF(-DDM)
NUD NSL
Parameter r p r p Parameter r p r p
DDM DDM DDM DDM
l .47 .025 .84 <.001 l .69 <.001 .93 <.001
v .69 <.001 .93 <.001 v .94 <.001 .91 <.001
DS DU
v .57 .005 .94 <.001 v .81 <.001 .97 <.001
PS DS
λ .66 <.001 .79 <.001 λ .56 .005 .67 <.001
κ .82 <.001 .93 <.001 κ .68 <.001 .87 <.001
p .32 .131 .69 <.001 p .94 <.001 .93 <.001
r r
γ .06 .798 γ .18 .413
1 1
γ .22 .315 γ .31 .146
2 2
vmod .55 .007 vmod .25 .246
1 1
vmod .60 .003 vmod .58 .004
2 2
a .95 <.001 a >.99 <.001
bs bs
t >.99 <.001 t >.99 <.001
nd nd
Cognitive Effort in the Two-Step Task 19
F AIF-DDM and AIF-DDM simulation results
NSD NPS
Fig.6:Comparisonofobservedandmodel-predictedbehaviourforAIF-DDM
NSD
and AIF-DDM models.
NPS
20 A. Garrido-Pérez et al.
G Reaction time Q-Q analysis
Tofurtherinvestigatethereactiontime(RT)misfitmentionedinSection3.4,we
generated Quantile-Quantile (Q-Q) plots comparing observed versus simulated
within-participantRTdeciles(10th−90th).SimulatedRTdecileswereproduced
for each participant by running 50 simulated experiments with 1000 trials each
(with the same reward volatility as the real experiment) using the best-fitted
parameters. As shown in Fig. 7, the plots reveal that all models consistently
overestimate the higher RT quantiles for both stages. This overestimation was
expected,asthemodels’predictionsareunconstrained,whereastheparticipants’
observedRTsweretruncatedbya2-secondresponsedeadline,withmissedtrials
excluded from the analysis.
Fig.7: Systematic overestimation of higher reaction time (RT) deciles by the
AIF-DDMmodels.TheQ-QplotscompareobservedversuspredictedRTdeciles
for Stage 1 (left) and Stage 2 (right). Points lying above the identity line (red)
indicate that the model’s predicted RT is higher than the observed RT. This ef-
fectismostpronouncedforthelongestreactiontimes,wherepredictionsdeviate
furthestfromtheidentityline.Notethatthepurplemarkersindicateanoverlap
of red and blue points.
Acknowledgments. This work was supported by European Union’s Horizon 2020
FETresearchprogramundergrantagreementNo.964464(ChronoPilot).Wegratefully
Cognitive Effort in the Two-Step Task 21
acknowledge the authors of [17] for making their dataset publicly available and the
authors of [4] for their open-source code.
Disclosure of Interests. The authors have no competing interests to declare that
are relevant to the content of this article.
References
1. Daunizeau, J., Adam, V., Rigoux, L.: Vba: a probabilistic treatment of nonlin-
earmodelsforneurobiologicalandbehaviouraldata.PLoScomputationalbiology
10(1), e1003441 (2014)
2. Daw, N.D., Gershman, S.J., Seymour, B., Dayan, P., Dolan, R.J.: Model-based
influences on humans’ choices and striatal prediction errors. Neuron 69(6), 1204–
1215 (2011)
3. Fiedler, S., Glöckner, A.: The dynamics of decision making in risky choice: An
eye-tracking analysis. Frontiers in psychology 3, 335 (2012)
4. Gijsen, S., Grundei, M., Blankenburg, F.: Active inference and the two-step task.
Scientific Reports 12(1), 17682 (2022)
5. Hodson, R., Mehta, M., Smith, R.: The empirical status of predictive coding and
active inference. Neuroscience & Biobehavioral Reviews 157, 105473 (2024)
6. Lai, L., Gershman, S.J.: Human decision making balances reward maximization
and policy compression. PLOS Computational Biology 20(4), e1012057 (2024)
7. Lee, D.G., d’Alessandro, M., Iodice, P., Calluso, C., Rustichini, A., Pezzulo, G.:
Riskydecisionsareinfluencedbyindividualattributesasafunctionofriskprefer-
ence. Cognitive psychology 147, 101614 (2023)
8. McDougle, S.D., Collins, A.G.: Modeling the influence of working memory, rein-
forcement,andactionuncertaintyonreactiontimeandchoiceduringinstrumental
learning. Psychonomic bulletin & review 28(1), 20–39 (2021)
9. Myers, C.E., Interian, A., Moustafa, A.A.: A practical introduction to using the
driftdiffusionmodelofdecision-makingincognitivepsychology,neuroscience,and
health sciences. Frontiers in Psychology 13, 1039172 (2022)
10. Parr, T., Holmes, E., Friston, K.J., Pezzulo, G.: Cognitive effort and active infer-
ence. Neuropsychologia 184, 108562 (2023)
11. Pedersen,M.L.,Frank,M.J.,Biele,G.:Thedriftdiffusionmodelasthechoicerule
in reinforcement learning. Psychonomic bulletin & review 24, 1234–1251 (2017)
12. Penny, W.D.: Comparing dynamic causal models using aic, bic and free energy.
Neuroimage 59(1), 319–330 (2012)
13. Proctor, R.W., Schneider, D.W.: Hick’s law for choice reaction time: A review.
Quarterly Journal of Experimental Psychology 71(6), 1281–1299 (2018)
14. Rigoux, L., Stephan, K.E., Friston, K.J., Daunizeau, J.: Bayesian model selection
for group studies—revisited. Neuroimage 84, 971–985 (2014)
15. Schwartenbeck, P., Passecker, J., Hauser, T.U., FitzGerald, T.H., Kronbichler,
M., Friston, K.J.: Computational mechanisms of curiosity and goal-directed ex-
ploration. elife 8, e41703 (2019)
16. Shahar,N.,Hauser,T.U.,Moutoussis,M.,Moran,R.,Keramati,M.,Consortium,
N.,Dolan,R.J.:Improvingthereliabilityofmodel-baseddecision-makingestimates
in the two-stage decision task with reaction-times and drift-diffusion modeling.
PLoS computational biology 15(2), e1006803 (2019)
22 A. Garrido-Pérez et al.
17. FeherdaSilva,C.,Hare,T.A.:Humansprimarilyusemodel-basedinferenceinthe
two-stage task. Nature Human Behaviour 4(10), 1053–1066 (2020)
18. Steinemann, N., Stine, G.M., Trautmann, E., Zylberberg, A., Wolpert, D.M.,
Shadlen, M.N.: Direct observation of the neural computations underlying a sin-
gle decision. Elife 12, RP90859 (2024)
19. Virtanen, P., Gommers, R., Oliphant, T.E., Haberland, M., Reddy, T., Courna-
peau, D., Burovski, E., Peterson, P., Weckesser, W., Bright, J., et al.: Scipy 1.0:
fundamentalalgorithmsforscientificcomputinginpython.Naturemethods17(3),
261–272 (2020)
20. Voskuilen, C.,Ratcliff, R., Smith, P.L.: Comparingfixed and collapsing boundary
versions of the diffusion model. Journal of mathematical psychology 73, 59–79
(2016)
21. Wellcome Centre for Human Neuroimaging: Satistical parametric mapping soft-
ware package. https://www.fil.ion.ucl.ac.uk (1991), accessed: 2025-5-10
22. Wiecki,T.V.,Sofer,I.,Frank,M.J.:Hddm:Hierarchicalbayesianestimationofthe
drift-diffusion model in python. Frontiers in neuroinformatics 7, 14 (2013)
23. Wilson, R.C., Collins, A.G.: Ten simple rules for the computational modeling of
behavioral data. Elife 8, e49547 (2019)
24. Yao, Z.F., Yang, M.H., Hsieh, S.: Neural correlates of span capacity during visual
discrimination under varying cognitive demands. Scientific Reports 15(1), 31071
(2025)
25. Zenon, A., Solopchuk, O., Pezzulo, G.: An information-theoretic perspective on
the costs of cognition. Neuropsychologia 123, 5–18 (2019)

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Cognitive Effort in the Two-Step Task: An Active Inference Drift-Diffusion Model Approach"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
