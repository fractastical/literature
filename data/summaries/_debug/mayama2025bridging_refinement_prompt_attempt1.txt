=== IMPORTANT: ISOLATE THIS PAPER ===
You are revising a summary for ONLY the paper below. Do NOT reference or use content from any other papers.
Paper Title: Bridging integrated information theory and the free-energy principle in living neuronal networks
Citation Key: mayama2025bridging
REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Issues to fix:
1. CRITICAL: The current summary has severe repetition issues. You MUST eliminate all repeated sentences, phrases, and paragraphs. Each idea should be expressed only once. If you find yourself repeating content, remove the duplicates entirely. Focus on variety and uniqueness in your wording.
2. Severe repetition detected: Same sentence appears 7 times (severe repetition)

Current draft (first 2000 chars):
This paper investigates the relationship between integrated information theory (IIT) and the free-energy principle (FEP) in living neuronal networks. The authors state: â€œThe authors state: â€œintegrated information increases specifically during belief updating, when sensory inputs are most informative, rather than tracking model efficiency.â€ They note: â€œThe authors state: â€œintegrated information correlates strongly and positively with Bayesian surprise, modestly and heterogeneously with accuracy, and shows no significant relationship with VFE.â€ The paper argues: â€œThe authors state: â€œintegrated information increases specifically during belief updating, when sensory inputs are most informative, rather than tracking model efficiency.â€ According to the research, â€œThe authors state: â€œintegrated information increases specifically during belief updating, when sensory inputs are most informative, rather than tracking model efficiency.â€ The study demonstrates that â€œThe authors state: â€œintegrated information increases specifically during belief updating, when sensory inputs are most informative, rather than tracking model efficiency.â€ The authors state: â€œThe authors state: â€œintegrated information increases specifically during belief updating, when sensory inputs are most informative, rather than tracking model efficiency.â€ The authors state: â€œThe authors state: â€œintegrated information increases specifically during belief updating, when sensory inputs are most informative, rather than tracking model efficiency.â€ The authors state: â€œThe authors state: â€œintegrated information increases specifically during belief updating, when sensory inputs are most informative, rather than tracking model efficiency.â€ The authors state: â€œThe authors state: â€œintegrated information increases specifically during belief updating, when sensory inputs are most informative, rather than tracking model efficiency.â€ The authors state: â€œThe authors state: â€œintegrated information increases specifically durin...

Key terms: principle, surprise, energy, integrated, bridging, theory, living, information

=== FULL PAPER TEXT ===
Bridging integrated information theory and the free-energy
principle in living neuronal networks
Teruki Mayama1, Sota Shimizu1, Yuki Takano1, Dai Akita1, and Hirokazu Takahashi1,*
1Department of Mechano-Informatics, Graduate School of Information Science and
Technology, The University of Tokyo, Tokyo, Japan
*takahashi@i.u-tokyo.ac.jp
Abstract
The relationship between Integrated Information Theory (IIT) and the Free-Energy Principle
(FEP) remains unresolved, particularly with respect to how integrated information, proposed as the
intrinsic substrate of consciousness, behaves within variational Bayesian inference. We investigated
thisissueusingdissociatedneuronalcultures,previouslyshowntoperformperceptualinferencecon-
sistent with the FEP. Repeated stimulation from hidden sources induced robust source selectivity:
variational free energy (VFE) decreased across sessions, whereas accuracy and Bayesian surprise
(complexity) increased. Network-level analyses revealed that a proxy measure of integrated infor-
mation and the size of the main complex followed a hill-shaped trajectory, with informational cores
organizing diverse neuronal activity. Across experiments, integrated information correlated strongly
and positively with Bayesian surprise, modestly and heterogeneously with accuracy, and showed no
significant relationship with VFE. The positive coupling between Î¦ and Bayesian surprise likely re-
flects the diversity of activity observed in critical dynamics. These findings suggest that integrated
information increases specifically during belief updating, when sensory inputs are most informative,
rather than tracking model efficiency. The hill-shaped trajectory of Î¦ during inference can be func-
tionally interpreted as a transition from exploration to exploitation. This work provides empirical
evidence linking the physical account of consciousness advanced by IIT with the functional perspec-
tive offered by the FEP, contributing to a unified framework for the mechanisms and adaptive roles
of phenomenology.
Keywords: Integrated Information Theory, Free-Energy Principle, Bayesian surprise, Dissociated
neuronal cultures
Introduction
Contemporary debates on the nature of consciousness have been shaped by two influential frameworks:
Integrated Information Theory (IIT) [1â€“5] and the Free Energy Principle (FEP) [6]. IIT, grounded
in phenomenology, holds that consciousness is identical to a systemâ€™s integrated causal structureâ€”an
irreduciblecauseâ€“effectrepertoirequantifiedbyÎ¦â€”whichspecifieshowexperienceexistshereandnowas
anintrinsicpropertyofthesystem. Incontrast, theFEPprovidesanormativeaccountofself-organizing
living systems, proposing that agents must minimize variational free energy (VFE) to constrain sensory
surprise. This framework unifies perception, learning, and action under variational Bayesian inference
andactiveinference. Withinthisview,deepgenerativemodelsthatenablelong-horizonpredictionconfer
adaptiveadvantages,suggestingwhyinformationalstructuresassociatedwithconsciousnessmayemerge.
Taken together, these perspectives suggest a complementary path toward synthesis. IIT provides
a proximate explanation, identifying conscious experience with integrated information structure itself,
whereas FEP-based theories of consciousness (e.g., [7â€“11]) offer an ultimate explanation in terms of tele-
ology and adaptive function, echoing Tinbergenâ€™s classic distinction between the proximate and ultimate
causes [12]. The proposal of conceptual bridges between the two frameworks is a relatively recent devel-
opment. Forexample,MarkovianmonismhighlightsformalparallelsbetweenIITâ€™scomplexesandFEPâ€™s
Markov-blanketed agents, both of which insulate internal dynamics while mediating perceptionâ€“action
exchanges [13]. Similarly, Integrated World Modeling Theory (IWMT) further argues that richly unified
1
5202
tcO
5
]CN.oib-q[
1v48040.0152:viXra
internalmodelsâ€”thosewithhigherÎ¦â€”arefavoredunderactiveinferencebecausetheysupportlong-term
free-energy minimization [14]. Consistent with this view, simulation studies have reported that evolving
agents exhibit decreasing surprise alongside increasing Î¦ [15]. Collectively, these lines of research moti-
vate a unified account in which integrated informational structure serves simultaneously as the substrate
of experience (IIT) and as an emergent outcome of adaptive inferential dynamics (FEP).
Nevertheless,severalimportantgapsremain. First,mostevidenceforanIITâ€“FEPassociationderives
from theoretical or simulation studies: direct neural evidence from living systems is still scarce. Second,
the often-postulated negative correlation between Î¦ and VFE lacks mechanistic grounding and may not
consistentlyhold,asthemoment-to-momentrelationshipsbetweenÎ¦andsurprisecanvaryinsignwithin
a single task [15]. Finally, it remains unsolved how intrinsically integrated information both arises and
operates during variational Bayesian inference under the FEP.
In this study, we address these gaps by employing in vitro dissociated neuronal cultures grown on
high-density multielectrode arrays (HD-MEAs). Previous works have shown that such cultures, when
driven by structured inputs, perform perceptual inference consistent with the FEP and can be modeled
by canonical neural networks whose cost function is asymptotically equivalent to VFE [16â€“19]. Building
on this framework, we repeatedly presented stimuli generated by hidden signal sources and recorded
spiking activity across successive sessions. From these data, we estimated VFE and its decomposition
into Bayesian surprise (complexity) and accuracy. To obtain proxy measures of integrated information,
we computed pairwise synergistic information (Î¦ ) [20] and constructed weighted graphs, from which
R
main complexes were extractedusing minimum-cut procedures inspired by IIT2.0-style analyses [21,22].
Based on these considerations, we address the following questions. First, does integrated information
necessarilyaccompanyadecreaseinVFE,ordoesitinsteadtrackotherFEP-relatedquantities? Second,
how does integrated information evolve as networks improve inferenceâ€”does it increase monotonically,
remainstable,orfollowanon-lineartrajectory? Finally,ifaconsistentevolutionpatternisobserved,how
can it be functionally interpreted? Our aim is to answer these questions and to advance the IITâ€“FEP
dialoguefromtheoreticalplausibilitytoempiricalgroundingbyjointlyquantifyingFEP-relatedquantities
andintegratedinformationalstructureinlivingneuralnetworks. Indoingso,weframeconsciousnessâ€”as
defined by IITâ€”as the intrinsic manifestation of adaptive belief updating (FEP), thereby bridging the
explanatory â€œhowâ€ and teleological â€œwhyâ€ within a unified framework.
Results
Study aims and experimental paradigm
To bridge IIT and the FEP within a living neural system, we examined how integrated information
emerges and functions within a form of self-organization suggested to follow the FEP. We employed
dissociatedneuronalculturesgrownonHD-MEAs,usingarepeated-stimulationparadigminwhichprob-
abilisticobservationsgeneratedbytwohiddensignalsourcesweredeliveredvia32electrodes(Fig.1,left).
Previousstudieshaveshownthatsuchculturesacquirethecapacitytoinferhiddensources, withVFEâ€”
empirically computed from a canonical neural network formulationâ€”decreasing during learning [16â€“19].
Building on this design, we recorded spiking activity as networks inferred and learned, computed FEP-
related quantities (VFE, Bayesian surprise, and accuracy), and derived proxy measures of integrated
information (Î¦mc and coreness) to analyze their trajectories and interrelationships during perceptual
R
inference.
Within the FEP framework, VFE in variational Bayesian inference under a generative process mod-
elled as a partially observable Markov decision process (POMDP; Fig.1, right) can be written as follows:
F(Q(s,A),o)= D (Q(s,A)âˆ¥P(s,A)) âˆ’E [lnP(o|s,A)]. (1)
KL Q(s,A)
| {z } | {z }
complexity(Bayesiansurprise) Accuracy
The canonical neural network [17,18] is mathematically equivalent to variational Bayes in this setting,
enabling the empirical estimates of VFE, Bayesian surprise, and accuracy directly from the recorded
activity.
The generative process comprised two independent binary signal sources s(1) and s(2), which stochas-
tically generated 32 binary observations through a 0.75/0.25 likelihood mapping across channel halves.
Eachobservationwasdeliveredtothecultureasanelectricalpulse(Fig.1,left). Oneexperimentconsisted
2
Environment
Hidden signal sources
ğ‘ ğŸ ğ‘ ğŸ (Hidden state, ğ’”) ğ‘  ğ·âˆ—
Probabilistic mapping
ğ´
(Likelihood mapping, ğ‘¨)
75% 25% 25% 75%
Sensory input
ğ’ğŸ ğ’ğŸ ãƒ»ãƒ»ãƒ» ğ’ğŸğŸ” ğ’ğŸğŸ• ğ’ğŸğŸ– ãƒ»ãƒ»ãƒ» ğ’ğŸ‘ğŸ ğ‘œ
(Observation, ğ’)
Agent
HD-MEA
(Body)
Cultured
neuronal network Synapse
ğ‘¨
(Brain) (Posterior parameter, ğ‘¨)
Neuronal activity
ğ’” ğ·
(Posterior state, ğ’”)
Figure 1: Experimental paradigm.
Setup (left) and corresponding POMDP (right), following the design of prior research [19]. In each trial,
hidden signal sources s=(s(1),s(2)) in a computer stochastically generate observations o through a like-
lihood mapping A. These hidden sources were not directly observable to the cultured neuronal network,
whereas the observations delivered via 32 electrodes on the HD-MEA were directly observable. These
electrical stimuli evoked synaptically mediated responses, corresponding to posterior states s mediated
by the posterior parameter A.
of 100 sessions, each comprising 256 trials presented at 1-s intervals, with a 244-s rest period between
sessions (see Methods â€™Electrophysiological experimentâ€™ section for details).
Perceptual inference by neuronal networks
We conducted 27 independent experiments across 12 cultures using an HD-MEA (26,400 electrodes;
up to 1,024 recorded simultaneously at a sampling rate of 20 kHz; 32 stimulation channels and â‰¤992
recording channels). Spike rasters and PSTHs at a representative electrode revealed source-selective
responses that strengthened progressively from session 1 to session 100 (Fig.2a). Across electrodes and
experiments,spikecountspeakedaround100â€“200mspost-stimulation; thus,thenumberofspikeswithin
a 10â€“300 ms window was defined as the evoked response (Fig.2b). Across electrodes, the Kullback-
Leibler divergence (KLD) of the responses between (s(1),s(2)) = (1,0) and (s(1),s(2)) = (0,1) increased
significantly (Fig.2c). Moreover, when tracking the changes in the average evoked responses of s(1)-
preferring electrodes (those selectively responsive to s(1)), responses grew more strongly during trials
in which s(1) was active, demonstrating the emergence and reinforcement of source selectivity under
3
a
b c d
Figure 2: Perceptual inference by neuronal networks.
(a) Changes in neuronal activity at a single representative electrode across sessions. Colors indicate
hiddensourcestates. (Left)Rasterplotsofspikingactivityacross256trialsinthefirstandlastsessions.
The horizontal axis denotes time after electrical stimulation (ms), and the vertical axis denotes trials,
sorted by hidden source states. Each dot represents a spike detected at the electrode. (Right) Post-
stimulus time histograms (PSTHs) from the first and last sessions. The horizontal axis denotes time
after stimulation again, and the vertical axis shows the mean spike counts. (b) PSTH averaged across
sessions, electrodes, and experiments. A peak is evident at âˆ¼100â€“200 ms post-stimulation. (c) Change
from the first session in the Kullbackâ€“Leibler divergence (KLD) between the distributions of evoked
spike counts for trials with (s(1),s(2)) = (1,0) and (s(1),s(2)) = (0,1), averaged across electrodes. KLD
significantly increased in the final session (Wilcoxon signed-rank test; final session, n=7,613 electrodes
from 27 independent experiments, ****p = 2.7Ã—10âˆ’144 < 0.001). (d) Change from the first session in
the mean evoked spike count of s(1)-preferring electrodes when s(1) =1 versus s(1) =0, averaged across
experiments. Responses when s(1) = 1 grew significantly more than those when s(1) = 0 (Wilcoxon
signed-rank test; final session, n=27, ***p=2.5Ã—10âˆ’3 <0.005).
repeated, source-generated stimulation (Fig.2d). Together, these findings indicate perceptual inference
byneuronalnetworks: theculturesbecamesensitivetothehiddensignalsourcesâ€™statesdespitereceiving
probabilistically generated electrical stimulation.
Canonical neural network and variational Bayesian inference
We formalized the inference using a canonical neural network [17,18] (Fig.3a), which is mathematically
equivalent to variational Bayesian inference under the POMDP (Fig.1, right). This formulation allowed
empirical evaluation of VFE, its complexity term (Bayesian surprise), and accuracy from recorded neu-
ronalresponsesandinferredparameters(seeMethodsâ€™FEP-basedanalysisâ€™sectionfordetails). Acrossex-
periments, VFE decreased, whereas both Bayesian surprise and accuracy increased significantly (Fig.3bâ€“
3d), consistent with self-organization under the FEP and reflecting enhanced belief updating and model
complexity. We further decomposed Bayesian surprise by source and found it to be selectively larger
for the currently true source (two-sided binomial sign tests; Fig.3e). Moreover, Bayesian surprise was
strongly coupled to response diversity quantified by the session-wise interquartile range (IQR) of evoked
activity (mean Ï=0.777, 95% CI [0.678, 0.848], Ï„2 =0.302, Q=634.5 and I2 =95.9; meta-analysis on
Spearman correlations under a random-effects model; Fig.3f, 3g).
4
a b c
Canonical neural network
Sensory stimuli
"" "!ãƒ»ãƒ»ãƒ» ""# ""% ""$ãƒ»ãƒ»ãƒ» "&! â†”Observations
! Synaptic strength
â†” Parameter posterior
#( #' "
Neural activity Firing threshold factor
â†”State posterior â†” State prior
Minimize #
Cost function
â†”Variational free energy
d f g
e
Figure 3: Variational Bayes formulation.
(a) Schematic of a canonical neural network. Neural activity x is determined by sensory input o through
synaptic weights W and a firing threshold factor Ï•. Assuming that the dynamics of x and W minimize
a common cost function L, the network is mathematically equivalent to variational Bayesian inference in
thePOMDPframeworkshowninFig.1. Specifically, sensoryinputscorrespondtoobservationso, synap-
tic strengths to parameter posteriors A, threshold factors to state priors D, and neural activity to state
posteriors s. (bâ€“d) Changes from the first session in VFE, Bayesian surprise, and accuracy, respectively,
averaged across experiments. VFE significantly decreased, whereas Bayesian surprise and accuracy sig-
nificantly increased (Wilcoxon signed-rank test; final session, n = 27, ***p = 1.4 Ã— 10âˆ’3 < 0.005,
****p = 6.0Ã—10âˆ’4 < 0.001, and ****p = 1.5Ã—10âˆ’8 < 0.001, respectively). (e) Distributions of mean
s(1) Bayesian surprise and mean s(2) Bayesian surprise within a session for trials with (s(1),s(2))=(1,0)
(left) and (0,1) (right). Each point represents one session, yielding 2,700 points across 27 experiments.
For (1,0), the s(1) Bayesian surprise was significantly greater than the s(2) Bayesian surprise (two-sided
binomial test on the sign of paired differences; k = 1,712,n = 2,700,p = 1.6Ã—10âˆ’44). Conversely, for
(0,1), the s(2) Bayesian surprise was significantly greater (k = 1,487,n = 2,700,p = 1.5Ã—10âˆ’7). (f)
Scatter plot of the interquartile range (IQR) of mean evoked responses of preferring electrodes versus
Bayesiansurprise. Eachpointrepresentsonesession(2,700pointsintotal)withcolorsindicatingdifferent
experiments. (g)SpearmancorrelationcoefficientsbetweenneuronalresponseIQRandBayesiansurprise
with95%confidenceintervalsforeachexperiment, andtheirmeta-analysisusingtheDerSimonianâ€“Laird
method. Shown are the Fisher-z-transformed mean correlation under the random-effects model, its 95%
confidence interval, and the 95% prediction interval. The mean correlation was significantly positive
(two-sided Z-test; ****p=7.8Ã—10âˆ’22 <0.001).
Integrated information and informational cores within neuronal networks
Totrackintegratedinformationduringlearning,weconstructedweightedgraphsforeachsessionbycom-
puting Î¦ [20]â€”an empirical measure of synergistic information [23,24]â€”between all pairs of preferring
R
5
a e g
! ! !
ãƒ»ãƒ»ãƒ» ãƒ»ãƒ»ãƒ»
# # #
!âˆ’1 ! !+1
b Complex extraction
minimum cut
!!(#;%)
3
"
1 1 $ f h
# 2 6 5 complex
& 6 % !! "#({#,%,),*,+})=.
"
# !! "#({#,%})=/ $ main complex
!! "# ),*,+ =00
)2345466= " .)2345 # 466=. & %
$ !! "# *,+ =1
)2345466=00 %
&
%
)2345466=00
&
)2345466=00
c d i
Figure 4: Integrated information.
(a) Example time series of two units, from which pairwise Î¦ was computed. (b) Weighted undirected
R
graphs were constructed by computing Î¦ between all pairs of preferring electrodes, yielding one graph
R
per session. Each graph was recursively partitioned using minimum cuts until single vertices remained.
For each vertex set, the sum of edge weights crossing the minimum cut was defined as Î¦mc. Based
R
on Î¦mc, complexes and main complexes were identified, and a coreness value was assigned to each
R
vertex. (c)ChangefromthefirstsessioninÎ¦mc,averagedacrossexperiments. Î¦mc increasedsignificantly
R R
(Wilcoxon signed-rank test; final session, n = 27 experiments, **p = 5.9Ã—10âˆ’3 < 0.01). (d) Change
from the first session in the ratio of the number of vertices in the main complex to the total number
of vertices. (e) Scatter plot of main complex Î¦mc versus the ratio of vertices in the main complex. (f)
R
Spearman correlations between main complex Î¦mc and the ratio of vertices in the main complex, with
R
95% confidence intervals and meta-analysis across experiments. A significant positive correlation was
observed (****p = 3.3Ã—10âˆ’4 < 0.001). (g) Scatter plot of the mean IQR of neuronal responses across
all electrodes versus the mean coreness across all electrodes. (h) Spearman correlations between mean
neuronal response IQR and mean coreness across electrodes, with 95% confidence intervals and meta-
analysis. A significant positive correlation was observed (****p = 9.6Ã—10âˆ’8 < 0.001). (i) Scatter plot
comparing the mean neuronal response IQR of electrodes inside versus outside the main complex. The
pink line indicates the identity line. The mean IQR inside the main complex was significantly larger
(Wilcoxon signed-rank test; n=2,700).
electrodes, and then extracted complexes using a minimum-cut procedure [21]. State transition proba-
bilities for Î¦ estimation were derived exclusively from stimulation trials, following the perturbational
R
6
approach recommended by IIT, to better capture causeâ€“effect power elicited by exogenous inputs. For
eachsubgraph,Î¦mcwasdefinedasthesumofedgeweightscrossingtheminimumcut. Bycomparingthese
R
values with those of its subsets or supersets, each subgraph was classified as a complex, main complex,
or neither (see Methods â€™Complex extractionâ€™ section for details). The Î¦mc of the main complex indexed
R
integrated information, while coreness [22] quantified each nodeâ€™s contribution to informational cores
(Fig.4a and 4b). Across sessions, Î¦mc exhibited a hill-shaped trajectory, rising early and stabilizing at a
R
lowerplateau,whilemain-complexsizefollowedasimilarexpansionâ€“contractionprofile(Fig.4c,4d). Î¦mc
R
scaled positively with main complex size (mean Ï = 0.411, 95% CI [0.196, 0.589], Q = 881.2,I2 = 97.0,
and Ï„2 = 0.389; Fig.4e, 4f). Response diversity also increased with mean coreness (mean Ï = 0.710,
95% CI [0.509, 0.838], Q = 1515.8,I2 = 98.3, and Ï„2 = 0.734) and was significantly higher inside than
outside the main complex (Wilcoxon signed-rank test; n = 2,700 session pairs) (Fig.4gâ€“4i). Together,
these results indicate that higher integrated information is accompanied by larger informational cores
that concentrate diverse neuronal activity.
Integrated information during perceptual inference under the FEP
WenextexaminedtherelationshipbetweenÎ¦mc andFEP-relatedquantitiesacrossallsessions. Bayesian
R
surpriseshowedarobustpositiveassociationwithÎ¦mcacrossexperiments,whereasaccuracywaspositivelyâ€”
R
but heterogeneouslyâ€”associated, and VFE showed no significant overall association (Î¦mcâ€“VFE: mean
R
Ï = 0.345, 95% CI [âˆ’0.00356, 0.619], Q = 2004.5,I2 = 98.7, and Ï„2 = 0.916; Î¦mcâ€“Bayesian surprise:
R
mean Ï = 0.879, 95% CI [0.790, 0.932], Q = 1226.9,I2 = 97.9, and Ï„2 = 0.623; Î¦mcâ€“Accuracy: mean
R
Ï = 0.393, 95% CI [0.0430, 0.657], Q = 2061.5,I2 = 98.7, and Ï„2 = 0.960; Fig.5aâ€“5f). Thus, integrated
information rises in tandem with belief updating, while model efficiency (i.e., low VFE) does not map
onto Î¦mc in a straightforward manner.
R
Finally, the contrast in coreness between s(1)- and s(2)-preferring electrodes tracked the contrast in
source-specific Bayesian surprise (mean Ï = 0.531, 95% CI [0.373, 0.660], Q = 616.3,I2 = 95.8, and
Ï„2 = 0.270; Fig.5g, 5h). In other words, stronger belief updating coincided with greater contributions
to informational cores, linking the content of inference to the geography of integration within the same
network.
Discussion
Summary of main findings
Weinvestigatedhowintegratedinformation(Î¦mc andcoreness)behaveswhenculturedcorticalnetworks
R
perform perceptual inference formalized under variational Bayes, or the free-energy principle (FEP).
Consistentwithpriorwork[16,17,19],repeatedpresentationofobservationsgeneratedbyhiddensources
elicitedrobustsourceselectivity,demonstratingtheemergenceofperceptualinferenceininvitroneuronal
networks (Fig.2). Session-wise analyses showed that variational free energy (VFE) decreased, while
Bayesian surprise (complexity) and accuracy increased, consistent with self-organization under the FEP
(Fig.3bâ€“3d).
Atthenetworklevel,Î¦mc andmain-complexsizefollowedahill-shapedtrajectoryacrosssessionsand
R
were positively correlated (Fig.4câ€“4f). Informational cores concentrated diverse neuronal activity: mean
coreness positively correlated with the session-wise interquartile range (IQR) of evoked responses, and
the mean IQR of electrodes inside the main complex consistently exceeded that of electrodes outside
(Fig.4gâ€“4i).
Across experiments, Î¦mc correlated strongly and positively with Bayesian surprise, showed a modest
R
and heterogeneous correlation with accuracy, and exhibited no significant overall relationship with VFE
(Fig.5aâ€“5f). Moreover, the spatial allocation of belief updating predicted the geography of informational
cores: coreness contrasts mirrored source-specific Bayesian surprise contrasts (Fig.5g, 5h). Taken to-
gether,thesefindingssuggestthatintegratedinformationalstructureduringFEP-guidedself-organization
increasesspecificallywhenbeliefsarebeingupdatedandsensoryinputsaremostinformative,ratherthan
directly reflecting model efficiency. In this way, our results provide empirical evidence bridging IIT and
the FEP, linking the intrinsic structure of experience with the adaptive dynamics of inference.
7
a b c c d d
e f g h
Figure 5: Integrated information in perceptual inference.
(a) Scatter plot of VFE versus Î¦mc. The upper panel shows raw values, with each point representing
R
one session (2,700 points in total across experiments) and colors indicating different experiments. The
lower panel shows Z-scores; for each experiment, sessions were plotted in red if the Spearman correlation
exceeded 0.3, in blue if less than âˆ’0.3, and in black otherwise. (b) Spearman correlations between VFE
and Î¦mc for each experiment with 95% confidence intervals, and their meta-analysis. No significant
R
overalleffectwasobserved(two-sidedZ-test;p=5.2Ã—10âˆ’2). (c)ScatterplotofBayesiansurpriseversus
Î¦mc, in the same format as (a). (d) Spearman correlations between Bayesian surprise and Î¦mc for each
R R
experimentwith95% confidence intervals, andtheirmeta-analysis. Asignificantpositive correlation was
observed (two-sided Z-test; ****p = 4.3Ã—10âˆ’19 < 0.001). (e) Scatter plot of accuracy versus Î¦mc,
R
in the same format as (a). (f) Spearman correlations between accuracy and Î¦mc for each experiment
R
with 95% confidence intervals, and their meta-analysis. A significant positive correlation was observed
(two-sided Z-test; *p = 2.9Ã—10âˆ’2 < 0.05). (g) Scatter plot of the contrast between the mean coreness
of s(1)-preferring versus s(2)-preferring electrodes against the contrast between s(1) Bayesian surprise
and s(2) Bayesian surprise. (h) Spearman correlations between coreness contrast and Bayesian surprise
contrastforeachexperimentwith95%confidenceintervals,andtheirmeta-analysis. Asignificantpositive
correlation was observed (two-sided Z-test; ****p=6.8Ã—10âˆ’9 <0.001).
Integrated information and Bayesian surprise
AcentralobservationistherobustpositiveassociationbetweenÎ¦mc andBayesiansurpriseacrosssessions
R
and experiments (meta-analytic Spearmanâ€™s Ï = 0.879; Fig.5c, 5d). Bayesian surprise quantifies the
divergence between the prior P(s) and the variational posterior Q(s)â‰ˆP(s|o), i.e., the degree of belief
updating elicited by new evidence. When belief change is smallâ€”because the current generative model
already explains inputs with high likelihoodâ€”processing can rely on pre-existing, localized, relatively
reflexive circuits with lower irreducibility. By contrast, when belief change is large, the model must be
reconstructed, yielding distributed and synergistic activity patterns that span subnetworks and increase
integratedinformation. Thismechanisticpicturealignswiththesession-wiseincreaseinresponsediversity
8
Time course
Lundbak Olesen,
Christoffer, et al., 2023 ?
Integrated Information
Variational free energy
Network
Gas-like Liquid-like Solid-like
Exploration Exploitation
Mutation Selection
Figure 6: Proposed framework
Schematicoftheproposedbehaviorofintegratedinformationduringself-organizationundertheFEP.As
VFE decreases over time, integrated information is predicted to follow a hill-shaped trajectory, reaching
a maximum at intermediate stages characterized by complex recurrent connectivity. Networks thus
transition from chaotic connections to more orderly structures. This trajectory parallels exploration
versus exploitation, and mutation versus selection in Darwinian dynamics. The previously reported
decrease in surprise with increasing Î¦ over evolutionary timescales [15] may reflect the ascending phase
of this trajectory.
(IQR) alongside Bayesian surprise (mean Ï = 0.879; Fig.3f, 3g), the positive association between main-
complex size and Î¦mc (mean Ï=0.411; Fig.4e, 4f), and the consistently higher IQR inside than outside
R
themaincomplex(Fig.4gâ€“4i). Thus, diverseandwidelycoupleddynamicsarelikelytoaccompanybelief
revision and covary with Î¦. Functionally, because sustaining large Î¦ entails substantial spatial and
metabolic costs, it is expected to emerge primarily when these costs are offset by highly informative
inputs, as reflected in high Bayesian surprise.
Response diversity, quantified as the session-wise IQR of evoked responses, tracked Bayesian surprise
across experiments (mean Ï = 0.879; Fig.3f, 3g). This result is consistent with operation near critical-
ity [25â€“29], a regime in which neural systems maximize dynamic range and stimulusâ€“response mutual
information, I(S;R), while exhibiting rich long-range correlations. When Q(s) approximates P(s | o)
and is averaged over observations, Bayesian surprise relates to the mutual information between observa-
tions and hidden states, I(o;s). Because observations o correspond to stimuli and beliefs about hidden
statessareencodedinneuralresponses, increasesinI(S;R)nearcriticalitycanenhancebothI(o;s)and
Bayesian surprise. Given the theoretical and empirical predictions that integrated information Î¦ peaks
near criticality [30â€“33], together with our findings of positive IQRâ€“coreness covariation and consistently
higher IQR inside than outside the main complex (Fig.4gâ€“4i), the observed positive correlation between
Î¦ and Bayesian surprise appears to share a common foundation in criticalityâ€”where diverse activity is
globally coordinated without loss of differentiation.
Importantly, Bayesian surprise accords with the intrinsicality emphasized by IIT. Integrated infor-
9
mation structure is fundamentally intrinsicâ€”as in dreamingâ€”but can be modulated by external stimuli.
Bayesian surprise is defined solely in terms of internal elements, the prior P(s) and the variational pos-
terior Q(s), yet depends implicitly on external observations o through Q(s)â‰ˆP(s|o). This perspective
alignswithIITâ€™sclaimthatintegratedinformationreflectsmeaningfulintrinsiccauseâ€“effectpower,rather
than the extrinsic Shannon-style messages or codes [34].
AssumingthatÎ¦underliesconsciousness,itscouplingwithBayesiansurpriseoffersaunifyingaccount
ofdiverseexperientialphenomena. Inmotoradaptation,suchaslearningtoplayaninstrument,theearly
stages involve awkward movements and vivid sensations (high Î¦) because internal models fail to predict
inputs and require reorganization (high Bayesian surprise). With practice, performance becomes fluent;
prediction improves, belief updating diminishes, and phenomenological vividness decreases (low Î¦ and
Bayesiansurprise). Asimilartrajectoryisseeninperceptualadaptation,asinglareadjustmentorTroxler
fading. The framework also explains why most spontaneous neural activity remains unconscious: such
fluctuations yield no new knowledge, elicit little belief updating, and are not integrated. By the same
logic, pathological experiencesâ€”such as hallucinations and delusions in schizophrenia, associated with
aberrant salience [35]â€”may reflect abnormal assignment of Bayesian surprise, which is closely related
to salience [36]. In such cases, trivial inputs are treated as highly informative, driving excessive belief
updating and distorting Î¦â€“structure. Thus, across normal learning, adaptation, spontaneous activity,
and pathology, Î¦ appears to index the informativeness of sensory evidence through its dependence on
belief updating.
Integrated information and accuracy
Overall,accuracyshowedasignificantbutheterogeneouspositiverelationshipwithÎ¦mc (meanÏ=0.393;
R
Fig.5e, 5f). This suggests that greater integration often accompanies better inference performance, yet
high accuracy is not strictly contingent on large Î¦mc: 7/27 experiments exhibited negative correlations.
R
These results are consistent with the view that rich Î¦â€“structures confer functional advantages [37,38],
whilealsoaligningwithIITâ€™spredictionthatfunctionallyequivalentsystemscandifferintheirintegrated
causal structure [4,5]. This dovetails with our observation that Î¦ is tightly coupled to belief updating
(complexity) rather than to performance per se. Three analogies illustrate this dissociation: Bayesian
surprise vs. accuracy, model parameter count vs. performance, and intrinsic integrated information vs.
extrinsic functionality. In each case, the former contributes to the latter but it is not strictly required.
Integrated information and variational free energy
Empirically, the Î¦mcâ€“VFE relationship was mixed across experiments and not significant overall (mean
R
Ï=0.345;CIcrosseszero;Fig.5a,5b). ToreconcilethiswithreportsthatÎ¦increasesassurprisefallsover
longer (evolutionary) timescales [15] and with the theoretical accounts suggesting that minimizing VFE
may entail maximizing Î¦ [13,14], here we newly propose a hill-shaped trajectory: as VFE decreases,
Î¦ initially rises, peaks, and then declines (conceptual Fig.6). This scheme accords with the observed
hill-shaped transitions of Î¦mc and main-complex size (Fig.4c, 4d). Under such a trajectory, Î¦â€“VFE
R
correlations can be positive or negative, depending on whether the system resides on the ascending or
descending slope. This framework thus accommodates the variability observed across experiments while
situatingthenegativerelationsreportedintheory[13,14]andinevolutionarysimulations[15]withinthe
ascending phase, without contradicting our findings.
AtahighVFE(amaladaptedregime),theentropyofobservationstendstobelargeâ€”underergodicity,
thelong-termaverageofVFEservesasanupperboundonobservationentropy[39]â€”sobehaviorbecomes
weakly structured and elements act almost independently. Integrated information is presumably low
owingtotheabsenceofthecauseâ€“effectpoweremphasizedbyIITâ€”conceptually,ahigh-entropyâ€œgas-likeâ€
network. AtaverylowVFE(anidealizedlimit),theagentâ€™sgenerativemodelwouldpredictperfectlyand
processingwouldbecomereflexiveandfeedforward,withminimalbeliefupdating. Integratedinformation
should again be low, both because of the spatial and metabolic cost of maintaining it and the absence
of recurrenceâ€”conceptually, a low-entropy â€œsolid-likeâ€ network. Between these extremes, the model is
competent yet uncertainty remains. Multiple competing hypotheses must be coordinated and revised
by ongoing input, fostering large recurrent causeâ€“effect structures and high Î¦â€”conceptually, a medium-
entropy â€œliquid-likeâ€ network.
Functionally, this hill-shaped trajectory can be interpreted as a progression from the exploration
10
(mutation)phasetotheexploitation(selection)phase. Earlyintraining,becausehigh-Î¦systemscoincide
with information harvestingâ€”high Bayesian surprise, related to the mutual information I(o;s)â€”where
substantial resources are invested to construct large integrated cores and explore models capable of
explainingtheinputswithsufficientlikelihood. Later,asthemodelcompressesandstabilizes,exploitation
dominates: Bayesian surprise and Î¦ subside while VFE continues to decline. A similar interpretation
applies to mutationâ€“selection metaphors in the neural Darwinism-like dynamics [40â€“42]: early training
expandstheresponsiveareaanddiversifiesneuralresponses(presumablyhigherÎ¦),whereaslatertraining
contractstheareaandstereotypesresponses(lowerÎ¦)evenasperformanceimproves[42]. Together,these
analysessuggestthatÎ¦isnotadirectproxyformodelefficiency. Instead,itpeaksduringphasesofbelief
revision embedded within longer-term free-energy descent.
Limitations of the present study
First,theproposedhill-shapedtrajectoryofÎ¦isanidealizedprinciplewhosefullexpressionisconstrained
in practice. Embodiment, bodily degrees of freedom, and environmental complexity often prolong devel-
opment,suchthatapost-developmentalstatewithdiminishedÎ¦mayrarelybereachedoutsideofsimple
tasks. Our in vitro, low-difficulty task with two binary hidden states likely enabled some cultures to
reach this exploitative regime. Because the preparation was disembodied and passively stimulated, the
exploratory stage was probably shorter than would occur in an embodied setting. In active inference,
agents minimize expected free energy, which includes the epistemic-value term (expected Bayesian sur-
prise) with a negative sign [43,44], thereby promoting exploration, sustaining higher Bayesian surprise,
and maintaining larger Î¦ during active sensing, as in daily active vision [45]. Second, Î¦ was approxi-
mated using Î¦ [20] and coreness with a minimum-cut-based method [21,22]. These are IIT-inspired
R
proxies rather than full IIT 3.0/4.0 quantifications. Our approaches emphasize synergistic coupling but
donotexhaustivelyassessstate-dependentcauseâ€“effectstructuresacrossspatiotemporalscales[5]. Third,
methodological constraints required us to fix the timescale (Ï„ = 10 ms), treat each electrode as a unit,
and to estimate transitions primarily from stimulation (perturbational) trials. While these approxima-
tions are likely reasonableâ€”given the emergence of integrated information at the macro timescale in
actual neural recordings [46] and the characteristic timescales of cultured neurons [47,48]â€”they remain
scale-dependent and warrant cautious interpretation. Finally, substantial between-experiment hetero-
geneity(highQ,highI2)inseveralmeta-analysescautionsthatculture-specificfactors(e.g.,maturation,
connectivity, excitability) may modulate the coupling between Î¦, Bayesian surprise, and performance.
Conclusion
Ourresultsshowthat,inlivingneuronalnetworksperformingperceptualinference,integratedinformation
is tightly coupled to belief updatingâ€”indexed by Bayesian surpriseâ€”rather than directly to variational
free energy. Informational cores expand and concentrate diverse activity when belief revision is stronger,
and a Î¦-proxy follows a hill-shaped trajectory across learning sessions, peaking within long-term free-
energy descent. These dynamics are consistent with operation near criticality, where response diversity,
belief updating, and integrated information co-peak. Conceptually, Î¦ can be interpreted as the intrinsic
manifestation of system reorganization required to incorporate informative evidence; once the generative
model becomes sufficiently complete, Î¦ is expected to decline. Functionally, Î¦ does not directly enhance
inference performance but indirectly facilitates it by supporting model updates. By situating integrated
informationwithinbeliefupdating,ourfindingsempiricallylinkIITâ€™smechanisticaccountwiththeFEPâ€™s
functionalperspective,advancingaunifiedframeworkthatbridgestheproximateâ€œhowâ€andtheultimate
â€œwhyâ€ of consciousness.
Methods
Dissociated neuronal cultures
All procedures complied with the â€œGuiding Principles for the Care and Use of Animals in the Field of
Physiological Scienceâ€ published by the Japanese Physiological Society. The Committee on the Ethics of
11
Animal Experiments at the Graduate School of Information Science and Technology, the University of
Tokyo, approved the experimental protocol (A2024IST003).
High-density microelectrode arrays (HD-MEAs, MaxOne, MaxWell Biosystems) were covered with 1
mL of 1% Tergazyme (Sigma-Aldrich) and left at room temperature for 2 h. The detergent was removed
withanaspirator,andthechipswererinsedthreetimeswithsterilizedwater. Eachchipwassubsequently
soaked in ethanol for 30 min, rinsed three additional times, overlaid with 1 mL of pre-warmed plating
medium (Neurobasal Plus, Thermo Fisher Scientific), covered to prevent drying, and maintained in an
incubator for at least 2 days.
After this pretreatment, the chips were rinsed three times with sterile water. Polyethylenimine (Su-
pelco) was diluted to 0.07 % in borate buffer (Thermo Fisher Scientific), and 50 ÂµL were applied to each
electrode surface. The chips were incubated overnight, washed three times and then coated with 50 ÂµL
of laminin (20 Âµg/mL; Sigma-Aldrich). After replacing the lids, the chips were incubated for 1 h.
PregnantWistarratswereanesthetizedwithinhaledisoflurane(Viatris)andeuthanisedbyguillotine
decapitation. Following abdominal disinfection with ethanol, the uterus was removed and placed in
Hankâ€™s Balanced Salt Solution (Life Technologies). Three E18 fetuses were harvested, their brains were
removed, and pieces of cerebral cortex were excised for cell seeding.
The cortical tissue was transferred to 2 mL of 0.25 % Trypsin-EDTA (Thermo Fisher Scientific)
and incubated for 20 min, with the tube shaken every 5 min. The tissue was then transferred to plating
mediumtostoptheenzymaticreaction,gentlyshaken,andplacedinfreshmedium. Cellsweredissociated
with trituration pipetting. One milliliter of the suspension was passed through a 40 Âµm cell strainer
(Falcon). Plating medium was added to adjust the density to 38,000 cells per 5 ÂµL.
Thelamininsolutionwasremovedfromthechipsurface,and50ÂµLofthecellsuspensionwasapplied
onto the electrodes. The chip was incubated for 120 min to allow cell attachment, after which 0.6 mL
of plating medium was added. The chip was then maintained in the incubator. To prevent evaporation,
the chip was covered with its lid, placed with a 35 mm dish of sterilized water inside a 90 mm dish, and
kept in an incubator at 36.5 Â°C in 5 % CO2.
In this study, 12 independent cell cultures were used to conduct 27 independent experiments. The
average days in vitro (DIV) was 18.4Â±6.96.
Electrophysiological experiments
HD-MEAs were used both to record the activity of cultured neuronal networks and to deliver electrical
stimulation. The HD-MEA employed in this study contained 26,400 electrodes arranged within an area
of3.85mmÃ—2.10mm,with17.5-Âµmspacingbetweenelectrodes,ofwhichupto1,024couldberecorded
simultaneously at a sampling rate of 20 kHz [49,50]. Prior to experiments, spontaneous activity was
recorded from all electrodes for 50 s. Based on the average spike amplitude during this period, up to
1,024 electrodes with the highest amplitudes were selected for subsequent recordings. From this set,
the 32 electrodes with the highest average spike amplitudes were designated as stimulation electrodes.
Amongthem, the16electrodeswithodd-numberedamplituderanksdeliveredstimulationcorresponding
to observations o(1) âˆ’ o(16), while the 16 electrodes with even-numbered ranks delivered stimulation
corresponding to observations o(17) âˆ’ o(32). Because recordings from the stimulation electrodes were
prone to noise interference, subsequent recordings were obtained from up to 992 electrodes, excluding
these 32 stimulation electrodes. Electrical stimulation consisted of biphasic pulses with a positive-first
phase, an amplitude of 350 mV, and a pulse width of 200 Âµs.
Data processing
For spike detection, the recorded potentials were band-pass filtered (300â€“3000 Hz, Butterworth filter). A
spike was detected when the measured potential at an electrode fell below a threshold set at five times
the standard deviation of the potential for that electrode.
In our samples, spike counts peaked within 100â€“200 ms after electrical stimulation (Fig.2b). Accord-
ingly, the evoked response strength r at an electrode i in a trial t was defined as the number of spikes
ti
occurring within a 10â€“300 ms window post-stimulation.
This treatment of evoked responses closely followed that of previous studies [16,19]; readers are
referred to those works for further details. For trials in which the source state was (s(1),s(2)) = (1,0)
(approximately 6,400 trials (= 100 sessionsÃ—256 trials/session/4 states)), the mean r was computed,
ti
12
as well as for trials in which (s(1),s(2))=(0,1) (âˆ¼6,400 trials). The difference between these two means
was then calculated for each electrode. Electrodes with differences >0 were classified as s(1)-preferring,
those with differences < 0 as s(2)-preferring, and those with differences = 0 as non-preferring/inactive.
Thenumbersofs(1)-preferring,s(2)-preferring,andnon-preferring/inactiveelectrodeswere352.0Â±359.3,
353.1Â±347.5, and 287.7Â±311.0, respectively (n=27).
Foreachtrial, the mean evoked responseover thes(1)-preferring electrodes was computedas x , and
t1
the mean over the s(2)-preferring as x . Both x and x were then mean-subtracted, detrended, and
t2 t1 t2
normalized to the range [0,1].
KLD of neuronal response
To evaluate the source selectivity of neuronal responses at each electrode, we used the Kullback-Leibler
divergence (KLD) method introduced in a previous study [16]. For electrode i, the distributions of
evoked spike counts in (s(1),s(2)) = (1,0) and (0,1) trials were each fitted with a Poisson distribution.
The empirical parameters Î» and Î» were estimated, and the KLD was computed according to the
1,0 0,1
following equation:
Î»
D (P(r |(1,0))âˆ¥P(r |(0,1)))=Î» ln 1,0 +Î» âˆ’Î» .
KL i i 1,0 Î» 0,1 1,0
0,1
In the Results, we report analyses restricted to the 7,613 electrodes for which the computed KLD
values converged (i.e., did not diverge).
FEP-based analysis
For FEP-based analysis, we closely followed the methods described in previous studies [17â€“19], including
the generative process, variational Bayesian inference, the canonical neural network, and the reverse-
engineering framework. For mathematical details, readers are referred to those prior studies.
Generative process of observations
We assumed a partially observable Markov decision process (POMDP) in which two independent binary
hidden sources s = (s(1),s(2)) âˆˆ {0,1}2, generated 32 binary sensory observations, o âˆˆ {0,1}32, via
t t t t
a stochastic mixing matrix A. In the actual experiment, the state of each hidden source was drawn
independently from a Bernoulli distribution with probability 0.5. For each observation channel, the
observation was generated from the hidden sources with specific conditional probabilities. In particular,
o(1)âˆ’o(16) conveyedthevalueofs(1) withprobability0.75orthatofs(2) withprobability0.25;conversely,
o(17)âˆ’o(32) conveyed the value of s(2) with probability 0.75 or that of s(1) with probability 0.25. This
definedthecategoricallikelihoodP(o(i) |s ,A)foreachelectrode,withP(A(i))assignedaDirichletprior.
t t
Variational free energy
Under a mean-field approximation Q(s ,A) = Q(A)Qt Q(s ), the variational free energy (i.e., the
1:t Ï„=1 Ï„
negative evidence lower bound) is given by
t
F = X s Â· (cid:0)lns âˆ’lnAÂ·o âˆ’lnD (cid:1)+O(lnt),
Ï„ Ï„ Ï„
Ï„=1
where D is the prior over hidden states. Minimizing F with respect to s and the Dirichlet parameters
Ï„
a yields
t
s =Ïƒ (cid:0)lnAÂ·o +lnD (cid:1) , aâ†a+ X o âŠ—s ,
Ï„ Ï„ Ï„ Ï„
Ï„=1
where Ïƒ(Â·) is the softmax function and âŠ— denotes the outer product.
13
Canonical neural network formulation
Neuronal responses x âˆˆ (0,1)2 to sensory inputs o were modelled as a canonical neural network with
t t
the following dynamics:
xË™ âˆâˆ’sigâˆ’1(x )+Wo +h,
t t t
where sigâˆ’1(Â·) is the elementwise logit function, W is a 2Ã—32 synaptic strength matrix, and h is the
adaptivefiringthresholdvector. ThematrixW =W âˆ’W iscomposedofexcitatory(W )andinhibitory
1 0 1
(W ) components.
0
Neural network cost function L
Integrating the network dynamics with respect to x yields a cost function
t
t (cid:18) (cid:19)âŠ¤ " (cid:18) (cid:19) Ë† Â¯Ë† !(cid:18) (cid:19) (cid:18) (cid:19)#
X x x W W o Ï•
L= Ï„=1 xÂ¯ Ï„ Ï„ ln xÂ¯ Ï„ Ï„ âˆ’ln W Ë† 1 0 W Â¯Ë† 1 0 oÂ¯ Ï„ Ï„ âˆ’ Ï• 1 0 +C,
where xÂ¯ = 1âˆ’x, oÂ¯= 1âˆ’o, W Ë† = sig(W ), and W Â¯Ë† = 1âˆ’sig(W ). The threshold factors Ï• = (Ï• ,Ï• )âŠ¤
â„“ â„“ â„“ â„“ 1 0
correspond to lnD. This L is asymptotically equivalent to F, with xâ†”s, W â†”A, and Ï•â†”lnD.
Reverse engineering from empirical neural activity
From experimental data, neuronal responses x were calculated for each trial. Given these responses, the
t
threshold factor Ï• was then estimated as:
(cid:18) (cid:19) (cid:18) (cid:19)
Ï• âŸ¨xâŸ©
Ï•= 1 =ln ,
Ï• 1âˆ’âŸ¨xâŸ©
0
where âŸ¨Â·âŸ© indicates the average over time. The threshold factor Ï• was held constant within each session.
Following previous studies, Ï• for the first 10 sessions was computed as the average of the neuronal
responses during those sessions. For subsequent sessions, Ï• was computed as the average of the neuronal
responses in the immediately preceding session.
The effective synaptic connectivity W was estimated from the outer products of x and o according
t t
to the fixed-point equations
(cid:18) âŸ¨xoâŠ¤âŸ© (cid:19) (cid:18) âŸ¨(1âˆ’x)oâŠ¤âŸ© (cid:19)
W =logit , W =logit , W =W âˆ’W .
1 âŸ¨x1âŠ¤âŸ© 0 âŸ¨(1âˆ’x)1âŠ¤âŸ© 1 0
Substituting x, W, and Ï• into L yielded the empirical variational free energy for each session. At the
same time, we computed empirical Bayesian surprise
t (cid:18) (cid:19)âŠ¤ " (cid:18) (cid:19) (cid:18) (cid:19)#
X x x Ï•
Ï„ ln Ï„ âˆ’ 1
xÂ¯ xÂ¯ Ï•
Ï„ Ï„ 0
Ï„=1
and empirical accuracy
t (cid:18) (cid:19)âŠ¤ Ë† Â¯Ë† !(cid:18) (cid:19)
X x W W o
Ï„=1
xÂ¯ Ï„
Ï„
ln
W
Ë† 1
0 W
Â¯Ë† 1
0
oÂ¯ Ï„
Ï„
.
Neuronal response IQR
Toevaluatethevariabilityofneuronalresponses,weusedtheinterquartilerange(IQR).Foreachsession,
the mean evoked response r(1) of s(1)-preferring electrodes was grouped by hidden source state, and the
IQR was calculated within each group. These IQR values were then averaged. The same procedure was
appliedtor(2) ofs(2)-preferringelectrodes. ThetworesultingIQRswerethenaveragedtoyieldanoverall
measure of response diversity for the network.
Similarly, to assess the variability of neuronal responses at a single electrode, trials were grouped by
hidden source state, and the IQR was calculated within each group and then averaged.
14
Transition probability
In IIT, the causeâ€“effect power is evaluated from the transition probabilities between system states. The
methodusedherecorrespondstowhathaspreviouslybeenreferredtoasthedownsamplingmethod[46].
Specifically, the time series was coarse-grained into states by segmenting it into windows of width Ï„, and
theempiricaldistributionofstatetransitionsbetweenadjacentwindowswascomputed. Foratimeseries
of length T, there are T âˆ’Ï„ +1 such windows, each represented by the mean value of the observations
within that window. These representative values were binarized using their median as the threshold.
Adjacent pairs of windows yield T âˆ’2Ï„ +1 transitions, which were used to compute state transition
probabilities. In each trial, evoked responses during 10â€“300 ms after stimulation were binned at 1-ms
resolution, resulting in a time series of length 290. For each session, a single state transition probabil-
ity matrix was computed using all trials in which electrical stimulation was delivered, i.e., those with
(s(1),s(2)) Ì¸= (0,0), amounting to approximately 256Ã—3/4=192 trials. The use of only trials containing
stimulation followed the rationale of the perturbational approach.
Complex extraction
For each session, a weighted undirected graph was constructed in which each vertex represented a pre-
ferring electrode, and all vertices were fully connected. The weight of each edge was given by Î¦ [20],
R
computed from the neuronal activity of the corresponding pair. The number of vertices occasionally
reached âˆ¼900. Due to computational constraints, Ï„ was fixed at 10 ms, and state transition probabilities
were calculated for all electrode pairs; Î¦ values were then derived from these transition probabilities.
R
The10-mswindowwidthwaschosenbasedonthetimestepusedinthepreviousstudiesofspatiotemporal
patterns in neuronal cultures [47,48]. The Î¦ between electrodes X and Y was expressed as:
R
Î¦ (X,Y)=I(X ,Y ;X ,Y )âˆ’I(X ;X )âˆ’I(Y ;Y )+ min I(Z ;W ),
R tâˆ’1 tâˆ’1 t t tâˆ’1 t tâˆ’1 t tâˆ’1 t
Z=X,Y,W=X,Y
where I is Shannonâ€™s mutual information, and the fourth term corresponds to the minimum mutual
information(MMI)[51]redundancyfunction,introducedasacorrectivemeasuretoavoidnegativevalues.
Themethodofcomplexextractionfollowedthatdescribedinpreviousresearch[21],andmathematical
detailsareprovidedtherein. Thegraphwasrecursivelypartitionedusingtheminimumcut(mc)untilall
vertices were isolated. Given a vertex set, the minimum cut is defined as the bipartition of the set into
two non-empty, disjoint subsets that minimizes the sum of the edge weights crossing the partition. The
sum of these edge weights crossing the minimum cut of a vertex set was denoted Î¦mc for that set. A
R
vertex set was defined as a complex if its Î¦mc was greater than that of any of its supersets, and, among
R
such complexes, was further defined as a main complex if its Î¦mc was not smaller than that of any of its
R
subsets. The maximum Î¦mc among main complexes â€” analogous to the integrated information quantity
R
in IIT 2.0 â€” was taken as the index of integrated information in this study. This index was computed
once per session. Finally, Î¦mc was normalized by the number of edges in the graph. This adjustment
R
was necessary because, for two graphs with comparable average edge weights but different numbers of
vertices, the graph with more vertices and edges would naturally yield a larger number of edges crossing
a cut, and thus a larger Î¦mc. Normalization by edge count therefore enabled comparisons across graphs
R
of different sizes.
Additionally, we computed the coreness measure [22]. For a given graph, the coreness of a vertex is
defined as the maximum Î¦mc among all complexes that include that vertex (noting that the set of all
R
vertices always constitutes at least one complex). In previous work [22], coreness was computed for the
mouse connectome and found to be high in regions such as the cerebral cortex, which are conducive to
large integrated information, and low in regions such as the cerebellum, which are less suited for inte-
grated information. Thus, coreness quantifies the contribution of each vertex to the systemâ€™s integrated
information.
Statistical tests
For comparisons between two paired groups, the Wilcoxon signed-rank test was used. For the meta-
analysis of Spearman correlation coefficients Ï obtained from each experiment, values were first trans-
i
formed into the Fisher-z domain: z = 1ln1+Ïi. Sampling variances were approximated as Var(z ) â‰ˆ
i 2 1âˆ’Ïi i
15
(1+Ï2/2)/(nâˆ’3)[52],wherendenotesthenumberofpairedobservations(i.e.,thenumberofdatapoints
i
per experiment contributing to the correlation). Between-experiment heterogeneity was assessed using
Cochranâ€™s Q statistic and the I2 statistic [53]. Given the presence of heterogeneity, we estimated pooled
effects using a random-effects model with DerSimonianâ€“Laird estimation [54] of the between-experiment
variance Ï„2. Random-effects weights were defined as w = 1/(Var(z )+Ï„2), and the pooled effect size
i i
was computed as z = P w z / P w . The corresponding standard error was SE = p1/ P w
RE i i i i i RE i i
and p-values were obtained from the two-sided Z-test. Finally, z and the 95% confidence interval
RE
z Â± 1.96SE were back-transformed to the correlation scale using Ï = tanh(z). Random-effects
RE RE
estimates, together with heterogeneity statistics (Q, I2, and Ï„2), are reported in the Results.
Data availability
Processed spike data and stimulation conditions (2 hidden source states and 32 observations per trial)
have been deposited in the DANDI Archive https://dandiarchive.org/dandiset/001611/draft.
Derivatives (neuronal responses, PSTH, response KLD, preferring electrodes, VFE, Bayesian surprise,
accuracy,Î¦ adjacencymatrices,main-complexmembership,andcoreness)andSourceDataareavailable
R
at Zenodo https://doi.org/10.5281/zenodo.17187550.
Code availability
All analysis code except for complex extraction is available at GitHub https://github.com/yunipok
e/Bridging_integrated_information_theory_and_the_free_energy_principle_in_living_neu
ronal_networks. For complex extraction, the original source https://github.com/JunKitazono/Bid
irectionallyConnectedCores was utilized. Code for the variational Bayesian metric was created with
significantreferencetotheoriginalsourcehttps://github.com/takuyaisomura/reverse_engineering.
Acknowledgments
We are deeply grateful to Drs. Naotsugu Tsuchiya, Masafumi Oizumi, Muneki Ikeda, Francesco Ellia,
MatteoGrasso,ShosukeNishimotoandTakuyaIsomuraforvaluablediscussionsandinsightfulcomments.
This work was partially supported by JSPS KAKENHI (23H03465, 24H01544, 24K20854, 25H02600),
AMED(24wm0625401h0001),theAsahiGlassFoundation,andtheSecomScienceandTechnologyFoun-
dation.
Conflict of interest
The authors have no conflicts to disclose.
Author contributions
TerukiMayama: Conceptualization(lead);Investigation(lead);Resources(supporting);Software(supporting);
Formal analysis (lead); Visualization (lead); Writing â€“ original draft (lead). Sota Shimizu: Software
(lead); Resources (supporting). Yuki Takano: Software (supporting); Resources (supporting). Dai
Akita: Formal analysis (supporting); Resources (lead); Funding acquisition (supporting); Project ad-
ministration(supporting);Supervision(supporting);Writingâ€“review&editing(supporting). Hirokazu
Takahashi: Funding acquisition (lead); Project administration (lead); Supervision (lead); Writing â€“ re-
view & editing (lead).
References
[1] Tononi G. An information integration theory of consciousness. BMC neuroscience. 2004;5:1-22.
16
[2] TononiG. Consciousnessasintegratedinformation: aprovisionalmanifesto. TheBiologicalBulletin.
2008;215(3):216-42.
[3] Balduzzi D, Tononi G. Integrated information in discrete dynamical systems: motivation and theo-
retical framework. PLoS computational biology. 2008;4(6):e1000091.
[4] Oizumi M, Albantakis L, Tononi G. From the phenomenology to the mechanisms of consciousness:
integrated information theory 3.0. PLoS computational biology. 2014;10(5):e1003588.
[5] AlbantakisL,BarbosaL,FindlayG,GrassoM,HaunAM,MarshallW,etal. Integratedinformation
theory (IIT) 4.0: formulating the properties of phenomenal existence in physical terms. PLoS
computational biology. 2023;19(10):e1011465.
[6] Friston K. The free-energy principle: a unified brain theory? Nature reviews neuroscience.
2010;11(2):127-38.
[7] Solms M, Friston K. How and why consciousness arises: some considerations from physics and
physiology. Journal of Consciousness Studies. 2018;25(5-6):202-38.
[8] Solms M. The hard problem of consciousness and the free energy principle. Frontiers in psychology.
2019;9:2714.
[9] Rudrauf D, Bennequin D, Granic I, Landini G, Friston K, Williford K. A mathematical model of
embodied consciousness. Journal of theoretical biology. 2017;428:106-31.
[10] Williford K, Bennequin D, Friston K, Rudrauf D. The projective consciousness model and phenom-
enal selfhood. Frontiers in Psychology. 2018;9:2571.
[11] Whyte CJ, Smith R. The predictive global neuronal workspace: A formal active inference model of
visual consciousness. Progress in neurobiology. 2021;199:101918.
[12] Tinbergen N. On aims and methods of ethology. Animal Biology. 2005;55(4).
[13] FristonKJ,WieseW,HobsonJA. Sentienceandtheoriginsofconsciousness: FromCartesianduality
to Markovian monism. Entropy. 2020;22(5):516.
[14] Safron A. An Integrated World Modeling Theory (IWMT) of consciousness: combining integrated
informationandglobalneuronalworkspacetheorieswiththefreeenergyprincipleandactiveinference
framework; toward solving the hard problem and characterizing agentic causation. Frontiers in
artificial intelligence. 2020;3:520574.
[15] LundbakOlesenC,WaadePT,AlbantakisL,MathysC. Phifluctuateswithsurprisal: Anempirical
pre-study for the synthesis of the free energy principle and integrated information theory. PLOS
Computational Biology. 2023;19(10):e1011346.
[16] Isomura T, Kotani K, Jimbo Y. Cultured cortical neurons can perform blind source separation
according to the free-energy principle. PLoS computational biology. 2015;11(12):e1004643.
[17] Isomura T, Friston K. Reverse-engineering neural networks to characterize their cost functions.
Neural computation. 2020;32(11):2085-121.
[18] Isomura T, Shimazaki H, Friston KJ. Canonical neural networks perform active inference. Commu-
nications Biology. 2022;5(1):55.
[19] Isomura T, Kotani K, Jimbo Y, Friston KJ. Experimental validation of the free-energy principle
with in vitro neural networks. Nature Communications. 2023;14(1):4547.
[20] Mediano PA, Rosas F, Carhart-Harris RL, Seth AK, Barrett AB. Beyond integrated information:
A taxonomy of information dynamics phenomena. arXiv preprint arXiv:190902297. 2019.
[21] Kitazono J, Kanai R, Oizumi M. Efficient search for informational cores in complex systems: Ap-
plication to brain networks. Neural Networks. 2020;132:232-44.
17
[22] Kitazono J, Aoki Y, Oizumi M. Bidirectionally connected cores in a mouse connectome: towards
extracting the brain subnetworks essential for consciousness. Cerebral Cortex. 2023;33(4):1383-402.
[23] Varley TF. Decomposing past and future: Integrated information decomposition based on shared
probability mass exclusions. PLOS ONE. 2023 03;18(3):1-31. Available from: https://doi.org/10
.1371/journal.pone.0282950.
[24] Luppi AI, Mediano PA, Rosas FE, Allanson J, Pickard J, Carhart-Harris RL, et al. A synergis-
tic workspace for human consciousness revealed by integrated information decomposition. Elife.
2024;12:RP88173.
[25] Beggs JM, Plenz D. Neuronal avalanches in neocortical circuits. Journal of neuroscience.
2003;23(35):11167-77.
[26] Pasquale V, Massobrio P, Bologna L, Chiappalone M, Martinoia S. Self-organization and neuronal
avalanches in networks of dissociated cortical neurons. Neuroscience. 2008;153(4):1354-69.
[27] Shew WL, Yang H, Yu S, Roy R, Plenz D. Information capacity and transmission are maximized in
balanced cortical networks with neuronal avalanches. Journal of neuroscience. 2011;31(1):55-63.
[28] YadaY,MitaT,SanadaA,YanoR,KanzakiR,BakkumDJ,etal.Developmentofneuralpopulation
activity toward self-organized criticality. Neuroscience. 2017;343:55-65.
[29] Ikeda N, Akita D, Takahashi H. Noise and spike-time-dependent plasticity drive self-organized
criticality in spiking neural network: Toward neuromorphic computing. Applied Physics Letters.
2023;123(2).
[30] Aguilera M, Di Paolo EA. Integrated information in the thermodynamic limit. Neural Networks.
2019;114:136-46.
[31] Kim H, Lee U. Criticality as a determinant of integrated information Î¦ in human brain networks.
Entropy. 2019;21(10):981.
[32] Popiel NJ, Khajehabdollahi S, Abeyasinghe PM, Riganello F, Nichols ES, Owen AM, et al.
The emergence of integrated information, complexity, and â€˜consciousnessâ€™ at criticality. Entropy.
2020;22(3):339.
[33] Mediano PA, Rosas FE, Farah JC, Shanahan M, Bor D, Barrett AB. Integrated information as a
commonsignatureofdynamicalandinformation-processingcomplexity. Chaos: AnInterdisciplinary
Journal of Nonlinear Science. 2022;32(1).
[34] Zaeemzadeh A, Tononi G. Shannon information and integrated information: message and meaning.
arXiv preprint arXiv:241210626. 2024.
[35] Kapur S. Psychosis as a state of aberrant salience: a framework linking biology, phenomenology,
and pharmacology in schizophrenia. American journal of Psychiatry. 2003;160(1):13-23.
[36] Itti L, Baldi P. Bayesian surprise attracts human attention. Advances in neural information pro-
cessing systems. 2005;18.
[37] Albantakis L, Hintze A, Koch C, Adami C, Tononi G. Evolution of integrated causal struc-
tures in animats exposed to environments of increasing complexity. PLoS computational biology.
2014;10(12):e1003966.
[38] Grasso M, Albantakis L, Lang JP, Tononi G. Causal reductionism and causal structures. Nature
neuroscience. 2021;24(10):1348-55.
[39] Friston K. Life as we know it. Journal of the Royal Society Interface. 2013;10(86):20130475.
[40] EdelmanGM. NeuralDarwinism: selectionandreentrantsignalinginhigherbrainfunction. Neuron.
1993;10(2):115-25.
18
[41] KilgardMP. Harnessingplasticitytounderstandlearningandtreatdisease. Trendsinneurosciences.
2012;35(12):715-22.
[42] Takahashi H, Yokota R, Kanzaki R. Response variance in functional maps: neural darwinism
revisited. PLoS One. 2013;8(7):e68705.
[43] FristonK,RigoliF,OgnibeneD,MathysC,FitzgeraldT,PezzuloG. Activeinferenceandepistemic
value. Cognitive neuroscience. 2015;6(4):187-214.
[44] Parr T, Pezzulo G, Friston KJ. Active inference: the free energy principle in mind, brain, and
behavior; 2022.
[45] ParrT,FristonKJ. Theactiveconstructionofthevisualworld. Neuropsychologia.2017;104:92-101.
[46] Leung A, Tsuchiya N. Emergence of integrated information at macro timescales in real neural
recordings. Entropy. 2022;24(5):625.
[47] MadhavanR,ChaoZC,PotterSM.Plasticityofrecurringspatiotemporalactivitypatternsincortical
networks. Physical biology. 2007;4(3):181.
[48] Yada Y, Kanzaki R, Takahashi H. State-dependent propagation of neuronal sub-population in
spontaneous synchronized bursts. Frontiers in systems neuroscience. 2016;10:28.
[49] BalliniM,MÃ¼llerJ,LiviP,ChenY,FreyU,StettlerA,etal. A1024-channelCMOSmicroelectrode
arraywith26,400electrodesforrecordingandstimulationofelectrogeniccellsinvitro. IEEEjournal
of solid-state circuits. 2014;49(11):2705-19.
[50] MÃ¼ller J, Ballini M, Livi P, Chen Y, Radivojevic M, Shadmani A, et al. High-resolution CMOS
MEA platform to study neurons at subcellular, cellular, and network levels. Lab on a Chip.
2015;15(13):2767-80.
[51] Barrett AB. Exploration of synergistic and redundant information sharing in static and dynamical
Gaussian systems. Physical Review E. 2015;91(5):052802.
[52] Bonett DG, Wright TA. Sample size requirements for estimating Pearson, Kendall and Spearman
correlations. Psychometrika. 2000;65(1):23-8.
[53] Higgins JP, Thompson SG. Quantifying heterogeneity in a meta-analysis. Statistics in medicine.
2002;21(11):1539-58.
[54] DerSimonianR,LairdN. Meta-analysisinclinicaltrials. Controlledclinicaltrials.1986;7(3):177-88.
19

=== REVISE TO ===
PROFESSIONAL TONE: Begin directly with content - NO conversational openings like 'Okay, here's...'

1. Fix all issues above
2. Title: "Bridging integrated information theory and the free-energy principle in living neuronal networks"
3. Include 10-15 quotes from paper text
   - Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
   - Use consistent quote formatting: 'The authors state: "quote"' or vary attribution phrases
   - Vary attribution phrases to avoid repetition
   - CRITICAL: Only extract quotes that actually appear in the paper text
4. ELIMINATE ALL REPETITION - each sentence must be unique
   - Check before each sentence: 'Have I already said this?' If yes, write something new
   - Vary attribution phrases - do NOT repeat 'The authors state' multiple times
5. Extract methodology, results with numbers, key quotes
6. 1000-1500 words, structured with ### headers

Generate COMPLETE revised summary.