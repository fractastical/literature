=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Active Inference in Hebbian Learning Networks
Citation Key: safa2023active
Authors: Ali Safa, Tim Verbelen, Lars Keuninckx

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2023

Key Terms: hebbian, imec, state, sparse, learning, inference, networks, active, network, coding

=== FULL PAPER TEXT ===

Active Inference in Hebbian Learning Networks
Ali Safa1,2,3, Tim Verbelen4, Lars Keuninckx2, Ilja Ocket2, Andr´e Bourdoux2,
Francky Catthoor1,2, Georges Gielen1,2, and Gert Cauwenberghs3
1 imec, Leuven, Belgium
2 ESAT, KU Leuven, Belgium
3 University of California at San Diego, La Jolla, USA
4 VERSES Research Lab, Los Angeles, California, USA
Ali.Safa@imec.be
Abstract. Thisworkstudieshowbrain-inspiredneuralensemblesequipped
with local Hebbian plasticity can perform active inference (AIF) in order
to control dynamical agents. A generative model capturing the environ-
mentdynamicsislearnedbyanetworkcomposedoftwodistinctHebbian
ensembles:aposterior network,whichinferslatentstatesgiventheobser-
vations, and a state transition network, which predicts the next expected
latent state given current state-action pairs. Experimental studies are
conducted using the Mountain Car environment from the OpenAI gym
suite, to study the effect of the various Hebbian network parameters
on the task performance. It is shown that the proposed Hebbian AIF
approach outperforms the use of Q-learning, while not requiring any
replay buffer, as in typical reinforcement learning systems. These results
motivate further investigations of Hebbian learning for the design of
AIF networks that can learn environment dynamics without the need for
revisiting past buffered experiences.
Keywords: Active Inference · Hebbian Learning · Sparse Coding.
1 Introduction
The study of Sparse Coding [1], [2], [3], [4] and Predictive Coding [5], [6], [7]
networkshasgainedmuchattentionforunderstandingthemechanismsunderlying
learning and inference in the brain [8]. In particular, it has been shown that the
learning of the weight dictionary used to project the input signals into sparse
codescanbeconductedviathebiologically-plausibleHebbianlearningmechanism
[9], with experimental evidence behind this mechanism observed in the brain
[10], [11]. Hebbian learning differs from the widely-used back-propagation of error
(backprop) technique due to its local nature [7], [12], [13], where the weight w
j
of neuron i is modified via a combination f of the weight’s input x and the
j
neuron’s output y (with η the learning rate parameter):
i d
w ←−w +η f(y ,x ) (1)
j j d i j
When applied to layers that embark some form of competition between their
neurons, the Hebbian mechanism in (1) leads to the unsupervised learning of
complementary features from the input signals [14].
At the same time, Active Inference (AIF) has gained huge interest as a
first-principle theory, explaining how biological agents evolve and perform ac-
tions in their environment [15], [16]. In recent years, the use of deep neural
3202
nuJ
22
]EN.sc[
2v35050.6032:viXra
2 A. Safa et al.
networks (DNNs) for parameterizing generative models has gained much atten-
tion in AIF research [17], [18], [19]. Deep AIF systems are typically composed
of a posterior network q (s |o ,a ), inferring the latent state s given an
ΦP l l−1 l−1 l
incoming observation-action pair {o ,a }, and a state-transition network
l−1 l−1
p (s |s ,a ), predicting the next latent state s given the current state-
ΦS l l−1 l−1 l
action pair {s ,a } [17]. The state-transition network is used to generate
l−1 l−1
the agent’s roll-outs for different policies in order to compute the Expected Free
Energy associated to each policy [17]. Finally, a likelihood network p (o |s )
ΦL l l
reconstructing the input observation o from the latent state s can also be
l l
implemented [20]. Each network parameterizes its respective density function
through weight tensors Φ ,Φ and Φ .
P S L
In this work, we aim to study how AIF can be performed in Hebbian learning
networks without resorting to backprop (as typically used in deep AIF systems).
Experiments conducted in the OpenAI Mountain Car environment [21] show
that the proposed Hebbian AIF approach outperforms the use of Q-learning and
compares favorably to the backprop-trained Deep AIF system of [17], while not
requiring any replay buffer, as in typical reinforcement learning systems [22]. Our
derivations and experiments add to a growing number of work addressing the
study of Hebbian Active Inference [23], [24].
Thispaperisorganizedasfollows.BackgroundtheoryaboutHebbianlearning
networks is provided in Section 2. Our Hebbian AIF methods are covered in
Section 3. Experimental results are shown in Section 4. Conclusions are provided
in Section 5.
2 Background Theory on Hebbian Learning Networks
Inspired by previous works that model the neural activity of biological agents
through Sparse Coding [5], [9] (such as in the mushroom body of an insect’s brain
[25]), we model each individual Hebbian Ensemble layer of our networks as an
identically-distributed Gaussian likelihood model with a Laplacian prior on the
neural activity c:
p(c|o,Φ)∼exp(−||Φc−o||2)exp(−λ||c|| ) (2)
2 1
where o is the input of dimension N, c is the output of dimension M, Φ is the
N ×M weight matrix of the layer (also called dictionary), and λ is a hyper-
parameter setting the scale of the Laplacian prior. Choosing a Laplacian prior is
motivatedbythefactthatitpromotessparsityintheoutputneuralcodeinaway
similartohowsparsityisinducedinnetworksofSpikingLeakyIntegrate-and-Fire
neurons, modelling cortical neural activity [9].
Under Sparse Coding (2), inference of c and learning of Φ is carried via [9]:
(cid:88)
C,Φ=argmin ||Φc −o ||2+λ||c || with C ={c ,∀l} (3)
l l 2 l 1 l
C,Φ
l
whichcanbesolvedviaProximalStochasticGradientDescent[26],byalternating
between: a) the inference of c , given the current input o and the weight Φ and
l l
b) the learning of Φ, given the current c and o .
l l
Hence, we instantiate Hebbian layers as the dynamical system given in (4),
where T denotes the transpose, η is the coding rate, η is the learning rate and
c d
Prox is the proximal operator to the l norm (non-linearity) [27]. For each
λ||.||1 1
input o , the neural and weight dynamics of the Hebbian network follows the
i
Active Inference in Hebbian Learning Networks 3
update rules in (4) for an arbitrary number of iterations (set to 100 in this work
as a good balance between speed of convergence and convergence quality), in
order to infer the corresponding c and learn Φ [9].
l
(cid:26) c ←Prox {c −η ΦT(Φc −o )}
l λ||.||1 l c l l (4)
Φ←Φ−η (Φc −o )cT
d l l l
with Prox acting as the neural non-linearity:
λ||.||1
Prox (c )=sign(c )max(0,|c |−η λ),∀i (5)
λ||.||1 i i i c
From aneural point ofview, the dynamicalsystem of (4)canbe implemented
asthenetworkarchitectureinFig.1,whereallweightupdatesfollowthestandard
Hebbian rule (1) [9].
Fig.1. Baseline Hebbian network architecture used in this work. The dynamics
ofthenetworkfollow(4)andminimize(3),givensubsequentinputvectorso.Eachlayer
possesses its own weight matrix Φ,Ψ which evolve through Hebbian plasticity (Ψ ∼ΦT
in (4), as an independent, local set of weights).
3 Active Inference in Hebbian Learning Networks
InthisSection,weshowhowtheHebbiannetworkdescribedaboveinSection2is
utilized in order to build an AIF system. First, we describe how Variational Free
Energy minimization can be performed by a cascade of two Hebbian networks: a
state-transition network predicting the next latent states given the previous ones,
andaposterior networkprovidinglatentstatesgiveninputobservations.Crucially,
itisshownthatFreeEnergyminimizationnecessitatestop-downHebbianlearning
connections from the state-transition network towards the posterior network,
steering the posterior output activity towards the state-transition output during
learning.Then,weshowhowtheExpectedFreeEnergyiscomputedbygenerating
state transition roll-outs.
4 A. Safa et al.
3.1 Minimizing the Variational Free Energy
The Variational Free Energy can be decomposed as [17] [28] (where E denotes
the expected value):
F =D [q (s |o ,a )||p (s |s ,a )]−E [log(p (o |s ))] (6)
KL ΦP l l−1 l−1 ΦS l l−1 l−1 q ΦL l l
with the parametrized densities q ,p ,p described in Section 1. Since the
ΦP ΦS ΦL
Hebbian network architecture used in this work intrinsically provides a mean to
reconstruct its input x in (3) (i.e., likelihood modelling) from its produced latent
l
code c in (3) (i.e., posterior modelling), using the same dictionary parameter
l
matrix Φ in (3) that was used to generate c via (4), we have Φ =Φ in (6) and
l L P
we will solely use Φ below to denote the posterior weight matrix.
P
Under the assumption of Gaussian likelihood with identity covariance in (2),
the KL divergence D in F can be simplified to [29]:
KL
F ∼||Φ c −{s (Φ ),a }||2+||Φ s −{o ,a }||2 (7)
S S,l l P l 2 P l l l 2
wheres (Φ )explicitsthedependencyofs onΦ (s beingtheposteriornetwork
l P l P l
outputactivity)andc denotestheoutputactivityofthestate-transitionnetwork
S
given s . The Free Energy in (7) must be minimized with regard to the state
l
transition weights Φ and the posterior weights Φ during learning:
S P
Φ ,Φ =arg min ||Φ c −{s (Φ ),a }||2+||Φ s −{o ,a }||2,∀l (8)
S P S S,l l P l 2 P l l l 2
ΦS,ΦP
This indicates that it is not only the state transition model that must be steered
towards the posterior model, but also, the posterior model must be steered
towards the output of the state-transition network. This effect can be achieved
by re-formulating the optimization in (8) as:
(cid:26) Φ =argmin ||Φ c −{s ,a }||2,∀l (a)
S ΦS S S,l l l 2 (9)
Φ =argmin ||Φ s −{o ,a }||2+||Φ (Φ c )−{o ,a }||2 (b)
P ΦP P l l l 2 P S S,l l l 2
Intuitively, the right-hand term in (9 b) steers the posterior model towards
the state-transition model by first re-projecting the output activity of the state-
transition network c into the latent space as Φ c (considering Φ fixed). Then,
S S S S
minimizing ||Φ (Φ c )−{o ,a }||2 modifies Φ in order to steer its posterior
P S S,l l l 2 P
output s towards the re-projected state-transition activity Φ c (considering
l S S
{o ,a } fixed).
l l
State-Transition Model Inspired by prior work on dictionary-based sequence
modeling [30], we implement the transition model p (s |s˜ ,a˜ ) as an auto-
ΦS l l−1 l−1
regressive Hebbiannetwork(seeFig.2a),takingasinputasequenceofstate-and-
action history s˜ =[s ,...,s ], a˜ =[a ,...,a ] and inferring
l−1 l−1 l−Lbuf l−1 l−1 l−Lbuf
thenextstates˜ =[s ,...,s ]asthere-projectionofitsinternalsparsecode
l l l−Lbuf
c in the input space through the network weights Φ :
S S
s˜ =Φ c (10)
l S S,l
where Φ and c respectively denote the weight vector and the sparse code
S,j S,j
of each layer j in the state transition network. Therefore, the state-transition
network effectively projects the L previous states (noted s˜ ) into a common
buf l−1
Active Inference in Hebbian Learning Networks 5
internal sparse code c and reconstructs the next states s˜ by re-projection of c
S l S
into the input space.
The state-transition network learns its weights Φ following (9 a) and infers
S
its output activity c via sparse coding (see Section 2):
S
Φ ,c =arg min ||Φ c −s˜||2+λ ||c || ,∀l (11)
S S,l S S,l l 2 P S,l 1
ΦS,cS,l
where λ is a parameter that sets the strength of the sparsity of the state-
P
transition output activity. (11) can therefore be implemented via the Sparse
Coding-based Hebbian learning ensemble described in Section 2. This auto-
regressive strategy enables the network to learn state predictions using Hebbian
learning, without the need for non-bio-plausible back-propagation through time
(BPTT) [30], [31].
In order to prevent the vanishing or exploding of the state transition model
when producing roll-outs further in time, we regularize the norm of the recon-
structed states s to an arbitrary magnitude α using (12). We keep α=5 in our
l
experiments in Section 4, giving a good balance for the dynamic range of the
network output activity (adjusted empirically).
s
s =α l (12)
l ||s ||
l 2
Fig.2. Hebbian Active Inference Architecture. a) The state-transition network
takes as input the L previous latent states produced by the posterior network, and
buf
projects them onto its internal representation c via the learned Φ . When producing
S S
roll-outs, the state-transition network estimates the next state sˆ by re-projecting the
l
output activity c due to [s ,...,s ] back onto the input space via (10). b) The
S l−Lbuf l−1
posterior network takes observation-action pairs as input and produces latent states s
(corresponding to the output in Fig. 1). In addition to the Hebbian mechanisms depicted
inFig.1,theweightsΦ oftheposteriornetworkarealsosubjecttoatop-downHebbian
P
learning mechanism for minimizing the second term in (9 b).
6 A. Safa et al.
Posterior Model Similar to the state-transition model, we use a Hebbian
ensemble as posterior model, where the internal sparse code c (see Fig. 2 b)
P
is identified as the hidden state s ≡ c inferred by the posterior network
l P,l
q (s |o ,a ), given the observation and action pair {o ,a } in (2).
ν l l−1 l−1 l−1 l−1
Therefore, the posterior network learns its weights Φ following (9 b) and
P
infers its output activity s =c via sparse coding (see Section 2):
l P,l
Top-down Connection
(cid:122) (cid:125)(cid:124) (cid:123)
Φ ,c =arg min ||Φ c −{o ,a }||2+λ ||c || +||Φ (Φ c )−{o ,a }||2
P P,l P P,l l l 2 Q P,l 1 P S S,l l l 2
ΦP,cP,l(cid:124) (cid:123)(cid:122) (cid:125)
StandardSparseCoding
(13)
foralll,whereλ setsthestrengthofthesparsity oftheposterior outputactivity.
Q
The left-hand standard sparse coding term in (13) can be implemented via the
Hebbian learning ensemble described in Section 2, while the right-hand term in
(13) can be implemented using top-down connections from the state-transition
output activity c towards the posterior network, via the state-transition weight
S
matrix Φ (see Fig. 2 b).
S
Finally,hereagain,weapplythehomeostasisrule(12)totheinferredposterior
state, effectively constraining s to lie on the α-sphere manifold.
l
3.2 Minimizing the Expected Free Energy
Given a policy π, the Expected Free Energy G(π) (EFE) can be written as [32]:
(cid:88)
G(π)= E [logq(s |π)−logp(s ,o |π)]
q(ol,sl|π) l l l
l
(cid:88)
= −H{q(s |π)}−E [logp(s ,o |π)] (14)
l q(ol,sl|π) l l
l
where H denotes the Shannon entropy. It can be seen in (14) that selecting a
policy that minimizes the EFE entails the maximization of the posterior entropy
(promoting exploration) and the joint posterior over the states and observations
(reaching the desired goal) [32].
In order to reach the desired goal, we produce roll-outs of states s given a
l
certain policy and approximate the term −E [logp(s ,o |π)] to be minimized as:
q l l
−E [logp(s ,o |π)]∼||s −s∗||2 (15)
q(ol,sl|π) l l l 2
wheres∗isthedesiredstatethattheagentmustreach,correspondingtoadesired
observation (e.g., the agent’s position). Since the observation o can encompass
l
more than just the goal to be reached (i.e., the observation o could be both the
l
position and the velocity of an agent, even though the desired goal is to reach a
specific position regardless of the velocity), we compute the desired goal state s∗
as:
(cid:90)
s∗ =argmax q(s|Ω∗,ω)dω (16)
s
ω∈Dω
where Ω∗ contains all observations that must be reached in order to attain the
desired goal and ω designates all observation modalities that are not taking part
in defining the goal that must be reached, with D their domain of definition.
ω
Active Inference in Hebbian Learning Networks 7
In practice, (16) is estimated by averaging the output of the posterior network,
while sweeping ω for a grid of possible values and keeping Ω∗ fixed.
Regardingtheexplorationtermin(14),ourHebbiannetworkdoesnotdirectly
allow the estimation of the entropy H{q(o ,s |π)}, since the network does not
l l
inferstandarddeviationsasinavariationalauto-encoder(VAE)[17].Wepropose
to replace the maximization of the entropy H{q(o ,s |π)} with a surrogate term,
l l
crafted to promote exploration as well. As a surrogate for H{q(o ,s |π)}, we
l l
choosetomaximizethevariance(notedVar)ofthestatetrajectorys ,∀l=1,...,L
l
alongtimeduringtheroll-outs.Intuitively,astatetrajectorythatpresentslotsof
variation in time will promote the exploration of new states, providing a similar
qualitative effect as maximizing H{q(o ,s |π)}. Therefore, we select the policy π
l l
such that the distance to the desired state is minimized, while achieving a state
trajectory variance larger than a certain threshold t .
v
L
(cid:88)
π∗ =argminG(π)= ||s −s∗||2 s.t. Var(||s −s∗||2,l=1,...,L)≥t
l 2 l 2 v
π
l=1
(17)
Given a set of N policies to try, t can be determined in an adaptive way as:
p v
1
t =β× [max(Var(||s (π)−s∗||2,∀l))+min(Var(||s (π)−s∗||2,∀l))] (18)
v 2 π l 2 π l 2
whereβ isthestrengthhyper-parameter(empiricallysetto0.5inourexperiments
reported below).
4 Experimental Results
The aim of our experimental studies is to determine i) how the main network
hyper-parameters (number of neurons, sparsity in output activity,...) impact the
successrateoftheproposedHebbianAIFsystem;ii) towhatextentHebbianAIF
is robust when learning without using a replay buffer and iii) how Hebbian AIF
compares to Q-learning (which uses dense rewards versus unsupervised learning
in Hebbian AIF).
4.1 Mountain Car Environment
We perform experiments in the Mountain Car environment from the OpenAI
gym suite [21]. In this task, a car starts at a random position at the bottom of a
hill and is expected to reach the top of a mountain within 200 time steps. The
agentissubjecttogravityandcannotreachthegoaltrivially,justbyaccelerating
towards it. Rather, the agent must learn to gain momentum before accelerating
towards the goal.
In this environment, the x-axis position x and the velocity v of the car
x
constitute the input observations to the Hebbian AIF network. Before feeding
the observation tuple (x,v ) to our Hebbian network, we normalize (x,v ) using
x x
(19) in order to equalize the dynamic range of the position and velocity signals:
(cid:40)
x←− x−µx
v
x
←− x σ −
σ
x
v
µ
x
vx (19)
8 A. Safa et al.
where (µ ,σ ) and (µ ,σ ) denote the mean and standard deviation of the
x x vx vx
position and velocity signals respectively (estimated during random environment
runs).
Weuseanactionspaceconstitutedbytwodiscreteactions:acceleratetotheleft
and accelerate to the right. In addition, each action is repeated for 10 consecutive
time steps once selected during the Expected Free Energy minimization in (17).
In order to compute the Expected Free Energy, we generate roll-outs of
L=200 time step predictions for 100 different random policies πj,j =1,...100
with equal probability of selecting the accelerate to the left or the accelerate to
the right actions.
As learning rate for the Hebbian learning mechanism (4), we use η =10−4
d
with a decay rate of 0.8 applied at the end of each successful episode, i.e. if the
episode terminates successfully, η ←−η ×0.8 (else no decay is applied on η ).
d d d
All weights are initialized randomly from a normal distribution with standard
deviation 0.01.
In the remainder of this Sections, we perform all our experiments using a
10-fold validation approach, by reporting the success rate curves as averages over
10 different runs (with 35 episodes per runs), with different random network
initializations. For each run, we compute the success rate curve using a moving
average window of size 5, and report the mean success rate curve by averaging
over the 10 runs, alongside with its standard deviation (see e.g. Fig. 3). We will
now study the impact of the various network hyper-parameters on the achieved
success rates.
4.2 Impact of the Number of Neurons in the Posterior and State
Transition Networks
Fig. 3 and 4 show the effect of sweeping the number of coding neurons M and
Q
M in both the posterior and state-transition networks. Fig. 3 shows that for
P
M <8, the success rate is sub-optimal, but reaches a steady plateau around
Q
M = 8 (orange curve in Fig. 3). Then, as M is increased for M > 8, the
Q Q Q
success rate becomes sub-optimal again, with dips in the performance along the
episodes (e.g., red curve in Fig. 3). This phenomenon can be explained as follows:
for M <8, the posterior network does not have enough parameters to capture
Q
the input dynamics into its latent space and under-fits, while for M >8, the
Q
posterior network starts over-fitting, reducing the success rate again.
Regarding the state-transition network, Fig. 4 shows that the higher the
number of neurons M , the flatter the success rate curves become, leading to
P
higher performance. The state transition network does not seem to over-fit as
M isincreased(forλ =10−4 keptfixed).Rather,Fig.4indicatesthatahigher
P P
state-transition network capacity is beneficial for capturing important dynamics
in the latent space, at the output of the posterior network.
4.3 Impact of the Sparsity of the Output Activity in the Posterior
and State Transition Networks
Fig. 5 and 6 show the effect of sweeping the sparsity-defining hyper-parameters
λ and λ in both the posterior and state-transition networks. For the posterior
Q P
network, Fig. 5 shows that the success rate performance initially grows as λ
Q
is increased from λ = 10−6 to λ = 10−5. Doing so, the non-linearity of the
Q Q
posterior network is increased, better capturing observation features into its
Active Inference in Hebbian Learning Networks 9
Fig.3.Impact on the success rate when changing the number of neurons M
Q
in the posterior network.
Fig.4.Impact on the success rate when changing the number of neurons M
P
in the state transition network.
10 A. Safa et al.
Fig.5.Impactonthesuccessratewhenchangingthesparsityhyper-parameter
λ in the posterior network.
Q
latent space. Then, as λ grows past λ =10−4, the success rate degrades again,
Q Q
indicating a too strong posterior network non-linearity.
Regarding the state-transition network, Fig. 6 shows that the lower λ , the
P
higher the success rate becomes. This suggests that making the state-transition
network more linear (i.e., lower λ ) better captures the dynamics of the latent
P
space produced by the posterior network (other parameters kept fixed).
4.4 Impact of the Time-Lag Buffer Length on Task Performance
Fig. 7 shows how the length L of the time-lag buffer impacts the achieved
buf
success rate. Initially, as L increases, the success rate increases as well, due
buf
to an increased availability of past latent states used by the state-transition
network to estimate the next expected state. Then, as L is further increased
buf
for L >20, the success rate drops again due to the addition of latent states
buf
from deep in the past that are less useful for estimating the present dynamics.
4.5 Comparing Hebbian AIF against the Use of a Replay Buffer and
against Q-learning
Fig.8comparesthesuccessrateobtainedusingourproposedHebbianAIFsystem
againsta) theuseofareplaybufferduringlearningandb) theuseofaQ-learning
agent. Experience replay is done by saving the history of observation-action pairs
in a buffer after each episode. After the end of the episode, a past experience is
randomly selected and used to train the Hebbian AIF system for one episode.
Regarding the Q-learning setup, we use a standard Q-table learning approach
[22], with the python implementation proposed in [33].
Active Inference in Hebbian Learning Networks 11
Fig.6.Impact on the success rate when changing the sparsity λ in the state
P
transition network.
Fig.7. Success rate when changing the time-lag buffer length L .
buf
12 A. Safa et al.
Fig. 8 shows that our Hebbian AIF system converges much faster than the
Q-learningsystemandbehavesinacomparablemannertotheHebbianAIFsetup
with a replay buffer. Indeed, the Q-learning agent needs much more episodes in
order to converge, despite the fact that it utilizes the dense rewards provided by
the Mountain Car environment [21] (versus unsupervised learning in the case of
Hebbian AIF). This confirms prior observations about the efficient convergence
of AIF systems, due to their ability to learn a generative model of environment
dynamics (versus supervised learning of a Q-table) [17].
Finally, it is interesting to note that, compared to the Deep AIF results
reported in [17] (using a fully-connected 2-hidden-layer network trained through
backprop), the Hebbian AIF system proposed in this work eventually reaches
∼ 100% success rate (see red curve in Fig. 6) while the system in [17] reaches
∼95%, motivating further investigations of Hebbian learning for AIF systems.
Fig.8. Hebbian AIF versus the use of a replay buffer and Q-learning (a).
Q-learning needs two orders of magnitude more episodes in order to converge (b).
5 Conclusion
This paper has investigated how neural ensembles equipped with local Hebbian
plasticity can perform active inference for the control of dynamical agents. First,
a Hebbian network architecture performing joint dictionary learning and sparse
coding has been introduced for implementing both the posterior and the state-
transition models forming our generative Active Inference system. Then, it has
been shown how Free Energy minimization can be performed by the proposed
Hebbian AIF system. Finally, extensive experiments for parameter exploration
and benchmarking have been performed to study the impact of the network
parameters on the task performance. Experimental results on the Mountain Car
environment show that the proposed system outperforms the use of Q-learning,
while not requiring the use of a replay buffer during learning, motivating future
investigations of using Hebbian learning for designing active inference systems.
Active Inference in Hebbian Learning Networks 13
Acknowledgement
This research was partially funded by a Long Stay Abroad grant from the
Flemish Fund of Research - Fonds Wetenschappelijk Onderzoek (FWO) - grant
V413023N. This research received funding from the Flemish Government under
the“OnderzoeksprogrammaArtifici¨eleIntelligentie(AI)Vlaanderen”programme.
References
1. Bruno A. Olshausen, David J. Field (1997). ”Sparse coding with an overcomplete
basis set: A strategy employed by V1?.” Vision Research, 37(23), 3311-3325.
2. Fang, M.S., Mudigonda, M., Zarcone, R., Khosrowshahi, A., Olshausen, B. (2022).
”LearningandInferenceinSparseCodingModelsWithLangevinDynamics.”Neural
Computation, 34(8), 1676-1700.
3. Lee, H., Battle, A., Raina, R., Ng, A. (2006). ”Efficient sparse coding algorithms.”
In Advances in Neural Information Processing Systems. MIT Press.
4. Ali Safa, Ilja Ocket, Andr´e Bourdoux, Hichem Sahli, Francky Catthoor, Georges
Gielen. (2022). ”A New Look at Spike-Timing-Dependent Plasticity Networks for
Spatio-Temporal Feature Learning.”
5. Friston, K., Kiebel, S. (2009). ”Predictive coding under the free-energy principle.”
Philosophical transactions of the Royal Society of London. Series B, Biological
sciences, 364, 1211-21.
6. Friston,K.Doespredictivecodinghaveafuture?.NatNeurosci21,1019–1021(2018).
https://doi.org/10.1038/s41593-018-0200-7
7. Umais Zahid, Qinghai Guo, Zafeirios Fountas.(2023). ”Predictive Coding as a Neu-
romorphic Alternative to Backpropagation: A Critical Evaluation.”
8. Olshausen, B., Field, D. (1996). ”Emergence of simple-cell receptive field properties
by learning a sparse code for natural images.” Nature, 381, 607-609.
9. Safa, A., Ocket, I., Bourdoux, A., Sahli, H., Catthoor, F., Gielen, G. (2022). ”Event
Camera Data Classification Using Spiking Networks with Spike-Timing-Dependent
Plasticity.” In 2022 International Joint Conference on Neural Networks (IJCNN)
(pp. 1-8).
10. Guo-qiangBi,Mu-mingPoo(1998).”SynapticModificationsinCulturedHippocam-
pal Neurons: Dependence on Spike Timing, Synaptic Strength, and Postsynaptic
Cell Type.” Journal of Neuroscience, 18(24), 10464–10472.
11. Rao, R., Ballard, D. (1999). ”Predictive coding in the visual cortex: a functional
interpretation of some extra-classical receptive-field effects.” Nature neuroscience,
2(1), 79–87.
12. A. Safa, I. Ocket, A. Bourdoux, H. Sahli, F. Catthoor and G. G. E. Gielen,
”STDP-driven Development of Attention-based People Detection in Spiking Neural
Networks,” in IEEE Transactions on Cognitive and Developmental Systems, 2022,
doi: 10.1109/TCDS.2022.3210278.
13. Neftci, E., Das, S., Pedroni, B., Kreutz-Delgado, K., Cauwenberghs, G. (2014).
”Event-driven contrastive divergence for spiking neuromorphic systems.” Frontiers
in Neuroscience, 7.
14. Dmitry Krotov, John J. Hopfield (2019). ”Unsupervised learning by competing
hiddenunits.”ProceedingsoftheNationalAcademyofSciences,116(16),7723-7731.
15. Parr, T., Pezzulo, G., Friston, K. (2022). ”Active Inference: The Free Energy
Principle in Mind, Brain, and Behavior.” The MIT Press.
16. Isomura,T.,Shimazaki,H.Friston,K.J.”Canonicalneuralnetworksperformactive
inference.” Commun Biol 5, 55 (2022). https://doi.org/10.1038/s42003-021-02994-2
17. C¸atal, O., Wauthier, S., De Boom, C., Verbelen, T., Dhoedt, B. (2020). ”Learning
Generative State Space Models for Active Inference.” Frontiers in Computational
Neuroscience, 14.
18. Ueltzh¨offer, K. ”Deep active inference.” Biol Cybern 112, 547–573 (2018).
https://doi.org/10.1007/s00422-018-0785-7
14 A. Safa et al.
19. Fountas, Z., Sajid, N., Mediano, P., Friston, K. (2020). ”Deep active inference
agents using Monte-Carlo methods.” In Advances in Neural Information Processing
Systems (pp. 11662–11675). Curran Associates, Inc..
20. Van de Maele, T., Verbelen, T., C¸atal, O., De Boom, C., Dhoedt, B. (2021).
”Active Vision for Robot Manipulators Using the Free Energy Principle.” Frontiers
in Neurorobotics, 15.
21. Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J.,
Zaremba, W. (2016). ”Openai gym.” arXiv preprint arXiv:1606.01540.
22. Sutton, R., Barto, A. (2018 ). ”Reinforcement Learning: An Introduction.” The
MIT Press.
23. Ororbia, A. G., Mali, A. (2022). ”Backprop-Free Reinforcement Learning with
ActiveNeuralGenerativeCoding.”ProceedingsoftheAAAIConferenceonArtificial
Intelligence, 36(1), 29-37.
24. Alexander Ororbia, Ankur Mali. ”Active Predicting Coding: Brain-Inspired Rein-
forcement Learning for Sparse Reward Robotic Control Problems.” IEEE Interna-
tional Conference on Robotics and Automation (ICRA) 2023.
25. Yuchen Liang, Chaitanya Ryali, Benjamin Hoover, Leopold Grinberg, Saket
Navlakha, Mohammed J Zaki, Dmitry Krotov (2021). ”Can a Fruit Fly Learn
Word Embeddings?.” In International Conference on Learning Representations.
26. Ablin, P., Moreau, T., Massias, M., Gramfort, A. (2019). ”Learning step sizes for
unfolded sparse coding.” In Advances in Neural Information Processing Systems.
Curran Associates, Inc.
27. Tsung-Han Lin, Ping Tak Peter Tang (2019). ”Sparse Dictionary Learning by Dy-
namicalNeuralNetworks.”InInternationalConferenceonLearningRepresentations.
28. Friston, K. ”The free-energy principle: a unified brain theory?.” Nat Rev Neurosci
11, 127–138 (2010). https://doi.org/10.1038/nrn2787
29. J. R. Hershey and P. A. Olsen, ”Approximating the Kullback Leibler Divergence
Between Gaussian Mixture Models,” 2007 IEEE International Conference on Acous-
tics, Speech and Signal Processing - ICASSP ’07, Honolulu, HI, USA, 2007, pp.
IV-317-IV-320, doi: 10.1109/ICASSP.2007.366913.
30. Kim, E., Lawson, E., Sullivan, K., Kenyon, G. (2019). ”Spatiotemporal Sequence
MemoryforPredictionUsingDeepSparseCoding.”InProceedingsofthe7thAnnual
Neuro-Inspired Computational Elements Workshop. Association for Computing
Machinery.
31. P.J.Werbos,”Backpropagationthroughtime:whatitdoesandhowtodoit,”inPro-
ceedingsoftheIEEE,vol.78,no.10,pp.1550-1560,Oct.1990,doi:10.1109/5.58337.
32. Schwartenbeck, P., FitzGerald, T., Dolan, R., Friston, K. (2013). ”Exploration,
novelty, surprise, and free energy minimization.” Frontiers in Psychology, 4.
33. https://gist.github.com/gkhayes/3d154e0505e31d6367be22ed3da2e955 (ac-
cessed May 1 2023)

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Active Inference in Hebbian Learning Networks"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
