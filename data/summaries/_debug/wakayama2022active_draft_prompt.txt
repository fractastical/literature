=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Active Inference for Autonomous Decision-Making with Contextual Multi-Armed Bandits
Citation Key: wakayama2022active
Authors: Shohei Wakayama, Nisar Ahmed

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2022

Key Terms: decision, armed, autonomous, bandits, contextual, inference, active, exploitation, making, multi

=== FULL PAPER TEXT ===

Active Inference for Autonomous Decision-Making with
Contextual Multi-Armed Bandits
Shohei Wakayama and Nisar Ahmed∗
Abstract—Inautonomousroboticdecision-makingunderun-
certainty, the tradeoff between exploitation and exploration of
availableoptionsmustbeconsidered.Ifsecondaryinformation
associated with options can be utilized, such decision-making
problems can often be formulated as contextual multi-armed
bandits (CMABs). In this study, we apply active inference,
which has been actively studied in the field of neuroscience
in recent years, as an alternative action selection strategy for
CMABs. Unlike conventional action selection strategies, it is
possible to rigorously evaluate the uncertainty of each option
whencalculatingtheexpectedfreeenergy(EFE)associatedwith
the decision agent’s probabilistic model, as derived from the
free-energy principle. We specifically address the case where a
categorical observation likelihood function is used, such that Fig.1. Aroboticlanderreasonsaboutwhethercandidatedigsitesonan
icy moon contain scientifically interesting data. To speed up the process,
EFE values are analytically intractable. We introduce new
it is desirable not only to balance exploitation and exploration, but also
approximationmethodsforcomputingtheEFEbasedonvaria-
incorporateaselectionbias,i.e.priorpreferences,fordesiredobservations.
tionalandLaplaceapproximations.Extensivesimulationstudy
results demonstrate that, compared to other strategies, active
inference generally requires far fewer iterations to identify high radiation and hours long round trip time delays for
optimal options and generally achieves superior cumulative speedoflightsignals)makesmoothhuman-robotinteraction
regret, for relatively low extra computational cost. hard in such missions [1], [2]. Thus, the robot must au-
tonomouslyandaccuratelyinfertheoutcomeprobabilitythat
I. INTRODUCTION
each area contains valuable objects, while using sensors that
In robotics, autonomous robots must often optimize are easily deployable and consume little energy. To improve
choices from among multiple alternatives to accomplish a efficiency,therobotmuststrikeabalancebetweenincreasing
task under uncertainty. Such decision-making can be com- the certainty of plausible areas (exploitation) and reducing
plex when the outcomes obtained for different options are the uncertainty of unknown areas (exploration), taking into
stochastic and their distributions are unknown a priori. If account the surrounding environment, mission status, etc.
a robot could devote unlimited time and resources to the
Such a decision-making problem can be formulated as a
decision-making process, it would be possible to simulate
contextual multi-bandit (CMAB), a type of reinforcement
all options a significant number of times and derive accu-
learning problem which has originally been studied and
rate outcome distributions before finalizing the best option.
applied in recommendation systems [3] and recently utilized
However, in many environments where autonomous robots
in robotics [4], [5]. Generally speaking, however, bandit
areactuallyexpectedtobedeployed,timeandresource(e.g.
problems(dependingontheirscaleandcomplexity)typically
bandwidth and energy) constraints are severe. This can also
require a large number of iterative interactions with the
hampertheabilityofhumanuserstoeffectivelyinteractwith
problem environment to finalize an optimal option, which
robots. In such settings, lightweight and effective algorithms
can be a bottleneck in applying them to many practical
are needed for decision-making with limited information.
robotic applications like space exploration. Therefore, to
For instance, as illustrated in Fig. 1, suppose a robot in a
address such concerns, this paper considers active inference
remote environment (such as an icy solar system moon) is
[6], [7] as an action selection strategy for CMAB problems.
assigned a task of collecting scientifically interesting objects
Activeinferencehasrecentlyattractedattentioninthefieldof
(e.g. ice deposits and minerals) from a single search area
neuroscience because it can mathematically rigorously take
using a resource-intensive manipulator. Although the robot
into account the uncertainty of each option in the process
could send image and spectral data about each area to
by calculating a measure named expected free energy (EFE)
human experts and let them decide where to investigate,
and incorporate a selection bias toward preferred outcomes.
mission time and communication constraints (e.g. due to
In this way, it is theoretically possible to determine the best
optionwithfeweriterations.Infact,activeinferencehasbeen
∗Work supported by the NASA COLDTech Program, grant
#80NSSC21K1031. S. Wakayama was also supported by the appliedtostandardstationaryanddynamicswitchingMABs
Masason Foundation. The authors are with the Smead Aerospace and those properties have been experimentally verified [8],
Engineering Sciences Department, University of Colorado
but no theory or studies have yet applied it to the important
Boulder, Boulder, CO 80303 USA [shohei.wakayama;
nisar.ahmed]@colorado.edu case of contextual bandits. Also, for CMABs with discrete
3202
beF
42
]OR.sc[
2v58190.9022:viXra
categoricalactionoutcomes,theEFEtermsaregenerallynot In standard MAB problems, only the information about
tractable for general context-dependent likelihood models. each option itself (e.g. the cumulative value of outcomes
Thus, we propose two novel statistical EFE approximation and/or the number of times it is selected) is taken into
methods based on variational inference and Laplace approx- account during action selection. However, it may be pos-
imation, to permit the use of active inference for solving sible to minimize the cumulative regret more effectively
CMABs. These methods are demonstrated and validated in and find the best option with fewer iterations, by utilizing
a simulated autonomous search area selection problem, and contextualinformationassociatedwitheachoption(e.g.user
their performances are compared with existing state of the age in case of web advertisement). Variants of stochastic
art CMAB action selection strategies. For the remainder of MABsthatincorporatesuchsecondaryinformationarecalled
the paper: Sec. II reviews the MAB and CMAB problems, contextual multi-armed bandits (CMABs) and have been
as well as an overview of active inference and the free- used for dialogue systems, anomaly detection, etc. [14]. In
energy principle behind the theory; Sec. III formulates the robotic decision-making problems, it is typical to collect
action selection strategy for CMAB problems using active and use multiple sources of side information, so we will
inference, and derives the EFE approximations; Sec. IV focus on CMAB problems hereafter. However, conventional
presents the setup and results of the simulation experiment action selection strategies for CMABs use heuristics (e.g.
(motivated by the autonomous remote exploration example); the exploration bonus term in UCB) to select options, and
and Sec. V presents conclusions. these methods usually require many iterations for informed
decision-making [3], [9]. Thus, it is desirable to develop an
II. BACKGROUNDANDRELATEDWORK alternative action selection strategy for robotics applications
where the number of iterations cannot be too large.
A. Multi Armed Bandits (MABs) and Contextual MABs
B. Free-Energy Principle and Active Inference
The multi-armed bandit (MAB) is a classic reinforcement
learning problem of optimal selection from multiple alterna- The free-energy principle is a theory in neuroscience that
tives with uncertain outcomes. The MAB allows a decision- isexpectedtoexplainwhole-brainmechanisms[18].Accord-
making agent like an autonomous robot to account for ing to this principle, biological agents perceive, learn, and
the tradeoff between exploitation (i.e. preferring alternatives act by minimizing free-energy, which is the upper bound of
with the higher expected outcome) and exploration (i.e. surprise (i.e. −logp(o), where o is an observation/outcome)
preferring alternatives with a more uncertain distribution observed from the external world, to maintain their home-
of outcomes) [9]. Although seemingly simpler than other ostasis according to neurally encoded probabilistic models.
sequential decision-making problems such as the Markov Inneuroscience,thisprinciplehasbeenusedtoexplainmany
decision process (MDP) [10], [11] and the partially ob- physiological phenomena, such as predictive coding [19].
servable Markov decision process (POMDP) [12], [13], the Activeinferenceisamathematicalframeworkthatapplies
theoryofMABshaslongbeenthesubjectofresearchinthe the free-energy principle to the behavioral norms of biologi-
field of statistical machine learning and has been applied to cal agents, and has recently begun to attract attention in not
solving problems in the fields of recommendation systems, only neuroscience and but also robotics [20]. Since future
healthcare, and finance [14]. While there are several major observations are unobservable until an action is executed,
formulationsofMABssuchasstochasticbandits[15]andad- in active inference, decision-making agents are assumed
versarialbandits[16],dependingonhowuncertainoutcomes to act to minimize a measure called expected free energy
are handled, here we focus on stochastic bandits treating (EFE).Thereasonwhythisframeworkistheoreticallysound
both present and future outcomes in the same fashion, i.e. for sequential decision-making problems is that the EFE is
withoutanydiscounting.Thissimplifiesthedevelopmentand composed of value and information gain terms. Thus, by
is appropriate in settings where it may not be possible to minimizing the EFE, it is possible to naturally balance the
predict in advance how many iterations will be required for tradeoff between exploitation and exploration in a princi-
an agent to learn an action selection policy. pled non ad-hoc manner. Additionally, active inference can
More formally written, a stochastic MAB problem aims easily incorporate prior information about the probability
to minimize the cumulative regret, i.e. the sum of difference distributionofoutcomesthatagentsdesiretoobserve,which
between the maximum expected outcome and an actual is called a prior preference (a.k.a “evolutionary prior”). By
outcomeoveriterations,viaalternatelyperformingtwosteps: adjusting this distribution, agent behavior can be varied.
(1) action selection: selecting one option (a.k.a “arm”) from Adjustment of the prior preference values corresponds to a
alternatives with unknown outcome distributions; and (2) typeofrewardtuningrequiredintypicalsequentialdecision-
observation update: updating the statistics for the selected makingproblems.Yet,specifyingtheprobabilityofobserved
option based on the outcome obtained. In stochastic bandits, outcomes desired by the agent arguably provides a theoreti-
itiscommontoapplytheBayes’ruletoupdatetheposterior callyintuitiveandelegantalternativetospecifyingnumerical
of the estimated outcome distribution of the selected option reward values over intermediate actions and states. 1
in step (2). For step (1), various action selection methods
1Thereexiststudiesattemptingtoestimateanappropriatepriorpreference
such as (cid:15)-greedy, the upper confidence bound (UCB) [15],
in terms of reward functions via inverse reinforcement learning [21],
and the Thompson sampling (TS) [17] have been devised. howeverthistypeoflearningisnotthefocusofourstudy.
However, active inference has only recently been consid- the CMAB problem defined in Sec. III-A can be expressed
ered for multi-armed bandit problems, namely in [8]. Their by marginalizing the following joint distribution,
work has shown that an active inference action selection (cid:90) (cid:90)
−logp(o)=−log ··· p(o,θ(cid:126) ,··· ,θ(cid:126) )dθ(cid:126) ···dθ(cid:126) . (3)
strategy produces higher performance (i.e. smaller cumula- 1 K 1 K
θ(cid:126)
1
θ(cid:126)
K
tive regrets) than the state-of-the-art methods, especially for
However, calculating (3) directly via multiple integral is
stationary MABs when the number of total iterations are oftenanalyticallyintractable,soinsteaditsupperbound(i.e.
relatively small and for dynamic switching MABs. Never- free-energy) is minimized. Yet, outcomes are unobservable
theless, their study does not focus on contextual bandits, until an option is executed, in the case of active inference,
the expected free energy (EFE) is considered. Note, in the
which are arguably even more applicable and interesting for
following, Θ(cid:126) represents [θ(cid:126) ,··· ,θ(cid:126) ]T,
autonomousrobotics.Thus,inthefollowing,afterreviewing 1 K
CMAB problems (where in particular outcomes are cate- EFE(a )= (cid:90) q(Θ(cid:126)|a ) (cid:88) p(o|Θ(cid:126),a )log q(Θ(cid:126)|a k ) dΘ(cid:126). (4)
gorical), we explain how the EFE expression is derived in k Θ(cid:126) k o k p(Θ(cid:126),o|a k )
these problems. We then derive novel EFE approximations
where q(Θ(cid:126)|a ) is a proposal distribution that approximates
to address the fact that the resulting terms are analytically k
the posterior distribution p(Θ(cid:126)|o,a ) and p(o|Θ(cid:126),a ) is an
intractable due to the observation likelihoods. k k
observation likelihood. By further transforming the log term
in (4) as follows
III. METHODOLOGIES
q(Θ(cid:126)|a ) q(Θ(cid:126)|a )
A. Problem Statement log k = log k
p(Θ(cid:126),o|a ) p(Θ(cid:126)|o,a )p(o|a )
In a contextual multi-armed (CMAB) problem introduced k k k
q(Θ(cid:126)|a ) 1
inSectionII-A,supposethetotalnumberofoptions(actions ≈ log k +log
a) taken into account by an autonomous robot is K. These p(Θ(cid:126)|o,a k ) p ev (o)
optionscorrespondtothebanditarms.Alsosupposethatthe q(Θ(cid:126)|a )p(o,a ) 1
= log k k +log
observed outcome o k of each option/action a is binary, i.e. p(Θ(cid:126),o,a k ) p ev (o)
o k ∈{0,1} ∀k ∈{1,··· ,K}. 2 Note, we use the shorthand q(Θ(cid:126)|a )p(o|a )p(a ) 1
the notation a k ↔a=k. = log p(o|Θ(cid:126),a k k )p(Θ(cid:126)| k a k )p( k a k ) +log p ev (o)
Since the number of outcomes is binary, the probability
q(o|a ) 1
ψ that each option k outputs a preferred outcome, o =1, ≈ log k +log (5)
de k pends on a hidden linear parameter vector θ(cid:126) (uniq k ue to p(o|Θ(cid:126),a k ) p ev (o)
k
and substituting (5) back to (4), the EFE is approximated as
eachoption)andasharedcontextvector(cid:126)x,andisrepresented
bythefollowingsigmoidfunction(withoutlossofgenerality, EFE(a )≈ (cid:90) q(Θ(cid:126)|a ) (cid:88) p(o|Θ(cid:126),a )log q(o|a k ) dΘ(cid:126),
(cid:126)x is assumed common for all options and iterations), k Θ k o k p ev (o)·p(o|Θ(cid:126),a k )
(6)
eθ(cid:126) k T(cid:126)x 1
ψ k =p(o k =1)= 1+eθ(cid:126)
k
T(cid:126)x = 1+e−θ(cid:126)
k
T(cid:126)x . (1) w
p
he
(
r
o
e
)i
q
s
(
a
o|
p
a
r
k
io
)
r
i
p
s
re
t
f
h
e
e
ren
m
ce
a
.
rg
F
i
o
n
r
al
b
iz
re
a
v
ti
i
o
ty
n
,b
o
y
f
fu
q
r
(
t
o
h
,
e
Θ(cid:126)
ra
|a
s
k
su
)
m
a
in
n
g
d
ev
thatthehiddenlinearparametervectorsareindependenteach
In the case of such a Bernoulli contextual bandit, the
other, eq.(6) can be rewritten as
expected outcome of each option is also ψ so that the
k
cumulative regret is expressed as follows, EFE(a k )≈
Regret(T)=Tψ∗− (cid:88) K N (k)ψ , (2) (cid:90) θ(cid:126) k q(θ(cid:126) k |a k ) (cid:88) o p(o|θ(cid:126) k ,a k ) (cid:110) log q p ( e o v |a (o k ) ) −logp(o|θ(cid:126) k ,a k ) (cid:111) dθ(cid:126) k
T k (cid:90)
k=1 =− (cid:88) q(θ(cid:126) k |a k )p(o|θ(cid:126) k ,a k )dθ(cid:126) k ·logpev(o)
o
θ(cid:126)
k
w e N x h p e e ( r k c e t ) e T d re is p o r u t e h t s c e e o n t m o ts t e al h ( n o u u w n m k m n b o e a r w n o y n f t i a i t m er p e a r s t i i o o a r n n i s, o f ψ o p r t ∗ io i t n s he t k he ro i m s bo a s x e t) l i , e m c a u t n e m d d − (cid:88) o (cid:90) θ(cid:126) k q(θ(cid:126) k |a k )p(o|θ(cid:126) k ,a k )·log p p (θ(cid:126) (θ k (cid:126) k |o |a ,a k k ) ) dθ(cid:126) k
T (cid:88)
within T iterations. The goal of CMAB problems is to
=− q(o|a
k
)·logpev(o)
o
minimize expected cumulative regret. To do so, the robot
n θ(cid:126) ee ∀ d k s , to k e ∈ sti { m 1 a , t · e ·· th , e K v } al i u n es th o e f p th ro e c h e i s d s d o e f n fi p n a d ra in m g e a te n r o v p e t c im to a rs l − (cid:88) o q(o|a k ) (cid:90) θ(cid:126) k q(θ(cid:126) k |o,a k )·log p p (θ(cid:126) (θ k (cid:126) k |o |a ,a k k ) ) dθ(cid:126) k
k (cid:104) (cid:105) (cid:104) (cid:105)
option by iteratively performing the two steps of action =−E q(o|ak) logpev(o) −E q(o|ak) DKL(q(θ(cid:126) k |o,a k )||q(θ(cid:126) k |a k )) .
selection and measurement update. (7)
As can be seen in (7), minimizing EFE value naturally
B. Expected Free Energy in CMABs achieves balancing exploitation (the first term) and explo-
As already mentioned in Sec. II-B, according to the free- ration (the second term). Thus, in CMAB problems with
energy principle, an agent wants to avoid (i.e. minimize) active inference, the robot needs to choose the option mini-
surprise tomaintain homeostasis.The surprisein thecase of
mizing the EFE term in each action selection step,
2The methodology described in this section is extendable to more than a =argmin EFE(a). (8)
selected
twooutcomecategorieswithafewstraightforwardmodifications. a∈|A|
Yet, as the observation likelihood p(o|θ(cid:126) ,a ) is the sigmoid unnormalizedGaussian.Bynormalizingthisdistribution,the
k k
function, the EFE term cannot be computed analytically. approximated normalization constant Cˆ = (cid:82) q(θ(cid:126) |a )·
(cid:16) (cid:17)
vb θ(cid:126)
k
k k
Hence, in the following subsections, we introduce the two exp G+HTθ(cid:126) − 1θ(cid:126)TKθ(cid:126) dθ(cid:126) required for calculating (7)
statistical approximation methods: (1) variational Bayesian k 2 k k k
can be obtained from the well-known Gaussian integral.
importancesampling(VBIS)[22],whichhasbeenappliedto
During this process, the VB posterior, which approximates
other probabilistic decision-making under uncertainty prob- the posterior p(θ(cid:126) |o,a ), is also derived as a side product.
k k
lems in robotics [23], [24]; and (2) Laplace approximation
[25],whichiscommonlyusedinstatisticalmachinelearning. p(θ(cid:126) k |o,a k )≈p vb (θ(cid:126) k |o,a k )
Note that, instead of the sigmoid function, a conditional q(θ(cid:126) |a )·exp(G+HTθ(cid:126) − 1θ(cid:126)TKθ(cid:126) )
= k k k 2 k k (15)
probability table (CPT) [25] could be used to express the (cid:82) q(θ(cid:126) |a )·exp(G+HTθ(cid:126) − 1θ(cid:126)TKθ(cid:126) )dθ(cid:126)
observation likelihood and achieve tractable posterior ex- k k k 2 k k k
pressions for the EFE (and CMAB inference more generally However, the value of Cˆ vb is smaller than the true nor-
using other approaches like (cid:15)-greedy). However, this is gen- malization constant C = (cid:82) θ(cid:126)
k
q(θ(cid:126) k |a k )p(o|θ(cid:126) k ,a k )dθ(cid:126) k , the
erallyfarlesspractical,sincethenumberofCPTparameters importance sampling (IS) is applied to improve the estimate
increases exponentially with the dimension of (cid:126)x, whereas by following [22].
this only increases linearly for the sigmoid likelihood. IS can approximate the expected value of an arbitrary
function f(θ(cid:126) ) over a pdf such as a posterior that is difficult
k
C. VBIS EFE Approximation to sample directly, by utilizing a proposal distribution that is
roughlysimilarinshapetotheposteriorandeasytosample.
ThevariationalBayesianimportancesampling(VBIS)isa
Here, in particular, the IS approximation is constructed as
hybrid data fusion method that approximates an analytically
intractable posterior distribution in human-robot collabora-
tivesensingwhenaflexiblediscrete-continuousfunction,e.g. (cid:104)f(θ(cid:126) )(cid:105)=(cid:104)θ(cid:126) (cid:105)≈ (cid:88) Ns w θ(cid:126) ,w ∝ q(θ(cid:126) k,s |a k )p(o|θ(cid:126) k,s ,a k ) ,
softmax function, is used as an observation likelihood [22]. k k s k,s s p (θ(cid:126) )
s=1 prop k,s
The main idea behind the VBIS algorithm is that a softmax (16)
functioncanbelower-boundedbyavariationalGaussianvia where N is the number of samples, w is the importance
s s
the inequality proved in [26] so that if a prior distribution is weight for sample s, and p (θ(cid:126) ) is the proposal pdf
a Gaussian, a posterior distribution is also approximated as prop k
another Gaussian. Here, since the logistic sigmoid function p (θ(cid:126) )=N(µ(cid:126) ,Σ ) (17)
intheEFEexpressionisaspecialcaseofasoftmaxfunction prop k vb prior
with two classes, the following log-sum-exponential (LSE) whereµ(cid:126) istheVBmeanandΣ isthepriorcovariance
vb prior
function is upper bounded as follows matrix of q(θ(cid:126) |a ). During this procedure, the improved
k k
log (cid:16)(cid:88) 1 eyh (cid:17) ≤α+ (cid:88) 1 y h −α−ξ h n (cid:80) orm w al / iz N ati . on Th c u o s n , s t t h an e t fi e r s s t t im te a r t m e C i ˆ n v s b i i d s e c t a h n e b su e m c m al a c t u i l o a n ted ov a e s r
2 s s s
(9) outcome o in the second equation of (7) is rewritten as
h=0 h=0
(cid:104) (cid:105)
+λ(ξ h ) (y h −α)2−ξ h 2 +log(1+eξh), log q(o|a k ) · (cid:90) q(θ(cid:126) |a )p(o|θ(cid:126) ,a )dθ(cid:126) = (cid:110) log C vbis (cid:111) ·C .
where y 0 = 0, y 1 = θ(cid:126)T(cid:126)x, λ(ξ h ) = 2ξ 1 h [ 1+e 1 −ξh − 2 1]. p ev (o) θ(cid:126) k k k k k k p ev (o) ( v 1 b 8 i ) s
α and ξ are free variational parameters which can be
h Similarly, the second term is approximated as
adjusted to achieve the tighter bound of the LSE and be
(cid:90)
iteratively updated within the VBIS procedure. By replacing q(θ(cid:126) |a )p(o|θ(cid:126) ,a )logp(o|θ(cid:126) ,a )dθ(cid:126)
k k k k k k k
thedenominatorof(1)withthisbound,thesigmoidfunction θ(cid:126)
k
is lower bounded by the variational Gaussian. (cid:90) (cid:16) 1 (cid:17)
=C p (θ(cid:126) |o,a ) G+HTθ(cid:126) − θ(cid:126)TKθ(cid:126) dθ(cid:126) . (19)
p(o=j|θ(cid:126) )≥exp
(cid:16)
G+HTθ(cid:126) −
1
θ(cid:126)TKθ(cid:126)
(cid:17)
, (10)
vbis θ(cid:126)
k
vbis k k k 2 k k k
k k 2 k k However, the quadratic term in (19) (i.e. G + HTθ(cid:126) −
k
where G, H, and K are 1θ(cid:126)TKθ(cid:126) ) has not yet been processed via the IS step. There-
2 k k
fore,itisrequiredtore-updatethelower-boundedvariational
1
G = (cid:88)(cid:104)ξ
2
h +λ(ξ
h
)(ξ
h
2−α2)−log(1+eξh) (cid:105) , (11) G
un
a
d
u
e
s
r
s
e
i
s
a
t
n
im
f
a
r
t
o
i
m
ng
t
a
h
n
e
E
f
F
o
E
llo
v
w
al
i
u
n
e
g
.
identical equation to avoid
h=0
 C vbis ·N(µ(cid:126) vbis ,Σ vbis )=N(µ(cid:126) prior ,Σ prior )·N(µ(cid:126) sigm ,Σ sigm )
1
H=
 −
2
(cid:126)x+2λ(ξ
1
)·α·(cid:126)x if o=0, (12)
After re-deriving G(cid:48), H(cid:48), and K(cid:48) terms of N(µ(cid:126) sigm ,Σ sig
(
m
20
)
)
,
1 (19) is further transformed as follows.
2 (cid:126)x+2λ(ξ
1
)·α·(cid:126)x if o=1, (13)
(cid:104) 1 (cid:105)
(19)=C ·E G(cid:48)+H(cid:48)Tθ(cid:126) − θ(cid:126)TK(cid:48)θ(cid:126)
K=2λ(ξ 1 )·(cid:126)x·(cid:126)xT. (14)
vbis
(cid:16)
pvbis(θ(cid:126) k|o,ak)
1(cid:16)
k 2 k k
(cid:17)(cid:17)
=C G(cid:48)+H(cid:48)Tµ(cid:126) − Tr(K(cid:48)Σ )+µ(cid:126)T K(cid:48)µ(cid:126)
Thus, in our problem, if we assume that the prior dis- vbis vbis 2 vbis vbis vbis
(21)
tribution of a hidden linear parameter vector q(θ(cid:126) |a ) is
k k
approximated as a (multivariate) Gaussian, the joint distri- After subtracting (21) from (18), and sum the EFE(a ,o)
k
bution q(θ(cid:126) |a ) · p(o|θ(cid:126) ,a ) in (7) becomes also another term over possible outcomes o, the EFE(a ) is computed.
k k k k k
Algorithm 1 Active inference action selection for CMABs N(µ(cid:126) ,Σ ) and re-deriving G(cid:48), H(cid:48), and K(cid:48) terms
laplace laplace
Input: Context vector(cid:126)x, evolutionary prior pev(o), estimated means and of N(µ(cid:126)
sigm
,Σ
sigm
)), the EFE(a
k
) of each option k is
covariances,numberofsamplesforimportancesamplingNs
calculated, and then the best option can be selected.
Output: aselectedindexforactionselection
1: foreachoptiondo
IV. SIMULATIONSTUDY
2: foreachoutcomedo
3: obtainµ(cid:126)posterior,Σposterior,andCˆ posterior fromtheVBIS A. Motivating Problem Scenario
ortheLaplaceapproximationalgorithm[22],[25]
4: re-deriveG(cid:48),H(cid:48),K(cid:48) termsfrom(20) NASA recently launched the Concepts for Ocean worlds
5: calculateEFE(a k ,o)via(18)and(21) LifeDetectionTechnology(COLDTech)programtoresearch
6: endfor
7: calculateEFE(a k )=(cid:80) o EFE(a k ,o) and develop software for ‘fail-operational’ autonomous
8: endfor robotic landers that will be dispatched to icy solar system
9: return aselectedindex=argminEFE(a) moonslikeEuropaandEnceladus[2].Currently,theconcept
a∈|A|
of operations calls for a robotic lander to perform basic
surface tasks such as digging, imaging, moving objects,
retrieving/analyzing samples with various instruments, and
Then,byfollowing(8),anactiveinferencerobotfinalizesthe
downlinked selected science data. As shown in Fig. 1, a key
option in an action selection step. Algorithm 1 summarizes
capability will be autonomous selection of surface areas to
the process of active inference action selection for CMABs
prioritize for science investigation. Although human experts
with the VBIS EFE approximation.
can specify preferences for possible areas in advance, the
D. Laplace EFE Approximation unknown dynamic nature of icy moon environments can
cause science priorities to shift unexpectedly, e.g. due to
In statistical machine learning, the Laplace approximation
plume eruptions or changes in surface conditions. Such
is commonly used to approximate a continuous probability
priorities must be considered in light of contextual infor-
densityfunctionasaGaussiandistribution[25].Thisusesthe
mation pertaining to overall mission progress, lander state,
second-order approximation of the vector Taylor expansion
and engineering factors.
of a logarithmic function whose gradient is (cid:126)0. Particularly,
In the following, we consider proof of concept CMAB
in the approximation of the EFE value in (7), a function
simulationsforaroboticlanderwhichhasbeengiventhetask
g(θ(cid:126) ) is defined as q(θ(cid:126) |a )p(o|θ(cid:126) ,a ), and the following
k k k k k of collecting an ice sample specimen from one of K non-
logarithmic function is used.
overlapping designated reachable areas within the landing
logg(θ(cid:126) k )≈logg(θ(cid:126) k (0))+ (cid:88) n (θ k,r −θ k (0 ,r )) ∂g ∂ ( θ θ(cid:126) k (0)) s b i a t s e e . d Th o e n c im an a d g i i d n a g te d s a a ta m c p o li l n le g ct s e i d tes im ar m e e s d e i l a e t c e t l e y d a b ft y er sc la ie n n d t i i n s g ts .
k,r
r=1 Since a sampling action consumes a considerable amount
+
1(cid:110)(cid:88) n
(θ −θ(0))
∂logg(θ(cid:126)
k
(0))(cid:111)2
, (22)
of resources (e.g. energy and time for grinding, drilling,
2 k,r k,r ∂θ scooping, etc.), the lander cannot just deploy a resource-
k,r
r=1
intensive manipulator in a haphazard way. It is necessary to
where θ(cid:126)(0) satisfies ∇logg(θ(cid:126)(0))=(cid:126)0 and n is the length of inferinadvancewhichareascanyieldscientificallyvaluable
k k
θ(cid:126) k . However, since it is analytically intractable to find θ(cid:126) k (0), information,i.e.o k =1,usingalaserspectrometersimilarto
θ(cid:126) is computed via Newton’s method [27]. Also, the thoseusedonMartianrovers[28],whichcanbemoreeasily
k,MAP
third term in (22) is rewritten as and cheaply deployed than the manipulator tools. Panoramic
images of the landing area taken onboard are also processed
(cid:110)(cid:88) n
(θ −θ(0))
∂logg(θ(cid:126)
k
(0))(cid:111)2
=
(cid:110)
(θ(cid:126) −θ(cid:126)(0))T∇logg(θ(cid:126)(0))
(cid:111)2
, by a learning-based computer vision algorithm to produce
k,r k,r ∂θ k k k
r=1 k,r a C-dimensional binary context feature vector (cid:126)x, to encode
(23)
variousfactorssuchassundirection,surfacereflectance,etc.
therefore, (22) reduces to
[29], [30]. The autonomous site selection problem can thus
logg(θ(cid:126) k )≈logg(θ(cid:126) k,MAP )+ beformulatedasaCMAB,wherethegoalistofindthebest
1 (θ(cid:126) −θ(cid:126) )TH (cid:104) logg(θ(cid:126) ) (cid:105) (θ(cid:126) −θ(cid:126) ), (24) site k to sample with the manipulator, using observations
2 k k,MAP k,MAP k k,MAP o = 1/0 (interesting/uninteresting science returns) from
k
where H is the Hessian. Thus, if we define A = −H and selected spectrometer target sites a and context (cid:126)x.
k
by taking the logarithm from (24),
B. Simulation Setup and Results
(cid:16) 1 (cid:17)
g(θ(cid:126) k )≈g(θ(cid:126) k,MAP )·exp − 2 (θ(cid:126) k −θ(cid:126) k,MAP )TA(θ(cid:126) k −θ(cid:126) k,MAP ) , As described in Sec. III-A, CMAB problems require
(25) alternately executing two steps: action selection (which site
and the normalization constant C laplace is computed as toilluminatewithlaserspectrometer)andobservationupdate
C laplace =g(θ(cid:126) k,MAP )· ( | 2 A π | ) 1 2 n 2 . (26) ( f in p o r ll o a o n c w e e s in s x g t s e p n so e s c i l v u tr e t o io m M n e o a t n e p t r p e r r o e C t a u a c r r h l n o e s s ) s . a tu I r n e dy c t : h o i ( n s i s ) i s d i b m e e r s u e t l d a p t a o io n s n d sib s c l t o e u m d a y p c , a ti r t o h e n d e
After following the same procedures as with the VBIS de- (site) selection, using an offline oracle (needed to compute
scribedinAlgorithm1(i.e.calculatingtheLaplaceposterior regret);(ii)(cid:15)-greedy(where(cid:15)=0.25wasfoundtoworkbest
Fig. 3. Cumulative regrets among different approaches in case of C=20
Fig.2. Comparisonofcumulativeregretsamongdifferentapproaches.For
andK=5.Priorpreferenceisnon-rewardseekingandfavorsmoreuncertain
eachsubplot,thecumulativeregretistheaverageof100simulations.Prior
preferenceofactiveinferenceagentsissetaspev(o)=[0.001,0.999]T.
outcomedistributions,i.e.pev(o)=[0.4,0.6]T.
the CMAB problem increases (counterclockwise from the
after initial trials); (iii) upper confidence bound (UCB); (iv)
bottom right to top right), the values of cumulative regrets
logistic Thompson sampling (LTS); and (v) active inference
tendtoincreaseregardlessofwhichactionselectionstrategy
(AI).Theactionselectionmethodsfor(iv)and(v)arepaired
isused,neverthelessthebothAIVBISandAILaplacerobots
with VBIS and Laplace approximations for the observation
greatly outperforms the others (Note: the slight performance
updates. In all simulations, 100 Monte Carlo (MC) runs
differences between the AI VBIS and AI Laplace robots
are performed, and the number of iterations T in each MC
are mainly from the accuracy of the posterior distribution
run is set to 102, which is small compared to common
approximation). This is owing to the fact that, as shown
MABalgorithmbenchmarks[8]andreflectsapracticalupper
in (7), AI can rigorously evaluate the uncertainty of each
limit for robotic lander sensor deployment. Thus, there is no
option and quickly find large ψ options, and prefers those
need to be too concerned about the suboptimal asymptotic
options under the influence of the prior preference p (o).
ev
behavior of active inference agents on very long time scales
On the other hand, when the prior preference is non-reward
(e.g. 105 iterations) as experimentally showed in [8]. The
seekingandfavorsmoreuncertainoutcomedistributions(i.e.
true hidden linear parameters θ(cid:126) k for each candidate search p (o) = [0.4,0.6]T), the AI robots perform as comparable
ev
sitekwererandomlygeneratedfromamultivariateGaussian
as other action selection strategies as presented in Fig. 3. In
distributions, where the mean is sampled from a uniform
such a case, in order to assess the performance of the AI
distributionof0to1,andthecovariancematrixisgenerated
robots, other evaluation metric such as the Kullback-Leibler
using the result of a singular value decomposition on the
divergence may be utilized (i.e. the extent to which the out-
Grammatrixofamatrixwithuniformlyrandomlydistributed
comedistributionobtainedbyexecutingtheoptionpreferred
entries.Thesearchcontextvector(cid:126)xwasrandomlygenerated
by the AI robots are aligned with the prior preference). In
assumingthateachelementtakesabinaryvaluewithuniform
terms of computation cost, although the cost of AI VBIS
probability, and the same value is used for all sites. To
approach is higher than other strategies due to importance
compare how each approach performs with respect to the
sampling, this can be significantly reduced by computing
difficulty of CMAB, the number of areas K and search
sampleweightsinparallel.WithregardtoAILaplace,while
context elements C, and the evolutionary prior p (o), were
ev the average single action selection time (0.010 sec in case
varied, such that K ∈ {5,10,20,40}, C ∈ {5,10,20},
of K = 10, C = 10) is again higher than that of (cid:15)-greedy
and p (o) ∈ {[0.001,0.999]T,[0.4,0.6]T}, i.e. in total,
ev (3.42E-06 sec in the same condition), still the computation
24×7 = 168 sets of 100 MC sims were run. This setup
cost may not be problematic since the number of total
producesdifficultCMABssincetherecanbemultiple“good” iterationsissmallenough(atmost102)andtheperformance
(large ψ) sites, which makes it hard to distinguish the best
of the AI Laplace robot outperforms the baseline methods.
site with highest chance of good science returns.
Even in the most difficult case (K = 40,C = 20), AI
Results: The cumulative regret (2) was used to compare Laplace takes only 0.26 sec on average, compared to 5.48E-
the performance of all approaches. Fig. 2 shows the char- 06 sec for (cid:15)-greedy.
acteristic results, including those obtained when the easiest
V. CONCLUSIONS
(bottomright;themostamountofinformationandthefewest
number of options) and most difficult (top right; lowest In this paper, we applied active inference as an action
amount of information and most options) conditions are selectionstrategyforcontextualmulti-armedbandit(CMAB)
used. As can be seen from this figure, as the difficulty of problems.Weshowedhowtoderivetheexpectedfreeenergy
(EFE) in categorical outcome CMAB problems and intro- [15] P. Auer, N. Cesa-Bianchi, and P. Fischer, “Finite-time analysis of
duced variational and Laplace approximation algorithms to the multiarmed bandit problem,” Mach. Learn., vol. 47, no. 2–3, p.
235–256, may 2002. [Online]. Available: https://doi.org/10.1023/A:
cope with analytically intractable EFE terms. The proposed
1013689704352
active inference strategy was demonstrated in a simulation [16] P. Auer, N. Cesa-Bianchi, Y. Freund, and R. Schapire, “Gambling
for an autonomous remote science site selection scenario, in a rigged casino: The adversarial multi-armed bandit problem,” in
ProceedingsofIEEE36thAnnualFoundationsofComputerScience,
and its cumulative regret was compared with the ones of
1995,pp.322–331.
other strategies. It was found that when the prior preference [17] W. R. Thompson, “On the likelihood that one unknown probability
is reward-seeking, active inference brings significant advan- exceedsanotherinviewoftheevidenceoftwosamples,”Biometrika,
vol.25,pp.285–294,1933.
tages, due to its ability to evaluate option uncertainty and
[18] K.Friston,“Thefree-energyprinciple:Aunifiedbraintheory?”Nature
bias towards preferred outcomes. ReviewsNeuroscience,vol.11,no.2,pp.127–138,2010.
The primary objective of this study was to validate the [19] K. Friston and S. Kiebel, “Predictive coding under the free-energy
principle,”PhilosophicaltransactionsoftheRoyalSocietyofLondon.
feasibility and performance of active inference strategy in
SeriesB,Biologicalsciences,vol.364,pp.1211–21,062009.
practical CMAB problems using proof of concept simula- [20] P. Lanillos, C. Meo, C. Pezzato, A. A. Meera, M. Baioumy,
tions that reflect key aspects of the motivating remote space W. Ohata, A. Tschantz, B. Millidge, M. Wisse, C. L. Buckley, and
J.Tani,“Activeinferenceinroboticsandartificialagents:Surveyand
science exploration problem. Future work will confirm the
challenges,” CoRR, vol. abs/2112.01871, 2021. [Online]. Available:
effectiveness of this strategy using OceanWATERS, [31], a https://arxiv.org/abs/2112.01871
testbed for the Europa lander recently released by NASA. [21] J.Y.Shin,C.Kim,andH.J.Hwang,“Priorpreferencelearningfrom
experts:Designingarewardwithactiveinference,”Neurocomputing,
In this work, the value of prior preferences was determined
vol.492,pp.508–515,2022.
andfixedapriori,althoughadjustingthisvalueaccordingto [22] N. R. Ahmed, E. M. Sample, and M. Campbell, “Bayesian mul-
search context, the regret analysis of active inference action ticategorical soft data fusion for human–robot collaboration,” IEEE
TransactionsonRobotics,vol.29,no.1,pp.189–206,2013.
selection strategies and results obtained from the actual
[23] L. Burks, I. Loefgren, and N. Ahmed, “Optimal continuous state
action execution are also future tasks. pomdpplanningwithsemanticobservations:Avariationalapproach,”
IEEETransactionsonRobotics,vol.35,pp.1488–1507,2019.
REFERENCES [24] S.WakayamaandN.Ahmed,“Probabilisticsemanticdataassociation
for collaborative human-robot sensing,” CoRR, vol. abs/2110.09621,
[1] C.B.PhillipsandR.T.Pappalardo,“EuropaClipperMissionConcept: 2021.[Online].Available:https://arxiv.org/abs/2110.09621
ExploringJupiter’sOceanMoon,”EOSTransactions,vol.95,no.20, [25] C.M.Bishop,PatternRecognitionandMachineLearning(Information
pp.165–167,May2014. ScienceandStatistics). Berlin,Heidelberg:Springer-Verlag,2006.
[2] J. McMahon, N. Ahmed, M. Lahijanian, P. Amorese, T. Deka, [26] G. Bouchard, “Efficient bounds for the softmax function and ap-
K. Muvvala, T. Slack, and S. Wakayama, “Expert-informed au- plications to approximate inference in hybrid models,” in Proc.
tonomous science planning for in-situ observations and discoveries,” Neural Inf. Process. Syst. Workshop Approx. Bayesian Inference
in2022IEEEAerospaceConference(AERO),2022,pp.1–11. Continuous/HybridSyst.,2007.
[3] L.Li,W.Chu,J.Langford,andR.E.Schapire,“Acontextual-bandit [27] A. Galantai, “The theory of newton’s method,” Journal of
approach to personalized news article recommendation.” in WWW. ComputationalandAppliedMathemathics,vol.124,pp.25–44,2000.
ACM,2010,pp.661–670. [28] J. Manne, T. Q. Bui, and C. R. Webster, “Determination of foreign
[4] S.McGuire,P.M.Furlong,T.Fong,C.Heckman,D.Szafir,S.J.Julier, broadeningcoefficientsformethanelinestargetedbythetunablelaser
and N. Ahmed, “Everybody needs somebody sometimes: Validation spectrometer(tls)onthemarscuriosityrover,”JournalofQuantitative
ofadaptiverecoveryinroboticspaceoperations,”IEEERoboticsand SpectroscopyandRadiativeTransfer,vol.191,pp.59–66,2017.
AutomationLetters,vol.4,no.2,pp.1216–1223,2019. [29] L.Matthies,M.Maimone,A.Johnson,Y.Cheng,R.Willson,C.Villal-
[5] L. Chan, D. Hadfield-Menell, S. Srinivasa, and A. Dragan, “The pando,S.Goldberg,A.Huertas,A.Stein,andA.Angelova,“Computer
assistivemulti-armedbandit,”inProceedingsofthe14thACM/IEEE vision on mars,” International Journal of Computer Vision, vol. 75,
International Conference on Human-Robot Interaction, ser. HRI ’19. no.1,pp.67–92,October2007.
IEEEPress,2019,p.354–363. [30] L. Palafox, C. Hamilton, S. Scheidt, and A. Alvarez, “Automated
[6] K. Friston, F. Rigoli, D. Ognibene, C. Mathys, T. FitzGerald, detectionofgeologicallandformsonmarsusingconvolutionalneural
and G. Pezzulo, “Active inference and epistemic value,” Cognitive networks,”Computers&Geosciences,vol.101,012017.
neuroscience,022015. [31] Ocean worlds autonomy testbed for exploration research &
[7] R. Kaplan and K. J. Friston, “Planning and navigation as active simulation (oceanwaters). [Online]. Available: https://github.com/
inference,” Biol. Cybern., vol. 112, no. 4, p. 323–343, aug 2018. nasa/owsimulator
[Online].Available:https://doi.org/10.1007/s00422-018-0753-2
[8] D. Markovic, H. Stojic, S. Schwobel, and S. Kiebel, J., “An empir-
ical evaluation of active inference in multi-armed bandits,” Neural
Networks;2021SpecialIssueonAIandBrainScience:AI-powered
BrainScience,vol.144,p.229–246,may2021.
[9] V. Kuleshov and D. Precup, “Algorithms for multi-armed bandit
problems,”JournalofMachineLearningResearch,vol.1,022014.
[10] R. BELLMAN, “A markovian decision process,” Journal of
Mathematics and Mechanics, vol. 6, no. 5, pp. 679–684, 1957.
[Online].Available:http://www.jstor.org/stable/24900506
[11] S.Thrun,W.Burgard,andD.Fox,ProbabilisticRobotics(Intelligent
RoboticsandAutonomousAgents). TheMITPress,2005.
[12] L.P.Kaelbling,M.L.Littman,andA.R.Cassandra,“Planningand
acting in partially observable stochastic domains,” Artif. Intell., vol.
101,no.1–2,p.99–134,may1998.
[13] H. Kurniawati, “Partially observable markov decision processes and
robotics,” Annual Review of Control, Robotics, and Autonomous
Systems,vol.5,no.1,pp.253–277,2022.
[14] D. Bouneffouf, I. Rish, and C. Aggarwal, “Survey on applications
of multi-armed and contextual bandits,” in 2020 IEEE Congress on
EvolutionaryComputation(CEC),2020,pp.1–8.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Active Inference for Autonomous Decision-Making with Contextual Multi-Armed Bandits"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
