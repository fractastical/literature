=== IMPORTANT: ISOLATE THIS PAPER ===
You are revising a summary for ONLY the paper below. Do NOT reference or use content from any other papers.
Paper Title: Path integrals, particular kinds, and strange things
Citation Key: friston2022path
REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Issues to fix:
1. CRITICAL: The current summary has severe repetition issues. You MUST eliminate all repeated sentences, phrases, and paragraphs. Each idea should be expressed only once. If you find yourself repeating content, remove the duplicates entirely. Focus on variety and uniqueness in your wording.
2. Severe repetition detected: Same phrase appears 5 times (severe repetition)

Current draft (first 2000 chars):
Okay, I will follow these instructions to produce a summary of the paper "Path integrals, particular kinds, and strange things" by Karl Friston et al.### OverviewThis paper introduces a novel framework for understanding cognition and action, termed â€œactive inference,â€ which posits that the brain continuously generates models of the world and updates these models based on sensory input. The core idea is that the brain doesnâ€™t simply react to stimuli but actively constructs a model of the world and uses this model to predict and interpret sensory input. This framework utilizes Bayesian inference, specifically variational inference, to efficiently update these models. The authors argue that this approach provides a more natural and computationally tractable explanation for a wide range of cognitive phenomena, including perception, action, and decision-making.### MethodologyThe authors employ a theoretical framework based on variational inference, a technique from Bayesian statistics that allows for efficient computation of posterior probability distributions in high-dimensional spaces. They use a Markov blanket to define the relevant variables in the Bayesian inference process. The authors use a specific type of variational inference, which they term â€œactive inference,â€ to model the brainâ€™s ability to generate models of the world and update these models based on sensory input. The key components of this framework are:***Model Generation:** The brain generates a model of the world, which is a probabilistic model that describes the relationships between sensory inputs and internal states.***Model Updating:** The model is updated based on sensory input, using Bayesian inference.***Markov Blanket:** The Markov blanket is a set of variables that are sufficient to determine the state of a variable. In the context of active inference, the Markov blanket includes the sensory inputs, the internal states, and the latent causes.The authors use a specific type of variational infer...

Key terms: path, brook, stony, university, things, states, kinds, department

=== FULL PAPER TEXT ===
Path integrals, particular kinds, and
strange things
Karl Friston1,3, Lancelot Da Costa1,2*, Dalton A.R. Sakthivadivel3,4,5, Conor Heins3,6,7, Grigorios A.
Pavliotis2, Maxwell Ramstead1,3, Thomas Parr1
1 Wellcome Centre for Human Neuroimaging, Institute of Neurology, University College London, London WC1N
3AR, UK
2 Department of Mathematics, Imperial College London, London SW7 2AZ, UK
3 VERSES Research Lab, Los Angeles, CA, USA
4 Department of Mathematics, Stony Brook University, Stony Brook, NY, USA
5 Department of Physics and Astronomy, Stony Brook University, Stony Brook, NY, USA
6 Department of Collective Behaviour, Max Planck Institute of Animal Behaviour, Konstanz D-78457, Germany
7 Centre for the Advanced Study of Collective Behaviour, University of Konstanz, Konstanz D-78457, Germany
Corresponding author*
Emails: k.friston@ucl.ac.uk, l.da-costa@imperial.ac.uk, dalton.sakthivadivel@stonybrook.edu,
cheins@ab.mpg.de, g.pavliotis@imperial.ac.uk, maxwell.ramstead@verses.io, thomas.parr.12@ucl.ac.uk
Keywords: self-organisation, variational inference, Bayesian, Markov blanket, active matter, path integral
Abstract
This paper describes a path integral formulation of the free energy principle. The ensuing
account expresses the paths or trajectories that a particle takes as it evolves over time. The main
results are a method or principle of least action that can be used to emulate the behaviour of
particles in open exchange with their external milieu. Particles are defined by a particular
partition, in which internal states are individuated from external states by active and sensory
blanket states. The variational principle at hand allows one to interpret internal dynamicsâ€”of
certain kinds of particlesâ€”as inferring external states that are hidden behind blanket states. We
consider different kinds of particles, and to what extent they can be imbued with an elementary
form of inference or sentience. Specifically, we consider the distinction between dissipative and
conservative particles, inert and active particles and, finally, ordinary and strange particles.
Strange particles can be described as inferring their own actions, endowing them with apparent
autonomy or agency. In shortâ€”of the kinds of particles afforded by a particular partitionâ€”
strange kinds may be apt for describing sentient behaviour.
1
Introduction
The free energy principle (FEP) describes a simple relationship between the dynamics of a
random dynamical system and a description of its behaviour as engaging in inference. The FEP
originated in neuroscience as an attempt to describe brain function and behaviour (Friston et al.
2006) and has since been extended to describe several kinds of things in the biological and
physical realms (Friston 2013; Friston et al. 2021) through a special kind of mechanicsâ€”a
Bayesian mechanicsâ€”that shares the same foundations with quantum, statistical, and classical
mechanics (Friston 2019; Friston et al. 2022). This paper is part of a series of technical papers
describing the FEP in progressively simpler and more qualified terms (Friston, 2013; Friston,
2019; Friston et al., 2022).
A path integral formulation
This paper focuses on a path integral formulation of the FEP. The path integral formulation
rests upon encoding the dynamics of random dynamical systems by a Lagrangian (Graham,
1977b; Seifert, 2012). The Lagrangian plays the role of self-information or surprisal, scoring
the implausibility of a path through state-space. In other words, we will be dealing with
probability densities over paths or trajectories, as opposed to densities over states, which have
concerned much of the recent literature on the FEP (Ramstead et al., 2022).
The move from a state-based to a path-based formulation reflects a shift in didactic accounts of
the FEP to its operational foundations, expressed in terms of probability densities over paths
(Friston, 2010; Friston, 2012; Ramstead et al., 2022). Much of the recent work on the FEP has
been concerned with formulating the dynamics of a system in terms of probability densities
over states; that is, expressing the probability that certain states or events will occur. Here, we
will be concerned with the probability that a system will take a certain trajectory through state
space.
The simplifications afforded by the path integral formulation rest on working with densities
over paths (as opposed to states). In particular, contrariwise to state-based formulations of the
FEP, we make no assumptions about the existence and functional form of a nonequilibrium
steady state (Friston et al., 2022; Friston et al., 2021).
Generalised coordinates of motion
Technically, we will work in generalised coordinates of motion (Balaji and Friston, 2011; Da
Costa et al., 2021a; Pavliotis, 2014): we augment the traditional state-space of a system with
additional degrees of freedom that represent the n-order time derivatives of the motion of states.
That is, the position, velocity, acceleration, etc., of a particle are represented explicitly as
distinct (generalised) states. This simplifies the derivations by replacing a time-series of states
(i.e., a path) with a series of time derivatives (i.e., a generalised state). In this setting, the
Lagrangian plays the role of an action, where paths of least action minimise the Lagrangian of
generalised states. Normally, the action and Lagrangian are treated differently, since the
Lagrangian is a function of generalised states, and the action is a function of paths, usually
defined as the path integral of the Lagrangian (Seifert, 2012). These notions coincide in
2
generalised coordinates of motion, as paths are equivalent to a point in generalised coordinates
of motion (i.e., a generalised state).
Particles and particular partitions
The FEP is concerned with self-organisation, which calls for an individuation of â€˜selfâ€™ from
nonself. We consider a particular partition of (generalised) states necessary to separate the
internal states of something (i.e., a particle) from its external states. This partition is defined in
terms of sparse coupling among external, sensory, active and internal states. The implications
of this particular partition are then unpacked in terms of a variational free energy lemma, which
says that the most likely autonomous (i.e., active and internal) paths minimise a free energy
functional of Bayesian beliefs about external paths. Crucially, particles with conservative
dynamics always pursue paths of least action and therefore minimise variational free energy.
The ensuing principle or method of least action is considered in the light of a Bayesian
mechanics for describing active particles that possess active states. The autonomous paths of
these kinds of particles have a Lagrangian, known as expected free energy, which can be
decomposed into terms corresponding to expected cost and expected information gain. In this
setting, cost is the Lagrangian or surprisal of sensory paths, which defines the characteristic
trajectories of a particle. This means that conservative particles comply with the dual optimality
principles of Bayesian decision theory (Berger, 2011; Wald, 1947) and Bayesian optimal design
(Lindley, 1956; Mackay, 1992). An alternative reading of sensory surprisal is in terms of
Bayesian model evidence (a.k.a., marginal likelihood) leading to a description of self-
organisation as self-evidencing (Hohwy, 2016).
Particular kinds
The ensuing formulation leads naturally to a typology of kinds of particles, or â€˜particular kindsâ€™.
We first consider the distinction between active and inert particles, which do and do not have
active states, respectively. We then consider a distinction between dissipative and conservative
particles, which are and are not subject to random fluctuations, respectively. Finally, we turn to
a distinction between ordinary and strange particles, whose active states do, and do not,
influence internal states, respectively. This distinction means that the Bayesian beliefs held by
strange particles cover (the consequences of) their action. In other words, the active states of
strange particles are hidden from internal states and become the latent causes of sensory states
that are inferred. This leads to a certain kind of (strange) particle that (looks as if it) believes
that it is a conservative particle. In consequence, it actively minimises expected free energy via
information and preference-seeking behaviour (Barto et al., 2013; Sun et al., 2011). This
sentient behaviour or active inference can be expressed as minimising a generalised free energy
functional, which could be regarded as a universal objective function in the design or modelling
of agents (Parr and Friston, 2019). This sort of enactive inference (Ramstead et al., 2020) can
be read as generalising planning as inference (Attias, 2003; Botvinick and Toussaint, 2012;
Lanillos et al., 2021).
3
The free energy principle
The FEP is a simple account of self-organisation that shares the same foundations with
quantum, statistical, and classical mechanics, and leads to a special kind of mechanicsâ€”a
Bayesian mechanicsâ€”that we can use to describe the dynamics of certain â€˜thingsâ€™ as engaging
in inference (Fields et al., 2021a; Friston, 2019; Ramstead et al., 2022). The FEP is derived by
paying careful attention to the way â€˜thingsâ€™ are defined and, thereby, the way in which they are
both individuated fromâ€”and coupled toâ€”everything else. Before laying down a path integral
formulation of the FEP, we summarise the steps in narrative form:
â€¢ The FEP addresses the following question: if something exists, in the sense of
possessing characteristic states or dynamics, what properties must it possess? To answer
this question, it is necessary to define a thing or particle.
â€¢ A particle is constituted by internal and blanket states. The blanket states constitute the
boundary between the states internal and external to the particle. Mathematically, this
means that internal paths are conditionally independent of external paths, given blanket
paths.
â€¢ This conditional independence means that, for every blanket path, there exists a most
likely internal path and a (posterior) probability density over external paths.
â€¢ The ensuing synchronisation map from the most likely internal path to the conditional
density over external paths can be read as inference, in the sense that the most likely
internal path encodes a Bayesian belief about external paths.
â€¢ This Bayesian interpretation can be made explicit by associating the Lagrangian with a
variational free energy; namely, a free energy functional of Bayesian beliefs about
external states, given blanket states.
â€¢ The internal paths of sufficiently large (i.e., conservative) particles are always the most
likely paths; namely, paths of least action, where action is the path integral of a
Lagrangian (a.k.a., self-information or surprisal) in the usual way.
â€¢ The Lagrangian of the active paths (i.e., blanket paths that can be influenced by internal
paths) of conservative particles reduces to variational free energy. This means the
autonomous (i.e., active and internal) states of such particles will appear to pursue paths
of least action that minimise variational free energy.
â€¢ In this case, the Lagrangian of autonomous paths can be decomposed into the expected
Lagrangian (i.e. implausibility or cost) of sensory paths minus expected information
gain; namely, the reduction of uncertainty about external paths.
â€¢ This has the interesting interpretation that autonomous paths will appear to minimise
expected costâ€”where cost is read as the Lagrangian of sensory paths that characterises
the particle in questionâ€”while maximising expected information gain. This is
4
consistent with the principles of Bayesian decision theoryâ€”e.g., expected utility theory
(Von Neumann and Morgenstern, 1944)â€”and optimal Bayesian design (Lindley,
1956), respectively.
â€¢ The combination of expected information gain and expected cost has the functional form
of an expected free energy.
â€¢ Finally, we turn to strange particles: conservative particles whose active paths only
influence internal paths vicariously, via sensory paths. Strange particles can be read as
inferring their own actionsâ€”in addition to the external worldâ€”endowing them with
apparent autonomy or agency.
In summary, starting with a definition of what it is to be a thingâ€”in terms of Markov blanketsâ€”
one ends up with a Bayesian mechanics of (certain kinds of) things. Internal paths look as if
they are inferring external pathsâ€”through minimising a variational free energy. Active paths
look as if they comply with the principles of optimal Bayesian design (Lindley, 1956) and
decision theory (Berger, 2011)â€”through maximising expected information gain and
minimising expected cost, respectively. Among the particular kinds delineated here, strange
things may be apt for describing the sentient behaviour of agents.
The resulting FEP can be read as a variational principle of least action, a gauge theory (Friston
et al., 2022; Sakthivadivel, 2022c; Sengupta et al., 2016), the dual to Jaynesâ€™ maximum entropy
or principle of maximum calibre (Sakthivadivel, 2022a; Sakthivadivel, 2022b) orâ€”in its
quantum-theoretic formulationâ€”asymptotically equivalent to the Principle of Unitarity (Fields
et al., 2021a).
On this view, the FEP is a first principles account or method that can be applied to any â€˜thingâ€™
or â€˜particleâ€™ in a way that dissolves the bright lines between physics, biology and psychology
(Chris Fields, personal communication). Such applications endorse many normative accounts
of sentient behaviour and self-organisation. These range from cybernetics to synergetics (Ao,
2004; Ashby, 1979; Haken, 1983; Kelso, 2021); from reinforcement learning to artificial
curiosity (Barto et al., 2013; Schmidhuber, 1991; Sutton and Barto, 1981; Tsividis et al., 2021);
from predictive processing to universal computation (Clark, 2013b; Hohwy, 2016; Hutter,
2006); from model predictive control to empowerment (Hafner et al., 2020; Klyubin et al.,
2005), and so on. We now unpack the narrative arguments above, using standard results from
statistical physics and information theory.
The path integral formulation
â€œWe do not find obvious evidence of life or mind in so-called inert matter; but if the scientific
point of view is correct, we shall ultimately find them, at least in rudimentary form, all through
the universe.â€ (Haldane, 1932)
5
We begin by describing the setup. We assume that the states of the systemâ€”which we will later
decompose into a particle and its external milieuâ€”evolve as a random dynamical system in a
d-dimensional Euclidean space that can be described with a Langevin equation,
xï€¦(t)= f(x)+w(t): 0Â£tÂ£T. (1)
This equation describes the rate of change of states x(t), in terms of their flow f(x) (i.e., a
vector field), and some random fluctuations w(t) with smooth (analytic) sample paths. This
setup speaks nicely to the fact that, in biology, fluctuations are often smooth up to a certain
order (Vasseur and Yodzis 2004) as they are generated by other random dynamical systems
(Friston 2019). We assume that the fluctuations are state-independent, and a stationary Gaussian
process1, which can be read as a consequence of the central limit theorem; i.e., fluctuations
should be normally distributed at each point in time. We make no assumptions about the
solutions of this equation, other than the flow operator does not change over some relevant time
interval. This furnishes an adiabatic approximation (Born and Fock, 1928) to systems in which
the parameters of the flow change slowly. Accordingly, we will be concerned with the dynamics
over a suitably small period of time and consider the implicit separation of timescales elsewhere
(e.g., the separation of fast inference about states and the slow learning of parameters).
ï²
Equation (1) can be expressed in generalised coordinates of motion, x =(x,xÂ¢,xÂ¢Â¢,ï‹), where, to
first-order:2
xï€¦ = xÂ¢= f(x)+w Ã¼
Ã¯ ï² ï² ï²
xï€¦Â¢= xÂ¢Ã‘Â¢=+Ã— f xÂ¢ wÂ¢ Ã¯ xï€¦ =f(x)+w
Ã½ Ã› ï² ï²
xï€¦Â¢Â¢= xÂ¢Ã‘Â¢Â¢=+Ã— f xÂ¢Â¢ wÂ¢Â¢ p(w)= N(w;0,2Î“)
Ã¯
Ã¯
ï Ã¾
Ã©0 1 Ã¹ Ã©Ã‘f Ã¹ Ã© G -GÂ¢Â¢ Ã¹
0 0
Ãª Ãº Ãª Ãº Ãª Ãº
0 1 ï² Ã‘f GÂ¢Â¢ -G(4)
= = D=Ã‘Ãª Ãº, =f(x) J Ãª Ãº, Î“ Ãª 0 0 Ãº
Ãª 0 ïÃº Ãª Ã‘f Ãº Ãª-GÂ¢Â¢ G(4) Ãº
0 0
Ãª Ãº Ãª Ãº Ãª Ãº
Ã« ïÃ» Ã« ïÃ» Ã« -G(4) ï Ã»
0
(2)
âˆ‡ğ‘“ is the Jacobian matrix of the flow, D is a (derivative) operator that sends each generalised
state to the next, f is the flow of the system in generalised coordinates of motion, and J is the
Jacobian matrix of the flow in generalised coordinates. In generalised coordinates of motion,
the state, velocity, acceleration, etc are treated as separate (generalised) states, whose flow is
1 In statistical physics, this type of noise is generally referred to as â€˜coloured noiseâ€™, the prototypical example of
which is the smoothing of white noise fluctuations with a Gaussian kernel.
2 Going from (1) to (2) corresponds to solving the stochastic realisation problem (Mitter et al. 1981; Da Costa et
al. 2021). In this instance, (2) is obtained by recursively differentiating (1) and ignoring the contribution of the
derivatives of the flow of order higher than one; see (Balaji and Friston 2011) for details. In other words, the
expansion is exact when the flow is linear, and it is accurate on a short timescale when the flow is non-linear.
6
supplemented with state-independent smooth fluctuations (Da Costa et al., 2021b; Friston,
2008; Kerr and Graham, 2000; Pavliotis, 2014). In summary, we go beyond white noise
assumptions about fluctuations3 and express analytic or smooth fluctuations w(t) with
autocovariance G=G =(t) E [w(t)w(0)]/2.4
t
Generalised coordinates of motion can be read as the coefficients of a Taylor series of the
trajectory or path, from any point in time, t:
xÂ¢Â¢(t)
x(t)= x(t)+x+Â¢-(t)(-+t t) (t t)2 ï‹ (3)
2!
This means that generalised states correspond to paths. The path integral formulation (Graham,
1977b; Seifert, 2012) considers the probability of a path in terms of its action (i.e., negative log
probability), which we can associate with the Lagrangian (i.e., negative log probability) of
generalised states.5 From the expansion in generalised coordinates in (2), we have that the
surprisal of the generalised states is as follows: 6
ï² ï² ï² ï²
L(x)ï€-ln p(x)= 1[ln|Î“|+wÃ— 1 w]
2 2Î“ (4)
ï² ï² ï²
w= xï€¦-f(x)
This Lagrangian scores the degree to which fluctuations cause the path to deviate from the path
of least action. The path of least actionâ€”i.e., the most likely pathâ€”is simply the flow in the
absence of deviations, when the random fluctuations take their most likely value of zero. Paths
of least action can be interpreted as the preferred trajectories that characterise a system. We will
denote these pathsâ€”and their generalised statesâ€”by boldface, where, from (4):
ï² ï² ï² ï² ï² ï² ï²
xï€¦ =Dx=fÃ›(Ã›x==) Ï‰ 0 Ã›x argmin L(x) Ã‘ L(x)=0 (5)
ï² ï²
x x
3 White noise fluctuations have trivial temporal autocovariance structure, i.e., ğ”¼[ğœ”(ğœ+ğœ–)ğœ”(ğœ)]=0 if ğœ– â‰ 0.
4 The covariance of the fluctuations on the n-th order motion then becomes the n-th order temporal derivative of
the autocovariance function describing their temporal correlation; please see Appendix A.5.3 in (Parr et al. 2022)
for details.
5 In the case of white noise fluctuations, the action is the integral of the Lagrangian over paths, where the usual
(Stratonovich) form reads (Seifert 2012):
L(x)= 1[ln|G+Ã—| Ã‘w+Ã— 1 w=- f]: w xÂ¢ f(x)
2 0 2G
T
A(xï€¥)=- ln p(xï€¥)= Ã² dtL(x): xï€¥= [x(t)]:0Â£ Â£t T
0
ï²
In generalised coordinates of motion, action can be defined as the Lagrangian A(xï€¥)=L(x), via the one-to-one
correspondence between paths and generalised states through (3).
6 Where does the term 1ln|Î“|come from? This term is simply the constant of proportionality that makes the
2
Lagrangian a log probability. This term is suppressed in (Seifert 2012), where it is assumed to be constant. The
Lagrangian is quadratic in the fluctuations, which follows from the fact that the probability density over
fluctuations is Gaussianâ€”and the normalising constant of a Gaussian density involves the (log) determinant of
its covariance.
7
That is, the flow operator D produces the most likely flow when the amplitude of random
ï²
fluctuations goes to zero, which is precisely the path x that minimises the Lagrangian.7
Generalised states afford a convenient way of expressing the path of least action as the solution
to the following equations (Friston et al., 2010b), which reduce to (5) when evaluated at the
path of least action:
ï² ï² ï² ï² ï² ï² ï² ï² ï²
Ã›-Ã‘Ã‘-=+=L-(x)Ã›-=(Ã‘xï€¦ Dx) 0 xï€¦ Dx L(x) xï€¦(t) Dx L(x). (6)
ï² ï² ï²
x x x
The first equality resembles a Lagrange equation of the first kind that ensures the generalised
motion of states is the state of generalised motion. Alternatively, it can be read as a gradient
descent on the Lagrangian, in a moving frame of reference. When the Lagrangian is convex,
any solution to this generalised gradient descent on the Lagrangian converges to the path of
least action. Equation (6) will be useful later, when recovering paths of least action, in the spirit
of generalised Bayesian filters (Friston et al., 2010b).
The uncertainty over paths derives from random fluctuations, which can be expressed in terms
of the expected Lagrangian (i.e., differential entropy): from (2) and (4):8
ï² ï² ï²
E[L(x)]= 1ln|Î“|Ã—+1 E[w 1 w]
2 2 2Î“ (7)
= 1ln|eÎ“|ï€S(Î“)ï€S
2
When the amplitude of random fluctuations tends to zero, there is no uncertainty, and the path
is always the path of least action. This corresponds to conservativeâ€”but potentially chaoticâ€”
dynamics. The differential entropy above be expressed in terms of continuous entropy, using
the limiting density of N discrete points [eq. 12.7 in (Jaynes 2003)] in relation to a (constant)
ï² ï²
invariant measure m(x)Âºm:"x:
ï² ï² ï²
H[p(x)]=-lnN D [p(x)||m(x)]
KL
ï²
=E[L(x)]+ln-Nm=S S
0
(8)
ï²
lim E[L(x)]=- lnNm= S(0)ï€S
Î“Â®0 0
ï²
Ã›=H[p(x)] 0
ï² ï² ï² ï²
Ã›=-p(x=)Ã›d(x x) w 0
These expressions say that paths of least action (and their associated generalised states) are
confined to sets of vanishing Lebesgue measure, where the expected Lagrangian approaches a
ï²
lower bound: S Â£ E [L(x)]. This inequality follows since the limiting density of discrete points
0
is always non-negative, as it is a limit of a discrete entropyâ€”the fact that it vanishes for paths
7 The gradient of the Lagrangian vanishes at the global minimum given by the path of least action.
ï² ï²
8 Noting that the expected value of the square of fluctuations (i.e., the covariance) is 2ğšª, the term 1 E [wÃ—w]
2Î“
1
(where we have pulled the constant out) evaluates to one, or ln|e|. We can then combine logarithms in the
2Î“
usual way to obtain (7).
8
of least action means that the corresponding generalised states occupy a set of measure zero
under the invariant measure [Section 12.3 in (Jaynes 2003)]. We will apply these results later
to subsets of sparsely coupled states. For example, given a subset pÃŒ x with parents
pa[p]ÃŒ x, we have:
ï² ï² ï² ï²
E ï² ï²
[L(p| pa[p])]= 1ln|Î“ |Ã—+1
E
[w 1 w]
p(p|pa[p]) 2 p 2 p 2Î“ p
p
=Â³1ln|eÎ“ |ï€S S Ã
2 p p 0
ï² ï²
E [L(p| pa[p])]=Â³ E ï² [S ] S
p(pa[a]) p 0 (9)
ï² ï² ï²
w =-Dp f (p, pa[p])
p p
ï² ï² ï² ï²
L(p| pa[p])ï€-ln p(p| pa[p])
ï² ï² ï² ï²
lim
E
[L(p| pa[p])]=Ã›S
H
[p(p| p=a[p])] 0
Î“ Â®0 0
p
Here, the second line is obtained as in (7). The last line expresses the fact that, in the absence
of random fluctuations on a subset of states pÃŒ x, their motion (and associated generalised
state) is completely determined by the parents pa[p]ÃŒ x, at which point the conditional
distribution is a Dirac delta with a continuous entropy of zero, and a differential entropy that
approaches its lower bound. In other words, in absence of fluctuations on their motion, a subset
of states follow paths of least action. This concludes our setup. We now turn to a particular
partition of states that defines â€˜thingsâ€™ and consider how different kinds of things behave.
Figure 1 â€“ Markov blankets. This influence diagram illustrates a particular partition of states into internal states
(blue) and external states (teal) that are separated by a Markov blanket comprising sensory (green) and active states
(red). The diagram shows this partition as it would be applied to a single-cell organism, where internal states are
associated with intracellular states, the sensory states become the surface states or cell membrane overlying active
states: e.g., the actin filaments of the cytoskeleton. Particular states constitute a particle; namely, autonomous and
sensory statesâ€”or blanket and internal states. The particular aspect of this coupling is that external states can only
influence themselves and sensory states, while internal states can only influence themselves and active states.
A particular partition and variational free energy
â€œHow can the events in space and time which take place within the spatial boundary of a living
organism be accounted for by physics and chemistry?â€ (SchrÃ¶dinger, 1944).
9
The free energy principle applies to a particular partition of states that distinguishes between
(s,a,Âµ)ÃŒ x hÃŒ x
the states of a particle or â€˜thingâ€™ and its external states , based on their sparse
coupling (see Figure 1):9
Ã©hï€¦(t) Ã¹ Ã© f (h,s,a) Ã¹ Ã©w(t) Ã¹
h h
Ãª Ãº Ãª Ãº Ãª Ãº
sï€¦(t) f (h,s,a) w(t)
Ãª Ãº = Ãª s Ãº + Ãª s Ãº , (10)
Ãª aï€¦(t) Ãº Ãª f (s,a,Âµw) ÃºtÃª ( ) Ãº
a a
Ãª Ãº Ãª Ãº Ãª Ãº
Ã«Âµï€¦t( ) Ã» ÃªÃ« f
Âµ
(s,a,Âµw) ÃºÃ» tÃªÃ«
Âµ
( ) ÃºÃ»
where fluctuations w,iÃ(hÂµ,s,a, ) are assumed to be mutually independent. The particular
i
states are partitioned into sensory, active and internal states with particular flow dependencies;
namely, external states can only influence themselves and sensory states, while internal states
can only influence themselves and active states. From (4), these coupling constraints mean that
external and internal paths are independent, when conditioned on blanket paths.
Â¶2f Â¶2L ï² ï² ï² ï² ï² ï² ï² ï² ï² ï² ï² ï² ï² ï²
=Ã0 ï²=Ã›
ï²
0 L(hÂµ,= |s,a) hL+( |s,a) ÂµÃ›L( |s^,a)h Âµ( )|s,a. (11)
Â¶hÂµÂ¶ hÂ¶ÂµÂ¶
This is because there are no flows that depend on both internal and external states (and
fluctuations are independent).
The conditional independence above distinguishes the dynamics of a particular â€˜thingâ€™ from
every â€˜thingâ€™ else. In other words, the statistical separation of internal dynamics from external
dynamics rests on the existence of a â€˜boundaryâ€™ supplied by a Markov blanket (Pearl, 2009).
Why start here? Because the only thing we have at hand is a probabilistic description of the
system (in terms of trajectories which correspond to generalised states), and the only way to
separate the states of something from its boundary states is in terms of probabilistic
independenciesâ€”in this instance, conditional independencies.10 This separation means the
internal paths of least action do not depend upon external paths, given blanket paths. From (6)
:11
ï² ï² ï² ï² ï² ï² ï² ï² ï² ï² ï²
Î¼ï€¦(t)=-DÃ‘Î¼ L(Î¼=,s-,Ã‘a,h) DÎ¼ L(Î¼|s,a,h)
ï² ï²
Âµ Âµ (12)
ï² ï² ï² ï² ï² ï² ï² ï²
=-DÃ‘Î¼ L(=Î¼-|Ã‘s,a) DÎ¼ L(s,a,Î¼)
ï² ï²
Âµ Âµ
9 A â€˜particular partitionâ€™ here is not meant in the sense of a partition function in statistical physics, rather as an
individuation between the states of a particle and the states external to it. However, it is interesting to note that
the free energy functionalâ€”we will arrive at by partitioning external from particular statesâ€”is itself effectively a
(log) partition function in the statistical physics sense.
10 Noting that if two subsets of trajectories were independent, as opposed to being conditionally independent, we
would be describing two separate systems.
ï² ï²
11 Equation (12) is tautologically equivalent to
Î¼ï€¦(t)=DÎ¼.
This implicit augmentation is important as it will
enable to relate these dynamics to generalised Bayesian filters.
10
Because internal paths depend only on blanket paths there is a deterministic mapping from
every blanket path to the corresponding internal path of least action. If this map is injective,
this implies a map between the internal path of least action and the conditional density over
external paths, given blanket paths.12 This injection can be expressed as a variational density:
ï² ï² ï² ï²
q
ï²
(h)ï€ p(h|s,a)
Î¼ (13)
ï² ï² ï² ï² ï² ï² ï²
Î¼ï€argmin
ï²
L(Âµ|s,a)Ã›=Ã‘
ï²
L(Î¼|s,a) 0
Âµ Âµ
Equation (13) can be interpreted as saying that the internal path of least action, for any given
blanket path, encodes beliefs about external paths. This licences the following definition:
Definition 1: a particle is defined by a particular partition with a nonempty set of internal
states, whose paths of least action parameterise a conditional density over external paths. In
other words, a particular partition (10) defines a particle when (13) holds.
Although based upon the same assumptions, the free energy principle can be seen as
complementing quantum, statistical and classical mechanics with a Bayesian mechanics, by
paying careful attention to the separation of external and internal dynamics (Friston, 2019).
This separation is via the conditional independence in (12). By construction, this endows
particles with an autonomy in the sense that active and internal (i.e., autonomous) paths,
ï² ï² ï² ï²
aÂµ=(a, )ÃŒ x do not depend on external paths, given sensory paths. The free energy principle
concerns the nature of this autonomous dynamics.
Lemma (variational free energy): Consider a variational density over external paths
ï²
parameterised by internal paths q ï² (h), where the path of least action encodes the posterior
Âµ
ï² ï² ï² ï²
density over external paths: q ï² (h)= p(h|s,a). Then internal paths of least action of particles
Î¼
minimise a free energy functional of Bayesian beliefs13 about external paths:
12 In the case of linear flows in (1), the flow of generalised states in (2) is also linear, which means that the
Lagrangian is quadratic, e.g., see Section 6 in (Friston et al. 2022). In this case, the condition under which a
variational density can be defined that satisfies (13) is the object of Lemma 2.1 in (Da Costa et al. 2021). In the
case of nonlinear flowsâ€”since the space of continuous paths is an infinite-dimensional Banach spaceâ€”we know
that many maps between two such spaces are injective. This motivates why, for each system that could be
modelled, out of all the maps one could obtain, many of them will afford (13).
13 Beliefs here are meant in a technical, Bayesian sense; that is, as probability distributions over external
variables. In this sense the free energy is a functional (i.e., function of a function) of beliefs, or equivalently, a
function of their parameters.
11
ï² ï² ï² ï² ï² ï² ï²
Ã› =Î¼=Ã›argmin
ï²
L(Âµ|s,a) Ã‘ ï²L(s,a,Î¼) 0
Âµ Âµ
ï² ï² ï² ï² ï² ï² ï²
Ã =Î¼=Ã›argÃ‘min F(s,a,Âµ) F(s,a,Î¼) 0
ï² ï²
Âµ Âµ
ï² ï² ï² ï² ï²
Î¼ï€¦ =-DÃ‘Î¼ F(s,a,Î¼)
ï²
Âµ
ï² ï² ï² ï² ï² ï²
F(s,a)=E
ï²
[L(h,s,a)+lnq(h)]
ï€±qï€´(h)ï€´ï€´ï€´ï€²ï€´ï€´ï€´ï€´ï€³
(14)
Variational free energy
ï² ï² ï² ï² ï²
=E [L(s,a|h)+L(h)]-E [-lnq(h)]
ï€±qï€´ï€´ï€´ï€²ï€´ï€´ï€´ï€³ ï€±qï€´ï€´ï€²ï€´ï€´ï€³
Energy constraint Entropy
ï² ï² ï² ï² ï²
= D [q(h)| p(h)]+E [L(s,a|h)]
ï€±Kï€´L ï€´ï€²ï€´ï€´ï€³ ï€±qï€´ï€´ï€²ï€´ï€´ï€³
Complexity (â€“) Accuracy
ï² ï² ï² ï² ï² ï² ï² ï²
= D [q(h)|| pÂ³(h|s,a)]+ L(s,a) L(s,a)
ï€±Kï€´L ï€´ï€´ï€²ï€´ï€´ï€´ï€³ ï€±ï€´ï€²ï€´ï€³
Divergence (â€“) Log evidence
This variational free energy can be rearranged in several ways. First, it can be expressed as an
energy constraint minus an entropy, which licences the name free energy (Feynman, 1972). In
this decomposition, minimising variational free energy corresponds to the maximum entropy
principle, under the constraint that the energy is minimised (Jaynes, 1957; Lasota and Mackey,
1994; Sakthivadivel, 2022b). The energy constraint is a functional of the marginal density over
external and particular paths, which plays the role of a generative model; namely, a joint density
over causes (i.e., external paths) and their consequences (i.e., particular paths). In (14) the
generative model has been expressed in terms of a likelihood and prior Lagrangian that can be
read as specifying characteristic or preferred trajectories in the space of causes (i.e., external
states) or consequences (i.e., particular states).
Secondâ€”on a statistical readingâ€”variational free energy can be decomposed into the
(negative) log likelihood of particular paths (i.e., accuracy) and the Kullback Leibler (KL)
divergence between posterior and prior densities over external paths (i.e., complexity). Finally,
it can be written as the negative log evidence plus the KL divergence between the variational
and conditional (i.e., posterior) density. In variational Bayesian inference14 (Beal, 2003),
negative free energy is called an evidence lower bound or ELBO (Bishop, 2006; Winn and
Bishop, 2005). It is called a bound because the KL divergence is never less than zero.
Proof: recall that the Lagrangian is a surprisal, and the KL divergence between two densities is
the expected difference in their surprisals. The various decompositions of the variational free
energy follows from this observation. The proof is straightforward by construction: substituting
the definition of the variational density in (13) into the final equality in (14) shows that the
Lagrangian and variational free energy of particular paths share the same minima:
14 When the set of active states is empty, and the particle is inert.
12
ï² ï² ï² ï² ï² ï² ï² ï² ï² ï² ï² ï²
F(s,a,Î¼)= D [q (h)|| p(h|s,a)]+L(s,a,Î¼)=L(s,a,Î¼)
ï²
ï€±Kï€´L ï€´Î¼ ï€´ï€´ï€²ï€´ï€´ï€´ï€´ï€³
=0
Ã
(15)
ï² ï² ï² ï² ï²
Î¼ï€¦(t)=-DÃ‘Î¼ L(s,a,Î¼)
ï²
Âµ
ï² ï² ï² ï²
=-DÃ‘Î¼ F(s,a,Î¼)
ï²
Âµ
Replacing the Lagrangian in (12) with variational free energy gives (14) . (cid:0)
Corollary: if autonomous and external paths are conditionally independent, given sensory
paths, then the autonomous path of least action can also be cast as a gradient flow on variational
free energy
ï² ï² ï² ï² ï² ï² ï²
(h^a)|s-=ÃÃ‘ Î±ï€¦ DÎ± F(s,Î±) (16)
ï²
a
ï² ï² ï²
Where Î±ï€argmin
ï²
L(aâˆ£s). In this case, the most likely paths of both internal and active
a
states perform a gradient descent on variational free energy.
Proof: the conditional independence above means that the autonomous path of least action is a
gradient flow on the Lagrangian of particular states:
ï² ï² ï² ï² ï² ï² ï² ï²
(h^a)|s Ã›L(a=|s,h) L(a|s)
Ã
ï² ï² ï² ï² ï² ï² ï² ï² ï² ï² ï² (17)
Î±ï€¦ =-DÃ‘Î± L(=Î±-|Ã‘s,h) =D-Î±Ã‘ L(Î±|s) DÎ± L(s,Î±)
ï² ï² ï²
a a a
ï² ï² ï²
=-DÃ‘Î± F(s,Î±)
ï²
a
Where the motion of autonomous paths of least action follows from
Error! Reference source not found. and the Lagrangian of particular states is the variational
free energy of internal paths of least action by (15) (cid:0)
Remark: The functional form of variational free energy licences a representational
interpretation of internal dynamics: the internal paths of least action play the role of sufficient
statistics or parameters of Bayesian beliefs about external dynamics. On this view, a gradient
flow on variational free energy corresponds to minimising the complexity of Bayesian beliefs
about external states, while providing accurate predictions of the dynamics of a particleâ€™s
sensory (and autonomous) states. Minimising complexity means that internal paths of least
action encode Bayesian beliefs about external paths that are as close as possible to prior beliefs.
The paths of least action can either be interpreted as the behaviour of a particle with classical
mechanics (see below), in which random fluctuations are negligible. Conversely, they can be
considered as the average responses over multiple realisations of the same dynamics, of the sort
solicited by event related averaging in various fields; e.g., (Licata and Chiatti, 2019; Singh et
13
al., 2002). In this case, we can think of the paths of least action as expressing the average (e.g.,
ensemble) behaviour of a system.
The variational free energy lemma says that the internal paths of least action minimise a
variational free energy functional. So, is minimisation of variational free energy an explanation
forâ€”or a description ofâ€”internal dynamics? The question is itself revealing. It is a question
about internal dynamics. However, the internal dynamics are unknowable (unmeasurable) by
construction. Only blanket states are observable (measurable). This means one can only say that
it looks as if internal dynamics are inferring external paths, under certain conditions: the
existence of a variational density.
A typology of particular kinds
We now begin to unpack a typology of particular kinds. First, we distinguish between active
and inert particles, which do and do not have active states. We will then distinguish conservative
and dissipative particles, for which random fluctuations can be, and cannot be, ignored. In the
final sections, we distinguish between ordinary and strange particles, whose active states do,
and do not, directly influence internal states. See Figure 2.
Figure 2 â€“ particular kinds. This schematic illustrates the definitive features of the particular kinds considered in
the main text. The upper panels show inert and active particles, without and with active states, respectively. When
the particular states of an active particle follow paths of least action, we have conservative particles. When the
active states of conservative particles are hidden from (i.e., do not directly influence) internal states, we have
strange particles. Strange particles will appear to infer their own actions in virtue of behaving as if their internal
dynamics encode a variational density (i.e., Bayesian belief) over the hidden causes of sensations; namely external
and active paths.
14
The inferential process of internal states can only manifest to the external world when internal
dynamics influence blanket flows, which, by construction, require active states (noting that
internal states cannot influence sensory states). This speaks to two kinds of particles with and
without active states: namely, active and inert particles, respectively.
Definition 2: an active particle is a particle with a nonempty set of active states. Conversely,
an inert particle is a particle with no active states.
This definition precludes any form of panpsychism when applying the free energy principle, in
the sense that any ontic or noetic claims can only concern active particles, or active matter
(Ramaswamy, 2010). Put simply, we can never observe internal dynamics directly because
these are hidden behind the Markov blanket. Whether internal paths of least action parameterise
beliefs about external paths and therefore minimise variational free energy can only manifest
via active states; that is, in active particles. Since these are non-existent in inert particles, there
is no way to observe whether a given inert particle minimises variational free energy (e.g., the
interior of a black hole). Before turning to the behaviour of active particles, we consider inert
particles through the lens of inference.
Inert particles and Bayesian filtering
Although internal dynamics are hidden behind their Markov blanket, we can simulate or
emulate (the most likely behaviour of) inert particles by integrating the following equations of
generalised motion, inserting (14) into (10):
ï² ï² ï² ï²
Ã©hï€¦Ã¹ Ã© f (h,s)+w Ã¹
h h
Ãªï²Ãº Ãª ï² ï² ï² Ãº
sï€¦ = f (h,s)+w (18)
Ãª Ãº Ãª s s Ãº
ï² ï² ï² ï² ï²
ÃªÎ¼ï€¦Ãº ÃªDÎ¼-Ã‘ F(s,a,Î¼)Ãº
Ã« Ã» Ã« Âµ ï² Ã»
Here, internal dynamics are expressed as a generalised gradient flow on variational free energy,
while external and sensory paths are specified with equations of motion and the statistics of
random fluctuations. In statistics, these constitute a state-space model. The functional form of
variational free energy can be unpacked under the Laplace approximation (Friston et al., 2007;
ï² ï² ï²
Ramstead et al., 2022). For example, if s(Î¼)=argmaxq (h)= Î·â€”the map from the internal
ï²
Î¼
to the external path of least actionâ€”is well-defined, we have15
15 Omitting constant terms for clarity.
15
ï² ï² ï² ï² ï²
F(s,Âµh)=E [L( h,s)+lnq( )]
ï€±qï€´ï€´ï€´ï€²ï€´ï€´ï€´ï€³
Variational free energy
ï² ï² ï² ï² ï²
Â» L(s |sÂµ( )) +sLÂµ( ( )) -1ln|S(Âµ)|+ 1tr(S(Âµ)Ã‘2L)
ï€±ï€´ï€²ï€´ï€³ ï€±ï€´ï€²ï€´ï€³ ï€±2 ï€´ï€²ï€´ï€³ ï€±2 ï€´ï€´ï€²ï€´ï€´ï€³
Likelihood constraint Prior constraint Entropy constant (dim Âµ ï² )
ï² ï² ï² ï² ï²
q(h)= N(h:s(Âµ),S(ÂµÃ‘)), =S(Âµ)-1 2L (19)
ï² ï² ï² ï² ï² ï²
L(h,s)=Ã—w +Ã—1 w w 1 w +S +S
h 4Î“ h s 4Î“ s h s
h s
ï² ï² ï² ï²
w =-Dh f (h,s)
h h
ï² ï² ï² ï²
w =-Ds f (h,s)
s s
This functional form renders the internal dynamics in (18) equivalent to a generalised Bayesian
filter (Friston et al., 2010b); namely, the generalisation of an extended Kalman-Bucy filter
(Loeliger, 2002; Schiff and Sauer, 2008) to high order motion. In this setting, internal states
parameterise a Gaussian posterior over external statesâ€”a.k.a., variational Laplace (Friston et
al., 2007).
Conservative particles and expected free energy
â€œIt is implied that, in some sense, a rudimentary consciousness is present even at the level of
particle physicsâ€ (Bohm, 2008).
We now turn to the distinction between microscopic and macroscopic particles, where
macroscopic particles possess classical mechanics, in which random fluctuations are (almost)
averaged away16 and the trajectories of particular states are (almost) paths of least action. This
introduces the next key distinction; namely, between dissipative and conservative particles
whose particular dynamics are, and are not, subject to random fluctuations, respectively.
Definition 3: a conservative (classical) particle is an active particle whose particular states
follow paths of least action. Equivalently, it is an active particle whose random fluctuations on
particular states have amplitudes that are infinitesimally small. In other words, we consider the
limiting regime where the covariance of the random fluctuations w(t),w(t),w(t) in (10)
s a Âµ
tends to zero Î“ Â®0.
p
Conservative (or classical) particles are especially interesting because they respond
deterministically and systematically to external influences, in virtue of the fact random
fluctuations on their dynamics (almost) disappear. It could be argued that all macroscopic
particles are conservative, simply because their random fluctuations are averaged away
(Friston, 2019). This means that their dynamics rests on chaotic itinerancy that is underwritten
16 Physically, this means that we are considering a description of particles at a sufficiently high-level of coarse
graining so that microscopic fluctuations are almost averaged away, and the laws of classical mechanics prevail.
See (Friston et al. 2020) for examples using the coarse graining apparatus of the renormalisation group.
16
by nonlinear coupling, breaking of detailed balance, and associated nonequilibria (Friston et
al., 2021; Lasota and Mackey, 1994; Da Costa and Pavliotis, 2022). Removing uncertainty
about particular paths has an important consequence. Because there is no uncertainty about
autonomous paths, given sensory paths, there is no further information about external paths
afforded by autonomous paths, rendering them conditionally independent. This can be
expressed in terms of expected Lagrangians using a bound argument. From (9):
ï² ï² ï² ï² ï²
E
[L(a|s)]Â³E [L(a|s,h)]Â³S
0
ï² ï² ï² ï² ï²
lim
E
[L(a|s)]=E [L(a|s,h)]=S
G Â®0 0
p
ï² ï² ï² ï² ï²
Ã=E
ï² ï²
[D [p(a|s,h)|| p(a|s)]] 0 (20)
p(s,h) KL
ï² ï² ï² ï² ï² ï² ï² ï²
ÃÃ›=L(a|^s,h) L(a|s) (a h)|s
ï² ï² ï² ï² ï² ï² ï² ï²
ÃÃ›=L(h^|s,a) L(h|s) (h a)|s
In other words, although active states influence external states, the active states are determined
completely by sensory paths, as they provide a Markov blanket separating external and
autonomous states. Please see Figure 3 for a diagrammatic illustration.
Crucially, this conditional independence means that conservative particles satisfy (16), which
yields an expression for the most likely behaviour of conservative particles
ï² ï² ï² ï² ï²
Ã©hï€¦Ã¹ Ã© f (h,s,a)+w Ã¹
h h
Ãªï²Ãº Ãª ï² ï² ï² ï² Ãº
sï€¦ f (h,s,a)+w
Ãª Ãª a ï² ï€¦ Ãº Ãº = Ãª ÃªDa ï² s -Ã‘ ï² F(s ï² ,a ï² , s Î¼ ï² ) Ãº Ãº . (21)
Ãª Ãº Ãª ï² a ï² ï² ï² Ãº
ï²
ÃªÃ«Î¼ï€¦ÃºÃ» ÃªÃ« DÎ¼-Ã‘
Âµ
ï² F(s,a,Î¼)ÃºÃ»
These equations of motion allow one to simulate the behaviour of conservative particles as a
(generalised) gradient flow of autonomous states on variational free energy. The resulting
dynamics can be read as a generalised homeostasis (cf., the behaviour of a thermostat or noise
cancellation scheme) or, in control theory, control as inference (Baltieri and Buckley, 2019;
Kappen, 2005; Todorov, 2008). In short, active states will look as if they are trying to minimise
the sensory prediction errors in (19). More generally, the active paths of conservative particles
will look as if they are trying to maximise the accuracy part of variational free energy, thereby
fulfilling the predictions encoded by internal dynamics (because the complexity part does not
depend upon active paths). This interpretation furnishes an elementary but expressive
formulation of active perception or inference (Friston et al., 2010a).
Can we say anything more definitive about the ensuing autonomous behaviour? We will see
that the autonomous dynamics of conservative particles acquire a purposeful aspectâ€”a purpose
that can be articulated in terms of information and preference seeking behaviour with the
following lemma (Barp et al. 2022; Friston et al. 2022):
Lemma (expected free energy): the Lagrangian of autonomous paths of conservative particles
can be expressed as a free energy functional that entails optimal Bayesian design and decision-
making:
17
ï² ï² ï² ï² ï² ï²
E=(a)=-E
ï² ï² ï²
[L(h,s) L(h|a)] L(a)
ï€±pï€´(h,sï€´|a)ï€´ï€´ï€²ï€´ï€´ï€´ï€´ï€³
Expected free energy
ï² ï² ï² ï² ï² ï²
= D [p(s |a)|| p(s)]+E
ï² ï² ï²
[L(s |h,a)] (22)
ï€±Kï€´L ï€´ï€´ï€²ï€´ï€´ï€´ï€³ ï€±pï€´(h,sï€´|aï€´) ï€²ï€´ï€´ï€´ï€³
Risk Ambiguity = S
0
ï² ï² ï² ï² ï² ï²
=-E ï² ï² [L(s)] E ï² ï² [D [p(h|s,a)|| p(h|a)]]
ï€±pï€´(ï€´s|aï€²) ï€´ï€´ï€³ ï€±pï€´(s|aï€´) ï€´Kï€´L ï€´ï€´ï€²ï€´ï€´ï€´ï€´ï€´ï€´ï€³
Expected cost Expected information gain
The functional forms of variational (14) and expected free energy (22) suggest that expected
(negative) accuracy becomes ambiguity, while expected complexity becomes risk. Similarly,
the expected (negative) divergence becomes expected information gain. If we read the
Lagrangian as a cost function of sensory outcomes, the most likely autonomous paths of
conservative particles will look as if they are maximising expected information gain, while
minimising expected cost. These expectations underwrite the principles of optimal Bayesian
design and decision theory, respectively.
Proof: when random fluctuations on the motion of particular states vanish, there is no
uncertainty about their paths given their parents. This means there is no uncertainty about
particular paths, given external paths; no uncertainty about autonomous paths, given sensory
paths and no uncertainty about sensory paths, given external and autonomous paths. Expressed
in terms of continuous and differential entropies this means, from (9):
ï² ï² ï² ï²
lim H [p(p|h)]=Ã›0 E [L(p=|h)] S
Î“ Â®0 0
p
ï² ï² ï² ï²
H [p(a|s)]=Ã›0 E [L(a=|sÃ)] S
0 (23)
ï² ï² ï² ï² ï² ï² ï² ï² ï²
H [p(s |h,a)]=Ã›0 E [L(s |h,=aÃ)] S E ï²ï² ï² [L(=s|h,a)] S
0 p(h,s|a) 0
ï² ï² ï² ï² ï² ï² ï² ï² ï²
H [p(a|h,s)]=Ã›0 E [L(a|h=,sÃ)] S E ï²ï²ï² [L(=a|h,s)] S
0 p(h,s|a) 0
These constraints allow us to express the Lagrangian of autonomous paths as an expected free
energy. From (23), we have:
ï² ï² ï² ï² ï² ï² ï²
L(a)=-
E ï²ï² ï²
[L(h,s,a) L(h,s|a)]
p(h,s|a)
ï² ï² ï² ï² ï² ï² ï² ï² ï² ï²
+ =--
E ï²ï²ï²
[L(h,s) L(h|a)]
E ï² ï²ï²
[L(a|h,s) L(s|h,a)] (24)
p(h,s|a) p(h,s|a)
ï² ï² ï² ï² ï²
= - + =- E ï²ï² ï² [L(h,s) L(h|a)] S S E(a)
ï€±pï€´(h,sï€´|a)ï€´ï€´ï€²ï€´ï€´ï€´ï€´ï€³ 0 0
Expected free energy
Substituting the final equality in (20) into (24) gives (22):
18
ï² ï² ï² ï² ï² ï² ï²
E(a)=E
ï² ï² ï²
[L(s-)+L(h|s,a) L(h|a)]
ï€±pï€´(h,sï€´|a)ï€´ï€´ï€´ï€´ï€²ï€´ï€´ï€´ï€´ï€´ï€´ï€³
Expected free energy
ï² ï² ï² ï² ï² ï²
=-E ï² ï² [L(s)] E ï² ï² [D [p(h|s,a)|| p(h|a)]]
ï€±pï€´(ï€´s|aï€²) ï€´ï€´ï€³ ï€±pï€´(s|aï€´) ï€´Kï€´L ï€´ï€´ï€²ï€´ï€´ï€´ï€´ï€´ï€´ï€³
Expected cost Expected information gain (25)
ï² ï² ï² ï² ï² ï²
=E
ï² ï²
[L(s)]-E
ï² ï²
[D [p(s |h,a)|| p(s |a)]]
ï€±pï€´(ï€´s|aï€²) ï€´ï€´ï€³ ï€±pï€´(h|aï€´) ï€´Kï€´L ï€´ï€´ï€²ï€´ï€´ï€´ï€´ï€´ï€´ï€³
Expected cost Expected information gain
ï² ï² ï² ï² ï² ï²
= D [p(s |a)|| p(s)]+E
ï² ï² ï²
[L(s |h,a)]
ï€±Kï€´L ï€´ï€´ï€²ï€´ï€´ï€´ï€³ ï€±pï€´(h,sï€´|aï€´) ï€²ï€´ï€´ï€´ï€³
Risk Ambiguity = S
0
These equalities rest on the fact that the KL divergences in the expected information gains are
the same; i.e., the mutual information between external and sensory paths, given an autonomous
path [p. 273 in (Batina et al. 2011)]. (cid:0)
Remarks: the relationship among the various terms contributions to expected free energy can
be seen clearly in terms of their expectations; namely, entropies. From (22):
ï² ï² ï² ï² ï² ï²
E ï²
[L(a)]-=S
H
[p(a=)]
H
[p-(h,s)]
H
[p(h|a)]
p(a) 0 ï€±ï€´ï€´ï€´ï€´ï€²ï€´ï€´ï€´ï€´ï€³
<Expected free energy>
ï² ï² ï²
=-H [p(h)] H [p(h|a)] (26)
ï€±ï€´ï€´ï€´ï€²ï€´ï€´ï€´ï€³
<Risk>
ï² ï² ï²
=-H [p(s)] H [p(s |a)]
ï€±ï€´ï€²ï€´ï€³ ï€±ï€´ï€²ï€´ï€³
ï€±<ï€´Cosï€´t> ï€´ï€²<Inï€´formï€´atioï€´n gaï€³in>
<Risk>
In the case of conservative particles, the conditional uncertainty about sensory paths, given
autonomous paths, is the information gain about external paths, afforded by sensory paths:
ï² ï² ï² ï² ï² ï² ï²
[p(s |a)]=- [p(h|a)] [p(h|s,a)] (27)
H H H
ï€±ï€´ï€´ï€´ï€´ï€´ï€²ï€´ï€´ï€´ï€´ï€´ï€³
<Information gain>
Figure 3 illustrates these decompositions for people who find it easier to think in terms in terms
of information diagrams.
19
Figure 3: dissipative and conservative particles. These information diagrams depict the entropy of external,
sensory and autonomous paths, where intersections correspond to shared or mutual information. A conditional
entropy corresponds to an area that is outside the variable upon which it is conditioned. The diagram on the left
shows the generic (dissipative) case, in which uncertainty about paths inherits from random fluctuations. When
the random fluctuations on the motion of particular states vanish, we have conservative particles, in which there
is (almost) no uncertainty about autonomous paths, given sensory paths and no uncertainty about particular paths,
given external paths (the right information diagram). This illustrates the various ways of expressing the entropy of
autonomous paths (upper equation)â€”or the entropy of sensory paths given autonomous paths (lower equation)â€”
in terms of conditional entropies. See main text for a fuller description.
In summary, the Lagrangian of autonomous paths can be expressed as a divergence; namely,
the divergence between the density over sensory (or external) paths with and without
autonomous paths. This can be regarded as the objective function used in engineering to
optimise the trajectory of control variables in model predictive control (Schwenzer et al., 2021).
An equivalent neurobiological perspective is provided by perceptual control theory, in which
sensory states are maintained near a preferred value through the consequences of action
(Mansell, 2011). On this view, risk is the divergence between sensory paths, given an
autonomous path, and the marginal density over sensory paths. This marginal density encodes
preferences, because they constitute the priors of the generative model, over the sensory
consequences of action (Parr and Friston, 2019). Intuitively, these paths can be read as
â€˜outcomesâ€™, â€˜eventsâ€™ or â€˜narrativesâ€™ the particle expects to encounter.
Expected free energy supplements risk with ambiguity; namely, the expected inaccuracy or
negative sensory likelihood, given an autonomous path. Clearly, this is redundant as a
description of conservative particles thatâ€”by definitionâ€”have minimal ambiguity (because
there are no random sensory fluctuations). However, in applications of the FEP, the ambiguity
term is retained to ensure the particle or agent seeks out unambiguous regimes of state space;
thereby, evincing the behaviour of conservative particles.
20
In discrete state-space formulations, ambiguity is usually expressed as the expected conditional
uncertainty (i.e., entropy) of sensory states, given external states by assuming conditional
independence between sensory and external states. Under this assumption we have, from (20):
ï² ï² ï² ï² ï² ï²
E ï²ï²ï² [L(s|h)]= E ï²ï² [ E ï²ï² [L(s|h)]]= E ï²ï² [ E ï²ï² [L(s|h)]] (28)
p(h,s|a) ï€±pï€´(h|aï€´) ï€´ï€´p(ï€²s|h)ï€´ï€´ï€´ï€´ï€³ ï€±pï€´(s|aï€´) ï€´ï€´p(hï€²|s)ï€´ï€´ï€´ï€´ï€³
Ambiguity Expected inaccuracy
This is why the expected inaccuracy is referred to as ambiguity. In brief, the expected free
energy lemma, suggests that conservative particles can be described as minimising the risk of
incurring external trajectories that diverge from prior preferences, while avoiding ambiguous
states of affairs.
The final term in (22) provides a Bayesian interpretation of expected free energy. It is the
expected divergence between posterior beliefs about external paths, given autonomous paths,
with and without sensory paths. In other words, it scores the resolution of uncertainty or
expected information gain17 afforded by sensory outcomes. This is the objective function in
optimal Bayesian design and active learning (Lindley, 1956; Mackay, 1992). In this sense, it is
sometimes referred to as intrinsic value or epistemic affordance (Friston et al., 2017c). The
remaining term is the expected Lagrangian, which scores the probability that a particle will
sample preferred sensory trajectories. In the setting of Bayesian decision theory, this minimises
expected cost (Berger, 2011). In this sense it is sometimes referred to as extrinsic value or
pragmatic affordance (Friston et al., 2017a; Schwartenbeck et al., 2015). Note that expected
free energy dissolves the exploration-exploitation dilemma (Cohen et al., 2007), because there
is a unique course or path (of least) action that subsumes explorative, information-seeking and
exploitative, preference-seeking imperatives. Please see Figure 4 for a schematic that links and
contextualises these normative accounts of behaviour through expected free energy. On this
17 Expected information gain can be expressed in a number of ways. The expression in (23) is perhaps the most
intuitive, showing the degree to which the conditional density of external paths changes once we take sensory
paths into account (i.e., how far we update our beliefs about external paths given sensory paths). It can also be
expressed as a mutual information, which scores the degree of conditional dependence between sensory and
external paths, or as the difference between two entropies:
ï² ï² ï² ï² ï² ï² ï² ï² ï² ï² ï² ï²
D [p(h,s |a)|| p(s |a)p(h|a)]=-
E ï²ï²
[L(s|a)]
E ï² ï²ï²
[L(s|h,a)]
ï€±Kï€´L ï€´ï€´ï€´ï€´ï€´ï€²ï€´ï€´ï€´ï€´ï€´ï€´ï€³ ï€±pï€´(s|aï€´) ï€´ï€´ï€´ï€´ï€´ï€²ï€´p(s,hï€´|aï€´) ï€´ï€´ï€´ï€´ï€³
Mutual information Expected information gain
This formulation offers an intuition as to the factors that determine the degree of belief-updating. If the entropy of
sensory paths (given autonomous paths) is very large, this implies a high level of uncertainty which, if resolved,
leads to a substantial change in our beliefs about external paths. The extent to which this uncertainty is resolvable
depends upon the conditional entropy of sensory paths given external and autonomous paths. In other words, if
there is a precise relationship between external and sensory paths, and we are uncertain about sensory paths, there
must be uncertainty about external paths which is resolvable by observing sensory paths.
For conservative particles, the expected information gain reduces to the entropy of sensory paths given autonomous
paths. It may therefore seem that the conservative particle assumption renders the expected information gain trivial.
However, it is worth noting that it may be possible to express the dynamics of a non-conservative particle (i.e.,
with sensory fluctuations) as a conservative particle, simply by treating the sensory fluctuations as (fast) external
states. In this case, the equality between expected information gain and the entropy of sensory paths, given
autonomous paths, holds in general.
21
view, the FEP may provide a first principles endorsementâ€”in terms of conservative particlesâ€”
of these normative accounts.
In summary, conservative particles look as if they have purposeful behaviour; in the sense that
they actively seek out preferred sensations, while trying to resolve uncertainty about the causes
of those sensations. Crucially, the sensorium (i.e., external and sensory paths) will not exhibit
this kind of behaviour. This is because conservative particlesâ€”in a dissipative worldâ€”break
the statistical symmetry between internal and external dynamics.
The next distinctionâ€”between different kinds of thingsâ€”rests on the circular causality inherent
in the coupling that underwrites â€˜thingnessâ€™. As explained below, this causal structure depends
on active paths (i.e., actions), which depend upon internal beliefs, which encode the sensory
consequences of action. This causal recursionâ€”cf., strange loop (Hofstadter, 2007) and
cybernetic feedback loop (Ashby 1956)â€”compels us to consider an internal representation of
action, which is distinct from the active states that realise those beliefs. In the final section, we
consider the principles of least action that apply to certain particles that manifest a stronger kind
of sentient behaviour; namely, planning.
Figure 4 â€“ Expected free energy and other normative accounts. This figure illustrates the various ways in which
expected free energy can be unpacked. Expected free energy subsumes several objective functions that
predominate across psychology, machine learning and economics. These special cases are disclosed when one
removes particular sources of uncertainty. For experimental science, this suggests which normative account is
most appropriate to describe behaviour based upon the nature of uncertainty. For example, if we ignore
preferences, or cost, then the expected free energy reduces to expected information gain (Lindley, 1956; MacKay,
2003) that underwrites optimum Bayesian design in statisticsâ€”or intrinsic motivation in machine learning and
robotics (Barto et al., 2013; Oudeyer and Kaplan, 2007; Ryan and Deci, 1985). This is mathematically the same
as expected Bayesian surprise and mutual information that underwrites salience in visual search (Itti and Baldi,
2009; Sun et al., 2011) and the computational architecture of our visual apparatus (Barlow, 1961; Barlow, 1974;
Linsker, 1990; Optican and Richmond, 1987). Ignoring ambiguity leads to risk-sensitive policies in economics
22
(Fleming and Sheu, 2002; Kahneman and Tversky, 1979) or KL control in engineering (Todorov, 2008; van den
Broek et al., 2010). Here, minimising risk corresponds to aligning predictions to prior preferences. In the absence
of expected information gain (i.e., reducible uncertainty), we are left with or expected preferences or utility in
economics (Von Neumann and Morgenstern, 1944)â€”a construction that underwrites reinforcement learning
(Sutton and Barto, 1998) and behavioural psychology. Maximising expected utility under uncertainty leads to
Bayesian decision theory (Berger, 2011). Finally, if we consider an unambiguous world with uninformative
preferences, expected free energy reduces to the negative entropy of posterior beliefs about the causes of data. This
is the maximum entropy principle, proposed as a method of inference by Jaynes (Jaynes, 1957; Lasota and Mackey,
1994). When conditioned on action, this corresponds to a form of empowerment (Klyubin et al., 2005); namely,
keeping options open.
Strange particles and generalised free energy
â€œWhat I mean by â€œstrange loopâ€ is â€¦ the cycling-around â€¦ from one level of abstraction (or
structure) to another, which feels like an upwards movement in a hierarchy, and yet somehow
the successive â€˜upwardâ€™ shifts turn out to give rise to a closed cycle. That is, despite one's sense
of departing ever further from one's origin, one winds up, to one's shock, exactly where one had
started out.â€ (Hofstadter, 2007) p.101
So far, we have considered how the dynamics of conservative (i.e., classical) particles can be
read as an elementary form of Bayesian inference (i.e., Bayesian mechanics). We now turn to
particles that admit particles within particles. In other words, particles whose internal states
have particular partitions (Palacios et al., 2020) that equip a particle with a hierarchical or deep
generative model. This means there exist some internal states that are not directly influenced
by active states.
We will consider the simplest instance of this sparse coupling, in which the active states of a
conservative particle are hidden from internal states. In other words, internal states are only
influenced by sensory states. In this kind of particle, the influence of active states on internal
states must be mediated vicariously via external (or sensory) states. Figure 2 illustrates the
ensuing causal architecture, where the generative modelâ€”and implicit variational densityâ€”
acquire a hierarchical depth.
If internal paths are only directly influenced by sensory paths, which furnish a Markov blanket
that renders the internal states conditionally independent of external and active states, this can
be expressed in terms of expected Lagrangians as follows, following (20):
ï² ï² ï² ï² ï² ï²
E
[L(Âµ|s)]Â³Eh[L(Âµ|s,a, )]Â³S
0
ï² ï² ï² ï² ï² ï²
lim
E
[L(Âµ|s)]=Eh[L(Âµ|s,a, )]=S
G Â®0 0
p
ï² ï² ï² ï² ï² ï²
Ã=E
ï² ï² ï²
[D [p(Âµh|Âµs,a, )|| p( |s)]] 0 (29)
p(s,h,a) KL
ï² ï² ï² ï² ï² ï² ï² ï² ï² ï²
Ã›Ã=L(^Âµh|sÂµ,a, ) hLÂµ( |s) ( ,a)|s
ï² ï² ï² ï² ï² ï² ï²
Ã=L(hÂµ,ha|s, ) L( ,a|s)
This means the internal paths of least action now become:
23
ï² ï² ï² ï² ï² ï² ï² ï² ï²
Î¼ï€¦(t)=-DÃ‘Î¼ L(Î¼=|s-,Ã‘a,h) DÎ¼ L(Î¼|s)
ï² ï²
Âµ Âµ (30)
ï² ï² ï²
=-DÃ‘Î¼ L(s,Î¼)
ï²
Âµ
Because the internal paths depend only directly on sensory paths, there is a deterministic
mapping from every sensory path to the corresponding internal path of least action. If this
mapping is injective, this implies a map between the internal path of least action and the
conditional density over external and active paths, given sensory paths. This means the
variational density can be defined stipulatively as a Bayesian belief over the whole sensorium18
ï² ï² ï² ï² ï²
q
ï²
(h,a)ï€ p(h,a|s)
Î¼ (31)
ï² ï² ï² ï² ï²
Î¼ï€argmin
ï²
L(Âµ|s)Ã›=Ã‘
ï²
L(Î¼|s) 0
Âµ Âµ
We will call these kinds of particles strange, where active states become hidden causes, lending
action a certain opacity (Limanowski and Friston, 2018), and where internal paths of least action
states encode beliefs about external and active paths. This leads to a notion of autonomy or
agency, in the sense that the particle has beliefs about its own actions, and a sense of selfhood:
the â€˜Iâ€™ that follows beliefs about â€˜myâ€™ actions.
â€œIn the end, we are self-perceiving, self-inventing, locked-in mirages that are little miracles of
self-reference.â€ (Hofstadter, 2007) p.363.
The notion that internal states encode representations about active states has some nice parallels
in neurophysiology in terms of efference copy (Jeannerod and Arbib 2003) and corollary
discharges in the brain (Gyr 1972).
Definition 4: strange particles are conservative particles whose active states do not directly
influence (i.e., are hidden from) internal states; that is, the flow of internal states in (10) does
not depend upon active states f (s,a,Âµ)= f (s,Âµ); and where (31) holds.
Âµ Âµ
As in the variational free energy lemma, we now construct a generalised free energy whose
minima coincide with the internal paths of least action of strange particles:
Lemma (generalised free energy): the internal paths of a strange particle minimise a free energy
functional of Bayesian beliefs about the hidden causes of sensory paths; namely, external and
ï² ï² ï² ï² ï²
active paths, q ï² (h,a)= p(h,a|s):
Î¼
18 In the case of linear flows in (1), the flow of generalised states in (2) is also linear, which means that the
Lagrangian is quadratic, e.g., see Section 6 in (Friston et al. 2022). In this case, the condition under which a
variational density can be defined that satisfies (31) is the object of Lemma 2.1 in (Da Costa et al. 2021) using
the fact that external and active states are hidden from internal states by the Markov blanket provided by sensory
states. Analogously to our reasoning for (non-strange) particles, we expect that in the nonlinear case, for many
such systems where sensory states form a Markov blanket, there will be a map affording (31).
24
ï² ï² ï² ï² ï²
Ã› =Î¼=Ã›argmin L(Âµ|s) Ã‘ L(s,Î¼) 0
ï² ï²
Âµ Âµ
ï² ï² ï² ï² ï²
Ã =Î¼=Ã›argmin G(s,Âµ) Ã‘ G(s,Î¼) 0
ï² ï²
Âµ Âµ
ï² ï² ï² ï²
Î¼ï€¦ =-DÃ‘Î¼ G(s,Î¼)
ï²
a
ï² ï² ï² ï² ï² ï² ï² ï²
G(s,Âµ)=
E ï² ï²
[L(h,s |a)+E(a)+lnq(h,a)]
ï€±qï€´(h,aï€´) ï€´ï€´ï€´ï€´ï€²ï€´ï€´ï€´ï€´ï€´ï€´ï€³
(32)
Generalised free energy
ï² ï² ï² ï² ï² ï²
=
E
[L(h,s,a,Âµ)]-
E
[-lnq(h,a)]
ï€±qï€´ï€´ï€²ï€´ï€´ï€³ ï€±qï€´ï€´ï€²ï€´ï€´ï€³
Energy constraint Entropy
ï² ï² ï² ï² ï² ï² ï² ï²
= D [q(h,a)|| p(h,a)]+
E
[L(s,Âµ|h,a)]
ï€±Kï€´L ï€´ï€´ï€²ï€´ï€´ï€´ï€³ ï€±qï€´ï€´ï€²ï€´ï€´ï€³
Complexity (â€“) Accuracy
ï² ï² ï² ï² ï² ï² ï² ï² ï²
= D [q(h,a)|| p(h,a|s)]+ L(s,Âµ) Â³L(s,Âµ)
ï€±Kï€´L ï€´ï€´ï€´ï€²ï€´ï€´ï€´ï€´ï€³ ï€±ï€´ï€²ï€´ï€³
Divergence (â€“) Log evidence
The generalised free energy expresses a generative model of sensory and internal dynamics in
terms of their causes, which include active paths. Choosing internal paths that minimise
generalised free energy maximises the evidence afforded to the generative model by sensory
paths. This has been referred to as self-evidencing (Hohwy, 2016). Heuristically, a strange
particle will look as if it is garnering evidence for its generative model, where its generative
model entails the prior belief that it acts like a conservative particle (because it is).
Proof: from the definition in Error! Reference source not found. the Lagrangian and
generalised free energy share the same minima on internal paths of least action, where their
gradients vanish:
ï² ï² ï² ï² ï² ï² ï² ï² ï² ï² ï²
G(s,Î¼)= D [q (h,a)|| p(h,a|s)]+L(s,Î¼)=L(s,Î¼)
ï²
ï€±Kï€´L ï€´Î¼ï€´ï€´ï€²ï€´ï€´ï€´ï€´ï€³
=0
Ã
(33)
ï² ï² ï² ï²
Î¼ï€¦(t)=-DÃ‘Î¼ L(s,Î¼)
ï²
Âµ
ï² ï² ï²
=-DÃ‘Î¼ F(s,Î¼)
ï²
Âµ
This means we can replace the Lagrangian gradients in Error! Reference source not found.
with generalised free energy gradients to give (32) (cid:0)
Remarks: For strange particles, there is distinction between active paths as realised variables
and the active paths in (32) as random variables that are inferred. This means that the actions
of an agent are distinct from beliefs about action, which are based on expected free energy.
When the generative model is supplemented with these priors, the ensuing functional
corresponds to (upper bound on) the Lagrangian or surprisal of sensory outcomes. In short,
sensory surprisal is a functional of Bayesian beliefs (i.e., a variational density) about the hidden
causes of sensations; namely, external and active dynamics. Internal dynamics minimise this
surprisal, while active paths can be described as realising the sensory consequences of inferred
25
action. In effect, the agent authors her sensorium, based upon prior beliefs about the way she
acts.
Note that only the internal paths minimise generalised free energy. Nothing has changed from
the perspective of the active paths, which can be cast as gradient flows on variational free
energy. However, from the perspective of an observer, behaviour will appear to be fulfilling the
epistemic and pragmatic imperatives afforded by expected free energy. From a statistical
perspective, this corresponds to a move from control as inference (Tschantz et al., 2022) to
planning as inference (Attias, 2003; Botvinick and Toussaint, 2012; Mirza et al., 2016). From
a biological perspective, this corresponds to a move from homeostasis (Cannon, 1929) to
allostasis (Ashby, 1947; Corcoran et al., 2020; Ramsay and Woods, 2014; Seth and Friston,
2016; Sterling and Eyer, 1988). Clearly, these distinctions depend upon the time over which
path integrals are taken.
We have been deliberately vague about the timescale (or order of generalised motion) over
which the expected free energy applies. This vagueness (Machina, 1976) admits a range of
temporal horizons for conservative particles: some may have a myopic generative model and
implicitly consider path integrals over short periods of time, responding to external fluctuations
in a largely reflexive, homeostatic manner (e.g., chemotaxis). Others may have a deep time
horizon and exhibit more adaptive, allostatic behaviours (e.g., curiosity). These behaviours are
usually associated with deep generative models that feature a separation of temporal scales: for
in silico examples, please see (Friston et al., 2017d; George and Hawkins, 2009).
Simulating sentience
This formulation of strange particles can be used to simulate sentient behaviour by solving the
following equations of motion yeah, in which internal dynamics minimise generalised free
energy and active dynamics minimise the ensuing variational free energy. Combining (21) with
(32):
ï² ï² ï² ï² ï²
Ã©hï€¦Ã¹ Ã© f ï² (h,s,a)+w Ã¹
h h
Ãªï²Ãº Ãª ï² ï² ï² ï² Ãº
Ãª Ãª a s ï² ï€¦ ï€¦ Ãº Ãº = Ãª ÃªD f a ï² s ï² ( - h Ã‘ ,s ï² , F a ( ) s ï² + , w a ï² , s Î¼ ï² ) Ãº Ãº (34)
Ãª Ãº Ãª ï² a ï² ï² Ãº
ï²
ÃªÃ«Î¼ï€¦ÃºÃ» ÃªÃ« DÎ¼-Ã‘
Âµ
ï² G(s,Î¼) ÃºÃ»
ï² ï²
Figure 5 provides an example in which the expected free energy E(a)=L(a) was specified
directly with a Lagrangian over autonomous paths, in terms of some equations of motion, c.f.,
a central pattern generator. When specifying generative models like this, the Lagrangian over
autonomous paths is usually specified as a prior over exogenous causes that intervene in
external dynamics. Intuitivelyâ€”unbeknown to the modelâ€”action fulfils the predictions of
some prior beliefs about causes that are exogenous to the external dynamics. In other words,
even though the particle is causing its sensations, it just â€˜thinksâ€™ it has the right prior beliefs
about exogenous dynamics.
26
As is common in these simulations, exogenous dynamics prescribe itinerant behaviour in the
form of a strange attractor. Equation (34) has been used to emulate many kinds of sentient
behaviour, ranging from morphogenesis (Friston et al., 2015), through action observation
(Friston et al., 2011) to birdsong (Isomura et al., 2019).
Figure 5 â€“ sentient behaviour and action observation. This figure illustrates simulations of active inference (here,
writing), in terms of conditional expectations about hidden states of the world, consequent predictions about
sensory input and ensuing action. The dynamics that underwrite this behaviour rest upon prior expectations about
exogenous causes that follow Lotka-Volterra dynamics: these are the six (arbitrarily) coloured lines in the upper
left inset. In this generative model, each state is associated with a location in Euclidean space that attracts the
agentâ€™s finger. In effect, the (generalised) internal states then supply predictions of what (generalised) sensory
states should register if the agentâ€™s beliefs were true. Active states suppress the ensuing prediction error (i.e.,
maximise accuracy) by fulfilling expected changes in sensed angular velocity, through exerting forces on the
agentâ€™s joints (not shown). The subsequent movement of the arm is traced out in the lower left panel. This trajectory
is plotted in a moving frame of reference, so that it looks like handwriting (e.g., a succession of â€˜jâ€™ and â€˜aâ€™ letters).
The lower left panels show the activity of the fourth hidden state under â€˜actionâ€™, and â€˜action-observationâ€™. During
action, sensory states register the visual and proprioceptive consequences of movement, while under action
observation, only visual sensations are availableâ€”as if the agent was watching another agent. The red dots
correspond to the times during which this state exceeded an arbitrary threshold. They key thing to note here is that
this internal state responds preferentially when, and only when, the motor trajectory produces a down-stroke, but
not an up-stroke moreâ€”evincing a cardinal feature of neuronal responses, namely, directional selectivity.
Furthermore, with a slight delay, this internal state responds during action and action observation. From a
neurobiological perspective, this is interesting because it speaks to an empirical phenomenon known as mirror
neuron activity (Gallese and Goldman, 1998; Kilner et al., 2007; Rizzolatti and Craighero, 2004). Please see
(Friston et al., 2011) for further details.
27
The handwriting example in Figure 5 illustrates a simple sort of active inference specified in
ï²
terms of a Lagrangian L(a) over autonomous (i.e., exogenous) causes. To simulate the curious
behaviour of strange particles like ourselves, one can evaluate expected free energy explicitly,
ï² ï²
E(a)=L(a).
Effectively, this enables a specification of autonomous dynamics in terms of Lagrangians (i.e.,
equations of motion) that engender information and preference seeking. Practically speaking,
knowing the functional form of expected free energy allows one to simulate or reproduce
sentient behaviour that is specified in terms of preferred outcomes. The requisite free energy
functionals are:
ï² ï² ï² ï² ï² ï²
= E(a)=-E
ï²ï² ï² ï²ï²
[L(h,s) L(h|a)] L(a)
ï€±pï€´(s|hï€´,a)ï€´q(h|ï€´a)ï€´ï€²ï€´ï€´ï€´ï€´ï€´ï€³
Expected free energy
ï² ï² ï² ï² ï² ï² ï² ï²
F(s,a)=E
ï²
[L(hÂ³,s,a)+lnq(h)] L(s,a)
ï€±qï€´(h)ï€´ï€´ï€´ï€²ï€´ï€´ï€´ï€´ï€³
Variational free energy
ï² ï² ï² ï² ï² ï² ï² ï² ï² ï²
G(s,Âµ)=E
ï² ï²
[L(h,s |a)+E(a)+lnq(h,a)]Â³L(s,Âµ) (35)
ï€±qï€´(h,aï€´) ï€´ï€´ï€´ï€´ï€²ï€´ï€´ï€´ï€´ï€´ï€´ï€³
Generalised free energy
ï² ï² ï² ï² ï² ï² ï² ï²
F(s,Î±)=L(s,Î±):a=argmin
ï²
F(s,a,Î¼)
a
ï² ï² ï² ï² ï² ï² ï²
G(s,Î¼)=L(s,Î¼):Î¼=argmin
ï²
G(s,Âµ)
Âµ
The expected free energy above is an expectation under a predictive density over hidden causes
and sensory consequences, based on beliefs about external states, supplied by the variational
density. Intuitively, based upon beliefs about the current state of affairs, the expected free
energy furnishes the most likely â€˜direction of travelâ€™ or path into the future. This construction
specifies the most likely behaviour in terms of expected outcomes, where the expected free
energy brings a belief-dependent or epistemic aspect to behaviour; namely, curiosity (Friston
et al., 2017b; Schmidhuber, 2006; Still and Precup, 2012).
This lends a complementary meaning to the expected free energy; in the sense it is expected in
the future (and past). This implicit prospection (and postdiction) means that sensory paths into
the future (and past) are random variables. In short, strange particles or agents (look as if they)
think they are conservative particles, and act accordingly. The example in Figure 6 suppresses
prior preferences to reveal pure information-seeking or epistemic behaviourâ€”a succession of
visual palpationsâ€”in the setting of visual search and scene construction. This example
specified a sequence of active trajectories to fixed points, specified as exogenous causes. These
visual fixation points minimised expected free energy; i.e., maximised expected information
gain.
In summary, strange particles look as if they have agency, in the sense their actions realise the
predictions of Bayesian beliefs. This leads to a slightly counterintuitive interpretation of the
ensuing behaviour, in which active states are hidden from internal representations of action.
Intuitively, this means beliefs about how the world should play out are realised, reflexively, by
active states. From the perspective of someone observing an agent, say a fish, it will look as if
28
the fish searches out particles of food. However, from the point of view of the fish, it believes
that it is propelled through water in a fortuitous and benevolent way that delivers food particles
to its mouth. In other words, the fish is unaware it is the agent of its actions, it just believes this
is how the world worksâ€”beliefs that are realised through action: cf., ideomotor and perceptual
control theory (Mansell, 2011; Pfister et al., 2014; Seth, 2015; Wiese, 2017). Note that this is
not inconsistent with the fish learning the association between specific internal (e.g., neural)
stimuli and the sensory consequences of action, thus becoming aware, that it can control its own
actionsâ€”or at least their consequences.
This speaks to further particular kinds, with deeper generative models, that (look as if they)
recognise action is underwritten by agency. Or indeed, they are the authors of their actions.
These kinds of agents may be distinguished by generative models of state-dependent random
fluctuations that introduce further sort of hierarchical depth; namely, beliefs about beliefs in the
form of beliefs about uncertainty or precision19 (Clark, 2013a; Limanowski, 2017; Limanowski
and Blankenburg, 2013; Seth, 2013).
Figure 6 â€“ Epistemic foraging. This figure shows the results of a simulation in which a face was presented to an
agent, whose responses were simulated by selecting actions that minimised expected free energy following an eye
movement. The agent had three internal beliefs or hypotheses about the stimuli she might sample (an upright face,
an inverted face and a rotated face). The agent was presented with an upright face and her posterior expectations
were evaluated over 16 (12 ms) time bins, until the next saccade was emitted. This was repeated for eight saccades.
19 That is, the inverse covariances in (19).
29
The ensuing fixation points are shown as red dots in the upper row. The corresponding sequence of eye movements
is shown in the inset on the upper left, where the red circles correspond roughly to the proportion of the visual
image sampled. These saccades are driven by predictive beliefs about the next fixation point based upon salience
maps in the second row. These salience maps are the (negative) expected free energy as a function of action;
namely, where to look next. Note that these maps change with successive saccades as posterior beliefs about hidden
causes become more confident. Note also that salience is depleted in locations that were foveated in the previous
saccade, because these locations no longer have epistemic affordance (i.e., the ability to reduce uncertainty or
expected information gain). Empirically, this is known as inhibition of return. Oculomotor responses are shown in
the third row in terms of the two hidden oculomotor states corresponding to vertical and horizontal eye movements.
The associated portions of the image sampled (at the end of each saccade) are shown in the fourth row. The final
two rows show the accompanying posterior beliefs, in terms of posterior expectations and 90% Bayesian credible
intervals. This illustrates the nature of active inference when sampling data to disambiguate among hypotheses or
percepts that best explain sensory evidence. Please see (Friston et al., 2012) for further details.
Epilogue: on the nature of strangeness
â€œNow, here, you see, it takes all the running you can do, to keep in the same placeâ€, the Red
Queen to Alice in Lewis Carroll's Through the Looking-Glass.
Clearly, we have taken some poetic licence in associating the recursion implicit in the dynamics
of strange particles with a strange loop (Hofstadter, 2007). However, there exists a
complementary perspective on conservative dynamics that brings us back to strange loops. This
is nicely exemplified by Red Queen dynamics (Ao, 2005; Zhang et al., 2012), which can be
read as: moving forwards â€œone winds up, to one's shock, exactly where one had started outâ€.
Red Queen dynamics are one of many formulations of conservative, divergence-free, or
solenoidal flow that underwrites nonequilibrium steady-states and the implicit breaking of
detailed balance that characterises living systems (Ao, 2008; Haken, 1983; Nicolis and
Prigogine, 1977).
To see why this kind of dynamics is characteristic of conservative particles, we turn to a
complementary decomposition of the flow afforded by a Helmholtz decomposition (Ao, 2004;
Barp et al., 2021; Da Costa and Pavliotis, 2022; Eyink et al., 1996; Graham, 1977a; Ma et al.,
2015; Shi et al., 2012). This decomposes the flow of states into a conservative solenoidal
component and a dissipative gradient flow that depends upon the amplitude of random
fluctuations
f(x)=Ã‘QÃ(x) -G(x) Ã‘Ã(x) -Ã‘Ã‘(xÃ—Ã—G)+ Q(x) (x). (36)
ï€±ï€´ï€²ï€´ï€³ ï€±ï€´ï€²ï€´ï€³ ï€±ï€´ï€´ï€´ï€²ï€´ï€´ï€´ï€³
Conservative Dissipative correction terms
Here, Ã-(=x) ln p(x) is the self-information or surprisal under the (nonequilibrium steady-
state) solution to the density dynamics in (1). The sensorimotor loop20â€”implicit in our
definition of particleâ€”ensures the presence of solenoidal flow, which breaks time-reversal
symmetry (Jiang et al., 2004).
When random fluctuations are small, the solenoidal contribution predominates, leading to
20 By sensorimotor loop we mean that sensory states influence internal states that in turn influence active states,
and not vice versa.
30
chaotic itinerancy turbulence, solenoidal mixing and strange attractors (Friston et al., 2021;
Namikawa, 2005; Parr et al., 2020; Pavlos et al., 2012; Takens, 1980; Tsuda and Fujii, 2004).
Solenoidal flow is conservative because it circulates on the level sets of the Lagrangian. This
licenses the name conservative particles, as conservative flow governs the motion of particular
states. In the limit of vanishingly small fluctuations, we have an attracting set of states to which
the paths are confined. In other words, trajectories will always bring systemic states back to the
neighbourhood of a previously occupied state; namely, ending up â€œwhere one had started outâ€.
It is interesting to note that most of the biomimetic simulations referenced above are based on
a Lagrangian (i.e., generative model) whose equations of motion have a strange attractorâ€”an
attracting set with a fractional dimension (Ma et al., 2014); e.g., (Friston and Frith, 2015). These
attractors feature recurrent loops through PoincarÃ© sections (Bramburger and Kutz, 2020) that
may qualify as a strange loop, of a sort.
Discussion
â€œWas it utterly absurd to seek behind the ordering structures of this world a â€˜consciousnessâ€™
whose â€˜intentionsâ€™ were these very structures?â€ (Heisenberg, 1971)
There are many issues that attend the free energy principle. We take this opportunity to briefly
consider four.
The first is a link to early formulations of self-organisation in cybernetics; namely, Ashbyâ€™s
Law of Requisite Variety (Ashby, 1956) that underpins the Good Regulator Theorem (Conant
and Ashby, 1970), later articulated as the Internal Model Principle of control theory (Francis
and Wonham, 1976). The Law of Requisite Variety pertains to the degrees of freedom of an
agent's internal model. Put simply, an organism (i.e., active particle) must have a repertoire of
states that is at least equal to the number of fluctuations in the environment (i.e., external states).
As often quoted: â€œonly variety can absorb [destroy] varietyâ€. In the present setting, this speaks
to the number of generalised internal states, in relation to the number of generalised external
states that matter. External states that matter are slow (dynamically unstable) states that can be
distinguished from fast (dynamically stable) fluctuations (Carr, 1981; Frank, 2004; Haken,
1983; Koide, 2017). The Law of Requisite Variety emerges under the above formalism from
the requirement that the number of generalised internal states must be greater than the number
of generalised external states (that matter). This follows because internal states play the role of
sufficient statistics of a variational density over generalised external states, with one or more
statistic for each external state. This instance of the Law of Requisite Variety speaks to the
vague nature of sentient behaviour described by the free energy principle.
Sentience is read here as an attribute of behaviour that rests on inference; namely, movement
on a statistical manifold, on which internal states evolve (Parr et al. 2020; Friston et al. 2022).
Movement on this (statistical) manifold corresponds to belief updating and thereby an
elementary kind of sentience or sense making with a well-defined information geometry
(Caticha, 2015; Ikeda et al., 2004; Kim, 2018). Having said this, sentient behaviour can be so
elemental as to be trivial. For example, a thermostat with one degree of freedom to act uponâ€”
31
and representâ€”its external milieu can only exhibit a weak sort of sentient behaviour from the
perspective of a particle (like you and me) with a more expressive model of the thermostat and
its external states. The FEP makes a clear commitment to describing sentience as embodied
cognition, where embodiment is supplied by the interactionâ€”and implicit boundaryâ€”between
a â€˜thingâ€™ and everything else, formulated as dynamical coupling via a Markov blanket.
This touches on our second issue; namely, any empirical and philosophical claims about the
free energy principle, in relation to sentient behaviour: in brief, the free energy principle is a
method for describing the behaviour of certain kinds of particles that may have strong or weak
sentience, depending upon the way in which one particle (e.g., you or me) makes sense of
another (e.g., a thermostat). These attributes of strong or weak sentience emerge as a
consequence of the interaction between the various states of a particle and the states external to
it. It may be possible to experimentally demonstrate where any given particle (e.g., molecules,
mice, and men) belongs in this sentient hierarchy by simply examining the nature of their
particular dynamics. For example, this treatment of the FEP rests upon the smooth nature of
fluctuations in (1), cf. (Friston et al. 2022), and the nature of the coupling in (10), both of which
may be empirically tested via Bayesian selection of stochastic dynamical models given time-
series data from a particle and its surrounding external states. On a more practical level, there
are procedures for identifying nested Markov blankets (across scales) from empirical time-
series. Please see (Friston et al. 2020) for a worked example using brain imaging data. Whether
these procedures find purchase in decomposing distributed systems across scales, in empirical
studies, remains to be seen.
When the world external to a particle is constituted of other particles, we enter the realm of
collective behaviour and interacting particles. From the perspective of active matter, the key
perspective offered by the FEP is the Markov blanket that segregates the internal degrees of
freedom of a particle from other particles. It implies that agents implicitly have beliefs about
other agentsâ€”that is, generative models that entail a sense of others, and perhaps a sense of
self. Interactions between agents occur through the blanket in a process of active inferenceâ€”
not solely by mechanical forcesâ€”introducing a sentient aspect to interactions. The distinction
between various particular kinds may be particularly apt to account for different kinds of active
matterâ€”from least to most sentientâ€”as is encountered in varying degrees throughout the life
sciences. It would be interesting to analyse the extent to which ensembles of increasingly
sentient particles feature richer interactions (e.g., communication) among agents, in relation to
the variously studied collective phenomena (e.g., pattern formation, flocking, and phase
transitions). In machine learning, where interacting particles are often used for the purpose of
achieving a goal faster than a single particle could (Borovykh et al., 2021), introducing more
sentient (i.e., beyond inert) particles may also lead to richer interactions and more â€˜intelligentâ€™
behaviour: from biological intelligence to distributed cognition (Levin, 2019).
Third, from a statistical perspective, there are interesting parallels between applications of the
free energy principle and the complete class theorem (Brown, 1981; Wald, 1947). The complete
class theorem asserts that for any pair of behaviours and preferences, there are some priors that
render the behaviour Bayes optimal. This translates, in the setting of the free energy principle,
into the assertion that there exists some internal dynamics, which encode prior beliefs about
32
external dynamics that render autonomous dynamics Bayes optimal. Whether this description
of autonomous behaviour is an apt description of internal dynamics per se is a question that
cannot be answered. This is because internal states are inaccessible, by constructionâ€”they are
private to the particle in question. This leaves one in the game of inferring internal dynamics
on the basis of sensory and active trajectories, which is a fair description of most of the life
sciences; especially, the neurosciences: e.g., (Zeki and Shipp, 1988). For example, much of
modern neuroscience attempts to peer through the brainâ€™s Markov blanket, using neuroimaging
and electrophysiology. Or, breaching the Markov blanket to observe internal states, through the
use of post-mortem studies and invasive procedures. Interestingly, much of computational
neuroscience can be framed as finding the generative model that best explains a subjectâ€™s
choices and responses (Parr et al., 2018).
Finally, it is worth noting many things have not been addressed in this brief account of the free
energy principle. For example, we have ignored states that endow dynamics with a functional
form; namely, the parameters of the equations of motion that define a Lagrangian. These
parameters necessarily introduce a separation of temporal scales, in the sense that the
parameters of the flow operators change slowly in relation to states. Anecdotally, this introduces
a distinction between inference and learning over fast and slow timescales, respectively. At this
level of analysis there are many outstanding issues (Fields et al., 2021b). For example, how
does inference underwrite learning? Can autopoiesis (Maturana and Varela, 1980) be
understood in terms of blanket-building at a slow timescale? Is there a natural progression over
scales to more conservative (and strange) mechanics (Jeffery et al., 2019)? How do particles of
particles behave (Heins et al., 2022; Kuchling et al., 2020; Levin, 2019; Palacios et al., 2020)
and so onâ€¦
Additional Information
Funding Statement
KF is supported by funding for the Wellcome Centre for Human Neuroimaging (Ref:
205103/Z/16/Z) and a Canada-UK Artificial Intelligence Initiative (Ref: ES/T01279X/1). L.D.
is supported by the Fonds National de la Recherche, Luxembourg (Project code: 13568875).
This publication is based on work partially supported by the EPSRC Centre for Doctoral
Training in Mathematics of Random Systems: Analysis, Modelling and Simulation
(EP/S023925/1). CH is supported by the U.S. Office of Naval Research (N00014-19-1-2556).
This research was funded in part by the Wellcome Trust [205103/Z/16/Z]. For the purpose of
Open Access, the author has applied a CC BY public copyright license to any Author Accepted
Manuscript version arising from this submission. The work of GAP was partially funded by the
EPSRC, grant number EP/P031587/1, and by JPMorgan Chase & Co through a Faculty
Research Award 2019 and 2021.
Acknowledgements
33
We would like to thank Samuel Tenka, Mel Andrews, and the co-organisers of the International
Physics Reading Group with Maxwell Ramstead, who generated many of the issues and
questions addressed in this paper. We thank our anonymous reviewers for their helpful
comments which improved the manuscript.
Competing Interests
The authors have no competing interests.
Authors' Contributions
All authors made substantial contributions to conception and design, and writing of the article,
and approved publication of the final version.
References
Ao, P., 2004. Potential in stochastic differential equations: novel construction. J Phys a-Math
Gen 37, L25-L30.
Ao, P., 2005. Laws in Darwinian evolutionary theory. Physics of Life Reviews 2, 117-156.
Ao, P., 2008. Emerging of Stochastic Dynamical Equalities and Steady State Thermodynamics
from Darwinian Dynamics. Commun Theor Phys 49, 1073-1090.
Ashby, W.R., 1947. Principles of the self-organizing dynamic system. J Gen Psychol 37, 125-
128.
Ashby, W.R., 1956. An introduction to cybernetics. John Wiley and Sons, Oxford, England
Attias, H., 2003. Planning by Probabilistic Inference, Proc. of the 9th Int. Workshop on
Artificial Intelligence and Statistics.
Balaji, B., Friston, K., 2011. Bayesian State Estimation Using Generalized Coordinates. Signal
Processing, Sensor Fusion, and Target Recognition Xx 8050, 80501Y
Baltieri, M., Buckley, C.L., 2019. PID Control as a Process of Active Inference with Linear
Generative Models dagger. Entropy 21, 257.
Barlow, H., 1961. Possible principles underlying the transformations of sensory messages, in:
Rosenblith, W. (Ed.), Sensory Communication. MIT Press, Cambridge, MA, pp. 217-234.
Barlow, H.B., 1974. Inductive inference, coding, perception, and language. Perception 3, 123-
134.
Barp, A., Takao, S., Betancourt, M., Arnaudon, A., Girolami, M., 2021. A Unifying and
Canonical Description of Measure-Preserving Diffusions. https://arxiv.org/abs/2105.02845.
Barp, A., Da Costa, L., FranÃ§a, G., et al, 2022. Geometric Methods for Sampling,
Optimisation, Inference and Adaptive Agents. In: Geometry and Statistics. Academic Press,
pp 21â€“78
Barto, A., Mirolli, M., Baldassarre, G., 2013. Novelty or surprise? Front Psychol 4, 907.
Beal, M.J., 2003. Variational algorithms for approximate Bayesian inference. University of
London United Kingdom.
Batina, L., Gierlichs, B., Prouff, E., et al (2011) Mutual Information Analysis:
a Comprehensive Study. J Cryptol 24:269â€“291. https://doi.org/10.1007/s00145-010-9084-8
Berger, J.O., 2011. Statistical decision theory and Bayesian analysis. Springer, New York;
London.
Bishop, C.M., 2006. Pattern recognition and machine learning. Springer, New York.
34
Bohm, D., 2008. A new theory of the relationship of mind and matter. Philosophical Psychology
3, 271-286.
Born, M., Fock, V., 1928. Beweis des Adiabatensatzes. Zeitschrift fï¿½r Physik 51, 165-180.
Borovykh, A., Kantas, N., Parpas, P., Pavliotis, G., 2021. Optimizing interacting Langevin
dynamics using spectral gaps, ICML.
Botvinick, M., Toussaint, M., 2012. Planning as inference. Trends in Cognitive Sciences 16,
485-488.
Bramburger, J.J., Kutz, J.N., 2020. Poincare maps for multiscale physics discovery and
nonlinear Floquet theory. Physica D-Nonlinear Phenomena 408, 132479.
Brown, L.D., 1981. A Complete Class Theorem for Statistical Problems with Finite-Sample
Spaces. Ann Stat 9, 1289-1300.
Cannon, W.B., 1929. Organization for physiological homeostasis. Physiological Reviews 9,
399-431.
Carr, J., 1981. Applications of Centre Manifold Theory. Springer-Verlag, Berlin.
Caticha, A., 2015. The Basics of Information Geometry. Aip Conf Proc 1641, 15-26.
Clark, A., 2013a. The many faces of precision (Replies to commentaries on "Whatever next?
Neural prediction, situated agents, and the future of cognitive science"). Front Psychol 4, 270.
Clark, A., 2013b. Whatever next? Predictive brains, situated agents, and the future of cognitive
science. The Behavioral and brain sciences 36, 181-204.
Cohen, J.D., McClure, S.M., Yu, A.J., 2007. Should I stay or should I go? How the human brain
manages the trade-off between exploitation and exploration. Philos Trans R Soc Lond B Biol
Sci. 362, 933-942.
Conant, R.C., Ashby, W.R., 1970. Every Good Regulator of a system must be a model of that
system. Int. J. Systems Sci. 1, 89-97.
Corcoran, A.W., Pezzulo, G., Hohwy, J., 2020. From allostatic agents to counterfactual
cognisers: active inference, biological regulation, and the origins of cognition. Biology &
Philosophy 35, 32.
Da Costa, L., Friston, K., Heins, C., Pavliotis, G.A., 2021a. Bayesian mechanics for stationary
processes. Proceedings. Mathematical, physical, and engineering sciences 477, 20210518.
Da Costa, L., Pavliotis, G.A., 2022. The entropy production of stationary diffusions.
arXiv.2212.05125
Eyink, G.L., Lebowitz, J.L., Spohn, H., 1996. Hydrodynamics and fluctuations outside of local
equilibrium: Driven diffusive systems. Journal of Statistical Physics 83, 385-472.
Feynman, R.P., 1972. Statistical mechanics. Benjamin, Reading MA.
Fields, C., Friston, K., Glazebrook, J.F., Levin, M., 2021a. A free energy principle for generic
quantum systems, p. arXiv:2112.15242.
Fields, C., Glazebrook, J.F., Levin, M., 2021b. Minimal physicalism as a scale-free substrate
for cognition and consciousness. Neuroscience of consciousness 2021, niab013.
Fleming, W.H., Sheu, S.J., 2002. Risk-sensitive control and an optimal investment model II.
Annals of Applied Probability 12, 730-767.
Francis, B.A., Wonham, W.M., 1976. The internal model principle of control theory.
Automatica 12, 457-465.
Frank, T.D., 2004. Nonlinear Fokker-Planck Equations: Fundamentals and Applications.
Springer Series in Synergetics. Springer, Berlin.
Friston, K., 2010. The free-energy principle: a unified brain theory? Nat Rev Neurosci 11, 127-
138.
Friston, K., 2012. A Free Energy Principle for Biological Systems. Entropy (Basel) 14, 2100-
2121.
Friston, K., 2013. Life as we know it. J R Soc Interface 10, 20130475.
Friston, K., 2019. A free energy principle for a particular physics, eprint arXiv:1906.10184.
35
Friston, K., Da Costa, L., Sajid, N., Heins, C., UeltzhÃ¶ffer, K., Pavliotis, G.A., Parr, T., 2022.
The free energy principle made simpler but not too simple, p. arXiv:2201.06387.
Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., Pezzulo, G., 2017a. Active Inference:
A Process Theory. Neural Comput 29, 1-49.
Friston, K., Frith, C., 2015. A Duet for one. Conscious Cogn 36, 390-405.
Friston, K., Heins, C., UeltzhÃ¶ffer, K., Da Costa, L., Parr, T., 2021. Stochastic Chaos and
Markov Blankets. Entropy.
Friston, K., Kilner, J., Harrison, L., 2006. A free energy principle for the brain. Journal of
Physiology-Paris 100:70â€“87. https://doi.org/10.1016/j.jphysparis.2006.10.001
Friston, K., Levin, M., Sengupta, B., Pezzulo, G., 2015. Knowing one's place: a free-energy
approach to pattern regulation. J R Soc Interface 12.
Friston, K., Mattout, J., Kilner, J., 2011. Action understanding and active inference. Biological
cybernetics 104, 137-160.
Friston, K., Mattout, J., Trujillo-Barreto, N., Ashburner, J., Penny, W., 2007. Variational free
energy and the Laplace approximation. Neuroimage 34, 220-234.
Friston, K.J., 2008. Variational filtering. Neuroimage 41, 747-766.
Friston, K.J., Adams, R.A., Perrinet, L., Breakspear, M., 2012. Perceptions as hypotheses:
saccades as experiments. Frontiers in Psychology 3, 151.
Friston, K.J., Daunizeau, J., Kilner, J., Kiebel, S.J., 2010a. Action and behavior: a free-energy
formulation. Biological cybernetics 102, 227-260.
Friston, K.J., Fagerholm, E.D., Zarghami, T.S., et al, 2020. Parcels and particles: Markov
blankets in the brain. arXiv:200709704 [q-bio]
Friston, K.J., Lin, M., Frith, C.D., Pezzulo, G., Hobson, J.A., Ondobaka, S., 2017b. Active
Inference, Curiosity and Insight. Neural Comput 29, 2633-2683.
Friston, K.J., Parr, T., de Vries, B., 2017c. The graphical brain: Belief propagation and active
inference. Network neuroscience (Cambridge, Mass.) 1, 381-414.
Friston, K.J., Rosch, R., Parr, T., Price, C., Bowman, H., 2017d. Deep temporal models and
active inference. Neuroscience and biobehavioral reviews 77, 388-402.
Friston, K.J., Stephan, K., Li, B.J., Daunizeau, J., 2010b. Generalised Filtering. Mathematical
Problems in Engineering 2010, 621670.
Gallese, V., Goldman, A., 1998. Mirror neurons and the simulation theory of mind-reading.
Trends Cogn Sci 2, 493-501.
George, D., Hawkins, J., 2009. Towards a mathematical theory of cortical micro-circuits. PLoS
Comput Biol 5, e1000532.
Graham, R., 1977a. Covariant formulation of non-equilibrium statistical thermodynamics.
Zeitschrift fÃ¼r Physik B Condensed Matter 26.
Graham, R., 1977b. Path integral formulation of general diffusion processes. Zeitschrift fÃ¼r
Physik B Condensed Matter 26.
Gyr, J.W., 1972. Is a theory of direct visual perception adequate? Psychological Bulletin
77:246â€“261. https://doi.org/10.1037/h0032352
Hafner, D., Ortega, P.A., Ba, J., Parr, T., Friston, K., Heess, N., 2020. Action and Perception
as Divergence Minimization, p. arXiv:2009.01791.
Haken, H., 1983. Synergetics: An introduction. Non-equilibrium phase transition and self-
selforganisation in physics, chemistry and biology. Springer Verlag, Berlin.
Haldane, J.B.S., 1932. The Inequality of Man and Other Essays. Chatto & Windus.
Heins, C., Klein, B., Demekas, D., Aguilera, M., Buckley, C., 2022. Spin glass systems as
collective active inference. https://arxiv.org/abs/2207.06970.
Heisenberg, W., 1971. Physics and Beyond: Encounters and Conversations. G. Allen & Unwin.
Hofstadter, D.R., 2007. I am a strange loop. Recording for Blind & Dyslexic, Princeton, NJ.
Hohwy, J., 2016. The Self-Evidencing Brain. Nous 50, 259-285.
36
Hutter, M., 2006. Universal Artificial Intellegence : Sequential Decisions Based on Algorithmic
Probability. Springer-Verlag Berlin and Heidelberg & Co. KG, Dordrecht.
Ikeda, S., Tanaka, T., Amari, S., 2004. Stochastic reasoning, free energy, and information
geometry. Neural Comput 16, 1779-1810.
Isomura, T., Parr, T., Friston, K., 2019. Bayesian Filtering with Multiple Internal Models:
Toward a Theory of Social Intelligence. Neural Comput 31, 2390-2431.
Itti, L., Baldi, P., 2009. Bayesian surprise attracts human attention. Vision Res 49, 1295-1306.
JakÅ¡iÄ‡, V., Pillet, C.-A., 1998. Ergodic properties of classical dissipative systems I. Acta
Mathematica 181, 245â€“282.
Jaynes, E.T., 1957. Information Theory and Statistical Mechanics. Physical Review Series II
106, 620â€“630.
Jaynes. E.T., 2003. Probability Theory: The Logic of Science. Cambridge University Press,
Cambridge
Jeannerod, M., Arbib, M., 2003. Action monitoring and forward control of movements. In:
The handbook of brain theory and neural networks, MIT Press. Cambridge, MA, USA, pp 83â€“
85
Jeffery, K., Pollack, R., Rovelli, C., 2019. On the Statistical Mechanics of Life: Schrodinger
Revisited. Entropy 21, 1-21.
Jiang, D.-Q., Qian, M., Qian, M.-P., 2004. Mathematical Theory of Nonequilibrium Steady
States: On the Frontier of Probability and Dynamical Systems. Springer-Verlag, Berlin
Heidelberg.
Kahneman, D., Tversky, A., 1979. Prospect Theory: An Analysis of Decision under Risk.
Econometrica 47, 263-291.
Kappen, H.J., 2005. Path integrals and symmetry breaking for optimal control theory. Journal
of Statistical Mechanics-Theory and Experiment 11, P11011.
Kelso, J.A.S., 2021. Unifying Large- and Small-Scale Theories of Coordination. Entropy
(Basel) 23, 537.
Kerr, W.C., Graham, A.J., 2000. Generalized phase space version of Langevin equations and
associated Fokker-Planck equations. European Physical Journal B 15, 305-311.
Kilner, J.M., Friston, K.J., Frith, C.D., 2007. Predictive coding: an account of the mirror neuron
system. Cognitive processing 8, 159-166.
Kim, E.J., 2018. Investigating Information Geometry in Classical and Quantum Systems
through Information Length. Entropy (Basel) 20, 574.
Klyubin, A.S., Polani, D., Nehaniv, C.L., 2005. Empowerment: A universal agent-centric
measure of control. 2005 Ieee Congress on Evolutionary Computation, Vols 1-3, Proceedings
1, 128-135.
Koide, T., 2017. Perturbative expansion of irreversible work in Fokker-Planck equation a la
quantum mechanics. Journal of Physics a-Mathematical and Theoretical 50, 325001.
Kuchling, F., Friston, K., Georgiev, G., Levin, M., 2020. Morphogenesis as Bayesian inference:
A variational approach to pattern formation and control in complex biological systems. Phys
Life Rev 33, 88-108.
Lanillos, P., Meo, C., Pezzato, C., Meera, A.A., Baioumy, M., Ohata, W., Tschantz, A.,
Millidge, B., Wisse, M., Buckley, C.L., Tani, J., 2021. Active Inference in Robotics and
Artificial Agents: Survey and Challenges, p. arXiv:2112.01871.
Lasota, A., Mackey, M.C., 1994. Chaos, fractals, and noise : stochastic aspects of dynamics,
2nd ed. Springer-Verlag, New York.
Levin, M., 2019. The Computational Boundary of a "Self": Developmental Bioelectricity
Drives Multicellularity and Scale-Free Cognition. Front Psychol 10, 2688.
Licata, I., Chiatti, L., 2019. Event-Based Quantum Mechanics: A Context for the Emergence
of Classical Information. Symmetry-Basel 11, 181.
37
Limanowski, J., 2017. (Dis-)Attending to the Body, in: Metzinger, T.K., Wiese, W. (Eds.),
Philosophy and Predictive Processing. MIND Group, Frankfurt am Main.
Limanowski, J., Blankenburg, F., 2013. Minimal self-models and the free energy principle.
Front Hum Neurosci 7, 547.
Limanowski, J., Friston, K., 2018. 'Seeing the Dark': Grounding Phenomenal Transparency and
Opacity in Precision Estimation for Active Inference. Front Psychol 9, 643.
Lindley, D.V., 1956. On a Measure of the Information Provided by an Experiment. Annals of
Mathematical Statistics 27, 986-1005.
Linsker, R., 1990. Perceptual Neural Organization - Some Approaches Based on Network
Models and Information-Theory. Annual Review of Neuroscience 13, 257-281.
Loeliger, H.A., 2002. Least Squares and Kalman Filtering on Forney Graphs, in: Blahut, R.E.,
Koetter, R. (Eds.), Codes, Graphs, and Systems. Springer US, Boston, MA, pp. 113-135.
Ma, Y.-A., Chen, T., Fox, E.B., 2015. A Complete Recipe for Stochastic Gradient MCMC, p.
arXiv:1506.04696.
Ma, Y., Tan, Q.J., Yuan, R.S., Yuan, B., Ao, P., 2014. Potential Function in a Continuous
Dissipative Chaotic System: Decomposition Scheme and Role of Strange Attractor.
International Journal of Bifurcation and Chaos 24, 1450015.
Machina, K.F., 1976. Truth, Belief, and Vagueness. Journal of Philosophical Logic 5, 47-78.
Mackay, D.J.C., 1992. Information-Based Objective Functions for Active Data Selection.
Neural Computation 4, 590-604.
MacKay, D.J.C., 2003. Information Theory, Inference and Learning Algorithms. Cambridge
University Press, Cambridge.
Mansell, W., 2011. Control of perception should be operationalized as a fundamental property
of the nervous system. Top Cogn Sci 3, 257-261.
Maturana, H.R., Varela, F., 1980. Autopoiesis: the organization of the living, in: Maturana HR,
V.F. (Ed.), Autopoiesis and Cognition. Reidel, Dordrecht, Netherlands.
Mirza, M.B., Adams, R.A., Mathys, C.D., Friston, K.J., 2016. Scene Construction, Visual
Foraging, and Active Inference. Frontiers in computational neuroscience 10, 56.
Mitter, S., Picci, G., Lindquist, A., 1981. Toward a theory of nonlinear stochastic realization,
in: Hinrichsen, A.I.a.D. (Ed.), Feedback and Synthesis of Linear and Nonlinear Systems.
Springer-Verlag, Bielefeld, Germany and Rome, Italy.
Namikawa, J., 2005. Chaotic itinerancy and power-law residence time distribution in stochastic
dynamical systems. Physical review. E, Statistical, nonlinear, and soft matter physics 72,
026204.
Nicolis, G., Prigogine, I., 1977. Self-organization in nonequilibrium systems : from dissipative
structures to order through fluctuations. Wiley, New York.
Optican, L., Richmond, B.J., 1987. Temporal encoding of two-dimensional patterns by single
units in primate inferior cortex. II Information theoretic analysis. J Neurophysiol. 57, 132-146.
Oudeyer, P.-Y., Kaplan, F., 2007. What is intrinsic motivation? a typology of computational
approaches. Frontiers in Neurorobotics 1, 6.
Palacios, E.R., Razi, A., Parr, T., Kirchhoff, M., Friston, K., 2020. On Markov blankets and
hierarchical self-organisation. Journal of theoretical biology 486, 110089.
Parr, T., Da Costa, L., Friston, K., 2020. Markov blankets, information geometry and stochastic
thermodynamics. Philosophical transactions. Series A, Mathematical, physical, and engineering
sciences 378, 20190159.
Parr, T., Friston, K.J., 2019. Generalised free energy and active inference. Biological
cybernetics 113, 495-513.
Parr, T., Pezzulo, G., Friston, K.J., 2022. Active Inference: The Free Energy Principle in
Mind, Brain, and Behavior. MIT Press, Cambridge, MA, USA
Parr, T., Rees, G., Friston, K.J., 2018. Computational Neuropsychology and Bayesian
Inference. Front Hum Neurosci 12, 61.
38
Pavliotis, G.A., 2014. Stochastic processes and applications : diffusion processes, the Fokker-
Planck and Langevin equations. Springer, New York.
Pavlos, G.P., Karakatsanis, L.P., Xenakis, M.N., 2012. Tsallis non-extensive statistics,
intermittent turbulence, SOC and chaos in the solar plasma, Part one: Sunspot dynamics.
Physica a-Statistical Mechanics and Its Applications 391, 6287-6319.
Pearl, J., 2009. Causality. Cambridge University Press, Cambridge.
Pfister, R., Melcher, T., Kiesel, A., Dechent, P., Gruber, O., 2014. Neural correlates of
ideomotor effect anticipations. Neuroscience 259, 164-171.
Ramaswamy, S., 2010. The Mechanics and Statistics of Active Matter, in: Langer, J.S. (Ed.),
Annual Review of Condensed Matter Physics, Vol 1, vol. 1, pp. 323-345.
Ramsay, D.S., Woods, S.C., 2014. Clarifying the roles of homeostasis and allostasis in
physiological regulation. Psychol Rev 121, 225-247.
Ramstead, M.J., Kirchhoff, M.D., Friston, K.J., 2020. A tale of two densities: active inference
is enactive inference. Adapt Behav 28, 225-239.
Ramstead, M.J.D., Sakthivadivel, D.A.R., Heins, C., Koudahl, M., Millidge, B., Da Costa, L.,
Klein, B., Friston, K.J., 2022. On Bayesian Mechanics: A Physics of and by Beliefs, p.
arXiv:2205.11543.
Rizzolatti, G., Craighero, L., 2004. The mirror-neuron system. Annu Rev Neurosci. 27, 169-
192.
Ryan, R., Deci, E., 1985. Intrinsic motivation and self-determination in human behavior.
Plenum, New York.
Sakthivadivel, D., 2022a. A Worked Example of the Bayesian Mechanics of Classical Objects.
Sakthivadivel, D.A.R., 2022b. A Constraint Geometry for Inference and Integration, p.
arXiv:2203.08119.
Sakthivadivel, D.A.R., 2022c. Towards a Geometry and Analysis for Bayesian Mechanics, p.
arXiv:2204.11900.
Schiff, S.J., Sauer, T., 2008. Kalman filter control of a model of spatiotemporal cortical
dynamics. J Neural Eng 5, 1-8.
Schmidhuber, J., 1991. Curious Model-Building Control-Systems. 1991 Ieee International Joint
Conference on Neural Networks, Vols 1-3 2, 1458-1463.
Schmidhuber, J., 2006. Developmental robotics, optimal artificial curiosity, creativity, music,
and the fine arts. Connection Science 18, 173-187.
SchrÃ¶dinger, E., 1944. What Is Life? : The Physical Aspect of the Living Cell. Trinity College,
Dublin, Dublin, pp. 1-32.
Schwartenbeck, P., FitzGerald, T.H., Mathys, C., Dolan, R., Kronbichler, M., Friston, K., 2015.
Evidence for surprise minimization over value maximization in choice behavior. Scientific
reports 5, 16575.
Schwenzer, M., Ay, M., Bergs, T., Abel, D., 2021. Review on model predictive control: an
engineering perspective. International Journal of Advanced Manufacturing Technology 117,
1327-1349.
Seifert, U., 2012. Stochastic thermodynamics, fluctuation theorems and molecular machines.
Reports on progress in physics. Physical Society (Great Britain) 75, 126001.
Sengupta, B., Tozzi, A., Cooray, G.K., Douglas, P.K., Friston, K.J., 2016. Towards a Neuronal
Gauge Theory. PLoS biology 14, e1002400.
Seth, A.K., 2013. Interoceptive inference, emotion, and the embodied self. Trends Cogn Sci 17,
565-573.
Seth, A.K., 2015. Inference to the Best Prediction, in: Metzinger, T.K., Windt, J.M. (Eds.),
Open MIND. MIND Group, Frankfurt am Main.
Seth, A.K., Friston, K.J., 2016. Active interoceptive inference and the emotional brain.
Philosophical transactions of the Royal Society of London. Series B, Biological sciences 371.
39
Shi, J.H., Chen, T.Q., Yuan, R.S., Yuan, B., Ao, P., 2012. Relation of a New Interpretation of
Stochastic Differential Equations to Ito Process. Journal of Statistical Physics 148, 579-590.
Singh, K.D., Barnes, G.R., Hillebrand, A., Forde, E.M., Williams, A.L., 2002. Task-related
changes in cortical synchronization are spatially coincident with the hemodynamic response.
Neuroimage 16, 103-114.
Sterling, P., Eyer, J., 1988. Allostasis: A new paradigm to explain arousal pathology, Handbook
of Life Stress, Cognition and Health. John Wiley & Sons, New York, pp. 629-649.
Still, S., Precup, D., 2012. An information-theoretic approach to curiosity-driven reinforcement
learning. Theory in biosciences = Theorie in den Biowissenschaften 131, 139-148.
Sun, Y., Gomez, F., Schmidhuber, J., 2011. Planning to Be Surprised: Optimal Bayesian
Exploration in Dynamic Environments, in: Schmidhuber, J., ThÃ³risson, K.R., Looks, M. (Eds.),
Artificial General Intelligence. Springer Berlin Heidelberg, Berlin, Heidelberg, pp. 41-51.
Sutton, R.S., Barto, A.G., 1981. Toward a modern theory of adaptive networks: expectation and
prediction. Psychol Rev 88, 135-170.
Sutton, R.S., Barto, A.G., 1998. Reinforcement Learning: An Introduction. MIT Press,
Cambridge, MA.
Takens, F., 1980. Detecting strange attractors in turbulence. Rijksuniversiteit Groningen.
Mathematisch Instituut, Groningen.
Todorov, E., 2008. General duality between optimal control and estimation, IEEE Conference
on Decision and Control.
Tschantz, A., Barca, L., Maisto, D., Buckley, C.L., Seth, A.K., Pezzulo, G., 2022. Simulating
homeostatic, allostatic and goal-directed forms of interoceptive control using active inference.
Biological psychology 169, 108266.
Tsividis, P.A., Loula, J., Jake, B., Foss, N., Campero, A., Pouncy, T., J., G.S., Tenenbaum, J.B.,
2021. Human-Level Reinforcement Learning through Theory-Based Modeling, Exploration,
and Planning. arXiv:2107.12544 [cs].
Tsuda, I., Fujii, H., 2004. A complex systems approach to an interpretation of dynamic brain
activity I: Chaotic itinerancy can provide a mathematical basis for information processing in
cortical transitory and nonstationary dynamics. Computational Neuroscience: Cortical
Dynamics 3146, 109-128.
van den Broek, J.L., Wiegerinck, W.A.J.J., Kappen, H.J., 2010. Risk-sensitive path integral
control. UAI 6, 1â€“8.
Vasseur, D.A., Yodzis, P., 2004. The Color of Environmental Noise. Ecology 85, 1146â€“1152.
Von Neumann, J., Morgenstern, O., 1944. Theory of Games and Economic Behavior. Princeton
University Press, Princeton.
Wald, A., 1947. An Essentially Complete Class of Admissible Decision Functions. Annals of
Mathematical Statistics 18, 549-555.
Wiese, W., 2017. Action Is Enabled by Systematic Misrepresentations. Erkenntnis 82, 1233-
1252.
Winn, J., Bishop, C.M., 2005. Variational message passing. Journal of Machine Learning
Research 6, 661-694.
Zeki, S., Shipp, S., 1988. The functional logic of cortical connections. Nature 335, 311-317.
Zhang, F., Xu, L., Zhang, K., Wang, E., Wang, J., 2012. The potential and flux landscape theory
of evolution. The Journal of chemical physics 137, 065102.
40

=== REVISE TO ===
PROFESSIONAL TONE: Begin directly with content - NO conversational openings like 'Okay, here's...'

1. Fix all issues above
2. Title: "Path integrals, particular kinds, and strange things"
3. Include 10-15 quotes from paper text
   - Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
   - Use consistent quote formatting: 'The authors state: "quote"' or vary attribution phrases
   - Vary attribution phrases to avoid repetition
   - CRITICAL: Only extract quotes that actually appear in the paper text
4. ELIMINATE ALL REPETITION - each sentence must be unique
   - Check before each sentence: 'Have I already said this?' If yes, write something new
   - Vary attribution phrases - do NOT repeat 'The authors state' multiple times
5. Extract methodology, results with numbers, key quotes
6. 1000-1500 words, structured with ### headers

Generate COMPLETE revised summary.