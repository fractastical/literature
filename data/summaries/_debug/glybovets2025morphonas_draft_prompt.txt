=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: MorphoNAS: Embryogenic Neural Architecture Search Through Morphogen-Guided Development
Citation Key: glybovets2025morphonas
Authors: Mykola Glybovets, Sergii Medvid

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Key Terms: system, embryogenic, search, development, into, neural, architecture, able, morphogen, morphonas

=== FULL PAPER TEXT ===

MorphoNAS: Embryogenic Neural Architecture
Search Through Morphogen-Guided Development
Mykola Glybovets, Sergii Medvid
Faculty of Informatics
National University of Kyiv-Mohyla Academy, Ukraine
{glib, smedvid}@ukma.edu.ua
Abstract—Whilebiologicalneuralnetworksdevelopfromcom- must minimize variational free energy. This results in emer-
pact genomes using relatively simple rules, modern artificial gent self-organization into predictable, low-entropy states via
neural architecture search methods mostly involve explicit and
perception (inference) and action (development or adapta-
routine manual work. In this paper, we introduce MorphoNAS
tion).Inmorphogenesis[3]context,thisresultsinembryonic
(Morphogenetic Neural Architecture Search), a system able to
deterministically grow neural networks through morphogenetic structuresemergingfromlocalinteractions,drivenbymor-
self-organizationinspiredbytheFreeEnergyPrinciple,reaction- phogens dynamics and gene regulatory networks (GRNs).
diffusionsystems,andgeneregulatorynetworks.InMorphoNAS, By merging these principles into an evolutionary computa-
simplegenomesencodejustmorphogensdynamicsandthreshold-
tional system, it becomes possible to evolve neural architec-
based rules of cellular development. Nevertheless, this leads to
tures not as explicit graphs, but as embryogenically grown
self-organization of a single progenitor cell into complex neural
networks, while the entire process is built on local chemical phenotypes, ruled by a compact genome which encodes
interactions. morphogen-based rulesandisguidedbyFEP-inspired self-
Our evolutionary experiments focused on two different do- organization. This implies bottom-up approach to building
mains: structural targeting, in which MorphoNAS system was cybernetic organism, a novel artificial neural network in this
able to find fully successful genomes able to generate predefined
case, complementing traditional top-down approaches from
random graph configurations (8–31 nodes); and functional per-
neuroscience and cybernetics [4].
formance on the CartPole control task achieving low complexity
6-7 neuron solutions when target network size minimization In this work, we introduce an evolutionary computational
evolutionary pressure was applied. The evolutionary process system, MorphoNAS, capable of generating neural networks
successfully balanced between quality of of the final solutions through morphogenetic self-organization. The developmental
andneuralarchitecturesearcheffectiveness.Overall,ourfindings
model utilizes the Free Energy Principle, reaction-diffusion
suggest that the proposed MorphoNAS method is able to grow
systems, and gene regulatory networks.
complexspecificneuralarchitectures,usingsimpledevelopmental
rules, which suggests a feasible biological route to adaptive and Thisapproachwassupportedbasedonexperimentsweper-
efficient neural architecture search. formedintwodomains:thegenerationofneuralnetworkswith
pre-defined topological properties (graph targeting), where all
I. INTRODUCTION
randomly generated configuration targets were achieved in
Biological brains are not assembled region-by-region, it 100%ofcases;andfunctionalcontroltaskinCartPoleenviron-
undergoes development stages using a compact genomic ment,whereminimalbutcapableneuralnetworkarchitectures
blueprint. Tony Zador’s genomic bottleneck hypothesis [1] were evolved under evolutionary pressure involving network
implies, that the genome, likely, does not encode a full size constraints.
diagramofthebrain,ratheracompressedclusterofdevelop- Wehypothesizethatcomplexneuralarchitecturescapableof
mentinstructions;mostlyrules,constraints,andtheconditions solving tasks may emerge from simple, biologically relevant,
these rules must satisfy. This results in the evolution of developmental rules. We provide examples of how networks
efficient and flexible architectures that are far more adaptable grown using the MorphoNAS method can be applied to real
than existing artificial systems. functional optimization tasks.
Moreover, Zador suggests [1] that the general machine These results suggest that the proposed evolutionary-
learning community has largely deviated from the original developmental approach has potential as a generic implemen-
biological inspiration. Zador argues that young animal brains tation for automated neural architecture search, suitable for a
are capable of effective learning without enormous numbers wide range of applied domains.
oflabeledexamplesusedbybothsupervisedandunsupervised
II. THEORETICALFOUNDATIONS
machine learning nowadays, and points to another important
A. Embryogenesis and FEP
implication of genomic bottleneck: it suggests a path toward
artificial neural networks capable of rapid learning [1]. Embryogenesis – the process by which a fertilized egg de-
The Free Energy Principle (FEP), introduced by Karl velopsintoacomplexmulticellularorganism–isaremarkable
Friston [2], brings theoretical foundation, extending Zador’s example of self-organization in biology. John Gurdon won
idea by arguing that all living systems, from cells to brains, Nobel Prize for demonstrating that all cells in a multicellular
5202
luJ
81
]EN.sc[
1v58731.7052:viXra
organism have the same genome [5]. Still, somehow, trillions B. Reaction-Diffusion Systems through the FEP Lens
of cells self-organize into complex organs, including brain,
Reaction-diffusion (RD) models, first proposed by Alan
from a single fertilized cell. This is a striking example of the
Turing [3], show how interacting diffusive chemicals (mor-
emergence principle [6], [7]. Understanding how organisms
phogens) can spontaneously break symmetry and form spatial
reliably establish their body plan and form during embryonic
patterns. These patterns emerge from morphogens’ dynamics,
development remains a grand challenge in developmental
mutual interaction and inhibition, a purely bottom-up mecha-
biology [4].
nism. When viewed through the FEP lens, morphogen gradi-
entsandRDdynamicscanbeinterpretedaspartofthesensory
1) Free Energy Principle (FEP): In [2], Karl Friston
and signaling landscape that cells use to infer their location.
introduced concept of free energy, which he derives from
So,inaFEPcontext,morphogensarenotjustpassivechemical
Expectation Maximization (EM) algorithm [8]. An important
cuesbutareactivelyregulatedsignalsthatcellsexpecttosense
conclusion is its connection to physics: the EM algorithm
if the pattern is correct [17]. Each cell has beliefs about the
maximizes function that corresponds to negative free energy
morphogenconcentrationsit“should”detectgivenitsposition
in physics [2]. In a later work [9], Friston and colleagues
in the embryo, and it can act (e.g. by secreting or absorbing
proposed FEP as a framework to explain how living systems
morphogens, moving, or changing receptor expression) to
self-organize and maintain low entropy by minimizing free
reduce any deviation from that expected profile [17]. As a
energy or “surprise”. Originally proposed as a brain theory
result,whatlookslikeaTuringpatternmayalsobeseenasan
[10], the FEP is now being applied beyond neuroscience to
emergentoutcomeofactiveinference:cellsreceivemorphogen
biological pattern formation and embryogenesis [11] [2], spe-
signals (sensory data) and take actions to drive those signals
cific physical models [12], and even generic quantum systems
toward an expected pattern, minimizing free energy along the
[13].Moreover,KarlFristonarguesthatformationoflifeitself
way.
is an inevitable emergent property of any random dynamical
Another study highlighting the link between RD models
system basing this claim on a simulation of primordial soup
and FEP comes indirectly from neuroscience. Atasoy et al.
[14].
showed [18] that reaction-diffusion systems can generate a
wide range of biologically relevant patterns, such as those
The FEP states that any self-organizing system at steady-
seen in morphogenesis or brain activity, through simple in-
state with its environment must minimize its internal varia-
teractions between spreading excitation and inhibition. These
tional free energy [10], which is a measure of the difference
patterns emerge naturally from the system’s geometry and the
between the system’s model of the world and its sensory
relative rates of diffusion, without the need to have detailed
inputs.Organismsavoidsurprisingstatesandtendtowardcon-
assumptions or fine-tune parameters for each specific pattern.
figurations that match their expectations (i.e. they minimize
This principle is further developed in another fundamental
surprise [10] or prediction error). By doing so, the system
neuroscience study by Safron [19] which directly maps it to
resists disorder and maintains its integrity, actively violating
FEP.
the second law of thermodynamics [10]. This principle has
By analogy, in embryogenesis, the chemical pre-patterns
been described by some authors as a “survival principle” for
of morphogens might be seen as solutions to a variational
living organisms [15], since minimizing informational free
principle(extremizingsomeinformationmetricorminimizing
energy is mathematically equivalent to maximizing the evi-
apotential).Indeed,Zhangetalshowthatthermodynamicen-
dencefortheorganism’sinternalmodelofitsenvironment.In
ergy dissipation enhances spatial accuracy and robustness of
the context of embryogenesis, this suggests that developing
Turing patterns [20], broadening the parameter ranges where
cellsmightactivelydrivethemselvestowardmorepredictable,
patterns form reliably. This suggests an energetic optimality
lower-surprise anatomical states.
playing role in natural pattern formation. The FEP reframes
it in informational terms: an embryo’s RD-mediated pattern is
2) Entropy Control in Development: The FEP involves
robust because it is stationary with respect to a free-energy
entropy minimization (or control) because minimizing sur-
functional [11]. The authors directly relate it to the least
prise constrains the system’s states to a limited set of states.
actionprinciple,whichisitselfaclassicexampleofvariational
In his book, What is life, Schro¨dinger famously noted that
principle. In summary, viewing reaction-diffusion systems
“living matter evades the decay to equilibrium” by “feeding
through the FEP lens provides a goal-directed interpretation
on‘negativeentropy”’[16,pp.74–75].FEPprovidesaformal
of morphogenetic patterning – morphogens become part of
explanation of this idea by formulating how organisms resist
an inferential feedback loop ensuring that spatial chemical
entropy through constant information exchange and update of
distributions converge to those predicted by the collective
their internal state. In the context of embryogenesis, theoret-
developmental program.
ical models suggest [10] that as development proceeds, the
system’s entropy is kept within bounds. A developing embryo
C. Gene Regulatory Networks (GRNs)
actively limits external randomness and organizes its internal
state along reliable scenarios – a perspective that unifies Such collective developmental programs are realized bio-
developmental robustness with principles of self-organization. logically through Gene Regulatory Networks (GRNs) which
interpret and respond to signals defined by morphogen con- minimization, corresponding to the fate decisions (division,
centrations,governingcellfatedecisions.Traditionally,GRNs differentiation, and axon growth) a cell must make in reaction
are modeled by regulatory logic circuits [21] or differential to its local morphogenetic environment. The cells operate to
equations [22] capturing transcription factor interactions and achieve homeostasis or predictability in their morphogenetic
morphogen inputs. The FEP perspective offers a higher-level locality, which is the source of the action(s) they take that are
interpretation: GRNs implement the generative model inside solely dependent on local morphogen concentrations with the
each cell, mapping environmental signals (e.g. morphogen decisions implemented as threshold-based transitions.
concentrations)tointernalstates(geneexpression,whichmod- 2) Reaction-DiffusionSystems: Thespatialdistributionand
ulatesmorphogensecretion)inawaythatmaximizesthecell’s dynamics of morphogens is modeled with a reaction-diffusion
certaintyaboutitsidentityorposition[4].Inotherwords,one process. A morphogen is released by a cell, which then dif-
can view a GRN as a Bayesian network that encodes prior fusesthroughoutthedevelopmentalfield,potentiallyinhibiting
expectations of gene expression patterns appropriate for each other morphogens. Together, this creates concentration gradi-
position in the embryo, and that updates expression based on ents which provide positional information for the cells. The
actual morphogen inputs to minimize discrepancy. Friston et morphogenconcentrationgradientsreflectTuringpatternsand
al. [4] explicitly suggested that genetic codes parameterize areresponsibleforsymmetrybreakingandspatialorganization
a generative model predicting the signals a cell should [3].
sense in the correct morphology. Epigenetic and signaling 3) Gene Regulatory Networks (GRNs): The genome G
processes then perform inference – adjusting gene activity represents the parameters that encode morphogen secretion
until the cell’s “beliefs” (gene expression profile) match the rates, diffusion profiles, inhibition interactions, and cellular
local signals, reaching a stable fate [4]. This approach views response thresholds. This is corresponding to the regulatory
cell differentiation as a Bayesian optimization process rather logic of GRNs which control how cells deal with morpho-
than a hardwired behavior directly encoded in the genome. genetic signals and their responses.
Instead, the genome provides a solution space of possible
patterns, and the intercellular interactions (via morphogen
III. MORPHONASFRAMEWORK
signaling and GRNs) navigate that space to find the pattern TheMorphoNASframeworkpresentedinthispaperisbased
with minimal free energy (which corresponds to maximum on the following four components: progenitors, which are
consistency between genes and signals). similar to stem cells, can divide and become differentiated;
A remarkable example of linking GRNs with an optimiza- neurons, which are differentiated cells capable of growing
tion principle is the recent work by Sokolowski, Tkacˇik, axons and forming synapse-type connections; morphogens,
Bialek, and Gregor [23]. They approached the classic gap chemical fields that direct cell growth, behavior, and decision
gene network in the early Drosophila (fruit fly) embryo – a making via concentration fields; and axon growth, which
well-studied GRN that reads a maternal morphogen gradient acts in a chemotactic-type manner, having axons grow in the
(Bicoid) and establishes segmental gene expression stripes. direction of increasing morphogen concentration, representing
Instead of manually fitting this network, they asked: Can a simplified form of neuronal connectivity.
the GRN be derived from an optimality principle? Using
A. Morphogenetic Development Model
an information-theoretic approach, they optimized a detailed
model of the gap gene system’s 50 parameters to maximize MorphoNAS can be defined as a framework to simulate
the positional information that the gene expression levels neural architectural formation via biologically inspired devel-
provide about nuclear position in the embryo [23]. Remark- opmental processes. The structure is defined formally as
ably, the optimized network spontaneously recapitulated
MorphoNAS=(G,Λ,C,M,S,D,I,T)
both the regulatory architecture and the spatial expression
patternsoftherealembryo’sgapgenes.Theauthorsrationalize where each of the components captures an integral aspect of
[23]howtheknownDrosophilaGRNemergesasthesolution the morphogenetic developmental process:
to a variational problem here maximizing information, which
isconceptuallyrelatedtominimizinguncertainty(freeenergy). Developmental Field (Λ) is the spatial environment in
This result strongly supports the idea that developmental which the neural growth occurs:
GRNs are shaped by and operate under optimality principles.
Λ={(i,j)|1≤i≤L ,1≤j ≤L }
x y
D. Key Mappings from Theory to Model
adopting a toroidal topology (connecting opposite edges),
Based on the theoretical foundations described above, let
serving to model continuous spatial development.
us now introduce a computational model that implements
morphogenetic development using self-organizing local inter- C denotes the cell types: C = {Progenitor,Neuron}. Pro-
actions to develop neural architectures. The key mappings genitor cells are defined as undifferentiated cells that can
from theory to the proposed model are the following: divide and differentiate to form neurons. Neuron cells are
1) Free Energy Principle: The behavior of the individ- defined as differentiated cells that can grow axons, and make
ual cells is driven by ”surprise” or variational free energy synapse-like connections.
Genome (G) encodes the developmental program as a disconnected from the cluster in the central region of growth
compact set of morphogenetic rules, which include: G = field. By Step 200, there are 10 neurons and 36 connections
dim
(L ,L ) ∈ N2, describing the spatial dimensions of the developed.
x y
developmental field Λ; G , which defines the morphogen
morph
specifications – each specification describing the secretion
rates for the appropriate cell type, the diffusion properties
of each morphogen (encoded as a tensor), and the parame-
ters concerning cross-inhibition; G describes the threshold
fates
values that govern progenitor cell division, progenitor cell
differentiation, and neuron axon growth; G , describes the
axon
axonextensionparameterstobetaken–drivingtheconnection
thresholds,andmaximumlengthsofaxons;andG ∈N,that
iter Fig.2. Neuralnetworkstructureatdifferentstagesofgrowth.
represents development time – in discrete time steps.
Morphogens (M) comprise chemical signals that guide IV. MORPHOGENETICDEVELOPMENTDYNAMICS
development: M = {m ,m ,...,m }, and diffuse spatially
1 2 k A. Initialization
across the developmental field, providing positional informa-
tion. At time t = 0, the developmental field Λ is initialized
with minimal structure: a single progenitor cell is placed at
Cellular State (S): S : Λ → R|M| ×(C ∪{∅}) defines position (i ,j ), like a neural stem cell; and all morphogen
0 0
local morphogen concentrations, and a resident cell (if any), concentrationsaresettozerointheentiredevelopmentalfield.
for each location.
Diffusion Dynamics (D): D :M→R(m(µ)×n(µ)) defines B. Iteration Steps
how each morphogen µ ∈ M diffuses through the develop- At each iteration t ∈ 0,1,...,G − 1, the following
iter
mental field, using diffusion tensors. sequence is executed:
Inhibition Dynamics (I): I : M×M → R defines the 1) Morphogen Secretion: Cells release chemical signals
cross-inhibition of morphogen behaviour, which can lead to into their local environment. For a cell of type c at
complex patterns through competitive interactions. position (i,j), morphogen concentrations are updated
as: c (i,j) ← c (i,j) + σ(m), where σ(m) is the
m m c c
Developmental Dynamics (T): T : S(Λ) → S(Λ) defines
secretion rate of morphogen m for cell type c.
thetemporalchangeofthesystem,implementationoftheself-
2) Diffusion:Morphogenconcentrationsareupdatedbased
organizing growth process at each time-step.
ondiffusionmatricesaccordingtooneofseveralformu-
AnexampleofneuralmorphogenesisusingtheMorphoNAS lations typically using convolution or a numerical inte-
system is shown in Figure 1. It demonstrates the self- grationscheme(e.g.,discreteLaplacianapproximation):
organizing development of a neural network on a 10 × 10 c m (i,j)←DiffusionStep(c m ,D(m))
developmental field after 2, 10 and, 200 simulated time steps 3) Inhibition: Local interactions will modify concentra-
resulting in 10 neurons and 36 connections. The gradients in tions according to the inhibition rules: c m (i,j) ←
(cid:81)
the background of these simulations reflect the morphogen c m (i,j) n̸=m (1 − α mn c n (i,j)), where α mn =
landscape achieved by the superposition of three chemicals. I(m,n) is the inhibition coefficient of morphogen n
inhibiting morphogen m.
4) Cell Fate Decisions: Each cell will evaluate its local
morphogen environment in order to assess its devel-
opmental fate. When progenitor cells need to make
decisions, they can exhibit three possible behaviors.
They can (1) divide, in this case producing a new
progenitor cell in an immediate neighbouring position;
this happens if the concentration of morphogens that
induce cell division exceeds a defined threshold; or (2)
differentiate into a neuron, if the concentration of mor-
Fig.1. ExampleofneuralmorphogenesisintheMorphoNASsystem. phogens that are responsible for differentiation exceeds
another predefined threshold; or (3) continue being in
The corresponding neural network structures at each stage their current state if no cells or differentiation need to
of development are shown in Figure 2. At Step 2, the neural occur.Neuroncellswillfollowthemorphogengradients
structure contains a single progenitor cell (not shown in the in the developmental environment to extend out axons
plot) and a single neuron. At Step 10, there are 4 neurons towardsneighbouringcells(otherneurons)basedonthe
and 6 connections in total. There is also an isolated neuron chemical signals in their local environment.
5) Axon Growth and Connection: Axons grow in a step- Algorithm 1: Morphogenetic Development Dynamics
wise fashion, following the gradient ascent in mor- Data: Initial developmental field Λ with a single
phogenfields.Whenagrowingaxonencountersanother progenitor cell; genome G
neuron it will form a stable connection if the local mor- Result: Final developmental state and neural network
phogen concentration exceeds the connection threshold 1 Initialize all morphogen concentrations to zero;
θ axon ∈G axon . 2 for each time step t=1 to G iter do
6) WeightAssignment:Whentheconnectionisformed,an 3 for each cell c in Λ do
initial synaptic weight w is assigned from morphogen 4 Secrete morphogens at c according to genome
concentration c at the target neuron and the distance rules;
d between the source neuron and target neuron. The
5 for each morphogen m do
weight calculation is:
6 Diffuse m across Λ using diffusion matrix
(cid:18) (cid:19)
c D(m);
w =max 0.01,
1+d
7 for each position (i,j) do
The weights range between 0.01 and 1.0. The minimum 8 Apply inhibition between morphogens at (i,j);
weight of 0.01 ensures that the edge exists, while a weight of
9 for each cell c in Λ do
0 indicates that no edge exists in the model.
10 if c is a progenitor then
Once edges are established, weights can change based on
11 if division conditions are met then
competitivescalingofthelocalconcentrationsofmorphogens.
12 Divide progenitor and place new
Let c be the concentration of morphogen at the location of
local progenitor in adjacent position;
a neuron, while c is the total concentration of morphogen
total 13 else if differentiation conditions are met
in the local neighborhood of this neuron. The new weight can
then
be calculated as:
14 Differentiate progenitor into neuron;
(cid:18) (cid:19)
c
w =max 0.01, local 15 else if c is a neuron then
new c
total 16 Grow axon toward local morphogen
This creates a competitive situation where even relatively gradient;
distant neurons with a high density of the axon-guiding 17 if axon reaches another neuron and
morphogenwillcreatemoreconnectionstootherneuronsthan connection threshold is satisfied then
close neurons with a low density of morphogen. 18 Form connection and initialize synaptic
weight;
C. Termination
ThiswillcontinueforG iterationsuntilthedevelopmental
iter 19 return final developmental state and neural network
process is terminated.
Thus,theMorphoNASsystemsimulatestheneuraldevelop-
mentoverdiscretetimesteps;whereeachlocationinthedevel- Second, biological systems rarely evolve toward exact single
opment field contains morphogen concentrations and possibly structure [25], rather exploring general properties that ensure
a cell (either progenitor or neuron). The system evolves step- functionality and robustness [26].
wiseandcontinuestheprocedurethroughsecretion,diffusion,
While in the proposed MorphoNAS one cell naturally self-
inhibition, cell fate decisions, and axon growth, according
organizes into complex graph-like structures, each genome
to local interactions and development rules encoded in the
deterministically defines a single graph. Evolutionary Algo-
genome. Beginning with only one progenitor cell, the system
rithms (EAs) offer a powerful framework to explore the space
self-organizes into a neural network through morphogen and
of possible genomes, which will let us select for those that
chemical gradient signaling. The complete process is detailed
guide MorphoNAS process to form graphs that meet specific
in Algorithm 1.
properties.
V. EVOLUTIONARYTARGETINGOFSELECTEDGRAPH
A. Problem definition
PROPERTIES
To evaluate the generative capacity of MorphoNAS without
Having formally defined the MorphoNAS framework, pa-
relying on exact graph matching (isomorphism), let’s define a
rameterized with RD system characteristics and a set of
toy problem that specifies the following global properties of
simple GRN rules triggered solely by morphogen thresholds
the resulting graph:
appliedlocally,let’sassessitspotentialtoproducegraphswith
predefined topological properties. • Number of nodes: The resulting graph should contain
For this assessment a full isomorphic equality to a target exactly N nodes (neurons)
target
graphisnotfeasible.First,despiterecentdevelopments,graph • Number of edges: It should contain exactly E target edges
isomorphism is still a difficult computational problem [24]. (synaptic connections)
• Number of sources: another easy-to-count property of
the target graph is number of nodes with zero in-degree;
the number of such nodes should match S
target
While the toy problem is limited to these 3 properties,
other potential metrics could be explored in future research.
Examplesofsuchproblemssolvableinpolynomialtimecom-
Algorithm 2: Genetic Algorithm for Evolving Mor-
pared to more complex graph isomorphism problem include
phoNAS
mean shortest path length, graph density, presence of a giant
Data: Initial population of P genomes with fixed
component, and others.
random seed
B. Fitness Function Design Result: Best evolved genome matching target graph
properties
Now we can define a fitness function that will represent
1 Initialize population with random genomes;
how closely the grown graph matches the target properties:
2 Evaluate fitness of all individuals in parallel;
F(G)= exp (cid:18) −w · |N(G)−N target | (cid:19) 3 while generation <G max and not converged do
N tol N 4 Select P parents parents using tournament selection
(cid:18) (cid:19) (tournament size T );
|E(G)−E | size
×exp −w E · tol target 5 Compute convergence ratio ρ=f avg /f best ;
(cid:18) E (cid:19) 6 Update mutation multiplier
|S(G)−S |
×exp −w S · target µ=µ min +(µ max −µ min )×ρ;
tol
S 7 for P replace offspring do
×P(G) 8 Randomly select two parents;
9 if parents have same morphogen count then
whereN(G),E(G),S(G)aretheactualnumbersofnodes,
10 Inherit simple parameters randomly;
edges, and sources; tol ,tol ,tol are tolerances that specify
N E S perform element-wise crossover on
the degree of deviation; w ,w ,w are weights indicating
N E S secretion rates; perform row-wise
how much importance to place on each term.
crossover on inhibition matrix; inherit
P(G) is a penalty for lack of weak connectivity defined as
diffusion patterns from one parent;
(cid:40) 1, if G is weakly connected 11 else
P(G)= 12 Randomly choose one parent’s morphogen
γ ≤1, otherwise
count and associated parameters;
where γ is a penalty factor, e.g. .1 or .5; if γ=1 then there randomly inherit other parameters;
is no penalty. 13 With probability p mut ×µ, apply mutation;
Some key features of the fitness function are: it returns a 14 if mutation occurs then
valueof1whenthereisanexactmatchtotargetvalues;small 15 Select mutation type according to
deviations result in small reductions in fitness; as deviations probability vector v : grid size mutation
mut
increase, the penalties catch up exponentially. Finally, if the (±3), growth steps mutation (±20),
graph is not weakly connected, its fitness is multiplied by the parameter mutation (90% scale float by
factor γ making it more difficult for disconnected graphs to [0.5, 2.0], 10% increment integer by ±2),
survive morphogen number mutation (±2), or
matrix mutation (modify inhibition or
C. Genetic Algorithm Setup
diffusion patterns), 5% chance of radical
For the experiment setup, a steady-state genetic algorithm mutation
[27] is employed.
The initial population consists of P individuals, each 16 Preserve P elite elite individuals;
17 Replace P replace individuals with new offspring;
having a genome set up as defined in chapter III-A. All
18 Evaluate fitness of all new individuals (use fitness
initial individuals define developmental field of the same size
cache);
(L ,L ),thesamenumberofmorphogensN ,andthesame
x y M 19 Update best fitness and convergence statistics;
maximum of G CA growth iterations. A fixed random seed
iter
ensured reproducibility of initial population. 20 return Best evolved genome
Parent selection is being performed using tournament
selection [28] with a tournament size of T . The algorithm
size
selection pressure is set to p for the entire run. As part
sel
of steady-state setup, P individuals get replaced in each
replace
generation. Elitism [27, p. 101] is used to preserve top P
elite
individuals without modification.
Crossover applies to the selected parents by randomly TABLEI
combining simple parameters from each genome. Morphogen SUMMARYOFTARGETEDGRAPHPROPERTIESEXPERIMENTSETUP
secretionratesaremixedusingelement-wisecrossover,inhibi-
InitialGenomeParameters
tionmatricesarerecombinedrow-wise,anddiffusionmatrices
Symbol Value Description
are inherited in whole from one parent. When parents differ
(Lx,Ly) (20,20) Developmental field dimensions (width ×
inmorphogencount,theentiremorphogen-specificparameters height)
set is inherited by each offspring from one randomly selected NM 3 Numberofmorphogens
parent, keeping other parameters mixed from both parents. G iter 200 Maxgrowthiterations
Mutation is applied with p probability to each produced GeneticAlgorithmParameters
mut
P 2000 Populationsize
offspring, selected randomly according to v probabilities
mut
vector from five mutation types: developmental field size mu- T size 7 Tournamentsize
Pparents 300 Parentspergeneration
tation,growthstepsmutation,parametermutation,morphogen
P replace 200 Replacedindividuals
number mutation, and matrix mutation. Parameter mutations
P elite 10 Eliteindividualspreserved
target either floating-point parameters, scaling them by a Gmax 1000 Maxnumberofgenerations
random factor between 0.5 and 2.0 (90% of cases), or integer pmut 0.4 Baselinemutationprobability
parameters, changing them by ±1 (10% of cases). µ min 1.0 Minimummutationmultiplier
A ”radical mutation” with 5% chance is also introduced: µmax 2.5 Maximummutationmultiplier
[0.15,
when it occurs, the number of morphogens set to any in
0.15,
[3..13] range which results in all morphogen-related param- 0.45,
eters randomized; the G growth steps set to random ones 0.10,
iter
in [100..1000] range. vmut 0.15] Mutation type probabilities (grid size, steps,
param,morphogens,matrix)
An adaptive mutation strategy was used for balancing
p rad 0.05 Probabilityofradicalmutation
exploration and exploitation. The effective mutation rate was
ExperimentalSetupParameters
scaled dynamically according to the population’s convergence
K 20 Numberoftargetpropertysets
ratio,definedastheaveragefitnessdividedbythebestfitness. R 20 Evolutionaryrunspertarget
The mutation rate multiplier µ was computed as follows: k min 1.5 Minavg.out-degree
dmax 0.3 Maxgraphdensity
µ=µ +(µ −µ )×ρ seedtarget 54210 Seedfortargetgraphgeneration
min max min
seedruns 65420 Seedforruninitialization
where µ and µ are the minimum and maximum
min max
scaling factors, respectively, and ρ represents convergence
ratio defined as the ratio of the average fitness f to the Performance of each run is defined by the best fitness
avg
best fitness f in the population: ρ= favg. recorded in that run, calculating the mean and standard de-
In each exp be e s r t iment run, the evolution f p bes r t ocess was running viation, while reporting the proportion of runs that achieve
for up to G generations or until convergence. the fitness threshold.
max
Table I contains the specific parameters for the experiment.
D. Assessing the Suggested Evolutionary Framework
Table II shows the success rates of the evolutionary algo-
To assess the suggested evolutionary framework, we will rithmon20differenttargetconfigurations,whereeachconfig-
defineK targetpropertiessets{N ,E ,S }todefine uration was defined by a triplet {N ,E ,S }, defin-
target target target target target target
the desired number of neurons, edges, and sources per set of ingthesearchednumbersofneurons,edgesandsourcenodes.
target properties. Success rate is the number of runs (out of R) that achieved
The target properties are created by sampling of K random a perfect fitness (the evolved graph specifically matched the
directed graphs created with Erdo˝s-Re´nyi G(n,m) method properties that we required in our target configuration for the
[29] with a fixed seed . The n and m parameters are graph).
target
sampledrandomlywhileensuringthereisaminimumaverage It is important to note that at least one valid solution was
outdegree k¯ ≥k and a maximum density of d . found for each target configuration, indicating the generality
out min max
For each target properties set, R independent evolutionary and robustness of our framework given a wide array of
runs were performed, each initialized with a different ran- structural constraints. Several target sets (K06, K17, K19)
dom seed, passed to Algorithm 2 represented in pseudocode did have lower success rates, but the algorithm still found a
above. For the seeding process we create a sequence s = suite of valid successful solutions. This means that while the
runs
{s ,s ,...,s },builtwithanotherfixedseed toalloweach degreeofsearchcomplexityhasrelativelyincreased,ithasnot
1 2 R runs
of the K target properties set reuse this same sequence. Each failed the representational capability of genes, i.e. it has still
of the evolutionary runs will be executing until a maximum been confirmed that these target configurations do have their
number of generations G is reached, or until the fitness respective genomes, despite they were found harder to locate
max
threshold has been achieved, or until the average population for the EA. In addition, several configurations (K01, K05,
fitness has converged to at least 95% of the best fitness. K07) reliably produced 100% of representations indicating
TABLEII
SUCCESSRATESACROSSTARGETSETSSAMPLEDFROMRANDOMGRAPHS.
TargetSet Ntarget Etarget Starget SuccessRate(R=20)
K01 11 30 1 1.00
K02 27 75 2 1.00
K03 14 42 1 0.95
K04 8 14 1 0.95
K05 22 92 1 1.00
K06 30 232 0 0.25
K07 31 124 1 1.00
K08 27 103 2 1.00
K09 19 87 0 0.95
K10 20 54 2 1.00
K11 10 18 1 0.80
K12 14 76 0 0.60
K13 9 35 0 1.00
K14 17 34 1 1.00
K15 18 119 0 0.80
K16 31 87 2 1.00
K17 23 223 0 0.40
K18 27 142 1 1.00
K19 28 189 0 0.35
Fig.4. Finalstateofthegrowngraphatstep215fortargetK16(R07),shown
K20 15 32 1 1.00
ina2Dprojectionofthetoroidaldevelopmentalfieldtopology.
Giter: 215 Lattice Size: 20 x 15
DM0 DM1 DM2
0.062 0.350 0.000 0.046 0.181 0.049 0.168 0.159 0.055
0.154 0.068 0.075 0.141 0.093 0.051 0.100 0.159 0.055
0.005 0.105 0.181 0.170 0.096 0.174 0.019 0.117 0.169
Inhibition: Gfates:
M0 M1 M2 Division: 0.599
M0 0.000 0.000 0.292 Differentiation: 0.570
M1 0.000 0.000 0.000 Axon Growth: 0.278
M2 0.000 0.000 0.000
Secretion Rates: Gaxon:
Progenitor: 0.429, 0.424, 0.104 Axon Connect: 0.753
Neuron: 0.158, 0.441, 0.183 Max Axon Length: 10
Fig.3. GenomeofasuccessfulindividualfortargetK16(R07).
the ease of convergence and the strength of the evolutionary
algorithm stabilizing in those configurations.
In figure 3, we present the genome G of a competent
individual that was evolved to target K16 (run R07); K16
specifiesagraphwiththefollowingproperties:31neurons,87
edges,and2inputs.Thesmallparametersetrepresentsgrowth
dynamics,morphogenactions,cellfatesandaxonconnectivity Fig.5. EvolvedgraphfortargetK16(R07)
in a 20×15 developmental field.
This genome defines three morphogens, each with three
distinct diffusion profiles: (D ,D ,D ), secretion rates evolutionary-developmental processes described here.
M0 M1 M2
for progenitor cells and neuron cells, and one non-zero cross- Figure 5 provides the connectivity structure of the evolved
inhibition term. Cell behaviours are defined by thresholds in graph that satisfies the K16 target specification. Each node
G for division, differentiation, and axon initiation. Axonal represents a neuron, each directed edge is associated with a
fates
growth is managed by two parameters in G : probability of networkaxonconnection.Thetwosourcenodes,shownatthe
axon
connection, and maximum axon reach. bottom of the image serve as input neurons to the rest of the
In Figure 4, we show the final grown graph state (step network and they have outgoing connectivity to other neurons
215) in a 2D projection based on the toroidal topology of inthenetwork.Overall,thetopologyisacomplexbutcoherent
the developmental field. wiring defined solely by the developmental process and the
For such a low dimensional structure, this genome has spatial and chemical constraints of the developmental field.
produced a valid offspring structure (neural graph) that The graph is the direct result of the morphogen driven growth
has functional compliance (i.e. complex K16 specification), process as illustrated in the previous figure in the toroidal
thereby illustrating the expressive power and generality of the developmental field.
E. Discussion AftercompletingG iterations,themorphogeneticprocess
iter
yields a directed weighted graph G =(V,E,W), where: V
The experiments showed that the proposed MorphoNAS
isthesetofneurons(nodes);E ⊆V ×V isthesetofdirected
Framework is capable of achieving its fundamental goal of
edges (axon connections); and W :E →[0.01,1.0] assigns a
defining simple RD- and GRN-only based genomes that yield
synaptic weight to each edge.
neural networks with precise and specific structural targets.
This graph defines a Recurrent Neural Network (RNN)
The fact that successful solutions were obtained across all
structure on which it’s required to deterministically define
20 target configurations highlights not only the viability of
input and output neurons, applying propagation dynamics
the methods incorporated into this process, but also their
afterwards.
generality for exploring different architectural constraints.
EvolutionaryEffectiveness:Thevariabilityinsuccessrates
A. Defining Input and Output Neurons
across targets (from more complex targets such as K06, K17,
First,let’sdefinetwosetsofinputandoutputneuronsV ⊆
K19 compared to a flat out 100% successful solutions like in
V and V ⊆ V. The number of such neurons is determined
K01, or K05) is aligned with the inherent complexities of the out
by the dimensionality of the problem d and d .
target specifications, and is not a fundamental limitation of in out
To make the neuron selection process deterministic, input
the framework. The fact that valid solutions could be found
neurons are assigned as the first d nodes of the grown graph
across all target configurations suggests that the proposed in
sorted by nodes’ in-degree deg−(v):
evolutionary-developmental framework has been able to ex-
plore the developmental programs effectively.
V ={v ,v ,...,v }⊂V ,
Developmental Expressiveness: The successful evolution
in 1 2 din sorted
on the example of target K16 (31 neurons, 87 edges, 2 where V sorted =sort(V,by deg−(v)).
source nodes) demonstrates the expressiveness of the pro- Independently, the last d nodes are selected from the
out
posed compact representation of a genome. The expressive original grown graph order of V:
power emerges from a low-dimensional parameter represen-
V ={v ,...,v }.
tation of the morphogen dynamics, cellular behaviours, and out |V|−dout+1 |V|
axonalgrowthrules,producingareasonablycomplexnetwork The following constraints are applied in the process:
throughself-organizeddevelopmentalprocesses.Thissuggests
thattheproposedevolutionary-developmentalprinciples,based • d in +d out ≤|V|, so the emergent graph must be at least
size of the problem’s input plus output dimensions, and
on biological systems, can be useful generative models for
neural architecture search. • V in ∩ V out = ∅, i.e. input and output neurons do not
intersect.
Scalability Indicators: The ability of the framework to
tackle varied target specifications which differed significantly B. RNN Propagation Dynamics
in terms of not only scale, but also network connectivity den-
Now, we will formalize the flow of information over the
sity and structural complexity, suggest scalability is likely to
emergent directed graph. The graph is intended to be seen as
beapromisingarea.Additionally,thespatialorganizationthat
a Recurrent Neural Network (RNN), in which neurons act as
emerges from morphogen-guided growth provides mechanism
processing units and axon connections determine how infor-
able to create structured connectivity patterns, which could be
mation flows among neurons in the network. The propagation
beneficial for more complex control tasks.
dynamics are modeled after [30]. The flow of information
Methodological Foundation: These results provide evi-
progress through the network is discrete over a series of time
dence that the proposed process is capable of consistently
steps and can be computed with matrix multiplications along
converting high-level architectural specifications into working
with non-linear activation functions, over the graph structure.
developmental programs. This capability forms a method-
Let
ological foundation for functional validation that will be
x ∈ R|V| be the initial state of activation of the neurons,
demonstratedinthenextexperimentdescribedbelow,inwhich 0
instantiated as follows:
the evolved networks must both develop to achieve target
structural properties and perform functional computations. • Inputs are injected into the appropriate input neurons
V ⊆V, as defined above.
The broad success across target configurations confirms in
that the proposed evolutionary-developmental framework can • All other neuron activations are initiated to the value of
zero.
be regarded as a viable mechanistic approach for automated
neuralarchitecturegeneration,andisnowreadytotacklemore W ∈ R|V|×|V| be the sparse weight matrix constructed
advanced functional optimization problems. from synaptic weights W of the original graph:
(cid:40)
W(i,j) if (i,j)∈E
VI. APPLYINGGROWNRNNSFORPROBLEMSOLVING W =
ij
0 otherwise
Let’s describe how the graphs grown using the proposed
MorphoNAS framework can be applied to concrete problem- f : R → R be the neuron activation function (take, for
solving tasks. example, tanh), which would be computed for each element.
Let us now define the propagation of the network over T andthisscoreisnormalizedtogiveconsistentfitnessfunction
discretetimesteps(whereT isthediameterofG,andoptional outputs.
additional timesteps are allowed): The evaluation procedure can be executed on Gymnasium
For each time step t≥0: environments such as CartPole, LunarLander, MountainCar,
and others. Each environment defines a predefined passing
x =Update(x ,f(Wx ))
t+1 t t score which helps to perform normalization of the actual
received score into a fitness function value.
where Update can operate in either of two modes: accu-
mulation mode implies x =x +f(Wx ) or replacement
The environment defines an observation space O ⊆ Rdin
mode implies x =f(W
t+1
x ).
t t and an action space A: either continuous Rdout or discrete
t+1 t {0,1,...,k−1}dout.
This mode of propagation has several characteristics. First The evaluation process will be defined for N independent
of all, the process is completely deterministic and follows the rollouts (episodes). For each rollout:
structureofthegrowngraph.Also,animportantfeatureisthat 1) Input and output neurons are defined in the RNN,
the network can be run in a computationally efficient manner as described in section VI-A above. The network may
on both CPUs and GPUs, considering the sparsity of the be so small it will trigger a fitness score of 0:
dynamics.Atthesametime,thesystemsupportsbothdiscrete
F(G)=0, if d +d >|V|
and continuous input and output modalities. All this leads to in out
the sustained state x changing in relation to the network
t 2) Inject observations. At time step t, the environment
topology and its weights, allowing for rich and expressive
provides observation o ∈ O. The input neurons are
t
dynamic behavior.
initialized as x [v ] = o [i], for each v ∈ V while
0 i t i in
all other neurons are set to zero.
VII. EVOLVINGRNNCONTROLLERSFORGYMNASIUM
3) Propagate.Thenetworkpropagatesasdefinedinsection
ENVIRONMENT
VI-B: x =Propagate(x ,W,T).
t+T 0
A. Problem Setting 4) Extract an action. The action a is extracted by the
t
To confirm functional completeness, we will use the stan- output neurons a t [i] = x t+T [v i ], for each v i ∈ V out .
dard CartPole task from the Gymnasium environment [31] as Then depending on whether the action space is con-
themostsimpleRLstandardbenchmark.InCartPole,thegoal tinuous or discrete, some postprocessing may be done:
is to balance a vertical pole on a moving cart by applying for discrete action spaces the actions are decided by
a force to the left or right. The state of the system can be argmax or discretization, while for continuous action
described by 4 numbers: cart position, cart speed, pole angle, spacestheactivationsshouldbescaledtothevalidaction
and pole angular velocity. The agent can choose between just boundaries.
two discrete actions: apply left force and apply right force. 5) Step the environment. Apply action a t , receive reward
The best (ideal) reward is 500 in this environment. r t , and get the next observation o t+1 .
The recurrent neural network (RNN) grown with Mor- 6) Accumulate rewards. Finally, the rewards are accumu-
phoNAS method will act as the cart controller. It accepts lated across all steps in the episode:
environtment state at its input, using the 4 input neurons V in , T(i)
episode
and generates actions to stabilize the CartPole system using R(i) = (cid:88) r(i)
the 2 output neurons V . t
out
t=0
The average reward across N episode rollouts is:
er
R¯ = 1 (cid:88)
Ner
R(i)
N
er
i=1
The average reward R¯ is then converted to a fitness score
F(G)∈[0,1] using a sigmoid transformation:
1
F(G)= ,
1+e−k(R¯−S)
Fig.6. CartPolefromtheGymnasiumenvironment
where S is the passing score for the given environment and
k > 0 is the scaling factor that controls the slope of the
B. Fitness Function Design
sigmoid.
Let’sdefineafitnessevaluationapproachabletoworkwith
C. Penalizing Excessive Networks
any standard environments from the Gymnasium library [31].
The fitness function will determine how well a RNN grown Let’s add an optional penalization to the fitness function so
by MorphoNAS performs on a set of random Gymnasium we can penalize excessive networks. It will cause the fitness
episodes. The performance a series of episodes is averaged todecreasedowntothevalueofα(e.g.0.8)asthenumberof
TABLEIII TABLEIV
EXPERIMENTSPARAMETERSDIFFERENCE NUMBEROFRUNSREACHINGTHEIDEALCARTPOLEFITNESSVALUEWITH
APOPULATIONSIZEOF500
RNN-Controller
Symbol TargetingGraphs RNN-Controller (min) TimeToIdealFitness NumberofRuns
F(G) TargetGraphFitness CartpoleFitness CartpoleFitness Initialpopulation(0generations) 94
(Lx,Ly) (20,20) (10,10) (10,10) 1generation 3
P 2000 500 500 4generations 1
Pparents 300 100 100 5generations 2
P replace 200 50 50
pmut 0.4 0.3 0.3
ExcessiveNetworksPenalization
α – – 0.8
θC – – 50
θ half – – 1000
connections increases. Let N be the number of connections
C
in the RNN, θ be the maximum number of unpenalized
C
connections, and θ be the half-decay threshold (that is, the
half
point where the fitness falls halfway between 1 and α, i.e. to
0.9 if α=0.8). The decay rate is defined as λ= ln(2) .
θhalf−θC
The connection penalization factor can then be defined as
P (N )=α+(1−α)×exp(−λ×max(0,N −θ )).
conn C C C
And the penalized fitness function is then computed as Fig.7. SizesofRNNnetworkssuccessfullyoperatingCartPolefoundwithin
ExperimentB1
F (G)=F(G)×P (N ).
eff conn C
This discourages graphs with more than θ edges as it
C 10x10sizeusedinthisexperiment.FortheCartpolecontroller
imposes a penalty on the fitness function which asymptotes
RNN,largenetworks(over90neurons)wereobservedin89%
back towards α as the number of connections increase.
of populations.
D. Genetic Algorithm Setup In the RNN-Controller (min) version, we added a penalty
into the fitness function to find smaller RNNs that could con-
The setup for the evolution of RNN solving Gymnasium
trol CartPole with smaller numbers of nodes and connections.
environments closely follows the parameterization described
ThisshiftedthedistributionoffoundRNNCartPolecontroller
in Table I for the Targeted Graph Properties experiment.
networks with ideal fitness. The comparison is shown on the
The main difference lies in the fitness function, where the
table.Intriguingly,39%ofallrunsinthisexperimentalversion
Gymnasium environment is employed as discussed in chapter
yielded 6-neuron RNNs with ideal fitness when controlling
VII-B.OtherparametermodificationsareprovidedinTableIII.
CartPole while 33% yielded 7-neuron RNNs.
E. Evaluation of Results The full distribution of RNN sizes for the RNN-controller
For the first RNN-Controller experiment, a fixed seed, (min) is shown in Figure 8. The values correspond to the
65420, was used to generate s sequence of 100 seeds to number of evolutionary runs that had valid solutions. The
runs
randomize each run. An independent evolutionary run was heatmap shows that the compact solutions were not only oc-
launched for all seeds in s . curring frequently, but also demonstrated consequtive patterns
runs
It is worth to note that the ideal fitness for the CartpoleFit- of evolutionary convergence, having highest concentration of
ness function was found in the initial generation for 94 out of successful solutions with 6 and 7 neurons. This clustering
100runs;thatis,noevolutionarystepsweretaken.Theresults indicates that the network size-penalized fitness function pro-
are summarized in Table IV, outlining number of generations vides enough evolutionary pressure to minimize the capacity
to reach optimal fitness in all runs.
In these 100 runs, the resulting RNNs contained fewer than
10 neurons in 8 cases, between 34 and 70 neurons in 3 cases, TABLEV
SUMMARYSTATISTICSOFRNNCARTPOLECONTROLLEREXPERIMENTS
andexactly100neuronsintheremaining89cases.Itisshown
on Figure 7.
RNN-Controller
Forsmallersearchspaces,suchastheCartpoletaskfromthe Metric RNN-Controller (min)
Gymnasium suite, the MorphoNAS-based generation process Smallnetworks(≤8neurons) 8% 78%
typicallyyieldsRNNsachievingtheidealfitnesswiththemax- Mediumnetworks(9-50neurons) 3% 20%
Largenetworks(≥90neurons) 89% 2%
imum number of neurons, 100, for the developmental field of
for continuous physical control and partially observable envi-
ronments, will help supply a rigorous experimental platform
capable of providing sufficient assessment of the method’s
outputcapacitybeyondthisinitialanddescriptiveverification.
The construction of both maximal and minimal viable
architectures suggests that MorphoNAS-based processes can
reliable grow functional neural networks within known solu-
tion spaces, thus laying the groundwork prior to entering and
addressing much more complex problems.
VIII. CONCLUSION
In conclusion, this paper has established that MorphoNAS
can produce neural networks with specified structural spec-
ifications and functional capabilities. The proposed biologi-
callyinspiredalternativeenhancesstandardneuralarchitecture
search by combining the Free Energy Principle, reaction-
Fig.8. Distributionofsuccessfulevolutionaryrunsacrossnetworksizeand
diffusion systems, and gene regulatory networks into one
generation count. The values correspond to the number of runs resulting in
findingavalidsolution. computational approach.
TheexperimentonGraph Targeting,inparticular,demon-
strated the generality of the system and produced valid solu-
of architecture while it is still able to accomplish the same
tionsacrossrandomtargetconfigurations.Despitesomeofthe
function.Itisinterestingtonotethatmostsuccessfulcompact
configurations were more difficult to search than others, the
solutionsrequiredadditionalselectionpressuretoyieldsmaller
successful solutions were found for all of them, showing po-
neural networks.
tential for vaibility of the method across various architectural
constraints.
F. Discussion
TheRNN-controllerexperimentillustratedfunctionalcom-
The results of RNN Controller experiment success- petence of the system; where, at the population level, 94%
fully demonstrate effectiveness of the proposed MorphoNAS of the random populations contained networks that could
method to produce functional control networks. High success successfully solve the CartPole task, demonstrating effective
rate, when 94 of 100 runs achieve optimal CartPole controller architecture sampling. At the same time, when network size
solustionsintheinitialpopulation,suggeststhatthegeneration penalties were added to the fitness function, evolution went
process effectively chooses viable network architectires. on to find smaller and no less effective controllers where
It’simportanttopointoutthatOlleretal.[32]havedemon- percentageofsolutionshavingcompactarchitecturewithonly
stratedthatover3%ofrandomlyweightedneuralnetworksare 6-7 neurons was 72%.
able to operate CartPole successfully, meaning that CartPole Integrating relevant principles from developmental biology
is a relatively simple control task. However, this does not and evolutionary computation lays the groundwork for auto-
affect the evaluation of the proposed experimental design: mated generation of neural architectures. The key conclusion
CartPole serves as a suitable proof-of-concept environment to is that simple morphogen- and GRN-based rules can create
test whether the MorphoNAS method can generate functional complex neural architecture through self-organization.
architectures, before we go up the complexity ladder. In addition, this work demonstrates how morphogenetic
Key Findings: Our method demonstrates the ability to systemscangeneratefunctionalcomputationalstructuresusing
evolve different architectures depending on selective pressure. a reduced set of biologically inspired developmental rules. It
To underline, the initial RNN Controller experiment evolved may also yield insights on the natural processes that occur
functionallyviablecontrolnetworksthatfrequentlyhadsizeof in development of the neural tissue, and could potentially be
100 neurons, maximum possible for the given developmental beneficial to models in computational neuroscience.
field size, while in the size penalised RNN Controller (min) While the validation was limited to simple control tasks
experiment,72%ofsuccessfulrunsevolvedacontrolnetwork withrelativelysmallnetworks(≤100neurons),theprinciples
with 6-7 neurons, i.e., demonstrating that the size of the of spatial organization in the proposed framework promise
architecture was explicitly being considered by MorphoNAS. scalabilitypotential.Futureworkwouldengagecomputational
Correspondingly, there is a trade-off between solution quality optimization, expand the researched tasks to include more
and efficiency of architecture that the evolutionary process is complex problem domains, and investigate hierarchical devel-
clearly able to adapt to. opmental processes for larger target architectures.
The emergence of compact, functional networks under By incorporating plasticity mechanisms at development
selective evolutionary pressure indicates strong scalability time or after development, MorphoNAS may be capable of
prospects. The planned future experimental applications that producingnetworksthatcouldthenundergofurtheradaptation,
involve more complex control tasks, including separate tasks potentially yielding a closer resemblance to biological neural
systems. Also, a systematic study that compares MorphoNAS [18] SelenAtasoy,IsaacDonnelly,andJoelPearson. Humanbrainnetworks
to the established neural architecture search methods would function in connectome-specific harmonic waves. Nature Communica-
tions,January2016.
also be beneficial to clarify strengths, weaknesses, and appro-
[19] A. Safron. An Integrated World Modeling Theory (IWMT) of Con-
priate domains of application. sciousness: Combining Integrated Information and Global Neuronal
As AI systems become more complex, bio-inspired devel- Workspace Theories With the Free Energy Principle and Active Infer-
enceFramework;TowardSolvingtheHardProblemandCharacterizing
opmental approaches may play significant role to discover
AgenticCausation. FrontiersinArtificialIntelligence,June2020.
neural architectures that maintain effectiveness, adaptability, [20] DongliangZhang,ChenghaoZhang,Q.Ouyang,andY.Tu.Freeenergy
and robustness of biological neural systems. dissipation enhances spatial accuracyand robustness of self-positioned
Turing pattern in small biochemical systems. Journal of the Royal
This preliminaryvalidation marksMorphoNAS asan excit-
SocietyInterface,July2023.
ingapproachinneuralarchitecturesearchtoolbox,andrestates [21] Rene´Thomas.Booleanformalizationofgeneticcontrolcircuits.Journal
a principled pathway in pursuit of creating more biologically ofTheoreticalBiology,December1973.
[22] A. Polynikis, Stephen John Hogan, and M. di Bernardo. Comparing
plausible artificial neural networks.
different ODE modelling approaches for gene regulatory networks.
JournalofTheoreticalBiology,December2009.
IX. CODEANDDATAAVAILABILITY [23] Thomas R. Sokolowski, Thomas Gregor, William Bialek, and Gasˇper
Tkacˇik. Deriving a genetic regulatory network from an optimization
The source code for MorphoNAS, including all experi- principle.,January2025.
[24] La´szlo´ Babai. Graph isomorphism in quasipolynomial time. In
mental setups and evolved genomes, will be made publicly
Proceedings of the forty-eighth annual ACM symposium on Theory of
available at https://github.com/sergemedvid/MorphoNAS fol- Computing,STOC’16,pages684–697,NewYork,NY,USA,June2016.
lowing publication. The repository will include: complete AssociationforComputingMachinery.
[25] GeraldM.EdelmanandJosephA.Gally.Degeneracyandcomplexityin
MorphoNAS framework implementation, evolutionary algo-
biological systems. Proceedings of the National Academy of Sciences,
rithm code, experimental configurations and seeds for repro- 98(24):13763–13768, November 2001. Publisher: Proceedings of the
ducibility, example evolved genomes and visualizations. NationalAcademyofSciences.
[26] Joanna Masel and Meredith V. Trotter. Robustness and evolvability.
Trends in genetics : TIG, 26(9):406–414, September 2010. Place:
REFERENCES England.
[27] KennethAlanDeJong.Ananalysisofthebehaviorofaclassofgenetic
[1] Anthony M. Zador. A critique of pure learning and what artificial adaptivesystems. PhDthesis,UniversityofMichigan,1975.
neuralnetworkscanlearnfromanimalbrains. NatureCommunications, [28] David E. Goldberg and Kalyanmoy Deb. A Comparative Analysis of
10(1):3770,August2019. Publisher:NaturePublishingGroup. Selection Schemes Used in Genetic Algorithms. In GREGORY J. E.
[2] KarlJ.Friston. Learningandinferenceinthebrain. NeuralNetworks, Rawlins, editor, Foundations of Genetic Algorithms, volume 1, pages
November2003. 69–93.Elsevier,January1991.
[3] A. M. Turing. The Chemical Basis of Morphogenesis. Bulletin of [29] P.Erdo˝sandA.Re´nyi.OnrandomgraphsI.Publicationesmathematicae
MathematicalBiology,January1990. (Debrecen),6:290–297,1959.
[4] KarlJ.Friston,M.Levin,B.Sengupta,andG.Pezzulo. Knowingone’s [30] EliasNajarro,ShyamSudhakaran,andS.Risi.TowardsSelf-Assembling
place:afree-energyapproachtopatternregulation.JournalofTheRoyal Artificial Neural Networks through Neural Developmental Programs,
SocietyInterface,April2015. July2023.
[5] J.Gurdon. Thedevelopmentalcapacityofnucleitakenfromintestinal [31] Mark Towers, Ariel Kwiatkowski, Jordan Terry, John U. Balis, Gi-
epithelium cells of feeding tadpoles. Journal of embryology and anluca De Cola, Tristan Deleu, Manuel Goula˜o, Andreas Kallinteris,
experimentalmorphology,December1962. Markus Krimmel, Arjun KG, Rodrigo Perez-Vicente, Andrea Pierre´,
[6] Stuart A Kauffman. The Origins of Order: Self-Organization and SanderSchulhoff,JunJetTai,HannahTan,andOmarG.Younis. Gym-
SelectioninEvolution. OxfordUniversityPress,June1993. nasium:AStandardInterfaceforReinforcementLearningEnvironments,
[7] JohnH.Holland. Emergence:FromChaostoOrder. January1998. November2024. arXiv:2407.17032[cs].
[8] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood from [32] Declan Oller, Tobias Glasmachers, and Giuseppe Cuccu. Analyzing
incompletedataviatheEM-algorithmplusdiscussionsonthepaper. Reinforcement Learning Benchmarks with Random Weight Guessing.
September1977. InProceedingsoftheInternationalConferenceonAutonomousAgents
[9] Karl J. Friston, James M. Kilner, and Lee H. Harrison. A free energy andMultiagentSystems(AAMAS),pages975–982,2020.
principleforthebrain. JournalofPhysiology-Paris,July2006.
[10] KarlJ.Friston.Thefree-energyprinciple:aunifiedbraintheory?Nature
ReviewsNeuroscience,February2010.
[11] Franz Kuchling, Karl J. Friston, Georgi Georgiev, and Michael Levin.
MorphogenesisasBayesianinference:Avariationalapproachtopattern
formation and control in complex biological systems. Physics of Life
Reviews,July2020.
[12] Karl J. Friston. A free energy principle for a particular physics, June
2019.
[13] ChrisFields,KarlJ.Friston,JamesF.Glazebrook,MichaelLevin,Chris
Fields,KarlJ.Friston,JamesF.Glazebrook,andMichaelLevin. Afree
energyprincipleforgenericquantumsystems.,May2022.
[14] Karl J. Friston. Life as we know it. Journal of the Royal Society
Interface,September2013.
[15] Yasunari Miyagi, Y. Mio, K. Yumoto, R. Hirata, T. Habara, and
N.Hayashi. KineticEnergyandtheFreeEnergyPrincipleintheBirth
ofHumanLife. ReproductiveMedicine,May2024.
[16] ErwinSchro¨dinger. Whatislife?:thephysicalaspectofthelivingcell
;&Mindandmatter. Cambridge:UniversityPress,1967.
[17] Le´o Pio-Lopez, Franz Kuchling, Angela Tung, G. Pezzulo, and
M. Levin. Active inference, morphogenesis, and computational psy-
chiatry. FrontiersinComputationalNeuroscience,November2022.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "MorphoNAS: Embryogenic Neural Architecture Search Through Morphogen-Guided Development"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
