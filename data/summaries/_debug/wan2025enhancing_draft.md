Enhancing Personalized Multi-Turn Dialogue with Curiosity RewardThis paper investigates a novel approach to enhancing personalized multi-turn dialogue by incorporating a curiosity-based intrinsic reward mechanism into large language models (LLMs). Existing reinforcement learning from human feedback (RLHF) methods often prioritize helpfulness and safety, falling short of fostering truly empathetic, adaptive, and personalized dialogues. Personalization is not merely a luxury but often an essential requirement for effective conversational agents, particularly in human-centric applications such as education and healthcare. However, conventional RLHF-trained models struggle to maintain consistent personalization across diverse users, neglecting individual user preferences and attributes.The authors propose leveraging a user model to incorporate a curiosity-based intrinsic reward into multi-turn RLHF. This novel reward mechanism encourages the LLM to actively infer user traits by optimizing conversation to improve its user model’s accuracy. Consequently, the agent delivers more personalized interactions by learning more about the user. The paper demonstrates this method’s effectiveness in two distinct domains: significantly improving personalization performance in a conversational recommendation task and personalizing conversations for different learning styles in an educational setting. The authors show improved generalization capabilities compared to traditional multi-turn RLHF, all while maintaining conversation quality.The core idea is to address the limitations of existing approaches by introducing an intrinsic motivation component that actively learns about user types through curiosity. This approach aims to overcome the challenges associated with relying solely on sparse end-of-conversation rewards, which often fail to capture the nuances of multi-turn dialogues. By integrating a curiosity-based reward, the LLM can dynamically adapt its responses to better understand and cater to individual user preferences.The paper introduces a framework called CURIO (Curiosity-driven User-modeling Reward as an Intrinsic Objective) for personalized dialogues. It leverages a user model to actively infer user traits by optimizing conversations to improve its user model’s accuracy. The key insight is that the LLM should learn about the user during the conversation—asking insightful questions to discover the user’s traits. This approach promotes a balance between exploring conversations and learning about the user.The authors detail the technical implementation of CURIO, including the optimization of the reward signal and the integration of the user model. They highlight the importance of maintaining a robust user model to effectively guide the LLM’s interactions.The paper’s findings demonstrate that CURIO significantly enhances personalization performance in multi-turn dialogues, showcasing its potential for creating more engaging and adaptive conversational agents.---Key Terms: reward, enhancing, adaptive, personalized, curiosity, multi-turn, llms, turnrlhf, turn---References[1] Ouyang, Long, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, SandhiniAgarwal, KatieSlama, AlexRay, JohnSchulman, JacobHilton, FraserKelton, LukeMiller, MaddieSimens, AmandaAskell, PeterWelinder, PaulChristiano, JanLeike, andRyanLowe. “Training Language Models to Follow Instructions with Human Feedback.” *Advances in Neural Information Processing Systems*,2022.[2] AnandSiththaranjan, CassidyLaidlaw, andDylanHadfield-Menell. “DistributionalPreferenceLearning:UnderstandingandAccountingforHiddenContextinRLHF.” *arXivpreprintarXiv:2408.10075*,2024.[3] Eileen du Plooy, Daleen Casteleijn, and DeniseFranzsen. “PersonalizedAdaptiveLearninginHigherEducation:AscopingReviewofKeyCharacteristicsandImpactonAcademicPerformanceandOutcomes.” *Heliyon*,2024.[4] AhmetB.Kocaballi,ShlomoBerkovsky,JuanC.Quiroz,LilianaLaranjo,HuongLyTong,DanaReza-zadegan,AgustinaBriatore,andEnricoCoiera. “ThePersonalizationofConversationalAgentsinHealthcare:SystematicReviewofKeyCharacteristicsandImpactonAcademicPerformanceandOutcomes.” *JournalofMedicalInternetResearch*,2019.[5] Sriyash Poddar, Yanming Wan, Hamish Ivison, Abhishek Gupta, and Natasha Jaques. “Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning.” *arXivpreprintarXiv:2408.10075*,2024.[6] JiaxingWu,LinNing,LuyangLiu,HarrisonLee,NeoWu,ChaoWang,SushantPrakash,ShawnO’Banion,BradleyGreen,andJunXie. “RLPF:ReinforcementLearningfromPredictionFeedbackforUsersummarizationwithLLMs.” *arXivpreprintarXiv:2409.04421*,2024.[7] DaiweiChen,YiChen,AniketRege,ZhiWang,andRamyaKorlakaiVinayak. “Pal:Sample-EfficientPersonalizedRewardModelingforPluralisticalignment.” *InInternationalConferenceonLearningRepresentations(ICLR),*2025.[8] YihangSun,TaoFeng,GeLiu,andJunXie. “Premium:LLMPersonalizationwithIndividual-LevelPreferenceFeedback.” *InInternationalConferenceonLearningRepresentations(ICLR),*2025.[9] IdanShenfeld,FelixFaltings,AldoPacchiano,andPulkitAgrawal. “LanguagemodelPersonalizationviaRewardFactorization.” *arXivpreprintarXiv:2503.06358*,2025.[10] Jürgen Schmidhuber. “Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990–2010).” *IEEEtransactionsonautonomousmentaldevelopment*,2010.[11] SatinderSingh,RichardL.Lewis,AndrewG.Barto,andJonathanSorg. “IntrinsicallyMotivatedReinforcementLearning:AEvolutionaryPerspective.” *IEEEtransactionsonautonomousmentaldevelopment*,2010.[12] DeepakPathak,MichaelDDennis,andTrevorDarrell. “Large-ScaleStudyofCuriosity-DrivenLearningbySelf-SupervisedPrediction.” *InInternationalConferenceonLearningRepresentations(ICLR),*2019.[13] ReinHouthooft,MichaelJordan,andStuartJ.Russell. “VIME:VariationalInformationMaximizingExploration.” *InAdvancesinNeuralInformationProcessingSystems(NeurIPS),*2016.[14] PulkitAgrawal,AlexeiA.Efros,andStuartJ.Russell. “Large-ScaleStudyofCuriosity-DrivenLearningbySelf-SupervisedPrediction.” *InInternationalConferenceonLearningRepresentations(ICLR),*2019.[15] PawelLadosz,Leen-KiatSoh,SamDevlin,DanielKudenko,andStuartJ.Russell. “Curiosity-DrivenExplorationbySelf-SupervisedPrediction.” *InProceedingssofthe34thInternationalConferenceonMachineLearning(ICML),*2017.[16] AndrewY.Ng,PhilippMoritz,SergeyLevine,MichaelJordan,andStuartJ.Russell. “Potential-BasedRewardShapingforPOMDPs.” *InProceedings ofthe12thInternationalConferenceonLearningRepresentation(ICLR),*2019.[17] AdamEck,Leen-KiatSoh,SamDevlin,andDanielKudenko. “Potential-BasedRewardShapingforPOMDPs.” *InProceedings ofthe12thInternationalConferenceonLearningRepresentation(ICLR),*2019.[18] NatashaJaques,ShixiangGu,DzmitryBahdanau,JoseMiguelHernandez-Lobato,RichardETurner,DouglasEck. “SequenceTutor:ConservativeFine-tuningofSequenceGenerationModelswithKL-Control.” *InInternationalConferenceonMachineLearning,* pages1645–1654.PMLR,2017.[19] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,ChongZhang,SandhiniAgarwal,KatieSlama,AlexRay,JohnSchulman,JacobHilton,FraserKelton,LukeMiller,MaddieSimens,AmandaAskell,PeterWelinder,PaulChristiano,JanLeike,andRyanLowe. “Training Language Models to Follow Instructions with Human Feedback.” *Advances in Neural Information Processing Systems*,2022.---