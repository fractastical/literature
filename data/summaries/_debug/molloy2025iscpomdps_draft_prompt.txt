=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: ISC-POMDPs: Partially Observed Markov Decision Processes with Initial-State Dependent Costs
Citation Key: molloy2025iscpomdps
Authors: Timothy L. Molloy

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Key Terms: decision, observed, markov, belief, state, partially, processes, dependent, pomdps, initial

=== FULL PAPER TEXT ===

ISC-POMDPs: Partially Observed Markov
Decision Processes with Initial-State
Dependent Costs
Timothy L. Molloy
filter)astheir“state”process.POMDPshavebeengeneralized
Abstract—We introduce a class of partially observed to encompass cost functions (denoted ρ by convention) that
Markov decision processes (POMDPs) with costs that can
areexplicitfunctionsofthecurrentbelief,notjuststatevalue,
depend on both the value and (future) uncertainty associ-
leading to ρ-POMDPs that can also be solved by constructing
atedwiththeinitialstate.TheseInitial-StateCostPOMDPs
(ISC-POMDPs) enable the specification of objectives rela- belief MDPs (cf. [5], [6]). ρ-POMDPs have proved important
tive to a priori unknown initial states, which is useful in when controlling state uncertainty is explicitly an objective,
applications such as robot navigation, controlled sensing, such as in active perception, controlled sensing, or privacy-
andactiveperception,thatcaninvolvecontrollingsystems
based applications (cf. [4]–[7] and [8, Chapter 8]). Never-
to revisit, remain near, or actively infer their initial states.
theless, both POMDPs and ρ-POMDPs, and their associated By developing a recursive Bayesian fixed-point smoother
to estimate the initial state that resembles the standard beliefMDPs,haveMarkoviandynamicsandcostsinthesense
recursive Bayesian filter, we show that ISC-POMDPs can that the current state (or belief) determines the current cost
be treated as POMDPs with (potentially) belief-dependent and evolution of the state (or belief). Initial-state costs are,
costs.WedemonstratetheutilityofISC-POMDPs,including
however, non-Markovian.
their ability to select controls that resolve (future) uncer-
Limited progress has been made in incorporating initial-
taintyabout(past)initialstates,insimulation.
state costs into (ρ-)POMDPs. Notably, [4] aimed to minimize
Index Terms—Markov processes, optimal control,
the entropy of the conditional distribution of the initial state
stochasticsystems.
given (all) measurements at a terminal time by redefining
the belief to be this conditional distribution. However, it is
I. INTRODUCTION
well-known from Bayesian smoothing that the conditional
THEinitialstateofadynamicalsystemoftenhasimportant distribution of the initial state does not have a Markovian
practical significance [1]–[4]. For example, the initial formcomputableviaaBayesianfilterrecursion(cf.[9,Section
positionofavehicleoftencorrespondstoitsowner’sresidence 4.1.1]). The approach of [4] therefore does not lead to refor-
[1]; the initial pose of a robot has enabled safe or recoverable mulations of initial-state cost problems as belief MDPs. More
navigation,pathplanning,andmapping[2],[3];and,theinitial generally, [10] considered non-Markovian costs in partially
configurationofteamsofagentscanenablerecognitionoftheir observedproblemsbygeneralizingtheapproachof[11]devel-
roles or intent [4]. This significance has given rise to partially opedforfullyobservedproblemsthatinvolvesaugmentingthe
observed stochastic optimal control problems with objectives underlyingstatesothedynamicsandcostsbecomeMarkovian.
tieddirectlytoinitialstates,suchastheproblemofcontrolling This approach suffers from the fact that even if the state
a system to hinder inference of its initial state to preserve can be modified so that it is Markovian, it may not have
privacy in networked control systems [1], or the problem of a corresponding Markovian belief. For example, if the costs
controllingasystemtoimproveinferenceofitsinitialstatefor only depend on the initial state (or its uncertainty, as in [4]),
active sensing and perception in target tracking and robotics thentakingthisoriginalinitialstateasanew(static)modified
[2]–[4]. However, a general framework for solving initial- stateleadstoatrivialconstantMarkovianstateprocessbutthe
state objective problems is lacking. We therefore introduce conditionaldistributionofthismodifiedstate(i.e.,theoriginal
and investigate Initial-State Cost Partially Observed Markov initialstate)doesnotitselfhaveaMarkovianformcomputable
Decision Processes (ISC-POMDPs). via a Bayesian filter recursion (cf. [9, Section 4.1.1]).
Partially Observed Markov Decision Processes (POMDPs) ThekeycontributionofthispaperistheintroductionofISC-
have costs that depend on the values of their (current) par- POMDPs with costs that can depend on both the value and
tially observed state, and are typically solved by constructing (future) uncertainty associated with initial states. We establish
equivalent Markov decision processes (MDPs) with the belief thatISC-POMDPswith(arbitrary)initial-statedependentcosts
(i.e., the conditional distribution of the current state given admit reformulations and solutions as standard (ρ-)POMDPs
observed measurements computed via a recursive Bayesian with augmented state processes consisting of the both initial
and current state, and that their associated belief can be
The author is with the CIICADA Lab, School of Engineering, The
computed with a recursive (fixed-point) Bayesian smoother
AustralianNationalUniversity(ANU),Canberra,ACT2601,Australia(e-
mail:timothy.molloy@anu.edu.au) that resembles the standard Bayesian filter. Surprisingly, state
©2025IEEE.Personaluseofthismaterialispermitted.PermissionfromIEEEmustbeobtainedforallotheruses,inanycurrentorfuturemedia,including
reprinting/republishingthismaterialforadvertisingorpromotionalpurposes,creatingnewcollectiveworks,forresaleorredistributiontoserversorlists,or
reuseofanycopyrightedcomponentofthisworkinotherworks.
5202
raM
6
]YS.ssee[
1v03050.3052:viXra
augmentation and recursive Bayesian smoothing have not theprobabilitylawinducedbyµ anditscorresponding
π π
∈P
previously been used to solve initial-state cost problems. expectation being E []. A POMDP (formulated as a belief
µ,π
·
This paper is structured as follows. We introduce ISC- MDP) then involves finding a policy that solves
POMDPs in Section II. We reformulate and analyse ISC- (cid:34) (cid:35)
POMDPs as (ρ-)POMDPs in Section III. We provide simu- inf E (cid:88)∞ γkC(π ,U )
µ,π k k
lations in Section IV, and conclusions in Section V. µπ∈Pπ
k=0
Notation: Random variables are denoted by capital letters s.t. π =Π(π ,U ,Y ), π ∆( ) (4)
k+1 k k k+1 0
(e.g., X), and their realizations by lower-case letters (e.g., x). ∈ X
Y π ,U p(y π ,u )
The probability mass function (pmf) of X is p(x), the joint k+1 | k k ∼ k+1 | k k
pmf of X and Y is p(x,y), and the conditional pmf of X U k =µ π (π k )
∈U
given Y = y is p(xy) or p(xY = y). The expectation of a
given a discount factor γ (0,1) and a cost function
| |
function f of X is E[f(X)], and the conditional expectation C : ∆( ) R that ∈ is the conditional expectation
of f under p(xy) is E[f(X)y] or E[f(X)Y = y]. For a X × U →
of an underlying (current) state-control cost function κ :
| | |
finite set , the set of all probability distributions (or pmfs) R; i.e., C(π ,u )≜E[κ(X ,U )π ,U =u ]=
S k k k k k k k
over is ∆( ). (cid:80)X ×U → |
π (x)κ(x,u ).Incontrast,aρ-POMDP(formulatedas
S S x k k
a be∈liXef MDP) involves finding a policy solving [5]
II. PRELIMINARIES AND PROBLEM FORMULATION (cid:34) (cid:35)
(cid:88)∞
We first revisit (ρ-)POMDPs and introduce ISC-POMDPs. inf E γkρ(π ,U )
µ,π k k
µπ∈Pπ
k=0
A. POMDPandρ-POMDPPreliminaries s.t. π k+1 =Π(π k ,U k ,Y k+1 ), π 0 ∆( ) (5)
∈ X
Y π ,U p(y π ,u )
Let X for k 0 be a discrete-time first-order Markov k+1 k k k+1 k k
k | ∼ |
chainwithfinitest ≥ atespace ≜ 1,2,...,N .Lettheinitial U =µ (π )
X { x } k π k ∈U
state X be distributed according to the pmf π ∆( ) with
compon 0 ents π 0 (x 0 )≜P(X 0 =x 0 ).1 Let the s 0 ta ∈ te X k X evolve d g e iv p e e n nd a en d t is c c o o s u t nt fu f n a c c t t i o o r n γ ρ ∈ : ∆ (0 ( ,1) ) and an ar R b . itr ρ a - r P y O b M e D lie P f s -
according to the state-transition probabilities: X × U →
(5) generalize POMDPs since the cost function ρ can be any
Ax,x¯(u)≜p(X k+1 =xX k =x¯,U k =u) (1) (potentially nonlinear) function of the belief π k whilst the
|
POMDP cost function C is limited to the linear form implied
for k 0 and x,x¯ , with the controls U =u belonging
k by conditional expectation. We propose ISC-POMDPs as an
tothe
≥
finiteset
≜∈
1
X
,2,...,N .ThestateX is(partially)
u k extension of (ρ-)POMDPs with costs that can depend on both
U { }
observed through stochastic observations Y for k 1 taking
k the value and uncertainty associated with the initial state X .
valuesinthefiniteset ≜ 1,2,...,N .Themea ≥ surements 0
y
Y { }
Y aremutuallyconditionallyindependentgiventhestatesX
k k
B. ISC-POMDPs
and controls U , and distributed according to the measure-
k 1
ment probabiliti−es: To introduce ISC-POMDPs, let us consider the possibility
of the controls U for k 0 being given by a policy µ that
Bx(y,u)≜p(Y =y X =x,U =u) (2) k ≥
k | k k − 1 is a (deterministic) function of the measurements and controls
for k 1, x , y , and u . (Yk,Uk − 1) directly, namely, U k =µ(Yk,Uk − 1). Let the set
In ≥ (standar ∈ d X infini ∈ te- Y horizon d ∈ is U counted) POMDPs or ρ- of all such policies be , and let the probability law induced
P
POMDPs, we may consider the controls U k to be given by by a policy µ ∈ P be p µ with corresponding expectation
t a he po s l t i a c t y e µ X π k : g ∆ ive ( X n ) th → e m U ea d su ep re e m nd e e n n ts t Y on k t ≜ he b Y e 1 l , ie Y f 2 , π . k .. a , b Y o k ut o E f µ t [ h · ] e . L st e a t te us X a k ls a o nd de t fi h n e e in th it e ia ( l jo st i a n t t e p X os 0 te g r i i v o e r) n c t o h n e d i i n ti f o o n rm al a p ti m on f
5 an .4 d .1 c ] o a n n tr d ol [ s 8, U C k h − a 1 pte ≜ r 7 { ] U ). 0 , S U pe 1 c , i . fi . c . a , l U ly k ,−l 1 e } t U ( { c k f. = [1 µ 2 π , ( S π e k c ) ti f o o n } r (yk,uk − ξ 1) ( a x va , i x lab ) le ≜ a p t ( t X ime = k x ≥ , 0 X as = ξ k x ∈ ∆ yk ( , X uk ×X 1) ), whe (6 re )
k 0 where the belief π ∆( ) is a conditional pmf, or k 0 k 0 0 k k | −
k
N x ≥ -dimensional vector, with ∈ com X ponents π k (x) ≜ p(X k = for x 0 ,x k . We introduce an ISC-POMDP as the problem
xyk,uk 1) satisfying the Bayesian filter recursion of finding ∈ a p X olicy that solves
−
|
Bx(y ,u ) (cid:80) Ax,x¯(u )π (x¯) (cid:34) (cid:35)
π k+1 (x)= (cid:80) k+1 Bx˜( k y x¯ , ∈ u X )Ax˜,x¯(u k )π k (x¯) (3) inf E µ (cid:88)∞ γk[c(X 0 ,X k ,U k )+ψ(ξ k ,U k )]
x¯,x˜ k+1 k k k µ
∈X ∈P k=0
Π fo ( r π x ,u ∈ ,y X ) and to k den ≥ ote 0 th g e ive fi n lte π r 0 . (3 W ), e an u d se n π o k t + e 1 th = at s.t. X k+1 | X k ,U k ∼ Axk+1,xk(u k ),X 0 ∼ π 0 ∈ ∆( X ) (7)
k k k+1 Y X ,U Bxk+1(y ,u ),
p(y π ,u ) is the denominator in (3). We denote the set k+1 | k+1 k ∼ k+1 k
k+1 k k
ofall(d | eterministicbelief)policiesµ as ,withp being U k =µ(Yk,Uk − 1)
π π µ,π ∈U
P
foragivendiscountfactorγ (0,1)wherec: R
1Note that π0 ∈ ∆(X) can be viewed as a Nx-dimensional probability
is an arbitrary cost function
∈
dependent on the
X
v
×
a
X
lu
×
es
U
o
→
f the
vector(avectorwithnonnegativecomponentsthatsumto1),andthus∆(X)
canbeviewedasthe(Nx−1)-dimensionalsimplex. current state X k , initial state X 0 , and controls U k , and where
ψ :∆( ) R is an arbitrary function of the (joint) denotesthefloorfunction.Let (x ,x )≜x +N (x 1)
0 k 0 x k
X ×X ×U → L − ∈
posterior pmf ξ and the controls U . for x ,x be the mapping implied by (8).2
k k 0 k
S ∈X
ISC-POMDPs (7) generalize POMDPs (4) by introducing Toderivetheprobabilisticstructureoftheaugmentedstates
costs c(X ,X ,U ) that can depend on the value of the S , let the pmf describing the initial augmented state S be
0 k k k 0
initial state X , not just on the value of the current state X . ξ ∆( ) with ξ (s) ≜ p(S = s) for s . Similarly,
0 k 0 0 0
∈ S ∈ S
This dependence enables objectives to specified with respect let the transition probabilities for the augmented states be
totheinitialstate;forexample,thecostc(x ,x,u)= x x A s,s¯ (u) ≜ p(S = sS = s¯,U = u) for k 0, s,s¯ ,
0 0 k+1 k k
| − | | ≥ ∈ S
for x ,x and u specifies the objective of keeping and u . Finally, let the measurement probabilities for the
0
(future) st ∈ ate X s X clos ∈ e t U o the (potentially a priori unknown) augmen ∈ te U dstatesbeB s (y,u)≜p(Y =y S =s,U =u)
k k k k 1
initial state X . ISC-POMDPs (7) also generalize ρ-POMDPs for k 1, s , y and u . The | se probabil−ities are
0
≥ ∈ S ∈ Y ∈ U
(5)byintroducingcostsψ(ξ ,U )thatcandependonthejoint developed in the following lemma.
k k
posterior ξ of the initial X and current state X at any time Lemma 3.1: Under the constraints in the ISC-POMDP (7),
k 0 k
k 0 (and hence also their marginals). This generalization the initial augmented-state probabilities satisfy
≥
enables the optimization of uncertainty measures associated (cid:40)
π (x) if s= (x,x),
with the initial and current states, and exploits that (future) ξ (s)= 0 L (9)
0
measurements Y for k 1 can contain information about 0 otherwise
k
≥
the (past) initial state X . For example, Bayesian (fixed-point
0 for s and x ; the augmented state-transition
smoother) estimates of the initial state at future times k > 0 ∈ S ∈ X
probabilities satisfy
can be improved by solving the ISC-POMDP (7) with the
ini (cid:80) tial-state p( e x ntr y o k p , y uk co 1 st )l ψ og (ξ p k ( , x U k y ) k, = uk H 1) (X 0 | yk,uk − w 1) he ≜ re A s,s¯ (u)= (cid:40) Ax,x¯(u) if s= L (x 0 ,x) & s¯= L (x 0 ,x¯),
p − (x x y 0 k∈,Xuk 1 0 ) | = (cid:80) − ξ (x , 0 x | ). − 0 otherwise
W 0 e | prop − ose solvi x n k g∈XISC k -PO 0 MD k Ps (7) by reformulating (10)
them as (ρ-)POMDPs with augmented state processes con- for s,s¯ , x,x¯,x , and u ; and, the augmented-
0
∈ S ∈ X ∈ U
sisting of both the original initial state X and the orig- state measurement probabilities satisfy
0
inal current state X k . We shall show that this choice of B s (y,u)=Bx(y,u) (11)
augmented state leads to an associated (augmented) belief,
equivalent to the joint posterior pmf p(x 0 ,x k | yk,uk − 1), that for s= L (x 0 ,x) ∈S , y ∈Y , and u ∈U where x 0 ,x ∈X .
is Markovian and given by a Bayesian filter recursion, thus Proof: By definition S = (X ,X ), and thus for any
0 0 0
L
enabling the solution of ISC-POMDPs using standard (ρ- s= (x,x¯) where x,x¯ we have that
L ∈S ∈X
)POMDP techniques (cf. [5], [6], [13], [14]). We note that
(cid:40)
ourchoiceofthisaugmentedstateisnecessarytoreformulate π 0 (x) if x=x¯
p(S =s)=p(X =x,X =x¯)=
0 0 0
ISC-POMDPs as (ρ-)POMDPs since insight from recursive 0 otherwise,
Bayesian smoothing implies that only the joint posterior pmf
p(x ,x yk,uk 1) has a recursive form whilst the marginal proving (9). Consider any k 0, u , s = (x 0 ,x)
0 k − ≥ ∈ U L ∈ S
posterior | pmf p(x yk,uk 1) does not (cf. [9, Section 4.1.1]). and s¯= (x¯ 0 ,x¯) where x,x¯,x 0 ,x¯ 0 , then
0 − L ∈S ∈X
|
Interestingly, this insight implies that the joint posterior
p(S =sS =s¯,U =u)
k+1 k k
pmf p(x ,x yk,uk 1) must be used as the belief for ISC- |
POMDPs 0 , ev k e | n whe − n their costs only depend on the initial =p(X k+1 =x,X 0 =x 0 | X k =x¯,X 0 =x¯ 0 ,U k =u)
(cid:40)
state X 0 or its posterior pmf p(x 0 | yk,uk − 1), as in the case of = p(X k+1 =x | X k =x¯,X 0 =x 0 ,U k =u) if x 0 =x¯ 0
the initial-state entropy H(X yk,uk 1).
0 | − 0 otherwise,
proving (10) since p(X = xX = x¯,X = x ,U =
III. REFORMULATION AND SOLUTION OF ISC-POMDPS k+1 | k s,s¯ 0 0 k
u) = p(X = xX = x¯,U = u) = A (u) due to X
k+1 k k k+1
In this section, we reformulate ISC-POMDPs as (ρ-
and X being con
|
ditionally independent given X and U .
0 k k
)POMDPs with an augmented state and a Markovian belief.
Finally, consider any k 1, s = (x ,x ) , y , and
0 0
≥ L ∈ S ∈ Y
u where x ,x , then p(Y = y S = s,U =
0 k k k 1
A. AugmentedStateandBeliefConstruction u) ∈ = U p(Y k = y X k ∈ = X x,U k 1 = u) sinc | e Y k and X − 0 are
conditionally ind | ependent give−n X and U , proving (11).
Let us introduce the augmented state The proof is complete. k k − 1
S k ≜X 0 +N x (X k 1) (8) The conditional pmf ξ k in (6) corresponds to the (aug-
− ∈S mented) belief for the augmented state S . In a slight abuse
k
for k 0, with corresponding augmented state space ≜ of notation, we shall therefore denote the conditional pmf
1,2,. ≥ ..,N where N ≜ N N . The augmented S state ξ introduced in (6) as ξ ∆( ) with ξ (s) ≜ p(S =
{ s } s x × x k k ∈ S k k
S provides an invertible representation of the pair (X ,X )
k 0 k
in the sense that given (X ,X ), we can compute S via 2ThemappingLisanalogoustothoseusedtoconstructlinear,vectorized,
0 k k
(8), and given S we can compute (X ,X ) via X =S or “flattened” indices of matrices (or tensors), with S k being the linear
k 0 k 0 k − index of the pair (X0,X k ). More generally, the augmented state could be
N x (S k 1)/N x and X k = (S k X 0 )/N x +1 where constructedwithanyinvertible(i.e.,bijective)mappingL:X ×X →S.
⌊ − ⌋ − ⌊·⌋
syk,uk 1) for s and k 0. The augmented state in (8) B. Augmented(ρ-)POMDPReformulation
−
| ∈S ≥
enables us to treat ξ as an N -dimensional probability vector
k s Our main result reformulating ISC-POMDPs follows.
and∆( )asthe(N
s
1)-dimensionalsimplex.Thefollowing
Theorem 3.1: Consider the ISC-POMDP (7). Define the
S −
lemma establishes that the augmented belief ξ evolves via a
k augmented-belief cost function
simple recursion resembling the Bayesian filter (3).
(cid:88)
Lemma 3.2: Under the constraints in the ISC-POMDP (7), ρ(ξ,u)≜ψ(ξ,u)+ ξ(s)c(s,u) (16)
the augmented belief ξ k evolves via the recursion s
∈S
B s (y ,u ) (cid:80) A s,s¯ (u )ξ (s¯) for ξ ∆( ) and u , where in a slight abuse of notation
ξ k+1 (s)= (cid:80) k+1 s˜ k s¯ ∈S s˜,s¯ k k (12) we de ∈ fine t S he augme ∈ nt U ed cost function c(s,u) ≜ c(x 0 ,x,u)
B (y ,u )A (u )ξ (s¯)
s¯,s˜ k+1 k k k for u and s = (x ,x) . Then the ISC-POMDP (7)
∈S ∈ U L 0 ∈ S
for s and k 0 given the initial augmented belief ξ 0 is equivalent to the augmented-belief ρ-POMDP:
∈ S ≥ ∈
∆( ),theaugmentedstate-transitionprobabilities(10)andthe (cid:34) (cid:35)
aug S mented measurement probabilities (11). Furthermore, inf E (cid:88)∞ γkρ(ξ ,U )
µ k k
(cid:88) s˜ s˜,s¯ µ
p(y k+1 ξ k ,u k )= B (y k+1 ,u k )A (u k )ξ k (s¯) (13) ∈P k=0
| s.t. ξ =Ξ(ξ ,U ,Y ), ξ ∆( ) (17)
s¯,s˜ k+1 k k k+1 0
∈S ∈ S
for y k+1 ∈Y , ξ k ∈ ∆( S ), and u k ∈U . Y k+1 | ξ k ,U k ∼ p(y k+1 | ξ k ,u k )
Proof: Consider any k 0. To establish (12), we show U =µ(ξ )
≥ k k ∈U
p(y s ,u )p(s yk,uk)
p(s yk+1,uk)= k+1 | k+1 k k+1 | (14) where the optimization is over (deterministic) policies µ :
k+1 | p(y k+1 yk,uk) ∆( ) that are functions of the augmented belief ξ, with
| S →U
where being the set of all such policies.
p(s yk,uk)= (cid:88) p(s s ,u )p(s yk,uk 1) (15) P Proof: For µ , the cost functional in (7) satisfies
k+1 k+1 k k k − ∈P
| | | (cid:34) (cid:35)
since (12) follows fr s o k m ∈S (14) and (15) via the definitions of E µ (cid:88)∞ γk[c(X 0 ,X k ,U k )+ψ(ξ k ,U k )]
the augmented state-transition probabilities (10), augmented k=0
(cid:34) (cid:35)
likelihoods (11), and ξ
k+1
(s
k+1
)=p(s
k+1 |
yk+1,uk).
=E
(cid:88)∞
γkE[c(X ,X ,U )+ψ(ξ ,U )Yk,Uk]
To see that (14) holds, note that Bayes’ rule gives µ 0 k k k k |
k=0
p(y s ,yk,uk)p(s yk,uk) (cid:34) (cid:35)
p(s
k+1 |
yk+1,uk)= k+1 | k+
p
1
(y yk,uk
k
)
+1 |
=E
(cid:88)∞
γkρ(ξ ,U )
k+1 µ k k
|
p(y s ,u )p(s yk,uk) k=0
k+1 k+1 k k+1
= | |
p(y yk,uk) where the first equality is due to the tower property of expec-
k+1
|
tations;and,thesecondequalityholdsduetothe(augmented)
wherethelastlineholdsbecauseY and(Yk,Uk)arecon-
k+1 belief ξ being a sufficient statistic for (Yk,Uk 1) since ξ
ditionally independent given X with U = µ(Yk,Uk 1), k − k
k+1 k − is a function of (Yk,Uk 1) and so
and thus Y and (Yk,Uk) are conditionally independent −
k+1
give p n (s S k k + + 1 1 | y = k, L u ( k X ) 0 = ,X (cid:88) k+1 p ). (s N k e + x 1 t | , s ( k 1 , 5 y ) k, h u ol k d ) s p( s s in k c | y e k,uk) E[c = (X ψ 0 ( , ξ X k , k U ,U k ) k ) + + (cid:88) ψ(ξ ξ k k , ( U s) k c ) ( | s Y ,U k, k U ) k = ] ρ(ξ k ,U k ).
s
(cid:88)
k∈S s
∈X
= p(s k+1 | s k ,u k )p(s k | yk,uk − 1) Since ξ k is a controlled Markov process via Lemma 3.2,
sk∈S POMDP (or belief MDP) results imply that this expectation
where:i)X and(Yk,Uk 1)areconditionallyindependent can be minimized over µ under the constraints in (7) by
k+1 − ∈P
givenX k ,andthusS k+1 = L (X 0 ,X k+1 )and(Yk,Uk − 1)are (deterministic) functions µ ∈ P of ξ k (cf. [12, Section 5.4.1]
conditionally independent given S = (X ,X ); and ii) S and [8, Theorem 6.2.2]). The proof is complete.
k 0 k k
L
and U are conditionally independent given (Yk,Uk 1) since A special case of Theorem 3.1 is that when there is no
k −
U = µ(Yk,Uk 1). Thus (14) and (15) hold, implying (12), (explicit) belief-dependent cost ψ, ISC-POMDPs (7) reduce
k −
with Bayes’ rule and the law of total probability giving (13). to POMDPs with states , measurements , controls , and
S Y U
The proof is complete. transition and observations probabilities (10) and (11).
We shall use the shorthand ξ = Ξ(ξ ,u ,y ) to Corollary 3.1: If ψ(ξ,u) = 0 for ξ ∆( ) and u in
k+1 k k k+1 ∈ S ∈ U
denote the recursion in (12) since it (surprisingly) resem- (7), then (7) is equivalent to the (augmented-state) POMDP
bles the Bayesian filter (3) but for the augmented state S (cid:34) (cid:35)
k
(cid:88)∞
with measurements Y k and controls U k . Interestingly, (12) is inf E µ γkC(ξ k ,U k )
equivalentlyarecursivefixed-pointBayesiansmootherforthe µ
∈P k=0
initial state X 0 , with similar recursive smoothers explored in s.t. ξ k+1 =Ξ(ξ k ,U k ,Y k+1 ), ξ 0 ∆( ) (18)
[9, Section 4.1.1] and references therein, but not exploited in ∈ S
Y ξ ,U p(y ξ ,u )
POMDPs (with controls). We next show that (12) enables the k+1 | k k ∼ k+1 | k k
U =µ(ξ )
reformulation of ISC-POMDPs (7) as (ρ-)POMDPs. k k
∈U
where
C(ξ,u)≜(cid:80)
ξ(s)c(s,u) for ξ ∆( ) and u .
s ∈ S ∈U
Corollary 3.1 imp∈liSes that all techniques for solving or Q2 Goal Q1 Goal
analyzing POMDPs of the form (4) apply directly to ISC-
POMDPs(7)thatdonothaveabelief-dependentcostfunction
Agent
ψ.Theorem3.1,moregenerally,impliesthatanoptimalpolicy
µ ∗ :∆( ) andvaluefunctionV :∆( ) Rsolvingan POMDP
S →U S →
ISC-POMDP (7) with arbitrary belief-dependent cost function
ψ can be found via Bellman’s equation ISC-POMDP
V(ξ)=min ρ(ξ,u)+γE[V(Ξ(ξ,u,Y))ξ,u] (19) Q3 Goal Q4 Goal Q4 Goal
u { | }
∈U
(a) (b)
for all ξ ∆( ), with µ (ξ) being a minimizing argument
∗
∈ S
in (19) (cf. [8, Theorem 6.2.2]). We next discuss structural Fig.1. SimulationExperiment:(a)Agentmustmovetogoalincorner
results useful for finding solutions to ISC-POMDPs via (19).
of quadrant of initial state X0 (agent shown must move to Q2 Goal).
(b)RealizationswithPOMDPmovingtocornerclosettolocationXkfor
k = 2 but ISC-POMDP taking steps to estimate X0 then moving to
correctgoal(Q4Goal).
C. StructuralResultsandApproximateSolutions
The structural result of foremost utility is that the value
function V is concave when ψ is concave (or constant) in ξ. Consider an agent moving in the grid shown in Fig. 1a that
Theorem 3.2: Consider the ISC-POMDP (7) reformulated seekstomovetothecornerclosesttoitsinitialposition.Each
as the ρ-POMDP (17). If ψ(,u) is concave in ξ ∆( ) for cell in the grid is a state in the state space = 1,...,16
u , then ρ(,u) is concave · in ξ ∆( ) for u ∈ a S nd the (enumerated top-to-bottom, left-to-right). Th X e ag { ent’s initia } l
∈U · ∈ S ∈U
value function V given by (19) is concave in ξ ∆( ). positionisaprioriunknownanddistributeduniformly(i.e.,π
∈ S 0
Proof: Given(16),thatρ(,u)isconcaveinξ ∆( )for is uniform). The agent’s controls = 1,...,5 correspond
· ∈ S
u when ψ(,u) is concave in ξ ∆( ) for u holds to moving one cell in each of th U e fou { r compas } s directions,
∈U · ∈ S ∈U
since it is the sum of concave functions. With this concavity,
or staying still. The controls fail (and the agent remains
the theorem follows via [5, Theorem 3.1].
still) with probability 0.2, and the walls (bold black lines in
Theorem 3.2 implies that the reformulation in (17) of ISC-
Fig. 1a) block movement, with the agent staying still if it
POMDPs (7) with concave belief-dependent cost functions
attemptstomoveintothem.Theagentreceivesmeasurements
ψ(,u) is a ρ-POMDP amenable to approximate solution via = 1,...,16 corresponding to whether or not a wall is
·
the approach developed in [5]. Indeed, following [5] and Y { }
immediately adjacent to its current cell in each of the four
using the concavity of ρ(,u) established in Theorem 3.2, compassdirections.Awallisdetectedwhenitispresent(resp.
·
we can first construct a piecewise-linear concave (PWLC)
not present) with probability 0.8 (resp. 0.2).
approximationofρ(,u)foru ,beforethenusingstandard
Weencodetheagent’sproblemasanISC-POMDP(7)with
· ∈U
POMDP solvers to compute PWLC approximations of the
no belief-dependent cost function (i.e., with ψ(ξ,u) = 0 for
value function V (see [5, Section 4] for details). The approx-
ξ ∆( ) and u ) and with initial-state cost function
imation errors are bounded if ρ satisfies the Ho¨lder-continuity ∈ S ∈U
conditionsof[5,Theorem4.3],andcan,inprinciple,bemade
1(x=1)
if x 1,2,5,6
a
la
r
r
bi
u
tr
n
a
c
ri
e
l
r
y
ta
s
i
m
nt
a
y
ll
c
(
o
s
s
e
t
e
s
[
a
5
r
,
e
Se
c
c
o
t
n
io
ca
n
v
4
e
.2
a
]
n
f
d
or
sa
d
t
e
is
ta
fy
ils
t
)
h
.
e
M
c
a
o
n
n
y
d
p
it
o
io
p
n
u
s
-
c(x ,x,u)=

1(x
̸
̸
=4) if x
0
0
∈
∈
{
{
3,4,7,8
}
} (20)
o
H
f
(X
[5
0
,
y
T
k
h
,
e
u
o
k
re
−
m
1)
4
i
.
s
3]
c
.
o
F
nc
o
a
r
v
e
e
xa
in
mp
ξ
l
k
e
,
,
a
t
n
h
d
e i
e
n
n
i
t
t
r
i
o
al
p
-
y
sta
f
t
u
e
nc
e
t
n
io
tr
n
o
a
p
l
y
s
0  1
1
(
(
x
x
̸
=
=
1
1
3
6
)
)
i
i
f
f
x
x
0
∈{
9
1
,
1
1
,
0
1
,
2
1
,1
3,
5
1
,
4
1
}
6
| 0
satisfy the conditions of [5, Theorem 4.3] (cf. [5]). However, ̸ ∈{ }
ifψ isnotconcavebutisLipschitzinξ,thenrecentLipschitz- for x and u , where 1() denotes the indicator func-
based approximations can be used (see [6], [15]). tion. E ∈ nc X oding th ∈ e a U gent’s objec · tive of moving to the corner
Finally, the reformulations of ISC-POMDPs in Theorem closest to its initial position is not directly possible using a
3.1 and Corollary 3.1 have state and belief spaces and (standard) POMDP (4) since they are limited to current-state
S
∆( )thatscalequadraticallywiththestatespace .However, dependent costs. For the purpose of comparison, we therefore
S X
they enable the solution of ISC-POMDPs with state-of-the- encode an approximation of the agent’s objective within a
art offline and online POMDP solvers capable of handling POMDP (4) with cost κ(x,u) = 1(x 1,4,13,16 ) for
very large state spaces (cf. [6], [8], [13], [14]). In contrast, u .Thiscostis onlyanapproximatio ̸∈ ns { inceitencou } rages
approaches tailored to specific initial-state costs (such as that the ∈ a U gent to move to the corner closest to its current location
of [4] for the entropy H(X 0 | yk,uk − 1)) have computational X k , rather than to that closest to X 0 . We use SARSOP [13]
and memory requirements that must be carefully managed via andγ =0.95tosolvethePOMDPandISC-POMDP(as(18)).
parameters such as memory length and number of samples. Being an offline anytime algorithm, SARSOP had 5 minutes
prior to deployment to compute each policy (and their use
IV. SIMULATION EXPERIMENT online was dominated by belief computation).
We now illustrate using ISC-POMDPs to optimize costs The results of 10,000 Monte Carlo simulations of the
defined with respect to an a priori unknown initial state X . ISC-POMDP and POMDP over T = 10 time steps with
0
TABLEI
MONTECARLOSIMULATIONRESULTS.BESTVALUESINBOLD.
2.5
Criteria ISC-POMDP POMDP
DiscountedCost 6.26 7.91 2.0
No.GoalsReached 8031 4116
FinalInitial-StateEntropy 1.54 1.72
1.5
FinalInitial-StateProb. 0.296 0.245
0 2 4 6 8 10
Timek
X π are summarized in Table I and Fig. 2. We report
0 0
∼
the: (average) total discounted cost under the ISC-POMDP
cost function in (7) with (20) (Discounted Cost); number of
timestheagentreachesthecorrectgoal,i.e.,thecornerclosest
to its initial position X (No. Goals Reached); (average) 0
entropy H(X yT,uT 1) of the final initial-state posterior
0 −
| pmf p(x yT,uT 1) (Final Initial-State Entropy); and, the 0 −
|
(average) probability at the (true) initial state X in the
0
final (marginal) posterior pmf p(x yT,uT 1) (Final Initial- 0 −
| State Prob.). Fig. 2 shows the (average) initial-state entropy
H(X yk,uk 1) and (average) probability at the initial-state
0 −
|
in the posterior pmf p(x yk,uk 1) at other times. Example
0 −
|
realizations of the agent’s position are shown in Fig. 1b.
Table I shows that the ISC-POMDP outperforms the
POMDP in terms of the discounted cost and the number
of times the agent successfully reaches the corner closest
to X (with failures occurring when the measurements do
0
not enable unambiguous estimation of X ). The superior
0
performance of the ISC-POMDP is due to it encoding the
agent’s exact objective with the initial-state costs (20) rather
than approximating it with the cost κ(x,u). Furthermore, the
challenge that the ISC-POMDP overcomes (that the POMDP
cannot) is that in order for the agent to move to the correct
goal, it must first determine its initial state X . The lower
0
initial-state entropy and higher posterior probability in Table
I and Figs. 2a and 2b for the ISC-POMDP compared to the
POMDP show that the ISC-POMDP selects controls that help
to estimate X , and hence determine the correct goal to move 0
to. The realizations shown in Fig. 1b illustrate that the ISC-
POMDP can take extra steps to estimate the initial state X
0
and the correct goal, whilst the POMDP will simply move
to the corner closest to its current location X when it first
k
becomes confident of its current location. This experiment
illustrates that ISC-POMDPs enable the optimization of costs
dependent on an a priori unknown initial state X , which is
0
importantsincetheoptimalpolicymustselectcontrolsU that
k
resolve uncertainty about the initial state X , rather than just 0
the current state X as in the case of standard (ρ-)POMDPs.
k
V. CONCLUSIONS AND FUTURE WORK
We propose ISC-POMDPs as a class of (ρ-)POMDPs with
costs dependent on the values and/or uncertainty of initial
states.WeuserecursiveBayesiansmoothingtoshowthatthey
admit reformulations and solutions as (ρ-)POMDPs with aug-
mentedstatesandbeliefs.Futureworkwillconsiderproblems
with continuous state, control, and measurement spaces.
REFERENCES
[1] L.Wang,I.R.Manchester,J.Trumpf,andG.Shi,“Differentialinitial-
valueprivacyandobservabilityoflineardynamicalsystems,”Automat-
ica,vol.148,p.110722,2023.
yportnEetatS-laitinI
)1
− ku,ky
|0X(H
ISC-POMDP
POMDP
(a)
0.4
0.2
0.0
0 2 4 6 8 10
Timek
.borPetatS-laitinI
)1
− ku,ky
|0x(p
ISC-POMDP
POMDP
(b)
Fig. 2. Simulation Results: (a) Entropy H(X0|yk,uk−1) of initial-
stateposteriorpmfp(x0|yk,uk−1).(b)Probabilityat(true)initialstate
X0ofposteriorpmfp(x0|yk,uk−1).
[2] G. L. Mariottini and S. I. Roumeliotis, “Active vision-based robot
localizationandnavigationinavisualmemory,”inIEEEInternational
ConferenceonRoboticsandAutomation,2011,pp.6192–6198.
[3] W.Xue,R.Ying,F.Wen,Y.Chen,andP.Liu,“ActiveSLAMWithPrior
Topo-MetricGraphStartingAtUncertainPosition,”IEEERoboticsand
AutomationLetters,vol.7,no.2,pp.1134–1141,2022.
[4] C.Shi,S.Han,M.Dorothy,andJ.Fu,“Activeperceptionwithinitial-
state uncertainty: A policy gradient method,” IEEE Control Systems
Letters,vol.8,pp.3147–3152,2024.
[5] M.Araya,O.Buffet,V.Thomas,andF.Charpillet,“APOMDPextension
with belief-dependent rewards,” in Advances in Neural Information
ProcessingSystems,vol.23. CurranAssociates,Inc.,2010,pp.64–72.
[6] M.Fehr,O.Buffet,V.Thomas,andJ.Dibangoye,“rho-POMDPshave
Lipschitz-Continuousepsilon-OptimalValueFunctions,”inAdvancesin
Neural Information Processing Systems, vol. 31. Curran Associates,
Inc.,2018.
[7] T. L. Molloy and G. N. Nair, “Smoother Entropy for Active State
TrajectoryEstimationandObfuscationinPOMDPs,”IEEETransactions
onAutomaticControl,vol.68,no.6,pp.3557–3572,2023.
[8] V.Krishnamurthy,PartiallyobservedMarkovdecisionprocesses. Cam-
bridgeUniversityPress,2016.
[9] O. Cappe´, E. Moulines, and T. Ryde´n, Inference In Hidden Markov
Models. NewYork,NY:Springer,2005.
[10] F. Belardinelli, B. G. Leo´n, and V. Malvone, “Enabling Markovian
Representations under Imperfect Information,” in Proceedings of the
14th International Conference on Agents and Artificial Intelligence,
2022,pp.450–457.
[11] F.Bacchus,C.Boutilier,andA.Grove,“Rewardingbehaviors,”inPro-
ceedingsoftheThirteenthNationalConferenceonArtificialIntelligence.
AAAIPress,1996,p.1160–1167.
[12] D. P. Bertsekas, Dynamic programming and optimal control, 3rd ed.
Belmont,MA:AthenaScientific,2005,vol.1.
[13] H.Kurniawati,D.Hsu,andW.S.Lee,“SARSOP:Efficientpoint-based
POMDPplanningbyapproximatingoptimallyreachablebeliefspaces.”
inRobotics:ScienceandSystems,2008.
[14] W. Zheng and H. Lin, “Provable-correct partitioning approach for
continuous-observationPOMDPswithspecialobservationdistributions,”
IEEEControlSystemsLetters,vol.7,pp.1135–1140,2023.
[15] Y.E.Demirci,A.D.Kara,andS.Yu¨ksel,“Averagecostoptimalityof
partiallyobservedMDPs:Contractionofnonlinearfiltersandexistence
ofoptimalsolutionsandapproximations,”SIAMJournalonControland
Optimization,vol.62,no.6,pp.2859–2883,2024.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "ISC-POMDPs: Partially Observed Markov Decision Processes with Initial-State Dependent Costs"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
