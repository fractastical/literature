=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Object-based active inference
Citation Key: bergen2022objectbased
Authors: Ruben S. van Bergen, Pablo L. Lanillos

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2022

Key Terms: ruben, world, action, objects, object, planning, pablo, inference, active, lanillos

=== FULL PAPER TEXT ===

Object-based active inference
Ruben S. van Bergen and Pablo L. Lanillos
Department of Artificial Intelligence &
Donders Institute for Brain, Cognition & Behavior
Radboud University, Nijmegen, the Netherlands
{ruben.vanbergen, pablo.lanillos}@donders.ru.nl
Abstract The world consists of objects: distinct entities possessing in-
dependentpropertiesanddynamics.Foragentstointeractwiththeworld
intelligently,theymusttranslatesensoryinputsintothebound-together
features that describe each object. These object-based representations
form a natural basis for planning behavior. Active inference (AIF) is an
influential unifying account of perception and action, but existing AIF
modelshavenotleveragedthisimportantinductivebias.Toremedythis,
weintroduce‘object-basedactiveinference’(OBAI),marryingAIFwith
recent deep object-based neural networks. OBAI represents distinct ob-
jects with separate variational beliefs, and uses selective attention to
route inputs to their corresponding object slots. Object representations
areendowedwithindependentaction-baseddynamics.Thedynamicsand
generativemodelarelearnedfromexperiencewithasimpleenvironment
(activemulti-dSprites).WeshowthatOBAIlearnstocorrectlysegment
the action-perturbed objects from video input, and to manipulate these
objects towards arbitrary goals.
Keywords: Multi-object representation learning · Active inference
1 Introduction
Intelligent agents are not passive entities that observe the world and learn its
causality. They learn the relationship of action and effect by interacting with
the world, in order to fulfil their goals [1]. In higher-order intelligence, such as
exhibited by primates, these interactions very often take place at the level of
objects[2,3].Whetherpickingaripefruitfromatreebranch,kickingafootball,
or taking a drink from a glass of water; all require reasoning and planning in
terms of objects. Objects, thus, are natural building blocks for representing the
world and planning interactions with it.
While there have been recent advances in unsupervised multi-object rep-
resentation learning and inference [4,5], to the best of the authors knowledge,
no existing work has addressed how to leverage the resulting representations
for generating actions. In addition, object perception itself could benefit from
being placed in an active loop, as carefully selected actions could resolve ambi-
guity about object properties (including their segmentations - i.e., which inputs
belong to which objects). Meanwhile, state-of-the-art behavior-based learning
2202
peS
2
]IA.sc[
1v85210.9022:viXra
2 van Bergen & Lanillos
(control), such as model-free reinforcement learning [6] uses complex encoding
of high-dimensional pixel inputs without taking advantage of objects as an in-
ductive bias (though see [7,8].
To bridge the gap between these different lines of work, we here introduce
‘object-based active inference’ (OBAI, pronounced /@’beI/), a new framework
thatcombinesdeep,object-basedneuralnetworks[4]andactiveinference[9,10].
OurproposedneuralarchitecturefunctionslikeaBayesianfilterthatiteratively
refines perceptual representations. Through selective attention, sensory inputs
are routed to high-level object modules (or slots [5]) that encode each object as
a separated probability distribution, whose evolution over time is constrained
by an internal model of action-dependent object dynamics. These object repre-
sentations are highly compact and abstract, thus enabling efficient unrolling of
possible futures in order to select optimal actions in a tractable manner. Fur-
thermore, we introduce a closed-form procedure to learn preferences or goals in
the network’s latent space.
As a proof-of-concept, we evaluate our proposed framework on an active
version of the multi-dSprites dataset, developed for this work (See Fig. 1a).
Our preliminary results show that OBAI is able to: i) learn to segment and
represent objects ii) learn the action-dependent, object-based dynamics of the
environment; and iii) plan in the latent space – obviating the need to imagine
detailed pixel-level outcomes in order to generate behavior. This work is a first
step towards building more complex object-based active inference systems that
can perform more cognitively challenging tasks on naturalistic input.
2 Methods
2.1 Object-structured generative model
We extend the IODINE architecture proposed in [4] for object representation
learning, to incorporate dynamics. Zablotskaia et al. [11] previously developed a
similarextensiontoIODINE,inwhichobjectdynamicsweremodeledimplicitly,
through LSTM units operating one level below the latent-space representation.
Here,weinsteadimplementthedynamicsdirectlyinthelatentspace,andallow
these dynamics to be influenced by actions on the part of the agent.
LikeIODINE,ourframeworkreliesoniterativeamortizedinference[12](IAI)
on an object-structured generative model. This model describes images of up to
K objects with a Normal mixture density (illustrated in Fig. 1):
p(o |{s(k)} ,m )= (cid:88) [m =k]N (cid:16) g (s(k)),σ2 (cid:17) (1)
i k∈1:K i i i o
k
where o is the value of the i-th image pixel, s(k) is the state of the k-th object,
i
g (•) is a decoder function (implemented as a deep neural network (DNN)) that
i
translatesanobjectstatetoapredictedmeanvalueatpixeli,σ2isthevariability
o
ofpixelsaroundtheirmeanvaluesand,crucially,m isacategoricalvariablethat
i
Object-based active inference 3
observation dynamics
model
sk ak
t t
() ()
sk
() K
sk ak
t t
m () ()
i +1 +1
o
i sk ak
M t t
() ()
+2 +2
emit
(a) (b) (c)
Figure1. Environment and generative model. (a) Active multi-dSprites. Non-
zero accelerations in action fields (in the 2nd frame) are indicated as white arrows
originatingattheacceleratedgridlocation.(b)Object-structuredgenerativemodelfor
asingleimage(timeindicesomittedhereforclarityofexposition).(c)Dynamicsmodel
in state-space for a single object k, shown for three time points.
indicates which object (out of a possible K choices) pixel i belongs to1. Note
thatthesamedecoderfunctionissharedbetweenobjects.Thepixelassignments
themselves also depend on the object states:
(cid:16) (cid:16) (cid:17)(cid:17)
p(m |{s(k)} )=Cat Softmax {π (s(k))} (2)
i k∈1:K i k∈1:K
where π (•) is another DNN that maps an object state to a log-probability at
i
pixeli,which(uptoaconstantofaddition)definestheprobabilitythatthepixel
belongstothatobject.Marginalizedovertheassignmentprobabilities,thepixel
likelihoods are given by:
p(o |{s(k)} )= (cid:88) mˆ N (cid:16) g (s(k)),σ2 (cid:17) (3)
i k∈1:K ik i o
k
mˆ =p(m =k|{s(k)} ) (4)
ik i k∈1:K
During inference, the soft pixel assignments {mˆ } introduce dynamics akin to
ik
selectiveattention,aseachobjectslotisincreasinglyabletofocusonthosepixels
that are relevant to that object.
2.2 Incorporating action-dependent dynamics
Sofar,thisformulationisidenticaltothegenerativemodelinIODINE.Wenow
extend this with an action-based dynamics model. We want to endow objects
with (approximately) linear dynamics, and to allow actions that accelerate the
objects. First, we redefine the state of an object at time point t in generalized
(cid:20) (cid:21)
s
coordinates, i.e. s† = t , where s(cid:48) refers to the first-order derivative of the
t s(cid:48)
t
1 Note the use of Iverson-bracket notation; the bracket term is binary and evaluates to 1 iff
theexpressioninsidethebracketsistrue.
4 van Bergen & Lanillos
state. The action-dependent state dynamics are then given by:
s(cid:48)(k) =s(cid:48)(k)+Da(k) +σ (cid:15) (5)
t t−1 t−1 s 1
s(k) =s(k) +s(cid:48)(k)+σ (cid:15) (6)
t t−1 t s 2
where a(k) is the action on object k at time t. This action is a 2-D vector that
t
specifiestheaccelerationontheobjectinpixelcoordinates.MultiplicationbyD
(which is learned during training) transforms the pixel-space acceleration to its
effect in the latent space2. Equations 5-6 thus define the object dynamics model
p(s†(k)|s†(k),a(k) ).
t t−1 t−1
We established that a(k) is the action on object k in the model at a given
t
time. However, note that the correspondence between objects represented by
the model, and the true objects in the (simulated) environment, is unknown.3
To solve this correspondence problem, we introduce the idea of action fields. An
actionfieldΨ=[ψ ,...,ψ ]T isan[M×2]matrix(withM thenumberofpixels
1 M
in an image or video frame), such that the i-th row in this matrix (ψ ) specifies
i
the(x,y)-accelerationappliedatpixeli.Inprinciple,adifferentaccelerationcan
beappliedateachpixelcoordinate(inpractice,weapplyaccelerationssparsely).
These pixel-wise accelerations affect objects through the rule that each object
receives the sum of all accelerations that occur at its visible pixels:
a(k) = (cid:88) [m =k]ψ +σ (cid:15) (7)
t i i ψ 3
i
whereweincludeasmallamountofNormallydistributednoiseinordertomake
this relationship amenable to variational inference. This definition of actions in
pixel-space is unambiguous and allows the model to interact with the environ-
ment.
2.3 Inference
On the generative model laid out in the previous section, we perform itera-
tive amortized inference (IAI).IAIgeneralizesvariationalautoencoders(VAEs),
which perform inference in a single feedforward pass, to architectures which use
several iterations (implemented in a recurrent network) to minimize the Evi-
dence Lower Bound (ELBO). As in VAEs, the final result is a set of variational
beliefs in the latent space of the network. In our case, this amounts to inferring
2 Sincethenetworkwillbetrainedunsupervised,wedonotknowinadvancethenatureofthe
latentspacerepresentationthatwillemerge.Inparticular,wedonotknowinwhatformat
(orevenif)thenetworkwillcometorepresentthepositionsoftheobjects.
3 In particular, since the representation across objects slots in the network is permutation-
invariant,theirorderisarbitrary–justastheorderinthememoryarraysthatspecifythe
environmentisalsoarbitrary.Thus,theobject-actions,asrepresentedinthemodel,cannot
beunambiguouslymappedtoobjectsintheenvironmentthattheagentinteractswith.This
problemisexacerbatedifthenetworkhasnotinferredtheobjectpropertiesandsegmenta-
tionswithperfectaccuracyorcertainty,andthuscannotaccuratelyorunambiguouslyrefer
toatrueobjectintheenvironment.
Object-based active inference 5
q({s†(k),a(k)} ). We choose these beliefs to be independent Normal distri-
k∈1:K
butions. Inference and learning both minimize the following ELBO loss:
L=− (cid:88) T (cid:34) H (cid:16) q (cid:16) {s†(k),a(k)} (cid:17)(cid:17) +E [logp(o |{s(k)})]
t t q({s(
t
k)}) t t
t=0
(cid:35)
+ (cid:88) E q(a( t k)) [logp(a( t k)|Ψ t )]+ (cid:88) E q (cid:16) s† t (k),s† t− (k 1 ),a( t k − ) 1 (cid:17)[logp(s t †(k)|s† t− (k 1 ),a( t− k) 1 )] (8)
k k
for some time horizon T. Note that for t = 0, we define p(s†(k)|s†(k),a(k) ) =
t t−1 t−1
p(s†(k)) = (cid:81) N(s†(k);0,1), i.e. a fixed standard-Normal prior. To compute
jk j
E [logp(a(k)|Ψ )],weemployasamplingprocedure,describedinAppendixD.
q(a(k)) t t
t
TheIAIarchitectureconsistsofadecodermodulethatimplementsthegener-
ative model, and a refinement module which outputs updates to the parameters
λ of the variational beliefs. Mirroring the decoder, the refinement module con-
sists of K copies of the same network (sharing the same parameters), such that
refinement network k outputs updates to λ(k). Network architectures for the
decoder and refinement modules are detailed in Appendix B. To perform infer-
ence across multiple video frames, we simply copy the refinement and decoder
networks across frames as well as object slots. Importantly, as in [4], each re-
finement network instance also receives as input a stochastic estimate of the
current gradient ∇ L. Since the ELBO loss includes a temporal dependence
λ(k)
t
term between time points, the inference dynamics in the network are automat-
ically coupled between video frames, constraining the variational beliefs to be
consistent with the dynamics model. To infer q({a(k)} ), we employ a sep-
k∈1:K
arate (small) refinement network (again copied across objects slots; details in
Appendix B.2).
2.4 Task & training
We apply OBAI to a simple synthetic environment, developed for this work,
which we term active-dSprites. This environment was created by re-engineering
the original dSprites shapes [13], to allow these objects to be translated at will
and by non-integer pixel offsets. The environment simulates these shapes mov-
ing along linear trajectories that can be perturbed through the action fields we
introduced above. In the current work, OBAI was trained on pre-generated ex-
perience with this environment, with action fields sampled arbitrarily (i.e. not
based on any intent on the part of the agent).
Specifically, we generated video sequences (4 frames, 64×64 pixels) from the
active-dSprites environment, with 3 objects per video, in which we applied an
action field only at a single time point (in the 2nd frame). Action fields were
sparsely sampled such that (whenever possible) every object received exactly
one non-zero acceleration at one of its pixels. Exactly one background pixel also
received a non-zero acceleration, to encourage the model to learn not to assign
backgroundpixelstothesegmentationmasksofforegroundobjects.Inpractice,
6 van Bergen & Lanillos
theappearanceofthebackgroundwasunaffectedbytheseactions(conceptually,
the background can be thought of as an infinitely large plane extending outside
the image frame, and thus shifting it by any amount will not change its visual
appearance in the image).
OBAI was trained on 50,000 pre-generated video sequences, to minimize the
ELBO loss from equation 8. This loss was augmented to include the losses
at intermediate iterations of the inference procedure, and this composite loss
was backpropagated through time to compute parameter gradients. More de-
tails about the environment and training procedure can be found in Appen-
dices A & C.
2.4.1 Learning goals in the latent space
The active-dSprites environment was conceived to support cognitive tasks that
require object-based reasoning. A natural task objective in active-dSprites is
to move a certain object to a designated location. This type of objective is
simpleinandofitself,buttherulesthatdeterminewhichobjectmustbemoved
where can be arbitrarily complex. For now, we restrict ourselves to the simple
objective of moving all objects in a scene to a fixed location, and focus on
how to encode this objective. We follow previous Active Inference work (e.g.
[14,15]) in conceptualizing goals as a preference distribution p˜. However, rather
than defining this preference to be over observations, as is common (though see
[16,17,18]), we instead opt to define it over latent states, i.e. p˜({s†(k)}), which
simplifiesactionselection(afulldiscussionofthemeritsofthischoiceisoutside
the scope of this paper).
Assuming that we can define a preference over the true state of the environ-
ment, s (e.g. the ground-truth object positions), the preference distribution
true
in latent space can be obtained through the following importance-sampling pro-
cedure:
p˜(s)∝ (cid:88) p(s|o∗)u ≈ (cid:88) q(s|o∗)u (9)
j j j j
j j
o∗ ∼p(o|s∗ ), u =p˜(s∗ ), s∗ ∼p(s )∝Constant (10)
j truej j truej truej true
Thisallowsthelatent-spacepreferencetobeestimatedinclosedformfromaset
oftrainingexamples,constructedbysamplingtruestatesuniformlyfromtheen-
vironmentandrenderingvideosfromthesestates.Inferenceisperformedonthe
resulting videos, and the latent-state preference is computed as the importance-
weightedaverageoftheinferredstate-beliefs(alternatively,wecansamplestates
directlyfromp˜(s∗ ),andlettheimportanceweightsdropoutoftheequation).
true
Inparticular,ifp˜(s∗ )isNormal,thenthepreferenceinthelatentspaceisalso
true
Normal:
q(s|o∗)=N (cid:0) µ(o∗),σ(o∗2) (cid:1) , p˜(s)=N(µ˜,σ˜2) (11)
j j j
(cid:115)
µ˜ = (cid:80) 1 u (cid:88) u j µ(o∗ j ), σ˜ = (cid:80) 1 u (cid:88) u j (cid:0) (µ˜ −µ(o∗ j ))2+σ(o∗ j )2 (cid:1) (12)
j j j j j j
Object-based active inference 7
2.4.2 Planning actions
OBAI can plan actions aimed at bringing the environment more closely in line
with its learned preferences. Specifically, we choose actions that minimize the
Free Energy of the Expected Future (FEEF) [18]. When preferences are de-
fined with respect to latent states, the FEEF of a policy (action sequence)
(cid:110) (cid:111)
π = [a(k),a(k),...,a(k)] is given by:
1 2 T
k∈1:K
T
(cid:88)(cid:88) (cid:16) (cid:17)
G(π)= D q(s(k)|π)||p˜(s) (13)
KL τ
τ=1 k
where q(s(k)|π) is the policy-conditioned variational prior, obtained by propa-
τ
gatingthemostrecentstatebeliefsthroughthedynamicsmodelbytherequisite
number of time steps.
Given this objective, the optimal policy can be calculated in closed form
for arbitrary planning horizons. In this work, as a first proof-of-principle, we
onlyconsidergreedy,one-step-aheadplanning.Inthiscase,theoptimal"policy"
(single action per object) is given by:
aˆ(k) =(DTLD)−1DTL(µ˜ −µ(k)) (14)
s
where L=diag(σ˜−2), and µ(k) is the mean of the current state belief for object
s
k.
3 Results
3.1 Object segmentation and reconstruction in dynamic scenes
We first evaluated OBAI on its inference and reconstruction capabilties, when
presented with novel videos of moving objects (not seen during training). To
evaluate this, we examined the quality of its object segmentations, and of the
video frame reconstructions (Fig. 2). Segmentation quality was computed using
the Adjusted Rand Index (ARI), as well as a modified version of this index
that only considers (ground-truth) foreground pixels (FARI). Across a test set
of 10,000 4-frame video sequences of 3 randomly sampled moving objects each,
OBAIachievedanaverageARIandFARIof0.948and0.939,respectively(where
1meansperfectaccuracyand0equalschance-levelperformance),andaMSEof
9.51×10−4 (note that pixel values were in the range of [0,1]). For comparison,
a re-implementation of IODINE, trained on 50,000 static images of 3 dSprite
objects, achieved an ARI of 0.081, FARI of 0.856 and MSE of 1.63×10−3 (on
a test set of identically sampled static images). The very low ARI score reflects
the fact that IODINE has no built-in incentive to assign background pixels to
their own object slot. OBAI, on the other hand, has to account for the effects of
actions being applied to the background, which must not affect the dynamics of
theforegroundobjects.Thus,forOBAItoaccuratelymodelthedynamicsinthe
trainingdata,itmustlearnnottoassignbackgroundpixelstothesegmentation
masks of foreground objects, lest and action might be placed on one of these
spurious pixels.
8 van Bergen & Lanillos
Figure2.Reconstructionandsegmentationofvideosofmovingobjectswith
actions.Threeinstancesofsegmentationfromthetestsetofvideos.Themasksshows
howeachslotattendstothethreeobjectsandthebackground.Theactionfieldinthis
experimentisrandomlygeneratedforeachinstanceatthebeginningofthesimulation.
ARI (↑) F-ARI (↑) MSE (↓)
IODINE (static) 0.081 0.856 1.63×10−3
OBAI (ours; 4 frames) 0.948 0.939 9.51×10−4
Table 1. Quantitative segmentation and reconstruction results.
3.2 Predicting the future state of objects
An advantage of our approach is that the network can predict future states of
theworldatthelevelofobjectsusingthelearnedstatedynamics.Figure3shows
three examples of the network predicting future video frames. The first 4 video
frames are used by the network to infer the state. Afterwards, we extrapolate
theinferredstateanddynamicsofthelastobservedvideoframeintothefuture,
and decode the thus-predicted latent states into predicted video frames. These
predictions are highly accurate, with a MSE of 4×10−3.
3.3 Goal-directed action planning
CanOBAIlearnandaccomplishbehavioralobjectives?Asafirstforayintothis
question, we asked OBAI to learn fixed preference distributions defined in the
truestate-spaceoftheenvironment,usingthemethoddescribedinsection2.4.1.
Specifically, we placed a Gaussian preference distribution on the location of the
objectandhadthenetworklearnthecorrespondingpreferenceinitslatentspace
from a set of 10,000 videos (annotated with the requisite importance weights).
We then presented the network with static images of dSprite objects in random
locations,andaskeditto"imagine"theactionthatwouldbringthestateofthe
environmentintoalignmentwiththelearnedpreference.Finally,weappliedthis
Object-based active inference 9
Figure3.Predictionoffuturevideoframes.Twoinstancesofpredictionfromthe
test set of videos. We let the network perform inference on 4 consecutive frames, and
then predict the future.
imagined action to the latent state, and decoded the image that would result
from this. As illustrated in Figure 4, the network is reliably able to imagine
actions that would accomplish the goals we wanted it to learn.
4 Conclusion
This work seeks to bridge an important gap in the field. On the one hand, com-
putervisionresearchhasdevelopedobject-basedmodels,buttheseonlyperform
passiveinference.Ontheotherhand,thereisawealthofresearchonbehavioral
learning (e.g. reinforcement learning and active inference), which has not gener-
ally leveraged objects as an inductive bias, built into the network architecture
(cf.[7,8]).OBAIreconcilesthesetwolinesofresearch,byextendingobject-based
visual inference models with action-based dynamics. We showed that OBAI can
accurately track and predict the dynamics (and other properties) of simple ob-
jectswhosemovementsareperturbedbyactions–animportantprerequisitefor
an agent to plan its own actions. In addition, we presented an efficient method
forinternalizinggoalsasapreferencedistributionoverlatentstates,andshowed
that the agent can infer the actions necessary to accomplish these goals, at the
Figure4. Goal-directed action planning. We give the network an arbitrary input
imagewiththreeobjects(in)anditinferstheactionthatwillmovethestatetowards
the learned preference, and imagines the resulting image (out). In (a), p˜(s ) was
true
biasedtowardsthecenteroftheimage;in(b),itwasbiasedmoretowardsthetop-left.
10 van Bergen & Lanillos
same abstract level of reasoning. While our results are preliminary, they are an
important proof-of-concept, establishing the potential of our approach. In fu-
ture work, we aim to scale OBAI to more naturalistic environments, and more
cognitively demanding tasks.
5 Acknowledgements
RSvB is supported by Human Brain Project Specific Grant Agreement 3 grant
ID 643945539: "SPIKEFERENCE".
References
1. Lanillos,P.,Dean-Leon,E.,Cheng,G.:Yieldingself-perceptioninrobotsthrough
sensorimotor contingencies. IEEE Transactions on Cognitive and Developmental
Systems 9(2), 100–112 (2016)
2. Kourtzi,Z.,Connor,C.E.:Neuralrepresentationsforobjectperception:structure,
category, and adaptive coding. Annual review of neuroscience 34, 45–67 (2011)
3. Peters,B.,Kriegeskorte,N.:Capturingtheobjectsofvisionwithneuralnetworks.
NatureHumanBehaviour5,1127–1144(92021).https://doi.org/10.1038/s41562-
021-01194-6
4. Greff,K.,Kaufman,R.L.,Kabra,R.,Watters,N.,Burgess,C.,Zoran,D.,Matthey,
L.,Botvinick,M.,Lerchner,A.:Multi-objectrepresentationlearningwithiterative
variationalinference.In:InternationalConferenceonMachineLearning.pp.2424–
2433. PMLR (2019)
5. Locatello, F., Weissenborn, D., Unterthiner, T., Mahendran, A., Heigold, G.,
Uszkoreit, J., Dosovitskiy, A., Kipf, T.: Object-centric learning with slot atten-
tion.AdvancesinNeuralInformationProcessingSystems33,11525–11538(2020)
6. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G.,
Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G., et al.: Human-level
control through deep reinforcement learning. Nature 518(7540), 529–533 (2015)
7. Veerapaneni,R.,Co-Reyes,J.D.,Chang,M.,Janner,M.,Finn,C.,Wu,J.,Tenen-
baum, J.B., Levine, S.: Entity abstraction in visual model-based reinforcement
learning (10 2019), http://arxiv.org/abs/1910.12827
8. Watters, N., Matthey, L., Bosnjak, M., Burgess, C.P., Lerchner, A.: Cobra: Data-
efficientmodel-basedrlthroughunsupervisedobjectdiscoveryandcuriosity-driven
exploration (5 2019), http://arxiv.org/abs/1905.09275
9. Parr, T., Pezzulo, G., Friston, K.J.: Active inference: the free energy principle in
mind, brain, and behavior. MIT Press (2022)
10. Lanillos,P.,Meo,C.,Pezzato,C.,Meera,A.A.,Baioumy,M.,Ohata,W.,Tschantz,
A., Millidge, B., Wisse, M., Buckley, C.L., et al.: Active inference in robotics and
artificial agents: Survey and challenges. arXiv preprint arXiv:2112.01871 (2021)
11. Zablotskaia, P., Dominici, E.A., Sigal, L., Lehrmann, A.M.: Unsuper-
vised video decomposition using spatio-temporal iterative inference (2020),
http://arxiv.org/abs/2006.14727
12. Marino, J., Yue, Y., Mandt, S.: Iterative amortized inference. 35th Interna-
tional Conference on Machine Learning, ICML 2018 8, 5444–5462 (7 2018),
http://arxiv.org/abs/1807.09356
Object-based active inference 11
13. Matthey, L., Higgins, I., Hassabis, D., Lerchner, A.: dsprites: Disentanglement
testing sprites dataset. https://github.com/deepmind/dsprites-dataset/ (2017)
14. Active inference and epistemic value. Cognitive Neuroscience 6, 187–214 (2015).
https://doi.org/10.1080/17588928.2015.1020053
15. Sajid, N., Ball, P.J., Parr, T., Friston, K.J.: Active in-
ference: Demystified and compared. Neural Computation
33, 674–712 (3 2021). https://doi.org/10.1162/neco_a_01357,
https://direct.mit.edu/neco/article/33/3/674-712/97486
16. Friston, K.: A free energy principle for a particular physics (6 2019),
http://arxiv.org/abs/1906.10184
17. Da Costa, L., Parr, T., Sajid, N., Veselic, S., Neacsu, V., Friston, K.: Active in-
ference on discrete state-spaces: a synthesis. Journal of Mathematical Psychology
99, 102447 (2020)
18. Millidge, B., Tschantz, A., Buckley, C.L.: Whence the expected free energy? (4
2020), http://arxiv.org/abs/2004.08128
19. Jang, E., Gu, S., Poole, B.: Categorical reparameterization with gumbel-softmax
(11 2016), http://arxiv.org/abs/1611.01144
12 van Bergen & Lanillos
Appendix
A Active-dSprites
Active-dSpritescanbethoughtofasan"activated"versionofthevariousmulti-
dSprites datasets that have been used in previous work on object-based visual
inference(e.g.[4,5]).Notonlydoesitincludedynamics,butthesedynamicscan
canbeactedonbyanagent.Thus,active-dSpritesisaninteractiveenvironment,
rather than a dataset.
Objectsintheactive-dSpritesenvironmentare2.5-Dshapes(squares,ellipses
and hearts): they have no depth dimension of their own, but can occlude each
otherwithinthedepthdimensionoftheimage.Whenanactive-dSpritesinstance
is intialized, object shapes, positions, sizes and colors are all sampled Uniformly
at random. Initial velocities are drawn from a Normal distribution with mean
0 and standard deviation 4 (in units of pixels). Shape colors are sampled at
discreteintervalsspanningthefullrangeofRGB-colors.Shapesarepresentedin
random depth order against a solid background with a random grayscale color.
Accelerationsinactionfields(atthoselocationsthathavebeenselectedtoincur
anon-zeroacceleration)aredrawnfromaNormaldistributionwithmean0and
s.d. of 4.
B Network architectures
The OBAI architecture discussed in this paper consists of two separate IAI
modules, each of which in turn contains a refinement and a decoder module.
The first IAI module concerns the inference of the state beliefs q({s†(k)}) – we
term this the state inference module. The second IAI module infers the object
action beliefs q({a(k)}), and we refer to this as the action inference module.
We ran inference for a total of F ×4 iterations, where F is the number of
frames in the input. Inference initially concerns just the first video frame, and
beliefsforthisframeareinitializedtoλ ,whichislearnedduringtraining.After
0
every 4 iterations, an additional frame is added to the inference window, and
the beliefs for this new frame are initialized predictively, by extrapolating the
object dynamics inferred up to that point. This procedure minimizes the risk
that object representations are swapped between slots across frames, which can
constitute a local minimum for the ELBO loss and leads to poor inference. We
trained all IAI modules with K=4 object slots.
B.1 State inference module
This module used a latent dimension of 16. Note that, in the output of the
refinementnetwork,thisnumberisdoubledonceaseachlatentbeliefisencoded
byameanandvariance,andthendoubledagainaswerepresent(andinfer)both
the states and their first-order derivatives. In the decoder, the latent dimension
isdoubledonlyonce,asthestatederivativesdonotenterintothereconstruction
Object-based active inference 13
ofavideoframe.AsinIODINE[4],weuseaspatialbroadcastdecoder,meaning
thatthelatentbeliefsarecopiedalongaspatialgridwiththesamedimensionsas
a video frame, and each latent vector is concatenated with the (x,y) coordinate
of its grid location, before passing through a stack of transposed convolution
layers. Decoder and refinement network architectures are summarized in the
tables below. The refinement network takes in 16 image-sized inputs, which are
identical to those used in IODONE [4], except that we omit the leave-one-out
likelihoods. Vector-sized inputs join the network after the convolutional stage
(which processes only the image-sized inputs), and consist of the variational
parameters and (stochastic estimates of) their gradients.
Decoder
Type Size/#Chan. Act. func. Comment
Input (λ) 32
Broadcast 34 Appends coordinate channels
ConvT 5×5 32 ELU
ConvT 5×5 32 ELU
ConvT 5×5 32 ELU
ConvT 5×5 32 ELU
ConvT 5×5 4 Outputs RGB + mask
Refinement network
Type Size/#Chan. Act. func. Comment
Linear 64
LSTM 128 tanh
Concat [...,λ,∇ L] 256 Appends vector-sized inputs
λ
Linear 128 ELU
Flatten 800
Conv 5×5 32 ELU
Conv 5×5 32 ELU
Conv 5×5 32 ELU
Inputs 16
B.2 Action inference module
The action inference module does not incorporate a decoder network, as the
quality of the action beliefs is computed by evaluating equation 7 and plugging
this into the ELBO loss from equation 8. While this requires some additional
tricks(seeAppendixD),noneuralnetworkisrequiredforthis.Thismoduledoes
includea(shallow)refinementnetwork,whichissummarizedinthetablebelow.
14 van Bergen & Lanillos
This network takes as input the current variational parameters λ (k) (2 means
a
(cid:80)
and 2 variances), their gradients, and the ‘expected object action’, mˆ ψ .
i ik i
Refinement network
Type Size/#Chan. Act. func. Comment
Linear 4
LSTM 32 tanh
Inputs 10
C Training procedure
The above network architecture was trained on pre-generated experience with
the active-dSprites environment, as described in the main text. The training
set comprised 50,000 videos of 4 frames each. An additional validation set of
10,000 videos was constructed using the same environment parameters as the
training set, but using a different random seed. Training was performed using
theADAMoptimizer[REF]withdefaultparametersandaninitiallearningrate
of3×10−4.Thislearningratewasreducedautomaticallybyafactor3whenever
the validation loss had not decreased in the last 10 training epochs, down to a
minimum learning rate of 3×10−5. Training was performed with a batch size of
64 (16 × 4 GPUs), and was deemed to have converged after 245 epochs.
C.1 Modified ELBO loss
OBAI optimizes an ELBO loss for both learning and inference. The basic form
of this loss is given by equation 8. In practice, we modify this loss in two ways
(similar to previous work, e.g. [4]). First, we re-weight the reconstruction term
in the ELBO loss as follows:
T (cid:34)
L =− (cid:88) H (cid:16) q (cid:16) {s†(k),a(k)} (cid:17)(cid:17) +βE [logp(o |{s(k)})]
β t t q({s(k)}) t t
t
t=0
(cid:35)
+ (cid:88) E q(a( t k)) [logp(a( t k)|Ψ t )]+ (cid:88) E q (cid:16) s† t (k),s† t− (k 1 ),a( t k − ) 1 (cid:17)[logp(s† t (k)|s† t− (k 1 ),a( t− k) 1 )]
k k
(15)
Second, we train the network to minimize not just the loss at the end of the
inferenceiterationsthroughthenetwork,butacompositelossthatalsoincludes
the loss after earlier iterations. Let L(n) be the loss after n inference iterations,
β
then the composite loss is given by:
L =
N (cid:88)iter n
L(n) (16)
comp N β
iter
n=1
Object-based active inference 15
C.2 Hyperparameters
OBAI includes a total of 4 hyperparameters: (1) the loss-reweighting coefficient
β (seeabove);(2)thevarianceofthepixelsaroundtheirpredictedvalues,σ2;(3)
o
the variance of the noise in the latent space dynamics, σ2; and (4) the variance
s
of the noise in the object actions, σ2. The results described in the current work
ψ
were achieved with the following settings:
Param. Value
β 5.0
σ 0.3
o
σ 0.1
s
σ 0.3
ψ
D Computing E [logp(a(k)|Ψ)]
q(a(k))
The expectation under q(a(k)) of logp(a(k)|Ψ), which appears in the ELBO loss
(eq. 8), cannot be computed in closed form, because the latter log probability
requires us to marginalize over all possible configurations of the pixel-to-object
assignments, and to do so inside of the logarithm. That is:
(cid:88) (cid:16) (cid:17)
logp(a(k)|Ψ)= log p(a(k)|Ψ,m)p(m|{s(k)}) (17)
m
(cid:16) (cid:17)
=log E [p(a(k)|Ψ,m)] (18)
p(m|{s(k)})
However, note that within the ELBO loss, we want to maximize the expected
value of this quantity (as its negative appears in the ELBO, which we want to
minimize). From Jensen’s inequality, we have:
(cid:16) (cid:17)
E [logp(a(k)|Ψ,m)]≤log E [p(a(k)|Ψ,m)] (19)
p(m|{s(k)}) p(m|{s(k)})
Therefore, the l.h.s. of this equation provides a lower bound on the quantity we
wanttomaximize.Thus,wecanapproximateourgoalbymaximizingthislower
boundinstead.Thisisconvenient,becausethislowerbound,anditsexpectation
under q(a(k)) can be approximated through sampling:
E (cid:104) E [logp(a(k)|Ψ,m)] (cid:105) ≈ 1 (cid:88) logp(a(k)∗|Ψ,m∗) (20)
q(a(k)) p(m|{s(k)}) N j j
samples
j
(cid:32) (cid:33)
= 1 (cid:88) logN a(k)∗; (cid:88) mˆ∗(i)ψ ,σ2I (21)
N j jk i ψ
samples
j i
mˆ∗(i) ∼p(m |{s(k)}), a(k)∗ ∼q(a(k)), s(k)∗ ∼q(s(k)) (22)
j i j j
where we slightly abuse notation in the sampling of the pixel assignments, as
a vector is sampled from a distribution over a categorical variable. The reason
16 van Bergen & Lanillos
this results in a vector is because this sampling step uses the Gumbel-Softmax
trick [19], which is a differentiable method for sampling categorical variables as
"approximately one-hot" vectors. Thus, for every pixel i, we sample a vector
mˆ∗(i), such that the k-th entry of this vector, mˆ∗(i), denotes the "soft-binary"
j jk
conditionofwhetherpixelibelongstoobjectk.Inpractice,weuseN =1,
samples
based on the intuition that this will still yield a good approximation over many
training instances, and that we rely on the refinement network to learn to infer
good beliefs. The Gumbel-Softmax sampling method depends on a temperature
τ, which we gradually reduce across training epochs, so that the samples grad-
ually better approximate the ideal one-hot vectors.
Itisworthnotingthat,astheentropyofp(m|{s(k)})decreases(i.e.asobject
slots "become more certain" about which pixels are theirs), the bound in equa-
tion 19 becomes tighter. In the limit as the entropy becomes 0, the network is
perfectly certain about the pixel assignments, and so the distribution collapses
toapointmass.Theexpectationthenbecomestrivial,andsothetwosidesofeq.
19becomeequal.Samplingthepixelassignmentsisequallytrivialinthiscase,as
the distribution has collapsed to permit only a single value for each assignment.
Inshort,atthisextremepoint,theprocedurebecomesentirelydeterministic.In
ourdata,wetypicallyobserveverylowentropyforp(m|{s(k)}),andsowelikely
operate in a regime close to the deterministic one, where the approximation is
very accurate.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Object-based active inference"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
