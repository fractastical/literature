=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: AXIOM: Learning to Play Games in Minutes with Expanding Object-Centric Models
Citation Key: heins2025axiom
Authors: Conor Heins, Toon Van de Maele, Alexander Tschantz

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Abstract: Currentdeepreinforcementlearning(DRL)approachesachievestate-of-the-art
performance in various domains, but struggle with data efficiency compared to
humanlearning,whichleveragescorepriorsaboutobjectsandtheirinteractions.
Activeinferenceoffersaprincipledframeworkforintegratingsensoryinformation
withpriorknowledgetolearnaworldmodelandquantifytheuncertaintyofitsown
beliefsandpredictions. However,activeinferencemodelsareusuallycraftedfora
singletaskwithbespokeknowledge,sotheylackthedomainflexibility...

Key Terms: object, play, centric, models, axiom, expanding, learning, games, vixra, minutes

=== FULL PAPER TEXT ===

5202
yaM
03
]IA.sc[
1v48742.5052:viXra
AXIOM: Learning to Play Games in Minutes with
Expanding Object-Centric Models
ConorHeins1‚àó ToonVandeMaele1‚àó AlexanderTschantz1,2‚àó
HampusLinander1 DimitrijeMarkovic3 TommasoSalvatori1 CorradoPezzato1
OzanCatal1 RanWei1 MagnusKoudahl1 MarcoPerin1 KarlFriston1,4
TimVerbelen1 ChristopherLBuckley1,2
1VERSESAI
2UniversityofSussex,DepartmentofInformatics
3TechnischeUniversit√§tDresden,FacultyofPsychology
4UniversityCollegeLondon,QueenSquareInstituteofNeurology
{conor.heins,toon.vandemaele,alec.tschantz}@verses.ai
Abstract
Currentdeepreinforcementlearning(DRL)approachesachievestate-of-the-art
performance in various domains, but struggle with data efficiency compared to
humanlearning,whichleveragescorepriorsaboutobjectsandtheirinteractions.
Activeinferenceoffersaprincipledframeworkforintegratingsensoryinformation
withpriorknowledgetolearnaworldmodelandquantifytheuncertaintyofitsown
beliefsandpredictions. However,activeinferencemodelsareusuallycraftedfora
singletaskwithbespokeknowledge,sotheylackthedomainflexibilitytypicalof
DRLapproaches.Tobridgethisgap,weproposeanovelarchitecturethatintegrates
aminimalyetexpressivesetofcorepriorsaboutobject-centricdynamicsandinter-
actionstoacceleratelearninginlow-dataregimes. Theresultingapproach,which
wecallAXIOM,combinestheusualdataefficiencyandinterpretabilityofBayesian
approacheswiththeacross-taskgeneralizationusuallyassociatedwithDRL.AX-
IOMrepresentsscenesascompositionsofobjects,whosedynamicsaremodeled
aspiecewiselineartrajectoriesthatcapturesparseobject-objectinteractions. The
structureofthegenerativemodelisexpandedonlinebygrowingandlearningmix-
turemodelsfromsingleeventsandperiodicallyrefinedthroughBayesianmodel
reductiontoinducegeneralization. AXIOMmastersvariousgameswithinonly
10,000interactionsteps,withbothasmallnumberofparameterscomparedtoDRL,
andwithoutthecomputationalexpenseofgradient-basedoptimization.
1 Introduction
Reinforcementlearning(RL)hasachievedremarkablesuccessasaflexibleframeworkformastering
complextasks. However,currentmethodshaveseveraldrawbacks: theyrequirelargeamountsof
trainingdata,dependonlargereplaybuffers,andfocusonmaximizingcumulativerewardwithout
structuredexploration[1]. Thiscontrastswithhumanlearning,whichreliesoncorepriorstoquickly
generalize to novel tasks [2‚Äì4]. Core priors represent fundamental organizational principles - or
hyperpriors-thatshapeperceptionandlearning,providingthescaffoldinguponwhichmorecomplex
knowledgestructuresarebuilt. Forexample,suchpriorsallowhumanstointuitivelyunderstandthat
objectsfollowsmoothtrajectoriesunlessexternalforcesintervene,andshapeourcausalreasoning,
helpingustograspaction-consequencerelationships[5‚Äì8]. Describingvisualscenesasfactorized
intoobjectshasshownpromiseinsampleefficiency,generalization,androbustnessonvarioustasks
[9‚Äì14]. ThesechallengesarenaturallyaddressedbyBayesianagentarchitectures,suchasactive
inference[15],thatprovideaprincipledframeworkforincorporatingpriorknowledgeintomodels,
Preprint.Underreview.
üéÆ
Object-Object
Interactions
Identity üèÜ
MiMoMdel rMM
sMM tMM
Figure1: InferenceandpredictionflowusingAXIOM:ThesMMextractsobject-centricrepre-
sentationsfrompixelinputs. Foreachobjectlatentanditsclosestinteractingcounterpart,adiscrete
identitytokenisinferredusingtheiMMandpassedtotherMM,alongwiththedistanceandthe
action,topredictthenextrewardandthetMMswitch. Theobjectlatentsarethenupdatedusingthe
tMMandthepredictedswitchtogeneratethenextstateforallobjects. (a)Projectionoftheobject
latentsintoimagespace. (b)Projectionofthekthlatentwhosedynamicsarebeingpredictedand(c)
ofitsinteractionpartner. (d)ProjectionoftherMMinimagespace;eachofthevisualizedclusters
correspondstoaparticularlineardynamicalsystemfromthetMM.(e)Projectionofthepredicted
latents. Thepastlatentsattimetareshowningray.
supportingcontinualadaptationwithoutcatastrophicforgetting. Ithasbeenarguedthatthisapproach
alignscloselywithhumancognitiveprocesses[16,17],wherebeliefsareupdatedincrementallyas
newevidenceemerges. Yet,despitethesetheoreticaladvantages,applicationsofactiveinference
havetypicallybeenconfinedtosmall-scaletaskswithcarefullydesignedpriors,failingtoachievethe
versatilitythatmakesdeepRLsopowerfulacrossdiversedomains.
Tobridgethisgap,weproposeanovelactiveinferencearchitecturethatintegratesaminimalyet
expressivesetofcorepriorsaboutobjectsandtheirinteractions[9‚Äì12,18]. Specifically,wepresent
AXIOM(ActiveeXpandingInferencewithObject-centricModels),whichemploysaobject-centric
statespacemodelwiththreekeycomponents: (1)aGaussianmixturemodelthatparsesvisualinput
intoobject-centricrepresentationsandautomaticallyexpandstoaccommodatenewobjects;(2)a
transitionmixturemodelthatdiscoversmotionprototypes(e.g.,falling,sliding,bouncing)[19]and
(3)asparserelationalmixturemodelovermulti-objectlatentfeatures, learningcausallyrelevant
interactionsasjointlydrivenbyobjectstates,actions,rewards,anddynamicalmodes. AXIOM‚Äôs
learningalgorithmoffersthreekindsofefficiency: first,itlearnssequentiallyoneframeatatime
with variational Bayesian updating [20]. This eliminates the need for replay buffers or gradient
computations,andenablesonlineadaptationtochangesinthedatadistribution. Second,itsmixture
architecturefacilitatesfaststructurelearningbybothaddingnewmixturecomponentswhenexisting
onescannotexplainnewdata, andmergingredundantonestoreducemodelcomplexity[21‚Äì24].
Finally, by maintaining posteriors over parameters, AXIOM can augment policy selection with
information-seekingobjectivesandthusuncertainty-awareexploration[15].
To empirically validate our model, we introduce the Gameworld 10k benchmark, a new set of
environmentsdesignedtoevaluatehowefficientlyanagentcanplaydifferentpixel-basedgames
in 10k interactions. Many existing RL benchmarks, such as the Arcade Learning Environment
(ALE)[25]orMuJoCo[26]domains,emphasizelong-horizoncreditassignment,complexphysics,
orvisualcomplexity. Thesefactorsoftenobscurecorechallengesinfastlearningandgeneralization,
especiallyunderstructureddynamics. Tothisend,eachofthegamesinGameworld 10kfollowsa
similar,object-focusedpattern: multipleobjectspopulatingavisualscene,aplayerobjectthatcan
becontrolledtoscorepoints,andobjectsfollowingcontinuoustrajectorieswithsparseinteraction
mechanics. Weformulateasetof10gameswithdeliberatelysimplifiedvisualelements(singlecolor
spritesofdifferentshapesandsizes)tofocusthecurrentworkontherepresentationalmechanisms
usedformodelingdynamicsandcontrol,ratherthanlearninganoverly-expressivemodelforobject
segmentation. The Gameworld environments also enable precise control of game features and
2
Aviate Bounce Cross Drive Explode Fruits Gold Hunt Impact Jump
Figure2: Gameworld10k: Visualimpressionofthe10gamesintheGameworld 10ksuite. Se-
quencesoftenframesareoverlayedwithincreasingopacitytoshowcasethegamedynamics.
dynamics,whichallowstestinghowsystemsadapttosparseinterventionstothecausalorvisual
structure of the game, e.g., the shape and color of game objects. On this benchmark, our agent
outperforms popular reinforcement learning models in the low-data regime (10,000 interaction
steps)withoutrelyingonanykindofgradient-basedoptimization. Toconclude,althoughwehave
not deployed AXIOM at the scale of complicated control tasks typical of the RL literature, our
resultsrepresentameaningfulsteptowardbuildingagentscapableofbuildingcompact,interpretable
worldmodelsandexploitingthemforrapiddecision-makingacrossdifferentdomains. Ourmain
contributionsarethefollowing:
‚Ä¢ WeintroduceAXIOM,anovelobject-centricactiveinferenceagentthatislearnedonline,inter-
pretable,sampleefficient,adaptableandcomputationallycheap.1
‚Ä¢ TodemonstratetheefficacyofAXIOM,weintroduceanew,modifiablebenchmarksuitetargeting
sample-efficientlearninginenvironmentswithobjectsandsparseinteractions.
‚Ä¢ Weshowthatourgradient-freemethodcanoutperformstate-of-the-artdeeplearningmethodsboth
intermsofsampleefficiencyandabsoluteperformance,withouronlinelearningschemeshowing
robustnesstoenvironmentalperturbations.
2 Methods
AXIOMisformulatedinthecontextofapartiallyobservableMarkovdecisionprocess(POMDP).
(cid:0) (cid:1)
Ateachtimestept,thehiddenstateh evolvesaccordingtoh ‚àº P h | h ,a ,wherea is
t t t t‚Äì1 t‚àí1 t
theactiontakenattimet. Theagentdoesnotobserveh directlybutinsteadreceivesanobservation
(cid:0) (cid:1) (cid:0) (cid:1) t
y ‚àº P y | h ,andarewardr ‚àº P r | h ,a . AXIOMlearnsanobject-centricstatespace
t t t t t t t
model by maximizing the Bayesian model evidence‚Äîequivalently, minimizing (expected) free
energy‚Äîthroughactiveinteractionwiththeenvironment[15]. Themodelfactorizesperceptionand
dynamicsintoseparategenerativeblocks: (i)Inperception,aslotMixtureModel(sMM)explains
pixelswithcompetitionbetweenobject-centriclatentvariablesO ={O(1),...,O(K)},associating
t t t
eachpixeltooneofK slotsusingtheassignmentvariablez ;(ii)dynamicsaremodeledper-
t,smm
object using their object-centric latent descriptions as inputs to a recurrent switching state space
model(similartoanrSLDS[19]). WedefinethefulllatentsequenceasZ = {O ,z }T .
0:T t t,smm t=0
EachslotlatentO(k)consistsofbothcontinuousx(k)anddiscretelatentvariables. Thecontinuous
t t
latentsrepresentpropertiesofanobject,suchasitsposition,colorandshape. Thediscretelatents
themselvesaresplitintotwosubtypes: z(k) ands(k). Weusez(k) todenotelatentdescriptorsthat
t t t
capturecategoricalattributesoftheslot(e.g.,objecttype),ands(k)todenoteapairofswitchstates
t
determiningtheslot‚Äôsinstantaneoustrajectory.2 ModelparametersŒòÀú aresplitintomodule-specific
subsets(e.g., Œò ,Œò ). Thejointdistributionoverinputsequencesy ,latentstatesequences
sMM tMM 0:T
Z andparametersŒòÀú canbeexpressedasahiddenMarkovmodel:
0:T
T
p(y ,Z ,ŒòÀú)=p(y ,Z )p(ŒòÀú) (cid:89) p(x ,|z ,Œò )p(x ,z ,s ,a ,r |Œò )
0:T 0:T 0 0 t‚Äì1 t iMM t‚Äì1 t t t‚Äì1 t rMM
(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
t=1
Identitymixturemodel Recurrentmixturemodel
K
(cid:89) p(y |x(k),z ,Œò )p(x(k) |x(k) ,s(k),Œò ), (1)
t t t,smm sMM t t‚àí1 t tMM
k=1 (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
Slotmixturemodel Transitionmixturemodel
1ThecodefortrainingAXIOMisavailableathttps://github.com/VersesTech/axiom
2Weusethesuperscriptindexkasinq(k)toselectonlythesubsetofq‚â°q(1:K)relevanttothekthslot.
3
wherep(ŒòÀú) = p(Œò )p(Œò )p(Œò )p(Œò ). ThesMMp(y |x ,z ,Œò )isalikeli-
sMM iMM rMM tMM t t t,smm sMM
hoodmodelthatexplainspixeldatausingmixturesofslot-specificlatentstates(seeschematicin
Figure1). Theidentitymixturemodel(iMM)p(x |z ,Œò )isalikelihoodmodelthatassigns
t‚Äì1 t iMM
eachobject-centriclatenttooneofasetofdiscreteobjecttypes. Thetransitionmixturemodel(tMM)
p(x(k)|x(k),s(k),Œò )describeseachobject‚Äôslatentdynamicsasapiecewiselinearfunctionof
t t‚Äì1 t tMM
itsownstate. Finally,therecurrentmixturemodel(rMM)p(x ,z ,s ,a ,r |Œò )modelsthe
t‚Äì1 t t t‚Äì1 t rMM
dependenciesbetweenmulti-objectlatentstates(liketheswitchstatesofthetransitionmixture),other
globalgamestateslikerewardr,actiona,andthecontinuousanddiscretefeaturesofeachobject.
ThismoduleiswhatallowsAXIOMtomodelsparseinteractionsbetweenobjects(e.g.,collisions),
whilestilltreatingeachslot‚Äôsdynamicsasconditionally-independentgiventheswitchstatess(k).
t
SlotMixtureModel(sMM). AXIOMprocessessequencesofRGBimagesoneframeatatime. Each
imageiscomposedofH √óW pixelsandisreshapedintoN=HW tokens{yn}N . Eachtoken
t n=1
yn is a vector containing the nth pixel‚Äôs color in RGB and its image coordinates (normalized to
t
(cid:2) (cid:3)
‚àí1,+1 ). AXIOMmodelsthesetokensatagiventimeasexplainedbyamixtureofthecontinuous
slotlatents;wetermthislikelihoodconstructiontheSlotMixtureModel(sMM,seefarleftsideof
Figure1). TheK componentsofthismixturemodelareGaussiandistributionswhoseparameters
aredirectlygivenbythecontinuousfeaturesofeachslotlatentx(1:K).
AssociatedtothisGaussian
t
mixture is a binary assignment variable zn ‚àà {0,1} indicating whether pixel n at time t is
t,k,smm
drivenbyslotk,withtheconstraintthat (cid:80) zn =1. ThesMM‚Äôslikelihoodmodelforasingle
k t,k,smm
pixelandtimepointyncanbeexpressedasfollows(droppingthetsubscriptfornotationalclarity):
t
K
p(yn |x(k),œÉ c (k),z k n ,smm )= (cid:89) N(Ax(k),diag( (cid:2) Bx(k),œÉ c (k)(cid:3)‚ä§ ))z k n ,smm. (2)
k=1
ThemeanofeachGaussiancomponentisgivenafixedlinearprojectionAofeachobjectlatent,
whichselectsonlyitspositionandcolorfeatures: Ax(k) = (cid:2) p(k),c(k)(cid:3) . Thecovarianceofeach
componentisadiagonalmatrixwhosediagonalisaprojectionofthe2-Dshapeoftheobjectlatent
Bx(k) = e(k) (its spatial extent in the X and Y directions), stacked on top of a fixed variance
foreachcolordimensionœÉ(k), whicharegivenindependentGammapriors. Thelatentvariables
c
p(k),c(k),e(k) aresubsetsofslotk‚Äôsfullcontinuousfeaturesx(k),andtheprojectionmatricesA,
t
B arefixed, unlearnedparameters. Eachtoken‚Äôsslotindicatorzn isdrawnfromaCategorical
smm
distribution zn | œÄ ‚àº Cat(œÄ ) with mixing weights œÄ . We place a truncated stick-
smm smm smm smm
breaking (finite GEM) prior on these weights, which is equivalent to a K-dimensional Dirichlet
withconcentrationvector(1,...,1,Œ± ),wherethefirstK‚àí1pseudcountsare1andthefinal
0,smm
pseudocount Œ± reflects the propensity to add new slots. All subsequent mixture models in
0,smm
AXIOMareequippedwiththesamesortoftruncatedstick-breakingpriorsonthemixingweights
[27].
IdentityMixtureModel(iMM). AXIOMusesanidentitymixturemodel(iMM)toinferadiscrete
identitycodez(k) foreachobjectbasedonitscontinuousfeatures. Theseidentitycodesareusedto
type
conditiontheinferenceoftherecurrentmixturemodelusedfordynamicsprediction. Conditioning
thedynamicsonidentity-codesinthisway,ratherthanlearningaseparatedynamicsmodelforeach
slot,allowsAXIOMtousethesamedynamicsmodelacrossslots. Thisalsoenablesthemodelto
learnthesamedynamicsinatype-specific,ratherthaninstance-specific,manner[28],andtoremap
identitieswhene.g.,theenvironmentisperturbedandcolorschange. Concretely,theiMMmodels
the5-Dcolorsandshapes{c(k),e(k)}K acrossslotsasamixtureofuptoV Gaussiancomponents
k=1
(objecttypes). Theslot-levelassignmentvariablez(k) indicateswhichidentityisassignedtothe
t,type
kthslot. ThegenerativemodelfortheiMMis(omittingthet‚àí1subscriptfromlatentvariables):
V
p( (cid:2) c(k),e(k)(cid:3)‚ä§ |z( ty k p ) e ,¬µ 1:V,type ,Œ£ 1:V,type )= (cid:89) N(¬µ j,type ,Œ£ j,type )z j ( , k ty ) pe (3)
j=1
p(¬µ ,Œ£‚àí1 )=NIW(m ,Œ∫ ,U ,n ) (4)
j,type j,type j,type j,type j,type j,type
The same type of Categorical likelihood for the type assignments z(k) | œÄ ‚àº Cat(œÄ ) and
type type type
(cid:0) (cid:1)
truncated stick-breaking prior Dir 1,...,1,Œ± over the mixture weights is used to allow an
0,type
4
arbitrary(uptoamaximumofV)numberoftypestobeusedtoexplainthecontinuousslotfeatures.
WeequiptheprioroverthecomponentlikelihoodparameterswithconjugateNormalInverseWishart
(NIW)priors.
TransitionMixtureModel(tMM). Thedynamicsofeachslotaremodelledasamixtureoflinear
functions of the slot‚Äôs own previous state. To stress the homology between this model and the
othermodulesofAXIOM,werefertothismoduleasthetransitionmixturemodelortMM,butthis
formulationismorecommonlyalsoknownasaswitchinglineardynamicalsystemorSLDS[29].
ThetMM‚Äôsswitchvariables(k) selectsasetoflinearparametersD ,b todescribethekthslot‚Äôs
t,tmm l l
trajectoryfromttot+1. Eachlinearsystemcapturesadistinctrigidmotionpatternforaparticular
object(e.g.,‚Äúballinfreeflight‚Äù,‚Äúpaddlemovingleft‚Äù).
L
p(x( t k) |x( t‚Äì k 1 ),s( t, k tm ) m ,D 1:L ,b 1:L )= (cid:89) N(D l x( t k)+b l ,2I)s( t k ,l ) ,tmm (5)
l=1
wherewefixthecovarianceofallLcomponentstobe2I,andallmixturelikelihoodsD ,b to
1:L 1:L
haveuniformpriors.ThemixingweightsœÄ fors(k) asbeforearegivenatruncatedstick-breaking
tmm t,tmm
(cid:0)
priorDir 1,...,1,Œ± )enablingthenumberoflinearmodesLtobedynamicallyadjustedtothe
0,tmm
databygrowingthemodelwithpropensityŒ± . Importantly,theLtransitioncomponentsofthe
0,tmm
tMMarenotslot-dependent,butaresharedandthuslearnedacrossallK slotlatents. ThetMMcan
thusexplainandpredictthemotionofdifferentobjectsusingashared,expandingsetofdynamical
motifs. Aswewillseeinthenextsection,interactionsbetweenobjectsaremodelledbyconditioning
s(k) onthestatesofotherobjects.
t,tmm
RecurrentMixtureModel(rMM). AXIOMemploysarecurrentmixturemodel(rMM)toinferthe
switchstatesofthetransitionmodeldirectlyfromcurrentslot-levelfeatures. Thisdependenceof
switch states on continuous features is the same construct used in the recurrent switching linear
dynamicalsystemorrSLDS[19]. However,liketherSLDS,whichusesadiscriminativemappingto
infertheswitchstatefromthecontinuousstate,rMMrecoversthisdependencegenerativelyusing
amixturemodelovermixedcontinuous‚Äìdiscreteslotstates[30]. Concretely,therMMmodelsthe
distributionofcontinuousanddiscretevariablesasamixturemodeldrivenbyanotherper-slotlatent
assignment variable s(k) . The rMM‚Äôs defines a mixture likelihood over continuous and discrete
rmm
slot-specificinformation: (f(k),d(k)). Thecontinuousslotfeaturesf(k)areafunctionofofboththe
t‚Äì1 t‚Äì1 t‚Äì1
kthslot‚Äôsowncontinuousstatestatex(k)aswellasthestatesofotherslotsx(1:K),suchasthedistance
t‚Äì1 t‚Äì1
totheclosestobject. Thediscretefeaturesincludecategoricalslotfeaturesliketheidentityofthe
closestobject,theswitchstateassociatedwiththetransitionmixturemodel,andtheactionandreward
atthecurrenttimestep: d(k) =(z(k),s(k) ,a ,r ). TherMMassignmentvariableassociatedto
t‚Äì1 t‚Äì1 t‚Äì1,tmm t‚Äì1 t
agivenslotisabinaryvectors(k) whosemthentrys(k) ‚àà{0,1}indicateswhethercomponent
t,rmm t,m,rmm
mexplainsthecurrenttupleofmixedcontinuous-discretedata. Eachcomponentlikelihoodselected
bys(k) factorizesintoaproductofcontinuous(Gaussian)anddiscrete(Categorical)likelihoods.
t,rmm
(cid:16) (cid:17) (cid:16) (cid:17)
f t ( ‚Äì k 1 ) = Cx t (k ‚Äì1 ),g(x( t 1 ‚Äì1 :K)) , d( t k ‚Äì1 ) = z( t k ‚Äì1 ),s( t, k tm ) m ,a t‚Äì1 ,r t (6)
M
Ô£Æ Ô£πst,m,rmm
p(f t ( ‚Äì k 1 ),d( t k ‚Äì1 ) |s( t, k r ) mm )= (cid:89) Ô£∞N (cid:0) f t ( ‚Äì k 1 );¬µ m,rmm ,Œ£ m,rmm (cid:1)(cid:89) Cat (cid:0) d t‚Äì1,i ;Œ± m,i (cid:1) Ô£ª (7)
m=1 i
wherethematrixC isaprojectionmatrixthatselectsasubsetofslotk‚Äôscontinuousfeaturesare
used,andg(x(1:K))summarizesfunctionsthatcomputeslot-to-slotinteractionfeatures,suchasthe
t‚Äì1
X and Y-displacement to the nearest object, the identity code associated with the nearest object
(andotherfeaturesdetailedinAppendixA).AswithalltheothermodulesofAXIOM,weequip
themixingweightsfors(k) withatruncatedstick-breakingpriorwhosefinalMth pseudocount
t,rmm
parametertunesthepropensitytoaddnewrMMcomponents. WeexploredanablationoftherMM
(fixed_distance)wheretheX
andY-displacementvectorisnotreturnedbyg(x(1:K));rather,the
t‚àí1
distancethattriggersdetectionofthenearestinteractingobjectisafixedhyperparameteroftheg
function. Thishyperparametercantobetunedtoattainhigherrewardonmostenvironmentsthanthe
5
standardmodelwheretherMMlearnsthedistanceonline. However,itcomesatthecostofhavingto
tunethishyperparamterinanenvironment-specificfashion(seeFigure3andTable1fortheeffectof
thefixed_distanceablationonperformance).
Variationalinference. AXIOMusesvariationalinferencetoperformstateinferenceandparameter
learning. Briefly,thisrequiresupdatinganapproximateposteriordistributionq(Z ,ŒòÀú)overlatent
0:T
variables and parameters to minimize the variational free energy F, an upper bound on negative
log evidence F ‚â• ‚àílogp(y ). In doing so, the variational posterior approximates the true
0:T
posteriorp(Z ,ŒòÀú |y )fromexactbutintractableBayesianinference. Weenforceindependence
0:T 0:T
assumptionsinthevariationalposterioroverseveralfactors: acrossstatesandparameters,acrossthe
K slotlatents,andovertimeT. Thisisknownasthemean-fieldapproximation:
Ô£´ Ô£∂Ô£´ Ô£∂
T N K
q(Z 0:T ,ŒòÀú)=q(ŒòÀú) (cid:89) Ô£≠ (cid:89) q(zn t,sMM )Ô£∏Ô£≠ (cid:89) q(O t (k))Ô£∏ (8)
t=0 n=1 k=1
q(O(k))=q(x(k))q(z(k))q(s(k)), q(ŒòÀú)=q(Œò )q(Œò )q(Œò )q(Œò ) (9)
t t t t sMM iMM tMM rMM
NotethatthemixturevariableofthesMMz isfactoredoutoftheotherobject-centriclatentsin
t,smm
boththegenerativemodelandtheposteriorbecauseunliketheotherdiscretelatentsz(k),itisnot
t
independentacrossK slots.
Weupdatetheposterioroverlatentstatesq(Z )(i.e.,thevariationalE-step)usingasimpleformof
0:T
forward-onlyfilteringandupdateparametersusingcoordinateascentvariationalinference,usingthe
sufficientstatisticsofthelatentsupdatedduringfilteringandthedatatoupdatetheparametersusing
simplenaturalparameterupdates. ThesevariationalE-Mupdatesarerunoncepertimestep,thus
implementingafast,streamingformofcoordinate-ascentvariationalinference[31,32]. Thesimple,
gradient-free form of these updates inherits from the exponential-family form of all the mixture
modelsusedinAXIOM.
2.1 Growingandpruningthemodel
Faststructurelearning. Inthespiritoffaststructurelearning[23],AXIOMdynamicallyexpands
allfourmixturemodules(sMM,iMM,tMM,rMM)usinganonlinegrowingheuristic: processeach
newdatapointsequentially,decidewhetheritisbestexplainedbyanexistingcomponentorwhether
anewcomponentshouldbecreated,andthenupdatetheselectedcomponent‚Äôsparameters. Wefixa
maximumnumberofcomponentsC foreachmixturemodelandletC ‚â§C bethenumber
max t‚àí1 max
currentlyinuse. ForeachcomponentcwestoreitsvariationalparametersŒò ,whereforaparticular
c
modelthismightbeasetofNormalInverseWishartparameters,e.g. Œò ={m ,Œ∫ ,U ,n }.
c,iMM c c c c
Uponobservinganewinputy ,wecomputeforeachcomponentc = 1,...,C thevariational
posterior‚Äìpredictive log‚Äìdensi t ty ‚Ñì = E (cid:2) logp(y | Œò ) (cid:3) . The truncat t e ‚àí d 1 stick‚Äìbreaking
t,c q(Œòc) t c
priorœÄ ‚àº Dir(1,...,1,Œ±)thendefinesa‚Äúnew‚Äìcomponent‚ÄùthresholdœÑ = logp (y ) + logŒ±
t 0 t
wherep isthepriorpredictivedensityunderanemptycomponent.Weselectthecomponentwith
0
highestscore,c‚àó = argmax ‚Ñì andhard-assigny toc‚àóif‚Ñì ‚â•œÑ ;otherwise‚Äîprovided
c‚â§Ct‚àí1 t,c t t,c‚àó t
C <C ‚Äîweinstantiateanewcomponentandassigny toit.Finally,giventhehardassignment
t‚àí1 max t
z ,weupdatethechosencomponent‚ÄôsparametersviaavariationalM-step(coordinateascent).The
t
last weight in the Dirichlet absorbs any remaining mass, so
(cid:80)CmaxœÄ
= 1.) This algorithm is a
c=1 c
deterministic,maximumaposterioriversionoftheCRPassignmentrule(seeEquation(8)of[27]).
The expansion threshold œÑ plays the role of the Dirichlet Process concentration Œ±. When Œ± is
t
smallthemodelprefersexplainingdatawithexistingslots;largerŒ±makesgrowthmorelikely. The
procedure is identical for the sMM, iMM, tMM and rMM‚Äîonly the form of p(¬∑ | Œò ) and the
c
model-specificcapsoncomponentsC andexpansionthresholdsœÑ differ.
max t
BayesianModelReduction(BMR). Every‚àÜT =500frameswesampleupton =2000used
BMR pair
rMMcomponents,scoretheirmutualexpectedlog-likelihoodswithrespecttodatageneratedfrom
themodelthroughancestralsampling,andgreedilytestmergecandidates. Amergeisacceptedif
itdecreasestheexpectedfreeenergyofthemultinomialdistributionsoverrewardandnexttMM
switch,conditionedonthesampleddatafortheremainingvariables;otherwiseitisrolledback. BMR
enablesAXIOMtogeneralizedynamicsfromsingleevents,forexamplelearningthatnegativereward
isobtainedwhenaballhitsthebottomofthescreen,bymergingmultiplesingleeventclusters(see
Section3,Figure4a).
6
Table 1: Cumulative reward over 10k steps for Gameworld 10k environments. Cumulative
rewardisreportedasmean¬±stdover10modelseeds. ItalicmeansAXIOMisbetterthanBBFand
Dreamer,boldisoverallbest.
Game AXIOM BBF DreamerV3 AXIOM(fixeddist.) AXIOM(noBMR) AXIOM(noIG)
Aviate ‚àí90¬±19 ‚àí90¬±05 ‚àí114¬±20 ‚àí76¬±13 ‚àí87¬±12 ‚àí71¬±16
Bounce 27¬±13 ‚àí1¬±15 14¬±16 34¬±12 8¬±03 ‚Üì 8¬±19
Cross ‚àí68¬±36 ‚àí48¬±07 ‚àí27¬±08 ‚àí18¬±21 ‚Üë ‚àí34¬±25 ‚àí7¬±03
Drive ‚àí49¬±04 ‚àí37¬±06 ‚àí45¬±06 ‚àí22¬±04 ‚Üë ‚àí67¬±03 ‚Üì ‚àí32¬±02 ‚Üë
Explode 180¬±30 101¬±13 35¬±59 234¬±16 ‚Üë 165¬±14 190¬±16
Fruits 182¬±21 86¬±15 60¬±07 209¬±19 141¬±19 ‚Üì 200¬±20
Gold 190¬±18 ‚àí26¬±12 ‚àí21¬±10 189¬±16 45¬±15 ‚Üì 207¬±17
Hunt 206¬±20 4¬±12 6¬±09 231¬±28 48¬±13 ‚Üì 216¬±11
Impact 189¬±45 122¬±20 168¬±83 192¬±09 197¬±21 181¬±72
Jump ‚àí55¬±09 ‚àí96¬±17 ‚àí55¬±17 ‚àí38¬±25 ‚àí45¬±05 ‚Üë ‚àí43¬±26
2.2 Planning
AXIOMusesactiveinferenceforplanning[33];itrollsoutfuturetrajectoriesconditionedondifferent
policies(sequencesofactions)andthendoesinferenceaboutpoliciesusingtheexpectedfreeenergy,
wherethechosenpolicyœÄ‚àóisthatwhichminimizestheexpectedfreeenergy:
H
œÄ‚àó =argmin (cid:88) ‚àí (cid:0)E [logp(r |O ,œÄ)‚àíD (q(Œ± |O ,œÄ)‚à•q(Œ± ))] (cid:1) (10)
œÄ
q(OœÑ|œÄ)
(cid:124) (cid:123)
œÑ
(cid:122)
œÑ
(cid:125) (cid:124)
KL rmm
(cid:123)
œÑ
(cid:122)
rmm
(cid:125)
œÑ=0
Utility Informationgain(IG)
Theexpectedper-timesteputilityE [logp(r |O ,œÄ)]isevaluatedusingthelearnedmodeland
q(OœÑ|œÄ) œÑ œÑ
slotlatentsatthetimeofplanning,andaccumulatedovertimestepsintotheplanninghorizon. The
expectedinformationgain(secondtermonRHSofEquation(10))iscomputedusingtheposterior
DirichletcountsoftherMMandscoreshowmuchinformationaboutrMMswitchstateswouldbe
gainedbytakingthepolicyunderconsideration.MoredetailsonplanningaregiveninAppendixA.11.
3 Results
ToevaluateAXIOM,wecompareitsperformanceonGameworldagainsttwostate-of-the-artbaselines
onsample-efficient,pixel-baseddeepreinforcementlearning: BBFandDreamerV3.
Benchmark. TheGameworldenvironmentsaredesignedtobesolvablebyhumanlearnerswithin
minutes,ensuringthatlearningdoesnothingeonbrittleexplorationorcomplexcreditassignment.
The suite includes 10 diverse games generated with the aid of a large language model, drawing
0.000
-0.020
0 10000
)K1(
draweR
egarevA
Aviate Bounce Cross Drive Explode
0.000 0.030
0.000
0.000
-0.006 0.000
-0.020
-0.020
0 10000 0 10000 0 10000 0 10000
0.020
0.000
0 10000
Step
)K1(
draweR
egarevA
Fruits Gold Hunt Impact Jump
0.050 0.000 0.025 0.025
0.000
0.000 0.000 -0.015
0 10000 0 10000 0 10000 0 10000
Step Step Step Step
AXIOM AXIOM (fixed distance) Dreamer V3 BBF
Figure3: Onlinelearningperformance. Movingaverage(1ksteps)rewardperstepduringtraining
forAXIOM,BBFandDreamerV3onGameworld 10kenvironments. Meanandstandarddeviation
over10parameterseedspermodelandenvironment.
7
inspirationfromALEandclassicvideogames,whilemaintainingalightweightandstructureddesign.
TheGameworldenvironmentsareavailableathttps://github.com/VersesTech/gameworld. Figure2
illustratesthevarietyandvisualsimplicityoftheincludedgames. Toevaluaterobustness,Gameworld
10ksupportscontrolledinterventionssuchaschangesinobjectcolororshape,testinganagent‚Äôs
abilitytogeneralizeacrosssuperficialdomainshifts.
Baselines. BBF[34]buildsonSR-SPR[35]andrepresentsoneofthemostsample-efficientmodel-
free approaches. We adapt its preprocessing for the Gameworld 10k suite by replacing frame-
skipwithmax-poolingovertwoconsecutiveframes; allotherpublishedhyperparametersremain
unchanged. Second, DreamerV3 [36] is a world-model-based agent with strong performance on
games and control tasks with only pixel inputs; we use the published settings but set the train
ratioto1024atbatchsize16(effectivetrainingratioof64:1). Wechosethesebaselinesbecause
theyrepresentstateoftheartinsample-efficientlearningfromrawpixels. NotethatforBBFand
DreamerV3,werescaletheframesto84√ó84and96√ó96pixelsrespectively(followingthepublished
implementations),whereasAXIOMoperatesonfull210√ó160framesofGameworld.
Reward. Figure3showsthe1000-stepmovingaverageofper-steprewardfromsteps0to10000on
theGameworld 10ksuite(mean¬±1standarddeviationover10seeds). Table1showsthecumulative
rewardattainedattheendofthe10kinteractionstepsforAXIOM,BBFandDreamerV3. AXIOM
attainshigher,oronpar,averagecumulativerewardthanBBFandDreamerV3ineveryGameworld
environment. Notably, AXIOM not only achieves higher peak scores on several games, but also
convergesmuchfaster,oftenreachingmostofitsfinalrewardwithinthefirst5ksteps,whereasBBF
andDreamerV3neednearlythefull10k. ForthosegameswhereBBFandDreamerseemedtoshow
no-better-than-randomperformanceat10k, weconfirmedthattheirperformancedoeseventually
improve,rulingoutthatthegamesthemselvesareintrinsicallytoodifficultforthesearchitectures
(seeAppendixE.1). Takentogether,thisdemonstratesthatAXIOM‚Äôsobject-centricworldmodel,
intandemwithitsfast,onlinestructurelearningandinferencealgorithms,canreducethenumber
ofinteractionsrequiredtoachievehighperformanceinpixel-basedcontrol. Fixingtheinteraction
distanceyieldshighercumulativerewardastheagentdoesn‚Äôtneedtospendactionslearningit,but
doingsorequirestuningtheinteractiondistanceforeachgame. Thisillustrateshowhavingextra
knowledgeaboutthedomainathandcanbeincorporatedintoaBayesianmodellikeAXIOMto
furtherimprovesampleefficiency. IncludingtheinformationgaintermfromEquation(10)allows
the agent to obtain reward faster in some games (e.g., Bounce), but actually results in a slower
increaseoftheaveragerewardforothers(e.g.,Gold),asencouragesvisitationofinformation-rich
but negatively-rewarding states. BMR is crucial for games that need spatial generalization (like
GoldandHunt),butactuallyhurtsperformanceonCross,asmergingclustersearlyondiscountsthe
informationgaintermanddiscouragesexploration. SeeAppendixE.2foramoredetaileddiscussion.
Computationalcosts. Table2comparesmodelsizesandper-steptrainingtiming(modelupdateand
planning)measuredonasingleA100GPU.WhileAXIOMincursplanningoverheadduetotheuse
ofmanymodel-basedrollouts,itsmodelupdateissubstantiallymoreefficientthanBBF,yielding
favorabletrade-offsinwall-clocktimepersample. Theexpandingobject-centricmodelofAXIOM
convergestoasufficientcomplexitygiventheenvironment,incontrasttothefixed(andmuchlarger)
modelsizesofBBFandDreamerV3.
Interpretability. UnlikeconventionaldeepRLmethods,AXIOMhasastructured,object-centric
modelwhoselatentvariablesandparameterscanbedirectlyinterpretedinhuman-readableterms(e.g.,
shape,color,position).AXIOM‚Äôstransitionmixturemodelalsodecomposescomplextrajectoriesinto
simplerlinearsub-sequences. Figure4ashowsimaginedtrajectoriesandreward-conditionedclusters
Table2: TrainingtimeperenvironmentsteponGameworld 10k. ParametercountforAXIOM
variesasthemodelfindsasufficientcomplexityforeachenvironment. PlanningtimeforAXIOM
showstherangefor64to512planningrollouts.
Model Parameters(M) Modelupdate(ms/step) Planning(ms/step)
BBF 6.47 135¬±36 N/A
DreamerV3 420 221¬±37 823¬±93
AXIOM 0.3-1.6 18¬±3 252-534
8
Image Data Imagined Trajectory Reward Clusters
2500
0
0 5000 10000
Step
(a)
stnenopmoC
#
BMR
No BMR
2.00
0.00
0 5000 10000
Step
(b)
)k1(
naideM
E[infogain]
E[utility]
0.00
-0.04
0 5000 10000
Step
(c)
)K1(
draweR
egarevA
None
Shape
Color Color (remap)
(d)
Figure4: TrackingAXIOM‚ÄôsBehavior. (a)ExampleframefromImpactattimet(left);imagined
trajectoryinlatentspaceconditionedontheobservationattimetand32timestepsintothefuture,
conditionedonanactionsequencewithhighpredictedreward(middle);andrMMclustersshownin2-
Dspaceandcoloredbyexpectedreward(greenpositivereward,rednegativerewardakapunishment)
(right). (b)ExpandingrMMcomponentsareprunedovertrainingusingBayesianModelReduction
(BMR)inExplode. (c)Informationgaindecreaseswhileexpectedutilityincreasesduringtraining,
showinganexploration-exploitationtrade-offinExplode. (d)Performancefollowingperturbationat
5kstepsshowsrobustnesstochangesingamemechanicsinExplode.
oftherMMfortheImpactgame.Theimaginedtrajectoriesinlatentspace(middlepanelofFigure4a)
are directly readable in terms of the colors and positions of the corresponding object. Because
therecurrentmixturemodel(rMM)conditionsswitchstatesonvariousgame-andobject-relevant
features,wecanconditiontheseswitchvariablesondifferentgamefeaturesandvisualizethemto
show the rMM‚Äôs learned associations (e.g., between reward and space). The right-most panel of
Figure4ashowtherMMclustersassociatedwithreward(green)andpunishment(red)plottedin
space. ThedistributionoftheseclustersexplainsAXIOM‚Äôsbeliefsaboutwhereinspaceitexpects
toencounterrewards,e.g.,expectingapunishmentwhentheplayermissestheball(redclusterat
bottomoftherightpanelofFigure4a).
Figure4bshowsthesharpdeclineinactiverMMcomponentsduringtraining. Byactivelymerging
clusterstominimizetheexpectedfreeenergyassociatedwiththereducedmodel,Bayesianmodel
reduction(BMR)improvescomputationalefficiencywhilemaintainingorimprovingperformance
(see Table 1). The resulting merged components enable interpolation beyond the training data,
enhancinggeneralization.Thisautomaticsimplificationrevealstheminimalsetofdynamicsnecessary
for optimal performance, making AXIOM‚Äôs decision process transparent and robust. Figure 4c
demonstratesthat,astrainingprogresses,per-stepinformationgaindecreaseswhileexpectedutility
rises,reflectingashiftfromexplorationtoexploitationastheworldmodelbecomesreliable.
PerturbationRobustness. Finally,wetestAXIOMundersystematicperturbationsofgamemechanics.
Here,weperformaperturbationtothecolororshapeofeachobjectatstep5000.Figure4dshowsthat
AXIOMisresilienttoshapeperturbations,asitstillcorrectlyinferstheobjecttypewiththeiMM.In
responsetoacolorperturbation,AXIOMaddsnewidentitytypesandneedstore-learntheirdynamics,
resultinginaslightdropinperformanceandsubsequentrecovery. Duetotheinterpretablestructure
ofAXIOM‚Äôsworldmodel,wecanprimeitwithknowledgeaboutpossiblecolorperturbations,and
thenonlyusetheshapeinformationintheiMMinferencestep,beforeremappingtheperturbedslots
basedonshapeandrescueperformance. Formoredetails,seeAppendixE.3.
4 Conclusion
Inthiswork,weintroducedAXIOM,anovelandfullyBayesianobject-centricagentthatlearnshow
toplaysimplegamesfromrawpixelswithimprovedsampleefficiencycomparedtobothmodel-
basedandmodel-freedeepRLbaselines. Importantly,itdoessowithoutrelyingonneuralnetworks,
gradient-basedoptimization, orreplaybuffers. Byemployingmixturemodelsthatautomatically
expandtoaccommodateenvironmentalcomplexity,ourmethoddemonstratesstrongperformance
within a strict 10,000-step interaction budget on the Gameworld 10k benchmark. Furthermore,
AXIOMbuildsinterpretableworldmodelswithanorderofmagnitudefewerparametersthanstandard
modelswhilemaintainingcompetitiveperformance. Tothisend,ourresultssuggestthatBayesian
methodswithstructuredpriorsaboutobjectsandtheirinteractionshavethepotentialtobridgethe
gapbetweentheexpressivenessofdeepRLtechniquesandthedata-efficiencyofBayesianmethods
withexplicitmodels,suggestingavaluabledirectionforresearch.
9
Limitationsandfuturework. Ourworkislimitedbythefactthatthecorepriorsarethemselves
engineeredratherthandiscoveredautonomously. Futureworkwillfocusondevelopingmethodsto
automaticallyinfersuchcorepriorsfromdata,whichshouldallowourapproachtobeappliedto
morecomplexdomainslikeAtariorMinecraft[36],wheretheunderlyinggenerativeprocessesare
lesstransparentbutstillgovernedbysimilarcausalprinciples. Webelievethisdirectionrepresents
acrucialsteptowardbuildingadaptiveagentsthatcanrapidlyconstructstructuralmodelsofnovel
environmentswithoutexplicitengineeringofdomain-specificknowledge.
Acknowledgements . We would like to thank Jeff Beck, Alex Kiefer, Lancelot Da Costa, and
membersoftheVERSESMachineLearningFoundationsandEmbodiedIntelligenceLabsforuseful
discussionsrelatedtotheAXIOMarchitecture.
References
[1] Y.Li,‚ÄúDeepreinforcementlearning: Anoverview,‚ÄùarXivpreprintarXiv:1701.07274,2017.
[2] E. S. Spelke and K. D. Kinzler, ‚ÄúCore knowledge,‚Äù Developmental science, vol. 10, no. 1,
pp.89‚Äì96,2007.
[3] B.M.Lake,T.D.Ullman,J.B.Tenenbaum,andS.J.Gershman,‚ÄúBuildingmachinesthatlearn
andthinklikepeople,‚ÄùBehavioralandBrainSciences,vol.40,p.e253,2017.
[4] B.M.Lake,R.Salakhutdinov,andJ.B.Tenenbaum,‚ÄúHuman-levelconceptlearningthrough
probabilisticprograminduction,‚ÄùScience,vol.350,no.6266,pp.1332‚Äì1338,2015.
[5] E.T√©gl√°s,E.Vul,V.Girotto,M.Gonzalez,J.B.Tenenbaum,andL.L.Bonatti,‚ÄúPurereasoning
in12-month-oldinfantsasprobabilisticinference,‚Äùscience,vol.332,no.6033,pp.1054‚Äì1059,
2011.
[6] E.S.Spelke,R.Kestenbaum,D.J.Simons,andD.Wein,‚ÄúSpatiotemporalcontinuity,smooth-
nessofmotionandobjectidentityininfancy,‚ÄùBritishjournalofdevelopmentalpsychology,
vol.13,no.2,pp.113‚Äì142,1995.
[7] E.S.Spelke,‚ÄúPrinciplesofobjectperception,‚ÄùCognitivescience,vol.14,no.1,pp.29‚Äì56,
1990.
[8] A.M.LeslieandS.Keeble,‚ÄúDosix-month-oldinfantsperceivecausality?,‚ÄùCognition,vol.25,
no.3,pp.265‚Äì288,1987.
[9] T.Wiedemer,J.Brady,A.Panfilov,A.Juhos,M.Bethge,andW.Brendel,‚ÄúProvablecomposi-
tionalgeneralizationforobject-centriclearning,‚ÄùarXivpreprintarXiv:2310.05327,2023.
[10] F.Kapl, A. M.K. Mamaghan, M. Horn, C.Marr, S. Bauer, andA. Dittadi, ‚ÄúObject-centric
representationsgeneralizebettercompositionallywithlesscompute,‚ÄùinICLR2025Workshop
onWorldModels: Understanding,ModellingandScaling,2025.
[11] W. Agnew and P. Domingos, ‚ÄúUnsupervised object-level deep reinforcement learning,‚Äù in
NeurIPSworkshopondeepRL,2018.
[12] T.Kipf,E.Fetaya,K.-C.Wang,M.Welling,andR.Zemel,‚ÄúNeuralrelationalinferencefor
interactingsystems,‚ÄùinInternationalconferenceonmachinelearning,pp.2688‚Äì2697,Pmlr,
2018.
[13] A.Lei,B.Sch√∂lkopf,andI.Posner,‚ÄúSpartan: Asparsetransformerlearninglocalcausation,‚Äù
arXivpreprintarXiv:2411.06890,2024.
[14] W. Zhang, A. Jelley, T. McInroe, and A. Storkey, ‚ÄúObjects matter: object-centric world
models improve reinforcement learning in visually complex environments,‚Äù arXiv preprint
arXiv:2501.16443,2025.
[15] T.Parr,G.Pezzulo,andK.J.Friston,Activeinference: thefreeenergyprincipleinmind,brain,
andbehavior. MITPress,2022.
10
[16] K.Friston,‚ÄúThefree-energyprinciple: aunifiedbraintheory?,‚ÄùNaturereviewsneuroscience,
vol.11,no.2,pp.127‚Äì138,2010.
[17] D.C.KnillandA.Pouget,‚ÄúThebayesianbrain: theroleofuncertaintyinneuralcodingand
computation,‚ÄùTRENDSinNeurosciences,vol.27,no.12,pp.712‚Äì719,2004.
[18] F.Locatello,D.Weissenborn,andO.Unsupervised,‚ÄúObject-centriclearningwithslotattention,‚Äù
inAdvancesinNeuralInformationProcessingSystems,vol.33,pp.1821‚Äì1834,2020.
[19] S. W. Linderman, A. C. Miller, R. P. Adams, D. M. Blei, L. Paninski, and M. J. Johnson,
‚ÄúRecurrentswitchinglineardynamicalsystems,‚ÄùarXivpreprintarXiv:1610.08466,2016.
[20] C.Heins,H.Wu,D.Markovic,A.Tschantz,J.Beck,andC.Buckley,‚ÄúGradient-freevariational
learningwithconditionalmixturenetworks,‚ÄùarXivpreprintarXiv:2408.16429,2024.
[21] K. J. Friston, V. Litvak, A. Oswal, A. Razi, K. E. Stephan, B. C. Van Wijk, G. Ziegler,
and P. Zeidman, ‚ÄúBayesian model reduction and empirical bayes for group (dcm) studies,‚Äù
Neuroimage,vol.128,pp.413‚Äì431,2016.
[22] K. Friston, T. Parr, and P. Zeidman, ‚ÄúBayesian model reduction,‚Äù arXiv preprint
arXiv:1805.07092,2018.
[23] K. Friston, C. Heins, T. Verbelen, L. Da Costa, T. Salvatori, D. Markovic, A. Tschantz,
M.Koudahl,C.Buckley,andT.Parr,‚ÄúFrompixelstoplanning: scale-freeactiveinference,‚Äù
arXivpreprintarXiv:2407.20292,2024.
[24] K.J.Friston,L.DaCosta,A.Tschantz,A.Kiefer,T.Salvatori,V.Neacsu,M.Koudahl,C.Heins,
N.Sajid,D.Markovic,etal.,‚ÄúSupervisedstructurelearning,‚ÄùBiologicalPsychology,vol.193,
p.108891,2024.
[25] M.G.Bellemare,Y.Naddaf,J.Veness,andM.Bowling,‚ÄúThearcadelearningenvironment:
Anevaluationplatformforgeneralagents,‚ÄùJournalofArtificialIntelligenceResearch,vol.47,
pp.253‚Äì279,jun2013.
[26] E.Todorov,T.Erez,andY.Tassa,‚ÄúMujoco: Aphysicsengineformodel-basedcontrol,‚Äùin
2012IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems,pp.5026‚Äì5033,
IEEE,2012.
[27] H.IshwaranandL.F.James,‚ÄúGibbssamplingmethodsforstick-breakingpriors,‚ÄùJournalof
theAmericanstatisticalAssociation,vol.96,no.453,pp.161‚Äì173,2001.
[28] J. Beck and M. J. Ramstead, ‚ÄúDynamic markov blanket detection for macroscopic physics
discovery,‚ÄùarXivpreprintarXiv:2502.21217,2025.
[29] Z. Ghahramani and G. E. Hinton, ‚ÄúSwitching state-space models,‚Äù University of Toronto
TechnicalReportCRG-TR-96-3,DepartmentofComputerScience,1996.
[30] C. Bishop and J. Lasserre, ‚ÄúGenerative or discriminative? getting the best of both worlds,‚Äù
Bayesianstatistics,vol.8,no.3,pp.3‚Äì24,2007.
[31] M.J.Wainwright,M.I.Jordan,etal.,‚ÄúGraphicalmodels,exponentialfamilies,andvariational
inference,‚ÄùFoundationsandTrends¬ÆinMachineLearning,vol.1,no.1‚Äì2,pp.1‚Äì305,2008.
[32] M.D.Hoffman,D.M.Blei,C.Wang,andJ.Paisley,‚ÄúStochasticvariationalinference,‚Äùthe
JournalofmachineLearningresearch,vol.14,no.1,pp.1303‚Äì1347,2013.
[33] K.Friston,T.FitzGerald,F.Rigoli,P.Schwartenbeck,andG.Pezzulo,‚ÄúActiveinference: a
processtheory,‚ÄùNeuralcomputation,vol.29,no.1,pp.1‚Äì49,2017.
[34] M.Schwarzer,J.S.O.Ceron,A.Courville,M.G.Bellemare,R.Agarwal,andP.S.Castro,‚ÄúBig-
ger,better,faster: Human-levelatariwithhuman-levelefficiency,‚ÄùinInternationalConference
onMachineLearning,pp.30365‚Äì30380,PMLR,2023.
11
[35] P.D‚ÄôOro,M.Schwarzer,E.Nikishin,P.-L.Bacon,M.G.Bellemare,andA.Courville,‚ÄúSample-
efficientreinforcementlearningbybreakingthereplayratiobarrier,‚ÄùinDeepReinforcement
LearningWorkshopNeurIPS2022,2022.
[36] D.Hafner,J.Pasukonis,J.Ba,andT.Lillicrap,‚ÄúMasteringdiversecontroltasksthroughworld
models,‚ÄùNature,pp.1‚Äì7,2025.
[37] T. P. Minka, ‚ÄúExpectation propagation for approximate bayesian inference,‚Äù arXiv preprint
arXiv:1301.2294,2013.
[38] M.OkadaandT.Taniguchi,‚ÄúVariationalinferencempcforbayesianmodel-basedreinforcement
learning,‚ÄùinConferenceonRobotLearning,2019.
[39] V.Mnih,K.Kavukcuoglu,D.Silver,A.Graves,I.Antonoglou,D.Wierstra,andM.Riedmiller,
‚ÄúPlayingatariwithdeepreinforcementlearning,‚ÄùarXivpreprintarXiv:1312.5602,2013.
[40] W.Ye,S.Liu,T.Kurutach,P.Abbeel,andY.Gao,‚ÄúMasteringatarigameswithlimiteddata,‚Äù
Advancesinneuralinformationprocessingsystems,vol.34,pp.25476‚Äì25488,2021.
[41] S.Wang,S.Liu,W.Ye,J.You,andY.Gao,‚ÄúEfficientzerov2:Masteringdiscreteandcontinuous
controlwithlimiteddata,‚ÄùarXivpreprintarXiv:2403.00564,2024.
[42] D.Hafner,T.Lillicrap,J.Ba,andM.Norouzi,‚ÄúDreamtocontrol: Learningbehaviorsbylatent
imagination,‚ÄùarXivpreprintarXiv:1912.01603,2019.
[43] D.Hafner,T.Lillicrap,M.Norouzi,andJ.Ba,‚ÄúMasteringatariwithdiscreteworldmodels,‚Äù
arXivpreprintarXiv:2010.02193,2020.
[44] K.Greff,R.L.Kaufman,R.Kabra,N.Watters,C.Burgess,D.Zoran,L.Matthey,M.Botvinick,
andA.Lerchner,‚ÄúMulti-objectrepresentationlearningwithiterativevariationalinference,‚Äùin
Internationalconferenceonmachinelearning,pp.2424‚Äì2433,PMLR,2019.
[45] F.Locatello,D.Weissenborn,T.Unterthiner,A.Mahendran,G.Heigold,J.Uszkoreit,A.Doso-
vitskiy,andT.Kipf,‚ÄúObject-centriclearningwithslotattention,‚ÄùAdvancesinneuralinformation
processingsystems,vol.33,pp.11525‚Äì11538,2020.
[46] R.SinghandC.L.Buckley,‚ÄúAttentionasimplicitstructuralinference,‚ÄùAdvancesinNeural
InformationProcessingSystems,vol.36,pp.24929‚Äì24946,2023.
[47] D.Kirilenko,V.Vorobyov,A.K.Kovalev,andA.I.Panov,‚ÄúObject-centriclearningwithslot
mixturemodule,‚ÄùarXivpreprintarXiv:2311.04640,2023.
[48] J. Jiang, F. Deng, G. Singh, M. Lee, and S. Ahn, ‚ÄúSlot state space models,‚Äù arXiv preprint
arXiv:2406.12272,2024.
[49] S.Ferraro,P.Mazzaglia,T.Verbelen,andB.Dhoedt,‚ÄúFocus: Object-centricworldmodelsfor
roboticmanipulation,‚ÄùFrontiersinNeurorobotics,vol.19,p.1585386,2025.
[50] J.Collu,R.Majellaro,A.Plaat,andT.M.Moerland,‚ÄúSlotstructuredworldmodels,‚ÄùarXiv
preprintarXiv:2402.03326,2024.
[51] C.Rasmussen,‚ÄúTheinfinitegaussianmixturemodel,‚ÄùAdvancesinneuralinformationprocess-
ingsystems,vol.12,1999.
[52] T.Champion,M.Grzes¬¥,andH.Bowman,‚ÄúStructurelearningwithtemporalgaussianmixture
formodel-basedreinforcementlearning,‚ÄùarXivpreprintarXiv:2411.11511,2024.
[53] Z. Ghahramani and G. E. Hinton, ‚ÄúVariational learning for switching state-space models,‚Äù
Neuralcomputation,vol.12,no.4,pp.831‚Äì864,2000.
[54] V.Geadah,J.W.Pillow,etal.,‚ÄúParsingneuraldynamicswithinfiniterecurrentswitchinglinear
dynamicalsystems,‚Äù inTheTwelfthInternationalConferenceonLearningRepresentations,
2024.
[55] S.Linderman,M.J.Johnson,andR.P.Adams,‚ÄúDependentmultinomialmodelsmadeeasy:
Stick-breakingwiththep√≥lya-gammaaugmentation,‚ÄùAdvancesinneuralinformationprocess-
ingsystems,vol.28,2015.
12
A FullModelDetails
AXIOM‚Äôs world model is a hidden Markov model with an object-centric latent state space. The
modelitselfhastwomaincomponents: 1)anobject-centric,slot-attention-like[18]likelihoodmodel;
and2)arecurrentswitchingstatespacemodel[19]. Therecurrentswitchingstatespacemodelis
appliedtoeachobjectorslotidentifiedbythelikelihoodmodel,andmodelsthedynamicsofeach
object with piecewise-linear trajectories. Unlike most other latent state-space models, including
otherobject-centricones,AXIOMisfurtherdistinguishedbyitsadaptablecomplexity‚Äìitgrowsand
prunesitsmodelonlinethroughiterativeexpansionroutines(seeAlgorithm1)andreduction(see
Algorithm2)tomatchthestructureoftheworldit‚Äôsinteractingwith. Thisincludesautomatically
inferringthenumberofobjectsinthesceneaswellasthenumberofdynamicalmodesneededto
describethemotionofallobjects. Thisisinspiredbytherecentfaststructurelearningapproach[23]
developedtoautomaticallylearnahierarchicalgenerativemodelofadatasetfromscratch.
Prefaceonnotation Capitalboldsymbolsdenotecollectionsofmatrix-orvector-valuedrandom
variablesandlowercaseboldsymbolsdenotemultivariatevariables.
A.1 Generativemodel
Themodelfactorizesperceptionanddynamicsintoseparategenerativeblocks: (i)Inperception,
a slot Mixture Model (sMM) models pixels y as competitively explained by continuous latent
t
variablesfactorizedacrossslotsorobjects: x = {x(1),...,x(K)}. Eachpixelisassignedtoone
t t t
of(upto)K slotsusingtheassignmentvariablez ;(ii)dynamicsaremodeledper-objectusing
t,smm
theirobject-centriclatentdescriptionsasinputstoarecurrentswitchingstatespacemodel(similar
toanrSLDS[19]). WedefinethefulllatentsequenceasZ ={O ,z }T . Eachslotlatent
0:T t t,smm t=0
O(k)consistsofbothcontinuousx(k)anddiscretelatentvariables. Thecontinuouslatentsrepresent
t t
continuouspropertiesofanobject, suchasitsposition, colorandshape. Thediscretelatentsare
themselvessplitintotwosubtypes: z(k)ands(k). Weusez(k)todenotefourlatentdescriptorsthat
t t t
capturecategoricalattributesoftheslot(e.g.,objecttype),ands(k)todenoteapairofswitchstates
t
determiningtheslot‚Äôsinstantaneoustrajectory.3 ModelparametersŒòÀú aresplitintomodule-specific
subsets(e.g., Œò ,Œò ). Thejointdistributionoverinputsequencesy ,latentstatesequences
sMM tMM 0:T
Z andparametersŒòÀú canbeexpressedasahiddenMarkovmodel:
0:T
T
p(y ,Z ,ŒòÀú)=p(y ,Z )p(ŒòÀú) (cid:89) p(x ,|z ,Œò )p(x ,z ,s ,a ,r |Œò )
0:T 0:T 0 0 t‚Äì1 t iMM t‚Äì1 t t t‚Äì1 t rMM
(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
t=1
Identitymixturemodel Recurrentmixturemodel
K
(cid:89) p(y |x(k),z ,Œò )p(x(k) |x(k) ,s(k),Œò ), (11)
t t t,smm sMM t t‚àí1 t tMM
k=1 (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
Slotmixturemodel Transitionmixturemodel
Weintentionallyexcludetheslotmixtureassignmentvariablez fromtheotherobject-centric
t,smm
discretelatents{z(k)}K becausethesMMassignmentvariableisimportantlynotfactorizedover
t k=1
slots,sinceitisacategoricaldistributionoverK-dimensionalone-hotvectors,z ‚àà{0,1}K.
t,smm
Latentobjectstates. Atagiventime‚Äìstept ‚àà 0,...,T eachobjectk ‚àà 1,...,K isdescribedby
setsofbothcontinuousanddiscretevariables(wereservekfor‚Äòslotindex‚Äôeverywhere):
O ={x(k),z(k),s(k)}K ,
t t t t k=1
Thecontinuousstatex(k) summarizelatentfeaturesordescriptorsassociatedwiththekth object,
t
includingits2-Dpositionp(k) = {p(k),p(k)}, acorresponding2-Dvelocityv(k) = {v(k),v(k)},
t t,x t,y t t,x t,y
itscolorc(k) = {c(k),c(k),c(k)}its2-DshapeencodedasitsextentalongtheXandYdirections
t t,r t,g t,b
3Weusethesuperscriptindexkasinq(k)toselectonlythesubsetofq‚â°q(1:K)relevanttothekthslot.
13
e(k) = {e(k),e(k)},andan‚Äòunusedcounter‚Äôu(k),whichtrackshowlongthekth objecthasgone
t t,x t,y t
undetected:
x(k) = (cid:2) p(k),c(k),v(k),u(k),e(k)(cid:3)‚ä§ ‚ààR10.
t t t t t t
Inadditiontothecontinuouslatents,eachobjectisalsocharacterizedbytwosetsofdiscretevariables:
z(k)ands(k). Thefirstsetz(k)capturescategoricalinformationabouttheobjectthatisrelevantto
t t t
predictingitsinstantaneousdynamics. Thisincludesalatentobject‚Äòtype‚Äôz(k) (usedtoidentify
t,type
dynamicsacrossobjectinstancesbasedontheirsharedcontinuousproperties,e.g.,objectsthathave
the same shape and color are expected to behave similarly); the object type index of the nearest
other object that slot k is interacting with (or if it isn‚Äôt interacting with anything) z(k) , its
t,interacting
presence/absenceinthecurrentframez(k) ,andwhethertheobjectismovingornotz(k) .
t,presence t,moving
Wedefineeachofthesediscretevariablesin‚Äòone-hot‚Äôvectorformat,soasvectorswhoseentries
z(k) areeither0or1, withtheconstraintthat (cid:80) z(k) = 1. Wecanthuswritethefull
t,m,name m t,m,name
discretez(k)latentasfollows:
t
z(k) = (cid:2) z(k) ,z(k) ,z(k) ,z(k) (cid:3)
t t,type t,interacting t,presence t,moving
‚àà{0,1}Ctype+Cinteracting+Cpresence+Cmoving
Ô£±
C =V ‚â§V
Ô£¥Ô£¥Ô£¥Ô£≤C type
=V
m
+
ax
1
with interacting
C =2
Ô£¥Ô£¥Ô£¥Ô£≥C presence
=2
moving
whereV isthenumberofobjecttypesinferredbytheidentitymixturemodeloriMM,subjecttoa
maximumvalueofV (seeAppendixA.6). NotethatC hasmaximumindexV +1because
max interacting
itincludesanextraindexforthestateof‚Äònotinteractingwithanyotherobject.‚Äô
Thesecondsetofdiscretevariabless(k)formapairofconcurrentswitchstatesthatjointlyselectone
t
ofanexpandingsetoflineardynamicalsystemstopredicttheobject‚Äôsfuturemotion:
s(k) = (cid:2) s(k) ,s(k) (cid:3)
t t,tmm t,rmm
‚àà{0,1}Stmm+Srmm
(cid:40)
S ‚â§L
with tmm
S ‚â§M
rmm
Thefirstvariables(k) istheswitchingvariableofthetransitionmixturemodelortMM,whereas
t,tmm
thesecondswitchvariables(k) istheassignmentvariableofanothermixturemodel‚Äìtherecurrent
t,rmm
mixturemodelorrMM‚Äìwhichfurnishesalikelihoodovers aswellasothercontinuousand
tmm
discretefeaturesofeachobject.
InthesectionsthatfollowwewilldetaileachofthecomponentsinthefullAXIOMworldmodel
andgive theirdescriptions intermsof generativemodels. Thesedescriptions willshowhowthe
latentvariablesZ = {O ,z }T andcomponent-specificparameters(e.g, Œò relateto
0:T t t,smm t=0 rMM
observationsandotherlatentvariables.
A.2 SlotMixtureModel(sMM)
AXIOMprocessessequencesofRGBimagesoneframeatatime.EachimageiscomposedofH√óW
pixelsandisreshapedintoN=HW tokens{yn}N . Eachtokenynisavectorcontainingthenth
t n=1 t
(cid:2) (cid:3)
pixel‚ÄôscolorinRGBanditsimagecoordinates(normalizedto ‚àí1,+1 ). AXIOMmodelsthese
tokensatagiventimeasexplainedbyamixtureofthecontinuousslotlatents;wetermthislikelihood
constructiontheSlotMixtureModel(sMM,seefarleftsideofFigure1). TheK componentsofthis
14
mixturemodelareGaussiandistributionswhoseparametersaredirectlygivenbythecontinuous
featuresofeachslotlatentx(1:K).AssociatedtothisGaussianmixtureisabinaryassignmentvariable
t
zn ‚àà {0,1}indicatingwhetherpixelnattimetisdrivenbyslotk, withtheconstraintthat
t,k,smm
(cid:80) zn =1. ThesMM‚Äôslikelihoodmodelforasinglepixelandtimepointyncanbeexpressed
k t,k,smm t
asfollows(droppingthetsubscriptfornotationalclarity):
K
p(yn |x(1:K),œÉ c (1:K),zn smm )= (cid:89) N(Ax(k),diag( (cid:2) Bx(k),œÉ c (k)(cid:3)‚ä§ ))z k n ,smm (12)
k=1
(cid:2) (cid:3) (cid:2) (cid:3)
A= I 0 , B = 0 I (13)
5 5√ó5 2√ó8 2
p(zn |œÄ )=Cat(œÄ ), p(œÄ )=Dir (cid:0) 1,...,1,Œ± ) (14)
smm smm smm smm 0,smm
(cid:124) (cid:123)(cid:122) (cid:125)
K‚àí1times
(cid:89)
p(œÉ(k))= Œì(Œ≥ ,1) (15)
c 0,j
j‚ààR,G,B
ThemeanofeachGaussiancomponentisgivenafixedlinearprojectionAofeachobjectlatent,which
selectsonlyitspositionandcolorfeatures: Ax(k) = (cid:2) p(k),c(k)(cid:3) . Thecovarianceofeachcomponent
isadiagonalmatrixwhosediagonalisaprojectionofthe2-DshapeoftheobjectlatentBx(k) =e(k)
(its spatial extent in the X and Y directions), stacked on top of a fixed variance for each color
dimensionœÉ(k), whicharegivenindependentGammapriors. Thelatentvariablesp(k),c(k),e(k)
c
aresubsetsofslotk‚Äôsfullcontinuousfeaturesx(k),andtheprojectionmatricesA,Bremainfixed
t
and unlearnable. Each token‚Äôs slot indicator zn is drawn from a Categorical distribution with
smm
mixing weights œÄ . We place a truncated stick-breaking (finite GEM) prior on these weights,
smm
whichisequivalenttoaK-dimensionalDirichletwithconcentrationvector(1,...,1,Œ± ),where
0,smm
thefirstK‚àí1pseudocountsare1andthefinalpseudocountŒ± reflectsthepropensitytoadd
0,smm
newslots. AllsubsequentmixturemodelsinAXIOMareequippedwiththesamesortoftruncated
stick-breakingpriorsonthemixingweights[27]. WecancollectparametersofthesMMtogether
intoŒò ={œÄ
,œÉ(1:K),A,B}.
Thepriorsovertheseparameterscanbewrittenasaproductof
sMM smm c
thetruncatedstick-breakingpriorandtheGammapriorsforeachcolorvariance,andthepriorsover
AandBcanbethoughtofasDiracdeltafunctions,renderingthemunlearnable.
A.3 Movingandpresencelatentvariables
Withinthediscretevariablesthatdescribeeachslotz(k),s(k),therearetwoone-hot-encodedBernoulli
t t
variables: z(k) ‚àà{0,1}2andz(k) ‚àà{0,1}2. Theserepresentwhethertheobjectassociated
t,moving t,present
tothekth slotismovingandpresentonthescreen,respectively. Thesevariableshaveaparticular
relationshiptothegenerativemodel,particularlythedynamicsmodel,whichallowsinferenceover
themtoactasa‚Äòpre-processingstep‚Äôorfilterforslotlatents,beforepassingtheircontinuousfeatures
furtherdowntheinferencechainforusebytheidentitymodel,recurrentmixturemodel,andtransition
model. ThewaythisgatingisfunctionallydefinedisdetailedinthesubsectionGateddynamics
learningbelow.
Thelatentstatesequencesz(k) andz(k) variablesaremodelledasevolvingaccordingto
0:T,presence 0:T,moving
discrete,object-factorizedmarkovchainswhichconcurrentlyemitobservationsviaaproxypresence
variableo(k)(seebelowfordetailsonhowthisiscomputed). ThisconditionalhiddenMarkovmodel
t
canbewrittenasfollowsforthetwovariables:
T
p(z(k) )= (cid:89) p(o(k) |z(k) )p(z(k) )|z(k) ,Œ∏ )
0:T,presence t t,presence t,presence t‚Äì1,presence presence
t=0
T
p(z(k) )= (cid:89) p(z(k) |z(k) ,o(k),v(k),Œ∏ ) (16)
0:T,moving t,moving t‚Äì1,moving t t‚Äì1 moving
t=1
15
Presence latent z The presence chain uses a time-homogeneous 2 √ó 2 transition matrix
t,presence
parametrisedby
Œ∏ ={œï , œï }, 0‚â§œï ,œï ‚â§1.
presence NP‚ÜíP P‚ÜíNP NP‚ÜíP P‚ÜíNP
wherethesubscriptsforthe‚Äònotpresent‚Äòand‚Äòpresent‚Äòstatesofz(k) areabbreviatedasNPand
t,presence
P,respectively. Writingœï =1‚àíœï ,thetransitionmatrixis
i‚Üínotpresent i‚Üípresent
(cid:32) (cid:33) (cid:32) (cid:33)
œï œï 1‚àíœï œï
NP‚ÜíNP NP‚ÜíP NP‚ÜíP NP‚ÜíP
T = = .
presence
œï œï œï 1‚àíœï
1‚ÜíNP P‚ÜíP P‚ÜíNP P‚ÜíNP
Forallexperimentswefixœï = 0, œï = 0.01,encodingthepriorthatanabsentslotcannot
0‚Üí1 1‚Üí0
spontaneouslyre-appearwhileapresentone‚Äúdiesout‚Äùwithprobability0.01eachframe.
Proxyobservation.
Wedefinetheassignment-countindicatoro(k)asavariablethatindicateswhether
t
anypixels1,2,...,N wereassignedtoslotkattimet. Thiscanbeexpressedasanelement-wise
productofallpixel-specificentriesoftherowofsMMassignmentvariablecorrespondingtoslotk‚Äôs
assignments:
z(1:N)
:
t,k,smm
N
o(k) = (cid:89) zn
t t,k,smm
n=1
Finally, the relationship between the count-assignments-indicator o(k) and the presence variable
t
z isaBernoullilikelihoodwiththefollowingform:
t,presence
p(o
t
|z
t,presence
)=o z
t
t,P,presence(1‚àío
t
)zt,A,presence (17)
whichmeansthatpresenceandtheassignmentcounto(k)areexpectedtobe‚Äòon‚Äôsimultaneously.
t
ThesubscriptsPandArefertotheindicesofz(k) thatcorrespondtothe‚Äòpresent‚Äôand‚Äòabsent‚Äô
t,presence
indicators,respectively.
Movinglatentz Wedefinethespeedofslotk asfollows(leavingoutthek superscriptto
t,moving
avoidoverloadedsuperscripts):
(cid:113)
œà = v2 +v2 (18)
t t,x t,y
Thetransitionlikelihoodforthemovinglatentz(k) dependsonthepresenceindicatoro(k)and
t,moving t
theslotspeedœà ;thisforcesinferenceofthemovingindicatorz(k) tobedrivenbytheinferred
t‚Äì1 t,moving
speedandpresenceofthek-thslot. Theformofthisdependenceisencodedinthe2√ó2transition
matrixT withparametersŒ∏ asfollows:
moving moving
Œ∏ ={Œª,Œ≤}, 0‚â§Œª‚â§1, 0‚â§Œ≤ ‚â§1, Œª+Œ≤ ‚â§1. (19)
moving
The parameters Œª and Œ≤ determine the two conditional probabilities œï (o(k),œà ) and
i‚ÜíM t t‚Äì1
œï (o(k),œà ). WeusethesubscriptsNM,Mtoindicatethe‚Äònot-moving‚Äôand‚Äòmoving‚Äòstatesof
i‚ÜíNM t t‚Äì1
z(k) ,respectively. ThedependenceoftheconditionalprobabilitiesontheŒª,Œ≤ hyperparameters
t,moving
canbewrittenasfollows:
Ô£±
œï (o(k),œà )=
Ô£≤Œªi+Œ≤œà
t‚Äì1
, o(
t
k) =1,
œï (o(k),œà )=1‚àíœï (o(k),œà ).
i‚ÜíM t t‚Äì1 Ô£≥i, o(k) =0, i‚ÜíNM t t‚Äì1 i‚Üí1 t t‚Äì1
t
ThefulltransitionmatrixT canbewritten:
moving
(cid:32) (cid:33)
œï œï
T (cid:0) o(k),œà ;Œ∏ (cid:1) = NM‚ÜíNM NM‚ÜíM . (20)
moving t t‚Äì1 moving
œï œï
M‚ÜíNM M‚ÜíM
16
Thissortofparameterizationresultsinthefollowinginterpretation: iftheslotisinferredtobeabsent
(i.e.,nopixelsareassignedtoitando(k) =0),z(k) staysinitspreviousstate. However,ifthe
t t,moving
slotisinferredtobepresent(o(k) =1),thentheprevious‚Äúmoving‚ÄùprobabilityisshrunkbyŒªand
t
nudgedupwardbyŒ≤œà .
t‚Äì1
InallexperimentswesetŒª=0.99, Œ≤ =0.01,buttheyremainexposedhyperparameters.
Gateddynamicslearning Thepresenceandmovinglatentsz t,presence ,z t,moving existinordertofilter
whichslotsgetfitbytherMM.Inordertoachievethisselectiveroutingofonlyactive,movingslots,
weintroduceanauxiliarygatevariablethatisconnectedtothemoving-andpresence-latentsviaa
multiplicationfactorthatparameterizesaBernoullilikelihoodoverthetwovalues(‚ÄòON‚Äòand‚ÄòOFF‚Äò)
ofthegatevariableG(k):
t
p(G(k) |z(k) z(k) )=Bernoulli (cid:0) p (cid:1) (21)
t t,moving t,present gate
where p =z(k) z(k)
gate t,M,moving t,P,present
Thisbinarygatevariablethenmodulatestheinputprecisionofthevariouslikelihoodsassociated
withtheidentitymodel(iMM),transitionmixturemodel(tMM),andrecurrentmixturemodel(rMM)
toeffectively‚Äòmask‚Äôthelearningofthesemodelsonuntrackedorabsentslots. Theendeffectisthat
slotswhichareinferredtobemovingandpresentkeepfullprecision,whileanyothercombination
deflatestheslot-specificinputcovarianceto0,removingtheinfluenceoftheirsufficientstatistics
fromparameterlearning.
A.4 Interactionvariable
Wealsoassociateeachobjectwithadiscretelatentvariablez(k) whichindicatesthetypeof
t,interacting
the closest object interacting with the focal object (i.e., that indexed by k). In practice, we infer
thisinteractionvariablebyfindingtheobjectwhosepositionvariableisclosesttothefocalslot,i.e.
argmin ‚à•p ‚àíp ‚à•,withinsomeconstrainedsetof‚Äònearest‚Äôobjectswhosepositionlatentsare
j‚àànearest j l
withinapredefinedinteractionradiusofthefocalobject(determinedbyafixedparameterr ). This
min
interactionradiuscanbetunedonagamespecificbasis‚ÄìseethemaintextresultsinSection3for
howfixingthisparameteraffectstheresults. Wethenperforminferenceontheidentitymodelusing
thecontinuousfeaturesofthatnearest-interactingslot:
(cid:2) c(j),e(j)(cid:3)‚ä§
. Theinferredidentityofthe
t t
resultingjthslotisthenconvertedintoaone-hotvectorrepresentingthetypeofthe‚Äòinteracting‚Äôlatent
z(k) ,whichcanthenbefedasinputintotherecurrentmixturemodelorrMMasdescribedin
t,interacting
thefollowingsection.
A.5 Unusedcounter
Tokeeptrackofhowlongaslothasremainedinactiveweintroduceanon-negative-integerlatent
thatistreatedasacontinuousvariableinR: u(k) orthe‚Äòunusedcounter‚Äô. Thisallowsthemodel
t
topredicttherespawningofobjectsaftertheygooff-screen. Wecoupletheunusedcounteragain
totheproxyassignment-countvariableo(k)usinganexponentially‚ÄìdecayingBernoullilikelihood
t
(identicalinspirittotheEP‚Äìstylepresencelikelihood):
P (cid:0) o( t k) =1|u( t k)(cid:1) =1‚àíexp (cid:8) ‚àíŒæe‚àíŒ≥uu( t k)(cid:9) ,
P (cid:0) o( t k) =0|u( t k)(cid:1) =exp (cid:8) ‚àíŒæe‚àíŒ≥uu( t k)(cid:9) , Œæ ‚àà(0,1], Œ≥ u >0. (22)
Whenu(k) =0theslotispresentwithprobability1‚àíe‚àíŒæ ‚âÉŒæ(fortypicalŒæ ‚â≥0.8). Eachincrement
t
(cid:0) (cid:1)
byŒΩ multipliesthatprobabilitybyexp‚àíŒæŒ≥ ŒΩ ,i.e.itdecaysroughlyonee-foldperunusedstep
u u u
whenŒ≥ ‚âÉŒΩ‚àí1.
u u
17
A.6 Identitymixturemodel
AXIOMusesanidentitymixturemodel(iMM)toinferadiscreteidentitycodez(k) foreachobject
type
based on its continuous features. These identity codes are used to condition the inference of the
recurrentmixturemodelusedfordynamicsprediction. Conditioningthedynamicsonidentity-codes
inthisway,ratherthanlearningaseparatedynamicsmodelforeachslot,allowsAXIOMtousethe
samedynamicsmodelacrossslots. Thisalsoenablesthemodeltolearnthesamedynamicsinatype-
specific,ratherthaninstance-specific,manner,andtoremapidentitieswhene.g.,theenvironmentis
perturbedandcolorschange. Concretely,theiMMmodelsthe5-Dcolorsandshapes{c(k),e(k)}K
k=1
acrossslotsasamixtureofuptoV Gaussiancomponents(objecttypes). Theslot-levelassignment
variablez(k) indicateswhichidentityisassignedtothekthslot. ThegenerativemodelfortheiMM
t,type
is(omittingthet‚àí1subscriptfromobjectlatents):
V
p( (cid:2) c(k),e(k)(cid:3)‚ä§ |z( ty k p ) e ,¬µ 1:V,type ,Œ£ 1:V,type )= (cid:89) N(¬µ j,type , (cid:16) G t (k) (cid:17)‚àí1 Œ£ j,type )z j ( , k ty ) pe (23)
j=1
p(¬µ ,Œ£‚àí1 )=NIW(m ,Œ∫ ,U ,n ) (24)
j,type j,type 0,j,type 0,j,type 0,j,type 0,j,type
p(z(k) |œÄ )=Cat(œÄ ), p(œÄ )=Dir (cid:0) 1,...,1,Œ± ) (25)
type type type type 0,type
(cid:124) (cid:123)(cid:122) (cid:125)
V‚àí1times
(cid:16) (cid:17)‚àí1
The G(k) Xnotationrepresentselement-wisebroadcastingofthereciprocalofG(k)acrossall
t t
elementsofthematrixorvectorX. Whenappliedasamasktoacovariancematrix,forinstance,the
effectisthatslotsthatareinferredtobemovingandpresentdonothavetheircovarianceaffected
(cid:16) (cid:17)‚àí1
G(k) = 1, whereas those slots that are inferred to be either not present or not moving will
t
(cid:16) (cid:17)‚àí1
‚Äòinflate‚Äôthecovarianceofthemixturemodel,dueto G(k) ‚Üí‚àû. ThesametypeofCategorical
t
likelihood for the type assignments and truncated stick-breaking prior over the mixture weights
is used to allow an arbitrary (up to a maximum of V) number of types to be used to explain the
continuousslotfeatures. Weequiptheprioroverthecomponentlikelihoodparameterswithconjugate
NormalInverseWishart(NIW)priors.
A.7 TransitionMixtureModel
Thedynamicsofeachslotaremodelledasamixtureoflinearfunctionsoftheslot‚Äôsownprevious
state. TostressthehomologybetweenthismodelandtheothermodulesofAXIOM,werefertothis
moduleasthetransitionmixturemodelortMM,butthisformulationismorecommonlyalsoknown
asaswitchinglineardynamicalsystemorSLDS[29]. ThetMM‚Äôsswitchvariables(k) selectsaset
t,tmm
oflinearparametersD ,b todescribethekth slot‚Äôstrajectoryfromttot+1. Eachlinearsystem
l l
capturesadistinctrigidmotionpatternforaparticularobject:
L
p(x( t k) |x( t‚Äì k 1 ),s( t, k tm ) m ,D 1:L ,b 1:L )= (cid:89) N(D l x( t k)+b l , (cid:16) G t (k) (cid:17)‚àí1 2I)s( t k ,l ) ,tmm (26)
l=1
p(s(k) |œÄ )=Cat(œÄ ), p(œÄ )=Dir (cid:0) 1,...,1,Œ± ) (27)
t,tmm tmm tmm tmm 0,tmm
(cid:124) (cid:123)(cid:122) (cid:125)
L‚àí1times
(28)
wherewefixthecovarianceofallLcomponentstobe2I,andallmixturelikelihoodsD ,b
1:L 1:L
tohaveuniformpriors. NotethatwegatethecovarianceonceagainusingthereciprocalofG(k)to
t
filterthetMMtoonlymodelobjectsthataremovingandpresent. Thetruncatedstick-breakingprior
(cid:0)
Dir 1,...,1,Œ± )overthecomponentmixingweightsenablesthenumberoflinearmodesLto
0,tmm
bedynamicallyadjustedtothedatabygrowingthemodelwithpropensityŒ± . Importantly,theL
0,tmm
transitioncomponentsofthetMMarenotslot-dependent,butaresharedandthuslearnedusingthe
18
datafromallK slotlatents. ThetMMcanthusexplainandpredictthemotionofdifferentobjects
usingashared,expandingsetof(uptoLdistinct)lineardynamicalsystems.
A.8 RecurrentMixtureModel
Therecurrentmixturemodel(rMM)isusedtoinfertheswitchstatesofthetransitionmodeldirectly
from current slot-level features. This dependence of switch states on continuous features is the
sameconstructusedintherecurrentswitchinglineardynamicalsystemorrSLDS[19]. However,
incontrasttotherSLDS,whichusesadiscriminativemappingtoinfertheswitchstatefromthe
continuousstate(usuallyasoftmaxorstick-breakingparameterizationthereof),therMMrecoversthis
dependencegenerativelyusingamixturemodelovermixedcontinuous‚Äìdiscreteslotstates[30]. In
thisway,‚Äòselecting‚ÄôtheswitchstateusedforconditioningthetMMactuallyemergesfrominference
overdiscretelatentvariables,whichhaveaparticularconditionalrelationship(inthiscontext,ajoint
mixturelikelihoodrelationship)tootherlatentandobservedvariables. Concretely,therMMmodels
thedistributionofcontinuousanddiscretevariablesasamixturemodeldrivenbyanotherper-slot
latentassignmentvariables(k) . TherMMdefinesamixturelikelihoodoveratupleofcontinuous
rmm
anddiscreteslot-specificinformation. Weusethenotationf(k) tocollectthecontinuousfeatures
t‚Äì1
thattherMMparameterizesadensityover: theyincludeasubsetofthekth slot‚Äôsowncontinuous
statex(k),aswellasanonlineartransformationgappliedtootherslots‚Äòfeatures,thatcomputesthe
t‚Äì1
2-Ddistancevectorpointingfromtheinferredpositionofthefocalobject(i.e.,slotk)tothenearest
interactingslotj. Wedetailhowthesefeaturesarecomputedbelow:
ContinuousfeaturesfortherMM
Thecontinuouslatentdimensionsofx(k)usedfortherMMinclude
t‚Äì1
the following: p(k),v(k),u(k). We represent extracting this subset as a sparse linear projection
t‚Äì1 t‚Äì1 t‚Äì1
appliedtothefullcontinuouslatentsCx(k). Inaddition, therMMmodelsthedistributionofthe
t‚Äì1
2-D vectors pointing from the focal object‚Äôs position (the slot k in consideration) to the position
of the nearest interacting object, i.e. ‚àÜp(k) ‚â° p(j) ‚àíp(k). The nearest interacting object with
t‚Äì1 t‚Äì1 t‚Äì1
indexj istheonewhoseinferredpositionistheclosesttothefocalobject‚Äôs,whileonlyconsidering
neighborswithinsomeinteractionzonewithradiusr . Thisisthesameinteractiondistanceused
min
tocomputenearest-neighborswhenpopulatingthez(k) latent, seeAppendixA.4fordetails.
interacting
If no object is within the interaction radius, then we set ‚àÜp(k) to a random sample from a 2-D
(cid:2) (cid:3)
t‚Äì1
(cid:2) (cid:3) (cid:2) (cid:3)
uniformdistributionwithmean 1.2,1.2 andlower/upperboundsof 1.198,1.198 , 1.202,1.202 .
Wesummarizethiscomputationwithagenericnonlinearfunctionappliedtothelatentstatesofall
slots
g(x(1:K)),
to intimate possible generalizations where different sorts of (possibly learnable)
t‚Äì1
neighborhoodrelationshipscouldbeinsertedhere.
DiscretefeaturesfortherMM Thediscretefeaturesincludethefollowing: z(k) ,z(k) ,
t‚àí1,type t‚àí1,interacting
theassignment-countindicatorvariableo(k),theswitchstateofthetransitionmixturemodels(k) ,
t‚Äì1 t,tmm
theactionattimestept‚àí1andrewardatthecurrenttimestepr .Werefertothisdiscretecollectionas
t
d(k). Theinclusionofs(k) inthediscreteinputstotherMMisacriticalingredientofthisrecurrent
t‚Äì1 t,tmm
dynamicsformulation‚ÄìitallowsinferenceonthistheswitchingstateofthetMMtobedrivenby
high-dimensionalconfigurationsofcontinuousanddiscretevariablesrelevantforpredictingmotion.
rMMdescription
TherMMassignmentvariableassociatedtoagivenslotisabinaryvectors(k)
t,rmm
whose mth entry s(k) ‚àà {0,1} indicates whether component m explains the current tuple of
t,m,rmm
mixed continuous-discrete data. Each component likelihood selected by s(k) factorizes into a
t,rmm
productofcontinuous(Gaussian)anddiscrete(Categorical)likelihoods.
(cid:16) (cid:17) (cid:16) (cid:17)
f t ( ‚Äì k 1 ) = Cx( t k ‚Äì1 ),g(x( t 1 ‚Äì1 :K)) , d( t k ‚Äì1 ) = z( t‚àí k) 1,type ,z( t‚àí k) 1,interacting ,o( t k ‚Äì1 ),s( t, k tm ) m ,a t‚Äì1 ,r t (29)
19
p(f t ( ‚Äì k 1 ),d( t k ‚Äì1 ) |s t ( , k r ) mm )= (cid:89) M
Ô£Æ
Ô£∞N (cid:0) f t ( ‚Äì k 1 );¬µ m,rmm , (cid:16) G t (k) (cid:17)‚àí1 Œ£ m,rmm (cid:1)(cid:89) Cat (cid:0) d t‚Äì1,i ;G t (k)a m,i (cid:1)
Ô£π
Ô£ª
st,m,rmm
m=1 i
(30)
p(¬µ ,Œ£‚àí1 )=NIW(m ,Œ∫ ,U ,n ), p(a )=Dir(a ) (31)
m,rmm m,rmm 0,m,rmm 0,m,rmm 0,m,rmm 0,m,rmm m,i 0,m,i
p(s(k) |œÄ )=Cat(œÄ ), p(œÄ )=Dir (cid:0) 1,...,1,Œ± ) (32)
t,rmm rmm rmm rmm 0,rmm
(cid:124) (cid:123)(cid:122) (cid:125)
M‚àí1times
TheparametersofthemultivariatenormalcomponentsareequippedwithNIWpriorsandthoseofthe
discreteCategoricallikelihoodswithDirichletpriors. AswithalltheothermodulesofAXIOM,we
equipthemixingweightsfors(k) withatruncatedstick-breakingpriorwhosefinalMthpseudocount
t,rmm
parametertunesthepropensitytoaddnewrMMcomponents. Notealsotheuseofthegatevariable
G(k)tofilterslotsfordynamicslearningbyinflatingthecovarianceassociatedwithanyslotinputs
t
notinferredmovingandpresent.
Fixeddistancevariant. WeexploredavariantoftherMM(fixed_distance)wherethedisplace-
mentvector‚àÜp(k)isnotreturnedbyg(x(1:K))andthereforenotincludedasoneofthecontinuous
t‚Äì1 t‚àí1
inputfeaturesfortherMM.Inthiscase, theentryofz(k) thatcorrespondstothenearest
t‚Äì1,interacting
interactingobjectisstilldeterminedbyr ,however. Inthiscase,thechoiceofr mattersmore
min min
forperformancebecausetherMMcannotlearntonuanceitsdynamicpredictionsbasedonprecise
distances. Ingeneral,thismeansthatr requiresmoregame-specifictuning. SeeSection3forthe
min
resultsoftuningr comparedtolearningitdirectlybyproviding‚àÜp(k)asinputtotherMM.
min t‚Äì1
A.9 Variationalinferenceandlearning
Toperforminferenceandlearningwithinourproposedmodel,weemployavariationalBayesian
approach. Thecoreideaistoapproximatethetrueposteriordistributionoverlatentvariablesand
parameterswithamoretractablefactorizeddistribution,q(Z ,ŒòÀú). Thisisachievedbyoptimizing
0:T
the variational free-energy functional, F(q), which establishes an upper-bound on the negative
log-marginallikelihoodoftheobserveddatay :
0:T
(cid:104) (cid:105)
F(q)=E logq(Z ,ŒòÀú)‚àílogp(y ,Z ,ŒòÀú) , suchthat F(q)‚â•‚àílogp(y ). (33)
q 0:T 0:T 0:T 0:T
MinimizingthisfunctionalF(q)withrespecttoqisequivalenttomaximizingtheEvidenceLower
Bound(ELBO),therebydrivingtheapproximateposteriorq(Z ,ŒòÀú)tomorecloselyresemblethe
0:T
trueposteriorp(Z ,ŒòÀú |y ).
0:T 0:T
Weassumeamean-fieldfactorizationfortheapproximateposterior, whichdecomposesoverthe
globalparametersŒòÀú andthesequenceoflatentvariablesZ :
0:T
T
q(Z ,ŒòÀú)=q(ŒòÀú) (cid:89) q(Z ), (34)
0:T t
t=0
whereq(Z )furtherfactorizesacrossindividuallatentvariablesforframet:
t
Ô£´ Ô£∂
N K
q(Z t )=Ô£≠ (cid:89) q (cid:0) zn t,smm (cid:1) Ô£∏ (cid:89)(cid:16) q (cid:0) x( t k)(cid:1) q (cid:0) z( t k)(cid:1) q (cid:0) s( t k)(cid:1)(cid:17) . (35)
n=1 k=1
ThevariationaldistributionovertheglobalparametersŒòÀú isalsoassumedtofactorizeaccordingto
thedistinctcomponentsofourmodel:
q(ŒòÀú)=q(Œò )q(Œò )q(Œò )q(Œò ). (36)
sMM iMM tMM rMM
Note that the pixel-to-slot assignment variables zn are specific to each pixel n but are not
t,smm
factorizedacrossslotsforagivenpixel,astheyrepresentasinglecategoricalchoicefromK slots.
Otherlatentvariables,suchasthecontinuousstatex(1:K)anddiscreteattributesz(1:K),s(1:K),are
t t t
factorizedperslotk.
20
Theinferenceandlearningprocedureforeachnewframetinvolvesaniterativealternationbetween
anE-step,wherelocallatentvariableposteriorsareupdated,andanM-step,whereglobalparameter
posteriorsarerefined.
E-step IntheExpectation-step(E-step),weholdthevariationalposteriorsoftheglobalparameters
q(Œò)fixed. Wethenupdatethevariationalposteriorsforeachlocallatentvariablefactorwithin
q(Z ),suchasq(zn ),q(x(k)),q(z(k)),andq(s(k)). Theseupdatesarederivedbyoptimizingthe
t t,smm t t t
ELBOwithrespecttoeachfactorinturnandoftenresultinclosed-formcoordinate-ascentupdates
duetoconjugacybetweenthelikelihoodtermsandthechosenformsofthevariationaldistributions.
M-step In the Maximization-step (M-step), we update the variational posteriors for the global
parametersq(Œò )associatedwitheachmodelcomponent¬µ ‚àà {sMM,iMM,tMM,rMM}. Each
¬µ
q(Œò )isassumedtobelongtotheexponentialfamily,characterizedbynaturalparametersŒ∑ :
¬µ ¬µ
q(Œò )=h(Œò ) exp (cid:8) Œ∑‚ä§T(Œò ) ‚àí A(Œ∑ ) (cid:9) , (37)
¬µ ¬µ ¬µ ¬µ ¬µ
whereT(Œò )representsthesufficientstatisticsforŒò ,andA(Œ∑ )isthelog-partitionfunction(or
¬µ ¬µ ¬µ
log-normalizer). TheM-stepupdateproceedsintwostagesforeachcomponent:
1. First,wecomputetheexpectedsufficientstatisticsT(cid:98)¬µ usingthecurrentposteriorsoverthe
latentvariablesq(Z )obtainedfromthet-thE-step:
t
T(cid:98)¬µ =E
q(Zt)
(cid:2) T (cid:0) Œò
¬µ
,Z
t
(cid:1)(cid:3) . (38)
TheseexpectedstatisticsarethencombinedwithpriornaturalparametersŒ∑ toformthe
¬µ,0
targetnaturalparametersfortheupdate: Œ∑
(cid:98)¬µ
=T(cid:98)¬µ +Œ∑
¬µ,0
.
2. Second, we update the current natural parameters Œ∑(t‚àí1) using a natural-gradient step,
¬µ
which acts as a stochastic update blending the previous parameters with the new target
parameters,controlledbyalearningratescheduleœÅ :
t
Œ∑(t) ‚Üê (1‚àíœÅ )Œ∑(t‚àí1) + œÅ Œ∑ , where0<œÅ ‚â§1. (39)
¬µ t ¬µ t(cid:98)¬µ t
SlotMixtureModel(sMM)
TheSlotMixtureModel(sMM)providesalikelihoodfortheobservedpixeldatay bymodeling
t
each pixel as originating from one of K object slots. The variational approximation involves
posteriors over pixel-to-slot assignments zn , slot mixing weights œÄ , slot-specific color
t,smm smm
variances œÉ(k), and the continuous latent states of slots x(k). Specifically, for each pixel n at
c,j t
time t, the posteriorprobability thatit belongsto slot k is q(zn = 1) = rn , ensuring that
t,k,smm t,k
(cid:80)K rn = 1. Theposteriorovertheslot-mixingprobabilitiesœÄ isaDirichletdistribution:
k=1 t,k smm
q(œÄ ) = Dir(œÄ | Œ± ,...,Œ± ),parameterizedbyconcentrationsŒ± > 0. For
smm smm 1,smm K,smm k,smm
eachslotk andcolorchannelj ‚àà {r,g,b},theposterioroverthecolorvarianceœÉ(k) isaGamma
c,j
distribution: q(œÉ(k))=Gamma(œÉ(k) |Œ≥ ,b ),withshapeŒ≥ andrateb . Finally,eachslot‚Äôs
c,j c,j k,j k,j k,j k,j
continuouslatentstatex(k) ‚àà R10 (encompassingposition,color,velocity,shape,andtheunused
t
counter) is modeled by a Gaussian distribution: q(x(k)) = N(x(k) | ¬µ(k), Œ£(k)). The precision
t t t t
matrixisdenotedŒõ=Œ£‚àí1,andtheprecision-weightedmeanish(k) =Œõ(k)¬µ(k).
t t t
E-stepUpdatesforsMM DuringtheE-stepforthesMMatframet, thevariationaldistributions
forlocallatentvariableszn (representedbytheresponsibilitiesrn )andx(k) (representedby
t,smm t,k t
¬µ(k),Œ£(k))areupdated,whiletheglobalsMMparametersq(Œò )areheldfixed.
t t sMM
1. Thepixelresponsibilitiesrn ,representingq(zn =1),areupdatedusingthestandard
t,k t,k,sMM
mixturemodelupdate:
exp (cid:0)E [logœÄ ]+E [logN(yn; Ax(k), Œ£(k))] (cid:1)
rn = q k,smm q t t . (40)
t,k (cid:80)K exp (cid:0)E [logœÄ ]+E [logN(yn; Ax(j), Œ£(j))] (cid:1)
j=1 q j,smm q t t
21
The per-slot observation covariance Œ£(k) is constructed as Œ£(k) =
diag (cid:0) BE [x(k)], E [œÉ(k)] (cid:1) , consistent with the generative model where Bx(k) pro-
q t q c
videsvariancerelatedtoshapeandœÉ(k)providescolorchannelvariances(seeEquation(12)
c
forthesMMlikelihoodequations).
2. TheparametersoftheGaussianposteriorq(x(k))areupdatedbyincorporatingevidence
t
fromthepixelsassignedtoslotk. Thisinvolvesupdatingitsnaturalparameters(precision
Œõ(k)andprecision-adjustedmeanh(k)):
t t
N
Œõ(k) =Œõ(k) + (cid:88) rn A‚ä§(cid:0) Œ£(k)(cid:1)‚àí1 A,
t t|t‚Äì1 t,k
n=1
(41)
N
h(k) =h(k) + (cid:88) rn A‚ä§(cid:0) Œ£(k)(cid:1)‚àí1 yn.
t t|t‚Äì1 t,k t
n=1
ThetermsŒõ(k) andh(k) arethenaturalparametersofthepredictivedistributionforx(k).
t|t‚Äì1 t|t‚Äì1 t
(cid:16) (cid:17)‚àí1
ThestandardparametersarethenŒ£(k) = Œõ(k) and¬µ(k) =Œ£(k)h(k).
t t t t t
M-stepUpdatesforsMM IntheM-step,theglobalparametersofthesMM,whicharetheDirichlet
parametersŒ± formixingweightsandtheGammaparameters(Œ≥ ,b )forcolorvariances,
k,smm k,j k,j
areupdated. ThisbeginsbyaccumulatingtheexpectedsufficientstatisticsfromtheE-step:
N
(cid:88)
N = rn ,
t,k t,k
n=1
N
(cid:88)
Sy = rn yn, (42)
1,t,k t,k t
n=1
N
(cid:88)
Sy = rn yn(yn)‚ä§.
2,t,k t,k t t
n=1
TheDirichletconcentrationparametersareupdatedas(nowusingthetindextorepresentthecurrent
vs. lastsettingsoftheposteriorparameters):
Œ± =(1‚àíœÅ )Œ± +œÅ (Œ± +N ),
t,k,smm t t‚Äì1,k,smm t 0,k,smm t,k
(cid:18) (cid:19)
N
Œ≥ =(1‚àíœÅ )Œ≥ +œÅ Œ≥ + t,k ,
t,k,j t t‚Äì1,k,j t 0,j 2
(43)
Ô£´ Ô£∂
N
b
t,k,j
=(1‚àíœÅ
t
)b
t‚Äì1,k,j
+œÅ
t
Ô£≠1+
2
1 (cid:88) r
t
n
,k
(y
t
n
,colorj
‚àí(AE
q
[x(
t
k)])
colorj
)2 Ô£∏.
n=1
Here,Œ± representsthepriorconcentrationfortheDirichletdistribution(e.g.,1forthefirst
0,k,smm
K‚àí1componentsandŒ± fortheK-th,ifusingatruncatedstick-breakingprior).FortheGamma
0,smm
parameters,Œ≥ isthepriorshapeand1isthepriorrate(orrelatedpriorparameters). Theprojection
0,j
matricesAandBareconsideredfixedandarenotlearned.
Presence,Motion,andUnusedCounterDynamics
Themodelincludeslatentvariablesforeachslotkthattrackitspresencez(k) ,motionz(k) ,
t,presence t,moving
andanunusedcounteru(k).
t
Inferenceoverthepresencelatent Thepresencestateisinformedbyan‚Äòassignment-count-indicator‚Äô
o(k). This indicator is set to 1 if the slot is actively explaining pixels (e.g., if the sum of its
re t sponsibilities (cid:80) rn exceedsasmallthresholdœµ ),and0otherwise.
n t,k active
22
Recall the Bernoulli likelihood over o(k) that links it to the z(k) latent as follows (cf. Equa-
t t,presence
tion(17)):
p(o
t
|z
t,presence
)=o z
t
t,P,presence(1‚àío
t
)zt,A,presence (44)
There is an implied superscript k on both o(k) and the presence variable z(k) , which are
t t,P,presence
leftouttoavoidvisualclutter. Thislinkageisincorporatedintoq(z(k) )usinganExpectation
t,presence
Propagation(EP)styleupdateviaapseudo-likelihood[37]:
‚ÑìÀú(cid:0) z( t, k p ) resence (cid:1) = (cid:104) (o( t k))z t ( , k P ) ,presence (cid:0) 1‚àío( t k)(cid:1)z t ( , k A ) ,presence (cid:105)Œ∂ , (45)
whereŒ∂ isadampingfactor. Thisupdateincreasesposteriorevidenceforpresenceifo(k) =1,and
t
forabsenceifo(k) =0. Thefirst-ordereffectofthispseudo-likelihoodwhenupdatingtheposterior
t
overz(k) is
t,presence
q(z(k) )‚âà(1‚àíŒ∂)q(z(k) )+Œ∂o(k). (46)
t,P,presence t‚Äì1,P,presence t
Recall that the A,P subscripts refer to the indices of z(k) that signify the ‚Äòis-absent‚Äò and
t‚Äì1,presence
‚Äòis-present‚Äôstates,respectively.
Inferenceoverthemovinglatent SimilarEPupdatesapplyforinferringq(z(k) )basedonits
t,M,moving
specificlikelihoodsinvolvingvelocityando(k).ThegateG(k) =q(z(k) =1)¬∑q(z(k) =1)
t t t,P,present t,M,moving
isthenformedfromtheseinferredprobabilities.
Inference over the unused counter The ‚Äòunused counter‚Äô u(k) tracks how long slot k has been
t
inactive. Appendix A.5 of the full model details describes a generative likelihood P(o(k) = 1 |
t
u(
t
k))=1‚àíexp(‚àíŒæe‚àíŒ≥uu(
t
k) ),forwhichdampedEPupdatesforq(u(
t
k))canbederivedandwould
remain in closed form. However, a specific case, by choosing hyperparameters such that a hard
constraint o(k) = 1 ‚áê‚áí u(k) = 0 is effectively enforced (e.g., by taking Œ≥ ‚Üí ‚àû and Œæ = 1
t t u
in the generative likelihood), leads to a simplified, deterministic update for the posterior mean
¬µ(k) =E [u(k)]:
t,u q t
¬µ(k) = (cid:0) 1‚àío(k)(cid:1)(cid:0) ¬µ(k) +ŒΩ (cid:1) , ŒΩ =0.05. (47)
t,u t t‚àí1,u u u
Inthissimplifiedregime,thecounterisresetto0ifo(k) = 1;otherwise,itincrementsbyafixed
t
amountŒΩ .
u
IdentityMixtureModel(iMM)
The Identity Mixture Model (iMM) assigns one of V possible discrete identities to each active
slot,basedonitscontinuousfeatures. Thisallowsforsharedcharacteristicsanddynamicsacross
instancesofthesameobjecttype. Thevariationalapproximationtargetsposteriorsovertheseslot-
to-identityassignmentsz(k) (whichisacomponentofz(k)),identitymixingweightsœÄ ,and
t,type t type
theparameters(¬µ ,Œ£ )foreachidentity‚Äôsfeaturedistribution. Thefeaturesy(k) utilized
j,type j,type t,imm
bytheiMMforslotkareitscolorc(k)andshapee(k),thusy(k) =[c(k),e(k)]‚ä§. Foreachslotk
t t t,imm t t
attimetwheretheactivitygateG(k) ‚âà 1,theposteriorprobabilitythatitbelongstoidentityv is
t
q(z(k) =1)=Œ≥(k),satisfying (cid:80)V Œ≥(k) =1. ForslotswhereG(k) ‚âà0,theseresponsibilities
t,v,imm t,v v=1 t,v t
are effectively null or uniform, contributing negligibly to parameter updates. The posterior over
identity-mixing probabilities œÄ is a Dirichlet distribution: q(œÄ ) = Dir(œÄ |
1:V,iMM 1:V,type 1:V,type
Œ± ,...,Œ± ), with Œ± > 0. Each identity v is characterized by a mean ¬µ and
1,type V,type v,type v,type
covariance Œ£ . The variational posterior over these parameters is a Normal‚ÄìInverse-Wishart
v,type
(NIW)distribution: q(¬µ ,Œ£ )=NIW(¬µ ,Œ£ |m ,Œ∫ ,U ,n ).
v,type v,type v,type v,type v,type v,type v,type v,type
23
E-stepUpdatesforiMM
IntheE-stepfortheiMM,thelocalassignmentprobabilitiesŒ≥(k)foreach
t,v
slotkareupdated. ThisupdateisprimarilydrivenbyslotswhereG(k) ‚âà1:
t
Œ≥(k) ‚àù exp (cid:0)E [logœÄ ] (cid:1) √ó exp (cid:0)E [logN(y(k) ; ¬µ ,Œ£ )] (cid:1) . (48)
t,v q v,type q t,iMM v,type v,type
Theseresponsibilitiesarenormalizedsuchthat (cid:80)V Œ≥(k) =1foreachactiveslot.
v=1 t,v
M-stepUpdatesforiMM DuringtheM-step,theglobalparametersoftheiMMareupdated. Suffi-
cientstatisticsareaccumulated,weightedbythegateG(k)toensurethatonlyactivelymovingslots
t
contributesignificantlytotheupdates:
K
N = (cid:88) G(k)Œ≥(k),
t,v t t,v
k=1
K
S = (cid:88) G(k)Œ≥(k)E [y(k) ], (49)
1,t,v t t,v q t,imm
k=1
K
S = (cid:88) G(k)Œ≥(k)E [y(k) (y(k) )‚ä§].
2,t,v t t,v q t,iMM t,iMM
k=1
TheDirichletparametersŒ± areupdatedusingN andpriorparametersŒ± (which,due
t,v,type t,v 0,v,type
tothestick-breakingpriorsareall1‚ÄôsexceptforthefinalcountŒ± ):
0,V,type
(cid:0) (cid:1)
Œ± =(1‚àíœÅ )Œ± +œÅ Œ± +N . (50)
t,v,type t t‚àí1,v,type t 0,v,type t,v
TheNIWparameters(m , Œ∫ , U , n )areupdatedbyblendingtheirnatural
t,v,type t,v,type t,v,type t,v,type
parameterrepresentations. ThetargetnaturalparametersŒ∑NIW arederivedfromthecurrentsufficient
(cid:98)t,v
statistics{N ,S ,S }andtheNIWpriorparameters:
t,v 1,t,v 2,t,v
(NatParamsNIW) ‚Üê (1‚àíœÅ )(NatParamsNIW ) + œÅ Œ∑NIW. (51)
t,v t t‚àí1,v t(cid:98)t,v
RecurrentMixtureModel(rMM)
TheRecurrentMixtureModel(rMM)providesagenerativemodelforacollectionofslot-specific
features,andimportantly,itfurnishesthedistributionovertheswitchstates(k) thatgovernsthe
t,tmm
Transition Mixture Model (tMM). The rMM itself is a mixture model with M components, and
its own slot-specific assignment variable is s(k) . The variational factors include the posterior
t,rmm
probabilityofassignmenttorMMcomponentm,q(s(k) =1)=œÅ(k) (whichsumstooneover
t,m,rmm t,m
m),aDirichletposteriorq(œÄ )=Dir(œÄ |Œ± ,...,Œ± )foritsmixingweights,NIW
rmm rmm 1,rmm M,rmm
posteriorsq(¬µ ,Œ£ )forcontinuousfeaturesf(k) modeledbyeachcomponentm,andDirichlet
m m t‚Äì1
posteriors q(a ) for the parameters of categorical distributions over various discrete features
i,m
d(k) also modeled by component m. These discrete features d(k) encompass inputs like z(k) ,
i i t‚Äì1,type
z(k) ,aswellasthetMMswitchstates(k) whichtherMMmodelsgeneratively.
t‚Äì1,interacting t,tmm
E-stepUpdatesforrMM IntherMME-step,foreachslotkconsideredactive(i.e.,G(k) ‚âà1),the
t
responsibilitiesœÅ(k) foritsM componentsareupdated. Theseupdatesdependonthelikelihoodof
t,m
theslot‚ÄôsinputfeaturesundereachrMMcomponent. Theinputfeaturesincludecontinuousaspects
f(k)(asubsetofx(k)suchaspositionandvelocity,andinteractionfeatureslike‚àÜp(k))andasetof
t‚Äì1 t‚Äì1 t‚Äì1
discretefeaturesd(k) (e.g.,typefromthepreviousstepz(k) ).
t‚Äì1,inputs t‚Äì1,type
œÅ(k) ‚àù exp (cid:0)E [logœÄ ] (cid:1) √ó exp (cid:0)E [logN(f(k);¬µ ,Œ£ )] (cid:1)
t,m q m,rmm q t‚Äì1 m,rmm m,rmm
√ó (cid:89) exp (cid:0)E [log Cat(d(k) ;a )] (cid:1) . (52)
q t‚Äì1,i i,m
i‚ààinputdiscretefeatures
24
Theseresponsibilitiesarenormalizedtosumtooneforeachactiveslotk. Duringrolloutsusedin
planning,thepredictedposteriordistributionforthetMMswitchstates(k) isthendeterminedasa
t,tmm
mixtureoftheoutputdistributionsfromtherMMcomponents,weightedbyœÅ(k):
t,m
M
q (cid:0) s(k) =1 (cid:1) = (cid:88) œÅ(k) E(cid:2) a (cid:3) ,
t,l,tmm t,m q tmm_switch,m,l
m=1
wherea istheprobabilityoftMMswitchstatelunderthelearnedparametersofrMM
tmm_switch,m,l
componentm.
M-stepUpdatesforrMM IntherMMM-step,itsglobalparametersareupdated,withcontributions
fromslotsweightedbythegateG(k). Theexpectedsufficientstatisticsareaccumulated. Forthe
t
mixingweights:
K
N = (cid:88) G(k)œÅ(k). (53)
t,m t t,m
k=1
Forthecontinuousfeaturedistributions(NIWparameters):
K
Sf = (cid:88) G(k)œÅ(k) E [f(k)],
1,t,m t t,m q t‚Äì1
k=1
(54)
K
Sf = (cid:88) G(k)œÅ(k) E [f(k)(f(k))‚ä§].
2,t,m t t,m q t‚Äì1 t‚Äì1
k=1
Foreachdiscretefeatured modeledbytherMM(thisincludesinputfeaturesd(k) andtheoutput
i t‚àí1,i
tMMswitchstates(k) ),anditscategory‚Ñì:
t,tmm
N = (cid:88) K G(k)œÅ(k) (cid:40) I[d( t‚àí k) 1,i =‚Ñì] ifd i isaninputfromt‚àí1 . (55)
t,m,i,‚Ñì t t,m q(s(k) =‚Ñì) ifd iss(k)
k=1 t,tmm i t,tmm
The parameters are then updated using these statistics. Dirichlet parameters for
rMM mixing weights Œ± (from N ), NIW parameters for continuous features
t,m,rmm t,m
(m , Œ∫ , U , n ) (from N ,Sf ,Sf ), and Dirichlet parameters
t,m,rmm t,m,rmm t,m,rmm t,m,rmm t,m 1,t,m 2,t,m
a foralldiscretefeatures(fromN )areupdatedvianaturalgradientblending:
t,i,m,‚Ñì t,m,i,‚Ñì
(cid:0) (cid:1)
Œ± =(1‚àíœÅ )Œ± +œÅ Œ± +N ,
t,m,rmm t t‚Äì1,m,rmm t 0,m,rmm t,m
(NatParamsNIW) ‚Üê (1‚àíœÅ )(NatParamsNIW )+œÅ Œ∑NIW, (56)
t,m t t‚àí1,m t(cid:98)t,m
(cid:0) (cid:1)
a =(1‚àíœÅ )a +œÅ a +N .
t,i,m,‚Ñì t t‚àí1,i,m,‚Ñì t 0,i,m,‚Ñì t,m,i,‚Ñì
TransitionMixtureModel(tMM)
TheTransitionMixtureModel(tMM)describesthedynamicsofslotstatesx(k)usingamixtureofL
t
lineartransitions. Weapproximateposteriorsovertransitionassignmentsandmixingweightswith
variationalfactors. Transitionresponsibilitiesforslotkusingtransitionlareq(s(k) =1)=Œæ(k),
t,l,tmm t,l
satisfying (cid:80)L Œæ(k) =1. Themixing-weightdistributionovertheLtransitionsœÄ isq(œÄ )=
l=1 t,l tmm tmm
Dir(œÄ |Œ± ,...,Œ± ).
tmm 1,tmm L,tmm
E-stepUpdates
InthetMME-step,foreachslotk,theresponsibilitiesŒæ(k)foreachtransitionlare
t,l
updatedbasedonhowwellthattransitionexplainstheobservedchangefromx(k) tox(k):
t‚àí1 t
exp (cid:0)E[logœÄ ] (cid:1) N (cid:0) x(k); D x(k) +b , G(k)2I (cid:1)
Œæ(k) = l t l t‚àí1 l t‚Äì1
t,l (cid:80)L exp (cid:0)E[logœÄ ] (cid:1) N (cid:0) x(k); D x(k) +b , G(k)2I (cid:1)
u=1 u t u t‚àí1 u t‚Äì1
forl=1,...,L. ThetermG(k)indicatesiftheslotwasactiveatt‚àí1,and2I istheprocessnoise
t‚Äì1
covariance,assumedfixedorshared.
25
Algorithm1MixtureModelExpansionAlgorithm
Input Output Hyperparameters/Settings
Y ‚àà RN√ód: Matrixwhoseith row Œ∏‚àó : Updated posterior NIW œÑ:expansionthreshold
1:Kt+1
(i = 1,...,N)isad-dimensional parameters (K components
t+1
token(e.g.,pixel). whereK ‚â•K )
t+1 t
Œ∏‚àó = E:maximumexpansionsteps
1:Kt
(m ,Œ∫ ,U ,n ):
1:Kt 1:Kt 1:Kt Kt
Initial posterior NIW parameters
(K components).
t
(cid:0) (cid:1)
1: InitialiseK ‚ÜêK andNIWparametersŒ∏ = m ,Œ∫ ,U ,n fork=1:K
t k k k k k
2: forg=1toE do ‚ñ∑outer‚Äúexpand-or-stop‚Äùloop
//E-step
3: fori=1toN do
4: ‚Ñì ‚ÜêE (cid:2) logp(y |Œ∏ ) (cid:3) , k=1:K
ik q(Œ∏k)
(cid:46)
i k
5: r ‚Üêexp(‚Ñì )
(cid:80)K
exp(‚Ñì )
ik ik j=1 ij
6: ‚Ñìmax ‚Üêmax ‚Ñì , i=1:N
i k‚â§K ik
7: ifmin‚Ñìmax >œÑ then
i
i
8: break ‚ñ∑alltokenswellexplained
9: i‚àó ‚Üêargmin ‚Ñìmax ‚ñ∑worst-explainedtoken
i i
10: K ‚ÜêK+1 ‚ñ∑instantiatenewcomponent
11: Hard-assignr i‚àó,k ‚Üê0(k<K),r i‚àó,K ‚Üê1
12: InitialisecomponentK:Œ∫
K
‚Üê1, ŒΩ
K
‚Üêd+2, ¬µ
K
‚Üêy i‚àó, Œ®
K
‚ÜêŒ®
0
//M-step(natural‚Äìgradientupdate)
13: fork=1toKdo
N
(cid:88)
14: Œ∑ ‚Üê r T(y ) +Œ∑ ‚ñ∑computetargetnaturalparameters
(cid:98)k ik i k,0
i=1
(cid:124) (cid:123)(cid:122) (cid:125)
T(cid:98)k
15: Œ∑(t) ‚Üê (1‚àíœÅ )Œ∑(t‚àí1) + œÅ Œ∑ ‚ñ∑natural-gradientupdatewithrateœÅ
k t k t(cid:98)k t
16: UnpackŒ∑(t) ‚Üí(m ,Œ∫ ,U ,n ) ‚ñ∑recoverNIWhyperparams
k k k k k
17: returnŒ∏‚àó
1:Kt+1
18:
M-stepUpdates ThetMMdoesnotuseanexplicitM-step. Instead,parametersarefixedtotheir
initial values identified during the expansion algorithm. In other words, once we identify a new
dynamicsmodeintheexpansionalgorithm,theseparametersareaddedasanewcomponentforthe
tMMandremainfixed.
A.10 Bayesianmodelreduction
Growingnewclustersensuresplasticity, butleftuncheckeditleadstoover-parameterisationand
over-fitting. Toenablegeneralization,every‚àÜT =500frameswethereforerunBayesianmodel
BMR
reduction on the rMM, merging pairs of components whenever doing so increases the expected
evidence lower bound (ELBO) of the multinomial distributions over the next reward and SLDS
switch. TheELBOiscomputedwithrespecttogenerateddatafromthemodelthroughancestral
sampling. Giventwocandidatecomponentsk ,k withposterior-sufficientstatistics(Œ∑ ,Œ∑ ),their
1 2 k1 k2
mergedstatisticsareŒ∑ =Œ∑ +Œ∑ ‚àíŒ∑prior,ensuringthatpriormassisnotdouble-counted.
k1‚à™k2 k1 k2 k2
Candidatepairsareproposedbyafastheuristicthat(i)samplesupton = 2000usedclusters,
pair
(ii)computestheirmutualexpectedlog-likelihoodundertheother‚Äôsparameters,and(iii)retainsthe
highest-scoringpairs. Eachproposalisacceptediff themergedELBO(line7)isnotsmallerthanthe
currentELBO(line2). TheprocedureisspelledoutinAlgorithm2.
26
Algorithm2Bayesianmodelreductionfortherecurrentmixturemodel
Input Output Hyper-parameters
M:PosteriorrMM ReducedmodelM‚Ä≤ n , n
pair samples
pruninginterval‚àÜT
BMR
1: D={(c ,d )}
nsamples
‚àºM ‚ñ∑Drawn pairsofcontinuousanddiscretedatasamplesfromM
i i i=0 samples
2: L(0) ‚ÜêELBO(M,D) ‚ñ∑ComputecurrentELBO
3: P ={(k ,k ) }
npairs
‚ñ∑Drawupton candidatepairsbyheuristicoverlap
1 2 i i=0 pair
4: fors=1to|P|do
5: (k ,k )‚ÜêP
1 2 s
6: Mtry‚ÜêMERGE(M,k
1
,k
2
)
7: Ltry ‚ÜêELBO(Mtry,D)
8: ifLtry ‚â•L(s‚àí1)then
9: M‚ÜêMtryandL(s) ‚ÜêLtry ‚ñ∑Setcurrentmodeltothecandidate
10: else
11: L(s) ‚ÜêL(s‚àí1)
12: returnM
A.11 Planningwithactiveinference
Inactiveinference,policiesœÄ =a areselectedthatminimizeexpectedFreeEnergy[33]:
0:H
p(œÄ)=œÉ(‚àíG(œÄ)),with
H
G(œÄ)=
(cid:88)
‚àí
(cid:0)E
[logp(r |O ,œÄ)‚àíD (q(Œ± |O ,œÄ)‚à•q(Œ± ))]
(cid:1)
, (57)
q(OœÑ|œÄ) œÑ œÑ KL rmm œÑ rmm
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
œÑ=0
Utility Informationgain(IG)
withH theplanninghorizonandœÉthesoftmaxfunction. However,asthenumberofpossiblepolicies
grows exponentially with a larger planning horizon, enumerating all policies at every timestep
becomesinfeasible. Therefore,wedrawinspirationfrommodelpredictivecontrol(MPC)solutions
suchasCrossEntropyMethod(CEM)andmodelpredictivepathintegral(MPPI),whichcanbecast
asapproximatinganactionposteriorbymomentmatching[38].
Inparticular,wesampleP policiesofhorizonH,andevaluatetheirexpectedFreeEnergyG. Instead
ofsamplingactionsforeachfuturetimestepœÑ uniformly,wemaintainahorizon-wisecategorical
proposal p(a ). After every planning step, we keep the top-k samples with minimum G, and
œÑ
importanceweighttogetanewprobabilityforeachactionp(a )atfuturetimestepœÑ:
œÑ
(cid:88)
exp(‚àíG(a(
œÑ
k)))
p(a )= (58)
œÑ (cid:80) exp(‚àíG(a(j)))
k j œÑ
Insteadofdoingmultiplecross-entropyiterationsperplanningstep,wemaintainamovingaverage
ofp(a ). Ateveryinstant,thefirstactionofthecurrentbestpolicyisactuallyexecutedbytheagent.
œÑ
Ourplanningloopishencecomposedofthefollowingthreestages(seeAlg.3).
Samplingpolicies. Foreachiterationwedraw
P ‚àíR + R policies
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124)(cid:123)(cid:122)(cid:125)
CEM random
wherethefirstsetissampledi.i.d.fromtheproposalp(a )andtheremainingRare‚Äúexploratory‚Äù
œÑ
sequencesgeneratedbya(smoothed)randomwalk. Inaddition,thepreviousbestplanisalways
injectedinslot0andtheAconstantactionsequencesoccupyslots1:Atoguaranteecoverage.
Evaluating a policy. Each policy is rolled forward S times through the world-model to obtain
per-steppredictionsofrewardrÀÜ
œÑ
andinformationgainI(cid:99)G
œÑ
,averagedoversamples. Wecalculatean
27
Algorithm3Planningalgorithm
Require: currentposteriorstateq,proposalp(a ),bestplanaÀú
œÑ
//SampleP candidatepolicies
1: A ‚ÜêCat(p(a ))‚äó(P‚àíR)
cem œÑ
2: A
rand
‚ÜêRANDOM(R)
3: A‚Üê[aÀú, const , A , A ]
0:A‚àí1 cem rand
//Evaluate
4: foralla(p) ‚ààAdo
5: (rÀÜ
0:H‚àí1
,I(cid:99)G
0:H‚àí1
)‚ÜêRollout(q,a(p))
6: G(p) ‚Üê (cid:80)
œÑ
Œ≥
d
œÑ
iscount
(rÀÜ
œÑ
+Œª
IG
I(cid:99)G
œÑ
)
//Refitproposal
7: K‚Üêtop-Kindicesof‚àíG(p)
8: forœÑ =0toH‚àí1do
9: pÀÜ(a )‚Üêsoftmax (cid:0) temperature‚àí1hist{a(p)} (cid:1)
œÑ œÑ p‚ààK
10: p(a )‚ÜêŒ± pÀÜ(a )+(1‚àíŒ± )p(a )
œÑ smooth œÑ smooth œÑ
11: returnfirstactionofbestplan,updatedp(a ),bestplanaÀú
œÑ
expectedFreeEnergyG,whereinaddition,weweightheinformationgaintermwithascalarŒª to
IG
tradeoffexplorationandexploitation,aswellasapplytemporaldiscounting:
S‚àí1H‚àí1
G=
S
1 (cid:88) (cid:88) Œ≥
d
œÑ
iscount
(cid:0) rÀÜ
œÑ
s +Œª
IG
I(cid:99)G s
œÑ
(cid:1) , Œ≥
discount
‚àà[0,1), Œª
IG
=info_gain weight.
s=0 œÑ=0
Proposalupdate. LetKbetheindicesofthetop-K = ‚åätopk_ratio¬∑P‚åãpoliciesby(negative)
expectedFreeEnergy.ForeveryhorizonstepœÑ weformtheempiricalactionhistogramof{a } ,
k,œÑ k‚ààK
convert it to probabilities with a tempered softmax (temperature) and perform an exponential-
moving-averageupdate
p(a )new =Œ± p(a )+(1‚àíŒ± )p(a )old, Œ± =alpha.
œÑ smooth œÑ smooth œÑ smooth
B Hyperparameters
WelistthehyperparametersusedformainAXIOMresultsshowninFigure3inTable3. Forthe
fixed_distanceablations,wefixedr = 1.25forthegamesExplode,Bounce,Impact,Hunt,
min
Gold,Fruits,andfixedr =1.25forthegamesJump,Drive,Cross,andAviate.
min
C Computationalresources,costsandscaling
AXIOMandbaselinemodelsweretrainedandevaluatedonA10040GGPUs. Allmodelsusea
singleA100GPUperenvironment. AXIOMandBBFtrainasingleenvironmentto10kstepsin
about30min,whereasDreamerV3trainsin2.5h. Thecorrespondingper-stepbreakdownofaverage
inferenceandplanningtimescanbeseeninTable2.
C.1 Planningandinferencetime
ForAXIOM,eachtimestepduringtrainingcanbebrokendownintoplanningandmodelinference.
Toquantifytheplanningtimescalingweperformablationsoverthenumberofplanningpolicies(P).
Sincemodelinferencecorrelateswiththenumberofmixturemodelcomponents,weevaluatethe
scalingusingtheenvironmentsExplode(fewobjects)andCross(manyobjects). Figure5showsthat
theplanningtimescaleslinearlywiththenumberofpoliciesrolledout(leftpanel),andhowmodel
inferencetimescaleswiththenumberofmixturemodelcomponents(rightpanel).
D Gameworld10kEnvironments
Inthissectionweprovideaninformaldescriptionoftheproposedarcade-styleenvironments,inspired
bytheArcadelearningenvironment[25]. Tothisend,ourenvironmentshaveanobservationspace
28
Table3:HyperparametersoftheAXIOMagenttrainedtoplayall10gamesofGameworldasreported
inthemaintext(seeFigure3).
InferenceHyperparameters
Parameter Value
œÑ (expansionthresholdofthesMM) 5.7
SMM
œÑ (expansionthresholdofthesMM) ‚àí1√ó102
iMM
œÑ (expansionthresholdoftherMM) ‚àí1√ó101
rMM
œÑ (expansionthresholdofthetMM) ‚àí1√ó10‚àí5
tMM
E (maximumnumberofexpansionsteps) 10
‚àÜT (timestepstoBMR) 500
BMR
Œ∂(exponentofthedampedz likelihood) 0.01
presence
r 0.075
min
PlanningHyperparameters
Parameter Value
H(planningdepth) 32
P (numberofrollouts) 512
S(numberofsamplesperrollout) 3
Œª (informationgainweight) 0.1
IG
Œ≥ (discountfactor) 0.99
discount
top-kratio 0.1
randomsampleratio 0.5
temperature 10.0
Œ± 1.0
smooth
GenerativeModelParameters
Structure
K(maxsMMcomponents) 32
V (maxiMMcomponents) 32
L(maxtMMcomponents) 500
M (maxrMMcomponents) 5000
Œª(fromŒ∏ ) 0.99
moving
Œ≤(fromŒ∏ ) 0.01
moving
Œ≥ ‚àû
u
Œæ 1.0
ŒΩ 0.05
u
Dirichlet/Gammapriors
Œ≥ 0.1
0,R,G,B
Œ± 1
0,smm
Œ± 1√ó10‚àí4
0,imm
Œ± 0.1
0,tmm
Œ± 0.1
0,rmm
Normal-Inverse‚ÄìWishart(1:V)
m 0
0,1:V,type
Œ∫ 1√ó10‚àí4
0,1:V,type
U 1I
0,1:V,type 4 5
n 0,1:V,type 11
Normal-Inverse‚ÄìWishart(1:M)
m 0
0,1:M,rmm
Œ∫ 1√ó10‚àí4
0,1:M,rmm
U 625I
0,1:M,rmm 7
n 0,1:M,rmm 15
Component‚ÄìwiseDirichletpriors(1:M)
a 1√ó10‚àí4
0,1:M,type
a 1√ó10‚àí4
0,1:M,interacting
a 1√ó10‚àí4
0,1:M,o
a 1√ó10‚àí4
0,1:M,tmm
a 1√ó10‚àí4
0,1:M,action
a 1.0
0,1:M,reward
29
ofshape210√ó160√ó3,thatcorrespondstoaRGBpixelarraysgamescreen. Agentsinteractvia
asetof2to5discreteactionsformovementorgame-specificinteractions. Asisstandardpractice,
positiverewards(+1)areawardedforachievingobjectives,whilenegativerewards(-1)aregivenfor
failures. Hereisabriefdescriptionofthegames:
Aviate. Thisenvironmentputstheplayerincontrolofabird,challengingthemtonavigatethrougha
seriesofverticalpipes. Thebirdfallsundergravityandcanbemadetojumpbyperforminga"flap"
action. Theplayer‚Äôsobjectiveistoguidethebirdthroughthenarrowhorizontalgapsbetweenthe
pipeswithoutcollidingwithanypartofthepipestructureorthetop/bottomedgesofthescreen. Any
collisionwithapipe,orgoingoutofscreenatthetoporbottomresultsinanegativerewardandends
thegame.
Bounce. ThisenvironmentsimulatesasimplifiedversionoftheclassicgamePong,wheretheplayer
controlsapaddletohitaballagainstanAI-controlledopponent. Theplayerhasthreediscreteactions:
movetheirpaddleup,moveitdown,orkeepitstationary,influencingtheball‚Äôsverticaltrajectory
uponcontact. Theobjectiveistoscorepointsbyhittingtheballpasttheopponent‚Äôspaddle(reward
+1),whilepreventingtheopponentfromdoingthesame(reward-1). Thegameisepisodic,resetting
onceapointisscoredbyeitherside.
Cross. InspiredbytheclassicAtarigameFreeway,thisenvironmenttaskstheplayer,represented
asayellowsquare,withcrossingamulti-laneroadwithoutbeinghitbycars. Theplayerhasthree
discreteactions: moveup,movedown,orstayinplace,controllingverticalmovementacrosseight
distinct lanes. Cars of varying colors and speeds continuously traverse these lanes horizontally,
wrappingaroundthescreen. Theobjectiveistoreachthetopofthescreenforapositivereward;
however,collidingwithanycarresetstheplayertothebottomofthescreenandincursanegative
reward.
Driver. Thisenvironmentsimulatesalane-baseddrivinggamewheretheplayercontrolsacarfrom
atop-downperspective, navigatingamulti-laneroad. Theplayercanchoosefromthreediscrete
actions: stayinthecurrentposition,moveleft,ormoveright,allowingforlanechanges. Thegoalis
todriveasfaraspossible,avoidingcollisionswithopponentcarsthatappearandmovedownthe
lanesatvaryingspeeds. Collidingwithanothercarresultsinanegativerewardandendsthegame.
Explode. In this game inspired by the arcade classic Kaboom!, the player controls a horizontal
bucketatthebottomofthescreen,taskedwithcatchingbombsdroppedbyamovingbomber. The
playercanchoosefromthreediscreteactions: remainstationary,moveleft,ormoveright,allowing
forprecisehorizontalpositioningtointerceptfallingprojectiles. Abombercontinuouslytraverses
thetopofthescreen,periodicallyreleasingbombsthataccelerateastheyfalltowardsthebottom.
Successfullycatchingabombinthebucketyieldsapositivereward,whereasallowingabombtofall
off-screenresultsinanegativereward.
0.8
0.4
200 400
P (planning policies)
)s(
emit
gninnalP
K
0.018
0
2
4
6
8
0.012
10
0 1 2 3 5 6 8 9 10
K (sMM components)
)s(
emit
ecnerefnI
Cross
Explode
Figure5: Computationalcosts. Scalingofplanningtimeasafunctionofthenumberofpolicies
(left),andmodelinferencetimeasafunctionofthenumberofsMMcomponents(right). Alltimes
measuredonasingleA100GPU.
30
Fruits. This game casts the player as a character who must collect falling fruits while dodging
dangerousrocks. Theplayercanperformoneofthreediscreteactions: moveleft,moveright,orstay
inplace,controllinghorizontalmovementatthebottomofthescreen. Fruitsofvariouscolorsfall
fromthetop,grantingapositiverewarduponbeingcaughtintheplayer‚Äôsinvisiblebasket.Conversely,
fallingrocks,representedasdarkgreyrectangles,willendthegameandincuranegativerewardif
collected.
Gold. Inthisgame,theplayercontrolsacharacter,representedbyayellowsquare,fromatop-down
perspective,movingacrossagrassyfieldtocollectgoldcoinsandavoiddogs. Theplayercanchoose
fromfivediscreteactions: stayput,moveup,moveright,movedown,ormoveleft,enablingagile
navigation across the screen. Gold coins are static collectibles that grant positive rewards upon
contact,whiledogsmovedynamicallyacrossthescreen,servingasobstaclesthatendthegameand
incuranegativerewardifcollidedwith.
Hunt. Thisgamefeaturesacharacternavigatingamulti-laneenvironment,akintoagrid,froma
top-downperspective. Theplayerhasfourdiscreteactionsavailable: moveleft,moveright,move
up, or move down, allowing full two-dimensional movement within the game area. The screen
continuouslypresentsaflowofitemsandobstaclesmovinghorizontallyacrosstheselanes. The
player‚Äôsgoalistocollectbeneficialitemstoearnpositiverewardswhiledeftlymaneuveringtoavoid
contactwithdetrimentalobstacles,whichincurnegativerewards,encouragingstrategicpathfinding.
Impact. ThisenvironmentsimulatestheclassicarcadegameBreakout,wheretheplayercontrolsa
horizontalpaddleatthebottomofthescreentobounceaballanddestroyawallofbricks. Theplayer
hasthreediscreteactions: movethepaddleleft,moveitright,orkeepitstationary. Theobjectiveis
toeliminateallthebricksbyhittingthemwiththeball,earningapositiverewardforeachdestroyed
brick. Iftheballgoespastthepaddle,theplayerincursanegativerewardandthegameresets. The
gameendswhenallbricksaredestroyed.
Jump. Inthisside-scrollingendlessrunnergame,theplayercontrolsacharacterwhocontinuously
runsforward,encounteringvariousobstacles. Theplayerhastwodiscreteactions: performnoaction
or initiate a jump allowing the character to avoid different types of obstacles. Colliding with an
obstacleresultsinanegativerewardandimmediatelyresetsthegame.
E Additionalresultsandablations
E.1 Baselineperformanceon100K
Extendingthewall-clockbudgetto100Kinteractionstepssharpensthecontrastbetweenmodel-
basedandmodel-freeagents. OnHunt,DreamerV3failstomakemeasurableprogressovertheentire
horizon,remainingnearitsrandom-playbaseline,whereasBBFcontinuestoimproveandultimately
attainsameanepisodicreturnonparwiththescoreourobject-centricagentalreadyreachesafteronly
10Ksteps. InGoldbothbaselinesdolearnwithin100Ksteps,buttheirasymptoticperformancestill
plateausbelowthelevelouragentachievesinthemuchshorter10K-stepregime(seeFigure6).
0.025
0.000
010K 100K
Step
)K1(
draweR
egarevA
Gold Hunt
0.040
0.000
010K 100K
Step
BBF Dreamer V3 AXIOM
Figure6: 100KperformanceonGold&Hunt.
31
0.000
-0.010
0 10000
)K1(
draweR
egarevA
Aviate Bounce Cross Drive Explode
0.000
0.000 0.020
0.000
-0.006
0.000
-0.010 -0.020
0 10000 0 10000 0 10000 0 10000
0.020
0.000
0 10000
Step
)K1(
draweR
egarevA
Fruits Gold Hunt Impact Jump
0.040 0.000 0.025 0.025
0.000 0.000 0.000 -0.010
0 10000 0 10000 0 10000 0 10000
Step Step Step Step
AXIOM AXIOM (no IG) AXIOM (no BMR)
Figure7: PerformanceofAXIOMablations. Averagerewardoverthefinal1,000framesacross10
Gameworld 10KenvironmentsforthreeAXIOMvariants: thefullAXIOMmodel,aversionwithout
BayesianModelReduction(AXIOM(noBMR)),andaversionexcludinginformationgainduring
planning(AXIOM(noIG)).
E.2 Ablations
Noinformationgain. Whendisablingtheinformationgain,weobtainthepurplecurvesinFigure7.
Ingeneral,atfirstglancethereappearstobelittleimpactoftheinformationgainonmostgames.
However,thisistobeexpected,asinFigure4cweshowedthate.g. forExplode,theinformationgain
isonlydrivingperformanceforthefirstfewhundredsteps,afterwhichexpectedutilitytakesover. In
termsofcumulativerewards,informationgainisactuallyhurtingperformanceonmostgameswhere
interactionsbetweenplayerandobjectresultinanegativereward. Thisisbecausetheseinteraction
eventswillbepredictedasinformation-richinthebeginning,encouragingtheagenttoexperience
thesemultipletimes. ThisisespeciallyapparentintheCrossgame,wheretheno-IG-ablatedagent
immediatelydecidesnottoattemptcrossingtheroadatallafterthefirstfewcollisions. Figure8
visualizesthecreatedrMMclusters,whichillustrateshownoinformationgainkillsexplorationin
Cross. Wehencebelievethatinformationgainwillplayamoreimportantroleinhardexploration
tasks,whichisaninterestingdirectionforfutureresearch.
NoBayesianModelReduction. The orange curves in Figure 7 show the impact of disabling the
BayesianModelReduction(BMR).BMRclearlyhasacrucialimpactonbothGoldandHunt,which
arethegameswheretheplayercanmovefreelyaroundthe2Darea. Inthiscase,BMRisableto
generalizethedynamicsandobjectinteractionsspatiallybymergingclusterstogether. Theexception
tothisisonceagainCross,wheredisablingBMRactuallyyieldsthebestperformingagent. Thisis
againexplainedbytheinterplaywithinformationgain. AsBMRwillmergesimilarclusterstogether,
movingupwithoutcollidingwillbeassignedtoasingle,oftenvisitedcluster. Thiswillrenderthis
clusterlessinformativefromaninformationgainperspective,andtheagentwillbemoreattractedto
collidewiththedifferentcarsfirst. However,whendisablingBMR,reachingeachspatiallocation
willgetitsowncluster,andtheagentwillbeattractedtovisitlessfrequentlyobservedlocations,
likethetopofthescreen. ThiscanalsobeseenqualitativelyifweplottheresultingrMMclusters
inFigure8c. ThisbegsthequestiononwhentobestscheduleBMRduringthecourseoflearning.
Clearly,BMRiscrucialtogeneralizeobservedeventstonovelsituations,butwhendonetooearlyin
learning,itcanbedetrimentalforlearning. Furtherinvestigatingthisinterplayremainsatopicfor
futurework.
Planningrolloutsandsamples. Aswesamplerolloutsateachtimestepduringtheplanningphase,
there is a clear tradeoff between the number of policies and rollout samples to collect in terms
ofcomputationtimespent(seeFigure5)andthequalityofthefoundplan. Weperformedagrid
search,varyingthenumberofrollouts[64,128,256,512]andnumberofsamplesperrollout[1,3,5],
evaluating3seedseach. Theresults,showninFigure9showstherearenosignificantperformance
differences,butmorerolloutsanddrawingmorethanonesampleseemtoperformslightlybetteron
32
(a)AXIOM (b)AXIOM(noIG) (c)AXIOM(noBMR)
Figure8: VisualizationsoftherMMclustersonCrossforinformationgainandBMRablations.
EachGaussianclusterdepictsaparticulardynamicsforaparticularobjecttype,coloredbytheobject
color,andtheedgecolorofanearby‚Äúinteracting‚Äùobject. (a)AXIOMhasvarioussmallclusters
fortheplayerobject(yellow)interactingwiththecoloredcarsinthevariouslanes,andelongated
clustersthatmodeltheplayerdynamicsofmovingupordown. (b)Withoutinformationgain,the
playercollideswiththebottommostcars,andthenstopsexploringbecauseofthenegativeexpected
utility. (c)WithoutBMR,allplayerpositionsgetsmallclustersassigned,whichinthiscasehelpsthe
playertocross,asvisitingtheselocationsisnowrenderedinformationgaining.
average. Thereforeforourmainevaluationsweused512policiesand3samplesperpolicy,butthe
resultsinFigure5Figure9suggestthatwhencomputetimeislimited,scalingthenumberofpolicies
downto128or64isaviablewaytoincreaseefficiencywithoutsacrificingperformance.
E.3 Perturbations
Perturbations. OneadvantageoftheGameworld 10kbenchmarkisitsabilitytoapplyhomoge-
neousperturbationsacrossenvironments,allowingustoquantifyhowrobustdifferentmodelsareto
changesinvisualfeatures. Inourcurrentexperiments,weintroducetwotypesofperturbations: a
colorperturbation,whichaltersthecolorsofallspritesandthebackground(seeFigure10b),anda
shapeperturbation,whichtransformsprimitivesfromsquaresintocirclesandtriangles(seeFigure
10c).
Toassessmodelrobustness,weapplyeachperturbationhalfwaythroughtraining(at5,000steps)and
plottheaveragerewardforAxiom,Dreamer,andBBFacrosseachgameinFigure11. Underthe
shapeperturbation,Axiomdemonstratesresilienceacrossgames. Weattributethistotheidentity
model(iMM),whichsuccessfullymapsthenewshapesontoexistingidentitiesdespitetheiraltered
appearance. Underthecolorperturbation,however,Axiom‚Äôsperformanceoftendrops-suggesting
theidentitymodelinitiallytreatstheperturbedspritesasnewobjects-butthenrapidlyrecoversasit
reassignsthosenewidentitiestothepreviouslylearneddynamics.
OurresultsalsoshowthatBBFandDreamerarerobusttoshapechanges. Forthecolorperturbation,
Dreamer - like Axiom - sometimes experiences a temporary performance decline (for example,
in Explode) but then recovers. BBF, by contrast, appears unaffected by either perturbation. We
hypothesizethatthisresiliencestemsfromapplyingtheperturbationearlyintraining-beforeBBF
hasconverged-sothatalteringvisualfeatureshasminimalimpactonitslearningdynamics.
Remappedslotidentityperturbations Inthisperturbation,shownbythepurplelineinFigure11,
weperformedaspecialtypeofperturbationtoshowcasethe‚Äòwhite-box‚Äô, interpretablenatureof
AXIOM‚Äôs world model. For this experiment, we performed a standard ‚Äòcolor perturbation‚Äô as
describedabove,butafterdoingso, weencodeknowledgeabouttheunreliabilityofobjectcolor
into AXIOM‚Äôs world model. Specifically, because the latent object features learned by AXIOM
aredirectlyinterpretableasthecolorsoftheobjectsintheframe,wecanremovetheinfluenceof
the latent dimensions corresponding to color from the inference step that extracts object identity
(namely,theinferencestepoftheiMM),andinsteadonlyuseshapeinformationtoperformobject
33
0.000
-0.015
0 10000
)K1(
draweR
egarevA
Aviate Bounce Cross Drive Explode
0.000
0.000 0.025
0.000
-0.015 -0.008 0.000
-0.010
0 10000 0 10000 0 10000 0 10000
0.020
0.000
0 10000
Step
)K1(
draweR
egarevA
Fruits Gold Hunt Impact Jump
0.000
0.040
0.020 0.020
-0.008
0.000
0.000 0.000
0 10000 0 10000 0 10000 0 10000
Step Step Step Step
AXIOM (64 x 1) AXIOM (128 x 1) AXIOM (256 x 1) AXIOM (512 x 1)
(a)
0.000
-0.015
0 10000
)K1(
draweR
egarevA
Aviate Bounce Cross Drive Explode
0.000
0.000 0.025
0.000
-0.006
0.000
-0.020
-0.015
0 10000 0 10000 0 10000 0 10000
0.025
0.000
0 10000
Step
)K1(
draweR
egarevA
Fruits Gold Hunt Impact Jump
0.000
0.040
0.025
0.020
-0.008
0.000
0.000 0.000
0 10000 0 10000 0 10000 0 10000
Step Step Step Step
AXIOM (64 x 3) AXIOM (128 x 3) AXIOM (256 x 3) AXIOM (512 x 3)
(b)
0.000
-0.015
0 10000
)K1(
draweR
egarevA
Aviate Bounce Cross Drive Explode
0.010 0.000 0.030
0.000
0.000
-0.015 -0.008 0.000
-0.010
0 10000 0 10000 0 10000 0 10000
0.020
0.000
0 10000
Step
)K1(
draweR
egarevA
Fruits Gold Hunt Impact Jump
0.040 0.000
0.025 0.025
-0.008
0.000
0.000 0.000
0 10000 0 10000 0 10000 0 10000
Step Step Step Step
AXIOM (64 x 5) AXIOM (128 x 5) AXIOM (256 x 5) AXIOM (512 x 5)
(c)
Figure9: Ablationontheamountofsampledpolicies. Thelabelindicatesthenumberofpolicies
√ónumberofsamplesforthatpolicy(a)1Sample(b)3Samples(c)5Samples
34
(a)Noperturbation.
(b)Colorperturbation(spritesandbackgroundrecolored).
(c)Shapeperturbation(primitivesreplacedwithcirclesandtriangles).
Figure10:Perturbations.Sampleframesfromeachofthetenenvironmentsunder(a)noperturbation,
(b)acolorperturbation,and(c)ashapeperturbation.
typeinference. Inpractice,whatthismeansisthatslotsthatchangedcolorsdon‚Äôtrapidlygetassigned
newidentities,meaningthesameidentity-conditioneddynamics(clustersoftherMM)canbeused
topredictandexplainthebehaviorofthesameobjects,despitetheircolorhavingchanged. This
explainstheabsenceofaneffectofperturbationforsomegameswhenusingthis‚Äòcolorremapping‚Äô
trickatthetimeofperturbation,especiallytheoneswhereobjectidentitycaneasilybeinferredfrom
shape,suchasExplode. Figure12showstheiMMidentityslots,withandwithoutthe‚Äòremapping
trick‚Äô. Impactonperformanceforallgamesisshownin11d). Forgameswherecertainobjectshave
thesameshape(e.g.,rewardsandobstaclesinHunt,orfruitsandrocksinFruits),thisremappingtrick
hasnoeffectbecauseshapeinformationaloneisnotenoughtoinferobjecttypeandthusconditionthe
dynamicsonobjecttypes. Insuchcases,onemightusemorefeaturestoinfertheobjectidentity,such
aspositionordynamics,butextendingourmodeltoincorporatethesetofurtherimproverobustness
isleftasfuturework.
35
Figure11: Impactofperturbationsonaveragereward. Smoothed1k-stepaveragerewardsfor
Axiom,BBF,andDreameracrosstengamesunder(a)noperturbation,(b)colorperturbation,(c)
shapeperturbation,and(d)Axiom‚Äôscolorperturbationwithandwithoutremapping.
36
(a)noperturbation (b)colorperturbation (c)colorperturbationwithremap
Figure12: iMMidentityslotsonExplode. (a)OnExplode,theiMMconstructsaslotfortheplayer,
bomber and bomb respectively. (b) Whencolor is perturbed, novel slots are created forthe blue
(player)andthepinkbomb,andtheyellowenemyismappedontotheoldplayerslot. (c)However,
withcolorremapping,theblueplayer,yellowbomberandpinkbombarecorrectlyremappedtothe
oldplayerandbomberslotsbasedontheirshape.
F Relatedworks
Object-CentricWorldModels. Thefirstbreakthroughsindeepreinforcementlearning,thatleveraged
deepQnetworkstoplayAtarigames[39],werenotmodel-based,andrequiredtrainingonmillions
ofimagestoreachhuman-levelperformance. Tothisend,recentworkshaveleveragedmodel-based
reinforcementlearning,whichlearnsworldmodelsandhencegeneralizesusingfewerenvironment
interactions [40, 41]. A notable example is the Dreamer set of models, which relies on a mix of
recurrentcontinuousanddiscretestatespacestomodelthedynamicsoftheenvironment[36,42,43].
Thisclassofworldmodelssimulatesaspectsofhumancognition,suchasintuitiveunderstandingof
physicsandobjecttracking[5,7]. Tothisend,itispossibletoaddpriorknowledgetothisclassof
architectures,inawaythatspecificstructuresoftheworldarelearnedfasterandbetter. Forexample,
modelinginteractionsattheobjectlevelhasshownpromisingperformanceinimprovingsample
efficiency,generalization,androbustnessacrossmanytasks[9‚Äì12].
Inrecentyears,thefieldofobjectsegmentationhasgainedmomentumthankstotheintroductionof
modelslikeIODINE[44]andSlotAttention[45],whichleveragesthestrengthsandefficiencyof
self-attentiontoenforcecompetitionbetweenslotlatentsinexplainingpixelsinimagedata. Theform
ofself-attentionusedinslotattentionisclosely-relatedtotheE-andM-stepsusedtofitGaussian
mixturemodels[46,47],whichinspiredour,whereAXIOMsegmentsobjectfromimagesusing
inferenceandlearningoftheSlotMixtureModel. Examplesofimprovementsoverthisseminalwork
includeLatentSlotDiffusion,whichimprovesupontheoriginalworkusingdiffusionmodelsand
SlotSSM[48]whichusesobject-factorizationnotonlyasaninductivebiasforimagesegmentation
butalsoforvideoprediction. Recentworksthathavealsoproposedobject-centric, model-based
approaches are FOCUS, that confirms how such approaches help towards generalization in the
lowdataregimeforrobotmanipulation[49],andOC-STORMandSSWM,thatuseobject-centric
informationtopredictenvironmentdynamicsandrewards[14,50]. Toconclude,SPARTANproposes
theuseofalargetransformerarchitecturethatidentifiessparselocalcausalmodelsthataccurately
predictfutureobjectstates[13]. UnlikeOC-STORM,whichusespre-extractedobjectfeaturesusing
apre-trainedvisionfoundationalmodelandsegmentationmasks,AXIOMlearnstoidentifysegment
objectsonlinewithoutobject-levelsupervision(albeitsofarwehaveonlytestedAXIOMonsimple
objectslikemonochromaticpolygons). AXIOMalsogrowsandprunesitsobject-centricstate-space
online,butlikeOC-STORMplansusingtrajectoriesgeneratedfromitsworldmodel.
BayesianInference. Inference, learning, and planning in our model are derived from the active
inferenceframework,thatallowsustointegrateBayesianprincipleswithreinforcementlearning,
balancingrewardmaximizationwithinformationgainbyminimizingexpectedfreeenergy[15,16].
Tolearnthestructureoftheenvironment,wedrawninspirationfromfaststructurelearningmethods
[24],thatfirstaddmixturecomponentstothemodel[51]andthenprunesthemusingBayesianmodel
reduction[21,22,24]. Ourapproachtotemporalmixturemodelingsharesconceptualsimilarities
withrecentworkonstructure-learningGaussianmixturemodelsthatadaptivelydeterminethenumber
ofcomponentsforperceptionandtransitionmodelinginreinforcementlearningcontexts[52]. An
importantdistinctionbetweenAXIOM‚Äôsmodelandtheoriginalfaststructurelearningapproach[23],
isthatAXIOMusesmorestructuredpriors(intheformoftheobject-centricfactorizationofthe
sMMandthepiecewiselineartMM),andusescontinuousmixturemodellikelihoods,ratherthan
purelydiscreteones. Thetransitionmixturemodelweuseisatypeoftruncatedinfiniteswitching
lineardynamicalsystem(SLDS)[29,53,54]. Inparticular,werelyonarecentformulationcalled
therecurrentSLDS[19],thatintroducesdependenceoftheswitchstateonthecontinuousstate,to
addresstwokeylimitationsofthestandardSLDS:state-independenttransitionsandcontext-blind
dynamics. OurinnovationisinhowwehandletherecurrentconnectionoftherSLDS:wedothis
37
usingagenerative, asopposedtodiscriminative, modelfortheswitchingstates. Thisallowsfor
moreflexibleconditioningoftheswitchstateonvariousinformationsources(bothcontinuousand
discrete),aswellasaswitchdependencethatisquadraticinthecontinuousfeatures;thisovercomes
the intrinsic linear separability assumptions made by using a classic softmax likelihood over the
switchstate,asusedintheoriginalrSLDSformulation[19,55].
38

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "AXIOM: Learning to Play Games in Minutes with Expanding Object-Centric Models"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ‚ùå BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ‚úÖ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ‚ùå BAD: Repeating the same claim 3+ times with slight variations
   ‚úÖ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
