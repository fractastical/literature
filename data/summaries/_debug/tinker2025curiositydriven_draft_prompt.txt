=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Curiosity-Driven Development of Action and Language in Robots Through Self-Exploration
Citation Key: tinker2025curiositydriven
Authors: Theodore Jerome Tinker, Kenji Doya, Jun Tani

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Abstract: Human infants acquire language and action gradually through de-
velopment, achieving strong generalization from minimal experience,
whereas large language models require exposure to billions of training
tokens. What mechanisms underlie such efficient developmental learn-
inginhumans?Thisstudyinvestigatesthisquestionthroughrobotsim-
ulationexperimentsinwhichagentslearntoperformactionsassociated
withimperativesentences(e.g.,pushredcube)viacuriosity-drivenself-
exploration. Our approach integrates ...

Key Terms: compositional, driven, action, generalization, language, development, actions, robots, curiosity, learning

=== FULL PAPER TEXT ===

Curiosity-Driven Development of Action and
Language in Robots Through Self-Exploration
Theodore J. Tinker1, Kenji Doya1, Jun Tani1âˆ—
1 OkinawaInstituteofScienceandTechnology,Okinawa,Japan.
* To whom correspondence should be addressed; E-mail: jun.tani@oist.jp.
Abstract
Human infants acquire language and action gradually through de-
velopment, achieving strong generalization from minimal experience,
whereas large language models require exposure to billions of training
tokens. What mechanisms underlie such efficient developmental learn-
inginhumans?Thisstudyinvestigatesthisquestionthroughrobotsim-
ulationexperimentsinwhichagentslearntoperformactionsassociated
withimperativesentences(e.g.,pushredcube)viacuriosity-drivenself-
exploration. Our approach integrates the active inference framework
with reinforcement learning, enabling intrinsically motivated develop-
mental learning. The simulations reveal several key findings: i) Gen-
eralization improves markedly as the scale of compositional elements
increases. ii) Curiosity combined with motor noise yields substantially
better learning than exploration without curiosity. iii) Rote pairing of
sentencesandactionsprecedestheemergenceofcompositionalgeneral-
ization.iv)Simpler,prerequisite-likeactionsdevelopearlierthanmore
complex actions that depend on them. v) When exception-handling
rules were introducedâ€”where certain imperative sentences required
1
5202
ceD
4
]LM.tats[
4v31050.0152:viXra
executing inconsistent actionsâ€”the robots successfully acquired these
exceptionsthroughexplorationanddisplayedaU-shapedperformance
curvecharacteristicofrepresentationalredescriptioninchildlanguage
learning. Together, these results suggest that curiosity-driven explo-
rationandactiveinferenceprovideapowerfulaccountofhowintrinsic
motivation and hierarchical sensorimotor learning can jointly support
scalable compositional generalization and exception handling in both
humansandartificialagents.
Introduction
Acentralquestioninbothcognitivescienceandartificialintelligenceishowhumansandartificial
systems can acquire competencies for language and action developmentally, despite having access
to only limited learning experiences. This question is exemplified in human infants, who achieve
remarkablegeneralizationwithsparseinput.Thisisastarkcontrasttolarge-scalemodelswhichrely
onmassivetrainingcorporatoreachsimilarcapabilities.Thisraisestheissueofwhatmechanisms
enablesuchefficientdevelopmentallearning.
Fromtheperspectiveofdevelopmentalpsychology,infantsacquirelanguagethroughrichinter-
actionwiththeirembodiedenvironments.Tomaselloâ€™sâ€œverb-islandâ€hypothesisarguesthatchildren
initiallylearnverbsinspecific,isolatedcontextsbeforegeneralizingacrossbroaderlinguisticstruc-
tures (1). He also emphasized the importance of embodiment in language acquisition, suggesting
that grounding linguistic symbols in sensorimotor experiences is fundamental to language learn-
ing (1). This view aligns with other studies in developmental psychology highlighting the role of
compositionalityandgeneralizationinlanguageacquisition(2,3,4).
In linguistic terms, compositionality refers to the ability to construct novel configurations by
systematically combining elements such as verbs, adjectives, and nouns. Generalization enables
infants to apply learned components flexibly, allowing for the production and interpretation of
utterances that have not been directly encountered previously. Although the number of possible
compositions grows multiplicatively with the vocabulary size (i.e., number of verbs Ã— number of
adjectivesÃ—numberofnouns),infantsachievegeneralizationafterexperiencingonlyasmallsubset
2
of learning examples. This suggests that the effective sample complexity could be proportional to
thesumofelementsratherthantheirproduct.Thisphenomenoniscloselyrelatedtotheâ€œpovertyof
thestimulusâ€problemarticulatedbyChomsky(5),whichaskshowlearnersgeneralizesoeffectively
givenseverelysparseinput.
To investigate these mechanisms, one promising approach is to reconstruct developmental
learning processes in machines and robots. The field of developmental robotics has long pursued
this line of research, aiming to replicate human-like learning trajectories in embodied systems
(6,7,8,9). However, relatively few studies have focused on development of language and motor
control under conditions of stimulus poverty. Existing work has primarily examined associative
mappings between linguistic input and motor commands in one-shot or supervised batch learning
schemes(10,11,12,13).Theseapproachesneglecttheself-directed,developmentalcontextofinfant
learning.
Inthisstudy,weproposeaself-exploratorylearningframeworkofrobotsinwhichreinforcement
learningisincorporatedwiththeactiveinferenceframework(14,15,16),enablingcuriosity-driven
exploration. Our approach to integrate reinforcement learning with active inference was originally
inspired by the work of Kawahara et al. (17). In our model, originally introduced in (18), motor
commands are reinforced by two intrinsic rewards: curiosity (seeking unpredictable sensory con-
sequences)andmotorentropy(seekingrandommovements).Motorcommandsarealsoreinforced
byextrinsicrewardsforsuccessfullyachievinggoalverbsspecifiedbygivenimperativesentences.
Importantly,ourpreviousexperimentsinmazenavigationdemonstratedthatthecombinationofcu-
riosityandmotorentropyiscrucialforenhancingself-exploration,asagentsachievedsignificantly
improvedexploratorybehaviorsunderthisdual-intrinsicrewardscheme.Ourapproachalignswith
broaderresearchonself-explorationinmachinelearning,inwhichagentsareintrinsicallyrewarded
fortakingmotorcommandsthatincreaseunpredictabilityorinformationgain(19,20).
Asimulatedmobilerobotequippedwithamanipulatorarm,visionsensor,anddistributedtactile
sensors learns to generate motor movements in response to imperative sentences presented during
each trial. These sentences are systematically composed of verbs, adjectives, and nouns, enabling
evaluationofgeneralizationperformanceunderdifferentlevelsofcompositionalcomplexity.
Beyond these primary developmental processes, we further explore the capacity for exception
handlingâ€”a hallmark of flexible cognition. In human development, exceptions such as irregular
3
verbs or inconsistent mappings often produce non-monotonic, U-shaped learning trajectories:
children first apply a correct form, then overgeneralize it (producing errors), and finally recover
the correct rule. This pattern has been widely interpreted as evidence of internal representational
reorganizationorrepresentationalredescription(21).Computationally,suchU-shapedperformance
has been demonstrated in models of language acquisition and rule learning (22,23,24,25,26).
Developmentally, these phenomena reflect the tension between rote memorization, generalization,
andthelaterrefinementofexceptionrules.
To examine whether similar mechanisms can emerge in artificial developmental systems, we
hypothesizethatrobotstrainedthroughcuriosity-drivenactiveinferencecanalsoacquireexception-
handling rules through exploratory learning. Specifically, we predict that when presented with a
small number of abnormal linguistic rulesâ€”commands requiring reversed or inconsistent ac-
tionsâ€”the robot will exhibit a U-shaped performance curve: initial correct learning, followed by
a decline due to overgeneralization, and eventual recovery as exception-specific representations
are internalized. This pattern would indicate that the system develops not merely through associa-
tive reinforcement, but through representational reorganization analogous to cognitive processes
observedinhumaninfants.
This study therefore tested the following hypotheses through simulation experiments: H1:
Generalization performance improves as the scale of compositionality in the task increases. H2:
Curiositycombinedwithmotorentropyenhancestheperformanceofdevelopmentallearning.H3:
In the early phase, actions are generated only for exactly learned imperative sentences, but in later
phases,thesystemgeneralizestonovel,unlearnedcompositions.H4:Primitiveactionsareacquired
earlier, followed by more complex, prerequisite-dependent actions. H5: Exception-handling rules
can be acquired through exploratory learning, exhibiting U-shaped developmental performance
similartothatobservedinhumancognition.
4
Results
Task Description
We created a robot like a truck crane in a physics simulator along with a set of objects with 5
differentshapeseachofwhichcanbewith6differentcolors(seeFig.1). Therobotcanmaneuver
Fig. 1. The simulated robot and a set of objects to act on. (A) The robot has two wheels and an
arm with two joints. The design is similar to a truck crane. (B) Left to right: a red pillar, a green
pole,abluedumbbell,acyancone,andamagentahourglass.Thecoloryellowisnotpicturedhere.
by controlling velocity of left and right wheels independently, and also can move its arm by
controlling rotation velocity of the yaw and pitch joint angles for acting on the objects. A camera
with 16 x 16 pixels was fixed to the body for visual sensation. 16 touch sensors were distributed
in the body and the arm, respectively, and rotation angles for the yaw and pitch were sensed as
proprioception.
Foreachtrialepisode,ataskgoalwasgivenintermsofanimperativesentencecomposedwith
verb, adjective, and noun. Possible words used for them are shown in Table 1. In the beginning of
eachepisode,twoobjectswerelocatedatrandompositionsinthearenawhereinoneobjectwasthe
one specified in the imperative sentence and the other was the one with randomly selected color
andshapecombinationamongpossibleones.
At each step, the robot receives visual sensation, proprioception for the arm, tactile sensation ,
5
EnglishWords
Verb Adjective Noun
watch red pillar
benear green pole
touchthetop blue dumbbell
pushforward cyan cone
pushleft magenta hourglass
pushright yellow
Table1.Englishwords.TheEnglishwordsusedforimperativesentencesspecifyinggoals.
andtwotypesofvoices:thecommandvoiceandthetutor-feedbackvoice.Thecommandvoicetakes
the format of the imperative sentence described previously, and it comes every step continuously
fromthebeginning.Ontheotherhand,thefeedbackvoicearriveswhenevertherobotachievesone
of possible goals even if the achieved goal is not the imperative sentence told by the command
voice, and it informs which goal has been achieved actually in the same format with the command
voice. This potentially enhances the forward model to learn about own action as associated with
linguistic representation. Finally, when the goal specified by the command voice is achieved, a
reward is provided. Each trial episode ran for 30 steps, or terminated when the specified goal is
achieved.
Effects of curiosity: Experiment 1
The experiment examined effects of different levels of curiosity to the developmental learning
processesusingthebasicsetup.Inthebasicsetup,fullcompositionsofwords(Table)wereusedto
generate the imperative sentences. However, the training was conducted using only 60 imperative
sentences (33 percentage) out of 180 possible sentences. 120 untrained sentences were used for
generalizationtest.Fortenrobotswithdifferentrandomseeds,thecompletedevelopmentallearning
processwasiteratedfor60000epochs.Thegeneralizationtestwithunlearnedimperativesentences
wasconductedforevery50epochs.
The experiment was conducted by changing the levels of curiosity. Since the random latent
6
variables are computed separately for each sensory modality, the complexity or curiosity can
be computed for each sensory modality. Three levels of curiosity were considered in computing
expected free energy ğº wherein no curiosity: the curiosity terms for all sensory modalities are
not included, sensory-motor curiosity: the curiosity terms only for vision, tactile sensation, and
proprioception are included, all curiosity: the curiosity terms for all sensory modalities including
feedbackvoiceareincluded.
Fig.2showsthedevelopmentofthegeneralizationtestperformancesintermsofsuccessratefor
goals specified by unlearned imperative sentences which are plotted for different action categories
withdifferentlevelsofcuriosity.Shadesareasrepresent99%confidenceintervals.
The plots shows that the performance was improved significantly as the curiosity level was
increased. Especially, the case of all actions with the all curiosity level shows the average success
rate for unlearned goals reached a quite high value of 85 percentage even though the learning was
conductedonlyfor33percentageofallpossiblecompositions.
It can be also seen in Fig. 2 that some action categories developed faster than other action
categories. Especially in the all curiosity case, â€œwatchâ€ developed the fastest, â€œbe nearâ€ did as the
second, â€œtouch the topâ€ as the third, and â€œpush leftâ€ â€œpush rightâ€, and â€œpush forwardâ€ developed
much later. This implies that simpler prerequisite-type actions develop earlier, and more complex
actionsrequiringthoseprerequisiteactionsdeveloplater.Actually,anactionofwatchinganobject
should be prerequisite for all other object-targeted actions including an action of moving near by
an object, which should be prerequisite again for actions of directly manipulating an object like
pushingleft/rightanobjectortouchingthetopofit.Ourobservationaccordswiththis.
Next,Fig.6(A)showsthesuccessratecomparisonbetweenlearnedandunlearnedgoalsunder
the all curiosity condition for each action category. These plots show that the test performance for
learned goals developed significantly faster than the case for the unlearned goals. This indicates
that actions are generated only for exactly learned compositional imperative sentences in the early
phase,butthesystemgeneralizestonovel,unlearnedonesinthelaterphase.
Video S1 shows examples of the behaviors of robots with the all curiosity level. It can be seen
thatintheintermediatephaseofdevelopment,therobotoftenactswithplay-likebehaviorwithout
achieving specified goals while it quickly and accurately achieves its goals in the final phase of
development.
7
Fig. 2. Rolling success-rates for unlearned goals. Compares agents with different levels of
curiosity.
Further analysis
Someanalysiswereconductedforthepurposeofgainingcomprehensionoftheinternalrepresenta-
tion developed. We applied Principle Component Analysis (PCA) to the estimated posterior latent
states corresponding to the command voice input, incorporating the verbs and adjectives stated in
each command. Fig. 3 presents evidence that robots with all curiosity developed a compositional
and generalizable understanding of these goals. At the midpoint of training, the latent representa-
8
tions begin to show consistent grouping of verbs and adjectives. For example, the verbs â€œwatch,â€
â€œbe near,â€ and â€œpush forwardâ€ are tightly grouped, suggesting that these verbs are interpreted as
similar.Incontrast,theverbâ€œpushrightâ€appearsheavilyseparatedfromotherverbs,andtheverbs
â€œtouch the topâ€ and â€œpush leftâ€ also appear as distinct categories. After training, these clusters
of verbs are more compact and separated. Within these clusters, there is loose sub-structuring by
color:greenandyellowtendtobeontheleftsideofthecluster,whileblueandmagentaareonthe
right side of the cluster. Therefore, it can be said that each cluster represents a distinct linguistic
conceptwhileexhibitingrelationshipwithotherssincethelearningofvisuo-proprioceptive-motor
also contributes to this structuring. In contrast, Fig. 4 shows the same type of PCA results for a
robotwithnocuriosity.â€œBenear,â€â€œpushforward,â€andâ€œpushrightâ€areheavilyentangledwitheach
other.Thussupportstheideathatcuriosityaidsverbdisentanglementandcompositionallearning.
In the current model, the robotâ€™s knowledge of the environment should develop richer along
the course of exploratory learning. For confirmation of this idea, we examined the capability of
the robots in generating mental plans for achieving goals without accessing the sensory inputs
except the initial step for an episode trial as compared between the half-trained case and the fully
trained case. A robotâ€™s mental planning can be visualized by allowing it to receive real sensory
observation only at the initial step, after which the robot must rely entirely on its own internal
predictions. In this setting, the robot views its predicted sensory observations as if they are true
inputs. This process may be likened to a dreamlike state or hallucinatory simulation, in which the
robotmentallysimulatesfutureeventsbasedonitsinternalmodeloftheworld.Fig.5(A)illustrates
a simulation example for the fully trained case. The robot was commanded to touch the top of the
yellow pillar. In that figure, the first row shows the ground truth environment from a view behind
therobotâ€™sshoulder.Thesecondrowshowswhattherobotwouldtrulyobserveifitwerenotinthis
planning setting. The third row shows the robotâ€™s look ahead predictions for visual observations,
which it will interpret as if they are real. It can be seen that even with only the first step sensory
observation,therobotcouldgeneratemostlyaccuratefuturelook-aheadpredictionforsensationas
well as motor command. These predictions are sufficiently accurate for the robot to maintain an
internal conceptualization of the environment and complete its command in the case of the end of
thedevelopmentallearning.Fig.5(B)illustratesthesamerobotafteronlyhalfofitstraininginthe
samescenario.Inthiscase,therobotâ€™spredictionsareinaccurate,causingittowanderandviewan
9
Fig. 3. PCA for language latent variables in the case of all curiosity. PCA of latent variables
corresponding to the command voice. (A) Halfway through development. (B) After complete
development.Clusterswithsubstructuresemergedearlyandbecamemorerefinedovertime.
objectwhichdoesnotactuallyexist.
Effects of scale in compositions: Experiment 2
Next experiment examines the effects of scales of compositionality in learned examples to the
generalization performance. For this purpose, experiments were conducted using reduced number
of words for generating imperative sentences. While the previous basic setup used sentences
10
Fig. 4. PCA for language latent variables in the case of no curiosity. PCA applied to latent
representationsofcommandvoiceinputsaftercompletedevelopment.
composed of 6 verbs, 6 adjectives, and 5 object nouns as the full scale case, the middle scale case
wasprepared with5verbs, 5adjectives,and 4objectnouns, andthesmall scalecase with4verbs,
4 adjectives, and 3 object nouns. The exact words used for each setup are listed in Table 2. For
all scaling cases, again only one third was used for learning examples while remained two third
was used for generalization test. Other experimental conditions were also set as the same as the
Experiment1.
TheexperimentalresultsareshowninFig.6.Shadesareasrepresent99%confidenceintervals.
Itcanbeseenthatalthoughthelearnedgoaltestcasesshowequallyhighperformanceforallscales
of compositionality, the generalization test for unlearned goal case shows that the success rate
in the final trial decreases significantly (85 percentage to 25 percentage) as the compositionality
scale decreases. This indicates that the generalization performance severely depends on the scale
ofcompositionalityinlearningexamples.
11
Fig. 5. Mental plans generated by the robot. This robot is commanded to touch the top of the
yellow pillar. The first row displays the ground truth from a view over the robotâ€™s right shoulder.
The second row displays the visual sequence of the ground truth. The third row shows the visual
sequence of mental planning. (A) The case for the end of complete developmental learning, and
(B)forthecaseofhalfwaydeveloped.
Exception rule handling: Experiment 3
Thisexperimentexaminedhowrobotscanacquireexceptionhandlingrulesthroughdevelopmental
learning. While most commandâ€“action mappings were preserved, the commands â€œwatch magenta
pillarâ€ and â€œbe near green poleâ€ were swapped: success required performing the other goal, not
the one commanded. These mismatches required the robot to override its learned generalized
knowledge.
Thissimulationexperimentwasconductedusingthesamemodelparameterswiththeonesused
inthepreviousexperimentswith10robotswith60,000epochsofdevelopmentaltrials.Attheend
ofdevelopmenttheaveragesuccessrateamong10robotswas84percentageforthelearnedgoals,
76percentageforunlearnedgoals,and50percentagefortheexceptionhandlingcases.Theaverage
12
Fig. 6. Rolling success-rates for learned and unlearned goals with different compositionality
scales.(A)Agentstrainedwithallsixverbs,allsixadjectives,andallfivenouns.(B)Agentstrained
withfiveverbs,fiveadjectives,andthreenouns.(C)Agentstrainedwithfourverbs,fouradjectives,
andthreenouns.
successratefortheexceptionhandlingcasesisnotsohighmostlikelyduetoover-generalization.
PanelAinFig.7showstherollingsuccess-rateforachievingtheexceptiongoalsforeachof10
individualrobotswhilePanelBshowsthesuccessrateforthesamegoalsbutwithoutapplyingthe
exception handling rules. The final success rate for the exception handling case is diverse ranging
from 15 percentage to 90 percentage as can be seen in Panel A. It was also discovered that 7 out
of 10 robots trained with these exceptions exhibit characteristic U-shaped curves: early success,
followedbyadrop,andeventualrecoverywithhighersuccessratethantheearlierone.Incontrast,
monotonicincreaseofsuccessratecanbeseeninall10individualsinthecaseoflearningwithout
theexceptionhandlingrules.Statisticalcomparisons(detailedintheSupplementaryText)confirm
thatU-shapedpatternsaresignificantlymoreprevalentintheexceptionconditionthaninthecontrol
(ğ‘ = .0025).
InFig.8,PCAillustrateshowtheinternalrepresentationsofofgoalsintheseventhrobotinFig.
7 A evolve in a manner consistent with the U-shaped success rates. In panel A, early in training,
goal embeddings are muddled without clear structure, reflecting a learning phase with minimal
generalization. In panel B, midway in training, the exception command â€œwatch magenta pillarâ€ is
embeddednearotherâ€œwatchâ€goals,whileâ€œbeneargreenpoleâ€clusterswithotherâ€œbenearâ€goals,
13
Name Verbs Adjectives Nouns
Watch Red
Pillar
BeNear Green
Pole
TouchtheTop Blue
LargestVocabulary Dumbbell
PushForward Cyan
Cone
PushLeft Magenta
Hourglass
PushRight Yellow
Watch Red
Pillar
BeNear Green
Pole
ReducedVocabulary PushForward Blue
Dumbbell
PushLeft Cyan
Cone
PushRight Magenta
Watch Red
Pillar
PushForward Green
SmallestVocabulary Pole
PushLeft Blue
Dumbbell
PushRight Cyan
Table2.Verbs,adjectives,andnounswhichareusedfortrainingagentsinthreedifferentways.
despite these associations being incorrect. These observation indicates that overgeneralization has
occurred. In panel C, late in training, â€œwatch magenta pillarâ€ is now embedded near â€œbe nearâ€
goals,andâ€œbeneargreenpoleâ€nearâ€œwatchâ€goals,indicatingthattherobothascorrectlyhandled
these exceptions as swapped pairs. Here, it can be said that the representational redescription took
placeinthecourseofdevelopmentallearningofexceptionhandlingrules.
Discussion
This study investigated how robots can develop action and language through self-exploration by
integrating active inference with reinforcement learning. The experiments were designed to test
fourspecifichypotheses,andtheresultsprovideclearsupportforeach.
14
Fig. 7. Comparison of the performance curve with and without applying the exception han-
dling rules. (A) The development of success rate in achieving two goals which are swapped as
exceptions.Redverticallinesdepictthepeaksandvalleysofthelearning,asdefinedinsupplemen-
tary text. (B) The development of success rate in achieving the same two goals without applying
theexceptionhandlingrules.
15
Fig. 8. PCA for language latent variables for an individual developed with the exception
handling rules. (A) Plot at 20000 epochs, (B) plot at 32500 epochs, and (C) plot at 57500 epochs
whereinthecircledâ€Nâ€andcircledâ€Wâ€denotethesentencesappliedwiththeexceptionhandling
.
H1: Generalization is enhanced by compositional scale. The experiments confirmed that
larger vocabularies of verbs, adjectives, and nouns led to greater generalization. Robots trained
with richer compositional repertoires achieved higher success rates on unlearned actions, whereas
smaller vocabularies constrained generalization severely. Our previous study (13) on supervised
16
training of language and action for an arm robot also showed that compositional generalization
improved as the size of verbâ€“noun combinations in training increased. However, that work was
limitedinscale,examiningonlycasesfrom3Ã—3to5Ã—8verbâ€“nouncombinations,wheresuccess
rates for unlearned goals improved only from 57% to 71% under the condition of 80% training.
By contrast, the present study examined much broader scaling, ranging from 48 to 180 possible
compositions, while using only 33% of them for training. Under these conditions, generalization
performance improved dramatically from near 25% to 85%. This contrast highlights that scale
plays a critical role in enhancing generalization, and that curiosity-driven developmental learning
provides a more powerful mechanism than supervised schemes under conditions of limited input.
Thefindingalsoconnectstotheclassicalâ€œpovertyofthestimulusâ€problemraisedbyChomsky(5),
asitshowshowcompositionalityenablespowerfulgeneralizationfromsparsetrainingdata.Once,
wehypothesizethatnecessarytrainingsizecouldbeproportionaltosummationofnumberofwords
appeared for each dimension instead of multiplication of it for all dimensions if compositionality
size increases (13). This hypothesis becomes more plausible by the results in the current study
which,however,shouldbeconfirmedinmuchmorescaledexperimentsinthefuture.
H2:Curiositycombinedwithmotorentropyenhancesdevelopmentallearning.Thesecond
hypothesiswasalsoconfirmed.Robotsequippedwithbothcuriosity-drivenexplorationandmotor
entropy consistently outperformed those without, achieving higher success rates in both learned
andunlearnedactions.Thisadvantagewasparticularlypronouncedwhencuriosityextendedacross
allsensorymodalities,includingvision,touch,proprioception,andvoicefeedback.Thesefindings
suggest that the synergy of curiosity (seeking novel, unpredictable outcomes) and motor entropy
(encouragingstochasticexploration)playsacrucialroleinacceleratingtheacquisitionoflanguage-
actionmappings.Thisresultisconsistentwithourpreviousstudy(18),whichshowedthatcombining
curiosityandmotorentropysignificantlyenhancedself-explorationinamazenavigationtask.More
broadly, this interpretation aligns with the active inference framework, in which agents minimize
expectedfreeenergybyreducinguncertaintythroughmaximizinginformationgain(16,27).
H3: Generalization follows rote learning. The results again align with this hypothesis: in
early phases, the robot succeeds only on exactly learned sentenceâ€“action pairs, and it is only over
time that the robot begins to generalize to novel combinations of known elements. This mirrors
developmental patterns in infants, who often begin with rigid pairings before achieving broader
17
generalization (28). Tomaselloâ€™s â€œverb-islandâ€ hypothesis, for example, emphasizes that children
initially acquire verbs in isolated contexts before generalizing across broader structures (1), in the
samewaythatthisrobotacquiredtrainedgoalsfirstbeforegeneralizingtountrainedgoals.Gerken&
Knight(2015)demonstratedthat10-to11-month-oldinfantscangeneralizefromjustfourlinguistic
examples under favorable conditions (29). Moreover, Gerken et al. (2014) provide evidence that
infants may generalize even from a single surprising example, suggesting that hypothesis-driven
generalization can follow minimal exposure (30). These studies lend developmental credence to
ourobservedprogressionfromrotemappingtowardflexiblecompositionalgeneralization.
H4: Primitive actions precede complex actions. The fourth hypothesis was also validated.
Simpler, prerequisite-like actions such as â€œwatchâ€ or â€œbe nearâ€ emerged earlier, while more com-
plexmanipulativeactionslikeâ€œpushleftâ€orâ€œtouchthetopâ€developedlater.Thisorderingmirrors
hierarchical dependencies in action acquisition observed in developmental psychology, where in-
fants first master basic motor primitives before acquiring coordinated, goal-directed behaviors.
Such progressive structuring of motor development has been well documented in studies showing
that motor and cognitive skills emerge through iterative interaction between perception, action,
and intrinsic motivation (4,31). The dynamic systems perspective proposed by Smith and Thelen
emphasizes that complex behaviors self-organize from simpler components through embodied ex-
ploration and adaptation, aligning closely with the hierarchical learning patterns observed in our
robotsimulations.
H5:Exception-handlingrulesexhibitU-shapeddevelopment.TheresultsfromExperiment3
providestrongsupportforthishypothesis.Whentrainedwithtwoswappedcommandâ€“actionmap-
pings, most robots initially produced the correct exceptional actions, subsequently regressed due
to overgeneralization, and finally recovered the appropriate exception-specific mappings. This
non-monotonic, U-shaped performance trajectory mirrors a well-established phenomenon in de-
velopmental psychology, in which children first succeed on irregular forms, later overgeneralize
newly learned rules (e.g., producing â€œgoedâ€ for â€œwentâ€), and ultimately reorganize their internal
representations to master both rules and exceptions. Classic accounts interpret these dynamics as
evidence for representational redescriptionâ€”a restructuring of internal knowledge that enables
moreabstract,generativerepresentations(21).
ComputationalmodelinghaslongshownthatsuchU-shapedlearningcanemergenaturallyfrom
18
error-driven or distributed representations, including Rumelhart and McClellandâ€™s connectionist
model of English past-tense acquisition (22), the multilayer perceptron models of Plunkett and
Marchman (23,24), and broader frameworks in computational developmental psychology (26).
Our robot simulations demonstrate that an analogous process arises in curiosity-driven active
inference: the agent first relies on rote pairings, then applies generalized compositional mappings
that overwrite earlier exceptions, and finally reconstructs its latent representation to encode the
exceptionrulescorrectly.
Taken together, these findings demonstrate that curiosity-driven exploration, motor entropy,
hierarchical acquisition of actions, and scalable compositional exposure jointly support efficient
developmental learning of language and action. The parallels with infant developmentâ€”rote-to-
generalization progression, prerequisite learning, the role of vocabulary scale, and representation
redescriptionâ€”suggest that the mechanisms implemented here capture essential aspects of devel-
opmentalpsychology.Morebroadly,theseresultsstrengthentheviewthatreconstructingdevelop-
mental processes in robots can offer insights into the â€œpoverty of the stimulusâ€ problem, showing
how powerful generalization can arise from limited input when guided by intrinsic motivation,
structuredexperience,andtheprinciplesofpredictivecodingandactiveinference(6,11,13,32).
The current study is still limited in many aspects, and several possible extensions can be
envisioned. One crucial limitation is that our experiments examined only a one-directional com-
munication pathway from tutors to robots, relying on the command and feedback voices to guide
the development of languageâ€“action mappings. In contrast, natural human development is charac-
terizedbyinteractiveandbidirectionalcommunication,whereinfantsnotonlyreceiveinstructions
butalsoactivelysolicitguidance,clarification,andscaffoldingfromcaregivers.
Future studies should extend the current framework to include interactive communication
between tutors and robots. For example, when a robot cannot successfully execute a command,
it could initiate a communicative act such as â€œTell me how to do itâ€ or â€œAsk me an easier one.â€
Such exchanges would allow tutors to adapt their teaching strategy dynamically, modulating the
complexity of instructions or providing additional cues. This adaptive interaction resonates with
Vygotskian ideas of scaffolding and the â€œzone of proximal development,â€ where caregivers adjust
supportaccordingtothelearnerâ€™scurrentabilities(33).Italsoalignswithresearchindevelopmental
psychology emphasizing the role of joint attention, imitation, and social feedback in language
19
learning (1,34). Incorporating interactive dialogue would thus move the current model closer to
capturing the social nature of language acquisition in human infants. By embedding mechanisms
forrobotstobothseekhelpandinfluencethetutoringprocess,futureworkcouldshedlightonhow
socialscaffoldingandcommunicativefeedbackacceleratethedevelopmentoflanguageandaction
innaturaldevelopmentalcontexts.
Another promising future direction concerns the development of robotâ€“robot communication
through the evolution of language. Previous research has explored this possibility from different
perspectives: Steels introduced the framework of â€œlanguage gamesâ€ to study the emergence of
shared vocabularies among agents (35), Miikkulainen and colleagues investigated the evolution
of artificial language through evolutionary reinforcement learning (36), and Taniguchi proposed
the emergence of symbols using a collective predictive coding approach (37). While these studies
have demonstrated the possibility of emergent communication in an impressive manner, they still
remain limited in that they mainly achieved the emergence of object labeling or naming, whereas
the evolution of action-related language, such as verbs, has been much less explored. In this
context, the current study based on active inference could be extended to address the evolution of
dynamiclinguisticstructures,includingverbs.Sinceourmodelimplementsactiveinferencewithin
a variational recurrent neural network, it is naturally suited for capturing temporal and dynamic
aspectsofactionandlanguage.Afutureextensionofthisworktowardmulti-robotinteractionunder
the framework of â€œcollective active inferenceâ€ may thus provide novel insights into the evolution
of embodied language, moving beyond static object labeling toward dynamic and action-oriented
communication.
Materials and Methods
In this section, we present the model architecture employed in this study. The current model, as
wellasourearlierwork(38),extendsastudybyKawaharaetal.(17).Thatstudydemonstratedthat
curiosity-driven reinforcement learning can be achieved by incorporating the framework of active
inference (AIF) (16,27), in which motor behavior is reinforced in the direction which minimizes
expected free energy. More details are shown in the â€œFree energy principle, Active Inference, and
Kawahara Modelâ€ section of the Supplementary Materials, along with a brief introduction of the
20
freeenergyprinciple(FEP)andAIF.
The Employed Model
The current model, as well as our previous one (38), extends the approach proposed by Kawahara
et al. (17) by implementing both the forward model and actor-critic using a variational recurrent
neural network (VRNN) (39) in order to deal with temporal complexity and stochasticity inherent
inrobotâ€“environmentinteractions.
Theexpectedfreeenergyğº canbecomputedas:
ğº ğ‘¡ = âˆ’ğœ‚ğ· (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) ğ¾ (cid:32)(cid:32)(cid:32)(cid:32) ğ¿ (cid:32)(cid:32)(cid:32)(cid:32) [ (cid:32)(cid:32) ğ‘ (cid:32)(cid:32)(cid:32)(cid:32) ( (cid:32)(cid:32)(cid:32) ğ‘§ (cid:32)(cid:32)(cid:32) ğ‘¡ (cid:32)(cid:32) | (cid:32)(cid:32) ğ‘œ (cid:32)(cid:32)(cid:32)(cid:32) ğ‘¡ (cid:32)(cid:32) , (cid:32) â„ ğ‘¡âˆ’(cid:32)(cid:32)1(cid:32)(cid:32) ) (cid:32)(cid:32)(cid:32) | (cid:32)(cid:32) | (cid:32)(cid:32) ğ‘ (cid:32)(cid:32)(cid:32)(cid:32)(cid:32) ( (cid:32)(cid:32) ğ‘§ (cid:32)(cid:32)(cid:32) ğ‘¡ (cid:32)(cid:32) | (cid:32)(cid:32) â„ (cid:32)(cid:32)(cid:32)(cid:32) ğ‘¡ (cid:32)(cid:32)âˆ’(cid:32)(cid:32)(cid:32)1(cid:32)(cid:32)(cid:32) )]âˆ’ ğ‘Ÿ( (cid:32)(cid:32)(cid:32) ğ‘  ğ‘¡ ,ğ‘ (cid:32)(cid:32)(cid:32) ğ‘¡ ) âˆ’ğ›¼ (cid:32) H (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) ( (cid:32)(cid:32)(cid:32) ğœ‹ (cid:32)(cid:32)(cid:32)(cid:32) ğœ™ (cid:32) (ğ‘ ğ‘¡ (cid:32)(cid:32) | (cid:32)(cid:32) â„ (cid:32)(cid:32)(cid:32)(cid:32) ğ‘¡ (cid:32)(cid:32) âˆ’ (cid:32)(cid:32)(cid:32)1(cid:32)(cid:32)(cid:32) )) (1)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Curiosity/complexity ExtrinsicReward Entropy
Thisequationisderivedbyreplacingğ‘¤,theprobabilisticmodellearningparameterusedinEq.S9,
with ğ‘§, the probabilistic model state. The weighting coefficients ğœ‚ and ğ›¼ are introduced to scale
the contributions of the curiosity and motor entropy terms, respectively. The complexity term is
computedasKullbackâ€“Leiblerdivergence(KLD)betweentheestimatedposteriordistributionand
the prior distribution over the latent variables at each time step. Both distributions are modeled as
Gaussiandistributionwithtime-dependentmeansandstandarddeviations.Theestimatedposterior
is conditioned on the current sensory observation and the previous hidden state, while the prior is
conditionedonlyontheprevioushiddenstate.TheresultingKLDthusreflectstheinformationgain
fromthatsensoryobservation,whichisdrivenbythemotorcommandexecutedattheprevioustime
step. Therefore, exploration of more novel situations (i.e., curiosity-driven exploration) tends to
resultwithhigherinformationgainthroughlargercomplexity.Themotorentropyinthethirdterm
of Eq. 1 reflects the expected uncertainty of the policy, and is computed as the negative expected
log-probabilityofgeneratingamotorcommand ğ‘ ğ‘¡ conditionedonthehiddenstate â„ ğ‘¡âˆ’1 .
By adopting an analogous approach to the Kawahara model, the policy for generating a motor
command ğ‘ ğ‘¡ is trained to minimize the expected free energy ğº ğ‘¡ (Eq. 1) through RL using the the
SoftActorCritic(SAC)algorithm(40).Accordingly,theğ‘„ ğ‘¡ valueisupdatedas:
ğ‘„ ğ‘¡ = ğ‘Ÿ ğ‘¡ +ğœ‚ğ· ğ¾ğ¿ [ğ‘(ğ‘§ ğ‘¡ |ğ‘œ ğ‘¡ ,â„ ğ‘¡âˆ’1 )||ğ‘(ğ‘§ ğ‘¡ |â„ ğ‘¡âˆ’1 )] +ğ›¼H(ğœ‹ ğœ™ (ğ‘ ğ‘¡+1 |â„ ğ‘¡ ))
+ğ›¾(1âˆ’ğ‘‘ğ‘œğ‘›ğ‘’ ğ‘¡ )E ğ‘œ ğ‘¡+1 âˆ¼ğ·,ğ‘ ğ‘¡+1 âˆ¼ğœ‹ ğœ™ [ğ‘„ ğœƒÂ¯ (ğ‘œ ğ‘¡+1 ,ğ‘ ğ‘¡+1 )]. (2)
21
Thefirsttermğ‘Ÿ ğ‘¡ representstheextrinsicreward.Thesecondtermğ· ğ¾ğ¿ [ğ‘(ğ‘§ ğ‘¡ |ğ‘œ ğ‘¡ ,â„ ğ‘¡âˆ’1 )||ğ‘(ğ‘§ ğ‘¡ |â„ ğ‘¡âˆ’1 )] is
theintrinsicrewardforcuriosity,scaledbyapositivecoefficientğœ‚.ThethirdtermH(ğœ‹
ğœ™
(ğ‘
ğ‘¡+1
|â„
ğ‘¡
))
is the intrinsic reward for motor entropy, scaled by a positive coefficient ğ›¼. The fourth term is the
bootstrappedestimateofthenextstepâ€™svalue,ğ‘„ (cid:100)ğ‘¡+1 ,whichisweightedbyadiscountrateparameter
ğ›¾ âˆˆ [0,1]. The variable ğ‘‘ğ‘œğ‘›ğ‘’ ğ‘¡ is zero for all steps except the episodeâ€™s final step, where it is set
to one. This restrains the definition of ğ‘„ ğ‘¡ to steps within the episode. The critic ğ‘„ ğœƒ (ğ‘œ ğ‘¡+1 ,ğ‘ ğ‘¡+1 )
is trained to generate ğ‘„ (cid:99)ğ‘¡, approximation of ğ‘„ ğ‘¡. The target critic ğ‘„ ğœƒÂ¯ (ğ‘œ ğ‘¡+1 ,ğ‘ ğ‘¡+1 ) is maintained for
stabilityinthecriticâ€™straining.Initiallyidenticaltothecritic,thetargetcriticisupdatedviaPolyak
averaging such that ğœƒÂ¯ â† ğœğœƒ + (1 âˆ’ ğœ)ğœƒÂ¯ with ğœ âˆˆ [0,1]. The actor ğœ‹ ğœ™ (ğ‘œ ğ‘¡ ) is trained to generate
motorcommandsğ‘ ğ‘¡ whichmaximizethecriticâ€™spredictionsofvalue.Tomitigatepositivebias,itis
commontotrainmultipleseparatecritics(eachwithitsowntargetcritic)(40).Theactoristrained
usingtheminimumpredictedvalueacrosscritics.Ourmodelemploystwoseparatecritics.
Theforwardmodelistraineddynamicallyoverthecourseofexploratorylearningbyoptimizing
the modelparameters ğœ“ tominimize theevidence free energy ğ¹ ğœ“ (Eq. S3)after each trialepisode.
The exact implementation of this process is described in the supplementary material subsection,
DetailsoftheModelArchitecture.
RobotActions
The robot and the objects were simulated in PyBullet, the python physics simulator. Each wheelâ€™s
velocitywasboundedwithintherangeof [âˆ’10,10] meterspersecond.Forscale,therobotâ€™sbody
is a cube measuring 2 meters along each dimension (length, width, and height). The robotâ€™s arm
featurestwojoints:yaw,whichrotatesleftandrightwithinarangeof [âˆ’30 Â°,30 Â°],andpitch,which
rotatesforwardandupwardwithinarangeof[0 Â°,90 Â°].Forsmoothmovement,therobotâ€™swheeland
armvelocitieswereimplementedwithlinearinterpolationfromthecurrenttothetargetvelocities.
Wedefinedsuccesscriteriaforeachactioncategory,whichdeterminedwhetherornottherobot
earned an extrinsic reward. The distance between the robot and an object was measured from the
objectâ€™scentertothecenteroftherobotâ€™sbody.Therobotwasconsideredtobeâ€œfacingtheobjectâ€
whentheangulardeviationbetweentherobotâ€™sforwarddirectionthelineconnectionittotheobject
waslessthan15degrees.
Watch:Therobotfacestheobjectbetween6and10metersofdistance.Thismustbemaintained
22
for6stepsinarow.
Be Near: The robot faces the object with distance of less than 6 meters, without touching the
object.Thismustbemaintainedfor5stepsinarow.
Touch the Top: The robotâ€™s hand contacts with the object while the center of the hand is at
least3.75metersabovethefloor.Thismustbemaintainedfor3stepsinarow.
Push Forward: The robot pushes the object farther than .1 meters, with respect to the robotâ€™s
facingdirection.Thismustbemaintainedfor3stepsinarow.
Push Left: The robot pushes the object to the robotâ€™s left farther than .2 meters, while the
robotâ€™s wheels have velocities below 5 meters per second (requiring use of the arm). This must be
maintainedfor3stepsinarow.
PushRight:SameasPushLeft,butintheoppositedirection.
There are some constraints in rewarding for actions which are described in the â€œConstraints in
PerformingActionsâ€subsectionintheSupplementaryMaterials.
Developmental learning with Exception handling rules
To examine characteristics in developmental learning with exception handling rules, we tested an
alternative training setup with a pair of swapped goal-action mappings. Robots were trained such
thatthecommandâ€œwatchmagentapillarâ€wasonlyrewardediftherobotperformedâ€œbeneargreen
poleâ€instead,andviceversa.Theseexceptionsconflictedwiththeregularcompositionalmappings.
Amatchedcontrolgroupofidenticalrobotswastrainedwithoutsuchexceptions.Thesuccess-rates
oftheserobotsisdepictedinFig.7.
23
References and Notes
1. M. Tomasello, Constructing a Language: A Usage-Based Theory of Language Acquisition
(HarvardUniversityPress)(2005).
2. L. R. Gleitman, The structural sources of verb meanings. Language Acquisition 1 (1), 3â€“55
(1990).
3. P.Bloom,HowChildrenLearntheMeaningsofWords(MITPress)(2000).
4. L. B. Smith, E. Thelen, Development of word learning: An embodied perspective. Develop-
mentalReview25(3),205â€“244(2005).
5. N.Chomsky,RulesandRepresentations(ColumbiaUniversityPress)(1980).
6. M. Asada, et al., Cognitive Developmental Robotics as a New Paradigm for the Design of
HumanoidRobots.RoboticsandAutonomousSystems37(2-3),185â€“193(2001),doi:10.1016/
S0921-8890(01)00115-4.
7. Y.Kuniyoshi,S.Sangawa,Earlymotordevelopmentfrompartiallyorderedneural-bodydynam-
ics: experiments with a cortico-spinal-musculo-skeletal model. Biological cybernetics 95 (6),
589â€“605(2006).
8. G. Sandini, G. Metta, J. Konczak, Developmental robotics: Insights from developmental psy-
chologyonroboticlearning.ProgressinBrainResearch164,327â€“346(2007).
9. T. J. Prescott, P. F. Dominey, Synthesizing the temporal self: robotic models of episodic and
autobiographicalmemory.PhilosophicalTransactionsB379(1913),20230415(2024).
10. A.Cangelosi,T.Riga,Simulationoflanguageandactionlearninginamulti-agentenvironment.
ProceedingsoftheIEEE 92(3),396â€“401(2004).
11. Y.Sugita,J.Tani,Cross-situationallearningofwordsandsentences:Adevelopmentalrobotics
experiment.ProceedingsoftheIEEE 92(3),428â€“442(2005).
24
12. A. Taniguchi, T. Taniguchi, T. Inamura, Spatial concept acquisition for a mobile robot that
integrates self-localization and unsupervised word discovery from spoken sentences. IEEE
TransactionsonCognitiveandDevelopmentalSystems8(4),285â€“297(2016).
13. R. Vijayaraghavan, D. Roy, A. Cangelosi, Grounding language learning in embodied interac-
tion:Areviewofapproachesandchallenges.FrontiersinRoboticsandAI 8,625891(2021).
14. K. Friston, J. Mattout, J. Kilner, Action understanding and active inference. Biological cyber-
netics104(1),137â€“160(2011).
15. G.Pezzulo,F.Rigoli,K.J.Friston,Hierarchicalactiveinference:atheoryofmotivatedcontrol.
Trendsincognitivesciences22(4),294â€“306(2018).
16. T. Parr, K. J. Friston, Generalised free energy and active inference. Biological Cybernetics
(2019).
17. D. Kawahara, S. Ozeki, I. Mizuuchi, A Curiosity Algorithm for Robots Based on the Free
Energy Principle, in 2022 IEEE/SICE International Symposium on System Integration (SII)
(Narvik,Norway)(2022).
18. J.Tinker,K.Doya,J.Tani,ActiveInferenceandReinforcementLearningforCuriosity-Driven
DevelopmentalRobotics.AdaptiveBehavior (2024).
19. P.-Y. Oudeyer, F. Kaplan, V. V. Hafner, Intrinsic motivation systems for autonomous mental
development.IEEEtransactionsonevolutionarycomputation11(2),265â€“286(2007).
20. J. Schmidhuber, A possibility for implementing curiosity and boredom in model-building
neural controllers. Proceedings of the International Conference on Simulation of Adaptive
Behavior:FromAnimalstoAnimatspp.222â€“227(1991).
21. A. Karmiloff-Smith, Beyond Modularity: A Developmental Perspective on Cognitive Science
(MITPress,Cambridge,MA)(1992).
22. D. E. Rumelhart, J. L. McClelland, On Learning the Past Tenses of English Verbs, in Par-
allel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 2, J. L.
McClelland,D.E.Rumelhart,Eds.(MITPress),pp.216â€“271(1986).
25
23. K.Plunkett,V.Marchman,U-shapedlearningandfrequencyeffectsinamultilayerperceptron:
Implicationsforchildlanguageacquisition.Cognition38(1),43â€“102(1991).
24. V.Marchman,K.Plunkett,FromU-shapedlearningtosystematicity:Aconnectionistaccount
ofEnglishpasttenseacquisition.Cognition48(3),279â€“304(1993).
25. J. L. Elman, et al., Rethinking Innateness: A Connectionist Perspective on Development (MIT
Press)(1996).
26. D. Mareschal, T. R. Shultz, Computational Developmental Psychology. Trends in Cognitive
Sciences5(5),178â€“185(2001).
27. K. Friston, et al., Active inference and learning. Neuroscience & Biobehavioral Reviews 68,
862â€“879(2016),doi:10.1016/j.neubiorev.2016.06.022.
28. M. Tomasello, Constructing a Language: A Usage-Based Theory of Language Acquisition
(HarvardUniversityPress,Cambridge,MA)(2003).
29. L. Gerken, S. Knight, Infants generalize from just (the right) four words. Cognition 143,
187â€“192(2015).
30. L. Gerken, C. Dawson, R. Chatila, J. Tenenbaum, Surprise! Infants consider possible bases of
generalizationforasingleinputexample.DevelopmentalScience(2014).
31. K.E.Adolph,J.M.Franchak,Learningtomove,movingtolearn:Aquartercenturyofprogress
instudyinginfantmotordevelopment.ChildDevelopmentPerspectives9(3),214â€“219(2015).
32. A. Cangelosi, M. Schlesinger, Developmental Robotics: From Babies to Robots (MIT Press)
(2015).
33. L.S.Vygotsky,MindinSociety:TheDevelopmentofHigherPsychologicalProcesses(Harvard
UniversityPress)(1978).
34. J.Bruner,Childâ€™sTalk:LearningtoUseLanguage(OxfordUniversityPress)(1983).
35. L. Steels, A self-organizing spatial vocabulary, in Artificial Life IV (MIT Press) (1995), pp.
179â€“184.
26
36. S.Li,R.Miikkulainen,Evolvingartificiallanguageusingevolutionaryreinforcementlearning,
in Proceedings of the 8th International Conference on the Simulation of Adaptive Behavior
(MITPress)(2006),pp.182â€“191.
37. T. Taniguchi, T. Nagai, T. Nakamura, Symbol emergence in cognitive developmental systems:
asurvey.IEEETransactionsonCognitiveandDevelopmentalSystems11(4),494â€“516(2019).
38. T. J. Tinker, K. Doya, J. Tani, Intrinsic Rewards for Exploration Without Harm From Obser-
vational Noise: A Simulation Study Based on the Free Energy Principle. Neural Computation
36 (9), 1854â€“1885 (2024), doi:10.1162/neco a 01690, https://doi.org/10.1162/neco_
a_01690.
39. J. Chung, et al., A recurrent latent variable model for sequential data. Advances in neural
informationprocessingsystems28(2015).
40. T. Haarnoja, A. Zhou, P. Abbeel, S. Levine, Soft Actor-Critic: Off-Policy Maximum Entropy
DeepReinforcementLearningwithaStochasticActor,inProceedingsofthe35thInternational
Conference on Machine Learning, J. Dy, A. Krause, Eds. (PMLR), vol. 80 of Proceedings of
Machine Learning Research (2018), pp. 1861â€“1870, https://proceedings.mlr.press/
v80/haarnoja18b.html.
41. K. J. Friston, A theory of cortical responses. Philosophical transactions of the Royal Society
B:Biologicalsciences360(1456),815â€“836(2005).
42. R. P. Rao, D. H. Ballard, Predictive coding in the visual cortex: a functional interpretation of
someextra-classicalreceptive-fieldeffects.Natureneuroscience2(1),79â€“87(1999).
43. J.Hohwy,Thepredictivemind (OUPOxford)(2013).
44. A. Clark, Surfing uncertainty: Prediction, action, and the embodied mind (Oxford University
Press)(2015).
45. C.Blundell,J.Cornebise,K.Kavukcuoglu,D.Wierstra,Weightuncertaintyinneuralnetwork,
inInternationalConferenceonMachineLearning(PMLR)(2015),pp.1613â€“1622.
27
Acknowledgments
Thank you to the Okinawa Institute of Science and Technology (OIST) and colleagues in OISTâ€™s
Cognitive Neurorobotics Unit and Neural Computation Unit for supporting this work. We also
thankTaroToyoizumiforvaluablediscussionsonacquiringexceptionhandlingrules.
Funding: T.J.T. was funded by OIST graduate school. J.T. was partially funded by the Japan
Society for the Promotion of Science (JSPS) KAKENHI, Transformative Research Area (A):
unifiedtheoryofpredictionandaction[24H02175],
Author contributions T.J.T. and J.T. designed the model and simulation. T.J.T., K.D., and J.T.
designedtheexperiments.T.J.T.performedallexperimentsanddataanalysis.T.J.T.wrotethepaper.
K.D.andJ.T.editedthepaper.J.T.supervisedthestudy.
Competinginterests: Authorsdeclarethattheyhavenocompetinginterests.
Data and materials availability: All files related to this project are available from a repository
attheOkinawaInstituteofScienceandTechnology.
The authors are grateful for the help and support provided by colleagues in OISTâ€™s Cognitive
NeuroroboticsResearchUnitandNeuralComputationUnit.
28
Supplementary Materials for
Curiosity-Driven Development of Action and Language in
Robots Through Self-Exploration
TheodoreJeromeTinker1,KenjiDoya1,JunTani1
1 OkinawaInstituteofScienceandTechnology,Okinawa,Japan.
*Towhomcorrespondenceshouldbeaddressed;E-mail:jun.tani@oist.jp.
ThisPDFfileincludes:
SupplementaryText
Figs.S1toS3
TablesS1toS8
OtherSupplementaryMaterialsforthismanuscript:
Videoathttps://www.youtube.com/watch?v=Gd1FXeb0ee0
S1
Variable Definition Variable Definition
ğ‘œ ğ‘¡ Observationattimeğ‘¡ ğ‘“ Forwardmodel
ğ‘œ ğ‘¡,ğ‘– ğ‘–ğ‘¡â„ partofobservation ğ‘œ ğ‘¡ ğœ“ Forwardmodelparameters
ğ‘œ ğ‘¡,ğ‘£ Ouragentâ€™s ğ‘œ ğ‘¡,0 ,vision ğ›¾ Discountforfuturerewards
ğ‘œ ğ‘¡,ğ‘¡ğ‘ ğ‘œ ğ‘¡,1 ,touch ğ›¼ Importanceofmotorentropy
ğ‘œ ğ‘¡,ğ‘ ğ‘œ ğ‘¡,2 ,proprioception ğœ‚ Importanceofcuriosity
ğ‘œ ğ‘¡,ğ‘ğ‘¤ ğ‘œ ğ‘¡,3 ,commandvoice ğœ‚ ğ‘– ğœ‚ forğ‘–ğ‘¡â„ partofobservation
ğ‘œ ğ‘¡,ğ‘“ğ‘¤ ğ‘œ ğ‘¡,4 ,feedbackvoice ğ‘(ğ‘§ ğ‘¡ ), ğ‘(ğ‘§ ğ‘¡ ) Prior,estimatedposterior
ğ‘ ğ‘¡ MotorCommand ğœ‡, ğœ Mean,standarddeviation
ğ‘Ÿ ğ‘¡ Extrinsicreward â„ ğ‘¡ RNNhiddenstate
ğ‘‘ğ‘œğ‘›ğ‘’ ğ‘¡ Finalstepofepisode ğ‘§ ğ‘¡ Samplefromposterior
ğ‘šğ‘ğ‘ ğ‘˜ ğ‘¡ Stepsinsideepisode ğ‘’ğ‘›ğ‘ ğ‘– Encoderfor ğ‘œ ğ‘¡,ğ‘–
ğ‘… Recurrentreplaybuffer ğœ“ ğ‘– ğ‘’ğ‘›ğ‘ ğ‘“ parametersfor ğ‘’ğ‘›ğ‘ ğ‘–
ğœ‹ Actor ğ‘‘ğ‘’ğ‘ ğ‘– Decoderfor ğ‘œ ğ‘¡,ğ‘–
ğœ™ Actorâ€™sparameters ğœ“ ğ‘– ğ‘‘ğ‘’ğ‘ ğ‘“ parametersfor ğ‘‘ğ‘’ğ‘ ğ‘–
ğ‘„ Critic ğ‘€ğ¿ğ‘ƒ ğ‘– ğ‘ğ‘Ÿğ‘–ğ‘œğ‘Ÿ Multilayerforpriorfor ğ‘œ ğ‘¡,ğ‘–
ğœƒ Criticâ€™sparameter ğ‘€ğ¿ğ‘ƒğ‘ğ‘œğ‘ ğ‘¡ Multilayerforestimated
ğ‘–
ğœƒÂ¯ Targetcriticâ€™sparameter posteriorfor ğ‘œ ğ‘¡,ğ‘–
ğœ Criticâ€™ssoftupdatecoefficient
TableS1.Definitionsofvariables.
Supplementary Text
Forfuturereference,tableS1includesdefinitionsforallrelevantvariables.
Free energy principle, Active Inference, and Kawahara Model
We begin by describing predictive coding and active inference (AIF), which are grounded in the
free energy principle (FEP) (41). The FEP posits that biological and artificial agents maintain
their existence by minimizing variational free energy, which is an upper bound on sensory sur-
S2
prise. In perception, this process is often instantiated as predictive coding (41,42,43,44), wherein
internal models reconstruct sensory inputs by updating beliefs or latent variables by minimizing
the reconstruction errors. More formally, this is minimizing evidence free energy defined for past
observations. In motor command generation, the FEP framework extends to AIF (14,16), where
agents minimize the future prediction error (quantified as expected free energy) by optimizing the
latent variables and motor commands in the future. These two processes are tightly coupled and
mustbeconsideredjointlyinembodiedcognitionsystems.
We next introduce the work of Kawahara et al. (17), who proposed a novel reinforcement
learning(RL)schemethatintegrates(AIF).
In the Bayesian framework, the true posterior probability distribution ğ‘(ğ‘§ ğ‘¡ |ğ‘œ ğ‘¡ ) over latent
variables ğ‘§ ğ‘¡,conditionedonsensoryobservations ğ‘œ ğ‘¡,isgivenbyBayesâ€™rule:
ğ‘(ğ‘œ |ğ‘§ )ğ‘(ğ‘§ )
ğ‘¡ ğ‘¡ ğ‘¡
ğ‘(ğ‘§ ğ‘¡ |ğ‘œ ğ‘¡ ) = âˆ«
ğ‘(ğ‘œ ,ğ‘§ )ğ‘‘ğ‘§
ğ‘¡ ğ‘¡
Here, ğ‘(ğ‘§ ğ‘¡ ) denotes the prior. The denominator, called the evidence, is usually intractable; to
overcome this, variational Bayes introduces an estimate of the posterior ğ‘(ğ‘§ ğ‘¡ ). This is optimized
tominimizetheKullback-Leiblerdivergence(KLD)betweentheestimatedposterior ğ‘(ğ‘§ ğ‘¡ ) andthe
trueposterior ğ‘(ğ‘§ ğ‘¡ |ğ‘œ ğ‘¡ ).
âˆ« ğ‘(ğ‘§ )
ğ‘¡
ğ· ğ¾ğ¿ [ğ‘(ğ‘§ ğ‘¡ )||ğ‘(ğ‘§ ğ‘¡ |ğ‘œ ğ‘¡ )] = ğ‘(ğ‘§ ğ‘¡ )log ğ‘‘ğ‘§ ğ‘¡
ğ‘(ğ‘§ |ğ‘œ )
ğ‘¡ ğ‘¡
âˆ« ğ‘(ğ‘§ )ğ‘(ğ‘œ )
ğ‘¡ ğ‘¡
= ğ‘(ğ‘§ ğ‘¡ )log ğ‘‘ğ‘§ ğ‘¡
ğ‘(ğ‘§ ,ğ‘œ )
ğ‘¡ ğ‘¡
âˆ« ğ‘(ğ‘§ )ğ‘(ğ‘œ )
ğ‘¡ ğ‘¡
= ğ‘(ğ‘§ ğ‘¡ )log ğ‘‘ğ‘§ ğ‘¡ (S1)
ğ‘(ğ‘§ )ğ‘(ğ‘œ |ğ‘§ )
ğ‘¡ ğ‘¡ ğ‘¡
= ğ¹ +logğ‘(ğ‘œ ğ‘¡ ) (S2)
Theterm ğ¹ hereistheevidencefreeenergy,equalto
ğ¹ ğ‘¡ = ğ· (cid:32)(cid:32) ğ¾ (cid:32)(cid:32)(cid:32)(cid:32) ğ¿ (cid:32)(cid:32)(cid:32)(cid:32) [ (cid:32)(cid:32)(cid:32) ğ‘ (cid:32)(cid:32)(cid:32)(cid:32) ( (cid:32) ğ‘§ ğ‘¡ ) (cid:32) | (cid:32)(cid:32) | (cid:32)(cid:32)(cid:32) ğ‘ (cid:32)(cid:32)(cid:32)(cid:32) ( (cid:32)(cid:32) ğ‘§ (cid:32)(cid:32)(cid:32) ğ‘¡ (cid:32)(cid:32) ) (cid:32) ]âˆ’E (cid:32) ğ‘ (cid:32)(cid:32)(cid:32) ( (cid:32)(cid:32) ğ‘§ (cid:32)(cid:32)ğ‘¡(cid:32)(cid:32) ) (cid:32)(cid:32) [ (cid:32)(cid:32)(cid:32) l (cid:32)(cid:32) o (cid:32)(cid:32)(cid:32) g (cid:32) ğ‘( (cid:32)(cid:32) ğ‘œ (cid:32)(cid:32)(cid:32)(cid:32) ğ‘¡ (cid:32)(cid:32) + (cid:32)(cid:32)(cid:32) 1 (cid:32)(cid:32)(cid:32) | (cid:32)(cid:32) ğ‘§ (cid:32)(cid:32)(cid:32) ğ‘¡ (cid:32)(cid:32) )]. (S3)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Complexity Accuracy
Since ğ‘(ğ‘œ ğ‘¡ ) isconstantforagivensensoryobservation,minimizingKLDisequivalenttominimiz-
S3
ing ğ¹ ğ‘¡.Therefore,theoptimalposteriorestimateis:
ğ‘âˆ—(ğ‘§ ğ‘¡ ) = argmin ğ¹ ğ‘¡ (S4)
ğ‘(ğ‘§ ğ‘¡)
Inactiveinference,theagentminimizesexpectedfreeenergyğº
ğœ
atafuturetimestepğœ â‰¥ ğ‘¡+1.
This is the expected value of the evidence free energy under the predictive distribution of future
outcomes(17).
ğº ğœ = E ğ‘(ğ‘œ ğœ|ğ‘§ ğœ) [ğ¹]
âˆ« ğ‘(ğ‘§ )
ğœ
= E ğ‘(ğ‘œ ğœ|ğ‘§ ğœ) [ ğ‘(ğ‘§ ğœ )log ğ‘(ğ‘œ ,ğ‘§ ) ğ‘‘ğ‘§]
ğœ ğœ
ğ‘(ğ‘§ )
ğœ
= E ğ‘(ğ‘œ ğœ|ğ‘§ ğœ) [E ğ‘(ğ‘§ ğœ) [log ğ‘(ğ‘§ |ğ‘œ ) âˆ’logğ‘(ğ‘œ ğœ )]]. (S5)
ğœ ğœ
Recallingthat ğ‘(ğ‘§ ğœ |ğ‘œ ğœ )ğ‘(ğ‘œ ğœ ) = ğ‘(ğ‘œ ğœ ,ğ‘§ ğœ ),weapproximate:
ğ‘(ğ‘§ )
ğœ
ğº
ğœ
â‰ˆ E
ğ‘(ğ‘œ ğœ ,ğ‘§ ğœ)
[log
ğ‘(ğ‘§ |ğ‘œ )
âˆ’logğ‘(ğ‘œ
ğœ
)]
ğœ ğœ
ğ‘(ğ‘§ |ğ‘œ )
ğœ ğœ
= âˆ’E ğ‘(ğ‘œ ğœ ,ğ‘§ ğœ) [log ğ‘(ğ‘§ ) ] âˆ’E ğ‘(ğ‘œ ğœ) [logğ‘(ğ‘œ ğœ )]
ğœ
BayesianSurprise
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:122) (cid:125)(cid:124) (cid:123)
= âˆ’E (cid:32) ğ‘ (cid:32)(cid:32)(cid:32) ( (cid:32)(cid:32) ğ‘œ (cid:32)(cid:32)(cid:32)ğœ(cid:32)(cid:32) ) (cid:32)(cid:32)(cid:32) [ (cid:32)(cid:32)(cid:32) ğ· (cid:32)(cid:32)(cid:32)(cid:32)(cid:32) ğ¾ (cid:32)(cid:32)(cid:32)(cid:32) ğ¿ (cid:32)(cid:32)(cid:32)(cid:32) [ (cid:32)(cid:32)(cid:32) ğ‘ (cid:32) (ğ‘§ ğœ (cid:32)(cid:32)(cid:32) | (cid:32)(cid:32) ğ‘œ (cid:32)(cid:32)(cid:32)(cid:32) ğœ (cid:32)(cid:32) ) (cid:32)(cid:32)(cid:32) | (cid:32)(cid:32) | (cid:32)(cid:32) ğ‘ (cid:32)(cid:32)(cid:32)(cid:32) ( (cid:32)(cid:32)(cid:32) ğ‘§ (cid:32)(cid:32)(cid:32) ğœ (cid:32)(cid:32)(cid:32) ) (cid:32)(cid:32) ] (cid:32) ]âˆ’E (cid:32) ğ‘ (cid:32)(cid:32)(cid:32) ( (cid:32)(cid:32) ğ‘œ (cid:32)(cid:32)(cid:32)ğœ(cid:32)(cid:32) ) (cid:32)(cid:32)(cid:32) [ (cid:32)(cid:32) log (cid:32)(cid:32) ğ‘ (cid:32)(cid:32)(cid:32)(cid:32) ( (cid:32)(cid:32)(cid:32) ğ‘œ (cid:32)(cid:32)(cid:32)(cid:32) ğœ (cid:32)(cid:32)(cid:32) )]. (S6)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
EpistemicValueorMutualInformation ExtrinsicValue
The first term, ğ¼(ğ‘§ ğœ ,ğ‘œ ğœ ) = E ğ‘(ğ‘œ ğœ) [ğ· ğ¾ğ¿ [ğ‘(ğ‘§ ğœ |ğ‘œ ğœ ]||ğ‘(ğ‘§ ğœ )]], is the mutual information (or Bayesian
surprise).Thisdepictsexpectedinformationgainbasedonnewsensoryobservationğ‘œ ğœ,andcanbe
expressedas:
ğ¼(ğ‘§ ğœ ,ğ‘œ ğœ ) = ğ»(ğ‘§ ğœ ) âˆ’ ğ»(ğ‘§ ğœ |ğ‘œ ğœ ) .
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:124)(cid:123)(cid:122)(cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ShannonEntropy ConditionalEntropy
The second term, ğ‘(ğ‘œ ğœ ), represents log-likelihood of the preferred sensory observation. This is
specified as the extrinsic reward designed by the experimenters. For the intrinsic value to reflect
mutual information or information gain, and the extrinsic value to reflect expected free energy, is
the same as the way shown by Fristonâ€™s group in the study of active inference (16,27). Separating
ğ‘œ ğ‘¡ into ğ‘œ ğ‘¡ and ğ‘ ğ‘¡,werewritetheexpectedfreeenergyas:
S4
ğ‘(ğ‘§ |ğ‘œ ,ğ‘ )
ğœ ğœ ğœ
ğº ğœ = âˆ’E ğ‘(ğ‘œ ğœ ,ğ‘ ğœ ,ğ‘§ ğœ) [log ğ‘(ğ‘§ ) ] âˆ’E ğ‘(ğ‘œ ğœ ,ğ‘ ğœ) [logğ‘(ğ‘œ ğœ ,ğ‘ ğœ )]
ğœ
ğ‘(ğ‘§ ,ğ‘ |ğ‘œ )
ğœ ğœ ğœ
= âˆ’E ğ‘(ğ‘œ ğœ ,ğ‘ ğœ ,ğ‘§ ğœ) [log ğ‘(ğ‘§ )ğ‘(ğ‘ |ğ‘œ ) ] âˆ’E ğ‘(ğ‘œ ğœ ,ğ‘ ğœ) [logğ‘(ğ‘œ ğœ ,ğ‘ ğœ )]
ğœ ğœ ğœ
ğ‘(ğ‘§ |ğ‘œ )ğ‘(ğ‘ |ğ‘œ ,ğ‘§ )
ğœ ğœ ğœ ğœ ğœ
â‰ˆ âˆ’E
ğ‘(ğ‘œ ğœ ,ğ‘ ğœ ,ğ‘§ ğœ)
[log
ğ‘(ğ‘§ )ğ‘(ğ‘ |ğ‘œ )
] âˆ’E
ğ‘(ğ‘œ ğœ ,ğ‘ ğœ)
[logğ‘(ğ‘œ
ğœ
,ğ‘
ğœ
)]
ğœ ğœ ğœ
= âˆ’E ğ‘(ğ‘ ğœ|ğ‘œ ğœ ,ğ‘§ ğœ)ğ‘(ğ‘œ ğœ) [ğ· ğ¾ğ¿ [ğ‘(ğ‘§ ğœ |ğ‘œ ğœ )||ğ‘(ğ‘§ ğœ )]]
âˆ’E [ğ· [ğ‘(ğ‘ |ğ‘œ ,ğ‘§ )||ğ‘(ğ‘ |ğ‘œ )]]
ğ‘(ğ‘œ ğœ ,ğ‘§ ğœ) ğ¾ğ¿ ğœ ğœ ğœ ğœ ğœ
âˆ’E ğ‘(ğ‘œ ğœ ,ğ‘ ğœ) [logğ‘(ğ‘œ ğœ ,ğ‘ ğœ )]. (S7)
Kawahara et al. developed a forward model ğ‘“ ğ‘¤ (ğ‘œ ğœ ,ğ‘ ğœ ) â†’ ğ‘œ (cid:98)ğœ+1 which learns to predict the
future sensory observation ğ‘œ ğœ+1 based ğ‘œ ğœ and ğ‘ ğœ using a Bayesian Neural Network (BNN) (45).
In this type of model, the network parameters ğ‘¤ ğœ are treated as random variables defined with
gaussian distribution. These parameters serve as latent causes of observed sensory transitions and
can be interpreted as random latent variables for the generative model. Therefore, ğ‘¤ ğœ corresponds
to ğ‘§ ğœ.
Lettheapproximateposteriorbedefinedas ğ‘ ğœ“ = N(ğ‘¤ ğœ |ğœ‡,ğœ),withparametersğœ“ = {ğœ‡,ğœ}.In
this setting, the actor ğœ‹ ğœ™ of a SAC can be trained to approximate ğœ‹ ğœ™ (ğ‘ ğœ |ğ‘œ ğœ ) â‰ˆ ğ‘(ğ‘ ğœ |ğ‘œ ğœ ,ğ‘¤ ğœ ). This
allowsrewritingtheexpectedfreeenergyas:
ğº(ğ‘œ ğœ ,ğ‘ ğœ ) = âˆ’E ğ‘(ğ‘ ğœ|ğ‘œ ğœ ,ğ‘¤ ğœ)ğ‘(ğ‘œ ğœ) [ğ· ğ¾ğ¿ [ğ‘(ğ‘¤ ğœ |ğ‘œ ğœ )||ğ‘(ğ‘¤ ğœ )]]
âˆ’E [ğ· [ğœ‹ (ğ‘ |ğ‘œ )||ğ‘(ğ‘ |ğ‘œ )]]
ğ‘(ğ‘œ ğœ ,ğ‘ ğœ) ğ¾ğ¿ ğœ™ ğœ ğœ ğœ ğœ
âˆ’E ğ‘(ğ‘œ ğœ ,ğ‘ ğœ) [logğ‘(ğ‘œ ğœ ,ğ‘ ğœ )]. (S8)
Letusinterpretthepriorpreferencelogğ‘(ğ‘œ ğœ ,ğ‘ ğœ ) astheextrinsicrewardğ‘Ÿ(ğ‘  ğœ ,ğ‘ ğœ ),where ğ‘  ğœ isthe
trueenvironmentalstate.Bringfocustothecurrenttimestepbysettingğœ = ğ‘¡.Becausetheforward
modeltrainstopredict ğ‘œ ğ‘¡+1 ,wecanfurtherrewritetheexpectedfreeenergyas:
S5
ğº(ğ‘œ ğ‘¡ ,ğ‘ ğ‘¡ ) = âˆ’ğ· ğ¾ğ¿ [ğ‘ ğœ“ (ğ‘¤ ğ‘¡ |ğ‘œ ğ‘¡+1 )||ğ‘ ğœ“ (ğ‘¤ ğ‘¡ )] âˆ’logğ‘(ğ‘œ ğ‘¡ ,ğ‘ ğ‘¡ )
âˆ’ ğ· [ğœ‹ (ğ‘ |ğ‘œ )||ğ‘(ğ‘ |ğ‘œ )]
ğ¾ğ¿ ğœ™ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡
= âˆ’ğ· ğ¾ğ¿ [ğ‘ ğœ“ (ğ‘¤ ğ‘¡ |ğ‘œ ğ‘¡+1 )||ğ‘ ğœ“ (ğ‘¤ ğ‘¡ )] âˆ’logğ‘(ğ‘œ ğ‘¡ ,ğ‘ ğ‘¡ )
âˆ« âˆ«
âˆ’ ğœ‹
ğœ™
(ğ‘
ğ‘¡
|ğ‘œ
ğ‘¡
)logğœ‹
ğœ™
(ğ‘
ğ‘¡
|ğ‘œ
ğ‘¡
)ğ‘‘ğ‘
ğ‘¡
+ ğœ‹
ğœ™
(ğ‘
ğ‘¡
|ğ‘œ
ğ‘¡
)logğ‘(ğ‘
ğ‘¡
|ğ‘œ
ğ‘¡
)ğ‘‘ğ‘
ğ‘¡
= âˆ’ğ· (cid:32)(cid:32) ğ¾ (cid:32)(cid:32)(cid:32)(cid:32) ğ¿ (cid:32)(cid:32)(cid:32)(cid:32) [ (cid:32)(cid:32)(cid:32) ğ‘ (cid:32)(cid:32)(cid:32)(cid:32) ğœ“ (cid:32)(cid:32)(cid:32)(cid:32) ( (cid:32)(cid:32) ğ‘¤ (cid:32)(cid:32)(cid:32)(cid:32)(cid:32) ğ‘¡ (cid:32)(cid:32) |ğ‘œ ğ‘¡+ (cid:32)(cid:32)(cid:32)1(cid:32)(cid:32) ) (cid:32)(cid:32)(cid:32) | (cid:32)(cid:32) | (cid:32)(cid:32) ğ‘ (cid:32)(cid:32)(cid:32)(cid:32) ğœ“ (cid:32)(cid:32)(cid:32)(cid:32) ( (cid:32)(cid:32) ğ‘¤ (cid:32)(cid:32)(cid:32)(cid:32)(cid:32) ğ‘¡ (cid:32)(cid:32) ) (cid:32) ]âˆ’ ğ‘Ÿ( (cid:32)(cid:32)(cid:32) ğ‘  ğ‘¡ ,ğ‘ (cid:32)(cid:32)(cid:32) ğ‘¡ ) âˆ’H (cid:32)(cid:32)(cid:32)(cid:32) ( (cid:32)(cid:32) ğœ‹ (cid:32)(cid:32)(cid:32)(cid:32) ğœ™ (cid:32) (ğ‘ (cid:32) ğ‘¡ (cid:32)(cid:32) | (cid:32)(cid:32) ğ‘œ (cid:32)(cid:32)(cid:32)(cid:32) ğ‘¡ (cid:32)(cid:32) ))âˆ’E (cid:32) ğœ‹ (cid:32)(cid:32)(cid:32)ğœ™(cid:32)(cid:32)(cid:32) ( (cid:32)(cid:32) ğ‘ (cid:32)(cid:32)(cid:32)ğ‘¡(cid:32)(cid:32) | (cid:32) ğ‘œ (cid:32)(cid:32)(cid:32)ğ‘¡(cid:32)(cid:32) ) (cid:32)(cid:32) [ (cid:32)(cid:32) log (cid:32)(cid:32)(cid:32) ğ‘ (cid:32)(cid:32)(cid:32)(cid:32) ( (cid:32)(cid:32) ğ‘ (cid:32)(cid:32)(cid:32)(cid:32) ğ‘¡ âˆ— (cid:32)(cid:32)(cid:32) | (cid:32)(cid:32) ğ‘œ (cid:32)(cid:32)(cid:32)(cid:32) ğ‘¡ (cid:32)(cid:32) )]
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Curiosity ExtrinsicReward Entropy Imitation
(S9)
Because ğ‘¤ ğœ represents the robotâ€™s probabilistic knowledge of their environment, the first term of
Eq. S9 can be said to represent the robotâ€™s gain in knowledge based on information acquired in a
newsensoryobservation.
In summary, the forward model is trained to minimize the evidence free energy ğ¹ (Eq. S3)
by accurately reconstructing sensory observations and minimizing posterior complexity based on
past experiences. Meanwhile, the actor-critic pair is trained to minimize expected free energy
ğº, which includes an inverted complexity term (i.e., curiosity) and motor entropy to encourage
exploration. This leads to emergent tension in an adversarial relationship: the actor is encouraged
tomaximizeinformationgainbyincreasingtheKLdivergencebetweenpriorandposterior,which
the forward model trains to minimize that same term. This establishes a dynamic push-pull effect,
driving self-organized exploration. Please note that the imitation term in Eq. S9 depends on
external demonstrations or expert policies; this term is ignored in our study, which focuses on
self-exploration.
Fromthisformulationofexpectedfreeenergy,theğ‘„-valuecanbeupdatedas:
ğ‘„(ğ‘¡) = ğ‘Ÿ ğ‘¡ +ğœ‚ğ· ğ¾ğ¿ [ğ‘ ğœ“ (ğ‘¤ ğ‘¡ |ğ‘œ ğ‘¡+1 )||ğ‘ ğœ“ (ğ‘¤ ğ‘¡ )]+
ğ›¾(1âˆ’ğ‘‘ğ‘œğ‘›ğ‘’ ğ‘¡ )E ğ‘œ ğ‘¡+1 âˆ¼ğ·,ğ‘ ğ‘¡+1 âˆ¼ğœ‹ ğœ™ [ğ‘„ ğœƒÂ¯ (ğ‘œ ğ‘¡+1 ,ğ‘ ğ‘¡+1 )] +ğ›¼H(ğœ‹ ğœ™ (ğ‘ ğ‘¡+1 |ğ‘œ ğ‘¡+1 )) (S10)
Here, ğœ‚ > 0 and ğ›¼ > 0 are hyperparameter weighting the intrinsic reward based on the curiosity
andthemotorentropy,respectively.
S6
Inourexperiments,eachepisodeendedafter30steps,orterminatedearlieriftheagentsuccess-
fullyexecutedthecommand.Completedepisodesarestoredinarecurrentreplaybuffer,whichcan
hold up to 256 episodes. When the buffer is full, the buffer discards the oldest episodes to accom-
modatenewepisodes.Toensureuniformepisodelength,allepisodeswerepaddedto30stepswith
emptytransitions.Hence,transitionsarestoredwiththeform{ğ‘œ
ğ‘¡
,ğ‘
ğ‘¡
,ğ‘Ÿ
ğ‘¡
,ğ‘œ
ğ‘¡+1
,ğ‘‘ğ‘œğ‘›ğ‘’
ğ‘¡
,ğ‘šğ‘ğ‘ ğ‘˜
ğ‘¡
},where
ğ‘šğ‘ğ‘ ğ‘˜ ğ‘¡ = 1 for real transitions, and ğ‘šğ‘ğ‘ ğ‘˜ ğ‘¡ = 0 for empty transitions added for padding. After each
episode, a batch of 32 episodes was sampled from the buffer and used to train the forward model,
actor,andcritics.Duringtraining,losstermsweremultipliedbyğ‘šğ‘ğ‘ ğ‘˜ ğ‘¡,removingtheinfluenceof
emptytransitions.
Details of the Model Architecture
Thissubsectionexplainsfurtherdetailsaboutthemodelarchitectureemployedinthiscurrentstudy.
As noted earlier, the present architecture extends our previous model (38), which is described in
the â€œFree energy principle, Active Inference, and Kawahara Modelâ€ section of the Supplementary
Materials. The primary extension involves the use of separate random latent variables, encoders,
and decoders for each sensory modality. This design allows the model to process multiple types
of sensation independently, including vision, tactile input, proprioception, command voice, and
feedback voice. In addition, our model uses an encoder for the 4-dimensional motor command,
whichincludesmotorvelocitiesfortwotherobotâ€™swheelsandtwojointanglesinitsarm.Thefull
architectureoftheproposedmodelisshowninFig.S1.
Computationinthisarchitectureproceedsasfollows:
1. The4-dimensionalmotorcommandfromtheprevioustimestepisfedintothemotorcommand
encoder,producinganencodedmotorcommandvector.
2. Thepriordistributionforthecurrenttimestepiscomputedusingtheencodedmotorcommand
vectorandtheprevioushiddenstate.
3. The sensory observation for each modality is fed through its corresponding encoder, com-
putingitsmodality-specificencodedvector.
S7
Fig.S1.Thedetailsoftheproposedmodelarchitecture.
4. Theestimatedposteriordistributionforeachmodalityisestimatedusingitssensoryencoded
vector,encodedmotorcommandvector,andtheprevioushiddenstate.
5. All posterior vectors from the current time step are concatenated across all modalities, then
sampledandcombinedwiththeprevioushiddenstatetocomputethecurrenthiddenstate.
6. Themotorcommandforthecurrenttimestepisgeneratedfromthecurrenthiddenstateusing
S8
theactor(policynetwork).
7. The model predicts the next sensory observation for each modality using the current hidden
stateandthecurrentmotorcommand,passedthroughthecorrespondingsensorydecoders.
8. Theğ‘„ ğ‘¡ valueisupdatedaccordingtoEq.2.
9. Iftheepisodeterminatesatthisstep,theepisodeâ€™sdataissavedinarecurrentreplaybuffer.A
batch of information is sampled from the buffer to train the forward model, actor, and critic.
SeetheSupplementaryMaterialsformoredetails.
Details of the encoders and decoders of each sensory modality (e.g., vision, tactile sensation,
etcetera), as well as the motor command encoder, are described in the â€œImplementation detailsâ€
sectionoftheSupplementaryMaterials.
Vision
The robot visually senses the environment in the direction the robot faces with a 16ğ‘¥16ğ‘¥4 image,
withthefourchannelsbeingred,green,blue,anddistance.SeeFig.S2.
Fig.S2.Theagentâ€™svision,o .Therobotisfacingamagentaconeandagreenpillar.Therobot
ğ’•,ğ’—
alsoseespartofitshand.Theimageontheleftdepictsthered,green,andbluechannels.Theimage
ontherightdepictsthedistance.
In our proposed model, in order to make the estimated posterior for visual sensations, images
are flattened and encoded using a linear neural network with Parametric Rectified Linear Unit
activation (PReLU). To generate a prediction of the next image,
â„ğ‘
and
ğ‘ğ‘’ğ‘›ğ‘
are concatenated and
ğ‘¡ ğ‘¡
decoded with another linear neural network, shaped into a 16ğ‘¥16ğ‘¥4 tensor, and finished with a
convolutionallayer.SeedetailsintableS2.
S9
Layer Type Activation Details
Encoder, ğ‘’ğ‘›ğ‘ ğ‘£
1 Flatten Shape(16,16,4)toshape(1024).
2 Linear PReLU Toshape(128).
Decoder, ğ‘‘ğ‘’ğ‘ ğ‘£
1 Linear BatchNorm2d,PReLU Fromshape(264)toshape(8*8*64).
2 Reshaping Toshape(8,8,64).
Kernelsize3,reflectivepadding1.
3 CNN Tanh
Toshape(8,8,8).
4 PixelShuffle Toshape(16,4,4).
TableS2.Encoderanddecoderofagentâ€™svisualsensations,o .
ğ’•,ğ’—
Touch
The second part of the sensory observation is the tactile sensation of touch. This is represented by
a one value between 0 and 1 for each of the 16 sensors. Each value is equal to the fraction of time
inthepreviousstepduringwhichtherespectivesensorwasincontactwithanobject.SeeFig.S3.
Fig.S3.Theagentâ€™ssensorsfortactilesensationsoftouch,o .Therobothas16sensors,which
ğ’•,ğ’•ğ’‚
are planes on the surface of the robotâ€™s body, arm, and hand. The camera and wheels are marked
justforclarity.
Inourproposedmodel,inordertomaketheestimatedposteriorfortactilesensation,thetensor
is encoded using a linear neural network with PReLU. To generate a prediction of the next tactile
S10
sensation,â„ğ‘ andğ‘ğ‘’ğ‘›ğ‘
areconcatenatedanddecodedwithanotherlinearneuralnetwork.Seedetails
ğ‘¡ ğ‘¡
intableS3.
Layer Type Activation Details
Encoder, ğ‘’ğ‘›ğ‘ ğ‘¡ğ‘
1 Linear BatchNorm2d,PReLU Fromshape(16)toshape(20).
Decoder, ğ‘‘ğ‘’ğ‘ ğ‘¡ğ‘
Fromshape(264)toshape(16).
1 Linear BatchNorm2d,TanH Resultaddedto1anddividedby2
forvaluesbetween0and1.
TableS3.Encoderanddecoderofagentâ€™stactilesensations,o .
ğ’•,ğ’•ğ’‚
Proprioception
Thethirdpartofthesensationistheangleandvelocityofthearmâ€™sjoints.(Thevelocityofthejoint
may not match the robotâ€™s motor commands, because collisions with objects may restrain it.) This
consistsofatensorwithfourvaluesbetween0and1:twojointanglesandtwojointvelocities.Each
value is the normalized proportion of the respective variable between its minimum and maximum
range.
Inourproposedmodel,inordertomaketheestimatedposteriorforsensationofproprioception,
the tensor is encoded using a linear neural network with PReLU. To generate a prediction of the
nextproprioception,
â„ğ‘ andğ‘ğ‘’ğ‘›ğ‘
areconcatenatedanddecodedwithanotherlinearneuralnetwork.
ğ‘¡ ğ‘¡
SeedetailsintableS4.
Voices
The fourth and fifth parts of the sensation are the command voice and the tutor-feedback voices,
which were described briefly in the Results section. Both voices are sequences of one-hot vectors.
Table S5 displays the 18 terms (including silence) and their indexes in the one-hot vectors. For
example,thecommandâ€œWatchtheRedPillarâ€isrepresentedby
S11
Layer Type Activation Details
Encoder, ğ‘’ğ‘›ğ‘ ğ‘ğ‘œ
1 Linear BatchNorm2d,PReLU Fromshape(4)toshape(4).
Decoder, ğ‘‘ğ‘’ğ‘ ğ‘ğ‘œ
Fromshape(264)toshape(4).
1 Linear BatchNorm2d,TanH Resultaddedto1anddividedby2
forvaluesbetween0and1.
TableS4.Encoderanddecoderofagentâ€™ssensationofproprioception,o .
ğ’•,ğ’‘
[0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
[0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0] (S11)
[0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0].
If the robot has not performed any action, then the feedback voice is only one one-hot vector
indicatingsilence:
[1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]. (S12)
The robotâ€™s forward modelâ€™s encoding of these two voices has two parts. The first part of the
encoding is an embedding and recurrent neural network. This part is identical for the command
voice and the feedback voices, ensuring that tokens are interpreted consistently across sources.
NotethatthisRNNisâ€œnestedâ€withintheforwardmodelâ€™sRNN,suchthateachoftherobotâ€™ssteps
includesthreestepsofinterpretingvoices.SeeFig.S4.Inthesecondpartoftheencoding,outputs
from the first part of the encoding are processed with unique linear layers to produce separate
estimatedposteriors.Togenerateapredictionofthenextvoices,
â„ğ‘
and
ğ‘ğ‘’ğ‘›ğ‘
areconcatenatedand
ğ‘¡ ğ‘¡
decoded using two separate recurrent neural networks for the command voice and feedback voice.
SeedetailsintableS6.
S12
EnglishWordIndexes Index Word
Index Word 7 Red
0 (Silence) 8 Green
1 Watch 9 Blue
2 BeNear 10 Cyan
3 TouchtheTop 11 Magenta
4 PushForward 12 Yellow
5 PushLeft 13 Pillar
6 PushRight 14 Pole
15 Dumbbell
16 Cone
17 Hourglass
Table S5. English words and indexes. The English words used and their positions in one-hot
vectors.
Fig.S4.Recurrentstepsharedbycommandvoiceandfeedbackvoice.
S13
Layer Type Activation Details
Encoderpartone, ğ‘’ğ‘›ğ‘ ğ‘¤ (sharedbycommandvoiceandfeedbackvoice)
Fromshape(Sequence-length,18)
1 Embedding PReLU
toshape(Sequence-length,8).
2 Linear PReLU Toshape(Sequence-length,64).
3 GRU PReLU Toshape(64).
4 Linear PReLU Toshape(256).
Decoders, ğ‘‘ğ‘’ğ‘ ğ‘ğ‘¤ and ğ‘‘ğ‘’ğ‘ ğ‘“ğ‘¤
1 Linear BatchNorm2d,PReLU Fromshape(264)toshape(192).
2 Reshaping Toshape(3,64).
3 GRU PReLU Toshape(3,64).
4 Linear Toshape(3,17).
TableS6.Encoderanddecoderofagentâ€™svoicesensation,o ando .
ğ’•,ğ’„ğ’˜ ğ’•,ğ’‡ğ’˜
MotorCommandEncoder
Forusageintheforwardmodel,therobotâ€™smotorcommandsğ‘ ğ‘¡ areencodedintoğ‘ ğ‘¡ ğ‘’ğ‘›ğ‘ withalinear
neuralnetworkwithPReLU.SeedetailsintableS6.
Layer Type Activation Details
Encoder, ğ‘’ğ‘›ğ‘ ğ‘
1 Linear PReLU Fromshape(4)toshape(8).
TableS7.Encodingmotorcommandforforwardmodel.
ConstraintsinPerformingActions
Ineachstep,therobotcanonlyperformoneofthesixactions.Thisisimplementedusingdefinitions
of actions and action prioritization. The actions Watch, Be Near, and Touch the Top cannot be
performedsimultaneouslybecauseofrequirementsregardingdistancefromtheobjectandtouching
the object. The actions Push Left and Push Right cannot be performed simultaneously because of
S14
thedirectionsofmovements.IftherobotsatisfiestherequirementsforTouchtheTop,werejectthe
actionsPushForward,PushLeft,orPushRight.IftherobotisperformingPushForwardandPush
LeftorPushRight,weallowonlytheactionswiththegreatestdistancepushed.
Details of Experiment Design
10robotstrainedineachwaydescribedintheResultssection:nocuriosity,sensory-motorcuriosity,
and all curiosity. The robots trained for 60000 epochs. In each epoch, the robot performed one
episodewhichwassavedinitsrecurrentreplaybuffer.Thentherobottrainedwithabatchof32of
itssavedepisodes.
Experiment1
Experiment 1 tests the effects of curiosity. We trained robots with three levels of curiosity: no
curiosity, sensory-motor curiosity, and all curiosity. In table S8, we share the value of the ğœ‚
hyperparameters for each of the four parts of the sensory observation which may be explored.
These represent the relative importance of each part of the sensory observation in the robotâ€™s
curiosities.
Name ğœ‚ ğ‘£ğ‘–ğ‘ ğ‘–ğ‘œğ‘› ğœ‚ ğ‘¡ğ‘œğ‘¢ğ‘â„ ğœ‚ ğ‘ğ‘Ÿğ‘œğ‘ğ‘Ÿğ‘–ğ‘œğ‘ ğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğœ‚ ğ‘“ğ‘’ğ‘’ğ‘‘ğ‘ğ‘ğ‘ğ‘˜
NoCuriosity 0 0 0 0
Sensory-MotorCuriosity .05 2 .1 0
CompleteCuriosity .05 2 .1 .3
TableS8.Hyperparametersforthreetypesofagents.
We measured the success-rates of these three types of robots in the six types of actions. The
plotsinFig.2showtherollingaverageofsuccess-ratesofthethreetypesrobotsfromthebeginning
of training to the end of training after 60000 epochs, with 99% confidence intervals. Specifically,
the plots show results of the robots regarding the goals with combinations of verb, adjective, and
noun which the robots were not shown in training, testing for the ability to generalize vocabulary
andsyntaxtounlearnedcombinations.
S15
As we predicted in hypothesis ğ‘–, robots with no curiosity performed the worst, with approxi-
mately 25% success-rate; robots with curiosity for sensory-motor observations performed better,
withapproximately75%success-rate;androbotswithcuriosityforsensory-motorobservationsand
thefeedbackvoicearethebest,withapproximately90%success-rate.Aswepredictedinhypothesis
ğ‘–ğ‘–,therobotâ€™sabilitytoperformsimpleractionsdevelopearliest,andtherobotâ€™sabilitytoperform
more complex actions develop later, having required the simpler actions as prerequisites. Merely
watchingtheobjectappearstobethesimplest,developingearliest,whilepushingobjecttheobject
totheleftorrightappeartobethemostcomplex,developinglater.
Experiment2
Experiment 2 tests the relationship between success-rates with learned goals and unlearned goals,
specifically by robots using all curiosity. See Fig. 6. The left column shows success-rate plots
of robots with learned actions, while the right column shows success-rate plots of robots with
unlearned actions. The first row shows results for robots using the complete vocabulary: 6 verbs,
6 adjectives, and 5 nouns. The second and third row show results for robots trained with smaller
vocabularies. In each of the three situations, the robots are trained with one third of the possible
goals,andtestedwiththeothertwothirds.
Aswepredictedinhypothesisğ‘–ğ‘–ğ‘–,therobotâ€™ssuccess-rateswithlearnedactionsinitiatesearlier
than its success-rates with unlearned actions. This suggests pairing sentences of words precedes
generalization with compositionality. As we predicted in hypothesis ğ‘–ğ‘£, larger vocabularies lead
to faster generalization. All three collections of robots had success-rates of approximately 100%
with learned actions. Robots which were trained with 60 of the 180 possible goals with 6 verbs,
6 adjectives, and 5 nouns had success-rates of approximately 90% with unlearned actions. Robot
which were trained with 25 of the 75 possible goals with 5 verbs, 5 adjectives, and 3 nouns had
success-ratesofapproximately50%withunlearnedactions.Androbotswhichweretrainedwith16
ofthe48possiblegoalswith4verbs,4adjectives,and3nounshassuccess-ratesofapproximately
30% with unlearned actions. The ability to generalize quickly is enhanced with the size of the
vocabularyinuse.
MovieS1.Exampleoftraining.Comparesrobotwithcuriositymid-trainingandaftertraining.
S16
Statistical Analysis of U-Shaped Patterns
ToquantifyU-shapedlearninginexceptionhandling,wescoredtheU-shapedstructureofsuccess-
rateâ€™s trajectories identifying non-monotonic developmental patterns consistent with representa-
tional redescription (21). The method combines robust smoothing, normalized scaling, and piece-
wiseisotonicregressiontofitatwo-phasemodelwithacentralvalley.
Consider one robotâ€™s rolling-average success rate over training epochs for goals which are
exceptions.TheU-shapescoreiscomputedasfollows:
1. Burn-inremoval.Thefirst10%oftrainingdataisremovedtoavoidinitializationnoise.
2. Smoothing. The curve is smoothed using a Savitzkyâ€“Golay filter with a window length of
approximately3%oftheseries,reducingspuriouslocalfluctuations.
3. Normalization. The smoothed curve is linearly scaled to the [0,1] range using the 5th and
95thpercentilestoensurerobustnessacrosssuccess-rateranges.
4. Valleylocalization.Theminimumpointğ‘– ğ‘€ islocatedbetween20%and80%ofthesequence
length.
5. Piecewiseisotonicregression.Foreachcandidatesplitpoint ğ‘˜ nearthevalley(withinÂ±25%
of the series), the left segment is fit with a decreasing isotonic regression and the right
segmentwithanincreasingisotonicregression.Acostfunctionisminimized:
Cost(ğ‘˜) = MSE(ğ‘˜) +ğœ† Â· (driftfromvalley)2 Â·MSE ,
base
whereğœ† = 2.0penalizesdriftingtoofarfromtheidentifiedvalley.Indicesof ğ‘˜ definetheleft
peakğ‘–
ğ¿
andrightpeakğ‘–
ğ‘…
.
6. Scorecalculation.Ifthebestsplitpassesdepthandwidthcriteria(minimum3%depth,6%
width),acompositeU-scoreiscomputed:
U-score = 0.6Â·improvement+0.25Â·depth+0.15Â·width,
where:
â€¢ ImprovementisthefractionalMSEreductionrelativetothebestmonotonicbaselinefit.
S17
â€¢ Depthisthedropfromthevalleytothe90thpercentileofthesurroundingpeaks.
â€¢ Widthistherelativeproportionofthesequencebefore/afterthevalley.
7. Indexreporting.Indicesoftheleftpeakğ‘– ğ¿,valleyğ‘– ğ‘€,andrightpeakğ‘– ğ‘… aremarkedwithred
verticallinesinFigure7.
To compare robots training with exceptions and without exceptions, we computed U-shape
scores for each robot individually and compared the two groups using a one-tailed Welchâ€™s ğ‘¡-test
(unequalvariances):
ğ‘¥Â¯ âˆ’ğ‘¥Â¯
ğ‘¡ = 1 2 .
âˆšï¸‚
ğ‘ 2 ğ‘ 2
1 + 2
ğ‘› ğ‘›
1 2
The resulting test statistic confirmed that robots trained with exceptions exhibited significantly
strongerU-shapedprofilesthanthosewithout,with ğ‘ = 0.0025.
S18

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Curiosity-Driven Development of Action and Language in Robots Through Self-Exploration"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   âŒ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   âœ… GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   âŒ BAD: Repeating the same claim 3+ times with slight variations
   âœ… GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
