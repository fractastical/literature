=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Inference of Affordances and Active Motor Control in Simulated Agents
Citation Key: scholz2022inference
Authors: Fedor Scholz, Christian Gumbsch, Sebastian Otte

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2022

Abstract: Flexible,goal-directedbehaviorisafundamentalaspectofhumanlife. Basedon
thefreeenergyminimizationprinciple,thetheoryofactiveinferenceformalizes
thegenerationofsuchbehaviorfromacomputationalneuroscienceperspective.
Basedonthetheory,weintroduceanoutput-probabilistic,temporallypredictive,
modularartificialneuralnetworkarchitecture,whichprocessessensorimotorinfor-
mation,infersbehavior-relevantaspectsofitsworld,andinvokeshighlyflexible,
goal-directedbehavior. Weshowthatourarchitecture,whichistrainede...

Key Terms: simulated, butz, neuro, cognitivemodelinggroup, agents, active, inference, germany, control, goal

=== FULL PAPER TEXT ===

Inference of Affordances and Active Motor Control in
Simulated Agents
FedorScholz ChristianGumbsch
Neuro-CognitiveModelingGroup AutonomousLearningGroup
UniversityofTübingen MaxPlanckInstituteforIntelligentSystems
Tübingen,Germany &Neuro-CognitiveModelingGroup
fedor.scholz@uni-tuebingen.de UniversityofTübingen
Tübingen,Germany
christian.gumbsch@tuebingen.mpg.de
SebastianOtte MartinV.Butz
Neuro-CognitiveModelingGroup Neuro-CognitiveModelingGroup
UniversityofTübingen UniversityofTübingen
Tübingen,Germany Tübingen,Germany
sebastian.otte@uni-tuebingen.de martin.butz@uni-tuebingen.de
Abstract
Flexible,goal-directedbehaviorisafundamentalaspectofhumanlife. Basedon
thefreeenergyminimizationprinciple,thetheoryofactiveinferenceformalizes
thegenerationofsuchbehaviorfromacomputationalneuroscienceperspective.
Basedonthetheory,weintroduceanoutput-probabilistic,temporallypredictive,
modularartificialneuralnetworkarchitecture,whichprocessessensorimotorinfor-
mation,infersbehavior-relevantaspectsofitsworld,andinvokeshighlyflexible,
goal-directedbehavior. Weshowthatourarchitecture,whichistrainedend-to-end
tominimizeanapproximationoffreeenergy, developslatentstatesthatcanbe
interpretedasaffordancemaps. Thatis,theemerginglatentstatessignalwhich
actionsleadtowhicheffectsdependentonthelocalcontext. Incombinationwith
active inference, we show that flexible, goal-directed behavior can be invoked,
incorporatingtheemergingaffordancemaps. Asaresult,oursimulatedagentflexi-
blysteersthroughcontinuousspaces,avoidscollisionswithobstacles,andprefers
pathwaysthatleadtothegoalwithhighcertainty. Additionally,weshowthatthe
learnedagentishighlysuitableforzero-shotgeneralizationacrossenvironments:
Aftertrainingtheagentinahandfuloffixedenvironmentswithobstaclesandother
terrainsaffectingitsbehavior,itperformssimilarlywellinprocedurallygenerated
environments containing different amounts of obstacles and terrains of various
sizesatdifferentlocations.
1 Introduction
We,ashumans,directouractionstowardsgoals. Buthowdoweselectgoalsandhowdowereach
them? Inthisworkwewillfocusonamorespecificversionofthelatterquestion: Givenagoaland
someinformationabouttheenvironment,howcansuitableactionsbeinferredthatultimatelyleadto
thegoalwithhighcertainty?
ThefreeenergyprincipleproposedinFriston[2005]servesasagoodstartingpointforananswer.
Itissometimesregardedasa“unifiedtheoryofthebrain”[Friston,2010],becauseitattemptsto
explainavarietyofbrainprocessessuchasperception,learning,andgoal-directedactionselection,
2202
guA
2
]IA.sc[
3v23511.2022:viXra
basedonasingleobjective: tominimizefreeenergy. Freeenergyconstitutesanupperboundon
surprise,whichresultsfrominteractionswiththeenvironment. Whenactionsareselectedinthis
way,wealsorefertoitasactiveinference. Activeinferencebasicallystatesthatagentsinfersuitable
actionsbyminimizingexpectedfreeenergy,leadingtogoal-directedplanning.
One limitation of active inference-based planning is computational complexity: Optimal active
inferencerequiresanagenttopredictthefreeenergyforallpossibleactionsequencespotentially
far into the future. This soon becomes computationally intractable, which is why so far mostly
simple, discrete environments with small state and action spaces have been investigated [Friston
etal.,2015]. Howdobiologicalagents, suchashumans, dealwiththiscomputationalexplosion
whenplanningbehaviorinourcomplex,dynamicworld? Itappearsthathumans,andotheranimals,
havedevelopedavarietyofinductivebiasesthatfacilitateprocessinghigh-dimensionalsensorimotor
informationinfamiliarsituations[Butz,2008,Butzetal.,2021]. Affordances[Gibson,1986],for
example,encodeobject-andsituation-specificactionpossibilities. Byequippinganactiveinference
agent with the tendency to infer affordances, then, inference-based planning could first focus on
affordedenvironmentalinteractions,significantlyalleviatingthecomputationalloadwhenconsidering
interactionoptions.
Inthiswork,wemodeltheseconjecturesbymeansofanoutput-probabilistic,temporallypredictive
artificialneuralnetworkarchitecture. Thearchitectureisdesignedtofocusonlocalenvironmental
properties, from which it predicts action-dependent interaction consequences via latent state en-
codings. Weshowthat,throughthisprocessingpipeline,affordancemapsemerge,whichencode
behavior-relevantpropertiesoftheenvironment. Theseaffordancemapscanthenbeemployedduring
goal-directed planning. Given spatially local visual information, the resulting latent affordance
codesconstraintheconsideredenvironmentalinteractions. Asaresult,planningviaactiveinference
becomesmoreeffectiveandenables,forexample,theavoidanceofuncertaintywhilemovingtowards
agivengoallocation. Wefurthermoreshowthatthearchitectureexhibitszero-shotlearningabilities
[Eppeetal.,2022],directlysolvingrelatedenvironmentsandtaskswithin.
2 Foundations
Thissectionintroducesthetheoreticalfoundationsofourwork. Wefirstspecifyourproblemsetting
and notation. We then introduce the free energy principle and show how we can perform active
inference-basedgoal-directedplanningwithtwodifferentalgorithms. Subsequently,wecombinethe
theoryofaffordanceswiththeideaofcognitivemapsandarriveattheconceptofaffordancemaps.
Weproposethattheincorporationofaffordancemapscanfacilitategoal-directedplanningviaactive
inference.
2.1 Problemformulationandnotation
Weconsiderproblemsinwhichanagentinteractswithitsenvironmentbyperformingactionsaand
inturnreceivingsensorystatess. Thesensorystatesmightrevealonlypartsoftheenvironmental
statesϑ,whichthereforearenotdirectlyobservable,i.e.,wearefacingapartiallyobservableMarkov
decisionprocess1. Ineverytimestept,anagentselectsandperformsanactionat,andreceivesa
sensorystate(oftencalledobservation)ofthenexttimestepst+1.
Model-basedplanning,suchasactiveinference,requiresamodeloftheworldtosimulateactions
andtheirconsequences. Weuseatransitionmodelt thatpredictstheunfoldingmotor-activity-
M
dependentsensorydynamicswhileanagentinteractswithitsenvironment. Inordertodealwith
partialobservability,thetransitionmodelcanbeequippedwithitsowninternalhiddenstateht. Its
purposeistoencodethestateoftheenvironment,includingpotentiallynon-observableparts. Givena
currentsensorystatest,aninternalhiddenstateht,andanactionat,thetransitionmodelcomputes
anestimateofthesensorystate˜st+1inthenexttimestepandacorrespondingnewhiddenstateht+1:
(˜st+1,ht+1)=t (st,ht,at) (1)
M
1InMarkovdecisionprocesses,usuallytheenvironmentadditionallyreturnsarewardineachtimestep,which
istobemaximizedbytheagent.Here,wedonotdefinearewardfunctionbutinsteadplaninamodel-predictive,
goal-directedmanner.
2
agent
s ˜s
action
ϑ env selection t M h
a
Figure1: Depictionofa(partiallyobservable)Markovdecisionprocess. Anagentinteractswithits
environmentbysendingactionsaandreceivingconsequentsensorystatess. Partialobservability
hereimpliesthatthesensorystatesdoesnotencodethewholeenvironmentalstateϑ. Rather,certain
aspectsremainhiddenfortheagentandmustbeinferredfromthesensorystate. Todealwiththis,our
agentutilizesatransitionmodelt withitsowninternalhiddenstateh. Itpredictssensorystates˜s,
M
whichaidtheactionselectionalgorithmtoproduceappropriateactions. Inordertostayintunewith
theenvironmentandtopredictmultipletimestepsintothefuture,thetransitionmodelalsoreceives
observedandpredictedsensorystates(dashedarrows).
SeeFigure1foradepictionofhowenvironment,agent,transitionmodel,andactionselectionrelate
toeachother.
2.2 Towardsfreeenergy-basedplanning
The free energy principle starts formalizing life itself, very generally, as having an interior and
exterior,separatedbysomeboundary[Friston,2013]. Forlifetomaintainhomeostasis,thisboundary,
protectingtheinterior,needstobemaintained.Itfollowsthatlivingthingsneedtobeinspecificstates
becauseonlyasmallnumberofallpossiblestatesensurehomeostasis. Thefreeenergyprinciple
formalizesthismaintenanceofhomeostaticstatesbymeansofminimizingentropy. Buthowcan
entropy be computed? One possibility is given by the presence of an internal, generative model
m of the world. In this case, we can regard entropy as the expected surprise about encountered
sensorystatesgiventhemodel[Friston,2010]. Inotherwords:Livingthingsmustminimizeexpected
surprise.
Thisimpliesthatalllivingthingsactasiftheystrivetomaintainamodeloftheirenvironmentover
timeinsomewayoranother. Surprise,however,isnotdirectlyaccessibleforalivingthing. Inorder
tocomputethesurprisecorrespondingtosomesensoryinputs,itisnecessarytointegrateoverall
possibleenvironmentalstatesϑthatcouldhaveledtothatinput[Friston,2009]. Wecanseethisin
theformaldefinitionofsurpriseforasensorystates[Fristonetal.,2010]:
(cid:90)
logp(s m)= log p(s,ϑ m)dϑ (2)
− | − |
ϑ
wheremisthemodelorthelivingthingitself,andϑareallenvironmentalstates,includingstates
thatarenotfullyobservableforthelivingthing. Theconsiderationofallthesestatesisinfeasible.
Thus,accordingtothefreeenergyprinciple,livingthingsminimizefreeenergy,whichisdefinedas
follows[Fristonetal.,2010]:
FE(s,h)=E [ logp(s,ϑ m)] E [ logq(ϑ h)] (3)
q(ϑ|h) q(ϑ|h)
− | − − |
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
energy entropy
whereE denotesexpectedvalueandqisanapproximateposteriorovertheexternalhiddenstateϑ
giveninternalhiddenstateh. Sincehereallparametersareaccessible,thisquantityiscomputable.
Rewritingitshowsthatfreeenergycanbedecomposedintoasurpriseandadivergenceterm:
FE(s,h)= logp(s m)+D[q(ϑ h) p(ϑ s,m)] (4)
− | | || |
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
surprise divergence
3
whereDdenotestheKullback-Leiblerdivergence. Sincethedivergencecannotbelessthanzero,
freeenergyisanupperboundonsurprise,ouroriginalquantityofinterest.
Givenagenerativemodeloftheworld,surprisecorrespondstoanunexpected,inaccurateprediction
ofsensoryinformation. Inordertominimizefreeenergy,anagentequippedwithagenerativeworld
modelthushastwowaystominimizethediscrepancybetweenpredictedandactuallyencountered
sensoryinformation: (i)Theinternalworldmodelcanbeadjustedtobetterresembletheworld. In
theshortterm,thisrelatestoperception,whileinthelongterm,thiscorrespondstolearning. (ii)The
agentcanmanipulatetheworldviaitsactions,suchthattheworldbetterfitsitsinternalmodel. In
thiscase,anagentchoosesactionsthatminimizeexpectedfreeenergyinthefuture,pursuingactive
inference.
2.2.1 Activeinference
When the free energy principle is employed as a process theory for action selection, it is called
activeinference. Thenamecomesfromthefactthatthebrainactivelysamplestheworldtoperform
inference: Itinfersactions(alsocalledcontrolstates)thatminimizeexpectedfreeenergy(EFE),that
is,anupperboundonsurpriseinanticipatedfuturestates. Thisiscloselyrelatedtotheprinciple
ofplanningasinferenceinthemachinelearningandcontroltheorycommunities[Botvinickand
Toussaint,2012,Lenzetal.,2015]. AccordingtoFristonetal.[2015],apolicyπisevaluatedattime
steptbyprojectingitintothefutureandevaluatingtheEFEatsometimestepτ >t. Includingthe
internalhiddenstatesht,EFEcanbeformalizedas
EFE(π,t,τ)=D[E [p(sτ hτ)] p(sτ m(τ))]+β E [H[p(sτ hτ)]], (5)
p(hτ|ht,π) p(hτ|ht,π)
| || | · |
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
predicteddivergencefromdesiredstates predicteduncertainty
wheretisthecurrenttimestep,τ >tisafuturetimestep,andβ isanewhyperparameterthatwe
introduce. ThisformulaequatesEFEwithasumoftwocomponents. ThefirstpartistheKullback-
Leiblerdivergence,whichestimateshowfarthepredictedsensorystatesdeviatefromdesiredones.
The second part is the entropy of the predicted sensory states, which quantifies uncertainty. We
introduceβ toweighthesecomponents. Itenablesustotunethetrade-offbetweenchoosingactions
thatminimizeuncertaintyandactionsthatminimizedivergencefromdesiredstates. Tocalculatethe
EFEforawholesequenceofT futuretimesteps,wetakethemeanoftheEFEoverthissequence:
t+T
1 (cid:88)
EFE(π,t)= EFE(π,t,τ) (6)
T
τ=t+1
Basedonthisformula,policiescanbeevaluatedandthepolicywiththeleastEFEcanbechosen:
πt =argminEFE(π,t) (7)
π
Intuitively speaking, active inference-based planning agents choose actions that lead to desired
sensorystateswithhighcertainty.
2.2.2 Planningviaactiveinference
On the computational level, active inference tells us to minimize EFE to perform goal-directed
planning. Thus,itprovidesanobjectivetooptimizeactions. However,itdoesnotspecifyhowto
optimizetheactionsonanalgorithmiclevel. Wethusdetailtwoplanningalgorithmsthatcanbe
employedforthiskindofactionselection. Inbothalgorithms,welimitourselvestoafiniteprediction
horizonT withfixedpolicylengths. Inordertoevaluatepolicies,bothalgorithmsemployatransition
modelt and“imagine”theexecutionofapolicy:
M
(cid:26) t (st,ht,at), ifτ =t
(˜sτ+1,hτ+1)= M (8)
t (˜sτ,hτ,aτ), ift<τ <t+T
M
Foractiveinference-basedplanning,wecancomputetheEFEforthepredictedsequenceandoptimize
theactionsusingoneoftheplanningalgorithms. Afterafixednumberofoptimizationcycles,both
algorithmsreturnasequenceofactions. Thefirstactioncanthenbeexecutedintheenvironment.
4
Gradient-basedactiveinference Actioninference[Otteetal.,2017,Butzetal.,2019]isagradient-
basedoptimizationalgorithmformodel-predictivecontrol. Therefore,itrequiresthetransitionmodel
t tobedifferentiable. Thealgorithmmaintainsapolicyπ,which,ineachoptimizationcycle,isfed
M
intothetransitionmodel. Afterwards,weusebackpropagationthroughtimetobackpropagatethe
EFEontothepolicy. WeobtainthegradientbytakingthederivativeoftheEFEwithrespecttoan
actionaτ fromthepolicy. Aftermultipleoptimizationcycles,thealgorithmreturnsthefirstactionof
theoptimizedpolicy.
Evolutionary-basedactiveinference Thecross-entropymethod(CEM,Rubinstein,1999)isan
evolutionaryoptimizationalgorithm. CEMmaintainstheparametersofaprobabilitydistribution
and minimizes the cross-entropy between this distribution and a distribution that minimizes the
givenobjective. Itdoessobysamplingcandidates,evaluatingthemaccordingtoEFE,andusing
thebestperformingcandidatestoestimatetheparametersofitsprobabilitydistributionforthenext
optimization cycle. Recently, CEM has been used as a zero order optimization technique for
−
model-basedcontrolandreinforcementlearning(RL)[Chuaetal.,2018,Hafneretal.,2019a,Pinneri
etal.,2020]. Insuchamodel-predictivecontrolsetting,CEMmaintainsasequenceofprobability
distributionsandcandidatescorrespondtopolicies. Aftermultipleoptimizationcycles,thealgorithm
returnsthefirstactionofthebestsampledpolicy.
2.3 Behavior-orientedpredictiveencodings
Intheory,givenasufficientlyaccuratemodel,activeinferenceenablesanagenttoplangoal-directed
behaviorregardlessofthecomplexityoftheproblem. Inpractice,however,consideringallpossible
actionsandconsequencesthereofquicklybecomescomputationallyintractable. Tocounteractthis
problem, itappearsthathumansandotheranimalhavedevelopedavarietyofinductivelearning
biasestofocustheplanningprocessbymeansofbehavior-oriented,internalrepresentations. Here,
wefocusonbiasesthatleadtothedevelopmentofaffordances,cognitivemaps,and,incombination,
affordancemaps.
2.3.1 Affordances
Gibson [1986] defines affordances as what the environment offers an animal: Depending on the
currentenvironmentalcontext,affordancesarepossibleinteractions. Asaresult,affordancesfunda-
mentallydeterminehowanimalsbehavedependingontheirenvironment. Theyconstitutebehavioral
optionsfromwhichtheanimalcanselectsuitableonesinordertofulfilitscurrentgoal. Togivean
example,imagineaflatsurfaceattheheightofahuman’sknees. Giventhestructureunderneath
issufficientlysturdy,itispossibletositonthesurfaceinawaythatrequiresrelativelylittleeffort.
Therefore,suchasurfaceissit-upon-able: Itoffersahumanthepossibilitytositonitinaneffortless
way.
Inthiswork,weuseamoregeneraldefinitionofaffordances. Wedefineanaffordanceasanything
intheenvironmentthatlocallyinfluencestheeffectsoftheagent’sactions. Thesedefinitionsdiffer
withrespecttothesetofpossibleactions. Gibson’sdefinitionentailsthatcertainactionsarepossible
onlyincertainenvironmentalcontexts: Forexamplesittingdownisonlypossibleinthepresenceof
achair. Inthiswork,weassumethateveryactionispossibleeverywhereintheenvironment,but
thattheeffectsdifferdependingontheenvironmentalcontext: Sittingdownisalsopossibleinthe
absenceofachair,buttheeffectiscertainlydifferent.
Thetheoryofaffordancesexplicitlystatesthatto(visually)perceivetheenvironmentistoperceive
whatitaffords. Animalsdonotseetheworldasitisandderivetheirbehavioraloptionsfromtheir
perspective. Rather, Gibson proposes that affordances are perceived directly, assigning distinct
meaningstodifferentpartsoftheenvironment. Fromanecologicalperspective,itappearsthatvision
mayhaveevolvedforexactlythispurpose[Gibson,1986]: toconveywhatbehaviorsarepossiblein
thecurrentsituation. First,however,ananimalneedstolearntherelationshipbetweenvisualstimuli
andtheirmeaningforbehavior. Thisisnon-trivial: Similarvisualstimulicanmeandifferentthings,
ortheotherwayround. Furthermore,visualinputisrichsuchthattheanimalneedstoeffectively
focusonthebehavior-relevantinformation.
5
2.3.2 Cognitivemaps
TheconceptofcognitivemapswasintroducedinTolman[1948]. Tolmanshowedthatafterexploring
agivenmaze,ratswereabletonavigatetowardsafoodsourceregardlessoftheirstartingposition.
Heconcludedthattheratsacquiredamentalrepresentationofthemaze: acognitivemap. Placecells
inthehippocampusseemtobeapromisingcandidatefortheneuralcorrelateofthisconcept[O’keefe
andNadel,1978]. Thesecellstendtofirewhentheanimalisatassociatedlocations. Visualinput
actsasstimuli,butalsotheolfactoryandvestibularsensesplayarole. Together,placecellsconstitute
acognitivemap,whichtheanimalappearstousefororientation,reflection,andplanning[Dibaand
Buzsaki,2007,PfeifferandFoster,2013].
2.3.3 Affordancemaps
Cognitive maps are well-suited for flexible navigation and goal-directed planning. However, to
improvetheefficiencyoftheplanningmechanisms,itwillbeusefultoencodebehavior-relevant
aspects,suchastheaforementionedaffordances,withinthecognitivemap. Accordingly,wecombine
thetheoryofaffordanceswithcognitivemaps,leadingtoaffordancemaps. Theirfunctionistomap
spatiallocationsontoaffordancecodes. Likecognitivemaps,theirencodingdependsonvisualcues.
Incontrasttocognitivemaps’traditionalfocusonmap-building,though,affordancemapssignal
distinctbehavioraloptionsatparticularenvironmentallocations. Asanexample,considerahallway
corner situation with corridors to your right and behind you. An affordance map would encode
successfulnavigationoptionsforturningtotherightorturningaround. Regardingaffordancesfor
spatialnavigationspecifically,BonnerandEpsteinshowedthattheseareautomaticallyencodedby
thehumanvisualsystemindependentofthecurrenttaskandproposealocationinthebrainwherea
neuralcorrelatecouldbesituated[BonnerandEpstein,2017].
2.4 Relatedneuralnetworkmodels
HaandSchmidhuber[2018]usedaworldmodeltofacilitateplanningviaRL.Theiroverallarchi-
tecture consists of a vision model which compresses visual information, a memory module, and
a controller model, which predicts actions given a history of the compressed visual information.
Theirvisionmodelisgivenbyavariationalautoencoder,whichistrainedinanunsupervisedmanner
to reconstruct its input. Therefore, and in contrast to ours, their vision model is not trained to
extractmeaningful,behavior-orientedinformation. Thisiswhywewouldnotregardtheemerging
compressedcodesasaffordancecodes.
AffordancemapswereusedbeforeinQietal.[2020]toaidplanning. Theauthorsputanagentinto
anenvironment(VizDoom)withhazardousregionsthatweretobeavoided. Theagentmovedaround
initsenvironmentandcollectedexperiencesofharmornoharm,whichwerebackprojectedontothe
pixelsoftheinputtotheagent’svisualsystem,therebyperformingimagesegmentation. Theauthors
thentrainedaconvolutionalneuralnetwork(CNN)ontheresultingdataofwhichtheoutputwas
utilizedbytheA*algorithmforplanning. Incontrasttoours,theirarchitecturewasnottrainedin
anend-to-endfashion,meaningthattheresultingaffordancecodeswerenotoptimizedtosuittheir
transitionmodel.
3 Model
Wenowdetailtheproposedarchitecture,whichlearnsatransitionmodeloftheenvironmentwith
spatialaffordanceencodings. Thearchitecturepredictsaprobabilitydensityfunctionoverchanges
insensorystatesgiventhecurrentsensorystateandactionaswellaspotentiallyaninternalhidden
state. Thisaction-dependenttransitionmodeloftheenvironmentthusenablesactiveinference-based
planning. Wefirstspecifythearchitecture,thendetailthemodellearningmechanism,andfinallyturn
toactiveinference.
3.1 Affordance-conditionedinference
Ourmodeladheresto thegeneralnotionintroducedabove(cf. Equation1). Ourmodelconsists
ofthreemaincomponents: atransitionmodelt ,avisionmodelv ,andalook-upmapωofthe
M M
environment. ThemodelwithitsdifferentcomponentsisillustratedinFigure2
6
Vision
pt Map ω vt
model v
M
+
∆pt ct
Transition µ Free µ
at ∆p˜t+1 ∆pˆt+1
model t σ energy σ
M ∆p˜t+1 ∆pˆt+1
Figure2: Affordancemaparchitecture: Basedonthecurrentpositionpt,thearchitectureperformsa
look-upinanenvironmentalmapω. Thevisionmodelv receivestheresultingvisualinformation
M
vt and produces a contextual code ct. The transition model t utilizes this context ct, the last
M
changeintheposition∆pt, anactionat, anditsinternalhiddenstateht topredictaprobability
distributionoverthenextchangeinposition. Duringtraining,thelossbetweenpredictedandactual
change in position is backpropagated onto t (red arrows) and further onto v (orange arrows)
M M
to train both models end-to-end. During planning, the map look-up is performed using position
predictions. Forgradient-basedactiveinference, EFEisbackpropagatedontotheactioncodeat
(red and green arrows). For planning with the cross-entropy method, at is modified directly via
evolutionaryoptimization.
Oursystemlearnsatransitionmodelt ofitsenvironment. Itreceivesacurrentsensorystatestand
M
actionatandpredictsaconsequentsensorystate˜st+1. Iftheenvironmentisonlypartiallyobservable,
thetransitionmodelcanfurthermorereceiveaninternalhiddenstateht andpredictaconsequent
ht+1. Focusingonmotioncontroltasks,weencodethesensorystatebyatwo-dimensionalpositional
encoding pt, where the transition model continues to predict changes in positions ∆p˜t+1 given
the last positional change ∆pt, potentially hidden state ht, and current action at. To enable the
modeltoconsiderthepropertiesofdifferentregionsinanenvironmentduringgoal-directedplanning,
though,weintroduceanadditionalcontextualinputct,whichisabletomodifythetransitionmodel’s
predictions(cf. Butzetal.2019,forarelatedapproachwithoutmap-specificity). Ineachtimestep,
thetransitionmodelt thusadditionallyreceivesacontextencodingvectorct,whichshouldencode
M
thelocallybehavior-relevantcharacteristicsoftheenvironment.
Thiscontextcodeisproducedbythevisionmodelv ,whichreceivesavisualrepresentationvt
M
oftheagent’scurrentsurroundingsintheformofasmallpixelimage. Thevisionmodelisthus
designedtogeneratevectorembeddingsthataccuratelymodifythetransitionmodel’spredictions
context-dependently.
Thepredictionofthetransitionmodelcanthusbeformalizedasfollows:
(∆p˜t+1,ht+1)=t (ct =v (vt),∆pt,ht,at) (9)
M M
Thevisualinformationvt canbeunderstoodasalocalviewoftheenvironmentsurroundingthe
agent. Thus,vt dependsontheagent’slocationpt. Toenablethemodeltopredictvt forvarious
agentpositions,forexample,for"imagined"trajectorieswhileplanning,thesystemisequippedwith
alook-upmapωtotranslatepositionspt intolocalviewsoftheenvironmentvt. Weaugmentthe
modelwiththeabilitytoprobeparticularmaplocations,translatethelocationintoalocalimage,
and extract behavior-relevant information from the image. Intuitively speaking, this is as if the
networkcanputitsfocusofattentiontoanylocationonthemapandconsiderthecontext-dependent
behavioral consequences at the considered location. As a result, the system is able to consider
behavioralconsequencesdependentonprobedenvironmentallocations. Infuturework,thelearning
ofcompletelyinternalmapsmaybeinvestigatedfurther.
Theconsequenceofthismodeldesignisthatthecontextcodectwilltendtoencodelocal,behavior-
influencingaspectsoftheenvironment,thatis,affordances. Thecontextisthereforeacompressed
version of the environment’s behaviorally-relevant characteristics at the corresponding position.
Therefore,theincorporationoftheaffordancecodescanbeexpectedtoimproveboththeaccuracyof
7
action-dependentpredictionsandactiveinference-basedplanning. Thisconnectionbetweenactive
inference and affordances can further be described as follows [Friston et al., 2012]: The desired
sensorystateencodedasapriorletstheagentexpecttoreachthetarget. Iftheagentthenisinfront
ofanobstaclee.g.,differentaffordancescompetewitheachother,whichisinlinewiththeaffordance
competitionhypothesis[Cisek,2007]: flyaroundorcrashintotheobstacle. Sinceflyingaround
the obstacle best explains the sensory input in light of the prior, the action corresponding to this
affordanceischosenbytheagent.
3.2 Uncertaintyestimation
The free energy principle is inherently probabilistic and therefore active inference requires our
architecturetoproduceprobabilitydensityfunctionsoversensorystates. Weimplementthisinterms
ofatransitionmodelt thatdoesnotpredictapointestimateofthechangeinsensorystateinthe
M
nexttimestep,butrathertheparametersofaprobabilitydistributionoverthisquantity. Wechoose
themultivariatenormaldistributionwithdiagonalcovariancematrix(i.e.,covariancesaresetto0).
Theoutputofthetransitionmodelisthengivenbyameanvectorµ andavectorofstandard
∆p˜t+1
deviationsσ . Wethusreplace∆p˜t+1withθ :=(µ ,σ ).
∆p˜t+1 ∆p˜t+1 ∆p˜t+1 ∆p˜t+1
3.3 Training
Wetrainbothcomponentsofourarchitecturejointlyinanend-to-end,self-supervisedfashionto
performone-stepaheadpredictionsonapregenerateddatasetviabackpropagationthroughtime.
ThegradientflowduringtrainingisdepictedinFigure2. Inputsconsistofthesensor-actiontuples
describedabove. Theonlyinducedtargetisgivenbythechangeinpositioninthenexttimestep
∆pt+1. This target signal is compared to the output of the transition model t by the negative
M
log-likelihood(NLL)2,whichapproximatesfreeenergyassumingnouncertaintyinourpointestimate
hofenvironmentalstateϑ(seeEquation1). Duetoend-to-endbackpropagation,thevisionmodel
v istrainedtooutputcompact,transitionmodel-conditioningrepresentationsofthevisualinput.
M
WhileweusetheNLLastheobjectiveduringtraininghere,wemakeuseoftheexpectedfreeenergy
duringgoal-directedcontrol. Infutureworkonecouldutilizefull-blownFEalsoduringtraining
inaprobabilisticarchitecture. However,thereisacloserelationshipbetweenNLLandFEdueto
theKullback-Leiblerdivergence: InAppendixF,weshowthatminimizingNLLisequivalentto
minimizingtheKullback-Leiblerdivergenceuptoaconstantfactorandaconstant. Thus,through
NLL-basedlearningwecanapproximatelearningthroughFEminimization.
3.4 Goal-directedcontrol
Weperformgoal-directedcontrolviagradient-andevolutionary-basedactiveinferenceasdescribed
inSection2.2.1. Usually,inordertopredictmultipletimestepsintothefuturegivenapolicy,the
transitionmodelt receivesitsownoutputasinput. Sinceourarchitecturepredictstheparametersof
M
anormaldistribution,weusethepredictedmeanasinputinthenextpredictiontimestep. Themodel
incorporatesvisualinformationvfromlocationscorrespondingtothepredictedmeans. Therefore,
themodeldoesnotblindlyimagineapathbutsimultaneously“looks”at,orfocuseson,predicted
positions,incorporatingtheinferredaffordancecodecintothetransitionmodel’spredictions.
Inordertocomparethepredictedpathtothegiventargetandtolookupthevisualinformation,we
need absolute locations. We thus take the cumulative sum and add the current absolute position.
TocomputeEFEalongapredictedpathwealsoneedtoconsiderthestandarddeviationsatevery
point. Forthat,wefirstconvertstandarddeviationstovariances,computethecumulativesum,and
convertbacktostandarddeviations. WethencancomputetheEFEbetweentheresultingsequenceof
probabilitydistributionsoverpredictedabsolutepositionsandthegiventargetaccordingtoEquation5.
Todoso,weencodethetargetwithamultivariatenormaldistributionaswell,settingthemeantothe
targetlocationandthestandarddeviationtoafixedvalue. Wecanthusoptimizethepolicyviathe
gradient-orevolutionary-basedEFEminimizationmethodintroducedinSection2.2.2above3.
2SeeAppendixDforadescriptionofhowtocomputegradientswhentheobjectiveisgivenbytheNLLina
multivariatenormaldistribution.
3AppendixBsummarizestheparticularadjustmentsweappliedtothesealgorithms.
8
Environment
2.0
1.5
1.0
0.5
0.0
1 0 1
−
Figure3: Thesimulationenvironmentweuseinourexperiments. Itresemblesaconfinedspace
inwhichavehicle(green)canmovearoundbyadjustingitsthrottles(blue). Itsgoalistonavigate
towardsthetarget(red). Dependingontheexperiment,obstaclesordifferentterrains(black)are
present,whichaffectthevehicle’ssensorimotordynamics.
4 Experimentsandresults
Toevaluatetheabilitiesofourneuralaffordancemaparchitecture,wefirstintroducetheenvironmental
simulatorandspecifyourevaluationprocedureingeneral. Theindividualexperimentalresultsthen
evaluate the system’s planning abilities to avoid obstacles and regional uncertainty as well as to
generalize to unseen environments. With respect to the affordance codes c, we show emerging
affordancemapsandexaminedisentanglement.
4.1 Environment
Theenvironmentusedinourexperimentsisaphysics-basedsimulationofacircular,vehicle-like
agentwithradius0.06ina2-dimensionalspacewithanarbitrarysizeof3 2units. Itisconfined
×
byborders,whichpreventthevehiclefromleavingthearea. Thevehicleisabletoflyaroundinthe
environmentbyadjustingits4throttles,whichareattachedbetweentheverticalandhorizontalaxes
inadiagonalfashion. Theytakevaluesbetween0and1resemblingactionsandenablethevehicle
to reach a maximum velocity of approximately 0.23 units per time step within the environment.
Therefore, at least 13 time steps are required for the vehicle to fly from the very left to the very
right. Duetoitsmass,thevehicleundergoesinertiaandperdefault,itisnotaffectedbygravity. See
Figure3foradepictionoftheenvironmentandanagent. ItisimplementedasanOpenAIGym
[Brockmanetal.,2016].
Theenvironmentcancontainobstacles,whichblocktheway. Frictionvaluesarelargerwhenthe
vehicletouchesobstaclesorborders. Furthermore,theenvironmentcancomprisedifferentterrains,
whichlocallychangethesensorimotordynamics. Forcefieldspulltheagentup-ordownwards. If
thevehicleisinsideafogterrain,theenvironmentreturnsapositionthatiscorruptedbyGaussian
noise. Twovaluesfromastandardnormaldistributionaresampledandaddedtoeachcoordinate.
Thisimpliesastandarddeviationofapproximately1.414onthedifferencebetweenpositionsfrom
twoconsecutivetimestepswithinfog. 4
Theenvironmentoutputsabsolutepositions,thechangeinthepositionsandallowsprobingofthe
mapatarbitrarypositions. Therefore,andapartfromthenoisypositionsinfogterrainsandthemap
havingalowerresolutionthantheenvironmentitself,theenvironmentusedinourexperimentsis
fullyobservable. Thismakestheincorporationoftheinternalhiddenstatehinthetransitionmodel
t obsolete. Inthefutureweplantotestourarchitectureonenvironmentswhichareonlypartially
M
observable.
√ √
4Since 12+12 = 2≈1.414.
9
4.2 Modelandagent
The vision model v is given by a CNN, which produces the context activations c. We always
M
evaluate contexts of sizes 0,1, 3, 5, and 8. Since the environment in our experiments is almost
fullyobservable,wedroptheinternalhiddenstatehanduseamultilayerperceptron(MLP)asour
transitionmodelt . Itconsistsofafullyconnectedlayerfollowedbytwoparallelfullyconnected
M
layersforthemeansandstandarddeviations,respectively. Foreachsetting,wetrain25versionsof
thesamearchitecturewithdifferentinitialweightsonapregenerateddatasetandreportaggregated
results. SeeAppendixAformoredetailsonthemodelandtraininghyperparameters,howthevisual
inputvisconstructed,howthedatasetisgenerated,andthetrainingprocedure.
Activeinferenceperformanceisevaluatedafterperforming100goal-directedcontrolrunspersetting
for 200 time steps. For each trained architecture instance, we consider 4 distinct start and target
positionscorrespondingtoeachcorneroftheenvironment. Thestartpositionischosenrandomly
withauniformdistributionovera0.2 0.2unitssquarewithadistanceof0.1unitstotheborders.
×
Thetargetpositionischoseninthesamewayinthediagonallyoppositecorner. Weconsiderthe
agenttohavereachedthetargetwhenitsdistancetothetargetfallsbelow0.1units. Theprediction
horizonalwayshasalengthof20. Foractiveinferencebasedplanning(Equation5),thetargetis
providedtothesystemasaGaussiandistributionwithstandarddeviation0.1. Wereducethestandard
deviation of the target distribution to 0.01 once the agent comes closer than 0.5 units. The two
differentstandarddeviationscanbeseenascorrespondingto,forexample,smellingandseeingthe
target,respectively. SeeAppendixBformoredetailsonthehyperparameters. Allhyperparameters
wereoptimizedempiricallyorwithHyperopt[Bergstraetal.,2013]viaTune[Liawetal.,2018].
Inordertogetanideaaboutthenatureoftheemergingaffordancecodesc,weplotaffordancemapsby
generatingposition-dependentcontextactivationsviathevisionmodelv foreachpossiblelocation
M
intheenvironment. Thatis,intheaffordancemapsshownbelow,thex-andy-axescorrespondto
locations in the environment while the color of each dot represents the context activation at that
position. For this, we performed a principal component analysis (PCA) on the resulting context
activationsinordertoreducethedimensionalityto3andtheninterpretedtheresultsasRGBvalues.
4.3 ExperimentI:Obstacleavoidance
The first of our experiments examines our architecture’s ability to avoid obstacles during active
inferencethroughtheuseofaffordancecodes. Asabaselineexperiment,weconsidercontextsize0,
disablinginformationflowfromthevisionmodelv tothetransitionmodelt . Withcontextsizes
M M
largerthan0,however,thetransitionmodelcanbeinformedaboutobstaclesandbordersviathe
context.
We train the architecture on the environment depicted in Figure 3, where black areas resemble
obstacles. One-hotencodedvisualinformationvhasonechannelonlyfortheobstaclesandborders.
Weperformgoal-directedcontrolonthesameenvironmentwetrainon.
Figure4showstheresults.Contextcodesofincreasingdimensionalityleadtosmallervalidationlosses
(Figure4A),indicatingtheirutilityinimprovingthetransitionmodel’saccuracy. Theaffordancemap
(Figure4B)showsthatobstaclesareencodeddifferentlyfromtherestoftheenvironment. Areas
where free flight if possible are encoded with a context code that corresponds to olive green. In
contrast,areaswhereitisonlypossibletoflyupwards,totheleft,ordownwardse.g.areencodedwith
acontextcodethatcorrespondstothecolororange. Lightgreen,ontheotherhand,representsareas
whereonlymovementtotheleftisblocked. Theaffordancemaprevealsthatdifferentsidesofthe
obstaclesareencodedsimilarlytothecorrespondingsidesoftheenvironment’sboundary. Moreover,
we find gradients in the colors when moving away from borders or obstacles, indicating that the
contextcodesnotonlyencodedirectionsbutalsodistancestoimpassableareas.Thisconfirmsthatthe
emergingcontextcodesconstitutebehavior-relevantencodingsofthevisuallyperceivedenvironment.
Weevaluategoal-directedplanningintermsofpredictionerror(Figure4C)andmeandistancetothe
target(Figure4D).Forevolutionary-basedactiveinference,wefindimprovementinbothmetricswith
increasingcontextsizes. Forgradient-basedactiveinference,wefindimprovementintheprediction
errorbutdeteriorationofthemeandistancetothetargetwithincreasingcontextsizes. Gradient-based
outperforms evolutionary-based active inference with context size 0. With larger context sizes,
evolutionary-basedactiveinferenceperformsbetter. Figure5showstwoexampletrajectoriesfor
contextsizes0and8. Acontextsizeof8allowstheagenttoincorporatelocalinformationaboutthe
10
environment,resultinginpastandplannedtrajectoriesthatbendaroundobstacles. Withcontextsize
0,however,itcanbeseenthattheagentflewagainstoneobstacleandplansitstrajectorythrough
anotherone.
8.0
−
8.5
−
9.0
−
9.5
−
10.0
−
0 10 20 30 40 50
Epoch
LLN
(A)Validationloss (B)Affordancemap
2.0
Contextsize
0
1.5
1
3
1.0
5
8
0.5
0.0
1.5 1.0 0.5 0.0 0.5 1.0 1.5
− − −
8
−
10 −
12
−
0 1 3 5 8
Contextsize
LLN
(C)Predictionerror
1.2
1.0
0.8
Optimizer 0.6
EB
0.4
GB
0 1 3 5 8
Contextsize
ecnatsiD
(D)Meandistancetotarget
Optimizer
EB
GB
Figure4: ResultsforExperimentI–Obstacleavoidance(Subsection4.3).Foreachsetting,theresults
areaggregatedover25differentlyinitializedmodelswhicheachperformed4goal-directedcontrol
runsfor200timesteps. Theboxplotsshowthemedians(horizontalbars),quartiles(boxes),and
minimaandmaxima(whiskers). Datapointsoutsideoftherangedefinedbyextendingthequartiles
by1.5timestheinterquartilerangeinbothdirectionsareignored. EBisshortforevolutionary-and
GBforgradient-basedactiveinference. (A)Validationlossduringtraining. Itisthenegativelog-
likelihoodoftheactualchangeinpositioninthetransitionmodel’spredictedprobabilitydistribution.
Shadedareasrepresentstandarddeviations. (B)Exemplaryaffordancemapforcontextsize8. To
generatethismap,weprobedtheenvironmentalmapateverysensiblelocation,appliedthevision
modeltoeachoutput,performeddimensionalityreductionto3viaPCA,andinterpretedtheresults
asRGBvalues. (C)Predictionerrorduringgoal-directedcontrol. Itisthenegativelog-likelihoodof
theactualchangeinpositioninthetransitionmodel’spredictedprobabilitydistribution. (D)Mean
distancetothetargetduringgoal-directedcontrol.
4.4 ExperimentII:Generalization
Inthisexperimentweexaminehowwellourarchitectureisabletogeneralizetosimilarenvironments.
In Experiment I (Subsection 4.3), we trained on a single environment. Once the architecture is
trained,weexpectthatoursystemshouldbeabletosuccessfullyperformgoal-directedcontrolin
otherenvironmentsaswell,givenweprovidethecorrespondingvisualinput. Thelocalviewonto
themapessentiallyallowsustochangepositionandsizeofobstacleswithoutexpectingsignificant
deteriorationinperformance.
WereusethetrainedmodelsfromExperiment4.3,andapplythemforgoal-directedcontrolontwo
additionalenvironments(seeFigure6). Weonlyconsiderevolutionary-basedactiveinference.
Figure7showstheresults. Predictionerrorandmeandistancetotarget(Figure7CandD)indicate
improvementwithincreasingcontextsize. Furthermore,wefindslightlyworseperformanceinthe
11
Exampletrajectorycontextsize0 Exampletrajectorycontextsize8
2.0 2.0
1.5 1.5
1.0 1.0
0.5 0.5
0.0 0.0
1 0 1 1 0 1
− −
Figure 5: Example trajectories from Experiment I – Obstacle avoidance (Subsection 4.3 with
evolutionary-basedactiveinference. Theagent(green)fliestowardsthetarget(red)withobstacles
(grey)ititsway. Theblacklinebehindtheagentshowsitspasttrajectoryandthelineinfrontits
plannedtrajectory. Circlesinfrontoftheagentshowthepredicteduncertaintyinthesensorystates.
Withcontextsize0,theagentcannotincorporateinformationabouttheenvironmentandtherefore
plansthorughandfliesagainsttheobstacles. Withcontextsize8,theagentcansuccessfullyplanits
wayaroundandthereforeavoidobstacles.
Environment2obstacles Environment12obstacles
2.0 2.0
1.5 1.5
1.0 1.0
0.5 0.5
0.0 0.0
1 0 1 1 0 1
− −
Figure6:Additionalenvironmentsusedduringgoal-directedcontrolinExperimentII–Generalization
(Subsection4.4). Blackareasrepresentobstacles.
environmentwith2obstacles,whileslightlybetterperformanceisachievedintheenvironmentwith
12obstacles. Webelievethisismainlyduetothefactthattheenvironmentwith2obstaclesblocks
thedirectpathmuchmoreseverely. Thus,overalltheseresultsindicatethat(i)thesystemgeneralized
well to similar environments and (ii) incorporating context codes is beneficial for performance
optimization.
4.5 ExperimentIII:Behavioral-relevanceofaffordancecodes
Affordancesshouldonlyencodevisualinformationifitisrelevanttothebehaviorofanagent. Isour
architectureabletoignorevisualinformationforcreatingitsaffordancemaps,ifthisinformation
has no effect on the agent’s behavior? Furthermore, affordances should encode different visual
informationwiththesamebehavioralmeaningsimilarly. Toinvestigateourarchitectureinthisregard,
weperformanexperimentsimilartoExperimentI(Subsection4.3),butwithtwoadditionalchannels
inthecognitivemap. Thefirstchannelencodesthebordersandupperobstacles,thesecondchannel
encodesthelowerobstacles,andthethirdchannelencodesmeaninglessinformation,whichdoesnot
affectthebehavioroftheagent. Figure8showsthecorrespondingenvironment. Wecomparethe
resultsfromthis“hardcondition”totheresultsofExperimentI(Subsection4.3),towhichwerefer
asthe“easycondition”. Weonlyconsiderevolutionary-basedactiveinference.
12
(A)Affordancemap2obstacles (B)Affordancemap12obstacles
2.0 2.0
1.5 1.5
1.0 1.0
0.5 0.5
0.0 0.0
1.5 1.0 0.5 0.0 0.5 1.0 1.5 1.5 1.0 0.5 0.0 0.5 1.0 1.5
− − − − − −
8
−
10
−
12
−
0 1 3 5 8
Contextsize
LLN
(C)Predictionerror
1.5
1.0
Environment
2obstacles
Original
0.5
12obstacles
0 1 3 5 8
Contextsize
ecnatsiD
(D)Meandistancetotarget
Environment
2obstacles
Original
12obstacles
Figure 7: Results from Experiment II – Generalization (Subsection 4.4). For each setting, the
resultsareaggregatedover25differentlyinitializedmodelswhicheachperformed4goal-directed
controlrunsfor200timesteps. Theboxplotsshowthemedians(horizontalbars),quartiles(boxes),
and minima and maxima (whiskers). Data points outside of the range defined by extending the
quartilesby1.5timestheinterquartilerangeinbothdirectionsareignored. "Original"referstothe
environmentfromExperimentI(Subsection4.3. (A)Exemplaryaffordancemapforcontextsize8
fromenvironmentwith2obstacles. Togeneratethismap,weprobedtheenvironmentalmapatevery
sensiblelocation,appliedthevisionmodeltoeachoutput,performeddimensionalityreductionto
3viaPCA,andinterpretedtheresultsasRGBvalues. (B)Exemplaryaffordancemapforcontext
size8fromenvironmentwith12obstacles. (C)Predictionerrorduringgoal-directedcontrolonall
threeenvironments. Itisthenegativelog-likelihoodoftheactualchangeinpositioninthetransition
model’s predicted probability distribution. (D) Mean distance to the target during goal-directed
controlonallthreeenvironments.
Figure9showstheresults. Wedonotfindasignificantdifferencebetweenthetwoconditions. The
developingaffordancemap(Figure9B)isqualitativelysimilartotheoneobtainedfromExperimentI
(Subsection4.3): neitherdosignificantvisualdifferencesbetweentheencodingsofthedifferent
obstaclesremain,nortracesofthemeaninglessinformation. AppendixCexemplarilyshowshowthis
affordancemapdevelopsoverthecourseoftraining. Finally,alsoperformanceintermsofprediction
errorandmeandistancetothetargetstayssimilartoExperimentIwhenanalyzinggoal-directed
control(Figure9CandD).
4.6 ExperimentIV:Uncertaintyavoidance
Activeinferenceconsidersuncertaintyduringgoal-directedcontrol. Inthisexperiment,weexamine
thearchitecture’sabilitytoavoidregionsofuncertaintyduringplanning. Weconsiderarunasuccess
iftheagentreachedthetargetandwasatnopointinsideafogterrain. Asmentionedabove, we
introduceanadditionalhyperparameterβ,whichscalestheinfluenceoftheentropytermonthefree
energy(seeEquation5). Here,wesetβ to10tofosteravoidanceofuncertainty. Weonlyconsider
evolutionary-basedactiveinference.
13
Environment
2.0
1.5
1.0
0.5
0.0
1 0 1
−
Figure8: Oneoftheenvironments(hardcondition)usedinExperimentIII–Behavioral-relevanceof
affordancecodes(Subsection4.5). Blackandgreycirclesrepresentobstacles. Theylookdifferently
totheagentbuthavethesameinfluenceonbehavior(i.e.pathblockage). Greencirclesareaswell
seen by the agent but represent open area and therefore mean the same as the white background
behaviorally.
WetrainthearchitectureontheenvironmentdepictedinFigure3,thistimeblackareasindicatefog
terrainsinsteadofobstacles. Thecognitivemapconsistsoftwochannels: onechannelforfogterrains
andonechannelfortheborders.
Figure10showstheresults. Wefindthatthecontextencodingclearlyimprovesthevalidationloss
(Figure10A).Theaffordancemap(Figure10B)showsthatfreeareas,borders,andfogareencoded
differently. Thepredictionerror(Figure10C)improveswhencontextiscomputed,whiletheratioof
successfulruns(Figure10D)staysrelativelycloseto1.
4.7 ExperimentV:Disentanglement
In our final experiment, we examined the architecture’s ability to combine previously learned
affordance codes. We trained each architecture instance on four different environments. The
environmentsareconstructedasshowninFigure3,blackareasresemblingobstaclesinthefirst,fog
terrainsinthesecond,forcefieldspointingupwardsinthethird,andforcefieldspointingdownwards
inthefourthenvironment. Accordingly,thecognitivemapconsistsoffourchannels—onechannel
foreachoftheaforementionedproperties. Weevaluatethearchitectureonprocedurallygenerated
environments. Ineachenvironment,arandomlychosenamountbetween6and10obstacles,fog
terrains,forcefieldspointingdownwards,andforcefieldspointingupwardswithrandomlychosen
radiibetween0.1and0.5areplacedatrandomlocationsintheenvironment. Allobstaclesandfog
terrainshaveaminimumdistanceof0.15unitsfromeachother,ensuringthattheagentisabletofly
betweenthem—thusprohibitingdeadends. Furthermore,allpropertieshaveaminimumdistance
of0.15toeachborder,againtoavoiddeadends. Patchesofsize0.4 0.4unitsareleftfreeinthe
×
corners such that start and target positions are not affected. We generate environments with two
differentconditions. Inthefirstcondition(easy),forcefieldsarehandledsimilarlytoobstaclesand
fogterrainsinthewaythattheyhaveaminimumdistanceof0.15unitstoallotherobstacles,terrains,
andforcefields. Thismeansthatpropertiesdonotoverlap. Inthesecondcondition(hard),force
fieldscanoverlapwitheachother,obstacles,andfogterrains. Weonlyconsiderevolutionary-based
activeinference. Inadditiontothecontextsizesfrombefore,wealsoevaluatethearchitecturefor
contextsizes16and32.
Figure 11 shows the results. Larger context sizes lead to smaller validation losses (Figure 11A).
Theaffordancemapcomputedonanenvironmentcontainingallproperties(Figure11B)showsthat
thenetworkhaslearnedtoencodethedistinctareasindeedwithdistinctencodings. Theencoding
alsoincorporatesboundarydirections,thusencodingthepropertiesrelativetothefreespacefrom
whichtheagentmayenterthearea(see,forexample,thebordersoftheenvironment). Asexpected,
the agent always performs better in the easy condition (Figure 11C and D). In both conditions,
performanceimproveswithincreasingcontextsize.
14
8.0
−
8.5
−
9.0
−
9.5
−
10.0
−
0 10 20 30 40 50
Epoch
LLN
(A)Validationloss (B)Affordancemaphardcondition
2.0
Condition
Easy
1.5
Hard
1.0
0.5
0.0
1.5 1.0 0.5 0.0 0.5 1.0 1.5
− − −
8
−
10
−
12
−
0 1 3 5 8
Contextsize
LLN
(C)Predictionerror
1.2
1.0
0.8
Condition
0.6
Easy
Hard 0.4
0 1 3 5 8
Contextsize
ecnatsiD
(D)Meandistancetotarget
Condition
Easy
Hard
Figure9: ResultsfromExperimentIII–Behavioral-relevanceofaffordancecodes(Subsection4.5).
Foreachsetting,theresultsareaggregatedover25differentlyinitializedmodelswhicheachper-
formed4goal-directedcontrolrunsfor200timesteps. Theboxplotsshowthemedians(horizontal
bars),quartiles(boxes),andminimaandmaxima(whiskers). Datapointsoutsideoftherangedefined
byextendingthequartilesby1.5timestheinterquartilerangeinbothdirectionsareignored. "Easy"
referstotheenvironmentfromExperimentI,"hard"referstotheconditionwithupperandlower
obstaclesencodeddifferentlyandadditionalmeaninglessinformation. (A)Validationlossduring
training. Itisthenegativelog-likelihoodoftheactualchangeinpositioninthetransitionmodel’s
predictedprobabilitydistribution. Shadedareasrepresentstandarddeviations. (B)Exemplaryaffor-
dancemapforcontextsize8fromenvironmentwith2obstacles. Togeneratethismap,weprobed
theenvironmentalmapateverysensiblelocation,appliedthevisionmodeltoeachoutput,performed
dimensionalityreductionto3viaPCA,andinterpretedtheresultsasRGBvalues. (C)Prediction
errorduringgoal-directedcontrol. Itisthenegativelog-likelihoodoftheactualchangeinposition
inthetransitionmodel’spredictedprobabilitydistribution. (D)Meandistancetothetargetduring
goal-directedcontrol.
5 Discussion
Inthispaper,wehaveconnectedactiveinferencewiththetheoryofaffordancesinordertoguidethe
searchforsuitablebehavioralpoliciesviaactiveinferenceinrecurrentneuralnetworks. Theresulting
architectureisabletoperformgoal-directedplanningwhileconsideringthepropertiesoftheagent’s
localenvironment. Thischapterprovidesasummaryofourarchitecture’sabilities,comparesitto
relatedwork,andeventuallypresentspossiblefutureworkdirections.
5.1 Conclusion
ExperimentI(Subsection4.3)showedthatourproposedarchitecturefacilitatesgoal-directedplanning
viaactiveinference. Boththevalidationlossaswellasperformanceduringgoal-directedcontrol
revealed an advantage of incorporating affordance information, i.e. using a context size larger
than0. Theaffordancemapsconfirmedthatthearchitectureisabletoinferrelationshipsbetween
15
2
−
4
−
6
−
8
−
0 10 20 30 40 50
Epoch
LLN
(A)Validationloss (B)Affordancemap
2.0
Contextsize
0
1.5
1
3
1.0
5
8
0.5
0.0
1.5 1.0 0.5 0.0 0.5 1.0 1.5
− − −
6
−
8
−
10
−
12
−
14
− 0 1 3 5 8
Contextsize
LLN
(C)Predictionerror
1.0
0.8
0.6
0.4
0.2
0.0
0 1 3 5 8
Contextsize
oitaR
(D)Ratioofsuccessfulruns
Figure10: ResultsfromExperimentIV–Uncertaintyavoidance(Subsection4.6). Foreachsetting,
theresultsareaggregatedover25differentlyinitializedmodelswhicheachperformed4goal-directed
controlrunsfor200timesteps. Theboxplotshowsthemedians(horizontalbars),quartiles(boxes),
and minima and maxima (whiskers). Data points outside of the range defined by extending the
quartilesby1.5timestheinterquartilerangeinbothdirectionsareignored. Thebarplotsshowthe
means,whereblacklinesrepresentthestandarddeviations. (A)Validationlossduringtraining. It
is the negative log-likelihood of the actual change in position in the transition model’s predicted
probabilitydistribution. Shadedareasrepresentstandarddeviations. (B)Exemplaryaffordancemap
forcontextsize8. Togeneratethismap,weprobedtheenvironmentalmapateverysensiblelocation,
applied the vision model to each output, performed dimensionality reduction to 3 via PCA, and
interpretedtheresultsasRGBvalues. (C)Predictionerrorduringgoal-directedcontrol. Itisthe
negativelog-likelihoodoftheactualchangeinpositioninthetransitionmodel’spredictedprobability
distribution. (D)Ratioofsuccessfulruns. Arunwassuccessfuliftheagentwasclosertothetarget
than0.1unitsinatleastonetimestepanddidnottouchfoginanytimestep.
environmentalfeaturesandtheirmeaningfortheagent’sbehavior: Dependingonthedirectionofand
thedistancetothenextobstacle,differentcodesemerged. Weassumethatcontextsizelargerthan1
allowsaneasierencodingofthedirectionofanddistancetotheobstaclesinrelationtotheagent.
Evolutionary-basedactiveinferenceoutperformsgradient-basedactiveifacontextisused.Thisisdue
tothefactthatgradient-basedactiveinferencereliesonthegradientsbeingbackpropagatedthrough
thepredictedsequenceofsensorystates. Thesegradientscannotbebackpropagatedthroughthe
contextcodes,sincethesedependonthevisualinformationwhichthemodelacquiresviaalook-upin
theenvironmentalmap. Therefore,vitalinformationismissingduringgradient-basedactiveinference
inordertooptimizethepolicyaccordingly. ExperimentII(Subsection4.4)showedthatoncethe
relationship between environmental features and their meaning was learned, this knowledge can
be generalized to other environments with similar, but differently sized and positioned obstacles
ofdifferentamounts. ExperimentIII(Subsection4.5)showedthatourarchitectureisabletomap
propertiesoftheenvironmentthatareencodeddifferentlyvisuallybuthavethesameinfluenceon
behaviorontothesameaffordancecodes. Thismatchesourgeneraldefinitionofaffordances,namely
anaffordanceencodinglocallybehavior-modifyingpropertiesoftheenvironment. Inthefuture,we
16
6
−
8
−
10
−
12
−
0 100 200 300 400 500
Epoch
LLN
(A)Validationloss (B)Affordancemap
2.0
Contextsize
0
1.5
1
3
1.0
5
8
0.5
16
32
0.0
1.5 1.0 0.5 0.0 0.5 1.0 1.5
− − −
8
−
10
−
12
−
14
−
16
−
18
−
0 1 3 5 8 16 32
Contextsize
LLN
(C)Predictionerror
1.0
Condition
Easy 0.8
Hard
0.6
0.4
0.2
0.0
0 1 3 5 8 16 32
Contextsize
oitaR
(D)Ratioofsuccessfulruns
Condition
Easy
Hard
Figure11: ResultsfromExperimentV–Disentanglement(Subsection4.7). Foreachsetting,the
resultsareaggregatedover25differentlyinitializedmodelswhicheachperformed4goal-directed
controlrunsfor200timesteps. Theboxplotshowsthemedians(horizontalbars),quartiles(boxes),
and minima and maxima (whiskers). Data points outside of the range defined by extending the
quartilesby1.5timestheinterquartilerangeinbothdirectionsareignored. Thebarplotshowsthe
means,whereblacklinesrepresentthestandarddeviations. "Easy"referstotheconditionwhere
obstaclesandterrainsdonotoverlap,"Hard"referstotheconditionwhere,forcefieldscanoverlap
witheachother,obstacles,andfogterrains. (A)Validationlossduringtraining. Itisthenegativelog-
likelihoodoftheactualchangeinpositioninthetransitionmodel’spredictedprobabilitydistribution.
Shadedareasrepresentstandarddeviations. (B)Exemplaryaffordancemapforcontextsize8. This
environmentcontainsallfourproporties(obstacleintheupperleft,forterrainsintheupperright,
forcefieldpointingdownwardsinthelowerleft,andforcefieldpoitingupwardsinthelowerright). It
wasnotusedduringgoal-directedcontrol. Togeneratethismap,weprobedtheenvironmentalmapat
everysensiblelocation,appliedthevisionmodeltoeachoutput,performeddimensionalityreduction
to3viaPCA,andinterpretedtheresultsasRGBvalues. (C)Predictionerrorduringgoal-directed
control. Itisthenegativelog-likelihoodoftheactualchangeinpositioninthetransitionmodel’s
predictedprobabilitydistribution. (D)Ratioofsuccessfulruns. Arunwassuccessfuliftheagentwas
closertothetargetthan0.1unitsinatleastonetimestepanddidnottouchfoginanytimestep.
plantoevaluateourarchitectureinenvironmentswheretheconnectiontotask-relevancyismore
concrete. Anexamplewouldbeakeythatneedstobepickedupbytheagentinorderforadoor
to be encoded as passable. Experiment IV (Subsection 4.6) showed that our architecture is able
toavoidregionsofuncertainty(fogterrains)duringplanningviaactiveinference. ExperimentV
(Subsection 4.7) emphasized our architecture’s generalization abilities, but also showed that the
learnedaffordancesarenotdisentangled. Ifpropertiesoftheenvironmentdonotoverlapandwith
sufficientlylargecontextsize,theagentcansuccessfullyreachthetargetwithouttouchingregionsof
uncertaintynearlyallthetime. Thisislesssoifpropertiesdooverlap. Weproposethatanadditional
regularizationthatmayfosteradisentanglementorfactorizationofthelearnedaffordancescouldlead
toafullysuccessfulgeneralizationtoarbitrarycombinationsofpreviouslyencounteredproperties.
17
OurnotionofaffordancesadmittedlyslightlydiffersfromtheoriginaldefinitioninGibson[1986].
In this work, we assume that every action is possible everywhere, but that only the effects differ
dependingontheenvironmentalcontext. Thiswascertainlythecaseintheenvironmentweused
inourexperiments. Wethinkthatthisisalsooftenthecaseintherealworld—particularlywhen
actionsareconsideredonthelowestlevelonly,i.e. musclemovements. Whenincreasingbehavioral
abstraction,though,thismightnotnecessarilybethecaseanylonger. Forexample,thehigh-level,
compositeactionofdrivinganailintoawallclearlyisnotpossibleineveryenvironmentalcontext. In
thefuture,wewanttoinvestigatehowourarchitecturecanbeexpandedtoflexiblyandhierarchically
processevent-likestructures[Zacksetal.,2007,Butzetal.,2021,ZacksandTversky,2001,Eppe
etal.,2022]. Inordertofostertheseeventstructures,inductivebiasesasinButzetal.[2019]and
Gumbschetal.[2021]mightbenecessary,whichassumethatmostofthetimeagentsarewithin
ongoingeventsandthateventboundariescharacterizetransitionsbetweenevents. Sucheventmodels
maythussetthegeneralcontext. Theproposedvisionmodelmaythenbeconditionedonthiscontext
toenableaccurate,event-conditionedaction-effectpredictions. Asaresult,theevent-conditioned
modelwouldlearntoencodewhentheactionresultthatisassociatedwithaparticularevent-specific
affordancecanbeaccomplished.
We treat the context size in our experiments as a hyperparameter, which needs to be set by the
experimenter. Our results show that when the context size is too small, the system is unable to
learnallenvironmentalinfluencesonactioneffects. Largercontextsizes,ontheotherhand,tendto
decreasepredictionerror. Improvementdoesnotonlydependonthecontextsize,though,butalso
onthecomputationalcapacityofthevisionandtransitionmodels. Whatcontextsizeisnecessary
for a certain number of possible environmental interactions remains an open question for future
research. Onepossibledirectionhereistoletthemodeladaptthecontextsizeondemand. Inthis
case,thecomputationalcapacityofthevisionandtransitionmodelsneedtobeadaptedaswell,which
posesachallenge. Furthermore,thetransitionmodelmayinfer,ifequippedwithrecurrencestodeal
withpartialobservability,informationthatwouldotherwisebeimmediatelyavailableviathevision
module. Thisleadstoacompetitionduringlearning,whichneedstobestudiedfurther.
Even though our architecture was successful in solving the presented tasks, it clearly has some
limitations. First,ourmodelcomputesaffordancecodesdirectlyfromvisualinformation. Sinceit
isnotabletomemorizewhichaffordancesarewhere, itconstantlyneedstoperformlook-upsin
theenvironmentalmap. Second,ourproposedarchitecturesolvestheconsideredtasksinagreedy
manner. Duringplanning,wecomparethepredictedsensorystatestoafixeddesiredsensorystate
overthepredictedtrajectory. Ourmodelthereforeprefersactionsthatleadclosertothetargetonly
within the prediction horizon. This can be problematic if we consider e.g. tool use. Imagine an
environmentwithkeysanddoors. Here,itmightbenecessarytotemporarilysteerawayfromthe
targetinordertopickupakeyandeventuallygetclosertothetargetafterunlockingandpassing
throughthedoor. Withoutfurthermodifications,ouragentwouldnotmakesuchadetourdeliberately.
Inthefutureworksectionbelow,wemakesuggestionsonhowtheselimitationsmaybeovercome.
Reinforcement learning (RL) [Sutton and Barto, 2018] is another popular approach for solving
POMDPs. Therefore,infuturework,itwouldbeinterestingtoseehowanRLagentperformsin
comparisontoouragent. Acentralaspectofourarchitectureisthelook-upintheenvironmental
whichmakestheemergenceofaffordancemapspossible. Whilecertainlypossible,itisnotstraight-
forwardhowthiswouldbeimplementedinaclassicalRLagent. ClassicalRLagentsdonotpredict
positionalchangeswhicharenecessaryforthelook-up. Furthermore,itwasshownthatRLagents
strugglewithofflinelearning[Levineetal.,2020]andgeneralizationtosimilarenvironments[Cobbe
etal.,2019].
5.2 Futurework
Inthiswork,wetrainedourarchitectureonpreviouslycollecteddataandonlyafterwardsperformed
goal-directedcontrol. Alternatively,onecouldperformgoal-directedcontrolfromtheverybeginning
andtrainthearchitectureoninferredactionsandthecorrespondingencounteredobservationsinaself-
supervisedlearningmanner. Thisshouldincreaseperformancesincethedistributionofthetraining
data for the transition model then more closely matches the distribution of the data encountered
duringcontrol. Inthiscase,theexploration-exploitationdilemmaneedstoberesolved: Howshould
theagentdecidewhetheritshouldexploitpreviouslyacquiredknowledgetoreachitsgoalorinstead
exploretheenvironmenttogainfurtherknowledgethatcanbeexploitedinlatertrials? Theactive
18
inferencemechanism[Fristonetal.,2015]generallyprovidesasolutiontothisproblem,although
optimalparametertuningremainschallenging[Tani,2017].
Futureworkcouldalsoexaminetowhatextentitispossibletofullymemorizeaffordancesakinto
acognitivemap. Astraightforwardapproachwouldbetotrainamulti-layerperceptronthatmaps
absolutepositionsontoaffordancecodes,inwhichcasetranslationalinvarianceislost. Alternatively,
a recurrent neural network that receives actions could predict affordances in future time steps
conditioneduponpreviouslyencounteredaffordances. Additionally,thetransitionmodelcouldbe
splitintoanencoder,whichmapssensorystatesontointernalhiddenstates,andatransitionmodel,
whichmapsinternalhiddenstatesandactionsontonextinternalhiddenstates. Theintroductionofan
observationmodelthattranslatesinternalhiddenstatesbackintosensorystateswouldthenenable
thewholeplanningprocesstotakeplaceinhiddenstatespaceakintoPlaNet[Hafneretal.,2019a]
andDreamer[Hafneretal.,2019b].
Funding
ThisresearchwassupportedbytheGermanResearchFoundation(DFG)withinPriority-Program
SPP2134-project“Developmentoftheagentiveself”(BU1335/11-1,EL253/8-1),theresearch
unit2718on“ModalandAmodalCognition: FunctionsandInteractions´´(BU1335/12-1),andthe
MachineLearningClusterofExcellencefundedbytheDeutscheForschungsgemeinschaft(DFG,
GermanResearchFoundation)underGermany’sExcellenceStrategy–EXCnumber2064/1–Project
number390727645. Finally,additionalsupportcamefromtheOpenAccessPublishingFundofthe
UniversityofTübingen.
Acknowledgements
TheauthorsthanktheInternationalMaxPlanckResearchSchoolforIntelligentSystems(IMPRS-IS)
forsupportingFedorScholzandChristianGumbsch.
References
Karl Friston. A theory of cortical responses. Philosophical transactions of the Royal Society B:
Biologicalsciences,360(1456):815–836,2005. Publisher: TheRoyalSocietyLondon.
KarlFriston. Thefree-energyprinciple: aunifiedbraintheory? Naturereviewsneuroscience,11(2):
127–138,2010. Publisher: Naturepublishinggroup.
Karl Friston, Francesco Rigoli, Dimitri Ognibene, Christoph Mathys, Thomas FitzGerald, and
GiovanniPezzulo. Activeinferenceandepistemicvalue. CognitiveNeuroscience,6:187–214,
2015. doi: 10.1080/17588928.2015.1020053.
MartinV.Butz. Howandwhythebrainlaysthefoundationsforaconsciousself. Constructivist
Foundations,4(1):1–42,2008.
MartinV.Butz,AsyaAchimova,DavidBilkey,andAlistairKnott. Event-predictivecognition: A
rootforconceptualhumanthought. TopicsinCognitiveScience,13:10–24,2021. doi: 10.1111/
tops.12522.
JamesJeromeGibson. Theecologicalapproachtovisualperception,volume1. PsychologyPress
NewYork,1986.
ManfredEppe,ChristianGumbsch,MatthiasKerzel,PhuongD.H.Nguyen,MartinV.Butz,andSte-
fanWermter. Intelligentproblem-solvingasintegratedhierarchicalreinforcementlearning. Nature
MachineIntelligence,4(1):11–20,2022. ISSN2522-5839. doi: 10.1038/s42256-021-00433-9.
Karl Friston. Life as we know it. Journal of The Royal Society Interface, 10(86), 2013. doi:
10.1098/rsif.2013.0475.
19
KarlFriston. Thefree-energyprinciple: aroughguidetothebrain? TrendsinCognitiveSciences,
13:293–301,72009. doi: 10.1016/j.tics.2009.04.005. URLhttp://dx.doi.org/10.1016/j.
tics.2009.04.005.
KarlJ.Friston,JeanDaunizeau,JamesKilner,andStefanJ.Kiebel. Actionandbehavior: afree-
energyformulation. BiologicalCybernetics,102(3):227–260,March2010. ISSN1432-0770. doi:
10.1007/s00422-010-0364-z. URLhttps://doi.org/10.1007/s00422-010-0364-z.
MatthewBotvinickandMarcToussaint. Planningasinference. TrendsinCognitiveSciences,16(10):
485–488,2012. ISSN1364-6613. doi: 10.1016/j.tics.2012.08.006.
IanLenz,RossKnepper,andAshutoshSaxena. Deepmpc: Learningdeeplatentfeaturesformodel
predictivecontrol. Robotics: ScienceandSystems,2015. doi: DOI:10.15607/RSS.2015.XI.012.
SebastianOtte,TheresaSchmitt,KarlFriston,andMartinV.Butz. Inferringadaptivegoal-directed
behaviorwithinrecurrentneuralnetworks. InAlessandraLintas, StefanoRovetta, PaulF.M.J.
Verschure,andAlessandroE.P.Villa,editors,ArtificialNeuralNetworksandMachineLearning
–ICANN2017,volume10613,pages227–235.SpringerInternationalPublishing,Cham,2017.
ISBN978-3-319-68599-1978-3-319-68600-4. doi: 10.1007/978-3-319-68600-4\_27. URLhttp:
//link.springer.com/10.1007/978-3-319-68600-4_27. Series Title: Lecture Notes in
ComputerScience.
Martin V. Butz, David Bilkey, Dania Humaidan, Alistair Knott, and Sebastian Otte. Learning,
planning,andcontrolinamonolithicneuraleventinferencearchitecture. arXiv:1809.07412[cs],
May2019. URLhttp://arxiv.org/abs/1809.07412. arXiv: 1809.07412.
Reuven Rubinstein. The cross-entropy method for combinatorial and continuous optimization.
Methodologyandcomputinginappliedprobability,1(2):127–190,1999.
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement
learninginahandfuloftrialsusingprobabilisticdynamicsmodels. InS.Bengio, H.Wallach,
H.Larochelle,K.Grauman,N.Cesa-Bianchi,andR.Garnett,editors,AdvancesinNeuralInforma-
tionProcessingSystems,volume31.CurranAssociates,Inc.,2018.URLhttps://proceedings.
neurips.cc/paper/2018/file/3de568f8597b94bda53149c7d7f5958c-Paper.pdf.
DanijarHafner,TimothyLillicrap,IanFischer,RubenVillegas,DavidHa,HonglakLee,andJames
Davidson. Learninglatentdynamicsforplanningfrompixels. InKamalikaChaudhuriandRuslan
Salakhutdinov,editors,Proceedingsofthe36thInternationalConferenceonMachineLearning,
volume97ofProceedingsofMachineLearningResearch,pages2555–2565.PMLR,09–15Jun
2019a. URLhttps://proceedings.mlr.press/v97/hafner19a.html.
Cristina Pinneri, Shambhuraj Sawant, Sebastian Blaes, Jan Achterhold, Joerg Stueckler, Michal
Rolinek,andGeorgMartius. Sample-efficientcross-entropymethodforreal-timeplanning. arXiv
preprintarXiv:2008.06389,Aug2020. URLhttp://arxiv.org/abs/2008.06389v1.
EdwardCTolman. Cognitivemapsinratsandmen. Psychologicalreview,55(4):189,1948.
JohnO’keefeandLynnNadel. Thehippocampusasacognitivemap. Oxford: ClarendonPress,1978.
KamranDibaandGyorgyBuzsaki. Forwardandreversehippocampalplace-cellsequencesduring
ripples. Natureneuroscience,10(10):1241–1242,2007. ISSN1097-6256. doi: 10.1038/nn1961.
Brad E. Pfeiffer and David J. Foster. Hippocampal place-cell sequences depict future paths to
rememberedgoals. Nature,497(7447):74–79,2013. doi: 10.1038/nature12112.
MichaelFBonnerandRussellAEpstein. Codingofnavigationalaffordancesinthehumanvisual
system. ProceedingsoftheNationalAcademyofSciences,114(18):4793–4798,2017.
DavidHaandJürgenSchmidhuber. Worldmodels. arXivpreprintarXiv:1803.10122,Mar2018. doi:
10.5281/zenodo.1207631. URLhttp://arxiv.org/abs/1803.10122v4.
William Qi, Ravi Teja Mullapudi, Saurabh Gupta, and Deva Ramanan. Learning to move
with affordance maps. arXiv preprint arXiv:2001.02364, ICLR 2020, 2020. URL https:
//openreview.net/forum?id=BJgMFxrYPB.
20
KarlJFriston,TamaraShiner,ThomasFitzGerald,JosephMGalea,RickAdams,HarrietBrown,Ray-
mondJDolan,RosalynMoran,KlaasEnnoStephan,andSvenBestmann. Dopamine,affordance
andactiveinference. PLoScomputationalbiology,8(1):e1002327,2012.
PaulCisek. Corticalmechanismsofactionselection: theaffordancecompetitionhypothesis. Philo-
sophicalTransactionsoftheRoyalSocietyB:BiologicalSciences,362(1485):1585–1599,2007.
GregBrockman,VickiCheung,LudwigPettersson,JonasSchneider,JohnSchulman,JieTang,and
WojciechZaremba. Openaigym. arXivpreprintarXiv:1606.01540,2016.
JamesBergstra,DanielYamins,andDavidCox. Makingascienceofmodelsearch: Hyperparameter
optimizationinhundredsofdimensionsforvisionarchitectures. InSanjoyDasguptaandDavid
McAllester, editors, Proceedings of the 30th International Conference on Machine Learning,
volume28ofProceedingsofMachineLearningResearch,pages115–123,Atlanta,Georgia,USA,
17–19Jun2013.PMLR. URLhttps://proceedings.mlr.press/v28/bergstra13.html.
RichardLiaw,EricLiang,RobertNishihara,PhilippMoritz,JosephEGonzalez,andIonStoica.Tune:
Aresearchplatformfordistributedmodelselectionandtraining. arXivpreprintarXiv:1807.05118,
2018.
JeffreyM.Zacks,NicoleK.Speer,KhenaM.Swallow,ToddS.Braver,andJeremyR.Reynolds.
Eventperception: Amind-brainperspective. PsychologicalBulletin,133:273–293,32007. doi:
10.1037/0033-2909.133.2.273. URLhttp://dx.doi.org/10.1037/0033-2909.133.2.273.
JeffreyMZacksandBarbaraTversky. Eventstructureinperceptionandconception. Psychological
bulletin,127(1):3,2001.
ChristianGumbsch,MartinV.Butz,andGeorgMartius. Sparselychanginglatentstatesforprediction
andplanninginpartiallyobservabledomains. InM.Ranzato,A.Beygelzimer,Y.Dauphin,P.S.
Liang,andJ.WortmanVaughan,editors,AdvancesinNeuralInformationProcessingSystems,
volume34,pages17518–17531.CurranAssociates,Inc.,2021. URLhttps://proceedings.
neurips.cc/paper/2021/file/927b028cfa24b23a09ff20c1a7f9b398-Paper.pdf.
RichardSSuttonandAndrewGBarto. Reinforcementlearning: Anintroduction. MITpress,2018.
SergeyLevine,AviralKumar,GeorgeTucker,andJustinFu. Offlinereinforcementlearning: Tutorial,
review,andperspectivesonopenproblems. arXivpreprintarXiv:2005.01643,2020.
KarlCobbe,OlegKlimov,ChrisHesse,TaehoonKim,andJohnSchulman.Quantifyinggeneralization
inreinforcementlearning. InKamalikaChaudhuriandRuslanSalakhutdinov,editors,Proceedings
ofthe36thInternationalConferenceonMachineLearning,volume97ofProceedingsofMachine
LearningResearch,pages1282–1289.PMLR,09–15Jun2019. URLhttps://proceedings.
mlr.press/v97/cobbe19a.html.
JunTani. Dialogue: Exploringroboticmindsbypredictivecodingprinciple. IEEECDSNewsletter:
CognitiveandDevelopmentalSystems,14(1):4–13,2017.
DanijarHafner,TimothyLillicrap,JimmyBa,andMohammadNorouzi. Dreamtocontrol: Learning
behaviors by latent imagination. arXiv preprint arXiv:1912.01603, Dec 2019b. URL http:
//arxiv.org/abs/1912.01603v3.
DiederikP.KingmaandJimmyBa. Adam: Amethodforstochasticoptimization. arXivpreprint
arXiv:1412.6980,2014.
RazvanPascanu,TomasMikolov,andYoshuaBengio. Onthedifficultyoftrainingrecurrentneural
networks. InInternationalconferenceonmachinelearning,pages1310–1318.PMLR,2013.
Steven J Nowlan and Geoffrey E Hinton. Simplifying neural networks by
soft weight sharing. In The Mathematics of Generalization, pages 373–
394. CRC Press, 2018. doi: 10.1201/9780429492525-13. URL https:
//www.taylorfrancis.com/chapters/edit/10.1201/9780429492525-13/
simplifying-neural-networks-soft-weight-sharing-steven-nowlan-geoffrey-hinton.
21
Pieter-TjerkDeBoer, DirkPKroese, ShieMannor, andReuvenYRubinstein. Atutorialonthe
cross-entropymethod. Annalsofoperationsresearch,134(1):19–67,2005.
TingwuWangandJimmyBa. Exploringmodel-basedplanningwithpolicynetworks. arXivpreprint
arXiv:1906.08649,2019.
Appendix
A Modelandtraininghyperparametersanddetails
Unlessnotedotherwise,weusethehyperparametersspecifiedinthissection. Visualinputvconsists
of 11 11 pixels with the number of channels depending on the experiment. We obtain it by
×
rasterizationofa0.5 0.5squareoftheenvironment,centeringandexcludingtheagent. Duetothe
×
maximumvelocityof0.23unitspertimestep,theagent’snextpositionisalwayswithinit’svisual
field. Eachchannelcorrespondstoaproperty(obstacle,fogterrain,forcefieldupanddown)ofthe
environment. Thepresenceofapropertyisencodedwith1s,whiletherestofthetensorissetto0.
Thevisionmodelv isgivenbyaCNN.Itconsistsofaconvolutionallayer,amaxpoolinglayer,and
M
anotherconvolutionallayerfollowedbyafullyconnectedlayer. Theconvolutionallayershavekernel
size3 3withstride1,nopadding,4channelsifdim(c)<32,and8channelsifdim(c)=32. The
×
maxpoolinglayerhasareceptivefieldsizeof3 3withstride2. Thefullyconnectedlayerhassize
×
8ifdim(c)<5,16if5<dim(c) 16,and32ifdim(c)=32. Weusethetanhactivationfunction
≤
inalllayers. Thevisionmodelhas
564+dim(i) 36+dim(c) 9,ifdim(c)<5
· ·
964+dim(i) 36+dim(c) 17,if5<dim(c) 16
· · ≤
4312+dim(i) 72,ifdim(c)=32
·
parameters in total, where dim(i) denotes the number of channels of the input. We use Adam
[Kingma and Ba, 2014] as our optimizer with learning rate 0.00075, β-values (0.9,0.999), and
(cid:15) = 1e 4. In Experiment V, we use a 10th of the learning rate. We perform gradient clipping
−
[Pascanuetal.,2013]andsetthemaximumnormto2.
The transition model t is given by a MLP. The first fully connected layer has hidden size 32
M
withbiasesturnedoff. Itisfollowedbythetanhactivationfunction. Thefirstofthetwoparallel
fullyconnectedlayerspredictsmeanvectorswiththelinearactivationfunction. Asecondparallel
fullyconnectedlayerpredictsvectorsofstandarddeviationsviatheexponentialactivationfunction,
providingnon-negativevaluesandthereforeensuringvalidstandarddeviations. Fromaprobabilistic
point of view, under the assumption that the values before the activation function are uniformly
distributed,thesemappingsimplementanuninformativepriorinaBayesianframework[Nowlanand
Hinton,2018]. Thechangesinpositionarescaledupbyaconstantfactorof4beforefeedingthem
intot ,suchthatitreceivesinputswhichapproximatelycovertheintervalbetween 1and1. The
M
−
transitionmodelhas
324+dim(c) 32
·
parametersintotal. WeuseAdam[KingmaandBa,2014]asouroptimizerwithlearningrate0.008,
β-values (0.9,0.999), and (cid:15) = 1e 4. In Experiment V, we use a 10th of the learning rate. We
−
performgradientclipping[Pascanuetal.,2013]andsetthemaximumnormto1.2.
Wegeneratetrainingdatabysendingrandomlygeneratedactionstotheenvironment. Actionswere
generatedinawaythatensuresgoodcoverageofthewholeenvironment. Foreachenvironment
usedinourexperiments,wegather200sequencesofsensor-action-tuples. Weuse160sequences
for training and 40 for validation. Each sequence has a length of 300 time steps. We train both
componentsjointlyend-to-endwithbatchsize10for50epochsinExperimentsI-IVandfor500
epochsinExperimentV.Webackpropagatetheerrorthroughtimeevery50timestepsandresetthe
hiddenstatesevery7batches. Thiswaywetrainthemodeltoavoidexplodinghiddenstatesalso
duringgoal-directedcontrol.
22
B Detailsonplanningalgorithms
Whenplanningwithgradient-basedactiveinference,weapplythefollowingadjustmentstoimprove
performance. Firstly,ifanoptimizationcycleincreasesEFE,weperformearlystoppingandusethe
policyfromthecyclebefore. Secondly,wedecreasethelearningrateexponentiallyoverthepolicy
fromthefuturetothepresent. Thisleadstomorestablepathssinceactionswhichlieinthelater
futureareadaptedmorethanactionstobeexecutedinthenearerfuture. Moreprecisely,givena
meanlearningrateαanddecayγ,wesetthelearningrateforactionat+τ to:
γP−τ
α =α
at+τ · (cid:80)P γP−τ
τ
SeeAppendixEforadescriptionofhowtocomputegradientswhentheobjectiveisgivenbytheFE
betweentwomultivariatenormaldistributions. Aftereachupdate,weclampthepolicytobeinthe
correctvaluerange. Finally,afteroptimization,weshiftthepolicywhilecopyingthelastelement.
Weusestochasticgradientdescentwithlearningrate0.005,settheexponentiallearningratedecayto
γ =0.9,andperform50optimizationcycles. Ifapolicyupdateleadstoworseperformance,westop
theoptimizationandusethepolicyfrombefore.
During evolutionary-based planning, we use normal distributions to model actions. In order to
improveperformance,weapplythefollowingmodifications. Weuseamomentumtermonthemeans
andcovariances[DeBoeretal.,2005]. Afterasingleoptimizationiteration,wekeepafixednumber
K oftheelitesforthenextiteration[Pinnerietal.,2020]. Afteroptimization,wedonotdiscardthe
meansbutshiftthem[WangandBa,2019,Chuaetal.,2018]whilecopyingthelastactioninorderto
notstartfromscratchinthenextoptimization.Weresetthevariances,however,toavoidlocalminima.
Analogously,weshifttheelitesthatwekeep[Pinnerietal.,2020]. Weusethefirstactionfromthe
bestsampledpolicyastheoptimizationresult[Pinnerietal.,2020]. Insteadofclippingsampled
actions,weperformrejectionsamplingandsampleuntilwehaveanactionwithintheallowedvalue
range. Wegenerate50trajectorycandidates, use5elitesforparametersestimation, keepK = 2
elitesforthenextoptimizationcycle,useaninitialcovarianceof0.5,andamomentumof0.1.
C AffordancemapsfromexperimentIIIafterdifferentamountsofepochs
Here, weshowaffordancemapsfromExperimentIII(Subsection4.5)afterdifferentamountsof
epochs. InFigure12weseethatwithincreasingamountsoftrainingepochs,theupperandlower
obstacles get encoded more similarly, the additional meaningless information gets more filtered
out, and the affordancemaps get more distinctive regarding the encodingof different behavioral
possibilities.
D Derivativeofnegativelog-likelihoodinanormaldistribution
Inthissectionwederivethenegativelog-likelihoodinamultivariatenormaldistributionwithrespect
tothedistribution’sparameters.
Thelikelihoodinamultivariatenormaldistributionisgivenbyitsprobabilitydensityfunction:
=p(x µ,Σ)
L |
= (x µ,Σ)
N | (10)
1
= (cid:112) e− 2 1(x−µ)TΣ−1(x−µ)
(2π)k Σ ·
| |
Thisleadstothefollowinglog-likelihood:
(cid:113) 1
=log1 log (2π)k Σ (x µ)TΣ−1(x µ)
LL − | |− 2 − −
(11)
1
= (k log(2π)+log Σ +(x µ)TΣ−1(x µ))
−2 · | | − −
Now, we take the derivative of the log-likelihood function with respect to the parameters of our
probabilitydistribution. Theresultingquantityisalsoreferredtoasthescore. Westartbycalculating
23
(A)Affordancemapafter1epochs (B)Affordancemapafter2epochs
2.0 2.0
1.5 1.5
1.0 1.0
0.5 0.5
0.0 0.0
1.5 1.0 0.5 0.0 0.5 1.0 1.5 1.5 1.0 0.5 0.0 0.5 1.0 1.5
− − − − − −
(C)Affordancemapafter5epochs (D)Affordancemapafter10epochs
2.0 2.0
1.5 1.5
1.0 1.0
0.5 0.5
0.0 0.0
1.5 1.0 0.5 0.0 0.5 1.0 1.5 1.5 1.0 0.5 0.0 0.5 1.0 1.5
− − − − − −
(E)Affordancemapafter30epochs (F)Affordancemapafter100epochs
2.0 2.0
1.5 1.5
1.0 1.0
0.5 0.5
0.0 0.0
1.5 1.0 0.5 0.0 0.5 1.0 1.5 1.5 1.0 0.5 0.0 0.5 1.0 1.5
− − − − − −
Figure12: ExemplaryaffordancemapsfromExperimentIII(Subsection4.5)forcontextsize8after
differentamountsofepochs. Togeneratethesemaps,weprobedtheenvironmentalmapatevery
sensiblelocation,appliedthevisionmodeltoeachoutput,performeddimensionalityreductionto3
viaPCA,andinterpretedtheresultsasRGBvalues.
thederivativewithrespecttothemeanµ:
∂ ∂ 1
LL = (k log(2π)+log Σ +(x µ)TΣ−1(x µ))
∂µ ∂µ − 2 · | | − −
1 ∂(x µ)TΣ−1(x µ) (12)
= ( − − )
−2 ∂µ
=Σ−1(x µ)
−
24
Next, we calculate the derivative with respect to the covariance matrix Σ. We assume Σ to be
symmetric:
∂ ∂ 1
LL = (k log(2π)+log Σ +(x µ)TΣ−1(x µ))
∂Σ ∂Σ − 2 · | | − −
1 ∂log Σ ∂(x µ)TΣ−1(x µ)
= ( | | + − − ) (13)
−2 ∂Σ ∂Σ
1
= (Σ−1 Σ−1(x µ)(x µ)TΣ−1)
−2 − − −
Wearenowabletocalculatethederivativesofthelog-likehoodfunctionofamultivariatenormal
distribution. Inthiswork,weappliedtwosimplifications:First,weusedthespecialcaseofabivariate
normaldistribution. Second,weassumeallcovariancestobe0,leadingtoadiagonalcovariance
matrix. With these assumptions, the multivariate normal distribution factors into two univariate
normaldistributions. WereplacethecovariancematrixΣwithavectorofvariancesσ2andendup
withthefollowingscore:
∂ x µ
i i
LL = −
∂µ σ2
i i (14)
∂ 1 1
LL = ( (x µ )2 1)
∂σ2 2σ2 σ2 i − i −
i i i
E Derivativeoffreeenergybetweennormaldistributions
Inthissectionwederivetheexpectedfreeenergyasusedinthisworkbetweentwomultivariate
normal distributions with respect to the parameters of one of the distributions. We first take the
derivativeoftheentropyandsubsequentlyofthedivergenceterm.
Theentropyofamultivariatenormaldistributionisgivenby:
1
H[p(xµ,Σ)]= log 2πeΣ
| 2 | |
(15)
1
= (log(2πe)k+log Σ)
2 | |
Nowwetakethederivativewithrespecttothemeanµ:
∂ ∂ 1
H[p(x µ,Σ)]= (log(2πe)k+log Σ)
∂µ | ∂µ2 | | (16)
=0
Next,wetakethederivativewithrespecttothecovariancematrixΣ(assumingΣtobesymmetric):
∂ ∂ 1
H[p(x µ,Σ)]= (log(2πe)k+log Σ)
∂Σ | ∂Σ2 | |
1∂log Σ
= | | (17)
2 ∂Σ
1
= Σ−1
2
We are now able to calculate the derivative of the entropy of a multivariate normal distribution.
Followingthesimplificationsfromabove(SectionD),weagainreplacethecovariancematrixΣwith
avectorofvariancesσ2andendupwiththefollowinggradients:
∂H[p(x µ,σ)]
| =0
∂µ
i (18)
∂H[p(x µ,σ)] 1
| = σ−2
∂σ2 2 i
i
25
TheKullback-Leiblerdivergencebetweentomultivariatenormaldistributionsisgivenby:
1
D[p(x µ ,Σ ) p(x µ ,Σ )]= (tr(Σ−1Σ )
0 | 0 0 || 1 | 1 1 2 1 0
+(µ 1 − µ 0 )TΣ− 1 1(µ 1 − µ 0 ) (19)
Σ
1
k log| |)
− − Σ
0
| |
Wefirsttakethederivativewithrespecttothemeanofthefirstdistributionµ :
0
∂ ∂ 1
D[p(x µ ,Σ ) p(x µ ,Σ )]= (µ µ )TΣ−1(µ µ )
∂µ 0 0 | 0 0 || 1 | 1 1 ∂µ 0 2 1 − 0 1 1 − 0 (20)
= Σ−1(µ µ )
− 1 1 − 0
NowwetakethederivativewithrespecttothecovariancematrixofthefirstdistributionΣ (assuming
0
Σ andΣ tobesymmetric):
0 1
∂ ∂ 1 Σ
D[p(x µ ,Σ ) p(x µ ,Σ )]= (tr(Σ−1Σ )+log| 1 |)
∂Σ 0 | 0 0 || 1 | 1 1 ∂Σ 2 1 0 Σ
0 0 | 0 | (21)
1
= (Σ−1 Σ−1)
2 1 − 0
WearenowabletocalculatethegradientsoftheKullback-Leiblerdivergencebetweentwomultivari-
atenormaldistributions. Followingthesimplificationsfromabove,weagainreplacethecovariance
matricesΣ andΣ withvectorsofvariancesσ2andσ2andendupwiththefollowinggradients:
0 1 0 1
∂D[p(x µ ,Σ ) p(x µ ,Σ )]
0 | 0 0 || 1 | 1 1 = σ−2(µ µ )
∂µ − 1,i 1,i − 0,i
0,i
(22)
∂D[p(x µ ,Σ ) p(x µ ,Σ )] 1
0 | 0 0 || 1 | 1 1 = (σ−2 σ−2)
∂σ2 2 1,i − 0,i
0,i
F Relationshipbetweennegativelog-likelihoodandKullback-Leibler
divergence
Inthiswork,wetrainedourarchitecturewiththenegativelog-likelihoodasthelossbutperformed
goal-directedcontrolviaEFEminimizationwhichincludestheKullback-Leiblerdivergence. Here,
weshowtherelationshipbetweenthenegativelog-likelihoodandtheKullback-Leiblerdivergencein
general.
TheKullback-Leiblerdivergencebetweentwoprobabilitydistributionspandqisdefinedas
(cid:20) (cid:21)
p(x)
D[p(x) q(x)]=E log
||
x∼p(x)
q(x)
(23)
=E [logp(x) logq(x)]
x∼p(x)
−
=E [logp(x)] E [logq(x)]
x∼p(x) x∼p(x)
−
whereE denotestheexpectedvalue. LetusnowassumethatP describesthedistributionofsome
datawewanttoapproximatewithq. Thelefttermdoesnotdependonqandthereforeisconstant. If
wenowtakeN samplesfromtherealdistributionwithlim weendupwith
N→∞
N
1 (cid:88)
E [logq(x)]= logq(x) (24)
−
x∼p(x)
−N
i
which,uptoaconstantfactor,isthedefinitionofthenegativelog-likelihood.
Weconcludethatminimizingnegativelog-likelihoodisequivalenttominimizingtheKullback-Leibler
divergence.
26

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Inference of Affordances and Active Motor Control in Simulated Agents"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
