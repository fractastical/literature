=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Active Inference and Epistemic Value in Graphical Models
Citation Key: laar2021active
Authors: Thijs van de Laar, Magnus Koudahl, Bart van Erp

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2021

Abstract: The Free Energy Principle (FEP) postulates that biological agents
perceive and interact with their environment in order to minimize a Vari-
ational Free Energy (VFE) with respect to a generative model of their
environment. Theinferenceofapolicy(futurecontrolsequence)according
to the FEP is known as Active Inference (AIF). The AIF literature de-
scribes multiple VFE objectives for policy planning that lead to epistemic
(information-seeking) behavior. However, most objectives have limited
modeling...

Key Terms: energy, environment, eindhoven, cbfe, models, generative, inference, active, value, free

=== FULL PAPER TEXT ===

Active Inference and Epistemic Value in
Graphical Models
Thijs van de Laar1, Magnus Koudahl1,2, Bart van Erp1, and Bert de
Vries1,3
1Department of Electrical Engineering, Eindhoven University of
Technology, Eindhoven, The Netherlands
2Nested Mind Solutions, Liverpool, England
3GN Hearing Benelux BV, Eindhoven, The Netherlands
March 8, 2022
Abstract
The Free Energy Principle (FEP) postulates that biological agents
perceive and interact with their environment in order to minimize a Vari-
ational Free Energy (VFE) with respect to a generative model of their
environment. Theinferenceofapolicy(futurecontrolsequence)according
to the FEP is known as Active Inference (AIF). The AIF literature de-
scribes multiple VFE objectives for policy planning that lead to epistemic
(information-seeking) behavior. However, most objectives have limited
modeling flexibility. This paper approaches epistemic behavior from a
constrained Bethe Free Energy (CBFE) perspective. Crucially, variational
optimizationoftheCBFEcanbeexpressedintermsofmessagepassingon
free-form generative models. The key intuition behind the CBFE is that
weimposeapoint-massconstraintonpredictedoutcomes,whichexplicitly
encodestheassumptionthattheagentwillmakeobservationsinthefuture.
We interpret the CBFE objective in terms of its constituent behavioral
drives. We then illustrate resulting behavior of the CBFE by planning
and interacting with a simulated T-maze environment. Simulations for
the T-maze task illustrate how the CBFE agent exhibits an epistemic
drive, and actively plans ahead to account for the impact of predicted
outcomes. Compared to an EFE agent, the CBFE agent incurs expected
reward in significantly more environmental scenarios. We conclude that
CBFE optimization by message passing suggests a general mechanism for
epistemic-aware AIF in free-form generative models.
Keywords: Free Energy Principle, Active Inference, Variational Optimiza-
tion, Constrained Bethe Free Energy, Message Passing
Thisistheauthor’sversionofthearticlethathasbeenacceptedforpublicationinFrontiersin
RoboticsandAI.
1
2202
raM
7
]LM.tats[
2v14500.9012:viXra
1 Introduction
Free energy can be considered as a central concept in the natural sciences. Many
natural laws can be derived through the principle of least action1, which rests on
variational methods to minimize a path integral of free energy over time [1]. In
neuroscience, an application of the least action principle to biological behavior
is formalized as the Free Energy Principle [2]. The Free Energy Principle (FEP)
postulates that biological agents perceive and interact with their environment in
order to minimize a Variational Free Energy (VFE) that is defined with respect
to a model of their environment.
Under the FEP, perception relates to the process of hidden state estimation,
wheretheagenttriestoinferhiddencausesofitssensoryobservations;andaction
(intervention) relates to a process where the agent actively tries to influence
its (predicted) future observations by manipulating the external environment.
Becausethefutureisunobserved(bydefinition), theagentincludespriorbeliefs2
aboutdesiredoutcomesinitsmodelandinfersapolicythatprescribesasequence
of future controls3. The corollary of the FEP that includes action is referred to
as Active Inference (AIF) [3].
The AIF literature describes multiple Free Energy (FE) objectives for policy
planning, e.g., the Expected FE [4], Generalized FE [5] and Predicted (Bethe)
FE[6](amongothers, seee.g. [7,8,9]). Traditionally, theExpectedFreeEnergy
(EFE) is evaluated for a selection of policies, and a posterior distribution over
policies is constructed from the corresponding EFEs. The EFE is designed to
balanceepistemic(knowledgeseeking)andextrinsic(goalseeking)behavior. The
active policy (the sequence of future controls to be executed in the environment)
is then selected from this policy posterior [4].
Several authors have attempted to formulate minimization of the EFE by
message passing on factor graphs [10, 5, 11, 12]. These formulations evaluate
the EFE objective with the use of a message passing scheme. In this paper we
revisit this problem and compare the EFE approach with the message passing
interpretation of the variational optimization of a Bethe Free Energy (BFE)
[13, 14, 15]. However, the BFE is known to lack epistemic (information-seeking)
qualities, and resulting BFE AIF agents therefore do not pro-actively seek
informative states [6].
As a solution to the lack of epistemic qualities of the BFE, in this paper
we approach epistemic behavior from a Constrained BFE (CBFE) perspective
[16]. We illustrate how optimization of a point-mass constrained BFE objective
instigates self-evidencing behavior. Crucially, variational optimization of the
CBFEcanbeexpressedintermsofmessagepassingonagraphicalrepresentation
of the underlying generative model (GM) [17, 18], without modification of the
GM itself. The contributions of this paper are as follows:
1Inthiscontext,“action”referstothepathintegral,andisdistinctfrom“action”inthe
contextofanintervention.
2Wewilluse“belief”and“distribution”interchangeably.
3We use “controls” refer to quantities in the generative model, and “actions” (in the
interventionsense)torefertoquantitiesintheexternalenvironment.
2
• WeformulatetheCBFEasanobjectiveforepistemic-awareactiveinference
(Sec. 2.6) that can be interpreted as message passing on a GM (Sec. 6);
• We interpret the constituent terms of the CBFE objective as drivers for
behavior (Sec. 4);
• We illustrate our interpretation of the CBFE by planning and interacting
with a simulated T-maze environment (Sec. 5).
• Simulations show that the CBFE agent plans epistemic policies multiple
time-steps ahead (Sec. 6.2), and accrues reward for a significantly larger
set of scenarios than the EFE (Sec. 7).
The main advantage of AIF with the CBFE objective, is that it allows
inferencetobefullyautomatedbymessagepassing,whileretainingtheepistemic
qualities of the EFE. Automated message passing absolves the need for manual
derivationsandremovescomputationalbarriersinscalingAIFtomoredemanding
settings [19].
2 Problem Statement
In this section we will introduce the free energy objectives as used throughout
the paper. We start by introducing the Variational Free Energy (VFE), and
explain how a VFE can be employed in an AIF context for perception and
policy planning. We then introduce the Expected Free Energy (EFE) as a
variational objective that is explicitly designed to yield epistemic behavior in
AIF agents, but also note that the EFE definition limits itself to (hierarchical)
state-space models. We then introduce the Bethe Free Energy (BFE), and argue
that theBFE allowsfor convenientoptimization on free-formmodelsbymessage
passing, but note that the BFE lacks information-seeking qualities. We conclude
this section by introducing the Constrained Bethe Free Energy (CBFE), which
equips the BFE with information-seeking qualities on free-form models through
additional constraints on the variational distribution.
Table 1 summarizes notational conventions throughout the paper.
2.1 Variational Free Energy
The Variational Free Energy (VFE) is a principled metric in physics, where a
time-integral over free energy is known as the action functional. Many natural
lawscanbederivedfromtheprincipleofleastaction,wheretheactionfunctional
is minimized with the use of variational calculus [1, 20].
3
Table 1: Summary of notational conventions throughout the paper.
Notation Def. Explanation
s Collection of (arbitrary) model variables
s Individual model variable with index j ∈S
j
f(s) (1) Factorized model of variables s
f (s ) (1) Factor (conditional or prior probability distribution) with argument variables
a a
s and index a∈F
a
q(s) (2) Variational distribution of (latent) variables s
U [f(s)] (2) Average energy
q(s)
H[q(s)] (2) Entropy
F[q] (2) Variational Free Energy
y , x , u Observation, state and control variable (at time k) respectively
k k k
yˆ Specific realization for observation or unobserved future (predicted) outcome
k
uˆ Specific control realization
k
p(y ,x |x ,u ) (7) Generative Model engine
k k k−1 k
p(y |x ) (7) Observation model
k k
p(x |x ,u ) (7) Transition model
k k−1 k
p(x ) (8) State prior
t−1
y, x, u Sequence of future observation variables y , state variables x
t:t+T−1 t−1:t+T−1
and control variables u , respectively
t:t+T−1
uˆj Policy (sequence of specific future controls), uˆj ∈C, where index j is usually
omitted
F∗(uˆj) (12) Optimized Variational Free Energy for policy uˆj
uˆ∗ (13) Optimal policy uˆ∗ ∈C
G[q;uˆj] (14) Expected Free Energy (EFE)
p(y|x) (16a) Aggregate observation model
p(x|u) (16b) Aggregate state transition model, including state prior
p˜(y) (18) Goal prior for sequence of future observation variables
p˜(y ) (18) Goal prior for observation variable at time k
k
f(y,x|u) (19), (25) Factorized model of future variables (at time t), for EFE and (C)BFE respec-
tively
B[q] (23) Bethe Free Energy (BFE)
H [q] (24) Bethe entropy
B
B[q;uˆj] (26) Bethe Free Energy of future model under policy uˆj
B[q;uˆj,yˆ] (28) Constrained Bethe Free Energy (CBFE) of future model under policy uˆj and
predicted outcomes yˆ
4
The VFE is defined with respect to a factorized generative model (GM).
We consider a GM f(s) with factors {f |a∈F} and variables {s |i∈S} that
a i
factorizes according to
(cid:89)
f(s)= f (s ), (1)
a a
a∈F
where s collects the argument variables of the factors f . As a notational
a a
convention, we write collections and sequences in bold script. In the model fac-
torization of (1), the factors f would correspond with the prior and conditional
a
probability distributions that define the GM. The VFE is then defined as a
functional of an (approximate) posterior q(s) over latent variables, as
(cid:20) (cid:21)
q(s)
F[q]=E log
q(s) f(s)
=U [f(s)]−H[q(s)] , (2)
q(s)
whichconsistsofanaverageenergyU [f(s)]=−E [logf(s)]andanentropy
q(s) q(s)
H[q(s)]=−E [logq(s)].
q(s)
Because the VFE is (usually) optimized with respect to the posterior q with
the use of variational calculus [14], the posterior q is also referred to as the
variational distribution. In this paper, we will strictly reserve the q notation for
variational distributions.
We can relate the exact posterior belief with the model definition through a
normalizing constant Z, as
f(s)
p(s)= , (3)
Z
where
(cid:88)
Z = f(s). (4)
s
Throughout this paper, summation can be replaced by integration in the case of
continuous variables.
In a Bayesian context, the normalizer Z is commonly referred to as the
marginal likelihood or evidence for model f. However, exact summation
(marginalization)of (4)overallvariablerealizationsisoftenprohibitivelydifficult
in practice, so that the evidence and exact posterior become unobtainable.
Substituting (3) in the VFE (2) expresses the VFE as an upper bound on
the surprise, that is the negative log-model evidence, as
F[q]= KL[q(s)(cid:107)p(s)] −logZ . (5)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
posteriordivergence surprise
Themarginalizationproblemof (4)isthusconvertedtoanoptimizationproblem
over q. After optimization,
q∗ =argminF[q], (6)
q
5
the VFE approximates the surprise, and the optimal variational distribution
becomes an approximation to the true posterior, p(s)≈q∗(s).
Crucially, we are free to choose constraints on q such that the optimization
becomes practically feasible, at the cost of an increased posterior divergence.
One such approximation is the Bethe assumption, as we will see in Sec. 2.5.
2.2 Inference for Perception
Wenowreturntothemodeldefinitionof (1). InthecontextofAIF,aGenerative
Model (GM) comprises of a probability distribution over states x , observations
k
y and controls u , at each time index k. We will use a hat to indicate specific
k k
variable realizations, i.e. yˆ for a specific outcome and uˆ for a specific control
k k
attimek. Asanotationalconvention, weusek asanarbitrarytimeindex, often
used in the context of iterations, and we use t to indicate the current simulation
time index.
We define a state-space model [21] for the generative model engine, which
represents our belief about how observations follow from a given control and
previous state, as
p(y ,x |x ,u )= p(y |x ) p(x |x ,u ) . (7)
k k k−1 k k k k k−1 k
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
observation transition
model model
We use a prior belief about past states p(x ) together with the generative
t−1
model engine (7) to define a generative model for perception
f(y ,x ,x |u )=p(x )p(y ,x |x ,u )
t t t−1 t t−1 t t t−1 t
=p(x )p(y |x )p(x |x ,u ), (8)
t−1 t t t t−1 t
which, after substitution in (2), results in the VFE objective for perception,
F[q]=U [f(yˆ,x ,x |uˆ )]−H[q(x ,x )] . (9)
q(xt,xt−1) t t t−1 t t t−1
At each time t, the process of perception then relates to inferring the opti-
mal variational distribution q∗(x ,x ) about latent states, given the current
t t−1
action uˆ and resulting outcome yˆ. The resulting variational distribution can
t t
then be used as a prior for the next time-step, such that p(x ) (cid:44) q∗(x ) =
t t
(cid:80) q∗(x ,x ).
xt−1 t t−1
2.3 Inference for Planning
At each time t, planning is concerned with selecting optimal future controls by
minimizing a Free Energy (FE) objective that is defined with respect to future
variables. We write y = y , x = x , and u = u as the
t:t+T−1 t−1:t+T−1 t:t+T−1
sequencesoffutureobservations,statesandcontrolsrespectively,forafixed-time
horizon of T time-steps ahead.
We will refer to a specific future control sequence uˆ as a policy. The optimal
policy uˆ∗ is then referred to as the active policy, where (local) optimality is
6
indicated by an asterisk. Inference for planning then aims to select the optimal
policy (in terms of FE) from a collection of candidate policies uˆj ∈ C, where
C represents the finite set of all (user-provided) candidate policies, and j the
selected policy index.
When we view the candidate policy uˆj as a model selection variable, the
problem of policy selection becomes equivalent to the problem of Bayesian
model selection, where we wish to find a probabilistic model with the highest
posterior probability among some given candidate models. When there is no
prior preference about models, the optimal model is the one with the highest
marginal likelihood (evidence).
Given a model f(y,x|u) of future observations and states given a future
control sequence, we can express the marginal likelihood (evidence) for a specific
policy choice, as
(cid:88)(cid:88)
Z = f(y,x|uˆj). (10)
j
y x
Using (3), we can then relate the exact posterior belief with the variational
distribution and the policy evidence, as
f(y,x|uˆj)
p(y,x|uˆj)= . (11)
Z
j
Under optimization of q, the minimal VFE then approximates the surprise (5),
as
F∗(uˆj)=minF[q;uˆj]≥−logZ . (12)
j
q
The optimal policy then minimizes the optimized VFE, as
uˆ∗ =argminF∗(uˆj). (13)
uˆj∈C
In the following, we omit the explicit indexing of the policy on j for notational
convenience, and simply write uˆ to represent a specific policy choice.
Because the VFE (2) involves expectations over the full joint variational
distribution,itmaybecomeprohibitivelyexpensivetocomputeforlargermodels.
Therefore,additionalassumptionsandconstraintsontheVFEareoftenrequired.
Asaresult,multiplefreeenergyobjectivesforpolicyplanninghavebeenproposed
in the literature, e.g., the Expected Free Energy (EFE) [4, 22], the Free Energy
of the Expected Future [23], the Generalized Free Energy [5], the Predicted
(Bethe) Free Energy [6], and marginal approximations [11].
2.4 Expected Free Energy
The Expected Free Energy (EFE) is an FE objective for planning that is
explicitly constructed to elicit information-seeking behavior [4]. Because future
7
observations are (by definition) unknown, the EFE is defined in terms of an
expectation that includes observation variables, as
(cid:20) (cid:21)
q(x)
G[q;uˆ]=E log . (14)
q(y,x) f(y,x|uˆ)
Construction of the (Markovian) model for the EFE starts by stringing
together a state prior with the generative model engine of (7) for future times,
as
t+T−1
(cid:89)
p(y,x|u)=p(x ) p(y ,x |x ,u )
t−1 k k k−1 k
k=t
t+T−1
(cid:89)
=p(x ) p(y |x )p(x |x ,u ), (15)
t−1 k k k k−1 k
k=t
where the state prior p(x ) follows from the perceptual process (Sec. 2.2).
t−1
For notational convenience, we often group the observation and state transition
models (including the state prior), according to
t+T−1
(cid:89)
p(y|x)= p(y |x ) (16a)
k k
k=t
t+T−1
(cid:89)
p(x|u)=p(x ) p(x |x ,u ). (16b)
t−1 k k−1 k
k=t
From the future generative model engine (15), the EFE defines a state
posterior
p(y,x|u)
p(x|y,u)= . (17)
(cid:80)
p(y,x|u)
x
Note that our notation differs from [4], where posterior distributions are denoted
by q. We strictly reserve the q notation for variational distributions.
Weintroducegoalpriorsp˜(y )overobservationvariables. Goalpriorsencode
k
prior beliefs about desired observations (or states) [24], and are annotated with
a tilde to avoid confusion with the marginal distribution over the same variable.
We then introduce a shorthand notation that aggregates (independent) goal
priors for the future generative model, as
t+T−1
(cid:89)
p˜(y)= p˜(y ). (18)
k
k=t
Together with the aggregated goal prior, the factorized model for the EFE is
then constructed as
f(y,x|u)=p(x|y,u)p˜(y). (19)
There are several things of note about the model of (19):
8
• The model includes variables that pertain to future time-points, t≤k ≤
t+T −1. As a result, the future observation variables y are latent;
• The model includes a state prior that is a result of inference for perception;
• The (informative) goal priors p˜ introduce a bias in the model towards
desired outcomes;
• Candidate policies will be given, as indicated by a conditioning on controls.
Upon substitution of (19), the EFE (14) factorizes into an epistemic and an
extrinsic value term [4], as
(cid:20) (cid:21)
p(x|y,uˆ)
G[q;uˆ]=−E log −E [logp˜(y)], (20)
q(y,x) q(x) q(y)
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125) extrinsicvalue
epistemicvalue
where the epistemic value relates to a mutual information between states and
observations. Thisdecompositionisoftenusedtomotivatetheepistemicqualities
of the EFE.
An alternative decomposition, in terms of ambiguity and observation risk,
can be obtained under the assumptions q(y|x) ≈ p(y|x) (approximation of
the observation model), and q(x|y) ≈ p(x|y,u) (approximation of the ex-
act posterior). These assumptions allow us to rewrite the exact relationship
q(y,x) = q(y|x)q(x) = q(x|y)q(y) in terms of the approximations q(y,x) ≈
p(y|x)q(x)≈p(x|y,u)q(y). As a result, we obtain
(cid:20) (cid:21)
q(y)
G[q;uˆ]≈E log
q(y,x) p(y|x)p˜(y)
≈E [H[p(y|x)]]+KL[q(y)(cid:107)p˜(y)], (21)
q(x)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ambiguity observationrisk
where q(x) and q(y) on the r.h.s. are implicitly conditioned on uˆ. This decom-
position is often used to motivate the explorative (ambiguity reducing) qualities
of the EFE.
In the current paper we evaluate the EFE in accordance with [4, 25], for
which the procedure is detailed in Appendix A.
Although the EFE leads to epistemic behavior, it does not fit the general
functional form of the VFE (2), where the expectation and numerator define the
samevariationaldistribution. Asaresult,EFEminimizationbymessagepassing
requires custom definitions, and limits itself to (hierarchical) state-space models.
Furthermore, note that the EFE involves the state posterior p(x|y,u) as part
of its definition, which is technically a quantity that needs to be inferred. The
EFE thus conflates the definition of the planning objective with the inference
procedure for planning itself.
9
2.5 Bethe Free Energy
The Bethe Free Energy (BFE) defines a variational distribution that factorizes
according to the Bethe assumption
(cid:89) (cid:89)
q(s)(cid:44) q (s ) q (s )1−di, (22)
a a i i
a∈F i∈S
where the degree d counts how many q ’s contain s as an argument. After
i a i
substituting the Bethe assumption (22) in the VFE (2), we obtain the BFE,
(cid:88) (cid:88) (cid:88)
B[q]= U [f (s )]− H[q (s )]− (1−d )H[q (s )] , (23)
qa(sa) a a a a i i i
a∈F a∈F i∈S
as a special case of the VFE. The entropy contributions are often summarized
in the Bethe entropy, as
(cid:88) (cid:88)
H [q]= H[q (s )]+ (1−d )H[q (s )] . (24)
B a a i i i
a∈F i∈S
Because the BFE fully factorizes into local contributions in F and S, it can
be optimized by message passing on the generative model [15, 14, 26, 16]. In
the context of AIF, the BFE for a model over future states is also referred to as
the Predicted Free Energy [6].
For a fixed time-horizon T, the factorized model for future states is con-
structed from the generative model engine and goal prior, as
f(y,x|u)=p(y,x|u)p˜(y)
t+T−1
(cid:89)
=p(x ) p(y ,x |x ,u )p˜(y )
t−1 k k k−1 k k
k=t
t+T−1
(cid:89)
=p(x ) p(y |x )p(x |x ,u )p˜(y ). (25)
t−1 k k k k−1 k k
k=t
Because the generative model engine and goal priors introduce a simultaneous
constraintonfutureobservations,themodelof(25)representsascaledprobability
distribution. The BFE of the future model under policy uˆ then becomes
B[q;uˆ]=U [f(y,x|u)]−H [q(y,x)] . (26)
q(y,x) B
A major advantage of the BFE over the EFE as an objective for AIF is
that message passing implementations can be automatically derived on free
form models, thus greatly enhancing model flexibility. A drawback of the BFE,
however, is that it lacks the epistemic qualities of the EFE [6], see also Sec. 4.
2.6 Constrained Bethe Free Energy
The Constrained Bethe Free Energy (CBFE) that we propose in this paper
combines the epistemic qualities of the EFE with the computational ease and
10
model flexibility of the BFE. The CBFE can be derived from the BFE by
imposing additional constraints on the variational distribution
q(y,x)(cid:44)q(x)δ(y−yˆ)
t+T−1
(cid:89)
=q(x) δ(y −yˆ ), (27)
k k
k=t
where δ(·) defines the appropriate (Kronecker or Dirac) delta function for the
domain of the observation variable y (discrete or continuous). The point-mass
k
(delta) constraints of the CBFE are motivated by the following key insight:
although the future is unknown, we know that we will observe something in the
future. However, because future outcomes are by definition unobserved, the yˆ
k
encode potential outcomes that need to be optimized for.
For the model of (25), the CBFE then becomes4
B[q;uˆ,yˆ]=U [f(y,x|uˆ)]−H [q(x)δ(y−yˆ)]
q(x)δ(y−yˆ) B
=U [f(yˆ,x|uˆ)]−H [q(x)] . (28)
q(x) B
The current paper investigates how point-mass constraints of the form (27)
affect epistemic behavior in AIF agents.
3 Methods
To minimize the (Constrained) Bethe Free Energy, the current paper uses
message passing on a Forney-style factor graph (FFG) representation [27] of the
factorized model (1). In an FFG, edges represent variables and nodes represent
the functional relationships between variables (i.e. the prior and conditional
probabilities).
Especially in a signal processing and control context, the FFG paradigm
leads to convenient message passing formulations [28, 29]. Namely, inference can
be described in terms of messages that summarize and propagate information
across the FFG. The BFE is well-known for being the fundamental objective of
the celebrated sum-product algorithm [15], which has been formulated in terms
of message passing on FFGs [30]. Extensions of the sum-product algorithm
to hybrid formulations, such as variational message passing (VMP) [17] and
expectation maximization (EM) [31] have also been formulated as message
passing on FFGs. More recently, more general hybrid algorithms have been
described in terms of message passing, see e.g. [32, 33]. A comprehensive
overview is provided in [16], where additional constraints, including point-mass
constraints, are imposed on the BFE and optimized for by message passing on
FFGs.
4ForcontinuousvariablesweneedtoadditionallyassumethattheentropyofaDiracdelta
H[δ(·)]=0[16].
11
3.1 Forney-Style Factor Graph Example
Let us consider an example model (1) that factorizes according to
f(s ,s ,s ,s )=f (s )f (s ,s ,s )f (s )f (s ,s ). (29)
1 2 3 4 a 1 b 1 2 3 c 3 d 2 4
The FFG representation of (29) is depicted in Fig. 1 (left).
1 2
s 1 s 2 → s 1 s 2 →
f f f f f f
a b d a ← b ← d
5 6 ↓ 4
s 3 s δ ↑ 3
4 s 3 ↑ 7 s 4
f f
c c
Figure 1: Forney-style factor graph representation for the example model of (29)
(left) and message passing schedule for the Bethe Free Energy minimization of
(30) (right). Shaded messages indicate variational message updates, and the
solid square node indicates given (clamped) values. The round node indicates a
point-mass constraint for which the value is optimized.
Nowsupposeweobserves andintroduceapoint-massconstraintons . The
4 3
variational distribution then factorizes as
q(s ,s ,s )=q(s ,s )q(s )
1 2 3 1 2 3
=q(s ,s )δ(s −sˆ ), (30)
1 2 3 3
where s is excluded from the variational distribution because it is observed and
4
therefore no longer a latent variable. Substituting (29) and (30) in (28) yields
the CBFE as4
B[q;sˆ ]=U [f(s ,s ,sˆ ,sˆ )]−H[q(s ,s )] , (31)
3 q(s1,s2) 1 2 3 4 1 2
where we directly substituted the observed value sˆ into the factorized model.
4
In this paper we adhere to the notation in [16], and indicate point-mass con-
straints by an unshaded round node with an annotated δ on the corresponding
edge of the FFG. A solid square node indicates a given value (e.g., an action,
observed outcome or given parameter), whereas an unshaded round node indi-
cates a point-mass constraint that is optimized for (e.g. a potential outcome).
Unshaded messages indicate sum-product messages [30] and shaded messages
indicate variational messages, as scheduled and computed in accordance with
[17]. The ForneyLab probabilistic programming toolbox [18] implements an
automated message passing scheduler and a lookup table of pre-derived message
updates [28, 34, App. A].
Variational optimization of (31) then yields the (iterative) message passing
schedule of Fig. 1 (right), where sˆ is initialized. After computation of the
3
12
messages, the mode of the product between message 6 and 7 becomes the
new value for sˆ , and the schedule is repeated until convergence. The result-
3
ing optimization procedure then resembles an expectation maximization (EM)
scheme where 6 acts as a likelihood and 7 as a prior [31], and where upon
each iteration the value sˆ is updated with the maximum a-posteriori (MAP)
3
estimate.
4 Value Decompositions
In this section we further investigate the drivers for behavior of the (C)BFE.
We assume that all variational distributions factorize according to the Bethe
assumption (22).
4.1 Confidence and Complexity
We substitute the model of (25) in the CBFE definition of (28) and combine to
identify three terms, as
B[q;yˆ,uˆ]=U [p(y,x|uˆ)]+U [p˜(y)]−H [q(x)δ(y−yˆ)]
q(x)δ(y−yˆ) δ(y−yˆ) B
=U [p(y|x)]−H [δ(y−yˆ)]+U [p(x|uˆ)]−H [q(x)]+U [p˜(y)]
q(x)δ(y−yˆ) B q(x) B δ(y−yˆ)
=E [KL[δ(y−yˆ)(cid:107)p(y|x)]]+KL[q(x)(cid:107)p(x|uˆ)]− logp˜(yˆ) .
q(x)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
negativeconfidence complexity extrinsicvalue
(32)
Table 2 is provided as an overview, and summarizes the properties of the
individual terms of (32) under optimization.
The extrinsic value induces a preference for extrinsically rewarding future
outcomes.
Minimizing complexity prefers policies that induce transitions that are in
line with state beliefs, and (vice versa) prefers state beliefs that remain close the
policy-induced state transitions.
The confidence expresses the expected difference (divergence) in information
between the outcomes as predicted by the observation model and the most
likely (expected) outcome. In other words, this term quantifies the information
difference between predictions and absolute certainty about outcomes. While
the negative confidence term could be interpreted as an ambiguity (deviation
fromcertainty), wechoosethisalternativeterminologytopreventconfusionwith
the ambiguity as defined in (21).
Specifically, the ambiguity (21) and negative confidence (32) are both of
the form4 U [p(y|x)] = −E [logp(y|x)]. However, where the ambigu-
q(y,x) q(y,x)
ity approximates q(y,x) ≈ p(y|x)q(x) (21), the confidence defines q(y,x) (cid:44)
q(x)δ(y−yˆ) (27). As a result, the ambiguity explicitly accounts for the full
spread of p(y|x), whereas the confidence evaluates p(y|x) at the expected maxi-
mum yˆ.
13
Maximizingconfidenceprefersoutcomesthatareinlinewithpredictions,and
simultaneously tries to maximize the precision of state beliefs (Table 2), see also
[35, p. 2093]. Note that all terms act in unison – the precision of state beliefs is
simultaneously influenced by the complexity, which prevents the collapse of the
state belief to a point-mass.
Table 2: Optima for the individual terms of the CBFE decompositions (32),
(34). Each row varies one quantity (variable or function) in their respective term
while other quantities remain fixed. Shaded cells indicate that the term (row) is
not a function(al) of that specific optimization quantity (column).
Vary
Optimize Fix yˆ uˆ q(x)
max ex. val. yˆ that maximizes p˜(yˆ)
q(x) uˆ ∈C that renders p(x|uˆ)
min
closest to q(x)
complexity
uˆ q(x)=p(x|uˆ)
q(x) yˆ that maximizes
expected outcomes4
E [logp(yˆ|x)]
max q(x)
confidence yˆ q(x) = δ(x−xˆ) where xˆ
maximizes the likelihood
p(yˆ|x)
uˆ yˆ that maximizes the
evidence4 p(yˆ|uˆ)
max
intrinsic yˆ uˆ ∈C that renders p(y|uˆ)
value closest to δ(y−yˆ)
q(x), uˆ yˆ that renders p(x|yˆ,uˆ)
closest to q(x)
min q(x), yˆ uˆ ∈ C that renders
posterior
p(x|yˆ,uˆ) closest to q(x)
divergence
yˆ, uˆ q(x)=p(x|yˆ,uˆ)
4.2 Intrinsic and Extrinsic Value
A second decomposition of the CBFE objective follows when we rewrite the
factorized model of (25) using the product rule, as
f(y,x|u)=p(y,x|u)p˜(y)
=p(x|y,u)p(y|u)p˜(y). (33)
14
Substituting (33) in the CBFE definition (28) and combining terms, then
yields
B[q;yˆ,uˆ]=U [p(y,x|uˆ)]+U [p˜(y)]−H [q(x)δ(y−yˆ)]
q(x)δ(y−yˆ) δ(y−yˆ) B
=U [p(x|yˆ,uˆ)]−H [q(x)]+U [p(y|uˆ)]−H [δ(y−yˆ)]+U [p˜(y)]
q(x) B δ(y−yˆ) B δ(y−yˆ)
=KL[q(x)(cid:107)p(x|yˆ,uˆ)]+KL[δ(y−yˆ)(cid:107)p(y|uˆ)]− logp˜(yˆ) . (34)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
posteriordivergence negativeintrinsicvalue extrinsicvalue
Table2againsummarizesthepropertiesoftheindividualtermsof (34)under
optimization.
The second term of (34) expresses the difference (divergence) in information
between the predicted outcomes and the point-mass constrained (expected)
outcome. In contrast to the extrinsic value (third term), this term quantifies a
(negative) intrinsic value that purely depends upon the agent’s intrinsic beliefs
about the environment (state prior and generative model engine (15)). Under
optimization (Table 2), this term prefers policies that lead to precise predictions
for the outcomes.
Theposteriordivergence(firstterm)isalwaysnon-negativeandwilldiminish
under optimization, which allows us to combine (32) and (34) into4
logp(yˆ|uˆ)≥E [logp(yˆ|x)]−KL[q(x)(cid:107)p(x|uˆ)] . (35)
q(x)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
predicted confidence complexity
log-evidence
Interestingly, (35) tells us that the intrinsic value of (34) relates to the model
evidenceaspredictedunderthepolicyandresultingexpectedoutcomes. Inference
for planning with the CBFE then attempts to make precise predictions for
outcomes by maximizing predicted model evidence. In this view, the CBFE
for planning can be considered (quite literally) as self-evidencing [36, 3]. As
a result of selected actions, environmental outcomes may still be surprising
under current generative model assumptions. Inference for perception then
subsequently corrects the generative model priors (Sec. 2.2), and the action-
perception loop repeats (see also Alg. 1). Epistemic qualities then emerge from
this continual pursuit of evidence [37].
Inshort,wenoteadistinctionintheinterpretationofepistemicvaluebetween
the EFE and the CBFE. In the EFE (20), epistemic value is directly related
with a mutual information term between states and outcomes. In the CBFE,
the epistemic drive appears to result from a self-evidencing mechanism.
4.3 Bethe Free Energy Value Decomposition
The BFE does not permit an interpretation in terms of intrinsic value. When
we substitute (33) in the BFE definition of (26) and combine terms, we obtain
B[q;uˆ]=U [p(y,x|uˆ)]+U [p˜(y)]−H [q(y,x)]
q(y,x) q(y) B
=KL[q(y,x)(cid:107)p(y,x|uˆ)]−E [logp˜(y)]
q(y)
15
=E [KL[q(x|y)(cid:107)p(x|y,uˆ)]]+KL[q(y)(cid:107)p(y|uˆ)]− E [logp˜(y)] .
q(y) q(y)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
expectedposteriordivergence predictivedivergence expectedextrinsicvalue
(36)
The intrinsic value term of (34) has been replaced by a predictive divergence
in (36). This term expresses the difference (divergence) in information between
the observations as predicted by the model under policy uˆ, and variational
distribution about outcomes q(y). Under optimization of q(x|y), the posterior
divergence will vanish for all y. Without the additional point-mass constraint
of (27), the predictive divergence then no longer quantifies the information
difference between uncertainty (predictive distribution) and certainty (predicted
outcomes). As a result, the BFE lacks the self-evidencing qualities and resulting
epistemic drive of the CBFE, as we will further illustrate in Sec. 4.4 and 6.
4.4 Example Application
We illustrate our interpretation of (34) and (36) by a minimal example model.
We consider a two-armed bandit, where an agent chooses between two levers,
u ∈ {0,1}. Each lever offers a distinct probability for observing an outcome
y ∈{0,1}. Specifically, choosing uˆ=0 will offer a 0.5 probability for observing
yˆ=0(ignorantpolicy),whereschoosinguˆ=1willalwaysleadtotheobservation
yˆ = 0 (informative policy). We do not equip the agent with any external
preference (there is no extrinsic reward). The agent’s factorized model then
becomes
f(y|u)=p(y =a |u=a )=A , (37)
i j ij
with a=(0,1)T and the conditional probability matrix
(cid:18) (cid:19)
0.5 1
A= . (38)
0.5 0
The BFE then follows as
B[q;uˆ]=U [p(y|uˆ)]−H[q(y)] . (39)
q(y)
The CBFE additionally constrains q(y)=δ(y−yˆ), and as a result corresponds
directly with the negative intrinsic value term of (34), as4
B(yˆ,uˆ)=U [p(y|uˆ)]−H[δ(y−yˆ)]
δ(y−yˆ)
=−logp(yˆ|uˆ). (40)
TheFFGforthemodeldefinitionof (37)togetherwiththeresultingschedule
for optimization of the (C)BFE is drawn in Fig. 2.
The results of Table 3 show that the BFE does not distinguish between
policies. The CBFE however penalizes the ignorant policy (uˆ=0), which does
not predict precise outcomes. This mechanism thus induces a preference for
the informative policy (uˆ = 1), which does predict precise outcomes. In the
following, we will further investigate this behavior in a less trivial setting.
16
↓ u ↓ u
→ →
T p(y|u) T p(y|u)
A A
1 ↓ y 1 ↓ y
δ
Figure 2: Message passing schedule for the example model of (37) for the BFE
(left) and CBFE (right). The dashed box summarizes the observation model.
Table 3: Free energies (in bits) per policy for the example application.
Policy BFE CBFE
Ignorant (uˆ=0) 0 1
Informative (uˆ=1) 0 0
5 Experimental Setting
A classic experimental setting that investigates epistemic behavior is the T-maze
task [4]. The T-maze environment consists of four positions P ={1,2,3,4}, as
drawninFig.3. Theagentstartsinposition1, andaimstoobtainarewardthat
resides in either arm 2 or 3, R={2,3}. The position of the reward is unknown
to the agent a priori, and once the agent enters one of the arms it remains there.
In order to learn the position of the reward, the agent first needs to move
to position 4, where a cue indicates the reward position. At each position, the
agent may observe one of four reward-related outcomes O ={1,2,3,4}:
1. The reward is indicated to reside at location two (left arm);
2. The reward is indicated to reside at location three (right arm);
3. The reward is obtained;
4. The reward is not obtained.
The key insight is that an epistemic policy would first inspect the cue at
position 4 and then move to the indicated arm, whereas a purely goal directed
agent would immediately move towards either of the potential goal positions
instead of visiting the cue.
5.1 Generative Model Specification
Wefollow[4],andassumeagenerativemodelwithdiscretestatesx ,observations
k
y and controls u . The state x ∈P×R, represents the agent position at time
k k k
k (four positions, Fig. 3) combined with the reward position (two possibilities).
The state vector thus comprises of eight possible realizations. The transition
17
2 3
1
4
Figure 3: Layout of the T-maze. The agent starts at position 1. The reward is
located at either position 2 or 3. Position 4 contains a cue which indicates the
reward position.
between states is affected by the control u ∈ P, which encodes the agent’s
k
attempted next position in the maze. The observation variables y ∈ O×P
k
represent the agent position at time k (four positions) in combination with the
reward-related outcome at that position (four possibilities).
The respective state prior, observation model, transition model and goal
priors are defined as
p(x )=Cat(x |d )
t−1 t−1 t−1
p(y =a |x =b )=A
k i k j ij
p(x =b |x =b ,u =uˆ )=(B )
k i k−1 j k k uˆk ij
p˜(y )=Cat(y |c ) ,
k k k
where a ∈O×P, b ∈P ×R, and b ∈P ×R.
i i j
The agent plans two steps ahead (T = 2), for which the FFG is drawn in
Fig. 4.
5.2 Parameter Assignments
We start the simulation at t=1, and assume that the initial position is known,
namely we start at position 1 (Fig. 3). However, the reward position is unknown
a priori. This prior information is encoded by the initial state probability vector
d =(1,0,0,0)T⊗(0.5,0.5)T,
0
where ⊗ denotes the Kronecker product.
The transition matrix B encodes the state transitions (from column-index
uk
to row-index), as
   
1 0 0 1 0 0 0 0
0 1 0 0 1 1 0 1
B 1 = 0 0 1 0   ⊗I 2 , B 2 = 0 0 1 0   ⊗I 2 ,
0 0 0 0 0 0 0 0
18
p(x t |x t−1 ,u t ) u t p(x t+1 |x t ,u t+1 ) u t+1
B B
MUX MUX
B B
ut ut+1
d x
t−1 t−1
Cat T = T
x t x t+1
p(x )
t−1
T T
A A
p(y t |x t ) y t p(y t+1 |x t+1 ) y t+1
Cat Cat
p˜(y ) p˜(y )
t t+1
c c
t t+1
Figure 4: Forney-style factor graph of the generative model for the T-maze. The
MUX nodes select the transition matrix as determined by the control variable.
Dashed boxes summarize the indicated distributions.
   
0 0 0 0 0 0 0 0
0 1 0 0 0 1 0 0
B 3 = 1 0 1 1   ⊗I 2 , B 4 = 0 0 1 0   ⊗I 2 .
0 0 0 0 1 0 0 1
The control affects the agent position, but not the reward position. Therefore,
Kronecker products with the two-dimensional unit matrix I ensure that the
2
transitionsareduplicatedforbothpossiblerewardpositions. Notethatpositions
2 and 3 (the reward arms) are attracting states, since none of the transition
matrices allow a transition away from these positions. This means that although
it is possible to propose any control at any time, not all controls will move the
agent to its attempted position. We denote the collection of transition matrices
by B ={B ,B ,B ,B }.
1 2 3 4
The observed outcome depends on the position of the agent. The position-
dependentobservationmatricesspecifyhowobservationsfollow,giventhecurrent
19
position of the agent (subscripts) and the reward position (columns), as
       
0.5 0.5 0 0 0 0 1 0
0.5 0.5  0 0   0 0  0 1
A 1 =  0 0   , A 2 =  α 1−α   , A 3 = 1−α α   , A 4 = 0 0   ,
0 0 1−α α α 1−α 0 0
(41)
with reward probability α. The columns of these position-dependent observation
matrices represent the two possibilities for the reward position. The position-
dependent observation matrices combine into the complete block-diagonal, 16-
by-8 observation matrix
A=A ⊕A ⊕A ⊕A ,
1 2 3 4
where ⊕ denotes the direct sum (i.e. block-diagonal concatenation).
The goal prior depends upon the future time,
c =(0.25,0.25,0.25,0.25)T⊗(0.25,0.25,0.25,0.25)T (42a)
1
c =σ
(cid:0) (0,0,c,−c)T⊗(1,1,1,1)T(cid:1)
for k >1, (42b)
k
with reward utility c, and σ the soft-max function where σ(s) = exp(si) . The
i (cid:80)
j
exp(sj)
flat prior c encodes a lack of external preference at t = 1, while c for k > 1
1 k
encodes a preference for observing rewards at subsequent times. This effectively
removes the goal prior for the first move (t=1), while in subsequent moves the
agent is rewarded for extrinsically rewarding states [19].
6 Inference for Planning
In this simulation we compare the behavior of a CBFE agent to the behavior
of a reference BFE agent (without point-mass constraints). We consider given
policies uˆ, and the optimal CBFE as a function of those policies
B∗(uˆ)=minB[q;yˆ,uˆ], (43)
yˆ
q,yˆ
where the yˆ subscript indicates the explicit inclusion of point-mass constraints.
TheunconstrainedBFErepresentstheobjectivewherethefutureobservation
variables y are not point-mass constrained by their potential outcomes yˆ. The
unconstrained agent will therefore optimize the joint belief over state and future
observation variables rather than potential outcomes, as
B∗(uˆ)=minB[q;uˆ]. (44)
q
We will evaluate the BFE, CBFE and EFE for all sixteen (T =2) possible
candidate policies uˆ ∈U×U. We consider several scenarios with varying reward
probabilities α (41) and reward utilities c (42).
20
In the current section we do not (yet) consider the interaction of the agent
with the environment. In other words, actions from optimal policy uˆ∗ are not
(yet) executed; we are purely interested in the inference for planning itself, and
the resulting free energy values as a function of the candidate policies (43).
6.1 Message Passing Schedule for Planning
The message passing schedules for planning are drawn in Fig. 5 (BFE) and
6 (CBFE), where light messages are computed by sum-product (SP) message
passing updates [30], and dark messages by variational message passing updates
[17]. An overview of message passing updates for discrete nodes can be found in
[34, App. A].
p(x |x ,u ) p(x |x ,u )
t t−1 t u t+1 t t+1
t+1
p t (x t−1 ) 1 u t ↓ 2 5 ↓ 6
→ → → →
=
← ← ←
x t−1 14 13 x t 10 x
t+1
↑ 9
11 ↓ ↑ 4
p(y |x ) p(y |x )
t t t+1 t+1
y y
t t+1
12 ↓ ↑ 3 7 ↓ ↑ 8
p˜(y ) p˜ (y )
t t t+1 t+1
Figure 5: Message passing schedule for planning in the T-maze with the BFE.
For the CBFE, the posterior beliefs associated with the observation variables
are constrained by point-mass (Dirac-delta) distributions, see (27), and the
corresponding potential outcomes are optimized for. The message passing
optimization scheme is derived from first principles in [16]. In order to obtain
a new value, e.g. yˆ, messages 11 and 12 are multiplied. The mode of the
t
product then becomes the new value yˆ, which is used to construct the belief
t
q(y )=δ(y −yˆ). Theupdatedbeliefissubsequentlyusedinthenextiterationto
t t t
compute 3. The resulting iterative expectation maximization (EM) procedure
initializes values for all yˆ , and is performed using message passing according to
k
[31]. Interestingly,whereoptimizationoftheEFEisperformedbyaforward-only
procedure(seeAppendixA),optimizationofthe(C)BFE,asillustratedinFig.5
and 6, also includes a complete backward (smoothing) pass over the model.
21
p(x |x ,u ) p(x |x ,u )
t t−1 t u t+1 t t+1
t+1
p t (x t−1 ) 1 u t ↓ 2 4 ↓ 5
→ → → →
=
← ← ←
x t−1 10 9 x t 7 x
t+1
↑ 6
8 ↓ ↑ 3
p(y |x ) p(y |x )
t t t+1 t+1
11 ↓ 13 ↓
δ δ
y t ↑ 12 y t+1 ↑ 14
p˜(y ) p˜ (y )
t t t+1 t+1
Figure 6: Message passing schedule for planning in the T-maze with the CBFE.
6.2 Inference Results for Planning
Optimization of the (C)BFE by message passing is performed with ForneyLab56
version 0.11.4 [18]. Free energies for planning, for three different agents and
T-maze scenarios, are plotted in Fig. 7. The distinct agents optimize the CBFE,
BFE and EFE, respectively. We summarize the most important observations
below.
The first column of diagrams in Fig. 7 shows the results for the CBFE agent,
for varying scenarios.
• ThefirstscenariofortheCBFEagent(upperleftdiagram)imposesalikely
reward (α=0.9) and positive reward utility (c=2). In this scenario, the
CBFE agent prefers the informative policies (4,2) and (4,3), where the
agent seeks the cue in the first move and the reward in the second move.
An epistemic (information seeking) agent would prefer these policies in
this scenario.
• In the upper left diagram, note the lack of preference between position 2
and 3 in the second move. Because the policy is not yet executed (moves
are only planned), the true reward location remains unknown. Therefore,
both of these informative policies are on equal footing.
The second column of diagrams shows the results for the BFE agent.
5ForneyLabisavailableathttps://github.com/biaslab/ForneyLab.jl.
6Simulationsourcecodeisavailableathttps://biaslab.github.io/materials/epistemic_
search.zip.
22
• In every scenario, the BFE agent fails to distinguish between the majority
of ignorant (first move to 1), informative (first move to 4) and greedy
policies (first move to 2 or 3). These policy preferences do not correspond
with the anticipated preferences of an epistemic agent.
• Comparing the BFE with the CBFE results, we observe that the point-
mass constraint on potential outcomes induces a differentiation between
ignorant, informative and greedy policies.
• More specifically, the third scenario (third row of diagrams) removes the
extrinsic value of reward (c = 0). While the CBFE still differentiates
between ignorant, informative and greedy policies, the BFE agent exhibits
a total lack of preference.
• The second scenario (second row of diagrams) removes the value of infor-
mation about the reward position (α=0.5). This scenario thus renders
the cue worthless. The BFE agent appears insusceptible to a change in
the epistemic α parameter.
Taken together, these observations support the interpretation of the BFE as a
purely extrinsically driven objective (Sec. 4).
The third column of diagrams produces the results for an EFE agent, as
implemented in accordance with [4], see also Appendix A.
• In all scenarios, the EFE agent exhibits a consistent preference for the
(4,4) policy. Compared to the CBFE agent, the EFE agent fails to plan
ahead to obtain future reward after observing the cue.
• AswewillseeinSec.7,theEFEagentonlyinfersapreferenceforareward
arm (position 2 or 3) after execution of the first move to the cue position.
In contrast, the CBFE agent predicts the impact of information and plans
accordingly.
• Thesecondscenario(middlerow)providesaninformativecue,butremoves
thepossibilitytoexploitthatinformation(α=0.5). Interestingly,theEFE
agent still moves to the que position (for the sake of getting information),
whereas the CBFE agent expresses ambivalence under an inoperable cue.
23
Figure 7: (Constrained) Bethe Free Energies ((C)BFE) and Expected Free
Energies (EFE) (in bits) for the T-maze policies under varying parameter
settings. Each diagram plots the minimized free energy values for all possible
policies (lookahead T = 2), with the first move on the vertical axis and the
second move on the horizontal axis. For example, the cell in row 4, column 3
represents the policy uˆ = (4,3), which first moves to position 4 and then to
position 3. The values for the optimal policies uˆ∗ are annotated red with an
asterisk.
24
6.3 Results for CBFE Value Decomposition
Simulated values for the CBFE decomposition (32) in the T-maze application
are shown in Fig. 8, for four different T-maze scenarios. We summarize the most
important observations below.
The first column of diagrams in Fig 8 represents the confidence (34) of the
CBFE objective for all (planned) policies. The confidence prefers (or ties) the
most informative policy (4,4) for all scenarios.
• In the first three scenarios (first three rows of diagrams), all policies
other than (4,4) dismiss the opportunity to obtain full information about
outcomes on two occasions (T = 2). This is reflected by a negative
confidence value, which measures the average rejected information in bits.
For example, the policy (1,1) rejects two possibilities to obtain 1 bit of
information, leading to a confidence of −2.
• A change in the external value parameter c does not affect the confidence,
which supports the interpretation of the confidence as an intrinsic quantity
(35).
• Inthefinalscenario,thegreedypolicies(movingfirsttoposition2or3)are
on equal footing with the informative policies (moving first to 4). This is
becauseinthefinalscenario,visitingposition2or3offersthesameamount
of information (namely, complete certainty) about the reward position, as
would visiting the cue position.
The complexity (second column of diagrams) opposes changes in state be-
liefs that are unwarranted by the policy-induced state transitions, and guards
against premature convergence of the state precision (Table 2). As a result, the
complexity prefers (or ties) the most conservative policy (1,1) for all scenarios.
• The complexity is unaffected by changes in utility (similar to the confi-
dence), which supports the interpretation of the complexity as an intrinsic
quantity (35).
• Inthethirdscenario(α=0.5), thegreedypoliciesbecometiedincomplex-
ity with (most of) the ignorant policies. Because neither visiting a reward
arm nor remaining at the initial position offers any useful information
about the reward position, the state belief remains unaltered, and these
policies incur no complexity penalty.
Theextrinsicvalue(thirdcolumnofdiagrams)representsthevalueofexternal
reward, and leads the agent to pursue extrinsically rewarding states.
• The extrinsic value is unaffected by changes in the epistemic reward
probability parameter α, which supports the interpretation of the extrinsic
value as an externally determined quantity.
• Inthesecondscenariotherewardutilityvanishes(c=0),andtheextrinsic
value becomes indifferent about policies.
25
Figure 8: Confidence, complexity and extrinsic value contributions (in bits) to
the Constrained Bethe Free Energy (32) for the T-maze policies (lookahead
26
T =2) under varying parameter settings. Optimal values are indicated red with
an asterisk.
7 Interactive Simulation
In this section we compare the resulting behavior of the CBFE agent with a
traditional EFE agent, in interaction with a simulated environment.
7.1 Experimental Protocol
The experimental protocol governs how the agent interacts with its environment.
Inourprotocol,theactionandoutcomeattimetaretheonlyquantitiesthatare
exchangedbetweentheagentandtheenvironment(generativeprocess). Thetask
of the agent is then to plan for actions that lead the agent to desired states. We
adapt the experimental protocol of [38] for the purpose of the current simulation.
We write the model f with a time-subscript to indicate the time-dependent
t
statistics of the state prior as a result of the perceptual process (Sec. 2.2). The
experimental protocol (Alg. 1) then consists of five steps per time t.
Algorithm 1 Experimental protocol.
Given a model f with initial state and goal priors
1
for t=1 to N do
uˆ∗ = plan(f ) # Execute the planning algorithm
t t
uˆ∗ = act(uˆ∗) # Select the first action
t t
execute(uˆ∗) # Execute the action in the simulated environment
t
yˆ = observe() # Observe the new environmental outcome
t
f = slide(uˆ∗,yˆ) # Prepare the model for the next iteration
t+1 t t
end for
The plan step solves the inference for planning (Sec. 2.3), and returns the
activepolicyuˆ∗ thatrepresentsthe(believed)optimalsequenceoffuturecontrols.
t
In the act step, the first action uˆ∗ is picked from the policy. The execute step
t
then subsequently executes this action in the simulated environment. Execution
will alter the state of the environment. In the observe step, the environment
responds with a new observation yˆ. Given the action and resulting observation,
t
the slide step then solves the inference for perception (Sec. 2.2) and prepares
the model for the next step.
Inference for the slide step is illustrated in Fig. 9, where message 3 propa-
gatesanobservedoutcomeyˆ,andwheremessage 5 summarizestheinformation
t
contained within in the dashed box. Only the dashed sub-model is relevant
to the slide step, that is, beliefs about the future do not influence 5. After
computation, message 5 is normalized, and the resulting state posterior q∗(x )
t
issubsequentlyusedasapriortoconstructthemodelf forthenexttime-step,
t+1
see also [38].
7.2 Results for Interactive Simulation
We initialize an environment with the reward in the right arm (position 3). We
then execute the experimental protocol of Alg. 1, with lookahead T = 2, for
27
p(x |x ,u )
t t−1 t u
t+1
p t (x t−1 ) 1 u t ↓ 2 5 p(x t+1 |x t ,u t+1 )
→ → →
=
x t−1 x t x
t+1
↑ 4
p(y |x ) p(y |x )
t t t+1 t+1
y t ↑ 3 y t+1
δ
p˜(y ) p˜ (y )
t t t+1 t+1
Figure 9: Message passing schedule for the slide step.
N =2moves,onadenselandscapeofvaryingrewardprobabilitiesαandutilities
c (scenarios). After the first move, the environment returns an observations to
the agent, which informs the agent about second move. After the second move,
theexpectedrewardthatisassociatedwiththeresultingpositionisreported. We
perform10simulationsperscenario,andcomputetheaveragerewardprobability.
The results of Fig. 10 compare the average rewards of the CBFE agent and the
EFE agent.
From the results of Fig. 10 it can be seen that the region of zero average
reward (dark region in lower left corner) is significantly smaller for the CBFE
agent than for the EFE agent. This indicates that the CBFE agent accrues
reward in a significantly larger portion of the scenario landscape than the EFE
agent. In the lower left corner, the resulting CBFE agent trajectory becomes
(4, 4), whereas the EFE agent trajectory becomes (4, 1). Although both agents
observe the cue after their first move, they do not visit the indicated reward
position in the second move, which leads to zero average reward. Note that
neither objective is explicitly designed to optimize for average reward; both
define a free energy instead, where multiple simultaneous forces are at play.
28
CBFE Agent EFE Agent
Figure 10: Average reward landscapes for the Constrained Bethe Free Energy
(CBFE) agent and the Expected Free Energy (EFE) agent.
In the upper right regions, with high reward probability and utility, both
agents consistently execute (4,3). With this trajectory, the cue is observed after
thefirstmove,andtheindicated(correct)rewardpositionisvisitedinthesecond
move, leading to an average reward of α. For reward probabilities close to α=1
however, the performance of both agents deteriorates. In this upper region, the
informative policies become tied with the greedy policies (see Fig. 8), and there
is no single dominant trajectory. In some trajectories the agent enters the wrong
arm on the first move, from which the agent cannot escape, and the average
reward deteriorates.
GreedybehaviorisalsoobservedfortheCBFEagentwheninformativepriors
c (42) are set for all k (including k =1), conforming with the configuration of
k
[4]. With this configuration, expected reward for the CBFE agent deteriorates
to 0.5 in the otherwise rewarding region. Interestingly, this change of priors does
not affect results for the EFE agent. The resulting change in behavior suggests
that the CBFE agent is more susceptible to temporal aspects of the goal prior
configuration. While this effect may be considered a nuisance in some cases, it
also allows for increased flexibility when assigning explict temporal requirements
about goals. For example, assigning an informative versus a flat prior for k =1
respectively encodes an urgency in obtaining immediate reward versus a freedom
to explore.
29
8 Discussion
In this paper, we focused on epistemic drivers for behavior. We noted that the
nature of the epistemic drive differs between an EFE and CBFE agent. Namely,
the epistemic drive for the EFE agent stems directly from maximizing a mutual
informationtermbetweenstatesandobservations(20), whiletheepistemicdrive
for the CBFE agent stems from a self-evidencing mechanism (Sec. 4.2). In order
to better understand the strengths and limitations of the driving forces for the
CBFE, it would be interesting to investigate its behavior in more challenging
setups, including continuous variables, inference for control [19], and the effects
point-mass constraints on other model variables.
Recent work by [39, 40] shows that epistemic behavior does not occur when
the goal prior goes to a point-mass. The work of [39] points to the entropy of
the observed variables H[q(y)] as a pivotal quantity for epistemic behavior. The
CBFE however does not include an entropy over observations, and still exhibits
epistemic qualities. The difference in methods lies with the constraint quantity;
namely [39, 25] constrain the goal prior p˜(y)=δ(y−yˆ), while the current paper
constrains the variational distribution q(y) = δ(y −yˆ) instead. While both
constraints remove H[q(y)] from the resulting FE objective, optimization of yˆ in
the CBFE still induces an epistemic drive (Sec. 4). Our results thus show that
epistemic drives for AIF prove to be more subtle than initially anticipated.
Our presented approach is uniquely scalable, because it employs off-the-shelf
message passing algorithms. All message computations are local, which makes
our approach naturally amenable to both parallel and on-line processing [41].
Especially AIF in deep hierarchical models might benefit from the improved
computational properties of the CBFE. It will be interesting to investigate how
the presented approach generalizes to more demanding (practical) settings.
As a generic variational inference procedure, the CBFE approach applies to
arbitrary models. This allows researchers to investigate epistemics in a much
wider class of models than previously available. One immediate avenue for
further research is the integration of CBFE with predictive coding schemes
[42, 43, 44]. Predictive coding has so far been driven mainly by minimizing free
energyinhierarchicalmodelsundertheLaplaceapproximation. Here,theCBFE
approach readily applies as well [16], allowing researchers to explore the effects
of augmenting existing predictive coding models with epistemic components.
Thederivationofalternativefunctionalsthatpreservethedesirableepistemic
behavior of EFE optimization is an active research area [45, 8]. There have been
several interesting proposals such as the Free Energy of the Expected Future
[23, 7, 9] or Generalized Free Energy [5], as well as amortization strategies
[46, 47] and sophisticated schemes [22]. Comparing behavior between the CBFE
and other free energy objectives might therefore prove an interesting avenue for
future research.
In the original description of active inference, a policy precision is opti-
mized during policy planning, and the policy for execution is sampled from
a distribution of precision-weighted policies [4]. The present paper does not
consider precision optimization, and effectively assumes a large, fixed precision
30
instead. In practice, this procedure consistently selects the policy with minimal
free energy; see also maximum selection (in terms of value) as described by
[6]. To accommodate for precision optimization, the CBFE objective might be
extended with a temperature parameter, mimicking thermodynamic descriptions
offreeenergy[48]. Optimizationofthetemperatureparametermightthenrelate
to optimization of the policy precision, as often seen in biologically plausible
formulations of AIF [49].
Another interesting avenue for further research would be the design of a
meta-agent that determines the statistics and temporal configuration of the goal
priors. Inourexperimentswedesignthegoalpriors(42)ourselves, suchthatthe
agent is free to explore in the first move and seeks reward on the second move.
The challenge then becomes to design a synthetic meta-agent that automatically
generates an effective lower-level goal sequence from a single higher-level goal
definition.
9 Conclusions
In this paper we presented mathematical arguments and simulations that show
howinclusionofpoint-massconstraintsontheBetheFreeEnergy(BFE)leadsto
epistemic behavior. The thus obtained Constrained Bethe Free Energy (CBFE)
has direct connections with formulations of the principle of least action in
physics[1], andcanbeconvenientlyoptimizedbymessagepassingonagraphical
representation of the generative model (GM).
Simulations for the T-maze task illustrate how a CBFE agent exhibits
an epistemic drive, whereas the BFE agent lacks epistemic qualities. The
key intuition behind the working mechanism of the CBFE is that point-mass
constraints on observation variables explicitly encode the assumption that the
agent will observe in the future. Although the actual value of these observation
remains unknown, the agent “knows” that it will observe in the future, and
it “knows” (through the GM) how these (potential) outcomes will influence
inferences about states.
We dissected the CBFE objective in terms of its constituent drivers for
behavior. In the CBFE framework, in addition to being functionals of the state
beliefs, the confidence and complexity are viewed as functions of the potential
outcomes and policy respectively. Simultaneous optimization of variational
distributions and potential outcomes then leads the agent to prefer epistemic
policies. Interactive simulations for the T-maze showed that, compared to an
EFE agent, the CBFE agent incurs expected reward in a significantly larger
portion of the scenario landscape.
We performed our simulations by message passing on a Forney-style factor
graph representation of the generative model. The modularity of the graphical
representation allows for flexible model search, and message passing allows for
distributed computations that scale well to bigger models. Constraining the
BFE and optimizing the CBFE objective by message passing thus suggests a
simple and general mechanism for epistemic-aware AIF in free-form generative
31
models.
Conflict of Interest Statement
The authors declare that the research was conducted in the absence of any
commercial or financial relationships that could be construed as a potential
conflict of interest.
Author Contributions
The original idea was conceived by TvdL. All authors contributed to further
conceptual development of the methods that are presented in this manuscript.
SimulationswereperformedbyTvdLandMK.Allauthorscontributedtowriting
the manuscript.
Funding
This research was made possible by funding from GN Hearing A/S. This work is
part of the research programme Efficient Deep Learning with project number
P16-25 project 5, which is (partly) financed by the Netherlands Organisation for
Scientific Research (NWO).
Acknowledgments
The authors gratefully acknowledge stimulating discussions with Dimitrije
Markovi´c of the Neuroimaging group at TU Dresden. The authors also thank
the two reviewers for their valuable comments.
Abbreviations
The following abbreviations are used in the manuscript:
FEP Free Energy Principle
AIF Active Inference
VFE Variational Free Energy
EFE Expected Free Energy
BFE Bethe Free Energy
CBFE Constrained Bethe Free Energy
GM Generative Model
EM Expectation Maximization
FFG Forney-style Factor Graph
VMP Variational Message Passing
SP Sum-Product
MAP Maximum A-Posteriori
32
References
[1] A.Caticha,Entropic Inference and the Foundations of Physics. EBEB-2012,
the 11th Brazilian Meeting on Bayesian Statistics, 2012.
[2] K. Friston, J. Kilner, and L. Harrison, “A free energy principle for the
brain,” Journal of Physiology, Paris, vol. 100, pp. 70–87, Sept. 2006.
[3] K. J. Friston, J. Daunizeau, J. Kilner, and S. J. Kiebel, “Action and
behavior: a free-energy formulation,” Biological cybernetics, vol. 102, no. 3,
pp. 227–260, 2010.
[4] K. Friston, F. Rigoli, D. Ognibene, C. Mathys, T. Fitzgerald, and G. Pez-
zulo, “Active inference and epistemic value,” Cognitive Neuroscience, vol. 6,
pp. 187–214, Mar. 2015.
[5] T. Parr and K. J. Friston, “Generalised free energy and active inference,”
Biologicalcybernetics,vol.113,no.5,pp.495–513,2019. Publisher: Springer.
[6] S. Schw¨obel, S. Kiebel, and D. Markovi´c, “Active Inference, Belief Prop-
agation, and the Bethe Approximation,” Neural Computation, vol. 30,
pp. 2530–2567, Sept. 2018.
[7] A. Tschantz, B. Millidge, A. K. Seth, and C. L. Buckley, “Reinforcement
Learning through Active Inference,” arXiv:2002.12636 [cs, eess, math, stat],
Feb. 2020. arXiv: 2002.12636.
[8] N. Sajid, F. Faccio, L. Da Costa, T. Parr, J. Schmidhuber, and K. Friston,
“BayesianbrainsandtheRenyidivergence,”arXivpreprintarXiv:2107.05438,
2021.
[9] D. Hafner, P. A. Ortega, J. Ba, T. Parr, K. Friston, and N. Heess, “Action
and Perception as Divergence Minimization,” arXiv:2009.01791 [cs, math,
stat], Sept. 2020. arXiv: 2009.01791.
[10] B.deVriesandK.J.Friston,“AFactorGraphDescriptionofDeepTemporal
Active Inference,” Frontiers in Computational Neuroscience, vol. 11, 2017.
[11] T. Parr, D. Markovic, S. J. Kiebel, and K. J. Friston, “Neuronal message
passing using Mean-field, Bethe, and Marginal approximations,” Scientific
Reports, vol. 9, p. 1889, Dec. 2019.
[12] T. Champion, M. Grze´s, and H. Bowman, “Realising Active Inference
in Variational Message Passing: the Outcome-blind Certainty Seeker,”
arXiv:2104.11798 [cs], Apr. 2021. arXiv: 2104.11798.
[13] A. Caticha, “Relative Entropy and Inductive Inference,” AIP Conference
Proceedings, vol. 707, pp. 75–96, 2004. arXiv: physics/0311093.
33
[14] J.S.Yedidia,W.Freeman,andY.Weiss,“Constructingfree-energyapproxi-
mations and generalized belief propagation algorithms,” IEEE Transactions
on Information Theory, vol. 51, pp. 2282–2312, July 2005.
[15] J. Pearl, Probabilistic reasoning in intelligent systems: networks of plausible
inference. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.,
1988.
[16] I˙. S¸eno¨z, T. van de Laar, D. Bagaev, and B. de Vries, “Variational Message
Passing and Local Constraint Manipulation in Factor Graphs,” Entropy,
vol. 23, no. 7, p. 807, 2021. Publisher: Multidisciplinary Digital Publishing
Institute.
[17] J. Dauwels, “On Variational Message Passing on Factor Graphs,” in IEEE
International Symposium on Information Theory, (Nice, France), pp. 2546–
2550, June 2007.
[18] M. Cox, T. van de Laar, and B. de Vries, “A factor graph approach to
automated design of Bayesian signal processing algorithms,” International
Journal of Approximate Reasoning, vol. 104, pp. 185–204, Jan. 2019.
[19] T. van de Laar, “Simulating Active Inference Processes With Message
Passing,” May 2018.
[20] C. Lanczos, The variational principles of mechanics. Courier Corporation,
2012.
[21] D. Koller and N. Friedman, Probabilistic graphical models: principles and
techniques. MIT press, 2009.
[22] K. Friston, L. Da Costa, D. Hafner, C. Hesp, and T. Parr, “Sophisticated
Inference,” Neural Computation, vol. 33, pp. 713–763, Mar. 2021.
[23] B. Millidge, A. Tschantz, and C. L. Buckley, “Whence the Expected Free
Energy?,” arXiv preprint arXiv:2004.08128, 2020.
[24] K. Friston, P. Schwartenbeck, T. Fitzgerald, M. Moutoussis, T. Behrens,
and R. J. Dolan, “The anatomy of choice: active inference and agency,”
Frontiers in Human Neuroscience, vol. 7, p. 598, 2013.
[25] L. Da Costa, T. Parr, N. Sajid, S. Veselic, V. Neacsu, and K. Friston,
“Active inference on discrete state-spaces: a synthesis,” arXiv:2001.07203
[q-bio], Jan. 2020. arXiv: 2001.07203.
[26] M. J. Wainwright and M. I. Jordan, “Graphical Models, Exponential Fam-
ilies, and Variational Inference,” Foundations and Trends® in Machine
Learning, vol. 1, pp. 1–305, Nov. 2008.
[27] G. Forney, “Codes on graphs: normal realizations,” IEEE Transactions on
Information Theory, vol. 47, pp. 520–548, Feb. 2001.
34
[28] S. Korl, A factor graph approach to signal modelling, system identification
and filtering. PhD thesis, Swiss Federal Institute of Technology, Zurich,
2005.
[29] H.-A. Loeliger, J. Dauwels, J. Hu, S. Korl, L. Ping, and F. R. Kschischang,
“TheFactorGraphApproachtoModel-BasedSignalProcessing,”Proceedings
of the IEEE, vol. 95, pp. 1295–1322, June 2007.
[30] H.-A. Loeliger, “An introduction to factor graphs,” Signal Processing Mag-
azine, IEEE, vol. 21, pp. 28–41, Jan. 2004.
[31] J. Dauwels, S. Korl, and H.-A. Loeliger, “Expectation maximization as
messagepassing,”inInternationalSymposiumonInformationTheory, 2005.
ISIT 2005. Proceedings, pp. 583–586, Sept. 2005.
[32] D. Zhang, W. Wang, G. Fettweis, and X. Gao, “Unifying Message Pass-
ing Algorithms Under the Framework of Constrained Bethe Free Energy
Minimization,” arXiv:1703.10932 [cs, math], Mar. 2017. arXiv: 1703.10932.
[33] T. van de Laar, I. S¸en¨oz, A. O¨z¸celikkale, and H. Wymeersch, “Chance-
Constrained Active Inference,” arXiv preprint arXiv:2102.08792, 2021.
[34] T. van de Laar, Automated Design of Bayesian Signal Processing Algo-
rithms. PhD thesis, Eindhoven University of Technology, Eindhoven, The
Netherlands, 2019.
[35] K.FristonandW.Penny,“PosthocBayesianmodelselection,”Neuroimage,
vol. 56, pp. 2089–2099, June 2011.
[36] J. Hohwy, “The Self-Evidencing Brain,” Nouˆs, vol. 50, pp. 259–285, June
2016.
[37] J.Hohwy,“Consciousself-evidencing,”ReviewofPhilosophyandPsychology,
pp. 1–20, 2021. Publisher: Springer.
[38] T. van de Laar and B. de Vries, “Simulating Active Inference Processes by
Message Passing,” Frontiers in Robotics and AI, vol. 6, p. 20, 2019.
[39] B.Millidge,A.Tschantz,A.Seth,andC.Buckley,“Understandingtheorigin
of information-seeking exploration in probabilistic objectives for control,”
arXiv preprint arXiv:2103.06859, 2021.
[40] L. Da Costa, N. Sajid, T. Parr, K. Friston, and R. Smith, “The relation-
ship between dynamic programming and active inference: the discrete,
finite-horizon case,” arXiv:2009.08111 [cs, math, q-bio], Sept. 2020. arXiv:
2009.08111.
[41] D.BagaevandB.deVries,“ReactiveMessagePassingforScalableBayesian
Inference,” arXiv:2112.13251 [cs], Dec. 2021. arXiv: 2112.13251.
35
[42] R.Bogacz,“Atutorialonthefree-energyframeworkformodellingperception
and learning,” Journal of Mathematical Psychology, vol. 76, pp. 198–211,
Feb. 2017.
[43] K.FristonandS.Kiebel,“Predictivecodingunderthefree-energyprinciple,”
Philosophical Transactions of the Royal Society of London B: Biological
Sciences, vol. 364, no. 1521, pp. 1211–1221, 2009.
[44] B. Millidge, A. Tschantz, and C. L. Buckley, “Predictive coding approx-
imates backprop along arbitrary computation graphs,” arXiv preprint
arXiv:2006.04182, 2020.
[45] A. Tschantz, M. Baltieri, A. K. Seth, and C. L. Buckley, “Scaling active
inference,” in 2020 International Joint Conference on Neural Networks
(IJCNN), pp. 1–8, IEEE, 2020.
[46] B. Millidge, “Deep Active Inference as Variational Policy Gradients,”
arXiv:1907.03876 [cs], July 2019. arXiv: 1907.03876.
[47] K. Ueltzho¨ffer, “Deep Active Inference,” Biological Cybernetics, Oct. 2018.
[48] P. A. Ortega and D. A. Braun, “Thermodynamics as a theory of decision-
makingwithinformation-processingcosts,” Proceedings of the Royal Society
of London A: Mathematical, Physical and Engineering Sciences, vol. 469,
p. 20120683, May 2013.
[49] T. H. B. FitzGerald, R. J. Dolan, and K. Friston, “Dopamine, reward
learning, and active inference,” Frontiers in Computational Neuroscience,
p. 136, 2015.
[50] H.-A.Loeliger, “LeastSquaresandKalmanFilteringonForneyGraphs,” in
Codes, Graphs, and Systems (R. E. Blahut and R. Koetter, eds.), vol. 670,
pp. 113–135, Boston, MA: Springer US, 2002.
Appendix
A. Evaluation of the Expected Free Energy
The standard procedure for evaluating the Expected Free Energy (EFE) collects
instantaneous EFE contributions over time by a forward filtering approach [4].
Following [4, 25], the EFE constructs an instantaneous model for each future
time-point τ ≥t, as
f(y ,x |u )=p(x |y ,u )p˜(y ), (45)
τ τ t:τ τ τ t:τ τ
with p˜(y ) the goal prior, and p(x |y ,u ) a state posterior that needs to be
τ τ τ t:τ
further defined.
36
UsingBayesrule,wecanexpressthestateposteriorintermsoftheobservation
model and a posterior predictive for the state, as
p(x |u )p(y |x )
p(x |y ,u )= τ t:τ τ τ (46a)
τ τ t:τ p(y |u )
τ t:τ
p(x |u )p(y |x )
= τ t:τ τ τ . (46b)
(cid:80)
p(x |u )p(y |x )
xτ τ t:τ τ τ
Theposteriorpredictivep(x |u )isexplicitlyconditionedonthepolicyu ,
τ t:τ t:τ
from current time t up to and including future time τ, and thus represents the
forward prediction (filtering solution) for the current state belief given preceding
controls (whilst excluding preceding goals). Using the generative model engine
definition, the posterior predictive for the state then becomes7
τ
(cid:88) (cid:88) (cid:89)
p(x |u )= p(x ) p(y ,x |x ,u ) (47a)
τ t:τ t−1 k k k−1 k
yt:τxt−1:τ−1 k=t
τ
(cid:88) (cid:89)
= p(x ) p(x |x ,u ). (47b)
t−1 k k−1 k
xt−1:τ−1 k=t
The second step of (47) simplifies the expression by marginalizing over y . The
t:τ
posterior predictive can then conveniently be computed by message passing on
the generative model [50], using a single forward pass.
We are now prepared to construct the instantaneous EFE (the EFE at time
τ), which is defined as [4]
(cid:20) (cid:21)
p(x |uˆ )
G (uˆ )=E log τ t:τ . (48)
τ t:τ p(yτ|xτ)p(xτ|uˆt:τ) f(y ,x |uˆ )
τ τ t:τ
Upon substitution of (46a) in (48), the instantaneous EFE factorizes into
ambiguity and risk, as
(cid:20) (cid:21)
p(y |uˆ )
G (uˆ )=E log τ t:τ
τ t:τ p(yτ|xτ)p(xτ|uˆt:τ) p(y |x )p˜(y )
τ τ τ
(cid:20) (cid:21)
=−E (cid:2)E [logp(y |x )] (cid:3) +E log p(y τ |uˆ t:τ )
p(xτ|uˆt:τ) p(yτ|xτ) τ τ p(yτ|xτ)p(xτ|uˆt:τ) p˜(y )
τ
=E [H[p(y |x )]]+KL[p(y |uˆ )(cid:107)p˜(y )] . (49)
p(xτ|uˆt:τ) τ τ τ t:τ τ
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ambiguity observationrisk
This decomposition is often used to compute the instantaneous EFE in practice.
The complete EFE of the full policy uˆ then follows by summation of all
instantaneous contributions
t+T−1
(cid:88)
G(uˆ)= G (uˆ ). (50)
τ t:τ
τ=t
7Thedefinitionof[4,p.192]implicitlydefinesthisforwardpredictionasamarginalization
overstates,whichismadeexplicitinthedefinitionof (47). Ingeneral,thismarginalization
neednotbetractable,inwhichcaseitcanalsobeapproximatedbyon-lineoptimizationofan
appropriateBFEobjectiveonthegenerativemodelengine.
37
To summarize, the procedure for computation of the EFE in practice [4, 25]
usuallyconsistsofthreesteps. First,foragivenpolicyuˆ,theposteriorpredictive
distributions (47) are computed for all t≤τ <t+T. Then, the instantaneous
EFE’sare(individually)computed. Finally,theinstantaneousEFE’saresummed
to produce the full-policy EFE (50).
38

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Active Inference and Epistemic Value in Graphical Models"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
