=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Realising Synthetic Active Inference Agents, Part II: Variational Message Updates
Citation Key: laar2023realising
Authors: Thijs van de Laar, Magnus Koudahl, Bert de Vries

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2023

Abstract: TheFreeEnergyPrinciple(FEP)describes(biological)agentsasminimisinga
variationalFreeEnergy(FE)withrespecttoagenerativemodeloftheirenviron-
ment.ActiveInference(AIF)isacorollaryoftheFEPthatdescribeshowagents
exploreandexploittheirenvironmentbyminimisinganexpectedFEobjective.In
tworelatedpapers,wedescribeascalable,epistemicapproachtosyntheticAIF,by
messagepassingonfree-formForney-styleFactorGraphs(FFGs).Acompanionpa-
per(partI)introducesaConstrainedFFG(CFFG)notationthatvisuallyrepresents
(generalis...

Key Terms: thenetherlands, agents, thefreeenergyprinciple, variational, updates, generalised, message, inference, active, activeinference

=== FULL PAPER TEXT ===

Realising Synthetic Active Inference Agents,
Part II: Variational Message Updates
ThijsvandeLaar1,MagnusKoudahl1,2,andBertdeVries1,3
1DepartmentofElectricalEngineering,EindhovenUniversityof
Technology,Eindhoven,TheNetherlands
2VERSESAIResearchLab,LosAngeles,California,90016,USA
3GNHearingBeneluxBV,Eindhoven,TheNetherlands
September27,2024
Abstract
TheFreeEnergyPrinciple(FEP)describes(biological)agentsasminimisinga
variationalFreeEnergy(FE)withrespecttoagenerativemodeloftheirenviron-
ment.ActiveInference(AIF)isacorollaryoftheFEPthatdescribeshowagents
exploreandexploittheirenvironmentbyminimisinganexpectedFEobjective.In
tworelatedpapers,wedescribeascalable,epistemicapproachtosyntheticAIF,by
messagepassingonfree-formForney-styleFactorGraphs(FFGs).Acompanionpa-
per(partI)introducesaConstrainedFFG(CFFG)notationthatvisuallyrepresents
(generalised)FEobjectivesforAIF.Thecurrentpaper(partII)derivesmessage
passingalgorithmsthatminimise(generalised)FEobjectivesonaCFFGbyvaria-
tionalcalculus.AcomparisonbetweensimulatedBetheandgeneralisedFEagents
illustrateshowthemessagepassingapproachtosyntheticAIFinducesepistemic
behaviouronaT-mazenavigationtask.ExtensionoftheT-mazesimulationto1)
learninggoalstatistics,and2)amulti-agentbargainingsetting,illustratehowthis
approachencouragesreuseofnodesandupdatesinalternativesettings.Withafull
messagepassingaccountofsyntheticAIFagents,itbecomespossibletoderiveand
reusemessageupdatesacrossmodelsandmoveclosertoindustrialapplicationsof
syntheticAIF.
Keywords: ActiveInference,FreeEnergyPrinciple,VariationalMessagePassing,
VariationalOptimisation
Thisistheauthor’sfinalversionofthemanuscript,asacceptedforpublicationin
MITNeuralComputation.
1
4202
peS
62
]LM.tats[
3v33720.6032:viXra
1 Introduction
TheFreeEnergyPrinciple(FEP)postulatesthatthebehaviourofbiologicalagentscan
bemodelledasminimisingaVariationalFreeEnergy(VFE)(Fristonetal.,2006).Active
Inference(AIF)isacorollaryoftheFEPthatdescribeshowagentsproposeeffective
actions by minimising an Expected Free Energy (EFE) objective that internalises a
GenerativeModel(GM)oftheagent’senvironmentandapriorbeliefaboutdesired
outcomes(Fristonetal.,2009,2015).
EarlyworksdescribeAIFasacontinuous-timeprocessintermsofcoupleddifferen-
tialequations(Fristonetal.,2010;Kiebeletal.,2009). Laterdiscrete-timeformulations
allowedforexplicitmodelsoffuture(desired)outcomes,anddescribeAIFinterms
ofvariationalinferenceinthecontextofapartiallyobservableMarkovdecisionpro-
cess(DaCostaetal.,2020;Fristonetal.,2013). Simulateddiscrete-timeagentsthen
engageininformation-seekingbehaviourandautomaticallytradeoffexploratoryand
exploitativemodes(Fristonetal.,2015). However,thesemethodsdonotreadilyscale
tofree-formmodels.
Variationalobjectivesfordiscrete-timeAIFcanbeminimisedbymessagepassing
onaForney-styleFactorGraph(FFG)representationoftheGM.Severalauthorshave
attemptedtoscaleAIFunderthismessagepassingframework(vandeLaaranddeVries,
2019;deVriesandFriston,2017). However,agentsbasedontheseapproacheslack
crucialepistemiccharacteristics(Schwo¨beletal.,2018;vandeLaaretal.,2022).
Intworelatedpapers,wedescribeamessagepassingapproachtoscalable,synthetic
AIF agents through Lagrangian optimisation. In part I, we identify a hiatus in the
AIFproblemspecificationlanguage(Koudahletal.,2023). Specifically,werecognise
thatoptimisationconstraints(S¸eno¨zetal.,2021)arenotincludedinthepresent-day
FFGnotation,whichmayleadtoambiguousproblemdescriptions. PartIintroducesa
ConstrainedFFG(CFFG)notationforconstraintspecificationonFFGs,andillustrates
howfreeenergyobjectives, includingtheGeneralisedFreeEnergy(GFE)(Parrand
Friston,2019),relatetospecificconstraintsandmessagepassingschedules.
In part II, which is the current paper, we use the CFFG notation as introduced
in part I to define locally constrained variational objectives, and derive variational
message updates for GFE-based control using variational calculus. The resulting
controlalgorithmstheninduceepistemicbehaviourinsyntheticAIFagents. Wereason
purelyfromanengineeringpoint-of-viewanddonotconcernourselveswithbiological
plausibility.
Inthispaper,ourcontributionsarefour-fold:
• We use variational calculus to derive general message update expressions for
GFE-basedcontrolinsyntheticAIFagents;
• Wederivespecialisedmessagesforadiscrete-variablemodelthatisoftenused
forAIFcontrolinpractice;
• Weimplementthesemessagesinareactiveprogrammingframeworkandsimulate
aperception-actioncycleontheT-mazenavigationtask;
• WeillustratehowthemessagepassingapproachtosyntheticAIFenablesfree-
formmodellingofAIFagentsbyextendingtheT-mazesimulationto1)learn
2
goalstatistics,and2)amulti-agentsetting.
WithafullmessagepassingaccountandreactiveimplementationofGFEoptimisa-
tion,itbecomespossibletoderiveandreusecustommessageupdatesacrossmodelsand
getastepclosertorealisingscalablesyntheticAIFagentsforindustrialapplications.
InSec.2wereviewvariationalBayesasaconstrainedoptimisationproblemthatcan
besolvedbymessagepassingonaConstrainedFFG(CFFG).InSec.3wereviewAIF
andformulateperception,learningandcontrolasmessagepassingonaCFFG.InSec.4,
wefocusontheconstraintdefinitionaroundasubmodeloftwofacingnodesandderive
stationary solutions and messages for GFE-based control. In Sec. 5 we apply these
generalresultstoaspecificdiscrete-variablegoal-observationsubmodelthatisoften
usedinAIFpractice. Wethenworktowardsimplementationofthederivedmessagesin
asimulatedsetting. TheT-mazetaskisdescribedinSec.6andsimulatedinareactive
programmingframeworkinSec.7. WefinishwithasummaryofrelatedworkinSec.8,
andourconclusionsinSec.9.
2 Review of Variational Message Passing
InthissectionwebrieflyreviewVariationalMessagePassing(VMP)asadistributed
approachtominimisingVariationalFreeEnergy(VFE)objectives.Westartbyreviewing
variationalBayesandthenreviewavisualrepresentationofconstrainedVFEobjectives
onaConstrainedForney-styleFactorGraph(CFFG).
2.1 VariationalBayes
Givenaprobabilisticmodelandsomeobserveddata,Bayesianinferenceconcernsthe
computation of posterior distributions over variables of interest. Because Bayesian
inferenceisintractableingeneral,theBayesianinferenceproblemisoftenconverted
toaconstrainedvariationaloptimisationproblem. Theoptimisationobjectiveforthe
so-calledvariationalBayesapproachisaninformation-theoreticquantityknownasthe
VariationalFreeEnergy(VFE),
q(s)
F[q]=E log ,
q
f(s)
(cid:20) (cid:21)
comprisedofaprobabilisticmodelf oversomegenericvariablessandavariational
distributionq. Asanotationalconvention,wewriteacollectionofvariablesincursive
boldscript. AnoverviewofnotationalconventionsisavailableinTable1. TheVFEis
optimisedunderasetofconstraints ,as
Q
q =argminF[q].
∗
q
∈Q
The VFE conveniently imposes an upper bound on the Bayesian surprise (i.e.,
thenegativelogarithmofmodelevidenceZ). MinimisationthenrenderstheVFEa
closeapproximationtothesurprise,whilethevariationaldistributionbecomesaclose
3
Symbol Explanation
s Genericvariablewithindexi
i
s Collectionofvariables
s Collectionexcludings
j j
\
s Sequenceofpastvariables
s Sequenceoffuturevariables
s Vector
S Matrix
s Expectationofavectorvariable
z,w Statevariables
x Observationvariable
θ,ϕ Parameters
f Factorfunction(possiblyunnormalised)
p Probabilitydistribution
q Variationaldistribution
Constraintset
Q
Forney-stylefactor(sub)graph
G
Nodes
V
Edges
E
F Variationalfreeenergy
G Generalisedfreeenergy
H Entropy
U Averageenergy
L Lagrangian
λ,ψ Lagrangemultipliers
µ Message
Table1: Overviewofnotationalconventions.
approximationtothe(intractable)exactposteriorp. Thenattheminimum,
F[q ]= logZ+KL[q p],
∗ ∗
− ∥
surprise posterior
divergence
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
withKLtheKullback-Leiblerdivergence.
2.2 Forney-StyleFactorGraphs
AForney-styleFactorGraph(FFG) =( , )canbeusedtographicallyrepresenta
G V E
factorisedfunction,withnodes andedges . Givenafactorisedmodel,
V E
f(s)= f (s ),
a a
a
(cid:89)∈V
4
Acronym Explanation
FEP FreeEnergyPrinciple
VFE VariationalFreeEnergy
AIF ActiveInference
EFE ExpectedFreeEnergy
GM GenerativeModel
FFG Forney-styleFactorGraph
GFE GeneralisedFreeEnergy
CFFG ConstrainedFFG
VMP VariationalMessagePassing
BFE BetheFreeEnergy
SSM State-SpaceModel
Table2: Overviewofacronyms.
edgesinthecorrespondingFFGrepresentvariablesandnodesrepresent(probabilistic)
relationshipsbetweenvariables(Forney,2001). Asanexample,considerthemodel
f(s)=f (s ,s )f (s ,s ,s )f (s ,s )f (s ),
a 1 3 b 1 2 4 c 2 5 d 5
forwhichtheFFGisdepictedinFig.1(left).
NotethatedgesinanFFGconnecttoatmosttwonodes. Therefore,equalityfactors
areusedtoeffectivelyduplicatevariablesforuseinmorethantwofactors. Technically,
the equality factor f (s ,s ,s ) = δ(s s )δ(s s ) constrains variables on
= i j k i j i k
− −
connectededgestobeequalthrough(DiracorKronecker)deltafunctions.
2.3 BetheLagrangianOptimisation
WecannowusethefactorisationofthemodeltoinduceaBethefactorisationonthe
variationaldistribution
q(s)≜ q
a
(s
a
) q
i
(s
i
)1
−
di,
a i
(cid:89)∈V (cid:89)∈E
withd thedegreeofedgei.
i
SubstitutingtheBethefactorisationintheVFE,theresultingBetheFreeEnergy
(BFE)thenfactorisesintonode-andedge-localcontributions. Asiscommoninproba-
bilisticnotation,weassumethatfactorsinthemodelandvariationaldistributionare
indexedbytheirargumentvariables(wherecontextallows). TheBFEthenfactorises
intonode-andedge-localcontributions,as
localfreeenergyF[qa]
localentropyH[qi]
q(s )
F[q]≜ E log a E [logq(s )](1 d ).
a (cid:88)∈V (cid:122) q(sa) (cid:20) (cid:125)(cid:124) f(s a ) (cid:21) (cid:123)− (cid:88) i ∈E(cid:122) − q(si) (cid:125)(cid:124) i (cid:123) − i
UsingLagrangemultipliers,wecanconverttheoptimisationproblemon toafree-
Q
formoptimisationproblemofaLagrangian,whereLagrangemultipliersenforcelocal
5
(e.g. normalisationandmarginalisation)constraints. Thefullylocalisedoptimisation
objectivethenbecomes
L[q]≜ F[q ] (1 d )H[q ]+norm[q]+marg[q], (1)
a i i
− −
a i
(cid:88)∈V (cid:88)∈E
withnormalisationandmarginalisationconstraints,
norm[q]= ψ q(s )ds 1 + ψ q(s )ds 1 , (2a)
a a a i i i
− −
a (cid:18)(cid:90) (cid:19) i (cid:18)(cid:90) (cid:19)
(cid:88)∈V (cid:88)∈E
marg[q]= λ (s ) q(s ) q(s )ds ds . (2b)
ia i i a a i i
− \
a (cid:88)∈V i ∈(cid:88)E (a)(cid:90) (cid:18) (cid:90) (cid:19)
TheLagrangianisoptimisedfor
q (s)=argminL[q],
∗
q
overtheindividualtermsinthevariationaldistributionfactorisation.
The belief propagation algorithm (Pearl, 1982) has been formulated in terms of
Bethe Lagrangian optimisation by message passing on factor graphs (Yedidia et al.,
2001). Additionalfactorisationofthevariationaldistributioninducesstructuredand
mean-fieldVariationalMessagePassing(VMP)algorithms(Dauwels,2007). Acompre-
hensiveoverviewofconstraintmanipulationandresultingmessagepassingalgorithms
isavailablein(S¸eno¨zetal.,2021).
2.4 ConstrainedForney-StyleFactorGraphs
AnFFGalonedoesnotunambiguouslydefineaconstrainedVFEobjective,andafull
expressionforalocalisedLagrangianalongsideanFFG(1)canbecomequiteverbose.
A visual representation may help interpret and disambiguate variational objectives
andconstraints. TheConstrainedFFG(CFFG)notationisintroducedindetailbyour
companionpaper(Koudahletal.,2023).
Inbrief,aCFFG(Fig.1,middle)annotatesanFFG(Fig.1,left)withbeadsand
bridgesthatimposeadditionalconstraintsontheBFELagrangianof(1)(e.g.,(struc-
tured)factorisationsofthevariationaldistributionanddataconstraints). TheCFFG
notationthenemphasisesconstraintsthatdeviatefromthe“standard”BFEconstraints.
Annotationsonthenodesrelatetothenode-localfreeenergies,andannotationson
edgesrelatetoedge-localentropies(1). Beadsonanodeindicateafactorisationofthe
correspondingnode-localvariationaldistribution. Bridgesthatconnectedgesthrough
nodesindicateastructuredvariationalfactor,wheretheconnectededge-variablesform
a joint distribution. A solid bead with inscribed delta on an edge indicates a data
constraint.
Asanexample,considertheCFFGofFig.1(middle),whichcorrespondstothe
Lagrangianof(1),whereannotationsimposeadditionalconstraintsasfollows.
Thebeadsatnodecindicateafulllocalfactorisationoverthevariablesonconnected
edges,
q (s ,s )≜q (s )q (s ).
c 2 5 c 2 c 5
6
1 2
f a s 1 f b s 2 f c f a s 1 f b s 2 f c f a s 1 → f b → s 2 f c
← ←
s 3 s 4 s 5 s 3 δ s 4 s 5 s 3 4 δ s 3 5 6
4 ↓ ↑s
5
f f
d d
f
d
Figure1: ExampleForney-stylefactorgraph(FFG)(left),constrainedFFG(CFFG)
(middle), and CFFG with indicated messages (right). The solid square indicates a
clampedvariable,andthesolidcircleadataconstraint. Sum-productandvariational
messagesareindicatedbywhiteanddarkmessagecirclesrespectively.
Thebeadandbridgeatnodebindicateastructuredfactorisationwheres ands belong
1 2
toajointfactor,
q (s ,s ,s )≜q (s ,s )q (s ).
b 1 2 4 b 1 2 b 4
Fornodea,themissingbeadatedge3indicatesthats isnotpartofthenode-localfree
3
energy. Togetherwiththeclampons ,thisdefinesthenode-localfreeenergy
3
q (s )
F[q ]≜E log a 1 .
a qa(s1) f (s ,s =sˆ )
(cid:20) a 1 3 3 (cid:21)
Finally,thedataconstraintatedge4connectswithnodeb,andenforces
q (s )≜δ(s sˆ ).
b 4 4 4
−
TheresultingmessagepassingschedulefortheCFFGexampleisindicatedinFig.1
(right). White circles indicate messages that are computed by sum-product updates
(Loeliger,2004)anddarkcirclesindicatevariationalmessageupdates(Dauwels,2007).
3 Review of Active Inference by Variational Message
Passing
InthissectionweworktowardsamessagepassingformulationofsyntheticAIF.We
startbyreviewingAIFandtheCFFGrepresentationforaGFEobjectiveforcontrol.
FurtherdetailsonvariationalobjectivesforAIFandepistemicconsiderationscanbe
foundin(Koudahletal.,2023).
3.1 ActiveInference
AIF defines an agent and an environment that are separated by a Markov blanket
(Kirchhoffetal.,2018). Ingeneral,ateachtimestep,theagentsendsanactiontothe
environment. Inturn,theenvironmentrespondswithanoutcomethatisobservedbythe
agent. Thegoaloftheagentistomanipulatetheenvironmenttoelicitdesiredoutcomes.
7
AGenerativeModel(GM)definesajointprobabilitydistributionthatrepresents
the agent’s beliefs about how interventions in the environment lead to observable
outcomes. Inordertoproposeeffectiveinterventions,theagentmustperformthetasks
ofperception,learningandcontrol.AIFperformsthesetasksby(approximate)Bayesian
inferenceontheGM,byinferringstates,parametersandcontrols,respectively.
3.2 GenerativeModelDefinition
Weassumethatanagentoperatesinadynamicalenvironment,anddefineasequence
of state variables z = (z ,z ,...,z ) that model the time-dependent latent state
0 1 T
of the environment. We also assume that the agent may influence the environment,
asmodelledbycontrolsu = (u ,...,u ),whichindirectlyaffectobservationsx =
1 T
(x ,...,x ).Finally,wedefinemodelparametersθ.Weassumethatparametersevolve
1 T
ataslowertemporalscalethanthestates,andcanthereforebeeffectivelyconsidered
time-independent.
TheGMthendefinesadistributionp(x,z,θ,u)thatrepresentstheagent’sbelief
abouthowcontrolsaffectstatesandobservationsundersomeparameters. Afirst-order
Markovassumptionthenimposesaconditionalindependencebetweenstates(Koller
andFriedman,2009). TheGMthenfactorisesasaState-SpaceModel(SSM),
T
p(x,z,θ,u)=p(z ) p(θ) p(x z ,θ) p(z z ,u ) p(u ), (3)
0 k k k k 1 k k
| | −
state parameterk (cid:89) =1 observation transition control
prior prior model model prior
(cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124)(cid:123)(cid:122)(cid:125)
whereweassumedaparameterisedobservationmodel. TheSSMof(3)canbegraphi-
callyrepresentedbytheFFGofFig.2.
u
k
... z k − 1 = z k ...
θ
x
k
... = ...
Figure2: AsinglesliceoftheForney-stylefactorgraphrepresentationofthegenerative
modelof(3). Stateandparameterpriorsnotdrawn.
8
u
k
u k z ↓ 4 5
z ↓ 4 5 k − 1
k − 1 ... → → = → ...
... → → = → ... ← ← ←z k
← ← ←z k 7 6
7 6
8 3
8 3 k t ↓ ↑
k <t ↓ ↑ ≥ θ
θ
1 9 x 1 9 x k
↑ ↓ δ k ↑ ↓
2
2
... → = → ...
← ← ... → = → ...
10 ← ←
10
Figure3:ConstrainedForney-stylefactorgraphrepresentationsforvariationalobjectives
onmodelsforpast(left)andfuturestates(right). Thedashedboxindicatesacomposite
structureforthegoal-observationsubmodel.
3.3 MessagePassing
InthissectionweformulatesyntheticAIFasamessagepassingprocedureonamodel
ofpastandfuturestates. Inferenceonamodelofpaststatesthenrelatestoperception
andlearning,whileinferenceonamodeloffuturestatesrelatestocontrol.
3.3.1 ModelofPastStates
With t the present time, we denote sequences of past variables x = x , z = z
<t <t
(includingz ),andu=u . Amodelofpaststatescanthenbeconstructedfrom(3),
0 <t
as
t 1
−
p(x,z,θ u)=p(θ)p(z ) p(x z ,θ)p(z z ,u ).
0 k k k k 1 k
| | | −
k=1
(cid:89)
InsteadofpresentingtheVFEobjectiveandconstraintsinformulas,wedrawthe
CFFGforpaststatesinFig.3(left). Theindicatedmessagepassingscheduledefinesa
forwardandbackwardpassontheCFFG.Theforwardpass,comprisedofmessages
1 – 5 ,representsinferenceforperceptionwhere(hierarchical)statesareestimated
byafilteringprocedure. Thecombinedforward-backwardpassrepresentsinferencefor
learning,whereallpastinformationisincorporatedtoinferaposterioroverparameters
byasmoothingprocedure. Theseposteriorscanthenbeusedas(empirical)priorsin
themodeloffuturestates.
9
3.3.2 ModelofFutureStates
AIFforcontrolinfersaposteriorbeliefoverpoliciesfromafreeenergyobjectivethatis
definedwithrespecttoamodeloffuturestates.Wedefinesequencesoffuture(including
present)variablesx=x ,z =z (includingz ),andu=u .Becausefuture
t t 1 t 1 t
≥ ≥ − − ≥
outcomesarebydefinitionunobserved,weincludegoalpriorsonthefutureobservation
variables. FromtheGMof(3),weconstructthemodeloffuturestates,as
T
f(x,z,θ,u)=p(z )p(θ) p(x z ,θ)p(z z ,u )p(u )p˜(x ),
t 1 k k k k 1 k k k
− | | −
k=t
(cid:89) goal
prior
(cid:124)(cid:123)(cid:122)(cid:125)
withT alookaheadtimehorizon. The(empirical)stateandparameterpriorfollowfrom
messagepassingforperceptionandlearning,respectively. Notethatthemodeloffuture
statesisunnormalisedasaresultofsimultaneousconstraintsonxbytheobservation
modelsandthegoalpriors.
WethendefineaVFEobjectiveforcontrol,as
q(x,z,θ,u)
F[q]≜E log
q(x,z,θ,u)
f(x,z,θ,u)
(cid:20) (cid:21)
q(u) q(x,z,θ u)
=E log +E log | ,
q(u)
p(u)
q(x,z,θ
|
u)
f(x,z,θ u)
(cid:20) (cid:20) | (cid:21)(cid:21)
F[q;u]
which(undernormalisation)isminimise(cid:124)dby (cid:123)(cid:122) (cid:125)
p(u)σ( F(u))
q ∗ (u)= − ,
p(u)σ( F(u))
u −
withσasoftmaxfunctionand (cid:80)
F(u)=minF[q;u],
q
∈Q
the optimalVFEvalueconditioned onthe policy u. This solutionthus constructsa
posterioroveraselectionofpoliciesbyevaluatingtheirrespectivefreeenergies. In
thecurrentpaperwechooseauniformpolicypriorandsimplyselectthepolicywith
highestposteriorprobability. Alternativestrategiesintroduceanattentionparameter
thatcanalsobeoptimisedunderthevariationalscheme(Fristonetal.,2015),andwhere
thepolicyissampledfromtheposterior. Inthisalternativescheme,thecurrentpolicy
selectionstrategythencorrespondswithafixed,largevaluefortheattentionparameter.
Theconditioningofthevariationaldistributiononuisimpliedbytheconditioning
oftheGMonu. Technically,thevariationaldistributionisalwaysconditionedonthe
valuesonwhichtheunderlyingmodelisconditioned. Thereforeweomitthisexplicit
conditioninginthevariationaldensity.
Wenowimposeafactorisationconstraint
T
q(x,z,θ)≜q(z)q(θ) q(x ),
k
k=t
(cid:89)
10
andsubstituteonlytheexpectationtermswiththeirrespectiveobservationmodels
q(x ) p(x z ,θ)inexp. termsforallk t. (4)
k k k
→ | ≥
This so-called p-substitution is introduced by our companion paper (Koudahl et al.,
2023),andtransformstheVFEtoaGeneralisedFreeEnergy(GFE)objective(Parrand
Friston,2019),
q(x)q(z)q(θ)
G[q;u]=E log . (5)
p(x
|
z,θ)q(z)q(θ)
f(x,z,θ u)
(cid:20) | (cid:21)
For convenience, we write p(xz,θ) = T p(x z ,θ) and q(x) = T q(x ).
| k=t k | k k=t k
WhenwesubstituteafactorintheexpectationtermoftheVFE(4)wewriteGinstead
(cid:81) (cid:81)
ofF forclarity.
MinimisationoftheGFEmaximisesamutualinformationbetweenfutureobserva-
tionsandstates(ParrandFriston,2019). Theagentistheninclinedtochoosepolicies
thatresolveinformationaboutexpectedobservations,leadingtoepistemicbehaviour.
AmathematicalexplorationofepistemicpropertiesinCFFGsisavailablein(Koudahl
etal.,2023).
Inthispaperwewillviewthep-substitution(4)aspartoftheoptimisationconstraints
. Wethendenotethep-substitutionbyasquarebeadintheCFFG,asdrawninFig.3
Q
(right). As a convention, the square bead is drawn at the factor that is substituted
(Koudahletal.,2023).
4 General GFE-Based Message Updates
Inthemodelforfuturestates,thegoalpriorandobservationmodelimposesimultaneous
constraintsontheobservationvariable. InthecorrespondingCFFG,thisconfiguration
is modelled by two facing nodes. In this section we derive the general GFE-based
messageupdatesforapairoffacingnodes. Weexpressthelocaloptimisationproblem
asaLagrangian. Usingvariationalcalculuswethenderivelocalstationarysolutions,
fromwhichweobtaingeneralupdateexpressionsforGFE-basedmessages.
4.1 GoalandObservationModel
Herewedefineageneralisedgoalandobservationmodel,whichdefinesimultaneous
constraintsontheobservationvariablex. Theobservationmodelp(xz,θ)consists
|
ofstateszandparametersθ. Thegoalpriorextendstoagoalmodelp˜(xw,ϕ),with
|
stateswandparametersϕ,expandingtherangeofapplicability.
The CFFG of Fig. 4 draws the observation and goal model as two facing nodes.
Crucially,fromtheperspectiveoftheCFFGtheroleofthesenodesinthebiggermodel
isirrelevant,expandingtherangeofapplicabilitybeyondobservationandgoalmodels.
Moreover,thefacingnodesarecontainedbyacompositestructurethatactsasaMarkov
blanketforcommunicationwiththeremaininggraph.
11
z
...
↑ 2
.
θ . . p
x
.
ϕ . . p˜
1
...
↓
w
Figure4:CFFGoftwofacingnodeswithindicatedp-substitutionandmessages.Ellipses
indicateanarbitrarynumberofadjacentedges(possiblyzero),withbeadsindicatinga
jointvariationaldistributionovertheadjacentedges.
TheCFFGofFig.4thendefinesthefreeenergyobjective,
q(x,z,θ,w,ϕ)
F[q]=E log , (6)
q(x,z,θ,w,ϕ)
p(xz,θ)p˜(xw,ϕ)
(cid:20) | | (cid:21)
wherebeadsimposethevariationaldistributionfactorisation
q(x,z,θ,w,ϕ)≜q(x)q(z)q(θ)q(w)q(ϕ), (7)
andwherethesquarebeadsubstitutes
q(x) p(xz,θ), (8)
→ |
intheexpectationterm.
4.2 LocalLagrangian
Afterapplicationofconstraints(7)and(8)to(6),weobtainthelocalGFE,
q(x)q(z)q(θ)q(w)q(ϕ)
G[q]=E log .
p(x | z,θ)q(z)q(θ)q(w)q(ϕ) p(xz,θ)p˜(xw,ϕ)
(cid:20) | | (cid:21)
TofindlocalstationarysolutionstothisGFE,weintroduceLagrangemultipliersthaten-
forcethenormalisationandmarginalisationconstraintsin .Thenode-localLagrangian
Q
thenbecomes
L[q]=G[q]+norm[q]+marg[q], (9)
withs x,z,θ,w,ϕ agenericvariable,underthenormalisationandmarginalisa-
i
∈{ }
tionconstraintsimposedby(2).
ThisLagrangianisthenoptimisedunderafree-formvariationaldensity
q =argminL[q],
∗
q
forallindividualfactorsinthevariationaldistributionfactorisation.
12
4.3 LocalStationarySolutions
Wearenowpreparedtoderivethestationarypointsofthenode-localLagrangian(9).
Westartbyconsideringthenode-localLagrangianasafunctionalofthevariational
factorq .
x
Lemma1. StationarypointsofL[q]asafunctionalofq x ,
L[q ]=G[q ]+ψ q(x)dx 1 +C , (10)
x x x x
−
(cid:20)(cid:90) (cid:21)
whereC collectsalltermsindependentfromq ,aregivenby
x x
q ∗ (x)=E q(z)q(θ) [p(xz,θ)] . (11)
|
Proof. TheproofisgivenbyAppendixA.1.
Next, we derive the stationary points of (9) as a functional of q . Note that, by
z
symmetry,asimilarresultappliestoq .
θ
Lemma2. StationarypointsofL[q]asafunctionalofq z ,
L[q ]=G[q ]+ψ q (z)dz 1
z z z z
−
(cid:20)(cid:90) (cid:21)
+ λ (z ) q(z ) q (z)dz dz +C , (12)
ip i i z i i z
− \
i ∈(cid:88)E (z)(cid:90) (cid:20) (cid:90) (cid:21)
whereC collectsalltermsindependentfromq ,aregivenby
z z
f˜(z) µ (z )
q ∗ (z)= f˜(z) zi∈ z µ ip (z i )dz , (13)
(cid:81)zi∈ z ip i
with (cid:82) (cid:81)
p(xz,θ)f˜(x)
f˜(z)=exp E log | (14a)
(cid:32) p(x | z,θ)q(θ) (cid:34) q(x) (cid:35)(cid:33)
f˜(x)=exp E [logp˜(xw,ϕ)] . (14b)
q(w)q(ϕ)
|
(cid:0) (cid:1)
Proof. TheproofisgivenbyAppendixA.2.
Finally,wederivethestationarypointsof(9)withrespecttoq . Again,bysymme-
w
try,asimilarresultfollowsforq .
ϕ
13
Lemma3. StationarypointsofL[q]asafunctionalofq w ,
L[q ]=G[q ]+ψ q(w)dw 1
w w w
−
(cid:20)(cid:90) (cid:21)
+ λ (w ) q(w ) q(w)dw dw +C , (15)
ip˜ i i i i w
− \
i ∈(cid:88)E (w)(cid:90) (cid:20) (cid:90) (cid:21)
whereC collectsalltermsindependentfromq ,aregivenby
w w
f˜(w) µ (w )
q ∗ (w)= f˜(w) wi∈ w µ ip˜ (w ) i dw , (16)
(cid:81)wi∈ w ip˜ i
with (cid:82) (cid:81)
f˜(w)=exp E [logp˜(xw,ϕ)] . (17)
q(x)q(ϕ)
|
(cid:0) (cid:1)
Proof. TheproofisgivenbyAppendixA.3.
4.4 MessageUpdates
InthissectionweshowthatthestationarysolutionsofSec.4.3correspondtothefixed
pointsofafixed-pointiterationscheme. Wefirstderivetheupdateruleformessage
µ (w ),withw w,wherethebulletindicatesthearbitrary(possiblyno)connected
j j j
• ∈
node.Weindicatemessagesofspecialinterestbyacirclednumber.Thecurrentmessage
isindicatedby 1 inFig.4. Bysymmetry,asimilarresultappliestoϕ j ϕ.
∈
Theorem1. Giventhestationarypointsofthenode-localLagrangianL[q],the
stationarymessageµ (w )correspondstoafixedpointoftheiterations
j j
•
µ(n+1)(w )= f˜(w) µ(n)(w )dw , (18)
j j ip˜ i j
• \
(cid:90) w w i (cid:89)i ̸ =∈w w j
withnaniterationindex,andf˜(w)givenby(17).
Proof. TheproofisgivenbyAppendixB.1.
Wenowderivetheupdateruleformessageµ j (z j ),withz j z,indicatedby 2
• ∈
inFig.4. Weapplythesameprocedureasbefore. Bysymmetry,asimilarresultapplies
toθ θ.
j
∈
14
Theorem2. Giventhestationarypointsofthenode-localLagrangianL[q],the
stationarymessageµ (z )correspondstoafixedpointoftheiterations
j j
•
µ(n+1)(z )= f˜(z) µ(n)(z )dz , (19)
j j ip i j
• \
(cid:90) z z i (cid:89)i ̸ =∈z z j
withnaniterationindex,andf˜(z)givenby(14),
Proof. TheproofisgivenbyAppendixB.2.
4.5 ConvergenceConsiderations
Whiledirectapplicationof(19)workswellinsomecases,thismessageupdatemay
alsoyieldalgorithmsforwhichtheGFEactuallydivergesoveriterations. Thisperhaps
counter-intuitiveeffectthenhasmajorimplicationsforthepracticalimplementationof
(19).
This divergence issue relates to a subtlety about what is actually proven by our
theorems. Whileourtheoremsprovethatthestationarymessagescorrespondtofixed-
pointsofthenode-localLagrangian,thetheoremsdonotguaranteethatiterationsof
thefixed-pointequationsactuallyconvergetosaidfixed-points. Inordertoimprove
convergence,wederiveanalternativemessageupdateruleformessage 2 below.
Corollary1. Giventhestationarypointsofthenode-localLagrangianL[q],the
stationarymessageµ (z )correspondsto
j j
•
q(z;ν )dz
∗ j
µ j (z j ) \ , (20)
• ∝ (cid:82) µ jp (z j )
withν asolutionto
∗
f˜(z;ν) µ (z )
q(z;ν)= ! zi∈ z ip i , (21)
f˜(z;ν) µ (z )dz
(cid:81)zi∈ z ip i
whereq isparameterisedbysta (cid:82) tisticsν, (cid:81) andwheref˜(z;ν)isgivenby(14),with
z
q(x;ν)=E [p(xz,θ)] , (22)
q(z;ν)q(θ)
|
whichisparameterisedbyν througharecursivedependenceonq .
z
Proof. TheproofisgivenbyAppendixB.3.
TheresultofCorollary1offersanexpressionforthestationarybeliefasafunction
ofparametersν. Locallyoptimalparametersν cannowbefoundthroughNewton’s
∗
method.
15
5 Application to a Discrete-Variable Model
In this section we apply the general message update rules of Sec. 4.4 to a specific
discrete-variablemodelthatisoftenusedinAIFpractice. Usingthegeneralresultswe
derivemessagesonthisspecificmodel.
5.1 Goal-ObservationSubmodel
Asconventionweuseuprightboldnotationforvectorsandmatrices. Weconsidera
discrete state z and observation variable x . To conveniently model these
∈ Z ∈ X
variableswithcategoricaldistributions,weconvertthemtoaone-hotrepresentation,
withx = e (x)thestandardunitvectoron withx = 1attheindexforx,and0
i
otherwise. ( X Andsimilarforz). Thenotation X at(xρ) = ρxi thenrepresentsthe
C | i i
categoricaldistributiononx(one-hot)withprobabilityvectorρ. Werelatethestateand
(cid:81)
observationvariablesbytransitionprobabilitymatrixA . ThecolumnsofA
∈X ×Z
arenormalisedsuchthatAzrepresentsaprobabilityvector.
With notation in place, we define the observation model and goal prior for con-
strainedsubmodel,
p(xz,A)= at(xAz)
| C |
p˜(xc)= at(xc) ,
| C |
asdrawninFig.5(left).
5.2 GFE-BasedMessageUpdates
TheCFFGofFig.5(left)definesthelocalGFEobjective,withanincomingmessage
µ (z)= at(zd). Wedenoteby
D C |
h(A)= diag(ATlogA),
−
thevectorofentropiesofthecolumnsoftheconditionalprobabilitymatrixA.
Asanotationalconvention,inthiscontextweuseanover-barshorthandtodenote
anexpectation,i.e.,z=E [z]. ThetableinFig.5(right)summarisestheresulting
q(z)
messageupdatesandaverageenergy,with
ξ(A)=AT logc log Az h(A)
− −
T
ρ=A (cid:0)logc log(cid:0)Az(cid:1)(cid:1) h(A).
− −
Thefullderivationsareavailablein(cid:0)AppendixC(cid:0). (cid:1)(cid:1)
Unfortunately,message 3 doesnotexpressa(scaled)standarddistributiontype
as a function of A. Therefore we pass the log-message as a function directly and
use importance sampling to evaluate expectations of q(A) (Akbayrak et al., 2021).
EstimationoftheobservationmatrixthroughimportancesamplingthusrendersGFE
optimisationastochasticprocedure. Asaresult,theGFEmayfluctuateoveriterations.
Forpolicyselection,wethereforeaveragetheGFEoveriterations,afterashortburn-in
period(teniterationsinthiscase).
16
D
↓
z 2 µ (c) ir cAz+1
↑ 1 ∝D |
A µ (z) at((cid:0)zσ(ρ)) (cid:1)
T 2 ∝C |
!
← Solve: z =σ(ρ(z)+logd)
3 x
µ (z) at(zσ(logz logd))
2 ∝C | ∗ −
at logµ (A) =zTξ(A)
C 3
1 ↓ c U x = − zTρ
Figure 5: Discrete-variable submodel with indicated constraints (left) and message
updates(right),withσasoftmaxfunction. Updatesformessagetwoindicatethedirect
andindirectcomputationrespectively.
Themessageupdatesforadata-constrainedobservationvariable(Fig.3,left)reduce
tostandardVMPupdates,asderivedby(vandeLaar,2019,App.A).
6 Experimental Setting
InthissectionwedescribeaT-mazetaskthatservesasaclassicalsettingforinvestigating
epistemicbehaviour. Thesetupcloselyfollowsthedefinitionsin(Fristonetal.,2015).
6.1 T-MazeLayout
TheT-mazeconsistsoffourpositions = (O,C,L,R)asillustratedinFig.6. The
P
agentstartsatpositionO,withtheobjectivetoobtainarewardthatislocatedeither
inarmLorarmR. Thehiddenrewardlocationisrepresentedby = (RL,RR)for
R
positionLandRrespectively. VisitingthecuepositionC revealstherewardlocationto
theagent.
L R
O
C
Figure6: LayoutoftheT-mazewithstartingpositionO,cuepositionC andpossible
rewardarmsLandR.
Theagentisallowedtwomoves(T =2),andaftereachmovetheagentobservesan
17
CL CR RW NR
P R
O RL 0.5 0.5 . .
RR 0.5 0.5 . .
L RL . . α 1 α
−
RR . . 1 α α
−
R RL . . 1 α α
−
RR . . α 1 α
−
C RL 1 . . .
RR . 1 . .
Table 3: Probabilities for outcomes ( ) as related to agent position ( ) and reward
O P
position( ).
R
outcome =(CL,CR,RW,NR)thatindicates
O
• CL: TherewardislocatedinarmL;
• CR: TherewardislocatedinarmR;
• RW: Therewardisobtained;
• NR: Norewardisobtained.
Theseoutcomesstochasticallyrelatetotheagentpositionandrewardlocationaslisted
inTable3,withαtherewardprobabilityuponvisitingthecorrectarm.
Toensurethattheagentobservesrewardnomorethanonce,amovetoeitherreward
arm is followed by a mandatory move back to the starting position (irrespective of
whether reward was obtained or not). An epistemic agent would first visit the cue
positionandthenmovetotheindicatedrewardposition.
6.2 T-MazeModelSpecification
HerewedefineaGMfortheT-mazeenvironment.Theobservationvariablesx
k
∈O×P
representtheoutcomeattheagent’spositionattimek(sixteenpossiblecombinations).
An observation matrix A then relates x to a state z . The state variable
k k
∈ P ×R
representstheagentpositionattimek,combinedwiththehiddenrewardposition(eight
possiblecombinations).
Thecontrolu representstheagent’sdesirednextposition(fourpossibilities).
k
∈P
ThecontrolthenselectsthetransitionmatrixB .
uk
As before, the GM represents categorical variables by one-hot encoded vectors.
WewillsimulatetheT-mazeforS consecutivetrials. Thegoal-constrainedmodelfor
simulationsthenbecomes
T
f (x,z,Ac,u)=p(z )p (A) p(x z ,A)p(z z ,u )p˜(x c ), (23)
s 0 s k k k k 1 k k k
| | | − |
k=1
(cid:89)
wherethe(empirical)parameterpriorisindexedbytrialnumbers.
18
Wespecifythesub-models,
p(z )= at(z d)
0 0
C |
p (A)= ir(AA )
s s 1
D | −
p(x z ,A)= at(x Az )
k k k k
| C |
p(z z ,u )= at(z B z )
k
|
k
−
1 k
C
k
|
uk k
−
1
p˜(x c )= at(x c ) ,
k k k k
| C |
wheretheDirichletpriorontheobservationmatrixassumesindependentcolumns.
Forthestatepriorweendowtheagentwithknowledgeaboutitsinitialposition,but
ignoranceabouttherewardposition
d=(1,0,0,0)T (0.5,0.5)T,
⊗
with theKroneckerproduct. Sincethegoaloftheagentistoobtainreward,wedefine
⊗
thegoalpriorstatistic
c =σ (0,0,c, c)T (1,1,1,1)T ,
k
− ⊗
withctherewardutility(identi (cid:0) calforbothtimesk). (cid:1)
ThepriorforthetransitionmatrixencodesthepriorknowledgethatpositionOdoes
notofferanydisambiguation, buttheotherpositionsmight. Formally, A definesa
0
block-diagonalmatrix,withblocks
10 10 1 ϵ
10 10 ϵ 1
A 0,O = ϵ ϵ  A 0, L,R,C = ϵ ϵ  ,
{ }
ϵ ϵ ϵ ϵ
   
   
suchthat
A =A A A A ,
0 0,O 0,L 0,R 0,C
⊕ ⊕ ⊕
with ablock-diagonalconcatenationandϵ=0.1arelativelysmallvalue,alsoonall
⊕
off-blockentries.
Finally,thecontrol-dependenttransitionmatricesaredefinedas
1 1 1 1 . 1 1 .
. . . . 1 . . 1
B O = . . . .  ⊗ I 2 B L = . . . .  ⊗ I 2
. . . . . . . .
   
. 1 1 . . 1 1 .
. . . . . . . .
B R = 1 . . 1  ⊗ I 2 B C = . . . .  ⊗ I 2 ,
. . . . 1 . . 1
   
   
withI the2 2identitymatrix. TheCFFGfortheT-mazeisshowninFig.7.
2
×
19
B B
u1 u2
d z 0
at T = T
C
z 1 z 2
T T
δx
1
x
2
at at
C C
c c
1 2
A s 1 A
− ir =
D
Figure7: ConstrainedForney-stylefactorgraphfortheT-mazewithtime-dependent
constraintsatt=2.
6.3 Perception-ActionCycle
Theperception-actioncyclefortheT-mazesettinginthecurrentpaperextendsuponthe
formulationof(ParrandFriston,2019),wherepastobservationscollapsethevariational
distributionfortheobservationmodeltoadeltafunction. IntheCFFGformulation,the
perception-actioncyclecanbevisualisedasaprocessthatmodifiesconstraints over
t
Q
time(Fig.7).
At the initial time t = 1 no observations are available, and we initialise the
perception-action cycle with p-substitution constraints at all times ( ). As actions
1
Q
areexecutedandobservationsbecomeavailable(1<t<T),data-constraintsreplace
thep-substitutionsontheobservationvariables. Whenthetimehorizonisreachedand
allobservationsareavailable(t=T),inferencecorrespondswithlearningaposterior
belief over parameters. The posterior q (A) is then used as prior p (A) for the
s s+1
nextsimulationtrial. Theperception-actioncyclewithtime-dependentconstraintsthus
unifiesthetasksofperception,controlandlearningunderasingleGMandschedule,
seealso(vandeLaaranddeVries,2019;vandeLaaretal.,2022).
7 Simulations
InthissectionweconsiderasimulationoftheT-mazeexperimentalsetting(CFFGof
Fig.7)andtwoextensionsthereupon.TheinitialT-mazesimulationconsidersperception
andlearningfromrepeatedtrials,wherewecomparebehaviourbetweenaGFEand
BetheFreeEnergy(BFE)basedagent. Afirstextensionintroducesahyperprioron
the goal statistics and learns a posterior over goals. A second extension considers
a bargaining setting, where a primary agent (navigating the T-maze) may purchase
20
GFE BFE
Figure8: Generalisedfreeenergyovertrialssasgroupedbytimet(top)withindicated
winorloss(middle). AwinindicatesthataRW wasobservedonthattrial(oneither
move). BottomplotsshowlearnedstatisticsfortheobservationmatrixA A ,as
S 0
−
groupedperposition.
informationfromasecondaryagentforashareoftherewardprobability.
1
Simulations are performed with the reactive message passing toolbox RxInfer
(BagaevanddeVries,2021).
7.1 PerceptionandLearning
Fortheinitialsimulationwesettherewardprobabilityα=0.9andrewardutilityc=2,
andexecutetheperception-actioncycleforS =100consecutivetrialsontheCFFGof
Fig.7. TheresultingminimumpolicyGFEovertrials,groupedbytime,isplottedin
Fig.8(topleft). ItcanbeseenthattheGFEdecreasesoverall,astheagentimprovesits
modeloftheenvironment. Withanimprovedmodelbetteractionscanbeproposed,and
theagentlearnstofirstseekthecueandthenvisittheindicatedrewardarm.
1
Sourcecodeforthesimulationsisavailableathttps://github.com/biaslab/LAIF.
21
ThemiddleplotsindicatewhethertheagenthasobservedaRW duringatrial(on
either move) which we designate as a win. We consider the trial a loss if the agent
hasfailedtoobserveaRW onbothmoves. Thefreeenergyplotshowsseveralspikes
during the learning phase (top left, t = 3). These spikes coincide with unexpected
losses(Fig.8,middleleft). Namely,aftersomemovestheagenthaslearnedtoexploit
thecuepositionC. However,evenwhentheagentvisitstheindicatedrewardarm,an
(unexpected)NRobservationmaystilloccurwithprobabilityα 1.
−
Afteralltrialshavecompleted,wecaninspectwhattheagenthaslearned. InFig.8
(bottomleft)weplotthereinforcedstatisticsA A ,asgroupedperagentposition.
S 0
−
Each sub-plot then indicates the learned interaction between outcome and reward
positionattheindicatedagentposition. TheGFE-basedagenthasconfidentlylearned
thatpositionC offersdisambiguationabouttherewardcontext,andthatpositionsL
andRofferacontext-relatedrewardRW (andsometimesNR). Thisknowledgethen
enablestheagenttoconfidentlypursueepistemicpolicies.
WecomparetheGFE-basedagentwithanagentthatinternalisesanobjectivewithout
substitutionconstraint. Specifically,intheCFFGofFig.7,thesquarethatindicatesa
substitutionconstraintisreplacedbyacircle. Thissimpleadjustmentthenreducesthe
GFEobjectivetoa(structured)BetheFreeEnergy(BFE)objective,whichisknownto
lackepistemicqualities(Schwo¨beletal.,2018;vandeLaaretal.,2022). Weexecute
thesameexperimentalprotocolasbeforeandplottheminimalfreeenergiesinFig.8
(topright).
TheBFE-basedreferenceagentfailstoidentifyepistemicmodesofbehaviour. The
specificchoiceofpriorfortheobservationmatrixpreventsanyextrinsicinformation
(atleastinitially)frominfluencingpolicyselection. Bylackofanepistemicdrive,the
BFE-basedagentthenstickstopoliciesthatconfirmitspriorbeliefs,withoutexploring
possibilitiestoexploitavailableinformationintheT-mazeenvironment(Fig.8,bottom
right).
We evaluate the reliability of the GFE-based agent by simulating R = 100 runs
withS =30trialseach. AhistogramofthenumberofwinsperrunisplottedinFig.9
(left). Thishistogramsuggestsabi-modaldistributionwithalargemassgroupedto
therightandasmallermassinthemiddle. Forreference,dashedcurvesindicateideal
performanceforagentsthatalreadyknowA=Aˆ (accordingtoTable3)fromthestart.
ForagentsthatmustfirstlearnA,deviationsfromidealperformanceareexpected. The
smallermiddlemassthenindicatesthatGFEoptimisationoffersnosilverbulletfor
simulatingfullysuccessfulepistemicagents. Namely,forsomechoicesofinitialisation,
theGFE-basedagentmaystillbecomestuckinlocaloptima.
ThewinaveragepertrialisplottedinFig.9(right),whichindicatesthataGFE-based
agent(onaverage)quicklylearnstoexploittheT-mazeenvironment.
7.2 LearningGoals
ToillustratethemodularityofthemessagepassingapproachtosyntheticAIF,afirst
extensionmodifiestheT-mazesimulationtolearnthegoalparameters(insteadofthe
observationmodelparameters). Inthissimulationwefixtheobservationmatrixtothe
knownconfigurationofTable3. Formally,wesetp (A)=δ(A Aˆ)andplaceahyper
s
−
prioronthegoalparametersinstead. ThehyperpriorextendstheGMof(23)witha
22
Figure9: AggregateresultsforR=100distinctrunswithS =30trialseach. Figures
showthenumberofwinsperrun(left)andthewinaveragepertrial(right). Dashed
curvesindicateidealperformanceofanagentwithknownAforcomparison.
Figure10:Learnedvaluesforthegoalpriorstatisticsc c fork =1(topleft)and
k,s k,0
−
k =2(bottomleft). Verticalaxislabelsindicatethe(preferred)outcomeperposition.
GeneralisedFreeEnergy(GFE)perpolicyisplottedontheright.
factor
p (c )= ir(c c ) .
s k k k,s 1
D | −
WesimulateS =10trials,wheretheposteriorsq (c )areusedas(hyper)priorsforthe
s k
nexttrialp (c ). Thegoalparametersareinitialisedwithapreferenceforreward,
s+1 k
c =(ϵ,ϵ,10,ϵ)T (1,1,1,1)T,
k,0
⊗
withϵasmallvalue. Theresultsforthereinforcedstatisticsc c andGFEsfor
k,s k,0
−
thepoliciesovertrialsareplottedinFig10. Onlythestatisticsthatwerereinforcedby
learningareplotted.
TheresultsinFig.10illustratehowtheagentconsolidatestheoutcomesofepistemic
policiesinthegoalstatistics. Forthegoalatthefirsttimestepc ,theagentlearnsto
1,s
preferavisittothecueposition. Forthesecondtimestepc ,theagentlearnstoprefer
2,s
23
avisittotherewardposition. Thisresultsinalearnedextrinsicpreferenceforepistemic
policies,asillustratedbythedivergingpolicyGFEsontheright.
7.3 ThePriceofInformation
Toillustratethescalabilityofourapproach,asecondextensionmodifiestheT-maze
simulationtoamulti-agentbargainingsetting,whereaprimaryagent(thebuyer)pur-
chasesitscue(indicatingtherewardposition)fromasecondaryagent(theseller). The
buyernavigatestheT-mazeandcanonlyaccessthecuethroughtheseller. Conversely,
thesellercanonlyaccessrewardthroughthebuyer. Thecurrencyoftheirexchangeis
therewardprobabilityα ,setbythesellerforeachtrials. Byvisitingthecueposition
s
C,thebuyer(primaryagent)agreestopaytheseller(secondaryagent)ashare1 α
s
−
oftherewardprobability,leavingα forthebuyer. Thepriceisdeterminedbytheseller
s
fromasetofLpotentialoffersα . Ifthebuyer(primaryagent)choosestoavoid
s
∈A
C (forfeitingthecue)thenthesellerreceivesnothingandthebuyerreceivesthefull
share(α =1)iftherewardisobtained.
s
Weextendthestatevariablez ofthebuyer(navigatingtheT-maze)withabargain
k
state =(CV,NC),indicatingtherespectiveacceptanceandrejectionoftheseller’s
C
offer. Thebuyer’sstatethenbecomesz . Thebuyerthenstartseach
k
∈ C ×P ×R
simulationtrialinanNC bargainstate,
d˜ =(0,1)T d.
⊗
Weannotatetheparametersofthebuyer’smodelbyatilde. Thebuyer’sobservation
matrixbecomesoffer-dependentandconcatenatestheknownobservationmatrices(from
Table3)fortherespectiveCV andNC states,
A˜(α )= Aˆ(α ) Aˆ(1) ,
s s
(cid:104) (cid:105)
where the latter term encodes full reward when the cue is forfeited. Transitions are
duplicatedforthebargainstates(exceptforthecueposition),
B˜ =I B .
O,L,R 2 O,L,R
{ } ⊗ { }
The transition to the cue position B˜ assumes a similar structure, but switches the
C
bargain state from NC to CV, marking acceptance of the offer. The (fixed) goal
statisticsremain,c˜ = c . TheCFFGforthebuyerthenfollowsthefamiliarFig.7,
k k
withtheextendedparametersandfixedobservationmatrixp (A)=δ(A A˜(α )).
s s
−
The seller’s CFFG is less conventional and illustrates the freedom of choice by
virtueofthemodulargoal-observationsubmodel. Weindicatevariablesintheseller’s
model by a prime. A conditional probability matrix A′ then relates the offer α
s
to
anoutcomex ,indicatingacceptanceorrejectionoftheoffer. Weconstructthe
′s
∈ C
generativemodelfortheseller,
f
s′
(x
′s
,A′,c
′s|
α
s
)=p
s
(A′)p(x
′s|
α
s
,A′)p˜(x
′s|
c
′s
)p(c
′s|
α
s
),
24
α
s
=
A′s
1 A′
− ir T
D
x
′s
at
C
c
′s
σ
Figure11: ConstrainedForney-stylefactorgraphfortheseller.
withsub-models
p
s
(A′)=
D
ir A′
|
A′s
− 1
p(x ′s| α s ,A′)=
C
at(cid:0)x ′s| A′α s (cid:1)
p˜(x c )= at(x c )
′s| ′s
C
(cid:0) ′s| ′s (cid:1)
p(c α )=δ(c σ((1 α )(c, c)T)).
′s| s ′s−
−
s
−
Here,thelattersub-modelencodesanoffer-dependentpreferencethatrewardslow-ball
offers. TheCFFGfortheseller’smodelisshowninFig.11. Parametersareinitialised
withA′0 a2
×
Lmatrixof(small)ϵ’s.
Wesimulateanestedperception-actioncycle,whereoneachtrialthesellersendsan
action(offer)αˆ totheprimaryagent,andwherethebuyersendsactions(moves)uˆ
s t
totheT-mazeenvironment. Conversely,theT-Mazeenvironmentreportsobservations
xˆ tothebuyer,whichinturnreportanobservationxˆ totheseller. Thissettingthus
t ′s
definestwonestedMarkovblankets,wherethesellercanonlyinteractwiththeT-maze
throughthebuyer. Alsonotethedifferenceintemporalscales;thebuyerexecutestwo
actions(T =2)foreachactionoftheseller. Attheendofeachtrial,theposteriorfor
q
s
(A′)issetasthepriorp
s+1
(A′)forthenexttrial.
ResultsforS = 30trials,withc = 2andL = 5potentialofferlevelsareplotted
inFig.12. ThefigureplotstheGFEofpotentialoffersovertrials,togetherwiththe
offersmadebytheseller(circles)andrejectedbythebuyer(crossedcircles). After
some initial exploration, the seller settles on a strategy that consistently (with some
exploration)makesthelowestofferthatisstilllikelytobeaccepted.
25
Figure12: Generalisedfreeenergy(grayscale,inbits)ofpotentialoffersbytheseller,
withexecutedoffersindicatedbywhitecircles. Crossesmarkoffersthatwererejected
bythebuyer.
8 Related Work
TheForney-styleFactorGraph(FFG)notationwasfirstintroducedby(Forney,2001).
Theworkof(Loeliger,2007)offersacomprehensiveintroductiontomessagepassing
onFFGsinthecontextofsignalprocessingandestimation.
Thebeliefpropagationalgorithmwaspioneeredby(Pearl,1982),andwasfurther
formalisedintermsofvariationaloptimisationby(Kschischangetal.,2001;Yedidia
etal.,2001). VariationalMessagePassing(VMP)wasintroducedby(WinnandBishop,
2005)andformulatedinthecontextofFFGsby(Dauwels,2007).Amorerecentviewon
constrainedfreeenergyoptimisationcanbefoundin(Zhangetal.,2017). Furthermore,
(S¸eno¨z et al., 2021) offers a comprehensive overview of common constraints and
resultingmessagepassingupdatesonfactorgraphs.
TowardsamessagepassingformulationofActiveInference(AIF),(ParrandFriston,
2019)proposedaGeneralisedFreeEnergy(GFE)objective,whichincorporatesprior
beliefsonfutureoutcomesaspartoftheGenerativeModel(GM).Thecurrentpaper
reformulates these ideas in a visual CFFG framework, which explicates the role of
backwardmessagesinGFEoptimisation(seealsoourcompanionpaper(Koudahletal.,
2023)). Inspiredby(WinnandBishop,2005),priorworkby(Championetal.,2021)
derivesvariationalmessagepassingupdatesforAIFbyaugmentingvariationalmessage
updateswithanExpectedFreeEnergy(EFE)term. Incontrast,thecurrentpapertakes
aconstrainedoptimisationapproach,augmentingthevariationalobjectiveitself,and
derivingmessageupdateexpressionsbyvariationaloptimisation.
MessagepassingformulationsofAIFallowformodularextensiontohierarchical
structures.Temporalthicknessinthecontextofmessagepassingisexploredby(deVries
and Friston, 2017), which formulates deep temporal AIF by message passing on an
FFG representation of a hierarchical GM. Implications of message passing in deep
temporalmodelsonneuralconnectivityarefurtherexploredby(Fristonetal.,2017).
AnoperationalframeworkandsimulationenvironmentforAIFbymessagepassingon
FFGsisdescribedby(vandeLaaranddeVries,2019).
Concerningepistemicsandtheexploration-exploitationtrade-off,thepioneering
work of (Friston et al., 2015) formally decomposes an EFE objective in constituent
driversforbehaviour,andmotivatesepistemicvaluefromtheperspectiveofmaximizing
26
informationgain. Adetailedviewby(Koudahletal.,2021)considersEFEminimisation
inthecontextoflinearGaussiandynamicalsystems,andshowsthatAIFdoesnotlead
topurposefulexplorativebehaviourinthiscontext.
Unfortunately, as our companion paper argues, the EFE optimisation viewpoint
doesnotreadilyextendtomessagepassingonfree-formmodels(Koudahletal.,2023).
Towardsresolvingthislimitation,(Schwo¨beletal.,2018)formulatedAIFasBFEopti-
misation,butalsonotesthattheBFElacksthecrucialambiguity-reducingcomponent
oftheEFEthatinducesexploration. Analternativeobjective,thefreeenergyofthe
expectedfuture,wasintroducedby(Millidgeetal.,2020). Thisobjectiveincludesthe
ambiguity-reducingcomponentoftheEFEandcanbeinterpretedasthedivergencefrom
abiasedtoaveridicalGM.AnalternativeAIFobjectivewasproposedby(vandeLaar
etal.,2022),whichconsidersepistemicbehaviourfromaconstrainedBFEperspective.
A biologically plausible view on message passing for AIF is described by (Parr
etal.,2019),whichcombinesthestrengthsofbeliefpropagationandVMPtodescribe
analternativetypeofmarginalmessageupdate.
9 Conclusions
Thispaperhastakenaconstraint-centricapproachtosyntheticActiveInference(AIF),
andsimulatedaperception-actioncyclethroughmessagepassingderivedfromasingle
GeneralisedFreeEnergy(GFE)objective. Specifically,wehaveusedaConstrained
Forney-style Factor Graph (CFFG) visual representation to distinguish between the
GenerativeModel(GM)andconstraintsontheVariationalFreeEnergy(VFE).
Throughconstraintvisualisationswehaveshownhowthefreeenergyobjectivesfor
perception,controlandlearningforAIFcanbeunifiedunderasingleGMspecification
andschedule,withtime-dependentconstraints.
Theimpactofourcontributionslieswithamodularandscalableformulationof
synthetic AIF agents. Using variational calculus, we have derived general message
updaterulesforGFE-basedcontrol. Thisallowsforamodularapproachtosynthetic
AIF, where custom message updates can be derived and reused across models (Cox
et al., 2019). As an example we have derived GFE-based messages for a general
configurationoftwofacingnodes,andappliedtheseresultstoderivespecificmessages
foradiscrete-variablegoal-observationsubmodelthatisoftenusedinAIFpractice.
ThegeneralupdaterulesallowforderivingGFE-basedmessagesaroundalternative
sub-models, including continuous-variable models and possibly chance-constrained
models (van de Laar et al., 2021). Additionally, the general message update results
allowforaparametrisedgoalprior,whichmaymemodelledbyasecondarydynamical
model(Senneshetal.,2022).
Crucially,thelocalupdatesincludenovelbackwardmessagesthathavenotbeen
expressed in traditional formulations of AIF. These backward messages ensure the
unifiedoptimisationofthefullGFEobjective,withoutresortingtodistinctschedulesfor
stateestimationandfreeenergyevaluation. Aslimitations,weidentifiedconvergence
issuesinthemessageupdates,whichwereaddressedbyanalternativeupdaterulethat
can be solved by Newton’s method. However, this method may still converge to a
sub-optimallocalminimumoralimitcycle. Also,weresortedtoimportancesampling
27
tocomputedifficultexpectations.
WithaCFFGrepresentationandlocalmessagepassingrulesavailable,itbecomes
straightforward to mix and match constraints. We formulated an experimental pro-
tocol that unifies the tasks for perception, control and learning under a single GM
andschedule. Wesimulatedtheperception-actioncyclebyareactiveprogramming
implementation(BagaevanddeVries,2021),wheremessageupdatesdynamicallyreact
totime-dependentconstraintsasobservationsbecomeavailable.
The presented T-maze simulations illustrate how the message passing approach
tosyntheticAIFinducesepistemicbehaviour. Namely, wheretheGFE-basedagent
exploresnovelparametersettingsandsalientstates,thereferenceBetheFreeEnergy
(BFE)basedagentconsistentlyfailstoidentifyinformativestatesintheenvironment.
WediscussedtwoextensionsoftheT-mazesetting,onelearninggoalstatisticsand
another simulating a bargaining simulation, where a seller agent shares information
withabuyeragentinexchangeforashareoftherewardprobability. Theseextensions
illustratehowthemessagepassingapproachtosyntheticAIFallowsforreuseofnodes
andmessagesinnon-conventionalmodelsandmulti-agentsettings.
Inthispaperwehaveadoptedapurelyengineeringpoint-of-view,andwehavenot
concernedourselveswithbiologicalplausibility. Specifically,thederivedmessageup-
datescomewithconsiderationsaboutstabilityandnon-standardexpressions. Although
wehaveengineeredsolutionstoovercomethesecomplications,itseemsunlikelytous
thatthebrainresortstosuchstrategies.
Acknowledgments
ThisresearchwasmadepossiblebyfundingfromGNHearingA/S.Thisworkispartof
theresearchprogrammeEfficientDeepLearningwithprojectnumberP16-25project
5,whichis(partly)financedbytheNetherlandsOrganisationforScientificResearch
(NWO).
WegratefullyacknowledgestimulatingdiscussionswiththemembersoftheBIASlab
researchgroupattheEindhovenUniversityofTechnology,inparticularIsmailSenoz,
BartvanErpandDmitryBagaev;andmembersofVERSESResearchLab,inparticular
KarlFriston,ChrisBuckley,ConorHeinsandTimVerbelen. WealsothankMateus
JoffilyoftheGATE-labatleCentreNationaldelaRechercheScientifiqueforstimulating
discussions.
Wethankthetwoanonymousreviewersfortheirvaluablecommentsandsuggestions.
28
Appendix
A Proofs of Local Stationary Solutions in Section 4.3
ThissectionderivesthestationarypointsofalocalGFEconstrainedobjective,asdefined
byFig.4.
A.1 ProofofLemma1
Proof. WritingoutthetermsintheLagrangianandsimplifying,weobtain
L[q ]=E [logq(x)]+ψ q(x)dx 1 +C .
x p(xz,θ)q(z)q(θ) x x
| −
(cid:20)(cid:90) (cid:21)
Thefunctionalderivativethenbecomes
δL E q(z)q(θ) [p(xz,θ)] !
= | +ψ =0.
x
δq q(x)
x
Solvingthisequationforq resultsin(11).
x
A.2 ProofofLemma2
Proof. WritingouttheLagrangian,weobtain
q(x)
L[q ]=E log +E [logq(z)]
z p(x | z,θ)q(z)q(θ)q(w)q(ϕ) p(xz,θ)p˜(xw,ϕ) q(z)
(cid:20) | | (cid:21)
+ψ q(z)dz 1 + λ (z ) q(z ) q(z)dz dz +C .
z ip i i i i z
− − \
(cid:20)(cid:90) (cid:21) i ∈(cid:88)E (z)(cid:90) (cid:20) (cid:90) (cid:21)
Thefunctionalderivativethenbecomes
δL q(x)
=E log +logq(z)+1+ψ λ (z )
δq z p(x | z,θ)q(θ)q(w)q(ϕ) (cid:20) p(x | z,θ)p˜(x | w,ϕ) (cid:21) z − i ∈(cid:88)E (z) ip i
q(x)
=E log +logq(z) λ (z )+Z
p(x | z,θ)q(θ) p(xz,θ)f˜(x) − ip i z
(cid:20) | (cid:21) i ∈(cid:88)E (z)
= logf˜(z)+logq(z) λ (z )+Z ,
ip i z
− −
i ∈(cid:88)E (z)
whereZ absorbsalltermsindependentofz,andwithf˜(z)andf˜(x)givenby(14a)
z
and(14b)respectively.
Settingtozeroandsolvingforq ,weobtain
z
logq (z)=logf˜(z)+ λ (z ) Z .
∗ ip i z
−
i ∈(cid:88)E (z)
Exponentiatingonbothsides,identifyingµ (z )=expλ (z ),andnormalizingthen
ip i ip i
resultsin(13).
29
A.3 ProofofLemma3
Proof. WritingouttheLagrangian,weobtain
q(x)
L[q ]=E log +E [logq(z)]
w p(x | z,θ)q(z)q(θ)q(w)q(ϕ) p(xz,θ)p˜(xw,ϕ) q(z)
(cid:20) | | (cid:21)
+ψ q(w)dw 1 + λ (w ) q(w ) q(w)dw dw +C .
w ip˜ i i i i w
− − \
(cid:20)(cid:90) (cid:21) i ∈(cid:88)E (w)(cid:90) (cid:20) (cid:90) (cid:21)
Thefunctionalderivativethenbecomes
δL q(x)
=E log +logq(w)+1+ψ λ (w )
δq w p(x | z,θ)q(z)q(θ)q(ϕ) (cid:20) p(x | z,θ)p˜(x | w,ϕ) (cid:21) w − i ∈(cid:88)E (w) ip˜ i
= E [logp˜(xw,ϕ)]+logq(w) λ (w )+Z
p(xz,θ)q(z)q(θ)q(ϕ) ip˜ i w
− | | −
i ∈(cid:88)E (w)
= E [logp˜(xw,ϕ)]+logq(w) λ (w )+Z
q(x)q(ϕ) ip˜ i w
− | −
i ∈(cid:88)E (w)
= logf˜(w)+logq(w) λ (w )+Z
ip˜ i w
− −
i ∈(cid:88)E (w)
whereZ absorbsalltermsindependentofw,thesecond-to-laststepusestheresultof
w
(11),andwheref˜(w)isgivenby(17).
Settingtozeroandsolvingforq ,weobtain
w
logq (w)=logf˜(w)+ λ (w ) Z .
∗ ip˜ i w
−
i ∈(cid:88)E (w)
Exponentiating on both sides, identifying µ (w ) = expλ (w ), and normalizing
ip˜ i ip˜ i
resultsin(16).
B Proofs of Message Update Expressions in Section 4.4
B.1 ProofofTheorem1
Proof. Firstly,Lemma3providesuswiththestationarysolutionstoL[q]asafunctional
of q . Secondly, the stationary solution of L[q] as a functional of the edge-local
w
variationaldistributionq (w ),definedas
j j
L[q ]=H[q ]+ψ q(w )dz 1
j j j j j
−
(cid:20)(cid:90) (cid:21)
+ λ (w ) q(w ) q(w)dw dw +C ,
ja j j j j j
− \
a ∈(cid:88)V (j)(cid:90) (cid:20) (cid:90) (cid:21)
30
whereC absorbsalltermsindependentofq ,directlyfollowsfrom(S¸eno¨zetal.,2021,
j j
Lemma2),as
µ (w )µ (w )
jp˜ j j j
q ∗ (w j )= • .
µ (w )µ (w )dw
jp˜ j j j j
•
Wethenapplythemarginalisation(cid:82)constraintontheedge-andnode-localvariational
distributions
q (w )= q (w)dw .
∗ j ∗ j
\
(cid:90)
Substitutingthestationarysolutionswecandirectlyapply(S¸eno¨zetal.,2021,Theo-
rem2). Itthenfollowsthatfixedpointsof(18)correspondtostationarysolutionsof
L[q].
The notation µ (w ) = µ(n+1)(w ) then conveniently represents the recursive
1 j j j
messageupdateschedule. •
B.2 ProofofTheorem2
Proof. Wefollowthesameprocedureasbefore. Firstly, Lemma2providesuswith
thestationarysolutionsofL[q]asafunctionalofz . Secondly, theLagrangianasa
j
functionalofq (z )isthenconstructedas
j j
L[q ]=H[q ]+ψ q(z )dz 1
j j j j j
−
(cid:20)(cid:90) (cid:21)
+ λ (z ) q(z ) q(z)dz dz +C ,
ja j j j j j
− \
a ∈(cid:88)V (j)(cid:90) (cid:20) (cid:90) (cid:21)
whereC absorbsalltermsindependentofq . Thestationarysolutionagainfollows
j j
from(S¸eno¨zetal.,2021,Lemma2),
µ (z )µ (z )
jp j j j
q ∗ (z j )= • .
µ (z )µ (z )dz
jp j j j j
•
Fromthemarginalisationconstra(cid:82)int
q (z )= q (z)dz ,
∗ j ∗ j
\
(cid:90)
wecanagaindirectlyapply(S¸eno¨zetal.,2021,Theorem2),fromwhichitfollowsthat
fixedpointsof(19)correspondtostationarysolutionsofL[q].
Intheschedule,thefixed-pointiterationisthenrepresentedbyµ (z )=µ(n+1)(z ).
2 j j j
•
B.3 ProofofCorollary1
Proof. Fromthemarginalisationconstraintweobtain(20). Wethenparameteriseq
z
withstatisticsν andsubstitute(13),(14)and(11)toobtainarecursivedependenceon
ν.
31
C Derivations of Message Updates in Figure 5
Herewederivethemessageupdatesforthediscrete-variablesubmodelofFig.5. To
streamlinethederivationsofwefirstderivesomeintermediateresults.
C.1 IntermediateResults
Firstweexpressthelog-observationmodel,
logp(xz,A)=log at(xAz)
| C |
= x log(A )z
j jk k
j k
(cid:88)(cid:88)
=xTlog(A)z,
wherethefinallogarithmistakenelement-wise.
Then,from(11),
logq(x)=log E [p(xz,θ)]
q(z)q(A)
|
=log(cid:0) E
q(z)q(A)
[ at(xAz(cid:1))]
C |
log(cid:0)at xAz (cid:1)
≈ C |
=xTlog(cid:0)Az ,(cid:1)
Whereweusedatentativedecisionappro(cid:0)xim(cid:1)ationtocomputetheexpectationswith
!
respecttoq(A)=δ(A A).
−
Next,from(14),
logf˜(x)=E [logp˜(xc)]
q(c)
|
=E [log at(xc)]
q(c)
C |
=E xTlogc
q(c)
=xTlog(cid:2)c.
(cid:3)
Combiningtheseresults,from(14),
p(xz,A)f˜(x)
logf˜(z)=E log |
p(x | z,A)q(A) (cid:34) q(x) (cid:35)
=E xTlog(A)z+xTlogc xTlog Az
p(xz,A)q(A)
| −
=E
q(A)
(Az)T (cid:2)log(A)z+(Az)Tlogc (Az)T (cid:0)log(cid:1)A(cid:3)z
−
=E (cid:2)zTdiag(ATlogA)+(Az)Tlogc (Az)T(cid:0)log(cid:1)A(cid:3)z
q(A)
−
=E (cid:104) zTh(A)+(Az)Tlogc (Az)Tlog Az (cid:0) (cid:1)(cid:105)
q(A)
− −
= zTh(cid:2)(A)+(Az)Tlogc (Az)Tlog Az (cid:0) (cid:1)(cid:3)
− −
=zTρ,
(cid:0) (cid:1)
32
with
T
ρ=A logc log Az h(A), (24)
− −
and (cid:0) (cid:0) (cid:1)(cid:1)
h(A)= diag(ATlogA),
−
theentropiesofthecolumnsofmatrixA.
WiththeseresultswecanderivethelocalGFEandmessages.
C.2 AverageEnergy
q(x)
U [q]=E log
x p(x | z,A)q(z)q(A)q(c) p(xz,A)p˜(xc)
(cid:20) | | (cid:21)
p(xz,A)f˜(x)
= E log |
− p(x | z,A)q(z)q(A) (cid:34) q(x) (cid:35)
= E logf˜(z)
q(z)
−
= zTρ, (cid:104) (cid:105)
−
withρgivenby(24).
C.3 Message 1
WeapplytheresultofTheorem1andexpressthedownwardmessage,
logµ (c)=logf˜(c)
1
=E [logp˜(xc)]
q(x)
|
=E xTlogc
q(x)
=(Az)T (cid:2) logc. (cid:3)
Exponentiationonbothsidesthenyields
µ (c) ir cAz+1 .
1 ∝D |
(cid:0) (cid:1)
C.4 DirectResultforMessage 2
HereweapplytheresultofTheorem2todirectlycomputethebackwardmessageforthe
state. Asexplainedbefore,thisupdatemayleadtodivergingFEforsomealgorithms.
logµ (z)=logf˜(z)
2
=zTρ,
withρgivenby(24). Exponentiationonbothsidesthenyields
µ (z) at(zσ(ρ)) ,
2 ∝C |
withσthesoftmaxfunction.
33
C.5 IndirectResultforMessage 2
HereweapplytheresultofCorollary1. Wesetthestatisticν =z,assumemessage D
(proportionally)Categorical,anduse(21)toexpress
logq(z;z)=logf˜(z;z)+logµ (z)+C
D z
=zTρ(z)+zTlogd+C ,
z
withρ(z)givenby(24),wherethecirculardependenceonzhasbeenmadeexplicit.
Exponentiatingonbothsidesandnormalizing,weobtain
q(z;z)= at(zz),with
C |
z=σ(ρ(z)+logd) , (25)
andσthesoftmaxfunction.
Wethenapproachthisequationasaroot-findingproblem,anduseNewton’smethod
tofindanz ∗thatsolvesfor(25). Wecanthencomputethebackwardmessagethrough
(20),as
µ (z) q(z;z )/µ (z)
2 ∝ ∗ D
= at(zz )/ at(zd)
∗
C | C |
at(zσ(logz logd)) .
∗
∝C | −
C.6 DirectResultforMessage 3
Here we apply the result of Theorem 2 and use the symmetry between z and A to
directlycomputethebackwardmessageforthestate,as
p(xz,A)f˜(x)
logµ (A)=E log |
3 p(x | z,A)q(z) (cid:34) q(x) (cid:35)
=E xTlog(A)z+xTlogc xTlog Az
p(xz,A)q(z)
| −
=E
q(z)
(Az)T (cid:2)log(A)z+(Az)Tlogc (Az)T (cid:0)log(cid:1)A(cid:3)z
−
=E (cid:2)zTdiag(ATlogA)+(Az)Tlogc (Az)T(cid:0)log(cid:1)A(cid:3)z
q(z)
−
=E (cid:104) zTh(A)+(Az)Tlogc (Az)Tlog Az (cid:0) (cid:1)(cid:105)
q(z)
− −
=zTξ(A(cid:2) ), (cid:0) (cid:1)(cid:3)
with
ξ(A)=AT logc log Az h(A). (26)
− −
(cid:0) (cid:0) (cid:1)(cid:1)
34
References
Akbayrak,S.,Bocharov,I.,anddeVries,B.(2021). ExtendedVariationalMessage
PassingforAutomatedApproximateBayesianInference. Entropy,23(7):815.
Bagaev,D.anddeVries,B.(2021). ReactiveMessagePassingforScalableBayesian
Inference. arXiv:2112.13251[cs]. arXiv: 2112.13251.
Champion, T., Grzes´, M., and Bowman, H. (2021). Realising Active Inference in
VariationalMessagePassing:theOutcome-blindCertaintySeeker. arXiv:2104.11798
[cs]. arXiv: 2104.11798.
Cox,M.,vandeLaar,T.,anddeVries,B.(2019). Afactorgraphapproachtoauto-
mated design of Bayesian signal processing algorithms. International Journal of
ApproximateReasoning,104:185–204.
DaCosta,L.,Parr,T.,Sajid,N.,Veselic,S.,Neacsu,V.,andFriston,K.(2020). Active
inferenceondiscretestate-spaces: asynthesis. arXiv:2001.07203[q-bio]. arXiv:
2001.07203.
Dauwels, J. (2007). On Variational Message Passing on Factor Graphs. In IEEE
InternationalSymposiumonInformationTheory,pages2546–2550,Nice,France.
deVries,B.andFriston,K.J.(2017). AFactorGraphDescriptionofDeepTemporal
ActiveInference. FrontiersinComputationalNeuroscience,11.
Forney, G. (2001). Codes on graphs: normal realizations. IEEE Transactions on
InformationTheory,47(2):520–548.
Friston,K.,Kilner,J.,andHarrison,L.(2006). Afreeenergyprincipleforthebrain.
JournalofPhysiology,Paris,100(1-3):70–87.
Friston,K.,Rigoli,F.,Ognibene,D.,Mathys,C.,Fitzgerald,T.,andPezzulo,G.(2015).
Activeinferenceandepistemicvalue. CognitiveNeuroscience,6(4):187–214.
Friston,K.,Schwartenbeck,P.,Fitzgerald,T.,Moutoussis,M.,Behrens,T.,andDolan,
R.J.(2013).Theanatomyofchoice:activeinferenceandagency.FrontiersinHuman
Neuroscience,7:598.
Friston,K.J.,Daunizeau,J.,andKiebel,S.J.(2009). ReinforcementLearningorActive
Inference? PLoSONE,4(7):e6421.
Friston,K.J.,Daunizeau,J.,Kilner,J.,andKiebel,S.J.(2010). Actionandbehavior: a
free-energyformulation. Biologicalcybernetics,102(3):227–260.
Friston,K.J.,Parr,T.,anddeVries,B.(2017). Thegraphicalbrain: beliefpropagation
andactiveinference. NetworkNeuroscience,pages1–78.
Kiebel, S. J., Daunizeau, J., and Friston, K. J. (2009). Perception and Hierarchical
Dynamics. FrontiersinNeuroinformatics,3.
35
Kirchhoff,M.,Parr,T.,Palacios,E.,Friston,K.,andKiverstein,J.(2018). TheMarkov
blanketsoflife: autonomy,activeinferenceandthefreeenergyprinciple. Journalof
TheRoyalSocietyInterface,15(138):20170792.
Koller,D.andFriedman,N.(2009). Probabilisticgraphicalmodels: principlesand
techniques. MITpress.
Koudahl, M., van de Laar, T., and De Vries, B. (2023). Realising Synthetic Active
InferenceAgents,PartI:EpistemicObjectivesandGraphicalSpecificationLanguage.
arXivpreprintarXiv:2306.08014.
Koudahl,M.T.,Kouw,W.M.,anddeVries,B.(2021). OnEpistemicsinExpectedFree
EnergyforLinearGaussianStateSpaceModels. Entropy,23(12):1565. Publisher:
MDPI.
Kschischang, F. R., Frey, B. J., and Loeliger, H.-A. (2001). Factor graphs and the
sum-productalgorithm. IEEETransactionsoninformationtheory,47(2):498–519.
Loeliger,H.-A.(2004). Anintroductiontofactorgraphs. SignalProcessingMagazine,
IEEE,21(1):28–41.
Loeliger, H.-A. (2007). Factor Graphs and Message Passing Algorithms – Part 1:
Introduction. http://www.crm.sns.it/media/course/1524/Loeliger A.pdf,lastaccessed
on3-4-2019.
Millidge, B., Tschantz, A., and Buckley, C. L. (2020). Whence the Expected Free
Energy? arXivpreprintarXiv:2004.08128.
Parr,T.andFriston,K.J.(2019). Generalisedfreeenergyandactiveinference. Biologi-
calcybernetics,113(5):495–513. Publisher: Springer.
Parr,T.,Markovic,D.,Kiebel,S.J.,andFriston,K.J.(2019).Neuronalmessagepassing
usingMean-field,Bethe,andMarginalapproximations. ScientificReports,9(1):1889.
Pearl, J. (1982). Reverend Bayes on Inference Engines: A Distributed Hierarchical
Approach. InProceedingsoftheSecondAAAIConferenceonArtificialIntelligence,
AAAI’82,pages133–136,Pittsburgh,Pennsylvania.AAAIPress.
Schwo¨bel,S.,Kiebel,S.,andMarkovic´,D.(2018).ActiveInference,BeliefPropagation,
andtheBetheApproximation. NeuralComputation,30(9):2530–2567.
Sennesh,E.,Theriault,J.,vandeMeent,J.-W.,Barrett,L.F.,andQuigley,K.(2022).
Derivingtime-averagedactiveinferencefromcontrolprinciples. arXiv:2208.10601
[cs,eess,q-bio,stat].
S¸eno¨z,˙I.,vandeLaar,T.,Bagaev,D.,anddeVries,B.(2021). VariationalMessage
PassingandLocalConstraintManipulationinFactorGraphs. Entropy,23(7):807.
Publisher: MultidisciplinaryDigitalPublishingInstitute.
vandeLaar,T.(2019). AutomatedDesignofBayesianSignalProcessingAlgorithms.
PhDthesis,EindhovenUniversityofTechnology,Eindhoven,TheNetherlands.
36
van de Laar, T. and de Vries, B. (2019). Simulating Active Inference Processes by
MessagePassing. FrontiersinRoboticsandAI,6:20.
vandeLaar,T.,Koudahl,M.,vanErp,B.,anddeVries,B.(2022). ActiveInference
andEpistemicValueinGraphicalModels. FrontiersinRoboticsandAI,9.
van de Laar, T., S¸eno¨z, I., O¨zc¸elikkale, A., and Wymeersch, H. (2021). Chance-
ConstrainedActiveInference. arXivpreprintarXiv:2102.08792.
Winn,J.andBishop,C.M.(2005). VariationalMessagePassing. JournalofMachine
LearningResearch,6(23):661–694.
Yedidia,J.S.,Freeman,W.T.,andWeiss,Y.(2001). UnderstandingBeliefPropagation
anditsGeneralizations.
Zhang,D.,Wang,W.,Fettweis,G.,andGao,X.(2017). UnifyingMessagePassing
AlgorithmsUndertheFrameworkofConstrainedBetheFreeEnergyMinimization.
arXiv:1703.10932[cs,math]. arXiv: 1703.10932.
37

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Realising Synthetic Active Inference Agents, Part II: Variational Message Updates"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
