=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Learning EFSM Models with Registers in Guards
Citation Key: vega2024learning
Authors: Germán Vega, Roland Groz, Catherine Oriat

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2024

Abstract: ThispaperpresentsanactiveinferencemethodforExtendedFiniteState Machines,where
inputs and outputs are parametrized, and transitions can be conditioned by guards in-
volving input parameters and internal variables called registers. The method applies to
(software) systems that cannot be reset, so it learns an EFSM model of the system on a
single trace.
Keywords: Activeinference,Querylearning,ExtendedAutomata,Genetic Programming

Key Terms: guards, software, automata, models, sheffield, efsm, learning, grenoble, alpes, inference

=== FULL PAPER TEXT ===

4202
nuJ
11
]LF.sc[
1v04070.6042:viXra
JournalofMachineLearningResearch1–14,2024 LearningandAutomata(LearnAut)–ICALP2024workshop
Learning EFSM Models with Registers in Guards
Germ´an Vega german.vega@imag.fr
Roland Groz Roland.Groz@univ-grenoble-alpes.fr
Catherine Oriat Catherine.Oriat@univ-grenoble-alpes.fr
LIG, Universit´e Grenoble Alpes, F-38058 Grenoble, France
Michael Foster m.foster@sheffield.ac.uk
Neil Walkinshaw n.walkinshaw@sheffield.ac.uk
Department of Computer Science, The University of Sheffield, UK
Adenilso Sim˜ao Adenilso@icmc.usp.br
Universidade de Sa˜o Paulo, ICMC, Sa˜o Carlos/Sa˜o Paulo, Brasil
Abstract
ThispaperpresentsanactiveinferencemethodforExtendedFiniteState Machines,where
inputs and outputs are parametrized, and transitions can be conditioned by guards in-
volving input parameters and internal variables called registers. The method applies to
(software) systems that cannot be reset, so it learns an EFSM model of the system on a
single trace.
Keywords: Activeinference,Querylearning,ExtendedAutomata,Genetic Programming
1. Introduction
Automata learning by active inference has attracted interest in software engineering and
validation since the turn of the century Peled et al. (1999) Hagerer et al. (2002). Although
classical automata (in particular Mealy models) have been used in software testing for a
longtimeChow(1978), software engineeringhasevolved torichermodelsof automata, with
parameters on actions, internal variables, and guards, as can be found in such formalisms
as SDL, Statecharts or UML. Inferring such complex models, whose expressive power is
usually Turing-complete, is challenging.
Asignificantstepwas theintroductionof registerautomata Isberner et al.(2014). How-
ever, the practical application of those inference methods is limited by two main problems.
Firstly, the expressive power of the inferred models is usually restricted (e.g. only boolean
values or infinite domains with only equality). Secondly, most inference methods learn the
SystemUnderLearning(SUL)bysubmittingsequencesofinputsfromtheinitial state, thus
requiringthe SUL to bereset for each query, which can becostly, impractical or impossible.
Recently, Foster et al. (2023) proposed an algorithm to infer EFSMs in the above cir-
cumstances. It uses an approach based on the hW algorithm by Groz et al. (2020) to infer
the structure of the control state machine and to work around the absence of a reliable re-
set, and uses Genetic Programming Poli et al. (2008) to infer registers, guards and output
functions from the corresponding data observations. This approach is, however, restrictive
because guards on transitions can only involve input parameters and cannot be based on
internal registers. Our challenge, therefore, is to provide a more general solution that not
only infers the presence of registers and corresponding output functions but also enables
©2024G.Vega,R.Groz,C.Oriat,M.Foster,N.Walkinshaw &A.Sim˜ao.
Vega Groz Oriat Foster Walkinshaw Sima˜o
us to include state transitions that depend upon register values. Our paper proposes an
algorithm that can infer EFSM models that include registers in guards.
Addressing this problem raises a number of challenges. i) it is necessary to identify the
underlying control state machine, by distinguishing what should (preferably) be encoded
in states and what should be encoded in registers; ii) for any transition in the EFSM, it is
necessary to identify any guards on inputs and the inferred registers, and to identify any
functions that may affect the state of the registers or the values of the observable outputs;
andiii)sincenoreliableresetis assumed,analyzingtheinfluenceofregisters shouldbedone
from comparable configurations of the SUL, so it requires finding a way of coming back to
a previously recognized configuration.
Thesechallenges are tightly interdependent; any guards and outputfunctionsare highly
dependentonthetransitionstructureofthemachine. Anyinferenceprocessmustgeneralise
from the executions observed so far (the inferred model must be capable of recognising or
accepting sequences that have not yet been observed). At the same time, it must avoid
overgeneralization (i.e. accepting impossible or invalid sequences of inputs).
2. Running example and background models
We consider a straightforward running example that exhibits most features of the model
yet is small enough to be fully illustrated in this paper. Our example is a vending machine,
shown in Figure 1, where the choice of drink and the money paid into the machine are
parameters. These are recorded by registers (internal variables), that influence later com-
putations. Throughoutthepaper,werefertotheparameterized events (suchasselect(tea))
as concrete inputs or outputs and to the event types (such as select) as abstract ones.
coin(i )/Display(t = r +i )
2 2 2
[r := r +i ]
2 2 2
select(i )/Pay(t = 0)
1
[r := i ,r := 0]
1 1 2
s s vend()[r < 100]/ω
0 1 2
vend()[r ≥ 100]/Serve(b = r )
2 1
(a)
hselect(tea)/Pay(0),coin(50)/Display(50),vend()/ω,coin(50)/Display(100),
coin(50)/Display(150),vend()/Serve(tea)i
(b)
Figure 1: Our vending machine EFSM and an example trace.
Figure 2 is taken from Foster et al. (2023). In that work, guards can only be influenced
by inputparameters, not registers, so coins are rejected if they are less than the price of the
drink. There are two distinct states after drink selection (s and s ), one where insufficient
1 2
money has been input, and one where the drink can be served. By allowing registers in
guards, Figure 1(a) represents a more realistic vending machine where we can pay with
smaller coins until we reach a sufficient amount. A single state after selection is enough to
allow different transitions depending on the amount stored in a register r .
2
2
Learning EFSM
coin(i )/Display(r +i )
0 1 0
coin(i )[i < 100]/Reject(i )
0 0 0 [r := r +i ]
1 1 0
select(i )/ǫ coin(i )[i ≥ 100]/
0 0 0
[r := 0,r := i ] Display(r +i )[r := r +i ]
1 2 0 1 0 1 1 0
s s s
0 1 2
vend()/Serve(r )
2
Figure 2: No register allowed in guard (from Foster et al. (2023)
vend
The output ω of the s −−−→ s transition is a special output indicating that the tran-
1 1
sition neither produces visible output nor changes the system state. In a vending machine,
this could model that the button cannot be pressed mechanically or is visibly ineffective.
Having named input and output parameters can also provide evidence that they share
a common register value, as is the case for the t output parameter in Figure 1(a), which
represents the total amount of money put into the system by coins for a selected drink. t
is shared by output types Pay and Display and mapped to a single register r .
2
3. Assumptions for tractability
In this work, an EFSM is a tuple (Q,R,I,O,T) where Q is a finite set of states, R is a
cartesian product of domains, representing the type of registers. I is the set of concrete
inputs, structured as a finite set of abstract inputs I each having associated parameters
and their domains P . Similarly O for concrete outputs based on O for abstract outputs.
I
′ ′
T is a finite set of transition, i.e. tuples (s,x,y,G,F,U,s ) where s,s ∈ Q, x ∈ I, y ∈ O,
G : P (x)×R → B is the transition guard, F : P (x)×R → P (y) is the output function
I I O
that gives the value of the output parameters, U : P (x)×R → R is the update function
I
that gives the value of the registers after the transition.
Associated with an EFSM, its control machine is the non-deterministic finite state ma-
chine (NFSM) that results from abstracting from parameters (and therefore guards and
registers). It defines the finite control structure of a given EFSM and is defined by a
quadruple (Q,I,O,∆), where ∆ is the transition function ∆ : Q × I × O → Q, which
∗ ∗
lifted to sequences of inputs outputs as ∆ : Q×(I ×O) → Q. The control machine for
Figure 1(a) is shown in Figure 3.
coin/Display
select/Pay
s s vend/ω
0 1
vend/Serve
Figure 3: Control NFSM of the vending machine.
WeassumethattheSULissemantically equivalenttoanEFSM,whichhasthefollowing
properties:
3
Vega Groz Oriat Foster Walkinshaw Sima˜o
• It is deterministic (at the concrete level), and its control NFSM is both strongly
connected (since we do not assume a reset, we can only learn a strongly connected
component) and observable (i.e. if an input triggers different transitions from the
same state, they have different abstract outputs).
• Registers can only take values from input and output parameters, and are therefore
observable: nohiddenvaluesinfluencethecomputationunnoticed. Wefurtherrestrict
them to storing only each parameter’s last value (it could be extended to a bounded
history). Note, however, that the guards and output functions can be arbitrarily
complex, so this does not restrict the expressive power.
Our method also relies on some inputs and hints provided on the system, which we
assume can be given, although they might be approximate. The main goal is to reduce the
practical complexity of the inference. These will be shown on our example in Section 4. In
I+
particular, we assume we are given a characterization set W ⊂ 2 ; and a homing sequence
∗
h ∈I , which also assigns a fixed value to all input parameters.
As the domains of inputparameters can beinfiniteor very large (e.g. integers or floats),
we will just use sample values in the learning process. We assume we are given (or pick)
3 levels of samples of concrete inputs I ⊂ I ⊂ I , with I giving one concrete instance
1 2 s 1
per abstract input, to infer the control machine, I providing a few more concrete inputs
2
to elicit guarded transitions, and I being a larger set of samples to have enough data for
s
the generalisation process to infer the output functions and guards. No specific knowledge
of the SUL is required to design those samples. We could pick either base values (e.g. for
integers, 0 in I , plus 1 or -1 in I , and any further values in I ) or just random values.
1 2 s
The initial subsets will be extended when counterexamples provide new sample values that
trigger so far unseen transitions.
We also assume we have 2 subsets of the registers R ⊆ R ⊆ R such that R are
w g g
the only registers that can be used in guards, and R are the registers used in guards
w
that may be traversed when applying sequences from W from any state. These can be
overapproximated with R = R = R in the worst case. However, if R contains output
w g w
registers, then the set of reachable values when applying inputs only from I must be finite.
1
When we exercise a system, we learn instances of transitions with concrete values for
inputandoutputparameters. For each transition ofacontrolmachine, wemay have several
R×I×O×R
samplevalues. We collect these in a structureΛ:Q×I×O → 2 , which associates
abstract transitions from ∆ with the observed concrete inputs and outputs and register
configurations.
A sampled FSM, which our backbone algorithm in Section 4 will learn, is a quintuple
(Q,I,O,∆,Λ). Our generalization process will then infer an EFSM from such a sampled
machine. The sampled machine for Figure 1 is shown in Figure 3.
Finally, we assume we are in the MAT framework from Angluin (1988), where an or-
acle can provide counterexamples: this can easily be approximated by a random walk on
the inferred machine (NFSM or EFSM) where (abstract, resp. concrete) outputs can be
compared with the observed responses from the SUL.
4
Learning EFSM
coin/Display
{((tea,0),50,50,(tea,50))
((tea,50),50,100,(tea,100))
((tea,100),50,150,(tea,150))}
select/Pay
{((⊥,⊥),tea,0,(tea,0))} vend/ω
s s
0 1
{(tea,50),(),⊥,(tea,50)}
vend/Serve ; {((tea,150),(),tea,(tea,150))}
Figure 4: Sampled FSM with trace from Figure 1(b)
4. Execution of method on running example
At its core, our ehW-inference algorithm (see Annex) adapts the Mealy hW-inference al-
gorithm by Groz et al. (2020), the so-called backbone, to learn the control structure of the
EFSM on abstract inputs and outputs. Thefact that registers can influenceguards has two
implications. First, from a given state, the same concrete input can produce two different
abstract outputs. Second, and more importantly, characterisations (abstract responses to
W) may be influenced by the register configuration (at least registers from R ).
w
Thereforeduringthelearningprocess,wecanonlyconservatively inferthatweareinthe
same state if we observe the same characterisation from the same register configuration. So
inthisnewalgorithm, thestatespaceoftheinferredNFSMis: Q ⊂
2W→O+
×R . Another
w
difference is that to be able to incrementally characterise a state (or learn a transition from
it) we need to reach it with the same register configuration. For this reason, we require a
strongernotionofhomingthatalwaysresetstheregisters,andthereisadditionalcomplexity
for transferring to the next state or transition to learn (as reflected in Algorithm 3).
In our example from Figure 1(a), there are four observable input/outputparameters (i
1
for choice of drink,i for thevalueof thecoin, t for thetotal amountalready inserted, andb
2
for the drink served). So there are at most 4 registers needed (let us name them i ,i ,b,t),
1 2
and the algorithm will track the last value of each.
We pick W = {select(coffee)}. The abstract output sequence Pay will characterise
s whereas the sequence Ω (inapplicable input) characterises s . The only guard in our
0 1
example is on input vend, so our W will never traverse it. But to show the robustness
of the approach, let us assume we do not have this precise information, and think the
choice of drink could play a role (which might be the case if drinks had different prices).
So we pick R = {i }. As for other guards, let us assume we have no clue, so we pick
w 1
R = R = {i ,i ,b,t}.
g 1 2
We pick as register homing sequence h = coin(100).vend.select(coffee). Notice that for
this simple example there are other shorter homing sequence, but we need one that resets
the registers from R (the chosen h will reset values of i to coffee, i to 100 and t to 0).
g 1 2
Notice also that this is actually a resetting sequence to s , but this may not be the case for
1
more complex examples.
Finally, we pick I = {coin(100),select(coffee),vend}. We always pick I such that it
1 1
includes all concrete inputs from h and W, so as to be able to follow transitions when
5
Vega Groz Oriat Foster Walkinshaw Sima˜o
walking the graph of the NFSM to transfer to the next transition to learn. And I = I =
2 s
I ∪{coin(50),coin(200),select(tea)}.
1
? Ω Ω Pay(0) ? Ω
→ → → →
1 2 3 4
s0 | {z } s1 | {z }
h=coin(100).vend.select(coffee) W=select(coffee)
? Display(100) Serve(coffee) Pay(0) ? Ω
→ → → →
5 6 7 8
s1 | {z } s1 | {z }
h=coin(100).vend.select(coffee) W=select(coffee)
? Disp
→
lay(100) Ser
→
ve(coffee) P
→
ay(0) q1
9 10 11
s1 | {z } s1
h=coin(100).vend.select(coffee)
Figure 5: Homing tails characterisations (internal SUL states shown only for reference)
Figure 5 shows how we first learn the homing tails. In steps 1-4 (steps are denoted
→ n in Figure 5), we learn that H(Ω.Ω.Pay) = q = (Ω,coffee), i.e. that after seeing the
1
abstractoutputsequenceΩ.Ω.Pay thestatewereachischaracterisedbytheabstractoutput
sequence Ω when W is executed in R configuration i = coffee. Similarly, in steps 5-8 we
w 1
learn that H(Display.Serve.Pay) = (Ω,coffee) = q . After a new homing (steps 9-11), we
1
observe a previously recorded output sequence, so we know the current state (q ) and we
1
can start learning its transitions.
q1 Dis
→
play(100) ?
→
Ω q1
→
Ω q1 Ser
→
ve(coffee) ? P
→
ay(0) ?
12 13 14 15 16
s1 | {z } s1 | {z } s1 | {z } s1 | {z } s0 | {z } s0
X=coin(100) W X=select(coffee) X=vend W
Dis
→
play(100) Ser
→
ve(coffee) P
→
ay(0) q1
→
ω Dis
→
play(100) Serv
→
e(coffee)
17 18 19 20 21 22
| {z } s1 | {z }
h transfer=vend.coin(100).vend
q2
→
Ω q2 P
→
ay(0) ?
→
Ω q1
23 24 25
s0 | {z } s0 | {z } s1 | {z } s1
X=coin(100) X=select(coffee) W
q1 Dis
→
play(100) Serv
→
e(coffee) q2
→
Ω q2
26 27 28
s1 | {z } s0 | {z } s0
transfer=coin(100).vend X=vend
Figure 6: NFSM learning (internal SUL states shown only for reference)
For each learnt state, we try to infer a transition using each of the concrete inputs in
I . We start from the current state q with input coin(100), see steps 12-13 in Figure 6,
1 1
and we discover a self loop. In step 14 we discover that input select is not allowed in the
state, so we stay in the same state and can immediately learn the next input. Finally, in
step 15 we discover that input vend leads to a newly discovered state q = (Pay,coffee).
2
Thus ∆(q ,vend,Serve) = q , and Λ(q , vend, Serve) = ([100, coffee, 100, coffee], vend,
1 2 1
Serve(coffee),[100,coffee,100,coffee]). Note that after applying W from q we do not know
2
the current state in the trace (shown as “?”), so we need to home again.
6
Learning EFSM
At this point we try to come back to state q to fully learn its transitions. First we
2
home to a known state (steps 17-19 in Figure 6). This leads to q , from which we have
1
learnt all inputs from I , so we follow a path in the partially inferred NFSM automaton to
1
transfer (steps 20-22 in Figure 6) to q . Compared to Mealy learning, there is an additional
2
complexity because we need to reach not just a node of the graph, but also a specific
register configuration. In step 20 in Figure 6, we optimistically try theshortest path (follow
transition vend), but that fails because we observe a new abstract output ω for the same
input. Thus we learnt that vend from q triggers a guarded transition with at least two
1
possibleabstract outputs. Sincethenewoutputis ω weknow itdidnotchangethestate, so
wearestill inq andneedto transfertoq . Toreach thatgoal, weneedto reach theregister
1 2
configuration [100,coffee,100,coffee] that we know enabled vend. The transfer sequence of
steps 21 and 22 does this. After reaching q , we are able to learn the transitions for input
2
coin(100) (step 23) and select(coffee) (step 24). Finally we need to do a new transfer to
learn transition vend (step 28), and we have an NFSM that is complete with respect to I .
1
q2 P
→
ay(0) q1 Dis
→
play(50) q1 Dis
→
play(250) q1
29 30 31
s0 | {z } s1 | {z } s1 | {z } s1
transfer=select(coffee) s=coin(50) s=coin(200)
Dis
→
play(350) Serv
→
e(coffee) P
→
ay(0) Dis
→
play(100) Serv
→
e(coffee) q2
32 33 34 35 36
| {z } s0
transfer=coin(100).vend.select(coffee).coin(100).vend
q2 P
→
ay(0) q1
37
s0 | {z } s1
s=select(tea)
Display(100) Serve(tea)
→ →
38 39
| {z }
CE=coin(100).vend
Figure 7: EFSM sampling (internal SUL states shown only for reference)
Once we have found the control structure (NFSM), we walk the graph to try incremen-
tally each input from I = I ∪{coin(50),coin(200),select(tea)} in each transition, so as to
s 1
collect enough samples for the generalise procedure to infer guards and output functions
to build the EFSM. Notice that if we have previouosly learnt that an input is not allowed
in a given state (Ω transitions), there is no need to sample further parameters for this
transition.
Figure 7 shows the sampling phase for our example, for state (Ω,coffee) we sample
coin(50) in step 30 and coin(200) in step 31. For state (Pay,coffee) we sample select(tea) in
step 37. Notice thatthemost complex partis to computethetransfer sequenceto reach the
R register configuration that enables a given transition. However, in the worst case, as the
g
state was reached from some homing that resets all R registers, there always exists a path
g
from some homing that leads to this previously visited state and register configuration.
In morecomplex examples, after samplingit is possibleto identify states that are equiv-
alentintheNFSMstructureandcompatiblewiththecollected samples. Thesestatescanbe
merged in the EFSM, as we can infer, given the evidence, that they correspond to different
register configurations of the same SUL state (this is done by the reduceFSMprocedure).
7
Vega Groz Oriat Foster Walkinshaw Sima˜o
Once the control structure (NFSM) is ascertained, we can use genetic programming to
generalise from the collected samples to get the EFSM as was done in Foster et al. (2023).
This is in fact simpler than in Foster et al. (2023) because, as discussed in Section 3, we
know the values of the internal registers (and how they are updated) at each point in the
trace. Thus, we only need to infer output functions and guards.
Notice in our example that even after sampling and generalisation, we have no evidence
that the parameter of Serve can be anything other than coffee. We rely on the oracle
to provide further counterexamples (see steps 38-39) that can be used to refine the func-
tions. After step 39, the generalise procedure produces exactly the EFSM of Figure 1(a)
(modulo renaming).
5. A more complex example where W traverses guards
To illustrate the need for other key elements of our method, we highlight them on a slightly
more complex example that entails problematic features. In Figure 8, the provided W =
b(i )[i ≥ r ]/B
b b a
s
3
) / B rb ) b(i
b(ib × a(i a )[r a < r b ]/ b)[i
A(ia
A(i ∗r )
b<
a(
a
i
(
a
i
) /
a /A(ia
+rb ) a
s 1
b a(i a)[r
a≥ r
r
b
a
]
]
/
/
B
B
b(ib
)/B a(i
a)/B
b(i )/B
b
s s
0 2
Figure 8: W = {b(0).a(0)} from state s can yield A.B or B.A
3
{b(0).a(0)} can have two outcomes when applied from state s : A.B (if r ≤ 0) or B.A
3 a
(otherwise); and B.A is also the output for W from state s . Actually, {b(0).a(0)} is not
1
even fully characterizing, but just as hW-inference, the algorithm is relatively robust and
can infer the correct structure from approximately correct h and W. There are 3 registers
linked to input and output parameters, which we name r ,r ,r . R = {r ,r }. We
a b A g a b
overapproximate R = {r ,r } even though in fact r is the only register really traversed
w a b a
when we know the SUL. When we apply the algorithm with I = {a(0),b(0)} and I =
1 2
I ∪{a(1),b(1)} therewillbedifferentstates intheNFSMcorrespondingtostate s , namely
1 3
(A.B,[0,0]),(A.B,[0,1]),(B.A,[1,0]),(B.A,[1,1]). But we shall also have several NFSM
states corresponding to a single other state, for instance (B.B, [0, 0]), (B.B, [1, 0]) for
state s . The backbone algorithm comes up with an 11-state NFSM, but the reduceFSM
0
procedurewouldmergeredundant(equivalent) copiessowecangetasampledmachinewith
only 4states at step310. Thisnumberofsteps was causedby ouroverapproximation onR
w
that led to exploring 11 states and 28 transitions. The whole inference process reached 372
steps for that machine when we added sample values {a(−5),b(−5)} to I (before reducing)
s
to get the exact data functions.
8
Learning EFSM
6. Conclusion
In this paper, we investigated how to infer EFSM models that include registers in guards.
By allowing registers to be used in guards, the inference method presented here should be
applicable to many systems. There are still a number of issues we want to investigate to
consolidate it. First, we assumed here that we could be given correct h and W for the SUL.
But just as in hW-inference, we can look for inconsistencies that could reveal the need to
expand h or W, which could even be empty (not provided) at the start. Extending h as
in hW-inference is straightforward, but there are more options for W. Another challenge
is to find optimal transfer sequences. In our current preliminary implementation, we fall
back on homing in all cases before transferring, and using a cached access table to states
and configurations, but shorter transfers would reduce the length of the inference trace.
Finally, we would like to assess the method and its scalability. It is classical to work with
randomly generated machines, but for EFSMs, there are many parameters to define what a
randomEFSMshouldbe. Althoughtherearebenchmarksforfiniteautomata1,itisnoteasy
to find strongly connected models with parameterized I/O behaviour. And automatically
assessing that the inferred model is correct can also be challenging, since in the general
case, equivalence of two EFSMs is undecidable.
References
Dana Angluin. Queries and concept learning. Machine learning, 2(4), 1988.
T.S. Chow. Testsoftwaredesignmodelled by finite state machines. IEEE Transactions on Software
Engineering, SE-4(3):178–187,1978.
Michael Foster, Roland Groz, Catherine Oriat, Adenilso da Silva Sim˜ao, Germ´an Vega, and Neil
Walkinshaw. Active inference of efsms without reset. In Yi Li and Sofi`ene Tahar, editors,
Formal Methods and Software Engineering - 24th International Conference on Formal Engi-
neering Methods, ICFEM 2023, Brisbane, QLD, Australia, November 21-24, 2023, Proceed-
ings, volume 14308 of Lecture Notes in Computer Science, pages 29–46. Springer, 2023. doi:
10.1007/978-981-99-7584-6\3. URL https://doi.org/10.1007/978-981-99-7584-6_3.
Roland Groz, Nicolas Bremond, Adenilso Simao, and Catherine Oriat. hw-inference: A heuristic
approach to retrieve models through black box testing. Journal of Systems and Software, 159,
2020.
Andreas Hagerer, Hardi Hungar, Oliver Niese, and Bernhard Steffen. Model gener-
ation by moderated regular extrapolation. In FASE, pages 80–95, 2002. URL
citeseer.ist.psu.edu/hagerer02model.html.
Malte Isberner, Falk Howar, and Bernhard Steffen. Learning register automata: from languages to
programstructures. Machine Learning, 96(1), 2014.
Doron Peled, Moshe Y. Vardi, and Mihalis Yannakakis. Black box checking. In Proceedings of
FORTE’99, Beijing, China, 1999.
Riccardo Poli, William B. Langdon, and Nicholas Freitag McPhee. A Field Guide
to Genetic Programming. lulu.com, 2008. ISBN 978-1-4092-0073-4. URL
http://www.gp-field-guide.org.uk/.
1http://automata.cs.ru.nl/Overview#Mealybenchmarks
9
Vega Groz Oriat Foster Walkinshaw Sima˜o
Algorithm 1 Homing into a known state
1 Function Home(T,r,h) ⊲ Apply h until we can know the state reached
2 repeat
3 (T,a,r) ← Apply(T,r,h) ⊲ Apply homing seq h, observe response a, update registers
∗
4 let η = π(a) ∈ O ⊲ State reached by homing is associated to η. π is abstraction
5 if H(η) is undefined for some w ∈ W then ⊲ Learn characterization of the tail state
6 (T,y,r) ← Apply(T,r,w)
7 H(η) ← H(η)∪{w 7→ π(y)}
8 else ⊲ We know the state reached after h/a
9 let q = (H(η),ρ (r,ǫ)) be the state reached at end of h ⊲ ρ (r,σ) first applies
w w
register updates from i/o sequence σ to r, then projects on R
w
10 Q ← Q∪{q}; A(π(a))(q,r) ← ǫ ⊲ A records known access to configuration (q,r)
from homing tail
11 until q 6= ⊥
12 return (T,q,r)
10
Learning EFSM
Algorithm 2 Backbone procedure with guards on registers
1 Function Backbone(T,I ,I ,h,W,Q,∆,Λ)
1 s
2 Initializing: H ← ∅, J ← I , q ← ⊥, r ← ⊥
1
3 repeat
4 if q = ⊥ then ⊲ We do not know where we are
5 (T,q,r) ← Home(T,r,h)
6 (q ′ ,r ′ ,X,Y,r ) ← Transfer(q,r,∆,Λ,I ,J \ I ) ⊲ Target next transition to
1 1 1
learn/sample
7 if such a path cannot be found then
8 goto line 30 ⊲ Graph not connected, try connecting with I
s
′ ′
9 if Y = Ω of ω then ⊲ π(X) is not enabled in q ,r
′ ′ ′ ′
10 Λ(q ,π(X),ω) ← {r ,X,Y,r }, ∆(q ,π(X),Y)← q
1
′ ′
11 q ← q , r ← r ⊲ We continue learning from the same state
′ ′ ′′ ′′′
12 else ⊲ (q ,r )−X/Y → (q ,r1) → w/ξ → (q ,r
2
)
′′ ′ ′′
13 Letq = ∆(q ,π(X),π(Y))⊲ q (and so ∆)might be a “state under construction”
′′
14 if q is fully defined then ⊲ We are sampling new values of a known transition
with same abstract output
′ ′ ′
15 Λ(q ,π(X),π(Y)) ← Λ(q ,π(X),π(Y))∪{(r ,X,Y,r )}, ⊲ Note we should not
1
′ ′
have a different Y with same r unless (W-)inconsistency
′
16 q ← ∆(q ,π(X),π(Y)), r ← r
1
′
17 else ⊲ Learn tail of transition from q on input X
′′
18 for some w 6∈ dom(π (q )),
1
19 (T,ξ,r )← Apply(T,r ,w)
2 1
′ ′ ′
20 Λ(q ,π(X),π(Y)) ← Λ(q ,π(X),π(Y))∪{(r ,X,Y,r )}
1
′′
21 π (q )(w)) ← π(ξ) ⊲ This updates ∆. π is first element of couple
1 1
′′
22 if dom(π (q )) = W then
1
′′
23 Q ← Q∪{(π (q ),ρ (r ,ǫ)}
1 w 1
24 A ← UpdateAccess(T,q ′′ ,ρ (r ,ǫ)) ⊲ We record the shortest access to
w 1
′′
(q ,r ) from (q,r) or the last homing in T
1
∗ ′
25 q ← ∆ (q ,π(X.w),π(Y.ξ)) if defined else q ← ⊥
26 r ← r
2
27 else
28 q ← ⊥
−
29 if ∆ (∆,Λ,h,a,A) (where h/a was latest homing in T) is defined and contains
−
a complete strongly connected component (Scc) then ⊲ ∆ trims the ∆ graph
from h/a by cutting transitions that cannot be accessed with A and Λ
30 J ← I
s
31 until ∆,Λ are complete over I on a Scc
s
32 return T,Q,∆,Λ
11
Vega Groz Oriat Foster Walkinshaw Sima˜o
Algorithm 3 Transferring from q,r to next transition to learn
1 Function Transfer(q,r,∆,Λ,I ,I ) ⊲ We look for deterministic transfer, either through
1 2
unguarded transitions, or when registers and input determine known transitions
′
2 ⊲ To reach a given input configuration r , we record the list of parameter values that
can be set by unguarded transitions while building the path, and can adapt the parameter
′
values at the end to set values to r
3 ⊲ First we look for a short transfer with a bounded search (k ≥ 0 is the bound, tailorable),
and if that fails, we resort to a path from re-homing
4 Find short(-est) α ∈ (I ×O)k and X ∈ I with ∆ ∗ (q,π(α)) = q ′ and
1
′ ′ ′
ρ (r,α) = π (q ) = ρ (r ,ǫ) s.t. π (∆(q ,π(X),∗)) is partial, ⊲ First try to learn new
w 2 w 1
input from a state, in its reference configuration
5 ⊲ “partial” means either not defined at all, or there is an output whose characterisation
′
is partial; if Ω was the output for π(X) it is not partial regardless of r and X; if it was
′ ′
ω for a given r and X, we can still look for a different r or X.
′ ′ ′ ′
6 or ∃r 6= π (q ),Y 6= Y s.t. ρ (r,α) = ρ (r ,ǫ) and
2 g g
′ ′ ′ ′
{(π (q ),X,Y,∗),(r ,X,Y ,∗)} ⊂ Λ(q ,π(X),π(Y)) and
2
′ ′
π (∆(q ,π(X),π(Y ))) is partial ⊲ or a guarded transition
1
7 if previous fails then ⊲ (otherwise) no transition to learn in current Scc
′
8 Find α and X ∈ I s.t. Λ(q ,π(X),∗) does not contain (∗,X,∗,∗) ⊲ transfer to a
2
transition to be sampled
′ ∗ ′
9 q = ∆ (q,π(α)), r = ρ(r,α)
10 if all previous fails then ⊲ bounded search failed, resort to homing
11 (T,a,r) ← Apply(T,r,h) and update Λ on the way (if transitions are in Q)
−
12 if ∆ (∆,Λ,h,π(a),A) no longer contains unsampled states and transitions then
13 return no path found ⊲ all transitions in reachable Scc already sampled on I ,
2
transfer fails
− ′ ′
14 LookbyBFSforshortestsequenceαin∆ (∆,Λ,h,π(a),A), pickA(π(a))(q ,r ) = α
′
and X s.t. π (∆(q ,π(X),∗)) is partial or has guarded transition to partial state, or
1
failing that has a transition to be sampled
′ ′ ′
⊲ Here q ,r ,α,X are defined. If ∆ was partial in q, then α = ǫ, q = q
15 (T,β,r ′ )← Apply(T,r,α) and update Λ on the way
16 if β 6= π (α) then ⊲ Transfer stopped prematurely on differing output
o
′ ′ ′ ′′ ′ ′′ ′ ′ ′
17 let β = β .o, α =α.X.α /β .o.β s.t. π (α) =β ,o 6= o
o
′ ′ ′ ′ ′ ∗ ′
18 Y ← o, r ← r ,r ← ρ(r,α),q ← ∆ (q,α) ⊲ We found a new guarded transition
1
′
19 ⊲ If h or W were not correct, we should handle inconsistency if after α we have
′
same input configuration r for o and o
′ ′ ′ ′ ′′ ′′′
20 else ⊲ (q,r)−α()/β() → (q,r)−X/Y → (q ,r )→ w/ξ → (q ,r )
1 2
21 (T,Y,r )← Apply(T,r ′ ,X)
1
22 A ← UpdateAccess(T,q ′ ,r ′ ) ⊲ We record the shortest access to (q ′ ,r ′ ) from (q,r) or
the last homing in T
′ ′
23 return (q ,r ,X,Y,r )
1
12
Learning EFSM
Algorithm 4 Main ehW algorithm
∗
1 Input: I ⊂ I ⊂ I ⊂ I, W ⊂ I , R ⊂ R ⊂ R
1 2 s 1 w g
2 h ∈I+, s.t. ρ (⊥,h) has a value for each register in R
1 g g
3 Initializing: T ← ǫ ⊲ T is the learning trace
4 repeat
5 Q,∆,Λ ← ∅, I ← I
o 1
6 repeat
7 T,Q,∆,Λ ← Backbone(T,I ,I ,h,W,Q,∆,Λ)
1 o
8 handle inconsistencies on the way to update h,W,I ⊲ if h & W not trustable
1
9 for I incrementally ranging from I to I ∪R (I ) do
o 1 1 w 2
10 ⊲ Sampling on R (I ) first to have the smallest set of register configurations to
w 2
compare, hence smallest number of redundant states
11 T,Q,∆,Λ ← Backbone (T,I ,I ,h,W,Q,∆,Λ)
1 o
12 I ← I ∪R (I )
o 1 g 2
13 T,Q,∆,Λ ← Backbone (T,I ,I ,h,W,Q,∆,Λ)
1 o
14 for I incrementally ranging from I to I ∪R (I ) then ∪R (I ) do
o 1 1 w 2 g 2
15 (T,CE) ← GetNFSMCounterExample(T,Q,∆,Λ,SUL,I ) ⊲ Ask for a CE
o
using only inputs from I
o
16 if CE found then
17 (W,I ,I ,Q,∆,Λ) ← ProcessCounterexample ⊲ If W changed, this re-
1 s
sets Q,∆,Λ
⊲ At this point, I would not be changed
s
18 continue to start of repeat Backbone loop
19 I ← I ⊲ If R is correct, I will not change NFSM, just feed generalise
o s g s
20 until Backbone terminates with no inconsistency
21 (Q,∆,Λ) ← reduceFSM(Q,∆,Λ) ⊲ Reduce, not Minimize, as there is no unique min-
imum
22 repeat
23 M ← generalise(T,h,Q,I,O,P ,P ,∆,Λ)
I O
24 (T,CE) ← GetCounterExample(M,SUL)
25 until ¬ (CE is a data CE)
26 if CE found then
27 (W,I ,I ,Q,∆,Λ) ←ProcessCounterexample⊲IfWmodified, Q,∆,Λare reset
1 s
28 until no counterexample found
29 return M
13
Vega Groz Oriat Foster Walkinshaw Sima˜o
14

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Learning EFSM Models with Registers in Guards"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
