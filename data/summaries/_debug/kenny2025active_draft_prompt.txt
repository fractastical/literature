=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Active Inference in Discrete State Spaces from First Principles
Citation Key: kenny2025active
Authors: Patrick Kenny

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Abstract: Weseektoclarifytheconceptofactiveinferencebydisentanglingit
from the Free Energy Principle. We show how the optimizations that
needtobecarriedoutinordertoimplementactiveinferenceindiscrete
statespacescanbeformulatedasconstraineddivergenceminimization
problems which can be solved by standard mean field methods that
do not appeal to the idea of expected free energy. When it is used
to model perception, the perception/action divergence criterion that
we propose coincides with variational free energ...

Key Terms: discrete, energy, principles, free, state, action, spaces, perception, divergence, entropy

=== FULL PAPER TEXT ===

Active Inference in Discrete State Spaces from
First Principles
Patrick Kenny
Patrick.Kenny.000@gmail.com
Centre de recherche informatique de Montr´eal
December 11, 2025
Abstract
Weseektoclarifytheconceptofactiveinferencebydisentanglingit
from the Free Energy Principle. We show how the optimizations that
needtobecarriedoutinordertoimplementactiveinferenceindiscrete
statespacescanbeformulatedasconstraineddivergenceminimization
problems which can be solved by standard mean field methods that
do not appeal to the idea of expected free energy. When it is used
to model perception, the perception/action divergence criterion that
we propose coincides with variational free energy. When it is used to
model action, it differs from an expected free energy functional by an
entropy regularizer.
Contents
1 Introduction 3
2 The Bayesian Brain 4
2.1 Probabilities and Beliefs . . . . . . . . . . . . . . . . . . . . . 4
2.2 Perception as Probabilistic Inference . . . . . . . . . . . . . . 5
2.3 Active Inference and the Free Energy Principle . . . . . . . . . 7
2.4 Road Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1
5202
ceD
01
]IA.sc[
2v12302.1152:viXra
3 Bayesian Machine Learning 10
3.1 Entropy, Cross Entropy and Divergence . . . . . . . . . . . . . 10
3.2 The Mean Field Approximation . . . . . . . . . . . . . . . . . 13
3.3 Variational Free Energy . . . . . . . . . . . . . . . . . . . . . 15
4 Analogies with Statistical Mechanics 16
5 Directed Graphical Models 22
5.1 A Toy Model of Predictive Processing in the Visual Cortex . 22
5.2 Static and Dynamic Bayesian Networks . . . . . . . . . . . . . 25
5.3 Hidden Markov Models . . . . . . . . . . . . . . . . . . . . . . 26
6 The Perception/Action Cycle 27
6.1 Perception and Action as Divergence Minimization . . . . . . 28
6.2 Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
6.3 Time-Domain Renormalization . . . . . . . . . . . . . . . . . 34
7 The Exploration/Exploitation Tradeoff 35
8 Learning 38
8.1 Dirichlet Priors . . . . . . . . . . . . . . . . . . . . . . . . . . 39
8.2 Learning as Divergence Minimization . . . . . . . . . . . . . . 40
9 Whither Expected Free Energy? 43
9.1 Expected Free Energy Functionals . . . . . . . . . . . . . . . . 43
9.2 Beliefs about Policies . . . . . . . . . . . . . . . . . . . . . . . 45
9.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
10 Conclusion 48
References 50
Appendix A Posterior Update Formulas 53
A.1 Predicting the Future . . . . . . . . . . . . . . . . . . . . . . . 53
A.2 Retrodicting the Past . . . . . . . . . . . . . . . . . . . . . . . 54
A.3 Evaluating the Divergence . . . . . . . . . . . . . . . . . . . . 56
2
1 Introduction
The idea that sense perception can be understood as an unconscious pro-
cess of inference dates back to Helmholtz. Among his many other scientific
contributions is the concept of Helmholtz free energy which quantifies the
energy in a system that is available to do physical work such as moving a
piston. Hinton and his collaborators realized that approximate Bayesian in-
ference in machine learning could be viewed as a problem of optimizing an
abstract mathematical quantity which is formally identical to Helmholtz free
energy and which has come to be known as variational free energy. (Thus
the Helmholtz machine [1] and the Boltzmann machine [2].) In a long series
of papers (notably [3, 4, 5]), Friston has developed the connections between
these ideas and greatly expanded their scope by showing how action as well
as sense perception can be modelled as approximate Bayesian inference.
If perceptions are inferences drawn from bodily sensations, then actions
can be viewed as fulfilling predictions of future sensations. Whereas per-
ceptual inference is thought to consist of computing approximate posterior
probability distributions, active inference is a matter of computing approxi-
mate predictive distributions. Both types of inference can be formulated as
optimizationproblemswhichareamenabletothesortofvariationalinference
algorithms that have been developed in machine learning. In the case of per-
ception, the objective function to be optimized is just variational free energy
and this optimization can be achieved by the well known mean field approx-
imation [6]. In the case of action, a new type of objective function known as
expected free energy has been proposed and new variational methods have
been developed to optimize it [7, 8, 9, 10, 11].
“It is said that the Free Energy Principle is difficult to understand.” Thus
the opening sentence of a review article that purports to simplify the Free
Energy Principle but brooks no compromise on Friston’s program of ground-
ing variational inference by biological agents in statistical physics [12]. For
a physicist studying active inference, it is natural to assume that a biolog-
ical agent models its world as a random dynamical system governed by a
stochastic differential equation and to construe the agent’s striving to main-
tain homeostasis as a pullback attractor. However, in simulating the be-
haviour of biological agents or in designing autonomous AI agents capable of
multistep hierarchical planning, an engineer would assume instead that an
agent models its world as a discrete state space that evolves in discrete time
steps using a Hidden Markov Model or Partially Observable Markov Decision
3
Process. The mathematical apparatus that the engineer needs to deploy is
much simpler than that required by the physicist. In this paper we aim to
give a self-contained and mathematically rigorous account of active inference
in discrete state spaces without appealing to any of the machinery that has
been developed in the context of continuous state spaces.
Although active inference and the Free Energy Principle are usually con-
flated (as in the title of the active inference textbook [13], for example) we
will distinguish between the two because, as we will see, active inference in
discrete state spaces (understood as the problem of inferring predictive prob-
ability distributions for an agent’s future sensations) is amenable to treat-
ment by standard mean field methods which do not appeal to the idea of
expected free energy. We will show how this approach enables us to model
the perception/action cycle in a unified way by optimizing a single Kullback-
Leibler divergence criterion rather than by optimizing variational free energy
to model perception and expected free energy to model action, as is usually
done.
We will briefly review the Bayesian Brain Hypothesis, and basic ideas
in Bayesian machine learning and statistical mechanics before getting down
to business. Readers who have had no previous exposure to the subject of
active inference may find it useful to consult popularizations by Andy Clark
[14], Anil Seth [15] and Mark Solms [16].
2 The Bayesian Brain
In this section, we sketch the Bayesian Brain Hypothesis, Active Inference
and the Free Energy Principle and we lay out a roadmap for the paper.
2.1 Probabilities and Beliefs
“In the greatest part of our concernments, [God] has afforded us only the
twilight, as I may so say, of probability; suitable, I presume, to that state
of mediocrity and probationership he has been pleased to place us in here;
wherein, to check our over-confidence and presumption, we might, by every
day’s experience, be made sensible of our short-sightedness and liableness to
error.”
Thus John Locke, writing in 1689 [17]. What Locke understood by prob-
ability is unclear and, although the mathematical theory of probability has
4
long been well established, there is still no consensus among statisticians,
engineers and physicists as to what probabilities mean. Neuroscientists how-
ever have settled on the Bayesian interpretation of probabilities as credences
or degrees of belief. (This explains why the terms “probability distribution”
and “belief” have come to be used interchangeably in the active inference
literature.)
Under this interpretation, probabilities refer to the state of an observer’s
knowledge of events rather than to objective knowledge of the events them-
selves. Physical scientists generally balk at this interpretation for obvious
reasons, but it has the great advantage of formalizing the problem of rea-
soning under uncertainty in a very parsimonious way: all inference, without
exception, reduces to the sum and product rules for combining probabilities.
Provided that numerical values are correctly assigned to beliefs and probabil-
ity calculations are carried out exactly, Bayesian decision theory is provably
optimal.
On the Bayesian Brain Hypothesis, the brain is a repository of beliefs
about how the body and environment work and how they interact with each
other. Bayesian beliefs about events are subject to revision as more informa-
tion becomes available. (More formally: prior probabilities are converted to
posterior probabilities by applying Bayes’ rule.) As the brain collects sense
data from the body in a given situation, it infers the causes of its sensations
— in other words, what is going on in its world — by approximate Bayesian
inference which converts its prior beliefs (about how the world works in gen-
eral) into posterior beliefs (about what the world is doing right now). These
posterior beliefs are what we think of as reality.
2.2 Perception as Probabilistic Inference
As a first step towards a general formulation of the Bayesian Brain Hypoth-
esis, consider that the primary function of the brain is to regulate the body
and to act on the environment in such a way as to maintain the body in a
state of homeostasis. All of the brain’s knowledge of what is going on in its
world (which includes the body as well as the environment) at a given time is
mediated by incoming sensory data. We can model this state of affairs with a
steady state joint probability distribution p(s,o|S) where s is a vector whose
componentsrepresentthestateoftheorganism’sworldandoisavectorcom-
prised of all of the incoming sensory data (interoceptive and proprioceptive
as well as exteroceptive). The state vector s can be thought of as compris-
5
ing the causes of the observation vector o. The marginal distribution p(s|S)
encodes the brain’s prior knowledge of its world. The state of the world at
a given time is hidden in the sense that it cannot be directly observed but
has to be inferred from incoming sense data. From this perspective, sense
perception is just this process of inference.
The terminology “observation” should not be read as suggesting that
sense data are generally accessible to consciousness. We are normally not
consciously aware of most of our visceral and proprioceptive sensations, just
as we are unaware of the sensations of sound pressure waves impinging on our
ears(exceptinthecaseofloudnoises)oroflightwavesimpingingonoureyes
(except in the case of sudden flashes). Consciousness appears mysterious to
us because, along with many of the sensations themselves, the process which
converts sensations to perceptions is hidden from us.
Foragivenobservationvectoro, themarginalprobabilityp(o|S)isknown
asthemodel evidenceanditsnegativelogarithm, −lnp(o|S), asthesurprisal.
(The less probable an event, the more surprising it is.) In principle the
evidence or surprisal could be evaluated by summing over all possible states
of the world:
(cid:88)
p(o|S) = p(s,o|S)
s
butthissummationisobviouslyintractable. Howeveriftheevidencecouldbe
evaluated, the problem of inferring the state of the world from sensory data
could be solved by a direct calculation of the posterior probability p(s|o,S):
p(s,o|S)
p(s|o,S) = . (2.1)
p(o|S)
Approximations cannot be avoided and, given the scale of the problem,
it is reasonable to assume that the brain uses variational approximations like
those that have been developed in machine learning rather than Monte Carlo
methods. So we postulate that the brain calculates some sort of mean field
approximation to the posterior distribution p(s|o,S). As a byproduct, this
calculation gives an approximation to the model evidence which is known
in machine learning as the evidence lower bound or ELBO. Variational free
energy is defined by changing the sign on the ELBO, so that one talks about
minimizing the variational free energy rather than maximizing the ELBO
and minimizing the surprisal rather than maximizing the evidence.
On this account, variational free energy is constantly being minimized as
the approximate posterior distribution is updated from one moment to the
6
next. It is proposed in [18] that qualia can be understood as transductions
of bodily sensations which are encoded in the brain by the parameters which
specify this approximate posterior distribution in much the same way as
photographs are encoded in jpeg files. (By a happy coincidence, variational
posterior distributions in machine learning are traditionally denoted by q
rather than p which is reserved for priors and exact posteriors.)
Note that approximating the full posterior distribution p(s|o,S) (rather
than merely computing a point estimate of s) enables the brain to assess
the uncertainty attaching to its explanation of the sense data o. This is
important because an agent cannot act effectively on its environment if it is
uncertain about the state of its world at a given time or in its immediate
future. In such a situation, gathering information with a view to reducing
uncertainty is a primary imperative.
2.3 Active Inference and the Free Energy Principle
In order to extend the Bayesian Brain Hypothesis to account for action as
well as perception, Friston observes that biological agents always seem to
act in ways that can be interpreted as striving to maximize the evidence
for their model of the world or, since the model evidence is intractable, to
minimize variational free energy [19]. This behaviour is often referred to as
“self-evidencing” in the active inference literature. Thus the agent is said to
act on its world in such a way as to maximize the evidence for its continued
existence or to minimize its surprise at what happens to it.
To this end, a biological agent needs to be able to predict its future
sensations in much the same way as large language models predict text.
Whereas the predictions made by large language models are driven by a
past history and the statistics of natural languages, the predictions made
by a biological agent are driven by its history up to the present moment
and the requirement that future sensations are drawn from the steady state
marginal distribution p(o|S). Action is then construed as fulfilling these
predictions. More specifically, actions ultimately reduce to autonomic and
motor reflexes (that is, secreting hormones and contracting muscles) which
fulfill predictions of interoceptive and proprioceptive sensations [20]. This
move brings action under the umbrella of the Bayesian Brain Hypothesis
without invoking extrinsic ideas about goals, rewards or utility functions.
Action is governed by prediction, that is, by probabilistic inference of the
future states of the world. Hence the term active inference.
7
Predictions in exteroceptive sense modalities such as seeing and hearing
over which the agent has limited control may be less precise, but navigating
an environment obviously requires the ability to anticipate events and not
merely to react to events after the fact. Indeed, it is difficult to see how
a biological agent could learn to act effectively on the world it finds itself
inhabiting other than by learning to predict the sensory consequences of its
behaviour in much the same way as large language models learn the statistics
of natural languages by learning to predict text one word at a time.
Predicting the future requires a probability model which is capable of
tracking trajectories in the state space over time. (The steady state distribu-
tion p(s,o|S) that we have been considering is not adequate in this respect
since it treats states at successive times as being statistically independent.)
The obvious choice is to model state trajectories as a Markov chain by as-
signing to each state a probability distribution over states that can be visited
next. This type of model is usually referred to as a Hidden Markov Model
(HMM) rather than a Markov chain since the state occupied at a given time
cannot be directly observed but has to be inferred probabilistically. If the
states were observable, trajectories in the state space would appear to be
directed towards regions that have high probability under the steady state
marginal distribution p(s|S), subject to perturbations due to random shocks
coming from the agent’s environment.
A HMM is a joint distribution on sequences of states and observations,
rather than on individual states and observations as in the case of the steady
state distribution. Under this sort of dynamic model, perception is under-
stood to be a matter of recognizing patterns that unfold over time (rather
than static objects) and variational free energies are associated with se-
quences of observations (rather than with individual observations). So an
agent contemplating an action could assign a variational free energy to it if
it could foretell how the sensory consequences of the action would unfold.
Of course the agent cannot foresee the future but, for each action that is
possible in a given situation, it could calculate an expected value for the
variational free energy of the sense data it would encounter if the action was
performed. This would enable the agent to assign approximate probabilities
to the various possible actions that are open it at a given time and use these
probabilities in inferring a predictive distribution for its future sensations
by variational methods. The Free Energy Principle asserts that biological
agents “decide” on courses of action in this way. (On Friston’s account [21],
the ability to look ahead in performing this sort of variational inference is a
8
hallmark of conscious behaviour.)
On the other hand, we will show in this paper that the problem of active
inference, understood as the problem of inferring a probability distribution
for an agent’s future sensations, can be solved by standard mean field meth-
ods that do not appeal to the idea of expected free energy.
2.4 Road Map
The Free Energy Principle suggests how an objective function known as ex-
pected free energy might be defined which would enable active inference to
be cast as an optimization problem in the same way as perceptual inference.
But, because it purports to be a general principle like Hamilton’s principle
of stationary action, the Free Energy Principle does not commit to a pre-
cise definition of this objective function. In this paper, we will proceed from
the assumption that, for an agent that uses a HMM to model its world, the
Kullback-Leibler divergence of a predictive distribution from the HMM is
a natural measure of the quality of the predictive distribution. So rather
than appealing to the Free Energy Principle, we posit that a (suitably con-
strained) Kullback-Leibler divergence can serve as the objective function for
active inference and we show how it can be optimized by standard mean field
methods.
The mean field approximation is generally used to infer approximate pos-
terior distributions of hidden variables after data has been collected. Thus,
given a HMM p(s,o) and a sequence of observations up to time t denoted
by o , minimizing variational free energy provides an approximation to the
≤t
posterior distribution p(s |o ) where s is the hidden state sequence that
≤t ≤t ≤t
accounts for o . (The lower bound on the model evidence p(o ) known as
≤t ≤t
the ELBO is a byproduct of this calculation.) We will explain the mean field
approximation in Sections 3 and 4. We will show how it can be used to pro-
vide an approximation to the predictive distribution p(s ,o |o ) on future
>t >t ≤t
states and observations in addition to the posterior distribution p(s |o ) in
≤t ≤t
Section 6. The core idea is very simple: it consists in treating future states
and observations s and o as hidden variables on the same footing as s
>t >t ≤t
and calculating a variational posterior (conditioned on the observations o )
≤t
for this augmented set of hidden variables. This perspective enables us to
treat the perception/action cycle in a unified way. We model it by minimiz-
ing a single Kullback-Leibler divergence functional rather than minimizing
variational free energy in the case of perception and expected free energy in
9
the case of action, as is usually done.
It is shown in [22] how a wide variety of reinforcement learning algo-
rithms can be formulated as solving constrained divergence minimization
problems and the exploration/exploitation tradeoff in reinforcement learning
is analyzed in this general setting. We discuss how this applies to HMMs in
Section 7.
Section 8 shows how mean field methods apply to the problem of learning
HMM parameters. Although this material is not new, we include it for
completeness.
We discuss the relationship between our perception/action divergence cri-
terion and various expected free energy functionals in Section 9. We explain
the (implicit or explicit) approximations that are used to derive these ex-
pected free energy functionals and how the divergence criterion avoids them.
It turns out that, when it is used to calculate predictive distributions, the
perception/action divergence criterion only differs from a free energy func-
tional by an entropy regularizer, although the functional in question is not
one that has been used in the active inference literature.
3 Bayesian Machine Learning
This section explains the mean field approximation and the concept of varia-
tionalfreeenergy. Webeginwithanoverviewoftherudimentsofinformation
theory and Bayesian machine learning.
3.1 Entropy, Cross Entropy and Divergence
Weusethenotationp(x)torefertoagenericprobabilitydistributionofavec-
tor valued random variable x and q(x) to refer to an approximate probability
distribution. Typically, the distribution p(x) is computationally intractable
and q(x) is constrained to belong to a family of tractable distributions. For
the most part, we take the values of x to be discrete.
In situations where we need to distinguish between a random variable x
and a value it takes, we denote such a value by x¯. For example, we use the
notation δ (x) to refer to the probability distribution all of whose mass is
x¯
concentrated on the value x¯:
(cid:40)
1 if x = x¯
δ (x) =
x¯
0 otherwise.
10
Theinformation contentorsurprisaloftheeventthattherandomvariable
takes the value x¯ is defined to be the negative log probability, −lnp(x¯).
(The more unlikely an event, the greater the surprise at its occurrence.) The
entropy of p(x) and the cross entropy of p(x) relative to q(x) are defined by
averaging the information with respect to p(x) and q(x) respectively:
(cid:88)
E [−lnp(x)] = − p(x)lnp(x)
p(x)
x
(cid:88)
E [−lnp(x)] = − q(x)lnp(x).
q(x)
x
We denote the entropy of p(x) by H[p(x)]. This has the property that
0 ≤ H[p(x)] ≤ lnK
where K is the number of values that x can take. The minimum value of 0 is
attained by point mass distributions and the maximum value by the uniform
distribution which assigns equal probabilities to all values of x. So H[p(x)]
can be thought of as a measure of how flat the distribution p(x) is.
The Kullback-Leibler divergence of q(x) from p(x) is the difference be-
tween the cross entropy and the entropy:
KL[q(x)∥p(x)] = E [−lnp(x)]−H[q(x)] (3.1)
q(x)
(cid:88) q(x)
= q(x)ln .
p(x)
x
It has the property that KL[q(x)∥p(x)] ≥ 0 with equality holding iff q(x)
and p(x) are identical. Note that the divergence is not symmetric in its
arguments (for more on this see [23]).
Either the forward divergence KL[p(x)∥q(x)] (the “divergence of p(x)
fromq(x)”)orthereverse divergenceKL[q(x)∥p(x)](the“divergenceofq(x)
from p(x)”) can be used to evaluate how well the target distribution p(x) is
approximated by q(x). We will mostly use reverse divergences in this paper
as we will usually be dealing with situations where the target distribution
p(x) is computationally intractable and the approximating distribution q(x)
is tractable so that the reverse divergence can be evaluated and optimized
whereas the forward divergence cannot.
A situation where it is natural to use the forward divergence as the opti-
mizationcriterionariseswhenthetargetdistributionp(x)isgivenempirically
11
in the form of a training data set. Since the entropy H[p(x)] is independent
of q(x), minimizing the forward divergence KL[p(x)∥q(x)] is equivalent to
minimizing the cross entropy E [−lnq(x)]. This is generally referred to as
p(x)
the cross-entropy loss function in non-Bayesian machine learning and mini-
mizing it is equivalent to maximum likelihood estimation of q(x) [23]. It is
well known that maximum likelihood estimation is vulnerable to overfitting.
On the other hand, when the reverse divergence is used as the optimization
criterion, the the entropy H[q(x)] which appears with a negative sign in (3.1)
mitigates against this tendency by penalizing approximating distributions of
low entropy. It is often referred to as an “entropy regularizer” for this reason.
Reverse divergences arise naturally when posterior distributions need to
beapproximated. SupposeweseektominimizethedivergenceKL[q(x)∥p(x)]
subject to the constraint that q(x) = 0 outside of a set A so that the opti-
mand is
(cid:88) q(x)
q(x)ln .
p(x)
x∈A
Define a probability distribution p′(x) by
(cid:40)
1 p(x) (x ∈ A)
p′(x) = p(A) (3.2)
0 (x ∈/ A)
so that
(cid:88) q(x) (cid:88) q(x)
q(x)ln = q(x)ln −lnp(A).
p(x) p′(x)
x∈A x∈A
The first term on the right-hand side is just the divergence of q(x) from p′(x)
and we can ignore the second term since it is independent of q(x). If no
additional constraints are imposed on q(x), then the divergence assumes the
minimum value of 0 when q(x) = p′(x). Since the expression in the first line
of (3.2) is just the conditional distribution of x given that x ∈ A, what this
calculation shows is that Bayes rule can be derived by solving a constrained
divergence minimization problem. All of the calculations that we carry out
in this paper will follow this pattern.
We will frequently need to calculate the divergence of one joint distribu-
tion from another. For this we will use the following “chain rule”. Given two
joint distributions q(x,y) and p(x,y), the divergence KL[q(x,y)∥p(x,y)]
12
can be written in either of the forms
KL[q(x)∥p(x)]+E [KL[q(y|x)∥p(y|x)]]
q(x)
KL[q(y)∥p(y)]+E [KL[q(x|y)∥p(x|y)]] (3.3)
q(y)
This is a straightforward consequence of the definition of the divergence.
3.2 The Mean Field Approximation
We can associate an energy function E(x) with a probability distribution
p(x) by setting E(x) = −lnp(x) so that p(x) = exp(−E(x)). (In physics,
low energy states are more probable than high energy states. Hence the
negative sign.) Conversely, given an energy function E(x), we can define a
probability distribution p(x) by setting
1
p(x) = e−E(x)
Z
where the partition function Z is determined by the requirement that prob-
abilities sum to 1.
A well known example is the multivariate Gaussian distribution which is
defined by an energy function of the form 1x⊤Σ−1x. In a situation where
2
the state space is of high dimension and sparsity constraints are imposed
on the precision matrix Σ−1, this distribution is referred to as a Gaussian
Markov Random Field. If x is constrained to be binary valued, the corre-
sponding distribution is known as the Ising model in statistical physics and
as the Boltzmann machine in machine learning. All of these distributions are
intractable in the case of high dimensional state spaces in the sense that the
partition function cannot be evaluated.
Regardlessofwhetherp(x)istractableornot, wecanseektoapproximate
itbyatractabledistributionq(x)usingthereversedivergenceKL[q(x)∥p(x)]
as the optimization criterion. In the mean field approximation, x is decom-
posed into subvectors x ,...,x and statistical independence constraints are
0 N
imposed so that the approximating distribution q(x) factorizes as
q(x )q(x )...q(x ). (3.4)
0 1 N
The procedure that we will present for calculating q(x) (which is referred
to as coordinate ascent variational inference in [6]), requires that the factors
13
are updated asynchronously (that is, one at a time rather than in parallel).
The divergence KL[q(x)∥p(x)] is guaranteed to decrease on each update
but, although the procedure is guaranteed to converge, it may converge to a
local rather than a global minimum of the divergence. So it may need to be
initialized carefully.
The rule for updating a factor q(x ) holding the remaining factors fixed
n
is very simple: q(x ) is the distribution defined by the energy function
n
E [E(x)]wherex denotesthesetofcomponentsofxotherthanx . To
q(x ) \n n
\n
see that this update is guaranteed to decrease the divergence KL[q(x)∥p(x)]
let us write the divergence in the form
(cid:104) (cid:105)
E E [lnq(x)−lnp(x)] .
q(xn) q(x
\n
)
We need to minimize this with respect to q(x ). Using the notation +... to
n
indicate terms which are independent of x and hence irrelevant,
n
lnq(x) = lnq(x )+...,
n
so minimizing the divergence is equivalent to minimizing
E [lnq(x )−lnq˜(x )] (3.5)
q(xn) n n
where q˜(x ) is defined by
n
lnq˜(x ) = E [lnp(x)].
n q(x )
\n
Generally speaking, the vector q˜(x ) does not define a probability distribu-
n
tion because its components may not sum to 1, but if they did the expression
(3.5) would be the divergence from q(x ) to q˜(x ) and it could be minimized
n n
by setting q(x ) = q˜(x ). Since rescaling the vector q˜(x ) only changes (3.5)
n n n
by an irrelevant additive constant, it follows that the update formula for
q(x ) is q(x ) ∝ q˜(x ) where the constant of proportionality is determined
n n n
by the condition that the components of q(x ) sum to 1. In sum:
n
q(x ) ∝ q˜(x ) where lnq˜(x ) = E [lnp(x)] (3.6)
n n n q(x )
\n
Implementing these update formulas does not require that the divergence
thatisbeingminimizedeverneedstobeevaluated. However,thefactthatthe
divergence is guaranteed to decrease whenever one of the factors is updated
is very useful for debugging.
14
In machine learning, the terms “mean field approximation” and “varia-
tional inference” are generally used interchangeably. As in the calculus of
variations, the mean field updates (3.6) are derived without imposing any
conditions the functional forms of the factors in (3.4) even in the case of con-
tinuous distributions. Hence the epithet “variational”. The term “inference”
generally refers to calculating exact or approximate posterior distributions
of hidden variables in a probability model. (We will see how the mean field
approximation accomplishes this in the next section.) However, as we have
presented it, the mean field approximation can be applied to the problem of
calculating a factorized approximation q(x) to any target distribution p(x)
provided that this problem is formulated as one of minimizing the reverse di-
vergence KL[q(x)∥p(x)]. We will see in Section 6 how variational methods
can be brought to bear on active inference as well as perceptual inference by
casting the problem of inferring predictive distributions as one of optimizing
a (suitably constrained) KL divergence rather than an expected free energy
functional.
3.3 Variational Free Energy
Suppose now that some of the components of x are observable but others
are not. In keeping with the notation that we will use later on, we set
x = (s,o) where o stands for the observable variables and s for the hidden
variables. Here we explain how variational inference can be used to derive
an approximation to the posterior distribution p(s|o) in situations where the
exact posterior is intractable.
Suppose that o is observed to take the value o¯. As we saw in Section 3.1,
the exact posterior p(s|o) can be viewed as the solution to the problem of
minimizing the divergence KL[q(s,o)∥p(s,o)] subject to the constraint that
q(s,o) = 0 unless o = o¯ or, equivalently, that q(s,o) can be written in the
form
q(s,o) = q(s)δ (o).
o¯
If q(s) is required to factorize as in (3.4), then the global minimum of the
divergence may not be attainable but the mean field algorithm returns an
approximation to the exact posterior whose quality can be measured by the
numerical value of the divergence. This numerical value is known as the
variational free energy (VFE):
VFE = KL[q(s,o)∥p(s,o)] where q(s,o) = q(s)δ (o). (3.7)
o¯
15
This is the most convenient way of defining variational free energy for our
purposes but we can use the chain rule (3.3) to express it in more familiar
ways. The divergence KL[q(s,o)∥p(s,o)] can be written in either of the
forms
KL[q(s)∥p(s)]+E [KL[q(o|s)∥p(o|s)]]
q(s)
or
KL[q(o)∥p(o)]+E [KL[q(s|o)∥p(s|o)]]
q(o)
which simplify to
KL[q(s)∥p(s)]+E [−lnp(o¯|s)] (3.8)
q(s)
and
−lnp(o¯)+KL[q(s)∥p(s|o¯)] (3.9)
under the assumption that q(s,o) = q(s)δ (o). In the active inference litera-
o¯
ture, thefirsttermin(3.8)is knownasthe complexityasitcanbeinterpreted
as penalizing distributions q(s) whose divergence from the prior p(s) is large.
The second term is referred to as the accuracy since it can be interpreted as
the average error incurred in reconstructing o¯ by sampling from the distri-
bution q(s).
Since the second term in (3.9) is non-negative, the variational free energy
bounds the surprise −lnp(o¯) from above. So choosing q(s) to minimize the
variational free energy can serve as a proxy for minimizing the surprise in
situations where the marginal distribution p(o) is intractable.
4 Analogies with Statistical Mechanics
Particularly in his early writings on the Free Energy Principle, Friston fre-
quently conflates variational free energy and thermodynamic free energy, the
energy in a system which is available to perform physical work such as mov-
ing a piston. This move has confused many of his readers and it is not needed
to understand active inference, but the analogies with statistical mechanics
are so fruitful that it is worth making an effort to understand how far they
can be pushed. The ideas presented in this section are speculative and they
16
will not be needed later so readers who are not interested in this topic are
invited to skip it.
A physicist using statistical mechanical methods to study the thermody-
namics of a neural population or of the brain as a whole would aim to write
down an energy function E(x) which would, at least in principle, enable her
to calculate the energy of the system given its microstate x. This energy
function would have the property that the probability distribution p(x) of
microstates when the system is in thermal equilibrium with its environment
is a Boltzmann distribution. That is,
1
p(x) = e−βE(x) (4.1)
Z(β)
where the parameter β is related to the temperature T of the system by
1
β =
k T
B
(k is Boltzmann’s constant). Taking β = 1, the free energy of the system
B
is the total energy minus the entropy. Calculating the total energy by aver-
aging over microstates and idetntifying the thermodynamic entropy with the
Shannon entropy H[p(x)], the free energy F[p(x)] is given by the expression
E [E(x)]−H[p(x)]
p(x)
which reduces to −lnZ. If the neural population is maintained in a non-
equilibrium steady state by sensory stimulation from the outside world and
q(x) is the distribution of microstates under this condition, then the non-
equilibrium free energy F[q(x)] is given by
E [E(x)]−H[q(x)]
q(x)
which can be written as
KL[q(x)∥p(x)]−lnZ.
So the equilibrium and non-equilibrium free energies are related by
F[q(x)] = KL[q(x)∥p(x)]+F[p(x)].
Thus the divergence KL[q(x)∥p(x)] can be interpreted as the additional
thermodynamic free energy that is dissipated as heat to the environment as
17
thesystemrelaxestowardsequilibrium. Regardedinthisway, theimperative
to minimize the divergence from q(x) to p(x) is mandated by the second law
of thermodynamics.
We have assumed that the non-equilibrium steady state is the result of
external simulation of sensory neurons so that some of the components of
x are fixed. We can restate this assumption in the notation introduced in
Section 3.3: x = (s,o) and o takes the value o¯, so that
KL[q(x)∥p(x)] = KL[q(s,o)∥p(s,o)]
and
q(s,o) = q(s)δ (o).
o¯
Soby(3.7), thedivergenceKL[q(x)∥p(x)]canbeinterpretedasavariational
free energy as well as a thermodynamic free energy.
The Boltzmann distribution arises as the solution of a constrained op-
timization problem, namely the problem of finding the maximum entropy
distribution on microstates for which the expected value of E(x) coincides
with a given observed value. (The parameter β arises as a Lagrange mul-
tiplier.) More generally, given similar constraints on a collection of energy
functions {E(c)(x)|c = 1,...,C} the maximum entropy distribution is a gen-
eralized Boltzmann distribution of the form
(cid:32) (cid:33)
C
1 (cid:88)
p(x) = exp − β(c)E(c)(x) (4.2)
Z(β)
c=1
where β = (β(1),...,β(C)). This way of assigning probability distributions
is known as the maximum entropy principle [24, 25]. Since maximizing the
entropy of a distribution is equivalent to minimizing its divergence from a
uniform distribution, the maximum entropy principle can be regarded as
another instance of constrained divergence minimization.
Considering that all of the probability distributions in statistical mechan-
ics have the form of generalized Boltzmann distributions, the question arises:
Can the brain as a whole be modelled in this way? For this we would need
to define a collection of energy functions which are localized in the sense
that each of them of is a function of the microstates of a (relatively small)
neuronal population. For the sake of argument, let us imagine representing
the brain by a sparsely connected graph as in Fig. 1. The nodes represent
18
neural populations (grey matter) and the branches represent communication
channels between populations (white matter). The variables x ,x ,... asso-
0 1
ciated with the nodes are the microstates of the corresponding populations.
We could associate energy functions E(0)(x ),E(1)(x ),... with each of the
0 1
nodes and define a probability distribution by invoking (4.2) but this would
result in a distribution under which the variables x ,x ,... are statistically
0 1
independentwhichisobviouslyinadequate. InthecaseofaGaussianMarkov
random field, this could be corrected by adding off-diagonal terms to the pre-
cision matrix. Analogously, we could model the dependencies between neural
populations by adding energy functions depending on two sets of variables
for each branch in the graph. For example, the branch joining node 0 and
node 3 would contribute an energy function E(03)(x ,x ) whose role is to
0 3
model the dependency between x and x .
0 3
x x
1 5
x x
x 2 4
0
x
3
Figure 1: A toy model of the brain as a Markov Random Field. Nodes cor-
respond to grey matter and branches to white matter. Energy functions are
associated with both nodes and branches. Energy functions associated with
the nodes model the microstates of neural populations. Energy functions as-
sociated with the branches model the dependencies between the microstates of
the neural populations associated with the nodes. One or more of the nodes
receives sensory data from the external world which changes from moment
to moment. The neural populations are continually striving to dissipate free
energy by exciting or inhibiting the populations with which they are in com-
munication via the branches.
We assume that one of nodes in the graph is distinguished from the others
by being in communication with the world outside the brain (which includes
the body as well as the environment). The neural population corresponding
to this node consists of sensory neurons and the microstate of this popula-
19
tion at a given time depends on what is going on in the world as well as the
microstates of the populations with it which communicates. In this picture,
the “brain” tracks changes in the body and environment by continually up-
dating approximate variational posterior distributions for each of the neural
populations.
Anattractivefeatureofthismodelisthatitsuggeststhatasimilarpicture
obtains at finer scales of resolution. Implementing the mean field update
formula 3.6 for each of the nodes in the graph Fig. 1 requires reading off the
variational posteriors associated with the nodes with which it communicates
andonlythosenodes. Forexample, thevariationalposteriorq(x )isupdated
0
by calculating the energy function E [E(x)] which is given (up to an
q(x )
\0
irrelevant additive constant) by
E(0)(x )+E (cid:2) E(01)(x ,x ) (cid:3) +E (cid:2) E(03)(x ,x ) (cid:3) .
0 q(x1) 0 1 q(x3) 0 3
So nodes 1 and 3 stand in the same relationship to node 0 as the sensory neu-
rons to the “brain” as a whole. Thus if the neural population corresponding
to node 0 admits of a graphical representation similar to Fig. 1, then the
mechanics of updating the local variational posterior q(x ) will be similar to
0
that of updating the global variational posterior q(x ,x ,...). The same sort
0 1
of analysis may be applicable to neural subpopulations at different scales,
perhaps even down to the scale of individual neurons.
There are no restrictions on the topology of the graph in Fig. 1. Vari-
ational free energy is continually being minimized by passing messages be-
tween communicating neural populations but this does not require that the
variational free energy of the brain as a whole be evaluated from moment
to moment. Hence there is no need for a “central processing unit” or neo-
Cartesian “inner screen” [26] in the picture we have sketched. All that is
going on is that each neural population and subpopulation is continually
striving to dissipate its own free energy by exciting and/or inhibiting the
activity of the other populations with which it communicates.
The Lagrange multipliers in (4.1) and (4.2) have an interesting role to
play. Decreasing β (that is to say, increasing the temperature) in (4.1) has
the effect of increasing the entropy of the distribution p(x) which quanti-
fies the uncertainty about the value that x actually takes. This operation
is usually referred to as precision weighting since the effect of applying it
to a Gaussian distribution is to leave the mean unchanged but scale the
precision (or inverse variance) by a factor of β. Similarly, changing the mul-
tipliers associated with the branches in the graph in Fig. 1 has the effect
20
of modulating the strength of the interactions between neural populations.
Precision weighting is thought to be the mechanism underlying attention [27]
and arousal modulation [16] and the computational neuroscience literature
abounds in (more or less plausible) just-so stories which use this mechanism
to explain neurological anomalies such as aphantasia, akinesia and apathy in
dementia, and the sensory effects of psychedelics.
An agent cannot act effectively on its environment if it is uncertain about
thecurrentstateofitsworldorabouttheaccuracyofitspredictionsoffuture
states. In such a situation actions have to be guided by the imperative to
reduce uncertainty. (We will discuss the exploration/exploitation tradeoff in
Section 7.) So an intelligent agent needs to assess the degree of confidence
it has in its inferences and predictions before it acts in a given situation.
Treating the Lagrange multipliers as random variables subject to probabilis-
tic inference would endow the agent with the flexibility needed to do this
well. This topic has hardly begun to be explored in the active inference lit-
erature ([28] is an exception but the “universal generative model” of [29, 30]
does not include precision weighting). So we will not dwell on it here except
to point out that there doesn’t seem to be any major obstacles in the way of
a fully probabilistic treatment should that prove to be useful.
For example, if we assume a joint distribution on microstates and multi-
pliers of the form
(cid:32) (cid:33)
C
(cid:88)
p(β,x) ∝ exp − β(c)E(c)(x)
c=1
and a variational approximation of the form
(cid:89) (cid:89)
q(β,x) = q(β(c)) q(x )
n
c n
the factors q(β(c)) turn out to be exponential distributions. The assumption
here that the β(c)’s are statistically independent in the approximating distri-
bution is arguably unsatisfactory if the intention is to model attention since
attention is usually thought of as a limited resource which is concentrated
on one object at a time. This objection might be answered by assuming a
joint distribution of the form
p(µ,x) ∝
(cid:89)(cid:0)
µ(c)
(cid:1)E(c)(x)
c
21
where the µ(c)’s are non-negative and sum to 1. Under these conditions, a
variational approximation of the form
(cid:89)
q(µ,x) = q(µ) q(x )
n
n
yields a Dirichlet distribution for q(µ). (The Dirichlet distribution is dis-
cussed in Section 8.1.)
5 Directed Graphical Models
Energy based models lend themselves naturally to variational approximation
methods but they are difficult to train in situations where the partition func-
tion cannot be evaluated and this is generally the case if the state space is
of high dimension. For this reason, probability distributions are not usually
specified by energy functions in machine learning but by directed graphical
models, also known as Bayesian networks.
5.1 A Toy Model of Predictive Processing in the Visual
Cortex
For example, the graph in Fig. 2 represents a toy model of hierarchical pre-
dictive processing in the visual cortex [31]. The variable x represents the
N
state of the retina, the variable x represents the proximate causes of x
N−1 N
and the directed arrow joining the two indicates that the causal relationship
is specified by a conditional probability distribution of the form p(x |x ),
N N−1
and so on up the hierarchy. So if we set x = (x ,...,x ), the assump-
0 N
tion is that the joint distribution p(x) factorizes as a product of conditional
probability distributions
p(x) = p(x )p(x |x )···p(x |x ). (5.1)
0 1 0 N N−1
As a warm-up exercise for the calculations that we will perform in Section 6
andtheappendix,considertheproblemofapproximatingp(x)byafactorized
distribution of the form
q(x) = q(x )q(x )···q(x ) (5.2)
0 1 N
by variational methods. In the case where x is not observed (panel (a) of
N
Fig. 2), we update all of the factors one at time so that, on convergence,
22
q(x ) is an approximation to the marginal distribution p(x ). In the case
N N
wherex isobserved(panel(b)ofFig. 2)weimposetheadditionalconstraint
N
that the distribution q(x ) is concentrated on the observed value x¯ . So
N N
q(x ) is never updated and, on convergence, q(x) is an approximation to the
N
posterior distribution p(x|x = x¯ ).
N N
x x ··· x x
0 1 N−1 N
(a) x is not observed
N
x x ··· x x
0 1 N−1 N
(b) x is observed
N
Figure 2: A toy model for predictive processing in the visual cortex. Hatching
indicates variables whose values are not observed.
If we assume that for n = 0,...,N, x can take K possible values, then
n
we can represent x as a 1-hot K ×1 vector and write
n
p(x ) = x⊤p
0 0 0
q(x ) = x⊤q
n n n
p(x |x ) = x⊤ A x
n n−1 n−1 n n
where p and q are K ×1 vectors and A is a K ×K matrix. Thus
0 n n
N
(cid:88)
lnp(x) = x⊤lnp + x⊤ lnA x
0 0 n−1 n n
n=1
where the logarithms of the vectors and matrices on the right-hand side are
taken element wise.
Recall that by (3.6), the recipe for updating one of the factors q(x ) in
n
(5.2) holding the others fixed calls for evaluating q˜(x ) defined by
n
lnq˜(x ) = E [lnp(x)].
n q(x )
\n
23
This gives

x⊤(lnp +lnA q )+... (n = 0)
  0 0 1 1
(cid:0) (cid:1)
lnq˜(x ) = x⊤ lnA⊤q +lnA q +... (0 < n < N)
n n n n−1 n+1 n+1

x⊤ lnA⊤ q +... (n = N)
N N−1 N
where the ellipses indicate terms which are independent of x and hence can
n
be ignored.1 (We have used the fact that the scalar q⊤ lnA x can be
n−1 n n
rewritten as x⊤lnA⊤q .) So the update formula for q is
n n n−1 n

lnp +lnA q +... (n = 0)
  0 1 1
lnq = lnA⊤q +lnA q +... (0 < n < N)
n n n−1 n+1 n+1

lnA⊤ q +... (n = N)
N−1 N
where the ellipses indicate constants whose role is to ensure that the compo-
nents of q sum to 1. In other words, for each n = 0,...,N, the vector q is
n n
derived from the corresponding vector on the right-hand side by passing it
through the softmax function. In the case were x is observed, the update
N
formula is not applied in the case n = N, and q is set to be the 1-hot vector
N
corresponding to the observed value of x .
N
To evaluate the divergence KL[q(x)∥p(x)], note that
N
(cid:88)
E [lnp(x)] = q⊤lnp + q⊤ lnA q
q(x) 0 0 n−1 n n
n=1
and
N
(cid:88)
E [lnq(x)] = q lnq
q(x) n n
n=0
so that
N N
(cid:88) (cid:88)
KL[q(x)∥p(x)] = q⊤lnq −q⊤lnp − q⊤ lnA q .
n n 0 0 n−1 n n
n=0 n=1
1We use the convention +... to indicate irrelevant additive terms throughout.
24
5.2 Static and Dynamic Bayesian Networks
A tree structure as in Fig. 3, rather than the linear structure in Fig. 2,
enables our toy predictive processing model to be extended to embrace mul-
tiple sensory modalities. Associated with each node n in the tree there is a
variable x , and a conditional probability distribution p(x |x ) is specified
n n′ n
for each branch n → n′. Think of the leaf nodes of the tree as the various
sense gates and of the variables associated with nodes at higher levels as
progressively more abstract representations of the sensorium.
Variational message passing in a tree works in much the same way as in
the linear graph Fig. 3: updating the variational posterior associated with
a given node depends on messages received from its parent and its children
(rather than from its successor and predecessor as in the case of Fig. 2) but
not from any other nodes.
Oddly enough, the term “predictive processing” in the neuroscience lit-
erature is primarily used to refer to prediction from top to bottom in a
hierarchical network such as Figs. 2 and 3 rather than prediction from past
to future which is evidently just as important. Fig. 4 illustrates a general
procedure for converting a given Bayesian Network to a Dynamic Bayesian
Network which enables temporal as well as top down prediction.
The construction consists in replacing a node in the original network rep-
resenting a random variable x by a chain of nodes representing instances
n
of x at successive times. This entails replacing the conditional distribution
n
p(x |x ) associated with a given branch in the original graph by a con-
n′ n
ditional distribution of the form p(x |x ,x ) for each time t. This
n′,t n,t n′,t−1
probability distribution is usually taken to be independent of t, although we
will encounter situations where time-dependent transition probabilities are
needed.
An important point to note is that this construction can be implemented
with slower clock speeds at higher levels of the hierarchy so that the various
levels can be thought of as representing the sensorium on different temporal
scales as well as on different spatial scales. Philosophy of mind has tradi-
tionally been stumped by the binding problem in sense perception but this
schema shows how the Bayesian Brain Hypothesis deals with it straightfor-
wardly.
25
Figure 3: A toy model for simultaneous predictive processing in multiple sen-
sory modalities. The leaf nodes of the tree correspond to sensors.
5.3 Hidden Markov Models
Henceforth, we will use the notation s to indicate a sequence of hidden vari-
ables or “states” s ,s ,...,s in a Dynamic Bayesian Network, o to indicate
0 1 T
a sequence of observable variables o ,...,o and o¯ ,...,o¯ to indicate a
1 T 1 T
sequence of observed values. For each time t = 1,...,T −1, s denotes the
>t
sequence s ,...,s and so forth.
t+1 T
A Hidden Markov Model (HMM) is a joint probability distribution p(s,o)
on state sequences s and observation sequences o of the form
T
(cid:89)
p(s,o) = p(s ) p(s |s )p(o |s ). (5.3)
0 t t−1 t t
t=1
So the sequence of states is a Markov chain and, at each time t, o is pre-
t
dicted from s alone rather than from the pair (s ,o ) (as would be the
t t t−1
case in more general Dynamic Bayesian Networks). The intuition here is
that sensory data (as distinct from, say, the text data which large language
models have to predict) is so noisy that useful predictions are only possible
in the space of hidden variables (or embeddings, to use the language of ar-
tificial intelligence [32, 33]). In a good probability model, the state space
will be of lower dimension than the observation space so that inferring states
26
x
n
(a) Static Network.
··· x x x ···
n,t−1 n,t n,t+1
(b) Dynamic Network.
Figure 4: Constructing a Dynamic Bayesian Network from a static network.
The static network supports prediction from top to bottom (as in figs. 2 and
3). The dynamic network supports prediction from past to future as well.
from observations can be thought of as a denoising operation which can be
expected to enhance the accuracy of perceptions and predictions.
6 The Perception/Action Cycle
Recall the basic tenets of the Bayesian Brain Hypothesis: A biological agent
infers the state of its limbs and viscera at a given time from its propriocep-
tive and interoceptive sensations and the state of its environment from its
exteroceptive sensations. This is known as perceptual inference. It consists
in calculating an approximate posterior distribution of the hidden variables
in the agent’s model of its world by minimizing variational free energy. The
agent navigates its world by calculating a predictive distribution on its fu-
ture sensations. This is known as active inference. If active inference is to
be cast as an optimization problem in the same way as perceptual inference
then an objective function analogous to variational free energy needs to be
27
defined. For an agent that models its world as a HMM, we propose that the
divergence of a predictive distribution from the HMM can serve as such an
objective function.
We have already seen in Sections 3.3 and 5.1 how calculating approximate
posterior and predictive distributions can be formulated as constrained di-
vergence minimization problems that can be solved by variational methods.
In this section we show how a suitably constrained mean field approximation
to the HMM that models the agent’s world can serve both as a posterior dis-
tribution for perceptual inference and as a predictive distribution for active
inference.
When it is used to model perception, the divergence criterion that we
optimizecoincideswithvariationalfreeenergy. Whenitusedtomodelaction,
itturnsouttobecloselyrelatedtobutdifferentfromtheexpectedfreeenergy
functionalsthathavepreviouslybeenusedinactiveinference. Wewilldiscuss
this relationship in Section 9 below.
6.1 Perception and Action as Divergence Minimiza-
tion
s 0 s 1 s 2 ··· s t s t+1 ··· s T
o o o o o
1 2 t t+1 T
Figure 5: Directed graphical model for the Perception/Action cycle. At time
t, o has been observed and o is yet to be observed.
≤t >t
The title of this section is borrowed from [22] where it is shown how a
wide range of reinforcement learning algorithms can be viewed as solving
constrained divergence minimization problems. Given a HMM p(s,o) and
an observation history o¯ ,...,o¯ , we show how perceptual and active infer-
1 t
ence can be cast as a problem of optimizing a joint divergence functional
KL[q(s,o)∥p(s,o)] where q(s,o) a suitably constrained variational approx-
imation to p(s,o). Our approach enables us to treat the perception/action
cycle in a unified way rather than modelling perception by optimizing a
28
variational free energy functional and action by optimizing an expected free
energy functional (as is usually done).
The graphical model for the calculation we will perform is shown in fig.
5. We assume that the planning horizon T is fixed in advance. At a given
time t = 0,...,T, the variables o ,...,o have been observed and we use the
1 t
observed values o¯ ,...,o¯ to calculate posterior distributions for the entire
1 t
state sequence s ,...,s up to time T. So, at each time t, we are retrodicting
0 T
the past as well as predicting the future. Retrodiction has to be supported
if an agent is to have the ability to update its beliefs about past events as
more information becomes available.
Following the standard convention in the active inference literature, we
use t to denote the present time and τ to denote any time (past, present, or
future) in the interval 0,...,T. For a given present time t, we assume that
q(s,o) factorizes as
T
(cid:89)
q(s,o) = q(s ) q(s ,o )
0 τ τ
τ=1
where q(s ) = p(s ) and
0 0
(cid:40)
q(s )δ (o ) (τ = 1,...,t)
q(s ,o ) =
τ o¯τ τ
(6.1)
τ τ
q(s )p(o |s ) (τ = t+1,...T).
τ τ τ
Another (arguably better) possibility which we will not explore here would
be to factorize q(s) as a product of conditional distributions so that
T
(cid:89)
q(s) = q(s ) q(s |s ).
0 τ τ−1
τ=1
This is the most commonly used factorization in Bayesian treatments of
HMMs [34]. In the context of active inference, this factorization is discussed
in [35, 13] where it is referred to as the Bethe approximation.
Note that because of the presence of the term p(o |s ) in (6.1) the fac-
τ τ
torization we are using is not a full mean field approximation to p(s,o). We
will explain the reason for this choice in Section 7 below.
Note also that, although our notation fails to reflect these dependencies,
the value of the divergence from q(s,o) to p(s,o) depends on the starting
distribution p(s ) (through (5.3)) as well as on the observations up to time t
0
29
(through (6.1)). We will treat the starting distribution as context-dependent
in the sense that it is determined by the agent’s activity prior to time 0 and
we will take the variational posterior q(s ) to be identical to p(s ). Equa-
0 0
tion (6.14) below explains the rationale for this choice. Our handling of the
starting distribution departs from the tradition in the active inference liter-
ature where it is specified by a vector denoted by D which is assumed to be
context-independent and the variational posterior at time 0 is updated in the
same way as the variational posteriors at other times.
Throughout this section, we ignore any structure that the state space of
the HMM may have and we assume that the number of states sufficiently
small that the emission probability distributions and transition probability
distributions that define the HMM can be precomputed and stored as ma-
trices which we denote by A and B respectively (following the conventions
in the active inference literature).
As in Section 5.1 we represent s and o by 1-hot vectors so that
τ τ
p(o |s ) = s⊤Ao
τ τ τ τ
p(s |s ) = s⊤ Bs
τ τ−1 τ−1 τ
p(s ) = s⊤p (6.2)
0 0 0
where p is the vector that represents the starting distribution p(s ). With
0 0
these conventions,
T
(cid:88)(cid:0) (cid:1)
lnp(s,o) = s⊤lnp + s⊤lnA o +s⊤ lnB s . (6.3)
0 0 τ τ τ−1 τ
τ=1
Likewise, we store each posterior distribution q(s ) as a vector q so that
τ τ
q(s ) = s⊤q . (6.4)
τ τ τ
Finally, we store the entropies of the emission probability distributions as a
vector h so that
H[p(o |s )] = s⊤h. (6.5)
τ τ τ
The components of h are the diagonal entries of the matrix −A(lnA)⊤.
We derive the formulas for updating each posterior q holding the others
τ
fixed in Sections A.1 and A.2 in the appendix. In the case where τ > t, the
30
update for q is given by (A.4):
τ
(cid:40)
−h+lnB⊤q +lnB q +... (τ = 1,...,T −1)
τ−1 τ+1
lnq = (6.6)
τ
−h+lnB⊤q +... (τ = T).
T−1
We refer to this as the prediction update formula. In the case where τ ≤ t,
the update for q is given by (A.8):
τ

lnp (τ = 0)
  0
lnq = lnA o¯ +lnB⊤q +lnB q ... (τ = 1,...,T −1) (6.7)
τ τ τ−1 τ+1

lnA o¯ +lnB⊤q +... (τ = T).
T T−1
We refer to this as the retrodiction update formula. But for the fact that we
have simplified the way the case τ = 0 is handled, the retrodiction updates
are the standard updates for minimizing the variational free energy of the
observations o¯ ,...,o¯ .
1 t
The only difference between the prediction and retrodiction updates is
that the vector lnAo¯ in the retrodiction updates (6.7) is replaced by−h in
τ
the prediction updates (6.6). The components of the vector lnAo¯ are just
τ
the state dependent log likelihoods lnp(o¯ |s ) and, by definition, the compo-
τ τ
nents of the vector−h are just the expected values of these log likelihoods.
So this is not surprising.
Broadly speaking, there are two ways of implementing these update for-
mulas at each present time t. Both implementations can be carried out in
real time, but they have different computational requirements. The sim-
plest, known as Bayesian filtering (by analogy with Kalman filtering), holds
q ,...,q fixed and updates the posteriors for the present and future times
0 t−1
q ,...,q asynchronously at time t. The alternative, known as Bayesian
t T
smoothing, updates all of the posteriors q ,...,q asynchronously at every
1 T
time present time t. This results in a lower value for the divergence criterion.
An obvious compromise between the two approaches would be to limit the
extent to which beliefs about the past are updated in Bayesian smoothing.
After calculating the posteriors at present time t, the divergence of q(s,o)
from p(s,o) can be evaluated using (A.10):
KL[q(s,o)∥p(s,o)] =
T t T
(cid:88) (cid:88) (cid:88)
q⊤lnq − q⊤lnA o¯ − q⊤ lnB q . (6.8)
τ τ τ τ τ−1 τ
τ=1 τ=1 τ=1
31
The cases t = 0 (the beginning of the cycle) and t = T (the end of the
cycle) are of particular interest. In the case t = 0, the variables o ,...,o
1 T
are yet to be observed, the observation history is empty and the posteriors
q ,...,q are updated using the prediction update formulas (6.6). By (6.1),
1 T
q(o|s) = p(o|s) so that the divergence KL[q(o|s)∥p(o|s)] is 0 and, by the
chain rule (3.3),
KL[q(s,o)∥p(s,o)] = KL[q(s)∥p(s)]. (6.9)
This divergence is given explicitly by
T T
(cid:88) (cid:88)
q⊤lnq − q⊤ lnB q . (6.10)
τ τ τ−1 τ
τ=1 τ=1
In the case t = T, all of the variables o ,...,o have been observed
1 T
and the posteriors q ,...,q are updated using the retrodiction update for-
1 T
mulas (6.7). By (6.1), q(s,o) = q(s)δ (o) and, by (3.7), the divergence
o¯
KL[q(s,o)∥p(s,o)] is just the variational free energy of the observations
o¯ ,...,o¯ calculated with the approximate posterior q(s). This divergence
1 T
is given explicitly by
T T T
(cid:88) (cid:88) (cid:88)
q⊤lnq − q⊤lnA o¯ − q⊤ lnB q . (6.11)
τ τ τ τ τ−1 τ
τ=1 τ=1 τ=1
More generally, for t = 0,...,T, we can split the right-hand side of (6.8)
into two terms, a contribution from the past an a contribution from the
future. That is,
KL[q(s,o)∥p(s,o)] = F +F (6.12)
≤t >t
where
t t t
(cid:88) (cid:88) (cid:88)
F = q⊤lnq − q⊤lnA o¯ − q⊤ lnB q
≤t τ τ τ τ τ−1 τ
τ=1 τ=1 τ=1
= KL[q(s ,o )∥p(s ,o )] (6.13)
≤t ≤t ≤t ≤t
which, by (3.7), is just the variational free energy of the observations up to
the present time t, and
T T
(cid:88) (cid:88)
F = q⊤lnq − q⊤ lnB q
>t τ τ τ−1 τ
τ=t+1 τ=t+1
= KL[q(s )∥p(s )] (6.14)
>t >t
32
where this divergence is evaluated using q(s ) as the starting distribution at
t
time t rather than p(s ).
0
6.2 Planning
Consider now an agent having the ability to perform several different types
of action, each modelled by a transition probability matrix. Suppose that
the agent is charged with a task which is characterized by the requirement
that the states of world at time T are sampled from a stationary preference
distribution p(s |C) such as the steady state distribution p(s |S) that we
τ τ
referred to in our discussion of homeostasis in Section 2.2. Suppose further
that the agent is given a list of the possible action sequences ending at time
T that are available to it. These action sequences are called policies in the
active inference literature.
Assume that at a given present time t = 0,...,T − 1, the agent has
performed actions a ,...,a and recorded observations o¯ ,...,o¯ . Which
1 t 1 t
action should the agent perform at time t+1?
One way to answer this question would be to start from the observation
that the calculations in Section 6.1 can be modified straightforwardly to
accommodate time-dependent transition probability matrices B for each
πτ
policy π and time τ. So for each policy π that is consistent with the actions
that have already been performed up to the present time t, we can use the
observations o¯ ,...,o¯ to calculate variational posteriors q(s |π) for τ =
1 t τ
1,...,T and use these posteriors to evaluate the divergence of q(s |π) from
>t
p(s |C). The agent would then plan to execute the policy which minimizes
>t
this divergence and this would determine the action that the agent performs
at time t+1, as required.
Another possibility, which is much less computationally expensive but
does not seem to have been noticed in the active inference literature, would
betocalculatedivergencesintheforwardratherthanthereversedirection(as
discussed in Section 3.1). Elsewhere in this paper we use reverse divergences
because in those situations the target distribution is intractable and hence
has to be approximated. In the case at hand, the target distribution, namely
p(s |C), is already factorized and the divergence KL[p(s |C)∥p(s |π)]
>t >t >t
can be evaluated straightforwardly without having to use the variational
approximationq(s |π)top(s |π). Indeed,ifp isthevectorrepresentation
>t >t C
33
of p(s |C) (which we are assuming to be independent of τ), then
τ
T T
(cid:88) (cid:88)
KL[p(s |C)∥p(s |π)] = p⊤lnp − p⊤lnB p
>t >t C C C πτ C
τ=t+1 τ=t+1
by (6.14).
6.3 Time-Domain Renormalization
The approach to planning that we have just outlined has the drawback that
the list of policies to be searched can be expected to increase exponentially
withtheplanninghorizon, T. Thisexplosioncouldbeavoidedbytreatingthe
process of generating a policy as a hidden stochastic process about which the
agent makes probabilistic inferences. The simplest possibility is to assume
that action sequences are generated by a Markov chain so that
T
(cid:89)
p(a ,...,a ) = p(a ) p(a |a ). (6.15)
0 T 0 τ τ−1
τ=1
More generally, we could assume as a generative model for action sequences
a second HMM whose states we will denote by σ so that
τ
T
(cid:89)
p(a) = p(σ ) p(σ |σ )p(a |σ ) (6.16)
0 τ τ−1 τ τ
τ=1
where a is the sequence of actions a ,...,a . Under a model of this sort, the
1 T
search space grows linearly rather than exponentially in T.
NotethatthetwoHMMsunderconsiderationcanbeintegratedintoasin-
gle HMM whose states are triples (σ ,a ,s ) and whose transition, emission
τ τ τ
and starting probability distributions are given by the equations
p(σ ,a ,s |σ ,a ,s ) = p(σ |σ )p(a |σ )p(s |s ,a )
τ τ τ τ−1 τ−1 τ−1 τ τ−1 τ τ τ τ−1 τ
p(o |σ ,a ,s ) = p(o |s )
τ τ τ τ τ τ
p(σ ,s ) = p(σ )p(s ). (6.17)
0 0 0 0
Iterating this construction enables a hierarchical structure to be built up.
This process is known as time domain renormalization because it can easily
accommodate slower clock speeds at higher levels of the hierarchy [30].
34
Although variational inference is traditionally thought of as a method of
inferringstateoccupanciesandtransitionsandactiveinferenceasamethodof
inferring actions that are currently underway or yet to be performed, folding
actions into states as in (6.17) does away with this distinction.
Of course the prediction and retrodiction updates (6.6) and (6.7) would
need to be modified if variational posteriors are to be calculated efficiently
with a structured HMM of this sort. We won’t go into details but just point
out that a natural mean field factorization to use for this purpose would be
to set
T
(cid:89)
q(σ,a,s,o) = q(σ )q(s ) q(σ )p(a |σ )q(s ,o |a )
0 0 τ τ τ τ τ τ
τ=1
where q(σ ) = p(σ ), q(s ) = p(s ) and q(s ,o |a ) factorizes in the same
0 0 0 0 τ τ τ
way as (6.1). Updating a full set of variational posteriors q(σ ,a ,s ,o )
τ τ τ τ
(where τ ranges from 1 to T) at the present time t gives, in particular, a
predictive distribution q(a ) on the action that will be performed at time
t+1
t+1. So the agent would decide what to do next by drawing a sample from
this distribution.
The reader may wonder what has happened to the stationary preference
distribution p(s |C) introduced in the last section. It is generally the case
τ
that a finite state Markov chain has a unique steady state distribution [23].
Applied to the integrated HMM (6.17), this implies that, regardless of the
starting distributions p(σ ) and p(s ), the triples (σ ,a ,s ) will eventu-
0 0 τ τ τ
ally be distributed according to a steady state distribution. The same will
therefore be true of the states s . The latter steady state distribution is
τ
determined by the HMM (6.16), rather than by an externally prescribed
preference distribution p(s |C). In order to ensure that the steady state and
τ
the preference distribution agree, the parameters of the HMM (6.16) would
have to be learned from an appropriate training dataset. (We will discuss
learning in Section 8.)
7 The Exploration/Exploitation Tradeoff
As shown in [22], many well known reinforcement learning algorithms can
be viewed as solving constrained divergence minimization problems. This
perspective enables the exploration/exploitation tradeoff in reinforcement
learning to be understood in a principled way. In this section, we show how
35
this analysis plays out in the case of HMMs.
The present time t is fixed throughout this section so we will use the
notation o rather than o to indicate future observations and similarly for
> >t
s . Using the factorizations
>
q(s ,o ) = q(o |s )q(s )
> > > > >
p(s ,o ) = p(o )p(s |o ),
> > > > >
we can write
lnq(s ,o )−lnp(s ,o ) =
> > > >
(lnq(o |s )−lnp(o ))+(lnq(s )−lnp(s |o )).
> > > > > >
So the divergence KL[q(s ,o )∥p(s ,o )] can be written in the form
> > > >
E [KL[q(o |s )∥p(o )]]+E [lnq(s )−lnp(s |o )]. (7.1)
q(s>) > > > q(s>,o>) > > >
Since the divergence of q(s |o ) from p(s |o ) is non-negative,
> > > >
E [lnq(s |o )] ≥ E [lnp(s |o )]
q(s>|o>) > > q(s>|o>) > >
which implies that
E [lnq(s )−lnp(s |o )] ≥ E [lnq(s )−lnq(s |o )]
q(s>,o>) > > > q(s>,o>) > > >
= −E [KL[q(o |s )∥q(o )]]
q(s>) > > >
= −KL[q(s ,o )∥q(s )q(o )]. (7.2)
> > > >
So combining (7.1) and (7.2), we obtain
KL[q(s ,o )∥p(s ,o )] ≥
> > > >
E [KL[q(o |s )∥p(o )]]−KL[q(s ,o )∥q(s )q(o )]. (7.3)
q(s>) > > > > > > >
The expression on the right hand side here is referred to as the free energy
of the expected future in [36, 37].
The divergence KL[q(s ,o )∥q(s )q(o )] is the mutual information be-
> > > >
tween s and o . This is a measure of the degree to which s and o fail
> > > >
to be statistically independent. In the active inference literature, the mu-
tual information is usually referred to as the expected information gain as it
36
can be interpreted as the reduction in uncertainty about s contingent on
>
observing o given by the expression
>
H[q(s )]−E [H[q(s |o )]]
> q(o>) > >
which is just another way of writing the mutual information between s
>
and o . The reason why we did not use a full mean field approximation in
>
(6.1) is to ensure that this reduction in uncertainty is non-zero. (If q(s ,o )
> >
factorized as q(s )q(o ), there would be no reduction in uncertainty.)
> >
Because the mutual information appears in (7.3) with a negative sign,
minimizing the divergence of q(s ,o ) from p(s ,o ) can be expected to
> > > >
increase the information gain, although it is not guaranteed to do so: firstly,
because this is an inequality not an equality, and secondly, because the first
term on the right-hand side of (7.3) depends on q(s ,o ) as well as the
> >
second. The first term can be viewed as a measure of how well the marginal
distribution p(o ) is approximated by q(s ,o ). So it is a pragmatic value or
> > >
extrinsic value in the terminology of active inference. It can be decreased by
acting on the world in a way that ensures that future observations have high
probability under the marginal distribution p(o ). But this sort of action
>
is not feasible if the agent is uncertain as to the current state of its world.
In such a situation, the right-hand side of (7.3) can only be minimized by
acting in such a way as to maximize the information gain. Thus (7.3) can
be viewed as formalizing the tradeoff between exploration and exploitation
that an intelligent agent needs to be able to make as it navigates its world.
Since o is yet to be observed, (6.1) implies that q(o |s ) = p(o |s )
> > > > >
and we can use this to rewrite (7.3) as follows. For the first term on the
right-hand side of (7.3) we have
E [KL[q(o |s )∥p(o )]]
q(s>) > > >
= E [KL[p(o |s )∥p(o )]]
q(s>) > > >
= E (cid:2)E [lnp(o |s )−lnp(o )] (cid:3)
q(s>) p(o>|s>) > > >
= E [−lnp(o )]−E [H[p(o |s )]].
q(o>) > q(s>) > >
In the active inference literature, the terms extrinsic or pragmatic value
are usually reserved for the cross entropy term on the right-hand side of
this equation and the expected entropy term is called the ambiguity. As
for the joint divergence on the left-hand side of (7.3), the assumption that
q(o |s ) = p(o |s ) implies (by the chain rule (3.3)) that
> > > >
KL[q(s ,o )∥p(s ,o )] = KL[q(s )∥p(s )].
> > > > > >
37
So (7.3) now takes the form
KL[q(s )∥p(s )] ≥
> >
E [−lnp(o )]−E [H[p(o |s )]]
q(o>) > q(s>) > >
−KL[q(s ,o )∥q(s )q(o )]. (7.4)
> > > >
and, since the mutual information between s and o can be written in the
> >
form
H[q(s )]−E [H[p(o |s )]],
> q(s>) > >
this simplifies to
KL[q(s )∥p(s )] ≥ E [−lnp(o )]−H[q(s )]. (7.5)
> > q(o>) > >
Transposing the ambiguity term on the right-hand side of (7.4) to the
left-hand side gives
KL[q(s )∥p(s )]+E [H[p(o |s )]] ≥
> > q(s>) > >
E [−lnp(o )]−KL[q(s ,o )∥q(s )q(o )]. (7.6)
q(o>) > > > > >
Intheactiveinferenceliterature,theexpressionontheright-handsideof(7.6)
is the standard way of defining expected free energy, although it is suggested
in [13] that the expression on the left-hand side might be used instead.
We will discuss expected free energy functionals in Section 9. For the
moment we note that inspection of (7.6) shows that optimizing the right-
hand side tends to decrease the cross-entropy term and increase the mutual
information. We say “tends to” because both of these terms depend on the
approximating distribution q(s ,o ) so that a decrease in the cross entropy
> >
may be bought at the cost of a decrease in the mutual information (or an
increase in the mutual information at the cost of an increase in the cross-
entropy). On the other hand optimizing the right-hand side of (7.5) tends to
increase the the entropy H[q(s )] instead of the mutual information. Opti-
>
mizing the left-hand side of (7.6) tends to decrease both the divergence term
and the ambiguity.
8 Learning
A fully Bayesian approach to the problem of estimating the transition and
emission probability distributions that define a HMM requires that these
38
probability distributions be treated as random variables. Thus a prior prob-
ability distribution is assigned to the HMM parameters before learning and
a posterior distribution after learning. This posterior distribution can be
updated on an ongoing basis (as more training data becomes available) and
point estimates of the HMM parameters can be calculated from the poste-
rior as needed. The Dirichlet distribution is the natural choice for a prior on
discrete probability distributions and on discrete HMMs.
8.1 Dirichlet Priors
TheDirichletdistributionisamultivariatedistributiononK-tuples(µ ,...,µ )
1 K
whose components are positive and sum to 1. It is defined by the probability
density
K
1 (cid:89)
µα
k
−1.
B(α) k
k=1
Here B(α) is the beta function defined by
Γ(α )...Γ(α )
1 K
B(α) =
Γ(α )
0
where α = α + ... + α . The α’s are referred to as concentration pa-
0 1 K
rameters or Dirichlet counts. If µ is interpreted as the probability that an
k
integer valued random variable takes the value k, α is usually thought of as
k
the number of times this event is observed to happen in a virtual training
set although the α’s are not required to be whole numbers. The Dirichlet
distribution has the property that
E[lnµ ] = ψ(α )−ψ(α ) (8.1)
k k 0
for k = 1,...,K. Here ψ is the digamma function (that is, the derivative of
lnΓ). If q(µ) and p(µ) are Dirichlet distributions defined by concentration
parameters α′ and α respectively then the divergence KL[q(µ)∥p(µ)] is
given by
K
(cid:88)
lnB(α)−lnB(α′)+ (α′ −α ))(ψ(α′)−ψ(α′)). (8.2)
k k k 0
k=1
Recall that a HMM M is specified by emission and transition probability
distributions stored as rows of matrices which we denoted by A and B. We
39
candefineaprioronMbyassigningDirichletpriorstoeachoftherowsofthe
these matrices. It is convenient to write this prior in matrix form as follows.
If C is the matrix whose entries are the concentration parameters for the
A
emission probabilities then we can write the prior on A in the logarithmic
domain as
(cid:16) (cid:17)
lnp(A) = tr lnA(C −1)⊤ +....
A
where 1 is the matrix all of whose entries are equal to 1. Similarly,
(cid:16) (cid:17)
lnp(B) = tr lnB(C −1)⊤ +....
B
where C is the matrix whose entries are the concentration parameters for
B
the transition probabilities. So the prior on M has the form
(cid:16) (cid:17) (cid:16) (cid:17)
lnp(M) = tr lnA(C −1)⊤ +tr lnB(C −1)⊤ +.... (8.3)
A B
8.2 Learning as Divergence Minimization
Suppose we are given a Dirichlet prior p(M) as in (8.3) and a training set
consisting of an observation sequence o¯ of length T. We can calculate a
variational posterior on M — which turns out to be a Dirichlet distribution
defined by another set of concentration parameters — as follows.
We define a joint prior on triples (M,s,o) by setting
p(M,s,o) = p(M)p(s,o|M).
and seek a variational approximation q(M,s,o) to p(M,s,o) of the form
q(M,s,o) = q(M)q(s)δ (o)
o¯
where
T
(cid:89)
q(s) = q(s ).
τ
τ=0
AsinSection6,werepresenteachdistributionq(s )byavectorq . Following
τ τ
theprescription(3.6), wealternatebetweenupdatingq(M)holdingq(s)fixed
and updating q(s) holding q(M) fixed.
Thus we update q(M) by setting q(M) ∝ q˜(M) where
lnq˜(M) = E [lnp(M,s,o)]
q(s,o)
= E [lnp(M,s,o¯)]
q(s)
= lnp(M)+E [lnp(s,o¯|M)]. (8.4)
q(s)
40
Recall that by (6.3),
T
(cid:88)(cid:0) (cid:1)
lnp(s,o¯|M) = s⊤lnp + s⊤lnA o¯ +s⊤ lnB s . (8.5)
0 0 τ τ τ−1 τ
τ=1
so the second term on the right hand side of (8.4) is equal to
T T
(cid:88) (cid:88)
q⊤lnA o¯ + q⊤ lnB q
τ τ τ−1 τ
τ=1 τ=1
(cid:32) (cid:33) (cid:32) (cid:33)
T T
(cid:88) (cid:88)
=tr lnA o¯ q⊤ +tr lnB q q⊤ .
τ τ τ τ−1
τ=1 τ=1
Combining this with (8.3) and collecting terms, (8.4) becomes
(cid:0) (cid:1) (cid:0) (cid:1)
lnq˜(M) = tr lnA (C′ −1)⊤ +tr lnB (C′ −1)⊤ +...
A B
where
T
(cid:88)
C′ = C + q o¯⊤
A A τ τ
τ=1
T
(cid:88)
C′ = C + q q⊤. (8.6)
B B τ−1 τ
τ=1
So, like p(M), q(M) is of the form (8.3). Thus it too is a Dirichlet distribu-
tion.
To update q(s) holding q(M) fixed, we set q(s ) = p(s ) and for each
0 0
τ = 1,...,T, we update q(s ) by setting q(s ) ∝ q˜(s ) where q˜(s ) is defined
τ τ τ τ
by
lnq˜(s ) = E (cid:2)E [lnp(M,s,o)] (cid:3)
τ q(s ) q(M,o)
\τ
= E (cid:2)E [lnp(M,s,o¯)] (cid:3)
q(s ) q(M)
\τ
= E (cid:2)E [lnp(s,o¯|M)] (cid:3) +... (8.7)
q(s ) q(M)
\τ
Recall that by (6.3),
T
(cid:88)(cid:0) (cid:1)
lnp(s,o¯|M) = s⊤lnp + s⊤lnA o¯ +s⊤ lnB s . (8.8)
0 0 τ τ τ−1 τ
τ=1
41
So
T
E [lnp(s,o¯|M)] = s⊤lnp + (cid:88)(cid:0) s⊤⟨lnA⟩o¯ +s⊤ ⟨lnB⟩s (cid:1) (8.9)
q(M) 0 0 τ τ τ−1 τ
τ=1
where the notation ⟨lnA⟩ indicates the expectation of lnA taken with re-
spect to q(M) and similarly for ⟨lnB⟩. These expectations can be calculated
using (8.1). Hence the update formula for q is formally the same as the
τ
retrodiction update formula (6.7):

lnp (τ = 0)
  0
lnq = ⟨lnA⟩ o¯ +⟨lnB⟩⊤q +⟨lnB⟩ q +... (τ = 1,...,T −1)
τ τ τ−1 τ+1
 ⟨lnA⟩ o¯ +⟨lnB⟩⊤q +... (τ = T).
T T−1
It remains to evaluate the joint divergence KL[q(M,s,o)∥p(M,s,o)].
By the chain rule (3.3), we can write this as
KL[q(M)∥p(M)]+E [KL[q(s,o)∥p(s,o|M)]].
q(M)
The first term here is given by (8.2). For the second, since o is observed to
take the value o¯, the divergence KL[q(s,o|M)∥p(s,o|M)] is given by the
variational free energy formula (6.11):
KL[q(s,o|M)∥p(s,o|M)]
T T T
(cid:88) (cid:88) (cid:88)
= q⊤lnq − q⊤lnA o¯ − q⊤ lnB q
τ τ τ τ τ−1 τ
τ=1 τ=1 τ=1
so that
E [KL[q(s,o)∥p(s,o|M)]]
q(M)
T T T
(cid:88) (cid:88) (cid:88)
= q⊤lnq − q⊤⟨lnA⟩o¯ − q⊤ ⟨lnB⟩q .
τ τ τ τ τ−1 τ
τ=1 τ=1 τ=1
Extending these derivations to accommodate the structured state spaces
discussedinSection6.3isstraightforward. Considerthematrix (cid:80)T q q⊤
τ=1 τ−1 τ
appearing the second line of (8.6). For each transition from one state to
another, there is a corresponding entry in this matrix whose value is the
expected number of times the transition occurs in the training data. If states
are decomposed as in (6.17) or into more complex hierarchies, then all that
is required is to accumulate matrices of expected transition counts for each
level in the hierarchy.
42
9 Whither Expected Free Energy?
Recall that, given observations that have already been made, variational free
energy is an approximation to the surprisal of these observations. Evaluating
it requires a generative model p(s,o) (such as a HMM), the observations o
themselves, and an approximation to the posterior distribution p(s|o).
If we can calculate an approximate posterior q(s |o ) for observations o
> > >
that have yet to be made, and in addition we have an approximate marginal
distribution q(o ) on these observations, then we can calculate an expected
>
value for the variational free energy of future observations. This expected
value is traditionally denoted by G. How should G be evaluated?
9.1 Expected Free Energy Functionals
Note that, taken together, q(o ) and q(s |o ) define a joint distribution
> > >
q(s ,o ). We assume that this joint distribution satisfies the condition that
> >
q(o |s ) = p(o |s ) in order to ensure that the mutual information between
> > > >
s and o is non zero (as explained in Section 7).
> >
Although this assumption seems to be needlessly restrictive (as seems to
have been first pointed out in [36]), the approximation
q(s |o ) ≈ q(s ) (9.1)
> > >
enables G to be evaluated easily. The variational free energy for o is
>
E [−lnp(s ,o )]−H[q(s )]
q(s>) > > >
so, taking the expectation with respect to q(o ),
>
G = E [−lnp(s ,o )]−H[q(s )]
q(o>)q(s>) > > >
= −E [lnp(s )]−H[q(s )]+E [−lnp(o |s )]
q(s>) > > q(s>)q(o>) > >
= KL[q(s )∥p(s )]+E [H[p(o |s )]] (9.2)
> > q(s>) > >
which is just the left-hand side of (7.6). However, it is the expression on the
right-hand side of (7.6) that is taken as standard definition of expected free
energy in the active inference literature. This can be derived by appealing
to an additional approximation, namely
p(s ,o ) ≈ q(s |o )p(o ). (9.3)
> > > > >
43
Using both of these approximations, we have
G = E [−lnp(s ,o )]−E [H[q(s |o )]]
q(s>,o>) > > q(o>) > >
≈ E [−lnq(s |o )−lnp(o )]−H[q(s )]
q(s>,o>) > > > >
= E [−lnq(s ,o )+q(s )+q(o )]+E [−lnp(o )]
q(s>,o>) > > > > q(o>) >
= −KL[q(s ,o )∥q(s )q(o )]+E [−lnp(o )] (9.4)
> > > > q(o>) >
which is just the right-hand side of (7.6), as required.
What happens if we proceed without making either of these approxima-
tions? The variational free energy for o is
>
E [−lnp(s ,o )]−H[q(s |o )]
q(s>|o>) > > > >
so, taking the expectation with respect to q(o ), we obtain the following
>
expression for the expected free energy:
G = E [−lnp(s ,o )]−E [H[q(s |o )]]. (9.5)
q(s>,o>) > > q(o>) > >
Since
H[q(s ,o )] = H[q(o )]+E [H[q(s |o )]],
> > > q(o>) > >
we can rewrite this as
G = E [−lnp(s ,o )]−H[q(s ,o )]+H[q(o )]
q(s>,o>) > > > > >
= KL[q(s ,o )∥p(s ,o )]+H[q(o )]
> > > > >
= KL[q(s )∥p(s )]+H[q(o )] (9.6)
> > >
since we are assuming that q(o |s ) = p(o |s ). So if the expected free
> > > >
energy G is defined in this way, the marginal divergence KL[q(s )∥p(s )]
> >
and G are very closely related:
KL[q(s )∥p(s )] = G−H[q(o )]. (9.7)
> > >
Comparing this with the general relationship (3.1)
KL[q(x)∥p(x)] = E [−lnp(x)]−H[q(x)],
q(x)
which we discussed in Section 3.1, we see that the entropy term H[q(o )] in
>
(9.7)canbeinterpretedasaregularizerwhichcanservetopreventoverfitting
when the divergence KL[q(s )∥p(s )] is used as a criterion for approximat-
> >
ing p(s ) by q(s ).
> >
44
9.2 Beliefs about Policies
Expected free energy is usually thought of as the objective function which an
agent needs to optimize in order to make probabilistic inferences about ac-
tionswhicharecurrentlyunderwayoryettobeperformed, ormoregenerally,
to update beliefs about policies.
Suppose that, at present time t, an agent has a prior distribution p(π)
over policies where each policy is a sequence of actions a ,...,a and that
1 T
observations o¯ have been recorded. According to equation (B.9) of [13],
≤t
the agent’s posterior belief about policies is given by
q(π) = σ(lnp(π)−F(π)−G(π))
whereσ isthesoftmaxfunctionand,foreachpolicyπ,F(π)isthevariational
free energy associated with the observation history, and G(π) is the expected
free energy associated with future observations.
To see how such an equation might be derived, let us define a joint dis-
tribution on triples (π,s,o) by setting
p(π,s,o) = p(π)p(s,o|π), (9.8)
We can obtain an approximate posterior distribution q(π) on π by assuming
a variational approximation to this joint distribution of the form q(π)q(s,o)
where q(s,o) factorizes as in (6.1). If q(s,o) is held fixed then, by (3.6), the
update formula for q(π) is q(π) ∝ q˜(π) where
lnq˜(π) = lnp(π)+E [lnp(s,o|π)]
q(s,o)
= lnp(π)−KL[q(s,o)∥p(s,o|π)]+...
since by (3.1),
KL[q(s,o)∥p(s,o|π)] = E [−lnp(s,o|π)]−H[q(s,o)]
q(s,o)
and the entropy term H[q(s,o)] is independent of π. As in (6.12), we can
write
KL[q(s,o)∥p(s,o|π)] = F (π)+F (π)
≤ >
where
F (π) = KL[q(s ,o )∥p(s ,o |π)]
≤ ≤ ≤ ≤ ≤
45
and
F (π) = KL[q(s )∥p(s |π)] (9.9)
> > >
where this divergence is evaluated using q(s ) as the starting distribution at
t
the present time t. So the update formula for q(π) is
q(π) = σ(lnp(π)−F (π)−F (π)) (9.10)
≤ >
which yields equation (B.9) of [13] if G(π) is taken to be F (π).
>
Alternatively, we could take
G(π) = E [−lnp(s ,o |π)]−E [H[q(s |o )]] (9.11)
q(s>,o>) > > q(o>) > >
since, as in the derivation of (9.7) from (9.5),
F (π) = G(π)−H[q(o )]. (9.12)
> >
The fact that the entropy term on the right-hand side here is independent
of π implies that the values returned by the softmax function would be
unaffected by substituting the right-hand side of (9.11) for F (π) in (9.10).
>
The expression for G(π) in (9.11) has the same form as the free energy
functional in (9.5) which we derived without appealing to either of the ap-
proximations (9.1) and (9.3). Note however that the argument we have just
given depends on calculating F (π) and F (π) with the distribution q(s,o)
≤ >
postulated by the variational approximation to the joint distribution defined
by (9.8). This is not the same as the policy dependent mean field approxi-
mation to p(s,o|π) that would be obtained by applying the prediction and
retrodiction update formulas (6.6) and (6.7) to each policy individually. So
this way of defining variational and expected free energy for policies depends
on how the distribution q(s,o) is estimated. The correct estimation proce-
dure is to update q(s,o) and q(π) alternately in the usual way.
We have already derived the update formula for q(π) holding q(s,o) fixed
so it only remains to derive the update formula for q(s,o) holding q(π) fixed.
By (3.6), q(s,o) ∝ q˜(s,o) where
lnq˜(s,o) = E [lnp(π,s,o))].
q(π)
By (6.3),
T
(cid:88)(cid:0) (cid:1)
lnp(s,o|π) = s⊤lnp + s⊤lnA o +s⊤ lnB s .
0 0 τ τ τ−1 πτ τ
τ=1
46
So
T
E [lnp(s,o|π)] = s⊤lnp + (cid:88)(cid:0) s⊤lnA o +s⊤ lnB s (cid:1)
q(π) 0 0 τ τ τ−1 τ τ
τ=1
where B is defined by
τ
(cid:88)
lnB = q(π)lnB
τ πτ
π
So if the matrices B are defined in this way, the update formulas for q(s)
τ
have the same form as the prediction and retrodiction update formulas (6.6)
and (6.7).
9.3 Discussion
Although it is not strictly speaking an expected free energy, the expression
on the right-hand side of (7.6) is generally taken as the definition of expected
free energy in the active inference literature thanks to its interpretation in
terms of pragmatic value and expected information gain. We have seen how
this is usually justified by appealing to the approximation (9.3). (This ap-
proximation is stated explicitly in [10, 8]. Other survey articles and tutorials
take the right-hand side of (7.6) as the definition of expected free energy
without attempting to justify it [7, 9, 11].)
Inspecting the derivation (9.4) shows that it also invokes the approxima-
tion (9.1). If it is assumed that q(s |o ) = q(s ) and that p(s ,o ) =
> > > > >
q(s |o )p(o ), then s and o are statistically independent under both
> > > > >
the the target distribution p(s ,o ) and the approximating distribution
> >
q(s ,o ) so that the mutual information between s and o is zero. This is
> > > >
clearly problematic.
If neither approximation is used then, as we saw in (9.7), the expres-
sion obtained for the expected free energy only differs from the divergence
KL[q(s )∥p(s )] by an entropy regularizer, H[q(o )]. Whether this sort
> > >
of regularization is practically useful can only be determined by running
experiments but is easy to imagine that a guardrail against making over
confident predictions about future events could prove useful for an agent en-
gaged in planning over extended time scales. As we discussed in Section 3.2,
the variational updates (3.6) are designed to optimize divergences (such as
KL[q(s )∥p(s )]) rather than functionals of the form (9.6). So if an ex-
> >
pected free energy functional such as G (or, for that mattter, any criterion
47
other than a reverse KL divergence) were to be used as the objective function
for active inference, another algorithm would need to be developed in order
to optimize it.
Expected free energy is usually thought of as furnishing an objective
function for variational inference about actions and policies as distinct from
states and transitions as in (9.10). However the distinction between states
and actions is not absolute as it hinges on the way states are defined. (For
example, thisdistinctionismootedbyfoldingactionsintostatesasinSection
6.3.) Although we had no need to appeal to an equation like (B.9) of [13]
in the body of this paper, we showed how such an equation can be derived
by defining variational and expected free energies for policies in terms of a
mean field approximation to the joint distribution defined by (9.8). This
entails defining the expected free energy associated with a policy by (9.9) or
by (9.11) rather than by the right-hand side of (7.6) as is usually done in the
active inference literature.
10 Conclusion
This work was motivated by a desire to find an objective function for ac-
tive inference and develop a variational algorithm to optimize it which has
the property that the objective function is guaranteed to decrease on every
belief update. As we saw in Section 3.2, in order to satisfy this condition
the objective function must be expressible as a Kullback-Leibler divergence.
Variational free energy has this property by (3.7) but expected free energy,
regardless of how it is defined, does not.
We showed that if an agent models its world by a Hidden Markov Model
p(s,o), it can solve the problems of perceptual and active inference in a
unified way by using variational inference to optimize a divergence of the
form KL[q(s,o)∥p(s,o)] where q(s,o) is a suitably factorized approximation
to p(s,o). Assuming that at a given time present time t the variables o
≤t
have been observed to take the values o¯ , the constraint that we imposed is
≤t
(cid:40)
q(s )δ (o ) (τ ≤ t)
q(s ,o ) =
τ o¯τ τ
τ τ
q(s )p(o |s ) (τ > t)
τ τ τ
as in (6.1). The update formulas needed to optimize q(s,o) subject to this
constraint are given in the appendix. These formulas are guaranteed to
48
decrease the divergence (given explicitly by (A.10)) on each iteration and
their derivation makes no appeal to the Free Energy Principle.
We teased out the relationship between this divergence criterion and the
notion of expected free energy in Section 9. Equation (6.12) shows that the
divergence from q(s,o) to p(s,o) can be decomposed as
KL[q(s ,o )∥p(s ,o )]+KL[q(s )∥p(s )].
≤t ≤t ≤t ≤t >t >t
where the divergence in the second term is calculated with the starting dis-
tribution q(s ) at the present time t. By (3.7), the first term here is the
t
variational free energy of the observations up to time t calculated with the
variational posterior q(s ). The second term can be written in the form
≤t
KL[q(s )∥p(s )] = G−H[q(o )]
>t >t >t
where G is the expected free energy defined by (9.5)
G = E [−lnp(s ,o )]−E [H[q(s |o )]].
q(s>t,o>t) >t >t q(o>t) >t >t
This was derived without appealing to either of the approximations (9.1) or
(9.3). So our approach is faithful to the spirit of the Free Energy Principle
even as it dispenses with the need to appeal to it. The entropy regular-
izer H[q(o )] which differentiates the divergence KL[q(s )∥p(s )] from
>t >t >t
the expected free energy G can be thought of as penalizing over confident
predictions of future events or as imposing a soft constraint on the degree to
which an agent can look ahead in planning its future.
In addition to providing a solution to the problem of calculating approxi-
matepredictive distributions givena startingdistribution andan observation
history, we have seen how standard mean field methods can be brought to
bear on the problems of learning HMM parameters (Section 8) and updating
beliefs about policies (Section 9.2). All of these applications follow the same
pattern: in each case, the variational update formulas are determined by the
joint distribution of the random variables of interest and the way the approx-
imating distribution is required to factorize. The practitioner who cleaves to
orthodox mean field methods stands to benefit from the monotone conver-
gence guarantee discussed in Section 3.2 but he does not have the latitude
claimed by the Free Energy Principle to choose variational update formulas
at will.
49
References
[1] P. Dayan, G. E. Hinton, R. M. Neal, and R. S. Zemel, “The Helmholtz
Machine,” in Advances in Neural Information Processing Systems 7
(NIPS 1994), pp. 3–9, MIT Press, 1995.
[2] R. Salakhutdinov and G. E. Hinton, “Deep Boltzmann Machines,” in
Proceedings of the 12th International Conference on Neural Information
Processing Systems (NeurIPS 2009), pp. 448–455, Curran Associates,
Inc., 2009.
[3] K. J. Friston, “Learning and Inference in the Brain,” Neural Networks,
vol. 16, no. 9, pp. 1325–1352, 2003.
[4] K. J. Friston, “A Theory of Cortical Responses,” Philosophical Trans-
actions of the Royal Society B: Biological Sciences, vol. 360, no. 1456,
pp. 815–836, 2005.
[5] K. J. Friston, “The Free-Energy Principle: A Unified Brain Theory?,”
Nature Reviews Neuroscience, vol. 11, no. 2, pp. 127–138, 2010.
[6] D. M. Blei, A. Kucukelbir, and J. D. McAuliffe, “Variational Inference:
A Review for Statisticians,” Journal of the American Statistical Associ-
ation, vol. 112, no. 518, pp. 859–877, 2017.
[7] J. v. Oostrum, C. Langer, and N. Ay, “A Concise Mathematical De-
scriptionofActiveInferenceinDiscreteTime,” Journal of Mathematical
Psychology, vol. 125, May 2025. arXiv:2406.07726 [cs].
[8] R. Smith, K. J. Friston, and C. J. Whyte, “A Step-by-Step Tutorial
on Active Inference and its Application to Empirical Data,” Journal of
Mathematical Psychology, vol. 107, pp. 1–60, 2022.
[9] L. D. Costa, T. Parr, N. Sajid, S. Veseli´c, V. Neac¸su, and K. J. Friston,
“Active Inference on Discrete State-Spaces: A Synthesis,” Journal of
Mathematical Psychology, vol. 99, 2020.
[10] N. Sajid, P. J. Ball, T. Parr, and K. J. Friston, “Active Inference: De-
mystified and Compared,” Neural Computation, vol. 33, no. 3, pp. 674–
712, 2021.
50
[11] Z. Zhang and F. Xu, “An Overview of the Free Energy Principle and
Related Research,” Neural Computation, vol. 36, no. 5, pp. 963–1021,
2024.
[12] K. Friston, L. D. Costa, N. Sajid, C. Heins, K. Ueltzho¨ffer, G. A. Pavli-
otis, and T. Parr, “The Free Energy Principle Made Simpler but not
Too Simple,” Physics Reports, vol. 1024, pp. 1–29, 2023.
[13] T. Parr, G. Pezzulo, and K. J. Friston, Active Inference: The Free En-
ergy Principle in Mind, Brain, and Behavior. MIT Press, 2022.
[14] A. Clark, Surfing Uncertainty: Prediction, Action, and the Embodied
Mind. Oxford / New York: Oxford University Press, 2015.
[15] A. K. Seth, Being You: A New Science of Consciousness. New York:
Dutton (Penguin Random House), 2021.
[16] M. Solms, The Hidden Spring: A Journey to the Source of Conscious-
ness. New York: W. W. Norton & Company, 2021.
[17] J. Locke, An Essay Concerning Human Understanding. New York: Ox-
ford University Press, 1689/2008.
[18] J. A. Hobson and K. J. Friston, “Consciousness, Dreams, and Inference:
The Cartesian Theatre Revisited,” Journal of Consciousness Studies,
vol. 21, no. 1-2, pp. 6–32, 2014.
[19] K. Friston, “Life as we know it,” Journal of the Royal Society Interface,
vol. 10, no. 86, 2013.
[20] R. A. Adams, S. Shipp, and K. J. Friston, “Predictions not Commands:
Active Inference in the Motor System,” Brain Structure and Function,
vol. 218, pp. 611 – 643, 2012.
[21] K. Friston, “The Mathematics of Mind-Time.” https://aeon.co/
essays/consciousness-is-not-a-thing-but-a-process-of-inference,
May 2017.
[22] D. Hafner, P. A. Ortega, J. Ba, T. Parr, K. Friston, and N. Heess,
“Action and Perception as Divergence Minimization,” Feb. 2022.
arXiv:2009.01791 [cs].
51
[23] K. P. Murphy, Probabilistic Machine Learning: Advanced Topics. Adap-
tive Computation and Machine Learning Series, Cambridge, Mas-
sachusetts: The MIT Press, 2023.
[24] E. Jaynes, “Information Theory and Statistical Mechanics,” Physical
Review, vol. 106, pp. 620–630, 1957.
[25] E. Jaynes, “Information Theory and Statistical Mechanics II,” Physical
Review, vol. 108, pp. 171–190, 1957.
[26] M. J. D. Ramstead, M. Albarracin, A. Kiefer, B. Klein, C. Fields,
K. Friston, and A. Safron, “The Inner Screen Model of Consciousness:
Applying the Free Energy Principle Directly to the Study of Conscious
Experience,” Jan. 2024. arXiv:2305.02205 [q-bio].
[27] H. Feldman and K. J. Friston, “Attention, Uncertainty, and Free-
Energy,” Frontiers in Human Neuroscience, vol. 4, p. 215, 2010.
[28] L. Sandved-Smith, C. Hesp, J. Mattout, K. Friston, A. Lutz, and
M. J. D. Ramstead, “Towards a Computational Phenomenology of
Mental Action: Modelling Meta-Awareness and Attentional Control
with Deep Parametric Active Inference,” Neuroscience of Conscious-
ness, vol. 2021, August 2021.
[29] K. J. Friston, L. D. Costa, A. Tschantz, A. Kiefer, T. Salvatori,
V. Neacsu, M. Koudahl, C. Heins, N. Sajid, D. Markovic, T. Parr,
T. Verbelen, and C. L. Buckley, “Supervised Structure Learning,” Nov.
2023. arXiv:2311.10300 [cs].
[30] K. Friston, C. Heins, T. Verbelen, L. D. Costa, T. Salvatori,
D. Markovic, A. Tschantz, M. Koudahl, C. Buckley, and T. Parr,
“From Pixels to Planning: Scale-Free Active Inference,” July 2024.
arXiv:2407.20292 [cs].
[31] B. Millidge, A. K. Seth, and C. L. Buckley, “Predictive Coding: A
Theoretical and Experimental Review,” 2021. arXiv:2107.12979 [cs].
[32] A.DawidandY.LeCun,“IntroductiontoLatentVariableEnergy-Based
Models: A Path Toward Autonomous Machine Intelligence,” Journal of
Statistical Mechanics: Theory and Experiment, vol. 2024, Oct. 2024.
52
[33] D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap, “Mastering Diverse
Domains through World Models,” Apr. 2024. arXiv:2301.04104 [cs].
[34] M. J. Beal, Variational Algorithms for Approximate Bayesian Inference.
PhD thesis, Gatsby Computational Neuroscience Unit, University Col-
lege London, 2003. Ph.D. Thesis.
[35] T. Parr, D. Markovi´c, and K. J. Friston, “Neuronal Message Passing
using Mean-field, Bethe, and Marginal Approximations,” Scientific Re-
ports, vol. 9, no. 1, pp. 1–18, 2019.
[36] B. Millidge, A. Tschantz, and C. L. Buckley, “Whence the Expected
Free Energy?,” Neural Computation, vol. 33, no. 2, pp. 447–482, 2021.
[37] A.Tschantz, B.Millidge, A.K.Seth, andC.L.Buckley, “Reinforcement
Learning through Active Inference,” 2020. arXiv:2002.12636.
A Posterior Update Formulas
A.1 Predicting the Future
Fix a present time t and a time τ in the range t+1,...,T. We derive the
formula for updating q(s ) holding fixed the other posteriors q(s ) (where τ′
τ τ′
takes values in the range 1,...,T other than τ).
By (3.6), to update q(s ) we need to evaluate q˜(s ) defined by
τ τ
lnq˜(s ) = E [lnp(s,o)].
τ q(s ,o)
\τ
Let us rewrite this as
lnq˜(s ) = E [lnp(s)]+E [lnp(o|s)]. (A.1)
τ q(s ) q(s ,o)
\τ \τ
To evaluate the first term on the right-hand side of (A.1), note that by (6.4),
(cid:40)
s if τ′ = τ
E [s ] = τ
q(s ) τ′
\τ q otherwise
τ′
and

s⊤lnB q if τ′ = τ −1
  τ τ+1
E [s lnB s ] = q⊤ lnB s if τ′ = τ
q(s \τ ) τ′−1 τ′ τ−1 τ

q⊤ lnB q otherwise
τ−1 τ
53
so that
(cid:34) (cid:35)
T
(cid:88)
E [lnp(s)] = E s⊤lnp + s⊤ lnB s
q(s \τ ) q(s \τ ) 0 0 τ′−1 τ′
τ′=1
(cid:40) (cid:0) (cid:1)
s⊤ lnB⊤q +lnB q +... (τ = 1,...,T −1)
= τ τ−1 τ+1
s⊤lnB⊤q +... (τ = T).
T T−1
(A.2)
To evaluate the second term on the right-hand side of (A.1), note that since
we are assuming that τ > t, (6.1) and (6.5) imply that
(cid:40)
−s⊤h if τ′ = τ
E [lnp(o |s )] = τ
q(s \τ ,o) τ′ τ′ −q⊤h if τ′ ̸= τ
τ′
so that
T
(cid:88)
E [lnp(o|s)] = E [lnp(o |s )]
q(s ,o) q(s ,o) τ′ τ′
\τ \τ
τ′=1
= −s⊤h+.... (A.3)
τ
Combining (A.2) and (A.3),
(cid:40) (cid:0) (cid:1)
s⊤ −h+lnB⊤q +lnB q +... (τ = 1,...,T −1)
lnq˜(s ) = τ τ−1 τ+1
τ (cid:0) (cid:1)
s⊤ −h+lnB⊤q +... (τ = T)
T T−1
so that, by (3.6), the update formula for q is
τ
(cid:40)
−h+lnB⊤q +lnB q +... (τ = 1,...,T −1)
τ−1 τ+1
lnq = (A.4)
τ
−h+lnB⊤q +... (τ = T).
T−1
if τ > t.
A.2 Retrodicting the Past
Now fix a present time t and a time τ in the range 1,...,t. We derive the
formula for updating q(s ) holding fixed the other posteriors q(s ) (where τ′
τ τ′
takes any value in the range 1,...,T other than τ).
54
As in (A.1), we need to evaluate q˜(s ) where
τ
lnq˜(s ) = E [lnp(s)]+E [lnp(o|s)] (A.5)
τ q(s ) q(s ,o)
\τ \τ
The first term on the right-hand side of (A.5) is handled in the same way as
(A.2) so that
(cid:34) (cid:35)
T
(cid:88)
E [lnp(s)] = E s⊤lnp + s⊤ lnB s
q(s \τ ) q(s \τ ) 0 0 τ′−1 τ′
τ′=1
(cid:40) (cid:0) (cid:1)
s⊤ lnB⊤q +lnB q +... (τ = 1,...,T −1)
= τ τ−1 τ+1
s⊤lnB⊤q +... (τ = T).
T T−1
(A.6)
To evaluate the second term on the right-hand side of (A.5), note that since
we are assuming that τ ≤ t, (6.1) implies that
(cid:40)
s⊤lnAo¯ (τ′ = τ)
E [lnp(o |s )] = τ τ
q(s \τ ,o) τ′ τ′ q⊤ lnAo¯ (τ′ ̸= τ)
τ′ τ′
so that
T
(cid:88)
E [lnp(o|s)] = E [lnp(o |s )]
q(s ,o) q(s ,o) τ′ τ′
\τ \τ
τ′=1
= s⊤lnA o¯ +.... (A.7)
τ τ
Combining (A.6) and (A.7),
(cid:40) (cid:0) (cid:1)
s⊤ lnA o¯ +lnB⊤q +lnB q +... (τ = 1,...,T −1)
lnq˜(s ) = τ τ τ−1 τ+1
τ (cid:0) (cid:1)
s⊤ lnA o¯ +lnB⊤q +... (τ = T)
T T T−1
and the update formula for q is
τ
(cid:40)
lnA o¯ +lnB⊤q +lnB q ... (τ = 1,...,T −1)
τ τ−1 τ+1
lnq = (A.8)
τ
lnA o¯ +lnB⊤q +... (τ = T).
T T−1
if τ ≤ t.
55
A.3 Evaluating the Divergence
After updating all of the posteriors at the present time t, we can evaluate
the divergence KL[q(s,o)∥p(s,o)] by rewriting it as
KL[q(s,o)∥p(s,o)]
= E [lnq(s,o)−lnp(s,o)]
q(s,o)
= E [lnq(s)−lnp(s)]+E [lnq(o|s)−lnp(o|s)] (A.9)
q(s) q(s,o)
and evaluating these expectations separately. Thus
E [lnq(s)−lnp(s)]
q(s)
(cid:34) (cid:35)
T T
(cid:88) (cid:88)
= E s⊤lnq −s⊤lnp − s⊤ lnB s
q(s) τ τ 0 0 τ−1 τ
τ=0 τ=1
T T
(cid:88) (cid:88)
= q⊤lnq −q⊤lnp − q⊤ lnB q
τ τ 0 0 τ−1 τ
τ=0 τ=1
T T
(cid:88) (cid:88)
= q⊤lnq − q⊤ lnB q
τ τ τ−1 τ
τ=1 τ=1
since we are assuming that q = p . And
0 0
E [lnq(o|s)−lnp(o|s)]
q(s,o)
t
(cid:88)
= E [lnq(o |s )−lnp(o |s )]
q(sτ,oτ) τ τ τ τ
τ=1
t
(cid:88)
= − E [lnp(o¯ |s )] by (6.1)
q(sτ) τ τ
τ=1
t
= − (cid:88) E (cid:2) s⊤lnA o¯ (cid:3)
q(sτ) τ τ
τ=1
t
(cid:88)
= − q⊤lnA o¯ by (6.4).
τ τ
τ=1
So (A.9) gives
KL[q(s,o)∥p(s,o)]
T t T
(cid:88) (cid:88) (cid:88)
= q⊤lnq − q⊤lnA o¯ − q⊤ lnB q . (A.10)
τ τ τ τ τ−1 τ
τ=1 τ=1 τ=1
56

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Active Inference in Discrete State Spaces from First Principles"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
