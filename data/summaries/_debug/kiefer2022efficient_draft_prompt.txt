=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Efficient search of active inference policy spaces using k-means
Citation Key: kiefer2022efficient
Authors: Alex B. Kiefer, Mahault Albarracin

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2022

Key Terms: each, large, search, spaces, means, space, efficient, inference, selection, active

=== FULL PAPER TEXT ===

Efficient search of active inference policy spaces
using k-means
Alex B. Kiefer1,2 and Mahault Albarracin1,3
1 VERSES Research Labs
2 Monash University
3 Université du Québec à Montréal
Abstract. We develop an approach to policy selection in active infer-
ence that allows us to efficiently search large policy spaces by mapping
each policy to its embedding in a vector space. We sample the expected
free energy of representative points in the space, then perform a more
thorough policy search around the most promising point in this initial
sample.
Weconsidervariousapproachestocreatingthepolicyembeddingspace,
andproposeusingk-meansclusteringtoselectrepresentativepoints.We
applyourtechniquetoagoal-orientedgraph-traversalproblem,forwhich
naive policy selection is intractable for even moderately large graphs.
Keywords: Active inference · Policy selection · Hierarchical search
1 Introduction
Activeinferenceenjoyswidespreadpopularityasamodelforcognitiveprocesses
involvingdiscretedecision-making.Typicalimplementationstreattheprocessof
active inference as a Partially Observed Markov Decision Processes (POMDP)
[22, 8, 10]. This kind of model is subject to important limitations of scale, how-
ever. In particular, the time complexity of the exhaustive policy search carried
out in standard POMDP active inference, in which the expected free energy
(EFE) of each policy is computed out to a specified time horizon, renders it
impractical for large state spaces involving many policies [14].
Therehavebeennumerouseffortstoaddressthislimitation[4],includingthe
explorationoftreesearchmethods[7,24]andvariousmethodsofpolicypruning
[6].Ourcontributioninthispapercombinespruningwiththeuseofvectorspace
embeddings [20] to create a structured policy representation in which qualita-
tively similar policies are proximal to one another. This representation can be
exploited to conduct a fast search over representative points in the space, fol-
lowed by a more thorough search in the neighborhood of the most promising
candidate, yielding a hierarchical scheme for policy search related to ideas in
hierarchical reinforcement learning [23].
The remainder of this paper is structured as follows: first, we briefly review
the standard representation and selection of policies in POMDP active infer-
ence. We then give an overview of vector space embeddings, and consider how
2202
tcO
6
]GL.sc[
3v05520.9022:viXra
2 Kiefer and Albarracin
embedding strategies similar to those used in the domain of natural language
may be applied to policies. We then consider how representative points in the
spacecanbeselected.Finally,wediscussexperimentalresultsinwhichweapply
our technique to an active inference graph-traversal problem. In this domain,
we show that it is possible to achieve accuracy comparable to exhaustive policy
search with drastically lower time complexity.
2 Policy selection in active inference
This paper presupposes familiarity with the active inference framework, but we
will briefly review the essentials of policy evaluation and selection in typical im-
plementations.AsisstandardinothersortsofMDPmodels,policiesinPOMDP
active inference are selected based on (a) the likelihood of states in the environ-
mentbeingrealized,contingentonvariousactions(decisions)throughwhichthe
agent can exert partial (probabilistic) control, and (b) the value to the agent of
those states according to some value function. In a partially observed process,
the effects of actions on states are not directly observed but are rather inferred
from the states of observation channels representing sensory input [22].
ThekeydifferencebetweenactiveinferenceandotherPOMDPmodelsandin
particularreinforcementlearningmodelsliesinthefunctionusedtocomputethe
value of the policies [18]. In active inference, the standard objective (though see
[19,11])istominimizeexpectedfreeenergy(EFE),whichistheaccumulationof
the variational free energy of the system along future trajectories, given beliefs
about the current environmental state plus a temporally deep generative model
of how states are likely to evolve.
The expected free energy G for a policy π can be computed as
i
(cid:104) (cid:105)
˙
G =Σ D [Q(o |π )||P(o )]+H(P(o |s ))Q(s |π ) (1)
πi t∈T KL t i t t t t i
where T is the time horizon, which is a hyperparameter of the model, D is
KL
a Kullback-Leibler divergence, Q(o |π) and P(o ) are the expected approximate
t t
posterior and prior generative distribution, respectively, over observations at t,
H(P(o |s ))istheentropyofthedistributionoverobservationsgivenstates,and
t t
finallyQ(s |π)isthevariational(approximateposterior)distributionoverstates
t
[21].ActiveinferencediffersfromreinforcementlearninginthatminimizingEFE
maximizes both an intrinsic reward (as defined by the generative density over
observationsP(o)encodedtheagent’s"preferencematrix")andinformationgain
(the entropy term) [5].
To decide what to do, an active inference agent first infers the current state
of the world from its observations (perceptual inference) [21], then uses the
inferred distribution over states to project the effects of action into the future,
given a transition matrix that defines p(s |s ,u), where u is a control state
t+1 t
correspondingtoaparticularaction.Thedistributionoverobservationsatfuture
time t+1 can then be calculated based on likelihoods p(o|s). These observation
probabilities are used to compute the EFE per policy, which in turn is used to
select an action (see Appendix A for equations describing these updates).
Efficient search of active inference policy spaces using k-means 3
3 Structuring policy spaces with embeddings
The serial calculation of the expected free energy over policies constitutes a
serious bottleneck that renders even relatively small-scale models intractable
given limited computational resources [3]. While performance can be improved
by parallel processing, algorithmic efficiency is always welcome to complement
raw compute power. In this paper we discuss a way to drastically increase the
efficiency of policy search by applying the concept of a vector space embedding,
in widespread use across machine learning, to policies for POMDP models.
Animportantcaveatbeforeproceedingisthatourtechniqueconstructsapol-
icy space from an initial enumeration of possible policies. This enumeration can
itselfrepresentacomputationalbottleneckwhichthepresentworkdoesnotaim
to address. Moreover, the construction of embeddings can be computationally
expensive,introducinganewbottleneckforlargeproblems(seebelow).However,
this overhead cost only needs to be computed once, rather than for every infer-
ence, analogously to the training cost for a neural network, rather than during
every iteration of a simulation, and scales with the existing policy-enumeration
bottleneck.
3.1 Vector space embeddings for policies
Inthemostgeneralterms,anembeddingisamappingfromsomesetofitemsina
domaintopointsinacontinuousvectorspace,withtheimportantpropertythat
geometricandarithmeticrelationsamongpointsinthespace(suchasEuclidean
distance) capture corresponding relations among the mapped items [13].
Vector space embeddings were established as an essential tool for machine
learning with the word2vec model of [16], which derived powerful vector repre-
sentations for the domain of natural language processing via a simple local pre-
diction task on large text corpora. Crucially, however, vector-space embeddings
are a completely general modelling tool applicable in principle to any domain in
whichsomeregularityexistssuchthatitcanbeexploitedtoconstructthevector
space. Image embeddings, for example, exploit the intrinsic structure of images
(correlations among pixel values) from various domains [1]. In the case of dis-
crete conventional symbol systems like natural language, the relevant structure
exists in the corpus as extrinsic relationships among words and phrases [12].
Fundamentally, the problem of representing policies using vector spaces is
similartothecaseoflanguageembeddings,ifpoliciesarethoughtofassequences
ormoregenerallycollectionsofactions.Forexample,considerthefollowingthree
policies, defined over abstract actions A - G:
A→B →B →C
B →A→C →C
B →E →D →G
Wemaygroupthesepoliciesinseveralways,basedontheidentitiesoftheactions
they contain. An approach analogous to the ‘bag-of-words’ model in language
4 Kiefer and Albarracin
processing, for example, would simply represent policies in terms of the counts
of all possible actions that occur in them. By this metric, the top two policies
above are more similar to one another than either is to the third, and would
land closer together in the vector space.
Alternativeembeddingstrategiesmighttakeintoaccounttheorderinwhich
actions occur. For example, a policy embedding might be constructed based on
theoccurrenceorcountsofN-grams(i.e.A→B),orgrouptogetherpoliciesthat
beginorendwiththesameorsimilaractions.Weproposeandtestavariationon
the ‘bag of actions’ approach, as well as an order-sensitive embedding strategy,
in the experimental results section below.
One may also wish to use embeddings that explicitly incorporate expected
value,e.g.bygroupingtogetherpoliciesthatallowtheagenttoachieveitsgoals
(thereisthenasimilaritytosuccessorrepresentationsinreinforcementlearning,
which are applied to active inference in [17]). In order to examine what can be
achieved on the basis of hierarchically organized policy spaces alone, without
encoding additional information about rewarding states in the representation,
we choose to focus instead on a purely structure-based grouping. This approach
oughtinprincipletobeverygenerallyapplicable,asitreliesonlyontheassump-
tion that, to some degree, structurally similar policies produce similar results.
4 Hierarchical policy selection
The point of constructing an embedding, for present purposes, is to avoid hav-
ing to compute the expected free energy of every possible policy. If we represent
policies as points in a vector space, we can get a sense of the quality (from the
agent’spointofview)ofthepoliciesinvariousregionsbysamplingasmallnum-
ber of representative points, and computing their expected free energy. We can
thenbeginfromthemostpromisingpoint(ortop-k points)andperformamore
exhaustive local search. The challenge is then to ensure that we sample policies
which are representative of the entire space while still reducing the amount of
computation required.
4.1 Clustering with k-means
In general, it is to be expected that the embeddings of real datapoints in a
vector space will be packed into relatively dense clusters separated by gulfs
corresponding to unlikely feature vectors. Visualizing the abstract vector spaces
learned by neural networks using dimensionality reduction techniques such as
t-SNE [15] often reveals precisely such separable clusters of datapoints.
Giventheassumptionofanon-uniformdistributionofdatapointsintheem-
bedding space, algorithms such as k-means clustering [9] may be effective in
selecting a representative range of initial points to sample. k-means is a rela-
tively simple unsupervised machine learning model closely related to the E-M
algorithm [2], in which k centroids are initialized (e.g. randomly) and each is
assigned the datapoints closest to it according to some distance metric (e.g. Eu-
clidean). The centroids are then re-calculated so as to minimize their mean dis-
tance to the datapoints to which they are assigned, and the assignment process
Efficient search of active inference policy spaces using k-means 5
repeats, converging on a solution in which the total distance between cluster
centers (centroids) and datapoints is minimized. The optimized centroids are
then effectively representative of different implicit ‘classes’ of datapoints.
Whileonemayconsidermanyotherclustering(and,moregenerally,unsuper-
visedstructure-discovery)algorithms,intheremainderofthispaperwefocuson
k-means and demonstrate its effectiveness, in conjunction with a suitable policy
embedding, on an applied active inference problem of some complexity.
4.2 Algorithms for policy selection
Giventheabove,weproposetwoalgorithmsforhierarchicalpolicyselection.The
first begins by selecting a cluster of policies based on the EFE of its represen-
tative point (‘cluster center’), then performs standard policy search within this
cluster. The second instead samples n points from each cluster from a uniform
distribution, and a cluster is chosen for exploration based on the mean EFE of
these points. Defining E as the embedding and C and c as policy cluster i and
i i
representativepoint(i.e.thepolicyclosesttotheclustercentroid)ofC ,respec-
i
tively,ourbasicalgorithmisAlgorithm1below.Thealternativesampling-based
algorithm is described in Appendix D.
Algorithm 1 Hierarchical policy selection
(C,c)←kmeans(E,k) (cid:46) output of the k-means algorithm
for 0<=i < k do
G =EFE(c ) (cid:46) Standard EFE computation
Ci i
end for
π←argmin(G )
C
C
u=select(π) (cid:46) Standard active inference policy selection on reduced policy space
Aninterestingfeatureofthisapproachtohierarchicalpolicyselectionisthat
unlike other techniques that have been successfully applied to active inference
such as MCTS [7], it exploits purely structural features of the embedding space
and does not require empirical tuning. That said, the quality of the solution
dependsheavilyonchoiceofthehyperparameterk,asdiscussedinthefollowing
section. We note that many interesting variations on this idea remain to be
explored, such as adding depth to the hierarchy (i.e. clusters of clusters) and
samplingrepresentativepointsfromdistributionsinformedbypastperformance.
5 Experiment: Graph navigation
We tested our proposal on a graph-navigation problem for which exhaustive
policysearchisimpracticalevenongraphsofmoderatecomplexity(>6densely
connected nodes). The agent’s goal is to choose the shortest route to its desired
destinationonarandomdirectedgraphwithweightededges.Whileagent-based
6 Kiefer and Albarracin
decision models are decidedly overkill for shortest-route discovery on graphs,
this task provides an ideal environment in which to test our approach to policy
search,sinceitrequiresinferencewithinatemporallydeepgenerativemodeland
also offers a highly structured implicit policy space, as discussed below.
5.1 Model
We run our simulations on randomly generated directed, weighted graphs, with
edge weights chosen from a small set of possible values. We first include each
possible edge with probability 2 (where |V| is the number of nodes in the
|V|
graph) and then enforce strong connectivity so that any node is reachable from
anyother.Inaddition,everynodeinthegraphcontainsaself-connection.Ineach
simulation, the agent is randomly assigned an initial location and ‘destination’
node, and at each step chooses to move to an adjacent node or stay still. The
self-connectionshaveweights>thelargestbetween-nodeedgeweightinthecase
ofallbutthe‘destination’node,whoseself-connectionweightis0.Theagenthas
preferences for being at its destination and against traversing edges with large
weights.Tosimplifytherepresentation,wedefinetheagent’spossiblelocationsin
termsofedgesonthegraph,withtheconventionthatthesecondnodeinanedge
represents the agent’s current location and the first node represents its previous
location (full details of the agent’s generative model are given in Appendix C).
Becausewewereinterestedinmodelingasituationinwhichagentsknewhowto
reach their goals, we set the policy length to the number of nodes in the graph,
sothatatleastonepolicythatreachesthegoalisalwaysavailable(givenstrong
connectivity).
Since the states of the environment are encoded in terms of node pairs or
edges,theactionsthatconstituteourpoliciesaretransitionsbetweenedges.For
example, (A,B) ⇒ (B,C) is a valid policy on a graph with nodes A, B, and
C and directed edges (A,B) and (B,C). We prune policies containing ‘invalid’
actions (that is, actions that imply impossible state transitions, such as moving
directly from edge (A,B) to edge (C,D)) from the policy space prior to search.
5.2 Policy embeddings
We experimented with three policy embedding strategies, an Edit Distance Ma-
trix (EDM), a Bag-of-Edges (BOE) representation, and a BOE representation
augmentedwithanextradimensionthatrecordstheterminalnodeofthepolicy
(aBOE).
The EDM for a list of policies is constructed by counting the number of
moves it would take to transform one policy (i.e., path through a graph) into
another. Intuitively, if the atomic moves are addition and deletion of nodes and
edges,thisshouldcorrespondtothenumberofelements(bothnodesandedges)
thatappearinonegraphandnottheother.Inotherwords,foredgesetsE and
i
E and vertex sets V and V of graphs G and G , the edit distance d is:
j i j i j ij
d =|(V ∪V \V ∩V )|+|(E ∪E \E ∩E )|
ij i j i j i j i j
Efficient search of active inference policy spaces using k-means 7
Fig.1. A: Two sample graphs from our experiments, with start node (green) and
destination node (purple) highlighted, along with the path the agent followed. Spatial
layout is random and visual path distance does not track edge weight. B: Expected
free energy of policies represented by k = 12 cluster centers. C: Mean EFE of all
clusters grouped by the corresponding cluster in plot (B). D: EFE of 20 randomly
sampledclustercentersfork=100.E:MeanEFEofcorrespondingclusters.F−I:2D
projectionsofpolicyembeddingspacesusingPCA(axesrepresentarbitrarydimensions
in the reduced embedding space). Blue points are all policies, and orange points are
cluster centers discovered by k-means. F: Global edit distance matrix embedding. G:
Global ‘bag-of-edges’ embedding. H, I: examples of corresponding local embeddings,
limited to policies viable from a given location.
8 Kiefer and Albarracin
Theembeddingd forpolicyiisthenthevectorofeditdistancestoallother
i
policies, which can combined with the other policy embeddings into a single
embedding matrix D.
The EDM representation is a priori desirable because it takes the order of
actions in a policy into account, but it can become computationally expensive
toconstructforlargergraphs,leadingtoadifferentcomputationalbottleneckto
the one we set out to avoid. A much simpler representation is the Bag-of-Edges
embedding, which as the name suggests is similar to the bag-of-words model
in that it represents each policy simply by a vector of counts of the edges that
occur in it. The augmented BOE embedding simply appends the identity of the
last node reached by a policy to the BOE embedding for the policy.
5.3 Deriving representative points using k-means
Animplicithypothesisbehindourapproachtopolicysearchisthattherewould
beacorrelationbetweenthedegreetowhichtwopoliciesarestructurallyrelated
(hencetheirlocationinembeddingspace)andtheirenergy.Totestthishypoth-
esis, we plotted the EFE of each of the policies closest to the cluster centers, or
ofa randomsample ofthem forlarge k,againstthemean EFEof thepolicies in
each cluster. We found that the degree to which the correlation holds is highly
dependent on choice of k, but that with the right hyperparameter choice, the
cluster centers are a good guide to the EFE in their regions (see Figure 1B-E).
Intuitively,assumingsuchcorrelationamongtheEFEsofpoliciesproximalin
the embedding space, there should be a tradeoff between the representativeness
of the clusters chosen by k-means with respect to the EFE of their neighbors
and efficiency gains due to using a small number of clusters for the initial sweep
over policies. We found however that at least for the sizes of graph we explored
(3-6 nodes), too large a k value actually hurt performance as well as being less
efficient. We performed a very limited hyperparameter search and found that a
value of k =12 worked well in practice, and report results for values [6,12].
As suggested by an anonymous reviewer, however, further optimization of k
is important, and a future avenue of research for this application would be to
explore schemes for optimizing k automatically, including online, e.g. so as to
maximize EFE returns and minimize EFE variance within each cluster dynami-
cally as the system evolves. This would bring our work more closely in line with
[7], in which Monto Carlo tree search and an amortized variational inference
procedure are used to improve the efficiency of policy selection.
5.4 Local VS global embeddings
We found that running k-means on the full embedding matrix for the entire
policy space (including policies that were ‘invalid’ from the agent’s current lo-
cation) sometimes returned no, or very few, valid policies among the cluster
centers, leading to sub-optimal choices.
To remedy this, we try pruning all policies not beginning at the agent’s cur-
rentlocationbydefininglocalembeddingsE foreachpossibleagentlocations
si i
as E ←{e ∈E:s ∈π } and perform clustering on these reduced subspaces
si j i j
togetper-locationk-meansclustersandclustercenters.Ateachsimulationstep
Efficient search of active inference policy spaces using k-means 9
we then run our hierarchical search algorithm on the appropriate subspace. For
a fair comparison, we benchmarked this procedure against standard active in-
ference policy selection run on the same local policy subspaces.
The utility of embeddings is best evaluated by measuring their performance,
but for visualization purposes we also constructed dimensionally reduced repre-
sentationsofthehigh-dimensionalembeddingspacesusingPrincipalComponent
Analysis(PCA),showninFigure1F-I.Thoughheuristic,theseplotssuggestthat
policies do indeed cluster in interesting ways, and that the k-means procedure
is good at finding these clusters.
5.5 Results
Fig.2.Selectedresultsongraphswith3−5nodes,for‘Global’(fullpolicyspace)and
‘Local’(location-basedsubspace)conditions.Toprow:Percentofsolutionsfoundthat
were optimal. Standard is standard active inference policy search, and the hyperpa-
rameters used for the embedding conditions are listed next to the embedding name.
Bottom row: Mean policy inference times for standard VS embedding conditions.
A sample of our results is presented in Figure 2 (please see Appendix E for
additional data). To obtain these results, we generated 40 random graphs in
each size category (3,4,5) and computed mean execution time and optimality
for each embedding type (including "None" / standard policy selection, EDM,
10 Kiefer and Albarracin
BOE and aBOE), as well as for two values of the hyperparameters k (number
of clusters) and n (number of samples used to calculate per-cluster EFE). An
optimal solution was defined as one in which the agent takes a shortest path
from its initial location to its ‘destination node’ and then remains there. Figure
2 shows only one hyperparameter combination for each embedding type.
For larger graphs, hierarchical policy search improved the calculation time
for action selection by about an order of magnitude, with similar relative re-
ductions in the global and local conditions. The mean inference time using k-
means/embeddingswasabout.17seconds,andforstandardpolicyinference,1.2
seconds. Construction times for all embeddings, including the time to carry out
k-means clustering, were negligible for all embedding styles except EDM, which
exponential time complexity precluded using on graphs of size > 5 nodes. The
EDM representation achieved near-optimal results on small graphs, but its per-
formanceinanycasedegradedonlargergraphs.Whilethesepreliminaryresults
arenotingeneralimpressiveintermsofoptimality,thebestresultssuggestthat
our technique could be made competitive with more extensive hyperparameter
tuning (as well as potentially different clustering and embedding methods).
Fig.3. A: Graph in which the agent failed to find a route. B(Left, Right): Cluster
center and mean cluster EFE values, respectively. C: Policy embeddings and cluster
centers for this example.
We analyzed one interesting failure case in which the agent did not move
from its initial location. As shown in Fig. 3, it appears that in this case, the
EFE of the clusters was not a good guide to the local energy landscape, and in
addition, k-means did not adequately cover the policy space (note the obvious
cluster on the left without an assigned cluster center).
6 Conclusion
The experiments reported in this paper are very much an initial cursory ex-
ploration of the application of embedding spaces and clustering to hierarchical
policy search in active inference. Very sophisticated graph embedding schemes,
usingneuralnetworkstrainedonrandomwalksforexample,couldbeappliedto
problemslikeours.Theinitialresultswereportsuggestthatthislineofresearch
may prove useful in expanding the sphere of applicability of active inference
POMDP models.
Efficient search of active inference policy spaces using k-means 11
Acknowledgements
Alex Kiefer and Mahault Albarracin are supported by VERSES Research.
Code Availability
Code for reproducing our experiments and analysis can be found at:
https://github.com/exilefaker/policy-embeddings
References
[1] Piotr Bojanowski et al. Optimizing the Latent Space of Generative Net-
works. 2017. doi: 10.48550/ARXIV.1707.05776. url: https://arxiv.
org/abs/1707.05776.
[2] Leon Bottou and Yoshua Bengio. “Convergence properties of the k-means
algorithms”.In:Advancesinneuralinformationprocessingsystems 7(1994).
[3] Théophile Champion, Howard Bowman, and Marek Grześ. “Branching
time active inference: Empirical study and complexity class analysis”. In:
Neural Networks (2022).
[4] Théophile Champion et al. “Branching Time Active Inference: The theory
and its generality”. In: Neural Networks 151 (2022), pp. 295–316. issn:
0893-6080. doi: https://doi.org/10.1016/j.neunet.2022.03.
036. url: https://www.sciencedirect.com/science/article/pii/
S0893608022001149.
[5] Lancelot Da Costa et al. “The relationship between dynamic program-
ming and active inference: the discrete, finite-horizon case”. In: ArXiv
abs/2009.08111 (2020).
[6] LancelotDaCostaetal.“Activeinferenceondiscretestate-spaces:Asyn-
thesis”. In: Journal of Mathematical Psychology 99 (2020), p. 102447.
[7] Zafeirios Fountas et al. “Deep Active Inference Agents Using Monte-Carlo
Methods”. In: Proceedings of the 34th International Conference on Neural
Information Processing Systems. NIPS’20. Vancouver, BC, Canada: Cur-
ran Associates Inc., 2020. isbn: 9781713829546.
[8] Karl Friston et al. “Active Inference: A Process Theory”. In: Neural Com-
putation 29.1 (Jan. 2017), pp. 1–49. issn: 0899-7667. doi: 10.1162/NECO_
a_00912. eprint: https://direct.mit.edu/neco/article-pdf/29/1/1/
984132/neco\_a\_00912.pdf. url: https://doi.org/10.1162/NECO%
5C_a%5C_00912.
[9] MGoyalandSKumar.“Improvingtheinitialcentroidsofk-meanscluster-
ingalgorithmtogeneralizeitsapplicability”.In:JournalofTheInstitution
of Engineers (India): Series B 95.4 (2014), pp. 345–350.
[10] ConorHeinsetal.“pymdp:APythonlibraryforactiveinferenceindiscrete
state spaces”. In: CoRR abs/2201.03904 (2022). arXiv: 2201.03904. url:
https://arxiv.org/abs/2201.03904.
12 Kiefer and Albarracin
[11] Tejas D. Kulkarni et al. Deep Successor Reinforcement Learning. 2016.
doi: 10.48550/ARXIV.1606.02396. url: https://arxiv.org/abs/
1606.02396.
[12] Yang Li and Tao Yang. “Word embedding for understanding natural lan-
guage:asurvey”.In:Guidetobigdataapplications.Springer,2018,pp.83–
104.
[13] Leo Liberti et al. “Euclidean distance geometry and applications”. In:
SIAM review 56.1 (2014), pp. 3–69.
[14] Jan-MatthisLueckmannetal.“Benchmarkingsimulation-basedinference”.
In:InternationalConferenceonArtificialIntelligenceandStatistics.PMLR.
2021, pp. 343–351.
[15] Laurens van der Maaten and Geoffrey Hinton. “Visualizing Data using t-
SNE”. In: Journal of Machine Learning Research 9.86 (2008), pp. 2579–
2605. url: http://jmlr.org/papers/v9/vandermaaten08a.html.
[16] Tomás Mikolov et al. “Distributed Representations of Words and Phrases
andtheirCompositionality”.In:CoRRabs/1310.4546(2013).arXiv:1310.
4546. url: http://arxiv.org/abs/1310.4546.
[17] Beren Millidge and Christopher L Buckley. Successor Representation Ac-
tive Inference. 2022. doi: 10.48550/ARXIV.2207.09897. url: https:
//arxiv.org/abs/2207.09897.
[18] BerenMillidgeetal.“Ontherelationshipbetweenactiveinferenceandcon-
trolasinference”.In:International workshop on active inference.Springer.
2020, pp. 3–11.
[19] Thomas Parr and Karl Friston. “Generalised free energy and active infer-
ence”. In: Biological Cybernetics 113.5-6 (Dec. 2019), pp. 495–513. doi:
10.1007/s00422-019-00805-w.
[20] Kaspar Riesen and Horst Bunke. “Graph classification based on vector
space embedding”. In: International Journal of Pattern Recognition and
Artificial Intelligence 23.06 (2009), pp. 1053–1081.
[21] Philipp Schwartenbeck et al. “Exploration, novelty, surprise, and free en-
ergy minimization”. In: Frontiers in psychology (2013), p. 710.
[22] RyanSmith,KarlJFriston,andChristopherJWhyte.“Astep-by-steptu-
torialonactiveinferenceanditsapplicationtoempiricaldata”.In:Journal
of mathematical psychology 107 (2022), p. 102632.
[23] LorenzoSteccanellaetal.“Hierarchicalreinforcementlearningforefficient
exploration and transfer”. In: CoRR abs/2011.06335 (2020). arXiv: 2011.
06335. url: https://arxiv.org/abs/2011.06335.
[24] NickWhiteley,ChristopheAndrieu,andArnaudDoucet.“EfficientBayesian
inference for switching state-space models using discrete particle Markov
chain Monte Carlo methods”. In: arXiv preprint arXiv:1011.2437 (2010).
Efficient search of active inference policy spaces using k-means 13
Appendix A: Computing per-policy EFE
Theexpectedfreeenergyforapolicycanbecomputedasfollows.First,weinfer
a distribution over states at the current step of the simulation, combining the
transition probabilities and current observations:
˙
Q(s )=σ[lnA +lnB Q(s )]
t ot u t−1
The inferred distribution over states can then be used to project the effects
of action into the future, given a parameter B that defines p(s |s ,u), where
t+1 t
u is a control state corresponding to a particular action:
˙
Q(s )=B Q(s )
t+1 u t
Givenaninferreddistributionoverfuturestates,thedistributionoverobser-
vations at future time t+1 can be calculated as
˙
Q(o )=AQ(s )
t+1 t+1
where A is the likelihood mapping from (beliefs about) states to observations.
The above assumes a single observation modality and controllable state factor,
but can straightforwardly be generalized to larger factorized state spaces and
multiple observation channels.
ByrepeatingtheaboveprocessusingtheQ(s )resultingfromtheprevious
t+1
time-step as input to the next, and accumulating the G defined in Eq. (1) out
πi
to the policy horizon, we can derive a distribution Q over policies as σ(−G ),
π π
where G is the vector of expected free energies for all available policies and
π
σ(x) is a softmax function. Finally, the next action is sampled from a softmax
distribution whose logits are the summed probabilities under Q of the policies
π
consistent with each action.
Note that in the above we have omitted aspects of typical active inference
models not material to our concerns in this paper, such as precision-weighting
of the expected free energy, habits, and inverse temperature parameters.
Appendix B: Policy selection and expected free energy
WeusethestandardprocedureoutlinedintheIntroductionforselectingpolicies
with one exception: the expected free energy has an additional term which is
the dot product of the posterior distribution over states (locations) with the
associated edge weights. We combine this with the standard EFE calculation
using a free hyperparameter λ:
(cid:104) (cid:105)
˙
G =Σ D [Q(o |π )||P(o )]+H(P(o |s ))Q(s |π ) +λ∗weights·Q(s |π)
πi t∈T KL t i t t t t i t
(2)
Thischoicewasmadepurelyforconveniencesinceotherwisepreferencesover
weightswouldhavetoberepresentedusinganawkwardcategoricaldistribution,
14 Kiefer and Albarracin
and it has no impact on the main comparison between policy search techniques
of interest to us in this paper.
Appendix C: Generative model details
In our experimental setup, an active inference agent’s generative model is au-
tomatically constructed when a graph is generated. The standard variables in
active inference POMDPs have the following interpretations in our model:
– states: An edge (node , node ) in the graph. We interpret the first
prev current
node in the edge as the agent’s previous location and the second node as its
current location.
– observations: A tuple ((node , node ), weight) representing obser-
prev current
vationsofedgesandcorrespondingedgeweights,wheretheedgecorresponds
to the node pair in states
– controlstates:Thereisanaction(andcorrespondingcontrolstate)forevery
possible local transition in the graph.
– A:Theagent’s‘A’orlikelihoodmatrix,whichinthiscaseissimplyaniden-
tity mapping from states to observations
– B: The state transition matrix, which encodes deterministic knowledge of
action-conditioned state transitions, and is constructed so as to exclude in-
valid transitions (i.e. between non-adjacent nodes in the graph)
– C: Preference matrix, which distributes probability mass equally over all
edges that end on the agent’s ‘destination’ node. There is also implicitly
a preference against high edge weights, but to simplify the representation
we incorporate this directly within the expected free energy calculation (see
Appendix B).
– D: Prior over initial location states.
Appendix D: Sample-based hierarchical policy selection
Withvariablesdefinedasabove,butwithc denotingthejthrandomlysampled
j
policy in a cluster, the alternative sample-based policy selection algorithm is:
Efficient search of active inference policy spaces using k-means 15
Algorithm 2 Sample-based hierarchical policy selection
for 0<=i<k do
for 0<=j <n do
c ∼U(C ) (cid:46) This is a uniform distribution over cluster members
j i
end for
G =
(cid:80) jEFE(cj)
Ci n
end for
π←argmin(G )
C
C
u=select(π)
Appendix E: Additional results
Here we present some additional experimental results. Figure 4 plots the com-
binedembeddingconstructionandk-meansclusteringtimesforeachembedding
type. Table 1 below shows the full set of optimality results we obtained, aver-
aged across trials (i.e. across particular graphs in each category). Here, "None"
denotesstandardpolicyselection.Bestembeddingresultsforeachgraphsizeare
bolded.
Fig.4.Left:Timetakentoconstructembeddingspacesandperformk-meansclustering
ontheresultingembeddings.Theincreasedtimesforbothconstructionandclustering
fortheEDMrepresentationareduetotherelativelymuchlargerdimensionalityofthe
EDM embedding: one dimension for each policy, rather than one for each vertex and
edge,asintheBOEandaBOErepresentations.Right:‘detail’plotoftheconstruction
times by graph size for the BOE and aBOE embeddings.
16 Kiefer and Albarracin
Table 1. Percent of solutions optimal
Graph size 3 4 5
Scope Embedding k n
Global None — — 100.0 97.5 97.4
BOE 6 1 70.0 65.0 56.4
3 77.5 62.5 46.1
12 1 60.0 87.5 59.0
3 77.5 62.5 51.3
EDM 6 1 70.0 67.5 48.7
3 80.0 72.5 56.4
12 1 67.5 72.5 64.1
3 80.0 72.5 61.5
aBOE 6 1 82.5 85.0 66.7
3 87.5 85.0 61.5
12 1 85.0 67.5 35.9
3 75.0 92.5 79.5
Local None — — 100.0 97.5 97.5
BOE 6 1 17.5 37.5 48.7
3 52.5 55.0 61.5
12 1 42.5 47.5 41.0
3 82.5 42.5 25.6
EDM 6 1 5.0 52.5 45.0
3 50.0 65.0 35.0
12 1 50.0 60.0 40.0
3 97.5 32.5 15.4
aBOE 6 1 15.0 12.5 5.1
3 35.0 12.5 5.1
12 1 5.0 22.5 12.8
3 70.0 20.0 12.8

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Efficient search of active inference policy spaces using k-means"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
