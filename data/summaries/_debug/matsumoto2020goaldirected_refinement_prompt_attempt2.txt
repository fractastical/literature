Fix these issues in your summary:
- Too short: 151 words (minimum 200)

Current summary:
Okay, here’s a revised summary of the paper, adhering to all the specified requirements:**Goal-Directed Planning for Habituated Agents by Active Inference Using a Variational Recurrent Neural Network**This research investigates goal-directed planning forhabituatedagentsbyutilizingactiveinference,a methodbasedonvariationalrecurrentneuralnetworks. Thestudy aims to address the problem of generating effective goal-directed plans based on learning from limited amounts of data. The proposed approach employs a variational recurrent neural network (PV-RNN) to achieve this. The PV-RNN is designed to learn and implement a hierarchical framework for goal-directed planning, incorporating elements of autonomy and adaptive control. The model’s architecture facilitates the learning of functional hierarchical elements, which are essential for achieving complex behaviors in dynamic environments. The research focuses on the development of a model that can learn to generate multiple goal-directed behaviors, and that can achieve success rates in accomplishing tasks. The model is designed to learn and implement a hierarchical framework for goal-directed planning, incorporating elements of autonomy and

Paper text:
Goal-Directed Planning for Habituated Agents by
Active Inference Using a Variational Recurrent Neural
Network
TakazumiMatsumoto1 JunTani2,∗
1OkinawaInstituteofScienceandTechnology,takazumi.matsumoto@oist.jp
2OkinawaInstituteofScienceandTechnology,tani1216jp@gmail.com
∗Correspondingauthor
Abstract
Itiscrucialtoaskhowagentscanachievegoalsbygeneratingactionplansusingonlypartial
models of the world acquired through habituated sensory-motor experiences. Although many
existingroboticsstudiesuseaforwardmodelframework,therearegeneralizationissueswithhigh
degreesoffreedom. Thecurrentstudyshowsthatthepredictivecoding(PC)andactiveinference
(AIF)frameworks,whichemployagenerativemodel,candevelopbettergeneralizationbylearning
apriordistributioninalowdimensionallatentstatespacerepresentingprobabilisticstructures
extracted from well habituated sensory-motor trajectories. In our proposed model, learning is
carriedoutbyinferringoptimallatentvariablesaswellassynapticweightsformaximizingthe
evidencelowerbound,whilegoal-directedplanningisaccomplishedbyinferringlatentvariablesfor
maximizingtheestimatedlowerbound. Ourproposedmodelwasevaluatedwithbothsimpleand
complexrobotictasksinsimulation,whichdemonstratedsufficientgeneralizationinlearningwith
limitedtrainingdatabysettinganintermediatevalueforaregularizationcoefficient. Furthermore,
comparativesimulationresultsshowthattheproposedmodeloutperformsaconventionalforward
model in goal-directed planning, due to the learned prior confining the search of motor plans
withintherangeofhabituatedtrajectories.
Keywords: goaldirectedplanning;activeinference;predictivecoding;variationalBayes;recurrentneural
network
1 Introduction
Itisgenerallyassumedthatagentscanneveraccessoracquirecompletemodelsoftheworld
whichtheyareinteractingwith[18,35]. Thisisbecausetheamountofexperienceaccumulatedby
interactingwiththeworldinafinitetimeislimited,andusuallytheworlditselfisalsodynamically
changing. Undersuchconditions,agentswithhighercognitivecapability,suchashumans,seemto
beabletogeneratefeasiblegoal-directedactionsbymentallyimagingpossiblebehavioralplans
usingonlypartiallydevelopedmodelsoftheworld,learningfromlimitedexperiencesofinteraction
withtheworld.
1
0202
yaM
72
]OR.sc[
1v65641.5002:viXra
Howisthispossible? Inaddressingthisproblem,thecurrentpaperproposesanovelmodelfor
goal-directedplangenerationreferredtoasgoal-directedlatentvariableinference(GLean),based
onlearningbyleveragingtworelatedframeworks,predictivecoding(PC)[34,38,28,12,21,10,13]
andactiveinference(AIF)[15,16,17,5,33,31]. Inparticular,weattempttoshowthatagentscan
generateadequategoal-directedbehaviorsbasedonlearninginthehabituatedrangeoftheworld
byconductingsimulationstudiesontheproposedmodel.
Inbrainmodelingstudies,ithaslongbeenconsideredthatthebrainusesaninternalgenerative
modeltopredictsensoryoutcomesofitsownactions. Inthisscenario,thefitbetweenthemodel’s
predictionandtheactualsensationcanbeimprovedintwoways. Thefirstisbyadaptingbelief
orintentionrepresentedbytheinternalstateofthegenerativemodelsothatthereconstruction
errorcanbeminimized[34,12,21,10]. Thiscorrespondstoperception. Thesecondapproachis
byactingontheenvironmentinamannersuchthattheresultantsensationcanbetterfitwiththe
model’sprediction[15,5,33]. TheformerideahasbeenformulatedintermsofPCandthelatterby
AIF.However,thesetwoframeworksshouldbeconsideredinunisonasperceptionandactionare
effectivelytwosidesofthesamecoininenactivecognition.
Originally, theideaofaninternalmodelforsensory-motor systems wasinvestigatedinthe
study of the forward model (FM) [29, 25, 24] (see Figure 1a). Although both FM and PC can
predictthenextlatentstateandassociatedsensoryinputs,differenttypesofconditioningonthe
predictionwereconsidered. InFM,thepredictedsensorystateisconditionedbythecurrentmotor
commandsandthecurrentlatentstate,whileinPCitisconditionedonlybythecurrentlatentstate.
Intheory,itispossibletoinferoptimalmotorcommandsforachievingdesiredstatesusingFMby
consideringadditionalcostfunctionssuchasjerkminimization,torqueminimization,trajectory
distanceminimizationetc. [25]. Inpractice,however,thisinferencetendstoproduceerroneous
solutions unless the predictive model learns the outcomes for all possible motor combinations,
whichisintractablewhenthemotorcomponenthasahighdegreeoffreedom.
(a) (b)
Figure 1: (a) The forward model and (b) the predictive coding and active inference framework
wherestate t andsense t+1 representthecurrentlatentstateandpredictionofthenextsensorystatein
termsoftheexteroceptionandproprioception. Thepredictedproprioceptioncanthenbeconverted
intoamotorcontrolsignalasnecessary,suchasbyusinganinversemodelasdepictedin(b).
However,thismaynotbethecaseifthePCandAIFframeworksareusedincombinationas
showninFigure1b. InthePCcomponent, aninternalmodelpredictsboththelatentstateand
sensoryinputsintermsoftheexteroceptionandproprioceptioninthenexttimestepbyreceivingthe
currentlatentstate. Itcanbeconsideredthatthesensorysequencesareembeddedinthelatentstate
spacethroughiterativepredictivelearning. IntheAIFcomponent,aninversemodelcanbeusedto
2
mapthepredictedsensoryinputstomotorcommandswhichcanrealizethepredictedsensation.
Suchaninversemodelcanbeimplementedinastraightforwardmannerby,forexample,aPID
controllerwhereinthenecessarymotortorquetogenerateexpectedmovementsintermsofposition
andvelocitycanbecomputedthrougherrorfeedbackbetweenthepredictedproprioception(for
e.g.,jointanglesinarobot)andtheoutcome.
Givenadesiredstatetobeachieved,theoptimalmotorcommandatthenexttimestepcanbe
obtainedbyfirstinferringanoptimallatentstateinthecurrenttimestepwhichcangeneratethebest
fitwiththedesiredstatewiththeminimalerror. Theobtainedlatentstateinthecurrenttimestepis
mappedtothesensorystateoftheproprioceptionandexteroceptionexpectedinthenexttimestep,
andtheproprioceptionisfinallymappedtoamotorcommandbytheinversemodel.
Ifweassumethatagentsactontheirenvironmentnotthroughallpossiblecombinationsof
motor command sequences but only a subset of them in terms of habituated trajectories, the
effective dimensionality of the sensory space can be reduced drastically. This also results in
significantreductionintherequireddimensionalityofthelatentstatespacewhichembedsthe
sensorysequencesthroughlearning.
Consequently, the problem of motor planning for achieving a desired state could become
tractablewhentheinferenceforanoptimallatentstatecanbelimitedinitsrelativelylowdimen-
sionalspace. Thesame,however,cannotbeappliedinthecaseofFMasthesearchforanoptimal
motorplancannotbeconstrainedwithintherangeofhabituatedmotortrajectories,asexplained
previously.
(a) (b) (c)
Figure2: Threedifferentmodelsforlearning-basedgoal-directedmotorplanning. (a)Theforward
modelimplementedinanRNN,(b)PCandAIFframeworksimplementedinanRNNusinginitial
sensitivitybylatentrandomvariablesattheinitialstep,eitherbythestochasticz orthedetermin-
t
isticd ,and(c)theproposedGLeanschemebasedonthePCandAIFframeworkimplemented
t
inavariationalRNN.Ineachcase,thehorizontalaxisindicatesprogressionthroughtime(leftto
right). Theblackarrowsrepresentcomputationintheforwardpass,whiletheredarrowsrepresent
predictionerrorbeingpropagatedduringbackpropagationthroughtime(BPTT).
TheGLeanschemeproposedinthispaperimplementstheaforementionedconsiderationsusing
avariationalrecurrentneuralnetwork(RNN)modelandtestsitinlearning-basedrobotmotor
planningtasks. Inthefollowing,webrieflyreviewhowmodelsoflearning-basedgoal-directed
planninghavebeenstudiedandhowsuchpriorstudieshavebeenextendedtothecurrentwork.
Tani[36]proposedagoal-directedplanningschemeforarobotnavigationtaskbasedonFM(see
Figure2a). Inthismodel,thelatentstatewasrepresentedbytheactivityofthecontextunitsin
3
a Jordan-type RNN [22]. During action planning, a sequence of discrete actions m ,...,m such
1 t
asbranchingornotbranchingateachencounteredbranchingpointinamazeenvironmentcan
beinferredbyminimizingtheerrorbetweenpredictedsensoryinputsatdistalstepv t+1 andthe
sensoryinputsassociatedwiththegivengoalv t+1 .
Arie et al. [2] developed a model of learning-based planning analogous to the PC and AIF
frameworks(seeFigure2b). Inthismodel,theinitialsensitivitycharacteristicsofdeterministic
dynamicsystemsisusedwhereindiversesensory-motorsequencesexperiencedareembedded
intoadistributionoftheinitiallatentstateofanRNNmodelthroughiterativelearning. Assuch,
learningasetofsensory-motorsequencesisconductedbymeansofadaptingtwodifferenttypes
ofvariables—connectivityweightsoftheRNNwhicharesharedbyallsequences,andtheinitial
statewhichisindividuallyadaptedforeachsequence. Afterlearningwithagiveninitiallatent
stated ,thecorrespondingsequenceconsistingoftheexteroceptionv andtheproprioception
0 1...t
p isgenerated. Byfeedingthepredictedproprioceptionateachtimestep p asthetargetbody
1...t t
posturetotheinversemodel,thecorrespondingmotorcommandm canbegenerated. Inplanning
t
mode,theinitialstateisinferredsuchthatthedistalstateinageneratedsensory-motorsequence
canagreewiththedesiredgoalstatewithminimalerror. Theinferredinitialstaterepresentsan
intentionorbelieftogenerateamotorprogramreachingthegoalstate.
Similar work by Choi et al. [8] employed a deterministic RNN architecture to accomplish
goal-directedmotorplanningwithvisualpredictionsforrobotictasksbysearchinginthisinitial
statespace.Inthiscase,whilethenetworkwasabledemonstrateadequategeneralizationforsimple
taskssuchastouchingapointwitharobotarm,thesuccessratewasconsiderablyreducedinamore
complexgraspandplacetask. Recently,Jungetal. [23]extendedthismodelbyallowingrandom
variablesz withmeanandvariancetorepresenttheinitialstatesforthepurposeofextractinga
0
probabilisticdistributionamongtrainedsensory-motorsequences(seeFigure2b).Theexperimental
resultsusingthismodelforataskofstackingmultipleobjectsbyarobotarmshowedthatthis
schemeofusingrandomvariablesfortheinitiallatentstateisbeneficialintermsofgeneralization
inbothtrainingandmotorplangeneration.
Inthiscurrentpaper,weproposeafurtherdevelopmentoftheaforementionedmodelusingthe
frameworkofPCandAIFtotackletheissueoflearning-basedgoal-directedmotorplanningby
expandinguponthevariationalBayesapproach. ThemainpurposeofourproposedGLeanscheme
istoenablethenetworktolearntoextractthetransitionprobabilitydistributionofthelatentstate
ateachtimestepasasequenceprior[9]andtoutilizeitforgeneratinggoal-directedplanswith
improvedgeneralization. Forthispurpose,weutilizearecentlyproposedvariationalRNNknown
as the predictive-coding inspired variational RNN (PV-RNN) [1] for implementing the PC and
AIFframeworkssuchthatthelatentstateateachtimestepisrepresentedbybothadeterministic
variabled andarandomvariablez asshowninFigure2c. Learningofthemodelisaccomplished
t t
bymaximizingtheevidencelowerbound,whereastheestimatedlowerboundismaximizedfor
goal-directed motor plan generation. Both lower bounds are computed as summations of the
accuracytermandthecomplexityterm. AformaldescriptionofthemodelisgiveninSection2.
The proposed model also uses ideas considered in development of the so-called Multiple
Timescale RNNs (MTRNN) [39], which is built on multiple layers of Continuous Time RNNs
(CTRNN)[3]whereinhigherlayershaveslowertimescaledynamicsandlowerlayershavefaster
dynamics (note that Figure 2 shows only a single layer for simplicity). It has been shown that
MTRNN enhances development of functional hierarchy among layers. It does so by using the
timescaledifferencebywhichmoreabstractrepresentationsofactionplansaredevelopedinthe
higherlayerswhileamoredetailedrepresentationofsensory-motorpatternsdevelopinthelower
4
layers[30].
In Section 3 we evaluate GLean by conducting two sets of simulated experiments. Using a
minimaltaskset,thefirstexperimentexaminesessentialcharacteristicsoftheproposedmodelin
learningtogenerategoal-directedplans. Inparticular,weinvestigatetheeffectsthatregulatingthe
strengthofthecomplexityterminthelowerboundhasuponlearningperformanceaswellasgoal-
directedmotorplangeneration. Furthermore,wecomparethedifferenceinplanningperformance
betweenmorehabituatedgoalstatesandlesshabituatedgoalstatesinordertoexaminetheeffect
ofhabituationinlearningongoal-directedplangeneration.
Thesecondsimulationexperimentusesamorerealisticrobotictaskemployingamodelofareal
robotandcomparestheperformancebetweenthreemodelsdepictedinFigure2: FM,PC+AIF
withinitialstatesensitivity,andtheproposedGLeanscheme. Theseexperimentswillclarifyhow
GLeancangeneratefeasiblegoal-directedplansandtheresultantactions. Itdoessobydeveloping
regionsofhabituationintermsofthesequencepriorinthelatentstatespacebymeansoflearning
fromalimitedamountofsensory-motorexperiences.
2 Model
Inthissection, wewillfirstpresentanoverviewofthePV-RNNmodelfollowedbyamore
detailedexplanationoftrainingandplanning,includingformulationoftheevidencelowerbound
andapproximatelowerboundusedintrainingandplanningrespectively. Wedonotattemptto
makeanexhaustivederivationofPV-RNNinthispaper,ratherwefocusonthesalientpointsand
changescomparedtotheoriginallyproposedmodelin[1].
2.1 OverviewofPV-RNN
Figure3showsagraphicalrepresentationofPV-RNNasimplementedinthispaper. Notethat
forgeneralitywedenotealltheoutputofthemodelasx. ComparedtotheoriginalPV-RNN,we
havemadethreekeychangestothemodel. Thefirstisatt =1,thepriordistributionisfixedasa
unitGaussian(depictedaszUG),whichactsasaweightedregularizationoninitialstatesensitivity
ofthenetwork.Thisisprimarilytoimprovethestabilityoflearningincertainedgeconditions.Note
thatthedeterministicvariablesarealwaysinitializedatzeroatt =0. Inpractice,thedeterministic
variableswilltendtolearnthemeanofthetrainingsequencewhilethestochasticvariableswill
learnthedeviationsfromthemean.
Secondly,bottom-upconnectionsfromlowerlayerstohigherlayershavebeenremovedinorder
tosimplifythemodel. Predictionerrorfromlowerlayersisstillconveyedtohigherlayersduring
back-propagation. Additionally, connections between z and the output x have been removed.
Preliminarytestinghasnotshownanydegradationofplanningperformanceduetothischange.
Finally, connections between d and the posterior distribution zq have been removed. Thus
t
informationfromtheprevioustimestepflowsbetweenstochasticunitsonlybyhowclosetheprior
andposteriordistributionsare,whichisregulatedbythemeta-priorsetting. Whilethischange
couldimpactlearningperformance,itmakesinferenceoftheadaptationvariables Asimpler.
As noted previously, PV-RNN is a variational RNN comprised of deterministic variables d
andstochasticvariablesz. Themodelinfersanapproximateposteriordistributionqbytheprior
distribution pbymeansoferrorminimizationonthegeneratedoutputx. Theparameterizedprior
generativemodel p isfactorizedasshowninEquation1.
θ
5
Figure3: GraphicalrepresentationofPV-RNNasimplementedinthispaper
T
∏
p θ (x 1:T ,d 1:T ,z 1:T |d 0 ) = p θx (x t |d t )p θd (d t |d t−1 ,z t )p θz (z t |d t−1 ) (1)
t=1
NotethatunlikeintheoriginalimplementationofPV-RNN,xisnotconditioneddirectlyonz,
onlythroughd,whichisaDiracdeltafunctionasdefinedinEquation2.
(cid:40)
0 ift =0
d = (2)
t
f θd (d t−1 ,z t ) ift >0
f isaneuralnetwork—MTRNNisusedinthispaper. disthentheoutputoftheMTRNN,
θd
whichistheinternalstatehafteractivation. Foramulti-layerMTRNNinthismodel,hiscalculated
asasumofthevalueofthestochasticvariablez,theprevioustimestepoutputofthecurrentlevell
andprevioustimestepoutputofthenexthigherlevell+1asshowninEquation3.
dl =tanh(hl)
t t
hl = (cid:18) 1− 1 (cid:19) hl + 1 (cid:16) Wl,ldl +Wl,lzl +Wl+1,ldl+1 (cid:17) (3)
t τl t−1 τl d,d t−1 z,d t d,d t−1
W representconnectivityweightmatrices,inthiscasebetweenlayersandbetweendeterministic
andstochasticunits.
Notethatatthetoplayer,Wl+1,ldl+1
isomitted.
d,d t−1
6
Thepriordistribution pofzisaGaussiandistributionwhichdependsond t−1 ,exceptatt =1
whichdoesnotdependond andisfixedasaunitGaussian. µandσ forthepriordistributionare
0
obtainedfromdasshowninEquation4.
p(z ) = N(0,I)
1
p(z
t
|d
t−1
) = N(µ
t
p ,(σ
t
p)2)wheret >1
µ t p =tanh(W d l, , l z,µp d t−1 ) (4)
σ t p =exp(W d l, , l z,σp d t−1 )
BasedonthereparameterizationtrickproposedbyKingmaandWelling[26],thelatentvaluez
forbothpriorandposteriordistributionsisafunctionofµandσandanoisesample(cid:101) ∼ N(0,I).
z = µ +σ ×(cid:101) (5)
t t t
Sincecomputingthetrueposteriordistributionisintractable,themodelinfersanapproximate
posteriorqofzasdescribedinEquation6. InPV-RNN,whilesensoryinformationxisnotdirectly
availabletothenetwork,anadaptationvariable Aisused,soforeachtrainingsequencex there
1:T
isacorresponding A . Aislearnedtogetherwiththeothernetworkparametersduringtraining
1:T
basedonthepredictionerrorsebetweenxandx.
q(z |e ) = N(µ q ,(σ q)2)
t t:T t t
µ q =tanh(A µ) (6)
t t
σ q =exp(Aσ)
t t
2.2 Learningwithevidencelowerbound
Following from Equation 1, we can express the marginal likelihood (evidence) as shown in
Equation7.Asthevalueofdisdeterministic,ifweletd˜ bethevalueofd asdescribedbyEquation
t t
2,then p θd (d t |d t−1 ,z t )isequivalenttoaDiracdistributiongivenbyδ(d t −d˜ t ),whichallowsthe
integraloverdtobeeliminated.
(cid:90) (cid:90) T
∏
p θ (x 1:T |d 0 ) = p θx (x t |d t )p θd (d t |d t−1 ,z t )p θz (z t |d t−1 )dzdd
t=1
(cid:90) (cid:90) T
= ∏ p θx (x t |d˜ t )δ(d t −d˜ t )p θz (z t |d˜ t−1 )dzdd (7)
t=1
(cid:90) T
= ∏ p θx (x t |d˜ t )p θz (z t |d˜ t−1 )dz
t=1
Factoringtheintegral,takingthelogarithmandrefactoringwiththeparameterizedposterior
distributionproducesanexpectationontheposteriordistributionasshowninEquation8.
7
T (cid:90)
logp θ (x 1:T |d 0 ) =log ∏ p θx (x t |d˜ t )p θz (z t |d˜ t−1 )dz t
t=1
T (cid:90)
= ∑ log p θx (x t |d˜ t )p θz (z t |d˜ t−1 )dz t (8)
t=1
= ∑ T log (cid:90) p (x |d˜) p θz (z t |d˜ t−1 ) q (z |e )dz
θx t t q (z |e ) φ t t:T t
t=1 φ t t:T
Finally, by applying Jensen’s inequality logE[x] ≥ E[logx], the variational evidence lower
bound(ELBO)L(θ,φ)isgiveninEquation9.
(cid:34) (cid:35)
L(θ,φ) = ∑ T (cid:90) log p (x |d˜) p θz (z t |d˜ t−1 ) q (z |e )dz (9)
θx t t q (z |e ) φ t t:T t
t=1 φ t t:T
Followingtheconceptoffreeenergyminimization[14],ELBOisrewrittenintermsofexpected
loglikelihoodundertheposteriordistribution(accuracy)andtheKullback-Leiblerdivergence(KLD)
betweentheposteriorandpriordistributions(complexity)inEquation10. Thedeterministicvalue
intheexpectedloglikelihoodissubstitutedwithallpreviousstochasticvariablesbyEquation2in
ordertoallowoptimizationoftheposterioradaptivevaluesagainstthetrainingdata.Forsimplicity,
weomitthesummationovereachlayeroftheRNNandovereachtrainingsample.
T
L(θ,φ) = ∑ E qφ (zt |z1:t−1,et:T ) (cid:2) p θx (x t |d˜ t ) (cid:3) −D KL (cid:2) q φ (z t |e t:T )||p θz (z t |d˜ t−1 ) (cid:3)
t=1
∑ T (cid:2) (cid:3) (cid:2) (cid:3) (10)
= E qφ (zt |z1:t−1,et:T ) p θx (x t |z 1:t ) −D KL q φ (z t |e t:T )||p θz (z t |z 1:t−1 )
t=1 (cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125) Complexity
Accuracy
Intuitively,theaccuracytermiscalculatedbythedistancebetweenthepredictedoutputxand
thesensorystateorgroundtruthx. Inpractice,thisisastandardmeasuresuchasmeansquared
error(MSE)orKLD.
InPV-RNN,themeta-priorwisahyperparameterwhichaffectsthedegreeofregularization(or
thetendencytooverfit). ItissimilartotheβparameterinVAE[26]althoughtheeffectisreversed,
thatis,inmodelsthatassumeapriornormaldistribution,alargerregularizationconstantimplies
astrongerpulltowardthenormaldistribution,reducingcomplexityandreducingthetendency
tooverfit. However,asPV-RNNthepriorisconditionedontheoutputofprevioustimesteps,a
largermeta-priorcausesthecomplexitytoriseastheoutputbecomesdeterministic,resultingina
tendencytooverfittrainingsamples. Duringlearning,themeta-priorwillaffecttheapproximate
posteriordistributionandcauseittodeviatefromthetrueposterior,whileduringinferencethe
meta-priorwillcontrolhowmuchthepriorandapproximateposteriorwilldeviate. Weexplore
thiseffectinthefollowingSection3.
InthisimplementationofPV-RNN,thecomplexitytermatt =1isaspecialcasewheretheprior
distributionisaunitGaussianN(0,I),andtheinitialGaussianweightw controlshowclosely
I
theposteriorfollows. Thishastwoeffects—firstly,theRNNcanbemademoreorlesssensitiveto
theinitialstateatt =1byadjustingthedegreeofregularizationwithaunitGaussian. Secondly,
8
asitisindependentofthemeta-prior,itavoidsdegeneratecaseswherelearningofaprobabilistic
trainingsetisunsuccessfulduetothemeta-priorforcingdeterministicbehavior. Frompreliminary
testing,wefoundsettingsofeitherw = 0.01orw = 0.001appropriatedependingonthedata.
I I
Additionally, in this implementation of PV-RNN, we use different values of w per layer l. For
simplicity,summationovertimestepsisomittedinEquation11.

l ∑ = L 1 wl·D KL (cid:2) q φ (z t |e t:T )||p θz (z t |d˜ t−1 ) (cid:3) =   ∑ ∑ l l L L = = 1 1 w w l I ∑ ∑ σ σ , , µ µ ∈ ∈ z z l l o o g g σ σ σ t 1 q p t q , , , l l l + + ( ( σ − t p µ ,l q t − ,l 2 ) µ 2 ( 2 q t + σ ,l) p ( 2 , σ l + ) t q 2 , ( l) σ 2 t q, − l)2 1 2 − 1 2 i i f f t t = > 1 1
t t
(11)
In practice, all parameters are optimized by gradient descent, using the Adam optimizer
providedbyTensorFlow. WenotetheconditionsusedinourexperimentsinSection3.
2.3 PlangenerationwithGLeanandtheestimatedlowerbound
Plangenerationusesavariationoferrorregression[37]inordertoinferthelatentvariablesthat
minimizetheerror. However,recentworksthatutilizeerrorregression[1,6]employaregression
windowinwhicherrorisminimizedinordertoimprovefutureprediction(seeFigure4a). GLean
attempts to minimize the errors at the initial timestep and the goal timestep (see Figure 4b) by
maximizingtheestimatedlowerbound,showninEquation12.
(a) (b)
Figure4: Differenceinhowerrorregressionisemployedin(a)futuresequencepredictionand(b)
goal-directedplanning. Solidblacklinesrepresenttheforwardgenerativemodelwhilethedashed
∅
redlinesrepresentback-propagationthroughtimeusedtoupdate A .
ComparedtoELBOshowninEquation9,theaccuracytermisnowcalculatedasthesummation
ofpredictionerrorintheinitial(t =1)anddistal(t = T)steps. Inthiswork,weassumethatthe
distalstepisatafixedpointintime;inpractice,ifthegoalisreachedearly,theagentshouldremain
stationaryuntilthefinaltimestep. Thecomplexitytermisalsomodifiedsuchthat,exceptforthe
firsttimestep,theposteriordistributionisconditionedonlyonthepredictionerroratthegoal.
9
(cid:2) (cid:3) (cid:2) (cid:3)
L (θ,φ) = E p (x |z ) +E p (x |z )
e qφ (z1 |e1,eT ) θx 1 1 qφ (zT |eT ) θx T 1:T
(cid:16) (cid:2) (cid:3) ∑ T (cid:2) (cid:3)(cid:17)
− D KL q φ (z 1 |e 1 )||p θz (z 1 ) + D KL q φ (z t |e T )||p θz (z t |z 1:t−1 ) (12)
t=2
Notethatwhilethetrainedmodelisloadedbeforeplangeneration,theadaptivevariable Ais
∅ ∅
resettozero(denotedas A ). Duringplangeneration,onlytheadaptivevariable A isupdated,
whileallotherparametersremainfixed. Theimplementationofplangenerationislargelysimilar
to training, although for practical reasons the number of epochs is significantly reduced. The
learningrate,whichwerefertoasplanadaptationrateinthecontextofplangeneration,israisedto
∅
compensateforthis. Inaddition,noisesamplingisemployedbyhavingmultiple A sequences
andselectingforplanswiththehighestlowerbound.
3 Experiments
InordertotestGLean,weconductedtwoexperimentswithsimulatedagents. Thefirstexper-
imentwascarriedoutwithavirtualmobileagentina2Dspaceinordertoexaminetheimpact
ofthemeta-prioronlearningaswellasplangenerationoutputs. Thesecondexperimentuseda
simulated8DOFarmrobotcarryingoutagoal-directedobjectmovingtask,andcomparedGLean
totwopreviouslymentionedmodels—aforwardmodelandastochasticinitialstateRNN.
Duetothecomputationalworkloadofgeneratinglongsequences,particularlywhenexecuting
errorregressionforplangeneration,allplansweregeneratedinanofflinemanner. Thisallowedthe
worktoberuninbatchesonacomputercluster. Similarly,usingasimulatortocollectdataandtest
theoutcomesallowedgreaterefficiencyandautomationcomparedtousingrealrobots. However,
inthefuture,weplantoextendthisworktoreal-timetrajectoryplanningusingaphysicalrobot.
Asmentionedpreviously,weimplementedPV-RNNandGLeanusingTensorFlow. TheAdam
optimizerwasusedwithdefaultparameters,exceptforlearningrateandεˆwhichwassetto1/10
oflearningrate. Additionallyweusedrandomdropoutoftheerrorsignal(i.e. thepredictionerror
e caneitherbex−xor0).
t
ThesourcecodeforGLeanispubliclyavailableathttps://github.com/oist-cnru/GLeanfor
bothPython2.7+TensorFlow1.x(astestedinthispaper)andPython3.x+TensorFlow2.x. The
testeddatasetsarealsoincluded,togetherwithinstructionsonhowtousethesoftware.
Forthesetwosimulationexperiments,weprepareddatasetsofpossibletrajectorieswherein
aportionofthetrajectorieswereusedfortrainingofthemodelandtheremainingheldbackfor
testing. Thisprovidesthegroundtruthundervariousconditionsincludingnon-goal-directedand
goal-directedgeneration. Toevaluatetheperformanceoftrajectorygenerationafterthetraining,we
providebothplotsoftrajectoriesforqualitativeevaluationaswellastablesofqualitativemeasures.
Forgoal-directedplangeneration,wejudgethequalityofthegeneratedoutputsbycomparing
thetrajectorytothegroundtruthtrajectoryandcalculatinganaveragerootmeansquarederror
(RMSE)value. Theerroratthefinaltimestepisalsogivenseparatelyasgoaldeviation(GD).The
averageKLDbetweenpriorandposterior(KLD )isstatedasanindicationofhowcloselythe
pq
networkisfollowingitstrainedpriordistribution. Notethisisequivalenttothecomplexityterm
withoutweightingbythemeta-prior.
10
3.1 Experiment1: simulatedmobileagentina2Dspace
Togainabetterunderstandingofthegenerativecapabilitiesofourmodel, wefirstconduct
anexperimentusingasimplesimulatedagentmovingina2DspaceasshowninFigure5. The
agent’spositionatagiventimestepisgivenbyXYcoordinatesintherange[0,1]. Thetrainingdata
consistsofhanddrawntrajectoriesresampledto30timesteps,startingat[0,0],movingtoacentral
‘branchpoint’atapproximately(0.38,0.42),andthenproceedingwitha50/50chancetooneoftwo
goalareas—thetopleftcenteredaround(0.2,0.8)andthebottomrightcenteredaround(0.85,0.3)
without colliding with obstacles shown as grey areas in Figure 5a, ideally while maintaining a
smoothtrajectory.
Thebranchpointisreachedatapproximatelyt =10,withthegoalreachedbetweent =20and
t =30. Asthetrajectoriesarehanddrawnwithamouse,thereisavaryingamountofnoiseineach
trajectoryandthegoalpointsarealsodistributedinafairlyuniformdistribution. Theresultisthat
whilethetaskitselfissimple(goingfromstarttooneofgoalareas),ahabituatedpathofsortsis
generatedoutofthebundleoftrajectoriesdrawn.
(a) (b) (c)
Figure5: Plotsofthetrajectoriespreparedforamobileagentgeneratinggoal-directedbehaviors
in2Dspace. (a)XYplotshowingtheinitialpositionoftheagent,thebranchpoint,andthetwo
goalareas,(b)theplotoftheXpositionovertime,and(c)theplotoftheYpositionovertime. The
branchpointisvisibleataroundt =10.
Asnotedpreviously,weusePV-RNNwhichitselfisbuiltonMTRNN.Inthisexperiment,we
configurethenetworktohavetwolayers(notethatlayer1isthebottomlayer)withparametersas
showninTable1. Neuronsrefertothenumberofdeterministicvariables,whileZ-unitsreferto
thenumberofstochasticvariables. Thesearekeptina10:1ratioasin[1]. τistheMTRNNtime
constant,withshortertimeconstantsusedinthelowerlayerswhichshouldbemoreresponsive,
andlongertimeconstantsinthehigherlayers. Thenetworkwastrainedfor50,000epochswitha
learningrateof0.001.
Inordertoexplorehowthemeta-prioraffectstheoutputintermsoftrajectorygenerationafter
learning,wepreparedthreenetworkstrainedwithdifferentmeta-priorvaluesthatwehavelabeled
‘weak’,‘intermediate’and‘strong’asshowninTable2.
3.1.1 Priorgeneration
Toevaluatetheabilityofthenetworktolearntoextracttheprobabilisticcharacteristicslatentin
thetrainingdata,wetestpriorgenerationoftrajectoriesusingthepriordistributionasillustrated
11
Table1: PV-RNNparametersforExperiment1
MTRNNlayer
1 2
. Neurons|dl| 20 10
Z-units|zl| 2 1
τ 4 8
Table2: Meta-priorsettingsforthe2Dexperiment
MTRNNlayer
Meta-priorsettingw 1 2
Weak 0.00001 0.000005
Intermediate 0.01 0.005
Strong 0.2 0.1
in Figure 6. Since there are no target outputs given and z is a unit Gaussian and d = 0, the
1 0
networkisnotinfluencedtogotoaparticulargoaldirection. Ideally,thedistributionoftrajectories
generatedinthismannershouldmatchthetrainingdataintermsofdistributionbetweenleftand
rightgoalareasasthepriorgenerationshouldrepresentthedistributionofhabituatedtrajectories.
Figure6: Generationusingastochasticinitialstate(unitGaussian)
Table3showsthedistributionsbetweenleftandrightgoalareasforthethreenetworkstrained
withdifferentmeta-priorsincomparisontothetrainingdata(groundtruth). Weobservedthata
weakermeta-priortendedtoallowslightlymoreskewinonedirection,howeverbyinspectingthe
plotsinFigure7,itisapparentthatthereisalargeamountofnoiseinthetrajectoriesgeneratedby
theweakmeta-priornetwork(Figure7b). Inparticular,therearelargedeviationsandtheoverall
shapeofthetrajectoriesdoesnotfollowthetrainingdataaccurately.
12
Table 3: Distribution of goals reached by networks with different meta-priors, after 60 prior
generationsequences
Trainingmeta-prior Leftgoal% Rightgoal%
Weak 38.3 61.7
Intermediate 46.7 53.3
Strong 55.0 45.0
Groundtruth 50.0 50.0
Incontrast,withalargemeta-prior(Figure7d),thereappearstohavebeenafailuretolearnthe
spreadofgoals,particularlyintherightgoalarea,resultinginsomeunexpectedtrajectories. Inthis
test,anintermediatemeta-priorwasbestatlearningtheprobabilisticstructureofthetrainingdata.
(a)
(b) (c) (d)
Figure7: Trajectoryplotsshowing(a)thetrainingdata(groundtruth),(b)priorgenerationwitha
weakmeta-prior,(c)withanintermediatemeta-prior,and(d)withastrongmeta-prior. Eachplot
contains60trajectories.
3.1.2 Targetregeneration
Asdescribedpreviously,PV-RNNlearnstheprobabilisticstructureoftrajectoriesbyembedding
ineitherinitialstatesensitivedeterministicdynamicsorstochasticdynamicsbasedonthemeta-
priorvalue.Thissuggeststhattrajectoriesforgoal-directedbehaviorswithmultiplegoals,including
13
decisionbranchingpoints,canbegeneratedeitherinaninitialstatesensitivemannerbasedon
deterministicdynamicsorinanoise-drivenmannerbasedonstochasticdynamics.
Forthepurposeofexaminingsuchpropertiesofthetrainednetworks,weconductatestfor
targetregenerationofthetrainedtrajectoriesinamannersimilartothatoriginallyusedin[1]. In
thistest,weattempttoregenerateaparticulartargetsequencefromthetrainingdatasetbyusing
theinformationofthelatentstateintheinitialstep. Thisinformationwasaresultofthetraining
process.
More specifically, as illustrated in Figure 8, the prior generation is computed but with the
posterioradaptationvariableat

Rewrite the summary fixing the issues. Use exact title: Goal-Directed Planning for Habituated Agents by Active Inference Using a Variational Recurrent Neural Network
PROFESSIONAL TONE: Begin directly with content - NO conversational openings.
NO REPETITION. Each sentence must be unique. Vary attribution phrases.
Extract quotes VERBATIM from paper text - do NOT modify or "correct" them.