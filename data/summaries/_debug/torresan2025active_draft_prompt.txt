=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Active inference for action-unaware agents
Citation Key: torresan2025active
Authors: Filippo Torresan, Keisuke Suzuki, Ryota Kanai

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Key Terms: energy, action, free, agents, expected, actions, japan, inference, active, unaware

=== FULL PAPER TEXT ===

Active inference for action-unaware agents
FilippoTorresan1,2, KeisukeSuzuki3, RyotaKanai1, ManuelBaltieri1,2,†
1ArayaInc.,Tokyo,Japan
2SchoolofEngineeringandInformatics,UniversityofSussex,Brighton,UK
3CenterforHumanNature,ArtificialIntelligenceandNeuroscience(CHAIN),HokkaidoUniversity,
Sapporo,Japan
Activeinferenceisaformalapproachtostudycognitionbasedonthenotionthatadaptiveagents
can be seen as engaging in a process of approximate Bayesian inference, via the minimisation
of variational and expected free energies. Minimising the former provides an account of percep-
tualprocessesandlearningasevidenceaccumulation,whileminimisingthelatterdescribeshow
agents select their actions over time. In this way, adaptive agents are able to maximise the like-
lihood of preferred observations or states, given a generative model of the environment. In the
literature,however,differentstrategieshavebeenproposedtodescribehowagentscanplantheir
future actions. While they all share the notion that some kind of expected free energy offers an
appropriatewaytoscorepolicies,sequencesofactions,intermsoftheirdesirability,therearedif-
ferentwaystoconsiderthecontributionofpastmotorexperiencetotheagent’sfuturebehaviour.
In some approaches, agents are assumed to know their own actions, and use such knowledge to
betterplanforthefuture. Inotherapproaches,agentsareunawareoftheiractions,andmustinfer
their motor behaviour from recent observations in order to plan for the future. This difference
reflects a standard point of departure in two leading frameworks in motor control based on the
presence, or not, of an efference copy signal representing knowledge about an agent’s own ac-
tions. In this work we compare the performances of action-aware and action-unaware agents in
twonavigationstasks,showinghowaction-unawareagentscanachieveperformancescomparable
toaction-awareoneswhileataseveredisadvantage.
Keywords: active inference, Bayesian inference, POMDP, variational free energy, expected free
energy
1. Introduction
Active inference is a framework originally developed in cognitive science and theoretical neuro-
science to account for the function(s) of adaptive agents and their nervous systems [22, 24, 33, 59,
62]. Different mathematical formulations of its core ideas have been proposed, and have been used
to formally account for the adaptive behaviour of agents in different domains, as well as to model
neuralandbehaviouraldataincomputationalcognitiveneuroscience[1,10,16,34,26,30,35,38,52,
53, 56, 61, 63, 64, 67]. The framework has also received a lot of attention in philosophy of mind and
cognitive science, with its key insights popularised under the banners of predictive processing and
predictionerrorminimisation[12,14,39,40,72].
Themainideadrivingactiveinferenceisthatinformationprocessinginthebraincanbeexplained
bypredictiveactivitythatapproximatesaprocessofhierarchicaldynamicBayesianinferenceonthe
†Correspondencee-mail:manuel baltieri@araya.org
5202
guA
61
]IA.sc[
1v72021.8052:viXra
Activeinferenceforaction-unawareagents
hiddenstatesoftheenvironmentthatproducesensoryinputsfortheagent[46,23,33]. Onthisview,
the dynamics of brain states implement approximate Bayesian inference updates consistent with
(the dynamics of) an implicit generative model of, i.e., a joint probability distribution over, sensory
signals(observations),motorcommands(actions),andinternalconfigurations(states). Inturn,these
updates allow an agent to infer its current predicament (perception), to infer the best sequence of
actions (policies) to reach favourable states (planning/goal-directed decision making), and to learn
whatispossibleinitseco-niche(learningrelevantsensorimotorcontingencies)[12,13,63,11,42,9].
Activeinference,itsdifferentimplementationsandensuingapplicationshavebeenpresentedand
reviewedextensivelyintheliterature. Forinstance,Buckleyetal.[10]providesareviewofactivein-
ferenceforcontinuous-timestate-spacemodelswhereasDaCostaetal.[16]offerasynthesisofactive
inference based on the discrete-time framework of partially observable Markov decision processes
(POMDPs). Themaindifferencebetweenthetwoformulationsrevolvesaroundthetechnicalitiesre-
quiredtoimplementa(variational)Bayesianinferenceschemeaccordingtothedynamicalevolution
ofrelevantquantities,occurringeitherincontinuoustimeoratdiscretetimesteps.
More recently, Smith, Friston, and Whyte [68] presents a more beginner-friendly, yet technical
tutorialintroductiontothediscrete-timeformulation,withaspecialfocusonempiricalapplications,
i.e.,howtofitactiveinferencemodelstobehaviouralandneuraldata. Therecentimplementationsof
activeinferenceinPython[37]andinJulia[55],togetherwiththeircompanionpapersandtutorials,
representanotherexcellententrypointandcouldbereadalongside[16]foradeeperunderstanding
of how the mathematical aspects of the theory have been implemented. Additionally, Lanillos et al.
[44] provides a survey of the approach with a special interest in robotics applications (especially in-
volving the continuous-time formulation) whereas Mazzaglia et al. [48] offers a similar survey but
examiningmoreindetailtheconnectionswithrelateddeeplearningapproaches. Otherworks,such
asGottwaldandBraun[36],provideanenlighteningmathematicalexplanationoffree-energymini-
mization, comparingthemainversionsoftheactiveinferencemachinery(thosethathaveappeared
up to 2020), and also makes a comparison with other Bayesian approaches to adaptive decision-
making such as control-as-inference [43, 71, 70, 47]. On the other hand, Millidge, Seth, and Buckley
[49]providesanintroductiontothemorefoundationalnotionofthefree-energyprinciple(i.e.,tying
free-energyminimizationtoself-organizationincertaindynamicalsystems),fromwhichatheoryof
sentientbehaviourlikeactiveinferencecanbeseentoemerge[see,also,27],whileParr,Pezzulo,and
Friston[59]bringeverythingtogetherinathoroughandaccessibletreatmentoftheapproachandits
applications.
Inspired by these and other relevant works in the area, in Sections 2 and 3 we provide a self-
contained introduction to the standard active inference framework, including in Sections S1 and S2
further details and derivations of the active inference equations for perception, action selection and
learning(ofbothtransitiondynamicsandemissionmaps),withabreakdownofsomeitsmoreunder-
exploredaspects. Ourgoalhereistoinvestigatesomeassumptionsthathaveappearedinpartsofthe
active inference literature, and their implications for the study of adaptive behaviour. In particular,
wewillfocusoncomparingtwoimplementationsforclassesofagentsweshalldefineasaction-aware,
inspiredbythecontrol-as-inferenceliterature[70,43,47],andaction-unaware,morecloselyrelatedto
classicalactiveinferenceformulationsthatdrawfromworkontheequilibriumpointhypothesisand
referentcontrol[19,20]toarguethatclassesofbiologicalagentsincludinghumansdonowhave,or
even need, access to explicit information about their motor signals [34, 28, 25, 1, 4, 5, 6]. Agents of
thefirstkindknowpreciselywhatactionstheytookinthepastandonlyneedtoplanforthefuture,
while the latter don’t, and thus have to infer sequences of actions that best fit their past, accounting
for their observations up to the present, as a pre-condition for inferring what is best to do in the
2
Activeinferenceforaction-unawareagents
future. Thismeansthataction-awareagentscanmakeuseofmoreknowledge,astheydon’tneedto
inferwhatactionstheytookinthepast.
We will highlight the main difference between these two strategies, related to how the agent’s
policies are conceived and used in perceptual inference and planning to infer relevant information
fromobservationsandevaluate/selectfutureactions. Action-unawareagentsbuildonthestandard
treatment presented in [16, 59], providing the bedrock of a computational and algorithmic frame-
work in which agents that are unaware of their own actions (executed in the past), are required to
infer (among other things) the most likely policy currently followed up until the present from evi-
dencerepresentedbypastobservations,andtodecidesubsequentlywhethertocontinueperforming
the same policy in the future. On the other hand, more recent proposals [37, 31] adopt a different,
action-awareapproachonpoliciesbyviewingthemassequencesofactionsinthefuture,sinceagents
knowexactlywhatactionstheytooksofar(cf.,efferencecopy[15]). Whilethelatterhasbecomethe
mostcommonapproachtosimulateactiveinferenceagentsindiscretesettings,aclearexperimental
comparisonbetweenthetwoisstillmissing.
WeprovidethusaPythonimplementationofthesetwovariationsofactiveinference,andunpack
resultsfromsimulationsthatcomparethesetwotreatments,showingadetailedbreakdownofwhat
and how agents learn in simple navigation tasks, shedding light on the extent to which an agent’s
awarenessofitspastmotortrajectoryhasanimpactonitslearningandadaptivebehaviour.
InSection2westartwithabriefoverviewofhowtheagent-environmentinteractionisformally
modelledinarigorousmannerwithintheactiveinferenceframework. Then,weexplainindetailthe
optimisationproblemthatanactiveinferenceagentisdesignedtosolve(Section3),andthevarious
components of the active inference algorithms that go into solving that problem. With two exper-
iments, we illustrate the typical learning trajectories of action-unaware and action-aware agents in
a simple grid-world environment (Section 4). We will conclude with a discussion of a few general
pointsaboutactiveinferenceaswellasafewmorespecificonesrelatedtothefindingsoftheexperi-
ments(Section5).
2. Formalising the Agent-Environment Interaction
Activeinferenceproposesaformalapproachtocharacterisecognitionandadaptivebehaviourstart-
ingfromafewbasicpremises:
1. biological and artificial agents can persist in a complex and ever changing world if and only if
theykeepsensorysignalwithincertainviableranges,basedonthedefinitionofasetofpreferred
states(orobservations),
2. anagent’sinternalstatesparametriseanimplicitgenerativemodelofthesurroundingenviron-
ment,
3. all the processes that constitute an agent, from perception to action, can be described as con-
tributing to the minimization of a single quantity, i.e., variational free energy, for a particular
classofpreferredstatesandagivengenerativemodel.
More formally, in the discrete state-space formulation of active inference, these intuitions are
translatedintothelanguageofdiscrete-timepartiallyobservableMarkovdecisionprocesses(POMDPs),
which are used to describe mathematically both the relevant parts of the environment (the genera-
tive process) and an active inference agent interacting with it (whose dynamics encode parameters’
updatesconsistentwithprobabilisticbeliefsofanimplicitgenerativemodel). Thecharacterisationof
3
Activeinferenceforaction-unawareagents
theagentalsorequiresthespecificationofaprobabilitydistributionoverpreferredstatesorobserva-
tions,therebyconstrainingitsbehaviourtobegoal-directed.
Definition2.1(POMDPinactiveinference,thegenerativeprocess). APOMDPisasix-elementtuple,
(S,O,A,T ,E,T),where:
• S isafinitesetofstates,
• O isafinitesetofobservations,
• Aisafinitesetofadmissibleactions,
• S,O,A ,withi ∈ [1,T],aretime-indexedrandomvariablesdefinedovertherespectivespaces,
i i i
wherethetimeindex T representsaterminaltimestep,
• T : S ×A → ∆(S)isatransitionfunctionthatmapsstate-actionpairstoaprobabilitydistribu-
tionintheset
∆(S)ofprobabilitydistributiondefinedoverS
• E : S → ∆(O) is an emission function that maps a state to a probability distribution in the set
∆(O)ofprobabilitydistributiondefinedoverO.1
Thetransitionandemissionfunctionsmapstate-actionpairsandstatestoconditionalprobability
distributions that will be denoted by P(·|s ,a ) and P(·|s ), respectively. These distributions define
t t t
thedynamicsofthePOMDPwhere,∀t ∈ [1,T],stateandobservationrandomvariablesaresampled,
S t+1 ∼ P(·|s t ,a t ) and O t ∼ P(·|s t ). In particular, the former can be used to specify the probability
thatthestaterandomvariableatt+1takesonacertainvalue, P(S
t+1
= s
t+1
|s
t
,a
t
),givenparticular
valuesofstateandactionrandomvariablesattheprevioustimestep. Thelattercaninsteadbeused
tospecifytheprobabilitythattheobservationrandomvariableattimestepttakesonacertainvalue,
P(O = o |s ),givenaparticularvalueofthestaterandomvariableatthecurrenttimestep.
t t t
We assume that S t+1 ∼ P(·|s t ,a t ) and O t ∼ P(·|s t ) correspond to categorical (i.e., discrete) ran-
domvariablestakingonavaluefromafiniteset,i.e.,thestatespaceS,withacertainprobability. In
active inference, the categorical distributions P(·|s t ,a t ) and P(·|s t ) are often indicated by Cat(s t+1 )
andCat(o
t
),wheres
t+1
∈ R|S| ando
t
∈ R|O| arevectorsofparametersoflength|S|and|O|,respec-
tively, and |·| indicates the cardinality of a set (these probability distributions assign a probability
to every state/observation in the respective spaces). Also, it is worth highlighting that we use the
words‘state’and‘observation’toindicatespecificallythevaluesofthecorrespondingrandomvari-
ables, i.e., the elements of the respective spaces, and not the random variable themselves; for the
latterweuse‘staterandomvariable’and‘observationrandomvariable’.
Given this formal setup, an active inference agent selects a sequence of actions π ∈ Π that can
giveaccesstooneormoredesiredstatesorobservations. Thisisusuallycapturedbypostulatingthat
theagenthasgoalsintheformofpreferredstatesorobservations,formalisedastheconcentrationof
probabilitymassonasubsetofthesupportforaprobabilitydistributionP∗(S)definedoverS orfor
P∗(O) defined over O (cf., first premise at the outset of this section). More precisely, by selecting an
appropriate sequence of action, the agent is trying to make the POMDP evolve or update in such a
way that P∗(S) or P∗(O) will be the “final” probability distribution over states or observations, i.e.,
thestationarydistributionoftheMarkovdecisionprocessinquestion.
1WenotealsothatstandarddefinitionsofPOMDPs[65,Ch.16,54,Ch.34,69,Ch.17]includealsoanotionofrewardfor
anagent,herewedon’thoweverincludethemsinceactiveinferencespecifiestargetsforanagentinadifferentway,see
Definition2.2.Formally,however,thiscanbeeasilyaccommodatedintheabovedefinitionbystatingthatourobservations
OincludebothobservationsY andrewardsRofstandardPOMDPdefinitions:O=Y×R.
4
Activeinferenceforaction-unawareagents
In general, however, an agent starts with no knowledge about the POMDP dynamics and emis-
sionmaps,i.e.,ofhowthenextstateandobservationrelatetothecurrentstateandaction(specified
by the two conditional probability distributions introduced above). Therefore, an active inference
agent’staskcorrespondstothechallengeofusingobservationsfromanenvironment(describedfor-
mallybythePOMDPofDefinition2.1)tolearntheparametersofanapproximatemodelthatcaptures
boththeenvironment’stransitiondynamicsof(hidden)statesandhowsuchstatesmaptogivenob-
servations. This is usually called a generative model because it allows the agent to predict or generate
the most likely next state given a state-action pair and the most likely observation resulting from
beinginthatstate(cf.,secondpremise). Theagentcanrelyonthesepredictivecapabilitiestoimple-
ment a decision-making strategy to pick an action, or a sequence of actions, that allow it to obtain a
preferredstateorobservation.
Therefore,anactiveinferenceagentcanbedefinedintermsofthefollowingcomponents:
Definition 2.2 (Active inference agent). An active inference agent is described by a five-element
tuple,(P∗(·), Π ,X,d,M),where:
• P∗(·)isthepreferredprobabilitydistributionoverstatesS orobservationsO,
• Π isasubsetofallpossiblesequencesofactions,orpolicies,oflengthH,i.e., Π ⊆ AH,whereAH
indicatesthe H-foldCartesianproductA×A×···×A = {(a ,...,a )|a ∈ A,∀i ∈ [1,H]},
1 H i
• X is either the state space S, the observation space O, the policy space Π , or possibly others,
usedasthedomainofthedecisionrulenext,
• d : X → A is a decision rule that outputs an action a ∈ A given a certain element of the space
X,
• Misagenerativemodelthatapproximatesthedynamicsoftheenvironment,andcomesinthe
formofaPOMDPgiveninDefinition2.1,
• Q(·) is the variational distribution that approximates components of the generative model (see
Sections3.2and3.3).
In the next few sections, we will spell out in some detail what the generative model M and
the distribution Q(·) involve, and what role they play in an active inference agent. We will show
that,fromacollectionofenvironmentalobservations,itispossibletocharacteriseperception,action,
andlearningofanembodiedactiveinferenceagentasparticularcomputationaloperationswiththe
generativemodelandthevariationaldistributiontominimiseasingleobjective,i.e.,variationalfree
energy(cf.,thirdpremise). Bydoingso,anactiveinferenceagentisabletobringaboutitspreferred
probabilitydistributionoverstates/observations.
3. Sequential Decision-Making with Approximate Bayesian Inference
3.1. TheGenerativeModel
As explained in the previous section, an active inference agent interacts with an environment, de-
scribed in terms of a POMDP, to move towards a preferred set of states (and/or corresponding ob-
servations),asencodedbytheprobabilitydistributionP∗(S)(orP∗(O)). Todoso,theagentcanonly
rely on observations received from the environment and its current generative model, M. This can
beconsideredasamoreorlessaccurate“replica”ofthePOMDPdescribingtheenvironment[16,59]
andisdefinedasfollows:
5
Activeinferenceforaction-unawareagents
Definition3.1(Generativemodelinactiveinference). ThegenerativemodelMofanactiveinference
agentisaPOMDPinthesenseofDefinition2.1. Wespecifyitinmoredetailusingajointprobability
distributionoverasequenceofstateandobservationrandomvariables,apolicyrandomvariablefor
sequencesofactions,andparametersstoredinmatrixA(fortheemissionmap)andtensorB(forthe
transitionmap),thatis,ajointthatfactorsas:
T T
∏ ∏
P(O
1:T
,S
1:T
,π,A,B) = P(π)P(A)P(B)P(S
1
) P(S
t
|S
t−1
,π,B) P(O
t
|S
t−1
,A). (1)
t=2 t=1
The matrix A and the tensor B (one matrix per action) store the parameters for the transition
and emission probabilities. Specifically, A ∈ Rn×m encodes the probabilities of state-observation
mappingsateverysingletimestep. Theseconddimensionofthematrix(numberofcolumns), m,is
the number of possible realisations (or values) s ∈ S of every state random variable S , ∀t ∈ [1,T].
t t
Theindexofeachcolumn canbethoughtofaspickingoneof theserealisations, i.e., oneamongthe
state values s1,...,sm. The first dimension of the matrix, n, is the number of possible realizations of
an observation random variable O , ∀t ∈ [1,T]. Similarly, the index of each row picks one of those
t
realisations,i.e.,oneamongtheobservationvalueso1,...,on. Thus,the jthcolumnofA,represented
byA ,storestheparameterso ofthecategoricaldistributionfollowedbyO conditionedonS = sj,
:,j t t t
i.e.,O ∼ P(·|sj;o )or,equivalently,O ∼ Cat(o |sj)(whereinbothexpressionswemadeexplicitthe
t t t t
conditioningvalueofS andtheparameters).
t
The tensor B ∈ R|A|×m×m stores all the state-transitions probabilities, depending on the action
underconsideration(indicatedbythevalueofthetensor’sfirstdimension). Specifically,thematrices
Ba 1,...,Bad, with d = |A| specify the most likely distributions over states conditioned on a specific
state value and the execution of a specific action (indicated by the superscript). For each matrix,
the row and column dimensions represent the number of possible realisations of a state random
variableS ,againmeaningthateachcolumnandrowindexidentifiesastatevalueamongs1,...,sm.
t
Thus, the jth column of a matrix Bx, represented by Bx , stores the parameters s of the categorical
:,j t
distribution followed by S t conditioned on S t−1 = sj, i.e., S t ∼ P(·|s t j −1 ,x;s t−1 ) or, equivalently,
S ∼ Cat(s |s j ,x). Inbothexpressionswemadeexplicitthatweareconditioningonavaluejofthe
t t t−1
staterandomvariableatt−1,i.e.,s j ,andonanaction x ∈ [a ,...,a ] = A.
t−1 1 d
Note that each column of A and Bx can be seen as an output of an approximation (learned by
the active inference agent) of the emission map E and the transition map T , respectively, given a
certain input value sj for the former and a certain state-action input pair sj,x for the latter. Both
maps are assumed to be time-independent: the probability that sj will produce a certain observation
and the probability that sj will lead to a certain state does not change depending on the particular
time step indexing the state random variable S . Also, note that s and o stand for one among the
t t t
valuess1,...,sm ando1,...,on,respectively,andwewillusethenotationwithoutsuperscripttorefer
generically to one of the values of S and O , when it is superfluous to indicate explicitly that we
t t
are working with state and observation values that correspond to particular columns/rows of the
matricesjustdescribed.
3.2. BayesianInference
Thegenerativemodelprovidesthebasisforthefollowingoperations:
6
Activeinferenceforaction-unawareagents
1. determining the most likely past and/or future states, s := s ,...,s , given a sequence of
1:T 1 T
observationsuptothepresenttimestep t, o := o ,...,o , with t = T whenweconsideronly
1:T 1 t
paststates(e.g.,attheendofanepisodeortrajectory)
2. predicting the most likely next states following the execution of certain actions and given the
mostprobablecurrentstate,
3. determiningthemostappropriatenextaction,
4. updatingkeyparameterstoreflectmorecloselytheactualPOMDPdescribingtheenvironment,
especiallywhenstep1–3alonedonotallowtheagenttoreachitsgoal.
From a computational point of view, these four steps characterise the cognitive life of an active
inferenceagent. Thefirstoneisusuallycalledperceptualinference,thesecondoneamountstoplanning
or policy inference, the third one corresponds to the decision-making or action-selection stage, and the
lastcorrespondstothelearningphase.
The ultimate goal of the agent is to perform actions that result in desired observations and/or
environmental states. Observations represent evidence or feedback from the environment for the
agentthatindicatewhetherthegenerativemodelcapturestheenvironmentaldynamicswellenough
to yield accurate predictions and goal-conducive actions. If not, that evidence can be used to up-
date the generative model to reflect more precisely what would happen in a certain environment.
Moreprecisely, itisformallypostulatedthatanagentistryingtosolveaninferenceproblem, corre-
spondingtoinferenceofthemostlikely(1)hiddenstatesgeneratinganobservation,inferenceofthe
mostlikely(2)policyand(3)actiongivensomepreferredstates, andinferenceofthemostlikely(4)
parametersofthegenerativemodeltomakemoreaccuratepredictionintheenvironment.
A principled way of performing inference involves Bayes’ rule, which in the POMDP setting
underconsiderationcanbespelledoutasfollows:
P(O |S ,π,A,B)P(S ,π,A,B)
P(S ,π,A,B|O ) = 1:T 1:T 1:T , (2)
1:T 1:T P(O )
1:T
where the generative models M appears in the numerator, factorised as the product between
twoprobabilitydistributions: (1)theprobabilityofasequenceofobservations, conditionedonase-
quence of states, the policy random variable, and certain parameters (explained below), and (2) the
(prior) probability of the sequence of states, the policy random variable, and the same parameters.
Importantly, the inference problem represented by Bayes’ rule in Eq. (2) involves probability distri-
butions over the parameters ofother probability distributions. To see this, we can factorise the prior
probabilitydistribution P(S ,π,A,B)asfollows:
1:T
P(S ,π,A,B) = P(S )P(π)P(A)P(Ba 1)···P(Bad), (3)
1:T 1:T
toshowexplicitlythatitinvolvesjointprobabilitydistributionsoverthe(vectorsofthe)matrices
A and Ba 1,...,Bad (where d is the number of actions), implying that Bayesian inference will update
the parameters of those distributions as well. In fact, each column of the above matrices should be
viewed as a random vector following a Dirichlet probability distribution. A realization of one of
these random vectors forms the set of parameters for another distribution, i.e., one of the categorical
distributionsthatspecifythestate-observationmappingortheaction-dependentstatetransitions.
Formally, P(A) is a more compact way of writing the joint over random vectors represented by
the columns A ∀i ∈ [1,m] of the matrix, that is: P(A) := P(A ,...,A ) = P(A )···P(A ),
:,i :,1 :,m :,1 :,m
with A ∼ P(A ), ∀i ∈ [1,m]. Further, the latter is defined as a Dirichlet probability distribution,
:,i :,i
7
Activeinferenceforaction-unawareagents
P(A ) := Dir(α ),whereα isacolumnvector(ofthesamelengthasA )storingitsparameters(note
:,i i i :,i
thattheseshouldbekeptdistinctfromtheelementsof A whichareparametersofcategoricaldistri-
butions instead or, when doing Bayesian inference, are seen as random vectors, whose realizations
determine the categorical parameters). The same analysis applies for the matrices Ba 1,...,Bad. In a
nutshell, Bayesian inference consists in updating the Dirichlet parameters α and β for each matrix
i i
above,sothatnewcategoricalparameterscanbesampledfromtheDirichletdistributions,replacing
theexistingelementsoftheobservationmappingandstatetransitionmatrices.
By means of a generative model specified as above and a sequence of observations o , Bayes’
1:T
ruleinEq.(2)allowsonetoderiveanapproximateposteriordistributionoverthestaterandomvari-
ables, the policy random variable, and the model’s parameters, i.e., the probabilities stored in A,B.
Deriving this posterior distribution is the inference problem the active inference agent has to solve.
Ultimately, this amounts to an update of the probabilistic beliefs encoded by the generative model
following the acquisition of observational evidence. However, since finding an analytic solution to
Eq.(2)isoftenintractable,activeinferenceproposestoimplementanapproximateBayesianinference
schemerevolvingaroundtheminimisationofvariationalfreeenergy. Thisquantityisdefinedinre-
lation to a given generative model, so in this case it can be written as follows (see also Section S1.4
forastandardderivation):
(cid:104) (cid:105)
F (cid:2) Q(S ,π,A,B) (cid:3) := E logQ(S ,π,A,B)−logP(O ,S ,π,A,B) , (4)
1:T Q 1:T 1:T 1:T
where Q(S ,π,A,B) is known as the variational posterior, a probability distribution introduced
1:T
to approximate the posterior distribution, P(S ,π,A,B|O ), in Eq. (2) (the outcome of Bayesian
1:T 1:T
inference).
3.3. OptimizationoftheFreeEnergyObjective
To minimize the free energy defined in Eq. (4), we make some assumptions about the variational
posterior so that the optimization procedure described above becomes more tractable. If we simply
assumed that Q(S ,π,A,B) had exactly the same form as P(S ,π,A,B|O ), making the varia-
1:T 1:T 1:T
tional posterior a replica of the actual posterior, one would incur again in issues of computational
intractability, similarly to the original problem of determining an analytic solution to Eq. (2). In
discrete-time active inference, it is thus common to adopt a mean-field approximation [16], meaning
thatthevariationalposteriorisfactorisedasfollows:
T
∏
Q(S ,π,A,B) = Q(A)Q(B)Q(π) Q(S |π). (5)
1:T t
t=1
By substituting this expression in Eq. (4) for the variational posterior, and by considering the
factorizationofthegenerativemodel,wecanrewritethefreeenergyasfollows(cf.,[16]):
(cid:16) (cid:13) (cid:17) (cid:16) (cid:13) (cid:17) (cid:16) (cid:13) (cid:17)
F (cid:2) Q(S ,π,A,B) (cid:3) =D Q(A) (cid:13) P(A) +D Q(B) (cid:13) P(B) +D Q(π) (cid:13) P(π)
1:T KL (cid:13) KL (cid:13) KL (cid:13)
(cid:34)
T (cid:104) (cid:105) τ (cid:104) (cid:105)
+E ∑E logQ(S |π ) − ∑E logP(o |S ,A)
Q(πk ) Q(St |πk ) t k Q(St |πk )Q(A) t t (6)
t=1 t=1
(cid:35)
(cid:104) (cid:105) T (cid:104) (cid:105)
−E Q(S
1
|πk ) logP(S 1 ) − ∑E Q(St |πk )Q(St−1 |πk ) logP(S t |S t−1 ,π k ) ,
t=2
8
Activeinferenceforaction-unawareagents
where we have singled out the KL divergences between the posterior probability distributions
fromthevariationalapproximationandthepriorprobabilitydistributionsfromthegenerativemodel
(first three terms), and grouped together all the terms involving one of the variational posteriors
Q(S |π ), k ∈ [1,p] where p is the number of policies (see Section S1.1), inside the expectation
1:T k
E [...](lastterm),whichcomputesanaveragewithrespecttoallpolicies.
Q(πk )
Technically, the free energy F is a functional (a term from the calculus of variations), i.e., a map-
pingfromaspaceoffunctionsto(inthiscase)therealnumbers. Findingitsminimumthusconsists
of looking for particular functions over given variables as opposed to particular values of given
variables for a function, as in more traditional optimization problems. In this case, the functions
we are looking for are probability distributions, i.e., the variational posteriors of Eq. (5). Given the
assumptions in Section 3.1, finding these functions amounts to tweaking the sets of parameters of
the variational distribution, until we find those that result in a distribution that minimises the free
energy. Since we are working with discrete probability distributions, there are analytical solutions
which can be found by simply setting the gradient of the free energy with respect to each set of pa-
rameterstozero,i.e.,∇ F[Q(S |π )] = 0,∇ F[A] = 0,...,onesetforeachprobabilitydistribution
st t k α
inquestion,andsolveforthecorrespondingparameters. InSectionS2,wedescribeindetailsomeof
thesesolutions.
When the expression in Eq. (6) is optimised with respect to the policy-conditioned variational
distributions, Q(S |π )∀k ∈ [1,p], we can simply focus on the argument of E [...] to compute
t k Q(πk )
the associated gradient (since that is the only term that contributes to the gradient and by noting
that ignoring the expectation does not change the solution of ∇ F[Q(S |π )] = 0). That argument
st t k
definesapolicy-conditionedfreeenergy:
T (cid:104) (cid:105) τ (cid:104) (cid:105)
F
(cid:2)
Q(S |π )
(cid:3)
:=
∑E
logQ(S |π ) −
∑E
logP(o |S ,A) −
πk 1:T k Q(St |πk ) t k Q(St |πk )Q(A) t t
t=1 (cid:124) (cid:123)(cid:122) (cid:125) t=1 (cid:124) (cid:123)(cid:122) (cid:125)
statelog-probabilities observationlog-likelihoods
(7)
(cid:104) (cid:105) T (cid:104) (cid:105)
−E Q(S
1
|πk ) logP(S 1 ) − ∑E Q(St |πk )Q(St−1 |πk ) logP(S t |S t−1 ,π k ) .
(cid:124) (cid:123)(cid:122) (cid:125) t=2 (cid:124) (cid:123)(cid:122) (cid:125)
statelog-probabilities transitionlog-likelihoods
TheupdaterulesforQ(S |π )∀k ∈ [1,p],derivedbytakingthecorrespondinggradientoftheex-
t k
pressioninEq.(7),defineanoptimization/inferenceschemecalledvariationalmessagepassingwhich
makesuseofpast,presentandfutureinformationtoupdate,inthiscase,variationalprobabilitydis-
tributions at different time points along a trajectory. Following standard treatments in the literature
of stochastic processes and (Bayesian) estimation, it is an example of smoothing, to be contrasted
withinference(whichusespresentinformationonly)andfiltering(whichreliesonpastandpresent
information),andprediction(whichusesthepastonly)[41,66].
From Eq. (6), one can derive an update rule for the probability distribution over policies, Q(π),
whichguidestheagentintheselectionofwhattodonext(itsnextaction). Thisupdateruleissome-
whattweakedinsuchawaythattheagentwillsampleactionsfromapolicythatbothminimisethe
policy-conditionedandtheexpectedfreeenergy(seeSectionS2.2forthedesignchoicethatintroduces
expected free energy). Expected free energy for policy π and for a single future time step t can be
k
definedasfollow:
9
Activeinferenceforaction-unawareagents
(cid:104) (cid:105) (cid:104) (cid:105)
G (π ) :=E H(cid:2) P(O |S ) (cid:3) −E D (cid:2) Q(A|o ,s )|Q(A) (cid:3)
t k Q(St |πk ) t t P(Ot |St )Q(St |πk ) KL t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
AMBIGUITY A-NOVELTY
(8)
(cid:104) (cid:105)
+D KL (cid:2) Q(S t |π k )|P ∗(S t ) (cid:3) −E Q(St+1 |πk )Q(St |πk ) D KL (cid:2) Q(B|s t+1 ,s t )|Q(B) (cid:3) ,
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125)
RISK
B-NOVELTY
where the risk term quantifies the divergence between the predicted and preferred state distri-
bution, the ambiguity terms quantifies the uncertainty related to the observation map, and the two
novelty terms are expected information gains for the parameters of the observation and the transi-
tion maps, thus indicating parts of the generative model that are still inaccurate. Therefore, we can
associate risk with the instrumental or extrinsic value of a policy, i.e., the extent to which it enables
an agent to reach its preferred states, whereas ambiguity and novelty with its epistemic or intrinsic
value, i.e., the extent to which it drives the agent to acquire informative observations (low ambigu-
ity) and visit states that provide new information about the environment (hight novelty). A policy
that minimises expected free energy does so by balancing the pursuit of these different targets, i.e.,
addressingtheexploitationvs.explorationdilemma: itmakessuretheagentreachesitsgoalswhile
atthesametimeexploringsufficientlyenoughtoacquirerelevantandusefulinformationaboutthe
environment. Formaldetailsabouttheminimisationofvariationalandexpectedfreeenergiesunder
variationalmessagepassingarecoveredinSectionS2forreference.
The components of the free energy in Eq. (6) and Eq. (7) involve terms of the variational ap-
proximation and of the generative model. It is important to note, however, that while the agent’s
generativemodel(Definition2.2)andgenerativeprocess(Definition2.1)arebothPOMDPs,theyare
in general different, they are not “synchronised”. To see why, consider when an agent is first put
incontactwithanewenvironment: theagentreceivesobservationsfromanewenvironmentandis
tryingtomakesenseofthestructurethatgeneratessuchsensoryinput,atthebeginningitsstatesand
parametersarelikelynotveryhelpful,butovertimetheycanbeoptimisedsotheagent’sgenerative
modelaligns,orsynchronises,withthegenerativeprocess. Todoso,ateveryfreeenergyminimisa-
tion stage, the agent uses the observations received so far to update the model’s parameters, aided
by the variational approximation (to overcome the burden of Bayesian inference). As learning pro-
gresses, the generative model will reflect the observation and transition dynamics of the POMDP
moreaccurately(which,recall,isusedtodescribeaparticularenvironment).
3.4. Action-awarevs.action-unawareagents
The policy-conditioned free energy in Eq. (7) is treated differently depending on whether the agent
knows what actions were performed in the past. This choice has several repercussions for various
aspects of active inference, mainly on the notion of policy and on what it means to condition on a
policy.
Action-aware agents use a known sequence of actions they performed in the past (a 1:τ−1 ). This
mean that, in Eq. (7), policy-conditioned variational distributions for past and present time steps,
Q(S | π ),...,Q(S | π )forallpoliciesk ∈ [1,p]oflengthT−12,areidentical,becauseallpolicies
1 k τ k
share the same sequence of actions a 1:τ−1 , i.e., the actions that were executed by the agent, but they
2IfweareconsideringanepisodictaskandTisthelengthofanepisode,thenapolicyconsistsofT−1actionsbecause
theagentdoesnotexecuteanyactionatthelasttimestep.
10
Activeinferenceforaction-unawareagents
differ with respect to future actions, a τ:T−1 3. On this view, given a sequence of actions already exe-
cuted and shared by all policies, perceptual inference corresponds to inferring the divergent future
trajectoriesinstate-spaceaffordedbythevariouspolicies,asrepresentedbyQ(S |π ),...,Q(S |π )
τ k T k
forallpoliciesk ∈ [1,p]oflengthT−1,whilepolicyinferencereliesoninferredvariationalbeliefsto
score each policy based on expected free energy. The main implication here is that policy inference
for action-aware agents involves updating the probability over policies by differentiating them only
with respect to their future consequences because all policies share the same past. Effectively, this
meansthatthenumberofpoliciestoevaluateshrinksovertime,asmoreknowledgeaboutexecuted
actionsisaccumulatedthatremovesactionsequencesthatwereneverperformed. Equivalently,one
can also conclude that an agent simply executes a single (known) policy from 1 to τ−1, a 1:τ−1 , and
thatdifferentpolicies a τ:T−1 needtobeevaluatedforfuturetimesteps,seealsoSection5).
In contrast, action-unaware agents must infer the unknown sequence of actions they performed
in the past, (a 1:τ−1 ), before they can successfully plan for the future. More precisely, for this class
of agents, each policy is a distinct sequence of past, present and future actions and therefore it
is no longer the case that all policies share the same sequence of past actions. During percep-
tual inference, an action-unaware agent will use the policy-conditioned variational distributions,
Q(S | π ),...,Q(S | π ) for all policies k ∈ [1,p] of length T −1, to represent the likelihood
1 k τ k
thatthehiddensequenceofactionsitexecuted,andthatgeneratedthesequenceofpastandpresent
observations,(o ),comesfromapolicyπ . Policy-conditionedfreeenergieswillthusgrowforpoli-
1:τ k
cies that do not explain observations collected up to the present and that most likely have not been
pursued. Policy inference on the other hand involves combining the evidence for each policy with
theexpectedfreeenergytoderiveanupdateofthepolicyprobabilities,guidingtheselectionofwhat
actiontoperformnext. Thus,policyinferenceforaction-unawareagentsinvolvesupdatingtheprob-
abilityoverpoliciesbytakingintoaccounttheirpast,presentandfutureconsequences(observations)
because each policy represents a distinct trajectory over the length T of an episode (as opposed to a
distincttrajectoryfortheremaining,future T−τ timestepsofanepisode).
Furtheralgorithmicdetailsonintegratingthevariationalmessagepassingscheme(introducedin
Section3.3)andtheaboveperspectivesonpoliciescanbefoundinalgorithmS1foraction-unaware
agents and algorithm S2 for action-aware ones. In the next sections, we will report findings from
simulations of the two types of agents in a T-maze and a Y-maze with episodes characterised by a
fixedduration,i.e.,finiteandfixedhorizonepisodes.
4. Experiments
4.1. Experiment1: LearninginaT-maze
In the first experiment, the agent moves inside the T-maze drawn in Fig. 1, starting from tile 5 and
withapreferencetoreachthegoalstateintheleftarm,i.e.,tile1. Wesimplifytheproblemstructure
to be a fully observable MDP (technically, the matrix A is not an identity, but it is diagonal and
known to the agent), with deterministic but unknown state transitions B. We trained 10 agents of
each type, with and without knowledge of past actions (i.e., action-aware and action-unaware), for
100episodes(of4stepseach,withapolicyhorizon H = 3,givingusatmost64policiestoevaluate)
intheenvironmentrepresentedinFig.1.
3Note that the action the agent takes at the present time step τ is part of the future sequence because it is executed
bytheagentaftertheperceptualinferenceandplanningstagesareover. However,thevariationaldistributionatτ,i.e.,
Q(Sτ |π
k
),isthesameforallpoliciesbecauseitdependsontheactiontakenatτ−1.
11
Activeinferenceforaction-unawareagents
Fig.1: Graphicalrepresentationofthe4-stepT-maze.
100%
80%
60%
40%
20%
0%
1 20 40 60 80 100
Episode
stnega
fo
egatnecreP
Agents solving the task
(action-unaware)
100%
80%
60%
40%
20%
0%
1 20 40 60 80 100
Episode
stnega
fo
egatnecreP
Agents solving the task
(action-aware)
Fig.2: Percentageofagentsreachingthegoalstateineachepisodeinthe4-stepT-maze(10totalagents).
4.1.1. Results
In Fig. 2, we report the percentage of agents solving the task across episodes. Both kinds of agents
areabletofindtheoptimalpolicywithinthefirst20episodes. Themaindifferencesaretheirlearning
speed and pattern. All action-aware agents fail in the fist 6 episodes, start finding their way to the
goal afterwards, and succeed consistently from episode 33 onwards, with a 100% success rate until
the end of the experiment, except for some drops in performance in a handful of episodes. Despite
not having access to past actions, action-unaware agents can also find the optimal policy relatively
quickly, with a 100% success rate from episode 36 onwards but, overall, make a few more mistakes
than their action-aware counterparts. These results indicate that both types of agents were able to
learnrelevantaspectsofthetransitionmodel,i.e.,theaction-dependenttransitionmatricesencoding
the (deterministic) effects of performing specific actions in specific states (see Section S3.1.7 for the
learned transition matrices, nearly identical in both types of agents, and compare them with the
ground truth ones in Section S3.1.6). To investigate further whether there other major differences
betweenthetwokindsofagents,weexamineandcomparefreeenergyandexpectedfreeenergyfor
the two groups throughout the experiment, the former shedding light on the perceptual side of the
12
Activeinferenceforaction-unawareagents
agents that takes into account its past trajectory, and the latter exploring more in detail its decision
makingsidethatinvolvesitspotentialfuturetrajectories.
10
8
6
4
2
0
1 20 40 60 80 100
Episode
ygrene
eerF
Free energy at step 4
(action-unaware)
10
8
6
4
2
0
1 20 40 60 80 100
Episode
ygrene
eerF
Free energy at step 4
(action-aware)
Fig.3: Freeenergyacrossepisodes(showingaverageof10agents).
Perceptual inference Figure 3 shows the free energy defined in Eq. (6), which needs to be min-
imised,atthelaststep(4th)ofeachepisodeforaction-awareandaction-unawareagents. Wepicked
the last step because it involves the entire past of an agent within an episode, i.e., the full, episodic
trajectoryofobservations,allowingforaquantificationoftheagent’suncertaintyovertheentiretime
intervalapolicycovers,andalsofortheinclusionintheexpressionforfreeenergy(Eq.(6))oftheKL
divergencebetweenpriorandposteriorBwhoseparametersareupdatedattheendofeachepisode
(steps1,2and3canbefoundinSectionS3.1.2). Thefreeenergyforbothagentsdecreasessmoothly
butconvergesataslightlylowervalueforaction-awareagentsthanforaction-unaware.
Figure4showsinsteadtheevolutionofthepolicy-conditionedfreeenergies(seeEq.(7))atstep4
forasubsetofallthe64policies(includingtheoptimalone),forbothtypesofagents(again,figures
withtheotherstepscanbefoundinSectionS3.1.3).
Starting with action-unaware agents in Fig. 4 (left), the figure reveals information hidden in the
averagereportedearlier: forthemostpart, thepolicy-conditionedfreeenergythatisminimisedthe
most is the one conditioned on the optimal policy. This makes sense since most action-unaware
agents learn to pursue π from the very few first episodes therefore the associated collection of ob-
8
servation minimises the free energy conditioned on that policy. However, this free energy is also
characterised by several spikes, especially towards the end of the experiment, indicating episodes
when the collected evidence is no longer consistent with the actions of π : those are episodes in
8
whichtheagenthaspickedasub-optimalpolicy,makingtheassociatedfreeenergydropinstead.
For action-aware agents instead we note that all the lines essentially overlap on the right side
of Fig. 4, so that the downward trend captured by the (unconditioned) free energy in Fig. 3 is rep-
resentative of the way the policy-conditioned ones evolve. More precisely, since the free energy is
computed as an average of all the policy-conditioned free energies, the above findings reveal that
the values of the latter are all identical. This should not be surprising because we are considering
thelast(4th)step: atthispoint,inaction-awareagents,allthe“policy-conditioned”freeenergiesare
computedbyconsideringthesamesequenceofactions,theonethatproducedthestate-observation
trajectoryofaparticularepisode,andthereisnolongeradivergentfuturerepresentedbythefuture
13
Activeinferenceforaction-unawareagents
10
8
6
4
2
0
1 20 40 60 80 100
Episode
ygrene
eerF
Policy-conditioned free energy at step 4
(action-unaware)
10
8
6
4
2
0
1 20 40 60 80 100
Episode
ygrene
eerF
Policy-conditioned free energy at step 4
(action-aware)
Policies
1: , , 5: , , 9: , , 13: , ,
2: , , 6: , , 10: , , 14: , ,
3: , , 7: , , 11: , , 15: , ,
4: , , 8: , , 12: , , 16: , ,
Fig.4: Policy-conditionedfreeenergiesacrossepisodes(showingaverageof10agents).
actionsofeachpolicy(seehowthedifferencesamongpolicy-conditionedfreeenergydecreaseacross
timestepsinSectionS3.1.3).
Overall, this means that observations collected by action-unaware agents correctly minimise the
policy-conditioned free energy associated with the policy that was executed, whereas observations
collected by action-aware agents simply minimise the variational free energy for the sequence of
actions that characterise an entire episode trajectory. To see if other difference emerges between the
two types of agents, we next examine expected free energy and other metrics connected with the
planningandactionselectionmechanisms.
Planningandlearning Figure5showsthetotalexpectedfreeenergyforeachselectedpolicyacross
episodes for both types of agents at time step 1. We chose this step because it involves the sum of
all the expected free energy in the future: from time step 1 until the end of the episode (see Sec-
tionS2.2),characterisingintermsofriskandB-noveltytheentiretrajectoryaffordedbyapolicy(for
completeness,timesteps2and3canbefoundinSectionS3.1.4).
Thefirstthingtonoticeisthatexpectedfreeenergyincreasesinthefirst30–40episodesforboth
kinds of agents. This is surprising because agents ought to minimise it, but can be explained by the
fact that, in our experiments, the transition model of an agent (representing the unknown ground
truthtransitionstobelearnt)israndomlyinitialisedatthebeginningoftheexperimentandupdated
onlyattheendofeachepisode. Sinceatanearlystagethetransitionmodelisnotagoodreflectionof
groundtruthtransitions,anagentcannotaccuratelypredictwhatwillhappenifapolicyisexecuted.
Moreprecisely,variationalbeliefsareuniformlyinitialisedatthebeginningofeachepisode,andneed
tobeupdatedthroughperceptualinferencebyusingthetransitionmodel. However,ifthelatterhas
yet to align with ground truth transitions, the agent will not be able to form accurate beliefs about
thelocationsvisitedbyacertainpolicy. Asaresult,sinceexpectedfreeenergyiscomputedbasedon
14
Activeinferenceforaction-unawareagents
12
10
8
6
4
2
0
1 20 40 60 80 100
Episode
ygrene
eerf
detcepxE
Expected free energy at step 1
(action-unaware)
12
10
8
6
4
2
0
1 20 40 60 80 100
Episode
ygrene
eerf
detcepxE
Expected free energy at step 1
(action-aware)
Policies
1: , , 5: , , 9: , , 13: , ,
2: , , 6: , , 10: , , 14: , ,
3: , , 7: , , 11: , , 15: , ,
4: , , 8: , , 12: , , 16: , ,
Fig.5: Expected free energy for each policy across episodes (showing average of 10 agents). Notice that we
onlydraw16expectedfreeenergies,representativeofthepossible64.
thosebeliefs,itsvaluesatstep1ofeachepisodewillnotaccuratelyestimateuncertainty/desirability
of any sequence of actions for the first few episodes. Thus, while agents are still learning the tran-
sition model, expected free energy increases for each policy until it converges to a value that scores
policies more precisely in the current environment, depending on the agent’s preferences and the
accuracyofitsvariationalbeliefs. Toseehowdifferentcomponentsofthisquantityevolveovertime,
inoursimplifiedsetupwithnoambiguityandconstantA-novelty,seeSectionS3.1.5.
Expectedfreeenergyalsoplaysasignificantroleintheupdateoftheprobabilitiesoverpoliciesat
eachstep,whichareobtainedasasoftmaxofthenegativesumofexpectedfreeenergiesandpolicy-
conditioned free energies (see Section S2.2 for more on expected free energy, and Eq. (S11) for the
softmaxpartspecifically). Toseethecontributionsofexpectedfreeenergies,andtheirbalanceagainst
policy-conditionedfreeenergies,wenextlookatFig.6,showingthefirst-steppolicyprobabilitiesfor
asubsetofalltheavailablepolicies,includingtheoptimalone,foreachtypeofagent. Forbothagents,
we observe that the optimal policy, π in the figure, is correctly selected and start to becomes more
8
probable than the others after approximately 20 episodes. Some sub-optimal policies also become
increasinglyprobableovertime,thoughneverenoughtosurpasstheoptimalone(e.g.,considerthe
spikes of probability mass for the blue and cyan policies in Fig. 6). Further investigations into the
underlyingreasonsforthispatternareleftforfuturework.
Overall, when we consider expected free energies and policy probabilities at the first step of an
episode, there is no significant difference between the two types of agent. This is to be expected
becauseatthebeginningofanepisodebothagentsperformperceptualinference, planning, andthe
update of policy probabilities on the same footing, i.e., there are no significant differences between
the respective policy-conditioned free energies. We have also observed that at a later stage in the
experiment both agents become less accurate in predicting the consequences of certain future ac-
15
Activeinferenceforaction-unawareagents
0.08
0.07
0.06
0.05
0.04
0.03
0.02
0.01
0.00
1 20 40 60 80 100
Episode
ssam
ytilibaborP
First-step policy probability
(action-unaware)
0.08
0.07
0.06
0.05
0.04
0.03
0.02
0.01
0.00
1 20 40 60 80 100
Episode
ssam
ytilibaborP
First-step policy probability
(action-aware)
Policies
1: , , 5: , , 9: , , 13: , ,
2: , , 6: , , 10: , , 14: , ,
3: , , 7: , , 11: , , 15: , ,
4: , , 8: , , 12: , , 16: , ,
Fig.6: Policiesprobabilitiesatstep1ofeachepisode(showingaverageof10agents). Noticeweonlydraw16
representativepoliciesoutofthepossible64.
tion sequences, with this phenomenon appearing more marked in action-aware agents (see plots
inFig.S1).
4.2. Experiment2: Learninginagridworld
Inthesecondexperiment,weconsideranenvironmentwithalargerstate-space,a3×3gridworld,as
depictedinFig.7. Whileonlyslightlybiggerintermsofstates,thepolicyspaceinthisenvironment
is much larger and includes multiple optimal policies, which could in principle affect our active
inference agents. The agent starts in tile 1 and its goal, tile 9 in the bottom right corner, is encoded
as the most preferred state (target location). Once again, the problem is simplified to be a fully
observable MDP (with A diagonal and known to the agent), with deterministic but unknown state
transitionsB. Heretoo,wetrained10agentsofeachkind,action-unawareandaction-aware,for180
ratherthan100episodes(of5stepseach,withapolicyhorizon H = 4,givingusatmost256policies
toevaluate)toallowourmetricstoconverge.
4.2.1. Results
Similarly to Section 4.1.1, we start by comparing the percentage of agent solving the task across
episodes in Fig. 8. Again, we note that both kinds of agent display a similar learning pattern, with
agentstakinglongertofindoneoftheoptimalpolicies(thereare6intotalthistime)duetothelarger
state-spaceandnumberofavailablepolicies. Thepercentageofsuccessfulagentsgrowsuntilepisode
38 and 37, when a 100% success rate is hit in action-unaware and action-aware agents, respectively,
and then drops afterwards to values below 50% in some episodes, with action-unaware agents reg-
istering the more dramatic dips. Both kinds of agent quickly recover and the success rate remains
16
Activeinferenceforaction-unawareagents
Fig.7: Graphicalrepresentationofthe5-stepgridworld.
above 60% for the most part from around episode 60 until the end of the experiment, with more
dropsinperformance(i.e.,toandbelow60%)forbothkindsofagentinahandfulofepisodes. These
results again indicate that both types of agents successfully learned relevant aspects of the transi-
tion matrices (see Section S3.2.7 for the learned transition matrices, nearly identical in both types of
agents,andcomparethemwiththegroundtruthonesinSectionS3.2.6)
100%
80%
60%
40%
20%
0%
1 20 40 60 80 100 120 140 160 180
Episode
stnega
fo
egatnecreP
Agents solving the task
(action-unaware)
100%
80%
60%
40%
20%
0%
1 20 40 60 80 100 120 140 160 180
Episode
stnega
fo
egatnecreP
Agents solving the task
(action-aware)
Fig.8: Percentageofagentsreachingthegoalstateineachepisodeinthe5-stepgridworld(10totalagents).
Perceptual inference The average free energies at the step 5 (last step), see Fig. 9, are predictably
minimised,butonceagainhidesomerelevantinformationthatcanbeunpackedbyshowingpolicy-
conditionedfreeenergies(steps1,2,3,and4canbefoundinSectionS3.2.2).
Forthepolicy-conditionedfreeenergiesatstep5,Fig.10,weselected16policies(amongthe256)
includingthe6optimalpoliciesthatleadtothegoalstate(again,figureswiththeotherstepscanbe
found in Section S3.2.3). As seen in the T-maze experiment, in action-aware agents the downward
trendofthe(unconditioned)freeenergyinFig.9isrepresentativeofthewaythepolicy-conditioned
ones evolve in the right plot of Fig. 10 (i.e., all the policy-conditioned free energies for the selected
policies overlap). In contrast, all the visualised policy-conditioned free energy of action-unaware
17
Activeinferenceforaction-unawareagents
16
14
12
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180
Episode
ygrene
eerF
Free energy at step 5
(action-unaware)
16
14
12
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180
Episode
ygrene
eerF
Free energy at step 5
(action-aware)
Fig.9: Freeenergyacrossepisodes(showingaverageof10agents).
agents, in the left plot of Fig. 10, fluctuate considerably throughout the experiment, with none of
theoptimalpoliciesattainingaconsistentdecreaseoftheassociatedfreeenergy. Thereasonforthat
is that action-unaware agents have discovered all the optimal policies, each offering an alternative
route to reach the goal state, and assigned them equal probability mass (see Fig. 12). Therefore,
at the beginning of an episode, agents can randomly choose among alternative paths to the goal,
resultingintheminimisationofdifferentpolicy-conditionedfreeenergiesattheendofeachepisode
(recall that for action-unaware agents, a policy-conditioned free energy at the end of an episode is
minimised when the observations the agent has received are consistent with having followed the
policyinquestion).
14
12
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180
Episode
ygrene
eerF
Policy-conditioned free energy at step 5
(action-unaware)
14
12
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180
Episode
ygrene
eerF
Policy-conditioned free energy at step 5
(action-aware)
Policies
1: , , , 5: , , , 9: , , , 13: , , ,
2: , , , 6: , , , 10: , , , 14: , , ,
3: , , , 7: , , , 11: , , , 15: , , ,
4: , , , 8: , , , 12: , , , 16: , , ,
Fig.10: Policy-conditionedfreeenergiesacrossepisodes(showingaverageof10agents).
18
Activeinferenceforaction-unawareagents
Planningandlearning Theexpectedfreeenergiesatstep1inFig.11,againforthesamesubsetof
policies considered in Fig. 10, evolve similarly in both kinds of agent: there is no clear distinction
between optimal vs. sub-optimal policies, not even at convergence, as a few sub-optimal policies
attain expected free energy values comparable to those of the optimal ones (expected free energies
at the other steps can be found in Section S3.2.4). As seen in the T-maze experiment, risk is much
larger than B-novelty in the composition of expected free energy to the point that the trend of the
latter does not differ substantially from that of risk (compare the expected free energy and risk fig-
ures,Fig.11andFig.S28,respectively,andseeFig.S29forB-novelty). Furthermore,thereareagain
sub-optimal policies for which risk (hence the expected free energy) drops sharply to levels lower
than,orcomparableto,thoseoftheoptimalpolicies.
18
16
14
12
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180
Episode
ygrene
eerf
detcepxE
Expected free energy at step 1
(action-unaware)
18
16
14
12
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180
Episode
ygrene
eerf
detcepxE
Expected free energy at step 1
(action-aware)
Policies
1: , , , 5: , , , 9: , , , 13: , , ,
2: , , , 6: , , , 10: , , , 14: , , ,
3: , , , 7: , , , 11: , , , 15: , , ,
4: , , , 8: , , , 12: , , , 16: , , ,
Fig.11: Expectedfreeenergyforeachpolicyacrossepisodes(showingaverageof10agents). Noticethatwe
onlydraw16expectedfreeenergies,representativeofthepossible256.
Figure 12 shows the policy probabilities across episodes, revealing key differences between the
two kinds of agents and between this and the previous experiment. In action-unaware agents, the
probabilities of the six optimal policies share the same upward trend from around episode 60 on-
wardswiththeircurvesalmostperfectlyoverlapping(onlytheredandbluearevisibleinthefigure,
therestbeinghiddenbeneath),andacleargapemergesbetweenthemandthoseofmostsub-optimal
policies from around episode 150 (see below for exceptions). By the end of the experiment, all op-
timal policies have been recognised and assigned roughly the same probability mass (see left plot
inFig.12). Inaction-awareagents,thereisalessperfectoverlapbetweentheprobabilitiesoftheop-
timalpolicies,andtheoptimalvs.suboptimalgapbeginssomewhatearlier,aroundepisode120,and
iswiderbytheendoftheexperiment(againwithsomeexceptions;seerightplotinFig.12andnext).
AsintheT-mazeexperiment,however,somesub-optimalpoliciesalsobecomeincreasinglyprobable
over time, narrowing the gap with optimal policies in both kinds of agents. Unlike in the previous
experiment,wenowfindthatsomeofthesepoliciesbecomemoreprobablethanoptimalones,even
inlaterepisodes,whenagentshavehadampleopportunitiestorefinethetransitionmodel.
19
Activeinferenceforaction-unawareagents
0.030
0.025
0.020
0.015
0.010
0.005
0.000
1 20 40 60 80 100 120 140 160 180
Episode
ssam
ytilibaborP
First-step policy probability
(action-unaware)
0.030
0.025
0.020
0.015
0.010
0.005
0.000
1 20 40 60 80 100 120 140 160 180
Episode
ssam
ytilibaborP
First-step policy probability
(action-aware)
Policies
1: , , , 5: , , , 9: , , , 13: , , ,
2: , , , 6: , , , 10: , , , 14: , , ,
3: , , , 7: , , , 11: , , , 15: , , ,
4: , , , 8: , , , 12: , , , 16: , , ,
Fig.12: Policiesprobabilitiesatthefirststepofeachepisode(showingaverageof10agents). Noticeweonly
draw16representativepoliciesoutofthepossible256.
As noted, the evolution of expected free energy is similar in both agents and is not particularly
informative about which policies are to be preferred. Yet, agents can infer a probability distribution
over policies that is mostly accurate as it singles out the six optimal policies (despite the significant
probabilitymassacquiredbysomesuboptimalpolicies). Thiscanbeexplainedbytherelativelylow
values,achievedbytheoptimalpolicies,oftheotherkeyquantityusedtocomputepolicyprobabil-
ities, i.e., the policy-conditioned free energy (at step 1, since we are considering policy probabilities
atthatstep;seeFig.S21). Whenapolicy-conditionedfreeenergyisminimisedatstep1,itmeansthe
agentismorecertainaboutthefutureconsequencesoffollowingthecorrespondingpolicyfortherest
oftheepisode. Therefore,moreinformativepolicy-conditionedfreeenergiescancompensateforless
informative expected free energies: given two policies with similar expected free energy, the agent
will assign more probability to the one associated with more accurate predictions at the perceptual
inferencestage(i.e.,withthelowestpolicy-conditionedfreeenergy).
Inthecaseofthesub-optimalpoliciesmentionedabove,whichatsomepointsurpasstheoptimal
ones in probability, both the expected free energy and the policy-conditioned free energy (at step
1) are informative, but in opposing ways: the expected free energy increases the likelihood of these
policies,whereasthepolicy-conditionedfreeenergydecreasesit(compareFig.11andFig.S21)4. For
bothkindsofagent,theneteffectinthiscaseisthatsub-optimalpoliciesgainmoreprobabilitythan
optimalones,indicatingthattheexpectedfreeenergyhadagreaterinfluenceonpolicyprobabilities
than the policy-conditioned free energy (see again plots in Fig. 12). Further investigations into the
opposingcontributionsofthesequantitiestothepolicyprobabilities,aswellasintothereasonswhy
agents’performancedoesnotdeterioratemoresubstantiallydespitetheincreasedprobabilityofsub-
optimalpolicies,areagainleftforfuturework.
4Thisisduetotheuseofthesoftmaxtocomputepolicyprobabilitiesbasedonthesumofthenegativeexpectedfree
energyandnegativepolicy-conditionedfreeenergy(seeSectionS2.2)
20
Activeinferenceforaction-unawareagents
5. Discussion
Animportantdifferencebetweenreinforcementlearningandmostactiveinferenceworksisthepar-
ticularmeaningattributedtotheword‘policy’. Intheformer,apolicyisoftendefinedasaprobability
functionπ : S×A → [0,1],andusuallywrittenasπ(a | s)toindicatethatapolicyreturnstheproba-
bilityofperformingacertainaction(atacertaintimestep)givenastate. Instandardactiveinference
algorithms (considered here), a policy is just a sequence of actions indexed by time (recent active
inference works have proposed slightly different algorithms in which the notion of policy is much
closertothatusedinreinforcementlearning,see,e.g.,[31,17]).
Furthermore,howanactiveinferenceagentcomputesandselectsamongitspoliciesateachtime
step,inthemoretraditionalactiveinferencesenseofactionsindexedbytime,isalsosubjecttodiffer-
ent interpretations. On the one hand, a policy can be intended as a motor plan covering a complete
trajectory of actions in the past, present and future of an agent’s experience. For instance, a policy
could cover a complete trajectory of H = T−1 actions, π := [a ,...,a ], from the beginning to the
1 H
end of an episode [16]. In this case, at a given time step τ (the present), an agent assigns a proba-
bility to each policy based on how likely it is that its past τ−1 actions have generated its past and
present observations, and on how likely it is that the policy will lead to the agent’s goal in the re-
maining H−τ+1(orT−τ)futureactions. Ontheotherhand,apolicycanbeseenasamotorplan
of future actions only. In this sense, policies correspond to sequences of H actions from the current
time step t = τ to a future time step t = H+τ, i.e., the planning horizon of each policy such that
π := [a ,...,a ],seeforinstance[36,37,17,31]5.
τ H
In this work, we have taken this difference to characterise agents that are aware and agents that
are not aware of the actions that they executed in the past, i.e., action-aware vs. action-unaware
agents. Action-awareagentsplantoinferthemostlikelysequencesofactionstobefollowedfromthe
current time step onwards, i.e., their policies contain exclusively future actions. On the other hand,
action-unaware agents plan to infer the most likely sequences of actions that should be continued
in the future given beliefs about what sequences of actions they performed in the past, since they
don’thaveaccesstoexplicitpastknowledgeoftheirownactions,i.e.,theirpoliciesincludebothpast
and future actions. Action-aware agents encode variational beliefs that need not be conditional on
different past action sequences for past state variables, S , because these agents know which ones
1:τ
were executed. On the other hand, if an agent does not know what its past actions were, then the
samebeliefsneedtobeconditionedonthepermissiblesequencesofactionsthatcanaccountforpast
andpresentobservationsproperly,i.e.,sequenceofactionscompatiblewiththeagent’sexperience.
Thisdistinctionhassomeimplicationsforhowtheagentevaluatesfutureactionsequences. Hav-
ing access to past actions, an agent can use them to more accurately infer its present location (state)
and from there consider different future action sequences, i.e., policies as future plans, based on
their most likely and desired consequences. This lends itself to a separation between free energy
minimizationofpaststates/observationsandoffutureones,potentiallyrelyingontwodistinctgen-
erative models (see [60, 57, 31] for examples of this sort of separation, and [4] for connection to the
separation principle of control theory). Without access to past actions, an agent’s variational beliefs
forpast,presentandfuturestatesareconditionalonallpossiblepolicies.
This discrepancy builds on an established literature in active inference that assumes that agents
do not have explicit knowledge of (or access to) the actions they take, either in the past or the
5Notethatinthecaseofepisodeswithafixednumberof T timesteps(asthoseoftheexperimentsdescribedinthis
work), for action-aware agent we would have that H ≤ T, if we use H to represent the length of policies intended as
sequencesofactionsfromthecurrenttimesteponwards.
21
Activeinferenceforaction-unawareagents
present[25,1,59],andconstitutesoneofitsmaindeparturesfrom“controlasinference”approaches,
whichinsteaddo[47]. Concretely,thismeansthatactionsexecutedbyanagent,groundtruthactions,
are not part of its generative model, but only of the generative process of the environment [32, 34,
28]. A generative model contains instead a policy random variable that stands for sequences of ac-
tions,whoselikelihoodneedstobeinferredfromobservationsbytheagent. Thesesequencesarenot
simple copies of ground truth actions [4] but represent all the possible motor paths an agent could
have initiated in the past and could be completing in the future. Agents without such knowledge
have to infer the consequences of their own hidden actions (indirectly, as part of the effects a policy
has) and the environment’s hidden states at the same time, from the same given observations, and
this puts a heavier burden on their ability to plan for the future, since they are effectively operating
without the classical efference copy mechanism [15]. This is however compatible with variations
of the “equilibrium point hypothesis” and “referent control” [21, 19, 20], which contrast proposals
of forward and inverse models based on linear quadratic Gaussian control and the separation prin-
ciple from control theory, see [2, 5, 3, 7] for a more comprehensive perspective, and constitute the
basis for continuous-time formulations of active inference minimising variational free energy [34,
28, 59]. While seemingly disadvantageous when considering the same active inference architecture
(knowing one’s actions would clearly help), it is often claimed that this constitutes an overall im-
provement over the standard use of inverse models, see [25], replacing complex (forward) model
inversions with proprioceptive predictions in a low dimensional latent space and pre-programmed
reflexestranslatingthosepredictionsintoactions,whichinturnoughttoprovideamorebiologically
plausibleaccountofmotorcontrolinhumansamongothers[25,1].
In this light, action-aware agents, following [37], deviate from the classic active inference litera-
ture just illustrated because, at each time step, they have access to a copy of the ground truth past
actions. Despite the fact that this occurs within a Bayesian framework that no longer distinguishes
between forward and inverse models ([see 14, Ch. 4]), it is closer to a reinstatement of the notion of
efferencecopymechanism. Onecouldobjectthatinthisactiveinferenceframeworkpoliciesandac-
tionsstillcorrespondtoproprioceptivepredictions, andthereforetheyshouldnotbeconfusedwith
the standard notion of efference copy. However, action-aware agents are required to store copies of
groundtruthproprioceptivepredictions,anoperationthatisnotpartofthestandardactiveinference
formulation (to the best of our knowledge) and that, again, brings the notion of proprioceptive pre-
dictionclosertothatofefferencecopy. Incontrast,action-unawareagents,asformulatedin[16],can
be seen as the discrete-time counterpart, operating at slower time scales and at a higher abstraction
level, tostandardcontinuous-timeactiveinference, usuallyfocusedonlow-levelmotiongeneration
skills [58], matching its architecture inspired by referent control, where action/motor commands as
proprioceptivepredictionsareinferredandconditioneduponsensoryobservations.
OurworkprovidesaPythonimplementationthatrelaxesthestrongassumptionofaction-aware
agents in [37], more closely follows standard formulations of active inference and its proposal of
more biological plausible models without the traditional mechanism of efference copy, and shows
evidence from simulations that action-unaware agents can match the performances of their action-
awarecounterpartswhichhaveexplicitknowledgeoftheirownactions.
Whileaction-unawareagentsconstitute,accordingtoactiveinference,amorebiologicallyplausi-
bleimplementationofactiveinferenceagents,thiscomesatacost. AsshowcasedbyalgorithmsAl-
gorithmS1andAlgorithmS2,thealternativewaysofviewingpoliciesthatcharacteriseaction-aware
andaction-unawareagentshaveimportantconsequencesonthecorrespondingimplementations. In
particular, they affect the computations that go into perceptual inference and, in turn, its time com-
plexity. At each step, action-aware agents need to update only one collection of variational distri-
22
Activeinferenceforaction-unawareagents
butions over past state variables: those conditioned on the sequence of actions that was executed
(this explains why in these agents there is only a single variational free energy for past states, as re-
markedearlierandaswesawinFigs.4and10). Incontrast,ateachtimestep,action-unawareagents
havetoupdateasmanycollectionsofvariationaldistributionasthenumberofpolicies,tocompute
thepolicy-conditionedfreeenergiesthatquantifytheextenttowhichapolicyisconsistentwiththe
collectedobservations. Inotherwords,thetimecomplexityoftheperceptualstageisO(n)inaction-
aware agent and O(nm) in action-unaware, where n is the number of past state variables and m is
thenumberofpolicies. Thismakesthealgorithmforaction-awareagentsclearlymoreefficientthan
thelatterbyassumingthattheagenthasaccesstomoreinformation.
This may suggest that action-unaware agents are more tailored for finite-horizon tasks in which
episodes have a fixed duration, corresponding to the number of actions in each policy plus 1, i.e.,
T = H+1timesteps,becausethereisnoactionatthelasttimestep. Inthislearningsetting,action-
unaware agents keeps track of how many time steps have passed from the beginning of an episode
toevaluatepoliciesbasedontheremainingactionsonly. Conversely, action-awareagentsappearto
bemorecongenialtofinite-horizontasksinwhichepisodeshaveanindefinitedurationbecauseofthe
separationbetweenpastandfuturesequencesofactionswhichpredisposesthemtoconsiderateach
timestepacertainfixednumberof H actionsintothefuture.
6. Concluding Remarks
Active inference has gained traction in computational neuroscience as a modelling framework to
study adaptive decision making in a variety of context. In this work, we introduced the essential
aspectsoftheframeworkindetail,showinghowfreeenergyminimisationcanbeusedasaguiding
principle to understand perception, planning, action-selection, and learning in an adaptive agent
movinginasimplegrid-worldenvironment.
We investigated active inference in two different regimes, studying the typical behaviour of
agentsthatarenotawareoftheirpastactions(action-unaware)andofagentsthatare(action-aware).
Theformerfollowsmorestrictlythetraditionofactiveinferenceframeworksinspiredbythe“equi-
libriumpointhypothesis”and“referentcontrol”[21,19],claimingthathumans,amongotherbiolog-
ical agents, do not possess or even need the ability to discount the effects of their actions from their
observations [8, 20, 45]. The latter assumes that knowledge of past action sequences is available to
anagent,whichcanthussimplydiscounttheeffectsofknownexecutedactionsfromitsrecollection
ofpastobservationsandfromcurrentonessotomoreeasilyplanforthefuture.
Our simulations in two toy environments, a T-maze and a 3x3 grid world, showed that, while
in principle at a severe disadvantage, action-unaware agents can overall match the performances
of action-aware ones. While impressive, this comes at a heavy computational cost, which currently
preventsaction-unawareagentsfrombeingfullyscalabletolargersimulations,sincethereisacom-
binatorialexplosionofpossibleactionsequencestobecheckedthatdependsnotonlyonpresentand
futuretimestepsandtheirassociatedactions,butalsoonpastones. Atthisstage,wespeculatethat
mechanisms such as weight-based sampling of action sequences may provide an affordable imple-
mentationinhigh-dimensionalaction-sequencespaces,butweleavethisandotherinvestigationsto
futurework.
23
Activeinferenceforaction-unawareagents
Acknowledgments
F.T., R.K. and M.B. were supported by JST, Moonshot R&D, Grant Number JPMJMS2012. F.T. and
K.S.werepartiallysupportedbyTokyoElectron. K.S.wassupportedbyJST,CRESTGrantNumber
JPMJCR21P4,Japan.
References
[1] Rick A. Adams, Stewart Shipp, and Karl J. Friston. “Predictions Not Commands: Active In-
ference in the Motor System”. In: Brain Structure and Function 218.3 (2013), pp. 611–643. ISSN:
18632653. DOI:10.1007/s00429-012-0475-5.
[2] Manuel Baltieri. “Active Inference: Building a New Bridge between Control Theory and Em-
bodiedCognitiveScience.”PhDthesis.UniversityofSussex,2019.
[3] ManuelBaltieri.“ABayesianPerspectiveonClassicalControl”.In:2020InternationalJointCon-
ferenceonNeuralNetworks(IJCNN).IEEE,2020,pp.1–8.
[4] Manuel Baltieri and Christopher L. Buckley. “The Modularity of Action and Perception Re-
visitedUsingControlTheoryandActiveInference”.In:ArtificialLifeConferenceProceedings30.
MITPressOneRogersStreet,Cambridge,MA02142-1209,USAjournals-info...,2018,pp.121–
128.
[5] ManuelBaltieriandChristopherL.Buckley.“ActiveInference:ComputationalModelsofMo-
torControlwithoutEfferenceCopy”.In:Proceedingsofthe2019ConferenceonCognitiveCompu-
tationalNeuroscience.ACM,NewYork,2019.
[6] ManuelBaltieriandChristopherL.Buckley.“NonmodularArchitecturesofCognitiveSystems
Based on Active Inference”. In: 2019 International Joint Conference on Neural Networks (IJCNN).
IEEE,2019,pp.1–8.
[7] ManuelBaltieriandChristopherL.Buckley.“PIDControlasaProcessofActiveInferencewith
Linear Generative Models”. In: Entropy. An International and Interdisciplinary Journal of Entropy
andInformationStudies21.3(2019),p.257.
[8] BruceBridgeman.“EfferenceCopyandItsLimitations”.In:ComputersinBiologyandMedicine.
Vision and Movement in Man and Machines 37.7 (July 1, 2007), pp. 924–929. ISSN: 0010-4825.
DOI:10.1016/j.compbiomed.2006.07.001.
[9] JelleBruineberg.“ActiveInferenceandthePrimacyofthe‘ICan’”.In:PhilosophyandPredictive
Processing. Ed. by Thomas Metzinger and Wanja Wiese. Frankfurt am Main, Germany: MIND
Group,2017,pp.1–18.ISBN:978-3-95857-306-2.
[10] Christopher L. Buckley et al. “The Free Energy Principle for Action and Perception: A Math-
ematical Review”. In: Journal of Mathematical Psychology 81 (2017), pp. 55–79. ISSN: 10960880.
DOI:10.1016/j.jmp.2017.09.004.
[11] Ozan C¸atal et al. “Learning Generative State Space Models for Active Inference”. In: Frontiers
in Computational Neuroscience 14 (2020), p. 103. ISSN: 1662-5188. DOI: 10.3389/fncom.2020.
574372.
[12] AndyClark.“Whatevernext?PredictiveBrains,SituatedAgents,andtheFutureofCognitive
Science”. In: Behavioral and Brain Sciences 36.3 (2013), pp. 181–204. ISSN: 14691825. DOI: 10.
1017/S0140525X12000477.PMID:23663408.
24
Activeinferenceforaction-unawareagents
[13] AndyClark.“RadicalPredictiveProcessing”.In:TheSouthernJournalofPhilosophy53.S1(2015),
pp.3–27. ISSN:00384283.DOI:10.1111/sjp.12120.
[14] AndyClark.SurfingUncertainty:Prediction,Action,andtheEmbodiedMind.Oxford,UK:Oxford
UniversityPress,2016.ISBN:978-0-19-021703-7.
[15] Trinity B. Crapse and Marc A. Sommer. “Corollary Discharge across the Animal Kingdom”.
In: Nature Reviews Neuroscience 9.8 (Aug. 2008), pp. 587–600. ISSN: 1471-0048. DOI: 10.1038/
nrn2457.
[16] Lancelot Da Costa et al. “Active Inference on Discrete State-Spaces: A Synthesis”. In: Journal
of Mathematical Psychology 99 (Dec. 1, 2020), p. 102447. ISSN: 0022-2496. DOI: 10.1016/j.jmp.
2020.102447.
[17] LancelotDaCostaetal.“RewardMaximizationThroughDiscreteActiveInference”.In:Neural
Computation35.5(Apr.18,2023),pp.807–852.ISSN:0899-7667. DOI:10.1162/neco_a_01574.
[18] MarcPeterDeisenroth,A.AldoFaisal,andChengSoonOng.MathematicsforMachineLearning.
CambridgeUniversityPress,Apr.2020.ISBN:978-1-108-45514-5.
[19] Anatol G. Feldman. “New Insights into Action–Perception Coupling”. In: Experimental Brain
Research194.1(Mar.1,2009),pp.39–58.ISSN:1432-1106.DOI:10.1007/s00221-008-1667-3.
[20] AnatolG.Feldman.“ActiveSensingwithoutEfferenceCopy:ReferentControlofPerception”.
In:JournalofNeurophysiology116.3(Sept.2016),pp.960–976.ISSN:0022-3077.DOI:10.1152/jn.
00016.2016.
[21] HarrietFeldmanandKarlJ.Friston.“Attention,Uncertainty,andFree-Energy”.In:Frontiersin
Human Neuroscience 4 (December 2010), pp. 1–23. ISSN: 16625161. DOI: 10.3389/fnhum.2010.
00215.
[22] KarlFriston.“ATheoryofCorticalResponses”.In:PhilosophicalTransactionsoftheRoyalSociety
B: Biological Sciences 360.1456 (2005), pp. 815–836. ISSN: 0962-8436. DOI: 10.1098/rstb.2005.
1622.
[23] Karl Friston. “Hierarchical Models in the Brain”. In: PLoS Computational Biology 4.11 (2008),
pp.1–24. ISSN:1553734X.DOI:10.1371/journal.pcbi.1000211.PMID:18989391.
[24] KarlFriston.“TheFree-EnergyPrinciple:ARoughGuidetotheBrain?”In:TrendsinCognitive
Sciences 13.7 (2009), pp. 293–301. ISSN: 13646613. DOI: 10.1016/j.tics.2009.04.005. PMID:
19559644.
[25] KarlFriston.“WhatIsOptimalaboutMotorControl?”In:Neuron72.3(Nov.3,2011),pp.488–
498. ISSN:0896-6273.DOI:10.1016/j.neuron.2011.10.018.
[26] Karl Friston. “Prediction, Perception and Agency”. In: International Journal of Psychophysiology
83.2(Feb.2012),pp.248–252. ISSN:01678760. DOI:10.1016/j.ijpsycho.2011.11.014.PMID:
22178504.
[27] KarlFriston.“AFreeEnergyPrincipleforaParticularPhysics”.In:(2019),pp.1–140.
[28] Karl Friston, Spyridon Samothrakis, and Read Montague. “Active Inference and Agency: Op-
timal Control without Cost Functions”. In: Biological Cybernetics 106.8–9 (2012), pp. 523–541.
ISSN:03401200.DOI:10.1007/s00422-012-0512-8.PMID:22864468.
[29] Karl Friston et al. “Active Inference and Learning”. In: Neuroscience and Biobehavioral Reviews
68 (2016), pp. 862–879. ISSN: 18737528. DOI: 10.1016/j.neubiorev.2016.06.022. PMID:
27375276.
25
Activeinferenceforaction-unawareagents
[30] Karl Friston et al. “Active Inference: A Process Theory”. In: Neural Computation 29.1 (2017),
pp.1–49. DOI:10.1162/NECO_a_00912.
[31] KarlFristonetal.“SophisticatedInference”.In:NeuralComputation33.3(Feb.24,2021),pp.713–
763. ISSN:0899-7667. DOI:10.1162/neco_a_01351.
[32] KarlJ.Friston,JeanDaunizeau,andStefanJ.Kiebel.“ReinforcementLearningorActiveInfer-
ence?”In:PLoSONE4.7(2009). ISSN:19326203. DOI:10.1371/journal.pone.0006421.PMID:
19641614.
[33] Karl J. Friston, Thomas Parr, and Bert de Vries. “The Graphical Brain: Belief Propagation and
Active Inference”. In: Network Neuroscience 1.4 (2017), pp. 381–414. ISSN: 2472-1751. DOI: 10.
1162/NETN_a_00018.
[34] Karl J. Friston et al. “Action and Behavior: A Free-Energy Formulation”. In: Biological Cyber-
netics 102.3 (2010), pp. 227–260. ISSN: 03401200. DOI: 10.1007/s00422-010-0364-z. PMID:
20148260.
[35] KarlJ.Fristonetal.“DeepTemporalModelsandActiveInference”.In:Neuroscience&Biobehav-
ioralReviews90(2018),pp.486–501.ISSN:0149-7634.DOI:10.1016/j.neubiorev.2018.04.004.
[36] Sebastian Gottwald and Daniel A. Braun. “The Two Kinds of Free Energy and the Bayesian
Revolution”. In: PLOS Computational Biology 16.12 (2020), pp. 1–32. DOI: 10.1371/journal.
pcbi.1008420.
[37] ConorHeinsetal.“Pymdp:APythonLibraryforActiveInferenceinDiscreteStateSpaces”.In:
JournalofOpenSourceSoftware7.73(May2022),p.4098. ISSN:2475-9066. DOI:10.21105/joss.
04098.
[38] R.ConorHeinsetal.“DeepActiveInferenceandSceneConstruction”.In:FrontiersinArtificial
Intelligence3(2020),pp.1–23. ISSN:2624-8212. DOI:10.3389/frai.2020.509354.
[39] JakobHohwy.ThePredictiveMind.Oxford,UK:OxfordUniversityPress,2013. ISBN:978-0-19-
968673-5.
[40] Jakob Hohwy. “New Directions in Predictive Processing”. In: Mind and Language 35.2 (2020),
pp.209–223.ISSN:14680017.DOI:10.1111/mila.12281.
[41] AndrewH.Jazwinski.StochasticProcessesandFilteringTheory.NewYork,USA:AcademicPress,
1970. ISBN:978-0-12-381550-7.
[42] RaphaelKaplanandKarlJ.Friston.“PlanningandNavigationasActiveInference”.In:Biolog-
icalCybernetics112.4(2018),pp.323–343.ISSN:14320770.DOI:10.1007/s00422-018-0753-2.
[43] HilbertJ.Kappen,Vicenc¸Go´mez,andManfredOpper.“OptimalControlasaGraphicalModel
InferenceProblem”.In:MachineLearning87.2(May1,2012),pp.159–182.ISSN:1573-0565.DOI:
10.1007/s10994-012-5278-7.
[44] Pablo Lanillos et al. “Active Inference in Robotics and Artificial Agents: Survey and Chal-
lenges”.In:ArXiv211201871v1CsRO(2021),pp.1–20.arXiv:2112.01871v1[cs.RO].
[45] Mark L. Latash. “Efference Copy in Kinesthetic Perception: A Copy of What Is It?” In: Journal
of Neurophysiology 125.4 (Apr. 2021), pp. 1079–1094. ISSN: 0022-3077. DOI: 10.1152/jn.00545.
2020.
[46] Tai Sing Lee and David Mumford. “Hierarchical Bayesian Inference in the Visual Cortex”. In:
Journal of the Optical Society of America A 20.7 (2003), pp. 1434–1448. ISSN: 1084-7529. DOI: 10.
1364/josaa.20.001434.PMID:12868647.
26
Activeinferenceforaction-unawareagents
[47] Sergey Levine. “Reinforcement Learning and Control as Probabilistic Inference: Tutorial and
Review”.In:ArXiv180500909v3CsLG(2018),pp.1–22.arXiv:1805.00909v3[cs.LG].
[48] PietroMazzagliaetal.“TheFreeEnergyPrincipleforPerceptionandAction:ADeepLearning
Perspective”.In:Entropy24.2(2022),pp.1–22.ISSN:1099-4300.DOI:10.3390/e24020301.
[49] Beren Millidge, Anil Seth, and Christopher L Buckley. Predictive Coding: A Theoretical and Ex-
perimental Review. 2021. DOI: 10.48550/arXiv.2107.12979. arXiv: 2107.12979v4 [cs.AI].
Pre-published.
[50] Beren Millidge, Alexander Tschantz, and Christopher L Buckley. Whence the Expected Free En-
ergy?2020.DOI:10.48550/arXiv.2004.08128.arXiv:2004.08128[cs.AI].Pre-published.
[51] Beren Millidge, Alexander Tschantz, and Christopher L. Buckley. “Whence the Expected Free
Energy?”In:NeuralComputation33.2(Feb.1,2021),pp.447–482.ISSN:0899-7667.DOI:10.1162/
neco_a_01354.
[52] M.BerkMirzaetal.“SceneConstruction,VisualForaging,andActiveInference”.In:Frontiers
in Computational Neuroscience 10 (2016), pp. 1–16. ISSN: 16625188. DOI: 10.3389/fncom.2016.
00056.
[53] M. Berk Mirza et al. “Introducing a Bayesian Model of Selective Attention Based on Active
Inference”. In: Scientific Reports 9.1 (2019), pp. 1–22. ISSN: 20452322. DOI: 10.1038/s41598-
019-50138-8.PMID:31558746.
[54] Kevin P. Murphy. Probabilistic Machine Learning: Advanced Topics. Cambridge, Massachusetts:
TheMITPress,2023.
[55] Samuel William Nehrer et al. “Introducing ActiveInference.Jl: A Julia Library for Simulation
and Parameter Estimation with Active Inference Models”. In: Entropy 27.1 (1 Jan. 2025), p. 62.
ISSN:1099-4300.DOI:10.3390/e27010062.
[56] ThomasParrandKarlJ.Friston.“Uncertainty,EpistemicsandActiveInference”.In:Journalof
TheRoyalSocietyInterface14.136(Nov.2017),p.20170376.ISSN:1742-5689.DOI:10.1098/rsif.
2017.0376.PMID:15944135.
[57] ThomasParrandKarlJ.Friston.“GeneralisedFreeEnergyandActiveInference”.In:Biological
Cybernetics 113.5–6 (2019), pp. 495–513. ISSN: 14320770. DOI: 10.1007/s00422-019-00805-w.
PMID:31562544.
[58] ThomasParr,KarlJ.Friston,andTanmayShankar.“TheDiscreteandContinuousBrain:From
Decisions to Movement—and Back Again”. In: Neural Computation 30.9 (2018), pp. 2319–2347.
DOI:10.1162/neco_a_01102.
[59] ThomasParr,GiovanniPezzulo,andKarlJ.Friston.ActiveInference:TheFreeEnergyPrinciplein
Mind,Brain,andBehavior.TheMITPress,2022.ISBN:978-0-262-04535-3.
[60] Thomas Parr et al. “Neuronal Message Passing Using Mean-Field, Bethe, and Marginal Ap-
proximations”.In:ScientificReports9.1(2019),pp.1–18.ISSN:20452322.DOI:10.1038/s41598-
018-38246-3.
[61] ThomasParretal.“PrefrontalComputationasActiveInference”.In:CerebralCortex30.2(2020),
pp.682–695.ISSN:1047-3211.DOI:10.1093/cercor/bhz118.
[62] Giovanni Pezzulo, Thomas Parr, and Karl Friston. “Active Inference as a Theory of Sentient
Behavior”.In:BiologicalPsychology186(Feb.1,2024),p.108741.ISSN:0301-0511.DOI:10.1016/
j.biopsycho.2023.108741.
27
Activeinferenceforaction-unawareagents
[63] Giovanni Pezzulo, Francesco Rigoli, and Karl Friston. “Active Inference, Homeostatic Regu-
lation and Adaptive Behavioural Control”. In: Progress in Neurobiology 134 (2015), pp. 17–35.
ISSN:18735118.DOI:10.1016/j.pneurobio.2015.09.001.PMID:26365173.
[64] GiovanniPezzulo,FrancescoRigoli,andKarlJ.Friston.“HierarchicalActiveInference:AThe-
oryofMotivatedControl”.In:TrendsinCognitiveSciences22.4(2018),pp.294–306.ISSN:1879307X.
DOI:10.1016/j.tics.2018.01.009.PMID:29475638.
[65] StuartRussellandPeterNorvig.ArtificialIntelligence:AModernApproach.4thed.PearsonSeries
inArtificialIntelligence.Pearson,2021.ISBN:978-1-292-15396-4.
[66] Simo Sa¨rkka¨. Bayesian Filtering and Smoothing. Cambridge: Cambridge University Press, 2013.
ISBN:978-1-107-03065-7. DOI:10.1017/CBO9781139344203.
[67] Anil K. Seth and Karl J. Friston. “Active Interoceptive Inference and the Emotional Brain”. In:
PhilosophicalTransactionsoftheRoyalSocietyB:BiologicalSciences371.1708(2016),pp.1–10.ISSN:
14712970. DOI:10.1098/rstb.2016.0007.
[68] RyanSmith,KarlJ.Friston,andChristopherJ.Whyte.“AStep-by-StepTutorialonActiveInfer-
ence and Its Application to Empirical Data”. In: Journal of Mathematical Psychology 107 (Apr. 1,
2022),p.102632. ISSN:0022-2496.DOI:10.1016/j.jmp.2021.102632.
[69] RichardS.SuttonandAndrewG.Barto.ReinforcementLearning:AnIntroduction.AdaptiveCom-
putationandMachineLearning.TheMITPress,2018.ISBN:978-0-262-03924-6.
[70] Emanuel Todorov. “General Duality between Optimal Control and Estimation”. In: 47th IEEE
ConferenceonDecisionandControl.47thIEEEConferenceonDecisionandControl.2008,pp.4286–
4292. DOI:10.1109/CDC.2008.4739438.
[71] Marc Toussaint and Amos Storkey. “Probabilistic Inference for Solving Discrete and Contin-
uous State Markov Decision Processes”. In: Proceedings of the 23rd International Conference on
MachineLearning.ICML’06.NewYork,NY,USA:AssociationforComputingMachinery,2006,
pp.945–952.ISBN:1-59593-383-2.DOI:10.1145/1143844.1143963.
[72] Wanja Wiese and Thomas K. Metzinger. “Vanilla PP for Philosophers: A Primer on Predic-
tive Processing”. In: Philosophy and Predictive Processing. Ed. by Thomas K. Metzinger and
WanjaWiese.FrankfurtamMain:MINDGroup,2017.ISBN:978-3-95857-302-4.DOI:10.15502/
9783958573024.
28
Activeinferenceforaction-unawareagents
Supplementary Material
S1. Mathematical Background
S1.1. Notation
TableS1: Summaryofnotation.
Symbol Meaning
t,τ,T integers,i.e.,generic,current,andterminaltimeindex,respectively
1:t,1:T sequencesoftimesstepsuptotandT,respectively
H integer,lengthofasequenceofactions(i.e.,thepolicyhorizon),ingeneralH ≤T
p integer,thenumberofactionsequences(policies)theagentconsiders
X randomvariablewithsupportinX,andwitht∈[1,T]
t
X ,x sequenceofrandomvariableswithtimeindexandrelatedvalues
1:T 1:T
X jthcolumnofmatrixXorrandomvectorassociatedwiththatcolumn
:,j
P(X),P(x ) probabilitydistributionofrandomvariableX andprobabilitythatX =x (whendefined)
t t t t t
H[X] ShannonentropyofrandomvariableX
t t
Cat(x ) categoricaldistributionwithvectorofparametersx
t t
Dir(x ) Dirichletdistributionwithvectorofparametersx
t t
S finitesetofcardinality|S|,i.e.,thesetofstates
O finitesetofcardinality|O|,i.e.,thesetofobservations
A finitesetofcardinality|A|,i.e.,thesetofactions
AH finitesetofactiontuples(H-foldCartesianproduct)
Π subsetofactionsequences,i.e.,Π⊆AH
(a ,...,a ) elementinAH,shortenedas(a )
1 H 1:H
S,O,A,π categoricalrandomvariableswithsupportinS,O,A,Π,respectively,i.e.,S ∼Cat(s ),...
t t t t t
s,o,a,π elementsinS,O,A,Π,respectively,wherek∈[1,p]andp∈[1,|Π|]
t t t k
s,o,π, columnvectorsofparametersforstate,observation,andpolicyrandomvariables,respectively
t t
s [i],o [i],π[i] ithelementoftheparametervectorforstate,observation,andpolicyrandomvariables,respectively
t t
s,o,a one-hotvectors,i.e.,forsomei∈|S|,s[i]=1,ands[j]=0,∀j̸=i,similarlyforo,a
t t t t t t t
T transitionmap/function
E emissionmap/function
P(s |s,a ) transitionprobabilitydistribution(returnedbyT)
t t t
P(o |s ) emissionprobabilitydistribution(returnedbyE)
t t
P∗(s ),P∗(o ) stationarydistributionsoverS andO,respectively
t t
d functionthatmapsanelementxofastatespaceX toanactiontinA
M generativemodel(collectionofprobabilitydistributions)
A matrixinRn×mstoringparametersofP(O t |S t−1 )(thesameforanyt)
B tensorinR|A|×m×mstoringparametersofP(S t |S t−1 )(thesameforanyt)
Ba1,...,Bad state-transitionmatricesinRm×mforeachavailableaction,d=|A|
F freeenergy
F action-conditionedfreeenergyinvanillaactiveinference
aτ−1
F policy-conditionedfreeenergyinvariationalmessagepassing
πk
G single-stepexpectedfreeenergy
t
G totalexpectedfreeenergy,i.e.,sumofexpectedfreeenergiesforHtimestepsinthefuture
H
∇ F gradientofpolicy-conditionedfreeenergywithrespecttovectorofparameterss
st πk t
∇ F gradientoffreeenergywithrespecttovectorofpolicyparametersπ
π
F ⊺ row-vectorinR1×|Π|ofpolicy-conditionedfreeenergies
π
G
H
⊺ row-vectorinR1×|Π|oftotalexpectedfreeenergy
29
Activeinferenceforaction-unawareagents
S1.2. Categoricalrandomvariable
If S follows a categorical distribution with m categories or values, then the random variable can
t
be realised in m different ways, each having a corresponding probability p, where ∑m p = 1 (the
j=1 j
probabilities sum to one). We can then indicate one such value of S (the state random variable
t
at t) as s and use P(S = s ) for the probability that the random variable takes that value, i.e.,
t t t
P(S = s ) = p for some j ∈ [1,m]. Note that these probabilities are regarded as parameters of the
t t j
⊺
categorical distribution and can be stored in a vector, s = [p ,...,p ] , which allowed us to write
t 1 m
S ∼ Cat(s )inthemaintext.
t t
S1.3. Dirichletdistributionasconjugateprior
The choice of Dirichlet distributions to model the parameters of categorical distributions is not ar-
bitrary. In the context of Bayesian inference, the former are conjugate priors for the latter, meaning
that using a Dirichlet distribution as a prior distribution in the presence of a categorical likelihood
(the other term in the numerator of Bayes’ rule) results in a posterior distribution (the outcome of
Bayesian inference) with the same form as the prior, i.e., a Dirichlet posterior. In other words, this
simplifies the process of inferring posterior parameters. So, for instance, if we consider inference
on A ∼ Dir(α ), this would roughly amount to adjusting the parameters α based on the acquired
:,i i i
observations, resulting in the Dirichlet posterior P∗(A ) := Dir(α∗), where we used the asterisk to
:,i i
identifytheposteriorandthenew,revised,setofDirichletparameters. Foramoredetailedintroduc-
tiontoBayesianinferencewithconjugatepriors[see,e.g.,18,Ch. 6].
S1.4. DerivationoftheFreeEnergyObjective
Given a sequence of observations o ,...,o , the goal of performing Bayesian inference using Eq. (2)
1 T
is to derive the posterior probability of a certain state trajectory of the POMDP, s ,...,s , the most
1 T
probable policy pursued so far, π, and the most probable parameters specifying state-observation
mapping A, and state transitions B. However, these posterior probabilities cannot be computed
analytically using that equation because this would require evaluating the denominator P(O ) =
1:T
∑P(O |S ,π,A,B)P(S ,π,A,B)byconsideringallthepossiblesequencesofobservations,o ,...,o ,
1:T 1:T 1:T 1 T
inrelationtoallpossiblestatesequences,s ,...,s ,allpossiblepolicies,andallcombinationsofma-
1 T
trices’parameters. Thisishoweverusuallycomputationallyintractable: withT = 5ando ,s ∈ [0,8],
t t
i.e.,statesandobservationstakingoneof9possiblevalues,thereare59049observationsequencesto
evaluate by summing 59049 probabilities, each related to one state sequence. In other words, there
wouldbeatotalof590492 valuestobecomputed,andthisisomittingthecombinationswithrespect
topoliciesandmatrices’parameters.
VariationalBayesianinferenceisatechniquetomaketheaboveinferenceproblemmoretractable.
Itinvolvestheintroductionofanapproximateposteriordistribution,Q(·),alsocalledthevariational
posterior,thatoughttobecomes“ascloseaspossible”tothetrueposterior. Thiscanbeachievedby
solving a tractable optimisation problem, thereby avoiding the intractable computations described
above.
The variational posterior is one of the defining elements of the active inference agent (see Defi-
nition2.2)anditiscommonlyindicatedby Q(S ,π,A,B). Tomakethisapproximateposterior“as
1:T
close as possible” to the true one, P(S ,π,A,B|O ), one usually minimises the Kullback-Leibler
1:T 1:T
(KL)divergence, D .
KL
Therefore,wecanwrite(cf. Equation2in[16]):
30
Activeinferenceforaction-unawareagents
(cid:16) (cid:13) (cid:17)
D Q(S ,π,A,B) (cid:13) P(S ,π,A,B|O ) (S1a)
KL 1:T (cid:13) 1:T 1:T
(cid:104) (cid:105)
= E logQ(S ,π,A,B)−logP(S ,π,A,B|O ) ≥ 0 (S1b)
Q 1:T 1:T 1:T
(cid:104) (cid:105)
= E logQ(S ,π,A,B)−logP(S ,π,A,B,O )+logP(O ) (S1c)
Q 1:T 1:T 1:T 1:T
(cid:104) (cid:105)
= E logQ(S ,π,A,B)−logP(O ,S ,π,A,B) +logP(O ), (S1d)
Q 1:T 1:T 1:T 1:T
whereeachexpectation E iswithrespecttoQ(S ,A,B,π)andisshortenedas E [...].
1:T Q
We obtain Eq. (S1b) using the definition of the KL divergence. Having noted that the second
logarithm corresponds to the posterior probability distribution in Eq. (2) (Bayes’ rule), we replaced
it with the right-hand side of that equation and obtain Eq. (S1c). Finally, since logP(O ) does not
1:T
involve variables over which the expectation is computed, we can take it out from the expectation
andarriveatEq.(S1d).
BydefiningthefreeenergyF astheexpectationinEq.(S1d),i.e.:
(cid:104) (cid:105)
F (cid:2) Q(S ,π,A,B) (cid:3) := E logQ(S ,π,A,B)−logP(O ,S ,π,A,B) , (S2)
1:T Q 1:T 1:T 1:T
andbythenon-negativityoftheKLdivergence,itfollowsthatthefreeenergyisanupperbound
onthenegativelogarithmofthesequenceofobservations:
(cid:2) (cid:3)
−logP(O ) ≤ F Q(S ,π,A,B) . (S3)
1:T 1:T
wherethetermontheleft-handsideofEq.(S3)isknownassurprisalandmeasureshowunlikely
asequenceofobservationis.
Thecloserthesurprisalandthefreeenergyaretoeachother,theclosertheKListozero. Thus,if
thegoalistoreducetheKLdivergencebetweenthevariationalposteriorandthetrueposterior,this
canbeachievedbyoptimisingthevariationaldistributions’parametersonwhichF dependssothat
thefreeenergyupperboundisastightaspossible.
Minimising the free energy is the tractable optimisation problem that provides a solution to the
intractable Bayesian inference problem described earlier. Also, the derivation reveals how perform-
ingBayesianinferenceviafreeenergyminimisationinvolvesfindingwaystoincreasethelikelihood
ofobservations(toreducesurprisal)becauseF canbeseenasaproxyforsurprisal. Then,thenotion
that an active inference agent exists insofar as it can avoid unexpected states or observations and
movetowardsdesiredonescanbeunderstoodasaconsequenceoffreeenergyminimisation.
S2. Perception, Planning, and Action Selection via variational message
passing
In active inference, perception, planning, action selection, and learning can be regarded as different
stagesintheprocessofminimisingtheexpressioninEq.(7). Inthenextfewsections,wewillillustrate
theactualupdateequationsusedtoimplementthem(SectionsS2.1toS2.4).
31
Activeinferenceforaction-unawareagents
S2.1. PerceptionasStateEstimation
Since the goal is to minimize Eq. (7), the expectations over the state log-probabilities need to be
minimisedsotoconcentratetheprobabilitymassontoonerealizationofeverystaterandomvariable
S ,...,S , dependingonwhattheactualtrajectoryaffordedbytheconditioningpolicyis(assigning
1 T
equal probabilities to all those realizations would not achieve the minimum of this term and would
misrepresentwhatapolicyreallyachieves). Thus,theminimisationhereconsistsinupdatingthepa-
rametersofthevariationaldistributions Q(S |π ),...,Q(S |π ) ateverytimestep, withanincreas-
1 k T k
ingly longer sequence of collected observations. This is akin to perception since the object of those
operations is to uncover the causes of sensory evidence, i.e., observations, and everything occurs at
afasttimescale,i.e.,ateverytimestep. Perceptionisthusframedastheupdateoftheparametersof
thevariationalprobabilitydistributionsQ(S |π ),...,Q(S |π ),accordingtotheavailable,collected
1 k T k
(cid:2) (cid:3)
evidence,withthegoalofminimisingF Q(S |π ) (seeEq.(7)).
πk 1:T k
To obtain the update equations for the collection of parameters s := s ,...,s , where each s
1:T 1 T t
is the vector of probabilities defining Q(S |π ), we rewrite F [Q(S |π )] in vectorised form, by
t k πk 1:T k
substitutingthevectorofparametersforthevariousprobabilitydistributions, thenwecomputethe
gradientswithrespecttoeachofthosevectors,namely:
 o ⊺ ·Sα+s ⊺ ·logBat +logs ⊺ if t = 1
∇ st F πk (s 1:T ) = 1+logs ⊺ t −   
   
o
s ⊺ t
⊺ t t
+
·
1
S
·
α
lo
+
g
s
B
⊺ t t
a
+ +
t
1 1
+
·
(cid:104)
l
(
o
l
g
og
B
B
at+
at−
(cid:104)
1
(
)
l
·
o
s
g
t
1
−
B
1
a
(cid:105)
t−
⊺
1)·s t−1 (cid:105)⊺
i
i
f
f 1
t >
<
τ
t ≤ τ (S4)
where:
⊺ ⊺
• o isthetransposed(one-hot)observationvector,i.e.,o [i] = 1ificorrespondstotheobservation
t t
category(value)ofO observedatt;
t
• Bat is the transition matrix for the action a , with j ∈ [1,|A|], that the policy π mandates at
j k
timestep t, notethatweindicatesuchactionby at, omittingthereferencetothepolicyandthe
subscript jtoavoidtoomuchnotationalclutter;
(cid:16) (cid:17) (cid:16) (cid:17)
• Sα := ψ [α ] −ψ J ·[α ] ,where:
1:m n,m 1:m
– α := α ,...,α arethecolumnvectorsofDirichletparametersforA(oneforeachcolumn,
1:m 1 m
see Section 3.2), and with [α ] ∈ Rn×m representing the matrix whose columns are those
1:m
vectors;
– ψ([α ]) is the digamma function applied element-wise to the matrix of Dirichlet param-
1:m
eters whereas ψ(J ·[α ]) is the digamma function applied to the result of the matrix
n,m 1:m
multiplication between the matrix of ones J and [α ] (note that a column j of the re-
n,m 1:m
⊺
sulting matrix is filled with the same value, namely, the dot product or sum 1 α , usually
j
indicatedbyα ,cf.[16])6;
0
6ThedifferencebetweendigammafunctionsappearsbecauseofthetermE Q(St |πk )Q(A) [logP(ot |St,A)]. Ifweconsider
thegradientofthefreeenergywithrespecttotheelementst [i]oftheparametervectorst,thenweobtaintheexpression
E di Q st ( r A ib :,i ) u [ t l e o d gA ra : n ,i ] d , o a m ssu v m ec i t n o g r w w e hi d c i h d i n s o e t q k u n a o l w to t ψ h ( e α v ) al − ue ψ o (α fO ) t w . T h h er a e te α xp i r s e t s h si e on ve i c s to th r e w e h x o p s e e ct e a l t e i m on en o t f s th ar e e lo a g ll o e f qu a a D l i t r o ic 1 h ⊺ le α t- .
i 0 0 i
ConsideringalltheelementsofstgivesusthematrixSαwhichisvector-multipliedbyo ttoarriveatthecorrectexpression
forthegradient.
32
Activeinferenceforaction-unawareagents
• finally, · stands for the inner product, log is applied element-wise to both the elements of vec-
tors and matrices, and the vectors of parameters are now transposed because we consider the
numeratorlayoutwhentakingthegradientofavector-valuedfunction7.
By setting the above gradients to zero, we can derive analytically the new unnormalised parame-
tersofthevariousQ(S |π )thatminimizefreeenergy,thatis:
t k
 o ⊺ ·Sα+s ⊺ ·logBat +logs ⊺ −1 if t = 1
logs ⊺ t =    o ⊺ t t ·Sα+s ⊺ t t + + 1 1 ·logBat+ (cid:104) (log 1 Bat−1)·s t−1 (cid:105)⊺ −1 if 1 < t ≤ τ (S5)
   s ⊺ t+1 ·logBat+ (cid:104) (logBat−1)·s t−1 (cid:105)⊺ −1 if t > τ
Since these parameters stand for probabilities defining categorical probability distributions, we
needtoimposethateachsetofparameterssumstoone. Thisisusuallydonebyapplyingthesoftmax
function σ(·) totheexpressionsinEq.(S5)(analternativemethodistosetupthewholeproblemas
oneofconstrainedoptimizationanduseLagrangemultipliers).
In active inference studies that aim to describe neuronal dynamics as a form of gradient descent
onfreeenergy,wherethegradientcanbeconsideredaspredictionerror,thefollowingupdateruleis
usedinstead.
s := σ(s −∇ F ), (S6)
t t st πi
where again the softmax function σ(·) is used to make sure the parameters represent legitimate
probabilitydistributions.
S2.2. PlanningwithExpectedFreeEnergy
The minimization of the free energy with respect to the policy random variable, π , also occurs at
k
every time step and can be associated with the cognitive operations of planning, but it requires a
(cid:2) (cid:3)
separatetreatmentthatconsiderstheexpectationover F Q(S |π ) (lasttermofEq.(6))andthe
πk 1:T k
expectedfreeenergy.
Oncetheagenthasupdateditsprobabilisticbeliefsaboutthepast,present,andfuturestatesvari-
ables, it can proceed to update its probabilistic beliefs over the set of policies. This is achieved by
predicting what is most likely to happen if a certain policy is followed and by scoring each policy
depending on whether it would result in a desired sensorimotor trajectory. An updated probability
distribution over the policies can then be paired with a decision rule d (see Definition 2.2) to deter-
minewhatactiontheagentwillperformatthenexttimestep.
For each policy, this process involves computing an expected free energy, one for each future time
stepofthepotentialtrajectorythatthepolicymightrealize:
7The numerator layout specifies the order in which to compute the partial derivatives of a vector-valued function,
resultinginaJacobianmatrixwhosenumbersofcolumnsandrowscorrespondtothenumberoffunction’sinputsand
outputs,respectively. Givenafunctionf : Rn → Rm thatmapsavectorx ∈ Rn toavectory ∈ Rm,themelementsin
ydefinetherowsoftheJacobianwhereasthenelementsinxdefineitscolumns,i.e.,J ∈ Rm×n. Forinstance,ifwehave
thefunction f : Rn → R1,thentheJacobianisarowvector,i.e.,∇ xf = [∂f(x)/∂x1 ,...,∂f(x)/∂xn ],wherex
1
,...,xn arethe
elementsofx. Inthemaintext,whenthefreeenergyisconsideredwithrespecttooneofitsvectorsofparameters,e.g.st,
itcanbeseenpreciselyasavector-valuedfunction,F
πk
(st ):Rn →R,withtheJacobianbeingarowvector.
33
Activeinferenceforaction-unawareagents
(cid:104) (cid:105) (cid:104) (cid:105)
G (π ) =E H(cid:2) P(O |S ) (cid:3) −E D (cid:2) Q(A|o ,s )|Q(A) (cid:3)
t k Q(St |πk ) t t P(Ot |St )Q(St |πk ) KL t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
AMBIGUITY A-NOVELTY
(S7)
(cid:104) (cid:105)
+D KL (cid:2) Q(S t |π k )|P ∗(S t ) (cid:3) −E Q(St+1 |πk )Q(St |πk ) D KL (cid:2) Q(B|s t+1 ,s t )|Q(B) (cid:3) .
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125)
RISK
B-NOVELTY
In other words, each of these expected free energies can be approximately regarded as the free
energy most likely to be registered at a future time step, if the actions of the conditioning policy are
performeduptothatpoint(seeMillidge,Tschantz,andBuckley[51]forwhy,technically,describing
theexpectedfreeenergyinthiswayisnotentirelycorrect,butnonethelesscommonintheactivein-
ferenceliterature). Thesumoftheseexpectedfreeenergiesisindicativeofhowmuchtheconsidered
policy would allow the agent to reduce uncertainty and achieve a preferred distribution over states
atsomepointinthefuture.
The total expected free energy for policy π , G (π ), is then defined as the sum of expected free
k H k
energiesatfuturetimestepsuptothepolicyhorizon, H,i.e.:
H
∑
G (π ) = G (π ). (S8)
H k t k
t=τ+1
The different terms that constitute the expected free energy tend to be associated with distinct
behaviouraldrives.
Theambiguitytermistheexpectedvalueoftheentropy, H·,ofthestate-observationmappings,
and quantifies the uncertainty about future outcomes given hidden states. A policy for which this
termislowisapolicythatdrivestheagenttowardsunambiguouspartsoftheenvironment.
Therisktermintroducesanotherdefiningcomponentoftheactiveinferenceagent(Definition2.2),
i.e., a probability distribution that specifies its preference(s) or goal(s), expressed in this case over
state variables. In other words, this is a probability distribution with the probability mass concen-
trated on one or a few states, representing states in the environment to which the agent is moving.
TheKLdivergencebetweentheexpectedstatesinthefutureandtheseagent’spreferencesoverstates
quantifieshowmuchthepolicywillallowtheagenttogetthere,therebyachievingitsgoal(s).
The two novelty terms are expected values of KL divergences between the posterior and prior
distributionsovergenerativemodel’sparameters,i.e.,thoseofstate-observationmappingsandtran-
sitionprobabilities. Sincetheyaresubtractedfromtheexpectedfreeenergyandtheagentislooking
forapolicythatminimisesit, theagentispushedtopickapolicywhoseactionsmayincreasethese
terms, i.e., leading to environmental consequences that were not very well captured by the current
generativemodel. Therefore, thesetermsareinterpretedascapturingtheexploratorydrivesofanac-
tiveinferenceagentinsofarastheyscoreapolicybasedonhowlikelyitwilldiscloseas-yetunknown
parts of the environment, which may be informative about state-observation mappings and transi-
tion probabilities. In other words, a policy for which these terms are very high will provide new
informationtotheagent,notalreadyencodedinitsgenerativemodel.
To understand how the computation of expected free energy fit in the process of free energy
minimisation, i.e., of minimising the free energy in Eq. (6), we take the gradient of that expression
withrespecttotheparametersπ ofQ(π),obtaining:
⊺ ⊺ ⊺
∇ F[Q(π)] = lnπ −lnπ +F +1, (S9)
π Q P π
34
Activeinferenceforaction-unawareagents
⊺ ⊺ ⊺
where π and π are the row vectors of parameters of Q(π) and P(π), respectively, and F is
Q P π
therowvectorofpolicy-conditionedfreeenergies(oneforeachpolicy,i.e.,foreachvaluethepolicy
randomvariablecantake,seeEq.(7)).
As done earlier, setting the above gradient to zero gives us the new, unnormalised vector of pa-
rametersforQ(π):
⊺ ⊺ ⊺
lnπ = lnπ −F −1. (S10)
Q P π
Again, to obtain a proper, normalised probability distribution, the softmax map is applied. For
this update equation there is a further issue, as we have not clarified what the (column) vector of
parametersπ represents. ThisvectorstorestheprobabilitiesthatdefinethedistributionP(π)which
P
getscomparedtoQ(π)inEq.(6)bymeansoftheKLdivergence. Therefore,itcanberegardedasthe
priorprobabilitydistributionoverthepoliciesprovidedbytheagent’sgenerativemodel. Essentially,
the above equation says that in order to update the probabilistic beliefs about the policy random
variable, the current evidence represented by the free energy values has to be integrated with the
agent’s prior beliefs (this is in line with the notion of Bayesian inference, and it holds true for the
othervariationalupdatesaswell).
Thecrucialquestionishowthatpriorshouldbespecified. Accordingtoactiveinference,thean-
swerisprovidedbyexpectedfreeenergyinsofarasitmanagestobalanceinstrumentalandepistemic
valueswhenitcomestopolicyandactionselection(offeringasolutiontotheexploration-exploitation
dilemma). In particular, we have that π := σ(−G ), where G is the vector of expected free en-
P H H
ergies (one for each policy, i.e., for each value the policy random variable can take), giving us the
followingupdaterule:
⊺ ⊺ ⊺
π = σ(−G −F ), (S11)
Q H π
where the constant 1 can be dropped as it does not affect the softmax function (whether using
expectedfreeenergyrepresentsaprincipledwayofspecifyingthatpriorhasbeenasubjectofdebate,
see[50])8.
Theaboveparameterscanalsobeusedtoupdatethepolicy-independentstateprobabilities,that
is:
|Π|
∑
Q(S ) = Q(S |π )Q(π ), (S12)
τ τ k k
k=0
where the marginal probabilities over the state random variable at τ are computed using the
updated policy probabilities. These marginal probabilities provide an indication of what the agent
believesarethemostprobablestatevaluesatacertainpointinthetrainingprocess. Aslearningpro-
gresses,theyshouldconvergetotheprobabilitiesrepresentingtheagent’spreferences(overstates).
After obtaining an approximate posterior probability distribution over policies, the agent can
proceedtoimplementanactionselectionprocedure,whichwillbedescribednext.
S2.3. ActionSelection
Withupdatedprobabilisticbeliefsonpastandfuturesensorimotortrajectories,affordedbydifferent
policies, and a new probability distribution over them, the agent is equipped to select an action to-
8In some active inference works, an additional prior term is included in the softmax, i.e., a probability distribution
representingpreferencesforoneormore“habitual”policy/policies,see,e.g.,[37,p.33].
35
Activeinferenceforaction-unawareagents
wardsthegoalstate. Thedecisionruledofanactiveinferenceagentimplementstheaction-selection
mechanismwhichcantakemanyforms.
One strategy is to pick the policy with the highest probability and perform one of its actions,
sincetheobjectiveoftheagentistominimisefreeenergynowandinthefuture,andthelook-ahead
operations with expected free energy ultimately scored the different policies based on that require-
ment. This could be described as a kind of greedy strategy that priorities executing whatever policy
was deemed to be more conducive to low free energy states in the future. In this case, the decision
rule would be a function d : Π×{1,...,T} → A (see Definition 2.2) that maps from the Cartesian
productbetweenthepolicyspaceandthesetoftimeindicestotheactionspace,inasuchawaythat
attimestepτ theagentwillpickthefollowingaction:
a
t
= d(π∗,t) = π∗ [t] (S13)
i.e., simply, this decision function returns the action that the policy chosen as input (the most
probableinthiscase)specifiesforthattimestep(recallthateachpolicyisasequenceoftime-indexed
actions). Alternatively, the policy that goes into the decision function could be sampled from Q(π),
addingsomerandomnessintotheactionselectionprocedure.
The above strategies narrowly focus on the action specified by a single policy. That is, once a
policyhasbeenpicked,theactionthatthepolicydictatesisgoingtobeperformedregardlessofwhat
theotherpoliciessuggest. Thismayturnouttobeasuboptimalwayofselectingthenextactionifin
thecurrentlearningphasetheagentdoesnothave(yet)ahighlyprobablepolicy. Forinstance,ifthe
selected policy is only slightly more probable thanthe others, and all theseagree on the sameaction
to perform, then it might be better to perform the action backed by most policies than the action
indicated by the most probable one. In light of the above considerations, in Da Costa et al. [16] and
inthefollowingexperiments, theagentpicksthemostlikelyactionunderallpossiblepolicies. This
specificactionisfoundbycomputingaBayesianmodelaverage,thatis:
(cid:32) (cid:33)
a t = d(Π ,t) = argmax ∑ δ a,c πk Q(π k ) , (S14)
a∈A πk ∈Π t
where, given a candidate action a from the set of possible actions A and for every policy π ,
k
k ∈ [1,...,p], we sum the products δ a,c πk Q(π k ). The first factor of those products is the Kronecker
t
deltabetweenthecandidateaction a andtheactionthattheconsideredpolicydictatesattimestep t,
indicatedby c π t k. Ifthesetwoactionsareequal,i.e., a = c π t k,then δ a,c πk = 1,andzerootherwise(this
t
isthedefinitionoftheKroneckerdeltafortwovariables). Thesecondfactoristheprobabilityofthe
givenpolicy.
Thus, for every action we are going to compute a sum of policy probabilities, according to how
many policies would suggest to perform that action at the current time step. The action with the
highest sum of probabilities will be selected. If an action is not dictated by any policy, then the
Kronecker delta will make it the worst possible candidate for what to do at time step t. If the same
action is dictated by very likely policies, then that will be a good candidate. In other words, this
procedure recommends to pick the action that most policies agree upon at the current time step as
longasthosepoliciesorasubsetthereofhaveahighprobability.
36
Activeinferenceforaction-unawareagents
S2.4. LearningasEvidenceAccumulation
The second term in Eq. (7), the negative sum of expectations of the observation log-likelihoods,
involveslog-probabilitiesoftheobservationvaluescollecteduntilthepresenttimestepτ. Intuitively,
the minimization of this term will occur when a high probability value for the current observation
is matched by a high probability for that state value that truly generated the observation in the first
place. Concretely, if the agent strongly believes that S = s , then it should be the case that the
t t
observation at t reflects that, i.e., that O = o and o = s , meaning that the categorical values of
t t t t
both random variables are equal (recall that for the categorical random variables in question s and
t
o amount to indices that identify one of state/observation categories). In contrast, for all the other
t
realizations of the same state random variable, the probability of receiving that observation should
below(unlessthereareenvironmentalstatesthatadmitofidenticalobservations).
The last term is the negative sum of expectations of the transition log-likelihoods. This term
captures how well the agent is modelling the transition dynamics of the environment. That is, if
performingtheactionpolicyπ k dictatesatt−1,fromstates t−1 ,leadstostates t atthenexttimestep
t,thenthecorrespondingprobabilitythatS t = s t givenS t−1 = s t−1 (andtheexecutionofthataction)
should be high (in other words, in expectation the agent should assign high probabilities to those
statetransitionsthatcharacterizeacertainsequenceofaction).
Theoptimizationofbothlog-likelihoodsrequiresupdatingtheparametersstoredintheA-matrices
and B-tensors, respectively. Thiscanalsobedoneateverytimestepbuttheimpactoftheupdateis
ingeneralsmallbecausethecollectedobservationwillaffectmostlyasinglestate-observationmap-
ping and state transitions, i.e., the one involving the present time step. Therefore, the update of
A-matrices and B-tensors occurs at a slower time scale as the agent acquires experience about the
most common state-observation mappings and state transition in the given environment. For this
reason, these computational operations have been regarded as a form of learning, i.e., adaptation
thatrequirelongertime(concretely,inimplementationstheupdateoftheseparametersiscarriedout
inbatchattheendofanepisodeortrajectory).
After the agent has collected a full trajectory of observations (e.g., at the end of an episode),
learning consists in updating the parameters of the emission and transition maps, stored in the ob-
servationmatrixAandinthetransitiontensorB,respectively(crucialcomponentsofthegenerative
modelusedtomodeltheenvironment,seeSection2).
To derive the update rules, one starts again with the free energy introduced in Eq. (6) and takes
its gradient with respect to each of the Dirichlet distributions P(A ),...,P(A ), defined on the
:,1 :,m
random vectors associated with the corresponding columns of the observation matrix, and each of
the Dirichlet distributions P(B ai ),...,P(B ai ), ∀i ∈ [1,...,|A|], defined on the random vectors as-
:,1 :,m
sociated with the corresponding columns of the various matrices forming the transition tensor. The
derivation requires considering the KL divergence between many pairs of Dirichlet distributions.
(cid:2) (cid:3)
For instance, the KL term D Q(A)|P(A) in Eq. (6) is a more compact way of writing those KL
KL
divergences,thatis:
m
D (cid:2) Q(A)|P(A) (cid:3) = ∑ D (cid:2) Q(A Q)|P(AP) (cid:3) , (S15)
KL KL :,i :,i
i=1
where A Q ∼ Dir(α Q) and AP ∼ Dir(αP), and the superscripts P and Q are used to indicate
:,i i :,i i
that we are dealing with different distributions and parameters, i.e., the prior and the posterior,
respectively.
Forreference,westatetheupdateruleshereasfollows:
37
Activeinferenceforaction-unawareagents
T
α Q := αP + ∑ o ⊙s [i] (S16)
:,i :,i t τ
t=1
T
β : Q ,i := β : P ,i + ∑ ∑ δ a,cπ t Q(π) (cid:0) sπ t ⊙sπ t−1 [i] (cid:1) (S17)
t=2πk ∈Π
∀i ∈ [1,m], where recall: s is the vector of parameters for Q(S |π ); o is a one-hot vector indi-
t t k t
catingthecategoryofobservationthathasbeenacquiredatthattimestep;andthesymbol⊙isused
toindicatetheelement-wise(orHadamard)productbetweenavectorandtheithelementofanother
vector(foraderivationoftheaboverulesreferto[16]).
To gain some insight on these update rules, first notice that they return the parameters of (ap-
proximate) posterior Dirichlet distributions (indicated by superscript Q) through an adjustment of
the prior parameters (indicated by superscript P). Crucially, this revision of the prior parameters is
madeusingtheobservationsandthe(updated)state-variablebeliefsobtainedfrominteractingwith
theenvironmentfor T timesteps.
More specifically, in Eq. (S16) the value of an element in αP is increased by the probability rep-
:,i
resented by s [i]. The particular value in αP which is updated depends on the location of the 1 in
t :,i
the one-hot vector o . This captures the idea that if a certain observation value, say, o , is repeatedly
t t
acquired at t, and the probability s [i] associated with the ith value of S is large, then the agent has
t t
some reasons to consider that observation as more likely when it is somewhat confident of being in
state S = s . In a nutshell, the probability P(O = o |S = s ) should be increased proportionally to
t t t t t t
theprobabilitythatS = s attwheno hasbeenregisteredfromtheenvironment.
t t t
Since the Dirichlet parameters αP are used to sample the values in A , which are in turn the
:,i :,i
parameters of the categorical distribution P(O |S = s ), adjusting the Dirichlet parameters using
t t t
theaboverulehasthedesiredlearningeffect,i.e.,thatofimprovingonthecurrentstate-observation
mappingbycapturingwhatarethemostlikelyobservationalconsequencesofbeinginvariousstates.
AsimilarreasoningappliestotheDirichletparametersinEq.(S17),wherethistimethekeyevi-
denceisrepresentedbytheamountofstatetransitionsofacertaintypethathavebeenencountered.
Forinstance,themorestatetransitionshavebeenobservedfromstate s t−1 tostate s t uponperform-
ingaction at,themorethecorrespondingprobabilities P(S = s |S = s ) inmatrix Bat shouldbein-
t t t t
creased. Oneofthesubtletieshereisthattheseprobabilityupdatesshouldbeweightedaccordingto
howlikelyitisthatanactionwasindeedperformedtorealisethatstatetransition,whichisachieved
withtheKroneckerdeltaterm,δ a,cπ
τ
Q(π),intheequation,i.e.,byconsideringtheprobabilityofthose
policiessuggestingthatactionatt.
The accumulation of experience throughout an episode of interaction with the environment al-
lows the agentto update its modelof what the most probable observations and state transitionsare
in that environment. These update rules establish a learning dynamics that is supposed to occur at
a much slower temporal scale than perceptual inference, planning, and action selection so they are
generally implemented at the end of an episode and/or a sequence of observation, and they have
beendescribedasaformofsynapticHebbianplasticity[see,e.g,29,33,16].
Insummary,duringanepisodeoflengthT,anactiveinferenceagentgoesthroughaphaseofper-
ceptualinference,planning,andactionselectionateachtimestepwhereasattheendoftheepisode
itcapitalisesontheacquiredexperiencethroughalearningphase,beforeanewepisodebegins. This
active inference cycle is summarised in algorithm S1 and algorithm S2 for the action-unaware and
action-awareagent,respectively.
38
Activeinferenceforaction-unawareagents
AlgorithmS1: Action-unawareActiveInference
Hyperparameters:: T,epidodelength, N,numberofepisodes, Tmax = T×N,max
numberofsteps,p = |Π|,numberofpolicies.
Data: sensoryobservations,o ,...,o ,andpolicies,π ...π .
1 t 1 p
Result: updatedQ(S |π ),...,Q(S |π ),∀k ∈ [1,p].
1 k T k
Result: updatedQ(π),andnextaction, a .
τ
Result: updatedQ(A)andQ(B)
1 fort ∈ [1,...,Tmax]do
2 1. PerceptualPhase:
3 fork ∈ [1,...,p]do
4 a. Updateprobabilitiesoverstates:
5 fort ∈ [1,...,T]do
6 ∇ st F πk = 0 ⇒ s t := σ(lns t ),seeEq.(S5)
7 endfor
8 endfor
9 2. PlanningPhase:
10 fork ∈ [1,...,p]do
11 a. Computetotalexpectedfreeenergy:
12
G
H
(π
k
) = ∑
t
T
=τ+1
G
t
(π
k
)
13 endfor
14 b. Updateprobabilitiesoverpolicies:
⊺ ⊺
15 π = σ(−G H −F π )
16
Q(π) ∼ Cat(π)
17 3. ActionSelectionPhase:
18 a t = d(Π ,t)
19 endfor
20 4. LearningPhase:
21 ift mod T = 0then
22 fori ∈ [1,...,m]do
23 α : Q ,i := α : P ,i +∑ t T =1 o t ⊙s τ [i]
24 β : Q ,i := β : P ,i +∑ t T =2 ∑ πk ∈Πδ a,cπ t Q(π) (cid:0) sπ t ⊙sπ t−1 [i] (cid:1)
25 endfor
26 5. Reset(beforeanewepisodestarts):
27
ResetQ(S
1
|π
k
),...,Q(S
T
|π
k
),∀k ∈ [1,p]touniformprobabilitydistributions.
28 endif
39
Activeinferenceforaction-unawareagents
AlgorithmS2: Action-awareActiveInference
Hyperparameters:: T,epidodelength, N,numberofepisodes, Tmax = T×N,max
numberofsteps,p = |Π|,numberofpolicies.
Data: sensoryobservations,o ,...,o ,andpolicies,π ...π .
1 t 1 p
Result: updatedQ(S
t
|a
t−1
),∀t ∈ [1,...,τ]andQ(S
τ+1
|π
k
),...,Q(S
T
|π
p
),
∀k ∈ [1,...,p].
Result: updatedQ(π),andnextaction, a .
τ
Result: updatedQ(A)andQ(B)
1 fort ∈ [1,...,Tmax]do
2 1. PerceptualPhase:
3 a. Updateprobabilitiesoverstates:
4 fort ∈ [1,...,T]do
5 ift < τ then
6 ∇ st F (a
1:τ−1
) = 0 ⇒ s t := σ(lns t ),seeEq.(S5)
7 else
8 fork ∈ [1,...,p]do
9 ∇ st F πk = 0 ⇒ s t := σ(lns t ),seeEq.(S5)
10 endfor
11 endif
12 endfor
13 2. PlanningPhase:
14 fork ∈ [1,...,p]do
15 a. Computetotalexpectedfreeenergy:
16
G
H
(π
k
) = ∑
t
T
=τ+1
G
t
(π
k
)
17 endfor
18 b. Updateprobabilitiesoverpolicies:
⊺ ⊺
19 π = σ(−G H −F (a
1:τ−1
) )
20
Q(π) ∼ Cat(π)
21 3. ActionSelectionPhase:
22 a t = d(Π ,t)
23 endfor
24 4. LearningPhase:
25 ift mod T = 0then
26 fori ∈ [1,...,m]do
27 α : Q ,i := α : P ,i +∑ t T =1 o t ⊙s τ [i]
28 β : Q ,i := β : P ,i +∑ t T =2 ∑ πk ∈Πδ a,cπ t Q(π) (cid:0) sπ t ⊙sπ t−1 [i] (cid:1)
29 endfor
30 5. Reset(beforeanewepisodestarts):
31
ResetQ(S
1
|π
k
),...,Q(S
T
|π
k
),∀k ∈ [1,p]touniformprobabilitydistributions.
32 endif
40
Activeinferenceforaction-unawareagents
S3. Further Information on Experiments
S3.1. Experiment1: 4-stepT-maze
S3.1.1. HowtoReproducetheResultsoftheExperiment
TheresultsreportedinSection4.1wereobtainedbyusingthefollowingcommandlinearguments.
Fortheaction-unawareagent:
1 main_aif_paths --exp_name aif_paths --gym_id gridworld-v1 --env_layout
tmaze4 --num_runs 10 --num_episodes 100 --num_steps 4 --inf_steps
10 --action_selection kd -lB --num_policies 64 --pref_loc all_goal
Fortheaction-awareagent:
1 main_aif_plans_pi_cutoff --exp_name aif_plans --gym_id gridworld-v1 --
env_layout tmaze4 --num_runs 10 --num_episodes 100 --num_steps 4 --
inf_steps 10 --action_selection kd -lB --num_policies 64
Theplotswereobtainedusingthefollowingcommandlineinstructions:
1 vis_aif -gid gridworld-v1 -el tmaze4 -nexp 2 -rdir
episodic_e100_pol16_maxinf10_learnB -fpi 0 1 2 3 -i 4 -v 8 -ti 4 -
tv 8 -vl 3 -hl 3 -xtes 20 -ph 3 -selrun 0 -selep 24 49 74 99 -npv
16 -sb 4 -ab 0 1 2 3
Withtheseinstructions, onecanvisualisemoremetricsthanthosereportedinthemaintext. We
offeraselectionnext.
S3.1.2. Freeenergyatsteps1-3
3.0
2.5
2.0
1.5
1.0
0.5
0.0
1 20 40 60 80 100
Episode
ygrene
eerF
Free energy at step 1
(action-unaware)
3.0
2.5
2.0
1.5
1.0
0.5
0.0
1 20 40 60 80 100
Episode
(a)Freeenergy.
ygrene
eerF
Free energy at step 1
(action-aware)
(b)Freeenergy.
Fig.S1: Freeenergyatstep1acrossepisodes(showingaverageof10agents).
41
Activeinferenceforaction-unawareagents
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
1 20 40 60 80 100
Episode
ygrene
eerF
Free energy at step 2
(action-unaware)
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
1 20 40 60 80 100
Episode
(a)Freeenergy.
ygrene
eerF
Free energy at step 2
(action-aware)
(b)Freeenergy.
Fig.S2: Freeenergyatstep2acrossepisodes(showingaverageover10agents).
6
5
4
3
2
1
0
1 20 40 60 80 100
Episode
ygrene
eerF
Free energy at step 3
(action-unaware)
6
5
4
3
2
1
0
1 20 40 60 80 100
Episode
(a)Freeenergy.
ygrene
eerF
Free energy at step 3
(action-aware)
(b)Freeenergy.
Fig.S3: Freeenergyatstep3acrossepisodes(showingaverageof10agents).
42
Activeinferenceforaction-unawareagents
S3.1.3. Policy-conditionedfreeenergyatsteps1-3
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
1 20 40 60 80 100
Episode
ygrene
eerF
Policy-conditioned free energy at step 1
(action-unaware)
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
1 20 40 60 80 100
Episode
ygrene
eerF
Policy-conditioned free energy at step 1
(action-aware)
Policies
1: , , 5: , , 9: , , 13: , ,
2: , , 6: , , 10: , , 14: , ,
3: , , 7: , , 11: , , 15: , ,
4: , , 8: , , 12: , , 16: , ,
Fig.S4: Policy-conditionedfreeenergiesatstep1acrossepisodes(showingaverageof10agents).
8
7
6
5
4
3
2
1
0
1 20 40 60 80 100
Episode
ygrene
eerF
Policy-conditioned free energy at step 2
(action-unaware)
8
7
6
5
4
3
2
1
0
1 20 40 60 80 100
Episode
ygrene
eerF
Policy-conditioned free energy at step 2
(action-aware)
Policies
1: , , 5: , , 9: , , 13: , ,
2: , , 6: , , 10: , , 14: , ,
3: , , 7: , , 11: , , 15: , ,
4: , , 8: , , 12: , , 16: , ,
Fig.S5: Policy-conditionedfreeenergiesatstep2acrossepisodes(showingaverageof10agents).
43
Activeinferenceforaction-unawareagents
8
7
6
5
4
3
2
1
0
1 20 40 60 80 100
Episode
ygrene
eerF
Policy-conditioned free energy at step 3
(action-unaware)
8
7
6
5
4
3
2
1
0
1 20 40 60 80 100
Episode
ygrene
eerF
Policy-conditioned free energy at step 3
(action-aware)
Policies
1: , , 5: , , 9: , , 13: , ,
2: , , 6: , , 10: , , 14: , ,
3: , , 7: , , 11: , , 15: , ,
4: , , 8: , , 12: , , 16: , ,
Fig.S6: Policy-conditionedfreeenergiesatstep3acrossepisodes(showingaverageof10agents).
S3.1.4. Expectedfreeenergyatsteps2–3
8
7
6
5
4
3
2
1
0
1 20 40 60 80 100
Episode
ygrene
eerf
detcepxE
Expected free energy at step 2
(action-unaware)
8
7
6
5
4
3
2
1
0
1 20 40 60 80 100
Episode
ygrene
eerf
detcepxE
Expected free energy at step 2
(action-aware)
Policies
1: , , 5: , , 9: , , 13: , ,
2: , , 6: , , 10: , , 14: , ,
3: , , 7: , , 11: , , 15: , ,
4: , , 8: , , 12: , , 16: , ,
Fig.S7: Expectedfreeenergyatstep2foreachpolicyacrossepisodes(showingaverageof10agents).
44
Activeinferenceforaction-unawareagents
3.0
2.5
2.0
1.5
1.0
0.5
0.0
1 20 40 60 80 100
Episode
ygrene
eerf
detcepxE
Expected free energy at step 3
(action-unaware)
3.0
2.5
2.0
1.5
1.0
0.5
0.0
1 20 40 60 80 100
Episode
ygrene
eerf
detcepxE
Expected free energy at step 3
(action-aware)
Policies
1: , , 5: , , 9: , , 13: , ,
2: , , 6: , , 10: , , 14: , ,
3: , , 7: , , 11: , , 15: , ,
4: , , 8: , , 12: , , 16: , ,
Fig.S8: Expectedfreeenergyatstep3foreachpolicyacrossepisodes(showingaverageof10agents).
Thereisnoexpectedfreeenergyatstep4becausethisisthestepatwhichtheenvironmentterminates
in the episodic setting considered in this work and, regardless of its location, the agent is no longer
giventheabilitytoplanforwardintime.
45
Activeinferenceforaction-unawareagents
S3.1.5. Expectedfreeenergyatstep1breakdown
12
10
8
6
4
2
0
1 20 40 60 80 100
Episode
ksiR
Risk at step 1 (action-unaware)
12
10
8
6
4
2
0
1 20 40 60 80 100
Episode
ksiR
Risk at step 1 (action-aware)
Policies
1: , , 5: , , 9: , , 13: , ,
2: , , 6: , , 10: , , 14: , ,
3: , , 7: , , 11: , , 15: , ,
4: , , 8: , , 12: , , 16: , ,
Fig.S9: Risk(expectedfreeenergyterm)foreachpolicyacrossepisodes(showingaverageof10agents).
1.0
0.8
0.6
0.4
0.2
0.0
1 20 40 60 80 100
Episode
ytlevon-B
B-novelty at step 1 (action-unaware)
1.0
0.8
0.6
0.4
0.2
0.0
1 20 40 60 80 100
Episode
ytlevon-B
B-novelty at step 1 (action-aware)
Policies
1: , , 5: , , 9: , , 13: , ,
2: , , 6: , , 10: , , 14: , ,
3: , , 7: , , 11: , , 15: , ,
4: , , 8: , , 12: , , 16: , ,
Fig.S10: B-novelty(expectedfreeenergyterm)foreachpolicyacrossepisodes(showingaverageof10agents).
46
Activeinferenceforaction-unawareagents
S3.1.6. Groundtruthtransitionmaps
1
2
3
4
5
1 2 3 4 5
States
setatS
Transition matrix for action
(action-unaware)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
1
2
3
4
5
1 2 3 4 5
States
setatS
Transition matrix for action
(action-unaware)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
Fig.S11: Groundtruthtransitionmapsforaction→and←intheT-maze.
1
2
3
4
5
1 2 3 4 5
States
setatS
Transition matrix for action
(action-unaware)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
1
2
3
4
5
1 2 3 4 5
States
setatS
Transition matrix for action
(action-unaware)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
Fig.S12: Groundtruthransitionmapsforaction↓and↑intheT-maze.
47
Activeinferenceforaction-unawareagents
S3.1.7. Learnedtransitionmapsinaction-unawareandaction-awareagents
1
2
3
4
5
1 2 3 4 5
States
setatS
Transition matrix for action
(action-unaware)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
1
2
3
4
5
1 2 3 4 5
States
setatS
Transition matrix for action
(action-aware)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
Fig.S13: Transitionmapsforaction→.
1
2
3
4
5
1 2 3 4 5
States
setatS
Transition matrix for action
(action-unaware)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
1
2
3
4
5
1 2 3 4 5
States
setatS
Transition matrix for action
(action-aware)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
Fig.S14: Transitionmapsforaction↓.
48
Activeinferenceforaction-unawareagents
1
2
3
4
5
1 2 3 4 5
States
setatS
Transition matrix for action
(action-unaware)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
1
2
3
4
5
1 2 3 4 5
States
setatS
Transition matrix for action
(action-aware)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
Fig.S15: Transitionmapsforaction←.
1
2
3
4
5
1 2 3 4 5
States
setatS
Transition matrix for action
(action-unaware)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
1
2
3
4
5
1 2 3 4 5
States
setatS
Transition matrix for action
(action-aware)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
Fig.S16: Transitionmapsforaction↑.
S3.2. Experiment2: 5-stepGridw9
S3.2.1. HowtoReproducetheResultsoftheExperiment
TheresultsreportedinSection4.2wereobtainedbyusingthefollowingcommandlinearguments.
Fortheaction-unawareagent:
1 main_aif_paths --exp_name aif_paths --gym_id gridworld-v1 --env_layout
gridw9 --num_runs 10 --num_episodes 180 --num_steps 5 --inf_steps
10 --action_selection kd -lB --num_policies 256 --pref_loc all_goal
Fortheaction-awareagent:
1 main_aif_plans_pi_cutoff --exp_name aif_plans --gym_id gridworld-v1 --
env_layout gridw9 --num_runs 10 --num_episodes 180 --num_steps 5 --
inf_steps 10 --action_selection kd -lB --num_policies 256
49
Activeinferenceforaction-unawareagents
Theplotswereobtainedusingthefollowingcommandlineinstructions:
1 vis_aif -gid gridworld-v1 -el gridw9 -nexp 2 -rdir
episodic_e180_pol16_maxinf10_learnB -fpi 0 1 2 3 4 -i 4 -v 8 -ti 4
-tv 8 -vl 3 -hl 3 -xtes 20 -ph 4 -selrun 0 -selep 24 49 74 99 -npv
16 -sb 4 -ab 0 1 2 3
Withtheseinstructions,onecanvisualizemoremetricsthanthosereportedinthemaintext. We
offeraselectionnext.
S3.2.2. Freeenergyatsteps1-4
3.0
2.5
2.0
1.5
1.0
0.5
0.0
1 20 40 60 80 100 120 140 160 180
Episode
ygrene
eerF
Free energy at step 1
(action-unaware)
3.0
2.5
2.0
1.5
1.0
0.5
0.0
1 20 40 60 80 100 120 140 160 180
Episode
(a)Freeenergy.
ygrene
eerF
Free energy at step 1
(action-aware)
(b)Freeenergy.
Fig.S17: Freeenergyatstep1acrossepisodes(showingaverageof10agents).
5
4
3
2
1
0
1 20 40 60 80 100 120 140 160 180
Episode
ygrene
eerF
Free energy at step 2
(action-unaware)
5
4
3
2
1
0
1 20 40 60 80 100 120 140 160 180
Episode
(a)Freeenergy.
ygrene
eerF
Free energy at step 2
(action-aware)
(b)Freeenergy.
Fig.S18: Freeenergyatstep2acrossepisodes(showingaverageof10agents).
50
Activeinferenceforaction-unawareagents
8
7
6
5
4
3
2
1
0
1 20 40 60 80 100 120 140 160 180
Episode
ygrene
eerF
Free energy at step 3
(action-unaware)
8
7
6
5
4
3
2
1
0
1 20 40 60 80 100 120 140 160 180
Episode
(a)Freeenergy.
ygrene
eerF
Free energy at step 3
(action-aware)
(b)Freeenergy.
Fig.S19: Freeenergyatstep3acrossepisodes(showingaverageof10agents).
12
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180
Episode
ygrene
eerF
Free energy at step 4
(action-unaware)
12
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180
Episode
(a)Freeenergy.
ygrene
eerF
Free energy at step 4
(action-aware)
(b)Freeenergy.
Fig.S20: Freeenergyatstep4acrossepisodes(showingaverageof10agents).
51
Activeinferenceforaction-unawareagents
S3.2.3. Policy-conditionedfreeenergyatsteps1–4
3.0
2.5
2.0
1.5
1.0
0.5
0.0
1 20 40 60 80 100 120 140 160 180
Episode
ygrene
eerF
Policy-conditioned free energy at step 1
(action-unaware)
3.0
2.5
2.0
1.5
1.0
0.5
0.0
1 20 40 60 80 100 120 140 160 180
Episode
ygrene
eerF
Policy-conditioned free energy at step 1
(action-aware)
Policies
1: , , , 5: , , , 9: , , , 13: , , ,
2: , , , 6: , , , 10: , , , 14: , , ,
3: , , , 7: , , , 11: , , , 15: , , ,
4: , , , 8: , , , 12: , , , 16: , , ,
Fig.S21: Policy-conditionedfreeenergiesatstep1acrossepisodes(showingaverageof10agents).
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180
Episode
ygrene
eerF
Policy-conditioned free energy at step 2
(action-unaware)
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180
Episode
ygrene
eerF
Policy-conditioned free energy at step 2
(action-aware)
Policies
1: , , , 5: , , , 9: , , , 13: , , ,
2: , , , 6: , , , 10: , , , 14: , , ,
3: , , , 7: , , , 11: , , , 15: , , ,
4: , , , 8: , , , 12: , , , 16: , , ,
Fig.S22: Policy-conditionedfreeenergiesatstep2acrossepisodes(showingaverageof10agents).
52
Activeinferenceforaction-unawareagents
12
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180
Episode
ygrene
eerF
Policy-conditioned free energy at step 3
(action-unaware)
12
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180
Episode
ygrene
eerF
Policy-conditioned free energy at step 3
(action-aware)
Policies
1: , , , 5: , , , 9: , , , 13: , , ,
2: , , , 6: , , , 10: , , , 14: , , ,
3: , , , 7: , , , 11: , , , 15: , , ,
4: , , , 8: , , , 12: , , , 16: , , ,
Fig.S23: Policy-conditionedfreeenergiesatstep3acrossepisodes(showingaverageof10agents).
12
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180
Episode
ygrene
eerF
Policy-conditioned free energy at step 4
(action-unaware)
12
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180
Episode
ygrene
eerF
Policy-conditioned free energy at step 4
(action-aware)
Policies
1: , , , 5: , , , 9: , , , 13: , , ,
2: , , , 6: , , , 10: , , , 14: , , ,
3: , , , 7: , , , 11: , , , 15: , , ,
4: , , , 8: , , , 12: , , , 16: , , ,
Fig.S24: Policy-conditionedfreeenergiesatstep4acrossepisodes(showingaverageof10agents).
53
Activeinferenceforaction-unawareagents
S3.2.4. Expectedfreeenergyatsteps2–4
14
12
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180
Episode
ygrene
eerf
detcepxE
Expected free energy at step 2
(action-unaware)
14
12
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180
Episode
ygrene
eerf
detcepxE
Expected free energy at step 2
(action-aware)
Policies
1: , , , 5: , , , 9: , , , 13: , , ,
2: , , , 6: , , , 10: , , , 14: , , ,
3: , , , 7: , , , 11: , , , 15: , , ,
4: , , , 8: , , , 12: , , , 16: , , ,
Fig.S25: Expectedfreeenergyatstep2foreachpolicyacrossepisodes(showingaverageof10agents).
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180
Episode
ygrene
eerf
detcepxE
Expected free energy at step 3
(action-unaware)
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180
Episode
ygrene
eerf
detcepxE
Expected free energy at step 3
(action-aware)
Policies
1: , , , 5: , , , 9: , , , 13: , , ,
2: , , , 6: , , , 10: , , , 14: , , ,
3: , , , 7: , , , 11: , , , 15: , , ,
4: , , , 8: , , , 12: , , , 16: , , ,
Fig.S26: Expectedfreeenergyatstep3foreachpolicyacrossepisodes(showingaverageof10agents).
54
Activeinferenceforaction-unawareagents
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
1 20 40 60 80 100 120 140 160 180
Episode
ygrene
eerf
detcepxE
Expected free energy at step 4
(action-unaware)
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
1 20 40 60 80 100 120 140 160 180
Episode
ygrene
eerf
detcepxE
Expected free energy at step 4
(action-aware)
Policies
1: , , , 5: , , , 9: , , , 13: , , ,
2: , , , 6: , , , 10: , , , 14: , , ,
3: , , , 7: , , , 11: , , , 15: , , ,
4: , , , 8: , , , 12: , , , 16: , , ,
Fig.S27: Expectedfreeenergyatstep4foreachpolicyacrossepisodes(showingaverageof10agents).
Thereisnoexpectedfreeenergyatstep5becausethisisthestepatwhichtheenvironmentterminates
in the episodic setting considered in this work and, regardless of its location, the agent is no longer
giventheabilitytoplanforwardintime.
55
Activeinferenceforaction-unawareagents
S3.2.5. Expectedfreeenergyatstep0breakdown
18
16
14
12
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180
Episode
ksiR
Risk at step 1 (action-unaware)
18
16
14
12
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180
Episode
(a)Risk
ksiR
Risk at step 1 (action-aware)
(b)Risk
Policies
1: , , , 5: , , , 9: , , , 13: , , ,
2: , , , 6: , , , 10: , , , 14: , , ,
3: , , , 7: , , , 11: , , , 15: , , ,
4: , , , 8: , , , 12: , , , 16: , , ,
Fig.S28: Risk(expectedfreeenergyterm)foreachpolicyacrossepisodes(showingaverageof10agents).
2.00
1.75
1.50
1.25
1.00
0.75
0.50
0.25
0.00
1 20 40 60 80 100 120 140 160 180
Episode
ytlevon-B
B-novelty at step 1 (action-unaware)
2.00
1.75
1.50
1.25
1.00
0.75
0.50
0.25
0.00
1 20 40 60 80 100 120 140 160 180
Episode
(a)Risk
ytlevon-B
B-novelty at step 1 (action-aware)
(b)Risk
Policies
1: , , , 5: , , , 9: , , , 13: , , ,
2: , , , 6: , , , 10: , , , 14: , , ,
3: , , , 7: , , , 11: , , , 15: , , ,
4: , , , 8: , , , 12: , , , 16: , , ,
Fig.S29: B-novelty(expectedfreeenergyterm)foreachpolicyacrossepisodes(showingaverageof10agents).
56
Activeinferenceforaction-unawareagents
S3.2.6. Groundtruthtransitionmaps
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
(action-unaware)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
(action-unaware)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
Fig.S30: Groundtruthtransitionmapsforaction→and←inthegridworld.
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
(action-unaware)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
(action-unaware)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
Fig.S31: Groundtruthransitionmapsforaction↓and↑inthegridworld.
57
Activeinferenceforaction-unawareagents
S3.2.7. Learnedtransitionmapsinaction-unawareandaction-awareagents
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
(action-unaware)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
(action-aware)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
Fig.S32: Transitionmapsforaction→.
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
(action-unaware)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
(action-aware)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
Fig.S33: Transitionmapsforaction↓.
58
Activeinferenceforaction-unawareagents
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
(action-unaware)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
(action-aware)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
Fig.S34: Transitionmapsforaction←.
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
(action-unaware)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
(action-aware)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
Fig.S35: Transitionmapsforaction↑.
59

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Active inference for action-unaware agents"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
