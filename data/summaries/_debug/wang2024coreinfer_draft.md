### CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation### OverviewThis paper investigates CoreInfer, a novel adaptive sparse activation inference method designed to accelerate large language model (LLM) inference. The authors state: "Large language models have sparked an exciting wave of AI applications, however, their high computational cost and memory demands during inference pose significant challenges." They note: "Adaptivesparse activationinference, which activates only a small number of neurons for each token, offers a novel way to accelerate model inference without degrading performance, showing great potential for resource-constrained hardware devices." The paper argues: “The key challenge is: how can we reduce the memory and computational requirements for model inference without degrading performance?” CoreInfer, as described, leverages sentence-level prediction to achieve this goal. The authors claim: “We define sentence-wise core neurons as the most essential neurons an LLM needs to process it.” They further state: “These core neurons are empirically demonstrated to be sufficient enough for an LLM to perform lossless generation tasks.”### MethodologyThe CoreInfer method, as presented, is based on two prediction strategies. First, the authors explain: “We explore the correlation between core neurons and its semantics.” They then state: “We performed explorations at the level of stability and similarity between core neurons and semantics.” The core neurons are determined during the pre-filling stage, and they are fixed during the encoding stage. The authors detail: “In the pre-filling stage, we first extract the token-wise core neurons based on the top-k selection and then further extract the top-k commonly activated core neurons among all tokens, which go through the stability estimation to determine how to update the sentence-wise core neurons set.” The authors further elaborate: “We design two semantic-based methods for predicting core neurons to fit different input scenarios.” They also state: “CoreInfer only needs to predict the core neurons during the pre-filling stage.” The method utilizes sentence-wise core neurons, defining them as “the most essential neurons an LLM needs to process it.” The authors explain: “We define sentence-wise core neurons as the most essential neurons an LLM needs to process it.”### ResultsThe paper demonstrates CoreInfer’s effectiveness through experiments on various datasets, including Xsum, SQuAD, TruthfulQA, and Wmt16-de-en. The authors report: “On a NVIDIA TITAN XP GPU, CoreInfer achieved a10.33× and2.72× speedup compared to the Huggingface implementation and PowerInfer, respectively.” They state: “We conducted experiments on six datasets, categorized into three types of tasks: Information Extraction, Question Answering, and Translation.” The results show that CoreInfer achieves a10.33x speedup compared to the Huggingface implementation and a2.72x speedup compared to PowerInfer on an NVIDIA TITAN XP GPU. The authors also report: “We evaluated the model generalization and task generalization of CoreInfer across various models and tasks.” They state: “For question answering and translation tasks, the two sub-columns refer to the results of six-shot and zero-shot scenarios.” The authors report that CoreInfer achieves a10.33x speedup compared to the Huggingface implementation and a2.72x speedup compared to PowerInfer on an NVIDIA TITAN XP GPU.### FindingsThe key finding of this research is the successful implementation of an adaptive sparse activation inference method that significantly accelerates LLM inference without compromising performance. The authors state: “We propose CoreInfer, a novel sparse inference strategy featuring the sentence-level activation sparsity without additional MLP predictors.” They further claim: “The core neurons exhibit both stability and similarity in relation to the sentence’s semantics.” The authors highlight: “We performed explorations at the level of stability and similarity between core neurons and semantics.” They state: “We discovered that core neurons exhibit both stability and similarity in relation to the sentence’s semantics.” The authors emphasize: “We demonstrate that our method possesses both model generalization and task generalization.” The authors report: “On a NVIDIA TITAN XP GPU, CoreInfer achieved a10.33×and2.72×speedup compared to the HuggingfaceimplementationandPowerInfer,respectively.”### DiscussionThe paper’s discussion centers around the insights gained from the experimental results. The authors state: “We observed that the more complex the task, the more neurons are needed.” They explain: “For simpler tasks such as information extraction, few-shot question answering, and translation, stability-guided prediction alone achieves good performance.” The authors highlight: “We demonstrate that our method possesses both model generalization and task generalization.” The authors report: “We evaluated the model generalization and task generalization of CoreInfer across various models and tasks.” The authors emphasize: “We observed that the more complex the task, the more neurons are needed.” The authors state: “For simpler tasks such as information extraction, few-shot question answering, and translation, stability-guided prediction alone achieves good performance.”