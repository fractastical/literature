=== IMPORTANT: ISOLATE THIS PAPER ===
You are revising a summary for ONLY the paper below. Do NOT reference or use content from any other papers.
Paper Title: An analytical model of active inference in the Iterated Prisoner's Dilemma
Citation Key: demekas2023analytical
REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Issues to fix:
1. CRITICAL: The current summary has severe repetition issues. You MUST eliminate all repeated sentences, phrases, and paragraphs. Each idea should be expressed only once. If you find yourself repeating content, remove the duplicates entirely. Focus on variety and uniqueness in your wording.
2. Severe repetition detected: Same sentence appears 3 times (severe repetition)
3. Paragraph-level repetition detected

Current draft (first 2000 chars):
### OverviewThis paper investigates the dynamics of the Iterated Prisoner’s Dilemma (IPD) using the framework of active inference. The authors design pairs of Bayesian agents that track the joint game state of their and their opponent’s choices, aiming to derive analytical conditions for phase transitions and convergence to optimal strategies. The core of the paper is to demonstrate that the agents can learn the Pavlov strategy—a win-stay, lose-shift strategy—under specific conditions.### MethodologyThe authors employ a Markovian decision process (MDP) model to represent the IPD. This model incorporates a transition model that describes the probability of transitioning between game states, parameterized by a Dirichlet distribution. The agents’ learning is governed by updating these transition probabilities based on observed outcomes. The key methodological contribution is the derivation of an analytical expression for the transition probabilities, allowing for the precise calculation of the conditions under which the agents converge to the optimal strategy. The model is deterministic, meaning that the agents always perform the same action given the same state and learning rate. The authors also explore stochastic sampling, where the agents sample their actions from a distribution that is influenced by the learning rate.### ResultsThe primary finding of the paper is that, under specific parameterizations (particularly when the learning rate is set to0.3), the agents converge to the Pavlov strategy. The authors demonstrate that the transition probabilities can be analytically derived, providing a precise mathematical description of the dynamics. The authors show that the agents can learn the optimal strategy, even when the learning rate is low. The authors also show that the agents can learn the optimal strategy when they sample their actions stochastically. The authors demonstrate that the agents can learn the optimal strategy, even when the learning rate is high. Th...

Key terms: konstanz, institute, university, dilemma, analytical, northeastern, arizona, game

=== FULL PAPER TEXT ===
An analytical model of active inference in the
Iterated Prisoner’s Dilemma
Daphne Demekas∗1,2, Conor Heins†1,3,4,5, and Brennan Klein‡1,5,6
1Network Science Institute, Northeastern University, Boston, Massachusetts, USA
2Wheeler Lab, University of Arizona, Tucson, Arizona, USA
3Department of Collective Behaviour, Max Planck Institute of
Animal Behavior, 78464 Konstanz, Germany
4Department of Biology and the Centre for the Advanced Study of Collective Behaviour,
University of Konstanz, 78464 Konstanz, Germany
5VERSES AI Research Lab, Los Angeles, CA 90016, USA
6The Institute for Experiential AI, Northeastern University, Boston, Massachusetts, USA
August 31, 2023
Abstract
This paper addresses a mathematically tractable model of the Prisoner’s Dilemma
using the framework of active inference. In this work, we design pairs of Bayesian
agents that are tracking the joint game state of their and their opponent’s choices in an
IteratedPrisoner’sDilemmagame. Thespecificationoftheagents’beliefarchitecturein
the form of a partially-observed Markov decision process allows careful and rigourous
investigation into the dynamics of two-player gameplay, including the derivation of
optimal conditions for phase transitions that are required to achieve certain game-
theoretic steady states. We show that the critical time points governing the phase
transition are linearly related to each other as a function of learning rate and the
rewardfunction. Wetheninvestigatethepatternsthatemergewhenvaryingtheagents’
learning rates, as well as the relationship between the stochastic and deterministic
solutions to the two-agent system.
1 Introduction
Studies of behavioural science, be it in biology, psychology, or machine learning, often rely
on the concept of rational thinking and decision making [1–4]. Game theory has had wide
∗daphnedemekas@arizona.edu
†cheins@ab.mpg.de
‡b.klein@northeastern.edu
1
3202
guA
92
]hp-cos.scisyhp[
2v49451.6032:viXra
successinpreciselyformulatingcontextsinwhichplayersoragentsarechallengedtoconverge
to an optimal yet counter-intuitive strategy that maximises reward. In particular, game
theory models communication among agents that can result in bounded-complex emergent
behaviour [5, 6]. The Iterated Prisoner’s Dilemma (IPD) is a quintessential game, in which
the ‘dilemma’ is that the highest reward is attributed to the action of defection, but the
optimal behaviour in the long run is to cooperate, because of the ‘Shadow of the Future’
phenomenon [7]1. When played iteratively, agents learn each other’s predictable behaviour
and can form an optimal strategy, away from the Nash equilibrium of the one-shot game.
To do so, agents need to be aware of what their opponent is likely to do, which is why the
IPD is widely used to study the evolution of cooperation for selfish agents [9].
This work addresses a computational model of the (memory-one) Iterated Prisoner’s
Dilemma under the framework of active inference (AIF) [10–12]. AIF is an agent-based
modelling framework derived from theoretical neuroscience, where cognitive processes like
action, perception, and learning are seen as solutions to an inference problem. As an explic-
itly model-based, Bayesian framework for simulating behaviour, AIF provides cognitively
‘transparent’ agents, whose posterior beliefs about the world and associated uncertainties
are accessible and interpretable. This enables careful investigation into the Bayesian basis of
behaviour in these simple models, in turn allowing us to identify the conditions under which
optimal behaviour is possible.
When two identical and deterministic AIF agents play against one another, we show that
the equation governing across-trial learning dynamics is mathematically tractable given one
approximation. This enables us to derive functions that model the specific conditions under
which convergence to an optimal strategy—namely the Pavlov Strategy [9]—for the IPD can
occur, given a multi-agent AIF model. The Pavlov strategy is win-stay-lose-change, where
agents will cooperate if the agent’s and opponent’s moves are the same in the previous round
and defect otherwise. We explore how these dynamics vary across different configurations of
the agents’ learning rates, as well as how stochasticity in the agent network determines the
probabilities of agents reaching the optimal outcome.
1.1 Iterated Prisoner’s Dilemma
InthePrisoner’sDilemma, ateachround, bothplayerscaneitherdefectorcooperate, leading
to 4 possible outcomes [8] (see Table 1 with different reward levels). The outcome with the
highest reward is if the player defects and its opponent cooperates (DC), which is also the
outcome with the lowest reward for the opponent (CD). The second-best outcome is if both
cooperate (CC), and the third-best outcome for both players is if they defect (DD). In this
model, the four reward levels are respectively [3,1,4,2]. This work specifically models the
memory-one IPD, where each player only considers the previous move of their opponent
when making their decision for the current round.
There are several notable strategies in the IPD, which have been categorised in different
1This is when agents in repeated play—without awareness of when the play will end—will be more
cooperativebecausetheyaremadetolearnaboutthepossibilityofbeingpunishedandplanaccordingly[8].
2
Player 2
Cooperate (C) Defect (D)
C (3,3) (1,4)
Player 1
D (4,1) (2,2)
Table 1: Example payout matrix in a Prisoner’s Dilemma game.
ways [13]. First, a dominant strategy produces the best possible payoff for an agent, re-
gardless of the strategies used by opponents. The most commonly cited dominant outcome
is when both players defect (choose to betray) in every round. From an individual player’s
perspective, defecting in every round provides a higher immediate payoff compared to coop-
eration, especially when the other player cooperates. However, defecting in every round is
not socially optimal as it leads to a lower overall payoff compared to mutual cooperation.
The challenge is to find strategies that can foster cooperation and lead to better outcomes
for both players in the long run, rather than succumbing to the dominant outcome of mutual
defection [14].
In order to reach the social optimum of cooperation, new heuristics or bounds on the
agents need to emerge in order for them to look beyond the reward function when deciding
their actions. This makes the IPD a good arena to study bounded rationality, in which
agents do not have access to the full generative process (encompassing both themselves
and their opponent), and therefore must make decisions given a bound on their awareness
or knowledge, of, for instance, the other player, or any external environmental factors.[4].
Agents playing the IPD has been studied in the context of reinforcement learning already
[15, 16], and the idea of bounded rationality serves as a motivation for using active inference
agents to model the IPD, as the AIF is a transparent and interpretable framework in which
agents infer actions and quantify uncertainty under the constraints of their generative model.
Thereareseveralwaystotraintheagentstoconvergetothesocialoptimum,whichwewill
refer to as the cooperative steady state. When agents sample their actions deterministically,
ourmodelshowsthatactiveinferenceagentsparameterisedwithaconstrainedsetoflearning
rates can converge to the cooperative steady state by learning the Pavlov Strategy [9], and
it also demonstrates learning rate configurations that get trapped in the Nash equilibrium,
in which agents converge to Unconditional Defection [17, 18].
1.2 Active inference
Activeinference(AIF)agentsareabletoplanandlearnabouttheirstatespaceandtransition
probabilities through observed experience. They infer which actions to take by minimising
the expected free energy anticipated to accrue from their actions [12]. This often allows these
agents to solve complex tasks often seen in reinforcement learning or neuroscience, such as
the Multi-Armed Bandit [19] and other Monte-Carlo based tasks [20].
Advances in the ability to quickly build and scale models of AIF agents, particularly in
3
Variable Name Notation
Hidden States s ∈ {CC,CD,DC,DD}
Observations o ∈ {CC,CD,DC,DD}
Actions u ∈ {uC,uD}
Observation Model P(o |s ;A) = Cat(A)
t t
Transition Model P(s |s ,u ;B) = Cat(B)
t+1 t−1 t−1
(cid:81)
Transition Model Parameter P(B) = P(B ), P(B ) = Dir(b )
ju •ju •ju •ju
Initial State Prior P(s ;D) = Cat(D)
1
˜
‘Biased’ State Prior (Reward) P(s;C) = Cat(C), s.t. lnC = [3,1,4,2]
Table 2: Generative model variables and notation.
Python using the pymdp library [21], have allowed for a much more scalable and accessible
meanstomodeltheseagentsindifferentandflexibleenvironments, aswellastoconnectthem
in networks and allow them to observe each other’s actions. This has allowed researchers
to ask more interesting questions about how relevant AIF is in terms of modelling rational
decision making, such as those observed in game theory. In this paper, we show that not
only can AIF agents effectively learn optimal strategies to the IPD, but the framework of
active inference enables us to derive the exact conditions for when this will occur and have
a layered understanding of the agents’ ‘mental process’ throughout the game.
The agents in this model actively entertain beliefs about the dynamics of the game
and iteratively update their beliefs about the game dynamics (i.e., a ‘transition model’) as
they play multiple rounds against their opponent. In the context of the discrete-time and
-space models used in the present work, this amounts to updating the elements of transition
probability matrices that represent each agent’s beliefs about game states from one trial
to the next. After every trial of iterated play, the agents update these state transition
probability distributions based on their actions and the outcomes that they observed. In
doing so, the agents have the capacity to learn strategies, manifested as patterns of learned
probabilities of transition from each state to each other state.
Our hypothesis is that throughout iterative play, the bounded-rational agents will learn
to infer actions based on learned patterns of their opponent’s behaviour (i.e., the ability
to predict revenge from defection), and this will result in a strategy leading to the social
optimum steady state in which both agents cooperate. Further, given the interpretability of
the AIF, we will be able to analytically derive the process that the agents undergo during
this learning process and thus predict how it might change with different parameters.
2 Simulation Dynamics
Here, we explore the long-term dynamics of the IPD. Agents play in turns for a finite set
of trials, updating their transition model beliefs Q(B;ϕ ) at each trial. Unless otherwise
b
4
Figure 1: Beliefs about transition probabilities over trials. Top: A representation
of Player 1’s beliefs at three phases of the simulation (t = 10,20,150). Each box con-
tains a graph representation of the transition probabilities, and histograms of the cooperate-
conditioned (top row) or defection-conditioned (bottom row) transition distributions at the
displayed trial indices. Darker values represent a higher probability. Bottom: The inferred
probabilities of cooperation in each trial. Agents select the action with the highest posterior
probability. The agents begin by continuously defecting, then undergo an oscillatory period
of defection and cooperation, and eventually reach a cooperative steady state. After this
period of training, they will have learned the Pavlov strategy, i.e. they will cooperate if the
agent’s and opponent’s moves are the same in the previous round and defect otherwise [9].
specified,agentsareconfiguredexactlythesame(samepriors,samelearningrate)andsample
their actions deterministically as described in (A.21). In this model, agents always converge
to the cooperative steady state and remain there indefinitely. The magnitude of the learning
rate η affects the rate of convergence by scaling the update to the transition matrix at each
timestep, as shown in (A.25). In Figure 1 we show the simulation dynamics for agents
configured with learning rate η = 0.3, but it’s important to note that at different learning
rates, the nature of these dynamics would not change - rather the critical time points would
only occur either sooner (for larger η) or later (for smaller η). Therefore, the amount of time
taken in order to converge is not representative of the performance of this model, but rather
a parameter that can be tweaked. Given the transparency of this deterministic system, it is
possible to explain exactly how these agents are ‘thinking’, given their posteriors over time.
Agents are initialised with uniform transition matrices as in (A.7). Upon the first obser-
vation, they infer the game state and calculate the expected free energies (EFEs, or G) of
cooperating and defecting. They take the action that has smaller EFE, i.e., argmin G (u).
u 0
At first, because of the reward parameterization and the uniformity in the transition prior
P(B;b), defection will minimise the EFE (i.e., predicts the highest reward), according to:
5
1 1
G (u = C,ϕC) = −(BC ·ϕC)·(lnBC ·ϕC −lnC) = ln(C C )−ln (1)
0 0 0 0 0 2 1 2 2
1 1
G (u = D,ϕD) = −(BD ·ϕD)·(lnBD ·ϕD −lnC) = ln(C C )−ln (2)
0 0 0 0 0 2 3 4 2
Therefore,aslongasln(C C ) < ln(C C ),theagentalwaysdefectsonthefirsttimestep.
3 4 1 2
Agents will then continue to defect, because the expected reward from realising the state
DC still outweighs that of any other predicted state. As the agents continue to defect, their
beliefs about P(s = DC|s = DD,u = D) will be decreasing with a proportional increase
t t−1
in P(s = DD|s = DD,u = D), meaning G(u = D) will increase as the probability of
t t−1
getting their desired reward decreases.
At a critical time, which we denote τ 2, the agents will begin assigning more probability
1
to cooperation than defection ϕC > ϕD, because the transition probabilities have decreased
sufficiently for the EFE of cooperation to outweigh that of defection. Once the agents begin
cooperating, they undergo an oscillatory period during which their actions fluctuate from
cooperation to defection. This is because at τ , the transition probabilities P(s |s = CC)
1 t t−1
are fixed at their initial value, since the agents have yet observed the previous state being
CC. Thus the agents will still be optimistic about realising the highest reward state DC via
the transition probability P(s = DC|s = CC,u = D).
t+1 t
The agents will eventually learn that inferring to defect will inevitably lead to observing
DD, and inferring to cooperate will inevitably lead to CC. The oscillatory period is crucial
to this because it teaches the agent that defecting in response to cooperation will only ever
lead to DD. The oscillation continues until the critical time point τ , in which the probability
2
p(s = DC|s = CC,u = D) becomes smaller than p(s = DD|s = CC,u = D), at
t+1 t t t+1 t t
which point the agents will cooperate for all remaining rounds.
2.1 The analytic transition function
In the above model of AIF agents, an analytic solution for the evolution of each agent’s
beliefs about the transition likelihood Q(B;ϕ∗) is available. This is formulated by deriving
b
approximations to τ —the critical trial in which the agents transition to an oscillatory period
1
between defection and cooperation—and τ , the second phase transition in which the agents
2
converge to the cooperative steady state. Given the expressions for τ and τ in (3), we can
1 2
writedowntheevolutionoftheDirichletparametersofthetransitionprobabilitymatrix. The
derivationsforthefollowingexpressionsareintheAppendix(A.4, A.5). Here, Ccorresponds
to the ‘biased’ state reward prior, and each entry of C corresponds to the reward value of
that observation (r ,r ,r ,r ). For the full definition see (A.5).
CC CD DC DD
R (β) R (β)
1 2
τ ≈ τ ≈ (3)
1 2
η η
2whose solution in terms of generative model parameters we derive in the next section.
6
where
2 3
R = −1 R = R (4)
1 (cid:113) 2 1
ln C3 +2− (ln C4 −2)2 −8(−ln √ C4 − 1) 2
C4 C3 2 C1C2 5
This means that τ , the number of trials it takes the system to reach the steady state,
2
can be precisely approximated as a linear function of τ , the number of trials it takes to
1
start the oscillatory period (see Figure 3)—i.e. the critical time points governing the phase
transition are linearly related to each other as a function of learning rate and the reward
function. Given these expressions, our analytic form of the transition rule for the posterior
Dirichlet parameters over the transition model is:
 
bC bD +ηsDD ⊗sDDt t < R1
  0   0 η
ϕC = bC + ηsCC ⊗sDD(t− R1) ϕD = bD + ηsDD ⊗sCC(t− R1) R1 < t < R2
bt+1 0 2 η bt+1 τ1 2 η η η
 
bC +ηsCC ⊗sCC(t− R2) bD t > R2
τ2 η τ2 η
(5)
which can be used to exactly replicate the trajectory of Q(s |s ,u ) over time (Figure 3).
t+1 t t
We conclude by noting that the agents in this model, after undergoing these two phase
transitions and converging to CC, have learned the well-known Pavlov (also known as the
“Win-Stay Lose-Shift”) strategy from IPD literature [9]. Agents learned during 0 < t < τ
1
that given the observation DD, the best strategy is to cooperate, and during τ < t < τ
1 2
they learned that cooperating is the best outcome given the observation CC—therefore,
having reached τ , they continue cooperating. To show that the agents learned the Pavlov
2
strategy, we performed an experiment where once an agent converged to the steady state, we
disabledadditionallearningandhadthisagentplayagainstanagentthatbehavescompletely
randomly. When playing against this random agent, they observe the new asymmetric states
DC or CD. The desire to maximise expected utility (via the drive to minimise KL risk, a.k.a.,
the expected free energy) will lead them to perform the ‘greedy’ strategy of defection, which
is how their behaviour is consistent with the Pavlov strategy3. Future work will further
characterise the space of learnable strategies under this framework.
3 Generalizing the model
In the previous section, we found an approximate solution for the belief-, action-, and
learning-dynamics, which completely describes the case of two symmetrically-parameterised
agents playing IPD. For any given parameterisation of the prior preferences C, we derived
the trials at which the critical transitions take place in the two-agent system, steering it
away from the Nash equilibrium and towards the cooperative steady state.
3AnagentexhibitingthePavlovstrategywillonlycooperateifintheprevioustrial,bothagentsperformed
the same action (i.e., the state was either CC or DD, otherwise they will defect).
7
Figure 2: Marginalised transition probabilities under different η. The dotted lines
represent the marginalised probabilities from all states to the highest reward state DC, and
the solid lines represent the marginalised probabilities from all states to the socially optimal
state CC. The transition probabilities to DC decrease initially during the period of defection,
then fluctuate during the period of oscillation and steady out close to 0 once the agents reach
the cooperate steady state, and the probabilities to state CC take the same pattern in the
opposite direction. This happens more rapidly for larger η, because the updates to the
parameters of the transition likelihood distribution are larger at every trial.
Thesimplicityofthismodelisthattheseagentsareconfiguredexactlyalike,andtherefore
there is complete symmetry in the state space. This means that the agents will only ever
observe two out of four possible states in the space. However, this case no longer holds when
either the agents are parameterised with different learning rates, or when they sample their
actions stochastically, according to (A.22). These cases open the space of possible strategies
thattheagentscanlearn, someofwhichwillleadtheagentstofallintotheNashequilibrium,
and others which will allow them to reach the optimal outcome.
3.1 Different learning rates
We now assume agents parameterised with different η and the same β, performing actions
deterministically. We denote the agent with larger η as a , and the agent with smaller
1 1
η as a . According to (A.38), the critical value τ depends on η, and since η > η , this
2 2 1 1 2
means τa1 < τa2. Thus, a will cooperate at τa1 = R1, but a will not yet deem cooperation
1 2 1 1 η1 2
a better policy than defection (namely, the EFE of defection will remain below that of
8
Figure 3: Simulated vs. derived relation between reward and learning rate. Sim-
ulated and approximated τs for three values of β parameterizing the reward function. On
the left, we approximate τ with the equation τ = R1 where R depends on the reward pa-
1 1 η 1
rameter of β. On the right, we approximate τ with τ = τ + 1R where again, R depends
2 2 1 η 2 2
on β. With a larger β, meaning a higher predicted reward for the state DC, the values of
τ increase as it will take more trials for the players to update their transition probabilities
away from having a preference to defect.
cooperation). Therefore, at τa1, the game state will be CD from a ’s perspective and DC
1 1
from a ’s perspective. This symmetry-breaking means that the system will not enter into
2
the typical oscillation phase triggered by mutual cooperation (as is guaranteed when η = η
1 2
and thus τa1 = τa2).
1 1
The nonidentical observations imply that after τa1, a believes P(s = CD|DD) is more
1 1 t+1
probable, thereby being disincentivised to continue cooperating, and a believes P(s =
2 t+1
DC|DD) is more probable, being incentivised to continue defecting. The degree of disincen-
tivisation(orincentivisation)willincreaseinproportiontoη orη , respectively, duetoacor-
1 2
responding η -scaled increase in Ga1(u = C) and an η -scaled decrease in Ga2(u = D). This
1 2
growing asymmetry in the agents’ beliefs means that (5) no longer holds. At this point, the
agents will return to continuous defection until another instance of G(u = D) = G(u = C)
occurs; the duration of this depends on η.
In sum, the conditions under which the joint-agent system converges to the optimal
steady state is determined by whether or not the agents’ learning rates are configured such
that there will be some time point t less than some threshold T in which both agents
max
cooperate simultaneously. If this is not the case, then as defection continues, the rate of
increase of G(u = D) slows, and after a certain amount of time (governed by η) it will
become too slow and never catch up to G(u = C) (see Figure 4). In other words, if at any
point, for either agent, G (u = D) < G (u = C) ∀t ∈ (0,T ], the agents are trapped in
t t max
9
Figure 4: Relative value of cooperation under different η parameterisations.
Above: Agents are configured with ηs along the tendrils of Figure 5. On the left, the
relative values of cooperation, calculated as G(u = C)−G(u = D), reach zero several times
and converging around 0.75 at the optimal outcome. On the right: the fluctuations in the
individual EFEs. There are periods before τ and between τ and τ in which one player will
1 1 2
cooperate and the opponent defects; this creates the spikes in the distribution, as one agent
is punished and the other is rewarded. Below: Agents with ηs that are not on the tendrils
in Figure 5, meaning that they do not converge to the cooperative steady state. We can see
on the left how G (u = D) is converging to something less than G (u = C).
t t
the Nash equilibrium.
Figure 4 shows EFE trajectories in scenarios where agents converge to the optimal out-
come (above) and where agents get trapped in the Nash equilibrium (below). Convergence
to the Nash equilibrium occurs in the absence of any trial where the relative value of coop-
eration reaches 0 simultaneously for both agents. Instead, the relative values of cooperation
slowly converge to different and nonoverlapping limits4. If the intersection of the condition
in (A.31) does occur, this guarantees that the agents will begin the oscillatory period which
will eventually lead them to convergence to CC (while there may be some instances of CD
and DC in the oscillatory period, this will not prevent eventual cooperation). In general,
when learning rates are close together, the likelihood of convergence to CC is more likely;
however, the actual pattern is more complicated than this. Figure 5 demonstrates the com-
4Note that even after the agents reach a cooperative steady state, the difference in expected free energy
takes time to flatten because the entropy is still decreasing as beliefs become more precise, via learning.
10
Figure 5: Parameter sweeps over η. Top row: Agents sample actions deterministically.
Wherever the average cooperation is nonzero, agents converged to the cooperative steady
state—yellow cells indicate faster cooperation, which is generally associated with higher
overallreward. Bottom row: Agentssampleactionsstochastically. Cooperationstilloccurs
most often along the diagonal, tapering off as learning rates become more different.
plex pattern of instances in which the agents converge to the cooperative steady state given
different learning rate combinations, with both the deterministic and stochastic sampling.
3.2 Stochastic sampling
Here, we introduce noise in the action selection such that agents sample actions with some
probability proportional to their (negative) EFE. Action stochasticity can be controlled with
an inverse temperature parameter α according to (A.22). In general, all of the principles
outlinedinSection2remain; however, nowtheagentswillsometimesperformthesuboptimal
11
action. This enables agents to experience the entire state space (different combinations of
defection and cooperation) and therefore estimate transitions between all the combinations
of states.
WecanseefromFigure5that, onaverage, endowingtheagentswithstochasticityenables
them to converge to the cooperative steady state for a larger number of combinations of
differentlearningrates. Thismakessense, becauseitincreasesthelikelihoodof‘escaping’the
pattern of continuous defection, and therefore learning about the advantages of cooperation.
In terms of the reward, the agents that have most similar learning rates will behave most
similarly and therefore accumulate more reward (along the diagonal).
4 Conclusion
Iterated Prisoners’ Dilemma games have long been the test bed for new developments in
behavioural science and game theory. Because of the relative simplicity of the game’s
structure—and its, at times, surprising experimental results—researchers often use it to de-
velop mathematical frameworks for understanding decision making in social or multi-agent
contexts. In this paper, we demonstrated how active inference can be used to model the
IPD transparently, such that in a simple set-up, we can derive a solution to the evolution of
the agents’ beliefs about the game dynamics, i.e., the transition probabilities. This allows
us to quantitatively reason about why the agents converge to their chosen optimal strategy
and how behaviour changes as a function of different learning rates and stochastic action
selection. While the simple case of similarly-configured agents resulted in both agents ex-
hibiting the Pavlov strategy, once we introduce asymmetry in the generative models, and/or
stochasticity in action sampling, then upon testing, agents are able to learn a variety of
different strategies, including the Pavlov strategy, Unconditional Defection, Unconditional
Cooperation, and Tit for Tat—or some variation of Tit for Tat [22, 23].
Thisfindingisastartingpointforfuturework,inwhichsuchamodelcouldbeextendedto
multiple agents interacting towards a common goal, and investigating the various strategies
that emerge from acting in a network order to minimise free energy. The current model
did not incorporate the information-seeking components that are often leveraged in action-
selection under active inference [24]. In our case, the ambiguity term of the expected free
energy was zero by construction (due to zero observation uncertainty), but future work
could explore the role of parameter information gain (resolving uncertainty about B) and
how that changes the multi-agent dynamics in IPD. Overall, in this work we demonstrated
that AIF can offer game theory a novel analytic transparency and simplicity for accounting
for multi-agent dynamics using a first-principles, Bayesian account.
Acknowledgements: The authors thank Wolfram Barfuss and Christoph Riedl for valu-
able feedback and comments that substantially improved the quality of the manuscript.
Funding information: DD, CH, & BK acknowledge the support of a grant from the John
Templeton Foundation (61780). The opinions expressed in this publication are those of the
authors and do not necessarily reflect the views of the John Templeton Foundation.
12
References
[1] Robert Axelrod and William D. Hamilton. “The Evolution of Cooperation”. In: Science
211.4489 (1981), pp. 1390–1396. doi: 10.1126/science.7466396.
[2] Martin A. Nowak. “Five rules for the evolution of cooperation”. In: Science 314.5805
(2006), pp. 1560–1563. doi: 10.1126/science.1133755.
[3] Martin Nowak and Karl Sigmund. “A strategy of win-stay, lose-shift that outperforms
tit-for-tat in the Prisoner’s Dilemma game”. In: Nature 364.6432 (1993), pp. 56–58.
doi: 10.1038/364056a0.
[4] Herbert A. Simon. “Bounded Rationality”. In: Utility and Probability. Ed. by John
Eatwell, Murray Milgate, and Peter Newman. The New Palgrave. London: Palgrave
Macmillan UK, 1990, pp. 15–18. isbn: 978-1-349-20568-4. doi: 10.1007/978-1-349-
20568-4_5.
[5] Mazui A. Niazi Aisha D. Farooqui. “Game theory models for communication between
agents: a review”. In: Complex Adaptive Systems Modeling 13 (2016). doi: 10.1186/
s40294-016-0026-7.
[6] Jeromos Vukov, György Szabó, and Attila Szolnoki. “Cooperation in the noisy case:
Prisoner’s dilemma game on two types of regular random graphs”. In: Physical Review
E 73 (6 2006), p. 067103. doi: 10.1103/PhysRevE.73.067103.
[7] Jan B. Heide and Anne S. Miner. “The shadow of the future: Effects of anticipated
interaction and frequency of contact on buyer-seller cooperation”. In: The Academy
of Management Journal 35.2 (1992), pp. 265–291. url: https://www.jstor.org/
stable/256374.
[8] Steven Kuhn. “Prisoner’s Dilemma”. In: The Stanford Encyclopedia of Philosophy. Ed.
by Edward N. Zalta. Winter 2019. Metaphysics Research Lab, Stanford University,
2019. url: https://plato.stanford.edu/archives/win2019/entries/prisoner-
dilemma/.
[9] Velumailum Mohanaraj Martin Dyer. “The Iterated Prisoner’s Dilemma on a Cycle”.
In: arXiv (2018). doi: 10.48550/arXiv.1102.3822.
[10] Maxwell J.D. Ramstead, Dalton A.R. Sakthivadivel, Conor Heins, Magnus Koudahl,
Beren Millidge, Lancelot Da Costa, Brennan Klein, and Karl J. Friston. “On Bayesian
mechanics: A physics of and by beliefs”. In: Interface Focus 13.3 (2023), p. 20220029.
doi: 10.1098/rsfs.2022.0029.
[11] Conor Heins, Brennan Klein, Daphne Demekas, Miguel Aguilera, and Christopher L.
Buckley. “Spin glass systems as collective active inference”. In: Active Inference. Ed. by
ChristopherL. Buckley, DanielaCialfi, PabloLanillos,MaxwellRamstead, NoorSajid,
Hideaki Shimazaki, and Tim Verbelen. Communications in Computer and Information
Science. Cham: Springer Nature Switzerland, 2023, pp. 75–98. doi: 10.1007/978-3-
031-28719-0_6.
13
[12] ThomasParr,GiovanniPezzulo,andKarlJ.Friston.Active Inference: The Free Energy
Principle in Mind, Brain, and Behavior. MIT Press, 2022.
[13] Steven Kuhn. “Strategies for the Iterated Prisoner’s Dilemma”. In: The Stanford En-
cyclopedia of Philosophy. Ed. by Edward N. Zalta. Winter 2019. Metaphysics Research
Lab, Stanford University, 2019. url: https://plato.stanford.edu/entries/
prisoner-dilemma/strategy-table.html.
[14] Charles A. Holt and Alvin E. Roth. “The Nash equilibrium: A perspective”. In: Pro-
ceedings of the National Academy of Sciences 101.12 (2004), pp. 3999–4002. doi: 10.
1073/pnas.0308738101.
[15] Robert H. Crites Tuomas W. Sandholm. “Multiagent reinforcement learning in the
Iterated Prisoner’s Dilemma”. In: Biosystems 37 (1996), pp. 147–166. doi: https:
//doi.org/10.1016/0303-2647(95)01551-5.
[16] Baihan Lin, Djallel Bouneffouf, and Guillermo Cecchi. “Online learning in Iterated
Prisoner’s Dilemma to mimic human behavior”. In: PRICAI 2022: Trends in Artificial
Intelligence. Ed. by Sankalp Khanna, Jian Cao, Quan Bai, and Guandong Xu. Lecture
Notes in Computer Science. Cham: Springer Nature Switzerland, 2022, pp. 134–147.
doi: 10.1007/978-3-031-20868-3_10.
[17] William H. Press and Freeman J. Dyson. “Iterated Prisoner’s Dilemma contains
strategies that dominate any evolutionary opponent”. In: Proceedings of the Na-
tional Academy of Sciences 109.26 (2012), pp. 10409–10413. doi: 10.1073/pnas.
1206569109.
[18] Tuomas W. Sandholm and Robert H. Crites. “Multiagent reinforcement learning in
the Iterated Prisoner’s Dilemma”. In: Biosystems 37.1 (1996), pp. 147–166. doi: 10.
1016/0303-2647(95)01551-5.
[19] DimitrijeMarković,HrvojeStojić,SarahSchwöbel,andStefanJ.Kiebel.“Anempirical
evaluation of active inference in multi-armed bandits”. In: Neural Networks 144 (2021),
pp. 229–246. doi: 10.1016/j.neunet.2021.08.018.
[20] Zafeirios Fountas, Noor Sajid, Pedro A.M. Mediano, and Karl J. Friston. “Deep active
inference agents using Monte-Carlo methods”. In: Proceedings of the 34th International
Conference on Neural Information Processing Systems. NIPS’20. Curran Associates
Inc., 2020. doi: 10.5555/3495724.3496702.
[21] Conor Heins, Beren Millidge, Daphne Demekas, Brennan Klein, Karl J. Friston, Iain
D. Couzin, and Alexander Tschantz. “pymdp: A Python library for active inference
indiscrete state spaces”. In: Journal of Open Source Software 7.73 (2022), p. 4098.
issn: 2475-9066. doi: 10.21105/joss.04098.
[22] Lorens A. Imhof, Drew Fudenberg, and Martin A. Nowak. “Tit-for-tat or win-stay,
lose-shift?” In: Journal of Theoretical Biology 247.3 (2007), pp. 574–580. doi: 10.
1016/j.jtbi.2007.03.027.
14
[23] Claus Wedekind and Manfred Milinski. “Human cooperation in the simultaneous and
the alternating Prisoner’s Dilemma: Pavlov versus Generous Tit-for-Tat”. In: Proceed-
ings of the National Academy of Sciences 93.7 (1996), pp. 2686–2689. doi: 10.1073/
pnas.93.7.2686.
[24] Karl J. Friston, Francesco Rigoli, Dimitri Ognibene, Christoph Mathys, Thomas
Fitzgerald, and Giovanni Pezzulo. “Active inference and epistemic value”. In: Cognitive
Neuroscience 6.4 (2015), pp. 187–214. doi: 10.1080/17588928.2015.1020053.
15
A Supplementary Information
A.1 Generative Model
In this section, we describe the Prisoner’s Dilemma game as a two-agent active inference sys-
tem and determine the conditions under which the agents reach the optimal state of constant
cooperative play, avoiding the Nash equilibrium. To enable active inference agents to reach
the cooperative steady state, we invoke the notion of parameter learning; specifically, the
ability of agents to infer likely sequences of game states by updating posterior beliefs about
transition probabilities. These transition probabilities parameterise a likelihood model that
describes transitions between game states (e.g., the transition from the state of ‘cooperate-
cooperate’ to ‘cooperate-defect’). Under active inference, this parameter learning is cast as
a problem of inferring generative model parameters. Usually, parameter inference unfolds
on a slow timescale (hence the term ‘learning’) relative to ‘fast’ inference of hidden states [1]
(See Table A.1 for full description of model parameters).
Variable Name Notation
Hidden States s ∈ {CC,CD,DC,DD}
Observations o ∈ {CC,CD,DC,DD}
Actions u ∈ {uC,uD}
Observation Model P(o |s ;A) = Cat(A)
t t
Transition Model P(s |s ,u ;B) = Cat(B)
t+1 t−1 t−1
(cid:81)
Transition Model Parameter P(B) = P(B ), P(B ) = Dir(b )
ju •ju •ju •ju
Initial State Prior P(s ;D) = Cat(D)
1
˜
‘Biased’ State Prior (Reward) P(s;C) = Cat(C), s.t. lnC = [3,1,4,2]
Table A.1: Generative model variables and notation.
The agent’s generative model is a Markov Decision Process [2] that encodes a joint
distribution over sequences of hidden states s observations o , actions u , and model
1:T 1:T 1:T
parameters A,B,D [3]. Markov Decision Processes assume that the dynamics are shallow,
with single-timestep dependency P(s |s ,u ;B); this Markov property means we can write
t+1 t t
the generative model as a product of time-dependent distributions:
T−1
(cid:89)
P(o ,s ,u ,A,B) = P(s ;D)P(π)P(A)P(B)P(D) P(o |s ;A)P(s |s ,u ;B)
1:T 1:T 1:T 1 t+1 t+1 t+1 t t
t=1
(A.1)
multiplied by initial priors over hidden states, policies, and parameters.
16
The hidden states s consist of a single factor with four possible states or levels, corre-
sponding to the game states (the four combinations of possible two-player choices): CC, CD,
DC, and DD. This game state factor comprises the primary random variable in each agent’s
model.
In our notation, the first letter of each game state corresponds to the focal agent’s choice,
and the second letter corresponds to that of its opponent. In our formulation, agents have
precise knowledge of the current game state, which they technically infer through (unam-
biguous) observation of their and their opponent’s action. Uncertainty comes into the game
insofar as agents must predict the subsequent game state and then act based on their pre-
dictions and their desires to maximise utility.
Thereisoneobservationmodalitywithfourobservations, whichagaincorresponddirectly
tothefourgamestates. Therefore, thefourobservationsareCC,CD,DC,andDD.Notethat
the agents will only observe the game state after-the-fact, i.e., each observation corresponds
to the game state in the previous round of iterative play. This is because in the Prisoner’s
Dilemma, the agents perform their actions at any given trial without knowing what their
opponent will do in that trial, but in iterative play, the agents can build a strategy over time
by observing the resulting game states after each trial ends.
A.1.1 Observation Likelihood
The observation model P(o |s ,A) is a conditional distribution encoding the agent’s beliefs
t t
about the relationship between the current (hidden) game state and its concurrent observa-
tion. Also known as the likelihood model, the agent uses this distribution to infer the most
likely game state, given an observation thereof.
In the simulations presented here, we assume that agents are equipped with a determin-
istic, unambiguous observation model, i.e., observations are deterministic indicators of the
game state. In the discrete state space models common in active inference, likelihoods like
P(o |s ,A) are often represented as multidimensional arrays (e.g., matrices) whose values are
t t
populated by parameters; in the case of the observation model, we represent this likelihood
directly as a matrix A whose entries are given by the likelihood parameters A. Hereafter
we use boldface X to indicate a representation of Categorical parameters in terms of vectors
and matrices, and use the standard italic notation X to indicate the random variable in the
generative model (e.g. P(A)). When we have an unambiguous or precise likelihood mapping,
this matrix is the identity matrix, representing the mapping from hidden states (columns)
to observations (rows):
P(o = i|s = j,[A] ) = δ(i−j) (A.2)
t t ij
 
1 0 0 0
0 1 0 0
A =   = I (A.3)
0 0 1 0
0 0 0 1
17
An agent with such a precise likelihood model will infer the game state in the previous
round of iterative play entirely based on the observed game state.
However, one can imagine introducing uncertainty into an agent’s beliefs by adding off-
diagonal, positive values into the A matrix – this would correspond to the agent believing
that game state observations are ambiguous with respect to the true game state. Concretely,
we could imagine that one agent might receive a misleading signal indicating that its oppo-
nent defected when they actually cooperated. A simple way to parameterise this uncertainty
is through an inverse temperature parameter ψ, which makes the A matrix totally uninfor-
mative (maximum entropy columns) in the limit of ψ → 0, and infinitely precise in the limits
of ψ → ∞:
Iψ
A = (A.4)
(cid:80)
Iψ
Finally, it is worth mentioning that we assume P(A) is infinitely precise and not subject
to learning. Therefore, we emit any parameterisation of the priors over this likelihood, while
we keep them for the transition likelihood parameters B, as we will update these in learning.
A.1.2 Reward
Different game states are assigned different rewards or desirabilities under the Prisoner’s
Dilemma problem formulation. Active inference converts the notion of ‘reward’ into prior
probability by equipping agents with biased prior beliefs about future states or observations
[4]. In the context of planning actions, this biased prior serves the role of a “goal-vector”
or reward function [3]. We denote this as a biased prior over states in our agent’s model
P ˜ (s;C)5. This special ‘goal prior’ is parameterised by a vector of Categorical parameters
˜
C. Reward and prior probability can be straightforwardly related via the relation P(s) ∝
exp(r) [5]; therefore, we typically parameterise C using relative log probabilities or nats,
i.e., C = lnP ˜ (s) + Z. Following from Table 1, the most desirable observation is sDC (the
agent defects and the opponent cooperates), followed by sCC (both players cooperate), then
sDD (both players defect), and finally sCD (the agent cooperates and the opponent defects).
Therefore, our C vector is C = [3,1,4,2].
Notethatthevaluesofthesenumbershaveaneffectonthedesirabilityoftheobservations
and therefore will impact the agents action-planning such that they plan actions that they
infer will result in the observation of the most desirable state. Changing the values of these
rewards will change the incentive and behaviour of the agents.
A.1.3 Different reward parameterizations
We can parameterise the reward function C in terms of a single precision that makes a
single ordered reward function with the constraints r < r < r < r more or less
CD DD CC DC
shallow/steep. We do this using the softmax (normalised exponential transformation):
5NotethatinmanyformulationsofactiveinferencethisisformulatedasaprioroverobservationsP˜(o;C).
18
  
r
CC
C = σ       r r C D D C    ,β    , where C CC = (cid:80) ex i p e ( x β p r ( C β C r ) i )
r
DD
(cid:32) (cid:33)
(cid:88)
lnC = βr −ln exp(βr )
CC CC i
i
 
r
CC
=⇒ lnC ∝ β


r CD
 (A.5)
r DC
r
DD
A.1.4 Policies
A policy π is comprised of individual actions, or control states, π = {u ,u ,...u }. At each
1 2 H
trial of iterative play, the agents can either defect or cooperate. This means that the policy
space consists of two control states, namely uC and uD. Once the action is inferred, the
intersection of both agents’ actions will result in the realised game state.
A.1.5 Transition Likelihood
The transition matrix encodes the beliefs that the agent holds about how game states will
evolve given previous trials and their actions. Because action selection under active inference
depends on model-based planning, this transition model also directly determines the agent’s
strategy. Although in this work we focus on how agents can automatically learn the game’s
dynamics and thus their strategies through experience, we nevertheless begin by constraining
what agents can learn by initialising agents‘ beliefs about transition dynamics, so that they
assumethattwogamestatetransitionsarealwaysimpossible. Agentsbelievethatwhenthey
cooperate, there is zero probability that the next state will be DC or DD, and conversely,
when they defect, they believe there is zero probability that the next state will be CD or
CC. Therefore, the transition matrix encodes the agent’s assumptions about whether the
other will cooperate or defect in the next trial, given the outcome of the current trial and
the agent’s own action.
We use existing formulations of parameter learning under active inference to allow our
agents to update their beliefs about transition model over time based on experience. Tech-
nically, the agents are updating a Dirichlet posterior belief over the Categorical parameters
B that characterise its transition model (a transition probability matrix, mapping from past
to current game states, further conditioned on action). They update this matrix of posterior
Dirichlet parameters at the end of each trial, based on that trial’s outcome.
At the beginning of iterative play, the agent will be initialised with no prior opinion or
knowledge about which of the possible transitions are more likely given its actions (aside
19
from the zero constraints laid out above). These uniform initial transition distributions are
shown in (A.6) and (A.7).
 
0.5 0.5 0.5 0.5
0.5 0.5 0.5 0.5
P(s |s ,u = C) =   (A.6)
t+1 t t  0 0 0 0 
0 0 0 0
 
0 0 0 0
 0 0 0 0 
P(s |s ,u = D) =   (A.7)
t+1 t t 0.5 0.5 0.5 0.5
0.5 0.5 0.5 0.5
Attheconclusionofeachtrialduringasessionofiterativeplay, agivenagentobservesthe
gamestateoftheprevioustrialandupdatesitsbeliefsabouttransitionsbasedontherealised
states and its actions. As these transition dynamics are learned, the agent is simultaneously
learning a strategy based on planning the most optimal action (cooperate or defect), given
its evolving beliefs.
A.2 Inference
A.2.1 State inference
At each trial of iterative play, the agents first infer the game state by inverting their Marko-
vian (POMDP) generative model using ongoing observations o .
t
The agent’s hidden state inference involves optimising a variational posterior over hid-
˜
den states and policies Q(s ,π) as a categorical distribution with parameters ϕ that are
1:T
factorised ‘mean-field’-style across timesteps [6]:
(cid:89)
˜
Q(s ,π;ϕ) = Q(π;ϕ ) Q(s ;ϕ )
1:T π t s,t
1:T
˜
Where the variational parameters ϕ = {ϕ ,ϕ } are themselves segregated into policy-
π s1:T
specific parameters ϕ and hidden-state-specific parameters ϕ .
π s1:T
At each timestep t, the agent performs inference by optimising the posterior parameters
˜
ϕ to minimise the timestep-specific variational free energy F , which due to the Markovian
t
factorisation of the generative model and mean-field factorisation of the posterior, can be
expressed in terms of only the generative model of the current timestep P(o ,s ,π,A,B,C):
t t
(cid:104) (cid:105)
F = E lnQ(s ,π;ϕ ˜ )−lnP(o ,s ,π,A,B,C) (A.8)
t Q(st,π;ϕ˜
t
) t t t t
20
˜∗
The optimal posterior parameters ϕ are those that minimise the free energy in (A.8)
and can be found by solving exactly for the fixed points of F . We begin by solving for the
t
parameters of the variational beliefs about hidden states ϕ :
st
∂F
t
= 0
∂ϕ
st
=⇒ ϕ∗ = σ (cid:0) lnATo +ln(B ·ϕ∗ ) (cid:1) (A.9)
st t ut−1 st−1
where σ represents the softmax (or normalised exponential) transform of a vector. The ith
entry of the softmaxed output is given by:
exp(x )
σ(x) ≜ i (A.10)
i (cid:80)
exp(x )
j j
The initial matrix-vector product in the last line of (A.9) lnATo represents the con-
t
tribution of sensory evidence to inference, and can be thought of as picking out the row of
the A matrix that corresponds to the observation at timestep t. The second matrix vec-
tor product ln(B · ϕ∗ ) represents the contribution of prior information to inference.
ut−1 st−1
This simple form is a consequence of the mean-field factorisation of the variational parame-
ters ϕ across timesteps and an ‘empirical prior’ assumption, where the prior term of the
s1:T
generative model P(s ) = E [P(s |s ,u ,B)] is evaluated at the parameters of the
t P(st−1) t t−1 t−1
previous timestep’s variational posterior, in a manner reminiscent of a belief propagation
step or empirical Bayes:
P(s ) = E [P(s |s ,u ,B)]
t P(st−1) t t−1 t−1
≈ E [P(s |s ,u ,B)] (A.11)
Q(st−1;ϕ
st−1
) t t−1 t−1
We can simplify the expression for the parameters of the variational beliefs due to the
Iψ
unambiguous form of the observation likelihood with infinite precision in (A.3), A = ,
(cid:80)
Iψ
as well as the fact that the agents are taking identical actions at every trial, thus limiting
the state space to {CC, DD} which implies that inference can be solved for exactly for any
trial t > 0 as
(cid:32) (cid:33)
(cid:18) Iψ (cid:19)T
ϕ∗ = σ ln o +ln (cid:0) B ·ϕ∗ (cid:1) (A.12)
st (cid:80) Iψ t ut−1 st−1
= lim σ (cid:16) ψln (cid:0) ITo (cid:1) +ln (cid:0) B ·ϕ∗ (cid:1) −ln (cid:88) Iψ (cid:17) (A.13)
ψ→∞
t ut−1 st
(cid:0) (cid:0) (cid:1)(cid:1)
= σ ln ITo = ITo (A.14)
1 1
21
A.2.2 Policy inference
Under active inference, action selection and planning are cast as an inference problem, where
policies are treated as a latent variable to be inferred. This has deep homology to contem-
porary approaches to model-based planning in reinforcement learning, such as planning as
inference and control as inference [5, 7–9]. In particular, active inference agents optimise a
variational posterior over policies Q(π). However, because policies inherently require esti-
mation of future, unobserved states, we use an augmented, ‘predictive’ generative model to
perform this policy inference. This predictive generative model is importantly augmented
˜
with the biased prior distribution over states P(s;C). Beliefs about policies, similar to those
about hidden states, are optimised by minimising a free energy functional of beliefs about
the consequences of action under the predictive generative model. This functional is known
as the expected free energy and exhibits many desirable properties such as a natural balance
between information-seeking (‘exploration’) and goal-directedness (‘exploitation’) [10]. The
approximate posterior over policies Q(π) is also a Categorical distribution with parameters
ϕ ; the optimal setting of these parameters ϕ∗ minimises the expected free energy, leading
u u
to the relationship:
Q(π;ϕ ) = σ(−G(π))
u
H
(cid:88)
G(π) = G (u ) (A.15)
t+τ t+τ−1
τ=1
The second line shows that the expected free energy of a policy is the sum of the expected
free energies that accrue for each action that comprises the policy: π = {u ,u ,...u }. For
1 2 H
the present purposes we only consider 1-step ahead policies (H = 1). This means that the
expected free energy of a policy is simply the expected free energy computed one timestep
into the future G (u ).
t+1 t
The expected free energy can be decomposed into expected ambiguity and risk terms:
G (u ) = E [H[P(o |s )]]+D (Q(s |u ) ∥ lnP(s |C)) (A.16)
t+1 t Q(st+1|ut) t+1 t+1 KL t+1 t t+1
We can write this general expression in terms of sufficient statistics of the variational
distribution over hidden states ϕ∗ . The ambiguity term of the expected free energy vanishes
st
because the agent’s likelihood matrix is the identity:
AB ·ϕ∗ · (cid:0) ln(AB ·ϕ∗ )−lnC (cid:1) (A.17)
t st t st
= B ·ϕ∗ (cid:0) lnB ·ϕ∗ −lnC (cid:1) −(AlnA)·ϕ∗ (A.18)
t st t st st
(cid:124) (cid:123)(cid:122) (cid:125)
=0
22
A.2.3 Action Selection
Having optimised a posterior over policies (which in this context simply reduce to control
states), action selection simply consists of sampling the action at trial t that minimises the
expected free energy, i.e., sampling an action from the posterior marginal over actions.
ϕ = σ(−G) (A.19)
u
u ∼ Q(u ;ϕ ) (A.20)
t+1 t+1 u
This can be done either deterministically by selecting the most probable control state at
every timestep:
u = argmaxQ(u ;ϕ ) (A.21)
t+1 t+1 u
u
Or, this can be done stochastically by sampling from the posterior over actions. The
stochasticity of this sampling can be further tuned by sampling from a transformed action
posterior scaled by a temperature parameter α.
u ∼ Q(u ;ϕ,α) (A.22)
t+1 t+1
A.2.4 B matrix learning
After every trial of iterative play, each agent updates its posterior beliefs about the transition
model B by optimizing Dirichlet parameters ϕ , which are the sufficient statistics of a
b
Dirichlet parameterization of the posterior Q(B;ϕ ). This is also known as ‘learning’ in the
b
active inference literature, and analogised to neuronal processes such as synaptic plasticity,
which typically occurs on a slower timescale than hidden state inference (analogised to rapid
dynamics of neural firing rates) [1]. Dirichlet distributions are used as the parameterizations
of discrete Categorical likelihood matrices, due to their natural role as conjugate priors for
the Categorical distribution.
We supplement the generative model with an additional prior over the parameters of the
transition model, the Dirichlet distribution P(B;b) parameterised by a vector of positive
real hyperparameters b, that can also be interpreted as ‘pseudocounts’, i.e., how many times
has the agent seen this particular transition occur, before the simulation starts. Alongside
this prior we introduce a variational posterior over B that is also a Dirichlet distribution
Q(B;ϕ ). This leads to a new expression for the variational free energy at a given time
b
point, which includes an additional Kullback-Leibler divergence between the variational and
generative model Dirichlet distributions over B [3]:
(cid:104) (cid:16) (cid:17) (cid:105)
F = E lnQ s ,u ,B;ϕ ˜ −lnP (o ,s ,u ,A,B,C;A,b,C)
t Q(st,ut,B;ϕ˜) t t t t t
= E (cid:2) lnQ (cid:0) s ,u ;ϕ (cid:1) −lnP (o ,s ,u ,A,C;A,C) (cid:3) +D (Q(B;ϕ ) ∥ P(B;b))
Qst,ut;ϕ s,u t t s,u t t t KL b
(A.23)
23
This new expression means that when we minimise F with respect to the variational
t
(Dirichlet) parameters ϕ , we get a closed-form expression for the variational beliefs over
b
B, which can be expressed in terms of the Dirichlet prior parameters b and the variational
posterior over hidden states at current and previous timesteps ϕ and ϕ .
st st−1
ϕ∗
B = b (A.24)
t+1
ϕ
b,0
ϕ∗ = b+η(ϕ ⊗ϕ ) (A.25)
b st st−1
where(A.24)representstheupdatetotheDirichletpriorforthetransitiondistributionduring
learning. This is updated with respect to the learning rate η and the transition probabilities
given the previously performed action a . It is this normalised updated Dirichlet prior that
t−1
then becomes the new transition probability distribution for the following trial.
Theupdatestothetransitionmodelaregovernedbythesequenceofgamestates. Wecan
imagine a fictive 1-turn sequence (two trials) to imagine how a particular sequence influences
learning. If at one trial, the agents both cooperated, then they will infer that the game state
was CC. Given this belief, they will infer which action to take. If they choose to defect,
hoping that the opponent will cooperate again, the resulting inferred state will be that the
optimal action is u = uD, and after the trial they will observe the resulting state, DD. At
t
this point, the agents will update their beliefs about likely transitions (encoded in the B
matrix parameters), such that there will be a small incremental increase in the conditional
probability of DD, given a past state of CC and a past action of uD, i.e., P(s = DD|s =
t+1 t
CC,u = uD). The size of this update is determined by a learning rate parameter η.
t
A.3 Deriving the analytic form of the transition function
When two deterministic agents have the same learning rate, they will perform the same
action at every timestep. This has the consequence that the two-agent system will only ever
explore two out of four states, namely CC and DD.
The posterior belief can be represented as a vector of its parameters, and in the solution
of two identical agents, it can take two possible values, which we denote as sCC and sDD.
Because the likelihood distribution is the identity matrix, these will be maximally precise
vectors:
   
1 0
0 0
sCC =   sDD =   (A.26)
0 0
0 1
The initial Dirichlet parameters of the prior distribution over the transition model are,
for the cooperate and defect-conditioned transitions, respectively,
24
   
0.5 0.5 0.5 0.5 0 0 0 0
0.5 0.5 0.5 0.5  0 0 0 0 
bC =   bD =   (A.27)
0  0 0 0 0  0 0.5 0.5 0.5 0.5
0 0 0 0 0.5 0.5 0.5 0.5
This means that at each timestep, there are four possible updates to the parameters of
each agent’s variational posterior over the transition model ϕ , given the two variational
b
beliefs a given agent might have (sCC and sDD):

bC +η ·(sCC ⊗sCC)t
 0


bD +η ·(sDD ⊗sCC)t
ϕ∗ = 0 (A.28)
bt+1 bC +η ·(sCC ⊗sDD)t
  0

 bD +η ·(sDD ⊗sDD)t
0
When the agents are both defecting (e.g., in the first timestep when the most likely action
is defect), then the update rule for the weights of the Dirichlet parameters of the transition
matrix is governed by:
(cid:0) (cid:1)
ϕ∗ = bD +η sDD ⊗sDD t (A.29)
bt<τ1 0
ϕ∗
B =
bt<τ1
(A.30)
t+1<τ1 ϕ∗
bt<τ1 ,0
At some critical time τ the probability of cooperation exceeds that of defection, due to
1
the change in the expected free energies of the two actions G (u = C) < G (u = D). This
τ1 τ1
triggers the beginning of the so-called “oscillation period” (see Section 2 in the main text),
where agents periodically oscillate between cooperating and defecting with the same phase.
We can expand this condition according to (A.18) into the following form:
(cid:0) (cid:1) (cid:0) (cid:1)
BC ·sDD · lnBC ·sDD −lnC = BD ·sDD · lnBD ·sDD −lnC (A.31)
0 τ1 0 τ1 τ1 τ1 τ1 τ1
As shown in Section A.4, the equality in (A.31) can be written in terms of η, C and τ :
1
(cid:20) (cid:21) (cid:18) (cid:19)
1 1 1+2ητ 1 1
1
ln +(1+2ητ )ln = ln , (A.32)
1
(2+η2τ ) 2(1+ητ )C 2(1+ητ )C 2 4C C
1 1 3 1 4 1 2
Letting y = 1 , which will always be between 0 and 1, we can now rewrite (A.32) as
2+2ητ1
(cid:18) (cid:19)
1 1
ylny −ylnC +(1−y)ln(1−y)−(1−y)lnC = ln (A.33)
3 4
2 4C C
1 2
25
To derive τ in terms of η, we must make an approximation. We use the fact that when
1
y is between 0 and 1, it can be approximated by y ≈ Ayb(y−1). This gives us the following
expression as an approximation for (35)
(cid:18) (cid:19)
1 1
Ayb(y −1)−ylnC −A(1−y)by −(1−y)lnC = ln (A.34)
3 4
2 4C C
1 2
The optimal values for the approximation are A = 4774 and b = 3, however, for simplicity,
4563 5
we let A = 1 and b = 1 and then the desired root of (A.34) can be solved as:
(cid:115)
1 C C C 1
y = (cid:0) ln 3 +2− (ln 4 −2)2 −8(−ln √ 4 − ) (cid:1) (A.35)
4 C C 2 C C 5
4 3 1 2
Therefore, since y = 1 , we have that
2+2ητ1
R
1
τ ≈ (A.36)
1
η
where
2
R = −1 (A.37)
1 (cid:113)
ln C3 +2− (ln C4 −2)2 −8(−ln √ C4 − 1)
C4 C3 2 C1C2 5
We now have an approximation for τ in terms of η and a constant R , which depends
1 1
on the reward C which can be parameterised by β according to (A.5).
R (β)
1
τ = (A.38)
1
η
for some precision β. We can plot this equation for different values of β to see how the values
in the reward function influence τ (see Figure A.1).
1
For τ < t < τ (i.e., during the period of oscillation dynamics shown in Figure 2), the
1 2
update rules then become:
1
ϕD = bD + η(sDD ⊗sCC)(t−τ ) (A.39)
bτ1<t<τ2 τ1 2 1
1
ϕC = bC + η(sCC ⊗sDD)(t−τ ) (A.40)
bτ1<t<τ2 0 2 1
The update rule changes from (A.39) to (A.40) at every other trial, from conditioning on
the previous action being D, to being C. The oscillation period persists until some time τ .
2
26
Figure A.1: Dynamics of the expected free energy. Left: The difference of EFE for
cooperation and defection (vertical axis). The roots of this equation are the values of τ for
1
different values of β, parameterizing the values in the reward function C as per (A.42), with
η = 0.2. It is clear that with a higher value of β, it will take agents longer to cooperate,
i.e. τ will be larger, demonstrated by the horizontal translations of the curves as β increases.
1
Right: Values of τ for different values of β parameterizing the reward function, at different
1
learning rates. Again, we see that as β increases, τ increases. We can also see that larger
1
η competes with higher β to decrease τ , as the agents update their transition probability
1
distributions at a higher frequency.
At τ we will have that, for the first time, G (u = C,ϕC) < G (u = C,ϕD). Again, we can
2 0 0
expand this according to (A.18) as:
(cid:0) (cid:1) (cid:0) (cid:1)
BC ·sCC · lnBC ·sCC −lnC = BD ·sCC · lnBD ·sCC −lnC (A.41)
τ2 τ2 τ2 τ1
Rewriting this equation in terms of η, τ , τ , and C leads to the following inequality (for
1 2
full derivation, see Section A.5):
(cid:20) (cid:21)
1 1 1 1+η(τ −τ ) 1
2 1
ln[ ( )]+(1+η(τ −τ ))ln = − ln(4C C )
2 1 1 2
2+η(τ −τ ) C 2+η(τ −τ ) C (2+η(τ −τ )) 2
2 1 3 2 1 4 2 1
(A.42)
This time, we let y = 1 and we have:
2+η(τ2−τ1)
1
(y −1)lny −ylnC +(1−y)ln(1−y)−(1−y)lnC = − ln(4C C ) (A.43)
3 4 1 2
2
27
Now, notice that this is the exact same equation as (A.33) above, which we know we can
approximate as (A.34). We can then write our solution in terms of R :
1
1 1 1 3
τ ≈ ( −2)+τ = ( R ) (A.44)
2 1 1
η y η 2
The resulting equation is obtained in terms of R , where R = 3R .
2 2 2 1
R (β)
2
τ ≈ (A.45)
2
η
After τ , agents will cooperate indefinitely according to the final steady state update rule:
2
(cid:0) (cid:1)
ϕ∗ = bC +η sCC ⊗sCC t (A.46)
bt>τ2 τ2
ϕ∗
B =
bt>τ2
(A.47)
t+1>τ2 ϕ∗
bt>τ2 ,0
A.4 Full derivation of τ
1
Here we derive τ for the following equality from (A.31):
1
(cid:0) (cid:1) (cid:0) (cid:1)
BC ·sDD · lnBC ·sDD −lnC = BD ·sDD · lnBD ·sDD −lnC (A.48)
0 τ1 0 τ1 τ1 τ1 τ1 τ1
Using the following:
ϕ
B =
bt
(A.49)
t
ϕ
bt,0
ϕD = bD +η(sDD ⊗sDD)t (A.50)
bt<τ1 0
ϕC = bC (A.51)
bt<τ1 0
   
0.5 0.5 0.5 0.5 0 0 0 0
0.5 0.5 0.5 0.5  0 0 0 0 
bC =   bD =   (A.52)
0  0 0 0 0  0 0.5 0.5 0.5 0.5
0 0 0 0 0.5 0.5 0.5 0.5
sDD = e , (A.53)
4
we have:
(cid:32) (cid:33) (cid:32) (cid:33)
ϕC ϕC ϕD ϕD
b0 ·sDD · ln b0 ·sDD −lnC = bτ1 ·sDD · ln bτ1 ·sDD −lnC (A.54)
ϕC ϕC ϕD ϕD
b0,0 b0,0 bτ1 ,0 bτ1 ,0
28
On the LHS:
(cid:32) (cid:33)
ϕC ϕC bC ·e 1
b0 ·sDD · ln b0 ·sDD −lnC = (bC ·e )·ln 0 4 = − ln(4C C ) (A.55)
ϕC ϕC 0 4 C 2 1 2
b0,0 b0,0
On the RHS:
(cid:32) (cid:33) (cid:32) (cid:33)
ϕD ϕD ϕ ϕ
bτ1 ·sDD · ln bτ1 ·sDD −lnC = bD τ1 ,j=4 · ln bD τ1 ,j=4 −lnC (A.56)
ϕD ϕD ϕD ϕD
bτ1 ,0 bτ1 ,0 bτ1 ,0 bτ1 ,0
1 1 1 11+2ητ 1+ητ
1 1
= ln( )+ ln( ) (A.57)
21+ητ 2(1+ητ )C 2 1+ητ 2(1+ητ )C
1 1 3 1 1 4
Our equality is therefore:
(cid:20) (cid:21)
1 1 1+2ητ 1
1
ln +(1+2ητ )ln = − ln(4C C ) (A.58)
1 1 2
(2+2ητ ) 2(1+ητ )C 2(1+ητ )C 2
1 1 3 1 4
A.5 Full derivation of τ
2
Our condition for deriving τ in terms of the expected free energies is
2
(cid:0) (cid:1) (cid:0) (cid:1)
BC ·sCC · lnBC ·sCC −lnC = BD ·sCC · lnBD ·sCC −lnC (A.59)
τ2 τ2 τ2 τ2
Here our ϕs between trials τ and τ are:
1 2
1
ϕD = ϕD + η(sDD ⊗sCC)(t−τ ) (A.60)
bτ1<t<τ2 bt<τ1 2 1
1
ϕC = bC + η(sCC ⊗sDD)(t−τ ) (A.61)
bτ1<t<τ2 0 2 1
And to solve for τ our inequality is
2
(cid:32) (cid:33) (cid:32) (cid:33)
ϕC ϕC ϕD ϕD
bτ2 ·sCC · ln bτ2 ·sCC −lnC = bτ2 ·sDD · ln bτ2 ·sDD −lnC (A.62)
ϕC ϕC ϕD ϕD
bτ2 ,0 bτ2 ,0 bτ2 ,0 bτ2 ,0
On the LHS we have:
(cid:32) (cid:33)
ϕC ϕC
1
bτ2 ·sCC · ln bτ2 ·sCC −lnC = − ln(4C C ) (A.63)
ϕC ϕC 2 1 2
bτ2 ,0 bτ2 ,0
29
On the RHS:
 
0
ϕC
bτ2 ·sDD = 1   0   (A.64)
ϕC 2+η(τ −τ )  1 
bτ2 ,0 2 1
1+η(τ −τ )
2 1
(cid:20) (cid:20) (cid:21) (cid:21)
1 1 1 1+η(τ −τ )
2 1
ln ( ) +(1+η(τ −τ ))ln (A.65)
2 1
2+η(τ −τ ) C 2+η(τ −τ ) C (2+η(τ −τ ))
2 1 3 2 1 4 2 1
(cid:20) (cid:21)
1 C 1+η(τ −τ )
4 2 1
ln ) +ln (A.66)
2+η(τ −τ ) C (1+η(τ −τ ) C (2+η(τ −τ ))
2 1 3 2 1 4 2 1
Finally, our inequality is:
(cid:34)
(cid:20) (cid:18) (cid:19)(cid:21)
1 1 1
ln +
2+η(τ −τ ) C 2+η(τ −τ )
2 1 3 2 1
(cid:35)
1+η(τ −τ ) 1
2 1
(1+η(τ −τ ))ln = − ln(4C C ) (A.67)
2 1 1 2
C (2+η(τ −τ )) 2
4 2 1
Supplemental References
[1] Karl J. Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, John
O’Doherty, and Giovanni Pezzulo. “Active inference and learning”. In: Neuroscience &
Biobehavioral Reviews 68 (2016), pp. 862–879. doi: 10.1016/j.neubiorev.2016.06.
022.
[2] Martin L. Puterman. “Markov decision processes”. In: Handbooks in Operations Re-
search and Management Science. Vol. 2. Stochastic Models. Elsevier, 1990, pp. 331–
434. doi: 10.1016/S0927-0507(05)80172-0.
[3] Conor Heins, Beren Millidge, Daphne Demekas, Brennan Klein, Karl J. Friston, Iain
D. Couzin, and Alexander Tschantz. “pymdp: A Python library for active inference
indiscrete state spaces”. In: Journal of Open Source Software 7.73 (2022), p. 4098.
issn: 2475-9066. doi: 10.21105/joss.04098.
[4] Karl J. Friston, Jean Daunizeau, and Stefan J. Kiebel. “Reinforcement learning or
active inference?” In: PloS One 4.7 (2009), e6421. doi: 10.1371/journal.pone.
0006421.
30
[5] Beren Millidge, Alexander Tschantz, Anil K. Seth, and Christopher L. Buckley. “On
therelationshipbetweenactiveinferenceandcontrolasinference”.In:Active Inference.
Ed. by Tim Verbelen, Pablo Lanillos, Christopher L. Buckley, and Cedric De Boom.
Communications in Computer and Information Science. Cham: Springer International
Publishing, 2020, pp. 3–11. doi: 10.1007/978-3-030-64919-7_1.
[6] David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. “Variational inference: A review
for statisticians”. In: Journal of the American Statistical Association 112.518 (2017),
pp. 859–877. doi: 10.1080/01621459.2017.1285773.
[7] Hagai Attias. “Planning by probabilistic inference”. In: Proceedings of the Ninth In-
ternational Workshop on Artificial Intelligence and Statistics. Ed. by Christopher M.
BishopandBrendanJ.Frey.Vol.R4.ProceedingsofMachineLearningResearch.2003,
pp. 9–16. url: https://proceedings.mlr.press/r4/attias03a.html.
[8] Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas
Heess, and Martin Riedmiller. “Maximum a posteriori policy optimisation”. In: Inter-
national Conference on Learning Representations. 2018. url: https://openreview.
net/forum?id=S1ANxQW0b.
[9] Matthew Botvinick and Marc Toussaint. “Planning as inference”. In: Trends in Cogni-
tive Sciences 16.10 (2012), pp. 485–488. doi: 10.1016/j.tics.2012.08.006.
[10] Beren Millidge, Alexander Tschantz, and Christopher L. Buckley. “Whence the ex-
pected free energy?” In: Neural Computation 33.2 (2021), pp. 447–482. doi: 10.1162/
neco_a_01354.
31

=== REVISE TO ===
PROFESSIONAL TONE: Begin directly with content - NO conversational openings like 'Okay, here's...'

1. Fix all issues above
2. Title: "An analytical model of active inference in the Iterated Prisoner's Dilemma"
3. Include 10-15 quotes from paper text
   - Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
   - Use consistent quote formatting: 'The authors state: "quote"' or vary attribution phrases
   - Vary attribution phrases to avoid repetition
   - CRITICAL: Only extract quotes that actually appear in the paper text
4. ELIMINATE ALL REPETITION - each sentence must be unique
   - Check before each sentence: 'Have I already said this?' If yes, write something new
   - Vary attribution phrases - do NOT repeat 'The authors state' multiple times
5. Extract methodology, results with numbers, key quotes
6. 1000-1500 words, structured with ### headers

Generate COMPLETE revised summary.