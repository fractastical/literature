=== IMPORTANT: ISOLATE THIS PAPER ===You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.Do NOT mix information from different papers. Only use information from THIS specific paper.Paper Title: Real-World Robot Control by Deep Active Inference With a Temporally Hierarchical World ModelCitation Key: fujii2025realworldAuthors: Kentaro Fujii, Shingo MurataREMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.Year:2025Key Terms: representations, world, action, abstract, temporally, actions, robot, deep, active, hierarchical=== FULL PAPER TEXT ===Real-World Robot Control by Deep Active Inferencewith a Temporally Hierarchical World ModelKentaro Fujii1 and Shingo Murata1Abstract—Robotsinuncertainreal-worldenvironmentsmust model, and an abstract world model. The world model learns hidden state transitions to represent environmentaldynamicsfromhuman-collectedrobotactionandobservationdata [14]–[16]. The action model maps a sequence of actualactions to one of a learned set of abstract actions [17]. The abstract world model learns the relationship between the state representations learnedby the world model and the abstract action representations[18]. By leveraging the abstract world model and the abstract action representations, the framework enables efficient active inference.To evaluate the proposed method, we conducted robotexperiments in real-world environments with uncertainty.We investigated whether the framework could reduce com-putational cost, enable the robot to achieve diverse goalsinvolving multiple objects, and perform exploratoryactions to resolve environmental uncertainty.II. RELATEDWORKA. Learning from Demonstration (LfD) for Robot ControlLfD is a method to train robots by imitating humanexperts,providingsafe,task-relevantdataforlearningcontrol[19]–[24].Akeyadvancementcontributingtorecentprogress in LfD for robotics is the idea of generating multi-step action sequences, rather than only single-step actions[1]–[3],[17],[25].However,amajorchallengeinLfDisthedifficulty of generalizing to environments with uncertainty,even when trained on large amounts of expert demonstrations[4]. In this work, we focus on the approach that usesquantizedfeaturesextractedfromactionsequences[17],andtreattheextractedfeaturesasabstractactionrepresentations.B. World ModelA world model captures the dynamics of the environmentby modeling the relationship between data (observations),their latent causes (hidden states), and actions. They haverecently attracted significant attention in the context ofmodel-based reinforcement learning [14], [15], especially inartificial agents and robotics [26]. However, when robotslearn using a world model, their performance is constrainedbythemodel’scapabilitytorepresentenvironmentaldynamics[27], [28]. In particular, learning long-term dependenciesin the environment remains a challenge. One solution is tointroduce temporal hierarchy into the model structure[27],[29]–[31].Furthermore,byincorporatingabstractactionrepresentations that capture slow dynamics, the model canmore efficiently predict future observations and states [18].To evaluate the proposed method, we conducted robotexperiments in real-world environments with uncertainty.We investigated whether the framework could reduce com-putational cost, enable the robot to achieve diverse goalsinvolving multiple objects, and perform exploratoryactions to resolve environmental uncertainty.II. RELATEDWORKA. Learning from Demonstration (LfD) for Robot ControlLfD is a method to train robots by imitating humanexperts,providingsafe,task-relevantdataforlearningcontrol[19]–[24].Akeyadvancementcontributingtorecentprogress in LfD for robotics is the idea of generating multi-step action sequences, rather than only single-step actions[1]–[3],[17],[25].However,amajorchallengeinLfDisthedifficulty of generalizing to environments with uncertainty,even when trained on large amounts of expert demonstrations[4]. In this work, we focus on the approach that usesquantizedfeaturesextractedfromactionsequences[17],andtreattheextractedfeaturesasabstractactionrepresentations.B. World ModelA world model captures the dynamics of the environmentby modeling the relationship between data (observations),their latent causes (hidden states), and actions. They haverecently attracted significant attention in the context ofmodel-based reinforcement learning [14], [15], especially inartificial agents and robotics [26]. However, when robotslearn using a world model, their performance is constrainedbythemodel’scapabilitytorepresentenvironmentaldynamics[27], [28]. In particular, learning long-term dependenciesin the environment remains a challenge. One solution is tointroduce temporal hierarchy into the model structure[27],[29]–[31].Furthermore,byincorporatingabstractactionrepresentations that capture slow dynamics, the model canmore efficiently predict future observations and states [18].To evaluate the proposed method, we conducted robotexperiments in real-world environments with uncertainty.We investigated whether the framework could reduce com-putational cost, enable the robot to achieve diverse goalsinvolving multiple objects, and perform exploratoryactions to resolve environmental uncertainty.III. THEFORMULATIONOFACTIVEINFERENCEThe free-energy principle [5], [6], [11] is a computationaltheory that accounts for various cognitive functions. Acomputational theory that accounts for various cognitive func-tions. According to this principle, human observation-soaregeneratedbyunobservablehiddenstatesz,whichevolvein-responsetoactionsa, following a partially observable Markovdecision process [5]. The brain is assumed to model thisgenerative process with the world model. Under the free-energy principle, human perception and action aim to min-imize the surprise −logp(o). However, since directly min-imizing mutual information between the statez and theobservation o , the EFE is defined as follows [35]:G(τ)=−E [logq (z |o ,π)−logq (z |π)]qθ(oτ,zτ|π) θ τ τ θ τ−E qθ(oτ,zτ|π) [logp(o τ |o pref )]≈−E [logq (sf |zs,o )−logq (sf |zs)]qθ(oτ,zτ|π) θ τ τ τ θ τ−E [logp(o |o )]qθ(oτ,zτ|π) τ pref(9)Here, q(z ) denotes the approximate posterior over the hid-ted state z , D [q(·)||p(·)] is the Kullback–Leibler (KL)divergence. Note that the first line of (1) is equivalent to thenegative evidence lower bound [36], [37].Actioncanbeformulatedastheminimizationofexpectedfree energy (EFE), which extends variational free energy toaccount for future states and observations. Let τ > t be afuture time step, The EFE is defined as follows [35]:G(τ)≈−E [logq (z |o ,π)−logq (z |π)]qθ(oτ,zτ|π) θ τ τ θ τ−E qθ(oτ,zτ|π) [logp(o τ |o pref )]≈−E [logq (sf |zs,o )−logq (sf |zs)]qθ(oτ,zτ|π) θ τ τ τ θ τ−E [logp(o |o )]qθ(oτ,zτ|π) τ pref(9)Here, q(z ) denotes the approximate posterior over the hid-ted state z , D [q(·)||p(·)] is the Kullback–Leibler (KL)divergence. Note that the first line of (1) is equivalent to thenegative evidence lower bound [36], [37].Actioncanbeformulatedastheminimizationofexpectedfree energy (EFE), which extends variational free energy toaccount for future states and observations. Let τ > t be afuture time step, The EFE is defined as follows [35]:G(τ)≈−E [logq (z |o ,π)−logq (z |π)]qθ(oτ,zτ|π) θ τ τ θ τ−E qθ(oτ,zτ|π) [logp(o τ |o pref )]≈−E [logq (sf |zs,o )−logq (sf |zs)]qθ(oτ,zτ|π) θ τ τ τ θ τ−E [logp(o |o )]qθ