=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Prior preferences in active inference agents: soft, hard, and goal shaping
Citation Key: torresan2025prior
Authors: Filippo Torresan, Ryota Kanai, Manuel Baltieri

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Key Terms: soft, distribution, agents, agent, prior, hard, inference, active, goal, preferences

=== FULL PAPER TEXT ===

Prior preferences in active inference agents: soft, hard, and
goal shaping
FilippoTorresan1,2, RyotaKanai1, ManuelBaltieri1,2,†
1ArayaInc.,Tokyo,Japan
2SchoolofEngineeringandInformatics,UniversityofSussex,Brighton,UK
Activeinferenceproposesexpectedfreeenergyasanobjectiveforplanninganddecision-making
to adequately balance exploitative and explorative drives in learning agents. The exploitative
drive, or what an agent wants to achieve, is formalised as the Kullback-Leibler divergence be-
tween a variational probability distribution, updated at each inference step, and a preference
probability distribution that indicates what states or observations are more likely for the agent,
hence determining the agent’s goal in a certain environment. In the literature, the questions of
how the preference distribution should be specified and of how a certain specification impacts
inferenceandlearninginanactiveinferenceagenthavebeengivenhardlyanyattention. Inthis
work,weconsiderfourpossiblewaysofdefiningthepreferencedistribution,eitherprovidingthe
agents with hard or soft goals and either involving or not goal shaping (i.e., intermediate goals).
We compare the performances of four agents, each given one of the possible preference distri-
butions, in a grid world navigation task. Our results show that goal shaping enables the best
performance overall (i.e., it promotes exploitation) while sacrificing learning about the environ-
ment’stransitiondynamics(i.e.,ithampersexploration).
Keywords: active inference, Bayesian inference, POMDP, variational free energy, expected free
energy,priorpreferences
1. Introduction
Activeinferencehasbecomeaninfluentialcomputationalframeworkusedtoaccountforseveralas-
pectsofcognitionandadaptivebehaviourincognitivescienceandcomputationalneuroscience[13,
15, 21, 36, 37]. The fundamental idea of active inference is that adaptive agents are continuously
engaged in a process of predicting upcoming sensory observations and inferring the best course of
action to minimize prediction error. This kind of perception-action loop is described at different
spatio-temporal levels as a form of variational Bayesian inference, on the hidden states of the en-
vironment, that relies on a (hierarchical) generative model to minimize (variational) free energy, a
proxy for prediction error [29, 14, 21]. In this Bayesian framework, the minimisation of free energy
and expected free energy enables an agent to infer its current state (perception), to infer the best se-
quenceofactions(policies)toreachpreferredstatesorobservation(planning/goal-directeddecision
making), and to progressively learn the transition dynamics and state-observation mappings in the
environment[7,8,38,6,27,3].
In contrast to reinforcement learning [47], the active inference framework tries to dispense with
the notion of reward, so it assumes that agents are endowed with goals in the form of prior pref-
erences to be achieved by performing free energy minimisation. In various works deploying active
†Correspondencee-mail:manuel baltieri@araya.org
5202
ceD
2
]IA.sc[
1v39230.2152:viXra
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
inference agents in more traditional reinforcement learning environments, it is common to use a
preference distribution over observations implicitly defined by regarding the reward signal in the
environmentasthedesiredobservationfortheagent[52,51,12,42]. However,despitethetechnical
andtheoreticalfundamentalsofactiveinferencehavebeenreviewedextensivelyintheliterature[4,
9,23,24,28,31,35],theissueofhowtospecifythepreferencedistributionanditsimpactoninference
andlearninginanactiveinferenceagenthavebeenlargelyoverlooked.
Inthiswork,ourgoalistoofferathoroughanalysisofhowdifferentspecificationsoftheprefer-
ence distribution over states affect perception, decision-making, and learning in an active inference
agentthathastosolveanavigationtaskinasimplegridworld. Inparticular,weconsiderpreference
distributions that vary along two dimensions, i.e., providing the agent (1) with soft goals vs. hard
goalsand(2)withgoalshapingornot. Theformerdetermineshowstronglytheagentwantstoreach
a certain goal, whereas the latter specifies whether to give an agent a series of intermediate goals to
reachtheultimateone.
In Section 2 we briefly review the fundamental aspects of the active inference framework, with
a focus on expected free energy and on how preference distributions play a key role in it. With
one experiment, we analyze inference and learning in four active inference agents, characterised by
one of the possible preference distributions (based on the considered dimensions of variations), in
a simple grid-world environment (Section 3). We will conclude with a discussion of how agent’s
performance and learning is affected by each of the considered preference distributions as well as
withafewmoregeneralconsiderationsaboutthenotionsofrewardandgoal-directednessinactive
inference(Section4).
2. Active Inference in discrete state spaces
Inthediscretestate-spaceformulationofactiveinference,anagent’sadaptivebehaviourismodelled
asaprocessofvariationalBayesianinferencegivenagenerativemodeloftheenvironment. Ateach
timestep,theagentreliesonitsupdatedBayesianbeliefstoexecuteanactionfromapolicy,π ∈ Π (a
sequenceofactionsfromasetofallowedsequences), soastoaccessoneormorepreferredstatesor
observations(seeSectionS1.1forasummaryofthenotationusedhereafter). Byactingthisway,the
generativemodelwillpartiallyreflectovertime(throughlearning)theemissionandtransitionmaps
that jointly characterise the environment’s generative process, i.e., how observations are generated
from states and how actions affect the transition from one state to another, respectively. In the next
few sections, we provide a brief overview of the main components and steps that characterize this
activeinferenceframework. Foramoredetailedoverviewoftheframework,seee.g.[9,48].
2.1. TheGenerativeprocessandthegenerativemodel
Both the generative process and model are specified as discrete-time partially observable Markov
decisionprocesses(POMDPs). Formally,wecandefinethegenerativeprocessasfollows:
Definition2.1(POMDPinactiveinference,thegenerativeprocess). APOMDPisasix-elementtuple,
(S,O,A,T ,E,T),where:
• S isafinitesetofstates,
• O isafinitesetofobservations,
• Aisafinitesetofadmissibleactions,
2
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
• S,O,A ,withi ∈ [1,T],aretime-indexedrandomvariablesdefinedovertherespectivespaces,
i i i
wherethetimeindex T representsaterminaltimestep,
• T : S ×A → ∆(S)isatransitionfunctionthatmapsstate-actionpairstoaprobabilitydistribu-
tionintheset
∆(S)ofprobabilitydistributiondefinedoverS
• E : S → ∆(O) is an emission function that maps a state to a probability distribution in the set
∆(O)ofprobabilitydistributiondefinedoverO 1.
The generative model is specified as a joint probability distribution that can be generated by a
POMDPinthesenseofDefinition2.1. Wethendefinethegenerativemodelasfollows:
Definition2.2(Generativemodelinactiveinference). ThegenerativemodelMofanactiveinference
agentisajointprobabilitydistributionoverasequenceofstateandobservationrandomvariables,a
policyrandomvariableforsequencesofactions,andparametersstoredinmatrixA(fortheemission
map)andtensorB(forthetransitionmap),thatis,ajointthatfactorsas:
T T
∏ ∏
P(O
1:T
,S
1:T
,π,A,B) = P(π)P(A)P(B)P(S
1
) P(S
t
|S
t−1
,π,B) P(O
t
|S
t−1
,A). (1)
t=2 t=1
ThematrixA ∈ Rn×mstoresthecategoricalprobabilitydistributionP(O |sj;o )asthejthcolumn,
t j
specifyingtheprobabilitiesoftheobservationsproducedbystateS = sj,forallstatevaluess1,...,sm
t
(thoseprobabilitiesarestoredbytheparametervectoro thatcoincideswiththejthcolumnofA). The
j
tensor B ∈ R|A|×m×m stores the action-dependent categorical distribution P(S |s j ,x;s ) as the jth
t t−1 j
column of Bx, where x indicates the action under consideration, specifying the probabilities of the
nextstatevaluesgiventhepreviousstate S t−1 = sj,forallstatevalues s1,...,sm andforeachaction
(again,thoseprobabilitiesarestoredbytheparametervector s thatcoincideswiththe jthcolumnof
j
Bx). Each column of A and Bx can be seen as an output of an approximation (learned by the active
inferenceagent)oftheemissionmapE andthetransitionmapT ,sothegenerativemodelhasindeed
thesamestructureofaPOMDPasdefinedinDefinition2.1.
The goal of an active inference agent is then, in an intuitive sense, to enforce a synchronization
between the generative model it parameterises and the generative process of the environment it in-
teractswith,see[2]foramoreindepthdiscussionofthisreadingofactiveinference. Moreindetail,
wecanimaginethatwhenanagentstartsinteractingwithanenvironment,beforeitsgoalisachieved,
the generativemodel has yet to capturethe emission and transitionmaps of the generativeprocess,
i.e., the ground-truth POMDP representing the environment. However, the agent can acquire such
knowledge through experience. At each time step, the agent performs inference on the most likely
statescorrespondingtoanobservation,whichalsomeansrevisingitsprobabilisticbeliefsaboutpast
and future consequences of performing different sequences of actions, then plans and decides what
action to perform next. At regular intervals, information about an experienced trajectory in the en-
vironment (i.e., a collection of observations plus probabilistic beliefs about the most likely states) is
used to update the generative model’s emission and transition map. Next, we will provide a few
moretechnicaldetailsonthisprocedure.
1WenotethatstandarddefinitionsofPOMDPs[40, Ch.16, 34, Ch.34, 47, Ch.17]includealsoanotionofrewardfor
anagent,herewedon’thoweverincludethemsinceactiveinferencenormallyspecifiestargetsforanagentbymeansof
a prior probability distribution over goal states or observations (see Section 2.4). Formally, however, this can be easily
accommodatedintheabovedefinitionbystatingthatourobservationsOincludebothobservationsY andrewardsRof
standardPOMDPdefinitions:O=Y×R.Activeinferenceworksinvolvinghigh-dimensionalstatespaceshaveadopted
thisapproachinpractice(see,e.g.,[51,52,12]).
3
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
2.2. VariationalBayesianinferenceforPOMDPs
Anactiveinferenceagentlearnstoperformactionsthatwillleadtoitsdesiredobservationsand/or
states in the environment. Observations received from the environment are evidence or feedback
that can indicate to the agent whether the generative model captures the environmental dynamics
well enough to yield accurate predictions and goal-conducive actions. Such observations are used
to infer the (1) most likely hidden states generating an observation at each time step, the (2) most
likely policy given some preferred states or observations, and the (3) most likely parameters of the
generativemodeltomakemoreaccuratepredictionsintheenvironment.
Given the agent’s generative model (see Definition 2.2), this process of inference can be imple-
mentedbyBayes’rule,whichinthissettingcorrespondstothefollowing:
P(O |S ,π,A,B)P(S ,π,A,B)
P(S ,π,A,B|O ) = 1:T 1:T 1:T , (2)
1:T 1:T P(O )
1:T
where the generative model M appears in the numerator, factorised as the product between the
likelihoodandthepriorprobabilitydistributions. Thegoalhereistofindaposteriorjointdistribution
(the left-hand side) over the state, policy, and transition and emission maps’ parameters random
variables. However, finding an analytic solution to Eq. (2) is often intractable, so active inference
proposes to implement an approximate Bayesian inference scheme based on the minimisation of
variational free energy. This quantity is defined in relation to the available generative model, so it
canbewrittenasfollows:
(cid:104) (cid:105)
F (cid:2) Q(S ,π,A,B) (cid:3) := E logQ(S ,π,A,B)−logP(O ,S ,π,A,B) , (3)
1:T Q 1:T 1:T 1:T
where Q(S ,π,A,B) is known as the variational posterior, a probability distribution introduced to
1:T
approximate the posterior distribution, P(S ,π,A,B|O ), in Eq. (2) (the outcome of Bayesian in-
1:T 1:T
ference). TominimizethefreeenergydefinedinEq.(3),wemakesomeassumptionsaboutthevari-
ational posterior so that the optimisation procedure described above becomes more tractable. In
discrete-time active inference, it is thus common to adopt a mean-field approximation [9], meaning
thatthevariationalposteriorisfactorisedasfollows:
T
∏
Q(S ,π,A,B) = Q(A)Q(B)Q(π) Q(S |π). (4)
1:T t
t=1
By substituting this expression in Eq. (3) for the variational posterior, and by considering the
factorizationofthegenerativemodel,wecanrewritethefreeenergyasfollows(cf.,[9]):
(cid:16) (cid:13) (cid:17) (cid:16) (cid:13) (cid:17) (cid:16) (cid:13) (cid:17)
F (cid:2) Q(S ,π,A,B) (cid:3) =D Q(A) (cid:13) P(A) +D Q(B) (cid:13) P(B) +D Q(π) (cid:13) P(π)
1:T KL (cid:13) KL (cid:13) KL (cid:13)
(cid:34)
T (cid:104) (cid:105) τ (cid:104) (cid:105)
+E ∑E logQ(S |π ) − ∑E logP(o |S ,A)
Q(πk ) Q(St |πk ) t k Q(St |πk )Q(A) t t (5)
t=1 t=1
(cid:35)
(cid:104) (cid:105) T (cid:104) (cid:105)
−E Q(S
1
|πk ) logP(S 1 ) − ∑E Q(St |πk )Q(St−1 |πk ) logP(S t |S t−1 ,π k ) ,
t=2
wherewehavesingledouttheKLdivergencesbetweentheposteriorprobabilitydistributionsfrom
the variational approximation and the prior probability distributions from the generative model
4
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
(first three terms), and grouped together all the terms involving one of the variational posteriors
Q(S |π ), k ∈ [1,p] where p is an integer indicating the maximum number of policies, inside the
1:T k
expectation E [...](lastterm),whichcomputesanaveragewithrespecttoallpolicies.
Q(πk )
When the expression in Eq. (5) is optimised with respect to the policy-conditioned variational
distributions, Q(S |π ), ∀k ∈ [1,p], we can simply focus on the argument of E [...] to compute
t k Q(πk )
the associated gradient (since that is the only term that contributes to the gradient and ignoring the
expectation does not change the solution of ∇ F[Q(S |π )] = 0). That argument defines a policy-
st t k
conditionedfreeenergy:
T (cid:104) (cid:105) τ (cid:104) (cid:105)
F
(cid:2)
Q(S |π )
(cid:3)
:=
∑E
logQ(S |π ) −
∑E
logP(o |S ,A) −
πk 1:T k Q(St |πk ) t k Q(St |πk )Q(A) t t
t=1 (cid:124) (cid:123)(cid:122) (cid:125) t=1 (cid:124) (cid:123)(cid:122) (cid:125)
statelog-probabilities observationlog-likelihoods
(6)
(cid:104) (cid:105) T (cid:104) (cid:105)
−E Q(S
1
|πk ) logP(S 1 ) − ∑E Q(St |πk )Q(St−1 |πk ) logP(S t |S t−1 ,π k ) .
(cid:124) (cid:123)(cid:122) (cid:125) t=2 (cid:124) (cid:123)(cid:122) (cid:125)
statelog-probabilities transitionlog-likelihoods
TheupdaterulesforQ(S |π ),∀k ∈ [1,p],derivedbytakingthecorrespondinggradientoftheex-
t k
pressioninEq.(6), defineanoptimisation/inferenceschemecalledvariationalmessagepassingwhich
makesuseofpast,presentandfutureinformationtoupdate,inthiscase,variationalprobabilitydis-
tributionsatdifferenttimepointsalongatrajectory(seeappendicesin[9,48]fordetailsontheupdate
equations). Following standard treatments in the literature of stochastic processes and (Bayesian)
estimation,itisanexampleofsmoothing,tobecontrastedwithinference(whichusespresentinfor-
mationonly)andfiltering(whichreliesonpastandpresentinformation),andprediction(whichuses
thepastonly)[25,44].
2.3. Expectedfreeenergy
Similarly,thederivationoftheupdatedprobabilitydistributionoverpolicies, Q(π),involvestaking
thegradientofthefreeenergyinEq.(5)butwithrespecttotheparametersof Q(π)thistime. While
we again refer to [9, 48] for the full details of the derivation, we briefly overview below the update
equationsotointroducethenotionofexpectedfreeenergynext.
⊺ ⊺
Byindicatingwithπ andπ therowvectorsofparametersofthevariationaldistributionQ(π)
Q P
⊺
andthepriordistribution P(π),respectively,andwithF therowvectorofpolicy-conditionedfree
π
energies(oneforeachpolicy,i.e.,foreachvaluethepolicyrandomvariablecantake,seeEq.(6)),the
updatedparametersforQ(π)arecomputedasfollows:
⊺ ⊺ ⊺
π = σ(lnπ −F ), (7)
Q P π
⊺
where the softmax function, σ(·), is used to obtain a vector π of normalised probabilities from
Q
⊺
the unnormalised probabilities of the vector lnπ , after setting the gradient ∇ F[Q(π)] to zero and
Q π
rearranging(seeagainappendicesin[9,48]).
⊺
Oneofthekeymovesofactiveinferenceistospecifythepriorparametersπ intermsofthevec-
⊺ P
tor G oftotalexpectedfreeenergies G (definedbelow),oneforeachpolicyunderconsideration,
H H
5
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
⊺ ⊺
i.e.,π := σ(−G ),inordertoarriveatthefolowingupdaterule2:
P H
⊺ ⊺ ⊺
π = σ(−G −F ), (8)
Q H π
whereeachcomponentof G isthetotalexpectedfreeenergy G (π) foracertainpolicy π. Specifi-
H H
cally,foragivenpolicyπ ,G (π )isdefinedasthesumofexpectedfreeenergiesatfuturetimesteps
k H k
uptothepolicyhorizon, H,i.e.:
H
∑
G (π ) = G (π ), (9)
H k t k
t=τ+1
wheretheexpectedfreeenergyattimetforthesamepolicyisspecifiedasfollows:
(cid:104) (cid:105) (cid:104) (cid:105)
G (π ) :=E H(cid:2) P(O |S ) (cid:3) −E D (cid:2) Q(A|o ,s )|Q(A) (cid:3)
t k Q(St |πk ) t t P(Ot |St )Q(St |πk ) KL t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
AMBIGUITY A-NOVELTY
(10)
(cid:104) (cid:105)
+D (cid:2) Q(S |π )|P ∗(S ) (cid:3) −E D (cid:2) Q(B|o ,s )|Q(B) (cid:3) .
KL t k t P(Ot |St )Q(St |πk ) KL t t
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125)
RISK
B-NOVELTY
In the above expression, the risk term quantifies the divergence between the variational state
distribution and a target distribution (here defined over state random variables, but see below), the
ambiguitytermsquantifiestheuncertaintyrelatedtotheobservationmap,andthetwonoveltyterms
are expected information gains for the parameters of the observation and the transition maps, thus
indicatingpartsofthegenerativemodelthatarestillinaccurate.
Therefore, according to Eq. (8), policy probabilities are updated at teach time step based on a
combination of negative expected free energies and free energies associated with each policy. Con-
cretely,thismeansthatapolicywillbecomemoreprobabletotheextentthatitminimisesfreeenergy
and expected free energy. A policy that minimises free energy represents a sequence of action that
is most likely associated with the observations collected so far, i.e., there is a good chance that the
policy’s actions produced a sequence of states emitting those observations. A policy that minimises
expected free energy tries to makes sure that the agent reaches its goals (i.e., by having a low risk,
equivalently: a high instrumental or extrinsic value) and that the agent explores sufficiently enough
the environment to acquire relevant information about emission and transition maps (i.e., by being
associated with low ambiguity and high novelty terms, equivalently: a high epistemic or intrinsic
value).
An informative and correct probability distribution over policies Q(π) is then a crucial compo-
nentofthedecision-makingstepofanactiveinferenceagent,involvingtheselectionofwhatactionto
performnext. Therearedifferentwaystospecifyanaction-selectionprocedureordecisionrule,e.g.,
anagentcouldpicktheactionthemostprobablepolicysuggestsattimet. Anotheroptionwouldbe
topicktheactionassociatedwiththehighestsumofprobabilitymasscomingfromeachpolicythat
suggeststhatactionatt. Thelatter,whichisusedinthefollowingexperiments,isknownasBayesian
modelaverage[9]andcanbeformallystatedasfollows:
(cid:32) (cid:33)
∑
a t = argmax δ a,c πk Q(π k ) , (11)
a∈A πk ∈Π t
⊺
2Notethatlnσ(−G H ),obtainedbysubstitutingthedefinitionforπ P inEq.(7),simplifiesto−G H becausethenatural
logarithmistheinverseofraisingtothepowerofeandbecausewecanignorethenormalizingconstant,whichwouldbe
⊺
anywayabsorbedintothesecondsoftmaxusedtoobtainπ .
Q
6
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
where the Kronecker delta δ a,c πk compares a potential action a ∈ A and the action c π t k that a policy
t
π dictatesattimestept,giving1iftheyareequaland0otherwise. Thisdecisionrulethuspicksthe
k
actionwiththehighestmarginalprobability, P(a) = ∑
πk
∈ΠP(a,π
k
),attimestept.
Acrucialpieceofinformationincludedintheexpressionfortheexpectedfreeenergyistheprior
probability distribution P∗(·), which can defined over observation random variables O , in the par-
t
tially observable case, or over state random variables S , in the fully observable case (e.g., when
t
O = S ). Henceforth, we will focus on the latter, because we are interested in agents that need to
t t
learn an environment’s transition model as opposed to the observation map (i.e., the B matrices in-
steadoftheAmatrix),andrefertoP∗(·)asthepreferencedistribution3. Inthenextsection,wewill
delveintosomedetailsabouttheformofthisdistributionandonwhatitaccomplishes.
2.4. Preferencedistributionsinactiveinferenceagents
In the discrete state-space setting, the agent’s preferences are represented by a categorical distribu-
tion,indicatedbyP∗(S)inthefullyobservablecase,thateffectivelyencodesanagent’sgoalsinterms
ofparticularinstantiationsoftherandomvariable S. Thatis,goal-directednessinactiveinferenceis
cast as the concentration of probability mass on some states more than others, for the possible m
realisations,s1,...,sm,ofS,asindicatedbytheparameters’vectorof P∗(S).
Thespecifiedpreferencedistributionconstrainspolicyandactionselectionbecausethecomputa-
tion of expected free energy depends on it. Specifically, the risk component of expected free energy
at time t is the KL divergence between the variational distribution Q(S |π ) and the preference dis-
t k
tribution P∗(S )(seeEq.(10)). ThisKLdivergenceencapsulatesthecontrastbetweenhowthingsare
t
and how they should be for the agent: the variational distributions at different time steps indicate
tothe agentwhatthe mostprobablestatesafforded byapolicy are(theyencode theagent’scurrent
beliefsabouttheconsequenceofapolicy,determinedbythelearnedgenerativemodel)whereasthe
fixedpreferencedistributionindicateswhatthemostprobablestateshouldbe(regardlessofanypol-
icy). Selecting a policy that minimises risk means to increase the chance that executing its actions
willresultinvariationalbeliefsthatalignwiththepreferencedistribution(whattheagentwants). To
theextentthatthevariationalbeliefsreflecttheenvironment’sdynamics,i.e.,wheretheagentsmost
probably will be located (and not just where it believes it will), the minimisation of risk will lead
the agent to achieve its goal(s). In other words, risk quantifies the instrumental value of the policy
beingevaluated,i.e.,whethertheagentsbelievesitwillleadtothepreferredstates. Ifriskislowfor
acertainpolicy,thenthispolicywillhaveachanceofbeingselectedandbringtheagenttooneofits
preferredstates.
Thepreferencedistributionspecifiesacertaingoal(e.g.,aparticularstate),potentiallyconnected
with the solution of a task, and establishes how strongly an agent wants to achieve it. This distri-
butionneedstobe specified inadvancesinceingeneralactiveinference agentsarenotabletolearn
what they should do (but see [42, 46]). This can be done in different ways, depending on the appli-
cation and on what the agent is supposed to do. There are at least two degrees of freedom in the
specification of the agent’s preferences: (1) how strongly an agent prefers to achieve a certain goal,
and(2)whetherthepreferencedistributionspecifiesdifferentgoalsatdifferenttimesteps(potentially
indicatingtotheagentatrajectorythroughstate-space).
3A similar argument can be made for partially observable scenarios, i.e., when agents have to learn the observation
model A, withtheriskinEq.(10)writtenintermsofvariationalandpreferencedistributionsoverobservationrandom
variables. In particular, the variational distribution over observations is obtained from the agent’s current observation
modelbymarginalizingoverstatesrandomvariable,i.e.,Q(Ot |π
k
)=∑
st
P(Ot |St )Q(St |π
k
)(seeappendixin[9]).
7
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
Table1: Typesofpreferencesconsideredinthiswork.
Preferencestrength
ecnedneped-emiT
Softpreferenceswith Hardpreferenceswith
goalshaping goalshaping
Softpreferences Hardpreferences
withoutgoalshaping withoutgoalshaping
The first dimension relates to how much the probability mass of P∗(S ) is concentrated on some
t
states as opposed to others. In particular, the agent could have hard goals or preferences, meaning
that most of the probability mass is concentrated on a single state, S = sj, while for all states the
t
probabilitiesarecloseto0,thusmaking P∗(S ) anapproximatedeltadistribution. Alternatively,the
t
agent could have soft goals or preferences, in the sense that the probability mass is more evenly
distributedamongatleasttwoormorestates.
Theseconddimensioncapturesthepossibletime-dependenceofpreferences: whethertheagent
has a different preference distribution for each time step, providing the agent with a distinct goal
for each time step in a given time interval. To understand this, recall that the expected free energy
definedbyEq.(10)iscomputedforeachfutureactionthatapolicysuggeststhereforeeachP∗(S )for
t
t ∈ [1,T]couldassignprobabilitymasstostatesdifferently. Again,thereareatleasttwopossibilities.
With preference or goal shaping, for every future time step t we specify a different preference dis-
tribution capturing what goal(s) the agent ought to strive towards at each of those time steps. This
meansthatwehaveeffectivelyacollectionofpreferencedistributionsdefiningadesiredtrajectoryin
state-space for the agent to follow, which can be viewed as a sequence of sub-goals or intermediate
goalstheagentneedstoobtainbeforereachingtheirmaingoal(e.g.,atthelasttimestep).
Incontrast,withoutpreferenceorgoalshaping,thereisonlyasingle,fixedpreferencedistribution
that is used to compute the risk term at each time step. This distribution could specify hard or soft
goalsandrepresentsthe“final”distributionoverstatestowhichtheagenttends. Inotherwords,the
agentisgivenoneormoregoalstoobtainassoonaspossible,butnoindicationofwhatintermediate
goalstopursuetogetthere.
In the active inference literature, the specifications of hard goals and soft goals have often been
used in grid world simulations (see, e.g., [17, 18, 22, 43] for hard goals, and [19] for soft goals).
The presence or absence of goal shaping implicitly appears in active inference works that involve
common reinforcement learning environments, where the agent’s preferences are encoded as states
orobservationsthatyieldhighrewards,anddependsonwhethertheenvironment’srewardfunction
wasdesignedtoprovideadensevs.sparserewardsignal[49,52,51,5,32].
In the experiment described in the next section, we consider active inference agents in a grid-
world navigation task, each with one of the four possible preference distributions (determined by
the dimensions described above), and examine how the specification of preference distribution af-
fectstheirabilitytoreachtheirfinalgoal,i.e.,beinginaparticularmaze’stile,thatcorrespondswith
8
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
thefinalsolutionofthetask. Inparticular,agentswithsoftgoalsandgoalshapinghavedifferentpref-
erencedistributionsP∗(S ),...,P∗(S ),withtheprobabilitymassdistributedamongstates,basedon
t T
theManhattandistanceofeachstatefromastaterepresentinganintermediategoal(tileposition),at
t, or from the state corresponding to the final goal, at T. Similarly, agents with hard goals and goal
shaping have different preference distributions, but this time each one approximates a Dirac delta.
Without goal shaping, agents with soft goals have a single preference distribution, P∗(S), used to
computetheriskintheexpectedfreeenergyateachtimestepintheepisode,againwithamoreeven
distributionofprobabilitymass(asexplainedabove). Similarly,agentswithhardgoalshaveasingle
preferencedistributionthatassignsmostoftheprobabilitymasstothefinalgoal.
3. Results
We trained action-unaware active inference agents, i.e., agents that have no access to previously
executed actions (see [48] for details), with different preferences in a 3×3 grid world. All agents
start in the top left corner and their goal is to reach the bottom right corner, with no obstacles. We
specifiedpreferencedistributionsinfourdifferentways,basedonTable1.
The problem is simplified to be a fully observable MDP (with A diagonal and known to the
agent),withdeterministicbutunknownstatetransitionsB. Wetrained10agentsofeachkindfor200
episodes of 5 steps each, i.e., T = 5, with a policy horizon H = 4, giving us at most 256 policies to
evaluate, to allowforconvergence. Of the256policies, 6are task-solving, sincethey allowanagent
to reach the bottom right corner of the grid via a combination of two ↓ and two → actions, in any
order. Theremaining250aretask-failingsincetheydon’tleadtothefinalgoal.
We start by comparing the percentage of agent solving the task across episodes in Fig. 1. Goal-
shaped agents learn more quickly and perform better overall than agents without goal shaping,
which is to be expected since the former know exactly what intermediate goal to aim for at each
timestep,guidingthemtoreachtheirfinalgoal(bottomrightcorner).
Withgoal-shaping,havinghardgoalsisalsoafurtheradvantageasallagentsareabletofindthe
path to the final goal in a handful of episodes and they stick to it for the rest of the experiment, i.e.,
a 100% success rate is consistently achieved from episode 8 onwards (see top-right plot in Fig. 1).
In contrast, agents with soft goals take longer to find their way to the final goal and the percentage
of successful agents falls below 100% in several episodes throughout the experiment, even after the
main learning phase is over, i.e., from episode 17 onwards (compare top-left and top-right plots
inFig.1).
Withoutgoal-shaping,bothkindsofagenttakelongertolearnandsucceed: afteraninitialphase
of performance improvements, there is a dramatic drop in the percentage of successful agents at
around episode 50, followed by a quick recovery and with at least 60% of the agents being able to
reach the final goal in most episodes until the end of the experiment (see bottom-left and bottom-
rightplotsinFig.1). Whiletheperformancedropataroundepisode50islesspronouncedforagents
with hard goals, overall they appear to reach the final goal fewer times than agents with soft goals.
The percentages of time the final goal was reached during the experiment, 15% in soft-goal agents
and 14.2% in hard-goal agents, confirm the above observation: without goal-shaping, agents with
softgoalaremarginallymoresuccessfulthanagentswithhardgoals(compareheatmapsinFig.S1).
These results indicate that agents with different kinds of preferences can successfully learn rele-
vantaspects ofthetransition dynamics andsolvethe task. Agentsthatare givenmoreinformation,
by means of intermediate goals (goal shaping) leading to the final goal have an advantage and can
9
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
100%
80%
60%
40%
20%
0%
1 20 40 60 80 100 120 140 160 180 200
Episode
stnega
fo
egatnecreP
Agents solving the task
(soft with goal shaping)
100%
80%
60%
40%
20%
0%
1 20 40 60 80 100 120 140 160 180 200
Episode
stnega
fo
egatnecreP
Agents solving the task
(hard with goal shaping)
100%
80%
60%
40%
20%
0%
1 20 40 60 80 100 120 140 160 180 200
Episode
stnega
fo
egatnecreP
Agents solving the task
(soft without goal shaping)
100%
80%
60%
40%
20%
0%
1 20 40 60 80 100 120 140 160 180 200
Episode
stnega
fo
egatnecreP
Agents solving the task
(hard without goal shaping)
Fig.1: Percentageofagentsreachingthegoalstateineachepisodeinthe5-stepgridworld(10agentsforeach
subplot).
completethetaskahighernumberoftimes. Atthesametime, softgoalsenablebetterperformance
whengoalshapingisnotavailablebuthardgoalsprovidesafurtheradvantagewhencombinedwith
goalshaping.
Foramorein-depthunderstandingofhowthesedifferencesinperformancearebroughtaboutby
thechoiceofpreferencedistribution,weanalyseperceptualinferenceandplanning/decision-making
inthesimulatedagentsbyconsideringrelatedmetrics, chiefly, policy-conditionedfreeenergiesand
expectedfreeenergies,respectively.
Perceptual inference over trajectories The successful performance of each type of agent hinges
upon an appropriate inference of its current state from the received observations, corresponding to
the perceptual stage in active inference. This is achieved via free energy minimisation. In episodic
setups, this minimisation happens at different time steps. Here we look at how free energy is min-
imised, across episodes, at the last step of each episode, as done in [48]. At the last step, an agent
has to evaluate the entire sequence of observations associated to a full episode so to infer the most
probable sequence of states that have been visited, conditional on having executed a certain policy.
This thus reveals how agents infer their past and current states. Policies that correspond to state
10
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
trajectories that agree with the received observations will be associated with low values of (policy-
conditioned)freeenergy.
16
14
12
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180 200
Episode
ygrene
eerF
Policy-conditioned free energy at step 5
(soft with goal shaping)
16
14
12
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180 200
Episode
ygrene
eerF
Policy-conditioned free energy at step 5
(hard with goal shaping)
16
14
12
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180 200
Episode
ygrene
eerF
Policy-conditioned free energy at step 5
(soft without goal shaping)
16
14
12
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180 200
Episode
ygrene
eerF
Policy-conditioned free energy at step 5
(hard without goal shaping)
Policies
1: , , , 5: , , , 9: , , , 13: , , ,
2: , , , 6: , , , 10: , , , 14: , , ,
3: , , , 7: , , , 11: , , , 15: , , ,
4: , , , 8: , , , 12: , , , 16: , , ,
Fig.2: Policy-conditionedfreeenergiesatstep5acrossepisodes(showingaverageof10agents).
Plotsofthepolicy-conditionedfreeenergiesatstep5,inFig.2,revealwhichpolicybestaccounts
for the observations collected in each episode. For all the figures involving policy-conditioned free
energies, we selected 16 policies, among the 256, including the 6 task-solving policies that lead to
thefinalgoal. Theseplotsshowthatthepresencevs.theabsenceofgoalshapingaffectswhatpolicy
yields the lowest policy-conditioned free energy, i.e., what policy the agent most likely executed
becauseitisinferredtobemoreconsistentwiththeobservations,therebyminimizingtheassociated
policy-conditionedfreeenergy.
In general, we can see that goal-shaped agents learned to pick π , one of the task-solving ones,
4
as the preferred policy and stick with it throughout the experiment (see top-left and top-right plots
inFig.2). Thisyieldsaconsistentminimisationofthecorrespondingpolicy-conditionedfreeenergy
11
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
acrossepisodes: agentsattheendofeachepisodecorrectlyinferthatπ wasthemostlikelyexecuted
4
policy. In contrast, for agents without goal shaping there is no policy that consistently minimises
the policy-conditioned free energy, see bottom-left and bottom-right plots in Fig. 2. This is due to
the fact that these agents are not constrained by their preference distribution to follow a particular
trajectory in state-space but only to reach the final goal as soon as possible. Therefore, they attempt
differenttask-solvingortask-failingpolicies,andnocleardemarcationbetweentheseisvisibleinthe
plots. Thisisbecauseindifferentepisodestheobservationsequenceiscorrectlyinferredtobemore
consistentwithdifferent,task-solvingortask-failing,policies.
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ygrene
eerF
Policy-conditioned free energy at step 1
(soft with goal shaping)
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ygrene
eerF
Policy-conditioned free energy at step 1
(hard with goal shaping)
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ygrene
eerF
Policy-conditioned free energy at step 1
(soft without goal shaping)
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ygrene
eerF
Policy-conditioned free energy at step 1
(hard without goal shaping)
Policies
1: , , , 5: , , , 9: , , , 13: , , ,
2: , , , 6: , , , 10: , , , 14: , , ,
3: , , , 7: , , , 11: , , , 15: , , ,
4: , , , 8: , , , 12: , , , 16: , , ,
Fig.3: Policy-conditionedfreeenergiesatstep1acrossepisodes(showingaverageof10agents).
Policy probabilities To examine how the choice of preference distribution affects planning and
decision-makingineachepisode,wecomparetherespectivecontributionsofthepolicy-conditioned
freeenergies andexpectedfreeenergies tothepolicy probabilitiesatthebeginning ofeachepisode,
12
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
i.e.,atstep1. Thisstepwaschosenbecauseitisthemostindicativeofhowwelltheagenthaslearned
thetransitionmodel,sinceatthissteptheagentperformsperceptualinferencewithrespecttofuture
state,beforeanyobservationisreceived(withtheexceptionofthatfromthestartingstate),andplans
for the entire episode, see also [48]. In general, we found that policy-conditioned free energies and
expectedfreeenergiesdonotalwaysagree,i.e.,byjointlyincreasingtheprobabilityofapolicyifitis
onethatleadstothegoalstate,revealingamismatchbetweenperceptionandplanningthatdepends
onthechoiceofpreferencedistribution.
18
16
14
12
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180 200
Episode
ygrene
eerf
detcepxE
Expected free energy at step 1
(soft with goal shaping)
18
16
14
12
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180 200
Episode
ygrene
eerf
detcepxE
Expected free energy at step 1
(hard with goal shaping)
18
16
14
12
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180 200
Episode
ygrene
eerf
detcepxE
Expected free energy at step 1
(soft without goal shaping)
18
16
14
12
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180 200
Episode
ygrene
eerf
detcepxE
Expected free energy at step 1
(hard without goal shaping)
Policies
1: , , , 5: , , , 9: , , , 13: , , ,
2: , , , 6: , , , 10: , , , 14: , , ,
3: , , , 7: , , , 11: , , , 15: , , ,
4: , , , 8: , , , 12: , , , 16: , , ,
Fig.4: Expected free energy for each policy across episodes (showing average of 10 agents). Notice that we
onlydraw16expectedfreeenergies,representativeofthepossible256.
Policy probabilities are computed as a softmax of the sum of negative policy-conditioned free
energy and negative expected free energy. Therefore, the policy probabilities at step 1 in Fig. 5 are
directly shaped by the contributions of policy-conditioned free energy and expected free energy at
step1,illustratedforeachagentbyFig.3andFig.4.
13
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
Inparticular, weobservethatinagentswithsoftgoalsandgoalshaping, thepolicy-conditioned
freeenergyassignedtothetask-solvingpolicyπ islow,seethetop-leftplotinFig.3,andthismakes
4
itmoreprobable. Ontheotherhand,theassociatedexpectedfreeenergyishigh,seethetop-leftplot
in Fig. 4, and this makes it less probable. Overall, this means that the resulting policy probability is
notsignificantlydifferentfromthatofotherpolicies,seetop-leftplotinFig.5.
0.06
0.05
0.04
0.03
0.02
0.01
0.00
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
First-step policy probability
(soft with goal shaping)
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
First-step policy probability
(hard with goal shaping)
0.06
0.05
0.04
0.03
0.02
0.01
0.00
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
First-step policy probability
(soft without goal shaping)
0.06
0.05
0.04
0.03
0.02
0.01
0.00
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
First-step policy probability
(hard without goal shaping)
Policies
1: , , , 5: , , , 9: , , , 13: , , ,
2: , , , 6: , , , 10: , , , 14: , , ,
3: , , , 7: , , , 11: , , , 15: , , ,
4: , , , 8: , , , 12: , , , 16: , , ,
Fig.5: Policies probabilities at the first step of each episode (showing average of 10 agents). Notice we only
draw16representativepoliciesoutofthepossible256.
In agents without goal shaping the opposite happens for some task-failing policies, which are
characterised by high policy-conditioned free energy values, see the bottom plots in Fig. 3. This
brings their probability down. At the same time, their expected free energy is remarkably low, see
thebottomplotsinFig.4,pushingthepolicyprobabilityup. Thismeansthatthesetask-failingpoli-
cies become more probable than some task-solving policies, see bottom plots in Fig. 5 (see also Sec-
tion S2.2.2 for a breakdown of these expected free energies showing how their risk component is
14
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
characterised by a significant decrease over the course of the experiment). Crucially, the policy-
conditioned free energies and expected free energies agree for all policies only in agents with goal
shapingandhardgoals. Inturnthismeansthatoneofthetask-solvingpolicies,π ,iscorrectlyiden-
4
tified as the most likely, i.e., that is the policy for which both quantities are the lowest (see top-right
plotinFig.5,andcomparetop-rightplotsinFig.3andFig.4).
Action selection and action probabilities Despite not inferring any of the task-solving policies
as more likely than task-failing ones, some classes of agents appear to anyway reach the final goal
state, albeit with varying degrees of success. We refer in particular to agents without goal-shaping
and agents with soft goals and goal-shaping, see Fig. 5. This seemingly unintuitive finding can
be explained by the fact that agents do not pick the action suggested by the most probable policy
at a certain time step but the one with the highest marginal probability, see Eq. (11). This action-
selectionmechanism,referredtoasBayesianmodelaveragein[9],preventsagentsfromperforming
the potentially wrong action of a task-failing policy that was mistakenly inferred as more probable
thanothers(perhapsonlymarginallymoreprobable).
Thanks to this, and as we see in Fig. 6, all agents assigned a sufficiently high probability mass
to the six task-solving policies so that actions ↓ or → became more likely to be picked. Note, as a
reminder, that all agents start from the top left corner of a 3×3 grid and need to read the goal at
thebottomrightcorner. Thismeansthatallthetask-solvingpoliciesareacombinationoftwo↓and
two → actions, in any order. In this way, most of the time, even agents that didn’t infer any of the
task-solving policies as more probable (agents without goal-shaping and agents with soft goals and
goal-shaping) ended up performing one of the correct sequences of actions to reach the goal state.
This finding highlights the importance that the action-selection mechanism can have depending on
the preference distribution given to the agent, e.g., one that may lead to a task-failing policy being
inferredasmoreprobable.
Moreindetail,Fig.6showsactionprobabilitiesatstep1,obtainedbymarginalisingprobabilities
for each action, see Eq. (11). Consistently with the fact that goal-shaped agents perform best, in
generaltheseagentsassignthemajorityofprobabilitymassatstep1toaction→,i.e.,thefirstcorrect
actionoftheirpreferredpathtothefinalgoalstate(seetop-leftandtop-rightplotsinFig.6). Inagents
withoutgoalshapingsomethingsimilaroccurs. However,sincetheseagentsdonothaveapreferred
path to the goal state, the probability mass at step 1 is almost evenly assigned to the two correct
actions → and ↓ of the available paths to the final goal state (see bottom-left and bottom-right plots
in Fig. 6). In both kinds of agents, action probabilities at other time steps also reflect this preference
foractionsthatformoneofthecorrectactionsequencestoreachthefinalgoal(seeplotsfortheother
stepsinSectionS2.2.3).
SinceBayesianmodelaveragefavourstheactionbackedbymostprobablepolicies,theseresults
are not surprising when one of the task-solving policies is significantly more probable than others,
i.e., in agents with a hard goal and goal shaping. But, in agents where this concentration of prob-
ability mass over policies does not occur, these findings indicates that the agents are able to spread
a sufficiently high degree of probability mass among the task-solving policies. Most of the time the
action selection mechanism can then build the right sequence of action to the goal state. When this
doesnotoccur,itisbecauseagentshaveoverallassignedmoreprobabilitymasstopoliciessuggest-
ingincorrectactions(forthereasonsexplainedinSection4.2),whicharethenexecutedandcausethe
performancedropsseeninFig.1. Theseaction-selectionmistakesbecomeapparentwhenexamining
the individiual action probabilities for specific agents, as shown by the plots in Section S2.2.4. For
thetwoagentswithoutgoalshaping,weobserveacyclicalincreaseintheprobabilityoftheincorrect
15
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
actions (most clearly at step 1 and 2, see Fig. S7 and Fig. S8), eventually leading to some episodes
wheretheseincorrectactionsareselectedoverthecorrectones,albeitonlybyasmallmargin.
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 1
(soft with goal shaping)
1.0
0.8
0.6
0.4
0.2
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 1
(hard with goal shaping)
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 1
(soft without goal shaping)
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 1
(hard without goal shaping)
Actions
Fig.6: Actionprobabilitiesatstep1ofeachepisode(averageof10agent).
4. Discussion
4.1. Learningandperformanceelicitedbydifferentpreferencedistributions
Simulations in a 3×3 grid world with the four preferences’ combinations showed that agents with
goalshapingandhardgoalsperformbest,attheexpenseofreducinguncertaintyabouttheground-
truth transition dynamics, which is to be expected as they are entirely guided by a collection of
preference distributions that specify a distinct trajectory through various intermediate goals (maze
16
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
positions)tothefinalgoal(bottom-righttile).
In more complex environments, e.g., when the specification of a desired trajectory is unfeasible,
agents will have to figure out by themselves the best way of solving a certain task or reaching a
certain goal. Not surprisingly, simulations of this kind of scenario using the same 3×3 grid world
showed that agents without goal shaping make more mistakes but eventually discover all the six
possibletrajectories tothe goal. This suggeststhat theymight bemore robustto perturbations, e.g.,
to one or more paths to the goal becoming obstructed, because they have spent more time learn-
ingabouttheenvironment’stransitiondynamics(seeFig.S11,andcompareground-truthtransition
mapsinSectionS2.2.6withlearnedonesinSectionS2.2.7).
Withoutgoalshaping,agentswithsoftgoalsturnedouttobemarginallymorecapableofreaching
the final goal state, which can be explained by the fact that properly specified soft preferences (e.g.,
using the Manhattan distance from the final goal state) allow the agent to explore the environment
while still having a (soft) preference for reaching a subset of states that are closer to the final goal
state.
4.2. TheRiskofunfamiliarpolicies
In describing how policy-conditioned free energies and expected free energies combine to deter-
minepolicyprobabilitiesatstep1,wenotedthatagentswithoutgoalshapingscoretask-failingpoli-
cies higher than task-solving ones at some point, especially thanks to low risk values (see also Sec-
tionS2.2.2). Onthefaceofit,thisseemspuzzlingbecausethosepoliciesaretask-failing,i.e.,theydo
notleadtheagenttoitsgoalstate,andyettheyarescoredfavourablyintermsofrisk.
However, since these policies haven’t been sufficiently explored during the experiment, active
inferenceagentshaveunresolveduncertaintyastowheretheylead,indicatedbyhigh-entropyvari-
ationaldistributions,Q(S |π ),...,Q(S |π ),foreveryk ∈ [1,p],updatedattheperceptualinference
2 k T k
stage(instep1ofeachepisode). Thesevariationalbeliefsproducelargevaluesofpolicy-conditioned
freeenergies(seeFig.3)andatthesametimedetermineadropofthepolicy’srisksalongthecorre-
spondingtrajectory(seeFig.S2).
Tounderstandwhythisdropoccurs,recallthatriskistheKLdivergencebetweentwoprobabil-
ity distributions, and that a low-entropy (peaked) distribution is “closer” to a high-entropy (flatter)
distributionthantoanotherlow-entropydistribution,i.e.,adistributionwithapeaksomewhereelse.
Thus, risk will be relatively low when computed between a low-entropy preference distribution,
whereprobabilitymassisconcentratedallononegoalorafewones,andahigh-entropyvariational
distribution,Q(S |π ),foratask-failingpolicyπ thatwasrarelyattempted(i.e.,forwhichtheagent
t k k
is not certain about the resulting state trajectory). In contrast, risk will be relatively high between
two different low-entropy distributions with different peaks, e.g., when a preference distribution
withconcentratedprobabilitymassiscomparedwiththevariationaldistributionofapolicy π that
k
wasexecutedmorefrequently(i.e.,forwhichtheagentiscertainabouttheresultingstatetrajectory).
This situation does not occur in agents with hard goals and goal shaping because in this case
we have a collection of preference distributions that traces a path through state-space and a unique
policy that realises that trajectory, making the corresponding variational beliefs match perfectly the
corresponding preference distributions. Thus, the risk (KL divergence) between matching distribu-
tion will always be lower than that between a high-entropy and low-entropy distribution. Because
of this, the risk of the task-solving policy correctly turns out to be the lowest (see top-right plot
inFig.S2).
Overall, when we consider agents without goal shaping (and, note, in an environment with de-
17
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
terministic state transitions), increasing the entropy of the preference distribution, i.e., using soft
goals,canbeseenasaneffectivemethodtolimitthis(incorrect)assignmentofinstrumentalvalueto
task-failingpolicies(seeagainFig.5).
Thephenomenonjustdescribedissomewhatmitigatedbythefactthathigh-entropyvariational
beliefs for the trajectory of a policy make the corresponding policy-conditioned free energy soar,
thereby penalising the policy when that quantity is combined with expected free energy to yield
the policy probability via the softmax (see Section 3). However, this does not prevent some task-
failingpoliciesfrombecomingmoreprobablethantask-solvingones(seeagainFig.5). Asremarked
inSection3,theseagentsdonotfailcompletelyonlybecausetheactionselectionproceduredoesnot
involvepickingtheactionfromthemostprobablepolicy.
Atthesametime,wenotethatevenifthatwerethecase,theagentswouldlearntheconsequences
ofthosetask-failingpolicies,resolvingtheiruncertaintyaboutthetransitiondynamicsofrarelytried
actions’ sequences (and for some agents this is what occurs, as indicated by the probabilities for
these task-failing policies declining from episode 160 onwards, see especially the bottom-right plot
inFig.5). Inotherwords,inadditiontothetwonoveltytermsoftheexpectedfreeenergy,thereisan
implicitepistemicdrivearisingoutoftherisktermaswell,thatcompelsagentstoexecutelesstried
actions’sequences,whenagentsaretrainedwithoutgoalshaping.
4.3. Relatedwork
One of the main distinctive features of active inference is the idea of integrating state estimation,
planning, decision-making, and learning under the same objective, i.e., the minimisation of some
kind of free energy, without the need to introduce ad-hoc exploratory bonuses (as done in some re-
inforcement learning approaches, see [30]) to deal with the exploration-exploitation dilemma (but
see [33]) and without relying on notions of reward and value functions (which are foundational in
reinforcementlearning,see[47]).
However, it does not follow that goal-directedness emerges from free-energy minimisation tout
court [45, 1]. The design of an active inference agent requires the specification of a preference dis-
tribution that creates a relevant learning signal for the agent, via expected free energy, so that some
kind of adaptive behaviour is elicited in a certain environment. The specification of this preference
distribution, that indicates what states or observations are preferred by an active inference agent,
effectivelycorrespondstothedesignofarewardfunctioninreinforcementlearning. Becauseofthis,
similartheoreticalconsiderationscanbemade,e.g.,relatedtotheoriginornatureofthispreference
distribution,cf.[26]. Indeed,thedistinctionbetweensoftvs.hardgoalscorrespondstothatbetween
densevs.sparserewardsignals,andgoalshapingcanbeunderstoodsimilarlyasrewardshaping[39,
11].
Inthisrespect,activeinferenceandreinforcementlearningappeartohavemoreincommonthan
what is often claimed [20, 16, 41]. After all, the specification of a preference distribution is a way
to let the agent knows what states or observations should be deemed to be rewarding, hence to be
preferred. This is not just a terminological issue, as in scaled-up applications of active inference it
is common to use the reward from environments used in reinforcement learning as part of the ob-
servationstheagentreceives, therebymarkingexplicitlysomeobservationsasthosetobepreferred
becauseassociatedwithhigherreward[50,51,32,53]. Furthermore,recenttheoreticalworkhasfor-
mally showed that minimising expected free energy implies reward maximisation in finite-horizon
MDPsandPOMDPs,withsophisticatedinference,arecursivekindofplanningbasedonexpectedfree
energy [10]. This work clarifies how goals can be interpreted as reward maximising states, by for-
18
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
mallydefiningthepreferencedistributionasaBoltzmanndistributionwiththerewardassignedtoa
stateastherandomvariable(see[10,p.818]forthedetails).
Ultimately, agents need a learning signal that guides them towards achieving what they are de-
signedtoachieve,andwhetherwecallitrewardorpreferences(foranagent)mightmatterlessthan
howthatlearningsignalisexploitedincleverways. Inactiveinference,thatlearningsignalarisesout
of variational free energy and expected free energy. In particular, the latter would combine instru-
mentalorextrinsicdrivestogetherwithepistemicorintrinsicdrives. Moregenerally,activeinference
invitesustotakeanagent-centricperspectiverevolvingaroundthenotionoffreeenergyminimisa-
tion: thetypeofagentweareconsidering,itsavailableactionsandhowitcaninteractwithacertain
environment,caninformusofthegoal-directedbehaviourtobeexpected,orprovidesomeguidance
on how that behaviour can be designed in a transparent way via the specification of the preference
distribution.
4.4. Limitationsandfuturework
The in-depth analysis of the impact of prior preferences in active inference agents was limited, in
this work, to the case of preferences defined over states, in agents trained in a low-dimensional
grid world. The states in question can be considered as internal to the agent, i.e., indicating its cur-
rentlocationintheenvironmentinourexperiment. However,asremarked,preferencedistributions
can also be defined over observations received from the environment and reward functions from
reinforcementlearningenvironmentscanbeincorporatedintothem,i.e.,rewardistreatedasanob-
servable part of the environment (see [31] for a nice summary of the possibilities). A study of the
trade-offs and peculiarities of these alternative approaches, often used in implementations that rely
ondeeplearningtotrainactiveinferenceagentsinhigh-dimensionalenvironments,mustbeleftfor
futurework. Inasimilarvein,thequestionofhowagent’spreferencescouldbeprogressivelylearned
indifferentenvironments,e.g.,byintroducingaDirichletpriorfortheparametersofthecategorical
preference distribution [42], was not taken into consideration here and is an interesting avenue for
futureresearch.
5. Concluding Remarks
In the present work, we investigated how different ways of fixing the preference distribution in
action-unaware active inference agents affects their performance in a toy environment. In partic-
ular,wefocussedonthedistinctionbetweentrainingagentswithorwithoutgoalshapingandwith
soft of hard preference. Goal shaping refers to the idea of using a collection of preference distribu-
tions to direct the agent towards pursuing a particular trajectory in state-space (e.g., a trajectory of
intermediategoalstoreachamaingoal);withoutit,theagentneedstofindbytrialanderrorwhich
trajectories converge onto its main goals, encoded by a single preference distribution. The second
dimension,whethertoencodesoftorhardgoalsintothepreferencedistribution,referstohowmuch
theprobabilitymassisconcentratedonasubsetofstates/observations.
In general, agents with goal-shaping perform better because they are provided with preference
distributions that together indicate a sequence of preferred states, facilitating the selection of the
single policy that will lead the agent through the corresponding path in state-space. In contrast,
agents without goal-shaping have to find that preferred path, relying only on a single preference
distributionthatindicatestheultimatestateorgoaltheyaresupposedtoachieve(inourcase: being
located in tile 9). As a result, their performance suffers but at the same time they are more free to
19
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
explore the environment, therefore they are able to find all the possible paths to the ultimate goal.
Thus, we can conclude that the absence of goal-shaping determines a more widespread learning of
the environment’s transition dynamics, i.e., of the transition probabilities for each available action
(seeagainFig.S11andlearnedtransitionmapsinSectionS2.2.7).
Acknowledgments
ThisworkwassupportedbyJST,MoonshotR&D,GrantNumberJPMJMS2012.
References
[1] Manuel Baltieri and Christopher L. Buckley. “The Dark Room Problem in Predictive Process-
ing and Active Inference, a Legacy of Cognitivism?” In: Proceedings of the 2019 Conference on
Artificial Life: How Can Artificial Life Help Solve Societal Challenges, ALIFE 2019. 2020, pp. 40–47.
DOI:10.1162/isal_a_00137.xml.
[2] Manuel Baltieri, Filippo Torresan, and Tomoya Nakai. A Coalgebraic Perspective on Predictive
Processing.2025. DOI:10.48550/arXiv.2508.16877.arXiv:2508.16877[q-bio.NC].
[3] JelleBruineberg.“ActiveInferenceandthePrimacyofthe‘ICan’”.In:PhilosophyandPredictive
Processing. Ed. by Thomas Metzinger and Wanja Wiese. Frankfurt am Main, Germany: MIND
Group,2017,pp.1–18. ISBN:978-3-95857-306-2.
[4] Christopher L. Buckley et al. “The Free Energy Principle for Action and Perception: A Math-
ematical Review”. In: Journal of Mathematical Psychology 81 (2017), pp. 55–79. ISSN: 10960880.
DOI:10.1016/j.jmp.2017.09.004.
[5] OzanC¸ataletal.“BayesianPolicySelectionUsingActiveInference”.In:(2019),pp.1–9.
[6] Ozan C¸atal et al. “Learning Generative State Space Models for Active Inference”. In: Frontiers
in Computational Neuroscience 14 (2020), p. 103. ISSN: 1662-5188. DOI: 10.3389/fncom.2020.
574372.
[7] AndyClark.“Whatevernext?PredictiveBrains,SituatedAgents,andtheFutureofCognitive
Science”. In: Behavioral and Brain Sciences 36.3 (2013), pp. 181–204. ISSN: 14691825. DOI: 10.
1017/S0140525X12000477.
[8] AndyClark.“RadicalPredictiveProcessing”.In:TheSouthernJournalofPhilosophy53.S1(2015),
pp.3–27. ISSN:00384283.DOI:10.1111/sjp.12120.
[9] LancelotDaCostaetal.“ActiveInferenceonDiscreteState-Spaces:ASynthesis”.In:Journalof
MathematicalPsychology99(Dec.2020),p.102447. ISSN:0022-2496. DOI:10.1016/j.jmp.2020.
102447.
[10] LancelotDaCostaetal.“RewardMaximizationThroughDiscreteActiveInference”.In:Neural
Computation35.5(Apr.2023),pp.807–852. ISSN:0899-7667. DOI:10.1162/neco_a_01574.
[11] JonasEschmann.“RewardFunctionDesigninReinforcementLearning”.In:ReinforcementLearn-
ing Algorithms: Analysis and Applications. Ed. by Boris Belousov et al. Cham: Springer Interna-
tionalPublishing,2021,pp.25–33. ISBN:978-3-030-41188-6. DOI:10.1007/978-3-030-41188-
6_3.
20
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
[12] Zafeirios Fountas et al. “Deep Active Inference Agents Using Monte-Carlo Methods”. In: Ad-
vancesinNeuralInformationProcessingSystems.Ed.byH.Larochelleetal.Vol.33.CurranAsso-
ciates,Inc.,2020,pp.11662–11675.
[13] KarlFriston.“ATheoryofCorticalResponses”.In:PhilosophicalTransactionsoftheRoyalSociety
B: Biological Sciences 360.1456 (2005), pp. 815–836. ISSN: 0962-8436. DOI: 10.1098/rstb.2005.
1622.
[14] Karl Friston. “Hierarchical Models in the Brain”. In: PLoS Computational Biology 4.11 (2008),
pp.1–24. ISSN:1553734X.DOI:10.1371/journal.pcbi.1000211.
[15] KarlFriston.“TheFree-EnergyPrinciple:ARoughGuidetotheBrain?”In:TrendsinCognitive
Sciences13.7(2009),pp.293–301. ISSN:13646613.DOI:10.1016/j.tics.2009.04.005.
[16] KarlFriston.“WhatIsOptimalaboutMotorControl?”In:Neuron72.3(Nov.2011),pp.488–498.
ISSN:0896-6273. DOI:10.1016/j.neuron.2011.10.018.
[17] KarlFristonetal.“ActiveInferenceandLearning”.In:NeuroscienceandBiobehavioralReviews68
(2016),pp.862–879. ISSN:18737528.DOI:10.1016/j.neubiorev.2016.06.022.
[18] Karl Friston et al. “Active Inference: A Process Theory”. In: Neural Computation 29.1 (2017),
pp.1–49. DOI:10.1162/NECO_a_00912.
[19] Karl Friston et al. “Sophisticated Inference”. In: Neural Computation 33.3 (Feb. 2021), pp. 713–
763. ISSN:0899-7667.DOI:10.1162/neco_a_01351.
[20] KarlJ.Friston,JeanDaunizeau,andStefanJ.Kiebel.“ReinforcementLearningorActiveInfer-
ence?”In:PLoSONE4.7(2009). ISSN:19326203.DOI:10.1371/journal.pone.0006421.
[21] Karl J. Friston, Thomas Parr, and Bert de Vries. “The Graphical Brain: Belief Propagation and
Active Inference”. In: Network Neuroscience 1.4 (2017), pp. 381–414. ISSN: 2472-1751. DOI: 10.
1162/NETN_a_00018.
[22] Karl J. Friston et al. “Active Inference, Curiosity and Insight”. In: Neural Computation 29.10
(2017),pp.2633–2683. DOI:10.1162/NECO_a_00999.
[23] Sebastian Gottwald and Daniel A. Braun. “The Two Kinds of Free Energy and the Bayesian
Revolution”. In: PLOS Computational Biology 16.12 (2020), pp. 1–32. DOI: 10.1371/journal.
pcbi.1008420.
[24] ConorHeinsetal.“Pymdp:APythonLibraryforActiveInferenceinDiscreteStateSpaces”.In:
JournalofOpenSourceSoftware7.73(May2022),p.4098. ISSN:2475-9066. DOI:10.21105/joss.
04098.
[25] AndrewH.Jazwinski.StochasticProcessesandFilteringTheory.NewYork,USA:AcademicPress,
1970. ISBN:978-0-12-381550-7.
[26] KenoJuechemsandChristopherSummerfield.“WhereDoesValueComeFrom?”In:Trendsin
Cognitive Sciences 23.10 (2019), pp. 836–850. ISSN: 1879307X. DOI: 10.1016/j.tics.2019.07.
012.
[27] RaphaelKaplanandKarlJ.Friston.“PlanningandNavigationasActiveInference”.In:Biolog-
icalCybernetics112.4(2018),pp.323–343. ISSN:14320770. DOI:10.1007/s00422-018-0753-2.
[28] Pablo Lanillos et al. “Active Inference in Robotics and Artificial Agents: Survey and Chal-
lenges”.In:arXiv:2112.01871v1[cs.RO](2021),pp.1–20.arXiv:2112.01871v1[cs.RO].
21
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
[29] Tai Sing Lee and David Mumford. “Hierarchical Bayesian Inference in the Visual Cortex”. In:
Journal of the Optical Society of America A 20.7 (2003), pp. 1434–1448. ISSN: 1084-7529. DOI: 10.
1364/josaa.20.001434.
[30] Sergey Levine. “Reinforcement Learning and Control as Probabilistic Inference: Tutorial and
Review”.In:arXiv:1805.00909v3[cs.LG](2018),pp.1–22.arXiv:1805.00909v3[cs.LG].
[31] PietroMazzagliaetal.“TheFreeEnergyPrincipleforPerceptionandAction:ADeepLearning
Perspective”.In:Entropy24.2(2022),pp.1–22. ISSN:1099-4300. DOI:10.3390/e24020301.
[32] Beren Millidge. “Deep Active Inference as Variational Policy Gradients”. In: Journal of Mathe-
maticalPsychology96(2020),p.102348. ISSN:10960880. DOI:10.1016/j.jmp.2020.102348.
[33] Beren Millidge, Alexander Tschantz, and Christopher L Buckley. Whence the Expected Free En-
ergy?2020. DOI:10.48550/arXiv.2004.08128.arXiv:2004.08128[cs.AI].
[34] Kevin P. Murphy. Probabilistic Machine Learning: Advanced Topics. Cambridge, Massachusetts:
TheMITPress,2023.
[35] Samuel William Nehrer et al. “Introducing ActiveInference.Jl: A Julia Library for Simulation
and Parameter Estimation with Active Inference Models”. In: Entropy 27.1 (Jan. 2025), p. 62.
ISSN:1099-4300. DOI:10.3390/e27010062.
[36] ThomasParr,GiovanniPezzulo,andKarlJ.Friston.ActiveInference:TheFreeEnergyPrinciplein
Mind,Brain,andBehavior.TheMITPress,2022. ISBN:978-0-262-04535-3.
[37] Giovanni Pezzulo, Thomas Parr, and Karl Friston. “Active Inference as a Theory of Sentient
Behavior”.In:BiologicalPsychology186(Feb.2024),p.108741.ISSN:0301-0511.DOI:10.1016/j.
biopsycho.2023.108741.
[38] Giovanni Pezzulo, Francesco Rigoli, and Karl Friston. “Active Inference, Homeostatic Regu-
lation and Adaptive Behavioural Control”. In: Progress in Neurobiology 134 (2015), pp. 17–35.
ISSN:18735118. DOI:10.1016/j.pneurobio.2015.09.001.
[39] JetteRandløvandPrebenAlstrøm.“LearningtoDriveaBicycleUsingReinforcementLearning
andShaping”.In:ProceedingsoftheFifteenthInternationalConferenceonMachineLearning.ICML
’98. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., July 1998, pp. 463–471. ISBN:
978-1-55860-556-5.
[40] StuartRussellandPeterNorvig.ArtificialIntelligence:AModernApproach.4thed.PearsonSeries
inArtificialIntelligence.Pearson,2021. ISBN:978-1-292-15396-4.
[41] Noor Sajid, Philip J. Ball, and Karl J. Friston. “Active Inference: Demystified and Compared”.
In:(2019).
[42] NoorSajidetal.“ExplorationandPreferenceSatisfactionTrade-offinReward-FreeLearning”.
In:arXiv:2106.04316[cs.AI](2021),pp.1–23.arXiv:2106.04316[cs.AI].
[43] AnnaC.Salesetal.“LocusCoeruleusTrackingofPredictionErrorsOptimisesCognitiveFlex-
ibility: An Active Inference Model”. In: PLOS Computational Biology 15.1 (2019), pp. 1–24. DOI:
10.1371/journal.pcbi.1006267.
[44] Simo Sa¨rkka¨. Bayesian Filtering and Smoothing. Institute of Mathematical Statistics Textbooks.
Cambridge:CambridgeUniversityPress,2013.ISBN:978-1-107-03065-7.DOI:10.1017/CBO9781139344203.
[45] Anil K. Seth. “The Cybernetic Bayesian Brain”. In: Open MIND. Ed. by Thomas K. Metzinger
andJenniferM.Windt.Vol.35.FrankfurtamMain:MINDGroup,2015,pp.1–24. ISBN:978-3-
95857-010-8.
22
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
[46] JinYoungShin,CheolhyeongKim,andHyungJuHwang.“PriorPreferenceLearningfromEx-
perts:DesigningaRewardwithActiveInference”.In:Neurocomputing492(July2022),pp.508–
515. ISSN:0925-2312.DOI:10.1016/j.neucom.2021.12.042.
[47] RichardS.SuttonandAndrewG.Barto.ReinforcementLearning:AnIntroduction.AdaptiveCom-
putationandMachineLearning.TheMITPress,2018. ISBN:978-0-262-03924-6.
[48] Filippo Torresan et al. Active Inference for Action-Unaware Agents. 2025. DOI: 10.48550/arXiv.
2508.12027.arXiv:2508.12027[cs.AI].
[49] Alexander Tschantz, Anil K. Seth, and Christopher L. Buckley. “Learning Action-Oriented
Models through Active Inference”. In: PLOS Computational Biology 16.4 (Apr. 2020), e1007805.
DOI:10.1371/journal.pcbi.1007805.
[50] AlexanderTschantzetal.“ScalingActiveInference”.In:(2019),pp.1–13.
[51] AlexanderTschantzetal.“ReinforcementLearningthroughActiveInference”.In:International
ConferenceonLearningRepresentations.Vol.BridgingAIandCognitiveScience.2020,pp.1–16.
[52] Alexander Tschantz et al. “Scaling Active Inference”. In: 2020 International Joint Conference on
NeuralNetworks(IJCNN).Glasgow,UnitedKingdom:IEEE,July2020,pp.1–8.ISBN:978-1-7281-
6926-2. DOI:10.1109/IJCNN48605.2020.9207382.
[53] OttovanderHimstandPabloLanillos.“DeepActiveInferenceforPartiallyObservableMDPs”.
In: IWAI 2020. Ed. by Tim Verbelen et al. Vol. 1326. Communications in Computer and Infor-
mationScience.Springer,2020. DOI:10.1007/978-3-030-64919-7_8.
23
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
Supplementary Material
S1. Mathematical background
S1.1. Notation
TableS1: Summaryofnotation.
Symbol Meaning
t,τ,T integers,i.e.,generic,current,andterminaltimeindex,respectively
1:t,1:T sequencesoftimesstepsuptotandT,respectively
H integer,lengthofasequenceofactions(i.e.,thepolicyhorizon),ingeneralH ≤T
p integer,thenumberofactionsequences(policies)theagentconsiders
X randomvariablewithsupportinX,andwitht∈[1,T]
t
X ,x sequenceofrandomvariableswithtimeindexandrelatedvalues
1:T 1:T
X jthcolumnofmatrixXorrandomvectorassociatedwiththatcolumn
:,j
P(X),P(x ) probabilitydistributionofrandomvariableX andprobabilitythatX =x (whendefined)
t t t t t
H[X] ShannonentropyofrandomvariableX
t t
S finitesetofcardinality|S|,i.e.,thesetofstates
O finitesetofcardinality|O|,i.e.,thesetofobservations
A finitesetofcardinality|A|,i.e.,thesetofactions
AH finitesetofactiontuples(H-foldCartesianproduct)
Π subsetofactionsequences,i.e.,Π⊆AH
S,O,A,π categoricalrandomvariableswithsupportinS,O,A,Π,respectively,i.e.,S ∼Cat(s ),...
t t t t t
s,o,a,π elementsinS,O,A,Π,respectively,wherek∈[1,p]andp∈[1,|Π|]
t k
sj,oj particularrealisationsofrandomvariablesS,O,withj∈[1,|S|]andj∈[1,|O|],respectively
t t
s,o,π, columnvectorsofparametersforstate,observation,andpolicyrandomvariables,respectively
t t
s [i],o [i],π[i] ithelementoftheparametervectorforstate,observation,andpolicyrandomvariables,respectively
t t
T transitionmap/function
E emissionmap/function
P(s |s,a ) transitionprobabilitydistribution(returnedbyT)
t t t
P(o |s ) emissionprobabilitydistribution(returnedbyE)
t t
P∗(s ),P∗(o ) stationarydistributionsoverS andO,respectively
t t
M generativemodel(collectionofprobabilitydistributions)
A matrixinRn×mstoringparametersofP(O t |S t−1 )(thesameforanyt)
B tensorinR|A|×m×mstoringparametersofP(S t |S t−1 )(thesameforanyt)
Ba1,...,Bad state-transitionmatricesinRm×mforeachavailableaction,d=|A|
F freeenergy
F action-conditionedfreeenergyinvanillaactiveinference
aτ−1
F policy-conditionedfreeenergyinvariationalmessagepassing
πk
G single-stepexpectedfreeenergy
t
G totalexpectedfreeenergy,i.e.,sumofexpectedfreeenergiesforHtimestepsinthefuture
H
∇ F gradientofpolicy-conditionedfreeenergywithrespecttovectorofparameterss
st πk t
∇ F gradientoffreeenergywithrespecttovectorofpolicyparametersπ
π
F ⊺ row-vectorinR1×|Π|ofpolicy-conditionedfreeenergies
π
G
H
⊺ row-vectorinR1×|Π|oftotalexpectedfreeenergy
24
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
S2. Further information on experiments
S2.1. HowtoReproducetheResultsoftheExperiment
The results reported in Section 3 were obtained by using the active inference implementation avail-
able at https://github.com/FilConscious/cleanAIF and with the following command line in-
structions.
Softpreferenceswithgoal-shaping:
1 main_aif_au --exp_name aif_au_softgs --gym_id gridworld-v1 --
env_layout gridw9 --num_runs 10 --num_episodes 200 --num_steps 5 --
inf_steps 10 --action_selection kd -lB --num_policies 256 --
pref_type states_manh --pref_loc all_diff
Hardpreferenceswithgoal-shaping:
1 main_aif_au --exp_name aif_au_hardgs --gym_id gridworld-v1 --
env_layout gridw9 --num_runs 10 --num_episodes 200 --num_steps 5 --
inf_steps 10 --action_selection kd -lB --num_policies 256 --
pref_type states --pref_loc all_diff
Softpreferenceswithoutgoal-shaping:
1 main_aif_au --exp_name aif_au_soft --gym_id gridworld-v1 --env_layout
gridw9 --num_runs 10 --num_episodes 200 --num_steps 5 --inf_steps
10 --action_selection kd -lB --num_policies 256 --pref_type
states_manh --pref_loc all_goal
Hardpreferenceswithoutgoal-shaping:
1 main_aif_au --exp_name aif_au_hard --gym_id gridworld-v1 --env_layout
gridw9 --num_runs 10 --num_episodes 200 --num_steps 5 --inf_steps
10 --action_selection kd -lB --num_policies 256 --pref_type states
--pref_loc all_goal
Theplotswereobtainedusingthefollowingcommandlineinstructions:
1 vis_aif -gid gridworld-v1 -el gridw9 -nexp 1 -rdir
episodic_e200_pol16_maxinf10_learnB_cr_Bparams_softgs -fpi 0 1 2 3
4 -i 4 -v 8 -ti 4 -tv 8 -vl 3 -hl 3 -xtes 20 -ph 4 -selrun 0 -npv
16 -sb 4 -ab 0 1 2 3
Withtheseinstructions,onecanvisualizemoremetricsthanthosereportedinthemaintext. We
offeraselectionnext.
25
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
S2.2. Additionalfigures
S2.2.1. State-accesfrequency
State-access frequency
(soft with goal shaping)
100%
21.0 20.0 0.3
80%
60%
0.5 19.3 0.3
40%
0.3 20.0 18.3 20%
0%
Percentage
of
time
steps
State-access frequency
(hard with goal shaping)
100%
20.2 20.0 0.1
80%
60%
0.1 20.0 0.0
40%
0.1 19.8 19.5 20%
0%
Percentage
of
time
steps
State-access frequency
(soft without goal shaping)
100%
23.5 11.0 4.6
80%
60%
11.3 11.7 9.8
40%
4.4 8.8 15.0 20%
0%
Percentage
of
time
steps
State-access frequency
(hard without goal shaping)
100%
25.5 13.3 7.6
80%
60%
11.2 9.0 9.1
40%
4.4 5.7 14.2 20%
0%
Percentage
of
time
steps
Fig.S1: State-accessfrequencyintheexperiment(showingaverageof10agents).
S2.2.2. Breakdownofexpectedfreeenergyatstep1
Riskisthelargestcomponentinexpectedfreeenergytothepointofdeterminingitstrend(compare
the expected free energy and risk figures, Fig. 4 and Fig. S2, respectively, and see Fig. S3 for B-
novelty). In agents with soft goals and goal-shaping, all policies converge on risk values between
2 and 4, with no clear distinction betweem task-solving and task-failing policies (see top-left plot
in Fig. S2). This is the case because in an environment characterised by deterministic transitions
(and without any kind of teleportation), there is no policy that can attain a probability distribution
overenvironment’sstatesthatmatchestheagent’sprior(soft)preferencesateachtimestep,thereby
minimizing risk. In contrast, in agents with hard goals, the policy minimizing risk is π because it
4
bringstheagenttovisitalltheintermediatesteps(goals),formingthetrajectorytothemaingoal(see
top-right plot in Fig. S2). In agents without goal shaping, risk evolves similarly: it increases in the
first60orsoepisodesbeforeconvergingtoastationaryvalue(seebottom-leftandbottom-rightplots
inFig.S2). Alltask-solvingpoliciesachievethesameriskvalue,butitisnotthelowestoneasafew
26
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
task-failingpoliciesappeartominimizeexpectedfreeenergyevenfurther. Thisisagainduetohow
preferencesareencodedandonhowriskiscomputed(seeSection4.2).
As to B-novelty, its evolution is more in line with expectations: the more a policy has been tried
and executed, the more its B-novelty will decrease. In agents with goal shaping, that is precisely
what happens to the preferred π (see top-left and top-right plots in Fig. S3). In agents without
4
goal shaping, the same reduction in B-novelty occurs for all the task-solving policies as well as few
task-failing(seebottom-leftandbottom-rightplotsinFig.S3).
18
16
14
12
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180 200
Episode
ksiR
Risk at step 1
(soft with goal shaping)
18
16
14
12
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180 200
Episode
ksiR
Risk at step 1
(hard with goal shaping)
18
16
14
12
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180 200
Episode
ksiR
Risk at step 1
(soft without goal shaping)
18
16
14
12
10
8
6
4
2
0
1 20 40 60 80 100 120 140 160 180 200
Episode
ksiR
Risk at step 1
(hard without goal shaping)
Policies
1: , , , 5: , , , 9: , , , 13: , , ,
2: , , , 6: , , , 10: , , , 14: , , ,
3: , , , 7: , , , 11: , , , 15: , , ,
4: , , , 8: , , , 12: , , , 16: , , ,
Fig.S2: Risk(expectedfreeenergyterm)foreachpolicyacrossepisodes(showingaverageof10agents).
27
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
2.00
1.75
1.50
1.25
1.00
0.75
0.50
0.25
0.00
1 20 40 60 80 100 120 140 160 180 200
Episode
ytlevon-B
B-novelty at step 1
(soft with goal shaping)
2.00
1.75
1.50
1.25
1.00
0.75
0.50
0.25
0.00
1 20 40 60 80 100 120 140 160 180 200
Episode
ytlevon-B
B-novelty at step 1
(hard with goal shaping)
2.00
1.75
1.50
1.25
1.00
0.75
0.50
0.25
0.00
1 20 40 60 80 100 120 140 160 180 200
Episode
ytlevon-B
B-novelty at step 1
(soft without goal shaping)
2.00
1.75
1.50
1.25
1.00
0.75
0.50
0.25
0.00
1 20 40 60 80 100 120 140 160 180 200
Episode
ytlevon-B
B-novelty at step 1
(hard without goal shaping)
Policies
1: , , , 5: , , , 9: , , , 13: , , ,
2: , , , 6: , , , 10: , , , 14: , , ,
3: , , , 7: , , , 11: , , , 15: , , ,
4: , , , 8: , , , 12: , , , 16: , , ,
Fig.S3: B-novelty(expectedfreeenergyterm)foreachpolicyacrossepisodes(showingaverageof10agents).
28
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
S2.2.3. Averageactionprobabilitiesatsteps2–4
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 2
(soft with goal shaping)
1.0
0.8
0.6
0.4
0.2
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 2
(hard with goal shaping)
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 2
(soft without goal shaping)
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 2
(hard without goal shaping)
Actions
Fig.S4: Actionprobabilitiesatstep2ofeachepisode(showingaverageof10agents).
29
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 3
(soft with goal shaping)
1.0
0.8
0.6
0.4
0.2
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 3
(hard with goal shaping)
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 3
(soft without goal shaping)
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 3
(hard without goal shaping)
Actions
Fig.S5: Actionprobabilitiesatstep3ofeachepisode(showingaverageof10agents).
30
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
1.0
0.8
0.6
0.4
0.2
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 4
(soft with goal shaping)
1.0
0.8
0.6
0.4
0.2
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 4
(hard with goal shaping)
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 4
(soft without goal shaping)
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 4
(hard without goal shaping)
Actions
Fig.S6: Actionprobabilitiesatstep4ofeachepisode(showingaverageof10agent).
31
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
S2.2.4. Single-agentactionsprobabilitiesatstep1–4
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 1
(soft with goal shaping)
1.0
0.8
0.6
0.4
0.2
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 1
(hard with goal shaping)
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 1
(soft without goal shaping)
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 1
(hard without goal shaping)
Actions
Fig.S7: Actionprobabilitiesatstep1ofeachepisode(showingresultsforoneagent).
32
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 2
(soft with goal shaping)
1.0
0.8
0.6
0.4
0.2
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 2
(hard with goal shaping)
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 2
(soft without goal shaping)
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 2
(hard without goal shaping)
Actions
Fig.S8: Actionprobabilitiesatstep2ofeachepisode(showingresultsforoneagent).
33
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 3
(soft with goal shaping)
1.0
0.8
0.6
0.4
0.2
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 3
(hard with goal shaping)
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 3
(soft without goal shaping)
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 3
(hard without goal shaping)
Actions
Fig.S9: Actionprobabilitiesatstep3ofeachepisode(showingresultsforoneagent).
34
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 4
(soft with goal shaping)
1.0
0.8
0.6
0.4
0.2
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 4
(hard with goal shaping)
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 4
(soft without goal shaping)
1.0
0.8
0.6
0.4
0.2
0.0
1 20 40 60 80 100 120 140 160 180 200
Episode
ssam
ytilibaborP
Action probabilities at step 4
(hard without goal shaping)
Actions
Fig.S10: Actionprobabilitiesatstep4ofeachepisode(showingresultsforoneagent).
35
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
S2.2.5. Comparisonofaction-dependentKLdivergencefromgroundtruth
30
25
20
15
10
5
0
1 20 40 60 80 100 120 140 160 180 200
Episodes
staN
Sum of KL divergences for each action
(soft with goal shaping)
30
25
20
15
10
5
0
1 20 40 60 80 100 120 140 160 180 200
Episodes
staN
Sum of KL divergences for each action
(hard with goal shaping)
30
25
20
15
10
5
0
1 20 40 60 80 100 120 140 160 180 200
Episodes
staN
Sum of KL divergences for each action
(soft without goal shaping)
30
25
20
15
10
5
0
1 20 40 60 80 100 120 140 160 180 200
Episodes
staN
Sum of KL divergences for each action
(hard without goal shaping)
Actions
Fig.S11: Sums of KL divergences between learned and ground-truth transition probabilities (columns of B-
matrices),i.e.,∑ D [P(S |s ,a )|P∗(S |s ,a )],foreachavailableactiona (showingaverageof10
st ∈S KL t t t t t t t
agents). ThedecreaseofthistotalKLdivergenceforoneactionacrossepisodesimpliesthattheagent
haslearnedtheconsequencesofperformingthatactionfrommultiplestatesintheenvironment.
36
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
S2.2.6. Groundtruthtransitionmaps
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
1.0
0.8
0.6
0.4
0.2
0.0
Probability
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
1.0
0.8
0.6
0.4
0.2
0.0
Probability
Fig.S12: Groundtruthtransitionmapsforaction→and←inthegridworld.
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
1.0
0.8
0.6
0.4
0.2
0.0
Probability
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
1.0
0.8
0.6
0.4
0.2
0.0
Probability
Fig.S13: Groundtruthransitionmapsforaction↓and↑inthegridworld.
37
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
S2.2.7. Learnedtransitionmaps
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
(soft with goal shaping)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
(hard with goal shaping)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
(soft without goal shaping)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
(hard without goal shaping)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
Fig.S14: Transitionmapsforaction→.
38
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
(soft with goal shaping)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
(hard with goal shaping)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
(soft without goal shaping)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
(hard without goal shaping)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
Fig.S15: Transitionmapsforaction↓.
39
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
(soft with goal shaping)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
(hard with goal shaping)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
(soft without goal shaping)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
(hard without goal shaping)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
Fig.S16: Transitionmapsforaction←.
40
Priorpreferencesinactiveinferenceagents: soft,hard,andgoalshaping
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
(soft with goal shaping)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
(hard with goal shaping)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
(soft without goal shaping)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6 7 8 9
States
setatS
Transition matrix for action
(hard without goal shaping)
1.0
0.8
0.6
0.4
0.2
0.0
Probability
Fig.S17: Transitionmapsforaction↑.
41

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Prior preferences in active inference agents: soft, hard, and goal shaping"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
