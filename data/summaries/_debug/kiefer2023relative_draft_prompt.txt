=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Relative representations for cognitive graphs
Citation Key: kiefer2023relative
Authors: Alex B. Kiefer, Christopher L. Buckley

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2023

Key Terms: representations, relative, university, latent, sussex, spaces, learned, across, cognitive, graphs

=== FULL PAPER TEXT ===

Relative representations for cognitive graphs
Alex B. Kiefer1,2 and Christopher L. Buckley1,3
1 VERSES Research Lab
2 Monash University
3 Sussex AI Group, Department of Informatics, University of Sussex
Abstract. Although the latent spaces learned by distinct neural net-
works are not generally directly comparable, even when model architec-
tureandtrainingdataareheldfixed,recentworkinmachinelearning[13]
hasshownthatitispossibletousethesimilaritiesanddifferencesamong
latent space vectors to derive “relative representations” with compara-
ble representational power to their “absolute” counterparts, and which
are nearly identical across models trained on similar data distributions.
Apart from their intrinsic interest in revealing the underlying structure
of learned latent spaces, relative representations are useful to compare
representations across networks as a generic proxy for convergence, and
for zero-shot model stitching [13].
In this work we examine an extension of relative representations to
discrete state-space models, using Clone-Structured Cognitive Graphs
(CSCGs) [16] for 2D spatial localization and navigation as a test case
in which such representations may be of some practical use. Our work
showsthattheprobabilityvectorscomputedduringmessagepassingcan
be used to define relative representations on CSCGs, enabling effective
communication across agents trained using different random initializa-
tionsandtrainingsequences,andononlypartiallysimilarspaces.Inthe
process,weintroduceatechniqueforzero-shotmodelstitchingthatcan
be applied post hoc, without the need for using relative representations
duringtraining.Thisexploratoryworkisintendedasaproof-of-concept
for the application of relative representations to the study of cognitive
maps in neuroscience and AI.
Keywords: Clone-structuredcognitivegraphs·Relativerepresentations
· Representational similarity
1 Introduction
In this short paper we explore the application of relative representations [13] to
discrete (graph-structured) models of cognition in the hippocampal-entorhinal
system — specifically, Clone-Structured Cognitive Graphs (CSCGs) [16]. In the
first two sections we introduce relative representations and their extension to
discrete latent state spaces via continuous messages passed on graphs. We then
introduceCSCGsandtheiruseinSLAM(SimultaneousLocalizationAndMap-
ping). Finally, we report preliminary experimental results using relative repre-
sentations on CSCGs showing that (a) relative representations can indeed be
3202
peS
9
]CN.oib-q[
1v35640.9032:viXra
2 Kiefer and Buckley
applied successfully to model the latent space structure of discrete, graph-like
representationssuchasCSCGs,andmoregenerallyPOMDPssuchasthoseem-
ployed in discrete active inference modeling [1, 8, 19]; (b) comparison of agents
across partially disparate environments reveals important shared latent space
structure; and (c) it is possible to use the messages or beliefs (probabilities
over states) of one agent to reconstruct the corresponding belief distributions of
another via relative representations, without requiring the use of relative rep-
resentations during training. These examples illustrate an extension of existing
representational analysis techniques developed within neuroscience [10], which
wehopewillproveapplicabletothestudyofcognitivemapsinbiologicalagents.
2 Relative representations
Relative representation [13] isa technique recentlyintroducedin machinelearn-
ing that allows one to map the intrinsically distinct continuous latent space
representations of different models to a common shared representation identi-
cal (or nearly so) across the source models, so that latent spaces can be directly
compared,evenwhenderivedfrommodelswithdifferentarchitectures.Thetech-
nique is conceptually simple: given anchor points A = [x ,x ,...,x ] sampled
1 2 N
from a data or observation space and some similarity function sim (e.g. cosine
similarity)4,therelativerepresentationrM ofdatapointx withrespecttomodel
i i
M can be defined in terms of M’s latent-space embeddings eM =f (x ) as:
i encM i
rM =[sim(eM,eM),sim(eM,eM),...,sim(eM,eM )] (1)
i i a1 i a2 i aN
where eM is the latent representation of anchor i in M.
ai
Crucially, the anchor points A must be matched across models in order for
their relative representations to be compatible. “Matching” is in the simplest
case simply identity, but there are cases in which it is feasible to use pairs of
anchors related by a map g(x)→y (see below).
In [13] it is shown that the convergence of a model M during training
target
is well predicted by the average cosine similarity between its relative represen-
tations of datapoints and those of an independently validated reference model
M . This is to be expected, given that there is an optimal way of partitioning
ref
the data for a given downstream task, and that distinct models trained on the
same objective approximate this optimal solution more or less closely, subject
to variable factors like random initialization and hyperparameter selection.
While relative representations were recently introduced in machine learning,
theytaketheirinspirationinpartfrompriorworkonrepresentationalsimilarity
analysis (RSA) in neuroscience [10, 4]. Indeed, there is a formal equivalence be-
tween relative representations and the Representational Dissimilarity Matrices
(RDMs)proposedasacommonformatforrepresentingdisparatetypesofneuro-
scientificdata(includingbrainimagingmodalitiesaswellassimulatedneuronal
4 The selection of both suitable anchor points and similarity metrics is discussed at
lengthin[13].Weexplainourchoicesforthesehyperparametersinsection5.2below.
Relative representations for cognitive graphs 3
activities in computational models) in [10]. Specifically, if a similarity rather
than dissimilarity metric is employed5, then each row (or, equivalently, column)
of the RDM used to characterize a representational space is, simply, a relative
representation of the corresponding datapoint.
Arguably the main contribution of [13] is to exhibit the usefulness of this
technique in machine learning, where relative representations may be employed
asanoveltypeoflatentspaceinmodelarchitectures.Givenalargeenoughsam-
ple of anchor points, relative representations bear sufficient information to play
functional roles similar to those of the “absolute” representations they model,
rather than simply functioning as an analytical tool (e.g. to characterize the
structure of latent spaces and facilitate abstract comparisons among systems).
The most obvious practical use of relative representations is in enabling “la-
tent space communication”: Moschella et al [13] show that the projection of
embeddings from distinct models onto the same relative representation enables
“zero-shot model stitching”, in which for example the encoder from one trained
model can be spliced to the decoder from another (with the relative represen-
tation being the initial layer supplied as input to the decoder). A limitation of
this procedure is that it depends on using a relative representation layer dur-
ing training, precluding its use for establishing communication between “frozen”
pretrained models. Below, we make use of a parameter-free technique that al-
lows one to map from the relative representation space back to the “absolute”
representations of the input models with some degree of success.
3 Extending relative representations to discrete
state-space models
Despite the remarkable achievements of continuous state-space models in deep
learning systems, discrete state spaces continue to be relevant, both in machine
learningapplications,wherediscrete“worldmodels” areresponsibleforstate-of-
the-art results in model-based reinforcement learning [6], and in neuroscience,
where there is ample evidence for discretized, graph-like representations, for
example in the hippocampal-entorhinal system [25, 18, 16] and in models of
decision-makingprocessesthatleveragePOMDPs(PartiallyObservableMarkov
Decision Processes) [19].
While typical vector similarity metrics such as cosine distance behave in a
somewhatdegeneratewaywhenappliedtomanytypesofdiscreterepresentations
(e.g., the cosine similarity between two one-hot vectors in the same space is 1 if
the vectors are identical and 0 otherwise), they can still be usefully applied in
this case (see section 5 below). More generally, the posterior belief distributions
inferredoverdiscretestatespacesduringsimulationsinagent-basedmodelsmay
provide suitable anchor points for constructing relative representations.
Concretely, such posterior distributions are often derived using message-
passingalgorithms,suchasbeliefpropagation[14]orvariationalmessagepassing
5 See [10] fn.2.
4 Kiefer and Buckley
[27]. We pursue such a strategy for deriving relative representations of a special
kindofhiddenMarkovmodel(theClone-StructuredHiddenMarkovModelor(if
supplemented with actions) Cognitive Graph [16]), in which it is simple to com-
pute forward messages which at each discrete time-step give the probability of
the hidden states z conditioned on a sequence of observations o (i.e. P(z |o )).
t 1:t
The CSCG/CHMM is particularly interesting both because of its fidelity as a
model of hippocampal-entorhinal representations in the brain and because, as
in the case of neural networks, distinct agents may learn superficially distinct
CSCGsthatnonethelessformnearlyisomorphiccognitivemaps,asshownbelow.
4 SLAM using Clone-Structured Cognitive Graphs
An important strand of research in contemporary machine learning and compu-
tationalneurosciencehasfocusedonunderstandingtheroleofthehippocampus
and entorhinal cortex in spatial navigation [20, 23, 25, 16], a perspective that
may be applicable to navigation in more abstract spaces as well [18, 21]. This
field of research has given rise to models like the Tolman-Eichenbaum machine
[25]andClone-StructuredCognitiveGraph[5,16].Wefocusonthelattermodel
inthepresentstudy,asitiseasytoimplementontoytestproblemsandyieldsa
suitablerepresentationforourpurposes(anexplicitdiscretelatentspacethrough
which messages can be propagated).
The core of the CSCG is a special kind of “clone-structured” Hidden Markov
Model (CHMM) [17], in which each of N possible discrete observations are
mapped deterministically to only a single “column” of hidden states by the like-
(cid:40)
1 if z ∈C(o)
lihood function, i.e. p(o|z)= , where C(o) is the set of “clones”
0 if z ∈/ C(o)
of observation o. The clone structure encodes the inductive bias that the same
observation may occur within a potentially large but effectively finite number
of contexts (i.e. within many distinct sequences of observations), where each
“clone” functionsasalatentrepresentationofoinadistinctcontext.Thisallows
the model to efficiently encode higher-order sequences [3] by learning transition
dynamics (“lateral” connections) among the clones. CSCGs supplement this ar-
chitecture with a set of actions which condition transition dynamics, creating in
effect a restricted form of POMDP.
The most obvious use of CSCG models (mirroring the function of the hip-
pocampal-entorhinal system) is to allow agents capable of moving in a space to
perform SLAM (Simultaneous Localization And Mapping) with no prior knowl-
edge of the space’s topology. Starting with a random transition matrix, CSCGs
trained on random walks in 2D “rooms”, in which each cell corresponds to an
observation,areshownin[16]tobecapableoflearningaction-conditionedtran-
sition dynamics among hidden states that exhibit a sparsity structure precisely
recapitulating the spatial layout of the room (see Fig. 1).6
6 Thetrainingusedtoobtainthisresultisbasedonanefficientimplementationofthe
Baum-Welch algorithm for E-M learning, followed by Viterbi training — please see
[16] for details.
Relative representations for cognitive graphs 5
Fig.1. Example of two cognitive graphs (B) learned by CSCG agents via distinct
random walks on the same room (A). Following the convention in [16], colors indicate
distinct discrete observations (in the room) or latent “clones” corresponding to those
observations (in the graphs). Code for training and producing plots is provided in the
supplementary materials for [16]. Note that the two graphs are obviously isomorphic
upon inspection (the left graph is visually rotated about 50 degrees clockwise relative
to the right one, and the node labels differ).
Given a sequence of observations, an agent can then infer states that cor-
respond to its location in the room, with increasing certainty and accuracy as
sequence length increases. Crucially, location is not an input to this model but
the agent’s representation of location is entirely “emergent” from the unsuper-
vised learning of higher-order sequences of observations.
Building on the codebase provided in [16], we examined the certainty of
agents’ inferred beliefs about spatial location during the course of a random
walk (see Figure 2.). Though less than fully confident, such agents are able to
reliably infer room location from observation sequences alone after a handful
of steps. Conditioning inference as well on the equivalent of “proprioceptive”
information(i.e.,aboutwhichactionsresultedintherelevantsequenceofobser-
vations) dramatically increases the certainty of the agents’ beliefs. We explored
both of these regimes of (un)certainty in our experiments.
5 Experiments: Communication across cognitive maps
We investigate the extent to which common structure underlying the “cognitive
maps” learned by distinct CSCG agents can be exploited to enable communi-
cation across them. As in the case of neural networks trained on similar data,
CSCGagentstrainedonthesameroombutwithdistinctrandominitializations
and observation sequences learn distinct representations that are nonetheless
isomorphic at one level of abstraction (i.e. when comparing the structural rela-
tionships among their elements, which relative representations make explicit —
cf. Appendix B, Fig. 5).
We also explore whether partial mappings can be obtained across agents
trained on somewhat dissimilar rooms. We used two metrics to evaluate the
quality of cross-agent belief mappings: (1) recoverability of the maximum a pos-
teriori belief of one agent at a given timestep, given those of another agent
6 Kiefer and Buckley
Fig.2.MaximumprobabilityassignedtoanyhiddenstateofaCSCGovertime(during
a random walk). The left panel shows confidence derived from messages inferred from
observations alone, and the right panel shows the case of messages inferred from both
actions and observations.
following an analogous trajectory; (2) cosine similarity between a given message
and its “reconstruction” via such a mapping. The main results of these prelimi-
nary experiments are reported in Table 1.
5.1 Mapping via permutation
We first confirmed that CSCG agents trained on distinct random walks of the
same room (and with distinct random transition matrix initializations) learn
functionally identical cognitive maps if trained to convergence using the proce-
dure specified in [16]. Visualizations of the learned graphs clearly demonstrate
topological isomorphism (see references as well as figure 1B), but in addition we
found that the forward messages for a given sequence of observations are identi-
cal across agents up to a permutation (i.e., which “clones” are used to represent
whichobservationcontextsdependsonthesymmetrybreakinginducedbydiffer-
entrandomwalksandinitializations).Itisthuspossibleto“translate” acrosssuch
cognitive maps in a simple way. First, we obtain message sequences M and M′
fromthefirstandsecondCSCGsconditionedonthesameobservationsequence,
and extract messages m and m′ corresponding to some particular observation
o .Wethenconstructamappingsort_index (z)→sort_index (z′)from
t mot m′ot
the sort order of entries z in m to that of entries z′ in m′. Using this mapping,
we can predict the maximum a posteriori beliefs in M′ nearly perfectly given
those in M under ideal conditions (see the “Permutation (identical)” condition
in Table 1).7
7 This procedure does not work if the chosen message represents a state of high un-
certainty, e.g. at the first step of a random walk with no informative initial state
Relative representations for cognitive graphs 7
5.2 Mapping via relative representations
Though it is thus relatively simple to obtain a mapping across cognitive graphs
in the ideal case of CSCGs trained to convergence on identical environments,
we confirm that relative representations can be used in this setting to obtain
comparable results. A message m′ from the second sequence (associated with
model B) can be reconstructed from message m in the first (model A’s) by
linearlycombiningmodelB’sembeddingsEB oftheanchorpoints,viaasoftmax
A
(σ)function(withtemperatureT)oftherelativerepresentationrA ofmderived
m
from model A’s anchor embeddings:8
mˆ′ = (cid:0) EB(cid:1) σ (cid:104)rA m (cid:105) (2)
A T
Intuitively, the softmax term scales the contribution of each vector in the
set of anchor embeddings to the reconstruction mˆ′ in proportion to its relative
similarity to the input embedding, so that the reconstruction is a weighted su-
perposition (convex combination) of the anchor points. The reconstruction of
a sequence M′ of m d′-dimensional messages from an analogous “source” se-
quence M of d-dimensional messages, with the “batch” relative representation
operation9 RA ∈Rm×|A| written out explicitly in terms of the matrix product
M
between M ∈ Rm×d and anchor embeddings EA ∈ R|A|×d, is then precisely
A
analogous to the self-attention operation in transformers:
(cid:104)M
(cid:2) EA(cid:3)T
(cid:105)
Mˆ′ =σ A EB (3)
T A
Here, the source messages M play the role of the queries Q, model A’s anchor
embeddings EA act as keys K, and model B’s anchor embeddings act as values
V in the atten A tion equation which computes output Z=σ (cid:2) QKT(cid:3) V.10
Since self-attention may be understood though the lens of its connection
to associative memory models [15, 12], this correspondence goes some way to-
ward theoretically justifying our choice of reconstruction method. In particular,
following [12], reconstruction via relative representations can be understood as
implementing a form of heteroassociative memory in which model A and B’s
anchor embeddings are, respectively, the memory and projection matrices.
Though empirical performance against a wider range of alternative methods
oflatentspacealignmentremainstobeassessed,wenoteaformalconnectionto
prior. The mapping also fails for many states since CSCGs, by construction, assign
zeroprobabilitytoallstatesnotwithintheclonesetofagivenobservation,leading
todegeneracyinthemapping.Wealsofoundthataccuracyofthismethoddegrades
rapidly to the extent that the learned map fails to converge to the ground truth
room topology.
8 In practice, a softmax with a low temperature worked best for reconstruction.
9 If M=A, this term is a representational similarity matrix in the sense of [10].
10 Inthepresentsetting,onemightevendrawaparallelbetweenthelinearprojection
oftransformerinputstothekey,queryandvaluematricesandthelinearprojection
ofobservationsandpriorbeliefsontomessagesvialikelihoodandtransitiontensors.
8 Kiefer and Buckley
regression-basedapproachessuchas[22],inwhicharepresentationY ofthedata
is expressed as a mixture of “guesses” (linear projections of local embeddings)
fromk experts,weightedaccordingtothefidelityofeachexpert’srepresentation
of the input data X. This can be expressed as a system of linear equations
Y = UL in which Y, U and L play roles analogous to those of Mˆ, σ (cid:2) RA(cid:3) and
M
EB above, with the “repsonsibility” terms (weights) introducing nonlinearity, as
A
the softmax does in our approach (see Appendix C for further details).
Not surprisingly, the results of our procedure improve with the number of
anchors used (see Appendix A, Figure 4). In our experiments, we used N =
5000 anchors. We obtained more accurate mappings using this technique when
the anchor points were sampled from the trajectory being reconstructed, which
raises the probability of an exact match in the anchor set; for generality, all
reported results instead sample anchor points (uniformly, without replacement)
from distinct random walks. While it would be possible in the present setting
to use similarity metrics tailored to probability distributions to create relative
representations, we found empirically that replacing cosine similarity with the
negative Jensen-Shannon distance slightly adversely affected performance.
5.3 Mapping across dissimilar models
Table 1. Mapping across distinct CSCG models*
Max belief recovery Reconstruction accuracy
Condition % accurate (±SD) mean cosine similarity (±SD)
Baseline: AR† (identical) 0.01(±0.01) 0.07(±0.07)
Permutation (identical) 84.09(±28.9) 0.69(±0.01)
Permutation (shifted) 3.41(±1.48) 0.69(±0.01)
Permutation (landmark) 20.70(±19.14) 0.89(±0.003)
RR‡ (identical) 89.44(±1.84) 0.99(±0.003)
RR (isomorphic) 41.0(±3.17) 0.67(±0.02)
RR (expansion: large → small) 97.42(±3.24) 0.98(±0.02)
RR (expansion: small → large) 47.47(±2.74) 0.59(±0.02)
RR (shifted) 34.81(±3.81) 0.63(±0.03)
RR (landmark) 34.13(±6.47) 0.52(±0.06)
†AbsoluteRepresentations‡RelativeRepresentations*Foreachcondition,meanresults
and standard deviation over 100 trials (each run on a distinct random graph) are
reported, for the more challenging case of messages conditioned only on observations.
For all but the (expansion) conditions, the results of mapping in either direction were
closely comparable and we report the mean.
Relative representations for cognitive graphs 9
Fig.3. Schematic illustration of experimental conditions. A and B indicate distinct
roomsonwhichparallelmodelsweretrained,exceptforthe“IDENTICAL” condition,
where multiple models are trained on a single room. Numbers within nodes illustrate
stochastic association of particular hidden state indices with positions in the learned
graphs. Graph sizes depicted here do not reflect those used in the experiments.
Asshownin[13],relativerepresentationscanrevealcommonstructureacross
superficially quite different models — for example those trained on sentences in
distinctnaturallanguages—viatheuseof“parallel” anchorpoints,inwhichthe
anchors chosen for each model are related by some mapping (e.g. being transla-
tionsofthesametext).InthecontextofCSCGs,anchors(forwardmessages)are
defined relative to an observation sequence. To sample parallel anchors across
agents, we therefore require partially dissimilar rooms in which similar but dis-
tinct observation sequences can be generated.
We used four experimental manipulations to generate pairs of partially dis-
similar rooms (see Figure 3), which we now outline along with a brief discussion
of our results on each.
Isomorphism Anyrandomlygeneratedgridor“room” ofagivenfixedsizewill
(if CSCG training converges) yield a cognitive map with the same topology. It
should thus be possible to generate parallel sequences of (action, observation)
pairs — and thus parallel anchor points for defining relative representations —
across two such random rooms, even if each contains a distinct set of possible
observations or a different number of clones, either of which would preclude the
use of a simple permutation-based mapping.
Therelationshipsamongobservationswilldifferacrosssuchrooms,however,
which matters under conditions of uncertainty, since every clone of a given ob-
servation will be partially activated when that observation is received, leading
10 Kiefer and Buckley
to different conditional belief distributions. This effect should be mitigated or
eliminated entirely when beliefs are more or less certain, in which case “lateral”
connections (transition dynamics) select just one among the possible clones cor-
responding to each observation. Indeed, we found that it is possible to obtain
near-perfect reconstruction accuracy across models trained on random rooms
with distinct observation sets, provided that messages are conditioned on both
actionsandobservations;whereasweonlyobtaineda<50%successrateinthis
scenario when conditioning on observations alone.
Expansion In this set of experiments, we generated “expanded” versions of
smaller rooms and corresponding “stretched” trajectories (paired observation
and action sequences) using Kroenecker products, so that each location in the
smaller room is expanded into a 2×2 block in the larger room, and each step
in the smaller room corresponds to two steps in the larger one. We can then
define parallel anchors across agents trained on such a pair of rooms, by taking
(a) all messages in the smaller room, and (b) every other message in the larger
one. In this condition, the large → small mapping can be performed much more
accuratelythantheoppositeone,sinceeachanchorpointinthesmaller(“down-
sampled”) room corresponds to four potential locations in the larger. Superior
results on the (large → small) condition VS our experiments on identical rooms
may be explained by the fact that the “small” room containts fewer candidate
locations than the room used in the “Identical” condition.
Shifting In a third set of experiments, we generated rooms by taking overlap-
ping vertical slices of a wider room, such that identical sequences were observed
whiletraversingtherooms,butwithindifferentwidercontexts.Inthiscaseonly
themessagescorrespondingtooverlappinglocationswereusedasanchorpoints,
buttests wereperformedonrandomwalksacrosstheentireroom.Undercondi-
tions of certainty, mapping across these two rooms can be solved near-perfectly
byusingallmessagesascandidateanchorpoints,sincetheroomsareisomorphic.
Without access to ground-truth actions, it was possible to recover the beliefs of
oneagentgiventheother’sonly∼35%ofthetime,evenifanchorsweresampled
from all locations. We hypothesize that this problem is more challenging than
the “Isomorphic” condition because similar patterns of observations (and thus
similar messages) correspond to distinct locations across the two rooms, which
should have the effect of biasing reconstructions toward the wrong locations.
Landmarks Finally, partially following the experiments in [16] on largely fea-
tureless rooms with unique observations corresponding to unique locations (e.g.
cornersandwalls),wedefinepairsofroomswiththesame(unique)observations
assigned to elements of the perimeter, filled by otherwise randomly generated
observations that differed across rooms. Using only the common “landmark” lo-
cations as anchors, it was still possible to use relative representations to recover
anagent’slocationfrommessagesinaparalleltrajectoryintheotherroomwith
some success.
Relative representations for cognitive graphs 11
Summary The results reported in Table 1 were obtained under conditions of
significantuncertainty,inwhichmessageswereconditionedonlyonobservations,
without knowledge of the action that produced those observations. In this chal-
lengingsetting,relativerepresentationsstillenabledrecovery(wellabovechance
in all experimental conditions, and in some cases quite accurate) of one agent’s
maximum a posteriori belief about its location from those of the other agent,
averaged across messages in a test sequence.11
In all settings, it was possible to obtain highly accurate mappings (> 99%
correct in most cases) by conditioning messages on actions as well as observa-
tions.Thisyieldsbeliefvectorssharplypeakedatthehiddenstatecorresponding
to an agent’s location on the map. In this regime, the reconstruction procedure
actsessentiallyasalookuptable,asagivenmessagemresemblesaone-hotvec-
tor and this sparsity structure is reflected in the relative representation (which
is ∼ 0 everywhere except for dimensions corresponding to anchor points nearly
identical to m). The softmax weighting then simply “selects” the correspond-
ing anchor in model B’s anchor set.12 Conditioning messages on probabilistic
knowledge of actions (perhaps the most realistic scenario) can be expected to
greatly improve accuracy relative to the observation-only condition, and is an
interesting subject for a follow-up study.
6 Discussion
The“messages” usedtodefinerelativerepresentationsinthepresentworkcanbe
interpreted as probability distributions, but they can also be interpreted more
agnostically as, simply, neuronal activity vectors. Recent work in systems neu-
roscience [2] has shown that it is possible to recover common abstract latent
spaces from real neuronal activity profiles. As noted above, relative representa-
tionswereanticipatedinneurosciencebyRSA,whichineffecttreatstheneuronal
responses, or computational model states, associated with certain fixed stimuli
as anchor points. This technique complements others such as the analysis of
attractor dynamics [26] as a tool to investigate properties of latent spaces in
brains, and has been shown to be capable of revealing common latent represen-
tational structure across not only individuals, but linguistic communities [28]
andevenspecies[11,7].Consistentwiththeaimsof[13]and[10],thisparadigm
might ultimately provide fascinating future directions for brain imaging studies
of navigational systems in the hippocampal-entorhinal system and elsewhere.
Relative representations generalize this paradigm to “parallel anchors”, and
also demonstrate the utility of high-dimensional representational similarity vec-
11 Itisworthnotingthatthisisessentiallyaone-of-Nclassificationtask,witheffective
valuesofNaround48inmostcases.Thisisbecause(following[16])mostexperiments
wereperformedon6×8rooms,andthereisone“active” clonecorrespondingtoeach
location in a converged CSCG.
12 There is a variation on this in which multiple matches exist in the anchor set, but
the result is the same as we then combine n identical anchor points.
12 Kiefer and Buckley
tors as latent representations in their own right, which can, as demonstrated
above, be used to establish zero-shot communication between distinct models.
Whiletheconditionsweconstructedinourtoyexperimentsareartificial,they
haveanaloguesinmorerealisticscenarios.Itisplausiblethatanimalsnavigating
structurallyhomeomorphicbutsuperficiallydistinctenvironments,forexample,
shouldlearnsimilarcognitivemapsatsomelevelofabstraction.Somethinganal-
ogous to the “expansion” setting may occur across two organisms that explore
thesamespacebut(forexampleduetodifferentsizesorspeedsoftraversal,and
thus sample rates) coarse-grain it differently. The idea of landmark-based navi-
gationiscentraltotheSLAMparadigmgenerally,andthestabilityoflandmarks
acrossotherwisedifferentspacesmayprovideamodelfortheabilitytonavigate
despite changes to the same environment over time. Finally, while experiments
on partially overlapping rooms seem somewhat contrived if applied naively to
spatial navigation scenarios, they may be quite relevant to models of SLAM in
abstract spaces [18], such as during language acquisition, where different speak-
ers of the same language may be exposed to partially disjoint sets of stimuli,
corresponding to different dialects (or in the limit, idiolects).
Crucially, the common reference frame provided by these techniques might
allow for the analysis of shared representations, which (when derived from well-
functioningsystems)shouldembodyanidealstructurethatindividualcognitive
systemsinsomesenseaimtoapproximate,allowingforcomparisonofindividual
brain-boundmodelsagainstashared,abstractgroundtruth.Suchanabstracted
“ideal” latent space could be used to measure error or misrepresentation [9], or
to assess progress in developmental contexts.
7 Conclusion
In this work we have considered a toy example of the application of relative
representations to graph-structured cognitive maps. The results reported here
are intended mainly to illustrate concrete directions for the exploration of the
latentstructureofcognitivemapsusingrelativerepresentations,andasaproof-
of-principle that the technique can be applied to the case of inferred posterior
distributionsoverdiscretelatentspaces.Wehavealsointroducedatechniquefor
reconstructing “absolute” representations from their relative counterparts with-
out learning.
In addition to further investigating hyperparameter settings (such as choice
of similarity function) to optimize performance in practical applications, future
work might explore the application of relative representations to more complex
models with discrete latent states, such as the discrete “world models” used in
cutting-edge model-based reinforcement learning [6], or to enable belief sharing
and cooperation in multi-agent active inference scenarios. Given the connection
toneuralself-attentiondescribedabove,whichhasalsobeennotedinthecontext
of the Tolman-Eichenbaum Machine [24], it would also be intriguing to explore
models in which such a translation process occurs within agents themselves, as
a means of transferring knowledge across local cognitive structures.
Relative representations for cognitive graphs 13
Acknowledgements
Alex Kiefer is supported by VERSES Research. CLB is supported by BBRSC
grantnumberBB/P022197/1andbyJointResearchwiththeNationalInstitutes
of Natural Sciences (NINS), Japan, program No. 0111200.
Code Availability
TheCSCGimplementationisbasedalmostentirelyonthecodebaseprovidedin
[16]. Code for reproducing our experiments and analysis can be found at:
https://github.com/exilefaker/cscg-rr
References
[1] LancelotDaCostaetal.“Activeinferenceondiscretestate-spaces:Asyn-
thesis”.In:Journal of Mathematical Psychology 99(2020),p.102447.issn:
0022-2496. doi: https://doi.org/10.1016/j.jmp.2020.102447. url:
https://www.sciencedirect.com/science/article/pii/S0022249620300857.
[2] MaxDabagia,KonradP.Kording,andEvaL.Dyer.“Aligninglatentrepre-
sentations of neural activity”. In: Nature Biomedical Engineering 7 (Apr.
2023), pp. 337–343. doi: https://doi.org/10.1038/s41551-022-
00962-7.
[3] AntoineDedieuetal.Learninghigher-ordersequentialstructurewithcloned
HMMs. 2019. arXiv: 1905.00507 [stat.ML].
[4] Halle R. Dimsdale-Zucker and Charan Ranganath. “Chapter 27 - Repre-
sentationalSimilarityAnalyses:APracticalGuideforFunctionalMRIAp-
plications”. In: Handbook of in Vivo Neural Plasticity Techniques. Ed. by
DeniseManahan-Vaughan.Vol.28.HandbookofBehavioralNeuroscience.
Elsevier,2018,pp.509–525.doi:https://doi.org/10.1016/B978-0-12-
812028-6.00027-6. url: https://www.sciencedirect.com/science/
article/pii/B9780128120286000276.
[5] Dileep George et al. “Clone-structured graph representations enable flexi-
ble learning and vicarious evaluation of cognitive maps”. In: Nature com-
munications 12.1 (2021), p. 2392.
[6] Danijar Hafner et al. “Mastering Atari with Discrete World Models”. In:
CoRR abs/2010.02193 (2020). arXiv: 2010.02193. url: https://arxiv.
org/abs/2010.02193.
[7] James V. Haxby, Andrew C. Connolly, and J. Swaroop Guntupalli. “De-
codingneuralrepresentationalspacesusingmultivariatepatternanalysis.”
In: Annual review of neuroscience 37 (2014), pp. 435–56. url: https:
//api.semanticscholar.org/CorpusID:6794418.
[8] ConorHeinsetal.“pymdp:APythonlibraryforactiveinferenceindiscrete
state spaces”. In: CoRR abs/2201.03904 (2022). arXiv: 2201.03904. url:
https://arxiv.org/abs/2201.03904.
14 Kiefer and Buckley
[9] Alex Kiefer and Jakob Hohwy. “Representation in the Prediction Error
Minimization Framework”. In: The Routledge Companion to Philosophy of
Psychology: 2nd Edition.Ed.bySarahK.Robins,JohnSymons,andPaco
Calvo. 2019, pp. 384–409.
[10] Nikolaus Kriegeskorte, Marieke Mur, and Peter Bandettini. “Representa-
tional Similarity Analysis – Connecting the Branches of Systems Neuro-
science”. In: Frontiers in systems neuroscience 2 (Feb. 2008), p. 4. doi:
10.3389/neuro.06.004.2008.
[11] NikolausKriegeskorteetal.“MatchingCategoricalObjectRepresentations
in Inferior Temporal Cortex of Man and Monkey”. In: Neuron 60 (2008),
pp. 1126–1141. url: https://api.semanticscholar.org/CorpusID:
313180.
[12] Beren Millidge et al. “Universal Hopfield Networks: A General Frame-
work for Single-Shot Associative Memory Models”. In: Proceedings of the
39th International Conference on Machine Learning. Vol. 162. Baltimore,
Maryland, USA, July 2022, pp. 15561–15583.
[13] LucaMoschellaetal.Relativerepresentationsenablezero-shotlatentspace
communication. 2023. arXiv: 2209.15430 [cs.LG].
[14] Judea Pearl. “Reverend Bayes on Inference Engines: A Distributed Hier-
archical Approach”. In: Proceedings of the Second AAAI Conference on
Artificial Intelligence. AAAI’82. Pittsburgh, Pennsylvania: AAAI Press,
1982, pp. 133–136.
[15] Hubert Ramsauer et al. Hopfield Networks is All You Need. 2021. arXiv:
2008.02217 [cs.NE].
[16] RajeevV.Rikhyeetal.“Learningcognitivemapsasstructuredgraphsfor
vicarious evaluation”. In: bioRxiv (2020). doi: 10.1101/864421. eprint:
https://www.biorxiv.org/content/early/2020/06/24/864421.full.
pdf. url: https://www.biorxiv.org/content/early/2020/06/24/
864421.
[17] Rajeev V. Rikhye et al. “Memorize-Generalize: An online algorithm for
learninghigher-ordersequentialstructurewithclonedHiddenMarkovMod-
els”. In: bioRxiv (2019). doi: 10.1101/764456. eprint: https://www.
biorxiv.org/content/early/2019/09/10/764456.full.pdf. url:
https://www.biorxiv.org/content/early/2019/09/10/764456.
[18] Adam Safron, Ozan Çatal, and Tim Verbelen. Generalized Simultaneous
Localization and Mapping (G-SLAM) as unification framework for natu-
ral and artificial intelligences: towards reverse engineering the hippocam-
pal/entorhinalsystemandprinciplesofhigh-levelcognition.Oct.2021.doi:
10.31234/osf.io/tdw82. url: psyarxiv.com/tdw82.
[19] Ryan Smith, Karl J. Friston, and Christopher J. Whyte. “A step-by-step
tutorialonactiveinferenceanditsapplicationtoempiricaldata”.In:Jour-
nal of Mathematical Psychology 107 (2022), p. 102632. issn: 0022-2496.
doi: https://doi.org/10.1016/j.jmp.2021.102632. url: https://
www.sciencedirect.com/science/article/pii/S0022249621000973.
Relative representations for cognitive graphs 15
[20] Kimberly Stachenfeld, Matthew Botvinick, and Samuel Gershman. “The
hippocampusasapredictivemap”.In:(July2017).doi:10.1101/097170.
[21] Sivaramakrishnan Swaminathan et al. Schema-learning and rebinding as
mechanisms of in-context learning and emergence. 2023. arXiv: 2307.
01201 [cs.CL].
[22] Yee Teh and Sam Roweis. “Automatic Alignment of Local Representa-
tions”. In: Advances in Neural Information Processing Systems. Ed. by
S. Becker, S. Thrun, and K. Obermayer. Vol. 15. MIT Press, 2002. url:
https://proceedings.neurips.cc/paper_files/paper/2002/file/
3a1dd98341fafc1dfe9bcf36360e6b84-Paper.pdf.
[23] JamesWhittingtonetal.“Howtobuildacognitivemap”.In:Nature Neu-
roscience 25 (Sept. 2022), pp. 1–16. doi: 10.1038/s41593-022-01153-y.
[24] James C. R. Whittington, Joseph Warren, and Timothy Edward John
Behrens. “Relating transformers to models and neural representations of
thehippocampalformation”.In:CoRRabs/2112.04035(2021).arXiv:2112.
04035. url: https://arxiv.org/abs/2112.04035.
[25] James C.R. Whittington et al. “The Tolman-Eichenbaum Machine: Uni-
fying Space and Relational Memory through Generalization in the Hip-
pocampal Formation”. In: Cell 183.5 (2020), 1249–1263.e23. issn: 0092-
8674. doi: https://doi.org/10.1016/j.cell.2020.10.024. url:
https://www.sciencedirect.com/science/article/pii/S009286742031388X.
[26] Tom J. Wills et al. “Attractor Dynamics in the Hippocampal Representa-
tion of the Local Environment”. In: Science 308.5723 (2005), pp. 873–876.
doi: 10.1126/science.1108905. eprint: https://www.science.org/
doi/pdf/10.1126/science.1108905. url: https://www.science.org/
doi/abs/10.1126/science.1108905.
[27] JohnWinnandChristopherM.Bishop.“VariationalMessagePassing”.In:
J. Mach. Learn. Res. 6 (Dec. 2005), pp. 661–694. issn: 1532-4435.
[28] BenjaminD.Zinszeretal.“SemanticStructuralAlignmentofNeuralRep-
resentational Spaces Enables Translation between English and Chinese
Words”. In: Journal of Cognitive Neuroscience 28 (2016), pp. 1749–1759.
url: https://api.semanticscholar.org/CorpusID:577366.
16 Kiefer and Buckley
Appendix A: Effect of anchor set size on reconstruction
Fig.4. Average cosine similarity ( u·v ) between ground-truth CSCG beliefs (mes-
∥u∥∥v∥
sages) and their reconstructions from those of a distinct CSCG model trained on the
sameroomandreceivingthesamesequenceofobservations,usingthemethodinEqua-
tion2,plottedagainstnumberN ofanchorsusedtodefinetherelativerepresentations.
WebeginbysettingN tothedimensionalityofthemodel’shiddenstate.Theaverage
is across all 5000 messages in a test sequence.
Relative representations for cognitive graphs 17
Appendix B: Visualizing the correspondence of relative
representations across models
Fig.5. Example representational similarity matrix comparing relative representations
of analogous message sequences (i.e. inferred from the same observation sequence)
from two distinct models trained on the same environment. This differs from the
(dis)similarity matrices typically used in RSA [10], as rows and columns in this case
representdistinctsetsoffirst-orderrepresentations,i.e.cell(i,j)representsthecosine
similarity between rA and rB. Thus the diagonal symmetry illustrates the empirical
i j
equivalence of these two sets of relative representations.
Appendix C: Comparison to LLC
LocallyLinearCoordination(LLC)[22]isamethodforaligningtheembeddings
of multiple dimensionality-reducing models so that they project to the same
18 Kiefer and Buckley
global coordinate system. While its aims differ somewhat from the procedure
outlined in the present study, LLC is also an approach to translating multi-
ple source embeddings to a common representational format. As noted above,
there is an interesting formal resemblance between the two approaches, which
we explore in this Appendix.
The LLC representation
LLCpresupposesamixturemodelofexpertstrainedonN D-dimensionalinput
datapoints X = [x ,x ,...,x ], in which each expert m is a dimensionality re-
1 2 N k
ducer that produces a local embedding z
nk
∈Rdk of datapoint x
n
. The mixture
weights or “responsibilities” for the model can be derived, for example, as pos-
teriors over each expert’s having generated the data, in a probabilistic setting.
Given the local embeddings and responsibilities, LLC proposes an algorithm
for discovering linear mappings L
k
∈Rd×dk from each expert’s embedding to a
common (lower-dimensional) output representation Y ∈ RN×d, which can then
be expressed as a responsibility-weighted mixture of these projections. That is
to say, leaving out bias terms for simplicity: each output image y of datapoint
n
x is computed as
n
(cid:88) (cid:0) (cid:1)
y = r L z (4)
n nk k nk
k
Cruciallyforwhatfollows,withthehelpofaflattened(1D)indexthatspans
the “batch” dimension N as well as the experts k, we can express this in simpler
termsasY =UL.WedefinematricesU ∈RN×(cid:80) k dk andL∈R(cid:80) k dk×d interms
of, respectively: (a) vectors u , where u = r zi (i.e. the jth element of u
n nj nk nk n
is the ith element of k’s embedding of x scaled by its responsibility term) —
n
and (b) re-indexed, transposed columns l = li of the L matrices. Intuitively,
j k k
each row u of U concatenates the experts’ responsibility-weighted embeddings
n
r z of datapoint x , while each of L’s d columns is a concatenation of the
nk nk n
correspondingrowoftheprojectionmatricesL ,sothatthematrixproductUL
k
returns a responsibility-weighted prediction for y in each row (see Figure 6).
n
Relationship to our proposal
Ignoringthemotivationofdimensionalityreductionwhichisirrelevantforpresent
purposes,thereisapreciseconceptualandformalequivalencebetweenthismodel
andtheprocedureforreconstructingmodelB’sembeddingsgiventhoseofmodel
A described above in Section 5.2.
Specifically, we can regard each of model A’s anchor embeddings eA as an
xk
"expert" in a fictitious mixture model, with an associated responsibility term
measuring its fidelity to the input x , which in this case is given by the cosine
i
similarity between the anchor embedding and the input embedding. Then like
therowsofU,eachrowofσ (cid:2) RA(cid:3) ,whichisarelativerepresentationrA =EAeA
X i A i
of input i after application of the softmax, acts as a responsibility-weighted
Relative representations for cognitive graphs 19
Fig.6. Visual schematic of the computation of a single entry of the output of (A)
the projection of input x to output y as in the Locally Linear Coordinates (LLC)
n n
mapping procedure; (B) the reconstruction of a latent embedding eB in model B’s
n
embeddingspacegiveninputx tomodelA.Thegroupingsinbracketsin(A)illustrate
n
theconcatenationsofvectorembeddings(scaledbyresponsibilitytermsr )inu ,and
nk n
ofprojectioncolumnsinl .1 in(B)denotesarowofk1s(wherekinthiscasedenotes
j k
the number of anchors, i.e. is set to |A|). Each entry in the column vector (cid:2) EB(cid:3)T is
A j
the jth dimension of one of model B’s anchor embeddings.
mixture of multiple “views” of the input. Similarly, since the rows of EB are
A
anchor embeddings in the output space, its columns j act precisely as do the
columnsofL,i.e.ascolumnsinaprojectionmatrix,sothatσ[rA]·EB outputs
i Aj
dimension j of the reconstructed target embedding eB.
i
There is at least one important difference between LLC and our procedure:
in LLC each expert uses an internal transform to generate an input-dependent
embedding, which is then scaled by its responsibility term, which also depends
on the input. Reconstruction via relative representations instead employs fixed
stored embeddings, so that each “expert” contributes a scalar value rather than
an embedding vector to the final output. However, the expression of LLC in
terms of a linear index demonstrates that this makes no essential difference
mathematically (conceptually, these scalar “votes” are 1D vectors; cf. Figure 6).
Thepointisnotthatthesetwoalgorithmsaredoingpreciselythesamething
(they are not, as LLC aims to align multiple embedding spaces by deriving a
mapping to a distinct common space, while our approach aims to recover the
contents of one embedding space from another). The use of LLC to reconstruct
input data X from its “global” embedding Y as in [22] is quite closely related
to our procedure, however, and at this level of abstraction the approaches may
be regarded as the same, with a difference in the nature of the “experts” used in
the mixture model and the attendant multiple “views” of the data. The relative
representation reconstruction procedure, while presumably not as expressive,
may compensate to some extent for the use of scalar “embeddings” by using a
largenumberof“experts”,andhasthevirtueofeschewingtheneedforamixture
model to assign responsibilities, or indeed for multiple intermediate embedding
models, to perform such a mapping.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Relative representations for cognitive graphs"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
