=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Home Run: Finding Your Way Home by Imagining Trajectories
Citation Key: tinguy2022home
Authors: Daria de Tinguy, Pietro Mazzaglia, Tim Verbelen

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2022

Key Terms: finding, your, environment, navigate, concept, trajectories, level, find, home, ghent

=== FULL PAPER TEXT ===

Home Run: Finding Your Way Home by
Imagining Trajectories
Daria de Tinguy1, Pietro Mazzaglia1, Tim Verbelen1, and Bart Dhoedt1
IDLab, Department of Information Technology
Ghent University - imec
Technologiepark-Zwijnaarde 126, B-9052 Ghent, Belgium
firstname.lastname@ugent.be
Abstract. Whenstudyingunconstrainedbehaviorandallowingmiceto
leave their cage to navigate a complex labyrinth, the mice exhibit for-
agingbehaviorinthelabyrinthsearchingforrewards,returningtotheir
homecagenowandthen,e.g.todrink.Surprisingly,whenexecutingsuch
a“homerun”,themicedonotfollowtheexactreversepath,infact,the
entrypathandhomepathhaveverylittleoverlap.Recentworkproposed
a hierarchical active inference model for navigation, where the low level
modelmakesinferencesabouthiddenstatesandposesthatexplainsen-
sory inputs, whereas the high level model makes inferences about mov-
ing between locations, effectively building a map of the environment.
However, using this “map” for planning, only allows the agent to find
trajectoriesthatitpreviouslyexplored,farfromtheobservedmice’sbe-
haviour.Inthispaper,weexplorewaysofincorporatingbefore-unvisited
pathsintheplanningalgorithm,byusingthelowlevelgenerativemodel
to imagine potential, yet undiscovered paths. We demonstrate a proof
of concept in a grid-world environment, showing how an agent can ac-
curately predict a new, shorter path in the map leading to its starting
point, using a generative model learnt from pixel-based observations.
Keywords: RobotNavigation·ActiveInference·FreeEnergyPrinciple
· Deep Learning.
1 Introduction
Humans rely on an internal representation of the environment to navigate, i.e.
they do not require precise geometric coordinates or complete mappings of the
environment; a few landmarks along the way and approximate directions are
enough to find our way back home [1]. This reflects the concept of a “cognitive
map” as introduced by Tolman [2], and matches the discovery of specific place
cells firing in the rodent hippocampus depending on the animal position [3] and
our representation of space [1].
Recently,Çataletal.[4]showedhowsuchmapping,localisationandpathin-
tegrationcannaturallyemergefromahierarchicalactiveinference(AIF)scheme
and are also compatible with the functions of the hippocampus and entorhinal
2202
guA
91
]GL.sc[
1v41901.8022:viXra
2 D. de Tinguy & al.
cortex[5].Thiswasimplementedonarealrobottoeffectivelybuildamapofits
environment, which could then be used to plan its way using previously visited
locations [6].
However, while investigating the exploratory behaviour of mice in a maze,
where mice were left free to leave their home to run and explore, a peculiar
observation was made. When the mice decided to return to their home location,
insteadofre-tracingtheirwayback,themicewereseentakingfullynew,shorter,
paths directly returning them home [7].
Onthecontrary,whengiventheobjectivetoreachahomelocation,thehier-
archical active inference model, as proposed by [4,6], can only navigate between
known nodes of the map, unable to extrapolate possible new paths without first
exploring the environment. To address this issue, we propose to expand the
high level map representation using the expected free energy of previously un-
explored transitions, by exploiting the learned low-level environment model. In
other worlds, we enlarge the projection capabilities of architecture [6] to unex-
plored paths.
IntheremainderofthispaperwewillfirstreviewthehierarchicalAIFmodel
[4], then explain how we address planning with previously unvisited paths by
imaginingnoveltrajectorieswithinthemodel.Asaproofofconcept,wedemon-
strate the mechanism on a Minigrid environment with a four-rooms setup. We
conclude by discussing our results, the current limitations and what is left to
improve upon the current results.
2 Navigation as hierarchical active inference
Theactiveinferenceframeworkreliesuponthenotionthatintelligentagentshave
an internal (generative) model optimising beliefs (i.e. probability distributions
over states), explaining the causes of external observations. By minimising the
surprise or prediction error, i.e, free energy (FE), agents can both update their
model as well as infer actions that yield preferred outcomes [8,9].
In the context of navigation, Çatal et al. [4] introduced a hierarchical active
inferencemodel,wheretheagentreasonsabouttheenvironmentontwodifferent
levels. On the low level, the agent integrates perception and pose, whereas on
the high level the agent builds a more coarse grained, topological map. This is
depicted in Figure 1.
The low level, depicted in blue, comprises a sequence of low-level action
commands a and sensor observations o , which are generated by hidden state
t t
variables s and p . Here s encodes learnable features that give rise to sensory
t t t
outcomes, whereas p encodes the agent’s pose in terms of its position and ori-
t
entation. The low level transition model p(s |s ,p ,a ) and likelihood model
t+1 t t t
p(o |s ) are jointly learnt from data using deep neural networks [10], whereas
t t
the pose transition model p(p |s ,p ,a ) is instantiated using a continuous
t+1 t t t
attractor network similar to [11].
At the high level, in red in the Figure, the agent reasons over more coarse
grained sequences of locations l , where it can execute a move m that gives
τ τ
Home Run: Finding Your Way Home by Imagining Trajectories 3
Fig.1. Navigation as a hierarchical generative model for active inference [4]. At the
lowerlevel,highlightedinblue,themodelentertainsbeliefsabouthiddenstatess and
t
p ,representinghiddencausesoftheobservationandtheposeatthecurrenttimestep
t
trespectively.Thehiddenstatesgiverisetoobservationso ,whereasactionsa impact
t t
futurestates.Atthehigherlevel,highlightedinred,theagentreasonsaboutlocations
l.Thenextlocationl isdeterminedbyexecutingamovem .Notethatthehigher
τ+1 τ
level operates on a coarser timescale. Grey shaded nodes are considered observed.
rise to a novel location l . In practice, this boils down to representing the
τ+1
environment as a graph-based map, where locations l are represented by nodes
τ
in the graph, whereas potential moves m are links between those nodes. Note
τ
thatasingletimestepatthehigherlevel,i.e.goingfromτ toτ+1,cancomprise
multiple time steps on the lower level. This enables the agent to first ‘think’ far
ahead in the future on the higher level.
To generate motion, the agent minimizes expected free energy (EFE) under
this hierarchical generative model. To reach a preferred outcome, the agent first
plans a sequence of moves that are expected to bring the agent to a location
rendering the preferred outcome highly plausible, after which it can infer the
actionsequencethatbringstheagentclosertothefirstlocationinthatsequence.
For a more elaborate description of the generative model, the (expected) free
energy minimisation and implementation, we refer to [4].
3 Imagining unseen trajectories
As discussed in [4], minimising expected free energy under such a hierarchical
model induces desired behaviour for navigation. In the absence of a preferred
outcome, an epistemic term in the EFE will prevail, encouraging the agent to
explore actions that yield information on novel (hidden) states, effectively ex-
panding the map while doing so. In the presence of a preferred state, the agent
will exploit the map representation to plan the shortest (known) route towards
the objective. However, crucially, the planning is restricted to previously vis-
ited locations in the map. This is not consistent with the behaviour observed in
mice [7], as these, apparently, can exploit new paths even when engaging in a
goal-directed run towards their home.
4 D. de Tinguy & al.
In order to address this issue, we hypothesize that the agent not only con-
siders previously visited links and locations in the map during planning, but
also imagines potential novel links. A potential link from a start location l to
A
a destination location l is hence scored by the minimum EFE over all plans
B
π (i.e. a sequence of actions) generating such a trajectory under the (low level)
generative model, i.e.:
H
(cid:88) (cid:2) (cid:3)
G(l ,l )=min D Q(s ,p |π)Q(s |l )(cid:107)Q(s ,p |l )
A B KL t+k t+k t A t+H t+H B
π
k=1(cid:124) (cid:123)(cid:122) (cid:125)
probabilityreachinglB fromlA (1)
+E (cid:2) H(P(o |s )) (cid:3) .
Q(st+k) t+k t+k
(cid:124) (cid:123)(cid:122) (cid:125)
observationambiguity
ThefirsttermisaKLdivergencebetweentheexpectedstatestovisitstarting
atlocationl andexecutingplanπ,andthestatedistributionexpectedatloca-
A
tion l . The second term penalizes paths that are expected to yield ambiguous
B
observations.
WecannowuseG(l ,l )toweigheachmovebetweentwocloselocations(the
A B
number of path grows exponentially the further the objective is), even through
ways not explored before, and plan for the optimal trajectory towards a goal
destination. In the next section, we work out a practical example using a grid-
world environment.
4 Experiments
4.1 MiniGrid setup
TheexperimentswererealisedinaMiniGridenvironment[12]of2×2upto5×5
rooms, of sizes going from 4 to 7 tiles and having a random floor color chosen
among6options:red,green,blue,purple,yellowandgrey.Roomsareconnected
by a single open tile, randomly spawned in the wall. The agent has 3 possible
actions at each time step: move one tile forward, turn 90 degrees left or turn 90
degrees right. It can’t see through walls and can only venture into an open grid
space.Notethatthewallblockingvisionisnotreallyrealisticandtheagentcan
see the whole room if there is an open door in its field of view, thus even if part
of the room should be masked by a wall (eg. Fig 2C raw observation). It can see
aheadandaroundinawindowof7×7tiles,includingitsownoccupiedtile.The
observation the agent receives is a pixel rendering in RGB of shape 3×56×56.
4.2 Model training and map building
Our hierarchical generative model was set up in similar fashion as [4]. To train
the lower level of the generative model, which consists of deep neural networks,
we let an agent randomly forage the MiniGrid environments, and train those
Home Run: Finding Your Way Home by Imagining Trajectories 5
A)
B)
C)
Fig.2. MiniGrid test maze and associated figures, A) An example of the maze with
a reachable goal (door open allowing shortcut) and the agent path toward a home-
run’sstartingpoint,thetransparentgreyboxcorrespondtotheagent’sfieldofviewat
the starting position. B) The topological map of the path executed in A as generated
by the high level of our generative model, C) The currently observed RGB image as
reconstructedbytheagent’smodelattheendofpathandtheviewatthedesiredgoal
position.
end to end by minimising the free energy on those sequences. Additional model
details and training parameters can be found in Appendix A.
The high level map is also built using the same procedure as [4]. However,
since we are dealing with a grid-world, distinct places in the grid typically yield
distinct location nodes in the map, unless these are near and actually yield
identical observations. Also, we found that predicting the effect of turning left
or right was harder for neural networks to predict, yielding a higher surprise
signal. However, despite these limitations, we can still demonstrate the main
contribution of this paper.
4.3 Home run
Inspired by the mice navigation in [7], we test the following setup in which the
agent first explores a maze, and at some point is provided with a preference of
returningtothestartlocation.Figure2showsanexampleofatestenvironment
andassociatedtrajectoriesrealisedbytheagent.Atthefinallocation,theagent
is instructed to go back home, provided by the goal observation in Fig. 2C.
Fig. 2B illustrates the map generated by the hierarchical model.
First, we test whether the agent is able to infer whether it can reach the
starting node in the experience map from the current location. We do so by
imagining all possible plans π, and evaluating the expected free energy of each
plan over an average of N = 3 samples from the model. Figure 3 shows the
6 D. de Tinguy & al.
Fig.3.Lowestexpectedfreeenergyofeachendpositionafter5steps.Therightfigure
shows the agent at position (0,0) facing the goal at position (0,5), as represented in
Figure2Ai).Intheleftfigure,thedoorisopen,thereforethegoalisreachable,onthe
right figure the door is closed, the goal cannot be reached in 5 steps.
EFE for all reachable locations in a 5 steps planning horizon. It is clear that
in case the door is open, the agent expects the lowest free energy when moving
forward through the door, expecting to reach the start node in the map. In case
the path is obstructed (the door as in 2A, allowing a shortcut, is closed), it can
stillimaginegoingforward5steps,butthiswillresultintheagentgettingstuck
against the wall, which it correctly imagines and reflects on the EFE.
However, the prior model learnt by the agent is far from perfect. When
inspecting various imagined rollouts of the model, as shown in Figure 4, we
see that the model has trouble encoding and remembering the exact position of
the door, i.e. predicting the agent getting stuck (top) or incorrect room colours
and size (bottom). While not problematic in our limited proof of concept, also
due to the fact that the EFE is averaged over multiple samples, this shows that
the effectiveness of the agent will be largely dependent on the accuracy of the
model.
To test the behaviour in a more general setting, we set multiple home-run
scenarios, where the agent’s end position is d = 5,6,7,9 steps away from the
start location. For each d, we sample at least 20 runs over 4 novel 2×2 rooms
environment, with different room sizes and colours, similar to the train set, in
which 10 have an open door between the start and goal, and 10 have not. We
count the average number of steps required by the agent to get back home,
and compare against two baseline approaches. First is the Greedy algorithm,
inspired by [13], in which the agent greedily navigates in the direction of the
goal location, and follows obstacles in the known path direction when bumping
into one. Second is a TraceBack approach, which retraces all its steps back
home, similar to Ariadne’s thread. Our approach uses the EFE with a planning
horizon of d to decide whether or not the home node is reachable based on a
fixedthreshold,andfallsbacktoplanninginthehierarchicalmodel,whichboils
down to a TraceBack strategy.
Home Run: Finding Your Way Home by Imagining Trajectories 7
open closed
d Greedy TraceBack Ours Greedy TraceBack Ours
5 5 25 6.5 29.5 25 25
6 6 31 6 41 31 31
7 7 27 11.5 31.5 27 27
9 9 36 23.7 46 36 36
Table1.Homerunstrategiesandtheresultingnumberofsteps,fordifferentdistances
dtohome,andopenversusclosedscenarios.Forsmalldourmodelcorrectlyimagines
the outcome. For d=9 the agent infers an open door about 27% of the time.
Incaseofsmalld(≤6),ourapproachsuccessfullyidentifieswhetherthegoal
is reachable or not, even when the agent is not facing it, which results in a
similarperformanceforaGreedyapproachinthe‘open’case,andarevertingto
TraceBackinthe‘closed’case.Thereisbeenonlyoneexceptioninourtest-bench
at 5steps range issued by a reconstruction error on all samples (the occurrence
probabilityis0.04%ashavingasamplewronglyestimatingthedoorpositionat
5steps is 33%). For d=7 our model misses some of the shortcut opportunities,
as the model’s imagination becomes more prone to errors for longer planning
horizons. For d=9, the rooms are larger and the wall separating the two rooms
is actually not visible to the agent. In this regime, we found the agent imagines
about27%ofthetimethatitwillbeopen,andtakesthegambletomovetowards
the wall, immediately returning on its path if the wall is obstructed.
Fig.4.Threeimaginedtrajectoriesofa5-stepsprojectionmovingforward.Thetrained
model is not perfectly predicting the future, only the middle sequence predicts the
correct dynamics.
8 D. de Tinguy & al.
5 Discussion
Our experiments show that using the EFE of imagined paths can yield more
optimal,goal-directedbehaviour.Indeed,ouragentisabletoimagineandexploit
shortcutswhenplanningitswayhome.However,ourcurrentexperimentalsetup
isstillpreliminaryandweplantofurtherexpanduponthisconcept.Forinstance
wecurrentlyarbitrarilysetthepointatwhichtheagentdecidetohome-run.Ina
realexperiment,themicelikelydecidetogohomeduetosomeinternalstimulus,
e.g., when they get thirsty and head back home where water is available. We
could further develop the experimental setup to incorporate such features and
do a more extensive evaluation.
One challenge of using the Minigrid environment as an experimental setup
[12]istheuseoftopviewvisualobservations.Usingapixel-wiseerrorforlearning
thelow-levelperceptionmodelcanbeproblematic,asforexamplethepixel-wise
error between a closed versus an open tile in the wall is small in absolute value,
and hence it’s difficult to learn for the model, as illustrated in Figure 4. A
potential approach to mitigate this is to use a contrastive objective instead, as
proposed by [14].
Another important limitation of the current model is that it depends on
the effective planning horizon of the lowest level model to imagine shortcuts.
Especially in the Minigrid environment, imagining the next observation for a 90
degreeturnischallenging,asitrequiresaformofmemoryoftheroomlayoutto
correctlypredictthenovelobservation.Thisseverelylimitstheplanninghorizon
of our current models. A potential direction of future work in this regard is to
learn a better location, state and pose mapping. For instance, instead of simply
associating locations with a certain state and pose, conditioning the transition
model on a learnt location descriptor might allow the agent to learn and encode
the shape of a complete room in a location node.
Other approaches have been proposed to address the navigation towards a
goal by the shortest way possible in a biologically plausible way. For instance,
Erdem et al. [15] reproduced the pose and place-cell principle of the rat’s hip-
pocampus with spiking neural networks and use a dense reward signal to drive
goal-directed behaviour, with more reward given the closer the agent gets to
the goal. Hence, the path with the highest reward is sought, and trajectories on
which obstacles are detected are discarded. In Vegard et al. [13], the process is
alsobio-inspired,basedonthecombinationofgridcell-basedvectorandtopolog-
ical navigation. The objective is now explicitly represented as a target position
in space, which is reached by vector navigation mechanisms with local obstacle
avoidance mediated by border cells and place cells. Both alternatives also adopt
topologicalmapsandpathintegrationinordertoreachtheirobjective.However,
both exhibit more greedy and reactive behaviour, whereas our model is able to
exploit the lower level perception model to already predict potential obstacles
upfront, before bumping into those.
Home Run: Finding Your Way Home by Imagining Trajectories 9
6 Conclusion
In this paper we have proposed how a hierarchical active inference model can
be used to improve planning by predicting novel, previously unvisited paths.
We demonstrated a proof of concept using a generative model learnt from pixel
based observations in a grid-world environment.
As future work we envision a more extensive evaluation, comparing shallow
versusdeephierarchicalgenerativemodelsinnavigationperformance.Moreover,
we aim to address several of the difficulties of our current perception model, i.e.
thelimitationsofpixel-wisepredictionerrors,thelimitedplanninghorizon,anda
more expressive representation for locations in the high level model. Ultimately,
ourgoalistodeploythisonareal-worldrobot,autonomouslyexploring,planning
and navigating in its environment.
Acknowledgment
ThisresearchreceivedfundingfromtheFlemishGovernmentunderthe“Onder-
zoeksprogramma Artificiële Intelligentie (AI) Vlaanderen” programme.
References
1. M. Peer, I. K. Brunec, N. S. Newcombe, and R. A. Epstein, “Structuring
knowledge with cognitive maps and cognitive graphs,” Trends in Cognitive
Sciences, vol. 25, no. 1, pp. 37–54, 2021. [Online]. Available: https:
//www.sciencedirect.com/science/article/pii/S1364661320302503
2. E. C. Tolman, “Cognitive maps in rats and men.” Psychological review, vol. 55 4,
pp. 189–208, 1948.
3. M.Milford,G.Wyeth,andD.Prasser,“Ratslam:ahippocampalmodelforsimul-
taneouslocalizationandmapping,” inIEEE International Conference on Robotics
and Automation, 2004. Proceedings. ICRA ’04. 2004, vol. 1, 2004, pp. 403–408
Vol.1.
4. O. Çatal, T. Verbelen, T. Van de Maele, B. Dhoedt, and A. Safron, “Robot
navigation as hierarchical active inference,” Neural Networks, vol. 142, pp. 192–
204, 2021. [Online]. Available: https://www.sciencedirect.com/science/article/pii/
S0893608021002021
5. A. Safron, O. Çatal, and T. Verbelen, “Generalized simultaneous localization
and mapping (g-SLAM) as unification framework for natural and artificial
intelligences: towards reverse engineering the hippocampal/entorhinal system
and principles of high-level cognition,” Oct. 2021. [Online]. Available: https:
//doi.org/10.31234/osf.io/tdw82
6. O. Çatal, W. Jansen, T. Verbelen, B. Dhoedt, and J. Steckel, “Latentslam:
unsupervised multi-sensor representation learning for localization and mapping,”
CoRR,vol.abs/2105.03265,2021.[Online].Available:https://arxiv.org/abs/2105.
03265
7. M. Rosenberg, T. Zhang, P. Perona, and M. Meister, “Mice in a labyrinth show
rapidlearning,suddeninsight,andefficientexploration,” eLife,vol.10,p.e66175,
jul 2021. [Online]. Available: https://doi.org/10.7554/eLife.66175
10 D. de Tinguy & al.
8. K. Friston, T. FitzGerald, F. Rigoli, P. Schwartenbeck, J. O. Doherty, and
G.Pezzulo,“Activeinferenceandlearning,” Neuroscience&BiobehavioralReviews,
vol. 68, pp. 862–879, 2016. [Online]. Available: https://www.sciencedirect.com/
science/article/pii/S0149763416301336
9. R.KaplanandK.Friston,“Planningandnavigationasactiveinference,” 122017.
10. O. Çatal, S. Wauthier, C. De Boom, T. Verbelen, and B. Dhoedt,
“Learning generative state space models for active inference,” Frontiers
in Computational Neuroscience, vol. 14, 2020. [Online]. Available: https:
//www.frontiersin.org/article/10.3389/fncom.2020.574372
11. M. Milford, A. Jacobson, Z. Chen, and G. Wyeth, RatSLAM: Using
Models of Rodent Hippocampus for Robot Navigation and Beyond. Cham:
Springer International Publishing, 2016, pp. 467–485. [Online]. Available:
https://doi.org/10.1007/978-3-319-28872-7_27
12. M.Chevalier-Boisvert,L.Willems,andS.Pal,“Minimalisticgridworldenvironment
for openai gym,” https://github.com/maximecb/gym-minigrid, 2018.
13. V. Edvardsen, A. Bicanski, and N. Burgess, “Navigating with grid and place cells
in cluttered environments,” Hippocampus, vol. 30, pp. 220 – 232, 2019.
14. P. Mazzaglia, T. Verbelen, and B. Dhoedt, “Contrastive active inference,”
in Advances in Neural Information Processing Systems, A. Beygelzimer,
Y. Dauphin, P. Liang, and J. W. Vaughan, Eds., 2021. [Online]. Available:
https://openreview.net/forum?id=5t5FPwzE6mq
15. U. Erdem and M. Hasselmo, “A goal-directed spatial navigation model using for-
wardplanningbasedongridcells,” The European journal of neuroscience,vol.35,
pp. 916–31, 03 2012.
16. D.P.KingmaandJ.Ba,“Adam:Amethodforstochasticoptimization.” [Online].
Available: https://arxiv.org/abs/1412.6980
Home Run: Finding Your Way Home by Imagining Trajectories 11
A Model details and training
Inthisappendix,weprovidesomeadditionaldetailsonthetrainingdata,model
parameters, training procedure and building the hierarchical map.
A.1 Training data
To optimize the neural network models a dataset composed of sequences of
action-observation pairs was collected by human demonstrations of interaction
with the environment. The agent was made to move around from rooms to
room, circle around and turn randomly. About 12000 steps were recorded in 39
randomly created environments having different room size, number of rooms,
open door emplacements and floor colors, as well as the agent having a random
starting pose and orientation. 2/3 of the data were used for training and 1/3 for
validation. Then a fully novel environment was used for testing.
A.2 Model parameters
The low level perception model is based on the architecture of [10], and is com-
posed of 3 neural networks that we call: prior, posterior and likelihood.
The prior neural network consists in a LSTM layer followed with a varia-
tional layer giving out a distribution (i.e mean and std).
The posterior model first consists of a convolutional network to compress
sensordata.Thisdataisthenconcatenatedwiththehotencodedactionandthe
previous state, all of that is then processed by a fully connected neural network
coupled with a variational layer to obtain a distribution.
The likelihood model performs the inverse of the convolutional part of the
posterior, generating an image out of a given state sample.
The detailed parameters are listed in Table 2.
A.3 Training the model
Thelowlevelperceptionpipelinewastrainedendtoendontimesequencesof10
steps using stochastic gradient descent with the minimization of the free energy
loss function [10]:
(cid:88)
FE = D [Q(s |s ,a ,o )||P(s |s ,a )]−E [logP(o |s )]
KL t t−1 t−1 t t t−1 t−1 Q(st) t t
t
The loss consists of a negative log likelihood part penalizing the error on
reconstruction, and a KL-divergence between the posterior and the prior distri-
butions on a training sequence. We trained the model for 300 epochs using the
ADAM optimizer [16] with a learning rate of 1·10–4.
12 D. de Tinguy & al.
Layer Neurons/FiltersStride
Concatenation
Prior LSTM 200
Linear 2*30
Convolutional 16 2
Convolutional 32 2
Convolutional 64 2
Convolutional 128 2
Posterior
Convolutional 256 2
Concatenation
Linear 200
Linear 2*30
Linear 200
Linear 256*2*2
Upsample
Convolutional 128 1
Upsample
Convolutional 64 1
Likelihood
Upsample
Convolutional 32 1
Upsample
Convolutional 16 1
Upsample
Convolutional 3 1
Table 2. Models parameters
A.4 Building the map
Thehighlevelmodelisimplementedasatopologicalgraphrepresentation,link-
ingposeandhiddenstaterepresentationtoalocationinthemap.Herewereuse
theLatentSLAMimplementation[6]consistingofposecells,localviewcellsand
an experience map.
TheposecellsareimplementedasaContinuousAttractorNetwork(CAN),
representing the local position x, y and heading θ of the agent. Pose cells rep-
resent a finite area, therefore the firing fields of a single grid cell correspond to
several periodic spatial locations.
The local view cells are organised as a list of cell, each cell containing a
hidden state representing an observation, the pose cell excited position, and the
map’s experience node linked to this view. After each motion, the encountered
scene is compared to all previous cells observation by calculating the cosine
distance between hidden state features. If the distance is smaller than a given
threshold, then the cell corresponding to this view is activated, else a new cell
is created.
The experience map contains the experience of the topological map. It
gives an estimate of the agent global pose in the environment and link the pose
cell position with the local view cell active at this moment. If those elements do
notmatchwithanyexistingnodeofthemap,anewoneiscreatedandlinkedto
thepreviousexperience,elseacloseloopisoperatedandtheexistingexperiences
are linked together.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Home Run: Finding Your Way Home by Imagining Trajectories"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
