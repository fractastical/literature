=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Online system identification in a Duffing oscillator by free energy minimisation
Citation Key: kouw2020online
Authors: Wouter M Kouw

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2020

Key Terms: system, energy, duffing, differential, factor, oscillator, identification, free, model, online

=== FULL PAPER TEXT ===

Online system identification in a Duffing
oscillator by free energy minimisation
Wouter M. Kouw
Bayesian Intelligent Autonomous Systems lab
TU Eindhoven, Eindhoven, 5612AP Netherlands
w.m.kouw@tue.nl
Abstract. Onlinesystemidentificationistheestimationofparameters
ofadynamicalsystem,suchasmassorfrictioncoefficients,foreachmea-
surementoftheinputandoutputsignals.Here,thenonlinearstochastic
differential equation of a Duffing oscillator is cast to a generative model
anddynamicalparametersareinferredusingvariationalmessagepassing
onafactorgraphofthemodel.Theapproachisvalidatedwithanexper-
imentondatafromanelectronicimplementationofaDuffingoscillator.
The proposed inference procedure performs as well as offline prediction
error minimisation in a state-of-the-art nonlinear model.
Keywords: Online system identification · Duffing oscillator · Free en-
ergy minimisation · Variational message passing · Forney factor graphs
1 Introduction
Natural agents are believed to develop an internal model of their motor system
bygeneratingactionsinmusclesandobservinglimbmovements[11].Ithasbeen
suggestedthatformingthisinternalmodelisanalogoustoaformofonlinesystem
identification [24]. System identification, i.e. estimating dynamical parameters
from observed input and output signals, has a rich history in engineering. But
there might still be much to gain from considering biologically-plausible proce-
dures. Here, I explore online system identification using a leading theory of how
brains process information: free energy minimisation [8,3].
Totestfreeenergyminimisationforuseinengineeringapplications,Iconsider
aspecificbenchmark1 problemcalledaDuffingoscillator.Duffingoscillatorsare
relatively well-behaved nonlinear differential equations, making them excellent
toy problems for methodological research. Its differential equation is cast to a
generative model, with a corresponding factor graph. The factor graph admits
a recursive parameter estimation procedure through message passing [14,12].
Specifically, variational message passing minimises free energy [5,13,18]. Here,
I infer the parameters of a Duffing oscillator using online variational message
passing. Experiments show that it performs as well as a nonlinear ARX model
with parameters trained offline using prediction error minimisation [2].
1 http://nonlinearbenchmark.org/
0202
peS
2
]GL.sc[
1v54800.9002:viXra
2 W.M. Kouw
2 System
Consider a rigid frame with two prongs facing rightwards (see Figure 1 left). A
steel beam is attached to the top prong. If the frame is driven by a periodic
forcing term, the beam will displace horizontally as a driven damped harmonic
oscillator. Two magnets are attached to the bottom prong, with the steel beam
suspended in between. These act as a nonlinear feedback term on the beam’s
position, attracting or repelling it as it gets closer [15].
Fig.1: (Left) Example of a physical implementation of a Duffing oscillator.
(Right) Example of input and output signals.
Let y(t) be the observed displacement, x(t) the true displacement, and u(t)
theobserveddrivingforce.Thepositionofthebeamisdescribedasfollows[25]:
d2x(t) dx(t)
m +c +ax(t)+bx3(t)= u(t)+w(t) (1a)
dt2 dt
y(t)= x(t)+v(t) , (1b)
where m is mass, c is damping, a the linear and b the nonlinear spring stiffness
coefficient.Boththestatetransitionaswellastheobservationlikelihoodcontain
noise terms, which are assumed to be Gaussian distributed: w(t) ∼ N(0,τ−1)
(process noise) and v(t) ∼ N(0,ξ−1) (measurement noise). The challenge is to
estimatem,c,a,b,τ andξ suchthattheoutputofthesystemcanbepredicted
as accurately as possible.
3 Identification
First, I discretise the state transition of Equation 1 using a central difference
for the second derivative and a forward difference for the first derivative. Re-
arranging to form an expression in terms of x yields:
t+1
2m+cδ−aδ2 −bδ2 −m δ2
x = x + x3+ x + (u +w ), (2)
t+1 m+cδ t m+cδ t m+cδ t−1 m+cδ t t
Online system identification by free energy minimisation 3
where δ is the sample time step. Secondly, to ease inference at a later stage, I
perform the following variable substitutions:
2m+cδ−aδ2 −bδ2 −m δ2 τ(m+cδ)2
θ = , θ = , θ = , η= , γ= , (3)
1 m+cδ 2 m+cδ 3 m+cδ m+cδ δ4
where the square in the numerator for γ stems from absorbing the coefficient
into the noise term (V[ηw ] = η2V[w ]). Note that the mapping between φ =
t t
(m,c,a,b,τ)andψ =(θ ,θ ,θ ,η,γ)canbeinvertedtorecoverpointestimates:
1 2 3
−θ δ2 (1+θ )δ 1−θ −θ −θ
m= 3 , c= 3 , a= 1 3 , b= 2 , τ =γη2. (4)
η η η η
Thirdly, the state transition can be cast to a multivariate first-order form:
(cid:20) (cid:21) (cid:20) (cid:21)(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
x 0 0 x 1 1 1
t+1 = t + g(θ,z )+ ηu + w˜ , (5)
x 1 0 x 0 t−1 0 t 0 t
t t−1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) (cid:124)(cid:123)(cid:122)(cid:125)
zt S zt−1 s
where g(θ,z )=θ x +θ x3+θ x and w˜ ∼N(0,γ−1). The system is now
t−1 1 t 2 t 3 t−1 t
a nonlinear autoregressive process. Lastly, integrating out w˜ and v produces a
t t
Gaussian state transition and a Gaussian likelihood, respectively:
z ∼ N(f(θ,z ,η,u ),V) (6a)
t t−1 t
y ∼ N(s(cid:62)z ,ξ−1), (6b)
t t
where f(θ,z ,η,u ) = Sz +sg(θ,z )+sηu and V = (cid:2) γ−1 0 ;0(cid:15) (cid:3) . The
t−1 t t−1 t−1 t
number (cid:15) represents a small noise injection to stabilise inference [6].
To complete the generative model description, priors must be defined. Mass
m and process precision τ are known to be strictly positive parameters, while
the damping and stiffness coefficients can be both positive and negative. By
examining the variable substitutions, it can be seen that θ , θ , θ and η can be
1 2 3
both positive and negative, but γ can only be positive. As such, the following
parametric forms can be chosen for the priors:
θ ∼N(m0,V0), η ∼N(m0,v0), γ ∼Γ(a0,b0), ξ ∼Γ(a0,b0). (7)
θ θ η η γ γ ξ ξ
3.1 Free energy minimisation
Given the generative model, a free energy functional with a recognition model q
can be formed as follows:
(cid:90)(cid:90) q(ψ,z)
−logp(y,u)≤ q(ψ,z) dzdψ =F[q] (8)
p(y,u,z,ψ)
where z = (z ,...,z ), y = (y ,...,y ) and u = (u ,...,u ). I assume the
1 T 1 T 1 T
states factor over time and that the parameters are largely independent:
T
(cid:89)
q(ψ,z)=q(θ)q(η)q(γ)q(ξ) q(z ). (9)
t
t=1
4 W.M. Kouw
AllrecognitiondensitiesareGaussiandistributed,exceptforq(γ)andq(ξ),which
areGammadistributed.Infreeenergyminimisation,theparametersoftherecog-
nition distributions depend on each other and are iteratively updated.
3.2 Factor graphs and message passing
In online system identification, parameter estimates should be updated at each
time-step.Thatputstimeconstraintsontheinferenceprocedure.Messagepass-
ing is an ideal inference procedure due to its efficiency in factorised generative
models [12]. Figure 2 is a graphical representation of the generative model, with
nodes for factors and edges for variables. Square nodes with Greek letters rep-
resent stochastic operations while · and = represent deterministic operations.
The node marked ”NLARX” represents the state transition described in Equa-
tion 6a.
γ
Γ ... = ...
η
N ... = ...
N ... = θ 4 ↓↑ 8 ...
3 ↓↑ 7
2 ↓↑ 6
N ...
zt−1
NLARX ←
5
=
zt
...
→ → →
1 9 12
· s
ut ξ
Γ ... = 11 ...
← N
→
10
yt
Fig.2: Forney-style factor graph of the generative model of a Duffing oscillator.
Nodes represent conditional distributions and edges represent variables. Nodes
send messages to connected edges. When two messages on an edge collide, the
marginal belief q for the corresponding variable is updated. Each belief update
reduces free energy. By iterating message passing, free energy is minimised.
The terminal nodes on the left represent the initial priors for the states
and dynamical parameters. Inference starts when these nodes pass messages.
The subgraph - separated by columns of dots - represents the structure of a
single time step, recursively applied. Messages 1 , 2 , 3 , 4 and 10 represent
beliefs q from previous time-steps. Message 5 , arriving at the state transition
node, originates from the likelihood node attached to observation y . Messages
t
Online system identification by free energy minimisation 5
6 , 7 , 8 , 9 and 11 combine priors from previous time steps and likelihoods
ofobservations,andareusedtoupdatebeliefsq.Message 12 isthecurrentstate
belief and becomes message 1 in the next time step.
The graph actually contains more messages, such as those sent by equal-
ity nodes. I have hidden them to avoid complicating the figure. Their form has
been extensively described in the literature and can be looked up easily [12,14].
Modern message passing toolboxes, such as Infer.NET and ForneyLab.jl, auto-
maticallyincorporatethem.However,theNLARXnodeisnew.Itsmessagescan
be computed with2:
6 →− ν(θ)=exp (cid:16) E (cid:2) logN(f(θ,z ,η,u ),V) (cid:3)(cid:17) (10a)
q(zt)q(zt−1)q(η)q(γ) t−1 t
7 →− ν(η)=exp (cid:16) E (cid:2) logN(f(θ,z ,η,u ),V) (cid:3)(cid:17) (10b)
q(zt)q(zt−1)q(θ)q(γ) t−1 t
8 →− ν(γ)=exp (cid:16) E (cid:2) logN(f(θ,z ,η,u ),V) (cid:3)(cid:17) (10c)
q(zt)q(zt−1)q(θ)q(η) t−1 t
9 →− ν(z )=exp (cid:16) E (cid:2) logN(f(θ,z ,η,u ),V) (cid:3)(cid:17) , (10d)
t q(zt−1)q(θ)q(η)q(γ) t−1 t
where I use a first-order Taylor expansion to approximate the expected value of
the nonlinear autoregressive function g(θ,z ).
t−1
Loeligeretal.(2007)havewrittenanaccessibleintroductiononmessagepass-
inginfactorgraphs[14].Variationalmessagepassinginautoregressiveprocesses
has been described in detail as well [5,19].
4 Experiment
The Duffing oscillator has been implemented in an electronic system called Sil-
verbox [25]. It consists of T = 131702 samples, gathered with a sampling fre-
quency of 610.35 Hz. Figure 3 shows the time-series, plotted at every 80 time
steps. There are two regimes: the first 40000 samples are subject to a linearly
increasing amplitude in the input (left of the black line in Figure 3) and the
remaining samples are subject to a constant amplitude but contain only odd
harmonics(rightoftheblackline).Thesecondregimeisusedasatrainingdata
set, where both input and output data were given and parameters needed to
be inferred. The first regime is used as a validation data set, where the inferred
parameters are fixed and the model needs to make predictions for the output
signal.
I performed two experiments3: a 1-step ahead prediction error and a simula-
tionerrorsetting.IusedForneyLab.jl,withNLARXasacustomnode,torunthe
message passing inference procedure [4]. I call the model above FEM-NLARX,
forNonlinearLatentAutoregressivemodelwitheXogenousinputusingFreeEn-
ergyMinimisation.Iimplementedtwobaselines:thefirstisNLARXwithoutthe
nonlinearity (i.e. the nonlinear spring coefficient b = 0), dubbed FEM-LARX.
2 Derivations at https://github.com/biaslab/IWAI2020-onlinesysid
3 Experiment notebooks at https://github.com/biaslab/IWAI2020-onlinesysid
6 W.M. Kouw
Fig.3: Silverbox data set, sampled at every 80 time steps for visualisation. The
black line splits it into validation data (left) and training data (right).
The second is a standard NARX model, implemented using MATLAB’s System
Identification Toolbox. I modelled the static nonlinearity with a sigmoid net-
work of 4 units (in line with the 4 coefficients used by NLARX and LARX).
Parameters were inferred offline using Prediction Error Minimisation. Hence,
this baseline is called PEM-NARX.
I chose uninformative priors for the coefficients θ and η: Gaussians centred
at 1 with precisions of 0.1. The authors of Silverbox indicate that the signal-
to-noise ratio at measurement time was high [25]. I therefore chose informative
priors for the noise parameters: a0 =1e8 and a0 =1e3 (shape parameters) and
ξ γ
b0 =1e3 and b0 =1e1 (scale parameters).
ξ γ
4.1 1-step ahead prediction error
At each time-step in the validation data, the models were given the previous
output signal y ,y and the current input signal u and had to infer the
t−1 t−2 t
current output y . It is a relatively easy task, which is reflected in all three
t
models’ performance. The top row in Figure 4 shows the predictions of all three
models in purple and their squared error with respect to the true output signal
in black. The left column shows the offline NARX baseline (PEM-NARX), the
middle column the linear online latent autoregressive baseline (FEM-LARX)
and the right column the nonlinear online latent autoregressive model (FEM-
NLARX).Notethattheerrorsinthetoprowseemcompletelyflat.Thebottom
rowinthefigureplotstheerrorsonalog-scale.PEM-NARXhasameansquared
errorof5.831e-5,FEM-LARXoneof5.945e-5andFEM-NLARXoneof5.830e-5.
4.2 Simulation error
In this experiment, the models were not given the previous output signal, but
had to use their predictions from the previous time-step. This is a much harder
task, because errors will accumulate. The top row in Figure 5 again shows the
predictions of all three models (purple) and their squared error (black). It can
alreadybeseenthattheerrorsincreaseastheinputsignal’samplituderises.The
bottomrowplotstheerrorsonalog-scale.PEM-NARXhasameansquarederror
of 1.000e-3, FEM-LARX one of 1.002e-3 and FEM-NLARX one of 0.926e-3.
Online system identification by free energy minimisation 7
Fig.4: 1-step ahead prediction errors. (Left) Offline NARX model with sigmoid
net (PEM-NARX), (middle) online linear model (FEM-LARX) and (right) on-
line nonlinear model (FEM-NLARX). (Top) Predictions (purple) and squared
error (black). (Bottom) Squared prediction errors in log-scale.
5 Discussion
The experimental results seem to justify looking to nature for inspiration. Free
energy minimisation, in the form of variational message passing, seems a gener-
ally applicable and well-performing inference technique. The difficulties mostly
lie in deriving variational messages (i.e. Equations 10).
Improvementsintheproposedprocedurecouldbemadewitharicherapprox-
imation of the nonlinear autoregressive function (e.g. unscented transform) [20].
Alternatively, a hierarchy of latent Gaussian filters or autoregressive processes
could be used to obtain time-varying noise parameters or time-varying coeffi-
cients [22,19]. Furthermore, instead of discretising such that an auto-regressive
model is obtained, one could express the evolution of the states in generalised
coordinates.Lastly,black-boxmodelscouldbeexploredforfurtherperformance
improvements.
A natural next step is for an active inference agent to determine the control
signal regime (i.e. optimal design). Unfortunately, this is not straightforward:
the current formulation relies on variational free energy which does not produce
an epistemic term in the objective. The epistemic term is needed to encourage
exploration; i.e. try sub-optimal inputs to reduce uncertainty. To arrive at an
epistemic term, one would need to work with expected free energy [17]. But it
is unclear how expected free energy could be incorporated into factor graphs.
5.1 Related work
Online system identification procedures typically employ recursive least-squares
or maximum likelihood inference, with nonlinearities modelled by basis expan-
8 W.M. Kouw
Fig.5: Simulation errors. (Left) Offline NARX model with sigmoid net (PEM-
NARX),(middle)onlinelinearmodel(FEM-LARX)and(right)onlinenonlinear
model (FEM-NLARX). (Top) Predictions (purple) and squared error (black).
(Bottom) Squared prediction errors in log-scale.
sions or neural networks [16,23,7]. Online Bayesian identification procedures
come in two flavours: sequential Monte Carlo samplers [10,1] and online vari-
ationalBayes[26,9].Thisworkisnovelintheuseofvariationalmessagepassing
as an efficient implementation of online variational Bayes and its application to
a nonlinear autoregressive model.
6 Conclusion
Ihavepresentedafreeenergyminimisationprocedureforonlinesystemidentifi-
cation. Experimental results showed comparable performance to a state-of-the-
art nonlinear model with parameters estimated offline. This indicates that the
procedure performs well enough to be deployed in engineering applications.
Futureworkshouldtestvariationalmessagepassinginmorechallengingnon-
linear identification settings, such as a Wiener-Hammerstein benchmark [21].
Furthermore,problemswithtime-varyingdynamicalparameters,suchasarobotic
arm picking up objects with mass, would be interesting for their connection to
natural agents.
7 Acknowledgements
The author thanks Magnus Koudahl, Albert Podusenko and Thijs van de Laar
for insightful discussions and the reviewers for their constructive feedback.
Online system identification by free energy minimisation 9
References
1. Abdessalem, A.B., Dervilis, N., Wagg, D., Worden, K.: Identification of nonlinear
dynamical systems using approximate Bayesian computation based on a sequen-
tial Monte Carlo sampler. In: International Conference on Noise and Vibration
Engineering (2016)
2. Aguirre, L.A., Letellier, C.: Modeling nonlinear dynamics and chaos: a review.
Mathematical Problems in Engineering 2009 (2009)
3. Buckley, C.L., Kim, C.S., McGregor, S., Seth, A.K.: The free energy principle for
action and perception: A mathematical review. Journal of Mathematical Psychol-
ogy 81, 55–79 (2017)
4. Cox, M., van de Laar, T., de Vries, B.: Forneylab.jl: Fast and flexible automated
inference through message passing in Julia. In: International Conference on Prob-
abilistic Programming (2018)
5. Dauwels, J.: On variational message passing on factor graphs. In: IEEE Interna-
tional Symposium on Information Theory. pp. 2546–2550 (2007)
6. Dauwels, J., Eckford, A., Korl, S., Loeliger, H.A.: Expectation maximization as
messagepassing-PartI:PrinciplesandGaussianmessages.arXiv:0910.2832(2009)
7. Engel,Y.,Mannor,S.,Meir,R.:Thekernelrecursiveleast-squaresalgorithm.IEEE
Transactions on Signal Processing 52(8), 2275–2285 (2004)
8. Friston, K., Kilner, J., Harrison, L.: A free energy principle for the brain. Journal
of Physiology 100(1-3), 70–87 (2006)
9. Fujimoto, K., Satoh, A., Fukunaga, S.: System identification based on variational
Bayesmethodandtheinvarianceundercoordinatetransformations.In:IEEECon-
ferenceonDecisionandControlandEuropeanControlConference.pp.3882–3888
(2011)
10. Green,P.L.:Bayesiansystemidentificationofanonlineardynamicalsystemusing
anovelvariantofsimulatedannealing.MechanicalSystemsandSignalProcessing
52, 133–146 (2015)
11. de Klerk, C.C., Johnson, M.H., Heyes, C.M., Southgate, V.: Baby steps: Investi-
gating the development of perceptual–motor couplings in infancy. Developmental
Science 18(2), 270–280 (2015)
12. Korl, S.: A factor graph approach to signal modelling, system identification and
filtering. Ph.D. thesis, ETH Zurich (2005)
13. vandeLaar,T.,Cox,M.,Senoz,I.,Bocharov,I.,deVries,B.:Forneylab:atoolbox
for biologically plausible free energy minimization in dynamic neural models. In:
Conference on Complex Systems (2018)
14. Loeliger, H.A., Dauwels, J., Hu, J., Korl, S., Ping, L., Kschischang, F.R.: The
factor graph approach to model-based signal processing. Proceedings of the IEEE
95(6), 1295–1322 (2007)
15. Moon,F.,Holmes,P.J.:Amagnetoelasticstrangeattractor.JournalofSoundand
Vibration 65(2), 275–296 (1979)
16. Paleologu, C., Benesty, J., Ciochina, S.: A robust variable forgetting factor re-
cursive least-squares algorithm for system identification. IEEE Signal Processing
Letters 15, 597–600 (2008)
17. Parr, T., Friston, K.J.: Generalised free energy and active inference. Biological
Cybernetics 113(5-6), 495–513 (2019)
18. Parr, T., Markovic, D., Kiebel, S.J., Friston, K.J.: Neuronal message passing us-
ing mean-field, Bethe, and marginal approximations. Scientific reports 9(1), 1–18
(2019)
10 W.M. Kouw
19. Podusenko, A., Kouw, W.M., de Vries, B.: Online variational message passing in
hierarchical autoregressive models. In: IEEE International Symposium on Infor-
mation Theory. pp. 1343–1348 (2020)
20. S¨arkka¨, S.: Bayesian filtering and smoothing, vol. 3. Cambridge University Press
(2013)
21. Schoukens, M., No¨el, J.P.: Three benchmarks addressing open challenges in non-
linear system identification. IFAC-PapersOnLine 50(1), 446–451 (2017)
22. Senoz, I., Podusenko, A., Kouw, W.M., de Vries, B.: Bayesian joint state and
parameter tracking in autoregressive models. In: Conference on Learning for Dy-
namics and Control. pp. 1–10 (2020)
23. Tangirala,A.K.:Principlesofsystemidentification:theoryandpractice.CRCPress
(2018)
24. Tin,C.,Poon,C.S.:Internalmodelsinsensorimotorintegration:perspectivesfrom
adaptive control theory. Journal of Neural Engineering 2(3), S147 (2005)
25. Wigren,T.,Schoukens,J.:Threefreedatasetsfordevelopmentandbenchmarking
in nonlinear system identification. In: European Control Conference (ECC). pp.
2933–2938 (2013)
26. Yoshimoto, J., Ishii, S., Sato, M.a.: System identification based on online varia-
tional Bayes method and its application to reinforcement learning. In: Artificial
NeuralNetworksandNeuralInformationProcessing,pp.123–131.Springer(2003)

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Online system identification in a Duffing oscillator by free energy minimisation"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
