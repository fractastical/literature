=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Realising Synthetic Active Inference Agents, Part I: Epistemic Objectives and Graphical Specification Language
Citation Key: koudahl2023realising
Authors: Magnus Koudahl, Thijs van de Laar, Bert de Vries

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2023

Abstract: The Free Energy Principle (FEP) is a theoretical framework for de-
scribinghow(intelligent)systemsself-organiseintocoherent,stablestruc-
tures by minimising a free energy functional. ActiveInference (AIF) is a
corollaryoftheFEPthatspecificallydetailshowsystemsthatareableto
planforthefuture(agents)functionbyminimisingparticularfreeenergy
functionalsthatincorporateinformation seekingcomponents. Thispaper
is the first in a series of two where we derive a synthetic version of AIF
on free form factor...

Key Terms: energy, agents, language, objectives, version, specification, netherlands, inference, active, eindhoven

=== FULL PAPER TEXT ===

3202
tcO
61
]IA.sc[
2v41080.6032:viXra
Realising Synthetic Active Inference Agents,
Part I: Epistemic Objectives and Graphical
Specification Language
Magnus Koudahl1,2, Thijs van de Laar1, and Bert de Vries1,3
1BIASLab, Department of Electrical Engineering, Eindhoven
University of Technology, The Netherlands
2VERSES AI Research Lab, Los Angeles, CA, USA, 90016
3GN Hearing, JF Kennedylaan 2, 5612 AB Eindhoven, The
Netherlands
October 17, 2023
Abstract
The Free Energy Principle (FEP) is a theoretical framework for de-
scribinghow(intelligent)systemsself-organiseintocoherent,stablestruc-
tures by minimising a free energy functional. ActiveInference (AIF) is a
corollaryoftheFEPthatspecificallydetailshowsystemsthatareableto
planforthefuture(agents)functionbyminimisingparticularfreeenergy
functionalsthatincorporateinformation seekingcomponents. Thispaper
is the first in a series of two where we derive a synthetic version of AIF
on free form factor graphs. Thepresent paperfocuses on derivingalocal
version of the free energy functionals used for AIF. This enables us to
construct a version of AIF which applies to arbitrary graphical models
andinterfaceswith priorworkon messagepassing(MP) algorithms. The
resultingmessagesarederivedinourcompanionpaper. Wealsoidentifya
gap inthegraphicalnotation usedforfactor graphs. Whilefactorgraphs
are great at expressing a generative model, they have so far been unable
to specify the full optimisation problem including constraints. To solve
this problem we develop constrained Forney-style factor graph (CFFG)
notation which permits a fully graphical description of variational infer-
ence objectives. We then proceed to show how CFFGs can be used to
reconstruct prior algorithms for AIF as well as derive new ones. The
latter is demonstrated by deriving an algorithm that permits direct pol-
icy inference for AIF agents, circumventing a long standing scaling issue
thathassofarhinderedtheapplication ofAIFinindustrialsettings. We
demonstrate our algorithm on the classic T-maze task and show that it
reproduces the information seeking behaviour that is a hallmark feature
of AIF.
1
1 Introduction
ActiveInference(AIF)isanemergingframeworkformodellingintelligentagents
interactingwithanenvironment. Originatinginthe fieldofcomputationalneu-
roscience, it has since been spread to numerous other fields such as modern
machine learning. At its core, AIF relies on variational inference techniques
to minimise a free energy functional. A key differentiator of AIF compared to
other approaches is the use of custom free energy functionals such as Expected
and Generalised free energies (expected free energy (EFE) and generalised free
energy (GFE), respectively). These functionals are specifically constructed so
astoelicitepistemic,information-seekingbehaviourwhenusedtoinfer actions.
Optimisation of these functionals have so far relied on custom algorithms
that require evaluating very large search trees which has rendered upscaling of
AIF difficult. Many recent works, such as Branching Time AIF [3] and sophis-
ticatedinference[7]haveinvestigatedalgorithmicwaystoprunethe searchtree
in order to solve this problem.
This paper is the first in a series of two parts where we take a different
approach and formulate a variation of the GFE optimisation problem using
a custom Lagrangian derived from constrained Bethe free energy (CBFE) on
factorgraphs. UsingvariationalcalculuswethenderiveacustomMPalgorithm
thatcandirectlysolveforfixedpointsoflocalGFEterms,removingtheneedfor
asearchtree. This allowsusto constructapurelyoptimisation-basedapproach
to AIF which we name LagrangianActive Inference (LAIF).
LAIF applies to arbitrary graph topologies and interfaces with generic MP
algorithms which allow for scaling up of AIF using off-the-shelf tools. We ac-
complish this by constructing a node-local GFE on generic factor graphs.
The present paper is structured as follows: In section 2, we review relevant
background material concerning Forney-style factor graphs (FFGs) and Bethe
freeenergy(BFE).Insection3weformalisewhatwemeanby"epistemics"and
construct an objective that is local to a single node on an FFG and possesses
an epistemic, information-seeking drive. This objective turns out to be a local
version of the GFE which we review in section 3.2.
Once we start modifying the free energy functional, we recognise a prob-
lem with current FFG notation. FFGs visualise generative models but fail to
display a significant part of the optimisation problem, namely, the variational
distribution and the functional to be optimised.
To remedy this, we develop the CFFG graphical notation in section 5 as a
method for visualising both the variationaldistribution and any adaptationsto
the free energy functional that are needed for AIF. These tools form the basis
of the update rules derived in our companion paper [15].
With the CFFG notation in hand, in section 6 we then proceed to demon-
strate how to recoverprior algorithms for AIF as MP on a CFFG. AIF is often
described as MP on a probabilistic graphicalmodel, see [4, 6, 13, 26] for exam-
ples. However,thisrelationshiphasnotbeenproperlyformalisedbefore,inpart
becauseadequatenotationhasbeenlacking. UsingCFFGsitisstraightforward
to accurately write down this relation. Further, due to the modular nature of
2
CFFGs it becomes easy to devise extensions to prior AIF algorithms that can
be implemented using off-the-shelf MP tools.
Finally, section 7 demonstrates a new algorithm for policy inference using
LAIFthatscaleslinearlyintheplanninghorizon,providingasolutiontoalong-
standing barrier for scaling AIF to larger models and more complex tasks.
2 The Lagrangian approach to message passing
In this section, we review the Bethe free energy along with FFGs. These con-
cepts form the foundation from which we will build towards local epistemic,
objectives.
As a free energy functional, the BFE is unique because stationary points of
theBFEcorrespondtosolutionsofthebeliefpropagationalgorithm[20,25,28]
which provides exact inference on tree-structured graphs.
Furthermore, by adding constraints to the BFE using Lagrangemultipliers,
one can form a custom Lagrangian for an inference problem. Taking the first
variationofthisLagrangian,onecanthensolveforstationarypointsandobtain
MP algorithms that solve the desired problem. Prior work [25, 28] have shown
that adding additional constraints to the BFE allows for deriving a host of dif-
ferentmessagepassingalgorithmsincludingvariationalmessagepassing(VMP)
[27] and expectation propagation (EP) [18] among others. We refer interested
readersto [25]foracomprehensiveoverviewofthistechnique andhowdifferent
choices of constraints can lead to different algorithms.
Constraints are specified at the level of nodes and edges, meaning this pro-
cedure can produce hybrid MP algorithms, foreshadowing the approach we are
going to take for deriving a local, epistemic objective for AIF.
2.1 Bethe free energy and Forney-style factor graphs
ThroughouttheremainderofthepaperwewilluseFFGstovisualiseprobabilis-
tic models models. In Section 5 we extend this notation to additionally allow
for specifying constraints on the variational optimisation objective.
Following [25] we define an FFG as a graph G = (V,E) with nodes V and
edges E ⊆ V ×V. For a node a ∈ V we denote the connected edges by E(a).
Similarly for an edge i∈E, we denote the connected nodes by V(i).
AnFFGcanbeusedtorepresentafactorisedfunction(model)overvariables
s, as
f(s)= f (s ), (1)
a a
a∈V
Y
where s collects the argument variables of the factor f . Throughout this
a a
paper we will use cursivebold font to denote collections of variables. In the
corresponding FFG, the factor f is denoted by a square node and connected
a
edges E(a) represent the argument variables s .
a
3
s s
1 2
f f f
a b c
s s
3 4
f f
d e
Figure 1: Example of an FFG
As anexample,wecanconsiderthe FFGshowninFig.1 whichcorresponds
to the model
f(s ,s ,s ,s )=f (s )f (s ,s ,s )f (s ,s )f (s )f (s ) (2)
1 2 3 4 a 1 b 1 2 3 c 2 4 d 3 e 4
In Fig. 1, the vertex set is V ={a,...,e} and the edge set is E ={1,...,4}.
Asanexample,theneighbouringedgesofthe nodecaregivenbyE(c)={2,4}.
Inthisway,FFGsallowforasimplevisualisationofthefactorisationproperties
of a high-dimensional function.
The problem we will focus on concerns minimisation of a free energy func-
tional over a generative model. More formally, given a model (1) and a "varia-
tional" distribution q(s), the variational free energy (VFE) is defined as
q(s)
F[q], q(s)log ds. (3)
f(s)
Z
Variational inference concerns minimising this functional, leading to the so-
lution
q∗ =argminF[q], (4)
q∈Q
with Q denoting the admissible family of functions q. The optimised VFE
upper-bounds the negative log-evidence (surprisal), as
q∗(s)
F[q∗]= q∗(s)log ds−logZ, (5)
p(s)
Z
Surprisal
Posteriordivergence
| {z }
withZ = f(s)dsis the m|odelevi{dzence and}the exactposterioris givenby
p(s)=f(s)/Z.
R
TheBFEappliestheBetheassumptiontothefactorisationofq whichyields
an objective that decomposes into a sum of local free energy terms, each local
to a node on the corresponding FFG. Each node local free energy will include
entropytermsfromallconnectededges. Sinceanedgeisconnectedto(atmost)
4
twonodes,thatmeansthe correspondingentropytermwouldbecountedtwice.
To prevent overcounting of the edge entropies, the BFE includes additional
terms entropy terms that cancel out overcountedterms.
Under the Bethe approximation q(s) is given by
q(s)= q
a
(s
a
) q
i
(s
i
)1−di (6)
a∈V i∈E
Y Y
with d the degree of edge i. As an example, on the FFG shown in Fig. 1
i
this would correspond to a variational distribution of the form
q (s )q (s ,s ,s )q (s ,s )q (s )q (s )
a 1 b 1 2 3 c 2 4 d 3 e 4
q(s ,...,s )= (7)
1 4
q (s )q (s )q (s )q (s )
1 1 2 2 3 3 4 4
where we see terms for the edges in the denominator and for the nodes in
the numerator. With this definition, the free energy factorises over the FFG as
q (s ) 1
F[q]= q (s )log a a ds + (1−d ) q (s )log ds . (8)
a a a i i i i
f (s ) q (s )
a a i i
a∈VZ i∈E Z
X X
F[qa] H[qi]
| {z } | {z }
Eq. (8) defines the BFE. Note that F defines a free energy functional which
canbeeitherlocalorglobaldependingonitsarguments. Morespecifically,F[q]
defines the free energy for the entire model, while F[q ] defines a node-local(to
a
the node a) BFE contribution of the same functional form.
Under optimisationof the BFE- solvingEq. (4) - the admissible setof func-
tions Q enforces consistent normalisation and marginalisation of the node and
edge local distributions, such that
q (s )ds =1 for all i∈E (9a)
i i i
Z
q (s )ds =1 for all a∈V (9b)
a a a
Z
q (s )ds =q (s ) for all a∈V,i∈E(a). (9c)
a a a\i i i
Z
Because we always assume the constraints given by Eq. (9) to be in effect,
wewillomitthesubscriptonindividualq’smovingforwardsandinsteadletthe
arguments determine which marginal we are referring to, for example writing
q(s ) instead of q (s ).
a a a
A core aspect of FFGs is that they allow for easy visualisation of MP al-
gorithms. MP algorithms are a family of distributed inference algorithms with
the common trait that they can be viewed as messages flowing on an FFG.
5
As a generalrule, to infer a marginalfor a variable, messages are passed on
the graph toward the associated edge for that variable. Multiplication of col-
liding (forwardandbackward)messageson anedge yields the desired posterior
marginal. We denote a message on an FFG by arrows pointing in the direction
that the message flows.
→ →←
f f f
a s b s c
1 2
↑ s 3 ↑ s 4
f f
d e
Figure 2: Example of messages flowing on an FFG
We show an example in Fig. 2 of inferring a posterior marginal for the
variable s . In this example, messages are flowing on the FFG of Fig. (1)
2
towards the variable s where we see two arrows colliding. The exact form of
2
the individual messages depend on the algorithm being used.
3 Defining epistemic objectives
Now we move on to the central topic of AIF, namely agents that interact with
theworldtheyinhabit. UndertheheadingofAIF,anagententailsagenerative
modelofitsenvironmentandisengagedintheprocessofachievingfuture goals
(specified by a targetdistribution or goalprior)throughactions. This task can
be cast as a process of free energy minimisation [5, 7, 8]. A natural question to
askis then, whatshouldthis free energyfunctional looklike andwhy? Herewe
wishtohighlightacorefeatureofAIFthatsetsitapartfromotherapproaches:
Systematic information gathering by targeted exploration of an environment.
Whatever the form of our free energy functional, it should lead to agents that
possess an exploratory drive consistent with AIF.
While it is tempting to default to a standard VFE, prior work [24] has
shownthatdirectlyoptimisingBFEorVFEwheninferringasequenceofactions
(which we will refer to as a policy) does not lead to agents that systematically
explore their environment. Instead, directly inferring a policy by minimising a
VFE/BFEleadstoKullback-Leiblerdivergence(KL)-control[12]1. Thismeans
that VFE/BFE is not the correct choice when we desire an agent that actively
samples its environment with the explicit purpose of gathering information.
Instead a hallmark feature of AIF is the use of alternative functionals in
place of either the VFE or the BFE, specifically for inferring policies. The
goalof these alternative functionals is often specifically to induce an epistemic,
explorative term that drives AIF agents to seek out information.
1Otherworkswillsometimesuseπ asasymboltodenote apolicy
6
Epistemics,epistemicbehaviouror"foragingforinformation"arecommonly
usedtermsintheAIFliteratureandrelatedfields. Whilewehaveusedtheterm
colloquiallyuntilthis point,wenowclarifyformallyhowweuse the terminthe
present paper and how it relates to the objective functionals we consider. A
core problem is that epistemics is most often defined in terms of the behaviour
of agents ("what does my agent do?") rather than from a mathematical point
of view. Prior work on this point includes [10, 17].
Wetaketheviewthatepistemicsarisefromtheoptimisationofeitheranmu-
tualinformation(MI)termoraboundthereon. TheMI(betweentwovariables
x and z) is defined as [16]
p(x,z)
I[x,z]= p(x,z)log dxdz. (10)
x
p(x)p(z)
To gain an intuition for why maximising MI leads to agents that seek out
information, we can rewrite MI as
p(z |x)
I[x,z]= p(x,z)log dxdz
x
p(z) (11)
=H[z]−H[z |x]=H[x]−H[x|z]
SinceMIissymmetricinitsarguments,Eq.(11)canequallywellbewritten
intermsofxratherthanz. Eq.(11)showsthatMIdecomposesasthedifference
between the marginal entropy z and the expected entropy of z conditional on
x.
If we let z denote an internal state of an agent and x an observation and
allow our agent to choose x - for instance through acting on an environment
- we can see why maximising Eq. (10) biases the agent towards seeking our
observations that reduce entropy in z. Maximising Eq. (10) means the agent
willpreferobservationsthatprovideuseful(inthesenseofreducinguncertainty)
information about its internal states z. For this reason MI is also known as
Information Gain.
Actually computing Eq. (10) is often intractable and in practice,a bound is
often optimised instead.
3.1 Constructing a local epistemic objective
At this point, we have seen that the BFE is defined over arbitrary FFGs, yet
does not lead to epistemic behaviour. On the other hand, maximising MI leads
tothetypesofepistemicbehaviourwedesire,yetisnotdistributedliketheBFE.
The question becomes whether there is a way to merge the two and obtain a
distributed functional - like the BFE- that includes an epistemic term?
We will now show how to construct such a functional. Our starting point
will be the BFE given by Eq. (8). We will focus on a single node a and its
7
associated local energy term and partition the incoming edges into two sets, x
and z. This gives the node local free energy
q(x,z)
F[q ]= q(x,z)log dxdz (12)
a x
f(x,z)
Now we need to add on an MI term to induce epistemics. Since we are
minimising our free energy functional and want to maximise MI, we augment
the free energy with a negative MI term as
Variationalfreeenergy Negativemutualinformation
q(x,z) q(x)q(z)
G[q ]= q(x,z)log dxdz+ q(x,z)log dxdz (13a)
a x x
z
}|f(x,z)
{ z
}|q(x,z)
{
✘q(x✘ , ✘ z ✘ ) q(x)q(z)
= x q(x,z)log f(x,z) dxdz+ x q(x,z)log ✘q(x✘ , ✘ z ✘ ) dxdz (13b)
q(x)q(z)
= q(x,z)log dxdz (13c)
x
f(x,z)
which we can recognise as a node-local GFE [19]. In section 3.2 we review
theresultsof[19]toexpanduponthisstatement. Eq.(13a)providesastraight-
forwardexplanation of the kinds of behaviour that we can expect out of agents
optimising a GFE. Namely, minimising BFE with a goal prior corresponds to
performing KL-control [12] while the MI term adds an epistemic, information-
seeking component. Viewed in this way, there is nothing mysterious about the
kind of objective optimised by AIF agents: It is simply the sum of two well-
known and established objectives that are each widely used within the control
and reinforcement learning communities.
3.2 Generalised free energy
The objective derived in Eq. (13) is a node local version of the GFE originally
introducedby[19]. Inthissection,wereviewthe GFE asconstructedby[19]in
order to relate our construction to prior work on designing AIF functionals. In
section 6, we show how to reconstruct the exact method of [19] using the tools
wedevelopinthis paper. Priorto [19], the functionalofchoice wasthe EFE [4,
6]. [19]identifiedsomeissueswiththeEFEandproposedtheGFEasapossible
solution.
We show how to reconstruct the original EFE-based algorithm of [6] using
our local objective in section 6. We also provide a detailed description of the
EFE in Appendix. A since it is still a popular choice for designing AIF agents.
AcoreissuewiththeEFEisthatitisstrictlylimitedtoplanningoverfuture
timesteps. This means that AIF agents that utilise the EFE functional need
to maintain two separate models: One for inferring policies (using EFE) and
one for state inference (using VFE/BFE) that updates as observations become
8
available. The key advantageof EFE is that it induces the epistemic drive that
we desire from AIF agents.
The goal of [19] was to extend upon the EFE by introducing a functional
that could induce similar epistemic behaviour when used to infer policies while
at the same time reducing to a VFE when dealing with past data points. In
this way, an agent would no longer have to maintain two separate models and
could instead utilise only one.
TheGFEasintroducedby[19]istiedtoaspecificchoiceofgenerativemodel.
The model introduced by [19] is given by
T
p(x,z|uˆ)∝p(z ) p(x |z )p(z |uˆ ,z )p˜(x ) (14)
0 k k k k k−1 k
k=1
Y
where x denotes observations, z denotes latent states and uˆ denotes a fixed
policy. Throughout this paper we will use t to refer to the current time step in
agivenmodelanddenote fixedvaluesbyahat, hereexemplified byuˆ. Further,
we also use z to denote vectors.
Given a current time step t, we have that for future time steps k >t, p˜(x )
k
defines a goalprior over desired future observations. For past time steps k ≤t,
we instead have p˜(x )= 1 which makes it uninformative 2. Writing the model
k
in this way allows for a single model for both perception (integrating past data
points) and action (inferring policies) since both past and future time steps are
included. GFE is defined as [19]
T
q(x |uˆ )q(z |uˆ )
G[q;uˆ]= q(x |z )q(z |uˆ )log k k k k dx dz (15)
x k k k k p˜(x )p(x ,z |uˆ ) k k
k k k k
k=1
X
where
δ(x −xˆ ) if k ≤t
q(x |z )= k k . (16)
k k
(p(x
k
|z
k
) if k >t
and xˆ denotes the observed data point at time step k. p(x ,z | uˆ ) can
k k k k
be foundrecursivelybyBayesiansmoothing,see [14,23]fordetails. Tosee how
the GFE reduces to a VFE when data is available, we refer to Appendix B.
The GFE introduced by [19] improves upon prior work utilising EFE but
stillhassomeissues. ThefirstliesintyingittothemodeldefinitioninEq.(14).
Being committed to a model specification aprioriseverely limits what the GFE
canbe appliedtosince notallproblemsaregoingtofit the modelspecification.
Additionally,amoresubtleissueliesinthecommitmenttoahardtimehorizon.
If t denotes the current timestep, at some point time will advance to the point
t > T. When this happens Eq. (14) loses the capacity to plan and becomes
2[19]writesthisasthepriorbeingflattoachieveasimilareffect
9
static since all observations are clamped by Eq. (16). A further complication
arises from uˆ being a fixed parameter and not a random variable. Being a
fixed parameter means it is not possible to perform inference for uˆ which in
turn makes scaling difficult. Moving to a fully local versionof the GFE instead
means we can construct a new synthetic approach to AIF that addresses these
issues.
4 LAIF - Lagrangian Active Inference
Armed with the node local GFE derived in Eq. (13), we can construct a La-
grangianforAIF.ThegoalistoadapttheLagrangianapproachtoMPsketched
insection2toderiveaMPalgorithmthatoptimiseslocalGFEinordertohave
a distributed inference procedure that incorporates epistemic terms. Naively
applyingthemethodof[25]toEq.(13)doesnotyieldusefulresultsbecausethe
numerator differs from the term we take the expectation with respect to. To
obtain a useful solution we need that
q(x,z)=q(x|z)q(z) (17)
and add the original assumption in [19], given by Eq. (16)
q(x|z),p(x|z). (18)
With these additional assumptions,we canobtain meaningful solutions and
derive a message passing algorithm that optimises a local GFE and induces
epistemic behaviourwhich we demonstrate in section7. The detailed derivation
of these results can be found in our companion paper [15]. For practical AIF
modelling, we provide the relevant messages in Fig. 21.
At this point, we will instead show a different way to arrive at the local
GFE. This approach is more "mechanical" but has the advantage that it can
more easily be written in terms of constraints on the BFE. Our starting point
will once again be a free energy term local to the node a
q(s )
a
F[q ]= q(s )log ds (19)
a a f(s ) a
a
Z
To turn Eq. (19) into a local GFE we need to perform two steps. The first
is to enforce a mean field factorisation
q(s )= q(s ) (20)
a i
i∈YE(a)
The second is to change the expectation to obtain
10
q(s ) q(s )
q(s )log a ds =⇒ p(s |s )q(s )log a ds (21)
a f(s ) a i a\i a\i f(s ) a
a a
Z Z
where i∈E(a). This partitions the connected variables into two sets: A set
of variables where the expectation is modified and any remainder that is not
modified. p(s | s ) denotes a conditional probability distribution. We write
i a\i
p here instead of f to emphasise that the conditionalneeds to be normalisedin
order for us to be able to take the expectation.
We refer to this move as a P-substitution. Once the mean field factorisation
is enforced, we can recognise Eq. (21) as a node-local GFE G[q ].
a
Asanexampleofconstructinganode-localGFEwiththisapproach,wecan
consider a node with two connected variables, {x,z}.
The local free energy becomes
q(x,z)
F[q ]= q(x,z)log dxdz (22)
a x
p(x,z)
Now we apply a mean field factorisation
q(x)q(z)
F[q ]= q(x)q(z)log dxdz (23)
a x
p(x,z)
And finally, perform P-substitution to obtain a node-local GFE
q(x)q(z) q(x)q(z)
q(x)q(z)log dxdz =⇒ p(x|z)q(z)log dxdz (24)
x x
p(x,z) p(x,z)
WedenotethesetofnodesforwhichwewanttoperformP-substitutionwith
P ⊆ V. When performing P-substitution, we are replacing a local variational
free energy F[q ] with a local generalised free energy G[q ]. Armed with P-
a a
substitution, we cannow write the simplest instance ofa LagrangianforActive
Inference
L[q]= G[q ] + F[q ]+ (1−d ) H[q(s )]
a b i i
a
X
∈P b∈XV\P
X
i∈E
Edgeentropy
P-substitutedsubgraph Nodelocal
w/naivemeanfield freeenergies | {z }
| {z } | {z }
+ λ (s ) q(s )− q(s )ds ds
ia i i a a\i i
" # (25)
a∈Vi∈EZ Z
XX
Marginalisation
| {z }
+ λ q(s )ds −1 + λ q(s )ds −1 .
a a a i i i
" # " #
a∈V Z i∈E Z
X X
Normalisationofnodemarginals Normalisationofedgemarginals
| {z } | {z }
11
WiththeActiveInferenceLagrangianinhand,wecannowsolveforstation-
ary points using variational calculus and obtain MP algorithms for LAIF. The
key insight is that messages flowing out of P derived from stationary points of
Eq.(25)willcorrespondtostationarypointsofthelocalGFEratherthanBFE,
meaningtheresultwillincludeanepistemiccomponent. Thispavesthewayfor
a localised version of AIF that applies to arbitrary graph structures, does not
suffer from scaling issues as the planning horizon increases, and can be solved
efficiently and asynchronouslyusing MP.
5 Constrained Forney-style factor graphs
While FFGs are a useful tool for writing down generative models, we have by
nowestablishedtheimportanceofknowingtheexactfunctionaltobeminimised.
This requires specifying not just the model f but also the family Q through
constraints and any potential P-substitutions. This is important if we want to
be able to succinctly specify not just the model but also the exact inference
problem we aim to solve.
Wewillnowdevelopjustsuchanewnotationforwritingconstraintsdirectly
as part of the FFG. We refer to FFGs with added constraints specification as
CFFGs.
Figure 3: An example FFG.
Fig 3 shows a model comprisedof five edges and six nodes. FFGs tradition-
ally represent the model f using squares connected by lines as shown in Fig. 3.
Thesquaresrepresentfactorsandthe connectionsbetweenthemrepresentvari-
ables. Connecting an edge to a square node indicates that the variable on that
edge is an argument of the factor it is connected to.
The notation for CFFGs adheres to similar principles when specifying f.
However, we augment the FFG with circular beads to indicate the constraints
that define our family Q. Each factor of the variational distribution in q will
correspond to a bead and the position of a bead indicates to which marginal
it refers - a bead on an edge denotes an edge marginal q(s ) and a bead inside
i
a node denotes a node marginal q(s ). An empty bead will denote the default
a
normalisationconstraintswhileaconnectionbetweenbeadsindicatesmarginal-
isationconstraintsfollowingEq.(9). Thesebeadsformthebasicbuildingblocks
of our notation.
TowritetheobjectivecorrespondingtothemodelinFig.3giventhedefault
constraintsofEq.(9),weaddbeadsforeverytermandextendedgesthroughthe
12
node boundary to connect variables that are under marginalisation constraints
as shown in Fig. 4.
Figure 4: Example CFFG with normalisation and marginalisationconstraints.
5.1 Factorisation constraints
We will now extend our notation with the most common types of constraints
used for defining Q. A common choice is factorisation of the variational distri-
bution with the most well-known example being the naive mean field approx-
imation. Under a naive mean-field factorisation, all marginals are considered
independent. Formally this means we enforce
q(s )= q(s ). (26)
a i
i∈YE(a)
To write Eq. (26) on a CFFG we need to replace the joint node marginal
with the product of adjacent edge marginals.
f
a
f
a
Figure 5: Changing a local joint factorisationto a naive mean field assumption
on a CFFG.
Todothiswecanreplacethebeadindicatingthejointmarginalwithabead
for each edge marginal in Eq. (26) as shown in Fig. 5.
The naive mean field is the strongest factorisation possible. It is possible
to utilise less aggressive factorisations by appealing to a structured mean field
approximationinstead. The structured mean field constraint takes the form
q(s )= qn(sn) (27)
a a
n∈Y l(a)
13
where l(a) denotes a set of one or more edges connected to the node a such
that each element in E(a) can only appear in l(a) once [25]. For example if
E(a) = {i,j,k} corresponding to variables {x,y,z}, we can factorise q(x,y,z)
as q(x)q(y,z) or q(z)q(x,y) but not as q(x,y)q(y,z) since y appears twice. The
naive mean field is a special case of the structured mean field where every
variable appears only by itself.
To write a structured mean field factorisation on a CFFG we can apply a
similar logic and replace the single bead denoting the joint with beads that
match the structure of l(a). Each set of variables that are factorised together
corresponds to a single bead connected to the edges in the set that factor to-
gether.
f
a
f
a
f
a
Figure 6: Changing a local joint factorisation to structured mean field on a
CFFG.
Fig. 6 shows two example factorisations. The first option factorises the four
incoming edges into two sets of two while the second partitions the incoming
edges into two sets of one and a single set of two. Using these principles it is
possible to specify complex factorisation constraints as part of the CFFG by
augmenting each node on the original FFG.
The final situation we need to consider is the case when a single node has
a variable or very high number of incoming edges. An example could be a
GaussianMixture Model with a variable number of mixture components. On a
CFFG we indicate variable or large numbers of identical edges by drawing two
of the relevant edges and separating them with dots (···) as shown in Fig. 7.
Toindicate factorisationconstraintswecanwriteeither ajointora naivemean
field factorisationbetween the two edges, letting the dots denote that a similar
factorisation applies to the remaining edges.
14
f f
a a
Figure7: Meanfield(left)andjoint(right)factorisationconstraintsforvariable
numbers of edges on a CFFG.
5.2 Form constraints
We will now extend CFFG notation with constraints on the functional form of
nodes and edges. Form constraints are used to enforce a particular form for a
local marginal on either an edge or a node. For an edge s they enforce
i
q(s )ds =q(s )=g(s ) (28)
a a\i i i
Z
whereg(s )denotesthefunctionalformweareconstrainingtheedgemarginal
i
s to take. Form constraints on node marginals take the form
i
q(s )=g(s ) (29)
a a
ConventionallyFFGsdenotetheformofafactorbyasymbolinsidethenode.
Weadoptasimilarconventiontodenoteformconstraintsonqbyaddingsymbols
within the correspondingbeads. For instance, we can indicate a Gaussianform
constraint on an edge as shown in Fig 8
N
Figure 8: Notation for enforcing a Gaussian form constraint on an edge.
Notethatthisisnotdependentontheformoftheneighbouringfactors. This
is a subtle point as it allows us to write approximations into the specification
of Q. As an example, the unconstrained marginal in Fig 8 might be bimodal
or highly skewed but by adding a form constraint, we are enforcing a Gaussian
approximation. Outside of a few special cases, enforcing form constraints on
edgesisrarelydone inpracticesincethe functionalformof q mostoftenfollows
from optimisation [25].
A special case of form constraints is the case of dangling edges (edges that
are not terminated by a factor node). Technically these would not warrant a
bead since they would not appear explicitly in the BFE due to having degree
1. Intuitively this means that the edge marginal is only counted once and we
therefore do not need to correct for overcounting. However, without a bead,
there is nowhere to annotate a form constraint which is problematic.
15
The solution for CFFG notation is to simply draw the beadanyway,in case
a form constraint is needed. This is formally equivalent to terminating the
danglingedgebya factornodewith the node function f (s )=1. Terminating
a a
the edge inthis waymeansthe edgeinquestionnowhas degree2andtherefore
warrantsa bead. This is alwaysa valid movesince multiplicationby 1 does not
change the underlying function [25].
We can denote form constraints on node marginals in the same manner as
edge marginals. We show an example in Fig 9 where we enforce a Gaussian
form constraint on one node marginal and a Wishart on the other. Again it is
importanttonotethattheseareconstraintsonq andnotpartoftheunderlying
model specification f.
N W
Figure 9: Notation for enforcing form constraints on nodes.
Two kinds of form constraints warrant extra attention: δ-constraints and
moment matching. We will now deal with these in turn.
5.3 δ-constraints and data points
δ-constraints are the most commonly used form constraints because they allow
us to incorporate data points into a model. A δ-constraint on an edge defines
the function g(s ) in Eq. (28) to be
i
g(s )=δ(s −sˆ) (30)
i i i
What makes the δ-constraint special is that sˆ can either be a known value
i
or a parameter to optimise [25]. In the case where sˆ is known, it commonly
i
correspondsto a data point. We will refer to this case as a data constraintand
denote it with a filled circle as shown in Fig 10
δ δ
Figure 10: Terminating and non-terminating notation for data constraints.
Data constraints are special because they denote observations. They also
block anyinformationflow acrossthe edge in question[25]. Becausethey block
informationflow,CFFGnotationoptionallyallowsdataconstraintstoterminate
edges.
Here we wish to raise a subtle point about prior FFG notation. Previous
work has used small black squares to denote data constraints following [21].
In keeping with our convention, a small black square on a CFFG denotes a
16
δ-distributed variable in the model f rather than the variational distribution
q. Being able to differentiate data constrained variables in q and apriori fixed
parameterofthemodelf allowsustobeexplicitaboutwhatactuallyconstitutes
a data point for the inference problem at hand [caticha_entropic_2012].
In the case where sˆ is not known, it can be treated as a parameter to be
j
optimised. We referto this caseasaδ-constraintorapointmassconstraintand
notate it with an unfilled circle as shown in Fig 11
δ
Figure 11: Notation for δ-constraints.
Unlikedataconstraints,theδ-constraintallowsmessagestopassandisthere-
fore not allowed to terminate an edge. Optimising the value of sˆ under a
i
δ-constraint leads to EM as message passing [25].
5.4 Moment matching constraints
Momentmatchingconstraintsarespecialinthattheyreplacethehardmarginal-
isation constraints of Eq. (8) with constraints of the form
q(s )= q(s )ds =⇒ q(s )T (s )ds = q(s )T (s )ds (31)
i a a\i a i i a i i i i
Z Z Z
whereT (s )arethesufficientstatisticsofanexponentialfamilydistribution.
i i
Thismoveloosensthe marginalisationconstraintbyinsteadonlyrequiringthat
themomentsinquestionalign. Whentakingthefirstvariationandsolving,one
obtains the expectation propagation(EP) algorithm [18].
For notational purposes, moment matching constraints are unique in that
theyinvolvebothanedge-andanode-marginal. Thatmeanstheeffectsarenot
localisedto a single bead. To indicate which beads are involved,we replace the
solid lines between them with dashed lines instead.
We denote moment matching constraints by an E inside the corresponding
edge-bead as shown in Fig 12. Choosing the edge-bead over the node-bead is
an arbitrary decision made mainly for convenience.
E E
Figure12: Notationformomentmatchingwithasingle-sided(left)anddouble-
sided (right) node/edge pairs.
The left side of Fig. 12 shows notation for constraining a single node/edge
pairbymomentmatching. Ifbothnodesconnectedtoanedgeareundermoment
matching constraints, the double-sided notation on the right of Fig. 12 applies.
17
Given the modular nature of CFFG notation it is easy to compose differ-
ent local constraints to accurately specify a Lagrangian and by extension an
inference problem. Adding custom marginal constraints to a CFFG is also
straightforwardas it simply requires defining the meaning of a symbol inside a
bead.
5.5 P-substitution on CFFGs
ThefinalpieceneededtorepresenttheActiveInferenceLagrangianonaCFFG
is P-substitution. Being able to represent LAIF on a CFFG is the reason for
constructingthe localGFE using amean-fieldfactorisationandP-substitution.
This constructionis muchmore amenable to the tools we havedevelopedso far
as we will now demonstrate.
Recall that P-substitution involves substituting part of the model p for q in
the expectation only. To write P-substitution on a CFFG, the logical notation
isthereforetoreplaceacirclewithasquare. Fig.13showsanexampleofadding
a P-substitution to a mean-field factorised node marginal
x x
z z
f f
a a
y y
Figure 13: P-substitution on a CFFG with naive mean field factorisation.
The square notation for P-substitution on CFFGs implies a conditioning
of the P-substituted variable on all other connected variables that are not P-
substituted. For example, in Fig. 13 the P-substitution changes local VFE to a
GFE by
q(y)q(x)q(z)
q(y)q(x)q(z)log dydxdz
y
p(y,x,z)
(32)
q(y)q(x)q(z)
=⇒ p(y |x,z)q(x)q(z)log dydxdz
y
p(y,x,z)
Here, the P-substituted variable is y, and the remainder are {x,z}.
5.6 CFFG Compression
The valueofCFFGnotationis measuredby howmuchit aidsotherresearchers
and practitioners in expressing their ideas accurately and succinctly. We envi-
18
sion two main groups for whom CFFGs might be of particular interest. The
firstgroupiscomprisedofmathematicalresearchersworkingonconstrainedfree
energy optimisation on FFGs For this group we expect that the notation de-
veloped so far will be both useful and practically applicable since work is often
focused onthe intricacies of performing localoptimisation. Commonly an FFG
in this tradition is small but a very high level of accuracy is desired in order to
be mathematically rigorous.
However there is a second group composed of applied researchers for whom
the challenge is to accurately specify a larger inference problem and its solu-
tion to solve an auxiliary goal - for instance controlling a drone, transmitting
a coded message or simulating some phenomenon using AIF. For this group
of researchers in particular, CFFG notation as described so far might be too
verbose and the overhead of using it may not outweigh the benefits gained. To
this end, we now complete CFFG notation by a mandatory compression step.
The compression step is designed to remove redundant information by en-
forcing an emphasis on deviations from a default BFE. By default, we mean
no constraints other than normalisation and marginalisation and with a joint
factorisation around every node. Recall that default normalisation constraints
are denoted by empty, round beads and marginalisation by connected lines. A
joint factorisation means all incoming edges are connected and the node only
has a single, internal bead.
To provide a recipe for compressing a CFFG, we will need the concept of a
bead chain. A beadchainis simply a seriesof beads connectedby edges. In the
followingrecipe,abeadwillonlybesummarisedaspartofachainifitcontains
no additional information, meaning if it is round and empty. To compress a
CFFG, we follow a series of four steps:
1. Summarise every bead chain by their terminating beads.
2. Fornodeswithnofactorisationconstraints,removeempty,internalbeads.
3. For nodes with no factorisation constraints, remove all internal edges.
4. For all factor nodes, push remaining internal beads to the border of the
corresponding factor node.
After performing these steps, we are left with a compressed version of the
original CFFG where each node can be much smaller and more concise. To
exemplify, we apply the recipe to the CFFG in Fig. 14.
19
f c f a f b δ
δ E
f = f
d e
f f
f g
Figure 14: Initial CFFG before compression.
Fig. 14 is a complicated graph with loops, dangling edges, and multiple
different factorisations in play. As a result, Fig. 14 covers a lot of special cases
thatonemightencounteronaCFFG.We willnowapplythe steps insequence,
starting by removing empty beads on chains. This removes most of the beads
in the inner loop as shown in Fig. 15
20
f c f a f b δ f c f a f b δ
δ E δ E
f = f f = f
d e d e
f f f f
f g f g
Figure 15: Step 1: Removing empty beads from chains. Affected beads are
highlighted in red.
Notethatthe danglingedgeextendingfromf istreatedasterminatedbya
a
beadandthe beadonthe edge isremoved. Next,we removeanyinternalbeads
for nodes with no factorisation constraints. We show this step in Fig. 16
21
f c f a f b δ f c f a f b δ
δ E δ E
f = f f = f
d e d e
f f f f
f g f g
Figure16: Step2. Removingbeadsonnodeswithdefaultfactorisation. Affected
nodes are highlighted in red.
The next step is to remove any internal edge extensions for nodes with no
factorisation constraints. We demonstrate this step in Fig. 17. Note that the
internal edge of the node f does not get cancelled since one of the connected
d
edges is a pointmass in the model and therefore not present in q, meaning f
d
does not have a default factorisation.
22
f c f a f b δ f c f a f b δ
δ E δ E
f = f f = f
d e d e
f f
f g
f f
f g
Figure 17: Step 3. Removing internal edges. Affected edges are highlighted in
red.
Finally,wecanpushanyremaininginternalbeadstotheirnodeborder,illus-
tratedinFig.18. Thisstepallowsforwritingthe CFFGmuchmorecompactly,
as we can see in Fig. 19.
23
f c f a f b δ f c f a f b δ
δ E δ E
f = f f = f
d e d e
f f f f
f g f g
Figure 18: Step 4. Pushing beads to node borders. Affected beads are high-
lighted in red.
At this point, we have removedmost of the beads and edges and still retain
most of the relevant information around all nodes. What is left is only what
deviatesfromadefaultBFEspecification. Thegoalhereis,asstatedinitially,to
makeiteasiertoworkwithCFFG’sforlargermodelswhichnecessitateworking
with smaller nodes.
24
f c f a f b δ f c f a f b δ
δ E
δ E
f = f
d e
f = f
d e
f f
f g
f f
f g
Figure 19: Compression allows for much more compact CFFGs which in turn
are better suited for larger inference problems.
From Fig. 19 it is clear that compression allows for much more compact
CFFG’s which in turn are more amenable to larger CFFG’s. This is especially
true in the absence of any factorisationor form constraintsand P-substitutions
in which case the compressedCFFG and underlying FFG are identical. We see
this with nodes f , f , f and = in Fig. 19.
c f g
6 Classical AIF and the original GFE algorithm
as special cases of LAIF
An integral part of working with MP algorithms is the choice of schedule -
the order in which messages are passed. Many MP algorithms are iterative in
nature and can therefore be sensitive to scheduling. LAIF is also an iterative
MP algorithm and might consequently be sensitive to the choice of schedule.
A particularly interesting observation is that we can recover both the clas-
sical AIF planning algorithm of [6] and the scheme proposed by [19] as special
casesofLAIFbycarefullychoosingthescheduleandperformingmodelcompar-
ison. Thisresultextendsuponpriorworkby[13]whoreinterpretedtheclassical
algorithm through the lens of MP on an FFG.
25
6.1 Example: CFFG and message updates for a discrete
observation model with goals
z ↑ 2
A
T
←
3
x
Cat
c ↓ 1
Figure 20: CFFG of composite node for LAIF on discrete state spaces, repro-
duced from [15].
In order to both demonstrate LAIF and recover prior work, we need to derive
the required messages. We refer to our companion paper [15] for the detailed
derivations and summarise the results here. For the remaining paragraphs, we
will use x to denote that x is vector-valued and A to denote that A is either
a matrix or a tensor. For the discrete case, we work with a composite node
corresponding to the factor
p(x|A,z)=Cat(x|Az) (33a)
p˜(x|c)=Cat(x|c). (33b)
We show the corresponding CFFG in Fig. 20 where we indicate the neces-
sary factorisation and P-substitution. In the parlance of AIF Eq. 33a defines
a model composed of a discrete state transition (T) with transition matrix A
and a categorical goal prior (Cat) with parameter vector c. Before stating the
messages, we define the vector
h(A)=−diag(AT logA) (34)
This term is often denoted H in other works [4, 19]. We instead opt for
using lowercasesince the term is a vectorand for making the dependence on A
explicit. In the following section, we will sometimes use an overbar to denote
expectations, such that E [g(x)]=g(x). Additionally, we define
q(x)
26
ξ(A)=AT logc−log Az −h(A) (35a)
T
ρ=A (cid:0)logc−log(cid:0)Az(cid:1)(cid:1)−h(A). (35b)
(cid:0) (cid:0) (cid:1)(cid:1)
in order to make the expressions more concise. Where expectations cannot
be computed in closed form, we instead resortto Monte Carlo estimates. With
this notation in place, we can write the required messages as
µ (c) ∝Dir(c|Az+1)
1
µ (z) ∝Cat(z|σ(ρ))
2
Solve: z = ! σ(ρ(z)+logd)
µ (z) ∝Cat(z|σ(logz∗−logd))
2
logµ (A) =zTξ(A)
3
U =−zTρ
x
Figure 21: Message updates for the discrete composite node, reproduced from
[15].
Fig.21showsthemessageupdatestowardsallconnectedvariablesaswellas
theaverageenergyterm(U )forthecompositenode. ddenotestheparameters
x
oftheincomingmessagefromtherestoftheCFFGontheedgez. Interestingly,
theenergytermU correspondsexactlytotheEFE asusedinstandardAIF[4,
x
6].
Specialattentionneedstobegiventothemessagesµ (z)andµ (A). While
2 3
µ (z) can be solved for in closed form, in practice applying this result directly
2
canleadtounstablesolutionsthatfluctuatebetweenmultipleextrema. Forthis
reason, we opt for parameterising the message by z and solving for parameters
of the marginal directly using Newtons method. Having found the optimum z∗
wecanthensubstitute the resultinto the messageexpressionto obtainastable
solution.
The message µ (A) does not follow a nice exponential family distribution.
3
To circumventthis problem, we can pass on the logpdf directly. When we need
tocomputethemarginalq(A),wecanthenusesamplingprocedurestoestimate
thenecessaryexpectations[1]. Furtherdetailsandfullderivationscanbefound
in our companion paper [15].
6.2 Reconstructing classical AIF
WithournewmessagesinhandandequippedwithCFFGnotation,wecannow
restate prior work unambiguously, starting with the classical algorithm of [6].
To reconstruct the algorithm of [6], we start by defining the generative model.
27
The generative model is a discrete partially observed Markov decision process
(POMDP) over future time steps given by
p(x ,z |uˆ ,x ,u )
t+1:T t:T t+1:T 1:t 1:t
Future Policy Past
T
| {z } | {z } | {z }
∝p(z |x ,u ) p(x |z )p(z |z ,uˆ ) p˜(x ) (36)
t 1:t 1:t k k k k−1 k k
k=t+1
StatePrior Y Likelihood StateTransition Goalprior
where | {z } | {z }| {z } |{z}
p(z |x ,u )=Cat(z |d) (37)
t 1:t 1:t t
p(z |z ,uˆ )=Cat(z |B z ) (38)
k k−1 k k uˆk k−1
p(x |z )=Cat(x |Az ) (39)
k k k k
p˜(x )=Cat(x |c ). (40)
k k k
Here we let x denote observations, z latent states, and uˆ a fixed control,
k k k
withthesubscriptkindicatingthefuturetimestepinquestion. Theinitialstate
z represents the filtering solution given the agents trajectory so far which we
t
summarise in the parameter vector d. Control signals correspond to particular
transition matrices B where we use the subscript to emphasize that each B
uˆk
matches a particular control uˆ . The observation model is given by the known
k
matrix A. Note that Eq. (36) is not normalised as it includes goal priors over
x. The CFFG of (36) is shown in Fig 22.
B B
uˆt+1 uˆt+2
d z t z t+1 z t+2
Cat T = T = ···
A A
T T
x x
t+1 t+2
c c
t+1 t+2
Cat Cat
Figure 22: CFFG of discrete POMDP as used for planning in standard AIF
models.
where T nodes denote a discrete state transition (multiplication of a cate-
gorical variable by a transition matrix). Given a generative model with a fixed
28
set of controls, the next step is to compute the EFE [4, 6, 13, 14]. We provide
a brief description here and refer to Appendix A and [4, 13, 14, 22] for more
detailed descriptions. The EFE is given by
T
q(z |uˆ )
G(uˆ )= p(x |z )q(z |uˆ )log k k dx dz (41)
t+1:T x k k k k p(x ,z |uˆ ) k k
k k k
k=t
X
With a slight abuse of notation, we can compute EFE by first applying
transitionmatrices to the latent state and generating predicted observations as
z =B z
k uˆk k−1
(42)
x =Az .
k k
In Eq. (42) we slightly abuse notation by having z (resp. x ) refer to the
k k
prediction after applying the transition matrix B (resp. A) instead of the
uˆk
random variable as we have done elsewhere.
WecanrecognisetheseoperationsasperformingaforwardsMPsweepusing
belief propagation messages. With this choice of generative model, the EFE of
a policy uˆ is found by [4, Eq. D.2-3].
t+1:T
T
T
G(uˆ )= −diag AT logA z +xT(logx −logc ) (43)
t+1:T k k k k
k= X t+1 (cid:16) (cid:17)
where c denotes the parametervector of a goalprior at the k’th time step.
k
To select a policy we simply pick the sequence uˆ that results in the lowest
t+1:T
numerical value when solving Eq. (43).
TowritethismethodontheCFFGinFig.22,wecansimplyaddmessagesto
theCFFGandnotethatthesumofenergytermsoftheP-substitutedcomposite
nodes in Fig. 21 matches Eq. (43). We show this result in Fig. 23.
↓ ↓
→ → → → → →
Cat T = T = ···
↓ ↓
→ →
T T
→ →
Cat Cat
Figure 23: Classical EFE computation on the corresponding CFFG.
29
Comparing different policies (choices of uˆ ) and computing the energy
t+1:T
terms of the P-substituted composite nodes is then exactly equal to the EFE-
computation detailed in [6].
6.3 Reconstructing the original GFE method
We can further exemplify the capabilities of CFFG notation by recapitulating
theupdaterulesgivenintheoriginalGFEpaper[19]. Torecovertheprocedure
of[19], we need to extend the generativemodel to encompasspast observations
as
p(x ,z |uˆ )
1:T 0:T 1:T
Goalprior
t T
∝p(z ) p(x |z )p(z |z ,uˆ ) p(x |z )p(z |z ,uˆ ) p˜(x )
0 l l l l−1 l k k k k−1 k k
Y l=1 k= Y t+1 z}|{
Past Future
(44)
| {z }| {z }
where
p(z )=Cat(z |d)
0 0
p(z |z ,uˆ )=Cat(z |B z )
t t−1 t t uˆt t−1
(45)
p(x |z )=Cat(x |Az )
t t t t
p˜(x )=Cat(x |c ).
k k k
For past time steps, we add data constraints and for future time steps we
performP-substitution. Wealsoapplyanaivemeanfieldfactorisationforevery
node. The final ingredient we need is a schedule that includes both forwards
and backwards passes as hinted at in [13]. For t = 1 the corresponding CFFG
is shownin Fig.24 with messagesoutof the P-substituted nodes highlightedin
red. These messages are given by µ (z) in Fig. 21.
2
30
↓ ↓ ↓
z
t
→ → → → → → →
Cat T = T = T = ···
← ← ← ← ← ← ←
↑ ↓ ↑ ↓ ↑
↑
→ → →
T T T
↑
→ →
δ Cat Cat
Figure 24: GFE computation on the CFFG.
With these choices, the update equations become identical to those of [19].
Indeed, careful inspection of the update rules given in [19] reveals the com-
ponent parts of the P-substituted message. However, by using CFFGs and
P-substitution, we can cast their results as MP on generic graphs which imme-
diately generalises their results to free-form graphical models.
Once inference has converged, we note that [19] shows that GFE evaluates
identically to the EFE, meaning we can use the same model comparison pro-
cedure to select between policies as we used for reconstructing the classical
algorithm. This showshowwecanobtainthe algorithmof[19]asaspecialcase
of LAIF.
7 LAIF for policy inference
The tools presented in this chapter are not limited to restating prior work. In-
deed, LAIF offers several advantages over prior methods, one of which is the
ability to directly infer a policy instead of relying on a post hoc comparison of
energy terms. To demonstrate, we solve two variations of the classic T-maze
task [6]. This is a well-studied setting within the AIF literature and therefore
constitutes a good minimal benchmark. In the T-maze experiment, the agent
lives in a maze with four locations as depicted in Fig. 25. The agent ( ) starts
in position 1 and knows that a reward is present at either position 2,or 3, but
not which one. At position 4 is a cue that informs the agent which arm con-
tains the reward. The optimal action to take is therefore to first visit 4 and
learn which arm contains the reward before going to the rewarded arm. Be-
cause this course of action requires delaying the reward, an agent following a
greedypolicy behavessub-optimally. The T-maze is thereforeconsidereda rea-
sonable minimal example of the epistemic, information-seeking behaviour that
is a hallmark of AIF agents. We implemented our experiments in the reactive
MP toolbox RxInfer [2]. The source code for our simulations is available at
31
https://github.com/biaslab/LAIF.
2 3
1
,
4
Figure 25: The T-maze environment
7.1 Model specification
The generative model for the T-maze is an adaptation of the discrete POMDP
used by [6, 19]. We assume the agent starts at some current timestep t and
want to infer a policy up to a known time horizon T. The generative model is
then
T
p(x,z,u)∝p(z ) p(x |z )p(z |z ,u )p(u )p˜(x ) (46)
t k k k k−1 k k k
k=t+1
Initial Y Observation Transition Control Goal
state model model prior prior
|{z} | {z }| {z }| {z }|{z}
where
p(z )=Cat(z |d) (47a)
t t
p(x |z )=Cat(x |Az ) (47b)
k k k k
p(z
k
|z
k−1
,u
k
)= Cat(z
k
|B
n
z
k−1
)unk (47c)
n
Y
p(u )=Cat(u |e ) (47d)
k k k
p˜(x )=Cat(x |c ). (47e)
k k k
u is a one-hotencoded vectoroflength n. The notationu picks the n’th
k nk
entry of u . We show the corresponding CFFG in Fig 26.
k
32
e e
t t+1
Cat Cat
u u
t t+1
d z t z t+1
Cat TM = TM
B ··· B B ··· B
1 4 1 4
A A
t t+1
T T
c c
t t+1
Cat Cat
Figure 26: CFFG for the T-maze experiment
Eq.(47c)definesamixturemodelovercandidatetransitionmatricesindexed
by u . We give the details of this node function and the required messages in
k
Appendix C. The MP schedule is shown in Fig. 27 with GFE-based messages
highlighted in red
33
→ →
Cat Cat
↑ ↓ ↑ ↓
→ → → → →
Cat TM = TM
← ← ←
↑ ··· ↑ ↑ ↓ ↑ ··· ↑ ↑
→ →
T T
→ →
Cat Cat
Figure 27: Message passing schedule for the T-maze experiment
Following [6] we define the initial state and control prior as
d=(1,0,0,0)T ⊗(0.5,0.5)T (48a)
e =(0.25,0.25,0.25,0.25)∀k (48b)
k
with ⊗ denoting the Kronecker product. The transition mixture node re-
quires a set of candidate transition matrices. The T-maze utilises four possible
transitions, given below
1 1 1 1 0 1 1 0
0 0 0 0 1 0 0 1
B
1
=
0 0 0 0
⊗I
2
,B
2
=
0 0 0 0
⊗I
2
0 0 0 0 0 0 0 0
    (49)
0 1 1 0 0 1 1 0
0 0 0 0 0 0 0 0
B
3
=
1 0 0 1
⊗I
2
,B
4
=
0 0 0 0
⊗I
2
0 0 0 0 1 0 0 1
   
   
whereI denotesa2×2identitymatrix. Notethatthesedifferslightlyfrom
2
the original implementation of [6]. In [6] invalid transitions are represented
by an identity mapping where we instead model invalid transitions by sending
the agent back to position 1. The likelihood matrix A is given by four blocks,
corresponding to the observation likelihood in each position.
34
A
1
A
A= 2  , (50)
A
3
 A 4
 
 
with everything outside the blocks being set to 0. The blocks are
0.5 0.5 0 0
0.5 0.5 0 0
A 1 = 0 0  , A 2 = α 1−α 
 0 0  1−α α 
   
 0 0  1 0 
0 0 0 1
A 3 = 1−α α  , A 4 = 0 0  , (51)
 α 1−α 0 0
   
   
with α being the probability of observing a reward. The goal prior is given
by
c =σ (0,0,c,−c)T ⊗(1,1,1,1)T ∀k (52)
k
(cid:0) (cid:1)
with c being the utility ascribed to a reward and σ(·) the softmax function.
Inference for the parts of the model not in P can be accomplished using belief
propagation. Wefollowtheexperimentalsetupof[6]andletc=2,α=0.9. For
inference, we perform two iterations of our MP procedure and use 20 Newton
steps to obtain the parameters of the outgoing message from the P-substituted
nodes.
We show the results in Fig. 28. The number in each cell is the posterior
probability mass assigned to the corresponding action, with the most likely
actions highlighted in red.
35
Controls at time step 1 Controls at time step 2
0.20 0.20 0.30 0.30
0.25 0.13
0.35 0.26
Figure 28: Posterior controls for the T-maze experiment
Fig. 28 shows an agent that initially prefers the epistemic action (move to
state 4) at time t+1 and subsequently exhibits a preference for either of the
potentiallyrewardingarms(indifferentbetweenstates2and3). Thisshowsthat
LAIF is able to infer the optimal policy and that our approach can reproduce
prior results on the T-maze.
Since CFFGs are inherently modular, they allow us to modify the inference
task without changing the model. To demonstrate, we now add δ-constraints
to the controlvariables. This correspondsto selecting the MAP estimate ofthe
control posterior [25] and results in the CFFG shown in Fig. 29. The schedule
andallmessagesremainthesameasourpreviousexperiment-howevernowthe
agent will select the most likely course of action instead of providing us with a
posterior distribution.
36
Cat Cat
δ δ
Cat TM = TM
B ··· B B ··· B
1 4 1 4
T T
Cat Cat
Figure 29: CFFG for the T-maze model with additional δ-constraints
Performing this experiment yields the policy shown in Fig. 30
Controls at time step 1 Controls at time step 2
0.0 0.0 1.0 0.0
0.0 0.0
1.0 0.0
Figure 30: Posterior controls for the T-maze experiment with δ-constraints
Fig.30onceagainshowsthattheagentisabletoinfertheoptimalpolicyfor
solving the task. For repeated runs, the agent will randomly select to move to
either position 2 or 3 at the secondstep due to minute differences in the Monte
Carlo estimates used for computing the messages. The pointmass constraint
obscuresthis since it forces the marginalto put all mass on the MAP estimate.
37
The point of repeating the experiments with δ-constraints is not to show
that the behaviour of the agent changes dramatically. Instead, the idea is to
demonstrate that CFFGs allow for modular specification of AIF agents which
allows for adapting parts of the model without having to touch the rest. In
this case,the only partsofinference thatchangearethoseinvolvingthe control
marginal. This means all messages out of the P-substituted composite nodes
areunaffectedsincetheδ-constraintsareonlyappliedtothecontrolmarginals.
8 Conclusions
In this paper we have proposed a novel approach to AIF based on lagrangian
optimisation which we have named Lagrangian Active Inference (LAIF). We
demonstrated LAIF on a classic benchmark problem from the AIF literature
andfoundthat itinherits the epistemic drivethatis ahallmarkfeature of AIF.
LAIF presents three main advantages over previous algorithms for AIF.
Firstly, an advantage of LAIF is the computational efficiency afforded by
being able to pass backward messages instead of needing to perform forwards
rolloutsforeverypolicy. While LAIFis stillaniterativeprocedure,the compu-
tational complexity of each iteration scales linearly in the size of the planning
horizon T instead of exponentially.
A second advantage is that LAIF allows for directly inferring posteriors
over control signals instead of relying on a model comparison step based on
EFE/GFE. This means that LAIF unifies inference for perception, learning,
and actions into a single procedure without any overhead - it all becomes part
ofthesameinferenceprocedure. Seeourcompanionpaper[15]formoredetails.
Thirdly,LAIFisinherentlymodularandconsequentlyworksforfreelydefin-
ableCFFGs,whilepriorworkhasfocusedmostlyonspecificgenerativemodels.
Extensions to hierarchicalor heterarchicalmodels are straightforwardand only
require writing out the corresponding CFFG.
We have also introduced CFFG notation for writing down constraints and
P-subtitutionsonanFFG.CFFGsareusefulnotjustforAIFbutforspecifying
free energy functionals in general. CFFGs accomplish this through a simple
andintuitivegraphicalsyntax. OurhopeisthatCFFGscanbecomeastandard
tool similar to FFGs when it is desirable to write not just a model f but also
a family of approximating distributions Q. Specifically in the context of AIF
we have also introduced P-substitution as a way to modify the underlying free
energyfunctional. IndoingsowehaveformalisedtherelationbetweenAIFand
message passing on a CFFG, paving the way for future developments.
Infuturework,weplantoextendLAIFtomorenodeconstructionstofurther
open up the scope of available problems that can be attacked using AIF.
38
Acknowledgements
This research was made possible by funding from GN Hearing A/S. This work
is partofthe researchprogrammeEfficient DeepLearningwith projectnumber
P16-25 project 5, which is (partly) financed by the Netherlands Organisation
for Scientific Research (NWO).
We gratefully acknowledge stimulating discussions with the members of the
BIASlab research group at the Eindhoven University of Technology and mem-
bersofVERSESResearchLab,inparticularIsmailSenoz,BartvanErp,Dmitry
Bagaev,Karl Friston, Chris Buckley, Conor Heins and Tim Verbelen.
References
[1] SemihAkbayrak,IvanBocharov,andBertdeVries.“Extendedvariational
message passing for automated approximate Bayesian inference”. In: En-
tropy 23.7 (2021), p. 815.
[2] DmitryBagaevandBertdeVries.“ReactiveMessagePassingforScalable
Bayesian Inference”. en. In: (2022). Submitted to the Journal of Machine
Learning Research.
[3] Théophile Champion et al. “Branching Time Active Inference: The the-
oryanditsgenerality”.In:NeuralNetworks 151(2022),pp.295–316.issn:
0893-6080.doi:https://doi.org/10.1016/j.neunet.2022.03.036.url:
https://www.sciencedirect.com/science/article/pii/S0893608022001149.
[4] LancelotDa Costaet al.“Active inferenceondiscrete state-spaces:a syn-
thesis”. en. In: arXiv:2001.07203 [q-bio] (Jan. 2020). arXiv: 2001.07203.
url: http://arxiv.org/abs/2001.07203(visited on 01/22/2020).
[5] KarlFriston.“Afreeenergyprincipleforaparticularphysics”.In:arXiv:1906.10184
[q-bio] (June2019).arXiv:1906.10184.url:http://arxiv.org/abs/1906.10184
(visited on 06/12/2020).
[6] Karl Friston et al. “Active inference and epistemic value”. In: Cogni-
tive Neuroscience 6.4 (Oct. 2015), pp. 187–214. issn: 1758-8928. doi:
10.1080/17588928.2015.1020053.url:https://doi.org/10.1080/17588928.2015.1020053
(visited on 09/09/2019).
[7] KarlFristonetal.“SophisticatedInference”.In: Neural Computation 33.3
(Mar. 2021),pp. 713–763.issn: 0899-7667.doi: 10.1162/neco_a_01351.
url:https://doi.org/10.1162/neco_a_01351(visitedon12/22/2021).
[8] KarlJ.Fristonetal.“Actionandbehavior:afree-energyformulation”.en.
In:BiologicalCybernetics 102.3(Mar.2010),pp.227–260.issn:0340-1200,
1432-0770.doi:10.1007/s00422-010-0364-z.url:http://link.springer.com/10.1007/s00422-010-0364-z
(visited on 04/13/2020).
[9] KarlJ.Fristonetal.SPM12toolbox,http://www.fil.ion.ucl.ac.uk/spm/software/.
2014.
39
[10] DanijarHafneretal.“ActionandPerceptionasDivergenceMinimization”.
In:arXiv:2009.01791 [cs,math,stat] (Oct.2020).arXiv:2009.01791.url:
http://arxiv.org/abs/2009.01791(visited on 01/07/2021).
[11] Conor Heins et al. “pymdp: A Python library for active inference in dis-
crete state spaces”. In: arXiv preprint arXiv:2201.03904 (2022).
[12] Hilbert J.Kappen, Vicenç Gómez, andManfredOpper.“Optimalcontrol
as a graphical model inference problem”. en. In: Machine Learning 87.2
(May2012),pp.159–182.issn:0885-6125,1573-0565.doi:10.1007/s10994-012-5278-7.
url:http://link.springer.com/10.1007/s10994-012-5278-7(visited
on 02/07/2020).
[13] Magnus Koudahl, Christopher L Buckley, and Bert de Vries. “A Mes-
sage Passing Perspective on Planning Under Active Inference”. In: Active
Inference: Third International Workshop, IWAI 2022, Grenoble, France,
September19, 2022, Revised SelectedPapers.Springer.2023,pp.319–327.
[14] MagnusT.Koudahl,WouterM.Kouw,andBertdeVries.“OnEpistemics
inExpectedFreeEnergyforLinearGaussianStateSpaceModels”.In:En-
tropy 23.12(Nov.2021),p.1565.issn:1099-4300.doi:10.3390/e23121565.
url: http://dx.doi.org/10.3390/e23121565.
[15] Thijs van de Laar, Magnus Koudahl, and Bert de Vries. “Realising Syn-
thetic ActiveInferenceAgents,PartII:VariationalMessageUpdates”.In:
(2023). arXiv: 2306.02733[stat.ML].
[16] David MacKay. Information theory, inference and learning algorithms.
Cambridge university press, 2003.
[17] Beren Millidge et al. “Understanding the Origin of Information-Seeking
ExplorationinProbabilisticObjectivesforControl”.In:arXiv:2103.06859
[cs] (June2021).arXiv:2103.06859.url:http://arxiv.org/abs/2103.06859
(visited on 07/02/2021).
[18] Thomas P. Minka. “Expectation Propagation for Approximate Bayesian
Inference”. In: Proceedings of the Seventeenth Conference on Uncertainty
in Artificial Intelligence. Uai’01. San Francisco, CA, USA: Morgan Kauf-
mann Publishers Inc., 2001, pp. 362–369. isbn: 978-1-55860-800-9. url:
http://dl.acm.org/citation.cfm?id=2074022.2074067(visitedon04/04/2018).
[19] Thomas Parr and Karl J. Friston. “Generalisedfree energy and active in-
ference”. en. In: Biological Cybernetics 113.5-6 (Dec. 2019), pp. 495–513.
issn: 0340-1200, 1432-0770. doi: 10.1007/s00422-019-00805-w. url:
http://link.springer.com/10.1007/s00422-019-00805-w(visited on
08/18/2020).
[20] JudeaPearl.“ReverendBayesonInferenceEngines:ADistributedHierar-
chical Approach”.In: Proceedings of the Second AAAI Conference on Ar-
tificial Intelligence.Aaai’82.Pittsburgh,Pennsylvania:AAAIPress,1982,
pp.133–136.url:http://www.aaai.org/Papers/AAAI/1982/AAAI82-032.pdf
(visited on 07/28/2017).
40
[21] ChristophReller.State-spacemethods in statistical signal processing: New
ideas and applications. Vol. 23. ETH Zurich, 2013.
[22] Noor Sajid, Philip J. Ball, and Karl J. Friston. “Active inference: demys-
tified and compared”. en. In: arXiv:1909.10863 [cs, q-bio] (Jan. 2020).
arXiv:1909.10863.url: http://arxiv.org/abs/1909.10863(visited on
02/10/2020).
[23] Simo Sarkka. Bayesian Filtering and Smoothing. en. Cambridge: Cam-
bridgeUniversityPress,2013.isbn:978-1-139-34420-3.doi:10.1017/cbo9781139344203.
url:http://ebooks.cambridge.org/ref/id/CBO9781139344203(visited
on 04/04/2019).
[24] SarahSchwöbel,StefanKiebel,andDimitrijeMarković.“ActiveInference,
BeliefPropagation,andtheBetheApproximation”.en.In:NeuralCompu-
tation 30.9 (Sept. 2018),pp. 2530–2567.issn: 0899-7667,1530-888x.doi:
10.1162/neco_a_01108.url:https://direct.mit.edu/neco/article/30/9/2530-2567/8396
(visited on 05/13/2021).
[25] İsmailŞenözetal.“VariationalMessagePassingandLocalConstraintMa-
nipulation in Factor Graphs”. en. In: Entropy 23.7 (July 2021). Number:
7 Publisher: Multidisciplinary Digital Publishing Institute, p. 807. doi:
10.3390/e23070807.url:https://www.mdpi.com/1099-4300/23/7/807
(visited on 10/04/2021).
[26] Ryan Smith, KarlFriston, and Christopher Whyte. A Step-by-Step Tuto-
rial on Active Inference and its Application to Empirical Data.Jan.2021.
doi: 10.31234/osf.io/b4jm6.
[27] John Winn and Christopher M. Bishop. “Variational message passing”.
In: Journal of Machine Learning Research 6.4 (2005), pp. 661–694. url:
http://www.jmlr.org/papers/volume6/winn05a/winn05a.pdf.
[28] Dan Zhang et al. “Unifying message passing algorithms under the frame-
work of constrained Bethe free energy minimization”. In: IEEE Transac-
tions on Wireless Communications 20.7 (2021), pp. 4144–4158.
A Expected free energy
ThissectionwillfocusonthemostcommonalternativefunctionalusedforAIF,
the expected free energy (EFE).
EFEisthestandardchoiceforAIFmodelsandiswhatisfoundinmostAIF
focused software such as SPM [9] and PyMDP [11]. In this section, we will cover
the details of how EFE is commonly treated.
EFE is defined specifically overfuture time steps and on a particular choice
ofgenerativemodel. Themodelinquestionisastatespacemodel(SSM) ofthe
form
41
T
p(x,z |uˆ)=p(z ) p(x |z ) p(z |z ,uˆ ) (53)
t k k k k−1 k
k=t+1
State Y Observation Transition
prior model model
|{z} | {z } | {z }
where uˆ denotes a particular control parameter that is known apriori and
k
fixed. Note that (53) does not include a prior over actions u and is instead
conditionalona fixed policy uˆ =uˆ . We use theˆnotationonu to indicate
t+1:T
that it is treated as a known value instead of a random variable.
TheEFEisevaluatedasafunctionofaparticularpolicy. TheEFEisdefined
as [6]
T
q(z |uˆ )
k k
G[q;uˆ]= p(x |z )q(z |uˆ )log dx dz (54a)
x k k k k k k
p(x ,z |uˆ )
k k k
k=t+1
X
VFEconditionedonuˆk
T
= p(x |z )q | (z |uˆ )log {zq(z k |uˆ k ) } −logp(x )dx dz (54b)
x k k k k k k k
p(z |x ,uˆ )
k k k
k=t+1
X
where p(x ) denotes a goal prior over preferred observations. Note that
k
p(x ) is not part of the generative model in Eq. (53). To compute Eq. (54) we
k
also need
p(x ,z |uˆ )= p(x |z )p(z |z ,uˆ )q(z )dz (55)
k k k k k k k−1 k k−1 k−1
Z
meaning we use the forwardprediction from the previous time step to com-
pute p(x ,z |uˆ ). If we further we assume that
k k k
q(x ,z |uˆ )=p(x |z )q(z |uˆ ) (56)
k k k k k k k
It can be shown [4, 6, 14, 22] that (54) can be decomposed into a bound on
a mutual informationterm and a cross-entropyloss between predicted observa-
tions and the goal prior.
B VFE and GFE
In this section, we walk through how the formulation of GFE given by [19]
reduces to the VFE when observations are available. To show how this comes
about, we note that [19] assumes that
δ(x −xˆ ) if k ≤t
q(x |z )= k k . (57)
k k
(p(x
k
|z
k
) if k >t
42
where t is the current time step and xˆ denotes the observed data point at
k
time step k. This is a problematic move since q(x |z ) is no longer a function
k k
of z for k ≤t. However if we do take Eq. (16) as valid and further assume
k
q(x ,z |uˆ )=q(x |z )q(z |uˆ ) (58a)
k k k k k k k
p˜(x )=1 if k ≤t (58b)
k
and note that
q(x |uˆ )= q(x |z )q(z |uˆ )dz . (59)
k k k k k k k
Z
We can plug this result into (15) and obtain for k ≤t
t
q(x |uˆ )q(z |uˆ )
q(x |z )q(z |uˆ )log k k k k dx dz
x k k k k p(x ,z |uˆ )p˜(x ) k k
k k k k
k=1
X
t
[q(x |z )q(z |uˆ )]dz q(z |uˆ )
k k k k k k k
= q(x |z )q(z |uˆ )log dx dz
x k k k k p(x ,z |uˆ ) k k
k=1 R k k k
X
(60a)
where the last line follows from Eq. (58b). Then by (16) we have
t
[δ(x −xˆ )q(z |uˆ )]dz q(z |uˆ )
k k k k k k k
= δ(x −xˆ )q(z |uˆ )log dx dz
x k k k k p(x ,z |uˆ ) k k
k=1 R k k k
X
(61)
Now we pull δ(x −xˆ ) out of the integral and solve to find
k k
Integratesto1
t δ(x k −xˆ k ) q(z k |uˆ k )dz k q(z k |uˆ k )
= x δ(x k −xˆ k )q(z k |uˆ k )log z hZ p(x ,z }| |uˆ ) i { dx k dz k
k k k
k=1
X
(62a)
t
q(z |uˆ )
= q(z |uˆ )log k k dz (62b)
k k p(xˆ ,z |uˆ ) k
k k k
k=1Z
X
which we can recognise as a VFE with data constraints.
43
C The transition mixture node
Inthissection,wederivetheMPrulesrequiredfortheTransitionMixturenode
used in our experiments on policy inference. Before doing so, we first establish
some preliminaryresults forthe categoricaldistribution andthe standardtran-
sition node. In this section we will also on occasion use an overbar to indicate
an expectation, E [g(x)]=g(x) The categoricaldistribution is given by
q(x)
I
Cat(x|z)= zxi (63a)
i
i=1
Y
I
= x z (63b)
i i
i=1
X
where x and z are both one-hot encoded vectors. The move Eq. (63a) to
Eq. (63b) is possible only because x is one-hot encoded. Similarly, we have for
the standard Transition node that
I I
Cat(x|Az)= A
xizj
(64a)
ij
i=1j=1
YY(cid:2) (cid:3)
I I
= x z A (64b)
i j ij
i=1j=1
XX
and again, since x and z are one-hot encoded
I I
logCat(x|Az)=log A
xizj
(65a)
ij
" #
i=1j=1
YY(cid:2) (cid:3)
I I
= log A
xizj
(65b)
ij
" #
i=1j=1
XX
I I
= x z logA (65c)
i j ij
i=1j=1
XX
Now, we are ready to startderivations for the transitionmixture node. The
transition mixture node has the node function
44
y
f(x,y,z,A)= Cat x|A z k (66a)
k
k
Y (cid:0) (cid:1)
y
k
= A
zixj
(66b)
k ij
" #
k i j
Y YY(cid:2) (cid:3)
= A
zixjy
k (66c)
ijk
i,j,k
Y
Where we use i to index over columns, j to index over rows and k to index
over factors. A is a 3-tensor that is normalised over columns. In other words,
each slice of A corresponds to a valid transition matrix and slices are indexed
by k. We assume a structured mean-field factorisation such that
q(x,y,z,A)=q(x,y,z) q(A ) (67)
k
k
Y
The CFFG of the transition mixture node is shown in Fig. 31
z
A
1
y
TM
A
k
x
Figure 31: The transition mixture node
We now define
logf (x,y,z)=E logf(x,y,z,A) (68a)
A q(A)
h i
=E z x y logA (68b)
q(A) i j k ijk
hX ijk i
= y E z x logA (68c)
k q(Ak) i j ijk
X k hX ij i
= y z x logA (68d)
k i j k
ij
X k X ij h i
45
which implies that
f (x,y,z)=exp y z x logA (69a)
A k i j k
ij!
X k X ijk h i
zixjy
k
= exp logA (69b)
k
Y ijk (cid:16) (cid:17)
= z x y exp logA (69c)
i j k k
ij
X ijk (cid:16) (cid:17)
A˜
ijk
| {z }
Now, we are ready to derive the desired messages. The first message ν(·)
will be towards x. If we use µ(y) to denote the incoming message on the edge
y (similar for µz), then the message towards x is given by
ν(x)= µ(z)µ(y)exp E logf(x,y,z,A) (70a)
q(A)
X z X y (cid:16) (cid:17)
= µ(z)µ(y)f (x,y,z) (70b)
A
X z X y (cid:17)
= µ(z)µ(y) z x y A˜ (70c)
i j k ijk
z y ijk
XX X
Assuming the incoming messages are Categoricallydistributed, we can con-
tinue as
=E E z x y A˜ (71a)
Cat(z|πz) Cat(y|πy) i j k ijk
ijk
X
= π π x A˜ (71b)
zi yk j ijk
ijk
X
= π π A˜
xj
(71c)
zi yk ijk
Y j hX ik i
where the last line follows from x being one-hot encoded. At this point, we
can recognise the message
π π A˜
ν(x)∝Cat(x|ρ) where ρ = ik zi yk ijk (72)
j π π A˜
Pijk zi yk ijk
By symmetry, similar results hold for messa P ges ν(z) and ν(y). ν(z) is given
by
46
ν(z)= µ(x)µ(y)f (x,y,z) (73a)
A
x y
XX
π π A˜
∝Cat(z|ρ) where ρ = jk xj yk ijk . (73b)
i π π A˜
Pijk xj yk ijk
Similarly, ν(y) evaluates to P
ν(y)= µ(x)µ(z)f (x,y,z) (74a)
A
x z
XX
π π A˜
∝Cat(y|ρ) where ρ = ij x,j z,i ijk . (74b)
k π π A˜
Pijk x,j z,i ijk
To compute the message towards the n’thPcandidate transition matrix A ,
n
we will need to take an expectation with respect to q(x,y,z). It is given by
q(x,y,z)=µ(y)µ(x)µ(z)f (x,y,z) (75a)
A
= π xjπziπ y kA˜xjziy k (75b)
xj zi yk ijk
ijk
Y
= π π π A˜
xjziy
k (75c)
xj zi yk ijk
Y ijkh i
∝Cat(x,y,z|B) (75d)
where B is a three-dimensional contingency tensor with entries
π π π A˜
B = xj zi yk ijk (76)
ijk π π π A˜
ijk xj zi yk ijk
Now we can compute the messPage towards a transition matrix A as
n
logν(A )=E logf(x,y,z,A) (77a)
n q(Ak\n)q(x,y,z)
h i
=E x z y logA (77b)
q(Ak\n)q(x,y,z) j i k ijk
hX ijk i
=E B logA (77c)
q(Ak\n) ijk ijk
hX ijk i
= B logA + B logA (77d)
ijk ijk ijn ijn
X k\nhX ijk i X ij
Constantw.r.tAn
∝t|r B ijn log{zA ijn } (77e)
(cid:16) (cid:17)
47
which implies that
ν(A )∝Dir(A |B +1). (78)
n n n
WeseethatmessagestowardseachcomponentinAaredistributedaccording
toaDirichletdistributionwithparametersB +1. Thefinalexpressionweneed
n
to derive is the averageenergy termfor the transitionmixture node. It is given
by
U [q]=−E logf(x,y,z,A) (79a)
x q(A)q(x,y,z)
(cid:16) (cid:17)
=−E f (x,y,z) (79b)
q(x,y,z) A
(cid:16) (cid:17)
=−E x z y logA (79c)
q(x,y,z) j i k k ij
hX ijk (cid:2) (cid:3) (cid:17)
=− B logA (79d)
ijk k ij
ijk
X (cid:2) (cid:3)
=− tr BTlogA (79e)
k k
X k (cid:16) (cid:17)
48

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Realising Synthetic Active Inference Agents, Part I: Epistemic Objectives and Graphical Specification Language"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
