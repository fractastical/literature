=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: The Work Capacity of Channels with Memory: Maximum Extractable Work in Percept-Action Loops
Citation Key: fiderer2025work
Authors: Lukas J. Fiderer, Paul C. Barth, Isaac D. Smith

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Key Terms: work, percept, future, action, environment, loops, channels, capacity, extractable, memory

=== FULL PAPER TEXT ===

The Work Capacity of Channels with Memory:
Maximum Extractable Work in Percept-Action Loops
Lukas J. Fiderer,∗ Paul C. Barth, Isaac D. Smith, and Hans J. Briegel
Universita¨t Innsbruck, Institut fu¨r Theoretische Physik, Technikerstraße 21a, A-6020 Innsbruck, Austria
Predicting future observations plays a central role in machine learning, biology, economics, and
manyotherfields. Itliesattheheartoforganizationalprinciplessuchasthevariationalfreeenergy
principle and has even been shown—based on the second law of thermodynamics—to be necessary
for reaching the fundamental energetic limits of sequential information processing. While the use-
fulnessofthepredictiveparadigmisundisputed,complexadaptivesystemsthatinteractwiththeir
environment are more than just predictive machines: they have the power to act upon their envi-
ronmentandcausechange. Inthiswork,wedevelopaframeworktoanalyzethethermodynamicsof
informationprocessinginpercept-actionloops—amodelofagent–environmentinteraction—allowing
us toinvestigate thethermodynamicimplications of actionsand percepts on equalfooting. To this
end, we introduce the concept of work capacity—the maximum rate at which an agent can expect
to extract work from its environment. Our results reveal that neither of two previously estab-
lished design principles for work-efficient agents—maximizing predictive power and forgetting past
actions—remains optimal in environments where actions have observable consequences. Instead,
a trade-off emerges: work-efficient agents must balance prediction and forgetting, as remembering
pastactionscanreducetheavailablefreeenergy. Thishighlightsafundamentaldeparturefromthe
thermodynamics of passive observation, suggesting that prediction and energy efficiency may be at
odds in active learning systems.
I. INTRODUCTION
Percept-action loops—cycles in which an agent per-
ceives its environment, processes and stores information,
and acts to influence future perception—underlie adap-
tive behavior in both biological and artificial systems.
Suchloopscanbeobservedacrossvariousdomains,from
humans learning chess, to animals foraging, to artificial
intelligence models engaging in dialogue. Despite the di-
verse range of examples, certain principles governing the
energetics of these processes are shared across domains. FIG. 1. Tape setting (a) and percept-action loop setting (b).
Energetic considerations in biology have been linked In the tape setting, (a), an agent processes symbols S from
t
to a wide range of animal behaviors and physiological a pre-existing tape. Outgoing symbols A t do not influence
processes. An example from the former includes the future inputs. In the percept-action loop setting, (b), the
agent interacts with an environment (Env.) in rounds. In
energy-saving flight patterns of albatrosses [1] and from
roundt,theagentprovidesanaction symbolA andreceives
the latter information processing in the brain, where en- t
a percept symbol S from the environment. Both the agent
ergyconsumptionassociatedwithneuralsignalingismin- t
and environment can have memory, allowing future percepts
imized through efficient coding strategies [2, 3]. At the
to depend on past actions.
molecular level, ribosomes have been shown to perform
simple decoding computations at energy costs within an
order of magnitude of Landauer’s bound—significantly
outperforming even the most advanced supercomputers lem to an information-theoretic model of percept-action
[4]. Indeed, in artificial intelligence, the energetic cost of loops. By abstracting away implementation-dependent
supercomputers is becoming an increasing concern, par- details, wederiveenergeticboundsthatarisesolelyfrom
ticularly in the training of large neural networks [5], re- the intrinsic cost of information processing, as analyzed
sultinginperformance-powertrade-offsinlargelanguage through nonequilibrium thermodynamics [7, 8]. Indeed,
models [6]. inspiredbyMaxwell’sdemon,nonequilibriumthermody-
namics has been applied to investigate energetics in the
Thisraisesfundamentalquestions: Whataretheener-
tapesetting (seeFigure1),whereagentssequentiallypro-
geticlimitsofadaptiveinformationprocessinginpercept-
cess and modify symbols on a pre-existing tape [9–22].
action loops? And how should efficient agents be de-
In this framework, predictable correlations in the input
signed?
serve as an energetic resource, while generating correla-
These questions can be tackled by reducing the prob-
tionsintheoutputincursanenergycost. However,exist-
ingworkstypicallyassumestationaryinputpatternsand
∗ lukasjfiderer@gmail.com exclude feedback between agent and environment, leav-
5202
rpA
8
]GL.sc[
1v90260.4052:viXra
2
ing the thermodynamics of genuine percept-action loops sequences, known as stochastic processes, are written as
largely unexplored (but see [23] for a recent exception, A=A ,withanalogousnotationfortheirrealizations,
0:∞
investigating quantum processes with feedback). a
t:n
=(a
t
,a
t+1
,...,a
n−1
)∈An and a=a
0:∞
∈AN0.
In this work, we model both agent and environment The environment and agent are described by channels
as hidden Markov channels. By combining results from (conditional probability distributions) νenv and ηagt ,
S|A A|S
stochastic thermodynamics [7, 8] with the information which stochastically map actions A to percepts S and
theoryofhiddenMarkovmodels[24],weobtainaframe- vice versa. We assume these channels are causal (re-
work which goes beyond prior work situated in the tape specting time ordering such that future outputs cannot
setting by relaxing the assumption of stationary input influence past inputs) and admit a finite memory imple-
patterns and incorporating feedback between agent and mentation. The finite-memory assumption is both prac-
environment. This framework is the primary contribu- tical and ensures well-behaved asymptotics in percept-
tion of this work. actionloops. Theseconstraintsdefinewhatiscommonly
The central quantity of this work is the work referredtointheliteratureasafinite-state [25]orhidden
capacity—the optimal rate of energy production achiev- Markov channel [24] (see Supplemental Material C for a
able by any agent—which, analogous to communication more in-depth exposition).
capacity,isanintrinsicinformation-theoreticpropertyof
Definition 1. A channel νenv is an environment
the environment channel. S|A
channel, denoted as
The investigation of work capacity in the framework
developed here leads to two key results: (i) in the ab-
env:=νenv, (1)
sence of feedback, where the agent’s percepts are not S|A
influenced by its actions, we extend prior results [17] if there exists a finite set of states Z, a distribution p
Z0
beyond the stationary regime, showing that agents can over Z, and a transition matrix Φ = (ϕ(j|i)) with i ∈
j,i
reach the work capacity of the environment if and only A×Z and j ∈S×Z such that
if they are maximally predictive of their percepts while
∞
choosing actions randomly, without retaining memory of (cid:88) (cid:89)
νenv(s|a)= penv(z ) ϕenv(s ,z |a ,z ) (2)
them; (ii) in the presence of feedback, maximally pre- S|A Z0 0 t t+1 t t
dictive agents aregenerally inefficient. Thiscounterintu- z t=0
itive result highlights crucial distinctions between cyclic where the sum runs over all z ∈ZN0. Then, the tuple
informationprocessinginpercept-actionloopsandlinear
information processing on a tape. envM:=(Φenv,penv) (3)
Z0
In the following sections, we first introduce the
is called a (hidden Markov) environment model of
percept-action loop framework (Section II) and define
νenv and z ∈Z the hidden states of the model.
what it means for an agent to be maximally predictive Y|X
(SectionIII).Wethenpresentourresultsontheworkca-
While a channel describes only the input-output be-
pacity of channels (Section IV) and the design principles
havior, a hidden Markov model provides an explicit
of work-efficient agents (Section V). Finally, we discuss
memory-basedmechanismthatgeneratestemporalcorre-
directions for future research and conclude by situating
lations. Agentsaredefinedanalogously,withthekeydis-
our findings in a broader context (Section VI).
tinction that the agent initiates the percept-action loop
by selecting a first action A (see Figure 2):
0
II. FRAMEWORK Definition 2. A channel ηagt is an agent channel,
A|S
denoted as
Weconsideraclassical(asopposedtoquantum)agent agt:=ηagt , (4)
interacting with a classical environment in discrete time A|S
steps (in the following called rounds) indexed by t∈N ,
0 if there exists a finite set of states M, a distribution
where N 0 denotes the nonnegative integers. In each pagt over A × M, and a transition matrix Θagt =
round t, the agent selects an action A and subsequently A0M0
t (θ(j|i)) with i∈S×M and j ∈A×M such that
receives a percept S (see Figure 1b). Since both the j,i
t
agent and environment may be stochastic, A and S are ∞
t t (cid:88) (cid:89)
randomvariablestakingvaluesinfinitealphabetsAand η A ag | t S (a|s)= pa A g 0 t M0 (a 0 ,m 0 ) θagt(a t+1 ,m t+1 |s t ,m t ),
S. Embedding the smaller alphabet into the larger, lets m t=0
us set A=S, which will be assumed in the following. (5)
Throughout this work, random variables are denoted
where the sum runs over all m∈MN0. Then, the tuple
by capital letters, their realizations by lowercase let-
ters,theiralphabets—suchasthesetsofpossibleactions agtM:=(Θagt,pagt ) (6)
and percepts—by calligraphic letters, and sequences of A0M0
random variables—interpreted as random variables on a is called a (hidden Markov) agent model, of agt and
product space—by A = (A ,A ,...,A ). Infinite m∈M the memory states of the model.
t:n t t+1 n−1
3
III. MAXIMALLY PREDICTIVE AGENTS
Foragiveninput-outputbehavioroftheagentanden-
vironment, agt→←env, what does it mean for an agent
to be as predictive as possible of its future percepts?
To approach this question, it is helpful to begin with
the following observation. In order to endow the agent
with knowledge that reduces uncertainty about future
percepts, a natural first step is to encode agtM→←env =
(Θagt,pagt ,νenv)intoitsfixedalgorithmicmemory. In
A0M0 S|A
what follows, we assume this is always the case.
FIG. 2. Circuit representation of percept-action loops, with
With this setup, the agent has access to the distribu-
timeflowingfromlefttoright. (a)Theagentandenvironment
tion of the underlying process, p , which results in
are modeled as channels with memory. (b) The agent and MAS
environmentarerepresentedbytheirhiddenMarkovmodels, anuncertaintyH(S t )aboutperceptS t ,whereH(S t )de-
characterized by finite adaptive memories M and Z . The notesShannonentropyinunitsofbits. If,inaddition,the
t t
transition matrices Θ and Φ remain fixed over time. agenttakesitsmemoryM intoaccountbeforeobserving
t
S , this memory reduces the agent’s expected (with re-
t
spect to memory states) uncertainty to H(S |M ). This
t t
An agent can be understood as possessing two types reduction in uncertainty,
ofmemory: (i)algorithmic memory, whichremainsfixed
for all times and stores the agent’s transition matrix Φ, I[M t ;S t ]=H(S t )−H(S t |M t ), (7)
effectively representing the agent’s algorithm (analogous
is simply the mutual information I[M ;S ] between S
to DNA in a biological context), and (ii) adaptive mem- t t t
and M , quantifying how much M enables the agent to
ory M,whichstoresinformationaboutthepastpercept- t t
predict S (see Supplemental Material A for some back-
action sequence and, through the action of Φ, influences t
ground on information measures).
future actions.
To enhance its predictive capabilities, the agent can
With the definitions of agent and environment chan-
store information from past percepts S and actions
nels, as well as their hidden Markov models, we define 0:t
A in its memory. Since the information that
percept-action loops as tuples consisting of an agent and 0:t+1
S A providesaboutS isgivenbyI[S A ;S ],
anenvironment. Tohighlightthattheagentandenviron- 0:t 0:t+1 t 0:t 0:t+1 t
we arrive at the following
mentmutuallyinteract,thesearedenotedasagt→←envor
agtM→←env,dependingonwhethertheagentisdescribed Definition 3. Let agt→←env be a percept-action loop. A
byitschanneloritsmodel(similarly,envcanbereplaced model agtM for agt is said to be maximally predictive,
with envM). or for short predictive, of percept S in round t if
t
Each percept-action loop model corresponds to an as-
sociated stochastic process. For instance, agt→←env de- I[A 0:t+1 S 0:t ;S t |M t ]=0, (8)
termines the input-output behavior of the agent and en-
and an agent model is said to be asymptotically mean
vironment, thereby defining the percept-action process
(a.m.) predictive if
AS =((A ,S ),(A ,S ),...) with distribution
0 0 1 1
⟨I[A S ;S |M ]⟩ =0, (9)
p =νenvηagt , 0:t+1 0:t t t t
AS S|A A|S
where
see also Figure 2a. Importantly, the stochastic process
n−1
1 (cid:88)
corresponding to agtM→←envM, which has a distribution ⟨•⟩ := lim • (10)
p includingboththeagent’sandtheenvironment’s t n→∞n
MASZ t=0
hidden memory, can be shown to form a finite-state
Markov chain. This constitutes the global Markov chain denotes the Ces´aro limit, the limit of the arithmetic
of a percept-action loop (see Supplemental Material D mean.
for a proof):
Note that eq. (8) expresses that the agent’s memory
M contains at least all the information from the past,
t
M A S Z →M A S Z →··· .
0 0 0 0 1 1 1 1 S A , which helps predicting the next percept, S ,
0:t 0:t+1 t
(see Figure 3), while eq. (9) requires this condition to
This Markovian property allows us to leverage exist- hold asymptotically on average [28].
ing results on finite-state Markov chains, ensuring that Interestingly, although environments, as per Defini-
theasymptoticdynamicsofpercept-actionloopsarewell- tion 1, are constrained to a finite number of hidden
behaved [26, 27] (see Supplemental Material B for an states, an agent may require a countably infinite num-
overview). ber of memory states to be predictive as t → ∞. This
4
The tape setting can be embedded within the percept-
Mt−1 Mt Mt+1
action loop framework [32] by making the environment
channel effectively generate the tape pattern, i.e., it acts
as a finite-state source of percepts unaffected by actions:
ν
S|A
(s|a) = ν
S|A
(s|a′) for all a,a′ ∈ AN0. Channels
At−1 St−1 At St At+1 with this property are also known as product channels
[33].
The following theorem reveals a remarkable property
of predictive agents in the stationary regime: being
a.m.predictive of the next percept is equivalent to be-
Zt−1 Zt Zt+1
ing predictive of all future percepts at all times.
FIG. 3. Bayesian network for a percept-action loop. Shown
Theorem 2. Let agtM→←env be such that the joint pro-
cess MAS of actions, percepts, and agent memory is
is a fragment for rounds t−1, t, and the beginning of round
t + 1. This type of Bayesian network plays an important stationary. Then, agtM is a.m.predictive, i.e.,
role in the information-theoretic framework underlying our
results (see Supplemental Material E for details). Note that ⟨I[A 0:t+1 S 0:t ;S t |M t ]⟩ t =0 (11)
to faithfully represent the dynamics of the agent and envi-
if and only if
ronment, auxiliary nodes (gray and reduced in size) are in-
cluded. The colorized nodes illustrate the condition for an
I[A S ;S |M ]=0 ∀t∈N . (12)
agenttobemaximallypredictiveinroundt: theagent’smem- 0:t+1 0:t t:∞ t 0
ory (blue) must store all information from past actions and
If in addition env is a product channel, agtM is
percepts S A (red) that is relevant for predicting the
0:t 0:t+1
current percept S (green). a.m.predictive if and only if
t
I[S ;S |M ]=0 ∀t∈N . (13)
0:t t:∞ t 0
is because the agent’s memory, which can be seen as a
See Supplemental Material F for a proof utilizing the
functionofthepast,A S ,mustserveasasufficient
0:t+1 0:t Markov conditions of the underlying Bayesian network
statistic for S for eq. (8) to vanish [29]. Computational
t (see Figure 3). The second part of the theorem connects
mechanics shows that there are channels that admit suf-
ourdefinitionofa.m.predictiveagentstotheonebyBoyd
ficientstatisticsonlywithacountablyinfinitenumberof
et al.[17] who define predictive agents via eq. (13) and
states [30]. In this work, instead of allowing agents infi-
another condition which is automatically fulfilled for the
nite memory, we consider so-called unifilar environment
type of channels considered in this work (see [34] for a
channels [25, 31], for which there always exist predictive
different notion of predictive agents) [35].
and a.m.predictive agents.
Definition 4. An environment model envM = (Φ,p )
is said to be unifilar if
Z0
IV. WORK CAPACITY OF CHANNELS
• p is a delta distribution and
Z0 So far, we have treated agents and environments as
• H(Z |A ,S ,Z )=0 for all t∈N . abstract information-processing systems. However, as
t+1 t t t 0
Landauerfamouslyquipped,informationisphysical [36]:
Anenvironmentchannel envissaidtobeunifilarifthere any implementation of an agent must ultimately rely
exists a unifilar model for it. on physical memory and dynamics subject to thermo-
dynamic laws. To analyze the energetic limits of such
Unifilar models have the useful property that the val-
implementations, we adopt a framework from stochas-
ues of A , S , and Z fully determine the value of Z
t t t t+1 tic thermodynamics that models the agent’s information
forallroundst,enablinganagent,giventheinitialvalue processing—described by its transition matrix Φagt—as
of Z , to perfectly track the hidden state of the environ-
0 a physical process acting on memory [7, 8]. We briefly
ment. The following theorem is based on this insight:
outline its assumptions.
In this framework, memory is represented by a phys-
Theorem 1. Let agt→←env be any percept-action loop.
ical system coupled to a thermal reservoir at temper-
If the environment channel is unifilar, then there exists
ature T. The system possesses a few degrees of free-
an a.m. predictive agent model agtM for agt.
dom, the information-bearing degrees of freedom, which
See Supplemental Material F for a proof. Before pre- are assumed to be meta-stable, i.e., their equilibration
sentingourresultsonworkcapacity,wefirstdemonstrate time τ is much larger than that of the system’s other
info
thatDefinition3forpredictiveagentsrecoversthedefini- degrees of freedom, τ . Information processing on
others
tionpreviouslyusedinthecontextofstationaryprocesses theinformation-bearingdegreesoffreedomiscarriedout
in the tape setting [17]. Note that a process X is sta- through an isothermal protocol, i.e., a protocol executed
tionary ifp =p foralln,t∈N andm>n. at constant temperature T, with a time scale such that
Xn:m Xn+t:m+t 0
5
Environment Channel env Cwork(env)
Noiseless 0
(cid:2) (cid:3)
Memoryless Invariant max H(S )−H(A )
0 0
pA0
Unifilar Product log|A|−h(S)
FIG. 4. An agent agt interacting with the cascade of two
TABLE I. Work capacity for different classes of environment
environment channel env and env .
1 2
channels(seeeq.(18)foradefinitionofh(S)andSupplemen-
tal Material G3 for a proof).
τ ≪ τ ≪ τ . The protocol has access to a
others protocol info
work reservoir for storing (or retrieving) work.
Under these assumptions, it can be shown [7, 8] that, does not exist [42]. Note, however, that here the limit
similar to equilibrium thermodynamics, the second law exists because the global Markov chain of the percept-
of thermodynamics sets an upper bound on the ex- action loop is asymptotically well-behaved (see Supple-
pected amount of work extractable from a system with mental Material B for details). We then arrive at the
information-bearing degrees of freedom X. This upper following
bound is a state function, known as the nonequilibrium
Definition 5. The work capacity Cwork of an envi-
free energy, F = U − k T ln2H(X), where U is the
B ronment channel env is defined as
memory system’s internal energy, k is the Boltzmann
B
constant. Note that we refer to the work as expected be- Cwork(env):= max W(agtM→←env). (16)
causeitistheworkthatcanbeexpectedtobeextracted agtM∈A→←env
on average based on the available knowledge about the
whereA→←env denotesthesetofallagentmodelswhichcan
input state p .
X
interact with env.
Inaddition, inordertofocusontheenergeticlimitsof
information processing alone, we assume that the inter-
Intuitively, the work capacity captures the maximum
nalenergylandscapeoverinformation-bearingdegreesof
rate at which an agent—optimally tailored to the envi-
freedom is flat and remains unchanged before and after
ronment channel—can expect to extract work, based on
executing the isothermal protocol, i.e., the internal en-
the second law of thermodynamics. The existing pro-
ergyU doesnotcontributetotheextractablework. Such
tocols for implementing transition matrices [16, 17, 19]
a memory model is also known as an information reser-
can be leveraged to construct optimal protocols for the
voir [37]. Then, the second law yields an upper bound
agent model agtM which maximizes eq. (16), making it,
on the expected extractable work W:
inprinciple,saturable(seeSupplementalMaterialG2for
W ≤H(X )−H(X ), (14) details).
out in
Returning to the question posed at the beginning of
from implementing Φ on X, mapping X in to X out . Here this section, the energetic limits of agents, in terms of
and throughout, all work expressions are understood to work rate, are determined by the work capacity of the
be in units of k B T ln2. environment channel.
The upper bound in eq. (14), imposed by the second Next we will provide some general properties of work
law, is tight in the sense that it can, in principle, be sat- capacity:
urated with protocols under idealized conditions. Con-
creteexamplesofsuchprotocolsaregivenin[16,17,19]. Theorem 3. For any environment channel env=νenv,
S|A
While our work is primarily concerned with the fun- work capacity Cwork(env) has the following properties:
damental limits imposed by the second law, it should
be noted that more realistic and resource-constrained (i) (Existence) Cwork(env) exists,
assumptions can be incorporated [38–41]. Within our
(ii) (Bounds) 0≤Cwork(env)≤ln|S|,
framework, this is most easily achieved when the ex-
tractable work can still be expressed through a state (iii) (Subadditivityunderchannelcascade,seeFigure4)
function, such as in [40], by replacing F with the new
state function. Cwork(env ◦env )≤Cwork(env )+Cwork(env ).
2 1 1 2
With this, the work rate, i.e., the asymptotically ex-
pected work per round that an agent model agtM can See Supplemental Material G3 for a proof. Note that
extract using the environment channel env is (see Sup- the bounds in Theorem 3 follow from the canonical
plemental Material G1 for a derivation) bounds on Shannon entropy.
W(agtM→←env)=⟨(H(A
t
|M
t
)−H(S
t
|M
t
))⟩
t
, (15)
Due to the Ces´aro limit, work capacity is generally
with the Ces´aro limit ⟨•⟩ defined in eq. (10). The ex- difficulttocompute. However, forspecialclassesofenvi-
t
istence of work rate is not guaranteed for arbitrary pro- ronment channels, the expression for work capacity sim-
cesses, as it is possible that the Ces´aro limit in eq. (15) plifies, as shown in Table I.
6
FIG.5. Amemorylessinvariantenvironmentwithbinaryper-
cept and action alphabets, A = S = {0,1}. The transition
labels follow the scheme percept|action : transition proba-
bility. Thetransitionontheleft(right)correspondstoaction
“0” (respectively, “1”).
FIG. 6. Set diagrams illustrating the relationships between
Fornoiseless environmentchannels[43],whereA =S different classes of agent models: those with maximum-
t t
entropyactions(mea),thosewhicharepredictive(pred),and
for all t ∈ N , the agent can predict a percept precisely
0 thosethatarework-efficient(eff). (a)(b)Appliestothemem-
totheextentthatithasremembereditspreviousaction,
orylessinvariantenvironmentchannelshowninFigure5(see
turning the tradeoff between actions and percepts into
Theorem 5). Unlike the tape setting, this environment in-
a zero-sum situation: H(A |M )−H(S |M ) = 0, and
t t t t volves feedback, forming a genuine percept-action loop.
work capacity vanishes.
For memoryless invariant environment channels,
whereν S|A (s|a)=
(cid:81)∞
t=0 ϕ(s t |a t )withthesameϕforall can be established. For a given environment env, three
t∈N 0 , we show that the absence of memory in the envi- subsets of the set of all agent models play a central role:
ronmentallowsonetoreducetheoptimizationoveragent
modelsineq.(16)toanoptimizationoverasingleaction. • A→←env: the set of random-action agent models. In
mea
For example, consider an environment env, as displayed the Ces`aro limit, these agents randomize their ac-
in Figure 5, with binary percept and action alphabets, tions without retaining memory of them, yielding
A = S = {0,1}, and with transition matrix Φenv given ⟨H(A t |M t )⟩ t = ln|A|. The subscript mea stands
by its coefficients ϕenv(j|0) = δ and ϕenv(j|1) = 1/2 for maximum entropy actions.
0,j
for j =0,1. For this environment, we find
• A→←env: the set of a.m.predictive agent models,
(cid:20) (cid:21) pred
1 3 1 which satisfy the a.m.predictive criterion (see Def-
Cwork(env)= ln + √ ≃0.272bits, (17)
2 4 2 inition 3).
which, in units of k B T ln2, is the work capacity of env. • A e →← ff env: thesetofwork-efficientagentmodels,whose
It can be reached by a memoryless agent √ which in every workrateequalstheworkcapacity(eq.(16))ofthe
round takes action 0 with probability 1/ 2. environment.
For unifilar product environment channels, percepts
arenotinfluencedbyactions. Consequently,tomaximize We now extend the results of Boyd et al. for station-
theexpressionineq.(15),theoptimalstrategyistomax- aryprocesses[17]byutilizingourframework—alongwith
imize H(A |M ), which corresponds to choosing actions thedefinitionsofa.m.predictiveandwork-efficientagent
t t
that are independent, identically distributed, and uni- models—to encompass all processes that can be gener-
formlyrandom. Crucially, theagentmustnotretainany ated by an environment, not just stationary ones:
information about its action in its memory. This results
Theorem4. Foranyunifilarproductenvironmentchan-
in H(A |M ) = log|A|. The second term in the work
t t nel env,
capacity expression, as shown in Table I, is the entropy
rate of the percept process S, A→←env =A→←env∩A→←env. (19)
eff mea pred
H(S )
h(S):= lim 0:n , (18) See Figure 6a for a set diagram illustrating the the-
n→∞ n orem. The proof (see Supplemental Material G4) re-
which was introduced by Shannon as the average uncer- lies on the expression for work capacity in unifilar prod-
tainty per symbol in a stochastic process [44]. It is also uct environments given in Table I. The assumption of
known, from the information-processing second law [45], unifilarityensures,byTheorem1,that(finite)predictive
that this entropy rate (in units of k B T ln2) represents agent models exist. This result establishes two design
the maximum rate of expected extractable work from a principlesforwork-efficientagentsinteractingwithapo-
stochastic process [46]. tentially nonstationary percept process: (i) randomizing
actions without retaining memory of them and (ii) em-
ploying predictive memory. These two principles can be
V. WORK-EFFICIENT AGENT MODELS directlylinkedtothetwotermsoftheworkrate,eq.(15),
where the first principle ensures that H(A |M ) is maxi-
t t
Finding agents that achieve work capacity is challeng- mized and the second principle ensures that H(S |M ) is
t t
ing, as it requires solving a nonlinear optimization prob- minimized.
lem (eq. (16)). However, for certain classes of environ- A natural question then arises: what happens when
ments, design principles for work-efficient agent models actions influence future percepts—that is, in genuine
7
percept-action loops? Based on Theorem 4, one might maximal rate of expected work extraction by an agent.
expect the same design principles to apply in these sce- Similar to communication capacity, work capacity is an
narios. In particular, it is not immediately clear why intrinsicinformationtheoreticpropertyofachannel. Ac-
anefficientagentmodelshouldnotbea.m.predictive, as cording to previously established design principles for
predictingitsperceptsreducestheuncertaintyH(S |M ), work-efficientagents—derivedinthecontextoflinearin-
t t
which contributes negatively to the work rate. However, formationprocessingonatape—anagent’sactions,from
the following theorem demonstrates that there exist en- its own perspective, should appear maximally random,
vironments where neither of these previously identified while its percepts should be as predictable as possible.
design principles is compatible with work-efficient agent Surprisingly, we find that neither of these two prin-
models. ciples remains valid in general. Most notably, maximal
predictability of percepts is no longer optimal.
Theorem5. Thereexistenvironmentchannelsenvsuch
This phenomenon arises specifically in percept-action
that the sets A
p
→←
r
e
e
n
d
v, A→←
m
e
e
n
a
v, and A→←
eff
env are all nonempty
loops with genuine feedback. In such settings, when pre-
and mutually exclusive.
dicting percepts requires remembering past actions, a
trade-off emerges and the goals of prediction and work-
See Supplemental Material G4 for a proof, and refer
efficiency diverge: as we prove in this work, there exist
to Figure 6b for a set diagram illustrating the theorem.
environmentsinwhichanyagentthatmaximizesworkef-
This result underscores a fundamental distinction be-
ficiencymustnecessarilyforgetcertainaspectsofitspast
tween the tape setting and the percept-action loop set-
actions—and,therefore,cannotbemaximallypredictive.
ting. In order to be maximally predictive of its percepts,
Buildingontheresultsestablishedinthiswork,several
an agent must, for some environments, retain informa-
natural directions for future research emerge:
tionaboutpastactions thatcarrypredictiveinformation
about future percepts. While doing so reduces the per- • Agents with goals—In this work, we considered
ceptentropyH(S |M ), therebyincreasingtheworkrate classes of agents with implicit objectives, such as
t t
(see eq. (15)), remembering actions reduces the action maximizing work rate or predictive power. A nat-
entropy H(A |M ), thereby decreasing the work rate. ural next step is to investigate agents with specific
t t
Conversely, randomizing actions without retaining goals within our framework. One approach is to
memory of them increases H(A |M ), but may drive fix a desired percept-action behavior, which corre-
t t
the environment into a less predictable regime, such sponds to specifying an agent channel. Then, the
that H(S |M ) increases. Crucially, there exist energetic limits of the agent’s behavior can be de-
t t
environments—such as the memoryless invariant envi- termined by optimizing over all models that im-
ronmentshowninFigure5—forwhichtheenergeticcosts plement this channel (see related ideas in the tape
outweigh the benefits of implementing either of the two setting [20, 22]). Alternatively, one could emulate
design principles. a reinforcement learning scenario by encoding re-
Consequently, the two design principles for work- wardsaspredictable(i.e.,low-entropy)percepts. In
efficient agents in the tape setting can no longer be pur- thiscase,anagentaimingtomaximizeitsworkrate
sued independently in percept-action loops. Instead, a could be guided toward desired behaviors through
tradeoff emerges between predictive memory and action suitable reward design.
forgetfulness, generally rendering both strategies subop-
• Dissipation in percept-action loops—If one
timal.
considersthatbothagentandenvironmentthermo-
dynamically implement their respective channels,
theagent’spositiveworkrateimpliesacorrespond-
VI. DISCUSSION AND FUTURE DIRECTIONS
ing work cost for the environment. In such a set-
ting,theenvironmentconvertsworkintostructured
Predictingfutureobservationsisacentralthemeacross
correlations, while the agent converts those corre-
various fields, including Bayesian and active inference
lations back into work. For memoryless channels,
[47], predictive analytics [48], computational mechanics
this conversion can happen without dissipation,
[49], and chaos theory [50]. It also plays a crucial role
with the energetic cost of implementing the envi-
in modern machine learning, particularly in transformer
ronment channel—known in the quantum context
modelsandlargelanguagemodels,whicharedesignedto
as the thermodynamic capacity [52, 53]—equaling
predict future states in a sequence [51].
the work capacity. However, for channels with
However, as we show in this work by analyzing the
memory, it remains an open question whether for
fundamental limits of information processing in percept-
any agt→←env the agent’s maximum work rate can
action loops, the mere act of remembering the past to
matchtheenvironment’sminimumworkcost. Any
predict the future has thermodynamic consequences. To
gap between these values would imply intrinsic en-
investigate this, we developed a framework for studying
tropy production in percept-action loops.
thestochasticthermodynamicsofinformationprocessing
in percept-action loops. Within this framework, we de- • Quantum work capacity—A natural extension
fine the work capacity of an environment channel as the of this work is to explore quantum generalizations
8
of work capacity. Our framework admits a quanti- Austrian Science Fund (FWF) [SFB BeyondC F7102,
zationbyreplacingclassicalchannelswithquantum 10.55776/F71]. For open access purposes, the author
combs, enabling analysis of percept-action loops has applied a CC BY public copyright license to any au-
in the quantum domain. This allows for study- thor accepted manuscript version arising from this sub-
ing fundamental quantum limits on work extrac- mission. We gratefully acknowledge support from the
tion and the design of quantum-enhanced agents European Union (ERC Advanced Grant, QuantAI, No.
[20–23, 54, 55]. 101055129). The views and opinions expressed in this
article are however those of the author(s) only and do
More broadly, our work opens the door to a search for not necessarily reflect those of the European Union or
newenergeticdesignprinciplestailoredtopercept–action the European Research Council - neither the European
loops with feedback. Such considerations may inform Union nor the granting authority can be held responsi-
novelorganizationalprinciplesforbiologicalandartificial bleforthem. LJFacknowledgessupportbytheAustrian
agents [56], moving beyond the predictive paradigm [19, Research Promotion Agency (FFG) and the European
30, 57]. Union via NextGeneration EU under Contract Number
FO999921407 (HDcode). LJF thanks Benjamin Morris
and Andrew Garner for early discussions that helped
VII. ACKNOWLEDGMENTS shape the direction of this work.
This research was funded in whole or in part by the
[1] G. Sachs, J. Traugott, A. P. Nesterova, G. Dell’Omo, versible Maxwell’s demon, Europhysics Letters 101,
F. Ku¨mmeth, W. Heidrich, A. L. Vyssotski, and 60001 (2013).
F. Bonadonna, Flying at No Mechanical Energy Cost: [12] A.BaratoandU.Seifert,UnifyingThreePerspectiveson
Disclosing the Secret of Wandering Albatrosses, PLoS Information Processing in Stochastic Thermodynamics,
ONE , 628 (2012). PRL 112, 090601 (2014).
[2] P. Lennie, The Cost of Cortical Computation, Current [13] J.HoppenauandA.Engel,Ontheenergeticsofinforma-
biology 13, 493 (2003). tion exchange, Europhysics Letters 105, 50002 (2014).
[3] L. Yu and Y. Yu, Energy-efficient neural information [14] N. Merhav, Sequence complexity and work extraction,
processing in individual neurons and neuronal networks, Journal of Statistical Mechanics: Theory and Experi-
Journal of Neuroscience Research 95, 2253 (2017). ment 2015, P06037 (2015).
[4] C. P. Kempes, D. Wolpert, Z. Cohen, and J. P´erez- [15] A. B. Boyd, D. Mandal, and J. P. Crutchfield,
Mercader, The thermodynamic efficiency of computa- Correlation-powered information engines and the ther-
tions made in cells across the range of life, Philosophi- modynamics of self-correction, PRE 95, 012152 (2017).
cal Transactions of the Royal Society A: Mathematical, [16] A.J.Garner,J.Thompson,V.Vedral,andM.Gu,Ther-
PhysicalandEngineeringSciences375,20160343(2017). modynamics of complexity and pattern manipulation,
[5] N. C. Thompson, K. Greenewald, K. Lee, G. F. Manso, PRE 95, 042140 (2017).
et al., The Computational Limits of Deep Learning, [17] A. B. Boyd, D. Mandal, and J. P. Crutchfield, Thermo-
arXiv:2007.05558 10 (2020). dynamicsofmodularity: Structuralcostsbeyondthelan-
[6] J. McDonald, B. Li, N. Frey, D. Tiwari, V. Gade- dauer bound, PRX 8, 031036 (2018).
pally, and S. Samsi, Great Power, Great Responsibil- [18] A. J. Garner, The fundamental thermodynamic bounds
ity: Recommendations for Reducing Energy for Train- on finite models, Chaos: An Interdisciplinary Journal of
ing Language Models, in Findings of the Association Nonlinear Science 31, 063131 (2021).
for Computational Linguistics: NAACL 2022, edited by [19] A. B. Boyd, J. P. Crutchfield, and M. Gu, Thermody-
M.Carpuat,M.-C.deMarneffe,andI.V.MezaRuiz(As- namicmachinelearningthroughmaximumworkproduc-
sociationforComputationalLinguistics,2022)pp.1962– tion, NJP 24, 083040 (2022).
1970. [20] T. J. Elliott, M. Gu, A. J. Garner, and J. Thomp-
[7] U. Seifert, Stochastic thermodynamics, fluctuation the- son,QuantumAdaptiveAgentswithEfficientLong-Term
orems and molecular machines, Reports on progress in Memories, PRX 12, 011007 (2022).
physics 75, 126001 (2012). [21] R. C. Huang, P. M. Riechers, M. Gu, and
[8] J.M.Parrondo,J.M.Horowitz,andT.Sagawa,Thermo- V. Narasimhachar, Engines for predictive work ex-
dynamicsofinformation,Naturephysics11,131(2015). traction from memoryful quantum stochastic processes,
[9] D.MandalandC.Jarzynski,Workandinformationpro- Quantum 7, 1203 (2023).
cessing in a solvable model of Maxwell’s demon, Pro- [22] J.Thompson,P.M.Riechers,A.J.Garner,T.J.Elliott,
ceedingsoftheNationalAcademyofSciences109,11641 andM.Gu,Energeticadvantagesforquantumagentsin
(2012). onlineexecutionofcomplexstrategies,arXiv:2503.19896
[10] D. Mandal, H. Quan, and C. Jarzynski, Maxwell’s Re- 10.48550/arXiv.2503.19896 (2025).
frigerator: AnExactlySolvableModel,PRL111,030602 [23] G. Zambon and G. Adesso, Quantum processes as ther-
(2013). modynamic resources: the role of non-markovianity,
[11] A. C. Barato and U. Seifert, An autonomous and re- arXiv:2411.05559 10.48550/arXiv.2411.05559 (2024).
9
[24] Y. Ephraim and N. Merhav, Hidden Markov processes, functional thermodynamics in autonomous Maxwellian
IEEE Transactions on information theory 48, 1518 ratchets, NJP 18, 023049 (2016).
(2002). [46] Iftheperceptprocessisastationaryfinite-stateMarkov
[25] R. G. Gallager, Information theory and reliable commu- chain,aclosed-formexpressionfortheentropyrateexists
nication, Vol. 588 (Springer, 1968). [44].
[26] R. B. Ash, Basic Probability Theory (Courier Corpora- [47] T.Parr,G.Pezzulo,andK.J.Friston,Active Inference:
tion, 2008). TheFreeEnergyPrincipleinMind,Brain,andBehavior
[27] M. Iosifescu, Finite Markov Processes and Their Appli- (MIT Press, 2022).
cations (Courier Corporation, 2014). [48] D. T. Larose, Data Mining and Predictive Analytics
[28] For example, eq. (9) is satisfied if eq. (8) holds true for (John Wiley & Sons, 2015).
all times, or it can be satisfied when the summands in [49] J. P. Crutchfield, Between order and chaos, Nature
eq. (9) decay sufficiently quickly. Physics 8, 17 (2012).
[29] T.M.CoverandJ.A.Thomas,ElementsofInformation [50] S. Boccaletti, C. Grebogi, Y.-C. Lai, H. Mancini, and
Theory (Wiley, 2005). D. Maza, The control of chaos: theory and applications,
[30] N. Barnett and J. P. Crutchfield, Computational me- Physics reports 329, 103 (2000).
chanics of input–output processes: Structured transfor- [51] T.Lin,Y.Wang,X.Liu,andX.Qiu,Asurveyoftrans-
mations and the epsilon-transducer, J. Stat. Phys. 161, formers, AI open 3, 111 (2022).
404 (2015). [52] M. Navascu´es and L. P. Garc´ıa-Pintos, Nonthermal
[31] R. B. Ash, Information Theory, Interscience Tracts in Quantum Channels as a Thermodynamical Resource,
Pure and Applied Mathematics No. 19 (John Wiley & PRL 115, 010405 (2015).
Sons). [53] P.Faist,M.Berta,andF.Branda˜o,ThermodynamicCa-
[32] Technically, this also requires allowing for an infinite pacity of Quantum Processes, PRL 122, 200601 (2019).
number of hidden states in the environment to generate [54] M.Gu, K.Wiesner, E.Rieper,andV.Vedral,Quantum
all percept processes allowed in the tape setting. mechanicscanreducethecomplexityofclassicalmodels,
[33] R. M. Gray, Probability, random processes, and ergodic Nat. Commun. 3, 762 (2012).
properties, Vol. 1 (Springer, 2009). [55] V. Dunjko, J. M. Taylor, and H. J. Briegel, Quantum-
[34] S.Still,D.A.Sivak,A.J.Bell,andG.E.Crooks,Ther- enhanced machine learning, PRL 117, 130501 (2016).
modynamics of Prediction, PRL 109, 120604 (2012). [56] A.RupeandJ.P.Crutchfield,Onprinciplesofemergent
[35] In fact, the other condition is I[S ;M | S ] = 0, organization, Physics Reports 1071, 1 (2024).
0:t t t:∞
whichcorrespondstoad-separationintheBayesiannet- [57] K. Friston, Life as we know it, Journal of the Royal So-
workunderlyingthepercept–actionloop(seeSupplemen- ciety Interface 10, 20130475 (2013).
tal Material E for details on d-separation). [58] A. S. Klyubin, D. Polani, and C. L. Nehaniv, Represen-
[36] R.Landauer,InformationisPhysical,PhysicsToday44, tations of Space and Time in the Maximization of In-
23 (1991). formation Flow in the Perception-Action Loop, Neural
[37] S. Deffner and C. Jarzynski, Information Processing computation 19, 2387 (2007).
and the Second Law of Thermodynamics: An Inclusive, [59] Note that the labeling convention is such that the value
Hamiltonian Approach, PRX 3, 041003 (2013). left of the colon in the subscript is included in the se-
[38] A. Kolchinsky and D. H. Wolpert, Dependence of dis- quence, while the value to the right is not.
sipation on the initial distribution over states, Journal [60] R. M. Gray, Entropy and information theory, first, cor-
of Statistical Mechanics: Theory and Experiment 2017, rected ed. (Springer Science & Business Media, 2011).
083202 (2017). [61] D. Hankerson, G. A. Harris, and P. D. Johnson Jr, In-
[39] N.Shiraishi,K.Funo,andK.Saito,SpeedLimitforClas- troductiontoInformationTheoryandDataCompression
sical Stochastic Processes, PRL 121, 070601 (2018). (CRC press, 2003).
[40] A. Kolchinsky and D. H. Wolpert, Work, Entropy Pro- [62] R. W. Yeung, A First Course in Information Theory
duction,andThermodynamicsofInformationunderPro- (Springer US, 2002).
tocol Constraints, PRX 11, 041024 (2021). [63] A. Kolchinsky, A novel approach to the partial informa-
[41] D. H. Wolpert, J. Korbel, C. W. Lynn, F. Tasnim, J. A. tion decomposition, Entropy 24, 403 (2022).
Grochow,G.Karde¸s,J.B.Aimone,V.Balasubramanian, [64] R. W. Yeung, A new outlook on Shannon’s information
E.DeGiuli,D.Doty,etal.,Isstochasticthermodynamics measures, IEEE transactions on information theory 37,
the key to understanding the energy costs of computa- 466 (1991).
tion?, Proceedings of the National Academy of Sciences [65] H. K. Ting, On the Amount of Information, Theory of
121, e2321112121 (2024). Probability & Its Applications 7, 439 (1962).
[42] An illustrative example of a sequence (a ) where the [66] H. C. Tijms, A First Course in Stochastic Models (John
t t
Cesa´rolimit⟨a ⟩ failstoexistis0110000...,whereone Wiley and sons, 2003).
t t
zero is followed by twice as many ones, followed again [67] R. G. Gallager, Discrete Stochastic Processes (Springer,
by twice as many zeros, and so on. This results in an 1996).
oscillating arithmetic mean 1/n
(cid:80)n
a as n→∞. [68] R. E. Edwards, Fourier Series: A Modern Introduction
t=1 t
[43] Thetermnoiselesschannel isinheritedfromcommunica- Volume 1, 2nd ed. (Springer, 1979).
tiontheory,whereitreferstoanidealchannelthattrans- [69] J. R. Munkres, Topology: Pearson New International
mits input symbols without alteration—that is, without Edition (Pearson Higher Ed, 2013).
introducing noise. [70] J.C.KiefferandM.Rahe,MarkovChannelsareAsymp-
[44] C. E. Shannon, A mathematical theory of communica- totically Mean Stationary, SIAM Journal on Mathemat-
tion, The Bell system technical journal 27, 379 (1948). ical Analysis 12, 293 (1981).
[45] A.B.Boyd,D.Mandal,andJ.P.Crutchfield,Identifying [71] R. G. James, J. R. Mahoney, and J. P. Crutchfield, In-
10
formation trimming: Sufficient statistics, mutual infor-
mation, and predictability from effective channel states,
PRE 95, 060102 (2017).
[72] I. Csisza´r, The Method of Types, IEEE Transactions on
Information Theory 44, 2505 (1998).
[73] J. Pearl, Bayesian Networks: A Model of Self-Activated
Memory for Evidential Reasoning, Proceedings of the
Annual Meeting of the Cognitive Science Society 7
(1985).
[74] J. Pearl, Probabilistic Reasoning in Intelligent Systems:
Networks of Plausible Inference (Morgan Kaufmann
Publishers Inc., 1988).
[75] T. Verma and J. Pearl, Causal Networks: Semantics
andExpressiveness,inMachine Intelligence and Pattern
Recognition,UncertaintyinArtificialIntelligence,Vol.9,
editedbyR.D.Shachter, T.S.Levitt, L.N.Kanal,and
J. F. Lemmer (North-Holland) pp. 69–76.
[76] D.JanzingandB.Scho¨lkopf,CausalInferenceUsingthe
Algorithmic Markov Condition, IEEE Transactions on
Information Theory 56, 5168 (2010).
[77] S.Lauritzen,Graphical Models (ClarendonPress,1996).
[78] In fact, V can be any sufficient statistic of WX about
YZ butforourpurposessettingV =YZ isthesimplest.
This solution was also pointed out in[58, figure 13a].
[79] R. J. Evans, Graphs for margins of bayesian networks,
Scandinavian Journal of Statistics 43, 625 (2016).
[80] N. Tishby and D. Polani, Information Theory of Deci-
sions and Actions, in Perception-Action Cycle: Models,
Architectures, and Hardware, edited by V. Cutsuridis,
A.Hussain,andJ.G.Taylor(Springer,NewYork,2010)
pp. 601–636.
[81] C. Salge, C. Glackin, and D. Polani, Empowerment–
AnIntroduction,inGuidedSelf-Organization: Inception,
editedbyM.Prokopenko(Springer,Berlin,2014)pp.67–
114.
[82] N. Ay and K. Zahedi, On the Causal Structure of the
Sensorimotor Loop, in Guided Self-Organization: Incep-
tion (Springer, 2014) pp. 261–294.
[83] Amemorylessagentwhichchoosesactionswithp (0)=
√ √ At
1/ 2andp (1)=1−1/ 2foralltachievesCwork(env).
At
CONTENTS
I. Introduction 1
II. Framework 2
III. Maximally Predictive Agents 3
IV. Work capacity of channels 4
11
V. Work-Efficient Agent Models 6
SUPPLEMENTAL MATERIAL
VI. Discussion and Future Directions 7
This Supplemental Material provides the full mathematical framework in a self-contained way, allowing it to be
VII. Acknowledgments 8
read from start to finish like a technical paper. The main text motivates and further explains the results, and puts
them in the context of existing literature.
References 8
Supplemental Material 11
Contents Contents 11
A. Some background on probability and information theory 11
1. Notation for random variables and stochastic processes 11
2. Information theory 12
a. Basic definitions 12
b. Information diagrams 13
c. Entropy rate 14
B. Finite-state Markov chains 14
C. Finite-alphabet finite-state hidden Markov channels 17
D. Percept-action loops 18
E. Markov conditions for percept-action loops 20
1. Bayesian networks and d-separation 20
2. d-separation conditions for percept-action loops 21
3. Existing approaches to the information theory of percept-action loops 25
F. Maximally predictive agent models 26
G. The extractable work in percept-action loops 32
1. Derivation of work capacity 32
2. Existence of Landauer-efficient agents 33
3. Definition and properties of work capacity 34
4. Efficient agent models 38
Appendix A: Some background on probability and information theory
1. Notation for random variables and stochastic processes
In this section, we establish some of the notation relating to random variables and stochastic processes used
throughoutthesequel. Randomvariableswillbedenotedbycapitallettersinstandardfont, i.e. X,Y,Z,etc. Theset
of values that each random variable can take, also called an alphabet, will be denoted by capital letters in calligraphic
font, i.e. the alphabet for X is X. In this work, we consider finite alphabets and occasionally also countably infinite
products of finite alphabets. The elements of these alphabets, at times referred to as symbols, will be denoted by
lower-case letters, i.e. the random variable can take value x∈X. Probability distributions associated to the random
variable X will be denoted by p with p (x) denoting the probability that X takes value x. The subscript may be
X X
omitted when the variable to which the distribution refers is clear.
Given two alphabets Y and Z related to random variables Y and Z respectively, we can consider a new random
variable X that takes values in X := Y ×Z. That is, X takes values x = (y,z) where y ∈ Y and z ∈ Z. Composite
random variables of this type can be constructed from any number of constituent variables, which will be useful for,
e.g., the treatment of stochastic processes below.
For the purposes of this work, a discrete-time stochastic process is given by a set of variables {X |t ∈ N } where,
t 0
for each t, the variable X takes values in the same (finite) alphabet X. This assumption simplifies measure-theoretic
t
treatmentssuchasthosein, e.g., [33]. Pursuanttotheparagraphabove, wecanassociateanewrandomvariable, de-
notedX,tothestochasticprocess,whichtakesvaluesinXN0 := × X. Thatis,X takesvaluesthataresequences
t∈N0
(x ,x ,...) with each x ∈ X. It will also be convenient to consider random variables associated to subsequences in
0 1 t
thefollowingway. Letl,m∈N suchthatl<m. WethendefineX tobetherandomvariablethattakesvaluesin
0 l:m
12
Xm−l := ×m X, that is, values that are tuples x :=(x ,x ,...,x ).[59] If m=l+1, then X is equivalent
i=l l:m l l+1 m−1 l:m
toasinglevariablefrom{X |t∈N },sowesimplywriteX andx . ThevariableX canbeconsideredasthelimiting
t 0 l l
case where l=0 and m goes to infinity.
The notation for distributions associated to the stochastic process X and to sequences of variables X follow the
l:m
same conventions: p denotes a distribution for X and p (x) denotes the probability that X takes as its value the
X X
sequence x := (x
0
,x
1
,...) ∈ XN0; similarly for p
Xl:m
and p
Xl:m
(x
l:m
). In the cases where l and m are “close”, we
sometimes represent the tuples of variable explicitly. For example, in the case of m=l+2, instead of p , we write
Xl:m
p . In particular, this allows us to consider the distribution over one part of the subsequence, when the other
Xl,Xl+1
parttakessomevalue. Forexample,ifp isknown,wecanconsiderthedistributionoverX thatresultsifX
Xl,Xl+1 l l+1
takes the value x , which we denote by p .
l+1 Xl,Xl+1=xl+1
2. Information theory
For an introductory treatment on information theory, see [29], for a measure-theoretic treatment, see [60].
a. Basic definitions
Let X be a random variable with distribution p . The Shannon entropy quantifies the uncertainty associated with
X
p as
X
(cid:88)
H (X):= p (i)s (i) (A1)
p X p
i∈X
wheres (i):=−log p (i)isknownasthesurprise ofobtainingoutcomeX =i[44](see[61]foranelegantaxiomatic
p 2 X
derivation), and the sum runs over all i ∈ X such that p (i) ̸= 0. If it is clear from context which distribution p is
X
used to compute entropy, we drop the index and simply write H(X).
Let X and Y be random variables with joint distribution p . The conditional entropy of X given Y is defined as
XY
(cid:88)
H(X|Y):= p (i,j)s (i|j), (A2)
XY p
i∈X,j∈Y
where s (i|j) := −logp (i|j) denotes the conditional surprise of obtaining outcome x given that y has been
p X|Y
observed.
Entropy obeys the chain rule of entropy
n−1
(cid:88)
H(X )= H(X |X ) (A3)
0:n t 0:t
t=0
where, if t=0, H(X |X ) is given by H(X ).
t 0:t 0
The mutual information I[X;Y] of random variables X and Y is defined as
I[X;Y]:=H(X)−H(X|Y), (A4)
which, with the chain rule of entropy eq. (A3), can be written in the symmetric form
I[X;Y]=H(XY)−H(Y|X)−H(X|Y). (A5)
The Conditional mutual information is then simply defined via the conditional entropy as
I[X;Y|Z]:=H(X|Z)−H(X|YZ), (A6)
or equivalently in a symmetric form:
I[X;Y|Z]=H(XY|Z)−H(Y|XZ)−H(X|YZ). (A7)
We say that X and Y are conditionally independent if I[X;Y|Z]=0. In fact, I[X;Y|Z]=0 iff
p =p p . (A8)
XY|Z X|Z Y|Z
13
The conditional mutual information inherits a chain rule from entropy, which can be written as
n−1
(cid:88)
I[X ;Y|Z]= I[X ;Y|ZX ]. (A9)
0:n t 0:t
t=0
The chain rule for mutual information is obtained by dropping Z on both sides. Often, we will use the chain rule for
a single step:
I[W;XY|Z]=I[W;X|Z]+I[W;Y|XZ]. (A10)
Themeasuresofinformationdefinedsofarareallnonnegativeandcanbeinterpretedbasedon(conditional)surprise,
respectively its averaged version, (conditional) entropy. For a consistent treatment of multiple random variables, it is
convenient to extend the definition of (conditional) mutual information to more than two arguments. The so-called
multivariate mutual information or interaction information [62, 63] can be defined inductively via
I[X ;...;X ;X ]:=I[X ;...X ]−I[X ;...;X |X ], (A11)
i j k i j i j k
and similarly conditional interaction information via
I[X ;...;X ;X |X ]:=I[X ;...X |X ]−I[X ;...;X |X X ]. (A12)
i j k l i j l j j l k
However, it should be noted that multivariate mutual information of three or more variables can assume negative
values which makes it difficult to interpret [63].
b. Information diagrams
The properties of Shannon’s basic measures of information such as entropy and mutual information bear a resem-
blancetosettheory. Ithasbeenshownthatonecanestablishaone-to-onecorrespondencebetweenthesemeasuresof
informationanda(signed)measureonsets[64,65]. WewriteX ,...,X todenoterandomvariables,andX˜ ,...,X˜
1 n 1 n
forthecorrespondingsets. TheunionofsetsX˜ ∪···∪X˜ correspondstothejointentropyH(X ,...,X ),theintersec-
i j i j
tionofsetsX˜ ∩···∩X˜ correspondstothemultivariatemutualinformationI[X ;...;X ],andthesetdifferenceX˜ \X˜
i j i j i j
corresponds to the conditional entropy H(X |X ). Conditional mutual information I[X ...X ;S ...S |C ...C ]
i j i j k l n m
(cid:16) (cid:17)
corresponds then to (X˜ ∪···∪X˜ )∩(S˜ ∪···∪S˜) \(C˜ ∪···∪C˜ ). This correspondence allows us to represent
i j k l n m
the relations between measures of information in terms of Venn diagrams, whose primary sets correspond to the
entropies of single random variables. One example of such an information diagram is given in Figure 7.
H(Z|X,Y) Z
I[X;Z|Y] I[Y;Z|X]
I[X;Y;Z]
X Y
H(X|Y,Z) H(Y|X,Z)
I[X;Y|Z]
FIG. 7. An example of an information diagram. An information diagram illustrates the relations between (conditional)
entropies and (conditional) mutual information.
14
c. Entropy rate
Let X be a stochastic process with distribution p . Then, the entropy rate, a process’s degree of intrinsic random-
X
ness, is defined as
H(X )
h(X):= lim 0:n (A13)
n→∞ n
if the limit exists. The entropy rate exists for a broad class of processes known as asymptotically mean stationary
processes[60]whichcontainsstationaryprocessesand,aswewillsee,alsoprocesseswhicharegeneratedbyfinite-state
hidden Markov models.
Beforeweproceed,weintroducethefollowingnotationfortheCes`aro limit,thelimitofthearithmeticmean,which
will be used throughout this work [66]:
n−1
1 (cid:88)
⟨f(t)⟩ := lim f(t). (A14)
t n→∞n
t=0
The Ces`aro limit is linear in the sense that
⟨af(t)+bg(t)⟩ =a⟨f(t)⟩ +b⟨g(t)⟩ (A15)
t t t
whenever ⟨f(t)⟩ and ⟨g(t)⟩ exist, where a,b∈R.
t t
Using the Ces`aro limit, we can state a chain rule for entropy rate as follows:
⟨H(X |X )⟩ =h(X). (A16)
t 0:t t
This chain rule is a consequence of eq. (A3).
When working on the information theory of stochastic processes, expressions which contain a infinite number of
random variables, such as I[X ;Y|Z], are commonly encountered. It should be noted that such expressions are
n:∞
always defined via a limit, that is, I[X ;Y|Z] = lim I[X ;Y|Z]. In particular, using the chain rule, one can
n:∞ n:m
m→∞
show that
Lemma 1. For any n∈N , I[X ;Y|Z] is finite, where Y, Z, and X for all t are finite random variables.
0 n:∞ t
Proof. Using the chain rule of mutual information, eq. (A9), we find
m
(cid:88)
I[X ;Y|Z]= lim I[X ;Y|ZX ]. (A17)
n:∞ t n:t
m→∞
t=n
Notethat,forfiniterandomvariablesA,D,S fort∈{n,...,m},allpartialsums
(cid:80)m
I[A;S |DS ]=I[A;S |D]
n:t t=n t n:t n:m
are upper bounded by H(A), and every summand is nonnegative, so the monotone convergence theorem ensures that
the limit exists.
Appendix B: Finite-state Markov chains
Thisappendixreviewssomeresultsonfinite-stateMarkovchainsfromtheliterature. Foramorecompletetreatment
of finite-state Markov chains, we refer the reader to [27, 67].
LetX beastochasticprocesswithdistributionp. A(first-order)MarkovchainisastochasticprocessX suchthat
p (x |x ,x )=p (cid:0) x |x′ ,x (cid:1) (B1)
Xt|X0:t t 0:t−1 t−1 Xt|X0:t t 0:t−1 t−1
for all x ,x ∈X and for any t≥1 and x ,x′ ∈Xt−1.
t−1 t 0:t−1 0:t−1
TheMarkovchainissaidtobefinite-state ifX isfinite,anditissaidtobehomogeneous ifeq.(B1)doesnotdepend
on time t, that is, p (j|i) = p (j|i) for all i,j ∈ X and t,t′ ∈ N. We then write ϕ(j|i) := p (j|i)
Xt|Xt−1 X t′|X t′−1 Xt|Xt−1
andcallϕ(j|i)thetransition probability. Fortheremainderofthisappendix,itisassumedthatallfinite-stateMarkov
chains are homogeneous.
Finite-state Markov chains are thus conveniently characterized by their initial distribution p(X ) and a |X|×|X|
0
stochastic matrix Φ=(ϕ(j|i)) . The matrix Φ is also called a transition matrix. We use the convention that Φ is
j,i∈X
15
a right stochastic matrix, i.e., that each row of Φ must sum to one. This convention is more common in the physical
literature, while the mathematical literature such as [27] often uses the convention that Φ is left stochastic. Results
can be translated from one convention to the other by a simple transposition.
Inwhatfollows,weintroducesometheoryoffinite-stateMarkovchainswhichwillbeneededtounderstandinwhat
sense finite-state Markov chains are well-behaved in the asymptotic time limit t→∞. The probability to reach state
j afternstepsstartingfromstateiisgivenby(Φn) . Ifiisareturnstate, i.e., (Φn) >0forsomen≥1, wedefine
j,i i,i
its period d as the greatest common divisor of all natural numbers n such that (Φn) > 0 [27, p.81]. Further, the
i i,i
first passage time to state j is defined as
Tfirst :=min{t≥1|X =j}. (B2)
j t
where Tfirst takes values in N∪{∞}. Note that the first passage time is a random variable. Define f(n,i,j) as
j
the probability p{Tfirst = n|X = i} that Tfirst = n given that the chain started in state i. Then, (f(n,i,j),n =
j 0 j
1,2,...,∞) is the probability distribution of Tfirst given that the Markov chain started in state i [27, p.86].
j
Let
∞
(cid:88)
f(i,j):= f(n,i,j)=p{Tfirst <∞|X =i}. (B3)
j 0
n=1
A state i is said to be recurrent if f(i,i) equals one, i.e., the chain is guaranteed to return to i eventually with
probability one. Otherwise, state i is called transient, i.e., there is a nonzero probability that the chain will never
return to i [27, p.88].
Let m(i,i) be the mean recurrence time of state i,
∞
(cid:88)
m(i,i):= nf(n,i,i), (B4)
n=1
where m can take values in [1,∞]. Note that m<∞ for recurrent states and m=∞ for transient states.
Further,letf (i,j)betheprobabilitythatX =j occursatleastforonet=r(modd )giventhatthechainstarted
r t j
in i:
(cid:88)
f (i,j):= f(md +r,i,j). (B5)
r j
m≥0
We are now in the position to state the following result which characterizes the asymptotic behavior of arbitrary
homogeneous finite-state Markov chains.
Lemma 2. [adapted from [27, p.153]] Let X be a homogeneous finite-state Markov chain over an alphabet X with
transition matrix Φ. Then, for any state i∈X and any transient state j we have
lim (Φn) =0. (B6)
n→∞ j,i
Further, for any state i∈X and any recurrent state j with period d we have for all r ∈{1,2,...,d },
j j j
f (i,j)d
lim (cid:0) Φndj+rj (cid:1) = rj j . (B7)
n→∞ j,i m(j,j)
For a proof see [27, p.153-154]. Note that when compared to [27, Thm 5.1, p.153], we treat the case of transient
states(eq.(B6))separatelybecausefortransientstatesj itcanhappenthatd isnotwelldefinedifthereisnon∈N
j
for which (Φn) >0. Lemma 2 states that for each starting state i, the probability for the chain to be in a transient
i,j
state goes to zero as n → ∞ while the probability for the chain to be in a recurrent state j is periodic with some
finite period d as n→∞.
j
The following corollary, which is adapted from [27, p.154], summarizes some useful consequences of Lemma 2. We
again make use of the notation ⟨•⟩ = lim 1 (cid:80)n−1•.
t n→∞n t=0
Corollary 1. Let X with distribution p be a homogeneous finite-state Markov chain over an alphabet X with
X
transition matrix Φ. For any recurrent state i∈X, let d be its period, and let d be the least common multiple of all
i
d . Then,
i
16
(i) [d convergent subsequences] for all r ∈{1,2,...,d} the limit
Φ(r) := lim Φnd+r (B8)
∞
n→∞
exists and in particular Φ(r) =ΦrΦ(d),
∞ ∞
(ii) [Ces`aro limit] the matrix
Π=
(cid:10) Φt(cid:11)
(B9)
t
exists and its coefficients are given by π = f(i,j),
j,i m(j,j)
(iii) [continuousfunctionofCes`arolimit] for any continuous function g :T →R where T denotes the set of |X|×|X|
transition matrices,
(cid:10)
g
(cid:0) Φt(cid:1)(cid:11)
(B10)
t
(cid:16) (cid:17)
exists, and is given by ⟨g(Φt)⟩ = (cid:80)d g Φ(r) /d.
t r=1 ∞
Proof.
(i) Existence follows from lemma 2 and Φ(r) =ΦrΦ(d) from lim (cid:0) ΦrΦnd(cid:1) =Φr lim (cid:0) Φnd(cid:1) .
∞ ∞
n→∞ n→∞
(ii) Follows from the fact that by corollary 1(i) the sequence (Φt) has d convergent subsequences each of which has
t
a convergent Ces`aro limit by the Ces`aro limit theorem [68, 5.3.1], and
1 (cid:88)
dj
f r (i,j)d j = f(i,j) . (B11)
d m(j,j) m(j,j)
j
r=1
(iii) A function g is continuous if and only if for a convergent sequence Π → Π the sequence g(Π ) converges to
n n
g(Π)[69,Thm. 21.3]. Itfollowsthenfromcorollary1(i),thatthesequenceg(Φ )hasdconvergentsubsequences,
t
and therefore converges in the Ce`saro limit to
d
1(cid:88) (cid:16) (cid:17)
⟨g(Φ )⟩ = g Φ(r) . (B12)
t t d ∞
r=1
Finally,itshouldbenotedthatnotonlytheper-stepdistributionsofMarkovchainsareasymptoticallywellbehaved
(asaconsequenceofcorollary1),butalsotheentropyrateasdefinedineq.(A13). Entropyrateexistsevenforbroader
classes of processes such as deterministic functions of Markov chains: Let X be a finite-state Markov chain. We say
that the process Y over a finite alphabet Y is a deterministic function of X if Y = f(X ) for all t ∈ N where
t t 0
f : X → Y is a deterministic function. (Note that the class of deterministic functions of finite-state Markov chains
is equivalent to finite-state finite-alphabet hidden Markov chains [24] in the sense that any deterministic function of
a Markov chain can be described as a finite-alphabet hidden Markov chain, and any finite-alphabet hidden Markov
chain can be described as a deterministic function of Markov chain with an augmented state space [24].)
Lemma 3. Let X and Y be finite alphabets, f : X → Y a map, X a finite-state Markov chain on X, and Y =
(f(X ),f(X ),f(X ),...). Then,
0 1 2
⟨H(Y )⟩ (B13)
0:t+1 t
exists.
Proof. This follows from [70, theorem 9] and the entropy ergodic theorem [60, theorem 3.1.1]. □
17
Appendix C: Finite-alphabet finite-state hidden Markov channels
ThisappendixdefineshiddenMarkovchannelsingeneralaswellassomespecialclassesofhiddenMarkovchannels.
For a review on hidden Markov processes see [24].
Let X and Y denote the finite input and output alphabets, respectively. A discrete-time, finite-alphabet channel
is defined as a function from input sequences x ∈ XN0 to distributions over the channel’s output process, Y. This
function can be represented as a conditional distribution, denoted ν . Thus, for a fixed input sequence X =x, a
Y|X
channel assigns probabilities ν
Y|X
(y|x) for all output sequences y ∈YN0.
Inthesimplestcase, ν ’sinputsaredistributedasp suchthatthejointdistributionbecomesp =ν p .
Y|X X XY Y|X X
However,notethatingeneral,ν ’sinputsmaydependon(someof)ν ’soutputs. Therefore,thejointprobability
Y|X Y|X
distribution over the joint process of inputs and outputs, XY is in general given by
p =ν η (C1)
XY Y|X X|Y
where η (x|y) is another channel which specifies how the ν ’s inputs are distributed, depending on ν ’s
X|Y Y|X Y|X
outputs. In such cases, the distribution
p (x,y)
p (y|x)= XY (C2)
Y|X p (x)
X
ν (y|x)η (x|y)
Y|X X|Y
= (C3)
(cid:80)
ν (y|x)η (x|y)
y∈YN0 Y|X X|Y
canbedifferentfromν (y|x). Thisdifferenceisalsothereasonwedenotethechannelbyν andreservethesymbol
Y|X
p for the joint distribution p (and distributions which can be obtained from it by marginalizing or conditioning,
XY
such as p and p ).
Y|X X
The conditional probability ν thus characterizes the behavior intrinsic to the channel while the conditional
Y|X
probability p also takes into account how the channel’s inputs are prepared.
Y|X
In this work, we focus on a subclass of discrete-time finite-alphabet channels commonly known as finite-state [25,
p.97] or hidden Markov channels [24].
Definition 6. A (discrete-time finite-alphabet) channel ν is a finite-state hidden Markov channel if there exists
Y|X
a distribution p over a finite set of states Z and a transition matrix Φ=(ϕ(j|i)) with i∈X ×Z and j ∈Y ×Z
Z0 j,i
such that
∞
(cid:88) (cid:89)
ν (y|x)= p (z ) ϕ(y ,z |x ,z ), (C4)
Y|X Z0 0 t t+1 t t
z t=0
where the sum runs over all z ∈ZN0. Then, the tuple (Φ,p
Z0
) is called a hidden Markov model of ν
Y|X
and z ∈Z
the hidden states of the Markov model.
In particular, since any such Markov model defines a hidden Markov channel and any hidden Markov channel by
definitionhasaMarkovmodel, eq.(C4)definesamany-to-onecorrespondencebetweenMarkovmodelsandchannels.
Further, hidden Markov channels are causal channels [30, definition 4] in the sense that
ν (y |x x )=ν (y |x x′ ) (C5)
Y0:n|X 0:n 0:n n:∞ Y0:n|X 0:n 0:n n:∞
for all n ∈ N and for all future input sequences x
n:∞
,x′
n:∞
∈ XZ0, where ν
Y0:n|X
(y
0:n
|x) = (cid:80)
yn:∞
ν
Y|X
(y|x). This
means that for a complete description of the channel’s behavior for the first n rounds (channel uses) it is sufficient to
know its input past x . In particular, hidden Markov channels can be understood as those causal channels which
0:n
admit an implementation using only finite memory resources as represented by the finite set of hidden states Z.
ThetransitionmatrixΦstoresasitscoefficientstheconditionalprobabilityassignmentsϕ(y ,z |x ,z )whichare
t t+1 t t
independent of t (and hence Φ generates a homogeneous Markov chain).
GiventhatoneknowsthetransitionmatrixΦ,thecurrenthiddenstatez,aswellasthecurrentinputxandoutput
y, the obtainable knowledge about the next hidden state z′ of the Markov model is represented by a distribution
determinedbyΦwhich,uptonormalization,isgivenby(ϕ(y,z′|x,z)) . Markovmodelsforwhichthisdistribution
z′∈Z
is a delta distribution, are said to be unifilar [24, 25, 31]. Unifilar Markov models represent an important class of
Markov models because, given the current hidden state, input, and output, for unifilar Markov models it is possible
to infer the next hidden state with certainty.
18
Definition 7. A Markov model (Φ,p ) of a hidden Markov channel ν is said to be unifilar if
Z0 Y|X
(i) p (z)=1 for some z and zero otherwise, and
Z0
(ii) H(Z |X Y Z )=0 for all t∈N .
t+1 t t t 0
A hidden Markov channel is said to be unifilar if there exists a unifilar Markov model for it.
That is a Markov model is unifilar if p is a delta distribution and X , Y , and Z determine the next hidden state
Z0 t t t
Z for all steps t. It should be noted that while there exists a systematic method to construct unifilar models from
t+1
non-unifilar ones [71], some hidden Markov channels only admit unifilar Markov models if one allows for an infinite
numberofhiddenstates. However,sinceMarkovmodelsasdefinedinDefinition6haveonlyafinitenumberofhidden
states, the set of unifilar hidden Markov channels is a strict subset of the set of hidden Markov channels. For an
example of a nonunifilar hidden Markov channel see [30, section 13].
Note that it follows from the definition of unifilar Markov models that it is always possible to construct a (deter-
ministic) function f :X ×Y ×Z →Z, in the following called a unifilarity map, such that ϕ(y,z′|x,z)̸=0 only if
uni
z′ = f (x,z,y). Then, given the transition matrix Φ and the initial state Z = z, one can infer the exact hidden
uni 0
state z at any time t by observing the input and output processes X and Y and by iteratively using the function
0:t 0:t
f .
uni
Unifilaritywasfirstintroducedinthecontextoffinite-statesources[31,p. 187],andunderthenameMarkovsource
in [25, Section 3.6]. Definition 7 extends unifilarity to Markov models of hidden Markov channels. In the context of
stationary input-output processes, unifilarity is one of the properties of ϵ-transducers [30]. Unifilarity often simplifies
the mathematical treatment of Markov models considerably, see for example [72].
Important classes of channels, which we consider in this work, are the following:
Definition 8. A channel ν is said to be
Y|X
• noiseless if ν (y|x)=δ and X =Y where δ is a Kronecker delta.
Y|X x,y x,y
• memoryless invariant if there exists a |X|×|Y| stochastic matrix Φ such that ν (y|x)= (cid:81)∞ ϕ(y |x ).
Y|X t=0 t t
• a product channel if ν
Y|X
(y|x)=ν
Y|X
(y|x′) for all x,x′ ∈XN0.
The output behavior of product channels is fully characterized without knowing their inputs. Thus, they can be
understood as an information source which produces a (hidden Markov) process over outputs [24]. Product channels
are also called completely random channels in the literature [60, chapter 9.4.2].
Appendix D: Percept-action loops
This appendix defines a model for percept-action loops and proves, based on this model, that the global process
(involving agent and environment) is Markov. In the following, we refer to the hidden Markov channel of interest as
the environment, abbreviated as env:
env:=νenv. (D1)
S|A
TheinputrandomvariablesA arecalledaction variablestakingvaluesa∈AandtheoutputrandomvariablesS are
t t
called percept variables taking values s ∈ S (S like state or sensory input is common nomenclature in reinforcement
learningandrelatedfields). Forsimplicity, weassumethatthefiniteinputandoutputalphabetsof envareidentical,
A=S. In terms of expressivity of the model, this assumption is not restrictive, as any Markov channel with distinct
input and output alphabets can be trivially extended to a channel with a common alphabet for inputs and outputs
by embedding both to the larger of the two.
A Markov model of the channel νenv (see Definition 6), denoted as
S|A
envM:=
(cid:0) Φenv,penv(cid:1)
, (D2)
Z0
is called a (Markov) model of env.
Hidden Markov product channels (see Definition 8) represent a special class of environments which we will call
product environment channel.
Protocols used to interact with environments are called agents. In full generality, agents, abbreviated as agt, can
be represented as a channel ηagt from percepts to actions. Similarly to environments, we assume that agents respect
A|S
19
a causal ordering and that they admit an implementation with finite memory. However, there is a small asymmetry
betweenagentandenvironment: theagentmustproducetheveryfirstactionA withoutbeingpromptedbyapercept
0
(incontrast, theenvironmentis promptedwithanactionbeforeitproduces thefirstpercept). Onaformal level, this
is easily taken into account by defining agents as a hidden Markov channel from percepts S to actions A where
1:∞
theinitialdistributionoverhiddenstatesisreplacedbyasuitablejointdistributionoverhiddenstatesandactionA .
0
For clarity, we suitably restate Definition 6:
Definition 9. A channel ηagt is an agent channel, denoted as
A|S
agt:=ηagt , (D3)
A|S
if there exists a finite set of states M, a distribution pagt over A×M, and a transition matrix Θagt = (θ(j|i))
A0M0 j,i
with i∈S×M and j ∈A×M such that
∞
(cid:88) (cid:89)
ηagt (a|s)= pagt (a ,m ) θagt(a ,m |s ,m ),
A|S A0M0 0 0 t+1 t+1 t t
m t=0
where the sum runs over all m∈MN0. Then, the tuple
agtM:=(Θagt,pagt ) (D4)
A0M0
is called a (hidden Markov) agent model, of agt and m∈M the memory states of the model.
Foranygivenenvironmentchannelenv,letA→←env denotethesetofagentmodelswithmatchingaction-perceptalphabet.
As before, eq. (D4) defines a many-to-one mapping correspondence between agent models and agents.
(a)
A0 SS10 A1 S1 A2 S2 A3 S3 A4
(b)
M0 M1 M2 M3 M4
Θ Θ Θ Θ
A0 SS10 A1 S1 A2 S2 A3 S3 A4
Φ Φ Φ Φ Φ
Z0 Z1 Z2 Z3 Z4
FIG. 8. Percept-action loops. a: Agent and environment are represented through channels such that the environment’s inputs
A (outputs S ) are the agent’s actions (percepts). b: Agent and environment are represented through Markov models with
t t
hidden memory Mand Z, respectively.
The two channels defining an agent and its environment are called percept-action loop, denoted by
(cid:16) (cid:17)
agt→←env:= η
A
ag
|
t
S
,ν
S
en
|A
v , (D5)
with the associated joint process AS, called the percept-action process, having distribution
p =ηagt νenv (D6)
AS A|S S|A
see also Figure 8a. Alternatively, it is possible to specify a Markov model for the agent and/or environment. For
instance,
agtM→←envM= (cid:0) Θagt,pa
M
g
0
t
A0
,Φenv,pe
Z
n
0
v(cid:1) , (D7)
20
denotes the percept-action loop where both Markov models are specified, with the associated process MASZ, called
the global process, having distribution over action, percept, and hidden states
∞
(cid:89)
p (m,a,s,z)=pagt (a ,m )penv(z ) θagt(a ,m |s ,m )ϕenv(s ,z |a ,z ), (D8)
MASZ A0M0 0 0 Z0 0 t+1 t+1 t t t t+1 t t
t=0
see also Figure 8b. The models agt→←envM and agtM→←env are defined correspondingly.
Lemma 4 (Global Markov chain). Let agtM→←envM be a percept-action loop global process distribution given in
eq. (D8). Then, the stochastic process U, where
U =(M ,A ,S ,Z ), (D9)
t t t t t
is a homogeneous finite-state Markov chain which will be called the global Markov chain of the percept-action loop.
Proof.
First we check the Markov property, that is,
p(u |u u )=p(u |u′ u ), (D10)
n 0:n−1 n−1 n 0:n−1 n−1
for any n≥1 and u ,u′ , where u =(x ,a ,s ,z ). Note that for better readability, we drop p’s index.
0:n−1 0:n−1 n n n n n
This is a direct consequence of the Markov property of the Markov models for agent and environment which can
be seen as follows. For any n≥1 we have by the definition of conditional probability
p(u )
p(u |u )= 0:n+1 , (D11)
n 0:n p(u )
0:n
where, by marginalizing the global distribution of a percept-action loop, eq. (D8) and writing u as (m ,a ,s ,z ):
n n n n n
 
n−1
(cid:88) (cid:89)
p(u
0:n+1
)=pa
A
g
0
t
M0
(a
0
,m
0
)pe
Z
n
0
v(z
0
) ϕenv(s
n
,z
n+1
|a
n
,z
n
) θagt(a
t+1
,m
t+1
|s
t
,m
t
)ϕenv(s
t
,z
t+1
|a
t
,z
t
). (D12)
zn+1 t=0
Due to the product structure of eq. (D12), most terms cancel out when we compute eq. (D11) and we are left with
(cid:104) (cid:105)
(cid:80) ϕenv(s ,z |a ,z ) θagt(a ,m |s ,m )ϕenv(s ,z |a ,z )
p(u |u ,u ,...)=
zn+1 n n+1 n n n n n−1 n−1 n−1 n n−1 n−1
. (D13)
n n−1 n−2 (cid:80) ϕenv(s ,z′|a ,z )
z′ n−1 n n−1 n−1
n
Since the right-hand side depends only on variables with time index n and n − 1, we have shown the Markov
chain property, eq. (D10). Further, since the right-hand side is determined by the transition matrices of agent and
environment, the Markov chain is homogeneous, and with this the lemma is proven. □
Appendix E: Markov conditions for percept-action loops
Bayesiannetworksaregraphicalmodelsthatrepresentprobabilisticrelationshipsamongrandomvariablesusingdi-
rectedacyclicgraphs [73–75]. Theyallowforefficientreasoningaboutconditionalindependencethroughd-separation.
d-separation is a key concept in Bayesian networks that determines whether two sets of variables are independent
givenathirdset,basedonthestructureofthegraph. Itprovidesaformalcriterionforunderstandinghowinformation
flows through the network. This appendix introduces Bayesian networks in general and shows how to use them for
percept-action loops.
1. Bayesian networks and d-separation
Let {V ,...,V } be a set of n random variables and let G be a directed acyclic graph (DAG) such that for each
1 n
randomvariablein{V ,...,V }thereispreciselyonenodeinG. LetPA bethesetofparentsofV andND theset
1 n j j j
of non-descendants of V except itself. If B,C,D are sets of random variables, I[B;C|D] is the conditional mutual
j
information with respect to the joint random variables constituting the sets, and I[B;C|D] = 0 means that B is
statistically independent of C, given D.
In the following, a path is defined as a sequence of nodes connected by edges, regardless of the direction of the
edges. The following definition is adapted from [76].
21
Definition 10 (d-separation). A path p in a DAG is said to be d-separated (or blocked) by a set of nodes D if at least
one of the following conditions holds:
(i) p contains a chain X →Y →Z or fork X ←Y →Z such that the middle node Y is in D, or
(ii) p contains an inverted fork (or collider) X → Y ← Z such that the middle node Y is not in D and such that
no descendant of Y is in D.
A set D is said to d-separate B from C if and only if D blocks every path from a node in B to a node in C.
Lemma 5 (EquivalentMarkovconditions, [77, Theorem3.27], seealso[76, Lemma1]). Let p(V ,...,V ) be the joint
1 n
distribution of random variables V ,...,V (as always, in this work, with respect to a product measure). Then the
1 n
following three statements are equivalent:
(i) Recursive form: p(V ,...,V ) admits the factorization
1 n
n
(cid:89)
p(V ,...,V )= p(V |PA ), (E1)
1 n j j
j=1
where the notation p(V |PA ) is understood as p(V ) if PA is empty.
j j j j
(ii) Local (or parental) Markov condition: for every node V we have
j
I[V ;ND |PA ]=0, (E2)
j j j
i.e., it is conditionally independent of its non-descendants (except itself), given its parents.
(iii) Global Markov condition:
I[B;C|D]=0 (E3)
for all three sets B,C,D of nodes for which B and C are d-separated by D.
Inthefollowing,wewillmakeextensiveuseofthenotionofcompatibilityofadistributionwithaBayesiannetwork,
which we define as follows.
Definition 11. Let p be a distribution over a set of variables W, and let G be a Bayesian network with nodes V such
that W ⊆V. Then, the distribution p is said to be compatible with G if
I[B;C|D]=0 (E4)
for all three sets B,C,D ⊆W of nodes for which B and C are d-separated by D.
Notethattheconditionsgivenbyeq.(E4)arethoseglobalMarkovconditionswithrespecttoGwhichonlyinvolve
variables of p. Compatibility of p with G thus means that the Markov conditions implied by G for the variables of p
hold.
2. d-separation conditions for percept-action loops
One may initially be tempted to think that the Bayesian network depicted in Figure 9b is compatible with any
Φ=(ϕ(j|i)) withi∈W×Mandj ∈Y×Z (seeFigure9a)inthesensethatalllocalMarkovconditionsimpliedby
j,i
Figure 9b hold for any distribution p (w,x,y,z) = ϕ(y,z|w,x)p (w,x). Figure 9b implies (by d-separation)
WXYZ WX
conditional independence of Y and Z given their parents, that is,
I[Y;Z|W,X]=0. (E5)
This condition, however, is easily shown to be violated by a channel which produces correlation independent of the
values of W and X. For instance, let ϕ(0,0|w,x)=ϕ(1,1|w,x)=1/2 for all w ∈W and x∈M and where 0,1∈Y,
0,1∈Z. Then, eq. (E5) is clearly not fulfilled.
The problem related to Figure 9b can be solved with a little sleight of hand: We introduce an additional variable
V = YZ, defined as the joint channel output, as depicted in Figure 9c [78]. This is by no means the only way to
22
(a) (b) (c)
W Y W Y W Y
Φ V
X Z X Z X Z
FIG.9. OnfindingacompatibleBayesiannetworkofaMarkovchannelwithtwoinputsandtwooutputs. (a): Circuitdiagram
ofamemorylesschannelwithinput(W,X)andoutput(Y,z)describedbytransitionmatrixΦ,(b): aBayesiannetworkwhich
is not compatible with arbitrary Φ, and (c): a Bayesian network with an auxiliary variable V =YZ which is compatible with
arbitrary Φ.
address the problem with Figure 9b (for instance [79, Lemma 1]), but it is a particularly simple solution which, as we
will see, allows one to use d-separation for percept-action loops. With this choice of V, we can write
p =p p p (E6)
YZV|WX Y|V Z|V V|WX
where, since V = YZ, the conditional distribution p is given by the transition matrix Φ, p (y,z|w,x) =
V|WX V|WX
ϕ(v|w,x) with v =(y,z), and p , p are delta distributions since
Y|V Z|V
p (y|v)=p (y|y′,z′)=δ , (E7)
Y|V Y|YZ y,y′
and similarly for p .
Z|V
We recover the original channel from eq. (E6) through marginalization:
(cid:88) (cid:88)
p (y,z,v|w,x)= p (y|v)p (z|v)p (v|w,x) (E8)
YZV|WX Y|V Z|V V|WX
v∈V v∈V
(cid:88)
= p (y|y′,z′)p (z|y′,z′)ϕ(y′,z′|w,x) (E9)
Y|YZ Z|YZ
y′∈Y,z′∈Z
(cid:88)
= δ δ ϕ(y′,z′|w,x) (E10)
y,y′ z,z′
y∈Y,z∈Z
=ϕ(y,z|w,x). (E11)
M0 M1 M2 M3 M4
V0 V1 V2 V3 V4
A0 S0 A1 S1 A2 S2 A3 S3 A4 S4
W0 W1 W2 W3 W4
Z0 Z1 Z2 Z3 Z4 Z5
FIG. 10. Bayesian network of a general percept-action loop.
Applying the Bayesian network representation Figure 9c to the Markov channels of agent and environment in
agtM→←envM leads us to the following
Given a distribution q over a set of variables V compatible with a Bayesian network B, for any subset W ⊆ V,
V
let G(q,W) denote the set of Markov conditions
I[S;T|R]=0 (E12)
with respect to q for all three sets S,T,R⊂W of nodes for which S and T are d-separated by R.
23
Lemma 6. For any agtM→←envM, the total distribution, of the form as given in eq. (D8), is compatible with the
Bayesian network in Figure 10.
The proof proceeds by constructing a distribution over all variables in Figure 10
(i) which admits a recursive form in the sense of Lemma 5(i) and thus, by Lemma 5, the global Markov conditions
must hold, and
(ii) such that the distribution of agtM→←envM is recovered through marginalization.
Proof. By Lemma 5, a distribution over the variables in the Bayesian network shown in Figure 10 fulfills the global
Markov conditions if and only if it factorizes as
∞
(cid:89)
p =p p p p p p p p . (E13)
MASZVW V0 Z0 Mt|Vt At|Vt Vt+1|StMt Zt+1|Wt St|Wt Wt|AtZt
t=0
Let p be of the form in eq. (E13). Then, in particular those d-separations which involve only variables M ,
MASZVW t
A
t
, S
t
, and Z
t
, t ∈ N
0
, must hold. All that is left to show is that the distribution of any agtM→←envM, as given in
eq. (D8), can be recovered through marginalization from a distribution of the form in eq. (E13).
For all t∈N , we set V =A M and W =S Z , and let
0 t t t t t t+1
agtM→←envM= (cid:0) Θagt,pa
M
g
0
t
A0
,Φenv,pe
Z
n
0
v(cid:1) (E14)
beanyperceptactionloop. Then,letp =penv,andforallt∈N definethoseconditionaldistributionsineq.(E13),
Z0 Z0 0
which do not reduce to a delta distribution, to be
p (v |s ,m )=θagt(a ,m |s ,m ) for all v =(a ,m ), and (E15)
Vt+1|StMt t+1 t t t+1 t+1 t t t+1 t+1 t+1
p (w |a ,z )=ϕenv(s ,z |a ,z ) for all w =(s ,z ). (E16)
Wt|AtZt t t t t t+1 t t t t t+1
For each t∈N, we consider all terms on the right-hand side of eq. (E13) which contain V and marginalize:
t
(cid:88)
p (m |v )p (a |v )p (v |s m )=θagt(a ,m |s ,m ) (E17)
Mt|Vt t t At|Vt t t Vt|St−1Mt−1 t t−1 t−1 t t t−1 t−1
vt∈V
which follows from V =A M , and thus p and p are delta distributions, and eq. (E15). For each t∈N , a
t t t Mt|Vt At|Vt 0
similar calculation for all terms on the right-hand side of eq. (E13) containing W yields ϕenv(s ,z |a ,z ). Finally,
t t t+1 t t
we consider all terms on the right-hand side of eq. (E13) which contain V and marginalize:
0
(cid:88)
p (v )p (m |v )p (a |v )=p (a ,m ), (E18)
V0 0 M0|V0 0 0 A0|V0 0 0 A0M0 0 0
v0∈V
which follows from V =A M . Finally, let p be such that p =pagt .
We thus constructe 0 d a d 0 istr 0 ibution p V0 such that m A ar 0 g M in 0 alizi A ng 0M o 0 ut V and W yields eq. (D8). □
MASZVW
ThefollowingcorollaryshowsthatasimplifiedBayesiannetworkcanbeusedwhentheenvironmentismemoryless.
Recall that for a memoryless environment envM , there exists a |A| × |S| stochastic matrix Φenv such that
memless
ν
S|A
(s|a)= (cid:81)∞
t=0
ϕenv(s
t
|a
t
) and, thus, the total distribution of the any agtM→←env
memless
is of the form
∞
(cid:89)
p (m,a,s)=pagt (a ,m ) θagt(a ,m |s ,m )ϕenv(s |a ). (E19)
MAS A0M0 0 0 t+1 t+1 t t t t
t=0
Corollary 2. For any env , the total distribution, of the form as given in eq. (E19), is compatible with the
memless
Bayesian network in Figure 11.
Proof. The corollary is a special case of lemma 6 where the environment is taken care of by setting p (a |s ) =
At|St t t
ϕenv(s |a ) for all t∈N.
t t
(cid:16) (cid:17)
Let env be a product environment channel. Then, the distribution of any agtM→←env= Θagt,pa
M
g
0
t
A0
,ν
S
en
|A
v takes
the form
∞
(cid:89)
p (m,a,s)=νenv(s|a)pagt (a ,m ) θagt(a ,m |s ,m ), (E20)
MAS S|A A0M0 0 0 t+1 t+1 t t
t=0
and we have the following
24
M0 M1 M2 M3 M4
V0 V1 V2 V3 V4
A0 S0 A1 S1 A2 S2 A3 S3 A4 S4
FIG. 11. Bayesian network of an agent interacting with a memoryless environment channel.
M0 M1 M2 M3 M4
V0 V1 V2 V3 V4
A0 S0 A1 S1 A2 S2 A3 S3 A4 S4
W0 W1 W2 W3 W4
Z0 Z1 Z2 Z3 Z4 Z5
FIG. 12. Bayesian network of an agent receiving percepts from a source. This is an edge case of a percept-action loop where
the environment is modeled as a product environment channel.
Lemma 7. Let env be a product environment channel.Then, for any percept-action loop agtM→←env with a total
distribution p over the variables (M,A,S), that is of the form in eq. (E20), the Bayesian network in Figure 12
MAS
is compatible with p .
MAS
The proof is similar to the proof of Lemma 6.
Proof. By Lemma 5, a distribution over the variables in the Bayesian network shown in Figure 12 fulfills the global
Markov conditions if and only if it factorizes as
∞
(cid:89)
p =p p p p p p p p . (E21)
MASZVW V0 Z0 Mt|Vt At|Vt Vt+1|StMt Zt+1|Wt St|Wt Wt|Zt
t=0
Let p be of the form in eq. (E21). Then, in particular those global Markov which involve only variables
MASZVW
M , A , and S , t∈N , must hold.
t t t 0
Further, SinceproductenvironmentchannelsarehiddenMarkovchannels, byDefinition9foranyproductenviron-
ment channel there must exist a Markov model (Φenv,penv) such that
Z0
∞
(cid:88) (cid:89)
νenv(s|a)= penv(z ) ϕenv(s ,z |a ,z ). (E22)
S|A Z0 0 t t+1 t t
z t=0
Further, by the definition of product environment channels (Definition 8) we have νenv(s|a) = νenv(s|a′) for all
S|A S|A
a,a′ ∈ MN0. Thus, for product environment channels, eq. (E22) must still hold if one sets all actions on the
right-hand side in eq. (E22) to some a∈A. In this case, we obtain
∞
νenv(s|a)= (cid:88) penv(z ) (cid:89) ϕ˜env(s ,z |z ), (E23)
S|A Z0 0 t t+1 t
z t=0
where we defined a new |S×Z|×|Z| transition matrix Φ˜env with coefficients
ϕ˜env(s ,z |z )=ϕenv(s ,z |a,z ). (E24)
t t+1 t t t+1 t
25
Plugging eq. (E23) into eq. (E20) yields
∞
p (m,a,s)= (cid:88) penv(z )pagt (a ,m ) (cid:89) ϕ˜env(s ,z |z )θagt(a ,m |s ,m ), (E25)
MAS Z0 0 A0M0 0 0 t t+1 t t+1 t+1 t t
z t=0
for the global distribution.
All that is left to show is that the distribution in eq. (E25) can be recovered through marginalization from a
distribution of the form in eq. (E21).
For all t∈N , we set V =A M and W =S Z and
0 t t t t t t+1
p (v |s ,m )=θagt(a ,m |s ,a ) for all v =(a ,m ), and (E26)
Vt+1|StMt t+1 t t t+1 t+1 t t t+1 t+1 t+1
p (w |z )=ϕ˜env(s ,z |z ) for all w =(s ,z ). (E27)
Wt|Zt t t t t+1 t t t t+1
For each t∈N, we consider all terms on the right-hand side of eq. (E21) which contain V and marginalize:
t
(cid:88)
p (m |v )p (a |v )p (v |s m )=θagt(a ,m |s ,a ) (E28)
Mt|Vt t t At|Vt t t Vt|St−1Mt−1 t t−1 t−1 t t t−1 t−1
vt∈V
which follows from V =A M , and thus p and p are delta distributions, and eq. (E26). Similarly, for each
t t t Mt|Vt At|Vt
t∈N , we consider all terms on the right-hand side of eq. (E21) which contain W and marginalize:
0 t
(cid:88) p (z |w )p (s |w )p (w |z )=ϕ˜env(s ,z |z ) (E29)
Zt+1|Wt t+1 t St|Wt t t Wt|Zt t t t t+1 t
wt∈W
which follows from W =S Z and eq. (E27).
t t t+1
Finally, we consider all terms on the right-hand side of eq. (E21) which contain V and marginalize:
0
(cid:88)
p (v )p (m |v )p (a |v )=p (a ,m ), (E30)
V0 0 M0|V0 0 0 A0|V0 0 0 A0M0 0 0
v0∈V
which follows from V =A M . Finally, let p be such that p =pagt .
0 0 0 V0 A0M0 A0M0
We thus constructed a distribution p such that marginalizing out V, W, and Z yields eq. (E25). □
MASZVW
In Bayesian networks of percept-action loops, there can in general be infinitely many paths between two nodes X
and Y, as the total process MASZ extends to the infinite future. However, note that paths that go through nodes
that lie in the future of both X and Y must necessarily contain a collider. Those paths are therefore d-separated if
the collider and all of its children are not part of the separating set.
3. Existing approaches to the information theory of percept-action loops
In the previous section, we introduced a Bayesian network (Figure 10) for a general class of percept-action loops.
Existing information-theoretic treatments of percept-action loops such as [58, 80–82] also provide Bayesian networks,
see for example [58, figure 1], [80, equation 11], [81, figure 4.1b], and [82, figure 4]. These Bayesian networks mainly
deviate from our network in how the agent dynamics is modeled.
The difference between our network and the ones from the literature can be understood as follows. Since we model
the environment (respectively the agent) with a Markov channel on an input-output and a hidden-state register (see
Figure 9) we focus on incoming and outgoing random variables of this channel while being agnostic to its inner
workings. In comparison, from the perspective of our framework, existing approaches model variables inside the
channel (such as V in Figure 9c). For example, we recover the Bayesian network in [80, equation 11] from Figure 10
by considering variables W and V as the agent’s memory while ignoring variables M and Z . While our approach
t t t t
requires the introduction of auxiliary hidden variables V and W to obtain a compatible Bayesian network, we only
t t
needasingle transitionmatrixtomodeltheagent(in[58,80–82]twotransitionmatricesarenecessary). Accordingly,
our model is suitable in those contexts where one wishes to model the environment (respectively the agent) with a
single Markov channel on an input-output and a memory register.
26
Appendix F: Maximally predictive agent models
Incomputationalmechanics,theconceptofamaximallypredictiveMarkovmodelisbasedontheideathatinorder
to optimally predict the future, the model’s memory must store all relevant information from the past. A commonly
studied scenario involves a fixed input process X, which is transformed by a channel into an output process Y. In
this context, a Markov model with memory states M is defined as maximally predictive at time t if [17, 30]:
I[X ;X |M ]=0 (F1)
0:t t:∞ t
and
I[M ;X |X ]=0. (F2)
t t:∞ 0:t
ThefirstconditioncapturesthenotationofamaximallypredictivememoryM whilethesecondconditionstatesthat
t
theMarkovmodelcannotpredicttheinputsbeyondtheircorrelations withthepast. Assumingthechannelis causal,
as we do in this work, the latter simply corresponds to a d-separation.
However, it is important to notice that the above definition of maximally predictive Markov models was made in
thecontextofstationaryergodicprocesseswithoutfeedback(seee.g.,[30]),i.e.,whereoutputsdonotinfluencefuture
inputs. Itturnsoutthat,inordertolifttheseassumptions,weneedtosuitablygeneralizethedefinitionofmaximally
predictive Markov models. As we will show, for the special case of stationary processes without feedback we recover
eq. (F1).
In the following, we use the convention that, for variables W,X,X ,Y,Z,Z with n,t∈N ,
n:t n:t 0
I[W;X Y|Z]=I[W;Y|Z] (F3)
n:t
and
I[X;Y|Z ]=I[X;Y] (F4)
n:t
if t≤n.
Definition 12. Let agt→←env be a percept-action loop. A model agtM for agt is said to be maximally predictive, or
for short predictive, of percept S in round t if
t
I[A S ;S |M ]=0, (F5)
0:t+1 0:t t t
and an agent model is said to be asymptotically mean (a.m.) predictive if
⟨I[A S ;S |M ]⟩ =0. (F6)
0:t+1 0:t t t t
In the following, A→←env denotes the set of agent models which are a.m.predictive for an environment channel env.
pred
NotethatCes`arolimitineq.(F6)existssinceconditionalmutualinformationcanberewrittenasasumof(positive
and negative) entropy rates, each of which converges by Lemma 3.
AnagentmodelwhichispredictiveattimetmustencodeinitsmemoryM allinformationfrompastperceptsS
t 0:t
and actions A (including the current action) which helps predicting the current percept S .
0:t+1 t
By Equation (F6), an agent is a.m. predictive if eq. (F5) holds asymptotically in the Ces`aro sense. There are
multiple ways this condition can be satisfied. One possibility is that the agent is predictive for sufficiently many
rounds (e.g., ona subset of N with unitnatural density). Alternatively, anagentwouldalso bea.m. predictive if the
0
summands in eq. (F6), I[A S ;S |M ], decay sufficiently fast — say as 1/n as n→∞. Arguably the simplest
0:n+1 0:n n n
case(whichalreadyreceivedsomeattentionintheliterature[19])iswhentheagentispredictiveatalltimes,meaning
that eq. (F5) holds for all n∈N . In that case, there is an equivalent condition for a Markov model to be predictive.
0
Based on this equivalence, we will show that our definition of a.m.predictive Markov models reduces to the condition
in eq. (F1) from [30] when applied to stationary processes.
Lemma 8. Letagt→←envbeapercept-actionloop. Anagentmodel agtMispredictive ofthenextpercept atalltimes,
i.e.,
I[A S ;S |M ]=0 ∀t∈N , (F7)
0:t+1 0:t t t 0
if and only if it is predictive of all future percepts at all times,
I[A S ;S |M ]=0 ∀t∈N . (F8)
0:t+1 0:t t:∞ t 0
27
Proof. (⇐) Suppose that I[A S ;S |M ] = 0 for all t ∈ N . By using the single-step chain rule of mutual
0:t+1 0:t t:∞ t 0
information (eq. (A9)), with W =A S , X =S , Y =S , and Z =M , we can write
0:t+1 0:t t t+1:∞ t
I[A S ;S |M ]=I[A S ;S |M ]+I[A S ;S |M S ] (F9)
0:t+1 0:t t:∞ t 0:t+1 0:t t t 0:t+1 0:t t+1:∞ t t
for all t ∈ N . Since the left-hand side vanishes by assumption (eq. (F8)), the nonnegativity of mutual infor-
0
mation implies that both terms on the right-hand side must independently vanish. In particular, that means
I[A S ;S |M ]=0 for all t∈N .
0:t+1 0:t t t 0
(⇒) The proof proceeds in two steps. First, we will show that
I[A S ;A S |M A S ]=0 (F10)
0:t+1 0:t j+1 j t t+1:j+1 t:j
for an arbitrary t∈N , and for j ∈{t,t+1,...}. Second, the proof is concluded by an application of the chain rule
0
of mutual information.
In order to show eq. (F10), first consider the case j =t: Using the chain rule of mutual information in the form of
eq. (A10) with W =A S , X =S , Y =A and Z =M gives
0:j+1 0:j j j+1 j
I[A S ;A S |M ]=I[A S ;S |M ]+I[A S ;A |M S ]. (F11)
0:j+1 0:j j+1 j j 0:j+1 0:j j j 0:j+1 0:j j+1 j j
However, both terms on the right-hand side vanish, the first by assumption (eq. (F7)) and the second due to d-
separataion (see Figure 13), leaving us with
I[A S ;A S |M ]=0 (F12)
0:j+1 0:j j+1 j j
for j ∈N . But eq. (F12) is just eq. (F10) with t=j.
0
Mj−3 Mj−2 Mj−1 Mj Mj+1 Mj+2 Mj+3
Aj−3 Sj−3 Aj−2 Sj−2 Aj−1 Sj−1 Aj Sj Aj+1 Sj+1 Aj+2 Sj+2 Aj+3
Zj−3 Zj−2 Zj−1 Zj Zj+1 Zj+2 Zj+3
FIG. 13. Bayesian network for a percept-action loop (lemma 6), used in the proof of Lemma 8. Here blue nodes d-separates
red and green nodes.
What is left to show is the case where j > t. First note that eq. (F12) still holds in that case. Additionally
we will make use of several other conditions involving the random variables A S , M , A S , M and
0:j+1 0:j t t+1:j+1 t:j j
A S . Relations between those random variables can be represented by the information diagram in Figure 14. For
j+1 j
example,eq.(F10)thencorrespondstotwoinformationatomsinthediagram,l+f. Altogetherwehavethefollowing
conditions:
I[A S ;A S |M ]=0=a+b+c+d+e+f, (F13)
0:j+1 0:j j+1 j j
I[A S ;M |M A S ]=0=k, (F14)
j+1 j t j 0:j+1 0:j
I[A S ;M |M A S ]=0=m+l, (F15)
0:t+1 0:t j t t+1:j+1 t:j
I[A S ;M |M A S A S ]=0=m, (F16)
0:t+1 0:t j t t+1:j+1 t:j j+1 j
where the last equality in each line expresses the condition through the information atoms defined in Figure 14.
The first condition, eq. (F13), is just eq. (F12). The conditions in eqs. (F14) to (F16) follow from d-separation (see
Figure 15 where for visualization purposes we set t to j−2).
From the information diagram in Figure 14 we see that eq. (F13) and eq. (F14) allow us to write
I[M A S ;A S |M ]=a+b+c+d+e+f +k =0. (F17)
t 0:j+1 0:j j+1 j j
28
A S
t+1:j+1 t:j
M t M j
A S A S
0:t+1 0:t j+1 j
FIG. 14. Information diagram used in the proof of Lemma 8. Relevant information atoms are labeled.
Rewriting the left-hand side using the chain rule for mutual information in the form of eq. (A10) with W =A S ,
j+1 j
X =M A S , Y =A S and Z =M gives
t t+1:j+1 t:j 0:t+1 0:t j
I[M A S ;A S |M ]=I[M A S ;A S |M ]+I[A S ;A S |M M A S ]. (F18)
t 0:j+1 0:j j+1 j j t t+1:j+1 t:j j+1 j j 0:t+1 0:t j+1 j t j t+1:j+1 t:j
Since the left-hand side vanishes (eq. (F17)), the nonnegativity of mutual information implies that both terms on the
right-hand side must independently vanish; in particular, I[A S ;A S |M M A S ]=f =0. Further,
0:t+1 0:t j+1 j t j t+1:j+1 t+j
since eq. (F15) and eq. (F16) imply l=0, we can then write
I[A S ;A S |M A S ]=f +l=0 (F19)
0:t+1 0:t j+1 j t t+1:j+1 t:j
for all j >t. Together, this then completes the proof of eq. (F10) for all j ≥t.
Applying the chain rule of mutual information (eq. (A9)) to eq. (F10) yields
∞
(cid:88)
I[A S ;A S |M A S ]=I[A S ;A S |M ]=0. (F20)
0:t+1 0:t j+1 j t t+1:j+1 t:j 0:t+1 0:t t+1:∞ t:∞ t
j=t
Further, by the chain rule of mutual information ( eq. (A10)) we have
0=I[A S ;A S |M ] (F21)
0:t+1 0:t t+1:∞ t:∞ t
=I[A S ;S |M ]+I[A S ;A S |M A ]. (F22)
0:t+1 0:t t:∞ t 0:t+1 0:t t+1:∞ t:∞ t t+1:∞
Now, by the nonnegativity of mutual information, each summand ion the right-hand side must vanish individually.
In particular, (I[A S ;S |M ]=0 which concludes the proof of the lemma.
0:t+1 0:t t:∞ t
The previous lemma can be used to show that definition 12 reduces to the condition given in eq. (F1) in the case
wheretheglobalprocessisstationaryandtheenvironmentismodeledbyaproductenvironmentchannel. Astochastic
process is said to be stationary if its distribution p admits [33, p.87]
X
p =p (F23)
Xn:m Xn+t:m+t
for all n,t∈N and m>n where p is obtained from p through marginalization.
0 Xn:m X
Theorem6. LetagtM→←envbesuchthatthejointprocessMAS ofactions,percepts,andagentmemoryisstationary.
Then, agtM is a.m.predictive, i.e.,
⟨I[A S ;S |M ]⟩ =0 (F24)
0:t+1 0:t t t t
if and only if
I[A S ;S |M ]=0 ∀t∈N . (F25)
0:t+1 0:t t:∞ t 0
If in addition env is a product channel (definition 8), agtM is a.m.predictive if and only if
I[S ;S |M ]=0 ∀t∈N . (F26)
0:t t:∞ t 0
29
(eq. (F14))
Mj−3 Mj−2 Mj−1 Mj Mj+1 Mj+2 Mj+3
Aj−3 Sj−3 Aj−2 Sj−2 Aj−1 Sj−1 Aj Sj Aj+1 Sj+1 Aj+2 Sj+2 Aj+3
Zj−3 Zj−2 Zj−1 Zj Zj+1 Zj+2 Zj+3
(eq. (F15))
Mj−3 Mj−2 Mj−1 Mj Mj+1 Mj+2 Mj+3
Aj−3 Sj−3 Aj−2 Sj−2 Aj−1 Sj−1 Aj Sj Aj+1 Sj+1 Aj+2 Sj+2 Aj+3
Zj−3 Zj−2 Zj−1 Zj Zj+1 Zj+2 Zj+3
(eq. (F16))
Mj−3 Mj−2 Mj−1 Mj Mj+1 Mj+2 Mj+3
Aj−3 Sj−3 Aj−2 Sj−2 Aj−1 Sj−1 Aj Sj Aj+1 Sj+1 Aj+2 Sj+2 Aj+3
Zj−3 Zj−2 Zj−1 Zj Zj+1 Zj+2 Zj+3
FIG. 15. Bayesian networks for a percept-action loop (lemma 6) with colorized d-separations (blue d-separates red and green)
used in the proof of Lemma 8.
Proof. For the first part of the theorem, we rewrite eq. (F24) as
lim c =0 (F27)
N
N→∞
where we define
N−1
c := (cid:88) b t (F28)
N N
t=0
b :=I[A S ;S |M ]. (F29)
t 0:t+1 0:t t t
First, we will show that b is nonnegative, bounded and monotone increasing as t → ∞. Clearly, nonnegativity is
t
given since conditional mutual information is nonnegative, and the expression for b is upper bounded by log|Y|. In
t
30
Mt−3 Mt−2 Mt−1 Mt Mt+1 Mt+2 Mt+3
At−3 St−3 At−2 St−2 At−1 St−1 At St At+1 St+1 At+2 St+2 At+3
Zt−3 Zt−2 Zt−1 Zt Zt+1 Zt+2 Zt+3
FIG.16. Bayesiannetworkforanproductenvironmentchannel(lemma7)withcoloredd-separarion(blued-separatesredand
green) used in the proof of Theorem 6.
order to show that (b ) is monotone increasing, we use the chain rule for mutual information in the form of eq. (A10)
t
with W =S , X =A S , Y =A S and Z =M :
t+j j+1:t+j+1 j:t+j 0:j 0:j t+j
I[A S ;S |M ]=I[A S ;S |M ]+I[A S ;S |M A S ]. (F30)
0:t+j+1 0:t+j t+j t+j j:t+j+1 j:t+j t+j t+j 0:j 0:j t t j:t+j+1 j:t+j
Using stationarity (eq. (F25)) of the process MAS, we find p = p for any
M0:t+1A0:t+1S0:t+1 Mj:t+j+1Aj:t+j+1Sj:t+j+1
t,j ∈N , whichcanbemarginalizedtothestatementp =p . Thus, I[A S ;S |M ]=
0 MtA0:t+1S0:t Mt+jAj:t+j+1Sj:t+j 0:t+1 0:t t t
I[A S ;S |M ]. Plugging this into eq. (F30) yields
j:t+j+1 j:t+j t+j t+j
I[A S ;S |M ]=I[A S ;S |M ]+I[A S ;S |M A S ]. (F31)
0:t+j+1 0:t+j t+j t+j 0:t+1 0:t t t 0:j 0:j t t j:t+j+1 j:t+j
From this, using the nonnegativity of mutual information, we obtain
I[A S ;S |M ]≥I[A S ;S |M ] ∀t,j ∈N , (F32)
0:t+j+1 0:t+j t+j t+j 0:t+1 0:t t t 0
or equivalently b ≥b , which proves that (b ) is monotone increasing.
t+j t t
Further, since c is defined as the arithmetic mean of b ,b ,...,b , we have that c is bounded and monotone
N 0 1 N−1 N
increasing as N →∞.
We are now in the position to prove the first part of the theorem. By the monotone convergence theorem and the
properties of c , the limit lim c exists and equals the supremum. Therefore, eq. (F24) holds true if and only if c
N N N
N→∞
is zero for all N ∈ N which, in turn, is the case if and only if b is zero for all t ∈ N . Further, by lemma 8, this is
0 t 0
equivalent to eq. (F26) which concludes the proof of the first part of the theorem.
For the second part of the theorem, we need to show that, given the assumption that the environment channel is
also a product channel, eq. (F25) is equivalent to eq. (F26). Using the single-step chain rule of mutual information
(eq. (A10)), we can split up eq. (F25) as
I[A S ;S |M ]=I[S ;S |M ]+I[A ;S |M S ] ∀t∈N (F33)
0:t+1 0:t t:∞ t 0:t t:∞ t 0:t+1 t t 0:t 0
Thesecondtermontheright-handsidevanishesforproductenvironmentchannelsduetod-separation(seeFigure16)
and the first term corresponds to eq. (F26) which concludes the proof.
The next theorem provides a condition for the existence of predictive agent models:
Theorem 7. Let agt→←env be any percept-action loop. If the environment channel is unifilar, then there exists an
a.m. predictive agent model agtM for agt.
The proof is based on the idea that the agent’s memory can be extended to store and update the hidden state
oftheunifilarenvironmentmodel. Knowledgeofthehiddenstatesofanenvironmentmodelmakestheagentpredictive.
Proof. The proof proceeds by construction.
Let
• agtM′ =(Θagt,p ) be a Markov model for agt with memory states M′;
M
0
′A0
• envM=(Φenv,p ) be a unifilar Markov model for env on some hidden-state alphabet Z.
Z0
31
We will now construct a transition matrix Θ
MY
on M×Y, where Y is the input-output alphabet of agt→←env and
M=M′×Y′×Z′ (F34)
where Y′ and Z′ are copies of Y and Z, respectively, and M′ is the hidden-state alphabet of agtM′.
Let Θ decompose as shown in the following circuit diagram, Figure 17. that is,
MY
M′ M′
Θagt
M′Y
Y Y
=
Θ
MY
Y′ Y′ copy
Z′ Z′ u(s,a,z)
U Y′Z′Y Γ YY′
FIG. 17. Circuit diagram for the decomposition of Θ . Time flows from left to right, wires correspond to alphabets, boxes
MY
tooperationsontherespectivealphabets,bulletsonwiresindicatethatthealphabetvaluecontrolsanotheroperation(above,
the unifilarity map u and a copy operation, respectively) but does not change itself.
Θ =(Γ ⊗1 ) (cid:0) Θagt ⊗1 (cid:1) (U ⊗1 ). (F35)
MY YY′ MZ′ M′Y Y′Z′ Y′Z′Y M′
For clarity, indices indicate the memories on which the respective transition matrices act, in particular (from right to
left)
• 1 is the identity matrix on the memories indicated as indices,
• Θagt =Θagt is the transition matrix of agtM’,
M′Y
• U is a deterministic transition matrix which acts as the identity on Y′Y and which sets the state of Z′
Y′Z′Y
to u(y′,y,z′) where y′, y, and z′ are the current symbols on memories Y′, Z′, and Y, respectively, and u is a
unifilarity map (see the discussion below definition 7) of the unifilar environment model envM,
• Γ isadeterministictransitionmatrixwhichcopiesthesymbolofmemoryY toY′ whileleavingY unchanged.
YY′
By construction, each of the three factors on the right-hand side of eq. (F35) is a valid transition matrix mapping
M×Y =M′×Y′×Z′×Y to itself and thus Θ is also.
MY
Defineδ tobeoneifi=j andzerootherwise. Definethedistributionp =p p p wherep isfromagtM’,
i,j M0 M
0
′ Y
0
′ Z
0
′ M
0
′
p (y) = δ where y is the initial action, and p (z) = δ where z is the initial hidden state of envM (recall
Y
0
′ y,y0 0 Z
0
′ z,z0 0
that by unifilarity there exists a definite initial state). Further, define agtM=(Θ ,p ,p ).
MY M0 A0
Byeq.(F35),thetransitionmatrixofagtMfirstappliesthetransitionmatrixof agtM’,thenupdatestheZ′ memory
using the unifilarity map, and updates the Y′ memory by copying Y to Y′.Thus, the only term which can lead to a
change of the Y and M′ memories is Θagt . Further, p and p coincide on M′. Therefore, agtM and agtM′ both
M′Y M0 M 0 ′
model agt.
What is left to show is that agtM is a.m.predictive. For this, note that M = (M′,Y′,Z′) is initialized such that
t t t t
Z′ = Z and Y′ = A . Further, by construction, agtM updates the Z′ and Y′ memories such that Z′ = Z and
0 0 0 0 t t
Y′ =Y for all times. We then have
t t
I[A S ;S |M ]=I[A S ;S |M′Y′Z′] (F36)
0:n+1 0:n n n 0:n+1 0:n n n n n
=I[A S ;S |M′A Z ] (F37)
0:n+1 0:n n n n n
=0, (F38)
where in eq. (F37) we used that Z′ = Z and Y′ = Y and eq. (F38) follows from d-separarion in the Bayesian
n n n n
network of agtM→←envM, see Figure 19. □
Corollary 3. Let agt→←env be any percept-action loop with env a unifilar source environment. Then there exists an
a.m. predictive agent model agtM for agt.
32
However, for future reference we point out that the proof and in particular the construction of the predictive
agent model can be simplified since, for unifilar source environments, there exist unifilar models whose hidden state
z′ = u(s,z) are a function of the current percept s and the current hidden state z but not of the action. Thus, the
decomposition of Θ in Figure 17 can be replaced by the simpler decomposition in Figure 18 and, in analogy to the
proof of theorem 7, one can show that agtM is predictive.
M′ M′
Θagt
M′Y
Y Θ Y
MY
=
Z′ Z′ u(s,z)
U Z′Y
FIG. 18. Circuit diagram for the decomposition of Θ for unifilar source environments.
MY
Mj−3 Mj−2 Mj−1 Mj Mj+1 Mj+2 Mj+3
Aj−3 Sj−3 Aj−2 Sj−2 Aj−1 Sj−1 Aj Sj Aj+1 Sj+1 Aj+2 Sj+2 Aj+3
Zj−3 Zj−2 Zj−1 Zj Zj+1 Zj+2 Zj+3
FIG. 19. Bayesian network for a percept-action loop (lemma 6) with colorized d-separarion (blue d-separates red and green)
used in the proof of Theorem 7.
Appendix G: The extractable work in percept-action loops
In this framework, memory is represented by a physical system coupled to a thermal reservoir at temperature T.
The system possesses a few degrees of freedom, the information-bearing degrees of freedom, which are assumed to
be meta-stable, i.e., their equilibration time τ is much larger than that of the system’s other degrees of freedom,
info
τ . Information processing on the information-bearing degrees of freedom is carried out through an isothermal
others
protocol, i.e., a protocol executed at constant temperature T, with a time scale such that τ ≪ τ ≪ τ .
others protocol info
The protocol has access to a work reservoir for storing (or retrieving) work.
1. Derivation of work capacity
Let X be a finite set of information-bearing degrees of freedom of an information reservoir [37], p an arbitrary
Xin
initial distribution over X, and Φ = (ϕ(j|i)) an arbitrary transition matrix where i,j ∈ X. Then, given that the
j,i
availableknowledgeofabouttheinformation-bearingdegreesoffreedomX isp ,theworkW whichonecanexpect
Xin
(with respect to p ) to extract by implementing an isothermal process realizing Φ on X is upper-bounded by the
Xin
second law of thermodynamics as [7, 8]
W ≤H(X )−H(X ) (G1)
out in
where W is in units of k T ln2, and X is distributed as
B out
(cid:88)
p (x )= ϕ(x |x )p (x ), (G2)
Xout out out in Xin in
xin∈X
33
calledtheoutputdistribution. Notethattheupperboundineq.(G1)canbepositive,negative,orzero. Inparticular,
if the expected extracted work W is negative, realizing the isothermal process requires work, if it is positive, work
can be gained.
If an agent implements an isothermal process such that the expected extracted work equals the upper bound in
eq. (G1), we say that the agent is Landauer efficient, in reference to Landauer’s bound on the erasure of one bit,
which is a special case of eq. (G1).
Basedontheassumptionthateq.(G1)holds, wewillderiveanupperboundontheworkanagentagtMcanexpect
to extract by undergoing a percept-action loop with an environment env.
(cid:16) (cid:17)
Let agtM→←env= Θagt,pa
M
g
0
t
A0
,ν
S
en
|A
v be a percept-action loop with identical action and percept alphabets A=S
and memory alphabet M of the agent. Then, based on eq. (G1), the work an agent can expect to extract by
implementing Θagt in between rounds (channel uses) t and t+1 is upper bounded by
W ≤H(A ,M )−H(S ,M ). (G3)
t→t+1 t+1 t+1 t t
Taking the Ces`aro limit (for a definition, see eq. (A14)), we find an upper bound on the expected extracted work per
round:
⟨W ⟩ ≤⟨H(A ,M )−H(S ,M )⟩ . (G4)
t→t+1 t t+1 t+1 t t t
It is convenient to regroup terms in the Ces`aro sum on the right-hand side of this expression:
n−1
1 (cid:88)
⟨H(A ,M )−H(S ,M )⟩ = lim [H(A ,M )−H(S ,M )] (G5)
t+1 t+1 t t t n→∞n t+1 t+1 t t
t=0
(cid:32) n−1 (cid:33)
1 (cid:88)
= lim H(A ,M )+ [H(A ,M )−H(S ,M )] (G6)
n→∞n 0 0 t+1 t+1 t t
t=0
(cid:32)n−1 (cid:33)
1 (cid:88)
= lim [H(A ,M )−H(S ,M )] (G7)
n→∞n t t t t
t=0
=⟨H(A ,M )−H(S ,M )⟩ (G8)
t t t t t
where in eq. (G6) we added H(A ,M ) inside the Ces`aro limit which does not change the result because it vanishes
0 0
as n→∞.
Then, we can rewrite the argument of the Ces`aro limit using twice the definition of conditional entropy:
H(A ,M )−H(S ,M )=H(A |M )+H(M )−H(S |M )−H(M ) (G9)
t t t t t t t t t t
=H(A |M )−H(S |M ). (G10)
t t t t
We define
W
t
(agtM→←env):=H(A
t
|M
t
)−H(S
t
|M
t
) (G11)
as the extractable work for round t and
W(agtM→←env):=⟨H(A
t
|M
t
)−H(S
t
|M
t
)⟩
t
(G12)
as the work rate, the a.m.extractable work (both in units of k T ln2).
B
2. Existence of Landauer-efficient agents
Theboundonexpectedextractedworkforasingleisothermalimplementationofatransitionmatrix, eq.(G1), can
be reached using efficient protocols. These protocols typically have idealized requirements such as arbitrary energy
functions or infinite timescales; see for example [19] for a protocol based on over-damped Brownian motion in a
controllable energy landscape.
In the following we will outline, for any percept-action loop agtM→←env and provided that such idealized protocols
are available, how an implementation for agtM can be found which extracts all a.m.extractable work, eq. (G12) using
only finite memory. Such agents will be called Landauer efficient.
34
FIG. 20. An agent interacting with the cascade of two environment channel env1 and env2.
To this end, recall that any agtM→←env can be represented through a finite-state global Markov process of some
agtM→←envM which models agtM→←env, see lemma 4. By corollary 1, this process is asymptotically periodic with a
finite period in the sense of corollary 1. Let d be this period. That is, there are only d asymptotically expected
input distributions for the agent’s transition matrix, lim p for c∈{1,2,...,d}, which repeat in the same
n→∞
Mdn+cSdn+c
periodic manner. We will now exploit this to construct a Landauer-efficient agent.
LetusextendtheagentagtMbyaseparatedeterministiccountercwhichstartsat1and,witheveryround,ifc<d
counts up or if c=d resets to 1. This additional counter memory is fully deterministic and thus has zero entropy for
all times. It therefore does not contribute to the extractable work.
Now, consideraprotocolimplementingtheagentwhich, conditionedonthecounterc, implementsoneofdefficient
protocols optimized for the asymptotically expected distribution lim p . Thereby, we have constructed a
n→∞
Mdn+cSdn+c
protocol which implements agtM in a Landauer-efficient way using only finite memory.
3. Definition and properties of work capacity
For a given environment, the extracted work in eq. (G12) depends not only on an agent’s input-output behavior,
as characterized by an agent’s channel agt, but also on the agent’s memory usage, as specified by a model agtM. The
environment’s capacity to do work is then defined as the supremum of the work rate with respect to all agent models
agtM.
Definition 13. The work capacity Cwork of channel env=νenv is defined as
A|S
Cwork(env):= max W(agtM→←env) (G13)
agtM∈A→←env
where W(agtM→←env) := ⟨H(A
t
|M
t
)−H(S
t
|M
t
)⟩
t
is the work rate. An agent model agtM is said to be efficient
with respect to an environment channel env if W(agtM→←env)=Cwork(env) and the set of efficient agents is denoted
A→←env.
eff
In the following we will prove various properties of work capacity.
Theorem 8. For any percept-action loop agtM→←env with action-percept alphabet Y, the channel capacity Cwork(env)
has the following properties:
(i) (Existence) The limit in the definition of Cwork(env) exists,
(ii) (Bounds) 0≤Cwork(env)≤ln|Y|,
(iii) (Subadditivity under channel cascade) Let env = penv1 and env = penv2 be two hidden Markov channels.
1 S|A 2 S|A
Define the cascade env ◦env =penv2◦env1 of env and env as
2 1 S|A 1 2
(cid:88)
penv2◦env1(s|a)= penv2(s|i)penv1(i|a), (G14)
S|A S|A′ S′|A
i∈YN0
see also Figure 20. Then,
Cwork(env ◦env )≤Cwork(env )+Cwork(env ). (G15)
2 1 1 2
35
Before we prove the theorem, the following definition is made.
Definition 14. For any environment channel env, let A→←env denote the set of agent models which interact with env
mea
such that
⟨H(A |M )⟩ =ln|A|, (G16)
t t t
i.e., the a.m.entropy over actions given the agent’s memory is maximal.
The index mea stands for maximum entropy actions.
Proof of (i):
By Lemma 4, the global process U = (U )∞ = (M ,A ,S ,Z )∞ is a homogeneous Markov chain. Let Λ be its
t t=0 t t t t t=0
transition matrix. Then, work capacity, as given in eq. (G13), can be rewritten as
Cwork(env)=max (cid:10) g (cid:0) Λt(cid:1)(cid:11) , (G17)
agtM pU0 t
where g is a function from the set of transition matrices to the real numbers:
pU0
g
(cid:0) Λt(cid:1)
=H(M ,A )−H(M ,S ), (G18)
pU0 t t t t
where p and p are obtained from p = Λtp through marginalization. Since g is continuous, existence
MtAt MtSt Ut U0
follows from corollary 1(iii). □
Proof of (ii):
We first prove the upper bound.
For all t∈N , the summands H(S |M )−H(A |M ) in the expression for work capacity, eq. (G13), are bounded
0 t t t t
from above as
H(S |M )−H(A |M )≤log|Y|−0, (G19)
t t t t
where the upper bound of conditional entropy, H(S |M ) ≤ H(S ) ≤ log(|Y|), was used to obtain an upper bound
t t t
for the first term, and the nonnegativity of conditional entropy, 0 ≤ H(A |M ), was used to obtain an upper bound
t t
forthesecondterm. (NotethatA takesvaluesinY.) Theupperboundineq.(G19)dependsonlyonthedimension
n
of the action-percept alphabet and thus is independent of the choice of the agent Markov model. Applying eq. (G19)
to each summand in eq. (G13) yields the upper bound on work capacity.
What is left is the proof for the lower bound.
The proof proceeds by showing that for any env there exists an agent model agtM which has zero extracted
work in each step. Consider an agent which implements the identity map from percept S to action A , that is
t t+1
p (a |s )=δ for all t∈N . This implies
At+1|St t+1 t at+1,st 0
H(S )=H(A ) (G20)
t t+1
for all t∈N . Further, note that since agt only employs the identity map there exists a memoryless agtM (i.e., with
0
|M|=1) which models it. We thus have
H(S |M )=H(S ) (G21)
t t t
H(A |M )=H(A ). (G22)
t+1 t t+1
Plugging eqs. (G20) to (G22) into the expression for a.m. extracted work (eq. (G12)) yields zero.
We have thus shown that for any environment there exists an agent with zero extracted work. Since the definition
of work capacity involves a maximum with respect to agents, this proves nonnegativity of work capacity for all
environments. □
Proof of (iii):
Let env be the channel which is obtained by alternating between env and env every round, see Figure 21. That
12 1 2
is, the action and percept processes of env are
12
A12 =A1A2A1A2··· (G23)
0 0 1 1
S12 =S1S2S1S2··· (G24)
0 0 1 1
36
FIG. 21. An agent which alternates between using channels env1 and env2.
where Ak (respectively Sk) are the inputs (respectively outputs) of env where k =1,2.
t t k
Then, the work capacity of env is by definition
12
(cid:80)N−1(cid:0)
H
(cid:0) A1|X1(cid:1)
−H
(cid:0) S1|X1(cid:1)
+H
(cid:0) A2|X2(cid:1)
−H
(cid:0) A2|X2(cid:1)(cid:1)
Cwork(env )=max lim t=0 t t t t t t t t , (G25)
12 agtM N→∞ N
where the notation X = X1X2X1X2 was used for the agent’s memory process in order to match the indexing of
0 0 1 1
eqs.(G23)and(G24). Then,byreplacingthesupremumoverasumoftermswithasupremumoverindividualterms,
we obtain an upper bound:
(cid:80)N−1(cid:0)
H
(cid:0) A1|X1(cid:1)
−H
(cid:0) S1|X1(cid:1)(cid:1)
Cwork(env )≤max lim t=0 t t t t + (G26)
12 agtM N→∞ N
(cid:80)N−1(cid:0)
H
(cid:0) A2|X2(cid:1)
−H
(cid:0) A2|X2(cid:1)(cid:1)
+max lim t=0 t t t t (G27)
agtM N→∞ N
=Cwork(env )+Cwork(env ). (G28)
1 2
Further, note that the a.m.extracted work of agents which implement an identity channel from outputs of env to
1
inputs of env is upper bounded by Cwork(env ◦env ). However, since restricting the set of agents can only lead to
2 2 1
a smaller a.m.extracted work we have:
Cwork(env )≥Cwork(env ◦env ). (G29)
12 2 1
Then, eq. (G15) follows by combining eq. (G28) and eq. (G29). □
The following lemma provides simplified expressions for the work capacity (in units of k T ln2) of environment
B
channel env for the classes of channels defined in Definition 8.
Lemma 9.

 0 if env is noiseless,
Cwork(env)= max[H(S 0 )−H(A 0 )] if env is memoryless invariant, (G30)
l p
o
A
g
0
|A|−h(S) if env is a unifilar product channel.
Proof.
(i) Let env be a noiseless channel.
By Definition 8, S = A for a noiseless environment channel. Setting S = A in the expression for work capacity,
t t
eq. (G13) yields Cwork(env)=0.
(ii) Let env be a memoryless invariant channel.
Cwork(env)=max(H(S )−H(A )) will be proven by showing the respective inequalities
0 0
pA0
Cwork(env)≤max[H(S )−H(A )] (G31)
0 0
pA0
Cwork(env)≥max[H(S )−H(A )]. (G32)
0 0
pA0
37
≤: Ingeneral,anupperboundonworkcapacitycanbeobtainedbyoptimizingeachsummandoftheworkcapacity
separately:
Cwork =max⟨H(A ,M )−H(S ,M )⟩ (G33)
t t t t t
agtM
(cid:28) (cid:29)
≤ max[H(A ,M )−H(S ,M )] . (G34)
t t t t
agtM
t
This upper bound simplifies further for memoryless invariant environments as follows. Note that memoryless
invariant environments admit a description with a |Y|×|Y| transition matrix Φ such that the global process at
any time t is given by
p (u )=p (m ,a ,s )=ϕ(s |a )p (m ,a ). (G35)
Ut t MtAtSt t t t t t MtAt t t
Thus,themaximizationineq.(G34)reducestoamaximizationoverp . Infact,thisisthesameoptimization
MtAt
problem for all t since Φ does not depend on t. Thus, the upper bound in eq. (G34) simplifies to
Cwork ≤ max [H(A M )−H(S M )]. (G36)
0 0 0 0
pA0M0
Further, we find
H(A ,M )−H(S ,M )=H(A )+H(M )−I[A ;M ]−H(S )−H(M )+I[S ;M ] (G37)
0 0 0 0 0 0 0 0 0 0 0 0
where we used H(X,Y) = H(X)+H(Y)−I[X;Y] which is easily checked with an information diagram, see
Supplemental Material A. Using I[X ,Y]−I[X ,Y]=I[X ,Y|X ]−I[X ,Y|X ], eq. (G37) becomes
1 2 1 2 2 1
H(A ,M )−H(S ,M )=H(A )−I[A ;M |S ]−H(S )+I[S ;M |A ]. (G38)
0 0 0 0 0 0 0 0 0 0 0 0
Note that I[S ;M |A ] = 0 due to d-separation (see Figure 22). Then, by the nonnegativity of conditional
0 0 0
M0 M1 M2
A0 S0 A1 S1 A2
FIG. 22. Bayesian network for a memoryless environment channel (corollary 2) with colorized d-separation (blue d-separates
red and green) used in the proof of Lemma 9.
mutual information, we find
H(A ,M )−H(S ,M )≤H(A )−H(S ) (G39)
0 0 0 0 0 0
which proves the upper bound.
≥: Consider a memoryless agent model which, for all t, prepares its action in argmax [H(A )−H(S )], i.e.,
pA0 0 0
its extracted work is given by max[H(S )−H(A )]. Since any agent’s extracted work is a lower bound on the
0 0
pA0
work capacity, this proves the lower bound.
Equations (G31) and (G32) imply equality.
(iii) We start by deriving an expression for the a.m.work production under the assumption that the environment
is modeled by a unifilar product environment channel. The a.m.work production in units of k T ln2 is given by
B
eq. (G12),
W(agtM→←env)=⟨H(A
t
|M
t
)−H(S
t
|M
t
)⟩
t
. (G40)
Rewriting the second term in the Ces`aro limit using twice the definition of conditional mutual information eq. (A6)
we find
H(S |M )=H(S |M S A )+I[S A ;S |M ] (G41)
t t t t 0:t 0:t+1 0:t 0:t+1 t t
=H(S |S )−I[S ;M A |S ]+I[S A ;S |M ]. (G42)
t 0:t t t 0:t+1 0:t 0:t 0:t+1 t t
38
Mt−3 Mt−2 Mt−1 Mt Mt+1 Mt+2 Mt+3
At−3 St−3 At−2 St−2 At−1 St−1 At St At+1 St+1 At+2 St+2 At+3
Zt−3 Zt−2 Zt−1 Zt Zt+1 Zt+2 Zt+3
FIG. 23. Bayesian network for an product environment channel (lemma 7) with colorized d-separation (blue d-separates red
and green) used in the proof of Theorem 9.
The term I[S ;M A |S ] vanishes because of the d-separatoin shown in Figure 23. Using linearity of the Ces`aro
t t 0:t+1 0:t
limit and the chain rule of entropy rate (eq. (A16)), we find for the a.m.work production:
W(agtM→←env)=⟨H(A
t
|M
t
)⟩
t
−h(S)−⟨I[S
0:t
A
0:t+1
;S
t
|M
t
]⟩
t
. (G43)
In particular, we see that eq. (G43) is upper bounded by setting the first term to its upper bound (log|Y|) and the
last term to its upper bound (zero):
W(agtM→←env)<log|Y|−h(S). (G44)
Work capacity equals this upper bound if there exist an agent model which saturates it.
Consider now a class of agent models with memory states denoted by M′ which distributes their actions A
t
uniformlyandindependentlyfromitsinputsS ,M′ anditsoutputmemoryM′,i.e.,H(A |M′)=H (cid:0) A |M′ (cid:1) =
t−1 t−1 t t t t t−1
H(A |S )=log|A|. This means, we have
t t−1
p =p p (G45)
M
t
′At|M
t
′
−1
St−1 At M
t
′|M
t
′
−1
St−1
for all t∈N which results in a simplification in the Bayesian network of the percept-action loop, see Figure 24.
0
Further, since the environment is unifilar, by corollary 3 for any such agent agtM′ there exists a predictive agent
model agtM with memory states denoted by M constructed as in Figure 18. For predictive agent models, the last
term in eq. (G43) is zero (definition 12). What is left to show is that H(A |M )=log|A| for agtM. By construction
t t
(Figure 18), we have M =M′Z and thus
t t t
H(A |M )=H(A |M′Z ) (G46)
t t t t t
and by the definition of conditional mutual information:
H(A |M′Z )=H(A |M′)−I[A ;Z |M′]. (G47)
t t t t t t t t
The first term on the right-hand side equals log|A| by the assumptions made for agtM′ and the second term vanishes
due to d-separation (actions are independent from all other variables, see 24).
Thus, work capacity equals the right-hand side in eq. (G44). □.
4. Efficient agent models
Theorem 9. For any unifilar product environment channel env,
A→←env =A→←env∩A→←env, (G48)
eff mea pred
with A→←env the set of efficient agent models (Definition 13), A→←env the set of agent models with a.m.maximum entropy
eff mea
actions (Definition 14), and A→←env the set of predictive agent models (Definition 12).
pred
39
Mt−1 Mt Mt+1
At−1 St−1 At St At+1
Zt−1 Zt Zt+1
FIG. 24. Bayesian network for an product environment channel (lemma 7) and agent with independently and uniformly
distributed actions (see eq. (G45)) used in the proof of Lemma 9.
Proof: Recall eq. (G43), the expression for work rate for a product environment channel:
W(agtM→←env)=⟨H(A
t
|M
t
)⟩
t
−h(S)−⟨I[S
0:t
A
0:t+1
;S
t
|M
t
]⟩
t
. (G49)
First assume that agtM∈A→←env∩A→←env. By Definition 14, agents in A→←env fulfill
mea pred mea
⟨H(A |M )⟩ =log|A|, (G50)
t t t
and, by Definition 12 agents in A→←env fulfill
pred
0=⟨I[S A ;S |M ]⟩ . (G51)
0:t 0:t+1 t t t
Plugging eqs. (G50) and (G51) into eq. (G49) yields W(agtM→←env)=log|Y|−h(S) which equals the work capacity
of unifilar product environment channels according to Lemma 9, and thus agtM∈A→←env.
eff
For the other direction, assume agtM∈A→←env. Then,
eff
0=Cwork(env)−W(agtM→←env) (G52)
=log|A|−⟨H(A |M )⟩ −⟨I[S A ;S |M ]⟩ (G53)
t t t 0:t 0:t+1 t t t
where for the second line we used the expressions for work capacity of product environment channels (Lemma 9) and
extractable work of agents using a product environment channel (eq. (G49)).
Note that −⟨I[S A ;S |M ]⟩ is upper bounded by zero and ⟨H(A |M )⟩ is upper bounded by log|A|. The
0:t 0:t+1 t t t t t t
expressionineq.(G53)isthusupperboundedbyzero. Thus,agtMmustbesuchthatboth upperboundsarereached.
ByDefinition14,thesetofagentswhichreachtheupperboundfor⟨H(A |M )⟩ isA→←env,and,byDefinition12,the
t t t mea
setofagentswhichreachtheupperboundfor−⟨I[S A ;S |M ]⟩ isA→←env. ItfollowsthatagtM∈A→←env∩A→←env.□
0:t 0:t+1 t t t pred mea pred
Theorem9showsthatefficientagentsshouldbeconstructedsuchthattheyarepredictivewhenevertheenvironment
is modeled by a unifilar product environment channel. This, however, is no longer true for general environment
channels. Wefirstprovethefollowinglemmawhichshowspropertiesforaparticularmemorylessenvironmentchannel.
Lemma 10. Let environment env be a memoryless environment channel and such that A and S take values in an
t t
alphabet A = S = {0,1}. Let the environment’s transition matrix Φenv = (ϕenv(j|i)) with j,i ∈ A be such that
j,i
ϕenv(j|0)=δ
0,j
and ϕenv(j|1)=1/2 for j =0,1. Then, for any agtM→←env we have
⟨I[A ;S |M ]⟩ =0⇔⟨H(A |M )⟩ =0. (G54)
t t t t t t t
Proof.
First note that if an agent model agtM admits ⟨H(A |M )⟩ =0, then
t t t
⟨I[A ;S |M ]⟩ =⟨H(A |M )⟩ −⟨H(A |M S )⟩ =0, (G55)
t t t t t t t t t t t
where we used the definition of mutual information (eq. (A6)), and the conclusion follows from the nonnegativity of
conditional mutual information and conditional entropy, proving one direction of eq. (G54).
40
For the other direction, for the environment env under consideration, by definition 6 there exists a Markov
model agtM on some state space Z and thus, by lemma 4, there also exists a global Markov chain. Let Γ be
the transition matrix and p the initial distribution of such a global Markov chain. By corollary 1)(i),
M0A0S0Z0
the global Markov chain must consist of convergent subsequences Γ(r) = lim Γnd+r with r ∈ {1,2,...,d} and d
∞
n→∞
(cid:16) (cid:17)
some finite integer. Let Γ(r) = γ(r)(j|i) and let M , A , S , and Z be random variables with distribution
∞ ∞ r r r r
j,i
p (j)= (cid:80) γ(r)(j|i)p (i) with i,j ∈M×A×S×Z. Then, according to corollary 1(iii), we have
ArSrMrZr i ∞ M0A0S0Z0
d
1(cid:88) (cid:2) (cid:3)
⟨I[A ;S |M ]⟩ = I A ;S |M , (G56)
t t t t d r r r
r=1
and similarly
d
1(cid:88) (cid:0) (cid:1)
⟨H(A |M )⟩ = H A |M . (G57)
t t t d r r
r=1
Using the definition of mutual information, we find for each summand in eq. (G56)
(cid:2) (cid:3) (cid:0) (cid:1) (cid:0) (cid:1)
I A ;S |M =H A |M −H A |M S . (G58)
r r r r r r r r
(cid:2) (cid:3) (cid:0) (cid:1)
We now want to show that, for any r ∈{1,2,...,d}, I A ;S |M =0 implies H A |M =0.
r r r r r
(cid:2) (cid:3) (cid:0) (cid:1)
The proof proceeds by contraction. Assume that I A ;S |M =0 but H A |M >0.
r r r r r
(cid:0) (cid:1) (cid:80) (cid:0) (cid:1)
First,usingbasicpropertiesofconditionalentropies,wehaveH A |M = p (m)H A |M =m where
(cid:0) (cid:1)
r r m∈M Mr r r
H A |M =m =0 iff p is a delta distribution.
The
r
n, d
r
uetoH (cid:0) A |M
A (cid:1)r|
>
Mr
0
=
,
m
thereexistsamemorystatem′ ∈Mwithp (m′)>0suchthatp (0|m′)>0
r r r Mr r Ar|Mr r
and p (1|m′)>0. We have
Ar|Mr r
(cid:0) (cid:1) (cid:88) (cid:2) (cid:3)
I A ;S |M = p (m )I A ,S |M =m (G59)
r r r Mr r r r r r
mr∈M
(cid:2) (cid:3) (cid:2) (cid:3)
where I A ,S |M =m is the mutual information I A ,S with A , S distributed as p . The expan-
r r r r r r r r ArSr|Mr=mr
sion in eq. (G59) can be obtained by writing out mutual information, eq. (A6), in terms of probabilities.
Now, by the nonnegativity of mutual information, for left-hand side of eq. (G59) to vanish, each summand
on the right-hand side of eq. (G59) must vanish individually. In particular, for the summand corresponding to
M = m′ to vanish, I (cid:2) A ,S |A =m′(cid:3) must be zero. Further, using basic properties of mutual information,
r r r r r r
I
(cid:2)
A ,S |M
=m′(cid:3)
= 0 iff p is a product distribution. However, note that for percept-action loops
r r r r ArSr|Mr=m′
r
with memoryless environment channel we have
p =p p (G60)
ArSr|Mr=m′
r
Sr|Ar Ar|Mr=m′
r
wherep (s|a)=ϕenv(s|a)isgivenbythememorylessenvironmentwhichischosensuchthatϕenv(s|0)̸=ϕenv(s|1)
Sr|Ar
for all s ∈ S and, thus, p is not a product distribution. By this contradiction, we have shown, for any
(cid:2)
ArSr|Mr=m
(cid:3)
′
r (cid:0) (cid:1)
r ∈ {1,2,...,d}, that I A ;S |M = 0 implies H A |M = 0. By eqs. (G56) and (G57) it then follows that
r r r r r
⟨I[A ;S |M ]⟩ =0, implies ⟨H(A |M )⟩ =0. □
t t t t t t t
Theorem 10. There exist environment channels env such that the sets A→←env, A→←env, and A→←env are all nonempty and
pred mea eff
mutually exclusive.
Proof. We start with noticing that A→←env and A→←env are not empty for any environment. Further, a.m.predictive
eff mea
agents (definition 12) must fulfill
0=⟨I[A S ;S |M ]⟩ (G61)
0:t+1 0:t t t t
=⟨I[A ;S |M ]⟩ +⟨I[A S ;S |M A ]⟩ (G62)
t t t t 0:t 0:t t t t t
wherethesecondlinefollowsfromthechainruleofmutualinformation(eq.(A10)). Further,hereandinthefollowing
we make repeated use of the fact that the Ces`aro limit is linear for terms which converge individually.
41
Mt−3 Mt−2 Mt−1 Mt Mt+1 Mt+2 Mt+3
At−3 St−3 At−2 St−2 At−1 St−1 At St At+1 St+1 At+2 St+2 At+3
FIG. 25. Bayesian network for a memoryless environment channel (corollary 2) with colorized d-separation (blue d-separates
red and green) used in the proof of Theorem 10.
From now on, let env be the memoryless environment considered in lemma 10. Then, the second term vanishes
because of d-separation, I[A S ;S |M A ]=0, depicted in Figure 25. Further, for the environment under consid-
0:t 0:t t t t
eration we have by lemma 10,
⟨I[A ;S |M ]⟩ =0⇔⟨H(A |M )⟩ =0. (G63)
t t t t t t t
Inparticular, wehavejustseenthattheleft-handsideofeq.(G63)istheconditionforanagenttobea.m.predictive.
Then, since there exist agents which remember their actions perfectly in the Ces`aro sense, i.e., they fulfill the right-
hand side of eq. (G63), such agents are also a.m.predictive. For example, take M = A for all t ∈ N . Thus,
t t 0
A→←env ̸=∅.
pred
Using the expression for work capacity derived for memoryless environments, see lemma 9, after some straightfor-
ward algebra we obtain for the env under consideration [83]:
(cid:20) (cid:21)
1 3 1
Cwork(env)= ln + √ >0. (G64)
2 4 2
Further, the extractable work of any a.m.predictive agent is (by eq. (G12) and the linearity of the Ces`aro limit)
W(agtM
pred
→←env)=⟨H(A
t
|M
t
)⟩
t
−⟨H(S
t
|M
t
)⟩
t
(G65)
=−⟨H(S |M )⟩ ≤0. (G66)
t t t
Since Cwork(env)>0, it follows that A→←env∩A→←env =∅.
eff pred
Next, we show that A→←env ∩A→←env = ∅ for the particular environment channel under consideration. For all agent
eff mea
models in A→←env we have
mea
W(agtM
mea
→←env)=⟨H(A
t
|M
t
)−H(S
t
|M
t
)⟩
t
(G67)
=⟨H(A |M )⟩ −⟨H(S |M )⟩ (G68)
t t t t t t
=log|A|−⟨H(S |M )⟩ , (G69)
t t t
Inthefollowingwewilldetermine⟨H(S |M )⟩ byshowingthat⟨H(S |M )⟩ =⟨H(S )⟩ whichtheniseasilycomputed
t t t t t t t t
for the environment under consideration.
First note that we have ⟨I[S ;A ;M ]⟩ ≥0 since
t t t t
⟨I[A ;M ;S ]⟩ =⟨I[M ;S ]−I[M ;S |A ]⟩ (G70)
t t t t t t t t t t
=⟨I[M ;S ]⟩ (G71)
t t t
≥0, (G72)
since I[M ;S |A ] = 0 is a d-separation (shown for t = 0 in Figure 22). Further, since for all agent models in
t t t
A→←env ⟨H(A |M )⟩ = log|A| takes its maximum value and since H(A |M ) ≤ H(A ) ≤ log|A| (see Supplemental
mea t t t t t t
Material A2), we have ⟨H(A |M )⟩ =⟨H(A )⟩ and thus ⟨I[A ;M ]⟩ =0. Then, we have
t t t t t t t t
0=⟨I[A ;M ]⟩ (G73)
t t t
=⟨I[A ;M |S ]⟩ +⟨I[A ;M ;S ]⟩ . (G74)
t t t t t t t t
The first term is nonnegative by the nonnegativity of conditional mutual information, the second term by eq. (G72).
Thus, both terms must vanish individually. With this, using a decomposition into information atoms we find
⟨H(S |M )⟩ =⟨H(S )−I[S ;A ;M ]−I[S ;M |A ]⟩ (G75)
t t t t t t t t t t t
=⟨H(S )⟩ −⟨I[S ;A ;M ]⟩ (G76)
t t t t t t
=⟨H(S )⟩ . (G77)
t t
42
For the environment under consideration and since agent models in A→←env actions are uniformly distributed,
mea
this is easily computed and found to be ln[256/27]/ln[16], which results in a work rate (in units of k T ln2) of
B
1−ln[256/27]/ln[16] for all agent models in A→←env. Since this is smaller than the work capacity, eq. (G64), it follows
mea
that A→←env∩A→←env =∅.
mea eff
What is left to show is that A→←env ∩A→←env = ∅. Above, we showed that for all predictive agent models for the
mea pred
environmentunderconsideration, wehave⟨H(A |M )⟩ =0whichcontradictsthedefinitionofagentmodelsinA→←env
t t t mea
which concludes the proof. □

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "The Work Capacity of Channels with Memory: Maximum Extractable Work in Percept-Action Loops"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
