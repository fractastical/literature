=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Object-Centric Scene Representations using Active Inference
Citation Key: maele2023objectcentric
Authors: Toon Van de Maele, Tim Verbelen, Pietro Mazzaglia

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2023

Key Terms: scene, representations, abstract, chen, object, bear, centric, multiple, inference, active

=== FULL PAPER TEXT ===

Object-Centric Scene Representations using Active Inference
ToonVandeMaele1 TimVerbelen1 PietroMazzaglia1 StefanoFerraro1 BartDhoedt1
Abstract et al., 2016a; Bear et al., 2020) or multiple (Chen et al.,
Representingasceneanditsconstituentobjects 2021) views. In order to have full 3D scene understand-
from raw sensory data is a core ability for en- ing,otherapproacheslearnrepresentationsofcomplete3D
ablingrobotstointeractwiththeirenvironment. shapesbytrainingon3DCADmodelsofobjectsofinter-
In this paper, we propose a novel approach for est(Wuetal.,2015). Suchrepresentationscanthenbeused
scene understanding, leveraging a hierarchical to infer objects shape and pose in 3D from one or more
object-centric generative model that enables an RGB-Dimages(Sucaretal.,2020). However,thesemeth-
agenttoinferobjectcategoryandposeinanal- odstypicallyoperateonstaticinputsanddonotallowthe
locentricreferenceframeusingactiveinference, agenttointeractwiththescene.
a neuro-inspired framework for action and per-
In contrast, humans learn by actively engaging and inter-
ception. Forevaluatingthebehaviorofanactive
acting with the world (James et al., 2014). A prevailing
visionagent,wealsoproposeanewbenchmark
accountofhumanperceptionisthatthebrainbuildsagen-
where, given a target viewpoint of a particular
erativemodeloftheworld,constantlyexplainingitsobser-
object,theagentneedstofindthebestmatching
vations(Fristonetal.,2017),i.e. activeinference. Inthis
viewpointgivenaworkspacewithrandomlypo-
regard,visioniscastasinvertingagenerativemodelofthe
sitionedobjectsin3D.Wedemonstratethatour
scene,inordertoinferitsconstituentobjects,theirappear-
activeinferenceagentisabletobalanceepistemic
ance,andpose(Parretal.,2021). Thisisconsistentwith
foraging and goal-driven behavior, and outper-
findingsinthebrain,wherevisualinputsareprocessedbya
formsbothsupervisedandreinforcementlearning
dorsal(“where”)streamontheonehand,representingwhere
baselinesbyalargemargin.
anobjectisinthespace,andaventral(“what”)streamon
theotherhand,representingobjectidentity(Mishkinetal.,
1983).
1.Introduction
Inthispaper,weproposeanovelmethodforspatialscene
Spatial sceneunderstanding isa coreability forenabling
understanding, using an agent that can actively move the
robots to understand and interact with their environment
camerainthescene. Weendowtheagentwithanobject-
andhasbeenalong-standingchallengeincomputervision.
centricgenerativemodel,whichisoptimizedbyminimizing
Humansnaturallydecomposescenesintoobject-centricrep-
free energy. By also inferring actions that minimize ex-
resentationsandinferinformationaboutobjects,theirap-
pected free energy, the agent engages in active inference
pearance,theirconstituentparts,aswellastheirposeand
and balancesgoal-directed withexplorative behavior. To
shapeinthe3Dspace(Hinton,1979). Forexample,when
evaluateourapproach,wepresentanovelbenchmarkthat
seeingacoffeecup, humansimmediatelyknowwhereto
castsactivevisionasataskinwhichanartificalagentneeds
reachforthehandle,evenwhenthehandleisnotdirectlyin
toreachagoalobservation. Todemonstratethis,ourpaper
view.
proposesthefollowingcontributions:
Inthepastdecade,advancesindeeplearninghaveenabled
todevisesystemsthatcandistinguishobjectsinimages,i.e. • We propose a novel hierarchical object-centric gen-
predictingsegmentationmasks(Minaeeetal.,2021),orthe erative model that factorizes object identity from a
objectposein3D(Xiangetal.,2018a;Duetal.,2021)by locationinbothegocentricandallocentricreference
supervisedtrainingonadatasetannotatedwithpredefined frames(Section3).
objectclassesandposes. Also,unsupervisedmethodshave
beenproposedtoinferseparateobjectsfromone(Eslami • Wedevelopedanewbenchmarkenvironmentforeval-
uatingtheactivevisionproblem. Inthisenvironment,
1IDLab,GhentUniversity,Belgium.Correspondenceto:Toon
theagentneedstofindaparticularobjectbyactively
VandeMaele<toon.vandemaele@ugent.be>.
engaging with the environment through moving the
camera(Section4.1).
3202
beF
7
]OR.sc[
1v88230.2032:viXra
Object-CentricSceneRepresentationsusingActiveInference 2
• Wecompareperformancebothquantitativelyandqual- intoalatentstatespace,andlearnadynamicsmodeltopre-
itatively against two other methods that adopt a su- dicthowactionsevolvefuturestates(Ha&Schmidhuber,
pervised (Xiang et al., 2018a) and a reinforcement 2018). Theseworldmodelscanthenbeusedforplanning
learning(Mendoncaetal.,2021)approachrespectively actions(Hafneretal.,2019),orforlearningpoliciesinimag-
(Section4.2)andshowthatweperformbetteronthe ination,achievingstate-of-the-artperformanceonvarious
activevisionbenchmark. RLbenchmarks(Hafneretal.,2020;2021;Mazzagliaetal.,
2022). Anincreasinglypopularparadigm,especiallyinRL
• Wedemonstratethatdrivingactivevisionthroughthe forrobotics,istoprovidetheagentwithagoalobservation
expectedfreeenergyisnaturallyrobustagainstocclu- toobtained(Andrychowiczetal.,2017),forwhichalsoa
sions,asitwillactivelyexploretheenvironmentwhen worldmodelcanbeleveragedtodiscoverandachievenovel
thetargetobjectisinitiallynotinview(Section4.2). goals(Mendoncaetal.,2021). Arecentlineofworktriesto
combinestructuredlatentstatespaces,i.e. usingdifferent
2.RelatedWork slots,forlearningworldmodels,typicallyforobject-related
taskssuchasblockpushingandstacking(Kipfetal.,2019;
Object-centric representations: One of the early works Watters et al., 2019; Veerapaneni et al., 2020; Lin et al.,
ondecomposingimagesofscenesintotheirconstituentob- 2020).
jectsisAttendInferRepeat(Eslamietal.,2016b),wherean
ActiveInference: Activeinferenceisatheorythatcharac-
observationisdecomposedintoobject-levelrepresentations
terizesperception,planning,andactionintermsofproba-
byhavingarecurrentneuralnetworkpredictingtheparame-
bilisticinference(Parretal.,2022),whichisappliedinvari-
tersofaspatialtransformernetwork(Jaderbergetal.,2015).
ousdomainsrangingfromneuroscience(Smithetal.,2020),
PSGNets(Bearetal.,2020)introducegraphpoolingand
neuropsychology (Parr et al., 2018), biology (Pio-Lopez
vectorization operations that convert image feature maps
et al., 2022) to robotics and artificial intelligence (Lanil-
intoobject-centricgraphstructures. InROOTS(Chenetal.,
los et al., 2021). In particular, visual foraging can be
2021), theauthorsconsidermultipleviewpointsfromthe
modeledusingactiveinferenceagents,althoughtypically
same scene, crop out the objects of each viewpoint, and
limitedtosimulationswithdiscreteobservationoraction
thengroupthemtogethertoencodethemwithagenerative
spaces(Mirzaetal.,2016;Dauce´,2018).VandeMaeleetal.
querynetworkinstanceforeachobject(Eslamietal.,2018).
(2022)adopttheactiveinferenceframeworktolearnobject-
Indoingso, theobjectscanberenderedindividuallyand
centric generative models of 3D objects, called cortical
aggregated in a full observation. Other methods adopt a
columnnetworks(CCN).Similartotheworkonstructured
representation with a fixed number of “slots” that can be
world models, van Bergen & Lanillos (2022) extend the
used to represent objects in the scene. MONet (Burgess
IODINEframeworkwithactionstoinferpoliciesina2D
etal.,2019)recurrentlypredictsamaskforeachavailable
spritesenvironment.
slotandthenusesavariationalautoencodertoencodeeach
maskedobservation. InIODINE(Greffetal.,2020),ajoint SceneRepresentationBenchmark: Thereexistsamulti-
decomposition and representation model is learned with tudeofdatasetsforlearning3Dscenerepresentations,but
fixedslotallocation. Toscalethisuptomoreobjects,the manyofthesedatasetsuseastaticcamera (Johnsonetal.,
encodersareadaptedforpredictingproposalsinparallelin- 2016; Yan et al., 2021; Kundu et al., 2022) or use prere-
steadofrecurrently(Crawford&Pineau,2019;Jiangetal., cordedmovingcameraframes(Sajjadietal.,2022b). Tothe
2020). GENESISinadditionlearnsanautoregressiveprior bestofourknowledge,noneofthesebenchmarksconsider
forscenegeneration(Engelckeetal.,2020). Locatelloet anactivecamerathatcanmoveinthescene.
al. introduce the Slot Attention module, which uses an
attention mechanism to bind input features to the set of
3.Method
slots, which proved to be more efficient in terms of both
memoryconsumptionandruntime(Locatelloetal.,2020). Inthissection,wedescribehowouragentcarriesoutscene
Object Scene Representation Transformer (Sajjadi et al., decompositionleveragingahierarchicalgenerativemodel.
2022a) combines slot attention with scene representation Wefirstdiscusstheactiveinferenceframeworkandshow
transformers(Sajjadietal.,2022b)tolearnobject-centric how optimizing the free energy functional yields a trade-
representationsfromasetofmultipleviewpointsofascene offbetweenepistemicforagingandgoal-directedbehavior.
inanend-to-endfashion. Whendealingwithsequencesof Then,weproposeagenerativemodelrepresentingascene
observationsovertime,mostmodelslearntopredictthedy- as a collection of multiple objects with distinct features,
namicsofeachobjectslotseparately(Kosioreketal.,2018; representedashiddenvariables. Next,wedescribehowto
Jiangetal.,2020). instantiatethegenerativemodelusingdeepneuralnetworks,
andhowtheagentcanbedriventowardsgoalthroughex-
World Models: In the context of reinforcement learning,
pectedfreeenergyminimization,updatingitsbeliefsabout
worldmodelshavebeendevisedtocompressobservations
Object-CentricSceneRepresentationsusingActiveInference 3
thescenerepresentationateverystep.
3.1.ActiveInference
Activeinferenceisaprocesstheoryofthebrainwhichstates
thatallneuronalprocessingandactionisdrivenbythemin-
imization of (a bound on) surprise, i.e. free energy (Parr
et al., 2022). This offers a first principles account of un-
derstandingperceptionandactionasapproximateBayesian
inferenceonhiddenstatesandactionsofagenerativemodel.
In general, this generative model is the joint probability
distributionoversequencesofobservationso˜,actionsa˜and
hiddenstates˜s:
(cid:89)
P(o˜,a˜,˜s)= P(o |s )P(s |s ,a )P(a ) (1)
t t t t−1 t−1 t−1
t
Thegoaloftheagentisthentominimizethevariationalfree
energyF (Parretal.,2022),i.e. thenegativeevidencelower
bound (Rezendeetal.,2014;Kingma&Welling,2014),by
introducingtheapproximateposteriorQ(s |o ).
t t
(cid:88)
F = D [Q(s |o )||P(s |s ,a )]
KL t t t t−1 t−1
t (2)
+E [−logP(o |s )]
Q(st|ot) t t
Figure1.Graphicalrepresentationoftheagents’generativemodel
Crucially,inactiveinferencetheagentalsoselectsactions foranobject-centricfactorizationofascene.Observedvariables
a thatitbelieveswillminimizetheso-calledexpectedfree aredenotedbybluecircles,whileunobservedvariablesaredenoted
t
energyG(a )(Parretal.,2022): bywhitecircles.
t
G(a )=E [−logP(o )
t Q(st+1,ot+1) t+1
(cid:124) (cid:123)(cid:122) (cid:125)
reference frame, and a latent representation of the object
RealizingPreferences
(3)
posep .
+logQ(s |a )−logQ(s |a ,o )], k,t
t+1 t t+1 t t+1
(cid:124) (cid:123)(cid:122) (cid:125) Tocomposeanobservationofaparticularviewpointv ,for
ExpectedInformationGain t
eachentity,anobject-centricobservationo isgenerated
k,t
consisting of a utility term based on a preferred distribu- fromtheidentityandpose,whichrendersthespecificobject
tionoffutureobservations,aswellasaninformationgain inaparticularpose. Next,giventheobjecttranslationand
term. Hence, minimizing expected free energy balances a camera pinhole model, the pixel-coordinates u and
k,t
goal-directedbehaviorandepistemicforaging. scale σ in the full observation can be generated. The
k,t
resultingobservationo theagentreceivesasinputisthena
t
3.2.AGenerativeModelforVision compositionoftheseobject-centricobservationsinaglobal
view. Notethattheidentityandtranslationparametersare
Theagent-anactivecamera-entailsagenerativemodelto
consistentoverthescene,donotchangeovertime,andare
explainitsobservationsandtheeffectofitsactionsthereon.
thereforenotdependentontheactionoftheagent.
In this case, the model should thus describe a 3D scene
consistingofasetofobjectsinaworkspacewhileobserving Theactiona representstherelativetransformtheagent
t−1
2D images from viewpoints reached through moving the musttakeintheglobalreferenceframe,inordertoreachthe
camera. Wefactorizethegenerativemodelofascenesasa nextviewpointv . Weparameterizetheresultingviewpoint
t
hierarchicalcompositionofK entitiese . Intheremainder, asthecombinationofthelocationl inspaceatwhichthe
k t
weassumethesceneisstaticandthereforetheseentitiesdo agentwantstodirectitsgaze,togetherwiththespherical
notchange. However,thismodelcouldalsobeextendedto coordinates with respect to this look-at point: range r ,
t
taketheobjectdynamicsintoaccountovertime. elevationφ andazimuthθ .
t t
Theimagetheagentobservesisconstructedthroughatop- Thefactorizationofthedescribedgenerativemodelisshown
downgenerativemodel,depictedinFigure1. Eachentity inFigure1,andadetailedformaldescriptionofthemodel
e consistsofanidentityi ,atranslationt w.r.t. aglobal isprovidedinAppendixA.Thiskindofdecompositionof
k k k
Object-CentricSceneRepresentationsusingActiveInference 4
Figure2.Flowoftheinferenceprocessofobservationintothedistinctlatentvariables.ForeachoftheK-consideredobjectcategories,the
objectpixelsareextractedfromtheobservation(red)byaneuralnetworkthatpredictspixel-center,scale,andamaskedcropoftheobject.
Fromthiscrop,thebeliefovertheposeandidentitylatentvariableispredicted(green)usingaCCN.Giventhepinholecameramodel,
pixel-wisecenter,andscaleoftheobject,anestimateoftheobject’slocationareformedwhichisusedtoupdateaparticlefilter(blue)
approximatingthebeliefovertheobjecttranslation.
asceneindistinctobjectswiththeirrespectivefeatures,as jectcategoryofinterest. Basedonthemaskedobservation
wellasthespecificationofactionsinourgenerativemodel o (cid:12)α ,wethenpredictthecenterpixelu ofwherethe
t k,t k,t
might also underpin how the visual system of the brain objectispresentintheobservation,aswellasthescaleσ .
k,t
works, andmapstothevisualcortices, cerebralattention Thisisthenfedintoaspatialtransformernetwork(Jader-
networks,andtheoculomotorsystem(Parretal.,2021). berg et al., 2015), which produces an object-centric crop
o fortheCCN.
k,t
3.3.ScenePerceptionwithSceneCCN
Theneuralnetworkscomprisingthesetwoblocksareopti-
In active inference, perception is cast as approximate mizedintwoseparatephases. Inthefirstphase,thepose
Bayesian inference over the hidden variables. At each representationislearnedbytrainingtheCCNonanobject-
timestep,theagentobserveso andinfersapproximatepos- centricdataset. Foreachobjectcategory,asetof500ob-
t
teriordistributionsovereachhiddenvariableinabottom-up servationsandviewpointpairsiscollected. Theviewpoints
process. Thisinferenceprocessisamortizedusingacollec- are sampled uniformly on the surface of a sphere, with a
tionofneuralnetworkmodels,whichisdepictedinFigure2. fixedradius,whileorientedtowardstheobjectcenter. For
eachstepduringtraining,twoviewsarerandomlysampled,
We ground our approach on Cortical Column Networks
andtheactioniscomputedastherelativedisplacementin
(CCN)(VandeMaeleetal.,2022),whichareobject-centric
azimuth and elevation. To learn how this action results
models that learn the approximate posterior over the la-
inatransitionedposerepresentation,wealsotrainapose
tentposeq (p |o )forviewsofaparticularobject,by
θ k,t k,t transition network p (p |p ,a ) that models these
θ k,2 k,1 t−1
predicting observations of novel poses. These consist of
dynamics. Formoredetailsabouttheconstructionofthis
a separate encoder-decoder model trained separately per
dataset,thereaderisreferredtoAppendixB.
object category, which is inspired by cortical columns in
thebrain(Hawkinsetal.,2017),andreflecthowtoddlers The objective is prediction error over transitioned poses,
typically interact with one object at a time (James et al., givenanaction:
2014). Thisallowsthesetobeefficientlytrainedinparallel
on a limited dataset consisting of views of one particular L 1 =λ 1 ||o k,1 −p θ (o k,1 |p k,1 )|| 2 +λ 2 ||o k,2 −p θ (o k,1 |p k,1 )|| 2
object,atthecostofpoorgeneralization(i.e. fornewobject (cid:124) (cid:123)(cid:122) (cid:125)
Reconstructionerrorbeforetransition
categories a new CCN will need to be trained). In addi-
+λ ||o −p (o |p )|| (4)
3 k,2 θ k,2,trans k,2,trans 2
tiontotheposerepresentation,theencoderalsooutputsa
(cid:124) (cid:123)(cid:122) (cid:125)
Bernoullivariableq (i |o )whichindicatesthebeliefthat Reconstructionerroraftertransition
θ k k,t
theobjectofinterestispresentintheview. +λ 4 D KL [q θ (p k,2 |o k,2 )||p θ (p k,2 |p k,1 ,a t−1 )],
(cid:124) (cid:123)(cid:122) (cid:125)
In order to infer object-centric views o , we learn Complexityoftransitionmodel
k,t
for each object category an object extraction network
These models are additionally regularized by a KL-
q (o ,u ,σ |o ) with parameters φ, which first pro-
φ k,t k,t k,t t divergence term between the outputted distributions and
duces a mask α which masks anything beside the ob-
k,t astandardnormaldistribution.
Object-CentricSceneRepresentationsusingActiveInference 5
Inthesecondphase,theobjectextractionnetworkistrained gainabouttheobjectpositionst ,whicharethemainsource
k
forextractingobject-centriccrops. Tothisend,wedonot ofuncertaintyinthescene. Moreover,sincetheloglikeli-
collectanewdataset,butinsteadaugmenttheobject-centric hoodinobservationspace(i.e. pixelspace)islessmean-
datasetbyscalingandtranslatingtheobservations, while ingful,wefirstinferthegoalobjectidentity,scaleandpose
alsoaddingascaledandtranslatedobjectfromadifferent fromo usingourSceneCCN,andthenscoreutilityw.r.t.
goal
category to simulate occlusion. On top of this, we add reachingthesefactorvalues.
randomcolorpatchesinthebackground. Themodellearns
The expected free energy for a candidate viewpoint v
toextractobject-centriccropsbyminimizing: t+1
becomes:
L 2 =||o k,t −q φ (o k,t |u k,t ,σ k,t ,o t )|| 2 G(v )=E (cid:2) −logP(t |v ,o )
(cid:124) (cid:123)(cid:122) (cid:125) t+1 Q(s,ot+1) k t+1 goal
Croppingerror (cid:124) (cid:123)(cid:122) (cid:125)
(5) Lookatgoalobjectposition
+||α −αˆ || +BCE(i ,q (i |o )),
k,t k,t 2 k θ k k,t −logP(σ |v ,o )
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) k,t+1 t+1 goal
Maskingerror Identificationerror (cid:124) (cid:123)(cid:122) (cid:125)
atthedesiredscale
wherethefirsttwotermsarecomputedasthemeansquared −logP(p k,t+1 |v t+1 ,o goal ) (6)
(cid:124) (cid:123)(cid:122) (cid:125)
errorbetweenthepredictedmasksandcrops,andtherespec-
anddesiredpose
tivegroundtruths.TheBCE-termisthebinarycrossentropy (cid:3)
+logQ(t |v )−logQ(t |v ,o )
k,t+1 t+1 k,t+1 t+1 t+1
lossoverthepredictedobjectidentityvariable. Forregular-
(cid:124) (cid:123)(cid:122) (cid:125)
izationpurposes,theextractedcropisalsofedthroughthe whilesearchingfortheobjectposition.
(frozen)CCNandtheKL-divergencetermsfromEquation4
Thisboilsdowntodirectingtheagenttowardslookingatthe
arealsooptimized.
goalobjectattherightposeandscale,whilealsosearching
InAppendixA,weshowhowtheselosstermsareineffect forwheretheobjectispositioned.WeuseMonteCarlosam-
consistentwithminimizingthevariationalfreeenergyofthe plingtoobtainthebestnextviewpoint,bysampling5000
generativemodelproposedSection3.2. Alsofortheexact targetsandevaluatingG.Insteadofsamplinguniformlyin
parameterizationsofallneuralnetworks,hyperparameters, theworkspace,weuseimportancesampling,puttingmore
andtrainingdetails,thereaderisreferredtoAppendixC. weightonviewpointsthatlookatpositionswherethetarget
object is more likely positioned given the current beliefs
Toinferposteriorbeliefsovertheobjecttranslationt in
k,t oftheparticlefilter. Oncev isdetermined,wefindthe
an allocentric reference frame, we use the inferred pixel t+1
nextaction,bymovingthecameraastepofmaximum5cm
coordinates u and scale σ of the object. Given the
k,t k,t towardsthetargetview.
absolute viewpoint of our camera and a pinhole camera
model,wecanbackprojectthepixelcoordinatetoarayin
4.Experiments
3D,alongwhichwespecifya3DGaussiandistributionwith
mean at the estimated depth given the scale, and a fixed
Weaimtoevaluatewhetherourproposedgenerativemodel
covariance matrix forming an ellipsoid density along the
entailsarepresentationthatenablestheagenttounderstand
ray. To get a better estimate of the object’s position over
thesceneandtoinfer: (i)whatthedifferentobjectsareand
time,thispositionbeliefisintegratedusingaparticlefilter.
(ii)wherethesedifferentobjectsarelocated. Tothisend,we
Whenanobjectisnotdetectedinanobservation,wealso
designanewenvironmentinwhichobjectsfromtheYCB
reducetheparticleweightsthatareinview,toalsoreflect
dataset(Callietal.,2015)arespawnedinrandompositions,
and integrate the information gained by not observing a
andtheagentcanmoveacameraintheworkspace.
particularobject.
4.1.ActiveSearchBenchmark
3.4.Actionselection
Inordertoevaluateanactivevisionagent,wedesigneda
Settinganagent’sgoalinactiveinferenceisdonebyspec-
benchmarkthatcaststhesceneperceptionproblemasaprob-
ifying a prior preference in observation space, which the
leminwhichagoalobservationhastobereachedbyavir-
agentisexpectedtoobtain. Inthiscase,thepreferenceis
tualcamera. Wecreatedasimulationenvironmentinwhich
the log probability of reaching the goal observation o .
goal betweenoneandfiveobjectsfromtheYCBdataset(Calli
Actionsarethenselectedthatminimizetheexpectedfree
etal.,2015)arespawned. Weconsiderthemasterchefcan,
energyG,asdefinedinEquation3,i.e. scoringhowmuch
thecrackerbox,thesugarbox,thetomatosoupcan,andthe
expected observations will realize the preferred observa-
mustardbottle. Thepositionsoftheobjectsarerandomly
tions,andhowmuchinformationthesewillbringoverthe
generatedtobeonatableofsize1mx1mwithauniformly
hiddenvariables.
randomlysampledcolor.Thegoalimageisanobject-centric
Inourcase,wemainlyfocusontheexpectedinformation observationofatargetobject. Inthisobservation,theother
Object-CentricSceneRepresentationsusingActiveInference 6
liefs over separate factors (Section 3.3), and adopts
theexpectedfreeenergyobjectiveforactionselection
(Section3.4).
• PoseCNN:Asupervisedmodelforobjectsegmenta-
tionandposeestimationofRGBDdata(Xiangetal.,
2018b)trainedontheYCBdataset. Theagentaction
is acquired by first estimating the identity and pose
Figure3.Anexampleoftheactivevisionenvironment. Theleft
directlyfromboththegoalobservationandtheagents’
imageshowsthescene,atablewith5objectsfromtheYCBdataset.
Thegreencamerashowsthecurrentposeoftheagentandtheblue currentobservation. Fromthesetwoestimatedposes,
camerashowsthetargetposeoftheagentintheenvironment.The the relative transform is computed, and a 5 cm step
middleimageshowstheobject-centrictargetgoalobservationin according to this transform is taken as action. The
whichtheotherobjectsarenotrendered.Therightimageshows agentexecutesthisprocesseverystepuntilthetarget
thecurrentobservationtheagentsees. poseisreached. Whentheagentdoesnotobservethe
preference the agent stops. Note that PoseCNN has
accesstothedepthinformation,whereasourapproach
objectsinthescenearenotrenderedtohavenoambiguity
hasnot.
onwhatthetargetis.
• PoseCNN + Infogain: The PoseCNN baseline, but
Theagentcanmoveapinholecamera,andtheactionspace
adding our object position particle filter. When the
isdefinedascontinuousin6dimensions,representingthe
target object is not in view, we use the information
relativedisplacementinthethree-dimensionalspaceandthe
gaintermoverobjectpositionsimilartoourapproach.
Euleranglesfortherelativerotation. Valuesintheaction
spacelieintherange[−0.5,0.5]. Theagentcanonlymove • LEXA:Amodel-basedRLagentthatadoptsunsuper-
over the table with a max height of 0.6 cm. If the action visedexploration,tolearnanaccurateworldmodelof
movesthecameraoutsideoftheenvironmentorinsideof the environment, and goal-conditioned RL, to learn
anobject,theresultingposeisclipped. to reach goals sampled from the agent experience
buffer(Mendoncaetal.,2021).
Thetaskisconsideredsuccessfuliftheagentreachesthe
goal pose within a translation error below 7.5 cm, and a
BenchmarkPerformance:Thebenchmarkconsistsof500
rotation error lower than 0.5 rad. The agent starts at an
initialpositionwith0radazimuth, π radelevation,anda evaluationscenes: 100foreachtargetobjectwith1-5ob-
4 jects randomly positioned in the workspace, which has a
rangeof0.65m,withrespecttothetablecenter. Avisual
random color. The exact scenes on which we evaluated
representationoftheenvironment,agoalobservation,and
are provided in the supplementary material. The error in
the initial observation of a randomly generated scene is
sphericalcoordinateswithrespecttothetargetobjectcen-
showninFigure3.
ter is plotted in Figure 4. Our approach outperforms the
Webenchmarktheperformanceinthisenvironmentusing PoseCNNbaseline,whereasaddinginformationgainalso
thefollowingmetrics: thesuccessrate(i.e. whentheagent improvesthevanillaPoseCNNbaseline.
reaches the goal in less than 350 steps), the azimuth, el-
A more detailed breakdown per object is provided in Ta-
evation, andrangeerrorofthegoalcamerapositionwith
ble1. Ourmodelconsistentlyoutperformsthebaselinesin
respecttothegoalobjectposition. Assymmetryinobject
bothintermsofposeerrorandsuccessrateofreachingthe
shapecancauseahighoveralltranslationerrorwhileactu-
targetposeaccordingtotheenvironmentstopcriteria. We
ally having an accurate final observation, opted for these
metricsinsteadofanerrorinallocentric6DOFspace.
We implemented this environment adopting the structure
ofOpenAIgym(Brockmanetal.,2016), andthecodeis
suppliedinthesupplementarymaterial.
4.2.Results
Baselines:Wecompareourapproachwithseveralbaselines
forsolvingthebenchmark:
Figure4.Boxplotrepresentingtheazimuth,elevation,andrange
• SceneCCN + AIF: Our agent uses the proposed error of the reached position for our approach and the two
SceneCCNtodecomposesceneobservationsintobe- PoseCNNbaselinesoverallscenesofthe25configurations.
Object-CentricSceneRepresentationsusingActiveInference 7
Table1.ComparisonofSceneCCNwithPoseCNNonthewholebenchmark.Wecompareoverthesuccessrate(%s),azimutherror∆φ,
elevationerror∆θandrangeerror∆r.Foreachtargetobject,100scenesareevaluated.Thevaluesarerepresentedasmean±standard
error.Thebestperformancesaremarkedinbold.
%s ∆φ ∆θ ∆r
Masterchefcan 17.0 1.333 0.418 0.203
±0.088 ±0.029 ±0.018
Crackerbox 24.0 1.219 0.387 0.148
±0.096 ±0.031 ±0.013
Sugarbox 16.0 1.092 0.447 0.185
PoseCNN ±0.087 ±0.035 ±0.015
Mustardbottle 8.0 1.466 0.508 0.175
±0.091 ±0.041 ±0.016
Tomatosoupcan 16.0 1.276 0.395 0.218
±0.100 ±0.030 ±0.018
Total 16.2 1.277 0.431 0.186
±0.042 ±0.015 ±0.007
Masterchefcan 22.0 1.198 0.419 0.171
±0.099 ±0.034 ±0.017
Crackerbox 28.0 1.115 0.380 0.133
±0.101 ±0.036 ±0.011
Sugarbox 35.0 0.978 0.330 0.149
PoseCNN+Infogain ±0.098 ±0.032 ±0.015
Mustardbottle 24.0 1.241 0.381 0.138
±0.100 ±0.037 ±0.013
Tomatosoupcan 33.0 1.065 0.327 0.157
±0.097 ±0.031 ±0.018
Total 28.4 1.119 0.367 0.150
±0.045 ±0.015 ±0.007
Masterchefcan 84.0 0.491 0.138 0.042
±0.067 ±0.014 ±0.005
Crackerbox 66.0 0.532 0.290 0.058
±0.075 ±0.039 ±0.007
Sugarbox 69.0 0.508 0.232 0.058
SceneCCN+AIF ±0.076 ±0.032 ±0.006
Mustardbottle 80.0 0.420 0.275 0.057
±0.064 ±0.041 ±0.008
Tomatosoupcan 46.0 0.896 0.220 0.120
±0.085 ±0.019 ±0.013
Total 69.0 0.569 0.231 0.067
±0.034 ±0.014 ±0.004
obtainanaveragesuccessrateof69%,whichismorethan nalLEXAsetup. LEXAsetsgoalsbyrandomlysampling
double than the PoseCNN models with and without info them from a replay buffer acquired through exploration.
gain(28.4%and16.2%respectively). Thisshowsthatusing Wetrainedthisagentfor∼7Msteps1andcompareourap-
aninfogainexplorationstrategyclearlyhelpsinfindingthe proachandbothPoseCNNapproachesonthisenvironment.
targetobject,buttheobject-centricmodelisalsoanimpor- QualitativeresultsareshowninFigure5andshowthegoal
tantsuccessfactor. Thisacknowledgesthatrepresentations observation,andthefinalreachedframeforallthebaselines
learntbyactivelyinteractingwithobjectsmightbebetter aswellasforourapproach.WecanseethattheLEXAagent
suitedforobjectunderstandingthansupervisedtrainingof sometimesisabletoreachthegoalalmostperfectly(second
6DOFposeregressiononstaticviews. andlastcolumn)butinothercases,itpicksthewrongtarget
object.
Whenwedisentanglethesefeaturesseparatelyforthediffer-
entobjects,showninTable1,weobservethattheazimuth Thequantitativeperformanceofallapproachesinthisen-
erroristhehighestfortheobjectwiththemostsymmetry vironmentisshowninTable2. WeobservethattheLEXA
(i.e. tomatosoupcan),andthelowestfortheobjectwiththe baseline is only able to solve the task in 11 out of 100
leastsymmetry(i.e. themustardbottle), whilethisisnot cases,whileourapproachwasabletosolvethis62times.
necessarilyreflectedbytheelevation. Wealsotonotethat PoseCNNwasonlyabletosolveit17timesand30times
theperformanceofourSceneCCNonthetomatosoupcan withtheaddedinfogainterm. Further,wenoticethesame
underperformcomparedtotheotherobjects. Weattribute trendsasobservedinTable1. WhileLEXAperformsworse
thistothefactthattheCCNmodelofthetomatosoupcan thanthePoseCNNbaselinesintermsofsuccessrate, the
waslessaccuratethantheonesoftheotherobjects. This rangeandelevationerrorarelower. Italsohasthelowest
is probably due to the fact that the tomato soup can was azimutherrorofallapproaches.
renderedinasmallerscalethantheothers(seeFigure7),
ExplorationvsExploitation: Finally. weinvestigatethe
whichmadeitharderforthemodelstoaccuratelyinferthe
emerging behavior when the agent does not have a goal
positionandpose.
image. Thefunctionoptimizedbytheagentthenbecomes
Comparison with model-based RL: We also evaluated
1Forsample-efficiencycomparison,wetrainedSceneCCNus-
LatentExplorerAchiever(LEXA)(Mendoncaetal.,2021)
ing500ksamplesperobject, foratotalof2.5Mframesonthe
onasingleenvironmentinstancewith5objectsinafixed
augmenteddataset.
configuration, but with varying goals to reflect the origi-
Object-CentricSceneRepresentationsusingActiveInference 8
Table2.ComparisonwithLEXAonthesingleenvironmentLEXA
wastrainedon. 100goalswererandomlyselectedandusedto
evaluatetheperformanceoftheagentsonthesuccessrate(%s),
azimutherror(∆φ),elevationerror(∆θ)andrangeerror(∆r).
%s ∆φ ∆θ ∆r
PoseCNN 17 1.824 0.425 0.238
±0.494 ±0.082 ±0.053
PoseCNN+Infogain 30 1.195 0.460 0.201
±0.398 ±0.118 ±0.057
SceneCCN+AIF 62 1.050 0.187 0.076 Figure6.2Dprojectionofthepositionbeliefevolutionthroughthe
±0.470 ±0.083 ±0.039 minimizingoftheexpectedfreeenergyGusingtheSceneCCN
LEXA 11 1.025 0.368 0.138 generativemodel. Thecolorsindicatetheobjectoverwhichthe
±0.300 ±0.103 ±0.030 belief is formed. The black circles represent the ground truth
positionsoftheseobjectsandtheblacklineshowsthetrajectory
takenbytheagent.
the expected free energy without the instrumental terms,
whichboilsdowntothe(negative)infogainovertheobject
5.Conclusion
position(seeEquation(6)). InFigure6,weshowthisfora
particularsceneinwhichthefiveYCBobjectsarepresent.
Inthispaper,weproposedanovelobject-centricgenerative
Atthefirststep,itisclearthatthebeliefoverobjectpositions
modelthatbuildsarepresentationforsceneswithmultiple
has a large variance, but over time the agent iterates and
objectsusingactiveinference. Wedevelopedanewbench-
attends to different objects, narrowing the variance over
markforevaluatingperformanceonactivevisiontasksusing
these specific object instances. This can be seen by the
apinholecameraagentina3Denvironmentwithaspecific
camera’strajectoryshowninblack. Weobservethatover
move-togoal. Ourapproachoutperformsbothasupervised
multiple timesteps the variance over the object position
poseestimationmodelandanunsupervisedmodel-based
reduces, and the particles mean lies closer to the ground
RLbaseline.
truthpositionoftheobjects. Inthisvisualization,weshow
thatdrivingbehaviorthroughtheminimizationofGactively ThemainlimitationofSceneCCNisthatwerequireasingle
reducestheambiguityoverthescene. neural network per object type, which will be difficult to
scaletoaworldwithhundredsofobjectcategorieswitheven
moredifferentappearances.Apotentialmitigationwouldbe
toextendourCCNmodelstoshareweightsofthedifferent
encoders,and/oralsoinferalatentrepresentationforshape
andappearanceasproposedbyFerraroetal.(2022).
Comparedtoslot-attentionmodels,weentailaseparateslot
perobjectentityinsteadofaslotperobjectinstance. This
ensuresthatwerequirelessdatatotrainasinglecategory
comparedtoanend-to-endsystemthathastolearntomap
eitherofthecategoriestoanyslot. However,ourapproach
comesthedownsidethatitismoredifficulttoscaletowards
sceneswithmultipleobjectsofasingleinstance,ortodeal
withheavilyclutteredenvironmentsofsimilarcategories.
Currently,ourmethodonlyconsidersstaticenvironments.
Infuturework,weplantoalsoconsiderobjectdynamics
byaddingadynamicsmodelforeachobjectcategorythat
giventhehistoryofposesandactioncouldpredictthenext
pose.
References
Figure5.FivegoalsintheenvironmentonwhichtheLEXAagent
wastrained.Weshowthefinallyreachedframeforallthebaselines Andrychowicz, M., Wolski, F., Ray, A., Schneider,
(PoseCNN,PoseCNN+Infogain,SceneCCN+AIFandLEXA). J., Fong, R., Welinder, P., McGrew, B., Tobin, J.,
Object-CentricSceneRepresentationsusingActiveInference 9
Pieter Abbeel, O., and Zaremba, W. Hindsight expe- Du, G., Wang, K., Lian, S., and Zhao, K. Vision-based
rience replay. In Guyon, I., Luxburg, U. V., Bengio, RoboticGraspingFromObjectLocalization,ObjectPose
S., Wallach, H., Fergus, R., Vishwanathan, S., and EstimationtoGraspEstimationforParallelGrippers: A
Garnett,R.(eds.),AdvancesinNeuralInformationPro- Review. ArtificialIntelligenceReview,54(3):1677–1734,
cessing Systems, volume 30. Curran Associates, March2021.
Inc., 2017. URL https://proceedings.
Engelcke,M.,Kosiorek,A.,ParkerJones,O.,andPosner,
neurips.cc/paper/2017/file/
H. Genesis: Generativesceneinferenceandsamplingof
453fadbd8a1a3af50a9df4df899537b5-Paper.
object-centriclatentrepresentations. OpenReview,2020.
pdf.
Eslami,S.M.A.,Heess,N.,Weber,T.,Tassa,Y.,Szepesvari,
Bear, D., Fan, C., Mrowca, D., Li, Y., Alter, S., Nayebi,
D., Kavukcuoglu, K., and Hinton, G. E. Attend, infer,
A., Schwartz, J., Fei-Fei, L. F., Wu, J., Tenenbaum,
repeat: Fastsceneunderstandingwithgenerativemodels.
J., and Yamins, D. L. Learning physical graph rep-
InProceedingsofthe30thInternationalConferenceon
resentations from visual scenes. In Larochelle, H.,
Neural Information Processing Systems, NIPS’16, pp.
Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H.
3233–3241.CurranAssociatesInc.,2016a.
(eds.), Advances in Neural Information Processing
Systems, volume 33, pp. 6027–6039. Curran Asso-
Eslami,S.M.A.,Heess,N.,Weber,T.,Tassa,Y.,Szepes-
ciates, Inc., 2020. URL https://proceedings.
vari, D., Kavukcuoglu, K., and Hinton, G. E. Attend,
neurips.cc/paper/2020/file/
Infer,Repeat: FastSceneUnderstandingwithGenerative
4324e8d0d37b110ee1a4f1633ac52df5-Paper.
Models. arXiv:1603.08575 [cs], August 2016b. URL
pdf.
http://arxiv.org/abs/1603.08575. arXiv:
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., 1603.08575.
Schulman, J., Tang, J., and Zaremba, W. Openai gym.
Eslami, S. M. A., Jimenez Rezende, D., Besse, F., Viola,
arXivpreprintarXiv:1606.01540,2016.
F., Morcos, A. S., Garnelo, M., Ruderman, A., Rusu,
Burgess, C. P., Matthey, L., Watters, N., Kabra, R., Hig- A.A.,Danihelka,I.,Gregor,K.,Reichert,D.P.,Buesing,
gins, I., Botvinick, M., and Lerchner, A. MONet: L.,Weber,T.,Vinyals,O.,Rosenbaum,D.,Rabinowitz,
Unsupervised Scene Decomposition and Representa- N., King, H., Hillier, C., Botvinick, M., Wierstra, D.,
tion. arXiv:1901.11390[cs,stat],January2019. URL Kavukcuoglu,K.,andHassabis,D. Neuralscenerepre-
http://arxiv.org/abs/1901.11390. arXiv: sentationandrendering. Science,360(6394):1204–1210,
1901.11390. June2018. ISSN0036-8075,1095-9203. doi: 10.1126/
science.aar6170. URL https://www.science.
Calli,B.,Singh,A.,Walsman,A.,Srinivasa,S.,Abbeel,P., org/doi/10.1126/science.aar6170.
andDollar,A.M. Theycbobjectandmodelset:Towards
commonbenchmarksformanipulationresearch. In2015 Ferraro,S.,VandeMaele,T.,Mazzaglia,P.,Verbelen,T.,
InternationalConferenceonAdvancedRobotics(ICAR), andDhoedt,B. Disentanglingshapeandposeforobject-
pp.510–517,2015. doi: 10.1109/ICAR.2015.7251504. centricdeepactiveinferencemodels,2022.URLhttps:
//arxiv.org/abs/2209.09097.
Chen, C., Deng, F., and Ahn, S. ROOTS: Object-
Centric Representation and Rendering of 3D Scenes. Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P.,
arXiv:2006.06130 [cs, stat], July 2021. URL and Pezzulo, G. Active inference: A process theory.
http://arxiv.org/abs/2006.06130. arXiv: NeuralComput.,29(1):1–49,jan2017. ISSN0899-7667.
2006.06130. doi: 10.1162/NECO a 00912.
Crawford, E. and Pineau, J. Exploiting Spatial In- Greff,K.,Kaufman,R.L.,Kabra,R.,Watters,N.,Burgess,
variance for Scalable Unsupervised Object Tracking. C., Zoran, D., Matthey, L., Botvinick, M., and Lerch-
arXiv:1911.09033 [cs, stat], November 2019. URL ner,A. Multi-ObjectRepresentationLearningwithItera-
http://arxiv.org/abs/1911.09033. arXiv: tiveVariationalInference. arXiv:1903.00450[cs,stat],
1911.09033. July 2020. URL http://arxiv.org/abs/1903.
00450. arXiv: 1903.00450.
Dauce´, E. Active Fovea-Based Vision Through
Computationally-Effective Model-Based Predic- Ha, D. and Schmidhuber, J. Recurrent world models fa-
tion. FrontiersinNeurorobotics,12:76,December2018. cilitate policy evolution. In Bengio, S., Wallach, H.,
ISSN1662-5218. doi: 10.3389/fnbot.2018.00076. URL Larochelle,H.,Grauman,K.,Cesa-Bianchi,N.,andGar-
https://www.frontiersin.org/article/ nett,R.(eds.),AdvancesinNeuralInformationProcess-
10.3389/fnbot.2018.00076/full. ingSystems,volume31.CurranAssociates,Inc.,2018.
Object-CentricSceneRepresentationsusingActiveInference 10
Hafner, D., Lillicrap, T. P., Fischer, I., Villegas, R., Ha, Kingma,D.P.andWelling,M. Auto-EncodingVariational
D.,Lee,H.,andDavidson,J. Learninglatentdynamics Bayes. arXiv:1312.6114 [cs, stat], May 2014. URL
for planning from pixels. In Proceedings of the 36th http://arxiv.org/abs/1312.6114. arXiv:
InternationalConferenceonMachineLearning,ICML 1312.6114.
2019, 9-15 June 2019, Long Beach, California, USA,
Kipf, T., van der Pol, E., and Welling, M. Contrastive
pp.2555–2565,2019. URLhttp://proceedings.
learning of structured world models. arXiv preprint
mlr.press/v97/hafner19a.html.
arXiv:1911.12247,2019.
Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M. Dream
Kosiorek, A. R., Kim, H., Posner, I., and Teh, Y. W. Se-
tocontrol: Learningbehaviorsbylatentimagination. In
quentialAttend,Infer,Repeat: GenerativeModellingof
InternationalConferenceonLearningRepresentations,
MovingObjects. arXiv:1806.01794[cs, stat], Novem-
2020.
ber 2018. URL http://arxiv.org/abs/1806.
01794. arXiv: 1806.01794.
Hafner,D.,Lillicrap,T.P.,Norouzi,M.,andBa,J. Master-
ingatariwithdiscreteworldmodels. In9thInternational Kundu, A., Tagliasacchi, A., Mak, A. Y., Stone, A., Do-
ConferenceonLearningRepresentations,ICLR2021,Vir- ersch, C., Oztireli, C., Herrmann, C., Gnanapragasam,
tualEvent,Austria,May3-7,2021,2021. URLhttps: D., Duckworth, D., Rebain, D., Fleet, D. J., Sun, D.,
//openreview.net/forum?id=0oabwyZbOu. Nowrouzezahrai, D., Lagun, D., Pot, E., Zhong, F.,
Golemo,F.,Belletti,F.,Meyer,H.,Liu,H.-T.D.,Laradji,
Hawkins, J., Ahmad, S., and Cui, Y. A Theory of How I.,Greff,K.,Yi,K.M.,Beyer,L.,Sela,M.,Sajjadi,M.
ColumnsintheNeocortexEnableLearningtheStructure S. M., Radwan, N., Sabour, S., Vora, S., Kipf, T., Wu,
oftheWorld.FrontiersinNeuralCircuits,11:81,October T.,Sitzmann,V.,Du,Y.,andMiao,Y.(eds.). Kubric: A
2017. ISSN1662-5110. doi: 10.3389/fncir.2017.00081. scalabledatasetgenerator,2022.
URL http://journal.frontiersin.org/
article/10.3389/fncir.2017.00081/full. Lanillos,P.,Meo,C.,Pezzato,C.,Meera,A.A.,Baioumy,
M., Ohata, W., Tschantz, A., Millidge, B., Wisse, M.,
Hinton,G.E. Somedemonstrationsoftheeffectsofstruc- Buckley,C.L.,andTani,J. Activeinferenceinrobotics
turaldescriptionsinmentalimagery. CognitiveScience and artificial agents: Survey and challenges. CoRR,
(COGSCI),3(3):231–250,1979. abs/2112.01871,2021. URLhttps://arxiv.org/
abs/2112.01871.
Jaderberg, M., Simonyan, K., Zisserman, A., and
Lin, Z., Wu, Y.-F., Peri, S., Fu, B., Jiang, J., and Ahn,
Kavukcuoglu, K. Spatial transformer networks, 2015.
URLhttps://arxiv.org/abs/1506.02025. S. Improving generative imagination in object-centric
worldmodels. InProceedingsofthe37thInternational
James,K.H.,Jones,S.S.,Smith,L.B.,andSwain,S.N. ConferenceonMachineLearning,ICML’20.JMLR.org,
Young children’s self-generated object views and ob- 2020.
ject recognition. Journal of Cognition and Develop-
Locatello,F.,Weissenborn,D.,Unterthiner,T.,Mahendran,
ment, 15(3):393–401, 2014. doi: 10.1080/15248372.
A., Heigold, G., Uszkoreit, J., Dosovitskiy, A., and
2012.749481.
Kipf,T. Object-centriclearningwithslotattention. In
Larochelle,H.,Ranzato,M.,Hadsell,R.,Balcan,M.,and
Jiang,J.,Janghorbani,S.,deMelo,G.,andAhn,S. Scalor:
Lin,H.(eds.),AdvancesinNeuralInformationProcess-
Generativeworldmodelswithscalableobjectrepresenta-
ingSystems, volume33, pp.11525–11538.CurranAs-
tions. InProceedingsofICLR2020,2020. URLhttps:
sociates,Inc.,2020. URLhttps://proceedings.
//openreview.net/pdf?id=SJxrKgStDH.
neurips.cc/paper/2020/file/
8511df98c02ab60aea1b2356c013bc0f-Paper.
Johnson,J.,Hariharan,B.,vanderMaaten,L.,Fei-Fei,L.,
pdf.
Zitnick,C.L.,andGirshick,R.B. CLEVR:Adiagnostic
datasetforcompositionallanguageandelementaryvisual
Mazzaglia, P., Verbelen, T., Dhoedt, B., Lacoste, A., and
reasoning. CoRR,abs/1612.06890,2016. URLhttp:
Rajeswar,S.Choreographer:Learningandadaptingskills
//arxiv.org/abs/1612.06890.
inimagination. 2022. URLhttps://openreview.
net/forum?id=BxYsP-7ggf.
Kingma, D. P. and Ba, J. Adam: A Method for
Stochastic Optimization. arXiv:1412.6980 [cs], Jan- Mendonca, R., Rybkin, O., Daniilidis, K., Hafner, D.,
uary 2017. URL http://arxiv.org/abs/1412. and Pathak, D. Discovering and achieving goals via
6980. arXiv: 1412.6980. world models. In Advances in Neural Information
Object-CentricSceneRepresentationsusingActiveInference 11
Processing Systems 34: Annual Conference on Neu- Deep Generative Models. arXiv:1401.4082 [cs, stat],
ral Information Processing Systems 2021, NeurIPS May2014. URLhttp://arxiv.org/abs/1401.
2021, December 6-14, 2021, virtual, pp. 24379– 4082. arXiv: 1401.4082.
24391, 2021. URL https://proceedings.
Sajjadi, M. S. M., Duckworth, D., Mahendran, A., van
neurips.cc/paper/2021/hash/
Steenkiste,S.,Pavetic´,F.,Lucˇic´,M.,Guibas,L.J.,Greff,
cc4af25fa9d2d5c953496579b75f6f6c-Abstract.
K.,andKipf,T. Objectscenerepresentationtransformer.
html.
InAdvancesinNeuralInformationProcessingSystems,
Minaee,S.,Boykov,Y.Y.,Porikli,F.,Plaza,A.J.,Kehtar- 2022a.
navaz,N.,andTerzopoulos,D.Imagesegmentationusing
Sajjadi, M. S. M., Meyer, H., Pot, E., Bergmann, U.,
deeplearning: Asurvey. IEEETransactionsonPattern
Greff, K., Radwan, N., Vora, S., Lucic, M., Duck-
AnalysisandMachineIntelligence,pp.1–1,2021. doi:
worth, D., Dosovitskiy, A., Uszkoreit, J., Funkhouser,
10.1109/TPAMI.2021.3059968.
T., and Tagliasacchi, A. Scene Representation Trans-
Mirza, M. B., Adams, R. A., Mathys, C. D., and former: Geometry-FreeNovelViewSynthesisThrough
Friston, K. J. Scene Construction, Visual Foraging, Set-LatentSceneRepresentations. CVPR,2022b. URL
and Active Inference. Frontiers in Computational https://srt-paper.github.io/.
Neuroscience, 10, June 2016. ISSN 1662-5188.
Smith, E. J., Meger, D., Pineda, L., Calandra, R., Ma-
doi: 10.3389/fncom.2016.00056. URL http:
lik, J., Romero, A., and Drozdzal, M. Active 3d
//journal.frontiersin.org/Article/10.
shape reconstruction from vision and touch. CoRR,
3389/fncom.2016.00056/abstract.
abs/2107.09584,2021. URLhttps://arxiv.org/
Mishkin,M.,Ungerleider,L.G.,andMacko,K.A. Object abs/2107.09584.
visionandspatialvision:twocorticalpathways.Trendsin
Smith, R., Badcock, P., and Friston, K. J. Recent ad-
Neurosciences,6:414–417,January1983. doi: 10.1016/
vancesintheapplicationofpredictivecodingandactive
0166-2236(83)90190-x. URL https://doi.org/
inferencemodelswithinclinicalneuroscience. Psychi-
10.1016/0166-2236(83)90190-x.
atry and Clinical Neurosciences, 75(1):3–13, Septem-
Parr, T., Rees, G., and Friston, K. J. Computational ber 2020. doi: 10.1111/pcn.13138. URL https:
neuropsychology and bayesian inference. Fron- //doi.org/10.1111/pcn.13138.
tiers in Human Neuroscience, 12, 2018. ISSN
Sucar, E., Wada, K., andDavison, A. Nodeslam: Neural
1662-5161. doi: 10.3389/fnhum.2018.00061. URL
object descriptors for multi-view shape reconstruction.
https://www.frontiersin.org/articles/
In 2020 InternationalConference on3D Vision (3DV),
10.3389/fnhum.2018.00061.
pp.949–958,nov2020. doi: 10.1109/3DV50981.2020.
Parr, T., Sajid, N., Da Costa, L., Mirza, M. B., and 00105.
Friston, K. J. Generative Models for Active Vision.
vanBergen,R.S.andLanillos,P.L. Object-basedactive
FrontiersinNeurorobotics,15:651432,April2021. ISSN
inference,2022. URLhttps://arxiv.org/abs/
1662-5218. doi: 10.3389/fnbot.2021.651432. URL
2209.01258.
https://www.frontiersin.org/articles/
10.3389/fnbot.2021.651432/full.
VandeMaele, T., Verbelen, T., C¸atal, O., andDhoedt, B.
EmbodiedObjectRepresentationLearningandRecogni-
Parr, T., Pezzulo, G., and Friston, K. J. Active In-
tion. FrontiersinNeurorobotics,16:840658,April2022.
ference: The Free Energy Principle in Mind, Brain,
ISSN1662-5218. doi: 10.3389/fnbot.2022.840658. URL
and Behavior. The MIT Press, March 2022. ISBN
https://www.frontiersin.org/articles/
978-0-262-36997-8. doi: 10.7551/mitpress/12441.
10.3389/fnbot.2022.840658/full.
001.0001. URL https://doi.org/10.7551/
mitpress/12441.001.0001.
Veerapaneni,R.,Co-Reyes,J.D.,Chang,M.,Janner,M.,
Finn,C.,Wu,J.,Tenenbaum,J.,andLevine,S. Entity
Pio-Lopez, L., Kuchling, F., Tung, A., Pezzulo, G., and
abstraction in visual model-based reinforcement learn-
Levin, M. Frontiers in Computational Neuroscience,
ing. In Kaelbling, L. P., Kragic, D., and Sugiura, K.
16,2022. ISSN1662-5188. doi: 10.3389/fncom.2022.
(eds.), Proceedings of the Conference on Robot Learn-
988977. URLhttps://www.frontiersin.org/
ing, volume 100 of Proceedings of Machine Learn-
articles/10.3389/fncom.2022.988977.
ing Research, pp. 1439–1456. PMLR, 30 Oct–01 Nov
Rezende, D. J., Mohamed, S., and Wierstra, D. Stochas- 2020.URLhttps://proceedings.mlr.press/
tic Backpropagation and Approximate Inference in v100/veerapaneni20a.html.
Object-CentricSceneRepresentationsusingActiveInference 12
Watters, N., Matthey, L., Bosnjak, M., Burgess, C. P.,
and Lerchner, A. COBRA: data-efficient model-based
RLthroughunsupervisedobjectdiscoveryandcuriosity-
drivenexploration. CoRR,abs/1905.09275,2019. URL
http://arxiv.org/abs/1905.09275.
Wu,Z.,Song,S.,Khosla,A.,Zhang,L.,Tang,X.,andXiao,
J. 3d shapenets: A deep representation for volumetric
shapemodeling.InIEEEConferenceonComputerVision
and Pattern Recognition (CVPR), Boston, USA, June
2015.
Xiang,Y.,Schmidt,T.,Narayanan,V.,andFox,D. Posecnn:
Aconvolutionalneuralnetworkfor6dobjectposeestima-
tioninclutteredscenes. Robotics: ScienceandSystems
(RSS),2018a.
Xiang, Y., Schmidt, T., Narayanan, V., and Fox,
D. PoseCNN: A Convolutional Neural Net-
work for 6D Object Pose Estimation in Cluttered
Scenes. arXiv:1711.00199 [cs], May 2018b. URL
http://arxiv.org/abs/1711.00199. arXiv:
1711.00199.
Yan, X., Yuan, Z., Du, Y., Liao, Y., Guo, Y., Li, Z., and
Cui, S. Clevr3d: Compositionallanguageandelemen-
taryvisualreasoningforquestionansweringin3dreal-
world scenes, 2021. URL https://arxiv.org/
abs/2112.11691.
Object-CentricSceneRepresentationsusingActiveInference 13
A.TheGenerativeModelandVariationalFreeEnergy
ThegenerativemodelshowninFigure1canbeformalizedas:
T
(cid:89)
P(s,o˜,a˜)=P(v ) P(v |v ,a )P(a )
0 t t−1 t−1 t−1
t=1
(cid:89)
P(o |o ,u ,σ )P(o |i ,p )P(p |p ,a )P(u |t ,v )P(σ |t ,v )P(i )P(t )
t k,t k,t k,t k,t k k,t k,t k,t−1 t−1 k,t k t k,t k t k k
k
(7)
(cid:81)
wherethetilderepresentsasequenceofthevariableovertime,andP(s)= P(i ,t ,p˜ ).
k k k k
TheapproximateposteriorQ(s|o˜)isfactorizedthroughthefollowingmeanfieldapproximation:
(cid:89)(cid:89)
Q(s|o˜)= Q(u |o )Q(σ |o )Q(t |u ,σ )Q(p |o )Q(i |o )
k,t t k,t t k k,t k,t k,t k,t k k,t (8)
t k
Forthegenerativemodelandapproximateposteriordescribedabove,thefreeenergyF isdefinedas:
F =E (cid:2) logQ(s|o˜)−logP(s,o˜,a˜) (cid:3)
Q(s|o˜)
= (cid:88)(cid:88) E (cid:2) −logP(v |v ,a )−logP(a ) (cid:3)
Q(s|o˜) t t−1 t−1 t−1
t k (cid:124) (cid:123)(cid:122) (cid:125)
Constant
(cid:2) (cid:3)
+D Q(p |o )||P(p |p ,a ))
KL k,t k,t k,t k,t−1 t−1
(cid:124) (cid:123)(cid:122) (cid:125)
PoseTransitionModel∗
(cid:2) (cid:3)
+D Q(i |o )||P(i )
KL k k,t k
(cid:124) (cid:123)(cid:122) (cid:125)
Objectcategoryclassification∗∗
(cid:2) (cid:3) (cid:2) (cid:3)
+D Q(σ |o )||P(σ |t ,v ) +D Q(u |o )||P(u |t ,v ) (9)
KL k,t t k,t t t KL k,t t k,t t t
(cid:124) (cid:123)(cid:122) (cid:125)
Object-centriccropping∗∗
(cid:2) (cid:3)
+D Q(t |o )||P(t )
KL k t k
(cid:124) (cid:123)(cid:122) (cid:125)
Objectpositions∗∗∗
+E (cid:2) −logP(o |i ,p )) (cid:3)
Q(s|o˜) k,t t k,t
(cid:124) (cid:123)(cid:122) (cid:125)
Object-centricobservationlikelihood∗
+E (cid:2) −logP(o |o ,u ,σ ) (cid:3)
Q(s|o˜) t k,t k,t k,t
(cid:124) (cid:123)(cid:122) (cid:125)
Fullobservationlikelihood
This decomposes in a number of terms that are optimized in our two training phases. In the first phase, we train
object-centricCCNs,whichjointlyoptimizeanobject-centriclikelihoodmodelp (o |i ,p ),aposetransitionmodel
θ k,t k k,t
p (p |p ,a )andanapproximateposeposteriormodelq (p |o )coveringthetermsdenotedwith∗. Inthe
θ k,t k,t−1 t−1 φ k,t k,t
second phase, we additionally train a model outputting the object category q (i |o ) as well as a model outputting
φ k k,t
object-centriccropparametersq (u ,σ |o ). Thisisdonebyoptimizingthetermsdenotedwith∗∗,andnotingthat
φ k,t k,t t
generatingboundingboxparametersu andσ fromobjectpositiontandcameraviewpointvisdeterminedbythe
k,t k,t
pinholecameramodel. Finally,theposterioroverpositions(denotedwith∗∗∗)isimplementedusingaparticlefilter,andhas
noparameterstooptimize. Inasimilarvein,theremainingtermshavenoinfluenceoverthelearnableparametersorare
constanttermsundertheexpectation.
Inpractice,optimizingthenegativeloglikelihoodtermsisequivalenttominimizingthemeansquarederrorbetweenthe
reconstructedobservationandthegroundtruthobservation. Similarly,optimizingtheKL-divergenceforaBernoullivariable
isequivalenttominimizingthebinarycrossentropyerroroverthisvariable. Forthisreason,weimplementedthelossusing
Object-CentricSceneRepresentationsusingActiveInference 14
Figure7. Afewexamplesfromtheobject-centricdataset(left)andtheaugmenteddataset(right).
theseterms. Insteadofregressingtheboundingboxparameters,weoptedforaspatialtransformerthatdirectlytransforms
theinputimageintoanobject-centriccropfromtheseparameters,andaddasupervisedlosstotheintermediatelycomputed
object-specificmaskasthisresultedinmorestableresults.
B.Object-CentricDatasets
ForeachoftheconsideredobjectsintheYCBdataset,adatasetiscreatedconsistingofobservation-viewpointpairs. Inthis
workweconsiderthefollowingobjects: masterchefcan,crackerbox,sugarbox,tomatosoupcan,andmustardbottle.
We only consider observing objects from a fixed distance, as this will allow an agent to use the scale of an object for
estimatingthedistancetotheobject. Wethensampleallobservationsfromafixeddistanceof40cmfromtheobjectcenter.
Pointsaresampleduniformlyonaspherearoundaroundtheobjectcenter,consideringafixedradiusof40cm. Boththe
RGBobservationandtheviewpointsarerecorded. Foreachobjectcategory,adatasetof500samplesiscreated. Some
samplesfromthisdatasetareshownontheleftofFigure7.
Fortrainingthesecondphasethatconsidersscene(notobject-centric)observations,wedonotcreateanoveldataset. Instead,
weaugmentandcombineourobject-centricobservationstolieinasimilardistributiontofullscenes. Tothisend, we
randomlyscaleandtranslatetheobjectintheobservationtosimulateasenseoftranslation. Thentworandomlyoriented
rectanglesindifferentcolorsareusedasabackground,whichincreasesrobustnesstodifferentenvironments. Ontopof
this,wealsoaddadifferentrandomobjectfromoneoftheothercategoriesthatisalsoscaledandtranslatedaccordingto
thesamedistributionasthemainobject. Thissometimesoccludestheobjectofinterestandmakesourmodelmorerobust.
Finally,toaddaugmentationfortrainingtheobjectidentityprediction,in20%ofthecasesweremovetheobjectfromthe
observation. SomesamplesfromthisdatasetareshowninFigure7.
C.NeuralNetworkParameters
Therearemultipleneuralnetworksworkingtogetherfortheapproximateinferenceofthehiddenvariables. Whentheagent
observesthescene,firstafullyconvolutionalneuralnetworkestimateswhichpixelsbelongtotheobjectofcategoryk,this
istheobservationmaskα . TheparameterizationofthislayerisshowninTable3.
k,t
Theobservationisthenmaskedusingapixel-wiseproducto (cid:12)α andfurtherprocessedtopredictobjectidentityi ,
t k,t k
i.e. whethertheobjectofinterestispresentintheobservation. Thisneuralnetworkpredictstheparametersforaspatial
transformer, i.e. the center of the object in pixel space and the object scale directly. Its parameterization is shown in
Table4. Thisdirectlypredictsthescaleandnormalizedcenteroftheobjectofinterest. Togetthepredictedscaleina
rangeforthemodeltooptimizewescaleandinvertthisoutputhoftheneuralnetworkusingthefollowingtransform:
σ =1/(3.8·h+0.2).
k,t
Object-CentricSceneRepresentationsusingActiveInference 15
Table3. configurationoftheMaskingNeuralNetwork.
Layer OutputNeurons/Filters
Interpolateto64x64 -
Convolutional(1x1)+LeakyReLU 16
Convolutional(3x3)+LeakyReLU 32
Convolutional(3x3)+LeakyReLU 64
Convolutional(3x3) 1
MedianPool(5x5) -
Sigmoid -
Interpolateto480x480 -
Table4. ConfigurationoftheSpatialTransformerNetwork.
Layer OutputNeurons/Filters
Interpolateto32x32 -
Flatten -
Linear+LeakyReLU 256
Linear+LeakyReLU 128
Linear+LeakyReLU 64
Linear+ELU 3
Using the output of the spatial transformer network, the masked observation can be processed into an object-centric
observationafterwhichthisisprocessedbytwoneuralnetworks. Onepredictstheobjectidentityi orwhethertheobjectof
k
interestispresentintheobservation. TheparameterizationisshowninTable5.
Table5. ConfigurationoftheIdentityEstimationNetwork.
Layer OutputNeurons/Filters
Convolutional(4x4)+LeakyReLU 8
Convolutional(4x4)+LeakyReLU 16
Convolutional(4x4)+LeakyReLU 32
Convolutional(4x4)+LeakyReLU 64
Flatten -
Linear+Sigmoid 1
Finally,aCCNisabletoestimatealatentvariabledescribingtheobjectpose. Thismodelconsistsofthreeneuralnetworks.
First,anencodertakesanobject-centricobservationasaninputandpredictstheparametersofamultivariateGaussianwith
8dimensionsandwithadiagonalcovariancematrix. Thisvariablerepresentstheobjectposeinanobject-centricreference
frame. ThedetailsofthismodelcanbefoundinTable6. Thefinaloutputofthismodelisavectoroftwicethelatentsize,
representingthemeanandstandarddeviationofthemultivariateGaussian. Adecoderorlikelihoodmodeltakesasinputa
samplefromthisposedistributionanddecodesitintoapixel-basedobservation. Usingthetransitionmodel,thedistribution
overtheposecanbeestimatedforfutureviewpoints. Themodeltakesasinputtheconcatenatedvectorofasamplefromthe
currentbeliefoverposeandtheaction. Theactioninthiscaseisatwo-dimensionalvectorrepresentingthedisplacementin
azimuthandelevation,asweonlyconsiderobservationsfromafixeddistance.
Theseneuralnetworksaretrainedintwodistinctsteps,usingthesameobject-centricdatasetof500observation-viewpoint
pairsperobject. Drawinginspirationfromobject-centriclearningininfants(Smithetal.,2021),thefirstphasefocuses
on learning object-centric representations. While the second phase focuses on learning to decompose the scene into
object-centricrepresentations. Inthefirstphase,theCCNmodelsisoptimizedontheL -loss,describedinEquation4. This
1
isdoneusingtheAdam(Kingma&Ba,2017)optimizerwiththeconfigurationparametersshowninTable9. Inthesecond
phase,theothermodelsareoptimizedbyminimizingtheL -loss(Equation5),againusingtheAdam(Kingma&Ba,2017)
2
Object-CentricSceneRepresentationsusingActiveInference 16
Table6. ConfigurationoftheCCNEncoder.
Layer OutputNeurons/Filters
Convolution(4x4)+LeakyReLU 8
Convolution(4x4)+LeakyReLU 16
Convolution(4x4)+LeakyReLU 32
Convolution(4x4)+LeakyReLU 64
Flatten -
Linear 2·8
Softplusforσoutput -
Table7. ConfigurationoftheCCNDecoder.
Layer OutputNeurons/Filters
Linear 4096
Unflattento64x8x8 -
Convolution(5x5)+LeakyReLU 64
Interpolateto17x17 -
Convolution(5x5)+LeakyReLU 64
Interpolateto35x35 -
Convolution(6x6)+LeakyReLU 32
Interpolateto69x69 -
Convolution(6x6)+LeakyReLU 16
Convolution(1x1) 3
optimizer,butnowwithalearningrateof1·10−4.
D.DetailsonActiveAgents
Whenweconsidertheagentdriventhroughexpectedfreeenergy. Anewtargetviewpointv iscomputedevery10steps,
t+1
orunlessthetargetisreached.
Whenestimatingthe3DmultivariateGaussian,themeanisplacedattheestimateddepthalongthenegativez-directionof
thecameraposeinOpenGLformat. Thedepthisacquiredusingthefollowingrelationwiththescale: d = 0.4/σ ,as
k,t
observationsaretrainedforadistanced=0.4. Thevarianceisthensetatavalueof0.1973/2alongthedepthdimension,
and0.02alongtheotherdimensions.
Theparticlefiltersforestimatingthepositionoftheindividualobjectsareinitializedwith10kparticles. Ateachstep,the
particlesareresampledwithastandarddeviationof0.025cm. Whentheagentdoesnotobservetheobjectinquestion,all
particlesthatareinviewofthecameraandcloserthanthenearest(estimated)object,aresettoalowvalueof10−5before
normalization.
Fortheimplementation,thereaderisreferredtothesupplementarymaterials.
Object-CentricSceneRepresentationsusingActiveInference 17
Table8. ConfigurationoftheCCNTransitionModel.
Layer OutputNeurons/Filters
Linear+LeakyReLU 128
Linear+LeakyReLU 256
Linear+LeakyReLU 256
Linear 2·8
Softplusforσoutput -
Table9. HyperparametersforoptimizationoftheCCNinthefirststageoftraining.
InitialValue Range
λ 40 [80,100]
1
λ 40 [80,100]
2
λ 40 [80,100]
3
λ 10 fixed
4
Adjustfrequency 500steps -
Adjustfactor 1.01 -
Learningrate 5·10−4 -
Object-CentricSceneRepresentationsusingActiveInference 18
E.AdditionalResults
All approaches are evaluated on a set of ten goals from the environment in which the LEXA agent was trained. The
qualitativeresultscanbeobservedinFigure8. Thegoalobservationisdisplayedinthetoprow,whiletheothergoalsshow
thefinalreachedframeoftheagentwithamaximumof350steps.
Figure8.QualitativeresultsofthesceneinwhichLEXAwastrained.Inthisconfiguration,fiveobjectsarerandomlyplacedonatable,
andtheagentmustreachagoalobservation. Thetoprowshowsthegoalobservation,whiletheotherrowsshowthefinalreached
observationforeachagent.Thesimulationisstoppedafter350steps.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Object-Centric Scene Representations using Active Inference"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
