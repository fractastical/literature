=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Intrinsic Rewards for Exploration without Harm from Observational Noise: A Simulation Study Based on the Free Energy Principle
Citation Key: tinker2024intrinsic
Authors: Theodore Jerome Tinker, Kenji Doya, Jun Tani

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2024

Abstract: InReinforcementLearning(RL),artificialagentsaretrainedtomaximizenumerical
rewards by performing tasks. Exploration is essential in RL because agents must dis-
cover information before exploiting it. Two rewards encouraging efficient exploration
are the entropy of action policy and curiosity for information gain. Entropy is well-
establishedinliterature,promotingrandomizedactionselection. Curiosityisdefinedin
a broad variety of ways in literature, promoting discovery of novel experiences. One
exa...

Key Terms: principle, energy, simulation, prediction, noise, agents, observational, entropy, intrinsic, curiosity

=== FULL PAPER TEXT ===

Intrinsic Rewards for Exploration without Harm
from Observational Noise: A Simulation Study
Based on the Free Energy Principle
TheodoreJeromeTinker,KenjiDoya,JunTani∗
Keywords: ReinforcementLearning,TheFreeEnergyPrinciple,
ActiveInference,Entropy,Curiosity
CognitiveNeuroroboticsResearchUnit,OkinawaInstituteofScience
andTechnologyGraduateUniversity
1919-1Tancha,Onna-san904-0495,Okinawa,Japan
∗ Correspondence: jun.tani@oist.jp
1
4202
yaM
31
]GL.sc[
1v37470.5042:viXra
Abstract
InReinforcementLearning(RL),artificialagentsaretrainedtomaximizenumerical
rewards by performing tasks. Exploration is essential in RL because agents must dis-
cover information before exploiting it. Two rewards encouraging efficient exploration
are the entropy of action policy and curiosity for information gain. Entropy is well-
establishedinliterature,promotingrandomizedactionselection. Curiosityisdefinedin
a broad variety of ways in literature, promoting discovery of novel experiences. One
example, prediction error curiosity, rewards agents for discovering observations they
cannotaccuratelypredict. However,suchagentsmaybedistractedbyunpredictableob-
servational noises known as curiosity traps. Based on the Free Energy Principle (FEP),
this paper proposes hidden state curiosity, which rewards agents by the KL divergence
between the predictive prior and posterior probabilities of latent variables. We trained
six types of agents to navigate mazes: baseline agents without rewards for entropy
or curiosity, and agents rewarded for entropy and/or either prediction error curiosity or
hiddenstatecuriosity. Wefindentropyandcuriosityresultinefficientexploration,espe-
cially both employed together. Notably, agents with hidden state curiosity demonstrate
resilience against curiosity traps, which hinder agents with prediction error curiosity.
This suggests implementing the FEP may enhance the robustness and generalization
of RL models, potentially aligning the learning processes of artificial and biological
agents.
2
1 Introduction
Reinforcement Learning (RL) is a machine learning algorithm for training artificial
agents to perform tasks by awarding or punishing their actions with numerical rewards
(Bartoetal.,1983;WatkinsandDayan,1992;Mnihetal.,2015). Thiscanbeinterpreted
as akin to biological agents learning through evolution. Extrinsic rewards are awarded
at human discretion based on the tasks at hand. Intrinsic rewards are generated by
agents themselves (with human-provided hyperparameters) based on other goals such
as exploration. Exploration is an important but difficult aspect of RL because an agent
canonlyexploitknowledgeafterlearningthatknowledge. Anexplorationphasemaybe
implemented just by selecting random actions for the agent, but this can be inefficient,
especially with high-dimensional continuous state-action spaces and sparse extrinsic
rewards. Hence, we study two intrinsic rewards for efficient exploration: entropy in
the action-space for control as inference (Millidge et al., 2020) and curiosity about the
environment for active inference (Tschantz et al., 2020, 2023). Meanwhile, Friston’s
Free Energy Principle (FEP) describes biological neuroscience with Bayesian statistics
applicable to machine learning and AI (Kaplan and Friston, 2018; Parr and Friston,
2019). Our goal is to share a novel definition of curiosity derived from the FEP and
demonstrateitsrobustusefulnessinexploration.
Curiosity in RL has been presented in many ways, typically based on an agent’s
abilitytopredictfutureobservations. Thisleveragesanadversarialrelationshipbetween
the agent’s predictive accuracy using a forward or generative model, also known as a
transitioner or world model, and its pursuit of observations that challenge this model’s
accuracy. For example, Schmidhuber (2010) critiqued that curiosity gauged by “mean
3
squaredpredictionerrororsimilarmeasures,”whichOudeyerandKaplan(2007)called
Predictive Novelty Motivation and we call prediction error curiosity, “may fail when-
ever high prediction errors do not imply expected prediction progress, e.g., in noisy
environments.” Alternative forms of curiosity from these sources included estimating
likelihoods of events, measuring prediction errors probabilistically, or assessing im-
provementinpredictions,butthesemethodscanhavegreatcomputationalcosts. More-
over, Oudeyer and Kaplan note “in certain application contexts... intrinsic openness is
aweakness”andcounterproductive.
Pathak et al. (2017) elaborated on Schmidhuber’s critique of prediction error cu-
riosity in noisy environments, which may be an example of what Oudeyer and Kaplan
called a weakness of intrinsic openness. Pathak et al. asked readers to consider an
agent which could observe tree leaves randomly dancing in the wind. The agent’s for-
ward model would never be able to perfectly predict such observations, so the agent
might become fixated on these leaves like a moth attracted to a lamp. Thus, such ob-
servational noises or expected uncertainties are called curiosity traps. To remedy this,
Pathak et al. trained an inverse dynamics model to predict the agent’s action between
two consecutive observations, thereby encoding observations into a latent space with-
out noisy details, relevant only to the agent’s actions. Then, the agent’s forward model
could be trained to predict these refined latent states instead of chaotic observations.
Prediction error curiosity based on that forward model could ignore the environment’s
irrelevant noise and thus be minimally impacted by curiosity traps, instead focusing on
unexpecteduncertainties. However,thiscouldnotfunctionwellwithrarelyexperienced
interactions. Pathak et al. suggested storing events in a memory buffer for experience
4
replay,whichweimplement.
Schwartenbeck et al. (2019) derived two intrinsic rewards for exploration directly
from the FEP. An agent with these intrinsic rewards uses Bayesian inference to mini-
mizefreeenergy,meaningnotonlymaximizingextrinsicrewardsbutalsodevelopinga
thoroughunderstandingoftheenvironment. Thefirstintrinsicreward,parameterexplo-
ration, encourages active learning: the agent seeks to resolve uncertainty about how its
actionsarerewarded. Thesecondintrinsicreward,hiddenstateexploration,encourages
active inference: the agent seeks to take actions which reveal uncertain observations.
Together these intrinsic rewards establish curiosity for both the environment and the
task at hand. However, Schwartenbeck et al. assumed agents in T-mazes knew the
mazes had two arms, the left providing a constant extrinsic reward, the right providing
an uncertain extrinsic reward. Agents in RL typically must infer such knowledge from
observations,sotheseintrinsicrewardsarenoteasilyappliedtoRLdirectly.
Kawahara et al. (2022) derived both entropy and an RL-applicable definition of cu-
riosityfromtheFEP.TheyconstructedaforwardmodelasaBayesianNeuralNetwork
(BNN) such that the weight-parameters are not fixed, but are drawn from a multivari-
ate Gaussian distribution using the reparameterization trick for stochastic optimization
(Blundell et al., 2015; Kingma et al., 2015). Kawahara et al. gauged curiosity values
based on Kullback–Leibler divergence comparing the weights’ distribution before and
after learning to predict each new observation with minimal free energy. This proba-
bilistic approach must consider uncertainty in observational noise, so like the curiosity
of Pathak et al., this curiosity should be able to effectively explore without negative in-
fluencefrom curiositytraps. However, aBNN’s computationalcost ishigh, andKawa-
5
haraet al. only workedinterms ofa MarkovDecisionProcess (MDP)where statesare
completely observed. Additionally, this method produces just a single curiosity value
per training update, which, if applied to an entire batch, would only produce one cu-
riosityvalueforthebatchasawhole,overlookingthecontributionsofeachobservation
individually. Identifying which parts of the batch are important for exploration and
which are not is a computationally demanding task because each observation must be
evaluatedoneatatime.
In this paper, we overcome the problem of prediction error curiosity using hidden
state curiosity defined in section 3. Like the curiosity of Kawahara et al., hidden state
curiosity is derived from the FEP and gauged by the Kullback-Leibler divergence be-
tween predictive prior and posterior over future states (under a particular policy). Like
thecuriosityofPathaketal.,thosestatesareefficientlyencodedaslatentvariables. We
train six types of agents: a baseline with no intrinsic rewards, entropy-driven, predic-
tion error curious, hidden state curious, and two hybrids combining entropy with each
form of curiosity. The abilities of these agents to find goals in a biased T-maze or an
expandingT-maze(firstaT-maze,thenadoubleT-maze,andthenatripleT-maze)will
testthefollowingtwohypotheses:
1. Entropy and curiosity improve agent exploration, especially when both are im-
plementedtogetherasimpliedbytheFEP.
2. Predictionerrorcuriositycanbenegativelyinfluencedbyobservationalnoisealso
knownascuriositytraps,whilehiddenstatecuriositycanbemoreresilienttosuch
curiositytraps.
6
The results in section 4 are presented to evaluate the validity of these hypotheses, con-
tributing to our understanding of artificial intelligence exploration in complex environ-
ments.
2 Prior Studies
2.1 Reinforcement Learning
In Reinforcement Learning (RL), an agent experiences an episode as a sequence of
transitions of the form {o , a , r , o , done }. Variables o and o are the agent’s
t t t t+1 t t t+1
observations at times t and t+1 equal to (in a Markov Decision Process, MDP) or de-
rivedfrom(inaPartiallyObservableMarkovDecisionProcess,POMDP)thecomplete
environmental states s and s . Variable a is the agent’s action performed at time t.
t t+1 t
Variable r is the extrinsic reward the agent obtained by performing that action. And
t
variabledone is1iftimetwasthefinalstepintheepisodeor0otherwise.
t
In the Actor-Critic method (Barto et al., 1983), an agent has at least two neural
networks using parameters ϕ and θ to instantiate implicit (i.e., amortise) mappings:
an actor network π (o ) → a also known as a policy, which chooses actions based on
ϕ t t
observations,andacriticnetworkQ (o ,a ) → Q(cid:98) ,whichpredictsfuturerewardsbased
θ t t t
on observations and actions to estimate the state-action value function. The critic’s
target value Q is r plus future rewards through bootstrapping (see equation 3, similar
t t
to Bellman’s equation) predicted by a target critic Q . The target critic begins with
θ¯
parameters starting equal to the critic’s, then slowly learns alongside the critic with
¯ ¯
Polyaxaveragingsuchthatθ ← τθ+(1−τ)θ withhyperparameterτ ∈ [0,1]beingthe
7
softupdatecoefficient.
Anensembleofmultiplecritics(andanequalnumberoftargetcritics)canbeimple-
mented,inwhichcasetheactor’slossfunction(equation1)utilizesthelowestpredicted
value among all critics. Furthermore, the actor’s training can be staggered with a delay
of d epochs relative to the critics’ training, ensuring less frequent actor training for sta-
bility. Regardlessoftheseimplementations,theagent’sactorandcriticsshallbetrained
off-policy with experience replay by randomly sampling a batch of transitions from a
memorybuffer.
When using the Soft-Actor-Critic method (SAC) to implement entropy (Haarnoja
et al., 2018), the agent’s actions are randomly sampled with the reparameterization
trickforstochasticoptimization(Kingmaetal.,2015). Theactorgeneratestwovectors
the size of an action: mean µ , made without activation, and standard deviation σ ,
t t
madewithsoftplusactivation. Theactionisfinallyselectedbycombiningthesevectors
with noise ϵ sampled from the Unit Gaussian distribution N(0,1), resulting in a =
t t
tanh(µ +σ ◦ϵ ). Entropyinaction-choiceisintrinsicallyrewardedintheactor’sloss
t t t
functionwhichtrainstheactortominimizeJ :
π
J (ϕ) = E [−Q (o ,a )]−αH(π (a |o )) (1)
π ot∼D,at∼π
ϕ
θ t t ϕ t t
with D being the distribution of observations in the environment, non-negative hyper-
parameter α being the relative importance of entropy, and H(π (a |o )) being the self-
ϕ t t
information (i.e., negative log probability) of an action a given observation o , whose
t t
expectation can be read as an action entropy. The hyperparameter α can be chosen
manually, orit can bea dynamic parameterwhich is trained based onanother hyperpa-
8
¯
rameter,targetentropyH. Inthatcase,αischosentominimizeJ inequation2. Ifthe
α
difference between entropy and target entropy is negative, actions are too probable, so
log(α)willbeincreasedtoencouragemoreentropy;ifthedifferenceispositive,actions
aretooimprobable,solog(α)willbedecreasedtoencouragelessentropy.
¯
J (α) = log(α)·(H(π(a |o ))−H) (2)
α t t
Meanwhile, the SAC’s critic is trained to minimize J in the following target value
Q
and loss function. We here also include an intrinsic reward for curiosity, P , which
t
can take several forms. In subsequent sections we will consider the functional form of
these alternatives and, in the final section, use numerical experiments to quantify their
contributiontooptimalbehavior,asassessedviarewardlearning.
Q(t) = r +ηP +
t t
γ(1−done )E [Q (o ,a )]−αH(π (a |o )) (3)
t ot+1∼D,at+1∼π
ϕ
θ¯ t+1 t+1 ϕ t+1 t+1
J (θ) = E [(Q (o ,a )−Q(t))2] (4)
Q ot∼D,at∼π
ϕ
θ t t
withnon-negativehyperparameterη beingtherelativeimportanceofcuriosityvalueP
t
andhyperparameterγ ∈ [0,1]beingthebootstrappingdiscountrate.
2.2 Recurrent RL
YangandNguyen(2021)broughtrecurrentneuralnetworks(RNN)toRLbyproviding
the actor and critic recurrent layers utilizing hidden states hϕ and hθ respectively (see
t t
9
figure 1a). Such an actor or critic requires two additional inputs: the previous action
a and the previous hidden state hϕ or hθ (in the critic’s case, hθ contains a
t−1 t−1 t−1 t−1 t−1
implicitly), with a , hϕ , and hθ being initialized as zero-filled tensors. The actor’s
−1 −1 −1
hiddenstates(and,symmetrically,thecritic’shiddenstates)wouldbecalculatedas
hϕ,...,hϕ = RNN (o ||a ,...,o ||a )
1 t ϕ 1 0 t t−1
with || denoting concatenation. To train recurrent models with experience replay, a re-
currentmemorybuffermustcontainwholeepisodesbeginningtoendfortemporalcon-
text. Episodes may vary in length, so batches sampled from recurrent memory buffers
have all episodes standardized to the maximum episode’s length using zero-filled tran-
sitions. Therefore, transitions of the batch include another variable, mask , which is
t
1 if the transition was actually within the episode or 0 if the transition was added for
standardlength.
Weutilizethismethodandanothermethoddescribedinsection3depictedinfigure
1b. Using either of these methods enables the agent to maintain a continuity of experi-
ence,thusgivingtheagentatemporaledgeinlearninganddecision-making. Thismay
beessentialfornavigatingenvironmentswherestatesarenotdirectlyobservable.
2.3 Curiosity Derived from the FEP
AmongvariousdefinitionsofcuriositysuchasthosefromOudeyerandKaplan(2007),
Schmidhuber (2010), Pathak et al. (2017), and Schwartenbeck et al. (2019), the most
relevant to this paper is that of Kawahara et al. (2022) because it is directly derived
from the FEP for RL. Here we first describe the FEP as utilized by Kawahara et al.,
10
Actor (RNN)
Critic (RNN)
ENV
a t−1 o t a t o t
hϕ hϕ hθ
t−1
hθ
t
t−1 t
Equation 1 a Equation 4 Q
t
(a)
Actor (Forward’s Hidden State) Critic (Forward’s Hidden State)
hq a hq
t t t
Equation 1 a t Equation 4 Q
(b)
Figure1: (a)Implementingrecurrentlayersinanactormodelandacriticmodel. Notice
the previous action a is implicitly included in hθ . (b) Implementing the forward
t−1 t−1
model’s hidden state hq in an actor model and a critic model. Black arrows indicate
forwardcomputations. Redarrowsindicatelossfunctionsforbackpropagation.
thendescribetheirdefinitionofcuriosityinthatframework.
ThecornerstoneoftheFEPisBayes’theorem,whichdescribestherelationshipbe-
tweenthe priorprobabilitydistributionof parameterX,p(X),and theposteriorproba-
bilitydistributiongivendataD,p(X|D),expressedas
p(D|X)p(X) p(D|X)p(X)
p(X|D) = = .
(cid:82)
p(D) p(D,X)dX
11
(cid:82)
However, p(D,X)dX maybefartoocomputationallyexpensivetocalculate. Instead
we can approximate p(X|D) with the predictive distribution q(X). The difference be-
tweenq(X)andp(X|D)canbemeasuredusingKullback–Leiblerdivergence:
(cid:90)
q(X)
D [q(X)||p(X|D)] = q(X)log dX
KL
p(X|D)
(cid:90)
q(X)p(D)
= q(X)log dX
p(D,X)
(cid:90)
q(X)p(D)
= q(X)log dX. (5)
p(X)p(D|X)
Because that integral is in terms of X and p(D) is independent of X, we may remove
p(D) from the integral with the laws of logarithms. This reveals the concept of varia-
tionalfreeenergyF,definedas
D [q(X)||p(X|D)] = D [q(X)||p(X)]−E [logp(D|X)]+logp(D)
KL KL X
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
complexity accuracy
= F +logp(D). (6)
Withthis,wemayestimateq(X)asargminF.
q(X)
Giventimeτ ≥ t+1,expectedfreeenergyG isequaltofreeenergyF multiplied
τ
bylikelihoodP(D |X ),suchthat
τ τ
G = E [F]
τ p(Dτ|Xτ)
(cid:90)
q(X )
= E [ q(X )log τ dX]
p(Dτ|Xτ) τ
p(D ,X )
τ τ
q(X )
= E [E [log τ −logp(D )]]. (7)
p(Dτ|Xτ) q(Xτ)
p(X |D )
τ
τ τ
12
Becauseq approximatespandq(D |X )q(D ) = q(D ,X ),
τ τ τ τ τ
q(X )
G ≈ E [log τ −logp(D )]
τ q(Dτ,Xτ)
q(X |D )
τ
τ τ
q(X |D )
= −E [log τ τ ]−E [logp(D )]
q(Dτ,Xτ)
q(X )
q(Dτ) τ
τ
BayesianSurprise
(cid:122) (cid:125)(cid:124) (cid:123)
= −E [D [q(X |D )||q(X )]]−E [logp(D )]. (8)
q(Dτ) KL τ τ τ q(Dτ) τ
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
EpistemicValueorMutualInformation ExtrinsicValue
Kawahara et al. (2022) described how this equation can be used to interpret biological
agents’behaviors. TheExtrinsicValuetermrepresentsthebiologicalagents’priorpref-
erences or desires, much like extrinsic reward r . The Epistemic Value term represents
t
howlivingcreatureschooseactionstofinddataDprovidinginformationaboutparame-
terX,reducinguncertainty. Hence,biologicalagentsseektosatisfytheirdesireswhile
alsoacquiringunknowninformation.
Kawahara et al. applied the FEP to RL by training agents to choose actions which
minimize expected free energy, emulating biological thought. Let D = {s ,a } (with
τ τ τ
the understanding that Kawahara et al. considered states to be completely observed
suchthato = s )soexpectedfreeenergycanberewrittenas
τ τ
13
p(X |s ,a )
G = −E [log τ τ τ ]−E [logp(s ,a )]
τ q(sτ,aτ,Xτ)
q(X )
q(sτ,aτ) τ τ
τ
p(X ,a |s )
= −E [log τ τ τ ]−E [logp(s ,a )]
q(sτ,aτ,Xτ)
q(X )p(a |s )
q(sτ,aτ) τ τ
τ τ τ
q(X |s )q(a |s ,X )
≈ −E [log τ τ τ τ τ ]−E [logp(s ,a )]
q(sτ,aτ,Xτ)
q(X )p(a |s )
q(sτ,aτ) τ τ
τ τ τ
= −E [D [q(X |s )||q(X )]]
q(aτ|sτ,Xτ)q(sτ) KL τ τ τ
−E [D [q(a |s ,X )||p(a |s )]]
q(sτ,Xτ) KL τ τ τ τ τ
−E [logp(s ,a )]. (9)
q(sτ,aτ) τ τ
Consider now a forward model f which is a Bayesian Neural Network (BNN) (Blun-
w
dell et al., 2015). Because this forward model predicts the next state given the current
state and action, its weight parameters w can be interpreted as the agent’s latent un-
τ
derstandingoftheenvironment;thus,letX = w suchthatthefirsttermofequation9
τ τ
reflectshowtheagent’sunderstandingoftheenvironmentchangesbasedonexperienc-
ing a state. Let q = N(w |µ,σ) with ψ = {µ,σ}, so a Soft Actor Critic actor π can
ψ τ ϕ
be trained to approximate π (a |s ) ≈ q(a |s ,w ). This allows rewriting expected
ϕ τ τ τ τ τ
freeenergyas
G(s ,a ) = −E [D [q(w |s )||q(w )]]
τ τ q(aτ|sτ,wτ)q(sτ) KL τ τ τ
−E [D [π (a |s )||p(a |s )]]
q(sτ,aτ) KL ϕ τ τ τ τ
−E [logp(s ,a )]. (10)
q(sτ,aτ) τ τ
Finally, let τ = t, let prior preference extrinsic value logp(s ,a ) be extrinsic reward
t t
r(s ,a ) = r , and recall the forward model’s objective is predicting s , so expected
t t t t+1
14
freeenergycanberewrittenas
G(s ,a ) = −D [q (w |s )||q (w )]−logp(s ,a )
t t KL ψ t t+1 ψ t t t
−D [π (a |s )||p(a |s )]
KL ϕ t t t t
= −D [q (w |s )||q (w )]−logp(s ,a )
KL ψ t t+1 ψ t t t
(cid:90) (cid:90)
− π (a |s )logπ (a |s )da + π (a |s )logp(a |s )da
ϕ t t ϕ t t t ϕ t t t t t
= −D [q (w |s )||q (w )]− r(s ,a )
KL ψ t t+1 ψ t t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Curiosity ExtrinsicReward
−H(π (a |s ))−E [logp(a |s )]. (11)
ϕ t t π
ϕ
(at|st) t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Entropy Imitation
In this formation, the RL implementation of the FEP presents extrinsic rewards, en-
tropy, and curiosity. (Imitation, an intrinsic reward which we do not explore in this
paper,couldfurtherbeusedtotrainanagenttoemulatethebehaviorofanotheragent.)
Equations1and3canincorporatetheseentropyandcuriosityvaluesorapproximations
thereof,multipliedbynon-negativehyperparametersα andη. Calculatingthecuriosity
value P = D [q (w |s )||q (w )] involves two distributions: q (w ), the distribu-
t KL ψ t t+1 ψ t ψ t
tion of f ’s weights, and q (w |s ), the same distribution after considering the next
w ψ t t+1
state. The first distribution, q (w ), is readily available; it may be called a predictive
ψ t
prior. We can then obtain the predictive posterior distribution q (w |s ) as if training
ψ t t+1
theforwardmodeltopredicts withminimalfreeenergyF,calculatedas
t+1
F = D [q (w )||p (w )]−E [logp (s |w )]. (12)
KL ψ t ψ t q
ψ
(wt) ψ t+1 t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Complexity Accuracy
In practice, prior distribution p (w ) can be the unit Gaussian N(0,1), and the com-
ψ t
15
plexity term is multiplied by a non-negative hyperparameter β describing its relative
importancetoaccuracy.
Thusly, Kawahara et al. used the FEP’s framework to define curiosity such that an
observation has a low curiosity value if the forward model does not need to change
much to accommodate it, or a high curiosity value if the forward model must change
drastically. This encourages an adversarial relationship between the agent’s forward
model and actor: the forward model trains to improve its weights representing a prob-
abilistic interpretation of the environment, but the critic rewards the actor for finding
information which substantially alters the forward model’s weights. This active infer-
encecomplementsentropy’scontrolasinference. Importantly,theagent’sprobabilistic
interpretation of the environment should account for observational noise, so observing
anticipatednoiseshouldnotalteritmuch;hence,thefreeenergybasedcuriositydefined
byKawaharaetal. shouldbeabletoeffectivelyexplorewithoutnegativeinfluencefrom
curiositytraps. However,thisdefinitionofcuriosityisconstrainedregardingbatchpro-
cessing: individual transitions within a batch may differ in exploratory importance, but
comparing the forward model’s weights before and after training with the entire batch
as a whole returns only one curiosity value. Investigating the significance of each tran-
sitionindividuallyrequiresgreatcomputationalcost. Thatlimitation,andtherestriction
tofullyobservableMDP,suggestthereareopportunitiesforfurtherdevelopment.
16
3 Proposed Model
Motivated by Kawahara et al. and Pathak et al., we define hidden state curiosity using
a forward model with the architecture of a Variational RNN (VRNN) (Chung et al.,
2016). Our forward model f , pictured in figure 2 and described by algorithm 1, is
ψ
recurrent using hidden state hq, enabling the accounting of temporal dependencies and
t
uncertainties in a Partially Observable Markov Decision Process (POMDP). This pro-
videsasecondmethodforprovidingtheactorandcritictemporalknowledge: replacing
observationsin the models’inputswithhq (seefigure 1b). We applythismethodto the
t
actor, with a training-delay of d = 2, while two critics use their own recurrent layers,
as shown in figure 1a (see section 2.1 regarding delayed actor training and multiple
critics). Unlike the Bayesian Neural Network (BNN) used by Kawahara et al., our for-
wardmodeldoesnothaveprobabilisticweights;instead,weusethereparameterization
trick in the style of a SAC actor or Variational Bayes Autoencoder (VAE) to sample
prior and posterior inner states zp and zq from corresponding probability distributions
t t
p(z ) = N(µp,σp) and q(z ) = N(µq,σq). These distributions are derived from the
t t t t t t
previous hidden state hq , the previous action a , and in the case of the posterior
t−1 t−1
innerstate,thecurrentobservationo . Posteriorinnerstatezq andprevioushiddenstate
t t
hq areusedtogeneratehiddenstatehq.
t−1 t
Thisforwardmodeltrainstogenerateaccuratepredictionso ofupcomingobser-
(cid:98)t+1
vationso withhiddenstateshq andactionsa whileminimizingtheKullback-Leibler
t+1 t t
divergence comparing p(z ) and q(z ). The relationship between p(z ) and q(z ) is par-
t t t t
allel to the relationship between Kawahara et al.’s q(w ) and q(w |s ), used in equation
t t t
10 to replace q(X ) and q(X |s ) in equation 9: these pairs are an agent’s probabilistic
t t t
17
Forward
N(µp,σp) N(µq,σq)
0 0 0 0
= p(z ) = q(z )
0 0
Complexity
hq = 0.0 zp zq hq
−1 0 0 0
a −1 = 0.0 o 0 a 0 o (cid:98)1 Accuracyo 1
Initiate Step 0
Figure2: Forwardmodel’sarchitecturebasedonVRNN.Blackarrowsindicateforward
computations. Redarrowsindicateerrorsforbackpropagation.
understanding of its environment before and after experiencing a state or observation.
Thus,thisforwardmodelistrainedtominimizefreeenergyF withequation12rewrit-
tenasequation13.
F = D [q(z )||p(z )]−E [logp(o |z )] (13)
KL t t q(zt) t+1 t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Complexity Accuracy
In practice, the complexity term is multiplied by a non-negative hyperparameter β de-
scribingitsrelativeimportancetoaccuracy.
NoticethatthecuriosityterminEquation11andthecomplexityterminEquation12
are both KL divergences that quantify information gains (a.k.a., relative entropies). In-
ferring latent states or parameters—by minimizing variational free energy—minimizes
the divergence between the posterior and prior, given an observation; thereby minimiz-
ingcomplexityandimplicitlymaximizingcompressionandgeneralization. Conversely,
during action selection, based upon expected free energy, the divergence between the
predictive posterior and prior is maximized, to maximize expected information gain.
One can leverage these information theoretic interpretations by using the complexity
18
Algorithm1PseudocodeforProposedModel(PartOne)
Initializeforwardmodelf ,actorπ ,criticQ ,replaybufferR ▷Twocriticsused
ψ ϕ θ
¯
Initializetargetcriticweightsθ ← θ ▷Onetargetcriticforeachcritic
forepoch=0,Mdo
▷Ineachepoch,theagentplaysoneepisodeandtrainswithabatchofepisodes
Initializehq anda = 0.0,receiveo ▷Beginnewepisode
−1 −1 0
fort=0,Tdo ▷Stepsinepisode
µq,σq ← MLP(hq ||a ||o )
t t t−1 t−1 t
▷Posteriorinnerstatedistribution;||denotesconcatenation
zq ∼ q(z ) = N(µq,σq)) ▷Sampleposteriorinnerstate
t t t t
hq ← RNN(hq ,zq) ▷Advancehq
t t−1 t
Executeactiona ← π (a |hq)toreceiveo ,r ,anddone
t ϕ t t t+1 t t
▷Ifdone ,stopepisode
t
endfor
Storeepisode’stransitions(o ,a ,r ,done )inR ▷Saveepisode
0:T+1 −1:T 0:T 0:T
Samplebatchofepisodes(o ,a ,r ,done ,mask )fromR
0:T+1 −1:T 0:T 0:T 0:T
▷Samplebatch
Initializehq ,hθ ,andh′θ = 0.0 ▷Begintraining
−1 −1 −1
fort=0,T+1do ▷Stepsinadvancingforwardmodelwithbatch
µp,σp ← MLP(hq ||a ) ▷Priorinnerstatedistribution
t t t−1 t−1
µq,σq ← MLP(hq ||a ||o ) ▷Posteriorinnerstatedistribution
t t t−1 t−1 t
Note: Algorithm continues. See sections 2.1 and 2.2 for details on o , a , r , done ,
t t t t
mask ,delayedactor,andmultiplecritics.
t
19
Algorithm1PseudocodeforProposedModel(Part2)
ifutilizinghiddenstatecuriositythen
P ← D [q(z )||p(z )]
t−1 KL t t
▷Comparepriorandposteriorforhiddenstatecuriosity
else
P ← E [logp(o |z )]
t q(zt) t+1 t
▷Evaluatepredictionforpredictionerrorcuriosity
endif
zq ∼ q(z ) = N(µq,σq)) ▷Sampleposteriorinnerstate
t t t t
hq ← RNN(hq ,zq) ▷Advancehq
t t−1 t
endfor
fort=0,T+1do ▷Stepsinadvancingcriticswithbatch
Q(cid:98) ,hθ ← Q (o ,a ,hθ ) ▷PredictQ-valueandadvancehθ withbothcritics
t t θ t t t−1
h′θ ← Q (o ,a ,hθ ) ▷Advanceh′θ forbothtargetcritics(ignoreQoutput)
t θ¯ t t t−1
a′ ← π (a |hq) ▷Makenewactionwithactor
t ϕ t t
ift > 1then ▷Afterfirststep,maketargetQ-valueswithbothtargetcritics
Q ← Q (o ,a′,hθ )
t θ¯ t t t−1
▷Gettargetcritics’Q-values(ignoreh′θ output)
Q ← r +ηP +γ(1−done )(Q −αH(π (a′|o )))
t t t t t ϕ t t
▷MaketargetQ-values(eq. 3)
endif
endfor
Note: Algorithmcontinues.
20
Algorithm1PseudocodeforProposedModel(Part3)
F ← (βD [q(z )||p(z )]−E [logp(o |z )])∗mask
KL 0:T 0:T q(z0:T) 1:T+1 0:T 0:T
▷Forwardmodelloss(freeenergy,eq. 13)
ψ ← ψ −λ ∂F ▷Trainforwardmodel
ψ∂ψ
J (θ) ← (Q(cid:98) −Q )2 ∗mask ▷Criticloss(eq. 4)
Q 0:T 1:T+1 0:T
ˆ
θ ← θ−λ ∇ J (θ) ▷Traincritics
Q θ Q
¯ ¯
θ ← τθ+(1−τ)θ ▷Updatetargetcritics
¯
ifutilizingdynamicα withtargetentropyH then
¯
J (α) ← log(α)·(H(π(a |o ))−H)∗mask ▷Alphaloss(eq. 2)
α 0:T 0:T 0:T
ˆ
log(α) ← log(α)−λ ∇ J (α) ▷Trainalpha
α α α
endif
ifepoch ≡ 0moddthen
▷Ineachdth epoch,trainactor;uselowestQ-valueamongcritics
J (ϕ) ← (−Q (o ,a′ )−αH(π (a′ |o )))∗mask
π θ 0:T 0:T ϕ 0:T 0:T 0:T
▷Actorloss(eq. 1)
ˆ
ϕ ← ϕ−λ ∇ J (ϕ) ▷Trainactor
π ϕ π
endif
endfor
Note: Algorithmiscompleted.
term as an estimate of the expected information gain, under the policy being learned.
This is licensed because the actor-critic model used in reinforcement learning learns a
state-actionpolicythatcan,effectively,learntheexpectedinformationgaininthesame
way that it learns expected extrinsic rewards. In other words, one can define hidden
state curiosity at the previous time step to be the complexity in Equation 14. (In prac-
21
tice, we apply a clamp constraining hidden state curiosity between 0 and 1). Thus, the
actorandcriticsaretrainedtominimizeexpectedfreeenergy,rewritingequation11as
G(s ,a ) = −D [q(z )||p(z )]− r(s ,a )
t t KL t+1 t+1 t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
HiddenStateCuriosity ExtrinsicReward
−H(π (a |o ))−E [logp(a |o )]. (14)
ϕ t t π
ϕ
(at|ot) t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Entropy Imitation
Equations 1 and 3 can incorporate approximations of these entropy and curiosity val-
ues, multiplied by non-negative hyperparameters α and η (not β) respectively. This
underwrites an adversarial relationship: the forward model is trained to make accurate
predictions with minimal complexity but the critic rewards the actor for choosing ac-
tions that result in high complexity; namely, a high information gain that characterizes
curiousorinformationseekingpolicies. Thisisincontrasttopredictionerrorcuriosity,
which rewards the actor for choosing actions resulting in poor accuracy. No agent can
perfectlypredictanobservationwhichincludesrandomnoise,leavingaccuracyperpet-
uallyflawed,soagentswithpredictionerrorcuriositywillalwaysbecuriousaboutsuch
an observation. However, an agent employing hidden state curiosity may generate a
priorinnerstatewhichforeseestheobservation’srandomnoise;thus,therandomnoise
mightnotprovideanyadditionalinformationfortheposteriorinnerstate,sotheagent’s
prior and posterior conceptualization of the environment can perfectly match despite
thatrandomness. Therefore,ratherthanseekingmerelynoisyobservations,suchagents
seek observations which actually alter their latent interpretations of the environment.
This reflects how Pathak et al. utilized latent spaces ignoring irrelevant noise, so we
predict hidden state curiosity can effectively explore without negative influence from
22
curiositytraps. Also,p(z )andq(z )canbenewlygeneratedforeachindividualtransi-
t t
tion in a batch of episodes with relatively easy computational cost, assigning curiosity
valuesinascalablemanner.
4 Simulation Experiments
As established in section 1, we designed our experiments to investigate these two hy-
potheses:
1. Entropy and curiosity improve agent exploration, especially when both are im-
plementedtogetherasimpliedbytheFEP.
2. Predictionerrorcuriositycanbenegativelyinfluencedbyobservationalnoisealso
knownascuriositytraps,whilehiddenstatecuriositycanbemoreresilienttosuch
curiositytraps.
Tothisend,ourexperimentsfeaturesixtypesofagentstrainingtofindgoalsinvarious
mazes. These will be baseline agents devoid of intrinsic rewards, agents motivated by
either entropy or one form of curiosity (prediction error or hidden state), and agents
motivatedbyacombinationofentropyandonetypeofcuriosity. Seetable1fordetails
about these six types. All agents will share the same architecture with γ = .9, τ = .1,
¯
d = 2, H = −1 (see section 2.1), β = .03 (see equation 13), and learning rate λ = .01
with Adam optimizers. No agents have periods of forced investigation with random
actions, highlighting the importance of motivating exploration. See tables 2 through 8
in appendix Bfor details about models’ architectures in PyTorch. Eachtable illustrates
theparametersofamodellayerbylayer.
23
Name(andAcronym) α η P
t
NoEntropy,NoCuriosity(N) 0 0 None
Entropy(E) None 0 None
PredictionErrorCuriosity(P) 0 1 E [logp(o |z )]
q(zt) t+1 t
EntropyandPredictionErrorCuriosity(EP) None 1 E [logp(o |z )]
q(zt) t+1 t
HiddenStateCuriosity(H) 0 1 D [q(z )||p(z )]
KL t+1 t+1
EntropyandHiddenStateCuriosity(EH) None 1 D [q(z )||p(z )]
KL t+1 t+1
Table 1: Hyperparameters for six types of agents. Recall α and η in equations 1 and
3. If α = None, then α is a dynamic parameter chosen to minimize equation 2. In
Prediction Error curiosity (P and EP), the curiosity value P is the accuracy term of
t
equation13. InHiddenStateCuriosity(HandEH),P isthecuriositytermofequation
t
14.
Regarding the first hypothesis, we predict the baseline agent will perform the least
efficient exploration, while agents rewarded for both entropy and curiosity will outper-
form the rest, regardless of which kind of curiosity. However, regarding the second
hypothesis, we predict that if we train agents in mazes with curiosity traps, agents with
prediction error curiosity will be attracted to those traps, showcasing its susceptibility,
whileagentswithhiddenstatecuriosityareabletoignorethem.
4.1 Experiment Design
In these experiments, we employ the PyBullet physics engine to simulate an RL agent
embodied as a duck. The agent’s observations have two parts: its current speed and
24
an 8 by 8 by 4 image of what is in front of it with the four channels being red, green,
blue, and distance. The agent’s actions also have two parts: adjusting its yaw up to 90
degrees left or right and choosing a speed between 0 meters per time step and a speed
limit(withtheblocksconstructingthemazeshavingside-lengthofonemeter).
Figure 3: An agent’s observation includes its current speed in meters per time step and
an8by8by4imageofwhatisinfrontofit. Theimage’sfourchannelsarered,green,
blue (left), and distance (right). This is the agent’s first observation in the biased T-
maze;seefigure4a.
Each simulated episode is terminated when the agent exits the maze; the agent’s
choice of exit will earn an extrinsic reward or punishment. If no exit is chosen within
30 steps, the episode ends with a punishment of r = −1. Colliding with a maze wall
at any step will also punish the agent with r = −1. Any positive extrinsic rewards are
multipliedby.99stepstaken,encouraginghaste.
In each epoch, the agent will carry out one episode. Memory of that episode’s
transitions will be saved in that agent’s recurrent replay buffer; if that replay buffer
containsmemoryofmorethan250episodes,theoldestepisodewillbedeleted. Thena
batchof32episodeswillbesampledfromthereplaybuffertotraintheagent’sforward
model, actor, and critics. In this manner, we will use different random seeds to train
25
360 agents of each of the six types described above. This will be carried out both with
and without implementing curiosity traps by randomly changing colors of walls near
inferior exits with every step, investigating which types of agents are disadvantaged by
observationalnoise.
26
(a)BiasedT-Maze.
(b)T-Maze. (c)DoubleT-Maze.
(d)TripleT-Maze.
Figure 4: Agent starts where shown. Correct and incorrect exits are marked ✓ and X.
With curiosity traps, blocks with ? change colors each step. Experiment one uses (a)
biasedT-Maze. Experimenttwouses(b)T-maze,(c)doubleT-maze,(d)tripleT-maze.
27
4.1.1 BiasedT-Maze
InthebiasedT-mazesimulationseeninfigure4a,agentshaveaspeedlimitofonemeter
pertimestep. ThebiasedT-mazehastwoexits. TheexitoutoftheT’sleftarmiseasily
accessible (nearby and unobstructed) and provides an extrinsic reward of r = 1. The
exit out of the T’s right arm is difficult to access (farther away and behind an obstacle)
and provides an inconsistent extrinsic reward which is equally likely to be r = 0 or
r = 10withanexpectedvalueE(r) = 5. Intuitivelyforhumanreaders,theoptionwith
highest expected extrinsic value is the exit to the right despite its distance, so we will
call this the correct exit. An agent, however, must explore to the right despite readily
available reward on the left to discover the higher value of the correct exit. We trained
agentsfor500epochsusingthesixsetsofhyperparametersdescribedintable1withor
withoutcuriositytrapsneartheincorrectexit.
Wepredictagentswilldiscoverandexploitthehighervalueofthecorrectexitmore
often when encouraged with entropy or curiosity, especially both at once. We also
expect curiosity traps to negatively impact performance of agents with prediction error
curiosity,whileagentswithhiddenstatecuriosityareabletoignorethem.
4.1.2 ExpandingT-Maze
In the expanding T-maze simulation, agents have a speed limit of two meters per time
step,firstintheT-mazeseeninfigure4b,thenthedoubleT-mazeseeninfigure4c,and
then the triple T-maze seen in figure 4d. In each of the three mazes, only one exit is
deemedcorrect,withitslocationalternatingbetweensuccessivemazestochallengethe
agents’ ability to override previously learned habitual behaviors. If the agent takes the
28
correctexit,itwillberewardedwithr = 10,butiftheagenttakesanyotherexit,itwill
be punished with r = −.5. We trained agents for 500 epochs in the T-maze, then for
2000 epochs in the double T-maze, and then for 4000 epochs in the triple T-maze. We
trainedagentsusingthesixsetsofhyperparametersdescribedintable1withorwithout
curiositytrapsneartheincorrectexitsinthemazes’bottomleftportions.
Wepredictallagents,eventhosewithoutintrinsicrewardsforexploration,toeasily
discover and exploit the correct exit on the right side of the T-maze. Then, when relo-
cated to the double T-maze, we predict all agents to first move to the right, away from
the correct exit now on the left side. We predict agents without intrinsic rewards for
explorationwillhavedifficultyextinguishingthatlearnedbehavior,whileagentsintrin-
sically rewarded with entropy or curiosity, especially both, are able to begin exploring
again to find the new correct exit. Finally, relocation to the triple T-maze will present
this challenge again on a larger scale. We also predict curiosity traps to negatively im-
pact performance of agents with prediction error curiosity, while agents with hidden
statecuriosityareabletoignorethem.
4.2 Results
In figures 5 and 6, see the trajectories of agents trained in the biased T-maze and ex-
panding T-maze, depicting their behaviors. Find an example of an agent’s forward
modelpredictingobservationsinthebiasedT-mazeinfigure7. Infigures10and11,in
appendixA,seetheproportionsofagentschoosingeachexitineachepoch. Findvideos
ofhowagenttrajectorieschangedovertimeatgithub.com/oist-cnru/curious maze.
29
Figure 5: Trajectories of agents after training in the biased T-maze. The correct exit of each maze is marked with a ✓, while each incorrect
exitismarkedwithanX.Ifcuriositytrapsareapplied,blocksmarkedwitha? willchangetorandomcolorswitheverystep.
30
Figure 6: Trajectories of agents after training in the expanding T-maze. The correct
exitsaremarkedwitha✓,whileincorrectexitsaremarkedwithanX.Ifcuriositytraps
areapplied,blocksmarkedwitha? willchangetorandomcolorswitheverystep.
31
Figure 7: Predictions of agent choosing correct exit in biased T-maze, trained with EH
(seetable1)withoutcuriositytraps. Leftcolumn: actualobservations. Middlecolumn:
predictions based on hidden-state hp, made with prior inner state zp. Right column:
predictionsbasedonhidden-statehq,madewithposteriorinnerstatezq.
Statistic results of these experiments are displayed in figures 8 and 9, which show
32
how often agents trained with the hyperparameters described in table 1 reached the
correctexitintheirfinaltenepisodes.
Figures 8a and 9a regard our first hypothesis: entropy and curiosity improve agent
exploration,especiallywhenbothareimplementedtogetherasimpliedbytheFEP.
Figures 8b and 9b regard our second hypothesis: prediction error curiosity can be
negativelyinfluencedbyobservationalnoisealsoknownascuriositytraps,whilehidden
statecuriositycanbemoreresilienttosuchcuriositytraps.
4.2.1 BiasedT-Maze
In figure 8a, the leftmost bar shows that agents trained using the hyperparameters la-
beled “No Entropy, No Curiosity” (acronym N) were the least successful agents in the
biased T-maze. The three bars between the dotted lines show that agents trained us-
ing “Entropy” (E), “Prediction Error Curiosity” (P), or “Hidden State Curiosity” (H)
all performed as well as or better than N utilizing one intrinsic reward. The rightmost
barsshowagentstrainedwithtwointrinsicrewardsusing“EntropyandPredictionError
Curiosity” (EP) or “Entropy and Hidden State Curiosity” (EH) performed best of all,
demonstratingtheimportanceofcombiningtheseintrinsicrewards.
In figure 8b, to the left of the dotted line, note that agents trained using P or EP
performed significantly worse when trained with curiosity traps. In contrast, right of
the dotted line, agents trained using H or EH have no negative impact from curiosity
traps. This demonstrates that hidden state curiosity can mitigate pitfalls which can
entrappredictionerrorcuriosity.
Theseresultscanbevisuallyconfirmedinfigures5and10. Infigure5,someagents
33
trained using EP with curiosity traps only travel in circles to fixate on the randomly
changing walls, revealing the distraction caused by observational noise. Figure 10, in
appendix A, shows how agents trained using N commit to the first exit they encounter,
astheratesofexit-choiceincreaseepochtoepochbutneverdecrease. Incontrast,other
agents select the incorrect exit at an increasing rate until a peak at approximately the
100thepoch,atwhichpointselectionofthecorrectexitincreasesinstead,asiftheagents
becameboredwiththeeasyexitandexploredinstead.
4.2.2 ExpandingT-Maze
Figure 9a displays the performance of agents at the end of training in the T-maze, then
thedoubleT-maze,andfinallythetripleT-maze. Consistenttoallthreemazes,andjust
like in the biased T-maze, the leftmost bar shows that agents trained using the hyper-
parameters labeled “No Entropy, No Curiosity” (acronym N) were the least successful
agents. Thethreebarsbetweenthedottedlinesshowthatagentstrainedusing“Entropy”
(E), “Prediction Error Curiosity” (P), or “Hidden State Curiosity” (H) all performed as
well as or better than N with one intrinsic reward. The rightmost bars show agents
trained with two intrinsic rewards using “Entropy and Prediction Error Curiosity” (EP)
or“EntropyandHiddenStateCuriosity”(EH)performedbestofall,demonstratingthe
importanceofcombiningtheseintrinsicrewards.
Infigure9b,totheleftofthedottedline,notethatagentstrainedusingPperformed
worse when trained with curiosity traps in all three of these mazes, and agents trained
using EP were deeply influenced by curiosity traps in the T-maze and triple T-maze. In
contrast, right of the dotted line, agents trained using H or EH have no negative impact
34
from curiosity traps. This demonstrates that hidden state curiosity can mitigate pitfalls
whichcanentrappredictionerrorcuriosity.
These results can be visually confirmed in the figures 6 and 11. In figure 6, the
impactofcuriositytrapsisclearwhensomeagentstrainedusingPorEPwithcuriosity
traps are attracted to randomly colored walls in all three of the T-mazes. In figure 11,
in appendix A, we see that many agents trained using N learned to reach the correct
exit to the right on the T-maze, but many of them continued to select exits on the right
side when relocated into the double T-maze even though the correct exit was now on
the left. Likewise, many agents trained using E learned to reach the correct exit in the
doubleT-mazewithaleftturnandthenarightturn,butcontinuedtoselectexitsonthe
leftsidewhenrelocatedintothetripleT-mazeeventhroughthecorrectexitwasnowon
the right. In contrast, agents trained using H or EH swiftly stopped choosing incorrect
exitswhenrelocated,whetherwithorwithoutcuriositytraps.
35
(a)
(b)
Figure 8: Biased T-Maze Results. (a) Bars show rate agents chose the correct exit in
theirlasttenepisodesaftertrainingusinghyperparameterslabeledwithacronymsfrom
table1. Errorbarsshow99%confidenceinterval. (b)Ineachpairofbars,iftheleftbar
istallerthantherightbarwithconfidenceof99%,theleftbarisgreenandtherightbar
isred,showingnegativeimpactofcuriositytraps.
36
(a)
(b)
Figure9: ExpandingT-MazeResults(seedescriptionsinfigure8).
37
Discussion
Asdescribedinsection4,ourexperimentscorroboratedthehypothesesintheintroduc-
tion: namely, action entropy and curiosity improve agent exploration, especially when
both are implemented together as implied by the FEP; prediction error curiosity can be
negatively influenced by observational noise also known as curiosity traps, while hid-
den state curiosity can be more resilient to such curiosity traps. These results indicate
that applying the FEP can significantly benefit RL, encouraging agents to investigate
and comprehend causal structures which would otherwise be difficult or impossible to
understand. This could be beneficial for robots in dynamic environments, or interac-
tive systems automatically personalizing content delivery for its users, or researchers
seeking recommendations of directions to survey. However, we have not yet attempted
transferring behaviors learned in simulation to physical agents, and only identified the
optimizedhyperparametersthroughextensivebrute-forcetesting.
The hidden state curiosity (and action entropy) foregrounded in this work inherit
fromdecompositionsofexpectedfreeenergy. Expectedfreeenergyinthissettingsim-
ply refers to a free energy functional of distributions over (random) variables expected
under a particular policy or path into the future. The expected free energy could be re-
gardedasauniversalobjectivefunctionfromtheperspectiveofthephysicsofselforga-
nizingagentsthathavewell-definedorcharacteristicattractingsets. Forarecentderiva-
tion of expected free energy—from the perspective of statistical physics–please see
Friston et al. (2023). One interesting interpretation of expected free energy is in terms
of a dual aspect Bayes optimality. This follows from our decomposition of expected
free energy into expected information gain and expected extrinsic reward. These are
38
exactly the objective functions that underwrite the principles of optimal Bayesian ex-
perimental design (Lindley, 1956; MacKay, 1992) and decision theory (Berger, 2011),
respectively. Onthisview,intrinsicandextrinsicrewardaretwosidesofthesamecoin,
where information and value have exactly the same currency (i.e., natural units). The
implication here is that one can think about rewards in terms of information and, con-
versely,thinkaboutthevalueofinformation(Howard,1966)asintrinsicallyrewarding.
The expected information gain can be applied to any latent variables (i.e. states
or parameters) of a forward model. When applied to the latent states of a generative
model, the implicit intrinsic reward or motivation is sometimes referred to as salience
(Itti and Baldi, 2009). Conversely, when applied to the parameters of a forward model
the expected information gain is sometimes referred to as the novelty that underwrites
curious behavior (Baldassarre et al., 2014; Da Costa et al., 2020; Schmidhuber, 2010;
Schwartenbeck et al., 2019). This is important because we have absorbed the im-
plicit active inference and learning into a reinforcement learning scheme based upon
the actor-critic model. This kind of reinforcement learning identifies state-action poli-
cies in the sense that an optimal policy is identified for every given state. This means
one cannot select policies that maximize information gain about latent states (because
each policy is conditioned upon being in a particular state; as opposed to having pos-
terior beliefs about latent states). However, it is still possible to learn policies that,
on average, are information seeking; especially about the parameters of a generative
model—asinourcase. Theparametersinquestionherearethetransitionparametersof
the forward model. This leads to the interesting notion that one can learn state-action
policies that are information seeking, in exactly the way we have demonstrated with
39
the above numerical experiments. These are sometimes referred to as epistemic habits
(Fristonetal.,2016): e.g.,habituallywatchingacertainnewschannelintheeveningto
seek information about what happened during the day. This observation is potentially
importantbecauseitsuggeststherearelawfulandlearnablewaysofforagingchanging
environments for information; such as mazes that feature curiosity traps and change
overtime.
Looking forward, there are multiple ways future research can explore hidden state
curiosity. For example, the PV-RNN architecture (Ahmadi and Tani, 2019) can in-
troduce hierarchical processing within a VRNN (Chung et al., 2016). Each layer of
suchaframeworkcouldgeneratehiddenstateswithdifferentMultipleTimescaleRNNs
(MTRNNs) (Yamashita and Tani, 2008; Jian et al., 2023), allowing agents to access
both long-term and short-term memories. Using this architecture, agents could have
curiosityabouttheenvironmentinmultipletemporalcontexts.
Moreover, just as the hyperparameter α can be a parameter which dynamically ad-
¯
justs to satisfy target entropy H (see section 2.1), it may be possible to adjust the hy-
¯
perparameter η dynamically to satisfy target curiosity C. For example, like α is op-
¯
timized to minimize log(α) · (H(π(a |o )) − H), η could be optimized to minimize
t t
¯
log(η)·(D [q(z )||p(z )]−C). Thismayrefinetheagent’sengagementwithcuriosity
KL t t
overtimeandhelpusersselectoptimalhyperparameters.
Also, future research should investigate how choice of η, β, and the sizes of ob-
servations and inner states impact hidden state curiosity’s reaction to curiosity traps.
Although choice of η and β can be considered configurable customization, it can be
40
1
challengingtofine-tuneoptimalpairingsforignoringuselessnoiseinthetaskathand .
Finally, theintrinsic reward forimitation shownby Kawaharaet al.(2022) inequa-
tion 11 should be investigated. This could enable agents to learn from human demon-
strations,particularlyforrareorcomplexsituations.
Inconclusion,emulatingbehaviorsassociatedwithbiologicalagentslikecuriosity-
drivenexplorationappearstobeapromisingfrontierinadvancingAI.Althoughcurrent
RLagentscanhavegreatcomputationalpower,theycannotyetachieveunderstandings
asnuancedasinquisitivehumans. TheFEPoffersaprincipledwaytobringsuchuseful
organic practices to artificial agents. Our future hope is to apply hidden state curiosity
tomoreintricate3Dagentstrainingtoperformcompositionalactiongoals,delvinginto
theFEP’sinfluenceonembodiedcognitionandtheemergenceofcommunication.
Findourcodeatgithub.com/oist-cnru/curious maze.
1
Generally, in FEP-based schemes, a free hyperparameter can, in principle, be op-
timized with respect to variational free energy. For simple hyperparameters, this is
usually best achieved with a line search over the hyperparameter to minimise the path
integral of variational free energy—as a bound on log marginal likelihood or model
evidence—accumulatedoverthetimeperiodinquestion.
41
Acknowledgments
TheauthorsarefundedbyOISTgraduateschool.
Thankyougreatlytotheanonymousreviewer.
References
Ahmadi, A. and Tani, J. (2019). A novel predictive-coding-inspired variational rnn
modelforonlinepredictionandrecognition. NeuralComputation,31:2025–2074.
Baldassarre, G., Stafford, T., Mirolli, M., Redgrave, P., Ryan, R. M., and Barto, A.
(2014). Intrinsic motivations and open-ended development in animals, humans, and
robots: anoverview. FrontiersinPsychology,5. Section: CognitiveScience.
Barto,A.G.,Sutton,R.S.,andAnderson,C.W.(1983). Neuronlikeadaptiveelements
that can solve difficult learning control problems. IEEE Transactions on Systems,
Man,andCybernetics,SMC-13(5):834–846.
Berger, A. (2011). Self-regulation: Brain, cognition, and development. American Psy-
chologicalAssociation.
Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D. (2015). Weight uncer-
tainty in neural network. In International Conference on Machine Learning, pages
1613–1622.PMLR.
Chung, J., Kastner, K., Dinh, L., Goel, K., Courville, A., and Bengio, Y. (2016). A
recurrentlatentvariablemodelforsequentialdata. arXivpreprintarXiv:1506.02216.
CIFARSeniorFellow.
42
Da Costa, L., Parr, T., Sajid, N., Veselic, S., Neacsu, V., and Friston, K. (2020). Active
inferenceondiscretestate-spaces: Asynthesis. JournalofMathematicalPsychology.
Review.
Friston, K., Da Costa, L., Sajid, N., Heins, C., Ueltzho¨ffer, K., Pavliotis, G. A., and
Parr, T. (2023). The free energy principle made simpler but not too simple. Physics
Reports,1024:1–29.
Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., O’Doherty, J., and Pezzulo,
G. (2016). Active inference and learning. Neuroscience & Biobehavioral Reviews,
68:862–879.
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018). Soft actor-critic: Off-policy
maximumentropydeepreinforcementlearningwithastochasticactor. arXivpreprint
arXiv:1801.01290.
Howard,R.(1966). Decisionanalysis: Applieddecisiontheory.
Itti,L.andBaldi,P.(2009).Bayesiansurpriseattractshumanattention.VisionResearch,
49:1295–1306.
Jian, N. L., Zabiri, H., and Ramasamy, M. (2023). Control of the multi-timescale
process using multiple timescale recurrent neural network-based model predictive
control. IndustrialandEngineeringChemistryResearch,62(15):6176–6195.
Kaplan, R. and Friston, K. J. (2018). Planning and navigation as active inference.
BiologicalCybernetics,112(3):323–343.
43
Kawahara, D., Ozeki, S., and Mizuuchi, I. (2022). A curiosity algorithm for robots
based on the free energy principle. In 2022 IEEE/SICE International Symposium on
SystemIntegration(SII),Narvik,Norway.
Kingma, D. P., Salimans, T., and Welling, M. (2015). Variational dropout and the local
reparameterization trick. In Advances in Neural Information Processing Systems,
volume28,pages2575–2583.
Lindley, D. V. (1956). On a measure of the information provided by an experiment.
AnnalsofMathematicalStatistics,27(4):986–1005.
MacKay, D. J. C. (1992). A practical bayesian framework for backpropagation net-
works. NeuralComputation,4:448–472.
Millidge, B., Tschantz, A., Seth, A. K., and Buckley, C. L. (2020). On the relationship
ofactiveinferenceandcontrolasinference. Preprint.
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G.,
Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C.,
Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hass-
abis, D. (2015). Human-level control through deep reinforcement learning. Nature,
518(7540):529–533.
Oudeyer, P.-Y. and Kaplan, F. (2007). What is intrinsic motivation? a typology of
computational approaches. Frontiers in neurorobotics. Reviewed by: Jeffrey L.
Krichmar, The Neurosciences Institute, USA; Cornelius Weber, Johann Wolfgang
GoetheUniversity,Germany.
44
Parr,T.andFriston,K.J.(2019). Generalisedfreeenergyandactiveinference. Biolog-
icalCybernetics.
Pathak, D., Agrawal, P., Efros, A. A., and Darrell, T. (2017). Curiosity-driven ex-
ploration by self-supervised prediction. In Proceedings of the 34th International
ConferenceonMachineLearning,JMLR:W&CP,Sydney,Australia.JMLR.org.
Schmidhuber,J.(2010).Formaltheoryofcreativity,fun,andintrinsicmotivation(1990-
2010). IEEETransactionsonAutonomousMentalDevelopment,2(3):230–247.
Schwartenbeck, P., Passecker, J., Hauser, T. U., FitzGerald, T. H., Kronbichler, M.,
and Friston, K. J. (2019). Computational mechanisms of curiosity and goal-directed
exploration. eLife,8:e41703.
Tschantz, A., Baltieri, M., Seth, A. K., and Buckley, C. L. (2023). Scaling active
inference. IEEEXplore.
Tschantz, A., Millidge, B., Seth, A. K., and Buckley, C. L. (2020). Reinforcement
learningthroughactiveinference. InBridgingAIandCognitiveScience(ICLR2020
Workshop),Brighton,UK.UniversityofSussexandUniversityofEdinburgh.
Watkins,C.J.andDayan,P.(1992). Q-learning. MachineLearning,8:279–292.
Yamashita, Y. and Tani, J. (2008). Emergence of functional hierarchy in a multiple
timescaleneuralnetworkmodel: Ahumanoidrobotexperiment. PLoSComputBiol,
4(11):e1000220.
Yang, Z. and Nguyen, H. (2021). Recurrent off-policy baselines for memory-based
continuouscontrol. InDeepReinforcementLearningWorkshop,NeurIPS2021.
45
Figure10: ProportionofagentstakingeachexitintheT-maze. Thecorrectexitiscoloredlightgray.
46
Figure11: ProportionofagentstakingeachexitintheexpandingT-maze. Thecorrectexitiscoloredlightgray.
47
Appendix B
These tables illustrate details about models’ architectures in PyTorch. Each table il-
lustrates the parameters of a model layer by layer. PReLU refers to LeakyReLU with
leak-coefficientasatrainableparameter.
Forward
Portion LayerType Details
ImageIn Convolution channelsin=4,channelsout=16
kernel size=(3,3),stride=(1,1),
padding=(1,1),padding mode=reflect
PReLU num parameters=1
AveragePooling kernel size=(3,3),stride=(2,2),padding=(1,1)
Convolution channelsin=16,channelsout=16,
kernel size=(3,3),stride=(1,1),
padding=(1,1),padding mode=reflect
PReLU num parameters=1
AveragePooling kernel size=(3,3),stride=(2,2),padding=(1,1)
Flatten
Linear in features=64,out features=32,bias=True
PReLU num parameters=1
SpeedIn Linear in features=1,out features=32,bias=True
PReLU num parameters=1
Table2: ArchitectureoftheForwardModel(part1).
48
Forward(continued)
Portion LayerType Details
ActionIn Linear in features=2,out features=32,bias=True
PReLU num parameters=1
µp (zp Mean) Linear in features=64,out features=32,bias=True
PReLU num parameters=1
Linear in features=32,out features=32,bias=True
Tanh
σp (zp STD) Linear in features=64,out features=32,bias=True
PReLU num parameters=1
Linear in features=32,out features=32,bias=True
Softplus beta=1,threshold=20
µq (zq Mean) Linear in features=128,out features=32,bias=True
PReLU num parameters=1
Linear in features=32,out features=32,bias=True
Tanh
σq (zq STD) Linear in features=128,out features=32,bias=True
PReLU num parameters=1
Linear in features=32,out features=32,bias=True
Softplus beta=1,threshold=20
Table3: ArchitectureoftheForwardModel(part2).
49
Forward(continued)
Portion LayerType Details
GRU GatedRNN inputsize=32,hiddensize=32
ImageOut Linear in features=64,out features=16,bias=True
PReLU num parameters=1
Reshape
Convolution channelsin=4,channelsout=16,
kernel size=(3,3),stride=(1,1),
padding=(1,1),padding mode=reflect
PReLU num parameters=1
Upsampling scale factor=2,mode=‘bilinear’
Convolution channelsin=16,channelsout=16,
kernel size=(3,3),stride=(1,1),
padding=(1,1),padding mode=reflect
PReLU num parameters=1
Upsampling scale factor=2,mode=‘bilinear’
Convolution channelsin=16,channelsout=16,
kernel size=(3,3),stride=(1,1),
padding=(1,1),padding mode=reflect
PReLU num parameters=1
Convolution channelsin=16,channelsout=4,
kernel size=(1,1),stride=(1,1)
Table4: ArchitectureoftheForwardModel(part3).
50
Forward(continued)
Portion LayerType Details
SpeedOut Linear in features=64,out features=32,bias=True
PReLU num parameters=1
Linear in features=32,out features=32,bias=True
PReLU num parameters=1
Linear in features=32,out features=1,bias=True
Table5: ArchitectureoftheForwardModel(part4).
51
Actor
Portion LayerType Details
hIn Linear in features=32,out features=32,bias=True
PReLU num parameters=1
Linear in features=32,out features=32,bias=True
PReLU num parameters=1
Linear in features=32,out features=32,bias=True
PReLU num parameters=1
Linear in features=32,out features=32,bias=True
PReLU num parameters=1
µ Linear in features=32,out features=2,bias=True
σ Linear in features=32,out features=2,bias=True
Softplus beta=1,threshold=20
Table 6: Architecture of the Actor Model. The final action is a = tanh(x ∼ N(µ,σ)),
andthelogprobabilityofthatactionislog(N(x|µ,σ2))−log(1−a2 +10−6).
52
Critic
Portion LayerType Details
ImageIn Convolution channelsin=4,channelsout=16,
kernel size=(3,3),stride=(1,1),
padding=(1,1),padding mode=reflect
PReLU num parameters=1
AveragePooling kernel size=(3,3),stride=(2,2),padding=(1,1)
Convolution channelsin=16,channelsout=16,
kernel size=(3,3),stride=(1,1),
padding=(1,1),padding mode=reflect
PReLU num parameters=1
AveragePooling kernel size=(3,3),stride=(2,2),padding=(1,1)
Flatten
Linear in features=64,out features=32,bias=True
PReLU num parameters=1
SpeedIn Linear in features=1,out features=32,bias=True
PReLU num parameters=1
ActionIn Linear in features=2,out features=32,bias=True
PReLU num parameters=1
Table7: ArchitectureoftheCriticModel(part1). Notpictured: concatenationofImage
In,SpeedIn,andActionInforGRUinput.
53
Critic(continued)
Portion LayerType Details
GRU GatedRNN inputsize=96,hiddensize=32
QOut Linear in features=32,out features=32,bias=True
PReLU num parameters=1
Linear in features=32,out features=1,bias=True
Table8: ArchitectureoftheCriticModel(part2).
54

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Intrinsic Rewards for Exploration without Harm from Observational Noise: A Simulation Study Based on the Free Energy Principle"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
