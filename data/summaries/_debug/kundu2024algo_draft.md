### OverviewThis paper investigates object-grounded visual commonsense reasoning for open-world egocentric action recognition, introducing the ALGO framework. The authors state: "egocentric activity understanding. We define an activity as a complex structure whose semantics are expressed by a combination of actions (verbs) and objects (nouns)." They note: "In an open world, this target search space can be unknown or exceptionally large, which severely restricts the performance of such models." The paper argues: "Learning to infer labels in an open world, i.e., in an environment where the target ‘labels’ are unknown, is an important characteristic for achieving autonomy." According to the research, "The goal is to recognize elementary concepts and infer the activity." The study demonstrates: "We propose a neuro-symbolic framework that leverages advances in multi-modal foundation models to ground concepts from symbolic knowledge bases, such as ConceptNet [26], in visual data." The authors state: "The overall approach is illustrated in Figure1."### MethodologyThe ALGO framework consists of two steps. First, the authors propose a neuro-symbolic prompting approach that uses object-centric vision-language models as a noise or oracle to ground objects in the video through evidence-based reasoning. They note: "To enhance object recognition, we propose a neuro-symbolic mechanism that leverages compositional properties of ConceptNet to compute the likelihood of an object’s presence." Second, driven by prior commonsense knowledge, they discover plausible activities through an energy-based symbolic pattern theory framework and learn to ground knowledge-based action (verb) concepts in the video. They state: "We assign an energy term to each label using configurations composed of generators connected by affinity-based bonds." The authors explain: "Each configuration includes a grounded object generator go and an action generator ga, structured by a graph derived from ConceptNet." They add: "The energy of a configuration c is expressed as: E(c)=ϕ(p(go|g¯o,I ,K ))+ϕ(p(ga,go|K ))+ϕ(p(ga|I ))". The paper argues: "After filtering for compositional properties, the path with the maximum weight is chosen."### ResultsThe authors evaluate the approach on GTEA Gaze, GTEA Gaze Plus, and EPIC-Kitchens-100 datasets. They report: "We compare against both closed-world learning and open-world setup (KGL)[4]." The study demonstrates: "We significantly outperform KGL (8.04% and6.73%) when measured on GTEA Gaze Plus." The paper argues: "We achieve an object recognition performance of13.07% on Gaze and26.23% on Gaze Plus." The authors state: "We report the results in Table1." The study demonstrates: "The authors note: "We achieve a top-1 action recognition performance of10.21% on Gaze and6.76% on Gaze Plus." The paper argues: "We achieve an object recognition performance of10.21% on Gaze and6.76% on Gaze Plus." The study demonstrates: "We report the results in Table2." The authors state: "We achieve a top-1 action recognition performance of10.21% on Gaze and6.76% on Gaze Plus."### Discussion, Limitations, and Future WorkThe authors state: "In this work, we proposed ALGO, a neuro-symbolic framework for open-world egocentric activity recognition that aims to learn novel action and activity classes without explicit supervision." They note: "While showing competitive performance, there are two key limitations: (i) it is restricted to ego-centric videos due to the need to navigate clutter by using human attention as a contextual cue for object grounding, and (ii) it requires a knowledge base such as ConceptNet to learn associations between actions and objects." The paper argues: "We plan to explore attention-based mechanisms [1,20] to extend the framework to third-person videos and use abductive reasoning [3,32] with neural knowledgebase completion models [6] to integrate visual commonsense into the reasoning."