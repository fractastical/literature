=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Structural Plasticity as Active Inference: A Biologically-Inspired Architecture for Homeostatic Control
Citation Key: hill2025structural
Authors: Brennen A. Hill

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Key Terms: madison, biologically, control, structural, cells, architecture, plasticity, structurally, homeostatic, levin

=== FULL PAPER TEXT ===

Structural Plasticity as Active Inference: A Biologically-Inspired Architecture for
Homeostatic Control
BrennenHill
UniversityofWisconsin-Madison
Madison,WI,USA
bahill4@wisc.edu
Abstract in turn, is the process of sampling the environment to
makesensationsconformtopredictions.
Traditionalneuralnetworks,whilepowerful,relyonbiolog-
2. BiologicalMorphogenesisandSomaticComputation:
icallyimplausiblelearningmechanismssuchasglobalback-
propagation. This paper introduces the Structurally Adap- MichaelLevinpositsthatmorphogenesisitselfisaform
tivePredictiveInferenceNetwork(SAPIN),anovelcompu- of collective intelligence (Levin 2019). In this view, so-
tationalmodelinspiredbytheprinciplesofactiveinference matic (non-neural) cells form bioelectric networks that
andthemorphologicalplasticityobservedinbiologicalneu- storeandprocessinformation,enablingthemtoworkto-
ralcultures.SAPINoperatesona2Dgridwhereprocessing wards anatomical goals like organ construction and re-
units, or cells, learn by minimizing local prediction errors. generation (Levin 2021). This suggests that structural
Themodelfeaturestwoprimary,concurrentlearningmech-
plasticityisafundamental,scale-invariantcomponentof
anisms: a local, Hebbian-like synaptic plasticity rule based biological problem-solving. The concept of embodied,
onthetemporaldifferencebetweenacell’sactualactivation
structuraladaptationisfurtherextendedbyCorticalLabs
anditslearnedexpectation,andastructuralplasticitymech-
(Kagan et al. 2022), which has demonstrated that dis-
anism where cells physically migrate across the grid to op-
timizetheirinformation-receptivefields.Thisdualapproach
sociatedneuralculturescanphysicallyreorganizethem-
allowsthenetworktolearnbothhowtoprocessinformation selves.
(synaptic weights) and also where to position its computa-
The Structurally Adaptive Predictive Inference Network
tionalresources(networktopology).WevalidatedtheSAPIN
(SAPIN)isproposedasacomputationalmodeltobridgethis
modelontheclassicCartPolereinforcementlearningbench-
gap.Wehypothesizethatbyendowingcomputationalagents
mark.Ourresultsdemonstratethatthearchitecturecansuc-
withtheabilitytophysicallymoveandrepositiontheircells,
cessfully solve the CartPole task, achieving robust perfor-
mance.Thenetwork’sintrinsicdrivetominimizeprediction theycanactivelystructuretheirowninputstreams,leading
error and maintain homeostasis was sufficient to discover a tomoreefficientlearning.
stable balancing policy. We also found that while continual The SAPIN architecture integrates two distinct learning
learning led to instability, locking the network’s parameters mechanismsoperatinginparallel:
afterachievingsuccessresultedinastablepolicy.Wheneval-
1. SynapticPlasticity:Alocal,Hebbian-likelearningrule
uatedfor100episodespost-locking(repeatedover100suc-
updates a cell’s directional connection strengths (s) and
cessful agents), the locked networks maintained an average
its homeostatic activation expectation (E). This update
82%successrate.
is driven entirely by the local prediction error, the dif-
ference between the cell’s actual activation (V) and its
Introduction
learnedexpectation(E).
Acentralchallengeinunderstandingintelligenceisexplain- 2. StructuralPlasticity:Anovelmovementmechanismal-
ing how stable, adaptive behavior emerges from systems lowsprocessingcellstophysicallymigrateacrossthe2D
that are fundamentally self-organizing rather than explic- grid.Thismovementisdrivenbythelong-termaverage
itlyprogrammed.Biologicalorganismsmaintaininternalor- predictionerror,ordesire.Acellthatischronicallyover-
derbycontinuouslypredictingandcounteractingdeviations orunder-activatedrelativetoitsexpectationwillmoveto
from expected sensory states. This research draws inspira- anewlocationtofindamorepredictableposition.
tionfromtwokeyareasoftheoreticalandexperimentalneu-
We test this architecture on the Cart Pole balancing
roscience:
task(Barto,Sutton,andAnderson1983).Thisenvironment
1. The Free Energy Principle & Active Inference: This serves as a canonical benchmark for homeostasis, the goal
framework posits that biological agents act to minimize istomaintainastablestate(anuprightpole)againstpertur-
aformofpredictionerrororsurprise(variationalfreeen- bations.OurfindingsdemonstratethatSAPINcansolvethis
ergy) (Friston 2010; Parr and Friston 2019). Perception task.Additionally,weshowthatthenetworklearnstomain-
andlearningarecastasprocessesofupdatinganinternal tainhomeostasiswithoutanyexternalrewardorpunishment
generativemodeltobetterpredictsensoryinputs.Action, signal.Theintrinsicdrivetominimizelocalpredictionerrors
5202
ceD
9
]EN.sc[
3v14220.1152:viXra
issufficient,providingacomputationalproof-of-conceptfor down predictions about the activity in lower levels. These
active inference as a self-sufficient driver for this class of predictionsarethencomparedwiththeactualbottom-upsig-
controlproblems. nals.Thediscrepancybetweenthepredictionandthesignal
constitutesapredictionerror.Thiserroristheonlyinforma-
TheoreticalFoundationsandRelatedWork tion that is propagated up the hierarchy, where it is used to
update the beliefs (or neural representations) at the higher
TheBrainasaGenerativeModel:Predictive
level to produce a better prediction. This recursive process
InferenceasaUnifyingPrinciple
continuesuntilpredictionerrorisminimized,atwhichpoint
The pursuit of designing more advanced artificial general thesystemhassettledonthemostlikelyexplanationforits
intelligence has increasingly led researchers to re-examine sensoryinput.Thismessage-passingschememapswithre-
the foundational principles of computation in the one sys- markable fidelity onto the known anatomy and physiology
tem known to possess it: the biological brain. A powerful of the neocortex, often referred to as the canonical micro-
and unifying perspective emerging from theoretical neuro- circuit(Isomura,Shimazaki,andFriston2022).Mathemat-
scienceisthatthebrainisanactive,generativeinferenceen- ically, the dynamics of a PC network can be shown to be
gineratherthanapassiveinformationprocessor.Thisview performing gradient descent on the variational free energy
posits that the brain’s primary function is to construct and (WhittingtonandBogacz2017).
maintain an internal model of the world, which it uses to
predictthecausesofitssensoryinputs. LearninginSilico:TheQuestforBiologically
AtthehighestlevelofabstractionistheFreeEnergyPrin- PlausibleCreditAssignment
ciple (FEP), a comprehensive theory of brain function and,
more broadly, of any self-organizing system. Proposed by While the predictive inference framework provides a com-
Karl Friston (Friston 2010), the FEP posits that for any bi- pellingaccountoftheobjectiveofbraincomputation,acrit-
ological agent to maintain its structural and functional in- ical question remains: how does the brain solve the credit
tegrityinaconstantlychangingandentropicworld,itmust assignment problem? The dominant algorithm in modern
minimizeaquantityknownasvariationalfreeenergy.Vari- deep learning, backpropagation, provides a powerful solu-
ationalfreeenergyisaninformation-theoreticquantitythat tionbutiswidelyconsideredbiologicallyimplausible(Lill-
servesasatractableupperboundonsurprise.Therefore,by icrapetal.2016;Zador2019).
minimizing free energy, an agent implicitly maximizes the Theprimaryobjectionscenteronitsviolationofthecon-
evidenceforitsownexistence,aprocessFristontermsself- straints of local computation in neural circuits. The weight
evidencing(ButzandKutter2017). transport problem requires that the error signal be prop-
Thissingleimperative,tominimizefreeenergy,provides agated backward via a feedback pathway whose synap-
aremarkablyunifiedaccountofbrainfunction,castingper- tic weights are precisely the transpose of the feedforward
ception,learning,andactionasdifferentfacetsofthesame weights(Lillicrapetal.2016;Akroutetal.2019).Thereis
underlyingprocess:approximateBayesianinference(Smith, noknownbiologicalmechanismthatcouldensuresuchper-
Friston, and Whyte 2022). Perception becomes the process fectsymmetry.Furthermore,theupdateruleforasynapsein
ofupdatingtheagent’sbeliefsaboutthehiddencausesofits a deep layer requires access to an error signal that is com-
sensationstominimizethediscrepancybetweenwhatispre- putedattheoutputlayer,violatingtheprinciplethatsynaptic
dicted and what is sensed (i.e., prediction error). Learning plasticity should depend only on locally available informa-
is the process of updating the parameters of the generative tion.
modelitselftoprovidebetterlong-termpredictions. The foundational constraint for any biologically realistic
The enactive corollary of the FEP is Active Inference, learningruleistheLocalLearningPrinciple.Thisprinciple
which extends this principle to action (Parr and Friston statesthatthechangeinasynapticweightcanonlydepend
2019). Under active inference, actions are selected to min- on variables that are available locally at that synapse, both
imize expected future free energy rather than to maximize inspaceandtime.ThearchetypallocallearningruleisHeb-
anexternalrewardsignal.Thismeansanagentwillactively bianplasticity,cellsthatfiretogether,wiretogether(Legen-
sample its environment to make sensations conform to its stein,Pecevski,andMaass2008).Initsmodernform,Spike-
predictions,therebyminimizingfuturesurprise.Thisformu- Timing-DependentPlasticity(STDP),thechangeinsynap-
lationelegantlyunifiestwofundamentaldriversofbehavior: tic strength depends on the precise relative timing of pre-
the drive to seek out preferred, goal-related states and the andpost-synapticspikes(Izhikevich2006).
drivetoreduceuncertaintyabouttheworld(e.g.,epistemic Tobridgethegaptogoal-directedlearning,neuroscience
foragingorexploration). has identified a broader class of neo-Hebbian three-factor
If the FEP and Active Inference describe the objective learning rules (Legenstein, Pecevski, and Maass 2008).
function that the brain optimizes, then Predictive Coding These rules augment the two local factors (pre- and post-
(PC)istheleadingcandidateforthealgorithmthebrainuses synaptic activity) with a third, globally broadcast signal,
to perform this optimization (Rao and Ballard 1999; Mil- typicallyaneuromodulatorlikedopamine.Thisthirdfactor
lidge et al. 2022). The core idea of PC is that the brain’s conveysinformationabouttheoutcomeofbehaviorsuchas
cortical hierarchy is engaged in a continuous, bidirectional reward,punishment,novelty,orsurpriseandactsasagate,
exchange of signals to minimize prediction error (Rao and modulating the Hebbian plasticity. This mechanism allows
Ballard 1999). Higher levels of the hierarchy generate top- the network to solve the temporal credit assignment prob-
lem, linking past neural activity to delayed behavioral out- powerfullydemonstratedthatmorphogenesis,theprocessby
comes. which an organism develops its shape, is itself a form of
Several algorithms, such as Feedback Alignment (FA) computation executed by a collective intelligence of non-
(Lillicrapetal.2016),ReciprocalFeedback(RF),andEqui- neuralsomaticcells(Levin2019).Allcellsinthebodycom-
librium Propagation (EP) (Scellier and Bengio 2017), have municate via bioelectric networks, using ion channels and
beenproposedtoapproximatethegradient-basedlearningof gap junctions to form a cognitive glue that stores a mem-
backpropagation using only local rules. These approaches, ory of the organism’s target morphology. This bioelectric
alongwithPredictiveCoding(Millidgeetal.2022),demon- softwareguidesthedeploymentofthegenomichardwareto
stratethateffectivecreditassignmentispossiblewithoutvi- achieve complex anatomical goals, such as constructing an
olatingbiologicalconstraints. eyeorregeneratingalimb(Levin2021).Thisrevealsthatthe
optimizationofphysicalstructuretomeetfunctionalgoalsis
TheDynamicBlueprint:StructuralPlasticityand ascale-invariantprincipleofbiologicalintelligence,provid-
TopographicSelf-Organization ing a deep motivation for exploring structural plasticity in
Thecomputationalprinciplesofpredictiveinferenceandlo- computationalmodelslikeSAPIN.
callearningaddressthefunctionalaspectsofbraincomputa-
EmbodiedInference:FromTheorytoAction
tion.However,theyoperateonaphysicalsubstratethatisfar
fromstatic.TheSpatiallyEmbeddednatureoftheSAPINar- Intelligence is an embodied and enactive process that un-
chitectureismotivatedbytheunderstandingthatthebrain’s folds through an agent’s interaction with its environment.
physicalstructureisacrucialanddynamiccomponentofits ActiveInferenceoffersafundamentallydifferentandmore
computationalpower. integratedperspectiveoncontrolthantraditionalReinforce-
The traditional view of the adult brain as a fixed, hard- ment Learning (RL) (Parr and Friston 2019; Smith, Fris-
wiredmachinehasbeenoverturned.Thebrainexhibitsare- ton, and Whyte 2022). An active inference agent seeks to
markable capacity for structural reorganization throughout minimize its Expected Free Energy (EFE). This EFE ob-
its entire lifespan (Butz, van Ooyen, and Wo¨rgo¨tter 2020). jectivecanbedecomposedintotwocomponents:aninstru-
Thislifelongstructuralplasticitymanifestsinseveralforms, mental value (exploitation, or seeking preferred outcomes)
including dendritic and axonal remodeling, synaptogenesis and an epistemic value (exploration, or reducing uncer-
andpruning,andevenadultneurogenesisinspecificregions. tainty).Bycastingactionselectionasinference,activeinfer-
Thesestructuralchangesareinfluencedbyanorganism’sex- ence provides a first-principles solution to the exploration-
periences.Thissuggeststhatthebrain’sarchitectureisitself exploitationdilemma.
aformofmemory,adynamicblueprintthatiscontinuously While computational models provide compelling proof-
optimized to meet the computational demands of the envi- of-concept,apowerfulvalidationoftheFreeEnergyPrinci-
ronment. pleasabasisforlearningcomesfromtheDishBrainexper-
Inspiredbythesefindings,computationalmodelsofstruc- iment by Cortical Labs (Kagan et al. 2022). This provided
turalplasticityhavebeendeveloped(Butz,vanOoyen,and theademonstrationthatacultureoflivingneuronsinadish
Wo¨rgo¨tter 2020; Galluppi, Lagorce, and Benosman 2015). could learn to perform a goal-directed task (playing Pong)
Activity-dependentruleslinktheformationandelimination inasimulatedenvironment.
of synapses to the neural activity they support, often via a Thelearningmechanismdidnotinvolveanyconventional
useitorloseitprinciple.Homeostaticrulesaimtomaintain rewardsignal.Instead,itwasbasedentirelyontheprinciple
astablelevelofoverallactivity,drivinganeurontocreateor ofsurpriseminimization.Whentheculturesuccessfullyhit
retractconnectionstorestoreitsfiringratetoahomeostatic the ball, it received a simple, predictable electrical stimu-
set-point(Butz,vanOoyen,andWo¨rgo¨tter2020).Incorpo- lus. When it missed, it received a prolonged, chaotic, and
ratingthesedynamicrewiringmechanismshasbeenshown unpredictablestimulus.InpreciseaccordancewiththeFEP
toincreaselearningspeedandmemorystoragecapacity. (Kagan et al. 2022), the neural culture spontaneously orga-
Beyond dynamic rewiring, another key structural feature nizeditsactivitytomakeitssensoryworldmorepredictable
ofthebrainisitsspatialorganizationintotopographicmaps, bylearningtoreturntheball,therebyavoidingthesurprising
wherethephysicalarrangementofneuronsreflectsthestruc- chaoticfeedback.Thisprovidespowerful,tangibleevidence
tureofthesensoryinput. thattheFEPisareal,physicalprinciplethatcandrivelearn-
The classic computational model for this is the Self- ingandself-organizationinbiologicalneuraltissue.
OrganizingMap(SOM),orKohonenmap(Kohonen1982), Thisprincipleofembodied,goal-directedbehaviordriven
anunsupervisednetworkthatusescompetitiveandcooper- by local rules also exists outside of neural tissue. Further
ativelearningtocreateafeaturemapthatreflectsthetopol- evidence comes from developmental bioelectricity, where
ogyofthedataspace.Recentbreakthroughs,suchasCredit- cellularcollectivessolvecomplexanatomicalproblems.For
Based Self-Organizing Maps (CB-SOMs) (Flesch et al. example,bymanipulatingtheendogenousbioelectricgradi-
2022), have successfully integrated this principle with top- ents in planarian flatworms, the memory of the organism’s
down, error-driven learning, demonstrating that the brain’s target morphology can be permanently rewritten, inducing
spatial organization is itself shaped by the same error- a fragment to regenerate into a stable, two-headed worm
minimizingimperativesthatdrivelearning. (Levin2021).Similarly,thecreationofXenobots,novel,liv-
Thisprincipleofadynamicphysicalblueprintextendsfar ing machines built from frog skin and heart cells, demon-
beyondthenervoussystem.TheworkofMichaelLevinhas strates that somatic cells, when removed from their nor-
malcontext,canself-organizetoperformnewfunctionslike
locomotion and collective object manipulation (Kriegman
etal.2020).Theseexamplesunderscorethatthefundamen-
tal principles of embodied agency and collective problem-
solvingaredeeplyconservedacrossdiversebiologicalsub-
strates, justifying their exploration in non-traditional com-
putationalarchitectures.
SAPIN is situated at the confluence of these research
streams, synthesizing predictive inference, local learning,
and dynamic structural plasticity into a single, unified ar-
chitecture.
ModelArchitecture
SystemOverview
TheSAPINmodelisinstantiatedona2Dgridofdimensions
9×9.Thesystemconsistsofthreedistinctcellpopulations:
• InputCells(I):Afixedsetof4cells. Figure1:GridSetup.TheSAPIN9x9gridarchitecture.The
4InputCells(blue)arefixed,asarethe2OutputCells(or-
• ProcessingCells(P):Adynamicpopulationof30cells.
ange).The30ProcessingCells(grey)arerandomlyinitial-
• OutputCells(O):Afixedsetof2cells.
izedandmigrateviastructuralplasticity.Thetop-rightcor-
Mostnumericalvaluesusedthroughoutthesystemincluding nerdepictsthecurrentstateofthecartpoleenvironment
inputs,activationsandweightsareconstrainedtotherange
[−1,1].
– AccumulatedTotalInflux(Vsaved):Ascalarthatac-
SpatialConfiguration i
cumulates the total activation V from each propaga-
i
Theplacementofinputandoutputcellsisfixed,whilepro- tionwave.
cessingcellsaredynamic.
• ImmediateState:Thesevariablesrepresentthecell’sac-
• InputLocations:The4inputcellsarefixedintheleft- tivationduringasinglepropagationwaveandareresetat
mostcolumnatcoordinates:(0,1),(0,3),(0,5),and(0,7). everyenvironmenttimestep.
• Output Locations: The 2 output cells are fixed in the
– Directional Value (v ): A 4-element vector accumu-
i
rightmostcolumnatcoordinates:(8,2)and(8,6).
latinginfluencefromthefourcardinaldirectionsdur-
• Processing Locations: The 30 processing cells are as- ingthecurrentwave.
signed random, unoccupied (x,y) coordinates upon ini-
– TotalValue(V ):Thecell’stotalactivationforthecur-
i
tialization.
renttimestep.
CellStateRepresentation Output cells have a fixed, uniform directional strength
s =[0.25,0.25,0.25,0.25].
Eachprocessingcelli∈P maintainsthreesetsofvariables. i
Inputandoutputcellsonlymaintainanimmediatestate.
NetworkDynamicsandAdaptation
• Long-TermMemory(LTM):Thesevariablesrepresent
SignalPropagationandActionSelection
thecell’slearnedknowledgeandareupdatedbysynaptic
plasticity. At each environment timestep, a wave of activation propa-
– Directional Strengths (s ): A 4-element vector s = gatesthroughthenetworktodetermineanaction.Thispro-
i i
[s ,s ,s ,s ] representing the cell’s learned cessisdetailedinAlgorithm1.
up right down left
connectivitybiasineachcardinaldirection.Initialized The propagation mechanism was designed to support a
withrandomvaluesin[−1,1]. spatially flexible architecture. Unlike a spiking neural net-
– Expectation (E ): A scalar value representing the work,whichusesafixedactivationthreshold,orastandard
i
cell’shomeostaticbaseline,oritspredictedactivation ANN,whichusesfixeddirectionalconnections,SAPINre-
level.Initializedwitharandomvaluein[−1,1]. quiresamoredynamicapproach.Becausecellsmustbefree
tomove(structuralplasticity)andthelearningruleisbased
• Short-TermMemory(STM):Thesevariablesaccumu-
oncomparingtotalactivationV toalearnedexpectationE ,
i i
lateinformationoveramacro-episode(definedas4full
afixed”spikingthreshold”isnotsuitable.Therefore,weuse
environment episodes) and are used to drive structural
a”winner-takes-all”propagationorder:theun-activatedcell
plasticity.
withthehighestabsoluteactivation|V |isthenexttoprop-
j
– Accumulated Directional Influx (vsaved): A 4- agateitssignal.Thisallowsinformationtoflowthroughthe
i
elementvectorthataccumulatesthedirectionalinputs gridinadata-driven,non-serialmannerwithoutpre-defined
v fromeachpropagationwave. layersorstaticconnections.
i
Algorithm1:SignalPropagationWave
1: Input:NormalizedstatevectorS =[s 0 ,s 1 ,s 2 ,s 3 ]
2: Initialize:
3: Resetv i ,V i ←0foralli∈P ∪O
4: A←∅(ActiveSet)
5: U ←P ∪O(AvailableSet)
6: forj ←0to3do ▷SeedInputCells
7: Letibethej-thinputcellinI
8: V i ←S j
9: AdditoA
10: endfor
11: whileU isnotemptydo
12: C ←∅(ContributionMap)
13: foreachsenders∈Ado
14: if|V s |=0thencontinue
15: endif
16: foreachreceiverr ∈U do
Figure2:SignalPropagation(Update).Informationprop-
17: d←ManhattanDistance(s,r)
agates from the input cells (left) to the output cells (right).
Thenextcelltopropagateistheonewiththehighestabso- 18: D d ←GetDistanceDecay(d)
lute activation |V|. Nonlinearity is introduced via tanh(V) 19: ifD d =0thencontinue
20: endif
and the trigonometric angular weighting. In this visual, the
21: W ,W ←GetAngularWeighting(s,r)
cell that is currently propagating information is shown in ang dir
pink. There are pink arrows pointing to the nearby cells 22: ∆V ←tanh(V s )·D d ·W ang
23: C[r]←C[r]+(∆V,∆V ·W )
which it is sending information to. The cells that have al- dir
24: endfor
readybeenactivatedarecoloredyellow.Thecellsthathave
25: endfor
not yet been activated are colored green. The right column
26: foreachreceiverr,(∆V ,∆v )inC do
depictsthecurrentstateofthecartpoleenvironmentabovea total total
listofthevalueofeachcell,withthevalueoftheinputcells 27: V r ←V r +∆V total
inblueandthatoftheoutputcellsinred.Belowthiscanbe 28: v r ←v r +∆v total
29: endfor
viewedtheexpressionforthecurrentequationtopropagate
30: ifU isemptythenbreak
valuefromonecelltothenearbycells.
31: endif
32: i next ←argmax j∈U |V j |
33: Movei fromU toA
The distance decay function D(d) is a discrete lookup next
34: endwhile
basedonManhattandistanced:
35: ActionSelection:
D(d)=
  1
0
.
.
0
75
i
i
f
f
d
d
=
=
0
1
3
3
6
7
:
:
L
re
e
t
t
u
o
r
0
n
,o
0
1
if
b
V
e
o
t
0
he
>
tw
V
o
o1
o
e
u
l
t
s
p
e
ut
1
cellsinO
0.25 ifd=2

0.0 ifd≥3
The angular weighting W interpolates the sender’s di- StructuralPlasticity(Movement)
ang
rectional strengths s . For an angle θ to the receiver,
s
the directional components (up, right, down, left) are Structural plasticity occurs after a macro-episode, which is
weightedbymax(0,−sinθ),max(0,cosθ),max(0,sinθ), definedas4fullenvironmentepisodes.Thisprocess(Algo-
andmax(0,−cosθ),respectively.W isthedotproductof rithm3)allowscellstophysicallyrelocate.Itisalsoskipped
ang
s andthesecalculatedweights. ifthenetworkisgloballylocked.
s
After the wave terminates, and if the network’s global The movement logic is driven by the same error-
lock is not engaged, the STM variables for all processing minimizingprincipleassynapticplasticity.Acell’s”desire”
cellsareupdated: tomoveisitslong-termaveragepredictionerror.Thedirec-
tion of movement is determined by the source of the unex-
vsaved ←vsaved+v (1)
i i i pectedinput.Acellmovesalongtheaxisfromwhichitre-
V i saved ←V i saved+V i (2) ceivedthemost”surprising”(highestmagnitude)averagein-
flux,ascapturedbytheweightedchoicebasedonv¯.Ifover-
i
SynapticPlasticity(LTMUpdate)
activated,itmovesawayfromthatsource;ifunder-activated,
After each action, the network updates its LTM (synaptic it moves toward it. Inspired by reinforcement learning, a
weights) based on the immediate activations. This process, small random chance (ϵ ) is included to encourage ex-
rand
detailedinAlgorithm2,isskippedifthenetworkisglobally ploration and prevent the network from getting ”stuck” in
locked.Thelearningratewassettoη =0.02. apoor,butstable,spatialconfiguration.
Algorithm2:SynapticPlasticity(LTMUpdate)
1: Input:Learningrateη =0.02
2: ifNetworkisgloballylockedthenreturn
3: endif
4: foreachcelli∈P do
5: error i ←V i −E i
6: ▷UpdateExpectation
7: E i ←E i +(η/2)·error i
8: ▷UpdateDirectionalStrengths
(cid:80)
9: v sum ← d∈dirs |v i,d |
10: ifv >10−6then
sum
11: p i ←v i /v sum ▷DirectionalProportions
12: s i ←s i +(η/2)·p i ·error i
13: s i ←clip(s i ,−1,1)
14: endif
15: endfor
Figure 3: Structural Plasticity (Movement). Cells with
high long-term prediction error (|V¯ −E |) move to a new
ExperimentalProtocol i i
location.Ifover-activated,acellmovesawayfromitsmain
EnvironmentandTask signal source; if under-activated, it moves toward it. The
blue arrows depict where a cell moves to. The red arrows
The SAPIN architecture was evaluated on the CartPole-v1
depict where a cell was unable to move due to a collision.
environment from the Gymnasium library (Towers et al.
Order of movement is determined by highest desire. The
2024).The4-dimensionalstatevector(cartposition,cartve-
right column depicts the current state of the cart pole en-
locity,poleangle,poleangularvelocity)wasnormalizedto
vironmentabovealistofthedesireofeachcelltomove.A
the range [−1,1] using the environment’s standard bounds
horizontallinecutsoffthelistofcellsbetweenthosewitha
(cart position ±2.4, cart velocity ±4.0, pole angle ±0.209
highenoughdesiretomoveandthosewithtoolowadesire
rad, pole angular velocity ±4.0). This normalized 4D vec-
tomove.
torwasmappeddirectlytothe4inputcells.Anepisodewas
consideredsuccessfuliftheagentbalancedthepolefor500
consecutivetimesteps.
However,thissuccesswasoftenunstable.Anetworkthat
PunishmentMechanism achieved500stepsmight,intheverynextepisode,failafter
only10steps.Thisisattributedtothefactthatthelearning
DrawinginspirationfromtheDishBrain’sexperiment’suse
rules (Algorithms 2 and 3) do not guarantee convergence.
of unpredictable stimuli (Kagan et al. 2022), we imple-
Thenetworkisconstantlyadapting,andagoodpolicycould
mentedapunishmentmechanismdesignedtoinjectsurprise
be forgotten as the network continues to explore its state
intothenetwork.Thiswasimplementedintwoways:
space. We also observed that poor random initial positions
1. Catastrophic Failure: Upon episode termination (pole for the processing cells could prevent the agent from suc-
falls), 10 epicenters were created at random (x,y) grid ceedingfor100episodesormore.
locations. Each epicenter emitted a random value p ∈
[−1,1]. TheLockingExperiment
2. Probabilistic Punishment: During non-terminal states, Toaddresstheinstability,weexperimentedwithlockingthe
if the pole angle was high (between 4 and 12 degrees), network’s parameters. A global boolean flag was added to
there was a 1-10% chance (scaling with the angle) of a theSAPINmodel.Whentheagentfirstachievedascoreof
punishmenteventof1-30%ofthe10epicenters. 500 steps, this flag was set to True, permanently disabling
all LTM updates (Algorithm 2) and all structural plasticity
In both cases, the punishment values initiated a special
(Algorithm3).
propagationwave(Algorithm1),andtheresultingcellacti-
vationsV wereusedtodriveasynapticupdate(Algorithm To test the stability of a found policy, the network was
i
evaluated for 100 episodes immediately after locking. This
2).Thiswasintendedtoupdatethenetwork’sLTMtoavoid
process was repeated for 100 different successfully trained
thestatesthatledtothepunishment.
agents.Onaverage,thelockednetworksmaintainedan82%
ResultsandAnalysis success rate (i.e., 82 out of 100 episodes lasted the full
500 steps). This demonstrates that the SAPIN architecture
PerformanceandInstability
is fully capable of finding and storing a robust, generaliz-
The SAPIN network proved highly successful at solving able policy for the Cart Pole task. This ’locking’ mecha-
the Cart Pole environment. It frequently achieved success nism serves as a computational analogue to synaptic con-
(500steps)withinthefirst10episodes.Theagent’sbehav- solidation,aprocesswherenewmemoriesarestabilizedby
iordemonstratedclearcorrectiveactionstobalancethepole. a reduction in plasticity. This finding suggests a potential
Algorithm3:StructuralPlasticity(Movement)
1: Input:Mindesireθ D =0.1,ϵ rand =0.05
2: Initialize:N ←stepsinmacro-episode
steps
3: ifNetworkisgloballylockedorN =0then
steps
4: ResetallSTM(vsaved,Vsaved ←0)
5: return
6: endif
7: CalculateDesire:
8: M←∅(MovementCandidates)
9: foreachcelli∈P do
10: V¯ i ←V i saved/N steps
11: v¯ i ←v i saved/N steps
12: D i ←|V¯ i −E i |
13: ifD i ≥θ D orrandom()<0.025then
14: Add(i,D i ,V¯ i ,v¯ i )toM
15: endif
16: endfor Figure4:ThePunishmentMechanism.Uponfailure,ran-
17: SortMbyD i descending dom epicenters (at empty or non-empty cells) generate
18: Occupied←setofallcellcoordinates chaotic signals that propagate through the network, driving
19: foreach(i,D i ,V¯ i ,v¯ i )inMdo asynapticupdateintendedtoassociatetheprecedingstates
20: ▷1.Choosemovementaxis withsurprise.Inthisvisual,redarrowsareshownfromeach
21: ifrandom()<ϵ rand then of the random epicenters and connect to the nearby cells.
22: dir←random direction() Thenapropagationwavehappenslikenormal.Thenextcell
23: else to propagate is the one with the farthest value form 0, like
(cid:80)
24: P ←|v¯ i |/ |v¯ i | normal.Thefirstcellisshowninpink.
25: dir←weighted choice(dirs,P)
26: endif
27: ▷2.Choosemovementvector(towards/away) tobalancethepoleevenwhenitwasneverpunishedforfail-
28: ifV¯ i >E i then ▷Over-activated ing.
29: move vec←−1·get vector(dir) This strongly implies that, for a homeostatic task like
30: else ▷Under-activated CartPole,theagent’sintrinsicdrivetominimizeitsownlo-
31: move vec←+1·get vector(dir) cal prediction errors is sufficient for learning. The network
32: endif learnstomaintainhomeostasis(keepthepolebalanced)be-
33: ▷3.AttemptMove causeabalancedpoleprovidesastable,andthereforehighly
34: (x new ,y new )←(x i ,y i )+move vec predictable,streamofsensoryinput.Droppingthepole,by
35: if(x new ,y new )isvalidandnotinOccupiedthen contrast,resultsinachaoticandunpredictablesensorystate.
36: Remove(x i ,y i )fromOccupied The network learns to seek the state of minimal prediction
37: Add(x new ,y new )toOccupied error,whichinthisenvironment,isthesuccessstate.
38: (x i ,y i )←(x new ,y new ) This finding suggests that the core mechanism of active
39: endif inference, minimizing surprise, can be a sufficient objec-
40: endfor tivefunctionforcertaincontroltasks,withoutanyneedfor
41: ResetallSTM(vsaved,Vsaved ←0) an externally defined reward or punishment signal (Friston
2010).
mechanismforresolvingthewell-knownstability-plasticity DiscussionandFutureWork
dilemma. The homeostasis-seeking nature of the agent is both a
strength and a weakness. It is perfectly suited for the Cart
TheSurprisingRoleofPunishment
Pole task, which is itself a problem of maintaining home-
A finding of this research relates to the punishment mech- ostasis. However, this raises questions about the model’s
anism described in Section . We ran a comparison of three abilitytosolvetasksthatrequirelong-termplanningorde-
experimentalconditions: liberatelymovingawayfromastablestatetoachieveamore
1. Punishmentoncatastrophicfailureonly. distantgoal.Thisrelianceonimmediatepredictionerrorisa
potential limitation, consistent with models that only mini-
2. Punishmentoncatastrophicfailureandprobabilisticpun-
mizeimmediatepredictionerror.Futureworkcouldaddress
ishmentduringpoorperformance.
thisbyimplementingdeepactiveinference,wheretheagent
3. Nopunishmentwhatsoever.
learns a temporal model to minimize expected future free
Countertoourinitialhypothesis,allthreeconditionspro- energy, enabling it to sacrifice short-term homeostasis for
ducedverysimilarresults.Thenetworksuccessfullylearned long-termgoals.
Ourattemptstouseapunishmentsignaltoguidetheagent References
hadnonoticeableeffect.Futureworkshouldinvestigatewhy
Akrout, M.; Wilson, C.; Humphreys, P.; Lillicrap, T.; and
thissignalwasineffective.Itmaybethatthelocal,homeo-
Tweed, D. 2019. Deep learning without weight transport.
static updates (Algorithm 2) are stronger than the updates
InAdvancesinNeuralInformationProcessingSystems,vol-
fromthepunishmentwave,orthattherandomnatureofthe
ume32.
punishmentsignalwastoonoisytoprovideausefullearning
Barto,A.G.;Sutton,R.S.;andAnderson,C.W.1983. Neu-
gradient. We also tested an alternative structural plasticity
ronlikeAdaptiveElementsThatCanSolveDifficultLearn-
rule where, instead of moving to reduce the error magni-
ingControlProblem. IEEETransactionsonSystems,Man,
tude,cellsmovedtoalocationwiththe*smallestvariation*
andCybernetics,SMC-13(5):834–846.
inerror.Thegoalwastoseekpredictability,evenifthemean
activationdidnotmatchtheexpectation.Thisapproachwas Butz,M.;vanOoyen,A.;andWo¨rgo¨tter,F.2020. Amodel
lesssuccessfulthanthedefaultrule,suggestingthatmatch- for activity-dependent structural plasticity. Frontiers in
ingahomeostaticset-pointisamoreeffectivedrive. SynapticNeuroscience,12:1.
An alternative to improving learning from bad initial
Butz,M.V.;andKutter,E.2017.Howthebrainmightwork:
stateswouldbetoimplementageneticalgorithmtoevolve A tutorial on the free-energy principle. Synthese, 194(1):
the optimal initial positions of the processing cells, which
179–206.
wouldthenbefine-tunedbytheplasticitymechanisms.
Flesch,T.;Balaguer,J.;Dekker,R.;andetal.2022. Credit-
Futureworkwillalsoincludeevaluatingthisnetworkon
Based Self-Organizing Maps. In International Conference
morecomplextasksthatrequiremulti-stepplanningtoeval-
onLearningRepresentations.
uatewhetherhomeostasiscanbemaintainedlong-termover
changingenvironments. Friston,K.2010. Thefree-energyprinciple:aunifiedbrain
Thestateforeachcelliscurrentlyrepresentedby5values: theory? NatureReviewsNeuroscience,11(2):127–138.
a single expectation and four directional strengths. Future Galluppi,F.;Lagorce,X.;andBenosman,R.2015. Aspik-
work could focus on giving the cell a larger look-up table ingnetworkmodelofstructuralplasticityforlearningvisual
withbinsconnectinginputstospecificvalues. representations. In2015InternationalJointConferenceon
Future work will extend this by implementing a contin- NeuralNetworks(IJCNN),1–8.IEEE.
uous rather than discrete system for SAPIN. This will de-
Isomura,T.;Shimazaki,H.;andFriston,K.J.2022. Canon-
creasethestepsizewhenthecellsmove,greatlystabalizing
ical neural circuits for predictive coding. Communications
themodel.
Biology,5(1):115.
Additionally, future work will explore more complex
Izhikevich,E.M.2006.Polychronization:computationwith
structures, such as configurations wrapped around a cylin-
spikes. NeuralComputation,18(2):245–282.
derratherthanonaflatgrid.
Oneoftheinterestingaspectsofbiologicalsystemsisthat Kagan,B.J.;Kitchen,A.C.;Tran,N.T.;andetal.2022. In
they are deformed by their own computation. We will in- vitroneuronslearnandexhibitsentiencewhenembodiedin
tegrate this changing structure into the environment more asimulatedgame-world. Neuron,110(21):3952–3969.e8.
closely. In the example of the cart pole environment, this Kohonen, T. 1982. Self-organized formation of topologi-
will be achieved by placing the cells onto the pole rather cally correct feature maps. Biological Cybernetics, 43(1):
than being separate. Thus, as the cells move, their location 59–69.
willdirectlyimpactthephysicsoftheenvironment.
Kriegman, S.; Blackiston, D.; Levin, M.; and Bongard, J.
2020. A scalable pipeline for designing reconfigurable or-
Conclusion
ganisms. ProceedingsoftheNationalAcademyofSciences,
We introduced SAPIN, a novel, biologically-inspired com- 117(4):1853–1859.
putationalarchitecturethatsynthesizestwoformsofplastic- Legenstein,R.;Pecevski,D.;andMaass,W.2008. Alearn-
ity:local,error-drivensynapticplasticityandglobal,desire- ing theory for reward-modulated spike-timing-dependent
driven structural plasticity. The model is grounded in the plasticity with application to bio-inspired reinforcement
principles of Active Inference and the Free Energy Princi- learning. PLoSComputationalBiology,4(10):e1000180.
ple.
Levin, M. 2019. The computational boundary of a ’self’:
We successfully demonstrated that this architecture can
developmental bioelectricity drives collective behavior and
solve the Cart Pole benchmark. The intrinsic objective to
basal cognition in somatic cell networks. Frontiers in Psy-
minimize local prediction error (i.e., to seek homeostasis)
chology,10:2688.
was a sufficient driver for discovering the correct policy.
Wealsoshowedthatwhilethenetwork’scontinualplasticity Levin,M.2021.Bioelectricsignaling:Reprogrammablecir-
createsinstability,alockedversionofasuccessfulnetwork cuits underlying embryogenesis, regeneration, and cancer.
providesastableandhighlyrobustpolicy.
Cell,184(8):1971–1989.
SAPIN serves as a computational proof-of-concept for Lillicrap, T.P.; Cownden,D.; Tweed, D.B.; and Akerman,
models that learn not only how to process information, but C.J.2016. Randomsynapticfeedbackweightssupporter-
wheretopositiontheircomputationalresources,grounding rorbackpropagationfordeeplearning. NatureCommunica-
abstractinferenceinadynamic,physicalsubstrate. tions,7(1):13276.
Millidge, B.; Salvatori, T.; Song, Y.; and Bogacz, R. 2022.
Predictive Coding: Towards a Future of Deep Learning be-
yondBackpropagation? arXivpreprintarXiv:2202.09467.
Parr, T.; and Friston, K. J. 2019. Generalised free energy
andactiveinference.BiologicalCybernetics,113(5-6):495–
513.
Rao, R. P.; and Ballard, D. H. 1999. Predictive coding in
the visual cortex: a functional interpretation of some extra-
classicalreceptive-fieldeffects. NatureNeuroscience,2(1):
79–87.
Scellier,B.;andBengio,Y.2017. EquilibriumPropagation:
BridgingtheGapBetweenDeepLearningandSpikingNeu-
ralNetworks.FrontiersinComputationalNeuroscience,11:
24.
Smith,R.;Friston,K.J.;andWhyte,C.J.2022. Astep-by-
steptutorialonactiveinferenceanditsapplicationtoempiri-
caldata.JournalofMathematicalPsychology,107:102632.
Towers,M.;Kwiatkowski,A.;Terry,J.K.;andetal.2024.
Gymnasium:AStandardInterfaceforReinforcementLearn-
ingEnvironments. arXivpreprintarXiv:2407.17032.
Whittington,J.C.;andBogacz,R.2017. Anapproximation
of the error backpropagation algorithm in a predictive cod-
ing network withlocal Hebbian synaptic plasticity. Neural
Computation,29(5):1229–1262.
Zador,A.M.2019. Acritiqueofpurelearningandwhatar-
tificialneuralnetworkscanlearnfromanimalbrains.Nature
Communications,10(1):3770.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Structural Plasticity as Active Inference: A Biologically-Inspired Architecture for Homeostatic Control"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
