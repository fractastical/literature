### OverviewThis paper investigates the concept of agency beyond traditional reward-maximization frameworks. It shows that any type of behaviour complying with physically sound assumptions about how macroscopic biological agents interact with the world canonically integrates exploration and exploitation in the sense of minimising risk and ambiguity about external states of the world. This description, known as active inference, refines the free energy principle, a popular descriptive framework for action and perception originating in neuroscience. The authors state: “The idea of agency as a process of minimising risk and ambiguity is a key component of active inference.” The paper argues that active inference provides a principled solution to the exploration-exploitation dilemma, englobing the principles of expected utility theory and Bayesian experimental design. The authors note: “Active inference provides a transparent recipe to simulate behaviour, by minimising risk and ambiguity with respect to an explicit generative world model.” This framework is universal in the sense that it is theoretically possible to rewrite any RL algorithm conforming to the descriptive assumptions of active inference as an active inference algorithm. The authors state: “Active inference can be used as a tool to uncover and compare the commitments and assumptions of more specific models of agency.”### MethodologyThe core methodology centres around active inference, a framework that models agency as a process of minimising risk and ambiguity. The framework is built upon the assumption that agents interact with their environment according to a stochastic process, where the agent and environment evolve together. The authors state: “An agent is precise when it responds deterministically to its environment, that is when h | s is a deterministic process.” The model is defined by a generative world model, which specifies the probability distribution over states of the world. The model is based on the free energy principle, which states that the goal of an agent is to minimise its free energy. The authors state: “The expected free energy functional of the agent’s predictions and preferences.” The model is implemented using a POMDP, which is a probabilistic model of a Markov decision process. The authors state: “Active inference provides a principled solution to the exploration-exploitation dilemma that usefully simulates biological agency.”### ResultsThe authors demonstrate that active inference can be used to simulate a wide range of behaviours, including those that involve exploration and exploitation. The authors state: “Active inference provides a transparent recipe to simulate behaviour by minimising risk and ambiguity.” The results show that active inference agents are able to learn to perform tasks that are difficult for traditional reinforcement learning agents. The authors state: “Active inference provides a principled solution to the exploration-exploitation dilemma that usefully simulates biological agency.” The authors also show that active inference can be used to compare different models of agency. The authors state: “Active inference can be used as a tool to uncover and compare the commitments and assumptions of more specific models of agency.”### DiscussionThis paper presents a compelling argument for the utility of active inference as a model of agency. The authors state: “Active inference provides a transparent recipe to simulate behaviour, by minimising risk and ambiguity.” The framework offers a principled approach to modelling behaviour that is both flexible and computationally tractable. The authors note: “Active inference provides a transparent recipe to simulate behaviour by minimising risk and ambiguity with respect to an explicit generative world model.” The paper highlights the importance of minimising risk and ambiguity (i.e., an expected free energy functional) under some prediction and preference models about the world.### AcknowledgementsThe authors are indebted to Alessandro Barp, Guilherme França, Karl Friston, Mark Girolami, Michael I. Jordan and Grigorios A. Pavliotis for helpful input on a preliminary version to this manuscript. The authors thank Joshua B. Tenenbaum and MIT’s Computational Cognitive Science group for interesting discussions that lead to some of the points discussed in this paper. LD is supported by the Fonds National de la Recherche, Luxembourg (Project code:13568875) and a G-Research grant. This publication is based on work partially supported by the EPSRC Centre for Doctoral Training in Mathematics of Random Systems: Analysis, Modelling and Simulation (EP/S023925/1). NS is funded by the Medical Research Council (MR/S502522/1) and2021–2022 Microsoft PhD Fellowship.4202naJ32]IA.sc[1v71921.1042:viXra1 IntroductionReinforcement learning (RL) is a collection of methods that describe and simulate agency—how to map situations to actions—traditionally framed as optimising a numerical reward signal [1]. The idea of maximising reward as underpinning agency is ubiquitous: with roots in utilitarianism [2] and expected utility theory [3], it also underwrites game theory [3], statistical decision-theory [4], optimal control theory [5,6], and much of modern economics. From its inception, RL practitioners have supplemented reward-seeking algorithms with various heuristics or biases geared towards simulating intelligent behaviour. Especially effective are intrinsic motivation or curiosity-driven reward signals that encourage exploration [7–9]. Thus, we ask: is there a canonical way to think of agency beyond reward maximisation?In this paper, we show that any behaviour complying with physically sound assumptions about how macroscopic biological agents interact with the world canonically integrates exploration and exploitation in the sense of minimising risk and ambiguity about external states of the world. This description, known as active inference, refines the free energy principle, a popular descriptive framework for action and perception birthed in neuroscience [10–12].Active inference provides a generic framework to simulate and model agency that is widely used in neuroscience [13–17], RL [18–21] and robotics [22–25]. The usefulness of active inference for RL is three-fold. a) Active inference provides an effective solution to the exploration-exploration dilemma that englobes the principles of expected utility theory [3] and Bayesian experimental design [26] and finesses the need for ad-hoc exploration bonuses in the reward function or decision-making objective. b) Active inference provides a transparent recipe to simulate behaviour by minimising risk and ambiguity with respect to an explicit generative world model. This enables safe and explainable decision-making by specifically encoding the commitments and goals of the agent in the world model [23]. c) Active inference is universal in the sense that it is theoretically possible to rewrite any RL algorithm conforming to the descriptive assumptions of active inference as an active inference algorithm, e.g., [27]. Thus, active inference can be used as a tool to uncover and compare the commitments and assumptions of more specific models of agency.2 Deriving agency from physicsWe describe systems that comprise an agent interacting with its environment. We assume that an agent and its environment evolve together according to a stochastic process x. This definition entails a notion of time T, which may be discrete or continuous, and a state space X, which should be a measure space (e.g., discrete space, manifold, etc.). Recall that a stochastic process x is a time-indexed collection of random variables x on state space X. Equivalently, x is a random variable over trajectories on the state space T → X. We denote by P the probability density of x on the space of trajectories T →X (with respect to an implicit base measure).In more detail, we factorise the state space X into states that belong to the agent H and states external to the agent S that belong to the environment. Furthermore, we factorise agent’s states into autonomous states A and observable o, respectively defined as the states which the agent does and does not have agency over. In summary, the system x is formed of external s and agent processes h, the latter which is formed of observable o, and autonomous a processesX≡S×H≡S×O×A =⇒ x≡(s,h)≡(s,o,a).The description adopted so far could aptly describe particles interacting with a heat bath [28–30] as well as humans interacting with their environment (Figure1.A). We would like a description of macroscopic biological systems; so what distinguishes people from small particles? A clear distinction is that human behaviour occurs at the macroscopic level and, thus, is subject to classical mechanics. In other words, people are precise agents:Definition2.1. An agent is precise when it responds deterministically to its environment, that is when h | s is a deterministic process.Remark2.2. It is important not to conflate an agent’s mental representations of external reality with external states; the former are usually an abstraction or coarse-grained representation of the latter. We do not consider agent’s representations in this paper. We simply posit that there exists a detailed enough description of the environment that determines the agent’s trajectory (e.g., observations and actions). This will always be true for agents evolving according to classical mechanics.Agency means being in control of one’s actions and using them to influence the environment [31]. At time, the information available to the agent is its past trajectory h = (o ,a ), i.e. its history. We define decision-making as a choice of autonomous trajectory in the future a given available knowledge h . Furthermore, we define agency as the process through which an agent makes and executes decisions. We interpret P(s,o|h )as expressing the agent’s preferences over environmental and observable trajectories given available data, and P(s,o|a,h ) as expressing the agent’s predictions over environmental and observable paths given a decision (e.g., Figure1.B