=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Goal-directed Planning and Goal Understanding by Active Inference: Evaluation Through Simulated and Physical Robot Experiments
Citation Key: matsumoto2022goaldirected
Authors: Takazumi Matsumoto, Wataru Ohata, Fabien C. Y. Benureau

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2022

Abstract: Weshowthatgoal-directedactionplanningandgenerationinateleologicalframeworkcanbe
formulatedusingthefreeenergyprinciple. Theproposedmodel,whichisbuiltonavariational
recurrent neural network model, is characterized by three essential features. These are that (1)
goalscanbespecifiedforbothstaticsensorystates,e.g.,forgoalimagestobereachedanddynamic
processes, e.g., formovingaroundanobject, (2)themodelcannotonlygenerategoal-directed
actionplans,butcanalsounderstandgoalsbysensoryobservation,and(3)themo...

Key Terms: understanding, planning, physical, robot, experiments, directed, inference, active, goal, evaluation

=== FULL PAPER TEXT ===

Goal-directed Planning and Goal Understanding by
Active Inference: Evaluation Through Simulated and
Physical Robot Experiments
∗
TakazumiMatsumoto,WataruOhata,FabienC.Y.BenureauandJunTani
1OkinawaInstituteofScienceandTechnology,904-045,Japan
∗Correspondingauthor:jun.tani@oist.jp
Abstract
Weshowthatgoal-directedactionplanningandgenerationinateleologicalframeworkcanbe
formulatedusingthefreeenergyprinciple. Theproposedmodel,whichisbuiltonavariational
recurrent neural network model, is characterized by three essential features. These are that (1)
goalscanbespecifiedforbothstaticsensorystates,e.g.,forgoalimagestobereachedanddynamic
processes, e.g., formovingaroundanobject, (2)themodelcannotonlygenerategoal-directed
actionplans,butcanalsounderstandgoalsbysensoryobservation,and(3)themodelgenerates
futureactionplansforgivengoalsbasedonthebestestimateofthecurrentstate,inferredusingpast
sensoryobservations. Theproposedmodelisevaluatedbyconductingexperimentsonasimulated
mobileagentaswellasonarealhumanoidrobotperformingobjectmanipulation.
Keywords: activeinference;teleology;goal-directedactionplanning
1 Introduction
Instudyingcontemporarymodelsofgoal-directedactionsofbiologicalandartificialagents,itis
worthwhiletoconsiderthesemodelsfromtheperspectiveofateleologicalframework. Teleologyis
aphilosophicalideathatoriginatedinthedaysofPlatoandAristotle. Itholdsthatphenomena
appearnotbytheircauses,butbytheirendresults. Whileteleologyiscontroversialandlargely
abandonedasameansofexplainingphysicalphenomena,theideahasbeenextendedformodeling
actiongeneration. Ateleologicalaccountexplainsactionsbyspecifyingthestateofaffairsorthe
eventtowardwhichtheactionsaredirected[1,2]. Moresimply,actionsaregeneratedtoachieve
purposes or goals. In psychology, Csibra et al. [3] proposes that a goal-directed action can be
explained by a well-formed teleological representation consisting of 3 elements: (1) a goal, (2)
actionsintendedtoachievethegoal,and(3)constraints,whicharephysicalconditionsthatimpose
limits on possible actions. In a well-formed teleological representation of an action, the action
shouldbeseenasaneffectivemeanstoachievethegoalwithintheconstraintofreality.Thisaccount
predictsthatagentswhorepresentgoal-directedactionsinthiswayshouldbeabletoinferunseen
1
2202
beF
12
]OR.sc[
1v67990.2022:viXra
goalsorunseenrealityconstraints,giventhetworemainingelements. Csibraetal.[3]verifiedthis
hypothesisbyshowingthatevenyear-oldinfantscanperformsuchinferencesintheirexperiments.
Brain-inspiredmodelsforgoal-directedactionhavebeendevelopedinvariousforms. Most
brain-inspiredmodelsarebasedonforwardmodels[4–6]forpredictingsensoryoutcomesofan
actiontobeexecuted. Forgoalsgivenintermsofapreferredsensorystateatthedistal(terminal)
step,optimalactionsequencesleadingtothegoalunderassumedconstraints,suchasaminimal
torquechangecriterion,canbeobtainedinverselyusingtheforwardmodel. Recently,Fristonand
colleagues[7,8]advancedtheideaofgoal-directedactionusingaforwardmodelbyincorporating
a Bayesian perspective. Goal-directed planning for achieving the preferred sensory states is
formulatedundertheframeworkofactiveinference[9–11]basedonthefreeenergyprinciple[12].
Theaforementionedstudiesongoal-directedactioncanbeexpandedinvariousways. One
suchpossibilityconcernsthegoalrepresentation. Althoughgoalsintheaforementionedmodels
arerepresentedbyaparticularsensorystateateachtimesteporatthedistalstep,someclassesof
on-goingprocessescanalsobegoalsforgeneratingactions. Forexample,onecansay”Igotup
earlythismorningtorun.”Inthiscase,thegoalisnotaparticulardestination,buttheprocessof
running. Intheteleologicalframework,goalsorthepurposeofactionscouldbestates,events,or
processeswhichcouldbeeitherspecificorabstractandconceptual. Whatsortsofmodelscould
incorporatesuchdiverseformsofgoalrepresentations? Anotherpossibilityforexplorationisto
incorporatethecapabilityforinferringunseengoalsorunseenrealityconstraintsprovidedwith
theremainingtwoelementsamongactions,constraintsandgoals,asdescribedbyCsibraetal.[3].
Forthepurposeofexaminingthesepossibilities,thecurrentstudyproposesanovelmodelby
extendingourpriorgoal-directedplanningmodel,GLean[13]. GLeanwasdevelopedfollowingthe
freeenergyprinciple[12]anditoperatesinthecontinuousdomainusingPV-RNN,avariational
recurrentneuralnetworkmodel[14]. Thenewlyproposedmodel, T-GLean, hasthreenewkey
featurescomparedtoGLean. First,goalscanbespecifiedeitherbytemporalprocesses(suchas
theearlierexampleof”goingoutforarun”)orstaticsensorystates,forexample,”goingtothe
cornerstore.”Second,themodelcaninfergoalsbysensoryobservation,aswellasbygenerating
goal-directedactionplans. ThisallowsagentsusingtheT-GLeanmodeltoanticipategoalsand
futureactionsbyobservingtheactionsofanotheragent. Third,themodelgeneratesfutureaction
plansforgivengoalsbasedonthebestestimateofthecurrenthiddenstateusingthepastsensory
observation. Whilethisfeatureisnotnecessarilynovel,giventhatfunctionsofsensoryreflectionor
postdiction[15]havebeenexaminedusingdeterministicgenerativeRNNmodels[16]andBayesian
probabilistic generative models [8], this feature is added to the model so that robots utilizing
T-GLeancangenerategoal-directedplansonlinewhileactionsarebeingexecuted,whereasGLean
canonlygenerateplansoffline.
OurproposedmodelisevaluatedinSection4byconductingtwoexperimentsusingtworobot
setups. Thefirstexperimentusesasimplesimulatedmobilerobotthatcannavigateanenvironment
withanobstacle,andthesecondexperimentusesaphysicalhumanoidrobotwithalargerdegreeof
freedomthattracksandmanipulatesobjects. Inbothexperiments,therobotsaretrainedtoachieve
twodifferenttypesofgoals. Forthemobilerobot,onetypeofgoalistoreachaspecifiedposition
whileavoidingcollisionswithobstacles,andtheothertypeofgoalistomovearoundaspecific
objectrepeatedly. Forthehumanoidrobot,onetypeofgoalistograspanobjectandthentoplace
itataspecifiedpositionandtheothertypeofgoalistoliftanobjectupanddownrepeatedly. In
bothexperiments,thetrainedrobotsareevaluatedingeneratinggoal-directedactionsforspecified
goals,aswellastoinfercorrespondingunseengoalsforgivensensorysequences. Wealsotouch
ongeneratingactionplansbasedontherationalityprinciple[3]. Thefollowingsectiondescribes
2
relatedstudiesinmoredetailsothatreaderscanunderstandeasilyhowtheproposedmodelhas
beendevelopedbyextendingandmodifyingthosepriorproposedmodels.
2 Related studies
Firstwelookindetailathowagoal-directedactioncanbegeneratedusingtheforwardmodel.
Kawatoetal.[4]proposedthatmultiplefuturestepsofproprioception(jointangles)aswellas
the distal position in task coordinate space of an arm robot can be predicted, given the motor
torqueateachtimestep,bytrainingtheforwarddynamicsmodelandthekinematicmodelthatare
implementedinafeed-forwardnetworkcascadedintime. Aftertraining,optimalmotorcommands
toachievethedesiredpositionsatthedistalstepfollowingtheminimaltorquechangecriterion
canbecomputedbyusingthepredictionerrorinformationgeneratedinthedistalstep. Inorder
todealwiththehiddenstateproblemencounteredbyrealrobotsnavigatingwithlimitedsensors,
Tani[17]extendedthismodelbyimplementingtheforwarddynamicsmodelinarecurrentneural
network(RNN)withdeterministiclatentvariables. Itwasshownthataphysicalmobilerobotcan
generateoptimal,goal-directednavigationplanswiththeminimumtraveldistancecriterionina
hiddenstateenvironmentusingthatproposedmodel. Figure1adepictstheschemeofgoal-directed
planningusingaforwarddynamicsmodelimplementedinanRNN.
(a) (b) (c)
Figure 1: Prior models for generating goal-directed behaviors. (a) Forward model using latent
variables, (b) active inference model using probabilistic latent variables, and (c) GLean as an
extensionofactiveinference.
Inthisgraphicalrepresentation,a ,d ,z andx¯ denotetheaction,deterministiclatentvariable,
t t t t
probabilisticlatentvariable,andsensorypredictionattimestept,respectively. xˆ isthesensory
T
goal image at the distal step T. Within the scheme of the forward model and active inference,
thepolicyisasequenceofactionsthatisoptimizedsuchthattheerrorbetweenthesensorygoal
imageandthepreferredimageatTcanbeminimized(theminimizationcriterionsuchasthetravel
distanceisomittedforbrevity.)
Friston and colleagues [7, 8] formulated goal-directed actions by extending the framework
of active inference [9–11] based on the free energy principle [12]. The major advantage of this
approach,comparedtotheaforementionedconventionalforwardmodel,isthatthemodelcancope
withuncertaintythatoriginatesfromthestochasticnatureoftheenvironmentbyincorporatinga
Bayesianframework. Originally,thefreeenergyprinciplewasdevelopedasatheoryforperception
3
undertheframeworkofpredictivecoding,whereinprobabilisticdistributionsofpastlatentvari-
ablesingenerativemodelsareinferredfortheobservedsensationbyminimizingthefreeenergy.
Later,activeinferencewasdevelopedasaframeworkforactiongenerationwhereinactionpolicies
aswellastheprobabilisticdistributionoffuturelatentvariablesingenerativemodelsareinferred
byminimizingtheso-calledexpectedfreeenergy. MinimizingtheexpectedfreeenergyG forthe
aif
futuremaximizesboththeepistemicvalueshowninthefirstandthesecondterms,andextrinsic
valueinthethirdterminEquation1.
 
T
G = ∑ −E  logp(z |x )−logq(z |π) + logP(x )  (1)
aif q(zt,xt |π) t t t t 
t>tc (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
epistemicvalue extrinsicvalue
whereπisanoptimalpolicyorasequenceofactions,q()istheposteriorpredictivedistribution,
and t isthecurrenttimestep. Theextrinsicvaluerepresentshowmuchthesensoryoutcomes
c
expectedinthegeneratedfutureplanareconsistentwiththepreferredoutcomes. Theepistemic
value,ontheotherhand,representstheexpectedinformationgainwithpredictedoutcomes. This
meansthatthisvalueindicatestheexpecteddecreaseofuncertaintywithregardtothehidden
stateprovidedbythesensoryobservation. Insummary,anoptimalactionpolicyforachievingthe
preferredsensoryoutcomesandthereductionofuncertaintyofthehiddenstatescanbegenerated
byminimizingtheexpectedfreeenergy. Figure1bdepictsthisframeworkwhereintheprobabilistic
latentvariablesz ,aswellasanactionpolicy,a areinferredbyminimizingtheexpectedfree
0:T 0:T
energy.
AlthoughFristonandcolleagues[7,8]implementedthisframeworkmostlyindiscretespace,
Hafneretal.[18]proposedananalogousmodelimplementedincontinuousstateproblems.
MatsumotoandTani[13]proposedanothertypeofgoal-directedactionplangenerationmodel,
GLean,whichisbasedonthefreeenergyprincipleusingavariationalRNN.Themaindifferenceof
thisschemefromthoseproposedbyFristonandcolleagues[7,8]andbyHafneretal.[18]isthatan
optimalgoal-directedplanisobtainedbyinferringthelowerdimensionalprobabilisticlatentspace,
insteadofthehigherdimensionalactionspace. Thegraphicalmodeloftheschemeisdepictedin
Figure1c,whereintheproprioceptionx¯ p andtheexteroceptionx¯e ateachtimesteptarepredicted
t t
bythelearnedgenerativemodelusingtheprobabilisticlatentvariablez anddeterministiclatent
t
variable d . Foragivenpreferredgoalrepresentedasanexteroceptivestateatthedistalstep T
t
as xˆ , a proprioceptive-exteroceptive sequence expected to reach this goal state is searched by
T
inferringanoptimalsequenceoftheposteriordistributionofthelatentvariablesq(z )bymeans
1:T
ofminimizingtheexpectedfreeenergyG asshowninEquation2.
gl
(cid:2) (cid:3) (cid:16) ∑ T (cid:2) (cid:3)(cid:17)
G gl = −E q(zT |xˆT ) p(x T |d T ) + D KL q(z t |xˆ T )||p(z t |d t−1 ) (2)
(cid:124) (cid:123)(cid:122) (cid:125) t=tc
goalerror (cid:124) (cid:123)(cid:122) (cid:125)
complexity
Here,theexpectedfreeenergyisrepresentedbyasumoftwoterms,namelythegoalerroratthe
distalstep,showninthefirstterm,andthecomplexitysummedoverallfuturesteps,shownin
thesecondterm. Complexityisthedivergencebetweentheposteriorpredictivedistributionand
thepriordistributionoftheprobabilisticlatentvariables,andisdescribedinmoredetaillaterin
Section3. Byminimizingthisformoffreeenergy, plansreachingthepreferredgoalstates, but
followingwell-habituatedtrajectorieslearnedduringlearning,canbegenerated. Wenotethatthis
4
modeldoesnotinferthepolicydirectly. Instead,themotorcommandoractionaateachtimestep
iscomputedbymeansoftheproprioceptiveerrorfeedbackbysendingthepredictionofthenext
stepproprioceptionasthetargettothemotorcontrollerwhentheagentgeneratesmovement.
3 The Proposed Model
Ournewlyproposedmodel,T-GLean,isanextensionofGLean[13]withanovelgoalrepresen-
tationscheme. Eachgoalisrepresentedbyitscategory,e.g.,reachingorcycling,associatedwith
optionalparameters,e.g.,forpositionorspeed. Thenetworkisdesignedtooutputtheexpected
goalateachtimestepbasedonlearning,asshowninFigure2.
Figure2: Overviewofthenewlyproposedmodel.
Whenthecurrentpreferredgoalisgivenateachtimestep,theposteriorpredictivedistributionis
updatedinthedirectionofminimizingthedivergencebetweenthepreferredgoalandtheexpected
goalateachtimestep. Thisgeneratestheexpectedexteroceptiveandproprioceptivetrajectory
leadingtothegoal. Inaddition,thenetworkcaninferunseengoalsfromtheobservationofon-
goingsensoryinputs. Foragivenexteroceptivetrajectory,theposteriordistributionisupdated
inthedirectionofminimizingthedivergencebetweenthegivenexteroceptivetrajectoryandits
reconstructedtrajectory. Thisgeneratestheexpectedgoalintheoutputsateachtimestep. This
modelutilizesthePV-RNNarchitecture,whichleveragestheideaofmultipletimescaleRNN[13,19]
withtheexpectationofdevelopmentofafunctionalhierarchythroughlearning. Thenetworkis
operatedinthreemodes,eachofwhichminimizesfreeenergyasitslossfunction. Inlearningmode,
predictivemodelsarelearnedusingprovidedtrainingsequencesbyminimizingtheevidencefree
energy. Inonlineactiongenerationmode,afuturegoal-directedplanisgeneratedbyminimizing
theexpectedfreeenergywhilethepastsensoryexperienceisreflectedbyminimizingtheevidence
freeenergy. Asthisoccursinrealtime,thepastsensoryexperienceisconstantlyupdatedonline. In
thegoalinferencemode,theexpectedgoalisinferredusingtheobservedexteroceptivesequenceby
minimizingtheevidencefreeenergy. Thefollowingsub-sectionsdescribethemodelinmoredetail.
5
3.1 Modelarchitecture
Figure3: Graphicaldescriptionoftheemployedarchitecture.
Figure3depictstheemployedarchitectureasagraphicalmodelconsistingofthreelayersof
PV-RNN.Itissimilartothearchitectureemployedin[13], withthemodificationintroducedin
[20]thathasthetop-downconnectionfromhigherlayerstolowerlayersd l+1 → d l inthesame
timestep,ratherthanfromtheprevioustimestep. ThegraphicalmodelshowstheRNNunrolled
intotothepastaswellastothefuture,withthecurrenttimestepatt . Eachlayerindexedbyl
c
p q
containsthestochasticvariablez,fromwhichz issampledfromthepriordistribution,andz is
l,t l,t
theposteriordistribution,aswellasthedeterministiclatentvariabled ,foreachtimestept. Note
l,t
thatdeterministicvariablestakeasinputthedeterministicvariablefromthelayeraboveexcepton
thetoplayer. Theoutputofthebottomlayer(L1)issplitintoproprioceptionx¯ p ,exteroceptionx¯e,
t t
expectedgoalg¯ ,anddistalprobabilitys¯ . Unlessnotedotherwise,thepreferredgoalgˆisgiven
t t
p
atalltimestepsasatarget. Whereavailable,theobservedproprioception x andexteroception
t
xe areusedastargetsforerrorminimization. Thedistalprobabilityattimesteps¯ (omittedfrom
t t
the diagram for brevity) is the probability of achieving the expected goal at t. When the distal
probabilityataparticulartimestepbecomesthehighestamongthoseatallothertimestepsandit
exceedsapredefinedthresholdvalue,itisassumedthatthepreferredgoalisachievedinthistime
step. Duringtraining,sˆisaone-hotvectorinthetimedimensionwithasinglepeakatthetimestep
6
thegoalisactuallyachieved. Theoutput(x¯,g¯)andtargets(x,gˆ)aresoftmaxencoded;however,
forbrevity,theoutputlayerthatdecodesthenetworkoutputintorawoutputfortheagentisnot
showninthisfigure. Insubsequentsubsections,wewilldescribeinmoredetailtheaforementioned
modesonwhichthenetworkcanoperate. WedonotmakeacompletederivationofPV-RNNin
thispaper,butfocusonkeyaspectsofthismodel.
3.2 Learning
BasedonthegraphconnectivityshowninFigure3,theforwardcomputationofPV-RNNis
giveninEquation3.TheinternalstateofeachPV-RNNcellh attimesteptandlevelliscomputed
l,t
asasumoftheconnectivityweightmultiplicationofz
l,t
,h
l,t−1
,andh
l+1,t
(iflisnotthetoplayer).
TheconnectivityweightmatrixW isindexedfromlayertolayerandfromunittounit. Forbrevity,
biastermshavebeenomitted.
h l,t = (cid:18) 1− τ 1 l (cid:19) h l,t−1 + τ 1 l (cid:16) W d l, , l d d l,t−1 +W z l , , d lz l,t +W d l+ ,d 1,ld l+1,t (cid:17) ,
(3)
d =tanh(h ),
l,t l,t
d =0.
l,0
Whereτl istheMTRNNtimeconstantoflayerl.
ThestochasticvariablezfollowsaGaussiandistribution. Eachsampleofthepriordistribution
p
z forlayerliscomputedasshownininEquation4.
t
(cid:40)
0, ift =1
µ p =
l,t tanh(W
d
l,
,
l zµp d l,t−1 ), otherwise
(cid:40)
1, ift =1 (4)
σ p =
l,t exp(W
d
l,
,
l zσp d l,t−1 ), otherwise
z p = µ p +σ p (cid:12)(cid:101).
l,t l,t l,t
Where(cid:101)isarandomnoisesamplesuchthat(cid:101) ∼ N(0,I). Samplesoftheposteriordistribution
q
z arecomputedasshowninEquation5. The Avariableisavectorforeachzunitandsequence.
t
Forbrevity,hereweassumewehaveasinglesequence. Ifanoutputsequenceisgeneratedusing
theposterioradaptedduringtraining,thecorrespondingtrainingsequenceshouldberegenerated.
Duringgoalinferenceandplangeneration, the A variablesareinferredbytheerrorregression
process,aswillbedescribedlater.
µ q =tanh(A µ ),
l,t l,t
σ q =exp(Aσ ), (5)
l,t l,t
z q = µ q +σ q (cid:12)(cid:101).
l,t l,t l,t
Tocomputetheoutputattimestept,therearethreesteps. First,thenetworkoutputoiscomputed
fromd ,asinEquation6. Thisusestheoutputfromlayer1andtreatstheoutputlayeraslayer0.
1,t
o =W1,0d . (6)
t d,o 1,t
7
We then compute the predicted probability distribution output x¯. For this purpose, we use a
softmaxfunctiontorepresenttheprobabilitydistributionofthei-thdimensionoftheoutputasin
Equation7.
exp(o i,j)
x¯ i,j = t . (7)
t ∑ exp(o i,j)
j t
i,j
wherex¯ isthepredictedprobabilitythatthej-thsoftmaxelementofthei-thoutputison.
t
Forexplainingthelearningschemefollowingtheprincipleoffreeenergyminimization,wefirst
describethemodelintermsoffreeenergy. Forbrevity,wewillassumethereisasinglelayeronly.
ThisisshowngraphicallyinFigure4.
Figure4: Networkduringtraining.
During learning, the evidence free energy shown in Equation 8 is minimized by iteratively
updatingtheposteriorof z aswellastheRNNlearningparametersW ateachtimestepforall
trainingsequences.
∑ T (cid:16) (cid:2) (cid:3) (cid:2) (cid:3)(cid:17)
F(x,gˆ,z) = w·D KL q(z t |x t:T ,gˆ t:T )||p(z t |d t−1 ) −E q(zt |xt:T,gˆt:T ) logP(x t ,g t |d t ) . (8)
t=1 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
complexity accuracy
Wherexandgˆaretheobservedsensorystatesandpreferredgoal,respectively,whiledandzare
thedeterministicandprobabilisticlatentstates,respectively. Freeenergyinthisworkismodified
byinclusionofthemeta-priorw,whichweightsthecomplexityterm. wisahyperparameterthat
affectsthedegreeofregularization,similartoβinVariationalAutoencoders[21]. Wealsonotethat
sincewearedealingwithsequencesofactions,thefreeenergyisasummationoveralltimestepsin
thesequence.
Thefirstterm,complexity,iscomputedastheKullback–Leibler(KL)divergencebetweenthe
priorandapproximateposteriordistributions. Thiscanbeexpressedasfollows:
D KL (cid:2) q(z t |x t:T ,gˆ t:T )||p(z t |d t−1 ) (cid:3) = (cid:90) q(z t |x t:T ,gˆ t:T )log q( p z t ( | z x t t | : d T t , − gˆ 1 t ) :T ) dz t (9)
8
Since we have µ and σ for both prior p and posterior q distributions, they can be expressed as
follows:
p(z t |d t−1 ) = (cid:113) 2π 1 (σ p)2 exp
(cid:34)
− 1 2
(cid:32)
z t − σ t p µ t
p(cid:33)2(cid:35)
,
t
(10)
q(z t |x t:T ,gˆ t:T ) = (cid:113) 2π 1 (σ q)2 exp (cid:34) − 1 2 (cid:32) z t − σ t q µ q t (cid:33)2(cid:35) .
t
Thus,continuingfromEquation9,complexitycanbecomputedas:
(cid:90) q(z |x ,gˆ ) σ p (µ q−µ p)2+(σ q)2 1
q(z |x ,gˆ )log t t:T t:T dz =log t + t t t − (11)
t t:T t:T p(z t |d t−1 ) t σ t q 2(σ t p)2 2
For brevity, a case of a single z-unit with a (µ,σ) pair is shown here. In practice we can have
multiplez-units,eachwithindependent(µ,σ),andinthatcasetheRHSisasummationoverall(µ,
σ),aswillbeshownlaterintheexperimentalsection. ThesecondtermofEquation8,accuracy,can
becomputedusingtheprobabilitydistributionestimatedinthesoftmaxoutputs. Duringlearning,
Equation8isusedasthelossfunctionwiththeAdamoptimizer,asnotedSection4.
3.3 Onlinegoal-directedactionplangeneration
Akeydifferencebetweenournewlyproposedmodel,T-GLean,andourpreviousmodel,GLean,
is the idea that the goal expectation g¯ is generated at every time step instead of expecting the
t
goalsensorystateatthedistaltimestep. Intuitively,thismeansthatateverytimestep,theagent
expectsagoalstatethattheon-goingactionsequencewillachieve. Anadvantageofthismodel
schemeisthatgoalscanberepresentednotonlybydistalsensorystatestobeachieved,butalso
bycontinuouslychangingsensorysequences. AnotherkeyfeatureofT-GLeanisthatthemodel
generatesplansonline, inreal-time, whiletheagentisactingontheenvironment, whereasour
priorstudyusingGLeanshowedonlyanofflineplangenerationscheme. InT-GLeanthenetwork
maintainstheobservedsensorysequenceinapastwindowwhileallocatingafuturewindowfor
thefuturetimesteps. Inthepastwindow,evidencefreeenergyisminimizedonlinebyupdating
theposteriorateachtimestepinordertosituateallthelatentvariablestotheobservedsensory
sequence. Inthefuturewindow,theerrorbetweenthepreferredgoalandtheexpectedgoaloutput
isminimizedforallstepsbyupdatingtheposteriorpredictivedistributioninthewindowiteratively,
ofwhichcomputationcanbeperformedbyminimizingtheexpectedfreeenergy. Thefutureplan
forachievingthepreferredgoalscanbegeneratedoncetheevidencefreeenergyinthepastwindow
isminimized,i.e.,thelatentvariablesarewellsituatedtothepastsensoryobservation. Thisscheme
isreferredtoasonlineerrorregression[16]andanalogousmodelscanbeseenalsoin[8,22]. The
schemeisshowngraphicallyinFigure5.
9
Figure 5: The network during planning with a planning window covering from t −winp to
c
t +winf wheret iscurrenttimestep.
c c
Withintheplanningwindow,thereisthepastwindowoflengthwinp andthefuturewindowof
lengthwinf. Inthecurrentimplementation,thelengthoftheplanningwindowisfixed,whilethe
pastwindowisallowedtogrowuptohalfthelengthoftheplanningwindow. Thecurrenttime
stept isonestepaheadoftheendofthepastwindow. Atthenextsensorimotortimestep,sensory
c
informationatt becomespartofthepastwindow,andt movesforwardonestep,shrinkingthe
c c
future window. Once the past window is filled, the entire planning window slides one step to
theright,discardingtheoldestentryinthepastwindow. Duringonlineplanning,thenetwork
minimizestheplanfreeenergyF byoptimizingtheposteriorzq atalltimestepswithinthepast
plan
andfuturewindows. TheplanfreeenergyconsistsofthesumoftheevidencefreeenergyF within
e
thepastwindowandtheexpectedfreeenergyGwithinthefuturewindow. Thisisexpressedin
Equation12.
t ∑ =tc (cid:16) (cid:2) (cid:3) (cid:2) (cid:3)(cid:17)
F e (x,gˆ,z) = w·D KL q(z t |x t:tc ,gˆ t:tc )||p(z t |d t−1 ) −E q(zt |xt:tc ,gˆt:tc ) logP(x t ,g t |d t ) ,
t=tc −winp
t=tc∑ +winf (cid:16) (cid:2) (cid:3) (cid:2) (cid:3)(cid:17)
G(gˆ,z) =
t=tc
w·D KL q(z t |gˆ t:tc +winf )||p(z t |d t−1 ) −E q(zt |gˆ t:tc+winf ) logP(g¯ t |d t ) ,
F = F +G.
plan e
(12)
3.4 Goalinference
Finally,beforeclosingthecurrentmodelsection,wedescribehowfuturegoalscanbeinferred
fromobservedsensorysequences. Figure6showsagraphicalmodelaccountingforthemechanism
ofthegoalinferencewithobservationoftheexteroception,butwithoutactuallygeneratingactions.
Byobservingtheexteroceptionsequencefromtimestept −winp tothecurrenttimestept ,the
c c
posteriorzq ateachstepinthepastwindowisoptimizedforminimizingtheerrorbetweenthe
10
observedandreconstructedexteroceptions. Thisresultsininferenceoftheexpectedgoal g¯ for
t
every time step t, both in the past and future. The network predicts simultaneously both the
exteroceptionandproprioceptionforfuturestepsleadingtotheinferredgoal. Wenotethatthis
schemecouldbeappliedtotheproblemofinferringgoalsofotheragentsthroughobservationof
theirmovements,providedthatthecoordinatetransformationbetweentheallocentricviewand
theegocentricviewcanbemade. Forsimplicity,thecurrentstudydoesnotdelveintothisview
coordinatetransformationproblem.
Figure6: Networkduringgoalinference.
Equation13showsthemodifiedevidencefreeenergyusedforgoalinference,whereonlythe
observationofexteroceptionisused.
F g (xe,z) = t
t
∑ =
=
t
1
c(cid:16) w·D KL (cid:2) q(z t |x t e :tc )||p(z t |d t−1 ) (cid:3) −E q(zt |x
t
e
:tc
) (cid:2) logP(x t e|d t ) (cid:3)(cid:17) . (13)
4 Experiments
Inordertotestourproposedmodel,weconductedtwoexperiments,oneusingasimulated
agentandtheotherusingaphysicalrobot. InExperiment1(Section4.1), weusedasimulated
mobile agent in 2D space in order to examine the model’s capacity in generating goal-directed
actions and in understanding goals from sensory observation. Experiment 2 (Section 4.2) was
carried out to test the model’s scalability in the real world setting by using a humanoid robot
withhigherdegreesoffreedom. T-GLeanisimplementedusingLibPvrnn,acustomC++library
implementingPV-RNNthatiscurrentlyunderdevelopment. Itisdesignedtobelightweightand
tooperateinreal-timesointeractionbetweenagentandexperimenterispossible. Apre-release
versionisavailableunderanopensourcelicensewithinstructionsonreproducingthesimulated
agentexperiments.
11
4.1 Experiment1: simulatedmobileagentina2Dspace
We conducted a set of experiments using a simulated mobile agent that can generate goal-
directedactionplansbasedonsupervisedlearningofsensorimotorexperiencesinordertoevaluate
theperformanceofT-GLeaninthefollowingfourrespects.
1. Generalizationinlearningforgoal-directedplangeneration;
2. Goal-directedplangenerationfordifferenttypesofgoals;
3. Goalunderstandingfromsensoryobservationfordifferenttypesofgoals;
4. Rationalplangeneration.
Followingourpreviouswork[13],wefirstevaluatethegeneralizationcapabilityoftheproposed
modelforreachinguntrainedgoalpositionsusingalimitednumberofteachingtrajectories. The
secondtestexamineshowthemodelcangenerategoal-directedplansandexecutethemfordifferent
typesofgoals,inthiscasereachingspecifiedgoalpositionsandcyclingaroundanobstacle. The
thirdtestdemonstratesthecapabilityofthemodeltoinferdifferenttypesofgoalsfromobserved
sensation (exteroception). The fourth test examines the model’s capability to generate optimal
travelplanstoreachspecifiedgoalsundertheconstraintoftheminimaltraveltime.
Ineachtest,thesimulatedagentisinasquareworkspacewith(x,y)coordinatesintherange
of [0.0,1.0] for both x and y. The agent always starts at position (0.5,0.1). In the center of the
workspaceisafixedobstacleofsize(0.3,0.05). Theagentdoesnotdirectlysensetheworkspace
coordinates. Insteaditobservesarelativebearinganddistance(θ ,δ )toafixedreferencepoint
t t
at(0.0,0.5)asexteroception. Thesimulatedagentcontrollerhandlesconversiontoandfromthe
workspacecoordinatestorelativebearing-distanceastherobotmoves. Attheonsetofeachtest
trial,theexperimentersetsthepreferredgoalasavector(gˆα,gˆ β). gˆ β isathreedimensionalone-hot
t t t
vector,witheachbucketrepresentingreaching,clockwisecycling,andcounter-clockwisecycling
goals,respectively. gˆα issetasthegoalxcoordinateifthereachinggoalisset. Otherwiseitisleftas
t
0. Asthenetworkestimatesthedistalprobabilitys¯ ateachtimestep,theagentstopsatthetime
t
stepwiththemaximumestimateddistalprobabilityprovidedthatitexceedsathresholdvalue,
assumingthatthegoalisachievedatthatpoint.
Unlessstatedotherwise,theexperimentshavedifferenttrainingdataandseparatelytrained
networks;however,networkparametersareidenticalbetweennetworks. Parametersusedforeach
layeroftheRNNareasshowninTable1. Eachnetworkwastrainedfor100,000epochs,usingthe
Adamoptimizerwithparametersα =0.001,β =0.9,β =0.999. Duringplanning,theparameters
1 2
areslightlymodifiedtoα =0.04,500iterationspersensorimotortimestep,andaplanningwindow
lengthof70. Themeta-priorwremainsthesameinallcases.
12
Table1: PV-RNNparametersforExperiment1. Rd andRz refertothenumberofdeterministic(d)
unitsandprobabilistic(z)unitsrespectively. w t=1 referstothemeta-priorsettingatthefirsttime
step(inourpreviousworkthiswasreferredtoasw ).
I
Layer
1 2 3
Rd 60 40 20
Rz 6 4 2
τ 2 4 8
w 0.0001 0.0005 0.001
w t=1 1.0 1.0 1.0
4.1.1 Experiment1A:Generalizationinplangenerationbylearning
Inordertoevaluatehowwellthenetworkcangeneralizegoalpositionsinthegoalareawith
a limited number of teaching trajectories, we prepared four teaching datasets with decreasing
numbersofgoallocationstobereachedasshowninFigure7. Thegoallocationsareonalineinthe
rangex = [0.2,0.8],y =0.9. Theagentacceleratesuptospeedtoabranchingpoint,andtheneither
turnsleftorrighttothesideoftheobstacle,beforemovingtowardthegoalposition. Astheagent
approachesthegoalposition,itdeceleratestoastop.
13
(a) (b)
(c) (d)
Figure7: Testinggoalgeneralizationbyreducingthenumberoftrainingsamples(a)25training
samples(b)13trainingsamples(c)7trainingsamples(d)4trainingsamples
Themaximumtrajectorylengthis70,althoughthedistalstepoccursataround40timesteps. To
accountforrandomnessintraining,fivenetworkswithdifferentinitialrandomseedsaretrained
foreachofthefourdatasets,for20trainednetworksintotal. Untrainedtestgoalsweredrawnfrom
auniformlyrandomdistributionintherange[0.2,0.8]. Eachnetworkwastestedwithtenuntrained
goals,withtheresultsaveragedoveralltestgoalsandnetworksforeachdataset.
Toevaluategoalgeneralization,weconsideredthedifferencebetweenthefinalagentposition
reachedattheendofplanexecutionandthepreferredgoalattheendforeachtesttrial,aswellas
theplanfreeenergythatremained. Thedifferencebetweenagentpositionandgoalisexpressedas
theroot-mean-squaredeviation,normalizedtotherangeofg¯α (NRMSD).Theresultissummarized
inFigure8.
14
Figure8: Comparisonofgoaldeviationandresidualfreeenergyfordifferentnumbersoftraining
goals.
We observed that the network achieved stable goal-position generalization when at least 7
trainingtrajectoriesareused. Also,itcanbeseenthattheplanfreeenergywasminimizedina
similarmanner.
4.1.2 Experiment1B:Goal-directedplangenerationfordifferenttypesofgoals
For this test, we used a more complex set of teaching trajectories, containing three distinct
goals with significantly different patterns. The first goal, shown in Figure 9a, is similar to the
previousgoal-reachingtrajectoriesshowninFigure7a;however,thisscenarioisill-posed. That
is, the trajectories alternate between short and long paths for the same goal position. This set
of teaching trajectories will also be reused in Experiment 1C and Experiment 1D. We note that
whilethetrainingdatathemselvesarenotill-posed,duetogeneralization,thelearningoutcomeis
ill-posed. WewillrevisitthisissueinSection4.1.4. Thesecondandthethirdgoals,consistingofthe
twotrainingtrajectoriesshowninFigure9b,involvetheagentcyclingaroundthecentralobstacle
inaclockwisedirectionandacounter-clockwisedirection,respectively. Forthereachinggoal,once
theagentreachesthegoalposition,thedistalstepissetandtheremainingstepsarepaddedwith
thefinalvalue,i.e.,theagentremainsstationary. Unlikethereachinggoal,thecyclinggoalshave
nodistalstep. Thetrainingsequencedemonstratesasinglecycle;however,itisdesirableforthe
agenttocontinuetheactionindefinitely. Thereareatotalof27teachingtrajectoriesinthistraining
dataset,eachofwhichhasalengthof70timesteps.
15
(a) (b)
Figure9: TeachingtrajectoriesusedforExperiment1B,withbothgoalreachingandcyclinggoals.
(a)25reachingtrajectories,withbothshortandlongpaths(b)2cyclingtrajectories,intheclockwise
andcounter-clockwisedirections
Weevaluatedhowpreciselytheagentcangeneratemovementtrajectoriesforachievinggoals
specified. InTable2wesummarizetheresultsforthereachingandcyclicgoals. Forthereaching
goal,asusedpreviously,averageNRMSDisgivenfor10randomuntrainedgoalpositions. Forthe
cyclicgoal,NRMSDbetweentheagent’smovementandthetrainingsequencefortheentire70time
stepsequenceistaken,andaveragedforfiveclockwiseandcounter-clockwiseorbitseach. Inthe
lattercase,normalizationisdoneovertheentiredatasetrangeratherthanthegoalrange. Table2
confirmsthatbothtypesofgoalscanbeachievedwithinminimalrangeofgoalerror.
Table2: Deviationfromthegroundtruth,givenasnormalizedRMS.
Reaching Cycling
NRMSD 0.033241 0.028158
Figure10showsthreeexamplesoftrajectoriesgeneratedforthreegoalcategories,withtrajecto-
riesofmotionintheleftpanel,therepresentativenetworkvaluesinthemiddle,andtheexpected
freeenergyforthepastwindowandtheevidencefreeenergyinthefuturewindowintheright
panel. Weobservedthatplangenerationstabilizedquicklyaftertheonsetoftravelbyminimizing
theexpectedfreeenergy,andthenetworkwasabletoaccuratelyestimatethedistalstepinthe
caseofreachingthegoal. Althoughthefutureplantrajectorywasconstantlychangingduetothe
schemeofonlineplangenerationandtheerrorregressioninthepast,themotorplangeneratedwas
stablyexecutedateachsensorimotortimestep. Itcanbealsoobservedthattheevidencefreeenergy
inthepastwindowcontinuedtoconvergeinallthreeplots,meaningthatalllatentvariableswere
graduallysituatedtothebehavioralcontext. Therefore,thedeviationoftheexecutedtrajectory
fromtheplannedtrajectorywaslimited. Thefullobservedtemporalprocessescanbeseeninthe
recordedvideosofthesimulationsatthislink.
16
(a)
(b)
(c)
Figure10: Examplesoftrajectoriesgeneratedforthreedifferentgoalcategories,(a)reaching,(b)
clockwisecycle,and(c)counter-clockwisecyclefromtheresultsofExperiment1B.Theleftcolumn
showsaviewofthe2Dworkspace,withthegeneratedplantrajectory,shownasasolidlinewith
dotsateachtimestepandthegoalpositionifavailable. Theagentisrepresentedbyatriangle,with
theagent’spositionhistorytrailingbehinditandoverlaidontheplantrajectory.Thesubsetalong
thebottom(enlargedforvisibility)istheexpectedgoal,theleft-mosthorizontalbarisg¯α,whilethe
threeverticalbarsisg¯β;theleftverticalbarrepresentsreaching,themiddlebarrepresentsclockwise
andtherightbarrepresentscounter-clockwisegoals. Theheightoftheverticalbarrepresentsthe
probability (confidence) of this goal. The distal step, if available, is also shown both as a circle
in2Dandintextinthetopright. Themiddlecolumnshowstheplanintermsofexteroceptive
trajectoryinthetopaswellastheobservedexteroceptiontrajectoryandpreferredgoal(encoded
fordisplayasscalarvalues1,0and-1forreaching,clockwiseandcounter-clockwiserespectively)
indottedlinesinthebottom,withtheverticalblackbarrepresentingthecurrentsensorimotortime
17
step. Therightcolumnshowstheevidencefreeenergy(darkpurple)andtheexpectedfreeenergy
(lightgreen)inthetopandZinformation,ameasureofhowmuchofacontributionismadebythe
probabilistic(z)unitsineachlayer(darkerlinesarehigherlayers),equaltoD [q||N]whereNis
KL
theunitnormaldistribution.
4.1.3 Experiment1C:Goalinferencebysensoryobservation
Next,weevaluatedhowpreciselythenetworktrainedinExperiment1Bcaninfergoalsaswell
asfuturemovementtrajectoriesbyobservingmovementtrajectoriesintermsoftheexteroception
sequence. Inthisexperiment,fourmovementtrajectoriesachievingdifferentgoalsareprepared,
whichreachgoalslocatedleftandrightofthegoalline,clockwisecycling,andcounter-clockwise
cycling. The four test trajectories, shown in Figure 11, were generated in the same way as the
trainingdata,butwithuntrainedgoalpositionsforthereachinggoalsandwithaslightvariation
andextendedlengthforthecyclinggoals.
(a) (b)
(c) (d)
Figure11:Testtrajectoriesusedforgoalinference,colorcodedbythetypeofgoal.Thesetrajectories
representagentactionsfor(a)reachinggoallocatedontheleft,(b)reachinggoallocatedonthe
right,(c)clockwisecycling,and(d)counter-clockwisecycling. Thereachingtrajectoriesandthe
cyclingtrajectoriesare40and90stepslong,respectively.
Figure12showstheanticipatedtrajectoriesandgoalsatdifferentpointsintimeasthenetwork
observedthegoal-reachingtrajectoriesinFigures11aand11b.
18
(a) (b) (c)
(d) (e) (f)
Figure12: Anticipatedgoalandtrajectoriesasthenetworkobservesthereachingtrajectoriesat
differenttimestepsduringtheobservationoftravels. Thecoloreddotsrepresenttheanticipated
trajectory,whilethesolidgraylineistheobservedtrajectory. Thetimestepisshowninthetop
right. (a–c)Agentobservingthetrajectoryreachingtotheleftgoal,(d–f)agentobservingthe
trajectoryreachingtotherightgoal.
Initially,att = 1(Figures12a&12d),beforetheagentobservesanymovement,thenetwork
makes a guess based on the learned prior distribution. Since the goal-reaching trajectories are
mostfrequentintheteachingtrajectorydistribution,reachingagoalisinferredasadefaultgoal
whenobservingthemovementtrajectoryatthestartingpoint. Severalstepslater,ataroundt =8
(Figures12b&12e),themovementtrajectorybrancheseitherleftorrightaroundtheobstacle. We
observedthatinthecaseofthegoallocatedontheleft,thenetworkinitiallyanticipatesalonger
pathgoingaroundtherightsideoftheobstaclebeforeobservingthatthemovementtrajectorygoes
aroundtheothersideoftheobstacle. Theobservedphenomenonisduetothefactthattheteaching
trajectoriescontaintwopossiblepathsreachingthesamegoalposition. Therefore,thenetworkcan
generatetwopossiblemovementtrajectoryplansinthecurrentill-posedsetting. Thisissuewillbe
revisitedinSection4.1.4. Theanticipatedgoalcoordinateg¯α isrefinedasthemovementtrajectory
approachesthegoalarea. Byt =32(Figures12c&12f),thegoalisfullyanticipated.
19
Figure13showsthetrajectoriesandgoalsinferredfortheobservedcyclictrajectoriesshownin
Figures11cand11d.
(a) (b) (c)
(d) (e) (f)
Figure13: Anticipatedgoalandtrajectoriesasthenetworkobservesthecyclictrajectories. The
coloreddotsrepresenttheanticipatedtrajectory,whilethesolidgraylineistheobservedtrajectory.
Thetimestepisshowninthetopright. (a–c)Agentfollowingthecounter-clockwisecyclicgoal
trajectory,(d–f)agentfollowingtheclockwisecyclicgoaltrajectory.
Untilt =22theobservedtrajectoriesaremostlyindistinguishablefromthereachingtrajectories,
ofwhichtheoccurrenceprobabilitywaslearnedashighinthepriordistribution,thenetworkinfers
goalreachingasthedefaultgoalfromtheseobservations. Whiletheobservedmovementtrajectory
beginstoenteracyclictrajectoryaftert =22,itstilltakessometimeforthenetworktocorrectly
infertheongoinggoalascyclic. Duringthistime,freeenergyincreases,anexampleofwhichis
showninFigure14. Thisisanalogoustoa‘surprising’observation. Aftersometime,thegoalsare
inferredcorrectlyasthecyclinggoalsasshowninFigures13cand13fforthecounter-clockwise
andclockwisecases,respectively. Thefreeenergyisreducedaccordinglyatthismoment,asalso
showninFigure14,whichshowsageneratedplan,theplanfreeenergy,andzinformationinthe
counter-clockwisecase. Wenotethattheactivityofzunits(zinformation)alsorisesandstays
20
elevatedtocontributetoproducingthecorrectinferenceofpossiblegoals.
Figure14: Networkinferringthecounter-clockwisecyclinggoalwhileobservingthemovement
trajectoryintheleftpanel. Thecenterpanelshowstheinferredplaninthetopandtheobserved
exteroceptioninthebottom. Notethepeakinthefreeenergybeforethegoaliscorrectlyinferredin
therightpanel.
Weassumethattheagentcanrecognizeconsecutiveswitchingofgoalsfromobservation;thus,
we also tested a scenario wherein the agent first observes a clockwise cycling trajectory, then a
counter-clockwisetrajectory,andagoal-reachingtrajectory,allinonecontinuoussequence. Thisis
achallengingtest,sincethenetworkwasnottrainedtocopewithsuchdynamicgoalswitching.
Avideoofthisexperimentalresultisprovidedatthislink. Intheanimationitcanbeseenthat
thenetworkinferredthechanginggoalssuccessfully. However,theanticipatedfuturetrajectories
werequiteunstabletowardtheendofthetrial. Asweobservedthatfreeenergybecomesquite
large,itispresumedthatthegoalinferencefromtheobservationmaytakearelativelylongtime
forconvergenceofthefreeenergyforunlearnedsituations. Therefore,itmaybedifficultforthe
networktocatchuptothegoalswitchingifitoccurstoofrequently.
4.1.4 Experiment1D:Goal-directedplanningenforcingthewell-posedcondition
InExperiment1B,thenetworkwastrainedusingteachingtrajectoriesthatincludedalternative
trajectories reaching similar goal positions as shown in Figure 9a. This made the goal-directed
planning ill-posed since the network cannot determine an optimal plan between two possible
choicesunderthecurrentdefinitionoftheexpectedfreeenergy. Figure15showsanillustrative
exampleastheresultoftheill-posedgoal-directedplanningwhereweseethatbothashortand
longpathcanbegeneratedforthesamegoal.
21
(a) (b)
Figure15: Examplesofill-posedgoal-directedplanning. (a)Generationofashortpathreachingthe
goal(39timesteps)and(b)analternativelongpathtothesamegoal(44timesteps).
Conventionally,ithasbeenshownthattheproblemofill-posed,goal-directedplanningcanbe
transformedintoawell-posedonebyaddingadequateconstraints,includingjointtorquemini-
mization[4]andtraveldistanceminimization[17]. Thecurrentexperimentshowsanexamination
ofthecaseusingthetraveltimeminimizationconstraintbyaddinganadditionalcostterminthe
planfreeenergy F shownpreviouslyinEquation12. Themodifiedplanfreeenergy F(cid:48) is
plan plan
showninEquation14.
γ = tlogP(s |d ),
t t t
F e (cid:48)(x,gˆ,z) = t ∑ =tc (cid:16) w·D KL (cid:2) q(z t |x t:tc ,gˆ t:tc )||p(z t |d t−1 ) (cid:3) −E q(zt |xt:tc ,gˆt:tc ) (cid:2) logP(x t ,g t |d t )−kγ t (cid:3)(cid:17) ,
t=tc −winp
G (cid:48)(gˆ,z) = t=t
t
c∑
=
+
t
w
c
inf (cid:16) w·D KL (cid:2) q(z t |gˆ t:tc +winf )||p(z t |d t−1 ) (cid:3) −E q(zt |gˆ t:tc+winf ) (cid:2) logP(g¯ t |d t )−kγ t (cid:3)(cid:17) ,
F (cid:48) = F (cid:48)+G (cid:48) .
plan e
(14)
Whereγistheaddedcosttermforminimizingthetraveltime. Thiscostcanbeexpressedbythe
summation of the estimated distal probability distribution P(s |d ) multiplied by the time step
t t
lengthateachtimestepoveralltimestepsintheplanwindow. Forthisexperiment,wesetthe
weightofthetraveltimecostk =0.1.
Toevaluatetheeffectofaddingtheconstraintfortravel-timeminimization,wepreparedthree
separately trained networks and four untrained goal positions that are shown with numbers
overlaidonthetrainingtrajectoriesinFigure16. Thegoalpositionsareselectedtoavoidtheedges
(lackoftrainingtrajectories)andthecenter(nodifferenceintrajectorylength).
22
Figure16: Trainingtrajectoriesoverlaidwiththefouruntrainedtestgoals.
Thegeneratedactionplansforthesetestgoalpositionswereclassifiedas‘short’iftheshorterof
thepossiblepathswasgenerated. Plangenerationwasrepeatedforeachtestgoalpositionwith
1000differentsamples. Theresultantprobabilitiesforgeneratingtheshorterplansforeachgoal
positionwithandwithouttraveltimecostareshowninFigure17.
Figure17:Comparisonoftheprobabilityofshorterplansbeinggeneratedforeachtestgoalposition
withandwithoutaddingthetraveltimeminimization(TTM).
Withoutintroducingthetraveltimecostγ,theprobabilityofgeneratingtheshortplanswas
around 50%, which is consistent with the ratio in the teaching trajectory set. This probability
increasedtoover90%withtheadditionoftraveltimecosttotheplanfreeenergy. Theseresults
confirmthatthecurrentmodifiedmodelsupportsgoal-directedplangenerationinawell-posed
manner.
23
4.2 Experiment2: objectmanipulationbyaphysicalhumanoidrobot
In order to verify the performance of the proposed model in a complex physical world, we
conductedexperimentsinvolvingobjectmanipulationbyahumanoidrobot,Torobo,manufactured
byTokyoRoboticsInc. Torobowasplacedinfrontofanobjectmanipulationworkspacewhereared
cylindricalobjectwaslocatedformanipulation. Twotypesofgoal-directedactionswereconsidered.
Onewastograsptheobjectlocatedatanarbitrarypositionintheworkspace(36cm×31cm)with
botharmsandthenplaceitataspecifiedgoalpositiononthegoalplatform(42cmwide)fixedat
oneendoftheworkspace. Theothertypeofgoalwastograsptheobjectlocatedatanarbitrary
positionintheworkspaceandtoswingitupanddown. TheTorobohumanoidrobot,redcylinder
object,workspace,andgoalplatformareshowninFigure18.
Figure18: TheTorobohumanoidrobot,withtheworkspace,goalplatformandobject.
The neural network controlled Torobo’s two arms (6 degrees of freedom for each arm) and
hipjoints(2degreesoffreedom)toperformthesegoal-directedactions(totalof14jointangles).
ThereadingofthesejointanglesrepresentstheproprioceptionofTorobo. Torobocantrackthe
positionoftheredcylinderlocatedintheworkspaceusingacameramountedinitshead. The
objectpositionisconstantlytrackedbycontrollingthepitchandyawofthecameraheadtokeep
thetargetobjectcenteredinthecamera’sfieldofview. Theredcylinderisvisuallylocatedusing
YOLOv3[23]. Therefore,thepitchandyawoftheheadindicatesthepositionoftheobject,andare
consideredtorepresenttheexteroceptionofTorobo. Thus,exteroceptioncanberepresentedby
onlyatwo-dimensionalvectorinsteadofahigh-dimensionalcameraimage. Thissimplificationin
visualimageprocessingwasnecessaryinordertogenerategoal-directedactionsinreal-time.
WeconductedtwoexperimentswithTorobo. InExperiment2A,weevaluatedtheperformance
ingeneratinggoal-directedplanninganditsexecutioninasimilarfashiontoExperiment1Bin
Section4.1.2,andinExperiment2Bweevaluatedthecapabilityofthenetworkforgoalinference
by observation. The parameters used for each layer of the RNN are as shown in Table 3. As
in Experiment 1, the network was trained for 100,000 epochs, using the Adam optimizer with
a learning rate α = 0.001, β = 0.9, β = 0.999. In order to maintain real-time operation with
1 2
therobot, planningusesdifferentparametersof α = 0.1and100errorregressioniterationsper
sensorimotortimestep.
24
Table3: PV-RNNparametersforExperiment2. ParametersettingsareidenticaltoExperiment1,
onlyusingalargerτtocompensateforlongersequences.
Layer
1 2 3
Rd 60 40 20
Rz 6 4 2
τ 2 10 20
w 0.0001 0.0005 0.001
w t=1 1.0 1.0 1.0
Thetrainingdatasetconsistsof120trajectoriesforoneofthegoal-directedactions,grasping
thenplacing,and80trajectoriesfortheothertypeofgoal-directedaction,graspingthenswinging.
Theobjectislocatedatarandompositionwithintheworkspaceforeachsampleoftheteaching
trajectories. Thegoalpositionforplacingisalsorandomlyselectedalongthegoalplatform’swidth.
Ateachsensorimotortimestep,werecorded12jointanglesforbotharmsand2jointanglesforthe
hipjointsrepresentingtheproprioceptionand2headjointanglesrepresentingtheexteroception
alongwitha3Dvectorrepresentingthepreferredgoaland1Dscalarforthedistalstepmarker.
Thepreferredgoal(gˆα,gˆβ)isrepresentedinasimilarmannertoExperiment1whereingˆβ isa2D
one-hotvectorrepresentingeithergoalofgrasping-placingorgrasping-swinging,andgˆα isascalar
representingthepreferredgoalpositioninthewidthdirectionofthegoalplatforminthecaseof
thegrasping-placinggoal.
4.2.1 Experiment2A:Goal-directedplangenerationandexecution
Toevaluatetheperformanceofgoal-directedplangenerationandexecutionwiththephysical
robot,wemeasuredtheRMSdeviationtothegroundtruth,normalizedtothedatarange,asshown
inExperiment1B.Toexaminetheperformanceforachievingthegraspingandplacinggoal,the
experimenterplacedtheobjectatanarbitrarygoalpositiononthegoalplatform,allowingTorobo
torecognizethegoalpositionbyvisualtracking. Thispositionwasmarkedasthegroundtruth.
The object was then placed at a random position in the workspace, and the network started to
generateactionplanstoachievethisspecifiedgoalwhileToroboexecutedthegeneratedmotor
plansimultaneously,inreal-time. Thedifferencebetweenthefinalpositionoftheplacedobjectand
thegroundtruthwasthenmeasuredfor10randomgoalpositions. Inthecaseofexaminingthe
graspingandswinginggoal,theobjectwasplacedinthreedifferentpositionsintheworkspace,
andthentheresultingrobottrajectoryintheswingingphasewascomparedtotheclosestteaching
trajectoryintheswingingphase. TheresultisshowninTable4.
Table4: DeviationfromthegroundtruthforExperiment2A,givenasnormalizedRMS.
Grasping-placing Grasping-swinging
NRMSD 0.10053 0.01514
ComparedtotheresultsobtainedinSection4.1.2,thenetworkgeneratedasimilarlowdeviation
forachievingthegraspingandswinginggoal, whilethedeviationwashigherforthegrasping
25
andplacinggoal. Thisislikelyduetotherelativelylowprecisionintrackingtheobject,especially
whenplacingtheobjectonthegoalplatform,whichwaslocatedatthefaredgeoftheworkspace.
Figure19presentstwoplotsshowinganexampleoftheplansgeneratedwhengiventhetwotypes
ofgoals. Examplevideosshowingtheseexperimentalresultscanbeseenatthislink.
(a) (b)
Figure19: Plotsgeneratedwhiletherobotisexecutingthegoal-directedplan. Toptobottom: the
sensorimotorhistory(goalinsetonthetopright,enlargedforvisibility),currentgeneratedplan,
Zinformation,andfreeenergy(darkorange: evidenceFE,lightgreen: expectedFE).Notethat
onlyaselectednumberofjointanglesareshownforclarity. (a)Anexampleofgrasping-placing,
asitreachesthedistalstep(redline),and(b)anexampleofgrasping-swinging,whichcancycle
indefinitely.
4.2.2 Experiment2B:Goalunderstanding
Finally,theexperimentalresultsforgoalunderstandingarebrieflydescribed. Inthisexperiment,
theobjectwasmovedbytheexperimenteremulatingthemanipulationoftheobjectbyTorobo
for each goal type. Torobo observed this object movement using object tracking while Torobo
remainedintheinitialposture,exceptforitsheadjoints,usedforobjecttracking. Thenetworkin
Toroboinferredtheexpectedgoalandpredictedthefuturemovementtrajectoryintermsofthe
sensorysequence,asshowninExperiment1B.Thisexperimentwasrepeated5timesforeachofthe
goal-directedgraspingactions,placingandswinging. Thenetworkwasjudgedtohavecorrectly
inferredthegoaliftheanticipatedgoalstablymatchedtheexperimenter’sactions, beforeplan
executionortheexperimenter’sactionsended. Avideoshowingthisexperimentcanbeseenatthis
26
link.
Theresultofthisexperimentwasthatthenetworkinferredthegoalcorrectlywith60%probabil-
itywhileobservingtheplacingactionswith100%probabilitywhileobservingtheswingingactions.
However,wenotethatthecapabilityofthenetworktocorrectlyinferthegoalandfutureactions
requirestheactionsofthehumangraspingthecylindricalobjecttocloselymatchtherobot’sown
learnedmovementimage,whichisnoteasyforhumanstoconsistentlyreproduce. Particularlyin
thecaseofgrasping-placing,precisetimingandpositioningraspingandplacingbecomecritical
andthelossofprecisioninthevisualtrackingoftheobjectatlongerdistancesposedadditional
challenges. When the demonstrated actions deviated from those learned by the network, we
observedthatthefreeenergyincreasedinsteadofdecreasingovertime,whichproducedunreliable
results. Afuturestudyshouldinvestigatemethodsofmakingthenetworkmorerobustinorderto
bettertolerateunreliablehumanfactors,whichwouldbeencounteredinrealworldsettings.
5 Discussion
Thecurrentstudyproposedanovelmodelforgoal-directedaction,planning,andexecution
underateleologicalframeworkusingthefreeenergyprinciple. Theproposedmodel,T-GLean,
ischaracterizedbythreefeatures. First,goalscanbespecifiedeitherbyaspecificsensorystate
expectedatadistalstepordynamicallychangingsensorysequences. Second,goalscanbeinferred
byobservedsensorysequences. Third,thegoal-directedplanisgeneratedbysituatingthelatent
statetotheobservedsensationbymeansoftheonlineinference.
Theproposedmodelwasevaluatedbyconductingtwoexperiments,thefirstusingasimulated
mobileagentfornavigationandthesecondusingaphysicalhumanoidforobjectmanipulation.
Theresultsofexperimentsusingasimulatedmobileagentshowedthatgeneralizationingeneration
ofreachingmovementstounlearnedgoalpositionsissufficientwitharelativelysmallnumberof
trainingsamples,withmodestimprovementasthenumberofteachingtrajectoriesisincreased. It
wasalsoshownthatbothtypesofgoal-directedplangenerationandtheirexecution,i.e.,reachinga
specifiedpositionandcyclingcouldbeperformedprecisely. Furthermore,itwasdemonstratedthat
goalscouldbeinferredadequatelyfromtheobservedsensation,eveninthecaseofdynamically
changinggoals. Finally,itwasshownthatthenetworkcouldgenerategoal-directedreachingplans
with the shortest path when an additional cost for travel time minimization was added to the
originalplanfreeenergyformula. Thisconfirmsthatthecurrentmodelusingthismodifiedplan
freeenergycangenerateoptimalgoal-directedplansunderwell-posedconditions.
Intheresultsoftheexperimentsscaleduptousingarealhumanoidrobot,itwasshownthat
goal-directed plan generation and execution, as well as goal inference by observation could be
performedwithreasonableperformancefortwodifferentgoal-directedactions,grasping-placing
andgrasping-swinging,althoughtheirperformancewasslightlyworsecomparedtothesimulated
mobileagentcase. Thiscouldbeduetovariousrealworldconstraints,includinglimitedprecision
inthevisualtrackingsystemandinmotorcontrol,aswellasunreliablehumanbehavioralfactors
in demonstrating emulated goal-directed actions to the robot. There is still plenty of room for
improvingperformanceinsuchreal-worldsituationsbymakingtechnicaleffortsinvariousregards.
Variousresearchtopicscanbeconsideredforextendingthecurrentstudyinthefuture. One
interestingtopicwouldbetoexaminehowrobotscandealwithunexpectedenvironmentalchanges,
usingthecurrentmodel. Forexample,ifTorobofailstograsptheobjectordropsit,canitrecover
fromthefailurebygeneratinganewrecoveryactionplan? Itwouldbeinterestingtoexaminehow
27
muchsuchanunexpectedsituationcanberecognizedbyinferringthelatentstatebymeansof
minimizationoftheevidencefreeenergyappliedtothepastwindow. Thismayrequireadditional
learningofvariousfailuresituationssothatnovelsituationscanbeadequatelyhandledthrough
generalization,whilemaintainingwell-posedsolutionsfornormalsituations.
Anotherdirectionforfuturestudywouldbefurtherscalingupinactionandgoalcomplexityby
introducingalanguagemodality. Byusingthepowerofrichlinguisticexpressions,itisexpected
thatvariouscomplexgoalscanberepresentedinacompositionalmanner. Itis,however,verylikely
thatitwillbequitedifficulttolearnanadequateamountoflanguagerelevanttogoal-directed
actions with different levels of complexity at once. With regard to this problem, one plausible
but challenging approach may be the introduction of developmental pathways in learning. It
wouldbenaturaltostartbylearningasetofsimplegoalrepresentationsthatcouldbeachievedby
someprimitivebehaviors. Whenlearningproceedsfurther,morecomplexgoal-directedactions
couldbelearnedbymeansofcompositionsoftheprior-learnedactionprimitivesassociatedwith
corresponding compositional linguistic expressions. This might lead to acquisition of a more
abstractgoalrepresentationattheconceptuallevel.
References
[1] ScottRSehon. Goal-directedactionandteleologicalexplanation. CausationandExplanation,
pages155–170,2007.
[2] GuidoLo¨hrer. Actions,reasonexplanations,andvalues. Tuttiidirittiriservati,page17,2016.
[3] GergelyCsibra,SzilviaB´ıro´,OrsolyaKoo´s,andGyo¨rgyGergely. One-year-oldinfantsuse
teleologicalrepresentationsofactionsproductively. CognitiveScience,27(1):111–133,2003.
[4] Maeda Kawato, Y Maeda, Y Uno, and R Suzuki. Trajectory formation of arm movement
by cascade neural network model based on minimum torque-change criterion. Biological
cybernetics,62(4):275–288,1990.
[5] RChrisMiallandDanielMWolpert. Forwardmodelsforphysiologicalmotorcontrol. Neural
networks,9(8):1265–1279,1996.
[6] MitsuoKawato. Internalmodelsformotorcontrolandtrajectoryplanning. Currentopinionin
neurobiology,9(6):718–727,1999.
[7] KarlFriston,FrancescoRigoli,DimitriOgnibene,ChristophMathys,ThomasFitzgerald,and
GiovanniPezzulo. Activeinferenceandepistemicvalue. Cognitiveneuroscience,6(4):187–214,
2015.
[8] Thomas Parr and Karl J Friston. Generalised free energy and active inference. Biological
cybernetics,113(5):495–513,2019.
[9] KarlFriston,Je´re´mieMattout,andJamesKilner. Actionunderstandingandactiveinference.
Biologicalcybernetics,104(1):137–160,2011.
[10] Karl Friston, Spyridon Samothrakis, and Read Montague. Active inference and agency:
optimalcontrolwithoutcostfunctions. Biologicalcybernetics,106(8):523–541,2012.
28
[11] ManuelBaltieriandChristopherLBuckley. Anactiveinferenceimplementationofphototaxis.
InArtificialLifeConferenceProceedings14,pages36–43.MITPress,2017.
[12] KarlFriston. Atheoryofcorticalresponses. PhilosophicaltransactionsoftheRoyalSocietyB:
Biologicalsciences,360(1456):815–836,2005.
[13] TakazumiMatsumotoandJunTani. Goal-directedplanningforhabituatedagentsbyactive
inferenceusingavariationalrecurrentneuralnetwork. Entropy,22(5):564,May2020. ISSN
1099-4300. doi: 10.3390/e22050564. URLhttp://dx.doi.org/10.3390/e22050564.
[14] AhmadrezaAhmadiandJunTani. Anovelpredictive-coding-inspiredvariationalrnnmodel
foronlinepredictionandrecognition. Neuralcomputation,31(11):2025–2074,2019.
[15] ShinsukeShimojo. Postdiction: itsimplicationsonvisualawareness,hindsight,andsenseof
agency. Frontiersinpsychology,5:196,2014.
[16] JunTani. Learningtogeneratearticulatedbehaviorthroughthebottom-upandthetop-down
interactionprocesses. Neuralnetworks,16(1):11–23,2003.
[17] Jun Tani. Model-based learning for mobile robot navigation from the dynamical systems
perspective. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 26(3):
421–436,1996.
[18] DanijarHafner,TimothyLillicrap,JimmyBa,andMohammadNorouzi. Dreamtocontrol:
Learningbehaviorsbylatentimagination. arXivpreprintarXiv:1912.01603,2019.
[19] YuichiYamashitaandJunTani. Emergenceoffunctionalhierarchyinamultipletimescale
neural network model: a humanoid robot experiment. PLoS computational biology, 4(11):
e1000220,2008.
[20] WataruOhataandJunTani. Investigationofthesenseofagencyinsocialcognition,based
onframeworksofpredictivecodingandactiveinference: Asimulationstudyonmultimodal
imitativeinteraction. FrontiersinNeurorobotics,14:61,2020. ISSN1662-5218. doi: 10.3389/fnbot.
2020.00061. URLhttps://www.frontiersin.org/article/10.3389/fnbot.2020.00061.
[21] DiederikP.KingmaandMaxWelling. Auto-encodingvariationalbayes. InYoshuaBengioand
YannLeCun,editors,2ndInternationalConferenceonLearningRepresentations,ICLR2014,Banff,
AB,Canada,April14-16,2014,ConferenceTrackProceedings,2014. URLhttp://arxiv.org/abs/
1312.6114.
[22] MartinVButz,DavidBilkey,DaniaHumaidan,AlistairKnott,andSebastianOtte. Learning,
planning,andcontrolinamonolithicneuraleventinferencearchitecture. NeuralNetworks,117:
135–144,2019.
[23] JosephRedmonandAliFarhadi. Yolov3: Anincrementalimprovement,2018.
29

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Goal-directed Planning and Goal Understanding by Active Inference: Evaluation Through Simulated and Physical Robot Experiments"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
