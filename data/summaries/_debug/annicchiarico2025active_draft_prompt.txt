=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: An Active Inference perspective on Neurofeedback Training
Citation Key: annicchiarico2025active
Authors: Côme Annicchiarico, Fabien Lotte, Jérémie Mattout

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Abstract: Neurofeedbacktraining(NFT)aimstoteachself-regulationofbrainactivitythroughreal-
timefeedback,butsuffersfromhighlyvariableoutcomesandpoorlyunderstoodmechanisms,
hampering its validation. To address these issues, we propose a formal computational model
of the NFT closed loop. Using Active Inference, a Bayesian framework modelling perception,
action, and learning, we simulate agents interacting with an NFT environment. This enables
ustotesttheimpactofdesignchoices(e.g.,feedbackquality,biomarkervali...

Key Terms: team, control, lyon, brain, france, learning, inference, active, neurofeedback, training

=== FULL PAPER TEXT ===

An Active Inference perspective on Neurofeedback Training
Comˆe ANNICCHIARICO1,2, Fabien LOTTE2, and Jérémie MATTOUT1
1COPHY Team, Lyon Neuroscience Research Center, CRNL, INSERM, U1028,
Lyon, FRANCE
2POTIOC Team, INRIA centre at the University of Bordeaux / LaBRI, Talence,
FRANCE
2025-05-07
Abstract
Neurofeedbacktraining(NFT)aimstoteachself-regulationofbrainactivitythroughreal-
timefeedback,butsuffersfromhighlyvariableoutcomesandpoorlyunderstoodmechanisms,
hampering its validation. To address these issues, we propose a formal computational model
of the NFT closed loop. Using Active Inference, a Bayesian framework modelling perception,
action, and learning, we simulate agents interacting with an NFT environment. This enables
ustotesttheimpactofdesignchoices(e.g.,feedbackquality,biomarkervalidity)andsubject
factors(e.g.,priorbeliefs)ontraining. Simulationsshowthattrainingeffectivenessissensitive
tofeedbacknoiseorbias,andtopriorbeliefs(highlightingtheimportanceofguidinginstruc-
tions),butalsorevealthatperfectfeedbackisinsufficienttoguaranteehighperformance. This
approachprovidesatoolforassessingandpredictingNFTvariability,interpretempiricaldata,
and potentially develop personalized training protocols.
1 Introduction
Brain-Computer Interfaces (BCIs) encompass a large number of technologies specialized in ac-
quiring brain signals, analyzing them and translating them into commands or information for an
external system [SKW12]. Neurofeedback is a specific use-case of BCIs in which the signals, typi-
callyacquiredusingelectroencephalography(EEG),areaimedatpromotingtheself-modulationof
brain activity. The ability of animals to control their own brain activity through Neurofeedback-
driven operant-conditioning has been established as early as the 1970’s [Fet69]. Beyond those first
experiments, the nature of the learning mechanisms that would allow a human to control a BCI
is still very much debated [WKWN14, HCW+15, SRS+17, CCS+20]. In fact, effective control of
such interfaces has proven very difficult or even impossible for a significant proportion of users
leading to the emergence of the debated concepts of BCI illiteracy [VB10, Tho19] and Neurofeed-
back non-responders [AARST18].
Neurofeedback training attempts to teach subjects how to control specific biomarkers derived
from their own brain activity (or neuromarker). A neuromarker designates a measurable neuro-
physiologicalactivitythatisassumedtospecificallyandtruthfullyreflectapsychologicalormental
stateofinterest. ThemostwidespreadNeurofeedbackprotocolsfeaturingEEGmeasurementstar-
getspontaneousbrainrhythms,thatisoscillationsinspecificfrequencybands[MMM16,ABB+17],
with a relatively loose spatial resolution. Other, less common approaches involve different brain
markers(suchasconnectivityestimates)ormoreinvolvedmeasuringapparatilikefunctionalMag-
netic Resonance Imaging (fMRI) [BBPD20] or magnetoencephalography (MEG) [OHL+15], with
better spatial resolution but narrower out-of-the-lab application perspectives.
[EGSPA19]enumeratesthreemajordomainsofapplicationofNeurofeedback. First,NFTtrain-
inghasbeenusedbyresearcherstoimprovehealthysubjectscognitiveperformances[Gru14,Gru13,
YHKI17]. Thisapproachreliesonbrainplasticitytoinitiatedirectionalchangesinhealthysubjects
behaviour,inareassuchasattention,spelling,confidence,etc. [LZH21]. Anotheremergingfieldof
useofNFTisasascientifictool[SRS+17]. Indeed,Neurofeedbackcanbeusedtofavourtheemer-
gence of specific neurophysiological patterns and study the associated behavioural ones [BBPD20]
1
5202
yaM
6
]CN.oib-q[
1v80330.5052:viXra
Figure 1: A schematic representation of the neurofeedback closed-loop paradigm. 1. The subject
physiological brain signals (electromagnetic, BOLD signal, etc.) are acquired using EEG, MEG,
fMRI, etc.2. They are used to infer the hidden subject cognitive state that caused it. The experi-
menterderivesatask-specificfeedbackfromtheinferedstate. Notethatthislatentstateinference
relies on strong hypotheses made by the experimenter about parts of the neurophysiological /
measuring process underlying the neurofeedback. 3. The subject perceives the feedback through
various sensory means. (auditory, visual, etc.) and tries to relate it to its own cognitive state
(4.).The subject thus tries to learn the relationship between his hidden cognitive states and the
indicatordisplayedonthescreen. Ontheleft,afewotherfactorswhichmayinfluencethecognitive
dynamics of the subject training are shown.
aswellastheassociatedsubjectivereports. Mostneurofeedbackstudieshoweverfocusonthedevel-
opment of non-pharmacological therapies for psychiatric and neurological disorders such as Atten-
tional Deficit and Hyperactivity Disorder (ADHD). [EGSPA19, ACT+20, HSHB20, DAH+19], in-
somnia[HPH+82,SGG+17,LBJPB21, HCBI11,LLZ+22], anxiety[MPP+11,HZL+21,CXB+21],
epilepsy[ZWLH09,SBWK14],chronicpain[RdlVJM20],etc. SuchNFTparadigmsusuallyaimat
having asignificant (positive)impact onbehavioral symptoms(e.g. focusing andlearning abilities
in children with attention disorder).
Wide-scale adoption of neurofeedback protocols for therapy by the general public has radically
increased in the last decade, yet no consensus on the efficacy of neurofeedback therapies has been
reached by the scientific community [LSSO95, MFGF+14, CFB+16]. One of the principal chal-
lengesin evaluatingthe overallefficacy ofNFTlies initsbroad therapeutic claims, which haveled
toitsapplicationacrossadiverserangeofbiomarkers. Thisdiversitycomplicatesthecomparisonof
findingsacrossdifferentstudies. Furthermore,thedisorder-specificrelevanceofthesebiomarkersis
often inadequately validated [BBC+19]. Typically, researchers rely on empirical evidence showing
a significant correlation between a cognitive disorder (e.g., attention deficit) and a specific neuro-
marker deviation (e.g., the theta/beta power ratio as measured by EEG). They hypothesize that
normalizingthisdeviationwillconsequentlyenhancecognitivefunctionintheaffectedindividuals.
To test these hypotheses and disambiguate effects beyond placebo or non-specific training factors,
double-blind randomized control trials have become the gold standard in Neurofeedback studies.
However, randomized controlled trials are particularly cumbersome to set up. The outcome of
existing studies are difficult to synthesize as they differ in many aspects including the targeted
population and biomarkers, the design and protocol used, etc. Moreover, sample size is often
limited. As a result, evidence is scarce and fragile [MMM16, EGSPA19, TKLM20, RMYBCSZ21].
For example, although neurofeedback therapy for ADHD patients has become common practice,
several studies concluded that changes in subjects’ behaviour cannot be proved to be more than a
placebo effect [LGH+07, AS13, CTCB13]. Even if the training is successful on the short term, the
long-termeffectsofneurofeedbacktherapiesareoftenlacklusterandinneedofcomplementarysolu-
tions[MHS+09]. Thishasledtodebatesregardinghowexperimentsshouldbeperformed: whatis
theaimofthetraining,howtoimplementapropercontrolconditione.g. withshamfeedback,how
to evaluate the effect and specificity of the training, etc. [DBVSWB13, CPS+14, vDBVSWB14].
2
In recent years, although a few experimental studies and meta-analysis concluded in favor of
encouraging results [ARS+09, ABB+17, PBAEG21], others pointed the empty part of the glass.
This obvious difficulty faced by the community to draw solid and common conclusions, has led to
a call for protocol and reporting standardization [MFGF+14, TVOR18, REGZ+20]. Moreover, to
explaintheinconsistencyoftheresults,researchershavealsoshowninterestincomparingmethod-
ological and technical details of those approaches and proposing different explanations for success
or failure [MMM16, CFB+16, BCB+19].
This joint effort and strong call for strengthening our methodology is very encouraging and a
clear sign of maturation of the young science of neurofeedback. However, it still lacks a common
generic description of the training process, used to clearly articulate our hypotheses around our
partial understanding of the mechanism at play during NFT. This missing piece is a mandatory
next step to further guide the development of neurofeedback, as it should act as a common lan-
guage for researchers to 1. understand what are the fundamental goals of NFT, 2. formalize
what hypotheses are made when designing a training paradigm, 3. quantify the effect of explicit
and implicit training factors in order to predict the outcome and 4. communicate their results
and interpret the ability (or inability) of the subjects to control their brain activity. This paper
shows that the Active Inference framework[FFR+16], briefly outlined in 2.2.1, can be leveraged to
provideacompletedescriptionofBCI/NFtraining. Intheremainderofthisintroduction,wegive
the reader an overview of the main components of a neurofeedback training loop and of the main
factors explaining the variability between outcomes. Then, we give a quick overview of existing
modeling works that tackled the NFT problem. Finally, we motivate the introduction of a novel
high-level subject-centered model of BCI interaction to account for all the previously explained
factors.
Neurofeedback uncertainties and hypotheses : Neurofeedback training relies on numer-
ousfunctionalcomponents. Therehavebeeneffortsbythescientificcommunitytoformalizewhich
elements of the neurofeedback pipeline could explain training failures. [ABB+17] summarizes the
main advantages and pitfalls of neurofeedback in children with ADHD. The authors point out
two categories of issues that mostly affect the efficacy of the training: technical ones related to
recordings or trial design parameters (signal quality, processing algorithms, reward threshold and
timing, feedback nature, session structure and frequency, trial-based and session-based learning
curves, experimenter’s influence, transfer exercises...) and issues related to learning mechanisms
(learnability, perceptibility, mastery, motivation, autonomy). Predictive approaches such as those
introduced in [AARST18, WEE20] focus on figuring out which neurophysiological markers could
explain training success or failure, and which mechanisms explain the differences of outcomes be-
tween subjects.
Finally,[BBC+19]proposeseveralintrinsicfactorschallengingneurofeedbackefficacyandslow-
ing its recognition as a valid therapeutic method by the larger neuroscientific community. First
is the biomarker hypothesis used to infer subject cognitive states. We still struggle to understand
the exact relationship between brain activity features and cognitive dimensions. It is also chal-
lenging to understand how a mental disorder affects brain activity. This limits attempting to use
brain-wide physiological signals as a vector for therapy requires to specific, reliable and observable
marker [YHY+17]. Second, a lot of neurofeedback training factors are subject-specific. We have
a very superficial knowledge on how to best fit the neurofeedback loop to the subject, and the
effect of endogenous factors (motivation, attention, drowsiness,etc.) and exogenous factors (effect
of instructions, task set, feedback design) is still hardly quantified. Finally, there is a significant
lack of understanding of the mechanisms behind conscious regulation of brain mechanisms. This
broadexpressionhidesasemanticambiguitybetweentwokeyquestionswithregardtothepractice
of neurofeedback : (1.) How is the subject learning ? and (2.) What is the subject learning ?
Although related, they tackle different angles of the paradigm :
• "How is the subject learning ?" targets the pseudo-algorithm that best describes subject
knowledge gathering during a training session. The nature of most NF exercises as ex-
ploratory tasks with the goal of optimizing a positive feedback naturally points towards op-
erantconditioning(Skinner)[GHA+09a]. Subjectsfavoractionsthatledtopositive(changes
in the) feedback, and refrain from actions that produced opposite effects. However, operant
conditioning alone struggle to tackle the issue of generalization after training, in the absence
of an explicit reward. An alternative view, that we will showcase in this paper, formalizes
3
the training as learning an efficient representation of the full neurofeedback loop. Instead
of learning what actions lead to positive feedbacks, subjects learn how their actions affected
their (mental) environment, with increasing the feedback being just part of their drive.
• "Whatisthesubjectlearning?"focusesonthegoalofthetrainingandtheeffectofthesessions
onthedynamicsofthesubject. Althoughintimatelylinkedtothepreviouspoint,itfocuseson
whatpartofthesubjectdynamicsareaffectedbythetraining. Inoperantconditioning,what
islearntisoftenthemappingbetweenastimulus(e.g. alowleveloffeedback)andanaction
probability. This is somewhat limited as it struggles to explain why such a training would
have any usefulness in the absence of an explicit feedback (in effect reducing the true NF
effect to a simple habit). Two potentially compatible elements may mitigate this fact : first,
NFTmaybeawayforthesubjecttolearnhowtointerpretimplicit(interoceptive)feedback
signals (heart rate, metacognitive awareness, etc.). By associating these signals with the
performed actions during training (following some kind of classical conditionning (Pavlov)),
the subject becomes able to use this knowledge outside of the training environment. Second,
subjectsmaylearnhowactionsaffecthiddenstatesratherthanwhatactionstoperformgiven
a specific feedback. This is in line with [VKC21] who argue that skill learning [GHA+09b],
defined as a conscious and active mechanism of model building following observations, plays
an integral part during neurofeedback training.
Factors of neurofeedback efficacy : Numerouspivotalelementshavebeendemonstratedto
strongly influence the outcome of BCI training. Ideally, models of Neurofeedback training should
capture one or several of the following effects :
• F1 : biomarker specificity. We still have superficial knowledge about brain dynamics
and the scientific consensus about how physiological markers relate to a behavioural (and
cognitive) features is often approximate and consistently shifting [PCB+20]. Nonspecific
biomarkers may be affected by several cognitive dimensions, and regulating them may lead
tounwantedcognitivealteration: forinstance,motorimagerytraining[ZCY+22]maycause
anxiety [CXB+21] because the chosen biomarkers may share common features. Training a
wide array of possible cognitive states may induce several possible limitations: the cognitive
training can fail due to the subject focusing on non-targeted cognitive states that prompt a
positive feedback [MJL14, RBCC13, GSF+17]. Physiological markers depending on several,
possiblyunrelatedcognitivestatesalsoconstitutepoortrainingindicators,astheirvaluecan
changedependingonstatecombinationsandpreventefficientlearning. Finally,thecognitive
pathwaysinvolvedinself-regulatingmayaffectunwantedcognitivedimensions(e.g. "tofocus
my attention, i have learnt to act in a way that also causes anxiety...").
• F2 : measurement noise. The brain signals used to provide a feedback to the subject
may be significantly altered during measurement. It may be due to deformation/diffusion
through the subject’s head, measurement noise sources, loss of resolution, blind spots, etc.
The experimenter uses a forward model to reconstruct the raw physiology of the brain
using measurements, but these inferences may be flawed and induce significant error sources
: for instance, high feedback noise may prevent subjects from effectively finding out which
mental strategy worked well to increase the feedback / find patterns in how the feedback
respondstotheirmentalactions. Thismayrenderthetraininguselessastheindicatorseems
uncontrollable or random.
• F3 : user factors (both exogenous and endogenous). Severalfactorslinkedtopatient
cognition and behaviour have a significant influence on the course of a BCI training. Ex-
ogenous elements such as the experimenter instructions before and during the training, the
task set or the form of the feedback provided to the subjects have been recognized as signifi-
cantfactorsoftrainingsuccess[JLB+18,REGZ+20,RPM+21]. Theinfluenceofendogenous
factorssuchassubjectfatigue,motivationorparticularemotionsonBCItraininghaveasig-
nificant impact on training [RBCC13, MJL14, GSF+17], but these effects have also proven
difficulttoexplain. Somemodelingstudies,suchas[OLPS17],havemadeuseofspontaneous
noise in the brain activity as a way to describe internal endogenous perturbations, but as
far as we know, these elements haven’t yet been the object of explicit quantitative modelling
and we believe it constitutes a key point to explain training outcomes. Thankfully, recent
approaches in the field of predictive coding and Active Inference have taken to model the
cognitiveeffectofsomeendogenousfactors(mindfulness,emotions),andprovidedconceptual
frameworks to study their effect on training [HSP+21, SSHL+20].
4
• F4: learningmechanisms. BecauseBCI/Neurofeedbacktrainingarelearningparadigms,
theyrequiretheexperimentertodesignawellthought-outenvironmenttoguidetheevolution
of the subject. The nature of the learning mechanisms at play during BCI training has been
wildly debated (see above).
• F5 : placebo effect. Finally, the impact of placebo effects on the training outcome has
proven hard to disentangle from "true" NF effect [OMHT23]. The prevailing consensus in
neurofeedback studies is to systematically assess Neurofeedback protocols through double-
blind studies [FMFV+17, SGG+17, REGZ+20]. This has led [TLR17] to coin the term of
"superplacebo" for NFT : a paradigm in which both experimenter and subject believe in the
therapeutic benefits of a practice despite of the lack of supporting evidence, leading to an
increased placebo effect and more biaised reporting . Other approaches underlined that the
placeboeffectwasaintegralpartoftheneurofeedbacktraining[OLRV21],playinganeutral-
ization or amplification role in the training efficacy. Whatever argument eventually prevails,
thenon-specificmechanismsofNFTrelatedtothetechnologyandtheuser[WK18,PRNL21],
withinoroutsideofthetrainingproper,arecrucialfeaturesofthesesstudies,whilestillsome-
times under-reported.
Existing modeling works : Modeling approaches have built upon those theoretical leads to
createcomputationalrepresentationsofneurofeedbacktraining. Theobjectiveofthesestudiesisto
capturepartofthedynamicsinherenttoneurofeedbacktraining, explainthecurrentexperimental
results and hopefully predict the impact of some parameters on the training outcome. Previous
formalizationapproacheshavefocusedondefiningthekeyconceptsofneurofeedback, mostpartic-
ularly subject feedback perception, feedback control and learning. Gaume et al. [GVMS+16] have
proposedaformaldescriptionofthepsychologicaldynamicsinvolvedinneurofeedbackandderived
a control-theory inspired model of NFT, featuring both implicit and explicit feedback modalities.
However, they did not provide an explicit computational model describing the dynamics of the
training . Davelaar has proposed multi-stage,anatomical-based model of EEG brainwave regula-
tionthroughneurofeedback[Dav18]aswellasmulti-stagemodelofneurofeedbacksubjectlearning
phases [Dav20]. In the former, pools of excitatory and inhibitory neurons were used to generate
artificial EEG data, as described in [Izh03]. An artificial striatal unit made of 1000 binary neu-
rons controlled the peak alpha frequency chosen as marker. The agent looked for the responsible
group of neurons through incremental probability updating using the feedback provided to him.
This approach was a low-level model of neuron selection which described the physiological path-
ways involved during self-regulation of the subjects but did not tackle higher-level function. For
instance, the model did not describe training effect on user mental state or the impact of prior
biases. [OLPS17] investigated the influence of temporal factors on subjects ability to self-regulate.
Itproposedamodelofsubjectself-regulationduringfMRIneurofeedbacktraininginvolvingadiffi-
cultcreditassignmentproblem, modulatedbyfeedbackdelayandnoise. Importantly, two distinct
learning structures were compared to test the influence of different subject learning strategies.
Cognitive learning was meant to represent explicit strategies the subjects could pick when faced
withthefeedback,whereasautomaticlearningwasassociatedwithamoreimplicitmodalitywhere
the subject tried to control an unconstrained set of cognitive states given a feedback signal, in a
formulation reminiscent of reinforcement learning (RL). More recently, a study by Lubianiker and
colleagues [LPDH22] featured a family of models similar to the ones shown in this paper, that
mapped traditional NFT components to canonical (model-free) RL elements. According to this
formulation, the feedback provided to the subjects constitutes states and the NF subjects are as-
similated to RL agents having to pick actions by learning a value function, relating those states
and potential mental actions. Although the proposed framework constitutes an adequate approx-
imation for the initial interaction between a subject and the BCI loop, it struggles to account for
later parts of the training, especially the ability of subjects to learn a more refined representation
of the environment or transfer the acquired knowledge in another environment. We argue that a
more model-based account of subject experience during NFT is needed to account for these phe-
nomenonsandtocapturetheeffectofendogenousfactorssuchassubjectmotivation,experimenter
instructions, and more generally to allow for a better interpretability of training mechanisms and
outcomes.
Motivation : This paper aims at improving upon the previous approaches to by providing an
alternative perspective on BCI/neurofeedback training. All in all, this paper answers a triple need
expressed by the neurofeedback community :
5
• 1. WeproposeacompletefamilyofmodelsbasedontheActiveInferenceframework,allowing
ustodescribesubjectlearningduringNFT,fromtheinitialexploratorystepstothetransfer
learning. Thisfamilyofmodelsisgenericandmayfeatureanumberofalternativehypotheses
regarding the driving mechanisms / goals of training.
• 2. We show how one may use this formulation to clearly frame NF training goals and
hypotheses: What kindof changeis my neurofeedback training aiming forwithin thesubjects
? What are my assumptions regarding the biomarker I am using ? How do I expect my
instructions to affect the training ?
• 3. Weshowthatthisfamilyofmodelsmaybeusedtoanswerresearcherquestionsandprovide
broad estimators for subject parameters. If the experimenters have precise expectations
regardingsomekeyparametersofthetraining(noiseofthebiomarker,layoutofthecognitive
states of the subject, etc.) , we may be able to explain variability in results and eventually
predict subject training curves depending on their internal parameters. This may eventually
lead to increased therapeutic efficiency through the design of training paradigms tailored for
each individual.
Our approach breaks from the previously established works by modeling the evolution of sub-
ject beliefs during training. Similarly to [LPDH22], we describe the whole BCI loop as an action
selection problem, but we leverage Active Inference to equip the subjects with a broader variety
of internal representation(s) of their environment. This allows us to explicitly model subject per-
ceptual uncertainty, the effect ofprior beliefs about their environment (beliefs about the feedback,
about their ability to control their brain activity), of experimenter instructions, of various mental
representations, etc. Contrary to the works of Davelaar and Oblak, our formulation focuses on
cognitive aspects of training and does not make explicit the brain activity of the subjects, instead
considering a direct mapping between their cognitive activity and the provided feedback.
2 Methods
2.1 Learning through interaction: a cognitive model of BCI Training
Neurofeedback , and more generally BCI interaction, can be understood as a distinctive form of
human–machineinteractioncharacterizedbyabidirectionallearningprocessbetweentwoprimary
agents: the subject and the experimenter. In this setting, the subject engages with an artificial
environment using modulations of their own neural activity, while the experimenter designs and
calibrates the environment’s responses to these neural signals. The efficacy of neurofeedback thus
depends on the internal models and assumptions held by both parties: the subject’s evolving
understanding of how to interact with the system, and the experimenter’s hypotheses about the
neural correlates of cognitive or behavioral traits.
The experimenter : Whendesigningthefeedbackloop, theexperimentertypicallypresumes
a relationship between measurable neurophysiological patterns and certain behavioral or cognitive
traits of interest—for instance, enhanced theta activity or decreased beta activity in individuals
with attentional deficits. Our framework introduces an intermediary conceptual construct—the
cognitive system—which serves to link observable behavior with measurable brain activity. Thus,
targeting a neural biomarker implicitly entails targeting a constellation of cognitive states that
can be externally inferred via behavior. For example, neurofeedback interventions for conditions
such as ADHD or insomnia often aim to modulate attentional processes [ACT+20, MMM16], un-
der the assumption that these are reflected in specific EEG signal features [MFBF+19, PCB+19].
This cognitive model allows us to decompose the neurofeedback interaction into two interrelated
components: (1) a biomarker hypothesis, which posits a mapping between cognitive states and
neurophysiological signals, and (2) a forward model, which describes how brain signals are pro-
cessed and transformed into feedback stimuli. In this light, the feedback presented to the subject
constitutes an inferred estimate of their ongoing cognitive state—for instance: "The feedback is
low, which means that your attention level is low".
The subject : In this environment, subjects have to perform two tasks simultaneously : a
perception task and a decision making task. First, they must make sense of the feedback ("The
feedback is low, what does it mean regarding my attention level ?"). Second, they must figure out
a mental strategy that allows them to increase the feedback or control its level voluntarily (How
can my actions change my cognitive state in order to perform well ?). Note that in this case,
6
the actions performed are mental actions (concentrate, perform mental imagery, recall a memory,
innerspeech,etc.). Tobeabletobetternavigatethisenvironmentacrosstraining,subjectsneedto
learntherulesoftheneurofeedbackenvironmenttoperformmoreinformedinferences. Ourmodel
assumesthatthesubjectsusetheprovidedfeedbacktobuildan(approximate)internalrepresenta-
tionofhis/herenvironmentandlearnfromanhistoryoffeedbackobservationsandmentalactions.
Note that the complexity of this representation need not match the full neurofeedback loop, but
it should allow them to navigate it as efficiently as possible. To be able to capture the effect of in-
trinsic factors within the NF subjects, like their prior knowledge of self-regulation, our model thus
explicitlydescribeshowthebeliefsofthesubjectabouttheircognitivestatesevolveacrosstraining.
To sum-up, we cast NF (and by extension BCI) training as a joint exercise of feedback percep-
tionandmentaldecision-makingandlearningforthesubject. Thisexerciseisparticularlyarduous
due to the numerous uncertainties surrounding the system (see 1) and to the numerous factors
affecting the training outlined above. Although the nature of the learning mechanisms leveraged
during neurofeedback/BCI training is still a matter of debate [EGHH17, VKC21], common ma-
chine learning paradigms have been proposed to account for such representation building. The
most notorious is probably that of model-free Reinforcment Learning [LPDH22], though we argue
that a more complex model-based approach grounded in the Active Inference framework provides
the basis for a broader family of subject representations that may be more applicable in the NF
context (e.g., explicit modeling of beliefs/uncertainty, handling partial observability, potential for
better generalization/transfer, explaining effect of instructions/priors). In the next section, we
introduce this framework and explain how it formalizes subject learning in BCI contexts.
For the remainder of this study, we separate an instance of neurofeedback training temporally
asfollows: thewholetrainingcomprisesT specifictrials(noted†∈[[1,T]]), whicharethemselves
made of T timesteps. We note t∈[[1,T]] an individual timestep.
2.2 A quick introduction to Active Inference
The Active Inference Framework (AIF) [FFR+16, SSPF20] is a broad term that encompasses a
family of models able to perform perception, action and learning through the minimization of
a single cost function (namely the Variational Free Energy). It provides a biologically plausible
framework for modelling subject cognitive states dynamics in response to feedback. This section
provides a concise overview of AIF’s theoretical foundations and its applicability to representing
learning processes in NF and related BCI paradigms.
2.2.1 Theoretical foundations
ActiveInferenceoperationalizestheBayesianbrainhypothesis[KP04],whichpositsthatthebrain
constructs and maintains a generative model of its environment to predict incoming sensory data
and guide adaptive behaviour. This perspective suggests that maintaining homeostasis (i.e., re-
maining within viable physiological bounds) requires organisms to continuously minimize predic-
tion errors through updating their internal model and selecting appropriate actions. Perception
corresponds to updating beliefs about the current causes of sensory input (model inversion), while
learning involves refining the model parameters over longer timescales [SSPF20].
A key tenet of AIF is the distinction between the generative process—the actual, often unob-
servable,dynamicsoftheenvironmentgeneratingsensorydata—andtheagent’sinternalgenerative
model—a probabilistic, approximate representation of this process (see Figure 2). The agent in-
teractswiththeexternalworldsolelythroughitssensoryinputs(observations)andmotoroutputs
(actions), which constitute the statistical boundary known as a Markov blanket. Consequently,
inferring the hidden states of the environment (s∗) based on observations (o) is fundamentally an
inference problem, constrained by the information available at this boundary. The brain must
entertain beliefs about the hidden causes of its sensations and update these beliefs in a Bayesian
manner. Selecting actions that either reduce uncertainty about the world (exploration) or lead
to preferred outcomes (exploitation) allows the agent to maintain accurate beliefs and ensure its
continued existence. The inherent limitations on computational resources necessitate a balance
between model accuracy and complexity, aligning with principles like Occam’s razor, which is
implicitly handled by the VFE minimization [MG05].
AIF formalizes the generative model as a joint probability distribution over observations o
and latent variables θ, P(o,θ) = P(θ)P(o|θ) (Eq. ). Here θ encompasses hidden states s, model
7
Figure2: Generativeprocessandgenerativemodel: thesubject’sbrainmodelstheneurofeedback
environment to predict outcomes and optimize its actions. Remarkably, BCI training casts cogni-
tivestatesasexternal"environment"variablesandincludestheminitsgenerativemodel. Itisthus
necessary to make a distinction between the hidden states themselves and how they are perceived
internally by the subject. The agent can only interact with its environment through boundary
states (its sensory and active states).
parameters, structure M and hyper-parameters ξ [DCPS+20]. We assume that observations are
caused by those parameters following the likelihood P(o|θ) . The subject initial belief about the
valuesofthosestatescanbeexpressedasP(θ)=P(s,ξ,M)(theso-calledprior). Toleveragethis
model, anagentneedstoperforminferenceoverthemodelparametersbycomputingtheposterior
distributionP(θ|o)statisticalinference. Ineffect, itneedstoanswerthefollowingquestion: Given
the current observable data, what would be the most likely causes in my model?. Thisisdoneusing
the famous Bayes’ Theorem :
LikelihoodPrior
z }| { z}|{
P(o|θ) P(θ)
P(θ|o) = (1)
P(o)
| {z }
Posterior |{z}
Evidence
R
However, calculating the normalization constant, the model evidence P(o)= P(o,θ)dθ =
θ∈Θ
R
P(o|θ)P(θ)dθ (equation ) is typically computationally intractable for complex models due to
θ∈Θ
the high-dimensional integration involved. As an example of the previous assertion, let’s imagine,
for a fixed model structure m and a given set of hyper-parameters ξ , a set of 3 latent states
0 0
s with 10 possible values each (which is arguably a very simple environment). Such a state
1,2,3
space leads to a summation over 1000 values of θ to compute an individual posterior P(θ |o), and
i
therefore 1000 computations of likelihoods P(o|θ). Inpractice, thisvulnerability tothe curseof di-
mensionalityhasledtoaneedtoeithershuntthecalculationofP(o)(asinMaximum-A-Posteriori
estimators) or approximate P(o) using methods like Monte-Carlo Sampling.
To circumvent this intractability, AIF employs Variational Inference (VI). VI reframes the in-
ference problem as an optimization problem by introducing an approximate posterior distribution
q(θ|χ)parametrizedbyvariationalparametersχ. Theaimofvariationalinferenceistominimizea
measure of distance between the true distribution P(θ|o) and our proposed approximation. Math-
ematically,wecandefinetheoptimalapproximationasthefunctionminimizingtheKL-divergence
between P and q :
q∗ =q(θ|χ∗) with χ∗ =argminD [q(θ|χ)||P(θ|o)]
KL
χ
8
Z q(θ|χ)
=argmin q(θ|χ)ln dθ (Continuous formulation)
p(θ|o)
χ θ∈Θ
X q(θ|χ)
=argmin q(θ|χ)ln (Discrete formulation)
p(θ|o)
χ
θ∈Θ
Where D is the Kullback-Leibler Divergence between two distributions. If D [Q||P] = 0,
KL KL
then Q = P and the inference problem is solved. Effectively, it means that the previous inference
problem, has been cast as a more ’classical’ optimization problem. [MSB21] Further transforma-
tions give :
P(θ,o)
D [q(θ|χ)||P(θ|o)]=D [q(θ|χ)|| ]
KL KL P(o)
| {z }
Errortominimize
=D [q(θ|χ)||P(θ,o)]+E [lnP(o)] (2)
KL q(θ|χ)
=D [q(θ||χ)||P(θ,o)]+ lnP(o)
KL
| {z } | {z }
VariationalFreeEnergy -Surprise
WecanthusdefineaquantityF =D [q(θ|χ)||P(θ,o)]whichverifiesF ≥D [q(θ|χ)||P(θ|o)].
KL KL
F is the Variational Free Energy, which creates an upper bound on the error between the true
posterior and the proposed approximation. The negative free energy term −F also provides a
lowerboundonmodel(log)evidence(ELBO).MaximizingtheELBO/minimizingthevariational
free energy would allow us to minimize "surprisal", which is defined as the negative log probability
of a given outcome −lnP(o) (not to be confused with psychological ’surprise’). Variational infer-
ence consists in minimizing Variational Free Energy (VFE) which mechanistically minimizes two
quantities: the difference between the true posterior distribution P(θ|o) and its approximation q
as well as surprisal, in effect maximizing model evidence. For the next steps, we assume a discrete
observation and latent state space.
X q(θ|o,χ) q(θ|χ)
F = q(θ|o,χ)ln =E [ln ]−lnP(o) (3)
P(o,θ) q(θ|χ) P(o,θ)
θ∈Θ
Byminimizingthefreeenergyfunctionalexpressedin3,agentsthusbuildincreasinglyaccurate
models of their environment.
Planning as inference: AIF extends this minimization principle to action selection. Agents
areassumedtoselectactions,orsequencesofactions(policiesπ),inordertominimizetheExpected
Free Energy (EFE) under a policy [SBPF21, FFR+17]. EFE quantifies the free energy expected
uponexecuting aparticularpolicy, considering futurepotential observationsand statetransitions.
By rewriting the heavy notation q(θ|χ) to q(θ|π), we get [SBPF21] :
F =D [q(θ|π)||P(θ|o,π)]− lnP(o) (4)
π KL
| {z } | {z }
Evidencebound Logevidence
Planning in such a way naturally balances the expected utility of outcomes (reaching preferred
states, encoded in prior preferences over outcomes) and the expected information gain (reducing
uncertainty about the hidden states or model parameters) [PF19a]. Our implementation utilizes
anexpansionoftheoriginalActiveInferenceplanningscheme: thesophisticatedinferencescheme
[FDCH+20],whichperformsatreesearchoverpossiblefutureaction-outcomesequencestoevaluate
EFE and select the optimal policy, albeit potentially computationally intensive for long planning
horizons.
Thisunifiedformulationallowsagentstobuildincreasinglycoherentinternalgenerativemodels,
beitthroughinferringthehiddenvariablesinthemodel,actingtoresolveuncertaintyaboutthem
or learning the intrinsic dynamics of the system. In essence, AIF posits that perception, action
selection, and learning all emerge from a single imperative: minimizing free energy (VFE for
perception and learning, EFE for action selection). This happens accross three different steps: 1)
during inference, the most likely hidden states are guessed using knowledge about hidden state
evolution and observations. This is a very short-term mechanic, which is leveraged once per
timestep during sequential simulations. 2) After state inference, agents perform actions to pursue
desired outcomes [SSPF20] and resolve uncertainty. 3) Finally, the agent changes its general
9
model of the world. This evolution affects wider timescales by updating its representation of
general environment dynamics. Together, these three instrumental mechanisms provide a general
description of agent planning and behaviour during a single trial as well as accross a large number
of trials :
Agentactions/policy
z }| {
With θ =( θ , θ , θ ) :
| st∈ {z [0,T } ] ut∈[0,T−1] |{ α z}
Beliefaboutlatentstates Agentgraphhyperparameters(seenextsection)

θ =argminF(θ,o ) (5a)
θ
u
s
t
t
=arg
θs
m
t
inF(θ,o
t
t
) (5b)
θut

θ
α
=argmin
X
T
F(θ,o
t
) (5c)
θα
t=0
With 5a the hidden state inference (computed every timestep), 5b the action selection (com-
puted every timestep) and 5c the environment dynamics learning (computed at the end of one
trial).
2.2.2 Discrete Active Inference : Variational Inference in a Markov Decision Process
To apply AIF to NF training, we model the subject-environment interaction using a discrete
PartiallyObservableMarkovDecisionProcess(POMDP),asdepictedinFigure3. Thisformalism
allows us to explicitly differentiate between the true environmental dynamics (generative process)
and the subject’s internal representation (generative model).
The generative process defines the objective reality of the NF loop. It is formalized using a
Hidden Markov Model which explicitely describes the true latent cognitive state of the subject sˆ,
t
as well as the rules of the environment :
• The true distribution of initial states P(sˆ ) parametrized by the upper-case vector D.
0
• ThetruetransitionsbetweenstatesinfluencedbysubjectactionsP(sˆ |sˆ,u )parametrized
t+1 t t
by the upper-case matrix B.
• The true mapping from these states to observable feedback (o ), P(o |sˆ) parametrized by
t t t
the upper-case matrix A.
These components proposed a description of the subject brain activity and feedback design and
remained fixed throughout a simulation.
The generative model represents the subject’s subjective beliefs and understanding of the NF
environment. Contrarily to the generative process, the generative model uses categorical prob-
ability distribution to represent the current (belief) state, denoted using the bold-case s . The
t
generative model includes the following mappings :
• Thesubject’sbeliefsabouttheinitialstatedistribution,P(s )parametrizedbythelower-case
0
vector d.
• Thesubject’sbeliefsaboutthestatetransitionruleP(s |s ,u )parametrizedbythelower-
t+1 t t
case matrix b.
• Thesubject’sbeliefsabouttheobservationmapping,P(o |s )parametrizedbythelower-case
t t
matrix a.
• Thesubject’spriorpreferencestowardscertainobservations,P(o )parametrizedbythelower-
t
case vector c.
• The subject’s prior over actions, representing biaises towards recurring strategies , P(u )
t
parametrized by the lower-case vector e.
These matrices have initial values that encode subject prior beliefs about their environments (e.g.
before training, how do I think the feedback relates to my hidden cognitive state ?), but may
evolvesignificantlythroughlearningacrosstrialsdependingonobservedoutcomes(feedbacklevels,
performedactions). Thisrepresentationallowstheagenttomaintainaneasilyinterpretablemodel
ofitsenvironment,andawaytoquantifyhowconfidenttheagentisaboutthedynamicsitmodels.
10
Figure 3: The Partially Observable Markov Decision Process (POMDP) used to model training.
The environment (generative) process figures a set of hidden states (ˆs ), corresponding to the
t
subject"actual"cognitivestatesinourformulation. Althoughimpossibletoseedirectly,eachstate
stochastically generates stimuli observable by the subject (o ) depending on a true observation
t
function A. It may consist in some external feedback or in an interoceptive observer the subject
must learn to interpret. Possible state transitions (B) and starting states (D) are fixed before
training. The subject generative model of the cognitive regulation is shown above, featuring a set
ofperceivedstates(s ). Thesubjectmodelsthefeedbackasarealizationofhiddenstateswiththe
t
function a, and the effect of his/her mental actions u on those states with the function b. The
t
Active Inference agent uses this formulation to update their model on two distinct timescales by
minimizingtheirFreeEnergyfollowingtheequationsdescribedin[FDCH+20,DCPS+20]. Onthe
timestep timescale, it infers the hidden states that best match observations and prior beliefs, and
the actions that best match its habits/preferences/exploration drive. On the trial timescale, the
agent updates its beliefs about the environment dynamics (a,b,d) to better predict and navigate
it.
11
2.2.3 Mathematical Formalism
This section details the core mathematical equations governing perception, planning, and learning
within the AIF-POMDP framework. We assume the subject potentially perceives multiple obser-
vation modalities (m) arising from multiple hidden state factors (f). Vectorized parameters are
bolded. For simplicity, we omit factor/modality superscripts where unambiguous.
Emission mappings: (formally P(o |s )).
t t
For the generative process :
For t∈[[0,T]],o ∼A[sˆ] (6)
t t
For the generative model :
For t∈[[0,T]],P(o |s ,a)=Cat(a s ) (7)
t t N t
Where a is the normalized a matrix to ensure it sums to 1 for each state s , ∼ is the sampling
N t
operation and Cat is the categorical distribution.
Initial states and transitions: (formally P(s ) and P(s |s ,u )) :
0 t+1 t t
For the generative process :
(cid:26) sˆ ∼D (8a)
0
For t∈[[0,T −1]],u∈[[1,U]], sˆ ∼B [sˆ] (8b)
t+1 u t
For the generative model :
(cid:26) P(s |d)=Cat(d) (9a)
0
For t∈[[0,T −1]],u∈[[1,U]], P(s |s ,u ,b)=Cat(b [u ]s ) (9b)
t+1 t t N t t
Where b is the normalized b matrix to ensure it sums to 1 for each state / action pairs s ,u .
N t t
Perception : Under the sophisticated inference scheme, the posterior belief update simplifies
to a combination of likelihood evidence and prior beliefs based on the previous state and action,
implementing a practical Bayesian filter :
s =σ(ln(a ·o )+ln(b [u ]s )) (10)
t N t N t t−1
Where σ is the softmax function and · is the inner product, meaning that a ·o =aTo .
N t N t
Planning and decision-making : Active inference agents attempt to minimize the expected
free energy [FFR+17] (EFE) of their model by performing actions that (a.) lead to preferred
outcomes(inourcase, ahighpositivefeedbacklevel)and(b.) improvetheagentknowledgeabout
its environment. The EFE can be seen as a sort of objective function when the agent plans its
next move. Sophisticated inference agents [FDCH+20] build a tree of future actions and outcomes
to plan their next actions. This planning scheme was chosen over traditional active inference
because it allowed for more flexible action selection without requiring predefined action sequences
(orpolicies). Foreachprospectivetimestepτ,anaction-outcomebranch(o ,u )hasthefollowing
t t
negative EFE :
G(u ,o )= g (su ,su,ou ) + uo .G(u ,o )ou
τ τ u τ+1 τ τ+1 τ+1 τ+1 τ+1 τ+1 (11)
| {z } | {z }
EFEforthetimestepτ+1 EFEforthesubsequenttimesteps
g (su ,su,ou )isthe(negative)EFEestimatorfortheprospectivetimestepτ+1(andonlythis
u τ+1 τ τ+1
one !) if the expected state and observations after action u and observation o are (su ,ou )
τ τ τ+1 τ+1
and the previous posterior su. In this study, we used the following (negative) EFE estimator :
τ
g (s,s ,o)= e[u] +o[lno+c]+ s.H − o·W s −s·W [u]s
u − a b −
|{z} | {z } |{z} | {z } | {z }
Habits Risk Ambiguity Novelty-seeking Novelty-seeking
(emissions) (transitions)
H=−diag(a·lna)
(12)
1
W = (a⊙−1−a⊙−1)
a 2 0
1
W = (b⊙−1−b⊙−1)
b 2 0
12
Where H is the entropy vector associated with a and W and W are novelty terms computed
a b
every timestep pushing subjects towards prospects of yet unexplored state transitions or outcome
generation. X is the sum of all the Dirichlet counts for a parameter matrix X and X⊙−1 is the
0
element-wise inverse of all elements of matrix X.
This results in a counterfactual planning scheme in which the agents visualize every plausible
future action-outcome paths and seek the ones who maximize the feedback preference (risk term)
butalsoallowagentstoimprovetheirknowledgeoftheenvironment(ambiguityandnoveltyseeking
terms). Finally, mental actions are picked through sampling the one resulting in the smallest EFE
:
u ∼σ(γG(u ,o )) (13)
t t t
With γ the action selection inverse temperature, leading to more random action selection when
γ →− 0 and deterministic action selection when γ →− +∞.
The Expected Free Energy (EFE) estimator presented here can be modified to account for
agents who are less novelty-driven by removing the weights W or W or changing the prior
a b
knowledge of the agents (accounting for prior instructions or beliefs about the dynamics of the
neurofeedbacksystem).Inpractice,thetreesearchdemonstratedwasconductedforshorttemporal
horizons, which is not an issue as we assumed a gradual preference matrix for the subjects.
Learning: agents use Bayesian learning to update their representation of latent environment
variables a,b,d through a correlation-based incremental process. After a trial, a subject uses pre-
vious observations and inferences to update its model through Bayesian belief updating. Because
the latter is generally hard to compute (for dimensional-related reasons explained before), Active
Inference uses the Dirichlet distributions as the conjugate prior for the categorical distribution :

d =d +s (14a)
b †
†
+
+
1
1
=b
†
†
+
T
X
†
−
,0
1
s
†,t+1
⊗s
†,t
⊗u
†,t
(14b)
t=0

a
†+1
=a
†
+
X
T
o
†,t
⊗s
†,t
(14c)
t=0
Where ⊗ is the outer tensor product and ∀t,†, s is the state posterior at time t computed during
†,t
trial †.
ThismeansthatthelearnedposteriorisalsoaDirichletdistribution, andcomputingtheposterior
given new data is made very easy [KE]. In essence, the Active Inference agent counts specific
state/observations co-occurrences and increments the Dirichlet parameters of its model (the a,b,d
distributions)accordingly. Thisallowsagentstolearnflexiblerepresentationsoftheirenvironment.
Thislearningmechanismoperatesonaslowertimescale(trial-by-trial)comparedtoperception
andactionselection(timestep-by-timestep). Thedurationoftrialscaninfluenceinitialexploration
patterns. Due to the accumulative nature of Dirichlet updates, prior beliefs become increasingly
entrenched with experience, potentially reducing flexibility in adapting to changes unless mech-
anisms such as parameter decay (’forgetting’) are introduced [HSP+21]. Importantly, effective
learning requires informative priors; agents with uninformative (e.g., flat) initial priors struggle to
build coherent models from observations [MKG22], underscoring the potential role of instructions
or prior experiences in facilitating BCI skill acquisition.
Numerous approaches [FFR+16, PF19b, FDCH+20, TSB20, DCPS+20, HSP+21, SSHL+20,
SFW21] have documented how Variational Free Energy and Expected Free Energy gradients are
computed with respect to subject model variables and parameters during state inference, action
inferenceandlearning. Inthissection,wehaveprovidedasimplifiedaccountofthemodelupdates
but we strongly encourage the interested reader to see the previously cited approaches for a more
complete explanation of the computations behind our simulations.
2.3 Neurofeedback training modeling with Active Inference
2.3.1 Models of training under uncertainty
Applying the AIF-POMDP framework allows us to formally model the subject’s experience and
learning trajectory during NF training, explicitly addressing the inherent uncertainties involved.
13
Neurofeedback training components Active Inference Component
The latent cognitive activity of the subject, driving the
Hidden states vectorˆs.
feedback through a mapping with brain signals
The subject perception of its own cognitive activity. In a
first order approximation, we consider that this perceptive Perceived states vector s.
state has negligible influence on the biomarker.
Feedback provided to the subject Observation vector o
True relationship between subject cognitive activity and
perceptible feedback. Affected by noise, biomarker hy-
True perception matrix Cat(A)=P(o|sˆ).
pothesis, experimenter choices regarding pipeline design,
etc.
Perceivedrelationshipbetweensubjectcognitiveactivity
andperceivedfeedback. Affectedbyexperimenterinstruc-
Perception model Cat(a)=P(o|s).
tions regarding feedback meaning, belief about feedback
efficacy, blindness of patients regarding treatment, etc.
True dynamics of cognition. Effect of subject mental ac- True transition/action matrix Cat(B) = P(sˆ |sˆ,u )
t+1 t t
tions and spontaneous evolution. (and initial states Cat(D)=P(sˆ )).
0
Perceiveddynamicsofcognition. Subjectrepresentation
ofitsownmentalactionsandability(orinability)tocon-
Transition model Cat(b) = P(s |s ,u ) (and initial
trol its own states. Affected by experimenter instructions t+1 t t
states model Cat(d)=P(s )).
regarding mental strategies to control the feedback, pre- 0
vious knowledge, etc.
Subjectendogenousdrivetowardshighperformances(cost
of effort, expectations behind treatment, motivation, fa- Preference matrix Cat(c)=P(o ).
t
tigue, etc.)
Subject bias towards a certain mental strategy Subject habits Cat(e)=P(u ).
t
Simulate control groups with various types of feedback
Subjects with varying priors (a,b,c,d and e) or experi-
(sham, feedback of varying bias and noise) and partici-
ments with varying characteristics (A,B,D,etc.).
pants
Simulated agents cognitive states s and feedback levels o
Report subjects training curves.
reached across trials.
Table 1: Correspondence between traditional components of NeuroFeedback Training systems
(from [REGZ+20]) and the Active Inference generic components we use to simulate training.
Table 1 provides a direct mapping between conventional NF components, drawing from consensus
guidelines [REGZ+20], and the corresponding elements within our AIF model.
NF/BCItrainingisquitedistinctfrommostbehaviouralparadigmsusuallyexploredwiththe
AIF Framework. One of the major distinction rests in the fact that the paradigm is affected by a
very significant amount of uncertainty. Two primary sources of uncertainty are central to the NF
learning challenge and are captured naturally within AIF: 1) the Perceptual Uncertainty : the
ambiguity regarding the relationship between the subject’s latent cognitive state and the observed
feedback signal. This is represented by the discrepancy between the true likelihood mapping (A)
andthesubject’slearnedmodel(a). Factorslikesignalnoise,thechoiceofbiomarker,andfeedback
processing contribute to A whereas experimenter instructions and prior beliefs shape the initial a.
2)Control Uncertainty: Ambiguityregardingthetopologyoftherelevantcognitivestatespace
and the effectiveness of specific mental actions in transitioning between states. This is represented
bythediscrepancybetweenthetruestatetransitiondynamics(B)andthesubjectlearntmodel(b).
WeusethecanonicalActiveInferencemodelcomponentstodescribesubjectexperienceduring
amentaltrainingtask. Thisentailsdescribinghowthesubjectcognitiveactivityevolvesinresponse
to his / her perception of the feedback and, in a second time, how a subject may select a variety
of mental actions that affect it. We formalize the cognitive topology explored by the subjects as a
setofdiscretelatentstatesˆs. Notallstatesaremodelledequalhowever! Somearemorepreferable
than others because they correlate with experimenter preferences regarding subject brain signals.
Because cognitive dynamics are driven by Bayesian dynamics in the Active Inference formulation,
the way beliefs evolve when confronted with new observations is heavily influenced by the quality
of initial priors and by the level of subjective uncertainty the subjects entertains. This idea is
central to the way we model neurofeedback and its therapeutic efficacy.Figure 4 provides a visual
14
(a) Inference during trials : The subject uses the feedback (right) to infer the true hidden mental
states (markedwithastar)givenitsmodeloftheworld(matricesaforperceptionandbfortransitions).
t
Planningthenoccurswhenthesubjectinfershis/hernextactiontogettospecifichiddenstatess and
t+1
achieve prefered observations (see matrix C). Note that state inference and action selection are complex
processes accounting for various MDP dynamics and balancing information-seeking and reward-seeking
behaviour in an all encompassing (Expected) Free Energy minimization dynamic [SFW22].
(b) Learning between trials : On a slower timescale, the agent uses its belief about what the hidden
stateswereduringtheprevioustrials [0,T]toupdateitsmodeloftheenvironmentthroughcorrelations.
t∈
The history of the observations are used to update the perception model a and the history of its actions
u to update the transition model.
Figure 4: The Active Inference framework unifies agent perception, action and learning by itera-
tively minimizing these functions across different timescales.
representation describing how we employ the overarching Active Inference framework to elucidate
subjects’ perception, planning, and learning throughout various BCI training sessions.
Mental actions: The agent can change its current cognitive state sˆ by performing mental
t
actionsu . Theagentmaynotinitiallyknowhowitsactionwillinfluenceitsmentalstates. Thusit
t
needsatransitionmodelofitsownmentalactionsb=P(s |s ,u ). Inpractice, weuseamatrix
t+1 t t
tokeeptrackofthismodel. Thetruedynamicsofcognitiveregulations,writtenB=P(sˆ |sˆ,u )
t+1 t t
are unknown by the subject and must be inferred.
Perception: The agent is not able to directly observe what its current mental state is, but
15
must infer it. We represent this belief as a distribution over the current states s. To do that,
it needs to rely on external (feedback) or internal (meta-cognitive or interoceptive) observation
modalities. Again, the agent may not know the true mapping A = P(o(1),...,o(M)|sˆ) and tries
t t t
to approximate it by updating its own distribution a =P(o(1),...,o(1)|s ). (here, 1,...,M are the
t t t
variousobservationmodalities. InmostBCIparadigms,1feedbackmodalityisprovidedbutthere
are exception, notably multimodal feedback and interoception)
Thankfully, one does not need a perfect model of the feedback to derive evidence in order to
learnhowtocontrolit. Humanshavebeenshownabletoderivecoherentansstructuredmodelsof
interactions from broad priors. However, in the precise case of non-invasive BCI interaction, the
low signal-to-noise ratio adds a layer of complexity to the already tough inference problem and
may explain poor subject performances. As is often the case in natural environments, the agent
generative model and the process it tries to mimic may end up differing wildly and lead to inaccu-
rate hidden state perception and sub-optimal actions. A subject model’s quality is not evalutated
on its ability to mirror the causes of its observations perfectly (which would be both incredibly
complexandcomputationallyimplausible),butonitscapacitytosupportaccuratepredictionsand
goal-directed control.
In considering this representation of the BCI training loop, one may pose the following ques-
tion: according to this formulation, what would be the mechanistic goal of NF / BCI training ?
Thisrepresentationproposesseveralanswers: Firstandforemost,learningwithinthemodelcorre-
spondstotheagentprogressivelymappingitsinternalcognitivelandscapeanddiscoveringeffective
mental actions for navigation, while potentially refining its model of the proposed (BCI-induced)
feedback signal. Then, the reinforcement of action sequences leading to desired feedback results
in the formation of robust control strategies, analogous to habit formation. Finally, the standard
neurofeedback loop may be expanded to incorporate an interoceptive observation modality. Un-
der this formulation, the subject receives two types of sensory information at each timestep: the
explicit, externally provided feedback, and an internal, interoceptive signal. Crucially, learning
in this context involves not only interpreting the explicit feedback but also developing the abil-
ity to recognize and regulate internal bodily signals associated with successful control. This has
been cited as a potential enabler of BCI training and a key mechanism in ensuring post-training
homeostasis [Dav18].
2.3.2 Underlying hypotheses
Our Active Inference formulation of BCI cognitive training makes a few implicit modeling hy-
potheses. First, and most importantly, we dissociate subject perception of their own cognitive
state("What I believe about my current attention level")andtheiractualcognitivestate("My cur-
rent attention level"). For example, in the case of attention training, we assume that the subject’s
cognitive attention level is distinctly regulated by an independent process during a neurofeedback
session. Thisassumesthatthesubjectneedstomakeaninferenceonitscurrentattentionstateby
usingthefeedbacksignalandpotentialinternalsensations. Moreover,weassumethatthefeedback
signal is not directly affected by the subject regulatory process.
Second, it assumes the structure of cognitive state space of the regulated dimension as well
as how the subject represents it. To propose an accurate description of neurofeedback training,
we have to set a plausible space of cognitive states the agents may navigate to mimic training
exercises. Due to the uncertainty surrounding most mental training paradigms however, it is
necessarily an approximation. In this paper, we propose a very generic graph in order to showcase
the capacities of the leveraged framework in modelling different neurofeedback properties. We
discuss this modelling choice and its implications in the discussion.
Third, thetabularActiveInferenceformalismdiscretizesthefeedbackvalues, thetopographies
of (true and perceived) cognitive states as well as the actions a subject may select. Although
continuousspectramaybemoresensiblerepresentationforsomeofthosemodelvariables(feedback
or actions for instance), making use of Active Inference’s discrete formulation allows us to create
more complex cognitive graphs and interactions, while retaining computational tractability.
2.3.3 Neurofeedback Inference Problem and performance indicators
Withinthisframework,theagent’staskduringeachNFtrial†∈[[1,T]]canbecastasaninference
problemoverthesequenceofstates,actions,andmodelparametersthatbestexplaintheobserved
feedback under the agent’s generative model. Formally, the agent seeks to optimize its beliefs
16
Figure 5: Full model. The environment (generative process) forwards observations to the subject
asadiscretefeedbackvalue(hereo =4)andreactstosubjectactionsu ∼Π . Inoursimulations,
t t t
the process state topography was a simple graph with 5 possible states and 5 possible ooutcomes.
Possiblestatetransitionsareshowedwithbluearrows(B)andstartingstatewithredarrows(D).
Each state may generates outcomes based on the feedback mapping (A) following a normal law
N(µ,σ ) (unbiaised noisy feedback). Initially, the agent believes each state is correlated with
process
a specific feedback level following a normal law N(µ,σ ) (unbiaised noisy feedback). Given
model
these priors, the agent attempts to learn state transitions and starting values (b and d) in order
to achieve higher feedback levels. This is done in 3 steps : the subject uses its priors to 1. infer
the hidden states, 2. pick the best actions (for exploratory or exploitative purposes) and finally
3. update its mapping from the observed succession of actions/observations.
about:
Policyselection
z}|{
θ† =( θ , θ , θ )
NF |{z s } u |{ α z} (NeuroFeedback Inference problem)
Hiddenstates Modelparameters
=(θ ,θ ,θ ,θ )
s0:T u0:T−1 a b
where θ represents beliefs about the sequence of hidden states, θ represents the sequence
s0:T u0:T−1
of actions (policy) chosen, θ (= a) represents beliefs about the observation likelihood mapping,
a
andθ (=b)representsbeliefsaboutthestatetransitiondynamicscontingentonactions. Learning
b
corresponds to updating θ and θ across trials. In this framework, if the generative model and
a b
process sport the same state space dimensions, agent performance may be quantified in terms of
distances between true environment dynamics (A,B) and learnt environment mechanics (a,b).
We give more detailed accounts of this performance metric in the Annex.
3 Simulations
3.1 General Simulation Setup
ToexploreNFdynamicsusingthisframework,wesimulatedAIFagentsengagedinataskrequiring
the regulation of a single cognitive dimension, represented by discrete latent states sˆ. The agents
17
received feedback signal(s) o and possessed prior preferences c favoring higher feedback values.
The objective was to learn a sequence of mental actions u to transition from initial, less preferred
t
states towards target states associated with higher feedback, potentially overcoming initially in-
accuratemodelsofperception(a)andcontrol(b). Figure5illustratesthegeneralmodelstructure.
Generative Process (Environment):
• State Space and Dynamics (B,D): The true latent state space consisted of N = 5
s
discrete states (sˆ∈ {0,1,2,3,4}). Subject mental actions (e.g. performing mental imagery,
inner speech, memorizing task, etc.) provoked changes in subject mental states. In all
subsequent models, the agent could only move from a state i to the adjacent states i−1 and
i+1, or remain in the current state. Transitions were probabilistic: a specific "up" action
could increase the state index (sˆ→ sˆ+1) with probability p = 0.99 (if sˆ< 4), otherwise
up
remaining in the current state. Any other action, or the "up" action failing, resulted in
remaining in the current state. Additionally, a "cognitive pull" towards a baseline state was
implemented: iftheagentdidnotsuccessfullyexecutethe"up"action,therewasaprobability
p = 0.5 of transitioning to the next lower state (sˆ→ sˆ−1, if sˆ> 0). These values were
rest
chosen to model a task requiring sustained effort against a baseline tendency. Agents initial
state was always in the bottom 2 states, encoded by the matrix D.
• Observations (Cat(A) = P(o |sˆ)): The environment generated observations o from a
t t t
set of N = 5 possible values (o ∈ {0,1,2,3,4}), based on the current (true) hidden mental
o
state. Theprimaryobservationmodalitywasanexplicitfeedbacksignal,whoselikelihood
followed a discretized normal distribution centered on the true state index sˆ with standard
t
deviation σ : Afeedback ∼ Discretize(N(sˆ,σ )) (we provide more information on
process t process
thatdiscretizationprocessintheannex). Thevalueofσ determinedthefeedbacknoise
process
level and typically varied between 0 (perfect feedback) and 2.0 (very noisy feedback). A
feedback noise of 5.0 could be considered as akin to a sham feedback signal (almost entirely
decorrelatedfromregulatedcognitiveactivity). Insomesimulations(Section3.2.4),asecond
observationmodalitywasproposedintheformofanimplicit(interoceptive)observation
modality (A ). This signal also depended on the true state sˆ, following Aintero =
intero t
Discretize(N(sˆ,σintero )), providing a potentially independent source of information about
t process
the latent state. The reliability of this observation source was also made to vary : σintero ∈
process
[0.0,2.0].
Generative Model (Agent):
• State Space and Preferences (d,c): The agent’s model assumed the same number of
states (N = 5) and observations (N = 5). Pre-training initial state beliefs d were
s o †=0
uniform. For the transition Model (b), agents started with uninformed priors about the
effects of their actions. The initial transition model b was initialized as a matrix where
†=0
each possible transition (s ,u ) → s had an equal, small probability mass, scaled by an
t t t+1
initial concentration parameter k . Formally, the Dirichlet priors were set such that b =
b †=0
k 1 (where 1 represents a tensor of appropriate dimensions filled with ones). k controlled
b b
the agent’s initial confidence in this uniform prior; lower values imply faster learning from
experience. Throughoutsimulations,k =0.01wasused,reflectinglowinitialconfidenceand
b
a prompting an exploratory drive in synthetic subjects.
• Likelihood Model (Cat(a) = P(o |s )): The agent’s model of the explicit feedback was
t t
initialized based on an assumed relationship between hidden state and provided feedback.
This prior was typically afeedback ∼ Discretize(N(s ,σ )). The initial confidence in this
t model
prior was controlled by a concentration parameter k1 . Specifically, the initial Dirichlet
a
counts were set via:
afeedback =(k1 ·Discretize(N(s ,σ ))+ϵ1) (15)
†=0 a t model
where ϵ is a small constant (e.g., 0.01) adding minimal uniform probability to prevent zero-
likelihood issues, and k1 scales the initial counts, determining resistance to updating this
a
prior. Higherk1 meansstrongerinitialbelief. Whentheinteroceptivemodalitywasincluded
a
(Section 3.2.4), agents started with an additional completely uniform prior regarding its
mapping: aintero =k2 1,typicallywithahighconcentrationparameterk2 =10.0,reflecting
†=0 a a
high initial confidence about the uncertain meaning of this signal relative to the cognitive
18
state. This reflects an observation modality to which we’re accustomed but which mapping
with the targeted cognitive state has not been noticed until the training.
aintero =k2 ·1 (16)
†=0 a
• Preferences and habits : Preferences c were set to favor higher feedback values linearly
(highest preference for o=4, lowest for o=0). Habits e were initially uniform and were not
learnt in these simulations.
Note that for simplicity’s sake and to allow for the direct comparison of true and learnt envi-
ronment rules,this study’s generative processes and models will always feature similar state space
sizes,thoughamismatchbetweenregulatedstatespaceandsubjectrepresentationisaninteresting
modelling prospect (see 5).
Key parameters manipulated across simulations are summarized in Table 2. Unless otherwise
specified,agentslearnbothtransition(b)andlikelihood(a)modelsovertrials. Duetothestochas-
ticnatureoftheprocesses,N=10agentinstancesweresimulatedforeachparametersettoensure
robustness of conclusions.
Model parameters Parameters Used in simulations
M1 : Topography of M1wasusedinallsim-
Onecognitivedimension. Togetfromastatetothenext,
the hidden states of ulations, but alterna-
the agent must pick an action depending on its starting
the generative model / tive topographies are
state.
process described in annex.
Variance of the feed- Set the hidden rule behind the generation of a specific
back generative pro- feedback. Hownoisycomparedtothetruecognitivestate All.
cess σ ?
process
Variance of the intero- Set the hidden rule behind the generation of a specific in-
ceptive generative pro- teroceptive signal. How noisy compared to the true cog- 3.2.4.
cess σintero nitive state ?
process
Variance of the subject
Settheexpectationsofthesubjectregardingthereliability
generativemodelofthe 3.2.2,3.2.3 and 3.2.4.
of feedback when the training starts.
feedback σ
model
How much weight the agent gives to its initial action
model. Thesubjectstartswithaflat/noisypriorregard-
Initialconfidenceofthe ing its mental actions. We set how confident the agent is
k1 = 0.01 in all simu-
subject in its action regarding this mapping with this factor. Higher values b
lations .
model k1 mean the subject is much less prone to changing its tran-
b
sition beliefs and may require a longer training to achieve
results.
How much weight the agent gives to its initial external
feedback model / internal observation modality. Higher
Confidence of the sub-
values mean the subject gives more credit to those prior 3.2.3 and 3.2.4 [feed-
ject in its feedback
instructions / beliefs and is much less prone to chang- back] 3.2.4[interocep-
modelk1 /initsinte-
a ing its observation model despite contradicting evidence. tion].
roceptive mapping k2
a Theseparameteronlyhadaninfluencewhenthesimulated
agents learned observation mappings.
Presence of an inte-
If agents were given an alternative observation signal to
roceptive observation 3.2.4.
complement the external (BCI based) feedback.
modality (IO)
Table 2: A general sum-up of the parameters we leverage in our simulations.
3.2 Tackling BCI questions with simulations
The AIF framework enables simulating diverse NF scenarios to address specific questions about
training mechanisms. We focused on the following key aspects:
19
3.2.1 How does the noise of the feedback affect the training ?
To isolate the impact of external feedback quality, we first simulated agents whose perception
model a was fixed throughout training and assumed perfect feedback fidelity. Specifically, agents
operated with a = I , meaning they fully trusted the feedback as a direct readout of their state
No
(s = o ). Learning was restricted to the action model b, simplifying the inference problem (Eq.
t t
NeuroFeedback Inference problem) to updating only θ :
b
θ† =(θ ,θ ,θ )
NF s u α
=(θ
s
,θ
u
,(cid:0)θ(cid:0)
a
, θ
b
) (Partial NF Inference problem)
|{z}
ACTIONbelief
Ofcourse,thatsimplificationdidnotimplyanythingconcerningthetruefeedbackmatrixofthe
generative process A. We varied the true noise level of the generative process feedback, σ ,
process
across conditions representing:
• High-fidelity feedback (σ ≈ 0.1): True mapping A ≈ I . Agent’s assumption
process Ns
holds true.
• Noisy feedback (σ =0.5): A is informative but imperfect, representing typical BCI
process
variability.
• Sham feedback(σ =5.0): Aisnearlyuniform,providingminimalinformationabout
process
sˆ.
t
Training trajectories (states, feedback, learned b) were analyzed to assess how objective feedback
noise affects learning when the agent assumes perfect feedback.
3.2.2 Influence of Initial Expectations about Feedback Quality
Inasecondtime,wequestionnedhowtheagent’sinitialbeliefaboutfeedbackreliabilityinfluenced
training, while still keeping the perception model a fixed (no learning of θ ). Unlike Section 3.2.1,
a
thefixedawasnotnecessarilyidentitybutreflectedvaryinglevelsofassumednoise,parameterized
byσ intheinitialsetupa∼Discretize(N(s ,σ )). Wevariedboththetruefeedbacknoise
model t model
σ (as before) and the agent’s fixed assumption σ (from near-perfect ≈ 0.01 to very
process model
unreliable = 2.5) to understand how congruence between reality and expectation affects mental
action learning (b).
3.2.3 Modelling changing subject confidence in the feedback
Wethenenabledagentstolearntheirperceptionmodelaalongsidetheiractionmodelb,addressing
the full inference problem (Eq. NeuroFeedback Inference problem). This simulation explored
how agents adapt their beliefs about feedback reliability based on experience. We examined the
interplaybetween: a)thetruefeedbacknoise(σ ),b)theagent’sinitialbeliefaboutthisnoise
process
(σ usedinEq. 15),andc)theagent’sinitialconfidenceinthisbelief,setbytheconcentration
model
parameterk1 (variedfrom1.0to10.0). Thissetupmodelssituationswheresubjectsmightinitially
a
trust or distrust experimenter instructions about feedback meaning and subsequently revise these
beliefs.
Training outcomes and the evolution of the learned perception model a were assessed. We
ran similar simulations as in section 3.2.2 with the true feedback noise σ and the initial
process
subject feedback model σ both varying between 0.01 and 2.0. and plotted the evolution of
model
the cognitive states / models of the subjects in order to investigate how initial beliefs (σ )
model
and confidence (k1 ) interact with true feedback noise (σ ) when agents can adapt their
a process
perception model.
3.2.4 Simulating training with Interoceptive Learning
Finally,toinvestigatepotentialmechanismsforNFskillgeneralization(i.e.,self-regulationwithout
external feedback), we introduced a second, implicit observation modality intended to represent
interoception (IO agents). These agents received both the standard external feedback (charac-
terized by σ , initial model σ , confidence k1 ) and an internal signal (characterized by
process model a
σintero and confidence k2 ). In these simulations, we compared three typical interoceptive sig-
process a
nals : highly informative (σintero = 0.0), highly uninformative (σintero = 2.0) and intermediate
process process
(σintero =0.9). Importantly, here the subject was much more accustomed to the latter source of
process
20
information(translatingintoahighk2 initialconfidenceparametersetto10.0inoursimulations)
a
but had not related these internal observations to their current cognitive states (flat initial prior):
agents started with highly uncertain priors about the interoceptive mapping (aintero uniform, Eq.
†=0
16) They were able to learn both a and a alongside b. We compared the performance
feedback intero
and learned models of IO agents versus agents receiving only external feedback, across various
levels of external feedback noise (σ ) and initial beliefs (σ ). The accuracy of the learned
process model
interoceptive mapping a at the end of training was used as a proxy for the potential for
intero
post-training generalization.
4 Results
Thefigurespresentedinthissectionandthesimulationsusedtogeneratetheresultsareavailableat
https://github.com/Erresthor/ActivPynference_Public/tree/main/paper_scripts/paper_
ActiveInference_BCI
4.1 How does the noise of the feedback affect the training ?
High-fidelity feedback : We first simulated a set of 10 artificial agents using a perfect feedback
to self-regulate. Figures 6b,6c,6d show a representation of one of these agents internal variables
during three trials (1, 5 and 15). The figures upper rows show observations (observed outcome
and sampled distribution), the middle rows show the hidden states (true hidden value and subject
inference) and the lower rows depict agent posterior over actions (how desirable a given action
appears to the subject) and selected action (in this case the action among the biggest action
posteriors). In all three trials, the agents were embedded with a perfect perception model, thus
making perception a trivial task and explaining the certainty with which the agents inferred their
hiddenmentalstatesgivenfeedback. Thesubjectusedtheinitialtrialstotestactionsandlearnthe
transitionrules. Bytrial15however,theactionmodelwasinformedenoughtopromptexploitative
behaviour in the subject (always taking the shortest path towards the wanted state). A clear
explorative/exploitative shift is noticeable at the trial level. Figure 6e gives an account of the
same situation, but at the whole training scale. This time, the results of all 10 subjects are shown
andsubjectperformancesarequantifiedusingasetofmetrics(definedinAnnex)toallowforbetter
readability. We plot the average feedback level and cognitive state achieved by the agents during
the last 15 trials of the training. We also plot the summed KL-divergence between their transition
andobservationmodels, and theirtruecounterparts. Againwenoticetheexplorative/exploitative
shift around trial 10.
Unbiased but noisy feedback : Figure 7 shows how agent performances are affected by a
noisyfeedbackwithnobias. ThismaydescribevariouspartsoftheBCIpipeline,andespeciallythe
biomarkerused. Toquestiontheusefulnessofnoisybiomarkers,anoisierfeedback(σprocess=0.5)
was provided to the subjects. As previously expected, higher levels of noise led to lower overall
feedback and cognitive levels and a final action model of poorer quality. Despite a rather low
amountofnoise,performancesattheendofthetrainingweresignificantlyaffectedanditprompted
us to simulate a wider range of biomarker noises.
Sham feedback : The agents who attempted to learn to regulate their hidden states using
sham feedback did not manage to improve their performances significantly. This comes as no sur-
priseaswedidnotimplementanynonspecificmechanismthatwouldexplainsuccessfulregulation.
The associated figure is available in Annex.
Figure 8 shows the training efficacy fall-off depending on the noise of the selected biomarker.
Basing ourselves on the previous simulations, we plot the training of subjects with feedbacks of
varying qualities. We selected the true feedback noise randomly between σ = 0.01 (perfect
process
feedback) and σ = 2.5 (sham feedback). For all data points, the subject expected the
process
feedback to be perfect (σ =0.01) and did not update this belief during training. We plot the
model
average cognitive regulation achieved by the artificial subjects at the end of the training (avg. last
20 trials) and the quality of their model. As expected, a lower biomarker quality leads to worse
self-regulation performances. The efficacy of the training falls of dramatically for σ ≃ 0.5,
process
eventuallyrenderingthewholetrainingparadigmnearlyuselessaroundσ ≃1.5. Thisshows
process
that when navigating a complex environment with sparse feedback, the reliability of the signal
is primordial to achieve self-regulation. In the next section, we explore if this conclusion can be
somewhat mitigated by agent explicit modelling of the noise.
21
(a) True effect of mental actions.
(b) Trial 1. (c) Trial 5.
(d) Trial 15. (e) Training curves.
Figure 6: Simulated trials of 10 agents with a perfect model of a perfect feedback (a = A = I )
5
butnoinitialmodelofmentalactions. Thetrueeffectofthosementalactionsisshownin6a(each
action has a different effect depending on the state of the agent (x-axis), and leads the subject
to another state (y-axis). We represent artificial agent behaviour and learning across trials. We
show how individual trials are conducted (6b, 6c, 6d), with the observations (first row), subject
mental states (second row) and action taken (third row). The colored dots represent the true
(observed/hidden/selected)valuesandtheblack-and-whitebackgroundrepresentsthedistributions
from which they were sampled (top/bottom row) and the subject infered current cognitive state
(middlerow). Wealsoplotthesubjecttransitionmodelduringthetrialtoexplainwhyitmayhave
picked some actions rather than others. Because this representation doesn’t easily qualify several
subjects accross 100+ trials, we also displayed training curves of the subjects (6e). It consists
in the evolution of their average feedback level, mental state, as well as transition and feedback
model error accross trials. The subjects were initially very uncertain about which action to take
in the initial trials, but their perception was always certain and the subjects knew exactly what
were their cognitive state at any point. Because the action model of the subject started off too
incomplete, the agent first engaged in explorative behaviour to gather informations and improve
itsmodel. Itquicklymadesenseoftheavailabledataandbuiltagoodmodelofitsmentalactions.
By trial 10, the model of the agent is good enough that it only focuses on achieving high feedback
levels. 22
(a) Trial 1. (b) Trial 5.
(c) Trial 15. (d) Training curves.
Figure 7: Simulated trials of 10 agents with a naive model (σ = 0.01) and a noisy feedback
model
(σ = 0.5). In contrast to figure 6, observations were sampled from a noiser distribution and
model
created mismatches between subject inferences and true mental states (7b), thus leading to lower
learnt transition model quality and regulation performances.
23
Figure 8: Subject performance metrics at the end of the training for various levels of biomarker
noise. We plot the average mental state achieved during the last 15 trials of the (100 trials)
training [left] as well as the error of the transition model learnt by the subjects at the end of the
training [right]. Each point corresponds to the a single agent which believed that the feedback
was perfect (a =I ) but was provided with a true feedback noiseσ ∈[0.0,2.5] (x-axis). The
5 process
noiseassociatedwiththechosenbiomarkerhasahugeinfluenceonagentperformanceandtraining
efficacy,andsubjectabilitytolearnfromthefeedbackfalloffdramaticallyafteracertainthreshold
(≃0.3)
24
(a) Trial 1. (b) Trial 5.
(c) Trial 15. (d) Training curves.
Figure9: Simulatedtrialsof10agentswithacautiousmodel(σ =0.5)andaperfectfeedback
model
(σ =0.01). Here,subjectsexplicitlydoubtedthemeaningofaspecificfeedbackvalue,assum-
model
ing that it could have been generated by a range of cognitive states (driven by σ ). Although
model
thesubjectswereoverlycautiousaboutthefeedback,theystillmanagedtoself-regulateoptimally.
4.2 Influence of Initial Expectations about Feedback Quality
We provided artificial subjects with a diversity of feedback noises. In contrast with previous
simulations, agents were cautious about the indication of this feedback. This meant that subject
perception of their cognitive state was not a trivial endeavour. Figure 9 shows the main difference
with simulations of section 4.1 : the subjects beliefs about their mental state became much more
uncertain,leadingtolessradicalmentalactionlearning. Indeed,thesubjectsuncertaintyregarding
theexplicitfeedbackreflectedonwhattheywereabletolearnfromit: iftheydoubtedittoomuch,
itprovedverydifficulttodevelopanaccuratemodelofinteractionwiththefeedback. Despitethat
fact, a reasonable amount of caution towards the feedback did not lead to significant impairment
regarding self-regulation for a perfect feedback. 9d.
In fact, cautious subjects (σ =0.5) performed better than naive subjects (σ =0.01)
model model
whenthefeedbackstoodinthecriticalσ ≃0.5range11. Asonewouldexpect,subjectswho
process
have a better understanding of the feedback tend to limit overfitting from noisy observations and
build a better model of their mental actions.
We generalized those findings to a wider set of true feedback noise / subject feedback noise
expectationscombinationstofigureoutwhichsetofsubjectinitialexpectationswouldfavorlearn-
ing from even noisy feedback. Figure 12a shows performance metrics for 20 x 20 groups of 10
agents. Each group trained used a specific (σ ,σ ) set of parameters. We show the final
process model
averagementalstate(last15trials), feedbackmodelerrorandactionmodelerrorforeachofthese
groups. As expected, good learning only occurs when the feedback quality is good enough, and
when the subjects actually believed that the feedback contained some useful information about
25
(a) Trial 1. (b) Trial 15.
(c) Trial 150. (d) Training curves.
Figure10: Simulatedtrialsof10agentswithacautiousmodel(σ =0.5)andanoisyfeedback
model
(σ =0.5). Here,subjectsexplicitlydoubtedthemeaningofaspecificfeedbackvalue,assuming
model
that it could have been generated by a range of cognitive states (driven by σ ). The explicit
model
modellingofthenoiseinherenttothesystemallowedsubjecttoeventuallyself-regulatemuchbetter
thanwhentheybelievedthefeedbackwasperfect(7),underliningtheimportanceofsubjectpriors
(and experimenter instructions) in a noisy context.
26
Figure 11: Compared training curves of two groups of 10 agents provided with a noisy feedback
(σ = 0.5). The "naive" group (blue) expected the feedback to be perfect (σ = 0.01),
process model
whereas the "cautious" group had less optimistic expectations (σ = 0.5), much closer to the
model
truth. Duetobeinglesssensibletonoisyobservations, thecautiousgroupwaslesspronetomodel
overfitting and eventually, albeit less quickly, developed a better understanding of their mental
actions.
27
(a) Agent performance map for a given expected feedback noise (x-axis) and true feedback noise (y-axis).
(b) Optimal subject internal belief depending on feedback noise.
Figure 12: Simulated trials for 20x20 groups of 10 agents. Each group trained used a specific
(σ ,σ ) ∈ [0.01,2.0] ∗ [0.01,2.0] set of parameters.12a shows the final average mental
process model
state (last 15 trials) and action model error (defined as the KL divergence between subject model
bandthetruementaltransitionsB)foreachofthesegroups. Figure12bfocusesonafewspecific
truefeedbacknoisevaluesandshowswhichsubjectpriormodelparametersleadtogoodregulation
results. Thissimulationyieldsexpectedresults(thetrainingcanonlybeusefulifthefeedbackisn’t
too noisy, if the subjects trust the feedback somewhat, etc.), and less instinctive ones (subjects
with some degree of caution σ ≃0.5 learn better than naive ones whatever the actual level of
model
feedback noise).
28
their mental activity. More surprisingly, having a perfect model of the true feedback (i.e. knowing
exactly the mapping from one’s mental state to the observed feedback) did not always result in
better mental action models. Finally, it appears that finding the optimal subject expectations
with regard to the quality of the chosen biomarker is a balancing act. In the simple simulations
weconducted,theoptimalsubjectexpectationsparameterevolveddynamicallywiththequalityof
the feedback 12b. However, for small biomarker noises (σ ∈[0.3,1.0]), σ ≃0.5 yielded
process model
the best training results, suggesting that when the feedback provided is reasonably noisy, cautious
priors lead to better learning efficiency. The cautious subjects also performed well under perfect
feedback, suggesting that under our approximations, a small degree of flexibility in the subject
feedback perception led to overall better learning.
4.3 Modelling changing subject confidence in the feedback
Previous simulations have shown how the noise affecting the biomarker affected the training of
a subject depending on its expectations. However, they also assumed that those expectations
were fixed and did not change accross the training. We believe this oversimplification glosses
over one of the major mechanics of BCI training : the subject changing level of confidence about
the reliability of the provided feedback accross the trials. The next simulations featured the
same model as previous sections (3.2.1 and 3.2.2) with an important distinction : the subjects
dynamically changed their observation mapping (relating cognitive state and observation), thus
accounting for BCI training-specific dual perception-action uncertainty.
We show the results of these simulations in figure 13. First, we simulated very typical training
setups, namely a cautious subject provided with a perfect feedback 13a, a naive subject provided
with a noisy feedback 13b and a cautious subject provided with a noisy feedback 13c. In an
effort to reduce their Free Energy, the agents updated their models of the feedback (bottom right
plots). Because the agents learnt action and perception models simultaneously, those learning
trajectories affected one another dynamically. In general, this manifested as a deterioration of
the feedback model while the action model improved. Once the mental action topography was
sufficiently explored, the models of the agents converged towards local free-energy minima.Figure
13d generalizes these remarks to a wider range of true feedback / subject expectation parameters.
Finally, we wanted to know if questioning the quality of the feedback provided negatively
impacted BCI training subjects. We compared the performances of these feedback-learning agents
to the results showed in 12a (fixed feedback mappings). Our simulations (13e) showed that the
impact actually varied based on the subject’s initial model : when the subject’s model initially
closelyalignedwiththetruefeedbackprocess,continuouslylearningthefeedbacktendedtohurtthe
performances of the training? In turn, for subjects with feedback mappings significantly divergent
fromthegroundtruth(suchasnaiveoroverlycautioussubjects),learningthefeedbackresultedin
better action mappings, displaying some kind of ’protection mechanism’ preventing subjects from
learning damaging mental actions.
4.4 Simulating training with Interoceptive Learning
We ran simulations where agents were provided with an additional (interoceptive) observation
modality. Simulatedsubjectsinternalsignalswereeitherinformative(lowσintero)oruninformative
model
(high σintero ). Figure 14 shows the performance maps of agents with various degrees of internal
process
observationabilities. Overall,agentsequippedwithaninternalobservationmodalityoutperformed
agents relying on the external feedback only. Particularly noteworthy is the superior performance
of subjects with precise internal signals, as depicted in Figure 14b, where they demonstrated the
ability to decipher high-noise feedback. This suggests that the substantial decline in training
efficiency, as evident in the alarming drop shown in Figure 8, might be alleviated by leveraging
this alternative observation pathway. Importantly, the efficacy of this supplementary observation
route is contingent upon the quality of the interoceptive pathway.
5 Discussion
The aim of this paper was two-fold : first, showing how the cognitive system of the subject
could help describe the complex regulation mechanisms at work during Neurofeedback Training.
Second,demonstratinghowtheActiveInferenceframeworkcouldprovideapertinentsetoftoolsto
computationallymodeltrainingandcontributetoBCIresearchbytestingcomputationallyvarious
hypotheses and the effect of possible instructions.
29
(a) σ =0.01,σ =0.5 (b) σ =0.5,σ =0.01 (c) σ =0.5,σ =0.5
process model process model process model
(e)Finalmentalstateandactionmodelerrordiffer-
(d) Agent performance map for a given expected encesbetween12aand13d. Bluevaluesmeanscon-
feedback noise (x-axis) and true feedback noise (y- tinuouslyupdatingyourfeedbackbeliefledtobetter
axis). performances.
(f) Optimal subject internal belief depending on feedback noise. Simultaneously learning action and feed-
back mapping leads to lower performances,higher within-group variance and the absence of the optimal
subject caution prior noticeable in 12b.
Figure 13: Simulated trials of agents which updated their model of the feedback and their action
model simultaneously. The agents trusted their feedback priors (k1 = 10.0) and dynamically
a
updatedtheirfeedbackmodel. Weshowthetrainingcurvesforsometypicalcases(agentconfidence
/ feedback quality mismatches) in 13a,13b,13c. We generalize these results to (σ ,σ )∈
process model
[0.01,2.0]∗[0.01,2.0]in13d. Agentsupdatingtheirfeedbackmodelledtooveralllowerperformances
comparedto12. Ourmodelsuggeststhatforlowervaluesofk1 ,itwaseasierforsubjectstodoubt
a
the reliability of the feedback signal rather than learning complex mental actions.
30
(a) No interoceptive observation modality. (b) σ ≃0.0
intero
(c) σ ≃0.9 (d) σ ≃2.0
intero intero
Figure 14: Simulated training results for four groups of agents. Agents of group 1 14a had no
interoception(justlikeinfigure13d). Agentsofgroups2,3,4(14b,14c,14d)wereallequippedwith
an interoceptive observation modality characterized by a specific noise (σ ≃0.0,σ ≃0.9
intero intero
andσ ≃2.0respectively). Eachgroupwassubdividedin20x20subgroupsof10agents,each
intero
usingaspecific(σ ,σ )setofparameters. Thefiguresshowthefinalaveragementalstate
process model
(last 15 trials), learnt action model error and learnt interoceptive model error for each subgroup
of agents.
5.1 Cognitive regulation at the heart of Neurofeedback learning
Inusualneurofeedbackmechanismaccounts,brainactivationhaslongbeenconsideredastheinde-
pendent variable, with cognition and behaviour being dependent variables [SRS+17]. We suggest
an alternative framing approach based on subject cognitive control of the feedback. In such a
formulation, the subject guides the dynamics of the training through perception, mental actions
and learning. In this formulation, brain activity is partially predicted by cognitive dynamics, in
turn dependent on high level perception, planning and learning. Note that these processes need
not be conscious for the subject. In over words, our model thus does not extend explicitly to the
physiological dimension of subjects’ brains, but focuses on the cognitive states they are coupled
with. Of course, the assumed link between the modeled cognitive states and the actual measured
biomarkers remains a critical (and often weak) point in practice, reinforcing the importance of
biomarker selection and proper reporting [REGZ+20].
Ability to generalize : Training success or failure is a difficult concept to formalize clearly,
mainlybecausetheappreciationofasuccessfultrainingvariesheavilydependingontheparadigm.
Moreover, the ability of subjects to transfer the knowledge gathered from training into situations
outside of the training environment is crucial for the success of BCI / neurofeedback training,
especially in therapeutic approaches. On the one hand, a general argument can be made that
BCI training is in essence a cognitive exercise aiming to provide the subjects with a way to learn
adequatementalstrategies. Followingthisargument, anuntrainedsubjectlacksacognitivemodel
accurate enough to navigate its (mental) environment easily. This may lead to observable be-
havioural afflictions, or the inability to control an external device, etc. Training would allow
subjects to build an accurate model of perception and action, making them able to control their
cognition more willingly, or build reliable internal observers (metacognition). Thus, training suc-
cessfully would be synonym to building a good internal model. Following our Active Inference
formulation, this would mean that a successful training would minimize the difference between
the true environment dynamics A, B and the modeled dynamics a,b in the targeted cognitive
dimension.
Another prevalent hypothesis postulates the learning of an alternative, internal observation
modality : other modeling approaches such as [Dav18] have already theorized the emergence of
an interoceptive observation pathway during training and discussed their ability to reinforce an
ongoing training and then be used as the only reinforcer after training. Our model show this
reinforcing effect during training (see simulation 4.4), and underlines the importance of this "gut-
feeling"asitwouldrenderthetraininguselesstothesubjectwhencutfromtheexternalfeedback.
31
In the initial simulations, we greatly simplified the nature of subject experience during training.
Indeed, we assumed that the only way for the subject to infer their cognitive activity was to use
an external (BCI-equipped) observation pipeline. Although this representation may suffice when
the end-goal of training is learning how to use an external device, it falls short when tackling
neurofeedback generalization problems : when deprived of the external feedback, how would the
subject perform self-regulation ? We believe that empirical problem is solved through learning an
internal observation modality. We modeled this interoceptive pathway as an independent obser-
vation modality, similar to the external feedback, but other representations, such as hierarchical
models [HSP+21, SSHL+20], may provide promising alternatives.
Tool learning & illiteracy : Although the task the synthetic agents were given would have
been trivial if the subjects knew all the dynamics of the environment (What is the effect of my
actions on my mental states ? and How does a given mental state translate to a feedback level ?),
it proved much harder when they were facing uncertainty from one (or both)modalities. Learning
to interact with a new tool requires the user to build a comprehensive model of observations
and actions. During BCI interaction this is particularly difficult, because subjects beliefs about
observation and action modalities lack the prior evidence needed to dissipate this uncertainty. To
learneffectively,thesubjectneedstobequiteconfidentinpartofitsmodel,inordertoprogressively
design better and better interaction models. Our simulations have shown that high amount of
uncertainties in the subject model led to poor training performances, even in the advent of a
precisefeedback. Thisistrueineveryenvironment(indeed,intheActiveInferenceliterature,most
approaches focus on learning either observation matrices given known action matrices [FFR+16,
SFW21, TSB20] or the contrary.) In our approach however, because neurofeedback and other
BCI-based applications rely on a much more uncertain set of dynamics, the model we propose will
feature various levels of uncertainty in both agent perception and action beliefs. The complexity
of the inference problem and its dependence to subject initial priors is a first explanation for high
inter-subject heterogeneity.
Finally,oursimulationssuggestthatinsomecases,itmaybeeasierforthesubjecttoexplaintheir
own poor training performances by questioning the reliability of the feedback provided (regardless
of its actual intrinsic quality), rather than their own failure to figure out the adequate transition
rules. Simulated agents who continuously updated their confidence in the feedback performed
worse than their more naive counterparts when the feedback reliability was high and matched
their initial beliefs, but better when the feedback was of poorer quality. It remains to be seen
wether such fine training effects may be captured using the data from usual BCIT tasks.
5.2 Modeling BCI training
The Active Inference framework proves particularly suitable for modelling Brain-Computer Inter-
face (BCI) training for several reasons:
• It accommodates subject-specific internal parameters and learning environment criteria ef-
fectively.
• It encompasses both action, perception and learning, essential components of BCI training
dynamics.
• Its versatility enables the modelling of various types of training loops and subject models
within a unified framework. The generic nature of the model described in the paper means
that the framework accounts for the great diversity in BCI training protocols (biomarkers,
feedback design, trained cognitive dimension).
• Possible extensions of the model offers scalability and hierarchical structuring, accommodat-
ing the complexity of BCI training scenarios. [FRP+17, PRF18]
• The framework reflects various timescales, encompassing quick processes such as state infer-
ence and decision making, and slower ones like learning.
By leveraging Active Inference, it becomes possible to conceptualize subject behaviour during
BCI training as akin to navigating through an uncertain environment. Crucially, this formalism
incorporates numerous initial and hyper-parameters, empowering modelers to replicate the reality
ofBCItraining. Manipulatingtheseparametersenablestheexplorationofthepotentiallimitations
ofspecifictrainingprotocolsandtheireffectsonagentbehaviour. Ofparticularsignificancearethe
biomarker reliability (including bias and noise) underlying the feedback and the priors within the
32
subject model (representing the subject’s initial beliefs about the system at the onset of training).
Our basic simulations demonstrate that a perfectly designed feedback mechanism alone does not
ensure successful training. Consequently, we can establish links between fundamental elements of
theBCItrainingprotocol[REGZ+20]andinformationtheory-basedvariablestoinvestigatelearn-
ing efficacy. The broad applicability of the Active Inference framework presents both advantages
and challenges. On one hand, its generic nature allows for the modeling of a diverse array of
situationsusingthesamesetoftools. Ontheotherhand,thisversatilitynecessitatesawiderange
of parameters, making the model difficult to test and ultimately challenging to fit. To restrict the
space of simulated parameters, we imposed constraints on the initial values of these parameters.
Specifically, we:
• Assumed the true topography of the mental states of the subject and their representation
in the subject model. We also chose to limit state transitions to neighboring mental states
in order to describe a continuous process. Finally, we used spontaneous state-transitions
towards lower states to mirror a mental resting state.
• Modeled the Feedback / biomarker as noisy, but unbiaised, in effect refraining from delving
intotheexpansivemodelspaceofspecificbiomarkerbiaises,astheireffectsmayprovetrivial
and difficult to thoroughly examine. We also described the subject initial feedback priors
noisy but unbiaised for the same reason.
Explaining BCI training variability : In our study, one of the primary objectives was to
elucidate the inter-subject differences observed in certain studies, where some participants suc-
ceeded in regulating their brain activity while others did not, leading to what has been termed
"BCI illiteracy". Within our formalism, these individual differences can be attributed to several
factors. Firstly, within a single group of agents with similar internal parameters, the stochas-
tic nature of the biomarker noise, and the decision-making process of our agents may introduce
randomness, particularly when multiple actions exhibit similar perceived value. As a result, simu-
lating the behaviour of a single agent with specific characteristics did not suffice, necessitating the
examination of mean performance indicators across multiple agents with similar initial properties
to draw meaningful conclusions.
Ourproposedformalizationunderscoresthestructuralsourcesofuncertaintyinherentingeneric
BCI training paradigms, emphasizing the complexity of the training task due to high uncertainty
in both perception and action domains. Typically, when faced with uncertainty regarding a set
of dynamics, strong priors in other dimensions can facilitate the interpolation and construction
of a coherent model of the uncertain dimension. However, in many BCI applications, including
neurofeedback, agents often start with uncertain priors in both perception and action, making
it challenging to establish the relationship between feedback and cognitive states. This dual un-
certainty offers a plausible mechanistic explanation for the variability observed in neurofeedback
efficacy studies.
Furthermore, discrepanciesininitialparameterscancontributetogroupeffectsintheoutcome
ofthetrainingprocess,withvariationsstemmingfromfactorssuchasmotivation,habits,andprior
knowledge. Our model allows researchers to shed light on the impact of these group differences by
considering alternative preference matrices, action habits, and prior models.
Nevertheless, our simulations demonstrate that under appropriate conditions, the challenges
posed by this uncertainty may be addressed, and successful model learning and system navigation
maybeachievedevenwithimperfectpriors. Simulatedtrainingswiththeadditionnalinteroceptive
observerprovedmuchmoresuccesful. Agentsequippedwiththisadditionnalobservatorypathway
required less demanding priors qualities to achieve satisfactory outcomes.
5.3 What lessons for BCI experimenters ?
Our basic simulations have underlined the general dynamics of some parameters of neurofeedback
training. First, we showed the importance of the reliability of the biomarker used : the threshold
noise above which the subjects became unable to self-regulate was especially low with a brutal
fall-off. This indicated that neurofeedback training paradigms based on nonspecific biomarkers
may face tough odds. In general, the ability of the subjects to make sense of the feedback was
directlyrelatedtohowdiscriminantitwas,i.e. howdifferenttwodistinctmentalstatesmanifested
themselves in the feedback.
The priors of the subjects regarding the feedback and their mental strategy played a particu-
larlyimportantroleinoursimulations. Althoughthesepriorsmaydependonsubject/population-
specificparameters,theyareheavilyinfluencedbyexperimenterinstructions. Ingeneral,anormal-
izationandclarificationoftheinstructionsgiventothesubjectshaslongbeencalledfor[REGZ+20]
33
and constitutes an important first step to (1.) simplify the patient task, (2.) allow for better re-
productibility, (3.) better explain the outcome of training. A more well-defined set of training
parameters may also be used to better understand the causes of training failure and hopefully,
solve them. More precisely though, basic simulations showed that :
• Feedback: Ingeneral,agentsbenefitedfromalimitedamountofpriorcautionregardingthe
feedback, i.e. notbeingtoonaiveaboutitsaccuracy. Doubtingthefeedback(4.3)actedasa
protection mechanism in very poor conditions (i.e. when subject priors were significantly off
orwhenfeedbacknoisewassignificantlyhigh)toavoidlearningflawedcorrelations. However,
it also hurt the performances of subjects with as initially adequate feedback perception,
suggesting that experimenters should honestly communicate about the reliability of their
NFT paradigms, though the nature of this communications remains vague.
• Mental strategies: In our simulations, we deliberately restricted the number of available
mentalactionstofacilitatelearningwithinafeasibletimeframe. Promptingagentstoexplore
a specific subset of their mental actions (through low initial action mapping confidence) was
crucial for initiating the necessary process of exploration and learning. This suggests that
pushingsubjectstotryoutspecificmentalstrategiesatthestartofthetrainingisparticularly
important, andmaypreventtrainingifoverlooked. Thisisadebatedissueinthelitterature,
withsomearguingthatprovidingaclearsetofstrategiesandshowingexamplesmaypromote
thelearningoftheBCIskill[LLM13],whileotherswarnagainstvagueinstructionsthatmay
be incongruent to the desired outcome [EJSV16].
• Interoception : We described subjects ability to perceive their own mental states without
thefeedbackasanindependentinternalobservationmodality. Morethanjustanreinforcing
mechanism during training, we argue it is actually a necessary pathway to allow subjects to
use the knowledge acquired during neurofeedback training once deprived of the external
feedback. Tofavorlearningthisgut intuition, theexperimenterinstructionsshoulddrivethe
subjects towards attending internal signals. More generally, this interoceptive observation
pathway has been cited as a prime candidate for post-training effect retention, by allowing
homeostasis [Dav18].
5.4 Modeling limits
Although our models of BCI interaction are, necessarily, greatly simplified, a few limitations are
worthy of a mention, both within and beyond the perimeter of our approach :
• Physiological activity of the subject : Brain Activity regulation has been at the center of
Neurofeedback training, with some studies assessing the success of their training based on
the degree physiological regulation (while sometimes forgetting to check its effect on subject
behaviour). Some pre-existing modeling approaches have explicitly described its evolution
throughregulation. Wechosetorenderthedimensionimplicitbyprovidingadirectfunction
mapping cognition and feedback. Simulating the NF subject physiology is a necessary next
step for our model to be deployed and used in actual BCI practice.
• Subject observable behaviour during and after training : Although cognitive actions directly
related to the feedback are modelled, observable behaviour is not simulated. Of course, this
is a major limit of our approach as it prevents us from comparing the results of regulation
and behaviour (the usual end goal of neurofeedback training).
• Thetemporaldynamicsofthefeedbacktrainingloop: Previousneurofeedbackmodelingap-
proacheshavetackledtheinfluenceoftemporalfactorssuchasdelaysorcontinuousfeedback
on training [OLPS17] . However, due to the sequential nature of our framework, it may not
be fit to explore the effect of feedback temporality on human perception on its own.
• Subject behaviour outside of the training environment. This thematic is particularly im-
portant when talking about therapeutic neurofeedback. The effect of NFT on the subjects
outside of the training sessions proper is not simulated directly : we can only make simpli-
fying assumptions regarding the relationship between agents models after training (learning,
habits, etc.) and their performances in other situations. This would most likely be linked to
concepts such as transfer learning that we don’t consider here.
34
Those limits prevent us from accounting for some dimensions of the training. Additionally, some
hypotheseshavebeenmaderegardingthepropertiesofthesubjectgenerativemodelandcognitive
states. Thosehypothesesaresummed-upinsection2.2andtendtomake(reasonable)assumptions
on perception and action selection rules during neurofeedback.
5.5 Next leads
The work presented here paves the way for three main developmental axes.
First, Active Inference models are good candidates to be used in (Bayesian) parameter esti-
mation methods [JJ00]. This would allow researchers to use experimental training data to figure
out the most likely subject / training pipeline characteristics to explain their results. Beyond
better result interpretation, proven training models may be used as the basis for training outcome
predictors or adaptive training protocols.
Second,equippingthemodelwithaneurophysiologicalcomponentinordertoexplicitlyfeature
the biomarker used during training would allow us to close the loop and propose a more complete
approximationofthecouplingbetweencognitionandphysiologyinthebrain. Thismayeventually
allow us to confront alternative models to experimental results in order to more closely fit subject
internal models as they interact with the BCI. Although tricky, this could be achieved by using
the predicted synthetic physiological responses generated during belief update [FFR+16]. Alter-
natively, adapting compatible methods used in signal classification such as deep autoregressive
Hierarchical Markov Model [WAMH18] may prove interesting.
Third, possible extensions of the current (cognitive) model to specific cognitive dynamics such
as attention may prove instrumental to study the influence of (meta-)cognitive dimensions such
as attention. Active Inference based approaches such as [HSP+21, SSHL+20] have already tack-
led the question of attention and valence in the Active Inference framework, and models of self-
regulationusingthissetofmodelsmayyieldmorespecifictrainingcurvesforneurofeedbacktrain-
ing paradigms targetting ADHD.
6 Concluding remarks
This paper seeks to contribute to the neurofeedback efficacy debate by proposing a new model-
ing angle, namely the cognitive system of the subject. Importantly, our study makes the brain
activity of the subject implicit. We propose a generic family of models to characterize most brain-
computer interface training paradigms as complex inference problems that subjects must solve
through perception, action and learning despite uncertain priors and feedback. This formulation
allows researchers to explicitly model the effect of key variables such as the prior confidence of the
subjects, the influence of experimenter instructions, the quality of the feedback or the motivation
of the subjects, where previous physiology-based modeling approaches struggled.
We utilize the Active Inference framework to simulate the behavior of biologically plausible
artificialagentstaskedwithsolvingourproposedproblems. Thisendeavorenablesexperimentation
on thousands of synthetic subjects, allowing us to estimate the outcomes of basic experimental
protocols without relying on time-consuming and often underpowered studies.
The predictions derived from these minimal models were used to provide a first quantitative
accountoftheinfluenceofbiomarkernoise,subjectexpectationsandlearningmechanismsinneuro-
feedbacktraining. Finally, thismechanisticaccountofsubjectexperiencesuggeststhatdeveloping
a precise interoceptive signal not only makes the training easier but may also be essential in order
to generalize the effect of neurofeedback training to everyday life environments.
7 Acknowledgements
The authors thank L. Da Costa for the insightful discussions.
8 Competing interests
The authors declare no competing interests.
35
9 Ressources
Computations of the Active Inference agents were made on a custom Python implementation
of the SPM12 toolbox Matlab MDP_VB_X.m and MDP_VB_XX.m scripts (available at
https://www.fil.ion.ucl.ac.uk/spm/) with minimal changes. The custom package used is
publicly available at url: https://github.com/Erresthor/ActivPynference_Public.
References
[AARST18] O. Alkoby, A. Abu-Rmileh, O. Shriki, and D. Todder. Can we predict who will
respond to neurofeedback? a review of the inefficacy problem and existing pre-
dictors for successful eeg neurofeedback learning. Neuroscience, 378:155–164, 5
2018.
[ABB+17] M. Arns, J.-M. Batail, S. Bioulac, M. Congedo, C. Daudet, D. Drapier, T. Fovet,
R. Jardri, M. Le-Van-Quyen, F. Lotte, D. Mehler, J.-A. Micoulaud-Franchi,
D. Purper-Ouakil, and F. Vialatte. Neurofeedback: One of today’s techniques
in psychiatry? L’Encéphale, 43(2):135–145, April 2017.
[ACT+20] MartijnArns,CRichardClark,MarkTrullinger,RogerdeBeus,MarthaMack,and
Michelle Aniftos. Neurofeedback and Attention-Deficit/Hyperactivity-Disorder
(ADHD) in children: Rating the evidence and proposed guidelines. Appl. Psy-
chophysiol. Biofeedback, 45(2):39–48, June 2020.
[ARS+09] MartijnArns,SabineDeRidder,UteStrehl,MarinusBreteler,andAntonCoenen.
Efficacyofneurofeedbacktreatmentinadhd: theeffectsoninattention,impulsivity
and hyperactivity: a meta-analysis. Clinical EEG and neuroscience, 40:180–189,
2009.
[AS13] Martijn Arns and Ute Strehl. Evidence for efficacy of neurofeedback in adhd?
American Journal of Psychiatry, 170:799–800, 7 2013.
[BBC+19] J.-M. Batail, S. Bioulac, F. Cabestaing, C. Daudet, D. Drapier, M. Fouillen,
T. Fovet, A. Hakoun, R. Jardri, C. Jeunet, F. Lotte, E. Maby, J. Mattout,
T. Medani, J.-A. Micoulaud-Franchi, J. Mladenovic, L. Perronet, L. Pillette,
T. Ros, and F. Vialatte. EEG neurofeedback research: A fertile ground for psy-
chiatry? L’Encéphale, 45(3):245–255, June 2019.
[BBPD20] YasamanBagherzadeh,DanielBaldauf,DimitriosPantazis,andRobertDesimone.
Alpha Synchrony and the Neurofeedback Control of Spatial Attention. Neuron,
105(3):577–587.e5, February 2020.
[BCB+19] Aurore Bussalb, Marco Congedo, Quentin Barthélemy, David Ojeda, Eric Acqua-
viva, Richard Delorme, and Louis Mayaud. Clinical and experimental factors
influencing the efficacy of neurofeedback in adhd: a meta-analysis. Frontiers in
Psychiatry, 10:PMC6388544, 10 2019.
[CCS+20] Marie-Constance Corsi, Mario Chavez, Denis Schwartz, Nathalie George, Laurent
Hugueville,AriE.Kahn,SophieDupont,DanielleS.Bassett,andFabrizioDeVico
Fallani. Bci learning induces core-periphery reorganization in m/eeg multiplex
brain networks. Journal of Neural Engineering, 18, 10 2020.
[CFB+16] SamueleCortese,MaiteFerrin,DanielBrandeis,MartinHoltmann,PascalAggen-
steiner, David Daley, Paramala Santosh, Emily Simonoff, Jim Stevenson, Ar-
gyrisStringaris,EdmundJ.S.Sonuga-Barke,PhilAsherson,TobiasBanaschewski,
DanielBrandeis,JanBuitelaar,DavidCoghill,SamueleCortese,DavidDaley,Ma-
rinaDanckaerts, RalfW.Dittmann, ManfredDöpfner, MaiteFerrin, ChrisHollis,
Martin Holtmann, Eric Konofal, Michel Lecendreux, Aribert Rothenberger, Para-
mala Santosh, Joseph A. Sergeant, Emily Simonoff, Edmund J. Sonuga-Barke,
Cesar Soutullo, HansChristoph Steinhausen, Jim Stevenson, Argyris Stringaris,
Eric Taylor, Saskia van der Oord, Ian Wong, and Alessandro Zuddas. Neurofeed-
back for Attention-Deficit/Hyperactivity Disorder: Meta-Analysis of Clinical and
NeuropsychologicalOutcomesFromRandomizedControlledTrials. Journal of the
American Academy of Child & Adolescent Psychiatry, 55(6):444–455, June 2016.
36
[CGBB23] ThéophileChampion, MarekGrzes,LisaBonheme,andHowardBowman. Decon-
structing deep active inference, 2023.
[CPS+14] Rex L. Cannon, H. Edmund Pigott, Tanju Surmeli, Deborah R. Simkin, R. W.
Thatcher,WernerVandenBergh,GeraldGluck,JoelF.Lubar,RichardE.Davis,
DaleS.Foster,JonathanDouglas,ATMalcolm,DonaldR.Bars,KirkLittle,Wes
Center, Marvin H. Berman, Harold L. Russell, Barbara Hammer, and J. Lucas
Koberda. The problem of patient heterogeneity and lack of proper training in
a study of eeg neurofeedback in children. The Journal of clinical psychiatry, 75
3:289–90, 2014.
[CTCB13] Andrea Chronis-Tuscano, Anil Chacko, and Russell Barkley. Key issues relevant
to the efficacy of behavioral treatment for adhd. American Journal of Psychiatry,
170:799–799, 7 2013.
[CXB+21] Chao Chen, Xiaolin Xiao, Abdelkader Nasreddine Belkacem, Lin Lu, Xin Wang,
WeiboYi,PenghaiLi,ChangmingWang,ShaSha,XixiZhao,andDongMing. Ef-
ficacyevaluationofneurofeedback-basedanxietyrelief. Frontiers in Neuroscience,
15, October 2021.
[DAH+19] Jessica Van Doren, Martijn Arns, Hartmut Heinrich, Madelon A. Vollebregt, Ute
Strehl, and Sandra K. Loo. Sustained effects of neurofeedback in adhd: a sys-
tematic review and meta-analysis. European Child and Adolescent Psychiatry,
28:293–305, 3 2019.
[Dav18] Eddy J. Davelaar. Mechanisms of Neurofeedback: A Computation-theoretic Ap-
proach. Neuroscience, 378:175–188, May 2018.
[Dav20] Eddy J. Davelaar. A multi-stage theory of neurofeedback learning. Lecture Notes
in Computer Science (including subseries Lecture Notes in Artificial Intelligence
and Lecture Notes in Bioinformatics), 12196 LNAI:118–128, 2020.
[DBVSWB13] Martine Van Dongen-Boomsma, Madelon A. Vollebregt, Dorine Slaats-Willemse,
and Jan K. Buitelaar. A randomized placebo-controlled trial of electroencephalo-
graphic(eeg)neurofeedbackinchildrenwithattention-deficit/hyperactivitydisor-
der. The Journal of Clinical Psychiatry, 74:16829, 8 2013.
[DCPS+20] LancelotDaCosta,ThomasParr,NoorSajid,SebastijanVeselic,VictoritaNeacsu,
and Karl Friston. Active inference on discrete state-spaces: A synthesis. Journal
of Mathematical Psychology, 99:102447, December 2020.
[EGHH17] Stefanie Enriquez-Geppert, René J. Huster, and Christoph S. Herrmann. Eeg-
neurofeedback as a tool to modulate cognition and behavior: A review tutorial.
Frontiers in Human Neuroscience, 11, 2 2017.
[EGSPA19] StefanieEnriquez-Geppert,DiedeSmit,MiguelGarciaPimenta,andMartijnArns.
NeurofeedbackasaTreatmentInterventioninADHD:CurrentEvidenceandPrac-
tice. Current Psychiatry Reports, 21(6):46, May 2019.
[EJSV16] Davelaar Eddy, Barnby Joe, Almasi Soma, and Eatough Virginia. Neurophe-
nomenology and neurofeedback: a pilot study. Frontiers in Human Neuroscience,
10, 2016.
[FDCH+20] KarlFriston,LancelotDaCosta,DanijarHafner,CasperHesp,andThomasParr.
Sophisticated inference, 2020.
[Fet69] E. E. Fetz. Operant conditioning of cortical unit activity. Science (New York,
N.Y.), 163(3870):955–958, February 1969.
[FFR+16] Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, John
O’Doherty, and Giovanni Pezzulo. Active inference and learning. Neuroscience &
Biobehavioral Reviews, 68:862–879, September 2016.
[FFR+17] Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, and
Giovanni Pezzulo. Active Inference: A Process Theory. Neural Computation,
29(1):1–49, January 2017.
37
[FK09] Karl Friston and Stefan Kiebel. Predictive coding under the free-energy principle.
Philosophical Transactions of the Royal Society B: Biological Sciences, 364:1211,
2009.
[FMFV+17] Thomas Fovet, Jean-Arthur Micoulaud-Franchi, François-Benoît Vialatte, Fabien
Lotte,ChristopheDaudet,Jean-MarieBatail,JérémieMattout,GuilhermeWood,
Renaud Jardri, Stefanie Enriquez-Geppert, and Tomas Ros. On assessing neu-
rofeedback effects: should double-blind replace neurophysiological mechanisms?
Brain, 140(10):e63–e63, October 2017.
[FRP+17] Karl J. Friston, Richard Rosch, Thomas Parr, Cathy Price, and Howard Bow-
man. Deep temporal models and active inference. Neuroscience & Biobehavioral
Reviews, 77:388–402, June 2017.
[FSMF20] Zafeirios Fountas, Noor Sajid, Pedro A. M. Mediano, and Karl Friston. Deep
active inference agents using monte-carlo methods, 2020.
[GHA+09a] Holger Gevensleben, Birgit Holl, Björn Albrecht, Dieter Schlamp, Oliver Kratz,
Petra Studer, Susanne Wangler, Aribert Rothenberger, Gunther H. Moll, and
HartmutHeinrich.Distincteegeffectsrelatedtoneurofeedbacktraininginchildren
with adhd: a randomized controlled trial. International journal of psychophys-
iology : official journal of the International Organization of Psychophysiology,
74:149–157, 11 2009.
[GHA+09b] Holger Gevensleben, Birgit Holl, Björn Albrecht, Claudia Vogel, Dieter Schlamp,
Oliver Kratz, Petra Studer, Aribert Rothenberger, Gunther H. Moll, and Hart-
mut Heinrich. Is neurofeedback an efficacious treatment for adhd? a randomised
controlled clinical trial. Journal of child psychology and psychiatry, and allied
disciplines, 50:780–789, 2009.
[Gru13] John Gruzelier. Eeg-neurofeedback for optimising performance. ii: Creativity, the
performing arts and ecological validity. Neuroscience and biobehavioral reviews,
44, 11 2013.
[Gru14] John H. Gruzelier. Eeg-neurofeedback for optimising performance. i: A review of
cognitiveandaffectiveoutcomeinhealthyparticipants. Neuroscience & Biobehav-
ioralReviews,44:124–141,2014.AppliedNeuroscience: Models,methods,theories,
reviews. A Society of Applied Neuroscience (SAN) special issue.
[GSF+17] Sebastian Grissmann, Martin Spuler, Josef Faller, Tanja Krumpe, Thorsten Zan-
der,AugustinKelava,ChristianScharinger,andPeterGerjets. Contextsensitivity
ofeeg-basedworkloadclassificationunderdifferentaffectivevalence. IEEE Trans-
actions on Affective Computing, pages 1–1, 2017.
[GVMS+16] A Gaume, A Vialatte, A Mora-Sánchez, C Ramdani, and F B Vialatte. A psy-
choengineering paradigm for the neurocognitive mechanisms of biofeedback and
neurofeedback. Neuroscience and Biobehavioral Reviews, 68:891–910, 2016.
[HCBI11] Barbara Hammer, Agatha Colbert, Kimberly Brown, and Elena Ilioi. Neuro-
feedback for insomnia: A pilot study of z-score smr and individualized protocols.
Applied psychophysiology and biofeedback, 36:251–64, 07 2011.
[HCW+15] Shivayogi V. Hiremath, Weidong Chen, Wei Wang, Stephen Foldes, Ying Yang,
ElizabethC.Tyler-Kabara, JenniferL.Collinger, andMichaelL.Boninger. Brain
computer interface learning for systems based on electrocorticography and intra-
cortical microelectrode arrays. Frontiers in Integrative Neuroscience, 9:1–10, 6
2015.
[HPH+82] Peter J. Hauri, Linda Percy, Carla Hellekson, Ernest Hartmann, and Diane Russ.
The treatment of psychophysiologic insomnia with biofeedback: A replication
study. Biofeedback and Self-regulation 1982 7:2, 7:223–235, 6 1982.
[HSHB20] John Hasslinger, Manoela D.Agostini Souto, Lisa Folkesson Hellstadius, and Sven
Bölte. Neurofeedback in adhd: A qualitative study of strategy use in slow cortical
potential training. PLoS ONE, 15, 6 2020.
38
[HSP+21] Casper Hesp, Ryan Smith, Thomas Parr, Micah Allen, Karl J. Friston, and
Maxwell J. D. Ramstead. Deeply Felt Affect: The Emergence of Valence in Deep
Active Inference. Neural Computation, 33(2):398–446, February 2021.
[HZL+21] Yue Hou, Shuqin Zhang, Ning Li, Zhaoyang Huang, Li Wang, and Yuping Wang.
Neurofeedback training improves anxiety trait and depressive symptom in gad.
Brain and Behavior, 11, 3 2021.
[Izh03] Eugene M. Izhikevich. Simple model of spiking neurons. IEEE Transactions on
Neural Networks, 14:1569–1572, 11 2003.
[JJ00] TommiS.JaakkolaandMichaelI.Jordan. Statistics and Computing,10(1):25–37,
2000.
[JLB+18] Camille Jeunet, Fabien Lotte, Jean-Marie Batail, Pierre Philip, and Jean-Arthur
Micoulaud Franchi. Using recent bci literature to deepen our understanding of
clinical neurofeedback: A short review. Neuroscience, 378:225–233, May 2018.
[KE] Volodymyr Kuleshov and Stefano Ermon. Bayesian learning.
[KP04] DavidC.KnillandAlexandrePouget. Thebayesianbrain: theroleofuncertainty
in neural coding and computation. Trends in Neurosciences, 27:712–719, 12 2004.
[LBJPB21] Florence Lambert-Beaudet, William-Girard Journault, Alexandre Rudziavic
Provençal, and Célyne H Bastien. Neurofeedback for insomnia: Current state
of research. World Journal of Psychiatry, 11:897, 10 2021.
[LGH+07] Ulrike Leins, Gabriella Goth, Thilo Hinterberger, Christoph Klinger, Nicola
Rumpf, and Ute Strehl. Neurofeedback for children with adhd: A comparison
of scp and theta/beta protocols. Applied Psychophysiology Biofeedback, 32:73–88,
6 2007.
[LLM13] FabienLotte,FlorianLarrue,andChristianMühl.Flawsincurrenthumantraining
protocolsforspontaneousbrain-computerinterfaces: lessonslearnedfrominstruc-
tional design. Frontiers in Human Neuroscience, 7, 2013.
[LLZ+22] Xiaodong Li, Zhonglin Li, Zhi Zou, Xiaolin Wu, Hui Gao, Caiyun Wang, Jing
Zhou, FeiQi, MiaoZhang, JunyaHe, XinQi, FengshanYan, SheweiDou, Hongju
Zhang, Li Tong, and Yongli Li. Real-time fmri neurofeedback training changes
braindegreecentralityandimprovessleepinchronicinsomniadisorder: Aresting-
state fmri study. Frontiers in Molecular Neuroscience, 0:48, 2 2022.
[LPDH22] NitzanLubianiker, ChristianParet, PeterDayan, andTalmaHendler. Neurofeed-
back through the lens of reinforcement learning. Trends Neurosci., 45(8):579–593,
August 2022.
[LSSO95] J.F.Lubar,M.O.Swartwood,J.N.Swartwood,andP.H.O’Donnell. Evaluation
of the effectiveness of EEG neurofeedback training for ADHD in a clinical set-
ting as measured by changes in T.O.V.A. scores, behavioral ratings, and WISC-R
performance. Biofeedback and Self-Regulation, 20(1):83–99, March 1995.
[LZH21] C. Loriette, C. Ziane, and S. Ben Hamed. Neurofeedback for cognitive enhance-
ment and intervention and brain plasticity. Revue Neurologique, 177:1133–1144,
11 2021.
[MFBF+19] Jean Arthur Micoulaud-Franchi, Jean Marie Batail, Thomas Fovet, Pierre Philip,
Michel Cermolacce, Aurore Jaumard-Hakoun, and François Vialatte. Towards
a pragmatic approach to a psychophysiological unit of analysis for mental and
brain disorders an eeg-copeia for neurofeedback. Applied Psychophysiology and
Biofeedback, 44:151–172, 5 2019.
[MFGF+14] Jean-Arthur Micoulaud-Franchi, Pierre Alexis Geoffroy, Guillaume Fond, Régis
Lopez, Stéphanie Bioulac, and Pierre Philip. EEG neurofeedback treatments in
children with ADHD: an updated meta-analysis of randomized controlled trials.
Frontiers in Human Neuroscience, 8:906, 2014.
39
[MG05] IainMurrayandZoubinGhahramani.Anoteontheevidenceandbayesianoccam’s
razor. 2005.
[MHS+09] Brooke S.G. Molina, Stephen P. Hinshaw, James M. Swanson, L. Eugene Arnold,
Benedetto Vitiello, Peter S. Jensen, Jeffery N. Epstein, Betsy Hoza, Lily Hecht-
man, Howard B. Abikoff, Glen R. Elliott, Laurence L. Greenhill, Jeffrey H. New-
corn, Karen C. Wells, Timothy Wigal, Robert D. Gibbons, Kwan Hur, and Patri-
ciaR.Houck. TheMTAat8Years: ProspectiveFollow-upofChildrenTreatedfor
Combined-Type ADHD in a Multisite Study. Journal of the American Academy
of Child & Adolescent Psychiatry, 48(5):484–500, May 2009.
[Mil19] Beren Millidge. Deep active inference as variational policy gradients, 2019.
[MJL14] ChristianMähl,CamilleJeunet,andFabienLotte. Eeg-basedworkloadestimation
across affective contexts. Frontiers in Neuroscience, 8, June 2014.
[MKG22] WeiJiMa,KonradKording,andDanielGoldreich. Bayesianmodelsofperception
and action, 2022.
[MMM16] H. Marzbani, H. Marateb, and M. Mansourian. Methodological Note: Neuro-
feedback: A Comprehensive Review on System Design, Methodology and Clinical
Applications. Basic and Clinical Neuroscience Journal, 7(2), 2016.
[MPP+11] Afsaneh Moradi, Farzaneh Pouladi, Nooshin Pishva, Bagher Rezaei, Maliheh Tor-
shabi, and Zahra Alam Mehrjerdi. Treatment of anxiety disorder with neuro-
feedback: Case study. Procedia - Social and Behavioral Sciences, 30:103–107, 1
2011.
[MSB21] BerenMillidge, AnilSeth, andChristopherBuckley. Predictivecoding: atheoret-
ical and experimental review. 07 2021.
[OHL+15] Yuka O Okazaki, Jörn M Horschig, Lisa Luther, Robert Oostenveld, Ikuya Mu-
rakami, and Ole Jensen. Real-time MEG neurofeedback training of posterior al-
pha activity modulates subsequent visual detection performance. Neuroimage,
107:323–332, February 2015.
[OLPS17] Ethan Oblak, Jarrod Lewis-Peacock, and James Sulzer. Self-regulation strategy,
feedback timing and hemodynamic properties modulate learning in a simulated
fmri neurofeedback environment. PLoS computational biology, 13:e1005681, 07
2017.
[OLRV21] Jay A. Olson, Michael Lifshitz, Amir Raz, and Samuel P.L. Veissière. Super
placebos: A feasibility study combining contextual factors to promote placebo
effects. Frontiers in Psychiatry, 12:644825, 3 2021.
[OMHT23] Ryoji Onagawa, Yoshihito Muraoka, Nobuhiro Hagura, and Mitsuaki Takemi.
An investigation of the effectiveness of neurofeedback training on motor perfor-
mance in healthy adults: A systematic review and meta-analysis. NeuroImage,
270:120000, April 2023.
[PBAEG21] Miguel Garcia Pimenta, Trevor Brown, Martijn Arns, and Stefanie Enriquez-
Geppert. <p>treatment efficacy and clinical effectiveness of eeg neurofeedback
as a personalized and multimodal treatment in adhd: A critical review</p>.
Neuropsychiatric Disease and Treatment, 17:637–648, 2 2021.
[PCB+19] Christie Picken, Adam R. Clarke, Robert J. Barry, Rory McCarthy, and Mark
Selikowitz. Thetheta/betaratioasanindexofcognitiveprocessinginadultswith
the combined type of attention deficit hyperactivity disorder. Clinical EEG and
Neuroscience, 51(3):167–173, December 2019.
[PCB+20] Christie Picken, Adam R. Clarke, Robert J. Barry, Rory McCarthy, and Mark
Selikowitz. Thetheta/betaratioasanindexofcognitiveprocessinginadultswith
the combined type of attention deficit hyperactivity disorder. Clinical EEG and
Neuroscience, 51:167–173, 5 2020.
[PF19a] Thomas Parr and Karl J. Friston. Generalised free energy and active inference.
Biological Cybernetics, 113(5-6):495–513, December 2019.
40
[PF19b] Thomas Parr and Karl J. Friston. Generalised free energy and active inference.
Biological cybernetics, 113:495–513, 12 2019.
[PRF18] Giovanni Pezzulo, Francesco Rigoli, and Karl J. Friston. Hierarchical active infer-
ence: Atheoryofmotivatedcontrol. Trends in Cognitive Sciences, 22(4):294–306,
April 2018.
[PRNL21] Léa Pillette, Aline Roc, Bernard N’Kaoua, and Fabien Lotte. Experimenters’
influence on mental-imagery based brain-computer interface user training. Inter-
national Journal of Human-Computer Studies, 149:102603, May 2021.
[RBCC13] Raphaelle N. Roy, Stephane Bonnet, Sylvie Charbonnier, and Aurelie Campagne.
Mentalfatigueandworkingmemoryloadestimation: Interactionandimplications
for eeg-based passive bci. In 2013 35th Annual International Conference of the
IEEE Engineering in Medicine and Biology Society (EMBC). IEEE, July 2013.
[RdlVJM20] Rubén Roy, Rocío de la Vega, Mark P. Jensen, and Jordi Miro. Neurofeedback
for pain management: A systematic review. Frontiers in Neuroscience, 14:671, 7
2020.
[REGZ+20] Tomas Ros, Stefanie Enriquez-Geppert, Vadim Zotev, Kymberly D Young, Guil-
herme Wood, Susan Whitfield-Gabrieli, Feng Wan, Patrik Vuilleumier, François
Vialatte, Dimitri Van De Ville, Doron Todder, Tanju Surmeli, James S Sulzer,
Ute Strehl, Maurice Barry Sterman, Naomi J Steiner, Bettina Sorger, Surjo R
Soekadar, Ranganatha Sitaram, Leslie H Sherlin, Michael Schönenberg, Frank
Scharnowski, Manuel Schabus, Katya Rubia, Agostinho Rosa, Miriam Reiner,
Jaime A Pineda, Christian Paret, Alexei Ossadtchi, Andrew A Nicholson, Wenya
Nan,JavierMinguez,Jean-ArthurMicoulaud-Franchi,DavidMAMehler,Michael
Lührs, Joel Lubar, Fabien Lotte, David E J Linden, Jarrod A Lewis-Peacock,
Mikhail A Lebedev, Ruth A Lanius, Andrea Kübler, Cornelia Kranczioch, Yury
Koush, Lilian Konicar, Simon H Kohl, Silivia E Kober, Manousos A Klados,
CamilleJeunet, TWPJanssen, ReneJHuster, KerstinHoedlmoser, LaurenceM
Hirshberg,StephanHeunis, TalmaHendler,MichelleHampson,AdrianGGuggis-
berg,RobertGuggenberger,JohnHGruzelier,RainerWGöbel,NicolasGninenko,
Alireza Gharabaghi, Paul Frewen, Thomas Fovet, Thalía Fernández, Carlos Es-
colano, Ann-Christine Ehlis, Renate Drechsler, R Christopher deCharms, Stefan
Debener,DirkDeRidder, EddyJDavelaar,MarcoCongedo,MarcCavazza,Mar-
inus H M Breteler, Daniel Brandeis, Jerzy Bodurka, Niels Birbaumer, Olga M
Bazanova, Beatrix Barth, Panagiotis D Bamidis, Tibor Auer, Martijn Arns, and
Robert T Thibault. Consensus on the reporting and experimental design of clini-
cal and cognitive-behavioural neurofeedback studies (CRED-nf checklist). Brain,
143(6):1674–1685, June 2020.
[RMYBCSZ21] Pablo Riesco-Matias, José Ramon Yela-Bernabé, Antonio Crego, and Elena
Sánchez-Zaballos. What do meta-analyses have to say about the efficacy of neu-
rofeedback applied to children with adhd? review of previous meta-analyses and
a new meta-analysis. Journal of Attention Disorders, 25:473–485, 2 2021.
[RPM+21] Aline Roc, Lea Pillette, Jelena Mladenovic, Camille Benaroch, Bernard N’Kaoua,
Camille Jeunet, and Fabien Lotte. A review of user training methods in brain
computer interfaces based on mental tasks. Journal of Neural Engineering,
18(1):011002, February 2021.
[SBPF21] Noor Sajid, Philip J. Ball, Thomas Parr, and Karl J. Friston. Active Inference:
Demystified and Compared. Neural Computation, 33(3):674–712, March 2021.
[SBWK14] UteStrehl,SarahM.Birkle,SonjaWörz,andBorisKotchoubey. Sustainedreduc-
tion of seizures in patients with intractable epilepsy after self-regulation training
of slow cortical potentials 10 years after. Frontiers in Human Neuroscience, 8, 8
2014.
[SFW21] Ryan Smith, Karl Friston, and Christopher Whyte. A Step-by-Step Tutorial on
Active Inference and its Application to Empirical Data. preprint, PsyArXiv, Jan-
uary 2021.
41
[SFW22] Ryan Smith, Karl J. Friston, and Christopher J. Whyte. A step-by-step tutorial
on active inference and its application to empirical data. Journal of Mathematical
Psychology, 107:102632, 4 2022.
[SGG+17] Manuel Schabus, Hermann Griessenberger, Maria-Teresa Gnjezda, Dominik P. J.
Heib, Malgorzata Wislowska, and Kerstin Hoedlmoser. Better than sham? A
double-blind placebo-controlled neurofeedback study in primary insomnia. Brain,
140(4):1041–1052, April 2017.
[SKW12] Jerry J. Shih, Dean J. Krusienski, and Jonathan R. Wolpaw. Brain-computer
interfaces in medicine. Mayo Clinic Proceedings, 87:268, 2012.
[SRS+17] RanganathaSitaram,TomasRos,LukeStoeckel,SvenHaller,FrankScharnowski,
Jarrod Lewis-Peacock, Nikolaus Weiskopf, Maria Laura Blefari, Mohit Rana,
Ethan Oblak, Niels Birbaumer, and James Sulzer. Closed-loop brain training:
the science of neurofeedback. Nature Reviews Neuroscience, 18(2):86–100, Febru-
ary 2017.
[SSHL+20] Lars Sandved-Smith, Casper Hesp, Antoine Lutz, Jérémie Mattout, Karl Friston,
and Maxwell James Ramstead. Towards a computational (neuro)phenomenology
of mental action: modelling meta-awareness and attentional control with deep-
parametric active inference. preprint, PsyArXiv, June 2020.
[SSPF20] RyanSmith,PhilippSchwartenbeck,ThomasParr,andKarlJ.Friston. Anactive
inferenceapproachtomodelingstructurelearning: Conceptlearningasanexample
case. Frontiers in Computational Neuroscience, 14:41, 5 2020.
[Tho19] Margaret C. Thompson. Critiquing the concept of bci illiteracy. Science and
engineering ethics, 25:1217–1233, 8 2019.
[TKLM20] Lucas Trambaiolli, Simon Kohl, David Linden, and David Mehler. Neurofeedback
traininginmajordepressivedisorder: asystematicreviewofclinicalefficacy,study
quality and reporting practices. 09 2020.
[TLR17] Robert T. Thibault, Michael Lifshitz, and Amir Raz. Neurofeedback or neuro-
placebo? Brain, 140(4):862–864, April 2017.
[TSB20] Alexander Tschantz, Anil K. Seth, and Christopher L. Buckley. Learning
action-oriented models through active inference. PLOS Computational Biology,
16:e1007805, 4 2020.
[TVOR18] RobertT.Thibault,SamuelVeissière,JayA.Olson,andAmirRaz. Treatingadhd
with suggestion: Neurofeedback and placebo therapeutics. Journal of attention
disorders, 22:707–711, 6 2018.
[Uel18] Kai Ueltzhöffer. Deep active inference. Biological Cybernetics, 112(6):547–573,
October 2018.
[VB10] CarmenVidaurreandBenjaminBlankertz. Towardsacureforbciilliteracy. Brain
Topography, 23:194, 6 2010.
[vDBVSWB14] Martine van Dongen-Boomsma, Madelon A. Vollebregt, Dorine Slaats-Willemse,
andJanK.Buitelaar. Drvandongen-boomsmaandcolleaguesreply. The Journal
of Clinical Psychiatry, 75:5149, 3 2014.
[VKC21] AnttiVeikkoPetteriVeilahti, LevasKovarskis, andBenjaminUltanCowley. Neu-
rofeedbackLearningIsSkillAcquisitionbutDoesNotGuaranteeTreatmentBene-
fit: Continuous-TimeAnalysisofLearning-CurvesFromaClinicalTrialforADHD.
Frontiers in Human Neuroscience, 15:668780, June 2021.
[WAMH18] Min Wang, Sherif Abdelfattah, Nour Moustafa, and Jiankun Hu. Deep gaussian
mixture-hidden markov model for classification of eeg signals. IEEE Transactions
on Emerging Topics in Computational Intelligence, 2(4):278–287, August 2018.
[WEE20] Lydia Anna Weber, Thomas Ethofer, and Ann Christine Ehlis. Predictors of
neurofeedback training outcome: A systematic review. NeuroImage: Clinical,
27:102301, 1 2020.
42
[WK18] Guilherme Wood and Silvia Erika Kober. Eeg neurofeedback is under strong
control of psychosocial factors. Applied Psychophysiology and Biofeedback,
43(4):293–300, August 2018.
[WKWN14] Guilherme Wood, Silvia Erika Kober, Matthias Witte, and Christa Neuper.
On the need to better specify the concept of "control" in brain-computer-
interfaces/neurofeedback research. Frontiers in Systems Neuroscience, 8:171, 9
2014.
[YHKI17] Ayumu Yamashita, Shunsuke Hayasaka, Mitsuo Kawato, and Hiroshi Imamizu.
Connectivity neurofeedback training can differentially change functional connec-
tivity and cognitive performance. Cerebral Cortex, 27:4960–4970, 10 2017.
[YHY+17] Takashi Yamada, Ryu Ichiro Hashimoto, Noriaki Yahata, Naho Ichikawa, Yujiro
Yoshihara,YasumasaOkamoto,NobumasaKato,HidehikoTakahashi,andMitsuo
Kawato. Resting-state functional connectivity-based biomarkers and functional
mri-based neurofeedback for psychiatric disorders: A challenge for developing
theranostic biomarkers. The international journal of neuropsychopharmacology,
20:769–781, 10 2017.
[ZCY+22] QingZhou,RuidongCheng,LinYao,XiangmingYe,andKediXu. Neurofeedback
trainingofalpharelativepowerimprovestheperformanceofmotorimagerybrain-
computer interface. Frontiers in Human Neuroscience, 16, April 2022.
[ZWLH09] Longlian Zhao, Wenqing Wu, Zuoqing Liang, and Guangshu Hu. Changes in
eeg measurements in intractable epilepsy patients with neurofeedback training.
Progress in Natural Science, 19:1509–1514, 11 2009.
43

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "An Active Inference perspective on Neurofeedback Training"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
