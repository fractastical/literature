=== IMPORTANT: ISOLATE THIS PAPER ===
You are revising a summary for ONLY the paper below. Do NOT reference or use content from any other papers.
Paper Title: Goal-Directed Planning for Habituated Agents by Active Inference Using a Variational Recurrent Neural Network
Citation Key: matsumoto2020goaldirected
REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Issues to fix:
1. Title mismatch: Paper title 'Goal-Directed Planning for Habituated Agents by Active Inference Using a Variational Recurrent Neural Network' not found in summary
2. Too short: 24 words (minimum 200)

Current draft:
Okay, I've completed the summary of the provided paper, adhering to all the specified instructions and requirements. The summary is approximately1000 words in length.

Key terms: recurrent, sensory, agents, planning, variational, network, neural, directed

=== FULL PAPER TEXT ===
Goal-Directed Planning for Habituated Agents by
Active Inference Using a Variational Recurrent Neural
Network
TakazumiMatsumoto1 JunTani2,∗
1OkinawaInstituteofScienceandTechnology,takazumi.matsumoto@oist.jp
2OkinawaInstituteofScienceandTechnology,tani1216jp@gmail.com
∗Correspondingauthor
Abstract
Itiscrucialtoaskhowagentscanachievegoalsbygeneratingactionplansusingonlypartial
models of the world acquired through habituated sensory-motor experiences. Although many
existingroboticsstudiesuseaforwardmodelframework,therearegeneralizationissueswithhigh
degreesoffreedom. Thecurrentstudyshowsthatthepredictivecoding(PC)andactiveinference
(AIF)frameworks,whichemployagenerativemodel,candevelopbettergeneralizationbylearning
apriordistributioninalowdimensionallatentstatespacerepresentingprobabilisticstructures
extracted from well habituated sensory-motor trajectories. In our proposed model, learning is
carriedoutbyinferringoptimallatentvariablesaswellassynapticweightsformaximizingthe
evidencelowerbound,whilegoal-directedplanningisaccomplishedbyinferringlatentvariablesfor
maximizingtheestimatedlowerbound. Ourproposedmodelwasevaluatedwithbothsimpleand
complexrobotictasksinsimulation,whichdemonstratedsufficientgeneralizationinlearningwith
limitedtrainingdatabysettinganintermediatevalueforaregularizationcoefficient. Furthermore,
comparativesimulationresultsshowthattheproposedmodeloutperformsaconventionalforward
model in goal-directed planning, due to the learned prior confining the search of motor plans
withintherangeofhabituatedtrajectories.
Keywords: goaldirectedplanning;activeinference;predictivecoding;variationalBayes;recurrentneural
network
1 Introduction
Itisgenerallyassumedthatagentscanneveraccessoracquirecompletemodelsoftheworld
whichtheyareinteractingwith[18,35]. Thisisbecausetheamountofexperienceaccumulatedby
interactingwiththeworldinafinitetimeislimited,andusuallytheworlditselfisalsodynamically
changing. Undersuchconditions,agentswithhighercognitivecapability,suchashumans,seemto
beabletogeneratefeasiblegoal-directedactionsbymentallyimagingpossiblebehavioralplans
usingonlypartiallydevelopedmodelsoftheworld,learningfromlimitedexperiencesofinteraction
withtheworld.
1
0202
yaM
72
]OR.sc[
1v65641.5002:viXra
Howisthispossible? Inaddressingthisproblem,thecurrentpaperproposesanovelmodelfor
goal-directedplangenerationreferredtoasgoal-directedlatentvariableinference(GLean),based
onlearningbyleveragingtworelatedframeworks,predictivecoding(PC)[34,38,28,12,21,10,13]
andactiveinference(AIF)[15,16,17,5,33,31]. Inparticular,weattempttoshowthatagentscan
generateadequategoal-directedbehaviorsbasedonlearninginthehabituatedrangeoftheworld
byconductingsimulationstudiesontheproposedmodel.
Inbrainmodelingstudies,ithaslongbeenconsideredthatthebrainusesaninternalgenerative
modeltopredictsensoryoutcomesofitsownactions. Inthisscenario,thefitbetweenthemodel’s
predictionandtheactualsensationcanbeimprovedintwoways. Thefirstisbyadaptingbelief
orintentionrepresentedbytheinternalstateofthegenerativemodelsothatthereconstruction
errorcanbeminimized[34,12,21,10]. Thiscorrespondstoperception. Thesecondapproachis
byactingontheenvironmentinamannersuchthattheresultantsensationcanbetterfitwiththe
model’sprediction[15,5,33]. TheformerideahasbeenformulatedintermsofPCandthelatterby
AIF.However,thesetwoframeworksshouldbeconsideredinunisonasperceptionandactionare
effectivelytwosidesofthesamecoininenactivecognition.
Originally, theideaofaninternalmodelforsensory-motor systems wasinvestigatedinthe
study of the forward model (FM) [29, 25, 24] (see Figure 1a). Although both FM and PC can
predictthenextlatentstateandassociatedsensoryinputs,differenttypesofconditioningonthe
predictionwereconsidered. InFM,thepredictedsensorystateisconditionedbythecurrentmotor
commandsandthecurrentlatentstate,whileinPCitisconditionedonlybythecurrentlatentstate.
Intheory,itispossibletoinferoptimalmotorcommandsforachievingdesiredstatesusingFMby
consideringadditionalcostfunctionssuchasjerkminimization,torqueminimization,trajectory
distanceminimizationetc. [25]. Inpractice,however,thisinferencetendstoproduceerroneous
solutions unless the predictive model learns the outcomes for all possible motor combinations,
whichisintractablewhenthemotorcomponenthasahighdegreeoffreedom.
(a) (b)
Figure 1: (a) The forward model and (b) the predictive coding and active inference framework
wherestate t andsense t+1 representthecurrentlatentstateandpredictionofthenextsensorystatein
termsoftheexteroceptionandproprioception. Thepredictedproprioceptioncanthenbeconverted
intoamotorcontrolsignalasnecessary,suchasbyusinganinversemodelasdepictedin(b).
However,thismaynotbethecaseifthePCandAIFframeworksareusedincombinationas
showninFigure1b. InthePCcomponent, aninternalmodelpredictsboththelatentstateand
sensoryinputsintermsoftheexteroceptionandproprioceptioninthenexttimestepbyreceivingthe
currentlatentstate. Itcanbeconsideredthatthesensorysequencesareembeddedinthelatentstate
spacethroughiterativepredictivelearning. IntheAIFcomponent,aninversemodelcanbeusedto
2
mapthepredictedsensoryinputstomotorcommandswhichcanrealizethepredictedsensation.
Suchaninversemodelcanbeimplementedinastraightforwardmannerby,forexample,aPID
controllerwhereinthenecessarymotortorquetogenerateexpectedmovementsintermsofposition
andvelocitycanbecomputedthrougherrorfeedbackbetweenthepredictedproprioception(for
e.g.,jointanglesinarobot)andtheoutcome.
Givenadesiredstatetobeachieved,theoptimalmotorcommandatthenexttimestepcanbe
obtainedbyfirstinferringanoptimallatentstateinthecurrenttimestepwhichcangeneratethebest
fitwiththedesiredstatewiththeminimalerror. Theobtainedlatentstateinthecurrenttimestepis
mappedtothesensorystateoftheproprioceptionandexteroceptionexpectedinthenexttimestep,
andtheproprioceptionisfinallymappedtoamotorcommandbytheinversemodel.
Ifweassumethatagentsactontheirenvironmentnotthroughallpossiblecombinationsof
motor command sequences but only a subset of them in terms of habituated trajectories, the
effective dimensionality of the sensory space can be reduced drastically. This also results in
significantreductionintherequireddimensionalityofthelatentstatespacewhichembedsthe
sensorysequencesthroughlearning.
Consequently, the problem of motor planning for achieving a desired state could become
tractablewhentheinferenceforanoptimallatentstatecanbelimitedinitsrelativelylowdimen-
sionalspace. Thesame,however,cannotbeappliedinthecaseofFMasthesearchforanoptimal
motorplancannotbeconstrainedwithintherangeofhabituatedmotortrajectories,asexplained
previously.
(a) (b) (c)
Figure2: Threedifferentmodelsforlearning-basedgoal-directedmotorplanning. (a)Theforward
modelimplementedinanRNN,(b)PCandAIFframeworksimplementedinanRNNusinginitial
sensitivitybylatentrandomvariablesattheinitialstep,eitherbythestochasticz orthedetermin-
t
isticd ,and(c)theproposedGLeanschemebasedonthePCandAIFframeworkimplemented
t
inavariationalRNN.Ineachcase,thehorizontalaxisindicatesprogressionthroughtime(leftto
right). Theblackarrowsrepresentcomputationintheforwardpass,whiletheredarrowsrepresent
predictionerrorbeingpropagatedduringbackpropagationthroughtime(BPTT).
TheGLeanschemeproposedinthispaperimplementstheaforementionedconsiderationsusing
avariationalrecurrentneuralnetwork(RNN)modelandtestsitinlearning-basedrobotmotor
planningtasks. Inthefollowing,webrieflyreviewhowmodelsoflearning-basedgoal-directed
planninghavebeenstudiedandhowsuchpriorstudieshavebeenextendedtothecurrentwork.
Tani[36]proposedagoal-directedplanningschemeforarobotnavigationtaskbasedonFM(see
Figure2a). Inthismodel,thelatentstatewasrepresentedbytheactivityofthecontextunitsin
3
a Jordan-type RNN [22]. During action planning, a sequence of discrete actions m ,...,m such
1 t
asbranchingornotbranchingateachencounteredbranchingpointinamazeenvironmentcan
beinferredbyminimizingtheerrorbetweenpredictedsensoryinputsatdistalstepv t+1 andthe
sensoryinputsassociatedwiththegivengoalv t+1 .
Arie et al. [2] developed a model of learning-based planning analogous to the PC and AIF
frameworks(seeFigure2b). Inthismodel,theinitialsensitivitycharacteristicsofdeterministic
dynamicsystemsisusedwhereindiversesensory-motorsequencesexperiencedareembedded
intoadistributionoftheinitiallatentstateofanRNNmodelthroughiterativelearning. Assuch,
learningasetofsensory-motorsequencesisconductedbymeansofadaptingtwodifferenttypes
ofvariables—connectivityweightsoftheRNNwhicharesharedbyallsequences,andtheinitial
statewhichisindividuallyadaptedforeachsequence. Afterlearningwithagiveninitiallatent
stated ,thecorrespondingsequenceconsistingoftheexteroceptionv andtheproprioception
0 1...t
p isgenerated. Byfeedingthepredictedproprioceptionateachtimestep p asthetargetbody
1...t t
posturetotheinversemodel,thecorrespondingmotorcommandm canbegenerated. Inplanning
t
mode,theinitialstateisinferredsuchthatthedistalstateinageneratedsensory-motorsequence
canagreewiththedesiredgoalstatewithminimalerror. Theinferredinitialstaterepresentsan
intentionorbelieftogenerateamotorprogramreachingthegoalstate.
Similar work by Choi et al. [8] employed a deterministic RNN architecture to accomplish
goal-directedmotorplanningwithvisualpredictionsforrobotictasksbysearchinginthisinitial
statespace.Inthiscase,whilethenetworkwasabledemonstrateadequategeneralizationforsimple
taskssuchastouchingapointwitharobotarm,thesuccessratewasconsiderablyreducedinamore
complexgraspandplacetask. Recently,Jungetal. [23]extendedthismodelbyallowingrandom
variablesz withmeanandvariancetorepresenttheinitialstatesforthepurposeofextractinga
0
probabilisticdistributionamongtrainedsensory-motorsequences(seeFigure2b).Theexperimental
resultsusingthismodelforataskofstackingmultipleobjectsbyarobotarmshowedthatthis
schemeofusingrandomvariablesfortheinitiallatentstateisbeneficialintermsofgeneralization
inbothtrainingandmotorplangeneration.
Inthiscurrentpaper,weproposeafurtherdevelopmentoftheaforementionedmodelusingthe
frameworkofPCandAIFtotackletheissueoflearning-basedgoal-directedmotorplanningby
expandinguponthevariationalBayesapproach. ThemainpurposeofourproposedGLeanscheme
istoenablethenetworktolearntoextractthetransitionprobabilitydistributionofthelatentstate
ateachtimestepasasequenceprior[9]andtoutilizeitforgeneratinggoal-directedplanswith
improvedgeneralization. Forthispurpose,weutilizearecentlyproposedvariationalRNNknown
as the predictive-coding inspired variational RNN (PV-RNN) [1] for implementing the PC and
AIFframeworkssuchthatthelatentstateateachtimestepisrepresentedbybothadeterministic
variabled andarandomvariablez asshowninFigure2c. Learningofthemodelisaccomplished
t t
bymaximizingtheevidencelowerbound,whereastheestimatedlowerboundismaximizedfor
goal-directed motor plan generation. Both lower bounds are computed as summations of the
accuracytermandthecomplexityterm. AformaldescriptionofthemodelisgiveninSection2.
The proposed model also uses ideas considered in development of the so-called Multiple
Timescale RNNs (MTRNN) [39], which is built on multiple layers of Continuous Time RNNs
(CTRNN)[3]whereinhigherlayershaveslowertimescaledynamicsandlowerlayershavefaster
dynamics (note that Figure 2 shows only a single layer for simplicity). It has been shown that
MTRNN enhances development of functional hierarchy among layers. It does so by using the
timescaledifferencebywhichmoreabstractrepresentationsofactionplansaredevelopedinthe
higherlayerswhileamoredetailedrepresentationofsensory-motorpatternsdevelopinthelower
4
layers[30].
In Section 3 we evaluate GLean by conducting two sets of simulated experiments. Using a
minimaltaskset,thefirstexperimentexaminesessentialcharacteristicsoftheproposedmodelin
learningtogenerategoal-directedplans. Inparticular,weinvestigatetheeffectsthatregulatingthe
strengthofthecomplexityterminthelowerboundhasuponlearningperformanceaswellasgoal-
directedmotorplangeneration. Furthermore,wecomparethedifferenceinplanningperformance
betweenmorehabituatedgoalstatesandlesshabituatedgoalstatesinordertoexaminetheeffect
ofhabituationinlearningongoal-directedplangeneration.
Thesecondsimulationexperimentusesamorerealisticrobotictaskemployingamodelofareal
robotandcomparestheperformancebetweenthreemodelsdepictedinFigure2: FM,PC+AIF
withinitialstatesensitivity,andtheproposedGLeanscheme. Theseexperimentswillclarifyhow
GLeancangeneratefeasiblegoal-directedplansandtheresultantactions. Itdoessobydeveloping
regionsofhabituationintermsofthesequencepriorinthelatentstatespacebymeansoflearning
fromalimitedamountofsensory-motorexperiences.
2 Model
Inthissection, wewillfirstpresentanoverviewofthePV-RNNmodelfollowedbyamore
detailedexplanationoftrainingandplanning,includingformulationoftheevidencelowerbound
andapproximatelowerboundusedintrainingandplanningrespectively. Wedonotattemptto
makeanexhaustivederivationofPV-RNNinthispaper,ratherwefocusonthesalientpointsand
changescomparedtotheoriginallyproposedmodelin[1].
2.1 OverviewofPV-RNN
Figure3showsagraphicalrepresentationofPV-RNNasimplementedinthispaper. Notethat
forgeneralitywedenotealltheoutputofthemodelasx. ComparedtotheoriginalPV-RNN,we
havemadethreekeychangestothemodel. Thefirstisatt =1,thepriordistributionisfixedasa
unitGaussian(depictedaszUG),whichactsasaweightedregularizationoninitialstatesensitivity
ofthenetwork.Thisisprimarilytoimprovethestabilityoflearningincertainedgeconditions.Note
thatthedeterministicvariablesarealwaysinitializedatzeroatt =0. Inpractice,thedeterministic
variableswilltendtolearnthemeanofthetrainingsequencewhilethestochasticvariableswill
learnthedeviationsfromthemean.
Secondly,bottom-upconnectionsfromlowerlayerstohigherlayershavebeenremovedinorder
tosimplifythemodel. Predictionerrorfromlowerlayersisstillconveyedtohigherlayersduring
back-propagation. Additionally, connections between z and the output x have been removed.
Preliminarytestinghasnotshownanydegradationofplanningperformanceduetothischange.
Finally, connections between d and the posterior distribution zq have been removed. Thus
t
informationfromtheprevioustimestepflowsbetweenstochasticunitsonlybyhowclosetheprior
andposteriordistributionsare,whichisregulatedbythemeta-priorsetting. Whilethischange
couldimpactlearningperformance,itmakesinferenceoftheadaptationvariables Asimpler.
As noted previously, PV-RNN is a variational RNN comprised of deterministic variables d
andstochasticvariablesz. Themodelinfersanapproximateposteriordistributionqbytheprior
distribution pbymeansoferrorminimizationonthegeneratedoutputx. Theparameterizedprior
generativemodel p isfactorizedasshowninEquation1.
θ
5
Figure3: GraphicalrepresentationofPV-RNNasimplementedinthispaper
T
∏
p θ (x 1:T ,d 1:T ,z 1:T |d 0 ) = p θx (x t |d t )p θd (d t |d t−1 ,z t )p θz (z t |d t−1 ) (1)
t=1
NotethatunlikeintheoriginalimplementationofPV-RNN,xisnotconditioneddirectlyonz,
onlythroughd,whichisaDiracdeltafunctionasdefinedinEquation2.
(cid:40)
0 ift =0
d = (2)
t
f θd (d t−1 ,z t ) ift >0
f isaneuralnetwork—MTRNNisusedinthispaper. disthentheoutputoftheMTRNN,
θd
whichistheinternalstatehafteractivation. Foramulti-layerMTRNNinthismodel,hiscalculated
asasumofthevalueofthestochasticvariablez,theprevioustimestepoutputofthecurrentlevell
andprevioustimestepoutputofthenexthigherlevell+1asshowninEquation3.
dl =tanh(hl)
t t
hl = (cid:18) 1− 1 (cid:19) hl + 1 (cid:16) Wl,ldl +Wl,lzl +Wl+1,ldl+1 (cid:17) (3)
t τl t−1 τl d,d t−1 z,d t d,d t−1
W representconnectivityweightmatrices,inthiscasebetweenlayersandbetweendeterministic
andstochasticunits.
Notethatatthetoplayer,Wl+1,ldl+1
isomitted.
d,d t−1
6
Thepriordistribution pofzisaGaussiandistributionwhichdependsond t−1 ,exceptatt =1
whichdoesnotdependond andisfixedasaunitGaussian. µandσ forthepriordistributionare
0
obtainedfromdasshowninEquation4.
p(z ) = N(0,I)
1
p(z
t
|d
t−1
) = N(µ
t
p ,(σ
t
p)2)wheret >1
µ t p =tanh(W d l, , l z,µp d t−1 ) (4)
σ t p =exp(W d l, , l z,σp d t−1 )
BasedonthereparameterizationtrickproposedbyKingmaandWelling[26],thelatentvaluez
forbothpriorandposteriordistributionsisafunctionofµandσandanoisesample(cid:101) ∼ N(0,I).
z = µ +σ ×(cid:101) (5)
t t t
Sincecomputingthetrueposteriordistributionisintractable,themodelinfersanapproximate
posteriorqofzasdescribedinEquation6. InPV-RNN,whilesensoryinformationxisnotdirectly
availabletothenetwork,anadaptationvariable Aisused,soforeachtrainingsequencex there
1:T
isacorresponding A . Aislearnedtogetherwiththeothernetworkparametersduringtraining
1:T
basedonthepredictionerrorsebetweenxandx.
q(z |e ) = N(µ q ,(σ q)2)
t t:T t t
µ q =tanh(A µ) (6)
t t
σ q =exp(Aσ)
t t
2.2 Learningwithevidencelowerbound
Following from Equation 1, we can express the marginal likelihood (evidence) as shown in
Equation7.Asthevalueofdisdeterministic,ifweletd˜ bethevalueofd asdescribedbyEquation
t t
2,then p θd (d t |d t−1 ,z t )isequivalenttoaDiracdistributiongivenbyδ(d t −d˜ t ),whichallowsthe
integraloverdtobeeliminated.
(cid:90) (cid:90) T
∏
p θ (x 1:T |d 0 ) = p θx (x t |d t )p θd (d t |d t−1 ,z t )p θz (z t |d t−1 )dzdd
t=1
(cid:90) (cid:90) T
= ∏ p θx (x t |d˜ t )δ(d t −d˜ t )p θz (z t |d˜ t−1 )dzdd (7)
t=1
(cid:90) T
= ∏ p θx (x t |d˜ t )p θz (z t |d˜ t−1 )dz
t=1
Factoringtheintegral,takingthelogarithmandrefactoringwiththeparameterizedposterior
distributionproducesanexpectationontheposteriordistributionasshowninEquation8.
7
T (cid:90)
logp θ (x 1:T |d 0 ) =log ∏ p θx (x t |d˜ t )p θz (z t |d˜ t−1 )dz t
t=1
T (cid:90)
= ∑ log p θx (x t |d˜ t )p θz (z t |d˜ t−1 )dz t (8)
t=1
= ∑ T log (cid:90) p (x |d˜) p θz (z t |d˜ t−1 ) q (z |e )dz
θx t t q (z |e ) φ t t:T t
t=1 φ t t:T
Finally, by applying Jensen’s inequality logE[x] ≥ E[logx], the variational evidence lower
bound(ELBO)L(θ,φ)isgiveninEquation9.
(cid:34) (cid:35)
L(θ,φ) = ∑ T (cid:90) log p (x |d˜) p θz (z t |d˜ t−1 ) q (z |e )dz (9)
θx t t q (z |e ) φ t t:T t
t=1 φ t t:T
Followingtheconceptoffreeenergyminimization[14],ELBOisrewrittenintermsofexpected
loglikelihoodundertheposteriordistribution(accuracy)andtheKullback-Leiblerdivergence(KLD)
betweentheposteriorandpriordistributions(complexity)inEquation10. Thedeterministicvalue
intheexpectedloglikelihoodissubstitutedwithallpreviousstochasticvariablesbyEquation2in
ordertoallowoptimizationoftheposterioradaptivevaluesagainstthetrainingdata.Forsimplicity,
weomitthesummationovereachlayeroftheRNNandovereachtrainingsample.
T
L(θ,φ) = ∑ E qφ (zt |z1:t−1,et:T ) (cid:2) p θx (x t |d˜ t ) (cid:3) −D KL (cid:2) q φ (z t |e t:T )||p θz (z t |d˜ t−1 ) (cid:3)
t=1
∑ T (cid:2) (cid:3) (cid:2) (cid:3) (10)
= E qφ (zt |z1:t−1,et:T ) p θx (x t |z 1:t ) −D KL q φ (z t |e t:T )||p θz (z t |z 1:t−1 )
t=1 (cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125) Complexity
Accuracy
Intuitively,theaccuracytermiscalculatedbythedistancebetweenthepredictedoutputxand
thesensorystateorgroundtruthx. Inpractice,thisisastandardmeasuresuchasmeansquared
error(MSE)orKLD.
InPV-RNN,themeta-priorwisahyperparameterwhichaffectsthedegreeofregularization(or
thetendencytooverfit). ItissimilartotheβparameterinVAE[26]althoughtheeffectisreversed,
thatis,inmodelsthatassumeapriornormaldistribution,alargerregularizationconstantimplies
astrongerpulltowardthenormaldistribution,reducingcomplexityandreducingthetendency
tooverfit. However,asPV-RNNthepriorisconditionedontheoutputofprevioustimesteps,a
largermeta-priorcausesthecomplexitytoriseastheoutputbecomesdeterministic,resultingina
tendencytooverfittrainingsamples. Duringlearning,themeta-priorwillaffecttheapproximate
posteriordistributionandcauseittodeviatefromthetrueposterior,whileduringinferencethe
meta-priorwillcontrolhowmuchthepriorandapproximateposteriorwilldeviate. Weexplore
thiseffectinthefollowingSection3.
InthisimplementationofPV-RNN,thecomplexitytermatt =1isaspecialcasewheretheprior
distributionisaunitGaussianN(0,I),andtheinitialGaussianweightw controlshowclosely
I
theposteriorfollows. Thishastwoeffects—firstly,theRNNcanbemademoreorlesssensitiveto
theinitialstateatt =1byadjustingthedegreeofregularizationwithaunitGaussian. Secondly,
8
asitisindependentofthemeta-prior,itavoidsdegeneratecaseswherelearningofaprobabilistic
trainingsetisunsuccessfulduetothemeta-priorforcingdeterministicbehavior. Frompreliminary
testing,wefoundsettingsofeitherw = 0.01orw = 0.001appropriatedependingonthedata.
I I
Additionally, in this implementation of PV-RNN, we use different values of w per layer l. For
simplicity,summationovertimestepsisomittedinEquation11.

l ∑ = L 1 wl·D KL (cid:2) q φ (z t |e t:T )||p θz (z t |d˜ t−1 ) (cid:3) =   ∑ ∑ l l L L = = 1 1 w w l I ∑ ∑ σ σ , , µ µ ∈ ∈ z z l l o o g g σ σ σ t 1 q p t q , , , l l l + + ( ( σ − t p µ ,l q t − ,l 2 ) µ 2 ( 2 q t + σ ,l) p ( 2 , σ l + ) t q 2 , ( l) σ 2 t q, − l)2 1 2 − 1 2 i i f f t t = > 1 1
t t
(11)
In practice, all parameters are optimized by gradient descent, using the Adam optimizer
providedbyTensorFlow. WenotetheconditionsusedinourexperimentsinSection3.
2.3 PlangenerationwithGLeanandtheestimatedlowerbound
Plangenerationusesavariationoferrorregression[37]inordertoinferthelatentvariablesthat
minimizetheerror. However,recentworksthatutilizeerrorregression[1,6]employaregression
windowinwhicherrorisminimizedinordertoimprovefutureprediction(seeFigure4a). GLean
attempts to minimize the errors at the initial timestep and the goal timestep (see Figure 4b) by
maximizingtheestimatedlowerbound,showninEquation12.
(a) (b)
Figure4: Differenceinhowerrorregressionisemployedin(a)futuresequencepredictionand(b)
goal-directedplanning. Solidblacklinesrepresenttheforwardgenerativemodelwhilethedashed
∅
redlinesrepresentback-propagationthroughtimeusedtoupdate A .
ComparedtoELBOshowninEquation9,theaccuracytermisnowcalculatedasthesummation
ofpredictionerrorintheinitial(t =1)anddistal(t = T)steps. Inthiswork,weassumethatthe
distalstepisatafixedpointintime;inpractice,ifthegoalisreachedearly,theagentshouldremain
stationaryuntilthefinaltimestep. Thecomplexitytermisalsomodifiedsuchthat,exceptforthe
firsttimestep,theposteriordistributionisconditionedonlyonthepredictionerroratthegoal.
9
(cid:2) (cid:3) (cid:2) (cid:3)
L (θ,φ) = E p (x |z ) +E p (x |z )
e qφ (z1 |e1,eT ) θx 1 1 qφ (zT |eT ) θx T 1:T
(cid:16) (cid:2) (cid:3) ∑ T (cid:2) (cid:3)(cid:17)
− D KL q φ (z 1 |e 1 )||p θz (z 1 ) + D KL q φ (z t |e T )||p θz (z t |z 1:t−1 ) (12)
t=2
Notethatwhilethetrainedmodelisloadedbeforeplangeneration,theadaptivevariable Ais
∅ ∅
resettozero(denotedas A ). Duringplangeneration,onlytheadaptivevariable A isupdated,
whileallotherparametersremainfixed. Theimplementationofplangenerationislargelysimilar
to training, although for practical reasons the number of epochs is significantly reduced. The
learningrate,whichwerefertoasplanadaptationrateinthecontextofplangeneration,israisedto
∅
compensateforthis. Inaddition,noisesamplingisemployedbyhavingmultiple A sequences
andselectingforplanswiththehighestlowerbound.
3 Experiments
InordertotestGLean,weconductedtwoexperimentswithsimulatedagents. Thefirstexper-
imentwascarriedoutwithavirtualmobileagentina2Dspaceinordertoexaminetheimpact
ofthemeta-prioronlearningaswellasplangenerationoutputs. Thesecondexperimentuseda
simulated8DOFarmrobotcarryingoutagoal-directedobjectmovingtask,andcomparedGLean
totwopreviouslymentionedmodels—aforwardmodelandastochasticinitialstateRNN.
Duetothecomputationalworkloadofgeneratinglongsequences,particularlywhenexecuting
errorregressionforplangeneration,allplansweregeneratedinanofflinemanner. Thisallowedthe
worktoberuninbatchesonacomputercluster. Similarly,usingasimulatortocollectdataandtest
theoutcomesallowedgreaterefficiencyandautomationcomparedtousingrealrobots. However,
inthefuture,weplantoextendthisworktoreal-timetrajectoryplanningusingaphysicalrobot.
Asmentionedpreviously,weimplementedPV-RNNandGLeanusingTensorFlow. TheAdam
optimizerwasusedwithdefaultparameters,exceptforlearningrateandεˆwhichwassetto1/10
oflearningrate. Additionallyweusedrandomdropoutoftheerrorsignal(i.e. thepredictionerror
e caneitherbex−xor0).
t
ThesourcecodeforGLeanispubliclyavailableathttps://github.com/oist-cnru/GLeanfor
bothPython2.7+TensorFlow1.x(astestedinthispaper)andPython3.x+TensorFlow2.x. The
testeddatasetsarealsoincluded,togetherwithinstructionsonhowtousethesoftware.
Forthesetwosimulationexperiments,weprepareddatasetsofpossibletrajectorieswherein
aportionofthetrajectorieswereusedfortrainingofthemodelandtheremainingheldbackfor
testing. Thisprovidesthegroundtruthundervariousconditionsincludingnon-goal-directedand
goal-directedgeneration. Toevaluatetheperformanceoftrajectorygenerationafterthetraining,we
providebothplotsoftrajectoriesforqualitativeevaluationaswellastablesofqualitativemeasures.
Forgoal-directedplangeneration,wejudgethequalityofthegeneratedoutputsbycomparing
thetrajectorytothegroundtruthtrajectoryandcalculatinganaveragerootmeansquarederror
(RMSE)value. Theerroratthefinaltimestepisalsogivenseparatelyasgoaldeviation(GD).The
averageKLDbetweenpriorandposterior(KLD )isstatedasanindicationofhowcloselythe
pq
networkisfollowingitstrainedpriordistribution. Notethisisequivalenttothecomplexityterm
withoutweightingbythemeta-prior.
10
3.1 Experiment1: simulatedmobileagentina2Dspace
Togainabetterunderstandingofthegenerativecapabilitiesofourmodel, wefirstconduct
anexperimentusingasimplesimulatedagentmovingina2DspaceasshowninFigure5. The
agent’spositionatagiventimestepisgivenbyXYcoordinatesintherange[0,1]. Thetrainingdata
consistsofhanddrawntrajectoriesresampledto30timesteps,startingat[0,0],movingtoacentral
‘branchpoint’atapproximately(0.38,0.42),andthenproceedingwitha50/50chancetooneoftwo
goalareas—thetopleftcenteredaround(0.2,0.8)andthebottomrightcenteredaround(0.85,0.3)
without colliding with obstacles shown as grey areas in Figure 5a, ideally while maintaining a
smoothtrajectory.
Thebranchpointisreachedatapproximatelyt =10,withthegoalreachedbetweent =20and
t =30. Asthetrajectoriesarehanddrawnwithamouse,thereisavaryingamountofnoiseineach
trajectoryandthegoalpointsarealsodistributedinafairlyuniformdistribution. Theresultisthat
whilethetaskitselfissimple(goingfromstarttooneofgoalareas),ahabituatedpathofsortsis
generatedoutofthebundleoftrajectoriesdrawn.
(a) (b) (c)
Figure5: Plotsofthetrajectoriespreparedforamobileagentgeneratinggoal-directedbehaviors
in2Dspace. (a)XYplotshowingtheinitialpositionoftheagent,thebranchpoint,andthetwo
goalareas,(b)theplotoftheXpositionovertime,and(c)theplotoftheYpositionovertime. The
branchpointisvisibleataroundt =10.
Asnotedpreviously,weusePV-RNNwhichitselfisbuiltonMTRNN.Inthisexperiment,we
configurethenetworktohavetwolayers(notethatlayer1isthebottomlayer)withparametersas
showninTable1. Neuronsrefertothenumberofdeterministicvariables,whileZ-unitsreferto
thenumberofstochasticvariables. Thesearekeptina10:1ratioasin[1]. τistheMTRNNtime
constant,withshortertimeconstantsusedinthelowerlayerswhichshouldbemoreresponsive,
andlongertimeconstantsinthehigherlayers. Thenetworkwastrainedfor50,000epochswitha
learningrateof0.001.
Inordertoexplorehowthemeta-prioraffectstheoutputintermsoftrajectorygenerationafter
learning,wepreparedthreenetworkstrainedwithdifferentmeta-priorvaluesthatwehavelabeled
‘weak’,‘intermediate’and‘strong’asshowninTable2.
3.1.1 Priorgeneration
Toevaluatetheabilityofthenetworktolearntoextracttheprobabilisticcharacteristicslatentin
thetrainingdata,wetestpriorgenerationoftrajectoriesusingthepriordistributionasillustrated
11
Table1: PV-RNNparametersforExperiment1
MTRNNlayer
1 2
. Neurons|dl| 20 10
Z-units|zl| 2 1
τ 4 8
Table2: Meta-priorsettingsforthe2Dexperiment
MTRNNlayer
Meta-priorsettingw 1 2
Weak 0.00001 0.000005
Intermediate 0.01 0.005
Strong 0.2 0.1
in Figure 6. Since there are no target outputs given and z is a unit Gaussian and d = 0, the
1 0
networkisnotinfluencedtogotoaparticulargoaldirection. Ideally,thedistributionoftrajectories
generatedinthismannershouldmatchthetrainingdataintermsofdistributionbetweenleftand
rightgoalareasasthepriorgenerationshouldrepresentthedistributionofhabituatedtrajectories.
Figure6: Generationusingastochasticinitialstate(unitGaussian)
Table3showsthedistributionsbetweenleftandrightgoalareasforthethreenetworkstrained
withdifferentmeta-priorsincomparisontothetrainingdata(groundtruth). Weobservedthata
weakermeta-priortendedtoallowslightlymoreskewinonedirection,howeverbyinspectingthe
plotsinFigure7,itisapparentthatthereisalargeamountofnoiseinthetrajectoriesgeneratedby
theweakmeta-priornetwork(Figure7b). Inparticular,therearelargedeviationsandtheoverall
shapeofthetrajectoriesdoesnotfollowthetrainingdataaccurately.
12
Table 3: Distribution of goals reached by networks with different meta-priors, after 60 prior
generationsequences
Trainingmeta-prior Leftgoal% Rightgoal%
Weak 38.3 61.7
Intermediate 46.7 53.3
Strong 55.0 45.0
Groundtruth 50.0 50.0
Incontrast,withalargemeta-prior(Figure7d),thereappearstohavebeenafailuretolearnthe
spreadofgoals,particularlyintherightgoalarea,resultinginsomeunexpectedtrajectories. Inthis
test,anintermediatemeta-priorwasbestatlearningtheprobabilisticstructureofthetrainingdata.
(a)
(b) (c) (d)
Figure7: Trajectoryplotsshowing(a)thetrainingdata(groundtruth),(b)priorgenerationwitha
weakmeta-prior,(c)withanintermediatemeta-prior,and(d)withastrongmeta-prior. Eachplot
contains60trajectories.
3.1.2 Targetregeneration
Asdescribedpreviously,PV-RNNlearnstheprobabilisticstructureoftrajectoriesbyembedding
ineitherinitialstatesensitivedeterministicdynamicsorstochasticdynamicsbasedonthemeta-
priorvalue.Thissuggeststhattrajectoriesforgoal-directedbehaviorswithmultiplegoals,including
13
decisionbranchingpoints,canbegeneratedeitherinaninitialstatesensitivemannerbasedon
deterministicdynamicsorinanoise-drivenmannerbasedonstochasticdynamics.
Forthepurposeofexaminingsuchpropertiesofthetrainednetworks,weconductatestfor
targetregenerationofthetrainedtrajectoriesinamannersimilartothatoriginallyusedin[1]. In
thistest,weattempttoregenerateaparticulartargetsequencefromthetrainingdatasetbyusing
theinformationofthelatentstateintheinitialstep. Thisinformationwasaresultofthetraining
process.
More specifically, as illustrated in Figure 8, the prior generation is computed but with the
posterioradaptationvariableatt =1, A ,fixedtothevalueobtainedaftertrainingonthesequence.
1
ThisresultsinthesettingoftheinitiallatentstatevaluesofZ andd withvaluesforthetrained
1 1
sequence.
Figure8: Generationusingagivenposterioradaptationvariable A
1
Inthistest,weloadaparticular A adaptationvalueforasingletrainingsequencethatendsin
1
theleftgoal(showninFigure9a)asaninitialstate,andallowthenetworktogeneratethetrajectory
60timesfromthesameinitialstatewithdifferentnoisesamplingintheZ-units. Iftheinformation
intheinitialstateissufficienttoregeneratethetrajectory,andthenetworkisdeterministic,the
generatedtrajectoryshouldalwaysmatchthegroundtruth. Werefertothisconditionas‘initial
statesensitive’.
Table 4: Distribution of goals reached by networks with different meta-priors, after 60 target
regenerationsequences
Trainingmeta-prior Leftgoal% Rightgoal%
Weak 56.7 43.3
Intermediate 70.0 30.0
Strong 100.0 0.0
Target 100.0 0.0
Results obtained after 60 target regeneration runs are summarized in Table 4 and Figure 9.
14
Table4showsthattheweakermeta-priornetworkstendtoignorethetargetandretaintheprior
distributionseenpreviously. Asthemeta-priorincreases,thedistributionofgoalsreachedskews
towardsthetarget. Fromthis,wecansurmisethatastrongtrainingmeta-priorcreatesanetwork
thatisinitialstatesensitive,whilenetworkswithaweakermeta-priordonotshowsuchatendency.
VisuallyinspectingtheplotsinFigure9showsthatincomparisontothepriorgenerationresults
inFigure7,whiletheoveralldistributionoftheoutputfromtheweakandintermediatemeta-prior
networkshavenotbeenaffectedbytheinitialstateA thetrajectoriesarenotasstable. Wealsonote
1
thatwhileastrongmeta-priorproducesanetworkwithastronginitialstatesensitivity,theresultis
notcompletelydeterministicasthereisstillanoisecomponentintheposteriordistribution.
(a)
(b) (c) (d)
Figure9: Trajectoryplotsshowing(a)thetarget(groundtruth),(b)targetregenerationwithaweak
meta-prior, (c)targetregenerationwithanintermediatemeta-prior, and(d)targetregeneration
withastrongmeta-prior. Eachplotcontains60trajectories.
Tobetterillustratetheactivityofthenetworksinregeneratingaparticularsequencegiven A at
1
eachtimestep,Figure10showsplotsoftheKLDatbothlayerswhileFigure11showsplotsofthex
coordinatesovertime.
Withastrongmeta-prior,alargespikeinKLDatt =2followedbyarapiddroptonearzero
is visible at both layers, due to the posterior taking a particular mean with negligible variance
toreconstructthetrajectoryinaninitialsensitivemanner. Notethatatt = 1,KLDisregulated
independently by w so all the networks show identical behavior. This suggests the branching
I
decisionistakenearlywithastrongmeta-priorinordertominimizeaverageKLD.
Thecasewithanintermediatemeta-priorshowsamoremoderatepeakinKLDaroundt =8
or t = 9, just beforethe branchpoint at t = 10, with KLDremaining flattowardthe end. This
suggeststhatuncertaintyinthepriorincreasesslowlyuntilthebranchpointwhileuncertaintyin
15
(a) (b) (c)
Figure 10: Plots of KLD during target regeneration given a particular A adaptation value. (a)
1
Shows KLD for weak, intermediate and strong meta-prior in the bottom layer, (b) shows KLD
forweak,intermediateandstrongmeta-priorinthetoplayer. (c)Adjuststhescaleof(b)sothe
intermediate meta-prior result can be more clearly seen. The peak in KLD in the intermediate
meta-priornetworkisvisiblearoundt =8. Theshadedareasindicatethestandarddeviationof
KLDover60generatedtrajectories.
theposterioriskeptsmallerinordertominimizethereconstructionerror.Therefore,itisconsidered
thatthebranchdecisionisbuiltupbyslowlyaccumulatingsamplednoiseintheZ-unitsuntilthe
branchpoint.
InFigure11b,wecanclearlyseethebranchpointatapproximatelyt =10. Thespreadofthe
trajectoriesisrelativelylowuntilthebranch,afterwhichisalargerspreadoftrajectoriesuntilthe
goalpointsarereached.
(a) (b) (c)
Figure11: Plotsofthexcoordinateovertime,targetregenerationgivenaparticular A adaptation
1
valuewith(a)aweakmeta-prior,(b)anintermediatemeta-prior,and(c)astrongmeta-prior. The
branchpointisvisiblearoundt =10,exceptin(c)whichdoesnotexhibitanybranchingbehavior.
Withaweakmeta-prior,KLDbehavesslightlydifferentlyinthetwolayers. Inthetoplayer,the
KLDoftheweakmeta-priornetworkbehavessimilarlytotheintermediatemeta-priornetwork
untilthebranchpoint,afterwhichtheKLDdoesnotdecline—ratheritcontinuestoriseandthe
spread of KLD values also increases significantly. In the bottom layer, KLD remains low until
aroundt =18,wherethereisasuddenriseinKLD.
16
Inthiscase,wesurmisethatuncertaintyinthepriorcontinuestobuildevenafterthebranch
point. ThisisvisibleinFigure11awithanumberoftrajectoriessuddenlyswitchingfromonegoal
totheotherafterthebranchpoint. NotethatduetoweightingofKLDbytheweakmeta-prior,the
complexitytermremainssmall,resultinginlittlepressuretofollowlearnedpriors. Inaddition,the
highuncertaintytowardtheendofthesequencesislikelythereasonwhyinthefollowingtestthe
weakmeta-priornetworkisabletogeneratetrajectoriesthatendclosertountrainedgoalsthan
othernetworks.
3.1.3 Plangeneration
ToevaluateGLean, wefirsttookthethreenetworkspreviouslytrainedwithdifferentmeta-
priorsandthenranplangenerationtoevaluatetheimpactofmeta-priorduringtrainingonthe
generatedmotorplans. Plangenerationwasconductedusingatestdatasetcontaining20untrained
sequences,withtheinitialXYandgoalXYcoordinatesofeachtestsequenceusedtogenerate20
motorplans. Thegoalcoordinatesofthetestdataaretakenfromthesamedistributionasinthe
trainingdata. GLeanwasallowedtorunfor500epochs,withaplanadaptationrate(equivalent
tolearningrateintraining)of0.05. Thegeneratedmotorplanswerethencomparedagainstthe
groundtruthsequences.
Inthefollowingresults,wepresentquantitativeresultsinatable,alongwiththemeta-priorset-
tingfortrainingandplanning. AsplangenerationusingGLeanisbynecessityanon-deterministic
process,plangenerationwasrepeated10timesforeachnetwork,withtheresultbeingaveraged
overthe10runs. Averagerootmeansquarederror(RMSE)representshowcloselythegenerated
plans match the ground truth trajectories for each goal, while the average goal deviation (GD)
represents the final distance to the goal. For both RMSE and GD, the standard deviation over
10runsisgiven,andthelowestresultishighlightedasthebest. AverageKLD representsthe
pq
KL-divergencebetweenthepriorandposteriordistributions,unweightedbythemeta-prior. Alow
averageKLD indicatesthegeneratedplansfollowthelearnedpriordistributionclosely,whilea
pq
highaverageKLD indicatessignificantdeviationfromlearnedpatterns. Qualitativeresultsare
pq
shownintrajectoryplotsthatshowall20generatedtrajectoriesandcanbevisuallycomparedto
thetrainingdatainFigure12a.
Table5: Plangenerationresultsonthe20trajectorytestsetwithvaryingmeta-prior. Bestresult
highlightedinbold
Meta-prior AverageKLD AverageRMSE±σ AverageGD±σ
pq
Weak 159.1 0.0615±0.0042 5.2×10−6±2.1×10−6
Intermediate 3.36 0.0344±0.0021 7.8×10−5±1.9×10−5
Strong 0.17 0.0375±0.0015 6.7×10−4±8.8×10−5
Whenconductingplangenerationwiththethreenetworkstrainedwithdifferentmeta-prior
values,Table5showsthatwhiletheintermediatemeta-priornetworkhasthelowestaverageRMSE,
theweakmeta-priornetworkhasthelowestgoaldeviation.Asexpected,theaverageKLDincreases
asthemeta-priorweightisreduced, althoughwenotethatwhiletheincreaseinKLDbetween
strongandintermediateisinverselyproportionaltothechangeinmeta-prior,theaverageKLD
increasefromintermediatetoweakdoesnotfollowthesamerelationship. Inaddition,whileit
appearsthattheweakmeta-priortradesafactorof2reductionofaverageRMSEforafactorof10
17
improvementingoaldeviation,theplotsinFigure12showthatthetrajectoriesgeneratedbythe
weakmeta-priornetworkareverynoisycomparedtotheoutputfromtheothertwonetworks.
(a)
(b) (c) (d)
Figure12: Plotsshowingmotorplansofthe20testsequences. (a)Showsthegroundtruthforun-
trainedtestdataset,withtheremainingplotsgeneratedwitha(b)weakmeta-prior,(c)intermediate
meta-prior,and(d)strongmeta-priorasdescribedinTable2.
Fromvisualinspectionofthetrajectoryplotscomparedtothegroundtruth(Figure12),wecan
additionallyseethattheintermediatemeta-priornetworktendstoaveragethetrajectoriesmore
thanthestrongmeta-priornetworkthatisfollowingthetrainingdatamorestronglyandasaresult
tendstomissthegoal. Insummary,plansweregeneratedwithhighestgeneralizationinthecaseof
anintermediatevalueformeta-prior,whereastheplannedtrajectoriesbecamesignificantlymore
noisyinthecasewithaweakmeta-prior,andinthecasewithastrongmeta-priorthetrajectories
couldnotreachthespecifiedgoalswell. InSection4,wediscussfurthertheimplicationsofthe
meta-priorsettinginplangeneration.
3.1.4 Plangenerationforgoalssetinunlearnedregions
ThefollowingtestexamineswhetherGLeancanachievegoalssetoutsideofthetrainedregions.
Figure13ashows10groundtruthtrajectoriesreachinggoalsinanuntrainedregionwhichisinthe
middleoftheleftandrighttrainedgoalregions.
Figure13bshowstheresultsofplangenerationperformedbythenetworktrainedwiththe
intermediatemeta-prior,whereitisapparentthatGLeanisnotabletoreachthespecifiedgoals. In
particular,itcanbeobservedthatthetrajectoriescannotgostraightatthebranchingpoint. Instead,
they branch left and head towards the left goal region. This is likely because the learned prior
18
(a) (b)
Figure13: Plotsshowingmotorplansofthe10testsequenceswithgoalssetinanuntrainedregion.
(a)Showsthegroundtruthtesttrajectories,and(b)showstheresultsofplangeneration.
stronglypreferseitherturningleftorrightbutnotgoingstraightatthebranchingpoint. Thisresult
impliesthatGLeanismorelikelytogenerategoal-directedplantrajectorieswithinwellhabituated
areas.
3.2 Experiment2: simulatedroboticobjectmanipulationtask
InordertotesttheperformanceofGLeaninaroboticenvironment,wepreparedasimulated
environmentintheV-REPsimulatorwithamodelofareal8DOFarmrobot(aTokyoRobotics
ToroboArm)withagripperendeffector(seeFigure14). Infrontoftherobotisaworkspaceof
approximately30cmsquare,withtwocubeblocks(5cm/side)andtwocircles(5cmdiameter). The
robotalwaysstartsinasethomepositionwiththegripperbetweenandabovethefourobjects,the
blocksplacedinfrontandbehindthegripper,andthecirclesplacedleftandrightofthegripper.
ThepositionsoftheobjectsarerandomizedfollowingaGaussiandistribution,withσ (cid:39)3cm. The
taskistogenerateamotorplantograsponeoftheblocksandplaceitonadisc,aswellaspredict
thecoordinatesofthegripperandthetwoblocks.
Figure14: Simulatedrobotexecutingthegraspandplacetask. Intheworkspaceinfrontofthe
robot,therearetwograspableblocksandtwogoalcircles. Crosshairmarkersshowthepredicted
positionsofthegripperandthetwoblocks.
Inordertoachievethis, therobotistrainedwith120trajectoriesthatinvolvetherobotarm
(1)movingforwardorbackwardtograsptheappropriateobject,then(2)carrytheobjecttothe
19
desired goal. Figure 15 shows the trajectories of the gripper in two dimensions, overlaid with
anillustrationofthegripperandobjects. Thetrainingdataisgeneratedbyacustomkinematic
routinethatconvertstheworkspacecoordinatestoaseriesofpre-recordedmovementstakenfrom
ourpreviousworkwiththisrobot. Therecordedtrajectoriesareresampledto80timesteps,with
paddingattheendasnecessary.
Figure15: Trajectoriesofthegripperintwodimensions,withthemeanpositionsoftheblocksand
goalcirclesoverlaid. Dashedcirclesrepresentthestandarddeviationofthepositions.
Theinitialstateoftheenvironmentisgivenasthejointanglesoftherobotaswellasthe3D
(x,y,z)coordinatesofthegripperandblocks(totaling17dimensions),whilethegoalisgivenas
onlythecoordinatesofthegripperandblocks. UsingGLean,amotorplantograsptheappropriate
blockandtakeittothecorrectgoalisgenerated,alongwithpredictionsofthecoordinatesofthe
gripperandbothblocksateachtimestep. Attheendofthegeneratedsequence,therobotreleases
theblockandthecontrolprogramrecordsthedistancebetweenthecentersoftheblockandgoal
circle. Iftheblockandgoalcircleoverlap,thetrialisconsideredsuccessful.
In order to compare the performance of GLeanagainst other typical trajectory planning ap-
proaches,wehaveimplementedaforwardmodelwithMTRNN(seeFigure16a)andastochastic
initialstateMTRNN(seeFigure16b). FM,asmentionedpreviously,iscommonlyusedinrobotic
trajectory planning and is able to predict the next sensory state given the current sensory and
motorstates. Forthispaper,itisimplementedusingthesameMTRNNasusedbyGLean,butwith
nostochasticunits. SIisimplementedsimilarly,althoughusinga1:1ratioofdeterministicand
stochasticunitsatt =1andnostochasticunitsatt >1(asin[23]). Table6showsanoverviewof
thenetworkparametersforGLean,FM,andSI.
BothFMandSIaretrainedinapartiallyclosedloopmannertoimprovetheirgenerativeability,
blendingthegroundtruthsensorystatev andthepredictedsensorystatev asin[23]. ForSI,we
t t
useglobalnormgradientclippingwithasettingof50toensurethenetworkremainsstable. Note
thatwhiletheforwardmodelhasthusfarbeendepictedasoperatingonthemotorspacedirectly,
inthiscomparisontheforwardmodeloperatesinproprioceptionspacetomatchtheothermodels.
vˆ =0.9v +0.1v (13)
t t t
Forthisexperiment,wehaveadjustedthemeta-priorsettingsasshowninTable7. Therangeof
20
(a) (b)
Figure16: Graphicalrepresentationsof(a)theforwardmodel(FM)and(b)thestochasticinitial
state(SI)modelasimplementedinthispaper
Table 6: Network parameters used for the simulated robot experiment for 3 different models—
GLean,FM,andSI.
(a)GLean (b)FM (c)SI
MTRNNlayer MTRNNlayer MTRNNlayer
1 2 3 1 2 3 1 2 3
Neurons|dl| 30 20 10 Neurons|dl| 30 20 10 Neurons|dl| 30 20 10
Z-units|zl| 3 2 1 Z-units|zl| 0 0 0 Z-units|zl| 30* 20* 10*
τ 2 4 8 τ 2 4 8 τ 2 4 8
*StochasticZ-unitsonlyatt =1
21
meta-priorvalueshasbeenreducedsignificantlyasthistaskismuchmoresensitivetothissetting,
asshowninthefollowingresults.
Table7: Meta-priorsettingsforthesimulatedrobotexperiment
MTRNNlayer
Meta-priorsettingw L1 L2 L2
Weak 0.0004 0.0002 0.0001
Intermediate 0.0008 0.0004 0.0002
Strong 0.002 0.001 0.0005
3.2.1 Plangeneration
As in plan generation results with the 2D dataset in Section 3.1.3, here we use a test set of
20untrainedtrajectories, withtheresultsbeingaveragedover10plangenerationruns. GLean
wasallowedtorunfor1000epochs, withaplanadaptationrateof0.1. InTable8, wecompare
generatedtrajectoriestogroundtruthtrajectories,andhereitcanbesurmisedagainthatfindingan
intermediatesettingofthemeta-priorgivesthebestresult—notonlyintermsofbeingclosetothe
groundtruthbutintermsofsuccessrateinaccomplishingthegraspandplacetaskinthesimulator.
InTable9,wesummarizetheresultsofexecutingthegeneratedplansusingtherobotsimulator—
comprisingofsuccessrateatthetaskaswellastheaveragedistanceofthefinalblockposition
fromthegoal,thelatteronlybeingcountedinsuccessfulattempts. Asinourpreviousworkwitha
similargraspingtask,succeedinginthistaskrequireshighaccuracyatthegrasppointinthemiddle
ofthetrajectory. Thus,eventhoughthedifferencesinthegeneratedtrajectoriesarerelativelysmall,
theoutcomesinsimulationaresignificantlyaltered.
Notethatdespitetheweakmeta-priorofferingatheoreticallylowergoaldeviation,theactual
measurederroratthegoalishigherthantheintermediatemeta-priornetwork. Thisisduetothe
averagedistancefromthegoalmeasuringfromtheblockcentertothegoalcenter,andiftheblock
wasoff-centerwhenitwasgrasped,itwouldlikelybeoff-centerwhenitisplacedonthegoal. Due
totheweakmeta-priornetworkbeinglessaccurateduringtheintermediatesteps,highererrorsat
thegrasppointislikely.
Table8: GLeangeneratedplanswithnetworkstrainedwithdifferentmeta-priors,comparedwith
groundtruth. Notethatinorderfortheresultsinthefollowingtablestobecomparabletothe
previousexperiment,theoutputvalueswererescaledto[0,1].Onlythesensorystatesarecompared
betweengeneratedandgroundtruthtrajectories. Bestresulthighlightedinbold
Meta-prior AverageKLD AverageRMSE±σ AverageGD±σ
pq
Weak 12.48 0.0387±0.00067 5.7×10−5±7.4×10−6
Intermediate 4.64 0.0230±0.00058 6.9×10−5±8.6×10−6
Strong 2.35 0.0242±0.00051 1.3×10−4±1.4×10−5
Asanillustrativeexample,Figure17showsgeneratedplansconsistingofpredictedsensoryand
motorstatesgiventheinitialenvironmentalstateandthegoalsensoryimage. Assuggestedbythe
22
Table9:SimulationresultsofexecutingGLeangeneratedplanswithnetworkstrainedwithdifferent
meta-priors. Bestresulthighlightedinbold
Meta-prior Successrate Averageerroratgoal±σ
Weak 51.5% 1.74±0.15cm
Intermediate 86.0% 1.52±0.07cm
Strong 60.5% 2.02±0.11cm
overallresults,GLeantrainedwithanintermediatemeta-priorappearstogeneratetrajectoriesthat
mostresemblethegroundtruthtrajectory. Whileinsomeothertasksitispossiblethatthetrajectory
betweenthestartandthegoalisnotcritical,inorderfortherobottosuccessfullycompletethetask
inthisexperimentaccuracyisrequiredatthepointwheretherobotgraspstheblock.
(a) (b) (c) (d)
Figure17: Plotsshowingthepredictedsensorystates(toprow)andthemotorplans(bottomrow)
foragivengoal. Thecoloredlineswithineachplotrepresentasequenceofpredictionsforone
sensoryorproprioceptiondimension. Thecolumnsofplotscorrespondto(a)weakmeta-prior,
(b)intermediatemeta-prior,(c)strongmeta-prior,and(d)groundtruth. Anarrowindicatesthe
grasppoint,wheretherobotattemptstopickuptheblock. Whiletheexacttimestepofthegrasp
pointcanvary,iftherelationshipbetweenthepredicteddimensionsisnotmaintainedthegrasping
attemptismorelikelytofail.
3.3 ComparisonbetweenGLean,FM,andSI
Finally,wecomparetheplangenerationperformanceofGLeanagainstthestochasticinitial
state(SI)modelandtheforwardmodel(FM).GLeaninthistestisrepresentedbytheintermediate
meta-prior network from the previous test. Results from SI are averaged over 10 runs, as with
23
GLean. FMisdeterministicandthussomestatisticssuchasKLD andσareomitted.
pq
Table10: PlangenerationresultsofGLean,FM,andSI.Bestresulthighlightedinbold
Model AverageKLD AverageRMSE±σ AverageGD±σ
pq
Forwardmodel – 0.1504 6.8×10−3
Stochasticinitialstate 3.32 0.0257±0.00085 7.8×10−5±3.3×10−6
GLean 4.64 0.0230±0.00058 6.9×10−5±8.6×10−6
FromTable10,whichevaluatesthethreealgorithms’planningperformancecomparedtothe
groundtruth,wecanobservethatFMistheworstperformer,withRMSEanorderofmagnitude
andGDtwoordersofmagnitudeworsethaneitherGLeanorSI.Ontheotherhand,GLeanand
SIarerelativelycloseinthistheoreticalplanningperformance. However,assummarizedinTable
11,executingthegeneratedplansintherobotsimulatordemonstratesasignificantadvantagefor
GLean. FMisunabletogenerateanyplausiblemotorplansandthusachievednosuccessfulruns.
Table 11: Simulation results of executing plans generated by GLean, FM, and SI. Best result
highlightedinbold
Model Successrate Averageerroratgoal±σ
Forwardmodel(FM) 0.0% –
Stochasticinitialstate(SI) 68.0% 2.02±0.14cm
GLean 86.0% 1.52±0.07cm
GiventhatGLeanandSIwereabletogenerateplanswithsuccessfuloutcomeswhileFMhadno
successfulplans,itisapparentthattheforwardmodelinthisconditionisnotcapableofgenerating
goal-directedplans. ThisisvisibleinFigure18,showingacomparisonbetweengeneratedsensory
predictionsandgroundtruthsensorystates,whereunlikeGLeanandSI,FMisunabletofindany
motorplaninordertogenerateaplausiblesensoryprediction. Naturally,wemayaskwhetherthe
(a)FM (b)SI (c)GLean
Figure18: Comparisonbetweenthegeneratedsensorypredictions(solidlines)andtheground
truthsensorystates(dashedlines)for(a)forwardmodel,(b)stochasticinitialstate,and(c)GLean
observedfailureofplangenerationbyFMisduetoinsufficientsensorypredictioncapability. In
24
ordertoexaminethis,one-steplookaheadpredictioncapabilityinthethreemodelswerecompared.
InFM,one-steplookaheadpredictionwasgeneratedateachcurrenttimestepbyprovidingthe
groundtruthsensory-motorsequenceinputsuptothecurrenttimestep. Theresultantsensory
predictionwascomparedwiththegroundtruthsensoryinputs. ForGLeanandSI,one-steplook
aheadpredictionwasgeneratedanalogously. Specifically, withGLean, thiswasdonebyusing
errorregressionforinferringthelatentstateateachtimestepuntilthecurrenttimestep. WithSI,the
latentstateisinferredattheinitialtimesteponly.
Table12: ComparisonofaverageerrorsinsensorypredictionsgeneratedbyGLean,FM,andSI
whenprovidedwiththegroundtruthmotorstates
Model AverageRMSE
Forwardmodel(FM) 0.0119
Stochasticinitialstate(SI) 0.0107
GLean 0.0086
Table12showstheresultofcomparisonamongthosethreemodels. Itcanbeobservedthatthe
predictioncapabilitiesofthesethreemodelsarerelativelysimilar. Inparticular,bylookingata
comparisonofone-stepaheadpredictionforanexampletrajectoryamongthemodelsasshownin
Figure19,weobservethatFMisabletogenerateadequatesensorypredictionsinasimilarmanner
toSIandGLean. ThisresultsuggeststhatthefailureofmotorplangenerationbyFMisnotdue
tolackofsensorypredictioncapabilitybutduetootherreasons. Wespeculatethatthisiscaused
bythefactthatnopriorknowledgeorconstraintsexistforgeneratingmotorsequencesinFM.We
discussthisissueinSection4.
(a)FM (b)SI (c)GLean
Figure19: Comparisonofone-steplookaheadsensoryprediction(solidlines)andthegroundtruth
(dashedlines)amongthreedifferentmodels—(a)forwardmodel,(b)stochasticinitialstate,and(c)
GLean
4 Conclusion and Discussion
ThecurrentstudyproposedGLeanasanovelgoal-directedplanningschemetoinvestigatethe
problemofhowagentscangenerateeffectivegoal-directedplansbasedonlearningusinglimited
25
amountandrangeofsensory-motorexperiences. GLeanwasdevelopedusingtheframeworksof
predictivecoding(PC)andactiveinference(AIF).Withtheseframeworks,learningisconducted
by inferring optimal latent variables and synaptic weights for maximizing the evidence lower
bound. In a similar manner, GLean accomplishes goal-directed planning by inferring optimal
latentvariablesformaximizingtheestimatedlowerbound. ActualimplementationofGLeanwas
achievedbyusingthepredictivecodinginspiredvariationalrecurrentneuralnetwork(PV-RNN)
previouslyproposedbyourgroup[1].
Themodelwasevaluatedusingasimplevirtualagentin2Denvironmentandamorecomplex
simulatedroboticpickandplacetask. Theanalysisbasedontheresultsfromthefirstexperiment
revealed that the prior distribution developed initial state sensitive deterministic dynamics by
increasing the complexity with a strong meta-prior. Meanwhile, it developed noisy stochastic
dynamicsbyreducingthecomplexitywithaweakermeta-prior.
BothexperimentsshowedthatGLeanproducesthebestperformanceingoal-directedplanning
by achieving sufficient generalization in learning when setting the meta-prior to an adequate
intermediate value between the two extremes of weak and strong during the learning phase.
Furthermore,itwasshownthatmotorplanscannotbegeneratedforthosegoalssetinunlearned
regions. Thisisbecausethelearnedpriortendstopreventthemotortrajectoryfromgoingbeyond
thelearnedregion.
TheperformanceofGLeaningoal-directedplanningwascomparedagainsttheforwardmodel
(FM)andstochasticinitialstatemodel(SI)usingtheroboticpickandplacetask. Theresultsshowed
thatGLeanoutperformstheothertwomodels,especiallywhenconsideringthesimulationresults.
Moreover, it was shown that FM cannot generate corresponding motor plans at all even with
sufficientcapabilityforpredictingnexttimestepsensoryinputswhenprovidedwiththecurrent
motorcommands.ThisoutcomecanbeaccountedforbythefactthatinFMthemotorplansearchis
carriedoutwithoutanylearnedpriorsforconstraininggenerationofhypotheticalmotorsequences,
sinceFMdoesnotfacilitateanyfunctionsforprobabilisticallypredictingnextmotorcommands.
Inthiscircumstance,improbabletrajectoriesthatseeminglyreachgivengoalscanbegenerated
byarbitrarilycombiningmotorstatesinsequencesthathappentominimizethedistalgoalerror.
Ontheotherhand,inthecaseofGLeanagenerativemodelislearnedasaprobabilisticmapping
fromthelatentstateinthecurrenttimesteptotheproprioceptionintermsofthejointanglesaswell
astheexteroceptionintermsofsensorystate. Inthiscase,motorsequenceplanscanbeinferred
undertheconstraintsofthelearnedpriorbywhichgoal-directedplanscanbegeneratedwithinthe
boundaryofwell-habituatedtrajectories.
Theaforementionedideaalignswellwiththeconceptof“nicheconstruction”ofagentsdiscussed
in[27]. Itisarguedthatagentsshouldnotattempttolearncompleteglobalmodelsoftheworld,
andinsteadshouldlearnlocalmodelsofhabituatedpatternswhichshouldbefeasiblegiventhat
theamountofpossibleexperiencesintheworldiscertainlylimited. Thefreeenergyminimization
principle[14]naturallyrealizesthisasitsinherentdriveforminimizingsurpriseplaceslimitson
plangenerationaswellasactionsbeyondtheboundaryofhabituatedspace.
Another similar line of research that we have recently become aware of is model-based re-
inforcement learning for plan generation [19, 20]. An agent learns either deterministic [19] or
stochastic[20]latentdynamicsforpredictingthesensationandrewardinasimilarmannertoFM.
Theagentafterlearningcangenerateactionsnotbyusinganactionpolicynetworkasisthecasein
model-freereinforcementlearningbutbyplanninginthelatentspaceusinganevolutionarysearch
formaximizingtherewardaccumulationinthefuture.
Theaforementionedstudies,however,couldsufferfromtheproblemofgeneratingfaultyaction
26
plansbecausetheplanningprocesscannotbeconstrainedbyalearnedactionpriorforpreventing
actionspacesearchfromgoingbeyondthelearnedregion,asthecurrentpaperhasdiscussed. This
problem could be solved if the model-based learning and the model-free learning components
couldbesufficientlycombinedasthelattercouldprovidetheactionpriortobeformer.
The current study has potential to be extended in various directions in future study. One
significantdrawbackinthecurrentstudyisthatanoptimalvalueformeta-priorwhichresults
inthebestgeneralizationinlearningandplanningcanbeobtainedonlythoughtrialanderror.
Althoughourpreliminarystudyshowedthatgeneralizationislesssensitivetothesettingofthe
meta-prior when an ample amount of training data is used, the meta-prior should still be set
withinareasonablerange. Futurestudyshouldexplorepossiblemeasuresforadaptingmeta-prior
automaticallydependingthetrainingdata.
One interesting possibility is that shifts of the meta-prior during planning could affect the
quality of motor plan generation analogously to the choking effect [4, 7]. The choking effect is
thetendencyofathleticexpertstoshowperformancedisruptionsuchasdropsinthequalityand
precisionofgeneratedsensorimotorbehavior. Cappuccioetal. [7]proposedthatthiseffectcan
beaccountedforbyimpreciseprecisionmodulationduringactiveinferenceforgeneratingmotor
plans. This imprecise precision modulation could take place during the inference of the latent
variable,ifthemeta-priorinourmodelissetwithdifferentvaluesduringtheplangenerationphase
comparedtotheoptimalvaluesusedduringthelearningphase. Futurestudyshouldexploresuch
mechanismsindetail.
Another drawback is that the current investigation is limited to an offline plan generation
processes. Extendedstudiesshouldinvestigatehowthemodelcoulddealwiththeproblemof
onlineplanning,whichrequiresthemodeltoberesponsivetodynamicallychangingenvironments
inrealtime. Forthispurpose,themodelshouldbeextendedsuchthatallthreeprocessesof(1)
recognizingthecurrentsituationbymaximizingtheevidencelowerbound,(2)updatingcurrent
goal-directedplansbasedoncurrentlyrecognizedsituationbymaximizingtheestimatedlower
bound, and (3) acting on the environment by executing the plan, to be carried out in real time,
ashasbeendemonstratedbyasimulationstudyontheretrospectiveandprospectiveinference
scheme(REPRISE)[6].
In such a situation, an interesting problem to be considered is how to dynamically allocate
cognitivecomputationalresourcesrequiredforrealtimecomputationofthesemultiplecognitive
processesbyadaptingtotheon-goingsituationunderaresourceboundedcondition. Itisalso
importanttoinvestigatehowagentscanassuretheminimumcognitiveandbehavioralcompe-
tencyfortheirsurvivalwhentheoptimizationinvolvedwiththesecognitiveprocessescannotbe
guaranteedunderrealtimeconstraints. Theseresearchproblemsareleftforfuturestudies.
Althoughthecurrentstudyemployedsupervisedtrainingschemesforacquiringthegenerative
models,itmayalsobeinterestingifself-exploration-basedlearningcanbeintroducedtothescheme.
Onepossiblescenarioforachievingthiswouldbetoincorporatetheideaofintrinsicmotivation
[32,11]intothemodel. Withintrinsicmotivation, agentstendtoexploreparticulargoalsmore
frequentlyforwhichthesuccessrateofachievementimprovesmorerapidlyorothergoals[11].
Theexplorationcanswitchtoothergoalswhentheexplorationofthecurrentgoalhitsaplateauin
itsimprovement. InordertoincorporatesuchamechanismintoGLean,GLeanshouldbeextended
to facilitate a mechanism for learning a meta-policy [11] to generate its own goals, some more
frequentlythanothers,bymonitoringtheimprovementrateforsuccessfulachievementofeachgoal.
Itmaybeworthwhiletoexaminewhatsortofdevelopmentingeneratingvariousgoal-directed
behaviors,fromsimpletomorecomplex,canbeobservedbyusingthisscheme.
27
Acknowledgments
TheauthorsaregratefulforthehelpandsupportprovidedbyDr. AhmadrezaAhmadi,aswell
astheScientificComputingsectionofResearchSupportDivisionatOIST.
References
[1] A.AhmadiandJ.Tani. Anovelpredictive-coding-inspiredvariationalrnnmodelforonline
predictionandrecognition. NeuralComputation,31(11):2025–2074,2019.
[2] H.Arie,T.Endo,T.Arakaki,S.Sugano,andJ.Tani. Creatingnovelgoal-directedactionsat
criticality: aneuro-roboticexperiment. NewMathematicsandNaturalComputation,5(1):307–334,
2009.
[3] R. Beer. On the dynamics of small continuous-time recurrent neural networks. Adaptive
Behavior,3:469–509,1995.
[4] S.L.BeilockandR.Gray. Whydoathleteschokeunderpressure? InG.TenenbaumandR.C.
Eklund,editors,HandbookofSportPsychology,pages425–444.JohnWiley&SonsInc.,2007.
[5] C. Buckley, C. S. Kim, S. McGregor, and A. Seth. The free energy principle for action and
perception: Amathematicalreview. JournalofMathematicalPsychology,81:55–79,2017.
[6] M.V.Butz,D.Bilkey,D.Humaidan,A.Knott,andS.Otte. Learning,planning,andcontrolin
amonolithicneuraleventinferencearchitecture. NeuralNetworks,117:135–144,2019.
[7] M. Cappuccio, M. D. Kirchhoff, F. Alnajjar, and J. Tani. Unfulfilled prophecies in sport
performance: Activeinferenceandthechokingeffect. JournalofConsciousnessStudies,27(3–4):
152–184,2019.
[8] M.Choi,T.Matsumoto,M.Jung,andJ.Tani. Generatinggoal-directedvisuomotorplansbased
onlearningusingapredictivecoding-typedeepvisuomotorrecurrentneuralnetworkmodel.
arXiv:1803.02578,2018.
[9] J. Chung, K. Kastner, L. Dinh, K. Goel, A. C. Courville, and Y. Bengio. A recurrent
latent variable model for sequential data. In C. Cortes, N. D. Lawrence, D. D. Lee,
M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems
28, pages 2980–2988. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/
5653-a-recurrent-latent-variable-model-for-sequential-data.pdf.
[10] A.Clark. Surfinguncertainty: Prediction,action,andtheembodiedmind. OxfordUniversityPress,
2015.
[11] S.Forestier,Y.Mollard,andP.-Y.Oudeyer. Intrinsicallymotivatedgoalexplorationprocesses
withautomaticcurriculumlearning. arXiv:1708.02190,2017.
[12] K. Friston. A theory of cortical responses. Philosophical transactions of the Royal Society B:
Biologicalsciences,360(1456):815–836,2005.
[13] K.Friston. Doespredictivecodinghaveafuture? NatureNeuroscience,21:1019–1021,2018.
28
[14] K.Friston,J.Kilner,andL.Harrison. Afreeenergyprincipleforthebrain. JournalofPhysiology,
100(1–3):70–87,2006.
[15] K.Friston,J.Daunizeau,andS.Kiebel. Reinforcementlearningoractiveinference? PLoSONE,
4(7):e6421,2009.
[16] K.Friston,J.Daunizeau,J.Kilner,andS.Kiebel. Actionandbehavior: afree-energyformula-
tion. Biologicalcybernetics,102(3):227–260,2010.
[17] K.Friston, J.Mattout, andJ.Kilner. Actionunderstandingandactiveinference. Biological
Cybernetics,104(1–2):137–160,2011.
[18] X.Gabaix. Asparsity-basedmodelofboundedrationality. TheQuarterlyJournalofEconomics,
129(4):1661–1710,2014.
[19] D.HaandJ.Schmidhuber. Worldmodels. arXiv:1803.10122,2018.
[20] D.Hafner,T.Lillicrap,I.Fischer,R.Villegas,D.Ha,H.Lee,andJ.Davidson. Learninglatent
dynamicsforplanningfrompixels. InProceedingsofthe36thInternationalConferenceonMachine
Learning,2019.
[21] J.Hohwy. Thepredictivemind. OxfordUniversityPress,2013.
[22] M.I.Jordan. Attractordynamicsandparallelisminaconnectionistsequentialmachine. In
Proceedingsofthe8thAnnualConferenceofCognitiveScienceSociety,pages531–546,1986.
[23] M. Jung, T. Matsumoto, and J. Tani. Goal-directed behavior under variational predictive
coding: Dynamicorganizationofvisualattentionandworkingmemorys. InProceedingsofthe
2019IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems,pages1040–1047,2019.
[24] M.Kawato. Internalmodelsformotorcontrolandtrajectoryplanning. CurrentOpinionin
Neurobiology,9(6):718–727,1999.
[25] M.Kawato,Y.Maeda,Y.Uno,andR.Suzuki.Trajectoryformationofarmmovementbycascade
neuralnetworkmodelbasedonminimumtorque-changecriterion. BiologicalCybernetics,62:
275–288,1990.
[26] D. P. Kingma and M. Welling. Auto-encoding variational bayes. In Proceedings of the 2nd
InternationalConferenceonLearningRepresentations,2014.
[27] M.Kirchhoff,T.Parr,E.Palacios,K.Friston,andJ.Kiverstein. Themarkovblanketsoflife:
autonomy,activeinferenceandthefreeenergyprinciple. JournaloftheRoyalSocietyInterface,
15(138),2018.
[28] T.S.LeeandD.Mumford. Hierarchicalbayesianinferenceinthevisualcortex. Journalofthe
OpticalSocietyofAmericaA,20(7):1434–1448,2003.
[29] R. C. Miall and D. M. Wolpert. Forward models for physiological motor control. Neural
Networks,9(8):1265–1279,1996.
[30] R. Nishimoto, J. Namikawa, and J. Tani. Learning multiple goal-directed actions through
self-organizationofadynamicneuralnetworkmodel:Ahumanoidrobotexperiment. Adaptive
Behavior,16(2/3):166–181,2008.
29
[31] G.Oliver,P.Lanillos,andG.Cheng.Activeinferencebodyperceptionandactionforhumanoid
robots. arXiv:1906.03022,2019.
[32] P.-Y.Oudeyer,F.Kaplan,andV.V.Hafner.Intrinsicmotivationsystemsforautonomousmental
development. IEEETransactionsonEvolutionaryComputation,11(2):265–286,2007.
[33] G. Pezzulo, F. Rigoli, and K. Friston. Hierarchical active inference: A theory of motivated
control. Trendsincognitivesciences,22(4):294–306,2018.
[34] R.P.N.RaoandD.H.Ballard.Predictivecodinginthevisualcortex:afunctionalinterpretation
ofsomeextra-classicalreceptive-fieldeffects. NatureNeuroscience,2:79–87,1999.
[35] R.Selten. Boundedrationality. JournalofInstitutionalandTheoreticalEconomics,146(4):649–658,
1990.
[36] J.Tani. Model-basedlearningformobilerobotnavigationfromthedynamicalsystemsper-
spective. IEEETransactionsonSystems,ManandCyberneticsPartB:Cybernetics,26(3):421–436,
1996.
[37] J.TaniandM.Ito. Self-organizationofbehavioralprimitivesasmultipleattractordynamics:
Arobotexperiment. IEEETransactionsonSystems,Man,andCybernetics-PartA:Systemsand
Humans,33(4):481–488,2003.
[38] J.TaniandS.Nolfi. Learningtoperceivetheworldasarticulated: anapproachforhierarchical
learninginsensory-motorsystems. NeuralNetworks,12(7-8):1131–1141,1999.
[39] Y.YamashitaandJ.Tani. Emergenceoffunctionalhierarchyinamultipletimescaleneural
networkmodel: Ahumanoidrobotexperiment. PLoSComputationalBiology,4(11),2008.
30

=== REVISE TO ===
PROFESSIONAL TONE: Begin directly with content - NO conversational openings like 'Okay, here's...'

1. Fix all issues above
2. Title: "Goal-Directed Planning for Habituated Agents by Active Inference Using a Variational Recurrent Neural Network"
3. Include 10-15 quotes from paper text
   - Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
   - Use consistent quote formatting: 'The authors state: "quote"' or vary attribution phrases
   - Vary attribution phrases to avoid repetition
   - CRITICAL: Only extract quotes that actually appear in the paper text
4. ELIMINATE ALL REPETITION - each sentence must be unique
   - Check before each sentence: 'Have I already said this?' If yes, write something new
   - Vary attribution phrases - do NOT repeat 'The authors state' multiple times
5. Extract methodology, results with numbers, key quotes
6. 1000-1500 words, structured with ### headers

Generate COMPLETE revised summary.