=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Message passing-based inference in an autoregressive active inference agent
Citation Key: kouw2025message
Authors: Wouter M. Kouw, Tim N. Nisslbeck, Wouter L. N. Nuijten

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Key Terms: continuous, passing, autoregressive, factor, inference, active, agent, model, algorithms, message

=== FULL PAPER TEXT ===

Message passing-based inference in an
autoregressive active inference agent
Wouter M. Kouw1, Tim N. Nisslbeck1, and Wouter L.N. Nuijten1,2
1Eindhoven University of Technology, Eindhoven, Netherlands
2Lazy Dynamics B.V., Eindhoven, Netherlands
w.m.kouw@tue.nl
Abstract. We present the design of an autoregressive active inference
agent in the form of message passing on a factor graph. Expected free
energyisderivedanddistributedacrossaplanninggraph.Theproposed
agentisvalidatedonarobotnavigationtask,demonstratingexploration
andexploitationinacontinuous-valuedobservationspacewithbounded
continuous-valued actions. Compared to a classical optimal controller,
theagentmodulatesactionbasedonpredictiveuncertainty,arrivinglater
but with a better model of the robot’s dynamics.
Keywords: Intelligentagents·Freeenergyminimization·Activeinfer-
ence · Autoregressive models · Factor graphs · Message passing
1 Introduction
Activeinferenceisacomprehensiveframeworkthatunifiesperception,planning,
and learning under the free energy principle, offering a promising approach to
designingautonomousagents[2,18].Wepresentthedesignofanactiveinference
agentimplementedasamessagepassingprocedureonaForney-stylefactorgraph
[10,8]. The agent is built on an autoregressive model, making continuous-valued
observations and inferring bounded continuous-valued actions [7,15]. We show
that leveraging the factor graph approach produces a distributed, efficient and
modular implementation [1,3,17,22].
Probabilistic graphical models have long been a unifying framework for the
design and analysis of information processing systems, including signal process-
ing, optimal controllers, and artificially intelligent agents [4,12,5,16,8]. Many
famous algorithms can be written as message passing algorithms, including
Kalman filtering, model-predictive control, and dynamic programming [12,16].
However, it can be a challenge to formulate new algorithms due to the require-
ment of local access to variables and the difficulty of deriving backwards mes-
sages. We highlight some of these challenges, and contribute with
• thederivationofexpectedfreeenergyminimizationinamultivariateautore-
gressivemodelwithcontinuous-valuedobservationsandboundedcontinuous-
valued actions (Sec. 4.2), and
5202
peS
92
]IA.sc[
1v28452.9052:viXra
2 W.M. Kouw et al.
• the formulation of the planning model as a factor graph with marginal dis-
tribution updates based on messages passed along the graph (Figure 3).
Wevalidatetheproposeddesignonarobotnavigationtask,comparingtheagent
to an adaptive model-predictive controller.
2 Problem statement
We focus on the class of discrete-time stochastic nonlinear dynamical systems
with state z
k
∈ RDz, control u
k
∈ RDu, and observation y
k
∈ RDy at time k.
Their evolution is governed by a state transition function f and an observation
function g:
z =f(z ,u )+w , y =g(z )+v , (1)
k k−1 k k k k k
where w ,v are stochastic contributions. The agent only receives noisy out-
k k
puts y
k
∈ RDy from a system and sends control inputs u
k
∈ U ⊂ RDu back. It
must drive the system to output y without knowledge of the system’s dynam-
∗
ics. Performance is measured with free energy (which in the proposed model is
equal to the negative log evidence), Euclidean distance to goal, and the 2-norm
magnitude of controls, over the course of a trial of length T.
3 Model specification
The model is autoregressive in nature, meaning that the system output at time
k is predicted from the system input u , M previous system inputs u¯ and M
k u k y
previous system outputs y¯ :
k
 u   y   
k−1 k−1 u
k
. .
u¯
k
=

.
.


, y¯
k
=

.
.


, x
k
=u¯ k. (2)
y¯
u y k
k−Mu k−My
The vector x is the concatenation of these elements and has dimension D =
k x
D (M +1)+D M .OurlikelihoodfunctionisbasedonaGaussiandistribution
u u y y
p(y |Θ,u ,u¯ ,y¯ )=N
(cid:0)
y |A
⊺
x
,W−1(cid:1)
, (3)
k k k k k k
where A ∈ RDx×Dy is a regression coefficient matrix and W ∈ RDy×Dy is a
+
precision matrix. Let Θ = (A,W) refer to the parameters jointly. Their prior
distribution is a matrix normal Wishart distribution [21, D175]:
p(Θ)=MNW (cid:0) A,W |M ,Λ−1,Ω−1,ν (cid:1) (4)
0 0 0 0
=MN (cid:0) A|M ,Λ−1,W−1(cid:1) W (cid:0) W |Ω−1,ν (cid:1) . (5)
0 0 0 0
The prior distributions over control inputs are independent Gaussian distribu-
tions, as are the goal prior distributions for future observations:
p(u )=N(u |0,Υ−1), p(y |y )=N(y |m ,S ), (6)
k k t ∗ t ∗ ∗
where Υ is a precision matrix and y = (m ,S ) are the goal mean vector and
∗ ∗ ∗
covariance matrix.
Message passing-based inference in an autoregressive active inference agent 3
4 Inference
4.1 Learning
We use Bayesian filtering to update parameter beliefs given y ,u [19,15]:
k k
likelihood
(cid:122) (cid:125)(cid:124) (cid:123)
(cid:0) (cid:1)
p y |Θ,u ,u¯ ,y¯
(cid:0) (cid:1) k k k k (cid:0) (cid:1)
p Θ|D = p Θ|D . (7)
k (cid:0) (cid:1) k-1
p y |u ,D
(cid:124) (cid:123)(cid:122) (cid:125) k k k-1 (cid:124) (cid:123)(cid:122) (cid:125)
posterior (cid:124) (cid:123)(cid:122) (cid:125) prior
evidence
where D = {y ,u }k is short-hand for data up to time k. Note that the
k i i i=1
memories u¯ ,y¯ are subsets of D . The evidence term is the evaluation of the
k k k−1
observation y under the predictive distribution, obtained by marginalizing the
k
likelihood over the parameters [14].
M Λ0 0 MNW ··· → 1 = Θ → 3 ···
Ω
ν0
0
2 ↑
uk-Mu
···
uk-1uk
[·] MARX
xk
· N
···
yk-My yk-1 yk
Fig.1. Forney-style factor graph of one time step (separated by dots) of Bayesian
filtering. Edges represent random variables and nodes operations on those variables.
Black squares represent observed variables or set parameters, and the dotted box rep-
resentsacustomnode,composedofthenodeswithin.Message1isthepriorbeliefover
parametersandmessage2thelikelihood-basedupdate.Thesearearemultipliedatthe
equality node, yielding the marginal posterior distribution (message 3).
We express the Bayesian filtering procedure as message passing on the factor
graph1 shown in Figure 1. Message 1 is the prior distribution on Θ,
1 =MNW(A,W |M ,Λ ,Ω ,ν ). (8)
k-1 k-1 k-1 k-1
Message 2 originates from the MARX likelihood function and is an improper
matrix normal Wishart distribution [15, Lemma 2],
2 =MNW(A,W |M¯ ,Λ¯−1,Ω¯−1,ν¯ ), (9)
k k k k
with parameters based on data and buffers at time k,
ν¯ =2−D +D , Λ¯ =x x ⊺ , M¯ =(x x ⊺ )−1x y ⊺ , Ω¯ =0 . (10)
k x y k k k k k k k k k Dy×Dy
1 For excellent introductions to the factor graph approach, see [11,20].
4 W.M. Kouw et al.
It is improper because ω¯ is singular. But when multiplied with the prior dis-
k
tribution, it produces the conjugate posterior distribution exactly [15, Thm. 1].
This multiplication occurs in the equality node and produces message 3 :
3 =p(Θ|D )=MNW(A,W |M ,Λ−1,Ω−1,ν ). (11)
k k k k k
The parameters of this distribution are
ν =ν +1, (12)
k k-1
⊺
Λ =Λ +x x , (13)
k k-1 k k
M =(Λ +x x ⊺ )−1(Λ M +x y ⊺ ), (14)
k k-1 k k k-1 k-1 k k
⊺ ⊺
Ω =Ω +y y +M Λ M − (15)
k k-1 k k k-1 k-1 k-1
(Λ M +x y ⊺ ) ⊺ (Λ +x x ⊺ )−1(Λ M +x y ⊺ ).
k-1 k-1 k k k-1 k k k-1 k-1 k k
Marginalizing the Gaussian likelihood in Eq. 3 over the parameter posterior
distribution (Eq. 11) yields a multivariate location-scale T-distribution [14]:
(cid:90)
(cid:0) (cid:1)
p(y |u ,D )= p(y |Θ,u ,u¯ ,y¯ )p(Θ|D )dΘ =T y |µ (u ),Σ (u ) , (16)
k k k k k k k k ηk k k k k k
with η =ν −D +1 degrees of freedom and a mean and covariance of
k k y
   ⊺  
u u u
k 1 (cid:16) t t (cid:17)
µ k (u k )=M k ⊺ u¯ t, Σ k (u k )= ν −D +1 Ω k 1+u¯ t Λ− k 1 u¯ t . (17)
y¯ k y y¯ y¯
t t t
The subscripts under µ and Σ indicate which parameters were used, i.e., here
they refer to M ,Λ ,Ω and ν .
k k k k
4.2 Actions
Planning We start by building a generative model for the input and output at
time t=k+1:
p(y ,Θ,u |D )=p(y |Θ,u ,u¯ ,y¯)p(Θ|D )p(u ). (18)
t t k t t t t k t
Note that u¯ and y¯ are absent on the left-hand side because, at time t=k+1,
t t
these buffers are subsets of D . We want the agent to pursue a target, a specific
k
future observation. To do so, we first isolate the marginal distribution p(y ),
t
p(y |Θ,u ,u¯ ,y¯)p(Θ|D )=p(Θ|y ,u ,D )p(y ), (19)
t t t t k t t k t
and then constrain it to be the goal prior, p(y ) → p(y |y ). We use Bayes’
t t ∗
rule in the reverse direction to relate the distribution over parameters given the
future output and input, to known distributions:
p(y |Θ,u ,u¯ ,y¯)p(Θ|D )
p(Θ|y ,u ,D )= t t t t k . (20)
t t k p(y |u ,D )
t t k
Message passing-based inference in an autoregressive active inference agent 5
To obtain an approximate marginal posterior distribution for the action u , we
t
form an expected free energy functional,
F [q]=E (cid:104) E (cid:2) ln q(Θ,u t ) (cid:3)(cid:105) . (21)
k q(yt|Θ,ut,u¯t,y¯t) q(Θ,ut) p(y ,Θ,u , |y ,D )
t t ∗ k
The variational model is q(y ,Θ,u ,u¯ ,y¯) = q(y |Θ,u ,u¯ ,y¯)q(Θ)q(u ). The
t t t t t t t t t
likelihood and parameter factors are not free variational distributions but fixed
tothesameformasthelikelihoodandparameterfactorsofthegenerativemodel:
q(y |Θ,u ,u¯ ,y¯)=p(y |Θ,u ,u¯ ,y¯)=N
(cid:0)
y |A
⊺
x
,W−1(cid:1)
(22)
t t t t t t t t t t
q(Θ)=p(Θ|D )=MNW(A,W |M ,Λ−1,Ω−1,ν ). (23)
k k k k k
We then minimize this expected free energy functional with respect to the vari-
ational distribution q(u ):
t
q∗(u )=argmin F [q]. (24)
t k
q∈Q
where Q represents the set of candidate variational distributions.
Theorem 1. The optimal variational posterior q∗(u ) under the free energy
t
functional defined in (21) is proportional to a prior times a likelihood,
q∗(u )∝p(u )exp (cid:0) −G(u ) (cid:1) , (25)
t t t
where G is the sum of a mutual information and a cross-entropy term
G(u )=−E (cid:2) ln p(y t ,Θ|u t ,D k ) (cid:3) −E (cid:2) lnp(y |y ) (cid:3) . (26)
t p(yt,Θ|ut,Dk) p(y |u ,D )p(Θ|D ) p(yt|ut,Dk) t ∗
t t k k
The proof can be found in Appendix A.
Corollary 1. The expected free energy function G(u ) evaluates to:
t
G(u )=constants− 1 ln|Σ (u )|+ 1 Tr (cid:104) S−1(cid:0) Σ (u ) η t +Ξ(u ) (cid:1)(cid:105) . (27)
t 2 t t 2 ∗ t t η −2 t
t
where Ξ(u )=(µ (u )−m )(µ (u )−m )⊺.
t t t ∗ t t ∗
The proof is also in Appendix A.
Figure2providesanexampleofhowthisinferenceprocesscanbemappedto
afactorgraph,usingM =M =2.Thenodemarked"MARX"isthecomposite
y u
nodedepictedinFigure1.Itisnowconnectedtoanothercompositenodemarked
"MARX-EFE", which is connected to previous observations, parameters, goal
prior parameters y and the to-be-taken action u . Message 3 is the same as
∗ t
in Figure 1, namely the parameter posterior distribution (Sec. 4.1). Note that
duringplanning,theparametersarenotupdated.Thisisindicatedbyadirected
edge from the equality node to the MARX-EFE node. Message 4 is the goal
6 W.M. Kouw et al.
M0
Θ N
Λ0 MNW ··· =
Ω
ν0
0
uk-2
uk-1
=
uk
5 ↓
δ
= 6 ↑
3 ↓
MARX MARX-EFE
= 4 ↑
yk-2 = N
yk-1 yk
y∗
Fig.2. Factor graph of the 1-step ahead planning model. The left half of the graph is
thesameasinFigure1.Theparameterposterior(message3)ispassedforwardstothe
MARX-EFE node, which takes in message 4 from the goal prior node and produces
message6containingtheexponentiatedEFEfunction.Combinedwithmessage5from
the control prior node, this produces the variational control posterior. The δ circle
denotes a collapse of the posterior to a Dirac delta distribution [9].
priordistribution(Eq.6), 5 isthecontrolpriordistribution(Eq.6)andmessage
6 is the unnormalized exponentiated expected free energy;
4 =N(y |m ,S ) , 5 =N(u |0,Υ−1) , 6 =exp(−G(u )). (28)
t ∗ ∗ t t
Note that message 6 is the result of the EFE derivation (Thm. 1) and not the
result of minimizing the Bethe free energy, a point discussed in more detail in
Section 6.
Normalizing q∗(u ), i.e., the product of 5 and 6 , requires integrating over
t
u . This is challenging and avoided by collapsing the approximate posterior to
t
its maximum a posteriori point-mass distribution, q∗(u )≈δ(u −uˆ ) where
t t t
1
uˆ =argmax q∗(u )=argmin u ⊺ Υu +G(u ). (29)
t ut∈U t ut∈U 2 t t t
We believe this is justified because collapsing the posterior to a point estimate
is anyway required to pass controls to actuators.
Horizon Extendingthetimehorizonischallenging,requiringadditionalmarginal-
izationsthatcomplicatetheaboveresults(seeSec.6foranextendeddiscussion).
In this paper, we adopt a simpler approach and generalize the planning factor
graph (Figure 2) by including additional MARX-EFE nodes. Figure 3 shows an
extension with M = M = 2 and a time horizon of H = 4. The main differ-
y u
ence is that, for t > k+2, the buffer y¯ will no longer contain delta distributed
variables. Note that the buffer u¯ will still contain delta variables, because we
constrain the marginal action posteriors to be delta’s (Eq. 29). Inspecting the
second MARX-EFE node in Figure 3 reveals that the only change from the
Message passing-based inference in an autoregressive active inference agent 7
Θ
= = =
N N N N
δ δ δ δ
ut-1 ut ut+1 ut+2 ut+3
= = =
ut-2 = = =
MARX-EFE MARX-EFE MARX-EFE MARX-EFE
= 8 ↓ = yt+3
yt-2
=
yt+1 N
yt-1
yt → 7 yt+2 y∗
Fig.3. Factor graph of a 4-step ahead planning model, showing repeated MARX-
EFE node from Figure 2. Some buffer variables are now latent as well. Message 7 is
the posterior predictive over y carrying forward system output predictions given a
t
selection control input. Message 8 is a predictive likelihood over y sent backwards
t
from the node at time t+1. Together the forward and backward pass of predictive
messages generates a sequence of goal priors.
MARX-EFE node in Figure 2 is the incoming message for y . This message is
t
the posterior predictive distribution (given a selected action uˆ ) sent out by the
t
first MARX-EFE node in the planning graph;
7 =p(y |uˆ ,D )=T (y |µ (uˆ ),Σ (uˆ )). (30)
t t t ηk t k t k t
This message is incorporated into the MARX-EFE node function through a
variational approximation:
p(y |u ,u¯ ,y˜ ,Θ)
t+1 t+1 t+1 t+1
≈exp
(cid:0)E (cid:2)
lnp(y |u ,u¯ ,y¯ ,Θ)
(cid:3)(cid:1)
(31)
p(yt|uˆt,Dt) t+1 t+1 t+1 t+1
∝exp (cid:0) − 1 (y ⊺ Wy −2y WA ⊺E (cid:2) u u¯ y yˆ (cid:3)⊺ ) (cid:1) (32)
2 t+1 t+1 t+1 p(yt|uˆt,Dt) t+1 t+1 t t-1
∝N(y |A ⊺(cid:2) u u¯ µ (uˆ )yˆ (cid:3)⊺ ,W−1). (33)
t+1 t+1 t+1 k t t-1
Notethatthisis,inessence,stilltheMARXlikelihoodfunctionexceptwiththe
mean of the posterior predictive µ (uˆ ) instead of an observed value for y in
k t t
y¯ . We mark this change with y˜instead of y¯.
t+1
For the backwards message from the MARX-EFE node at t+1 towards the
variabley ,wefirstutilizethesamevariationalapproximationasabovebutnow
t
8 W.M. Kouw et al.
with respect to the variational factor q(y )=N(y |m ,S ) (Eq. 39);
t+1 t+1 t+1 t+1
exp
(cid:0)E (cid:2)
lnp(y |u ,u¯ ,y¯ ,Θ)
(cid:3)(cid:1)
q(yt+1) t+1 t+1 t+1 t+1
∝exp (cid:0) − 1 E (cid:2) y ⊺ Wy −2y WA ⊺(cid:2) u u¯ y yˆ (cid:3)⊺(cid:3)(cid:1) (34)
2 q(yt+1) t+1 t+1 t+1 t+1 t+1 t t-1
∝N(m |A ⊺(cid:2) u u¯ y yˆ (cid:3)⊺ ,W−1). (35)
t+1 t+1 t+1 t t-1
Then we marginalize over the parameter posterior distribution,
8 ∝E (cid:2) N(m |A ⊺(cid:2) u u¯ y yˆ (cid:3)⊺ ,W−1) (cid:3) (36)
p(Θ|Dk) t+1 t+1 t+1 t t-1
∝T (cid:0) m |µ¯ (y ),Σ¯ (y ) (cid:1) , (37)
η¯ t+1 t+1 t t+1 t
with η¯=ν −D +1 degrees of freedom and mean and covariance
k y
   ⊺  
u u u
t+1 t+1 t+1
µ¯ t+1 (y t )=M k ⊺   u¯ y t+ t 1   , Σ¯ t+1 (y t )= η 1 ¯ Ω k (cid:16) 1+    u¯ y t+ t 1   Λ− k 1   u¯ y t+ t 1   (cid:17) . (38)
yˆ yˆ yˆ
t-1 t-1 t-1
In essence, this distribution scores which values of y best predict y , with
t t+1
m as a pseudo-observation. At the y edge, we perform a variational factor
t+1 t
update based on the product of messages 7 and 8 :
q(y )∝T (cid:0) y |µ (uˆ ),Σ (uˆ ) (cid:1) T (cid:0) m |µ¯ (y ),Σ¯ (y ) (cid:1) . (39)
t ηk t k t k t η¯ t+1 t+1 t t+1 t
This product is not part of a known parametric family of distributions. We
perform a Laplace approximation to produce q(y )≈N(y |m ,S ) where [13]:
t t t t
m =argmax lnT (cid:0) y |µ (uˆ ),Σ (uˆ ) (cid:1) T (cid:0) m |µ¯ (y ),Σ¯ (y ) (cid:1) (40)
t ηk t k t k t η¯ t+1 t+1 t t+1 t
yt
(cid:12)
S−1=−∇2 lnT (cid:0) y |µ (uˆ ),Σ (uˆ ) (cid:1) T (cid:0) m |µ¯ (y ),Σ¯ (y ) (cid:1)(cid:12) . (41)
t yt ηk t k t k t η¯ t+1 t+1 t t+1 t (cid:12)
yt=mt
This Gaussian variational factor effectively becomes a goal prior for time t. So,
intheextendedtimehorizon,weseethattheforwardandbackwardpassesover
thefutureobservationsgenerateasequenceofintermediategoalpriors.Assuch,
at each future time point, the agent needs only to solve a 1-step ahead expected
free energy minimization problem.
5 Experiments
Weperformsimulationexperimentsinwhichagentshavetoreachatargetstate
inasingletrial.WerefertotheproposedfreeenergyminimizingagentasMARX-
EFE2.Thebenchmarkisthesameagentbutwithcontrolsfoundbyminimizing
a standard model-predictive control cost function;
k+H
uˆMPC = argmin (cid:88) u ⊺ Υu + (cid:0) µ (u )−m (cid:1)⊺(cid:0) µ (u )−m (cid:1) . (42)
k+1:k+H t t t t ∗ t t ∗
uk+1:k+H∈UH
t=k+1
2 Agent built with RxInfer; https://github.com/biaslab/IWAI2025-MARXEFE-MP
Message passing-based inference in an autoregressive active inference agent 9
This agent will be called MARX-MPC. We evaluate the probabilities of the
system output y under the goal prior distribution, p(y |y ). Additionally, we
k k ∗
evaluate model evidence, i.e., the probability of the system output observation
under the agent’s predictive distribution, p(y |u ,D ).
k k k
System Consider a linear Gaussian dynamical system where the state vector
z contains the two-dimensional position and velocity of a robot. The robot’s
k
state transition and measurement functions are
   
10∆t 0 0 0
(cid:20) (cid:21)
01 0 ∆t  0 0  1000
f(z k-1 ,u k )= 00 1 0   z k-1 + ∆t 0   u k , g(z k )= 0100 z k , (43)
00 0 1 0 ∆t
where ∆t is the time step size. Its covariance matrices are
 ∆t3ς 0 ∆t2ς 0 
3 1 2 1
 0 ∆t3ς 0 ∆t2ς  (cid:20) ρ 0 (cid:21)
Q= 3 2 2 2 , R= 1 , (44)
  ∆ 2 t2ς 1 0 ∆tς 1 0   0 ρ 2
0 ∆t2ς 0 ∆tς
2 2 2
with ς = (cid:2) 10−6 10−6(cid:3)⊺ and ρ= (cid:2) 10−3 10−3(cid:3)⊺ .
Prior parameters The prior parameters are weakly informative; ν = 100,
0
M =1/(D D )·I , Λ =10−2·I , Ω =I , and Υ =10−6·I . The
0 x y Dx× (cid:2) Dy 0 (cid:3) Dx 0 Dy (cid:2) D (cid:3)u⊺
system starts at z = 0000 and the goal prior has mean m = 01 and
0 ∗
covariance matrix S = 10−6·I . Buffers are fixed at M = M = 2 and the
∗ Dy u y
time horizon at H =3. Controls are limited to U=[−1, 1] for T =10000 steps
at ∆t=0.1.
Results Figure 4 shows the experimental results comparing MARX-EFE to
MARX-MPC.TheleftfigureshowsthatMARX-EFEconsistentlyscoresasmaller
freeenergythanMARX-MPC,demonstratingthatitcaresmorestronglyabout
accuratelypredictingits nextobservation. Themiddlefigure showsthe distance
to goal over the duration of the trial where, on average, MARX-MPC reaches
the goal sooner than MARX-EFE. MARX-MPC does not care about making
accurate predictions, only to close the gap to the target as quickly as possible.
It is successful in that regard but struggles to park itself on the target exactly
becauseitignoredopportunitiestolearnthefinerpartsofrobot’sdynamicsear-
lierinthetrial.TheMARX-EFEagentultimatelygetscloserthanMARX-MPC
because - by the time it gets to the goal - it has a much better model of the
robot’s dynamics. The right figure shows the 2-norm of the controls, highlight-
√
ing that MARX-MPC consistently utilizes maximum power (max∥u ∥ = 2)
t 2
to get closer. The MARX-EFE agent takes very small actions in the beginning,
when it is uncertain of their outcomes, and slowly takes larger actions when its
uncertainty shrinks. We interpret this behaviour as some form of "caution".
10 W.M. Kouw et al.
Fig.4. MARX-EFE (blue) vs. MARX-MPC (red) over a trial of 1000 seconds, com-
paredintermsoffreeenergy(left),Euclideandistancetogoal(m ;middle)and2-norm
∗
of controls (right). Results are averaged over 10 experiments. MARX-EFE initially
takessmalleractions,aimingtoimproveparametersandpredictionsfirst.Itarrivesat
the goal later than MARX-MPC but is better able to park on the goal itself. MARX-
EFE’s actions are small initially but increase in magnitude as uncertainty shrinks.
6 Discussion
For planning, we observe that the expectation over the future observation is
actually not necessary in the above model. If the likelihood is incorporated into
the numerator of Eq. 21 as well, i.e.,
(cid:104) q(y ,Θ,u ) (cid:105)
F [q]=E ln t t , (45)
k q(yt,Θ,ut) p(y ,Θ,u |y ,D )
t t ∗ k
then - following the same steps as in Appendix A - the EFE function becomes:
G(u )=E (cid:2) lnp(y |u ,D ) (cid:3) +E (cid:2) −lnp(y |y ) (cid:3) . (46)
t p(yt|ut,Dk) t t k p(yt|ut,Dk) t ∗
In both the mutual information in Eq. 26 and in the entropy of the posterior
predictiveabove,theonlytermthatdependsonu isthevarianceoftheposterior
t
predictive Σ (u ). All other terms drop out due to the translation invariance of
k t
differential entropies. Thus, we find the same solution when using a standard
free energy functional instead of an expected free energy functional [22].
Limitations In Sec. 4.2 we avoided forming the joint posterior predictive distri-
bution over all future outputs in the horizon;
(cid:90) k+H
(cid:89)
p(y |u ,D )= p(Θ|D ) p(y |Θ,u ,u¯ ,y¯) dΘ . (47)
k+1:k+H k+1:k+H k k t t t t
t=k+1
This marginalization is a challenge because blocks of the autoregressive coeffi-
cient start to nest in both the mean and covariance matrix of the joint Gaus-
sian likelihood. To illustrate this, consider an example with M = 1 such that
y
y¯ =y .Thejointdistributionofthelikelihoodsfork+1andk+2isGaussian
t t-1
distributed with mean vector and covariance matrix
(cid:20) A ⊺ u +A ⊺ u¯ +A ⊺ y (cid:21)(cid:20) W-1 W-1A (cid:21)
A ⊺ u +A ⊺ u¯ 1 k+ + 1 A ⊺(cid:0) A 2 ⊺ k u + 3 A k ⊺ u¯ +A ⊺ y (cid:1) A ⊺ W-1 A ⊺ W-1A + 3 W-1 ,
1 k+2 2 k+1 3 1 k+1 2 k 3 k 3 3 3
(48)
Message passing-based inference in an autoregressive active inference agent 11
whereA representrow-indexedblocksofthecoefficientmatrixA.Marginalizing
i
this joint future likelihood over p(Θ|D ) is difficult but a solution would avoid
k
the variational and Laplace approximation errors in Eqs. 31, 34, and 40.
7 Conclusion
We designed an active inference agent with continuous-valued actions as a mes-
sage passing procedure on a factor graph. The forward and backward pass of
predictionsoverfuturesystemoutputsgeneratesasequenceofintermediategoal
priors. Each node in the planning graph only has to solve a 1-step ahead EFE
minimization problem to find appropriate controls. The agent successfully navi-
gates a robot to a goal position under unknown dynamics.
Acknowledgments. TheauthorsgratefullyacknowledgesupportfromtheEindhoven
Artificial Intelligence Systems Institute.
Disclosure of Interests. The authors have no competing interests to declare that
are relevant to the content of this article.
A Appendix
Proof. Usingthefactorisationofthevariationalmodel,theexpectedfreeenergy
functional can be re-arranged to isolate the expectation over q(u ):
t
F [q]=E (cid:104) E (cid:2) ln q(Θ,u t ) (cid:3)(cid:105) (49)
t q(yt|Θ,ut,u¯t,y¯t) q(Θ,ut) p(y ,Θ,u |y ,D )
t t ∗ k
=E (cid:104) ln q(u t ) +E (cid:2) ln q(Θ) (cid:3)(cid:105) (50)
q(ut) p(u ) q(yt|Θ,ut,u¯t,y¯t)q(Θ) p(y ,Θ,u |y ,D )
t t t ∗ k
(cid:16) q(u ) (cid:17)
=E ln t , (51)
q(ut) p(u )exp(−G(u ))
t t
for G(u ) = E (cid:2) lnq(Θ)/p(y ,Θ |u ,y ,D ) (cid:3) and the identity
t
(cid:0)
q(yt|Θ,ut,u¯t,y¯t(cid:1) )q(Θ) t t ∗ k
G(u )=ln 1/exp(−G(u )) .Constrainingq(u )tobeaprobabilitydistribution
t t t
over the space of affordable controls U is done with a Lagrange multiplier:
(cid:16)(cid:90) (cid:17)
L[q,γ]=F [q]+γ q(u )du −1 . (52)
k t t
U
The stationary solution q∗(u ) of the Lagrangian is found at δL[q,γ]/δq = 0
t
[23]). Let δq(u ) = ϵϕ(u ) be a variation with ϕ a continuous and differentiable
t t
test function. Then the variational derivative can be found with:
(cid:90) δL[q,γ] dL[q(u )+ϵϕ(u ),γ](cid:12)
ϕ(u )du = t t (cid:12) (53)
U δq t t dϵ (cid:12) ϵ=0
(cid:90) (cid:16) q(u ) (cid:17)
= ln t +1+γ ϕ(u )du . (54)
p(u )exp(−G(u )) t t
U t t
12 W.M. Kouw et al.
Setting the variational derivative to 0, yields
q(u )
ln t +1+γ =0→q(u )=exp(−γ−1)p(u )exp(−G(u )). (55)
p(u )exp(−G(u )) t t t
t t
(cid:82)
Pluggingthisintotheconstraintgivesexp(−γ−1)=1/ p(u )exp(−G(u ))du .
U t t t
As such, we have:
p(u )exp(−G(u ))
q(u t )= (cid:82) t t . (56)
p(u )exp(−G(u ))du
U t t t
Using (18), (19) and (22), G(u ) can be simplified to a negative mutual infor-
t
mation plus a cross-entropy term:
G(u )=E (cid:2) ln p(Θ|D k )p(y t |u t ,D k ) (cid:3) (57)
t p(yt|Θ,ut,u¯t,y¯t)p(Θ|Dk) p(y |Θ,u ,u¯ ,y¯)p(Θ|D )p(y |y )
t t t t k t ∗
=−E (cid:2) ln p(y t ,Θ|u t ,D k ) (cid:3) +E (cid:2) −lnp(y |y ) (cid:3) . (58)
p(yt,Θ|ut,Dk) p(y |u ,D )p(Θ|D ) p(yt|ut,Dk) t ∗
t t k k
Proof. We split the mutual information into a joint entropy minus the entropy
of the posterior predictive and that of the parameter posterior [13]:
−E (cid:2) ln p(y t ,Θ|u t ,D k ) (cid:3) =E (cid:2) lnp(y |u ,D ) (cid:3)
p(yt,Θ|ut,Dk) p(y |u ,D )p(Θ|D ) p(yt|ut,Dk) t t k
t t k k
+E (cid:2) lnp(Θ|D ) (cid:3) −E (cid:2) lnp(y ,Θ|u ,D ) (cid:3) . (59)
p(Θ|Dk) k p(yt,Θ|ut,Dk) t t k
Since entropies are invariant to translation, only the entropy of the posterior
predictive affects G(u ) [7]. The entropy of a location-scale T-distribution is [6]:
t
E (cid:2) lnp(y |u ,D ) (cid:3)
p(yt|ut,Dk) t t k
=−E (cid:2) −lnT (y |0,Σ (u )) (cid:3) (60)
Tηk (yt|0,Σk(ut)) ηk t k t
=−ln (η
Γ
k
(
π
D
)
y
D 2
)
y B (cid:0)D
2
y, η
2
k(cid:1) − η k +
2
D y(cid:0) ψ( η k +
2
D y)−ψ( η
2
k) (cid:1) − 1
2
ln (cid:12) (cid:12)Σ
k
(u
t
) (cid:12) (cid:12). (61)
2
whereB(·),Γ(·),ψ(·)arethebeta,gammaanddigammafunctions,respectively.
Note that only the last term depends on u . The cross-entropy from posterior
t
predictive to goal distribution is:
E (cid:2) −lnp(y |y ) (cid:3)
p(yt|ut,Dk) t ∗
= 1(cid:16) ln2π|S |+E (cid:2) (y −m ) ⊺ S−1(y −m ) (cid:3)(cid:17) (62)
2 ∗ Tηt (yt|µt(ut),Σt(ut)) t ∗ ∗ t ∗
= 1 ln2π|S |+ 1 Tr (cid:104) S−1(cid:0) Σ (u ) η t +Ξ(u ) (cid:1)(cid:105) , (63)
2 ∗ 2 ∗ t t η −2 t
t
where Ξ(u )=(µ (u )−m )(µ (u )−m )⊺. Note that the first term is constant.
t t t ∗ t t ∗
Message passing-based inference in an autoregressive active inference agent 13
References
1. De Vries, B., Friston, K.J.: A factor graph description of deep temporal active
inference. Frontiers in Computational Neuroscience 11(95) (2017)
2. Friston, K., Kilner, J., Harrison, L.: A free energy principle for the brain. Journal
of Physiology 100(1-3), 70–87 (2006)
3. Friston, K.J., Parr, T., de Vries, B.: The graphical brain: Belief propagation and
active inference. Network Neuroscience 1(4), 381–414 (2017)
4. Hewitt, C.: Viewing control structures as patterns of passing messages. Artificial
Intelligence 8(3), 323–364 (1977)
5. Hoffmann, C., Rostalski, P.: Linear optimal control on factor graphs—a message
passing perspective—. IFAC-PapersOnLine 50(1), 6314–6319 (2017)
6. Kotz,S.,Nadarajah,S.:MultivariateT-distributionsandtheirapplications.Cam-
bridge University Press (2004)
7. Kouw, W.M.: Information-seeking polynomial NARX model-predictive control
through expected free energy minimization. IEEE Control Systems Letters 8, 37–
42 (2023)
8. van de Laar, T., Koudahl, M., van Erp, B., de Vries, B.: Active inference and
epistemic value in graphical models. Frontiers in Robotics and AI 9 (2022)
9. van de Laar, T., Koudahl, M., de Vries, B.: Realizing synthetic active inference
agents, part II: Variational message updates. Neural Computation 37(1), 38–75
(2024)
10. van de Laar, T.W., de Vries, B.: Simulating active inference processes by message
passing. Frontiers in Robotics and AI 6(20) (2019)
11. Loeliger,H.A.:Anintroductiontofactorgraphs.IEEESignalProcessingMagazine
21(1), 28–41 (2004)
12. Loeliger,H.A.,Dauwels,J.,Hu,J.,Korl,S.,Ping,L.,Kschischang,F.R.:Thefactor
graphapproachtomodel-basedsignalprocessing.ProceedingsoftheIEEE95(6),
1295–1322 (2007)
13. MacKay, D.J.: Information theory, inference and learning algorithms. Cambridge
University Press (2003)
14. Nisslbeck, T.N., Kouw, W.M.: Factor graph-based online Bayesian identification
andcomponentevaluationformultivariateautoregressiveexogenousinputmodels.
Entropy 27(7) (2025)
15. Nisslbeck, T.N., Kouw, W.M.: Online Bayesian system identification in multivari-
ate autoregressive models via message passing. In: IEEE European Control Con-
ference (2025)
16. Palmieri, F.A., Pattipati, K.R., Di Gennaro, G., Fioretti, G., Verolla, F., Buo-
nanno, A.: A unified view of algorithms for path planning using probabilistic in-
ference on factor graphs. arXiv:2106.10442 (2021)
17. Parr,T.,Markovic,D.,Kiebel,S.J.,Friston,K.J.:Neuronalmessagepassingusing
mean-field,Bethe,andmarginalapproximations.ScientificReports9(1889)(2019)
18. Parr, T., Pezzulo, G., Friston, K.J.: Active inference: the free energy principle in
mind, brain, and behavior. MIT Press (2022)
19. Särkkä, S.: Bayesian filtering and smoothing. Cambridge University Press (2013)
20. Şenöz, İ., van de Laar, T., Bagaev, D., de Vries, B.: Variational message passing
and local constraint manipulation in factor graphs. Entropy 23(7) (2021)
21. Soch,J.,Faulkenberry,T.J.,Petrykowski,K.,Allefeld,C.:TheBookofStatistical
Proofs (2021). https://doi.org/https://doi.org/10.5281/zenodo.4305950
14 W.M. Kouw et al.
22. de Vries, B., Nuijten, W., van de Laar, T., Kouw, W., Adamiat, S., Nisslbeck,
T., Lukashchuk, M., Nguyen, H.M.H., Araya, M.H., Tresor, R., Jenneskens, T.,
Nikoloska, I., Ganapathy Subramanian, R., van Erp, B., Bagaev, D., Podusenko,
A.:Expectedfreeenergy-basedplanningasvariationalinference.arXiv:2504.14898
(2025)
23. Wainwright, M.J., Jordan, M.I., et al.: Graphical models, exponential families,
andvariationalinference.FoundationsandTrends®inMachineLearning1(1–2),
1–305 (2008)

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Message passing-based inference in an autoregressive active inference agent"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
