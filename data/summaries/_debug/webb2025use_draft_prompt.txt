=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: The Use of Gaze-Derived Confidence of Inferred Operator Intent in Adjusting Safety-Conscious Haptic Assistance
Citation Key: webb2025use
Authors: Jeremy D. Webb, Michael Bowman, Songpo Li

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Key Terms: feedback, adjusting, however, confidence, tasks, robot, inferred, haptic, conscious, operator

=== FULL PAPER TEXT ===

The Use of Gaze-Derived Confidence of Inferred
Operator Intent in Adjusting Safety-Conscious
Haptic Assistance
Jeremy D. Webb, Michael Bowman, Songpo Li, and Xiaoli Zhang∗, Member, IEEE
Abstract—Humans directly completing tasks in dangerous or mentally accomplish the task. The correspondence issue is
hazardous conditions is not always possible where these tasks inherent in the system as the operator lacks sufficient sensory
are increasingly be performed remotely by teleoperated robots.
feedback. Specifically, in a typical setup, an operator views a
However, teleoperation is difficult since the operator feels a
screenandcontrolsarobotwithajoystick;however,feedback
disconnect with the robot caused by missing feedback from
severalsenses,includingtouch,andthelackofdepthinthevideo from touch and sound as well as depth into the screen are
feedbackpresentedtotheoperator.Toovercomethisproblem,the all missing. Not being able to distinguish the Z-order of
proposedsystemactivelyinferstheoperator’sintentandprovides objects can create erroneous complications. For example, in
assistance based on the predicted intent. Furthermore, a novel
surgery, unexpected tissue damage, longer operating times,
method of calculating confidence in the inferred intent modifies
and increased stress for the surgeon can all result from a lack
the human-in-the-loop control. The operator’s gaze is employed
to intuitively indicate the target before the manipulation with of depth information. Likewise, for a bomb disposal robot,
the robot begins. A potential field method is used to provide a errorscausedbytheoperator’sunclearunderstandingindepth
guidingforcetowardstheintendedtarget,andasafetyboundary could result in a bomb denotating prematurely. It also has
reducesriskofdamage.Modifyingtheseassistancesbasedonthe
beenshownthatdepthperceptionisparamountinsuccessfully
confidence level in the operator’s intent makes the control more
performing grasping tasks for human’s using their own hands,
natural, and gives the robot an intuitive understanding of its
human master. Initial validation results show the ability of the where performance degrades as the depth perception becomes
systemtoimproveaccuracy,executiontime,andreduceoperator inaccurate [5]. Furthermore, trying to determine the depth of
error. an object can distract a teleoperator. Since the operator is not
using their own arm to complete the task, they will not have
a good intuitive understanding of the dynamic behavior of the
I. INTRODUCTION
robot being controlled. These issues can cause mistakes that
A. Context and Motivation
lead to unintentionally harming the surrounding environment
Remotely operated robotic procedures performed has con- which has the potential to be far more costly than failing to
tinuedtoincreaseeachyear.Theseproceduresincludescenar- complete the teleoperation task.
ioswhereahumanmayfinditdifficulttoachieveatask,such Existing interfaces for teleoperated robots attempt to solve
as telesurgery, and environments where it is dangerous for a theseissuesinvariousways,butmanyaredifficulttooperate.
humantobepresentsuchasinenvironmentscontaminatedby One interface uses fixed targets as reference points and an
chemical,radioactive,orexplosivehazards.Theenvironmental oscillatingcameratowardsandawayfromthesepointstogive
challengeswheretheseroboticsystemsareusedincludebomb an operator a better sense of the environment [6].The largest
disposal/mine clearing robots [1], robots for making repairs problem with this approach is the difficulty for the operator
in space, robots for handling nuclear material [2], hazardous to accomplish their task with a constantly moving camera.
waste handling robots [3]. Another suggested method to help the operator understand
Useoftheserobotsgreatlyimprovesthesafetyandcomfort depth in a teleoperation scenario is to reconstruct a virtual
of the humans performing the tasks, yet also adds complexity 3D environment by using stereoscopic video [7], [8]. Other
and difficulty in achieving the goal. The reason for the solutions to aid the operator determine depth is to change
difficulty is mainly due to the “disembodiment” problem [4]. the lighting conditions of the environment and provide visual
Where this problem describes the fact that the operator is not cues [9],] and provide a target object’s pose using machine
physically performing the tasks in the environment yet must vision [10]. It should be noted this last method will fail
whenever new objects are encountered. Another approach
Jeremy D. Webb is an engineer with Apple Inc.(e-mail: jew-
is to immerse the operator in the environment by using a
ebb@mines.edu.)
Michael Bowman is a Postdoctoral Fellow in Perelman School of head-mounted display, which allows the user to look around
MedicineatUniversityofPennsylvania,Philadelphia,PA19096USA(e-mail: naturally [11], but this requires more complex equipment.
michael.bowman@pennmedicine.upenn.edu).
There is therefore a need for an intuitive control interface
SongpoLiisanAssociateScientistwithHondaResearchInstituteUS(e-
mail:songpo li@honda-ri.com). that can restore some of the sensory feedback lost during
Xiaoli Zhang is an Associate Professor in the Department of Mechan- teleoperation and increase the accuracy of task completion.
ical Engineering at Colorado School of Mines, Golden, CO 80401 USA
Such a system should take into account the operator’s intent
(∗corresponding author, phone: 303-384-2343; fax: 303-273-3602; email:
xlzhang@mines.edu). to provide accurate assistance for real-time control applica-
5202
rpA
4
]OR.sc[
1v89030.4052:viXra
tions, and cooperate in a way that is comfortable for the beusedtoteachamorestraightforwardpathtothegoal.Other
operator [12]. In this case, the operator’s gaze can serve to studies have shown the use of potential fields in haptics to
indicate their intent, or the final goal of a manipulation, and guide the operator by pushing their hand away from objects
this information can be used to guide the operator’s hand and/or towards the goal (termed guidance virtual fixtures)
to the target. This can be accomplished through the use [26]–[28]. One such project developed a potential field to
of haptic forces, which function as a partial restoration of control unmanned aerial vehicles [29].
sensoryfeedback.Furthermore,theprovidedassistanceshould Often,providingforcefeedbackintermsofavirtualfixture
beadjustedbasedonthesystem’sconfidenceintheoperator’s only solves one issue with teleoperation, accurately reaching
intent [13]. Since the system cannot be 100% sure of the the target. For many teleoperation tasks, such as minimally
predicted operator’s intent, the confidence level in the intent invasive surgery (MIS), some damage to the surrounding
shouldbeusedtomoderatethestrengthoftheprovidedhaptic environmentisunavoidable[30].However,thisdamagecanbe
forces. This will ensure the system is robust and provides minimized by ensuring the operator only moves the robot in
accurate assistance. allowable regions. This is enforceable using haptic forbidden-
The proposed system takes advantage of the natural visuo- region virtual fixtures. A number of forbidden-region virtual
motor behavior of human beings. Several human visuomotor fixtureshavebeendemonstratedinvariousresearch,especially
and cognitive behavior studies indicate that one’s gaze leads those concerned with MIS. One shows that forbidden-region
their hands during execution of a grasping or reaching task. virtual fixtures that move with a portion of the environment,
Specifically, when a human decides to pick up an object, such as a beating heart, can increase user precision [31]. A
he first looks at the object, then focuses on the part of methodwhichassiststheuserperformingMISbyplacingcon-
the object to be grasped, and finally executes the reaching ical forbidden-region virtual fixtures at a set of predetermined
movement. Typically, the gaze fixates on an object before locations has also been developed [28]. Others have built
interacting with it and stays fixed on the object until the task methods to automatically generate forbidden-region virtual
is completed [14], [15]. The average lead time for a grasping fixtures based on the output of RGB-D cameras to protect
task has been found to be 3 seconds [16]. A human’s eye sensitive areas [32], [33].
gaze has been shown to focus on certain parts of an object Thedrawbacktotheapproachesdescribedaboveisthatthey
depending on the current task [17], however, initially the rely mainly on situational context alone to determine how to
gaze is focused on the object’s center [18]. Furthermore, in implement the virtual fixtures. Instead, as noted earlier, the
a comprehensive review [19] describes how the human brain systemshouldincorporatetheoperator’sintentandconfidence
maintains a model of the “eye-head-shoulder system” and in the prediction of that intent into the control loop to provide
treats gaze as a feedforward mechanism when reaching for intuitive and accurate assistance. One way to do this is to use
an object. Even on 2D displays, eye-movements indicate the the operator’s gaze.
user’s intention and thus, can be used for “highly-intuitive” Gaze as a control input has been used in a variety of assis-
computers [20]. ]. These anticipatory fixations also happen in tive mechanisms to help the teleoperator visualize the robot
teleoperation[21].Therefore,incorporatingtheoperator’sgaze workspace [12], [34]–[36], and to direct robot navigation. In
into the teleoperationcontrol interface to specify thecenter of particular, researchers have demonstrated successful use of
the haptic assistance is a natural extension of normal human gaze gestures to control teleoperated drones [37], and gaze
behavior. There is a need for such a system, as described by contingent regions, or “hot-spots”, have been used to specify
[22] which speaks of the need for human perception models a robot’s direction of movement [38], [39]. These approaches
in haptic teleoperation to improve human-in-the-loop control. donotusetheoperator’sgazeasanindicationofintent,instead
theusermustconsciouslyfocusonaparticularareatoprovide
input for the signal. This can cause fatigue for the operator
B. Previous Work
and distract them from completing the goal.
Incorporating haptic feedback into a teleoperation system Using the operator’s gaze to infer their intent can provide
can restore some of the sensory feedback that is lacking. a more natural control scheme. This is demonstrated in “pre-
Haptic feedback, in this case, refers to applying forces to dicting a driver’s intent to change lanes”, which used head
the operator that are dependent on the system’s state. Us- motion and eye data to train a discriminative classifier to
ing haptic feedback in teleoperation has been explored in a perform the prediction [40]. As explained, use of the user’s
variety of studies, especially in remote surgery applications. intent has been shown in other applications, but using intent
Researchers have shown that using a “computerized force prediction in haptics is a new area that has not been explored.
feedback endoscopic surgical grasper” in minimally invasive Additionally, the shortcoming in the previous approaches are
surgery leads to significant performance gains over using a that they provide only a binary output. For example, the user
regular endoscopic grasper [23]. Similarly, using force feed- intends to change lanes, or does not intend to. In this case, an
back in blunt dissection reduces tissue damage and the force important aspect of the system’s ability to make decisions has
used in robotic surgery [24]. Haptic feedback has been shown beenleftout:theprobabilitythatthepredictedintentiscorrect.
toimproveteleoperationcontrolingeneral,aswell.Onestudy This component is necessary to ensure robust control and
investigated using haptic feedback for training one’s hands to decision making. For example, undesired behavior could be
follow a certain trajectory, demonstrating that haptic feedback encounteredifthesystemattemptstoassistwithlanechanging
does improve the training [25]. This indicates that haptics can when the intent prediction is hovering between intent and no
intent. Instead the system should provide assistance based on
its confidence in the predicted intent. Another reason for this
isbecausetheuser’sgazeisreallyanobservationmechanism,
notacontrolinput(knownastheMidasTouchproblem[41]).
Therefore, to reduce inference error the system should take
intoaccountthelikelihoodthattheuseractuallyhasanintent.
C. Research Contributions
The proposed system aims to reduce risk and enhance per-
formance in realtime teleoperation through three approaches:
gentlyguidetheoperator’shandtowardthegoalpoint;prevent
unwanteddestructionofthesurroundingteleoperationenviron-
ment;andensurecontrolisnaturalandintuitivebymodifying
Fig. 1: System overview
theprevioustwoapproachesbasedonthesystem’sconfidence
intheoperator’sinferredintent.Theintentinferencerestoresa
teleoperator’s eye-hand coordination through incorporation of
the spatial uncertainty in the predicted target location to be
theoperator’snaturalvisuomotorbehaviorbyusingtheirgaze
taken into account. Errors in gaze tracking, target location
to determine the reaching target before the process begins.
determination, and robot end-effector location all contribute
A force then gently pushes the operator’s hand towards the
to the spatial uncertainty.
target.Simultaneously,asafetyboundarypreventsharmtothe
The proposed system will increase precision, safety, and
environment by restricting joystick movement to a small area
ease the use of teleoperation, thus improve task performance
around the target point. The size and strength of both of these
by reducing the time it takes to complete a task and increase
virtual fixtures is adjusted based on the system’s confidence
the comfort of remote operators.
in the inferred intent and the specific task. As discussed
in [42], including awareness of the environment and task in a
teleoperation control scheme can give great improvements in II. SYSTEMOVERVIEW
performance. Additionally, the biggest challenge when using
The overall system predicts the goal position from the
virtual fixtures is determining the appropriate strength of
operator’s gaze, determines the confidence in the prediction,
the fixture [42]. As noted, this system deals with this issue
guidestheoperator’shandtothetargetusingaforcebasedon
by dynamically assigning the strength based on the current
the output of the potential hybrid control, and places a safety
situationandtheprobabilitythatthepredictedoperator’sintent
boundary (forbidden-region virtual fixture) around the goal
is correct.
point. In this system, shown in in Fig. 1, the video feedback
The contributions of this system include:
showstheoperator’sgazelocation,whichisthegoalposition,
1) development of haptic virtual fixtures which are based while a 6 DOF joystick gives the operator the manual control
on the operator’s inferred intent to ensure control is input for orientation and position. The guidance force and the
instinctive and improves performance safety boundary is provided by the joystick, which is also a
2) a novel gaze-driven method for determining the level of haptic device. Most of the time, the operator should not feel
confidence in the predicted human intent the boundary at all. Their hand will only come into contact
3) real time adjustment of the haptic virtual fixtures based with it if they attempt to command the robot to a position too
on the operator’s predicted intent and confidence in that far from the target.
prediction,whichreducesriskandincreasessuccessrate Theoverallcontrolflowwithamoredetailedviewisshown
in teleoperated tasks in Fig. 2. The operator’s fixation location is determined by
4) evaluationoftheeffectivenessoftheintent-drivenhaptic acquiring data from the eye tracker and filtering it as further
assistance with confidence adjustment discussed in section III. The confidence in the operator’s
The operator’s gaze is used to indicate the final goal of the intent is also calculated using the gaze data. By combining
joystickmotionbecause,forexecutinggraspingtasks,thehand the gaze information from the robot environment, a fully
oftheoperatorfollowstheirgaze.Additionally,theconfidence specifiedspatialpositionofthetargetisdetermined.Thesafety
in the intent is computed using features inherent in the boundaryisplacedwithitscenteratthetargetpositionandits
operator’s gaze. Using this approach, the system can predict parameters are adjusted based on the confidence level of the
the operator’s intention and determine its own confidence in intent. Simultaneously, the target position is blended with the
thepredictionwithouttheneedforextraeffortbytheoperator. position of the joystick using the novel potential field method
Adjusting the strength of guidance and safety boundary based todeterminetheforcetoapplytotheoperator’shand.Thepose
on the system’s confidence in the inferred intent allows the of the joystick is then fed into the controller for the robot.
user to teleoperate the robot as normal. The shape of the As shown in Fig. 2, this system is considered a closed
safety boundary is chosen to minimize the risk associated loop through the user viewing video feedback from the robot
withcompletingagiventask.Theguidanceforceiscomputed and adjusting the joystick position or orientation accordingly.
using a potential field method. Using a potential field allows Additionally, the guidance force pushes the operator’s hand
Intent Confidence Intent Confidence
Calculation
Intent Safety Boundary
Filtering
Gaze Location Boundary Adjustment
Data
Environment
Guidance Force
Data
Force Adjustment
Commanded
Position Robot Position
Operator Robot
Visual Feedback
Haptic Forces
Fig. 2: Overall control flow which illustrates the role of the
operator’s gaze in the hybrid joystick-gaze control method.
Fig. 3: Example of the gaze features used to determine the
The blue lines represent the control flow for traditional tele-
intentconfidence.XandYarethescreencoordinatesinpixels.
operation.
Training data for the classifier was gathered by recording
towards the target. The operator’s gaze assists with this be-
alltheeyedatafromtheeyetrackerwhiledifferentvolunteers
cause it incorporates the operator’s intention into the control
lookedatascreenfilledwithcolorednumbers.Eachvolunteer
by indicating the target position. In summary, the operator’s
indicatedtheirintentbyclickingthespacebarwhilelookingat
gaze location indicates their intended target and the system
a number of their choosing. This caused the gazed-at-number
assists them in reaching this target by actively guiding their
to move to the center of the screen and labeled two seconds
hand towards this position.
worthoftheprecedingvaliddatapointswiththeclass“intent”.
At anytime other time, gathered data was labeled with “no
III. MOTIONINTENTEXTRACTIONFROMEYEMOVEMENT
intent”. Valid data includes all data where both eyes are fully
Humans eyes naturally make involuntary movements and tracked by the gaze tracker.
motions such as blinking, rolling, and microsaccades. There- After the model has been trained, during actual intent
fore, it is necessary for a method to filter the raw gaze data prediction, each segment of data is taken from the last two
to determine the operator’s fixation location. The filter which seconds of valid collected gaze data and the gaze features are
was chosen to combat this was an adaptive-length sliding calculated. The classifier is then run on the input data.
window [12]. Thepredictionoutputfromtheclassifierincludestheposte-
Whengazeisusedtocontrolasystem,itbecomesnecessary rior probabilities of belonging to the classes “intent” and “no
to determine a way to distinguish an intentional command intent”. These correspond to the operator beginning an action,
from an unintentional one. Because the gaze is always “ac- orjustobservingthesituation.Sincethereareonlytwoclasses,
tive”, this complicates the problem which is referred to as aposteriorprobabilityofover50%for“intent”indicatesthatit
the Midas touch problem [41]. One approach to overcoming ismostlikelythattheoperatorhasavalidintention.However,
this distinction is to use the dwell time method. This method avalueofjustover50%indicatesthattheintentisjustbarely
considers a command to be confirmed when the gaze stays likely. Therefore, the posterior probability for intent will be
on a location for a set amount of time. Alternatively, an linearly rescaled to a range from 0.5 to 1.0:
option could be to require a certain number of blinks to
(cid:40)
confirm a command. In this system, a method of determining 0 p <0.5
ci= i (1)
the likelihood that a predicted intention is correct has been 1 (p −0.5) p ≥0.5
developed. The following section describes the approach. 0.5 i i
p =p(i|G ,G ,G )
The confidence in the predicted intent derived from the i 1 2 3
where ci is the confidence in the predicted intent and
operator’sgazeisdeterminedbyusinganaiveBayesclassifier
p(i|G ,G ,G ) is the probability of an intent given the three
fitted to three processed gaze features. These features are: 1 2 3
gazefeaturesdescribed.Additionally,iftheoperator’seyesare
the maximum euclidean distance of the gaze points to the
not tracked for over 0.75 seconds, the intent confidence is set
gaze center, the average distance to the gaze center, and the
to zero.
number of gaze points that are closer to the center than the
average distance to the center. In this case, the gaze center
refers to the average point taken for all gaze points over the
IV. INTENT-BASEDHAPTICASSISTANCE
data segment considered. These features are shown in Fig. 3. Twodifferenthapticvirtualfixturesareemployedtoprovide
Before computing each gaze feature, the data was smoothed assistance to the operator. Both are centered at the gaze-
by running it through a five point moving average filter. derivedintentlocation.Aguidanceforcepushestheoperator’s
Severaldifferentfeaturesfortheclassifierwereinvestigated. hand towards the target position with its strength based on
Although the ones selected are not independent, they provide distance from the target. Scaling the force this way ensures
a good estimate of the reliability of the predicted intent based the system respects the operator’s control. Even though the
on how focused the gaze is. predictedintentlocationmaybecorrect,theoperatormayneed
to navigate around obstacles so the guidance force should be Where⃗c∈R3 isthefinalcombinedlocation.W istheweight
relatively weak until the operator begins to move towards the calculated from the potential field, and d⃗ is a 3x1 vector of
target. Similarly, the safety boundary prevents destruction of coordinate weights,
 
the environment by preventing movement outside of a region d
x
close to the target. d⃗= d y 
d
z
A. Haptic Guidance Force From Eq. 2, W is the amount of influence that the target
The purpose of the guidance force is to gently push the op- point p⃗ g has on the resultant ⃗c. It is calculated from the
erator’s hand towards the gaze-indicated target position. This potential field and bounded by [0,1] where a higher value of
will help overcome the lack of sensory feedback, especially W approaching 1 means the resultant⃗c will correspond to the
in the depth direction. To this end, the profile of the guidance targetlocation,whilealowervalueofW willmeanitfollows
force is based on a method described in our previous work,
thehandposition.d⃗isaweightedvectorthatcontrolshowthe
termed potential hybrid control [43]. potential field affects the final combination in each direction.
The potential hybrid controller takes two position inputs, Forexample,avalueofd x =0.9wouldgivethepotentialfield
a target (the gaze target, p⃗ ∈ R3) and a manually-defined a 90% influence on the final combination for the x coordinate
g
control position( joystick control location, p⃗ ∈ R3). It then only.
j
combines them in a way that takes into account the target 2) Potential Field: A potential field describes how a body
uncertainty and the behavior of the operator. In particular, if interacts with an entity that exerts an influence on that body.
the manually-defined control position is far, or very close, to For example, in physics, there is a potential field description
the target position, then the potential hybrid controller does for the gravitational pull exerted by a planet. This potential
not affect the output very much. In the case of the manually- fieldgivesarepresentationofwhatforcesanotherbodywould
definedcontrolpositionbeingfarfromthetarget,theoperator feel(duetotheplanet)whenplacedatanygivenlocationinthe
should have complete control over the robot position. On the field. Analogously, the potential field in this method describes
other hand, when the manually-defined control position is the effect the p⃗ g has on the final position of the robot,⃗c.
closetothetarget,theoperatordoesnotneedmuchassistance The peak of the potential field is the gaze-indicated target
becausethetargetpositionhasalreadybeenreached.Itisonly position, this take advantage of the operator’s visuomotor
in the travel between these two extremes that assistance is behavior. By placing the potential field centered there, the
needed. The potential hybrid controller follows this approach. robot’s end-effector is drawn towards the intended target. The
potential field provides a smooth combination or transition
from the joystick position and the target position, which has
itsmaximumeffectattheintendedlocation,however,thisdoes
not impact the robot position very much when the joystick is
farfromthetarget.Thisattributeofthepotentialfieldensures
the control follows the operator’s intent.As shown in Eq. 3, a
Gaussian curve was used for the potential field. While other
potential fields could be used including parabolic, cubic, etc.
[28], a Gaussian curve was selected because it is a smooth,
continuous function and the shape is easy to manipulate by
adjusting its parameters. The shape determines how quickly
the influence of the field increases in each direction.
(cid:18) (cid:19)
1
W =exp − (p⃗ −p⃗ )T Σ−1(p⃗ −p⃗ ) (3)
2 j g j g
 σ2 0 0 
x
Fig. 4: An illustration of the potential hybrid control method Σ−1 =0 σ
y
2 0 (4)
for a two-dimensional control space. p is the goal point, p 0 0 σ2
g j z
isthejoystickpoint,cisthecombinedpoint,andσ isthesize In the above equation p⃗ represents the center of the field
g
of the field in each direction. andΣrepresentsthecovariancematrix.Thecovariancematrix
controls the tightness of the field. The smaller the variance in
1) PotentialHybridControllerMethod: Thetargetlocation, each directional component the tighter the field becomes. In
derived from the operator’s gaze, is combined with motion this case, since the off-diagonal elements are zero so only
commands from the joystick through a potentially weighted the variance are left in the covariance matrix. When this
influence method shown in Fig. 4. This approach uses the occurs, each coordinate direction in the field is independently
distance from v⃗ to p⃗ along a potential field to determine the controlled from one another.
j g
influenceofaresultingpoint,⃗c.Themethodisrepresentedby There are a few reasons for using the method to combine
the following equations: thecontrolinputs.Oneisthesimplicityandeaseofcombining
⃗c=(p⃗)(1⃗.0−Wd⃗)+(p⃗ )Wd⃗ (2) multiple inputs. Additionally, the method is intuitive from a
j g
physical sense. The potential field represents the probability
the target location determined from the gaze is correct. As
the operator joystick command motion approaches the target
position, the system becomes more confident of its own
guess at the target location and continues to increase its own
influenceovertherobotend-effector.If,ontheotherhand,the
robot end-effector is far away from the gaze-selected target
then the system has a lower confidence in the target location
so it affects the final end-effector location less. This also
modelsthewayhumansnaturallybehave.Ifoneislookingata
particularlocationinspace,theydonotwanttheirhandmoved
there automatically, but if they are focusing on something
intently (for example when threading a needle) their hand
moves to wherever they are looking.
3) Potential Hybrid Control Approach Guidance Force:
Fig. 5: Magnitude of the guidance force in two dimensions
After determining the location of ⃗c in the previous section,
for a combination with equal strengths in both X and Y. The
theguidanceforceforthehapticfeedbackmustbedetermined.
target position is placed at (0.5, 0.5). The profile of the force
By basing the strength of the guidance force on the potential
is the same for all directions of approach.
hybrid control, all the advantages of the method noted above,
are gained. The strength of this force is proportional to the
degree of influence given by the potential hybrid control for
a given direction. It is calculated using:
⃗c−p⃗
g⃗f = j (5)
gf
max
where g⃗f ∈ R3 is the normalized, dimensionless strength of
the guidance force in Cartesian space,⃗c is found using (2), p⃗
j
isthenormalizedjoystickposition,andgf isthemaximum
max
value of ⃗c − p⃗. gf is pre-calculated using numerical
j max
methods.gf isthenscaledtoanappropriatelevelforthehaptic
device to give the actual guidance force. The scaling ensures
the joystick will not pull itself out of a user’s hand. A size of
σ = 0.4 was used for all coordinate directions, as suggested
Fig. 6: Magnitude of the guidance force in one dimension.
in our previous work. In the simplest case, the direction of
The target position is placed at Y =0.5.
the force points directly towards the target. However, just like
theamountofinfluenceforeachdirectioncouldbecontrolled
withthepotentialhybridcontrolapproachdescribedinsection At other points, at values of X increasingly farther from the
IV-A1, the strength of the force in each direction can be target, the peak strength of the guidance force decreases. This
controlled independently. In most cases, the strength in the allows the operator to control the robot with less opposition
depthdirectionshouldbemuchlargerthantheotherdirections if the target position is incorrect, or the approach path needs
since that is the direction lacking feedback for the operator. to deviate from its current trajectory, for example due to an
The effect of the guidance force is to provide a gentle push obstacle. Along the line Y = 0.5 the force is completely
in the direction of the target when the joystick is moving
zero. This is because the target position in the depth direction
towards the target. Similar to the potential hybrid control
has already been reached, so minimal additional assistance is
influence, if the joystick is far from the target, or close to
necessary.
the target, then the force is small, but otherwise the force is
larger. An illustration of the magnitude of the force (strength
B. Safety Boundary
in each direction combined together) is presented in Fig. 5 in
two dimensions for the case where the strengths, d and d , An alternative to the guidance force, a haptic safety
X Y
ineachdirectionareequal.Theforceprofileisthesamefrom boundary is considered. The purpose of the safety boundary
every direction of approach. This is also illustrated in Fig. 6, (forbidden-region virtual fixture) is to minimize collateral
whichshowstheforcemagnitudeforasingledirection,orcan damagebyrestrictingmovementofthejoysticktoasmallarea
be thought of as a section view of Fig. 5. surrounding p⃗ Various different shapes for this boundary are
g
Figure 7 shows the normalized strength when the force is examinedinthefollowingsections.Theshapeoftheboundary
onlyappliedinthedepthdirection,Y.Movingalongthedepth is very important because the boundary has the potential to
direction at the point where X is equal to the target position, maketeleoperationsafer,butitcouldalsopreventtheoperator
X =0.5, gives the same profile as the magnitude plot in Fig. from controlling the joystick effectively if the shape is not
5. This is helpful to the operator, because the depth direction chosen carefully, which would lead to more mistakes and
is the only direction in this situation that requires assistance. environmental damage.
damage, but a high likelihood of success.
The set of parameters for each task should be chosen to
minimize the overall risk associated with the task, including
the risk of damage and the risk of failure. There is no clear
set of parameters to achieve this, and the selection is task
dependent. Therefore, it is necessary to investigate how the
variation of parameters affects the risks. This was done by
measuring the failure rate of each task while using different
boundaryparameters.Failurerateconsideredbothfailuresdue
to not completing the task, and failures due to damaging
the surroundings. The set of parameters with the minimum
failure rate was selected as the initial set for each task. Two
Fig. 7: Strength of the guidance force in two dimensions
differenttasksweremeasured,graspingandcutting.Theseare
for a combination where only the depth is applied. Y is the
explained in more detail in section VI-B.
depth direction in this case and the only direction with force
Eight different combinations of the parameters were tested.
assistance. The target position is placed at (0.5, 0.5).
The combinations tested were selected by dividing the range
of each parameter into thirds and using the cutoffs between
each segment. This was done to test each parameter at a high
1) Boundary Design: The shape of the safety boundary,
and low point, and also to leave room for adjustment later.
shown as a section-view in Fig. 8, was chosen to provide
The specific combinations of parameters measured for each
minimum intrusion to the operator’s standard operating man-
task are shown in Table I. A mouse click was used to set the
ner while still restricting access to areas unnecessary for
center of the boundary on the target position for each task.
completing the task. The full shape can be created by rotating
the profile in Fig. 8 by pi radians about its center axis. TABLE I: SAFETYBOUNDARYPARAMETERSETS
The upper plane prevents unintended damage during general
Set θ [deg] H[cm] S[cm]
motion, while the cone allows the robot room to move in and
1 30 5 3
complete the task at p⃗ .
g
2 30 5 5
3 30 10 3
4 30 10 5
5 60 5 3
H
6 60 5 5
θ
7 60 10 3
8 60 10 5
S
Fig. 8: Safety boundary shape with parameters. Eightvolunteersweretested,witheachoneperformingeach
task under every set described in Table I for two to three
There are three parameters that govern the shape of the trials. All the volunteers were aged 18-28 and were able-
boundary: S, H, and θ. S is the radius of the flat bottom, H bodied. Three of the volunteers had prior experience using
is the height limit of the cone, and θ is the angle of the cone. the system, but all were given as much time as they needed
Ingeneral,asmallS,andalargeH andθwillcreatethemost to familiarize themselves with how it worked. This was done
restrictive boundary. For the purposes of this system, S was by practicing picking up a tennis ball with no boundary until
limitedfrom1centimeterto7centimeters,H waslimitedfrom they felt comfortable with the system. The order of the sets
0 to 15 centimeters, and θ from 5◦ to 85◦. These limits were was randomized for every participant to prevent acclimation
chosen based on the physical limitations of the haptic device to the system, which would affect the results. The failure rate
and on empirical experience of the necessary room needed to was then averaged for each trial and over all the participants
maneuver to complete a task. to determine the overall failure rate for every parameter set.
2) Parameter Selection: Selecting the correct set of pa- The results of the tests are shown in Table II. As expected,
rameters for the safety boundary is important because of the the average failure rate for the cutting task, 59%, was higher
competingaimsofadjustment.Tighteningthesafetyboundary, than for the grasping task, 54%. This is simply because the
whichcorrespondstoincreasingH,decreasingS,andincreas- cutting task is more difficult and requires higher precision
ing θ can reduce the risk of collateral damage by preventing while manipulating the robot arm. For the cutting task, there
access to areas of the workspace farther from p⃗ . However, are multiple sets which have equal failure rates; sets 2, 6, and
g
whilethismayreducetheriskofdamage,itincreasestherisk 7 have the lowest failure rate of 50%. These sets correspond
offailure.Withsuchatightboundary,theremaynotbeenough totheleastrestrictiveboundary(set2)andthemostrestrictive
room to maneuver. At the extreme, if θ = 90◦, S = 0, and boundary(set7),aswellasalessrestrictiveboundary(set6).
H > 0 then the workspace is not accessible at all. Opening Thiscouldbeexplainedbythedifficultyofthetask.Sincethe
up the boundary, decreasing H, increasing S, and decreasing task was challenging, it was easiest to accomplish with low
θ can lead to the opposite problem: high risk of collateral restrictions, which would give the most room to maneuver, or
withhighrestrictionswhichwouldprovidethemostprotection is more confident in the intent than not. For this method,
from collateral damage. Further testing would likely help to ithresh was set to 60%. Once the scaled confidence, sci, is
differentiate between these sets. Any one of these sets can be computed, each parameter is adjusted from its initial value
chosentominimizetherisk.Sincethecuttingtaskisdifficult, based on the paramMaxAdjustAmount. The value of this
set 2 was chosen to proceed with because it will give the parameter for S, H, and θ was chosen as a third of the range
operator more control. For the grasping task, set 5 had the foreachparameter,asdescribedinsectionIV-B.Inparticular,
minimum failure rate. SMaxAdjustAmount=−2 cm, HMaxAdjustAmount=
5cm,andθMaxAdjustAmount=25◦.Thiswillalloweach
TABLEII:SAFETYBOUNDARYPARAMETERTESTRESULTS
parameter to be scaled to the maximum or minimum of its
SHOWING THE FAILURE RATE OF EACH TASK. THE HIGH-
range depending on the intent confidence. For example, S
LIGHTED CELL(S) IN EACH COLUMN INDICATE THE PA-
will scale down to its lowest value of 3 centimeters when
RAMETERSET(S)WITHTHEMINIMUMFAILURERATE
the intent confidence is 100%. This process enables the safety
Set CuttingTask GraspingTask boundarytodynamicallyadjusttotheconfidencelevelinorder
1 64% 65% to decrease the risk of damaging the environment.
2 50% 42% Additionally, the guidance force strength is also adjusted
3 64% 58% based on the confidence level. In this case, it is applied as a
4 64% 73% simple linear scaling, meaning that if the scaled confidence
5 71% 38% level is 0 then no guidance force is applied to the operator,
6 50% 65% and if the scaled confidence level is 1, then the full strength
7 50% 48% of the guidance force is exerted on the operator.
8 57% 44%
VI. EXPERIMENTALVALIDATION
In order to evaluate the effectiveness of the proposed
V. ADJUSTMENTBASEDONINTENTCONFIDENCE
approach,eachcomponentwastestedwiththesetupdescribed
Both the guidance force and the safety boundary are ad- inthefollowingsection.Boththeguidanceforceandthesafety
justed from their initial settings based on the level of confi- boundary were tested separately to gain an understanding of
denceintheoperator’sintent.Specifically,thesafetyboundary how each affected the teleoperation performance.
will become less restrictive and the guidance force weaker
when the intent confidence is low while the opposite will
A. Experiment Setup
occur when the intent confidence is high. The reason for this
isthattheoperatorshouldnotberestrictedwhenthesystemis For validation, a system was built following Fig. 1. The
not confident in its prediction of the intent. This would cause joystick used is a Geomagic Touch created by 3D Systems.
frustration, errors, and possible damage to the surrounding This is a haptic device with 6 degrees of freedom that was
environment as the operator has to fight the system to get configured to output the pose in space represented by the
to where she actually wants to go. On the other hand, if the stylus. Additionally, a small amount of constant friction was
system is highly confident in the predicted intent, then the applied to stabilize motion of the stylus and make it easier
strength of the haptic assistance should be increased to guide for the operator to produce precise adjustments. The eye-
the operator to the target position and minimize the risk of tracking portion of the project was based on the Tobii Rex
damaging the environment. eye tracker. This eye tracker is a video-based remote system
Therefore, the safety boundary will open up when the which can track the user’s eyes from 40-90 cm away and
confidence level is low, and tighten when the confidence level allows significant head movement as long as it stays inside
is high. Specifically, S and θ will be increased while H is the trackable volume. To determine the fully specified target
decreased when the confidence is low. This relationship is position, a structured light sensor, the Microsoft Kinect, was
shown in (6). used. The Kinect provides a depth image at a resolution of
640x480 that is used to determine the depth of the target
(cid:40)
ci−ithresh ci≥ithresh position. The Kinect also supplies the video feedback looking
sci= 1−ithresh (6a)
ci−ithresh ci<ithresh straight-on to the scene. The robot arm used was a three-
ithresh
fingered Mico robot from Kinova. Opening and closing of the
paramAdjust=sci∗paramMaxAdjustAmount (6b)
robot’s fingers was controlled by a button on the Geomagic
where sci is the scaled confidence in the intent, ci is the con-
Touch. Updates to change the desired position or state of the
fidence in the predicted intent calculated using (1), ithresh is
fingers were sent to the robot at approximately 20 Hz, unless
theconfidencethresholdlevel,paramAdjustistheamountto
the joystick was immobile and the fingers were not being
adjustoneoftheparameters,andparamMaxAdjustAmount
controlled. The experimental setup can be seen in Fig. 9.
is the maximum amount that the parameter can be adjusted.
Theconfidencethresholdlevelistheconfidencelevelatwhich
B. Validation Process
theintentpredictionishighenoughtobegintomakethesafety
boundary more restrictive. It is suggested that this be set to a Two different tasks were tested in the experiment, cutting
value over 50% because that is the point at which the system and grasping. For the grasping task, volunteers were asked to
(a) Operator-side setup (b) Robot-side setup
Fig. 9: Experimental setup which shows both the operator-side and the robot-side setup.
TABLE III: Combinations tested for the validation of the
use the teleoperation system to pick up a tennis ball. The task
haptic assistance.
was considered a failure if the tennis ball was knocked off its
stand, or if any of the surrounding obstacles were disturbed.
Test Task HapticAssistance
The setup for this task is shown in Fig. 10a. For the cutting
1 cutting safetyboundary
task, volunteers were asked to cut a strip of paper in a special
2 cutting guidanceforce
marked area using the teleoperation system. This is illustrated
3 grasping safetyboundary
in Fig. 10b. Failure occurred during this task if the strip was
4 grasping guidanceforce
cutinthewronglocationorthesurroundingareawasharmed.
5 cutting noassistance
This task simulated an action similar to one that might be
6 grasping noassistance
required in telesurgery.
The testing procedure began by calibrating the eye tracker
for each volunteer and verifying its accuracy. Each volunteer criteria: success rate, completion time, and attempts by the
was then given as much time as they needed to become operator to close the scissors/fingers.
comfortablewiththesystem,orre-familiarizethemselveswith
it if they had already used it. No force feedback was applied A. Cutting Task
during this part, and practice was done on the grasping task
1) Success Rate: Due to the number of trials obtained, a
with no obstacles. Once they wereready, each task was tested
Laplace estimate is used to determine the best success rate
with both the guidance force and the safety boundary in a
for each condition. Further a 95% adjusted-Wald Interval is
randomized order to ensure results were not skewed by a
used to compare the theoretical bounds on the success rate
learning curve. Additionally, both tasks were tested without
observed. The confidence intervals with the Laplace estimate
any haptic assistance (using the joystick only) to provide a
are observed in Figure 11. For ease of reading, the joystick
baseline for comparison. Explicitly, the system was tested in
successratehasbeenduplicatedonthefigureforbothcontrol
thecombinationslaidoutinTableIII.Twotothreetrialswere
modes. In both cases, the assistance improves the success rate
performed for each combination and the success rate, as well
over the joystick only control. The boundary assistance out-
as the joystick and robot trajectory were recorded. The target,
performs the force guidance assistance. The intent adjustment
either the tennis ball or paper strip, was randomly placed for
appearstohelptheforceguidancemodeperformbetter(likely
eachtrial.Thetargetpositionwascontinuouslyacquiredfrom
due toimpacting the magnitudeof the forcemore intuitively).
thegazeandbeforeeachtrialtheGeomagicTouchwasplaced
Yet, the intent adjustment in the boundary assistance does not
into a starting position that the robot mirrored.
seeimprovement,althoughthesuccessratesarerathersimilar.
Four volunteers were tested. The ages of those who partici-
Thisislikelyduetotheusersperceivingthischangeassubtle
pated in the testing were in the range 18 to 25 and two of the
adaption.AnN-1Chi-Squaretestwasconductedtodetermine
volunteers wore glasses. One was left-handed and three had
statistical significance between any proportions. No statistical
prior experience using the system.
significance was found.
2) Execution Time: Time based evaluations are notorious
VII. RESULTSANDDISCUSSION
for being positively skewed [44], and for this reason the
During each trial, the target location, indicated by the gaze, analysis is done by log-transforming the data. The geometric
and the trajectory of the Geomagic Touch and the robot arm means and 95% confidence intervals are presented in Figure
was recorded. Additional information recorded for each trial 12. The boundary assistance does better than joystick control.
includedthenumberoftimestheparticipanttriedtograspthe The boundary assistance also outperforms the force guidance.
ballbyclosingthefingersandwhetherornottheywerefinally The intent adjustment helps both the boundary and guidance
successful. The results are broken down into two separate force improve the speed to complete the task. However, the
tasks, cutting and grasping. For each we will evaluate three improvement is more noticeable in the force guidance. A
(a) Grasping task setup. (b) Cutting task setup.
Fig. 10: Task setups used for testing the haptic assistance.
3) Cutting Attempts: A standard arithmetic mean and 95%
confidence interval was obtained for the cutting attempts
of each control strategy. They are displayed in Figure 13.
The confidence intervals of the boundary assistance are as
low or lower than the joystick control. The guidance force
required users to make more attempts to cut. This surge in
attempts is most likely responsible for the time increase. The
intent adjustment reduces the number of attempts needed to
accomplish the task. A two-sample t-test was conducted for
intent adjusted vs not intent adjusted control modes for a fair
comparison. No statistical significance was found.
Fig. 11: Confidence intervals for the success rates for each
combination of haptic assistance per task.
two-sample t-test was conducted on each condition, and no
statistical significance was found. Although no significance
wasfound,theconfidenceintervalsreinforcethattheboundary
approachisabettercontrolstrategy.Ithasthesmallestbounds
while accomplishing the least amount of time. The force
guidance appears to be a hindrance to users as if it requires
effort to resist undesired movements. However, the extra time
observed from the guidance force may be a result incorrect
depth registering as evident by the cutting attempts. Fig. 13: Confidence intervals for the cutting attempts for each
combination of haptic assistance per task.
B. Grasping Task
For the grasping task, only the assistance modes are com-
pared. The goal of this task is to see if the intent adjustment
is different from no intent adjustment. Issues from this task
occurred when the robot hand would occasionally be the
inferred gaze target.
1) Success Rate: Due to the number of trials obtained, a
Laplace estimate is used to determine the best success rate
for each condition. Further a 95% adjusted-Wald Interval is
Fig. 12: Confidence intervals for execution time for each
used to compare the theoretical bounds on the success rate
combination of haptic assistance per task.
observed. The confidence intervals with the Laplace estimate
areobservedinFigure14.Theintentadjustmentdoesnothave
a positive influence on the success rate. In the force guidance
case, it appears to lower the success. An N-1 Chi-Square test
was conducted to determine statistical significance between
any proportions. No statistical significance was found.
Fig.16:Confidenceintervalsforthegraspingattemptsforeach
combination of haptic assistance per task.
damage. In addition, the intent confidence is a valuable ad-
Fig.14:Confidenceintervalsforthegraspingsuccessforeach dition to the approach which allows the system to respond to
combination of haptic assistance per task. theoperator’sfocus,andprovidesmorenaturalcontrolforthe
operator.
2) Execution Time: The geometric means and 95% confi- Theresultsalsorevealsomedetailsabouthoweachformof
dence intervals are presented in Figure 15. The completion haptic assistance affects each task. While the safety boundary
time for the grasping task leads to mixed results. The intent with intent confidence appears to be quite helpful, the same
adjustment helps the boundary approach; however, it does not is not true of the guidance force. For maximum success, the
helptheguidanceforceassistance.Forthenointentadjustment intent confidence adjustment should not be used with the
cases, the force guidance does better than the boundary. For guidance force on the grasping task as this addition decreased
intent adjustment, the boundary outperforms the guidance the success rate. Furthermore, it seems that the guidance
force. A two-sample t-test was conducted on each condition. force for the cutting task is not as helpful. It may be a
Despite the variations of the confidence intervals no statistical better idea to use the safety boundary with intent adjustment
significance was found. and use a strategy such as the potential hybrid control for
position investigated in our previous work. This will provide
the operator with partial visual feedback and partial haptic
feedback.Inthiscase,theoperatorwillstillclosetheloopand
have full control over the system, but will not be distracted
by the guidance force.
VIII. CONCLUSION
The presented haptic assistance adjusted based on the sys-
tem’sconfidenceinthegaze-derivedoperator’sintentfortele-
operation increases the control performance in teleoperation.
It is natural and easy to use because it takes advantage of
a natural characteristic of the operator’s behavior. It prevents
Fig. 15: Confidence intervals for the grasping execution time
collateral damage through the use of a safety boundary which
for each combination of haptic assistance per task.
alsohelpstheoperatorapproachthecorrectdepth.Theresults
in section VII show that users are faster and more accurate
3) Grasping Attempts: A standard arithmetic mean and
when using this system.
95% confidence interval was obtained for the grasping at-
Future work will involve increasing the accuracy of the in-
tempts of each control strategy. They are displayed in Figure
ferredintentandtheconfidenceinthisintent.Thisnecessarily
16. The intent adjustment forces more attempts to occur to
requires more information than the operator’s eye movements
graspthetennisball.Thebestscenarioistheboundarywithout
alone. The reason for this is that the gaze is really an
intent adjustment. A two-sample t-test was conducted for
observationalmechanismandwasnotintendedtobeacontrol
adjusted vs not adjusted control modes for a fair comparison.
input. In order for control to be truly natural, the operator’s
No statistical significance was found.
gaze has to be used in such a way as to not interrupt their
regularbehavior.However,theoperatorwillnotjustlookatthe
C. Summary of Results
targetduringcompletionoftheteleoperationtask.Hewillalso
In summary, the presented results show that this system look at the robot end-effector, at the surrounding obstacles, or
improves teleoperation control by assisting the operator in other distractions depending on the environment. Separating
reaching the correct target depth and preventing collateral these eye movements, which have little to do with the final
goaloftheteleoperationtask,fromthe“valid”fixationsonthe [17] A. Belardinelli, O. Herbort, and M. V. Butz, “Goal-oriented gaze
target position is very difficult without additional information. strategiesaffordedbyobjectinteraction,”VisionResearch,vol.106,pp.
47–57,2015.
Thisisespeciallytrueinamoregeneralsettingwherethetasks
[18] L.vanderLinden,S.Mathoˆt,andF.Vitu,“Theroleofobjectaffordances
maynotberelatedtoreaching.Therefore,eye-movementdata and center of gravity in eye movements toawrd isolated daily-life
aloneislikelynotsufficientforahighlyaccuratedetermination objects,”JournalofWisdom,vol.15,no.5,2015.
[19] J.Crawford,W.Medendorp,andJ.Marotta,“Spatialtransformationsfor
of the operator’s intent.
eye-handcoordination,”JournalofNeurophysiology,vol.92,pp.10–14,
Introducing context into the intent inference process could 2004.
go a long way to solving the aforementioned issues. For [20] A.BelardinelliandM.V.Butz,“Gazestrategiesinobjectidentification
andmanipulation,”inCogSci,Berlin,2013.
example, since the location of the robot hand is known,
[21] Y. P. Rybarczyk, O. Ait-Aider, P. Hoppenot, and E. Colle, “Remote
fixationsontherobotend-effectorcanautomaticallybefiltered control of a biometrics robot assistance system for disabled persons,”
from the intent inference process. Additional steps may be to AMSEModelling,Measurement,andControl,vol.63,2002.
[22] S. Hirche and M. Buss, “Human-oriented control for haptic teleopera-
considertheactualstructureoftheenvironmentbeinggazedat.
tion,”ProceedingsoftheIEEE,vol.100,no.3,pp.623–647,2012.
Ifthereisnoobjectatthefixationlocation,ortheobjectisnot [23] J.Rosen,B.Hannaford,M.P.MacFarlane,andM.N.Sinanan,“Force
graspable, then the gazed-at location must not be the intended controlled and teleoperated endoscopic grasper for minimally invasive
surgery-experimentalperformanceevaluation,”IEEETransactionson
goal of the action. Furthermore, taking into account gaze
BiomedicalEngineering,vol.46,no.10,pp.1212–1221,2009.
history could provide additional insight into the true intent [24] C. R. Wagner, R. D. Howe, and N. Stylopoulos, “The role of force
of the operator. These considerations will improve the intent feedbackinsurgery:Analysisofbluntdissection,”inHapticInterfaces
forVirtualEnvironmentandTeleoperatorSystems,Orlando,2002.
inference, confidence level, and overall control significantly.
[25] D.Feygin,M.Keehner,andF.Tendick,“Hapticguidance:Experimental
evaluationofahaptictrainingmethodforaperceptualmotorskill,”in
REFERENCES Haptic Interfaces for Virtual Environment and Teleoperator Systems,
Orlando,2002.
[1] Y.BaudoinandM.K.Habib,UsingRobotsinHazardousEnvironments: [26] J. J. Abbot, P. Marayong, and A. M. Okamura, Robotics Research.
LandmineDetection,De-miningandOtherApplications. Cambridge: SpringerBerlinHeidelberg,2007,vol.28.
WoodheadPublishingLimited,2011. [27] N. Turro, O. Khatib, and E. Coste-Maniere, “Haptically augmented
[2] B. Brumson, “Chemical and hazardous material handling teleoperation,” in IEEE International Conference on Robotics and Au-
robotics,” Jan. 2007. [Online]. Available: http://www. tomation,Seoul,2001.
robotics.org/content-detail.cfm/Industrial-Robotics-Industry-Insights/ [28] G. P. Mylonas, K.-W. Kwok, A. Darzi, and G.-Z. Yang, “Gaze-
Chemical-and-Hazardous-Material-Handling-Robotics/content id/614 contingent motor channeling and haptic constraints for minimally in-
[3] M. Fachot, “International electrotechnical commission,” Jul. 2011. vasive robotic surgery,” in Medical Image Computing and Computer-
[Online].Available:http://www.iec.ch/etech/2011/etech 0711/ind-1.htm AssistedIntervention,NewYork,2008.
[4] Y. P. Rybarczyk, E. Colle, and P. Hoppenot, “Contribution of nuero- [29] T. M. Lam, H. W. Boschloo, M. Mulder, and M. M. van Paassen,
science to the teleoperation of rehabilitation robot,” in Systems, Man “Artificial force field for haptic feedback in uav teleoperation,” IEEE
andCybernetics,Hammamet,Tunisia,2002. TransactionsonSystems,Man,andCybernetics-PartA:Systemsand
[5] D. Y. P. Henriques, W. P. Medendorp, C. C. A. M. Gielen, and J. D. Humans,vol.39,no.6,pp.1316–1330,2009.
Crawford,“Geometriccomputationsunderlyingtheeye-handcoordina- [30] N.Famaey,E.Verbeken,S.Vinckier,B.Willaert,P.Herijgers,andJ.V.
tion: Orientations of the two eyes and the head,” Experimental Brain Sloten,“Invivosofttissuedamageforapplicationsinsurgery,”Medical
Research,vol.152,pp.70–78,2003. Engineering&Physics,vol.32,no.5,pp.437–443,2010.
[6] J. Gomer, C. Dash, K. Moore, and C. Pagano, “Using radial outflow [31] T. L. Gibo, L. N. Verner, and D. Okamura, “Design considerations
to provide depth information during teleoperation,” Presence, vol. 18, and human-machine performance of moving virtual fixtures,” in IEEE
no.4,pp.304–320,2009. InternationalConferenceonRoboticsandAutomation,Kobe,2009.
[7] D. Drascic, “Skill acquisition and task performance in teleoperation [32] F.Ryden,S.N.Kosari,andH.J.Chizeck,“Acomputervisionapproach
usingmonoscopicandstereoscopicvideoremoteviewing,”Proceedings to virtual fixtures in surgical robotics,” 2012. [Online]. Available:
oftheHumanFactorsandErgonomicsSocietyAnnualMeeting,vol.35, http://automation.berkeley.edu/RSS2012Workshop/abstract2.pdf
no.19,pp.1367–1371,1991. [33] F. Ryden, H. J. Chizeck, S. N. Kosari, H. King, and B. Hannaford,
[8] G. P. Mylonas, A. Darzi, and G.-Z. Yang, Medical Imaging and “Using kinect and a haptic interface for implementation of real-time
AugmentedReality. SpringerBerlinHeidelberg,2004. virtualfixtures,”inRoboticsSciencesandSystems,WorkshoponRGB-
[9] T.BrooksandI.Ince,“Operatorvisonaidsfortelerobotassemblyand D:AdvancedReasoningwithDepthCameras,LosAngeles,2011.
servicinginspace,”inIEEEInternationalConferenceonRoboticsand [34] F.Despinoy,J.L.Torres,M.Vitrani,andB.Herman,“Towardremote
Automation,Nice,1992. teleoperationwitheyeandhand:Afirstexperimentalstudy,”in3rdJoint
[10] W. A. Hoff, L. B. Gatrell, and J. R. Spofford, “Machine-vison-based Workshop on New Technoogies for Computer/Robot Assisted Surgery,
teleoperation aid,” Telematics and Informatics, vol. 8, no. 4, pp. 403– 2013.
423,1991. [35] K. Fujii, Gaze Contingent Robotic Control in Minimally Invasive
[11] H. Martins and R. Ventura, “Immersive 3-d teleoperation of a search Surgery. London:ImperialCollegeofLondon,2014.
and rescue robot using a head-mounted display,” in IEEE Conference [36] A.Pandya,L.A.Reisner,B.King,N.Lucas,A.Composto,M.Klein,
onEmergingTechnologiesandFactoryAutomation,Mallorca,2009. and R. D. Ellis, “A review of camera viewpoint automation in robotic
[12] S. Li, X. Zhang, F. J. Kim, R. D. da Silva, D. Gustafson, and andlaparoscopicsurgery,”Robotics,vol.3,no.3,pp.310–329,2014.
W. R. Molina, “Attention-aware robotic laparoscope based on fuzzy [37] M. Yu, Y. Lin, D. Schmidt, X. Wang, and Y. Wang, “Human-robot
interpretationofeye-gazepatterns,”JournalofMedicalDevices,vol.9, interaction based on gaze gestures for the drone teleoperation,” Eye
no.4,2015. MovementResearch,vol.7,no.4,pp.1–14,2014.
[13] A. D. Dragan, S. S. Srinivasa, and K. C. T. Lee, “Teleoperation [38] H. O. Latif, “Mobile robot teleoperation through eye-gaze (telegaze),”
withintelligentandcustomizableinterfaces,”JournalofHuman-Robot Ph.D.dissertation,NottinghamTrentUniversity,2010.
Interaction,vol.2,no.2,pp.33–57,2013. [39] Z. Ahmed and A. Shahzad, “Mobile robot navigation using gaze
[14] J.B.PelzandR.Canosa,“Oculomotorbehaviorandperceptualstrate- contingent dynamic interface,” Master’s thesis, Blekinge Institute of
giesincomplextasks,”VisionResearch,vol.41,no.25-26,pp.3587– Technology,2010.
3596,2001. [40] A. Doshi and M. M. Trivedi, “On the roles of eye gaze and head dy-
[15] M.F.LandandM.Mayhoe,“Inwhatwaysdoeyemovementscontribute namicsinpredictingdriver’sintenttochangelanes,”IEEETransactions
toeverydayactivities?”VisionResearch,vol.41,no.25-26,pp.3559– onIntelligentTrasportationSystems,vol.10,no.3,pp.453–462,2009.
3565,2001. [41] S.Nilsson,T.Gustafsson,andP.Carleberg,“Handsfreeinteractionwith
[16] N.Mennie,M.Hayhoe,andB.Sullivan,“Look-aheadfixations:Antic- virtualinformationinarealenvironment:Eyegazeasaninteractiontool
ipatoryeyemovementsinnaturaltasks,”ExperimentalBrainResearch, inanaugmentedrealitysystem,”PsychNologyJournal,vol.7,no.2,pp.
vol.179,no.3,pp.427–442,2006. 175–196,2009.
[42] C. Passenberg, A. Peer, and M. Buss, “A survey of environment-
, operator-, and task-adapted controllers for teleoperation systems,”
Mechatronics,vol.20,no.7,pp.787–801,2010.
[43] J. D. Webb, S. Li, and X. Zhang, “Using visuomotor tendencies to
increase control performance in teleoperation,” in American Controls
Conference,Boston,2016.
[44] J. Sauro and J. R. Lewis, “Average task times in usability tests:
Whattoreport?”inProceedingsoftheSIGCHIConferenceonHuman
Factors in Computing Systems, ser. CHI ’10. New York, NY, USA:
Association for Computing Machinery, 2010, p. 2347–2350. [Online].
Available:https://doi.org/10.1145/1753326.1753679

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "The Use of Gaze-Derived Confidence of Inferred Operator Intent in Adjusting Safety-Conscious Haptic Assistance"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
