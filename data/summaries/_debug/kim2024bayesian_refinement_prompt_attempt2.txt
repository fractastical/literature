Fix these issues in your summary:
- Too short: 129 words (minimum 200)

Current summary:
Okay, here’s a revised summary of the paper, adhering to all the specified requirements.### Bayesian Mechanics of Synaptic Learning under the Free Energy PrincipleThe core of this paper investigates the mechanisms of synaptic learning through the lens of the Free Energy Principle (FEP), proposing a framework for understanding how the brain learns and adapts. The research centers on developing a computational model that reflects the principles of Bayesian inference, offering a novel approach to understanding cognitive processes. The work aims to provide a framework for understanding how the brain learns and adapts.The authors state: “The free energy principle provides a unified brain theory.” 

The core of the research focuses on developing a computational model that reflects the principles of 

Bayesian inference, offering a novel approach to understanding cognitive processes

Paper text:
4202
tcO
3
]CN.oib-q[
1v27920.0142:viXra
Bayesian Mechanics of Synaptic Learning under the
Free Energy Principle
Chang Sub Kim
Department of Physics, Chonnam National University, Gwangju 61186,Republic of
Korea
E-mail: cskim@jnu.ac.kr
Abstract. Thebrainisabiologicalsystemcomprisingnervecellsandorchestratesits
embodiedagent’sperception,behavior,andlearninginthedynamicenvironment. The
free energy principle (FEP) advocated by Karl Friston explicates the local, recurrent,
andself-supervisedneurodynamicsofthebrain’shigher-orderfunctions. Inthispaper,
we continue to finesse the FEP through the physics-guided formulation; specifically,
we apply our theoryto synaptic learningby consideringit aninference problemunder
the FEP and derive the governing equations, called Bayesian mechanics. Our study
uncovers how the brain infers weight change and postsynaptic activity, conditioned
on the presynaptic input, by deploying the generative models of the likelihood and
prior belief. Consequently, we exemplify the synaptic plasticity in the brain with a
simple model: we illustrate that the brain organizes an optimal trajectory in neural
phasespaceduringsynapticlearningincontinuoustime,whichvariationallyminimizes
synaptic surprisal.
Keywords free energy principle; synaptic learning; Bayesian mechanics; continuous-
state formulation
Bayesian Mechanics of Synaptic Learning under the Free Energy Principle 2
1. Introduction
The brain’s perception, body movement, and learning are conjointly organized to
ensure the embodied agents’ homeostasis and adaptive fitness in the environment. It
is tempting to imagine a neural observer in the brain presiding over higher animals’
cognitive control. Such a homunculus idea is untenable and must be discarded in the
present-day brain theory [1]. However, there is still a much distance to the complete
scientific understanding of the emergent higher-order functions from the brain matter; it
demands a comprehension of the profound interplay between the scientific reductionism
and teleological holism standpoints [2, 3].
The brain-inspired FEP is a purposive theory that bridges the gap between top-
down teleology and bottom-up scientific constructionism. According to the FEP [4, 5],
allliving systems areself-organizedtotendtoavoidanatypical niche intheenvironment
forexistence. TheFEPadoptstheautopoietichypothesis[6]andscientificallyformalizes
the abductive rationale of organisms’ making optimal predictions and behavior from
incomplete sensory data. To be precise, the FEP suggests an information-theoretic,
variational measure of environmental atypicality, termed free energy (FE). The FE
objective is technically defined as a functional of the probabilistic generative density
specifying the brain’s internal model of sensory-data generation and environmental
change and an online auxiliary density actuating variation. The Bayesian brain
computes the posterior of the environmental causes of uncertain sensory data by
minimizing the FE, whose detailed continuous-state description can be found in [7]. For
discrete-state models of the FEP with discrete time, we recommend [8, 9] to readers.
When a Gaussian probability is employed for the variational density [10], the FE
becomes a L 2 norm specified by the Gaussian means and variances and termed the
Laplace-encoded FE [7]. Thus, the Laplace-encoded FE provides a scientific base of the
L 2 objectives in a principled manner, which are widely used in machine learning and
artificial intelligence. For instance, the optimization function in the predictive-coding
framework is proposed to be a sum of the squared prediction errors [11]. Also, the
loss function of a typical artificial neural network (ANN) is often written as a sum of
squareddifferencesbetweenthegroundtruthandthepredictiveentriesfromthenetwork
[12]. Furthermore, it is argued that the Gaussian sufficient statistics are encoded by
the biophysical brain variables, which form the brain’s low-dimensional representations
of environmental states. This way the brain acquires access to the encoded FE for
minimization as it becomes fully specified in terms of the brain’s internal states.
Our research over the years has been devoted to developing continuous-state
implementation of the FE minimization in a manner guided by physics laws and
principles [13, 14, 15]. We endeavored to advance the FEP to the point where it
coalesces into a unified principle of top-down architecture and material base. Moreover,
to promote the FEP to nonstationary problems, we incorporated the fact that the
physical brainis ina nonequilibrium (NEQ) stationary stateandis generally continually
aroused by nonstationary sensory stimuli. The functional brain must perform the
Bayesian Mechanics of Synaptic Learning under the Free Energy Principle 3
variational Bayesian inversion of nonstationary sensory data to compute the posterior
mentioned above. Previously, we accounted for the brain behavior of perception and
motor control as described by attractor dynamics and termed the governing equations
Bayesian mechanics (BM). The BM coordinates the brain’s sensory estimation and
motor prediction in neural phase space. In this paper, we make further progress by
incorporating the brain’s synaptic learning into the BM, which we did not accommodate
in our earlier studies. Learning constitutes the crucial brain function of consolidating
memory, e.g., via Hebbian plasticity [16].
This paper aims to provide a simple but insightful model for synaptic learning
in the brain. Our agendas are that the functional brain operates continually using
continuous environmental representations and that synaptic learning is a cognitive
phenomenon that may very well be understood when statistical-physical laws guide
it. The notion cognition throughout this paper is meant to be the brain’s higher-
order capability that involves a top-down, internal model. We consider the NEQ brain
a problem-solving matter, cognitively interacting with the environment. To quantify
the synaptic cognition, we will specify the generative densities furnishing the Laplace-
encoded FEin a manner to meet the NEQ stationarity and present the FE minimization
scheme by practicing the principle of least action (Hamilton’s principle) [17]. The novel
contributions worked out in this paper are discussed in Section 7.
The rest of the paper is organized as follows. In Section 2, the single-synapse
structure of our interest is described. The essence of the FEP is recapitulated with
revision made for synaptic learning in Section 3. In Section 4, an NEQ formulation
is presented, which determines the likelihood and prior densities in the physical brain.
Next, Section 5 identifies the FEobjective asa classical action andderives the governing
equations of synaptic dynamics by exercising the Hamilton principle. The utility of our
theory is demonstrated in Section 6 using a simple model. After the discussion in
Section 7, a conclusion is given in Section 8.
2. Single Synapse Model
This workconcerns thebrain’ssynapticlearning without considering howenvironmental
processes arouse stimuli at the sensory interface; as a parsimonious model, we focus
on a single synapse within the brain’s internal environment. For instance, in the
hippocampus, the postsynaptic action potential in the dentate gyrus is evoked by a
presynaptic signal from the entorhinal cortex caused by a neural signal from other
brain areas. Accordingly, the synaptic coupling between two pyramidal neurons in the
hippocampus constitutes a single synaptic assembly of interest.
We depict the single synaptic model in Figure 1, where the presynaptic and
postsynaptic signals are denoted by s and µ, respectively; both are the brain’s
representations of noisy synaptic signals. In addition, thesynaptic plasticity is mediated
by the weight strength denoted by w. The synaptic structure considered is generic for
all neurons; accordingly, the ensuing formulation below applies to other brain regions.
Bayesian Mechanics of Synaptic Learning under the Free Energy Principle 4
Figure 1. Single synaptic assembly. The postsynaptic neural state µ is
neurophysicallyevokedbythepresynapticsignals,mediatedbytheweightchange∆w
accordingtoHebb’srule9sµ. WeadopttheBayesian-inferenceperspective,suggesting
thatthe brainstate µ infers the causeofthe presynapticinput s, andthe weightstate
w makes up the synaptic input-output interface.
Note that we will handle the weight variable w as a neurophysical degree of freedom
like s and µ; this handling contrasts with ANN models, where the weights are treated
as a static parameter.
3. Free Energy Principle for Synaptic Learning
The brain-inspired FEP is built on three hypotheses: 1) surprisal hypothesis, 2)
representation hypothesis, and 3) computability hypothesis, which we recapitulate here
with the revision applying to the synaptic learning problem.
3.1. Surprisal Hypothesis
We assume that the presynaptic signals s streaming into the synaptic interface are
prescribed and focus on the resulting synaptic dynamics. For convenience, here we
introduce the notation ϑ˜ , by which we collectively denote the postsynaptic variable M
and the weight variable W:
ϑ˜ M,W .
“ t u
The variables M and W are stochastic and unknown, so they are hidden from the
brain’s perspective.
The brain’s cognitive goal is to compute the posterior p ϑ˜s , which the FEP fulfills
p | q
via variational Bayes in the following manner: First, we define the information-theoretic
measure called the Kullback-Leibler (KL) divergence:
q ϑ˜
D q ϑ˜ p ϑ˜s dϑ˜q ϑ˜ ln p q , (1)
KL p q} p | q “ p q p ϑ˜s
´ ¯ ż p | q
Bayesian Mechanics of Synaptic Learning under the Free Energy Principle 5
which is always positive [18], where dϑ˜ means dMdW. Some terminologies: q ϑ˜ is
p q
called R-density, which approximates the trueposterior p ϑ˜s inthe variational scheme.
p | q
The posterior makes up the so-called G-density p ϑ˜,s p ϑ˜s p s together with the
p q “ p | q p q
marginal density p s [7]. Second, using the preceding product rule, the above KL
p q
divergence can be decomposed to
D q ϑ˜ p ϑ˜s F q ϑ˜ ,p ϑ˜,s lnp s .
KL
p q} p | q “ r p q p qs` p q
The functional F o ´ n the right-h ¯ and side (RHS), which is identified to be
q ϑ
F q ϑ˜ ,p ϑ˜,s dϑ˜ q ϑ˜ ln p q , (2)
r p q p qs ” p q p ϑ˜,s
ż p q
istheinformational freeenergy(FE).Third, thepositivityofD leadstotheinequality
KL
lnp s F q ϑ˜ ,p ϑ˜,s . (3)
´ p q ď r p q p qs
Equation (3) is the mathematical statement of the brain-inspired FEP accounting
for life and cognitive phenomena in a universal manner, which comprises the surprisal
hypothesis. In the present context, the preceding inequality enunciates that synaptic
learningcorrespondstothebrain’sminimizing F, whichisaproxyforsynaptic surprisal,
lnp s , as an upper bound. In practice, it is intractable to determine the marginal
´ p q
density p s , which provides synaptic evidence to the brain. Note here that F is called
p q
FE by mimicking thermodynamic FE in physics, which monotonically decreases upon
spontaneous changes in a macroscopic open system, conforming to the second law of
thermodynamics [15].
3.2. Representation Hypothesis
Accordingtotheinequality[Equation(3)], thebrainvariationallyminimizesF bymeans
of the R-density q ϑ˜ : when the synaptic interface is elicited by the presynaptic stream
p q
s, the brain launches q ϑ˜ , an online approximation of the posterior; in the face of the
p q
synaptic stream, the R-density probabilistically represents the uncertain, hidden causes
ϑ˜ M,W , trying to match best with the posterior. Here, we adopt the Laplace
“ t u
approximation for the R-density, which assumes a Gaussian form [19]:
1 1
q ϑ˜ exp ϑ˜ µ˜ 2 , (4)
p q “ ?2πσ˜2 r´2σ˜2p ´ q s
where µ˜ and σ˜ are the sufficient statistics of the Gaussian density. In particular, we
intend that the means denoted by
µ˜ µ,w
“ t u
are the coarse-grained representations of high-dimensional ϑ˜ M,W ; they are the
“ t u
latent brain variables in low dimensional neural space [20]. Also, the dependence on
the variances σ˜ can be eliminated by further manipulation as elaborated in [7]. Then,
under the Laplace approximation, the FE functional reduces to
F q ϑ˜ ,p ϑ˜,s lnp µ˜,s constants.
r p q p qs “ ´ p q`
Bayesian Mechanics of Synaptic Learning under the Free Energy Principle 6
The nontrivial part in the reduced expression is the Laplace-encoded FE denoted by F:
F µ,w,s lnp µ,w,s , (5)
p q “ ´ p q
which is a function of only the brain variables µ and w. In the present work, the
presynaptic input s is not a dynamical variable but is handled as an external time-
dependent input.
Here, the brain is assumed to be endowed with the generative density p µ,w,s
p q
encoded over the evolutionary and developmental time scales. The FEP inequality
given in Equation (3) now becomes
lnp s F µ,w,s ; (6)
´ p q ď p q
the brain has an access to F µ,w,s by means of its internal variables µ and w. The
p q
preceding expression comprises the representation hypothesis in the FEP; the brain
uses the coarse-grained representations µ and w in variationally minimizing synaptic
surprisal. Then, by applying the product rule p µ,w,s p w µ,s p µ,s , the Laplace-
p q “ p | q p q
encoded FE is completed as
F µ,w s lnp w µ,s p µ,s , (7)
p | q “ ´ p | q p q
where p w µ,s is the likelihood of the weight strength w given a postsynaptic signal µ,
p | q
andp µ,s istheprioraboutthepostsynapticdynamics, bothsubject tothepresynaptic
p q
input s.
Equation(7)istheobjectivefunctionforsynapticlearningundertheFEP,furnished
with only brain variables, which makes the brain-inspired FEP a biologically plausible
theory. Previously, we suggested that all the involved probabilities be specified as NEQ
stationary densities derived from the Fokker–Planck equation [15]. This work takes a
different approach to determining the NEQ densities in Section 4.
3.3. Computability Hypothesis
The brain is endowed with the mechanism that actuates the FE minimization, which
comprises the computability hypothesis in the FEP. The conventional continuous-state
implementation assumes that the brain employs gradient descent (GD) methods to
execute theFEminimization [5]. The GDschemes updatethe neural activity µ downhill
on the FE landscape, which is woven by the generalized coordinates of motion of all
dynamical orders surpassing the second order, namely, acceleration [21, 22]. It is argued
that generalized motion can effectively incorporate the temporal correlation of random
fluctuationsinstochasticdynamics beyond whitenoise. However, theidea ofgeneralized
motion transcends normative Newtonian physics; thus, its theoretical ground draws
critical attention in the literature [23, 14]. For the weight variable w, to incorporate its
slower change than the neural activity, a different update rule is applied: for instance,
instead of the weight (parameters or hyper-parameters), its rate may be updated under
the GD scheme [7]. Recently, researchers have extended the applicability of FEP-based
GD algorithms to robotics and artificial intelligence problems, emphasizing colored-
noise modeling [24]. However, it is significant to note that using GD methods is not
Bayesian Mechanics of Synaptic Learning under the Free Energy Principle 7
legitimate when the environmental inputs vary fast so that the FE landscape becomes
non-static (see, for further discussion, Section 7). Our formulation aims at the general
time-dependent situation and, thus, avoids using a GD scheme; instead, we identify the
FE objective to be a classical action in mechanics and exercise Hamilton’s principle
for the FE minimization according to the standard theory [17]. The details are given
in Section 5, where we derive the governing equations of motion for synaptic inference
regarding the canonical physical variables without invoking the generalized motion.
4. Nonequilibrium Generative Densities
We argued that the physical brain probabilistically encodes the representations of the
internal and external hidden states (Section 3.2). The encoded probabilities constitute
thegenerative densities thatfurnishthebrainwithFEobjective forvariationalBayesian
inference. Therefore, the generative densities must be specified in terms of the
biophysical brain variables in an NEQ stationary state. Here, we present a stochastic
thermodynamic model for the NEQ densities, viewing the brain as a soft material
consisting of neural constituents. This perspective brings us closer to understanding
the brain’s NEQ states.
4.1. Prior for Postsynaptic Activity
For a simple description, we assume that the brain variable µ obeys an overdamped
Langevin dynamics on a mesoscopic scale:
dµ
f µ,θ ξ, (8)
dt “ p q`
where f and ξ on the RHS are the deterministic and random forces, respectively,
causing the neural change; θ encapsulated in f denotes an input parameter affecting the
system dynamics. The solution to Equation (8) describes a stochastic path or trajectory
µ µ t in continuous state space. Recall that the neural variable µ is the mean of
“ p q
the R-density probabilistically representing external environmental states online, which
may be viewed as a mean field. Also, it is evident that the state transition between two
arbitrarily-close times described by Equation (8) is Markovian. Further assumptions
imposed are i) the noise ξ is Gaussian about zero mean, rendering ξ 0, and ii) the
x y “
noise is delta-correlated, a.k.a. white, through
ξ t1 ξ t σ 2 δ t1 t , (9)
µ
x p q p qy “ p ´ q
where
σ2
is the noise strength. Strictly considering, the biological brain is in an
µ
NEQ stationary state, whose temperature T is distinct from the environmental value;
however, here, we consider that the brain is locally in equilibrium characterized by its
body temperature. Also, we assume that the noise strength is given, according to the
fluctuation-dissipation theorem [25], as
σ
2 2γ´1
k T, (10)
µ “ µ B
Bayesian Mechanics of Synaptic Learning under the Free Energy Principle 8
where γ is the frictional coefficient of the brain matter, and k is the Boltzmann
µ B
constant.
Under the prescribed assumptions, we build the transition probability along a
trajectory µ µ t as time t elapses. To proceed with the derivation, we first note
“ p q
a technical subtlety involved in the white noise ξ t : it is mathematically ill-defined
p q
becausethevarianceisdivergent [seeEquation(9)]. Toaddressthis, theWiener process,
definedthrough∆W ξ∆t,isoftenconceived. Thisprocessintroducesaformofcoarse-
”
graining over a short time interval ∆t, effectively bypassing the singularity of the white
noise at an instant time. The Wiener process is also Gaussian about zero mean with
the well-defined variance ∆W 2 σ2 ∆t. However, one must pay the price for the
µ
xp q y “
Wiener recipe when the Riemann integral is performed for state functions over a finite-
time elapse. In our derivation, we adopt the Ito convention that interprets the integral
of Equation (8) over the time interval ∆t t n`1 t n as
“ ´
∆µ f µ ,θ ∆t ∆W ,
n n n n
“ p q `
where the first term on the RHS was approximated by choosing the value for f µ at
p q
the initial time t n ; other terms are ∆µ n µ n`1 µ n and ∆W n W n`1 W n . Next,
“ ´ “ ´
using the Gussianity of ∆W , we define the transition probability p n 1 n from the
n
p ` | q
Wiener state W n to the next W n`1 as [26]
1 2
p n 1 n exp ∆µ f µ ,θ ∆t .
p ` | q » ´2σ2∆t n ´ p n n q
" µ *
´ ¯
Then, the full Markovian transition over N t ∆t time steps during the finite time
p“ { q
0 t1 t can be built as
ď ď
N´1
∆t ∆µ 2
n
p n 1 n exp f µ ,θ .
p ` | q » ´2σ2 ∆t ´ p n n q
n
ź
“0 # µ
ÿ
n
´ ¯
+
As a final step, we take the continuous limit ∆t 0 in the preceding expression and
Ñ
obtain the path probability p µ,θ , up to a normalization constant, as
p q
1 t dµ 2
p µ,θ exp dt1 f µ,θ t1 , (11)
p q „ ´2σ2 dt1 ´ p p qq
# µ ż 0 ˆ ˙ +
which is known as the Onsager-Machlup function [27].
The above Onsager–Machlup expression specifies the transition probability of the
neural state µ, given initial condition µ 0 , along the continuous path µ µ t .
p q “ p q
When the parameter θ is replaced with s, it represents the prior density p µ,s in
p q
Equation (7) accounting for the brain’s belief about or already-acquired knowledge of
how the postsynaptic activity µ behaves.
4.2. Likelihood of Synaptic Change
Neurotransmitter transport at the synaptic interface mediates synaptic coupling
between two neurons, which is often effectively described by the weight variable w. We
assume that the brain is endowed with an internal model of weight dynamics leveraging
Bayesian Mechanics of Synaptic Learning under the Free Energy Principle 9
learning; learning constitutes the crucial brain function of consolidating memory, e.g.,
via long-term potentiation.
We consider the synaptic weight w a time-dependent variable rather than a static
parameter, and the synaptic plasticity is described by its the rate w9 dw dt. We
“ {
proposethatsimilartoEquation(8), thesynapticplasticityisgovernedbythestochastic
equation:
dw
h w,θ χ, (12)
dt “ p q`
where h is the biophysical force causing the weight change, and χ is the additive
white noise associated with the synaptic process; again, θ is a time-dependent input
parameter. The noise is assumed to be Gaussian about zero mean and delta-correlated:
χ t χ t1 σ2δ t t1 with σ2 being the noise strength.
x p q p qy “ w p ´ q w
Next, to smooth the temporal singularity associated with the white noise χ, we
consider the Wiener process ∆W χ∆t, which is also Gaussian about zero mean
“
with the well-defined variance, ∆W 2 σ2 ∆t. Then, we proceed with the same
xp q y “ w
formulation with Section 4.1 to specify the NEQ likelihood density p w θ . The result
p | q
is given as
1 t dw 2
p w θ exp dt1 h w,θ t1 , (13)
p | q „ ´2σ2 dt1 ´ p p qq
# w ż 0 ˆ ˙ +
which represents the Onsager–Machlup transition probability along the continuous path
w w t , subject to initial condition w 0 .
“ p q p q
In obtaining the above likelihood and prior densities, Equations (11) and (13),
respectively, we assumed that the random fluctuations in the neuronal dynamics were
delta-correlated, i.e., white noises. The brain signals, by contrast, evidently reveal
the frequency spectrum reflecting color-correlated dynamics [28], which supports the
criticality idea in the brain [29]. In this work, we consider only the ideal white noise for
a practical illustration of determining the NEQ brain densities in a physics-grounded
manner. To obtain an analytic expression for the NEQ densities is intractable under
generalconditionseveninthesteadystate[15]; theyareusuallyassumedtobeaninstant
Gaussian set by the Gaussian random noises imposed on the Langevin description [4, 5].
5. Bayesian Mechanics: Computability of Synaptic Learning
The FE landscape becomes nonstatic when the input parameter θ in the generative
densities [Equations (11) and (13)] is explicitly time-dependent. In this case, it
is anticipated that the GD implementation on the FE landscape will fail. Here,
we formulate the brain’s computability under nonstationary conditions, facilitating
nonautonomous neural computation.
In the synaptic learning problem, the presynaptic signal s acts as the input
parameter θ. Accordingly, we replace θ with s in the Onsager–Maclup representations
Bayesian Mechanics of Synaptic Learning under the Free Energy Principle 10
forthe NEQ densities and substitute the results into Equation(7) to obtaintheLaplace-
encoded FE. The outcome is given as
t
F L µ,w;µ9,w9 ;s dt1, (14)
“ p q
0
ż
where the integrand L is expressed as
2 2
1 dµ 1 dw
L µ,w;µ9,w9 ;s f µ,w;s t1 h µ,w,s t1 .(15)
p q ” 2σ2 dt1 ´ p p qq `2σ2 dt1 ´ p p qq
µ ˆ ˙ w ˆ ˙
Note that in Equation (15), we concretely displayed the autonomous dependence on
the variables µ and w and the nonautonomous dependence on the input s through the
generative functions f and h.
Equation (14) manifests a specific association of the FE objective F with the
mathematical object L; namely, F is given as a time integral of L. This observation
is reminiscent of the relation between the action and Lagrangian in classical mechanics
[17]. Accordingly, by analogy, if we identify F as an effective action S and the integrand
Lasaneffective Lagrangian forthebrain’scognitive computation, theFEminimization,
which is mathematically performed by δF 0 under the FEP, is precisely mapped to
“
exercising Hamilton’s principle, δS 0. Then, the Euler-Lagrange equations of motion
“
for determining the optimal trajectories µ t and w t will follow straightforwardly,
p q p q
constituting the synaptic BM. Note the temperature dependence of the Lagrangian
[Equation (15)] via the noisy strengths σ2 and σ2 , see Equation (10), which makes L a
µ w
thermal Lagrangian [27].
Here, working in the Hamiltonian description is more suitable for our purposes. To
this end, we carried out a Legendre transformation to derive an effective Hamiltonian
H; the outcome is expressed as
p2 p2
H µ w p f µ,w;s p h w,µ;s . (16)
µ w
“ 2m ` 2m ` p q` p q
µ w
In the preceding expression of Hamiltonian, the new variables p and p appear, which
µ w
are mechanically conjugate to the variables µ and w, respectively; they are determined
from the definitions:
L L
p B and p B . (17)
µ
“
µ9 w
“
w9
B B
Also, the constants, m and m were defined to be
µ w
2 2
m 1 σ and m 1 σ , (18)
µ “ { µ w “ { w
which are a measure of respective precision of the probabilistic generative models,
Equations (11) and (13). Equation (10) suggests that the generative precisions are
a biophysical constant specified by the body temperature and the friction of the brain
matter. A few points about the Hamiltonian H are noteworthy: The variables (µ, w)
and (p , p ) correspond to positions and momenta, respectively, and the generative
µ w
precisions m and m may be interpreted as a neural mass as a metaphor. The
µ w
Hamiltonian is not breakable into the kinetic and potential energies because the third
Bayesian Mechanics of Synaptic Learning under the Free Energy Principle 11
and fourth terms on the RHS in Equation (16) are given as a product of momentum
and position variables. The H function does not furnish a conservative-energy surface
because of its explicit time dependence through thepresynaptic signal s t , which makes
p q
synaptic learning a nonautonomous problem.
The generative functions f and h for synaptic learning were introduced in
Equations (8) and (12) without specifying them; they are the biophysical forces driving
synaptic dynamics at the neuronal level. We now specify them by the following models:
f µ,w;s γ µ µ ws, (19)
µ d
p q “ ´ p ´ q`
h µ,w;s γ w w sµ. (20)
w d
p q “ ´ p ´ q`
The first terms on the RHSs, involving the damping coefficients γ and γ , prevent
µ w
an unlimited growth of µ and w [30]. The linear damping models may be replaced
with a nonlinear alternative; for instance, the modified γ s2 w w may be used in
w d
´ p ´ q
Equation (20) [31]. The second term ws on the RHS of Equation (19) describes the
presynaptic evoking weighted by w. Moreover, the term sµ in Equation (20) accounts
for Hebb’s rule; one can explore anti-Hebbian learning by inverting its sign. The extra
parameters µ and w are the steady-state values of µ and w, respectively, without
d d
driving terms ws and sµ. After substituting Equations (19) and (20) into Equation (15)
and by evaluating Equation (17), one can determine the neural representations of the
momenta p and p . The results are given as
µ w
p m µ9 f , (21)
µ µ
“ p ´ q
p m w9 h . (22)
w w
“ p ´ q
Note that momentum represents the discrepancy between the state rate and its
prediction from the generative model, modulated by precision, which corresponds to
prediction error in predictive coding theory (see discussion in Section 7).
Having specified the synaptic Hamiltonian given in Equation (16), we now derive
Hamilton’s equations of motion by practicing the standard procedure [17]. Here, we
present only the outcome without intermediate steps:
1
µ9 p γ µ µ ws, (23)
µ µ d
“ m ´ p ´ q`
µ
1
w9 p γ w w sµ, (24)
w w d
“ m ´ p ´ q`
w
p9 γ p sp , (25)
µ µ µ w
“ ´
p9 γ p sp . (26)
w w w µ
“ ´
The resulting Equations (23)–(26) are a set of coupled differential equations for four
dynamical variables µ, w, p , and p , subject to the time-dependent input source
µ w
s, which constitute the synaptic BM governing co-evolution of the state and weight
variables. In Figure 2, we show the neural circuitry implied by the derived BM. We
argue that the functional behavior depicted in the circuitry is generic in every synapse
inthebrainlike every corticalcolumn intheneocortexbehaves asa sensorimotor system
performing the same intrinsic function [32].
Bayesian Mechanics of Synaptic Learning under the Free Energy Principle 12
Figure 2. Schematic of the neural circuitry. The diagram manifests the workings
of the synaptic BM: the presynaptic input sptq drives the interconnected, recurrent
dynamicsamongthestatepw,µqandmomentumpp ,p qvariables. Thelinksdepicted
w µ
by arrowheads indicate an excitatory coupling within a neural unit or between two
neural units, whereas the dot-head links indicate an inhibitory coupling.
For a more compact description, we shall define the cognitive state Ψ as a column
vector in four-dimensional phase space:
ΨT µ,w,p
µ
,p
w
ψ
1
,ψ
2
,ψ
3
,ψ
4
,
“ p q ” p q
where T denotes a transpose operation. Then, the preceding Equations (23)–(26) can
be compactly expressed as
9
Ψ RΨ I, (27)
“ `
where R is a 4 4 matrix identified as
ˆ
γ s 1 m 0
µ µ
´ {
s γ 0 1 m
R ¨ ´ w { w ˛, (28)
“ 0 0 γ s
µ
˚ ´ ‹
˚ 0 0 s γ w ‹
˚ ´ ‹
and the inhomogen ˝ eous vector I is identified to ‚ be
IT γ µ ,γ w ,0,0 . (29)
µ d w d
“ p q
Equation (27) can be formally integrated to bring about the solution:
t
Ψ t e 0 tRpt1qdt1 Ψ 0 dt1e t t 1 RpτqdτI, (30)
p q “ p q`
ş ż 0 ş
where the first term on the RHS is a homogeneous solution, given the initial condition
Ψ 0 , and the second term is the inhomogeneous solution, driven by the source I.
p q
The formal solution represents a continuous path in 4-dimensional phase space, whic

Rewrite the summary fixing the issues. Use exact title: Bayesian Mechanics of Synaptic Learning under the Free Energy Principle
PROFESSIONAL TONE: Begin directly with content - NO conversational openings.
NO REPETITION. Each sentence must be unique. Vary attribution phrases.
Extract quotes VERBATIM from paper text - do NOT modify or "correct" them.