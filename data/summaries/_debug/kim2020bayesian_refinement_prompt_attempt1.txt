=== IMPORTANT: ISOLATE THIS PAPER ===
You are revising a summary for ONLY the paper below. Do NOT reference or use content from any other papers.
Paper Title: Bayesian mechanics of perceptual inference and motor control in the brain
Citation Key: kim2020bayesian
REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Issues to fix:
1. CRITICAL: The current summary has severe repetition issues. You MUST eliminate all repeated sentences, phrases, and paragraphs. Each idea should be expressed only once. If you find yourself repeating content, remove the duplicates entirely. Focus on variety and uniqueness in your wording.
2. Severe repetition detected: Same phrase appears 383 times (severe repetition)

Current draft (first 2000 chars):
Okay, let's begin.**1. Start with exact title:** "Bayesian mechanics of perceptual inference and motor control in the brain"**2. EXTRACT QUOTES:**Here are10 key quotes from the paper, as requested:1.“The neurobiological mechanism of action-oriented learning through active inference.”2.“The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors state: “The authors s...

Key terms: principle, energy, free, mechanics, neural, space, perceptual, inference

=== FULL PAPER TEXT ===
1202
naJ
12
]CN.oib-q[
3v72990.8002:viXra
Bayesian mechanics of perceptual inference and
motor control in the brain
Chang Sub Kim
Department of Physics, Chonnam National University, Gwangju 61186,Republic of
Korea
E-mail: cskim@jnu.ac.kr
Abstract. The free energy principle (FEP) in the neurosciences stipulates that all
viable agents induce and minimize informational free energy in the brain to fit their
environmental niche. In this study, we continue our effort to make the FEP a more
physically principled formalism by implementing free energy minimization based on
the principle of least action. We build a Bayesian mechanics (BM) by casting the
formulationreported in the earlier publication (Kim in Neural Comput 30:2616–2659,
2018) to considering active inference beyond passive perception. The BM is a neural
implementationof variationalBayesunder the FEP in continuoustime. The resulting
BM is provided as an effective Hamilton’s equation of motion and subject to the
controlsignalarisingfromthe brain’s predictionerrorsat the proprioceptivelevel. To
demonstrate the utility of our approach, we adopt a simple agent-based model and
presentaconcretenumericalillustrationofthe brainperformingrecognitiondynamics
by integrating BM in neural phase space. Furthermore, we recapitulate the major
theoretical architectures in the FEP by comparing our approach with the common
state-space formulations.
Keywords free energy principle, Bayesian mechanics, recognition dynamics,
continuous state-space models, neural phase space, limit cycles
Bayesian mechanics of perceptual inference and motor control in the brain 2
1. Introduction
The free energy principle (FEP) in the field of neurosciences rationalizes that all viable
organisms cognize and behave in the natural world by calling forth the probabilistic
models in their neural system — the brain — in a manner that ensures their adaptive
fitness(Friston2010a). Theneurobiologicalmechanism thatendows anorganism’sbrain
— the neural observer — with this ability is theoretically framed into an inequality that
weighs two information-theoretical measures: surprisal and informational free energy
(IFE) (see, fora review, Buckley andKimet al. 2017). Thesurprisal provides a measure
of the atypicality of an environmental niche, and the IFE is the upper bound of the
surprisal. The inequality enables a cognitive agent to minimize the IFE as a variational
objective function indirectly instead of the intractable surprisal. The minimization
;
corresponds to inferring the external causes of afferent sensory data, which are encoded
as a probability density at the sensory interface, e.g., sensory organs. The brain of
an organism neurophysically performs the Bayesian computation of minimizing the
induced variational IFE; this is termed as recognition dynamics (RD), which emulates,
under the Laplace approximation (Friston et al. 2007), the predictive coding scheme of
message processing or recognition (Rao and Ballard 1999; Bogacz2017). The neuronal
self-organizationin vitro undertheFEPwasstudiedrecentlyatthelevel ofsingleneuron
responses (Isomura et al. 2015; Isomura and Friston 2018). Owing to its explanatory
power of perception, learning, and behavior of living organisms within a framework, it is
suggested a promising unified biological principle (Friston 2010a; Friston 2013; Colombo
and Wright 2018; Ramstead et al. 2018).
The neurophysical mechanisms of the abductive inference in the brain are yet
to be understood; therefore, researchers mostly rely on information-theoretic concepts
(Elfwing2016;Ramsteadetal. 2019; Kuzma2019; Shimazaki2019; Kiefer2020,Sanders
et al. 2020). The FEP facilitates dynamic causal models in the brain’s generalized-state
space (Friston 2008b; Friston et al. 2010b), which pose a mixed discrete-continuous
Bayesian filtering (Jazwinski 1970; Balaji and Friston 2011). In this work, we consider
that the brain confronts the continuous influx of stochastic sensations and conducts the
Bayesian inversion of inferring external causes in the continuous state representations.
Biological phenomena are naturally continuous spatiotemporal events; accordingly, we
suggestthatthecontinuous-stateapproachesusedtodescribecognitionandbehaviorare
better suited than discrete-state descriptions for studying the perceptual computation
in the brain.
Recently, we carefully evaluated the FEP while clarifying technical assumptions
that underlie the continuous state-space formulation of the FEP (Buckley and Kim
et al. 2017). A full account of the discrete-state formulation complementary to our
; Freeenergy(FE)isanotiondevelopedbyHermannvonHelmholtzinthermodynamics;itisaphysical
energymeasuredinjoules. The FE inthe FEP is aninformation-theoreticmeasuredefined interms of
probabilities, which serves as an objective function for variational Bayesianinference. Accordingly, we
call it variational IFE in our formulation.
Bayesian mechanics of perceptual inference and motor control in the brain 3
formulation can be found in (Da Costa et al. 2020a). In a subsequent paper (Kim
2018), we reported a different variational scheme that the Bayesian brain may utilize
in conducting inference. In particular, by postulating that “surprisal” plays the role
of a Lagrangian in theoretical mechanics (Landau and Lifshitz 1976; Sengupta et al.
2016), we worked a plausible computational implementation of the FEP by utilizing the
principleofleastaction. WebelievedthatalthoughtheFEPreliesonBayesianabductive
computation, it must be properly formulated conforming to the physical principles and
laws governing the matter comprising the brain. To this end, we proposed that any
process theory of the FEP ought to be based on the full implication of the inequality
(Kim 2018)
dt lnp ϕ dtF q ϑ ,p ϕ,ϑ , (1)
t´ p qu ď r p q p qs
ż ż
where ϕ and ϑ collectively denote the sensory inputs and environmental hidden states,
respectively. The integrand on the left-hand side (LHS) of the preceding equation
lnp ϕ is the aforementioned surprisal, which measures the “self-information”
´ p q
contained in the sensory density p ϕ (Cover 2006), and F on the right-hand side (RHS)
p q
is the variational IFE defined as
q ϑ
F q ϑ ,p ϕ,ϑ dϑq ϑ ln p q , (2)
r p q p qs ” p q p ϕ,ϑ
ż p q
which encapsulates the recognition (R-) density q ϑ and the generative (G-) density
p q
p ϑ,ϕ (Buckley and Kim et al. 2017). While the G-density represents the brain’s
p q
belief (or assumption) of sensory generation and hidden environmental dynamics, the
R-density is the brain’s current estimate of the environmental cause of the sensory
perturbation. The G- and R- densities together induce variational IFE when receptors
at the brain-environment interface are excited by sensory perturbations.
According toEq. (1), theFEParticulatesthatthebrainminimizes theupperbound
of the sensory uncertainty, which is a long-term surprisal. We identify this bound as an
informational action (IA) within the scope of the mechanical principle of least action
(LandauandLifshitz1976). Then, bycomplying withtherevised FEP, weformulatethe
Bayesian mechanics (BM) that executes the RD in the brain. The RD neurophysically
performs the computation for minimizing the IA when the neural observer encounters
continuous streams of sensory data. The advantage of our formulation is that the brain
andthe environmental states arespecified using only barecontinuous variablesand their
first-order derivatives (velocities or equivalent momenta). The momentum variables
represent prediction errors, which quantify the discrepancy between an observed input
and its top-down belief of a cognitive agent in the predictive coding language (Huang
and Rao 2011; de Gardelle et al. 2013; Kozunov et al. 2020).
The goal of this work is to cast our previous study to include the agent’s motor
control, which acts on the environment to alter sensory inputs.§ Previously, by utilizing
§ Inthiswork,weusethetermcontrol insteadofthefrequentlyusedterm“action”tomeanthemotion
of a living agent’s effectors (muscles) acting on the environment. This is done to avoid any confusion
with the term action appearing in the nomenclature of “the principle of least action”.
Bayesian mechanics of perceptual inference and motor control in the brain 4
the principle of least action, we focused on the formulation of perceptual dynamics for
the passive inference of static sensory inputs (Kim 2018) without incorporating motor
control for the active perception of nonstationary sensory streams. Here, we apply
our approach to the problem of active inference derived from the FEP (Friston et al.
2009; Friston et al. 2010c; Friston et al. 2011a), which proposes that organisms can
minimize the IFE by altering sensory observations when the outcome of perceptual
inference alone is not in accordance with the internal representation of the environment
(Buckley and Kim et al. 2017). Living systems are endowed with the ability to adjust
their sensations via proprioceptive feedback, which is attributed to an inherited trait
of all motile animals embodied in the reflex pathways (Tuthill and Azim 2018). In
this respect, motor control is considered an inference of the causes of motor signals
encoded as prediction errors at proprioceptors, and motor inference is realized at the
spinal level by classical reflex arcs (Friston 2011b; Adams et al. 2013). Our formulation
evinces time-dependent driving terms in the obtained BM, which arise from sensory
prediction errors, as control (motor) signals. Accordingly, the BM bears resemblance
to the deterministic control derived from Pontryagin’s maximum principle in optimal
control theory (Todorov 2007). In this work, we consider the agent’s locomotion for
action inference only implicitly: our formulation focuses on the implementation of the
control signal (or commands) at the neural level of description and not at the behavioral
level of biological locomotion; accordingly, the additional minimization mechanism of
the IA inferring optimal control was not explicitly handled, which is left as a future
work. There are other systematic approaches that try to relate the active inference
formalism to the existing control theories (Baltieri and Buckley 2019; Millidge et al.
2020a; Da Costa et al. 2020c).
Technically, a variation in the IA yields the BM that computes Bayesian inversion,
which is given as a set of coupled differential equations for the brain variables and their
conjugate momenta. The brain variables are ascribed to the brain’s representation of
the environmental states, and their conjugate momenta are the combined prediction
errors of the sensory data and the rate of the state representations. The neural
computation of active inference corresponds to the BM integration and is subject to
nonautonomous motor signals. The obtained solution results in optimal trajectories
in the perceptual phase space, which yields a minimum accumulation of the IFE over
continuous time, i.e., a stationary value of the IA. Our IA is identical to that of “free
action” defined in the Bayesian filtering schemes (Friston et al. 2008c). When the
minimization of free action is formulated in the generalized filtering scheme (Friston et
al. 2010b), two approaches are akin to each other such that both assume the Laplace-
encoded IFE as a mechanical Lagrangian. The difference lies in their mathematical
realizationof minimization: our approachapplies the principle of least actionin classical
mechanics, whilegeneralizedfilteringusesthegradientdescentmethodinthegeneralized
state space, where generalized states are interpreted as a solenoidal gradient flow in a
nonequilibrium steady state.
The remainder of this paper is organized as follows. In Sect. 2, we unravel some of
Bayesian mechanics of perceptual inference and motor control in the brain 5
the theoretical details in the formulation of the FEP. In Sect. 3, we formulate the BM
of the sensorimotor cycle by utilizing the principle of least action. Then, in Sect. 4, we
present a parsimonious model for a concrete manifestation of our formulation. Finally,
in Sect. 5, we provide the concluding remarks.
2. Recapitulation of technical developments
Here, we recapitulatetheoretical architectures inthecontinuous-state formulationunder
the FEP while discussing technical features that distinguish our formulation from
prevailing state-space approaches.
2.1. Perspective on generalized states
The Bayesian filtering formalism of the FEP adopts the concept of the generalized
motion of a dynamical object by defining its mechanical state beyond position and
velocity (momentum). The generalized states of motion are generated by recursively
taking time derivatives of the bare states. A point in the hyperspace defined by
the generalized states is interpreted as an instantaneous trajectory. This notion
provides an essential theoretical basis for ensuring an equilibrium solution of the RD
in the conventional formulation of the FEP (Kim 2018); it is commonly employed by
researchers (Parr and Friston 2018; Baltieri and Buckley 2019).
The motivation behind the generalized coordinates of motion is to describe noise
correlation in the generative processes beyond white noise (Wiener process), and thus,
to provide a more detailed specification of the dynamical states (Friston 2008a; Friston
2008b; Friston et al. 2010b). The mathematical theory of a quasi-Markovian process
undergirds this formulation, which describes general stochastic dynamics with a colored-
noise correlation with a finite-dimensional Markovian equation in an extended state
space by adding auxiliary variables (Pavliotis 2014). State-space augmentation in
terms of generalized coordinates may be considered a special realization of the Pavliotis
formalism. The state-extension procedure adopts some specific approximations, such
as the local linearization procedure developed in nonlinear time-series analysis (Ozaki
1992).
From the physics perspective, higher-order states possess a different dynamical
status in comparison with Newtonian mechanical states, specified only by position (bare
order) and velocity (first order). A change in the Newtonian states is caused by a force
that specifies acceleration (second order) (Landau and Lifshitz 1976). Although there
are no “generalized forces” causing the jerk (third order), snap (fourth order) etc.,
the jerk can be measured phenomenologically by observing a change in acceleration.
This induces all higher-order states to the kinematic level. Another perspective is
whether update equations in terms of generalized coordinates are equivalent to the
Pavliotis’ quasi-Markovian description. Auxiliary variables in Pavliotis’ analysis are not
generated by the recursive temporal derivatives of a bare state. The generalized phase
Bayesian mechanics of perceptual inference and motor control in the brain 6
space considered in (Kerr and Graham 2000) is also spanned in terms of canonical
displacement and momentum variables. A further in-depth analysis is required.
Our formulation does not employ the generalized states, but instead, it follows
the normative rules in specifying generative models (Kim 2018). The derived BM
performs the brain’s Bayesian inference in terms of only the bare brain variable and its
conjugate momentum in phase space, and not in an extended state space. Accordingly,
our formulation is restricted to the white noise in the generative processes; however,
it provides a natural approach to determine the equilibrium solutions of the BM (see
Sect. 4). Forthegeneralbrainmodelsdescribed bymanybrainvariables, thebrain’sBM
can be set up in multi-dimensional phase space, which is distinctive from the state-space
augmentation in the generalized coordinate formulation (see Sect. 2.3).
2.2. Continuous state implementation of recognition dynamics (RD)
The conventional FEPemploys thegradient-descent minimization ofthevariational IFE
by the brain’s internal states. To incorporate the time-varying feature of sensory inputs,
the method distinguishes the path of a mode and the mode of a path in the generalized
state space (Friston 2008b; Friston et al. 2008c; Friston et al. 2010b). This theoretical
construct intuitively considers the nonequilibrium dynamics of generalized brain states
as drift-diffusion flows that locally conserve the ensemble density in the hyperspace of
the generalized states (Friston and Ao 2012b; Friston 2019).
Mathematically, the gradient descent formulation is based on the general idea for a
fast and efficient convergence and it ensures that formulations reach a sophisticate level
by incorporating the Riemannian metric in information geometry (Amari 1998; Surace
et al. 2020); the idea is applied to the FEP (Sengupta and Friston 2017, Da Costa et
al. 2020b).
In our proposed formulation, we replace the gradient descent scheme with the
standard mechanical formulation of the least action principle (Kim 2018). However,
there is a disadvantage in that we incorporate only the Gaussian white noise in the
generative processes of the sensory data and environmental dynamics [see Sect. 2.3].
The resulting novel RD described by an effective Hamiltonian mechanics entails optimal
trajectoriesbutnosinglefixedpointsinthecanonicalstate(phase)space, whichprovides
an estimate of the minimum sensory uncertainty, i.e., the average surprisal over a finite
temporal horizon. The phase space comprises the positions (predictions) and momenta
(prediction errors) of the brain’s representations of the causal environment.
Our implementation of the minimization procedure is an alternative to the gradient
descent algorithms in the FEP. A crucial difference between the two approaches is that
while the gradient descent scheme searches for an instantaneous trajectory representing
a local minimum on the IFE landscape in the multidimensional generalized state space,
our theory determines an optimal trajectory minimizing the continuous-time integral of
the IFE in two-dimensional phase space for a single variable problem.
Bayesian mechanics of perceptual inference and motor control in the brain 7
2.3. Treatment of noise correlations
The FEP requires the brain’s internal model of the G-density p ϕ,ϑ encapsulating
p q
the likelihood p ϕ ϑ and prior p ϑ . The likelihood density is determined by the
p | q p q
random fluctuation in the expected sensory-data generation, and the prior density is
determined by that in the believed environmental dynamics. The brain encounters
sensory signals on a timescale, which is often shorter than the correlation time of the
random processes (Friston 2008a); accordingly, in general, the noises embrace a non-
Markovian stochastic process with an intrinsic temporal correlation that surmounts the
ideal white-noise stochasticity. Conventional formulations (Friston 2008b; Friston et al.
2010b) consider that colored noises are analytic (i.e., differentiable) to allow correlation
between the distinct dynamical orders of the continuous states. In practice, to furnish
a closed dynamics for a finite number of variables, the recursive equations of motion for
the continued generalized states need to be truncated at an arbitrary embedding order.
Our formulation considers the BM in the brain in terms of the standard Newtonian
(Hamiltonian) construct; the drawback is that our theory does not explore the nature
of temporal correlation in the assumed Gaussian noises in the generative processes.
Accordingly, our generative models assume and account for the white noise describing
the Wiener processes. The delta-correlated white noise is mathematically singular; they
need to be smoothed to describe fast biophysical processes. There are approaches in
stochastic theories that formulate non-Markovian processes with colored noises without
resorting to generalized states of motion (van Kampen:1981; Fox 1987; Risken 1989;
Moon and Wettlaufer 2014), which are not discussed here.
Instead, we discuss an approach to extend the phase-space dimension for the white
noise processes. At the level of the Hodgkin–Huxley description of the biophysical
brain dynamics, the membrane potential, gating variables, and ionic concentrations are
relevant coarse-grainedbrainvariables(Hille 2001). Thus, ifone employs thefluctuating
Hodgkin-Huxley models with Gaussian white noises as neurophysically plausible
generative models (Kim 2018), one can proceed with our Lagrangian (equivalently,
Hamiltonian) approach to formulate the RD in an extended phase space. Such a state-
space augmentation is different from and alternative to that in terms of the generalized
coordinates of motion [see Sect. 2.1], while accommodating only delta-correlated noises.
2.4. Lagrangian formulation of Bayesian mechanics (BM)
The (classical) “action” is defined as an ordinary time-integral of the Lagrangian
for an arbitrary trajectory (Landau and Lifshitz 1976). Our formulation of the BM
proposes the Laplace-encoded IFE — an upper bound on the sensory surprisal — as
an informational Lagrangian and hypothesizes the time-integral of the IFE — an upper
bound on the sensory Shannon uncertainty — as an informational action (IA). By
applying the principle of least action, we minimize the IA to find a tight bound for the
sensory uncertainty and derive the BM that performs the brain’s Bayesian inference of
the external cause of sensory data. In turn, we cast the working BM in our formulation
Bayesian mechanics of perceptual inference and motor control in the brain 8
as effective Hamilton’s equations of motion in terms of position and momentum in phase
space.
Meanwhile, the BM described in (Friston 2019) intuitively adopts the idea of
Feynman’s path integral formulation (Feynman and Hibbs 2005). The Feynman’s
path integral formulation extends the idea of classical action to quantum dynamics
and provides an approach to determine the “propagator” that specifies the transition
probability between initial and final states. The propagator is defined as a functional
integral oftheexponentiated action, whichsummates allpossibletrajectoriesconnecting
initial and final states. The description provided in (Friston 2019) identifies the
propagator using the probability density over neural states, and it makes the connection
to the Bayesian FEP. In this manner, the surprisal may be identified as a negative log of
the steady-state density in nonequilibrium ensemble dynamics (Parr et al. 2020), which
is governed by a Fokker-Plank equation. The generalized Bayesian filtering scheme
(Friston et al. 2008c; Friston et al. 2010b) provides a continuous-state formulation of
minimizing the surprisal, and it delivers the BM in terms of the generalized coordinates
of motion using the concept of gradient flow.
In some technical details, the Lagrangian presented in (Friston 2019), which is
the integrand in the classical action, encloses two terms. They are the quadratic term
arising from the state equation and the term involving a state-derivative (divergence
in three dimension) of the force, which appears in the Langevin-type state equation.
The former term is included in our Lagrangian but with an additional quadratic term
from the observation equation. In contrast, the latter is not present in our Lagrangian,
which is known to arise from the Stratonovich convention (Seifert 2012; Cugliandolo
and Lecomte 2017).
2.5. Closure of the sensorimotor loop in active inference
The conventional FEP facilitates gradient descent minimization for the mechanistic
implementation of active inference, which makes the motor-control dynamics available
in the brain’s RD (Friston et al. 2009; Friston et al. 2010c; Friston et al. 2011a). The
gradient-descent scheme is mathematically expressed as
F dϕ
a9 ∇ F B , (3)
a
“ ´ Ñ ´ ϕ da
B
where a denotes an agent’s motor variable, and F represents the Laplace-encoded
IFE by the biophysical brain variables (Buckley and Kim et al. 2017). An agent’s
capability of subjecting sensory inputs to motor control is considered a functional
dependence ϕ ϕ a in the environmental generative processes (Friston et al. 2009).
“ p q
According to Eq. (3), an agent performs the minimization by effectuating the sensory
data dϕ da and obtains the best result for motor inference when a9 0, where the
{ “
condition F ϕ 0 must be met. Because F ϕ produces terms proportional
B {B “ B {B
to the sensory prediction errors, the fulfillment of motor inference is equivalent to
suppressing proprioceptive errors. Thus, motor control attempts to minimize prediction
Bayesian mechanics of perceptual inference and motor control in the brain 9
errors, while prediction errors convey motor signals for control dynamics; this forms a
sensorimotor loop. Some subtle questions arise here regarding the dynamical status of
the motor-control variable a: Equation (3) evidently handles a as a dynamical state;
however, the corresponding equation of motion governing its dynamics is not given in
the environmental processes. Instead, the mechanism of motor control that vicariously
alters the sensory-data generation is presumed (Friston et al. 2009). In addition,
motor variables are represented as the active states of the brain, e.g., motor-neuron
activities in the ventral horn of the spinal cord (Friston et al. 2010c); however, they
are treated differently from other hidden-state representations. Recall that the internal
state representations are expressed as generalized states, whereas the active states are
not.
In the following, we pose a semi-active inference problem that does not explicitly
address optimal motor control (motor inference) in the RD but encompasses the motor-
control signal as a time-dependent driving term arising from nonstationary prediction
errors in the sensory-data cause.
3. Closed-loop dynamics of perception and motor control
The brain is not divided into sensory and motor systems. Instead, it is one inference
machine that performs the closed-loop dynamics of perception and motor control. Here,
we develop a framework of active inference within the scope of the least action principle
by employing the Laplace-encoded IFE as an informational Lagrangian.
The environmental states ϑ undergo deterministic or stochastic dynamics by
obeying physical laws and principles. Here, we do not explicitly consider their equations
of motion because they are hidden from the brain’s perspective, i.e., the brain as a
neural observer does not possess direct epistemic access. Similarly, sensory data ϕ
are physically generated by an externally hidden process at a sensory receptor, which
constitutes the brain-environment interface. However, to emphasize the effect of an
agent’s motor control a on sensory generation, we facilitate the generative process of
sensory data using an instantaneous mapping
ϕ h ϑ,a z , (4)
gp
“ p q`
where h ϑ,a denotes thelinear or nonlinear mapof input generation, andz represents
gp
p q
the noise involved. Note that an agent’s motor-control a is explicitly included in the
generative map. However, the neural observer is not aware of how the sensory streams
are effectuated by the agent’s motion in the environment (Friston et al. 2010c).
The FEP circumvents this epistemic difficulty by hypothesizing a formal homology
between external physical processes and the corresponding internal models foreseen by
the neural observer (Friston et al. 2010c). Upon receiving sensory-data influx, the
brain launches R-density q ϑ to infer the external causes via variational Bayes. The R-
p q
density is the probabilistic representation of the environment, whose sufficient statistics
are assumed to be encoded by neurophysical brain variables, e.g., neuronal activity or
Bayesian mechanics of perceptual inference and motor control in the brain 10
synaptic efficacy. When a fixed-form Gaussian density is considered for the R-density,
which is called Laplace approximation, only the first-order sufficient statistic, i.e., the
mean µ is needed to specify the IFE effectively (Buckley and Kim et al. 2017). The
brain continually updates the R-density using its internal dynamics, described here as
a Langevin-type equation
dµ
f µ w, (5)
dt “ p q`
where f µ represents the brain’s belief regarding the external dynamics encoded by
p q
a neurophysical driving mechanism of the brain variables µ, and w is random noise.
The sensory perturbations at the receptors are predicted by the neural observer via the
instantaneous mapping
ϕ a g µ z, (6)
p q “ p q`
where the belief g µ is encoded by the internal variables, and z is the associated noise.
p q
Our sensory generative model provides a mechanism for sampling sensory data ϕ using
the brain’s active states a, which represent an external motor control embedded in
Eq. (4). Note that Eq. (4) describes the environmental processes that generate sensory
inputs ϕ, while its homolog “Eq. (6)” prescribes the brains’ prior belief of ϕ that can be
altered by the active states a. The instantaneous state of the brain µ, which is specified
by Eq. (5), selects a particular R-density q ϑ when the brain seeks the true posterior
p q
(the goal of perceptual inference). The motor control fulfills the prior expectations by
modifying the sensory generation via active-state effectuation at the proprioceptors.
Through Laplace approximation (Buckley and Kim et al. 2017), the G-density
p ϕ,ϑ is encoded in the brain as p p ϕ,µ , where the sensory stimuli ϕ are predicted
p q “ p q
by the neural observer µ via Eq. (6). Here, we argue that the physical sensory-recording
process is conditionally independent of thebrain’s internal dynamics; however, the brain
states must be neurophysically involved in computing the sensory prediction. In other
words, from the physics perspective, the sensory perturbation ϕ at the interface is a
source for exciting the neuronal activity µ. This observation renders the set of Eqs. (5)
and (6) to be dynamically coupled, and not conditionally independent. We incorporate
this conditional dependence into our formulation by introducing a statistical coupling
via the covariance connection between the likelihood p ϕ µ andprior p µ that together
p | q p q
furnish the Laplace-encoded G-density.
For simplicity, we consider the stationary Gaussian processes for the bivariate
variable Z as a column vector
w
Z ,
” z
˜ ¸
where w µ9 f µ and z ϕ g µ , and we specify the Laplace-encoded G-density
“ ´ p q “ ´ p q
p ϕ,µ p ϕ µ p µ as
p q “ p | q p q
1 1
p ϕ,µ exp
ZTΣ´1
Z , (7)
p q “ 2π 2 Σ ´2
p q | | ˆ ˙
a
Bayesian mechanics of perceptual inference and motor control in the brain 11
where Σ and
Σ´1
are the determinant and the inverse of the matrix Σ, respectively;
| |
ZT is the transpose of Z. The covariance matrix Σ for the above is given as
σ φ t
Σ w p q ,
“ φ t σ
˜ z ¸
p q
where the stationary variances σ (i w,z) and the transient covariance φ are defined,
i
“
respectively, as
2 2
σ 0 w , σ 0 z , and φ t w 0 z t .
w z
p q “ x y p q “ x y p q “ x p q p qy
With the prescribed internal model of the brain for the G-density, the Laplace-encoded
IFE can be specified as F ϕ,µ lnp ϕ,µ (for details, see Buckley and Kim et al.
p q “ ´ p q
2017). Then, it follows that
1 1
F ϕ,µ;t m µ9 f µ 2 m ϕ g µ 2 (8)
w z
p q “ 2 p ´ p qq ` 2 p ´ p qq
?m m ρ µ9 f µ ϕ g µ
w z
´ p ´ p qqp ´ p qq
1
2
ln 2π 1 ρ σ σ ,
w z
` 2 p ´ q
where ρ denotes the correlation fu`nction defined a˘s a normalized covariance
φ
ρ . (9)
” ?σ σ
w z
Furthermore, we introduce notations m i w,z as
i
p “ q
1
m , (10)
i ” σ 1 ρ2
i
p ´ q
which are precisions, scaled by the correlation in the conventional FEP.
Next, as proposed in (Kim 2018), we identify F as an informational Lagrangian L
within the scope of the principle of least action, and we define
1 1
L m µ9 f µ 2 m ϕ g µ 2
w z
” 2 p ´ p qq ` 2 p ´ p qq
?m m ρ µ9 f µ ϕ g µ , (11)
w z
´ p ´ p qqp ´ p qq
which is viewed as a function of µ and µ9 for the given sensory inputs ϕ t , i.e.,
p q
L L µ,µ9 ;ϕ . Note that we dropped the last term in Eq. (8) when translating F
“ p q
into L because it can be expressed as a total time-derivative term that does not affect
the resulting equations of motion (Landau and Lifshitz 1976). Then, the theoretical
action S that effectuates the variational objective functional under the revised FEP is
set up as
S µ t F µ t ,µ9 t ;ϕ dt. (12)
r p qs “ p p q p q q
ż
The Euler-Lagrange equation of motion, which determines the trajectory µ µ t for
“ p q
a given initial condition µ 0 , is derived by minimizing the action δS 0.
p q ”
Equivalently, the equations of motion can be considered in terms of the position
µ and its conjugate momentum p, instead of the position µ and velocity µ9 . We used
the terms position and velocity as a metaphor to indicate dynamical variables µ and
Bayesian mechanics of perceptual inference and motor control in the brain 12
µ9 , respectively. For this, we need to convert Lagrangian L into Hamiltonian H by
performing a Legendre transformation
H µ,p pµ9 L µ,µ9 ,
p q “ ´ p q
where p denotes the canonical momentum conjugate to µ, which is calculated from L as
L
p B m µ9 f ?m m ϕ g ρ. (13)
“
µ9
“
w
p ´ q´
w z
p ´ q
B
After some manipulation, the functional form of H can be obtained explicitly as
H µ,p;ϕ T µ,p;ϕ V µ;ϕ , (14)
p q “ p q` p q
where we indicated its dependence on the sensory influx ϕ. In addition, the terms T
and V on the RHS are defined as
p2 m
z
T ρ ϕ g µ f µ p, (15)
” 2m ` m ´ p q ` p q
w ˆ c w ˙
´ ¯
1 2
2
V m ρ 1 ϕ g µ . (16)
z
” 2 ´ ´ p q
´ ¯
Here, T and V represen`t the k˘inetic and potential energies, respectively, which define
the informational Hamiltonian of the brain. Similarly, m and m represent the neural
w z
inertial masse as a metaphor. Unlike that in standard mechanics, the second term in
the expression for kinetic energy is dependent on linear momentum and position.
We generate the Hamilton equations of motion, which are equivalent to the
Lagrange equation using
H H
µ9 B and p9 B .
“ p “ ´ µ
B B
As described below, Hamilton’s equations are better suited for our purposes because
they specify the RD as coupled first-order differential equations of the brain state µ
and its conjugate momentum p. In contrast, the Lagrange equation is a second-order
differential equation of the state variable (Landau and Lifshitz 1976). The results are
1
µ9 p f µ α∆ , (17)
ϕ
“ m ` p q`
w
f g g
p9 B βB p 1 γ 2 B ∆ , (18)
ϕ
“ ´ µ ´ µ ´ ´ µ
ˆB B ˙ B
where parameters α, β, and γ have been`respec˘tively defined for notational convenience
as
ρ
α , β m α, and γ ρ?κ, (19)
z
” ?m m ” ”
z w
where κ denotes the tuning parameter to spawn stability. In Eqs. (17) and (18), we
defined the notation ∆ as
ϕ
∆ µ;t m ϕ a g µ . (20)
ϕ z
p q ” p p q´ p qq
It measures the discrepancy between the adjustable sensory input ϕ by anagent’s motor
control a and the top-down neural prediction g µ , weighted by the neural inertial mass
p q
m .
z
Bayesian mechanics of perceptual inference and motor control in the brain 13
Below, we appraise the BM prescribed by Eqs. (17) and (18) and note some
significant aspects:
(i) The derived RD suggests that both brain activities µ and their conjugate momenta
p are dynamic variables. The instantaneous values of µ and p correspond to a point
in the brain’s perceptual phase space, and the continuous solution over a temporal
horizon forms an optimal trajectory that minimizes the theoretical action, which
represents sensory uncertainty.
(ii) ThecanonicalmomentumpdefinedinEq.(13)canberewrittenasp m µ9 f
w
“ p ´ q´
ρ m m ∆ . Accordingly, when the normalized correlation ρ is nonvanishing,
w z ϕ
{
the momentum quantifies combined errors in predicting changing states and
a
sensory stimuli. Prediction errors propagate through the brain by obeying coupled
dynamics according to Eqs. (17) and (18).
(iii) Terms involving time-dependent ∆ in Eqs. (17) and (18) are identified as driving
ϕ
forces C , i µ, p,
i
“
C α∆ , (21)
µ ϕ
”
g
2
C 1 γ B ∆ . (22)
p ϕ
” ´ ´ µ
B
` ˘
The sensory prediction error ∆ defined in Eq. (20) quantifies motor signals
ϕ
engaging the brain’s nervous control in integrating the RD.
Equations (17) and (18) are the highlights of our formulation, which prescribe
the brain’s BM of semi-actively inferring the external causes of sensory inputs under
the revised FEP. Note that the motor variable a is not explicitly included in our
derived RD; instead, it implicitly induces nonautonomous sensory inputs ϕ t in
p q
the motor signal ∆ . The motor signal appears as a time-dependent driving force;
ϕ
accordingly, our Hamiltonian formulation bears a resemblance to the motor-control
dynamics described by the Hamilton-Jacobi-Bellman (HJB) equation in the control
theory (Todorov 2007). If one regards the Lagrangian Eq. (11) as a negative cost rate
and the canonical momentum p as a costate, our IA is equivalent to the total cost
function that generates the continuous-state HJB equations. In optimal control theory,
the associated Hamiltonian function is further minimized with respect to the control
signal, which we do not explicitly consider in this work. In our formulation, the motor
signals are produced by the discrepancy between the sensory streams ϕ t and those
p q
predictedbythebrain. Thenonstationarydataarepresentedtoasensorimotorreceptor,
whose field position in the environment is specified by the agent’s locomotive motion.
Theneural observer continuously integratestheBMsubject toamotorsignaltoperform
thesensory-uncertainty minimization, therebyclosingtheperceptionandmotioncontrol
within a reflex arc. When we neglect the correlation ρ between the sensory prediction
modeled by Eq. (6) and the internal dynamics of predicting the neuronal state modeled
by Eq. (5), we can recover the RD reported in the previous publication (Kim 2018),
which demonstrates the consistency of our formulation.
Bayesian mechanics of perceptual inference and motor control in the brain 14
In the present treatment, we consider only a single brain variable µ; accordingly,
the ensuing BM specified by Eqs. (17) and (18) is described in a two-dimensional phase
space. The extension of our formulation to the general case of the multivariate brain
is possible by applying the same line of work proposed in (Kim 2018). Under the
independent-particle approximation, the multivariate Lagrangian takes the form
1 N 2 2
L m µ9 f µ m ϕ g µ
wα α α zα α α
” 2 ´ pt uq ` ´ pt uq
α
ÿ
“1
” ´ ¯ ´ ¯
2ρ ?m m µ9 f µ ϕ g µ , (23)
α wα zα α α α α
´ ´ pt uq ´ pt uq
´ ¯´ ¯ı
where µ µ 1 ,µ 2 , ,µ N denotes a row vector of N brain states that respond to
t u “ p ¨¨¨ q
multiple sensory inputs ϕ ϕ 1 ,ϕ 2 , ,ϕ N in a general manner. Note that our
t u “ p ¨¨¨ q
proposed multivariate formulation is different from the state-space augmentation using
the higher-order states [see Sect. 2.1]. In our case, multiple brain states are, for instance,
the membrane potential, gating variables, and ionic concentrations, that can be viewed
as the fluctuating variables on a corase-grained time scale, influnced by Gaussian white
noises [see Sect. 2.3].
Furthermore, the implication of our formulation in the hierarchical brain can be
achieved in a straightforward manner as in (Kim 2018), which adopts the bidirectional
facet in information flow of descending predictions and ascending prediction errors
(Markov and Kennedy 2013; Michalareas et al. 2016). Note that in ensuing
formulation, both descending predictions and ascending prediction errors will constitute
the dynamical states governed by the closed-loop RD in the functional architecture
of the brain’s sensorimotor system. This feature is in contrast to the conventional
implementationoftheFEP,whichdeliversthebackwardprediction—beliefpropagation
— as neural dynamics and the forward prediction error as an instant message passing
without causal dynamics (Friston 2010a; Buckley and Kim et al. 2017).
4. Simple Bayesian-agent model: Implicit motor control
Inthissection, wenumerically demonstrate theutilityofourformulationusinganagent-
based model, which is based on a previous publication (Buckley and Kim et al. 2017).
Unlike that in the previous study, the current model does not employ generalized states
and their motions; instead, the RD is specified using only position µ and its conjugate
momentum p for incoming sensory data ϕ. Environmental objects invoking an agent’s
sensations can be either static or time dependent, and in turn, the time dependence
can be either stationary (not moving on average) or nonstationary. According to the
framework of active inference, the inference of static properties corresponds to passive
perception without motorcontrol a. Meanwhile, theinference oftime-varying properties
renders an agent’s active perception of proprioceptive sensations by discharging motor
signals ∆ via classic reflex arcs.
ϕ
In the present simulation, the external hidden state ϑ is a point property, e.g.,
temperature or a salient visual feature, which varies with the field point x. As the
Bayesian mechanics of perceptual inference and motor control in the brain 15
φ(t)
20
15
10
5
t
0 1 2 3 4 5
Figure1. Influxofstochasticsensorydataϕptqinthebluecurvewasgeneratedbythe
environmental process shown in Eq. (24), which instantly enters the sensory receptor
locatedatthefieldpointxptq. Thedashedcurverepresentstheagent’spositionatxptq
as a function of time, with its movement starting from xp0q “ 10. The dotted curve
represents the magnitude of the latent motor variable aptq that controls the agent’s
location. [All curves are in arbitrary units.]
simplest environmental map, we consider h ϑ,a ϑ x a and assume that the sensory
p q “ p p qq
influx at the corresponding receptor is given by
ϕ ϑ z , (24)
gp
“ `
where z denotes the random fluctuation. The external property, e.g., temperature, is
gp
assumed to display a spatial profile as
2
ϑ x ϑ 0 x 1 ,
p q “ {p ` q
where ϑ 0 denotes the value at the field origin, and the desired environmental niche is
situated at x x , where ϑ x ϑ . The biological agent that senses temperature
d d d
“ p q “
is allowed to navigate through a one-dimensional environment by exploiting the hidden
property. The agent initiates its motion from x 0 , where the temperature does not
p q
accord with the desired value. In this case, the agent must fulfill its allostasis at the
cost of biochemical energy by exploiting the environment based on
t
x t x 0 a t1 dt1, (25)
p q “ p q` p q
0
ż
where a t denotes a motor variable, e.g., agent’s velocity. The nonstationary sensory
p q
data ϕ t are afferent at the receptor subject to noise z ; its time dependence is caused
gp
p q
by the agent’s own motion, i.e., ϕ t ϑ x a t , which is assumed to be latent to the
p q “ p p p qqq
agent’s brain in the current model. With the prescribed sensorimotor control, the rate
of sensory data averaged over the noise is related to the control variable as
ϕ x
ϕ9 B p qa.
“ x
B
The neural observer is not aware of how sensory inputs at the proprioceptor are
affected by the motor reflex control of the agent. In the case of saccadic motor control
Bayesian mechanics of perceptual inference and motor control in the brain 16
(Friston et al. 2012b), an agent may stand at a field point without changing its position;
however, sampling the salient visual features of the environment through a fast eye
movement a t makes the visual input nonstationary, i.e., ϕ t ϑ a t .
p q p q “ p p qq
In Fig. 1, we depict streams of sensory data at the agent’s receptor as a function of
time. For this simulation, the latent motor variable in Eq. (25) is considered as
et
a t a 0 ,
p q “ p q 1 et 2
p ` q
which renders the agent’s position in the environment as x t 2x 0 1 et with
p q “ p q{p ` q
x 0 2a 0 . For simplicity, we assume that this is hardwired in the agent’s reflex
p q “ ´ p q
pathway over evolutionary and developmental time scales. The figure shows that
the agent, initially located at x 0 10, senses an undesirable stimulus ϑ 0 0.2;
p q “ p q “
accordingly, it reacts by using motor control to determine an acceptable ambient niche.
For this illustration, we assumed the environmental property at theorigin to beϑ 0 20.
“
After a period of ∆t 5, the agent finds itself at the origin x 0, where the
“ “
environmental state is marked by the value ϑ 20.
“
Having prescribed the nonstationary sensory data, we now set up the BM to be
integrated by applying Eqs. (17) and (18) to the generative models below. We assume
that the agent has already learned an optimal generative model; therefore, the agent
retains prior expectations regarding the observations and dynamics. Here, for the
demonstration, we consider the learned generative model in its simplest linear form
g µ µ, (26)
p q “
f µ µ ϑ . (27)
d
p q “ ´p ´ q
Note that the motor control a is not included in the generative model, and the desired
sensory data ϑ , e.g., temperature, appear as the brain’s prior belief of the hidden state.
d
Accordingly, Eqs. (17) and (18) are reduced to a coupled set of differential equations for
the brain variable µ and its conjugate momentum p as
1
µ9 µ ϑ p α∆ , (28)
d ϕ
“ ´p ´ q` m `
w
p9 1 β p 1 γ 2 ∆ . (29)
ϕ
“ p ` q ´ ´
Parameters α, β, and γ are pr`oportio˘nal to the correlation ρ; see Eq. (19). Hence, they
become zero when the neural response to the sensory inputs is uncorrelated with neural
dynamics, which is not the case in general. Time-dependent driving terms appearing on
the RHS of both equations, namely Eqs. (28) and (29), include the sensorimotor signal
∆ µ;ϕ t given in Eq. (20). The motor variable a, which drives the nonstationary
ϕ
p p qq
inputs ϕ t , is unknown to the neural observer in our implementation.
p q
In the following, for a compact mathematical description, we denote the brain’s
perceptual state as a column vector
µ
Ψ .
” p
˜ ¸
Bayesian mechanics of perceptual inference and motor control in the brain 17
p
(t)
-150 -100 -50
50 -100
-200
t
10 20 30 40
-300
-400
-50
-500
-600
Figure 2. Perceptual inference of static sensory data: (Left) Oscillatory brain
variablesµ“µptqintimetdevelopedfromacommonspontaneousstatepµp0q,pp0qq“
p0,0q by responding to sensory inputs ϕ “ 10 (gray), 15 (red), and 20 (blue). The
horizontaldottedlineindicatestheagent’spriorbeliefregardingsensoryinput. (Right)
Limit cycles in the perceptual state space from an input ϕ “ 4.0 for three initial
conditions pµp0q,pp0qq“ p0,0q, p´20,´100q, and p´40,´200q; where the state space
is spanned by continuous brain state µ and its conjugate momentum p variables. The
common fixed point is indicated by an orangebullet at the center of the orbits, which
predicts the sensory cause incorrectly. [All curves are in arbitrary units.]
VectorΨrepresents thebrain’scurrent expectationµandtheassociatedpredictionerror
p with respect to the sensory causes, as encoded by the neuronal activities performed
whenencounteringasensoryinflux. Therefore, intermsofperceptualvectorΨ,Eqs.(28)
and (28) are expressed as
dΨ
RΨ S, (30)
dt ` “
where relaxation matrix R is defined as
1
1 β
R ` ´mw , (31)
“ γ2 1 m 1 β
˜ z ¸
p ´ q ´p ` q
and source vector S encompassing the sensory influx ϕ t is defined as
p q
ϑ βϕ
S d ` . (32)
“ γ2 1 m ϕ
˜ z ¸
p ´ q
Unless it is a pathological case, the steady-state (or equilibrium) solution ψ of Eq. (30)
eq
is uniquely obtained as
µ
Ψ R´1 S eq . (33)
eq
“ ” p
˜ eq ¸
We find it informative to consider the general solution Ψ t of Eq. (30) with respect to
p q
the fixed point ψ by setting
eq
ψ t Ψ t Ψ .
eq
p q ” p q´
To this end, we seek time-dependent solutions for the shifted measure ψ t as follows
p q
dψ
Rψ δS,
dt ` “
Bayesian mechanics of perceptual inference and motor control in the brain 18
where δS S t S . It is straightforward to integrate the above inhomogeneous
“ p q ´ p8q
differential equation to obtain a formal solution, which is given by
t
ψ t e´Rtψ 0 dt1e´Rpt´t1qδS t1 . (34)
p q “ p q` p q
0
ż
Note that δS becomes zero identically for static sensory inputs; therefore, the relaxation
admits simple homogeneous dynamics. In contrast, for time-varying sensory inputs, the
inhomogeneous dynamics driven by the source term is expected to be predominant.
However, on time scales longer than the sensory-influx saturation time τ, it can be
shown that δS 0; for instance, τ 5 in Fig. 1. Therefore, for such a time scale, the
Ñ “
inhomogeneous contribution in the relaxation diminishes even for time-varying sensory
inputs, and the homogeneous contribution is dominant for further time-development.
The ensuing homogeneous relaxation can be expressed in terms of eigenvalues λ and
l
eigenvectors ξplq of the relaxation matrix R as
2
ψ t c e´λltξplq, (35)
l
p q “
l“1
ÿ
where expansion coefficients c arefixed by initial conditions ψ 0 . The initial conditions
l
p q
ψ 0 represent a spontaneous or resting cognitive state. In Eq. (35), eigenvalues and
p q
eigenvectors are determined by the secular equation
Rξplq λ ξplq. (36)
l
“
Then, the solution for the linear RD Eq. (30) is given by
2
Ψ t Ψ c e´λltξplq, (37)
eq l
p q “ `
l“1
ÿ
which is exact for perceptual inference, and legitimate for active inference on timescales
t τ.
ą
Before presenting the numerical outcome, we first inspect the nature of fixed points
by analyzing the eigenvalues of the relaxation matrix R given in Eq. (31). First, it
can be seen that the trace of R is zero, which indicates that the two eigenvalues have
opposite signs, i.e., λ 1 λ 2. Second, the determinant of R can be calculated as
“ ´
m
2 2 z
Det R 1 β γ 1 .
p q “ ´p ` q ` ´ m
w
Therefore, if the correlation φ 0,`it can ˘be conjectured that both eigenvalues are
Ñ
real. This is because Det R λ 1 λ 2 1 m z m w 0, which yields λ2 1 λ2 2 0
p q “ Ñ ´ ´ { ă “ ą
using the first conjecture. Thus, we can conclude that the two eigenvalues are real and
have opposite signs. Therefore, for φ 0, the solution is unstable. In contrast, when
“
the correlation is retained, Det R can be positive for a suitable choice of statistical
p q
parameters, namely m w , m z , and φ. In the latter case, the condition λ 1 λ 2 0 renders
ą
λ2 l ă 0 for both l “ 1,2. Accordingly, λ 1 and λ 2 that have opposite signs are purely
imaginary, which makes the fixed point Ψ a center (Strogatz 2015). If we define
eq
λ 1,2 iω, the long-time solution of RD with respect to Ψ eq is expressed as
” ˘
ψ t c
eiωtξp1q
c
e´iωtξp2q,
1 2
p q “ `
Bayesian mechanics of perceptual inference and motor control in the brain 19
which specifies a limit cycle with angular frequency ω. Thus, according to our
formulation, the effect of correlation on the brain’s RD is not a subsidiary but a crucial
component. Below, we consider numerical illustrations with finite correlation.
We exploited a wide range of parameters for numerically solving Eqs. (28) and
(29) and found through numerical observation that there exists a narrow window in the
statistical parameters σ , σ , and φ, within which a stable trajectory is allowed for a
w z
successful inference. This finding implies that the agent’s brain must learn and hardwire
this narrow parameter range over evolutionary and developmental timescales; namely,
generative models are conditioned on an individual biological agent. We denote the
instantaneous cognitive state as µ t ,p t for notational convenience.
p p q p qq
In Fig. 2, we depict the numerical outcome from the perceptual inference of static
sensory inputs. To obtain the results, we select a particular set of statistical parameters
as
σ 1.0, σ 10, and φ 2.8,
w z
“ “ “ ´
which specify the neural inertial masses
m 4.6 and m 0.1 m
w z w
“ “ ˆ
and the coefficients that enter the RD, namely
α 0.60,β 0.28,and γ 2.8 κ 10 .
“ ´ “ ´ “ ´ p “ q
In Fig. 2Left, we depict the brain variable µ as a function of time, which represents the
cognitive expectation of a registered sensory input under the generative model [Eq. (26)]
for three values, namely ϕ 10, 15,and 20. For all illustrations, the agent’s prior belief
“
with regard to the sensory input is set as
ϑ 20,
d
“
which is indicated by the horizontal dotted line. The blue curve represents the case in
which sensory data are in line with the belief. The RD of the perceptual inference
delivers an exact output µ ,p 20,0 ; where µ and p are the perceptual
eq eq eq eq
p q “ p q
outcome of the sensory cause and its prediction error, respectively. Note that µ and
eq
p correspond to the temporal averages of µ t and p t , respectively. The other two
eq
p q p q
inferences underscore the correct answer. Figure 2Right corresponds to the case of a
single sensory data ϕ 4.0, which the standing agent senses at the field point x 2.
“ “
The ensuing trajectories from all three initial spontaneous states have their limit cycles
in the state space defined by µ and p. We numerically determined the fixed point to
be µ ,p 65.6, 306 and the two eigenvalues of the relaxation matrix R to be
eq eq
p q “ p´ ´ q
λ 1 ,λ 2 1.84i, 1.84i , which are purely imaginary and have opposite signs. Again,
p q “ p ´ q
the perceptual outcome does not accord with the sensory input; it deviates significantly.
Next, in Fig. 3, we depict the results for active inference, which were calculated
using the same generative parameters used in Fig. 2. The agent is initially situated at
x 0 2, where it senses the sensory influx ϑ 0 4, which does not match the desired
p q “ p q “
value ϑ 20. Therefore, the agent reacts to identify a comfortable environmental
d
“
Bayesian mechanics of perceptual inference and motor control in the brain 20
30
20
10
t
10 20 30 40
-10
-20
Figure 3. Active perception: Time-development of the perceptual state inferring
the external causes of sensory inputs altered by the agent’s motor control. Blue
and magenta curves depict the brain activity µptq and correspondingmomentum pptq,
respectively. In addition, the noisy curve indicates the nonstationary sensory inputs
ϕptq entering the sensory receptor at instant t. For numerical illustration, we used
σ “1.0, σ “10, and φ“´2.8. [All curves are in arbitrary units.]
w z
p
300
200
100
μ
-50 50 100
-100
-200
-300
Figure4. Activeinference: Temporaldevelopmentoftrajectoriesrenderingstationary
limit cycles in the perceptual phase space, spanned by continuous neural state µ and
its conjugate momentum p variables. Data were obtained from the same statistical
parameters used in Fig. 2. The blue, red, and gray curves correspond to the three
initial conditions, pµp0q,pp0qq “p0,0q, p0,100q, and p35,0q, respectively. The angular
frequency of the limit cycles is the magnitude of the imaginary eigenvalues of the
relaxation matrix R given in Eq. (31). The common fixed point is indicated by a
magenta bullet at the center of the orbits. [All curves are in arbitrary units.]
niche matching its prior belief, which generates nonstationary sensory inputs at the
receptors (Fig. 1). The brain variable µ initially undergoes a transient period at t 5.
ď
The RD commences from the resting condition µ 0 ,p 0 0,0 and then develops a
p p q p qq “ p q
stationaryevolution. Furthermore, we numerically confirmed that the brain’s stationary
Bayesian mechanics of perceptual inference and motor control in the brain 21
Δ Δ
φ φ
30
60 20
10
40 t
10 20 30 40
20 -10
-20
t
10 20 30 40 -30
-20 -40
Figure 5. Motorsignals∆ pµ;ϕptqqasafunctionoftimetevokedbythediscrepancy
ϕ
between the nonstationary sensory stream and its top-down prediction [Eq. (20)].
Here, we set the prior belief ϑ “ 20 (Left) and ϑ “ 10 (Right). The blue and
d d
red curves represent the results from the initial condition pµp0q,pp0qq “ p0,0q and
p0,100q, respectively. The gray curves represent the corresponding signals from the
plain perception of the static sensory input. All data were obtained by setting the
statistical parameters as σ “ 1.0, σ “ 10, and φ “ ´2.8. [All curves are in
w z
arbitrary units.]
prediction µ , which is the brain’s perceptual outcome of the sensory cause, is close
eq
to but not in line with the prior belief ϑ . The stationary value p is estimated to be
d eq
approximately 8.0, which is the average of the stationary oscillation of prediction error
p t .
p q
In Fig. 4, the trajectory corresponding to that in Fig. 3 is illustrated in blue in the
perceptual state space spanned by µ and p, including two other time developments from
different choices of initial conditions. All data were calculated using the same generative
parameters and sensory inputs used for Fig. 3. Regardless of the initial conditions, after
each transient period, the trajectories approach stationary limit cycles about a common
fixed point, as seen in the case of static sensory inputs in Fig. 2Right. The fixed point
Ψ and stationary frequency ω of the limit cycles are not affected by initial conditions,
eq
which are solely determined by the generative parameters m , m , and φ and the prior
w z
belief ϑ for a given sensory input ϕ [Eqs. (33) and (36)]. In addition, we numerically
d
observed that the precise location of the fixed points is stochastic, thereby reflecting the
noise from the nonstationary sensory influx ϕ.
Intheframeworkofactiveinference, motorbehavior isattributedtotheinference of
the causes of proprioceptive sensations (Adams et al. 2013), and in turn, the prediction
errors convey the motor signals in the closed-loop dynamics of perception and motor
control. In Fig. 5, we depict the sensorimotor signals ∆ µ;ϕ t that appear as time-
ϕ
p p qq
dependent driving terms in Eqs. (28) and (29). In both figures, the agent is assumed
to be initially situated such that it can sense the sensory data ϕ 0 4. After an
p q “
initial transient period elapses, the motor signals exhibit a stationary oscillation about
average zero in Fig. 5 (Left), implying the successful fulfillment of the active inference
of nonstationary sensory influx matching the desired belief ϑ 20. The amplitude
d
“
of the motor signal shown by the blue curve is smaller than that shown by the red
curve, which is also reflected in the size of the corresponding limit cycles in Fig. 4. The
Bayesian mechanics of perceptual inference and motor control in the brain 22
prediction-error signal from the plain perception exhibits an oscillatory feature in the
gray curve, which arises from the stationary time dependence of the brain variable µ t .
p q
The amplitude shows a large variationcaused by the significant discrepancy between the
static sensory input ϕ 4 and its prior belief ϑ 20. In Fig. 5 (Right), we repeated
d
“ “
the calculation with another value: ϑ 10. In this case, the prior belief ϑ regarding
d d
“
the sensory input does not accord with stationary sensory streams. Therefore, the blue
and red signals for active inference oscillate about the negatively shifted values from
average zero. In contrast to Fig. 5 (Left), the error-signal amplitude of the static input
is reduced because the difference between the sensory data and prior belief decreases.
Next, we consider the role of correlation φ in the brain’s RD, whose value is
limited by the constraint φ ?σ σ . To this end, we select three values of φ for
w z
| | ď
the fixed variances σ and σ , and we integrate the RD for active inference. In Fig. 6,
w z
we present the resulting time evolution of the brain states µ for the initial condition
µ 0 ,p 0 0,0 . In this figure, the conjugate momentum variables are not shown.
p p q p qq “ p q
The noticeable features in the results include the changes in the fixed point and the
amplitude of the stationary oscillation with correlation. The average value of µ t in
p q
the periodic oscillation corresponds to the perceptual outcome µ of the sensory data
eq
in the stationary limit. We remark that for all numerical data presented in this work,
we selected only negative values for φ. This choice was made because our numerical
inspection revealed that positive correlation does not yield stable solutions.
In Fig. 7, as the final numerical manifestation, we show the temporal buildup of the
limit cycles in the perceptual phase space; however, this time, we fix σ while varying
w
σ and φ. To generate the red, blue, and gray curves, the tuning parameter κ was
z
selected as κ 50, 10, and 100, respectively. The resulting fixed points are located
“
approximately at the center of each limit cycle, which are not shown. Similar to that
in Fig. 6, it can be observed that the positions of the fixed point and amplitudes of
oscillation are altered by variations in the statistical parameters. Evidently, a different
set of parameters, namely σ , σ , and φ, which are the learning parameters encoded by
w z
the brain, result in a distinctive BM of active inference.
Here, we summarize the major findings from the application of our formulation to
a simple nonstationary model. The brain’s BM, i.e., Eqs. (28) and (29), employ linear
generative models given in Eqs. (26) and (27).
(i) The steady-state solutions ofthe RDturn out to bea center about which stationary
limitcycles (periodicoscillations) areformedasanattractor(FristonandAo2012a)
in the perceptual phase space, which constitute the brain’s nonequilibrium resting
states.
(ii) Thenonequilibriumstationaritystemsfromthepairofpurelyimaginaryeigenvalues
of therelaxationmatrixwith oppositesigns, given by Eq. (31); the equal magnitude
specifies the angular frequency ω of the periodic trajectory.
(iii) Centers are determined by generative parameters and the prior belief for a given
sensory input [Eq. (33)], which represents the outcome of active inference and the
Bayesian mechanics of perceptual inference and motor control in the brain 23
μ(t)
30
20
10
t
10 20 30 40
-10
Figure 6. Time evolution of the brain variable µ. Here, we vary the correlation φ
for fixed variances σ “ 1.0 and σ “ 10. The red, blue, and gray curves correspond
w z
to φ “ ´3.0, ´2.8, and ´2.6, respectively. For all data, the agent is environmentally
situated at x “ 2, where it senses the transient sensory inputs ϕptq induced by the
motorreflexesattheproprioceptivelevel. Theagent’sinitialcognitivestateisassumed
to be pµp0q,pp0qq “ p0,0q, and the prior belief is set as ϑ “ 20. [All curves are in
d
arbitrary units.]
entailed prediction error.
(iv) The theoretical assumption of the statistical dependence of two generative noises
describing the brain’s expectation of the external dynamics and sensory generation
is consequential to ensuring a stationary solution. Furthermore, based on numerical
experience, a negative covariance is necessary for obtaining stable solutions using
the current model.
5. Concluding remarks
In the present study, we continued our effort to make the FEP a more physically
principled formalism based on our previous publication (Kim 2018). We implemented
the FEP in the scope of the principle of least action by casting the minimization scheme
to the BM described by the effective Hamiltonian equations in the neural phase space.
We deconstructed some of the theoretical details in the first part, which are embedded
in the formulation of the FEP, while comparing our approach with other currently
prevailing approaches. In the second half, we demonstrated our proposed continuous-
state RD in the Bayesian brain using a simple model, which is biologically relevant
to sensorimotor regulation such as motor reflex arcs or saccadic eye movement. In our
theory, thetime-integral oftheinducedIFEinthebrain, nottheinstant variationalIFE,
performs as an objective function. In other words, our minimization scheme searches
for the tight bound on the sensory uncertainty (average surprisal) and not the instant
sensory surprisal.
Topresentthenovel aspectsofourformulation, thisstudyfocusedontheperceptual
inference of nonstationary sensory influx at the interface. The nonstationary sensory
Bayesian mechanics of perceptual inference and motor control in the brain 24
p
50
μ
-5 5 10 15 20 25
-50
-100
-150
-200
-250
Figure 7. Limit cycles in the perceptual phase space spanned by the brain state µ
and its conjugate momentum p. Here, we considered several sets of σ and φ for a
z
fixedσ “1.0. Thered,blue,andgraycurveswereobtainedfrompσ ,φq“p50,´6.6q,
w z
p10,´2.8q,andp100,´9.5q,respectively. Foralldata,theagent’sinitialcognitivestate
is assumedto be pµp0q,pp0qq“p0,0q,and the priorbelief is set as ϑ “20. The agent
d
is environmentally situated at x“2, where it senses the transient sensory inputs ϕptq
induced by the motor reflexes at the proprioceptive level. [All curves are in arbitrary
units.]
inputs were assumed to be unknown or contingent to the neural observer without
explicitly engaging in motor-inference dynamics in the BM. Instead, we considered that
the motor signals are triggered by the discrepancies between the sensory inputs at the
proprioceptive level and their top-down predictions. They appeared as nonautonomous
source terms in the derived BM, thus completing the sensorimotor dynamics via
reflex arcs or oculomotor dynamics of sampling visual stimuli. This closed-loop
dynamics contrastswiththegradient-descent implementation, whichinvolves thedouble
optimization of the top-down belief propagation and the motor inference in message-
passing algorithms. In our present formulation, the sensorimotor inference was not
included; however, a mechanism of motor inference can be included explicitly by
considering a Langevin equation for a sensorimotor state. This procedure extends the
probabilistic generative model by accommodating the prior density for motor planning
for active perception, which is similar to what was done in (Bogacz 2020).
By integrating the Bayesian equations of motion for the considered parsimonious
model, we manifested transient limit cycles in the neural phase space, which numerically
illustratethebrain’sperceptual trajectoriesperformingactiveperceptionofthecausesof
nonstationary sensory stimuli. Moreover, we revealed thatensuing trajectories andfixed
pointsareaffected by theinput values of thelearning parameters (bothdiagonal andoff-
diagonal elements of the covariance matrix) and prior belief regarding sensory data. The
Bayesian mechanics of perceptual inference and motor control in the brain 25
idea of exploring the effect of noise covariance was purely from the theoretical insight
without a supporting empirical evidence, which allowed us to drive a stable solution
in perceptual and motor-control dynamics. We did not attempt to explicate in detail
the effect of neural inertial masses (precisions) and correlation (noise covariance) on the
numerically observed limit cycles. This was because of the numerical limitation set by
the presented model, which permits stable solutions in a significantly narrow window of
statistical parameters. In neurosciences, it is commonly recognized that neural system
dynamics implement cognitive processes influencing psychiatric states (Durstewitz et al.
2020). We hope that the key features of our manifestation will serve to motivate and
guide further investigations on more realistic generative models with neurobiological
and psychological implications.
Finally, we mention the recent research efforts on synthesizing perception, motor
control, and decision making within the FEP (Friston et al. 2015; Friston et al. 2017;
Biel et al. 2018; Parr and Friston 2019; van de Laar and de Vries 2019; Tschantz
et al. 2020; Da Costa et al. 2020a). The underlying idea of these studies is rooted
in machine learning (Sutton and Barto 1998) and the intuition from nonequilibrium
thermodynamics (Parr et al. 2020; Friston 2019), and they attempt to widen the scope
of active inference by incorporating prior beliefs regarding behavioral policies. The new
trend supplements the instant IFE to the future expected IFE in a time series, and
it formulates the adaptive decision-making processes in action-oriented models. The
assimilation of this feature needs to be studied in depth (Millidge et al. 2020b; Tschantz
et al. 2020). We are currently considering a formulation of motor inference together
with the assimilation of extended IFEs in the scope of the least action principle.
Acknowledgments
This is a post-peer-review, pre-copyedit version of an article published in
Biological Cybernetics. The final authenticated version is available online at:
https://doi.org/10.1007/s00422-021-00859-9.
References
[1] Adams RA, Shipp S, Friston KJ (2013) Predictions not commands: active inference in the motor
system. Brain Struct Funct 218:611–643.https://doi.org/10.1007/s00429-012-0475-5
[2] Amari S (1998) Natural gradient works efficiently in learning. Neural Comput 10(2): 251–276.
https://doi.org/10.1162/089976698300017746
[3] Balaji B, Friston K (2011) Bayesian state estimation using generalized coordinates. Proc.
SPIE 8050, Signal Processing, Sensor Fusion, and Target Recognition XX, 80501Y.
https://doi.org/10.1117/12.883513.
[4] Baltieri M, Buckley CL (2019) PID control as a process of active inference with linear generative
models. Entropy 21:257
[5] Biehl M, Guckelsberger C, Salge C, Smith SC, Polani D (2018) Expanding the active inference
landscape: More intrinsic motivations in the perception-action loop. Front Neurorobot 12:45.
doi: 10.3389/fnbot.2018.00045
Bayesian mechanics of perceptual inference and motor control in the brain 26
[6] Bogacz R (2017) A tutorial on the free-energy framework for modelling perception and learning.
J Math Psychol 76(B):198–211.https://doi.org/10.1016/j.jmp.2015.11.003
[7] Bogacz R (2020) Dopamine role in learning and action inference. eLife 9:e53262. DOI:
https://doi.org/10.7554/eLife.53262
[8] Buckley CL, Kim CS, McGregor S, Seth AK (2017) The free energy principle
for action and perception: A mathematical review. J Math Psychol 81:55–79.
https://doi.org/10.1016/j.jmp.2017.09.004
[9] Colombo M, Wright C (2018) First principles in the life sciences: The free-energy principle,
organicism, and mechanism. Synthese. https://doi.org/10.1007/s11229-018-01932-w
[10] CoverT,ThomasJA(2006)ElementsofInformationTheory2nded.Wiley-Interscience,Hoboken
[11] CugliandoloLFandLecomteV(2017)Rulesofcalculusinthepathintegralrepresentationofwhite
noise Langevin equations: the Onsager–Machlupapproach. J Phys A: Math Theor 50:345001
[12] Da Costa L, Parr T, Sajid N, Veselic S, Neacsu V, Friston K (2020a)Active inference on discrete
state-spaces: Asynthesis.JMathPsychol99:102447.https://doi.org/10.1016/j.jmp.2020.102447
[13] Da Costa L, Parr T, Sengupta B, Friston K (2020b) Natural selection finds natural gradient.
arXiv:200108028[q-bio]
[14] Da Costa L, Sajid N, Parr T, Friston K, Smith R (2020c) The relationship between dynamic
programmingandactive inference: The discrete, finite horizoncase. arXiv:2009.08111v3 [cs.AI]
[15] de Gardelle V, Waszczuk M, Egner T, Summerfield C (2013) Concurrent repetition enhancement
and suppression responses in extrastriate visual cortex. Cerebral Cortex 23(9):2235–2244.
https://doi.org/10.1093/cercor/bhs211
[16] DurstewitzD,HuysQ,KoppeG(2020)Psychiatricillnessesasdisordersofnetworkdynamics.Bi-
ologicalPsychiatry: Cognitive Neurosci Neuroimg. https://doi.org/10.1016/j.bpsc.2020.01.001
[17] Elfwing S, Uchibe E, Doya K (2016) From free energy to expected energy: Improving energy-
based value function approximation in reinforcement learning. Neural Networks 84:17–27.
http://dx.doi.org/10.1016/j.neunet.2016.07.013
[18] Feynman RP and Hibbs AR (2005) Quantum mechanics and path integrals Emended Edition.
Dover Publication, Mineola
[19] Fox RF (1987) Stochastic calculus in physics. J Stat Phys 46:1145–1157.
https://doi.org/10.1007/BF01011160
[20] Friston K, Mattout J, Trujillo-Barreto N, Ashburner J, Penny W (2007) Varia-
tional free energy and the Laplace approximation. NeuroImage 34(1):220–234.
https://doi.org/10.1016/j.neuroimage.2006.08.035.
[21] Friston K (2008a) Hierarchical models in the brain. PLoS Comput Biol 4(11): e1000211.
doi:10.1371/journal.pcbi.1000211
[22] Friston KJ (2008b) Variational filtering. Neuroimage 41:747–766
[23] Friston KJ, Trujillo-Barreto N, Daunizeau J (2008c). DEM: a variational treatment of dynamic
systems. Neuroimage 41(3): 849–885
[24] FristonKJ,DaunizeauJ,KiebelSJ(2009)Reinforcementlearningoractiveinference?.PLoSONE
4(7):e6421.doi:10.1371/journal.pone.0006421
[25] FristonK(2010a)The free-energyprinciple: aunifiedbraintheory? NatureRevNeurosci11:127–
138
[26] Friston K, Stephan K, Li B, Daunizeau J (2010b) Generalized filtering. Math Problem in Eng
2010:261670.
[27] Friston KJ, Daunizeau J, Kilner J, Kiebel SJ (2010c) Action and behavior: a free-energy
formulation. Biol Cybern 102(3):227–260
[28] Friston K, Mattout J, Kilner J (2011a) Action understanding and active inference. Biol Cybern
104:137–160.
[29] Friston K (2011b) What is optimal about motor control? Neuron 72(3):488–498. DOI
10.1016/j/neuron.2011.10.018
[30] Friston K, Ao P (2012a) Free Energy, value, and attractors. Comput Math Methods Med
Bayesian mechanics of perceptual inference and motor control in the brain 27
2012:937860.https://doi.org/10.1155/2012/937860
[31] Friston K, Adams R, Perrinet L, Breakspear M (2012b) Perceptions as hypotheses: saccades as
experiments. Front in Psychol 3:151. https://doi.org/10.3389/fpsyg.2012.00151
[32] Friston K (2013) Life as we know it. J R Soc Interface 10:1020130475
http://doi.org/10.1098/rsif.2013.0475
[33] Friston K, Rigoli F, Ognibene D, Mathys C, Fitzgerald T, Pezzulo G (2015) Active inference and
epistemic value. Cogn Neurosci 6:187–214.doi: 10.1080/17588928.2015.1020053
[34] FristonKJ,ParrT,deVriesB(2017)Thegraphicalbrain: Beliefpropagationandactiveinference.
Network Neurosci 1(4):381–414
[35] Friston K (2019) A free energy principle for a particular physics. arXiv:190610184[q-bio]
[36] Hille, B. (2001)Ion channels of excitable membranes 3rd Edition. Sinauer Associates,Sunderland
[37] HuangY, RaoRPN(2011)Predictivecoding.WIREs CogniSci2:580–593.DOI:10.1002/wcs.142
[38] Isomura T, Kotani K, Jimbo Y (2015) Cultured cortical neurons can perform blind source
separation according to the free-energy principle. PLoS Comput Biol 11(12):e1004643.
doi:10.1371/journal.pcbi.1004643
[39] IsomuraTandFristonK(2018)Invitroneuralnetworksminimisevariationalfreeenergy.SciRep
8:16926.https://doi.org/10.1038/s41598-018-35221-w
[40] Jazwinski AH (1970) Stochastic process and filtering theory. Academic Press, New York
[41] Kerr W and Graham A (2000) Generalized phase space version of Langevin equa-
tions and associated Fokker-Planck equations. Eur. Phys. J. B 15:305–311.
https://doi.org/10.1007/s100510051129
[42] Kiefer AB (2020) Psychophysical identity and free energy. J R Soc Interface 17: 20200370.
http://dx.doi.org/10.1098/rsif.2020.0370
[43] KimCS(2018)Recognitiondynamicsinthebrainunderthefreeenergyprinciple.NeuralComput
30:2616-2659.https://doi.org/10.1162/necoa 01115
[44] Kozunov VV, West TO, Nikolaeva AY, Stroganova TA, Friston KJ (2020) Object recognition is
enabled by an experience-dependent appraisal of visual features in the brain’s value system.
Neuroimage 221:117143.https://doi.org/10.1016/j.neuroimage.2020.117143
[45] Kuzma S (2019)Energy-informationcoupling during integrativecognitive processes.J Theor Biol
469:180–186.https://doi.org/10.1016/j.jtbi.2019.03.005
[46] Landau LD, Lifshitz EM (1976) Mechanics: Volume 1 (Course of Theoretical Physics S) 3rd
Edition. Elsevier Ltd, Amsterdam
[47] Markov NT, Kennedy H (2013) The importance of being hierarchical. Curr Opin Neurobiol
23(2):187–194doi:10.1016/j.conb.2012.12.008
[48] Michalareas G, Vezoli J, van Pelt S, Schoffelen JM, Kennedy H, Fries P (2016) Alpha-beta and
gamma rhythms subserve feedback and feedforward influences among human visual cortical
areas. Neuron 89(2):384–397doi:10.1016/j.neuron.2015.12.018
[49] MillidgeB,TschantzA,SethAK,BuckleyCL(2020a)OntheRelationshipbetweenactiveinference
and control as inference. arXiv:2006.12964v3 [cs.AI]
[50] Millidge B, TschantzA, Buckley CL (2020b)Whence the expected free energy? arXiv:2004.08128
[cs.AI]
[51] Moon W, Wettlaufer J (2014) On the interpretation of Stratonovich calculus. New J Phys
16:055017.http://dx.doi.org/10.1088/1367-2630/16/5/055017
[52] Ozaki T (1992) A bride between nonlinear time series models and nonlinear stochstic dynamical
systems: Alocal linearization approach.Statistica Sinica 2:113—135
[53] Parr T, Friston KJ (2018) Active inference and the anatomy of oculomotion. Neuropsychologia
111:334–343.https://doi.org/10.1016/j.neuropsychologia.2018.01.041
[54] ParrT, Friston KJ (2019)Generalisedfree energy and active inference. Biol Cybern 113:495–513.
https://doi.org/10.1007/s00422-019-00805-w
[55] Parr T, Da Costa L, Friston K (2020) Markov blankets, information geometry and stochastic
thermodynamics. Phil Trans R Soc A 37820190159.http://doi.org/10.1098/rsta.2019.0159
Bayesian mechanics of perceptual inference and motor control in the brain 28
[56] Pavliotis GA (2014) Stochastic processes and applications: diffusion processes the Fokker-Planck
and Langevin equations. Springer, New York
[57] RamsteadMJD,BadcockPB,FristonKJ(2018)AnsweringSchr¨odinger’squestion: Afree-energy
formulation. Phys Life Rev 24:1–16.https://doi.org/10.1016/j.plrev.2017.09.001
[58] RamsteadMJD, ConstantA, Badcock PB,Friston KJ (2019)Variationalecologyand the physics
of sentient systems. Phys Life Rev 31:188–205.https://doi.org/10.1016/j.plrev.2018.12.002
[59] Rao RPN, Ballard DH (1999) Predictive coding in the visual cortex: a functional
interpretation of some extra-classical receptive-field effects. Nat Neurosci 2(1):79–87.
https://doi.org/10.1038/4580
[60] Risken H (1989) The Fokker-PlanckEquation 2nd edition. Springer-Verlag,Berlin
[61] Sanders H, Wilson MA, Gershman SJ (2020) Hippocampal remapping as hidden state inference.
eLife 9:e51140.https://doi.org/10.7554/eLife.51140
[62] Seifert U (2012) Stochastic thermodynamics, fluctuation theorems and molecular machines. Rep
Prog Phys 75:126001.http://dx.doi.org/10.1088/0034-4885/75/12/126001
[63] Sengupta B, Tozzi A, Cooray GK, Douglas PK, Friston KJ (2016) Towards a neuronal gauge
theory. PLoS Biol 14(3): e1002400.https://doi.org/10.1371/journal.pbio.1002400
[64] Sengupta B, Friston K (2017) Approximate Bayesian inference as a gauge theory.
arXiv:1705.06614v2 [q-bio.NC]
[65] Shimazaki H (2019) The principles of adaptation in organisms and machines I: machine learning,
information theory, and thermodynamics. arXiv:1902.11233
[66] StrogatzSH(2015)Nonlineardynamicsandchaos: withapplicationstophysics,biology,chemistry,
and engineering (Studies in Nonlinearity) 2nd Edition. Westview Press, Cambridge
[67] Surace SC, Pfister JP, Gerstner W, Brea J (2020) On the choice of metric in
gradient-based theories of brain function. PLoS Comput Biol 16(4):e1007640.
https://doi.org/10.1371/journal.pcbi.1007640
[68] SuttonRS,BartoAG(1998)Reinforcementlearning: AnIntroduction.TheMITPress,Cambridge
[69] Todorov E (2007) Optimal control theory. Bayesian brain: probabilistic approaches to neural
coding. 269–298.The MIT Press, Cambridge
[70] TschantzA,SethAK,BuckleyCL(2020)Learningaction-orientedmodelsthroughactiveinference.
PLOS Comput Biol 16(4):E1007805.https://doi.org/10.1371/journal.pcbi.1007805
[71] Tuthill JC, Azim E (2018) Proprioception. Current Biol 28(5):R194–R203
doi:10.1016/j.cub.2018.01.064
[72] van Kampen NG (1981) Itoˆ versus Stratonovich. J Stat Phys 24:175–187.
https://doi.org/10.1007/BF01007642
[73] van de Laar TW, de Vries B (2019) Simulating active inference processes by message passing.
Front Robot and AI 6:20. https://doi.org/10.3389/frobt.2019.00020

=== REVISE TO ===
PROFESSIONAL TONE: Begin directly with content - NO conversational openings like 'Okay, here's...'

1. Fix all issues above
2. Title: "Bayesian mechanics of perceptual inference and motor control in the brain"
3. Include 10-15 quotes from paper text
   - Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
   - Use consistent quote formatting: 'The authors state: "quote"' or vary attribution phrases
   - Vary attribution phrases to avoid repetition
   - CRITICAL: Only extract quotes that actually appear in the paper text
4. ELIMINATE ALL REPETITION - each sentence must be unique
   - Check before each sentence: 'Have I already said this?' If yes, write something new
   - Vary attribution phrases - do NOT repeat 'The authors state' multiple times
5. Extract methodology, results with numbers, key quotes
6. 1000-1500 words, structured with ### headers

Generate COMPLETE revised summary.