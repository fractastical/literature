=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Expected Free Energy-based Planning as Variational Inference
Citation Key: vries2025expected
Authors: Bert de Vries, Wouter Nuijten, Thijs van de Laar

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Abstract: We address the problem of planning under uncertainty, where an
agent must choose actions that not only achieve desired outcomes but
also reduce uncertainty. Traditional methods often treat exploration
and exploitation as separate objectives, lacking a unified inferential
foundation. Active inference, grounded in the Free Energy Princi-
ple, provides such a foundation by minimizing Expected Free Energy
(EFE),acostfunctionthatcombinesutilitywithepistemicdrives,such
as ambiguity resolution and nove...

Key Terms: wouter, energy, planning, expected, variational, netherlands, inference, active, free, thijs

=== FULL PAPER TEXT ===

Expected Free Energy-based Planning
as Variational Inference
Bert de Vriesâˆ—1,2, Wouter Nuijten1,2, Thijs van de Laar1, Wouter
Kouw1, Sepideh Adamiat1, Tim Nisslbeck1, Mykola Lukashchuk1,
Hoang Minh Huu Nguyen1, Marco Hidalgo Araya1, RaphaÃ«l TrÃ©sor1,
Thijs Jenneskens1, Ivana Nikoloska1, Raaja Ganapathy
Subramanian1,3, Bart van Erp1,2, Dmitry Bagaev1,2, and Albert
Podusenko1,2
1Eindhoven University of Technology, Eindhoven, the Netherlands
2Lazy Dynamics B.V., Eindhoven, the Netherlands
3ASML, Veldhoven, the Netherlands
October 8, 2025
Abstract
We address the problem of planning under uncertainty, where an
agent must choose actions that not only achieve desired outcomes but
also reduce uncertainty. Traditional methods often treat exploration
and exploitation as separate objectives, lacking a unified inferential
foundation. Active inference, grounded in the Free Energy Princi-
ple, provides such a foundation by minimizing Expected Free Energy
(EFE),acostfunctionthatcombinesutilitywithepistemicdrives,such
as ambiguity resolution and novelty seeking. However, the computa-
tional burden of EFE minimization had remained a significant obsta-
cle to its scalability. In this paper, we show that EFE-based planning
arises naturally from minimizing a variational free energy functional
on a generative model augmented with preference and epistemic pri-
ors. ThisresultreinforcestheoreticalconsistencywiththeFreeEnergy
Principlebycastingplanningunderuncertaintyitselfasaformofvari-
ational inference. Our formulation yields policies that jointly support
goal achievement and information gain, while incorporating a com-
plexitytermthataccountsforboundedcomputationalresources. This
unifying framework connects and extends existing methods, enabling
scalable, resource-aware implementations of active inference agents.
Keywords: Active Inference, Bounded Rationality, Epistemic Uncer-
tainty, Expected Free Energy, Free Energy Principle, Planning as Inference,
Policy Optimization, Variational Inference
âˆ—bert.de.vries@tue.nl
1
5202
tcO
7
]LM.tats[
4v89841.4052:viXra
Contents
1 Introduction 2
2 The Expected Free Energy Cost Function 3
3 Related Work 5
4 EFE-based Planning as Variational Inference 7
5 Discussion 9
5.1 Optimal Planning by Variational Inference . . . . . . . . . . . 9
5.2 Interpretation of the Epistemic Priors . . . . . . . . . . . . . 9
5.3 On the Complexity Term C(u) . . . . . . . . . . . . . . . . . 10
5.4 PAI in a Synthetic Active Inference Agent . . . . . . . . . . . 10
5.5 Toward Scalable Synthetic Active Inference . . . . . . . . . . 11
5.6 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
6 Conclusions 13
A Proof of the Main Theorem 15
B The Entropy and Kullback-Leibler Divergence 17
1 Introduction
Planningunderuncertaintyisafundamentalchallengeinbothartificialintel-
ligence and cognitive neuroscience. Agents must select actions that not only
achieve desired outcomes but also reduce uncertainty about their environ-
ment. Classical approachesâ€”rooted in reinforcement learning and optimal
controlâ€”typicallyaddressthisbyoptimizinglong-termutilitythroughvalue
function estimation or policy learning Sutton and Barto [2018], Bertsekas
[2012]. However, these methods often treat reward maximization (exploita-
tion) and uncertainty reduction (exploration) as separate objectives, using
heuristics to strike a balance between them. Moreover, they struggle in
high-dimensional or deep-horizon settings due to compounding errors and
the curse of dimensionality.
Active inference offers a principled alternative. Grounded in the Free
Energy Principle (FEP)1, it casts perception, learning, and action selection
as inference processes that minimize a variational bound on surprise Friston
[2010],Parretal.[2022]. CentraltothisframeworkistheExpectedFreeEn-
ergy (EFE), a unified objective that combines instrumental (goal-directed)
1We use the following abbreviations in this paper: Expected Free Energy (EFE), Free
EnergyPrinciple(FEP),Kullback-Leibler(KL),PlanningasInference(PAI),Variational
Free Energy (VFE).
2
and epistemic (information-seeking) components Friston et al. [2015]. Mini-
mizing EFE yields behavior that simultaneously pursues preferred outcomes
and resolves uncertainty, providing a theoretically grounded solution to the
explorationâ€“exploitation trade-off.
Despiteitspromise,practicalimplementationsofEFE-basedplanningre-
maincomputationallydemandingKappenetal.[2012],Palmierietal.[2022],
van de Laar et al. [2024], Friston et al. [2021], Paul et al. [2024]. Existing
methods often resort to approximations that undermine alignment with the
FEP, particularly the foundational claim that all processing arises from vari-
ational free energy minimization. In this work, we address these limitations.
We show that EFE-based planning can be rigorously formulated as vari-
ational inference on a generative model augmented with preference and epis-
temic priors. Our central result demonstrates that minimizing a well-defined
variationalfreeenergyfunctionalnaturallyyieldspoliciesthatintegrategoal-
directedbehavior, information-seekingexploration, andboundedrationality.
This formulation improves full theoretical alignment with the FEP, and uni-
fies active inference with the broader planning-as-inference paradigm, offer-
ing a scalable and principled framework for decision-making under uncer-
tainty.
The next section introduces the formal definition of EFE and highlights
its desirable properties for planning under uncertainty. Section 3 reviews
prior work on EFE minimization and outlines several limitations of existing
approaches. Thecentralcontributionofthispaperâ€”demonstratinghowEFE
minimization can be recast as standard variational inferenceâ€”is presented
as a formal theorem in Section 4. The paper concludes with a discussion
of the theoremâ€™s implications and its relevance for building scalable active
inference agents.
2 The Expected Free Energy Cost Function
Consider an agent described by a generative model p(yxÎ¸u).2 In this paper,
we are only concerned with planning, so we will assume that the model
predicts a sequence of future observations. A typical example of this model
would be a rollout of a state space model, for instance
T
(cid:89)
p(yxÎ¸u) = p(x )p(Î¸) p(y |x ,Î¸)p(x |x ,u )p(u ) (1)
t k k k kâˆ’1 k k
k=t+1
(cid:124) (cid:123)(cid:122) (cid:125)
rollouttothefuture
wheretholdsthecurrenttimestep. Inthismodel,ydenotesthesequence
of future observations, x represents the (latent) states, Î¸ contains the model
2For brevity, we omit commas in the notation for joint variables, e.g., p(yxÎ¸u) =
p(y,x,Î¸,u).
3
parameters, and u refers to the policy, i.e., a sequence of future actions
(controls). Because all of these variables are defined as part of a model
rollout into the future, they are all treated as unobserved variables. Since
(1) is designed to predict how the future is expected to unfold, we refer to
it as the predictive model.
In model (1), the prior distribution p(u) can be understood as an empir-
ical distribution over allowable policies based on contextual data. Assume
that we are additionally provided with a distribution
pË†(x), (2)
which describes the preferred future states, sometimes referred to as target
states. The planning objective is to infer a policy posterior q(u) that, if
executed, would efficiently guide the agent to these preferred states.3
In the active inference literature, candidate policies are evaluated by a
cost function G(u), known as the Expected Free Energy, which is defined as
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
q(x|u) 1 q(Î¸|yx)
G(u) = E log +E log âˆ’E log , (3)
q q q
pË†(x) q(y|x) q(Î¸|x)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
risk ambiguity novelty
(cid:124) (cid:123)(cid:122) (cid:125)
epistemiccosts
where the expectations are with respect to q = q(yxÎ¸|u). In (3), the as-
sumption is that q(yxÎ¸u) is the variational posterior with respect to the
joint model p(yxÎ¸u)pË†(x). In section 4, we will refine this statement. Policies
with a lower G(u) value are regarded as more favorable, i.e., a priori more
likelytobeselected. ActiveinferenceprocessesarebasedontheFreeEnergy
Principle, a sophisticated theory grounded in core physics concepts that ac-
counts for the behavior of living systems as if G(u) were their planning cost
function. The most contemporary reference is Friston et al. [2023]. On a
more practical level, we summarize this theory by discussing the incentives
behind the three components of G(u).
â€¢ Risk refers to the KL-divergence between q(x|u), which is the state
thatweexpecttoreachunderpolicyu, andthetargetstatepË†(x). EFE
minimization aligns with risk minimization.
â€¢ Ambiguity is the expected entropy E (cid:2) H[q(y|x)] (cid:3) of future ob-
q(x|u)
servations y under policy u. Minimizing EFE results in policies that
seek well-predicted (i.e., unambiguous) observations, leading to accu-
rate state estimates.
3Sinceq(u)isconditionedsolelyonpriors,namelythegenerativemodelp(yxÎ¸u)anda
preferencepriorpË†(x),itwouldbemoreappropriatetospeakaboutanupdatedpriorq(u)
rather than a posterior. For simplicity, in this paper, all distributions that result from
inference are denoted by q(Â·) and termed posteriors.
4
â€¢ Novelty extendsinformation-seekingpoliciestoincludeactive param-
eter learning. Minimizing EFE results in policies that maximize the
mutual information between observations y and parameters Î¸.
EFE minimization can be viewed as a unifying framework for planning
under uncertainty, integrating principles from both decision theory and op-
timal control. Several established paradigms emerge as special cases of EFE
minimization under specific assumptions. For example, Kullback-Leibler
(KL) control Todorov [2006], Rawlik et al. [2013] arises when epistemic
(information-seeking) terms are omitted, effectively reducing EFE to a risk-
based utility optimization. Conversely, when the risk term is removed, EFE
minimization reduces to Bayesian experimental design Lindley [1956], which
focuses purely on maximizing information gain.
As an aside, if preferences were instead described by a distribution pË†(y)
over desired future observations, it is common to define an alternative EFE
as
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
1 q(x|y) q(Î¸|yx)
Gâ€²(u) = E log âˆ’E log âˆ’E log , (4)
q q q
pË†(y) q(x|u) q(Î¸|x)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
pragmatic salience novelty
costs (cid:124) (cid:123)(cid:122) (cid:125)
epistemiccosts
where, as before, the expectations are with respect to q = q(yxÎ¸|u). Al-
though G(u) and Gâ€²(u) are distinct cost functions with different definitions
of epistemic costs, they are centered around the same criteria. They can
be regarded as specific instances of a broader policy cost function template
given by
(cid:20) (cid:21)
q(xÎ¸|u)
E log (5)
q(yxÎ¸|u) pÂ¯(yxÎ¸)
where the denominator features a â€œbiasedâ€ model pÂ¯(yxÎ¸), incorporating both
preferenceincentivespË†(Â·)andvariationalposteriorsq(Â·). Specifically,G(u)in
(3)usespÂ¯(yxÎ¸) = q(Î¸|yx)q(y|x)pË†(x),andpÂ¯(yxÎ¸)in(4)factorizesaspÂ¯(yxÎ¸) =
q(Î¸|yx)q(x|y)pË†(y). Inthefollowingdiscussion, wefocusonG(u), notingthat
similar derivations and arguments apply to Gâ€²(u).
3 Related Work
We shortly review recent efforts on how to find policies that minimize EFE
efficiently.
IntheSophisticatedInference(SI)frameworkintroducedbyFristonetal.
[2021], a layer of recursive belief modeling is added to the EFE formulation.
Thisenablesdeeperformsofplanning,whereagentsconsidernotonlyâ€œWhat
5
will happen if I do this?â€ but also â€œWhat will I believe will happen if I do
this?â€â€”allowing for richer introspective evaluation of future outcomes.
While conceptually compelling, the computational implementation of SI
reliesonanexplicittreesearchovercandidatepolicies. Astheplanninghori-
zonincreases,thenumberofpossiblepolicysequencesgrowscombinatorially,
making the exhaustive evaluation of EFE increasingly intractable.
To address this scalability issue, Paul et al. proposed the Dynamic Pro-
grammingExpectedFreeEnergy(DPEFE)frameworkPauletal.[2024]. By
leveragingdynamicprogrammingprinciples,DPEFEcomputesexpectedfree
energyrecursively,reducingthecomputationalcostoflong-horizonplanning.
This reformulation allows active inference agents to plan efficiently in more
complex environments without sacrificing theoretical rigor.
A conceptual limitation of approaches such as Sophisticated Inference
and dynamic programming-based policy selection is that they rely on ex-
plicitly designed, human-crafted algorithms for policy selection. This sits
uncomfortably with the FEP, which posits that all cognitive and behavioral
processes should emerge from the automatic, event-driven minimization of
variational free energy. From this perspective, policy selection should ideally
arise entirely through an inference process, rather than through externally
imposed algorithmic procedures.
ThemotivationforadoptingaPlanning-as-Inference(PAI)perspectiveis
notmerelyphilosophicalalignmentwiththeFEP;italsostemsfrompractical
considerations. Specifically,policyselectionshouldbeinterruptibleâ€”capable
ofproducingavalidapproximateresultatanytimeâ€”andshouldscalegrace-
fully with available computational resources. These properties are naturally
afforded by embedding policy selection within a reactive message-passing
scheme on a factor graph, where each local message incrementally reduces
free energy [Bagaev and de Vries, 2023]. In such a framework, computa-
tion is inherently flexible and distributed, and intermediate solutions retain
semantic coherence. In contrast, algorithmic approaches based on procedu-
ral codeâ€”with nested loops and conditionalsâ€”lack this interruptibility and
adaptability, making them ill-suited for real-time or resource-constrained
settings.
The PAI framework, proposed initially by Attias [2003] and later ex-
tended by Toussaint [2009] and Solway and Botvinick [2012], reinterprets
planning as a probabilistic inference problem: the goal is to infer action tra-
jectoriesthat are most consistent withprior preferencesover outcomes. This
perspectiveenablestheuseofapproximateinferencetechniques,suchasvari-
ational inference and message passing, to develop computationally efficient
planning algorithms.
However, the mentioned PAI formulations focus on maximizing expected
utility and do not explicitly incorporate epistemic value, i.e., the drive to
reduce uncertainty, which is a defining feature of EFEâ€“based approaches.
As a result, their applicability in highly uncertain or partially observable
6
environmentsislimited,astheylackaprincipledmechanismforinformation-
seeking behavior.
Palmieri et al. [2022] introduced a comprehensive framework that unifies
estimation and control through belief propagation on factor graphs, with a
particularemphasisonpathplanningapplications. Buildingonthisperspec-
tive, van de Laar et al. [2024] extended the PAI framework by integrating
epistemicvalueintothepolicyevaluationprocess,enablingagentstoaccount
for both expected utility and information gain during planning. Specifically,
they propose modifying the Variational Free Energy (VFE) by subtract-
ing a mutual information term when inference is performed over future (i.e.,
planned) segments of the factor graph. This adjustment allows reactive mes-
sage passing to naturally account for both instrumental and epistemic value,
yieldinganinterruptibleandentirelylocalinferenceprocedureforevaluating
candidate policies.
In contrast to above mentioned PAI methods, the approach proposed by
Van de Laar and Koudahl yields results that align with EFE minimization,
but unfortunately it also introduces some conceptual and practical chal-
lenges. Conceptually, it is somewhat inelegant to alternate between different
cost functions depending on the location of computation within the factor
graph. This bifurcation undermines the principle of a unified objective func-
tion underlying all natural inference processes. Practically, it complicates
the design and implementation of inference toolkits: developers must now
account for two distinct message computations for each nodeâ€”one for stan-
dard inference and another for planningâ€”thereby significantly increasing
implementation complexity and reducing modularity.
Finally, outside the FEP community and more within the reinforcement
learning literature, the recent work by LÃ¡zaro-Gredilla et al. [2024] offers
a compelling perspective on the relationship between planning and infer-
ence. Similar to our approach, their work highlights the role of entropy and
information-seeking behavior in planning. The key distinction lies in the
formulation of the inference objective: while LÃ¡zaro-Gredilla et al. [2024]
demonstrate that planning corresponds to a specific weighting of entropy
terms within a general variational objective, we introduce a VFE functional
for a generative model that is augmented with epistemic priors, which yields
EFE-based planning as a natural consequence.
In the next section, we develop a PAI framework that is not only consis-
tent with the FEP but also addresses some of the conceptual and practical
limitations of the approaches discussed above.
4 EFE-based Planning as Variational Inference
The main contribution of the paper is described by a theorem, which we
conveniently label as the Expected Free Energy theorem.
7
Theorem 1 (ExpectedFreeEnergyTheorem). Consider an agent with gen-
erative (predictive) model
p(yxÎ¸u), (6)
and prior beliefs
pË†(x) (7)
about future desired states.
Let the Variational Free Energy functional F[q] be defined as
posterior
(cid:122) (cid:125)(cid:124) (cid:123)
(cid:20) (cid:21)
q(yxÎ¸u)
F[q] â‰œ E log , (8)
q(yxÎ¸u) p(yxÎ¸u) pË†(x) pËœ(u)pËœ(x)pËœ(yx)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124)(cid:123)(cid:122)(cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
generativepreference epistemic priors
model prior
where the generative model in the denominator is augmented by both a pref-
erence prior pË†(Â·) and epistemic priors pËœ(Â·).
Let the epistemic priors be defined as
pËœ(u) = exp(H[q(x|u)]) (9a)
pËœ(x) = exp(âˆ’H[q(y|x)]) (9b)
pËœ(yx) = exp(D[q(Î¸|yx),q(Î¸|x)]). (9c)
Then, F[q] decomposes as
(cid:20) (cid:21)
F[q] = E (cid:2) G(u) (cid:3) +E log q(yxÎ¸u) . (10)
q(u) q(yxÎ¸u) p(yxÎ¸u)
(cid:124) (cid:123)(cid:122) (cid:125)
expected policy (cid:124) (cid:123)(cid:122) (cid:125)
costs complexity
where G(u) is the expected free energy as defined in (3). In (9), H[q] =
E [âˆ’logq] is the entropy functional, and D[q,p] = E [logq âˆ’ logp] is the
q q
Kullback-Leibler divergence (see Appendix B).
Proof. The proof of (10) is given in Appendix A.
A key consequence of (10) is that minimizing F[q] leads to reducing the
expected policy costs E [G(u)], while also balancing this with the drive to
q(u)
reduce complexity, which are the costs associated with changing beliefs.
As shown in Appendix A, if one wishes to enforce normalization of the
prior distributions, the epistemic prior pËœ(u) in (9a) can alternatively be cho-
sen as
exp(H[q(x|u)])
pËœ(u) = Ïƒ(H[q(x|u)]) â‰œ , (11)
(cid:80) exp(H[q(x|uâ€²)])
uâ€²
where Ïƒ denotes the softmax function applied to the entropy H[q(x|u)].
Similar constructions apply to the priors pËœ(x) and pËœ(xy). In that case, the
variational free energy F[q] is shifted by a constant that does not affect the
location of its minimum, and thus does not influence the outcome of the
optimization process.
8
5 Discussion
5.1 Optimal Planning by Variational Inference
Starting from (16c) (see proof in Appendix A), we can compute the optimal
policy posterior through
(cid:20) (cid:21)
q(u) (cid:2) q(yxÎ¸|u)(cid:3)
F[q] = E log +G(u)+E log
q(u) p(u) q(yxÎ¸|u) p(yxÎ¸|u)
(cid:124) (cid:123)(cid:122) (cid:125)
=C(u)(complexity)
(cid:20) (cid:21)
q(u)
= E log , (12)
q(u) (cid:0) (cid:1)
exp âˆ’P(u)âˆ’G(u)âˆ’C(u)
where P(u) = âˆ’logp(u) denotes the policy prior expressed as a cost func-
tion. Equation (12) is a Kullback-Leibler divergence that is minimized for
qâˆ—(u) = argminF[q]
q
= Ïƒ (cid:0) âˆ’P(u)âˆ’G(u)âˆ’C(u) (cid:1) , (13)
where
exp(a )
Ïƒ(a) = k (14)
k (cid:80)
exp(a )
kâ€² kâ€²
is a normalized exponential function.
Equation (13) is not new. A comparable formula for the optimal policy
can be found in Equation 2.1 of Friston et al. [2021]. A main contribution of
thispaperistodemonstratethatthe(previouslyestablished)optimalpolicy,
given by (13), can be obtained through standard variational minimization of
an appropriately defined free energy functional F[q].
5.2 Interpretation of the Epistemic Priors
The epistemic prior pËœ(u) = exp(H[q(x|u)]), introduced in (9a), imposes a
bias toward selecting policies that maximize the entropy over future states
x. This reflects an information-seeking preference, as high entropy states
indicate that the agent is maintaining flexibilityâ€”keeping future states open
for adaptation. Additionally, the epistemic prior pËœ(x) = exp(âˆ’H[q(y|x)])
in (9b), favors policies that reduce the uncertainty over future states by
selecting observations that are informative about them. Together, pËœ(u) and
pËœ(x) induce a bias toward ambiguity-minimizing behavior. Similarly, the
priors pËœ(u) and pËœ(y,x) jointly shape a preference for policies that maximize
novelty.
9
5.3 On the Complexity Term C(u)
In (13), P(u) and G(u) reflect past and future information about the policy
posterior q(u), respectively. The complexity term C(u) represents the dis-
crepancy (expressed as a KL divergence) between the (inferred) variational
posterior q(yxÎ¸|u) and the (ideal) Bayesian posterior p(yxÎ¸|u).
Is C(u) simply an unavoidable cost lacking benefits since, in contrast to
P(u) and G(u), it does not provide information regarding effective policies?
Not quite. Inference of q must be executed on a specific platform in a partic-
ular context that provides access to a certain set of computational resources.
For example, the resources available for tracking a specific car in traffic may
differ depending on the overall complexity of the traffic situation. The pres-
ence of bounded computational resources can be regarded as a constraint on
the inference process.
Typicalinferenceconstraintsincludemean-fieldassumptionsonq,aswell
as assumptions about the posterior form (e.g., q(u) must be Gaussian, even
if p(u) is not). Latency assumptions also apply; for instance, q(u) may need
to be executed within 5 milliseconds, regardless of the state of the inference
process.
While these inference constraints are not incorporated into the objective
function F[q] as seen in (8), the term C(u) can be understood as a drive
to minimize the unavoidable effects of these inference constraints. In other
words, due to the complexity term C(u) in (13), active inference agents that
minimize (8) are Bayes-optimal planners for a given set of constraints. We
refer the reader to ÅenÃ¶z et al. [2021]for a discussion on how to express
inference constraints explicitly in the objective function.
5.4 PAI in a Synthetic Active Inference Agent
We discuss here how the inference process within an active inference agent
may proceed. Consider the generative model for a dynamical system given
by
T
(cid:89)
p(y |x )p(x |x ,u ) (15)
k k k kâˆ’1 k
k=1
which is represented by the white nodes in the factor graph shown in Fig.1.
We assume that both the initial and desired final states of this system are
constrained by priors pË†(x |x+) and pË†(x |x+), respectively. These priors are
0 T
generated by a higher-level state x+, and are visualized as orange (initial)
and blue (target) nodes in Fig.1.
At time step k = 0, the agentâ€™s task is to infer a sequence of future
actions u such that the expected posterior q(x |y ) over the final state
1:T T 1:T
closelymatchesthetargetdistributionpË†(x |x+). Inferenceproceedsentirely
T
via spontaneous ("reactive") message passing in the factor graph, without
external orchestration.
10
ğ‘((ğ‘¢%#$) ğ‘((ğ‘¢â€™)
ğ‘¥" # ğ‘¥" #
ğ‘¢$ ğ‘¢% ğ‘¢%#$ ğ‘¢â€™
ğ‘¥!
=
ğ‘¥%&$
=
ğ‘¥%
=
ğ‘¥â€™&$
=
ğ‘¥â€™
ğ‘Ì‚ ğ‘¥!ğ‘¥" #) ğ‘ ğ‘¥%#$ğ‘¥%,ğ‘¢%#$) ğ‘Ì‚ ğ‘¥â€™ğ‘¥" #)
ğ‘ ğ‘¦$ğ‘¥$)
ğ‘¦$ ğ‘¦% ğ‘((ğ‘¥%#$) ğ‘((ğ‘¥â€™)
ğ‘¡
Figure 1: The state of a (layer in an) active inference agent during the
inference process. See section 5.4 for details.
Fig. 1 illustrates the state of this process at time step t, following the
executionofactionsu andtheobservationofoutcomesy . Thegreenand
1:t 1:t
red terminal nodes representing future actions and future states correspond
to epistemic priors. At time t, the systemâ€™s future rollout comprises the
generative model terminated by both epistemic and target priors.
As time progresses, inference within this future model remains active:
green and red epistemic priors are progressively replaced by posterior factors
(depicted as small black boxes), each replacement introducing new opportu-
nities for free energy minimization through message passing. Consequently,
beliefs over the remaining policy u continue to evolve as the system
t+1:T
integrates new information.
This inference mechanism can, in principle, be executed entirely auto-
matically using a reactive message passing toolbox such as RxInfer [Bagaev
etal.,2023]. Weintendtodescribesimulationsofthisprocessinforthcoming
publications.
5.5 Toward Scalable Synthetic Active Inference
The results presented in this paper may also pave the way for scalable and
energy-efficient active inference agents. In particular, we consider how the
main theorem could be extended to support message computation at the
level of individual nodes within a factor graph. This discussion outlines a
promising direction for future research.
In De Vries [2023], we proposed that the implementation of synthetic ac-
tive inference agents should follow the procedure outlined in Algorithm 1. In
essence,theinferenceprocessshouldnotrelyonanyhand-craftedalgorithms
beyond the instruction to react whenever an opportunity arises to minimize
(variational) free energyâ€”provided the agentâ€™s energy budget permits. This
process is to be implemented via reactive message passing in a factor graph,
11
enabling a fully autonomous and distributed inference mechanism.
Algorithm 1 Idealized Implementation of an Active Inference Agent
1: Specify initial model p(y,x,Î¸,u) and priors pË†(x)
2: while true do â–· Deployment loop
React to any free energy minimization opportunity
3:
4: end while
Figure 2: Pseudo-code for realizing a synthetic active inference agent. See
section 5.5 for details.
A key phrase here is â€œreact to any opportunity,â€ which underscores that
the agent (or at a finer level of abstraction, any node in the factor graph)
should only compute anything when there is an actionable opportunity to
minimize free energy. If we interpret the decision to compute (and send) a
variationalmessageversusnotcomputing(andremainingsilent)asanaction
choice, then the EFE of these alternatives can serve as a decision criterion.
A message should only be computed and sent if the EFE associated with
doing so is lower than the EFE of abstaining. The EFE Theorem opens
the door to evaluating the EFE of action choices via standard variational
freeenergyminimizationwithinanappropriatelyextendedgenerativemodel.
Thus, we foresee an energy-efficient and fully autonomous active inference
process, driven solely by localized variational free energy minimization. In
this framework, hand-crafted planning algorithms based on ad hoc pruning
in a tree search process are replaced by an autonomously operating Bayes-
optimalreactiveinferenceprocess. AtoolboxlikeRxInferwould,inprinciple,
be capable of automating this process Bagaev et al. [2023].
5.6 Limitations
Thisworkalsoraisesseveralopenquestions. While,intheory,itispossibleto
rank policy alternatives by EFE through VFE minimization on a generative
modelequippedwithcustomizedepistemicpriors, thecurrentresultsremain
conceptual: implementation details are lacking, and validation through sim-
ulations has yet to be performed. To simplify the mathematical exposition,
we omitted time indices from the variables; however, incorporating time ex-
plicitly, as would be required in a full dynamical system specification, may
introduce additional complexity that warrants further elaboration.
Moreover, although the epistemic priors in (9) are given in closed-form
expressions, their practical implementation and online updating procedures
are not straightforward. In a factor graph framework, for example, mes-
sage passing through nodes representing these epistemic priors would likely
require pre-computation or approximation strategies.
12
In summary, we view the contributions presented in this paper as a con-
ceptual foundation for a line of research aimed at realizing PAI within active
inference agents.
6 Conclusions
We have presented a principled formulation of planning under uncertainty
by casting Expected Free Energy minimization as a problem of variational
inference. Our central result shows that EFE-based policy optimization nat-
urally emerges from minimizing a variational free energy functional defined
over a generative model augmented with preference and epistemic priors.
This formulation restores theoretical alignment with the Free Energy Princi-
ple, resolving previous challenges where planning and inference were treated
as conceptually distinct operations.
By treating all inference, including policy selection, as message pass-
ing in a factor graph, our framework supports scalable, interruptible, and
fully distributed planning. This perspective not only strengthens the the-
oretical foundation of active inference but also opens the door to practical
implementations using reactive message-passing toolkits. These results pave
the way for the design of synthetic active inference agents that are fully
self-organizing and capable of performing Bayes-optimal planning without
relying on handcrafted algorithms.
References
Hagai Attias. Planning by probabilistic inference. In Advances in Neural
Information Processing Systems, volume 16, 2003.
Dmitry Bagaev and Bert de Vries. Reactive Message Passing for Scal-
able Bayesian Inference. Scientific Programming, 2023:e6601690, May
2023. ISSN 1058-9244. doi: 10.1155/2023/6601690. URL https:
//www.hindawi.com/journals/sp/2023/6601690/. Publisher: Hindawi.
Dmitry Bagaev, Albert Podusenko, and Bert De Vries. RxInfer: A Julia
package for reactive real-time Bayesian inference. Journal of Open Source
Software, 8(84):5161, April 2023. ISSN 2475-9066. doi: 10.21105/joss.
05161. URL https://joss.theoj.org/papers/10.21105/joss.05161.
Dimitri Bertsekas. Dynamic Programming and Optimal Control, volume 2,
4th edition. Athena Scientific, 2012.
Bert De Vries. Toward Design of Synthetic Active Inference Agents by
Mere Mortals. CoRR, abs/2307.14145, 2023. doi: 10.48550/ARXIV.2307.
14145. URL https://doi.org/10.48550/arXiv.2307.14145. arXiv:
2307.14145.
13
Karl Friston. The free-energy principle: a unified brain theory? Nature
Reviews Neuroscience, 11(2):127â€“138, 2010.
Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck,
and Giovanni Pezzulo. Active inference and epistemic value. Cognitive
Neuroscience, 6(4):187â€“214, 2015.
Karl Friston, Lancelot Da Costa, Danijar Hafner, Casper Hesp, and Thomas
Parr. Sophisticated Inference. Neural Computation, 33(3):713â€“763, March
2021. ISSN 0899-7667. doi: 10.1162/neco_a_01351. URL https://doi.
org/10.1162/neco_a_01351.
Karl Friston, Lancelot Da Costa, Dalton A. R. Sakthivadivel, Conor Heins,
Grigorios A. Pavliotis, Maxwell Ramstead, and Thomas Parr. Path in-
tegrals, particular kinds, and strange things. Physics of Life Reviews,
47:35â€“62, December 2023. ISSN 1571-0645. doi: 10.1016/j.plrev.2023.
08.016. URL https://www.sciencedirect.com/science/article/pii/
S1571064523001094.
H.J. Kappen, V. GÃ³mez, and M. Opper. Optimal control as a graphical
model inference problem. Machine Learning, 87(2):159â€“182, 2012. doi:
10.1007/s10994-011-5252-8.
Dennis V. Lindley. Bayesian statistics and the design of experiments. The
Annals of Mathematical Statistics, 27(2):568â€“578, 1956. doi: 10.1214/
aoms/1177728069. URL https://projecteuclid.org/euclid.aoms/
1177728069.
Miguel LÃ¡zaro-Gredilla, Li Yang Ku, Kevin P. Murphy, and
Dileep George. What type of inference is planning? In Ad-
vances in Neural Information Processing Systems, 2024. URL
https://proceedings.neurips.cc/paper_files/paper/2024/hash/
d39e3ae9a11b79691709a7a6e06a63d9-Abstract-Conference.html.
Francesco A. N. Palmieri, Krishna R. Pattipati, Giovanni Di Gennaro, Gio-
vanni Fioretti, Francesco Verolla, and Amedeo Buonanno. A Unifying
View of Estimation and Control Using Belief Propagation With Applica-
tion to Path Planning. IEEE Access, 10:15193â€“15216, 2022. ISSN 2169-
3536. doi: 10.1109/ACCESS.2022.3148127.
Thomas Parr, Giovanni Pezzulo, and Karl Friston. Active Inference: The
Free Energy Principle in Mind, Brain, and Behavior. MIT Press, 2022.
Aswin Paul, Takuya Isomura, and Adeel Razi. On predictive planning
and counterfactual learning in active inference. Entropy, 26(6), 2024.
ISSN 1099-4300. doi: 10.3390/e26060484. URL https://www.mdpi.com/
1099-4300/26/6/484.
14
KonradRawlik,MarcToussaint,andSethuVijayakumar. Stochasticoptimal
control as approximate inference: A new perspective. Proceedings of the
International Conference on Machine Learning (ICML), 2013.
AlecSolwayandMatthewMBotvinick. Optimalbehavioralhierarchy. PLoS
Computational Biology, 8(10):e1002774, 2012.
Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Intro-
duction, 2nd edition. MIT Press, Cambridge, MA, 2018.
Emanuel Todorov. Linearly-solvable markov decision problems. Advances in
neural information processing systems, 19:1369â€“1376, 2006.
MarcToussaint. Robottrajectoryoptimizationusingapproximateinference.
In Proceedings of the 26th Annual International Conference on Machine
Learning, pages 1049â€“1056. ACM, 2009.
Thijs van de Laar, Magnus Koudahl, and Bert de Vries. Realizing Synthetic
Active Inference Agents, Part II: Variational Message Updates. Neural
Computation, pages 1â€“38, September 2024. ISSN 0899-7667. doi: 10.
1162/neco_a_01713. URL https://doi.org/10.1162/neco_a_01713.
Ä°smail ÅenÃ¶z, Thijs van de Laar, Dmitry Bagaev, and Bert de Vries. Vari-
ational Message Passing and Local Constraint Manipulation in Factor
Graphs. Entropy (Basel, Switzerland), 23(7):807, June 2021. ISSN 1099-
4300. doi: 10.3390/e23070807.
A Proof of the Main Theorem
Proof of Theorem 1.
(cid:20) (cid:21)
q(yxÎ¸u)
F[q] = E log (16a)
q(yxÎ¸u) p(yxÎ¸u)pË†(x)pËœ(u)pËœ(x)pËœ(yx)
(cid:20) (cid:21)
= E log q(u) +E (cid:2) log q(yxÎ¸|u) (cid:3) (16b)
q(u) p(u) q(yxÎ¸|u) p(yxÎ¸|u)pË†(x)pËœ(u)pËœ(x)pËœ(yx)
(cid:124) (cid:123)(cid:122) (cid:125)
B(u)
(cid:20) (cid:21)
= E log q(u) +G(u)+E (cid:2) log q(yxÎ¸|u)(cid:3) (16c)
q(u) p(u) q(yxÎ¸|u) p(yxÎ¸|u)
(cid:124) (cid:123)(cid:122) (cid:125)
B(u)if (9)holds
(cid:20) (cid:21)
= E (cid:2) G(u) (cid:3) +E log q(yxÎ¸u) if (9) holds (16d)
q(u) q(yxÎ¸u) p(yxÎ¸u)
15
Intheabovederivation,westillneedtoprovethetransitionforB(u)from
(16b) to (16c), which we address next. In the following, all expectations are
with respect to q(yxÎ¸|u) unless otherwise indicated.
Lemma 1 (Proof of equivalence B(u) in (16b) and (16c)).
posterior
(cid:122) (cid:125)(cid:124) (cid:123)
(cid:20) (cid:21)
q(yxÎ¸|u)
B(u) = E log (17a)
p(yxÎ¸|u)pË†(x)pËœ(u)pËœ(x)pËœ(yx)
(cid:124) (cid:123)(cid:122) (cid:125)(cid:124)(cid:123)(cid:122)(cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
predictive utility epistemic priors
(cid:20) (cid:18) (cid:19)(cid:21)
q(x|u) 1 q(Î¸|x)
= E log Â· Â· + (17b)
pË†(x) q(y|x) q(Î¸|yx)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
risk ambiguity âˆ’novelty
(cid:124) (cid:123)(cid:122) (cid:125)
G(u)=Expected Free Energy
(cid:20) (cid:18) (cid:19)(cid:21)
pË†(x)q(y|x)q(Î¸|yx) q(yxÎ¸|u)
+E log Â·
q(x|u)q(Î¸|x) p(yxÎ¸|u)pË†(x)pËœ(u)pËœ(x)pËœ(yx)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
inverse factors from G(u) factors from (17a)
(cid:20) (cid:21) (cid:20) (cid:21)
q(yxÎ¸|u) q(y|x)q(Î¸|yx)
= G(u)+E log +E log (17c)
p(yxÎ¸|u) q(x|u)q(Î¸|x)pËœ(u)pËœ(x)pËœ(yx)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
=C(u) choose epistemic priors to let this vanish
= G(u)+C(u)+ (17d)
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
1 q(y|x) q(Î¸|yx)
+E log +E log +E log
q(x|u)pËœ(u) pËœ(x) q(Î¸|x)pËœ(yx)
= G(u)+C(u)+ (17e)
(cid:18) (cid:19)
(cid:88) (cid:88) (cid:88)
+ q(yÎ¸|x) âˆ’ q(x|u)logq(x|u)âˆ’ q(x|u)logpËœ(u) (17f)
yÎ¸ x x
(cid:124) (cid:123)(cid:122) (cid:125)
=H[q(x|u)]
(cid:124) (cid:123)(cid:122) (cid:125)
=0 if pËœ(u)=exp(H[q(x|u)])
(cid:18) (cid:19)
(cid:88) (cid:88) (cid:88)
+ q(x|u) q(y|x)logq(y|x)âˆ’ q(y|x)logpËœ(x)
x y y
(cid:124) (cid:123)(cid:122) (cid:125)
=âˆ’H[q(y|x)]
(cid:124) (cid:123)(cid:122) (cid:125)
=0 if pËœ(x)=exp(âˆ’H[q(y|x)])
(cid:18) (cid:19)
(cid:88) (cid:88) q(Î¸|yx) (cid:88)
+ q(yx|u) q(Î¸|yx)log âˆ’ q(Î¸|yx)logpËœ(yx)
q(Î¸|x)
yx Î¸ Î¸
(cid:124) (cid:123)(cid:122) (cid:125)
D[q(Î¸|yx),q(Î¸|x)]
(cid:124) (cid:123)(cid:122) (cid:125)
=0 if pËœ(yx)=exp(D[q(Î¸|yx),q(Î¸|x)])
(cid:20) (cid:21)
q(yxÎ¸|u)
= G(u)+E log if (9) holds. (17g)
q(yxÎ¸|u) p(yxÎ¸|u)
16
The term in brackets in 17f works out as follows:
(cid:88) (cid:88)
âˆ’ q(x|u)logq(x|u)âˆ’ q(x|u)logpËœ(u) (18a)
x x
= H[q(x|u)]âˆ’logpËœ(u) (18b)
(cid:40)
0 if pËœ(u) = exp(H[q(x|u)])
= (18c)
const if pËœ(u) = Ïƒ(H[q(x|u)])
where const = log(Î£ exp(H[q(x|uâ€²)]) and
uâ€²
exp(H[q(x|u)])
Ïƒ(H[q(x|u)]) â‰œ (19)
Î£ exp(H[q(x|uâ€²)]
uâ€²
is the (normalized) softmax function. Similar derivations apply to the other
epistemic priors in (9).
InthecontextofvariationalinferencewithvariationalfreeenergyF[q]as
in (8), the normalization of pËœ(u) is inconsequential, as the additive constant
does not affect the location of the minimum of F[q]. As long as pËœ(u) âˆ
exp(H[q(x|u)]), the results of VFE minimization will be the same.
B The Entropy and Kullback-Leibler Divergence
In (9a), the entropy of the conditional distribution q(x|u) is defined as
(cid:88)
H[q(x|u)] = âˆ’ q(x|u)logq(x|u). (20)
x
NotethatH[q(x|u)]isafunctionofuandthereforeÏƒ(H[q(x|u)])canserveas
aprobabilitydistributionoveru. H[q(x|u)]isnotthesameastheconditional
entropy Hâ€²[q(x|u)], which is a scalar, defined as
(cid:88)
Hâ€²[q(x|u)] â‰œ âˆ’ q(xu)logq(x|u) = E [H[q(x|u)]] . (21)
q(u)
(cid:124) (cid:123)(cid:122) (cid:125)
xu
conditional
entropy
For two given distributions q(Î¸|yx) and q(Î¸|x), the Kullback-Leibler di-
vergence is defined as
(cid:88) q(Î¸|yx)
D[q(Î¸|yx),q(Î¸|x)] â‰œ q(Î¸|yx)log
q(Î¸|x)
Î¸
(cid:88) q(yÎ¸|x)
= q(Î¸|yx)log , (22)
q(y|x)q(Î¸|x)
Î¸
which is a function of y and x. Note that (22) is not the same as, but is
close to the mutual information between y and Î¸, given x, which is a scalar
17
value defined as
(cid:88) q(yÎ¸|x)
I[y,Î¸|x] â‰œ q(yxÎ¸)log
q(y|x)q(Î¸|x)
yxÎ¸
(cid:88)
= q(yx)D[q(Î¸|yx),q(Î¸|x)]
yx
= E [D[q(Î¸|yx),q(Î¸|x)]] . (23)
q(yx)
Note the similarity between (21) and (23).
18

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Expected Free Energy-based Planning as Variational Inference"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   âŒ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   âœ… GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   âŒ BAD: Repeating the same claim 3+ times with slight variations
   âœ… GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
