# Boosting MCTS with Free Energy Minimization

**Authors:** Mawaba Pascal Dao, Adrian M. Peter

**Year:** 2025

**Source:** arxiv

**Venue:** N/A

**DOI:** N/A

**PDF:** [dao2025boosting.pdf](../pdfs/dao2025boosting.pdf)

**Generated:** 2025-12-13 20:55:34

**Validation Status:** ✓ Accepted
**Quality Score:** 1.00

---

### Boosting MCTS with Free Energy Minimization

### OverviewThis paper investigates a novel planning framework integrating Monte Carlo Tree Search (MCTS) with active inference objectives to systematically reduce epistemic uncertainty while pursuing extrinsic rewards. The core insight is that MCTS, already renowned for its search efficiency, can be naturally extended to incorporate free energy minimization by blending expected rewards with information gain. Specifically, the Cross-Entropy Method (CEM) is employed to optimize action proposals at the root node, while tree expansions leverage reward modeling alongside intrinsic exploration bonuses. This synergy allows the planner to maintain coherent estimates of value and uncertainty throughout planning, without sacrificing computational tractability.### MethodologyThe authors propose a framework that addresses key challenges in planning under uncertainty, including computational complexity and the need for reliable value estimation. The framework is built upon the principles of active inference, which posits that agents minimize a quantity called free energy, representing how well an internal generative model predicts observations. The free energy is defined as: F(Q,y) = D [Q(x)∥P(x|y)]−lnP(y), where Q(x) is the approximate posterior distribution over hidden states, P(x|y) is the true posterior distribution, and lnP(y) is the log evidence. The authors emphasize that the key innovation lies in integrating MCTS with this framework.The Cross-Entropy Method (CEM) is used to optimize action proposals at the root node. CEM is an iterative optimization algorithm that samples candidate solutions from a probability distribution and then updates the distribution based on the quality of the sampled solutions. In this context, CEM is used to fit a Gaussian action distribution at the root node, where the mean of the distribution is the action proposed and the variance is determined by the uncertainty in the action. The authors also incorporate intrinsic exploration by integrating epistemic value (Information Gain) into the planning process. The Information Gain is defined as the expected reduction in uncertainty about the environment’s dynamics when taking an action. This dual exploration mechanism, achieved through both the expected free energy criterion and MCTS exploration, improves the agent’s ability to navigate high-dimensional continuous domains.The authors describe the MCTS algorithm as follows: Selection: Starting from the root node, the algorithm traverses the tree by selecting child nodes based on the Upper Confidence Bounds (UCB) policy. Expansion: If the leaf node does not represent a terminal state, one or more child nodes are added to the tree, representing unexplored actions. Simulation: A simulation, or rollout, is performed from the expanded node, where actions are sampled according to the root action distribution. Backpropagation: The results of the simulation are propagated back up the tree, updating the values and visit counts of all nodes.### ResultsThe authors benchmark their planner on a diverse set of continuous control tasks, demonstrating performance gains over both stand-alone CEM and MCTS with random rollouts. Specifically, in the Pendulum environment, the planner consistently achieves performance comparable to CEM, highlighting the synergy between the two approaches. The authors state: "Our key insight is that MCTS—already renowned for its search efficiency—can be naturally extended to incorporate free energy minimization by blending expected rewards with information gain."The authors further report that the framework successfully balances exploration and exploitation, enabling the agent to efficiently navigate complex environments. The authors note: "This synergy allows our planner to maintain coherent estimates of value and uncertainty throughout planning, without sacrificing computational tractability."In the Sparse Mountain Car environment, the planner demonstrates a significant improvement over both CEM and MCTS with random rollouts, showcasing the effectiveness of the framework in sparse reward settings. The authors state: "Our approach incorporates intrinsic motivation—count-based exploration strategies—to address the challenges of sparse rewards."The authors also report that the framework is scalable and can be applied to a wide range of continuous control tasks. The authors state: "The integration of MCTS with active inference enables sophisticated planning under uncertainty, facilitating the agent’s ability to balance exploration and exploitation."### DiscussionThe authors highlight the importance of reducing epistemic uncertainty in planning under uncertainty. The framework’s ability to maintain coherent estimates of value and uncertainty throughout planning is a key factor in its success. The framework’s dual exploration mechanism, achieved through both the expected free energy criterion and MCTS exploration, improves the agent’s ability to navigate high-dimensional continuous domains. The authors emphasize that the integration of MCTS with active inference enables sophisticated planning under uncertainty.The authors acknowledge that the computational demands of planning in continuous spaces can be significant, and that ensuring reliable value estimation during tree-based search is a key challenge. The framework addresses this challenge by incorporating the free energy minimization objective, which provides a principled way to balance exploration and exploitation. The authors also extend these methods to practical applications and benchmarks beyond example problems.The authors propose a novel framework that integrates MCTS with Active 

Inference to address these challenges. 

Our approach incorporates intrinsic motivation—count-based exploration strategies—to address the challenges of sparse rewards
