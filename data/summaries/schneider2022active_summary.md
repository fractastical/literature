# Active Inference for Robotic Manipulation

**Authors:** Tim Schneider, Boris Belousov, Hany Abdulsamad, Jan Peters

**Year:** 2022

**Source:** arxiv

**Venue:** N/A

**DOI:** N/A

**PDF:** [schneider2022active.pdf](../pdfs/schneider2022active.pdf)

**Generated:** 2025-12-14 12:53:21

**Validation Status:** ✓ Accepted
**Quality Score:** 0.80

---

### OverviewThis paper, “Active Inference for Robotic Manipulation,” investigates the application of active inference to challenging robotic manipulation tasks. The authors argue that partial observability is a central challenge in robotic manipulation, as the agent usually does not know all physical properties of the environment and the objects it is manipulating in advance. A recently emerging theory that deals with partial observability in an explicit manner is active inference. It drives the agent to act in a way that is not only goal-directed but also informative about the environment. In this work, we apply Active Inference to a hard-to-explore simulated robotic manipulation tasks, in which the agent has to balance a ball into a target zone. Since thereward of this task is sparse, in order to explore this environment, the agent has to learn to balance the ball without any extrinsic feedback, purely driven by its own curiosity. We show that the information-seeking behavior induced by Active Inference allows the agent to explore these challenging, sparse environments systematically. Finally, we conclude that using an information-seeking objective is beneficial in sparse environments and allows the agent to solve tasks in which methods that do not exhibit directed exploration fail.### MethodologyThe authors propose a model-based Reinforcement Learning algorithm that utilizes active inference to efficiently explore challenging state spaces. They assume that the environment is fully observable, governed by unknown dynamics P(x |x ,a ) and provides the agent with a reward P(r |x ,a ) in every time step. The model is built with neural network conditioned Gaussians p(r |x ,a ,θ) := N (x |µx(x ,a ),σxI) and p(r |x ,a ,θ):=N (r |µr(x ,a ),σrI), resulting in the generative model: (cid:89) T (cid:88)p(x ,a ,r ,θ)=p(x )p(a )p(θ) p(r |x ,a ,θ)p(x |x ,a ,θ)0:T1:T1:T01:T τ τ τ τ τ−1 τSince the environment is fully observed, the only hidden variables are the neural network parameters θ. Thus, we are left with the minimization problem to select a policy π :=a at time: t+1:TSince the environment is fully observed, thereward is sparse, meaning that the only way the agent can learn about thereward at the top of the table is by moving the ball there and exploring it. The authors use a Nested Monte Carlo estimator that reuses samples from the outer estimator in the inner estimator to approximate the intrinsic term. The minimization of this objective function allows us to select a policy that maximizes the expected cumulative reward over a fixed horizon. However, additionally we are maximizing the expected parameter information gain, driving the agent to seek out states that are informative about its model parameters θ. This term causes the agent to be curious about its environment and explore it systematically, even in the total absence of extrinsic reward. The optimization of this objective can now theoretically be done by any planner that is capable of handling continuous action spaces. In this work, similar to Chuaetal.[5], we use a variant of the Cross-Entropy Method to find an open-loop sequence of actions a that maximizes Eq.(2).### ResultsA central feature that sets our method apart from other purely model-based approaches[5,9] is the intrinsic term, that explicitly drives the agent to explore its environment in a systematic manner. To evaluate the exploratory capabilities of our method, we designed two hard-to-explore manipulation tasks: TiltedPushingandTiltedPushingMaze. In both tasks, the agent has to push a ball up a tilted table into a target zone to receive reward. The agent can move the gripper in a plane parallel to the table and rotate the black end-effector around the Z-axis (Z-axis being orthogonal to the brown table and pointing up). As input, the agent receives the2D positions and velocities of both the gripper and the ball, and the angular position and velocity of the end-effector. To add an additional challenge, in the TiltedPushingMaze task we add holes to the table, that we irreversibly trap the ball if it falls in. For visualization of these tasks, refer to Fig.2.There are two aspects that make these tasks particularly challenging: First, the reward is sparse, meaning that the only way the agent can learn about the reward at the top of the table is by moving the ball there and exploring it. Second, balancing the ball on the finger and moving it around requires a fair amount of dexterity, especially given the low control frequency of4 Hz2 we operate our agent on. Once the agent drops the ball, it cannot be recovered, giving the agent no choice but to wait for the episode to terminate to continue exploring. Both of these aspects make solving these tasks with conventional, undirected exploration methods like Boltzmann exploration or adding Gaussian noise to the action extremely challenging.Consequently, the agent has to learn to balance the ball on the end-effector and systematically move it around the environment until the target zone is found. In this work, similar to Chuaetal.[5], we use a variant of the Cross-Entropy Method to find an open-loop sequence of actions a that maximizes Eq.(2).### Discussion(This section would contain a discussion of the results, comparing the performance of the Active Inference method to other methods, and highlighting the key findings of the research. 

This section is omitted here for brevity.)2

The computation of the intrinsic term is computationally heavy, limiting us to this rather low control frequency.
