# Relative representations for cognitive graphs

**Authors:** Alex B. Kiefer, Christopher L. Buckley

**Year:** 2023

**Source:** arxiv

**Venue:** N/A

**DOI:** N/A

**PDF:** [kiefer2023relative.pdf](../pdfs/kiefer2023relative.pdf)

**Generated:** 2025-12-14 11:21:49

**Validation Status:** ✓ Accepted
**Quality Score:** 0.80

---

=== IMPORTANT: ISOLATE THIS PAPER ===You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.Do NOT mix information from different papers. Only use information from THIS specific paper.Paper Title: Relative representations for cognitive graphsCitation Key: kiefer2023relativeAuthors: Alex B. Kiefer, Christopher L. BuckleyYear:2023Key Terms: representations, relative, university, latent, sussex, spaces, learned, across, cognitive, graphs=== FULL PAPER TEXT ===Relative representations for cognitive graphsAlex B. Kiefer1,2 and Christopher L. Buckley1,31 VERSES Research Lab2 Monash University3 Sussex AI Group, Department of Informatics, University of SussexAbstract. Although the latent spaces learned by distinct neural networks are not generally directly comparable, even when model architectureandtrainingdataareheldfixed,recentworkinmachinelearning[13]hasshownthatitispossibletousethesimilaritiesanddifferencesamonglatent space vectors to derive “relative representations” with comparable representational power to their “absolute” counterparts, and which are nearly identical across models trained on similar data distributions. Apart from their intrinsic interest in revealing the underlying structure of learned latent spaces, relative representations are useful to compare representations across networks as a generic proxy for convergence, and for zero-shot model stitching [13].In this work we examine an extension of relative representations to discrete state-space models, using Clone-Structured Cognitive Graphs (CSCGs) [16] for2D spatial localization and navigation as a test case in which such representations may be of some practical use. Our work showsthattheprobabilityvectorscomputedduringmessagepassingcan be used to define relative representations on CSCGs, enabling effective communication across agents trained using different random initializationsandtrainingsequences,andononlypartiallysimilarspaces.Intheprocess,weintroduceatechniqueforzero-shotmodelstitchingthatcan be applied post hoc, without the need for using relative representations duringtraining.Thisexploratoryworkisintendedasaproof-of-concept for the application of relative representations to the study of cognitive maps in neuroscience and AI.Keywords: Clone-structuredcognitivegraphs·Relativerepresentations· Representational similarity1 IntroductionIn this short paper we explore the application of relative representations [13] to discrete (graph-structured) models of cognition in the hippocampal-entorhinal system — specifically, Clone-Structured Cognitive Graphs (CSCGs) [16]. In the first two sections we introduce relative representations and their extension to discrete latent state spaces via continuous messages passed on graphs. We then introduceCSCGsandtheiruseinSLAM(SimultaneousLocalizationAndMapping) . Finally, we report preliminary experimental results using relative representations on CSCGs showing that (a) relative representations can indeed be applied successfully to model the latent space structure of discrete, graph-like representationssuchasCSCGs,andmoregenerallyPOMDPsashasbeenemployed in discrete active inference modeling [1,8,19]; (b) comparison of agents across partially disparate environments reveals important shared latent space structure; and (c) it is possible to use the messages or beliefs (probabilities over states) of one agent to reconstruct the corresponding belief distributions of another via relative representations, without requiring the use of relative representations during training. These examples illustrate an extension of existing representational analysis techniques developed within neuroscience [10], whichwehopewillproveapplicabletothestudyofcognitivemapsinbiologicalagents.2 Relative representationsRelative representation [13]isa technique recentlyintroducedin machinelearning that allows one to map the intrinsically distinct continuous latent space representations of different models to a common shared representation identical (or nearly so) across the source models, so that latent spaces can be directly compared,evenwhenderivedfrommodelswithdifferentarchitectures.Thetechnique is conceptually simple: given anchor points A = [x ,x ,...,x ] sampled from a data or observation space and some similarity function sim (e.g. cosine similarity)4,therelativerepresentationrM ofdatapointx withrespecttomodel i iencM irM =[sim(eM,eM),sim(eM,eM),...,sim(eM,eM )] (1)i i a1 i a2 i aNwhere eM is the latent representation of anchor i in M.aiCrucially, the anchor points A must be matched across models in order for their relative representations to be compatible. “Matching” is in the simplest case simply identity, but there are cases in which it is feasible to use pairs of anchors related by a map g(x)→y (see below).In [13] it is shown that the convergence of a model M during trainingtargetis well predicted by the average cosine similarity between its relative representations of datapoints and those of an independently validated reference modelM . This is to be expected, given that there is an optimal way of partitioningrefthe data for a given downstream task, and that distinct models trained on the same objective approximate this optimal solution more or less closely, subject to variable factors like random initialization and hyperparameter selection.While relative representations were recently introduced in machine learning,theytaketheirinspirationinpartfrompriorworkonrepresentationalsimilarityanalysis (RSA) in neuroscience [10,4]. Indeed, there is a formal equivalence between relative representations and the Representational Dissimilarity Matrices(RDMs)proposedasacommonformatforrepresentingdisparatetypesofneuro-scientificdata(includingbrainimagingmodalitiesaswellassimulatedneuronalactivities in computational models) in [10]. Specifically, if a similarity rather than dissimilarity metric is employed5, then each row (or, equivalently, column) of the RDM used to characterize a representational space is, simply, a relative representation of the corresponding datapoint.Arguably the main contribution of [13] is to exhibit the usefulness of thistechnique in machine learning, where relative representations may be employedasanoveltypeoflatentspaceinmodelarchitectures.Givenalargeenoughsam-ple of anchor points, relative representations bear sufficient information to playfunctional roles similar to those of the “absolute” representations they model,rather than simply functioning as an analytical tool (e.g. to characterize thestructure of latent spaces and facilitate abstract comparisons among systems).The most obvious practical use of relative representations is in enabling “latent space communication”: Moschella et al [13] show that the projection of embeddings from distinct models onto the same relative representation enables “zero-shot model stitching”, in which for example the encoder from one trained model can be spliced to the decoder from another (with the relative representation being the initial layer supplied as input to the decoder). A limitation of this procedure is that it depends on using a relative representation layer during training, precluding its use for establishing communication between “frozen” pretrained models. Below, we make use of a parameter-free technique that allows one to map from the relative representation space back to the “absolute” representations of the input models with some degree of success.3 Extending relative representations to discretestate-space modelsDespite the remarkable achievements of continuous state-space models in deeplearning systems, discrete state spaces continue to be relevant, both in machinelearningapplications,wherediscrete“worldmodels” areresponsibleforstate-of-the-art results in model-based reinforcement learning [6], and in neuroscience,where there is ample evidence for discretized, graph-like representations, forexample in the hippocampal-entorhinal system [25,18,16] and in models ofdecision-makingprocessesthatleveragePOMDPs(PartiallyObservableMarkovDecision Processes) [19].While typical vector similarity metrics such as cosine distance behave in asomewhatdegeneratewaywhenappliedtomanytypesofdiscreterepresentations(e.g., the cosine similarity between two one-hot vectors in the same space is1 ifthe vectors are identical and0 otherwise), they can still be usefully applied inthis case (see section5 below). More generally, the posterior belief distributionsinferredoverdiscretestatespacesduringsimulationsinagent-basedmodelsmayprovide suitable anchor points for constructing relative representations.Concretely, such posterior distributions are often derived using message-passingalgorithms,suchasbeliefpropagation[14]orvariationalmessagepassing[27]. We pursue such a strategy for deriving relative representations of a specialkindofhiddenMarkovmodel(theClone-StructuredHiddenMarkovModelor(ifsupplemented with actions) Cognitive Graph [16]), in which it is simple to com-pute forward messages which at each discrete time-step give the probability ofthe hidden states z conditioned on a sequence of observations o (i.e. P(z |o )).t1:tThe CSCG/CHMM is particularly interesting both because of its fidelity as amodel of hippocampal-entorhinal representations in the brain and because, asin the case of neural networks, distinct agents may learn superficially distinctCSCGs that nonetheless form nearly isomorphic cognitive maps, as shown below.4 SLAM using Clone-Structured Cognitive GraphsAn important strand of research in contemporary machine learning and compu-tationalneurosciencehasfocusedonunderstandingtherole
