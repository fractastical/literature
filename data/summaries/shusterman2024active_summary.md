# An Active Inference Strategy for Prompting Reliable Responses from Large Language Models in Medical Practice

**Authors:** Roma Shusterman, Allison C. Waters, Shannon O`Neill, Phan Luu, Don M. Tucker

**Year:** 2024

**Source:** arxiv

**Venue:** N/A

**DOI:** N/A

**PDF:** [shusterman2024active.pdf](../pdfs/shusterman2024active.pdf)

**Generated:** 2025-12-14 03:49:09

**Validation Status:** ⚠ Rejected
**Quality Score:** 0.00
**Validation Errors:** 1 error(s)
  - Severe repetition detected: Same phrase appears 23 times (severe repetition)

---

### An Active Inference Strategy for Prompting Reliable Responses from Large Language Models in Medical 

Practice – 

Summary

### OverviewThis paper investigates the potential of Large Language Models (LLMs) for medical applications, highlighting their capacity to intuitively access and use medical knowledge in many contexts, including education and treatment. The authors recognize that LLMs, while promising, face challenges regarding reliability and safety, particularly concerning the generation of inaccurate or harmful responses, and cannot be regulated to assure quality control. If these issues could be corrected, optimizing LLM technology could benefit patients and physicians by providing affordable, point-of-care medical knowledge. This structured framework refines LLM responses by restricting their primary knowledge base to domain-specific datasets containing validated medical information, alongside methods to enhance domain-specific document processing for accurate model inputs. Additionally, we introduce an actor-critic LLM prompting architecture reflecting the dual roles of a Therapist agent and a Supervisor agent, mirroring the roles of a clinician and a supervisor. This approach leverages the inherent associative nature of LLMs, aligning with the theory of active inference, which posits that human cognition involves generating predictions and correcting them based on sensory feedback. We conducted a validation study where expert cognitive behavior therapy for insomnia (CBT-I) therapists evaluated responses from the LLM in a blind format. Experienced human CBT-I therapists rated all three responses (Therapist agent, Supervisor agent, and VSC) on a1-to-5 Likert scale. Patient queries (N =100) were generated by two clinical psychologists experienced with patient interactions in psychotherapy and behavior therapy settings. The findings demonstrated that LLM responses were rated highly, frequently surpassing the ratings of therapist-generated appropriate responses.### MethodologyThe core of our approach is an actor-critic LLM architecture, where the Therapist agent initially responds to patient queries, and the Supervisor agent evaluates and adjusts responses to ensure accuracy and reliability. The framework is built around Retrieval-Augmented Generation (RAG), which integrates verified medical information from domain-specific databases into the LLM’s response generation process. The RAG architecture utilizes a vector database to store and retrieve relevant information, enabling more accurate and contextually relevant responses. The core of the framework is built around active inference principles, mirroring the dual roles of a clinician and a supervisor. The system leverages the inherent associative nature of LLMs, aligning with the theory of active inference, which posits that human cognition involves generating predictions and correcting them based on sensory feedback. We first provide a brief review of existing methods for improving the contextual knowledge base for a domain-specific application, for optimizing the use of the domain-specific knowledge base, and for integrating retrieval mechanisms.### ResultsThe extensive information within general-purpose LLMs can provide relevant and useful medical information but can also produce meaningless, inappropriate, or even bizarre responses. The authors state: “The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "The authors state: "

The authors state: "

The authors state: "

The authors state: "

The authors state
