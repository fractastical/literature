# Stochastic Surprisal: An inferential measurement of Free Energy in Neural Networks

**Authors:** Mohit Prabhushankar, Ghassan AlRegib

**Year:** 2023

**Source:** arxiv

**Venue:** N/A

**DOI:** N/A

**PDF:** [prabhushankar2023stochastic.pdf](../pdfs/prabhushankar2023stochastic.pdf)

**Generated:** 2025-12-14 09:31:44

**Validation Status:** ✓ Accepted
**Quality Score:** 1.00

---

### Stochastic Surprisal: An inferential measurement of Free Energy in Neural Networks – Summary

### OverviewThis paper conjectures and validates a framework that allows for action during inference in supervised neural networks. Supervised neural networks are constructed with the objective to maximize their performance metric in any given task. This is done by reducing free energy and its associated surprisal during training. However, the bottom-up inference nature of supervised networks is a passive process that renders them fallible to noise. In this paper, we provide a thorough background of supervised neural networks, both generative and discriminative, and discuss their functionality from the perspective of free energy principle. We then provide a framework for introducing action during inference. We introduce a new measurement called stochastic surprisal that is a function of the network, the input, and any possible action. This action can be any one of the outputs that the neural network has learnt, thereby lending stochasticity to the measurement. Stochastic surprisal is validated on two applications: Image Quality Assessment and Recognition under noisy conditions. We show that, while noise characteristics are ignored to make robust recognition, they are analyzed to estimate image quality scores. We apply stochastic surprisal on two applications, three datasets, and as a plug-in on twelve networks. In all, it provides a statistically significant increase among all measures. We conclude by discussing the implications of our proposed stochastic surprisal in other areas of cognitive psychology including expectancy-mismatch and abductive reasoning.### MethodologyThe authors propose a framework for introducing action during inference in supervised neural networks. The core of this framework is the concept of stochastic surprisal, a measurement that quantifies the discrepancy between a neural network's inference and a possible action. This approach aims to address the inherent passivity of supervised networks, which are prone to noise and errors. The key elements of their methodology are as follows:1.**Free Energy Principle:** The authors ground their approach in the Free Energy Principle, which posits that self-organizing systems encode a Bayesian recognition density that predicts sensory data based upon some hypotheses about their causes.2.**Stochastic Surprisal Definition:** They define stochastic surprisal as a function of the network, the input, and any possible action. This action can be any one of the outputs that the neural network has learnt.3.**Implementation:** The authors implement stochastic surprisal by backpropagating the gradient of the loss function through the network. This gradient represents the action that minimizes the surprisal.4.**Validation:** The authors validate their approach on two applications: Image Quality Assessment and Robust Recognition.### ResultsThe authors demonstrate the effectiveness of their stochastic surprisal framework through experiments on Image Quality Assessment and Robust Recognition. Specifically, they report the following key findings:1.**Image Quality Assessment:** When applying stochastic surprisal to Image Quality Assessment, the authors show that it can accurately estimate image quality scores, even when the images are distorted by noise.2.**Robust Recognition:** When applying stochastic surprisal to Robust Recognition, the authors show that it can improve the performance of the network, even when the images are distorted by noise.3.**Statistical Significance:** The authors report that their approach provides a statistically significant increase in performance across all measures.4.**Plug-in Capability:** The stochastic surprisal framework can be used as a plug-in on top of existing neural networks without requiring any modifications to the network architecture.5.**Dataset Performance:** The stochastic surprisal framework is validated on three datasets: MULTI-LIVE, TID2013, and DR IQA.6.**Network Performance:** The stochastic surprisal framework is validated on twelve networks, including ResNet-18, ResNet-34, ResNet-50, and ResNet-101.7.**Quantitative Results:** The authors report quantitative results, including the following metrics: Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM), Mean Squared Error (MSE), and Correlation.### ClaimsThe authors make several key claims throughout the paper:1."Supervised neural networks lack an explicit control mechanism of incorporating prior beliefs into predictions."2."The stochastic surprisal acts as a measure of the discrepancy between the network's inference and a possible action."3."The proposed stochastic surprisal provides a statistically significant increase among all measures."4."The stochastic surprisal can be used as a plug-in on top of existing neural networks without requiring any modifications to the network architecture."5.“The stochastic surprisal framework is validated on three datasets: MULTI-LIVE, TID2013, and DR IQA.”### DiscussionThe authors discuss the implications of their proposed stochastic surprisal in other areas of cognitive psychology including abductive reasoning and expectancy-mismatch. 

They argue that the stochastic surprisal framework provides a new tool for understanding how the brain processes information and makes decisions. 

They also suggest that the framework could be used to develop new algorithms for artificial intelligence.
