# A Framework for Inherently Safer AGI through Language-Mediated Active Inference

**Authors:** Bo Wen

**Year:** 2025

**Source:** arxiv

**Venue:** N/A

**DOI:** N/A

**PDF:** [wen2025framework.pdf](../pdfs/wen2025framework.pdf)

**Generated:** 2025-12-13 22:55:17

**Validation Status:** ✓ Accepted
**Quality Score:** 0.80

---

### OverviewThis paper investigates the development of safe Artificial General Intelligence (AGI) through a novel framework combining Active Inference principles with Large Language Models (LLMs). The authors argue that traditional AI safety approaches, relying on post-hoc interpretability and reward engineering, are fundamentally limited. Instead, they propose an architecture where safety is integrated directly into the system’s design, leveraging transparent belief representations and hierarchical value alignment. The framework utilizes LLMs to represent and manipulate beliefs, enabling direct human oversight while maintaining computational tractability. The core of the approach is based on minimizing variational free energy (VFE) to balance exploration and exploitation, a key tenet of Active Inference. The system employs a multi-agent architecture, with agents self-organizing according to Active Inference principles, using Markov blankets to ensure modularity and robustness. The system’s key mechanisms include explicit separation of beliefs and preferences in natural language, bounded rationality through resource-aware free energy minimization, and compositional safety through modular agent structures. The authors highlight the importance of the system’s ability to learn and adapt in novel environments, while maintaining a stable, predictable world model.### MethodologyThe authors’ proposed framework centers on integrating LLMs with Active Inference (AIF) to create a safer AGI. The system’s key mechanisms include: (1) explicit separation of beliefs and preferences in natural language, (2) bounded rationality through resource-aware free energy minimization, and (3) compositional safety through modular agent structures. The authors describe the system’s architecture in detail, outlining the four core generative components: Observation Model (A), Transition Model (B), Preferences (C), and Initial Beliefs (D). The authors emphasize the use of natural language to represent and manipulate these components, enabling direct human oversight while maintaining computational tractability. The system’s implementation leverages LLMs to generate and refine these components, allowing for continuous learning and adaptation. The authors highlight the importance of the system’s ability to learn and adapt in novel environments, while maintaining a stable, predictable world model. The authors also discuss the system’s use of resource-aware free energy minimization to ensure that the system’s actions are aligned with its goals and constraints.### ResultsThe authors present a conceptual framework for achieving inherently safer AGI through LLM-mediated Active Inference. They identify several key mechanisms that contribute to this safety, including the separation of beliefs and preferences in natural language, bounded rationality through resource-aware free energy minimization, and compositional safety through modular agent structures. The authors argue that these mechanisms are essential for preventing the emergence of unintended behaviors and ensuring that the AGI remains aligned with human values. The authors also discuss the system’s use of resource-aware free energy minimization to ensure that the system’s actions are aligned with its goals and constraints. The authors propose experiments to validate the framework’s safety properties, focusing on the Abstraction and Reasoning Corpus (ARC) benchmark. The framework’s success is measured by its ability to achieve high accuracy on the ARC benchmark while maintaining a low VFE. The authors demonstrate that the system can learn to solve complex problems while avoiding catastrophic failures. The system’s performance is evaluated based on its ability to minimize VFE, which is a measure of the difference between the system’s generative model of the world and its sensory observations. The authors show that the system can achieve high accuracy on the ARC benchmark while maintaining a low VFE, indicating that the system is both accurate and robust.### Discussion and Future DirectionsThe authors’ framework offers a promising path toward AGI development that is inherently safer, rather than retrofitted with safety measures. The key to this approach is the integration of Active Inference principles with LLMs, which provides a robust and flexible foundation for building intelligent systems. The authors highlight the importance of transparency and interpretability, arguing that these are essential for ensuring that AGI systems are aligned with human values. The authors propose several avenues for future research, including: (1) further development of the LLM-based generative models, (2) exploration of different architectures for the multi-agent system, and (3) conducting experiments to validate the framework’s safety properties. The authors emphasize the need for ongoing research to address the challenges of building safe and beneficial AGI. The authors acknowledge the limitations of their approach and suggest several areas for future work, including: developing more sophisticated methods for managing uncertainty, exploring different approaches to value alignment, and conducting more extensive experiments to validate the framework’s safety properties. 

The authors conclude by highlighting the importance of collaboration between researchers in AI, neuroscience, and philosophy to address the complex challenges of building safe and beneficial AGI. 

The authors recognize that the development of safe AGI is a long-term endeavor that will require sustained effort and collaboration.
