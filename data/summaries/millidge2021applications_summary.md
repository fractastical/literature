# Applications of the Free Energy Principle to Machine Learning and Neuroscience

**Authors:** Beren Millidge

**Year:** 2021

**Source:** arxiv

**Venue:** N/A

**DOI:** N/A

**PDF:** [millidge2021applications.pdf](../pdfs/millidge2021applications.pdf)

**Generated:** 2025-12-13 16:23:29

**Validation Status:** ✓ Accepted
**Quality Score:** 0.80

---

Here’s a revised summary of the paper, addressing all the specified issues:**Applications of the Free Energy Principle to Machine Learning and Neuroscience**This thesis explores the application of the Free Energy Principle (FEP) to two key areas: machine learning and neuroscience. The central postulate of the FEP posits that complex systems, including the brain, can be understood as performing variational Bayesian inference to minimize a variational free energy, thereby self-organizing and maintaining a stable state.As Friston (2019a) notes, “the free energy principle (FEP) is an emerging theory in theoretical neuroscience which aims to tackle extremely deep and fundamental question–can one characterise necessary behaviour of any system that maintains a statistical separation from its environment.”The FEP, originating in theoretical neuroscience, has spawned numerous neurophysiologically realistic theories.It’s formulated as a Markov blanket, representing the internal states of a system separated from the external environment by sensor and effector states.As Parr, DaCosta, and Friston (2020) state, “any such system can be seen as performing an elemental kind of Bayesian inference where the dynamics of the internal states of such a system can be interpreted as minimizing a variational free energy functional.”The thesis investigates predictive coding, a neurobiologically plausible process theory derived from the FEP under certain assumptions. This theory argues that the primary function of the brain is to minimize prediction errors, scaling up predictive coding architectures and simulating large-scale predictive coding networks for perception on machine learning benchmarks.Furthermore, the research examines the relationship between predictive coding and other classical filtering algorithms, demonstrating that many biologically implausible aspects of current models of predictive coding can be relaxed without unduly harming performance.The work also applies methods deriving from the free energy principle to action. Specifically, it extends the methods of ‘active inference,’ a neurobiologically grounded account of action through variational message passing, to utilize deep artificial neural networks, allowing these methods to ‘scale up’ to be competitive with state-of-the-art deep reinforcement learning methods.As Beals (2003) explains, “the free energy principle (FEP) is an emerging theory in theoretical neuroscience which aims to tackle extremely deep and fundamental question–can one characterise necessary behaviour of any system that maintains a statistical separation from its environment.”This approach reveals the importance of deep generative models and model-based planning for adaptive action, alongside information-seeking exploration which arises under a unified mathematical framework from active inference.Finally, the thesis explores application of the free energy principle to question of learning.It demonstrates that, under certain conditions, the predictive coding algorithm can closely approximate the backpropagation of error algorithm along arbitrary computation graphs, which underlie the training of essentially all contemporary machine learning architectures.Moreover, the research presents ActivationRelaxation, an algorithm which can approximate backprop using only local learning rules which are substantially simpler than those necessary for predictive coding.As Conant (2019) notes, “the freeenergy principleoriginatedin, andhas beenextremely influentialin theoretical neuroscience,havingspawnedanumberofneurophysiologicallyrealistic processtheories,andmaintainingcloselinkswithBayesianBrainviewpoints.”In summary, this thesis demonstrates the theoretical utility of the free energy principle by demonstrating how methods inspired by it can interface productively with other fields, specifically neuroscience and machine learning, to develop and improve existing methods, as well as inspire novel advances in all three areas of perception, action, and learning.
