# Benchmarking Dynamic SLO Compliance in Distributed Computing Continuum Systems

**Authors:** Alfreds Lapkovskis, Boris Sedlak, Sindri Magnússon, Schahram Dustdar, Praveen Kumar Donta

**Year:** 2025

**Source:** arxiv

**Venue:** N/A

**DOI:** 10.1109/EDGE67623.2025.00020

**PDF:** [lapkovskis2025benchmarking.pdf](../pdfs/lapkovskis2025benchmarking.pdf)

**Generated:** 2025-12-13 19:58:45

**Validation Status:** ✓ Accepted
**Quality Score:** 0.80

---

### OverviewThis paper investigates dynamic Service Level Objective (SLO) compliance in Distributed Computing Continuum Systems (DCCS). The authors state: "Ensuring SLOs in large-capacity, cost, and priority, enhancing real-time processing and minimizing latency. Unlike traditional edge computing, they offer fault tolerance by reallocating tasks to other available servers in case of device failure, ensuring uninterrupted computation [7], [8]." They note: "DCCS prioritize resource efficiency, enabling scalable and adaptive computing across the continuum. They maintain a high Quality of Experience (QoE) despite changing system requirements and environmental uncertainties [9], [10]." The study demonstrates that AIF offers lower memory usage, stable CPU utilization, and fast convergence.### MethodologyThe authors state: “Active Inference (AIF)—a concept from neuroscience—is gaining significant attention due to its ability to efficiently predict and adapt to changing conditions.” They note: “AIF agents attempt to explain the behavior and learning of sentient creatures; to raise the level of intelligence in DCCS, AIF is also increasingly adopted.” The paper implements three reinforcement learning (RL) algorithms: DeepQ-Network (DQN), Advantage Actor-Critic (A2C), and Proximal Policy Optimization (PPO). According to the paper, DQN is a common scheme here to evaluate different approaches, while A2C is used to evaluate the performance of the algorithms. The authors state: “To effectively highlight the differences between AIF and RL techniques, we compare AIF against three well-established RL algorithms.” The authors state: “To ensure SLO compliance, the server hosts an intelligent agent that continuously learns an optimal policy (i.e., streaming configuration) through one of the aforementioned algorithms.” The authors state: “To facilitate reproducible results, we pre-collect a dataset containing various system performance metrics.” The authors state: “We continuously monitor key performance metrics, including CPU usage (M ), memory usage (M ), throughput (M ), average latency (M ), average render scale factor (M ) and thermal state (M )”. The authors state: “To ensure device SLOs, the server hosts an intelligent agent that continuously learns an optimal policy (i.e., streaming configuration) through one of the aforementioned algorithms.” The authors state: “The system configuration itself is characterized by the following streaming parameters: streams, resolution, and fps.” The authors state: “To facilitate reproducible results, we pre-collect a dataset containing various system performance metrics.” 

The authors state: “

We continuously monitor key performance metrics, including CPU usage (M ), memory usage (M ), throughput (M ), average latency (M ), average render scale factor (M ) and thermal state (M )”
