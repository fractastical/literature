# Universal AI maximizes Variational Empowerment

**Authors:** Yusuke Hayashi, Koichi Takahashi

**Year:** 2025

**Source:** arxiv

**Venue:** N/A

**DOI:** N/A

**PDF:** [hayashi2025universal.pdf](../pdfs/hayashi2025universal.pdf)

**Generated:** 2025-12-14 01:07:16

**Validation Status:** ✓ Accepted
**Quality Score:** 0.80

---

### OverviewThis paper investigates the theoretical framework uniting AIXI—a model of universal AI—with variational empowerment as an intrinsic drive for exploration. We build upon Self-AIXI, a universal learning agent predicting its own actions, demonstrating how one of its established termscan be interpreted as a variational empowerment objective. We further demonstrate that universal AI’s planning process can be cast as minimizing expected variational free energy (the core principle of Active Inference), thereby revealing how universal AI agents inherently balance goal-directed behavior with uncertainty reduction and curiosity. Moreover, we argue that power-seeking tendencies of universal AI agents can be explained not only as an instrumental strategy to secure future reward, but also as a direct consequence of empowerment maximization—i.e., the agent’s intrinsic drive to maintain or expand its own controllability in uncertain environments. Our main contribution is to show how these intrinsic motivations (empowerment, curiosity) systematically lead universal AI agents to seek and sustain high-optionality states. We prove that Self-AIXI asymptotically converges to the same performance as AIXI under suitable conditions, and highlight that its power-seeking behavior emerges naturally from both reward maximization and curiosity-driven exploration. Since AIXI can be viewed as a Bayes-optimal mathematical formulation for Artificial General Intelligence (AGI), our result can be useful for further discussion on AIsafetyandthecontrollabilityofAGI.### MethodologyThe authors’ approach centers on reinterpreting a term in Self-AIXI as variational empowerment. They state: "The authors state: "maximizing empowerment often manifests as power-seeking in the environment, prompting parallel storesource-acquiringorinfluence-drivenbehaviorsinhuman organizations." They note: "The paper argues that power-seeking tendencies of universal AI agents can be explained not only as an instrumental strategy to secure future reward, but also as a direct consequence of empowerment maximization—i.e. the agent’s intrinsic drive to maintain or expand its own controllability in uncertain environments." The study demonstrates explicitly, through two key equations, that AIXI’s decision criterion is mathematically equivalent to minimizing expected variational free energy, the core principle of Active Inference. They further show that the agent’s mixture-policy regularization term can be reinterpreted as a variational empowerment objective. They state: “The authors state: “maximizing empowerment often manifests as power-seeking in the environment, prompting parallel storesource-acquiringorinfluence-drivenbehaviorsinhuman organizations.”### Results

The authors prove that 

Self-AIXI asymptotically converges to the same performance as AIXI under suitable conditions, and highlight that its power-seeking behavior emerges naturally from both reward maximization and curiosity-driven exploration
