"""Additional comprehensive tests for infrastructure/llm/core.py.

Tests LLM core functionality using real data (No Mocks Policy):
- Pure logic tests for config, context, options
- Integration tests marked with @pytest.mark.requires_ollama for network calls
"""

import pytest

from infrastructure.llm.core.client import LLMClient, ResponseMode
from infrastructure.llm.core.config import LLMConfig, GenerationOptions


class TestResponseModeDetails:
    """Test ResponseMode enum details."""
    
    def test_response_mode_comparison(self):
        """Test ResponseMode can be compared."""
        assert ResponseMode.SHORT == ResponseMode.SHORT
        assert ResponseMode.SHORT != ResponseMode.LONG
    
    def test_response_mode_str(self):
        """Test ResponseMode string conversion."""
        assert str(ResponseMode.SHORT) == "ResponseMode.SHORT"
        assert ResponseMode.SHORT.value == "short"


class TestGenerationOptionsDetails:
    """Test GenerationOptions in detail."""
    
    def test_options_with_seed(self):
        """Test options with seed for reproducibility."""
        opts = GenerationOptions(seed=42, temperature=0.0)
        
        assert opts.seed == 42
        assert opts.temperature == 0.0
    
    def test_options_with_max_tokens(self):
        """Test options with max_tokens."""
        opts = GenerationOptions(max_tokens=100)
        
        assert opts.max_tokens == 100
    
    def test_options_with_all_params(self):
        """Test options with all parameters."""
        opts = GenerationOptions(
            temperature=0.5,
            max_tokens=1000,
            top_p=0.95,
            top_k=50,
            seed=42,
            stop=["END"],
            format_json=True,
            repeat_penalty=1.1,
            num_ctx=4096,
        )
        
        assert opts.temperature == 0.5
        assert opts.max_tokens == 1000
        assert opts.top_p == 0.95
        assert opts.top_k == 50
        assert opts.seed == 42
        assert opts.stop == ["END"]
        assert opts.format_json is True
        assert opts.repeat_penalty == 1.1
        assert opts.num_ctx == 4096
    
    def test_options_defaults(self):
        """Test default option values."""
        opts = GenerationOptions()
        
        assert opts.temperature is None
        assert opts.max_tokens is None
        assert opts.seed is None
        assert opts.format_json is False


class TestContextManagementPure:
    """Test conversation context management (pure logic)."""
    
    def test_context_accumulates(self):
        """Test context accumulates messages."""
        config = LLMConfig(auto_inject_system_prompt=False)
        client = LLMClient(config=config)
        
        client.context.add_message("user", "First message")
        client.context.add_message("assistant", "First response")
        client.context.add_message("user", "Second message")
        
        messages = client.context.get_messages()
        user_messages = [m for m in messages if m.get('role') == 'user']
        assert len(user_messages) == 2
    
    def test_context_preserves_order(self):
        """Test context preserves message order."""
        config = LLMConfig(auto_inject_system_prompt=False)
        client = LLMClient(config=config)
        
        client.context.add_message("user", "First")
        client.context.add_message("assistant", "Second")
        client.context.add_message("user", "Third")
        
        messages = client.context.get_messages()
        contents = [m['content'] for m in messages]
        assert contents == ["First", "Second", "Third"]
    
    def test_context_reset(self):
        """Test context reset clears messages."""
        config = LLMConfig(auto_inject_system_prompt=False)
        client = LLMClient(config=config)
        
        client.context.add_message("user", "Test")
        client.reset()
        
        messages = client.context.get_messages()
        assert len(messages) == 0
    
    def test_context_token_estimation(self):
        """Test context tracks token estimation."""
        config = LLMConfig(auto_inject_system_prompt=False)
        client = LLMClient(config=config)
        
        # Empty context
        assert client.context.estimated_tokens == 0
        
        # Add message
        client.context.add_message("user", "Test message here")
        assert client.context.estimated_tokens > 0


class TestLLMClientPropertiesPure:
    """Test LLMClient properties (pure logic)."""
    
    def test_config_property(self):
        """Test config property."""
        config = LLMConfig(temperature=0.5)
        client = LLMClient(config=config)
        
        assert client.config.temperature == 0.5
    
    def test_context_property(self):
        """Test context property."""
        client = LLMClient()
        
        assert client.context is not None
        assert hasattr(client.context, 'get_messages')
        assert hasattr(client.context, 'add_message')
        assert hasattr(client.context, 'clear')
    
    def test_config_immutability(self):
        """Test config changes don't affect client."""
        config = LLMConfig(temperature=0.5)
        client = LLMClient(config=config)
        
        # Changing external config shouldn't affect client
        # (config is stored by reference, but we test the pattern)
        assert client.config.temperature == 0.5


class TestSystemPromptManagementPure:
    """Test system prompt management (pure logic)."""
    
    def test_set_system_prompt(self):
        """Test setting system prompt."""
        config = LLMConfig(
            system_prompt="Initial prompt",
            auto_inject_system_prompt=True
        )
        client = LLMClient(config=config)
        
        client.set_system_prompt("New system prompt")
        
        messages = client.context.get_messages()
        system_messages = [m for m in messages if m.get('role') == 'system']
        assert len(system_messages) == 1
        assert system_messages[0]['content'] == "New system prompt"
    
    def test_inject_system_prompt_manual(self):
        """Test _inject_system_prompt method manually."""
        config = LLMConfig(
            system_prompt="Test prompt",
            auto_inject_system_prompt=False
        )
        client = LLMClient(config=config)
        
        # Initially no system prompt
        messages = client.context.get_messages()
        assert len(messages) == 0
        
        # Manually inject
        client._inject_system_prompt()
        
        messages = client.context.get_messages()
        system_messages = [m for m in messages if m.get('role') == 'system']
        assert len(system_messages) == 1
    
    def test_system_prompt_preserved_on_reset(self):
        """Test system prompt is preserved when auto-inject is on."""
        config = LLMConfig(
            system_prompt="Persistent prompt",
            auto_inject_system_prompt=True
        )
        client = LLMClient(config=config)
        
        # Add user message
        client.context.add_message("user", "Test")
        
        # Reset
        client.reset()
        
        # System prompt should be back
        messages = client.context.get_messages()
        assert len(messages) == 1
        assert messages[0]['role'] == 'system'


class TestLLMCoreEdgeCasesPure:
    """Test edge cases for LLM core (pure logic)."""
    
    def test_empty_system_prompt(self):
        """Test handling empty system prompt."""
        config = LLMConfig(
            system_prompt="",
            auto_inject_system_prompt=True
        )
        client = LLMClient(config=config)
        
        # Empty system prompt still results in a system message (empty content)
        # Actually, when system_prompt is empty, _inject_system_prompt skips injection
        # because of the falsy check on self.config.system_prompt
        messages = client.context.get_messages()
        # Empty prompt is falsy, so no system message is added
        assert len(messages) == 0
    
    def test_long_system_prompt(self):
        """Test handling long system prompt."""
        long_prompt = "System instruction. " * 100
        config = LLMConfig(
            system_prompt=long_prompt,
            auto_inject_system_prompt=True
        )
        client = LLMClient(config=config)
        
        messages = client.context.get_messages()
        system_msg = next((m for m in messages if m.get('role') == 'system'), None)
        assert system_msg is not None
        assert len(system_msg['content']) > 1000
    
    def test_multiple_resets(self):
        """Test multiple reset calls."""
        config = LLMConfig(
            system_prompt="Test",
            auto_inject_system_prompt=True
        )
        client = LLMClient(config=config)
        
        # Multiple resets should be idempotent
        client.reset()
        client.reset()
        client.reset()
        
        messages = client.context.get_messages()
        system_msgs = [m for m in messages if m.get('role') == 'system']
        assert len(system_msgs) == 1
    
    def test_config_response_mode_limits(self):
        """Test config response mode token limits."""
        config = LLMConfig(
            short_max_tokens=100,
            long_max_tokens=5000,
            long_min_tokens=400
        )
        
        assert config.short_max_tokens == 100
        assert config.long_max_tokens == 5000
        assert config.long_min_tokens == 400


class TestOptionsConversion:
    """Test options conversion to Ollama format."""
    
    def test_options_num_ctx(self):
        """Test num_ctx option."""
        config = LLMConfig(num_ctx=8192)
        opts = GenerationOptions()
        
        ollama_opts = opts.to_ollama_options(config)
        assert ollama_opts.get("num_ctx") == 8192
    
    def test_options_override_num_ctx(self):
        """Test num_ctx override."""
        config = LLMConfig(num_ctx=4096)
        opts = GenerationOptions(num_ctx=16384)
        
        ollama_opts = opts.to_ollama_options(config)
        assert ollama_opts.get("num_ctx") == 16384
    
    def test_options_repeat_penalty(self):
        """Test repeat_penalty option."""
        config = LLMConfig()
        opts = GenerationOptions(repeat_penalty=1.2)
        
        ollama_opts = opts.to_ollama_options(config)
        assert ollama_opts.get("repeat_penalty") == 1.2
    
    def test_options_top_k(self):
        """Test top_k option."""
        config = LLMConfig()
        opts = GenerationOptions(top_k=50)
        
        ollama_opts = opts.to_ollama_options(config)
        assert ollama_opts.get("top_k") == 50


# =============================================================================
# INTEGRATION TESTS (Require Ollama)
# =============================================================================

@pytest.mark.requires_ollama
class TestLLMQueryModesIntegration:
    """Integration tests for query modes requiring Ollama.
    
    Run with: pytest -m requires_ollama
    Skip with: pytest -m "not requires_ollama"
    """
    
    @pytest.fixture(autouse=True)
    def check_ollama(self):
        """Skip tests if Ollama is not available."""
        client = LLMClient()
        if not client.check_connection():
            pytest.skip("Ollama server not available")
    
    def test_query_short(self):
        """Test short query mode."""
        client = LLMClient()
        result = client.query_short("What is 2+2? Answer briefly.")
        
        assert result is not None
        if len(result) == 0:
            pytest.skip("Model returned empty response (transient issue)")
    
    @pytest.mark.timeout(120)
    def test_query_long(self):
        """Test long query mode."""
        client = LLMClient()
        result = client.query_long("Explain what Python is in detail.")
        
        assert result is not None
        assert len(result) > 0
    
    def test_query_raw(self):
        """Test raw query mode."""
        config = LLMConfig(auto_inject_system_prompt=False)
        client = LLMClient(config=config)
        result = client.query_raw("Complete: Hello")
        
        assert result is not None
        assert len(result) > 0
    
    def test_query_with_options(self):
        """Test query with generation options."""
        client = LLMClient()
        opts = GenerationOptions(temperature=0.0, max_tokens=50)
        
        result = client.query("Say 'test'", options=opts)
        assert result is not None
    
    def test_context_maintained(self):
        """Test context is maintained across queries."""
        config = LLMConfig(auto_inject_system_prompt=False)
        client = LLMClient(config=config)
        
        client.query("My name is TestBot.")
        response = client.query("What is my name?")
        
        # Context should help the model remember
        assert response is not None
